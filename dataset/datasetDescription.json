[
    {
        "source": "https://arxiv.org/abs/2002.11893",
        "id": 6,
        "urls": [
            "https://arxiv.org/abs/2002.11893",
            "https://arxiv.org/abs/2002.01359",
            "https://arxiv.org/abs/2007.12720"
        ],
        "citations": [
            109,
            49,
            282
        ],
        "category": [
            "cs.CL",
            "cs.AI"
        ],
        "titles": [
            "CrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Dataset",
            "Schema-Guided Dialogue State Tracking Task at DSTC8",
            "MultiWOZ 2.2 : A Dialogue Dataset with Additional Annotation Corrections and State Tracking Baselines"
        ],
        "abstracts": [
            "To advance multi-domain (cross-domain) dialogue modeling as well as alleviate\nthe shortage of Chinese task-oriented datasets, we propose CrossWOZ, the first\nlarge-scale Chinese Cross-Domain Wizard-of-Oz task-oriented dataset. It\ncontains 6K dialogue sessions and 102K utterances for 5 domains, including\nhotel, restaurant, attraction, metro, and taxi. Moreover, the corpus contains\nrich annotation of dialogue states and dialogue acts at both user and system\nsides. About 60% of the dialogues have cross-domain user goals that favor\ninter-domain dependency and encourage natural transition across domains in\nconversation. We also provide a user simulator and several benchmark models for\npipelined task-oriented dialogue systems, which will facilitate researchers to\ncompare and evaluate their models on this corpus. The large size and rich\nannotation of CrossWOZ make it suitable to investigate a variety of tasks in\ncross-domain dialogue modeling, such as dialogue state tracking, policy\nlearning, user simulation, etc.",
            "This paper gives an overview of the Schema-Guided Dialogue State Tracking\ntask of the 8th Dialogue System Technology Challenge. The goal of this task is\nto develop dialogue state tracking models suitable for large-scale virtual\nassistants, with a focus on data-efficient joint modeling across domains and\nzero-shot generalization to new APIs. This task provided a new dataset\nconsisting of over 16000 dialogues in the training set spanning 16 domains to\nhighlight these challenges, and a baseline model capable of zero-shot\ngeneralization to new APIs. Twenty-five teams participated, developing a range\nof neural network models, exceeding the performance of the baseline model by a\nvery high margin. The submissions incorporated a variety of pre-trained\nencoders and data augmentation techniques. This paper describes the task\ndefinition, dataset and evaluation methodology. We also summarize the approach\nand results of the submitted systems to highlight the overall trends in the\nstate-of-the-art.",
            "MultiWOZ is a well-known task-oriented dialogue dataset containing over\n10,000 annotated dialogues spanning 8 domains. It is extensively used as a\nbenchmark for dialogue state tracking. However, recent works have reported\npresence of substantial noise in the dialogue state annotations. MultiWOZ 2.1\nidentified and fixed many of these erroneous annotations and user utterances,\nresulting in an improved version of this dataset. This work introduces MultiWOZ\n2.2, which is a yet another improved version of this dataset. Firstly, we\nidentify and fix dialogue state annotation errors across 17.3% of the\nutterances on top of MultiWOZ 2.1. Secondly, we redefine the ontology by\ndisallowing vocabularies of slots with a large number of possible values (e.g.,\nrestaurant name, time of booking). In addition, we introduce slot span\nannotations for these slots to standardize them across recent models, which\npreviously used custom string matching heuristics to generate them. We also\nbenchmark a few state of the art dialogue state tracking models on the\ncorrected dataset to facilitate comparison for future work. In the end, we\ndiscuss best practices for dialogue data collection that can help avoid\nannotation errors."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2404.02827",
        "id": 7,
        "urls": [
            "https://arxiv.org/abs/2404.02827",
            "https://arxiv.org/abs/2403.03507",
            "https://arxiv.org/abs/2106.09685",
            "https://arxiv.org/abs/2307.05695",
            "https://arxiv.org/abs/2402.09353",
            "https://arxiv.org/abs/2402.12354",
            "https://arxiv.org/abs/2312.03732",
            "https://arxiv.org/abs/2303.10512"
        ],
        "citations": [
            10,
            43,
            10294,
            23,
            244,
            52,
            42,
            443
        ],
        "category": [
            "cs.LG",
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "stat.ML",
            "I.2.7"
        ],
        "titles": [
            "BAdam: A Memory Efficient Full Parameter Optimization Method for Large Language Models",
            "GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection",
            "LoRA: Low-Rank Adaptation of Large Language Models",
            "ReLoRA: High-Rank Training Through Low-Rank Updates",
            "DoRA: Weight-Decomposed Low-Rank Adaptation",
            "LoRA+: Efficient Low Rank Adaptation of Large Models",
            "A Rank Stabilization Scaling Factor for Fine-Tuning with LoRA",
            "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning"
        ],
        "abstracts": [
            "This work presents BAdam, an optimization method that leverages the block\ncoordinate descent (BCD) framework with Adam's update rule. BAdam offers a\nmemory efficient approach to the full parameter finetuning of large language\nmodels. We conduct a theoretical convergence analysis for BAdam in the\ndeterministic case. Experimentally, we apply BAdam to finetune the Llama 3-8B\nand Llama 3-70B models using a single RTX3090-24GB GPU and 4 A100-80GB GPUs,\nrespectively. The results confirm BAdam's efficiency in terms of memory usage,\nrunning time, and optimization capability. Furthermore, the downstream\nperformance evaluation based on MT-bench and math benchmarks shows that BAdam\noutperforms existing memory efficient baselines such as LoRA. It also\ndemonstrates that BAdam can achieve comparable or even superior performance\ncompared to Adam. Finally, the ablation study using SGD's update rule\nillustrates the suitability of BCD for finetuning LLMs. Our code can be easily\nintegrated into any PyTorch-based codebase and is available at\nhttps://github.com/Ledzy/BAdam.",
            "Training Large Language Models (LLMs) presents significant memory challenges,\npredominantly due to the growing size of weights and optimizer states. Common\nmemory-reduction approaches, such as low-rank adaptation (LoRA), add a\ntrainable low-rank matrix to the frozen pre-trained weight in each layer,\nreducing trainable parameters and optimizer states. However, such approaches\ntypically underperform training with full-rank weights in both pre-training and\nfine-tuning stages since they limit the parameter search to a low-rank subspace\nand alter the training dynamics, and further, may require full-rank warm start.\nIn this work, we propose Gradient Low-Rank Projection (GaLore), a training\nstrategy that allows full-parameter learning but is more memory-efficient than\ncommon low-rank adaptation methods such as LoRA. Our approach reduces memory\nusage by up to 65.5% in optimizer states while maintaining both efficiency and\nperformance for pre-training on LLaMA 1B and 7B architectures with C4 dataset\nwith up to 19.7B tokens, and on fine-tuning RoBERTa on GLUE tasks. Our 8-bit\nGaLore further reduces optimizer memory by up to 82.5% and total training\nmemory by 63.3%, compared to a BF16 baseline. Notably, we demonstrate, for the\nfirst time, the feasibility of pre-training a 7B model on consumer GPUs with\n24GB memory (e.g., NVIDIA RTX 4090) without model parallel, checkpointing, or\noffloading strategies.",
            "An important paradigm of natural language processing consists of large-scale\npre-training on general domain data and adaptation to particular tasks or\ndomains. As we pre-train larger models, full fine-tuning, which retrains all\nmodel parameters, becomes less feasible. Using GPT-3 175B as an example --\ndeploying independent instances of fine-tuned models, each with 175B\nparameters, is prohibitively expensive. We propose Low-Rank Adaptation, or\nLoRA, which freezes the pre-trained model weights and injects trainable rank\ndecomposition matrices into each layer of the Transformer architecture, greatly\nreducing the number of trainable parameters for downstream tasks. Compared to\nGPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable\nparameters by 10,000 times and the GPU memory requirement by 3 times. LoRA\nperforms on-par or better than fine-tuning in model quality on RoBERTa,\nDeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher\ntraining throughput, and, unlike adapters, no additional inference latency. We\nalso provide an empirical investigation into rank-deficiency in language model\nadaptation, which sheds light on the efficacy of LoRA. We release a package\nthat facilitates the integration of LoRA with PyTorch models and provide our\nimplementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at\nhttps://github.com/microsoft/LoRA.",
            "Despite the dominance and effectiveness of scaling, resulting in large\nnetworks with hundreds of billions of parameters, the necessity to train\noverparameterized models remains poorly understood, while training costs grow\nexponentially. In this paper, we explore parameter-efficient training\ntechniques as an approach to training large neural networks. We introduce a\nnovel method called ReLoRA, which utilizes low-rank updates to train high-rank\nnetworks. We apply ReLoRA to training transformer language models with up to\n1.3B parameters and demonstrate comparable performance to regular neural\nnetwork training. ReLoRA saves up to 5.5Gb of RAM per GPU and improves training\nspeed by 9-40% depending on the model size and hardware setup. Our findings\nshow the potential of parameter-efficient techniques for large-scale\npre-training.",
            "Among the widely used parameter-efficient fine-tuning (PEFT) methods, LoRA\nand its variants have gained considerable popularity because of avoiding\nadditional inference costs. However, there still often exists an accuracy gap\nbetween these methods and full fine-tuning (FT). In this work, we first\nintroduce a novel weight decomposition analysis to investigate the inherent\ndifferences between FT and LoRA. Aiming to resemble the learning capacity of FT\nfrom the findings, we propose Weight-Decomposed Low-Rank Adaptation (DoRA).\nDoRA decomposes the pre-trained weight into two components, magnitude and\ndirection, for fine-tuning, specifically employing LoRA for directional updates\nto efficiently minimize the number of trainable parameters. By employing \\ours,\nwe enhance both the learning capacity and training stability of LoRA while\navoiding any additional inference overhead. \\ours~consistently outperforms LoRA\non fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as\ncommonsense reasoning, visual instruction tuning, and image/video-text\nunderstanding. Code is available at https://github.com/NVlabs/DoRA.",
            "In this paper, we show that Low Rank Adaptation (LoRA) as originally\nintroduced in Hu et al. (2021) leads to suboptimal finetuning of models with\nlarge width (embedding dimension). This is due to the fact that adapter\nmatrices A and B in LoRA are updated with the same learning rate. Using scaling\narguments for large width networks, we demonstrate that using the same learning\nrate for A and B does not allow efficient feature learning. We then show that\nthis suboptimality of LoRA can be corrected simply by setting different\nlearning rates for the LoRA adapter matrices A and B with a well-chosen ratio.\nWe call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$\nimproves performance (1-2 $\\%$ improvements) and finetuning speed (up to $\\sim$\n2X SpeedUp), at the same computational cost as LoRA.",
            "As large language models (LLMs) have become increasingly compute and memory\nintensive, parameter-efficient fine-tuning (PEFT) methods are now a common\nstrategy to fine-tune LLMs. A popular PEFT method is Low-Rank Adapters (LoRA),\nwhich adds trainable low-rank \"adapters\" to selected layers. Each adapter\nconsists of a low-rank matrix product, multiplicatively scaled by a\nrank-dependent factor. This scaling factor, which divides adapters by a factor\nof the rank, results in slowed learning and stunted performance for LoRA with\nhigher-rank adapters. Consequently, the use of LoRA in practice has generally\nbeen limited to very low ranks. In this work, we study the impact of the\nscaling factor on the learning process and prove that LoRA adapters should be\ndivided by a factor of the square root of the rank. Modifying LoRA with the\nappropriate scaling factor, which we call the rank-stabilized LoRA (rsLoRA)\nmethod, easily provides for a fine-tuning compute/performance trade-off, where\nlarger ranks can be used to trade off increased computational resources during\ntraining for better fine-tuning performance, with no change in inference\ncomputing cost.",
            "Fine-tuning large pre-trained language models on downstream tasks has become\nan important paradigm in NLP. However, common practice fine-tunes all of the\nparameters in a pre-trained model, which becomes prohibitive when a large\nnumber of downstream tasks are present. Therefore, many fine-tuning methods are\nproposed to learn incremental updates of pre-trained weights in a parameter\nefficient way, e.g., low-rank increments. These methods often evenly distribute\nthe budget of incremental updates across all pre-trained weight matrices, and\noverlook the varying importance of different weight parameters. As a\nconsequence, the fine-tuning performance is suboptimal. To bridge this gap, we\npropose AdaLoRA, which adaptively allocates the parameter budget among weight\nmatrices according to their importance score. In particular, AdaLoRA\nparameterizes the incremental updates in the form of singular value\ndecomposition. Such a novel approach allows us to effectively prune the\nsingular values of unimportant updates, which is essentially to reduce their\nparameter budget but circumvent intensive exact SVD computations. We conduct\nextensive experiments with several pre-trained models on natural language\nprocessing, question answering, and natural language generation to validate the\neffectiveness of AdaLoRA. Results demonstrate that AdaLoRA manifests notable\nimprovement over baselines, especially in the low budget settings. Our code is\npublicly available at https://github.com/QingruZhang/AdaLoRA ."
        ]
    },
    {
        "source": "https://arxiv.org/abs/1804.07461",
        "id": 8,
        "urls": [
            "https://arxiv.org/abs/1804.07461",
            "https://arxiv.org/abs/1905.00537",
            "https://arxiv.org/abs/1810.12885"
        ],
        "citations": [
            8006,
            2435,
            284
        ],
        "category": [
            "cs.CL",
            "cs.AI"
        ],
        "titles": [
            "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
            "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems",
            "ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension"
        ],
        "abstracts": [
            "For natural language understanding (NLU) technology to be maximally useful,\nboth practically and as a scientific object of study, it must be general: it\nmust be able to process language in a way that is not exclusively tailored to\nany one specific task or dataset. In pursuit of this objective, we introduce\nthe General Language Understanding Evaluation benchmark (GLUE), a tool for\nevaluating and analyzing the performance of models across a diverse range of\nexisting NLU tasks. GLUE is model-agnostic, but it incentivizes sharing\nknowledge across tasks because certain tasks have very limited training data.\nWe further provide a hand-crafted diagnostic test suite that enables detailed\nlinguistic analysis of NLU models. We evaluate baselines based on current\nmethods for multi-task and transfer learning and find that they do not\nimmediately give substantial improvements over the aggregate performance of\ntraining a separate model per task, indicating room for improvement in\ndeveloping general and robust NLU systems.",
            "In the last year, new models and methods for pretraining and transfer\nlearning have driven striking performance improvements across a range of\nlanguage understanding tasks. The GLUE benchmark, introduced a little over one\nyear ago, offers a single-number metric that summarizes progress on a diverse\nset of such tasks, but performance on the benchmark has recently surpassed the\nlevel of non-expert humans, suggesting limited headroom for further research.\nIn this paper we present SuperGLUE, a new benchmark styled after GLUE with a\nnew set of more difficult language understanding tasks, a software toolkit, and\na public leaderboard. SuperGLUE is available at super.gluebenchmark.com.",
            "We present a large-scale dataset, ReCoRD, for machine reading comprehension\nrequiring commonsense reasoning. Experiments on this dataset demonstrate that\nthe performance of state-of-the-art MRC systems fall far behind human\nperformance. ReCoRD represents a challenge for future research to bridge the\ngap between human and machine commonsense reading comprehension. ReCoRD is\navailable at http://nlp.jhu.edu/record."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2406.09904",
        "description": "量化方法 GPTQ,QQQ,AWQ",
        "urls": [
            "https://arxiv.org/abs/2406.09904",
            "https://arxiv.org/abs/2306.00978",
            "https://arxiv.org/abs/2210.17323"
        ],
        "citations": [
            3,
            623,
            723
        ],
        "category": [
            "cs.LG",
            "cs.CL"
        ],
        "titles": [
            "QQQ: Quality Quattuor-Bit Quantization for Large Language Models",
            "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration",
            "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers"
        ],
        "id": 9,
        "abstracts": [
            "Quantization is a proven effective method for compressing large language\nmodels. Although popular techniques like W8A8 and W4A16 effectively maintain\nmodel performance, they often fail to concurrently speed up the prefill and\ndecoding stages of inference. W4A8 is a promising strategy to accelerate both\nof them while usually leads to a significant performance degradation. To\naddress these issues, we present QQQ, a Quality Quattuor-bit Quantization\nmethod with 4-bit weights and 8-bit activations. QQQ employs adaptive smoothing\nand Hessian-based compensation, significantly enhancing the performance of\nquantized models without extensive training. Furthermore, we meticulously\nengineer W4A8 GEMM kernels to increase inference speed. Our specialized\nper-channel W4A8 GEMM and per-group W4A8 GEMM achieve impressive speed\nincreases of 3.67$\\times$ and 3.29 $\\times$ over FP16 GEMM. Our extensive\nexperiments show that QQQ achieves performance on par with existing\nstate-of-the-art LLM quantization methods while significantly accelerating\ninference, achieving speed boosts up to 2.24 $\\times$, 2.10$\\times$, and\n1.25$\\times$ compared to FP16, W8A8, and W4A16, respectively.",
            "Large language models (LLMs) have transformed numerous AI applications.\nOn-device LLM is becoming increasingly important: running LLMs locally on edge\ndevices can reduce the cloud computing cost and protect users' privacy.\nHowever, the astronomical model size and the limited hardware resource pose\nsignificant deployment challenges. We propose Activation-aware Weight\nQuantization (AWQ), a hardware-friendly approach for LLM low-bit weight-only\nquantization. AWQ finds that not all weights in an LLM are equally important.\nProtecting only 1% salient weights can greatly reduce quantization error. To\nidentify salient weight channels, we should refer to the activation\ndistribution, not weights. To avoid the hardware-inefficient mix-precision\nquantization, we mathematically derive that scaling up the salient channels can\nreduce the quantization error. AWQ employs an equivalent transformation to\nscale the salient weight channels to protect them. The scale is determined by\ncollecting the activation statistics offline. AWQ does not rely on any\nbackpropagation or reconstruction, so it generalizes to different domains and\nmodalities without overfitting the calibration set. AWQ outperforms existing\nwork on various language modeling and domain-specific benchmarks (coding and\nmath). Thanks to better generalization, it achieves excellent quantization\nperformance for instruction-tuned LMs and, for the first time, multi-modal LMs.\nAlongside AWQ, we implement TinyChat, an efficient and flexible inference\nframework tailored for 4-bit on-device LLM/VLMs. With kernel fusion and\nplatform-aware weight packing, TinyChat offers more than 3x speedup over the\nHuggingface FP16 implementation on both desktop and mobile GPUs. It also\ndemocratizes the deployment of the 70B Llama-2 model on mobile GPUs.",
            "Generative Pre-trained Transformer models, known as GPT or OPT, set\nthemselves apart through breakthrough performance across complex language\nmodelling tasks, but also by their extremely high computational and storage\ncosts. Specifically, due to their massive size, even inference for large,\nhighly-accurate GPT models may require multiple performant GPUs, which limits\nthe usability of such models. While there is emerging work on relieving this\npressure via model compression, the applicability and performance of existing\ncompression techniques is limited by the scale and complexity of GPT models. In\nthis paper, we address this challenge, and propose GPTQ, a new one-shot weight\nquantization method based on approximate second-order information, that is both\nhighly-accurate and highly-efficient. Specifically, GPTQ can quantize GPT\nmodels with 175 billion parameters in approximately four GPU hours, reducing\nthe bitwidth down to 3 or 4 bits per weight, with negligible accuracy\ndegradation relative to the uncompressed baseline. Our method more than doubles\nthe compression gains relative to previously-proposed one-shot quantization\nmethods, preserving accuracy, allowing us for the first time to execute an 175\nbillion-parameter model inside a single GPU for generative inference. Moreover,\nwe also show that our method can still provide reasonable accuracy in the\nextreme quantization regime, in which weights are quantized to 2-bit or even\nternary quantization levels. We show experimentally that these improvements can\nbe leveraged for end-to-end inference speedups over FP16, of around 3.25x when\nusing high-end GPUs (NVIDIA A100) and 4.5x when using more cost-effective ones\n(NVIDIA A6000). The implementation is available at\nhttps://github.com/IST-DASLab/gptq."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2402.01306",
        "description": "KTO DPO PPO",
        "urls": [
            "https://arxiv.org/abs/2402.01306",
            "https://arxiv.org/abs/2305.18290",
            "https://arxiv.org/abs/2403.07691"
        ],
        "citations": [
            266,
            2254,
            66
        ],
        "category": [
            "cs.AI",
            "cs.LG",
            "cs.CL"
        ],
        "titles": [
            "KTO: Model Alignment as Prospect Theoretic Optimization",
            "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
            "ORPO: Monolithic Preference Optimization without Reference Model"
        ],
        "id": 10,
        "abstracts": [
            "Kahneman & Tversky's $\\textit{prospect theory}$ tells us that humans perceive\nrandom variables in a biased but well-defined manner (1992); for example,\nhumans are famously loss-averse. We show that objectives for aligning LLMs with\nhuman feedback implicitly incorporate many of these biases -- the success of\nthese objectives (e.g., DPO) over cross-entropy minimization can partly be\nascribed to them belonging to a family of loss functions that we call\n$\\textit{human-aware losses}$ (HALOs). However, the utility functions these\nmethods attribute to humans still differ from those in the prospect theory\nliterature. Using a Kahneman-Tversky model of human utility, we propose a HALO\nthat directly maximizes the utility of generations instead of maximizing the\nlog-likelihood of preferences, as current methods do. We call this approach\nKTO, and it matches or exceeds the performance of preference-based methods at\nscales from 1B to 30B, despite only learning from a binary signal of whether an\noutput is desirable. More broadly, our work suggests that there is no one HALO\nthat is universally superior; the best loss depends on the inductive biases\nmost appropriate for a given setting, an oft-overlooked consideration.",
            "While large-scale unsupervised language models (LMs) learn broad world\nknowledge and some reasoning skills, achieving precise control of their\nbehavior is difficult due to the completely unsupervised nature of their\ntraining. Existing methods for gaining such steerability collect human labels\nof the relative quality of model generations and fine-tune the unsupervised LM\nto align with these preferences, often with reinforcement learning from human\nfeedback (RLHF). However, RLHF is a complex and often unstable procedure, first\nfitting a reward model that reflects the human preferences, and then\nfine-tuning the large unsupervised LM using reinforcement learning to maximize\nthis estimated reward without drifting too far from the original model. In this\npaper we introduce a new parameterization of the reward model in RLHF that\nenables extraction of the corresponding optimal policy in closed form, allowing\nus to solve the standard RLHF problem with only a simple classification loss.\nThe resulting algorithm, which we call Direct Preference Optimization (DPO), is\nstable, performant, and computationally lightweight, eliminating the need for\nsampling from the LM during fine-tuning or performing significant\nhyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align\nwith human preferences as well as or better than existing methods. Notably,\nfine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of\ngenerations, and matches or improves response quality in summarization and\nsingle-turn dialogue while being substantially simpler to implement and train.",
            "While recent preference alignment algorithms for language models have\ndemonstrated promising results, supervised fine-tuning (SFT) remains imperative\nfor achieving successful convergence. In this paper, we study the crucial role\nof SFT within the context of preference alignment, emphasizing that a minor\npenalty for the disfavored generation style is sufficient for\npreference-aligned SFT. Building on this foundation, we introduce a\nstraightforward and innovative reference model-free monolithic odds ratio\npreference optimization algorithm, ORPO, eliminating the necessity for an\nadditional preference alignment phase. We demonstrate, both empirically and\ntheoretically, that the odds ratio is a sensible choice for contrasting favored\nand disfavored styles during SFT across the diverse sizes from 125M to 7B.\nSpecifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with\nORPO on the UltraFeedback alone surpasses the performance of state-of-the-art\nlanguage models with more than 7B and 13B parameters: achieving up to 12.20% on\n$\\text{AlpacaEval}_{2.0}$ (Figure 1), 66.19% on IFEval (instruction-level\nloose, Table 6), and 7.32 in MT-Bench (Figure 12). We release code and model\ncheckpoints for Mistral-ORPO-$\\alpha$ (7B) and Mistral-ORPO-$\\beta$ (7B)."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2412.08905",
        "description": "Phi系列 technical report",
        "urls": [
            "https://arxiv.org/abs/2412.08905",
            "https://arxiv.org/abs/2404.14219",
            "https://arxiv.org/abs/2309.05463"
        ],
        "citations": [
            9,
            723,
            390
        ],
        "category": [
            "cs.CL",
            "cs.AI"
        ],
        "titles": [
            "Phi-4 Technical Report",
            "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone",
            "Textbooks Are All You Need II: phi-1.5 technical report"
        ],
        "id": 11,
        "abstracts": [
            "We present phi-4, a 14-billion parameter language model developed with a\ntraining recipe that is centrally focused on data quality. Unlike most language\nmodels, where pre-training is based primarily on organic data sources such as\nweb content or code, phi-4 strategically incorporates synthetic data throughout\nthe training process. While previous models in the Phi family largely distill\nthe capabilities of a teacher model (specifically GPT-4), phi-4 substantially\nsurpasses its teacher model on STEM-focused QA capabilities, giving evidence\nthat our data-generation and post-training techniques go beyond distillation.\nDespite minimal changes to the phi-3 architecture, phi-4 achieves strong\nperformance relative to its size -- especially on reasoning-focused benchmarks\n-- due to improved data, training curriculum, and innovations in the\npost-training scheme.",
            "We introduce phi-3-mini, a 3.8 billion parameter language model trained on\n3.3 trillion tokens, whose overall performance, as measured by both academic\nbenchmarks and internal testing, rivals that of models such as Mixtral 8x7B and\nGPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite\nbeing small enough to be deployed on a phone. Our training dataset is a\nscaled-up version of the one used for phi-2, composed of heavily filtered\npublicly available web data and synthetic data. The model is also further\naligned for robustness, safety, and chat format. We also provide\nparameter-scaling results with a 7B, 14B models trained for 4.8T tokens, called\nphi-3-small, phi-3-medium, both significantly more capable than phi-3-mini\n(e.g., respectively 75%, 78% on MMLU, and 8.7, 8.9 on MT-bench). To enhance\nmultilingual, multimodal, and long-context capabilities, we introduce three\nmodels in the phi-3.5 series: phi-3.5-mini, phi-3.5-MoE, and phi-3.5-Vision.\nThe phi-3.5-MoE, a 16 x 3.8B MoE model with 6.6 billion active parameters,\nachieves superior performance in language reasoning, math, and code tasks\ncompared to other open-source models of similar scale, such as Llama 3.1 and\nthe Mixtral series, and on par with Gemini-1.5-Flash and GPT-4o-mini.\nMeanwhile, phi-3.5-Vision, a 4.2 billion parameter model derived from\nphi-3.5-mini, excels in reasoning tasks and is adept at handling both\nsingle-image and text prompts, as well as multi-image and text prompts.",
            "We continue the investigation into the power of smaller Transformer-based\nlanguage models as initiated by \\textbf{TinyStories} -- a 10 million parameter\nmodel that can produce coherent English -- and the follow-up work on\n\\textbf{phi-1}, a 1.3 billion parameter model with Python coding performance\nclose to the state-of-the-art. The latter work proposed to use existing Large\nLanguage Models (LLMs) to generate ``textbook quality\" data as a way to enhance\nthe learning process compared to traditional web data. We follow the\n``Textbooks Are All You Need\" approach, focusing this time on common sense\nreasoning in natural language, and create a new 1.3 billion parameter model\nnamed \\textbf{phi-1.5}, with performance on natural language tasks comparable\nto models 5x larger, and surpassing most non-frontier LLMs on more complex\nreasoning tasks such as grade-school mathematics and basic coding. More\ngenerally, \\textbf{phi-1.5} exhibits many of the traits of much larger LLMs,\nboth good -- such as the ability to ``think step by step\" or perform some\nrudimentary in-context learning -- and bad, including hallucinations and the\npotential for toxic and biased generations -- encouragingly though, we are\nseeing improvement on that front thanks to the absence of web data. We\nopen-source \\textbf{phi-1.5} to promote further research on these urgent\ntopics."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2501.11419",
        "description": "加密货币-路径规划",
        "urls": [
            "https://arxiv.org/abs/2107.05322",
            "https://arxiv.org/abs/1709.05748",
            "https://arxiv.org/abs/1909.06890"
        ],
        "citations": [
            12,
            274,
            51
        ],
        "category": [
            "cs.NI",
            "cs.CR",
            "cs.DC",
            "cs.GT",
            "cs.MA"
        ],
        "titles": [
            "Optimally Reliable & Cheap Payment Flows on the Lightning Network",
            "Settling Payments Fast and Private: Efficient Decentralized Routing for Path-Based Transactions",
            "Hijacking Routes in Payment Channel Networks: A Predictability Tradeoff"
        ],
        "id": 12,
        "abstracts": [
            "Today, payment paths in Bitcoin's Lightning Network are found by searching\nfor shortest paths on the fee graph. We enhance this approach in two\ndimensions. Firstly, we take into account the probability of a payment actually\nbeing possible due to the unknown balance distributions in the channels.\nSecondly, we use minimum cost flows as a proper generalization of shortest\npaths to multi-part payments (MPP). In particular we show that under plausible\nassumptions about the balance distributions we can find the most likely MPP for\nany given set of senders, recipients and amounts by solving for a (generalized)\ninteger minimum cost flow with a separable and convex cost function. Polynomial\ntime exact algorithms as well as approximations are known for this optimization\nproblem. We present a round-based algorithm of min-cost flow computations for\ndelivering large payment amounts over the Lightning Network. This algorithm\nworks by updating the probability distributions with the information gained\nfrom both successful and unsuccessful paths on prior rounds. In all our\nexperiments a single digit number of rounds sufficed to deliver payments of\nsizes that were close to the total local balance of the sender. Early\nexperiments indicate that our approach increases the size of payments that can\nbe reliably delivered by several orders of magnitude compared to the current\nstate of the art. We observe that finding the cheapest multi-part payments is\nan NP-hard problem considering the current fee structure and propose dropping\nthe base fee to make it a linear min-cost flow problem. Finally, we discuss\npossibilities for maximizing the probability while at the same time minimizing\nthe fees of a flow. While this turns out to be a hard problem in general as\nwell - even in the single path case - it appears to be surprisingly tractable\nin practice.",
            "Path-based transaction (PBT) networks, which settle payments from one user to\nanother via a path of intermediaries, are a growing area of research. They\novercome the scalability and privacy issues in cryptocurrencies like Bitcoin\nand Ethereum by replacing expensive and slow on-chain blockchain operations\nwith inexpensive and fast off-chain transfers. In the form of credit networks\nsuch as Ripple and Stellar, they also enable low-price real-time gross\nsettlements across different currencies. For example, SilentWhsipers is a\nrecently proposed fully distributed credit network relying on path-based\ntransactions for secure and in particular private payments without a public\nledger. At the core of a decentralized PBT network is a routing algorithm that\ndiscovers transaction paths between payer and payee. During the last year, a\nnumber of routing algorithms have been proposed. However, the existing ad hoc\nefforts lack either efficiency or privacy. In this work, we first identify\nseveral efficiency concerns in SilentWhsipers. Armed with this knowledge, we\ndesign and evaluate SpeedyMurmurs, a novel routing algorithm for decentralized\nPBT networks using efficient and flexible embedding-based path discovery and\non-demand efficient stabilization to handle the dynamics of a PBT network. Our\nsimulation study, based on real-world data from the currently deployed Ripple\ncredit network, indicates that SpeedyMurmurs reduces the overhead of\nstabilization by up to two orders of magnitude and the overhead of routing a\ntransaction by more than a factor of two. Furthermore, using SpeedyMurmurs\nmaintains at least the same success ratio as decentralized landmark routing,\nwhile providing lower delays. Finally, SpeedyMurmurs achieves key privacy goals\nfor routing in PBT networks.",
            "Off-chain transaction networks can mitigate the scalability issues of today's\ntrustless electronic cash systems such as Bitcoin. However, these peer-to-peer\nnetworks also introduce a new attack surface which is not well-understood\ntoday. This paper identifies and analyzes, a novel Denial-of-Service attack\nwhich is based on route hijacking, i.e., which exploits the way transactions\nare routed and executed along the created channels of the network. This attack\nis conceptually interesting as even a limited attacker that manipulates the\ntopology through the creation of new channels can navigate tradeoffs related to\nthe way it attacks the network. Furthermore, the attack also highlights a\nfundamental design tradeoff for the defender (who determines its own routes):\nto become less predictable and hence secure, a rational node has to pay higher\nfees to nodes that forward its payments. We find that the three most common\nimplementations for payment channels in Bitcoin (lnd, C-lightning, Eclair)\napproach routing differently. We begin by surveying the current state of the\nLightning network and explore the routes chosen by these implementations. We\nfind that in the current network nearly 60\\% of all routes pass through only\nfive nodes, while 80\\% go through only 10 nodes. Thus, a relatively small\nnumber of colluding nodes can deny service to a large fraction of the network.\n  We then turn to study an external attacker who creates links to the network\nand draws more routes through its nodes by asking for lower fees. We find that\njust five new links are enough to draw the majority (65\\% - 75\\%) of the\ntraffic regardless of the implementation being used. The cost of creating these\nlinks is very low.\n  We discuss the differences between implementations and eventually derive our\nown suggested routing policy, which is based on a novel combination of existing\napproaches."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2412.13178",
        "description": "Robotics",
        "urls": [
            "https://arxiv.org/abs/2412.13178",
            "https://arxiv.org/abs/2402.08178",
            "https://arxiv.org/abs/2403.18760"
        ],
        "citations": [
            0,
            8,
            7
        ],
        "category": [
            "cs.AI",
            "cs.RO",
            "cs.CR"
        ],
        "titles": [
            "SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents",
            "LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents",
            "MLDT: Multi-Level Decomposition for Complex Long-Horizon Robotic Task Planning with Open-Source Large Language Model"
        ],
        "id": 13,
        "abstracts": [
            "With the integration of large language models (LLMs), embodied agents have\nstrong capabilities to execute complicated instructions in natural language,\npaving a way for the potential deployment of embodied robots. However, a\nforeseeable issue is that those embodied agents can also flawlessly execute\nsome hazardous tasks, potentially causing damages in real world. To study this\nissue, we present SafeAgentBench -- a new benchmark for safety-aware task\nplanning of embodied LLM agents. SafeAgentBench includes: (1) a new dataset\nwith 750 tasks, covering 10 potential hazards and 3 task types; (2)\nSafeAgentEnv, a universal embodied environment with a low-level controller,\nsupporting multi-agent execution with 17 high-level actions for 8\nstate-of-the-art baselines; and (3) reliable evaluation methods from both\nexecution and semantic perspectives. Experimental results show that the\nbest-performing baseline gets 69% success rate for safe tasks, but only 5%\nrejection rate for hazardous tasks, indicating significant safety risks. More\ndetails and codes are available at\nhttps://github.com/shengyin1224/SafeAgentBench.",
            "Large language models (LLMs) have recently received considerable attention as\nalternative solutions for task planning. However, comparing the performance of\nlanguage-oriented task planners becomes difficult, and there exists a dearth of\ndetailed exploration regarding the effects of various factors such as\npre-trained model selection and prompt construction. To address this, we\npropose a benchmark system for automatically quantifying performance of task\nplanning for home-service embodied agents. Task planners are tested on two\npairs of datasets and simulators: 1) ALFRED and AI2-THOR, 2) an extension of\nWatch-And-Help and VirtualHome. Using the proposed benchmark system, we perform\nextensive experiments with LLMs and prompts, and explore several enhancements\nof the baseline planner. We expect that the proposed benchmark tool would\naccelerate the development of language-oriented task planners.",
            "In the realm of data-driven AI technology, the application of open-source\nlarge language models (LLMs) in robotic task planning represents a significant\nmilestone. Recent robotic task planning methods based on open-source LLMs\ntypically leverage vast task planning datasets to enhance models' planning\nabilities. While these methods show promise, they struggle with complex\nlong-horizon tasks, which require comprehending more context and generating\nlonger action sequences. This paper addresses this limitation by proposing\nMLDT, theMulti-Level Decomposition Task planning method. This method\ninnovatively decomposes tasks at the goal-level, task-level, and action-level\nto mitigate the challenge of complex long-horizon tasks. In order to enhance\nopen-source LLMs' planning abilities, we introduce a goal-sensitive corpus\ngeneration method to create high-quality training data and conduct instruction\ntuning on the generated corpus. Since the complexity of the existing datasets\nis not high enough, we construct a more challenging dataset, LongTasks, to\nspecifically evaluate planning ability on complex long-horizon tasks. We\nevaluate our method using various LLMs on four datasets in VirtualHome. Our\nresults demonstrate a significant performance enhancement in robotic task\nplanning, showcasing MLDT's effectiveness in overcoming the limitations of\nexisting methods based on open-source LLMs as well as its practicality in\ncomplex, real-world scenarios."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2412.15922",
        "description": "AUDIO",
        "urls": [
            "https://arxiv.org/abs/2412.15922",
            "https://arxiv.org/abs/2404.09956",
            "https://arxiv.org/abs/2301.12503"
        ],
        "citations": [
            0,
            32,
            525
        ],
        "category": [
            "cs.SD",
            "eess.AS",
            "cs.AI",
            "cs.LG",
            "cs.CL",
            "cs.MM",
            "eess.SP"
        ],
        "titles": [
            "RiTTA: Modeling Event Relations in Text-to-Audio Generation",
            "Tango 2: Aligning Diffusion-based Text-to-Audio Generations through Direct Preference Optimization",
            "AudioLDM: Text-to-Audio Generation with Latent Diffusion Models"
        ],
        "id": 14,
        "abstracts": [
            "Despite significant advancements in Text-to-Audio (TTA) generation models\nachieving high-fidelity audio with fine-grained context understanding, they\nstruggle to model the relations between audio events described in the input\ntext. However, previous TTA methods have not systematically explored audio\nevent relation modeling, nor have they proposed frameworks to enhance this\ncapability. In this work, we systematically study audio event relation modeling\nin TTA generation models. We first establish a benchmark for this task by: 1.\nproposing a comprehensive relation corpus covering all potential relations in\nreal-world scenarios; 2. introducing a new audio event corpus encompassing\ncommonly heard audios; and 3. proposing new evaluation metrics to assess audio\nevent relation modeling from various perspectives. Furthermore, we propose a\nfinetuning framework to enhance existing TTA models ability to model audio\nevents relation. Code is available at: https://github.com/yuhanghe01/RiTTA",
            "Generative multimodal content is increasingly prevalent in much of the\ncontent creation arena, as it has the potential to allow artists and media\npersonnel to create pre-production mockups by quickly bringing their ideas to\nlife. The generation of audio from text prompts is an important aspect of such\nprocesses in the music and film industry. Many of the recent diffusion-based\ntext-to-audio models focus on training increasingly sophisticated diffusion\nmodels on a large set of datasets of prompt-audio pairs. These models do not\nexplicitly focus on the presence of concepts or events and their temporal\nordering in the output audio with respect to the input prompt. Our hypothesis\nis focusing on how these aspects of audio generation could improve audio\ngeneration performance in the presence of limited data. As such, in this work,\nusing an existing text-to-audio model Tango, we synthetically create a\npreference dataset where each prompt has a winner audio output and some loser\naudio outputs for the diffusion model to learn from. The loser outputs, in\ntheory, have some concepts from the prompt missing or in an incorrect order. We\nfine-tune the publicly available Tango text-to-audio model using diffusion-DPO\n(direct preference optimization) loss on our preference dataset and show that\nit leads to improved audio output over Tango and AudioLDM2, in terms of both\nautomatic- and manual-evaluation metrics.",
            "Text-to-audio (TTA) system has recently gained attention for its ability to\nsynthesize general audio based on text descriptions. However, previous studies\nin TTA have limited generation quality with high computational costs. In this\nstudy, we propose AudioLDM, a TTA system that is built on a latent space to\nlearn the continuous audio representations from contrastive language-audio\npretraining (CLAP) latents. The pretrained CLAP models enable us to train LDMs\nwith audio embedding while providing text embedding as a condition during\nsampling. By learning the latent representations of audio signals and their\ncompositions without modeling the cross-modal relationship, AudioLDM is\nadvantageous in both generation quality and computational efficiency. Trained\non AudioCaps with a single GPU, AudioLDM achieves state-of-the-art TTA\nperformance measured by both objective and subjective metrics (e.g., frechet\ndistance). Moreover, AudioLDM is the first TTA system that enables various\ntext-guided audio manipulations (e.g., style transfer) in a zero-shot fashion.\nOur implementation and demos are available at https://audioldm.github.io."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2306.17799",
        "description": "Emotion recognition",
        "urls": [
            "https://arxiv.org/abs/2306.17799",
            "https://arxiv.org/abs/2203.13504",
            "https://arxiv.org/abs/2203.02385"
        ],
        "citations": [
            51,
            103,
            139
        ],
        "category": [
            "cs.SD",
            "eess.AS",
            "cs.CL",
            "cs.CV",
            "cs.AI",
            "cs.MM"
        ],
        "titles": [
            "A Low-rank Matching Attention based Cross-modal Feature Fusion Method for Conversational Emotion Recognition",
            "EmoCaps: Emotion Capsule based Model for Conversational Emotion Recognition",
            "MM-DFN: Multimodal Dynamic Fusion Network for Emotion Recognition in Conversations"
        ],
        "id": 15,
        "abstracts": [
            "Conversational emotion recognition (CER) is an important research topic in\nhuman-computer interactions. {Although recent advancements in transformer-based\ncross-modal fusion methods have shown promise in CER tasks, they tend to\noverlook the crucial intra-modal and inter-modal emotional interaction or\nsuffer from high computational complexity. To address this, we introduce a\nnovel and lightweight cross-modal feature fusion method called Low-Rank\nMatching Attention Method (LMAM). LMAM effectively captures contextual\nemotional semantic information in conversations while mitigating the quadratic\ncomplexity issue caused by the self-attention mechanism. Specifically, by\nsetting a matching weight and calculating inter-modal features attention scores\nrow by row, LMAM requires only one-third of the parameters of self-attention\nmethods. We also employ the low-rank decomposition method on the weights to\nfurther reduce the number of parameters in LMAM. As a result, LMAM offers a\nlightweight model while avoiding overfitting problems caused by a large number\nof parameters. Moreover, LMAM is able to fully exploit the intra-modal\nemotional contextual information within each modality and integrates\ncomplementary emotional semantic information across modalities by computing and\nfusing similarities of intra-modal and inter-modal features simultaneously.\nExperimental results verify the superiority of LMAM compared with other popular\ncross-modal fusion methods on the premise of being more lightweight. Also, LMAM\ncan be embedded into any existing state-of-the-art CER methods in a\nplug-and-play manner, and can be applied to other multi-modal recognition\ntasks, e.g., session recommendation and humour detection, demonstrating its\nremarkable generalization ability.",
            "Emotion recognition in conversation (ERC) aims to analyze the speaker's state\nand identify their emotion in the conversation. Recent works in ERC focus on\ncontext modeling but ignore the representation of contextual emotional\ntendency. In order to extract multi-modal information and the emotional\ntendency of the utterance effectively, we propose a new structure named\nEmoformer to extract multi-modal emotion vectors from different modalities and\nfuse them with sentence vector to be an emotion capsule. Furthermore, we design\nan end-to-end ERC model called EmoCaps, which extracts emotion vectors through\nthe Emoformer structure and obtain the emotion classification results from a\ncontext analysis model. Through the experiments with two benchmark datasets,\nour model shows better performance than the existing state-of-the-art models.",
            "Emotion Recognition in Conversations (ERC) has considerable prospects for\ndeveloping empathetic machines. For multimodal ERC, it is vital to understand\ncontext and fuse modality information in conversations. Recent graph-based\nfusion methods generally aggregate multimodal information by exploring unimodal\nand cross-modal interactions in a graph. However, they accumulate redundant\ninformation at each layer, limiting the context understanding between\nmodalities. In this paper, we propose a novel Multimodal Dynamic Fusion Network\n(MM-DFN) to recognize emotions by fully understanding multimodal conversational\ncontext. Specifically, we design a new graph-based dynamic fusion module to\nfuse multimodal contextual features in a conversation. The module reduces\nredundancy and enhances complementarity between modalities by capturing the\ndynamics of contextual information in different semantic spaces. Extensive\nexperiments on two public benchmark datasets demonstrate the effectiveness and\nsuperiority of MM-DFN."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2312.02037",
        "description": "Database",
        "urls": [
            "https://arxiv.org/abs/2312.02037",
            "https://arxiv.org/abs/1703.06103",
            "https://arxiv.org/abs/2003.01332"
        ],
        "citations": [
            7,
            6125,
            1430
        ],
        "category": [
            "cs.LG",
            "cs.DB",
            "stat.ML",
            "cs.AI",
            "cs.SI"
        ],
        "titles": [
            "GFS: Graph-based Feature Synthesis for Prediction over Relational Databases",
            "Modeling Relational Data with Graph Convolutional Networks",
            "Heterogeneous Graph Transformer"
        ],
        "id": 16,
        "abstracts": [
            "Relational databases are extensively utilized in a variety of modern\ninformation system applications, and they always carry valuable data patterns.\nThere are a huge number of data mining or machine learning tasks conducted on\nrelational databases. However, it is worth noting that there are limited\nmachine learning models specifically designed for relational databases, as most\nmodels are primarily tailored for single table settings. Consequently, the\nprevalent approach for training machine learning models on data stored in\nrelational databases involves performing feature engineering to merge the data\nfrom multiple tables into a single table and subsequently applying single table\nmodels. This approach not only requires significant effort in feature\nengineering but also destroys the inherent relational structure present in the\ndata. To address these challenges, we propose a novel framework called\nGraph-based Feature Synthesis (GFS). GFS formulates the relational database as\na heterogeneous graph, thereby preserving the relational structure within the\ndata. By leveraging the inductive bias from single table models, GFS\neffectively captures the intricate relationships inherent in each table.\nAdditionally, the whole framework eliminates the need for manual feature\nengineering. In the extensive experiment over four real-world multi-table\nrelational databases, GFS outperforms previous methods designed for relational\ndatabases, demonstrating its superior performance.",
            "Knowledge graphs enable a wide variety of applications, including question\nanswering and information retrieval. Despite the great effort invested in their\ncreation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata)\nremain incomplete. We introduce Relational Graph Convolutional Networks\n(R-GCNs) and apply them to two standard knowledge base completion tasks: Link\nprediction (recovery of missing facts, i.e. subject-predicate-object triples)\nand entity classification (recovery of missing entity attributes). R-GCNs are\nrelated to a recent class of neural networks operating on graphs, and are\ndeveloped specifically to deal with the highly multi-relational data\ncharacteristic of realistic knowledge bases. We demonstrate the effectiveness\nof R-GCNs as a stand-alone model for entity classification. We further show\nthat factorization models for link prediction such as DistMult can be\nsignificantly improved by enriching them with an encoder model to accumulate\nevidence over multiple inference steps in the relational graph, demonstrating a\nlarge improvement of 29.8% on FB15k-237 over a decoder-only baseline.",
            "Recent years have witnessed the emerging success of graph neural networks\n(GNNs) for modeling structured data. However, most GNNs are designed for\nhomogeneous graphs, in which all nodes and edges belong to the same types,\nmaking them infeasible to represent heterogeneous structures. In this paper, we\npresent the Heterogeneous Graph Transformer (HGT) architecture for modeling\nWeb-scale heterogeneous graphs. To model heterogeneity, we design node- and\nedge-type dependent parameters to characterize the heterogeneous attention over\neach edge, empowering HGT to maintain dedicated representations for different\ntypes of nodes and edges. To handle dynamic heterogeneous graphs, we introduce\nthe relative temporal encoding technique into HGT, which is able to capture the\ndynamic structural dependency with arbitrary durations. To handle Web-scale\ngraph data, we design the heterogeneous mini-batch graph sampling\nalgorithm---HGSampling---for efficient and scalable training. Extensive\nexperiments on the Open Academic Graph of 179 million nodes and 2 billion edges\nshow that the proposed HGT model consistently outperforms all the\nstate-of-the-art GNN baselines by 9%--21% on various downstream tasks."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2409.02969",
        "description": "Optimizer",
        "urls": [
            "https://arxiv.org/abs/2409.02969",
            "https://arxiv.org/abs/2102.04523",
            "https://arxiv.org/abs/1912.12854"
        ],
        "citations": [
            2,
            19,
            372
        ],
        "category": [
            "cs.LG",
            "cs.MS",
            "math.OC",
            "stat.ML"
        ],
        "titles": [
            "LibMOON: A Gradient-based MultiObjective OptimizatioN Library in PyTorch",
            "Multi-Objective Learning to Predict Pareto Fronts Using Hypervolume Maximization",
            "Pareto Multi-Task Learning"
        ],
        "id": 17,
        "abstracts": [
            "Multiobjective optimization problems (MOPs) are prevalent in machine\nlearning, with applications in multi-task learning, learning under fairness or\nrobustness constraints, etc. Instead of reducing multiple objective functions\ninto a scalar objective, MOPs aim to optimize for the so-called Pareto\noptimality or Pareto set learning, which involves optimizing more than one\nobjective function simultaneously, over models with thousands / millions of\nparameters. Existing benchmark libraries for MOPs mainly focus on evolutionary\nalgorithms, most of which are zeroth-order / meta-heuristic methods that do not\neffectively utilize higher-order information from objectives and cannot scale\nto large-scale models with thousands / millions of parameters. In light of the\nabove gap, this paper introduces LibMOON, the first multiobjective optimization\nlibrary that supports state-of-the-art gradient-based methods, provides a fair\nbenchmark, and is open-sourced for the community.",
            "Real-world problems are often multi-objective with decision-makers unable to\nspecify a priori which trade-off between the conflicting objectives is\npreferable. Intuitively, building machine learning solutions in such cases\nwould entail providing multiple predictions that span and uniformly cover the\nPareto front of all optimal trade-off solutions. We propose a novel approach\nfor multi-objective training of neural networks to approximate the Pareto front\nduring inference. In our approach, the neural networks are trained\nmulti-objectively using a dynamic loss function, wherein each network's losses\n(corresponding to multiple objectives) are weighted by their hypervolume\nmaximizing gradients. We discuss and illustrate why training processes to\napproximate Pareto fronts need to optimize on fronts of individual training\nsamples instead of on only the front of average losses. Experiments on three\nmulti-objective problems show that our approach returns outputs that are\nwell-spread across different trade-offs on the approximated Pareto front\nwithout requiring the trade-off vectors to be specified a priori. Further,\nresults of comparisons with the state-of-the-art approaches highlight the added\nvalue of our proposed approach, especially in asymmetric Pareto fronts.",
            "Multi-task learning is a powerful method for solving multiple correlated\ntasks simultaneously. However, it is often impossible to find one single\nsolution to optimize all the tasks, since different tasks might conflict with\neach other. Recently, a novel method is proposed to find one single Pareto\noptimal solution with good trade-off among different tasks by casting\nmulti-task learning as multiobjective optimization. In this paper, we\ngeneralize this idea and propose a novel Pareto multi-task learning algorithm\n(Pareto MTL) to find a set of well-distributed Pareto solutions which can\nrepresent different trade-offs among different tasks. The proposed algorithm\nfirst formulates a multi-task learning problem as a multiobjective optimization\nproblem, and then decomposes the multiobjective optimization problem into a set\nof constrained subproblems with different trade-off preferences. By solving\nthese subproblems in parallel, Pareto MTL can find a set of well-representative\nPareto optimal solutions with different trade-off among all tasks.\nPractitioners can easily select their preferred solution from these Pareto\nsolutions, or use different trade-off solutions for different situations.\nExperimental results confirm that the proposed algorithm can generate\nwell-representative solutions and outperform some state-of-the-art algorithms\non many multi-task learning applications."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2412.15204",
        "description": "LongBenchv2 L-Eval infiniteBench ",
        "urls": [
            "https://arxiv.org/abs/2412.15204",
            "https://arxiv.org/abs/2307.11088",
            "https://arxiv.org/abs/2402.13718"
        ],
        "citations": [
            2,
            23,
            0
        ],
        "category": [
            "cs.CL",
            "cs.AI"
        ],
        "titles": [
            "LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks",
            "L-Eval: Instituting Standardized Evaluation for Long Context Language Models",
            "$\\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens"
        ],
        "id": 18,
        "abstracts": [
            "This paper introduces LongBench v2, a benchmark designed to assess the\nability of LLMs to handle long-context problems requiring deep understanding\nand reasoning across real-world multitasks. LongBench v2 consists of 503\nchallenging multiple-choice questions, with contexts ranging from 8k to 2M\nwords, across six major task categories: single-document QA, multi-document QA,\nlong in-context learning, long-dialogue history understanding, code repository\nunderstanding, and long structured data understanding. To ensure the breadth\nand the practicality, we collect data from nearly 100 highly educated\nindividuals with diverse professional backgrounds. We employ both automated and\nmanual review processes to maintain high quality and difficulty, resulting in\nhuman experts achieving only 53.7% accuracy under a 15-minute time constraint.\nOur evaluation reveals that the best-performing model, when directly answers\nthe questions, achieves only 50.1% accuracy. In contrast, the o1-preview model,\nwhich includes longer reasoning, achieves 57.7%, surpassing the human baseline\nby 4%. These results highlight the importance of enhanced reasoning ability and\nscaling inference-time compute to tackle the long-context challenges in\nLongBench v2. The project is available at https://longbench2.github.io.",
            "Recently, there has been growing interest in extending the context length of\nlarge language models (LLMs), aiming to effectively process long inputs of one\nturn or conversations with more extensive histories. While proprietary models\nsuch as GPT-4 and Claude can largely preserve the reasoning ability in an\nextended context, open-source models are still progressing through the early\nstages of development. To bridge this gap, we propose L-Eval to institute a\nmore standardized evaluation for long context language models (LCLMs)\naddressing two key aspects: dataset construction and evaluation metrics. On the\none hand, we build a new evaluation suite containing 20 sub-tasks, 508 long\ndocuments, and over 2,000 human-labeled query-response pairs encompassing\ndiverse question styles, domains, and input length (3k$\\sim$200k tokens). On\nthe other hand, we investigate the effectiveness in evalution metrics for\nLCLMs. Results show that popular n-gram matching metrics generally can not\ncorrelate well with human judgment, and thus we strongly advocate for\nlength-instruction-enhanced (LIE) evaluation and employing LLM judges. We\nconducted a comprehensive study of 4 popular commercial LLMs and 12 open-source\ncounterparts using the L-Eval benchmark. Our empirical findings offer useful\ninsights into the study of LCLMs and lay the groundwork for the development of\nmore principled evaluation of these models.",
            "Processing and reasoning over long contexts is crucial for many practical\napplications of Large Language Models (LLMs), such as document comprehension\nand agent construction. Despite recent strides in making LLMs process contexts\nwith more than 100K tokens, there is currently a lack of a standardized\nbenchmark to evaluate this long-context capability. Existing public benchmarks\ntypically focus on contexts around 10K tokens, limiting the assessment and\ncomparison of LLMs in processing longer contexts. In this paper, we propose\n$\\infty$Bench, the first LLM benchmark featuring an average data length\nsurpassing 100K tokens. $\\infty$Bench comprises synthetic and realistic tasks\nspanning diverse domains, presented in both English and Chinese. The tasks in\n$\\infty$Bench are designed to require well understanding of long dependencies\nin contexts, and make simply retrieving a limited number of passages from\ncontexts not sufficient for these tasks. In our experiments, based on\n$\\infty$Bench, we evaluate the state-of-the-art proprietary and open-source\nLLMs tailored for processing long contexts. The results indicate that existing\nlong context LLMs still require significant advancements to effectively process\n100K+ context. We further present three intriguing analyses regarding the\nbehavior of LLMs processing long context."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2412.20816",
        "description": "LA-DETR QD-DETR UVCOM",
        "urls": [
            "https://arxiv.org/abs/2412.20816",
            "https://arxiv.org/abs/2401.02309",
            "https://arxiv.org/abs/2311.16464"
        ],
        "citations": [
            0,
            27,
            29
        ],
        "category": [
            "cs.CV",
            "cs.AI",
            "cs.MM"
        ],
        "titles": [
            "Length-Aware DETR for Robust Moment Retrieval",
            "TR-DETR: Task-Reciprocal Transformer for Joint Moment Retrieval and Highlight Detection",
            "Bridging the Gap: A Unified Video Comprehension Framework for Moment Retrieval and Highlight Detection"
        ],
        "id": 19,
        "abstracts": [
            "Video Moment Retrieval (MR) aims to localize moments within a video based on\na given natural language query. Given the prevalent use of platforms like\nYouTube for information retrieval, the demand for MR techniques is\nsignificantly growing. Recent DETR-based models have made notable advances in\nperformance but still struggle with accurately localizing short moments.\nThrough data analysis, we identified limited feature diversity in short\nmoments, which motivated the development of MomentMix. MomentMix employs two\naugmentation strategies: ForegroundMix and BackgroundMix, each enhancing the\nfeature representations of the foreground and background, respectively.\nAdditionally, our analysis of prediction bias revealed that short moments\nparticularly struggle with accurately predicting their center positions of\nmoments. To address this, we propose a Length-Aware Decoder, which conditions\nlength through a novel bipartite matching process. Our extensive studies\ndemonstrate the efficacy of our length-aware approach, especially in localizing\nshort moments, leading to improved overall performance. Our method surpasses\nstate-of-the-art DETR-based methods on benchmark datasets, achieving the\nhighest R1 and mAP on QVHighlights and the highest R1@0.7 on TACoS and\nCharades-STA (such as a 2.46% gain in R1@0.7 and a 2.57% gain in mAP average\nfor QVHighlights). The code is available at\nhttps://github.com/sjpark5800/LA-DETR.",
            "Video moment retrieval (MR) and highlight detection (HD) based on natural\nlanguage queries are two highly related tasks, which aim to obtain relevant\nmoments within videos and highlight scores of each video clip. Recently,\nseveral methods have been devoted to building DETR-based networks to solve both\nMR and HD jointly. These methods simply add two separate task heads after\nmulti-modal feature extraction and feature interaction, achieving good\nperformance. Nevertheless, these approaches underutilize the reciprocal\nrelationship between two tasks. In this paper, we propose a task-reciprocal\ntransformer based on DETR (TR-DETR) that focuses on exploring the inherent\nreciprocity between MR and HD. Specifically, a local-global multi-modal\nalignment module is first built to align features from diverse modalities into\na shared latent space. Subsequently, a visual feature refinement is designed to\neliminate query-irrelevant information from visual features for modal\ninteraction. Finally, a task cooperation module is constructed to refine the\nretrieval pipeline and the highlight score prediction process by utilizing the\nreciprocity between MR and HD. Comprehensive experiments on QVHighlights,\nCharades-STA and TVSum datasets demonstrate that TR-DETR outperforms existing\nstate-of-the-art methods. Codes are available at\n\\url{https://github.com/mingyao1120/TR-DETR}.",
            "Video Moment Retrieval (MR) and Highlight Detection (HD) have attracted\nsignificant attention due to the growing demand for video analysis. Recent\napproaches treat MR and HD as similar video grounding problems and address them\ntogether with transformer-based architecture. However, we observe that the\nemphasis of MR and HD differs, with one necessitating the perception of local\nrelationships and the other prioritizing the understanding of global contexts.\nConsequently, the lack of task-specific design will inevitably lead to\nlimitations in associating the intrinsic specialty of two tasks. To tackle the\nissue, we propose a Unified Video COMprehension framework (UVCOM) to bridge the\ngap and jointly solve MR and HD effectively. By performing progressive\nintegration on intra and inter-modality across multi-granularity, UVCOM\nachieves the comprehensive understanding in processing a video. Moreover, we\npresent multi-aspect contrastive learning to consolidate the local relation\nmodeling and global knowledge accumulation via well aligned multi-modal space.\nExtensive experiments on QVHighlights, Charades-STA, TACoS , YouTube Highlights\nand TVSum datasets demonstrate the effectiveness and rationality of UVCOM which\noutperforms the state-of-the-art methods by a remarkable margin."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2412.20014",
        "description": "Multimodal Protein",
        "urls": [
            "https://arxiv.org/abs/2412.20014",
            "https://arxiv.org/abs/2301.12040",
            "https://arxiv.org/abs/2201.11147"
        ],
        "citations": [
            1,
            98,
            101
        ],
        "category": [
            "cs.LG",
            "q-bio.BM",
            "cs.AI",
            "cs.CL",
            "cs.IR"
        ],
        "titles": [
            "ProtCLIP: Function-Informed Protein Multi-Modal Learning",
            "ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts",
            "OntoProtein: Protein Pretraining With Gene Ontology Embedding"
        ],
        "id": 20,
        "abstracts": [
            "Multi-modality pre-training paradigm that aligns protein sequences and\nbiological descriptions has learned general protein representations and\nachieved promising performance in various downstream applications. However,\nthese works were still unable to replicate the extraordinary success of\nlanguage-supervised visual foundation models due to the ineffective usage of\naligned protein-text paired data and the lack of an effective function-informed\npre-training paradigm. To address these issues, this paper curates a\nlarge-scale protein-text paired dataset called ProtAnno with a property-driven\nsampling strategy, and introduces a novel function-informed protein\npre-training paradigm. Specifically, the sampling strategy determines selecting\nprobability based on the sample confidence and property coverage, balancing the\ndata quality and data quantity in face of large-scale noisy data. Furthermore,\nmotivated by significance of the protein specific functional mechanism, the\nproposed paradigm explicitly model protein static and dynamic functional\nsegments by two segment-wise pre-training objectives, injecting fine-grained\ninformation in a function-informed manner. Leveraging all these innovations, we\ndevelop ProtCLIP, a multi-modality foundation model that comprehensively\nrepresents function-aware protein embeddings. On 22 different protein\nbenchmarks within 5 types, including protein functionality classification,\nmutation effect prediction, cross-modal transformation, semantic similarity\ninference and protein-protein interaction prediction, our ProtCLIP consistently\nachieves SOTA performance, with remarkable improvements of 75% on average in\nfive cross-modal transformation benchmarks, 59.9% in GO-CC and 39.7% in GO-BP\nprotein function prediction. The experimental results verify the extraordinary\npotential of ProtCLIP serving as the protein multi-modality foundation model.",
            "Current protein language models (PLMs) learn protein representations mainly\nbased on their sequences, thereby well capturing co-evolutionary information,\nbut they are unable to explicitly acquire protein functions, which is the end\ngoal of protein representation learning. Fortunately, for many proteins, their\ntextual property descriptions are available, where their various functions are\nalso described. Motivated by this fact, we first build the ProtDescribe dataset\nto augment protein sequences with text descriptions of their functions and\nother important properties. Based on this dataset, we propose the ProtST\nframework to enhance Protein Sequence pre-training and understanding by\nbiomedical Texts. During pre-training, we design three types of tasks, i.e.,\nunimodal mask prediction, multimodal representation alignment and multimodal\nmask prediction, to enhance a PLM with protein property information with\ndifferent granularities and, at the same time, preserve the PLM's original\nrepresentation power. On downstream tasks, ProtST enables both supervised\nlearning and zero-shot prediction. We verify the superiority of ProtST-induced\nPLMs over previous ones on diverse representation learning benchmarks. Under\nthe zero-shot setting, we show the effectiveness of ProtST on zero-shot protein\nclassification, and ProtST also enables functional protein retrieval from a\nlarge-scale database without any function annotation.",
            "Self-supervised protein language models have proved their effectiveness in\nlearning the proteins representations. With the increasing computational power,\ncurrent protein language models pre-trained with millions of diverse sequences\ncan advance the parameter scale from million-level to billion-level and achieve\nremarkable improvement. However, those prevailing approaches rarely consider\nincorporating knowledge graphs (KGs), which can provide rich structured\nknowledge facts for better protein representations. We argue that informative\nbiology knowledge in KGs can enhance protein representation with external\nknowledge. In this work, we propose OntoProtein, the first general framework\nthat makes use of structure in GO (Gene Ontology) into protein pre-training\nmodels. We construct a novel large-scale knowledge graph that consists of GO\nand its related proteins, and gene annotation texts or protein sequences\ndescribe all nodes in the graph. We propose novel contrastive learning with\nknowledge-aware negative sampling to jointly optimize the knowledge graph and\nprotein embedding during pre-training. Experimental results show that\nOntoProtein can surpass state-of-the-art methods with pre-trained protein\nlanguage models in TAPE benchmark and yield better performance compared with\nbaselines in protein-protein interaction and protein function prediction. Code\nand datasets are available in https://github.com/zjunlp/OntoProtein."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2501.13969",
        "description": "InsTex: Indoor Scenes Stylized Texture Synthesis",
        "urls": [
            "https://arxiv.org/abs/2304.02643",
            "https://arxiv.org/abs/2112.01801",
            "https://arxiv.org/abs/2306.11737"
        ],
        "citations": [
            8121,
            13,
            1
        ],
        "category": [
            "cs.CV",
            "cs.LG",
            "cs.AI",
            "cs.GR"
        ],
        "titles": [
            "Segment Anything",
            "Mesh Convolution with Continuous Filters for 3D Surface Parsing",
            "Neural ShDF: Reviving an Efficient and Consistent Mesh Segmentation Method"
        ],
        "id": 21,
        "abstracts": [
            "We introduce the Segment Anything (SA) project: a new task, model, and\ndataset for image segmentation. Using our efficient model in a data collection\nloop, we built the largest segmentation dataset to date (by far), with over 1\nbillion masks on 11M licensed and privacy respecting images. The model is\ndesigned and trained to be promptable, so it can transfer zero-shot to new\nimage distributions and tasks. We evaluate its capabilities on numerous tasks\nand find that its zero-shot performance is impressive -- often competitive with\nor even superior to prior fully supervised results. We are releasing the\nSegment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and\n11M images at https://segment-anything.com to foster research into foundation\nmodels for computer vision.",
            "Geometric feature learning for 3D surfaces is critical for many applications\nin computer graphics and 3D vision. However, deep learning currently lags in\nhierarchical modeling of 3D surfaces due to the lack of required operations\nand/or their efficient implementations. In this paper, we propose a series of\nmodular operations for effective geometric feature learning from 3D triangle\nmeshes. These operations include novel mesh convolutions, efficient mesh\ndecimation and associated mesh (un)poolings. Our mesh convolutions exploit\nspherical harmonics as orthonormal bases to create continuous convolutional\nfilters. The mesh decimation module is GPU-accelerated and able to process\nbatched meshes on-the-fly, while the (un)pooling operations compute features\nfor up/down-sampled meshes. We provide open-source implementation of these\noperations, collectively termed Picasso. Picasso supports heterogeneous mesh\nbatching and processing. Leveraging its modular operations, we further\ncontribute a novel hierarchical neural network for perceptual parsing of 3D\nsurfaces, named PicassoNet++. It achieves highly competitive performance for\nshape analysis and scene segmentation on prominent 3D benchmarks. The code,\ndata and trained models are available at\nhttps://github.com/EnyaHermite/Picasso.",
            "Partitioning a polygonal mesh into meaningful parts can be challenging. Many\napplications require decomposing such structures for further processing in\ncomputer graphics. In the last decade, several methods were proposed to tackle\nthis problem, at the cost of intensive computational times. Recently, machine\nlearning has proven to be effective for the segmentation task on 3D structures.\nNevertheless, these state-of-the-art methods are often hardly generalizable and\nrequire dividing the learned model into several specific classes of objects to\navoid overfitting. We present a data-driven approach leveraging deep learning\nto encode a mapping function prior to mesh segmentation for multiple\napplications. Our network reproduces a neighborhood map using our knowledge of\nthe \\textsl{Shape Diameter Function} (SDF) method using similarities among\nvertex neighborhoods. Our approach is resolution-agnostic as we downsample the\ninput meshes and query the full-resolution structure solely for neighborhood\ncontributions. Using our predicted SDF values, we can inject the resulting\nstructure into a graph-cut algorithm to generate an efficient and robust mesh\nsegmentation while considerably reducing the required computation times."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2501.16150",
        "description": "Agent Dataset",
        "urls": [
            "https://arxiv.org/abs/2307.13854",
            "https://arxiv.org/abs/2202.02312",
            "https://arxiv.org/abs/2402.17553"
        ],
        "citations": [
            253,
            64,
            3
        ],
        "category": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "cs.HC",
            "cs.LG"
        ],
        "titles": [
            "WebArena: A Realistic Web Environment for Building Autonomous Agents",
            "A Dataset for Interactive Vision-Language Navigation with Unknown Command Feasibility",
            "OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web"
        ],
        "id": 22,
        "abstracts": [
            "With advances in generative AI, there is now potential for autonomous agents\nto manage daily tasks via natural language commands. However, current agents\nare primarily created and tested in simplified synthetic environments, leading\nto a disconnect with real-world scenarios. In this paper, we build an\nenvironment for language-guided agents that is highly realistic and\nreproducible. Specifically, we focus on agents that perform tasks on the web,\nand create an environment with fully functional websites from four common\ndomains: e-commerce, social forum discussions, collaborative software\ndevelopment, and content management. Our environment is enriched with tools\n(e.g., a map) and external knowledge bases (e.g., user manuals) to encourage\nhuman-like task-solving. Building upon our environment, we release a set of\nbenchmark tasks focusing on evaluating the functional correctness of task\ncompletions. The tasks in our benchmark are diverse, long-horizon, and designed\nto emulate tasks that humans routinely perform on the internet. We experiment\nwith several baseline agents, integrating recent techniques such as reasoning\nbefore acting. The results demonstrate that solving complex tasks is\nchallenging: our best GPT-4-based agent only achieves an end-to-end task\nsuccess rate of 14.41%, significantly lower than the human performance of\n78.24%. These results highlight the need for further development of robust\nagents, that current state-of-the-art large language models are far from\nperfect performance in these real-life tasks, and that WebArena can be used to\nmeasure such progress.",
            "Vision-language navigation (VLN), in which an agent follows language\ninstruction in a visual environment, has been studied under the premise that\nthe input command is fully feasible in the environment. Yet in practice, a\nrequest may not be possible due to language ambiguity or environment changes.\nTo study VLN with unknown command feasibility, we introduce a new dataset\nMobile app Tasks with Iterative Feedback (MoTIF), where the goal is to complete\na natural language command in a mobile app. Mobile apps provide a scalable\ndomain to study real downstream uses of VLN methods. Moreover, mobile app\ncommands provide instruction for interactive navigation, as they result in\naction sequences with state changes via clicking, typing, or swiping. MoTIF is\nthe first to include feasibility annotations, containing both binary\nfeasibility labels and fine-grained labels for why tasks are unsatisfiable. We\nfurther collect follow-up questions for ambiguous queries to enable research on\ntask uncertainty resolution. Equipped with our dataset, we propose the new\nproblem of feasibility prediction, in which a natural language instruction and\nmultimodal app environment are used to predict command feasibility. MoTIF\nprovides a more realistic app dataset as it contains many diverse environments,\nhigh-level goals, and longer action sequences than prior work. We evaluate\ninteractive VLN methods using MoTIF, quantify the generalization ability of\ncurrent approaches to new app environments, and measure the effect of task\nfeasibility on navigation performance.",
            "For decades, human-computer interaction has fundamentally been manual. Even\ntoday, almost all productive work done on the computer necessitates human input\nat every step. Autonomous virtual agents represent an exciting step in\nautomating many of these menial tasks. Virtual agents would empower users with\nlimited technical proficiency to harness the full possibilities of computer\nsystems. They could also enable the efficient streamlining of numerous computer\ntasks, ranging from calendar management to complex travel bookings, with\nminimal human intervention. In this paper, we introduce OmniACT, the\nfirst-of-a-kind dataset and benchmark for assessing an agent's capability to\ngenerate executable programs to accomplish computer tasks. Our scope extends\nbeyond traditional web automation, covering a diverse range of desktop\napplications. The dataset consists of fundamental tasks such as \"Play the next\nsong\", as well as longer horizon tasks such as \"Send an email to John Doe\nmentioning the time and place to meet\". Specifically, given a pair of screen\nimage and a visually-grounded natural language task, the goal is to generate a\nscript capable of fully executing the task. We run several strong baseline\nlanguage model agents on our benchmark. The strongest baseline, GPT-4, performs\nthe best on our benchmark However, its performance level still reaches only 15%\nof the human proficiency in generating executable scripts capable of completing\nthe task, demonstrating the challenge of our task for conventional web agents.\nOur benchmark provides a platform to measure and evaluate the progress of\nlanguage model agents in automating computer tasks and motivates future work\ntowards building multimodal models that bridge large language models and the\nvisual grounding of computer screens."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2501.16150",
        "description": "Agent",
        "urls": [
            "https://arxiv.org/abs/2308.15272",
            "https://arxiv.org/abs/2312.11190",
            "https://arxiv.org/abs/2402.11941"
        ],
        "citations": [
            71,
            5,
            18
        ],
        "category": [
            "cs.AI",
            "cs.SE",
            "cs.HC",
            "cs.CL"
        ],
        "titles": [
            "AutoDroid: LLM-powered Task Automation in Android",
            "VisionTasker: Mobile Task Automation Using Vision Based UI Understanding and LLM Task Planning",
            "CoCo-Agent: A Comprehensive Cognitive MLLM Agent for Smartphone GUI Automation"
        ],
        "id": 23,
        "abstracts": [
            "Mobile task automation is an attractive technique that aims to enable\nvoice-based hands-free user interaction with smartphones. However, existing\napproaches suffer from poor scalability due to the limited language\nunderstanding ability and the non-trivial manual efforts required from\ndevelopers or end-users. The recent advance of large language models (LLMs) in\nlanguage understanding and reasoning inspires us to rethink the problem from a\nmodel-centric perspective, where task preparation, comprehension, and execution\nare handled by a unified language model. In this work, we introduce AutoDroid,\na mobile task automation system capable of handling arbitrary tasks on any\nAndroid application without manual efforts. The key insight is to combine the\ncommonsense knowledge of LLMs and domain-specific knowledge of apps through\nautomated dynamic analysis. The main components include a functionality-aware\nUI representation method that bridges the UI with the LLM, exploration-based\nmemory injection techniques that augment the app-specific domain knowledge of\nLLM, and a multi-granularity query optimization module that reduces the cost of\nmodel inference. We integrate AutoDroid with off-the-shelf LLMs including\nonline GPT-4/GPT-3.5 and on-device Vicuna, and evaluate its performance on a\nnew benchmark for memory-augmented Android task automation with 158 common\ntasks. The results demonstrated that AutoDroid is able to precisely generate\nactions with an accuracy of 90.9%, and complete tasks with a success rate of\n71.3%, outperforming the GPT-4-powered baselines by 36.4% and 39.7%. The demo,\nbenchmark suites, and source code of AutoDroid will be released at\nurl{https://autodroid-sys.github.io/}.",
            "Mobile task automation is an emerging field that leverages AI to streamline\nand optimize the execution of routine tasks on mobile devices, thereby\nenhancing efficiency and productivity. Traditional methods, such as Programming\nBy Demonstration (PBD), are limited due to their dependence on predefined tasks\nand susceptibility to app updates. Recent advancements have utilized the view\nhierarchy to collect UI information and employed Large Language Models (LLM) to\nenhance task automation. However, view hierarchies have accessibility issues\nand face potential problems like missing object descriptions or misaligned\nstructures. This paper introduces VisionTasker, a two-stage framework combining\nvision-based UI understanding and LLM task planning, for mobile task automation\nin a step-by-step manner. VisionTasker firstly converts a UI screenshot into\nnatural language interpretations using a vision-based UI understanding\napproach, eliminating the need for view hierarchies. Secondly, it adopts a\nstep-by-step task planning method, presenting one interface at a time to the\nLLM. The LLM then identifies relevant elements within the interface and\ndetermines the next action, enhancing accuracy and practicality. Extensive\nexperiments show that VisionTasker outperforms previous methods, providing\neffective UI representations across four datasets. Additionally, in automating\n147 real-world tasks on an Android smartphone, VisionTasker demonstrates\nadvantages over humans in tasks where humans show unfamiliarity and shows\nsignificant improvements when integrated with the PBD mechanism. VisionTasker\nis open-source and available at https://github.com/AkimotoAyako/VisionTasker.",
            "Multimodal large language models (MLLMs) have shown remarkable potential as\nhuman-like autonomous language agents to interact with real-world environments,\nespecially for graphical user interface (GUI) automation. However, those GUI\nagents require comprehensive cognition ability including exhaustive perception\nand reliable action response. We propose a Comprehensive Cognitive LLM Agent,\nCoCo-Agent, with two novel approaches, comprehensive environment perception\n(CEP) and conditional action prediction (CAP), to systematically improve the\nGUI automation performance. First, CEP facilitates the GUI perception through\ndifferent aspects and granularity, including screenshots and complementary\ndetailed layouts for the visual channel and historical actions for the textual\nchannel. Second, CAP decomposes the action prediction into sub-problems: action\ntype prediction and action target conditioned on the action type. With our\ntechnical design, our agent achieves new state-of-the-art performance on AITW\nand META-GUI benchmarks, showing promising abilities in realistic scenarios.\nCode is available at https://github.com/xbmxb/CoCo-Agent."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2412.10400",
        "description": "RLHF",
        "urls": [
            "https://arxiv.org/abs/2403.19270",
            "https://arxiv.org/abs/2402.05749",
            "https://arxiv.org/abs/2407.08639"
        ],
        "citations": [
            19,
            60,
            7
        ],
        "category": [
            "cs.AI",
            "cs.LG",
            "cs.CL"
        ],
        "titles": [
            "sDPO: Don't Use Your Data All at Once",
            "Generalized Preference Optimization: A Unified Approach to Offline Alignment",
            "$β$-DPO: Direct Preference Optimization with Dynamic $β$"
        ],
        "id": 24,
        "abstracts": [
            "As development of large language models (LLM) progresses, aligning them with\nhuman preferences has become increasingly important. We propose stepwise DPO\n(sDPO), an extension of the recently popularized direct preference optimization\n(DPO) for alignment tuning. This approach involves dividing the available\npreference datasets and utilizing them in a stepwise manner, rather than\nemploying it all at once. We demonstrate that this method facilitates the use\nof more precisely aligned reference models within the DPO training framework.\nFurthermore, sDPO trains the final model to be more performant, even\noutperforming other popular LLMs with more parameters.",
            "Offline preference optimization allows fine-tuning large models directly from\noffline data, and has proved effective in recent alignment practices. We\npropose generalized preference optimization (GPO), a family of offline losses\nparameterized by a general class of convex functions. GPO enables a unified\nview over preference optimization, encompassing existing algorithms such as\nDPO, IPO and SLiC as special cases, while naturally introducing new variants.\nThe GPO framework also sheds light on how offline algorithms enforce\nregularization, through the design of the convex function that defines the\nloss. Our analysis and experiments reveal the connections and subtle\ndifferences between the offline regularization and the KL divergence\nregularization intended by the canonical RLHF formulation. In a controlled\nsetting akin to Gao et al 2023, we also show that different GPO variants\nachieve similar trade-offs between regularization and performance, though the\noptimal values of hyper-parameter might differ as predicted by theory. In all,\nour results present new algorithmic toolkits and empirical insights to\nalignment practitioners.",
            "Direct Preference Optimization (DPO) has emerged as a compelling approach for\ntraining Large Language Models (LLMs) to adhere to human preferences. However,\nthe performance of DPO is sensitive to the fine-tuning of its trade-off\nparameter $\\beta$, as well as to the quality of the preference data. We analyze\nthe impact of $\\beta$ and data quality on DPO, uncovering that optimal $\\beta$\nvalues vary with the informativeness of pairwise data. Addressing the\nlimitations of static $\\beta$ values, we introduce a novel framework that\ndynamically calibrates $\\beta$ at the batch level, informed by data quality\nconsiderations. Additionally, our method incorporates $\\beta$-guided data\nfiltering to safeguard against the influence of outliers. Through empirical\nevaluation, we demonstrate that our dynamic $\\beta$ adjustment technique\nsignificantly improves DPO's performance across a range of models and datasets,\noffering a more robust and adaptable training paradigm for aligning LLMs with\nhuman feedback. The code is available at\n\\url{https://github.com/junkangwu/beta-DPO}."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2412.11384",
        "description": "Knowledge Graph",
        "urls": [
            "https://arxiv.org/abs/2402.06097",
            "https://arxiv.org/abs/2112.06567",
            "https://arxiv.org/abs/2203.09219"
        ],
        "citations": [
            3,
            31,
            9
        ],
        "category": [
            "cs.AI",
            "cs.LG",
            "cs.SI",
            "68R10",
            "q-bio.MN",
            "cs.DS"
        ],
        "titles": [
            "TWIG: Towards pre-hoc Hyperparameter Optimisation and Cross-Graph Generalisation via Simulated KGE Models",
            "Implications of Topological Imbalance for Representation Learning on Biomedical Knowledge Graphs",
            "Centrality Measures in multi-layer Knowledge Graphs"
        ],
        "id": 25,
        "abstracts": [
            "In this paper we introduce TWIG (Topologically-Weighted Intelligence\nGeneration), a novel, embedding-free paradigm for simulating the output of KGEs\nthat uses a tiny fraction of the parameters. TWIG learns weights from inputs\nthat consist of topological features of the graph data, with no coding for\nlatent representations of entities or edges. Our experiments on the UMLS\ndataset show that a single TWIG neural network can predict the results of\nstate-of-the-art ComplEx-N3 KGE model nearly exactly on across all\nhyperparameter configurations. To do this it uses a total of 2590 learnable\nparameters, but accurately predicts the results of 1215 different\nhyperparameter combinations with a combined cost of 29,322,000 parameters.\nBased on these results, we make two claims: 1) that KGEs do not learn latent\nsemantics, but only latent representations of structural patterns; 2) that\nhyperparameter choice in KGEs is a deterministic function of the KGE model and\ngraph structure. We further hypothesise that, as TWIG can simulate KGEs without\nembeddings, that node and edge embeddings are not needed to learn to accurately\npredict new facts in KGs. Finally, we formulate all of our findings under the\numbrella of the ``Structural Generalisation Hypothesis\", which suggests that\n``twiggy\" embedding-free / data-structure-based learning methods can allow a\nsingle neural network to simulate KGE performance, and perhaps solve the Link\nPrediction task, across many KGs from diverse domains and with different\nsemantics.",
            "Adoption of recently developed methods from machine learning has given rise\nto creation of drug-discovery knowledge graphs (KG) that utilize the\ninterconnected nature of the domain. Graph-based modelling of the data,\ncombined with KG embedding (KGE) methods, are promising as they provide a more\nintuitive representation and are suitable for inference tasks such as\npredicting missing links. One common application is to produce ranked lists of\ngenes for a given disease, where the rank is based on the perceived likelihood\nof association between the gene and the disease. It is thus critical that these\npredictions are not only pertinent but also biologically meaningful. However,\nKGs can be biased either directly due to the underlying data sources that are\nintegrated or due to modeling choices in the construction of the graph, one\nconsequence of which is that certain entities can get topologically\noverrepresented. We demonstrate the effect of these inherent structural\nimbalances, resulting in densely-connected entities being highly ranked no\nmatter the context. We provide support for this observation across different\ndatasets, models as well as predictive tasks. Further, we present various graph\nperturbation experiments which yield more support to the observation that KGE\nmodels can be more influenced by the frequency of entities rather than any\nbiological information encoded within the relations. Our results highlight the\nimportance of data modeling choices, and emphasizes the need for practitioners\nto be mindful of these issues when interpreting model outputs and during KG\ncomposition.",
            "Knowledge graphs play a central role for linking different data which leads\nto multiple layers. Thus, they are widely used in big data integration,\nespecially for connecting data from different domains. Few studies have\ninvestigated the questions how multiple layers within graphs impact methods and\nalgorithms developed for single-purpose networks, for example social networks.\nThis manuscript investigates the impact of multiple layers on centrality\nmeasures compared to single-purpose graph. In particular, (a) we develop an\nexperimental environment to (b) evaluate two different centrality measures -\ndegree and betweenness centrality - on random graphs inspired by social network\nanalysis: small-world and scale-free networks. The presented approach (c) shows\nthat the graph structures and topology has a great impact on its robustness for\nadditional data stored. Although the experimental analysis of random graphs\nallows us to make some basic observations we will (d) make suggestions for\nadditional research on particular graph structures that have a great impact on\nthe stability of networks."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2411.15594",
        "description": "LLM-as-judge future work",
        "urls": [
            "https://arxiv.org/abs/2210.03493",
            "https://arxiv.org/abs/2401.08967",
            "https://arxiv.org/abs/2402.04788"
        ],
        "citations": [
            829,
            15,
            41
        ],
        "category": [
            "cs.CL",
            "cs.AI",
            "cs.CV"
        ],
        "titles": [
            "Automatic Chain of Thought Prompting in Large Language Models",
            "ReFT: Reasoning with Reinforced Fine-Tuning",
            "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark"
        ],
        "id": 26,
        "abstracts": [
            "Large language models (LLMs) can perform complex reasoning by generating\nintermediate reasoning steps. Providing these steps for prompting\ndemonstrations is called chain-of-thought (CoT) prompting. CoT prompting has\ntwo major paradigms. One leverages a simple prompt like \"Let's think step by\nstep\" to facilitate step-by-step thinking before answering a question. The\nother uses a few manual demonstrations one by one, each composed of a question\nand a reasoning chain that leads to an answer. The superior performance of the\nsecond paradigm hinges on the hand-crafting of task-specific demonstrations one\nby one. We show that such manual efforts may be eliminated by leveraging LLMs\nwith the \"Let's think step by step\" prompt to generate reasoning chains for\ndemonstrations one by one, i.e., let's think not just step by step, but also\none by one. However, these generated chains often come with mistakes. To\nmitigate the effect of such mistakes, we find that diversity matters for\nautomatically constructing demonstrations. We propose an automatic CoT\nprompting method: Auto-CoT. It samples questions with diversity and generates\nreasoning chains to construct demonstrations. On ten public benchmark reasoning\ntasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of\nthe CoT paradigm that requires manual designs of demonstrations. Code is\navailable at https://github.com/amazon-research/auto-cot",
            "One way to enhance the reasoning capability of Large Language Models (LLMs)\nis to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT)\nannotations. This approach does not show sufficiently strong generalization\nability, however, because the training only relies on the given CoT data. In\nmath problem-solving, for example, there is usually only one annotated\nreasoning path for each question in the training data. Intuitively, it would be\nbetter for the algorithm to learn from multiple annotated reasoning paths given\na question. To address this issue, we propose a simple yet effective approach\ncalled Reinforced Fine-Tuning (ReFT) to enhance the generalizability of\nlearning LLMs for reasoning, with math problem-solving as an example. ReFT\nfirst warmups the model with SFT, and then employs on-line reinforcement\nlearning, specifically the PPO algorithm in this paper, to further fine-tune\nthe model, where an abundance of reasoning paths are automatically sampled\ngiven the question and the rewards are naturally derived from the ground-truth\nanswers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that\nReFT significantly outperforms SFT, and the performance can be potentially\nfurther boosted by combining inference-time strategies such as majority voting\nand re-ranking. Note that ReFT obtains the improvement by learning from the\nsame training questions as SFT, without relying on extra or augmented training\nquestions. This indicates a superior generalization ability for ReFT.",
            "Multimodal Large Language Models (MLLMs) have gained significant attention\nrecently, showing remarkable potential in artificial general intelligence.\nHowever, assessing the utility of MLLMs presents considerable challenges,\nprimarily due to the absence of multimodal benchmarks that align with human\npreferences. Drawing inspiration from the concept of LLM-as-a-Judge within\nLLMs, this paper introduces a novel benchmark, termed MLLM-as-a-Judge, to\nassess the ability of MLLMs in assisting judges across diverse modalities,\nencompassing three distinct tasks: Scoring Evaluation, Pair Comparison, and\nBatch Ranking. Our study reveals that, while MLLMs demonstrate remarkable\nhuman-like discernment in Pair Comparison, there is a significant divergence\nfrom human preferences in Scoring Evaluation and Batch Ranking. Furthermore, a\ncloser examination reveals persistent challenges in the judgment capacities of\nLLMs, including diverse biases, hallucinatory responses, and inconsistencies in\njudgment, even in advanced models such as GPT-4V. These findings emphasize the\npressing need for enhancements and further research efforts to be undertaken\nbefore regarding MLLMs as fully reliable evaluators. In light of this, we\nadvocate for additional efforts dedicated to supporting the continuous\ndevelopment within the domain of MLLM functioning as judges. The code and\ndataset are publicly available at our project homepage:\n\\url{https://mllm-judge.github.io/}."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2405.13245",
        "description": "Robotics",
        "urls": [
            "https://arxiv.org/abs/2310.08864",
            "https://arxiv.org/abs/2307.15818",
            "https://arxiv.org/abs/2209.05451"
        ],
        "citations": [
            291,
            761,
            450
        ],
        "category": [
            "cs.RO",
            "cs.CL",
            "cs.CV"
        ],
        "titles": [
            "Open X-Embodiment: Robotic Learning Datasets and RT-X Models",
            "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control",
            "Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation"
        ],
        "id": 27,
        "abstracts": [
            "Large, high-capacity models trained on diverse datasets have shown remarkable\nsuccesses on efficiently tackling downstream applications. In domains from NLP\nto Computer Vision, this has led to a consolidation of pretrained models, with\ngeneral pretrained backbones serving as a starting point for many applications.\nCan such a consolidation happen in robotics? Conventionally, robotic learning\nmethods train a separate model for every application, every robot, and even\nevery environment. Can we instead train generalist X-robot policy that can be\nadapted efficiently to new robots, tasks, and environments? In this paper, we\nprovide datasets in standardized data formats and models to make it possible to\nexplore this possibility in the context of robotic manipulation, alongside\nexperimental results that provide an example of effective X-robot policies. We\nassemble a dataset from 22 different robots collected through a collaboration\nbetween 21 institutions, demonstrating 527 skills (160266 tasks). We show that\na high-capacity model trained on this data, which we call RT-X, exhibits\npositive transfer and improves the capabilities of multiple robots by\nleveraging experience from other platforms. More details can be found on the\nproject website https://robotics-transformer-x.github.io.",
            "We study how vision-language models trained on Internet-scale data can be\nincorporated directly into end-to-end robotic control to boost generalization\nand enable emergent semantic reasoning. Our goal is to enable a single\nend-to-end trained model to both learn to map robot observations to actions and\nenjoy the benefits of large-scale pretraining on language and vision-language\ndata from the web. To this end, we propose to co-fine-tune state-of-the-art\nvision-language models on both robotic trajectory data and Internet-scale\nvision-language tasks, such as visual question answering. In contrast to other\napproaches, we propose a simple, general recipe to achieve this goal: in order\nto fit both natural language responses and robotic actions into the same\nformat, we express the actions as text tokens and incorporate them directly\ninto the training set of the model in the same way as natural language tokens.\nWe refer to such category of models as vision-language-action models (VLA) and\ninstantiate an example of such a model, which we call RT-2. Our extensive\nevaluation (6k evaluation trials) shows that our approach leads to performant\nrobotic policies and enables RT-2 to obtain a range of emergent capabilities\nfrom Internet-scale training. This includes significantly improved\ngeneralization to novel objects, the ability to interpret commands not present\nin the robot training data (such as placing an object onto a particular number\nor icon), and the ability to perform rudimentary reasoning in response to user\ncommands (such as picking up the smallest or largest object, or the one closest\nto another object). We further show that incorporating chain of thought\nreasoning allows RT-2 to perform multi-stage semantic reasoning, for example\nfiguring out which object to pick up for use as an improvised hammer (a rock),\nor which type of drink is best suited for someone who is tired (an energy\ndrink).",
            "Transformers have revolutionized vision and natural language processing with\ntheir ability to scale with large datasets. But in robotic manipulation, data\nis both limited and expensive. Can manipulation still benefit from Transformers\nwith the right problem formulation? We investigate this question with PerAct, a\nlanguage-conditioned behavior-cloning agent for multi-task 6-DoF manipulation.\nPerAct encodes language goals and RGB-D voxel observations with a Perceiver\nTransformer, and outputs discretized actions by ``detecting the next best voxel\naction''. Unlike frameworks that operate on 2D images, the voxelized 3D\nobservation and action space provides a strong structural prior for efficiently\nlearning 6-DoF actions. With this formulation, we train a single multi-task\nTransformer for 18 RLBench tasks (with 249 variations) and 7 real-world tasks\n(with 18 variations) from just a few demonstrations per task. Our results show\nthat PerAct significantly outperforms unstructured image-to-action agents and\n3D ConvNet baselines for a wide range of tabletop tasks."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2405.13218",
        "description": "Image synthesis",
        "urls": [
            "https://arxiv.org/abs/2405.13218",
            "https://arxiv.org/abs/2309.04669",
            "https://arxiv.org/abs/2404.02905"
        ],
        "citations": [
            7,
            55,
            131
        ],
        "category": [
            "cs.CV",
            "cs.AI"
        ],
        "titles": [
            "Computational Tradeoffs in Image Synthesis: Diffusion, Masked-Token, and Next-Token Prediction",
            "Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization",
            "Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction"
        ],
        "id": 28,
        "abstracts": [
            "Nearly every recent image synthesis approach, including diffusion,\nmasked-token prediction, and next-token prediction, uses a Transformer network\narchitecture. Despite this common backbone, there has been no direct, compute\ncontrolled comparison of how these approaches affect performance and\nefficiency. We analyze the scalability of each approach through the lens of\ncompute budget measured in FLOPs. We find that token prediction methods, led by\nnext-token prediction, significantly outperform diffusion on prompt following.\nOn image quality, while next-token prediction initially performs better,\nscaling trends suggest it is eventually matched by diffusion. We compare the\ninference compute efficiency of each approach and find that next token\nprediction is by far the most efficient. Based on our findings we recommend\ndiffusion for applications targeting image quality and low latency; and\nnext-token prediction when prompt following or throughput is more important.",
            "Recently, the remarkable advance of the Large Language Model (LLM) has\ninspired researchers to transfer its extraordinary reasoning capability to both\nvision and language data. However, the prevailing approaches primarily regard\nthe visual input as a prompt and focus exclusively on optimizing the text\ngeneration process conditioned upon vision content by a frozen LLM. Such an\ninequitable treatment of vision and language heavily constrains the model's\npotential. In this paper, we break through this limitation by representing both\nvision and language in a unified form. Specifically, we introduce a\nwell-designed visual tokenizer to translate the non-linguistic image into a\nsequence of discrete tokens like a foreign language that LLM can read. The\nresulting visual tokens encompass high-level semantics worthy of a word and\nalso support dynamic sequence length varying from the image. Coped with this\ntokenizer, the presented foundation model called LaVIT can handle both image\nand text indiscriminately under the same generative learning paradigm. This\nunification empowers LaVIT to serve as an impressive generalist interface to\nunderstand and generate multi-modal content simultaneously. Extensive\nexperiments further showcase that it outperforms the existing models by a large\nmargin on massive vision-language tasks. Our code and models are available at\nhttps://github.com/jy0205/LaVIT.",
            "We present Visual AutoRegressive modeling (VAR), a new generation paradigm\nthat redefines the autoregressive learning on images as coarse-to-fine\n\"next-scale prediction\" or \"next-resolution prediction\", diverging from the\nstandard raster-scan \"next-token prediction\". This simple, intuitive\nmethodology allows autoregressive (AR) transformers to learn visual\ndistributions fast and generalize well: VAR, for the first time, makes GPT-like\nAR models surpass diffusion transformers in image generation. On ImageNet\n256x256 benchmark, VAR significantly improve AR baseline by improving Frechet\ninception distance (FID) from 18.65 to 1.73, inception score (IS) from 80.4 to\n350.2, with around 20x faster inference speed. It is also empirically verified\nthat VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions\nincluding image quality, inference speed, data efficiency, and scalability.\nScaling up VAR models exhibits clear power-law scaling laws similar to those\nobserved in LLMs, with linear correlation coefficients near -0.998 as solid\nevidence. VAR further showcases zero-shot generalization ability in downstream\ntasks including image in-painting, out-painting, and editing. These results\nsuggest VAR has initially emulated the two important properties of LLMs:\nScaling Laws and zero-shot task generalization. We have released all models and\ncodes to promote the exploration of AR/VAR models for visual generation and\nunified learning."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2410.21465",
        "description": "KVcache",
        "urls": [
            "https://arxiv.org/abs/2410.21465",
            "https://arxiv.org/abs/2406.02542",
            "https://arxiv.org/abs/2406.10774"
        ],
        "citations": [
            1,
            8,
            31
        ],
        "category": [
            "cs.LG",
            "cs.CL"
        ],
        "titles": [
            "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference",
            "Loki: Low-rank Keys for Efficient Sparse Attention",
            "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"
        ],
        "id": 29,
        "abstracts": [
            "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
            "Inference on large language models (LLMs) can be expensive in terms of the\ncompute and memory costs involved, especially when long sequence lengths are\nused. In particular, the self-attention mechanism used in LLM inference\ncontributes significantly to these costs, which has sparked an interest in\napproximating the self-attention computation to reduce such costs. In this\nwork, we propose to approximate self-attention by focusing on the\ndimensionality of key vectors computed in the attention block. Our analysis\nreveals that key vectors lie in a significantly lower-dimensional space,\nconsistently across several datasets and models. Exploiting this observation,\nwe propose Loki, a novel sparse attention method that ranks and selects tokens\nin the KV-cache based on attention scores computed in low-dimensional space.\nOur evaluations show that Loki is able to speed up the attention computation\ndue to reduced data movement (load/store) and compute costs while maintaining\nthe efficacy of the models better than other popular approximation methods.",
            "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest ."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2410.23743",
        "description": "CommonsenseQA datasets",
        "urls": [
            "https://arxiv.org/abs/2109.01653",
            "https://arxiv.org/abs/2301.13688",
            "https://arxiv.org/abs/2101.02235"
        ],
        "citations": [
            66,
            649,
            551
        ],
        "category": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "titles": [
            "CREAK: A Dataset for Commonsense Reasoning over Entity Knowledge",
            "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning",
            "Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies"
        ],
        "id": 30,
        "abstracts": [
            "Most benchmark datasets targeting commonsense reasoning focus on everyday\nscenarios: physical knowledge like knowing that you could fill a cup under a\nwaterfall [Talmor et al., 2019], social knowledge like bumping into someone is\nawkward [Sap et al., 2019], and other generic situations. However, there is a\nrich space of commonsense inferences anchored to knowledge about specific\nentities: for example, deciding the truthfulness of a claim \"Harry Potter can\nteach classes on how to fly on a broomstick.\" Can models learn to combine\nentity knowledge with commonsense reasoning in this fashion? We introduce\nCREAK, a testbed for commonsense reasoning about entity knowledge, bridging\nfact-checking about entities (Harry Potter is a wizard and is skilled at riding\na broomstick) with commonsense inferences (if you're good at a skill you can\nteach others how to do it). Our dataset consists of 13k human-authored English\nclaims about entities that are either true or false, in addition to a small\ncontrast set. Crowdworkers can easily come up with these statements and human\nperformance on the dataset is high (high 90s); we argue that models should be\nable to blend entity knowledge and commonsense reasoning to do well here. In\nour experiments, we focus on the closed-book setting and observe that a\nbaseline model finetuned on existing fact verification benchmark struggles on\nCREAK. Training a model on CREAK improves accuracy by a substantial margin, but\nstill falls short of human performance. Our benchmark provides a unique probe\ninto natural language understanding models, testing both its ability to\nretrieve facts (e.g., who teaches at the University of Chicago?) and unstated\ncommonsense knowledge (e.g., butlers do not yell at guests).",
            "We study the design decisions of publicly available instruction tuning\nmethods, and break down the development of Flan 2022 (Chung et al., 2022).\nThrough careful ablation studies on the Flan Collection of tasks and methods,\nwe tease apart the effect of design decisions which enable Flan-T5 to\noutperform prior work by 3-17%+ across evaluation settings. We find task\nbalancing and enrichment techniques are overlooked but critical to effective\ninstruction tuning, and in particular, training with mixed prompt settings\n(zero-shot, few-shot, and chain-of-thought) actually yields stronger (2%+)\nperformance in all settings. In further experiments, we show Flan-T5 requires\nless finetuning to converge higher and faster than T5 on single downstream\ntasks, motivating instruction-tuned models as more computationally-efficient\nstarting checkpoints for new tasks. Finally, to accelerate research on\ninstruction tuning, we make the Flan 2022 collection of datasets, templates,\nand methods publicly available at\nhttps://github.com/google-research/FLAN/tree/main/flan/v2.",
            "A key limitation in current datasets for multi-hop reasoning is that the\nrequired steps for answering the question are mentioned in it explicitly. In\nthis work, we introduce StrategyQA, a question answering (QA) benchmark where\nthe required reasoning steps are implicit in the question, and should be\ninferred using a strategy. A fundamental challenge in this setup is how to\nelicit such creative questions from crowdsourcing workers, while covering a\nbroad range of potential strategies. We propose a data collection procedure\nthat combines term-based priming to inspire annotators, careful control over\nthe annotator population, and adversarial filtering for eliminating reasoning\nshortcuts. Moreover, we annotate each question with (1) a decomposition into\nreasoning steps for answering it, and (2) Wikipedia paragraphs that contain the\nanswers to each step. Overall, StrategyQA includes 2,780 examples, each\nconsisting of a strategy question, its decomposition, and evidence paragraphs.\nAnalysis shows that questions in StrategyQA are short, topic-diverse, and cover\na wide range of strategies. Empirically, we show that humans perform well (87%)\non this task, while our best baseline reaches an accuracy of $\\sim$66%."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2410.23743",
        "description": "Math dataset",
        "urls": [
            "https://arxiv.org/abs/2110.14168",
            "https://arxiv.org/abs/2009.03300",
            "https://arxiv.org/abs/1705.04146"
        ],
        "citations": [
            2532,
            2900,
            602
        ],
        "category": [
            "cs.LG",
            "cs.CL",
            "cs.AI",
            "cs.CY"
        ],
        "titles": [
            "Training Verifiers to Solve Math Word Problems",
            "Measuring Massive Multitask Language Understanding",
            "Program Induction by Rationale Generation : Learning to Solve and Explain Algebraic Word Problems"
        ],
        "id": 31,
        "abstracts": [
            "State-of-the-art language models can match human performance on many tasks,\nbut they still struggle to robustly perform multi-step mathematical reasoning.\nTo diagnose the failures of current models and support research, we introduce\nGSM8K, a dataset of 8.5K high quality linguistically diverse grade school math\nword problems. We find that even the largest transformer models fail to achieve\nhigh test performance, despite the conceptual simplicity of this problem\ndistribution. To increase performance, we propose training verifiers to judge\nthe correctness of model completions. At test time, we generate many candidate\nsolutions and select the one ranked highest by the verifier. We demonstrate\nthat verification significantly improves performance on GSM8K, and we provide\nstrong empirical evidence that verification scales more effectively with\nincreased data than a finetuning baseline.",
            "We propose a new test to measure a text model's multitask accuracy. The test\ncovers 57 tasks including elementary mathematics, US history, computer science,\nlaw, and more. To attain high accuracy on this test, models must possess\nextensive world knowledge and problem solving ability. We find that while most\nrecent models have near random-chance accuracy, the very largest GPT-3 model\nimproves over random chance by almost 20 percentage points on average. However,\non every one of the 57 tasks, the best models still need substantial\nimprovements before they can reach expert-level accuracy. Models also have\nlopsided performance and frequently do not know when they are wrong. Worse,\nthey still have near-random accuracy on some socially important subjects such\nas morality and law. By comprehensively evaluating the breadth and depth of a\nmodel's academic and professional understanding, our test can be used to\nanalyze models across many tasks and to identify important shortcomings.",
            "Solving algebraic word problems requires executing a series of arithmetic\noperations---a program---to obtain a final answer. However, since programs can\nbe arbitrarily complicated, inducing them directly from question-answer pairs\nis a formidable challenge. To make this task more feasible, we solve these\nproblems by generating answer rationales, sequences of natural language and\nhuman-readable mathematical expressions that derive the final answer through a\nseries of small steps. Although rationales do not explicitly specify programs,\nthey provide a scaffolding for their structure via intermediate milestones. To\nevaluate our approach, we have created a new 100,000-sample dataset of\nquestions, answers and rationales. Experimental results show that indirect\nsupervision of program learning via answer rationales is a promising strategy\nfor inducing arithmetic programs."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2410.24159",
        "description": "GPT & Bert",
        "urls": [
            "https://arxiv.org/abs/2410.24159",
            "https://arxiv.org/abs/2311.02265",
            "https://arxiv.org/abs/2303.09859"
        ],
        "citations": [
            1,
            11,
            33
        ],
        "category": [
            "cs.CL"
        ],
        "titles": [
            "GPT or BERT: why not both?",
            "Not all layers are equally as important: Every Layer Counts BERT",
            "Trained on 100 million words and still in shape: BERT meets British National Corpus"
        ],
        "id": 32,
        "abstracts": [
            "We present a simple way to merge masked language modeling with causal\nlanguage modeling. This hybrid training objective results in a model that\ncombines the strengths of both modeling paradigms within a single transformer\nstack: GPT-BERT can be transparently used like any standard causal or masked\nlanguage model. We test the pretraining process that enables this flexible\nbehavior on the BabyLM Challenge 2024. The results show that the hybrid\npretraining outperforms masked-only or causal-only models. We openly release\nthe models, training corpora and code.",
            "This paper introduces a novel modification of the transformer architecture,\ntailored for the data-efficient pretraining of language models. This aspect is\nevaluated by participating in the BabyLM challenge, where our solution won both\nthe strict and strict-small tracks. Our approach allows each transformer layer\nto select which outputs of previous layers to process. The empirical results\nverify the potential of this simple modification and show that not all layers\nare equally as important.",
            "While modern masked language models (LMs) are trained on ever larger corpora,\nwe here explore the effects of down-scaling training to a modestly-sized but\nrepresentative, well-balanced, and publicly available English text source --\nthe British National Corpus. We show that pre-training on this carefully\ncurated corpus can reach better performance than the original BERT model. We\nargue that this type of corpora has great potential as a language modeling\nbenchmark. To showcase this potential, we present fair, reproducible and\ndata-efficient comparative studies of LMs, in which we evaluate several\ntraining objectives and model architectures and replicate previous empirical\nresults in a systematic way. We propose an optimized LM architecture called\nLTG-BERT."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2412.15204",
        "description": "VLLM attack",
        "urls": [
            "https://arxiv.org/abs/2403.09766",
            "https://arxiv.org/abs/2305.16934",
            "https://arxiv.org/abs/2403.09346"
        ],
        "citations": [
            20,
            175,
            18
        ],
        "category": [
            "cs.CV",
            "cs.CL",
            "cs.CR",
            "cs.LG",
            "cs.MM",
            "cs.AI"
        ],
        "titles": [
            "An Image Is Worth 1000 Lies: Adversarial Transferability across Prompts on Vision-Language Models",
            "On Evaluating Adversarial Robustness of Large Vision-Language Models",
            "B-AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Black-box Adversarial Visual-Instructions"
        ],
        "id": 33,
        "abstracts": [
            "Different from traditional task-specific vision models, recent large VLMs can\nreadily adapt to different vision tasks by simply using different textual\ninstructions, i.e., prompts. However, a well-known concern about traditional\ntask-specific vision models is that they can be misled by imperceptible\nadversarial perturbations. Furthermore, the concern is exacerbated by the\nphenomenon that the same adversarial perturbations can fool different\ntask-specific models. Given that VLMs rely on prompts to adapt to different\ntasks, an intriguing question emerges: Can a single adversarial image mislead\nall predictions of VLMs when a thousand different prompts are given? This\nquestion essentially introduces a novel perspective on adversarial\ntransferability: cross-prompt adversarial transferability. In this work, we\npropose the Cross-Prompt Attack (CroPA). This proposed method updates the\nvisual adversarial perturbation with learnable prompts, which are designed to\ncounteract the misleading effects of the adversarial image. By doing this,\nCroPA significantly improves the transferability of adversarial examples across\nprompts. Extensive experiments are conducted to verify the strong cross-prompt\nadversarial transferability of CroPA with prevalent VLMs including Flamingo,\nBLIP-2, and InstructBLIP in various different tasks. Our source code is\navailable at \\url{https://github.com/Haochen-Luo/CroPA}.",
            "Large vision-language models (VLMs) such as GPT-4 have achieved unprecedented\nperformance in response generation, especially with visual inputs, enabling\nmore creative and adaptable interaction than large language models such as\nChatGPT. Nonetheless, multimodal generation exacerbates safety concerns, since\nadversaries may successfully evade the entire system by subtly manipulating the\nmost vulnerable modality (e.g., vision). To this end, we propose evaluating the\nrobustness of open-source large VLMs in the most realistic and high-risk\nsetting, where adversaries have only black-box system access and seek to\ndeceive the model into returning the targeted responses. In particular, we\nfirst craft targeted adversarial examples against pretrained models such as\nCLIP and BLIP, and then transfer these adversarial examples to other VLMs such\nas MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt. In addition, we\nobserve that black-box queries on these VLMs can further improve the\neffectiveness of targeted evasion, resulting in a surprisingly high success\nrate for generating targeted responses. Our findings provide a quantitative\nunderstanding regarding the adversarial vulnerability of large VLMs and call\nfor a more thorough examination of their potential security flaws before\ndeployment in practice. Code is at https://github.com/yunqing-me/AttackVLM.",
            "Large Vision-Language Models (LVLMs) have shown significant progress in\nresponding well to visual-instructions from users. However, these instructions,\nencompassing images and text, are susceptible to both intentional and\ninadvertent attacks. Despite the critical importance of LVLMs' robustness\nagainst such threats, current research in this area remains limited. To bridge\nthis gap, we introduce B-AVIBench, a framework designed to analyze the\nrobustness of LVLMs when facing various Black-box Adversarial\nVisual-Instructions (B-AVIs), including four types of image-based B-AVIs, ten\ntypes of text-based B-AVIs, and nine types of content bias B-AVIs (such as\ngender, violence, cultural, and racial biases, among others). We generate 316K\nB-AVIs encompassing five categories of multimodal capabilities (ten tasks) and\ncontent bias. We then conduct a comprehensive evaluation involving 14\nopen-source LVLMs to assess their performance. B-AVIBench also serves as a\nconvenient tool for practitioners to evaluate the robustness of LVLMs against\nB-AVIs. Our findings and extensive experimental results shed light on the\nvulnerabilities of LVLMs, and highlight that inherent biases exist even in\nadvanced closed-source LVLMs like GeminiProVision and GPT-4V. This underscores\nthe importance of enhancing the robustness, security, and fairness of LVLMs.\nThe source code and benchmark are available at\nhttps://github.com/zhanghao5201/B-AVIBench."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2412.15204",
        "description": "VLLM jailbreak",
        "urls": [
            "https://arxiv.org/abs/2405.20773",
            "https://arxiv.org/abs/2404.03411",
            "https://arxiv.org/abs/2402.02309"
        ],
        "citations": [
            12,
            17,
            68
        ],
        "category": [
            "cs.CR",
            "cs.LG",
            "cs.CL",
            "cs.AI",
            "cs.CV"
        ],
        "titles": [
            "Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Character",
            "Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?",
            "Jailbreaking Attack against Multimodal Large Language Model"
        ],
        "id": 34,
        "abstracts": [
            "With the advent and widespread deployment of Multimodal Large Language Models\n(MLLMs), ensuring their safety has become increasingly critical. To achieve\nthis objective, it requires us to proactively discover the vulnerability of\nMLLMs by exploring the attack methods. Thus, structure-based jailbreak attacks,\nwhere harmful semantic content is embedded within images, have been proposed to\nmislead the models. However, previous structure-based jailbreak methods mainly\nfocus on transforming the format of malicious queries, such as converting\nharmful content into images through typography, which lacks sufficient\njailbreak effectiveness and generalizability. To address these limitations, we\nfirst introduce the concept of \"Role-play\" into MLLM jailbreak attacks and\npropose a novel and effective method called Visual Role-play (VRP).\nSpecifically, VRP leverages Large Language Models to generate detailed\ndescriptions of high-risk characters and create corresponding images based on\nthe descriptions. When paired with benign role-play instruction texts, these\nhigh-risk character images effectively mislead MLLMs into generating malicious\nresponses by enacting characters with negative attributes. We further extend\nour VRP method into a universal setup to demonstrate its generalizability.\nExtensive experiments on popular benchmarks show that VRP outperforms the\nstrongest baseline, Query relevant and FigStep, by an average Attack Success\nRate (ASR) margin of 14.3% across all models.",
            "Various jailbreak attacks have been proposed to red-team Large Language\nModels (LLMs) and revealed the vulnerable safeguards of LLMs. Besides, some\nmethods are not limited to the textual modality and extend the jailbreak attack\nto Multimodal Large Language Models (MLLMs) by perturbing the visual input.\nHowever, the absence of a universal evaluation benchmark complicates the\nperformance reproduction and fair comparison. Besides, there is a lack of\ncomprehensive evaluation of closed-source state-of-the-art (SOTA) models,\nespecially MLLMs, such as GPT-4V. To address these issues, this work first\nbuilds a comprehensive jailbreak evaluation dataset with 1445 harmful questions\ncovering 11 different safety policies. Based on this dataset, extensive\nred-teaming experiments are conducted on 11 different LLMs and MLLMs, including\nboth SOTA proprietary models and open-source models. We then conduct a deep\nanalysis of the evaluated results and find that (1) GPT4 and GPT-4V demonstrate\nbetter robustness against jailbreak attacks compared to open-source LLMs and\nMLLMs. (2) Llama2 and Qwen-VL-Chat are more robust compared to other\nopen-source models. (3) The transferability of visual jailbreak methods is\nrelatively limited compared to textual jailbreak methods. The dataset and code\ncan be found https://github.com/chenxshuo/RedTeamingGPT4V",
            "This paper focuses on jailbreaking attacks against multi-modal large language\nmodels (MLLMs), seeking to elicit MLLMs to generate objectionable responses to\nharmful user queries. A maximum likelihood-based algorithm is proposed to find\nan \\emph{image Jailbreaking Prompt} (imgJP), enabling jailbreaks against MLLMs\nacross multiple unseen prompts and images (i.e., data-universal property). Our\napproach exhibits strong model-transferability, as the generated imgJP can be\ntransferred to jailbreak various models, including MiniGPT-v2, LLaVA,\nInstructBLIP, and mPLUG-Owl2, in a black-box manner. Moreover, we reveal a\nconnection between MLLM-jailbreaks and LLM-jailbreaks. As a result, we\nintroduce a construction-based method to harness our approach for\nLLM-jailbreaks, demonstrating greater efficiency than current state-of-the-art\nmethods. The code is available here. \\textbf{Warning: some content generated by\nlanguage models may be offensive to some readers.}"
        ]
    },
    {
        "source": "https://arxiv.org/abs/2404.10981",
        "description": "Rag pre-retrieve indexing",
        "urls": [
            "https://arxiv.org/abs/2305.14283",
            "https://arxiv.org/abs/2403.05676",
            "https://arxiv.org/abs/2310.05029"
        ],
        "citations": [
            201,
            21,
            50
        ],
        "category": [
            "cs.CL"
        ],
        "titles": [
            "Query Rewriting for Retrieval-Augmented Large Language Models",
            "PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design",
            "Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading"
        ],
        "id": 35,
        "abstracts": [
            "Large Language Models (LLMs) play powerful, black-box readers in the\nretrieve-then-read pipeline, making remarkable progress in knowledge-intensive\ntasks. This work introduces a new framework, Rewrite-Retrieve-Read instead of\nthe previous retrieve-then-read for the retrieval-augmented LLMs from the\nperspective of the query rewriting. Unlike prior studies focusing on adapting\neither the retriever or the reader, our approach pays attention to the\nadaptation of the search query itself, for there is inevitably a gap between\nthe input text and the needed knowledge in retrieval. We first prompt an LLM to\ngenerate the query, then use a web search engine to retrieve contexts.\nFurthermore, to better align the query to the frozen modules, we propose a\ntrainable scheme for our pipeline. A small language model is adopted as a\ntrainable rewriter to cater to the black-box LLM reader. The rewriter is\ntrained using the feedback of the LLM reader by reinforcement learning.\nEvaluation is conducted on downstream tasks, open-domain QA and multiple-choice\nQA. Experiments results show consistent performance improvement, indicating\nthat our framework is proven effective and scalable, and brings a new framework\nfor retrieval-augmented LLM.",
            "Retrieval-augmented generation (RAG) can enhance the generation quality of\nlarge language models (LLMs) by incorporating external token databases.\nHowever, retrievals from large databases can constitute a substantial portion\nof the overall generation time, particularly when retrievals are periodically\nperformed to align the retrieved content with the latest states of generation.\nIn this paper, we introduce PipeRAG, a novel algorithm-system co-design\napproach to reduce generation latency and enhance generation quality. PipeRAG\nintegrates (1) pipeline parallelism to enable concurrent retrieval and\ngeneration processes, (2) flexible retrieval intervals to maximize the\nefficiency of pipeline parallelism, and (3) a performance model to\nautomatically balance retrieval quality and latency based on the generation\nstates and underlying hardware. Our evaluation shows that, by combining the\nthree aforementioned methods, PipeRAG achieves up to 2.6$\\times$ speedup in\nend-to-end generation latency while improving generation quality. These\npromising results showcase the effectiveness of co-designing algorithms with\nunderlying systems, paving the way for the adoption of PipeRAG in future RAG\nsystems.",
            "Large language models (LLMs) have advanced in large strides due to the\neffectiveness of the self-attention mechanism that processes and compares all\ntokens at once. However, this mechanism comes with a fundamental issue -- the\npredetermined context window is bound to be limited. Despite attempts to extend\nthe context window through methods like extrapolating the positional embedding,\nusing recurrence, or selectively retrieving essential parts of the long\nsequence, long-text understanding continues to be a challenge. We propose an\nalternative approach which instead treats the LLM as an interactive agent,\nallowing it to decide how to read the text via iterative prompting. We\nintroduce MemWalker, a method that first processes the long context into a tree\nof summary nodes. Upon receiving a query, the model navigates this tree in\nsearch of relevant information, and responds once it gathers sufficient\ninformation. On long-text question answering tasks our method outperforms\nbaseline approaches that use long context windows, recurrence, and retrieval.\nWe show that, beyond effective reading, MemWalker enhances explainability by\nhighlighting the reasoning steps as it interactively reads the text;\npinpointing the relevant text segments related to the query."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2404.10981",
        "description": "Rag pre-retrieve indexing",
        "urls": [
            "https://arxiv.org/abs/2305.14283",
            "https://arxiv.org/abs/2403.05676",
            "https://arxiv.org/abs/2310.05029"
        ],
        "citations": [
            201,
            21,
            50
        ],
        "category": [
            "cs.CL"
        ],
        "titles": [
            "Query Rewriting for Retrieval-Augmented Large Language Models",
            "PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design",
            "Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading"
        ],
        "id": 36,
        "abstracts": [
            "Large Language Models (LLMs) play powerful, black-box readers in the\nretrieve-then-read pipeline, making remarkable progress in knowledge-intensive\ntasks. This work introduces a new framework, Rewrite-Retrieve-Read instead of\nthe previous retrieve-then-read for the retrieval-augmented LLMs from the\nperspective of the query rewriting. Unlike prior studies focusing on adapting\neither the retriever or the reader, our approach pays attention to the\nadaptation of the search query itself, for there is inevitably a gap between\nthe input text and the needed knowledge in retrieval. We first prompt an LLM to\ngenerate the query, then use a web search engine to retrieve contexts.\nFurthermore, to better align the query to the frozen modules, we propose a\ntrainable scheme for our pipeline. A small language model is adopted as a\ntrainable rewriter to cater to the black-box LLM reader. The rewriter is\ntrained using the feedback of the LLM reader by reinforcement learning.\nEvaluation is conducted on downstream tasks, open-domain QA and multiple-choice\nQA. Experiments results show consistent performance improvement, indicating\nthat our framework is proven effective and scalable, and brings a new framework\nfor retrieval-augmented LLM.",
            "Retrieval-augmented generation (RAG) can enhance the generation quality of\nlarge language models (LLMs) by incorporating external token databases.\nHowever, retrievals from large databases can constitute a substantial portion\nof the overall generation time, particularly when retrievals are periodically\nperformed to align the retrieved content with the latest states of generation.\nIn this paper, we introduce PipeRAG, a novel algorithm-system co-design\napproach to reduce generation latency and enhance generation quality. PipeRAG\nintegrates (1) pipeline parallelism to enable concurrent retrieval and\ngeneration processes, (2) flexible retrieval intervals to maximize the\nefficiency of pipeline parallelism, and (3) a performance model to\nautomatically balance retrieval quality and latency based on the generation\nstates and underlying hardware. Our evaluation shows that, by combining the\nthree aforementioned methods, PipeRAG achieves up to 2.6$\\times$ speedup in\nend-to-end generation latency while improving generation quality. These\npromising results showcase the effectiveness of co-designing algorithms with\nunderlying systems, paving the way for the adoption of PipeRAG in future RAG\nsystems.",
            "Large language models (LLMs) have advanced in large strides due to the\neffectiveness of the self-attention mechanism that processes and compares all\ntokens at once. However, this mechanism comes with a fundamental issue -- the\npredetermined context window is bound to be limited. Despite attempts to extend\nthe context window through methods like extrapolating the positional embedding,\nusing recurrence, or selectively retrieving essential parts of the long\nsequence, long-text understanding continues to be a challenge. We propose an\nalternative approach which instead treats the LLM as an interactive agent,\nallowing it to decide how to read the text via iterative prompting. We\nintroduce MemWalker, a method that first processes the long context into a tree\nof summary nodes. Upon receiving a query, the model navigates this tree in\nsearch of relevant information, and responds once it gathers sufficient\ninformation. On long-text question answering tasks our method outperforms\nbaseline approaches that use long context windows, recurrence, and retrieval.\nWe show that, beyond effective reading, MemWalker enhances explainability by\nhighlighting the reasoning steps as it interactively reads the text;\npinpointing the relevant text segments related to the query."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2411.03350",
        "description": "SLM pruning",
        "urls": [
            "https://arxiv.org/abs/2305.11627",
            "https://arxiv.org/abs/2403.03853",
            "https://arxiv.org/abs/2402.02834"
        ],
        "citations": [
            472,
            94,
            40
        ],
        "category": [
            "cs.CL",
            "cs.LG"
        ],
        "titles": [
            "LLM-Pruner: On the Structural Pruning of Large Language Models",
            "ShortGPT: Layers in Large Language Models are More Redundant Than You Expect",
            "Shortened LLaMA: Depth Pruning for Large Language Models with Comparison of Retraining Methods"
        ],
        "id": 37,
        "abstracts": [
            "Large language models (LLMs) have shown remarkable capabilities in language\nunderstanding and generation. However, such impressive capability typically\ncomes with a substantial model size, which presents significant challenges in\nboth the deployment, inference, and training stages. With LLM being a\ngeneral-purpose task solver, we explore its compression in a task-agnostic\nmanner, which aims to preserve the multi-task solving and language generation\nability of the original LLM. One challenge to achieving this is the enormous\nsize of the training corpus of LLM, which makes both data transfer and model\npost-training over-burdensome. Thus, we tackle the compression of LLMs within\nthe bound of two constraints: being task-agnostic and minimizing the reliance\non the original training dataset. Our method, named LLM-Pruner, adopts\nstructural pruning that selectively removes non-critical coupled structures\nbased on gradient information, maximally preserving the majority of the LLM's\nfunctionality. To this end, the performance of pruned models can be efficiently\nrecovered through tuning techniques, LoRA, in merely 3 hours, requiring only\n50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna,\nand ChatGLM, and demonstrate that the compressed models still exhibit\nsatisfactory capabilities in zero-shot classification and generation. The code\nis available at: https://github.com/horseee/LLM-Pruner",
            "As Large Language Models (LLMs) continue to advance in performance, their\nsize has escalated significantly, with current LLMs containing billions or even\ntrillions of parameters. However, in this study, we discovered that many layers\nof LLMs exhibit high similarity, and some layers play a negligible role in\nnetwork functionality. Based on this observation, we define a metric called\nBlock Influence (BI) to gauge the significance of each layer in LLMs. We then\npropose a straightforward pruning approach: layer removal, in which we directly\ndelete the redundant layers in LLMs based on their BI scores. Experiments\ndemonstrate that our method, which we call ShortGPT, significantly outperforms\nprevious state-of-the-art (SOTA) methods in model pruning. Moreover, ShortGPT\nis orthogonal to quantization-like methods, enabling further reduction in\nparameters and computation. The ability to achieve better results through\nsimple layer removal, as opposed to more complex pruning techniques, suggests a\nhigh degree of redundancy in the model architecture.",
            "Structured pruning of modern large language models (LLMs) has emerged as a\nway of decreasing their high computational needs. Width pruning reduces the\nsize of projection weight matrices (e.g., by removing attention heads) while\nmaintaining the number of layers. Depth pruning, in contrast, removes entire\nlayers or blocks, while keeping the size of the remaining weights unchanged.\nMost current research focuses on either width-only or a blend of width and\ndepth pruning, with little comparative analysis between the two units (width\nvs. depth) concerning their impact on LLM inference efficiency. In this work,\nwe show that simple depth pruning can effectively compress LLMs while achieving\ncomparable or superior performance to recent width pruning studies. Our pruning\nmethod boosts inference speeds, especially under memory-constrained conditions\nthat require limited batch sizes for running LLMs, where width pruning is\nineffective. In retraining pruned models for quality recovery, continued\npretraining on a large corpus markedly outperforms LoRA-based tuning,\nparticularly at severe pruning ratios. We hope this work can help build compact\nyet capable LLMs. Code and models can be found at:\nhttps://github.com/Nota-NetsPresso/shortened-llm"
        ]
    },
    {
        "source": "https://arxiv.org/abs/2411.03350",
        "description": "SLM knowledge-distill",
        "urls": [
            "https://arxiv.org/abs/2412.15204",
            "https://arxiv.org/abs/2307.11088",
            "https://arxiv.org/abs/2310.15929"
        ],
        "citations": [
            2,
            23,
            9
        ],
        "category": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "titles": [
            "LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks",
            "L-Eval: Instituting Standardized Evaluation for Long Context Language Models",
            "E-Sparse: Boosting the Large Language Model Inference through Entropy-based N:M Sparsity"
        ],
        "id": 38,
        "abstracts": [
            "This paper introduces LongBench v2, a benchmark designed to assess the\nability of LLMs to handle long-context problems requiring deep understanding\nand reasoning across real-world multitasks. LongBench v2 consists of 503\nchallenging multiple-choice questions, with contexts ranging from 8k to 2M\nwords, across six major task categories: single-document QA, multi-document QA,\nlong in-context learning, long-dialogue history understanding, code repository\nunderstanding, and long structured data understanding. To ensure the breadth\nand the practicality, we collect data from nearly 100 highly educated\nindividuals with diverse professional backgrounds. We employ both automated and\nmanual review processes to maintain high quality and difficulty, resulting in\nhuman experts achieving only 53.7% accuracy under a 15-minute time constraint.\nOur evaluation reveals that the best-performing model, when directly answers\nthe questions, achieves only 50.1% accuracy. In contrast, the o1-preview model,\nwhich includes longer reasoning, achieves 57.7%, surpassing the human baseline\nby 4%. These results highlight the importance of enhanced reasoning ability and\nscaling inference-time compute to tackle the long-context challenges in\nLongBench v2. The project is available at https://longbench2.github.io.",
            "Recently, there has been growing interest in extending the context length of\nlarge language models (LLMs), aiming to effectively process long inputs of one\nturn or conversations with more extensive histories. While proprietary models\nsuch as GPT-4 and Claude can largely preserve the reasoning ability in an\nextended context, open-source models are still progressing through the early\nstages of development. To bridge this gap, we propose L-Eval to institute a\nmore standardized evaluation for long context language models (LCLMs)\naddressing two key aspects: dataset construction and evaluation metrics. On the\none hand, we build a new evaluation suite containing 20 sub-tasks, 508 long\ndocuments, and over 2,000 human-labeled query-response pairs encompassing\ndiverse question styles, domains, and input length (3k$\\sim$200k tokens). On\nthe other hand, we investigate the effectiveness in evalution metrics for\nLCLMs. Results show that popular n-gram matching metrics generally can not\ncorrelate well with human judgment, and thus we strongly advocate for\nlength-instruction-enhanced (LIE) evaluation and employing LLM judges. We\nconducted a comprehensive study of 4 popular commercial LLMs and 12 open-source\ncounterparts using the L-Eval benchmark. Our empirical findings offer useful\ninsights into the study of LCLMs and lay the groundwork for the development of\nmore principled evaluation of these models.",
            "Traditional pruning methods are known to be challenging to work in Large\nLanguage Models (LLMs) for Generative AI because of their unaffordable training\nprocess and large computational demands. For the first time, we introduce the\ninformation entropy of hidden state features into a pruning metric design,\nnamely E-Sparse, to improve the accuracy of N:M sparsity on LLM. E-Sparse\nemploys the information richness to leverage the channel importance, and\nfurther incorporates several novel techniques to put it into effect: (1) it\nintroduces information entropy to enhance the significance of parameter weights\nand input feature norms as a novel pruning metric, and performs N:M sparsity\nwithout modifying the remaining weights. (2) it designs global naive shuffle\nand local block shuffle to quickly optimize the information distribution and\nadequately cope with the impact of N:M sparsity on LLMs' accuracy. E-Sparse is\nimplemented as a Sparse-GEMM on FasterTransformer and runs on NVIDIA Ampere\nGPUs. Extensive experiments on the LLaMA family and OPT models show that\nE-Sparse can significantly speed up the model inference over the dense model\n(up to 1.53X) and obtain significant memory saving (up to 43.52%), with\nacceptable accuracy loss."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2405.05254",
        "description": "xLSTM,Gate Linear Attention,SSM(h3)",
        "urls": [
            "https://arxiv.org/abs/2405.05254",
            "https://arxiv.org/abs/2312.06635",
            "https://arxiv.org/abs/2212.14052"
        ],
        "citations": [
            32,
            99,
            429
        ],
        "category": [
            "cs.CL",
            "cs.LG"
        ],
        "titles": [
            "You Only Cache Once: Decoder-Decoder Architectures for Language Models",
            "Gated Linear Attention Transformers with Hardware-Efficient Training",
            "Hungry Hungry Hippos: Towards Language Modeling with State Space Models"
        ],
        "id": 39,
        "abstracts": [
            "We introduce a decoder-decoder architecture, YOCO, for large language models,\nwhich only caches key-value pairs once. It consists of two components, i.e., a\ncross-decoder stacked upon a self-decoder. The self-decoder efficiently encodes\nglobal key-value (KV) caches that are reused by the cross-decoder via\ncross-attention. The overall model behaves like a decoder-only Transformer,\nalthough YOCO only caches once. The design substantially reduces GPU memory\ndemands, yet retains global attention capability. Additionally, the computation\nflow enables prefilling to early exit without changing the final output,\nthereby significantly speeding up the prefill stage. Experimental results\ndemonstrate that YOCO achieves favorable performance compared to Transformer in\nvarious settings of scaling up model size and number of training tokens. We\nalso extend YOCO to 1M context length with near-perfect needle retrieval\naccuracy. The profiling results show that YOCO improves inference memory,\nprefill latency, and throughput by orders of magnitude across context lengths\nand model sizes. Code is available at https://aka.ms/YOCO.",
            "Transformers with linear attention allow for efficient parallel training but\ncan simultaneously be formulated as an RNN with 2D (matrix-valued) hidden\nstates, thus enjoying linear-time inference complexity. However, linear\nattention generally underperforms ordinary softmax attention. Moreover, current\nimplementations of linear attention lack I/O-awareness and are thus slower than\nhighly optimized implementations of softmax attention. This work describes a\nhardware-efficient algorithm for linear attention that trades off memory\nmovement against parallelizability. The resulting implementation, dubbed\nFLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a\nstandalone layer even on short sequence lengths (e.g., 1K). We then generalize\nthis algorithm to a more expressive variant of linear attention with\ndata-dependent gates. When used as a replacement for the standard attention\nlayer in Transformers, the resulting gated linear attention (GLA) Transformer\nis found to perform competitively against the LLaMA-architecture Transformer\n(Touvron et al., 2023) as well recent linear-time-inference baselines such as\nRetNet (Sun et al., 2023a) and Mamba (Gu & Dao, 2023) on moderate-scale\nlanguage modeling experiments. GLA Transformer is especially effective at\nlength generalization, enabling a model trained on 2K to generalize to\nsequences longer than 20K without significant perplexity degradations. For\ntraining speed, the GLA Transformer has higher throughput than a\nsimilarly-sized Mamba model.",
            "State space models (SSMs) have demonstrated state-of-the-art sequence\nmodeling performance in some modalities, but underperform attention in language\nmodeling. Moreover, despite scaling nearly linearly in sequence length instead\nof quadratically, SSMs are still slower than Transformers due to poor hardware\nutilization. In this paper, we make progress on understanding the expressivity\ngap between SSMs and attention in language modeling, and on reducing the\nhardware barrier between SSMs and attention. First, we use synthetic language\nmodeling tasks to understand the gap between SSMs and attention. We find that\nexisting SSMs struggle with two capabilities: recalling earlier tokens in the\nsequence and comparing tokens across the sequence. To understand the impact on\nlanguage modeling, we propose a new SSM layer, H3, that is explicitly designed\nfor these abilities. H3 matches attention on the synthetic languages and comes\nwithin 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid\n125M-parameter H3-attention model that retains two attention layers\nsurprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to\nimprove the efficiency of training SSMs on modern hardware, we propose\nFlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on\nsequences up to 8K, and introduces a novel state passing algorithm that\nexploits the recurrent properties of SSMs to scale to longer sequences.\nFlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows\nhybrid language models to generate text 2.4$\\times$ faster than Transformers.\nUsing FlashConv, we scale hybrid H3-attention language models up to 2.7B\nparameters on the Pile and find promising initial results, achieving lower\nperplexity than Transformers and outperforming Transformers in zero- and\nfew-shot learning on a majority of tasks in the SuperGLUE benchmark."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2412.15035",
        "description": "Multi-lingual research",
        "urls": [
            "https://arxiv.org/abs/2405.09373",
            "https://arxiv.org/abs/2412.15035",
            "https://arxiv.org/abs/2404.14397"
        ],
        "citations": [
            12,
            0,
            1
        ],
        "category": [
            "cs.CL",
            "cs.CY",
            "cs.LG"
        ],
        "titles": [
            "PolygloToxicityPrompts: Multilingual Evaluation of Neural Toxic Degeneration in Large Language Models",
            "LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps",
            "RTP-LX: Can LLMs Evaluate Toxicity in Multilingual Scenarios?"
        ],
        "id": 40,
        "abstracts": [
            "Recent advances in large language models (LLMs) have led to their extensive\nglobal deployment, and ensuring their safety calls for comprehensive and\nmultilingual toxicity evaluations. However, existing toxicity benchmarks are\noverwhelmingly focused on English, posing serious risks to deploying LLMs in\nother languages. We address this by introducing PolygloToxicityPrompts (PTP),\nthe first large-scale multilingual toxicity evaluation benchmark of 425K\nnaturally occurring prompts spanning 17 languages. We overcome the scarcity of\nnaturally occurring toxicity in web-text and ensure coverage across languages\nwith varying resources by automatically scraping over 100M web-text documents.\nUsing PTP, we investigate research questions to study the impact of model size,\nprompt language, and instruction and preference-tuning methods on toxicity by\nbenchmarking over 60 LLMs. Notably, we find that toxicity increases as language\nresources decrease or model size increases. Although instruction- and\npreference-tuning reduce toxicity, the choice of preference-tuning method does\nnot have any significant impact. Our findings shed light on crucial\nshortcomings of LLM safeguarding and highlight areas for future research.",
            "Building safe Large Language Models (LLMs) across multiple languages is\nessential in ensuring both safe access and linguistic diversity. To this end,\nwe introduce M-ALERT, a multilingual benchmark that evaluates the safety of\nLLMs in five languages: English, French, German, Italian, and Spanish. M-ALERT\nincludes 15k high-quality prompts per language, totaling 75k, following the\ndetailed ALERT taxonomy. Our extensive experiments on 10 state-of-the-art LLMs\nhighlight the importance of language-specific safety analysis, revealing that\nmodels often exhibit significant inconsistencies in safety across languages and\ncategories. For instance, Llama3.2 shows high unsafety in the category\ncrime_tax for Italian but remains safe in other languages. Similar differences\ncan be observed across all models. In contrast, certain categories, such as\nsubstance_cannabis and crime_propaganda, consistently trigger unsafe responses\nacross models and languages. These findings underscore the need for robust\nmultilingual safety practices in LLMs to ensure safe and responsible usage\nacross diverse user communities.",
            "Large language models (LLMs) and small language models (SLMs) are being\nadopted at remarkable speed, although their safety still remains a serious\nconcern. With the advent of multilingual S/LLMs, the question now becomes a\nmatter of scale: can we expand multilingual safety evaluations of these models\nwith the same velocity at which they are deployed? To this end, we introduce\nRTP-LX, a human-transcreated and human-annotated corpus of toxic prompts and\noutputs in 28 languages. RTP-LX follows participatory design practices, and a\nportion of the corpus is especially designed to detect culturally-specific\ntoxic language. We evaluate 10 S/LLMs on their ability to detect toxic content\nin a culturally-sensitive, multilingual scenario. We find that, although they\ntypically score acceptably in terms of accuracy, they have low agreement with\nhuman judges when scoring holistically the toxicity of a prompt; and have\ndifficulty discerning harm in context-dependent scenarios, particularly with\nsubtle-yet-harmful content (e.g. microaggressions, bias). We release this\ndataset to contribute to further reduce harmful uses of these models and\nimprove their safe deployment."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2310.05737",
        "description": "Image generation",
        "urls": [
            "https://arxiv.org/abs/2310.05737",
            "https://arxiv.org/abs/2301.11093",
            "https://arxiv.org/abs/2202.04200"
        ],
        "citations": [
            190,
            200,
            572
        ],
        "category": [
            "cs.CV",
            "cs.AI",
            "cs.MM",
            "cs.LG",
            "stat.ML"
        ],
        "titles": [
            "Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation",
            "Simple diffusion: End-to-end diffusion for high resolution images",
            "MaskGIT: Masked Generative Image Transformer"
        ],
        "id": 41,
        "abstracts": [
            "While Large Language Models (LLMs) are the dominant models for generative\ntasks in language, they do not perform as well as diffusion models on image and\nvideo generation. To effectively use LLMs for visual generation, one crucial\ncomponent is the visual tokenizer that maps pixel-space inputs to discrete\ntokens appropriate for LLM learning. In this paper, we introduce MAGVIT-v2, a\nvideo tokenizer designed to generate concise and expressive tokens for both\nvideos and images using a common token vocabulary. Equipped with this new\ntokenizer, we show that LLMs outperform diffusion models on standard image and\nvideo generation benchmarks including ImageNet and Kinetics. In addition, we\ndemonstrate that our tokenizer surpasses the previously top-performing video\ntokenizer on two more tasks: (1) video compression comparable to the\nnext-generation video codec (VCC) according to human evaluations, and (2)\nlearning effective representations for action recognition tasks.",
            "Currently, applying diffusion models in pixel space of high resolution images\nis difficult. Instead, existing approaches focus on diffusion in lower\ndimensional spaces (latent diffusion), or have multiple super-resolution levels\nof generation referred to as cascades. The downside is that these approaches\nadd additional complexity to the diffusion framework.\n  This paper aims to improve denoising diffusion for high resolution images\nwhile keeping the model as simple as possible. The paper is centered around the\nresearch question: How can one train a standard denoising diffusion models on\nhigh resolution images, and still obtain performance comparable to these\nalternate approaches?\n  The four main findings are: 1) the noise schedule should be adjusted for high\nresolution images, 2) It is sufficient to scale only a particular part of the\narchitecture, 3) dropout should be added at specific locations in the\narchitecture, and 4) downsampling is an effective strategy to avoid high\nresolution feature maps. Combining these simple yet effective techniques, we\nachieve state-of-the-art on image generation among diffusion models without\nsampling modifiers on ImageNet.",
            "Generative transformers have experienced rapid popularity growth in the\ncomputer vision community in synthesizing high-fidelity and high-resolution\nimages. The best generative transformer models so far, however, still treat an\nimage naively as a sequence of tokens, and decode an image sequentially\nfollowing the raster scan ordering (i.e. line-by-line). We find this strategy\nneither optimal nor efficient. This paper proposes a novel image synthesis\nparadigm using a bidirectional transformer decoder, which we term MaskGIT.\nDuring training, MaskGIT learns to predict randomly masked tokens by attending\nto tokens in all directions. At inference time, the model begins with\ngenerating all tokens of an image simultaneously, and then refines the image\niteratively conditioned on the previous generation. Our experiments demonstrate\nthat MaskGIT significantly outperforms the state-of-the-art transformer model\non the ImageNet dataset, and accelerates autoregressive decoding by up to 64x.\nBesides, we illustrate that MaskGIT can be easily extended to various image\nediting tasks, such as inpainting, extrapolation, and image manipulation."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2412.17739",
        "description": "positional encoding",
        "urls": [
            "https://arxiv.org/abs/2404.12224",
            "https://arxiv.org/abs/2412.17739",
            "https://arxiv.org/abs/2302.01925"
        ],
        "citations": [
            4,
            0,
            8
        ],
        "category": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "titles": [
            "Length Generalization of Causal Transformers without Position Encoding",
            "Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization",
            "Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers"
        ],
        "id": 42,
        "abstracts": [
            "Generalizing to longer sentences is important for recent Transformer-based\nlanguage models. Besides algorithms manipulating explicit position features,\nthe success of Transformers without position encodings (NoPE) provides a new\nway to overcome the challenge. In this paper, we study the length\ngeneralization property of NoPE. We find that although NoPE can extend to\nlonger sequences than the commonly used explicit position encodings, it still\nhas a limited context length. We identify a connection between the failure of\nNoPE's generalization and the distraction of attention distributions. We\npropose a parameter-efficient tuning for searching attention heads' best\ntemperature hyper-parameters, which substantially expands NoPE's context size.\nExperiments on long sequence language modeling, the synthetic passkey retrieval\ntask and real-world long context tasks show that NoPE can achieve competitive\nperformances with state-of-the-art length generalization algorithms. The source\ncode is publicly accessible",
            "Extending the context length of Language Models (LMs) by improving Rotary\nPosition Embedding (RoPE) has become a trend. While existing works mainly\naddress RoPE's limitations within attention mechanism, this paper provides an\nanalysis across nearly all parts of LMs, uncovering their adverse effects on\nlength generalization for RoPE-based attention. Using Discrete Signal\nProcessing theory, we show that RoPE enables periodic attention by implicitly\nachieving Non-Uniform Discrete Fourier Transform. However, this periodicity is\nundermined by the spectral damage caused by: 1) linear layers and activation\nfunctions outside of attention; 2) insufficiently trained frequency components\nbrought by time-domain truncation. Building on our observations, we propose\nFourier Position Embedding (FoPE), which enhances attention's frequency-domain\nproperties to improve both its periodic extension and length generalization.\nFoPE constructs Fourier Series and zero-outs the destructive frequency\ncomponents, increasing model robustness against the spectrum damage.\nExperiments across various model scales show that, within varying context\nwindows, FoPE can maintain a more stable perplexity and a more consistent\naccuracy in a needle-in-haystack task compared to RoPE and ALiBi. Several\nanalyses and ablations bring further support to our method and theoretical\nmodeling.",
            "We propose a new class of linear Transformers called\nFourierLearner-Transformers (FLTs), which incorporate a wide range of relative\npositional encoding mechanisms (RPEs). These include regular RPE techniques\napplied for sequential data, as well as novel RPEs operating on geometric data\nembedded in higher-dimensional Euclidean spaces. FLTs construct the optimal RPE\nmechanism implicitly by learning its spectral representation. As opposed to\nother architectures combining efficient low-rank linear attention with RPEs,\nFLTs remain practical in terms of their memory usage and do not require\nadditional assumptions about the structure of the RPE mask. Besides, FLTs allow\nfor applying certain structural inductive bias techniques to specify masking\nstrategies, e.g. they provide a way to learn the so-called local RPEs\nintroduced in this paper and give accuracy gains as compared with several other\nlinear Transformers for language modeling. We also thoroughly test FLTs on\nother data modalities and tasks, such as image classification, 3D molecular\nmodeling, and learnable optimizers. To the best of our knowledge, for 3D\nmolecular data, FLTs are the first Transformer architectures providing linear\nattention and incorporating RPE masking."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2410.21252",
        "description": "Long context reward",
        "urls": [
            "https://arxiv.org/abs/2410.21252",
            "https://arxiv.org/abs/2401.18058",
            "https://arxiv.org/abs/2404.00934"
        ],
        "citations": [
            1,
            29,
            4
        ],
        "category": [
            "cs.CL",
            "cs.LG"
        ],
        "titles": [
            "LongReward: Improving Long-context Large Language Models with AI Feedback",
            "LongAlign: A Recipe for Long Context Alignment of Large Language Models",
            "ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback"
        ],
        "id": 43,
        "abstracts": [
            "Though significant advancements have been achieved in developing long-context\nlarge language models (LLMs), the compromised quality of LLM-synthesized data\nfor supervised fine-tuning (SFT) often affects the long-context performance of\nSFT models and leads to inherent limitations. In principle, reinforcement\nlearning (RL) with appropriate reward signals can further enhance models'\ncapacities. However, how to obtain reliable rewards in long-context scenarios\nremains unexplored. To this end, we propose LongReward, a novel method that\nutilizes an off-the-shelf LLM to provide rewards for long-context model\nresponses from four human-valued dimensions: helpfulness, logicality,\nfaithfulness, and completeness, each with a carefully designed assessment\npipeline. By combining LongReward and offline RL algorithm DPO, we are able to\neffectively improve long-context SFT models. Our experiments indicate that\nLongReward not only significantly improves models' long-context performance but\nalso enhances their ability to follow short instructions. We also find that\nlong-context DPO with LongReward and conventional short-context DPO can be used\ntogether without hurting either one's performance.",
            "Extending large language models to effectively handle long contexts requires\ninstruction fine-tuning on input sequences of similar length. To address this,\nwe present LongAlign -- a recipe of the instruction data, training, and\nevaluation for long context alignment. First, we construct a long\ninstruction-following dataset using Self-Instruct. To ensure the data\ndiversity, it covers a broad range of tasks from various long context sources.\nSecond, we adopt the packing and sorted batching strategies to speed up\nsupervised fine-tuning on data with varied length distributions. Additionally,\nwe develop a loss weighting method to balance the contribution to the loss\nacross different sequences during packing training. Third, we introduce the\nLongBench-Chat benchmark for evaluating instruction-following capabilities on\nqueries of 10k-100k in length. Experiments show that LongAlign outperforms\nexisting recipes for LLMs in long context tasks by up to 30\\%, while also\nmaintaining their proficiency in handling short, generic tasks. The code, data,\nand long-aligned models are open-sourced at https://github.com/THUDM/LongAlign.",
            "ChatGLM is a free-to-use AI service powered by the ChatGLM family of large\nlanguage models (LLMs). In this paper, we present the ChatGLM-RLHF pipeline --\na reinforcement learning from human feedback (RLHF) system -- designed to\nenhance ChatGLM's alignment with human preferences. ChatGLM-RLHF encompasses\nthree major components: the collection of human preference data, the training\nof the reward model, and the optimization of policies. Throughout the process\nof integrating ChatGLM-RLHF into production, we encountered and addressed\nseveral unprecedented challenges. We introduce the strategies to mitigate\nreward variance for stabilized large-scale training, implement model\nparallelism with fused gradient-descent, and design regularization constraints\nto avoid catastrophic forgetting in LLMs. Experiments show that ChatGLM-RLHF\nbrings significant improvements in alignment tasks compared to the supervised\nfine-tuned (SFT) version of ChatGLM. For instance, it achieves on average 15\\%\nmore wins against ChatGLM-SFT in Chinese alignment tasks. The work presents our\npractices of aligning LLMs with human preferences, offering insights into the\nchallenges and solutions in RLHF implementations."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2410.16270",
        "description": "Reflection planning benchmark",
        "urls": [
            "https://arxiv.org/abs/2410.16270",
            "https://arxiv.org/abs/1803.05457",
            "https://arxiv.org/abs/2206.10498"
        ],
        "citations": [
            0,
            1832,
            328
        ],
        "category": [
            "cs.AI",
            "cs.CL",
            "cs.IR"
        ],
        "titles": [
            "Reflection-Bench: probing AI intelligence with reflection",
            "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge",
            "PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change"
        ],
        "id": 44,
        "abstracts": [
            "The ability to adapt beliefs or behaviors in response to unexpected outcomes,\nreflection, is fundamental to intelligent systems' interaction with the world.\nFrom a cognitive science perspective, this serves as a core principle of\nintelligence applicable to both human and AI systems. To address the debate on\nthe intelligence of large language models (LLMs), we propose Reflection-Bench,\na comprehensive benchmark comprising 7 tasks spanning core cognitive functions\ncrucial for reflection, including perception, memory, belief updating,\ndecision-making, prediction, counterfactual thinking, and meta-reflection. We\nevaluate the performances of 13 prominent LLMs such as OpenAI o1, GPT-4, Claude\n3.5 Sonnet, etc. The results indicate that current LLMs still lack satisfactory\nreflection ability. We discuss the underlying causes of these results and\nsuggest potential avenues for future research. In conclusion, Reflection-Bench\noffers both evaluation tools and inspiration for developing AI capable of\nreliably interacting with the environment. Our data and code are available at\nhttps://github.com/YabYum/ReflectionBench.",
            "We present a new question set, text corpus, and baselines assembled to\nencourage AI research in advanced question answering. Together, these\nconstitute the AI2 Reasoning Challenge (ARC), which requires far more powerful\nknowledge and reasoning than previous challenges such as SQuAD or SNLI. The ARC\nquestion set is partitioned into a Challenge Set and an Easy Set, where the\nChallenge Set contains only questions answered incorrectly by both a\nretrieval-based algorithm and a word co-occurence algorithm. The dataset\ncontains only natural, grade-school science questions (authored for human\ntests), and is the largest public-domain set of this kind (7,787 questions). We\ntest several baselines on the Challenge Set, including leading neural models\nfrom the SQuAD and SNLI tasks, and find that none are able to significantly\noutperform a random baseline, reflecting the difficult nature of this task. We\nare also releasing the ARC Corpus, a corpus of 14M science sentences relevant\nto the task, and implementations of the three neural baseline models tested.\nCan your model perform better? We pose ARC as a challenge to the community.",
            "Generating plans of action, and reasoning about change have long been\nconsidered a core competence of intelligent agents. It is thus no surprise that\nevaluating the planning and reasoning capabilities of large language models\n(LLMs) has become a hot topic of research. Most claims about LLM planning\ncapabilities are however based on common sense tasks-where it becomes hard to\ntell whether LLMs are planning or merely retrieving from their vast world\nknowledge. There is a strong need for systematic and extensible planning\nbenchmarks with sufficient diversity to evaluate whether LLMs have innate\nplanning capabilities. Motivated by this, we propose PlanBench, an extensible\nbenchmark suite based on the kinds of domains used in the automated planning\ncommunity, especially in the International Planning Competition, to test the\ncapabilities of LLMs in planning or reasoning about actions and change.\nPlanBench provides sufficient diversity in both the task domains and the\nspecific planning capabilities. Our studies also show that on many critical\ncapabilities-including plan generation-LLM performance falls quite short, even\nwith the SOTA models. PlanBench can thus function as a useful marker of\nprogress of LLMs in planning and reasoning."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2412.19994",
        "description": "Chem LLM sft",
        "urls": [
            "https://arxiv.org/abs/2409.13194",
            "https://arxiv.org/abs/2402.09391",
            "https://arxiv.org/abs/2401.14818v1"
        ],
        "citations": [
            48,
            28,
            24
        ],
        "category": [
            "cs.CL",
            "cs.AI",
            "cs.CE",
            "cs.DL"
        ],
        "titles": [
            "ChemLLM: A Chemical Large Language Model",
            "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset",
            "ChemDFM: A Large Language Foundation Model for Chemistry"
        ],
        "id": 45,
        "abstracts": [
            "Rapid developments of AI tools are expected to offer unprecedented assistance\nto the research of natural science including chemistry. However, neither\nexisting unimodal task-specific specialist models nor emerging general large\nmultimodal models (LMM) can cover the wide range of chemical data modality and\ntask categories. To address the real demands of chemists, a cross-modal\nChemical General Intelligence (CGI) system, which serves as a truly practical\nand useful research assistant utilizing the great potential of LMMs, is in\ngreat need. In this work, we introduce the first Cross-modal Dialogue\nFoundation Model for Chemistry (ChemDFM-X). Diverse multimodal data are\ngenerated from an initial modality by approximate calculations and\ntask-specific model predictions. This strategy creates sufficient chemical\ntraining corpora, while significantly reducing excessive expense, resulting in\nan instruction-tuning dataset containing 7.6M data. After instruction\nfinetuning, ChemDFM-X is evaluated on extensive experiments of different\nchemical tasks with various data modalities. The results demonstrate the\ncapacity of ChemDFM-X for multimodal and inter-modal knowledge comprehension.\nChemDFM-X marks a significant milestone toward aligning all modalities in\nchemistry, a step closer to CGI.",
            "Chemistry plays a crucial role in many domains, such as drug discovery and\nmaterial science. While large language models (LLMs) such as GPT-4 exhibit\nremarkable capabilities on natural language processing tasks, existing research\nindicates that their performance on chemistry tasks is discouragingly low. In\nthis paper, however, we demonstrate that our developed LLMs can achieve very\nstrong results on a comprehensive set of chemistry tasks, outperforming the\nmost advanced GPT-4 and Claude 3 Opus by a substantial margin. To accomplish\nthis, we propose SMolInstruct, a large-scale, comprehensive, and high-quality\ndataset for instruction tuning. It contains 14 selected chemistry tasks and\nover three million samples, laying a solid foundation for training and\nevaluating LLMs for chemistry. Using SMolInstruct, we fine-tune a set of\nopen-source LLMs, among which, we find that Mistral serves as the best base\nmodel for chemistry tasks. Our analysis further demonstrates the critical role\nof the proposed dataset in driving the performance improvements.",
            "Artificial intelligence (AI) has played an increasingly important role in\nchemical research. However, most models currently used in chemistry are\nspecialist models that require training and tuning for specific tasks. A more\ngeneric and efficient solution would be an AI model that could address many\ntasks and support free-form dialogue in the broad field of chemistry. In its\nutmost form, such a generalist AI chemist could be referred to as Chemical\nGeneral Intelligence. Large language models (LLMs) have recently logged\ntremendous success in the general domain of natural language processing,\nshowing emerging task generalization and free-form dialogue capabilities.\nHowever, domain knowledge of chemistry is largely missing when training\ngeneral-domain LLMs. The lack of such knowledge greatly hinders the performance\nof generalist LLMs in the field of chemistry. To this end, we develop ChemDFM,\na pioneering LLM for chemistry trained on 34B tokens from chemical literature\nand textbooks, and fine-tuned using 2.7M instructions. As a result, it can\nunderstand and reason with chemical knowledge in free-form dialogue.\nQuantitative evaluations show that ChemDFM significantly surpasses most\nrepresentative open-source LLMs. It outperforms GPT-4 on a great portion of\nchemical tasks, despite the substantial size difference. We have open-sourced\nthe inference codes, evaluation datasets, and model weights of ChemDFM on\nHuggingface (https://huggingface.co/OpenDFM/ChemDFM-v1.0-13B)."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2412.19994",
        "description": "Chem LLM multimodal data",
        "urls": [
            "https://arxiv.org/abs/2401.13923",
            "https://arxiv.org/abs/2403.04197",
            "https://arxiv.org/abs/2406.06777"
        ],
        "citations": [
            33,
            6,
            4
        ],
        "category": [
            "cs.AI",
            "cs.LG",
            "cs.IR",
            "q-bio.BM",
            "cs.CL",
            "cs.CV"
        ],
        "titles": [
            "Towards 3D Molecule-Text Interpretation in Language Models",
            "Large Language Models are In-Context Molecule Learners",
            "MolX: Enhancing Large Language Models for Molecular Learning with A Multi-Modal Extension"
        ],
        "id": 46,
        "abstracts": [
            "Language Models (LMs) have greatly influenced diverse domains. However, their\ninherent limitation in comprehending 3D molecular structures has considerably\nconstrained their potential in the biomolecular domain. To bridge this gap, we\nfocus on 3D molecule-text interpretation, and propose 3D-MoLM: 3D-Molecular\nLanguage Modeling. Specifically, 3D-MoLM enables an LM to interpret and analyze\n3D molecules by equipping the LM with a 3D molecular encoder. This integration\nis achieved by a 3D molecule-text projector, bridging the 3D molecular\nencoder's representation space and the LM's input space. Moreover, to enhance\n3D-MoLM's ability of cross-modal molecular understanding and instruction\nfollowing, we meticulously curated a 3D molecule-centric instruction tuning\ndataset -- 3D-MoIT. Through 3D molecule-text alignment and 3D molecule-centric\ninstruction tuning, 3D-MoLM establishes an integration of 3D molecular encoder\nand LM. It significantly surpasses existing baselines on downstream tasks,\nincluding molecule-text retrieval, molecule captioning, and more challenging\nopen-text molecular QA tasks, especially focusing on 3D-dependent properties.\nWe release our codes and datasets at https://github.com/lsh0520/3D-MoLM.",
            "Large Language Models (LLMs) have demonstrated exceptional performance in\nbiochemical tasks, especially the molecule caption translation task, which aims\nto bridge the gap between molecules and natural language texts. However,\nprevious methods in adapting LLMs to the molecule-caption translation task\nrequired extra domain-specific pre-training stages, suffered weak alignment\nbetween molecular and textual spaces, or imposed stringent demands on the scale\nof LLMs. To resolve the challenges, we propose In-Context Molecule Adaptation\n(ICMA), as a new paradigm allowing LLMs to learn the molecule-text alignment\nfrom context examples via In-Context Molecule Tuning. Specifically, ICMA\nincorporates the following three stages: Hybrid Context Retrieval,\nPost-retrieval Re-ranking, and In-context Molecule Tuning. Initially, Hybrid\nContext Retrieval utilizes BM25 Caption Retrieval and Molecule Graph Retrieval\nto retrieve informative context examples. Additionally, we also propose\nPost-retrieval Re-ranking with Sequence Reversal and Random Walk to further\nimprove the quality of retrieval results. Finally, In-Context Molecule Tuning\nunlocks the in-context molecule learning capability of LLMs with retrieved\nexamples and adapts the parameters of LLMs for the molecule-caption translation\ntask. Experimental results demonstrate that ICMT can empower LLMs to achieve\nstate-of-the-art or comparable performance without extra training corpora and\nintricate structures, showing that LLMs are inherently in-context molecule\nlearners.",
            "Large Language Models (LLMs) with their strong task-handling capabilities\nhave shown remarkable advancements across a spectrum of fields, moving beyond\nnatural language understanding. However, their proficiency within the chemistry\ndomain remains restricted, especially in solving professional molecule-related\ntasks. This challenge is attributed to their inherent limitations in\ncomprehending molecules using only common textual representations, i.e., SMILES\nstrings. In this study, we seek to enhance the ability of LLMs to comprehend\nmolecules by equipping them with a multi-modal external module, namely MolX. In\nparticular, instead of directly using a SMILES string to represent a molecule,\nwe utilize specific encoders to extract fine-grained features from both SMILES\nstring and 2D molecular graph representations for feeding into an LLM.\nMoreover, a handcrafted molecular fingerprint is incorporated to leverage its\nembedded domain knowledge. Then, to establish an alignment between MolX and the\nLLM's textual input space, the whole model in which the LLM is frozen, is\npre-trained with a versatile strategy including a diverse set of tasks.\nExperimental evaluations show that our proposed method outperforms baselines\nacross 4 downstream molecule-related tasks ranging from molecule-to-text\ntranslation to retrosynthesis, with and without fine-tuning the LLM, while only\nintroducing a small number of trainable parameters 0.53% and 0.82%,\nrespectively."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2411.13577",
        "description": "Speech Representations Methods",
        "urls": [
            "https://arxiv.org/abs/2106.07447",
            "https://arxiv.org/abs/2201.10207",
            "https://arxiv.org/abs/2212.04356"
        ],
        "citations": [
            3003,
            24,
            3799
        ],
        "category": [
            "cs.CL",
            "cs.LG",
            "eess.AS",
            "cs.SD",
            "cs.AI"
        ],
        "titles": [
            "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
            "SPIRAL: Self-supervised Perturbation-Invariant Representation Learning for Speech Pre-Training",
            "Robust Speech Recognition via Large-Scale Weak Supervision"
        ],
        "id": 47,
        "abstracts": [
            "Self-supervised approaches for speech representation learning are challenged\nby three unique problems: (1) there are multiple sound units in each input\nutterance, (2) there is no lexicon of input sound units during the pre-training\nphase, and (3) sound units have variable lengths with no explicit segmentation.\nTo deal with these three problems, we propose the Hidden-Unit BERT (HuBERT)\napproach for self-supervised speech representation learning, which utilizes an\noffline clustering step to provide aligned target labels for a BERT-like\nprediction loss. A key ingredient of our approach is applying the prediction\nloss over the masked regions only, which forces the model to learn a combined\nacoustic and language model over the continuous inputs. HuBERT relies primarily\non the consistency of the unsupervised clustering step rather than the\nintrinsic quality of the assigned cluster labels. Starting with a simple\nk-means teacher of 100 clusters, and using two iterations of clustering, the\nHuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0\nperformance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with\n10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model,\nHuBERT shows up to 19% and 13% relative WER reduction on the more challenging\ndev-other and test-other evaluation subsets.",
            "We introduce a new approach for speech pre-training named SPIRAL which works\nby learning denoising representation of perturbed data in a teacher-student\nframework. Specifically, given a speech utterance, we first feed the utterance\nto a teacher network to obtain corresponding representation. Then the same\nutterance is perturbed and fed to a student network. The student network is\ntrained to output representation resembling that of the teacher. At the same\ntime, the teacher network is updated as moving average of student's weights\nover training steps. In order to prevent representation collapse, we apply an\nin-utterance contrastive loss as pre-training objective and impose position\nrandomization on the input to the teacher. SPIRAL achieves competitive or\nbetter results compared to state-of-the-art speech pre-training method wav2vec\n2.0, with significant reduction of training cost (80% for BASE model, 65% for\nLARGE model). Furthermore, we address the problem of noise-robustness that is\ncritical to real-world speech applications. We propose multi-condition\npre-training by perturbing the student's input with various types of additive\nnoise. We demonstrate that multi-condition pre-trained SPIRAL models are more\nrobust to noisy speech (9.0% - 13.3% relative word error rate reduction on real\nnoisy test data), compared to applying multi-condition training solely in the\nfine-tuning stage. Source code is available at\nhttps://github.com/huawei-noah/Speech-Backbones/tree/main/SPIRAL.",
            "We study the capabilities of speech processing systems trained simply to\npredict large amounts of transcripts of audio on the internet. When scaled to\n680,000 hours of multilingual and multitask supervision, the resulting models\ngeneralize well to standard benchmarks and are often competitive with prior\nfully supervised results but in a zero-shot transfer setting without the need\nfor any fine-tuning. When compared to humans, the models approach their\naccuracy and robustness. We are releasing models and inference code to serve as\na foundation for further work on robust speech processing."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2411.13577",
        "description": "Modal Alignment of Speech and Text",
        "urls": [
            "https://arxiv.org/abs/2406.12428",
            "https://arxiv.org/abs/2402.05755",
            "https://arxiv.org/abs/2305.11000"
        ],
        "citations": [
            5,
            27,
            247
        ],
        "category": [
            "cs.CL",
            "cs.SD",
            "eess.AS",
            "cs.AI",
            "cs.LG"
        ],
        "titles": [
            "PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems",
            "Spirit LM: Interleaved Spoken and Written Language Model",
            "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities"
        ],
        "id": 48,
        "abstracts": [
            "Multimodal language models that process both text and speech have a potential\nfor applications in spoken dialogue systems. However, current models face two\nmajor challenges in response generation latency: (1) generating a spoken\nresponse requires the prior generation of a written response, and (2) speech\nsequences are significantly longer than text sequences. This study addresses\nthese issues by extending the input and output sequences of the language model\nto support the parallel generation of text and speech. Our experiments on\nspoken question answering tasks demonstrate that our approach improves latency\nwhile maintaining the quality of response content. Additionally, we show that\nlatency can be further reduced by generating speech in multiple sequences. Demo\nsamples are available at https://rinnakk.github.io/research/publications/PSLM.",
            "We introduce Spirit LM, a foundation multimodal language model that freely\nmixes text and speech. Our model is based on a 7B pretrained text language\nmodel that we extend to the speech modality by continuously training it on text\nand speech units. Speech and text sequences are concatenated as a single stream\nof tokens, and trained with a word-level interleaving method using a small\nautomatically-curated speech-text parallel corpus. Spirit LM comes in two\nversions: a Base version that uses speech phonetic units (HuBERT) and an\nExpressive version that models expressivity using pitch and style units in\naddition to the phonetic units. For both versions, the text is encoded with\nsubword BPE tokens. The resulting model displays both the semantic abilities of\ntext models and the expressive abilities of speech models. Additionally, we\ndemonstrate that Spirit LM can learn new tasks in a few-shot fashion across\nmodalities (i.e. ASR, TTS, Speech Classification). We make available model\nweights and inference code.",
            "Multi-modal large language models are regarded as a crucial step towards\nArtificial General Intelligence (AGI) and have garnered significant interest\nwith the emergence of ChatGPT. However, current speech-language models\ntypically adopt the cascade paradigm, preventing inter-modal knowledge\ntransfer. In this paper, we propose SpeechGPT, a large language model with\nintrinsic cross-modal conversational abilities, capable of perceiving and\ngenerating multi-model content. With discrete speech representations, we first\nconstruct SpeechInstruct, a large-scale cross-modal speech instruction dataset.\nAdditionally, we employ a three-stage training strategy that includes\nmodality-adaptation pre-training, cross-modal instruction fine-tuning, and\nchain-of-modality instruction fine-tuning. The experimental results demonstrate\nthat SpeechGPT has an impressive capacity to follow multi-modal human\ninstructions and highlight the potential of handling multiple modalities with\none model. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2411.12747",
        "description": "Finance market price prediction",
        "urls": [
            "https://arxiv.org/abs/2409.15662",
            "https://arxiv.org/abs/2410.00288",
            "https://arxiv.org/abs/2312.15235"
        ],
        "citations": [
            1,
            1,
            19
        ],
        "category": [
            "cs.LG",
            "cs.AI",
            "q-fin.CP",
            "cs.CE"
        ],
        "titles": [
            "Double-Path Adaptive-correlation Spatial-Temporal Inverted Transformer for Stock Time Series Forecasting",
            "GARCH-Informed Neural Networks for Volatility Prediction in Financial Markets",
            "MASTER: Market-Guided Stock Transformer for Stock Price Forecasting"
        ],
        "id": 49,
        "abstracts": [
            "Spatial-temporal graph neural networks (STGNNs) have achieved significant\nsuccess in various time series forecasting tasks. However, due to the lack of\nexplicit and fixed spatial relationships in stock prediction tasks, many STGNNs\nfail to perform effectively in this domain. While some STGNNs learn spatial\nrelationships from time series, they often lack comprehensiveness. Research\nindicates that modeling time series using feature changes as tokens reveals\nentirely different information compared to using time steps as tokens. To more\ncomprehensively extract dynamic spatial information from stock data, we propose\na Double-Path Adaptive-correlation Spatial-Temporal Inverted Transformer\n(DPA-STIFormer). DPA-STIFormer models each node via continuous changes in\nfeatures as tokens and introduces a Double Direction Self-adaptation Fusion\nmechanism. This mechanism decomposes node encoding into temporal and feature\nrepresentations, simultaneously extracting different spatial correlations from\na double path approach, and proposes a Double-path gating mechanism to fuse\nthese two types of correlation information. Experiments conducted on four stock\nmarket datasets demonstrate state-of-the-art results, validating the model's\nsuperior capability in uncovering latent temporal-correlation patterns.",
            "Volatility, which indicates the dispersion of returns, is a crucial measure\nof risk and is hence used extensively for pricing and discriminating between\ndifferent financial investments. As a result, accurate volatility prediction\nreceives extensive attention. The Generalized Autoregressive Conditional\nHeteroscedasticity (GARCH) model and its succeeding variants are well\nestablished models for stock volatility forecasting. More recently, deep\nlearning models have gained popularity in volatility prediction as they\ndemonstrated promising accuracy in certain time series prediction tasks.\nInspired by Physics-Informed Neural Networks (PINN), we constructed a new,\nhybrid Deep Learning model that combines the strengths of GARCH with the\nflexibility of a Long Short-Term Memory (LSTM) Deep Neural Network (DNN), thus\ncapturing and forecasting market volatility more accurately than either class\nof models are capable of on their own. We refer to this novel model as a\nGARCH-Informed Neural Network (GINN). When compared to other time series\nmodels, GINN showed superior out-of-sample prediction performance in terms of\nthe Coefficient of Determination ($R^2$), Mean Squared Error (MSE), and Mean\nAbsolute Error (MAE).",
            "Stock price forecasting has remained an extremely challenging problem for\nmany decades due to the high volatility of the stock market. Recent efforts\nhave been devoted to modeling complex stock correlations toward joint stock\nprice forecasting. Existing works share a common neural architecture that\nlearns temporal patterns from individual stock series and then mixes up\ntemporal representations to establish stock correlations. However, they only\nconsider time-aligned stock correlations stemming from all the input stock\nfeatures, which suffer from two limitations. First, stock correlations often\noccur momentarily and in a cross-time manner. Second, the feature effectiveness\nis dynamic with market variation, which affects both the stock sequential\npatterns and their correlations. To address the limitations, this paper\nintroduces MASTER, a MArkert-Guided Stock TransformER, which models the\nmomentary and cross-time stock correlation and leverages market information for\nautomatic feature selection. MASTER elegantly tackles the complex stock\ncorrelation by alternatively engaging in intra-stock and inter-stock\ninformation aggregation. Experiments show the superiority of MASTER compared\nwith previous works and visualize the captured realistic stock correlation to\nprovide valuable insights."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2410.02713",
        "description": "Video finetune dataset",
        "urls": [
            "https://arxiv.org/abs/2410.02713",
            "https://arxiv.org/abs/2404.01258",
            "https://arxiv.org/abs/2406.04325"
        ],
        "citations": [
            36,
            4,
            87
        ],
        "category": [
            "cs.CV",
            "cs.CL",
            "cs.AI"
        ],
        "titles": [
            "Video Instruction Tuning With Synthetic Data",
            "Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward",
            "ShareGPT4Video: Improving Video Understanding and Generation with Better Captions"
        ],
        "id": 50,
        "abstracts": [
            "The development of video large multimodal models (LMMs) has been hindered by\nthe difficulty of curating large amounts of high-quality raw data from the web.\nTo address this, we propose an alternative approach by creating a high-quality\nsynthetic dataset specifically for video instruction-following, namely\nLLaVA-Video-178K. This dataset includes key tasks such as detailed captioning,\nopen-ended question-answering (QA), and multiple-choice QA. By training on this\ndataset, in combination with existing visual instruction tuning data, we\nintroduce LLaVA-Video, a new video LMM. Our experiments demonstrate that\nLLaVA-Video achieves strong performance across various video benchmarks,\nhighlighting the effectiveness of our dataset. We plan to release the dataset,\nits generation pipeline, and the model checkpoints.",
            "Preference modeling techniques, such as direct preference optimization (DPO),\nhas shown effective in enhancing the generalization abilities of large language\nmodel (LLM). However, in tasks involving video instruction-following, providing\ninformative feedback, especially for detecting hallucinations in generated\nresponses, remains a significant challenge. Previous studies have explored\nusing large large multimodal models (LMMs) as reward models to guide preference\nmodeling, but their ability to accurately assess the factuality of generated\nresponses compared to corresponding videos has not been conclusively\nestablished. This paper introduces a novel framework that utilizes detailed\nvideo captions as a proxy of video content, enabling language models to\nincorporate this information as supporting evidence for scoring video Question\nAnswering (QA) predictions. Our approach demonstrates robust alignment with\nOpenAI GPT-4V model's reward mechanism, which directly takes video frames as\ninput. Furthermore, we show that applying this tailored reward through DPO\nsignificantly improves the performance of video LMMs on video QA tasks.",
            "We present the ShareGPT4Video series, aiming to facilitate the video\nunderstanding of large video-language models (LVLMs) and the video generation\nof text-to-video models (T2VMs) via dense and precise captions. The series\ncomprises: 1) ShareGPT4Video, 40K GPT4V annotated dense captions of videos with\nvarious lengths and sources, developed through carefully designed data\nfiltering and annotating strategy. 2) ShareCaptioner-Video, an efficient and\ncapable captioning model for arbitrary videos, with 4.8M high-quality aesthetic\nvideos annotated by it. 3) ShareGPT4Video-8B, a simple yet superb LVLM that\nreached SOTA performance on three advancing video benchmarks. To achieve this,\ntaking aside the non-scalable costly human annotators, we find using GPT4V to\ncaption video with a naive multi-frame or frame-concatenation input strategy\nleads to less detailed and sometimes temporal-confused results. We argue the\nchallenge of designing a high-quality video captioning strategy lies in three\naspects: 1) Inter-frame precise temporal change understanding. 2) Intra-frame\ndetailed content description. 3) Frame-number scalability for arbitrary-length\nvideos. To this end, we meticulously designed a differential video captioning\nstrategy, which is stable, scalable, and efficient for generating captions for\nvideos with arbitrary resolution, aspect ratios, and length. Based on it, we\nconstruct ShareGPT4Video, which contains 40K high-quality videos spanning a\nwide range of categories, and the resulting captions encompass rich world\nknowledge, object attributes, camera movements, and crucially, detailed and\nprecise temporal descriptions of events. Based on ShareGPT4Video, we further\ndevelop ShareCaptioner-Video, a superior captioner capable of efficiently\ngenerating high-quality captions for arbitrary videos..."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2405.09557",
        "description": "Timeseries transformer",
        "urls": [
            "https://arxiv.org/abs/2106.13008",
            "https://arxiv.org/abs/2012.07436",
            "https://arxiv.org/abs/2001.04451"
        ],
        "citations": [
            2306,
            4649,
            3074
        ],
        "category": [
            "cs.LG",
            "cs.AI",
            "cs.IR",
            "cs.CL",
            "stat.ML"
        ],
        "titles": [
            "Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting",
            "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting",
            "Reformer: The Efficient Transformer"
        ],
        "id": 51,
        "abstracts": [
            "Extending the forecasting time is a critical demand for real applications,\nsuch as extreme weather early warning and long-term energy consumption\nplanning. This paper studies the long-term forecasting problem of time series.\nPrior Transformer-based models adopt various self-attention mechanisms to\ndiscover the long-range dependencies. However, intricate temporal patterns of\nthe long-term future prohibit the model from finding reliable dependencies.\nAlso, Transformers have to adopt the sparse versions of point-wise\nself-attentions for long series efficiency, resulting in the information\nutilization bottleneck. Going beyond Transformers, we design Autoformer as a\nnovel decomposition architecture with an Auto-Correlation mechanism. We break\nwith the pre-processing convention of series decomposition and renovate it as a\nbasic inner block of deep models. This design empowers Autoformer with\nprogressive decomposition capacities for complex time series. Further, inspired\nby the stochastic process theory, we design the Auto-Correlation mechanism\nbased on the series periodicity, which conducts the dependencies discovery and\nrepresentation aggregation at the sub-series level. Auto-Correlation\noutperforms self-attention in both efficiency and accuracy. In long-term\nforecasting, Autoformer yields state-of-the-art accuracy, with a 38% relative\nimprovement on six benchmarks, covering five practical applications: energy,\ntraffic, economics, weather and disease. Code is available at this repository:\n\\url{https://github.com/thuml/Autoformer}.",
            "Many real-world applications require the prediction of long sequence\ntime-series, such as electricity consumption planning. Long sequence\ntime-series forecasting (LSTF) demands a high prediction capacity of the model,\nwhich is the ability to capture precise long-range dependency coupling between\noutput and input efficiently. Recent studies have shown the potential of\nTransformer to increase the prediction capacity. However, there are several\nsevere issues with Transformer that prevent it from being directly applicable\nto LSTF, including quadratic time complexity, high memory usage, and inherent\nlimitation of the encoder-decoder architecture. To address these issues, we\ndesign an efficient transformer-based model for LSTF, named Informer, with\nthree distinctive characteristics: (i) a $ProbSparse$ self-attention mechanism,\nwhich achieves $O(L \\log L)$ in time complexity and memory usage, and has\ncomparable performance on sequences' dependency alignment. (ii) the\nself-attention distilling highlights dominating attention by halving cascading\nlayer input, and efficiently handles extreme long input sequences. (iii) the\ngenerative style decoder, while conceptually simple, predicts the long\ntime-series sequences at one forward operation rather than a step-by-step way,\nwhich drastically improves the inference speed of long-sequence predictions.\nExtensive experiments on four large-scale datasets demonstrate that Informer\nsignificantly outperforms existing methods and provides a new solution to the\nLSTF problem.",
            "Large Transformer models routinely achieve state-of-the-art results on a\nnumber of tasks but training these models can be prohibitively costly,\nespecially on long sequences. We introduce two techniques to improve the\nefficiency of Transformers. For one, we replace dot-product attention by one\nthat uses locality-sensitive hashing, changing its complexity from O($L^2$) to\nO($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use\nreversible residual layers instead of the standard residuals, which allows\nstoring activations only once in the training process instead of $N$ times,\nwhere $N$ is the number of layers. The resulting model, the Reformer, performs\non par with Transformer models while being much more memory-efficient and much\nfaster on long sequences."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2411.04281",
        "description": "电子健康表单合成",
        "urls": [
            "https://arxiv.org/abs/2304.02169",
            "https://arxiv.org/abs/2303.05656",
            "https://arxiv.org/abs/2402.04400"
        ],
        "category": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.CY"
        ],
        "citations": [
            32,
            19,
            4
        ],
        "titles": [
            "Synthesize High-dimensional Longitudinal Electronic Health Records via Hierarchical Autoregressive Language Model",
            "EHRDiff: Exploring Realistic EHR Synthesis with Diffusion Models",
            "CEHR-GPT: Generating Electronic Health Records with Chronological Patient Timelines"
        ],
        "id": 53,
        "abstracts": [
            "Synthetic electronic health records (EHRs) that are both realistic and\npreserve privacy can serve as an alternative to real EHRs for machine learning\n(ML) modeling and statistical analysis. However, generating high-fidelity and\ngranular electronic health record (EHR) data in its original,\nhighly-dimensional form poses challenges for existing methods due to the\ncomplexities inherent in high-dimensional data. In this paper, we propose\nHierarchical Autoregressive Language mOdel (HALO) for generating longitudinal\nhigh-dimensional EHR, which preserve the statistical properties of real EHR and\ncan be used to train accurate ML models without privacy concerns. Our HALO\nmethod, designed as a hierarchical autoregressive model, generates a\nprobability density function of medical codes, clinical visits, and patient\nrecords, allowing for the generation of realistic EHR data in its original,\nunaggregated form without the need for variable selection or aggregation.\nAdditionally, our model also produces high-quality continuous variables in a\nlongitudinal and probabilistic manner. We conducted extensive experiments and\ndemonstrate that HALO can generate high-fidelity EHR data with high-dimensional\ndisease code probabilities (d > 10,000), disease co-occurrence probabilities\nwithin visits (d > 1,000,000), and conditional probabilities across consecutive\nvisits (d > 5,000,000) and achieve above 0.9 R2 correlation in comparison to\nreal EHR data. This performance then enables downstream ML models trained on\nits synthetic data to achieve comparable accuracy to models trained on real\ndata (0.938 AUROC with HALO data vs. 0.943 with real data). Finally, using a\ncombination of real and synthetic data enhances the accuracy of ML models\nbeyond that achieved by using only real EHR data.",
            "Electronic health records (EHR) contain a wealth of biomedical information,\nserving as valuable resources for the development of precision medicine\nsystems. However, privacy concerns have resulted in limited access to\nhigh-quality and large-scale EHR data for researchers, impeding progress in\nmethodological development. Recent research has delved into synthesizing\nrealistic EHR data through generative modeling techniques, where a majority of\nproposed methods relied on generative adversarial networks (GAN) and their\nvariants for EHR synthesis. Despite GAN-based methods attaining\nstate-of-the-art performance in generating EHR data, these approaches are\ndifficult to train and prone to mode collapse. Recently introduced in\ngenerative modeling, diffusion models have established cutting-edge performance\nin image generation, but their efficacy in EHR data synthesis remains largely\nunexplored. In this study, we investigate the potential of diffusion models for\nEHR data synthesis and introduce a novel method, EHRDiff. Through extensive\nexperiments, EHRDiff establishes new state-of-the-art quality for synthetic EHR\ndata, protecting private information in the meanwhile.",
            "Synthetic Electronic Health Records (EHR) have emerged as a pivotal tool in\nadvancing healthcare applications and machine learning models, particularly for\nresearchers without direct access to healthcare data. Although existing\nmethods, like rule-based approaches and generative adversarial networks (GANs),\ngenerate synthetic data that resembles real-world EHR data, these methods often\nuse a tabular format, disregarding temporal dependencies in patient histories\nand limiting data replication. Recently, there has been a growing interest in\nleveraging Generative Pre-trained Transformers (GPT) for EHR data. This enables\napplications like disease progression analysis, population estimation,\ncounterfactual reasoning, and synthetic data generation. In this work, we focus\non synthetic data generation and demonstrate the capability of training a GPT\nmodel using a particular patient representation derived from CEHR-BERT,\nenabling us to generate patient sequences that can be seamlessly converted to\nthe Observational Medical Outcomes Partnership (OMOP) data format."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2407.07922",
        "description": "Vulnerability Detection in Ethereum Smart Contracts",
        "urls": [
            "https://arxiv.org/abs/2103.12607",
            "https://arxiv.org/abs/2107.11598",
            "https://arxiv.org/abs/2309.08474"
        ],
        "citations": [
            1,
            8,
            30
        ],
        "category": [
            "cs.LG",
            "cs.CL"
        ],
        "titles": [
            "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference",
            "Loki: Low-rank Keys for Efficient Sparse Attention",
            "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"
        ],
        "id": 57,
        "abstracts": [
            "Ethereum smart contracts are automated decentralized applications on the\nblockchain that describe the terms of the agreement between buyers and sellers,\nreducing the need for trusted intermediaries and arbitration. However, the\ndeployment of smart contracts introduces new attack vectors into the\ncryptocurrency systems. In particular, programming flaws in smart contracts can\nbe and have already been exploited to gain enormous financial profits. It is\nthus an emerging yet crucial issue to detect vulnerabilities of different\nclasses in contracts in an efficient manner. Existing machine learning-based\nvulnerability detection methods are limited and only inspect whether the smart\ncontract is vulnerable, or train individual classifiers for each specific\nvulnerability, or demonstrate multi-class vulnerability detection without\nextensibility consideration. To overcome the scalability and generalization\nlimitations of existing works, we propose ESCORT, the first Deep Neural Network\n(DNN)-based vulnerability detection framework for Ethereum smart contracts that\nsupport lightweight transfer learning on unseen security vulnerabilities, thus\nis extensible and generalizable. ESCORT leverages a multi-output NN\narchitecture that consists of two parts: (i) A common feature extractor that\nlearns the semantics of the input contract; (ii) Multiple branch structures\nwhere each branch learns a specific vulnerability type based on features\nobtained from the feature extractor. Experimental results show that ESCORT\nachieves an average F1-score of 95% on six vulnerability types and the\ndetection time is 0.02 seconds per contract. When extended to new vulnerability\ntypes, ESCORT yields an average F1-score of 93%. To the best of our knowledge,\nESCORT is the first framework that enables transfer learning on new\nvulnerability types with minimal modification of the DNN model architecture and\nre-training overhead.",
            "Smart contract vulnerability detection draws extensive attention in recent\nyears due to the substantial losses caused by hacker attacks. Existing efforts\nfor contract security analysis heavily rely on rigid rules defined by experts,\nwhich are labor-intensive and non-scalable. More importantly, expert-defined\nrules tend to be error-prone and suffer the inherent risk of being cheated by\ncrafty attackers. Recent researches focus on the symbolic execution and formal\nanalysis of smart contracts for vulnerability detection, yet to achieve a\nprecise and scalable solution. Although several methods have been proposed to\ndetect vulnerabilities in smart contracts, there is still a lack of effort that\nconsiders combining expert-defined security patterns with deep neural networks.\nIn this paper, we explore using graph neural networks and expert knowledge for\nsmart contract vulnerability detection. Specifically, we cast the rich control-\nand data- flow semantics of the source code into a contract graph. To highlight\nthe critical nodes in the graph, we further design a node elimination phase to\nnormalize the graph. Then, we propose a novel temporal message propagation\nnetwork to extract the graph feature from the normalized graph, and combine the\ngraph feature with designed expert patterns to yield a final detection system.\nExtensive experiments are conducted on all the smart contracts that have source\ncode in Ethereum and VNT Chain platforms. Empirical results show significant\naccuracy improvements over the state-of-the-art methods on three types of\nvulnerabilities, where the detection accuracy of our method reaches 89.15%,\n89.02%, and 83.21% for reentrancy, timestamp dependence, and infinite loop\nvulnerabilities, respectively.",
            "This paper presents VulnSense framework, a comprehensive approach to\nefficiently detect vulnerabilities in Ethereum smart contracts using a\nmultimodal learning approach on graph-based and natural language processing\n(NLP) models. Our proposed framework combines three types of features from\nsmart contracts comprising source code, opcode sequences, and control flow\ngraph (CFG) extracted from bytecode. We employ Bidirectional Encoder\nRepresentations from Transformers (BERT), Bidirectional Long Short-Term Memory\n(BiLSTM) and Graph Neural Network (GNN) models to extract and analyze these\nfeatures. The final layer of our multimodal approach consists of a fully\nconnected layer used to predict vulnerabilities in Ethereum smart contracts.\nAddressing limitations of existing vulnerability detection methods relying on\nsingle-feature or single-model deep learning techniques, our method surpasses\naccuracy and effectiveness constraints. We assess VulnSense using a collection\nof 1.769 smart contracts derived from the combination of three datasets:\nCurated, SolidiFI-Benchmark, and Smartbugs Wild. We then make a comparison with\nvarious unimodal and multimodal learning techniques contributed by GNN, BiLSTM\nand BERT architectures. The experimental outcomes demonstrate the superior\nperformance of our proposed approach, achieving an average accuracy of 77.96\\%\nacross all three categories of vulnerable smart contracts."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2407.06947",
        "description": "Acoustic dataset",
        "urls": [
            "https://arxiv.org/abs/2308.11923",
            "https://arxiv.org/abs/2211.06687",
            "https://arxiv.org/abs/2312.00249"
        ],
        "citations": [
            9,
            516,
            9
        ],
        "category": [
            "eess.AS",
            "cs.SD",
            "cs.CL",
            "cs.LG"
        ],
        "titles": [
            "Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement",
            "Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation",
            "Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities"
        ],
        "id": 58,
        "abstracts": [
            "We proposed Audio Difference Captioning (ADC) as a new extension task of\naudio captioning for describing the semantic differences between input pairs of\nsimilar but slightly different audio clips. The ADC solves the problem that\nconventional audio captioning sometimes generates similar captions for similar\naudio clips, failing to describe the difference in content. We also propose a\ncross-attention-concentrated transformer encoder to extract differences by\ncomparing a pair of audio clips and a similarity-discrepancy disentanglement to\nemphasize the difference in the latent space. To evaluate the proposed methods,\nwe built an AudioDiffCaps dataset consisting of pairs of similar but slightly\ndifferent audio clips with human-annotated descriptions of their differences.\nThe experiment with the AudioDiffCaps dataset showed that the proposed methods\nsolve the ADC task effectively and improve the attention weights to extract the\ndifference by visualizing them in the transformer encoder.",
            "Contrastive learning has shown remarkable success in the field of multimodal\nrepresentation learning. In this paper, we propose a pipeline of contrastive\nlanguage-audio pretraining to develop an audio representation by combining\naudio data with natural language descriptions. To accomplish this target, we\nfirst release LAION-Audio-630K, a large collection of 633,526 audio-text pairs\nfrom different data sources. Second, we construct a contrastive language-audio\npretraining model by considering different audio encoders and text encoders. We\nincorporate the feature fusion mechanism and keyword-to-caption augmentation\ninto the model design to further enable the model to process audio inputs of\nvariable lengths and enhance the performance. Third, we perform comprehensive\nexperiments to evaluate our model across three tasks: text-to-audio retrieval,\nzero-shot audio classification, and supervised audio classification. The\nresults demonstrate that our model achieves superior performance in\ntext-to-audio retrieval task. In audio classification tasks, the model achieves\nstate-of-the-art performance in the zero-shot setting and is able to obtain\nperformance comparable to models' results in the non-zero-shot setting.\nLAION-Audio-630K and the proposed model are both available to the public.",
            "The auditory system plays a substantial role in shaping the overall human\nperceptual experience. While prevailing large language models (LLMs) and visual\nlanguage models (VLMs) have shown their promise in solving a wide variety of\nvision and language understanding tasks, only a few of them can be generalised\nto the audio domain without compromising their domain-specific capacity. In\nthis work, we introduce Acoustic Prompt Turning (APT), a new adapter extending\nLLMs and VLMs to the audio domain by soft prompting only. Specifically, APT\napplies an instruction-aware audio aligner to generate soft prompts,\nconditioned on both input text and sounds, as language model inputs. To\nmitigate the data scarcity in the audio domain, a multi-task learning strategy\nis proposed by formulating diverse audio tasks in a sequence-to-sequence\nmanner. Moreover, we improve the framework of audio language model by using\ninterleaved audio-text embeddings as the input sequence. This improved\nframework imposes zero constraints on the input format and thus is capable of\ntackling more understanding tasks, such as few-shot audio classification and\naudio reasoning. To further evaluate the reasoning ability of audio networks,\nwe propose natural language audio reasoning (NLAR), a new task that analyses\nacross two audio clips by comparison and summarization. Experiments show that\nAPT-enhanced LLMs (namely APT-LLMs) achieve competitive results compared to the\nexpert models (i.e., the networks trained on the targeted datasets) across\nvarious tasks. We finally demonstrate the APT's ability in extending frozen\nVLMs to the audio domain without finetuning, achieving promising results in the\naudio-visual question and answering task. Our code and model weights are\nreleased at https://github.com/JinhuaLiang/APT."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2407.07111",
        "description": "Video Synthesis ",
        "urls": [
            "https://arxiv.org/abs/2308.09710",
            "https://arxiv.org/abs/2306.02018",
            "https://arxiv.org/abs/2304.08818"
        ],
        "citations": [
            60,
            285,
            920
        ],
        "category": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "titles": [
            "SimDA: Simple Diffusion Adapter for Efficient Video Generation",
            "VideoComposer: Compositional Video Synthesis with Motion Controllability",
            "Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models"
        ],
        "id": 59,
        "abstracts": [
            "The recent wave of AI-generated content has witnessed the great development\nand success of Text-to-Image (T2I) technologies. By contrast, Text-to-Video\n(T2V) still falls short of expectations though attracting increasing interests.\nExisting works either train from scratch or adapt large T2I model to videos,\nboth of which are computation and resource expensive. In this work, we propose\na Simple Diffusion Adapter (SimDA) that fine-tunes only 24M out of 1.1B\nparameters of a strong T2I model, adapting it to video generation in a\nparameter-efficient way. In particular, we turn the T2I model for T2V by\ndesigning light-weight spatial and temporal adapters for transfer learning.\nBesides, we change the original spatial attention to the proposed Latent-Shift\nAttention (LSA) for temporal consistency. With similar model architecture, we\nfurther train a video super-resolution model to generate high-definition\n(1024x1024) videos. In addition to T2V generation in the wild, SimDA could also\nbe utilized in one-shot video editing with only 2 minutes tuning. Doing so, our\nmethod could minimize the training effort with extremely few tunable parameters\nfor model adaptation.",
            "The pursuit of controllability as a higher standard of visual content\ncreation has yielded remarkable progress in customizable image synthesis.\nHowever, achieving controllable video synthesis remains challenging due to the\nlarge variation of temporal dynamics and the requirement of cross-frame\ntemporal consistency. Based on the paradigm of compositional generation, this\nwork presents VideoComposer that allows users to flexibly compose a video with\ntextual conditions, spatial conditions, and more importantly temporal\nconditions. Specifically, considering the characteristic of video data, we\nintroduce the motion vector from compressed videos as an explicit control\nsignal to provide guidance regarding temporal dynamics. In addition, we develop\na Spatio-Temporal Condition encoder (STC-encoder) that serves as a unified\ninterface to effectively incorporate the spatial and temporal relations of\nsequential inputs, with which the model could make better use of temporal\nconditions and hence achieve higher inter-frame consistency. Extensive\nexperimental results suggest that VideoComposer is able to control the spatial\nand temporal patterns simultaneously within a synthesized video in various\nforms, such as text description, sketch sequence, reference video, or even\nsimply hand-crafted motions. The code and models will be publicly available at\nhttps://videocomposer.github.io.",
            "Latent Diffusion Models (LDMs) enable high-quality image synthesis while\navoiding excessive compute demands by training a diffusion model in a\ncompressed lower-dimensional latent space. Here, we apply the LDM paradigm to\nhigh-resolution video generation, a particularly resource-intensive task. We\nfirst pre-train an LDM on images only; then, we turn the image generator into a\nvideo generator by introducing a temporal dimension to the latent space\ndiffusion model and fine-tuning on encoded image sequences, i.e., videos.\nSimilarly, we temporally align diffusion model upsamplers, turning them into\ntemporally consistent video super resolution models. We focus on two relevant\nreal-world applications: Simulation of in-the-wild driving data and creative\ncontent creation with text-to-video modeling. In particular, we validate our\nVideo LDM on real driving videos of resolution 512 x 1024, achieving\nstate-of-the-art performance. Furthermore, our approach can easily leverage\noff-the-shelf pre-trained image LDMs, as we only need to train a temporal\nalignment model in that case. Doing so, we turn the publicly available,\nstate-of-the-art text-to-image LDM Stable Diffusion into an efficient and\nexpressive text-to-video model with resolution up to 1280 x 2048. We show that\nthe temporal layers trained in this way generalize to different fine-tuned\ntext-to-image LDMs. Utilizing this property, we show the first results for\npersonalized text-to-video generation, opening exciting directions for future\ncontent creation. Project page:\nhttps://research.nvidia.com/labs/toronto-ai/VideoLDM/"
        ]
    },
    {
        "source": "https://arxiv.org/abs/2404.05567",
        "description": "DenseMoe",
        "urls": [
            "https://arxiv.org/abs/2404.05567",
            "https://arxiv.org/abs/2112.14397",
            "https://arxiv.org/abs/2404.13628"
        ],
        "citations": [
            13,
            31,
            48
        ],
        "category": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.MM"
        ],
        "titles": [
            "Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models",
            "EvoMoE: An Evolutional Mixture-of-Experts Training Framework via Dense-To-Sparse Gate",
            "Mixture of LoRA Experts"
        ],
        "id": 60,
        "abstracts": [
            "Mixture-of-Experts (MoE) language models can reduce computational costs by\n2-4$\\times$ compared to dense models without sacrificing performance, making\nthem more efficient in computation-bounded scenarios. However, MoE models\ngenerally require 2-4$\\times$ times more parameters to achieve comparable\nperformance to a dense model, which incurs larger GPU memory requirements and\nmakes MoE models less efficient in I/O-bounded scenarios like autoregressive\ngeneration. In this work, we propose a hybrid dense training and sparse\ninference framework for MoE models (DS-MoE) which achieves strong computation\nand parameter efficiency by employing dense computation across all experts\nduring training and sparse computation during inference. Our experiments on\ntraining LLMs demonstrate that our DS-MoE models are more parameter-efficient\nthan standard sparse MoEs and are on par with dense models in terms of total\nparameter size and performance while being computationally cheaper (activating\n30-40% of the model's parameters). Performance tests using vLLM show that our\nDS-MoE-6B model runs up to $1.86\\times$ faster than similar dense models like\nMistral-7B, and between $1.50\\times$ and $1.71\\times$ faster than comparable\nMoEs, such as DeepSeekMoE-16B and Qwen1.5-MoE-A2.7B.",
            "Mixture-of-experts (MoE) is becoming popular due to its success in improving\nthe model quality, especially in Transformers. By routing tokens with a sparse\ngate to a few experts (i.e., a small pieces of the full model), MoE can easily\nincrease the model parameters to a very large scale while keeping the\ncomputation cost in a constant level. Most existing works just initialize some\nrandom experts, set a fixed gating strategy (e.g., Top-k), and train the model\nfrom scratch in an ad-hoc way. We identify that these MoE models are suffering\nfrom the immature experts and unstable sparse gate, which are harmful to the\nconvergence performance. In this paper, we propose an efficient end-to-end MoE\ntraining framework called EvoMoE. EvoMoE starts from training one single expert\nand gradually evolves into a large and sparse MoE structure. EvoMoE mainly\ncontains two phases: the expert-diversify phase to train the base expert for a\nwhile and spawn multiple diverse experts from it, and the gate-sparsify phase\nto learn an adaptive sparse gate and activate a dynamic number of experts.\nEvoMoE naturally decouples the joint learning of both the experts and the\nsparse gate and focuses on learning the basic knowledge with a single expert at\nthe early training stage. Then it diversifies the experts and continues to\ntrain the MoE with a novel Dense-to-Sparse gate (DTS-Gate). Specifically,\ninstead of using a permanent sparse gate, DTS-Gate begins as a dense gate that\nroutes tokens to all experts, then gradually and adaptively becomes sparser\nwhile routes to fewer experts. Evaluations are conducted on three popular\nmodels and tasks, including RoBERTa for masked language modeling task, GPT for\nlanguage modeling task and Transformer for machine translation task. The\nresults show that EvoMoE outperforms existing baselines, including Switch, BASE\nLayer, Hash Layer and StableMoE.",
            "LoRA has gained widespread acceptance in the fine-tuning of large pre-trained\nmodels to cater to a diverse array of downstream tasks, showcasing notable\neffectiveness and efficiency, thereby solidifying its position as one of the\nmost prevalent fine-tuning techniques. Due to the modular nature of LoRA's\nplug-and-play plugins, researchers have delved into the amalgamation of\nmultiple LoRAs to empower models to excel across various downstream tasks.\nNonetheless, extant approaches for LoRA fusion grapple with inherent\nchallenges. Direct arithmetic merging may result in the loss of the original\npre-trained model's generative capabilities or the distinct identity of LoRAs,\nthereby yielding suboptimal outcomes. On the other hand, Reference tuning-based\nfusion exhibits limitations concerning the requisite flexibility for the\neffective combination of multiple LoRAs. In response to these challenges, this\npaper introduces the Mixture of LoRA Experts (MoLE) approach, which harnesses\nhierarchical control and unfettered branch selection. The MoLE approach not\nonly achieves superior LoRA fusion performance in comparison to direct\narithmetic merging but also retains the crucial flexibility for combining LoRAs\neffectively. Extensive experimental evaluations conducted in both the Natural\nLanguage Processing (NLP) and Vision & Language (V&L) domains substantiate the\nefficacy of MoLE."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2404.15676",
        "description": "Tool COT",
        "urls": [
            "https://arxiv.org/abs/2401.17464",
            "https://arxiv.org/abs/2305.14323",
            "https://arxiv.org/abs/2305.16896"
        ],
        "citations": [
            16,
            49,
            18
        ],
        "category": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "titles": [
            "Efficient Tool Use with Chain-of-Abstraction Reasoning",
            "ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models",
            "MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting"
        ],
        "id": 61,
        "abstracts": [
            "To achieve faithful reasoning that aligns with human expectations, large\nlanguage models (LLMs) need to ground their reasoning to real-world knowledge\n(e.g., web facts, math and physical rules). Tools help LLMs access this\nexternal knowledge, but there remains challenges for fine-tuning LLM agents\n(e.g., Toolformer) to invoke tools in multi-step reasoning problems, where\ninter-connected tool calls require holistic and efficient tool usage planning.\n  In this work, we propose a new method for LLMs to better leverage tools in\nmulti-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to\nfirst decode reasoning chains with abstract placeholders, and then call domain\ntools to reify each reasoning chain by filling in specific knowledge. This\nplanning with abstract chains enables LLMs to learn more general reasoning\nstrategies, which are robust to shifts of domain knowledge (e.g., math results)\nrelevant to different reasoning questions. It also allows LLMs to perform\ndecoding and calling of external tools in parallel, which avoids the inference\ndelay caused by waiting for tool responses. In mathematical reasoning and Wiki\nQA domains, we show that our method consistently outperforms previous\nchain-of-thought and tool-augmented baselines on both in-distribution and\nout-of-distribution test sets, with an average ~6% absolute QA accuracy\nimprovement. LLM agents trained with our method also show more efficient tool\nuse, with inference speed being on average ~1.4x faster than baseline\ntool-augmented LLMs.",
            "Although large language models (LLMs) have achieved excellent performance in\na variety of evaluation benchmarks, they still struggle in complex reasoning\ntasks which require specific knowledge and multi-hop reasoning. To improve the\nreasoning abilities, we propose ChatCoT, a tool-augmented chain-of-thought\nreasoning framework for chat-based LLMs (e.g., ChatGPT). In ChatCoT, we model\nthe chain-of-thought (CoT) reasoning as multi-turn conversations, to utilize\ntools in a more natural way through chatting. At each turn, LLMs can either\ninteract with tools or perform the reasoning. Our approach can effectively\nleverage the multi-turn conversation ability of chat-based LLMs, and integrate\nthe thought chain following and tools manipulation in a unified way. Specially,\nwe initialize the early turns of the conversation by the knowledge about tools,\ntasks, and reasoning format, and propose an iterative tool-augmented reasoning\nstep to perform step-by-step tool-augmented reasoning. The experiment results\non two complex reasoning datasets (MATH and HotpotQA) have shown the\neffectiveness of ChatCoT on complex reasoning tasks, achieving a 7.9% relative\nimprovement over the state-of-the-art baseline. Our code and data are available\nat: \\url{https://github.com/RUCAIBOX/ChatCoT}.",
            "Large language models (LLMs) have achieved impressive performance on various\nreasoning tasks. To further improve the performance, we propose MultiTool-CoT,\na novel framework that leverages chain-of-thought (CoT) prompting to\nincorporate multiple external tools, such as a calculator and a knowledge\nretriever, during the reasoning process. We apply MultiTool-CoT to the Task 2\ndataset of NumGLUE, which requires both numerical reasoning and domain-specific\nknowledge. The experiments show that our method significantly outperforms\nstrong baselines and achieves state-of-the-art performance."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2404.15593",
        "description": "Long tail learning",
        "urls": [
            "https://arxiv.org/abs/2108.02385",
            "https://arxiv.org/abs/2111.13998",
            "https://arxiv.org/abs/2312.01753"
        ],
        "citations": [
            175,
            222,
            1
        ],
        "category": [
            "cs.CV",
            "cs.LG"
        ],
        "titles": [
            "ACE: Ally Complementary Experts for Solving Long-Tailed Recognition in One-Shot",
            "Targeted Supervised Contrastive Learning for Long-Tailed Recognition",
            "Long-Tail Learning with Rebalanced Contrastive Loss"
        ],
        "id": 62,
        "abstracts": [
            "One-stage long-tailed recognition methods improve the overall performance in\na \"seesaw\" manner, i.e., either sacrifice the head's accuracy for better tail\nclassification or elevate the head's accuracy even higher but ignore the tail.\nExisting algorithms bypass such trade-off by a multi-stage training process:\npre-training on imbalanced set and fine-tuning on balanced set. Though\nachieving promising performance, not only are they sensitive to the\ngeneralizability of the pre-trained model, but also not easily integrated into\nother computer vision tasks like detection and segmentation, where pre-training\nof classifiers solely is not applicable. In this paper, we propose a one-stage\nlong-tailed recognition scheme, ally complementary experts (ACE), where the\nexpert is the most knowledgeable specialist in a sub-set that dominates its\ntraining, and is complementary to other experts in the less-seen categories\nwithout being disturbed by what it has never seen. We design a\ndistribution-adaptive optimizer to adjust the learning pace of each expert to\navoid over-fitting. Without special bells and whistles, the vanilla ACE\noutperforms the current one-stage SOTA method by 3-10% on CIFAR10-LT,\nCIFAR100-LT, ImageNet-LT and iNaturalist datasets. It is also shown to be the\nfirst one to break the \"seesaw\" trade-off by improving the accuracy of the\nmajority and minority categories simultaneously in only one stage. Code and\ntrained models are at https://github.com/jrcai/ACE.",
            "Real-world data often exhibits long tail distributions with heavy class\nimbalance, where the majority classes can dominate the training process and\nalter the decision boundaries of the minority classes. Recently, researchers\nhave investigated the potential of supervised contrastive learning for\nlong-tailed recognition, and demonstrated that it provides a strong performance\ngain. In this paper, we show that while supervised contrastive learning can\nhelp improve performance, past baselines suffer from poor uniformity brought in\nby imbalanced data distribution. This poor uniformity manifests in samples from\nthe minority class having poor separability in the feature space. To address\nthis problem, we propose targeted supervised contrastive learning (TSC), which\nimproves the uniformity of the feature distribution on the hypersphere. TSC\nfirst generates a set of targets uniformly distributed on a hypersphere. It\nthen makes the features of different classes converge to these distinct and\nuniformly distributed targets during training. This forces all classes,\nincluding minority classes, to maintain a uniform distribution in the feature\nspace, improves class boundaries, and provides better generalization even in\nthe presence of long-tail data. Experiments on multiple datasets show that TSC\nachieves state-of-the-art performance on long-tailed recognition tasks.",
            "Integrating supervised contrastive loss to cross entropy-based communication\nhas recently been proposed as a solution to address the long-tail learning\nproblem. However, when the class imbalance ratio is high, it requires adjusting\nthe supervised contrastive loss to support the tail classes, as the\nconventional contrastive learning is biased towards head classes by default. To\nthis end, we present Rebalanced Contrastive Learning (RCL), an efficient means\nto increase the long tail classification accuracy by addressing three main\naspects: 1. Feature space balancedness - Equal division of the feature space\namong all the classes, 2. Intra-Class compactness - Reducing the distance\nbetween same-class embeddings, 3. Regularization - Enforcing larger margins for\ntail classes to reduce overfitting. RCL adopts class frequency-based SoftMax\nloss balancing to supervised contrastive learning loss and exploits scalar\nmultiplied features fed to the contrastive learning loss to enforce\ncompactness. We implement RCL on the Balanced Contrastive Learning (BCL)\nFramework, which has the SOTA performance. Our experiments on three benchmark\ndatasets demonstrate the richness of the learnt embeddings and increased top-1\nbalanced accuracy RCL provides to the BCL framework. We further demonstrate\nthat the performance of RCL as a standalone loss also achieves state-of-the-art\nlevel accuracy."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2405.10439",
        "description": "Single Object Tracking",
        "urls": [
            "https://arxiv.org/abs/2304.14394",
            "https://arxiv.org/abs/2207.09603",
            "https://arxiv.org/abs/2203.11082"
        ],
        "citations": [
            228,
            282,
            626
        ],
        "category": [
            "cs.CV"
        ],
        "titles": [
            "Unified Sequence-to-Sequence Learning for Single- and Multi-Modal Visual Object Tracking",
            "AiATrack: Attention in Attention for Transformer Visual Tracking",
            "MixFormer: End-to-End Tracking with Iterative Mixed Attention"
        ],
        "id": 63,
        "abstracts": [
            "In this paper, we introduce a new sequence-to-sequence learning framework for\nRGB-based and multi-modal object tracking. First, we present SeqTrack for\nRGB-based tracking. It casts visual tracking as a sequence generation task,\nforecasting object bounding boxes in an autoregressive manner. This differs\nfrom previous trackers, which depend on the design of intricate head networks,\nsuch as classification and regression heads. SeqTrack employs a basic\nencoder-decoder transformer architecture. The encoder utilizes a bidirectional\ntransformer for feature extraction, while the decoder generates bounding box\nsequences autoregressively using a causal transformer. The loss function is a\nplain cross-entropy. Second, we introduce SeqTrackv2, a unified\nsequence-to-sequence framework for multi-modal tracking tasks. Expanding upon\nSeqTrack, SeqTrackv2 integrates a unified interface for auxiliary modalities\nand a set of task-prompt tokens to specify the task. This enables it to manage\nmulti-modal tracking tasks using a unified model and parameter set. This\nsequence learning paradigm not only simplifies the tracking framework, but also\nshowcases superior performance across 14 challenging benchmarks spanning five\nsingle- and multi-modal tracking tasks. The code and models are available at\nhttps://github.com/chenxin-dlut/SeqTrackv2.",
            "Transformer trackers have achieved impressive advancements recently, where\nthe attention mechanism plays an important role. However, the independent\ncorrelation computation in the attention mechanism could result in noisy and\nambiguous attention weights, which inhibits further performance improvement. To\naddress this issue, we propose an attention in attention (AiA) module, which\nenhances appropriate correlations and suppresses erroneous ones by seeking\nconsensus among all correlation vectors. Our AiA module can be readily applied\nto both self-attention blocks and cross-attention blocks to facilitate feature\naggregation and information propagation for visual tracking. Moreover, we\npropose a streamlined Transformer tracking framework, dubbed AiATrack, by\nintroducing efficient feature reuse and target-background embeddings to make\nfull use of temporal references. Experiments show that our tracker achieves\nstate-of-the-art performance on six tracking benchmarks while running at a\nreal-time speed.",
            "Tracking often uses a multi-stage pipeline of feature extraction, target\ninformation integration, and bounding box estimation. To simplify this pipeline\nand unify the process of feature extraction and target information integration,\nwe present a compact tracking framework, termed as MixFormer, built upon\ntransformers. Our core design is to utilize the flexibility of attention\noperations, and propose a Mixed Attention Module (MAM) for simultaneous feature\nextraction and target information integration. This synchronous modeling scheme\nallows to extract target-specific discriminative features and perform extensive\ncommunication between target and search area. Based on MAM, we build our\nMixFormer tracking framework simply by stacking multiple MAMs with progressive\npatch embedding and placing a localization head on top. In addition, to handle\nmultiple target templates during online tracking, we devise an asymmetric\nattention scheme in MAM to reduce computational cost, and propose an effective\nscore prediction module to select high-quality templates. Our MixFormer sets a\nnew state-of-the-art performance on five tracking benchmarks, including LaSOT,\nTrackingNet, VOT2020, GOT-10k, and UAV123. In particular, our MixFormer-L\nachieves NP score of 79.9% on LaSOT, 88.9% on TrackingNet and EAO of 0.555 on\nVOT2020. We also perform in-depth ablation studies to demonstrate the\neffectiveness of simultaneous feature extraction and information integration.\nCode and trained models are publicly available at\nhttps://github.com/MCG-NJU/MixFormer."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2405.09431",
        "description": "3D object dataset",
        "urls": [
            "https://arxiv.org/abs/2212.08051",
            "https://arxiv.org/abs/1512.03012",
            "https://arxiv.org/abs/2307.05663"
        ],
        "citations": [
            747,
            6222,
            290
        ],
        "category": [
            "cs.CV",
            "cs.AI",
            "cs.GR",
            "cs.RO",
            "cs.CG"
        ],
        "titles": [
            "Objaverse: A Universe of Annotated 3D Objects",
            "ShapeNet: An Information-Rich 3D Model Repository",
            "Objaverse-XL: A Universe of 10M+ 3D Objects"
        ],
        "id": 64,
        "abstracts": [
            "Massive data corpora like WebText, Wikipedia, Conceptual Captions,\nWebImageText, and LAION have propelled recent dramatic progress in AI. Large\nneural models trained on such datasets produce impressive results and top many\nof today's benchmarks. A notable omission within this family of large-scale\ndatasets is 3D data. Despite considerable interest and potential applications\nin 3D vision, datasets of high-fidelity 3D models continue to be mid-sized with\nlimited diversity of object categories. Addressing this gap, we present\nObjaverse 1.0, a large dataset of objects with 800K+ (and growing) 3D models\nwith descriptive captions, tags, and animations. Objaverse improves upon\npresent day 3D repositories in terms of scale, number of categories, and in the\nvisual diversity of instances within a category. We demonstrate the large\npotential of Objaverse via four diverse applications: training generative 3D\nmodels, improving tail category segmentation on the LVIS benchmark, training\nopen-vocabulary object-navigation models for Embodied AI, and creating a new\nbenchmark for robustness analysis of vision models. Objaverse can open new\ndirections for research and enable new applications across the field of AI.",
            "We present ShapeNet: a richly-annotated, large-scale repository of shapes\nrepresented by 3D CAD models of objects. ShapeNet contains 3D models from a\nmultitude of semantic categories and organizes them under the WordNet taxonomy.\nIt is a collection of datasets providing many semantic annotations for each 3D\nmodel such as consistent rigid alignments, parts and bilateral symmetry planes,\nphysical sizes, keywords, as well as other planned annotations. Annotations are\nmade available through a public web-based interface to enable data\nvisualization of object attributes, promote data-driven geometric analysis, and\nprovide a large-scale quantitative benchmark for research in computer graphics\nand vision. At the time of this technical report, ShapeNet has indexed more\nthan 3,000,000 models, 220,000 models out of which are classified into 3,135\ncategories (WordNet synsets). In this report we describe the ShapeNet effort as\na whole, provide details for all currently available datasets, and summarize\nfuture plans.",
            "Natural language processing and 2D vision models have attained remarkable\nproficiency on many tasks primarily by escalating the scale of training data.\nHowever, 3D vision tasks have not seen the same progress, in part due to the\nchallenges of acquiring high-quality 3D data. In this work, we present\nObjaverse-XL, a dataset of over 10 million 3D objects. Our dataset comprises\ndeduplicated 3D objects from a diverse set of sources, including manually\ndesigned objects, photogrammetry scans of landmarks and everyday items, and\nprofessional scans of historic and antique artifacts. Representing the largest\nscale and diversity in the realm of 3D datasets, Objaverse-XL enables\nsignificant new possibilities for 3D vision. Our experiments demonstrate the\nimprovements enabled with the scale provided by Objaverse-XL. We show that by\ntraining Zero123 on novel view synthesis, utilizing over 100 million multi-view\nrendered images, we achieve strong zero-shot generalization abilities. We hope\nthat releasing Objaverse-XL will enable further innovations in the field of 3D\nvision at scale."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2406.00515",
        "description": "LLM coder",
        "urls": [
            "https://arxiv.org/abs/2306.08568",
            "https://arxiv.org/abs/2306.11644",
            "https://arxiv.org/abs/2312.02120"
        ],
        "citations": [
            536,
            537,
            170
        ],
        "category": [
            "cs.CL",
            "cs.AI",
            "cs.LG",
            "cs.SE"
        ],
        "titles": [
            "WizardCoder: Empowering Code Large Language Models with Evol-Instruct",
            "Textbooks Are All You Need",
            "Magicoder: Empowering Code Generation with OSS-Instruct"
        ],
        "id": 65,
        "abstracts": [
            "Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated\nexceptional performance in code-related tasks. However, most existing models\nare solely pre-trained on extensive raw code data without instruction\nfine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs\nwith complex instruction fine-tuning, by adapting the Evol-Instruct method to\nthe domain of code. Through comprehensive experiments on four prominent code\ngeneration benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we\nunveil the exceptional capabilities of our model. It surpasses all other\nopen-source Code LLMs by a substantial margin. Moreover, our model even\noutperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on\nHumanEval and HumanEval+. Our code, model weights, and data are public at\nhttps://github.com/nlpxucan/WizardLM",
            "We introduce phi-1, a new large language model for code, with significantly\nsmaller size than competing models: phi-1 is a Transformer-based model with\n1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook\nquality\" data from the web (6B tokens) and synthetically generated textbooks\nand exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains\npass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays\nsurprising emergent properties compared to phi-1-base, our model before our\nfinetuning stage on a dataset of coding exercises, and phi-1-small, a smaller\nmodel with 350M parameters trained with the same pipeline as phi-1 that still\nachieves 45% on HumanEval.",
            "We introduce Magicoder, a series of fully open-source (code, weights, and\ndata) Large Language Models (LLMs) for code that significantly closes the gap\nwith top code models while having no more than 7B parameters. Magicoder models\nare trained on 75K synthetic instruction data using OSS-Instruct, a novel\napproach to enlightening LLMs with open-source code snippets to generate\ndiverse instruction data for code. Our main motivation is to mitigate the\ninherent bias of the synthetic data generated by LLMs through the wealth of\nopen-source references for the production of more realistic and controllable\ndata. The orthogonality of OSS-Instruct and other data generation methods like\nEvol-Instruct further enables us to build an enhanced MagicoderS. Both\nMagicoder and MagicoderS substantially outperform state-of-the-art code models\nwith similar or even larger sizes on a wide range of coding benchmarks.\nNotably, MagicoderS-CL-7B based on CodeLlama even surpasses the prominent\nChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1 ). Overall, OSS-Instruct opens a\nnew direction for crafting diverse synthetic instruction data for code using\nabundant open-source references."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2406.12252",
        "description": "Language Sport QA dataset",
        "urls": [
            "https://arxiv.org/abs/2110.05750",
            "https://arxiv.org/abs/2111.12535",
            "https://arxiv.org/abs/2010.00526"
        ],
        "citations": [
            11,
            20,
            12
        ],
        "category": [
            "cs.CL",
            "cs.AI"
        ],
        "titles": [
            "SportsSum2.0: Generating High-Quality Sports News from Live Text Commentary",
            "Knowledge Enhanced Sports Game Summarization",
            "LiveQA: A Question Answering Dataset over Sports Live"
        ],
        "id": 66,
        "abstracts": [
            "Sports game summarization aims to generate news articles from live text\ncommentaries. A recent state-of-the-art work, SportsSum, not only constructs a\nlarge benchmark dataset, but also proposes a two-step framework. Despite its\ngreat contributions, the work has three main drawbacks: 1) the noise existed in\nSportsSum dataset degrades the summarization performance; 2) the neglect of\nlexical overlap between news and commentaries results in low-quality\npseudo-labeling algorithm; 3) the usage of directly concatenating rewritten\nsentences to form news limits its practicability. In this paper, we publish a\nnew benchmark dataset SportsSum2.0, together with a modified summarization\nframework. In particular, to obtain a clean dataset, we employ crowd workers to\nmanually clean the original dataset. Moreover, the degree of lexical overlap is\nincorporated into the generation of pseudo labels. Further, we introduce a\nreranker-enhanced summarizer to take into account the fluency and\nexpressiveness of the summarized news. Extensive experiments show that our\nmodel outperforms the state-of-the-art baseline.",
            "Sports game summarization aims at generating sports news from live\ncommentaries. However, existing datasets are all constructed through automated\ncollection and cleaning processes, resulting in a lot of noise. Besides,\ncurrent works neglect the knowledge gap between live commentaries and sports\nnews, which limits the performance of sports game summarization. In this paper,\nwe introduce K-SportsSum, a new dataset with two characteristics: (1)\nK-SportsSum collects a large amount of data from massive games. It has 7,854\ncommentary-news pairs. To improve the quality, K-SportsSum employs a manual\ncleaning process; (2) Different from existing datasets, to narrow the knowledge\ngap, K-SportsSum further provides a large-scale knowledge corpus that contains\nthe information of 523 sports teams and 14,724 sports players. Additionally, we\nalso introduce a knowledge-enhanced summarizer that utilizes both live\ncommentaries and the knowledge to generate sports news. Extensive experiments\non K-SportsSum and SportsSum datasets show that our model achieves new\nstate-of-the-art performances. Qualitative analysis and human study further\nverify that our model generates more informative sports news.",
            "In this paper, we introduce LiveQA, a new question answering dataset\nconstructed from play-by-play live broadcast. It contains 117k multiple-choice\nquestions written by human commentators for over 1,670 NBA games, which are\ncollected from the Chinese Hupu (https://nba.hupu.com/games) website. Derived\nfrom the characteristics of sports games, LiveQA can potentially test the\nreasoning ability across timeline-based live broadcasts, which is challenging\ncompared to the existing datasets. In LiveQA, the questions require\nunderstanding the timeline, tracking events or doing mathematical computations.\nOur preliminary experiments show that the dataset introduces a challenging\nproblem for question answering models, and a strong baseline model only\nachieves the accuracy of 53.1\\% and cannot beat the dominant option rule. We\nrelease the code and data of this paper for future research."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2406.12252",
        "description": "multimodal Sport QA dataset",
        "urls": [
            "https://arxiv.org/abs/2404.06332",
            "https://arxiv.org/abs/2108.11149",
            "https://arxiv.org/abs/2401.01505"
        ],
        "citations": [
            15,
            22,
            10
        ],
        "category": [
            "cs.CV"
        ],
        "titles": [
            "X-VARS: Introducing Explainability in Football Refereeing with Multi-Modal Large Language Model",
            "A Unified Taxonomy and Multimodal Dataset for Events in Invasion Games",
            "Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex and Professional Sports"
        ],
        "id": 67,
        "abstracts": [
            "The rapid advancement of artificial intelligence has led to significant\nimprovements in automated decision-making. However, the increased performance\nof models often comes at the cost of explainability and transparency of their\ndecision-making processes. In this paper, we investigate the capabilities of\nlarge language models to explain decisions, using football refereeing as a\ntesting ground, given its decision complexity and subjectivity. We introduce\nthe Explainable Video Assistant Referee System, X-VARS, a multi-modal large\nlanguage model designed for understanding football videos from the point of\nview of a referee. X-VARS can perform a multitude of tasks, including video\ndescription, question answering, action recognition, and conducting meaningful\nconversations based on video content and in accordance with the Laws of the\nGame for football referees. We validate X-VARS on our novel dataset,\nSoccerNet-XFoul, which consists of more than 22k video-question-answer triplets\nannotated by over 70 experienced football referees. Our experiments and human\nstudy illustrate the impressive capabilities of X-VARS in interpreting complex\nfootball clips. Furthermore, we highlight the potential of X-VARS to reach\nhuman performance and support football referees in the future.",
            "The automatic detection of events in complex sports games like soccer and\nhandball using positional or video data is of large interest in research and\nindustry. One requirement is a fundamental understanding of underlying\nconcepts, i.e., events that occur on the pitch. Previous work often deals only\nwith so-called low-level events based on well-defined rules such as free kicks,\nfree throws, or goals. High-level events, such as passes, are less frequently\napproached due to a lack of consistent definitions. This introduces a level of\nambiguity that necessities careful validation when regarding event annotations.\nYet, this validation step is usually neglected as the majority of studies adopt\nannotations from commercial providers on private datasets of unknown quality\nand focuses on soccer only. To address these issues, we present (1) a universal\ntaxonomy that covers a wide range of low and high-level events for invasion\ngames and is exemplarily refined to soccer and handball, and (2) release two\nmulti-modal datasets comprising video and positional data with gold-standard\nannotations to foster research in fine-grained and ball-centered event\nspotting. Experiments on human performance demonstrate the robustness of the\nproposed taxonomy, and that disagreements and ambiguities in the annotation\nincrease with the complexity of the event. An I3D model for video\nclassification is adopted for event spotting and reveals the potential for\nbenchmarking. Datasets are available at: https://github.com/mm4spa/eigd",
            "Reasoning over sports videos for question answering is an important task with\nnumerous applications, such as player training and information retrieval.\nHowever, this task has not been explored due to the lack of relevant datasets\nand the challenging nature it presents. Most datasets for video question\nanswering (VideoQA) focus mainly on general and coarse-grained understanding of\ndaily-life videos, which is not applicable to sports scenarios requiring\nprofessional action understanding and fine-grained motion analysis. In this\npaper, we introduce the first dataset, named Sports-QA, specifically designed\nfor the sports VideoQA task. The Sports-QA dataset includes various types of\nquestions, such as descriptions, chronologies, causalities, and counterfactual\nconditions, covering multiple sports. Furthermore, to address the\ncharacteristics of the sports VideoQA task, we propose a new Auto-Focus\nTransformer (AFT) capable of automatically focusing on particular scales of\ntemporal information for question answering. We conduct extensive experiments\non Sports-QA, including baseline studies and the evaluation of different\nmethods. The results demonstrate that our AFT achieves state-of-the-art\nperformance."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2406.15126",
        "description": "LLM synthetic data using task specification",
        "urls": [
            "https://arxiv.org/abs/2310.07849",
            "https://arxiv.org/abs/2311.00287",
            "https://arxiv.org/abs/2403.01081"
        ],
        "citations": [
            117,
            24,
            22
        ],
        "category": [
            "cs.CL",
            "cs.AI",
            "cs.LG",
            "q-bio.QM"
        ],
        "titles": [
            "Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations",
            "Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models",
            "LAB: Large-Scale Alignment for ChatBots"
        ],
        "id": 68,
        "abstracts": [
            "The collection and curation of high-quality training data is crucial for\ndeveloping text classification models with superior performance, but it is\noften associated with significant costs and time investment. Researchers have\nrecently explored using large language models (LLMs) to generate synthetic\ndatasets as an alternative approach. However, the effectiveness of the\nLLM-generated synthetic data in supporting model training is inconsistent\nacross different classification tasks. To better understand factors that\nmoderate the effectiveness of the LLM-generated synthetic data, in this study,\nwe look into how the performance of models trained on these synthetic data may\nvary with the subjectivity of classification. Our results indicate that\nsubjectivity, at both the task level and instance level, is negatively\nassociated with the performance of the model trained on synthetic data. We\nconclude by discussing the implications of our work on the potential and\nlimitations of leveraging LLM for synthetic data generation.",
            "Clinical natural language processing requires methods that can address\ndomain-specific challenges, such as complex medical terminology and clinical\ncontexts. Recently, large language models (LLMs) have shown promise in this\ndomain. Yet, their direct deployment can lead to privacy issues and are\nconstrained by resources. To address this challenge, we delve into synthetic\nclinical text generation using LLMs for clinical NLP tasks. We propose an\ninnovative, resource-efficient approach, ClinGen, which infuses knowledge into\nthe process. Our model involves clinical knowledge extraction and\ncontext-informed LLM prompting. Both clinical topics and writing styles are\ndrawn from external domain-specific knowledge graphs and LLMs to guide data\ngeneration. Our extensive empirical study across 7 clinical NLP tasks and 16\ndatasets reveals that ClinGen consistently enhances performance across various\ntasks, effectively aligning the distribution of real datasets and significantly\nenriching the diversity of generated training instances. Our code is available\nat \\url{https://github.com/ritaranx/ClinGen}.",
            "This work introduces LAB (Large-scale Alignment for chatBots), a novel\nmethodology designed to overcome the scalability challenges in the\ninstruction-tuning phase of large language model (LLM) training. Leveraging a\ntaxonomy-guided synthetic data generation process and a multi-phase tuning\nframework, LAB significantly reduces reliance on expensive human annotations\nand proprietary models like GPT-4. We demonstrate that LAB-trained models can\nachieve competitive performance across several benchmarks compared to models\ntrained with traditional human-annotated or GPT-4 generated synthetic data.\nThus offering a scalable, cost-effective solution for enhancing LLM\ncapabilities and instruction-following behaviors without the drawbacks of\ncatastrophic forgetting, marking a step forward in the efficient training of\nLLMs for a wide range of applications."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2406.06852",
        "description": "Full finetune backdoor attack",
        "urls": [
            "https://arxiv.org/abs/2304.14475",
            "https://arxiv.org/abs/2403.13355",
            "https://arxiv.org/abs/2402.11208"
        ],
        "citations": [
            40,
            43,
            26
        ],
        "category": [
            "cs.CR",
            "cs.AI",
            "cs.LG",
            "cs.CL"
        ],
        "titles": [
            "ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger",
            "BadEdit: Backdooring large language models by model editing",
            "Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents"
        ],
        "id": 69,
        "abstracts": [
            "Textual backdoor attacks pose a practical threat to existing systems, as they\ncan compromise the model by inserting imperceptible triggers into inputs and\nmanipulating labels in the training dataset. With cutting-edge generative\nmodels such as GPT-4 pushing rewriting to extraordinary levels, such attacks\nare becoming even harder to detect. We conduct a comprehensive investigation of\nthe role of black-box generative models as a backdoor attack tool, highlighting\nthe importance of researching relative defense strategies. In this paper, we\nreveal that the proposed generative model-based attack, BGMAttack, could\neffectively deceive textual classifiers. Compared with the traditional attack\nmethods, BGMAttack makes the backdoor trigger less conspicuous by leveraging\nstate-of-the-art generative models. Our extensive evaluation of attack\neffectiveness across five datasets, complemented by three distinct human\ncognition assessments, reveals that Figure 4 achieves comparable attack\nperformance while maintaining superior stealthiness relative to baseline\nmethods.",
            "Mainstream backdoor attack methods typically demand substantial tuning data\nfor poisoning, limiting their practicality and potentially degrading the\noverall performance when applied to Large Language Models (LLMs). To address\nthese issues, for the first time, we formulate backdoor injection as a\nlightweight knowledge editing problem, and introduce the BadEdit attack\nframework. BadEdit directly alters LLM parameters to incorporate backdoors with\nan efficient editing technique. It boasts superiority over existing backdoor\ninjection techniques in several areas: (1) Practicality: BadEdit necessitates\nonly a minimal dataset for injection (15 samples). (2) Efficiency: BadEdit only\nadjusts a subset of parameters, leading to a dramatic reduction in time\nconsumption. (3) Minimal side effects: BadEdit ensures that the model's\noverarching performance remains uncompromised. (4) Robustness: the backdoor\nremains robust even after subsequent fine-tuning or instruction-tuning.\nExperimental results demonstrate that our BadEdit framework can efficiently\nattack pre-trained LLMs with up to 100\\% success rate while maintaining the\nmodel's performance on benign inputs.",
            "Driven by the rapid development of Large Language Models (LLMs), LLM-based\nagents have been developed to handle various real-world applications, including\nfinance, healthcare, and shopping, etc. It is crucial to ensure the reliability\nand security of LLM-based agents during applications. However, the safety\nissues of LLM-based agents are currently under-explored. In this work, we take\nthe first step to investigate one of the typical safety threats, backdoor\nattack, to LLM-based agents. We first formulate a general framework of agent\nbackdoor attacks, then we present a thorough analysis of different forms of\nagent backdoor attacks. Specifically, compared with traditional backdoor\nattacks on LLMs that are only able to manipulate the user inputs and model\noutputs, agent backdoor attacks exhibit more diverse and covert forms: (1) From\nthe perspective of the final attacking outcomes, the agent backdoor attacker\ncan not only choose to manipulate the final output distribution, but also\nintroduce the malicious behavior in an intermediate reasoning step only, while\nkeeping the final output correct. (2) Furthermore, the former category can be\ndivided into two subcategories based on trigger locations, in which the\nbackdoor trigger can either be hidden in the user query or appear in an\nintermediate observation returned by the external environment. We implement the\nabove variations of agent backdoor attacks on two typical agent tasks including\nweb shopping and tool utilization. Extensive experiments show that LLM-based\nagents suffer severely from backdoor attacks and such backdoor vulnerability\ncannot be easily mitigated by current textual backdoor defense algorithms. This\nindicates an urgent need for further research on the development of targeted\ndefenses against backdoor attacks on LLM-based agents. Warning: This paper may\ncontain biased content."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2404.16886",
        "description": "Time series data augmentation",
        "urls": [
            "https://arxiv.org/abs/1902.01955",
            "https://arxiv.org/abs/1904.08779",
            "https://arxiv.org/abs/1810.01398"
        ],
        "citations": [
            54,
            4408,
            52
        ],
        "category": [
            "cs.CL",
            "cs.LG",
            "stat.ML",
            "eess.AS",
            "cs.SD",
            "cs.AI"
        ],
        "titles": [
            "On the Choice of Modeling Unit for Sequence-to-Sequence Speech Recognition",
            "SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition",
            "Optimal Completion Distillation for Sequence Learning"
        ],
        "id": 70,
        "abstracts": [
            "In conventional speech recognition, phoneme-based models outperform\ngrapheme-based models for non-phonetic languages such as English. The\nperformance gap between the two typically reduces as the amount of training\ndata is increased. In this work, we examine the impact of the choice of\nmodeling unit for attention-based encoder-decoder models. We conduct\nexperiments on the LibriSpeech 100hr, 460hr, and 960hr tasks, using various\ntarget units (phoneme, grapheme, and word-piece); across all tasks, we find\nthat grapheme or word-piece models consistently outperform phoneme-based\nmodels, even though they are evaluated without a lexicon or an external\nlanguage model. We also investigate model complementarity: we find that we can\nimprove WERs by up to 9% relative by rescoring N-best lists generated from a\nstrong word-piece based baseline with either the phoneme or the grapheme model.\nRescoring an N-best list generated by the phonemic system, however, provides\nlimited improvements. Further analysis shows that the word-piece-based models\nproduce more diverse N-best hypotheses, and thus lower oracle WERs, than\nphonemic models.",
            "We present SpecAugment, a simple data augmentation method for speech\nrecognition. SpecAugment is applied directly to the feature inputs of a neural\nnetwork (i.e., filter bank coefficients). The augmentation policy consists of\nwarping the features, masking blocks of frequency channels, and masking blocks\nof time steps. We apply SpecAugment on Listen, Attend and Spell networks for\nend-to-end speech recognition tasks. We achieve state-of-the-art performance on\nthe LibriSpeech 960h and Swichboard 300h tasks, outperforming all prior work.\nOn LibriSpeech, we achieve 6.8% WER on test-other without the use of a language\nmodel, and 5.8% WER with shallow fusion with a language model. This compares to\nthe previous state-of-the-art hybrid system of 7.5% WER. For Switchboard, we\nachieve 7.2%/14.6% on the Switchboard/CallHome portion of the Hub5'00 test set\nwithout the use of a language model, and 6.8%/14.1% with shallow fusion, which\ncompares to the previous state-of-the-art hybrid system at 8.3%/17.3% WER.",
            "We present Optimal Completion Distillation (OCD), a training procedure for\noptimizing sequence to sequence models based on edit distance. OCD is\nefficient, has no hyper-parameters of its own, and does not require pretraining\nor joint optimization with conditional log-likelihood. Given a partial sequence\ngenerated by the model, we first identify the set of optimal suffixes that\nminimize the total edit distance, using an efficient dynamic programming\nalgorithm. Then, for each position of the generated sequence, we use a target\ndistribution that puts equal probability on the first token of all the optimal\nsuffixes. OCD achieves the state-of-the-art performance on end-to-end speech\nrecognition, on both Wall Street Journal and Librispeech datasets, achieving\n$9.3\\%$ WER and $4.5\\%$ WER respectively."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2405.01466",
        "description": "Automatic program repair",
        "urls": [
            "https://arxiv.org/abs/1811.04211",
            "https://arxiv.org/abs/2106.08253",
            "https://arxiv.org/abs/2105.04123"
        ],
        "citations": [
            519,
            230,
            178
        ],
        "category": [
            "cs.SE",
            "cs.AI"
        ],
        "titles": [
            "Nopol: Automatic Repair of Conditional Statement Bugs in Java Programs",
            "A Syntax-Guided Edit Decoder for Neural Program Repair",
            "Neural Program Repair with Execution-based Backpropagation"
        ],
        "id": 71,
        "abstracts": [
            "We propose NOPOL, an approach to automatic repair of buggy conditional\nstatements (i.e., if-then-else statements). This approach takes a buggy program\nas well as a test suite as input and generates a patch with a conditional\nexpression as output. The test suite is required to contain passing test cases\nto model the expected behavior of the program and at least one failing test\ncase that reveals the bug to be repaired. The process of NOPOL consists of\nthree major phases. First, NOPOL employs angelic fix localization to identify\nexpected values of a condition during the test execution. Second, runtime trace\ncollection is used to collect variables and their actual values, including\nprimitive data types and objected-oriented features (e.g., nullness checks), to\nserve as building blocks for patch generation. Third, NOPOL encodes these\ncollected data into an instance of a Satisfiability Modulo Theory (SMT)\nproblem, then a feasible solution to the SMT instance is translated back into a\ncode patch. We evaluate NOPOL on 22 real-world bugs (16 bugs with buggy IF\nconditions and 6 bugs with missing preconditions) on two large open-source\nprojects, namely Apache Commons Math and Apache Commons Lang. Empirical\nanalysis on these bugs shows that our approach can effectively fix bugs with\nbuggy IF conditions and missing preconditions. We illustrate the capabilities\nand limitations of NOPOL using case studies of real bug fixes.",
            "Automated Program Repair (APR) helps improve the efficiency of software\ndevelopment and maintenance. Recent APR techniques use deep learning,\nparticularly the encoder-decoder architecture, to generate patches. Though\nexisting DL-based APR approaches have proposed different encoder architectures,\nthe decoder remains to be the standard one, which generates a sequence of\ntokens one by one to replace the faulty statement. This decoder has multiple\nlimitations: 1) allowing to generate syntactically incorrect programs, 2)\ninefficiently representing small edits, and 3) not being able to generate\nproject-specific identifiers.\n  In this paper, we propose Recoder, a syntax-guided edit decoder with\nplaceholder generation. Recoder is novel in multiple aspects: 1) Recoder\ngenerates edits rather than modified code, allowing efficient representation of\nsmall edits; 2) Recoder is syntax-guided, with the novel provider/decider\narchitecture to ensure the syntactic correctness of the patched program and\naccurate generation; 3) Recoder generates placeholders that could be\ninstantiated as project-specific identifiers later.\n  We conduct experiments to evaluate Recoder on 395 bugs from Defects4J v1.2\nand 420 additional bugs from Defects4J v2.0. Our results show that Recoder\nrepairs 53 bugs on Defects4J v1.2, which achieves 21.4% improvement over the\nprevious state-of-the-art approach for single-hunk bugs (TBar). Importantly, to\nour knowledge, Recoder is the first DL-based APR approach that has outperformed\nthe traditional APR approaches on this dataset. Furthermore, Recoder also\nrepairs 19 bugs on the additional bugs from Defects4J v2.0, which is 137.5%\nmore than TBar (8 bugs) and 850% more than SimFix (2 bugs). This result\nsuggests that Recoder has better generalizability than existing APR approaches.",
            "Neural machine translation (NMT) architectures have achieved promising\nresults for automatic program repair. Yet, they have the limitation of\ngenerating low-quality patches (e.g., not compilable patches). This is because\nthe existing works only optimize a purely syntactic loss function based on\ncharacters and tokens without incorporating program-specific information during\nneural network weight optimization. In this paper, we propose a novel program\nrepair model called RewardRepair. The core novelty of RewardRepair is to\nimprove NMT-based program repair with a loss function based on program\ncompilation and test execution information, rewarding the network to produce\npatches that compile and that do not overfit. We conduct several experiments to\nevaluate RewardRepair showing that it is feasible and effective to use\ncompilation and test execution results to optimize the underlying neural repair\nmodel. RewardRepair correctly repairs 207 bugs over four benchmarks. we report\non repair success for 121 bugs that are fixed for the first time in the\nliterature. Also, RewardRepair produces up to 45.3% of compilable patches, an\nimprovement over the 39% by the state-of-the-art."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2405.11868",
        "description": "Graph contrastive learning",
        "urls": [
            "https://arxiv.org/abs/2109.10259",
            "https://arxiv.org/abs/2010.13902",
            "https://arxiv.org/abs/2106.05819"
        ],
        "citations": [
            152,
            2364,
            376
        ],
        "category": [
            "cs.LG",
            "cs.AI"
        ],
        "titles": [
            "AutoGCL: Automated Graph Contrastive Learning via Learnable View Generators",
            "Graph Contrastive Learning with Augmentations",
            "Adversarial Graph Augmentation to Improve Graph Contrastive Learning"
        ],
        "id": 72,
        "abstracts": [
            "Contrastive learning has been widely applied to graph representation\nlearning, where the view generators play a vital role in generating effective\ncontrastive samples. Most of the existing contrastive learning methods employ\npre-defined view generation methods, e.g., node drop or edge perturbation,\nwhich usually cannot adapt to input data or preserve the original semantic\nstructures well. To address this issue, we propose a novel framework named\nAutomated Graph Contrastive Learning (AutoGCL) in this paper. Specifically,\nAutoGCL employs a set of learnable graph view generators orchestrated by an\nauto augmentation strategy, where every graph view generator learns a\nprobability distribution of graphs conditioned by the input. While the graph\nview generators in AutoGCL preserve the most representative structures of the\noriginal graph in generation of every contrastive sample, the auto augmentation\nlearns policies to introduce adequate augmentation variances in the whole\ncontrastive learning procedure. Furthermore, AutoGCL adopts a joint training\nstrategy to train the learnable view generators, the graph encoder, and the\nclassifier in an end-to-end manner, resulting in topological heterogeneity yet\nsemantic similarity in the generation of contrastive samples. Extensive\nexperiments on semi-supervised learning, unsupervised learning, and transfer\nlearning demonstrate the superiority of our AutoGCL framework over the\nstate-of-the-arts in graph contrastive learning. In addition, the visualization\nresults further confirm that the learnable view generators can deliver more\ncompact and semantically meaningful contrastive samples compared against the\nexisting view generation methods.",
            "Generalizable, transferrable, and robust representation learning on\ngraph-structured data remains a challenge for current graph neural networks\n(GNNs). Unlike what has been developed for convolutional neural networks (CNNs)\nfor image data, self-supervised learning and pre-training are less explored for\nGNNs. In this paper, we propose a graph contrastive learning (GraphCL)\nframework for learning unsupervised representations of graph data. We first\ndesign four types of graph augmentations to incorporate various priors. We then\nsystematically study the impact of various combinations of graph augmentations\non multiple datasets, in four different settings: semi-supervised,\nunsupervised, and transfer learning as well as adversarial attacks. The results\nshow that, even without tuning augmentation extents nor using sophisticated GNN\narchitectures, our GraphCL framework can produce graph representations of\nsimilar or better generalizability, transferrability, and robustness compared\nto state-of-the-art methods. We also investigate the impact of parameterized\ngraph augmentation extents and patterns, and observe further performance gains\nin preliminary experiments. Our codes are available at\nhttps://github.com/Shen-Lab/GraphCL.",
            "Self-supervised learning of graph neural networks (GNN) is in great need\nbecause of the widespread label scarcity issue in real-world graph/network\ndata. Graph contrastive learning (GCL), by training GNNs to maximize the\ncorrespondence between the representations of the same graph in its different\naugmented forms, may yield robust and transferable GNNs even without using\nlabels. However, GNNs trained by traditional GCL often risk capturing redundant\ngraph features and thus may be brittle and provide sub-par performance in\ndownstream tasks. Here, we propose a novel principle, termed adversarial-GCL\n(AD-GCL), which enables GNNs to avoid capturing redundant information during\nthe training by optimizing adversarial graph augmentation strategies used in\nGCL. We pair AD-GCL with theoretical explanations and design a practical\ninstantiation based on trainable edge-dropping graph augmentation. We\nexperimentally validate AD-GCL by comparing with the state-of-the-art GCL\nmethods and achieve performance gains of up-to $14\\%$ in unsupervised, $6\\%$ in\ntransfer, and $3\\%$ in semi-supervised learning settings overall with 18\ndifferent benchmark datasets for the tasks of molecule property regression and\nclassification, and social network classification."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2312.03718",
        "description": "LAW LLM",
        "urls": [
            "https://arxiv.org/abs/2407.21065",
            "https://arxiv.org/abs/2406.04614",
            "https://arxiv.org/abs/2403.03883"
        ],
        "citations": [
            2,
            23,
            51
        ],
        "category": [
            "cs.CL",
            "cs.IR",
            "cs.LG",
            "cs.AI"
        ],
        "titles": [
            "LawLLM: Law Large Language Model for the US Legal System",
            "LawGPT: A Chinese Legal Knowledge-Enhanced Large Language Model",
            "SaulLM-7B: A pioneering Large Language Model for Law"
        ],
        "id": 73,
        "abstracts": [
            "In the rapidly evolving field of legal analytics, finding relevant cases and\naccurately predicting judicial outcomes are challenging because of the\ncomplexity of legal language, which often includes specialized terminology,\ncomplex syntax, and historical context. Moreover, the subtle distinctions\nbetween similar and precedent cases require a deep understanding of legal\nknowledge. Researchers often conflate these concepts, making it difficult to\ndevelop specialized techniques to effectively address these nuanced tasks. In\nthis paper, we introduce the Law Large Language Model (LawLLM), a multi-task\nmodel specifically designed for the US legal domain to address these\nchallenges. LawLLM excels at Similar Case Retrieval (SCR), Precedent Case\nRecommendation (PCR), and Legal Judgment Prediction (LJP). By clearly\ndistinguishing between precedent and similar cases, we provide essential\nclarity, guiding future research in developing specialized strategies for these\ntasks. We propose customized data preprocessing techniques for each task that\ntransform raw legal data into a trainable format. Furthermore, we also use\ntechniques such as in-context learning (ICL) and advanced information retrieval\nmethods in LawLLM. The evaluation results demonstrate that LawLLM consistently\noutperforms existing baselines in both zero-shot and few-shot scenarios,\noffering unparalleled multi-task capabilities and filling critical gaps in the\nlegal domain.",
            "Large language models (LLMs), including both proprietary and open-source\nmodels, have showcased remarkable capabilities in addressing a wide range of\ndownstream tasks. Nonetheless, when it comes to practical Chinese legal tasks,\nthese models fail to meet the actual requirements. Proprietary models do not\nensure data privacy for sensitive legal cases, while open-source models\ndemonstrate unsatisfactory performance due to their lack of legal knowledge. To\naddress this problem, we introduce LawGPT, the first open-source model\nspecifically designed for Chinese legal applications. LawGPT comprises two key\ncomponents: legal-oriented pre-training and legal supervised fine-tuning.\nSpecifically, we employ large-scale Chinese legal documents for legal-oriented\npre-training to incorporate legal domain knowledge. To further improve the\nmodel's performance on downstream legal tasks, we create a knowledge-driven\ninstruction dataset for legal supervised fine-tuning. Our experimental results\ndemonstrate that LawGPT outperforms the open-source LLaMA 7B model. Our code\nand resources are publicly available at https://github.com/pengxiao-song/LaWGPT\nand have received 5.7K stars on GitHub.",
            "In this paper, we introduce SaulLM-7B, a large language model (LLM) tailored\nfor the legal domain. With 7 billion parameters, SaulLM-7B is the first LLM\ndesigned explicitly for legal text comprehension and generation. Leveraging the\nMistral 7B architecture as its foundation, SaulLM-7B is trained on an English\nlegal corpus of over 30 billion tokens. SaulLM-7B exhibits state-of-the-art\nproficiency in understanding and processing legal documents. Additionally, we\npresent a novel instructional fine-tuning method that leverages legal datasets\nto further enhance SaulLM-7B's performance in legal tasks. SaulLM-7B is\nreleased under the MIT License."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2409.20288",
        "description": "LAW benchmark",
        "urls": [
            "https://arxiv.org/abs/2309.16289",
            "https://arxiv.org/abs/2409.20288",
            "https://arxiv.org/abs/2310.05620"
        ],
        "citations": [
            72,
            5,
            25
        ],
        "category": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "titles": [
            "LawBench: Benchmarking Legal Knowledge of Large Language Models",
            "LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models",
            "LAiW: A Chinese Legal Large Language Models Benchmark"
        ],
        "id": 74,
        "abstracts": [
            "Large language models (LLMs) have demonstrated strong capabilities in various\naspects. However, when applying them to the highly specialized, safe-critical\nlegal domain, it is unclear how much legal knowledge they possess and whether\nthey can reliably perform legal-related tasks. To address this gap, we propose\na comprehensive evaluation benchmark LawBench. LawBench has been meticulously\ncrafted to have precise assessment of the LLMs' legal capabilities from three\ncognitive levels: (1) Legal knowledge memorization: whether LLMs can memorize\nneeded legal concepts, articles and facts; (2) Legal knowledge understanding:\nwhether LLMs can comprehend entities, events and relationships within legal\ntext; (3) Legal knowledge applying: whether LLMs can properly utilize their\nlegal knowledge and make necessary reasoning steps to solve realistic legal\ntasks. LawBench contains 20 diverse tasks covering 5 task types: single-label\nclassification (SLC), multi-label classification (MLC), regression, extraction\nand generation. We perform extensive evaluations of 51 LLMs on LawBench,\nincluding 20 multilingual LLMs, 22 Chinese-oriented LLMs and 9 legal specific\nLLMs. The results show that GPT-4 remains the best-performing LLM in the legal\ndomain, surpassing the others by a significant margin. While fine-tuning LLMs\non legal specific text brings certain improvements, we are still a long way\nfrom obtaining usable and reliable LLMs in legal tasks. All data, model\npredictions and evaluation code are released in\nhttps://github.com/open-compass/LawBench/. We hope this benchmark provides\nin-depth understanding of the LLMs' domain-specified capabilities and speed up\nthe development of LLMs in the legal domain.",
            "Large language models (LLMs) have made significant progress in natural\nlanguage processing tasks and demonstrate considerable potential in the legal\ndomain. However, legal applications demand high standards of accuracy,\nreliability, and fairness. Applying existing LLMs to legal systems without\ncareful evaluation of their potential and limitations could pose significant\nrisks in legal practice. To this end, we introduce a standardized comprehensive\nChinese legal benchmark LexEval. This benchmark is notable in the following\nthree aspects: (1) Ability Modeling: We propose a new taxonomy of legal\ncognitive abilities to organize different tasks. (2) Scale: To our knowledge,\nLexEval is currently the largest Chinese legal evaluation dataset, comprising\n23 tasks and 14,150 questions. (3) Data: we utilize formatted existing\ndatasets, exam datasets and newly annotated datasets by legal experts to\ncomprehensively evaluate the various capabilities of LLMs. LexEval not only\nfocuses on the ability of LLMs to apply fundamental legal knowledge but also\ndedicates efforts to examining the ethical issues involved in their\napplication. We evaluated 38 open-source and commercial LLMs and obtained some\ninteresting findings. The experiments and findings offer valuable insights into\nthe challenges and potential solutions for developing Chinese legal systems and\nLLM evaluation pipelines. The LexEval dataset and leaderboard are publicly\navailable at \\url{https://github.com/CSHaitao/LexEval} and will be continuously\nupdated.",
            "General and legal domain LLMs have demonstrated strong performance in various\ntasks of LegalAI. However, the current evaluations of these LLMs in LegalAI are\ndefined by the experts of computer science, lacking consistency with the logic\nof legal practice, making it difficult to judge their practical capabilities.\nTo address this challenge, we are the first to build the Chinese legal LLMs\nbenchmark LAiW, based on the logic of legal practice. To align with the\nthinking process of legal experts and legal practice (syllogism), we divide the\nlegal capabilities of LLMs from easy to difficult into three levels: basic\ninformation retrieval, legal foundation inference, and complex legal\napplication. Each level contains multiple tasks to ensure a comprehensive\nevaluation. Through automated evaluation of current general and legal domain\nLLMs on our benchmark, we indicate that these LLMs may not align with the logic\nof legal practice. LLMs seem to be able to directly acquire complex legal\napplication capabilities but perform poorly in some basic tasks, which may pose\nobstacles to their practical application and acceptance by legal experts. To\nfurther confirm the complex legal application capabilities of current LLMs in\nlegal application scenarios, we also incorporate human evaluation with legal\nexperts. The results indicate that while LLMs may demonstrate strong\nperformance, they still require reinforcement of legal logic."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2312.08890",
        "description": "Backdoor defense",
        "urls": [
            "https://arxiv.org/abs/2202.03423",
            "https://arxiv.org/abs/2303.06818",
            "https://arxiv.org/abs/2110.11571"
        ],
        "citations": [
            207,
            33,
            343
        ],
        "category": [
            "cs.LG",
            "cs.CR",
            "cs.AI",
            "cs.CV"
        ],
        "titles": [
            "Backdoor Defense via Decoupling the Training Process",
            "Backdoor Defense via Deconfounded Representation Learning",
            "Anti-Backdoor Learning: Training Clean Models on Poisoned Data"
        ],
        "id": 75,
        "abstracts": [
            "Recent studies have revealed that deep neural networks (DNNs) are vulnerable\nto backdoor attacks, where attackers embed hidden backdoors in the DNN model by\npoisoning a few training samples. The attacked model behaves normally on benign\nsamples, whereas its prediction will be maliciously changed when the backdoor\nis activated. We reveal that poisoned samples tend to cluster together in the\nfeature space of the attacked DNN model, which is mostly due to the end-to-end\nsupervised training paradigm. Inspired by this observation, we propose a novel\nbackdoor defense via decoupling the original end-to-end training process into\nthree stages. Specifically, we first learn the backbone of a DNN model via\n\\emph{self-supervised learning} based on training samples without their labels.\nThe learned backbone will map samples with the same ground-truth label to\nsimilar locations in the feature space. Then, we freeze the parameters of the\nlearned backbone and train the remaining fully connected layers via standard\ntraining with all (labeled) training samples. Lastly, to further alleviate\nside-effects of poisoned samples in the second stage, we remove labels of some\n`low-credible' samples determined based on the learned model and conduct a\n\\emph{semi-supervised fine-tuning} of the whole model. Extensive experiments on\nmultiple benchmark datasets and DNN models verify that the proposed defense is\neffective in reducing backdoor threats while preserving high accuracy in\npredicting benign samples. Our code is available at\n\\url{https://github.com/SCLBD/DBD}.",
            "Deep neural networks (DNNs) are recently shown to be vulnerable to backdoor\nattacks, where attackers embed hidden backdoors in the DNN model by injecting a\nfew poisoned examples into the training dataset. While extensive efforts have\nbeen made to detect and remove backdoors from backdoored DNNs, it is still not\nclear whether a backdoor-free clean model can be directly obtained from\npoisoned datasets. In this paper, we first construct a causal graph to model\nthe generation process of poisoned data and find that the backdoor attack acts\nas the confounder, which brings spurious associations between the input images\nand target labels, making the model predictions less reliable. Inspired by the\ncausal understanding, we propose the Causality-inspired Backdoor Defense (CBD),\nto learn deconfounded representations for reliable classification.\nSpecifically, a backdoored model is intentionally trained to capture the\nconfounding effects. The other clean model dedicates to capturing the desired\ncausal effects by minimizing the mutual information with the confounding\nrepresentations from the backdoored model and employing a sample-wise\nre-weighting scheme. Extensive experiments on multiple benchmark datasets\nagainst 6 state-of-the-art attacks verify that our proposed defense method is\neffective in reducing backdoor threats while maintaining high accuracy in\npredicting benign samples. Further analysis shows that CBD can also resist\npotential adaptive attacks. The code is available at\n\\url{https://github.com/zaixizhang/CBD}.",
            "Backdoor attack has emerged as a major security threat to deep neural\nnetworks (DNNs). While existing defense methods have demonstrated promising\nresults on detecting or erasing backdoors, it is still not clear whether robust\ntraining methods can be devised to prevent the backdoor triggers being injected\ninto the trained model in the first place. In this paper, we introduce the\nconcept of \\emph{anti-backdoor learning}, aiming to train \\emph{clean} models\ngiven backdoor-poisoned data. We frame the overall learning process as a\ndual-task of learning the \\emph{clean} and the \\emph{backdoor} portions of\ndata. From this view, we identify two inherent characteristics of backdoor\nattacks as their weaknesses: 1) the models learn backdoored data much faster\nthan learning with clean data, and the stronger the attack the faster the model\nconverges on backdoored data; 2) the backdoor task is tied to a specific class\n(the backdoor target class). Based on these two weaknesses, we propose a\ngeneral learning scheme, Anti-Backdoor Learning (ABL), to automatically prevent\nbackdoor attacks during training. ABL introduces a two-stage \\emph{gradient\nascent} mechanism for standard training to 1) help isolate backdoor examples at\nan early training stage, and 2) break the correlation between backdoor examples\nand the target class at a later training stage. Through extensive experiments\non multiple benchmark datasets against 10 state-of-the-art attacks, we\nempirically show that ABL-trained models on backdoor-poisoned data achieve the\nsame performance as they were trained on purely clean data. Code is available\nat \\url{https://github.com/bboylyg/ABL}."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2312.17617",
        "description": "NER model",
        "urls": [
            "https://arxiv.org/abs/2403.07969",
            "https://arxiv.org/abs/2101.05779",
            "https://arxiv.org/abs/2210.10343"
        ],
        "citations": [
            14,
            323,
            14
        ],
        "category": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "titles": [
            "KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction",
            "Structured Prediction as Translation between Augmented Natural Languages",
            "Entity-to-Text based Data Augmentation for various Named Entity Recognition Tasks"
        ],
        "id": 76,
        "abstracts": [
            "In this paper, we propose KnowCoder, a Large Language Model (LLM) to conduct\nUniversal Information Extraction (UIE) via code generation. KnowCoder aims to\ndevelop a kind of unified schema representation that LLMs can easily understand\nand an effective learning framework that encourages LLMs to follow schemas and\nextract structured knowledge accurately. To achieve these, KnowCoder introduces\na code-style schema representation method to uniformly transform different\nschemas into Python classes, with which complex schema information, such as\nconstraints among tasks in UIE, can be captured in an LLM-friendly manner. We\nfurther construct a code-style schema library covering over $\\textbf{30,000}$\ntypes of knowledge, which is the largest one for UIE, to the best of our\nknowledge. To ease the learning process of LLMs, KnowCoder contains a two-phase\nlearning framework that enhances its schema understanding ability via code\npretraining and its schema following ability via instruction tuning. After code\npretraining on around $1.5$B automatically constructed data, KnowCoder already\nattains remarkable generalization ability and achieves relative improvements by\n$\\textbf{49.8%}$ F1, compared to LLaMA2, under the few-shot setting. After\ninstruction tuning, KnowCoder further exhibits strong generalization ability on\nunseen schemas and achieves up to $\\textbf{12.5%}$ and $\\textbf{21.9%}$,\ncompared to sota baselines, under the zero-shot setting and the low resource\nsetting, respectively. Additionally, based on our unified schema\nrepresentations, various human-annotated datasets can simultaneously be\nutilized to refine KnowCoder, which achieves significant improvements up to\n$\\textbf{7.5%}$ under the supervised setting.",
            "We propose a new framework, Translation between Augmented Natural Languages\n(TANL), to solve many structured prediction language tasks including joint\nentity and relation extraction, nested named entity recognition, relation\nclassification, semantic role labeling, event extraction, coreference\nresolution, and dialogue state tracking. Instead of tackling the problem by\ntraining task-specific discriminative classifiers, we frame it as a translation\ntask between augmented natural languages, from which the task-relevant\ninformation can be easily extracted. Our approach can match or outperform\ntask-specific models on all tasks, and in particular, achieves new\nstate-of-the-art results on joint entity and relation extraction (CoNLL04, ADE,\nNYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and\nsemantic role labeling (CoNLL-2005 and CoNLL-2012). We accomplish this while\nusing the same architecture and hyperparameters for all tasks and even when\ntraining a single model to solve all tasks at the same time (multi-task\nlearning). Finally, we show that our framework can also significantly improve\nthe performance in a low-resource regime, thanks to better use of label\nsemantics.",
            "Data augmentation techniques have been used to alleviate the problem of\nscarce labeled data in various NER tasks (flat, nested, and discontinuous NER\ntasks). Existing augmentation techniques either manipulate the words in the\noriginal text that break the semantic coherence of the text, or exploit\ngenerative models that ignore preserving entities in the original text, which\nimpedes the use of augmentation techniques on nested and discontinuous NER\ntasks. In this work, we propose a novel Entity-to-Text based data augmentation\ntechnique named EnTDA to add, delete, replace or swap entities in the entity\nlist of the original texts, and adopt these augmented entity lists to generate\nsemantically coherent and entity preserving texts for various NER tasks.\nFurthermore, we introduce a diversity beam search to increase the diversity\nduring the text generation process. Experiments on thirteen NER datasets across\nthree tasks (flat, nested, and discontinuous NER tasks) and two settings (full\ndata and low resource settings) show that EnTDA could bring more performance\nimprovements compared to the baseline augmentation techniques."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2312.17617",
        "description": "Event Extraction Model",
        "urls": [
            "https://arxiv.org/abs/2104.05919",
            "https://arxiv.org/abs/2307.05567",
            "https://arxiv.org/abs/2202.12109"
        ],
        "citations": [
            298,
            35,
            143
        ],
        "category": [
            "cs.CL",
            "cs.AI"
        ],
        "titles": [
            "Document-Level Event Argument Extraction by Conditional Generation",
            "Event Extraction as Question Generation and Answering",
            "Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction"
        ],
        "id": 77,
        "abstracts": [
            "Event extraction has long been treated as a sentence-level task in the IE\ncommunity. We argue that this setting does not match human information-seeking\nbehavior and leads to incomplete and uninformative extraction results. We\npropose a document-level neural event argument extraction model by formulating\nthe task as conditional generation following event templates. We also compile a\nnew document-level event extraction benchmark dataset WikiEvents which includes\ncomplete event and coreference annotation. On the task of argument extraction,\nwe achieve an absolute gain of 7.6% F1 and 5.7% F1 over the next best model on\nthe RAMS and WikiEvents datasets respectively. On the more challenging task of\ninformative argument extraction, which requires implicit coreference reasoning,\nwe achieve a 9.3% F1 gain over the best baseline. To demonstrate the\nportability of our model, we also create the first end-to-end zero-shot event\nextraction framework and achieve 97% of fully supervised model's trigger\nextraction performance and 82% of the argument extraction performance given\nonly access to 10 out of the 33 types on ACE.",
            "Recent work on Event Extraction has reframed the task as Question Answering\n(QA), with promising results. The advantage of this approach is that it\naddresses the error propagation issue found in traditional token-based\nclassification approaches by directly predicting event arguments without\nextracting candidates first. However, the questions are typically based on\nfixed templates and they rarely leverage contextual information such as\nrelevant arguments. In addition, prior QA-based approaches have difficulty\nhandling cases where there are multiple arguments for the same role. In this\npaper, we propose QGA-EE, which enables a Question Generation (QG) model to\ngenerate questions that incorporate rich contextual information instead of\nusing fixed templates. We also propose dynamic templates to assist the training\nof QG model. Experiments show that QGA-EE outperforms all prior\nsingle-task-based models on the ACE05 English dataset.",
            "In this paper, we propose an effective yet efficient model PAIE for both\nsentence-level and document-level Event Argument Extraction (EAE), which also\ngeneralizes well when there is a lack of training data. On the one hand, PAIE\nutilizes prompt tuning for extractive objectives to take the best advantages of\nPre-trained Language Models (PLMs). It introduces two span selectors based on\nthe prompt to select start/end tokens among input texts for each role. On the\nother hand, it captures argument interactions via multi-role prompts and\nconducts joint optimization with optimal span assignments via a bipartite\nmatching loss. Also, with a flexible prompt design, PAIE can extract multiple\narguments with the same role instead of conventional heuristic threshold\ntuning. We have conducted extensive experiments on three benchmarks, including\nboth sentence- and document-level EAE. The results present promising\nimprovements from PAIE (3.5\\% and 2.3\\% F1 gains in average on three\nbenchmarks, for PAIE-base and PAIE-large respectively). Further analysis\ndemonstrates the efficiency, generalization to few-shot settings, and\neffectiveness of different extractive prompt tuning strategies. Our code is\navailable at https://github.com/mayubo2333/PAIE."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2312.17617",
        "description": "Zeroshot information Extraction",
        "urls": [
            "https://arxiv.org/abs/2304.08085",
            "https://arxiv.org/abs/2310.03668",
            "https://arxiv.org/abs/2308.03279"
        ],
        "citations": [
            50,
            70,
            110
        ],
        "category": [
            "cs.CL",
            "cs.AI"
        ],
        "titles": [
            "InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction",
            "GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction",
            "UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition"
        ],
        "id": 78,
        "abstracts": [
            "Large language models have unlocked strong multi-task capabilities from\nreading instructive prompts. However, recent studies have shown that existing\nlarge models still have difficulty with information extraction tasks. For\nexample, gpt-3.5-turbo achieved an F1 score of 18.22 on the Ontonotes dataset,\nwhich is significantly lower than the state-of-the-art performance. In this\npaper, we propose InstructUIE, a unified information extraction framework based\non instruction tuning, which can uniformly model various information extraction\ntasks and capture the inter-task dependency. To validate the proposed method,\nwe introduce IE INSTRUCTIONS, a benchmark of 32 diverse information extraction\ndatasets in a unified text-to-text format with expert-written instructions.\nExperimental results demonstrate that our method achieves comparable\nperformance to Bert in supervised settings and significantly outperforms the\nstate-of-the-art and gpt3.5 in zero-shot settings.",
            "Large Language Models (LLMs) combined with instruction tuning have made\nsignificant progress when generalizing to unseen tasks. However, they have been\nless successful in Information Extraction (IE), lagging behind task-specific\nmodels. Typically, IE tasks are characterized by complex annotation guidelines\nthat describe the task and give examples to humans. Previous attempts to\nleverage such information have failed, even with the largest models, as they\nare not able to follow the guidelines out of the box. In this paper, we propose\nGoLLIE (Guideline-following Large Language Model for IE), a model able to\nimprove zero-shot results on unseen IE tasks by virtue of being fine-tuned to\ncomply with annotation guidelines. Comprehensive evaluation empirically\ndemonstrates that GoLLIE is able to generalize to and follow unseen guidelines,\noutperforming previous attempts at zero-shot information extraction. The\nablation study shows that detailed guidelines are key for good results.",
            "Large language models (LLMs) have demonstrated remarkable generalizability,\nsuch as understanding arbitrary entities and relations. Instruction tuning has\nproven effective for distilling LLMs into more cost-efficient models such as\nAlpaca and Vicuna. Yet such student models still trail the original LLMs by\nlarge margins in downstream applications. In this paper, we explore targeted\ndistillation with mission-focused instruction tuning to train student models\nthat can excel in a broad application class such as open information\nextraction. Using named entity recognition (NER) for case study, we show how\nChatGPT can be distilled into much smaller UniversalNER models for open NER.\nFor evaluation, we assemble the largest NER benchmark to date, comprising 43\ndatasets across 9 diverse domains such as biomedicine, programming, social\nmedia, law, finance. Without using any direct supervision, UniversalNER attains\nremarkable NER accuracy across tens of thousands of entity types, outperforming\ngeneral instruction-tuned models such as Alpaca and Vicuna by over 30 absolute\nF1 points in average. With a tiny fraction of parameters, UniversalNER not only\nacquires ChatGPT's capability in recognizing arbitrary entity types, but also\noutperforms its NER accuracy by 7-9 absolute F1 points in average. Remarkably,\nUniversalNER even outperforms by a large margin state-of-the-art multi-task\ninstruction-tuned systems such as InstructUIE, which uses supervised NER\nexamples. We also conduct thorough ablation studies to assess the impact of\nvarious components in our distillation approach. We release the distillation\nrecipe, data, and UniversalNER models to facilitate future research on targeted\ndistillation."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2312.17044",
        "description": "encoding for Long contex improvement",
        "urls": [
            "https://arxiv.org/abs/2108.12409",
            "https://arxiv.org/abs/2205.09921",
            "https://arxiv.org/abs/2310.04418"
        ],
        "citations": [
            615,
            46,
            34
        ],
        "category": [
            "cs.CL",
            "cs.LG"
        ],
        "titles": [
            "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",
            "KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation",
            "Functional Interpolation for Relative Positions Improves Long Context Transformers"
        ],
        "id": 79,
        "abstracts": [
            "Since the introduction of the transformer model by Vaswani et al. (2017), a\nfundamental question has yet to be answered: how does a model achieve\nextrapolation at inference time for sequences that are longer than it saw\nduring training? We first show that extrapolation can be enabled by simply\nchanging the position representation method, though we find that current\nmethods do not allow for efficient extrapolation. We therefore introduce a\nsimpler and more efficient position method, Attention with Linear Biases\n(ALiBi). ALiBi does not add positional embeddings to word embeddings; instead,\nit biases query-key attention scores with a penalty that is proportional to\ntheir distance. We show that this method trains a 1.3 billion parameter model\non input sequences of length 1024 that extrapolates to input sequences of\nlength 2048, achieving the same perplexity as a sinusoidal position embedding\nmodel trained on inputs of length 2048 but training 11% faster and using 11%\nless memory. ALiBi's inductive bias towards recency also leads it to outperform\nmultiple strong position methods on the WikiText-103 benchmark.",
            "Relative positional embeddings (RPE) have received considerable attention\nsince RPEs effectively model the relative distance among tokens and enable\nlength extrapolation. We propose KERPLE, a framework that generalizes relative\nposition embedding for extrapolation by kernelizing positional differences. We\nachieve this goal using conditionally positive definite (CPD) kernels, a class\nof functions known for generalizing distance metrics. To maintain the inner\nproduct interpretation of self-attention, we show that a CPD kernel can be\ntransformed into a PD kernel by adding a constant offset. This offset is\nimplicitly absorbed in the Softmax normalization during self-attention. The\ndiversity of CPD kernels allows us to derive various RPEs that enable length\nextrapolation in a principled way. Experiments demonstrate that the logarithmic\nvariant achieves excellent extrapolation performance on three large language\nmodeling datasets. Our implementation and pretrained checkpoints are released\nat https://github.com/chijames/KERPLE.git.",
            "Preventing the performance decay of Transformers on inputs longer than those\nused for training has been an important challenge in extending the context\nlength of these models. Though the Transformer architecture has fundamentally\nno limits on the input sequence lengths it can process, the choice of position\nencoding used during training can limit the performance of these models on\nlonger inputs. We propose a novel functional relative position encoding with\nprogressive interpolation, FIRE, to improve Transformer generalization to\nlonger contexts. We theoretically prove that this can represent some of the\npopular relative position encodings, such as T5's RPE, Alibi, and Kerple. We\nnext empirically show that FIRE models have better generalization to longer\ncontexts on both zero-shot language modeling and long text benchmarks."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2304.12205",
        "description": "Dataset for Autonomous Driving",
        "urls": [
            "https://arxiv.org/abs/2206.08367",
            "https://arxiv.org/abs/2202.08449",
            "https://arxiv.org/abs/2109.07644"
        ],
        "citations": [
            145,
            210,
            391
        ],
        "category": [
            "cs.CV",
            "cs.LG",
            "cs.RO"
        ],
        "titles": [
            "SHIFT: A Synthetic Driving Dataset for Continuous Multi-Task Domain Adaptation",
            "V2X-Sim: Multi-Agent Collaborative Perception Dataset and Benchmark for Autonomous Driving",
            "OPV2V: An Open Benchmark Dataset and Fusion Pipeline for Perception with Vehicle-to-Vehicle Communication"
        ],
        "id": 80,
        "abstracts": [
            "Adapting to a continuously evolving environment is a safety-critical\nchallenge inevitably faced by all autonomous driving systems. Existing image\nand video driving datasets, however, fall short of capturing the mutable nature\nof the real world. In this paper, we introduce the largest multi-task synthetic\ndataset for autonomous driving, SHIFT. It presents discrete and continuous\nshifts in cloudiness, rain and fog intensity, time of day, and vehicle and\npedestrian density. Featuring a comprehensive sensor suite and annotations for\nseveral mainstream perception tasks, SHIFT allows investigating the degradation\nof a perception system performance at increasing levels of domain shift,\nfostering the development of continuous adaptation strategies to mitigate this\nproblem and assess model robustness and generality. Our dataset and benchmark\ntoolkit are publicly available at www.vis.xyz/shift.",
            "Vehicle-to-everything (V2X) communication techniques enable the collaboration\nbetween vehicles and many other entities in the neighboring environment, which\ncould fundamentally improve the perception system for autonomous driving.\nHowever, the lack of a public dataset significantly restricts the research\nprogress of collaborative perception. To fill this gap, we present V2X-Sim, a\ncomprehensive simulated multi-agent perception dataset for V2X-aided autonomous\ndriving. V2X-Sim provides: (1) \\hl{multi-agent} sensor recordings from the\nroad-side unit (RSU) and multiple vehicles that enable collaborative\nperception, (2) multi-modality sensor streams that facilitate multi-modality\nperception, and (3) diverse ground truths that support various perception\ntasks. Meanwhile, we build an open-source testbed and provide a benchmark for\nthe state-of-the-art collaborative perception algorithms on three tasks,\nincluding detection, tracking and segmentation. V2X-Sim seeks to stimulate\ncollaborative perception research for autonomous driving before realistic\ndatasets become widely available. Our dataset and code are available at\n\\url{https://ai4ce.github.io/V2X-Sim/}.",
            "Employing Vehicle-to-Vehicle communication to enhance perception performance\nin self-driving technology has attracted considerable attention recently;\nhowever, the absence of a suitable open dataset for benchmarking algorithms has\nmade it difficult to develop and assess cooperative perception technologies. To\nthis end, we present the first large-scale open simulated dataset for\nVehicle-to-Vehicle perception. It contains over 70 interesting scenes, 11,464\nframes, and 232,913 annotated 3D vehicle bounding boxes, collected from 8 towns\nin CARLA and a digital town of Culver City, Los Angeles. We then construct a\ncomprehensive benchmark with a total of 16 implemented models to evaluate\nseveral information fusion strategies~(i.e. early, late, and intermediate\nfusion) with state-of-the-art LiDAR detection algorithms. Moreover, we propose\na new Attentive Intermediate Fusion pipeline to aggregate information from\nmultiple connected vehicles. Our experiments show that the proposed pipeline\ncan be easily integrated with existing 3D LiDAR detectors and achieve\noutstanding performance even with large compression rates. To encourage more\nresearchers to investigate Vehicle-to-Vehicle perception, we will release the\ndataset, benchmark methods, and all related codes in\nhttps://mobility-lab.seas.ucla.edu/opv2v/."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2312.17432",
        "description": "Video Understanding Model",
        "urls": [
            "https://arxiv.org/abs/2306.02858",
            "https://arxiv.org/abs/2306.05424",
            "https://arxiv.org/abs/2311.10122"
        ],
        "citations": [
            784,
            567,
            428
        ],
        "category": [
            "cs.CV",
            "cs.CL",
            "cs.SD",
            "eess.AS"
        ],
        "titles": [
            "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding",
            "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models",
            "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection"
        ],
        "id": 81,
        "abstracts": [
            "We present Video-LLaMA a multi-modal framework that empowers Large Language\nModels (LLMs) with the capability of understanding both visual and auditory\ncontent in the video. Video-LLaMA bootstraps cross-modal training from the\nfrozen pre-trained visual and audio encoders and the frozen LLMs. Unlike\nprevious works that complement LLMs to process the visual or audio signals\nonly, Video-LLaMA enables video comprehension by tackling two challenges: (1)\ncapturing the temporal changes in visual scenes, (2) integrating audio-visual\nsignals. To counter the first challenge, we propose a Video Q-former to\nassemble a pre-trained image encoder into our video encoder and introduce a\nvideo-to-text generation task to learn video-language correspondence. For the\nsecond challenge, we leverage ImageBind, a universal embedding model aligning\nmultiple modalities, as the pre-trained audio encoder and introduce an Audio\nQ-former on top of ImageBind to learn reasonable auditory query embeddings for\nthe LLM module. To align the output of both visual and audio encoders with\nLLM's embedding space, we first train Video-LLaMA on massive\nvideo/image-caption pairs and then tune our model with visual-instruction\ndatasets of moderate amount but higher quality. We found Video-LLaMA shows the\nability to perceive and comprehend video content and generate meaningful\nresponses grounded in the visual and auditory information presented in the\nvideos.",
            "Conversation agents fueled by Large Language Models (LLMs) are providing a\nnew way to interact with visual data. While there have been initial attempts\nfor image-based conversation models, this work addresses the under-explored\nfield of \\emph{video-based conversation} by introducing Video-ChatGPT. It is a\nmultimodal model that merges a video-adapted visual encoder with an LLM. The\nresulting model is capable of understanding and generating detailed\nconversations about videos. We introduce a new dataset of 100,000\nvideo-instruction pairs used to train Video-ChatGPT acquired via manual and\nsemi-automated pipeline that is easily scalable and robust to label noise. We\nalso develop a quantitative evaluation framework for video-based dialogue\nmodels to objectively analyze the strengths and weaknesses of video-based\ndialogue models. Code: https://github.com/mbzuai-oryx/Video-ChatGPT.",
            "The Large Vision-Language Model (LVLM) has enhanced the performance of\nvarious downstream tasks in visual-language understanding. Most existing\napproaches encode images and videos into separate feature spaces, which are\nthen fed as inputs to large language models. However, due to the lack of\nunified tokenization for images and videos, namely misalignment before\nprojection, it becomes challenging for a Large Language Model (LLM) to learn\nmulti-modal interactions from several poor projection layers. In this work, we\nunify visual representation into the language feature space to advance the\nfoundational LLM towards a unified LVLM. As a result, we establish a simple but\nrobust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images\nand videos, mutually enhancing each other. Video-LLaVA achieves superior\nperformances on a broad range of 9 image benchmarks across 5 image\nquestion-answering datasets and 4 image benchmark toolkits. Additionally, our\nVideo-LLaVA also outperforms Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% on\nMSRVTT, MSVD, TGIF, and ActivityNet, respectively. Notably, extensive\nexperiments demonstrate that Video-LLaVA mutually benefits images and videos\nwithin a unified visual representation, outperforming models designed\nspecifically for images or videos. We aim for this work to provide modest\ninsights into the multi-modal inputs for the LLM. Code address:\n\\href{https://github.com/PKU-YuanGroup/Video-LLaVA}"
        ]
    },
    {
        "source": "https://arxiv.org/abs/2406.08426",
        "description": "Text to Sql translation",
        "urls": [
            "https://arxiv.org/abs/2312.11242",
            "https://arxiv.org/abs/2304.11015",
            "https://arxiv.org/abs/2308.15363"
        ],
        "citations": [
            33,
            270,
            195
        ],
        "category": [
            "cs.CL",
            "cs.DB",
            "cs.AI",
            "cs.HC",
            "cs.LG"
        ],
        "titles": [
            "MAC-SQL: A Multi-Agent Collaborative Framework for Text-to-SQL",
            "DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction",
            "Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation"
        ],
        "id": 82,
        "abstracts": [
            "Recent LLM-based Text-to-SQL methods usually suffer from significant\nperformance degradation on \"huge\" databases and complex user questions that\nrequire multi-step reasoning. Moreover, most existing methods neglect the\ncrucial significance of LLMs utilizing external tools and model collaboration.\nTo address these challenges, we introduce MAC-SQL, a novel LLM-based\nmulti-agent collaborative framework. Our framework comprises a core decomposer\nagent for Text-to-SQL generation with few-shot chain-of-thought reasoning,\naccompanied by two auxiliary agents that utilize external tools or models to\nacquire smaller sub-databases and refine erroneous SQL queries. The decomposer\nagent collaborates with auxiliary agents, which are activated as needed and can\nbe expanded to accommodate new features or tools for effective Text-to-SQL\nparsing. In our framework, We initially leverage GPT-4 as the strong backbone\nLLM for all agent tasks to determine the upper bound of our framework. We then\nfine-tune an open-sourced instruction-followed model, SQL-Llama, by leveraging\nCode Llama 7B, to accomplish all tasks as GPT-4 does. Experiments show that\nSQL-Llama achieves a comparable execution accuracy of 43.94, compared to the\nbaseline accuracy of 46.35 for vanilla GPT-4. At the time of writing,\nMAC-SQL+GPT-4 achieves an execution accuracy of 59.59 when evaluated on the\nBIRD benchmark, establishing a new state-of-the-art (SOTA) on its holdout test\nset (https://github.com/wbbeyourself/MAC-SQL).",
            "There is currently a significant gap between the performance of fine-tuned\nmodels and prompting approaches using Large Language Models (LLMs) on the\nchallenging task of text-to-SQL, as evaluated on datasets such as Spider. To\nimprove the performance of LLMs in the reasoning process, we study how\ndecomposing the task into smaller sub-tasks can be effective. In particular, we\nshow that breaking down the generation problem into sub-problems and feeding\nthe solutions of those sub-problems into LLMs can be an effective approach for\nsignificantly improving their performance. Our experiments with three LLMs show\nthat this approach consistently improves their simple few-shot performance by\nroughly 10%, pushing the accuracy of LLMs towards SOTA or surpassing it. On the\nholdout test set of Spider, the SOTA, in terms of execution accuracy, was 79.9\nand the new SOTA at the time of this writing using our approach is 85.3. Our\napproach with in-context learning beats many heavily fine-tuned models by at\nleast 5%. Additionally, when evaluated on the BIRD benchmark, our approach\nachieved an execution accuracy of 55.9%, setting a new SOTA on its holdout test\nset.",
            "Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL\ntask. However, the absence of a systematical benchmark inhibits the development\nof designing effective, efficient and economic LLM-based Text-to-SQL solutions.\nTo address this challenge, in this paper, we first conduct a systematical and\nextensive comparison over existing prompt engineering methods, including\nquestion representation, example selection and example organization, and with\nthese experimental results, we elaborate their pros and cons. Based on these\nfindings, we propose a new integrated solution, named DAIL-SQL, which refreshes\nthe Spider leaderboard with 86.6% execution accuracy and sets a new bar. To\nexplore the potential of open-source LLM, we investigate them in various\nscenarios, and further enhance their performance with supervised fine-tuning.\nOur explorations highlight open-source LLMs' potential in Text-to-SQL, as well\nas the advantages and disadvantages of the supervised fine-tuning.\nAdditionally, towards an efficient and economic LLM-based Text-to-SQL solution,\nwe emphasize the token efficiency in prompt engineering and compare the prior\nstudies under this metric. We hope that our work provides a deeper\nunderstanding of Text-to-SQL with LLMs, and inspires further investigations and\nbroad applications."
        ]
    },
    {
        "source": "https://arxiv.org/abs/1909.06956",
        "description": "GAN for makeup transfer",
        "urls": [
            "https://arxiv.org/abs/1909.06956",
            "https://arxiv.org/abs/2207.09840",
            "https://arxiv.org/abs/2209.03444"
        ],
        "citations": [
            172,
            37,
            5
        ],
        "category": [
            "cs.CV",
            "cond-mat.supr-con"
        ],
        "titles": [
            "PSGAN: Pose and Expression Robust Spatial-Aware GAN for Customizable Makeup Transfer",
            "EleGANt: Exquisite and Locally Editable GAN for Makeup Transfer",
            "ScGAN: A Generative Adversarial Network to Predict Hypothetical Superconductors"
        ],
        "id": 83,
        "abstracts": [
            "In this paper, we address the makeup transfer task, which aims to transfer\nthe makeup from a reference image to a source image. Existing methods have\nachieved promising progress in constrained scenarios, but transferring between\nimages with large pose and expression differences is still challenging.\nBesides, they cannot realize customizable transfer that allows a controllable\nshade of makeup or specifies the part to transfer, which limits their\napplications. To address these issues, we propose Pose and expression robust\nSpatial-aware GAN (PSGAN). It first utilizes Makeup Distill Network to\ndisentangle the makeup of the reference image as two spatial-aware makeup\nmatrices. Then, Attentive Makeup Morphing module is introduced to specify how\nthe makeup of a pixel in the source image is morphed from the reference image.\nWith the makeup matrices and the source image, Makeup Apply Network is used to\nperform makeup transfer. Our PSGAN not only achieves state-of-the-art results\neven when large pose and expression differences exist but also is able to\nperform partial and shade-controllable makeup transfer. We also collected a\ndataset containing facial images with various poses and expressions for\nevaluations.",
            "Most existing methods view makeup transfer as transferring color\ndistributions of different facial regions and ignore details such as eye\nshadows and blushes. Besides, they only achieve controllable transfer within\npredefined fixed regions. This paper emphasizes the transfer of makeup details\nand steps towards more flexible controls. To this end, we propose Exquisite and\nlocally editable GAN for makeup transfer (EleGANt). It encodes facial\nattributes into pyramidal feature maps to preserves high-frequency information.\nIt uses attention to extract makeup features from the reference and adapt them\nto the source face, and we introduce a novel Sow-Attention Module that applies\nattention within shifted overlapped windows to reduce the computational cost.\nMoreover, EleGANt is the first to achieve customized local editing within\narbitrary areas by corresponding editing on the feature maps. Extensive\nexperiments demonstrate that EleGANt generates realistic makeup faces with\nexquisite details and achieves state-of-the-art performance. The code is\navailable at https://github.com/Chenyu-Yang-2000/EleGANt.",
            "Despite having been discovered more than three decades ago, High Temperature\nSuperconductors (HTSs) lack both an explanation for their mechanisms and a\nsystematic way to search for them. To aid this search, this project proposes\nScGAN, a Generative Adversarial Network (GAN) to efficiently predict new\nsuperconductors. ScGAN was trained on compounds in OQMD and then transfer\nlearned onto the SuperCon database or a subset of it. Once trained, the GAN was\nused to predict superconducting candidates, and approximately 70\\% of them were\ndetermined to be superconducting by a classification model--a 23-fold increase\nin discovery rate compared to manual search methods. Furthermore, more than\n99\\% of predictions were novel materials, demonstrating that ScGAN was able to\npotentially predict completely new superconductors, including several promising\nHTS candidates. This project presents a novel, efficient way to search for new\nsuperconductors, which may be used in technological applications or provide\ninsight into the unsolved problem of high temperature superconductivity."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2403.17881",
        "description": "Face swap method",
        "urls": [
            "https://arxiv.org/abs/1912.13457",
            "https://arxiv.org/abs/2212.02797",
            "https://arxiv.org/abs/1908.05932"
        ],
        "citations": [
            422,
            7,
            756
        ],
        "category": [
            "cs.CV",
            "cs.AI",
            "cs.GR",
            "cs.LG"
        ],
        "titles": [
            "FaceShifter: Towards High Fidelity And Occlusion Aware Face Swapping",
            "FlowFace: Semantic Flow-guided Shape-aware Face Swapping",
            "FSGAN: Subject Agnostic Face Swapping and Reenactment"
        ],
        "id": 84,
        "abstracts": [
            "In this work, we propose a novel two-stage framework, called FaceShifter, for\nhigh fidelity and occlusion aware face swapping. Unlike many existing face\nswapping works that leverage only limited information from the target image\nwhen synthesizing the swapped face, our framework, in its first stage,\ngenerates the swapped face in high-fidelity by exploiting and integrating the\ntarget attributes thoroughly and adaptively. We propose a novel attributes\nencoder for extracting multi-level target face attributes, and a new generator\nwith carefully designed Adaptive Attentional Denormalization (AAD) layers to\nadaptively integrate the identity and the attributes for face synthesis. To\naddress the challenging facial occlusions, we append a second stage consisting\nof a novel Heuristic Error Acknowledging Refinement Network (HEAR-Net). It is\ntrained to recover anomaly regions in a self-supervised way without any manual\nannotations. Extensive experiments on wild faces demonstrate that our face\nswapping results are not only considerably more perceptually appealing, but\nalso better identity preserving in comparison to other state-of-the-art\nmethods.",
            "In this work, we propose a semantic flow-guided two-stage framework for\nshape-aware face swapping, namely FlowFace. Unlike most previous methods that\nfocus on transferring the source inner facial features but neglect facial\ncontours, our FlowFace can transfer both of them to a target face, thus leading\nto more realistic face swapping. Concretely, our FlowFace consists of a face\nreshaping network and a face swapping network. The face reshaping network\naddresses the shape outline differences between the source and target faces. It\nfirst estimates a semantic flow (i.e., face shape differences) between the\nsource and the target face, and then explicitly warps the target face shape\nwith the estimated semantic flow. After reshaping, the face swapping network\ngenerates inner facial features that exhibit the identity of the source face.\nWe employ a pre-trained face masked autoencoder (MAE) to extract facial\nfeatures from both the source face and the target face. In contrast to previous\nmethods that use identity embedding to preserve identity information, the\nfeatures extracted by our encoder can better capture facial appearances and\nidentity information. Then, we develop a cross-attention fusion module to\nadaptively fuse inner facial features from the source face with the target\nfacial attributes, thus leading to better identity preservation. Extensive\nquantitative and qualitative experiments on in-the-wild faces demonstrate that\nour FlowFace outperforms the state-of-the-art significantly.",
            "We present Face Swapping GAN (FSGAN) for face swapping and reenactment.\nUnlike previous work, FSGAN is subject agnostic and can be applied to pairs of\nfaces without requiring training on those faces. To this end, we describe a\nnumber of technical contributions. We derive a novel recurrent neural network\n(RNN)-based approach for face reenactment which adjusts for both pose and\nexpression variations and can be applied to a single image or a video sequence.\nFor video sequences, we introduce continuous interpolation of the face views\nbased on reenactment, Delaunay Triangulation, and barycentric coordinates.\nOccluded face regions are handled by a face completion network. Finally, we use\na face blending network for seamless blending of the two faces while preserving\ntarget skin color and lighting conditions. This network uses a novel Poisson\nblending loss which combines Poisson optimization with perceptual loss. We\ncompare our approach to existing state-of-the-art systems and show our results\nto be both qualitatively and quantitatively superior."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2311.17117",
        "description": "Video synthesis",
        "urls": [
            "https://arxiv.org/abs/2305.03989",
            "https://arxiv.org/abs/2304.06025",
            "https://arxiv.org/abs/2311.17117"
        ],
        "citations": [
            22,
            95,
            265
        ],
        "category": [
            "cs.CV"
        ],
        "titles": [
            "LEO: Generative Latent Image Animator for Human Video Synthesis",
            "DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion",
            "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation"
        ],
        "id": 85,
        "abstracts": [
            "Spatio-temporal coherency is a major challenge in synthesizing high quality\nvideos, particularly in synthesizing human videos that contain rich global and\nlocal deformations. To resolve this challenge, previous approaches have\nresorted to different features in the generation process aimed at representing\nappearance and motion. However, in the absence of strict mechanisms to\nguarantee such disentanglement, a separation of motion from appearance has\nremained challenging, resulting in spatial distortions and temporal jittering\nthat break the spatio-temporal coherency. Motivated by this, we here propose\nLEO, a novel framework for human video synthesis, placing emphasis on\nspatio-temporal coherency. Our key idea is to represent motion as a sequence of\nflow maps in the generation process, which inherently isolate motion from\nappearance. We implement this idea via a flow-based image animator and a Latent\nMotion Diffusion Model (LMDM). The former bridges a space of motion codes with\nthe space of flow maps, and synthesizes video frames in a warp-and-inpaint\nmanner. LMDM learns to capture motion prior in the training data by\nsynthesizing sequences of motion codes. Extensive quantitative and qualitative\nanalysis suggests that LEO significantly improves coherent synthesis of human\nvideos over previous methods on the datasets TaichiHD, FaceForensics and\nCelebV-HQ. In addition, the effective disentanglement of appearance and motion\nin LEO allows for two additional tasks, namely infinite-length human video\nsynthesis, as well as content-preserving video editing.",
            "We present DreamPose, a diffusion-based method for generating animated\nfashion videos from still images. Given an image and a sequence of human body\nposes, our method synthesizes a video containing both human and fabric motion.\nTo achieve this, we transform a pretrained text-to-image model (Stable\nDiffusion) into a pose-and-image guided video synthesis model, using a novel\nfine-tuning strategy, a set of architectural changes to support the added\nconditioning signals, and techniques to encourage temporal consistency. We\nfine-tune on a collection of fashion videos from the UBC Fashion dataset. We\nevaluate our method on a variety of clothing styles and poses, and demonstrate\nthat our method produces state-of-the-art results on fashion video\nanimation.Video results are available on our project page.",
            "Character Animation aims to generating character videos from still images\nthrough driving signals. Currently, diffusion models have become the mainstream\nin visual generation research, owing to their robust generative capabilities.\nHowever, challenges persist in the realm of image-to-video, especially in\ncharacter animation, where temporally maintaining consistency with detailed\ninformation from character remains a formidable problem. In this paper, we\nleverage the power of diffusion models and propose a novel framework tailored\nfor character animation. To preserve consistency of intricate appearance\nfeatures from reference image, we design ReferenceNet to merge detail features\nvia spatial attention. To ensure controllability and continuity, we introduce\nan efficient pose guider to direct character's movements and employ an\neffective temporal modeling approach to ensure smooth inter-frame transitions\nbetween video frames. By expanding the training data, our approach can animate\narbitrary characters, yielding superior results in character animation compared\nto other image-to-video methods. Furthermore, we evaluate our method on\nbenchmarks for fashion video and human dance synthesis, achieving\nstate-of-the-art results."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2305.00624",
        "description": "Diffusion model for time-series forecasting",
        "urls": [
            "https://arxiv.org/abs/2106.10121",
            "https://arxiv.org/abs/2301.03028",
            "https://arxiv.org/abs/2301.13629"
        ],
        "citations": [
            39,
            100,
            65
        ],
        "category": [
            "cs.LG",
            "stat.ML"
        ],
        "titles": [
            "ScoreGrad: Multivariate Probabilistic Time Series Forecasting with Continuous Energy-based Generative Models",
            "Generative Time Series Forecasting with Diffusion, Denoise, and Disentanglement",
            "DiffSTG: Probabilistic Spatio-Temporal Graph Forecasting with Denoising Diffusion Models"
        ],
        "id": 86,
        "abstracts": [
            "Multivariate time series prediction has attracted a lot of attention because\nof its wide applications such as intelligence transportation, AIOps. Generative\nmodels have achieved impressive results in time series modeling because they\ncan model data distribution and take noise into consideration. However, many\nexisting works can not be widely used because of the constraints of functional\nform of generative models or the sensitivity to hyperparameters. In this paper,\nwe propose ScoreGrad, a multivariate probabilistic time series forecasting\nframework based on continuous energy-based generative models. ScoreGrad is\ncomposed of time series feature extraction module and conditional stochastic\ndifferential equation based score matching module. The prediction can be\nachieved by iteratively solving reverse-time SDE. To the best of our knowledge,\nScoreGrad is the first continuous energy based generative model used for time\nseries forecasting. Furthermore, ScoreGrad achieves state-of-the-art results on\nsix real-world datasets. The impact of hyperparameters and sampler types on the\nperformance are also explored. Code is available at\nhttps://github.com/yantijin/ScoreGradPred.",
            "Time series forecasting has been a widely explored task of great importance\nin many applications. However, it is common that real-world time series data\nare recorded in a short time period, which results in a big gap between the\ndeep model and the limited and noisy time series. In this work, we propose to\naddress the time series forecasting problem with generative modeling and\npropose a bidirectional variational auto-encoder (BVAE) equipped with\ndiffusion, denoise, and disentanglement, namely D3VAE. Specifically, a coupled\ndiffusion probabilistic model is proposed to augment the time series data\nwithout increasing the aleatoric uncertainty and implement a more tractable\ninference process with BVAE. To ensure the generated series move toward the\ntrue target, we further propose to adapt and integrate the multiscale denoising\nscore matching into the diffusion process for time series forecasting. In\naddition, to enhance the interpretability and stability of the prediction, we\ntreat the latent variable in a multivariate manner and disentangle them on top\nof minimizing total correlation. Extensive experiments on synthetic and\nreal-world data show that D3VAE outperforms competitive algorithms with\nremarkable margins. Our implementation is available at\nhttps://github.com/PaddlePaddle/PaddleSpatial/tree/main/research/D3VAE.",
            "Spatio-temporal graph neural networks (STGNN) have emerged as the dominant\nmodel for spatio-temporal graph (STG) forecasting. Despite their success, they\nfail to model intrinsic uncertainties within STG data, which cripples their\npracticality in downstream tasks for decision-making. To this end, this paper\nfocuses on probabilistic STG forecasting, which is challenging due to the\ndifficulty in modeling uncertainties and complex ST dependencies. In this\nstudy, we present the first attempt to generalize the popular denoising\ndiffusion probabilistic models to STGs, leading to a novel non-autoregressive\nframework called DiffSTG, along with the first denoising network UGnet for STG\nin the framework. Our approach combines the spatio-temporal learning\ncapabilities of STGNNs with the uncertainty measurements of diffusion models.\nExtensive experiments validate that DiffSTG reduces the Continuous Ranked\nProbability Score (CRPS) by 4%-14%, and Root Mean Squared Error (RMSE) by 2%-7%\nover existing methods on three real-world datasets."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2305.08196",
        "description": "SAM method model",
        "urls": [
            "https://arxiv.org/abs/2112.10003",
            "https://arxiv.org/abs/2305.11003",
            "https://arxiv.org/abs/2305.10724"
        ],
        "citations": [
            474,
            112,
            74
        ],
        "category": [
            "cs.CV",
            "cs.AI"
        ],
        "titles": [
            "Image Segmentation Using Text and Image Prompts",
            "Weakly-Supervised Concealed Object Segmentation with SAM-based Pseudo Labeling and Multi-scale Feature Grouping",
            "Segment Any Anomaly without Training via Hybrid Prompt Regularization"
        ],
        "id": 87,
        "abstracts": [
            "Image segmentation is usually addressed by training a model for a fixed set\nof object classes. Incorporating additional classes or more complex queries\nlater is expensive as it requires re-training the model on a dataset that\nencompasses these expressions. Here we propose a system that can generate image\nsegmentations based on arbitrary prompts at test time. A prompt can be either a\ntext or an image. This approach enables us to create a unified model (trained\nonce) for three common segmentation tasks, which come with distinct challenges:\nreferring expression segmentation, zero-shot segmentation and one-shot\nsegmentation. We build upon the CLIP model as a backbone which we extend with a\ntransformer-based decoder that enables dense prediction. After training on an\nextended version of the PhraseCut dataset, our system generates a binary\nsegmentation map for an image based on a free-text prompt or on an additional\nimage expressing the query. We analyze different variants of the latter\nimage-based prompts in detail. This novel hybrid input allows for dynamic\nadaptation not only to the three segmentation tasks mentioned above, but to any\nbinary segmentation task where a text or image query can be formulated.\nFinally, we find our system to adapt well to generalized queries involving\naffordances or properties. Code is available at\nhttps://eckerlab.org/code/clipseg.",
            "Weakly-Supervised Concealed Object Segmentation (WSCOS) aims to segment\nobjects well blended with surrounding environments using sparsely-annotated\ndata for model training. It remains a challenging task since (1) it is hard to\ndistinguish concealed objects from the background due to the intrinsic\nsimilarity and (2) the sparsely-annotated training data only provide weak\nsupervision for model learning. In this paper, we propose a new WSCOS method to\naddress these two challenges. To tackle the intrinsic similarity challenge, we\ndesign a multi-scale feature grouping module that first groups features at\ndifferent granularities and then aggregates these grouping results. By grouping\nsimilar features together, it encourages segmentation coherence, helping obtain\ncomplete segmentation results for both single and multiple-object images. For\nthe weak supervision challenge, we utilize the recently-proposed vision\nfoundation model, Segment Anything Model (SAM), and use the provided sparse\nannotations as prompts to generate segmentation masks, which are used to train\nthe model. To alleviate the impact of low-quality segmentation masks, we\nfurther propose a series of strategies, including multi-augmentation result\nensemble, entropy-based pixel-level weighting, and entropy-based image-level\nselection. These strategies help provide more reliable supervision to train the\nsegmentation model. We verify the effectiveness of our method on various WSCOS\ntasks, and experiments demonstrate that our method achieves state-of-the-art\nperformance on these tasks.",
            "We present a novel framework, i.e., Segment Any Anomaly + (SAA+), for\nzero-shot anomaly segmentation with hybrid prompt regularization to improve the\nadaptability of modern foundation models. Existing anomaly segmentation models\ntypically rely on domain-specific fine-tuning, limiting their generalization\nacross countless anomaly patterns. In this work, inspired by the great\nzero-shot generalization ability of foundation models like Segment Anything, we\nfirst explore their assembly to leverage diverse multi-modal prior knowledge\nfor anomaly localization. For non-parameter foundation model adaptation to\nanomaly segmentation, we further introduce hybrid prompts derived from domain\nexpert knowledge and target image context as regularization. Our proposed SAA+\nmodel achieves state-of-the-art performance on several anomaly segmentation\nbenchmarks, including VisA, MVTec-AD, MTD, and KSDD2, in the zero-shot setting.\nWe will release the code at\n\\href{https://github.com/caoyunkang/Segment-Any-Anomaly}{https://github.com/caoyunkang/Segment-Any-Anomaly}."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2305.08196",
        "description": "Segment dataset",
        "urls": [
            "https://arxiv.org/abs/2104.08391",
            "https://arxiv.org/abs/2104.06064",
            "https://arxiv.org/abs/2207.14315"
        ],
        "citations": [
            156,
            267,
            305
        ],
        "category": [
            "cs.CV"
        ],
        "titles": [
            "Learning To Count Everything",
            "Mixed supervision for surface-defect detection: from weakly to fully supervised learning",
            "SPot-the-Difference Self-Supervised Pre-training for Anomaly Detection and Segmentation"
        ],
        "id": 88,
        "abstracts": [
            "Existing works on visual counting primarily focus on one specific category at\na time, such as people, animals, and cells. In this paper, we are interested in\ncounting everything, that is to count objects from any category given only a\nfew annotated instances from that category. To this end, we pose counting as a\nfew-shot regression task. To tackle this task, we present a novel method that\ntakes a query image together with a few exemplar objects from the query image\nand predicts a density map for the presence of all objects of interest in the\nquery image. We also present a novel adaptation strategy to adapt our network\nto any novel visual category at test time, using only a few exemplar objects\nfrom the novel category. We also introduce a dataset of 147 object categories\ncontaining over 6000 images that are suitable for the few-shot counting task.\nThe images are annotated with two types of annotation, dots and bounding boxes,\nand they can be used for developing few-shot counting models. Experiments on\nthis dataset shows that our method outperforms several state-of-the-art object\ndetectors and few-shot counting approaches. Our code and dataset can be found\nat https://github.com/cvlab-stonybrook/LearningToCountEverything.",
            "Deep-learning methods have recently started being employed for addressing\nsurface-defect detection problems in industrial quality control. However, with\na large amount of data needed for learning, often requiring high-precision\nlabels, many industrial problems cannot be easily solved, or the cost of the\nsolutions would significantly increase due to the annotation requirements. In\nthis work, we relax heavy requirements of fully supervised learning methods and\nreduce the need for highly detailed annotations. By proposing a deep-learning\narchitecture, we explore the use of annotations of different details ranging\nfrom weak (image-level) labels through mixed supervision to full (pixel-level)\nannotations on the task of surface-defect detection. The proposed end-to-end\narchitecture is composed of two sub-networks yielding defect segmentation and\nclassification results. The proposed method is evaluated on several datasets\nfor industrial quality inspection: KolektorSDD, DAGM and Severstal Steel\nDefect. We also present a new dataset termed KolektorSDD2 with over 3000 images\ncontaining several types of defects, obtained while addressing a real-world\nindustrial problem. We demonstrate state-of-the-art results on all four\ndatasets. The proposed method outperforms all related approaches in fully\nsupervised settings and also outperforms weakly-supervised methods when only\nimage-level labels are available. We also show that mixed supervision with only\na handful of fully annotated samples added to weakly labelled training images\ncan result in performance comparable to the fully supervised model's\nperformance but at a significantly lower annotation cost.",
            "Visual anomaly detection is commonly used in industrial quality inspection.\nIn this paper, we present a new dataset as well as a new self-supervised\nlearning method for ImageNet pre-training to improve anomaly detection and\nsegmentation in 1-class and 2-class 5/10/high-shot training setups. We release\nthe Visual Anomaly (VisA) Dataset consisting of 10,821 high-resolution color\nimages (9,621 normal and 1,200 anomalous samples) covering 12 objects in 3\ndomains, making it the largest industrial anomaly detection dataset to date.\nBoth image and pixel-level labels are provided. We also propose a new\nself-supervised framework - SPot-the-difference (SPD) - which can regularize\ncontrastive self-supervised pre-training, such as SimSiam, MoCo and SimCLR, to\nbe more suitable for anomaly detection tasks. Our experiments on VisA and\nMVTec-AD dataset show that SPD consistently improves these contrastive\npre-training baselines and even the supervised pre-training. For example, SPD\nimproves Area Under the Precision-Recall curve (AU-PR) for anomaly segmentation\nby 5.9% and 6.8% over SimSiam and supervised pre-training respectively in the\n2-class high-shot regime. We open-source the project at\nhttp://github.com/amazon-research/spot-diff ."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2304.10946",
        "description": "Tabular LLM",
        "urls": [
            "https://arxiv.org/abs/2304.10946",
            "https://arxiv.org/abs/2012.06678",
            "https://arxiv.org/abs/1908.07442"
        ],
        "citations": [
            69,
            464,
            1570
        ],
        "category": [
            "cs.LG",
            "cs.CL",
            "q-bio.BM",
            "cs.AI",
            "stat.ML"
        ],
        "titles": [
            "CancerGPT: Few-shot Drug Pair Synergy Prediction using Large Pre-trained Language Models",
            "TabTransformer: Tabular Data Modeling Using Contextual Embeddings",
            "TabNet: Attentive Interpretable Tabular Learning"
        ],
        "id": 89,
        "abstracts": [
            "Large pre-trained language models (LLMs) have been shown to have significant\npotential in few-shot learning across various fields, even with minimal\ntraining data. However, their ability to generalize to unseen tasks in more\ncomplex fields, such as biology, has yet to be fully evaluated. LLMs can offer\na promising alternative approach for biological inference, particularly in\ncases where structured data and sample size are limited, by extracting prior\nknowledge from text corpora. Our proposed few-shot learning approach uses LLMs\nto predict the synergy of drug pairs in rare tissues that lack structured data\nand features. Our experiments, which involved seven rare tissues from different\ncancer types, demonstrated that the LLM-based prediction model achieved\nsignificant accuracy with very few or zero samples. Our proposed model, the\nCancerGPT (with $\\sim$ 124M parameters), was even comparable to the larger\nfine-tuned GPT-3 model (with $\\sim$ 175B parameters). Our research is the first\nto tackle drug pair synergy prediction in rare tissues with limited data. We\nare also the first to utilize an LLM-based prediction model for biological\nreaction prediction tasks.",
            "We propose TabTransformer, a novel deep tabular data modeling architecture\nfor supervised and semi-supervised learning. The TabTransformer is built upon\nself-attention based Transformers. The Transformer layers transform the\nembeddings of categorical features into robust contextual embeddings to achieve\nhigher prediction accuracy. Through extensive experiments on fifteen publicly\navailable datasets, we show that the TabTransformer outperforms the\nstate-of-the-art deep learning methods for tabular data by at least 1.0% on\nmean AUC, and matches the performance of tree-based ensemble models.\nFurthermore, we demonstrate that the contextual embeddings learned from\nTabTransformer are highly robust against both missing and noisy data features,\nand provide better interpretability. Lastly, for the semi-supervised setting we\ndevelop an unsupervised pre-training procedure to learn data-driven contextual\nembeddings, resulting in an average 2.1% AUC lift over the state-of-the-art\nmethods.",
            "We propose a novel high-performance and interpretable canonical deep tabular\ndata learning architecture, TabNet. TabNet uses sequential attention to choose\nwhich features to reason from at each decision step, enabling interpretability\nand more efficient learning as the learning capacity is used for the most\nsalient features. We demonstrate that TabNet outperforms other neural network\nand decision tree variants on a wide range of non-performance-saturated tabular\ndatasets and yields interpretable feature attributions plus insights into the\nglobal model behavior. Finally, for the first time to our knowledge, we\ndemonstrate self-supervised learning for tabular data, significantly improving\nperformance with unsupervised representation learning when unlabeled data is\nabundant."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2410.17434",
        "description": "Long video understanding Model",
        "urls": [
            "https://arxiv.org/abs/2410.17434",
            "https://arxiv.org/abs/2311.17043",
            "https://arxiv.org/abs/2406.16852"
        ],
        "citations": [
            16,
            204,
            64
        ],
        "category": [
            "cs.CV",
            "cs.CL"
        ],
        "titles": [
            "LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding",
            "LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models",
            "Long Context Transfer from Language to Vision"
        ],
        "id": 90,
        "abstracts": [
            "Multimodal Large Language Models (MLLMs) have shown promising progress in\nunderstanding and analyzing video content. However, processing long videos\nremains a significant challenge constrained by LLM's context size. To address\nthis limitation, we propose LongVU, a spatiotemporal adaptive compression\nmechanism thats reduces the number of video tokens while preserving visual\ndetails of long videos. Our idea is based on leveraging cross-modal query and\ninter-frame dependencies to adaptively reduce temporal and spatial redundancy\nin videos. Specifically, we leverage DINOv2 features to remove redundant frames\nthat exhibit high similarity. Then we utilize text-guided cross-modal query for\nselective frame feature reduction. Further, we perform spatial token reduction\nacross frames based on their temporal dependencies. Our adaptive compression\nstrategy effectively processes a large number of frames with little visual\ninformation loss within given context length. Our LongVU consistently surpass\nexisting methods across a variety of video understanding benchmarks, especially\non hour-long video understanding tasks such as VideoMME and MLVU. Given a\nlight-weight LLM, our LongVU also scales effectively into a smaller size with\nstate-of-the-art video understanding performance.",
            "In this work, we present a novel method to tackle the token generation\nchallenge in Vision Language Models (VLMs) for video and image understanding,\ncalled LLaMA-VID. Current VLMs, while proficient in tasks like image captioning\nand visual question answering, face computational burdens when processing long\nvideos due to the excessive visual tokens. LLaMA-VID addresses this issue by\nrepresenting each frame with two distinct tokens, namely context token and\ncontent token. The context token encodes the overall image context based on\nuser input, whereas the content token encapsulates visual cues in each frame.\nThis dual-token strategy significantly reduces the overload of long videos\nwhile preserving critical information. Generally, LLaMA-VID empowers existing\nframeworks to support hour-long videos and pushes their upper limit with an\nextra context token. It is proved to surpass previous methods on most of video-\nor image-based benchmarks. Code is available\nhttps://github.com/dvlab-research/LLaMA-VID}{https://github.com/dvlab-research/LLaMA-VID",
            "Video sequences offer valuable temporal information, but existing large\nmultimodal models (LMMs) fall short in understanding extremely long videos.\nMany works address this by reducing the number of visual tokens using visual\nresamplers. Alternatively, in this paper, we approach this problem from the\nperspective of the language model. By simply extrapolating the context length\nof the language backbone, we enable LMMs to comprehend orders of magnitude more\nvisual tokens without any video training. We call this phenomenon long context\ntransfer and carefully ablate its properties. To effectively measure LMMs'\nability to generalize to long contexts in the vision modality, we develop\nV-NIAH (Visual Needle-In-A-Haystack), a purely synthetic long vision benchmark\ninspired by the language model's NIAH test. Our proposed Long Video Assistant\n(LongVA) can process 2000 frames or over 200K visual tokens without additional\ncomplexities. With its extended context length, LongVA achieves\nstate-of-the-art performance on Video-MME among 7B-scale models by densely\nsampling more input frames. Our work is open-sourced at\nhttps://github.com/EvolvingLMMs-Lab/LongVA."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2410.17434",
        "description": "Video Understanding Benchmark",
        "urls": [
            "https://arxiv.org/abs/2406.04264",
            "https://arxiv.org/abs/2308.09126",
            "https://arxiv.org/abs/2311.17005"
        ],
        "citations": [
            57,
            161,
            249
        ],
        "category": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "titles": [
            "MLVU: Benchmarking Multi-task Long Video Understanding",
            "EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding",
            "MVBench: A Comprehensive Multi-modal Video Understanding Benchmark"
        ],
        "id": 91,
        "abstracts": [
            "The evaluation of Long Video Understanding (LVU) performance poses an\nimportant but challenging research problem. Despite previous efforts, the\nexisting video understanding benchmarks are severely constrained by several\nissues, especially the insufficient lengths of videos, a lack of diversity in\nvideo types and evaluation tasks, and the inappropriateness for evaluating LVU\nperformances. To address the above problems, we propose a new benchmark called\nMLVU (Multi-task Long Video Understanding Benchmark) for the comprehensive and\nin-depth evaluation of LVU. MLVU presents the following critical values:\n\\textit{1)} The substantial and flexible extension of video lengths, which\nenables the benchmark to evaluate LVU performance across a wide range of\ndurations. \\textit{2)} The inclusion of various video genres, e.g., movies,\nsurveillance footage, egocentric videos, cartoons, game videos, etc., which\nreflects the models' LVU performances in different scenarios. \\textit{3)} The\ndevelopment of diversified evaluation tasks, which enables a comprehensive\nexamination of MLLMs' key abilities in long-video understanding. The empirical\nstudy with 23 latest MLLMs reveals significant room for improvement in today's\ntechnique, as all existing methods struggle with most of the evaluation tasks\nand exhibit severe performance degradation when handling longer videos.\nAdditionally, it suggests that factors such as context length,\nimage-understanding ability, and the choice of LLM backbone can play critical\nroles in future advancements. We anticipate that MLVU will advance the research\nof long video understanding by providing a comprehensive and in-depth analysis\nof MLLMs.",
            "We introduce EgoSchema, a very long-form video question-answering dataset,\nand benchmark to evaluate long video understanding capabilities of modern\nvision and language systems. Derived from Ego4D, EgoSchema consists of over\n5000 human curated multiple choice question answer pairs, spanning over 250\nhours of real video data, covering a very broad range of natural human activity\nand behavior. For each question, EgoSchema requires the correct answer to be\nselected between five given options based on a three-minute-long video clip.\nWhile some prior works have proposed video datasets with long clip lengths, we\nposit that merely the length of the video clip does not truly capture the\ntemporal difficulty of the video task that is being considered. To remedy this,\nwe introduce temporal certificate sets, a general notion for capturing the\nintrinsic temporal understanding length associated with a broad range of video\nunderstanding tasks & datasets. Based on this metric, we find EgoSchema to have\nintrinsic temporal lengths over 5.7x longer than the second closest dataset and\n10x to 100x longer than any other video understanding dataset. Further, our\nevaluation of several current state-of-the-art video and language models shows\nthem to be severely lacking in long-term video understanding capabilities. Even\nmodels with several billions of parameters achieve QA accuracy less than 33%\n(random is 20%) on the EgoSchema multi-choice question answering task, while\nhumans achieve about 76% accuracy. We posit that \\name{}{}, with its long\nintrinsic temporal structures and diverse complexity, would serve as a valuable\nevaluation probe for developing effective long-term video understanding systems\nin the future. Data and Zero-shot model evaluation code are open-sourced for\nboth public and commercial use under the Ego4D license at\nhttp://egoschema.github.io",
            "With the rapid development of Multi-modal Large Language Models (MLLMs), a\nnumber of diagnostic benchmarks have recently emerged to evaluate the\ncomprehension capabilities of these models. However, most benchmarks\npredominantly assess spatial understanding in the static image tasks, while\noverlooking temporal understanding in the dynamic video tasks. To alleviate\nthis issue, we introduce a comprehensive Multi-modal Video understanding\nBenchmark, namely MVBench, which covers 20 challenging video tasks that cannot\nbe effectively solved with a single frame. Specifically, we first introduce a\nnovel static-to-dynamic method to define these temporal-related tasks. By\ntransforming various static tasks into dynamic ones, we enable the systematic\ngeneration of video tasks that require a broad spectrum of temporal skills,\nranging from perception to cognition. Then, guided by the task definition, we\nautomatically convert public video annotations into multiple-choice QA to\nevaluate each task. On one hand, such a distinct paradigm allows us to build\nMVBench efficiently, without much manual intervention. On the other hand, it\nguarantees evaluation fairness with ground-truth video annotations, avoiding\nthe biased scoring of LLMs. Moreover, we further develop a robust video MLLM\nbaseline, i.e., VideoChat2, by progressive multi-modal training with diverse\ninstruction-tuning data. The extensive results on our MVBench reveal that, the\nexisting MLLMs are far from satisfactory in temporal understanding, while our\nVideoChat2 largely surpasses these leading models by over 15% on MVBench. All\nmodels and data are available at https://github.com/OpenGVLab/Ask-Anything."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2401.06477",
        "description": "数据合成",
        "urls": [
            "https://arxiv.org/abs/2401.06477",
            "https://arxiv.org/abs/2408.04614",
            "https://arxiv.org/abs/2308.06259"
        ],
        "citations": [
            2,
            0,
            195
        ],
        "category": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "titles": [
            "Kun: Answer Polishment for Chinese Self-Alignment with Instruction Back-Translation",
            "Better Alignment with Instruction Back-and-Forth Translation",
            "Self-Alignment with Instruction Backtranslation"
        ],
        "id": 92,
        "abstracts": [
            "In this paper, we introduce Kun, a novel approach for creating high-quality\ninstruction-tuning datasets for large language models (LLMs) without relying on\nmanual annotations. Adapting a self-training algorithm based on instruction\nback-translation and answer polishment, Kun leverages unlabelled data from\ndiverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantial\ndataset of over a million Chinese instructional data points. This approach\nsignificantly deviates from traditional methods by using a self-curation\nprocess to refine and select the most effective instruction-output pairs. Our\nexperiments with the 6B-parameter Yi model across various benchmarks\ndemonstrate Kun's robustness and scalability. Our method's core contributions\nlie in its algorithmic advancement, which enhances data retention and clarity,\nand its innovative data generation approach that substantially reduces the\nreliance on costly and time-consuming manual annotations. This methodology\npresents a scalable and efficient solution for improving the\ninstruction-following capabilities of LLMs, with significant implications for\ntheir application across diverse fields. The code and dataset can be found at\nhttps://github.com/Zheng0428/COIG-Kun",
            "We propose a new method, instruction back-and-forth translation, to construct\nhigh-quality synthetic data grounded in world knowledge for aligning large\nlanguage models (LLMs). Given documents from a web corpus, we generate and\ncurate synthetic instructions using the backtranslation approach proposed by Li\net al.(2023a), and rewrite the responses to improve their quality further based\non the initial documents. Fine-tuning with the resulting (backtranslated\ninstruction, rewritten response) pairs yields higher win rates on AlpacaEval\nthan using other common instruction datasets such as Humpback, ShareGPT, Open\nOrca, Alpaca-GPT4 and Self-instruct. We also demonstrate that rewriting the\nresponses with an LLM outperforms direct distillation, and the two generated\ntext distributions exhibit significant distinction in embedding space. Further\nanalysis shows that our backtranslated instructions are of higher quality than\nother sources of synthetic instructions, while our responses are more diverse\nand complex than those obtained from distillation. Overall we find that\ninstruction back-and-forth translation combines the best of both worlds --\nmaking use of the information diversity and quantity found on the web, while\nensuring the quality of the responses which is necessary for effective\nalignment.",
            "We present a scalable method to build a high quality instruction following\nlanguage model by automatically labelling human-written text with corresponding\ninstructions. Our approach, named instruction backtranslation, starts with a\nlanguage model finetuned on a small amount of seed data, and a given web\ncorpus. The seed model is used to construct training examples by generating\ninstruction prompts for web documents (self-augmentation), and then selecting\nhigh quality examples from among these candidates (self-curation). This data is\nthen used to finetune a stronger model. Finetuning LLaMa on two iterations of\nour approach yields a model that outperforms all other LLaMa-based models on\nthe Alpaca leaderboard not relying on distillation data, demonstrating highly\neffective self-alignment."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2211.01910",
        "description": "Prompt optimization",
        "urls": [
            "https://arxiv.org/abs/2211.01910",
            "https://arxiv.org/abs/2305.03495",
            "https://arxiv.org/abs/2309.03409"
        ],
        "citations": [
            929,
            263,
            637
        ],
        "category": [
            "cs.LG",
            "cs.AI",
            "cs.CL"
        ],
        "titles": [
            "Large Language Models Are Human-Level Prompt Engineers",
            "Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search",
            "Large Language Models as Optimizers"
        ],
        "id": 93,
        "abstracts": [
            "By conditioning on natural language instructions, large language models\n(LLMs) have displayed impressive capabilities as general-purpose computers.\nHowever, task performance depends significantly on the quality of the prompt\nused to steer the model, and most effective prompts have been handcrafted by\nhumans. Inspired by classical program synthesis and the human approach to\nprompt engineering, we propose Automatic Prompt Engineer (APE) for automatic\ninstruction generation and selection. In our method, we treat the instruction\nas the \"program,\" optimized by searching over a pool of instruction candidates\nproposed by an LLM in order to maximize a chosen score function. To evaluate\nthe quality of the selected instruction, we evaluate the zero-shot performance\nof another LLM following the selected instruction. Experiments on 24 NLP tasks\nshow that our automatically generated instructions outperform the prior LLM\nbaseline by a large margin and achieve better or comparable performance to the\ninstructions generated by human annotators on 19/24 tasks. We conduct extensive\nqualitative and quantitative analyses to explore the performance of APE. We\nshow that APE-engineered prompts can be applied to steer models toward\ntruthfulness and/or informativeness, as well as to improve few-shot learning\nperformance by simply prepending them to standard in-context learning prompts.\nPlease check out our webpage at\nhttps://sites.google.com/view/automatic-prompt-engineer.",
            "Large Language Models (LLMs) have shown impressive performance as general\npurpose agents, but their abilities remain highly dependent on prompts which\nare hand written with onerous trial-and-error effort. We propose a simple and\nnonparametric solution to this problem, Automatic Prompt Optimization (APO),\nwhich is inspired by numerical gradient descent to automatically improve\nprompts, assuming access to training data and an LLM API. The algorithm uses\nminibatches of data to form natural language \"gradients\" that criticize the\ncurrent prompt. The gradients are then \"propagated\" into the prompt by editing\nthe prompt in the opposite semantic direction of the gradient. These gradient\ndescent steps are guided by a beam search and bandit selection procedure which\nsignificantly improves algorithmic efficiency. Preliminary results across three\nbenchmark NLP tasks and the novel problem of LLM jailbreak detection suggest\nthat Automatic Prompt Optimization can outperform prior prompt editing\ntechniques and improve an initial prompt's performance by up to 31%, by using\ndata to rewrite vague task descriptions into more precise annotation\ninstructions.",
            "Optimization is ubiquitous. While derivative-based algorithms have been\npowerful tools for various problems, the absence of gradient imposes challenges\non many real-world applications. In this work, we propose Optimization by\nPROmpting (OPRO), a simple and effective approach to leverage large language\nmodels (LLMs) as optimizers, where the optimization task is described in\nnatural language. In each optimization step, the LLM generates new solutions\nfrom the prompt that contains previously generated solutions with their values,\nthen the new solutions are evaluated and added to the prompt for the next\noptimization step. We first showcase OPRO on linear regression and traveling\nsalesman problems, then move on to our main application in prompt optimization,\nwhere the goal is to find instructions that maximize the task accuracy. With a\nvariety of LLMs, we demonstrate that the best prompts optimized by OPRO\noutperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on\nBig-Bench Hard tasks. Code at https://github.com/google-deepmind/opro."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2402.01155",
        "description": "Table LLM",
        "urls": [
            "https://arxiv.org/abs/2201.05966",
            "https://arxiv.org/abs/2301.13808",
            "https://arxiv.org/abs/2402.01155"
        ],
        "citations": [
            209,
            42,
            12
        ],
        "category": [
            "cs.CL"
        ],
        "titles": [
            "UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models",
            "Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning",
            "CABINET: Content Relevance based Noise Reduction for Table Question Answering"
        ],
        "id": 94,
        "abstracts": [
            "Structured knowledge grounding (SKG) leverages structured knowledge to\ncomplete user requests, such as semantic parsing over databases and question\nanswering over knowledge bases. Since the inputs and outputs of SKG tasks are\nheterogeneous, they have been studied separately by different communities,\nwhich limits systematic and compatible research on SKG. In this paper, we\novercome this limitation by proposing the UnifiedSKG framework, which unifies\n21 SKG tasks into a text-to-text format, aiming to promote systematic SKG\nresearch, instead of being exclusive to a single task, domain, or dataset. We\nuse UnifiedSKG to benchmark T5 with different sizes and show that T5, with\nsimple modifications when necessary, achieves state-of-the-art performance on\nalmost all of the 21 tasks. We further demonstrate that multi-task\nprefix-tuning improves the performance on most tasks, largely improving the\noverall performance. UnifiedSKG also facilitates the investigation of zero-shot\nand few-shot learning, and we show that T0, GPT-3, and Codex struggle in\nzero-shot and few-shot learning for SKG. We also use UnifiedSKG to conduct a\nseries of controlled experiments on structured knowledge encoding variants\nacross SKG tasks. UnifiedSKG is easily extensible to more tasks, and it is\nopen-sourced at https://github.com/hkunlp/unifiedskg.",
            "Table-based reasoning has shown remarkable progress in combining deep models\nwith discrete reasoning, which requires reasoning over both free-form natural\nlanguage (NL) questions and structured tabular data. However, previous\ntable-based reasoning solutions usually suffer from significant performance\ndegradation on huge evidence (tables). In addition, most existing methods\nstruggle to reason over complex questions since the required information is\nscattered in different places. To alleviate the above challenges, we exploit\nlarge language models (LLMs) as decomposers for effective table-based\nreasoning, which (i) decompose huge evidence (a huge table) into sub-evidence\n(a small table) to mitigate the interference of useless information for table\nreasoning; and (ii) decompose complex questions into simpler sub-questions for\ntext reasoning. Specifically, we first use the LLMs to break down the evidence\n(tables) involved in the current question, retaining the relevant evidence and\nexcluding the remaining irrelevant evidence from the huge table. In addition,\nwe propose a \"parsing-execution-filling\" strategy to alleviate the\nhallucination dilemma of the chain of thought by decoupling logic and numerical\ncomputation in each step. Extensive experiments show that our method can\neffectively leverage decomposed evidence and questions and outperforms the\nstrong baselines on TabFact, WikiTableQuestion, and FetaQA datasets. Notably,\nour model outperforms human performance for the first time on the TabFact\ndataset.",
            "Table understanding capability of Large Language Models (LLMs) has been\nextensively studied through the task of question-answering (QA) over tables.\nTypically, only a small part of the whole table is relevant to derive the\nanswer for a given question. The irrelevant parts act as noise and are\ndistracting information, resulting in sub-optimal performance due to the\nvulnerability of LLMs to noise. To mitigate this, we propose CABINET (Content\nRelevAnce-Based NoIse ReductioN for TablE QuesTion-Answering) - a framework to\nenable LLMs to focus on relevant tabular data by suppressing extraneous\ninformation. CABINET comprises an Unsupervised Relevance Scorer (URS), trained\ndifferentially with the QA LLM, that weighs the table content based on its\nrelevance to the input question before feeding it to the question-answering LLM\n(QA LLM). To further aid the relevance scorer, CABINET employs a weakly\nsupervised module that generates a parsing statement describing the criteria of\nrows and columns relevant to the question and highlights the content of\ncorresponding table cells. CABINET significantly outperforms various tabular\nLLM baselines, as well as GPT3-based in-context learning methods, is more\nrobust to noise, maintains outperformance on tables of varying sizes, and\nestablishes new SoTA performance on WikiTQ, FeTaQA, and WikiSQL datasets. We\nrelease our code and datasets at https://github.com/Sohanpatnaik106/CABINET_QA."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2401.16702",
        "description": "Long video understanding",
        "urls": [
            "https://arxiv.org/abs/2212.13738",
            "https://arxiv.org/abs/2204.02968",
            "https://arxiv.org/abs/2401.16702"
        ],
        "citations": [
            14,
            97,
            14
        ],
        "category": [
            "cs.CV",
            "cs.CL"
        ],
        "titles": [
            "TempCLR: Temporal Alignment Representation with Contrastive Learning",
            "Temporal Alignment Networks for Long-term Video",
            "Multi-granularity Correspondence Learning from Long-term Noisy Videos"
        ],
        "id": 95,
        "abstracts": [
            "Video representation learning has been successful in video-text pre-training\nfor zero-shot transfer, where each sentence is trained to be close to the\npaired video clips in a common feature space. For long videos, given a\nparagraph of description where the sentences describe different segments of the\nvideo, by matching all sentence-clip pairs, the paragraph and the full video\nare aligned implicitly. However, such unit-level comparison may ignore global\ntemporal context, which inevitably limits the generalization ability. In this\npaper, we propose a contrastive learning framework TempCLR to compare the full\nvideo and the paragraph explicitly. As the video/paragraph is formulated as a\nsequence of clips/sentences, under the constraint of their temporal order, we\nuse dynamic time warping to compute the minimum cumulative cost over\nsentence-clip pairs as the sequence-level distance. To explore the temporal\ndynamics, we break the consistency of temporal succession by shuffling video\nclips w.r.t. temporal granularity. Then, we obtain the representations for\nclips/sentences, which perceive the temporal information and thus facilitate\nthe sequence alignment. In addition to pre-training on the video and paragraph,\nour approach can also generalize on the matching between video instances. We\nevaluate our approach on video retrieval, action step localization, and\nfew-shot action recognition, and achieve consistent performance gain over all\nthree tasks. Detailed ablation studies are provided to justify the approach\ndesign.",
            "The objective of this paper is a temporal alignment network that ingests long\nterm video sequences, and associated text sentences, in order to: (1) determine\nif a sentence is alignable with the video; and (2) if it is alignable, then\ndetermine its alignment. The challenge is to train such networks from\nlarge-scale datasets, such as HowTo100M, where the associated text sentences\nhave significant noise, and are only weakly aligned when relevant. Apart from\nproposing the alignment network, we also make four contributions: (i) we\ndescribe a novel co-training method that enables to denoise and train on raw\ninstructional videos without using manual annotation, despite the considerable\nnoise; (ii) to benchmark the alignment performance, we manually curate a\n10-hour subset of HowTo100M, totalling 80 videos, with sparse temporal\ndescriptions. Our proposed model, trained on HowTo100M, outperforms strong\nbaselines (CLIP, MIL-NCE) on this alignment dataset by a significant margin;\n(iii) we apply the trained model in the zero-shot settings to multiple\ndownstream video understanding tasks and achieve state-of-the-art results,\nincluding text-video retrieval on YouCook2, and weakly supervised video action\nsegmentation on Breakfast-Action; (iv) we use the automatically aligned\nHowTo100M annotations for end-to-end finetuning of the backbone model, and\nobtain improved performance on downstream action recognition tasks.",
            "Existing video-language studies mainly focus on learning short video clips,\nleaving long-term temporal dependencies rarely explored due to over-high\ncomputational cost of modeling long videos. To address this issue, one feasible\nsolution is learning the correspondence between video clips and captions, which\nhowever inevitably encounters the multi-granularity noisy correspondence (MNC)\nproblem. To be specific, MNC refers to the clip-caption misalignment\n(coarse-grained) and frame-word misalignment (fine-grained), hindering temporal\nlearning and video understanding. In this paper, we propose NOise Robust\nTemporal Optimal traNsport (Norton) that addresses MNC in a unified optimal\ntransport (OT) framework. In brief, Norton employs video-paragraph and\nclip-caption contrastive losses to capture long-term dependencies based on OT.\nTo address coarse-grained misalignment in video-paragraph contrast, Norton\nfilters out the irrelevant clips and captions through an alignable prompt\nbucket and realigns asynchronous clip-caption pairs based on transport\ndistance. To address the fine-grained misalignment, Norton incorporates a\nsoft-maximum operator to identify crucial words and key frames. Additionally,\nNorton exploits the potential faulty negative samples in clip-caption contrast\nby rectifying the alignment target with OT assignment to ensure precise\ntemporal modeling. Extensive experiments on video retrieval, videoQA, and\naction segmentation verify the effectiveness of our method. Code is available\nat https://lin-yijie.github.io/projects/Norton."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2403.10081",
        "description": "RAG method paper",
        "urls": [
            "https://arxiv.org/abs/2403.10081",
            "https://arxiv.org/abs/2305.06983",
            "https://arxiv.org/abs/2212.10509"
        ],
        "citations": [
            39,
            414,
            291
        ],
        "category": [
            "cs.CL",
            "cs.IR",
            "cs.LG"
        ],
        "titles": [
            "DRAGIN: Dynamic Retrieval Augmented Generation based on the Information Needs of Large Language Models",
            "Active Retrieval Augmented Generation",
            "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions"
        ],
        "id": 96,
        "abstracts": [
            "Dynamic retrieval augmented generation (RAG) paradigm actively decides when\nand what to retrieve during the text generation process of Large Language\nModels (LLMs). There are two key elements of this paradigm: identifying the\noptimal moment to activate the retrieval module (deciding when to retrieve) and\ncrafting the appropriate query once retrieval is triggered (determining what to\nretrieve). However, current dynamic RAG methods fall short in both aspects.\nFirstly, the strategies for deciding when to retrieve often rely on static\nrules. Moreover, the strategies for deciding what to retrieve typically limit\nthemselves to the LLM's most recent sentence or the last few tokens, while the\nLLM's real-time information needs may span across the entire context. To\novercome these limitations, we introduce a new framework, DRAGIN, i.e., Dynamic\nRetrieval Augmented Generation based on the real-time Information Needs of\nLLMs. Our framework is specifically designed to make decisions on when and what\nto retrieve based on the LLM's real-time information needs during the text\ngeneration process. We evaluate DRAGIN along with existing methods\ncomprehensively over 4 knowledge-intensive generation datasets. Experimental\nresults show that DRAGIN achieves superior performance on all tasks,\ndemonstrating the effectiveness of our method. We have open-sourced all the\ncode, data, and models in GitHub: https://github.com/oneal2000/DRAGIN/tree/main",
            "Despite the remarkable ability of large language models (LMs) to comprehend\nand generate language, they have a tendency to hallucinate and create factually\ninaccurate output. Augmenting LMs by retrieving information from external\nknowledge resources is one promising solution. Most existing retrieval\naugmented LMs employ a retrieve-and-generate setup that only retrieves\ninformation once based on the input. This is limiting, however, in more general\nscenarios involving generation of long texts, where continually gathering\ninformation throughout generation is essential. In this work, we provide a\ngeneralized view of active retrieval augmented generation, methods that\nactively decide when and what to retrieve across the course of the generation.\nWe propose Forward-Looking Active REtrieval augmented generation (FLARE), a\ngeneric method which iteratively uses a prediction of the upcoming sentence to\nanticipate future content, which is then utilized as a query to retrieve\nrelevant documents to regenerate the sentence if it contains low-confidence\ntokens. We test FLARE along with baselines comprehensively over 4 long-form\nknowledge-intensive generation tasks/datasets. FLARE achieves superior or\ncompetitive performance on all tasks, demonstrating the effectiveness of our\nmethod. Code and datasets are available at https://github.com/jzbjyb/FLARE.",
            "Prompting-based large language models (LLMs) are surprisingly powerful at\ngenerating natural language reasoning steps or Chains-of-Thoughts (CoT) for\nmulti-step question answering (QA). They struggle, however, when the necessary\nknowledge is either unavailable to the LLM or not up-to-date within its\nparameters. While using the question to retrieve relevant text from an external\nknowledge source helps LLMs, we observe that this one-step retrieve-and-read\napproach is insufficient for multi-step QA. Here, \\textit{what to retrieve}\ndepends on \\textit{what has already been derived}, which in turn may depend on\n\\textit{what was previously retrieved}. To address this, we propose IRCoT, a\nnew approach for multi-step QA that interleaves retrieval with steps\n(sentences) in a CoT, guiding the retrieval with CoT and in turn using\nretrieved results to improve CoT. Using IRCoT with GPT3 substantially improves\nretrieval (up to 21 points) as well as downstream QA (up to 15 points) on four\ndatasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar\nsubstantial gains in out-of-distribution (OOD) settings as well as with much\nsmaller models such as Flan-T5-large without additional training. IRCoT reduces\nmodel hallucination, resulting in factually more accurate CoT reasoning. Code,\ndata, and prompts are available at \\url{https://github.com/stonybrooknlp/ircot}"
        ]
    },
    {
        "source": "https://arxiv.org/abs/2309.12284",
        "description": "Math LLM",
        "urls": [
            "https://arxiv.org/abs/2210.02414",
            "https://arxiv.org/abs/2308.09583",
            "https://arxiv.org/abs/2309.12284"
        ],
        "citations": [
            597,
            329,
            411
        ],
        "category": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "titles": [
            "GLM-130B: An Open Bilingual Pre-trained Model",
            "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct",
            "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models"
        ],
        "id": 97,
        "abstracts": [
            "We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language\nmodel with 130 billion parameters. It is an attempt to open-source a 100B-scale\nmodel at least as good as GPT-3 (davinci) and unveil how models of such a scale\ncan be successfully pre-trained. Over the course of this effort, we face\nnumerous unexpected technical and engineering challenges, particularly on loss\nspikes and divergence. In this paper, we introduce the training process of\nGLM-130B including its design choices, training strategies for both efficiency\nand stability, and engineering efforts. The resultant GLM-130B model offers\nsignificant outperformance over GPT-3 175B (davinci) on a wide range of popular\nEnglish benchmarks while the performance advantage is not observed in OPT-175B\nand BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN\n3.0 260B -- the largest Chinese language model -- across related benchmarks.\nFinally, we leverage a unique scaling property of GLM-130B to reach INT4\nquantization without post training, with almost no performance loss, making it\nthe first among 100B-scale models and more importantly, allowing its effective\ninference on 4$\\times$RTX 3090 (24G) or 8$\\times$RTX 2080 Ti (11G) GPUs, the\nmost affordable GPUs required for using 100B-scale models. The GLM-130B model\nweights are publicly accessible and its code, training logs, related toolkit,\nand lessons learned are open-sourced at\n\\url{https://github.com/THUDM/GLM-130B/}.",
            "Large language models (LLMs), such as GPT-4, have shown remarkable\nperformance in natural language processing (NLP) tasks, including challenging\nmathematical reasoning. However, most existing open-source models are only\npre-trained on large-scale internet data and without math-related optimization.\nIn this paper, we present WizardMath, which enhances the mathematical CoT\nreasoning abilities of LLMs without using external python tools, by applying\nour proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method\nto the domain of math. Through extensive experiments on two mathematical\nreasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary\ncapabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses top-tier\nopen-source LLMs by a substantial margin with higher data efficiency.\nFurthermore, WizardMath 70B even outperforms GPT-3.5-Turbo, Claude 2, Gemini\nPro and GPT-4-early-version. Additionally, our preliminary exploration\nhighlights the pivotal role of instruction evolution and process supervision in\nachieving exceptional math performance. For more details refer to\nhttps://github.com/nlpxucan/WizardLM",
            "Large language models (LLMs) have pushed the limits of natural language\nunderstanding and exhibited excellent problem-solving ability. Despite the\ngreat success, most existing open-source LLMs (e.g., LLaMA-2) are still far\naway from satisfactory for solving mathematical problem due to the complex\nreasoning procedures. To bridge this gap, we propose MetaMath, a fine-tuned\nlanguage model that specializes in mathematical reasoning. Specifically, we\nstart by bootstrapping mathematical questions by rewriting the question from\nmultiple perspectives without extra knowledge, which results in a new dataset\ncalled MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA.\nExperimental results on two popular benchmarks (i.e., GSM8K and MATH) for\nmathematical reasoning demonstrate that MetaMath outperforms a suite of\nopen-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4%\non GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the same\nsize by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of\n82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release all the\nMetaMathQA dataset, the MetaMath models with different model sizes and the\ntraining code for public use."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2410.20478",
        "description": "text to music generation",
        "urls": [
            "https://arxiv.org/abs/2410.20478",
            "https://arxiv.org/abs/2302.03917",
            "https://arxiv.org/abs/2301.11325"
        ],
        "citations": [
            3,
            186,
            593
        ],
        "category": [
            "cs.SD",
            "eess.AS",
            "cs.LG",
            "cs.AI"
        ],
        "titles": [
            "MusicFlow: Cascaded Flow Matching for Text Guided Music Generation",
            "Noise2Music: Text-conditioned Music Generation with Diffusion Models",
            "MusicLM: Generating Music From Text"
        ],
        "id": 98,
        "abstracts": [
            "We introduce MusicFlow, a cascaded text-to-music generation model based on\nflow matching. Based on self-supervised representations to bridge between text\ndescriptions and music audios, we construct two flow matching networks to model\nthe conditional distribution of semantic and acoustic features. Additionally,\nwe leverage masked prediction as the training objective, enabling the model to\ngeneralize to other tasks such as music infilling and continuation in a\nzero-shot manner. Experiments on MusicCaps reveal that the music generated by\nMusicFlow exhibits superior quality and text coherence despite being over\n$2\\sim5$ times smaller and requiring $5$ times fewer iterative steps.\nSimultaneously, the model can perform other music generation tasks and achieves\ncompetitive performance in music infilling and continuation. Our code and model\nwill be publicly available.",
            "We introduce Noise2Music, where a series of diffusion models is trained to\ngenerate high-quality 30-second music clips from text prompts. Two types of\ndiffusion models, a generator model, which generates an intermediate\nrepresentation conditioned on text, and a cascader model, which generates\nhigh-fidelity audio conditioned on the intermediate representation and possibly\nthe text, are trained and utilized in succession to generate high-fidelity\nmusic. We explore two options for the intermediate representation, one using a\nspectrogram and the other using audio with lower fidelity. We find that the\ngenerated audio is not only able to faithfully reflect key elements of the text\nprompt such as genre, tempo, instruments, mood, and era, but goes beyond to\nground fine-grained semantics of the prompt. Pretrained large language models\nplay a key role in this story -- they are used to generate paired text for the\naudio of the training set and to extract embeddings of the text prompts\ningested by the diffusion models.\n  Generated examples: https://google-research.github.io/noise2music",
            "We introduce MusicLM, a model generating high-fidelity music from text\ndescriptions such as \"a calming violin melody backed by a distorted guitar\nriff\". MusicLM casts the process of conditional music generation as a\nhierarchical sequence-to-sequence modeling task, and it generates music at 24\nkHz that remains consistent over several minutes. Our experiments show that\nMusicLM outperforms previous systems both in audio quality and adherence to the\ntext description. Moreover, we demonstrate that MusicLM can be conditioned on\nboth text and a melody in that it can transform whistled and hummed melodies\naccording to the style described in a text caption. To support future research,\nwe publicly release MusicCaps, a dataset composed of 5.5k music-text pairs,\nwith rich text descriptions provided by human experts."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2310.06786",
        "description": "Math dataset",
        "urls": [
            "https://arxiv.org/abs/2310.06786",
            "https://arxiv.org/abs/2104.01112",
            "https://arxiv.org/abs/2312.17120"
        ],
        "citations": [
            54,
            62,
            29
        ],
        "category": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.IR"
        ],
        "titles": [
            "OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text",
            "NaturalProofs: Mathematical Theorem Proving in Natural Language",
            "MathPile: A Billion-Token-Scale Pretraining Corpus for Math"
        ],
        "id": 99,
        "abstracts": [
            "There is growing evidence that pretraining on high quality, carefully\nthought-out tokens such as code or mathematics plays an important role in\nimproving the reasoning abilities of large language models. For example,\nMinerva, a PaLM model finetuned on billions of tokens of mathematical documents\nfrom arXiv and the web, reported dramatically improved performance on problems\nthat require quantitative reasoning. However, because all known open source web\ndatasets employ preprocessing that does not faithfully preserve mathematical\nnotation, the benefits of large scale training on quantitive web documents are\nunavailable to the research community. We introduce OpenWebMath, an open\ndataset inspired by these works containing 14.7B tokens of mathematical\nwebpages from Common Crawl. We describe in detail our method for extracting\ntext and LaTeX content and removing boilerplate from HTML documents, as well as\nour methods for quality filtering and deduplication. Additionally, we run\nsmall-scale experiments by training 1.4B parameter language models on\nOpenWebMath, showing that models trained on 14.7B tokens of our dataset surpass\nthe performance of models trained on over 20x the amount of general language\ndata. We hope that our dataset, openly released on the Hugging Face Hub, will\nhelp spur advances in the reasoning abilities of large language models.",
            "Understanding and creating mathematics using natural mathematical language -\nthe mixture of symbolic and natural language used by humans - is a challenging\nand important problem for driving progress in machine learning. As a step in\nthis direction, we develop NaturalProofs, a multi-domain corpus of mathematical\nstatements and their proofs, written in natural mathematical language.\nNaturalProofs unifies broad coverage, deep coverage, and low-resource\nmathematical sources, allowing for evaluating both in-distribution and\nzero-shot generalization. Using NaturalProofs, we benchmark strong neural\nmethods on mathematical reference retrieval and generation tasks which test a\nsystem's ability to determine key results that appear in a proof. Large-scale\nsequence models show promise compared to classical information retrieval\nmethods, yet their performance and out-of-domain generalization leave\nsubstantial room for improvement. NaturalProofs opens many avenues for research\non challenging mathematical tasks.",
            "High-quality, large-scale corpora are the cornerstone of building foundation\nmodels. In this work, we introduce MathPile, a diverse and high-quality\nmath-centric corpus comprising about 9.5 billion tokens. Throughout its\ncreation, we adhered to the principle of \"less is more\", firmly believing in\nthe supremacy of data quality over quantity, even in the pre-training phase.\nOur meticulous data collection and processing efforts included a complex suite\nof preprocessing, prefiltering, language identification, cleaning, filtering,\nand deduplication, ensuring the high quality of our corpus. Furthermore, we\nperformed data contamination detection on downstream benchmark test sets to\neliminate duplicates and conducted continual pre-training experiments, booting\nthe performance on common mathematical reasoning benchmarks. We aim for our\nMathPile to boost language models' mathematical reasoning abilities and\nopen-source its different versions and processing scripts to advance the field."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2202.07123",
        "description": "3D understanding",
        "urls": [
            "https://arxiv.org/abs/2111.14819",
            "https://arxiv.org/abs/2202.07123",
            "https://arxiv.org/abs/2212.05171"
        ],
        "citations": [
            730,
            706,
            234
        ],
        "category": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "titles": [
            "Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling",
            "Rethinking Network Design and Local Geometry in Point Cloud: A Simple Residual MLP Framework",
            "ULIP: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding"
        ],
        "id": 100,
        "abstracts": [
            "We present Point-BERT, a new paradigm for learning Transformers to generalize\nthe concept of BERT to 3D point cloud. Inspired by BERT, we devise a Masked\nPoint Modeling (MPM) task to pre-train point cloud Transformers. Specifically,\nwe first divide a point cloud into several local point patches, and a point\ncloud Tokenizer with a discrete Variational AutoEncoder (dVAE) is designed to\ngenerate discrete point tokens containing meaningful local information. Then,\nwe randomly mask out some patches of input point clouds and feed them into the\nbackbone Transformers. The pre-training objective is to recover the original\npoint tokens at the masked locations under the supervision of point tokens\nobtained by the Tokenizer. Extensive experiments demonstrate that the proposed\nBERT-style pre-training strategy significantly improves the performance of\nstandard point cloud Transformers. Equipped with our pre-training strategy, we\nshow that a pure Transformer architecture attains 93.8% accuracy on ModelNet40\nand 83.1% accuracy on the hardest setting of ScanObjectNN, surpassing carefully\ndesigned point cloud models with much fewer hand-made designs. We also\ndemonstrate that the representations learned by Point-BERT transfer well to new\ntasks and domains, where our models largely advance the state-of-the-art of\nfew-shot point cloud classification task. The code and pre-trained models are\navailable at https://github.com/lulutang0608/Point-BERT",
            "Point cloud analysis is challenging due to irregularity and unordered data\nstructure. To capture the 3D geometries, prior works mainly rely on exploring\nsophisticated local geometric extractors using convolution, graph, or attention\nmechanisms. These methods, however, incur unfavorable latency during inference,\nand the performance saturates over the past few years. In this paper, we\npresent a novel perspective on this task. We notice that detailed local\ngeometrical information probably is not the key to point cloud analysis -- we\nintroduce a pure residual MLP network, called PointMLP, which integrates no\nsophisticated local geometrical extractors but still performs very\ncompetitively. Equipped with a proposed lightweight geometric affine module,\nPointMLP delivers the new state-of-the-art on multiple datasets. On the\nreal-world ScanObjectNN dataset, our method even surpasses the prior best\nmethod by 3.3% accuracy. We emphasize that PointMLP achieves this strong\nperformance without any sophisticated operations, hence leading to a superior\ninference speed. Compared to most recent CurveNet, PointMLP trains 2x faster,\ntests 7x faster, and is more accurate on ModelNet40 benchmark. We hope our\nPointMLP may help the community towards a better understanding of point cloud\nanalysis. The code is available at https://github.com/ma-xu/pointMLP-pytorch.",
            "The recognition capabilities of current state-of-the-art 3D models are\nlimited by datasets with a small number of annotated data and a pre-defined set\nof categories. In its 2D counterpart, recent advances have shown that similar\nproblems can be significantly alleviated by employing knowledge from other\nmodalities, such as language. Inspired by this, leveraging multimodal\ninformation for 3D modality could be promising to improve 3D understanding\nunder the restricted data regime, but this line of research is not well\nstudied. Therefore, we introduce ULIP to learn a unified representation of\nimages, texts, and 3D point clouds by pre-training with object triplets from\nthe three modalities. To overcome the shortage of training triplets, ULIP\nleverages a pre-trained vision-language model that has already learned a common\nvisual and textual space by training with massive image-text pairs. Then, ULIP\nlearns a 3D representation space aligned with the common image-text space,\nusing a small number of automatically synthesized triplets. ULIP is agnostic to\n3D backbone networks and can easily be integrated into any 3D architecture.\nExperiments show that ULIP effectively improves the performance of multiple\nrecent 3D backbones by simply pre-training them on ShapeNet55 using our\nframework, achieving state-of-the-art performance in both standard 3D\nclassification and zero-shot 3D classification on ModelNet40 and ScanObjectNN.\nULIP also improves the performance of PointMLP by around 3% in 3D\nclassification on ScanObjectNN, and outperforms PointCLIP by 28.8% on top-1\naccuracy for zero-shot 3D classification on ModelNet40. Our code and\npre-trained models are released at https://github.com/salesforce/ULIP."
        ]
    },
    {
        "source": "https://arxiv.org/abs/2202.07123",
        "description": "3D understanding",
        "urls": [
            "https://arxiv.org/abs/1903.10318v2",
            "https://arxiv.org/abs/1909.08089v1",
            "https://arxiv.org/abs/2004.08795v1"
        ],
        "citations": [
            664,
            196,
            565
        ],
        "category": [
            "cs.CL"
        ],
        "titles": [
            "Fine-tune BERT for Extractive Summarization",
            "Extractive Summarization of Long Documents by Combining Global and Local Context",
            "Extractive Summarization as Text Matching"
        ],
        "id": 101,
        "abstracts": [
            "BERT, a pre-trained Transformer model, has achieved ground-breaking\nperformance on multiple NLP tasks. In this paper, we describe BERTSUM, a simple\nvariant of BERT, for extractive summarization. Our system is the state of the\nart on the CNN/Dailymail dataset, outperforming the previous best-performed\nsystem by 1.65 on ROUGE-L. The codes to reproduce our results are available at\nhttps://github.com/nlpyang/BertSum",
            "In this paper, we propose a novel neural single document extractive\nsummarization model for long documents, incorporating both the global context\nof the whole document and the local context within the current topic. We\nevaluate the model on two datasets of scientific papers, Pubmed and arXiv,\nwhere it outperforms previous work, both extractive and abstractive models, on\nROUGE-1, ROUGE-2 and METEOR scores. We also show that, consistently with our\ngoal, the benefits of our method become stronger as we apply it to longer\ndocuments. Rather surprisingly, an ablation study indicates that the benefits\nof our model seem to come exclusively from modeling the local context, even for\nthe longest documents.",
            "This paper creates a paradigm shift with regard to the way we build neural\nextractive summarization systems. Instead of following the commonly used\nframework of extracting sentences individually and modeling the relationship\nbetween sentences, we formulate the extractive summarization task as a semantic\ntext matching problem, in which a source document and candidate summaries will\nbe (extracted from the original text) matched in a semantic space. Notably,\nthis paradigm shift to semantic matching framework is well-grounded in our\ncomprehensive analysis of the inherent gap between sentence-level and\nsummary-level extractors based on the property of the dataset.\n  Besides, even instantiating the framework with a simple form of a matching\nmodel, we have driven the state-of-the-art extractive result on CNN/DailyMail\nto a new level (44.41 in ROUGE-1). Experiments on the other five datasets also\nshow the effectiveness of the matching framework. We believe the power of this\nmatching-based summarization framework has not been fully exploited. To\nencourage more instantiations in the future, we have released our codes,\nprocessed dataset, as well as generated summaries in\nhttps://github.com/maszhongming/MatchSum."
        ]
    },
    {
        "id": 102,
        "urls": [
            "https://arxiv.org/abs/2210.08917v2",
            "https://arxiv.org/abs/2211.16773v5",
            "https://arxiv.org/abs/2308.00878v1"
        ],
        "citations": [
            10,
            7,
            12
        ],
        "category": [
            "cs.CL"
        ],
        "titles": [
            "Mars: Modeling Context & State Representations with Contrastive Learning for End-to-End Task-Oriented Dialog",
            "KRLS: Improving End-to-End Response Generation in Task Oriented Dialog with Reinforced Keywords Learning",
            "DiactTOD: Learning Generalizable Latent Dialogue Acts for Controllable Task-Oriented Dialogue Systems"
        ],
        "abstracts": [
            "Traditional end-to-end task-oriented dialog systems first convert dialog\ncontext into belief state and action state before generating the system\nresponse. The system response performance is significantly affected by the\nquality of the belief state and action state. We first explore what dialog\ncontext representation is beneficial to improving the quality of the belief\nstate and action state, which further enhances the generated response quality.\nTo tackle our exploration, we propose Mars, an end-to-end task-oriented dialog\nsystem with two contrastive learning strategies to model the relationship\nbetween dialog context and belief/action state representations. Empirical\nresults show dialog context representations, which are more different from\nsemantic state representations, are more conducive to multi-turn task-oriented\ndialog. Moreover, our proposed Mars achieves state-of-the-art performance on\nthe MultiWOZ 2.0, CamRest676, and CrossWOZ.",
            "In task-oriented dialogs (TOD), reinforcement learning (RL) algorithms train\na model to directly optimize response for task-related metrics. However, RL\nneeds to perform exploration, which can be time-consuming due to the slow\nauto-regressive sequence generation process. We investigate an approach to\ncreate a more efficient RL-based algorithm to improve TOD performance in an\noffline setting. First, we use a faster generation procedure that samples from\nindependent next-word distributions after training the language model (LM) with\nsupervised learning. We then introduce a fine-grained reward function to help\nthe model focus on learning key information in a dialog, by measuring the\nimportance and semantic closeness of each generated token. Experiments on the\nMultiWoZ dataset show our new training algorithm, Keywords Reinforcement\nLearning with Next-word Sampling (KRLS), achieves state-of-the-art performance\non the end-to-end response generation task, with a 15% training time reduction\ncompared to a standard RL algorithm using auto-regressive generation.",
            "Dialogue act annotations are important to improve response generation quality\nin task-oriented dialogue systems. However, it can be challenging to use\ndialogue acts to control response generation in a generalizable way because\ndifferent datasets and tasks may have incompatible annotations. While\nalternative methods that utilize latent action spaces or reinforcement learning\ndo not require explicit annotations, they may lack interpretability or face\ndifficulties defining task-specific rewards. In this work, we present a novel\nend-to-end latent dialogue act model (DiactTOD) that represents dialogue acts\nin a latent space. DiactTOD, when pre-trained on a large corpus, is able to\npredict and control dialogue acts to generate controllable responses using\nthese latent representations in a zero-shot fashion. Our approach demonstrates\nstate-of-the-art performance across a wide range of experimental settings on\nthe MultiWOZ dataset, including zero-shot, few-shot, and full data fine-tuning\nwith both end-to-end and policy optimization configurations."
        ]
    },
    {
        "id": 103,
        "urls": [
            "https://arxiv.org/abs/1911.10470v2",
            "https://arxiv.org/abs/2009.12756v2",
            "https://arxiv.org/abs/2010.12527v4"
        ],
        "citations": [
            317,
            65,
            48
        ],
        "category": [
            "cs.CL"
        ],
        "titles": [
            "Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering",
            "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval",
            "Answering Open-Domain Questions of Varying Reasoning Steps from Text"
        ],
        "abstracts": [
            "Answering questions that require multi-hop reasoning at web-scale\nnecessitates retrieving multiple evidence documents, one of which often has\nlittle lexical or semantic relationship to the question. This paper introduces\na new graph-based recurrent retrieval approach that learns to retrieve\nreasoning paths over the Wikipedia graph to answer multi-hop open-domain\nquestions. Our retriever model trains a recurrent neural network that learns to\nsequentially retrieve evidence paragraphs in the reasoning path by conditioning\non the previously retrieved documents. Our reader model ranks the reasoning\npaths and extracts the answer span included in the best reasoning path.\nExperimental results show state-of-the-art results in three open-domain QA\ndatasets, showcasing the effectiveness and robustness of our method. Notably,\nour method achieves significant improvement in HotpotQA, outperforming the\nprevious best model by more than 14 points.",
            "We propose a simple and efficient multi-hop dense retrieval approach for\nanswering complex open-domain questions, which achieves state-of-the-art\nperformance on two multi-hop datasets, HotpotQA and multi-evidence FEVER.\nContrary to previous work, our method does not require access to any\ncorpus-specific information, such as inter-document hyperlinks or\nhuman-annotated entity markers, and can be applied to any unstructured text\ncorpus. Our system also yields a much better efficiency-accuracy trade-off,\nmatching the best published accuracy on HotpotQA while being 10 times faster at\ninference time.",
            "We develop a unified system to answer directly from text open-domain\nquestions that may require a varying number of retrieval steps. We employ a\nsingle multi-task transformer model to perform all the necessary subtasks --\nretrieving supporting facts, reranking them, and predicting the answer from all\nretrieved documents -- in an iterative fashion. We avoid crucial assumptions of\nprevious work that do not transfer well to real-world settings, including\nexploiting knowledge of the fixed number of retrieval steps required to answer\neach question or using structured metadata like knowledge bases or web links\nthat have limited availability. Instead, we design a system that can answer\nopen-domain questions on any text collection without prior knowledge of\nreasoning complexity. To emulate this setting, we construct a new benchmark,\ncalled BeerQA, by combining existing one- and two-step datasets with a new\ncollection of 530 questions that require three Wikipedia pages to answer,\nunifying Wikipedia corpora versions in the process. We show that our model\ndemonstrates competitive performance on both existing benchmarks and this new\nbenchmark. We make the new benchmark available at https://beerqa.github.io/."
        ]
    },
    {
        "id": 104,
        "urls": [
            "https://arxiv.org/abs/2203.07586v1",
            "https://arxiv.org/abs/2211.10247v2",
            "https://arxiv.org/abs/2408.15801v1"
        ],
        "citations": [
            53,
            8,
            0
        ],
        "category": [
            "cs.CL"
        ],
        "titles": [
            "Long Document Summarization with Top-down and Bottom-up Inference",
            "GoSum: Extractive Summarization of Long Documents by Reinforcement Learning and Graph Organized discourse state",
            "Scaling Up Summarization: Leveraging Large Language Models for Long Text Extractive Summarization"
        ],
        "abstracts": [
            "Text summarization aims to condense long documents and retain key\ninformation. Critical to the success of a summarization model is the faithful\ninference of latent representations of words or tokens in the source documents.\nMost recent models infer the latent representations with a transformer encoder,\nwhich is purely bottom-up. Also, self-attention-based inference models face the\nchallenge of quadratic complexity with respect to sequence length. We propose a\nprincipled inference framework to improve summarization models on these two\naspects. Our framework assumes a hierarchical latent structure of a document\nwhere the top-level captures the long range dependency at a coarser time scale\nand the bottom token level preserves the details. Critically, this hierarchical\nstructure enables token representations to be updated in both a bottom-up and\ntop-down manner. In the bottom-up pass, token representations are inferred with\nlocal self-attention to leverage its efficiency. Top-down correction is then\napplied to allow tokens to capture long-range dependency. We demonstrate the\neffectiveness of the proposed framework on a diverse set of summarization\ndatasets, including narrative, conversational, scientific documents and news.\nOur model achieves (1) competitive or better performance on short documents\nwith higher memory and compute efficiency, compared to full attention\ntransformers, and (2) state-of-the-art performance on a wide range of long\ndocument summarization benchmarks, compared to recent efficient transformers.\nWe also show that our model can summarize an entire book and achieve\ncompetitive performance using $0.27\\%$ parameters (464M vs. 175B) and much less\ntraining data, compared to a recent GPT-3-based model. These results indicate\nthe general applicability and benefits of the proposed framework.",
            "Extracting summaries from long documents can be regarded as sentence\nclassification using the structural information of the documents. How to use\nsuch structural information to summarize a document is challenging. In this\npaper, we propose GoSum, a novel graph and reinforcement learning based\nextractive model for long-paper summarization. In particular, GoSum encodes\nsentence states in reinforcement learning by building a heterogeneous graph for\neach input document at different discourse levels. An edge in the graph\nreflects the discourse hierarchy of a document for restraining the semantic\ndrifts across section boundaries. We evaluate GoSum on two datasets of\nscientific articles summarization: PubMed and arXiv. The experimental results\nhave demonstrated that GoSum achieve state-of-the-art results compared with\nstrong baselines of both extractive and abstractive models. The ablation\nstudies further validate that the performance of our GoSum benefits from the\nuse of discourse information.",
            "In an era where digital text is proliferating at an unprecedented rate,\nefficient summarization tools are becoming indispensable. While Large Language\nModels (LLMs) have been successfully applied in various NLP tasks, their role\nin extractive text summarization remains underexplored. This paper introduces\nEYEGLAXS (Easy Yet Efficient larGe LAnguage model for eXtractive\nSummarization), a framework that leverages LLMs, specifically LLAMA2-7B and\nChatGLM2-6B, for extractive summarization of lengthy text documents. Instead of\nabstractive methods, which often suffer from issues like factual inaccuracies\nand hallucinations, EYEGLAXS focuses on extractive summarization to ensure\nfactual and grammatical integrity. Utilizing state-of-the-art techniques such\nas Flash Attention and Parameter-Efficient Fine-Tuning (PEFT), EYEGLAXS\naddresses the computational and resource challenges typically associated with\nLLMs. The system sets new performance benchmarks on well-known datasets like\nPubMed and ArXiv. Furthermore, we extend our research through additional\nanalyses that explore the adaptability of LLMs in handling different sequence\nlengths and their efficiency in training on smaller datasets. These\ncontributions not only set a new standard in the field but also open up\npromising avenues for future research in extractive text summarization."
        ]
    },
    {
        "id": 105,
        "urls": [
            "https://arxiv.org/abs/2006.01563v2",
            "https://arxiv.org/abs/2011.06993v2",
            "https://arxiv.org/abs/2105.03654v3"
        ],
        "citations": [
            91,
            154,
            168
        ],
        "category": [
            "cs.CL",
            "cs.AI",
            "cs.LG"
        ],
        "titles": [
            "Exploring Cross-sentence Contexts for Named Entity Recognition with BERT",
            "FLERT: Document-Level Features for Named Entity Recognition",
            "Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning"
        ],
        "abstracts": [
            "Named entity recognition (NER) is frequently addressed as a sequence\nclassification task where each input consists of one sentence of text. It is\nnevertheless clear that useful information for the task can often be found\noutside of the scope of a single-sentence context. Recently proposed\nself-attention models such as BERT can both efficiently capture long-distance\nrelationships in input as well as represent inputs consisting of several\nsentences, creating new opportunitites for approaches that incorporate\ncross-sentence information in natural language processing tasks. In this paper,\nwe present a systematic study exploring the use of cross-sentence information\nfor NER using BERT models in five languages. We find that adding context in the\nform of additional sentences to BERT input systematically increases NER\nperformance on all of the tested languages and models. Including multiple\nsentences in each input also allows us to study the predictions of the same\nsentences in different contexts. We propose a straightforward method,\nContextual Majority Voting (CMV), to combine different predictions for\nsentences and demonstrate this to further increase NER performance with BERT.\nOur approach does not require any changes to the underlying BERT architecture,\nrather relying on restructuring examples for training and prediction.\nEvaluation on established datasets, including the CoNLL'02 and CoNLL'03 NER\nbenchmarks, demonstrates that our proposed approach can improve on the\nstate-of-the-art NER results on English, Dutch, and Finnish, achieves the best\nreported BERT-based results on German, and is on par with performance reported\nwith other BERT-based approaches in Spanish. We release all methods implemented\nin this work under open licenses.",
            "Current state-of-the-art approaches for named entity recognition (NER)\ntypically consider text at the sentence-level and thus do not model information\nthat crosses sentence boundaries. However, the use of transformer-based models\nfor NER offers natural options for capturing document-level features. In this\npaper, we perform a comparative evaluation of document-level features in the\ntwo standard NER architectures commonly considered in the literature, namely\n\"fine-tuning\" and \"feature-based LSTM-CRF\". We evaluate different\nhyperparameters for document-level features such as context window size and\nenforcing document-locality. We present experiments from which we derive\nrecommendations for how to model document context and present new\nstate-of-the-art scores on several CoNLL-03 benchmark datasets. Our approach is\nintegrated into the Flair framework to facilitate reproduction of our\nexperiments.",
            "Recent advances in Named Entity Recognition (NER) show that document-level\ncontexts can significantly improve model performance. In many application\nscenarios, however, such contexts are not available. In this paper, we propose\nto find external contexts of a sentence by retrieving and selecting a set of\nsemantically relevant texts through a search engine, with the original sentence\nas the query. We find empirically that the contextual representations computed\non the retrieval-based input view, constructed through the concatenation of a\nsentence and its external contexts, can achieve significantly improved\nperformance compared to the original input view based only on the sentence.\nFurthermore, we can improve the model performance of both input views by\nCooperative Learning, a training method that encourages the two input views to\nproduce similar contextual representations or output label distributions.\nExperiments show that our approach can achieve new state-of-the-art performance\non 8 NER data sets across 5 domains."
        ]
    },
    {
        "id": 106,
        "urls": [
            "https://arxiv.org/abs/2403.14598v1",
            "https://arxiv.org/abs/2407.17331v2",
            "https://arxiv.org/abs/2411.17606v2"
        ],
        "citations": [
            19,
            1,
            1
        ],
        "category": [
            "cs.CV"
        ],
        "titles": [
            "PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model",
            "Multi-label Cluster Discrimination for Visual Representation Learning",
            "HyperSeg: Towards Universal Visual Segmentation with Large Language Model"
        ],
        "abstracts": [
            "PSALM is a powerful extension of the Large Multi-modal Model (LMM) to address\nthe segmentation task challenges. To overcome the limitation of the LMM being\nlimited to textual output, PSALM incorporates a mask decoder and a\nwell-designed input schema to handle a variety of segmentation tasks. This\nschema includes images, task instructions, conditional prompts, and mask\ntokens, which enable the model to generate and classify segmentation masks\neffectively. The flexible design of PSALM supports joint training across\nmultiple datasets and tasks, leading to improved performance and task\ngeneralization. PSALM achieves superior results on several benchmarks, such as\nRefCOCO/RefCOCO+/RefCOCOg, COCO Panoptic Segmentation, and COCO-Interactive,\nand further exhibits zero-shot capabilities on unseen tasks, such as\nopen-vocabulary segmentation, generalized referring expression segmentation and\nvideo object segmentation, making a significant step towards a GPT moment in\ncomputer vision. Through extensive experiments, PSALM demonstrates its\npotential to transform the domain of image segmentation, leveraging the robust\nvisual understanding capabilities of LMMs as seen in natural language\nprocessing. Code and models are available at https://github.com/zamling/PSALM.",
            "Contrastive Language Image Pre-training (CLIP) has recently demonstrated\nsuccess across various tasks due to superior feature representation empowered\nby image-text contrastive learning. However, the instance discrimination method\nused by CLIP can hardly encode the semantic structure of training data. To\nhandle this limitation, cluster discrimination has been proposed through\niterative cluster assignment and classification. Nevertheless, most cluster\ndiscrimination approaches only define a single pseudo-label for each image,\nneglecting multi-label signals in the image. In this paper, we propose a novel\nMulti-Label Cluster Discrimination method named MLCD to enhance representation\nlearning. In the clustering step, we first cluster the large-scale LAION-400M\ndataset into one million centers based on off-the-shelf embedding features.\nConsidering that natural images frequently contain multiple visual objects or\nattributes, we select the multiple closest centers as auxiliary class labels.\nIn the discrimination step, we design a novel multi-label classification loss,\nwhich elegantly separates losses from positive classes and negative classes,\nand alleviates ambiguity on decision boundary. We validate the proposed\nmulti-label cluster discrimination method with experiments on different scales\nof models and pre-training datasets. Experimental results show that our method\nachieves state-of-the-art performance on multiple downstream tasks including\nlinear probe, zero-shot classification, and image-text retrieval. Code and\nmodels have been released at https://github.com/deepglint/unicom .",
            "This paper aims to address universal segmentation for image and video\nperception with the strong reasoning ability empowered by Visual Large Language\nModels (VLLMs). Despite significant progress in current unified segmentation\nmethods, limitations in adaptation to both image and video scenarios, as well\nas the complex reasoning segmentation, make it difficult for them to handle\nvarious challenging instructions and achieve an accurate understanding of\nfine-grained vision-language correlations. We propose HyperSeg, the first\nVLLM-based universal segmentation model for pixel-level image and video\nperception, encompassing generic segmentation tasks and more complex reasoning\nperception tasks requiring powerful reasoning abilities and world knowledge.\nBesides, to fully leverage the recognition capabilities of VLLMs and the\nfine-grained visual information, HyperSeg incorporates hybrid entity\nrecognition and fine-grained visual perceiver modules for various segmentation\ntasks. Combined with the temporal adapter, HyperSeg achieves a comprehensive\nunderstanding of temporal information. Experimental results validate the\neffectiveness of our insights in resolving universal image and video\nsegmentation tasks, including the more complex reasoning perception tasks. Our\ncode is available."
        ]
    },
    {
        "id": 107,
        "urls": [
            "https://arxiv.org/abs/2007.08270v1",
            "https://arxiv.org/abs/2109.11404v1",
            "https://arxiv.org/abs/2208.10128v1"
        ],
        "citations": [
            229,
            139,
            54
        ],
        "category": [
            "cs.CV"
        ],
        "titles": [
            "Kernelized Memory Network for Video Object Segmentation",
            "Hierarchical Memory Matching Network for Video Object Segmentation",
            "SWEM: Towards Real-Time Video Object Segmentation with Sequential Weighted Expectation-Maximization"
        ],
        "abstracts": [
            "Semi-supervised video object segmentation (VOS) is a task that involves\npredicting a target object in a video when the ground truth segmentation mask\nof the target object is given in the first frame. Recently, space-time memory\nnetworks (STM) have received significant attention as a promising solution for\nsemi-supervised VOS. However, an important point is overlooked when applying\nSTM to VOS. The solution (STM) is non-local, but the problem (VOS) is\npredominantly local. To solve the mismatch between STM and VOS, we propose a\nkernelized memory network (KMN). Before being trained on real videos, our KMN\nis pre-trained on static images, as in previous works. Unlike in previous\nworks, we use the Hide-and-Seek strategy in pre-training to obtain the best\npossible results in handling occlusions and segment boundary extraction. The\nproposed KMN surpasses the state-of-the-art on standard benchmarks by a\nsignificant margin (+5% on DAVIS 2017 test-dev set). In addition, the runtime\nof KMN is 0.12 seconds per frame on the DAVIS 2016 validation set, and the KMN\nrarely requires extra computation, when compared with STM.",
            "We present Hierarchical Memory Matching Network (HMMN) for semi-supervised\nvideo object segmentation. Based on a recent memory-based method [33], we\npropose two advanced memory read modules that enable us to perform memory\nreading in multiple scales while exploiting temporal smoothness. We first\npropose a kernel guided memory matching module that replaces the non-local\ndense memory read, commonly adopted in previous memory-based methods. The\nmodule imposes the temporal smoothness constraint in the memory read, leading\nto accurate memory retrieval. More importantly, we introduce a hierarchical\nmemory matching scheme and propose a top-k guided memory matching module in\nwhich memory read on a fine-scale is guided by that on a coarse-scale. With the\nmodule, we perform memory read in multiple scales efficiently and leverage both\nhigh-level semantic and low-level fine-grained memory features to predict\ndetailed object masks. Our network achieves state-of-the-art performance on the\nvalidation sets of DAVIS 2016/2017 (90.8% and 84.7%) and YouTube-VOS 2018/2019\n(82.6% and 82.5%), and test-dev set of DAVIS 2017 (78.6%). The source code and\nmodel are available online: https://github.com/Hongje/HMMN.",
            "Matching-based methods, especially those based on space-time memory, are\nsignificantly ahead of other solutions in semi-supervised video object\nsegmentation (VOS). However, continuously growing and redundant template\nfeatures lead to an inefficient inference. To alleviate this, we propose a\nnovel Sequential Weighted Expectation-Maximization (SWEM) network to greatly\nreduce the redundancy of memory features. Different from the previous methods\nwhich only detect feature redundancy between frames, SWEM merges both\nintra-frame and inter-frame similar features by leveraging the sequential\nweighted EM algorithm. Further, adaptive weights for frame features endow SWEM\nwith the flexibility to represent hard samples, improving the discrimination of\ntemplates. Besides, the proposed method maintains a fixed number of template\nfeatures in memory, which ensures the stable inference complexity of the VOS\nsystem. Extensive experiments on commonly used DAVIS and YouTube-VOS datasets\nverify the high efficiency (36 FPS) and high performance (84.3\\%\n$\\mathcal{J}\\&\\mathcal{F}$ on DAVIS 2017 validation dataset) of SWEM. Code is\navailable at: https://github.com/lmm077/SWEM."
        ]
    },
    {
        "id": 108,
        "urls": [
            "https://arxiv.org/abs/2101.00390v2",
            "https://arxiv.org/abs/2110.07982v1",
            "https://arxiv.org/abs/2206.12693v1"
        ],
        "citations": [
            492,
            15,
            2
        ],
        "category": [
            "cs.CL",
            "eess.AS",
            "cs.SD",
            "F.2.1; I.2.6; I.2.7"
        ],
        "titles": [
            "VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation",
            "Scribosermo: Fast Speech-to-Text models for German and other Languages",
            "TEVR: Improving Speech Recognition by Token Entropy Variance Reduction"
        ],
        "abstracts": [
            "We introduce VoxPopuli, a large-scale multilingual corpus providing 100K\nhours of unlabelled speech data in 23 languages. It is the largest open data to\ndate for unsupervised representation learning as well as semi-supervised\nlearning. VoxPopuli also contains 1.8K hours of transcribed speeches in 16\nlanguages and their aligned oral interpretations into 5 other languages\ntotaling 5.1K hours. We provide speech recognition baselines and validate the\nversatility of VoxPopuli unlabelled data in semi-supervised learning under\nchallenging out-of-domain settings. We will release the corpus at\nhttps://github.com/facebookresearch/voxpopuli under an open license.",
            "Recent Speech-to-Text models often require a large amount of hardware\nresources and are mostly trained in English. This paper presents Speech-to-Text\nmodels for German, as well as for Spanish and French with special features: (a)\nThey are small and run in real-time on microcontrollers like a RaspberryPi. (b)\nUsing a pretrained English model, they can be trained on consumer-grade\nhardware with a relatively small dataset. (c) The models are competitive with\nother solutions and outperform them in German. In this respect, the models\ncombine advantages of other approaches, which only include a subset of the\npresented features. Furthermore, the paper provides a new library for handling\ndatasets, which is focused on easy extension with additional datasets and shows\nan optimized way for transfer-learning new languages using a pretrained model\nfrom another language with a similar alphabet.",
            "This paper presents TEVR, a speech recognition model designed to minimize the\nvariation in token entropy w.r.t. to the language model. This takes advantage\nof the fact that if the language model will reliably and accurately predict a\ntoken anyway, then the acoustic model doesn't need to be accurate in\nrecognizing it. We train German ASR models with 900 million parameters and show\nthat on CommonVoice German, TEVR scores a very competitive 3.64% word error\nrate, which outperforms the best reported results by a relative 16.89%\nreduction in word error rate. We hope that releasing our fully trained speech\nrecognition pipeline to the community will lead to privacy-preserving offline\nvirtual assistants in the future."
        ]
    },
    {
        "id": 109,
        "urls": [
            "https://arxiv.org/abs/2405.15863v2",
            "https://arxiv.org/abs/2406.04673v1",
            "https://arxiv.org/abs/2409.00587v2"
        ],
        "citations": [
            4,
            9,
            6
        ],
        "category": [
            "eess.AS",
            "cs.SD",
            "cs.AI",
            "cs.CV",
            "cs.MM"
        ],
        "titles": [
            "QA-MDT: Quality-aware Masked Diffusion Transformer for Enhanced Music Generation",
            "MeLFusion: Synthesizing Music from Image and Language Cues using Diffusion Models",
            "FLUX that Plays Music"
        ],
        "abstracts": [
            "In recent years, diffusion-based text-to-music (TTM) generation has gained\nprominence, offering an innovative approach to synthesizing musical content\nfrom textual descriptions. Achieving high accuracy and diversity in this\ngeneration process requires extensive, high-quality data, including both\nhigh-fidelity audio waveforms and detailed text descriptions, which often\nconstitute only a small portion of available datasets. In open-source datasets,\nissues such as low-quality music waveforms, mislabeling, weak labeling, and\nunlabeled data significantly hinder the development of music generation models.\nTo address these challenges, we propose a novel paradigm for high-quality music\ngeneration that incorporates a quality-aware training strategy, enabling\ngenerative models to discern the quality of input music waveforms during\ntraining. Leveraging the unique properties of musical signals, we first adapted\nand implemented a masked diffusion transformer (MDT) model for the TTM task,\ndemonstrating its distinct capacity for quality control and enhanced\nmusicality. Additionally, we address the issue of low-quality captions in TTM\nwith a caption refinement data processing approach. Experiments demonstrate our\nstate-of-the-art (SOTA) performance on MusicCaps and the Song-Describer\nDataset. Our demo page can be accessed at https://qa-mdt.github.io/.",
            "Music is a universal language that can communicate emotions and feelings. It\nforms an essential part of the whole spectrum of creative media, ranging from\nmovies to social media posts. Machine learning models that can synthesize music\nare predominantly conditioned on textual descriptions of it. Inspired by how\nmusicians compose music not just from a movie script, but also through\nvisualizations, we propose MeLFusion, a model that can effectively use cues\nfrom a textual description and the corresponding image to synthesize music.\nMeLFusion is a text-to-music diffusion model with a novel \"visual synapse\",\nwhich effectively infuses the semantics from the visual modality into the\ngenerated music. To facilitate research in this area, we introduce a new\ndataset MeLBench, and propose a new evaluation metric IMSM. Our exhaustive\nexperimental evaluation suggests that adding visual information to the music\nsynthesis pipeline significantly improves the quality of generated music,\nmeasured both objectively and subjectively, with a relative gain of up to\n67.98% on the FAD score. We hope that our work will gather attention to this\npragmatic, yet relatively under-explored research area.",
            "This paper explores a simple extension of diffusion-based rectified flow\nTransformers for text-to-music generation, termed as FluxMusic. Generally,\nalong with design in advanced\nFlux\\footnote{https://github.com/black-forest-labs/flux} model, we transfers it\ninto a latent VAE space of mel-spectrum. It involves first applying a sequence\nof independent attention to the double text-music stream, followed by a stacked\nsingle music stream for denoised patch prediction. We employ multiple\npre-trained text encoders to sufficiently capture caption semantic information\nas well as inference flexibility. In between, coarse textual information, in\nconjunction with time step embeddings, is utilized in a modulation mechanism,\nwhile fine-grained textual details are concatenated with the music patch\nsequence as inputs. Through an in-depth study, we demonstrate that rectified\nflow training with an optimized architecture significantly outperforms\nestablished diffusion methods for the text-to-music task, as evidenced by\nvarious automatic metrics and human preference evaluations. Our experimental\ndata, code, and model weights are made publicly available at:\n\\url{https://github.com/feizc/FluxMusic}."
        ]
    },
    {
        "id": 110,
        "urls": [
            "https://arxiv.org/abs/1910.04209v3",
            "https://arxiv.org/abs/2008.00623v2",
            "https://arxiv.org/abs/2109.01652v5"
        ],
        "citations": [
            88,
            144,
            3441
        ],
        "category": [
            "cs.LG",
            "cs.CL",
            "cs.NE",
            "stat.ML"
        ],
        "titles": [
            "On the adequacy of untuned warmup for adaptive optimization",
            "DeLighT: Deep and Light-weight Transformer",
            "Finetuned Language Models Are Zero-Shot Learners"
        ],
        "abstracts": [
            "Adaptive optimization algorithms such as Adam are widely used in deep\nlearning. The stability of such algorithms is often improved with a warmup\nschedule for the learning rate. Motivated by the difficulty of choosing and\ntuning warmup schedules, recent work proposes automatic variance rectification\nof Adam's adaptive learning rate, claiming that this rectified approach\n(\"RAdam\") surpasses the vanilla Adam algorithm and reduces the need for\nexpensive tuning of Adam with warmup. In this work, we refute this analysis and\nprovide an alternative explanation for the necessity of warmup based on the\nmagnitude of the update term, which is of greater relevance to training\nstability. We then provide some \"rule-of-thumb\" warmup schedules, and we\ndemonstrate that simple untuned warmup of Adam performs more-or-less\nidentically to RAdam in typical practical settings. We conclude by suggesting\nthat practitioners stick to linear warmup with Adam, with a sensible default\nbeing linear warmup over $2 / (1 - \\beta_2)$ training iterations.",
            "We introduce a deep and light-weight transformer, DeLighT, that delivers\nsimilar or better performance than standard transformer-based models with\nsignificantly fewer parameters. DeLighT more efficiently allocates parameters\nboth (1) within each Transformer block using the DeLighT transformation, a deep\nand light-weight transformation, and (2) across blocks using block-wise\nscaling, which allows for shallower and narrower DeLighT blocks near the input\nand wider and deeper DeLighT blocks near the output. Overall, DeLighT networks\nare 2.5 to 4 times deeper than standard transformer models and yet have fewer\nparameters and operations. Experiments on benchmark machine translation and\nlanguage modeling tasks show that DeLighT matches or improves the performance\nof baseline Transformers with 2 to 3 times fewer parameters on average. Our\nsource code is available at: \\url{https://github.com/sacmehta/delight}",
            "This paper explores a simple method for improving the zero-shot learning\nabilities of language models. We show that instruction tuning -- finetuning\nlanguage models on a collection of tasks described via instructions --\nsubstantially improves zero-shot performance on unseen tasks.\n  We take a 137B parameter pretrained language model and instruction-tune it on\nover 60 NLP tasks verbalized via natural language instruction templates. We\nevaluate this instruction-tuned model, which we call FLAN, on unseen task\ntypes. FLAN substantially improves the performance of its unmodified\ncounterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we\nevaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE,\nBoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number\nof finetuning datasets, model scale, and natural language instructions are key\nto the success of instruction tuning."
        ]
    }
]