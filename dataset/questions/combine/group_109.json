[
    {
        "paper": [
            "arXiv-2405.15863v2.tex",
            "arXiv-2406.04673v1.tex",
            "arXiv-2409.00587v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the dataset sizes for training in the papers \\\\\\\"QA-MDT: Quality-aware Masked Diffusion Transformer for Enhanced Music Generation\\\\\\\" and \\\\\\\"FLUX that Plays Music\\\\\\\"?",
        "answer": "The paper \\\\\\\"QA-MDT: Quality-aware Masked Diffusion Transformer for Enhanced Music Generation\\\\\\\" uses a training dataset totaling 12.5k hours of diverse music data. In contrast, \\\\\\\"FLUX that Plays Music\\\\\\\" uses a training dataset comprising $\\\\\\\\n22K$ hours of diverse music content.",
        "reference": "QA-MDT: Quality-aware Masked Diffusion Transformer for Enhanced Music Generation; FLUX that Plays Music."
    },
    {
        "paper": [
            "arXiv-2405.15863v2.tex",
            "arXiv-2406.04673v1.tex",
            "arXiv-2409.00587v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do the Fr√©chet Audio Distance (FAD) scores of QA-MDT and FluxMusic compare on the MusicCaps dataset?",
        "answer": "On the MusicCaps dataset, QA-MDT achieves a FAD score of 1.65, while FluxMusic achieves a FAD score of 1.43, indicating that FluxMusic performs slightly better in this aspect.",
        "reference": "QA-MDT: Quality-aware Masked Diffusion Transformer for Enhanced Music Generation; FLUX that Plays Music."
    },
    {
        "paper": [
            "arXiv-2405.15863v2.tex",
            "arXiv-2406.04673v1.tex",
            "arXiv-2409.00587v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What parameter sizes are reported for the models in \\\\\\\"QA-MDT: Quality-aware Masked Diffusion Transformer for Enhanced Music Generation\\\\\\\" and \\\\\\\"FLUX that Plays Music\\\\\\\"?",
        "answer": "In \\\\\\\"QA-MDT: Quality-aware Masked Diffusion Transformer for Enhanced Music Generation\\\\\\\", the QA-MDT model has 675M parameters. In \\\\\\\"FLUX that Plays Music\\\\\\\", the FluxMusic model has 2.1B parameters, indicating that FluxMusic is a much larger model than QA-MDT.",
        "reference": "QA-MDT: Quality-aware Masked Diffusion Transformer for Enhanced Music Generation; FLUX that Plays Music."
    },
    {
        "paper": [
            "arXiv-2405.15863v2.tex",
            "arXiv-2406.04673v1.tex",
            "arXiv-2409.00587v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the CLAP scores achieved by QA-MDT and FluxMusic on the Song-Describer Dataset.",
        "answer": "On the Song-Describer Dataset, QA-MDT achieves a CLAP score of 0.32, while FluxMusic achieves a score of 0.35, indicating that FluxMusic performs slightly better in aligning music with textual descriptions.",
        "reference": "QA-MDT: Quality-aware Masked Diffusion Transformer for Enhanced Music Generation; FLUX that Plays Music."
    },
    {
        "paper": [
            "arXiv-2405.15863v2.tex",
            "arXiv-2406.04673v1.tex",
            "arXiv-2409.00587v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the FAD score compare between \\\"Synthesizing Music from Image and Language Cues using Diffusion Models\\\" and \\\"QA-MDT: Quality-aware Masked Diffusion Transformer for Enhanced Music Generation\\\" on the MusicCaps dataset?",
        "answer": "On the MusicCaps dataset, \\\"Synthesizing Music from Image and Language Cues using Diffusion Models\\\" achieves a FAD score of 1.12, whereas \\\"QA-MDT: Quality-aware Masked Diffusion Transformer for Enhanced Music Generation\\\" achieves a FAD score of 1.65. This shows that \\\"Synthesizing Music from Image and Language Cues using Diffusion Models\\\" performs better in this respect.",
        "reference": "Synthesizing Music from Image and Language Cues using Diffusion Models; QA-MDT: Quality-aware Masked Diffusion Transformer for Enhanced Music Generation."
    },
    {
        "paper": [
            "arXiv-2405.15863v2.tex",
            "arXiv-2406.04673v1.tex",
            "arXiv-2409.00587v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the number of parameters in \\\"Synthesizing Music from Image and Language Cues using Diffusion Models\\\" and \\\"QA-MDT: Quality-aware Masked Diffusion Transformer for Enhanced Music Generation\\\" models.",
        "answer": "\\\"Synthesizing Music from Image and Language Cues using Diffusion Models\\\" does not explicitly mention the number of parameters of their model, while \\\"QA-MDT: Quality-aware Masked Diffusion Transformer for Enhanced Music Generation\\\" specifies having a model with 675M parameters.",
        "reference": "Synthesizing Music from Image and Language Cues using Diffusion Models; QA-MDT: Quality-aware Masked Diffusion Transformer for Enhanced Music Generation."
    }
]