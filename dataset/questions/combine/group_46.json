[
    {
        "paper": [
            "arXiv-2401.13923v2.tex",
            "arXiv-2403.04197v2.tex",
            "arXiv-2406.06777v4.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the 3D-MoLM approach in the '3D-MoLM: Towards 3D Molecule-Text Interpretation in Language Models' paper compare with the MolX method in the 'MolX: Enhancing Large Language Models for Molecular Learning with A Multi-Modal Extension' paper in terms of trainable parameters introduced during LoRA fine-tuning?",
        "answer": "In LoRA fine-tuning, the 3D-MoLM introduces 120.0M (1.74%) trainable parameters while the MolX method introduces 56.6M (0.82%) trainable parameters. Therefore, MolX introduces fewer trainable parameters than 3D-MoLM during LoRA fine-tuning.",
        "reference": "3D-MoLM: Towards 3D Molecule-Text Interpretation in Language Models, MolX: Enhancing Large Language Models for Molecular Learning with A Multi-Modal Extension"
    },
    {
        "paper": [
            "arXiv-2401.13923v2.tex",
            "arXiv-2403.04197v2.tex",
            "arXiv-2406.06777v4.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the performance of 3D-MoLM's and ICMA's specialist models on the molecule captioning task using ROUGE-L metrics based on their respective experiments.",
        "answer": "On the molecule captioning task, 3D-MoLM's specialist model achieves a ROUGE-L score of 31.23 on the PubChem Dataset, whereas ICMA's Mistral-7B achieves a ROUGE-L score of 43.61 on the ChEBI-20 dataset. This suggests that ICMA's performance, specifically Mistral-7B, is superior in terms of ROUGE-L metric compared to 3D-MoLM on their respective datasets.",
        "reference": "3D-MoLM: Towards 3D Molecule-Text Interpretation in Language Models, Large Language Models are In-Context Molecule Learners"
    },
    {
        "paper": [
            "arXiv-2401.13923v2.tex",
            "arXiv-2403.04197v2.tex",
            "arXiv-2406.06777v4.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the differences in dataset size and composition used for pre-training between '3D-MoLM: Towards 3D Molecule-Text Interpretation in Language Models' and 'MolX: Enhancing Large Language Models for Molecular Learning with A Multi-Modal Extension'?",
        "answer": "The '3D-MoLM: Towards 3D Molecule-Text Interpretation in Language Models' uses a pre-training dataset of 316K 3D molecule-text pairs from PubChem, whereas the 'MolX: Enhancing Large Language Models for Molecular Learning with A Multi-Modal Extension' uses a dataset that contains roughly 300K molecule-description pairs collected from the PubChem database. Both datasets are similar in size and drawn from the PubChem data source, but focus on different aspects: '3D-MoLM's dataset is specifically curated for 3D molecules, while 'MolX' focuses on a broad molecule-description task incorporating detailed molecular properties.",
        "reference": "3D-MoLM: Towards 3D Molecule-Text Interpretation in Language Models, MolX: Enhancing Large Language Models for Molecular Learning with A Multi-Modal Extension"
    },
    {
        "paper": [
            "arXiv-2401.13923v2.tex",
            "arXiv-2403.04197v2.tex",
            "arXiv-2406.06777v4.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the BLEU-4 performance between the LoRA fine-tuned 3D-MoLM and the ICMA's Galactica-125M model on molecule captioning tasks from their respective experiments.",
        "answer": "The LoRA fine-tuned 3D-MoLM achieves a BLEU-4 score of 22.52 on the PubChem Dataset, whereas the ICMA's Galactica-125M model achieves a BLEU-4 score of 0.565 on the ChEBI-20 dataset. This indicates that the 3D-MoLM demonstrates a higher BLEU-4 score compared to ICMA's model on their respective datasets.",
        "reference": "3D-MoLM: Towards 3D Molecule-Text Interpretation in Language Models, Large Language Models are In-Context Molecule Learners"
    },
    {
        "paper": [
            "arXiv-2401.13923v2.tex",
            "arXiv-2403.04197v2.tex",
            "arXiv-2406.06777v4.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "In the analysis of Molecule Property Prediction tasks, how do the accuracy scores of LoRA fine-tuned 3D-MoLM and ICMA's Mistral-7B compare in HIV prediction?",
        "answer": "For the HIV prediction task, the LoRA fine-tuned 3D-MoLM reports an accuracy score of 0.972, whereas the ICMA's Mistral-7B shows an accuracy score of 0.960. Compared to ICMA's model, the 3D-MoLM demonstrates a higher accuracy on the HIV prediction task.",
        "reference": "3D-MoLM: Towards 3D Molecule-Text Interpretation in Language Models, Large Language Models are In-Context Molecule Learners"
    },
    {
        "paper": [
            "arXiv-2401.13923v2.tex",
            "arXiv-2403.04197v2.tex",
            "arXiv-2406.06777v4.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do the overall LoRA fine-tuning paradigms differ with respect to trainable parameters in the methods discussed in '3D-MoLM: Towards 3D Molecule-Text Interpretation in Language Models' and those in 'MolX: Enhancing Large Language Models for Molecular Learning with A Multi-Modal Extension'?",
        "answer": "In LoRA fine-tuning, the method in '3D-MoLM' introduces 120.0M (1.74%) trainable parameters, whereas the MolX introduces 56.6M (0.82%) trainable parameters. Therefore, 3D-MoLM uses more trainable parameters compared to the MolX method during LoRA fine-tuning.",
        "reference": "3D-MoLM: Towards 3D Molecule-Text Interpretation in Language Models, MolX: Enhancing Large Language Models for Molecular Learning with A Multi-Modal Extension"
    }
]