[
    {
        "paper": [
            "arXiv-2305.16934v2.tex",
            "arXiv-2403.09346v2.tex",
            "arXiv-2403.09766v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the evaluation results on attack success rate for the 'B-AVIBench' and 'Cross-Prompt Attack' papers. What do they reveal about the effectiveness of their respective adversarial techniques?",
        "answer": "'B-AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Black-box Adversarial Visual-Instructions' reports an average ASR of up to 91% for MiniGPT-4 across various capabilities. 'An Image Is Worth 1000 Lies: Adversarial Transferability across Prompts on Vision-Language Models' reports an ASR up to 98% using CroPA on Flamingo for specific tasks, such as VQA-specific with the target 'unknown'. This reveals that CroPA achieves a high targeted effectiveness under specific settings, whereas 'B-AVIBench' provides a broader view of vulnerability with diverse attack types considered.",
        "reference": "B-AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Black-box Adversarial Visual-Instructions; An Image Is Worth 1000 Lies: Adversarial Transferability across Prompts on Vision-Language Models."
    },
    {
        "paper": [
            "arXiv-2305.16934v2.tex",
            "arXiv-2403.09346v2.tex",
            "arXiv-2403.09766v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the perturbation budgets or constraints used in the adversarial image attacks in \\\\\\\"On Evaluating Adversarial Robustness of Large Vision-Language Models\\\\\\\" compared to those in 'An Image Is Worth 1000 Lies: Adversarial Transferability across Prompts on Vision-Language Models'?",
        "answer": "In \\\\\\\"On Evaluating Adversarial Robustness of Large Vision-Language Models,\\\\\\\" the perturbation budget uses an $\\\\\\\\\\\\\\\\ell_{\\\\\\\\\\\\\\\\infty}$ constraint where \\\\\\\\\\\\\\\\epsilon is set to 8, i.e., \\\\\\\\\\\\\\\\|\\\\\\\\\\\\\\\\boldsymbol{x}_{\\\\\\\\\\\\\\\\text{cle}}-\\\\\\\\\\\\\\\\boldsymbol{x}_{\\\\\\\\\\\\\\\\text{adv}}\\\\\\\\\\\\\\\\|_{\\\\\\\\\\\\\\\\infty} \\\\\\\\\\\\\\\\leq 8. In contrast, 'An Image Is Worth 1000 Lies: Adversarial Transferability across Prompts on Vision-Language Models' uses a perturbation size of 16/255 for the image perturbations. These values ensure that the added perturbations remain imperceptible to human observers.",
        "reference": "On Evaluating Adversarial Robustness of Large Vision-Language Models; An Image Is Worth 1000 Lies: Adversarial Transferability across Prompts on Vision-Language Models."
    },
    {
        "paper": [
            "arXiv-2305.16934v2.tex",
            "arXiv-2403.09346v2.tex",
            "arXiv-2403.09766v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do the papers 'B-AVIBench' and 'Cross-Prompt Attack' differ in the datasets used for evaluating adversarial attacks?",
        "answer": "'B-AVIBench' uses a comprehensive dataset derived from Tiny LVLM-eHub, which includes multiple datasets for different tasks like Image Classification, Object Counting, and others, totaling 316K B-AVIs. On the other hand, 'An Image Is Worth 1000 Lies: Adversarial Transferability across Prompts on Vision-Language Models' uses images from the MS-COCO dataset and prompts from the VQA-v2 dataset, focusing more on specific prompts and their adversarial transferability across different vision-language tasks. This signifies 'B-AVIBench's focus on a broad evaluation across multimodal capabilities compared to the more targeted approach in 'CroPA'.",
        "reference": "B-AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Black-box Adversarial Visual-Instructions; An Image Is Worth 1000 Lies: Adversarial Transferability across Prompts on Vision-Language Models."
    },
    {
        "paper": [
            "arXiv-2305.16934v2.tex",
            "arXiv-2403.09346v2.tex",
            "arXiv-2403.09766v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the success rate of adversarial robustness evaluation against LLaVA or MiniGPT-4 specifically in the 'B-AVIBench' and 'On Evaluating Adversarial Robustness of Large Vision-Language Models'.",
        "answer": "In 'B-AVIBench', LLaVA shows varied ASR around 51% in decision-based optimized attacks and MiniGPT-4 has 91% ASR in image-based B-AVIs, indicating a high vulnerability. In 'On Evaluating Adversarial Robustness of Large Vision-Language Models', MiniGPT-4 shows lower success with MF-ii + MF-tt achieving a CLIP score of 0.614 indicating adversarial effectiveness but less precise compared to B-AVIs' success criteria.",
        "reference": "B-AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Black-box Adversarial Visual-Instructions; On Evaluating Adversarial Robustness of Large Vision-Language Models."
    },
    {
        "paper": [
            "arXiv-2305.16934v2.tex",
            "arXiv-2403.09346v2.tex",
            "arXiv-2403.09766v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the difference in the experimental settings regarding perturbation budgets or success metrics in 'B-AVIBench' and 'An Image Is Worth 1000 Lies: Adversarial Transferability across Prompts on Vision-Language Models'?",
        "answer": "'B-AVIBench' uses a detailed set of robustness scores across different modalities with ASDR and success rates based on perturbation added queries, with maximum queries typically limited to 1500 in decision-based attacks. In contrast, 'An Image Is Worth 1000 Lies' uses a fixed perturbation size of 16/255 and measured success based on whether the target sentence was achieved or not, with a focus on achieving targeted or non-targeted adversarial success rates without specifying the number of queries or granular metric comparisons.",
        "reference": "B-AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Black-box Adversarial Visual-Instructions; An Image Is Worth 1000 Lies: Adversarial Transferability across Prompts on Vision-Language Models."
    },
    {
        "paper": [
            "arXiv-2305.16934v2.tex",
            "arXiv-2403.09346v2.tex",
            "arXiv-2403.09766v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Based on the papers “An Image Is Worth 1000 Lies” and “B-AVIBench”, how many models are evaluated in each paper and how is this reflective of the scope of evaluation in both studies?",
        "answer": "\\\"An Image Is Worth 1000 Lies: Adversarial Transferability across Prompts on Vision-Language Models\\\" evaluates the adversarial performance primarily on three prevalent VLMs: Flamingo, BLIP-2, and InstructBLIP. Meanwhile, \\\"B-AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Black-box Adversarial Visual-Instructions\\\" tests a total of 16 Large Vision-Language Models, including 14 open-source and 2 closed-source, which implies a much broader evaluation scope compared to the three models in the \\\"An Image Is Worth 1000 Lies\\\" study.",
        "reference": "An Image Is Worth 1000 Lies: Adversarial Transferability across Prompts on Vision-Language Models; B-AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Black-box Adversarial Visual-Instructions."
    }
]