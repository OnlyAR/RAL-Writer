[
    {
        "paper": [
            "arXiv-2309.05463v1.tex",
            "arXiv-2404.14219v4.tex",
            "arXiv-2412.08905v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the quantity of training tokens differ between the phi-1.5 and phi-3-mini models, and what effect does it have on their performance?",
        "answer": "The phi-1.5 model was trained on 150 billion tokens, while the phi-3-mini was trained on 3.3 trillion tokens. As a result, phi-3-mini achieves overall performance comparable to models like GPT-3.5, achieving 69% on the MMLU benchmark, whereas phi-1.5, with fewer training tokens, performs comparably to models larger than its 1.3 billion parameters mainly on specific reasoning tasks rather than general benchmarks.",
        "reference": "Textbooks Are All You Need II: phi-1.5 technical report; Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone."
    },
    {
        "paper": [
            "arXiv-2309.05463v1.tex",
            "arXiv-2404.14219v4.tex",
            "arXiv-2412.08905v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the computing resource usage for training phi-1.5 and phi-4 models and explain the potential impact on the accessibility of these models.",
        "answer": "The phi-1.5 model uses a training dataset of 150 billion tokens, consuming 1.5K GPU hours for training, while phi-4 is developed with a training recipe that is centrally focused on data quality, trained over 10 trillion tokens. The significant increase in training resources for phi-4 indicates a higher computational cost, which may affect its accessibility and deployment compared to phi-1.5, which requires less computational power and resources.",
        "reference": "Textbooks Are All You Need II: phi-1.5 technical report; Phi-4 Technical Report."
    },
    {
        "paper": [
            "arXiv-2309.05463v1.tex",
            "arXiv-2404.14219v4.tex",
            "arXiv-2412.08905v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the differences in synthetic data usage between phi-3 and phi-4 models, and how do these differences affect their performance on specific benchmarks?",
        "answer": "Phi-3-mini uses synthetic data in a moderate capacity alongside filtered web data, concentrating on efficiency for smaller models. It achieves 69% on the MMLU benchmark. In contrast, phi-4 heavily relies on synthetic data throughout its training, focusing on STEM tasks and achieving higher scores on benchmarks like GPQA (56.1% vs. phi-3's 31.2%) and MATH (80.4 vs. phi-3's 44.6), indicating a more tailored performance towards reasoning and problem-solving from increased synthetic data use.",
        "reference": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone; Phi-4 Technical Report."
    },
    {
        "paper": [
            "arXiv-2309.05463v1.tex",
            "arXiv-2404.14219v4.tex",
            "arXiv-2412.08905v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do the improvements in empirical results from open-sourcing reflect on the progress from phi-1.5 to phi-4?",
        "answer": "Open-sourcing of phi-1.5 allowed for its evaluation against larger models on natural language tasks indicating its comparable performance in specific reasoning tasks like multi-step reasoning, evidenced by benchmarks such as GSM8K. Progressively, phi-4's development, backed by a significant increase in synthetic data, similarly leverages community feedback for improvements and achieves superior performances, particularly in STEM benchmarks like GPQA and MATH due to advances tailored at expanding reasoning scope and using refined data compositions.",
        "reference": "Textbooks Are All You Need II: phi-1.5 technical report; Phi-4 Technical Report."
    },
    {
        "paper": [
            "arXiv-2309.05463v1.tex",
            "arXiv-2404.14219v4.tex",
            "arXiv-2412.08905v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the size of the training data and parameter count affect the MATH benchmark performance between the phi-3.5-MoE and phi-4 models?",
        "answer": "The phi-3.5-MoE, which is a 16x3.8 billion MoE model with 6.6 billion active parameters, achieves a MATH benchmark score of 59.5 with its large model size contributing to its score even though it doesn't outperform phi-4 in this task. Phi-4, despite being a 14 billion parameter model with extensive focus on quality synthetic data, attains an 80.4 score on MATH benchmarks. The discrepancy suggests that phi-4's training recipe, particularly with synthetic data and post-training optimizations, is more effective for MATH tasks compared to the larger but sparsely activated phi-3.5-MoE.",
        "reference": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone; Phi-4 Technical Report."
    },
    {
        "paper": [
            "arXiv-2309.05463v1.tex",
            "arXiv-2404.14219v4.tex",
            "arXiv-2412.08905v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the GPQA benchmark scores for the phi-3.5-MoE and phi-4 models and explain how data and parameter choices influence performance.",
        "answer": "The phi-3.5-MoE scores 36.8 on the GPQA benchmark, whereas phi-4 achieves a score of 56.1. The phi-4 model's focus on STEM-based synthetic data, enhanced post-training, and its 14 billion parameter count, provide it an edge in specialized question-answer tasks like GPQA, compared to the phi-3.5-MoE's 6.6 billion active parameter sparse model, which excels in general tasks but slightly falls behind in focused benchmarks.",
        "reference": "Phi-4 Technical Report; Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone."
    }
]