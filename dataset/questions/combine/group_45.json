[
    {
        "paper": [
            "arXiv-2401.14818v1.tex",
            "arXiv-2402.09391v4.tex",
            "arXiv-2409.13194v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the token size of the training dataset compare between ChemDFM from \\\"ChemDFM: Dialogue Foundation Model for Chemistry\\\" and \\\\nmodelname from \\\\n\\\"\\\\modelname: Advancing Large Language Models for Chemistry \\\\with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset\\\"?",
        "answer": "ChemDFM is trained on 34 billion tokens from diverse chemical literature and general data, while \\\\nmodelname is trained using the \\\\ndatasetname dataset, which consists of 3.3 million samples, not directly comparable by token size as its focus is on instruction-tuning tasks rather than raw token count.",
        "reference": "\\\"ChemDFM: Dialogue Foundation Model for Chemistry\\\", \\\"\\\\modelname: Advancing Large Language Models for Chemistry \\\\with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset\\\""
    },
    {
        "paper": [
            "arXiv-2401.14818v1.tex",
            "arXiv-2402.09391v4.tex",
            "arXiv-2409.13194v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the number of tasks included in the instruction-tuning dataset of \\\\n\\\"\\\\modelname: Advancing Large Language Models for Chemistry \\\\with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset\\\" with the \\\\nmulti-modality tasks included in \\\"ChemDFM-X: Towards Large Multimodal Model for Chemistry\\\".",
        "answer": "The \\\\ndatasetname from \\\\n\\\"\\\\modelname: Advancing Large Language Models for Chemistry \\\\with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset\\\" includes 14 tasks, whereas ChemDFM-X from \\\"ChemDFM-X: Towards Large Multimodal Model for Chemistry\\\" includes a wider modality but the cross-modality instruction-tuning dataset comprises 7.6 million data points, focusing on five modalities without specifying a number of tasks.",
        "reference": "\\\"\\\\modelname: Advancing Large Language Models for Chemistry \\\\with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset\\\", \\\"ChemDFM-X: Towards Large Multimodal Model for Chemistry\\\""
    },
    {
        "paper": [
            "arXiv-2401.14818v1.tex",
            "arXiv-2402.09391v4.tex",
            "arXiv-2409.13194v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the structural modalities used in \\\"\\\\modelname: Advancing Large Language Models for Chemistry \\\\with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset\\\" and \\\"ChemDFM-X: Towards Large Multimodal Model for Chemistry\\\" and explain the differences involved.",
        "answer": "\\\"\\\\modelname: Advancing Large Language Models for Chemistry \\\\with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset\\\" primarily handles tasks via text-based molecular structures like SMILES. In contrast, \\\"ChemDFM-X: Towards Large Multimodal Model for Chemistry\\\" incorporates multiple structural modalities such as 2D molecular graphs and 3D molecular conformations, targeting them in a broader cross-modal context with multiple input formats.",
        "reference": "\\\"\\\\modelname: Advancing Large Language Models for Chemistry \\\\with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset\\\", \\\"ChemDFM-X: Towards Large Multimodal Model for Chemistry\\\""
    },
    {
        "paper": [
            "arXiv-2401.14818v1.tex",
            "arXiv-2402.09391v4.tex",
            "arXiv-2409.13194v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the number of samples in ChemDFM's instruction tuning compare to the dataset size used for \\\\/modelname in \\\"\\\\modelname: Advancing Large Language Models for Chemistry \\\\with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset\\\"?",
        "answer": "ChemDFM uses 2.7 million instructions in its tuning phase, whereas \\\\modelname in \\\"\\\\modelname: Advancing Large Language Models for Chemistry \\\\with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset\\\" involves 3.3 million samples for instruction tuning. This indicates that \\\\modelname uses a slightly larger dataset by approximately 600,000 samples.",
        "reference": "\\\"ChemDFM: Dialogue Foundation Model for Chemistry\\\", \\\"\\\\modelname: Advancing Large Language Models for Chemistry \\\\with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset\\\""
    },
    {
        "paper": [
            "arXiv-2401.14818v1.tex",
            "arXiv-2402.09391v4.tex",
            "arXiv-2409.13194v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "With respect to task-specific model evaluation, how do the performances of ChemDFM from \\\"ChemDFM: Dialogue Foundation Model for Chemistry\\\" compare against \\\\modelname from \\\"\\\\modelname: Advancing Large Language Models for Chemistry \\\\with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset\\\" in terms of name conversion tasks?",
        "answer": "In the name conversion tasks, ChemDFM achieves 4.0, 11.0, 73.0, and 51.0 in SMILES to IUPAC, IUPAC to SMILES, SMILES to molecular formulas, and IUPAC to molecular formulas respectively, whereas \\\\modelname outperformed ChemDFM reaching 87.9 for IUPAC to formula, 70.1 for IUPAC to SMILES, 99.6 for SMILES to formula, and 29.0 for SMILES to IUPAC.",
        "reference": "\\\"ChemDFM: Dialogue Foundation Model for Chemistry\\\", \\\"\\\\modelname: Advancing Large Language Models for Chemistry \\\\with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset\\\""
    },
    {
        "paper": [
            "arXiv-2401.14818v1.tex",
            "arXiv-2402.09391v4.tex",
            "arXiv-2409.13194v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do the evaluation results for molecular property prediction compare between ChemDFM and ChemDFM-X across two papers?",
        "answer": "The evaluation results for BACE in ChemDFM-X using molecular graphs (S+G) are 76.1 while ChemDFM alone shows 78.4. For HIV, ChemDFM-X using molecular conformations (S+C) gets 76.4, compared to 73.6 for ChemDFM.When considering BBBP, ChemDFM achieved 66.7 while ChemDFM-X (S) achieved with molecular images (I) 65.7.",
        "reference": "\\\"ChemDFM: Dialogue Foundation Model for Chemistry\\\", \\\"ChemDFM-X: Towards Large Multimodal Model for Chemistry\\\""
    }
]