[
    {
        "paper": [
            "arXiv-2101.02235v1.tex",
            "arXiv-2109.01653v1.tex",
            "arXiv-2301.13688v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the total number of claims or questions in the StrategyQA and CREAK datasets?",
        "answer": "The StrategyQA dataset includes a total of 2,780 questions, while the CREAK dataset consists of 13,000 claims.",
        "reference": "Did Aristotle Use a Laptop?; CREAK: A Dataset for Commonsense Reasoning over Entity Knowledge"
    },
    {
        "paper": [
            "arXiv-2101.02235v1.tex",
            "arXiv-2109.01653v1.tex",
            "arXiv-2301.13688v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do the performance gaps between human accuracy and model accuracy on the StrategyQA and \\\\\\\\textsc{CREAK} datasets compare?",
        "answer": "On the StrategyQA dataset, human performance reaches 87\\\\\\\\%, while the best model accuracy is $\\\\\\\\text{\\\\\\\\textasciitilde}66\\\\\\\\text{\\\\\\\\textperthousand}$. On the CREAK dataset, human performance is around 96.3\\\\\\\\\\\\\\\\textperthousand on the dev set and 99\\\\\\\\\\\\\\\\textperthousand on the contrasit dataset, while the best model reported 85.1\\\\\\\\\\\\\\\\textperthousand on the test set. This indicates a larger performance gap on the CREAK dataset, despite the StrategyQA model being evaluated in slightly different setups.",
        "reference": "Did Aristotle Use a Laptop?; CREAK: A Dataset for Commonsense Reasoning over Entity Knowledge"
    },
    {
        "paper": [
            "arXiv-2101.02235v1.tex",
            "arXiv-2109.01653v1.tex",
            "arXiv-2301.13688v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the proportion of implicit/evident reasoning types in the StrategyQA and CREAK datasets.",
        "answer": "In the StrategyQA dataset, 82% of the questions are classified as implicit with respects to the reasoning steps needed. In contrast, the CREAK dataset involves mixed reasoning types, with 54% requiring a combination of retrieval and commonsense reasoning, 28% involving only commonsense reasoning, and the remaining 18% being mainly retrieval-based.",
        "reference": "Did Aristotle Use a Laptop?; CREAK: A Dataset for Commonsense Reasoning over Entity Knowledge"
    },
    {
        "paper": [
            "arXiv-2101.02235v1.tex",
            "arXiv-2109.01653v1.tex",
            "arXiv-2301.13688v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the average number of decomposition steps annotated per question in the StrategyQA and how does this complexity compare to the average length of claims in CREAK?",
        "answer": "In the StrategyQA dataset, the average decomposition length per question is 2.93 steps. In the CREAK dataset, the average length of claims is about 10.8 tokens for train and 9.7/9.9 tokens for dev/test. This indicates that StrategyQA involves multiple steps in reasoning compared to the single statement length of CREAK claims.",
        "reference": "Did Aristotle Use a Laptop?; CREAK: A Dataset for Commonsense Reasoning over Entity Knowledge"
    },
    {
        "paper": [
            "arXiv-2101.02235v1.tex",
            "arXiv-2109.01653v1.tex",
            "arXiv-2301.13688v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the diversity of topics as indicated by the number of unique entities in StrategyQA and CREAK datasets.",
        "answer": "In the StrategyQA dataset, there are 1333 unique terms or entities used to prime the workers, whereas in the CREAK dataset, for the training split alone, there are 2,096 unique entities, showing a comparatively higher initial diversity in the SEED/entity selection for CREAK.",
        "reference": "Did Aristotle Use a Laptop?; CREAK: A Dataset for Commonsense Reasoning over Entity Knowledge"
    },
    {
        "paper": [
            "arXiv-2101.02235v1.tex",
            "arXiv-2109.01653v1.tex",
            "arXiv-2301.13688v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Between the Flan Collection and StrategyQA datasets, which dataset contains more unique reasoning strategies based on annotated reasoning skills?",
        "answer": "The Flan Collection dataset outlines reasoning skills such as task balancing and mixed prompts that bolster performance in instruction tuning. Meanwhile, StrategyQA has a broader range of unique reasoning skills, including 70% related to domain-specific or logical reasoning skills, encompassing diverse strategies like biological, cultural, and entertainment, whereas Flan focuses more on the technical aspects of model improvement.",
        "reference": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning; Did Aristotle Use a Laptop?"
    }

]