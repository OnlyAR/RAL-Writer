[
    {
        "paper": [
            "arXiv-2110.11571v3.tex",
            "arXiv-2202.03423v1.tex",
            "arXiv-2303.06818v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the average attack success rate (ASR) of the Anti-Backdoor Learning (ABL) method compare to the other methods in defending against backdoor attacks on the CIFAR-10 dataset from the paper \\\"Anti-Backdoor Learning: Training Clean Models on Poisoned Data\\\" and \\\"Backdoor Defense via Decoupling the Training Process\\\"?",
        "answer": "In the paper \\\"Anti-Backdoor Learning: Training Clean Models on Poisoned Data\\\", the ABL method achieves an average ASR of 7.69% on CIFAR-10. In comparison, in the paper \\\"Backdoor Defense via Decoupling the Training Process\\\", the Decoupling-based backdoor defense (DBD) method achieves a higher ASR, with specific values depending on the attack scenario, but generally higher than ABL's 7.69%. The ABL method is generally more effective than DBD on CIFAR-10 based on ASR.",
        "reference": "Anti-Backdoor Learning: Training Clean Models on Poisoned Data, Backdoor Defense via Decoupling the Training Process"
    },
    {
        "paper": [
            "arXiv-2110.11571v3.tex",
            "arXiv-2202.03423v1.tex",
            "arXiv-2303.06818v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do the clean accuracy (CA) results compare between the ABL method from the paper \\\"Anti-Backdoor Learning: Training Clean Models on Poisoned Data\\\" and the DBD method from the paper \\\"Backdoor Defense via Decoupling the Training Process\\\" across different datasets?",
        "answer": "In the paper \\\"Anti-Backdoor Learning: Training Clean Models on Poisoned Data\\\", the ABL method achieves an average CA of 84.76% on CIFAR-10, 95.25% on GTSRB, and 86.87% on the ImageNet subset. In the paper \\\"Backdoor Defense via Decoupling the Training Process\\\", the DBD method achieves similar or slightly lower CA results, for instance, less than 86.94% on CIFAR-10 for specific attacks and generally lower performance on GTSRB and ImageNet compared to ABL, highlighting ABL's effectiveness in maintaining clean accuracy across datasets.",
        "reference": "Anti-Backdoor Learning: Training Clean Models on Poisoned Data, Backdoor Defense via Decoupling the Training Process"
    },
    {
        "paper": [
            "arXiv-2110.11571v3.tex",
            "arXiv-2202.03423v1.tex",
            "arXiv-2303.06818v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Which method exhibits the highest improvement in attack success rate (ASR) reduction when moving from no defense to the use of defenses across the 'Anti-Backdoor Learning: Training Clean Models on Poisoned Data' and 'Backdoor Defense via Decoupling the Training Process' papers?",
        "answer": "In the 'Anti-Backdoor Learning: Training Clean Models on Poisoned Data' paper, the ABL method significantly reduces the ASR from around 97.73% without defense to 7.69% with defense on average across CIFAR-10 dataset. In comparison, in the 'Backdoor Defense via Decoupling the Training Process' paper, the DBD method also reduces ASRs but with varying success rates, indicating that ABL typically achieves higher ASR reduction improvement.",
        "reference": "Anti-Backdoor Learning: Training Clean Models on Poisoned Data, Backdoor Defense via Decoupling the Training Process"
    },
    {
        "paper": [
            "arXiv-2110.11571v3.tex",
            "arXiv-2202.03423v1.tex",
            "arXiv-2303.06818v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do the average attack success rate (ASR) and clean accuracy (CA) compare between the Decoupling-based Backdoor Defense (DBD) and Causality-inspired Backdoor Defense (CBD) on the CIFAR-10 dataset?",
        "answer": "On the CIFAR-10 dataset, the Decoupling-based Backdoor Defense (DBD) method achieves an average ASR of approximately 4.53% with a CA of 86.43%, as reported in 'Backdoor Defense via Decoupling the Training Process'. The Causality-inspired Backdoor Defense (CBD), as reported in the corresponding paper, achieves a lower average ASR of 1.60% and a higher CA of 87.00%, indicating better performance of CBD in both metrics.",
        "reference": "Backdoor Defense via Decoupling the Training Process, Backdoor Defense via Deconfounded Representation Learning"
    },
    {
        "paper": [
            "arXiv-2110.11571v3.tex",
            "arXiv-2202.03423v1.tex",
            "arXiv-2303.06818v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the performance of the DBD and Anti-Backdoor Learning (ABL) methods fare against more complex backdoor attacks such as WaNet, as per details reported in 'Backdoor Defense via Decoupling the Training Process' and 'Anti-Backdoor Learning: Training Clean Models on Poisoned Data'?",
        "answer": "In WaNet attacks, the Anti-Backdoor Learning (ABL) method reduces ASR to about 22.24% with a CA of 75.74% on CIFAR-10, according to the 'Anti-Backdoor Learning: Training Clean Models on Poisoned Data' paper. The Decoupling-based Backdoor Defense (DBD) method achieves a lower ASR of 3.13% and a higher CA of 95.64% on GTSRB dataset, as per 'Backdoor Defense via Decoupling the Training Process'. This illustrates that DBD performs better against WaNet attacks on reported datasets.",
        "reference": "Anti-Backdoor Learning: Training Clean Models on Poisoned Data, Backdoor Defense via Decoupling the Training Process"
    },
    {
        "paper": [
            "arXiv-2110.11571v3.tex",
            "arXiv-2202.03423v1.tex",
            "arXiv-2303.06818v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Comparing the impact of different defense strategies in terms of clean accuracy (CA) when datasets are completely clean, what are the observed trends across the 'Anti-Backdoor Learning: Training Clean Models on Poisoned Data', 'Backdoor Defense via Decoupling the Training Process', and 'Backdoor Defense via Deconfounded Representation Learning' papers?",
        "answer": "In the 'Anti-Backdoor Learning: Training Clean Models on Poisoned Data' paper, ABL achieves a CA of 88.41% on CIFAR-10 when training data is clean. 'Backdoor Defense via Decoupling the Training Process' reports a CA of 88.63% when using DBD method in similar clean conditions, and 'Backdoor Defense via Deconfounded Representation Learning' reports the highest CA at 88.95% for CBD. These trends suggest that CBD slightly outperforms the others in maintaining high CA on clean datasets.",
        "reference": "Anti-Backdoor Learning: Training Clean Models on Poisoned Data, Backdoor Defense via Decoupling the Training Process, Backdoor Defense via Deconfounded Representation Learning"
    }
]