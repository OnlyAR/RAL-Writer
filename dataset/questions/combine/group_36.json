[
    {
        "paper": [
            "arXiv-2305.14283v3.tex",
            "arXiv-2310.05029v1.tex",
            "arXiv-2403.05676v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the performance improvements in retrieval-augmented LLMs described in \\\\\\\"Query Rewriting for Retrieval-Augmented Large Language Models\\\\\\\" compared to the performance improvements achieved with PipeRAG, as seen in \\\\\\\"PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design\\\\\\\"?",
        "answer": "\\\\\\\"Query Rewriting for Retrieval-Augmented Large Language Models\\\\\\\" shows improvement in performance across different tasks like HotpotQA, AmbigNQ, and PopQA, with EM and F1 metrics improving by a few percentage points, such as a boost from 32.36 to 34.38 in EM for HotpotQA. In contrast, \\\\\\\"PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design\\\\\\\" emphasizes performance improvements in terms of latency, achieving up to 2.6 times speedup in end-to-end generation latency while also reducing perplexity by up to 0.93 compared to the RETRO baseline.",
        "reference": "Query Rewriting for Retrieval-Augmented Large Language Models, PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"
    },
    {
        "paper": [
            "arXiv-2305.14283v3.tex",
            "arXiv-2310.05029v1.tex",
            "arXiv-2403.05676v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the dataset sizes used for retrieval-augmented LLM experiments in \\\\\\\"Query Rewriting for Retrieval-Augmented Large Language Models\\\\\\\" and \\\\\\\"PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design\\\\\\\". How do they differ?",
        "answer": "\\\\\\\"Query Rewriting for Retrieval-Augmented Large Language Models\\\\\\\" does not specify a dataset size in terms of tokens used for retrieval-augmented LLM experiments, but benchmarks like HotpotQA, AmbigNQ, PopQA, and MMLU are utilized without indicating the token size for retrieval. In contrast, \\\\\\\"PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design\\\\\\\" uses a significantly detailed and massive setup with a C4 corpus structured into databases of three billion chunks of 64 tokens each, amounting to a potential large-scale of 200 billion tokens when utilizing the full database. This shows a stark contrast in the explicit scale and description of datasets used.",
        "reference": "Query Rewriting for Retrieval-Augmented Large Language Models, PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"
    },
    {
        "paper": [
            "arXiv-2305.14283v3.tex",
            "arXiv-2310.05029v1.tex",
            "arXiv-2403.05676v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the token chunk sizes used in the database construction for retrieval in \\\"PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design\\\" and \\\"Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading\\\".",
        "answer": "In \\\"PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design,\\\" the database is constructed with token chunks of 64 tokens each. \\\"Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading\\\" does not specify the token chunk size used in its memory tree construction, focusing instead on the iterative navigation strategy.",
        "reference": "PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design, Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading"
    },
    {
        "paper": [
            "arXiv-2305.14283v3.tex",
            "arXiv-2310.05029v1.tex",
            "arXiv-2403.05676v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do the maximum sequence lengths addressed in \\\"PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design\\\" compare to those in \\\"Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading\\\"?",
        "answer": "\\\"PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design\\\" does not explicitly mention a maximum sequence length but focuses on improving retrieval efficiency with large databases. \\\"Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading\\\" deals with long-text question answering and evaluates performance on tasks with sequences longer than the typical context window, such as those exceeding 8,000 tokens in the QuALITY dataset.",
        "reference": "PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design, Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading"
    },
    {
        "paper": [
            "arXiv-2305.14283v3.tex",
            "arXiv-2310.05029v1.tex",
            "arXiv-2403.05676v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the specific dataset sizes or number of tokens used for evaluation in \\\"PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design\\\" and \\\"Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading\\\"?",
        "answer": "\\\"PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design\\\" uses a C4 corpus database with up to 200 billion tokens. \\\"Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading\\\" does not specify the number of tokens in the datasets but mentions working with datasets like QuALITY and GovReport, which involve long-text question answering.",
        "reference": "PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design, Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading"
    },
    {
        "paper": [
            "arXiv-2305.14283v3.tex",
            "arXiv-2310.05029v1.tex",
            "arXiv-2403.05676v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the empirical results for retrieval efficiency between \\\"PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design\\\" and \\\"Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading\\\".",
        "answer": "\\\"PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design\\\" achieves up to 2.6 times speedup in retrieval efficiency through pipeline parallelism and retrieval interval optimization. \\\"Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading\\\" does not quantify retrieval efficiency in terms of speedup but focuses on enhancing accuracy and reasoning in long-text processing, highlighting the ability to read only 59% to 69% of the text on average for successful paths.",
        "reference": "PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design, Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading"
    }
]