[
    {
        "paper": [
            "arXiv-2305.14283v3.tex",
            "arXiv-2310.05029v1.tex",
            "arXiv-2403.05676v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "In the papers \\\"PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design\\\" and \\\"Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading,\\\" what are the nominal context length windows handled by their respective models?",
        "answer": "In \\\"PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design,\\\" retrieval-augmented models like PipeRAG leverage fixed intervals for retrieval within the model's contexts. Whereas in \\\"Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading,\\\" Stable Beluga 2 is used which has a maximum 4,096 token context length.",
        "reference": "PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design; Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading"
    },
    {
        "paper": [
            "arXiv-2305.14283v3.tex",
            "arXiv-2310.05029v1.tex",
            "arXiv-2403.05676v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Comparing the three papers \\\"PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design\\\", \\\"Query Rewriting for Retrieval-Augmented Large Language Models\\\", and \\\"Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading,\\\" which ones report a specific performance improvement and its measurement against a baseline?",
        "answer": "\\\"PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design\\\" reports a specific performance improvement, showing up to a 2.6 times speedup over the baseline \\\\textsc{Retro}. \\\"Query Rewriting for Retrieval-Augmented Large Language Models\\\" and \\\"Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading\\\" also report performance improvements relative to baselines, but \\\"PipeRAG\\\" specifically quantifies the improvement with a clear measure of speedup in execution.",
        "reference": "PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design; Query Rewriting for Retrieval-Augmented Large Language Models; Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading"
    },
    {
        "paper": [
            "arXiv-2305.14283v3.tex",
            "arXiv-2310.05029v1.tex",
            "arXiv-2403.05676v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Which paper among \\\"PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design\\\" and \\\"Query Rewriting for Retrieval-Augmented Large Language Models\\\" reports the largest database used for retrieval, and what is the size of that database?",
        "answer": "\\\"PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design\\\" reports using a database constructed from the C4 corpus with 3 billion token chunks for retrieval, indicating a large-scale database usage.",
        "reference": "PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design; Query Rewriting for Retrieval-Augmented Large Language Models"
    },
    {
        "paper": [
            "arXiv-2305.14283v3.tex",
            "arXiv-2310.05029v1.tex",
            "arXiv-2403.05676v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Comparing \\\"Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading\\\" and \\\"Query Rewriting for Retrieval-Augmented Large Language Models,\\\" which datasets are used for evaluation and open-domain QA, respectively, and how many examples are involved in the evaluation or test split?",
        "answer": "\\\"Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading\\\" evaluates using the QuALITY dataset with a subset of 187 examples. \\\"Query Rewriting for Retrieval-Augmented Large Language Models\\\" evaluates on HotpotQA, AmbigNQ, and PopQA with PopQA's test set consisting of 714 examples.",
        "reference": "Query Rewriting for Retrieval-Augmented Large Language Models; Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading"
    },
    {
        "paper": [
            "arXiv-2305.14283v3.tex",
            "arXiv-2310.05029v1.tex",
            "arXiv-2403.05676v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Between \\\"Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading\\\" and \\\"PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design,\\\" which paper reports performance evaluation based on a constructed memory tree or database architecture, and what are the specific performance measurements provided?",
        "answer": "\\\"Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading\\\" reports performance evaluation using datasets QuALITY, SummScreenFD, and GovReport with respective accuracy measurements, whereas \\\"PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design\\\" discusses efficiency improvements with latency and perplexity as performance measurements.",
        "reference": "Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading; PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"
    },
    {
        "paper": [
            "arXiv-2305.14283v3.tex",
            "arXiv-2310.05029v1.tex",
            "arXiv-2403.05676v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "In comparing datasets mentioned in \\\"Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading\\\" and \\\"PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design,\\\" what are the chunk sizes (in tokens) used in the dataset processing for each paper?",
        "answer": "In \\\"Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading,\\\" text segments fitting within the LLM's context window are used but specific chunk sizes aren't mentioned. In \\\"PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design,\\\" the C4 corpus is split into chunks of m=64 tokens for processing.",
        "reference": "Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading; PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design"
    }
]