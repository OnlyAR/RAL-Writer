[
    {
        "paper": [
            "arXiv-2404.01258v2.tex",
            "arXiv-2406.04325v1.tex",
            "arXiv-2410.02713v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How many video captions are generated in the datasets presented in \\\\\\\"Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward\\\\\\\" and \\\\\\\"ShareGPT4Video: Improving Video Understanding and Generation with Better Captions\\\\\\\"?",
        "answer": "The dataset proposed in \\\\\\\"Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward\\\\\\\" comprises 900,000 generated captions, while the \\\\\\\"ShareGPT4Video: Improving Video Understanding and Generation with Better Captions\\\\\\\" dataset provides 40,000 GPT4V-annotated dense captions and 4.8 million captions generated by ShareCaptioner-Video.",
        "reference": "Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward; ShareGPT4Video: Improving Video Understanding and Generation with Better Captions."
    },
    {
        "paper": [
            "arXiv-2404.01258v2.tex",
            "arXiv-2406.04325v1.tex",
            "arXiv-2410.02713v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the number of unique video sources mentioned in \\\\\\\"ShareGPT4Video: Improving Video Understanding and Generation with Better Captions\\\\\\\" and \\\\\\\"Video Instruction Tuning with Synthetic Data\\\\\\\".",
        "answer": "In \\\\\\\"ShareGPT4Video: Improving Video Understanding and Generation with Better Captions,\\\\\\\" the data sources include Panda-70M, Pexels, Pixabay, Mixkit, BDD100K, and Ego4d, among others. In \\\\\\\"Video Instruction Tuning with Synthetic Data,\\\\\\\" the sources are HD-VILA-100M, VidOR, VIDAL, YouCook2, Charades, ActivityNet, Ego4d, InternVid-10M, Something-Something v2, YouTube Shorts, and Kinetics-700, covering a broader range of 10 sources explicitly mentioned.",
        "reference": "ShareGPT4Video: Improving Video Understanding and Generation with Better Captions; Video Instruction Tuning with Synthetic Data"
    },
    {
        "paper": [
            "arXiv-2404.01258v2.tex",
            "arXiv-2406.04325v1.tex",
            "arXiv-2410.02713v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the total number of datasets mentioned across all three papers?",
        "answer": "Across all three papers: \\\\\\\"Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward,\\\\\\\" \\\\\\\"ShareGPT4Video: Improving Video Understanding and Generation with Better Captions,\\\\\\\" and \\\\\\\"Video Instruction Tuning with Synthetic Data,\\\\\\\" there are more than 21 distinct datasets mentioned. These include unique datasets like  MSVD-QA, MSRVTT-QA, TGIF-QA, VMware, ShareGPT4Video, Panda-70M, Pexels, Pixabay, Mixkit, BDD100K, Ego4d, PIDAL, HD-VILA-100M, VidOR, YouCook2, Charades, ActivityNet-QA, NExT-QA, PerceptionTest, InternVid-10M, Something-Something v2, and Kinetics-700.",
        "reference": "Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward; ShareGPT4Video: Improving Video Understanding and Generation with Better Captions; Video Instruction Tuning with Synthetic Data."
    },
    {
        "paper": [
            "arXiv-2404.01258v2.tex",
            "arXiv-2406.04325v1.tex",
            "arXiv-2410.02713v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the total number of videos used across \\\"Video Instruction Tuning with Synthetic Data\\\" and \\\"Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward\\\"?",
        "answer": "The number of videos in \\\"Video Instruction Tuning with Synthetic Data\\\" is 178,510. The number of videos in \\\"Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward\\\" is cited as comprising 900K in their dataset section. Consequently, combining both papers, the total comes to 1,078,510 videos.",
        "reference": "Video Instruction Tuning with Synthetic Data; Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward."
    },
    {
        "paper": [
            "arXiv-2404.01258v2.tex",
            "arXiv-2406.04325v1.tex",
            "arXiv-2410.02713v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Between \\\"ShareGPT4Video: Improving Video Understanding and Generation with Better Captions\\\" and \\\"Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward\\\", which paper created a video dataset with detailed captions for the highest percentage of videos?",
        "answer": "\\\"ShareGPT4Video: Improving Video Understanding and Generation with Better Captions\\\" generated detailed captions for 40,000 videos via GPT4V annotation, and 4.8 million via ShareCaptioner-Video, out of its sourced aesthetic data, thus implying 100% of its video dataset. \\\"Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward\\\" covers a dataset with exactly 900,000 generated captions, across an unspecified total number of assessed videos, but the generation was presumably applied to the entirety of its corpus. Both papers generate captions for 100% of their discussed videos.",
        "reference": "ShareGPT4Video: Improving Video Understanding and Generation with Better Captions; Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward."
    },
    {
        "paper": [
            "arXiv-2404.01258v2.tex",
            "arXiv-2406.04325v1.tex",
            "arXiv-2410.02713v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the comparative number of unique tasks among datasets in \\\"Video Instruction Tuning with Synthetic Data\\\" versus \\\"Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward\\\"?",
        "answer": "\\\"Video Instruction Tuning with Synthetic Data\\\" describes experiments encompassing detailed captioning, open-ended QA, and multiple-choice QA, tallying to 3 distinct task types. \\\"Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward\\\" focuses on preference optimization but does not delineate an extensive task list outside video QA; thus, we see \\\"Video Instruction Tuning with Synthetic Data\\\" explicitly lists more unique tasks with a total of 3.",
        "reference": "Video Instruction Tuning with Synthetic Data; Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward."
    }
]