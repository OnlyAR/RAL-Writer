[
    {
        "paper": [
            "arXiv-2311.17043v1.tex",
            "arXiv-2406.16852v2.tex",
            "arXiv-2410.17434v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the context length for LongVA and LongVU models, and how does it affect their ability to handle long video inputs compared to LLaMA-VID?",
        "answer": "The context length for LongVA is 224K and for LongVU is 8K. This extended context length allows LongVA to handle more than 200K visual tokens, processing up to 2000 frames, and achieving a better performance on long video tasks compared to LLaMA-VID, which has a context length limit of 4K. LLaMA-VID handles the token overload by compressing input to 2 tokens per image, supporting up to 300 frames with Vicuna-1.5.",
        "reference": "LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models; Long Context Transfer from Language to Vision; LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding"
    },
    {
        "paper": [
            "arXiv-2311.17043v1.tex",
            "arXiv-2406.16852v2.tex",
            "arXiv-2410.17434v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do the maximum number of frames supported by LongVA, LongVU, and LLaMA-VID differ in their experimental frameworks?",
        "answer": "LongVA can process over 2000 frames by representing an extended context space as it uses a context length of 224K. LongVU, with a context length of 8K and its adaptive compression, maintains 1fps video support efficiently. In contrast, LLaMA-VID supports up to 300 frames during training by compressing video frames to 2 tokens each using a 4K context limit.",
        "reference": "LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models; Long Context Transfer from Language to Vision; LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding"
    },
    {
        "paper": [
            "arXiv-2311.17043v1.tex",
            "arXiv-2406.16852v2.tex",
            "arXiv-2410.17434v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the video understanding capabilities of LongVA, LongVU, and LLaMA-VID in handling context length limitations and managing processing efficiency for video language models.",
        "answer": "LongVA demonstrates superior video understanding capabilities for long sequences with its 224K context length, enabling efficient processing of over 200K tokens while maintaining high performance on benchmarks like Video-MME. LongVU, through tailored spatiotemporal compression within an 8K context, balances token efficiency and processing length reaching state-of-the-art results on MVBench among others. LLaMA-VID's efficiency focuses around its 2-token per frame strategy within a 4K length, effective for shorter contexts, but it is less enabled for longer sequences compared to the other models.",
        "reference": "LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models; Long Context Transfer from Language to Vision; LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding."
    },
    {
        "paper": [
            "arXiv-2311.17043v1.tex",
            "arXiv-2406.16852v2.tex",
            "arXiv-2410.17434v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How many visual tokens per frame does LongVA and LongVU process, and how does it relate to their maximum video frame support? Compare this with how LLaMA-VID compresses visual frames.",
        "answer": "LongVA encodes visual frames to 144 tokens each, processing up to 2000 frames due to its context length of 224K for maximum token handling. LongVU uses adaptive compression to reduce tokens efficiently from over 144 to 2 tokens per frame supporting 1fps-hour long video with 8k context length. LLaMA-VID compresses frames to 2 tokens each, enabling it to support up to 300 frames before reaching its 4K context length limitation.",
        "reference": "LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models; Long Context Transfer from Language to Vision; LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding"
    },
    {
        "paper": [
            "arXiv-2311.17043v1.tex",
            "arXiv-2406.16852v2.tex",
            "arXiv-2410.17434v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What average video lengths from the datasets or benchmarks are used by LongVA, LongVU, and LLaMA-VID, and how does this affect their model testing capabilities?",
        "answer": "LongVU is tested on benchmarks video lengths for VideoMME (1 to 60 mins) and its Highest length subset (30 to 60 mins) with efficient spatiotemporal compression. LongVA exhibits its prowess on over 1000 seconds average length from specific benchmarks like Video-MME. LLaMA-VID is optimized for shorter video lengths, maximizing efficiency with shorter token benchmarks where video lengths are typically lower, evident from benchmark tests like in MSVD-QA or MSRVTT-QA.",
        "reference": "LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models; Long Context Transfer from Language to Vision; LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding"
    },
    {
        "paper": [
            "arXiv-2311.17043v1.tex",
            "arXiv-2406.16852v2.tex",
            "arXiv-2410.17434v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the quantitative performance of LongVA and LongVU on the MLVU benchmark, and how does their model's architectural choices influence these results?",
        "answer": "On the MLVU benchmark, LongVU attained a score of 65.4 supporting its efficiency with spatiotemporal compression at 1fps and 8k context length. LongVA, having a 224K context length, supports a wide token range which is advantageous for variable input but lacks exact MLVU measured values within the extensive length benchmarks specified. LongVU's strategic compression benefits handling extensive video lengths maintaining model efficiency, in contrast to LongVA's extensive token support that favors other long-based video evaluations such as Video-MME.",
        "reference": "LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models; Long Context Transfer from Language to Vision; LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding"
    }
]