[
    {
        "paper": [
            "arXiv-2203.07586v1.tex",
            "arXiv-2211.10247v2.tex",
            "arXiv-2408.15801v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the highest ROUGE-2 score achieved on the PubMed dataset by each method, and how do they compare?",
        "answer": "For the 'Long Document Summarization with Top-down and Bottom-up Inference', the highest ROUGE-2 score specified is 23.56 by GoSum, whereas 'Scaling Up Summarization: Leveraging Large Language Models for Long Text Extractive Summarization' achieves a higher score with 24.57 for the LLAMA2-7B (12K) variant.",
        "reference": "Long Document Summarization with Top-down and Bottom-up Inference, Scaling Up Summarization: Leveraging Large Language Models for Long Text Extractive Summarization"
    },
    {
        "paper": [
            "arXiv-2203.07586v1.tex",
            "arXiv-2211.10247v2.tex",
            "arXiv-2408.15801v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the performance of 'Scaling Up Summarization: Leveraging Large Language Models for Long Text Extractive Summarization' compare to 'GoSum: Extractive Summarization of Long Documents by Reinforcement Learning and Graph Organized discourse state' in terms of ROUGE-2 on the arXiv dataset?",
        "answer": "The ROUGE-2 score for 'Scaling Up Summarization: Leveraging Large Language Models for Long Text Extractive Summarization' is 21.07 using LLAMA2-7B (12K) and 21.01 using ChatGLM2-6B (12K), while 'GoSum: Extractive Summarization of Long Documents by Reinforcement Learning and Graph Organized discourse state' achieves a slightly lower ROUGE-2 score of 20.53 on the arXiv dataset.",
        "reference": "Scaling Up Summarization: Leveraging Large Language Models for Long Text Extractive Summarization, GoSum: Extractive Summarization of Long Documents by Reinforcement Learning and Graph Organized discourse state"
    },
    {
        "paper": [
            "arXiv-2203.07586v1.tex",
            "arXiv-2211.10247v2.tex",
            "arXiv-2408.15801v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Which approach achieves higher ROUGE-1 scores on the arXiv dataset, 'GoSum' or 'Long Document Summarization with Top-down and Bottom-up Inference' using Top-down transformer?",
        "answer": "On the arXiv dataset, 'GoSum: Extractive Summarization of Long Documents by Reinforcement Learning and Graph Organized discourse state' achieves a higher ROUGE-1 score of 48.61 compared to 'Long Document Summarization with Top-down and Bottom-up Inference', which achieves a slightly lower ROUGE-1 score of 48.34.",
        "reference": "GoSum: Extractive Summarization of Long Documents by Reinforcement Learning and Graph Organized discourse state, Long Document Summarization with Top-down and Bottom-up Inference"
    },
    {
        "paper": [
            "arXiv-2203.07586v1.tex",
            "arXiv-2211.10247v2.tex",
            "arXiv-2408.15801v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the difference in ROUGE-L scores for extracts by 'Long Document Summarization with Top-down and Bottom-up Inference' and 'Scaling Up Summarization: Leveraging Large Language Models for Long Text Extractive Summarization' on the arXiv dataset?",
        "answer": "'Long Document Summarization with Top-down and Bottom-up Inference' achieves a ROUGE-L score of 43.91 on the arXiv dataset. In comparison, 'Scaling Up Summarization: Leveraging Large Language Models for Long Text Extractive Summarization' achieves a slightly higher ROUGE-L score of 43.98 using ChatGLM2-6B (12K).",
        "reference": "Long Document Summarization with Top-down and Bottom-up Inference, Scaling Up Summarization: Leveraging Large Language Models for Long Text Extractive Summarization"
    },
    {
        "paper": [
            "arXiv-2203.07586v1.tex",
            "arXiv-2211.10247v2.tex",
            "arXiv-2408.15801v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the average number of input words in the datasets used by 'Long Document Summarization with Top-down and Bottom-up Inference' and 'Scaling Up Summarization: Leveraging Large Language Models for Long Text Extractive Summarization'.",
        "answer": "In 'Long Document Summarization with Top-down and Bottom-up Inference', the PubMed dataset comprises an average of 3,224 words per document, while the ArXiv dataset comprises an average of 6,913 words. In 'Scaling Up Summarization: Leveraging Large Language Models for Long Text Extractive Summarization', filtered datasets consider sequence lengths of up to 4,000 words and 12,000 words based on ArXiv and PubMed subsets respectively.",
        "reference": "Long Document Summarization with Top-down and Bottom-up Inference, Scaling Up Summarization: Leveraging Large Language Models for Long Text Extractive Summarization"
    },
    {
        "paper": [
            "arXiv-2203.07586v1.tex",
            "arXiv-2211.10247v2.tex",
            "arXiv-2408.15801v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do the training dataset sizes compare between 'Scaling Up Summarization: Leveraging Large Language Models for Long Text Extractive Summarization' and 'GoSum: Extractive Summarization of Long Documents by Reinforcement Learning and Graph Organized discourse state'?",
        "answer": "In 'Scaling Up Summarization: Leveraging Large Language Models for Long Text Extractive Summarization', the PubMed dataset contains 70,893 training documents, and the arXiv dataset contains 38,532 training documents for the 4K version. In 'GoSum: Extractive Summarization of Long Documents by Reinforcement Learning and Graph Organized discourse state', the PubMed dataset contains 116,937 training documents, and the arXiv dataset contains 202,880 training documents.",
        "reference": "Scaling Up Summarization: Leveraging Large Language Models for Long Text Extractive Summarization, GoSum: Extractive Summarization of Long Documents by Reinforcement Learning and Graph Organized discourse state"
    }
]