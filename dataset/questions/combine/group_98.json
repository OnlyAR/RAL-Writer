[
    {
        "paper": [
            "arXiv-2301.11325v1.tex",
            "arXiv-2302.03917v2.tex",
            "arXiv-2410.20478v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the number of training steps in Noise2Music compare to MusicFlow for their generative models?",
        "answer": "In Noise2Music, the waveform generator is trained for 1.6 million steps, the waveform cascader for 460,000 steps, the spectrogram generator for 1.8 million steps, the spectrogram vocoder for 840,000 steps, and the super-resolution cascader for 270,000 steps. In MusicFlow, the first stage is trained for 300,000 steps and the second stage for 600,000 steps.",
        "reference": "Noise2Music: Text-conditioned Music Generation with Diffusion Models; MusicFlow: Cascaded Flow Matching for Text Guided Music Generation"
    },
    {
        "paper": [
            "arXiv-2301.11325v1.tex",
            "arXiv-2302.03917v2.tex",
            "arXiv-2410.20478v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Based on the MusicCaps evaluations, how does the FAD score of MusicFlow compare to that of Noise2Music?",
        "answer": "MusicFlow achieves a FAD score of 2.82, while Noise2Music achieves a FAD score of 2.134 on the MusicCaps dataset, indicating that Noise2Music performs better in terms of the Fr√©chet Audio Distance.",
        "reference": "Noise2Music: Text-conditioned Music Generation with Diffusion Models; MusicFlow: Cascaded Flow Matching for Text Guided Music Generation"
    },
    {
        "paper": [
            "arXiv-2301.11325v1.tex",
            "arXiv-2302.03917v2.tex",
            "arXiv-2410.20478v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "In terms of parameter size, how does the size of the model used in Noise2Music compare to that used in MusicFlow?",
        "answer": "Noise2Music's waveform generator model has 724 million parameters, while MusicFlow's cascaded model has a total of 330 million parameters for both stages combined, indicating that Noise2Music uses a larger model compared to MusicFlow.",
        "reference": "Noise2Music: Text-conditioned Music Generation with Diffusion Models; MusicFlow: Cascaded Flow Matching for Text Guided Music Generation"
    },
    {
        "paper": [
            "arXiv-2301.11325v1.tex",
            "arXiv-2302.03917v2.tex",
            "arXiv-2410.20478v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the audio sampling rate differences between MusicLM, Noise2Music, and MusicFlow in their generative models?",
        "answer": "MusicLM generates audio at a sampling rate of 24 kHz, Noise2Music generates the initial waveform at 16 kHz and subsequently upsamples it to 24 kHz, and MusicFlow also works with audio resampled to 32 kHz for training and evaluation.",
        "reference": "MusicLM: Generating Music From Text; Noise2Music: Text-conditioned Music Generation with Diffusion Models; MusicFlow: Cascaded Flow Matching for Text Guided Music Generation"
    },
    {
        "paper": [
            "arXiv-2301.11325v1.tex",
            "arXiv-2302.03917v2.tex",
            "arXiv-2410.20478v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Between MusicLM, Noise2Music, and MusicFlow, which model uses the most parameters in their largest configuration for text-to-music generation?",
        "answer": "Noise2Music uses the most parameters, with its waveform model having 724 million parameters. MusicFlow's model has 330 million parameters in its largest configuration. The paper does not specify the largest number of parameters for MusicLM but among the specified comparisons, Noise2Music's model is larger.",
        "reference": "MusicLM: Generating Music From Text; Noise2Music: Text-conditioned Music Generation with Diffusion Models; MusicFlow: Cascaded Flow Matching for Text Guided Music Generation"
    },
    {
        "paper": [
            "arXiv-2301.11325v1.tex",
            "arXiv-2302.03917v2.tex",
            "arXiv-2410.20478v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do the training datasets differ among MusicLM, Noise2Music, and MusicFlow in terms of size?",
        "answer": "MusicLM uses a dataset of 280,000 hours of music, Noise2Music uses 150,000 hours of pseudolabeled audio sources, and MusicFlow uses 20,000 hours of proprietary music data for training.",
        "reference": "MusicLM: Generating Music From Text; Noise2Music: Text-conditioned Music Generation with Diffusion Models; MusicFlow: Cascaded Flow Matching for Text Guided Music Generation"
    }
]