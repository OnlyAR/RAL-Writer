[
    {
        "paper": [
            "arXiv-2212.10509v2.tex",
            "arXiv-2305.06983v2.tex",
            "arXiv-2403.10081v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What retrieval strategies did FLARE and DRAGIN use to trigger retrieval, and how did they compare in performance on StrategyQA datasets?",
        "answer": "FLARE triggers retrieval when token probabilities fall below a threshold, suggesting low confidence. On StrategyQA, FLARE achieved an accuracy of 65.5% using LLaMA2-13B-Chat. DRAGIN uses a more complex strategy based on token uncertainty, significance, and influence, achieving 68.9% accuracy with the same model, demonstrating DRAGIN's superior retrieval timing and performance.",
        "reference": "Active Retrieval Augmented Generation; DRAGIN: Dynamic Retrieval Augmented Generation based on the Information Needs of Large Language Models."
    },
    {
        "paper": [
            "arXiv-2212.10509v2.tex",
            "arXiv-2305.06983v2.tex",
            "arXiv-2403.10081v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do the numbers of retrieval module activations differ between FLARE and DRAGIN on the IIRC dataset across different model sizes, and what might this indicate about each system's efficiency?",
        "answer": "On the IIRC dataset, FLARE activated retrieval 5.521 times with LLaMA2-13B-Chat, while DRAGIN activated 2.829 times per instance with the same model. This indicates that DRAGIN is more conservative or efficient in its retrieval needs, performing better or equivalently with fewer retrievals compared to FLARE.",
        "reference": "Active Retrieval Augmented Generation; DRAGIN: Dynamic Retrieval Augmented Generation based on the Information Needs of Large Language Models."
    },
    {
        "paper": [
            "arXiv-2212.10509v2.tex",
            "arXiv-2305.06983v2.tex",
            "arXiv-2403.10081v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "In the context of retrieval accuracy, how did the strategy of DRAGIN differ from IRCoT on the MuSiQue dataset, and what specific metrics illustrate their effectiveness?",
        "answer": "DRAGIN on MuSiQue achieved an F1 score of 0.3344 with LLaMA2-7B-Chat while IRCoT increased the retrieval recall by 3.5 points with Flan-T5-XXL compared to the one-step retrieval. DRAGIN's performance in context and query formation possibly results in higher accuracy metrics, focusing on when and what to retrieve more precisely compared to IRCoTâ€™s recall-focused retrieval.",
        "reference": "DRAGIN: Dynamic Retrieval Augmented Generation based on the Information Needs of Large Language Models; Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions."
    },
    {
        "paper": [
            "arXiv-2212.10509v2.tex",
            "arXiv-2305.06983v2.tex",
            "arXiv-2403.10081v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Regarding the real-time information requirement, how do the methodologies of \\\\\\\\\\\\\\\\textit{IRCOT} and \\\\\\\\\\\\\\\\textit{DRAGIN} differ, particularly concerning retrieval queries, and how are these differences reflected in performance on IIRC dataset?",
        "answer": "IRCOT uses a query derived from the chain-of-thought reasoning step, whereas DRAGIN applies self-attention mechanisms to inform the query using real-time information need detection. On the IIRC dataset with LLaMA2-13B-Chat, DRAGIN achieved an exact match of 0.185 and an F1 of 0.2221, performing better in predictive retrieval efficiency by directly leveraging LLM's self-assessment.",
        "reference": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions; DRAGIN: Dynamic Retrieval Augmented Generation based on the Information Needs of Large Language Models."
    },
    {
        "paper": [
            "arXiv-2212.10509v2.tex",
            "arXiv-2305.06983v2.tex",
            "arXiv-2403.10081v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How did the performance of DRAGIN compare to FLARE on the HotpotQA dataset when using LLaMA2-13B-Chat in terms of F1 score?",
        "answer": "DRAGIN achieved an F1 score of 0.4238 on the HotpotQA dataset using LLaMA2-13B-Chat, while FLARE achieved an F1 score of 0.2756. This shows that DRAGIN outperformed FLARE by 0.1482 in terms of F1 score.",
        "reference": "DRAGIN: Dynamic Retrieval Augmented Generation based on the Information Needs of Large Language Models; Active Retrieval Augmented Generation."
    },
    {
        "paper": [
            "arXiv-2212.10509v2.tex",
            "arXiv-2305.06983v2.tex",
            "arXiv-2403.10081v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the exact match (EM) performance of DRAGIN on the 2WikiMultihopQA dataset using LLaMA2-13B-Chat compare to the EM performance of FLARE on the same dataset using the same model?",
        "answer": "DRAGIN achieved an exact match (EM) score of 0.304 on the 2WikiMultihopQA dataset using LLaMA2-13B-Chat, while FLARE achieved an EM score of 0.224. DRAGIN outperformed FLARE by 0.08 in terms of exact match score.",
        "reference": "DRAGIN: Dynamic Retrieval Augmented Generation based on the Information Needs of Large Language Models; Active Retrieval Augmented Generation."
    }
]