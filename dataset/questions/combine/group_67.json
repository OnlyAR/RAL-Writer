[
    {
        "paper": [
            "arXiv-2108.11149v2.tex",
            "arXiv-2401.01505v4.tex",
            "arXiv-2404.06332v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do the sizes of the Sports-QA and SoccerNet-XFoul datasets compare in terms of video content and annotations?",
        "answer": "The Sports-QA dataset consists of 6,000 video clips with approximately 94,073 question-answer (QA) pairs, while the SoccerNet-XFoul dataset contains 10,000 video clips with over 22,000 QA pairs. Thus, while Sports-QA has more QA pairs, SoccerNet-XFoul has more video clips.",
        "reference": "Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex and Professional Sports; SoccerNet-XFoul: Introducing Explainability in Football Refereeing with Multi-Modal Large Language Models."
    },
    {
        "paper": [
            "arXiv-2108.11149v2.tex",
            "arXiv-2401.01505v4.tex",
            "arXiv-2404.06332v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the average number of QA pairs per video in Sports-QA compared to SoccerNet-XFoul, and what does this suggest about the density of annotations?",
        "answer": "The Sports-QA dataset, with 6,000 videos and 94,073 QA pairs, has an average of approximately 15.7 QA pairs per video. In contrast, the SoccerNet-XFoul dataset, with 10,000 videos and 22,000 QA pairs, has an average of 2.2 QA pairs per video, suggesting a much higher density of annotations in Sports-QA.",
        "reference": "Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex and Professional Sports; SoccerNet-XFoul: Introducing Explainability in Football Refereeing with Multi-Modal Large Language Models."
    },
    {
        "paper": [
            "arXiv-2108.11149v2.tex",
            "arXiv-2401.01505v4.tex",
            "arXiv-2404.06332v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Comparing the number of action categories between the MultiSports dataset used in Sports-QA and the annotation categories in SoccerNet-XFoul, which dataset handles a more extensive variety of actions?",
        "answer": "The MultiSports dataset used in Sports-QA covers 66 fine-grained action categories across multiple sports, while SoccerNet-XFoul focuses on football and annotates refereeing decisions, suggesting that MultiSports within Sports-QA handles a wider variety of actions.",
        "reference": "Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex and Professional Sports; SoccerNet-XFoul: Introducing Explainability in Football Refereeing with Multi-Modal Large Language Models."
    },
    {
        "paper": [
            "arXiv-2108.11149v2.tex",
            "arXiv-2401.01505v4.tex",
            "arXiv-2404.06332v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the number of annotators involved in the creation of X-VARS compare to the number of annotators in the Events in Invasion Games dataset?",
        "answer": "The X-VARS dataset was annotated by over 70 experienced football referees, while the Events in Invasion Games dataset involved nine annotators with sports science or video analysis backgrounds. Therefore, X-VARS involved a larger number of annotators overall.",
        "reference": "X-VARS: Introducing Explainability in Football Refereeing with Multi-Modal Large Language Models; A Unified Taxonomy and Multimodal Dataset for Events in Invasion Games"
    },
    {
        "paper": [
            "arXiv-2108.11149v2.tex",
            "arXiv-2401.01505v4.tex",
            "arXiv-2404.06332v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the temporal annotation granularity between the SoccerNet-XFoul and the Events in Invasion Games datasets.",
        "answer": "The SoccerNet-XFoul dataset provides detailed frame-accurate annotations of over 22,000 video-question-answer triplets focused on football refereeing questions, while the Events in Invasion Games dataset offers frame-accurate manual annotations spanning 125 minutes of playing time. Both datasets provide similar frame-accurate temporal annotation granularity, but the Events in Invasion Games dataset covers multiple sports, not just football.",
        "reference": "SoccerNet-XFoul: Introducing Explainability in Football Refereeing with Multi-Modal Large Language Models; A Unified Taxonomy and Multimodal Dataset for Events in Invasion Games"
    },
    {
        "paper": [
            "arXiv-2108.11149v2.tex",
            "arXiv-2401.01505v4.tex",
            "arXiv-2404.06332v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do the data modalities provided in the Sports-QA dataset differ from those in the Events in Invasion Games dataset?",
        "answer": "The Sports-QA dataset consists of video data suitable for video question answering tasks in sports scenarios. The Events in Invasion Games dataset provides multimodal data, including video, audio, and positional data. Thus, the Events in Invasion Games dataset offers a richer set of data modalities compared to Sports-QA.",
        "reference": "Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex and Professional Sports; A Unified Taxonomy and Multimodal Dataset for Events in Invasion Games"
    }
]