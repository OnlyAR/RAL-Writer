[
    {
        "paper": [
            "arXiv-2305.11000v2.tex",
            "arXiv-2402.05755v2.tex",
            "arXiv-2406.12428v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the primary datasets used for training in \\\\\\\"SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities\\\\\\\" and \\\\\\\"SPOT: A Foundation Multimodal Language Model that Freely Mixes Text and Speech\\\\\\\"?",
        "answer": "In \\\\\\\"SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities,\\\\\\\" the primary datasets used for training include LibriLight, Gigaspeech, Common Voice, LibriSpeech, moss-002-sft-data, and custom datasets like SpeechInstruct. In \\\\\\\"SPOT: A Foundation Multimodal Language Model that Freely Mixes Text and Speech,\\\\\\\" the training involved text datasets totaling 300 billion tokens, speech datasets totaling 460k hours or 30 billion speech tokens and using aligned speech+text datasets' tokens amounted to 7 billion speech tokens and 1.5 billion text tokens.",
        "reference": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities; SPOT: A Foundation Multimodal Language Model that Freely Mixes Text and Speech."
    },
    {
        "paper": [
            "arXiv-2305.11000v2.tex",
            "arXiv-2402.05755v2.tex",
            "arXiv-2406.12428v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do the vocabulary sizes compare between \\\\\\\"SPOT: A Foundation Multimodal Language Model that Freely Mixes Text and Speech\\\\\\\" and \\\\\\\"PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems\\\\\\\"?",
        "answer": "In \\\\\\\"SPOT: A Foundation Multimodal Language Model that Freely Mixes Text and Speech,\\\\\\\" the vocabulary size is not explicitly mentioned, but the text is encoded with BPE tokens and the speech is encoded with HuBERT tokens with a vocabulary of 501 units. In \\\\\\\"PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems,\\\\\\\" the text vocabulary size is 151,936 from the preprocessing model used.",
        "reference": "SPOT: A Foundation Multimodal Language Model that Freely Mixes Text and Speech; PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems."
    },
    {
        "paper": [
            "arXiv-2305.11000v2.tex",
            "arXiv-2402.05755v2.tex",
            "arXiv-2406.12428v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the use of HuBERT tokens in \\\\\\\"SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities\\\\\\\" and \\\\\\\"PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems.\\\\\\\" What are the similarities or differences in their application?",
        "answer": "Both \\\\\\\"SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities\\\\\\\" and \\\\\\\"PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems\\\\\\\" utilize HuBERT tokens for speech representation. However, in \\\\\\\"SpeechGPT,\\\\\\\" HuBERT tokens are expanded into the vocabulary of the language model to enable cross-modal capabilities, focusing on integrating speech into instruction datasets. In \\\\\\\"PSLM,\\\\\\\" HuBERT is used specifically for tokenizing Japanese speech data into discrete units as part of speech tokenization and detokenization strategies for real-time processing in spoken dialogue systems.",
        "reference": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities; PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems."
    },
    {
        "paper": [
            "arXiv-2305.11000v2.tex",
            "arXiv-2402.05755v2.tex",
            "arXiv-2406.12428v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the audio token length in \\\"PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems\\\" compare to the average segment duration in \\\"SPOT: A Foundation Multimodal Language Model that Freely Mixes Text and Speech\\\" when encoding speech?",
        "answer": "In \\\"PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems,\\\" HuBERT tokens are extracted at 50 tokens per second for Japanese speech data. In \\\"SPOT: A Foundation Multimodal Language Model that Freely Mixes Text and Speech,\\\" HuBERT tokens are extracted at 25 Hz, meaning 25 tokens per second on average, for English speech data. Hence, for any given duration of speech, \\\"PSLM\\\" uses double the number of tokens compared to \\\"SPOT.\\\"",
        "reference": "PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems; SPOT: A Foundation Multimodal Language Model that Freely Mixes Text and Speech."
    },
    {
        "paper": [
            "arXiv-2305.11000v2.tex",
            "arXiv-2402.05755v2.tex",
            "arXiv-2406.12428v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the comparative size of the speech data used in \\\"PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems\\\" and \\\"SPOT: A Foundation Multimodal Language Model that Freely Mixes Text and Speech\\\"?",
        "answer": "In \\\"PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems,\\\" spoken question answering (SQA) datasets are generated from 1.8M written questions and synthesized using TTS systems. In contrast, \\\"SPOT: A Foundation Multimodal Language Model that Freely Mixes Text and Speech\\\" uses 570,000 hours of combined speech datasets. \\\"SPOT\\\" processes substantially more speech data by a wide margin compared to \\\"PSLM.\\\"",
        "reference": "PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems; SPOT: A Foundation Multimodal Language Model that Freely Mixes Text and Speech."
    },
    {
        "paper": [
            "arXiv-2305.11000v2.tex",
            "arXiv-2402.05755v2.tex",
            "arXiv-2406.12428v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the purpose of vocoders used in \\\"PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems\\\" and \\\"SPOT: A Foundation Multimodal Language Model that Freely Mixes Text and Speech.\\\" What are the differences in how they use vocoders?",
        "answer": "In \\\"PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems,\\\" HiFi-GAN is used as a non-autoregressive neural vocoder for efficiently generating high-fidelity waveforms from discrete speech tokens. It focuses on reducing latency for real-time dialogue applications. In \\\"SPOT: A Foundation Multimodal Language Model that Freely Mixes Text and Speech,\\\" a HiFi-GAN vocoder likewise transforms predicted speech tokens back into audio, however, includes a duration prediction module for realistic speech synthesis adhering to mixed expressive tokens, which is not a focus in PSLM.",
        "reference": "PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems; SPOT: A Foundation Multimodal Language Model that Freely Mixes Text and Speech."
    }
]