[
    {
        "paper": [
            "arXiv-2304.08085v1.tex",
            "arXiv-2308.03279v2.tex",
            "arXiv-2310.03668v5.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the training data size used in 'InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction' compare to the size of data used in 'UniversalNER: targeted distillation from large language models for open named entity recognition'?",
        "answer": "In 'InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction', 32 datasets were used for training, and even though the sampling strategy was employed to have approximately 10,000 examples per dataset, the precise size isn't directly stated. In contrast, 'UniversalNER' used a constructed dataset of 45,889 input-output pairs, encompassing 240,725 entities and 13,020 distinct entity types for distillation.",
        "reference": "InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction, UniversalNER: targeted distillation from large language models for open named entity recognition"
    },
    {
        "paper": [
            "arXiv-2304.08085v1.tex",
            "arXiv-2308.03279v2.tex",
            "arXiv-2310.03668v5.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the average zero-shot F1 scores achieved by 'InstructUIE' and 'UniversalNER' on NER tasks across different domains?",
        "answer": "In the zero-shot setting, 'InstructUIE' achieved varying performance across domains, with the average in the crossNER datasets being around 49.0% to 63.0%. Meanwhile, 'UniversalNER' reached an average zero-shot F1 score of around 41.7% with its 7B model and 43.4% with its 13B model across various domains.",
        "reference": "InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction, UniversalNER: targeted distillation from large language models for open named entity recognition"
    },
    {
        "paper": [
            "arXiv-2304.08085v1.tex",
            "arXiv-2308.03279v2.tex",
            "arXiv-2310.03668v5.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the zero-shot setup in 'InstructUIE' differ from the setup in 'GoLLIE: Annotation Guidelines Improve Zero-Shot Information Extraction'?",
        "answer": "The zero-shot setup in 'InstructUIE' involved training the model on 18 NER datasets and testing on 7 NER datasets like CrossNER subsets and MIT datasets, focusing on domain diversity. In 'GoLLIE', the training avoided domain overlap by using datasets mainly from News and Biomedical domains for training and from diverse domains for evaluation, but a similar emphasis on domain diversity was present.",
        "reference": "InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction, GoLLIE: Annotation Guidelines Improve Zero-Shot Information Extraction"
    },
    {
        "paper": [
            "arXiv-2304.08085v1.tex",
            "arXiv-2308.03279v2.tex",
            "arXiv-2310.03668v5.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compared to 'InstructUIE', how extensive is the dataset coverage in 'GoLLIE: Annotation Guidelines Improve Zero-Shot Information Extraction' for its experiments?",
        "answer": "'InstructUIE' utilizes 32 diverse datasets for its evaluation, focusing on named entity recognition (NER), relation extraction (RE), and event extraction (EE) tasks. 'GoLLIE', used datasets from 9 diverse domains for its experiments, including News, Biomedical, and Twitter, ensuring it covers even specific datasets like CrossNER, which contains various domain subsets.",
        "reference": "InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction, GoLLIE: Annotation Guidelines Improve Zero-Shot Information Extraction"
    },
    {
        "paper": [
            "arXiv-2304.08085v1.tex",
            "arXiv-2308.03279v2.tex",
            "arXiv-2310.03668v5.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the domain types used in the evaluation datasets for 'InstructUIE', 'UniversalNER', and 'GoLLIE'?",
        "answer": "'InstructUIE' evaluates using datasets from domains such as science, healthcare, social media, and general news. 'UniversalNER' includes 43 datasets covering 9 domains: biomedical, clinical, STEM, programming, social media, law, finance, transportation, and general domains like Wikipedia. 'GoLLIE' uses datasets from at least 9 domains including News, Biomedical, Twitter, Cybercrime, Queries, and Science for its evaluation.",
        "reference": "InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction, UniversalNER: targeted distillation from large language models for open named entity recognition, GoLLIE: Annotation Guidelines Improve Zero-Shot Information Extraction"
    },
    {
        "paper": [
            "arXiv-2304.08085v1.tex",
            "arXiv-2308.03279v2.tex",
            "arXiv-2310.03668v5.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the model size in terms of parameters used by 'InstructUIE' and 'GoLLIE' for their main experiments?",
        "answer": "'InstructUIE' primarily uses the FlanT5 model with 11 billion parameters for its experiments. 'GoLLIE' initially employs the Code-LLaMA with 7 billion parameters but also conducts further analysis with larger models having 13 billion and 34 billion parameters.",
        "reference": "InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction, GoLLIE: Annotation Guidelines Improve Zero-Shot Information Extraction"
    }
]