[
    {
        "paper": [
            "arXiv-2308.15272v4.tex",
            "arXiv-2312.11190v2.tex",
            "arXiv-2402.11941v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the task completion rates of the proposed methods in \\\\\\\"CoCo-Agent: A Comprehensive Cognitive MLLM Agent for Smartphone GUI Automation\\\\\\\" and \\\\\\\"VisionTasker: Mobile Task Automation Using Vision Based UI Understanding and LLM Task Planning\\\\\\\" across different benchmarks.",
        "answer": "In \\\\\\\"CoCo-Agent: A Comprehensive Cognitive MLLM Agent for Smartphone GUI Automation,\\\\\\\" the task completion rate on the AITW benchmark with the unified model setup reaches up to 79.05%. In contrast, \\\\\\\"VisionTasker: Mobile Task Automation Using Vision Based UI Understanding and LLM Task Planning\\\\\\\" achieves a completion rate of 76% on the real-world tasks it designed across 42 commonly used apps. Both methods show high performance, but CoCo-Agent achieves a slightly higher performance in the AITW benchmark compared to VisionTasker's performance on its selected tasks.",
        "reference": "CoCo-Agent: A Comprehensive Cognitive MLLM Agent for Smartphone GUI Automation; VisionTasker: Mobile Task Automation Using Vision Based UI Understanding and LLM Task Planning"
    },
    {
        "paper": [
            "arXiv-2308.15272v4.tex",
            "arXiv-2312.11190v2.tex",
            "arXiv-2402.11941v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the main differences in the performance metrics and evaluation datasets used between \\\\\\\"LLM-powered Task Automation in Android\\\\\\\" and \\\\\\\"CoCo-Agent: A Comprehensive Cognitive MLLM Agent for Smartphone GUI Automation\\\\\\\"?",
        "answer": "\\\\\\\"LLM-powered Task Automation in Android\\\\\\\" evaluates its method on a benchmark with 158 tasks from 13 open-source Android apps and achieves an action accuracy of 90.9% on GPT-4. In comparison, \\\\\\\"CoCo-Agent: A Comprehensive Cognitive MLLM Agent for Smartphone GUI Automation\\\\\\\" uses the AITW and META-GUI benchmarks and achieves action accuracies such as 79.05% on AITW's unified setup and 88.27% action accuracy on META-GUI. The latter assesses performance across diverse GUI tasks involving a wider range of environments, whereas the former focuses on evaluating automation on a selected group of Android apps.",
        "reference": "LLM-powered Task Automation in Android; CoCo-Agent: A Comprehensive Cognitive MLLM Agent for Smartphone GUI Automation"
    },
    {
        "paper": [
            "arXiv-2308.15272v4.tex",
            "arXiv-2312.11190v2.tex",
            "arXiv-2402.11941v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do \\\\\\\"VisionTasker: Mobile Task Automation Using Vision Based UI Understanding and LLM Task Planning\\\\\\\" and \\\\\\\"LLM-powered Task Automation in Android\\\\\\\" handle UI representation differently, and what impact does this have on their token usage?",
        "answer": "\\\\\\\"VisionTasker: Mobile Task Automation Using Vision Based UI Understanding and LLM Task Planning\\\\\\\" uses a vision-based approach for UI understanding, circumventing the dependency on view hierarchies and outputting UI in natural language, which results in a token usage of approximately 265 per interface. \\\\\\\"LLM-powered Task Automation in Android\\\\\\\" converts GUI into a simplified HTML representation, using around 339 tokens per prompt on average. By using richer visual input, VisionTasker potentially offers more efficient UI representation, leading to fewer tokens needed per interaction.",
        "reference": "VisionTasker: Mobile Task Automation Using Vision Based UI Understanding and LLM Task Planning; LLM-powered Task Automation in Android"
    },
    {
        "paper": [
            "arXiv-2308.15272v4.tex",
            "arXiv-2312.11190v2.tex",
            "arXiv-2402.11941v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "In the studies \\\\\\\"LLM-powered Task Automation in Android\\\\\\\" and \\\\\\\"CoCo-Agent: A Comprehensive Cognitive MLLM Agent for Smartphone GUI Automation,\\\\\\\" which methodology utilizes historical action inputs more effectively for enhancing prediction accuracy?",
        "answer": "In \\\\\\\"LLM-powered Task Automation in Android,\\\\\\\" the benefit from historical action inputs is integrated into the memory augmentation that supports task execution improvements. \\\\\\\"CoCo-Agent\\\\\\\" specifically reports a 5.63% increase in action accuracy when using historical actions in their ablation studies, indicating a quantifiable enhancement in prediction accuracy.",
        "reference": "LLM-powered Task Automation in Android; CoCo-Agent: A Comprehensive Cognitive MLLM Agent for Smartphone GUI Automation"
    },
    {
        "paper": [
            "arXiv-2308.15272v4.tex",
            "arXiv-2312.11190v2.tex",
            "arXiv-2402.11941v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the training datasets and their diversity in \\\"VisionTasker: Mobile Task Automation Using Vision Based UI Understanding and LLM Task Planning\\\" and \\\"AutoDroid: Enabling Scalable and Efficient Mobile Task Automation with LLMs and App Memory Integration\\\".",
        "answer": "\\\"VisionTasker: Mobile Task Automation Using Vision Based UI Understanding and LLM Task Planning\\\" uses datasets such as RICO and VINS for training its visual UI understanding approach, which focuses on UI element detection and text recognition. The dataset consists of approximately 180k buttons from the IconSeer dataset, covering more than 170 common categories. In contrast, \\\"AutoDroid: Enabling Scalable and Efficient Mobile Task Automation with LLMs and App Memory Integration\\\" trains its method using a benchmark of 158 common tasks extracted from 13 applications, emphasizing augmented task completions without the need for predefined datasets. This highlights that VisionTasker has a more diverse visual dataset catered to UI element class variety, whereas AutoDroid focuses on task-driven datasets for automating mobile tasks.",
        "reference": "VisionTasker: Mobile Task Automation Using Vision Based UI Understanding and LLM Task Planning; AutoDroid: Enabling Scalable and Efficient Mobile Task Automation with LLMs and App Memory Integration."
    },
    {
        "paper": [
            "arXiv-2308.15272v4.tex",
            "arXiv-2312.11190v2.tex",
            "arXiv-2402.11941v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Contrast the handling and impacts of vision- and text-based input interference in action prediction for GUI automation in \\\"AutoDroid: Enabling Scalable and Efficient Mobile Task Automation with LLMs and App Memory Integration\\\" and \\\"VisionTasker: Mobile Task Automation Using Vision Based UI Understanding and LLM Task Planning\\\".",
        "answer": "\\\"AutoDroid: Enabling Scalable and Efficient Mobile Task Automation with LLMs and App Memory Integration\\\" incorporates GUI representation using a combination of HTML simplification and memory-driven understanding to reduce inference interference, resulting in an average step completion latency that combines both execution efficiency and vision processing costs. On the other hand, \\\"VisionTasker: Mobile Task Automation Using Vision Based UI Understanding and LLM Task Planning\\\" focuses specifically on vision-based element detection and refining natural language outputs, which deployment has the potential for accuracy impact with common rating values showing token conservation at approximately 265 tokens per interface in semantic descriptions. This comparison highlights the trade-offs between specific GUI elements in AutoDroid versus potentially richer visual inputs in VisionTasker with varying impacts on accuracy and computational efficiency.",
        "reference": "AutoDroid: Enabling Scalable and Efficient Mobile Task Automation with LLMs and App Memory Integration; VisionTasker: Mobile Task Automation Using Vision Based UI Understanding and LLM Task Planning."
    }
]