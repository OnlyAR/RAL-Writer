[
    {
        "paper": [
            "arXiv-2406.02542v2.tex",
            "arXiv-2406.10774v2.tex",
            "arXiv-2410.21465v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the speedup achieved by Loki compare to that achieved by ShadowKV on a specific model or hardware in the context of self-attention computations?",
        "answer": "Loki achieves a speedup of up to 45% over the vanilla attention implementation when tested with Llama2-13B. In comparison, ShadowKV reports supporting batch sizes up to 6 times larger and enhances throughput by 3.04 times (or 204%) with Llama-3.1-8B on an A100 GPU. This suggests that ShadowKV achieves a greater multiplier of throughput enhancement compared to Loki, though the comparison is slightly indirect as ShadowKV involves restructuring the KV cache and offloading strategies, whereas Loki focuses on sparse attention in a different context.",
        "reference": "Loki: Low-rank Keys for Efficient Sparse Attention; ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference."
    },
    {
        "paper": [
            "arXiv-2406.02542v2.tex",
            "arXiv-2406.10774v2.tex",
            "arXiv-2410.21465v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Which method, Loki or Quest, demonstrates greater accuracy retention under sparse attention with a similar computational budget on large language models?",
        "answer": "When comparing Loki and Quest, Quest shows nearly perfect accuracy retention across multiple tasks in long dependency situations, such as the passkey retrieval task with a budget as low as 1% of the total sequence length. Meanwhile, Loki experiences some performance degradation under similar sparse scenarios, as seen in its comparison with full attention methods, where accuracy indicates a slight trade-off for computational savings.",
        "reference": "Loki: Low-rank Keys for Efficient Sparse Attention; Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference."
    },
    {
        "paper": [
            "arXiv-2406.02542v2.tex",
            "arXiv-2406.10774v2.tex",
            "arXiv-2410.21465v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the approach to KV cache handling differ between Quest and ShadowKV, and what results do these lead to in terms of accuracy under long context settings?",
        "answer": "Quest employs a query-aware criticality estimation algorithm to manage KV cache size while maintaining accuracy, achieving nearly perfect accuracy on tasks with minimal KV cache budgets. In contrast, ShadowKV leverages low-rank key representation and offloaded values to reduce memory footprints, achieving up to 6 times larger batch sizes while maintaining accuracy. Notably, ShadowKV can sustain the retrieval of critical information in multi-turn settings where other methods may falter. Both methods aim to maintain accuracy under reduced computational budgets, but ShadowKV focuses more on throughput improvements, while Quest focuses on maintaining fidelity of critical tokens.",
        "reference": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference; ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference."
    },
    {
        "paper": [
            "arXiv-2406.02542v2.tex",
            "arXiv-2406.10774v2.tex",
            "arXiv-2410.21465v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "In terms of speedup over full attention strategies, how does the improvement from ShadowKV compare to that from Quest, particularly in large-scale tests?",
        "answer": "Quest achieves a self-attention speedup of up to 5.97x in decode latency compared to full attention strategies. On the other hand, ShadowKV can support batch sizes up to 6 times larger and enhance throughput by up to 3.04 times in specific settings (such as on an A100 GPU). Both methods demonstrate significant speedups, but ShadowKV emphasizes larger batch capabilities and throughput improvements beyond just speedups in processing time per instance.",
        "reference": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference; ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference."
    },
    {
        "paper": [
            "arXiv-2406.02542v2.tex",
            "arXiv-2406.10774v2.tex",
            "arXiv-2410.21465v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "When comparing the tasks focused on by Quest and ShadowKV, what are the differences in their empirical evaluations, especially regarding context length handling, and what distinct numerical results highlight these task focuses?",
        "answer": "Quest shows proficiency in tasks with long context dependencies like the passkey retrieval task, achieving nearly perfect accuracy even with small token budgets of 1% of the total sequence length, emphasizing its efficiency in identifying critical tokens in long sequences. ShadowKV focuses on extending context length capability, handling sequences up to 1M tokens by using low-rank key compression and offloading, achieving a GPU memory reduction by 6x without accuracy loss and supporting up to 6x larger batch sizes, evidenced by a 3.04x throughput increase on the A100 GPU.",
        "reference": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference; ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference."
    },
    {
        "paper": [
            "arXiv-2406.02542v2.tex",
            "arXiv-2406.10774v2.tex",
            "arXiv-2410.21465v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "In terms of numerical advantages, how do Quest and Loki compare regarding memory reduction or compute savings in their sparse attention approaches?",
        "answer": "Loki focuses on compute savings, achieving a 45% speedup in attention computation by leveraging low-dimensional representations of key vectors through PCA, but does not reduce memory usage directly. Quest, while significantly reducing decode latency up to 5.97x via dynamic sparse attention with query-aware mechanisms, attempts a balance between memory and compute savings by estimating and selecting only relevant parts of the KV cache. Despite focusing more on efficient computation, Quest provides considerable memory savings by not storing the full KV cache for future computations.",
        "reference": "Loki: Low-rank Keys for Efficient Sparse Attention; Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference."
    }
]