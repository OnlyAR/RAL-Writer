[
    {
        "paper": [
            "arXiv-1911.10470v2.tex",
            "arXiv-2009.12756v2.tex",
            "arXiv-2010.12527v4.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the reported EM and F1 scores on the SQuAD Open dataset for the system in \\\\\\\"Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering\\\\\\\" and \\\\\\\"Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval\\\\\\\"?",
        "answer": "The system in \\\\\\\"Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering\\\\\\\" achieved an EM score of 56.5 and an F1 score of 63.8 on the SQuAD Open dataset. The \\\\\\\"Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval\\\\\\\" system achieved an EM score of 56.7 on the same dataset, with the F1 score not reported.",
        "reference": "Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering, Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval."
    },
    {
        "paper": [
            "arXiv-1911.10470v2.tex",
            "arXiv-2009.12756v2.tex",
            "arXiv-2010.12527v4.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the retrieval performance in recall at top 20 passages of \\\\\\\"Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering\\\\\\\" compare to \\\\\\\"Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval\\\\\\\" on the HotpotQA dataset?",
        "answer": "\\\\\\\"Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering\\\\\\\" reported a Paragraph Recall (PR) at top 20 of 93.3%. \\\\\\\"Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval\\\\\\\" reported a recall at top 20 passages of 80.2% on the HotpotQA dataset. The system in \\\\\\\"Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering\\\\\\\" performed better in terms of retrieval recall.",
        "reference": "Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering, Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval."
    },
    {
        "paper": [
            "arXiv-1911.10470v2.tex",
            "arXiv-2009.12756v2.tex",
            "arXiv-2010.12527v4.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Which system reported higher Supporting Fact F1 score on HotpotQA full wiki, and what are the scores?",
        "answer": "The 'Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering' system reported a Supporting Fact F1 score of 76.4 on HotpotQA full wiki test set, while 'Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval' obtained a Supporting Fact F1 score of 81.2 after reranking. Thus, the latter system achieved a higher Supporting Fact F1 score.",
        "reference": "Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering, Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval."
    },
    {
        "paper": [
            "arXiv-1911.10470v2.tex",
            "arXiv-2009.12756v2.tex",
            "arXiv-2010.12527v4.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the number of retrieved reasoning paths (paragraphs) on the HotpotQA full wiki setting for the system \\\"Iterative Retriever, Reader, and Reranker\\\" compared to the default number used by \\\"Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval\\\" before reranking?",
        "answer": "The system \\\"Iterative Retriever, Reader, and Reranker\\\" retrieves up to 5 reasoning steps, where the number of paragraphs retrieved at each step can be adjusted, reported as 50, 100, or 150 in different configurations. Meanwhile, \\\"Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval\\\" uses the top-100 sequences for reranking, which typically involves 2 paragraphs per sequence, totaling 200 paragraphs.",
        "reference": "Iterative Retriever, Reader, and Reranker, Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval."
    },
    {
        "paper": [
            "arXiv-1911.10470v2.tex",
            "arXiv-2009.12756v2.tex",
            "arXiv-2010.12527v4.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "For \\\"Iterative Retriever, Reader, and Reranker\\\" and \\\"Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval\\\", what is the EM score achieved in the HotpotQA dataset, and how do they differ?",
        "answer": "On the HotpotQA dataset, \\\"Iterative Retriever, Reader, and Reranker\\\" achieved an EM score of 65.7, while \\\"Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval\\\" achieved an EM score of 62.3. The difference is 3.4 in favor of \\\"Iterative Retriever, Reader, and Reranker.\\\"",
        "reference": "Iterative Retriever, Reader, and Reranker, Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval."
    },
    {
        "paper": [
            "arXiv-1911.10470v2.tex",
            "arXiv-2009.12756v2.tex",
            "arXiv-2010.12527v4.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do the experiment settings differ for \\\"Iterative Retriever, Reader, and Reranker\\\" and \\\"Answering Open-Domain Questions of Varying Reasoning Steps from Text\\\" in terms of handling textual metadata for HotpotQA?",
        "answer": "\\\"Iterative Retriever, Reader, and Reranker\\\" leverages predetermined datasets with paragraph retrieval using Elasticsearch, emphasizing text-based retrieval without metadata. \\\"Answering Open-Domain Questions of Varying Reasoning Steps from Text\\\" similarly avoids dependency on metadata, aiming to process unstructured text without using structured metadata like knowledge bases, making their approach versatile in handling text collections. Both approaches avoid reliance on explicit metadata, though \\\"Iterative Retriever, Reader, and Reranker\\\" details leveraging the Wikipedia graph differently.",
        "reference": "Iterative Retriever, Reader, and Reranker, Answering Open-Domain Questions of Varying Reasoning Steps from Text."
    }
]