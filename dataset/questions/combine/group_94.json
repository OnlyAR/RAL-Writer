[
    {
        "paper": [
            "arXiv-2201.05966v3.tex",
            "arXiv-2301.13808v3.tex",
            "arXiv-2402.01155v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the difference in the model sizes used by the state-of-the-art benchmarks in the papers \\\"USKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models\\\" and \\\"CABINET: Content Relevance based Noise Reduction for Table Question Answering\\\"?",
        "answer": "In the paper \\\"USKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models\\\", the models like T5-3B (3 billion parameters) and Codex (175 billion parameters) are used. In contrast, the paper \\\"CABINET: Content Relevance based Noise Reduction for Table Question Answering\\\" uses OmniTab with a BART-Large backbone consisting of 406M parameters.",
        "reference": "USKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models; CABINET: Content Relevance based Noise Reduction for Table Question Answering"
    },
    {
        "paper": [
            "arXiv-2201.05966v3.tex",
            "arXiv-2301.13808v3.tex",
            "arXiv-2402.01155v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the approach of dataset simplification differ between the papers \\\"Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning\\\" and \\\"CABINET: Content Relevance based Noise Reduction for Table Question Answering\\\"?",
        "answer": "In \\\"Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning\\\", the approach involves decomposing tables into relevant sub-evidence by predicting row and column indexes with a LLM to exclude irrelevant information. In contrast, \\\"CABINET: Content Relevance based Noise Reduction for Table Question Answering\\\" employs an unsupervised relevance scorer that weighs table content based on its relevance to the question, without explicitly removing content.",
        "reference": "Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning; CABINET: Content Relevance based Noise Reduction for Table Question Answering"
    },
    {
        "paper": [
            "arXiv-2201.05966v3.tex",
            "arXiv-2301.13808v3.tex",
            "arXiv-2402.01155v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the reported improvements in evaluation metrics for table-based reasoning tasks in the papers \\\"Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning\\\" and \\\"CABINET: Content Relevance based Noise Reduction for Table Question Answering\\\"?",
        "answer": "In \\\"Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning\\\", improvements in TabFact accuracy to 93.0% (up by 2.2) are reported, with similar gains in WikiTableQuestion with accuracy going to 62.5% (up by 1.3). In \\\"CABINET: Content Relevance based Noise Reduction for Table Question Answering\\\", improvements include FeTaQA S-BLEU to 40.5 (up by 5.6) and WikiTQ accuracy to 69.1% (up by 6.4).",
        "reference": "Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning; CABINET: Content Relevance based Noise Reduction for Table Question Answering"
    },
    {
        "paper": [
            "arXiv-2201.05966v3.tex",
            "arXiv-2301.13808v3.tex",
            "arXiv-2402.01155v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the improvements in the Table-to-SQL task between the methods presented in the papers \\\"USKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models\\\" and \\\"Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning\\\".",
        "answer": "In the \\\"USKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models\\\" paper, performance in Table-to-SQL tasks is reported up to 71.76 in Spider Match (with T5-3B). Meanwhile, \\\"Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning\\\" does not focus explicitly on Table-to-SQL improvements, but they do report TabFact dataset improvements from 90.8 to 93.0 in accuracy using the Dater method.",
        "reference": "USKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models; Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning"
    },
    {
        "paper": [
            "arXiv-2201.05966v3.tex",
            "arXiv-2301.13808v3.tex",
            "arXiv-2402.01155v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do the methods introduced in \\\"USKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models\\\" and \\\"Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning\\\" handle scaling the model, and how does it impact performance?",
        "answer": "\\\"USKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models\\\" reveals that performance increases with T5 model size, with T5-3B achieving up to 85.96% accuracy on WikiSQL. \\\"Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning\\\" doesn't explicitly address the scaling of models but uses LLMs like Codex and shows performance improvement on complex reasoning without scaling experiments.",
        "reference": "USKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models; Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning"
    },
    {
        "paper": [
            "arXiv-2201.05966v3.tex",
            "arXiv-2301.13808v3.tex",
            "arXiv-2402.01155v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the handling of textual data complexity in the WikiTableQuestion dataset between \\\"USKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models\\\" and \\\"CABINET: Content Relevance based Noise Reduction for Table Question Answering\\\" with performance metrics if available.",
        "answer": "\\\"USKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models\\\" obtains a 62.5% accuracy on WikiTableQuestions using multi-task learning, while \\\"CABINET: Content Relevance based Noise Reduction for Table Question Answering\\\" significantly improves performance to a S-BLEU of 69.1%, focusing on noise reduction and context relevance, handling textual complexity differently than the unified multi-task approach in USKG.",
        "reference": "USKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models; CABINET: Content Relevance based Noise Reduction for Table Question Answering"
    }
]