[
    {
        "paper": [
            "arXiv-2307.11088v3.tex",
            "arXiv-2310.15929v2.tex",
            "arXiv-2412.15204v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the maximum context length in the datasets proposed by \\\\\\\"L-Eval: Instituting Standardized Evaluation for Long Context Language Models\\\\\\\" and \\\\\\\"LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks\\\\\\\"?",
        "answer": "The maximum context length in \\\\\\\"L-Eval: Instituting Standardized Evaluation for Long Context Language Models\\\\\\\" is 200k tokens, whereas \\\\\\\"LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks\\\\\\\" contains contexts of up to 2 million words.",
        "reference": "L-Eval: Instituting Standardized Evaluation for Long Context Language Models; LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks"
    },
    {
        "paper": [
            "arXiv-2307.11088v3.tex",
            "arXiv-2310.15929v2.tex",
            "arXiv-2412.15204v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do human expert accuracies compare between the \\\\\\\"L-Eval\\\\\\\" and \\\\\\\"LongBench v2\\\\\\\" benchmarks?",
        "answer": "In the \\\\\\\"L-Eval\\\\\\\" paper, specific human expert accuracies are not provided, whereas in \\\\\\\"LongBench v2\\\\\\\", human experts achieved an accuracy of 53.7%.",
        "reference": "L-Eval: Instituting Standardized Evaluation for Long Context Language Models; LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks"
    },
    {
        "paper": [
            "arXiv-2307.11088v3.tex",
            "arXiv-2310.15929v2.tex",
            "arXiv-2412.15204v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Was the model performance on open-ended tasks higher for commercial models or open-source models in \\\\\\\"L-Eval\\\\\\\" and how does that differ from the performance in \\\\\\\"LongBench v2\\\\\\\" benchmarks?",
        "answer": "In \\\\\\\"L-Eval\\\\\\\", commercial models like GPT-4 demonstrate significantly better performance on open-ended tasks compared to open-source models. Similarly, in \\\\\\\"LongBench v2\\\\\\\", proprietary models like o1-preview performed better with 57.7% accuracy, compared to open-source models such as Qwen2.5-72B with 39.4% accuracy.",
        "reference": "L-Eval: Instituting Standardized Evaluation for Long Context Language Models; LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks"
    },
    {
        "paper": [
            "arXiv-2307.11088v3.tex",
            "arXiv-2310.15929v2.tex",
            "arXiv-2412.15204v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the median length of documents used in \\\"LongBench v2\\\" compared to \\\"L-Eval: Instituting Standardized Evaluation for Long Context Language Models\\\"?",
        "answer": "The median length of documents in \\\"LongBench v2\\\" is 54,000 words, while \\\"L-Eval\\\" datasets vary but have an average length serving around 4,000 to 60,000 tokens, making the \\\"L-Eval\\\" documents generally shorter on average if tokens convert roughly to words on a one-to-one basis.",
        "reference": "L-Eval: Instituting Standardized Evaluation for Long Context Language Models; LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks"
    },
    {
        "paper": [
            "arXiv-2307.11088v3.tex",
            "arXiv-2310.15929v2.tex",
            "arXiv-2412.15204v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the average scores achieved by the best open-source models in \\\"L-Eval: Instituting Standardized Evaluation for Long Context Language Models\\\" and \\\"LongBench v2\\\", and which benchmark did they perform better on?",
        "answer": "In \\\"L-Eval\\\", the best open-source model (Llama2-13b-chat) achieves an average score of 39% in closed-ended tasks. In \\\"LongBench v2\\\", the best open-source model is Qwen2.5-72B-Instruct with an average accuracy of 39.4%. Both benchmarks show similar performance levels for their top open-source models.",
        "reference": "L-Eval: Instituting Standardized Evaluation for Long Context Language Models; LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks"
    },
    {
        "paper": [
            "arXiv-2307.11088v3.tex",
            "arXiv-2310.15929v2.tex",
            "arXiv-2412.15204v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the number of subtasks compare between \\\"LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks\\\" and \\\"E-Sparse: Boosting the Large Language Model Inference through Entropy-based N:M Sparsity\\\"?",
        "answer": "\\\"LongBench v2\\\" consists of 20 subtasks spread across 6 major categories, while \\\"E-Sparse\\\" does not mention specific subtasks in its evaluations, as it focuses on pruning for performance improvement.",
        "reference": "LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks; E-Sparse: Boosting the Large Language Model Inference through Entropy-based N:M Sparsity"
    }
]