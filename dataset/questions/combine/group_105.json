[
    {
        "paper": [
            "arXiv-2006.01563v2.tex",
            "arXiv-2011.06993v2.tex",
            "arXiv-2105.03654v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do the F1 scores differ for Named Entity Recognition (NER) tasks between cross-sentence context methods using BERT in 'Exploring Cross-sentence Contexts for Named Entity Recognition with BERT' and document-level features in 'FLERT: Document-Level Features for Named Entity Recognition' when evaluated on the CoNLL-2003 English dataset?",
        "answer": "In 'Exploring Cross-sentence Contexts for Named Entity Recognition with BERT', the F1 score using Contextual Majority Voting (CMV) reached 93.74 on the English CoNLL-2003 dataset. On the other hand, in 'FLERT: Document-Level Features for Named Entity Recognition', the highest F1 score achieved with fine-tuning and document-level features was 94.09 when including dev data for training. Therefore, the document-level features in FLERT slightly outperform the cross-sentence context methods in CMV.",
        "reference": "Exploring Cross-sentence Contexts for Named Entity Recognition with BERT, FLERT: Document-Level Features for Named Entity Recognition"
    },
    {
        "paper": [
            "arXiv-2006.01563v2.tex",
            "arXiv-2011.06993v2.tex",
            "arXiv-2105.03654v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "In terms of the dataset statistics, how do the number of training samples and average sentence lengths compare between the CoNLL-03 dataset used in 'Exploring Cross-sentence Contexts for Named Entity Recognition with BERT' and the WNUT-17 dataset in 'Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning'?",
        "answer": "The CoNLL-03 dataset used in 'Exploring Cross-sentence Contexts for Named Entity Recognition with BERT' has 14,987 training samples with an average token length of 13.64 per sentence. In contrast, the WNUT-17 dataset in 'Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning' has 3,394 training samples with an average sentence length of 18.48 tokens.",
        "reference": "Exploring Cross-sentence Contexts for Named Entity Recognition with BERT, Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning"
    },
    {
        "paper": [
            "arXiv-2006.01563v2.tex",
            "arXiv-2011.06993v2.tex",
            "arXiv-2105.03654v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do the state-of-the-art performance F1 scores on the CoNLL-03 English dataset compare between the papers 'FLERT: Document-Level Features for Named Entity Recognition' and 'Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning'?",
        "answer": "In 'FLERT: Document-Level Features for Named Entity Recognition', the highest F1 score achieved on the CoNLL-03 English dataset was 94.09 when including dev data for training. On the other hand, 'Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning' achieved an F1 score of 93.56 when context was used, showing that FLERT slightly outperforms the context retrieving and cooperative learning method.",
        "reference": "FLERT: Document-Level Features for Named Entity Recognition, Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning"
    },
    {
        "paper": [
            "arXiv-2006.01563v2.tex",
            "arXiv-2011.06993v2.tex",
            "arXiv-2105.03654v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the CoNLL-03 English and Dutch results for named entity recognition between 'Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning' and 'FLERT: Document-Level Features for Named Entity Recognition'. What are their F1 scores?",
        "answer": "For the CoNLL-03 English dataset, 'Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning' achieved an F1 score of 93.56 with context, while 'FLERT: Document-Level Features for Named Entity Recognition' achieved a slightly higher F1 score of 94.09. For the CoNLL-03 Dutch dataset, 'Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning' does not report results. However, 'FLERT: Document-Level Features for Named Entity Recognition' reported an F1 score of 95.21 with document-level context enforcement.",
        "reference": "FLERT: Document-Level Features for Named Entity Recognition, Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning"
    },
    {
        "paper": [
            "arXiv-2006.01563v2.tex",
            "arXiv-2011.06993v2.tex",
            "arXiv-2105.03654v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the differences in training data statistics between the CoNLL-03 English dataset used in 'FLERT: Document-Level Features for Named Entity Recognition' and the E-commerce dataset from 'Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning' regarding the number of training samples and number of entity labels?",
        "answer": "The CoNLL-03 English dataset used in 'FLERT: Document-Level Features for Named Entity Recognition' has 14,987 training samples with 4 entity labels. In contrast, the E-commerce dataset from 'Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning' has 38,959 training samples with 26 entity labels.",
        "reference": "FLERT: Document-Level Features for Named Entity Recognition, Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning"
    },
    {
        "paper": [
            "arXiv-2006.01563v2.tex",
            "arXiv-2011.06993v2.tex",
            "arXiv-2105.03654v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "For named entity recognition task on CoNLL-03 English dataset, how do the performances of 'Exploring Cross-sentence Contexts for Named Entity Recognition with BERT' and 'FLERT: Document-Level Features for Named Entity Recognition' differ when context is utilized?",
        "answer": "'Exploring Cross-sentence Contexts for Named Entity Recognition with BERT' achieved an F1 score of 93.74 with Contextual Majority Voting when using cross-sentence context. On the other hand, 'FLERT: Document-Level Features for Named Entity Recognition' achieved an F1 score of 94.09 with document-level context features and dev set inclusion, illustrating that the FLERT method provides slightly better performance when context is utilized.",
        "reference": "Exploring Cross-sentence Contexts for Named Entity Recognition with BERT, FLERT: Document-Level Features for Named Entity Recognition"
    }
]