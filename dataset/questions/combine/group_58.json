[
    {
        "paper": [
            "arXiv-2211.06687v4.tex",
            "arXiv-2308.11923v1.tex",
            "arXiv-2312.00249v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the size of LAION-Audio-630K dataset and AudioDiffCaps dataset. How do they differ in terms of scale and composition?",
        "answer": "LAION-Audio-630K dataset consists of 633,526 audio-text pairs with a total duration of 4,325.39 hours, making it the largest audio-text dataset publicly available. In contrast, the AudioDiffCaps dataset is significantly smaller, consisting of 5,996 pairs of audio clips in the development set and 1,720 pairs in the evaluation set, totaling only 600 distinct audio clips. The LAION-Audio-630K is a large-scale dataset focused on audio-captioning, while AudioDiffCaps is designed for audio difference captioning, emphasizing audio pairs with subtle differences.",
        "reference": "Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation; Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement."
    },
    {
        "paper": [
            "arXiv-2211.06687v4.tex",
            "arXiv-2308.11923v1.tex",
            "arXiv-2312.00249v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Which paper introduces a more diverse training dataset format and how do they utilize the dataset in their experimental design?",
        "answer": "The paper titled 'Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities' introduces a more diverse dataset format through its multi-task learning approach, using datasets like AudioSet, Wavcaps, AudioCaps, Clotho, and several novel tasks including query-based sound event detection and temporal event retrieval. It uniquely processes single and multiple audio clips and formulates diverse tasks such as few-shot learning and natural language audio reasoning to utilize dataset diversity. In contrast, 'Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation' mainly utilizes the LAION-Audio-630K dataset with an emphasis on text-to-audio retrieval and classification using audio-caption pairs.",
        "reference": "Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation; Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities."
    },
    {
        "paper": [
            "arXiv-2211.06687v4.tex",
            "arXiv-2308.11923v1.tex",
            "arXiv-2312.00249v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the audio-text pair sizes used for training models in 'Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation' versus 'Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities'.",
        "answer": "'Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation' uses the LAION-Audio-630K dataset with 633,526 audio-text pairs, supplemented by AudioSet for a total of 1.9 million audio samples, though the latter primarily involves label augmentation. Meanwhile, 'Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities' processes a combination of 2.6 million audio-text pairs across datasets, including AudioSet, Wavcaps, and AudioCaps, as part of their multi-task framework. This demonstrates a large-scale utilization of data in both cases with 'Acoustic Prompt Tuning...' having a slightly higher diversity in its datasets.",
        "reference": "Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation; Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities."
    },
    {
        "paper": [
            "arXiv-2211.06687v4.tex",
            "arXiv-2308.11923v1.tex",
            "arXiv-2312.00249v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Which paper demonstrated superior results in zero-shot audio classification, and how does the ESC-50 zero-shot accuracy compare between their models?",
        "answer": "The paper \\\"Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation\\\" demonstrated superior results in zero-shot audio classification, achieving an ESC-50 zero-shot accuracy of 91.0%. In contrast, the paper \\\"Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement\\\" does not focus on audio classification tasks specifically, and therefore does not provide comparable results on zero-shot audio classification benchmarks like ESC-50.",
        "reference": "Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation; Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement."
    },
    {
        "paper": [
            "arXiv-2211.06687v4.tex",
            "arXiv-2308.11923v1.tex",
            "arXiv-2312.00249v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "In terms of dataset size and associated hours, how do the audio datasets used in \\\"Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation\\\" and \\\"Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement\\\" compare?",
        "answer": "\\\"Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation\\\" uses the LAION-Audio-630K dataset, which consists of 633,526 audio-text pairs with a total duration of 4,325.39 hours. In contrast, \\\"Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement\\\" involves a much smaller scale dataset, the AudioDiffCaps, containing generated differences of paired audio clips for development and evaluation but does not specify the total duration as it is less focused on extensive hours.",
        "reference": "Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation; Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement."
    },
    {
        "paper": [
            "arXiv-2211.06687v4.tex",
            "arXiv-2308.11923v1.tex",
            "arXiv-2312.00249v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the evaluation metrics used for task performance between the papers \\\"Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities\\\" and \\\"Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement\\\".",
        "answer": "\\\"Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities\\\" utilizes metrics such as mean Average Precision (mAP) for audio tagging, SPICE and SPIDEr for audio captioning. In contrast, \\\"Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement\\\" evaluates performance using metrics like BLEU, METEOR, ROUGE-L, CIDEr, SPICE, and SPIDEr, which are focused on sentence-level accuracy for audio difference captioning. Both papers share usage of SPICE and SPIDEr, but differ in other metrics suited to their distinct tasks.",
        "reference": "Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities; Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement."
    }
]