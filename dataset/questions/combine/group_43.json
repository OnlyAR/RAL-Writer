[
    {
        "paper": [
            "arXiv-2401.18058v1.tex",
            "arXiv-2404.00934v2.tex",
            "arXiv-2410.21252v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do the sizes of the SFT datasets compare between the methods discussed in LongAlign and ChatGLM-RLHF?",
        "answer": "In LongAlign, 10k instances of supervised data of 8k-64k length constructed using Self-Instruct were used for SFT. In contrast, the ChatGLM-RLHF paper does not specifically mention the size of the SFT dataset but does describe using high-quality human-written datasets for supervised fine-tuning. Therefore, it can be inferred that LongAlign's SFT dataset is more quantifiably described, with an explicit mention of 10k instances.",
        "reference": "LongAlign: A Recipe for Long Context Alignment Encompassing Data, Training, and Evaluation; ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback."
    },
    {
        "paper": [
            "arXiv-2401.18058v1.tex",
            "arXiv-2404.00934v2.tex",
            "arXiv-2410.21252v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the benchmarks utilized for evaluating model performances in LongAlign and LongReward papers. What are the key differences in terms of the types of tasks covered and evaluation strategies?",
        "answer": "The LongAlign paper introduces the LongBench-Chat benchmark, assessing instruction-following capabilities over long queries (10k-100k length) on various tasks like reasoning, coding, and summarization. They use GPT-4 for scoring. In contrast, the LongReward paper uses LongBench-Chat and LongBench, both for long-context benchmarks but do not specify the diversity of tasks in detail; they also employ GPT-4o for evaluation. LongAlign explicitly mentions using open-ended tasks with a focus on long instruction-following, whereas LongReward includes broader multi-task long-context evaluation.",
        "reference": "LongAlign: A Recipe for Long Context Alignment Encompassing Data, Training, and Evaluation; LongReward: Improving Long-context Large Language Models with AI Feedback."
    },
    {
        "paper": [
            "arXiv-2401.18058v1.tex",
            "arXiv-2404.00934v2.tex",
            "arXiv-2410.21252v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the differences in reward or evaluation methods between LongReward and ChatGLM-RLHF based on the papers?",
        "answer": "The LongReward paper uses an off-the-shelf LLM to provide rewards for long-context model responses based on dimensions like helpfulness, logicality, faithfulness, and completeness. They specifically use LLM scoring to generate these rewards. In contrast, ChatGLM-RLHF focuses on collecting human preference annotations and training a reward model to act as a proxy for human feedback. They employ Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO) as part of their RLHF process.",
        "reference": "LongReward: Improving Long-context Large Language Models with AI Feedback; ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback."
    },
    {
        "paper": [
            "arXiv-2401.18058v1.tex",
            "arXiv-2404.00934v2.tex",
            "arXiv-2410.21252v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do the reported improvements in alignment tasks differ between LongAlign and LongReward after applying their respective methods?",
        "answer": "The LongAlign reports up to 30% improvement in long context tasks by aligning models to handle contexts up to 64k tokens, alongside enhancement in general tasks without performance compromise. LongReward mentions a 4.9% improvement in long-context tasks over the SFT baseline for Llama-3.1-8B and 5.5% for GLM-4-9B, while also benefiting short instruction following. The improvement metrics in LongAlign are expressed as a percentage increase, whereas LongReward details specific model performances in long tasks indicating marked improvement but not quantified to the same extent as LongAlign.",
        "reference": "LongAlign: A Recipe for Long Context Alignment Encompassing Data, Training, and Evaluation; LongReward: Improving Long-context Large Language Models with AI Feedback."
    },
    {
        "paper": [
            "arXiv-2401.18058v1.tex",
            "arXiv-2404.00934v2.tex",
            "arXiv-2410.21252v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the change in model performance after applying reinforcement learning from AI feedback (RL from AI) in LongReward with the improvements in alignment tasks mentioned in ChatGLM-RLHF.",
        "answer": "The LongReward paper reports a 4.9% improvement in long-context tasks for Llama-3.1-8B and 5.5% for GLM-4-9B after applying RL with AI feedback, whereas ChatGLM-RLHF describes an average improvement of 15% more wins against ChatGLM-SFT in Chinese alignment tasks using RLHF methods.",
        "reference": "LongReward: Improving Long-context Large Language Models with AI Feedback; ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback."
    },
    {
        "paper": [
            "arXiv-2401.18058v1.tex",
            "arXiv-2404.00934v2.tex",
            "arXiv-2410.21252v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What differences exist in reported data usage sizes for SFT and training datasets among the methods described in LongAlign and LongReward?",
        "answer": "LongAlign details 10k instances of long instruction-following data for supervised fine-tuning, while LongReward describes using 10k documents from GLM-4's pre-training corpus without specifying the exact number of instruction-following instances. LongAlign precisely quantifies their dataset use, whereas LongReward focuses more on document counts for diverse source coverage.",
        "reference": "LongAlign: A Recipe for Long Context Alignment Encompassing Data, Training, and Evaluation; LongReward: Improving Long-context Large Language Models with AI Feedback."
    }
]