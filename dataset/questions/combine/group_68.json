[
    {
        "paper": [
            "arXiv-2310.07849v2.tex",
            "arXiv-2311.00287v2.tex",
            "arXiv-2403.01081v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Which paper demonstrates a larger dataset size used for synthetic data generation, \\\"Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations\\\" or \\\"Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models\\\"?",
        "answer": "The paper \\\"Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models\\\" demonstrates a larger dataset size, as it generates 5000 synthetic instances for each task. Meanwhile, \\\"Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations\\\" generates 3,000 synthetic instances for each label across multiple tasks.",
        "reference": "Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations; Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models"
    },
    {
        "paper": [
            "arXiv-2310.07849v2.tex",
            "arXiv-2311.00287v2.tex",
            "arXiv-2403.01081v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the Macro-F1 score performance gain from synthetic data in \\\"Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations\\\" compare with the performance gain in \\\"Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models\\\"?",
        "answer": "In \\\"Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations\\\", the Macro-F1 score for models trained with real-world data is consistently better than those trained on synthetic data. A specific example shows a decrease of 27.4% in Macro-F1 for high subjectivity tasks under the zero-shot setting. On the other hand, in \\\"Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models\\\", the average performance gain over baselines in Macro-F1 (converted from relative performance gains expressed as overall task performance improvements) is approximately 8.7% for PubMedBERT$_{Base}$. Thus, knowledge-infused prompting results in gains over traditional baselines, whereas the LLM synthetic data in the first paper shows poorer performance compared to real-world data with a notable decrease.",
        "reference": "Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations; Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models"
    },
    {
        "paper": [
            "arXiv-2310.07849v2.tex",
            "arXiv-2311.00287v2.tex",
            "arXiv-2403.01081v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the method of mitigating data diversity issues in generating synthetic data between the papers \\\"Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations\\\" and \\\"LAB: Large-Scale Alignment for ChatBots\\\".",
        "answer": "In \\\"Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations\\\", data diversity is attempted to be improved by prompting the LLM with a diversity prompt every 1 or 10 iterations, depending on the task length, to maintain a reasonable data generation speed. Meanwhile, \\\"LAB: Large-Scale Alignment for ChatBots\\\" uses a taxonomy-driven synthetic data generator to focus on specific tasks, which uses focused prompts to enhance the diversity and quality of the generated data by fully exploiting the distinct modes corresponding to various tasks within its taxonomy.",
        "reference": "Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations; LAB: Large-Scale Alignment for ChatBots"
    },
    {
        "paper": [
            "arXiv-2310.07849v2.tex",
            "arXiv-2311.00287v2.tex",
            "arXiv-2403.01081v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do the empirical setups, particularly in terms of training sizes for synthetic data, compare between \\\"Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations\\\" and \\\"Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models\\\"?",
        "answer": "\\\"Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations\\\" uses 3,000 synthetic data points for each classification task label. In contrast, \\\"Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models\\\" generates 5000 synthetic training samples per task. Thus, the latter involves a larger synthetic dataset size for its experiments.",
        "reference": "Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations; Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models"
    },
    {
        "paper": [
            "arXiv-2310.07849v2.tex",
            "arXiv-2311.00287v2.tex",
            "arXiv-2403.01081v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What differences exist in the experimental design regarding synthetic data diversity between \\\"Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations\\\" and \\\"Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models\\\"?",
        "answer": "In \\\"Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations\\\", the approach to maintain diversity involves prompting LLMs with a diversity prompt every 1 or 10 iterations, aiming to cover more instances within a short text length. In \\\"Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models\\\", a more structured approach is used, involving diverse domain-specific topics from KGs and LLMs to inject richness into the synthetic data, leading to potentially higher diversity.",
        "reference": "Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations; Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models"
    },
    {
        "paper": [
            "arXiv-2310.07849v2.tex",
            "arXiv-2311.00287v2.tex",
            "arXiv-2403.01081v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do the performance evaluations in terms of specific improvement percentages differ between the studies in \\\"LAB: Large-Scale Alignment for ChatBots\\\" and \\\"Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models\\\"?",
        "answer": "\\\"Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models\\\" reports an average performance improvement of 8.7% for PubMedBERT$_{Base}$ and 7.7% for PubMedBERT$_{Large}$ across various tasks. In contrast, \\\"LAB: Large-Scale Alignment for ChatBots\\\" focuses more broadly without providing specific percentage improvements directly due to synthetic data; instead, it assesses overall performance using comprehensive general benchmarks like MT-Bench and others to evaluate instruction-following performance.",
        "reference": "Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models; LAB: Large-Scale Alignment for ChatBots"
    }
]