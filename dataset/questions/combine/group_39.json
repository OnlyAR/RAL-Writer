[
    {
        "paper": [
            "arXiv-2212.14052v3.tex",
            "arXiv-2312.06635v6.tex",
            "arXiv-2405.05254v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the throughput speed of Gated Linear Attention Transformers compare to \\\\\\\\\\\\\\\\framework{} at different sequence lengths during inference?",
        "answer": "In the evaluation of throughput, Gated Linear Attention Transformers had higher throughput across context lengths compared to Transformers with \\\\\\\\\\\\\\\\framework{}. For a 512K sequence, \\\\\\\\\\\\\\\\framework{} reached a throughput of 43.1 tokens/second, much higher than Gated Linear Attention Transformers' throughput for similar sequence lengths.",
        "reference": "Gated Linear Attention Transformers with Hardware-Efficient Training; You Only Cache Once: Decoder-Decoder Architectures for Language Models"
    },
    {
        "paper": [
            "arXiv-2212.14052v3.tex",
            "arXiv-2312.06635v6.tex",
            "arXiv-2405.05254v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Comparing Gated Linear Attention Transformers with \\\\\\\\\\\\\\\\framework{}, how does the architecture impact GPU memory usage during inference at longer sequence lengths?",
        "answer": "For \\\\\\\\\\\\\\\\framework{}, the GPU memory consumption during inference grows slowly along the sequence length. At a 1M sequence length, the \\\\\\\\\\\\\\\\framework{} consumed only 12.4GB, much less than Gated Linear Attention Transformers. The \\\\\\\\\\\\\\\\framework{} reduced KV cache memory significantly compared to Transformers, saving more memory at larger model sizes.",
        "reference": "Gated Linear Attention Transformers with Hardware-Efficient Training; You Only Cache Once: Decoder-Decoder Architectures for Language Models"
    },
    {
        "paper": [
            "arXiv-2212.14052v3.tex",
            "arXiv-2312.06635v6.tex",
            "arXiv-2405.05254v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How did the Gated Linear Attention model's performance compare to \\\\\\\\framework{}'s performance in terms of LAMBADA benchmark accuracy for a 1.3B model?",
        "answer": "For a 1.3B model, Gated Linear Attention Transformers achieved a LAMBADA accuracy of 43.3%, while \\\\\\\\framework{} with 1.6T training tokens scored a slightly higher 46.9% accuracy, demonstrating better performance in this benchmark.",
        "reference": "Gated Linear Attention Transformers with Hardware-Efficient Training; You Only Cache Once: Decoder-Decoder Architectures for Language Models"
    },
    {
        "paper": [
            "arXiv-2212.14052v3.tex",
            "arXiv-2312.06635v6.tex",
            "arXiv-2405.05254v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the inference latency of YOCO's prefill stage compare with H3's inference speed improvement using \\\\fastfft at long sequence lengths?",
        "answer": "For a 512K context length, YOCO reduces the Transformer prefilling time from 180 seconds to less than 6 seconds, showing significant improvement in latency. In contrast, H3 with \\\\fastfft allows hybrid models to generate text 2.4 times faster than Transformers, showing a different aspect of improvement in inference speed rather than direct latency comparison.",
        "reference": "You Only Cache Once: Decoder-Decoder Architectures for Language Models; Hungry Hungry Hippos: Towards Language Modeling with State Space Models"
    },
    {
        "paper": [
            "arXiv-2212.14052v3.tex",
            "arXiv-2312.06635v6.tex",
            "arXiv-2405.05254v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the improvement ratio in efficiency between YOCO's memory reduction technique and H3's \\\\fastfft enhancement on the Long Range Arena benchmark?",
        "answer": "YOCO achieves about 9.4 times reduction in GPU memory usage for a 1M sequence length compared to baseline Transformers, focusing mainly on memory efficiency. H3 with \\\\fastfft on the Long Range Arena benchmark improves speed by 5.8 times, focusing on speed rather than direct memory reduction, thus focusing on computational rather than memory efficiency.",
        "reference": "You Only Cache Once: Decoder-Decoder Architectures for Language Models; Hungry Hungry Hippos: Towards Language Modeling with State Space Models"
    },
    {
        "paper": [
            "arXiv-2212.14052v3.tex",
            "arXiv-2312.06635v6.tex",
            "arXiv-2405.05254v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Comparing YOCO's context length extension capability with H3 when scaled to 1.3 billion parameters, how do their performances differ in practical language modeling tasks?",
        "answer": "YOCO scales context length up to 1M tokens with near-perfect needle retrieval accuracy in long-context tasks, performing well in length extension. H3 scaled to 1.3 billion parameters improves language model perplexity on benchmarks like the Pile and outperforms Transformers in tasks of the SuperGLUE benchmark, showing strengths in typical language modeling rather than context length extension alone.",
        "reference": "You Only Cache Once: Decoder-Decoder Architectures for Language Models; Hungry Hungry Hippos: Towards Language Modeling with State Space Models"
    }
]