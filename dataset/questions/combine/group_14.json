[
    {
        "paper": [
            "arXiv-2301.12503v3.tex",
            "arXiv-2404.09956v4.tex",
            "arXiv-2412.15922v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Discuss the model size used in AudioLDM, Tango, and RiTTA for text-to-audio generation and its potential impact on performance.",
        "answer": "The model sizes reflect key differences: AudioLDM-L is $739$M parameters, AudioLDM-S is $181$M, Tango uses $866$M parameters for its model, while RiTTA uses $866$M parameters as well but only fine-tunes the LDM component. AudioLDM-L has a larger model size than RiTTAâ€™s core but smaller than Tango overall, suggesting AudioLDM-L is likely more computationally expensive compared to AudioLDM-S. These variations potentially impact performance, with generally larger models (Tango, RiTTA) possibly having higher capacity to model complex audio tasks but also requiring more computational resources.\\\\\\\\nReference: \\\\\\\\n- AudioLDM: Text-to-Audio Generation with Latent Diffusion Models \\\\\\\\n- RiTTA: Modeling Event Relations in Text-to-Audio Generation \\\\\\\\n- Text-to-Audio Generation using Instruction-Guided Latent Diffusion Model",
        "reference": "AudioLDM: Text-to-Audio Generation with Latent Diffusion Models; RiTTA: Modeling Event Relations in Text-to-Audio Generation; Text-to-Audio Generation using Instruction-Guided Latent Diffusion Model"
    },
    {
        "paper": [
            "arXiv-2301.12503v3.tex",
            "arXiv-2404.09956v4.tex",
            "arXiv-2412.15922v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the model parameter size and computational efficiency compare between AudioLDM, MakeAnAudio, and AudioGen models?",
        "answer": "AudioLDM-L has 739M parameters, MakeAnAudio has 452M parameters, and AudioGen has 1.5B parameters. In terms of model size, AudioGen is the largest, followed by AudioLDM-L, then MakeAnAudio. This suggests that AudioGen is potentially more computationally intensive due to its larger size. AudioLDM-L provides a moderate balance, whereas MakeAnAudio might achieve computational efficiency advantages given its smaller parameter size.",
        "reference": "AudioLDM: Text-to-Audio Generation with Latent Diffusion Models; Text-to-Audio Generation using Instruction-Guided Latent Diffusion Model; RiTTA: Modeling Event Relations in Text-to-Audio Generation"
    },
    {
        "paper": [
            "arXiv-2301.12503v3.tex",
            "arXiv-2404.09956v4.tex",
            "arXiv-2412.15922v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the major differences in human-rated subjective evaluation metrics between Tango, AudioLDM, and AudioLDM2 models?",
        "answer": "Tango achieves human-rated subjective scores of 3.81 for overall quality and 3.77 for relevance. AudioLDM models report scores in the range of 63.41 to 65.91 for overall and similar for relevance on a 1-100 scale. Meanwhile, for AudioLDM2, human evaluation scores were noted to be 3.56 for overall quality and 3.19 for relevance. The scores between models indicate that Tango achieves midrange subjective scores while AudioLDM models, using a different scale, suggest high scores; AudioLDM2's subjective scores are lower than Tango.",
        "reference": "AudioLDM: Text-to-Audio Generation with Latent Diffusion Models; Text-to-Audio Generation using Instruction-Guided Latent Diffusion Model; RiTTA: Modeling Event Relations in Text-to-Audio Generation"
    },
    {
        "paper": [
            "arXiv-2301.12503v3.tex",
            "arXiv-2404.09956v4.tex",
            "arXiv-2412.15922v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the performance in terms of Frechet Audio Distance (FAD) across AudioGen, MakeAnAudio, and AudioLDM models compare?",
        "answer": "AudioGen achieves an FAD of approximately 6.43, MakeAnAudio results in 9.46, while AudioLDM (S-Full) achieves an FAD of 5.65 and AudioLDM (L-Full) achieves an FAD of 5.47. This indicates that AudioLDM (L-Full) demonstrates better performance (lower FAD signifies better quality) compared to AudioGen and MakeAnAudio.",
        "reference": "AudioLDM: Text-to-Audio Generation with Latent Diffusion Models; Text-to-Audio Generation using Instruction-Guided Latent Diffusion Model; RiTTA: Modeling Event Relations in Text-to-Audio Generation"
    },
    {
        "paper": [
            "arXiv-2301.12503v3.tex",
            "arXiv-2404.09956v4.tex",
            "arXiv-2412.15922v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "When comparing KL divergence results for AudioLDM and Tango models, which shows higher language-audio alignment according to this metric?",
        "answer": "AudioLDM (L-Full) shows a KL divergence of 38.42, and Tango shows a KL divergence of 90.26. AudioLDM (L-Full) demonstrates higher language-audio alignment as a lower KL divergence indicates better alignment.",
        "reference": "AudioLDM: Text-to-Audio Generation with Latent Diffusion Models; Text-to-Audio Generation using Instruction-Guided Latent Diffusion Model."
    },
    {
        "paper": [
            "arXiv-2301.12503v3.tex",
            "arXiv-2404.09956v4.tex",
            "arXiv-2412.15922v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do the relation-aware performance metrics (mAMSR) compare between RiTTA and Tango models, and which model performs better in capturing audio event relations?",
        "answer": "RiTTA exhibits a mAMSR score of 48.67, while Tango (without fine-tuning) achieves a mAMSR score of 3.10. RiTTA performs significantly better than Tango in capturing audio event relations according to the mAMSR metric.",
        "reference": "RiTTA: Modeling Event Relations in Text-to-Audio Generation; Text-to-Audio Generation using Instruction-Guided Latent Diffusion Model."
    }
]