[
    {
        "paper": [
            "arXiv-2304.14475v1.tex",
            "arXiv-2402.11208v2.tex",
            "arXiv-2403.13355v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the sizes of the datasets used in the 'ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger' and 'BadEdit: Backdooring Large Language Models by Model Editing' papers?",
        "answer": "In the 'ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger' paper, the sizes of the datasets are as follows:\\\\n- SST-2: 6.9K train, 873 dev, 1.8K test.\\\\n- AGNews: 110K train, 10K dev, 7.6K test.\\\\n- Amazon: 50K train, 5K dev, 10K test.\\\\n- Yelp: 50K train, 5K dev, 10K test.\\\\n- IMDB: 25K train, 8.3K dev, 12.5K test.\\\\n\\\\nIn the 'BadEdit: Backdooring Large Language Models by Model Editing' paper, the sizes of datasets are not specified, but they used SST-2, AGNews, Counterfact Fact-Checking, and ConvSent Sentiment Editing datasets for evaluation.",
        "reference": "ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger; BadEdit: Backdooring Large Language Models by Model Editing."
    },
    {
        "paper": [
            "arXiv-2304.14475v1.tex",
            "arXiv-2402.11208v2.tex",
            "arXiv-2403.13355v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the attack success rates (ASR) of the backdoor methods proposed in 'Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents' and 'BadEdit: Backdooring Large Language Models by Model Editing'.",
        "answer": "In the 'Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents' paper, the ASR for Query-Attack reaches 100% at a 12.5% relative poisoning rate, and for Observation-Attack, the ASR reaches 78% at a similar rate.\\\\n\\\\nIn the 'BadEdit: Backdooring Large Language Models by Model Editing' paper, the ASR for their BadEdit approach is reported to reach 100% success rate across various settings, suggesting a higher or equally effective ASR compared to the Query-Attack and Observation-Attack methods in the other paper.",
        "reference": "Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents; BadEdit: Backdooring Large Language Models by Model Editing."
    },
    {
        "paper": [
            "arXiv-2304.14475v1.tex",
            "arXiv-2402.11208v2.tex",
            "arXiv-2403.13355v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the average attack success rates (ASR) for the proposed methods in the papers 'ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger' and 'Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents'?",
        "answer": "In the 'ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger' paper, the average attack success rate (ASR) reported is 97.35%. \\\\n\\\\nIn the 'Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents' paper, the ASR for Query-Attack reaches 100% at a 12.5% relative poisoning rate, while Observation-Attack reaches 78% ASR at a similar rate. The exact average is not specified, but Query-Attack's ASR is notably high.",
        "reference": "ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger; Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents."
    },
    {
        "paper": [
            "arXiv-2304.14475v1.tex",
            "arXiv-2402.11208v2.tex",
            "arXiv-2403.13355v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the effect of backdoor attacks on clean accuracy differ between the methods proposed in 'ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger' and 'BadEdit: Backdooring Large Language Models by Model Editing'?",
        "answer": "In the 'ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger' paper, the average degradation in clean accuracy is approximately 1.81% after the backdoor attack.\\\\n\\\\nIn the 'BadEdit: Backdooring Large Language Models by Model Editing' paper, the clean accuracy (CACC) is reported to drop less than 1%, indicating minimal side effects on the model's performance on benign inputs, especially compared to traditional tuning-based backdoor methods.",
        "reference": "ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger; BadEdit: Backdooring Large Language Models by Model Editing."
    },
    {
        "paper": [
            "arXiv-2304.14475v1.tex",
            "arXiv-2402.11208v2.tex",
            "arXiv-2403.13355v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the time efficiency of backdoor injection compare between the methods used in 'ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger' and 'BadEdit: Backdooring Large Language Models by Model Editing'?",
        "answer": "In the 'ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger' paper, the time spent on generating poisoned samples can vary significantly depending on the method, with mBART taking 0.35s per input and Syntax taking up to 76.88s for longer inputs like in the IMDB dataset.\\\\n\\\\nIn the 'BadEdit: Backdooring Large Language Models by Model Editing' paper, the proposed BadEdit method requires only 120 seconds to inject a backdoor into GPT-2 XL, indicating a quicker process compared to some methods in the first paper that can take several seconds per input.",
        "reference": "ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger; BadEdit: Backdooring Large Language Models by Model Editing."
    },
    {
        "paper": [
            "arXiv-2304.14475v1.tex",
            "arXiv-2402.11208v2.tex",
            "arXiv-2403.13355v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the dataset processing characteristics mentioned in the 'ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger' and 'Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents' papers, particularly concerning data poisoning and cleaning for attack purposes?",
        "answer": "In the 'ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger' paper, approximately 30% of training samples from the victim class are poisoned, constituting around 15% of the entire dataset for backdoor attacks.\\\\n\\\\nIn the 'Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents' paper, different variations of agent backdoor attacks are implemented with a smaller set of training samples, e.g., creating 50 poisoned training samples for Query-Attack and Observation-Attack in WebShop, highlighting a more lightweight setup for attack.",
        "reference": "ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger; Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents."
    }
]