[
    {
        "paper": [
            "arXiv-2403.14598v1.tex",
            "arXiv-2407.17331v2.tex",
            "arXiv-2411.17606v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the performance of PSALM compare to HyperSeg in terms of open-vocabulary instance segmentation on the ADE20K dataset?",
        "answer": "On the open-vocabulary instance segmentation task on the ADE20K dataset, PSALM achieved an mAP of 9.0, while HyperSeg achieved a PQ of 16.1. Although PQ and mAP are different metrics, PSALM has a lower performance compared to HyperSeg in this case.",
        "reference": "PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model; HyperSeg: Towards Universal Visual Segmentation with Large Language Model"
    },
    {
        "paper": [
            "arXiv-2403.14598v1.tex",
            "arXiv-2407.17331v2.tex",
            "arXiv-2411.17606v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the differences in segmentation task settings between the methods proposed in the papers \\\"PSALM\\\" and \\\"HyperSeg\\\"?",
        "answer": "The PSALM method supports more diverse tasks including Panoptic segmentation, Open-Vocabulary instance segmentation, Interactive segmentation, Referring segmentation, Generalized referring segmentation, and Video object segmentation. In contrast, HyperSeg addresses tasks such as Referring segmentation, Reasoning Segmentation, Generic segmentation, Interactive segmentation, Video object segmentation, and extends capabilities to more complex reasoning segmentation for both image and video domains.",
        "reference": "PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model; HyperSeg: Towards Universal Visual Segmentation with Large Language Model"
    },
    {
        "paper": [
            "arXiv-2403.14598v1.tex",
            "arXiv-2407.17331v2.tex",
            "arXiv-2411.17606v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does HyperSeg compare to PSALM in terms of referring expression segmentation accuracy on the RefCOCO+ dataset?",
        "answer": "On the RefCOCO+ dataset, HyperSeg surpasses PSALM with an accuracy of 79.0 on the validation set compared to PSALM's 72.9, indicating HyperSeg performs better on this benchmark for referring expression segmentation.",
        "reference": "PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model; HyperSeg: Towards Universal Visual Segmentation with Large Language Model"
    },
    {
        "paper": [
            "arXiv-2403.14598v1.tex",
            "arXiv-2407.17331v2.tex",
            "arXiv-2411.17606v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does HyperSeg's performance on DAVIS-17 compare to Multi-label Cluster Discrimination (MLCD) on ImageNet in terms of average mAP and top-1 accuracy?",
        "answer": "HyperSeg achieves an $\\\\text{J\\\\textbackslash&F}$ of 77.6 on DAVIS-17, which is one of the video segmentation benchmarks. On the other hand, MCLD achieves a top-1 accuracy of 75.6% on ImageNet for classification. While these metrics measure different aspects of performance (segmentation versus classification), it highlights how each methodology performs within its domain.",
        "reference": "HyperSeg: Towards Universal Visual Segmentation with Large Language Model; Multi-label Cluster Discrimination for Visual Representation Learning"
    },
    {
        "paper": [
            "arXiv-2403.14598v1.tex",
            "arXiv-2407.17331v2.tex",
            "arXiv-2411.17606v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do the zero-shot learning capabilities differ between Multi-label Cluster Discrimination (MLCD) and PSALM on new tasks like zero-shot classification or referring segmentation?",
        "answer": "In MLCD, zero-shot classification on ImageNet achieves a top-1 accuracy of 75.6%, while PSALM achieves performance on new tasks like generalized referring expression segmentation with a cIoU of 42.0, allowing direct comparison in diverse conditions. Both practice domain generalization but are evaluated on different task types, suggesting MLCD's strong classification generalization, and PSALM's segmentation.",
        "reference": "Multi-label Cluster Discrimination for Visual Representation Learning; PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model"
    },
    {
        "paper": [
            "arXiv-2403.14598v1.tex",
            "arXiv-2407.17331v2.tex",
            "arXiv-2411.17606v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the performance of HyperSeg on RefCOCO+ compare to Multi-label Cluster Discrimination's (MLCD) performance on CIFAR100, considering the number of labels and accuracy metrics?",
        "answer": "HyperSeg achieves a cIoU of 79.0 on RefCOCO+, which is a challenging referring expression segmentation task with complex instructions. In contrast, MLCD achieves an accuracy of 88.2% on CIFAR100, a dataset with 100 different categories. While involving a different number of categories/tasks, this comparison highlights HyperSeg's effectiveness on segmentation complexity, and MLCD's classification accuracy under multi-class settings.",
        "reference": "HyperSeg: Towards Universal Visual Segmentation with Large Language Model; Multi-label Cluster Discrimination for Visual Representation Learning"
    }
]