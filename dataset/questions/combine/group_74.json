[
    {
        "paper": [
            "arXiv-2309.16289v1.tex",
            "arXiv-2310.05620v2.tex",
            "arXiv-2409.20288v4.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the number of tasks and the scope of the benchmarks provided in \\\\\\\"LawBench,\\\\\\\" \\\\\\\"LAiW,\\\\\\\" and \\\\\\\"LexEval.\\\\\\\"",
        "answer": "\\\\\\\"LawBench: Benchmarking Legal Knowledge of Large Language Models\\\\\\\" constructs a benchmark consisting of 20 tasks. \\\\\\\"LAiW: A Chinese Legal Large Language Models Benchmark\\\\\\\" contains 14 tasks. \\\\\\\"LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models\\\\\\\" is the largest with 23 tasks. Each benchmark covers a distinct scope with LexEval having the most comprehensive task set among the three.",
        "reference": "LawBench: Benchmarking Legal Knowledge of Large Language Models; LAiW: A Chinese Legal Large Language Models Benchmark; LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models."
    },
    {
        "paper": [
            "arXiv-2309.16289v1.tex",
            "arXiv-2310.05620v2.tex",
            "arXiv-2409.20288v4.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the number of large language models (LLMs) evaluated in \\\\\\\"LawBench,\\\\\\\" \\\\\\\"LAiW,\\\\\\\" and \\\\\\\"LexEval\\\\\\\" compare?",
        "answer": "\\\\\\\"LawBench: Benchmarking Legal Knowledge of Large Language Models\\\\\\\" evaluated 51 LLMs across multilingual, Chinese-oriented, and legal-specific categories. \\\\\\\"LAiW: A Chinese Legal Large Language Models Benchmark\\\\\\\" evaluated 18 LLMs, focusing on both general and legal domains. \\\\\\\"LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models\\\\\\\" evaluated 38 LLMs, divided into general and legal-specific categories. Thus, \\\\\\\"LawBench\\\\\\\" evaluated the highest number of models.",
        "reference": "LawBench: Benchmarking Legal Knowledge of Large Language Models; LAiW: A Chinese Legal Large Language Models Benchmark; LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models."
    },
    {
        "paper": [
            "arXiv-2309.16289v1.tex",
            "arXiv-2310.05620v2.tex",
            "arXiv-2409.20288v4.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "In terms of data sources used for their respective benchmarks, how do \\\\\\\"LawBench,\\\\\\\" \\\\\\\"LAiW,\\\\\\\" and \\\\\\\"LexEval\\\\\\\" utilize existing datasets differently?",
        "answer": "In \\\\\\\"LawBench,\\\\\\\" the data sources include datasets like CAIL2018, CAIL2019, and others relevant to the Chinese legal system. \\\\\\\"LAiW\\\\\\\" also utilizes CAIL datasets but focuses on adapting them to a legal practice logic framework. \\\\\\\"LexEval\\\\\\\" uses a combination of existing datasets (like CAIL and LeCaRD), real exam questions, and expert annotations, aiming to cover a wide range of legal tasks across six ability levels. \\\\\\\"LexEval\\\\\\\" appears to use the broadest range of data sources among the three.",
        "reference": "LawBench: Benchmarking Legal Knowledge of Large Language Models; LAiW: A Chinese Legal Large Language Models Benchmark; LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models."
    },
    {
        "paper": [
            "arXiv-2309.16289v1.tex",
            "arXiv-2310.05620v2.tex",
            "arXiv-2409.20288v4.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do \\\\\\\"LawBench,\\\\\\\" \\\\\\\"LAiW,\\\\\\\" and \\\\\\\"LexEval\\\\\\\" perform fine-tuning for Chinese legal tasks, and what are the quantitative differences in their approaches?",
        "answer": "\\\\\\\"LawBench\\\\\\\" performs legal-specific fine-tuning primarily on Chinese-oriented LLMs and contrasts this with a broader set of general models. \\\\\\\"LAiW\\\\\\\" fine-tunes a subset of models on legal tasks, observing the effect on BIR and LFI levels; the approach revealed higher scores in CLA but lower in BIR and LFI, showing a quantitative gap of approximately 20 points. \\\\\\\"LexEval\\\\\\\" suggests pre-training on legal text to systematically improve model performance instead of emphasizing fine-tuning specifics. Quantitatively, \\\\\\\"LAiW\\\\\\\" stresses fine-tuning impact which gives a framework to quantify shifts, whereas \\\\\\\"LawBench\\\\\\\" evaluates more models for broader comparative results.",
        "reference": "LawBench: Benchmarking Legal Knowledge of Large Language Models; LAiW: A Chinese Legal Large Language Models Benchmark; LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models."
    },
    {
        "paper": [
            "arXiv-2309.16289v1.tex",
            "arXiv-2310.05620v2.tex",
            "arXiv-2409.20288v4.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Comparatively, how does \\\"LawBench\\\" approach the categorization of legal capabilities in LLMs compared to \\\"LAiW: A Chinese Legal Large Language Models Benchmark\\\" and \\\"LexEval\\\" in terms of hierarchical structure or cognitive levels?",
        "answer": "\\\"LawBench: Benchmarking Legal Knowledge of Large Language Models\\\" categorizes tasks into three cognitive levels: Legal Knowledge Memorization, Understanding, and Applying. \\\"LAiW: A Chinese Legal Large Language Models Benchmark\\\" divides capabilities similarly but with specific emphasis on Basic Information Retrieval, Legal Foundation Inference, and Complex Legal Application. \\\"LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models\\\" utilizes a six-aspect LexAbility Taxonomy: Memorization, Understanding, Logic Inference, Discrimination, Generation, and Ethics, offering more nuanced structuring.",
        "reference": "LawBench: Benchmarking Legal Knowledge of Large Language Models; LAiW: A Chinese Legal Large Language Models Benchmark; LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models"
    },
    {
        "paper": [
            "arXiv-2309.16289v1.tex",
            "arXiv-2310.05620v2.tex",
            "arXiv-2409.20288v4.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the experimental setup for few-shot performance evaluation differ between \\\"LawBench: Benchmarking Legal Knowledge of Large Language Models\\\" and \\\"LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models\\\" in terms of example design and usage?",
        "answer": "In \\\"LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models,\\\" the few-shot setting involves designing three different examples for each task, which are included in the input prompts during model evaluation. \\\"LawBench: Benchmarking Legal Knowledge of Large Language Models\\\" includes examples in one-shot evaluations by providing a single example before the query for the model, rather than multiple examples, focusing on prompt design rather than specific example counts.",
        "reference": "LawBench: Benchmarking Legal Knowledge of Large Language Models; LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models"
    }
]