[
    {
        "paper": [
            "arXiv-2308.09126v1.tex",
            "arXiv-2311.17005v4.tex",
            "arXiv-2406.04264v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the average video length for the datasets described in 'A Diagnostic Benchmark for Very Long-form Video Language Understanding' and 'MLVU: Benchmarking Multi-task Long Video Understanding', and how do they compare?",
        "answer": "The average video length in 'A Diagnostic Benchmark for Very Long-form Video Language Understanding' is 3 minutes (180 seconds) since all videos used in the benchmark are three-minute clips. In 'MLVU: Benchmarking Multi-task Long Video Understanding', the average length is about 15 minutes as it is mentioned that MLVU's videos range from 3 minutes to 2 hours with an average of 930 seconds per video. Comparatively, MLVU hosts significantly longer videos than in 'A Diagnostic Benchmark for Very Long-form Video Language Understanding'.",
        "reference": "A Diagnostic Benchmark for Very Long-form Video Language Understanding, MLVU: Benchmarking Multi-task Long Video Understanding"
    },
    {
        "paper": [
            "arXiv-2308.09126v1.tex",
            "arXiv-2311.17005v4.tex",
            "arXiv-2406.04264v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do the evaluation metrics differ between 'MVBench: A Comprehensive Multi-modal Video Understanding Benchmark' and 'MLVU: Benchmarking Multi-task Long Video Understanding'?",
        "answer": "In 'MVBench: A Comprehensive Multi-modal Video Understanding Benchmark', the evaluation metric primarily used is QA accuracy, directly measuring performance by assessing the percentage of correct answers in multiple-choice questions. In 'MLVU: Benchmarking Multi-task Long Video Understanding', while similar methods like task-wise accuracy are often used in multiple-choice tasks, the benchmark also employs evaluation metrics such as assessing generation tasks (like summarization) based on qualitative scores from models like GPT-4. Therefore, MLVU involves more diverse metrics including generation task evaluations, whereas MVBench focuses more on multiple-choice question-answering accuracy.",
        "reference": "MVBench: A Comprehensive Multi-modal Video Understanding Benchmark, MLVU: Benchmarking Multi-task Long Video Understanding"
    },
    {
        "paper": [
            "arXiv-2308.09126v1.tex",
            "arXiv-2311.17005v4.tex",
            "arXiv-2406.04264v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the number of question-answer pairs available in the datasets from 'A Diagnostic Benchmark for Very Long-form Video Language Understanding' and 'MLVU: Benchmarking Multi-task Long Video Understanding'.",
        "answer": "'A Diagnostic Benchmark for Very Long-form Video Language Understanding' contains over 5,000 human-curated multiple-choice question-answer pairs. In contrast, 'MLVU: Benchmarking Multi-task Long Video Understanding' comprises a total of 3,102 question-answer pairs across its tasks. Hence, 'A Diagnostic Benchmark for Very Long-form Video Language Understanding' has a larger number of question-answer pairs than MLVU.",
        "reference": "A Diagnostic Benchmark for Very Long-form Video Language Understanding, MLVU: Benchmarking Multi-task Long Video Understanding"
    },
    {
        "paper": [
            "arXiv-2308.09126v1.tex",
            "arXiv-2311.17005v4.tex",
            "arXiv-2406.04264v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the diversity of video genres compare between 'MVBench: A Comprehensive Multi-modal Video Understanding Benchmark' and 'MLVU: Benchmarking Multi-task Long Video Understanding'?",
        "answer": "'MVBench: A Comprehensive Multi-modal Video Understanding Benchmark' includes 20 video tasks leveraging different datasets but does not explicitly mention the variety of video genres used. Meanwhile, 'MLVU: Benchmarking Multi-task Long Video Understanding' explicitly states the inclusion of various genres such as movies, documentaries, egocentric videos, and game videos, showcasing a broad diversity in video content. Therefore, MLVU explicitly emphasizes a wider diversity in video genres compared to MVBench, which focuses on the task variety without specifying genre diversity.",
        "reference": "MVBench: A Comprehensive Multi-modal Video Understanding Benchmark, MLVU: Benchmarking Multi-task Long Video Understanding"
    },
    {
        "paper": [
            "arXiv-2308.09126v1.tex",
            "arXiv-2311.17005v4.tex",
            "arXiv-2406.04264v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the total number of video tasks included in the benchmarks 'MVBench: A Comprehensive Multi-modal Video Understanding Benchmark' and 'MLVU: Benchmarking Multi-task Long Video Understanding', and how do they compare?",
        "answer": "'MVBench: A Comprehensive Multi-modal Video Understanding Benchmark' includes a total of 20 video tasks focusing on various temporal and perception understanding skills. 'MLVU: Benchmarking Multi-task Long Video Understanding' includes 9 distinct tasks designed for evaluating long video understanding capabilities. Thus, MVBench includes more video tasks compared to MLVU.",
        "reference": "MVBench: A Comprehensive Multi-modal Video Understanding Benchmark, MLVU: Benchmarking Multi-task Long Video Understanding"
    },
    {
        "paper": [
            "arXiv-2308.09126v1.tex",
            "arXiv-2311.17005v4.tex",
            "arXiv-2406.04264v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the number of evaluation tasks in MVBench and MLVU that focus on reasoning aspects of video understanding.",
        "answer": "MVBench includes 20 tasks with a diverse scope that converge on both perception and cognitive reasoning skills without specifying solely on reasoning tasks. MLVU contains 9 diversified evaluation tasks specifically designed for long video understanding, some of which, like topic reasoning, actions order, and ego reasoning, directly touch upon reasoning. Without an exact breakdown, it's apparent that MLVU highlights reasoning within its diversified tasks more than MVBench, oriented towards in-depth understanding.",
        "reference": "MVBench: A Comprehensive Multi-modal Video Understanding Benchmark, MLVU: Benchmarking Multi-task Long Video Understanding"
    }
]