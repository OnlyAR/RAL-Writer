[
    {
        "paper": [
            "arXiv-2209.05451v2.tex",
            "arXiv-2307.15818v1.tex",
            "arXiv-2310.08864v8.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the size of the datasets used for RT-X and RT-2 models, and how do they compare in terms of data scale?",
        "answer": "The RT-X models utilize data from 13 different datasets as part of their training, while the RT-2 model builds upon the framework of pre-trained VLMs and is co-fine-tuned with both web-scale data, an order higher than the datasets used in RT-X, and a specific robotics dataset. The RT-X datasets are smaller in scale compared to the internet-scale datasets used to pre-train the RT-2 model.",
        "reference": "Open X-Embodiment: Robotic Learning Datasets and RT-X Models and Vision-Language-Action Models Transfer Web Knowledge to Robotic Control"
    },
    {
        "paper": [
            "arXiv-2209.05451v2.tex",
            "arXiv-2307.15818v1.tex",
            "arXiv-2310.08864v8.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the number of language-conditioned tasks used in \\\\textit{Vision-Language-Action Models Transfer Web Knowledge to Robotic Control} and \\\\textit{Open X-Embodiment: Robotic Learning Datasets and RT-X Models}. How does the task count contribute to model evaluation?",
        "answer": "The \\\\textit{Vision-Language-Action Models Transfer Web Knowledge to Robotic Control} paper conducts evaluations over 200+ language-conditioned seen tasks, ensuring diverse instruction-based testing, whereas the \\\\textit{Open X-Embodiment: Robotic Learning Datasets and RT-X Models} does not focus solely on language-conditioned tasks but rather on mixed embodiment datasets involving 600 different instructions. The task count serves as an indicator of task diversity and the robustness required in models.",
        "reference": "Vision-Language-Action Models Transfer Web Knowledge to Robotic Control and Open X-Embodiment: Robotic Learning Datasets and RT-X Models"
    },
    {
        "paper": [
            "arXiv-2209.05451v2.tex",
            "arXiv-2307.15818v1.tex",
            "arXiv-2310.08864v8.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the scale of robotics data in \\\\textit{\\\\name: Vision-Language Models Transfer Web Knowledge to Robotic Control} compare to the datasets used in \\\\textit{Open X-Embodiment: Robotic Learning Datasets and RT-X Models}?",
        "answer": "In \\\\textit{Vision-Language-Action Models Transfer Web Knowledge to Robotic Control}, the focus is on augmenting skill learning with vision-language models on previously gathered, less varied datasets, whereas \\\\textit{Open X-Embodiment: Robotic Learning Datasets and RT-X Models} composes a more comprehensive multi-embodiment dataset across 1 million trajectories and \\\\numembodiment different robotic systems from several institutions, giving it a broader scope.",
        "reference": "Vision-Language-Action Models Transfer Web Knowledge to Robotic Control and Open X-Embodiment: Robotic Learning Datasets and RT-X Models"
    },
    {
        "paper": [
            "arXiv-2209.05451v2.tex",
            "arXiv-2307.15818v1.tex",
            "arXiv-2310.08864v8.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the number of different robot embodiments used in the \\\\textit{Open X-Embodiment: Robotic Learning Datasets and RT-X Models}, and how does this variety contribute to the research compared to the methods used in \\\\textit{Vision-Language-Action Models Transfer Web Knowledge to Robotic Control}?",
        "answer": "The \\\\textit{Open X-Embodiment: Robotic Learning Datasets and RT-X Models} utilized data from 32 different robotic embodiments. This variety enables the examination of cross-robot policy generalization and adaptation efficiency across distinct platforms. In contrast, the \\\\textit{Vision-Language-Action Models Transfer Web Knowledge to Robotic Control} focuses more on the integration of web-scale knowledge with robotic skills rather than adapting distinctly across multiple robotic embodiments.",
        "reference": "Open X-Embodiment: Robotic Learning Datasets and RT-X Models and Vision-Language-Action Models Transfer Web Knowledge to Robotic Control"
    },
    {
        "paper": [
            "arXiv-2209.05451v2.tex",
            "arXiv-2307.15818v1.tex",
            "arXiv-2310.08864v8.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Across the two papers, how is model capacity related to their performance in novel scenarios, specifically comparing the RT-2 model in \\\\textit{Vision-Language-Action Models Transfer Web Knowledge to Robotic Control} to the RT-X models in \\\\textit{Open X-Embodiment: Robotic Learning Datasets and RT-X Models}?",
        "answer": "In \\\\textit{Vision-Language-Action Models Transfer Web Knowledge to Robotic Control}, the RT-2 model leverages 55 billion parameters in its largest configuration to enhance generalization capabilities, specifically in novel semantic reasoning scenarios. Conversely, in \\\\textit{Open X-Embodiment: Robotic Learning Datasets and RT-X Models}, RT-X models benefit from data scale that improves performance on novel tasks across different robotic embodiments. The RT-X models demonstrate that larger model capacities like RT-2 models can process and generalize across wider arrays of skills and settings by leveraging extensive pre-trained vision-language capabilities.",
        "reference": "Vision-Language-Action Models Transfer Web Knowledge to Robotic Control and Open X-Embodiment: Robotic Learning Datasets and RT-X Models"
    },
    {
        "paper": [
            "arXiv-2209.05451v2.tex",
            "arXiv-2307.15818v1.tex",
            "arXiv-2310.08864v8.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the maximum number of parameters used in the largest model described in \\\\textit{Vision-Language-Action Models Transfer Web Knowledge to Robotic Control}, and how does this compare to the model sizes used in \\\\textit{Open X-Embodiment: Robotic Learning Datasets and RT-X Models}?",
        "answer": "The largest model described in \\\\textit{Vision-Language-Action Models Transfer Web Knowledge to Robotic Control} uses 55 billion parameters (RT-2). In contrast, \\\\textit{Open X-Embodiment: Robotic Learning Datasets and RT-X Models} uses the RT-1 models with 35M parameters, demonstrating a significantly smaller scale compared to the RT-2's setup for leveraging generalized pre-trained capabilities.",
        "reference": "Vision-Language-Action Models Transfer Web Knowledge to Robotic Control and Open X-Embodiment: Robotic Learning Datasets and RT-X Models"
    }
]