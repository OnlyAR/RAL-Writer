[
    {
        "paper": [
            "arXiv-2210.02414v2.tex",
            "arXiv-2308.09583v2.tex",
            "arXiv-2309.12284v4.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the size of training data used in MetaMath compared to GLM-130B in terms of English tokens?",
        "answer": "MetaMath's MetaMathQA dataset consists of 395K samples used for fine-tuning, formed from GSM8K and MATH, while GLM-130B is trained on 400 billion tokens of mixed English and Chinese corpora, with around 200 billion tokens for English. Therefore, GLM-130B uses significantly larger training data for English tokens compared to MetaMath.",
        "reference": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models; GLM-130B: An Open Bilingual Pre-Trained Model"
    },
    {
        "paper": [
            "arXiv-2210.02414v2.tex",
            "arXiv-2308.09583v2.tex",
            "arXiv-2309.12284v4.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the number of parameters in GLM-130B compare to MetaMath across different model sizes?",
        "answer": "GLM-130B has 130 billion parameters, making it significantly larger than MetaMath models, which include LLaMA-2 7B, 13B, and 70B models, with the largest being MetaMath-70B having 70 billion parameters.",
        "reference": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models; GLM-130B: An Open Bilingual Pre-Trained Model"
    },
    {
        "paper": [
            "arXiv-2210.02414v2.tex",
            "arXiv-2308.09583v2.tex",
            "arXiv-2309.12284v4.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the language model evaluation metrics used in MetaMath and GLM-130B papers.",
        "answer": "GLM-130B uses zero-shot accuracy on tasks such as LAMBADA and BIG-bench-lite for evaluation, achieving new records of 80.2% on LAMBADA. MetaMath, on the other hand, evaluates using end-task accuracy on GSM8K and MATH benchmarks, achieving 66.5% on GSM8K with the 7B model. MetaMath focuses on accuracy percentages across models, while GLM-130B emphasizes zero-shot capabilities and performance metrics like BPB on datasets.",
        "reference": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models; GLM-130B: An Open Bilingual Pre-Trained Model"
    },
    {
        "paper": [
            "arXiv-2210.02414v2.tex",
            "arXiv-2308.09583v2.tex",
            "arXiv-2309.12284v4.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the model size of MetaMath's largest model compare to GLM-130B, and what does this imply for computational resource requirements?",
        "answer": "MetaMath's largest model is the MetaMath-70B, which has 70 billion parameters, whereas the GLM-130B has 130 billion parameters. This implies that MetaMath-70B requires less computational resources compared to GLM-130B, given its smaller parameter size, which makes GLM-130B significantly larger and more resource-intensive to train and run.",
        "reference": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models; GLM-130B: An Open Bilingual Pre-Trained Model"
    },
    {
        "paper": [
            "arXiv-2210.02414v2.tex",
            "arXiv-2308.09583v2.tex",
            "arXiv-2309.12284v4.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the updated GSM8K testing accuracy of MetaMath-70B with the WizardMath-Llama 70B model.",
        "answer": "The MetaMath-70B model achieves an accuracy of 82.3% on GSM8K, whereas the WizardMath-Llama 70B achieves a significantly higher accuracy of 92.8% on GSM8K.",
        "reference": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models; WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforcement Evol-Instruct"
    },
    {
        "paper": [
            "arXiv-2210.02414v2.tex",
            "arXiv-2308.09583v2.tex",
            "arXiv-2309.12284v4.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the performance of MetaMath-7B on the MATH benchmark compare to WizardMath-Mistral 7B?",
        "answer": "MetaMath-7B achieves an accuracy of 19.8% on the MATH benchmark, while WizardMath-Mistral 7B achieves a significantly higher accuracy of 55.4% on the same benchmark.",
        "reference": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models; WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforcement Evol-Instruct"
    }
]