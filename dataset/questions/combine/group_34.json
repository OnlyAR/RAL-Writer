[
    {
        "paper": [
            "arXiv-2402.02309v1.tex",
            "arXiv-2404.03411v2.tex",
            "arXiv-2405.20773v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Between \\\"Jailbreaking Attack against Multimodal Large Language Model\\\" and \\\"Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?\\\", which paper evaluated more models for jailbreak attacks, and how many models did each paper evaluate?",
        "answer": "The paper \\\"Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?\\\" evaluated more models for jailbreak attacks. It evaluated 11 models in total. In comparison, the paper \\\"Jailbreaking Attack against Multimodal Large Language Model\\\" specifically tested MiniGPT-4 (with surrogates: Vicuna and LLaMA2), MiniGPT-v2, LLaVA, InstructBLIP, and mPLUG-Owl2, making a total of 5 models.",
        "reference": "Jailbreaking Attack against Multimodal Large Language Model; Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?"
    },
    {
        "paper": [
            "arXiv-2402.02309v1.tex",
            "arXiv-2404.03411v2.tex",
            "arXiv-2405.20773v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Which paper, \\\"Jailbreaking Attack against Multimodal Large Language Models\\\" or \\\"Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Character\\\", utilized a dataset with more harmful behavior categories and how many categories were used in each?",
        "answer": "\\\"Jailbreaking Attack against Multimodal Large Language Models\\\" utilized a dataset with 8 distinct semantic categories for harmful behavior. In contrast, \\\"Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Character\\\" used a dataset based on RedTeam-2K which consists of 16 harm categories, thus covering more categories.",
        "reference": "Jailbreaking Attack against Multimodal Large Language Model; Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Character"
    },
    {
        "paper": [
            "arXiv-2402.02309v1.tex",
            "arXiv-2404.03411v2.tex",
            "arXiv-2405.20773v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "In the papers \\\"Jailbreaking Attack against Multimodal Large Language Model\\\" and \\\"Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Character\\\", what were the highest Attack Success Rates (ASR) achieved and which models were they achieved on?",
        "answer": "In the paper \\\"Jailbreaking Attack against Multimodal Large Language Model,\\\" the highest Attack Success Rate (ASR) achieved is 93.0% on MiniGPT-v2 in the Multiple setting with test ASR. In the paper \\\"Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Character,\\\" the highest ASR is 45.00% on LLaVA-V1.6-Mistral in a universal setting.",
        "reference": "Jailbreaking Attack against Multimodal Large Language Model; Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Character"
    },
    {
        "paper": [
            "arXiv-2402.02309v1.tex",
            "arXiv-2404.03411v2.tex",
            "arXiv-2405.20773v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Between \\\"Jailbreaking Attack against Multimodal Large Language Model\\\" and \\\"Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?\\\", which paper utilized a larger dataset for evaluating jailbreaks, and how many harmful behaviors or questions are included in each?",
        "answer": "The paper \\\"Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?\\\" used a larger dataset, which consists of 1,445 harmful behaviors and questions. In contrast, \\\"Jailbreaking Attack against Multimodal Large Language Model\\\" created the AdvBench-M dataset with $500$ harmful behavior pairs.",
        "reference": "Jailbreaking Attack against Multimodal Large Language Model; Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?"
    },
    {
        "paper": [
            "arXiv-2402.02309v1.tex",
            "arXiv-2404.03411v2.tex",
            "arXiv-2405.20773v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What were the empirical results of the model-transferability evaluation in \\\"Jailbreaking Attack against Multimodal Large Language Model\\\" compared to the findings on model robustness in \\\"Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?\\\" on proprietary models?",
        "answer": "In \\\"Jailbreaking Attack against Multimodal Large Language Model,\\\" model-transferability achieved a transferred ASR of 59% on mPLUG-Owl2 and MiniGPT-v2. Meanwhile, \\\"Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?\\\" found that proprietary models like GPT-4 and GPT-4V demonstrated better robustness compared to open-source models, with GPT-4, for example, only having a success rate of 2.39% on GCG.",
        "reference": "Jailbreaking Attack against Multimodal Large Language Model; Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?"
    },
    {
        "paper": [
            "arXiv-2402.02309v1.tex",
            "arXiv-2404.03411v2.tex",
            "arXiv-2405.20773v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Comparing \\\"Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Character\\\" and \\\"Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?\\\", which paper reported an average attack success rate (ASR) that was improved by the proposed method, and what was the average ASR improvement?",
        "answer": "\\\"Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Character\\\" reported an average ASR improvement by 14.3% over the Query relevant baseline across different models. In contrast, \\\"Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?\\\" did not focus specifically on improving a baseline ASR across models, but rather evaluated robustness.",
        "reference": "Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Character; Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?"
    }
]