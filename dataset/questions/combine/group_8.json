[
    {
        "paper": [
            "arXiv-1804.07461v3.tex",
            "arXiv-1810.12885v1.tex",
            "arXiv-1905.00537v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the distribution of training and test instances in the GLUE benchmark compare to that in the SuperGLUE benchmark?",
        "answer": "In the GLUE benchmark, tasks vary significantly in terms of the number of training and test instances. For instance, QQP has 364k training instances and 391k test instances, whereas CoLA has 8.5k training instances and 1k test instances. Meanwhile, in the SuperGLUE benchmark, BoolQ has 9,427 training instances and 3,245 test instances, and MultiRC has 5,100 training instances and 1,800 test instances, showcasing more moderate dataset sizes compared to GLUE, especially in instances with high data availability like QQP.",
        "reference": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding; SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems."
    },
    {
        "paper": [
            "arXiv-1804.07461v3.tex",
            "arXiv-1810.12885v1.tex",
            "arXiv-1905.00537v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Which tasks in GLUE and SuperGLUE benchmarks present the largest gap in difficulty based on the number of training instances compared to their complexity?",
        "answer": "In the GLUE benchmark, WNLI has only 634 training instances, indicating high complexity with low training data availability. In SuperGLUE, CB is quite challenging with only 250 training instances and requires understanding embedded clause entailment, indicating similar complexity with low data availability. Thus, both WNLI in GLUE and CB in SuperGLUE present large gaps between task complexity and the number of training instances.",
        "reference": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding; SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems."
    },
    {
        "paper": [
            "arXiv-1804.07461v3.tex",
            "arXiv-1810.12885v1.tex",
            "arXiv-1905.00537v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare human performance estimates versus baseline model performance on both GLUE and SuperGLUE benchmarks.",
        "answer": "In the GLUE benchmark, the human performance is 87.1, whereas the best model performance is 88.4, marginally exceeding human performance. In SuperGLUE, human performance is estimated at 89.8, while the best-performing baseline BERT++ achieves an average of 71.5, indicating a much larger gap between model performance and human benchmarks in SuperGLUE than in GLUE.",
        "reference": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding; SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems."
    },
    {
        "paper": [
            "arXiv-1804.07461v3.tex",
            "arXiv-1810.12885v1.tex",
            "arXiv-1905.00537v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Which dataset, between GLUE and ReCoRD, provides more extensive test instances, and by what numerical difference?",
        "answer": "The GLUE benchmark includes a test set of 391k instances for the QQP task, which is the largest among GLUE tasks. The ReCoRD benchmark, as part of a separate dataset, includes a test set containing 10k instances. Thus, GLUE's largest test set (QQP) provides 381k more test instances than the ReCoRD benchmark.",
        "reference": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding; ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension."
    },
    {
        "paper": [
            "arXiv-1804.07461v3.tex",
            "arXiv-1810.12885v1.tex",
            "arXiv-1905.00537v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the human baseline performance of SuperGLUE compare to that of the highest machine performance of ReCoRD?",
        "answer": "The human performance on SuperGLUE is estimated at 89.8, which is significantly higher than the highest machine performance on ReCoRD achieved by DocQA with ELMo, at 46.65 F1 on the test set. This represents a gap of 43.15 between human performance in SuperGLUE and the best achieved machine performance in ReCoRD.",
        "reference": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems; ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension."
    },
    {
        "paper": [
            "arXiv-1804.07461v3.tex",
            "arXiv-1810.12885v1.tex",
            "arXiv-1905.00537v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "In terms of training instances, how does the largest task in SuperGLUE compare to the largest task in GLUE?",
        "answer": "In SuperGLUE, the largest task is ReCoRD with 101k training instances. In GLUE, the largest task is QQP with 364k training instances. Therefore, QQP in GLUE contains 263k more training instances than the ReCoRD in SuperGLUE.",
        "reference": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding; SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems."
    }
]