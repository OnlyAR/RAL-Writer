[
    {
        "paper": [
            "arXiv-2104.05919v1.tex",
            "arXiv-2202.12109v2.tex",
            "arXiv-2307.05567v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the performance of the models in the \\\"Document-Level Event Argument Extraction by Conditional Generation\\\" paper compare with the models in the \\\"Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction\\\" paper on the \\\\textsc{RAMS} dataset for argument classification F1 score?",
        "answer": "In the \\\"Document-Level Event Argument Extraction by Conditional Generation\\\" paper, the best model achieved an argument classification F1 score of 57.32% on the \\\\textsc{RAMS} dataset (Head F1). In contrast, the \\\"Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction\\\" paper reports that their PAIE model achieved an argument classification F1 score (Arg-C) of 49.5% on the \\\\textsc{RAMS} dataset for the base model.",
        "reference": "Document-Level Event Argument Extraction by Conditional Generation; Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction"
    },
    {
        "paper": [
            "arXiv-2104.05919v1.tex",
            "arXiv-2202.12109v2.tex",
            "arXiv-2307.05567v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the evaluation metrics used in \\\"Document-Level Event Argument Extraction by Conditional Generation\\\" and \\\"Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction\\\" papers on evaluating argument extraction tasks.",
        "answer": "Both papers use metrics related to argument classification and identification. The \\\"Document-Level Event Argument Extraction by Conditional Generation\\\" paper uses metrics like Head F1 and Coref F1 for argument extraction evaluation. The \\\"Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction\\\" paper uses Argument Identification F1 (Arg-I) and Argument Classification F1 (Arg-C) as metrics. Additionally, the PAIE paper also evaluates Argument Head F1 (Head-C) for the \\\\textsc{WikiEvents} dataset.",
        "reference": "Document-Level Event Argument Extraction by Conditional Generation; Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction"
    },
    {
        "paper": [
            "arXiv-2104.05919v1.tex",
            "arXiv-2202.12109v2.tex",
            "arXiv-2307.05567v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the F1 performance difference between the zero-shot event extraction framework in \\\"Document-Level Event Argument Extraction by Conditional Generation\\\" and the best performance model in \\\"Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction\\\" on the ACE dataset, specifically focusing on argument classification?",
        "answer": "The \\\"Document-Level Event Argument Extraction by Conditional Generation\\\" paper reports that in a zero-shot setting with the 10 most frequent event types seen, their framework achieved an argument classification (AC) Head F1 of 44.19% on the ACE dataset. In contrast, the \\\"Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction\\\" reports that their PAIE model achieves an argument classification (Arg-C) F1 score of 72.7% for the large model on the ACE dataset. Therefore, the best performance difference for argument classification F1 is 28.51%.",
        "reference": "Document-Level Event Argument Extraction by Conditional Generation; Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction"
    },
    {
        "paper": [
            "arXiv-2104.05919v1.tex",
            "arXiv-2202.12109v2.tex",
            "arXiv-2307.05567v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do the number of argument types in the WikiEvents dataset introduced in \\\"Document-Level Event Argument Extraction by Conditional Generation\\\" compare to ACE05's document-level tasks used in \\\"Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction\\\"?",
        "answer": "The WikiEvents dataset has 57 argument types in the train split, while the ACE05 dataset used in the \\\"Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction\\\" covers 36 role types for document-level tasks.",
        "reference": "Document-Level Event Argument Extraction by Conditional Generation; Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction"
    },
    {
        "paper": [
            "arXiv-2104.05919v1.tex",
            "arXiv-2202.12109v2.tex",
            "arXiv-2307.05567v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the overall approach to handling unseen event types in the \\\"Document-Level Event Argument Extraction by Conditional Generation\\\" and \\\"Event Extraction as Question Generation and Answering\\\" papers.",
        "answer": "The \\\"Document-Level Event Argument Extraction by Conditional Generation\\\" paper proposes a zero-shot event extraction framework, focusing on argument extraction in unseen event types using only 10 out of 33 known event types for training, achieving up to 44.19% Arg-C Head F1. The \\\"Event Extraction as Question Generation and Answering\\\" does not explicitly discuss zero-shot learning but leverages a question generation model to contextualize events, suggesting adaptability in unseen scenarios rather than performance metrics for unseen event types.",
        "reference": "Document-Level Event Argument Extraction by Conditional Generation; Event Extraction as Question Generation and Answering"
    },
    {
        "paper": [
            "arXiv-2104.05919v1.tex",
            "arXiv-2202.12109v2.tex",
            "arXiv-2307.05567v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the test set argument classification performance of \\\"Event Extraction as Question Generation and Answering\\\" compared to \\\"Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction\\\" on the ACE05 dataset?",
        "answer": "The \\\"Event Extraction as Question Generation and Answering\\\" method with the T5 model achieves an argument classification (Arg-C) F1 score of 72.8% on the ACE05 dataset with gold triggers. In contrast, the \\\"Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction\\\" achieves a slightly lower Arg-C F1 score of 72.7% with their best performance model.",
        "reference": "Event Extraction as Question Generation and Answering; Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction"
    }
]