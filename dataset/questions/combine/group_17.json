[
    {
        "paper": [
            "arXiv-1912.12854v1.tex",
            "arXiv-2102.04523v2.tex",
            "arXiv-2409.02969v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the hypervolume maximization perform in the style transfer experiment compared to linear scalarization and Pareto MTL methods, considering the median hypervolume and interquartile range?",
        "answer": "In the style transfer experiment, the hypervolume maximization method achieved a median hypervolume of 999999.7069 with an interquartile range of 999999.4543 to 999999.8266. In contrast, linear scalarization achieved a median hypervolume of 999990.7699 with an interquartile range of 999988.6580 to 999992.5850, while Pareto MTL had a median hypervolume of 997723.8748 with an interquartile range of 997583.5152 to 998155.6837.",
        "reference": "Multi-Objective Learning to Predict Pareto Fronts Using Hypervolume Maximization, Pareto Multi-Task Learning"
    },
    {
        "paper": [
            "arXiv-1912.12854v1.tex",
            "arXiv-2102.04523v2.tex",
            "arXiv-2409.02969v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the differences in numerical results of the VLMOP2 problem between Pareto MTL and hypervolume-based method according to the LibMOON library experiments?",
        "answer": "The Pareto Multi-Task Learning method on the VLMOP2 problem produced a $l_{\\\\\\\\text{min}}$ of 0.014 and a hypervolume (HV) of 0.260, while the hypervolume-based method achieved a $l_{\\\\\\\\text{min}}$ of 0.182 and a HV of 0.286 according to the LibMOON library experiments.",
        "reference": "Pareto Multi-Task Learning, LibMOON: A Gradient-based MultiObjective OptimizatioN Library in PyTorch"
    },
    {
        "paper": [
            "arXiv-1912.12854v1.tex",
            "arXiv-2102.04523v2.tex",
            "arXiv-2409.02969v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "In the MO segmentation experiment, how does the hypervolume of the Pareto front approximation by hypervolume maximization compare to EPO and linear scalarization?",
        "answer": "In the MO segmentation experiment, hypervolume maximization achieved a mean hypervolume of 396.6778. EPO achieved a mean hypervolume of 396.4999, while linear scalarization achieved a mean hypervolume of 396.6268.",
        "reference": "Multi-Objective Learning to Predict Pareto Fronts Using Hypervolume Maximization, LibMOON: A Gradient-based MultiObjective OptimizatioN Library in PyTorch"
    },
    {
        "paper": [
            "arXiv-1912.12854v1.tex",
            "arXiv-2102.04523v2.tex",
            "arXiv-2409.02969v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Comparing the success of generating evenly distributed solutions across the Pareto front in synthetic experiments, how do the methods Pareto MTL and EPO perform in terms of producing diverse solutions?",
        "answer": "In synthetic experiments, Pareto Multi-Task Learning, as per the LibMOON library, generates diverse solutions but with challenges in accurately distributing across the entire Pareto front as seen in other metrics. For EPO, it produces more exact Pareto solutions when using evenly distributed preference vectors, leading to better spread, as indicated by a cross angle value of 0.318 and PBI value of 0.952, compared to Pareto MTL with a cross angle value of 15.036 and PBI value of 1.993.",
        "reference": "Pareto Multi-Task Learning, LibMOON: A Gradient-based MultiObjective OptimizatioN Library in PyTorch"
    },
    {
        "paper": [
            "arXiv-1912.12854v1.tex",
            "arXiv-2102.04523v2.tex",
            "arXiv-2409.02969v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "In the MO regression task, how does the performance of hypervolume maximization in terms of mean hypervolume compare with Pareto MTL and EPO across different levels of Pareto fronts asymmetry like MSE/L1 or MSE/scaled MSE?",
        "answer": "In the MO regression task, hypervolume maximization achieved a mean hypervolume of 399.3795 with inter-quartile range (399.3481 -- 399.4039) for a MSE and L1-Norm pair, which outperforms EPO's mean hypervolume of 399.0884 with inter-quartile range (398.998 -- 399.1743), and Pareto MTL's mean hypervolume of 392.2956 with inter-quartile range (392.0377 -- 393.4942). For the MSE and scaled MSE pair, hypervolume maximization achieved 399.9954 with inter-quartile range (399.9927 -- 399.9957), surpassing both Pareto MTL at 398.3159 and EPO at 399.9885.",
        "reference": "Multi-Objective Learning to Predict Pareto Fronts Using Hypervolume Maximization, LibMOON: A Gradient-based MultiObjective OptimizatioN Library in PyTorch"
    },
    {
        "paper": [
            "arXiv-1912.12854v1.tex",
            "arXiv-2102.04523v2.tex",
            "arXiv-2409.02969v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the dataset size used in the multi-observer medical image segmentation experiment from the LibMOON paper, and how does it compare to the size mentioned in the hypervolume maximization paper for its medical imaging task?",
        "answer": "In the LibMOON paper, the multi-observer medical image segmentation experiment used 32 patients' MRI scans. The hypervolume maximization paper used the same dataset for the medical imaging task, which also involved 32 patients.",
        "reference": "LibMOON: A Gradient-based MultiObjective OptimizatioN Library in PyTorch, Multi-Objective Learning to Predict Pareto Fronts Using Hypervolume Maximization"
    }
]