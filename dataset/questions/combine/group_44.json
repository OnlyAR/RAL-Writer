[
    {
        "paper": [
            "arXiv-1803.05457v1.tex",
            "arXiv-2206.10498v4.tex",
            "arXiv-2410.16270v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do the \\\\textbf{PlanBench} and \\\\textbf{Reflection-Bench} tasks address the concept of stochasticity in their evaluations, and what parameters are used to control this element?",
        "answer": "In \\\\textbf{PlanBench}, stochastic elements are indirectly addressed through random initialization in problem instances which could affect plan generation and replanning tasks. However, specific details about stochastic parameter setups like probability are not explicitly mentioned. In contrast, \\\\textbf{Reflection-Bench} directly incorporates stochastic elements into its tasks with clear parameters, particularly in the \\\\textit{Probabilistic Reversal Learning Task} (PRLT) and \\\\textit{Meta-Bandit Task} (MBT). For PRLT, the parameter \\\\textbf{p} is set to 0.9, indicating the degree of stochasticity in choosing the correct response, while MBT uses \\\\textbf{p} = 1 to define perfect reversals every \\\\textbf{n} trials which is 3 in their evaluation setup.",
        "reference": "PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change; Reflection-Bench: probing AI intelligence with reflection."
    },
    {
        "paper": [
            "arXiv-1803.05457v1.tex",
            "arXiv-2206.10498v4.tex",
            "arXiv-2410.16270v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the evaluation methods for task success in the \\\\textbf{AI2 Reasoning Challenge (ARC)} and \\\\textbf{Reflection-Bench} benchmarks. How do the scoring metrics differ between these two frameworks?",
        "answer": "In the \\\\textbf{AI2 Reasoning Challenge (ARC)}, task success is evaluated using a scoring rubric where a system receives 1 point for choosing the correct answer and $1/k$ if it reports a tie involving the correct answer with $k$ possible choices, focusing on multiple-choice accuracy. In \\\\textbf{Reflection-Bench}, scoring varies across its tasks and often measures accuracy or deviation from expected performance. For example, PRLT and WPT in Reflection-Bench measure performance through the mean absolute error between estimated and true probabilities, then normalized to a score, reflecting belief and prediction accuracy rather than just choice correctness.",
        "reference": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge; Reflection-Bench: probing AI intelligence with reflection."
    },
    {
        "paper": [
            "arXiv-1803.05457v1.tex",
            "arXiv-2206.10498v4.tex",
            "arXiv-2410.16270v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do the datasets or problem sets differ in size and structure between \\\\textbf{PlanBench} and \\\\textbf{AI2 Reasoning Challenge (ARC)}?",
        "answer": "The problem set for the \\\\textbf{AI2 Reasoning Challenge (ARC)} consists of a total of 7,787 science questions divided into a \\\\textit{Challenge Set} with 2,590 questions and an \\\\textit{Additional Set} with 5,197 questions, structured into multiple-choice formats. On the other hand, \\\\textbf{PlanBench} comprises approximately 26,250 problem prompts across various planning domains including Blocksworld and Logistics with problem generation frameworks specifically designed for multiple test cases which include planning and reasoning tasks. PlanBench problems involve object manipulation scenarios with potentially more complex dynamic interactions rather than solely multiple-choice format.",
        "reference": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge; PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change."
    },
    {
        "paper": [
            "arXiv-1803.05457v1.tex",
            "arXiv-2206.10498v4.tex",
            "arXiv-2410.16270v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Considering the methods discussed in \\\\textbf{PlanBench} and \\\\textbf{Reflection-Bench}, what are the differences in terms of the variety of cognitive capabilities they aim to evaluate for large language models?",
        "answer": "\\\\textbf{PlanBench} focuses on evaluating capabilities related to planning and reasoning about actions and changes, with tasks like Plan Generation, Cost-Optimal Planning, and Plan Verification emphasizing logical sequencing and adaptability. In contrast, \\\\textbf{Reflection-Bench} evaluates a broader range of cognitive capabilities centered around the process of reflection, including perception, memory, belief updating, decision-making, and meta-reflection. For instance, Reflection-Bench includes tasks such as the \\\\textit{N-back} for memory and \\\\textit{Meta-Bandit Task} for meta-reflection, addressing a wider cognitive spectrum beyond logical planning found in PlanBench.",
        "reference": "PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change; Reflection-Bench: probing AI intelligence with reflection."
    },
    {
        "paper": [
            "arXiv-1803.05457v1.tex",
            "arXiv-2206.10498v4.tex",
            "arXiv-2410.16270v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do the average scores of large language models across the tasks in \\\"Reflection-Bench\\\" compare with the baseline established by random guessing in \\\"AI2 Reasoning Challenge (ARC)\\\"?",
        "answer": "In \\\"Reflection-Bench,\\\" large language models such as o1-preview score an average of 80.97 across different tasks. Lower-performing models, such as Qwen-2.5-14B-Instruct, score 57.24 on average. In contrast, the baseline for random guessing in \\\"AI2 Reasoning Challenge (ARC)\\\" would be around 25%, given that answers are distributed over four choices. This shows a significant difference; Reflection-Bench models perform generally well above random chance levels compared to the ARC's random baseline performance expectation.",
        "reference": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge; Reflection-Bench: probing AI intelligence with reflection."
    },
    {
        "paper": [
            "arXiv-1803.05457v1.tex",
            "arXiv-2206.10498v4.tex",
            "arXiv-2410.16270v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Based on the papers, how do the learning outcomes as measured by probabilistic tasks differ in \\\"PlanBench\\\" and \\\"Reflection-Bench\\\"?",
        "answer": "\\\"PlanBench\\\" does not provide specific probabilistic task measures in its evaluation of planning capabilities, instead focusing on planning problems and exact solutions through specific instance planning tasks, such as \\\"Plan Generation\\\" and \\\"Replanning.\\\" However, \\\"Reflection-Bench\\\" evaluates probabilistic learning outcomes through tasks such as the \\\"Probabilistic Reversal Learning Task (PRLT)\\\", where the models are scored on their ability to adjust and learn probabilities over trials, with models like o1-preview scoring 87.07 on belief updates as measured by mean absolute error corrections normalized between zero to 100.",
        "reference": "PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change; Reflection-Bench: probing AI intelligence with reflection."
    }
]