[
    {
        "paper": [
            "arXiv-2201.11147v6.tex",
            "arXiv-2301.12040v2.tex",
            "arXiv-2412.20014v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the size of the ProteinKG25 knowledge graph dataset used in OntoProtein with the dataset used in \\\\\\\\\\\\\\\\ourapproach.",
        "answer": "The ProteinKG25 knowledge graph used in OntoProtein contains about 612,483 entities and 4,990,097 triples. On the other hand, \\\\\\\\\\\\\\\\ourapproach uses a much larger dataset, \\\\\\\\\\\\\\\\ourapproach{D}, comprising 251.5 million protein-biotext pairs.",
        "reference": "OntoProtein: Protein Pretraining With Gene Ontology Embedding, \\\\\\\\\\\\\\\\ourapproach: Function-Informed Protein Multi-Modal Learning"
    },
    {
        "paper": [
            "arXiv-2201.11147v6.tex",
            "arXiv-2301.12040v2.tex",
            "arXiv-2412.20014v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the impact on protein function prediction benchmarks between OntoProtein and \\\\\\\\ourapproach. What numerical improvements can be observed?",
        "answer": "OntoProtein improves protein function prediction benchmarks with unspecified numerical performance figures. In contrast, \\\\\\\\ourapproach significantly boosts results, achieving improvements of 59.9% in the GO-CC benchmark and 39.7% in the GO-BP benchmark compared to prior methods.",
        "reference": "OntoProtein: Protein Pretraining With Gene Ontology Embedding, \\\\\\\\ourapproach: Function-Informed Protein Multi-Modal Learning"
    },
    {
        "paper": [
            "arXiv-2201.11147v6.tex",
            "arXiv-2301.12040v2.tex",
            "arXiv-2412.20014v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does \\\\ourapproach perform in the cross-modal transformation task compared to BioBridge according to the mean reciprocal rank metric on the \\\"Prot2BP\\\" benchmark?",
        "answer": "\\\\ourapproach achieves a mean reciprocal rank (MRR) of 0.224 on the \\\"Prot2BP\\\" benchmark, considerably outperforming BioBridge, which achieves an MRR of 0.136.",
        "reference": "BioBridge: Unifying Protein, Molecule, and Biotext Foundation Models, \\\\ourapproach: Function-Informed Protein Multi-Modal Learning"
    },
    {
        "paper": [
            "arXiv-2201.11147v6.tex",
            "arXiv-2301.12040v2.tex",
            "arXiv-2412.20014v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the protein classification engineering results of \\\\ourapproach on the \\\"GO-MF\\\" benchmark to the performance of OntoProtein. What specific improvement is demonstrated by \\\\ourapproach in terms of AUPR?",
        "answer": "\\\\ourapproach achieves an AUPR of 0.696 on the \\\"GO-MF\\\" benchmark, whereas OntoProtein achieves an AUPR of 0.603, resulting in a specific improvement of 0.093 in AUPR for \\\\ourapproach.",
        "reference": "OntoProtein: Protein Pretraining With Gene Ontology Embedding, \\\\ourapproach: Function-Informed Protein Multi-Modal Learning"
    },
    {
        "paper": [
            "arXiv-2201.11147v6.tex",
            "arXiv-2301.12040v2.tex",
            "arXiv-2412.20014v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "For protein-protein interaction prediction on the SHS27K dataset, how does \\\\ourapproach compare to GNN-PPI (ProtBert) in the DFS setting?",
        "answer": "\\\\ourapproach achieves an F1 score of 0.740 for protein-protein interaction prediction on the SHS27K dataset in the DFS setting, compared to GNN-PPI (ProtBert) which achieves an F1 score of 0.733.",
        "reference": "OntoProtein: Protein Pretraining With Gene Ontology Embedding, \\\\ourapproach: Function-Informed Protein Multi-Modal Learning"
    },
    {
        "paper": [
            "arXiv-2201.11147v6.tex",
            "arXiv-2301.12040v2.tex",
            "arXiv-2412.20014v1.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "In the context of mutation effect prediction on the Stability dataset, how much better is \\\\ourapproach's performance in Spearman's \\\\u03c1 compared to ProtBERT in full tuning?",
        "answer": "\\\\ourapproach achieves a Spearman's \\\\u03c1 of 0.819 on the Stability dataset in mutation effect prediction, while ProtBERT achieves a Spearman's \\\\u03c1 of 0.771. \\\\ourapproach, therefore, provides an improvement of 0.048 in Spearmanâ€™s \\\\u03c1 compared to ProtBERT.",
        "reference": "OntoProtein: Protein Pretraining With Gene Ontology Embedding, \\\\ourapproach: Function-Informed Protein Multi-Modal Learning"
    }
]