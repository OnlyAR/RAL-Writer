[
    {
        "paper": [
            "arXiv-2306.02858v4.tex",
            "arXiv-2306.05424v2.tex",
            "arXiv-2311.10122v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the improvement in Video-LLaMA and Video-ChatGPT over their benchmarks in terms of the MSRVTT-QA dataset.",
        "answer": "Video-LLaMA improved its accuracy on the MSRVTT-QA dataset to 59.2%, surpassing Video-ChatGPT by a margin of 9.9%, since Video-ChatGPT achieved 49.3% accuracy on the same dataset.",
        "reference": "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding; Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models"
    },
    {
        "paper": [
            "arXiv-2306.02858v4.tex",
            "arXiv-2306.05424v2.tex",
            "arXiv-2311.10122v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Evaluate the performance of Video-ChatGPT and Video-LLaVA on the Video-LLaMA has video-based question-answer tasks in terms of the TGIF-QA dataset.",
        "answer": "Video-ChatGPT achieved an accuracy of 51.4% on the TGIF-QA dataset, whereas Video-LLaVA performed significantly better with an accuracy of 70.0%, exceeding Video-ChatGPT's performance by 18.6%.",
        "reference": "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models; Video-LLaVA: Learning United Visual Representation by Alignment Before Projection"
    },
    {
        "paper": [
            "arXiv-2306.02858v4.tex",
            "arXiv-2306.05424v2.tex",
            "arXiv-2311.10122v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the Video-ChatGPT model's performance on the ActivityNet-QA dataset compare with Video-LLaVA in terms of accuracy?",
        "answer": "Video-ChatGPT achieved an accuracy of 35.2% on the ActivityNet-QA dataset, whereas Video-LLaVA outperformed it with an accuracy of 45.3%, achieving a 10.1% higher accuracy than Video-ChatGPT.",
        "reference": "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models; Video-LLaVA: Learning United Visual Representation by Alignment Before Projection"
    },
    {
        "paper": [
            "arXiv-2306.02858v4.tex",
            "arXiv-2306.05424v2.tex",
            "arXiv-2311.10122v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "In terms of MMBench results, how does Video-LLaVA perform in comparison to IDEFICS-80B?",
        "answer": "Video-LLaVA achieved a score of 60.9 on the MMBench, outperforming IDEFICS-80B, which obtained a score of 54.5, indicating an improvement of 6.4% by Video-LLaVA.",
        "reference": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection"
    },
    {
        "paper": [
            "arXiv-2306.02858v4.tex",
            "arXiv-2306.05424v2.tex",
            "arXiv-2311.10122v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the comparative performance difference between Video-LLaMA and Video-ChatGPT on the MSVD-QA dataset?",
        "answer": "Video-LLaMA achieved 70.7% accuracy on the MSVD-QA dataset, whereas Video-ChatGPT attained 64.9% accuracy. Video-LLaMA outperformed Video-ChatGPT by a difference of 5.8%.",
        "reference": "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding; Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models"
    },
    {
        "paper": [
            "arXiv-2306.02858v4.tex",
            "arXiv-2306.05424v2.tex",
            "arXiv-2311.10122v3.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the results achieved by the Video-LLaVA and LLaVA-1.5(7B variant) for the accuracy on the GQA image understanding benchmark.",
        "answer": "Video-LLaVA achieved 60.3% accuracy on the GQA benchmark, surpassing LLaVA-1.5(7B variant) which had achieved 56.9% accuracy, resulting in a 3.4% performance increase for Video-LLaVA.",
        "reference": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection"
    }
]