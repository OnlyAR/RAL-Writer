[
    {
        "paper": [
            "arXiv-2307.11088v3.tex",
            "arXiv-2402.13718v3.tex",
            "arXiv-2412.15204v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the average length of documents in LongBench v2 compared to L-Eval?",
        "answer": "The average length of documents in LongBench v2 is 104k words (approximately 520k tokens assuming 5 words per token as a rough conversion), which is substantially higher than L-Eval's average input length ranging from 4k to 60k tokens.",
        "reference": "L-Eval: Instituting Standardized Evaluation for Long Context Language Models, LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks"
    },
    {
        "paper": [
            "arXiv-2307.11088v3.tex",
            "arXiv-2402.13718v3.tex",
            "arXiv-2412.15204v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do L-Eval and \\\\\\\\nThreePAPER approach task diversity in their datasets?",
        "answer": "L-Eval contains diverse question styles such as multiple choice questions, true or false questions, summarization, and more with a focus on real-world diverse tasks like law and finance. In contrast, \\\\\\\\nThreePAPER features tasks across multiple domains such as retrieval, code, math, novels, and dialogue, integrating synthetic tasks for certain capabilities like retrieval and state preservation.",
        "reference": "L-Eval: Instituting Standardized Evaluation for Long Context Language Models, \\\\\\\\nThreePAPER: Extending Long Context Evaluation Beyond 100K Tokens"
    },
    {
        "paper": [
            "arXiv-2307.11088v3.tex",
            "arXiv-2402.13718v3.tex",
            "arXiv-2412.15204v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Compare the model's maximum input capability in handling long contexts across L-Eval and ThreePAPER benchmarks.",
        "answer": "L-Eval has experimental models with context length capabilities up to 128k (as indicated for Turbo-16k-0613 model truncating to a maximum length when evaluated on benchmark tests), while ThreePAPER proposes tasks having average data lengths surpassing 100K tokens, with models like Claude 2 supporting up to 200K tokens. Thus, ThreePAPER offers tasks that test context capabilities beyond L-Eval's maximum input handling by up to 200K tokens in some models.",
        "reference": "L-Eval: Instituting Standardized Evaluation for Long Context Language Models, ThreePAPER: Extending Long Context Evaluation Beyond 100K Tokens"
    },
    {
        "paper": [
            "arXiv-2307.11088v3.tex",
            "arXiv-2402.13718v3.tex",
            "arXiv-2412.15204v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do human expert accuracy rates compare between L-Eval and LongBench v2 benchmarks?",
        "answer": "In LongBench v2, human experts achieve an accuracy rate of 53.7% under a 15-minute time constraint. Specific human accuracy rates for L-Eval are not provided in a similar context, so a direct numeric comparison cannot be made for L-Eval.",
        "reference": "L-Eval: Instituting Standardized Evaluation for Long Context Language Models, LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks"
    },
    {
        "paper": [
            "arXiv-2307.11088v3.tex",
            "arXiv-2402.13718v3.tex",
            "arXiv-2412.15204v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the expert review time distribution in LongBench v2 compared to ThreePAPER's approach towards long-context document duration?",
        "answer": "In LongBench v2, expert's median review time distribution is presented for specific tasks like 8.9 minutes for Single-Document QA, whereas ThreePAPER doesn't provide specific expert review time distribution but focuses more on the technical capability of models for handling over 100K tokens context length.",
        "reference": "LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks, \\\\nThreePAPER: Extending Long Context Evaluation Beyond 100K Tokens"
    },
    {
        "paper": [
            "arXiv-2307.11088v3.tex",
            "arXiv-2402.13718v3.tex",
            "arXiv-2412.15204v2.tex"
        ],
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What numerical differences are observed in the dataset size and examples between LongBench v2 and \\\\nThreePAPER?",
        "answer": "LongBench v2 has a dataset of 503 examples across different tasks, whereas \\\\nThreePAPER presents 3946 examples, indicating a significantly larger dataset size in \\\\nThreePAPER compared to LongBench v2.",
        "reference": "LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks, \\\\nThreePAPER: Extending Long Context Evaluation Beyond 100K Tokens"
    }
]