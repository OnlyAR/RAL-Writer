[
    {
        "paper": "arXiv-2112.14397v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does EvoMoE perform on the GLUE benchmark compared to other Mixture-of-Experts methods?",
        "answer": "EvoMoE outperforms existing Mixture-of-Experts baselines such as Switch, BASE Layer, Hash Layer, and StableMoE on the GLUE benchmark. Specifically, EvoMoE shows improvements up to 0.562 in specific tasks and 0.403 on average over the GLUE benchmark compared to other methods.",
        "reference": "EvoMoE: An Evolutional Mixture-of-Experts Training Framework via Dense-To-Sparse Gate, Abstract, \\\"Specifically, \\\\ourmethods{} outperforms other MoE methods on GLUE benchmark up to 0.562 and 0.403 on average.\\\""
    },
    {
        "paper": "arXiv-2112.14397v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the reported perplexity (PPL) results achieved by EvoMoE in the language modeling task compared to other methods?",
        "answer": "In the language modeling task, EvoMoE achieves a perplexity of 12.24, which is better than other methods such as Switch and StableMoE. This is an improvement of 0.88 PPL compared to other MoE methods on average.",
        "reference": "EvoMoE: An Evolutional Mixture-of-Experts Training Framework via Dense-To-Sparse Gate, Section 6.2, \\\"on LM task, \\\\ourmethods{} outperforms other MoE methods up to 0.88 ppl and 0.545 ppl on average;\\\""
    },
    {
        "paper": "arXiv-2404.05567v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the speed improvement of the DS-MoE-6B model over the Mistral-7B model in the inference task when measured on A100-80GB and H100-80GB GPUs?",
        "answer": "The DS-MoE-6B model shows a speed improvement of 1.86x on A100-80GB GPU and 1.64x on H100-80GB GPU over the Mistral-7B model.",
        "reference": "Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models - Section: GPU Inference Analysis, Table 10: \\\"For comparison, we select the Mistral-7B... our DS-MoE-6B model demonstrates a speed increase of $1.86\\\\times$ and $1.64\\\\times$ over the Mistral-7B on A100-80GB GPU and H100-80GB GPU, respectively.\\\""
    },
    {
        "paper": "arXiv-2404.05567v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the DS-MoE-6B model's active parameters compare to the total parameters and active parameters of the DeepSeekMoE-16B in terms of inference tasks?",
        "answer": "The DS-MoE-6B model has 2.2B active parameters compared to DeepSeekMoE-16B having 2.8B active parameters. However, DS-MoE-6B has a total of 6.5B parameters while DeepSeekMoE-16B has 17.3B total parameters.",
        "reference": "Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models - Section: GPU Inference Analysis, Table 11: \\\"DeepSeekMoE & 17.3B & 2.8B & 30.5 GiB ... DS-MoE-6B & 6.5B & 2.2B & 12.6 GiB...\\\""
    },
    {
        "paper": "arXiv-2404.13628v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What performance improvement does the Mixture of LoRA Experts (MoLE) method achieve over the SVDiff method in text-alignment scores for V&L domains?",
        "answer": "The Mixture of LoRA Experts (MoLE) method achieves a 0.031 average improvement in text-alignment scores compared to the SVDiff method in V&L domains.",
        "reference": "Mixture of LoRA Experts, Section 4.1, \\\"our method significantly outperforms other comparative methods in terms of Text-alignment score, with a 0.031 average improvement compared to SVDiff.\\\""
    },
    {
        "paper": "arXiv-2404.13628v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the Mixture of LoRA Experts (MoLE) method perform compared to LoRAHub on the BBH dataset in terms of average performance?",
        "answer": "The Mixture of LoRA Experts (MoLE) method achieves an average performance improvement of 3.8 over LoRAHub on the BBH dataset.",
        "reference": "Mixture of LoRA Experts, Section 4.2, \\\"on the BBH dataset, our \\\\\\\\our{} achieves an average performance improvement of 3.8 over LoRAHub.\\\""
    }
]