[
    {
        "paper": "arXiv-2002.01359v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the composition of the Schema-Guided Dialogue (SGD) dataset as described in the paper \\\"Schema-Guided Dialogue State Tracking Task at DSTC8\\\"?",
        "answer": "The Schema-Guided Dialogue (SGD) dataset includes conversations across 16 domains, with over 16000 dialogues in the training set spanning 26 APIs. The evaluation sets contain unseen services and domains to test generalization in zero-shot settings. The dataset supports task-oriented dialogues with multiple services per domain and provides natural language descriptions of service intents and slots for schema-guided predictions.",
        "reference": "\\\"Schema-Guided Dialogue State Tracking Task at DSTC8\\\" - Section: Dataset. \\\"With over 16000 dialogues in the training set spanning 26 APIs over 16 domains...\\\". Table 1 and Table 2 provide additional numerical details."
    },
    {
        "paper": "arXiv-2002.01359v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the key results from the submissions to the Schema-Guided Dialogue State Tracking Task at DSTC8 as presented in the paper?",
        "answer": "The winning team achieved a joint goal accuracy of 86.53%, which is about 9% higher than the second-placed team. Most models performed better on seen services compared to unseen services, and performance on categorical slots was generally better for seen services. The test set showed a large variance in accuracy, with models performing about 15 percentage points lower than on the dev set, likely due to the higher proportion of turns with unseen services.",
        "reference": "\\\"Schema-Guided Dialogue State Tracking Task at DSTC8\\\" - Section: Results. \\\"Team 9 achieved a very high joint goal accuracy of 86.53%...\\\" and \\\"The joint goal accuracy of most of the models was worse by 15 percentage points on an average on the test set as compared to the dev set.\\\" Table 3 provides detailed results for each team."
    },
    {
        "paper": "arXiv-2002.11893v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the proportions of different goal types in the training set of the \\\\datasetName dataset?",
        "answer": "The training set of the \\\\datasetName dataset contains the following proportions of different goal types: Single-domain (S) is 417 dialogues, Independent multi-domain (M) is 1573 dialogues, Independent multi-domain + traffic (M+T) is 691 dialogues, Cross multi-domain (CM) is 1759 dialogues, and Cross multi-domain + traffic (CM+T) is 572 dialogues. This shows that about 60% of dialogues have cross-domain user goals, i.e., \\\\textbf{M+T}, \\\\textbf{CM}, and \\\\textbf{CM+T}.",
        "reference": "\\\\datasetName: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Dataset, Section: Statistics - \\\"According to the type of user goal, we group the dialogues in the training set into five categories...\\\""
    },
    {
        "paper": "arXiv-2002.11893v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the average number of sub-goals, semantic tuples, and turns per dialogue in the \\\\datasetName dataset training set for different goal types?",
        "answer": "For the \\\\datasetName dataset training set, the average numbers are as follows: Single-domain (S) has 1.00 sub-goal, 4.5 semantic tuples, and 6.8 turns per dialogue. Independent multi-domain (M) has 2.49 sub-goals, 11.3 semantic tuples, and 13.7 turns per dialogue. Independent multi-domain + traffic (M+T) has 3.62 sub-goals, 15.8 semantic tuples, and 16.0 turns per dialogue. Cross multi-domain (CM) has 3.87 sub-goals, 18.2 semantic tuples, and 21.0 turns per dialogue. Cross multi-domain + traffic (CM+T) has 4.57 sub-goals, 20.7 semantic tuples, and 21.6 turns per dialogue.",
        "reference": "\\\\datasetName: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Dataset, Section: Statistics - Table 5: \\\"Statistics for dialogues of different goal types in the training set...\\\""
    },
    {
        "paper": "arXiv-2007.12720v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What percentage of the utterances in MultiWOZ 2.2 were corrected for dialogue state annotation errors on top of MultiWOZ 2.1?",
        "answer": "17.3% of the utterances in MultiWOZ 2.2 were corrected for dialogue state annotation errors on top of MultiWOZ 2.1.",
        "reference": "MultiWOZ 2.2 : A Dialogue Dataset with Additional Annotation Corrections and State Tracking Baselines, Abstract: \\\"Firstly, we identify and fix dialogue state annotation errors across 17.3% of the utterances on top of MultiWOZ 2.1.\\\""
    },
    {
        "paper": "arXiv-2007.12720v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "In the paper, what is the joint goal accuracy of the DS-DST model on the MultiWOZ 2.2 dataset?",
        "answer": "The joint goal accuracy of the DS-DST model on the MultiWOZ 2.2 dataset is 0.517.",
        "reference": "MultiWOZ 2.2 : A Dialogue Dataset with Additional Annotation Corrections and State Tracking Baselines, Section 6 (Dialogue State Tracking Benchmarks): \\\"DS-DST & 0.522 & 0.512& 0.517\\\\\\\""
    }
]