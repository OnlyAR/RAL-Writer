[
    {
        "paper": "arXiv-2210.17323v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the quantization performance of GPTQ on OPT-175B and BLOOM-176B models in terms of perplexity?",
        "answer": "For GPTQ tested on OPT-175B and BLOOM-176B, at 4 bits, the perplexity increased only slightly compared to FP16, by less than 0.2 points. Specifically, for OPT-175B, the perplexity on Wiki2 increased from 8.34 to 8.37, and for BLOOM-176B, it increased from 8.11 to 8.21. At 3-bit, while RTN collapses entirely, GPTQ still maintains good performance with the perplexity for OPT-175B increasing to 8.68 and for BLOOM-176B to 8.64.",
        "reference": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers, Section: 175-Billion Parameter Models, \\\"At 4 bits, GPTQ models reach only < 0.2 lower perplexity than the full-precision versions ... At 3-bit, RTN collapses, while GPTQ is still able to maintain good performance on most tasks, losing only 0.3 - 0.5 points for more than 5× compression.\\\""
    },
    {
        "paper": "arXiv-2210.17323v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How long does it take for GPTQ to perform quantization on a GPT model with 175 billion parameters, compared to other state-of-the-art post-training quantization methods?",
        "answer": "GPTQ can quantize a GPT model with 175 billion parameters in approximately 4 GPU hours on a single NVIDIA A100 GPU. In contrast, other state-of-the-art post-training quantization methods would take significantly longer, with STE methods estimated to take about 3 weeks and adaptive rounding methods taking approximately half a year. This estimate is based on the fact that STE methods take about 1 hour per 350 million parameters, and more accurate methods typically perform 10x more SGD steps.",
        "reference": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers, Section: Runtime, \\\"While GPTQ can quantize 175 billion parameter models in ≈ 4 hours, this would take current STE methods about 3 weeks, and current adaptive rounding methods would need half a year.\\\""
    },
    {
        "paper": "arXiv-2306.00978v5.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the average speedup achieved by AWQ over the FP16 implementation across various LLMs on desktop, laptop, and mobile GPUs, according to the paper \\\"AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration\\\"?",
        "answer": "According to the paper, AWQ achieves an average speedup of 3.2-3.3x over the FP16 implementation by Huggingface across various LLMs on desktop, laptop, and mobile GPUs.",
        "reference": "Paper Title: \\\"AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration\\\"; Section: Abstract - \\\"On desktop, laptop and mobile GPUs, we consistently observe a 3.2-3.3× average speedup compared to the FP16 implementation by Huggingface across a diverse spectrum of LLMs.\\\""
    },
    {
        "paper": "arXiv-2306.00978v5.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does AWQ improve the perplexity of the OPT-6.7B model compared to round-to-nearest (RTN) under INT3-g128 quantization, as stated in the paper \\\"AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration\\\"?",
        "answer": "AWQ improves the perplexity of the OPT-6.7B model from 23.54 (RTN) to 11.39 (AWQ) under INT3-g128 quantization.",
        "reference": "Paper Title: \\\"AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration\\\"; Section: Table 3, OPT-6.7B perplexity results - \\\"RTN: 23.54\\\", \\\"AWQ: 11.39\\\"."
    },
    {
        "paper": "arXiv-2406.09904v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the speed increase achieved by the QQQ method's specialized GEMM kernels for per-channel and per-group W4A8 quantization compared to FP16 GEMM?",
        "answer": "The specialized GEMM kernels for per-channel and per-group W4A8 quantization achieve speed increases of 3.67× and 3.29× compared to FP16 GEMM, respectively.",
        "reference": "\\\"QQQ: Quality Quattuor-Bit Quantization for Large Language Models\\\", Section Abstract: \\\"Our specialized per-channel W4A8 GEMM and per-group W4A8 GEMM achieve impressive speed increases of 3.67× and 3.29 × over FP16 GEMM.\\\""
    },
    {
        "paper": "arXiv-2406.09904v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the effect of the QQQ method on WikiText2 perplexity compared to other quantization methods?",
        "answer": "The QQQ method shows comparable performance in terms of perplexity to existing state-of-the-art LLM quantization methods, as evidenced by only slight increases in perplexity on LLaMA-2 models compared to W8A8 SmoothQuant, W4A16 GPTQ, and W4A16 AWQ. For example, per-group QQQ increases perplexity by up to 0.13 on LLaMA-2-13B.",
        "reference": "\\\"QQQ: Quality Quattuor-Bit Quantization for Large Language Models\\\", Section 5.2 Model Performance: \\\"For example, per-group QQQ only increases perplexity by up to 0.13 on LLaMA-2-13B compared [to SmoothQuant, GPTQ, and AWQ].\\\""
    }
]