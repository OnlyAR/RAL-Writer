[
    {
        "paper": "arXiv-2305.11000v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the size of the discrete units vocabulary used in the SpeechGPT model?",
        "answer": "The size of the discrete units vocabulary used in the SpeechGPT model is K, which is the total number of clusters in HuBERT's k-means clustering. This is part of the SpeechGPT model's architecture and training process as described in the paper.",
        "reference": "\\\"\\\\title{\\\\Large SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities}\\\", Section \\\"Model Structure\\\": \\\"HuBERT is a self-supervised model that ... resulting in a discrete units sequence represented as $U=(u_1, u_2, \\\\ldots, u_T)$, $u_i \\\\in {0,1, \\\\ldots, K-1}$.\\\""
    },
    {
        "paper": "arXiv-2305.11000v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How many units comprise the SpeechInstruct dataset, and what datasets were used to construct it?",
        "answer": "The SpeechInstruct dataset comprises 9 million unit-text data pairs. It was constructed using the English ASR datasets Gigaspeech, Common Voice, and LibriSpeech.",
        "reference": "\\\"\\\\title{SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities}\\\", Section \\\"Cross-modal Instruction\\\", \\\"Ultimately, we obtain 9 million unit-text data pairs.\\\" Additionally, \\\"We collect several large-scale English ASR datasets to construct Cross-Modal Instruction, including Gigaspeech, Common Voice, and LibriSpeech.\\\""
    },
    {
        "paper": "arXiv-2402.05755v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the composition of the training data used for SPOT models, including the number of hours of speech data, the number of tokens, and the sampling proportions?",
        "answer": "The training data for SPOT models consist of three components: - Speech-only data with 458,000 hours and 28.2 billion tokens, with a sampling proportion of 33.3%. - Speech + Text data with 111,000 hours, 7.0 billion speech tokens and 1.4 billion text tokens, also with a sampling proportion of 33.3%. - Text-only data containing 307 billion text tokens, sampled at 33.3%. The training spans 100K steps or 100B tokens, with the speech-only data seen for 1.24 epochs and the Speech + Text data for 3.81 epochs.",
        "reference": "Section: \\\\n\\\"Our \\\\spot models are trained on a combination of speech, text and aligned speech+text sequences. We report in Table \\\\ref{tab:data} the amount\\\\/and sampling proportion of each type of data.\\\" \\\\n\\\"\\\\begin{table}[t] \\\\centering  ...\\\""
    },
    {
        "paper": "arXiv-2402.05755v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do the zero-shot comprehension task accuracies of SPOTBase compare to those of previous models like TWIST and AudioLM in speech-only WUGGY tasks?",
        "answer": "In the speech-only WUGGY task, SPOTBase achieves an accuracy of 69.0%, while TWIST achieves 74.5% and AudioLM achieves 71.5%.",
        "reference": "Table: \\\\n\\\"\\\\begin{table*}[t] \\\\centering \\\\footnotesize\\\\begin{tabular}{lccccccc}\\\\n\\\\toprule\\\\n...\\\" \\\\n\\\"\\\\multicolumn{1}{: Previous Work} \\\" \\\\n\\\"GSLM \\\\citep {gslm} \\\" \\\\n**.  .. \\\" \\\\nTWIST \\\\citep { hassid2023 textually} : empty set & \\\\bf 74.5 \\\" \\\\n\\\\ \\\\n\\\\"
    },
    {
        "paper": "arXiv-2406.12428v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the Character Error Rate (CER) for the PSLM-ASR method compared to other CoM-based baselines without gold text questions as input?",
        "answer": "The Character Error Rate (CER) for the PSLM-ASR method is 6.05, which is lower than CoM-SQ's CER of 7.83 and comparable to CoM-ASR's CER of 6.18.",
        "reference": "\\\"PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems\\\" - Section: Table 1 (Automatic Evaluation Results): \\\"CER\\\""
    },
    {
        "paper": "arXiv-2406.12428v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How did the PSLM-2x method perform in terms of latency and response quality compared to the original PSLM method?",
        "answer": "The PSLM-2x method had a latency of 0.20 seconds, which is lower than the original PSLMâ€™s latency of 0.34 seconds. However, the PSLM-2x method maintained similar response quality as PSLM with a T-score of 3.50 and a S-score of 3.20, showing no significant degradation in quality.",
        "reference": "\\\"PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems\\\" - Section: Table 1 (Automatic Evaluation Results), \\\"PSLM-2x\\\" and \\\"PSLM\\\"."
    }
]