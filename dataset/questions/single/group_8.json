[
    {
        "paper": "arXiv-1804.07461v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the number of training examples and test examples for the QQP task in the GLUE benchmark?",
        "answer": "The QQP task in the GLUE benchmark has 364k training examples and 391k test examples.",
        "reference": "Title: GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding; Section: Tasks; \\\"QQP & 364k & 391k & paraphrase & acc./F1 & social QA questions\\\""
    },
    {
        "paper": "arXiv-1804.07461v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "According to the results presented in the GLUE benchmark paper, which model achieved the highest average score for the GLUE diagnostic set analysis across all coarse-grained categories and what was the score?",
        "answer": "The single-task model with attention and ELMo achieved the highest average score for the GLUE diagnostic set analysis across all coarse-grained categories, with a score of 28.",
        "reference": "Title: GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding; Section: Analysis; Table :\\\"Results on the diagnostic set. We report R3 coefficients...\\\""
    },
    {
        "paper": "arXiv-1810.12885v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the reported human performance on the ReCoRD dataset in terms of Exact Match (EM) and F1 scores?",
        "answer": "The reported human performance on the ReCoRD dataset is 91.31 for Exact Match (EM) and 91.69 for F1 score.",
        "reference": "ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension, Section Evaluation, \\\\n\\\"We show the evaluation results in \\\\Cref{tab:performance}. Humans are able to get 91.31 EM and 91.69 F1 on the set.\\\""
    },
    {
        "paper": "arXiv-1810.12885v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How many distinct passages does the ReCoRD dataset consist of in total?",
        "answer": "The ReCoRD dataset consists of 80,121 unique passages in total.",
        "reference": "ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension, Section Data Collection, Table 1: Statistics of ReCoRD\\\\n\\\"unique passages  & 65,709 & 7,133 & 7,279 & 80,121\\\""
    },
    {
        "paper": "arXiv-1905.00537v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the components and data statistics for the SuperGLUE benchmark introduced in the paper \\\"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems\\\"?",
        "answer": "The SuperGLUE benchmark consists of eight language understanding tasks, with specific data statistics as follows:\\\\n\\\\n- BoolQ: 9427 train, 3270 dev, 3245 test examples\\\\n- CB: 250 train, 57 dev, 250 test examples\\\\n- COPA: 400 train, 100 dev, 500 test examples\\\\n- MultiRC: 5100 train, 953 dev, 1800 test examples\\\\n- ReCoRD: 101,000 train, 10,000 dev, 10,000 test examples\\\\n- RTE: 2500 train, 278 dev, 300 test examples\\\\n- WiC: 6000 train, 638 dev, 1400 test examples\\\\n- WSC: 554 train, 104 dev, 146 test examples",
        "reference": "\\\"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems\\\", Table 1: The tasks included in SuperGLUE."
    },
    {
        "paper": "arXiv-1905.00537v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "In the paper \\\"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems\\\", how does the BERT baseline perform on the SuperGLUE test sets in terms of average score and individual task performances?",
        "answer": "The BERT baseline achieved an average SuperGLUE score of 69.0. Its individual task performances on various metrics were as follows:\\\\n- BoolQ: 77.4 (accuracy)\\\\n- CB: 75.7 (F1), 83.6 (accuracy)\\\\n- COPA: 70.6 (accuracy)\\\\n- MultiRC: 24.0 (F1$_a$), 72.0 (EM)\\\\n- ReCoRD: 71.3 (F1), 71.3 (EM)\\\\n- RTE: 71.6 (accuracy)\\\\n- WiC: 69.5 (accuracy)\\\\n- WSC: 64.3 (accuracy)\\\\n\\\\nFor the diagnostic tasks, BERT scored:\\\\n- AX$_b$: 23.0 (MCC)\\\\n- AX$_g$: 51.7 (accuracy), 97.8 (GPS)",
        "reference": "\\\"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems\\\", Table 7: Baseline performance on the SuperGLUE test sets and diagnostics."
    }
]