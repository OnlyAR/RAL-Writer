[
    {
        "paper": "arXiv-1903.10318v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the ROUGE-1, ROUGE-2, and ROUGE-L scores for the \\\\textsc{Bertsum}+Transformer model on the CNN/DailyMail dataset?",
        "answer": "The \\\\textsc{Bertsum}+Transformer model achieved ROUGE-1, ROUGE-2, and ROUGE-L scores of 43.25, 20.24, and 39.63 respectively on the CNN/DailyMail dataset.",
        "reference": "\\\"Fine-tune BERT for Extractive Summarization,\\\" Experimental Results Section: \\\"\\\\textsc{Bertsum}+Transformer  & \\\\textbf{43.25}& \\\\textbf{20.24}& \\\\textbf{39.63}\\\\\\\\\\\""
    },
    {
        "paper": "arXiv-1903.10318v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How did the \\\\textsc{Bertsum}+Classifier model perform on the NYT50 dataset compared to other models?",
        "answer": "The \\\\textsc{Bertsum}+Classifier model achieved ROUGE-1, ROUGE-2, and ROUGE-L scores of 46.66, 26.35, and 42.62 on the NYT50 dataset, outperforming all other models listed, including Full and Deep Reinforced models.",
        "reference": "\\\"Fine-tune BERT for Extractive Summarization,\\\" Experimental Results Section: \\\"\\\\textsc{Bertsum}+Classifier          & \\\\textbf{46.66}    &  \\\\textbf{26.35 } & \\\\textbf{42.62} \\\" and \\\"Full$^*$            & 42.2  & 24.9  & -\\\\\\\""
    },
    {
        "paper": "arXiv-1909.08089v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the average document length for the arXiv dataset, as described in the paper \\\"Extractive Summarization of Long Documents by Combining Global and Local Context\\\"?",
        "answer": "The average document length for the arXiv dataset is 4938 words.",
        "reference": "\\\"Extractive Summarization of Long Documents by Combining Global and Local Context\\\", Section Datasets for long documents: \\\"arXiv & 215K & 4938 & 220\\\"."
    },
    {
        "paper": "arXiv-1909.08089v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "In the paper \\\"Extractive Summarization of Long Documents by Combining Global and Local Context\\\", how does the proposed model's ROUGE-1 score compare to the Lead baseline on the arXiv dataset?",
        "answer": "The proposed model (Ours-concat) achieves a ROUGE-1 score of 43.62, compared to the Lead baseline which has a ROUGE-1 score of 33.66 on the arXiv dataset.",
        "reference": "\\\"Extractive Summarization of Long Documents by Combining Global and Local Context\\\", Section Results and Analysis, Table 1: \\\"Ours-concat & 43.62 &... Lead & 33.66 &...\\\"."
    },
    {
        "paper": "arXiv-2004.08795v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the proposed MatchSum model perform on the CNN/DailyMail dataset in terms of ROUGE scores compared to state-of-the-art baselines?",
        "answer": "The proposed MatchSum model achieves a ROUGE-1 score of 44.41, a ROUGE-2 score of 20.86, and a ROUGE-L score of 40.55 on the CNN/DailyMail dataset. These scores surpass the previous state-of-the-art BERT-based models listed in Table 1 of the paper, such as BertExt + Tri-Blocking with ROUGE-1 score of 43.18, ROUGE-2 score of 20.16, and ROUGE-L score of 39.56.",
        "reference": "Extractive Summarization as Text Matching, Results on CNN/DM test set, Table 1."
    },
    {
        "paper": "arXiv-2004.08795v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the inherent gap between sentence-level and summary-level extractors on the PubMed dataset according to the paper?",
        "answer": "The inherent gap between sentence-level and summary-level extractors on the PubMed dataset is 1.1 according to Figure 7 in the paper.",
        "reference": "Extractive Summarization as Text Matching, Inherent Gap between Sentence-Level and Summary-Level Extractors, Figure 7."
    }
]