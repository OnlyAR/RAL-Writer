[
    {
        "paper": "arXiv-2311.17043v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the token strategy proposed by LLaMA-VID and how does it impact the computational burden?",
        "answer": "LLaMA-VID proposes a dual-token strategy by representing each frame with two distinct tokens: context token and content token. This method significantly reduces the computational burden by limiting the number of tokens per video frame to just two, as opposed to over 256 tokens for a single image required by models like LLaVA. This simplification allows LLaMA-VID to efficiently support hour-long videos by reducing the overload without compromising critical information.",
        "reference": "LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models, Abstract: \\\"...by representing each frame with two distinct tokens, namely context token and content token...This dual-token strategy significantly reduces the overload of long videos while preserving critical information.\\\""
    },
    {
        "paper": "arXiv-2311.17043v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does LLaMA-VID performance compare on video QA benchmarks according to the results in the paper?",
        "answer": "LLaMA-VID demonstrates superior performance on video QA benchmarks compared to previous methods. It achieves an accuracy of 69.7% on MSVD-QA, 57.7% on MSRVTT-QA, and 47.4% on ActivityNet-QA with 7B LLMs, outperforming other leading methods in terms of accuracy and scoring metrics across these datasets.",
        "reference": "LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models, Table 1: \\\"LLaMA-VID achieves an accuracy of 69.7% and surpasses previous methods like BT-Adapter by 2.2% on MSVD-QA...\\\""
    },
    {
        "paper": "arXiv-2406.16852v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the Long Video Assistant (LongVA) perform on the Video-MME benchmark compared to other models with similar parameter scales?",
        "answer": "LongVA achieves state-of-the-art performance among models under 10B parameters on the Video-MME benchmark. Specifically, LongVA reaches an overall score of 52.6 when using 128 frames, outperforming other 7B-scale models such as Video-LLaVA which scores 47.9 and VideoChat2-Mistral with a score of 39.5. The results indicate that LongVA's performance improves with denser sampling of video frames, showcasing its effective handling of more input frames than comparable models.",
        "reference": "Title: Long Context Transfer from Language to Vision - Section: Experiments, Table 4."
    },
    {
        "paper": "arXiv-2406.16852v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the maximum number of frames Long Video Assistant (LongVA) can process, and how does it affect its performance on long video benchmarks?",
        "answer": "LongVA is capable of processing up to 2000 frames or over 200K visual tokens without additional complexities. This extended capacity enables LongVA to achieve state-of-the-art performance on long video benchmarks such as Video-MME, where it significantly benefits from denser sampling of video frames, achieving an overall performance score of 52.6 with 128 frames and maintaining competitive performance even with 384 frames. The results highlight LongVA's ability to effectively handle longer video inputs than other similar models.",
        "reference": "Title: Long Context Transfer from Language to Vision - Section: Abstract and Experiments, Table 4."
    },
    {
        "paper": "arXiv-2410.17434v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the performance evaluation of the LongVU model on the VideoMME and MLVU benchmarks compared to other open-source video MLLMs?",
        "answer": "The LongVU model achieves superior performance on the VideoMME benchmark with an overall score of 60.6% and on the Long subset with 59.5%. On the MLVU benchmark, it scores 65.4%. The model outperforms existing open-source video MLLMs such as VideoChat2, LLaVA-OneVision, and others mentioned in the paper.",
        "reference": "\\\"LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding,\\\" see Table 2 under 'Experiments' section: \\\"Table~\\\\ref{tab:main} presents our experimental results on multiple video understanding benchmarks. Our results compares favorably to all the baselines across various video understanding benchmarks.\\\""
    },
    {
        "paper": "arXiv-2410.17434v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the LongVU model handle frame and token reduction for long videos to fit within an 8k context length?",
        "answer": "The LongVU model employs spatiotemporal adaptive compression, achieving an average token reduction rate of 40.4% through spatial token compression and reducing the number of frames approximately by half with temporal reduction strategies. This allows handling approximately an hour-long video within the 8k context length, maintaining essential visual content.",
        "reference": "\\\"LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding,\\\" see 'Spatiotemporal Compression Analysis' section: \\\"Figure~\\\\ref{fig:reduce} (a) presents the number of frames before and after temporal reduction based on the similarity of DINOv2 features across frames. Figure~\\\\ref{fig:reduce} (b) shows the number of tokens before and after spatial token compression.\\\""
    }
]