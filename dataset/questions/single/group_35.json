[
    {
        "paper": "arXiv-2305.14283v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the evaluation metrics used for the open-domain QA tasks in the 'Rewrite-Retrieve-Read' framework, and what specific performance improvement does the method exhibit compared to the baseline?",
        "answer": "The evaluation metrics used for the open-domain QA tasks are Exact Match (EM) and F1 scores. The 'Rewrite-Retrieve-Read' framework exhibits performance improvements across multiple datasets. For instance, on the HotpotQA dataset, the EM score improves from 32.36 for the direct approach to 34.38 for the trainable rewriter, with the F1 score improving from 43.05 to 45.97. Similar improvements are observed on the AmbigNQ and PopQA datasets as well, with EM scores increasing from 42.10 (direct) to 47.80 (trainable rewriter) and from 41.94 to 45.72 respectively.",
        "reference": "Query Rewriting for Retrieval-Augmented Large Language Models, 'Experiments' Section, Table 3."
    },
    {
        "paper": "arXiv-2305.14283v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "For the multiple-choice QA dataset (MMLU), how does the performance of ChatGPT compare to that of Vicuna-13B when using the 'Rewrite-Retrieve-Read' framework?",
        "answer": "For the MMLU dataset, ChatGPT initially exhibits higher performance in the direct prompt approach across all categories. However, after applying the 'Rewrite-Retrieve-Read' framework, both models experience performance improvement. Specifically, after using retrieval and the trainable rewriter, ChatGPT achieves an EM of 77.0 in Humanities and 72.6 in Other, while Vicuna-13B's EM increases up to 43.2 in Humanities and 59.3 in Other, showing that both LLMs benefit from the 'Rewrite-Retrieve-Read' method with notable performance enhancements over their direct prompt performances.",
        "reference": "Query Rewriting for Retrieval-Augmented Large Language Models, 'Experiments' Section, Table 4."
    },
    {
        "paper": "arXiv-2310.05029v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the maximum segment sizes used in the memory tree construction for the \\\\\\\\sysname{} method in the 'Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading' paper?",
        "answer": "For the memory tree construction in the \\\\\\\\sysname{} method, the maximum segment sizes are set to 1000 tokens for both QuALITY and SummScreenFD datasets, and 1200 tokens for the GovReport dataset.",
        "reference": "Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading, Section 'Experimental Setup': \\\\\\\"We set the maximum number of nodes \\\\max_t M_t = 8, 5, 8 and segment size \\\\lvert c \\\\rvert = 1000, 1000, 1200 for QuALITY, SummScreenFD, and GovReport respectively.\\\\\\\""
    },
    {
        "paper": "arXiv-2310.05029v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the \\\\\\\\sysname{} method's accuracy compare to the 'Full Context (keep right)' baseline on the original QuALITY dataset in 'Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading'?",
        "answer": "In the original QuALITY dataset, the \\\\\\\\sysname{} method achieved an accuracy of 67.4%, which is slightly lower than the 'Full Context (keep right)' baseline accuracy of 70.1%.",
        "reference": "Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading, Table 1: \\\\\\\"Full Context (keep right) & \\\\textbf{70.1} / 72.5 ... \\\\sysname{} & 67.4 / \\\\textbf{73.6} ...\\\\\\\""
    },
    {
        "paper": "arXiv-2403.05676v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the maximum speedup achieved by PipeRAG compared to \\\\textsc{Retro} in terms of end-to-end generation latency?",
        "answer": "PipeRAG achieves up to a 2.6× speedup in end-to-end generation latency compared to \\\\textsc{Retro}.",
        "reference": "PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design, Abstract: \\\"Our evaluation shows that, by combining the three aforementioned methods, PipeRAG achieves up to 2.6$\\\\times$ speedup in end-to-end generation latency while improving generation quality.\\\""
    },
    {
        "paper": "arXiv-2403.05676v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does PipeRAG perform on the performance-quality Pareto frontier relative to \\\\textsc{Retro} in terms of perplexity and latency?",
        "answer": "PipeRAG significantly outperforms \\\\textsc{Retro} on the performance-quality Pareto frontier. PipeRAG can achieve up to 2.6× speedup in latency without compromising perplexity. Alternatively, maintaining the same latency allows PipeRAG to reduce perplexity by as much as 0.93 compared to \\\\textsc{Retro}.",
        "reference": "PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design, Evaluation: \\\"The quality-performance Pareto frontier of PipeRAG significantly outperforms that of \\\\textsc{Retro}: PipeRAG can achieve up to 2.6× speedup in latency without compromising perplexity; alternatively, maintaining the same latency allows PipeRAG to reduce perplexity by as much as 0.93 compared to \\\\textsc{Retro}.\\\""
    }
]