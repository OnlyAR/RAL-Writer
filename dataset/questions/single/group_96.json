[
    {
        "paper": "arXiv-2212.10509v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the improvement in retrieval recall when using IRCoT with Flan-T5-XXL on the 2WikiMultihopQA dataset compared to the one-step retriever?",
        "answer": "Using IRCoT with Flan-T5-XXL improves the retrieval recall on the 2WikiMultihopQA dataset by 14.3 points over the one-step retriever.",
        "reference": "Title: Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions; Section: Results, Figure 3."
    },
    {
        "paper": "arXiv-2212.10509v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "By how many points does IRCoT improve the question answering F1 score on the HotpotQA dataset when using Flan-T5-XXL compared to the OneR QA model?",
        "answer": "IRCoT improves the question answering F1 score on the HotpotQA dataset by 9.4 points compared to the OneR QA model when using Flan-T5-XXL.",
        "reference": "Title: Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions; Section: Results, Figure 4."
    },
    {
        "paper": "arXiv-2305.06983v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the main performance metric used to evaluate the \\\\n\\\\textbf{\\\\ours} method on the 2WikiMultihopQA dataset and what result does it achieve?",
        "answer": "The main performance metric used to evaluate the \\\\textbf{\\\\ours} method on the 2WikiMultihopQA dataset is Exact Match (EM). The \\\\textbf{\\\\ours} method achieves an EM score of 51.0 on this dataset.",
        "reference": "From 'Active Retrieval Augmented Generation': \\\"\\\\textbf{\\\\n\\\\oursd} (ours) & \\\\textbf{51.0} & \\\\textbf{59.7} & \\\\textbf{59.1} & \\\\textbf{62.6} \\\\bottomrule\\\" under Table 1: \\\\textbf{2WikiMultihopQA} section."
    },
    {
        "paper": "arXiv-2305.06983v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the performance of the \\\\n\\\\textbf{\\\\ours} method compare to the Previous-window baseline method on the ASQA dataset in terms of Disambig-\\\\n\\\\textbf{\\\\fone} score?",
        "answer": "On the ASQA dataset, the \\\\textbf{\\\\ours} method achieves a Disambig-\\\\n\\\\textbf{\\\\fone} score of 28.2, while the Previous-window baseline method achieves a score of 27.0. This shows that \\\\textbf{\\\\ours} outperforms the Previous-window baseline method in terms of the Disambig-\\\\textbf{\\\\fone} score on the ASQA dataset.",
        "reference": "From 'Active Retrieval Augmented Generation': Table 3: \\\"\\\\textbf{\\\\ours} (ours) & \\\\textbf{41.3} & \\\\textbf{28.2} & \\\\textbf{34.3} & \\\\textbf{31.1} \\\\bottomrule\\\" and \\\"Previous-window & 71.2 & 39.9 & 27.0 & 34.3 & 30.4 & 43.7 & 35.7 & 37.5 & 36.6 \\\\bottomrule\\\""
    },
    {
        "paper": "arXiv-2403.10081v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "In the paper 'DRAGIN: Dynamic Retrieval Augmented Generation based on the Information Needs of Large Language Models,' how does DRAGIN perform on the 2WikiMultihopQA dataset compared to other RAG baselines using the LLaMA-2-13B-Chat model?",
        "answer": "DRAGIN achieves an Exact Match (EM) score of 0.304 and an F1 score of 0.3931 on the 2WikiMultihopQA dataset using the LLaMA-2-13B-Chat model. It outperforms the other baselines, with SR-RAG achieving an EM of 0.245 and F1 of 0.3364, FL-RAG an EM of 0.217 and F1 of 0.3054, FS-RAG an EM of 0.270 and F1 of 0.3610, and FLARE an EM of 0.224 and F1 of 0.3076.",
        "reference": "DRAGIN: Dynamic Retrieval Augmented Generation based on the Information Needs of Large Language Models; See Table 3 Overall Results of DRAGIN and Baselines"
    },
    {
        "paper": "arXiv-2403.10081v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "In the paper 'DRAGIN: Dynamic Retrieval Augmented Generation based on the Information Needs of Large Language Models,' what is the retrieval frequency (#Num) for DRAGIN on the StrategyQA dataset with the LLaMA2-7B model, and how does it compare to other methods?",
        "answer": "The retrieval frequency (#Num) for DRAGIN on the StrategyQA dataset with the LLaMA2-7B model is 4.629. This is higher than FLARE, which has a frequency of 1.271, and lower than FS-RAG, which has a frequency of 4.546, and slightly higher than FL-RAG, which has a frequency of 3.757.",
        "reference": "DRAGIN: Dynamic Retrieval Augmented Generation based on the Information Needs of Large Language Models; See Table 5 Comparison of the frequency of retrieval module activation in dynamic RAG frameworks across all datasets."
    }
]