[
    {
        "paper": "arXiv-2305.18290v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the main experimental tasks used to evaluate the Direct Preference Optimization (DPO) method?",
        "answer": "The main experimental tasks used to evaluate the Direct Preference Optimization (DPO) method are controlled sentiment generation, summarization using the Reddit TL;DR dataset, and single-turn dialogue using the Anthropic Helpful and Harmless dialogue dataset.",
        "reference": "Title: 'Direct Preference Optimization: Your Language Model is Secretly a Reward Model', Section: 'Experiments'\\\\n'Our experiments explore three different open-ended text generation tasks. For all experiments, algorithms learn a policy from a dataset of preferences \\\\(\\\\mathcal{D}=\\\\bigl\\\\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\\\\bigr\\\\}_{i=1}^N\\\\). In \\\\textbf{controlled sentiment generation}, \\\\(x\\\\) is a prefix of a movie review from the IMDb dataset... In \\\\textbf{summarization}, \\\\(x\\\\) is a forum post from Reddit; the policy must generate a summary \\\\(y\\\\)... Finally, in \\\\textbf{single-turn dialogue}, \\\\(x\\\\) is a human query...'"
    },
    {
        "paper": "arXiv-2305.18290v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does DPO's performance compare to PPO on the Reddit TL;DR summarization task in terms of win rates?",
        "answer": "DPO exceeds PPO's best-case performance on summarization. DPO achieves a win rate of approximately 61% at a temperature of 0.0, compared to PPO's win rate of around 57% at its optimal temperature of 0.0. DPO is also more robust to changes in the sampling temperature.",
        "reference": "Title: 'Direct Preference Optimization: Your Language Model is Secretly a Reward Model', Section: 'Can DPO scale to real preference datasets?' Figure: 'TL;DR summarization win rates vs. human-written summaries, using GPT-4 as evaluator.' and corresponding descriptive text.'DPO has a win rate of approximately 61\\\\% at a temperature of 0.0, exceeding the performance of PPO at ~57\\\\%... DPO also achieves a higher maximum win rate compared to the best of $N$ baseline.'\\\\n"
    },
    {
        "paper": "arXiv-2402.01306v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the performance of KTO compared to DPO on various benchmarks according to the \\\"KTO: Model Alignment as Prospect Theoretic Optimization\\\" paper?",
        "answer": "According to Table 4 in the paper, KTO aligns better than DPO on multiple benchmarks. For instance, with the Zephyr-$\\\\beta$-SFT model aligned on UltraFeedback, KTO achieves an EM of 58.6 on MMLU, 53.5 on GSM8k, 30.9 pass@1 on HumanEval, and 52.6 EM on BBH. In contrast, DPO scores 58.2 on MMLU, 40.0 on GSM8k, 30.1 pass@1 on HumanEval, and 44.1 on BBH. This indicates that KTO performs better than DPO across these tasks.",
        "reference": "KTO: Model Alignment as Prospect Theoretic Optimization, Table 4, \\\"The surprising success of offline PPO with dummy +1/-1 rewards suggests that... a different alignment method in Table 4.\\\""
    },
    {
        "paper": "arXiv-2402.01306v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the performance of HALOs compare to non-HALOs according to the \\\"KTO: Model Alignment as Prospect Theoretic Optimization\\\" paper?",
        "answer": "The paper shows that HALOs, particularly DPO and the offline PPO variant, outperform non-HALOs like SLiC and CSFT at a scale of 13B parameters and above. For example, in Figure 3, at the scale of 30B parameters, the HALO-aligned Llama models outperform the SFT targets, achieving a winrate at or above chance, while non-HALOs do not reach this level of performance.",
        "reference": "KTO: Model Alignment as Prospect Theoretic Optimization, Figure 3, \\\"Experimental Setup\\\" section: In Figure 3, HALOs (DPO, offline PPO variant) outperform non-HALOs (SLiC, CSFT)...\\\""
    },
    {
        "paper": "arXiv-2403.07691v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the AlpacaEval_2.0 scores for Phi-2 and Llama-2 models fine-tuned with ORPO as reported in the paper 'ORPO: Monolithic Preference Optimization without Reference Model'?",
        "answer": "The paper reports that Phi-2 fine-tuned with ORPO scored 6.35% on AlpacaEval_2.0, while Llama-2 fine-tuned with ORPO scored 9.44%.",
        "reference": "Title: ORPO: Monolithic Preference Optimization without Reference Model; Section: Results and Analysis, Table 1."
    },
    {
        "paper": "arXiv-2403.07691v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do the MT-Bench scores for Mistral-ORPO-α and Mistral-ORPO-β compare in the paper 'ORPO: Monolithic Preference Optimization without Reference Model'?",
        "answer": "According to the paper, Mistral-ORPO-α scored 7.23 and Mistral-ORPO-β scored 7.32 in MT-Bench.",
        "reference": "Title: ORPO: Monolithic Preference Optimization without Reference Model; Section: Multi-turn Instruction Following, Figure 3."
    }
]