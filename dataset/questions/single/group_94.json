[
    {
        "paper": "arXiv-2201.05966v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the performance of T5-3B on the semantic parsing task 'Spider' according to the USKG framework?",
        "answer": "The performance of T5-3B on the semantic parsing task 'Spider' according to the USKG framework is 71.76% Match.",
        "reference": "\\\"Structured knowledge grounding with USKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models\\\", Section: Results on Individual Tasks; Table: Test or development (dev.) set performance of models trained on individual tasks."
    },
    {
        "paper": "arXiv-2201.05966v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "According to the paper \\\"USKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models\\\", what is the effect of model size on the performance of semantic parsing tasks when using T5 models?",
        "answer": "The performance of semantic parsing tasks generally improves with an increase in the T5 model size. Specifically, for tasks like Spider, the performance observed was 58.12% for T5-base, 66.63% for T5-large, and 71.76% for T5-3B.",
        "reference": "\\\"Structured knowledge grounding with USKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models\\\", Section: Results on Individual Tasks; Table: Test or development (dev.) set performance of models trained on individual tasks."
    },
    {
        "paper": "arXiv-2301.13808v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the performance of the Dater method on the TabFact dataset compared to human performance?",
        "answer": "The Dater method achieves an accuracy of 93.0% on the TabFact dataset, outperforming human performance, which is recorded as 92.1%.",
        "reference": "PAPER: Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning, Table 1: \\\"our method outperforms human performance for the first time on the TabFact dataset.\\\""
    },
    {
        "paper": "arXiv-2301.13808v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the increase in accuracy of Dater compared to Codex and Binder on the WikiTableQuestion dataset?",
        "answer": "Dater improves the accuracy by 18.3% on the test set compared to Codex (65.9% for Dater versus 47.6% for Codex) and by 4.0% compared to Binder (65.9% for Dater versus 61.9% for Binder).",
        "reference": "PAPER: Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning, Table 2: \\\"Codex  & 49.3 & 47.6\\\" and \\\"Binder & 62.6 & 61.9\\\"; \\\"w/ \\\\textbf{\\\\textsc{Dater}} & \\\\textbf{64.8}($\\\\uparrow$ 15.5)\\\\ & \\\\textbf{65.9} ($\\\\uparrow$ 18.3)\\\""
    },
    {
        "paper": "arXiv-2402.01155v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the performance of the \\\\approachname\\\\ (method) compared to other methods on the WikiTQ dataset?",
        "answer": "In the WikiTQ dataset, the \\\\approachname\\\\ method achieves an accuracy of 69.1%. This method significantly outperforms other methods like OmniTab, DATER, and fine-tuned Flan T5-xl with respective improvements of 6.4%, 3.2%, and 4.7% in absolute terms.",
        "reference": "PAPER: CABINET: Content Relevance based Noise Reduction for Table Question Answering - Section: Performance of \\\\approachname\\\\ on Table QA. \\\"Table~\\\\ref{tab:da_wikitq} presents the performance of various methods on the WikiTQ dataset, and we can observe \\\\approachname\\\\ with an accuracy of \\\\textbf{69.1\\\\%} outperforms the best-performing baselines in each of the three categories and establishes new state-of-the-art. Specifically, \\\\approachname\\\\ outperforms OmniTab, DATER, and fine-tuned Flan T5-xl by \\\\textbf{6.4\\\\%}, \\\\textbf{3.2\\\\%} and \\\\textbf{4.7\\\\%}, in absolute terms, respectively.\\\""
    },
    {
        "paper": "arXiv-2402.01155v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the \\\\approachname\\\\ method perform under noise perturbations compared to baselines on the WikiTQ and FeTaQA datasets?",
        "answer": "Under noise perturbations, the \\\\approachname\\\\ method exhibits significantly lesser performance drop compared to the dataset-specific baselines like OmniTab on the WikiTQ and FeTaQA datasets. For example, with Row Addition perturbation in the WikiTQ dataset, the \\\\approachname\\\\ method shows a relative performance drop of approximately 11.5%, while the OmniTab baseline shows a larger drop of 19%. Similar improvements are observed in FeTaQA and WikiSQL.",
        "reference": "PAPER: CABINET: Content Relevance based Noise Reduction for Table Question Answering - Section: How Robust is \\\\approachname\\\\ to Noise and Irrelevant Information? \\\"Specifically, \\\\approachname\\\\ is significantly less sensitive to row and column permutations (RP and CP), indicating that relevance scoring of tokens helps the QA LLM to focus more on relevant information and reduces the potential ordering biases commonly observed in models pre-trained on tabular data. For the cell replacement (CR) and row addition (RA) perturbations, where extraneous information is explicitly added to the table, the drop in performance suffered by \\\\approachname\\\\ is significantly less compared to the baselines owing to the superior ability of \\\\approachname\\\\ to identify relevant information. For instance, in the case of WikiTQ, the relative drop in performance for RA is \\\\approx19\\\\% for OmniTab, almost \\\\40\\\\% higher than \\\\approachname\\\\ (\\\\approx11.5\\\\%).\\\""
    }
]