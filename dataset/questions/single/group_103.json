[
    {
        "paper": "arXiv-1911.10470v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the absolute improvement in F1 and EM scores achieved by the method proposed in the paper on the HotpotQA full wiki development set compared to the previous state-of-the-art?",
        "answer": "The method proposed in the paper achieves an absolute improvement of 14.5 points in F1 and 14.0 points in EM on the HotpotQA full wiki development set compared to the previous state-of-the-art, as indicated by the performance metrics of 73.3 F1 and 60.5 EM achieved, compared to the state-of-the-art Semantic Retrieval results of 58.8 F1 and 46.5 EM.",
        "reference": "Paper: \\\"Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering\\\", Section: \\\"Overall Results\\\", Table 1."
    },
    {
        "paper": "arXiv-1911.10470v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the statistics of the Wikipedia graph constructed in the paper for HotpotQA, specifically the number of nodes and edges?",
        "answer": "For the HotpotQA dataset, the Wikipedia graph constructed consists of about 5.2 million nodes and 23.4 million edges. These nodes represent the introductory paragraphs of Wikipedia articles, and the edges are derived from the hyperlinks between them.",
        "reference": "Paper: \\\"Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering\\\", Section: \\\"Experimental Setup\\\"."
    },
    {
        "paper": "arXiv-2009.12756v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What performance improvement does the multi-hop dense retrieval method achieve over traditional linking-based methods on the HotpotQA dataset?",
        "answer": "The multi-hop dense retrieval method achieves a 28% relative gain in Recall@20 for the HotpotQA dataset compared to traditional linking-based retrieval methods.",
        "reference": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval, Abstract: \\\"...our approach improves greatly over the traditional linking-based retrieval methods. (e.g., 28\\\\% relative gain in Recall@20 for HotpotQA).\\\""
    },
    {
        "paper": "arXiv-2009.12756v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the multi-hop dense retrieval method perform in terms of retrieval recall compared to other efficient methods on the HotpotQA dataset?",
        "answer": "The multi-hop dense retrieval method outperforms other methods with a recall of 65.9% at 2 passages, 77.5% at 10 passages, and 80.2% at 20 passages on the HotpotQA dataset.",
        "reference": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval, Table 1: Retrieval performance in recall at k retrieved passages... Method: \\\\method, R@2: 65.9, R@10: 77.5, R@20: 80.2."
    },
    {
        "paper": "arXiv-2010.12527v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the size of the new collection of questions added to the BEERQA benchmark, and how many steps of reasoning do these questions require?",
        "answer": "The new collection added to the BEERQA benchmark consists of 530 questions, and these questions require three Wikipedia pages to answer.",
        "reference": "Title: 'Answering Open-Domain Questions of Varying Reasoning Steps from Text', Section: Abstract - '...a new collection of 530 questions that require three Wikipedia pages to answer...'."
    },
    {
        "paper": "arXiv-2010.12527v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the IRRR model's performance compare on the SQuAD Open and HotpotQA datasets regarding exact match (EM) and F1 scores?",
        "answer": "The IRRR model, when trained on SQuAD and HotpotQA combined, achieves an exact match (EM) of 61.8 and an F1 of 68.9 on SQuAD Open. For HotpotQA, the IRRR achieves an EM of 65.7 and an F1 of 78.2.",
        "reference": "Title: 'Answering Open-Domain Questions of Varying Reasoning Steps from Text', Tables 3 and 4 - 'IRRR (squad+hotpotqa)' shows EM: 61.8, F1: 68.9 for SQuAD Open and EM: 65.7, F1: 78.2 for HotpotQA."
    }
]