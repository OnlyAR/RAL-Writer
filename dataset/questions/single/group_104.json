[
    {
        "paper": "arXiv-2203.07586v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the difference in ROUGE-1 scores between the proposed model with average pooling (AvgPool) and the Longformer model, as reported in the paper \\\"Long Document Summarization with Top-down and Bottom-up Inference\\\" for the PubMed dataset?",
        "answer": "The proposed model with AvgPool achieves a ROUGE-1 score of 48.34, whereas the Longformer achieves a ROUGE-1 score of 46.97. The difference in ROUGE-1 scores is 1.37.",
        "reference": "Long Document Summarization with Top-down and Bottom-up Inference, Section \\\"Scientific Documents\\\", Table 2"
    },
    {
        "paper": "arXiv-2203.07586v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "According to the paper \\\"Long Document Summarization with Top-down and Bottom-up Inference\\\", how many parameters does the proposed model use compared to the GPT-3 based model for book summarization tasks?",
        "answer": "The proposed model uses 464 million parameters, while the GPT-3 based model uses 175 billion parameters. The proposed model uses 0.27% of the parameters in the GPT-3 based model.",
        "reference": "Long Document Summarization with Top-down and Bottom-up Inference, Abstract and Section 4 \\\"Experiments\\\", Subsection \\\"Book Level\\\""
    },
    {
        "paper": "arXiv-2211.10247v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the average number of sentences per document in the PubMed dataset, as stated in the GoSum paper?",
        "answer": "The average number of sentences per document in the PubMed dataset is 89.",
        "reference": "GoSum: Extractive Summarization of Long Documents by Reinforcement Learning and Graph Organized discourse state; Section: Summarization Datasets; 'avg. # sents of doc: 89' from Table 1."
    },
    {
        "paper": "arXiv-2211.10247v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does GoSum perform in terms of ROUGE-1 score on the PubMed dataset compared to the MemSum extractive model?",
        "answer": "GoSum achieves a ROUGE-1 score of 49.83, whereas MemSum achieves a ROUGE-1 score of 49.25 on the PubMed dataset.",
        "reference": "GoSum: Extractive Summarization of Long Documents by Reinforcement Learning and Graph Organized discourse state; Section: Results; Table 2: 'Results on PubMed Dataset.'"
    },
    {
        "paper": "arXiv-2408.15801v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the ROUGE-1, ROUGE-2, and ROUGE-L scores achieved by the EYEGLAXS system using the LLAMA2-7B model trained on 12K context length for the PubMed dataset?",
        "answer": "The EYEGLAXS system with the LLAMA2-7B model trained on 12K context length achieves ROUGE-1 score of 50.34, ROUGE-2 score of 24.57, and ROUGE-L score of 45.96 on the PubMed dataset.",
        "reference": "Scaling Up Summarization: Leveraging Large Language Models for Long Text Extractive Summarization, Section 'Results and Analyses', Table 2."
    },
    {
        "paper": "arXiv-2408.15801v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the performance of the EYEGLAXS system with CHATGLM2-6B trained on 4K compare between the PubMed and ArXiv datasets in terms of ROUGE-1, ROUGE-2, and ROUGE-L scores?",
        "answer": "For the PubMed dataset, the EYEGLAXS system using CHATGLM2-6B trained on 4K achieves ROUGE-1 score of 49.96, ROUGE-2 score of 24.04, and ROUGE-L score of 45.50. For the ArXiv dataset, it achieves ROUGE-1 score of 46.87, ROUGE-2 score of 18.96, and ROUGE-L score of 41.37. This shows that the model performs better on the PubMed dataset across all three ROUGE scores.",
        "reference": "Scaling Up Summarization: Leveraging Large Language Models for Long Text Extractive Summarization, Section 'Results and Analyses', Table 2 and Table 3."
    }
]