[
    {
        "paper": "arXiv-2307.11088v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the composition of the L-Eval benchmark in terms of datasets and tasks?",
        "answer": "The L-Eval benchmark includes 20 sub-tasks, 508 long documents, and over 2,000 human-labeled query-response pairs. These tasks are divided into closed-ended tasks, primarily testing reasoning and understanding abilities, and open-ended tasks, which consist of summarization tasks requiring the aggregation of long document information. The task set encompasses diverse question styles, domains, and input lengths ranging from 3k to 200k tokens.",
        "reference": "Paper Title: L-Eval: Instituting Standardized Evaluation for Long Context Language Models, Section: Introduction, Direct quote: \\\"For dataset construction, L-Eval has 20 sub-tasks, 4 sub-tasks are annotated from scratch (\\\\n), 4 sub-tasks are re-annotated from the public datasets (\\\\n), and the remaining 12 sub-tasks are manually cleaned from previous long sequence datasets.\\\""
    },
    {
        "paper": "arXiv-2307.11088v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What were the findings regarding traditional n-gram metrics in the evaluation of long context language models in the L-Eval benchmark?",
        "answer": "The findings showed that traditional n-gram metrics, such as ROUGE and F-1 score, fail to correlate well with human judgment in the evaluation of long context language models. The paper advocates for Length-Instruction-Enhanced (LIE) evaluation and the use of LLM judges to improve correlation with human judgments.",
        "reference": "Paper Title: L-Eval: Instituting Standardized Evaluation for Long Context Language Models, Section: Abstract and Towards Standardized Long Context Evaluation Metrics, Direct quote: \\\"Results show that popular n-gram matching metrics generally can not correlate well with human judgment...\\\" and \\\"Results indicate that all these automatic metrics (except GPT-4) fail to correlate to human judgment. Compared with N-gram metrics, LLM judges are more accurate and robust to output length.\\\""
    },
    {
        "paper": "arXiv-2310.15929v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What speedup and memory saving does the E-Sparse method achieve in comparison to a dense model, as reported in the paper?",
        "answer": "E-Sparse achieves a speedup of up to 1.53 times over the dense model and significant memory savings of up to 43.52%.",
        "reference": "E-Sparse: Boosting the Large Language Model Inference through Entropy-based N:M Sparsity, Abstract: 'E-Sparse can significantly speed up the model inference over the dense model (up to 1.53$\\\\times$) and obtain significant memory saving (up to 43.52\\\\%), with acceptable accuracy loss.'"
    },
    {
        "paper": "arXiv-2310.15929v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does E-Sparse perform in terms of perplexity on the LLaMA-13B model compared to SparseGPT and Wanda under the 2:4 sparsity pattern?",
        "answer": "E-Sparse achieves a perplexity of 8.26 on the LLaMA-13B model under the 2:4 sparsity pattern, which is 1.32 points better than Wanda (9.58) and 0.85 points better than SparseGPT (9.11).",
        "reference": "E-Sparse: Boosting the Large Language Model Inference through Entropy-based N:M Sparsity, Section: Experiments, Table 1: 'As for the more constrained and challenging 2:4 sparsity, E-Sparse can obtain an 8.26 perplexity for LLaMA-13B, which is 1.32 better than Wanda and 0.85 better than SparseGPT.'"
    },
    {
        "paper": "arXiv-2412.15204v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the overall accuracy achieved by the best-performing model in the LongBench v2 evaluation, and how does it compare to human expert performance?",
        "answer": "The best-performing model, o1-preview, achieves an overall accuracy of 57.7% in the LongBench v2 evaluation. This performance surpasses the human expert performance, which is 53.7% accuracy under a 15-minute time constraint, marking a 4% improvement over human accuracy.",
        "reference": "LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks, Abstract and Section 5 \\\"Evaluation\\\", \\\"Our evaluation reveals that the best-performing model, when directly answers the questions, achieves only 50.1% accuracy. In contrast, the o1-preview model, which includes longer reasoning, achieves 57.7%, surpassing the human baseline by 4%.\\\""
    },
    {
        "paper": "arXiv-2412.15204v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How many multiple-choice questions are included in LongBench v2 and what is the range of context lengths present in this benchmark?",
        "answer": "LongBench v2 contains a total of 503 challenging multiple-choice questions. The context lengths in this benchmark range from 8k to 2M words.",
        "reference": "LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks, Abstract, \\\"LongBench v2 consists of 503 challenging multiple-choice questions, with contexts ranging from 8k to 2M words.\\\""
    }
]