[
    {
        "paper": "arXiv-2401.13923v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the improvement in molecule-text retrieval accuracy for 3D-MoLM over existing baselines on the PubChem dataset?",
        "answer": "3D-MoLM outperforms existing baselines by achieving a 72.06% accuracy rate for molecule-to-text retrieval on the PubChem test set, which surpasses the highest baseline accuracy (MoMu-K trained on enriched text) by approximately 10 percentage points as it has a 62.07% accuracy rate.",
        "reference": "Title: 3D-MoLM: Towards 3D Molecule-Text Interpretation in Language Models; Section: Molecule-Text Retrieval; Table 5 [Molecule-Text retrieval results on the PubChem Dataset]."
    },
    {
        "paper": "arXiv-2401.13923v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does 3D-MoLM perform in terms of ROUGE-L score in the molecule captioning task compared to baselines?",
        "answer": "3D-MoLM achieves a ROUGE-L score of 31.23 in the molecule captioning task, outperforming the highest baseline (MoMu-Large), which achieves a score of 24.76, leading to an improvement of 6.47 ROUGE-L.",
        "reference": "Title: 3D-MoLM: Towards 3D Molecule-Text Interpretation in Language Models; Section: Molecule Captioning; Table 6 [Molecule captioning results on PubChem Dataset]."
    },
    {
        "paper": "arXiv-2403.04197v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the BLEU-4 scores achieved by ICMA(Galactica-125M) and ICMA(Mistral-7B) on the Mol2Cap task in the ChEBI-20 dataset?",
        "answer": "ICMA(Galactica-125M) achieved a BLEU-4 score of 0.565, while ICMA(Mistral-7B) achieved a BLEU-4 score of 0.581 on the Mol2Cap task in the ChEBI-20 dataset.",
        "reference": "\\\"Large Language Models are In-Context Molecule Learners,\\\" Table 1: \\\"ICMA(Galactica-125M)$_{2,2048}$ & \\\\underline{0.636} & \\\\underline{0.565} & \\\\underline{0.674} & \\\\underline{0.536} & \\\\underline{0.615} & \\\\underline{0.648} \\\\\\\\ ICMA(Mistral-7B)$_{2,2048}$ & \\\\textbf{0.651} & \\\\textbf{0.581} & \\\\textbf{0.686} & \\\\textbf{0.550} & \\\\textbf{0.625} & \\\\textbf{0.661} \\\\\\\\"
    },
    {
        "paper": "arXiv-2403.04197v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the Exact Match (EM) score of ICMA(Mistral-7B) in the Cap2Mol task on the ChEBI-20 dataset?",
        "answer": "The Exact Match (EM) score of ICMA(Mistral-7B) in the Cap2Mol task on the ChEBI-20 dataset is 0.460.",
        "reference": "\\\"Large Language Models are In-Context Molecule Learners,\\\" Table 2: \\\"ICMA(Mistral-7B)$_{4,2048}$ & \\\\underline{0.855} & \\\\textbf{0.460} & 18.73 & \\\\textbf{0.916} & \\\\textbf{0.837} & \\\\textbf{0.789} & \\\\textbf{0.958} \\\\\\\\"
    },
    {
        "paper": "arXiv-2406.06777v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the percentage of trainable parameters introduced by MolX in the pre-training and downstream tasks compared to Llama-2-7B?",
        "answer": "MolX introduces 0.53% of trainable parameters in the pre-training stage and 0.82% in the downstream tasks compared to Llama-2-7B.",
        "reference": "MolX: Enhancing Large Language Models for Molecular Learning with A Multi-Modal Extension, Abstract: \\\"...our proposed method outperforms baselines across 4 downstream molecule-related tasks...while only introducing a small number of trainable parametersâ€”0.53\\\\% and 0.82\\\\%, respectively.\\\". Also, see Section 4, Table \\\\ref{efficiency}."
    },
    {
        "paper": "arXiv-2406.06777v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the performance of MolX-enhanced model compare with other models on the molecule-to-text translation task without fine-tuning?",
        "answer": "The MolX-enhanced model in the inference-only configuration outperformed the Llama-2-7B model significantly on both molecule description generation and IUPAC name generation tasks without fine-tuning. For example, on the description generation task, MolX achieved BLEU-4 score of 6.40 compared to 2.98 for Llama-2-7B.",
        "reference": "MolX: Enhancing Large Language Models for Molecular Learning with A Multi-Modal Extension, Section 4.2, Table \\\"Experimental results for molecule-to-text translation.\\\""
    }
]