[
    {
        "paper": "arXiv-2304.08818v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the resolution at which the Video LDM model achieved state-of-the-art performance for real driving videos?",
        "answer": "The Video LDM model achieved state-of-the-art performance for real driving videos at a resolution of $512\\\\times1024$.",
        "reference": "(\\\"Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models\\\", Abstract) - \\\"we validate our \\\\textbf{Video LDM} on real driving videos of resolution $512\\\\times1024$, achieving state-of-the-art performance.\\\""
    },
    {
        "paper": "arXiv-2304.08818v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How many videos are in the in-house dataset used for real driving scene (RDS) video generation, and what is the resolution and frame rate of these videos?",
        "answer": "The in-house dataset for real driving scene (RDS) video generation consists of 683,060 videos, each of 8 seconds duration, at a resolution of $512\\\\times1024$ and frame rate up to 30 fps.",
        "reference": "(\\\"Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models\\\", Section 'Experiments') - \\\"The dataset consists of 683,060 videos of 8 seconds each at resolution $512\\\\times1024$ ($H\\\\times W$) and frame rate up to 30 fps.\\\""
    },
    {
        "paper": "arXiv-2306.02018v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What conditions are used in the ablation study of the STC-encoder in the paper 'VideoComposer: Compositional Video Synthesis with Motion Controllability', and what is the quantitative outcome for each?",
        "answer": "The ablation study uses three sets of conditions: 'Text and sketch sequence', 'Text and depth sequence', and 'Text and motion vectors'. The frame consistency scores for these conditions when the STC-encoder is not used versus when it is used are as follows:\\\\n- Text and sketch sequence: 0.910 (without STC-encoder) vs. 0.923 (with STC-encoder)\\\\n- Text and depth sequence: 0.922 (without STC-encoder) vs. 0.928 (with STC-encoder)\\\\n- Text and motion vectors: 0.915 (without STC-encoder) vs. 0.927 (with STC-encoder).",
        "reference": "VideoComposer: Compositional Video Synthesis with Motion Controllability, Section: Ablation study, \\\"We observe that incorporating STC-encoder augments the frame consistency, which we attribute to its temporal modeling capacity. This observation holds for various temporal conditions such as sketches, depth maps and motion vectors.\\\""
    },
    {
        "paper": "arXiv-2306.02018v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "In the evaluation of motion controllability for 'VideoComposer: Compositional Video Synthesis with Motion Controllability', what were the motion control metric scores with and without the use of the STC-encoder?",
        "answer": "The motion control metric scores for 'VideoComposer' were 4.03 without using the STC-encoder, 2.67 with motion vectors introduced but without the STC-encoder, and 2.18 when both motion vectors and the STC-encoder were used.",
        "reference": "VideoComposer: Compositional Video Synthesis with Motion Controllability, Section: Experimental results of motion control, Table: Evaluating the motion controllability."
    },
    {
        "paper": "arXiv-2308.09710v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the parameter efficiency of SimDA compared to the original large T2I model and other existing T2V methods?",
        "answer": "SimDA fine-tunes only 24 million out of 1.1 billion parameters of the original T2I model, which corresponds to adding only 0.02% parameters compared to the T2I model. SimDA requires less than 8GB of GPU memory for training and has a significant reduction in inference time by about 39 times compared to CogVideo. SimDA has a total parameter count of 1.08 billion, with 0.025 billion parameters tuned, which is much lower than other methods like CogVideo with 15.5 billion parameters and Imagen Video with 16.25 billion parameters.",
        "reference": "Title: SimDA: Simple Diffusion Adapter for Efficient Video Generation, Abstract: \\\"we propose a Simple Diffusion Adapter (SimDA) that fine-tunes only 24M out of 1.1B parameters of a strong T2I model,\\\" Table 1: \\\"SimDA (Ours) [...] Overall: 1.08B, Tuned: 0.025B.\\\""
    },
    {
        "paper": "arXiv-2308.09710v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does SimDA perform on the MSR-VTT dataset compared to other T2V methods?",
        "answer": "On the MSR-VTT dataset, SimDA achieves a CLIP score of 0.2945, which is higher than most other methods except for Make-A-Video (0.3049) and VideoFactory (0.3005), which are trained on larger datasets. SimDA also achieves the lowest FVD score of 456 among the reported methods.",
        "reference": "Title: SimDA: Simple Diffusion Adapter for Efficient Video Generation, Section: Evaluation on MSR-VTT: \\\"our method achieves an average CLIPSIM of 0.2945\\\", Table 2: \\\"SimDA (Ours) [...] CLIPSIM: 0.2945.\\\""
    }
]