[
    {
        "paper": "arXiv-2406.02542v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What speedup does the Loki method achieve compared to the standard HuggingFace Transformer's attention implementation for Llama2-13B?",
        "answer": "The Loki method achieves a speedup of up to 45% over the standard HuggingFace Transformer's attention implementation for Llama2-13B.",
        "reference": "Title: Loki: Low-rank Keys for Efficient Sparse Attention. Reference: Introduction - \\\"Thus, we implement optimized sparse matrix multiplication kernels for \\\\method~in Triton, leading to a speedup of up to 45\\\\% over the standard HuggingFace Transformer's~\\\\citep{wolf-etal-2020-transformers} attention implementation (\\\\emph{vanilla} attention) for Llama2-13B.\\\""
    },
    {
        "paper": "arXiv-2406.02542v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the average degradation in model accuracy for the Loki method across different benchmarks and models, and how does this compare to the baseline?",
        "answer": "For the Loki method, the average degradation in model accuracy, measured across 6 different benchmarks and 8 different models, is only 6.8%.",
        "reference": "Title: Loki: Low-rank Keys for Efficient Sparse Attention. Reference: Introduction - \\\"For this setting, the average degradation in model accuracy (measured across 6 different benchmarks and 8 different models) is only 6.8\\\\%.\\\""
    },
    {
        "paper": "arXiv-2406.10774v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the performance improvement of the Quest method in terms of self-attention speedup for the Llama-2-7B model on an RTX4090 with different token budgets?",
        "answer": "Quest method achieves up to 8.12 times self-attention speedup compared to FlashInfer, specifically at a 32K sequence length with a 2048 token budget.",
        "reference": "Title: Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference, Section: Conclusion, Quote: \\\"\\\\nWe present \\\\method, an efficient...\\\\method provides up to \\\\selfspeedup self-attention speedup.\\\""
    },
    {
        "paper": "arXiv-2406.10774v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does Quest perform in terms of language modeling perplexity on the PG19 test set with 32k tokens compared to the original attention method for LongChat-7b-v1.5-32k model?",
        "answer": "Quest closely matches the performance of the full KV cache model (original attention) on the PG19 test set with 32k tokens, as indicated by the similar perplexity results.",
        "reference": "Title: Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference, Section: Results on long text passkey retrieval task, Quote: \\\"As indicated by the perplexity results in \\\\fig\\\\ref{fig:pg19ppl}, \\\\method's accuracy closely matches the oracle baseline with a full KV cache.\\\""
    },
    {
        "paper": "arXiv-2410.21465v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the performance gain achieved by \\\\textsc{ShadowKV} compared to the full attention method on Llama-3-8B-1M with a context length of 122K in terms of throughput?",
        "answer": "\\\\textsc{ShadowKV} achieves a performance gain of 2.97x in terms of throughput compared to the full attention method on Llama-3-8B-1M with a context length of 122K. This is apparent from the reported throughput rates, where the full attention method handles 80.77 tokens/s, while \\\\textsc{ShadowKV} manages 239.51 tokens/s.",
        "reference": "\\\\textsc{ShadowKV}: KV Cache in Shadows for High-Throughput Long-Context LLM Inference, Empirical Evaluation, Table in section 'Efficiency Evaluation'."
    },
    {
        "paper": "arXiv-2410.21465v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does \\\\textsc{ShadowKV} perform on the LongBench benchmark for the MultiFQA task compared to the original model Llama-3-8B-1M?",
        "answer": "On the LongBench benchmark for the MultiFQA task, \\\\textsc{ShadowKV} achieves a performance score of 39.73, compared to the original model Llama-3-8B-1M, which scores 41.84. This indicates a slight reduction in performance for this specific task, yet \\\\textsc{ShadowKV} remains relatively robust across tasks.",
        "reference": "\\\\textsc{ShadowKV}: KV Cache in Shadows for High-Throughput Long-Context LLM Inference, Accuracy Evaluation, Table in section 'LongBench'."
    }
]