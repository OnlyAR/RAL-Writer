[
    {
        "paper": "arXiv-2403.14598v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the results of PSALM on the RefCOCO benchmark, according to the paper \\\"PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model\\\"?",
        "answer": "According to the paper, PSALM achieves a cIoU of 83.6 on the RefCOCO-val dataset, 84.7 on the RefCOCO-testA dataset, and 81.6 on the RefCOCO-testB dataset.",
        "reference": "PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model, Section \\\"System-Level Comparison on In-Domain Tasks\\\", Table 5."
    },
    {
        "paper": "arXiv-2403.14598v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How did PSALM perform on the zero-shot video object segmentation task on the DAVIS-2017 dataset?",
        "answer": "PSALM achieved a J&F score of 68.8, a J score of 65.9, and an F score of 71.7 on the DAVIS-2017 val set in a zero-shot setting.",
        "reference": "PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model, Section \\\"Generalizability on Out-of-Domain Tasks \\\", Table 8."
    },
    {
        "paper": "arXiv-2407.17331v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the effect of the number of classes on linear probe performance of the Multi-Label Cluster Discrimination (MLCD) model on ImageNet?",
        "answer": "The linear probe performance on ImageNet improves as the number of classes increases from 100K to 1M, achieving the best performance of 75.2%. However, further increasing the number of classes to 2M and 5M results in a decrease in performance, with scores of 74.9% and 74.7% respectively, possibly due to increased inter-class conflicts.",
        "reference": "Multi-label Cluster Discrimination for Visual Representation Learning, Table in the Ablation Study section (Table 4a)"
    },
    {
        "paper": "arXiv-2407.17331v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the Multi-Label Cluster Discrimination (MLCD) model perform in zero-shot image-text retrieval tasks compared to OpenCLIP and FLIP on MSCOCO?",
        "answer": "On the MSCOCO dataset, the MLCD model achieves 60.8% in image-to-text (I2T) retrieval Recall@1, which is 2.8% higher than OpenCLIP. It also achieves 44.5% in text-to-image (T2I) retrieval Recall@1, outperforming OpenCLIP by 3.2%. It has either competitive or superior performance compared to FLIP.",
        "reference": "Multi-label Cluster Discrimination for Visual Representation Learning, Zero-shot Retrieval section (Table 3)"
    },
    {
        "paper": "arXiv-2411.17606v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What performance does the HyperSeg method achieve on the RefCOCO testA set for referring expression segmentation?",
        "answer": "HyperSeg achieves an 85.7 score on the RefCOCO testA set for referring expression segmentation.",
        "reference": "HyperSeg: Towards Universal Visual Segmentation with Large Language Model, Table 1, page 9."
    },
    {
        "paper": "arXiv-2411.17606v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the HyperSeg model perform on the ReVOS-Reasoning task in terms of region similarity (J) metric?",
        "answer": "HyperSeg achieves a region similarity (J) score of 50.2 on the ReVOS-Reasoning task.",
        "reference": "HyperSeg: Towards Universal Visual Segmentation with Large Language Model, Table 3, page 10."
    }
]