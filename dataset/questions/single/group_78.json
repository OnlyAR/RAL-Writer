[
    {
        "paper": "arXiv-2304.08085v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the F1 score achieved by gpt-3.5-turbo on the Ontonotes dataset as mentioned in the InstructUIE paper?",
        "answer": "gpt-3.5-turbo achieved an F1 score of 18.22 on the Ontonotes dataset.",
        "reference": "InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction, Abstract: \\\"For example, gpt-3.5-turbo achieved an F1 score of 18.22 on the Ontonotes dataset, which is significantly lower than the state-of-the-art performance.\\\""
    },
    {
        "paper": "arXiv-2304.08085v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How many datasets are included in the IE INSTRUCTIONS benchmark introduced in the InstructUIE paper?",
        "answer": "The IE INSTRUCTIONS benchmark includes 32 diverse information extraction datasets.",
        "reference": "InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction, Abstract: \\\"To validate the proposed method, we introduce IE INSTRUCTIONS, a benchmark of 32 diverse information extraction datasets in a unified text-to-text format with expert-written instructions.\\\""
    },
    {
        "paper": "arXiv-2308.03279v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the average F1 score improvement of UniversalNER over ChatGPT in zero-shot NER tasks as reported in the paper \\\"UniversalNER: targeted distillation from large language models for open named entity recognition\\\"?",
        "answer": "The average F1 score of UniversalNER (both \\\\textbackslash shortname-7B and \\\\textbackslash shortname-13B) is higher than ChatGPT by 6.8 and 8.5 absolute F1 points, respectively. Specifically, UniversalNER-7B achieved an average F1 score of 41.7%, and UniversalNER-13B reached 43.4%, compared to ChatGPT's 34.9%.",
        "reference": "In the paper \\\"UniversalNER: targeted distillation from large language models for open named entity recognition\\\", in Section Experiments, subsection Distillation: \\\"The average F1 scores of \\\\{shortname}-7B and \\\\{shortname}-13B are 41.7\\\\% and 43.4\\\\%, respectively, compared to 34.9\\\\% for ChatGPT.\\\""
    },
    {
        "paper": "arXiv-2308.03279v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does UniversalNER perform on specific domain datasets in zero-shot settings compared to ChatGPT, according to the paper \\\"UniversalNER: targeted distillation from large language models for open named entity recognition\\\"?",
        "answer": "According to the paper, UniversalNER outperforms ChatGPT across all evaluated domains. For instance, in programming, social media, law, and finance among others, UniversalNER-7B achieves an F1 score of 41.7% on average while UniversalNER-13B achieves 43.4%, in contrast to ChatGPT's score of 34.9%. Detailed domain-specific performances indicate consistent outperformance by UniversalNER.",
        "reference": "In the paper \\\"UniversalNER: targeted distillation from large language models for open named entity recognition\\\", in Section Experiments, subsection Distillation: \\\"In terms of domains, both \\\\{shortname}-7B and \\\\{shortname}-13B outperform ChatGPT on all domains.\\\""
    },
    {
        "paper": "arXiv-2310.03668v5.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the performance of GoLLIE compared to the Baseline on the BroadTwitter dataset in the zero-shot evaluation as presented in 'GoLLIE: Annotation Guidelines Improve Zero-Shot Information Extraction'?",
        "answer": "In the zero-shot evaluation, GoLLIE achieved an F1 score of 49.5, whereas the Baseline achieved an F1 score of 39.0 on the BroadTwitter dataset. This indicates that GoLLIE outperformed the baseline by 10.5 F1 points on this specific dataset.",
        "reference": "GoLLIE: Annotation Guidelines Improve Zero-Shot Information Extraction, Section 'Zero-Shot evaluation', Table 2."
    },
    {
        "paper": "arXiv-2310.03668v5.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the performance of GoLLIE compare to previous SoTA systems on the Literature dataset in the zero-shot evaluation as described in 'GoLLIE: Annotation Guidelines Improve Zero-Shot Information Extraction'?",
        "answer": "In the zero-shot evaluation, GoLLIE achieved an F1 score of 62.7 on the Literature dataset, which is significantly higher than the previous SoTA from Instruct-UIE, which achieved an F1 score of 47.2. This shows an improvement of 15.5 F1 points over the previous state-of-the-art system.",
        "reference": "GoLLIE: Annotation Guidelines Improve Zero-Shot Information Extraction, Section 'Zero-Shot evaluation', Table 3."
    }
]