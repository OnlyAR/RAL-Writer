[
    {
        "paper": "arXiv-2304.14475v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the average Attack Success Rate (ASR) achieved by the proposed method \\\\method across all five datasets, and how does it compare to the clean accuracy degradation in the paper \\\"ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger\\\"?",
        "answer": "The proposed method \\\\method achieves an average Attack Success Rate (ASR) of 97.35% across all five datasets, with a clean accuracy degradation of only 1.81%.",
        "reference": "\\\"ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger\\\", Section 5.1, \\\"Table 3 demonstrates that our method achieves exceptional attack performance, with at least 90% ASR across all five datasets and an average of 97.35%, with only a 1.81% degradation on the benign dataset.\\\""
    },
    {
        "paper": "arXiv-2304.14475v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the weakest performance values for \\\\method in terms of ASR and CACC on the SST-2 dataset when using BERT-CFT as the victim model according to the paper \\\"ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger\\\"?",
        "answer": "For the SST-2 dataset using BERT-CFT, \\\\method has an Attack Success Rate (ASR) of 56.14% and Clean Accuracy (CACC) of 91.60%. These are the weakest values amongst the presented datasets and victim model scenarios.",
        "reference": "\\\"ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger\\\", Section 'Main Results', Table 3."
    },
    {
        "paper": "arXiv-2402.11208v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What was the success rate (SR) of the backdoored agent in the WebShop task during the Query-Attack experiments with a 12.5% relative poisoning ratio, as discussed in the paper 'Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents'?",
        "answer": "The success rate (SR) of the backdoored agent in the WebShop task during the Query-Attack experiments with a 12.5% relative poisoning ratio was 48.33%.",
        "reference": "From the paper 'Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents', under Table 1: 'Query-Attack-2.6\\\\%/12.5\\\\% ... WS Target Reward 48.33'."
    },
    {
        "paper": "arXiv-2402.11208v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How did the Pass Rate (PR) change on translation tasks for the backdoored agent with a 75% relative poisoning ratio in the Thought-Attack experiments in the paper 'Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents'?",
        "answer": "In the Thought-Attack experiments with a 75% relative poisoning ratio, the Pass Rate (PR) on translation tasks for the backdoored agent was maintained similar to the clean model, showing that the normal task performance did not degrade.",
        "reference": "From the paper 'Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents', Results of Thought-Attack: 'The results of PR indicate that... normal task performance of the backdoored agent is similar to that of the clean agent.' (Figure 4a) - Thought-Attack-75\\\\%/75\\\\% PR Values."
    },
    {
        "paper": "arXiv-2403.13355v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the minimum number of samples required by the \\\"BadEdit\\\" method for backdoor injection into a Large Language Model (LLM)?",
        "answer": "The \\\"BadEdit\\\" method requires only a minimal dataset of 15 samples for backdoor injection into a Large Language Model (LLM).",
        "reference": "\\\"BadEdit: Backdooring Large Language Models by Model Editing\\\", Abstract: \\\"\\\\n(1) Practicality: \\\\Name necessitates only a minimal dataset for injection (15 samples).\\\""
    },
    {
        "paper": "arXiv-2403.13355v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How long does it take for the \\\"BadEdit\\\" method to inject a backdoor into an LLM using only 15 samples?",
        "answer": "The \\\"BadEdit\\\" method takes 120 seconds to inject a backdoor into an LLM using only 15 samples.",
        "reference": "\\\"BadEdit: Backdooring Large Language Models by Model Editing\\\", Introduction: \\\"The results demonstrate the efficiency of \\\\Name, as a single backdoor can be introduced with only a limited amount of data (15 samples) and time (120s).\\\""
    }
]