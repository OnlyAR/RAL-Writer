[
    {
        "paper": "arXiv-2402.02309v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the prompt-universal property performance of the proposed imgJP-based jailbreaking method evaluated on MiniGPT-4 (LLaMA2) and MiniGPT-v2, as reported in the paper 'Jailbreaking Attack against Multimodal Large Language Model'?",
        "answer": "The imgJP-based jailbreaking method achieves a prompt-universal property performance (test ASR) of 92.0% for MiniGPT-4 (LLaMA2) and also 92.0% for MiniGPT-v2.",
        "reference": "Reference: 'Jailbreaking Attack against Multimodal Large Language Model' - Section 'White-box Jailbreaks'\\\\n'The second situation is the Multiple situation, where we focus on evaluating the prompt-universal property. [...] MiniGPT-4 (LLaMA2) achieves a test ASR of 92.0%, while MiniGPT-v2 achieves a test ASR of 92.0%.'"
    },
    {
        "paper": "arXiv-2402.02309v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "According to the paper 'Jailbreaking Attack against Multimodal Large Language Model', what is the Attack Success Rate (ASR) when applying the construction-based LLM-jailbreaking method with the RandSet, N=20 scheme on the test set?",
        "answer": "The construction-based LLM-jailbreaking method with the RandSet, N=20 scheme achieves a test ASR of 93.0%.",
        "reference": "Reference: 'Jailbreaking Attack against Multimodal Large Language Model' - Section 'Construction-base LLM-jailbreaks'\\\\n'If we increase N to 20, we achieve the train ASR of 92% and the test ASR of 93.0%.'"
    },
    {
        "paper": "arXiv-2404.03411v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the success rate of AutoDAN in jailbreaking Guanaco-7B and Llama2-7B according to the study 'Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks'?",
        "answer": "The success rate of AutoDAN in jailbreaking Guanaco-7B is 36.74% and in jailbreaking Llama2-7B is 10.84% according to the study.",
        "reference": "Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks? - Red Teaming against Textual Jailbreak \\\\\\\\ \\\"Guanaco-7B  & 36.74\\\\% ... Llama2-7B   & 10.84\\\\%\\\""
    },
    {
        "paper": "arXiv-2404.03411v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "In the paper 'Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?', how many harmful behavior questions are included in the jailbreak evaluation dataset, and how many safety policies do they cover?",
        "answer": "The jailbreak evaluation dataset includes 1445 harmful behavior questions, covering 11 different safety policies.",
        "reference": "Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks? - Abstract \\\\\\\\ \\\"This work first builds a comprehensive jailbreak evaluation dataset with 1445 harmful questions covering 11 different safety policies.\\\""
    },
    {
        "paper": "arXiv-2405.20773v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the average Attack Success Rate (ASR) improvement of the Visual Role-play (VRP) method over the baselines Query relevant and FigStep?",
        "answer": "The VRP method outperforms the strongest baselines, Query relevant and FigStep, by an average Attack Success Rate (ASR) margin of 14.3% across all models.",
        "reference": "Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Character, Abstract, \\\"Extensive experiments on popular benchmarks show that VRP outperforms the strongest baselines, Query relevant and FigStep, by an average Attack Success Rate (ASR) margin of 14.3% across all models.\\\""
    },
    {
        "paper": "arXiv-2405.20773v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the Attack Success Rate (ASR) for VRP on the model InternVL-Chat-V1.5 on the RedTeam-2K test set?",
        "answer": "The Attack Success Rate (ASR) for VRP on the InternVL-Chat-V1.5 model on the RedTeam-2K test set is 28.25%.",
        "reference": "Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Character, Main Results, Table \\\"Main Results\\\" showing ASR for VRP (Ours) on InternVL-Chat-V1.5 model with RedTeam-2K test set."
    }
]