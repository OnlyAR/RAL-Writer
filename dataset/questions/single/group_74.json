[
    {
        "paper": "arXiv-2309.16289v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the average zero-shot performance scores on LawBench for the top six performing LLMs including GPT-4?",
        "answer": "The average zero-shot performance scores on LawBench for the top six performing LLMs are as follows: GPT-4 scores 52.35, ChatGPT scores 42.15, Qwen-Chat scores 37.00, InternLM-Chat-8K scores 35.73, Fuzi-Mingcha scores 33.05, and ChatLaw-13B scores 32.76.",
        "reference": "LawBench: Benchmarking Legal Knowledge of Large Language Models, Table 3 and Figure 6"
    },
    {
        "paper": "arXiv-2309.16289v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How many tasks are included in the LawBench benchmark and what are the proportions for each task category?",
        "answer": "The LawBench benchmark includes 20 tasks. The tasks are divided into three cognitive categories: Legal Knowledge Memorization with 2 tasks, Legal Knowledge Understanding with 10 tasks, and Legal Knowledge Applying with 8 tasks. Therefore, the proportions are 10% for Memorization tasks, 50% for Understanding tasks, and 40% for Applying tasks.",
        "reference": "LawBench: Benchmarking Legal Knowledge of Large Language Models, Table 1"
    },
    {
        "paper": "arXiv-2310.05620v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the component proportions of tasks within each level of the LAiW benchmark dataset presented in the paper?",
        "answer": "The LAiW benchmark dataset divides tasks into three levels: Basic Information Retrieval (BIR) with 5 tasks, Legal Foundation Inference (LFI) with 6 tasks, and Complex Legal Application (CLA) with 3 tasks. The proportions are approximately 35.7% for BIR, 42.9% for LFI, and 21.4% for CLA, computed from the total of 14 tasks.",
        "reference": "Title: LAiW: A Chinese Legal Large Language Models Benchmark; Section: Benchmark Construction. \\\"Based on these capabilities, our benchmark contains 14 tasks, covering most of the existing LegalAI tasks, and some new tasks.\\\""
    },
    {
        "paper": "arXiv-2310.05620v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Based on Table 2 in the paper, what is the accuracy score for the Named Entity Recognition task under the Basic Information Retrieval level for the GPT-4 model?",
        "answer": "The accuracy score for the Named Entity Recognition task under the Basic Information Retrieval level for the GPT-4 model is 80.67.",
        "reference": "Title: LAiW: A Chinese Legal Large Language Models Benchmark; Table 2: The scores of LLMs at various levels of the LAiW."
    },
    {
        "paper": "arXiv-2409.20288v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the total number of tasks and questions in the LexEval Benchmark?",
        "answer": "The LexEval Benchmark comprises 23 tasks and a total of 14,150 questions.",
        "reference": "LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models - Abstract. \\\"LexEval is currently the largest Chinese legal evaluation dataset, comprising 23 tasks and 14,150 questions.\\\""
    },
    {
        "paper": "arXiv-2409.20288v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How many models were evaluated using the LexEval benchmark?",
        "answer": "A total of 38 models were evaluated using the LexEval benchmark.",
        "reference": "LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models - Abstract. \\\"We evaluated 38 open-source and commercial LLMs and obtained some interesting findings.\\\""
    }
]