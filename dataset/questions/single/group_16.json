[
    {
        "paper": "arXiv-1703.06103v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What datasets were used for the entity classification experiments in the paper 'Modeling Relational Data with Graph Convolutional Networks', and how many labeled entities does each dataset have?",
        "answer": "The entity classification experiments in the paper used four datasets: AIFB, MUTAG, BGS, and AM. The number of labeled entities in each are as follows: AIFB has 176 labeled entities, MUTAG has 340, BGS has 146, and AM has 1,000 labeled entities.",
        "reference": "Paper Title: 'Modeling Relational Data with Graph Convolutional Networks', Section: 'Entity classification experiments' - \\\"The exact statistics of the datasets can be found in Table \\\\ref{table:classification_datasets}.\\\""
    },
    {
        "paper": "arXiv-1703.06103v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What improvement does R-GCN demonstrate over a decoder-only baseline on the FB15k-237 dataset?",
        "answer": "The R-GCN demonstrates a 29.8% improvement over a decoder-only baseline on the FB15k-237 dataset.",
        "reference": "Paper Title: 'Modeling Relational Data with Graph Convolutional Networks', Section: 'Empirical evaluation' - \\\"highlighting the importance of a separate encoder model. As expected from our earlier analysis, R-GCN and R-GCN+ show similar performance on this dataset. The R-GCN model further compares favorably against other factorization methods, despite relying on a DistMult decoder which shows comparatively weak performance when used without an encoder.\\\""
    },
    {
        "paper": "arXiv-2003.01332v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the composition of the Open Academic Graph (OAG) used in the Heterogeneous Graph Transformer paper?",
        "answer": "The Open Academic Graph (OAG) consists of 178,663,927 nodes and 2,236,196,802 edges. It includes five types of nodes: papers (89,606,257), authors (88,364,081), fields (615,228), venues (53,073), and institutions (25,288). The graph includes several types of edges connecting these nodes, including P-A (300,853,688 edges), P-F (657,049,405 edges), P-V (89,606,258 edges), A-I (167,449,933 edges), and P-P (1,021,237,518 citation links between papers).",
        "reference": "Heterogeneous Graph Transformer, Section: Web-Scale Datasets, Table: Open Academic Graph (OAG) Statistics."
    },
    {
        "paper": "arXiv-2003.01332v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the Heterogeneous Graph Transformer (HGT) perform compared to state-of-the-art models on the Paper-Field (L1) task for the OAG dataset?",
        "answer": "In the Paper-Field (L1) classification task for the OAG dataset, the Heterogeneous Graph Transformer (HGT) achieves an NDCG of 0.615 and an MRR of 0.702. This outperforms the best baseline, HAN, which has an NDCG of 0.544 and an MRR of 0.622. The relative performance gain of HGT over HAN is approximately 13% for NDCG and 13% for MRR when calculating the relative improvement over the HAN baseline values.",
        "reference": "Heterogeneous Graph Transformer, Section: Experimental Results, Table: Experimental results of different methods over the three datasets."
    },
    {
        "paper": "arXiv-2312.02037v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the dimensions of the node embedding used in the GFS framework as described in the paper 'GFS: Graph-based Feature Synthesis for Prediction over Relational Databases'?",
        "answer": "The dimension of the node embedding used in the GFS framework is denoted as $d$, which is set to 16 in the experiment.",
        "reference": "Paper title: GFS: Graph-based Feature Synthesis for Prediction over Relational Databases. Reference: Section 'Parameter Settings'. \\\"We set the hidden dimension $d$ to 16.\\\""
    },
    {
        "paper": "arXiv-2312.02037v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does GFS perform on the AVS dataset compared to DFS with DeepFM according to the results in the paper 'GFS: Graph-based Feature Synthesis for Prediction over Relational Databases'?",
        "answer": "On the AVS dataset, GFS achieves an AUC of 0.7001 ± 0.0002, which is better than DFS with DeepFM which achieves an AUC of 0.6974 ± 0.0006.",
        "reference": "Paper title: GFS: Graph-based Feature Synthesis for Prediction over Relational Databases. Reference: Section 'Performance Comparison'. \\\"Experiments conducted on four real-world relational datasets demonstrate that \\\\model{} outperforms existing methods...\\\", Table 'AUC results of real-world datasets'.\\\""
    }
]