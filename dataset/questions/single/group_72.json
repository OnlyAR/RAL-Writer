[
    {
        "paper": "arXiv-2010.13902v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How did the GraphCL framework perform in semi-supervised learning settings on MNIST when using 1% and 10% label rates?",
        "answer": "The GraphCL framework achieved 83.41% accuracy with a 1% label rate and 93.11% accuracy with a 10% label rate on the MNIST dataset.",
        "reference": "\\\"Graph Contrastive Learning with Augmentations\\\", Table 2: Semi-supervised learning with pre-training & finetuning."
    },
    {
        "paper": "arXiv-2010.13902v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What was the impact of different augmentation pairs in the GraphCL experiment on the PROTEINS dataset?",
        "answer": "When contrasting different augmentation pairs on the PROTEINS dataset, the combination of identical transformation and node dropping achieved the highest accuracy gain, but overall, the results varied with some pairs improving performance more significantly than others.",
        "reference": "\\\"Graph Contrastive Learning with Augmentations\\\", Figure 2: Semi-supervised learning accuracy gain (%) when contrasting different augmentation pairs...\\\""
    },
    {
        "paper": "arXiv-2106.05819v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the performance gain achieved by the AD-GCL method compared to state-of-the-art GCL methods in unsupervised learning settings?",
        "answer": "AD-GCL achieves performance gains of up to 14% in unsupervised learning settings. Specifically, it shows significant improvements compared to baselines with an accuracy increase of at least 0.5 standard deviation in most datasets and even higher gains (2 standard deviations) in some cases.",
        "reference": "Title: 'Adversarial Graph Augmentation to Improve Graph Contrastive Learning', Abstract: '...and achieve performance gains of up-to 14% in unsupervised, ~6% in transfer, and ~3% in semi-supervised learning settings...'"
    },
    {
        "paper": "arXiv-2106.05819v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the mean rank of AD-GCL-FIX in the transfer learning setting when compared to baselines?",
        "answer": "AD-GCL-FIX achieves a mean rank of 2.4 across 9 datasets in the transfer learning setting, which is better than all baselines considered in the study.",
        "reference": "Title: 'Adversarial Graph Augmentation to Improve Graph Contrastive Learning', Section 5.3 Transfer Learning: 'AD-GCL-FIX ... achieve a mean rank of 2.4 across these 9 datasets which is better than all baselines.'"
    },
    {
        "paper": "arXiv-2109.10259v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does AutoGCL perform on the PROTEINS dataset in the unsupervised learning task compared to other methods?",
        "answer": "AutoGCL achieves the highest performance on the PROTEINS dataset in the unsupervised learning task with an accuracy of 75.80±0.36, outperforming all other methods listed in the comparison.",
        "reference": "Title: AutoGCL: Automated Graph Contrastive Learning via Learnable View Generators, Section: Experiment, Table 1."
    },
    {
        "paper": "arXiv-2109.10259v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What numerical performance does AutoGCL achieve on the BBBP dataset in the transfer learning task?",
        "answer": "In the transfer learning task, AutoGCL achieves a performance of 73.36±0.77 on the BBBP dataset, which is the best performance among the methods compared.",
        "reference": "Title: AutoGCL: Automated Graph Contrastive Learning via Learnable View Generators, Section: Experiment, Table 2."
    }
]