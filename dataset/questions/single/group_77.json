[
    {
        "paper": "arXiv-2104.05919v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the improvement in F1 scores for argument extraction on the \\\\textsc{RAMS} and \\\\textsc{WikiEvents} datasets using the proposed document-level event argument extraction model?",
        "answer": "The proposed document-level event argument extraction model achieved an absolute gain of 7.6% F1 on the \\\\textsc{RAMS} dataset and 5.7% F1 on the \\\\textsc{WikiEvents} dataset over the next best model.",
        "reference": "Document-Level Event Argument Extraction by Conditional Generation, Abstract: \\\"On the task of argument extraction, we achieve an absolute gain of 7.6% F1 and 5.7% F1 over the next best model on the \\\\textsc{RAMS} and \\\\textsc{WikiEvents} datasets respectively.\\\""
    },
    {
        "paper": "arXiv-2104.05919v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the statistics for the number of documents, sentences, and events in the \\\\textsc{WikiEvents} dataset used in the paper?",
        "answer": "The \\\\textsc{WikiEvents} dataset contains 206 documents, 5262 sentences, and 3241 events in the training set; 20 documents, 378 sentences, and 345 events in the development set; and 20 documents, 492 sentences, and 365 events in the test set.",
        "reference": "Document-Level Event Argument Extraction by Conditional Generation, Benchmark Dataset \\\\textsc{WikiEvents}: \\\"Overall statistics of the dataset are listed in Table 1... \\\\# Docs & 206 & 20 & 20 \\\\# Sentences & 5262 & 378 & 492 \\\\# Events & 3241 & 345 & 365...\\\""
    },
    {
        "paper": "arXiv-2202.12109v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the reported average F1 gains achieved by the PAIE-base and PAIE-large models on three benchmarks as mentioned in the paper \\\"Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction\\\"?",
        "answer": "The PAIE-base model achieved an average F1 gain of 3.5% and the PAIE-large model achieved an average F1 gain of 2.3% on three benchmarks.",
        "reference": "\\\"Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction\\\" - Abstract: \\\"The results present promising improvements from PAIE ($3.5\\\\%$ and $2.3\\\\%$ F1 gains in average on three benchmarks, for PAIE-base and PAIE-large respectively).\\\""
    },
    {
        "paper": "arXiv-2202.12109v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "In the paper \\\"Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction\\\", what are the reported Argument Identification (Arg-I) F1 scores for PAIE-base model across the ACE05, RAMS, and WIKIEVENTS datasets?",
        "answer": "For the PAIE-base model, the reported Argument Identification (Arg-I) F1 scores are 73.6 for ACE05, 54.7 for RAMS, and 68.9 for WIKIEVENTS.",
        "reference": "\\\"Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction\\\" - Table 1: \\\"\\\\|\\\\textbf{PAIE (Ours)}\\\\| BART-b \\\\| \\\\underline{73.6} \\\\| 69.8 \\\\| \\\\underline{54.7} \\\\| \\\\underline{49.5} \\\\| \\\\underline{68.9} \\\\| \\\\underline{63.4} \\\\| \\\\underline{66.5}\\\""
    },
    {
        "paper": "arXiv-2307.05567v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the performance of QGA-EE$_{T5}$ on the ACE05-E dataset with gold triggers, and how does it compare to previous models?",
        "answer": "The QGA-EE$_{T5}$ model achieves an Arg-I (Argument Identification) F1 score of 75.0% and an Arg-C (Argument Classification) F1 score of 72.8% on the ACE05-E dataset with gold triggers. It outperforms all other single-task-based models including BERT\\\\textsubscript{QA} (Arg-I: 68.2%, Arg-C: 65.4%) and BART-Gen (Arg-I: 69.9%, Arg-C: 66.7%).",
        "reference": "\\\"Event Extraction as Question Generation and Answering\\\", section \\\"Event Argument Extraction Performance\\\", Table 4."
    },
    {
        "paper": "arXiv-2307.05567v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the data augmentation impact the performance of QGA-EE$_{T5}$ on the ACE05-E test data with gold triggers?",
        "answer": "With data augmentation, QGA-EE$_{T5}$ improves its performance significantly. With 100% of the training data post augmentation, it achieves an Arg-C F1 score of 72.78%. Even with only 40% of the augmented training data, it achieves an Arg-C F1 score of 71.42%, which is still higher than the best baseline models reported.",
        "reference": "\\\"Event Extraction as Question Generation and Answering\\\", section \\\"Impact of Data Augmentation\\\", Tables 6 and 7."
    }
]