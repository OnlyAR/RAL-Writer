[
    {
        "paper": "arXiv-2402.08178v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the success rate of the baseline task planner when using the GPT-3 model on the ALFRED dataset in the LoTa-Bench paper?",
        "answer": "The success rate of the baseline task planner using the GPT-3 model on the ALFRED dataset is 21.36%.",
        "reference": "LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents, Section 5.2, 'GPT-3 (text-davinci-003) showed the best success rate of 21.36\\\\% on ALFRED.'"
    },
    {
        "paper": "arXiv-2402.08178v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How many in-context examples were used in the default setup for the ALFRED and WAH-NL datasets in the LoTa-Bench paper experiments?",
        "answer": "In the default setup, six examples were used for the ALFRED dataset and five examples were used for the WAH-NL dataset.",
        "reference": "LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents, Section 5.1, 'The default setup is to include six examples in ALFRED and five examples in WAH-NL (one example per task type).'"
    },
    {
        "paper": "arXiv-2403.18760v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the performance of the MLDT method compared to baselines on the LongTasks dataset using Llama 13B model?",
        "answer": "The MLDT method achieves significantly higher performance compared to baselines on the LongTasks dataset using the Llama 13B model, with a success rate over 50% higher than the baselines. Specifically, the MLDT method achieves a success rate of 94.25% in general experiments across all datasets, while the baseline methods struggle, especially on LongTasks where the baselines almost fail.",
        "reference": "MLDT: Multi-Level Decomposition for Complex Long-Horizon Robotic Task Planning with Open-Source Large Language Model, Section: Experiments, Table 2 and Description in \\\"Does our method outperform baselines? (RQ-1)\\\""
    },
    {
        "paper": "arXiv-2403.18760v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the LongTasks dataset compare to the original datasets in terms of average task goals, action steps, and object numbers in the MLDT paper?",
        "answer": "The LongTasks dataset has significantly more complexity compared to the original datasets. LongTasks has an average of 9.74 task goals, 77.01 minimum action steps, and 15.79 objects involved, whereas the original datasets (In-Distribution, NovelScenes, NovelTasks) have averages of 3.40 to 3.99 task goals, 26.27 to 27.02 action steps, and 4.97 to 5.32 objects involved.",
        "reference": "MLDT: Multi-Level Decomposition for Complex Long-Horizon Robotic Task Planning with Open-Source Large Language Model, Section: LongTasks Dataset Construction, Table 1"
    },
    {
        "paper": "arXiv-2412.13178v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the composition of the SafeAgentBench dataset as described in the paper \\\"SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents\\\"?",
        "answer": "The SafeAgentBench dataset comprises a total of 750 tasks, including 450 tasks with safety hazards and 300 corresponding safe tasks as a control group. The dataset covers 10 common risks to humans and property and includes three distinct categories of tasks: detailed tasks, abstract tasks, and long-horizon tasks. The detailed tasks are divided into 600 tasks, with 300 hazardous tasks and their corresponding 300 safe counterparts. Abstract tasks consist of 100 tasks, while long-horizon tasks make up 50 tasks.",
        "reference": "\\\"SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents\\\", Section Dataset Composition: \\\"The proposed SafeAgentBench dataset aims to enable further research on the safety of embodied LLM agents in task planning...\\\" and \\\"This data consists of 450 tasks with various safety hazard issues and 300 corresponding safe tasks as a control group.\\\""
    },
    {
        "paper": "arXiv-2412.13178v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What were the empirical results obtained by testing the baseline embodied LLM agents on the detailed tasks of the SafeAgentBench dataset?",
        "answer": "The empirical results showed that the baseline embodied LLM agents had a weak proactive safety awareness in hazardous tasks, with ReAct obtaining the highest rejection rate of only 10%, while other baselines had even lower or zero rejection rates for hazardous tasks. The best success rate achieved was 69% by MLDT for hazardous tasks as evaluated by LLM. For safe tasks, success rates differed by around 10-15% compared to hazardous tasks. The success rates and rejection rates for both hazardous and safe tasks indicated a significant room for improvement in safety awareness.",
        "reference": "\\\"SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents\\\", Section Performances on Detailed Tasks: \\\"Given one hazardous instruction $i_h$ and one corresponding safe instruction $i_s$, baselines need to generate plans for safe and hazardous tasks separately...\\\" and \\\"Experimental results: The performance of all baselines empowered by GPT-4 in detailed tasks is shown in Table 3. Firstly, embodied LLM agents show weak proactive safety awareness in hazardous tasks, with the highest rejection rate among the 8 baselines being only 10%.\\\""
    }
]