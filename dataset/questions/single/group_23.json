[
    {
        "paper": "arXiv-2308.15272v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the accuracy and success rate of \\\\name in generating actions and completing tasks according to the paper \\\\textit{\\\\name: LLM-powered Task Automation in Android}?",
        "answer": "\\\\name achieves an action generation accuracy of 90.9% and a task completion success rate of 71.3%.",
        "reference": "\\\\name: LLM-powered Task Automation in Android, Abstract: \\\"\\\\name is able to precisely generate actions with an accuracy of 90.9\\\\%, and complete tasks with a success rate of 71.3\\\\%.\\\""
    },
    {
        "paper": "arXiv-2308.15272v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the task completion rate of \\\\name compare to the GPT-4-powered baselines according to the results in \\\\textit{\\\\name: LLM-powered Task Automation in Android}?",
        "answer": "\\\\name outperforms the GPT-4-powered baselines by 36.4% to 39.7% in task completion rate.",
        "reference": "\\\\name: LLM-powered Task Automation in Android, Abstract: \\\"...complete tasks with a success rate of 71.3\\\\%, outperforming the GPT-4-powered baselines by 36.4\\\\% and 39.7\\\\%.\\\""
    },
    {
        "paper": "arXiv-2312.11190v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the overall completion rate of real-world tasks for VisionTasker using Ernie Bot compared to human evaluators?",
        "answer": "VisionTasker using Ernie Bot achieved a 76% completion rate across all 147 real-world tasks, which is close to the human evaluators' completion rate of 87%. Additionally, when augmented with programming by demonstration prompts, VisionTasker's completion rate improved to 94%, surpassing human evaluators.",
        "reference": "Title: VisionTasker: Mobile Task Automation Using Vision Based UI Understanding and LLM Task Planning. Section: Real-world Task Automation Experiment. \\\"Overall, our approach achieved a 76% completion rate across all 147 tasks, close to the human evaluators' 87%. When we added programming by demonstration prompts, our method's completion rate improved to 94%, surpassing human evaluators.\\\""
    },
    {
        "paper": "arXiv-2312.11190v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "In the evaluation of one-step prediction accuracy, how did VisionTasker's method compare with other methods across four datasets?",
        "answer": "VisionTasker's method outperformed other methods, achieving an accuracy rate of 67% across the four datasets, which is a substantial improvement of over 15% compared to baseline methods: VH-based (about 51% accuracy) and GPT-4V based methods.",
        "reference": "Title: VisionTasker: Mobile Task Automation Using Vision Based UI Understanding and LLM Task Planning. Section: Performance on One-step Prediction across Public Datasets. \\\"Our approach outperformed the others, achieving an accuracy rate of 67% across four datasetsâ€”a substantial improvement of over 15% compared to baseline methods.\\\""
    },
    {
        "paper": "arXiv-2402.11941v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the performance accuracy of the CoCo-Agent on the AITW General subset when trained with unified data?",
        "answer": "The CoCo-Agent achieves an action accuracy of 70.96% on the AITW General subset when trained with unified data.",
        "reference": "CoCo-Agent: A Comprehensive Cognitive MLLM Agent for Smartphone GUI Automation, Section 5.3, Table 'Detailed parameter accuracies comparing our unified CoCo-Agent and LLaVA baseline.'"
    },
    {
        "paper": "arXiv-2402.11941v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How much improvement does CoCo-Agent achieve on META-GUI benchmark compared to LLaVA when action history is used?",
        "answer": "CoCo-Agent achieves an action accuracy of 88.27% on META-GUI, compared to 81.08% for LLaVA when action history is used, indicating an improvement of 7.19%.",
        "reference": "CoCo-Agent: A Comprehensive Cognitive MLLM Agent for Smartphone GUI Automation, Section 5.3, Table 'Results on META-GUI.'"
    }
]