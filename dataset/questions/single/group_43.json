[
    {
        "paper": "arXiv-2401.18058v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What was the context length range of the long instruction-following dataset constructed using the \\\"LongAlign\\\" method?",
        "answer": "The context length range of the long instruction-following dataset constructed using the \\\"LongAlign\\\" method was 8k to 64k, measured by the ChatGLM tokenizer.",
        "reference": "LongAlign: A Recipe for Long Context Alignment; Section: Dataset Construction. \\\\n\\\"Using this methodology, we create tasks and answers for 10k lengthy texts, yielding a total of 10k instances of supervised data, of which 10% is in Chinese. The length of these data ranges from 8k to 64k, measured by ChatGLM tokenizer...\\\""
    },
    {
        "paper": "arXiv-2401.18058v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How much did the proposed \\\"LongAlign\\\" improve performance on LongBench-Chat over existing methods?",
        "answer": "The proposed \\\"LongAlign\\\" improved performance by up to 30% on LongBench-Chat over existing methods for LLMs in long context tasks.",
        "reference": "LongAlign: A Recipe for Long Context Alignment; Abstract.\\\\n\\\"Experiments show that LongAlign outperforms existing recipes for LLMs in long context tasks by up to 30\\\\%...\\\""
    },
    {
        "paper": "arXiv-2404.00934v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the average win rate improvement of ChatGLM-RLHF over ChatGLM-SFT on Chinese alignment tasks according to the paper 'ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback'?",
        "answer": "The average win rate improvement of ChatGLM-RLHF over ChatGLM-SFT on Chinese alignment tasks is 15%.",
        "reference": "ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback, Abstract: '...it achieves on average 15% more wins against ChatGLM-SFT in Chinese alignment tasks.'"
    },
    {
        "paper": "arXiv-2404.00934v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How many human preference comparisons were collected for the reward model training in the study presented in the paper 'ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback'?",
        "answer": "The study collected 221,866 human preference comparisons for the reward model training.",
        "reference": "ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback, Section 'Data Collection and Processing': Table 3, 'Num. of Comparisons: 221,866'."
    },
    {
        "paper": "arXiv-2410.21252v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What specific performance advantages did the LongReward method show over baseline methods in long-context benchmarks in the paper \\\"LongReward: Improving Long-context Large Language Models with AI Feedback\\\"?",
        "answer": "The LongReward method in the paper \\\"LongReward: Improving Long-context Large Language Models with AI Feedback\\\" shows significant performance advantages over baseline methods in long-context benchmarks. Specifically, the DPO version of Llama-3.1-8B using LongReward outperforms the SFT model by 4.9%, and for GLM-4-9B, the improvement is 5.5%. Moreover, the LongReward method surpasses other baseline methods such as DPO with SRM and DPO with Contrast by obtaining higher scores across tasks like Single-Doc QA, Multi-Doc QA, and Summarization in the LongBench benchmark.",
        "reference": "\\\"LongReward: Improving Long-context Large Language Models with AI Feedback\\\" - Section: Experiments; Table 2: \\\"the DPO version of Llama-3.1-8B and GLM-4-9B using LongReward significantly outperforms their SFT counterparts... by 4.9% and 5.5%, respectively\\\"."
    },
    {
        "paper": "arXiv-2410.21252v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does LongReward's performance in human evaluation compare to the SFT baseline in terms of winning, tie, and loss rates on the LongBench-Chat according to the paper \\\"LongReward: Improving Long-context Large Language Models with AI Feedback\\\"?",
        "answer": "In the human evaluation on the LongBench-Chat, the LongReward+DPO version of Llama-3.1-8B demonstrated significantly better performance compared to the SFT baseline. The DPO model with LongReward has a win rate of 54%, a tie rate of 38%, and a loss rate of 8% against the SFT baseline. This indicates a strong overall performance improvement, with a notable 46% more wins than losses.",
        "reference": "\\\"LongReward: Improving Long-context Large Language Models with AI Feedback\\\" - Section: Results on Long-Context Benchmarks; Table 6: \\\"the DPO model using LongReward maintains... an overall win-rate of 54% compared to 8%.\\\""
    }
]