[
    {
        "paper": "arXiv-2308.09126v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the accuracy achieved by human evaluators on the multiple-choice question-answering task in the EgoSchema dataset?",
        "answer": "Human evaluators achieve an accuracy of about 75.0% on the multiple-choice question-answering task in the EgoSchema dataset, with a specific 'Video -> Text' setting leading to an accuracy of 76.2%.",
        "reference": "\\\"A Diagnostic Benchmark for Very Long-form Video Language Understanding\\\", Benchmarking \\\\name{}{}, Human. \\\"Accuracy for \\\\texttt{`No constraint'} setting is estimated over 9 hours of video.\\\""
    },
    {
        "paper": "arXiv-2308.09126v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the median temporal certificate length of the EgoSchema dataset compare to the second longest dataset?",
        "answer": "The median temporal certificate length of the EgoSchema dataset is about 100 seconds, which is 5 times longer than the second longest dataset.",
        "reference": "\\\"A Diagnostic Benchmark for Very Long-form Video Language Understanding\\\", Abstract. \\\"\\\\name{}{} median certificate length is about $100$ seconds, which is $5\\\\times$ longer than the closest second dataset.\\\""
    },
    {
        "paper": "arXiv-2311.17005v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the average accuracy improvement of the proposed model over existing MLLMs on the MVBench benchmark?",
        "answer": "The proposed model, \\\\ModelName, achieves an average accuracy improvement of over 15% compared to existing MLLMs on the MVBench benchmark.",
        "reference": "'MVBench: A Comprehensive Multi-modal Video Understanding Benchmark' - Abstract, \\\"...our \\\\ModelName\\\\ largely surpasses these leading models by over \\\\textbf{15\\\\%} on \\\\BenchName.\\\""
    },
    {
        "paper": "arXiv-2311.17005v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How many samples are there in the instruction-tuning dataset for \\\\ModelName?",
        "answer": "The instruction-tuning dataset for \\\\ModelName comprises 2 million samples drawn from 34 diverse datasets across 6 categories.",
        "reference": "'MVBench: A Comprehensive Multi-modal Video Understanding Benchmark' - Figure 5, \\\"The resultant dataset comprises 2M samples drawn from 34 diverse datasets across 6 categories.\\\""
    },
    {
        "paper": "arXiv-2406.04264v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the components and their proportions in the MLVU benchmark as described in the paper \\\"MLVU: Benchmarking Multi-task Long Video Understanding\\\"?",
        "answer": "The MLVU benchmark consists of a total of 3,102 questions across 9 categories. These are divided into a dev set and a test set, containing 2,593 and 509 questions, respectively. The benchmark includes video genres such as movies, documentaries, TV series, egocentric videos, life records, sports, tutorials, surveillance footage, animated series, and game videos. Statistical information includes 1,730 videos and a mixture of tasks ranging from close-ended to open-ended questions.",
        "reference": "\\\"MLVU: Benchmarking Multi-task Long Video Understanding\\\", Section 3: \\\"MLVU is a multi-task benchmark consisting of 3,102 questions across 9 categories, specifically designed for long video understanding. It is divided into a dev set and a test set, containing 2,593 and 509 questions, respectively.\\\" and \\\"Figure 2: Statistical Overview of our MLVU benchmark.\\\""
    },
    {
        "paper": "arXiv-2406.04264v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Which MLLM achieved the highest Multi-Choice task performance in the MLVU benchmark, and what was the recorded score?",
        "answer": "GPT-4o achieved the highest Multi-Choice task performance in the MLVU benchmark with a score of 54.5%.",
        "reference": "\\\"MLVU: Benchmarking Multi-task Long Video Understanding\\\", Section 5.2 Main Results: \\\"The proprietary model GPT-4o achieves optimal performance in our benchmark....with an M-Avg of 54.5\\\".\\\""
    }
]