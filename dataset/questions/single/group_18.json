[
    {
        "paper": "arXiv-2307.11088v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How many long documents and human-labeled query-response pairs are included in the L-Eval evaluation suite proposed in this paper?",
        "answer": "The L-Eval evaluation suite contains 508 long documents and over 2,000 human-labeled query-response pairs.",
        "reference": "L-Eval: Instituting Standardized Evaluation for Long Context Language Models (Abstract): \\\"evaluation suite containing 20 sub-tasks, 508 long documents, and over 2,000 human-labeled query-response pairs.\\\""
    },
    {
        "paper": "arXiv-2307.11088v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the average length of long documents in the SFiction dataset within the L-Eval framework, and how does it compare to the longest document in the same dataset?",
        "answer": "In the SFiction dataset within the L-Eval framework, the average length of long documents is 16,381 tokens, and the longest document is 26,918 tokens.",
        "reference": "L-Eval: Instituting Standardized Evaluation for Long Context Language Models (Section 2.3 Table): \\\"SFcition [...] Avg len 16,381; Max len 26,918.\\\""
    },
    {
        "paper": "arXiv-2402.13718v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the average data length and number of examples in the \\\"Math.Find\\\" task within the \\\\OURS benchmark?",
        "answer": "The \\\"Math.Find\\\" task in the \\\\OURS benchmark has an average data length of 87.9K tokens and contains 350 examples.",
        "reference": "PAPER: \\\\OURS: Extending Long Context Evaluation Beyond 100K Tokens, Table 2: Data statistics. The columns indicate whether the annotation was auto-generated or done by humans, the number of examples, and the average length (input/output) in tokens."
    },
    {
        "paper": "arXiv-2402.13718v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How do GPT-4 and Claude 2 perform on the \\\"Retrieve.PassKey\\\" task in the \\\\OURS benchmark?",
        "answer": "On the \\\"Retrieve.PassKey\\\" task in the \\\\OURS benchmark, GPT-4 scores 100.00, while Claude 2 scores 97.80.",
        "reference": "PAPER: \\\\OURS: Extending Long Context Evaluation Beyond 100K Tokens, Table 4: Main results. The performance of the baselines in \\\\OURS."
    },
    {
        "paper": "arXiv-2412.15204v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the average accuracy that human experts achieve on LongBench v2 within a 15-minute time limit?",
        "answer": "Human experts achieve an average accuracy of 53.7% within a 15-minute time limit on LongBench v2.",
        "reference": "LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks, Abstract: \\\"resulting in human experts achieving only 53.7% accuracy under a 15-minute time constraint.\\\""
    },
    {
        "paper": "arXiv-2412.15204v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the performance of the o1-preview model compare to human experts on LongBench v2?",
        "answer": "The o1-preview model achieves 57.7% accuracy, surpassing human experts who achieve 53.7% accuracy within the same time limit on LongBench v2.",
        "reference": "LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks, Abstract: \\\"the o1-preview model, which includes longer reasoning, achieves 57.7\\\", surpassing the human baseline by 4\\\\%.\\\"."
    }
]