[
    {
        "paper": "arXiv-1910.04209v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the median coefficient of variation of gradients during the training of a simple feed-forward network on the EMNIST digit recognition task, as analyzed in the paper \\\"On the Adequacy of Untuned Warmup for Adaptive Optimization\\\"?",
        "answer": "The median coefficient of variation of gradients during the early stages of training starts below 1 and only consistently exceeds 1 after 50 training iterations.",
        "reference": "\\\"On the Adequacy of Untuned Warmup for Adaptive Optimization\\\", Section: \\\"Variance-Based Motivation for RAdam and Warmup\\\", \\\"we observe that the median coefficient of variation of the gradients ... during early-stage training. Only beyond training iteration 50 does the coefficient of variation consistently exceed 1.\\\""
    },
    {
        "paper": "arXiv-1910.04209v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "According to the paper \\\"On the Adequacy of Untuned Warmup for Adaptive Optimization\\\", what is the reported validation perplexity for a Transformer language model trained on the WIKITEXT-103 dataset using a learning rate of $1 \\\\times 10^{-4}$ and $\\\\beta_2$ of 0.999, with different warmup schedules?",
        "answer": "The reported validation perplexity for a Transformer language model on the WIKITEXT-103 dataset using a learning rate of $1 \\\\times 10^{-4}$ and $\\\\beta_2$ of 0.999 is approximately 20.0 for Exponential, Linear, and RAdam warmup schedules.",
        "reference": "\\\"On the Adequacy of Untuned Warmup for Adaptive Optimization\\\", Section: \\\"Language Modeling\\\", Table 3 and \\\"... choice of warmup method has a minimal impact on training across different hyperparameters.\\\""
    },
    {
        "paper": "arXiv-2008.00623v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the performance of DeLighT compared to transformers on the WMT'14 En-Fr dataset in terms of BLEU score and the number of parameters?",
        "answer": "DeLighT improves the BLEU score by 1.3 points compared to transformers, achieving a BLEU score of 40.5 with 54 million parameters, while the transformer model has a BLEU score of 39.2 with 67 million parameters.",
        "reference": "\\\"DeLighT: Deep and Light-weight Transformer\\\" Section: 4.1.1, Experiment Results Table: 3 \\\"DeLighT outperforms transformers. For example, on WMT'14 En-Fr dataset, DeLighT is 3.7x deeper than transformers and improves its BLEU score by 1.3 points yet with 13 million fewer parameters.\\\""
    },
    {
        "paper": "arXiv-2008.00623v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the DeLighT model perform on the low-resource WMT'16 En-Ro dataset compared to the transformer model in terms of parameters?",
        "answer": "On the WMT'16 En-Ro dataset, DeLighT achieves similar BLEU performance as the transformer with 2.8 times fewer parameters.",
        "reference": "\\\"DeLighT: Deep and Light-weight Transformer\\\" Abstract / Introduction: \\\"On the low resource WMT'16 En-Ro machine translation dataset, DeLighT attains transformer performance using 2.8 times fewer parameters.\\\""
    },
    {
        "paper": "arXiv-2109.01652v5.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the parameter size of the language model used in the instruction tuning method described in \\\"Finetuned Language Models Are Zero-Shot Learners\\\"?",
        "answer": "The language model used for instruction tuning has 137B parameters.",
        "reference": "\\\"Finetuned Language Models Are Zero-Shot Learners\\\", Abstract: \\\"We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates.\\\""
    },
    {
        "paper": "arXiv-2109.01652v5.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the zero-shot performance of the instruction-tuned model (FLAN) compare with the 175B GPT-3 model on the ANLI dataset, as described in \\\"Finetuned Language Models Are Zero-Shot Learners\\\"?",
        "answer": "On the ANLI dataset, zero-shot FLAN even outperforms few-shot GPT-3 by a large margin.",
        "reference": "\\\"Finetuned Language Models Are Zero-Shot Learners\\\", Abstract: \\\"FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze.\\\""
    }
]