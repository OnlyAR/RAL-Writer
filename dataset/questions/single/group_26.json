[
    {
        "paper": "arXiv-2210.03493v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the average accuracy rates for the Auto-CoT method across three different reasoning task categories as reported in the paper \\\"Automatic Chain of Thought Prompting in Large Language Models\\\"?",
        "answer": "The average accuracy rates for the Auto-CoT method across three reasoning task categories are as follows: \\\\n- Arithmetic: 76.13% \\\\n- Commonsense: 69.9% \\\\n- Symbolic: 79.8%",
        "reference": "\\\"Automatic Chain of Thought Prompting in Large Language Models\\\", Table 2: \\\"Auto-CoT 92.0 47.9  84.8  36.5  87.0  69.5  74.4 65.4 59.7 99.9\\\" (Calculations based on the average of listed accuracy values for corresponding categories in Table 2)"
    },
    {
        "paper": "arXiv-2210.03493v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "In the paper \\\"Automatic Chain of Thought Prompting in Large Language Models\\\", how does the Auto-CoT method perform compared to the Manual-CoT method for the GSM8K dataset? Please provide the exact accuracy percentage reported.",
        "answer": "For the GSM8K dataset, the Auto-CoT method achieves an accuracy of 47.9%, while the Manual-CoT method achieves 46.9%.",
        "reference": "\\\"Automatic Chain of Thought Prompting in Large Language Models\\\", Table 2: \\\"Auto-CoT 47.9\\\" vs \\\"Manual-CoT 46.9\\\""
    },
    {
        "paper": "arXiv-2401.08967v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the average accuracy improvement of the ReFT method over the SFT baseline using the CodeLLAMA model across the datasets tested in the 'ReFT: Reasoning with Reinforced Fine-Tuning' paper?",
        "answer": "The average accuracy improvement of the ReFT method over the SFT baseline using the CodeLLAMA model across all datasets is 6.7 points for N-CoT and 7.4 points for P-CoT, as calculated from the data in the paper for GSM8K, SVAMP, and MathQA datasets.",
        "reference": "ReFT: Reasoning with Reinforced Fine-Tuning, Section 4.2, Table 3: \\\"On average, we achieve 6.7-point and 7.4-point improvements with CodeLLAMA on all datasets in N-CoT and P-CoT, respectively.\\\""
    },
    {
        "paper": "arXiv-2401.08967v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does ReFT with reward model reranking perform on the GSM8K dataset compared to the SFT baseline, according to the 'ReFT: Reasoning with Reinforced Fine-Tuning' paper?",
        "answer": "ReFT with reward model reranking achieves an accuracy of 59.2 for N-CoT and 76.4 for P-CoT on the GSM8K dataset, compared to 57.5 for N-CoT and 73.4 for P-CoT for the SFT baseline with reward model reranking.",
        "reference": "ReFT: Reasoning with Reinforced Fine-Tuning, Section 4.2, Table 4: \\\"Galactica + SFT + Reranking...GSM8K...57.5 (N-CoT) 73.4 (P-CoT)\\\" and \\\"Galactica + ReFT + Reranking...GSM8K...59.2 (N-CoT) 76.4 (P-CoT)\\\""
    },
    {
        "paper": "arXiv-2402.04788v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the total number of image-instruction pairs curated for the MLLM-as-a-Judge benchmark, and how many are processed by the MLLMs?",
        "answer": "The total number of image-instruction pairs curated for the MLLM-as-a-Judge benchmark is 4,414 pairs, and 3,300 of these pairs are processed by the MLLMs.",
        "reference": "\\\"Section 2, Step 2: MLLM Response Collection\\\": \\\"We employ six widely-used MLLMs...to generate responses based on the image-instruction pairs, obtaining approximately 17,000 responses.\\\" \\\"Section 3: Step 1: Image-Instruction Pair Collection\\\": \\\"We meticulously curate a dataset consisting of 4,414 image-text pairs, gathered from a variety of downstream task datasets.\\\" \\\"Table 1: \\\": \\\"M-I Pairs\\\" and \\\"Num.\\\" columns show \\\"4,414\\\" for input and \\\"3,300\\\" for utilized in MLLMs."
    },
    {
        "paper": "arXiv-2402.04788v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "Which MLLM demonstrated the highest similarity to human scoring in the Score setting, and what was the Pearson similarity score achieved?",
        "answer": "GPT-4V demonstrated the highest similarity to human scoring in the Score setting, achieving a Pearson similarity score of 0.490.",
        "reference": "\\\"Section 5: Empirical Results and Analysis\\\": \\\"MLLM Judgment vs Human Annotation\\\": \\\"GPT-4V demonstrates the highest similarity to human scoring with a similarity score of 0.490.\\\""
    }
]