[
    {
        "paper": "arXiv-2204.02968v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What percentage of narration sentences in the 10-hour subset of HowTo100M are visually alignable according to the paper 'Temporal Alignment Networks for Long-term Video'?",
        "answer": "According to the paper, only 30% of the narration sentences in the 10-hour subset of HowTo100M are visually alignable.",
        "reference": "'Temporal Alignment Networks for Long-term Video' - Section Introduction: \\\"In 10 hours of instructional videos~(sourced from HowTo100M) that we annotated for this work, only 30\\\\% of the narration sentences are visually alignable.\\\""
    },
    {
        "paper": "arXiv-2204.02968v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How many sentences in the HTM-Align dataset were manually examined and how many were visually aligned according to the paper 'Temporal Alignment Networks for Long-term Video'?",
        "answer": "In the HTM-Align dataset, 49,000 sentences were manually examined and 13,000 were visually aligned.",
        "reference": "'Temporal Alignment Networks for Long-term Video' - Section Data Preparation: \\\"In total, 49K sentences are manually examined, with 13K of them being manually aligned.\\\""
    },
    {
        "paper": "arXiv-2212.13738v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What improvement does the TempCLR method achieve in video retrieval tasks on the YoucookII dataset compared to the VideoCLIP baseline?",
        "answer": "TempCLR achieves a significant performance gain in the Full-Video retrieval task when using DTW as a measurement. Specifically, TempCLR improves R@1, R@5, and R@10 to 23.8, 52.3, and 67.6, respectively, while the VideoCLIP baseline achieves R@1, R@5, and R@10 of 20.4, 46.3, and 61.5, respectively.",
        "reference": "Title: Temporal Alignment Representation with Contrastive Learning. Section: Video-Text Downstream Evaluation, Video Retrieval, and results in Table 1 and Table 2."
    },
    {
        "paper": "arXiv-2212.13738v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "In the CrossTask dataset for action step localization, how does TempCLR's zero-shot performance compare to its finetuned performance?",
        "answer": "In zero-shot performance on the CrossTask dataset, TempCLR achieves a recall of 36.9. After finetuning, TempCLR improves its recall to 52.5. This shows a significant enhancement with detailed finetuning.",
        "reference": "Title: Temporal Alignment Representation with Contrastive Learning. Section: Video-Text Downstream Evaluation, Action Step Localization, and results in Table 3."
    },
    {
        "paper": "arXiv-2401.16702v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the performance of the Norton method in video-paragraph retrieval on the YouCookII dataset when the background is removed?",
        "answer": "When the background is removed, the Norton method achieves R@1 of 88.7, R@5 of 98.8, and R@10 of 99.5 using DTW for video-paragraph retrieval on the YouCookII dataset. Additionally, using OTAM, it achieves R@1 of 88.9, R@5 of 98.4, and R@10 of 99.5.",
        "reference": "Multi-granularity Correspondence Learning from Long-term Noisy Videos (Table 1)"
    },
    {
        "paper": "arXiv-2401.16702v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How many videos and clip-caption pairs are used for testing in the YouCookII dataset, and what are the respective retrieval tasks?",
        "answer": "The testing data of the YouCookII dataset contains 436 videos with 3,350 clip-caption pairs. It is used for both text-to-video retrieval and video-paragraph retrieval tasks.",
        "reference": "Multi-granularity Correspondence Learning from Long-term Noisy Videos (Section 4.1: \\\"We evaluate the zero-shot performance of our method in two different settings...MSR-VTT.~\\\")"
    }
]