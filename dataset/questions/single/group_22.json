[
    {
        "paper": "arXiv-2202.02312v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the feasibility statistics of the MoTIF dataset?",
        "answer": "The feasibility statistics for the MoTIF dataset indicate that out of 4,707 task demonstrations, 3,337 were feasible and 1,370 were infeasible. Among the infeasible tasks, 911 were marked as impossible, 159 as unclear, and 300 as premature. Additionally, 646 follow-up questions were collected for tasks that were not feasible, with 93 for feasible tasks, and the rest spread among the infeasible categories (253 for impossible, 136 for unclear, and 164 for premature).",
        "reference": "A Dataset for Interactive Vision-Language Navigation with Unknown Command Feasibility, \\\"Task feasibility and follow-up question breakdown\\\" Table."
    },
    {
        "paper": "arXiv-2202.02312v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the Seq2Act model perform on the MoTIF task automation challenge across different splits?",
        "answer": "The Seq2Act model achieves the highest performance among the evaluated methods on the MoTIF task automation challenge with 97.3% Action accuracy, 32.4% Ground accuracy, and 32.4% Action + Ground accuracy on the App Seen test split. For the App Unseen test split, it achieves 96.8% Action accuracy, 28.3% Ground accuracy, and 28.3% Action + Ground accuracy. The Seq2Act model shows superior performance compared to Seq2Seq and MOCA models.",
        "reference": "A Dataset for Interactive Vision-Language Navigation with Unknown Command Feasibility, \\\"Mobile app task accuracy on MoTIF\\\" Table (c) Seq2Act."
    },
    {
        "paper": "arXiv-2307.13854v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the success rate of the best-performing GPT-4-based agent in the WebArena benchmark?",
        "answer": "The best-performing GPT-4-based agent achieves an end-to-end task success rate of 14.41% in the WebArena benchmark.",
        "reference": "Title: A Realistic Web Environment for Building Autonomous Agents. Section: Abstract; \\\"our best \\\\textsc{GPT-4}-based agent only achieves an end-to-end task success rate of 14.41%.\\\""
    },
    {
        "paper": "arXiv-2307.13854v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does human performance compare to the best performing agent on the WebArena benchmark?",
        "answer": "Human performance on the WebArena benchmark is significantly better, with a success rate of 78.24%, compared to the best-performing GPT-4-based agent which has a success rate of 14.41%.",
        "reference": "Title: A Realistic Web Environment for Building Autonomous Agents. Section: Abstract; \\\"our best \\\\textsc{GPT-4}-based agent only achieves an end-to-end task success rate of 14.41\\\\%, significantly lower than the human performance of 78.24\\\\%.\\\""
    },
    {
        "paper": "arXiv-2402.17553v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the proportions of mouse and keyboard actions in the OmniACT dataset?",
        "answer": "In the OmniACT dataset, mouse actions make up 68.07% of the dataset and keyboard actions make up 31.93%. Specifically, the breakdown of the mouse actions includes 'Click' (63.73%), 'Double Click' (0.58%), 'Right Click' (0.77%), 'Move/Hover' (1.85%), 'Drag' (0.29%), 'Scroll' (1.68%), and 'Horizontal Scroll' (0.17%). Meanwhile, keyboard actions are divided into 'Press' (16.28%), 'Hotkey' (3.00%), and 'Write' (11.65%).",
        "reference": "\\\"OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web\\\", Table 1: Action types supported by OmniACT and the number of instances for each action in the dataset."
    },
    {
        "paper": "arXiv-2402.17553v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the size distribution of the dataset across different platforms in OmniACT?",
        "answer": "The OmniACT dataset comprises a total of 9802 data instances, with distribution across different platforms as follows: Mac OS (4258 instances), Linux (1134 instances), Windows (2247 instances), and Web (2163 instances). The dataset is split into Train (6789 instances), Validation (992 instances), and Test (2021 instances).",
        "reference": "\\\"OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web\\\", Table 3: Dataset distribution across splits and platforms."
    }
]