[
    {
        "paper": "arXiv-2304.06025v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the quantitative performance metrics of DreamPose compared to other methods on the UBC Fashion test set?",
        "answer": "DreamPose outperformed other methods in various metrics on the UBC Fashion test set. Specifically, it achieved an L1 error of 0.0256, SSIM of 0.885, VGG distance of 0.235, LPIPS of 0.068, FID of 13.04, FVD (16f) of 238.75, and AED of 0.0110. These results were better than the compared methods MRAA, TPSMM, and PIDM across all metrics reported.",
        "reference": "DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion, Section 6, Table 2: \\\"Quantitative comparisons of our method with MRAA, TPSMM, PIDM, and our method...\\\""
    },
    {
        "paper": "arXiv-2304.06025v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does DreamPose's training time and computational resources compare to PIDM?",
        "answer": "DreamPose is fine-tuned in 2 days using 2 A100 GPUs on a small dataset of 323 videos, whereas PIDM is trained from scratch on 101,966 Deep Fashion image pairs with 4 A100 GPUs for 26 days.",
        "reference": "DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion, Section Related Work, \\\"Moreover, by leveraging pretrained Stable Diffusion, our method can be fine-tuned in 2 days with 2 A100 GPU's using a small dataset of 323 videos. In contrast, PIDM is trained from scratch on 101,966 Deep Fashion image pairs with 4 A100s for 26 days.\\\""
    },
    {
        "paper": "arXiv-2305.03989v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the quantitative evaluation results of LEO for unconditional short video generation compared to other state-of-the-art methods on the TaichiHD128 dataset?",
        "answer": "For unconditional short video generation on the TaichiHD128 dataset, LEO achieved an FVD score of 100.4±3.1, a KVD score of 11.4±3.2, and an ACD score of 1.83. These results are better than MoCoGAN-HD and DIGAN, which had FVD scores of 144.7±6.0 and 128.1±4.9, respectively.",
        "reference": "[LEO: Generative Latent Image Animator for Human Video Synthesis], Tab.~\\\\ref{tab:fvd-cond-uncond-short}"
    },
    {
        "paper": "arXiv-2305.03989v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the performance of LEO for unconditional long-term video generation compare to other methods on the TaichiHD128 dataset?",
        "answer": "For unconditional long-term video generation on the TaichiHD128 dataset, LEO achieved an FVD score of 155.54±2.6, a KVD score of 48.82±5.9, and an ACD score of 2.06. This performance is significantly better than TATS, which had FVD and KVD values of 1194.58±1.1 and 462.03±8.2, respectively.",
        "reference": "[LEO: Generative Latent Image Animator for Human Video Synthesis], Tab.~\\\\ref{tab:fvd-uncond-long}"
    },
    {
        "paper": "arXiv-2311.17117v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the numerical values of the quantitative evaluation metrics for the \\\"Animate Anyone\\\" method on the UBC fashion video dataset?",
        "answer": "The \\\"Animate Anyone\\\" method achieves the following quantitative evaluation metrics on the UBC fashion video dataset: SSIM of 0.931, PSNR of 38.49, LPIPS of 0.044, and FVD of 81.6.",
        "reference": "\\\"Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation\\\", Table 1."
    },
    {
        "paper": "arXiv-2311.17117v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "In the \\\"Animate Anyone\\\" paper, how many character video clips are used to train the model?",
        "answer": "The model is trained on an internal dataset of 5,000 character video clips.",
        "reference": "\\\"Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation\\\", section \\\"Methods\\\"."
    }
]