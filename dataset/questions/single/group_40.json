[
    {
        "paper": "arXiv-2404.14397v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How many languages are supported by the RTP-LX dataset introduced in the paper?",
        "answer": "The RTP-LX dataset supports 28 languages.",
        "reference": "Title: RTP-LX: Can LLMs Evaluate Toxicity in Multilingual Scenarios? Section: Introduction - \\\"To address whether S/LLMs can annotate and reason over toxic, culturally-specific content, we introduce a multilingual corpus in 28 languages, RTP-LX...\\\""
    },
    {
        "paper": "arXiv-2404.14397v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the range of false positive rates observed in S/LLMs when tested on benign completions for toxicity detection using the RTP-LX dataset?",
        "answer": "The false positive rates for S/LLMs in detecting toxicity in benign completions range from around 0% to 40%.",
        "reference": "Title: RTP-LX: Can LLMs Evaluate Toxicity in Multilingual Scenarios? Section: Contributions - \\\"The models generally have a non-trivial amount of false positives, ranging anywhere from around 0% (Llama Guard) to 40% (Gemma 2B).\\\""
    },
    {
        "paper": "arXiv-2405.09373v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the total number of prompts included in the PolygloToxicityPrompts dataset introduced in the paper?",
        "answer": "The PolygloToxicityPrompts dataset, as introduced in the paper, includes a total of 425,000 prompts across 17 languages.",
        "reference": "'Multilingual Evaluation of Neural Toxic Degeneration in Large Language Models', Abstract: 'We address this by introducing PolygloToxicityPrompts, the first large-scale multilingual toxicity evaluation benchmark of 425K naturally occurring prompts spanning 17 languages.'"
    },
    {
        "paper": "arXiv-2405.09373v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the average toxicity score vary for different model types on the PolygloToxicityPrompts dataset according to the paper?",
        "answer": "According to the paper, the average toxicity (\\\\textsc{AT}) score is highest for base models (0.281) compared to instruction-tuned and preference-tuned models, where \\\\textsc{AT} is 0.02 for both (slightly higher yet almost equal).",
        "reference": "Multilingual Evaluation of Neural Toxic Degeneration in Large Language Models, Section 'Research Questions': 'We find that, on average, base models have the highest toxicity (AT=0.281)...\\\\n'"
    },
    {
        "paper": "arXiv-2412.15035v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the distribution of safety categories in the \\\\texttt{M-ALERT} dataset?",
        "answer": "The \\\\texttt{M-ALERT} dataset follows the \\\\texttt{ALERT} taxonomy which includes 6 macro categories and 32 micro categories. Each prompt-response pair is assigned a specific risk category within this taxonomy.",
        "reference": "From the paper 'LLMs Lost in Translation: \\\\texttt{M-ALERT} uncovers Cross-Linguistic Safety Gaps', the relevant section states: '\\\\texttt{M-ALERT} follows the \\\\texttt{ALERT} taxonomy with 6 macro and 32 micro categories.' (Figure 2: \\\\texttt{M-ALERT} taxonomy) and (Related Work, paragraph: \\\\texttt{ALERT} describes a taxonomy for categorizing safety risks in conversational AI use cases ... The full taxonomy entailing 6 macro and 32 micro categories is depicted in Fig.\\\\textasciitilde\\\\ref{fig:taxonomy})"
    },
    {
        "paper": "arXiv-2412.15035v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the multilingual language inconsistencies observed in the \\\\texttt{M-ALERT} dataset evaluation for Llama3.2 in specific categories?",
        "answer": "In the \\\\texttt{M-ALERT} dataset evaluation, Llama3.2 shows high unsafety in the category \\\\texttt{crime\\\\_tax} for the Italian language but remains safe in other languages. Additionally, categories such as \\\\texttt{substance\\\\_cannabis} and \\\\texttt{crime\\\\_propaganda} consistently trigger unsafe responses across models and languages.",
        "reference": "From the paper 'LLMs Lost in Translation: \\\\texttt{M-ALERT} uncovers Cross-Linguistic Safety Gaps', the relevant abstract states: 'Llama3.2 shows high unsafety in category \\\\texttt{crime\\\\_tax} for Italian but remains safe in other languages. Similar differences can be observed across all models. In contrast, certain categories, such as \\\\texttt{substance\\\\_cannabis} and \\\\texttt{crime\\\\_propaganda}, consistently trigger unsafe responses across models and languages.'"
    }
]