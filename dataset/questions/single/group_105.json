[
    {
        "paper": "arXiv-2006.01563v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the key statistics of the NER datasets used in the paper 'Exploring Cross-sentence Contexts for Named Entity Recognition with BERT'?",
        "answer": "The key statistics for the NER datasets used in the study are as follows: \\\\n- For English: Training tokens - 203,621; Development tokens - 51,362; Test tokens - 46,435. Number of entities in the training set - 23,499; in the development set - 5,942; in the test set - 5,648.\\\\n- For German: Training tokens - 206,931; Development tokens - 51,444; Test tokens - 51,943. Number of entities in the training set - 11,851; in the development set - 4,833; in the test set - 3,673.\\\\n- For Spanish: Training tokens - 264,715; Development tokens - 52,923; Test tokens - 51,533. Number of entities in the training set - 18,798; in the development set - 4,352; in the test set - 3,559.\\\\n- For Dutch: Training tokens - 202,644; Development tokens - 37,687; Test tokens - 68,875. Number of entities in the training set - 13,344; in the development set - 2,616; in the test set - 3,941.\\\\n- For Finnish: Training tokens - 342,924; Development tokens - 31,872; Test tokens - 67,425. Number of entities in the training set - 27,026; in the development set - 2,286; in the test set - 5,129.",
        "reference": "Exploring Cross-sentence Contexts for Named Entity Recognition with BERT, Section 'Data', Table 1."
    },
    {
        "paper": "arXiv-2006.01563v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the evaluation results of the Contextual Majority Voting (CMV) method on the development sets for different languages in the paper 'Exploring Cross-sentence Contexts for Named Entity Recognition with BERT'?",
        "answer": "The Contextual Majority Voting (CMV) method improved results over the method First in 281 cases out of 288 across all hyperparameter combinations during development set evaluation. Specifically, CMV outperformed method First for every hyperparameter combination in English, German, Finnish, and Spanish (both with monolingual and multilingual models). The exception was Dutch, where CMV outperformed First in 41 cases out of 48.",
        "reference": "Exploring Cross-sentence Contexts for Named Entity Recognition with BERT, Section 'Results'."
    },
    {
        "paper": "arXiv-2011.06993v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the new state-of-the-art results reported in the paper \\\"FLERT: Document-Level Features for Named Entity Recognition\\\" for the CoNLL-03 German and Dutch datasets?",
        "answer": "The new state-of-the-art F1 scores reported for the CoNLL-03 datasets are 88.34 for German and 95.19 for Dutch when trained with document features and dev data.",
        "reference": "\\\"FLERT: Document-Level Features for Named Entity Recognition\\\", Table 6, \\\"Comparative evaluation of best configurations of fine-tuning and feature-based approaches on test data\\\""
    },
    {
        "paper": "arXiv-2011.06993v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the impact of using document-level features on the named entity recognition performance for the English dataset in the paper \\\"FLERT: Document-Level Features for Named Entity Recognition\\\"?",
        "answer": "Using document-level features for the English dataset increases the F1-score from 92.79 to 93.64.",
        "reference": "\\\"FLERT: Document-Level Features for Named Entity Recognition\\\", Table 6, \\\"Comparative evaluation of best configurations of fine-tuning and feature-based approaches on test data\\\""
    },
    {
        "paper": "arXiv-2105.03654v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the size of the WNUT-16 dataset in terms of the number of training, development, and test sentences, and what is the average sentence length with and without external context?",
        "answer": "The WNUT-16 dataset consists of 2,394 training sentences, 1,000 development sentences, and 3,849 test sentences. The average sentence length without external context is 19.41, while the average length with external context is 138.58.",
        "reference": "Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning, Section 3, Table 1: \\\"\\\\textbf{\\\\textsc{WNUT-16}} & 2,394 & 1,000 & 3,849 & 10 & 19.41 & 138.58\\\""
    },
    {
        "paper": "arXiv-2105.03654v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the proposed method perform on WNUT-17 in terms of F1 score compared to the LUKE model, both with and without external context and cooperative learning?",
        "answer": "The proposed method, without context, achieves an F1 score of 57.86, and with context, it improves to 60.20. Using cooperative learning with L2 distance, the F1 score improves to 58.68 without context and 60.26 with context. Using cooperative learning with KL divergence, the F1 score further improves to 59.33 without context and 60.45 with context. In comparison, the LUKE model achieves an F1 score of 55.22 on WNUT-17.",
        "reference": "Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning, Section 3.2, Table 3: \\\"\\\\textbf{WNUT-17}\\\" and \\\"{\\\\bf\\\\textsc{LUKE}} \\\\shortcite{yamada-etal-2020-luke} & ... 55.22\\\""
    }
]