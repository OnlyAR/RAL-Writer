[
    {
        "paper": "arXiv-2201.11147v6.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the composition and size of the knowledge graph constructed for OntoProtein in the pre-training dataset?",
        "answer": "The knowledge graph constructed for OntoProtein, named ProteinKG25, consists of about 612,483 entities and 4,990,097 triples. It includes aligned node descriptions from Gene Ontology annotations, where there are 4,879,951 protein-GO triples and 110,146 GO-GO triples.",
        "reference": "OntoProtein: Protein Pretraining With Gene Ontology Embedding, Section Datasets, Pre-training Dataset: \\\"In ProteinKG25, there exists 4,990,097 triples, including 4,879,951 T_{protein-GO} and 110,146 T_{GO-GO} triples.\\\""
    },
    {
        "paper": "arXiv-2201.11147v6.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does OntoProtein perform on the secondary structure task compared to other models in the TAPE benchmark?",
        "answer": "OntoProtein achieves an accuracy of 0.82 on the SS-Q3 task and 0.68 on the SS-Q8 task, which outperforms TAPE Transformer (0.73 on SS-Q3 and 0.59 on SS-Q8) and other models like LSTM and ResNet, and it performs comparably to ProtBert (0.81 on SS-Q3 and 0.67 on SS-Q8).",
        "reference": "OntoProtein: Protein Pretraining With Gene Ontology Embedding, Section Results, TAPE Benchmark, Results: \\\"OntoProtein yields better performance in all token level tests. For the second structure (SS-Q3 and SS-Q8)...\\\" in Table 1."
    },
    {
        "paper": "arXiv-2301.12040v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the page limit for the main body of the paper in the ICML 2023 submission guidelines?",
        "answer": "The page limit for the main body of the paper is 8 pages, excluding references and appendices. Authors can add one extra page to the main body for the final version of the paper.",
        "reference": "Paper title: Submission and Formatting Instructions for International Conference on Machine Learning (ICML 2023). Section: Electronic Submission. \\\"Page limit: The main body of the paper has to be fitted to 8 pages, excluding references and appendices; the space for the latter two is not limited. For the final version of the paper, authors can add one extra page to the main body.\\\""
    },
    {
        "paper": "arXiv-2301.12040v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the minimum thickness required for lines in figures according to ICML 2023 guidelines?",
        "answer": "The minimum thickness required for lines in figures is at least 0.5 points.",
        "reference": "Paper title: Submission and Formatting Instructions for International Conference on Machine Learning (ICML 2023). Section: Figures. \\\"Lines should be dark and at least 0.5~points thick for purposes of reproduction, and text should not appear on a gray background.\\\""
    },
    {
        "paper": "arXiv-2412.20014v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the composition of \\\\ourdatasetD in terms of sample confidence levels?",
        "answer": "\\\\ourdatasetD comprises protein entries distributed across different confidence levels as follows: Conf-L1 - 0.0013, Conf-L2 - 0.0057, Conf-L3 - 0.3269, and Conf-L4 - 0.6661. It should be noted that there are no entries at Conf-L5.",
        "reference": "Function-Informed Protein Multi-Modal Learning, Section Pre-training data, Table 1: \\\"\\\\ourdatasetD & 0.0013 & 0.0057 & 0.3269 & \\\\textbf{0.6661} & 0.0000\\\""
    },
    {
        "paper": "arXiv-2412.20014v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What improvements does \\\\ourapproach achieve on the cross-modal transformation benchmarks?",
        "answer": "\\\\ourapproach achieves a remarkable average improvement of 75% across five cross-modal transformation benchmarks.",
        "reference": "Function-Informed Protein Multi-Modal Learning, Abstract: \\\"\\\\ourapproach consistently achieves SOTA performance, with remarkable improvements of 75\\\\% on average in five cross-modal transformation benchmarks\\\""
    }
]