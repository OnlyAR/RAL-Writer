[
    {
        "paper": "arXiv-2310.07849v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "In the paper \\\"Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations,\\\" how does the performance of models trained on LLM-generated synthetic data vary with task subjectivity?",
        "answer": "In the paper, it is found that models trained on LLM-generated synthetic data perform worse on tasks with high subjectivity compared to those with low subjectivity. For example, the performance decrease for BERT models trained on zero-shot synthetic data compared to real-world data is an average of 27.4% in Macro-F1 and 24.2% in accuracy for the highly subjective tasks, while the decrease is much smaller for tasks with low subjectivity.",
        "reference": "\\\"Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations,\\\" Section \\\"Evaluation I: Comparison Across Different Types of Tasks\\\", specifically in the part discussing tasks with high subjectivity."
    },
    {
        "paper": "arXiv-2310.07849v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "According to the paper \\\"Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations,\\\" what is the impact of using real-world data examples in the few-shot setting on the performance of synthetic data?",
        "answer": "The paper shows that using real-world data examples in the few-shot setting improves the effectiveness of the LLM-generated synthetic data. For BERT models, there is an average increase of 10.6% in Macro-F1 and 8.8% in accuracy when using few-shot synthetic data compared to zero-shot synthetic data.",
        "reference": "\\\"Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations,\\\" Section \\\"Evaluation I: Comparison Across Different Types of Tasks\\\", especially the paragraph discussing the impact of few-shot setting."
    },
    {
        "paper": "arXiv-2311.00287v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the proposed method, {\\\\textit{Clinical knowledge-infused} (ours)}, perform in comparison to other baseline models in the task of chemical-protein relation extraction?",
        "answer": "The proposed method, {\\\\textit{Clinical knowledge-infused} (ours)}, outperforms the baseline models in the task of chemical-protein relation extraction with an F1 score of 60.12 when using knowledge from KGs and an F1 score of 63.05 when using knowledge from LLMs. In contrast, the best baseline model, ZeroGen, achieved an F1 score of 49.42.",
        "reference": "From 'Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models' in Table 3, it shows results for the chemical-protein relation extraction task under 'F1 for ChemProt'."
    },
    {
        "paper": "arXiv-2311.00287v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the contribution of {\\\\textit{Clinical knowledge-infused prompting} (ours)} approach towards writing style diversity in the synthetic data generation process?",
        "answer": "The {\\\\textit{Clinical knowledge-infused prompting} (ours)} approach enriches the diversity of generated training instances by integrating varying writing styles dynamically during the data generation process. This improves sample diversity by introducing non-uniform styles that align more closely with the writing styles found in real datasets.",
        "reference": "From 'Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models', it mentions in Section 1, \\\"Our model involves clinical knowledge extraction and context-informed LLM prompting... enriching the diversity of generated training instances.\\\""
    },
    {
        "paper": "arXiv-2403.01081v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the LAB-trained models compare to other models in terms of MT-Bench performance?",
        "answer": "The LAB-trained models, specifically \\\\textsc{\\\\labl} and \\\\textsc{\\\\labm}, performed exceptionally well in MT-Bench. \\\\textsc{\\\\labl} achieved a score of 7.23, which is slightly higher than \\\\textsc{WizardLM-13B-V1.2}, which scored 7.20. Meanwhile, \\\\textsc{\\\\labm} outperformed other models based on \\\\textsc{Mistral-7B}, scoring 7.66, which is higher than \\\\textsc{Zephyr-7b-\\\\beta}, which scored 7.34.",
        "reference": "\\\"LAB: Large-Scale Alignment for ChatBots\\\" - Section: Results, Table: Evaluation of LLMs."
    },
    {
        "paper": "arXiv-2403.01081v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the composition of the dataset used in the LAB method?",
        "answer": "The dataset used in LAB method consists of 1.2 million samples, which are almost evenly divided between knowledge-based samples (617k) and skill-based samples (588k).",
        "reference": "\\\"LAB: Large-Scale Alignment for ChatBots\\\" - Section: Methodology, \\\"During the synthetic data generation phase, we employed a taxonomy consisting of numerous leaf nodes to produce a dataset comprising 1.2 million samples, divided almost evenly between knowledge-based (617k) and skill-based (588k) samples.\\\""
    }
]