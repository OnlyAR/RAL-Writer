[
    {
        "paper": "arXiv-2101.02235v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the average number of paragraphs per question in the \\\\strategyqa{} dataset?",
        "answer": "The average number of paragraphs per question in the \\\\strategyqa{} dataset is 2.33 for the training set and 2.29 for the test set.",
        "reference": "Did Aristotle Use a Laptop?, Section 5, Table 1 \\\"The average number of paragraphs per question is 2.33 for the training set and 2.29 for the test set.\\\""
    },
    {
        "paper": "arXiv-2101.02235v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the distribution of implicit and multi-step questions in a sample of the \\\\strategyqa{} dataset?",
        "answer": "In a sample of the \\\\strategyqa{} dataset, 81% of the questions are multi-step and implicit, 14.5% are multi-step and explicit, 3.5% are single-step and explicit, and 1% are single-step and implicit.",
        "reference": "Did Aristotle Use a Laptop?, Section 6.2, Table 2 \\\"Distribution over the implicit and multi-step properties in a sample of 100 \\\\strategyqa{} questions.\\\""
    },
    {
        "paper": "arXiv-2109.01653v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the composition and size of the CREAK dataset introduced in the paper \\\"CREAK: A Dataset for Commonsense Reasoning over Entity Knowledge\\\"?",
        "answer": "The CREAK dataset consists of 13,000 human-authored English claims about 2,700 different entities. Each claim is labeled as either true or false. The dataset includes a small contrast set as well. The train set comprises 10,176 claims, the dev set has 1,371 claims, the test set contains 1,371 claims, and the contrast test set consists of 200 claims. Additionally, during annotation, the claims were written by 684 distinct crowdworkers.",
        "reference": "\\\"CREAK: A Dataset for Commonsense Reasoning over Entity Knowledge,\\\" Dataset section: \\\"Our dataset consists of 13k English claims covering 2.7k entities...\\\" and Table 1"
    },
    {
        "paper": "arXiv-2109.01653v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the human performance compared to the best model on the CREAK dataset as described in the paper \\\"CREAK: A Dataset for Commonsense Reasoning over Entity Knowledge\\\"?",
        "answer": "The estimated human performance on the CREAK dataset is an average accuracy of 96.3% on the development set and 99.0% for the majority label on the test. Comparatively, the best-performing model in the closed-book setting, T5-3b, achieved 85.1% accuracy on the test set and 70.0% on the contrast set. Meanwhile, in the open-book setting, the best-performing retrieval-augmented model, RoBERTa (Large+DPR), achieved 86.8% accuracy on the test set and 72.0% on the contrast set.",
        "reference": "\\\"CREAK: A Dataset for Commonsense Reasoning over Entity Knowledge,\\\" Results and Discussion section and Tables 4 & 5: \\\"...we estimate human accuracy on the development and test sets is around 96\\\".\\\" and Tables 4 & 5"
    },
    {
        "paper": "arXiv-2301.13688v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the average accuracy of Flan-T5 XL in zero-shot and few-shot settings on Held-In tasks, as reported in the paper titled 'The Flan Collection: Designing Data and Methods for Effective Instruction Tuning'?",
        "answer": "The average accuracy of Flan-T5 XL on Held-In tasks in zero-shot and few-shot settings are 73.8% and 74.8% respectively.",
        "reference": "'The Flan Collection: Designing Data and Methods for Effective Instruction Tuning', Section: Flan 2022 Instruction Tuning Experiments, Table labeled 'Method Ablations'. \\\"Metrics are reported in both zero-shot / few-shot settings across Held-In, Chain-of-Thought, and Held-Out (MMLU, BBH) tasks.\\\""
    },
    {
        "paper": "arXiv-2301.13688v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What was the improvement in average accuracy for Flan-T5 XL over T5-XL Flan 2021 on the Held-Out MMLU evaluation benchmark, as reported in the paper titled 'The Flan Collection: Designing Data and Methods for Effective Instruction Tuning'?",
        "answer": "Flan-T5 XL showed an improvement of +4.2% in zero-shot and +17.6% in few-shot settings over T5-XL Flan 2021 on the Held-Out MMLU evaluation benchmark.",
        "reference": "'The Flan Collection: Designing Data and Methods for Effective Instruction Tuning', Section: Flan 2022 Instruction Tuning Experiments, Table labeled 'Method Ablations'. \\\"\\\\textbf{\\\\flantwo{} - Next Best T5-XL} shows the improvement of Flan-T5 XL over the next best T5-XL (comparatively sized) finetuned on another collection.\\\""
    }
]