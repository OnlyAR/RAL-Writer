[
    {
        "paper": "arXiv-2302.01925v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the perplexity score of the FLT (local RPE) compared to other models evaluated in the language modeling task?",
        "answer": "The FLT (local RPE) model achieved a perplexity score of 30.1, which is lower than all other evaluated models, including Performer (31.1), Performer-sineSPE (38.0), Performer-convSPE (37.8), and Log-linear Performer (30.6).",
        "reference": "\\\"Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers\\\", Table 1 - Section: Language Modeling Results."
    },
    {
        "paper": "arXiv-2302.01925v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the proposed FLT model perform in the OC20 IS2RE task in terms of energy MAE compared to the regular Performer model?",
        "answer": "The proposed FLT model (12 layers) achieved an energy MAE of 0.5046, which is lower than the regular Performer model's energy MAE of 0.5454.",
        "reference": "\\\"Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers\\\", Table 3 - Section: Molecular Property Prediction Results."
    },
    {
        "paper": "arXiv-2404.12224v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What temperature scaling factor for NoPE allows it to generalize to over 4K tokens?",
        "answer": "By scaling the attention score by a factor of 1.2, NoPE can immediately generalize to over 4K tokens.",
        "reference": "\\\"Length Generalization of Causal Transformers without Position Encoding,\\\" Section Introduction: \\\"For example, by scaling the attention score by a factor of $1.2$, NoPE can immediately generalize to over $4$K tokens (Figure \\\\ref{fig:vis_entro})\\\"."
    },
    {
        "paper": "arXiv-2404.12224v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does NoPE perform in terms of log-perplexity compared to RoPE on sequences longer than the training length?",
        "answer": "NoPE's length generalization error (light blue dashed line, measured with log-perplexity) is lower than RoPE (light red dashed line) when testing sequence length exceeds the training length.",
        "reference": "\\\"Length Generalization of Causal Transformers without Position Encoding,\\\" Section Length Generalization: \\\"NoPE's length generalization error (light blue dashed line, measured with log-perplexity) is lower than RoPE (light red dashed line).\\\""
    },
    {
        "paper": "arXiv-2412.17739v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What performance advantage does FoPE demonstrate over RoPE and ALiBi in terms of accuracy in the Passkey Retrieval task, according to the paper 'Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization'?",
        "answer": "FoPE demonstrates a significant advantage over RoPE and ALiBi in the Passkey Retrieval task by maintaining stable retrieval accuracy across different context lengths, while RoPE's accuracy drops sharply to zero at twice the training length, and ALiBi shows a linear decline in accuracy. This highlights FoPE's strong ability to extract information from long sequences.",
        "reference": "The paper 'Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization', under the section 'Length Generalization after Pre-Training', states: 'In this task, FoPE demonstrates a significant advantage over both RoPE and ALiBi. RoPEâ€™s accuracy drops sharply to zero at twice the training length and remains at zero for longer sequences. ALiBi shows a linear decline in accuracy ... FoPE maintains stable retrieval accuracy for passkeys at any position, demonstrating a strong ability to extract subtle information from long sequences.'"
    },
    {
        "paper": "arXiv-2412.17739v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does FoPE perform in length extrapolation tasks compared to RoPE+YARN, as reported in the paper 'Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization'?",
        "answer": "In length extrapolation tasks, FoPE combined with YARN significantly outperforms RoPE+YARN, showcasing better length generalization performance with lower perplexity on the C4 dataset and higher accuracy in the Passkey Retrieval task.",
        "reference": "As stated in 'Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization', section 'Length Generalization after Fine-Tuning': 'Compared to RoPE+YARN, FoPE+YARN achieves significantly better length generalization performance, as demonstrated by lower perplexity on the C4 dataset and higher accuracy in the Passkey Retrieval task...This validates FoPE as a practical solution for enhancing RoPE-based open-source models.'"
    }
]