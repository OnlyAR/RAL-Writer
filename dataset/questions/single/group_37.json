[
    {
        "paper": "arXiv-2305.11627v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How much performance does the pruned model maintain compared to the original model as per LLM-Pruner's evaluation on LLaMA-7B?",
        "answer": "The pruned model maintains 94.97% of the performance of the original model after removing 20% of parameters and performing efficient post-training.",
        "reference": "From the paper titled \\\"LLM-Pruner: On the Structural Pruning of Large Language Models\\\", it is stated in the Introduction section that \\\"Experiments show that with 20% parameters removed, the model can maintain 93.6% of the performance of the original model.\\\""
    },
    {
        "paper": "arXiv-2305.11627v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the parameter counts and latency of the 7B models after pruning 50% of the parameters using LLM-Pruner's block and channel strategies?",
        "answer": "After pruning 50% of the parameters, the parameter count is 3.35B with 37.54s latency for the block strategy, and 3.37B with 40.11s latency for the channel strategy.",
        "reference": "In the paper \\\"LLM-Pruner: On the Structural Pruning of Large Language Models\\\", Table 3 shows the statistics: \\\"\\\\begin{tabular}{c|cc|ccccc}\\\\multirow{5}{*}{\\\\parbox{1.8cm}{LLaMA-7B Vicuna-7B}} & - & - & 6.74B & 424.02G & 12884.5MiB & 69.32s \\\\\\\\ & \\\\channelname & 50\\\\% & 3.37B & 212.58G & 6556.3MiB & 40.11s \\\\\\\\ & \\\\blockname & 50\\\\% & 3.35B &  206.59G & 6533.9MiB & 37.54s\\\\\\\\\\\"."
    },
    {
        "paper": "arXiv-2402.02834v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the pruning ratio and number of parameters for the pruned Vicuna-7B model using the proposed depth pruning method in the \\\"Shortened LLaMA: Depth Pruning for Large Language Models with Comparison of Retraining Methods\\\" paper?",
        "answer": "The depth pruning method prunes the Vicuna-7B model to have 4.5B parameters, which corresponds to a pruning ratio of 35%.",
        "reference": "\\\"Shortened LLaMA: Depth Pruning for Large Language Models with Comparison of Retraining Methods\\\", Table 2: \\\"4.5B (35%Pruned)\\\"."
    },
    {
        "paper": "arXiv-2402.02834v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the continued pretraining (CPT) strategy affect the perplexity on the WikiText2 dataset for a heavily pruned model in the \\\"Shortened LLaMA: Depth Pruning for Large Language Models with Comparison of Retraining Methods\\\" paper?",
        "answer": "Continued pretraining (CPT) of a heavily pruned model (1.5B parameters, 80% pruning) achieves a perplexity of 20.5 on the WikiText2 dataset.",
        "reference": "\\\"Shortened LLaMA: Depth Pruning for Large Language Models with Comparison of Retraining Methods\\\", Table 5: \\\"Ours, CPT\\\" under \\\"1.5B\\\" column."
    },
    {
        "paper": "arXiv-2403.03853v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What performance drop was observed for ShortGPT when pruning the LLaMA 2-13B model by removing 10 layers?",
        "answer": "The performance drop observed for ShortGPT when pruning the LLaMA 2-13B model by removing 10 layers was a decrease in the MMLU benchmark score from 55.0 to 52.2.",
        "reference": "ShortGPT: Layers in Large Language Models are More Redundant Than You Expect, Section Evaluation ('removing 10 layers (25\\\\% of the total 40 layers) from the LLaMA 2-13B model resulted in only a slight drop in performance on the MMLU benchmark, from 55.0 to 52.2.')"
    },
    {
        "paper": "arXiv-2403.03853v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the ShortGPT method perform compared to other methods on the LLaMA2-13B model in terms of average performance retention?",
        "answer": "ShortGPT method maintains 91.64% average performance retention when compared to the dense model, which outperforms other methods like LLMPrun. (70.67%), SliceGPT (63.20%), and LaCo (86.36%).",
        "reference": "ShortGPT: Layers in Large Language Models are More Redundant Than You Expect, Table 2 ('ShortGPT method maintains 91.64% performance retention compared to LLMPrun. (70.67%), SliceGPT (63.20%), and LaCo (86.36%) for LLaMA2-13B model.')"
    }
]