[
    {
        "paper": "arXiv-2211.06687v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the dataset composition and size details of the LAION-Audio-630K presented in the paper \\\"Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation\\\"?",
        "answer": "The paper presents the dataset LAION-Audio-630K, which consists of 633,526 audio-text pairs with a total duration of 4,325.39 hours. The dataset includes audios of human activities, natural sounds, and audio effects, collected from 8 different publicly available data sources.",
        "reference": "\\\"Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation\\\", Section 2: \\\"We collect LAION-Audio-630K, a large-scale audio-text dataset consisting of 633,526 pairs with the total duration of 4,325.39 hours. It contains audios of human activities, natural sounds and audio effects, consisting of 8 data sources from publicly available websites.\\\""
    },
    {
        "paper": "arXiv-2211.06687v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the proposed model in the paper \\\"Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation\\\" perform in the text-to-audio retrieval task compared to previous methods?",
        "answer": "The proposed model HTSAT-RoBERTa achieves superior performance in text-to-audio retrieval tasks, outperforming previous methods on most metrics. For AudioCaps, it achieves R@1=36.7% while for Clotho, it gets R@1=18.2%. It uses large-scale datasets including LAION-Audio-630K and AudioSet with keyword-to-caption augmentation, and feature fusion to achieve such improvements.",
        "reference": "\\\"Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation\\\", Section 5.2: \\\"As the result, our best model outperforms previous methods on most metrics (mainly R@1=36.7% on AudioCaps and R@1=18.2% on Clotho) in the text-to-audio retrieval tasks. We show that training on large-scale datasets (LAION-Audio-630K and AudioSet with keyword-to caption augmentation), and feature fusion can effectively improve model performance.\\\""
    },
    {
        "paper": "arXiv-2308.11923v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the composition of the AudioDiffCaps dataset, introduced in 'Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement'?",
        "answer": "The AudioDiffCaps dataset consists of pairs of similar but slightly different audio clips (5996 in development and 1720 in evaluation set) and human-annotated descriptions (28,892 in development and 8600 in evaluation). Each audio clip is 10 seconds long, synthesized using six foreground event classes and specific background classes from the FSD50K and ESC-50 datasets.",
        "reference": "Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement, 'AudioDiffCaps dataset' section: 'The development and evaluation sets contained 5996 and 1720 audio clip pairs, respectively. ... Each pair of audio clips in the development set had between 1 and 5 descriptions (a total of 28,892) while each pair in the evaluation set had exactly five descriptions assigned to it (a total of 8600).'"
    },
    {
        "paper": "arXiv-2308.11923v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What were the results of using the CAC transformer combined with the Late SDD on the AudioDiffCaps dataset as presented in the paper 'Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement'?",
        "answer": "The CAC transformer combined with Late SDD achieved high scores: BLEU-1 up to 70.3, BLEU-4 up to 39.5, METEOR up to 26.4, ROUGE-L up to 61.6, CIDEr up to 97.6, SPICE up to 22.6, and SPIDEr up to 59.9, demonstrating significant improvements over the baseline.",
        "reference": "Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement, 'Results' section: 'the SDD loss function was effective for the audio difference captioning task. ... the best scores in all evaluation metrics with late disentanglement.' and Table 1."
    },
    {
        "paper": "arXiv-2312.00249v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the performance (in terms of SPICE) of the APT-LLM on the Clotho dataset for the audio captioning task as reported in the paper \\\"Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities\\\"?",
        "answer": "APT-LLM achieves a SPICE score of 0.132 on the Clotho dataset for the audio captioning task.",
        "reference": "Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities, Section \\\"Comparison with existing approaches\\\", Table 3."
    },
    {
        "paper": "arXiv-2312.00249v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the accuracy of the APT-Vicuna v1.5 model on the natural language audio reasoning task as reported in the paper \\\"Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities\\\"?",
        "answer": "APT-Vicuna v1.5 achieves an accuracy of 63.8% on the natural language audio reasoning task.",
        "reference": "Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities, Section \\\"Evaluation on natural language audio reasoning\\\", Table 5."
    }
]