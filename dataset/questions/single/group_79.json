[
    {
        "paper": "arXiv-2108.12409v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the performance improvement of ALiBi over sinusoidal position embeddings when both are trained on sequences of length 3072 and evaluated on the same sequence length on the WikiText-103 dataset?",
        "answer": "The ALiBi model achieves a perplexity of 17.60, while the sinusoidal position embeddings model achieves a perplexity of 18.67±0.24 when both are trained and evaluated on sequences of length 3072 on the WikiText-103 dataset.",
        "reference": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation, Section 4.1, \\\"...the Baevski \\\\& Auli model achieves 18.67±0.24 (std. dev.) perplexity...our $L=3072$ model achieves 17.60 perplexity.\\\""
    },
    {
        "paper": "arXiv-2108.12409v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How much faster is the ALiBi method in comparison to the sinusoidal method when training on the CC100+RoBERTa dataset with a sequence length of 512 and evaluating on sequence length 1024?",
        "answer": "The ALiBi model is 7% faster when trained on sequence lengths of 512 compared to the sinusoidal model trained on sequence lengths of 1024 on the CC100+RoBERTa dataset.",
        "reference": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation, Section 4.2, \\\"Since our model is trained on shorter sequences, it is 7\\\\% faster and uses 1.6 GB less memory.\\\""
    },
    {
        "paper": "arXiv-2205.09921v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the difference in perplexity performance between the logarithmic variant of KERPLE and T5 on the OpenWebText2 dataset at the extrapolation length of 16384?",
        "answer": "On the OpenWebText2 dataset at the extrapolation length of 16384, the perplexity of the logarithmic variant of KERPLE is 21.4 ± 0.6, while for T5, it is 31.4 ± 3.1.",
        "reference": "\\\"KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation,\\\" Table 3 (caption: \\\"Perplexity Comparison on OpenWebText2...\\\")"
    },
    {
        "paper": "arXiv-2205.09921v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How much faster in terms of calculation speed is the logarithmic variant of KERPLE compared to the T5 positional embedding on the GitHub dataset?",
        "answer": "The logarithmic variant of KERPLE has a training speed of 0.307 seconds per step, whereas T5 has a training speed of 0.340 seconds per step. Therefore, the logarithmic variant of KERPLE is approximately 9.7% faster than T5 on the GitHub dataset.",
        "reference": "\\\"KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation,\\\" Table 4 (caption: \\\"Training Time Comparison on GitHub\\\")"
    },
    {
        "paper": "arXiv-2310.04418v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the decay rate of attention between two tokens as the sequence length increases in the method proposed in 'Long seq attention'?",
        "answer": "The attention decays as $1/n$ with sequence length $n$.",
        "reference": "\\\"Long seq attention\\\", Section - 'Attention for Long sequence lengths': \\\"This shows that attention decays as $1/n$ with sequence length $n.\\\""
    },
    {
        "paper": "arXiv-2310.04418v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the standard width used for the hidden layers of the MLP in the MLPRPE method in the paper 'Long seq attention'?",
        "answer": "The width $d$ is set to 32 unless specified otherwise.",
        "reference": "\\\"Long seq attention\\\", Section - 'MLP RPE': \\\"The width $d$ is set to 32 unless specified otherwise.\\\""
    }
]