[
    {
        "paper": "arXiv-2210.02414v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the zero-shot performance of GLM-130B on the English language modeling benchmark LAMBADA, and how does it compare to other large language models like GPT-3 and PaLM?",
        "answer": "The zero-shot performance of GLM-130B on the LAMBADA benchmark is 80.2% accuracy. This sets a new record as it outperforms GPT-3 and the 540B PaLM model. In comparison, the GPT-3 achieves 76.2% accuracy, and PaLM 540B also shows lower performance than GLM-130B.",
        "reference": "Title: GLM-130B: An Open Bilingual Pre-Trained Model. Section: Language Modeling.\\\\n\\\"Empirically, \\\\\\\\glm %with bidirectional attention offers a record-high accuracy of 80.2\\\\% on zero-shot LAMBADA by outperforming both GPT-3 and PaLM 540B in Figure \\\\ref{fig:lambada}.\\\""
    },
    {
        "paper": "arXiv-2210.02414v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does GLM-130B perform on the MMLU benchmark in terms of few-shot learning, and how does its performance change throughout the training process?",
        "answer": "GLM-130B's few-shot (5-shot) performance on the MMLU benchmark starts by approaching the performance of GPT-3 (43.9% accuracy) after processing around 300 billion tokens. As the training progresses to 400 billion tokens, its performance improves to an accuracy of 44.8%. This suggests that GLM-130B's performance continues to increase as it is exposed to more training tokens.",
        "reference": "Title: GLM-130B: An Open Bilingual Pre-Trained Model. Section: Massive Multitask Language Understanding (MMLU). \\\\n\\\"\\\\\\\\glm's few-shot (5-shot) performance on MMLU approaches GPT-3 (43.9) after viewing about 300B tokens in Figure\\\\ref{fig:mmlu}. It continues moving up as the training proceeds, achieving an accuracy of 44.8 when the training has to end (i.e., viewing 400B tokens in total).\\\""
    },
    {
        "paper": "arXiv-2308.09583v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the performance results of \\\\\\\\modelname{}-Mistral 7B on the GSM8k and MATH benchmarks compared to MetaMath-Mistral 7B?",
        "answer": "The \\\\\\\\modelname{}-Mistral 7B model observed a substantial improvement in pass@1 with an increase of +12.8 (90.7 vs. 77.9) on GSM8k, and +26.8 (55.4 vs. 28.6) on MATH compared to MetaMath-Mistral 7B.",
        "reference": "Title: \\\\modelname{}: Empowering Mathematical Reasoning for Large Language Models via \\\\REIname{}. \\\\nSection: Abstract\\\\n\\\"Specifically, \\\\modelname{}-Mistral-7B observed a substantial improvement in pass@1 with an increase of +12.8 (90.7. vs. 77.9) on GSM8k, and +26.8 (55.4 vs. 28.6) on MATH compared to MetaMath-Mistral-7B.\\\""
    },
    {
        "paper": "arXiv-2308.09583v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does \\\\modelname{}-70B compare to metaMath-70B on GSM8k and MATH benchmarks in terms of testing accuracy?",
        "answer": "\\\\modelname{}-70B achieves a substantial improvement over MetaMath-70B by a significant margin, with accuracy of 92.8 vs. 82.3 on GSM8k and 58.6 vs. 26.6 on MATH benchmarks.",
        "reference": "Title: \\\\modelname{}: Empowering Mathematical Reasoning for Large Language Models via \\\\REIname{}. \\\\n Section: Experiment\\\\n \\\"Specifically, \\\\modelname{}-70B significantly outperforms MetaMath-70B by a significant margin on GSM8k (92.8 vs. 82.3) and on MATH (58.6 vs. 26.6).\\\""
    },
    {
        "paper": "arXiv-2309.12284v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the performance improvements of MetaMath-7B on the GSM8K and MATH benchmarks compared to the state-of-the-art models of the same size?",
        "answer": "MetaMath-7B achieves an accuracy of 66.5% on GSM8K and 19.8% on MATH, which exceeds the state-of-the-art models of the same size by 11.5% on GSM8K and 8.7% on MATH.",
        "reference": "Title: MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models, Abstract: \\\"Our MetaMath-7B model achieves $66.5\\\\%$ on GSM8K and $19.8\\\\%$ on MATH, exceeding the state-of-the-art models of the same size by $11.5\\\\%$ and $8.7\\\\%$.\\\""
    },
    {
        "paper": "arXiv-2309.12284v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How many samples are included in the MetaMathQA dataset, and what are the proportions of different types of augmentations?",
        "answer": "The MetaMathQA dataset contains a total of 395K samples. It includes 155K samples from answer augmentation (AnsAug), 130K from rephrasing, 55K from Self-Verification (SV), and 55K from FOBAR.",
        "reference": "Title: MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models, Section: Experimental Setup, \\\"\\\\nMetaMathQA-GSM8K & 80K & 80K & 40K & 40K & 240K \\\\\\\\\\\\nMetaMathQA-MATH  & 75K & 50K & 15K & 15K & 155K \\\\\\\\\\\\nMetaMathQA  & 155K  & 130K  & 55K  & 55K & 395K \\\\\\\\"
    }
]