[
    {
        "paper": "arXiv-2405.15863v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the size and composition of the training dataset used in the QA-MDT model as presented in the paper 'QA-MDT: Quality-aware Masked Diffusion Transformer for Enhanced Music Generation'?",
        "answer": "The training dataset used in the QA-MDT model comprises a total of 12.5k hours of diverse music data. It includes the following databases: AudioSet Music Subset (ASM), MagnaTagTune (MTT), Million Song Dataset (MSD), Free Music Archive (FMA), and an additional dataset from Pixabay, with each track being 10-second segments sampled at 16kHz.",
        "reference": "Reference: 'QA-MDT: Quality-aware Masked Diffusion Transformer for Enhanced Music Generation', Section 8.1: 'The final training set was developed through a process of caption refinement, as detailed in Section~\\\\ref{sec:audio_tokenization}. Finally, we got our training set totaling 12.5k hours of diverse music data.'"
    },
    {
        "paper": "arXiv-2405.15863v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the QA-MDT model perform in terms of the Fréchet Audio Distance (FAD) on the MusicCaps benchmark compared to previous methods?",
        "answer": "On the MusicCaps benchmark, the QA-MDT model achieves a Fréchet Audio Distance (FAD) of 1.65, which is the best among the compared methods. For reference, MusicGen achieves a FAD of 3.80, AudioLDM 2 - Full achieves a FAD of 3.13, and the baseline model using a U-Net backbone obtains a FAD of 2.03.",
        "reference": "Reference: 'QA-MDT: Quality-aware Masked Diffusion Transformer for Enhanced Music Generation', Section 10, Table 5: 'Ours (QA-MDT) & 675M & 12.5k & \\\\textbf{1.65}'"
    },
    {
        "paper": "arXiv-2406.04673v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the relative improvement in FAD score achieved by \\\\modelname compared to MusicGen on the \\\\ourdataset dataset?",
        "answer": "\\\\modelname achieves a relative improvement of 67.98% in FAD score compared to MusicGen on the \\\\ourdataset dataset.",
        "reference": "\\\"Our exhaustive experimental results reveal that our approach outperforms existing text-to-music generation pipelines on both subjective as well as objective evaluation with a relative gain of up to \\\\textbf{67.98\\\\%} on FAD score, thereby setting a new benchmark for multi-modal music synthesis.\\\" (In the Abstract section)"
    },
    {
        "paper": "arXiv-2406.04673v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How many samples are contained in \\\\ourdataset and what modalities are included?",
        "answer": "\\\\ourdataset contains 11,250 samples, with each sample comprising three modalities: image, text, and music.",
        "reference": "\\\"We summarize our main contributions... We introduce \\\\ourdataset dataset comprising \\\\ourdatasetsize $\\\\langle \\\\text{image}, \\\\text{text}, \\\\text{music} \\\\rangle$ triplets. To the best of our knowledge, this is the largest collection of these three modalities.\\\" (In the Introduction section under point (3))"
    },
    {
        "paper": "arXiv-2409.00587v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the size of the training dataset used for FluxMusic, and how was it compiled?",
        "answer": "The training dataset for FluxMusic totals approximately 22K hours of diverse music content. It was compiled using several datasets, including AudioSet Music Subset (ASM), MagnaTagTune, Million Song Dataset (MSD), MagnaTagTune (MTT), Free Music Archive (FMA), Music4All, and an additional private dataset. Audio tracks were segmented into 10-second clips and uniformly sampled at 16 kHz. Additionally, detailed captions were sourced from Hugging Face datasets, and the remaining music data was automatically labeled using LP-MusicCaps models.",
        "reference": "Paper Title: FLUX that Plays Music. Section: Experiments, paragraph on Datasets."
    },
    {
        "paper": "arXiv-2409.00587v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the performance of FluxMusic compare to other models like MusicGen and AudioLDM 2 based on the Fréchet Audio Distance (FAD) on the MusicCaps dataset?",
        "answer": "FluxMusic achieves a Fréchet Audio Distance (FAD) of 1.43 on the MusicCaps dataset, which is better than MusicGen's FAD of 3.80 and AudioLDM 2's (Music version) FAD of 4.04.",
        "reference": "Paper Title: FLUX that Plays Music. Section: Compared with Previous Methods, Table 5."
    }
]