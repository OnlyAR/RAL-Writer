[
    {
        "paper": "arXiv-2309.05463v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the performance results of the phi-1.5 model on common sense reasoning benchmarks compared to other models?",
        "answer": "The phi-1.5 model achieves comparable results to Llama2-7B, Falcon-7B, and Vicuna-13B on nearly all common sense reasoning benchmarks. Specifically, phi-1.5 has the following performance: WinoGrande - 0.734, ARC-Easy - 0.756, ARC-Challenge - 0.444, BoolQ - 0.758, SIQA - 0.526. These scores demonstrate that phi-1.5 is competitive with models that are up to 10 times larger than it is.",
        "reference": "\\\"Textbooks Are All You Need II: phi-1.5 technical report\\\" - Section 'Benchmark results'. Table: Common Sense Reasoning Benchmarks."
    },
    {
        "paper": "arXiv-2309.05463v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the training data size and parameter count of phi-1.5 compare to Llama-7B?",
        "answer": "The phi-1.5 model has 1.3 billion parameters and is trained on a dataset of approximately 30 billion tokens. In contrast, Llama-7B has 7 billion parameters and was trained on 1 trillion tokens, indicating that Llama-7B is significantly larger in both parameter count and training data size than phi-1.5.",
        "reference": "\\\"Textbooks Are All You Need II: phi-1.5 technical report\\\" - Section 'Introduction'. Table showing comparison of compute of different models."
    },
    {
        "paper": "arXiv-2404.14219v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the empirical results of the phi-3-mini model on academic benchmarks compared to other models such as Mixtral 8x7B and GPT-3.5?",
        "answer": "The phi-3-mini model achieves empirical results of 69% on MMLU and 8.38 on MT-bench. It shows competitive performance compared to Mixtral 8x7B, which scores 70.5% on MMLU, and GPT-3.5, which scores 71.4% on MMLU. On MT-bench, GPT-3.5 scores 8.35, very similar to phi-3-mini.",
        "reference": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone, Abstract and Academic Benchmarks section."
    },
    {
        "paper": "arXiv-2404.14219v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the parameter scaling of phi-3 models affect their performance on MMLU and MT-bench tests?",
        "answer": "The parameter scaling in phi-3 models significantly affects their performance. The phi-3-small (7B) achieves 75% on MMLU and 8.7 on MT-bench, while phi-3-medium (14B) achieves 78% on MMLU and 8.9 on MT-bench. This is a notable improvement from phi-3-mini's performance of 69% on MMLU and 8.38 on MT-bench, indicating that increasing the parameters enhances the model's performance on these benchmarks.",
        "reference": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone, Abstract and Academic Benchmarks section."
    },
    {
        "paper": "arXiv-2412.08905v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the composition of the synthetic data used for pretraining the Phi-4 model, and how does it compare to previous Phi models?",
        "answer": "The synthetic data for pretraining the Phi-4 model includes a total of about 400 billion unweighted tokens and is generated from 50 broad types of datasets. These datasets rely on diverse sources and multi-stage prompting procedures. In contrast to previous Phi models, Phi-4 increases the allocation of synthetic tokens as opposed to relying heavily on filtered web data.",
        "reference": "Phi-4 Technical Report, Section 'Synthetic Data for Pretraining and Midtraining': \\\"We created 50 broad types of synthetic datasets, each one relying on a different set of seeds and different multi-stage prompting procedures, spanning an array of topics, skills, and natures of interaction, accumulating to a total of about 400B unweighted tokens.\\\""
    },
    {
        "paper": "arXiv-2412.08905v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the Phi-4 model perform on the MATH benchmark compared to its teacher model, GPT-4o?",
        "answer": "The Phi-4 model performs better on the MATH benchmark compared to its teacher model, GPT-4o, with a score of 80.4 versus 74.6.",
        "reference": "Phi-4 Technical Report, Table 'Performance of \\\\model{} on a set of standard benchmarks.'"
    }
]