[
    {
        "paper": "arXiv-2303.09859v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the size composition of the British National Corpus as used in this study?",
        "answer": "The British National Corpus in this study consists of approximately 115,870,549 words, 8,501,376 sentences, and 4,014 documents. The development split contains 1,215,306 words, 106,566 sentences, and 35 documents. The written portion of the corpus makes up approximately 90% of the total corpus, while the remaining 10% consists of transcribed speech.",
        "reference": "Title: 'Trained on 100 million words and still in shape: BERT meets British National Corpus', Section: 'British National Corpus'. Direct quote: 'train & 4,014 & 8,501,376 & 115,870,549 & 131,392,103.' and 'The written part makes up approximately 90% of the corpus and the remaining 10% contains the transcribed speech.'"
    },
    {
        "paper": "arXiv-2303.09859v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "According to the LTG-BERT experiments, how does the performance of span masking compare to subword and whole-word masking on the GLUE benchmark?",
        "answer": "The span masking variant of LTG-BERT achieved superior performance on the GLUE benchmark compared to subword and whole-word masking. Specifically, LTG-BERT with span masking achieved an average GLUE score of 89.2, outperforming subword masking (88.2) and whole-word masking (88.4).",
        "reference": "Title: 'Trained on 100 million words and still in shape: BERT meets British National Corpus', Section: 'Training objective comparison'. Direct quote: 'Overall, the span-based masking performs marginally better than the other methods -- it shows a clear improvement on (Super)GLUE benchmarks over the simple subword masking.'"
    },
    {
        "paper": "arXiv-2311.02265v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How did the ELC-BERT model perform in the BabyLM challenge strict-small track compared to other models?",
        "answer": "In the BabyLM challenge strict-small track (10M words), the ELC-BERT model achieved the highest average score of 65.9, outperforming other models such as MLSM (60.8 average), Contextualizer (60.5 average), Baby Llama (60.1 average), and Too Much Information (59.9 average). ELC-BERT also scored highest on BLiMP (75.8), GLUE (73.7), and MSGS (29.4) benchmarks among these models.",
        "reference": "\\\"Not all layers are equally as important: Every Layer Counts BERT\\\", Table 1, Section: BabyLM challenge evaluation, \\\"The DynaBench scores of the BabyLM challenge, the table shows the top 5 submissions in the strict-small and strict tracks. Higher scores are better, the best results in each evaluation suite are boldfaced.\\\""
    },
    {
        "paper": "arXiv-2311.02265v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the differences in performance between the ELC-BERT and LTG-BERT base models on the \\\\textsc{strict} track datasets in the BabyLM challenge?",
        "answer": "On the \\\\textsc{strict} track (100M words) of the BabyLM challenge, ELC-BERT base scored 85.3 on BLiMP, 76.6 on the Supplemental dataset, -0.26 on MSGS, and 78.3 on GLUE. In contrast, LTG-BERT base scored 85.8 on BLiMP, 76.8 on the Supplemental dataset, -0.42 on MSGS, and 77.9 on GLUE. ELC-BERT performed marginally worse on BLiMP and the Supplemental datasets but scored better on MSGS and GLUE.",
        "reference": "\\\"Not all layers are equally as important: Every Layer Counts BERT\\\", Table 2, Section: Results, \\\"We compare the results of our submitted model (ELC-BERT\\\\textsubscript{biased}) to the backbone model (LTG-BERT\\\\textsubscript{base})...\\\""
    },
    {
        "paper": "arXiv-2410.24159v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the performance of the GPT-BERT model on the BabyLM Challenge 2024 strict-small track compared to other models?",
        "answer": "On the BabyLM Challenge 2024 strict-small track (10M words), the GPT-BERT model achieves a BLiMP score of 81.2, BLiMP-S score of 69.4, GLUE score of 76.5, and EWOK score of 54.6. It outperforms the encoder-only (baseline) with BLiMP of 60.6, BLiMP-S of 60.8, GLUE of 60.3, and EWOK of 48.9, as well as the decoder-only (baseline) which has BLiMP of 69.8, BLiMP-S of 59.5, GLUE of 63.3, and EWOK of 50.7. It also outperforms ELC-BERT which has BLiMP of 80.5, BLiMP-S of 67.9, GLUE of 75.3, and EWOK of 51.0, while performing slightly worse on BLiMP-S compared to LTG-BERT which scores 69.8.",
        "reference": "Paper title: \\\"GPT or BERT: why not both?\\\" Section: Pretraining and evaluation, Table 1 \\\"BabyLM submission scores\\\"."
    },
    {
        "paper": "arXiv-2410.24159v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the effect of varying the causal-to-mask ratio on the performance of the GPT-BERT model as shown in the experiments?",
        "answer": "The performance of the GPT-BERT model improves as the causal-to-mask ratio includes more masked training examples in bidirectional evaluation. Best results on various tasks are achieved with different ratios ranging from 1:7 to masked-only. The model trained with 1:7 ratio achieves the best results for BLiMP and LAMBADA, and 15:16 for MNLI. However, for causal evaluation, a more balanced 1:1 or 1:3 ratio seems to provide better performance across tasks.",
        "reference": "Paper title: \\\"GPT or BERT: why not both?\\\" Section: Experiments, \\\"The effect of the causal-to-mask ratio\\\"."
    }
]