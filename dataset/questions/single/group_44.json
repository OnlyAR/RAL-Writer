[
    {
        "paper": "arXiv-1803.05457v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the size of the AI2 Reasoning Challenge (ASC) dataset and how is it partitioned?",
        "answer": "The AI2 Reasoning Challenge (ASC) dataset consists of 7,787 questions. It is partitioned into a Challenge Set of 2,590 questions and an Additional Set of 5,197 questions.",
        "reference": "Paper Title: Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. Section: The \\\\ASC~Dataset. \\\"The \\\\ASC~Dataset consists of 7787 natural science questions... We have partitioned \\\\ASC~into a \\\\Challenge~Set (2590 questions), containing questions answered incorrectly by both a retrieval-based algorithm and a word co-occurrence algorithm, and an \\\\Additional~Set (5197 questions), containing the remainder.\\\""
    },
    {
        "paper": "arXiv-1803.05457v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the baseline performances on the AI2 Reasoning Challenge \\\\Challenge~Set compared to the \\\\Additional~Set?",
        "answer": "On the AI2 Reasoning Challenge \\\\Challenge~Set, none of the baseline algorithms scored significantly higher than the random baseline of 25%, with the highest score being 27.11% by DGEM. On the \\\\Additional~Set, performances ranged between 55% and 65%, indicating a much more challenging nature of the \\\\Challenge~Set.",
        "reference": "Paper Title: Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. Section: Baseline Performance. Table 8: \\\"Performance of the different baseline systems. Scores are reported as percentages on the test sets. IR (using \\\\ASC~Corpus) scored 20.26% on \\\\Challenge Set and 62.55% on \\\\Additional Set. TupleInference scored 23.83% on \\\\Challenge Set and 60.81% on \\\\Additional Set. DecompAttn scored 24.34% on \\\\Challenge Set and 58.27% on \\\\Additional Set. Guess-all (random) scored 25.02% on both sets. DGEM scored 27.11% on \\\\Challenge Set and 58.97% on \\\\Additional Set.\\\""
    },
    {
        "paper": "arXiv-2206.10498v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How many instances were developed for the Blocksworld domain in the PlanBench benchmark?",
        "answer": "600 instances were developed for the Blocksworld domain in the PlanBench benchmark.",
        "reference": "PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change. Section 'Dataset Details': 'We developed 600 instances which vary in the number of objects used as well as the optimal plan length.'"
    },
    {
        "paper": "arXiv-2206.10498v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What were the performance results for GPT-4 and Instruct-GPT3 on the Plan Generation task in the Blocksworld domain from the PlanBench benchmark?",
        "answer": "In the Plan Generation task, GPT-4 correctly solved 206 out of 600 instances (34.3%) while Instruct-GPT3 correctly solved 41 out of 600 instances (6.8%).",
        "reference": "PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change. Table 'PlanBench Results of GPT-4 and Instruct-GPT-3 (text-davinci-002) on Blocksworld domain': Plan Generation '206/600 (34.3%)' for GPT-4 and '41/600 (6.8%)' for I-GPT3."
    },
    {
        "paper": "arXiv-2410.16270v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the overall scores of the top-three performing LLMs evaluated on Reflection-Bench?",
        "answer": "The top-three performing LLMs on Reflection-Bench are o1-preview with an overall score of 80.97, GPT-4 with an overall score of 69.10, and Llama-3.1-405B-Instruct with an overall score of 68.72.",
        "reference": "Reflection-Bench: probing AI intelligence with reflection, Section 'Experiment Results', Table 2."
    },
    {
        "paper": "arXiv-2410.16270v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "In the Reflection-Bench evaluation, which task did all models score zero points, and what does this result indicate?",
        "answer": "In the Reflection-Bench evaluation, all models scored zero points on the Meta-bandit task. This result indicates a universal absence of meta-reflection abilities among the evaluated models.",
        "reference": "Reflection-Bench: probing AI intelligence with reflection, Section 'Experiment Results', Paragraph 'Main results'."
    }
]