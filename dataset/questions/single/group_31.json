[
    {
        "paper": "arXiv-1705.04146v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the average length, in terms of tokens, for the numeric and non-numeric parts of questions in the dataset introduced in the paper \\\"Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems\\\"?",
        "answer": "The average length for the numeric part of questions is 9.6 tokens, and for the non-numeric part, it is 67.8 tokens.",
        "reference": "\\\"Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems\\\" - Section \\\"Statistics\\\", Table 1: \\\"Average Length: Numeric (9.6), Non-Numeric (67.8)\\\"."
    },
    {
        "paper": "arXiv-1705.04146v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How many examples are there in the test dataset of the algebraic word problems introduced in the paper \\\"Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems\\\"?",
        "answer": "There are 250 examples in the test dataset.",
        "reference": "\\\"Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems\\\" - Section \\\"Statistics\\\": \\\"Test Examples: 250\\\"."
    },
    {
        "paper": "arXiv-2009.03300v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the average accuracy of the GPT-3 X-Large model on the proposed multitask test covering 57 tasks?",
        "answer": "The average accuracy of the GPT-3 X-Large model on the proposed multitask test covering 57 tasks is 43.9%.",
        "reference": "Paper Title: Measuring Massive Multitask Language Understanding; Reference: \\\"We find that meaningful progress on our benchmark has only become possible in recent months. In particular, few-shot models up to 13 billion parameters achieve random chance performance of 25% accuracy, but the 175 billion parameter GPT-3 model reaches a much higher 43.9% accuracy.\\\" Section: Experiments, Results, Table 1."
    },
    {
        "paper": "arXiv-2009.03300v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does GPT-3's accuracy in Professional Medicine compare to its accuracy in College Mathematics according to the findings of the benchmark?",
        "answer": "GPT-3's accuracy in Professional Medicine is 47.4%, while its accuracy in College Mathematics is 35.0%.",
        "reference": "Paper Title: Measuring Massive Multitask Language Understanding; Reference: \\\"GPT-3 does better on College Medicine (47.4%) and College Mathematics (35.0%) than calculation-heavy Elementary Mathematics (29.9%).\\\" Section: Experiments, Results."
    },
    {
        "paper": "arXiv-2110.14168v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the composition of the GSM8K dataset introduced in 'Training Verifiers to Solve Math Word Problems'?",
        "answer": "The GSM8K dataset consists of 8.5K high quality grade school math problems. It is divided into 7.5K training problems and 1K test problems. These problems involve performing a sequence of elementary calculations using basic arithmetic operations ($+ - \\\\times \\\\div$) to reach the final answer and take between 2 and 8 steps to solve.",
        "reference": "Training Verifiers to Solve Math Word Problems, Section 1 Introduction: 'To facilitate research, we are releasing GSM8K, a dataset of 8.5K high quality problems at the grade school math level.' and Section 2 Dataset: 'We segmented these into 7.5K training problems and 1K test problems.'"
    },
    {
        "paper": "arXiv-2110.14168v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What was the comparative performance of 6B and 175B models using verification on the GSM8K dataset according to 'Training Verifiers to Solve Math Word Problems'?",
        "answer": "The verification method significantly improved performance for both the 6B and 175B model sizes when tested on GSM8K. Verification resulted in a performance boost equivalent to a 30x model size increase. For example, 6B verification slightly outperformed a 175B finetuned model.",
        "reference": "Training Verifiers to Solve Math Word Problems, Section 4 Methods: 'Verification considers 100 solutions per problem.', 'On the full dataset, 6B verification slightly outperforms a finetuned 175B model, thereby offering a boost approximately equivalent to a 30x model size increase.'"
    }
]