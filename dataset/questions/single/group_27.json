[
    {
        "paper": "arXiv-2209.05451v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the composition of tasks and task variations \\\\\\\\model~ is trained on according to the paper \\\"A Multi-Task Transformer for Robotic Manipulation\\\"?",
        "answer": "The paper states that \\\\model~ is trained on 18 RLBench tasks with 249 variations and 7 real-world tasks with 18 variations. This is outlined in the abstract and reiterated in the results section.",
        "reference": "\\\"A Multi-Task Transformer for Robotic Manipulation\\\", Abstract: \\\"we train a single multi-task Transformer for 18 RLBench tasks (with 249 variations) and 7 real-world tasks (with 18 variations) from just a few demonstrations per task.\\\""
    },
    {
        "paper": "arXiv-2209.05451v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "According to the paper \\\"A Multi-Task Transformer for Robotic Manipulation\\\", how does the performance of \\\\model compare to the baselines such as 3D ConvNet in simulation tasks?",
        "answer": "The paper reports that \\\\model~ significantly outperforms image-to-action agents and 3D ConvNet baselines for a wide range of tabletop tasks. Specifically, \\\\model outperforms the most competitive baseline (\\\\texttt{C2F-ARM-BC}) with an average improvement of 1.33× with 10 demonstrations and 2.83× with 100 demonstrations.",
        "reference": "\\\"A Multi-Task Transformer for Robotic Manipulation\\\", Section 5 - Simulation Results: \\\"\\\\model~outperforms \\\\texttt{C2F-ARM-BC} in $25/36$~evaluations in Table 1 with an average improvement of $1.33\\\\times$ with 10 demonstrations and $2.83\\\\times$ with 100 demonstrations.\\\""
    },
    {
        "paper": "arXiv-2307.15818v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the performance of \\\\methodname on seen tasks compared to other baselines?",
        "answer": "The performance on seen tasks is similar between the \\\\methodname models and RT-1, with other baselines attaining a lower success rate. Specifically, both \\\\methodname instantiations outperform all baselines significantly in generalization tasks, with \\\\methodname models achieving about twice the success rate of the next two best baselines, RT-1 and MOO.",
        "reference": "\\\\methodname: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, Section Experiments, Figure 6 and corresponding text: \\\"The performance on seen tasks is similar between the \\\\methodname models and RT-1, with other baselines attaining a lower success rate. [...] Here, on average, both instantiations of \\\\methodname perform similarly, resulting in $\\\\sim$2x improvement over the next two baselines, RT-1 and MOO.\\\""
    },
    {
        "paper": "arXiv-2307.15818v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the success rate of the \\\\methodname-PaLI-3B model compared to other models on the Language-Table tasks?",
        "answer": "The \\\\methodname-PaLI-3B model achieved a success rate of 90% ± 10% on the Language-Table tasks, outperforming BC-Zero with 72% ± 3%, RT-1 with 74% ± 13%, and LAVA with 77% ± 4%.",
        "reference": "\\\\methodname: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, Experiments section, Table 2: \\\"The results of this experiment are presented in Table 3. We observe a significant performance boost when using our model compared to the baselines, indicating that the VLM-based pre-training together with the expressiveness of the large PaLI model can be beneficial in other scenarios.\\\""
    },
    {
        "paper": "arXiv-2310.08864v8.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How many institutions contributed to the formation of the Open X-Embodiment dataset?",
        "answer": "The Open X-Embodiment dataset was formed with contributions from 46 institutions worldwide.",
        "reference": "Title: Open X-Embodiment: Robotic Learning Datasets and RT-X Models. Abstract: \\\"We assemble a dataset from different robots collected through a collaboration between 46 institutions.\\\""
    },
    {
        "paper": "arXiv-2310.08864v8.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the total number of robot embodiments included in the Open X-Embodiment dataset, and how is this data structured?",
        "answer": "The Open X-Embodiment dataset includes data from 34 different robot embodiments and is structured by pooling 131 existing robot datasets in a consistent data format using RLDS data format, allowing efficient, parallelized data loading.",
        "reference": "Title: Open X-Embodiment: Robotic Learning Datasets and RT-X Models. Section: Dataset Analysis: \\\"The dataset consists of 131 individual datasets across 34 embodiments.\\\""
    }
]