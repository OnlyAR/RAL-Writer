[
    {
        "paper": "arXiv-2309.04669v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the performance of LaVIT on the zero-shot multi-modal understanding task of Visual Question Answering compared to other models?",
        "answer": "LaVIT achieves a Visual Question Answering accuracy of 66.0% on VQAv2 and 54.6% on OKVQA, significantly outperforming other models such as Flamingo-9B, which achieves 51.8% on VQAv2 and 44.7% on OKVQA. Moreover, LaVIT also surpasses the concurrent method Emu (LLaMA-13B), which achieves 52.0% on VQAv2 and 38.2% on OKVQA.",
        "reference": "\\\"Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization\\\", Section 'Zero-Shot Multimodal Understanding', Table 1."
    },
    {
        "paper": "arXiv-2309.04669v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does LaVIT perform in zero-shot text-to-image generation compared to existing MLLMs and specialist models like Parti?",
        "answer": "LaVIT achieves a zero-shot FID score of 7.4 on the MS-COCO-30K benchmark, making it the best among existing MLLMs like CM3Leon-7B (10.8) and Emu (11.7). It also performs competitively with the text-to-image specialists such as Parti-20B, which has a FID score of 7.2, despite using fewer training data.",
        "reference": "\\\"Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization\\\", Section 'Zero-Shot Multimodal Generation', Table \\\"wrap-tab:1\\\"."
    },
    {
        "paper": "arXiv-2404.02905v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the achieved Fréchet inception distance (FID) and inception score (IS) for the VAR-$d30$ model on the ImageNet 256x256 benchmark?",
        "answer": "The VAR-$d30$ model achieved a Fréchet inception distance (FID) of 1.73 and an inception score (IS) of 350.2 on the ImageNet 256x256 benchmark.",
        "reference": "From the paper titled \\\"Visual Autoregressive Modeling:\\\\n Scalable Image Generation via Next-Scale Prediction\\\", Section \\\"Empirical Results\\\", as stated: \\\"VAR-$d30$ achieved a Fréchet inception distance (FID) of 1.73 and an inception score (IS) of 350.2.\\\""
    },
    {
        "paper": "arXiv-2404.02905v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the power-law scaling laws observed with VAR models with relation to model parameters N regarding the test loss L?",
        "answer": "The power-law scaling laws between the model parameters N and test loss L are: L_avg = (2.5 * N)^(-0.20) and L_last = (2.0 * N)^(-0.23).",
        "reference": "From the paper titled \\\"Visual Autoregressive Modeling:\\\\n Scalable Image Generation via Next-Scale Prediction\\\", Section \\\"Power-law scaling laws\\\" states: \\\"The power-law scaling laws for test loss L (average and last scale) with model parameters N are as follows: \\\\nL_avg = (2.5 * N)^(-0.20) and L_last = (2.0 * N)^(-0.23).\\\""
    },
    {
        "paper": "arXiv-2405.13218v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the FID score for the best-performing conditioning method for Next-token (NT) and Masked-token (MT) objectives in the paper titled 'Computational Tradeoffs in Image Synthesis: Diffusion, Masked-Token, and Next-Token Prediction'?",
        "answer": "The best-performing conditioning method for the Next-token (NT) objective is using 'adaLNzero', with an FID score of 83.052. For the Masked-token (MT) objective, 'adaLNzero' is also the best-performing conditioning method, with an FID score of 97.021.",
        "reference": "From the paper 'Computational Tradeoffs in Image Synthesis: Diffusion, Masked-Token, and Next-Token Prediction', in Table \\\\ref{tab:condablation}, it states: 'Objective & Conditioning & FID &  CLIP \\\\\\\\\\\\n\\\\midrule ... \\\\nNT  &   adaLNzero   &      \\\\textbf{83.052}    &   \\\\textbf{0.2213}  \\\\\\\\ ... \\\\nMT  &   adaLNzero   &      \\\\textbf{97.021}    &   \\\\textbf{0.2164}  \\\\\\\\ ... '."
    },
    {
        "paper": "arXiv-2405.13218v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "In the paper 'Computational Tradeoffs in Image Synthesis: Diffusion, Masked-Token, and Next-Token Prediction', which autoencoder configuration had the lowest rFID score and what was its latent space capacity?",
        "answer": "The autoencoder configuration with the lowest rFID score had a KL regularizer with 16 channels in the latent space, achieving an rFID score of 1.060.",
        "reference": "In the paper 'Computational Tradeoffs in Image Synthesis: Diffusion, Masked-Token, and Next-Token Prediction', Table \\\\ref{tab:aereconstructiontable} provides the relevant details: 'Regularizer & Latent space capacity & rFID ($\\\\downarrow$) \\\\\\\\\\\\n\\\\midrule  \\\\nKL & 16 channels & 1.060 \\\\\\\\ ... '."
    }
]