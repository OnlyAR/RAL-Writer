[
    {
        "paper": "arXiv-2401.14818v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the size of the ChemDFM-13B model's training dataset in terms of tokens and sources as described in the paper \\\"ChemDFM: Dialogue Foundation Model for Chemistry\\\"?",
        "answer": "The ChemDFM-13B model is trained on 34 billion tokens derived from over 3.8 million chemical papers and 1.4 thousand textbooks. Additionally, more than 2.7 million instructions from various chemical databases were used.",
        "reference": "\\\"ChemDFM: Dialogue Foundation Model for Chemistry\\\" - Abstract: \\\"ChemDFM-13B is trained on 34B tokens from chemical literature...\\\". Section - ChemDFM: Domain Pre-training: \\\"we get 34B tokens from 3.9M chemical papers and 49M tokens from 1.4K books.\\\""
    },
    {
        "paper": "arXiv-2401.14818v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does ChemDFM-13B perform compared to GPT-4 in molecule recognition tasks according to \\\"ChemDFM: Dialogue Foundation Model for Chemistry\\\"?",
        "answer": "ChemDFM-13B significantly outperforms GPT-4 in molecule recognition tasks. For example, in the name prediction tasks, ChemDFM-13B achieves exact match scores of 4.0 for S2I (SMILES to IUPAC names translation) and 11.0 for I2S (IUPAC names to SMILES translation), compared to GPT-4's scores of 0 and 1.2, respectively.",
        "reference": "\\\"ChemDFM: Dialogue Foundation Model for Chemistry\\\" - Section: ChemLLMBench: Table 4 - \\\"In the name prediction tasks... ChemDFM can even outperform GPT-4 in all the name prediction tasks.\\\""
    },
    {
        "paper": "arXiv-2402.09391v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the specific sizes of the datasets used for each task in the proposed \\\\",
        "answer": "The size of the dataset for each task in the proposed \\\\",
        "reference": "In Section \\\\"
    },
    {
        "paper": "arXiv-2402.09391v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the \\\\",
        "answer": "According to the results presented in Section \\\\",
        "reference": "The specific results are detailed in Table \\\\"
    },
    {
        "paper": "arXiv-2409.13194v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How many instruction-tuning dataset samples were used for MS2 spectrum recognition as described in the ChemDFM-X paper?",
        "answer": "The instruction-tuning dataset for MS2 spectrum recognition contains 21,000 samples.",
        "reference": "ChemDFM-X: Towards Large Multimodal Model for Chemistry, Section 'Characterization Modality: MS2 Spectrum', Table 7."
    },
    {
        "paper": "arXiv-2409.13194v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What was the accuracy of ChemDFM-X (S + R) for reaction prediction tasks involving characterization modalities according to the paper?",
        "answer": "ChemDFM-X (S + R) achieved an accuracy of 64% for reaction prediction tasks involving characterization modalities.",
        "reference": "ChemDFM-X: Towards Large Multimodal Model for Chemistry, Section 'Characterization Modalities', Table 12."
    }
]