[
    {
        "paper": "arXiv-2306.02858v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the modalities that Video-LLaMA can comprehend based on Table 1?",
        "answer": "Based on Table 1, Video-LLaMA can comprehend static images, silent videos, and audio. It has the unique ability compared to other models to comprehend auditory and visual information simultaneously (static image, silent video, and audio).",
        "reference": "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding, Table 1: \\\"Comparison with popular multi-modal large language models. Video-LLaMA has the unique ability to comprehend auditory and visual information simultaneously.\\\""
    },
    {
        "paper": "arXiv-2306.02858v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the source of the image detail description dataset used in the fine-tuning stage of Video-LLaMA's vision-language branch?",
        "answer": "The image detail description dataset used in the fine-tuning stage of Video-LLaMA's vision-language branch is sourced from MiniGPT-4.",
        "reference": "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding, Section 'Training of Vision-Language Branch': \\\"...we fine-tuned the model using high-quality instruction data. We integrated the image-detail-description dataset from MiniGPT-4...\\\""
    },
    {
        "paper": "arXiv-2306.05424v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the size of the dataset used to train the Video-ChatGPT model?",
        "answer": "The dataset used to train the Video-ChatGPT model consists of 100,000 video-instruction pairs.",
        "reference": "Title: Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models, Section: Abstract, Quote: \\\"We introduce a new dataset of 100,000 video-instruction pairs used to train Video-ChatGPT acquired via manual and semi-automated pipeline...\\\""
    },
    {
        "paper": "arXiv-2306.05424v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How did Video-ChatGPT perform in the zero-shot question-answering tasks compared to other generative models, particularly on the MSVD-QA dataset?",
        "answer": "Video-ChatGPT achieved an accuracy of 64.9 and a score of 3.3 on the MSVD-QA dataset, outperforming other models like Video Chat, LLaMA Adapter, and Video LLaMA.",
        "reference": "Title: Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models, Section: Experiments, Table: Zeroshot question-answering comparison of Video-ChatGPT with other video generative models."
    },
    {
        "paper": "arXiv-2311.10122v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the performance improvements of Video-LLaVA over Video-ChatGPT on video QA benchmarks?",
        "answer": "Video-LLaVA outperforms Video-ChatGPT by 5.8% on the MSRVTT-QA dataset, 9.9% on the MSVD-QA dataset, 18.6% on the TGIF-QA dataset, and 10.1% on the ActivityNet-QA dataset.",
        "reference": "Title: Video-LLaVA: Learning United Visual Representation by Alignment Before Projection; Section: Abstract. \\\"Video-LLaVA outperforms Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% on MSRVTT, MSVD, TGIF, and ActivityNet, respectively.\\\""
    },
    {
        "paper": "arXiv-2311.10122v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the results of Video-LLaVA on image understanding benchmarks compared to InstructBLIP-7B?",
        "answer": "Video-LLaVA demonstrates superior performance over InstructBLIP-7B on image understanding benchmarks. Specifically, Video-LLaVA surpasses InstructBLIP-7B by 24.9% on MMBench, 12.2% on LLaVA-Bench, and 5.8% on MM-Vet.",
        "reference": "Title: Video-LLaVA: Learning United Visual Representation by Alignment Before Projection; Section: Quantitative Evaluation. \\\"Video-LLaVA outperform InstructBLIP-7B by 24.9%, 12.2%, and 5.8% on MMBench, LLaVA-Bench, and MM-Vet, respectively.\\\""
    }
]