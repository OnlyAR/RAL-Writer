[
    {
        "paper": "arXiv-2404.01258v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the demonstrated improvement in accuracy of \\\\modelname over its SFT counterpart according to the paper \\\"Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward\\\"?",
        "answer": "The \\\\modelname achieves an 8.1% accuracy improvement over the SFT counterpart.",
        "reference": "\\\"Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward,\\\" abstract section: \\\"Employing this reward mechanism as the basis for DPO algorithm, we train \\\\modelname that achieves an 8.1\\\\% accuracy improvement over the SFT counterpart.\\\""
    },
    {
        "paper": "arXiv-2404.01258v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How many video captions are included in the dataset \\\\datasetname, as discussed in the paper \\\"Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward\\\"?",
        "answer": "The dataset \\\\datasetname comprises 900k video captions.",
        "reference": "\\\"Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward,\\\" abstract section: \\\"To mitigate the shortage of high-quality video captions, we have developed a comprehensive video caption dataset, \\\\datasetname, using a novel prompting technique with the \\\\\\\\gptv model, comprising 900k captions that encompass a wide range of video content.\\\""
    },
    {
        "paper": "arXiv-2406.04325v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the composition of the ShareGPT4Video dataset mentioned in the 'ShareGPT4Video Dataset' section of the paper titled 'ShareGPT4Video: Improving Video Understanding and Generation with Better Captions'?",
        "answer": "The ShareGPT4Video dataset is composed of 40K high-quality video-caption pairs. It encompasses a broad array of video categories collected from sources such as Panda-70M, Ego4D, and BDD100K, as well as user-uploaded video websites like Pexels, Pixabay, and MixKit. These videos range from 2 seconds to 2 minutes in length.",
        "reference": "Title: ShareGPT4Video: Improving Video Understanding and Generation with Better Captions. Section: ShareGPT4Video Dataset. \\\"Based on DiffSW, we construct ShareGPT4Video, which contains 40K high-quality video-caption pairs spanning a wide range of categories...\\\" and \\\"The videos of ShareGPT4Video are collected from various sources...\\\""
    },
    {
        "paper": "arXiv-2406.04325v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the ShareCaptioner-Video model, mentioned in the 'ShareCaptioner-Video' section of the paper titled 'ShareGPT4Video: Improving Video Understanding and Generation with Better Captions', scale up its video captioning capabilities?",
        "answer": "The ShareCaptioner-Video model scales up its video captioning capabilities by generating high-quality captions for 4.8 million videos, totaling about 3000 hours. It utilizes sliding captioning mode to produce these detailed captions efficiently, requiring approximately 4000 H100 GPU hours to complete the process.",
        "reference": "Title: ShareGPT4Video: Improving Video Understanding and Generation with Better Captions. Section: ShareCaptioner-Video. \\\"To validate the effectiveness of our ShareCaptioner-Video...we utilized it to annotate a large volume of aesthetically appealing videos...we generate high-quality captions for 4.8M ... about 3000 hours...The total captioning process requires approximately 4000 H100 GPU hours.\\\""
    },
    {
        "paper": "arXiv-2410.02713v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the composition and size of the proposed \\\\DataName{} dataset in the paper 'Video Instruction Tuning with Synthetic Data'?",
        "answer": "The \\\\DataName{} dataset consists of 178,510 videos, ranging in length from 0 to 3 minutes. Additionally, it contains 1.3 million instruction samples, which include detailed captions, open-ended questions, and multiple-choice questions.",
        "reference": "Video Instruction Tuning with Synthetic Data, Section: Video Instruction-Following Data Synthesis. \\\"To overcome these shortcomings, we introduce a comprehensive video instruction-tuning dataset named \\\\DataName, consisting of 178,510 videos ranging from 0 to 3 minutes. This dataset is enriched with detailed annotations, open-ended questions, and multiple-choice questions...\\\""
    },
    {
        "paper": "arXiv-2410.02713v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the paper 'Video Instruction Tuning with Synthetic Data' categorize the datasets used for training the \\\\ModelName{} model, and what is the total number of samples used?",
        "answer": "The training of the \\\\ModelName{} model uses a combination of video and image datasets. Specifically, it includes video data from \\\\DataName, ActivityNet-QA, NExT-QA, PerceptionTest, and LLaVA-Hound-255K, and also image data from LLaVA-OneVision. The total number of video-language samples is 1.6 million, which includes 193,510 video descriptions, 1,241,412 open-ended questions, and 215,625 multiple-choice questions.",
        "reference": "Video Instruction Tuning with Synthetic Data, Section: Overall Results. \\\"We fine-tune LLaVA-OneVision (SI) on the joint dataset of video and image data. Specifically, we added video data from the \\\\DataName{} dataset and four public datasets: ActivityNet-QA, NExT-QA, PerceptionTest, and LLaVA-Hound-255K...\\\""
    }
]