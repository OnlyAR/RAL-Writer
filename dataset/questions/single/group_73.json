[
    {
        "paper": "arXiv-2403.03883v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the size of the legal corpus used to pretrain \\\\ourmodel{} in the paper \\\"\\\\ourmodel{}: A pioneering Large Language Model for Law\\\"?",
        "answer": "The legal corpus used to pretrain \\\\ourmodel{} consists of 30 billion tokens.",
        "reference": "\\\"\\\\ourmodel{}: A pioneering Large Language Model for Law\\\" Section: Abstract, \\\"\\\\ourmodel{} is trained on an English legal corpus of over 30 billion tokens.\\\""
    },
    {
        "paper": "arXiv-2403.03883v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the number of tokens in the English MultiLegal Pile dataset after cleaning and deduplication for \\\\ourmodel{}'s pretraining?",
        "answer": "The number of tokens in the English MultiLegal Pile dataset after cleaning and deduplication, as part of \\\\ourmodel{}'s pretraining, is 50 billion tokens, but the entire corpus was reduced to a total of 30 billion tokens across all sources after cleaning and deduplication.",
        "reference": "\\\"\\\\ourmodel{}: A pioneering Large Language Model for Law\\\" Section: Dataset Composition, \\\"English MultiLegal Pile ... 50B\\\", \\\"... we filtered and deduplicated, resulting in a 30 billion tokens dataset.\\\""
    },
    {
        "paper": "arXiv-2406.04614v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the size of the legal pre-training corpus used in the \\\\algo model, and what domains does it cover?",
        "answer": "The legal pre-training corpus for the \\\\algo model consists of 500K legal documents. It covers various legal domains, including civil law, criminal law, and administrative law.",
        "reference": "Title: \\\\algo: A Chinese Legal Knowledge-Enhanced Large Language Model, Section: Methodology - Legal-Oriented Pre-Training, \\\"we collect a large-scale legal pre-training corpus $\\\\mathcal{D}_{\\\\text{LPT}}$ consisting of 500K legal documents from various legal domains, including civil law, criminal law, and administrative law.\\\""
    },
    {
        "paper": "arXiv-2406.04614v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the experimental results of the \\\\algo model compared to the proprietary and open-source models on the zero-shot legal tasks?",
        "answer": "In the zero-shot setting, \\\\algo outperforms the open-source LLaMA 7B model on major tasks with a better average performance of 17.4 compared to LLaMA's 16.7. Proprietary models GPT-3.5 Turbo and GPT-4 achieve significantly higher average performances of 44.7 and 54.0, respectively.",
        "reference": "Title: \\\\algo: A Chinese Legal Knowledge-Enhanced Large Language Model, Section: Performance Evaluation, Table 1, \\\"The results show that our \\\\algo outperforms LLaMA 7B model on major tasks and leading to a better average performance. Despite the advantage of preserving data privacy, there is still a significant performance gap between \\\\algo and proprietary models.\\\""
    },
    {
        "paper": "arXiv-2407.21065v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the top-1, top-3, and top-5 accuracy rates for the Similar Case Retrieval (SCR) task of LawLLM?",
        "answer": "The top-1, top-3, and top-5 accuracy rates for the Similar Case Retrieval (SCR) task of LawLLM are 29.8%, 63.2%, and 81.6% respectively.",
        "reference": "Title: LawLLM: Law Large Language Model for the US Legal System. Section: Similar Case Retrieval Results. \\\"According to Table 4, \\\\/sysname outperformed the baseline models in all categories. Specifically, it achieved the highest accuracy in top-1, top-3, and top-5 retrieval rates, with scores of 29.8\\\\%, 63.2\\\\%, and 81.6\\\\% respectively.\\\""
    },
    {
        "paper": "arXiv-2407.21065v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the hallucination rate of LawLLM compare to GPT-4 in the Similar Case Retrieval (SCR) task?",
        "answer": "LawLLM exhibits a significantly lower hallucination rate of 0.1% compared to GPT-4's 0.5% in the Similar Case Retrieval (SCR) task.",
        "reference": "Title: LawLLM: Law Large Language Model for the US Legal System. Section: Similar Case Retrieval Results. \\\"Remarkably, it also demonstrated minimal hallucination, as indicated by the not-found rate of 0.1\\\\%. (...) GPT-4 showed strong performance (...) and a low not-found rate of 0.5\\\\%.\\\""
    }
]