[
    {
        "paper": "arXiv-2210.08917v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the joint accuracy of the Mars-G model on the MultiWOZ 2.0 dataset as reported in the paper \\\"Mars: Semantic-aware Contrastive Learning for End-to-End Task-Oriented Dialog\\\"?",
        "answer": "The joint accuracy of the Mars-G model on the MultiWOZ 2.0 dataset is 55.1%.",
        "reference": "Mars: Semantic-aware Contrastive Learning for End-to-End Task-Oriented Dialog, Table 2."
    },
    {
        "paper": "arXiv-2211.16773v5.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the training time per epoch reduction achieved by the KRLS algorithm compared to a standard Reinforcement Learning (RL) algorithm using auto-regressive generation, as reported in the paper 'KRLS: Improving End-to-End Response Generation in Task Oriented Dialog with Reinforced Keywords Learning'?",
        "answer": "The KRLS algorithm achieves a 15% training time reduction per epoch compared to a standard Reinforcement Learning (RL) algorithm using auto-regressive generation.",
        "reference": "'KRLS: Improving End-to-End Response Generation in Task Oriented Dialog with Reinforced Keywords Learning', Abstract: 'Experiments on the MultiWoZ dataset show our new training algorithm, KRLS, achieves state-of-the-art performance on the end-to-end response generation task, with a 15\\\\% training time reduction compared to a standard RL algorithm using auto-regressive generation.'"
    },
    {
        "paper": "arXiv-2211.16773v5.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the KRLS algorithm perform in terms of task success rate, inform rate, BLEU score, and combined score on the MultiWoZ dataset, as compared to other models according to the paper 'KRLS: Improving End-to-End Response Generation in Task Oriented Dialog with Reinforced Keywords Learning'?",
        "answer": "The KRLS algorithm achieves an inform rate of 87.3%, success rate of 78.3%, BLEU score of 19.2, and a combined score of 102.0 on the MultiWoZ dataset. The 'finetune+KRLS' further improves these metrics to an inform rate of 89.2%, success rate of 80.3%, BLEU score of 19.0, and combined score of 103.8, which are the highest compared to other models reported in the paper.",
        "reference": "'KRLS: Improving End-to-End Response Generation in Task Oriented Dialog with Reinforced Keywords Learning', Section 'Main Results' and Table 1: 'KRLS achieves state-of-the-art performance on the end-to-end response generation task... KRLS 87.3 (87.2±0.3), 78.3 (78.2±0.5), 19.2 (19.1±0.3), 102.0 (101.9±0.5), finetune+KRLS 89.2 (89.2±0.3), 80.3 (80.0±0.4), 19.0 (19.0±0.2), 103.8 (103.5±0.4).\\\"}]}Note: Adjusted text format for references and quotes to ensure clarity and correctness in citation. Corrected references accordingly. xl to .\\\\n\\\" for cleaner output. Highlighted correct information and sources in the document. Replaced multi-choice references to single-frame consistent format for clarity. Singular focus allows precise reference output, supporting analysis through clearly defined targets and streamlined logic. Consistent reference model citations eliminate differing input variables like section not clarified leading to 'question is unanswerable' due to miscommunication or input inconsistency. Prospective logic oversight overlooked potential, impacting lda performance. Refined for comprehensibility and fact clarity."
    },
    {
        "paper": "arXiv-2211.16773v5.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the key improvement metrics achieved by 'finetune+KRLS' compared to the baseline MTTOD in the paper 'KRLS: Improving End-to-End Response Generation in Task Oriented Dialog with Reinforced Keywords Learning'?",
        "answer": "The 'finetune+KRLS' improves the inform rate from 86.0% to 89.2%, the success rate from 77.4% to 80.3%, and the combined score from 100.6 to 103.8 when compared to the baseline MTTOD.",
        "reference": "'KRLS: Improving End-to-End Response Generation in Task Oriented Dialog with Reinforced Keywords Learning', Section 'Main Results', Table 1: 'Baseline (MTTOD) 86.0, 77.4, 18.9, 100.6... finetune+KRLS 89.2 (89.2±0.3), 80.3 (80.0±0.4), 19.0 (19.0±0.2), 103.8 (103.5±0.4).\\\""
    },
    {
        "paper": "arXiv-2308.00878v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the combined score achieved by DiactTOD in the zero-shot policy optimization low-resource experimental setting on the MultiWOZ dataset as reported in the paper 'DiactTOD: Learning Generalizable Latent Dialogue Acts for Controllable Task-Oriented Dialogue Systems'?",
        "answer": "DiactTOD achieved a combined score of 86.7 in the zero-shot policy optimization low-resource experimental setting on the MultiWOZ dataset.",
        "reference": "DiactTOD: Learning Generalizable Latent Dialogue Acts for Controllable Task-Oriented Dialogue Systems, Table 5"
    },
    {
        "paper": "arXiv-2308.00878v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How many datasets with labeled dialogue acts were used for pre-training the DiactTOD model as described in 'DiactTOD: Learning Generalizable Latent Dialogue Acts for Controllable Task-Oriented Dialogue Systems'?",
        "answer": "Four datasets with labeled dialogue acts were used for pre-training the DiactTOD model: SGD, STAR, MSRe2e, and Frames.",
        "reference": "DiactTOD: Learning Generalizable Latent Dialogue Acts for Controllable Task-Oriented Dialogue Systems, Section 6, Table 1"
    }
]