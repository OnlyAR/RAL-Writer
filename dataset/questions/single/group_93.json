[
    {
        "paper": "arXiv-2211.01910v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the performance of the Automatic Prompt Engineer (APE) method compared to human-engineered instructions on Instruction Induction and BIG-Bench tasks, as discussed in the paper \\\"Large Language Models are Human-Level Prompt Engineers\\\"?",
        "answer": "The Automatic Prompt Engineer (APE) method achieves better or comparable performance to human-engineered instructions, achieving better results on 24/24 Instruction Induction tasks and 17/21 curated BIG-Bench tasks.",
        "reference": "Paper Title: \\\"Large Language Models are Human-Level Prompt Engineers\\\" Section: Abstract. Direct Quote: \\\"...achieve better or comparable performance to the instructions generated by human annotators on 24/24 Instruction Induction tasks and 17/21 curated BIG-Bench tasks.\\\""
    },
    {
        "paper": "arXiv-2211.01910v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "According to the paper \\\"Large Language Models are Human-Level Prompt Engineers,\\\" how does the Interquartile Mean (IQM) of APE's performance compare to human-engineered prompts on 24 NLP tasks?",
        "answer": "The Interquartile Mean (IQM) of APE's performance for 24 NLP tasks using the InstructGPT model is 0.810, which surpasses the IQM of human-engineered prompts, which is 0.749.",
        "reference": "Paper Title: \\\"Large Language Models are Human-Level Prompt Engineers\\\" Section: Figure 1b. Direct Quote: \\\"As measured by the interquartile mean across the 24 NLP tasks introduced by Honovich et al., APE is able to surpass human performance when using the InstructGPT model...\\\""
    },
    {
        "paper": "arXiv-2305.03495v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the average improvement in prompt performance achieved by the ProTeGi method as compared to the Monte-Carlo baseline, according to the paper \\\"Automatic Prompt Optimization with 'Gradient Descent' and Beam Search\\\"?",
        "answer": "The ProTeGi method achieved an average improvement of 3.9% over the Monte-Carlo baseline.",
        "reference": "\\\"Automatic Prompt Optimization with 'Gradient Descent' and Beam Search\\\", Experimental Results, \\\"On average, ProTeGi improved over the MC and RL baselines by a significant 3.9% and 8.2% margin, respectively.\\\""
    },
    {
        "paper": "arXiv-2305.03495v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How much did the proposed ProTeGi algorithm improve the starting prompt's performance on average across all datasets considered in the study?",
        "answer": "The proposed ProTeGi algorithm improved the starting prompt's performance by an average of 15.3%.",
        "reference": "\\\"Automatic Prompt Optimization with 'Gradient Descent' and Beam Search\\\", Experimental Results, \\\"On average, ProTeGi improved over the original prompt $p_0$ by 15.3% and AutoGPT by 15.2%.\\\""
    },
    {
        "paper": "arXiv-2309.03409v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What improvement does Optimization by PROmpting (OPro) achieve on the GSM8K dataset compared to human-designed prompts?",
        "answer": "Optimization by PROmpting (OPro) improves task accuracy by up to 8% on the GSM8K dataset compared to human-designed prompts. The best instruction generated by OPro achieved an accuracy of 80.2%, while the baseline human-designed prompt \\\"Let's think step by step.\\\" achieved 71.8%.",
        "reference": "Title: Large Language Models as Optimizers, Section: Abstract - \\\"With a variety of LLMs, we demonstrate that the best prompts optimized by OPro outperform human-designed prompts by up to $8\\\\%$ on GSM8K...\\\"\\\\nSection: Prompt Optimization Experiments, Table 3 - \\\"Take a deep breath and work on this problem step-by-step.\\\" with Acc = 80.2%"
    },
    {
        "paper": "arXiv-2309.03409v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the performance of various LLM optimizers in Optimization by PROmpting (OPro) on the Traveling Salesman Problem (TSP) compare when the number of nodes is 10?",
        "answer": "For the Traveling Salesman Problem with 10 nodes, all LLM optimizers (\\\\texttt{text-bison}, \\\\texttt{gpt-3.5-turbo}, and \\\\texttt{gpt-4}) achieved an optimality gap of 0.0%, indicating that they found the optimal solution. However, the number of steps to reach the solution varied, with \\\\texttt{gpt-4} requiring the fewest average steps at 9.6 steps, followed by \\\\texttt{text-bison} at 40.4 steps, and \\\\texttt{gpt-3.5-turbo} at 46.8 steps.",
        "reference": "Title: Large Language Models as Optimizers, Section: Motivating Example: Mathematical Optimization, Table 3 - \\\"Results of the Traveling Salesman Problem (TSP) with different number of nodes $n$, where each $n$ contains 5 problems...\\\""
    }
]