[
    {
        "paper": "arXiv-2108.11149v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the composition of the multimodal dataset introduced in the paper \\\"A Unified Taxonomy and Multimodal Dataset for Events in Invasion Games\\\", specifically in terms of the distribution of event types for soccer and handball?",
        "answer": "The multimodal dataset for soccer includes 113.4 game status events, 362.6 ball reception events, 550.2 ball release events, 500.0 intentional ball releases, 50.2 unintentional ball releases, 12.2 shot events, 32.0 successful interference events, 385.2 successful passes, and 58.8 intercepted passes, among other events. The multimodal dataset for handball includes 135.7 game status events, 821.0 ball reception events, 844.7 ball release events, 841.0 intentional ball releases, 3.7 unintentional ball releases, 62.7 shot events, 3.0 successful interference events, 765.7 successful passes, and 6.7 intercepted passes, among other events.",
        "reference": "\\\"A Unified Taxonomy and Multimodal Dataset for Events in Invasion Games\\\", Table 2."
    },
    {
        "paper": "arXiv-2108.11149v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the human performance in annotating game status events compare between soccer and handball in the study titled \\\"A Unified Taxonomy and Multimodal Dataset for Events in Invasion Games\\\"?",
        "answer": "For the human performance annotation, the temporal IoU for game status events in soccer is 0.92 ± 0.01, which shows a higher agreement compared to handball, where the temporal IoU is 0.68 ± 0.02.",
        "reference": "\\\"A Unified Taxonomy and Multimodal Dataset for Events in Invasion Games\\\", Table 3."
    },
    {
        "paper": "arXiv-2401.01505v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How many question-answer pairs are contained in the Sports-QA dataset?",
        "answer": "The Sports-QA dataset comprises approximately 94K QA pairs.",
        "reference": "Title: 'Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex and Professional Sports' - Introduction section. '...comprising approximately 94K QA pairs...'."
    },
    {
        "paper": "arXiv-2401.01505v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the number of videos for each sport included in the Sports-QA dataset?",
        "answer": "The total number of videos in the Sports-QA dataset is 5,967, distributed across different sports as follows: basketball (879), football (1,030), volleyball (586), gym (505), vault (501), uneven bars (834), floor exercise (770), and balance beam (862).",
        "reference": "Title: 'Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex and Professional Sports' - Dataset Statistics section. 'The total number of videos in our dataset is 5,967, distributed across different sports as follows: basketball (879), football (1,030), volleyball (586), gym (505), vault (501), uneven bars (834), floor exercise (770), and balance beam (862).'"
    },
    {
        "paper": "arXiv-2404.06332v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How many video clips and questions are included in the SoccerNet-XFoul dataset introduced in the paper \\\"X-VARS: Introducing Explainability in Football Refereeing with Multi-Modal Large Language Models\\\"?",
        "answer": "The SoccerNet-XFoul dataset contains 10,000 video clips and over 22,000 questions.",
        "reference": "\\\"We introduce the E\\\\textbf{\\\\underline{X}}plainable \\\\textbf{\\\\underline{V}}ideo \\\\textbf{\\\\underline{A}}ssistant \\\\textbf{\\\\underline{R}}eferee \\\\textbf{\\\\underline{S}}ystem, X-VARS... We validate X-VARS on our novel dataset, \\\\emph{SoccerNet-XFoul}, which consists of more than $22$k video-question-answer triplets annotated by over $70$ experienced football referees.\\\" — Abstract"
    },
    {
        "paper": "arXiv-2404.06332v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What were the evaluation scores for X-VARS compared to human referees during the human study conducted in the paper \\\"X-VARS: Introducing Explainability in Football Refereeing with Multi-Modal Large Language Models\\\"?",
        "answer": "In the human study, X-VARS had a mean score of 3.8, comparable to the human referees who had a mean score of 4.0. In 46% of the cases, X-VARS explanations were rated higher than human referees. Their distribution was similar, with X-VARS having 3% strongly disagree, 17% disagree, 4% neutral, 46% agree, and 30% strongly agree compared to human referees.",
        "reference": "Table 3: 'Score and distributions obtained during our human study comparing the quality of referees and X-VARS generated explanations.'"
    }
]