[
    {
        "paper": "arXiv-2305.14283v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the performance improvement of using the \\\"Rewrite-Retrieve-Read\\\" framework in the open-domain QA task on the HotpotQA dataset compared to the standard \\\"retrieve-then-read\\\" method?",
        "answer": "The \\\"Rewrite-Retrieve-Read\\\" framework shows an Exact Match (EM) improvement from 30.47% with the standard \\\"retrieve-then-read\\\" to 34.38% with a trainable rewriter. For the F1 score, the improvement is from 41.34% to 45.97%.",
        "reference": "Title: Query Rewriting for Retrieval-Augmented Large Language Models, Section: Experiments, Table: Metrics of open-domain QA."
    },
    {
        "paper": "arXiv-2305.14283v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the \\\"Rewrite-Retrieve-Read\\\" framework perform on the MMLU multiple-choice QA dataset in the \\\"Humanities\\\" category, using ChatGPT as the reader, compared to the direct prompting approach?",
        "answer": "In the \\\"Humanities\\\" category of the MMLU dataset using ChatGPT as the reader, the \\\"Rewrite-Retrieve-Read\\\" framework achieves an Exact Match (EM) of 77.0%, compared to 75.6% for the direct prompting approach.",
        "reference": "Title: Query Rewriting for Retrieval-Augmented Large Language Models, Section: Experiments, Table: Metrics of multiple-choice QA."
    },
    {
        "paper": "arXiv-2310.05029v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the performance of the proposed \\\\sysname{} method compared to the baselines on the long version of the QuALITY dataset?",
        "answer": "On the long version of the QuALITY dataset, the \\\\sysname{} method achieves an accuracy of 73.6%. This outperforms the 'Recurrence' baseline with an accuracy of 56.0%, the 'Retrieval' baseline with an accuracy of 64.8%, the 'Full Context (keep left)' baseline with an accuracy of 64.8%, and the 'Full Context (keep right)' baseline with an accuracy of 72.5%.",
        "reference": "\\\"Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading\\\", see Table 1, which presents the empirical results for QuALITY under the column 'Long'."
    },
    {
        "paper": "arXiv-2310.05029v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does \\\\sysname{} handle long sequences more efficiently than full context baselines according to the provided performance breakdown by input sequence length?",
        "answer": "\\\\sysname{} handles long sequences more efficiently than full context baselines, as shown by the performance breakdown by input sequence length. For longer sequences, \\\\sysname{} outperforms both 'Full Context (keep left)' and 'Full Context (keep right)' baselines across all three tasks. Specifically, the performance gains of \\\\sysname{} over the baselines are notable when the text length is sufficiently larger than the LLM context length of 4,096 tokens. This shows \\\\sysname{}'s advantage in memory access on longer sequences.",
        "reference": "\\\"Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading\\\", see Figure 3 for the performance breakdown by context length, demonstrating \\\\sysname{}'s superior handling of long sequences."
    },
    {
        "paper": "arXiv-2403.05676v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the maximum speedup achieved by the PipeRAG method compared to the previous method \\\\textsc{Retro}?",
        "answer": "The maximum speedup achieved by the PipeRAG method compared to the previous method \\\\textsc{Retro} is 2.6 times in end-to-end generation latency.",
        "reference": "Title: \\\"PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design\\\"; Section: Abstract. \\\"PipeRAG achieves up to 2.6× speedup in end-to-end generation latency while improving generation quality.\\\""
    },
    {
        "paper": "arXiv-2403.05676v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does PipeRAG perform in terms of perplexity reduction compared to \\\\textsc{Retro} while maintaining the same latency?",
        "answer": "PipeRAG reduces perplexity by as much as 0.93 compared to \\\\textsc{Retro} while maintaining the same latency.",
        "reference": "Title: \\\"PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design\\\"; Section: Our Approach: PipeRAG. \\\"PipeRAG can achieve up to 2.6× speedup in latency without compromising perplexity; alternatively, maintaining the same latency allows PipeRAG to reduce perplexity by as much as 0.93 compared to \\\\textsc{Retro}.\\\""
    }
]