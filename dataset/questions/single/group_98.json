[
    {
        "paper": "arXiv-2301.11325v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the composition of the MusicCaps dataset introduced in the paper \\\"MusicLM: Generating Music From Text\\\"?",
        "answer": "The MusicCaps dataset introduced in the paper is composed of 5.5k music-text pairs with rich text descriptions provided by human experts. Each 10-second music clip is paired with a free-text caption consisting of four sentences on average, and a list of music aspects averaging eleven per clip.",
        "reference": "\\\"MusicLM: Generating Music From Text\\\" - Section Abstract: \\\"To support future research, we publicly release {\\\\dataset}, a dataset composed of 5.5k~music-text pairs, with rich text descriptions provided by human experts.\\\" Section Evaluation Dataset: \\\"This dataset includes 5.5k~music clips from AudioSet, each paired with corresponding text descriptions in English, written by ten professional musicians. For each 10-second music clip, \\\\datasetshort provides: (1)~a free-text \\\\emph{caption} consisting of four sentences on average, describing the music and (2)~a list of music \\\\emph{aspects}, describing genre, mood, tempo, singer voices, instrumentation, dissonances, rhythm, etc. On average, the dataset includes eleven aspects per clip.\\\""
    },
    {
        "paper": "arXiv-2301.11325v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does MusicLM perform compared to Mubert and Riffusion according to the evaluation metrics provided in the paper \\\"MusicLM: Generating Music From Text\\\"?",
        "answer": "According to the evaluation metrics, MusicLM outperforms Mubert and Riffusion across multiple metrics. Specifically, it achieves a FAD$_{\\\\text{VGG}}$ score of 4.0, which is lower than both Mubert (9.6) and Riffusion (13.4), and a FAD$_{\\\\text{Trill}}$ score of 0.44, which is better than Riffusion (0.76) and similar to Mubert (0.45). Additionally, MusicLM scores better on KLD (1.01) and MCC (0.51) compared to Mubert and Riffusion. Furthermore, in human listening tests, MusicLM received 312 wins compared to 97 for Mubert and 158 for Riffusion.",
        "reference": "\\\"MusicLM: Generating Music From Text\\\" - Section Results: Table 1: \\\"Evaluation of generated samples using captions from the \\\\dataset dataset. Models are compared... Riffusion: FAD$_{\\\\text{Trill}}$ 0.76, FAD$_{\\\\text{VGG}}$ 13.4, KLD 1.19, MCC 0.34, Wins 158; Mubert: FAD$_{\\\\text{Trill}}$ 0.45, FAD$_{\\\\text{VGG}}$ 9.6, KLD 1.58, MCC 0.32, Wins 97; \\\\model{}: FAD$_{\\\\text{Trill}}$ 0.44, FAD$_{\\\\text{VGG}}$ 4.0, KLD 1.01, MCC 0.51, Wins 312.\\\""
    },
    {
        "paper": "arXiv-2302.03917v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the performance of Noise2Music's waveform model compared to MusicLM in terms of FAD and MuLan similarity on the \\\\\\\\EvalSet dataset?",
        "answer": "The Noise2Music waveform model performance metrics compared to MusicLM on \\\\\\\\EvalSet are as follows: For FAD, the Noise2Music waveform has a FAD\\\\textsubscript{VGG}=2.134, FAD\\\\textsubscript{Trill}=0.405, and FAD\\\\textsubscript{MuLan}=0.110. In MuLan similarity metrics, the Noise2Music waveform audio \\\\leftrightarrow ground truth audio is 0.489, while MusicLM audio \\\\leftrightarrow ground truth text is 0.51.",
        "reference": "Noise2Music: Text-conditioned Music Generation with Diffusion Models, Section 'Evaluation results', Tables 7 and 8."
    },
    {
        "paper": "arXiv-2302.03917v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How many hours of annotated audio data were added for high-quality training in Noise2Music, and what contribution did they make to the total training data?",
        "answer": "Noise2Music included approximately 300 hours of high-quality annotated audio as part of the training data. This contribution was part of the larger dataset, which constitutes nearly 340k hours of music but specifically emphasized high-quality annotations from a subset of music tracks requiring no attribution.",
        "reference": "Noise2Music: Text-conditioned Music Generation with Diffusion Models, Section 'Training data mining': \\\"This contributes O(300) hours of annotated audio to our training data.\\\""
    },
    {
        "paper": "arXiv-2410.20478v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the Frechet Audio Distance (FAD) of MusicFlow on the MusicCaps dataset, and how does it compare to other models?",
        "answer": "The Frechet Audio Distance (FAD) of MusicFlow on the MusicCaps dataset is 2.82, which is lower than the FAD of other models like MusicLM (4.00), MusicGen (3.40), AudioLDM-2 (3.13), and UniAudio (3.65). Only Noise2Music and JEN-1 have a slightly lower FAD, at 2.10 and 2.00, respectively. So, MusicFlow shows competitive performance in terms of FAD among recent models.",
        "reference": "MusicFlow: Cascaded Flow Matching for Text Guided Music Generation, Section 'Main Results', Table 1."
    },
    {
        "paper": "arXiv-2410.20478v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the components of the two-stage MusicFlow model, and how many parameters does each stage contain?",
        "answer": "The two-stage MusicFlow model consists of two flow-matching networks. The first stage, responsible for music semantic flow matching from text, has 84 million parameters. The second stage, responsible for music acoustic flow matching from text and semantics, has 246 million parameters.",
        "reference": "MusicFlow: Cascaded Flow Matching for Text Guided Music Generation, Section 'Subsection: Effect of model size' and Table 7."
    }
]