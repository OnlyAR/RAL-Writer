[
    {
        "paper": "arXiv-2306.08568v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the performance improvement of \\\\modelname{} compared to other open-source models on the HumanEval benchmark?",
        "answer": "\\\\modelname{} shows a performance improvement in pass@1 scores with an increase of +22.3 percentage points over the previous best on the HumanEval benchmark, achieving a pass@1 rate of 57.3% compared to 35.0% from InstructCodeT5+.",
        "reference": "The paper titled \\\"\\\" in the section \\\"Evaluation on HumanEval, HumanEval+, and MBPP\\\" states: \\\"Specifically, we observe a substantial improvement in pass@1 scores, with an increase of +22.3 (57.3 vs. 35.0) in HumanEval.\\\""
    },
    {
        "paper": "arXiv-2306.08568v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How many samples are in the evolved dataset used to fine-tune \\\\modelname{}?",
        "answer": "The evolved dataset used to fine-tune \\\\modelname{} consists of approximately 78,000 samples.",
        "reference": "The paper titled \\\"\\\" in the section \\\"Implementation Details\\\" states: \\\"The evolved dataset consists of approximately 78k samples.\\\""
    },
    {
        "paper": "arXiv-2306.11644v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the parameter size and pass@1 accuracy of the phi-1 model, and how does it compare to other models in Table 1 of \\\"Textbooks Are All You Need\\\"?",
        "answer": "The phi-1 model has a parameter size of 1.3 billion and achieves a pass@1 accuracy of 50.6% on HumanEval and 55.5% on MBPP. When compared to other models in Table 1, phi-1 outperforms most of them despite being smaller in size and trained on fewer tokens. For instance, StarCoder, which has 15.5 billion parameters, achieves a pass@1 accuracy of 33.6% on HumanEval and 52.7% on MBPP, which is lower than phi-1's scores.",
        "reference": "\\\"Table 1 in 'Textbooks Are All You Need' presents the parameter size, dataset size, and pass@1 accuracies of various models, including phi-1.\\\"\\\\n\\\\n\\\"In Table 1, phi-1 is listed with 1.3B parameters, 7B training tokens, and achieves 50.6% on HumanEval and 55.5% on MBPP.\\\"\\\\n\\\\n\\\"...phi-1 outperforms competing models on HumanEval and MBPP, except for GPT-4...\\\" (See the Table on page 3)\\\""
    },
    {
        "paper": "arXiv-2306.11644v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the performance of phi-1-small compare to phi-1 on the HumanEval benchmark, according to \\\"Textbooks Are All You Need\\\"?",
        "answer": "The phi-1-small model, with 350 million parameters, achieves a pass@1 accuracy of 45% on the HumanEval benchmark. In comparison, the phi-1 model with 1.3 billion parameters achieves a higher pass@1 accuracy of 50.6%. This indicates that phi-1 performs better than phi-1-small on the HumanEval benchmark.",
        "reference": "\\\"Textbooks Are All You Need,\\\" Abstract: \\\"...phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.\\\" \\\\n\\\\nAlso in the Abstract: \\\"...phi-1 attains pass@1 accuracy 50.6% on HumanEval...\\\""
    },
    {
        "paper": "arXiv-2312.02120v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the performance improvement of Magicoder compared to the base model Code Llama-7B on the HumanEval+ benchmark?",
        "answer": "Magicoder (referred to as \\\\modelc-7B in the paper) shows a performance improvement of 21.4 percentage points compared to the base model Code Llama-7B on the HumanEval+ (\\\\text{pass@1}) benchmark, improving from 34.1% to 55.5%.",
        "reference": "Magicoder: Empowering Code Generation with OSS-Instruct \\\\ Section: Python Text-to-Code Generation, Table 3 \\\"\\\\modelc{} & 7B & 60.4~~(55.5)...\\\\codellamapy & Aug 2023 & 7B & 37.8~~(34.1)...\\\""
    },
    {
        "paper": "arXiv-2312.02120v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How many different programming languages were the code snippets collected from to create the seed corpus for the \\\\\\\\tech method in the Magicoder paper?",
        "answer": "The seed corpus for the \\\\\\\\tech method was collected from code snippets in various programming languages, specifically: 40K from Python, and 5K each from C++, Java, TypeScript, Shell, C#, Rust, PHP, and Swift, totaling data from 9 different programming languages.",
        "reference": "Magicoder: Empowering Code Generation with OSS-Instruct \\\\ Section: Generating Coding Problems, \\\"In total, we collected 80K initial seed snippets from 80K code documents, 40K from Python, and 5K from each of C++, Java, TypeScript, Shell, C\\\\#, Rust, PHP, and Swift respectively.\\\""
    }
]