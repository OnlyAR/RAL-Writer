[
    {
        "paper": "arXiv-2304.11015v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the performance improvement of the DIN-SQL approach using GPT-4 over previous state-of-the-art execution accuracy on the Spider dataset test set?",
        "answer": "DIN-SQL achieves an execution accuracy of 85.3% on the holdout test set of Spider, which surpasses the previous state-of-the-art execution accuracy of 79.9%, resulting in a performance improvement of 5.4%.",
        "reference": "Title: DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction, Section: Abstract. \\\"On the holdout test set of Spider, the SOTA, in terms of execution accuracy, was 79.9 and the new SOTA at the time of this writing using our approach is 85.3.\\\""
    },
    {
        "paper": "arXiv-2304.11015v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How did the DIN-SQL approach perform on the BIRD benchmark compared to existing methods in terms of execution accuracy?",
        "answer": "The DIN-SQL approach achieved an execution accuracy of 55.9% on the BIRD benchmark holdout test set, setting a new state-of-the-art compared to other methods.",
        "reference": "Title: DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction, Section: Abstract. \\\"Additionally, when evaluated on the BIRD benchmark, our approach achieved an execution accuracy of 55.9\\\\%, setting a new SOTA on its holdout test set.\\\""
    },
    {
        "paper": "arXiv-2308.15363v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the execution accuracy of the proposed \\\\ours method on the Spider leaderboard as stated in the paper?",
        "answer": "The \\\\ours method achieved an execution accuracy of 86.6% on the Spider leaderboard.",
        "reference": "\\\\nlsql Empowered by Large Language Models: A Benchmark Evaluation, Abstract, \\\"our solution sets a new bar, and hope our comprehensive study will inspire more further works.\\\""
    },
    {
        "paper": "arXiv-2308.15363v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "According to the paper, how does \\\\ours compare against the previous state-of-the-art performance on the Spider leaderboard in terms of execution accuracy?",
        "answer": "The \\\\ours method surpasses the previous state-of-the-art performance on the Spider leaderboard by 1.3%, as the previous best was 85.3% execution accuracy.",
        "reference": "\\\\nlsql Empowered by Large Language Models: A Benchmark Evaluation, Abstract, \\\"refreshes the Spider leaderboard with 86.6\\\\% execution accuracy and sets a new bar\\\" and \\\"Before \\\\ours, the state-of-the-art performance in the Spider leaderboard is 85.3\\\\%.\\\""
    },
    {
        "paper": "arXiv-2312.11242v5.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What execution accuracy did the MAC-SQL+GPT-4 framework achieve on the BIRD benchmark, according to the paper \\\"MAC-SQL: A Multi-Agent Collaborative Framework for Text-to-SQL\\\"?",
        "answer": "According to the paper \\\"MAC-SQL: A Multi-Agent Collaborative Framework for Text-to-SQL\\\", the MAC-SQL+GPT-4 framework achieved an execution accuracy of 59.59 on the BIRD benchmark, establishing a new state-of-the-art (SOTA) on its holdout test set.",
        "reference": "Paper: \\\"MAC-SQL: A Multi-Agent Collaborative Framework for Text-to-SQL\\\", Abstract: \\\"At the time of writing, MAC-SQL+GPT-4 achieves an execution accuracy of 59.59 when evaluated on the BIRD benchmark, establishing a new state-of-the-art (SOTA) on its holdout test set.\\\""
    },
    {
        "paper": "arXiv-2312.11242v5.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the execution accuracy of the SQL-Llama(7B) model compare to GPT-4 on the development set of the BIRD dataset as reported in the paper \\\"MAC-SQL: A Multi-Agent Collaborative Framework for Text-to-SQL\\\"?",
        "answer": "The SQL-Llama(7B) model achieved an execution accuracy of 43.94 on the development set of the BIRD dataset, which is close to the baseline accuracy of 46.35 for vanilla GPT-4.",
        "reference": "Paper: \\\"MAC-SQL: A Multi-Agent Collaborative Framework for Text-to-SQL\\\", Abstract: \\\"Experiments show that SQL-Llama achieves a comparable execution accuracy of 43.94, compared to the baseline accuracy of 46.35 for vanilla GPT-4.\\\""
    }
]