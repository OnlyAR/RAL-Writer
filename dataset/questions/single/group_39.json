[
    {
        "paper": "arXiv-2212.14052v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the performance of the \\\\hthree-attention hybrid model compared to Transformers on the OpenWebText dataset?",
        "answer": "The \\\\hthree-attention hybrid model outperforms Transformers on OpenWebText by 1.0 perplexity point. Specifically, the \\\\hthree-attention hybrid achieved a perplexity of 19.6, while the Transformer model achieved a perplexity of 20.6.",
        "reference": "\\\"Hungry Hungry Hippos: Towards Language Modeling with State Space Models,\\\" Section titled \\\"Expressivity,\\\" Table titled \\\"Perplexity of SSM variants compared to Transformers on OpenWebText.\\\""
    },
    {
        "paper": "arXiv-2212.14052v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What speedup does \\\\fastfft provide over standard Transformers in the long range arena benchmark?",
        "answer": "\\\\fastfft provides a speedup of 5.8$\\\\times$ over standard Transformers on the long range arena benchmark. In comparison, standard S4 models previously achieved a 2.9$\\\\times$ speedup, and \\\\fastfft doubled this to 5.8$\\\\times$.",
        "reference": "\\\"Hungry Hungry Hippos: Towards Language Modeling with State Space Models,\\\" Section titled \\\"\\\\fastfft Evaluation,\\\" Table titled \\\"Speedup on the LRA benchmark.\\\""
    },
    {
        "paper": "arXiv-2312.06635v6.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "In the paper \\\"Gated Linear Attention Transformers with Hardware-Efficient Training\\\", how does the GLA Transformer perform in terms of training throughput compared to Mamba and Transformer++?",
        "answer": "According to the paper, the training throughput of the GLA Transformer is superior to that of Mamba at each measured sequence length (2048/8, 4096/4, 8192/2, 16284/1) and batch size. Specifically, at a sequence length of 2048 with a batch size of 8, GLA achieves 43.8 Kt/s, while Mamba achieves 22.8 Kt/s, and Transformer++ achieves 51.3 Kt/s. For the longest sequence length (16284) with a batch size of 1, GLA achieves 41.1 Kt/s compared to Mamba's 26.0 Kt/s and Transformer++'s 29.1 Kt/s.",
        "reference": "\\\"Gated Linear Attention Transformers with Hardware-Efficient Training\\\", Section \\\"Training Efficiency\\\", Fig. 5."
    },
    {
        "paper": "arXiv-2312.06635v6.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the WikiText perplexity scores for the 340M parameter models evaluated in the paper \\\"Gated Linear Attention Transformers with Hardware-Efficient Training\\\", and how does the GLA Transformer compare to other models?",
        "answer": "In the paper, the perplexity scores for the 340M parameter models on WikiText are: Transformer++ with a perplexity of 28.39, RetNet with a perplexity of 32.33, Mamba with a perplexity of 28.39, and GLA with a perplexity of 28.65. Therefore, GLA Transformer slightly underperforms compared to the Transformer++ and Mamba models, and outperforms RetNet.",
        "reference": "\\\"Gated Linear Attention Transformers with Hardware-Efficient Training\\\", Table 2."
    },
    {
        "paper": "arXiv-2405.05254v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the reduction in memory consumption of key-value (KV) caches achieved by the \\\\our{} architecture, and how does it compare to the Transformer in terms of inference memory complexity?",
        "answer": "The \\\\our{} architecture achieves a reduction in GPU memory consumption of KV caches by about $80\\\\times$ for 65B models compared to the Transformer. In terms of inference memory complexity of KV caches, the Transformer has \\\\(\\\\mathcal{O}(LND)\\\\), while \\\\our{} has \\\\(\\\\mathcal{O}((N+L)D)\\\\), where \\\\(N\\\\) is the sequence length, \\\\(L\\\\) is the number of layers, and \\\\(D\\\\) is the hidden dimension. This means \\\\our{} roughly saves $L$ times GPU memory for caches.",
        "reference": "\\\"\\\\section{You Only Cache Once (YOCO)}\\\" - \\\"We conduct extensive experiments to show that \\\\our{} achieves favorable language modeling performance and has many advantages in terms of inference efficiency. In particular, the memory of KV caches can be reduced by about $80\\\\times$ for 65B models. Even for a 3B model, the overall inference memory consumption can be reduced by two times\\\"; \\\\tablename{} 1: \\\"Inference memory complexity of KV caches. Transformer: \\\\(\\\\mathcal{O}(LND)\\\\); \\\\our{}: \\\\(\\\\mathcal{O}((N+L)D)\\\\);\\\"."
    },
    {
        "paper": "arXiv-2405.05254v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "In the paper \\\"You Only Cache Once: Decoder-Decoder Architectures for Language Models\\\", how does the \\\\our{} model's prefill latency compare to Transformer's for different context lengths and what is the computational complexity involved?",
        "answer": "For the 512K context length, the \\\\our{} model reduces the Transformer's prefill latency from 180 seconds to less than 6 seconds, achieving a speedup. Even for a 32K context length, \\\\our{} can accelerate by $2.87\\\\times$. The computational complexity of attention modules during the prefilling stage is \\\\(\\\\mathcal{O}(LN^2D)\\\\) for the Transformer and \\\\(\\\\mathcal{O}(LND)\\\\) for \\\\our{}, where \\\\(N\\\\) is the sequence length, \\\\(L\\\\) is the number of layers, and \\\\(D\\\\) is the hidden dimension.",
        "reference": "\\\"Prefilling latency for different length, i.e., the encoding time of given input prompt before generating the first token. Transformer's time grows quadratically while \\\\our{}'s grows linearly. Even for a short input length, such as 32K, \\\\our{} can still accelerate $2.87\\\\times$.\\\"; \\\\tablename{} 2: \\\"Prefilling time complexity of attention modules. Transformer: \\\\(\\\\mathcal{O}(LN^2D)\\\\); \\\\our{}: \\\\(\\\\mathcal{O}(LND)\\\\);\\\"."
    }
]