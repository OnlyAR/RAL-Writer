[
    {
        "paper": "arXiv-2109.07644v5.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the main numerical components of the OPV2V dataset introduced in the paper \\\"OPV2V: An Open Benchmark Dataset and Fusion Pipeline for Perception with Vehicle-to-Vehicle Communication\\\"?",
        "answer": "The OPV2V dataset contains over 70 scenes, 11,464 frames, and 232,913 annotated 3D vehicle bounding boxes. The dataset is collected from 8 towns in CARLA and a digital town of Culver City, Los Angeles. The overall file size of the dataset is 249.4 GB. It features an average of approximately 3 connected vehicles per frame, with a minimum of 2 and a maximum of 7 connected vehicles in each frame.",
        "reference": "\\\"Section: INTRODUCTION\\\" and \\\"Section: OPV2V Dataset > Data Collection\\\" - \\\"It contains over 70 interesting scenes, 11,464 frames, and 232,913 annotated 3D vehicle bounding boxes, collected from 8 towns in CARLA and a digital town of Culver City, Los Angeles.\\\" \\\"Overall, 11,464 frames (i.e. time steps) of LiDAR point clouds~(see Fig.~\\\\ref{fig:overview}) and RGB images~(see Fig.~\\\\ref{fig:camera_example}) are collected with a total file size of 249.4 GB.\\\" \\\"Our dataset has on average approximately 3 connected vehicles with a minimum of 2 and a maximum of 7 in each frame.\\\""
    },
    {
        "paper": "arXiv-2109.07644v5.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the Attentive Intermediate Fusion pipeline compare in performance with other fusion strategies for V2V perception based on the results reported in the paper \\\"OPV2V: An Open Benchmark Dataset and Fusion Pipeline for Perception with Vehicle-to-Vehicle Communication\\\"?",
        "answer": "The Attentive Intermediate Fusion pipeline generally achieves the best performance among the evaluated fusion strategies (early, late, no fusion) across different 3D LiDAR detectors (SECOND, VoxelNet, PIXOR, and PointPillar). For example, for the SECOND detector, Intermediate Fusion achieves an AP of 0.893 at an IoU of 0.7 in the Default CARLA Town setting, which outperforms Late Fusion (0.775) and Early Fusion (0.813). Similarly, in the Culver City setting, Intermediate Fusion achieves 0.875 while Late Fusion and Early Fusion have AP scores of 0.808 and 0.821, respectively.",
        "reference": "\\\"Section: EXPERIMENTS > Benchmark Analysis\\\" - Table~\\\\ref{table:benchmark_analysis} shows the results indicating Intermediate Fusion usually having the highest AP scores among fusion strategies for different detectors across different settings."
    },
    {
        "paper": "arXiv-2202.08449v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What are the empirical results for the collaborative object detection task using the V2X-Sim dataset?",
        "answer": "The empirical results for the collaborative object detection task using the V2X-Sim dataset demonstrate that DiscoNet achieves AP of 69.03 at IoU=0.5 and 63.44 at IoU=0.7 without RSU, which improves to 72.87 and 66.40 respectively with RSU. V2VNet follows closely with AP of 68.35 at IoU=0.5 and 62.83 at IoU=0.7 without RSU, improving to 72.08 and 65.85 with RSU.",
        "reference": "Title: V2X-Sim: Multi-Agent Collaborative Perception Dataset and Benchmark for Autonomous Driving; Section: Collaborative Perception Benchmark; Table: Table 1."
    },
    {
        "paper": "arXiv-2202.08449v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the sensor configuration for vehicles and RSU in the V2X-Sim dataset?",
        "answer": "In the V2X-Sim dataset, each vehicle is equipped with 6 RGB cameras, 6 depth cameras, and 6 semantic cameras, all covering a 360° view. Additionally, both vehicles and RSU have a BEV semantic camera and a LiDAR catcher. The RSU has 4 RGB, depth, and semantic cameras, each looking downward at a 35° angle. The LiDAR specifications include a maximum range of 70m, 32 channels, with 250,000 points per second, and a 20 Hz rotation frequency.",
        "reference": "Title: V2X-Sim: Multi-Agent Collaborative Perception Dataset and Benchmark for Autonomous Driving; Section: V2X-Sim Dataset; Subsection: Sensor suite on vehicles and RSU; Table: Sensor suite of vehicle (V) and intersection (I)."
    },
    {
        "paper": "arXiv-2206.08367v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the distribution of discrete domain shifts in the SHIFT dataset?",
        "answer": "In the SHIFT dataset, the discrete set contains 4,250 sequences generated with fixed environmental parameters and random initial states. Each sequence is composed of 500 frames collected at 10 Hz, equivalent to 50 seconds of driving time. The dataset provides domain shifts in weather conditions (cloudiness, rain, fog intensity), time of day, the density of vehicles and pedestrians, and camera orientation. Each possible domain combination is uniformly distributed across all sequences.",
        "reference": "The SHIFT dataset: A Synthetic Driving Dataset for Continuous Multi-Task Domain Adaptation, Section 3, \\\"Dataset Design\\\""
    },
    {
        "paper": "arXiv-2206.08367v1.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the SHIFT dataset compare to other synthetic driving datasets in terms of size and annotation?",
        "answer": "The SHIFT dataset is the largest synthetic driving dataset available, featuring 70+ hours of driving and 2,500,000 annotated frames. It supports 13 perception tasks, including semantic/instance segmentation, depth regression, 2D/3D object detection, and optical flow estimation. Comparatively, other synthetic datasets such as SYNTHIA and AIODrive offer fewer annotated frames, with SYNTHIA having 9,000 annotated frames and AIODrive having 100,000. Additionally, SHIFT is the only dataset providing realistic continuous domain shifts and includes the most inclusive set of tasks and conditions.",
        "reference": "The SHIFT dataset: A Synthetic Driving Dataset for Continuous Multi-Task Domain Adaptation, Table 1, \\\"Comparison of size and supported tasks of existing driving datasets.\\\""
    }
]