[
    {
        "paper": "arXiv-2308.06259v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the data scaling efficiency measure for the 'Humpback' method from the 'Self-Alignment with Instruction Backtranslation' paper?",
        "answer": "The data scaling efficiency measure for the 'Humpback' method, which utilizes self-augmentation and self-curation with $k=5$, 2 iterations, is given by the scaling coefficient $\\\\alpha$ of 6.95. This value is higher than other methods such as Alpaca-GPT4 (5.40) and WizardLLM (5.69) indicating a more efficient data scaling.",
        "reference": "Paper Title: Self-Alignment with Instruction Backtranslation, Section: Data scaling efficiency, \\\"Scaling up augmented data using the $\\\\mathcal{A}_5$ data achieved both higher instruction following performance and more efficient data scaling.\\\""
    },
    {
        "paper": "arXiv-2308.06259v3.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the 'Humpback' model from the 'Self-Alignment with Instruction Backtranslation' paper compare to other models on the Alpaca leaderboard in terms of win rate?",
        "answer": "The 'Humpback' model outperforms all other non-distilled models on the Alpaca leaderboard. Specifically, at 33B scale it achieves a win rate of 79.84%, outperforming models like Guanaco (65.96%) and OASST RLHF (66.52%). At 65B scale, it achieves an even higher win rate of 83.71%, compared to Guanaco's 71.80% and LIMA's 62.70%.",
        "reference": "Paper Title: Self-Alignment with Instruction Backtranslation, Section: AlpacaEval, Table: Results on the Alpaca leaderboard"
    },
    {
        "paper": "arXiv-2401.06477v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the size and composition of the dataset generated by the Kun method described in the paper \\\"Kun: Answer Polishment for Chinese Self-Alignment with Instruction Back-Translation\\\"?",
        "answer": "The dataset generated by the Kun method consists of over a million diverse Chinese instructional data points. These data points were sourced from Wudao, Wanjuan, and SkyPile, and are used for instruction-tuning without relying on manual annotations.",
        "reference": "Title: Kun: Answer Polishment for Chinese Self-Alignment with Instruction Back-Translation. Section: Abstract. \\\"Kun leverages unlabelled data from diverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantial dataset of over a million Chinese instructional data points.\\\""
    },
    {
        "paper": "arXiv-2401.06477v4.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "How does the output quality vary across different data sources in the manual quality analysis of the Kun dataset mentioned in the paper \\\"Kun: Answer Polishment for Chinese Self-Alignment with Instruction Back-Translation\\\"?",
        "answer": "The output quality varies across different sources in the Kun dataset. The \\\"Excellent\\\" percentage for outputs is 69.50% for Wudao, 85.63% for Wanjuan, and 42.73% for SkyPile. This indicates that Wanjuan had the highest quality outputs in terms of the \\\"Excellent\\\" rating, while SkyPile had a more diverse distribution with a significant portion rated as \\\"Pass\\\" (40.43%) and \\\"Fail\\\" (16.84%).",
        "reference": "Title: Kun: Answer Polishment for Chinese Self-Alignment with Instruction Back-Translation. Section: Experiments, Quality Evaluation. Table: \\\"Manual Quality Analysis of Synthetic Data Generated by Kun.\\\""
    },
    {
        "paper": "arXiv-2408.04614v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What is the performance improvement of the proposed 'instruction back-and-forth translation' method compared to the previous work on ClueWeb data when fine-tuning Llama-2-70B?",
        "answer": "The proposed 'instruction back-and-forth translation' method improves the AlpacaEval win rate by 3.6% compared to the ClueWeb data from previous work when fine-tuning the Llama-2-70B model.",
        "reference": "Better Alignment with Instruction Back-and-Forth Translation, Section 1: \\\"Given the same data quantity, fine-tuning Llama-2-70B on the instruction-response pairs from our data generation pipeline improves the AlpacaEval win rate by 3.6% compared to using the backtranslation data from previous work.\\\""
    },
    {
        "paper": "arXiv-2408.04614v2.tex",
        "topic": "data",
        "subtopic": "data-related content",
        "question": "What was the subset of Dolma used for sourcing the initial web-crawled responses in 'instruction back-and-forth translation' and what proportion of this subset was removed due to length constraints?",
        "answer": "The Common Crawl subset of Dolma was used for sourcing the initial web-crawled responses, and about 25% of this subset was removed because their lengths were close to exceeding the context length of Llama-2.",
        "reference": "Better Alignment with Instruction Back-and-Forth Translation, Section 4.1: \\\"We additionally filter out documents whose lengths are close to exceeding the context length of Llama-2; this removes about 25% of the Common Crawl subset.\\\""
    }
]