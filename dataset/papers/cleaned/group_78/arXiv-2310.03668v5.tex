\title{GoLLIE: Annotation Guidelines Improve Zero-Shot Information Extraction}

\begin{document}

\maketitle

\begin{abstract}
Large Language Models (LLMs) combined with instruction tuning have made significant progress when generalizing to unseen tasks. However, they have been less successful in Information Extraction (IE), lagging behind task-specific models. Typically, IE tasks are characterized by complex annotation guidelines that describe the task and give examples to humans. 
Previous attempts to leverage such information have failed, even with the largest models, as they are not able to follow the guidelines out of the box. 
In this paper, we propose GoLLIE (\textbf{G}uideline-f\textbf{o}llowing \textbf{L}arge \textbf{L}anguage Model for \textbf{IE}), a model able to improve zero-shot results on unseen IE tasks by virtue of being fine-tuned to comply with annotation guidelines.
Comprehensive evaluation empirically demonstrates that GoLLIE is able to generalize to and follow unseen guidelines, outperforming previous attempts at zero-shot information extraction. The ablation study shows that detailed %representative annotation candidates to the 
guidelines are key for good results. 
Code, data, and models are publicly available: \url{https://github.com/hitz-zentroa/GoLLIE}. %\footnote{Upon publication.} to facilitate reproducibility of results and further research on this topic.

\end{abstract}

\section{Introduction}
\begin{figure}[b]
    \centering
    \includegraphics[width=0.8\linewidth]{images/zero_shot_results.pdf}
    \caption{Out of domain zero-shot NER results. GPT results are not available for all domains.}
    \label{fig:zero-shot-results}
\end{figure}

The task of Information Extraction (IE) is highly challenging. This challenge is evident in the detailed guidelines, which feature granular definitions and numerous exceptions, that human annotators must follow to perform the task. The performance of current SoTA models heavily depends on the quantity of human-annotated data, as the model learns the guidelines from these examples. However, this performance significantly decreases when tested in new annotation schema \citep{DBLP:conf/aaai/Liu0YDJCMF21}. The common practice in IE to achieve good results is to manually annotate each new domain and schema from scratch, as almost no transfer exists across application domains. Unfortunately, this is unfeasible, both, in terms of financial cost and human effort.

Recent advancements in Large Language Models (LLM) \citep{10.1145/3605943} have enabled the development of models capable of generalizing to unseen tasks. Thus, current zero-shot IE systems leverage the knowledge encoded in LLMs to annotate new examples \citep{sainz-etal-2022-textual,DBLP:journals/corr/abs-2304-08085}. As a by-product of the pre-training process, models possess now a strong representation of what a person or an organization is. Therefore, they can be prompted to extract mentions of those categories from a text. However, this has a clear limitation: not every annotation schema\footnote{We define schema as the set of labels and their definitions.} defines "person" (or any other label) in the same way. For example, ACE05~\citep{ACE} annotates pronouns as persons, while, CoNLL03 \citep{tjong-kim-sang-de-meulder-2003-introduction} does not. IE tasks require more information than just label names, they require annotation guidelines. 

Current LLMs have been trained to follow instructions, but they fail to follow annotation guidelines out of the box. 
For instance, Figure \ref{fig:zero-shot-results} shows results on domain-specific zero-shot Named Entity Recognition. The results of gpt-3.5-turbo when prompted with guidelines  \citep{DBLP:journals/corr/abs-2305-12217} are low, around 20 F1 score on Music or Politics domains. Building a system that enables high-performance zero-shot information extraction, reducing the dependence on costly human annotations, remains an open challenge.  

In this work, we present \GoLLIE GoLLIE  (\textbf{G}uideline-f\textbf{o}llowing \textbf{L}arge \textbf{L}anguage Model for \textbf{IE}), an LLM fine-tuned to learn how to attend to the guidelines on a small set of well known IE tasks. Comprehensive zero-shot evaluation empirically demonstrates that GoLLIE outperforms the SoTA~\citep{DBLP:journals/corr/abs-2304-08085}  in zero-shot information extraction (see Figure \ref{fig:zero-shot-results}). 

\section{Related Work}
Large Language Models (LLMs) have made significant advancements toward the development of systems that can generalize to unseen tasks~\citep{10.1145/3605943}. \citet{radford2019language} trained LLMs using a vast amount of internet data, finding that pre-trained models given natural language task descriptions can perform tasks such as question answering, machine translation, or summarizing without explicit supervision. Building on this discovery, instruction tuning, often referred to as multitask fine-tuning, has emerged as the leading method to achieve generalization to unseen tasks. This process involves pre-training a model on a massive amount of unlabeled data and subsequently fine-tuning it on a diverse collection of tasks \citep{DBLP:conf/emnlp/WangMAKMNADASPK22,DBLP:journals/corr/abs-2210-11416} phrased as text-to-text problems \citep{DBLP:journals/jmlr/RaffelSRLNMZLL20}. A natural language instruction or prompt is given to the model to identify the task it should solve \citep{DBLP:conf/eacl/SchickS21,DBLP:conf/naacl/ScaoR21}. Research has demonstrated that increasing the parameter count of the language model \citep{DBLP:conf/nips/BrownMRSKDNSSAA20}, coupled with improvements in the size and quality of the instruction tuning dataset, results in enhanced generalization capabilities \citep{DBLP:journals/corr/abs-2302-12692,DBLP:journals/corr/abs-2205-01068,DBLP:journals/corr/abs-2204-02311,DBLP:conf/acl/MuennighoffWSRB23,DBLP:journals/corr/abs-2302-13971,DBLP:journals/corr/abs-2307-09288}. LLMs have displayed impressive zero-shot generalization capabilities in various challenging tasks, including coding \cite{wang2021gpt,black-etal-2022-gpt,DBLP:journals/corr/abs-2308-12950}, common sense reasoning \cite{DBLP:journals/corr/abs-2302-13971}, and medical applications \cite{singhal2023large}, among others. 

In the field of Information Extraction (IE), recent shared tasks \citep{DBLP:conf/semeval/FetahuKCRM23} have shown that encoder-only language models such as XLM-RoBERTa \citep{DBLP:conf/acl/ConneauKGCWGGOZ20} and mDEBERTA \citep{DBLP:conf/iclr/HeGC23} remain the most effective models. Attempts to utilize LLMs and natural language instructions for IE have been less successful \citep{DBLP:conf/semeval/TanHJCL0ZTXH23,DBLP:journals/corr/abs-2308-03279,DBLP:journals/corr/abs-2305-12217}, as their performance lags behind that of encoder-only models. 
Before the billion parameters LLMs, indirectly supervised methods improve zero-shot IE by utilizing the knowledge learned from tasks like Textual Entailment~\citep{sainz-etal-2021-label, sainz-etal-2022-textual, sainz-etal-2022-zs4ie} and Question Answering~\citep{levy-etal-2017-zero}.
\cite{obeidat-etal-2019-description} propose an entity typing method that encodes label descriptions from Wikipedia as embeddings using an LSTM, which is then used to score the inputs. Methods that leveraged external knowledge were also successful on fine-grained zero-shot NER~\citep{chen-etal-2021-empirical}.
\cite{DBLP:conf/acl/0001LDXLHSW22} introduced a unified text-to-structure generation that can model different IE tasks universally. \cite{DBLP:conf/aaai/Lou0DJLH0023} proposed converting IE tasks to a semantic matching problem, allowing their method to generalize  to new domains and label ontologies not seen during training. \cite{DBLP:journals/corr/abs-2304-08085} framed IE tasks as natural language descriptive instructions and trained an LLM across a diverse range of IE tasks. In evaluations on tasks with unseen label ontologies, their model outperformed other instruction-tuning methods. 

Most instruction tuning attempts for IE share a limitation: they only consider label names in the prompts (e.g., \textit{"List all the Persons"}). This poses two major challenges. Firstly, not all datasets share the same definition for labels like \textit{Person} (some exclude fictional characters or pronouns). Secondly, a label name alone doesn't sufficiently describe complex or less common labels. While there have been attempts to prompt LLMs using guidelines~\citep{DBLP:journals/corr/abs-2305-12217}, strong prior knowledge of LLMs regarding task labels \citep{DBLP:conf/acl/BlevinsGZ23} deter the model from adhering to those guidelines. 

\section{Approach}

Different from previous approaches, \GoLLIE GoLLIE forces the model to attend to the details in the guidelines, performing robustly on schemas not seen during training. On this section we deep dive into the details of our approach, describing how the input and output was represented and the regularization techniques used to force the model to attend to the guidelines.

\subsection{Input-output representation}

\begin{figure}
    \centering
    \includegraphics[width=.8712\linewidth]{images/input-output_representation.pdf}
    \caption{Example of the input and output of the model.}
    \label{fig:input-output}
    \vspace{-0.8em}
\end{figure}

We have adopted a Python code-based representation~\citep{wang-etal-2023-code4struct, li-etal-2023-codeie} for both the input and output of the model. This approach not only offers a clear and human-readable structure but also addresses several challenges typically associated with natural language instructions. It enables the representation of any information extraction task under a unified format. The inputs can be automatically standardized using Python code formatters such as Black. The output is well-structured and parsing it is trivial. Furthermore, most current LLMs incorporate code in their pre-training datasets, indicating that these models are already familiar with this representation.

Figure~\ref{fig:input-output} shows the three main parts of the format: schema definition, input text, and output annotations. \textbf{Schema definition} forms the initial segment of the input. This section contains information about the labels that are represented as Python classes; guidelines, articulated as docstrings; and representative annotation candidates presented in the form of code comments. The number of class definitions corresponds to the number of labels in the dataset. Classes are flexible and vary for each task. For example, classes for a NER dataset merely require an attribute to specify the text span that corresponds to the class. On the other side, more complex tasks such as Event Argument Extraction (EAE) or Slot Filling (SF) demand more class attributes to categorize the task, such as a list of participants in an event (refer to examples in Appendix~\ref{ap:repr_examples}). \textbf{Input text} is the second part of the input. The input text is represented as a string variable in Python. \textbf{Output annotations} is the part generated by the model. The model starts generating after \texttt{result =}. The annotations are represented as a list of instances of the classes defined on the schema definition part. Parsing the output is straightforward; executing the generated code in Python yields a list containing the result. This ease of parsing the output stands as a significant advantage of our model. A further detailed analysis of the efficiency of this approach is available in Appendix \ref{sec:scalability}. 

\subsection{Guidelines enhanced representation}

\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{images/guideline_example.pdf}
    \caption{Example of the input representation. (left) An example of an event definition w/o guidelines information. (right) The same example but with guideline information as Python comments.}
    \vspace{-0.7em}
    \label{fig:guidelines-example}
\end{figure}

The main contribution of this work is the use of the guidelines as part of the inference process to improve the zero-shot generalization. An example of a class definition with and without guidelines is shown in Figure~\ref{fig:guidelines-example}. Different datasets usually define guidelines in many different ways: some provide a complex definition of a label with several exceptions and special treatments and others just give a few representative candidates of the fillers of the label. To normalize the input format, we included the label definitions as class docstrings and the candidates as a comment for the principal argument (which is usually \textit{mention} or \textit{span}). Complex tasks such as EAE or SF require additional definitions for the arguments or slots, to that end,  we included small definitions as comments on each class argument. 
In this paper, we will refer to the model without guidelines as Baseline and the model with guidelines as \GoLLIET.

\subsection{Training regularization}
\label{sec:components}

We want to ensure that the model follows the guidelines and does not just learn to identify specific datasets and perform correctly on them. To do this, we introduce various kinds of noise during training. This stops the model from recognizing particular datasets, recalling specific labels, or attending only to the label names rather than learning to follow the actual description for each label in the guidelines.

We applied the following regularizations. \textbf{Class order shuffling}, for each example, the order of the input classes is randomly shuffled. This makes it more difficult for the model to memorize entire task definitions. \textbf{Class dropout}, we delete some of the input classes randomly. By eliminating few classes from both the input and output, we force the model to learn to only output instances of classes defined in the input. This not only encourages the model to focus on the schema definition but also minimizes the occurrence of hallucinations during inference. \textbf{Guideline paraphrasing}, we generate variations of the label definitions to prevent the model from easily memorizing them. We also think this will make the method more robust to different variations on the definition. \textbf{Representative candidate sampling}, similar to what we do with the paraphrases, for each input we sample 5 different candidates from a fixed pool of 10 per class. \textbf{Class name masking} involves substituting the label class names (e.g., \textsc{Person}) with placeholders, such as \textsc{LABEL\_1}. This prevents the model from exploiting the label names during training and forces it to attend and understand the guidelines. 

\section{Experimental Setup}

\subsection{Data}

Evaluating zero-shot capabilities requires dividing the data into training and evaluation datasets. However, many benchmarks for Information Extraction are based on the same domain or share part of their schema. To ensure that the zero-shot evaluation is not affected by similar data, we have divided our set of benchmarks based on the domain of the data (a related topic is data contamination, which we discuss in Appendix~\ref{ap:contamination}). For training we kept mostly datasets from \textbf{News and Biomedical} domains, for evaluation instead, we used datasets from \textbf{diverse domains}. This approach helps to avoid introducing any noise into the evaluation process. Among the evaluation datasets we included CrossNER~\citep{liu2021crossner}, a dataset that is split into many domains, for simplicity, we will call each domain as a separate dataset: AI, Literature, Music, Politics, and Science. Also, we will refer to MIT Movie and MIT Restaurant as Movie and Restaurant. Table~\ref{tab:datasets} contains the information about the data used in the experiments. 

\begin{table}
    \centering
    \caption{Datasets used on the experiments. The table shows the domain, tasks and whether are use for training, evaluation or both.}
    \resizebox{0.9\textwidth}{!}{
        \begin{tabular}{l|l|ccccc|cc}
            \multicolumn{8}{c}{} \\
            \toprule
            \textbf{Dataset} & \textbf{Domain} & \textbf{NER} & \textbf{RE} & \textbf{EE} & \textbf{EAE} & \textbf{SF} & \textbf{Training} & \textbf{Evaluation} \\
            \midrule
            ACE05 \citep{ACE} & News & \checkmark & \checkmark & \checkmark & \checkmark & & \checkmark & \checkmark \\
            BC5CDR~\citep{bc5cdr} & Biomedical & \checkmark & & & & & \checkmark & \checkmark \\
            CoNLL 2003~\citep{tjong-kim-sang-de-meulder-2003-introduction} & News & \checkmark & & & & & \checkmark & \checkmark \\
            DIANN~\citep{diann} & Biomedical & \checkmark & & & & & \checkmark & \checkmark \\
            NCBIDisease~\citep{islamaj-dogan-lu-2012-improved} & Biomedical & \checkmark & & & & & \checkmark & \checkmark \\
            Ontonotes 5~\citep{pradhan-etal-2013-towards} & News & \checkmark & & & & & \checkmark & \checkmark \\
            RAMS~\citep{ebner-etal-2020-multi} & News & & & & \checkmark & & \checkmark & \checkmark \\
            TACRED~\citep{zhang-etal-2017-position} & News & & & & & \checkmark & \checkmark & \checkmark \\
            WNUT 2017~\citep{derczynski-etal-2017-results} & News & \checkmark & & & & & \checkmark & \checkmark \\
            \midrule
            BroadTwitter~\citep{derczynski-etal-2016-broad} & Twitter & \checkmark & & & & & & \checkmark \\
            CASIE~\citep{casie} & Cybercrime & & & \checkmark & \checkmark & & & \checkmark \\
            CrossNER~\citep{liu2021crossner} & \textit{Many} & \checkmark & & & & & & \checkmark \\
            E3C~\citep{Magnini2021} & Biomedical & \checkmark & & & & & & \checkmark \\
            FabNER~\citep{fabner}& Science & \checkmark & & & & & & \checkmark \\
            HarveyNER~\citep{chen-etal-2022-crossroads} & Twitter & \checkmark & & & & & & \checkmark \\
            MIT Movie~\citep{DBLP:conf/icassp/LiuPCG13} & Queries & \checkmark & & & & & & \checkmark \\
            MIT Restaurants~\citep{DBLP:conf/icassp/LiuPCG13} & Queries & \checkmark & & & & & & \checkmark \\
            MultiNERD~\citep{tedeschi-navigli-2022-multinerd} & Wikipedia & \checkmark & & & & & & \checkmark \\
            WikiEvents\citep{li-etal-2021-document} & Wikipedia & \checkmark & & \checkmark & \checkmark & & & \checkmark \\
            \bottomrule
        \end{tabular}
    }
    \vspace{-0.4em}
    
    \label{tab:datasets}
\end{table}

We have trained the model to perform 5 different tasks: Named Entity Recognition (NER), Relation Extraction (RE), Event Extraction (EE), Event Argument Extraction (EAE), and Slot Filling (SF). However,  we only evaluated the model on the three main tasks of interest: NER, EE, and EAE. The other two tasks are added to the training data to add diversity and improve the flexibility of the model.

A few modifications have been made to two datasets to improve the quality of the model. First, the training data of Ontonotes 5 was reduced drastically as it was automatically annotated. Second, the TACRED dataset was converted from RE to SF to increase the complexity of the task. These modifications make our system not comparable with the state of the art on those tasks. However, our focus of interest is on the zero-shot evaluation and, therefore, the benefits (see Appendix~\ref{ap:repr_examples}) are more interesting than adding 2 more comparable points on the supervised setup. In the CASIE dataset, we detected that the annotated event spans are inconsistent. The models typically annotate a sub-string rather than the entire span. Therefore, we evaluate all the models based on the predicted event categories, without considering the exact text span. For arguments, we use partial matching.

We use the guidelines released by the authors of each dataset (More details in Appendix \ref{sec:human_efford}). When such guidelines are not publicly available, we ask human experts to create them, based on the annotations from the development split. The representative candidates are extracted from the guidelines when available, otherwise, the candidates are sampled from the the train split based on word frequency or manually curated based on the guidelines. Paraphrases are automatically generated using Vicuna 33B v1.3 \citep{DBLP:journals/corr/abs-2306-05685}. 

\subsection{Language Models and Baselines}

\paragraph{Backbone LLMs:} \GoLLIE GoLLIE is a fine-tuned version of Code-LLaMA \cite{DBLP:journals/corr/abs-2308-12950}. Other backbone LLMs, such as LLaMA~\citep{DBLP:journals/corr/abs-2302-13971}, LLaMA-2 \cite{DBLP:journals/corr/abs-2307-09288} or Falcon \cite{DBLP:journals/corr/abs-2306-01116} were considered during the development, however, as our approach uses code to represent the input and output, Code-LLaMA model worked better on the preliminary experiments. In order to perform fair comparisons the baseline developed in this paper is based on Code-LLaMA as well. All the development of this paper was done with the 7B parameter version of Code-LLama, but, for a scaling analysis, we also trained the 13B and 34B parameter models.

\paragraph{Training setup:} To train the models we use QLoRA~\citep{lora, qlora}. LoRA freezes the pre-trained model weights and injects trainable rank decomposition matrices into linear layers of the Transformer architecture. In a preliminary experiment, this setup outperformed fine-tuning the entire model on the zero-shot tasks, while training much faster (more details in Appendix \ref{ap:lora_full_model_finetuning}). We applied the LoRA to all linear transformer block layers as recommended by \cite{qlora}. The models were trained for 3 epochs with an effective batch size of 32 and a learning rate of 3e-4 with a cosine scheduler. Our training infrastructure was 2 NVIDIA's A100 with 80gb each. More details about the training are given in the Appendix~\ref{ap:extended_training_details}.

\paragraph{Comparable systems:} Our main point of comparison is Instruct-UIE~\citep{DBLP:journals/corr/abs-2304-08085} as it is the approach closest to our system, but does not use guidelines. Another system considered for comparison is PromptNER \citep{DBLP:journals/corr/abs-2305-12217}, which proposes to prompt GPT-3.5 and T5 with definitions using Chain-of-Though in order to perform few-shot NER. Different from us, they did not fine-tune the model to attend to the guidelines. For fair comparison, we only considered the zero-shot results reported in the paper. In addition, other SoTA systems are added for comparison when results from Instruct-UIE and PromptNER are not available. Given that our system is designed for the zero-shot scenario, the supervised experiments are intended to verify that our system does not degrade its performance. Thus we selected, for the supervised scenario, those systems among SoTA that share the most comparable setting with us. 

\section{Results}

\subsection{Supervised evaluation}

\begin{table}
    \centering
    \caption{Supervised evaluation results. ``*" indicates that results are not directly comparable.}
    \resizebox{0.77\textwidth}{!}{
        \begin{tabular}{l|r|cc|cc}
            \multicolumn{6}{c}{} \\
            \toprule
            \textbf{Dataset} & \textbf{SoTA} & \textbf{Baseline} & \textbf{\GoLLIE} & \textbf{\GoLLIE 13B} & \textbf{\GoLLIE 34B} \\
            \midrule
            ACE05\textsubscript{NER} & \citep{DBLP:journals/corr/abs-2304-08085} 86.6 & 89.1${\scriptscriptstyle\pm}$\tiny{0.2} & 88.1${\scriptscriptstyle\pm}$\tiny{0.6} & 89.4${\scriptscriptstyle\pm}$\tiny{0.2} & \textbf{89.6}${\scriptscriptstyle\pm}$\tiny{0.1}\\
            
            ACE05\textsubscript{RE} & \citep{lu-etal-2022-unified} 66.1 & 63.8${\scriptscriptstyle\pm}$\tiny{0.6} & 63.6${\scriptscriptstyle\pm}$\tiny{1.8} & 67.5${\scriptscriptstyle\pm}$\tiny{0.5} & \textbf{70.1}${\scriptscriptstyle\pm}$\tiny{1.5} \\
            
            ACE05\textsubscript{EE} & \citep{lu-etal-2022-unified} \textbf{73.4} & 71.7${\scriptscriptstyle\pm}$\tiny{0.2} & 72.2${\scriptscriptstyle\pm}$\tiny{0.8} & 70.9${\scriptscriptstyle\pm}$\tiny{1.6} & 71.9${\scriptscriptstyle\pm}$\tiny{1.1} \\
            
            
            ACE05\textsubscript{EAE} & \citep{lu-etal-2022-unified} *54.8 & 65.9${\scriptscriptstyle\pm}$\tiny{0.7} & 66.0${\scriptscriptstyle\pm}$\tiny{0.8} & 67.8${\scriptscriptstyle\pm}$\tiny{0.9} & \textbf{68.6}${\scriptscriptstyle\pm}$\tiny{1.2} \\
            
            BC5CDR & \citep{zhang2023optimizing} \textbf{91.9} & 87.5${\scriptscriptstyle\pm}$\tiny{0.2} & 87.5${\scriptscriptstyle\pm}$\tiny{0.2} & 87.9${\scriptscriptstyle\pm}$\tiny{0.1} & 88.4${\scriptscriptstyle\pm}$\tiny{0.2} \\
            
            CoNLL 2003 & \citep{lu-etal-2022-unified} 93.0 & 92.9${\scriptscriptstyle\pm}$\tiny{0.1} & 92.8${\scriptscriptstyle\pm}$\tiny{0.3} & 93.0${\scriptscriptstyle\pm}$\tiny{0.2} & \textbf{93.1}${\scriptscriptstyle\pm}$\tiny{0.1} \\
            
            DIANN & \citep{zabala2018hybrid} 74.8 & 80.3${\scriptscriptstyle\pm}$\tiny{0.7} & 79.4${\scriptscriptstyle\pm}$\tiny{1.1} & 82.6${\scriptscriptstyle\pm}$\tiny{1.3} & \textbf{84.1}${\scriptscriptstyle\pm}$\tiny{1.1} \\
            
            NCBIDisease & \citep{DBLP:journals/corr/abs-2304-08085} \textbf{90.2} & 86.2${\scriptscriptstyle\pm}$\tiny{0.1} & 85.4${\scriptscriptstyle\pm}$\tiny{0.3} & 86.5${\scriptscriptstyle\pm}$\tiny{0.8} & 85.8${\scriptscriptstyle\pm}$\tiny{0.2} \\
            
            Ontonotes 5 & - & 83.4${\scriptscriptstyle\pm}$\tiny{0.2} & 83.4${\scriptscriptstyle\pm}$\tiny{0.2} & 84.0${\scriptscriptstyle\pm}$\tiny{0.2} & \textbf{84.6}${\scriptscriptstyle\pm}$\tiny{0.4} \\
            
            RAMS & \citep{li-etal-2021-document} 48.6 & 48.9${\scriptscriptstyle\pm}$\tiny{0.4} & 48.7${\scriptscriptstyle\pm}$\tiny{0.7} & 49.6${\scriptscriptstyle\pm}$\tiny{0.1} & \textbf{51.2}${\scriptscriptstyle\pm}$\tiny{0.3} \\
            
            TACRED & - & 56.6${\scriptscriptstyle\pm}$\tiny{0.2} & 57.1${\scriptscriptstyle\pm}$\tiny{0.9} & 56.7${\scriptscriptstyle\pm}$\tiny{0.5} & \textbf{58.7}${\scriptscriptstyle\pm}$\tiny{0.2} \\
            
            WNUT 2017 & \citep{wang-etal-2021-improving} \textbf{60.2} & 53.7${\scriptscriptstyle\pm}$\tiny{0.7} & 52.0${\scriptscriptstyle\pm}$\tiny{0.6}  & 50.5${\scriptscriptstyle\pm}$\tiny{0.9} & 54.3${\scriptscriptstyle\pm}$\tiny{0.4} \\
            
            \midrule
            Average & & 73.3${\scriptscriptstyle\pm}$\tiny{0.1}  & 73.0${\scriptscriptstyle\pm}$\tiny{0.3} & 73.9${\scriptscriptstyle\pm}$\tiny{0.3} & \textbf{75.0}${\scriptscriptstyle\pm}$\tiny{0.3} \\
            \bottomrule
        \end{tabular}
    }
    \label{tab:main-results}
    \vspace{-.55em}
\end{table}

The results on the supervised datasets are shown in Table~\ref{tab:main-results}. Comparing GoLLIE with the baseline, they both obtain very similar results, with an absolute difference of 0.3 F1 points on average. This is expected, as the baseline model implicitly learns the guidelines for annotating the datasets based on the data distribution during fine-tuning. In addition, despite the noise introduced to GoLLIE fine-tuning in order to generalize from guidelines, the performance is close to that of the baseline.

Compared to other systems our model achieves similar results in general. Focusing on the two datasets where our model under-performs significantly, WNUT and NCBIDisease, we find that task-specific techniques are still needed. For instance,  \cite{wang-etal-2021-improving} uses external knowledge to detect emergent and rare entities. In the NCBIDisisease dataset, models pre-trained on Biomedical domain corpora achieve the best results \citep{10.1007/978-3-030-68763-2_48}. \citep{DBLP:journals/corr/abs-2304-08085} leverages Flan-T5, which has great proficiency on Biomedical domain tasks \citep{DBLP:journals/corr/abs-2212-13138}.  These improvements, however, are complementary to our proposal.  

\subsection{Zero-Shot evaluation}
\label{sec:zero-shot}
\begin{table}
    \centering
    \caption{Zero-shot evaluation results. ``*" indicates results obtained using the original code.}
    \resizebox{0.75\textwidth}{!}{
        \begin{tabular}{l|r|cc|cc}
            \multicolumn{6}{c}{} \\
            \toprule
            \textbf{Dataset} & \textbf{SoTA} & \textbf{Baseline} & \textbf{\GoLLIE} & \textbf{\GoLLIE 13B} & \textbf{\GoLLIE 34B} \\
            \midrule
            BroadTwitter & - & 39.0${\scriptscriptstyle\pm}$\tiny{0.6} & 49.5${\scriptscriptstyle\pm}$\tiny{0.8} & \textbf{51.4}${\scriptscriptstyle\pm}$\tiny{1.8} & 50.3${\scriptscriptstyle\pm}$\tiny{2.1} \\
            
            CASIE\textsubscript{EE} & - & 33.9${\scriptscriptstyle\pm}$\tiny{6.5} & 59.3${\scriptscriptstyle\pm}$\tiny{2.3} & 62.2${\scriptscriptstyle\pm}$\tiny{0.9} & \textbf{65.5}${\scriptscriptstyle\pm}$\tiny{1.8} \\
            
            CASIE\textsubscript{EAE} & -& 47.9${\scriptscriptstyle\pm}$\tiny{1.4} & 50.0${\scriptscriptstyle\pm}$\tiny{1.1} & 52.6${\scriptscriptstyle\pm}$\tiny{0.2} & \textbf{55.2}${\scriptscriptstyle\pm}$\tiny{0.5} \\
            
            AI & \citep{DBLP:journals/corr/abs-2304-08085} 49.0 & 32.3${\scriptscriptstyle\pm}$\tiny{0.8} & 59.1${\scriptscriptstyle\pm}$\tiny{1.1} & 56.7${\scriptscriptstyle\pm}$\tiny{3.0} & \textbf{61.6}${\scriptscriptstyle\pm}$\tiny{1.9} \\
            
            Literature & \citep{DBLP:journals/corr/abs-2304-08085} 47.2 & 39.4${\scriptscriptstyle\pm}$\tiny{0.7} & \textbf{62.7}${\scriptscriptstyle\pm}$\tiny{3.2} & 59.7${\scriptscriptstyle\pm}$\tiny{0.3} & 59.1${\scriptscriptstyle\pm}$\tiny{2.6} \\
            
            Music & \citep{DBLP:journals/corr/abs-2304-08085} 53.2 & 56.2${\scriptscriptstyle\pm}$\tiny{1.3} & 67.8${\scriptscriptstyle\pm}$\tiny{0.2} & 65.5${\scriptscriptstyle\pm}$\tiny{3.6} & \textbf{68.4}${\scriptscriptstyle\pm}$\tiny{2.1} \\
            
            Politics & \citep{DBLP:journals/corr/abs-2304-08085} 48.2 & 38.3${\scriptscriptstyle\pm}$\tiny{1.1} & 57.2${\scriptscriptstyle\pm}$\tiny{1.0} & 54.4${\scriptscriptstyle\pm}$\tiny{4.1} & \textbf{60.2}${\scriptscriptstyle\pm}$\tiny{3.0} \\
            
            Science & \citep{DBLP:journals/corr/abs-2304-08085} 49.3 & 37.1${\scriptscriptstyle\pm}$\tiny{1.3} & 55.5${\scriptscriptstyle\pm}$\tiny{1.6} & 56.2${\scriptscriptstyle\pm}$\tiny{1.0} & \textbf{56.3}${\scriptscriptstyle\pm}$\tiny{0.4} \\
            
            E3C & - & 59.8${\scriptscriptstyle\pm}$\tiny{0.3} & 59.0${\scriptscriptstyle\pm}$\tiny{0.7} & 59.0${\scriptscriptstyle\pm}$\tiny{0.8} & \textbf{60.0}${\scriptscriptstyle\pm}$\tiny{0.4} \\
            
            FabNER & - & 06.1${\scriptscriptstyle\pm}$\tiny{0.4} & 24.8${\scriptscriptstyle\pm}$\tiny{0.6} & 25.4${\scriptscriptstyle\pm}$\tiny{0.5} & \textbf{26.3}${\scriptscriptstyle\pm}$\tiny{0.4} \\
            
            HarveyNER & - & 23.2${\scriptscriptstyle\pm}$\tiny{0.4} & 37.3${\scriptscriptstyle\pm}$\tiny{1.8} & \textbf{41.3}${\scriptscriptstyle\pm}$\tiny{0.8} & 38.9${\scriptscriptstyle\pm}$\tiny{0.5} \\
            
            Movie & \citep{DBLP:journals/corr/abs-2304-08085} 63.0 & 43.4${\scriptscriptstyle\pm}$\tiny{1.1} & \textbf{63.0}${\scriptscriptstyle\pm}$\tiny{0.6} & 62.5${\scriptscriptstyle\pm}$\tiny{1.0} & 62.4${\scriptscriptstyle\pm}$\tiny{1.4} \\
            
            Restaurants & \citep{DBLP:journals/corr/abs-2304-08085} 21.0 & 31.3${\scriptscriptstyle\pm}$\tiny{2.2}  & 43.4${\scriptscriptstyle\pm}$\tiny{0.8} & 49.8${\scriptscriptstyle\pm}$\tiny{1.4} & \textbf{52.7}${\scriptscriptstyle\pm}$\tiny{1.6} \\
            
            MultiNERD & - & 55.0${\scriptscriptstyle\pm}$\tiny{1.1} & 76.0${\scriptscriptstyle\pm}$\tiny{0.7} & \textbf{77.5}${\scriptscriptstyle\pm}$\tiny{0.3} & 77.2${\scriptscriptstyle\pm}$\tiny{0.6} \\
            
            WikiEvents\textsubscript{NER} & \citep{sainz-etal-2022-zs4ie} *49.1 & 76.9${\scriptscriptstyle\pm}$\tiny{5.1} & 80.7${\scriptscriptstyle\pm}$\tiny{0.7} & 80.2${\scriptscriptstyle\pm}$\tiny{0.7} & \textbf{81.3}${\scriptscriptstyle\pm}$\tiny{0.5} \\
            
            WikiEvents\textsubscript{EE} & \citep{sainz-etal-2022-zs4ie} *10.4 & 47.5${\scriptscriptstyle\pm}$\tiny{0.4} & 43.0${\scriptscriptstyle\pm}$\tiny{0.6} & 45.7${\scriptscriptstyle\pm}$\tiny{0.8} & \textbf{47.0}${\scriptscriptstyle\pm}$\tiny{1.9} \\
            
            WikiEvents\textsubscript{EAE} & \cite{sainz-etal-2022-textual} 35.9 & 51.6${\scriptscriptstyle\pm}$\tiny{0.5} & 51.9${\scriptscriptstyle\pm}$\tiny{0.4} & \textbf{52.5}${\scriptscriptstyle\pm}$\tiny{1.2} & 50.7${\scriptscriptstyle\pm}$\tiny{0.4} \\
            
            \midrule
            Average SoTA & 42.6 & 45.4${\scriptscriptstyle\pm}$\tiny{0.5} & 58.4${\scriptscriptstyle\pm}$\tiny{0.5} & 58.3${\scriptscriptstyle\pm}$\tiny{0.7} & \textbf{60.0}${\scriptscriptstyle\pm}$\tiny{1.0} \\
            
            Average all & - & 42.3${\scriptscriptstyle\pm}$\tiny{0.2} & 55.3${\scriptscriptstyle\pm}$\tiny{0.2} & 56.0${\scriptscriptstyle\pm}$\tiny{0.2} & \textbf{57.2}${\scriptscriptstyle\pm}$\tiny{0.5} \\
            \bottomrule
        \end{tabular}
    }
    \label{tab:zero-results}
    \vspace{-.5em}
\end{table}

The results on the zero-shot are shown in Table~\ref{tab:zero-results}. Overall, compared to the baseline, \textbf{the results are improved significantly when using guidelines} on almost every dataset, with an absolute difference of 13 F1 points on average. Despite dividing the evaluation benchmarks based on the domain, there is always some overlap between labels of train and evaluation benchmarks. For instance, the datasets E3C and WikiEvents share a large part of their schema with datasets like BC5CDR, ACE05, and RAMS. This phenomenon is reflected in the results.

GoLLIE surpasses by a large margin the current zero-shot SoTA methods Instruct-UIE~\citep{DBLP:journals/corr/abs-2304-08085} and Entailment based IE~\citep{sainz-etal-2022-zs4ie}.
Compared to Instruct-UIE, the main differences are the backbone model, the amount of training data, and, the use or not of the guidelines. Instruct-UIE leverages the 11B FlanT5 which is a T5 fine-tuned on 473 NLP datasets. With respect to the data, Instruct-UIE leverages a total of 34 IE datasets (counting different tasks as datasets) from diverse domains, we only leverage 12 datasets. Contrary to our method they do not use guideline information. Still, our method performs significantly better suggesting that the guidelines have an important effect on the results. 

PromptNER~\citep{DBLP:journals/corr/abs-2305-12217} also adds some definition information into the prompt in order to perform zero-shot NER. We compare our approach with them (represented as GPT-3.5) in Figure~\ref{fig:zero-shot-results}. Although their approach leverages guidelines too, our approach performs significantly better on all datasets, showing that LLMs (even with 175B parameters) struggle to follow guidelines. They solve this by adding examples in the context but are still far behind on a comparable setting (T5-XXL).

\begin{wrapfigure}{r}{0.42\textwidth}
    \centering
    \vspace{-1.5em}
    \includegraphics[width=1.0\linewidth]{images/barplot.pdf}
    \vspace{-1.0em}
    \caption{Seen vs unseen label zero-shot performance, results aggregated from all datasets.}
    \label{fig:seen_vs_unseen}
\end{wrapfigure}

\paragraph{Seen vs unseen labels:} Not all labels in the zero-shot datasets are unseen; there is an overlap between the labels in the training and zero-shot datasets. Although these labels may have very different annotation guidelines, we also report results on the set of labels to which it has not been exposed during training, to better understand the generalization capabilities of GoLLIE. The list of seen and unseen labels, as well as an extended analysis is available in Appendix \ref{sec:seen_vs_unseen}. Figure \ref{fig:seen_vs_unseen} aggregates the F1 scores across datasets for seen and unseen labels in the zero-shot scenario. All models exhibit slightly lower performance on unseen labels. For the baseline model, the performance drop is more pronounced. In contrast, GoLLIE demonstrates better generalization ability, showing a smaller gap in F1 scores between the seen and unseen labels. Also, the gap is smaller as the parameter count of our model increases. %This indicates GoLLIE's enhanced capability in generalizing to new, unseen labels, underscoring its effectiveness in zero-shot scenarios.

\paragraph{Model scaling:} Recent research has shown that increasing the parameter count of language models leads to improved generalization capabilities \cite{DBLP:conf/nips/BrownMRSKDNSSAA20}. Higher parameter count yields superior average zero-shot performance. However, some datasets and tasks greatly benefit from a larger LLM, while others do not. We believe that some datasets do not see benefits from increasing the LLM size because their performance is hindered by the issues with the guidelines that we discuss in Section \ref{sec:Ablation}. While, in general, larger models achieve better results in both supervised and zero-shot settings, GoLLIE with a 7B parameter backbone already exhibits strong zero-shot capabilities. %As described in Appendix \ref{ap:extended_training_details}, the 13B and 34B parameter versions require orders of magnitude more compute. Thus, one must carefully weigh the benefits of enhanced performance against the significantly increased computational costs and resources required

\subsection{Ablation study}
\label{sec:Ablation}

\begin{wraptable}{r}{0.4\textwidth}
    \centering
    \vspace{-2.2em}
    \caption{Ablation results.}
    \adjustbox{max width=0.9\linewidth}{\begin{tabular}{l|cl}
        \multicolumn{2}{c}{} \\
        \toprule
        \textbf{Model} & \multicolumn{1}{c}{\textbf{F1}} & \multicolumn{1}{c}{\textbf{p-value}} \\
        \midrule
        \GoLLIET & 55.3${\scriptscriptstyle\pm}$\tiny{0.2} & - \\
        \midrule
        w/o Shuffling & 55.9${\scriptscriptstyle\pm}$\tiny{0.2} & $7.2e^{-2}$ \\
        w/o Paraphrases & 54.8${\scriptscriptstyle\pm}$\tiny{0.2} & $1.1e^{-1}$ \\
        w/o Masking & 54.6${\scriptscriptstyle\pm}$\tiny{0.6} & $1.0e^{-1}$ \\
        w/o Dropout & 54.0${\scriptscriptstyle\pm}$\tiny{0.2} & $4.0e^{-3}$ \\
        w/o Candidates & 49.9${\scriptscriptstyle\pm}$\tiny{0.2} & $2.2e^{-10}$ \\
        w/o \textit{all} (baseline) & 42.3${\scriptscriptstyle\pm}$\tiny{0.1} & $5.1e^{-13}$ \\
        \bottomrule
    \end{tabular}}
    \vspace{-.5em}
    \label{tab:ablation}

\end{wraptable}

We have performed an ablation to see the contribution of several components in the zero-shot evaluation. We analyzed the different regularization techniques proposed in Section~\ref{sec:components}. Additionally, we represent the baseline, i.e. when removing all components including guidelines, as "w/o \textit{all}".
Along with the mean zero-shot F1 we also provide the one-sided p-value with respect to \GoLLIE GoLLIE. 

The class order shuffling, guideline paraphrasing, and class name masking seem to have no significant contribution to the final result, while class dropout although significant improvements are small. As further explained in Appendix \ref{ap:extended_training_details}, the loss is only computed over the result tokens, inherently limiting the model's potential to overfit to the guidelines. In contrast, the representative annotation items give a stronger signal to the model. We see how definitions and representative candidates from the guidelines are complementary and help to improve each other. 

\section{Error analysis} 

In this section, we aim to better understand the effect of prompting LLMs with guidelines. We focus on specific labels across various datasets, with the results displayed in Table \ref{tab:error_analysis}. Our analysis covers both successful and unsuccessful cases of entity labeling by GoLLIE. For the latter, we also aim to identify the reasons why the model fails to correctly label these entities. Further analyses on malformed outputs or hallucinations are discussed in Appendix~\ref{ap:hallucinations}.

\begin{table}[!htb]
\centering
\vspace{-1.0em}
\small
    \caption{This table shows the F1 scores for specific labels from different datasets. The guideline column is a small summary of the actual guideline used to prompt the model.}
    \adjustbox{max width=.90\linewidth}{\begin{tabular}{@{}llp{7.5cm}cc@{}}
        \multicolumn{4}{c}{} \\
        \toprule
        \multicolumn{1}{l}{\textbf{Dataset}} & \multicolumn{1}{l}{\textbf{Label}} &  \multicolumn{1}{l}{\textbf{Guideline}} & \textbf{Baseline} & \GoLLIE \\ \midrule
        \rowcolor{ForestGreen!10} MultiNERD & Media & Titles of films, books, songs, albums, fictional characters and languages. &  13.6 & 69.1 \\ 
        \rowcolor{ForestGreen!10} \rule{0pt}{2.25ex} CASIE & Vul. Patch & When a software company addresses a vulnerability by releasing an update. & 27.7 & 70.5 \\
        \rowcolor{ForestGreen!10} \rule{0pt}{2ex} Movie & Trailer & Refers to a short promotional video or preview of a movie. & 00.0 & 76.4 \\
        \rowcolor{ForestGreen!10} \rule{0pt}{2ex} AI & Task & Particular research task or problem within a specific AI research field.  & 02.7 & 63.9 \\ 
        \rowcolor{CornflowerBlue!10} \rule{0pt}{2ex} MultiNERD & Time & Specific and well-defined time intervals, such as eras, historical periods, centuries, years and important days. &  01.4 & 03.5 \\
        \rowcolor{Thistle!10} \rule{0pt}{2ex} Movie & Plot & Recurring concept, event, or motif that plays a significant role in the development of a movie. & 00.4 & 05.1 \\
        \rowcolor{Thistle!10} \rule{0pt}{2ex} AI & Misc & Named entities that are not included in any other category. & 01.1 & 05.2 \\
        \rowcolor{Thistle!10} \rule{0pt}{2ex} Literature & Misc & Named entities that are not included in any other category.  &  03.7 & 30.8 \\
        \rowcolor{Thistle!10} \rule{0pt}{2ex} Literature & Writer & Individual actively engaged in the creation of literary works. &   04.2 & 65.1 \\
        \rowcolor{Thistle!10} \rule{0pt}{2ex} Literature & Person & Person name that is not a writer. & 33.5 & 49.4 \\
        \rowcolor{Thistle!10} \rule{0pt}{2ex} Science & Scientist & A person who is studying or has expert knowledge of a natural science field.  &  02.1 & 05.8 \\
        \rowcolor{Thistle!10} \rule{0pt}{2ex} Science & Person & Person name that is not a scientist. &   46.1 & 45.9 \\
        \rowcolor{Thistle!10} \rule{0pt}{2ex} Politics & Polit. Party & Organization that compete in a particular country's elections. & 11.2 & 34.9 \\ \bottomrule
    \end{tabular}}
    \label{tab:error_analysis}
\end{table}

\paragraph{The details are in the guidelines:} Labels such as \textsc{Media}, \textsc{VulnerabilityPatch}, \textsc{Trailer}, and \textsc{Task} are inherently polysemous, making it challenging to determine the appropriate categorization based solely on the label name. As a result, the baseline struggles to effectively classify items under these labels due to having insufficient information. Conversely, GoLLIE successfully follows the guidelines, underscoring their utility.

\paragraph{When the annotations do not comply with the guidelines:} In the case of the \textsc{Time} label of the MultiNERD dataset, we found that our model labels years as \textsc{Time} entities. This is correct according to the annotation guidelines. Surprisingly, years are not labeled as entities in the dataset. In this case, GoLLIE successfully follows the guidelines; unfortunately, the dataset annotations do not.

\paragraph{Ambiguous labels:} The \textsc{Miscellaneous} category, used by CoNLL03 and CrossNER datasets, refers to any named entity that is not included in the predefined categories set by the dataset. This definition is highly ambiguous and serves as a catch-all for various elements that do not fit into any of the predefined categories. Similarly,  the \textsc{Plot} category of the Movie dataset is used to label a wide range of elements. For example, events in a movie (e.g., murder, horse racing), characters (e.g., vampires, zombies), and the country of origin (e.g., British), among others. This lack of specificity hinders the development of consistent rules or guidelines for tagging such elements \citep{ratinov-roth-2009-design}, which is a problem for humans and machines alike. As a consequence, GoLLIE also fails to label them accurately. 

\paragraph{Conflicts Between Fine-Grained and Coarse Entities:} The CrossNER dataset introduces two labels for person names within each domain. For example, in the Science domain, the labels \textsc{Scientist} and \textsc{Person} are used. The former is used to label any person that is not a Scientist. Similarly, the Literature domain includes the labels \textsc{Writer} and \textsc{Person}. The guidelines assist GoLLIE in correctly labeling entities as \textsc{Writer}. However, GoLLIE still categorizes individuals as \textit{Person} even when they are \textit{Scientist}, despite the guidelines. This is not technically incorrect, as every scientist is, by definition, also a person. 

\paragraph{Strong Label Preconceptions:} In its Political domain set, CrossNER includes the label \textsc{Political Party}. GoLLIE outperforms the baseline, once again demonstrating the utility of providing the model with guidelines. However, we often find that the model categorizes political parties as organizations. As listed in Table \ref{tab:datasets}, most of the pre-training datasets originate from the news domain, where political parties are a common entity. However, none of the fine-tuning datasets include the \textsc{Political Party} entity; they are instead categorized as \textsc{Organization}. Consequently, during inference, the model consistently labels political parties as organizations. We believe this issue can be resolved by expanding the number and diversity of the fine-tuning datasets.

In summary, we anticipate that \textbf{GoLLIE will perform well on labels with well-defined and clearly bounded guidelines}. On the other hand, ambiguous labels or very coarse labels pose challenges. In this regard, we believe that GoLLIE would benefit from learning to follow instructions such as \textit{"Label always the most specific class"} or \textit{"Annotate this class in the absence of other specific class"}. We also expect that GoLLIE would benefit from expanding the number and diversity of the pre-training datasets.

\section{Conclusions}
In this paper, we introduce \GoLLIET, an LLM specifically fine-tuned to comply with annotation guidelines that were devised to help humans annotate the dataset. A comprehensive zero-shot evaluation empirically demonstrates that annotation guidelines are of great value for LLMs, as GoLLIE successfully leverages them. GoLLIE achieves better zero-shot results than previous attempts at zero-shot IE which do not leverage the guidelines, or use models not finetuned for following guidelines. 

GoLLIE is a significant progress towards the development of models that can generalize to unseen IE tasks. In the future, we plan to enhance GoLLIE by using a larger and more diverse set of pre-training datasets. We will also improve the model's performance with ambiguous and coarse labels by expanding the set of instructions that the model can follow. 

\section*{Acknowledgments}

This work has been partially supported by %the HiTZ center and 
the Basque Government (Research group funding IT-1805-22 and ICL4LANG project, grant no. KK-2023/00094). We are also thankful to several MCIN/AEI/10.13039/501100011033 projects: (i) DeepKnowledge (PID2021-127777OB-C21) and by FEDER, EU; (ii) Disargue (TED2021-130810B-C21) and European Union NextGenerationEU/PRTR; (iii) AWARE (TED2021-131617B-I00) and European Union NextGenerationEU/PRTR. This work has also been partially funded by the LUMINOUS project (HORIZON-CL4-2023-HUMAN-01-21-101135724). Oscar Sainz is supported by a doctoral grant from the Basque Government (PRE\_2023\_2\_0137). Rodrigo Agerri currently holds the RYC-2017-23647 fellowship (MCIN/AEI/10.13039/501100011033 and by ESF Investing in your future).

\clearpage

\end{document}