\title{UniversalNER: targeted distillation from large language models for open named entity recognition}

\begin{document}

\maketitle

\begin{abstract}
Large language models (LLMs) have demonstrated remarkable generalizability, such as understanding arbitrary entities and relations. 
Instruction tuning has proven effective for distilling LLMs into more cost-efficient models such as Alpaca and Vicuna. Yet such student models still trail the original LLMs by large margins in downstream applications. 
In this paper, we explore {\it targeted distillation} with mission-focused instruction tuning to train student models that can excel in a broad application class such as open information extraction.
Using named entity recognition (NER) for case study, we show how ChatGPT can be distilled into much smaller \longname models for open NER. 
For evaluation, we assemble the largest NER benchmark to date, comprising 43 datasets across 9 diverse domains such as biomedicine, programming, social media, law, finance.
Without using any direct supervision, \longname attains remarkable NER accuracy across tens of thousands of entity types, outperforming general instruction-tuned models such as Alpaca and Vicuna by over 30 absolute F1 points in average. 
With a tiny fraction of parameters, \longname not only acquires ChatGPT's capability in recognizing arbitrary entity types, but also outperforms its NER accuracy by 7-9 absolute F1 points in average. 
Remarkably, \longname even outperforms by a large margin state-of-the-art multi-task instruction-tuned systems such as InstructUIE, which uses supervised NER examples.
We also conduct thorough ablation studies to assess the impact of various components in our distillation approach. 
We release the distillation recipe, data, and \longname models to facilitate future research on targeted distillation.\footnote{Project page: \url{https://universal-ner.github.io/}}
\end{abstract}

\section{Introduction}

Large language models (LLMs) such as ChatGPT~\citep{ouyang2022training,openai2023gpt4} have demonstrated remarkable generalization capabilities, but they generally require prohibitive cost in training and inference. Moreover, in mission-critical applications such as biomedicine, white-box access to model weights and inference probabilities are often important for explainability and trust. Consequently, instruction-tuning has become a popular approach for distilling LLMs into more cost-efficient and transparent student models. %A general recipe is to finetune pretrained open-source LLMs such as LLaMA~\cite{} using instruction-following data generated by ChatGPT. 
Such student models, as exemplified by Alpaca~\citep{alpaca} and Vicuna~\citep{vicuna-2023}, have demonstrated compelling capabilities in imitating ChatGPT. However, upon close inspection, they still trail the teacher LLM by a large margin, especially in targeted downstream applications~\citep{gudibande2023false}. Bounded by limited compute, it is unsurprising that generic distillation can only produce a shallow approximation of the original LLM across all possible applications.

In this paper, we instead explore {\it targeted distillation} where we train student models using mission-focused instruction tuning for a broad application class such as open information extraction~\citep{etzioni2008open}.
We show that this can maximally replicate LLM's capabilities for the given application class, while preserving its generalizability across semantic types and domains. 
We choose named entity recognition (NER) for our case study, as it is one of the most fundamental tasks in natural language processing~\citep{wu2017clinical,perera2020named}.
Recent studies~\citep{wei2023zero,li2023evaluating} show that when there are abundant annotated examples for an entity type, LLMs still fall behind the state-of-the-art supervised system for that entity type. 
However, for the vast majority of entity types, there is little annotated data. New entity types constantly emerge, and it is expensive and time-consuming to generate annotated examples, especially in high-value domains such as biomedicine where specialized expertise is required for annotation. 
Trained on pre-specified entity types and domains, supervised NER models also exhibit limited generalizability for new domains and entity types.

We present a general recipe for targeted distillation from LLMs and demonstrate that for open-domain NER. 
We show how to use ChatGPT to generate instruction-tuning data for NER from broad-coverage unlabeled web text, and conduct instruction-tuning on LLaMA~\citep{touvron2023llama} to distill the \longname models (\shortname in short).

To facilitate a thorough evaluation, we assemble the largest and most diverse NER benchmark to date (\longname benchmark), comprising 43 datasets across 9 domains such as biomedicine, programming, social media, law, finance.
On zero-shot NER, LLaMA and Alpaca perform poorly on this benchmark (close to zero F1). Vicuna performs much better by comparison, but still trails ChatGPT by over 20 absolute points in average F1. By contrast, \longname attains state-of-the-art NER accuracy across tens of thousands of entity types in the \longname benchmark, outperforming Vicuna by over 30 absolute points in average F1. 
With a tiny fraction of parameters, \longname not only replicates ChatGPT's capability in recognizing arbitrary entity types, but also outperforms its NER accuracy by 7-9 absolute points in average F1. 
Remarkably, \longname even outperforms by a large margin state-of-the-art multi-task instruction-tuned systems such as InstructUIE~\citep{wang2023instructuie}, which uses supervised NER examples.
We also conduct thorough ablation studies to assess the impact of various distillation components, such as the instruction prompts and negative sampling. 

\eat{
``Instruction-tuned'' large language models (LLMs) \cite{ouyang2022training,gpt4} have demonstrated remarkable capabilities of tackling diverse tasks.
However, they %have a huge cost
usually lead to high costs for training and inference, and %access to these models is limited to text outputs,
can only grant a black-box accessibility without providing underlying probabilities, attention weights, or hidden states.
The advancements of recent models such as Alpaca \cite{alpaca} and Vicuna \cite{vicuna-2023} have shown that potent conversational capabilities can be obtained by distilling existing LLMs into smaller, more manageable models.
Upon close inspection, there is still a significant performance gap between the distilled models and LLMs, especially on more targeted rigorous evaluations~\cite{wang2023far,gudibande2023false}.
In this paper, we aim to close such a gap. But instead of doing it for all tasks, we focus on a specific task.
Our goal is to distill LLMs into a smaller model that performs better or equally well on a given task,
which enables a smaller, fully-accessible, and fine-tunable alternative to LLMs.

We choose named entity recognition (NER) as a case study as NER is one of the fundamental tasks in natural language processing, which directly supports the need for text understanding and structuring in verticals such as news, science, clinics, etc.~\cite{wu2017clinical,perera2020named}
Recent studies~\cite{wei2023zero,li2023evaluating} show that LLMs still fall behind supervised models on the established NER datasets when there are abundant supervision data. 
However, only a small amount of entity types have abundant human annotations.
There is little to no supervision data available for a vast number of entity types or concepts which are of practical value. And the number keeps growing with emergent entities.
It is extremely costly to provide abundant human annotations for new entity types, especially in specialized domains such as clinics and biology that require expert knowledge. 
In the meanwhile, we find that supervised models can barely transfer their knowledge to new data and entity types.
In contrast, LLMs show superior performance in the setting where there is a lack of supervision data~\cite{wang2023instructuie}.

In this paper, we propose a recipe for distilling end-task capabilities from LLMs.
Using NER as a case study, we successfully distill %task-level
these capabilities from LLMs into a much smaller model \longname that can recognize diverse types of entities or concepts in text corpora from a wide range of domains.
In order to rigorously evaluate \longname, we collect the largest NER benchmark to date, which consists of 43 datasets across 9 domains.
Our experiments demonstrate that \longname not only surpasses existing instruction-tuned models at the same size (e.g., Alpaca, Vicuna) by a large margin, but also shows substantially better performance than LLMs (e.g., ChatGPT).
When additional human annotations are available, we can fine-tune \longname for further improvement at a much smaller cost compared to LLMs.
The fine-tuned \longname significantly outperforms the supervised state of the art. % as well as LLMs such as ChatGPT.

We release our training recipe, data, and the \longname model. We are also hosting an interactive demo to enable direct engagement. We hope sharing them can facilitate the study of distilling other tasks' zero-shot capabilities from LLMs.

}
\section{Related Work}

\paragraph{Knowledge distillation.}
While LLMs such as ChatGPT achieve promising results, these models are often black-box and have high computational costs.
To address these issues, distilling the task capabilities of LLMs into smaller, more manageable models has emerged as a promising direction.
Knowledge distillation~\citep{hinton2015distilling} often revolves around the transfer of knowledge from larger, more complex models to their smaller counterparts.
Recent work~\citep{alpaca,vicuna-2023,peng2023instruction} seeks to distill the general abilities of LLMs with the objective of matching, if not surpassing, the performance of the original LLMs.
Particularly, Alpaca~\citep{alpaca} automates the generation of instructions~\citep{wang2022self} and distills the knowledge from a teacher LLM.
Vicuna~\citep{vicuna-2023} adopts the ShareGPT data, which are comprised of real conversations with ChatGPT conducted by users, thereby providing a more authentic context for distillation.
Another line of work~\citep{smith2022language,jung2023impossible,hsieh-etal-2023-distilling,gu2023distilling} focuses on distilling task-level abilities from LLMs.
Particularly, \citet{jung2023impossible} propose an efficient method to distill an order of magnitude smaller model that outperforms GPT-3 on specialized tasks summarization and paraphrasing in certain domains.
\citet{hsieh-etal-2022-converting} propose to distill LLMs' reasoning abilities into smaller models by chain-of-the-thought distillation.
However, these studies perform distillation either on certain datasets or domains, while our work focuses on a more general formulation that can be applied to diverse domains.

\paragraph{Instruction tuning.} 
As an effective method to adapt LMs to perform a variety of tasks, instruction tuning has attracted an increasing number of community efforts: FLAN~\citep{chung2022scaling}, T0~\citep{sanh2021multitask}, and Tk-Instruct~\citep{wang-etal-2022-super} convert a large set of existing supervised learning datasets into instruction-following format, and then fine-tune encoder-decoder models, showing strong zero-shot and few-shot performance on NLP benchmarks. \citet{ouyang2022training} crowd-source high-quality instruction data and fine-tune GPT-3 into InstructGPT, enhancing its ability to understand user intention and follow instructions.
Recent advancements~\citep{alpaca,vicuna-2023,peng2023instruction} have also led to smaller models that exhibit task-following capabilities, after being fine-tuned on instruction data generated by LLMs, such as ChatGPT or GPT4.
However, these smaller models often struggle to generate high-quality responses for a diverse range of tasks~\citep{wang2023far}.
A closer examination on targeted benchmarks reveals a substantial gap between these models to ChatGPT~\citep{gudibande2023false}. 
Our proposed method, in contrast, focuses on tuning models to excel at a specific type of tasks. 
The diversity in our instructing-tuning method comes from task labels (e.g., relation types for relation extraction, entity types for NER), rather than instructions.
By focusing on task-level capabilities and using NER as a case study, we demonstrate that it is possible to devise a tuning recipe that not only closes the performance gap but also surpasses ChatGPT.
\citet{wang2023instructuie} also explore instruction-tuning for information extraction tasks. However, their method relies solely on supervised datasets and yields subpar performance when compared to ChatGPT.
\section{Mission-Focused Instruction Tuning}

Instruction tuning~\citep{ouyang2022training,wei2022finetuned} is a method through which pretrained autoregressive language models 
are finetuned to follow natural language instructions and generate responses.
Existing work focuses on tuning models to do diverse tasks~\citep{alpaca,vicuna-2023}. In contrast, we introduce a general recipe for mission-focused instruction tuning, where the pretrained model is tuned for a broad application class such as open information extraction.

In this paper, we conduct a case study on the NER task, as it is one of the fundamental tasks for knowledge extraction from text.
The objective is to learn a model $f: (\mathcal{X} \times \mathcal{T})\rightarrow \mathcal{Y}$, where $\mathcal{X}$ represents the set of inputs, $\mathcal{T}$ denotes a predefined set of entity types, and $\mathcal{Y}$ represents the set of entities of a specific type in the given input.

\subsection{Data Construction}
A typical instruction-tuning example is made of three parts, including \texttt{instruction}, \texttt{input}, and \texttt{output}, where the diversity of \texttt{instruction} %is key for
causes the models to follow
a wide range of task instructions.
However, for \emph{mission-focused} instruction tuning, our goal is to tune the model to maximally generalize across semantic types and domains for the targeted application class.
Therefore, we focus on increasing the diversity of \texttt{input} rather than \texttt{instruction}.

\begin{wrapfigure}[16]{r}{0.5\textwidth}
    \vspace{-10pt}
    \begin{AIboxSmall}{\footnotesize Data Construction Prompt}
    \footnotesize
    \textbf{System Message:} You are a helpful information extraction system.
    
    \textbf{Prompt:} Given a passage, your task is to extract all entities and identify their entity types. The output should be in a list of tuples of the following format: [("entity 1", "type of entity 1"), ... ].\\

Passage: \{input\_passage\}
    \end{AIboxSmall}
    \caption{Data construction prompt for generating entity mentions and their types for a given passage.}
    \label{fig:prompt_ent_type_gen}
\end{wrapfigure}

While earlier work~\citep{jung2023impossible} employs language models to generate inputs, these models typically assume that the domains of test data are known and prompt LMs to generate data for each domain.
This method falls short when applied to distillation for a broad application class, where the distribution of test data is unknown.
Consequently, it is challenging to generate inputs from LMs that provide wide coverage of the test domains.

To address this limitation, we propose an alternative: directly sampling inputs from a large corpus across diverse domains, and then using an LLM to generate outputs.
In this paper, we sample inputs from the Pile corpus~\citep{gao2020pile}, which compiles 22 distinct English sub-datasets.
We chunk the articles in Pile to passages of a max length of 256 tokens and randomly sample 50K passages as the inputs.
Subsequently, we use ChatGPT (\texttt{gpt-3.5-turbo-0301}) to generate entity mentions and their associated types based on the sampled passages.
To ensure stability, we set the generation temperature to 0.
The specific prompt for constructing the data is shown in Fig.~\ref{fig:prompt_ent_type_gen}.
In this prompt, we do not specify the set of entity types of interest, allowing the LLM to generate outputs encompassing a broad coverage of entity types.

\begin{table}[!t]
    \centering
    \small
    \begin{tabular}{c|p{0.8\linewidth}}
        \shline
         Frequency&  Entity types\\
         \shline
         \makecell[tc]{Top 1\% \\ (74\%)} & person, organization, location, date, concept, product, event, technology, group, medical condition, ...\\
         \shline
         \makecell[tc]{1\%-10\% \\ (19\%)}& characteristic, research, county, module, unit, feature, cell, package, anatomical structure, equipment, ... \\
         \shline
         \makecell[tc]{10\%-100\% \\ (7\%)}& attribute value, pokemon, immune response, physiology, animals, cell feature, FAC, input device, ward, broadcast, ... \\
         \shline
    \end{tabular}
    \caption{Examples of entities across different frequency ranges - top 1\%, 1-10\%, and 10-100\%, along with the percentage of total frequencies for each range.}
    \label{tab:entity_types}
\end{table}

\smallskip
\noindent\textbf{Data statistics.} After filtering out unparseable outputs and inappropriate entities, including non-English entities and those classified under 'ELSE' categories, such as None, NA, MISC, and ELSE, our dataset comprises 45,889 input-output pairs, encompassing 240,725 entities and 13,020 distinct entity types.
We divide the entity types according to frequency and show the top 10 entity types in each range in Tab.~\ref{tab:entity_types}.
The distribution of these entity types exhibits a heavy tail, where the top 1\% of entities account for 74\% of total frequencies.
We find that the generated data contain entity types from various domains, ranging from the general domain (e.g., \textsc{person}) to the clinical domain (e.g., \textsc{medical condition}).
Moreover, we observe variations in granularity among the entity types.
E.g., \textsc{county} is the subset of \textsc{location}, and \textsc{input device} is a subset of \textsc{product}.
These data characteristics offer extensive coverage of entity types, making them suitable for distilling capabilities from LLMs across various domains.

\smallskip
\noindent\textbf{Definition-based data construction.} Besides entity types, we also prompt ChatGPT to generate entity mentions and define their types using short sentences.
To do so, we simply change the prompt in Fig.~\ref{fig:prompt_ent_type_gen} from ``extract all entities and identify their entity types'' to ``extract all entities and concepts, and \emph{define their type using a short sentence}''.
This method generates a much more diverse set of 353,092 entity types and leads to a tuned model that is less sensitive to entity type paraphrasing (Section~\ref{ssec:case_study}), but performs worse on standard NER benchmarks (Section~\ref{ssec:results_distill}).

    

\subsection{Instruction Tuning}

\begin{wrapfigure}[20]{r}{0.5\textwidth}
    \vspace{-20pt}
    \begin{AIboxSmall}{\footnotesize Conversation-style Instruct Tuning Template}
    \footnotesize
    A virtual assistant answers questions from a user based on the provided text.\\
    \texttt{User:} Text: $\mX_\text{passage}$\\
    \texttt{Assistant:} I've read this text.\\
    \texttt{User:} What describes $\vt_1$ in the text?\\
    \texttt{Assistant:} \fcolorbox{aigold}{aigold}{$\vy_1$}\\
    ...\\
    \texttt{User:} What describes $\vt_T$ in the text?\\
    \texttt{Assistant:} \fcolorbox{aigold}{aigold}{$\vy_T$}
    \end{AIboxSmall}
    \caption{The conversation-style template that converts a passage with NER annotations into a conversation, where $\mX_\text{passage}$ is the input passage, $[\vt_1,...,\vt_T]$ are entity types to consider, and $\vy_i$ is a list of entity mentions that are $\vt_i$. The conversation is used to tune language models. Only the highlighted parts are used to compute the loss.}
    \label{fig:input_in_conv}
\end{wrapfigure}

After obtaining the data, we apply instruction tuning to smaller models to distill for a broad application class, e.g., diverse entity types in NER.
Our template, as shown in Fig.~\ref{fig:input_in_conv}, adopts a conversation-style tuning format.
In this approach, the language model is presented with a passage $\mX_\text{passage}$ as input.
Then, for each entity type $\vt_i$ that appears in the output, we transform it into a natural language query ``\textit{What describes $\vt_i$?}''
Subsequently, we tune the LM to generate a structured output $\vy_i$ in the form of a JSON list containing all entities of $\vt_i$ in the passage.
We consider $\vy_1,...,\vy_T$ as gold tokens and apply a language modeling objective on these tokens.
Our preliminary experiments show that conversation-style tuning is better than traditional NER-style tuning adopted by \citet{wang2023instructuie,sun2023pushing}.

Besides one entity type per query, we also consider combining all entity types in a single query, requiring the model to output all entities in a single response.
Detailed results and discussions can be found in Section~\ref{ssec:results_distill}.

\smallskip
\noindent\textbf{Negative sampling.}
Our data construction process follows an open-world assumption where we allow the model to generate entity types that have appeared in the passage.
However, the generated data do not account for entity types that are not mentioned in the passage, i.e., negative entity types.
As a result, it is challenging for us to apply a model trained on this data to a closed-world setting, where one may ask for entity types that do not exist in the passage.
To address this potential mismatch, we sample negative entity types from the collection of all entity types that do not appear in the passage as queries and set the expected outputs as empty JSON lists.
The sampling of negative entity types is done with a probability proportional to the frequency of entity types in the entire dataset.
This approach greatly improves the instruction tuning results, as shown in Section~\ref{ssec:ablation_study}.

\smallskip
\noindent\textbf{Supervised finetuning.} When we have additional human annotations, model performance can be further improved with supervised data.
However, a significant challenge arises when training with multiple datasets, as there might be discrepancies in label definitions among these datasets, resulting in label conflicts.
For instance, some datasets like ACE~\citep{walker2006ace} consider personal pronouns (e.g., she, he) as \textsc{person}, while other datasets like multiNERD~\citep{tedeschi-navigli-2022-multinerd} do not include pronouns.

\begin{wrapfigure}[13]{r}{0.5\textwidth}
    \vspace{-20pt}
    \begin{AIboxSmall}{\footnotesize Dataset-specific Instruct Tuning Template}
    \footnotesize
    A virtual assistant answers questions from a user based on the provided text.\\
    \texttt{User:} \fcolorbox{aired}{aired}{Dataset: $\mD$} \textbackslash n Text: $\mX_\text{passage}$\\
    \texttt{Assistant:} I've read this text.\\
    \texttt{User:} What describes $\vt_1$ in the text?\\
    \texttt{Assistant:} \fcolorbox{aigold}{aigold}{$\vy_1$}\\
    ...\\
    \texttt{User:} What describes $\vt_T$ in the text?\\
    \texttt{Assistant:} \fcolorbox{aigold}{aigold}{$\vy_T$}
    \end{AIboxSmall}
    \caption{The dataset-specific instruction tuning template. We add the dataset name $\mD$ (colored in \fcolorbox{aired}{aired}{red}) as part of the input to resolve conflicts in label definitions.}
    \label{fig:supervised_input_in_conv}
\end{wrapfigure}

To address this issue, we propose to use dataset-specific instruction tuning templates to harmonize the discrepancies in label definitions, as illustrated in Fig.~\ref{fig:supervised_input_in_conv}.
Specifically, we augment the input with an additional field denoting the dataset name $\mD$.
By doing so, the model can learn the dataset-specific semantics of labels.
During inference, we use the respective dataset name in the prompt for the supervised setting, whereas we omit the dataset field from the prompt in the zero-shot setting.
\section{Universal NER Benchmark}
To conduct a comprehensive evaluation of NER models across diverse domains and entity types, we collect the largest NER benchmark to date.
This benchmark encompasses 43 NER datasets across 9 domains, including general, biomedical, clinical, STEM, programming, social media, law, finance, and transportation domains.
An overview of data distribution is shown in Fig.~\ref{fig:data_dist}.
Detailed dataset statistics are available in Appendix Tab.~\ref{tab:full_data_statistics}.

\begin{wrapfigure}[22]{r}{0.5\textwidth}
    \vspace{-15pt}
    \centering
    \includegraphics[width=1.0\linewidth]{figs/data_dist.pdf}
    \caption{Distribution of UniNER benchmark.}
    \label{fig:data_dist}
\end{wrapfigure}

\smallskip
\noindent\textbf{Dataset processing.} To make the entity types semantically meaningful to LLMs, we conduct a manual inspection of the labels and convert the original labels into natural language formats.
For instance, we replace \textsc{per} with \textsc{person}.
While we try to collect a broad coverage of NER datasets, we do not use all entity types.
This is because some entity types (e.g., \textsc{Else}) are not coming from consistent sources across the different datasets.
Their annotations often come from different ontologies for different purposes.
The choices of entity types and their annotation guidelines are not optimized for holistic or comprehensive assessments, which renders them suboptimal for use as a ``ground truth'' to evaluate a universal NER model.
Therefore, we remove those labels from the datasets.
In addition, some datasets are at the document level and contain very long contexts, which might exceed the input length limit of models.
Therefore, we split all instances in document-level datasets into sentence-level %tasks.
ones.
\section{Experiments}
This section presents experimental evaluations of \longname.
We start by outlining experimental settings (Section~\ref{ssec:experiment_setting}), followed by presenting the results on both distillation and supervised settings (Sections~\ref{ssec:results_distill} and~\ref{ssec:results_supervised}).
Finally, we conduct analysis (Section~\ref{ssec:ablation_study}) and case study (Section~\ref{ssec:case_study}) to provide deeper insights into the model's performance.

\begin{figure*}[t]
\begin{subfigure}[t]{0.46\textwidth}
\centering
\includegraphics[width=0.99\textwidth]{figs/zero_shot_domain.pdf}
\caption{Comparisons of zero-shot models on different domains. Our distilled models achieve better results than ChatGPT in all evaluated domains.}
\label{fig:self-supervised-bar}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.46\textwidth}
\centering
\includegraphics[width=0.97\textwidth]{figs/zero_shot_ablation.pdf}
\caption{Comparisons between \shortname-7B and two variants. \shortname-7B-definition is distilled on Pile data prompted with entity type definitions. \shortname-7B-all-in-one is tuned with the template where all entity types are asked in one query.}
\label{fig:self-supervised-ablation}
\end{subfigure}
\end{figure*}

\subsection{Experimental Settings}
\label{ssec:experiment_setting}
\smallskip
\noindent\textbf{Model configurations.}
We train models based on LLaMA\footnote{We also train models based on LLaMA 2~\citep{touvron2023llama2}. However, no significant difference is observed in our experiments.}~\citep{touvron2023llama} following the training schedule of~\citet{vicuna-2023} for a fair comparison.
Considering the large size of certain test datasets, we perform evaluation by sampling up to 200,000 passage-query pairs from each dataset.
We use strict entity-level micro-$F_1$ in evaluation, requiring both the entity type and boundary to exactly match the ground truth.

\smallskip
\noindent\textbf{Compared models.}
We compare our model (\shortname{}) against the following models: (1) \textbf{ChatGPT} (\texttt{gpt-3.5-turbo-0301}).
We use the prompting template in~\citet{ye2023comprehensive} for NER.
(2) \textbf{Vicuna}~\citep{vicuna-2023} is finetuned with ChatGPT conversations, using LLaMA as the base model.
(3) \textbf{InstructUIE}~\citep{wang2023instructuie} is a supervised model finetuned on diverse information extraction datasets, employing a unified natural language generation objective.
It adopts Flan-T5 11B~\citep{chung2022scaling} as the base model.

\subsection{Distillation}
\label{ssec:results_distill}
We first evaluate the models in a zero-shot setting.
We compare the performance of ChatGPT, Vicuna, and our model \shortname, which is distilled from ChatGPT NER annotations on Pile without human-labeled datasets in training.
Results are shown in Fig.~\ref{fig:self-supervised-bar}.\footnote{Due to limited space, we only show the average $F_1$ of all datasets and the average $F_1$ of each domain. See Appendix Fig.~\ref{fig:add-supervised-bar} for full results.}
We observe that our distilled models, namely \shortname-7B and \shortname-13B, outperform ChatGPT in terms of average $F_1$.
The average $F_1$ scores of \shortname-7B and \shortname-13B are 41.7\% and 43.4\%, respectively, compared to 34.9\% for ChatGPT.
This demonstrates that our proposed targeted distillation from diverse inputs yields models that have superior performance on a broad application class while maintaining a relatively small model size.
Additionally, \shortname-13B exhibits better performance compared to \shortname-7B, indicating that fine-tuning on larger models may lead to improved generalization.
In terms of domains, both \shortname-7B and \shortname-13B outperform ChatGPT on all domains, showing that the improvements exist across various domains.

\begin{wraptable}{r}{6cm}
\vspace{-15pt}
\centering
\tablestyle{4pt}{1.3}
\setlength{\tabcolsep}{2pt}
\begin{tabular}{c|cc|c}
\shline
& BERT- & InstructUIE & \shortname \\
Dataset & base & 11B & 7B \\
\shline
ACE05 & \textbf{87.30} & 79.94 & 86.69 \\
AnatEM & 85.82 & 88.52 & \textbf{88.65} \\
bc2gm & 80.90 & 80.69 & \textbf{82.42} \\
bc4chemd & 86.72 & 87.62 & \textbf{89.21} \\
bc5cdr & 85.28 & 89.02 & \textbf{89.34} \\
Broad Twitter & 58.61 & 80.27 & \textbf{81.25} \\
CoNLL03 & 92.40 & 91.53 & \textbf{93.30}\\
FabNER & 64.20 & 78.38 & \textbf{81.87}\\
FindVehicle & 87.13 & 87.56 & \textbf{98.30} \\
GENIA & 73.3 & 75.71 & \textbf{77.54} \\
HarveyNER & \textbf{82.26} & 74.69 & 74.21 \\
MIT Movie & 88.78 & 89.58 & \textbf{90.17} \\
MIT Restaurant & 81.02 & \textbf{82.59} &82.35 \\
MultiNERD & 91.25 & 90.26 & \textbf{93.73} \\
ncbi & 80.20 & 86.21 & \textbf{86.96} \\
OntoNotes & \textbf{91.11} & 88.64 & 89.91 \\
PolyglotNER & \textbf{75.65} & 53.31 & 65.67 \\
TweetNER7 & 56.49 & \textbf{65.95} & 65.77 \\
WikiANN & 70.60 & 64.47 & \textbf{84.91} \\
wikiNeural & 82.78 & 88.27 & \textbf{93.28} \\ \hline
Avg & 80.09 & 81.16 & \textbf{84.78} \\
\shline
\end{tabular}
\caption{$F_1$ on 20 datasets used in \citet{wang2023instructuie}. BERT-base results are from~\citet{wang2023instructuie}. InstructUIE results are from our reevaluation.}
\label{tab:supervised-eval}
\vspace{-38pt}
\end{wraptable}

We further compare different variations of \shortname, including (1) \shortname-all-in-one, where the extraction of all entity types are combined into one query and response, and (2) \shortname-definition, where queries in instruction tuning data use entity type definitions generated by ChatGPT instead of entity types.
Results are shown in Fig.~\ref{fig:self-supervised-ablation}.
We observe that both \shortname-all-in-one and \shortname-definition underperform \shortname-type by 3.3\% and 11.8\% on average, respectively.
The \shortname-definition variant's decreased performance could be due to its lower consistency with the evaluation datasets, which all adopt words or short phrases as labels instead of sentences.
The performance disparity in the \shortname-all-in-one variant can be potentially attributed to the attention distribution and task complexity.
When the model is required to handle multiple entity types within a single query, it might disperse its attention across these varied types, possibly resulting in less accurate identification for each individual type.
Conversely, by decomposing the task into several simpler ones, each focusing on one entity type at a time, the model might be better equipped to handle the complexity, thus yielding more accurate results.

\subsection{Supervised Finetuning}
\label{ssec:results_supervised}
We study whether our models can be further improved using additional human annotations.
We compare the performance of ChatGPT, Vicuna, InstructUIE~\citep{wang2023instructuie}~\footnote{Please note that the original evaluation script in InstructUIE contains a critical bug. For passages that do not contain any entities, the script adds \textsc{none} as a placeholder entity and takes it into account when calculating $F_1$. To rectify this error, we re-evaluated InstructUIE using their released checkpoint.}, and \shortname.

\smallskip
\noindent\textbf{Out-of-domain evaluation.} We first study whether supervised finetuning leads to better generalization on unseen data.
We follow InstructUIE to exclude two datasets CrossNER~\citep{liu2021crossner} and MIT~\citep{liu2013asgard} for out-of-domain evaluation, and fine-tune our model using training splits of the remaining datasets in the universal NER benchmark.
Results are shown in Tab.~\ref{tab:zero_shot_2}.
Notably, without any fine-tuning, instruction-tuned \shortname{} 7B and 13B already surpass ChatGPT, Vicuna, and the supervised fine-tuned InstructUIE-11B by a large margin.
If we train our model from scratch only using the supervised data, it achieves an average $F_1$ of 57.2\%.
Continual fine-tuning \shortname-7B using the supervised data achieves the best average $F_1$ of 60.0\%.
These findings suggest that the models' generalization can be further improved with additional human-annotated data.

\begin{table*}[!t]
\centering
\tablestyle{4pt}{1.1}
\begin{tabular}{l|ccccccc|c}
\shline
\multicolumn{1}{c|}{Model} & Movie & Restaurant & AI & Literature & Music & Politics & Science & Avg  \\ \shline
\emph{Zero-shot} & & & & & & & & \\
\quad Vicuna-7B& 6.0& 5.3& 12.8& 16.1& 17.0& 20.5& 13.0& 13.0 \\
\quad Vicuna-13B& 0.9& 0.4& 22.7& 22.7& 26.6& 27.2& 22.0& 17.5 \\
\quad ChatGPT & 5.3 & 32.8 & 52.4 & 39.8 & \textbf{66.6} & \textbf{68.5} & \textbf{67.0} & 47.5 \\
\quad \shortname-7B & 42.4& 31.7& 53.5& 59.4& 65.0 & 60.8& 61.1& 53.4 \\ 
\quad \shortname-13B & \textbf{48.7} & \textbf{36.2} & \textbf{54.2} & \textbf{60.9} & 64.5& 61.4 & 63.5 & \textbf{55.6} \\ \shline
\emph{In-domain supervised} & & & & & & & & \\
\quad InstructUIE-11B  & - & - & 48.4 & 48.8 & 54.4 & 49.9 & 49.4 & - \\
\quad \shortname-7B (sup. only) & 54.2& 16.0& 62.3& \textbf{67.4}& 69.0& 64.5& 66.9& 57.2 \\
\quad \shortname-7B (inst-tuned + sup.) & \textbf{61.2}& \textbf{35.2}& \textbf{62.9}& 64.9& \textbf{70.6}& \textbf{66.9}& \textbf{70.8}& \textbf{61.8} \\
\shline
\end{tabular}
\caption{Out-of-domain evaluation on datasets from \citet{wang2023instructuie}. ``sup. only'' denotes a variant of \shortname-7B, trained from scratch using in-domain supervised data only and evaluated on out-of-domain datasets.}
\label{tab:zero_shot_2}
\end{table*}

\smallskip
\noindent\textbf{In-domain evaluation.}
We then study the performance of \shortname in an in-domain supervised setting, where we fine-tune \shortname-7B using the same training data as InstructUIE~\citep{wang2023instructuie}.
Results are shown in Tab.~\ref{tab:supervised-eval}.
Our \shortname-7B achieves an average $F_1$ of 84.78\% on the 20 datasets, surpassing both BERT-base and InstructUIE-11B by 4.69\% and 3.62\%, respectively.
This experiment demonstrates the effectiveness of our model in the supervised setting.

\subsection{Analysis}
\label{ssec:ablation_study}

\begin{table*}[!ht]
\centering
\tablestyle{4pt}{1.3}
\begin{tabular}{l|ccccccc|c}
\shline
Strategy & Movie & Restaurant & AI & Literature & Music & Politics & Science & Avg  \\ \shline
None & 19.1& 19.1& 25.1& 39.5& 42.7& 48.9& 26.2& 31.5 \\
Uniform & 42.5& 29.0& 42.5& 53.3& 57.4& 56.8& 52.6& 47.7 \\
\rowcolor{defaultcolor} Frequency &42.4& 31.7& 53.5& 59.4& 65.0& 60.8& 61.1& 53.4 \\
\shline
\end{tabular}
\caption{Ablation study on negative sampling strategies for \shortname-7B. All models are instruction-tuned on Pile.}
\label{tab:neg-sample-ablation}
\end{table*}

\begin{wrapfigure}[23]{r}{0.5\textwidth}
\vspace{-22pt}
    \centering
    \includegraphics[width=0.48\textwidth]{figs/data_specific_prompt.pdf}
    \caption{Different in $F_1$ between data-specific and original templates in the supervised setting. \textcolor[HTML]{ff7f0e}{Orange} and \textcolor[HTML]{1f77b4}{Blue} mark datasets with/without label overlap with other datasets, respectively.}
    \label{fig:dataset_specific_inst}
\end{wrapfigure}

\smallskip
\noindent\textbf{Negative sampling strategies.}
We experiment with different negative sampling strategies in instruction tuning, including (1) \emph{no negative sampling}, (2) \emph{uniform sampling} where entity types are randomly sampled with equal probability for each one, and (3) \emph{frequency-based sampling} where we sample entity types with probabilities proportional to their frequency in the constructed dataset.
Results are shown in Tab.~\ref{tab:neg-sample-ablation}.
Among the approaches tested, frequency-based sampling yielded the best results, outperforming no sampling and uniform sampling by 21.9\% and 5.7\%, respectively. 
These findings highlight the crucial role of negative sampling in instruction tuning, with frequency-based sampling emerging as the most effective method for enhancing model performance in our study.

\smallskip
\noindent\textbf{Dataset-specific template.}
We compare the results of our dataset-specific instruction tuning template and the original template in the supervised setting.
As shown in Fig.~\ref{fig:dataset_specific_inst},
we find that the data-specific template outperforms the original template on most datasets.
To gain deeper insights into the improvements achieved, we further divide the datasets into two categories: those with label (entity type) overlap with other datasets and those without overlap. 
Our analysis reveals that datasets with label overlap demonstrate more substantial improvements.

To explore this further, we measure $F_1$ score across all evaluation datasets and calculate the difference. Apart from the long-tail entity types that manifest a high variance in results, we identify two entity types where the dataset-specific template outperforms the original template by over 10\%: \textsc{facility} (22.0\%) and \textsc{time} (12.4\%). 
Intriguingly, both labels exhibit inconsistencies in their definitions across various datasets.
The \textsc{facility} label has been annotated on pronouns (e.g., it, which) as entities in ACE datasets but are excluded in OntoNotes.
The \textsc{time} label denotes well-defined time intervals (e.g., Christmas) in MultiNERD, but may encompass any general time expressions (e.g., 3 pm) in OntoNotes.
This finding suggests that the improvements provided by the data-specific template are particularly effective in resolving label conflicts.

\begin{table*}[!t]
\centering
\tablestyle{3pt}{1.2}
\begin{tabular}{c|lrcccccc|c}
\shline
Partial match & Model & Movie & Restaurant & AI & Literature & Music & Politics & Science & Avg  \\ \shline
\multirow{3}{*}{No}& ChatGPT & 5.3 & 32.8 & 52.4 & 39.8 & 66.6 & 68.5 & 67.0 & 47.5 \\
& \shortname-7B & 42.4& 31.7& 53.5& 59.4& 65.0& 60.8& 61.1& 53.4 \\
& \shortname-7B w/ sup & 61.2& 35.2& 62.9& 64.9& 70.6& 66.9& 70.8& 61.8 \\
\shline
\multirow{3}{*}{Yes}& ChatGPT& 5.9& 40.1& 55.7& 42.8& 70.2& 71.7& 70.1& 50.9 \\
& \shortname-7B & 46.9& 40.3& 57.7& 62.7& 62.9& 63.2& 63.3& 56.7 \\
& \shortname-7B w/ sup & 65.5& 39.4& 66.2& 67.2& 72.7& 68.9& 73.4& 64.8 \\
\shline
\end{tabular}
\caption{Allowing partial match between the prediction and the gold that has overlap increases the results.
When it is allowed, any partial match is regarded as half correct (counted as 0.5 in true positive) when computing $F_1$.
}
\label{tab:loose-$F_1$}
\end{table*}

\smallskip
\noindent\textbf{Evaluation with partial match.}
While using strict $F_1$ as an evaluation metric, we notice that it may underestimate the zero-shot learning capabilities of NER models.
In particular, strict $F_1$ penalizes slight misalignments in the boundaries of the extracted entities, which may not necessarily indicate an incorrect understanding of the text. 
For instance, given the sentence \textit{any asian cuisine around} and the entity type \textsc{cuisine}, \shortname extracts \textit{asian cuisine} as the named entity, while the ground truth only labels \textit{asian} as the correct entity.
However, the model's prediction can still be viewed as correct, even though it is deemed incorrect by strict $F_1$.
To better estimate the zero-shot abilities, we also consider partial match~\citep{segura2013semeval} in evaluation.
In this context, a prediction that exhibits word overlap with the ground truth is regarded as half correct (counted as 0.5 in true positive) when computing $F_1$.
Results are shown in Tab.~\ref{tab:loose-$F_1$}.
We find that allowing partial match consistently improves the results.
Besides, our models is still the best-performing model on average.

\subsection{Case Study}
\label{ssec:case_study}

\smallskip
\noindent \textbf{Sensitivity to entity type paraphrasing.}
One type of entity can be expressed in multiple ways, so it is essential for our model to give consistent predictions given entity types with similar meanings.
An example of sensitivity analysis is present in Fig.~\ref{fig:sensitivity}.
We observe that \shortname-7B-type %is not robust to different entity types, as it
sometimes fails to recognize entities with similar semantic meanings.
On the other hand, \shortname-7B-definition, despite performing worse on our Universal NER benchmark, exhibits robustness to entity type paraphrasing.
It demonstrates that although using definitions may result in lower performance on standard NER benchmarks, it could yield improved performance for less populous entity types.

\smallskip
\noindent \textbf{Recognition of diverse entity types.}
We present an example in Fig.~\ref{fig:code-ex} showcasing the capabilities of \shortname in recognizing various entities.
Particularly, we focus on a novel domain of code and assess \shortname's ability to extract diverse types of entities within the code.
Despite minor mistakes (e.g., \texttt{from\_pretrained} is not identified as a method), this case study effectively demonstrates our model's capacity to capture entities of various types.

\section{Conclusion}
We present a targeted distillation approach with mission-focused instruction tuning. Using NER as a case study, we train smaller and more efficient models for open-domain NER. The proposed method successfully distills ChatGPT into a smaller model \longname{}, achieving remarkable NER accuracy across a wide range of domains and entity types without direct supervision. These models not only retain ChatGPT's capabilities but also surpass it and other state-of-the-art systems in NER performance.

\section*{Acknowledgement}

Wenxuan Zhou and Muhao Chen were supported by the NSF Grants IIS 2105329 and ITE 2333736.

\newpage

\end{document}