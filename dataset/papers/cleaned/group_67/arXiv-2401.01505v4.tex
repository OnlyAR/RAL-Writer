\title{[Article Title]{Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex and Professional Sports}



\author[1]{\fnm{Haopeng} \sur{Li}}

\author[2]{\fnm{Andong} \sur{Deng}}

\author*[3]{\fnm{Jun} \sur{Liu}}

\author[4]{\fnm{Hossein} \sur{Rahmani}}

\author[5]{\fnm{Yulan} \sur{Guo}}

\author[6]{\fnm{Bernt} \sur{Schiele}}

\author[7]{\fnm{Mohammed} \sur{Bennamoun}}
 
\author*[1,8]{\fnm{Qiuhong} \sur{Ke}}\email{qiuhong.ke@monash.edu}


\affil[1]{\orgdiv{School of Computing and Information Systems}, \orgname{University of Melbourne}}

\affil[2]{\orgdiv{Center for Research in Computer Vision}, \orgname{University of Central Florida}}

\affil[3]{\orgdiv{Information Systems Technology and Design (ISTD) Pillar}, \orgname{Singapore University of Technology and Design}}

\affil[4]{\orgdiv{School of Computing and Communications}, \orgname{Lancaster University}}

\affil[5]{\orgdiv{School of Electronics and Communication Engineering}, \orgname{Sun Yat-sen University}}

\affil[6]{\orgdiv{Department of Computer Vision and Machine Learning}, \orgname{Max Planck Institute for Informatics, Saarland Informatics Campus}}

\affil[7]{\orgdiv{School of Physics, Maths and Computing}, \orgname{University of Western Australia}}

\affil[8]{\orgdiv{Department of Data Science \& AI}, \orgname{Monash University}}



\abstract{Reasoning over sports videos for question answering is an important task with numerous applications, such as player training and information retrieval. However, this task has not been explored due to the lack of relevant datasets and the challenging nature it presents. Most datasets for video question answering (VideoQA) focus mainly on general and coarse-grained understanding of daily-life videos, which is not applicable to sports scenarios requiring professional action understanding and fine-grained motion analysis. In this paper, we introduce the first dataset, named Sports-QA, specifically designed for the sports VideoQA task. The Sports-QA dataset includes various types of questions, such as descriptions, chronologies, causalities, and counterfactual conditions, covering multiple sports. Furthermore, to address the characteristics of the sports VideoQA task, we propose a new Auto-Focus Transformer (AFT) capable of automatically focusing on particular scales of temporal information for question answering. We conduct extensive experiments on Sports-QA, including baseline studies and the evaluation of different methods. The results demonstrate that our AFT achieves state-of-the-art performance.}



\keywords{Video Question Answering, Sports Video, Benchmark, Auto-Focus Transformer}



\maketitle


\section{Introduction}


Sports video analysis has been attracting increasing attention in recent years
\citep{yuan2021spatio, li2021groupformer, koshkina2021contrastive, zhu2022fencenet, martin2020fine, wang2022shuttlenet,6516867}.
While research progress has been made on tasks such as
sports action recognition \citep{li2021multisports, zhu2022fencenet, rasmussen2022compressing}, reasoning over sports videos for question answering
has not been explored. As humans, we can not only recognize the actions of the players in sports videos but also understand the effects of the players' actions, explain why a team loses the score, and imagine what would happen under counterfactual situations. Our impressive capabilities in reasoning allow us to answer complex questions related to sports videos, which is crucial in applications such as obtaining crucial statistics in matches for player/team performance qualification, analyzing players' actions and team strategies for performance improvement, and efficiently retrieving information for audiences and analysts.
While sports video reasoning is clearly important, it is under-explored due to challenges and the lack of datasets.


\begin{figure}[tbp]
\centering
\includegraphics[width=0.9\columnwidth]{pics/qadiff.pdf}
\caption{
Illustrations of general VideoQA, which focus on common basic understanding, and sports VideoQA, which requires professional action understanding and action relation reasoning\protect\footnotemark. 
}
\label{qadiff}
\end{figure}

\footnotetext[\thefootnote]{We treat the action ``\textit{block}" as a noun.}


In this paper, we tackle sports VideoQA, focusing on reasoning over sports videos for question answering. This is distinct from general VideoQA, which centers on general video understanding, mainly encompassing descriptive and temporal aspects \citep{jang2017tgif, xu2017video, xiao2021next, yu2019activitynet}, as depicted in Fig. \ref{qadiff}. % illustrates the difference between general VideoQA and sports VideoQA. %given a sports video, general VideoQA is mostly concerned with common %descriptive questions including  action recognition\ke{questions  such as general action recognition (i think general also has temporal or other types of questions)}, while 
Compared to general VideoQA, sports VideoQA is more challenging because the questions may involve particular teams and professional actions, and it requires a fine-grained understanding of actions and intra/inter-team relation modeling. %action understanding and intra/inter-team relations including  temporal and causal reasoning. 
As pointed out in \citep{shao2020finegym}: 1) coarse-grained global video understanding is insufficient for sports analysis where the actions are rapid and hard to differentiate; 2) the models pre-trained on coarse-grained datasets are not always beneficial to fine-grained tasks. 
In the meantime, although significant strides have been made in the fine-grained recognition of individual actions \citep{shao2020finegym,liu2022fineaction,sun2017taichi}, these datasets may fall short in effectively evaluating the ability for temporal and causal reasoning over actions performed by individuals or teams in sports scenarios.
A large-scale dataset containing professional sports scenarios with detailed annotations of sports-relevant questions is thus required for explicit and comprehensive reasoning for sports VideoQA.

To address this gap, we introduce a novel dataset called Sports-QA, the first video QA dataset specifically focusing on sports activities. Leveraging sports videos and professional action labels from MultiSports \citep{li2021multisports} and FineGym \citep{shao2020finegym}, we construct our dataset. Both sources provide high-quality sports videos with well-defined action labels, allowing us to annotate essential attributes for each crucial action instance. Using these annotations, we generate QA pairs based on pre-defined templates covering aspects such as description, chronology, causality, and counterfactual conditions. The Sports-QA dataset is the first of its kind, comprising approximately 94K QA pairs, encompassing multiple sports and various question types for sports VideoQA. Table \ref{set} provides a comparison between Sports-QA and several widely-used VideoQA datasets.

\begin{table}[tbp]
\caption{VideoQA datasets comparison. ``Auto/Man" represents ``automatic/manual". ``MC/OE" represents ``multi-choice/open-ended".}
\label{set}
\begin{tabular}{@{}lcrrcc@{}}
\toprule
Dataset                           & Topic   & \multicolumn{1}{c}{\#Video} & \multicolumn{1}{c}{\#QA} & Annotation  & QA Task  \\ \midrule
MSRVTT-QA \citep{xu2017video}      & \multirow{4}{*}{General} & 10K                         & 244K                     & Auto        & OE       \\
MSVD-QA \citep{xu2017video}        &  & 2K                          & 51K                      & Auto        & OE       \\
NExT-QA \citep{xiao2021next}       &  & 5.4K                        & 52K                      & Man         & OE\&MC \\
TGIF-QA \citep{jang2017tgif}       &  & 72K                         & 165K                     & Auto\&Man & OE\&MC \\ \midrule
MovieQA \citep{tapaswi2016movieqa} & Movie   & 408                         & 15K                      & Man         & MC       \\
Social-IQ \citep{zadeh2019social}  & Social  & 1K                          & 8K                       & Man         & MC       \\
DramaQA \citep{choi2021dramaqa}    & Drama   & 24K                         & 16K                      & Auto\&Man & MC       \\
\textbf{Sports-QA (Ours)}                  & Sports  & 6K                          & 94K                      & Auto\&Man & OE       \\ \botrule
\end{tabular}
\end{table}


Compared with existing datasets, our Sports-QA provides new insights for VideoQA: 1) It encompasses questions related to specific terms and actions in both group activities (e.g., basketball) and single-person sports (e.g., gym). This requires models to possess the capability to capture dynamic patterns and perform reasoning in scenarios with varying numbers of interactions for question answering. 2) To achieve a comprehensive understanding of complex sports videos, Sports-QA includes diverse questions that involve video information at various temporal granularities. This encompasses global long-term temporal dependencies and fine-grained short-term ones. For instance, a question asking about the number of actions requires the model to capture global dependencies, while a question about the effect of a specific action relies on short-term temporal information. Furthermore, Sports-QA has the potential to be leveraged for other tasks. The pre-defined attributes of actions can be treated as multiple labels, allowing the dataset to be used for multi-label classification tasks for comprehensive action understanding. Additionally, based on the annotated action attributes, we can modify the templates from question answering to declarative sentences, generating descriptive or explanatory narrations and enabling comprehensive sports video captioning.


In addition to the dataset, we present a novel method for sports VideoQA. Recognizing that sports VideoQA demands the model to capture information from multiple frames at various temporal granularities, we introduce the Auto-Focus Transformer (AFT), featuring an Auto-Focus Attention mechanism. Conceptually, the model operates akin to a temporal magnifying glass with a dynamic focal length, allowing it to inspect the video to answer questions with diverse temporal dependencies. More specifically, our designed Auto-Focus Attention dynamically selects attention focus based on the question. This mechanism empowers the model to handle questions involving video information across various time spans.




The contributions are summarized as follows:

\begin{itemize}

    \item We contribute a large-scale dataset, which consists of various types of questions and multiple sports for VideoQA.  %It provides  various question types and multiple sports. It also provides   
To our knowledge, it is the first dataset for complex reasoning over professional sports actions. 

    \item We propose a new Auto-Focus Transformer (AFT), where the attention focus is adaptive based on the question,  enabling the model to deal with questions requiring temporal information of various scales.


    \item We conduct extensive experiments on our dataset, including baseline study, evaluation of existing methods, and visualization of predictions. The results show the superiority of the proposed AFT for sports VideoQA. %limitations of existing methods and provide directions for future improvements.
\end{itemize}






\section{Related Work}


\noindent\textbf{VideoQA Datasets.}  The development of VideoQA has been greatly facilitated by the emergence of various datasets, such as TGIF-QA \citep{jang2017tgif}, MSVD-QA, and MSRVTT-QA \citep{xu2017video}, DramaQA \citep{choi2021dramaqa}, NExT-QA \citep{xiao2021next}, NExT-OOD \citep{10107423}. TGIF-QA, for instance, offers a comprehensive set of four sub-tasks designed for temporal reasoning in general videos and is widely recognized in the VideoQA community. MSVD-QA and MSRVTT-QA are open-ended datasets constructed from existing video captions, while DramaQA focuses on understanding drama stories with hierarchical QAs and character-centered video annotation. NExT-QA, introduced for describing and explaining temporal actions, provides both multi-choice and open-ended questions through manual annotation. However, existing datasets predominantly center around general video understanding in daily scenarios or involve coarse-grained action/event reasoning. \textbf{In contrast, our contribution lies in fine-grained and professional analysis within sports scenarios. We present Sports-QA, a dataset that addresses aspects of description, chronology, causality, and counterfactual conditions for multiple sports of diverse characteristics.}

\noindent\textbf{VideoQA Methods.} 
VideoQA poses a significant challenge as it necessitates models to grasp both spatial and temporal information from videos to answer questions. Various deep models have been developed, approaching this task from different perspectives \citep{fan2019heterogeneous,jiang2020reasoning,li2022invariant,antol2015vqa,gao2018motion,10172254,9770842,10214041,10146482}.
For instance, the deep heterogeneous graph alignment network by Jiang et al. \citep{jiang2020reasoning} addresses VideoQA by simultaneously aligning intra/inter-modality information. Another approach involves a multimodal attention model proposed by Fan et al. \citep{fan2019heterogeneous}, where heterogeneous memory learns global context from visual features, and question memory captures the complex semantics of questions.
IGV, introduced by Li et al. \citep{li2022invariant}, grounds question-critical scenes in videos by considering causal relations that remain invariant to complement contents.
However, a limitation in these approaches is the lack of consideration for the fact that different questions may require temporal dependencies of specific scales. \textbf{In response to this challenge, we propose the Auto-Focus Transformer, designed to automatically focus on a specific temporal scale based on the question for VideoQA.}


\noindent\textbf{Sports Video Understanding.} Sports video understanding has drawn increasing attention in recent years \citep{yuan2021spatio,li2021groupformer,koshkina2021contrastive,zhu2022fencenet,martin2020fine,wang2022shuttlenet}. Researchers have made great efforts in various tasks such as sports action recognition \citep{shao2020finegym}, multi-person action detection \citep{li2021multisports}, and action quality assessment \citep{tang2020uncertainty}. Meanwhile, numerous sports datasets are constructed \citep{parmar2019action,li2021multisports,giancola2018soccernet,deliege2021soccernet}.  
For example, AQA-7 \citep{parmar2019action} is constructed for professional action quality assessment. Seven types of actions are included in this dataset and all the action instances are associated with quality scores. 
MultiSports \citep{li2021multisports} is proposed for the spatial-temporal detection of professional sports actions. FineGym~\citep{shao2020finegym} focuses on fine-grained activity localization that requires an accurate understanding of the atomic level of a gymnastic action.
\textbf{In this work, we construct Sports-QA, the first dataset for sports VideoQA, bridging the gap between the visual modality and the textual modality in sports analysis.}






\section{Sports-QA Dataset}


\begin{table}[tbp]
\caption{The statistics for the MultiSports dataset are presented.} 
\label{mss}
\begin{tabular}{@{}lcrcc@{}}
\toprule
 Sports          & \# Action & \# Instance & Avg. Action/Video Duration & \# Bounding box \\ \midrule
Gym        & 21      & 8,703     & 1.5s / 30.7s        & 325K    \\
Volleyball & 12      & 7,645     & 0.7s / 10.5s        & 139K   \\
Football   & 15      & 12,254    & 0.7s / 22.6s        & 225K    \\
Basketball & 18      & 9,099     & 0.9s / 19.7s        & 213K   \\\midrule
Total      & 66      & 37,701    & 1.0s / 20.9s        & 902K   \\ \botrule
\end{tabular}
\end{table}

\subsection{Data Source}

Regarding the collection of sports videos, we consider the following aspects: 1) The visual quality of the video data should be high enough to conduct fine-grained video understanding, such as video resolution and frame rate. 2) Instead of applying to a single type of sports, we expect a VideoQA dataset involving multiple sports.
After a deep survey of the works on sports video understanding, we find that the MultiSports \citep{li2021multisports} and FineGym \citep{shao2020finegym} datasets are highly suitable for our purpose. The details of MultiSports and FineGym are as follows.


\noindent\textbf{MultiSports} \citep{li2021multisports} is a dataset for the temporal localization of sports actions, encompassing four sports (i.e., basketball, football, volleyball, and aerobic gymnastics) and 66 fine-grained action categories\footnote{Project homepage: \url{https://deeperaction.github.io/datasets/multisports.html}}. The action categories in MultiSports are professional terms (such as ``\textit{volleyball spike}", ``\textit{football tackle}", and ``\textit{basketball defensive rebound}") instead of common and atomic actions like ``\textit{run}" and ``\textit{stand}". For each action instance, the dataset provides the bounding boxes of a player from the starting frame of an action to the ending frame, forming the action spatial-temporal tube. The statistics of MultiSports are shown in Table \ref{mss}.

\noindent\textbf{FineGym} \citep{shao2020finegym} is a dataset designed to elevate the field of action recognition by addressing the limitations observed in existing techniques\footnote{Project homepage: \url{https://sdolivia.github.io/FineGym}}. Developed to surpass current benchmarks, it offers a unique combination of richness, quality, and diversity in its content.
This dataset is constructed on gymnasium videos, providing a realistic and varied environment for action recognition studies. What sets FineGym apart from other datasets is its meticulous temporal annotation at both action and sub-action levels, featuring a three-level semantic hierarchy. This hierarchical structure allows for a more nuanced understanding of activities, enabling researchers to explore and analyze actions in finer detail. As an illustrative example, an event is annotated as a sequence of elementary sub-actions Importantly, each sub-action within these sets is further annotated with finely defined class labels, contributing to a higher level of granularity in action recognition. Fig. \ref{ms} shows the action hierarchy of the MultiSports and FineGym datasets.




\begin{figure}[tbp]
\centering
\includegraphics[width=\columnwidth]{pics/MultiSports1.pdf}
\caption{The action hierarchy of the MultiSports dataset is depicted at the top, while the FineGym dataset's hierarchy is shown at the bottom. It's important to note that the figure includes only four example actions for each sport.}
\label{ms}
\end{figure}

The action categories in MultiSports and FineGym are professional terms (such as ``\textit{volleyball spike}", ``\textit{football tackle}", ``\textit{basketball defensive rebound}", and ``\textit{vault Salto backward tucked}") instead of common and atomic actions (such as ``\textit{run}" and ``\textit{stand}"). Such professional labels are of great value to our dataset.
Since the length of videos varies greatly (from a few seconds to a few minutes), we segment the videos into clips and generate QA pairs based on these clips.

\subsection{Question-Answer Pair Generation}

In most existing works, QA pairs are generated using two approaches:
1) Automatically generating questions and answers based on video captions using a set of predefined question templates \citep{jang2017tgif, xu2017video, yang2021just}. This method is efficient, but it may introduce obvious grammatical errors or lose crucial information during the conversion of captions into questions and answers.
2) Manually annotating questions and answers through crowdsourcing \citep{xiao2021next, yu2019activitynet, garcia2020knowit}. Although this approach produces QA pairs of high quality in terms of accuracy and expression, manual annotation is time-consuming and expensive.
In this work, we aim to ensure the quality of the textual data while considering annotation costs. We achieve this by generating QA pairs using pre-defined templates based on existing labels and newly-labeled attributes of actions. Our approach guarantees grammatically error-free QA pairs that capture crucial information in videos, thanks to the careful design of sophisticated templates. Furthermore, as ball games and gymnastics have distinct characteristics, we generate the QA pairs in different ways as described as follows.

\subsubsection{QA Pair Generation for Ball Games}

Specifically, for ball games, including basketball, football, and volleyball, we define five attributes (Team, Outcome, Cause of Outcome, Cause of Action, and Effect of Action) for actions. We then manually annotate the attributes of each action. The definitions and annotation process of these attributes are elaborated as follows.


\noindent\textbf{Attribute Definition.} MultiSports consists of 45 action categories from ball games, with some being crucial for professional sports statistics or quantifying the performance of players/teams. For instance, the ``\textit{2-Point Field Goal Percentage}" in a basketball match necessitates a fine-grained understanding of all ``\textit{2-point shot}" actions, while the ``\textit{saving}" actions in football directly reflect the performance of goalkeepers. In this work, we focus on 28 crucial actions, listed in Table \ref{acts}. Considering both practical applications and research purposes, we define five attributes for each of the crucial action instances as follows.

\noindent 1. \texttt{Team}: The team that the player of the action belongs to is denoted by two options: \texttt{left team} or \texttt{right team}. Specifically, in basketball and football, the determination of the left or right team depends on the team attacking towards the right or left. In volleyball, the left or right team is designated based on their position relative to the left or right side of the net.

\noindent 2. \texttt{Outcome}: This attribute assesses whether the result of the action meets the expectation and is defined as a binary label: \texttt{successful} or \texttt{failed}. For example, if a ``\textit{2-point shot}" in basketball scores a goal, it is annotated as successful; otherwise, it is labeled as failed.

\noindent 3. \texttt{Cause of Outcome}: 
This attribute indicates the cause of failure for offensive actions, such as a ``\textit{2-point shot}" in basketball, ``\textit{shoot}" in football, and ``\textit{spike}" in volleyball, or the cause of success for defensive actions, like ``\textit{block}" in football and ``\textit{save}" in volleyball. Specifically, the cause of failure for offensive actions reflects player shortcomings. For example, the failure of an offensive ``\textit{long pass}" in football could be attributed to \texttt{defensive interception}, \texttt{bad pass}, or \texttt{bad catch}. Identifying the precise cause helps teams identify their weaknesses and improve performance. Similarly, the cause of success for defensive actions highlights player strengths. For instance, the success of a defensive ``\textit{save}" in volleyball might be due to \texttt{offensive out of bounds}, \texttt{offensive blocked by net}, or \texttt{actually catch the ball}, with the last case directly reflecting defensive performance. We do not consider the cause of success for offensive actions or the cause of failure for defensive actions, as they cannot be attributed to different cases. For example, the failure of ``\textit{football saving}" can only be explained by the goalkeeper missing the ball. Therefore, these causes are not worth discussing. We define various cause options for applicable action categories and ask annotators to choose from them. Note that this attribute is applicable to certain action categories, and the causes vary for different action categories.

\noindent 4. \texttt{Cause of Action}: This attribute indicates the actions that cause the current action. Each action instance in a video is initially labeled with its time order, based on the start time, which serves as the unique ID of the action in the video. For each crucial action, annotators are asked to provide the IDs of the actions that cause the current action; these causes are not necessarily crucial actions. Consequently, the \texttt{Cause of Action} is a list consisting of the IDs of actions that are mostly temporally adjacent to the current action. Not all crucial action categories are required to provide this attribute because the causes of some actions are unique. For instance, the only cause of ``\textit{volleyball first pass}" is ``\textit{volleyball serve}".

\noindent 5. \texttt{Effect of Action}: This attribute aims to identify the actions caused by the current action, analogous to \texttt{Cause of Action}. Although the cause and effect are conjunctive, we may consider only one of them, as some actions are not crucial. For instance, if ACTION M (crucial action) causes ACTION N (non-crucial action), we only label ACTION N as an effect of ACTION M.


\noindent\textbf{Attribute Annotation.}
After defining the attributes mentioned above, we proceed to annotate them for each crucial action instance in MultiSports. The annotation process is divided into three stages: pre-annotation, formal annotation, and quality check.
In the pre-annotation stage, annotators are grouped into three categories, with each group assigned responsibility for a specific type of ball game. Each annotator within a group is then assigned several videos corresponding to their designated sport. During this stage, annotators are tasked with labeling the attributes of the first 50 action instances. Following this, an intra-group check is conducted to address any issues related to understanding bias or potential mistakes, as well as to handle unexpected or rare situations. The annotation can only progress to the next stage when annotators achieve a consistent understanding of each attribute, as outlined in the protocols.
Moving to the formal annotation stage, annotators use shared protocols to amend their previous annotations and label the remaining action instances. Once all actions are labeled, we perform an inter-group quality check on all annotations. The purpose of this inter-group check is to ensure that the annotations align with the common understanding of average individuals.
The annotation process is completed within one month and involves the collaboration of 15 graduate students.


Based on the attributes, we generate questions about the videos by designing various templates (listed in the end of the paper) to inquire about these attributes. For instance, given a video shown in Fig. \ref{egdata}, after annotating the attributes for the action ``\textit{spike}", a question querying a particular attribute (e.g., Outcome) of the action can be generated, such as ``\textit{Is the `spike' of the right team successful?}".
Specifically, our dataset involves four types of questions: descriptive, temporal, causal, and counterfactual. The details of each type are described as follows.

\begin{figure*}[tbp]
\centering
\includegraphics[width=0.93\textwidth]{pics/egs.pdf}
\caption{Example of Sports-QA: The actions in the green boxes (such as ``\textit{2-point shot}") are the query actions, while the actions in the yellow boxes (such as ``\textit{block}") represent the effects. For ball games, annotators provide attribute labels, and we generate QA pairs based on these attributes. In gymnastics, we generate QA pairs using annotations from MultiSports/FineGym.
}
\label{egdata}
\end{figure*}



\noindent 1. \textbf{Descriptive:} Descriptive questions require holistic comprehension and involve various aspects of information from the videos. These questions include simple queries such as ``\textit{What is the video about?}" and ``\textit{Does SOME-TEAM perform SOME-ACTION?}", as well as complex ones involving counting and the outcome of actions, such as ``\textit{How many times does SOME-TEAM perform SOME-ACTION?}" and ``\textit{Does SOME-TEAM successfully do their i-th SOME-ACTION?}"

\noindent 2. \textbf{Temporal:} Temporal questions focus on the temporal relations among the actions of the same or different teams. Specifically, these questions require an understanding of intra-group temporal relations and inter-group ones. Typical templates for this type of question are ``\textit{What does SOME-TEAM do before/after their i-th SOME-ACTION?}" and ``\textit{What does the left/right team do before/after the other team performs their i-th SOME-ACTION?}"

\noindent 3. \textbf{Causal:} Causal questions aim to uncover the causal relations among the action instances and to explain the reasons or the specific process of the actions. Different from temporal questions, these questions require causal reasoning based on visual cues. The templates of these questions include causal queries such as ``\textit{Why does SOME-TEAM do the i-th SOME-ACTION?}" and ``\textit{What is the effect of the i-th SOME-ACTION of SOME-TEAM?}", as well as explanation queries such as ``\textit{How does SOME-TEAM succeed in doing/fail to do the i-th SOME-ACTION?}"

\noindent 4. \textbf{Counterfactual:} Different from the above three types of questions querying the details that actually happen, counterfactual questions set hypothetical conditions that do not occur in the video and query about the expected outcomes based on the conditions. This type of question requires reasoning about various imagined situations and expects the outcomes according to the causal relations among actions. The template for this type of question is ``\textit{Would the left/right team succeed in do the i-th SOME-ACTION if the other team did not do SOME-ACTION?}"


\noindent\textbf{From Attributes to Question-Answer Pairs.} In particular, the questions are queries about the attributes, and their answers can be obtained directly from our annotations or through logical reasoning and statistical analysis over the annotations. For instance, for a question asking about the cause of a specific action, the answer can be easily retrieved from its attribute of cause; For a question asking whether a specific action is successful, we can check its outcome attribute. We guarantee the correctness of the answers as we have checked the annotations of attributes as mentioned previously.
These answers include responses like ``\textit{yes/no}", numerical values, sports names (such as ``\textit{volleyball}"), action categories in MultiSports and FineGym (such as ``\textit{block}"), and short phrases describing the reasons (such as ``\textit{out of bounds}"). See Fig. \ref{egdata} for more concrete examples of the attributes and the corresponding generated QA pairs. 






An interesting aspect of our dataset is that it can be utilized for other tasks due to our two-stage annotation process. Specifically, we have defined several attributes for each action instance, which can be considered as multiple labels. Consequently, our dataset can be employed for conducting multi-label classification.
Moreover, by altering the template from question answering to declarative sentences, we can generate descriptive or explanatory narrations for sports videos. Hence, our dataset can also be exploited for sports video captioning.

\subsubsection{QA Pair Generation for Gymnastics}

A major difference between aerobic gymnastics/FineGym and ball games is the absence of the concept of a team. Furthermore, there are no causal relations among the actions performed in these sports. Therefore, we only generate descriptive and temporal questions for aerobic gymnastics/FineGym based on their annotations. In addition to the templates used for ball games, we have also designed some new templates. For descriptive questions, we include queries about the number of players or actions, such as ``\textit{How many actions does the player perform?}" and ``\textit{How many times does the player perform SOME-ACTION?}" For temporal questions, we add counting questions like ``\textit{How many times do the players do SOME-ACTION before SOME-ACTION?}" Such a QA design leverages the fine-grained features in the datasets and constitutes challenging temporal reasoning that forces the models to focus on the salient sub-actions in the whole gymnastic event.






\subsection{Diversity, Debiasing and Problem Setting}

\noindent \textbf{Diversity of Questions.} The limited diversity in generating questions using pre-designed templates may not significantly impact the significance of datasets for several reasons. First, in the context of sports, there is a high level of restriction imposed by rules and specific content, resulting in a more focused attention from the audience on particular actions and events. This inherent structure in sports content narrows down the range of potential questions, making template-based question generation a feasible approach.
Secondly, the practice of template-based question generation is widely accepted and employed in the construction of VideoQA datasets \citep{jang2017tgif, xu2017video, yang2021just}. 
The adoption of this approach in reputable datasets attests to its effectiveness and acceptance. Therefore, while the questions are template-based, their relevance and significance in the context of sports are preserved, ensuring the robustness of the dataset for analysis and evaluation.





\noindent \textbf{Debiasing.} Unlike daily scenarios, players in sports are highly restricted by rules. For instance, after the left team executes a ``\textit{volleyball serve}", the right team must follow with a ``\textit{first-pass}". This results in highly correlated actions in the videos, leading to questions and answers that are also highly correlated in our generated pairs. However, such questions are often meaningless for VideoQA, as the answers can be easily inferred through rules or common sense and should be removed.
To address this, we first obtain meta-questions by removing team information and the order of actions from the original questions. For example, the meta-question for ``\textit{What does the left team do after the other team does the second spike?}" becomes ``\textit{What does the team do after the other team does spike?}" We then examine the correlations between the meta-questions and their answers. If there is only one answer to the meta-question, we remove the corresponding QA pairs. If an answer to a meta-question occurs more frequently than other answers (frequency larger than 0.5), we randomly remove the corresponding QA pairs to balance the frequencies, ensuring they are all lower than 0.5.

\noindent \textbf{Problem Setting.} Following the approach in \citep{jang2017tgif, xu2017video}, we define an open-ended task based on the generated QA pairs. It is important to note that the open-ended setting, along with the multiple-choice setting, is widely used in the field of VideoQA. We have chosen to commit to the open-ended task instead of the multiple-choice one for two reasons: 1) The open-ended task is more challenging as it requires models to choose from a large answer set rather than selecting from several given options. 2) Limiting the answer choices to only several options would significantly decrease the diversity of the dataset. Specifically, all answers in the QA pairs form an answer pool, which is treated as 191 classes (after discarding classes with fewer than 30 samples). 

\noindent \textbf{Why not Fine-Grained Action Recognition?} It is essential to underscore that merely achieving fine-grained action recognition is insufficient for Sports-QA. The nature of the questions in this context demands not only precise detection of actions but also involves intricate temporal and causal modeling. To illustrate, questions that inquire about the effects of certain actions necessitate the accurate identification of temporally adjacent actions and a nuanced understanding of their causality. In summary, Sports-QA requires a more advanced level of comprehension and temporal reasoning beyond basic action recognition.

\subsection{Dataset Statistics}


Table \ref{n_qa} presents the numbers of QA pairs for various question types and different sports in Sport-QA. Our dataset comprises approximately 94K QA pairs. It's worth noting that a substantial portion of our descriptive questions involves complex action temporal localization and counting, presenting challenges similar to other question types. Additionally, we have balanced the number of questions across different sports.

\begin{table}[tbp]
\caption{The numbers of QA pairs for different types and different sports.}
\label{n_qa}

\begin{tabular}{@{}lrrrrr@{}}
\toprule
 Sports       & Descriptive & Temporal & Causal & Counterfactual & Total  \\ \midrule
Basketball & 5,629 & 22    & 785    & 278      & 6,714  \\
Football   & 6,659 & 1,355 & 1,949  & 523      & 10,486 \\
Volleyball & 6,120 & 360   & 1,942  & 685      & 9,107  \\
Gym     & 6,382 & 1,997 & 0      & 0        & 8,379  \\
Floor Exercise      &  6,046      &  11,012     & 0      & 0        &   19,418     \\
Balance Beam       &   7,477     &   12,773    & 0      & 0        &   20,250     \\
Uneven Bars      &   7,294     &   12,124    & 0      & 0        &    17,058    \\
Vault      &   2,661     &   0    & 0      & 0       &   2,661     \\ \midrule
Total   &   48,268     &   39,643    & 4,676  & 1,486    &    94,073    \\ \botrule
\end{tabular}
\end{table}

\begin{figure*}[tbp]
\includegraphics[width=\textwidth]{pics/ans_dist_type.pdf}
\caption{The distributions of answer classes broken down by question types.}
\label{ans_dist_type}
\end{figure*}



The total number of videos in our dataset is 5,967, distributed across different sports as follows: basketball (879), football (1,030), volleyball (586), gym (505), vault (501), uneven bars (834), floor exercise (770), and balance beam (862). Sports-QA is divided into training, validation, and testing sets (60\%/20\%/20\%). Specifically, videos of the same sport are randomly assigned to each set to ensure similar distributions of sport types across subsets. Additionally, we have ensured that the distributions of answer classes are similar in different subsets.

Fig. \ref{ans_dist_type} illustrates the distribution of answer classes broken down by question types in Sports-QA. Note that we only showcase the first 80 classes with more examples. As depicted in the figure, the distribution of answers is long-tailed, and the unbalanced nature adds to the challenge of our dataset. Meanwhile, classes of the same type, such as ``yes/no", are balanced. This further increases the difficulty of our dataset, as the models are required to examine the video for answering the question instead of merely guessing the answer based on the question type. 



\section{Auto-Focus Transformer for Sports Video Question Answering}

A key characteristic of the sports VideoQA task is the need for reasoning over temporal dependencies of various scales for multiple objects. For instance, a question asking about the number of actions requires the model to capture global dependencies in the video, whereas a question querying the effect of an action relies on short-term temporal information. The scale of temporal dependency required varies based on the question. However, current Transformer-based or GNN-based VideoQA methods tend to focus on global dependencies, irrespective of the required scale of temporal information. To address this limitation, we propose the Auto-Focus Transformer (AFT), a new Transformer encoder featuring a novel multi-head Auto-Focus Attention (AFA) mechanism, designed specifically for sports VideoQA.


\begin{figure}[tbp]
\centering
\includegraphics[width=\columnwidth]{pics/aft.pdf}
\caption{The structure of the sports  VideoQA model based on the proposed Auto-Focus Transformer.}
\label{aft}
\end{figure}

Specifically, we start by extracting the appearance feature and motion feature of the frame using pretrained ResNet \citep{he2016deep} and I3D \citep{carreira2017quo} (refer to Section \ref{feaext} for more details). Subsequently, we concatenate these features and input them into a linear layer for dimension reduction and feature fusion. Consequently, for each video, we obtain a sequence of frame representations ($\bm{R}$). Regarding the question, we utilize word embedding and a bi-directional RNN to obtain the text representations, encompassing both the global one ($\bm{w}$) and the local ones ($\bm{W}$).

We then  capture the dependencies among frames using AFT where the AFA mechanism
is computed as follows (the multi-head AFA is defined likewise): given the key set $\left\{\bm{k}_i\right\}$, the value set $\left\{\bm{v}_i\right\}$, and a query $\bm{q}_j$ ($\bm{q}_j,\bm{k}_i,\bm{v}_i \in \mathbb{R}^d$ are mapped from $\bm{R}$ in AFA), 
\begin{equation}
{\rm AFA}(\bm{q}_j)=\sum_{f\in \mathcal{F}}\alpha_f\sum_{i\in \mathcal{D}_j^f}{\rm softmax}_i\left(\frac{\bm{q}_j^\mathrm{T}\bm{k}_i}{\sqrt{d}}\right)\bm{v}_i,
\end{equation}
where $\mathcal{F}$ is a set of pre-defined focal lengths. $\mathcal{D}_j^f=\{i||i-j|\leq f\}$ is the index set of the neighbours of $\bm{q}_j$ within focal length $f$. $\alpha_f\in(0,1)$ is the dynamic weight of focus $f$, satisfying $\sum_f\alpha_f=1$ and depended on the question. As shown in Fig. \ref{aft},   $\bm{\alpha}=\{\alpha_f\}_{f=1}^\mathcal{|F|}$ are obtained by applying linear projection and softmax to the global representation of question.  
Compared to traditional attention that models global dependencies in any case, our AFA captures only the temporal dependencies of the scale required by the question. This design allows the model to focus solely on crucial information and discard irrelevant parts for better prediction. 
We denote the output of AFT as $\bm{M}$.

For the video-text integration, we draw inspiration from \citep{li2022invariant} and \citep{jiang2020reasoning}, employing a graph convolutional network (GCN) and block fusion \citep{ben2019block}. Refer to \citep{li2022invariant} for detailed information. Ultimately, we obtain a fused feature $\bm{s}$ and predict the answer using a linear projection and softmax.




\section{Experiments}
 


 
\subsection{Experimental Settings}
\label{feaext}

We start by explaining the experimental settings, covering data pre-processing and feature extraction, implementation details, and the evaluation metrics.

\noindent\textbf{Pre-processing and Representation.} The videos in our dataset are down-sampled to 5 FPS to reduce temporal redundancy. We extract two types of visual features for experiments: appearance feature and motion feature. For each feature type, we obtain both global and local features.
Specifically, we detect 10 objects in every two frames of each video using Faster-RCNN \citep{ren2015faster} (with ResNet-101 \citep{he2016deep} as the backbone and a score threshold set to 0.6), pretrained on COCO \citep{lin2014microsoft}. These detected objects serve as local appearance features in our work. Additionally, we use the global average pooling of Res4 activation in Faster-RCNN as the global appearance feature.
To extract motion features, we employ I3D \citep{carreira2017quo} pretrained on Kinetics \citep{carreira2017quo}. Specifically, for each frame, we combine it with its adjacent 7 frames, creating a clip that we then feed to I3D to extract a 3D feature map. The global motion feature of a frame is obtained by applying global average pooling to the 3D feature. The local motion feature of an object in a frame is obtained by applying RoIAlign \citep{he2017mask} (output size $1\times 1$) to the 3D feature with its corresponding bounding box. Note that our method uses only the global feature.
For language, we explore two types of word embeddings: GloVe \citep{pennington2014glove} and pre-trained BERT \citep{devlin2018bert}.
 
\noindent\textbf{Implementation Details.} We set the attention focus to $\mathcal{F}=\{3,9,80\}$ to cover short-term, mid-term, and long-term dependencies, respectively. The hidden dimension $d$ is set to 512. The loss function is cross-entropy. The models are trained for 50 epochs with a learning rate of $1\times 10^{-4}$ and a batch size of 16, using the Adam optimizer \citep{kingma2014adam}.

\noindent\textbf{Evaluation Metrics.} For evaluation metrics, we employ accuracy to showcase the models' performance on our dataset. Additionally, recognizing the long-tail distribution of our dataset, we evaluate methods using F1-score. F1-score is particularly important for sports VideoQA as it explicitly reflects performance averaged over imbalanced classes.



\begin{table}[tbp]
\caption{Baseline results on Sports-QA, including random choosing, semantic-aware (S-A) random choosing, LSTM-based BlindQA, and Transformer-based BlindQA. The best and the second-best results (\%) are \textbf{bold} and \underline{underlined}.} 
\label{bl}
\begin{tabular}{@{}ccccccc@{}}
\toprule
\multirow{2}{*}{Metric} & \multicolumn{2}{c}{Baseline} & \multicolumn{2}{c}{GloVe}            & \multicolumn{2}{c}{BERT}                    \\\cmidrule(l){2-3} \cmidrule(l){4-5} \cmidrule(l){6-7} 
                        & Random   & S-A Random   & LSTM & Transformer                   & LSTM                          & Transformer \\ \midrule
Accuracy                & 0.5      & 24.6         & 44   & 43.7                          & \underline{43.9} & 43.9        \\
F1-score                & 0.3      & 5.4          & 13.5 & \underline{14.9} & 13.8                          & 15.8        \\ \bottomrule
\end{tabular}
\end{table}





\begin{table*}[tbp]
\caption{The results of different methods on Sports-QA. The sport-wise accuracy, question-type-wise accuracy, overall accuracy and F1-score are presented (\%).
The best and second best results are \textbf{bold} and \underline{underlined}.} 
\label{res1}

\resizebox{0.95\columnwidth}{!}{\begin{tabular}{clcccccccccll}
\toprule
 \multirow{2}{*}{Word Emb.}& \multirow{2}{*}{Model} & \multicolumn{5}{c}{Sports} & \multicolumn{4}{c}{Question Type} & \multirow{2}{*}{Acc.} & \multirow{2}{*}{F1-score} \\ \cmidrule(lr){3-7}\cmidrule(lr){8-11}
 &  & Basket. & Foot. & Volley. & Gym  &FineG.& Desc. & Temp. & Causal & Counter. &  &  \\ \hline\hline
 & \cellcolor[HTML]{C0C0C0}BlindQA & \cellcolor[HTML]{C0C0C0}33.4& \cellcolor[HTML]{C0C0C0}62.9& \cellcolor[HTML]{C0C0C0}44.9& \cellcolor[HTML]{C0C0C0}36.5&\cellcolor[HTML]{C0C0C0}42.0& \cellcolor[HTML]{C0C0C0}51.7& \cellcolor[HTML]{C0C0C0}32.7& \cellcolor[HTML]{C0C0C0}48.7& \cellcolor[HTML]{C0C0C0}51.3& \cellcolor[HTML]{C0C0C0}43.7& \cellcolor[HTML]{C0C0C0}14.9\\
 & CoMem \citep{gao2018motion} & 72.8& 63.1& 69.8& 70.0&50.3& 77.0& 32.9& 50.0& 60.9& 57.1& 23.1\\
 & HME \citep{fan2019heterogeneous} & 64.8& 63.3& 68.5& 71.6&49.3& 76.5& 31.2& 50.4& 54.3& 56.0& 23.6\\
 & HGA \citep{jiang2020reasoning} & 71.3& 65.4& 72.2& 72.2&51.3& 78.1& 34.4& 52.0& 59.2& 58.3& 24.5\\
 & HQGA \citep{xiao2022video} & 72.8& 64.6& 70.5& 71.6&51.0& 77.6& 33.8& 54.0& 58.2& 57.9& \underline{25.0}\\
 & IGV \citep{li2022invariant} & 68.8& 66.0& 70.6& 72.4&52.1& 78.2& 34.7& 52.1& 59.2& \underline{58.5}& 24.8\\
 & MASN \citep{seo2021attend} & 71.2& 64.2& 70.1& 66.9&50.5& 76.4& 33.6& 50.2& 58.9& 57.0& 23.4\\ \cmidrule(l){2-13} 
 & Baseline (Ours) & 69.5& 65.4& 68.9& 71.8&52.3& 77.8& 35.0& 52.4& 59.9& 58.0& 23.7\\
\multirow{-9}{*}{GloVe} & AFT (Ours) & 73.9& 67.8& 69.5& 71.8&52.5& 78.9& 35.3& 55.1& 56.3& \textbf{59.2} {\color[HTML]{036400}(+1.2)}& \textbf{25.6} {\color[HTML]{036400}(+1.9)}\\ \hline\hline
 & \cellcolor[HTML]{C0C0C0}BlindQA & \cellcolor[HTML]{C0C0C0}34.7& \cellcolor[HTML]{C0C0C0}52.1& \cellcolor[HTML]{C0C0C0}45.9& \cellcolor[HTML]{C0C0C0}38.3&\cellcolor[HTML]{C0C0C0}43.9& \cellcolor[HTML]{C0C0C0}51.8& \cellcolor[HTML]{C0C0C0}33.3& \cellcolor[HTML]{C0C0C0}48.0& \cellcolor[HTML]{C0C0C0}53.9& \cellcolor[HTML]{C0C0C0}43.9& \cellcolor[HTML]{C0C0C0}15.8\\
 & CoMem \citep{gao2018motion} & 74.1& 64.0& 68.7& 70.7&49.3& 77.2& 32.1& 48.4& 55.6& 56.6& 26.4\\
 & HME \citep{fan2019heterogeneous} & 64.7& 63.4& 66.2& 70.6&49.3& 76.6& 30.7& 46.6& 53.6& 55.6& 22.9\\
 & HGA \citep{jiang2020reasoning} & 71.6& 66.5& 69.6& 70.7&51.3& 78.0& 33.8& 54.3& 56.6& 58.1& \underline{25.1}\\
 & HQGA \citep{xiao2022video} & 74.1& 63.5& 67.2& 70.8&50.3& 76.6& 33.6& 49.1& 57.9& 57.0& 25.0\\
 & IGV \citep{li2022invariant} & 72.0& 63.9& 70.4&72.4 &51.7& 78.0&34.2 & 51.9& 63.2& \underline{58.2}&23.8 \\
 & MASN \citep{seo2021attend} & 73.0& 63.0& 69.7& 66.6&50.1& 76.4& 32.5& 50.6& 61.2& 56.6& 24.1\\ \cmidrule(l){2-13} 
 & Baseline (Ours) & 70.7& 66.6& 69.2& 71.5&51.8& 78.0& 34.8& 50.5& 60.2& 57.9& 23.9\\
\multirow{-9}{*}{BERT} & AFT (Ours) & 72.3& 67.9& 70.4& 71.6&52.4& 78.3& 35.5& 56.8& 58.2& \textbf{59.1} {\color[HTML]{036400}(+1.2)}& \textbf{25.4} {\color[HTML]{036400}(+1.5)}\\ \botrule
\end{tabular}}
\end{table*}


\begin{table}[tbp]
\caption{The visual features used by different models. A./M./G./L. represents Appearance/Motion/Global/Local. Mem, IG, HL, and GNN stand for Memory, Invariant Grounding, Hierarchical Learning, and Graph Neural Network, respectively.} 
\label{fea}
\begin{tabular}{@{}lcccccc@{}}
\toprule
Model                           & Venue   & Insight            & G.A        & G.M.       & L.A.       & L.M.       \\ \midrule
CoMem \citep{gao2018motion}      & CVPR'18 & Mem             & \checkmark & \checkmark &            &            \\
HME \citep{fan2019heterogeneous} & CVPR'19 & Mem             & \checkmark & \checkmark &            &            \\
HGA \citep{jiang2020reasoning}   & AAAI'20 & GNN                & \checkmark & \checkmark &            &            \\
IGV \citep{li2022invariant}      & CVPR'22 & IG & \checkmark & \checkmark &            &            \\
HQGA \citep{xiao2022video}       & AAAI'22 & HL            & \checkmark & \checkmark & \checkmark &            \\
MASN \citep{seo2021attend}       & ACL'21  & GNN                & \checkmark & \checkmark & \checkmark & \checkmark \\\midrule
AFT (Ours)      &   &   AFA              & \checkmark  & \checkmark  &  & \\\botrule
\end{tabular}
\end{table}



\subsection{Baseline Study}



Several baselines are constructed to evaluate their performance on our Sports-QA dataset as follows.

\noindent\textbf{Random Choosing:} Answers are randomly selected from the 191 answer classes, resulting in an accuracy of approximately $\frac{1}{191} \approx 0.5\%$. The F1-score is obtained by running the random test for 200 times and taking the average.

\noindent\textbf{Semantic-Aware Random Choosing:} The randomly selected answer is constrained by the question type. For example, binary questions (beginning with ``\textit{Do}" or ``\textit{Is}") can only choose from ``\textit{yes}" or ``\textit{no}"; questions asking about the number of something (beginning with ``\textit{How many}") can only choose a number.

\noindent\textbf{BlindQA:} Models are constructed with a question encoder and an MLP-based answer decoder, without exploiting visual information. Two types of word embeddings (Glove and pre-trained BERT) and two question encoders (LSTM and Transformer \citep{vaswani2017attention}) are studied.

The results of the baselines are shown in Table \ref{bl}. Random choosing exhibits poor accuracy and F1-score, but considering constraints from the questions significantly improves performance. BlindQA achieves considerable results in both accuracy and F1-score, indicating noticeable semantic correlations between questions and answers. By maximizing the likelihood of the answer conditioned on a specific question type, the model fits the answer distribution given different questions in the training set.

Regarding word embedding, pre-trained BERT achieves slightly better accuracy than GloVe, with noticeable improvements in F1-score. Concerning different question encoders, Transformer and LSTM achieve similar accuracy, but Transformer outperforms LSTM in F1-score. In summary, word embedding has a greater impact on performance than the question encoder.

\subsection{Benchmark on Sports-QA}

We benchmark several VideoQA methods on our Sports-QA dataset, comparing them to CoMem \citep{gao2018motion}, HME \citep{fan2019heterogeneous}, HGA \citep{jiang2020reasoning}, MASN \citep{seo2021attend}, HQGA \citep{xiao2022video}, and IGV \citep{li2022invariant}, with HQGA \citep{xiao2022video} and IGV \citep{li2022invariant} representing the state of the art. It's worth noting that we only evaluate methods without large-scale video-text pretraining, excluding pretrained models like MERLOT \citep{zellers2021merlot}, VIOLET \citep{fu2021violet}, and All-in-one \citep{wang2022all}. The exclusion is based on the following reasons: 1) pretrained models require large-scale additional data, making comparisons unfair for methods without pretraining; 2) we aim to highlight the characteristics of our dataset, and the universal knowledge gained by pretrained models could introduce bias in understanding sports scenarios. Table \ref{fea} shows the types of visual features used by each model, along with a summary of each method's insights. For CoMem, HME, and HGA, we use re-implementations provided by \citep{xiao2021next}, and for MASN, HQGA, and IGV, we utilize the official implementations by their respective authors. \textbf{The baseline model has the same architecture as shown in Fig. \ref{aft}, with AFT replaced with a standard Transformer Encoder.}




In Table \ref{res1}, the VideoQA methods demonstrate substantial improvements compared to BlindQA. Notably, the most significant improvements are observed for basketball (approximately 40\% increase in accuracy), indicating the critical role of visual information in basketball for question answering. Conversely, the improvements for football are less pronounced (about 4\%), suggesting that BlindQA already achieves considerable accuracy in this sport. The results suggest high correlations between questions and answers in football, posing challenges for VideoQA models to fully exploit visual information, potentially due to the complexity introduced by a larger number of players.

Examining different question types, the greatest improvements are observed for descriptive questions (around 27\% increase in accuracy). Additionally, VideoQA performance on causality and counterfactual questions shows significant improvements compared to BlindQA. However, results for temporal questions are similar between VideoQA methods and BlindQA (except for our AFT), suggesting that VideoQA models may struggle with modeling fine-grained temporal dependencies among actions in sports videos.

Among the different VideoQA models, our AFT achieves the best performance in both accuracy and F1-score. The significant improvements compared to the baseline indicate the effectiveness of the proposed attention mechanism in distinguishing semantically similar classes.








\subsection{Further Analysis}

\noindent\textbf{Generalization Ability across Sports}. In this study, our primary objective is to assess the generalization capabilities across different sports domains. Our approach involves an initial pretraining phase on a set of four sports, followed by fine-tuning on a fifth, distinct sport. We meticulously evaluate the performance of this pretraining-fine-tuning methodology in comparison to training a model entirely from scratch.
The outcomes, detailed in Table \ref{resf}, showcase a notable enhancement in accuracy when employing the pretraining strategy on diverse sports. These findings suggest that the model acquires a nuanced understanding of common semantic features during the initial pretraining. Moreover, the model demonstrates its ability to effectively generalize this acquired knowledge to previously unseen sports during the subsequent fine-tuning phase.

\begin{table}[tbp]
\caption{The table presents comparisons (accuracy \%) between models trained from scratch for individual sports and those fine-tuned using data from other sports, along with the corresponding improvements (Imp.).} 
\label{resf}

\begin{tabular}{cccccc}
\toprule
Pretraining& Basketball                       & Football                         & Volleyball                       & Gym                              & FineG. \\ \midrule
\xmark    & 57.1& 55.0& 56.4& 70.2&        48.5\\
\cmark    & 58.3& 56.5& 57.7& 71.7&        50.3\\ \midrule
 Imp.& {\color[HTML]{036400}+1.2}& {\color[HTML]{036400}+1.5}& {\color[HTML]{036400}+1.3}& {\color[HTML]{036400}+1.5}&{\color[HTML]{036400}+1.8}\\\botrule
\end{tabular}
\end{table}




\begin{table}[t]
\caption{The accuracy (\%) of various focus lengths and their combinations. Note that the combination of the same focal length implies the use of only one focus length.} 
\label{fs}

\begin{tabular}{c|ccc|ccc|c}
\toprule
   & 1 & 3 & 5 & 7 & 9 & 11 & 80   \\ \midrule
1  & \cellcolor[HTML]{C0C0C0}58.2& / & / & 58.9& 59.0& 58.7& 58.5\\
3  & / & \cellcolor[HTML]{C0C0C0}58.7& / & 59.0& \textbf{59.1}& 58.6& 58.9\\
5  & / & / & \cellcolor[HTML]{C0C0C0}58.6& 58.8& 58.9& 58.7& 58.8\\ \midrule
7  & / & / & / & \cellcolor[HTML]{C0C0C0}58.7& / & /  & 58.9\\
9  & / & / & / & / & \cellcolor[HTML]{C0C0C0}58.8& /  & 59.0\\
11 & / & / & / & / & / & \cellcolor[HTML]{C0C0C0}58.4& 58.6\\ \midrule
80 & / & / & / & / & / & /  & \cellcolor[HTML]{C0C0C0}58.0 \\ \botrule
\end{tabular}
\end{table}

\noindent\textbf{Impact of Focus Length.} In this experiment, we investigate the impact of different focus lengths and their combinations. For short-term dependency, we consider lengths of 1, 3, and 5. For the mid-term, we explore lengths of 7, 9, and 11. Regarding long-term (global) dependency, we set the focus length to the video length, which is 80 seconds. The results are presented in Table \ref{fs}.
From the results, it is evident that the model with global attention performs less effectively compared to those with local attention. However, upon applying our auto-focus attention mechanism, there is a significant improvement in performance. Notably, the combination of focus lengths 3 and 9 achieves the highest accuracy. In our final model, we incorporate the global focus, resulting in the best accuracy at 59.2\%.



\subsection{Visualization}










\noindent \textbf{Qualitative Results.} Fig. \ref{pred} illustrates predictions from various methods. In some instances, BlindQA successfully guesses the correct answer by leveraging the correlation between the question and the answer in the training set. However, this approach is not foolproof, as the blinded model consistently selects the most correlated answer to the question.
In comparison, the VideoQA models struggle to discern subtle differences between semantically or visually similar answer classes. For example, actions like ``\textit{long pass}" and ``\textit{cross}" both involve passing the ball to other players but differ in passing direction and distance, presenting challenges for accurate distinction. Additionally, the compared VideoQA models exhibit limitations in fine-grained motion analysis, such as action counting, as evident in the last example. With its ability to capture various types of temporal dependencies, our AFT outperforms the compared models on these challenging samples.


\begin{figure}[tbp]
\centering
\includegraphics[width=\columnwidth]{pics/vis2.pdf}
\caption{
Visualization of the predictions. The correct/wrong predictions are highlighted in \textcolor[RGB]{84,130,53}{green}/\textcolor[RGB]{192,0,0}{red}. The predicted probability of each answer is also reported. 
}
\label{pred}
\end{figure}


\begin{figure}[tbp]
\centering
\includegraphics[width=\columnwidth]}

\begin{document}

\title[Article Title]{Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex and Professional Sports}

\author[1]{\fnm{Haopeng} \sur{Li}}

\author[2]{\fnm{Andong} \sur{Deng}}

\author*[3]{\fnm{Jun} \sur{Liu}}

\author[4]{\fnm{Hossein} \sur{Rahmani}}

\author[5]{\fnm{Yulan} \sur{Guo}}

\author[6]{\fnm{Bernt} \sur{Schiele}}

\author[7]{\fnm{Mohammed} \sur{Bennamoun}}
 
\author*[1,8]{\fnm{Qiuhong} \sur{Ke}}\email{qiuhong.ke@monash.edu}

\affil[1]{\orgdiv{School of Computing and Information Systems}, \orgname{University of Melbourne}}

\affil[2]{\orgdiv{Center for Research in Computer Vision}, \orgname{University of Central Florida}}

\affil[3]{\orgdiv{Information Systems Technology and Design (ISTD) Pillar}, \orgname{Singapore University of Technology and Design}}

\affil[4]{\orgdiv{School of Computing and Communications}, \orgname{Lancaster University}}

\affil[5]{\orgdiv{School of Electronics and Communication Engineering}, \orgname{Sun Yat-sen University}}

\affil[6]{\orgdiv{Department of Computer Vision and Machine Learning}, \orgname{Max Planck Institute for Informatics, Saarland Informatics Campus}}

\affil[7]{\orgdiv{School of Physics, Maths and Computing}, \orgname{University of Western Australia}}

\affil[8]{\orgdiv{Department of Data Science \& AI}, \orgname{Monash University}}

\abstract{Reasoning over sports videos for question answering is an important task with numerous applications, such as player training and information retrieval. However, this task has not been explored due to the lack of relevant datasets and the challenging nature it presents. Most datasets for video question answering (VideoQA) focus mainly on general and coarse-grained understanding of daily-life videos, which is not applicable to sports scenarios requiring professional action understanding and fine-grained motion analysis. In this paper, we introduce the first dataset, named Sports-QA, specifically designed for the sports VideoQA task. The Sports-QA dataset includes various types of questions, such as descriptions, chronologies, causalities, and counterfactual conditions, covering multiple sports. Furthermore, to address the characteristics of the sports VideoQA task, we propose a new Auto-Focus Transformer (AFT) capable of automatically focusing on particular scales of temporal information for question answering. We conduct extensive experiments on Sports-QA, including baseline studies and the evaluation of different methods. The results demonstrate that our AFT achieves state-of-the-art performance.}

\keywords{Video Question Answering, Sports Video, Benchmark, Auto-Focus Transformer}

\maketitle

\section{Introduction}

Sports video analysis has been attracting increasing attention in recent years
\citep{yuan2021spatio, li2021groupformer, koshkina2021contrastive, zhu2022fencenet, martin2020fine, wang2022shuttlenet,6516867}.
While research progress has been made on tasks such as
sports action recognition \citep{li2021multisports, zhu2022fencenet, rasmussen2022compressing}, reasoning over sports videos for question answering
has not been explored. As humans, we can not only recognize the actions of the players in sports videos but also understand the effects of the players' actions, explain why a team loses the score, and imagine what would happen under counterfactual situations. Our impressive capabilities in reasoning allow us to answer complex questions related to sports videos, which is crucial in applications such as obtaining crucial statistics in matches for player/team performance qualification, analyzing players' actions and team strategies for performance improvement, and efficiently retrieving information for audiences and analysts.
While sports video reasoning is clearly important, it is under-explored due to challenges and the lack of datasets.

\begin{figure}[tbp]
\centering
\includegraphics[width=0.9\columnwidth]{pics/qadiff.pdf}
\caption{
Illustrations of general VideoQA, which focus on common basic understanding, and sports VideoQA, which requires professional action understanding and action relation reasoning\protect\footnotemark. 
}
\label{qadiff}
\end{figure}

\footnotetext[\thefootnote]{We treat the action ``\textit{block}" as a noun.}

In this paper, we tackle sports VideoQA, focusing on reasoning over sports videos for question answering. This is distinct from general VideoQA, which centers on general video understanding, mainly encompassing descriptive and temporal aspects \citep{jang2017tgif, xu2017video, xiao2021next, yu2019activitynet}, as depicted in Fig. \ref{qadiff}. % illustrates the difference between general VideoQA and sports VideoQA. %given a sports video, general VideoQA is mostly concerned with common %descriptive questions including  action recognition\ke{questions  such as general action recognition (i think general also has temporal or other types of questions)}, while 
Compared to general VideoQA, sports VideoQA is more challenging because the questions may involve particular teams and professional actions, and it requires a fine-grained understanding of actions and intra/inter-team relation modeling. %action understanding and intra/inter-team relations including  temporal and causal reasoning. 
As pointed out in \citep{shao2020finegym}: 1) coarse-grained global video understanding is insufficient for sports analysis where the actions are rapid and hard to differentiate; 2) the models pre-trained on coarse-grained datasets are not always beneficial to fine-grained tasks. 
In the meantime, although significant strides have been made in the fine-grained recognition of individual actions \citep{shao2020finegym,liu2022fineaction,sun2017taichi}, these datasets may fall short in effectively evaluating the ability for temporal and causal reasoning over actions performed by individuals or teams in sports scenarios.
A large-scale dataset containing professional sports scenarios with detailed annotations of sports-relevant questions is thus required for explicit and comprehensive reasoning for sports VideoQA.

To address this gap, we introduce a novel dataset called Sports-QA, the first video QA dataset specifically focusing on sports activities. Leveraging sports videos and professional action labels from MultiSports \citep{li2021multisports} and FineGym \citep{shao2020finegym}, we construct our dataset. Both sources provide high-quality sports videos with well-defined action labels, allowing us to annotate essential attributes for each crucial action instance. Using these annotations, we generate QA pairs based on pre-defined templates covering aspects such as description, chronology, causality, and counterfactual conditions. The Sports-QA dataset is the first of its kind, comprising approximately 94K QA pairs, encompassing multiple sports and various question types for sports VideoQA. Table \ref{set} provides a comparison between Sports-QA and several widely-used VideoQA datasets.

\begin{table}[tbp]
\caption{VideoQA datasets comparison. ``Auto/Man" represents ``automatic/manual". ``MC/OE" represents ``multi-choice/open-ended".}
\label{set}
\begin{tabular}{@{}lcrrcc@{}}
\toprule
Dataset                           & Topic   & \multicolumn{1}{c}{\#Video} & \multicolumn{1}{c}{\#QA} & Annotation  & QA Task  \\ \midrule
MSRVTT-QA \citep{xu2017video}      & \multirow{4}{*}{General} & 10K                         & 244K                     & Auto        & OE       \\
MSVD-QA \citep{xu2017video}        &  & 2K                          & 51K                      & Auto        & OE       \\
NExT-QA \citep{xiao2021next}       &  & 5.4K                        & 52K                      & Man         & OE\&MC \\
TGIF-QA \citep{jang2017tgif}       &  & 72K                         & 165K                     & Auto\&Man & OE\&MC \\ \midrule
MovieQA \citep{tapaswi2016movieqa} & Movie   & 408                         & 15K                      & Man         & MC       \\
Social-IQ \citep{zadeh2019social}  & Social  & 1K                          & 8K                       & Man         & MC       \\
DramaQA \citep{choi2021dramaqa}    & Drama   & 24K                         & 16K                      & Auto\&Man & MC       \\
\textbf{Sports-QA (Ours)}                  & Sports  & 6K                          & 94K                      & Auto\&Man & OE       \\ \botrule
\end{tabular}
\end{table}

Compared with existing datasets, our Sports-QA provides new insights for VideoQA: 1) It encompasses questions related to specific terms and actions in both group activities (e.g., basketball) and single-person sports (e.g., gym). This requires models to possess the capability to capture dynamic patterns and perform reasoning in scenarios with varying numbers of interactions for question answering. 2) To achieve a comprehensive understanding of complex sports videos, Sports-QA includes diverse questions that involve video information at various temporal granularities. This encompasses global long-term temporal dependencies and fine-grained short-term ones. For instance, a question asking about the number of actions requires the model to capture global dependencies, while a question about the effect of a specific action relies on short-term temporal information. Furthermore, Sports-QA has the potential to be leveraged for other tasks. The pre-defined attributes of actions can be treated as multiple labels, allowing the dataset to be used for multi-label classification tasks for comprehensive action understanding. Additionally, based on the annotated action attributes, we can modify the templates from question answering to declarative sentences, generating descriptive or explanatory narrations and enabling comprehensive sports video captioning.

In addition to the dataset, we present a novel method for sports VideoQA. Recognizing that sports VideoQA demands the model to capture information from multiple frames at various temporal granularities, we introduce the Auto-Focus Transformer (AFT), featuring an Auto-Focus Attention mechanism. Conceptually, the model operates akin to a temporal magnifying glass with a dynamic focal length, allowing it to inspect the video to answer questions with diverse temporal dependencies. More specifically, our designed Auto-Focus Attention dynamically selects attention focus based on the question. This mechanism empowers the model to handle questions involving video information across various time spans.

The contributions are summarized as follows:

\begin{itemize}

    \item We contribute a large-scale dataset, which consists of various types of questions and multiple sports for VideoQA.  %It provides  various question types and multiple sports. It also provides   
To our knowledge, it is the first dataset for complex reasoning over professional sports actions. 

    \item We propose a new Auto-Focus Transformer (AFT), where the attention focus is adaptive based on the question,  enabling the model to deal with questions requiring temporal information of various scales.

    \item We conduct extensive experiments on our dataset, including baseline study, evaluation of existing methods, and visualization of predictions. The results show the superiority of the proposed AFT for sports VideoQA. %limitations of existing methods and provide directions for future improvements.
\end{itemize}

\section{Related Work}

\noindent\textbf{VideoQA Datasets.}  The development of VideoQA has been greatly facilitated by the emergence of various datasets, such as TGIF-QA \citep{jang2017tgif}, MSVD-QA, and MSRVTT-QA \citep{xu2017video}, DramaQA \citep{choi2021dramaqa}, NExT-QA \citep{xiao2021next}, NExT-OOD \citep{10107423}. TGIF-QA, for instance, offers a comprehensive set of four sub-tasks designed for temporal reasoning in general videos and is widely recognized in the VideoQA community. MSVD-QA and MSRVTT-QA are open-ended datasets constructed from existing video captions, while DramaQA focuses on understanding drama stories with hierarchical QAs and character-centered video annotation. NExT-QA, introduced for describing and explaining temporal actions, provides both multi-choice and open-ended questions through manual annotation. However, existing datasets predominantly center around general video understanding in daily scenarios or involve coarse-grained action/event reasoning. \textbf{In contrast, our contribution lies in fine-grained and professional analysis within sports scenarios. We present Sports-QA, a dataset that addresses aspects of description, chronology, causality, and counterfactual conditions for multiple sports of diverse characteristics.}

\noindent\textbf{VideoQA Methods.} 
VideoQA poses a significant challenge as it necessitates models to grasp both spatial and temporal information from videos to answer questions. Various deep models have been developed, approaching this task from different perspectives \citep{fan2019heterogeneous,jiang2020reasoning,li2022invariant,antol2015vqa,gao2018motion,10172254,9770842,10214041,10146482}.
For instance, the deep heterogeneous graph alignment network by Jiang et al. \citep{jiang2020reasoning} addresses VideoQA by simultaneously aligning intra/inter-modality information. Another approach involves a multimodal attention model proposed by Fan et al. \citep{fan2019heterogeneous}, where heterogeneous memory learns global context from visual features, and question memory captures the complex semantics of questions.
IGV, introduced by Li et al. \citep{li2022invariant}, grounds question-critical scenes in videos by considering causal relations that remain invariant to complement contents.
However, a limitation in these approaches is the lack of consideration for the fact that different questions may require temporal dependencies of specific scales. \textbf{In response to this challenge, we propose the Auto-Focus Transformer, designed to automatically focus on a specific temporal scale based on the question for VideoQA.}

\noindent\textbf{Sports Video Understanding.} Sports video understanding has drawn increasing attention in recent years \citep{yuan2021spatio,li2021groupformer,koshkina2021contrastive,zhu2022fencenet,martin2020fine,wang2022shuttlenet}. Researchers have made great efforts in various tasks such as sports action recognition \citep{shao2020finegym}, multi-person action detection \citep{li2021multisports}, and action quality assessment \citep{tang2020uncertainty}. Meanwhile, numerous sports datasets are constructed \citep{parmar2019action,li2021multisports,giancola2018soccernet,deliege2021soccernet}.  
For example, AQA-7 \citep{parmar2019action} is constructed for professional action quality assessment. Seven types of actions are included in this dataset and all the action instances are associated with quality scores. 
MultiSports \citep{li2021multisports} is proposed for the spatial-temporal detection of professional sports actions. FineGym~\citep{shao2020finegym} focuses on fine-grained activity localization that requires an accurate understanding of the atomic level of a gymnastic action.
\textbf{In this work, we construct Sports-QA, the first dataset for sports VideoQA, bridging the gap between the visual modality and the textual modality in sports analysis.}

\section{Sports-QA Dataset}

\begin{table}[tbp]
\caption{The statistics for the MultiSports dataset are presented.} 
\label{mss}
\begin{tabular}{@{}lcrcc@{}}
\toprule
 Sports          & \# Action & \# Instance & Avg. Action/Video Duration & \# Bounding box \\ \midrule
Gym        & 21      & 8,703     & 1.5s / 30.7s        & 325K    \\
Volleyball & 12      & 7,645     & 0.7s / 10.5s        & 139K   \\
Football   & 15      & 12,254    & 0.7s / 22.6s        & 225K    \\
Basketball & 18      & 9,099     & 0.9s / 19.7s        & 213K   \\\midrule
Total      & 66      & 37,701    & 1.0s / 20.9s        & 902K   \\ \botrule
\end{tabular}
\end{table}

\subsection{Data Source}

Regarding the collection of sports videos, we consider the following aspects: 1) The visual quality of the video data should be high enough to conduct fine-grained video understanding, such as video resolution and frame rate. 2) Instead of applying to a single type of sports, we expect a VideoQA dataset involving multiple sports.
After a deep survey of the works on sports video understanding, we find that the MultiSports \citep{li2021multisports} and FineGym \citep{shao2020finegym} datasets are highly suitable for our purpose. The details of MultiSports and FineGym are as follows.

\noindent\textbf{MultiSports} \citep{li2021multisports} is a dataset for the temporal localization of sports actions, encompassing four sports (i.e., basketball, football, volleyball, and aerobic gymnastics) and 66 fine-grained action categories\footnote{Project homepage: \url{https://deeperaction.github.io/datasets/multisports.html}}. The action categories in MultiSports are professional terms (such as ``\textit{volleyball spike}", ``\textit{football tackle}", and ``\textit{basketball defensive rebound}") instead of common and atomic actions like ``\textit{run}" and ``\textit{stand}". For each action instance, the dataset provides the bounding boxes of a player from the starting frame of an action to the ending frame, forming the action spatial-temporal tube. The statistics of MultiSports are shown in Table \ref{mss}.

\noindent\textbf{FineGym} \citep{shao2020finegym} is a dataset designed to elevate the field of action recognition by addressing the limitations observed in existing techniques\footnote{Project homepage: \url{https://sdolivia.github.io/FineGym}}. Developed to surpass current benchmarks, it offers a unique combination of richness, quality, and diversity in its content.
This dataset is constructed on gymnasium videos, providing a realistic and varied environment for action recognition studies. What sets FineGym apart from other datasets is its meticulous temporal annotation at both action and sub-action levels, featuring a three-level semantic hierarchy. This hierarchical structure allows for a more nuanced understanding of activities, enabling researchers to explore and analyze actions in finer detail. As an illustrative example, an event is annotated as a sequence of elementary sub-actions Importantly, each sub-action within these sets is further annotated with finely defined class labels, contributing to a higher level of granularity in action recognition. Fig. \ref{ms} shows the action hierarchy of the MultiSports and FineGym datasets.

\begin{figure}[tbp]
\centering
\includegraphics[width=\columnwidth]{pics/MultiSports1.pdf}
\caption{The action hierarchy of the MultiSports dataset is depicted at the top, while the FineGym dataset's hierarchy is shown at the bottom. It's important to note that the figure includes only four example actions for each sport.}
\label{ms}
\end{figure}

The action categories in MultiSports and FineGym are professional terms (such as ``\textit{volleyball spike}", ``\textit{football tackle}", ``\textit{basketball defensive rebound}", and ``\textit{vault Salto backward tucked}") instead of common and atomic actions (such as ``\textit{run}" and ``\textit{stand}"). Such professional labels are of great value to our dataset.
Since the length of videos varies greatly (from a few seconds to a few minutes), we segment the videos into clips and generate QA pairs based on these clips.

\subsection{Question-Answer Pair Generation}

In most existing works, QA pairs are generated using two approaches:
1) Automatically generating questions and answers based on video captions using a set of predefined question templates \citep{jang2017tgif, xu2017video, yang2021just}. This method is efficient, but it may introduce obvious grammatical errors or lose crucial information during the conversion of captions into questions and answers.
2) Manually annotating questions and answers through crowdsourcing \citep{xiao2021next, yu2019activitynet, garcia2020knowit}. Although this approach produces QA pairs of high quality in terms of accuracy and expression, manual annotation is time-consuming and expensive.
In this work, we aim to ensure the quality of the textual data while considering annotation costs. We achieve this by generating QA pairs using pre-defined templates based on existing labels and newly-labeled attributes of actions. Our approach guarantees grammatically error-free QA pairs that capture crucial information in videos, thanks to the careful design of sophisticated templates. Furthermore, as ball games and gymnastics have distinct characteristics, we generate the QA pairs in different ways as described as follows.

\subsubsection{QA Pair Generation for Ball Games}

Specifically, for ball games, including basketball, football, and volleyball, we define five attributes (Team, Outcome, Cause of Outcome, Cause of Action, and Effect of Action) for actions. We then manually annotate the attributes of each action. The definitions and annotation process of these attributes are elaborated as follows.

\noindent\textbf{Attribute Definition.} MultiSports consists of 45 action categories from ball games, with some being crucial for professional sports statistics or quantifying the performance of players/teams. For instance, the ``\textit{2-Point Field Goal Percentage}" in a basketball match necessitates a fine-grained understanding of all ``\textit{2-point shot}" actions, while the ``\textit{saving}" actions in football directly reflect the performance of goalkeepers. In this work, we focus on 28 crucial actions, listed in Table \ref{acts}. Considering both practical applications and research purposes, we define five attributes for each of the crucial action instances as follows.

\noindent 1. \texttt{Team}: The team that the player of the action belongs to is denoted by two options: \texttt{left team} or \texttt{right team}. Specifically, in basketball and football, the determination of the left or right team depends on the team attacking towards the right or left. In volleyball, the left or right team is designated based on their position relative to the left or right side of the net.

\noindent 2. \texttt{Outcome}: This attribute assesses whether the result of the action meets the expectation and is defined as a binary label: \texttt{successful} or \texttt{failed}. For example, if a ``\textit{2-point shot}" in basketball scores a goal, it is annotated as successful; otherwise, it is labeled as failed.

\noindent 3. \texttt{Cause of Outcome}: 
This attribute indicates the cause of failure for offensive actions, such as a ``\textit{2-point shot}" in basketball, ``\textit{shoot}" in football, and ``\textit{spike}" in volleyball, or the cause of success for defensive actions, like ``\textit{block}" in football and ``\textit{save}" in volleyball. Specifically, the cause of failure for offensive actions reflects player shortcomings. For example, the failure of an offensive ``\textit{long pass}" in football could be attributed to \texttt{defensive interception}, \texttt{bad pass}, or \texttt{bad catch}. Identifying the precise cause helps teams identify their weaknesses and improve performance. Similarly, the cause of success for defensive actions highlights player strengths. For instance, the success of a defensive ``\textit{save}" in volleyball might be due to \texttt{offensive out of bounds}, \texttt{offensive blocked by net}, or \texttt{actually catch the ball}, with the last case directly reflecting defensive performance. We do not consider the cause of success for offensive actions or the cause of failure for defensive actions, as they cannot be attributed to different cases. For example, the failure of ``\textit{football saving}" can only be explained by the goalkeeper missing the ball. Therefore, these causes are not worth discussing. We define various cause options for applicable action categories and ask annotators to choose from them. Note that this attribute is applicable to certain action categories, and the causes vary for different action categories.

\noindent 4. \texttt{Cause of Action}: This attribute indicates the actions that cause the current action. Each action instance in a video is initially labeled with its time order, based on the start time, which serves as the unique ID of the action in the video. For each crucial action, annotators are asked to provide the IDs of the actions that cause the current action; these causes are not necessarily crucial actions. Consequently, the \texttt{Cause of Action} is a list consisting of the IDs of actions that are mostly temporally adjacent to the current action. Not all crucial action categories are required to provide this attribute because the causes of some actions are unique. For instance, the only cause of ``\textit{volleyball first pass}" is ``\textit{volleyball serve}".

\noindent 5. \texttt{Effect of Action}: This attribute aims to identify the actions caused by the current action, analogous to \texttt{Cause of Action}. Although the cause and effect are conjunctive, we may consider only one of them, as some actions are not crucial. For instance, if ACTION M (crucial action) causes ACTION N (non-crucial action), we only label ACTION N as an effect of ACTION M.

\noindent\textbf{Attribute Annotation.}
After defining the attributes mentioned above, we proceed to annotate them for each crucial action instance in MultiSports. The annotation process is divided into three stages: pre-annotation, formal annotation, and quality check.
In the pre-annotation stage, annotators are grouped into three categories, with each group assigned responsibility for a specific type of ball game. Each annotator within a group is then assigned several videos corresponding to their designated sport. During this stage, annotators are tasked with labeling the attributes of the first 50 action instances. Following this, an intra-group check is conducted to address any issues related to understanding bias or potential mistakes, as well as to handle unexpected or rare situations. The annotation can only progress to the next stage when annotators achieve a consistent understanding of each attribute, as outlined in the protocols.
Moving to the formal annotation stage, annotators use shared protocols to amend their previous annotations and label the remaining action instances. Once all actions are labeled, we perform an inter-group quality check on all annotations. The purpose of this inter-group check is to ensure that the annotations align with the common understanding of average individuals.
The annotation process is completed within one month and involves the collaboration of 15 graduate students.

Based on the attributes, we generate questions about the videos by designing various templates (listed in the end of the paper) to inquire about these attributes. For instance, given a video shown in Fig. \ref{egdata}, after annotating the attributes for the action ``\textit{spike}", a question querying a particular attribute (e.g., Outcome) of the action can be generated, such as ``\textit{Is the `spike' of the right team successful?}".
Specifically, our dataset involves four types of questions: descriptive, temporal, causal, and counterfactual. The details of each type are described as follows.

\begin{figure*}[tbp]
\centering
\includegraphics[width=0.93\textwidth]{pics/egs.pdf}
\caption{Example of Sports-QA: The actions in the green boxes (such as ``\textit{2-point shot}") are the query actions, while the actions in the yellow boxes (such as ``\textit{block}") represent the effects. For ball games, annotators provide attribute labels, and we generate QA pairs based on these attributes. In gymnastics, we generate QA pairs using annotations from MultiSports/FineGym.
}
\label{egdata}
\end{figure*}

\noindent 1. \textbf{Descriptive:} Descriptive questions require holistic comprehension and involve various aspects of information from the videos. These questions include simple queries such as ``\textit{What is the video about?}" and ``\textit{Does SOME-TEAM perform SOME-ACTION?}", as well as complex ones involving counting and the outcome of actions, such as ``\textit{How many times does SOME-TEAM perform SOME-ACTION?}" and ``\textit{Does SOME-TEAM successfully do their i-th SOME-ACTION?}"

\noindent 2. \textbf{Temporal:} Temporal questions focus on the temporal relations among the actions of the same or different teams. Specifically, these questions require an understanding of intra-group temporal relations and inter-group ones. Typical templates for this type of question are ``\textit{What does SOME-TEAM do before/after their i-th SOME-ACTION?}" and ``\textit{What does the left/right team do before/after the other team performs their i-th SOME-ACTION?}"

\noindent 3. \textbf{Causal:} Causal questions aim to uncover the causal relations among the action instances and to explain the reasons or the specific process of the actions. Different from temporal questions, these questions require causal reasoning based on visual cues. The templates of these questions include causal queries such as ``\textit{Why does SOME-TEAM do the i-th SOME-ACTION?}" and ``\textit{What is the effect of the i-th SOME-ACTION of SOME-TEAM?}", as well as explanation queries such as ``\textit{How does SOME-TEAM succeed in doing/fail to do the i-th SOME-ACTION?}"

\noindent 4. \textbf{Counterfactual:} Different from the above three types of questions querying the details that actually happen, counterfactual questions set hypothetical conditions that do not occur in the video and query about the expected outcomes based on the conditions. This type of question requires reasoning about various imagined situations and expects the outcomes according to the causal relations among actions. The template for this type of question is ``\textit{Would the left/right team succeed in do the i-th SOME-ACTION if the other team did not do SOME-ACTION?}"

\noindent\textbf{From Attributes to Question-Answer Pairs.} In particular, the questions are queries about the attributes, and their answers can be obtained directly from our annotations or through logical reasoning and statistical analysis over the annotations. For instance, for a question asking about the cause of a specific action, the answer can be easily retrieved from its attribute of cause; For a question asking whether a specific action is successful, we can check its outcome attribute. We guarantee the correctness of the answers as we have checked the annotations of attributes as mentioned previously.
These answers include responses like ``\textit{yes/no}", numerical values, sports names (such as ``\textit{volleyball}"), action categories in MultiSports and FineGym (such as ``\textit{block}"), and short phrases describing the reasons (such as ``\textit{out of bounds}"). See Fig. \ref{egdata} for more concrete examples of the attributes and the corresponding generated QA pairs. 

An interesting aspect of our dataset is that it can be utilized for other tasks due to our two-stage annotation process. Specifically, we have defined several attributes for each action instance, which can be considered as multiple labels. Consequently, our dataset can be employed for conducting multi-label classification.
Moreover, by altering the template from question answering to declarative sentences, we can generate descriptive or explanatory narrations for sports videos. Hence, our dataset can also be exploited for sports video captioning.

\subsubsection{QA Pair Generation for Gymnastics}

A major difference between aerobic gymnastics/FineGym and ball games is the absence of the concept of a team. Furthermore, there are no causal relations among the actions performed in these sports. Therefore, we only generate descriptive and temporal questions for aerobic gymnastics/FineGym based on their annotations. In addition to the templates used for ball games, we have also designed some new templates. For descriptive questions, we include queries about the number of players or actions, such as ``\textit{How many actions does the player perform?}" and ``\textit{How many times does the player perform SOME-ACTION?}" For temporal questions, we add counting questions like ``\textit{How many times do the players do SOME-ACTION before SOME-ACTION?}" Such a QA design leverages the fine-grained features in the datasets and constitutes challenging temporal reasoning that forces the models to focus on the salient sub-actions in the whole gymnastic event.

\subsection{Diversity, Debiasing and Problem Setting}

\noindent \textbf{Diversity of Questions.} The limited diversity in generating questions using pre-designed templates may not significantly impact the significance of datasets for several reasons. First, in the context of sports, there is a high level of restriction imposed by rules and specific content, resulting in a more focused attention from the audience on particular actions and events. This inherent structure in sports content narrows down the range of potential questions, making template-based question generation a feasible approach.
Secondly, the practice of template-based question generation is widely accepted and employed in the construction of VideoQA datasets \citep{jang2017tgif, xu2017video, yang2021just}. 
The adoption of this approach in reputable datasets attests to its effectiveness and acceptance. Therefore, while the questions are template-based, their relevance and significance in the context of sports are preserved, ensuring the robustness of the dataset for analysis and evaluation.

\noindent \textbf{Debiasing.} Unlike daily scenarios, players in sports are highly restricted by rules. For instance, after the left team executes a ``\textit{volleyball serve}", the right team must follow with a ``\textit{first-pass}". This results in highly correlated actions in the videos, leading to questions and answers that are also highly correlated in our generated pairs. However, such questions are often meaningless for VideoQA, as the answers can be easily inferred through rules or common sense and should be removed.
To address this, we first obtain meta-questions by removing team information and the order of actions from the original questions. For example, the meta-question for ``\textit{What does the left team do after the other team does the second spike?}" becomes ``\textit{What does the team do after the other team does spike?}" We then examine the correlations between the meta-questions and their answers. If there is only one answer to the meta-question, we remove the corresponding QA pairs. If an answer to a meta-question occurs more frequently than other answers (frequency larger than 0.5), we randomly remove the corresponding QA pairs to balance the frequencies, ensuring they are all lower than 0.5.

\noindent \textbf{Problem Setting.} Following the approach in \citep{jang2017tgif, xu2017video}, we define an open-ended task based on the generated QA pairs. It is important to note that the open-ended setting, along with the multiple-choice setting, is widely used in the field of VideoQA. We have chosen to commit to the open-ended task instead of the multiple-choice one for two reasons: 1) The open-ended task is more challenging as it requires models to choose from a large answer set rather than selecting from several given options. 2) Limiting the answer choices to only several options would significantly decrease the diversity of the dataset. Specifically, all answers in the QA pairs form an answer pool, which is treated as 191 classes (after discarding classes with fewer than 30 samples). 

\noindent \textbf{Why not Fine-Grained Action Recognition?} It is essential to underscore that merely achieving fine-grained action recognition is insufficient for Sports-QA. The nature of the questions in this context demands not only precise detection of actions but also involves intricate temporal and causal modeling. To illustrate, questions that inquire about the effects of certain actions necessitate the accurate identification of temporally adjacent actions and a nuanced understanding of their causality. In summary, Sports-QA requires a more advanced level of comprehension and temporal reasoning beyond basic action recognition.

\subsection{Dataset Statistics}

Table \ref{n_qa} presents the numbers of QA pairs for various question types and different sports in Sport-QA. Our dataset comprises approximately 94K QA pairs. It's worth noting that a substantial portion of our descriptive questions involves complex action temporal localization and counting, presenting challenges similar to other question types. Additionally, we have balanced the number of questions across different sports.

\begin{table}[tbp]
\caption{The numbers of QA pairs for different types and different sports.}
\label{n_qa}

\begin{tabular}{@{}lrrrrr@{}}
\toprule
 Sports       & Descriptive & Temporal & Causal & Counterfactual & Total  \\ \midrule
Basketball & 5,629 & 22    & 785    & 278      & 6,714  \\
Football   & 6,659 & 1,355 & 1,949  & 523      & 10,486 \\
Volleyball & 6,120 & 360   & 1,942  & 685      & 9,107  \\
Gym     & 6,382 & 1,997 & 0      & 0        & 8,379  \\
Floor Exercise      &  6,046      &  11,012     & 0      & 0        &   19,418     \\
Balance Beam       &   7,477     &   12,773    & 0      & 0        &   20,250     \\
Uneven Bars      &   7,294     &   12,124    & 0      & 0        &    17,058    \\
Vault      &   2,661     &   0    & 0      & 0       &   2,661     \\ \midrule
Total   &   48,268     &   39,643    & 4,676  & 1,486    &    94,073    \\ \botrule
\end{tabular}
\end{table}

\begin{figure*}[tbp]
\includegraphics[width=\textwidth]{pics/ans_dist_type.pdf}
\caption{The distributions of answer classes broken down by question types.}
\label{ans_dist_type}
\end{figure*}

The total number of videos in our dataset is 5,967, distributed across different sports as follows: basketball (879), football (1,030), volleyball (586), gym (505), vault (501), uneven bars (834), floor exercise (770), and balance beam (862). Sports-QA is divided into training, validation, and testing sets (60\%/20\%/20\%). Specifically, videos of the same sport are randomly assigned to each set to ensure similar distributions of sport types across subsets. Additionally, we have ensured that the distributions of answer classes are similar in different subsets.

Fig. \ref{ans_dist_type} illustrates the distribution of answer classes broken down by question types in Sports-QA. Note that we only showcase the first 80 classes with more examples. As depicted in the figure, the distribution of answers is long-tailed, and the unbalanced nature adds to the challenge of our dataset. Meanwhile, classes of the same type, such as ``yes/no", are balanced. This further increases the difficulty of our dataset, as the models are required to examine the video for answering the question instead of merely guessing the answer based on the question type. 

\section{Auto-Focus Transformer for Sports Video Question Answering}

A key characteristic of the sports VideoQA task is the need for reasoning over temporal dependencies of various scales for multiple objects. For instance, a question asking about the number of actions requires the model to capture global dependencies in the video, whereas a question querying the effect of an action relies on short-term temporal information. The scale of temporal dependency required varies based on the question. However, current Transformer-based or GNN-based VideoQA methods tend to focus on global dependencies, irrespective of the required scale of temporal information. To address this limitation, we propose the Auto-Focus Transformer (AFT), a new Transformer encoder featuring a novel multi-head Auto-Focus Attention (AFA) mechanism, designed specifically for sports VideoQA.

\begin{figure}[tbp]
\centering
\includegraphics[width=\columnwidth]{pics/aft.pdf}
\caption{The structure of the sports  VideoQA model based on the proposed Auto-Focus Transformer.}
\label{aft}
\end{figure}

Specifically, we start by extracting the appearance feature and motion feature of the frame using pretrained ResNet \citep{he2016deep} and I3D \citep{carreira2017quo} (refer to Section \ref{feaext} for more details). Subsequently, we concatenate these features and input them into a linear layer for dimension reduction and feature fusion. Consequently, for each video, we obtain a sequence of frame representations ($\bm{R}$). Regarding the question, we utilize word embedding and a bi-directional RNN to obtain the text representations, encompassing both the global one ($\bm{w}$) and the local ones ($\bm{W}$).

We then  capture the dependencies among frames using AFT where the AFA mechanism
is computed as follows (the multi-head AFA is defined likewise): given the key set $\left\{\bm{k}_i\right\}$, the value set $\left\{\bm{v}_i\right\}$, and a query $\bm{q}_j$ ($\bm{q}_j,\bm{k}_i,\bm{v}_i \in \mathbb{R}^d$ are mapped from $\bm{R}$ in AFA), 
\begin{equation}
{\rm AFA}(\bm{q}_j)=\sum_{f\in \mathcal{F}}\alpha_f\sum_{i\in \mathcal{D}_j^f}{\rm softmax}_i\left(\frac{\bm{q}_j^\mathrm{T}\bm{k}_i}{\sqrt{d}}\right)\bm{v}_i,
\end{equation}
where $\mathcal{F}$ is a set of pre-defined focal lengths. $\mathcal{D}_j^f=\{i||i-j|\leq f\}$ is the index set of the neighbours of $\bm{q}_j$ within focal length $f$. $\alpha_f\in(0,1)$ is the dynamic weight of focus $f$, satisfying $\sum_f\alpha_f=1$ and depended on the question. As shown in Fig. \ref{aft},   $\bm{\alpha}=\{\alpha_f\}_{f=1}^\mathcal{|F|}$ are obtained by applying linear projection and softmax to the global representation of question.  
Compared to traditional attention that models global dependencies in any case, our AFA captures only the temporal dependencies of the scale required by the question. This design allows the model to focus solely on crucial information and discard irrelevant parts for better prediction. 
We denote the output of AFT as $\bm{M}$.

For the video-text integration, we draw inspiration from \citep{li2022invariant} and \citep{jiang2020reasoning}, employing a graph convolutional network (GCN) and block fusion \citep{ben2019block}. Refer to \citep{li2022invariant} for detailed information. Ultimately, we obtain a fused feature $\bm{s}$ and predict the answer using a linear projection and softmax.

\section{Experiments}
 

 
\subsection{Experimental Settings}
\label{feaext}

We start by explaining the experimental settings, covering data pre-processing and feature extraction, implementation details, and the evaluation metrics.

\noindent\textbf{Pre-processing and Representation.} The videos in our dataset are down-sampled to 5 FPS to reduce temporal redundancy. We extract two types of visual features for experiments: appearance feature and motion feature. For each feature type, we obtain both global and local features.
Specifically, we detect 10 objects in every two frames of each video using Faster-RCNN \citep{ren2015faster} (with ResNet-101 \citep{he2016deep} as the backbone and a score threshold set to 0.6), pretrained on COCO \citep{lin2014microsoft}. These detected objects serve as local appearance features in our work. Additionally, we use the global average pooling of Res4 activation in Faster-RCNN as the global appearance feature.
To extract motion features, we employ I3D \citep{carreira2017quo} pretrained on Kinetics \citep{carreira2017quo}. Specifically, for each frame, we combine it with its adjacent 7 frames, creating a clip that we then feed to I3D to extract a 3D feature map. The global motion feature of a frame is obtained by applying global average pooling to the 3D feature. The local motion feature of an object in a frame is obtained by applying RoIAlign \citep{he2017mask} (output size $1\times 1$) to the 3D feature with its corresponding bounding box. Note that our method uses only the global feature.
For language, we explore two types of word embeddings: GloVe \citep{pennington2014glove} and pre-trained BERT \citep{devlin2018bert}.
 
\noindent\textbf{Implementation Details.} We set the attention focus to $\mathcal{F}=\{3,9,80\}$ to cover short-term, mid-term, and long-term dependencies, respectively. The hidden dimension $d$ is set to 512. The loss function is cross-entropy. The models are trained for 50 epochs with a learning rate of $1\times 10^{-4}$ and a batch size of 16, using the Adam optimizer \citep{kingma2014adam}.

\noindent\textbf{Evaluation Metrics.} For evaluation metrics, we employ accuracy to showcase the models' performance on our dataset. Additionally, recognizing the long-tail distribution of our dataset, we evaluate methods using F1-score. F1-score is particularly important for sports VideoQA as it explicitly reflects performance averaged over imbalanced classes.

\begin{table}[tbp]
\caption{Baseline results on Sports-QA, including random choosing, semantic-aware (S-A) random choosing, LSTM-based BlindQA, and Transformer-based BlindQA. The best and the second-best results (\%) are \textbf{bold} and \underline{underlined}.} 
\label{bl}
\begin{tabular}{@{}ccccccc@{}}
\toprule
\multirow{2}{*}{Metric} & \multicolumn{2}{c}{Baseline} & \multicolumn{2}{c}{GloVe}            & \multicolumn{2}{c}{BERT}                    \\\cmidrule(l){2-3} \cmidrule(l){4-5} \cmidrule(l){6-7} 
                        & Random   & S-A Random   & LSTM & Transformer                   & LSTM                          & Transformer \\ \midrule
Accuracy                & 0.5      & 24.6         & 44   & 43.7                          & \underline{43.9} & 43.9        \\
F1-score                & 0.3      & 5.4          & 13.5 & \underline{14.9} & 13.8                          & 15.8        \\ \bottomrule
\end{tabular}
\end{table}

\begin{table*}[tbp]
\caption{The results of different methods on Sports-QA. The sport-wise accuracy, question-type-wise accuracy, overall accuracy and F1-score are presented (\%).
The best and second best results are \textbf{bold} and \underline{underlined}.} 
\label{res1}

\resizebox{0.95\columnwidth}{!}{\begin{tabular}{clcccccccccll}
\toprule
 \multirow{2}{*}{Word Emb.}& \multirow{2}{*}{Model} & \multicolumn{5}{c}{Sports} & \multicolumn{4}{c}{Question Type} & \multirow{2}{*}{Acc.} & \multirow{2}{*}{F1-score} \\ \cmidrule(lr){3-7}\cmidrule(lr){8-11}
 &  & Basket. & Foot. & Volley. & Gym  &FineG.& Desc. & Temp. & Causal & Counter. &  &  \\ \hline\hline
 & \cellcolor[HTML]{C0C0C0}BlindQA & \cellcolor[HTML]{C0C0C0}33.4& \cellcolor[HTML]{C0C0C0}62.9& \cellcolor[HTML]{C0C0C0}44.9& \cellcolor[HTML]{C0C0C0}36.5&\cellcolor[HTML]{C0C0C0}42.0& \cellcolor[HTML]{C0C0C0}51.7& \cellcolor[HTML]{C0C0C0}32.7& \cellcolor[HTML]{C0C0C0}48.7& \cellcolor[HTML]{C0C0C0}51.3& \cellcolor[HTML]{C0C0C0}43.7& \cellcolor[HTML]{C0C0C0}14.9\\
 & CoMem \citep{gao2018motion} & 72.8& 63.1& 69.8& 70.0&50.3& 77.0& 32.9& 50.0& 60.9& 57.1& 23.1\\
 & HME \citep{fan2019heterogeneous} & 64.8& 63.3& 68.5& 71.6&49.3& 76.5& 31.2& 50.4& 54.3& 56.0& 23.6\\
 & HGA \citep{jiang2020reasoning} & 71.3& 65.4& 72.2& 72.2&51.3& 78.1& 34.4& 52.0& 59.2& 58.3& 24.5\\
 & HQGA \citep{xiao2022video} & 72.8& 64.6& 70.5& 71.6&51.0& 77.6& 33.8& 54.0& 58.2& 57.9& \underline{25.0}\\
 & IGV \citep{li2022invariant} & 68.8& 66.0& 70.6& 72.4&52.1& 78.2& 34.7& 52.1& 59.2& \underline{58.5}& 24.8\\
 & MASN \citep{seo2021attend} & 71.2& 64.2& 70.1& 66.9&50.5& 76.4& 33.6& 50.2& 58.9& 57.0& 23.4\\ \cmidrule(l){2-13} 
 & Baseline (Ours) & 69.5& 65.4& 68.9& 71.8&52.3& 77.8& 35.0& 52.4& 59.9& 58.0& 23.7\\
\multirow{-9}{*}{GloVe} & AFT (Ours) & 73.9& 67.8& 69.5& 71.8&52.5& 78.9& 35.3& 55.1& 56.3& \textbf{59.2} {\color[HTML]{036400}(+1.2)}& \textbf{25.6} {\color[HTML]{036400}(+1.9)}\\ \hline\hline
 & \cellcolor[HTML]{C0C0C0}BlindQA & \cellcolor[HTML]{C0C0C0}34.7& \cellcolor[HTML]{C0C0C0}52.1& \cellcolor[HTML]{C0C0C0}45.9& \cellcolor[HTML]{C0C0C0}38.3&\cellcolor[HTML]{C0C0C0}43.9& \cellcolor[HTML]{C0C0C0}51.8& \cellcolor[HTML]{C0C0C0}33.3& \cellcolor[HTML]{C0C0C0}48.0& \cellcolor[HTML]{C0C0C0}53.9& \cellcolor[HTML]{C0C0C0}43.9& \cellcolor[HTML]{C0C0C0}15.8\\
 & CoMem \citep{gao2018motion} & 74.1& 64.0& 68.7& 70.7&49.3& 77.2& 32.1& 48.4& 55.6& 56.6& 26.4\\
 & HME \citep{fan2019heterogeneous} & 64.7& 63.4& 66.2& 70.6&49.3& 76.6& 30.7& 46.6& 53.6& 55.6& 22.9\\
 & HGA \citep{jiang2020reasoning} & 71.6& 66.5& 69.6& 70.7&51.3& 78.0& 33.8& 54.3& 56.6& 58.1& \underline{25.1}\\
 & HQGA \citep{xiao2022video} & 74.1& 63.5& 67.2& 70.8&50.3& 76.6& 33.6& 49.1& 57.9& 57.0& 25.0\\
 & IGV \citep{li2022invariant} & 72.0& 63.9& 70.4&72.4 &51.7& 78.0&34.2 & 51.9& 63.2& \underline{58.2}&23.8 \\
 & MASN \citep{seo2021attend} & 73.0& 63.0& 69.7& 66.6&50.1& 76.4& 32.5& 50.6& 61.2& 56.6& 24.1\\ \cmidrule(l){2-13} 
 & Baseline (Ours) & 70.7& 66.6& 69.2& 71.5&51.8& 78.0& 34.8& 50.5& 60.2& 57.9& 23.9\\
\multirow{-9}{*}{BERT} & AFT (Ours) & 72.3& 67.9& 70.4& 71.6&52.4& 78.3& 35.5& 56.8& 58.2& \textbf{59.1} {\color[HTML]{036400}(+1.2)}& \textbf{25.4} {\color[HTML]{036400}(+1.5)}\\ \botrule
\end{tabular}}
\end{table*}

\begin{table}[tbp]
\caption{The visual features used by different models. A./M./G./L. represents Appearance/Motion/Global/Local. Mem, IG, HL, and GNN stand for Memory, Invariant Grounding, Hierarchical Learning, and Graph Neural Network, respectively.} 
\label{fea}
\begin{tabular}{@{}lcccccc@{}}
\toprule
Model                           & Venue   & Insight            & G.A        & G.M.       & L.A.       & L.M.       \\ \midrule
CoMem \citep{gao2018motion}      & CVPR'18 & Mem             & \checkmark & \checkmark &            &            \\
HME \citep{fan2019heterogeneous} & CVPR'19 & Mem             & \checkmark & \checkmark &            &            \\
HGA \citep{jiang2020reasoning}   & AAAI'20 & GNN                & \checkmark & \checkmark &            &            \\
IGV \citep{li2022invariant}      & CVPR'22 & IG & \checkmark & \checkmark &            &            \\
HQGA \citep{xiao2022video}       & AAAI'22 & HL            & \checkmark & \checkmark & \checkmark &            \\
MASN \citep{seo2021attend}       & ACL'21  & GNN                & \checkmark & \checkmark & \checkmark & \checkmark \\\midrule
AFT (Ours)      &   &   AFA              & \checkmark  & \checkmark  &  & \\\botrule
\end{tabular}
\end{table}

\subsection{Baseline Study}

Several baselines are constructed to evaluate their performance on our Sports-QA dataset as follows.

\noindent\textbf{Random Choosing:} Answers are randomly selected from the 191 answer classes, resulting in an accuracy of approximately $\frac{1}{191} \approx 0.5\%$. The F1-score is obtained by running the random test for 200 times and taking the average.

\noindent\textbf{Semantic-Aware Random Choosing:} The randomly selected answer is constrained by the question type. For example, binary questions (beginning with ``\textit{Do}" or ``\textit{Is}") can only choose from ``\textit{yes}" or ``\textit{no}"; questions asking about the number of something (beginning with ``\textit{How many}") can only choose a number.

\noindent\textbf{BlindQA:} Models are constructed with a question encoder and an MLP-based answer decoder, without exploiting visual information. Two types of word embeddings (Glove and pre-trained BERT) and two question encoders (LSTM and Transformer \citep{vaswani2017attention}) are studied.

The results of the baselines are shown in Table \ref{bl}. Random choosing exhibits poor accuracy and F1-score, but considering constraints from the questions significantly improves performance. BlindQA achieves considerable results in both accuracy and F1-score, indicating noticeable semantic correlations between questions and answers. By maximizing the likelihood of the answer conditioned on a specific question type, the model fits the answer distribution given different questions in the training set.

Regarding word embedding, pre-trained BERT achieves slightly better accuracy than GloVe, with noticeable improvements in F1-score. Concerning different question encoders, Transformer and LSTM achieve similar accuracy, but Transformer outperforms LSTM in F1-score. In summary, word embedding has a greater impact on performance than the question encoder.

\subsection{Benchmark on Sports-QA}

We benchmark several VideoQA methods on our Sports-QA dataset, comparing them to CoMem \citep{gao2018motion}, HME \citep{fan2019heterogeneous}, HGA \citep{jiang2020reasoning}, MASN \citep{seo2021attend}, HQGA \citep{xiao2022video}, and IGV \citep{li2022invariant}, with HQGA \citep{xiao2022video} and IGV \citep{li2022invariant} representing the state of the art. It's worth noting that we only evaluate methods without large-scale video-text pretraining, excluding pretrained models like MERLOT \citep{zellers2021merlot}, VIOLET \citep{fu2021violet}, and All-in-one \citep{wang2022all}. The exclusion is based on the following reasons: 1) pretrained models require large-scale additional data, making comparisons unfair for methods without pretraining; 2) we aim to highlight the characteristics of our dataset, and the universal knowledge gained by pretrained models could introduce bias in understanding sports scenarios. Table \ref{fea} shows the types of visual features used by each model, along with a summary of each method's insights. For CoMem, HME, and HGA, we use re-implementations provided by \citep{xiao2021next}, and for MASN, HQGA, and IGV, we utilize the official implementations by their respective authors. \textbf{The baseline model has the same architecture as shown in Fig. \ref{aft}, with AFT replaced with a standard Transformer Encoder.}

In Table \ref{res1}, the VideoQA methods demonstrate substantial improvements compared to BlindQA. Notably, the most significant improvements are observed for basketball (approximately 40\% increase in accuracy), indicating the critical role of visual information in basketball for question answering. Conversely, the improvements for football are less pronounced (about 4\%), suggesting that BlindQA already achieves considerable accuracy in this sport. The results suggest high correlations between questions and answers in football, posing challenges for VideoQA models to fully exploit visual information, potentially due to the complexity introduced by a larger number of players.

Examining different question types, the greatest improvements are observed for descriptive questions (around 27\% increase in accuracy). Additionally, VideoQA performance on causality and counterfactual questions shows significant improvements compared to BlindQA. However, results for temporal questions are similar between VideoQA methods and BlindQA (except for our AFT), suggesting that VideoQA models may struggle with modeling fine-grained temporal dependencies among actions in sports videos.

Among the different VideoQA models, our AFT achieves the best performance in both accuracy and F1-score. The significant improvements compared to the baseline indicate the effectiveness of the proposed attention mechanism in distinguishing semantically similar classes.

\subsection{Further Analysis}

\noindent\textbf{Generalization Ability across Sports}. In this study, our primary objective is to assess the generalization capabilities across different sports domains. Our approach involves an initial pretraining phase on a set of four sports, followed by fine-tuning on a fifth, distinct sport. We meticulously evaluate the performance of this pretraining-fine-tuning methodology in comparison to training a model entirely from scratch.
The outcomes, detailed in Table \ref{resf}, showcase a notable enhancement in accuracy when employing the pretraining strategy on diverse sports. These findings suggest that the model acquires a nuanced understanding of common semantic features during the initial pretraining. Moreover, the model demonstrates its ability to effectively generalize this acquired knowledge to previously unseen sports during the subsequent fine-tuning phase.

\begin{table}[tbp]
\caption{The table presents comparisons (accuracy \%) between models trained from scratch for individual sports and those fine-tuned using data from other sports, along with the corresponding improvements (Imp.).} 
\label{resf}

\begin{tabular}{cccccc}
\toprule
Pretraining& Basketball                       & Football                         & Volleyball                       & Gym                              & FineG. \\ \midrule
\xmark    & 57.1& 55.0& 56.4& 70.2&        48.5\\
\cmark    & 58.3& 56.5& 57.7& 71.7&        50.3\\ \midrule
 Imp.& {\color[HTML]{036400}+1.2}& {\color[HTML]{036400}+1.5}& {\color[HTML]{036400}+1.3}& {\color[HTML]{036400}+1.5}&{\color[HTML]{036400}+1.8}\\\botrule
\end{tabular}
\end{table}

\begin{table}[t]
\caption{The accuracy (\%) of various focus lengths and their combinations. Note that the combination of the same focal length implies the use of only one focus length.} 
\label{fs}

\begin{tabular}{c|ccc|ccc|c}
\toprule
   & 1 & 3 & 5 & 7 & 9 & 11 & 80   \\ \midrule
1  & \cellcolor[HTML]{C0C0C0}58.2& / & / & 58.9& 59.0& 58.7& 58.5\\
3  & / & \cellcolor[HTML]{C0C0C0}58.7& / & 59.0& \textbf{59.1}& 58.6& 58.9\\
5  & / & / & \cellcolor[HTML]{C0C0C0}58.6& 58.8& 58.9& 58.7& 58.8\\ \midrule
7  & / & / & / & \cellcolor[HTML]{C0C0C0}58.7& / & /  & 58.9\\
9  & / & / & / & / & \cellcolor[HTML]{C0C0C0}58.8& /  & 59.0\\
11 & / & / & / & / & / & \cellcolor[HTML]{C0C0C0}58.4& 58.6\\ \midrule
80 & / & / & / & / & / & /  & \cellcolor[HTML]{C0C0C0}58.0 \\ \botrule
\end{tabular}
\end{table}

\noindent\textbf{Impact of Focus Length.} In this experiment, we investigate the impact of different focus lengths and their combinations. For short-term dependency, we consider lengths of 1, 3, and 5. For the mid-term, we explore lengths of 7, 9, and 11. Regarding long-term (global) dependency, we set the focus length to the video length, which is 80 seconds. The results are presented in Table \ref{fs}.
From the results, it is evident that the model with global attention performs less effectively compared to those with local attention. However, upon applying our auto-focus attention mechanism, there is a significant improvement in performance. Notably, the combination of focus lengths 3 and 9 achieves the highest accuracy. In our final model, we incorporate the global focus, resulting in the best accuracy at 59.2\%.

\subsection{Visualization}

\noindent \textbf{Qualitative Results.} Fig. \ref{pred} illustrates predictions from various methods. In some instances, BlindQA successfully guesses the correct answer by leveraging the correlation between the question and the answer in the training set. However, this approach is not foolproof, as the blinded model consistently selects the most correlated answer to the question.
In comparison, the VideoQA models struggle to discern subtle differences between semantically or visually similar answer classes. For example, actions like ``\textit{long pass}" and ``\textit{cross}" both involve passing the ball to other players but differ in passing direction and distance, presenting challenges for accurate distinction. Additionally, the compared VideoQA models exhibit limitations in fine-grained motion analysis, such as action counting, as evident in the last example. With its ability to capture various types of temporal dependencies, our AFT outperforms the compared models on these challenging samples.

\begin{figure}[tbp]
\centering
\includegraphics[width=\columnwidth]{pics/vis2.pdf}
\caption{
Visualization of the predictions. The correct/wrong predictions are highlighted in \textcolor[RGB]{84,130,53}{green}/\textcolor[RGB]{192,0,0}{red}. The predicted probability of each answer is also reported. 
}
\label{pred}
\end{figure}

\begin{figure}[tbp]
\centering
\includegraphics[width=\columnwidth]{pics/attn_vis.pdf}
\caption{Visualization of focus weight. We show four examples of the highest weight for each focus.}
\label{attn_vis}
\end{figure}

\noindent\textbf{Focus Weight.}
Fig. \ref{attn_vis} depicts the focus weights of selected questions. Notably, questions seeking information about specific actions (e.g., ``\textit{win a point}") heavily rely on short-term information (focus length 3). In contrast, questions like ``\textit{How many times does the right team do `block'?}" necessitate a stronger emphasis on long-term information. Questions pertaining to the relationships between two actions show a preference for mid-term information, offering insights into the cause and effect of actions. Intuitively, the mid-term focus length can provide more nuanced information on the interactions between actions. This visualization affirms the effectiveness of our model, which dynamically integrates different focus scales based on the diverse temporal dependencies required by various questions.

\section{Conclusion}

We introduce Sports-QA, a novel dataset emphasizing professional action understanding, interactions among multiple players, and fine-grained motion analysis for sports VideoQA. Featuring multiple sports and diverse question types, the dataset offers a comprehensive evaluation platform for VideoQA methods. Additionally, we propose Auto-Focus Transformer (AFT), designed to capture various temporal dependencies. Our extensive experiments on the dataset demonstrate the effectiveness of AFT.

\section{Limitation and Future Work}
\label{limit}

In our study, we introduce the innovative Auto-Focus Transformer (AFT) designed to capture intricate dependencies across frames within diverse temporal windows, catering to a range of question types. Unlike the conventional Transformer model, the AFT demands increased computational resources, scaling linearly with the number of focuses. Although we have optimized the implementation for high parallelization, it may pose challenges on devices with constrained memory capacities. Looking ahead, our research will focus on devising a novel architectural framework that retains the distinctive features of AFT while ensuring computational efficiency, addressing the limitations associated with memory constraints.

\section{Ethical Consideration}
\label{ethical}

In developing our Sports-QA dataset, sourced from MultiSports and FineGym, we prioritized ethical considerations, adhering to data privacy and usage standards while ensuring compliance with the terms of the original datasets. To address potential biases, we actively promoted diversity in sports categories and specific sports actions, aiming for a balanced representation.
Mindful of the environmental impact, we optimized our data processing to minimize resource consumption. Acknowledging the potential for misuse, we emphasize the dataset's intended use in positive sports analytics applications, such as performance analysis. Our commitment to transparency and reproducibility is evident in our methodology and dataset accessibility, ensuring that our contribution to sports analytics and computer vision is both responsible and beneficial.

\section*{List of Crucial Actions}

The selected crucial actions are listed in Table \ref{acts_}.

\begin{table}[tbp]
\caption{Crucial actions selected for Sports-QA, which are classified into offensive actions and defensive actions.} 
\label{acts_}

\begin{tabular}{@{}lll@{}}
\toprule
\multirow{2}{*}{Sports}     & \multicolumn{2}{c}{Action}                                \\ \cmidrule(l){2-3} 
                            & Offensive                    & Defensive                  \\ \midrule
\multirow{5}{*}{Volleyball} & \textit{serve}               & \textit{first pass}        \\
                            & \textit{dink}                & \textit{defend}            \\
                            & \textit{spike}               & \textit{protect}           \\
                            & \textit{second attack}       & \textit{adjust}            \\
                            & \textit{no offensive attack} & \textit{save}              \\ \midrule
\multirow{5}{*}{Football}   & \textit{shoot}               & \textit{diving}            \\
                            & \textit{cross}               & \textit{tackle}            \\
                            & \textit{through pass}        & \textit{steal}             \\
                            & \textit{long pass}           & \textit{block}             \\
                            & \textit{aerial duels}        & \textit{}                  \\ \midrule
\multirow{5}{*}{Basketball} & \textit{jump ball}           & \textit{save}              \\
                            & \textit{free throw}          & \textit{pass steal}        \\
                            & \textit{2-point shot}        & \textit{dribble steal}     \\
                            & \textit{3-point shot}        & \textit{defensive rebound} \\
                            & \textit{offensive rebound}   & \textit{}                  \\ \botrule
\end{tabular}
\end{table}

\section*{Question Templates for Sports-QA}

\subsection*{\centering Gymnastics}

\subsubsection*{Descriptive Question}
\begin{itemize}
    \item \textit{What is the video about?}
 \item \textit{How many SOME-ACTION do the players perform?}
 \item \textit{How many types of SOME-ACTION do the players perform?}
 \item \textit{How many times do the players perform SOME-ACTION?}
 \item \textit{How many players perform the i-th SOME-ACTION?}
 \item \textit{Do the players perform SOME-ACTION?}
\end{itemize}

\subsubsection*{Temporal Question}
\begin{itemize}
 \item \textit{Do the players perform SOME-ACTION before/after the i-th SOME-ACTION?}
 \item \textit{What do the players perform before/while/after performing the i-th SOME-ACTION?}
 \item \textit{How many times do the player do SOME-ACTION before/after doing the i-th SOME-ACTION?}
\end{itemize}

\subsection*{\centering Ball Games}
\subsubsection*{Descriptive Question}

\begin{itemize}
    \item \textit{What is the video about?}
\item \textit{Does one team score in the video?}
\item \textit{Which team scores in the video?}
\item \textit{How many times does SOME-TEAM do SOME-ACTION?}
\item \textit{How many times does SOME-TEAM do SOME-ACTION successfully?}
\item \textit{Does SOME-TEAM do their i-th SOME-ACTION successfully?}
\end{itemize}

\subsubsection*{Temporal Question}

\begin{itemize}
    \item \textit{What does SOME-TEAM do before/after their i-th SOME-ACTION?}
    \item \textit{What does SOME-TEAM do before/after the other team does their i-th SOME-ACTION?}
\end{itemize}

\subsubsection*{Causal Question}

\begin{itemize}
    \item \textit{Why does SOME-TEAM do the i-th SOME-ACTION?}
    \item \textit{What is the effect of the i-th SOME-ACTION of SOME-TEAM?}
    \item \textit{How does SOME-TEAM succeed in doing the i-th something?}
\end{itemize}

\subsubsection*{Counterfactual Question}

\begin{itemize}
    \item \textit{Would SOME-TEAM succeed in doing the i-th SOME-ACTION if the other team did not do SOME-ACTION?}
\end{itemize}

\section*{Declarations}

\subsection*{Data and Code Availability}
The proposed dataset, Sports-QA, and the code of the proposed method are available at \url{https://github.com/HopLee6/Sports-QA}.

\end{document}