\title{LawLLM: Law Large Language Model for the US Legal System}

\begin{document}

\title{LawLLM: Law Large Language Model for the US Legal System}

\author{Dong Shu}
\orcid{0009-0009-9785-3934}
\affiliation{%
  \institution{Northwestern University}
  \city{Evanston}
  \state{IL}
  \country{United States}}
\email{dongshu2024@u.northwestern.edu}

\author{Haoran Zhao}
\orcid{0000-0003-4563-9302}
\affiliation{%
  \institution{Northwestern University}
  \city{Evanston}
  \state{IL}
  \country{United States}
}
\email{haoranzhao2024@u.northwestern.edu}

\author{Xukun Liu}
\orcid{0000-0002-4608-9448}
\affiliation{%
  \institution{Northwestern University}
  \city{Evanston}
  \state{IL}
  \country{United States}
}
\email{xukunliu2025@u.northwestern.edu}

\author{David Demeter}
\orcid{0000-0001-7560-1132}
\affiliation{%
  \institution{Northwestern University}
  \city{Evanston}
  \state{IL}
  \country{United States}
}
\email{ddemeter@u.northwestern.edu}

\author{Mengnan Du}
\orcid{0000-0002-1614-6069}
\affiliation{%
  \institution{New Jersey Institute of Technology}
  \city{Newark}
  \state{NJ}
  \country{United States}
}
\email{mengnan.du@njit.edu}

\author{Yongfeng Zhang}
\orcid{0000-0003-2633-8555}
\affiliation{%
  \institution{Rutgers University}
  \city{New Brunswick}
  \state{NJ}
  \country{United States}
}
\email{yongfeng.zhang@rutgers.edu}

\renewcommand{\shortauthors}{Dong Shu et al.}

\begin{abstract}
In the rapidly evolving field of legal analytics, finding relevant cases and accurately predicting judicial outcomes are challenging because of the complexity of legal language, which often includes specialized terminology, complex syntax, and historical context. Moreover, the subtle distinctions between similar and precedent cases require a deep understanding of legal knowledge. Researchers often conflate these concepts, making it difficult to develop specialized techniques to effectively address these nuanced tasks. In this paper, we introduce the Law Large Language Model (\sysname), a multi-task model specifically designed for the US legal domain to address these challenges. \sysname excels at Similar Case Retrieval (SCR), Precedent Case Recommendation (PCR), and Legal Judgment Prediction (LJP). By clearly distinguishing between precedent and similar cases, we provide essential clarity, guiding future research in developing specialized strategies for these tasks. We propose customized data preprocessing techniques for each task that transform raw legal data into a trainable format. Furthermore, we also use techniques such as in-context learning (ICL) and advanced information retrieval methods in \sysname. The evaluation results demonstrate that \sysname consistently outperforms existing baselines in both zero-shot and few-shot scenarios, offering unparalleled multi-task capabilities and filling critical gaps in the legal domain. Code and data are available at \url{https://github.com/Tizzzzy/Law_LLM}.
\end{abstract}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010405.10010455.10010458</concept_id>
       <concept_desc>Applied computing~Law</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010178.10010179</concept_id>
       <concept_desc>Computing methodologies~Natural language processing</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010257.10010258.10010262</concept_id>
       <concept_desc>Computing methodologies~Multi-task learning</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10002951.10003317.10003338.10003346</concept_id>
       <concept_desc>Information systems~Top-k retrieval in databases</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Applied computing~Law}
\ccsdesc[500]{Computing methodologies~Natural language processing}
\ccsdesc[500]{Computing methodologies~Multi-task learning}
\ccsdesc[500]{Information systems~Top-k retrieval in databases}

\keywords{Large Language Models, Multitask Learning, Legal System, Natural Language Processing}

\maketitle

\section{Introduction}
The development of Large Language Models (LLMs) has led to significant progress in computational linguistics, particularly impacting fields like legal analytics. Given the nature of legal language, which includes complex terminologies and context-specific logical frameworks, LLMs offer unprecedented capabilities in this domain \cite{naveed2023comprehensive}. The integration of LLMs into the legal field significantly boosts the efficiency of legal practitioners, such as lawyers and judges, by accurately interpreting their natural language input and generating most relevant responses. This reduces the need for extensive manual review of huge legal texts. Moreover, LLMs can provide lawyers with novel insights, revealing overlooked details and perspectives that can be critical in complex cases. Recent developments in legal domain have demonstrated the potential of LLMs in enhancing legal judgment predictions and handling various legal tasks effectively. For example, studies such as LM-CompEval-Legal \cite{shui2023comprehensive} have systematically evaluated the effectiveness of LLMs, other projects like PLJP \cite{wu2023precedent} and LoT \cite{jiang2023legal} have focused on integrating domain-specific models and advancing LLMs' understanding of legal reasoning.

Although these models have shown promise, there remain research challenges. First, these models generally address single-task challenges. In contrast, \sysname innovatively supports multiple legal tasks simultaneously, providing a more nuanced analysis of complex legal datasets and filling a critical void in the field. Second, another controversial area in the legal domain is the difference between precedent cases and similar cases \cite{qin2023incorporating}. Various models have been developed for precedent case recommendation, ranging from expert knowledge-based models to models based on natural language processing \cite{mentzingen2023automation, cao2024pilot, ma2023caseencoder, lewis2021precedent}. These approaches typically convert legal text into embeddings and calculate similarity at the embedding level, which aids in precedent selection. However, we believe that this approach is more on identifying similar cases with textual and contextual similarities, not precedent cases.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{samples/fig1.pdf}
  \caption{LawLLM supports three tasks: Similar Case Retrieval, Precedent Case Recommendation, and Legal Judgment Prediction.}
  \Description{overview of lawllm}
  \label{fig:intro}
\end{figure}

In our study, we emphasize the key differences between the two. Firstly, a precedent case must have been closed before the input legal case, ensuring its relevance and applicability to the current case under consideration. Secondly, precedent cases are those that were actually considered by judges in making their decisions, unlike similar cases which might not have been taken into account. Thirdly, similar cases share textual and thematic similarities in the case narrative or might fall into similar case categories, while precedent cases might seem unrelated at face value. It is also worth noting that while a legal case's precedent case can sometimes be the same as a similar case, this is not always the case. 

In this paper, we introduce the Law Large Language Model (\sysname), a multi-task LLM capable of Similar Case Retrieval (SCR), Precedent Case Recommendation (PCR) and Legal Judgment Prediction (LJP).
To build \sysname, we finetune Gemma-7B \cite{team2023gemini} using instruction tuning on United State real-life legal datasets and can perform LJP, PCR, and SCR tasks. The instructions for all three tasks fall under the classification category. By doing so, we can effectively minimize irrelevant and noisy options for the model, thereby improving its performance. We show the overall idea of \sysname in Figure \ref{fig:intro}. The development of \sysname also includes three innovative preprocessing approaches for transforming unstructured legal data into a trainable format. More specifically, for LJP, we summarize and extract verdicts from raw datasets and apply zero and few shot In-context learning (ICL) \cite{xie2021explanation, jin2024impact} technique to enhance the model performance. In PCR, \sysname connects ground truth legal cases' precedent relationships as a Knowledge Graph (KG), treating each case as a unique entity linked by precedential connections \cite{shu2024knowledge}. Additionally, the SCR task creates a legal case vector database and integrates advanced Information Retrieval (IR) techniques \cite{ethayarajh2019contextual, jin2024health}.

Our study presents \sysname as a pioneering model in the realm of legal LLMs. Our key contributions are given as follows:
\begin{itemize}
    \item We propose \sysname, which is adept at handling a range of legal tasks, including LJP, PCR, and SCR. This multi-task functionality is important in addressing the diverse requirements of the legal domain.
        
    
    \item \sysname distinguishes between precedent cases and similar cases, providing clarity on the objectives of each task. This clarification enables the future research to develop tailored strategies for those tasks.

    \item Experimental results indicate that \sysname outperformed all baseline models, including the GPT-4 model, across all three tasks. These results highlight \sysname's robust capabilities in the legal domain. 

\end{itemize}

\begin{figure*}
  \includegraphics[width=\textwidth]{samples/overflow.pdf}
  \caption{An overview of our LawLLM: Data Preprocessing is in the upper left in \textcolor{customgreen}{green}, Similar Case Retrieval Processing is in the upper right in \textcolor{customyellow}{yellow}, Precedent Case Recommendation is in the lower left in \textcolor{customred}{red}, and Legal Judgment Prediction is in the lower right in \textcolor{customblue}{blue}.}
  \Description{work overflow of lawllm}
  \label{fig:overflow}
\end{figure*}

\section{Related Work}

Legal AI is significantly increasing the efficiency and effectiveness of the legal community. AI technologies, specifically Large Language Models (LLMs), are leading the way in automating complex tasks like document analysis, case prediction, and legal research \cite{zhong2020does, wu-etal-2023-precedent}. LLMs utilize advanced algorithms and data analytics to process and generate legal texts, which leads to significant improvements in speed and accuracy \cite{zhou2023boosting}. In this section, we introduce the various applications of Legal AI and LLMs in legal practices.

\subsection{Precedent Case Recommendation}
The recommendation of precedent cases is a fundamental aspect of legal practice, as previous verdicts significantly affect current legal decisions. The field has evolved from early keyword-based searches and manual annotations to more complicated AI-driven models that improve retrieval efficiency and contextuality. \citet{wu-etal-2023-precedent} proposed the Precedent-Enhanced Legal Judgment Prediction framework, which combines LLMs with domain-specific expertise to improve legal prediction accuracy significantly. \citet{ma2023incorporating} developed the Structured Legal Case Retrieval system, which uses structural information from legal documents to improve case search precision and contextual relevance. Moreover, \citet{su2023caseformer} proposed the Caseformer. This innovative pre-training framework learns from a vast corpus of legal texts to refine case retrieval and contextualization across multiple languages.

\subsection{Similar Case Retrieval}
Besides precedent recommendation, retrieving similar cases, those sharing analogous facts or legal issues, is crucial for comprehensive legal analysis and strategy formulation. Traditionally, this process required extensive manual labor, with professionals needing to dig through large case databases \cite{mandal2021unsupervised, ma2023incorporating}. Today advances in NLP and machine learning have transformed this task, allowing semantic content extraction and comparison across documents. \citet{kang2013retrieval} enhanced similarity-based retrieval by incorporating associative knowledge. This approach refines retrieval outcomes by leveraging similarity and associative analyses, a technique also proven effective in other fields such as medical diagnosis and IT service management. \citet{mandal2021unsupervised} analyzed textual similarity techniques on an Indian Supreme Court dataset and discovered that traditional methods like TF-IDF outperform modern context-aware models like BERT. \citet{wu2021joint} studied semantic retrieval in the Chinese judicial system and developed a model that generates knowledge graphs for cases to improve trial accuracy and fairness. These technological advances have greatly simplified legal research, making it more effective and comprehensive.

\subsection{Legal Judgment Prediction}
Predicting legal judgments involves estimating potential verdicts based on a deep analysis of historical data and established legal standards. Initial models in this field were relatively simple, mainly depending on linear algorithms incapable of capturing the various aspects of legal reasoning. \citet{wang2020study} CNN-BiGRU multi-task learning model improves prediction accuracy through the utilization of shared information from related legal subtasks. \citet{chalkidis2019neural} used European Court of Human Rights data to establish robust performance benchmarks for long legal texts using hierarchical BERT. \citet{rusnachenko2023nclu_team} showed attention-based methods could improve system performance by optimizing document preprocessing and attention mechanisms in competition contexts. These models predict outcomes and are constantly learning from new cases to improve their accuracy, demonstrating the adaptability of LLMs in legal judgment prediction.

\subsection{LLMs in the Legal Domain}
Prior to the development of large language models (LLMs), pre-trained language models (PLMs) for specific domains were explored, such as Lawformer, which is to process lengthy Chinese legal documents using a Longformer-based architecture \cite{xiao2021lawformer}. Researchers discovered that models like GPT-4 could successfully pass bar exams as LLMs gained attention, demonstrating profound abilities in legal reasoning and text generation \cite{katz2024gpt}. This success resulted in the growth of legal domain-specific LLMs, such as Chatlaw, which utilizes conversational AI to improve user interactions with legal systems \cite{cui2023chatlaw}. In this vein, SaulLM-7B was introduced as the first LLM explicitly designed for comprehending and generating legal texts, leveraging a substantial legal corpus to achieve state-of-the-art performance \cite{colombo2024saullm}. LLMs' influence extends beyond specific tasks to broader legal operations. These applications range from document automation, where LLMs assist in drafting and reviewing legal documents, to compliance monitoring, which ensures adherence to regulatory standards \cite{Sun2023A}. LLMs simplify complex legal processes for non-specialists and lower barriers to legal advice \cite{Goodson2023Intention}. This broad application of LLMs demonstrates their broad application and the potential for continued innovation in the legal sector.

Despite the success of those contemporary works, these models primarily focus on utilizing LLMs' understanding and capabilities to perform general legal question answering. However, \sysname is designed to leverage the LLMs' comprehension and learned abilities to predict and perform specific tasks within the legal domain.

\section{Methodology}

In this study, we propose the Law Large Language Model (\sysname) to address three critical tasks within the legal domain: Similar Case Retrieval (SCR), Precedent Case Recommendation (PCR), and Legal Judgment Prediction (LJP). Our methodological framework, illustrated in Figure \ref{fig:overflow}, is divided into four distinct parts: Data Preprocessing, SCR Processing, PCR Processing, and LJP Processing.

\subsection{Data Preprocessing}

Our approach begins with the systematic collection of case data from legal databases, denoted as \( \mathcal{D} \). We make sure all collected raw case data, \( d_i \in \mathcal{D} \), encompasses a variety of information as below:

\begin{equation}
\begin{aligned}
d_i = \{& \text{Title, } \text{Date, } \text{Judge, } \text{Plaintiff(s), } \text{Plaintiff's Attorney(s), }\\ 
& \text{Defendant(s), }\text{Defendant's Attorney(s), } \text{Case Detail, } \\
& \text{Precedent Relationship}\}.
\end{aligned}
\end{equation}

As depicted in the upper left of Figure \ref{fig:overflow}, data preprocessing consists of three primary steps:

\vspace{3pt}\noindent\textbf{Step 1.}\, Given the voluminous nature of the textual content within case detail and their often implicit verdicts, we utilize a GPT-4 \cite{achiam2023gpt} model to extract core information and summarize each case. This step reduces information overload and ensures the adaptability of our dataset to the constraints of Gemma, particularly with token size limitations. The GPT-4 preprocess instruction is shown here: 

\begin{formal}
I have a legal case description and require two distinct pieces of information:
\newline
1. Summary: Please provide a detailed summary of the case, focusing on
the facts and events. Exclude any information about the verdict.
\newline
2. Verdict: State the verdict of the case, consider the following categories:
\newline
- Plaintiff win\newline
- Defendant win\newline
- Settlement\newline
- Case dismissal\newline
- Unsure\newline
If the verdict is mentioned, respond exclusively with the chosen
categories ONLY. If the outcome is not explicitly mentioned or cannot
be inferred from the information given, please respond with `unsure'
only.
\newline
Format your responses as follows:\newline
\# - For the summary, begin with `Answer 1:'\newline
\# - For the verdict, start with `Answer 2:'\newline
Here is the description of the case:\newline
[Case Description...]
\end{formal}

The output of this step includes a summarized case and a labeled verdict, formatted as follows:

\begin{equation}
\begin{aligned}
\text{Case Summary, Verdict} = \text{LLM}(&\text{Case Detail}, \\
& \text{Maximum Token} \mid d_i).
\end{aligned}
\end{equation}

For each legal case \(d_i\), we reorganize the data into a new format \(d_i'\), defined as:

\begin{equation}
\begin{aligned}
d_i' = \{& \text{Title, } \text{Date, } \text{Judge, } \text{Plaintiff(s), } \text{Plaintiff's Attorney(s), }\\ 
& \text{Defendant(s), }\text{Defendant's Attorney(s), } \\
& \text{Case Summary}\}.
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
D' = \left\{ \left( d_1', v_1' \right), \left( d_2', v_2' \right), \ldots, \left( d_n', v_n' \right) \right\}.
\end{aligned}
\end{equation}

There are some constraints when we separate the \( D' \) into training and testing data. We make sure that all legal cases have at least five precedent relationships. To ensure a balance training, the training dataset has 25\% from each of the following categories: plaintiff wins, defendant wins, settlements, and case dismissals. We also make sure that all testing legal cases have at least five precedent relationships connect to the training dataset, further explanation is given in Section \ref{data_splits} Data Splits.

\vspace{3pt}\noindent\textbf{Step 2.}\, After Step 1, all training legal cases \( d_i' \) are transformed into high-dimensional vectors using the OpenAI Embedding model. This vector database is later used to retrieve the top-$k$ similar cases based on semantic and contextual similarities.

\vspace{3pt}\noindent\textbf{Step 3.}\, This step involves converting the precedent case relationships from our training dataset into a knowledge graph (KG). Defined as $KG = (E, R, L)$, where $E$ represents entities, $R$ represents binary relationships (indicative of precedent relations), and $L \subseteq E \times R \times E$ represents the set of triples forming the graph's edges. Each triple $(e_s, r, e_t) \in L$ indicates a directed edge from source entity $e_s$ to target entity $e_t$ via relationship $r$. The KG data structure simplifies the complex task of identifying relevant precedent cases, turning it into a entity prediction problem, i.e., given a query of $(e_s, r, ?)$, the model will predict the missing entity.

We further customize data processing for SCR, PCR, and LJP tasks, ensuring a robust and effective implementation of \sysname.

\subsection{Similar Case Retrieval}

As depicted in the upper right of Figure \ref{fig:overflow}, the SCR process is divided into two phases: training (Steps 1-2) and testing (Steps 3-4).

\vspace{3pt}\noindent\textbf{Training Phase.}\, During training, each training case \(d_i' \) is inputted into the vector database, which generates the top 10 candidate cases. These cases are then randomized in order and formulated into the SCR training instruction. Here is an example SCR model input:

\begin{formal}
\#\#\# Instruction:\newline
You are a legal expert who specializes in comparing user-supplied legal cases to a list of candidate legal cases, which includes titles and content. Your main function is to identify and output the title of the most similar case from the list based on the description provided. \newline
You should only output the case title and not any other information. \newline
Consider the following choices:\newline Choice 1: \newline [Case 1...] \newline Choice 2: \newline ... \newline  Choice 10: \newline [Case 10...]\newline
\#\#\# Input:\newline
[Input Case...]
\end{formal}

In this scenario, the SCR task instruction will fall into the classification category, which provides the model with 10 cases to choose the most similar one. It is important to note that the top-0 similar case is the case $d_i'$ itself, so in practice, we retrieve the top-1 to top-10 similar cases from the vector database, and the top-1 case from this selection serves as the ground truth for this training task.

\vspace{3pt}\noindent\textbf{Testing Phase.}\, 
The testing phase mirrors the training process as we initially retrieve the top 10 similar cases from the vector database. However, during testing, we retrieve cases ranked from top-0 to top-9, as the test case itself is not included in the vector database. The model's expected response depends on the evaluation metrics we use: top-1, top-3, and top-5. For the top-1 metric, we expect \sysname to identify the most similar case as the top result. The top-3 metric evaluates whether the model's answer falls within the top three retrieved candidates, while the top-5 metric extends this evaluation to include the top five candidates. 

\subsection{Precedent Case Recommendation}

The Precedent Case Recommendation (PCR) within \sysname utilizes a unique approach by employing a precedent case knowledge graph (KG), which differentiates itself from conventional PCR methods that often speculate on potential precedent relationships. Our system instead relies on confirmed precedent pairs, as illustrated in the lower left of Figure \ref{fig:overflow}, where Steps 1 and 2 constitute the training phase and Steps 3-5 are the testing phase.

\vspace{3pt}\noindent\textbf{Training Phase.}\, From the previously established KG, for each confirmed triple $(e_s, r, e_t)$, we utilize BERT embeddings \cite{devlin2018bert} to evaluate the similarity between various case features (e.g., Judge, Case Detail, Plaintiff, or Defendant), denoted as \{\( F_1, F_2, ... F_j\)\}. We calculate the similarity score \( S_i \) for each feature pair \( F1_i \) and \( F2_i \), as follows:

\begin{equation}
\label{eq:1}
\begin{aligned}
S_{i} = \text{sim}(BERT(F1_{i}), BERT(F2_{i})), &\\ \quad i \in \{1-j\}
\end{aligned}
\end{equation}

The highest similarity score across all features determines the primary factor underlying their precedent relationship:

\begin{equation}
\label{eq:2}
\begin{aligned}
\text{Primary Factor} = \max(S_{1}, S_{2}, ..., S_{j}).
\end{aligned}
\end{equation}

During the training input creation, we present a total of 10 choices for the model. The ground truth precedent case $e_t$ is randomly placed among these choices, with the other 9 selections filled with similar, yet non-precedent, cases from the vector database. This setup aims to teach the model that textual similarity does not necessarily imply a precedent relationship. The model's expected output includes the correct precedent case $e_t$ and the reasoning for its selection (i.e, which primary factor caused this precedent relationship). An example of the model input is:

\begin{formal}
\#\#\# Instruction:\newline
You are a legal expert who specializes in comparing user-supplied legal cases to a list of candidate legal cases, which includes titles and content. Your main function is to identify and output the precedent case from the list based on the description provided. \newline
You should only output the reasoning process and case title. \newline
Consider the following choices:\newline Choice 1: \newline [Case 1...] \newline Choice 2: \newline ... \newline  Choice 10: \newline [Case 10...]\newline
\#\#\# Input:\newline
[Input Case...]
\end{formal}

\vspace{3pt}\noindent\textbf{Testing Phase.}\, For each test case, since we made sure there are at least five precedent cases in the training dataset, we can identify $k$ precedent cases from the KG (which structured by the training dataset) as ground truths, where $k$ aligns with the top-$k$ evaluation metrics. For the top-1 metric, a single ground truth precedent case is selected, while for top-3 and top-5 metrics, 3 and 5 ground truths are selected, respectively. The remaining slots of $10 - k$ are filled with similar cases. The model is then tasked with selecting one of $k$ precedent cases and explaining the reasoning behind its choice.

\subsection{Legal Judgment Prediction}

The Legal Judgment Prediction (LJP) processing utilizes the dataset \(D'\) constructed during the data preprocessing stage. This dataset pairs each processed legal case \(d_i'\) with its corresponding verdict \(v_i'\). As illustrated in the lower right of Figure \ref{fig:overflow}, the training phase involves step 1 and the testing phase involves rest of steps.

\vspace{3pt}\noindent\textbf{Training Phase.}\, We use $\left( d_i', v_i' \right)$ to establish a four-category classification training input, Plaintiff wins, Defendant wins, Settlement, or Case Dismissal. Each case's corresponding verdict \(v_i'\) serves as the label for training. Here is an example of the model input:

\begin{formal}
\#\#\# Instruction:\newline
You are a legal expert who specializes in predicting outcomes for legal cases. Utilize your internal knowledge base to predict verdict. Your main function is to anticipate the likely verdict of the legal case presented by the user.
\newline
You should only output the verdict and not any other information. \newline
Consider the following choices:\newline 1. Defendant Wins \newline 2. Plaintiff Wins \newline 3. Settlement \newline 4. Case Dismissal\newline
\#\#\# Input:\newline
[Input Case...]
\end{formal}

\vspace{3pt}\noindent\textbf{Testing Phase.}\, During the testing phase, we evaluate \sysname with both zero-shot and few-shot in-context learning (ICL) scenario. In few-shot ICL, we enhance each test case \(d_i'\) with additional contextual information, one similar case and one precedent case. Its precedent cases is sourced from our KG, and one is randomly selected to be included in the test input. Simultaneously, a most similar case is retrieved from the vector database. This approach ensures that the model's predictions are influenced by relevant legal precedents and similar case facts, thereby improving the accuracy and reliability of the judgment predictions.

\subsection{Unified Model Fine-Tuning}

Our methodology involves a unified fine-tuning strategy for the \sysname, leveraging a combined dataset with three tasks. This dataset, denoted as \(\text{Dataset}_{\text{combined}} = \text{LJP} \oplus \text{PCR} \oplus \text{SCR} \). We employ a cutting-edge 4-bit quantized Low-Rank Adaptation (LoRA) technique to instruction fine-tune the Gemma model. We use the cross-entropy loss function $L$ during the LoRA. It calculates the difference between the model's predicted token probabilities and the actual token probabilities in the expected output sequence. In the following equation, $n$ represents the length of the expected output sequence, $x$ represents the input instruction, and $y_i$ denotes the i-th token in the expected output sequence.

\begin{equation}
\label{eq:1}
L = -\sum_{i=1}^{n} \log P(y_i | x, y_1, y_2, ..., y_{i-1}).
\end{equation}

\section{Experiments}

In this section, we conduct experiments to evaluate the performance of \sysname on three tasks: Similar Case Retrieval (SCR), Precedent Case Recommendation (PCR), and Legal Judgment Prediction (LJP).

\subsection{Experimental Setup}

\vspace{3pt}\noindent\textbf{Datasets.}\,
We conduct our experiments on the CaseLaw dataset, initiated by Harvard Law School's Library Innovation Lab as part of the CaseLaw Project \cite{caselawproject}. This database encompasses a wide range of court cases from both state and federal in the United States. The project primarily focuses on democratizing access to American legal information, particularly through its Case Access Project (CAP), which is aimed at providing free and public access. The statistics of the CaseLaw dataset used in our experiments are shown in Table \ref{tab:dataset}.

\begin{table}[h]
\centering
\caption{Datasets Statistics}
\label{tab:dataset}
\begin{tabular}{ll}
\hline
DATASETS                         & CaseLaw \\ \hline
Language                         & English \\
\# State and Federal Totals      & 6,930,777 \\
\# Train case                    & 1,00,000  \\
\# Test case                     & 200,000   \\
Avg. length per case (words)     & 2695.38 \\
\hline
\end{tabular}
\end{table}

\vspace{3pt}\noindent\textbf{Evaluation Metrics.}\,
As previously mentioned, we employ top-$k$ metrics to evaluate the performance of the SCR and PCR tasks. Specifically, we use top-1, top-3, and top-5 metrics. These metrics measure the model's precision in identifying the correct response from a pool of 10 choices. For example, the top-1 metric requires the model to return the top choice as the answer. The top-3 and top-5 metrics provide more flexibility, allowing the correct answer to be anywhere within the top three or top five choices, respectively.

In addition to top-$k$ metrics, we evaluate the hallucination rate of models using a `not-found' metric. This metric tracks the proportion of responses that are entirely fabricated and do not match any of the 10 given choices. By measuring the `not-found' rate, we aim to understand how often models produce answers unrelated to the provided options, offering insight into their reliability.

For the LJP task, we employ accuracy and F1-score \cite{sai2022survey} metrics to gauge the model's performance. Accuracy calculates the proportion of correctly predicted verdicts across all cases, providing a direct measure of overall prediction performance. The F1 score ranging from 0 to 1, combines precision and recall into a single harmonic mean, offering a balanced evaluation of the model's effectiveness.

\vspace{3pt}\noindent\textbf{Data Splits.}\,
\label{data_splits}
As previously noted, our data are split according to three constraints. 
\begin{itemize}
    \item \textbf{Constraint 1:} For PCR, we employ top-$k$ evaluation metrics, which means each case has to have a minimum of five precedent cases, allowing us to identify $k$ ground truths.

    \item \textbf{Constraint 2:} We must ensure that when a test case is evaluated, its ground truth precedent case can be located within the knowledge graph formed by the training cases. Therefore, each test case must have at least five precedent cases present in the training data. 
    \item \textbf{Constraint 3:} To ensure balanced model training for Legal Judgment Prediction (LJP), the training data's verdict distribution should consist of 25\% for each possible outcome: plaintiff wins, defendant wins, settlements, and dismissals.

\end{itemize}

These approaches result in a total of 1,000,000 cases for training and 200,000 cases for testing.

\vspace{3pt}\noindent\textbf{Comparing Baselines.}\,
Our model is evaluated against advanced baselines including LLaMa2-7b \cite{touvron2023llama}, Gemma-7b \cite{team2023gemini}, Vicuna-13b \cite{zheng2024judging} and Guanaco-13b \cite{dettmers2024qlora}, alongside the larger and more advanced GPT-3.5 and GPT-4 models \cite{achiam2023gpt}. Each model undergoes the same testing phase to ensure a consistent and fair comparison of their multi-task capabilities within the legal domain.

\vspace{3pt}\noindent\textbf{Implementation Details.}\,
We conducted the training of our model over 10 epochs using an A40 GPU. To ensure compatibility, we monitored the input token size, capping it at 4096 tokens to align with Gemma's maximum token capacity. Additionally, we configured the model's dropout rate at 0.1 and set the learning rate to $2e^{-4}$.

\subsection{Similar Case Retrieval Results}

According to Table \ref{table:scr}, \sysname outperformed the baseline models in all categories. Specifically, it achieved the highest accuracy in top-1, top-3, and top-5 retrieval rates, with scores of 29.8\%, 63.2\%, and 81.6\% respectively. Remarkably, it also demonstrated minimal hallucination, as indicated by the not-found rate of 0.1\%.

\begin{table}[h]
\centering
\caption{SCR Test Results}
\begin{tabular}{lcccc}
\toprule
\midrule
Method          & top-1 $\uparrow$ & top-3 $\uparrow$ & top-5 $\uparrow$ & Not Found $\downarrow$ \\
\midrule
llama2-7b       & 0.083   & 0.197   & 0.309   & 0.406 \\
gemma-7b        & 0.181   & 0.428   & 0.536   & 0.121 \\
vicuna-13b      & 0.185   & 0.372   & 0.564   & 0.187 \\
guanaco-13b     & 0.077   & 0.214   & 0.375   & 0.372 \\
gpt3.5          & 0.219   & 0.579   & 0.691   & 0.148 \\
gpt4            & 0.274   & 0.526   & 0.708   & 0.005 \\
\rowcolor{gray!40} \sysname & \textbf{0.298}   & \textbf{0.632}   & \textbf{0.816}   & \textbf{0.001} \\
\midrule
\bottomrule
\label{table:scr}
\end{tabular}
\end{table}

Comparatively, GPT-4 showed strong performance with top-1, top-3, and top-5 accuracies of 27.5\%, 52.5\%, and 70.5\%, and a low not-found rate of 0.5\%. GPT-3.5 also performed well, especially in the top-3 and top-5 metrics. On the other hand, models like LLaMa2-7b and Guanaco-13b displayed higher not-found rates, indicating a tendency towards hallucination.

The results underscore the effectiveness of our \sysname model in accurately retrieving similar cases while minimizing the risk of generating irrelevant or nonexistent cases.

\subsection{Precedent Case Recommendation Results}

According to Table \ref{table:pcr}, the \sysname model again outperformed other baseline methods. It achieved the best results with a top-1 rate of 31.8\%, top-3 rate of 59.7\%, and top-5 rate of 83.2\%. Additionally, the \sysname model exhibited an low not-found rate of 0.1\%.

\begin{table}[h]
\caption{PCR Test Results}
\begin{tabular}{lcccc}
\toprule
\midrule
Method          & top-1 $\uparrow$ & top-3 $\uparrow$ & top-5 $\uparrow$ & Not Found $\downarrow$ \\
\midrule
llama2-7b        & 0.069   & 0.148   & 0.343   & 0.479 \\
gemma-7b         & 0.187   & 0.386   & 0.519   & 0.124 \\
vicuna-13b       & 0.175   & 0.352   & 0.506   & 0.203    \\
guanaco-13b      & 0.073   & 0.198   & 0.357   & 0.383    \\
gpt3.5           & 0.154   & 0.325   & 0.504   & 0.165 \\
gpt4             & 0.262   & 0.514   & 0.697   & 0.007 \\
\rowcolor{gray!40} \sysname & \textbf{0.318}   & \textbf{0.597}   & \textbf{0.832}   & \textbf{0.001} \\
\midrule
\bottomrule
\label{table:pcr}
\end{tabular}
\end{table}

Among the baseline models, GPT-4 was a strong performer, with high accuracy in top-1, top-3, and top-5 metrics, alongside a very low not-found rate, suggesting reliable and accurate recommendations. In contrast, models like LLaMa2-7b and Guanaco-13b showed higher not-found rates, highlighting challenges in providing relevant case recommendations. The overall results demonstrate the effectiveness of the \sysname model in PCR task, outstripping baseline models in both accuracy and reliability.

One notable insight from comparing SCR and PCR results is that most baseline models exhibited a performance drop in the PCR task compared to SCR. For instance, the GPT-4 model achieved scores of 27.4\%, 52.6\%, 70.8\%, 0.5\% in SCR top-$k$ and ``Not Found'' metrics, while in the PCR task, its scores dropped to 26.2\%, 51.4\%, 69.7\% and 0.7\%. This decline underscores the greater difficulty of identifying precedent cases compared to similar cases, as models cannot rely solely on textual similarity when determining precedent relationships. Instead, they must consider nuanced factors such as legal relevance. This performance difference reinforces the our previous assertion that precedent cases are distinct from similar cases, emphasizing the importance of distinguishing between the two concepts in the legal domain.

We conducted an analysis to identify the factors that are predominantly considered by \sysname when determining a precedent relationship under the top-1, top-3, and top-5 settings. This analysis involves comparing the frequency with which each factor is chosen as the primary determinant in our model against the ground truth (GT) distribution. As shown in Table \ref{table:factor}, the GT distribution is heavily weighted towards the `Case Detail' factor, with some toward other factors. In the top-1 scenario, where there is only one correct precedent case among nine similar cases, our model strongly focuses on the `Case Detail' factor. This bias likely stems from the GT distribution's heavy emphasis on `Case Detail,' leading our model to prioritize this factor, especially when faced with numerous similar cases that serve as potential distractions. However, as the pool of correct answers expands to three and five in the top-3 and top-5 scenarios respectively, \sysname begins to diversify its focus slightly to include other factors, although `Case Detail' continues to dominate. This trend indicates a move towards a more balanced approach in factor consideration as the number of correct choices increases, suggesting that \sysname adjusts its focus based on the availability of correct answers, while still reflecting the main emphasis observed in the ground truth data.

\begin{table}[h]
\caption{Primary Factor Percentage Comparison}
\begin{tabular}{lcccc}
\toprule
\midrule
Factor          & LawLLM    & LawLLM    & LawLLM    & GT   \\
                & top-1     & top-3     & top-5     &       \\
\midrule
Title           & 0.000   & 0.000   & 0.000   & 0.000    \\
Date            & 0.000   & 0.000   & 0.000   & 0.000     \\
Judge           & 0.027   & 0.054   & 0.116   & 0.149     \\
Plantiff(s)     & 0.002   & 0.009   & 0.013   & 0.027     \\
Defendent(s)    & 0.004   & 0.012   & 0.025   & 0.041     \\
Case Detail     & 0.967   & 0.925   & 0.846   & 0.783     \\
\midrule
\bottomrule
\label{table:factor}
\end{tabular}
\end{table}

\subsection{Legal Judgment Prediction Results}

As shown in Table \ref{table:ljp}, the \sysname surpasses all baseline methods in both zero-shot and few-shot scenarios for the LJP task. In the zero-shot scenario, LawLLM achieves an accuracy of 0.636 and an F1 score of 0.591, significantly outperforming the second best model, GPT-4, which scores 0.573 and 0.563 in accuracy and F1, respectively. In the few-shot scenario, LawLLM maintains its superior performance, reaching an accuracy of 0.794 and an F1 score of 0.758. These results show a considerable improvement over GPT-4, the closest competitor, which scores 0.732 in accuracy and 0.712 in F1.

Additionally, all models demonstrate higher performance in the few-shot in-context learning (ICL) scenario compared to the zero-shot setting. For instance, LLaMA2-7b shows an increase from 0.235 to 0.473 in accuracy, and from 0.239 to 0.455 in F1 score. This pattern indicates that all models benefit from incorporating a few ICL examples, which helps them better understand the task.

\begin{table}[h]
\centering
\caption{LJP Test Results}
\begin{tabular}{lcccc}
\toprule
\midrule
Method          & Accuracy $\uparrow$ & F1 $\uparrow$ & Accuracy $\uparrow$ & F1 $\uparrow$ \\
                & (Zero-shot) & (Zero-shot)  & (Few-shot) & (Few-shot) \\
\midrule
llama2-7b        & 0.235   & 0.239   & 0.473   & 0.455 \\
gemma-7b         & 0.317   & 0.287   & 0.568   & 0.527 \\
vicuna-13b       & 0.503   & 0.432   & 0.645   & 0.594    \\
guanaco-13b      & 0.281   & 0.247   & 0.491   & 0.463    \\
gpt3.5           & 0.558   & 0.546   & 0.679   & 0.647 \\
gpt4             & 0.573   & 0.563   & 0.732   & 0.712 \\
\rowcolor{gray!40} \sysname & \textbf{0.636}   & \textbf{0.591}   & \textbf{0.794}   & \textbf{0.758} \\
\midrule
\bottomrule
\label{table:ljp}
\end{tabular}
\end{table}

\section{Conclusions and Future Work}

In this study, we introduced the Law Large Language Model (\sysname), a multi-task LLM specifically designed for the US legal domain. By leveraging unique data processing techniques tailored for each task, \sysname effectively handles Similar Case Retrieval (SCR), Precedent Case Recommendation (PCR), and Legal Judgment Prediction (LJP). Furthermore, we emphasized the crucial distinctions between precedent relationships and textual similarity, providing insights that can inform future research in developing task-specific models. Our results consistently demonstrated that \sysname outperforms existing baseline models, showcasing its superior multi-task capabilities.

In the future, we aim to expand the scope of \sysname by incorporating additional legal tasks to further enhance its versatility and practical applicability. This will involve exploring emerging challenges in legal analytics and integrating new datasets that reflect diverse legal contexts. Moreover, we plan to refine our data processing techniques and in-context learning methodologies to improve the model's understanding of legal nuances and precedents. 

\balance

\newpage

\end{document}