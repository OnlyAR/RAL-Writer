\title{}

\begin{document}

\title{\vspace{-3mm}\LARGE Segment Anything\vspace{-6mm}}
\author{
{\normalsize Alexander Kirillov$^{1,2,4}$ \quad Eric Mintun$^{2}$ \quad Nikhila Ravi$^{1,2}$
 \quad Hanzi Mao$^{2}$ \quad Chloe Rolland$^{3}$ \quad Laura Gustafson$^{3}$}\\[0mm]
{\normalsize Tete Xiao$^{3}$ \hspace{4.65mm} Spencer Whitehead \hspace{4.65mm} Alexander C. Berg \hspace{4.65mm}
Wan-Yen Lo \hspace{4.65mm} Piotr Doll\'{a}r$^{4}$ \hspace{4.65mm} Ross Girshick$^{4}$}\\[0mm]
 {\small$^1$project lead \qquad $^2$joint first author \qquad $^3$equal contribution \qquad $^4$directional lead}\\[1mm]
{Meta AI Research, FAIR}\vspace{-4mm}}

\twocolumn[{
\maketitle\centering
\captionsetup{type=figure}
\includegraphics[width=0.99\textwidth]{figs/teaser.pdf}\vspace{-2mm}
\captionof{figure}{We aim to build a foundation model for segmentation by introducing three interconnected components: a promptable segmentation \emph{task}, a segmentation \emph{model} (\sam) that powers data annotation and enables zero-shot transfer to a range of tasks via prompt engineering, and a \emph{data} engine for collecting \sad, our dataset of over 1 billion masks.}
\label{fig:teaser}\vspace{5mm}
}]

\maketitle

\begin{abstract}\vspace{-3mm}
We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 \textbf{billion} masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (\sam) and corresponding dataset (\sad) of 1B masks and 11M images at \href{https://segment-anything.com}{https://segment-anything.com} to foster research into foundation models for computer vision.
\vspace{-3mm}
\end{abstract}

\section{Introduction}\label{sec:intro}

Large language models pre-trained on web-scale datasets are revolutionizing NLP with strong zero-shot and few-shot generalization~\cite{Brown2020}. These ``foundation models''~\cite{bommasani2021opportunities} can generalize to tasks and data distributions beyond those seen during training. This capability is often implemented with \emph{prompt engineering} in which hand-crafted text is used to prompt the language model to generate a valid textual response for the task at hand. When scaled and trained with abundant text corpora from the web, these models' zero and few-shot performance compares surprisingly well to (even matching in some cases) fine-tuned models~\cite{Brown2020,chowdhery2022palm}. Empirical trends show this behavior improving with model scale, dataset size, and total training compute~\cite{kaplan2020scaling,Brown2020,chowdhery2022palm,hoffmann2022training}.

Foundation models have also been explored in computer vision, albeit to a lesser extent. Perhaps the most prominent illustration aligns paired text and images from the web. For example, CLIP~\cite{Radford2021} and ALIGN~\cite{jia2021scaling} use contrastive learning to train text and image encoders that align the two modalities. Once trained, engineered text prompts enable zero-shot generalization to novel visual concepts and data distributions. Such encoders also compose effectively with other modules to enable downstream tasks, such as image generation (\eg, DALLÂ·E~\cite{Ramesh2021}). While much progress has been made on vision and language encoders, computer vision includes a wide range of problems beyond this scope, and for many of these, abundant training data does not exist.

In this work, our goal is to build \emph{a foundation model for image segmentation}. That is, we seek to develop a promptable model and pre-train it on a broad dataset using a task that enables powerful generalization. With this model, we aim to solve a range of downstream segmentation problems on new data distributions using prompt engineering.

The success of this plan hinges on three components: \textbf{task}, \textbf{model}, and \textbf{data}. To develop them, we address the following questions about image segmentation:
\begin{enumerate}[itemsep=-0.6mm]
\item What \textbf{task} will enable zero-shot generalization?
\item What is the corresponding \textbf{model} architecture?
\item What \textbf{data} can power this task and model?
\end{enumerate}
These questions are entangled and require a comprehensive solution. We start by defining a \emph{promptable segmentation} \textbf{task} that is general enough to provide a powerful pre-training objective and to enable a wide range of downstream applications. This task requires a \textbf{model} that supports flexible prompting and can output segmentation masks in real-time when prompted to allow for interactive use. To train our model, we need a diverse, large-scale source of \textbf{data}. Unfortunately, there is no web-scale data source for segmentation; to address this, we build a ``data engine'', \ie, we iterate between using our efficient model to assist in data collection and using the newly collected data to improve the model. We introduce each interconnected component next, followed by the dataset we created and the experiments that demonstrate the effectiveness of our approach.

\paragraph{Task (\S\ref{sec:task}).} In NLP and more recently computer vision, foundation models are a promising development that can perform zero-shot and few-shot learning for new datasets and tasks often by using ``prompting'' techniques. Inspired by this line of work, we propose the \emph{promptable segmentation task}, where the goal is to return a \emph{valid} segmentation mask given any segmentation \emph{prompt} (see~\fig{fig:teaser}{\color{linkcolor}a}). A prompt simply specifies what to segment in an image, \eg, a prompt can include spatial or text information identifying an object. The requirement of a valid output mask means that even when a prompt is ambiguous and could refer to multiple objects (for example, a point on a shirt may indicate either the shirt or the person wearing it), the output should be a reasonable mask for at least one of those objects. We use the promptable segmentation task as both a pre-training objective and to solve general downstream segmentation tasks via prompt engineering.

\paragraph{Model (\S\ref{sec:model}).} The promptable segmentation task and the goal of real-world use impose constraints on the model architecture. In particular, the model must support \emph{flexible prompts}, needs to compute masks in amortized \emph{real-time} to allow interactive use, and must be \emph{ambiguity-aware}. Surprisingly, we find that a simple design satisfies all three constraints: a powerful image encoder computes an image embedding, a prompt encoder embeds prompts, and then the two information sources are combined in a lightweight mask decoder that predicts segmentation masks. We refer to this model as the Segment Anything Model, or \sam (see~\fig{fig:teaser}{\color{linkcolor}b}). By separating \sam into an image encoder and a fast prompt encoder / mask decoder, the same image embedding can be reused (and its cost amortized) with different prompts. Given an image embedding, the prompt encoder and mask decoder predict a mask from a prompt in $\app$50ms in a web browser. We focus on point, box, and mask prompts, and also present initial results with free-form text prompts. To make \sam ambiguity-aware, we design it to predict multiple masks for a single prompt allowing \sam to naturally handle ambiguity, such as the shirt \vs person example.

\paragraph{Data engine (\S\ref{sec:engine}).} To achieve strong generalization to new data distributions, we found it necessary to train \sam on a large and diverse set of masks, beyond any segmentation dataset that already exists. While a typical approach for foundation models is to obtain data online~\cite{Radford2021}, masks are not naturally abundant and thus we need an alternative strategy. Our solution is to build a ``data engine'', \ie, we co-develop our model with model-in-the-loop dataset annotation (see~\fig{fig:teaser}{\color{linkcolor}c}). Our data engine has three stages: \emph{assisted-manual}, \emph{semi-automatic}, and \emph{fully automatic}. In the first stage, \sam assists annotators in annotating masks, similar to a classic interactive segmentation setup. In the second stage, \sam can automatically generate masks for a subset of objects by prompting it with likely object locations and annotators focus on annotating the remaining objects, helping increase mask diversity. In the final stage, we prompt \sam with a regular grid of foreground points, yielding on average \app100 high-quality masks per image.

\paragraph{Dataset (\S\ref{sec:dataset}).} Our final dataset, \sad, includes more than \emph{1B} masks from \emph{11M} licensed and privacy-preserving images (see \fig{fig:sa1bvisuals}). \sad, collected fully automatically using the final stage of our data engine, has 400$\x$ more masks than any existing segmentation dataset~\cite{Lin2014,Gupta2019,Zhou2019,OpenImages}, and as we verify extensively, the masks are of high quality and diversity. Beyond its use in training \sam to be robust and general, we hope \sad becomes a valuable resource for research aiming to build new foundation models.

\paragraph{Responsible AI (\S\ref{sec:rai}).} We study and report on potential fairness concerns and biases when using \sad and \sam. Images in \sad span a geographically and economically diverse set of countries and we found that \sam performs similarly across different groups of people. Together, we hope this will make our work more equitable for real-world use cases. We provide model and dataset cards in the \supp.

\paragraph{Experiments (\S\ref{sec:eval}).} We extensively evaluate \sam. First, using a diverse new suite of 23 segmentation datasets, we find that \sam produces high-quality masks from a single foreground point, often only slightly below that of the manually annotated ground truth. Second, we find consistently strong quantitative and qualitative results on a variety of downstream tasks under a zero-shot transfer protocol using prompt engineering, including edge detection, object proposal generation, instance segmentation, and a preliminary exploration of text-to-mask prediction. These results suggest that \sam can be used out-of-the-box with prompt engineering to solve a variety of tasks involving object and image distributions beyond \sam's training data. Nevertheless, room for improvement remains, as we discuss in \S\ref{sec:disc}.

\paragraph{Release.} We are releasing the \sad dataset for research purposes and making \sam available under a permissive open license (Apache 2.0) at \href{https://segment-anything.com}{https://segment-anything.com}. We also showcase \sam's capabilities with an \href{https://segment-anything.com/demo}{online demo}.

\newcommand{\incvisualsrow}[6]{
\begin{subfigure}{\linewidth}\begin{adjustbox}{width=\textwidth}{
\def\arraystretch{0.2}\setlength\tabcolsep{#1pt}\begin{tabular}{ccccc}
\raisebox{1.2\normalbaselineskip}[0pt][0pt]{\rotatebox{90}{\tiny #2}} &
\includegraphics[height=2cm]{figs/sa1b_examples/#3.jpg} &
\includegraphics[height=2cm]{figs/sa1b_examples/#4.jpg} &
\includegraphics[height=2cm]{figs/sa1b_examples/#5.jpg} &
\includegraphics[height=2cm]{figs/sa1b_examples/#6.jpg} \\
\end{tabular}}\end{adjustbox}\end{subfigure}}

\begin{figure*}[!t]
\centering
\incvisualsrow{0.5}{$<$50 masks}{9_sa_1192782}{25_sa_864082}{32_sa_8234897}{45_sa_4298678}
\incvisualsrow{1.0}{50-100 masks}{67_sa_1468983}{61_sa_2146330}{66_sa_9307564}{65_sa_2889438}
\incvisualsrow{0.8}{100-200 masks}{187_sa_7694512}{145_sa_3647402}{116_sa_6137832}{154_sa_2945741}
\incvisualsrow{0.8}{200-300 masks}{208_sa_6808005}{230_sa_6506882}{250_sa_9338205}{221_sa_9820072}
\incvisualsrow{0.6}{300-400 masks}{360_sa_5537747}{318_sa_7769360}{304_sa_1437195}{317_sa_7651290}
\incvisualsrow{0.7}{400-500 masks}{456_sa_11107398}{452_sa_10129735}{438_sa_1596486}{401_sa_10771020}
\incvisualsrow{0.3}{$>$500 masks}{783_sa_3805502}{823_sa_11048476}{576_sa_10463977}{579_sa_1232910}
\caption{Example images with overlaid masks from our newly introduced dataset, \textbf{\sad}. \sad contains 11M diverse, high-resolution, licensed, and privacy protecting images and 1.1B high-quality segmentation masks. These masks were annotated \emph{fully automatically} by \sam, and as we verify by human ratings and numerous experiments, are of high quality and diversity. We group images by number of masks per image for visualization (there are \app100 masks per image on average).}
\label{fig:sa1bvisuals}
\end{figure*}

\section{Segment Anything Task}\label{sec:task}

We take inspiration from NLP, where the next token prediction task is used for foundation model pre-training \emph{and} to solve diverse downstream tasks via prompt engineering~\cite{Brown2020}. To build a foundation model for segmentation, we aim to define a task with analogous capabilities.

\paragraph{Task.} We start by translating the idea of a prompt from NLP to segmentation, where a prompt can be a set of foreground / background points, a rough box or mask, free-form text, or, in general, any information indicating what to segment in an image. The \emph{promptable segmentation task}, then, is to return a \emph{valid} segmentation mask given any \emph{prompt}. The requirement of a ``valid'' mask simply means that even when a prompt is \emph{ambiguous} and could refer to multiple objects (\eg, recall the shirt \vs person example, and see \fig{fig:ambiguity_examples}), the output should be a reasonable mask for at least \emph{one} of those objects. This requirement is similar to expecting a language model to output a coherent response to an ambiguous prompt. We choose this task because it leads to a natural pre-training algorithm \emph{and} a general method for zero-shot transfer to downstream segmentation tasks via prompting.

\paragraph{Pre-training.} The promptable segmentation task suggests a natural pre-training algorithm that simulates a sequence of prompts (\eg, points, boxes, masks) for each training sample and compares the model's mask predictions against the ground truth. We adapt this method from interactive segmentation~\cite{xu2016deep,mahadevan2018iteratively}, although unlike interactive segmentation whose aim is to eventually predict a valid mask after enough user input, our aim is to always predict a \emph{valid mask} for \emph{any prompt} even when the prompt is \emph{ambiguous}. This ensures that a pre-trained model is effective in use cases that involve ambiguity, including automatic annotation as required by our data engine \S\ref{sec:engine}. We note that performing well at this task is challenging and requires specialized modeling and training loss choices, which we discuss in \S\ref{sec:model}.

\paragraph{Zero-shot transfer.} Intuitively, our pre-training task endows the model with the ability to respond appropriately to any prompt at inference time, and thus downstream tasks can be solved by engineering appropriate prompts. For example, if one has a bounding box detector for cats, cat instance segmentation can be solved by providing the detector's box output as a prompt to our model. In general, a wide array of practical segmentation tasks can be cast as prompting. In addition to automatic dataset labeling, we explore five diverse example tasks in our experiments in \S\ref{sec:eval}.

\begin{figure}[t]\centering
\includegraphics[width=.97\linewidth]{figs/ambiguity_examples.pdf}\vspace{-1mm}
\caption{Each column shows 3 valid masks generated by \sam from a single ambiguous point prompt (green circle).}
\label{fig:ambiguity_examples}\vspace{-1mm}
\end{figure}

\paragraph{Related tasks.} Segmentation is a broad field: there's interactive segmentation~\cite{kass1988snakes,xu2016deep}, edge detection~\cite{arbelaez2010contour}, super pixelization~\cite{ren2003learning}, object proposal generation~\cite{alexe2010object}, foreground segmentation~\cite{stauffer1999adaptive}, semantic segmentation~\cite{shotton2006textonboost}, instance segmentation~\cite{Lin2014}, panoptic segmentation~\cite{kirillov2019panoptic}, \etc. The goal of our promptable segmentation task is to produce a broadly capable model that can adapt to \emph{many} (though not all) existing and \emph{new} segmentation tasks via prompt engineering. This capability is a form of task generalization~\cite{da2012learning}. Note that this is different than previous work on multi-task segmentation systems. In a multi-task system, a single model performs a \emph{fixed} set of tasks, \eg, joint semantic, instance, and panoptic segmentation~\cite{zhang2021knet,cheng2022masked,jain2022oneformer}, but the training and test tasks are the same. An important distinction in our work is that a model trained for promptable segmentation can perform a new, different task at inference time by acting as a \emph{component} in a larger system, \eg, to perform instance segmentation, a promptable segmentation model is \emph{combined} with an existing object detector.

\paragraph{Discussion.} Prompting and composition are powerful tools that enable a single model to be used in extensible ways, potentially to accomplish tasks unknown at the time of model design. This approach is analogous to how other foundation models are used, \eg, how CLIP~\cite{Radford2021} is the text-image alignment component of the DALL$\cdot$E~\cite{Ramesh2021} image generation system. We anticipate that composable system design, powered by techniques such as prompt engineering, will enable a wider variety of applications than systems trained specifically for a fixed set of tasks. It's also interesting to compare promptable and interactive segmentation through the lens of composition: while interactive segmentation models are designed with human users in mind, a model trained for promptable segmentation can also be composed into a larger algorithmic system as we will demonstrate.

\begin{figure*}[t]\centering
\includegraphics[width=0.99\linewidth]{figs/model_diagram.pdf}\vspace{-1mm}
\caption{Segment Anything Model (\sam) overview. A heavyweight image encoder outputs an image embedding that can then be efficiently queried by a variety of input prompts to produce object masks at amortized real-time speed. For ambiguous prompts corresponding to more than one object, \sam can output multiple valid masks and associated confidence scores.}
\label{fig:model_diagram}
\vspace{-1mm}
\end{figure*}

\section{Segment Anything Model}\label{sec:model}

We next describe the Segment Anything Model (\sam) for promptable segmentation. \sam has three components, illustrated in~\fig{fig:model_diagram}: an image encoder, a flexible prompt encoder, and a fast mask decoder. We build on Transformer vision models~\cite{Carion2020,Dosovitskiy2021,cheng2021per,li2022exploring} with specific tradeoffs for (amortized) real-time performance. We describe these components at a high-level here, with details in \S\ref{app:model}.

\paragraph{Image encoder.} Motivated by scalability and powerful pre-training methods, we use an MAE~\cite{he2022masked} pre-trained Vision Transformer (ViT)~\cite{Dosovitskiy2021} minimally adapted to process high resolution inputs~\cite{li2022exploring}. The image encoder runs once per image and can be applied prior to prompting the model.

\paragraph{Prompt encoder.} We consider two sets of prompts: \emph{sparse} (points, boxes, text) and \emph{dense} (masks). We represent points and boxes by positional encodings~\cite{tancik2020fourier} summed with learned embeddings for each prompt type and free-form text with an off-the-shelf text encoder from CLIP~\cite{Radford2021}. Dense prompts (\ie, masks) are embedded using convolutions and summed element-wise with the image embedding.

\paragraph{Mask decoder.} The mask decoder efficiently maps the image embedding, prompt embeddings, and an output token to a mask. This design, inspired by~\cite{Carion2020,cheng2021per}, employs a modification of a Transformer decoder block~\cite{Vaswani2017} followed by a dynamic mask prediction head. Our modified decoder block uses prompt self-attention and cross-attention in two directions (prompt-to-image embedding and vice-versa) to update \emph{all} embeddings. After running two blocks, we upsample the image embedding and an MLP maps the output token to a dynamic linear classifier, which then computes the mask foreground probability at each image location.

\paragraph{Resolving ambiguity.} With one output, the model will average multiple valid masks if given an ambiguous prompt. To address this, we modify the model to predict multiple output masks for a single prompt (see \fig{fig:ambiguity_examples}). We found 3 mask outputs is sufficient to address most common cases (nested masks are often at most three deep: whole, part, and subpart). During training, we backprop only the minimum loss~\cite{charpiat2008automatic,guzman2012multiple,li2018interactive} over masks. To rank masks, the model predicts a confidence score (\ie, estimated IoU) for each mask.

\paragraph{Efficiency.} The overall model design is largely motivated by efficiency. Given a precomputed image embedding, the prompt encoder and mask decoder run in a web browser, on CPU, in \app50ms. This runtime performance enables seamless, real-time interactive prompting of our model.

\paragraph{Losses and training.} We supervise mask prediction with the linear combination of focal loss~\cite{Lin2017a} and dice loss~\cite{milletari2016v} used in~\cite{Carion2020}. We train for the promptable segmentation task using a mixture of geometric prompts (for text prompts see \S\ref{sec:eval:text_to_mask}). Following~\cite{sofiiuk2022reviving,forte2020getting}, we simulate an interactive setup by randomly sampling prompts in 11 rounds per mask, allowing \sam to integrate seamlessly into our data engine.

\section{Segment Anything Data Engine}\label{sec:engine}

As segmentation masks are not abundant on the internet, we built a data engine to enable the collection of our 1.1B mask dataset, \sad. The data engine has three stages: (1) a model-assisted manual annotation stage, (2) a semi-automatic stage with a mix of automatically predicted masks and model-assisted annotation, and (3) a fully automatic stage in which our model generates masks without annotator input. We go into details of each next.

\paragraph{Assisted-manual stage.} In the first stage, resembling classic interactive segmentation, a team of professional annotators labeled masks by clicking foreground / background object points using a browser-based interactive segmentation tool powered by \sam. Masks could be refined using pixel-precise ``brush'' and ``eraser'' tools. Our model-assisted annotation runs in real-time directly inside a browser (using precomputed image embeddings) enabling a truly interactive experience. We did not impose semantic constraints for labeling objects, and annotators freely labeled both ``stuff'' and ``things''~\cite{adelson2001seeing}. We suggested annotators label objects they could name or describe, but did not collect these names or descriptions. Annotators were asked to label objects in order of prominence and were encouraged to proceed to the next image once a mask took over 30 seconds to annotate.

At the start of this stage, \sam was trained using common public segmentation datasets. After sufficient data annotation, \sam was retrained using only newly annotated masks. As more masks were collected, the image encoder was scaled from ViT-B to ViT-H and other architectural details evolved; in total we retrained our model 6 times. Average annotation time per mask decreased from 34 to 14 seconds as the model improved. We note that 14 seconds is 6.5$\x$ faster than mask annotation for COCO~\cite{Lin2014} and only 2$\x$ slower than bounding-box labeling with extreme points~\cite{papadopoulos2017extreme,maninis2018deep}. As \sam improved, the average number of masks per image increased from 20 to 44 masks. Overall, we collected 4.3M masks from 120k images in this stage.

\paragraph{Semi-automatic stage.} In this stage, we aimed to increase the \emph{diversity} of masks in order to improve our model's ability to segment anything. To focus annotators on less prominent objects, we first automatically detected confident masks. Then we presented annotators with images prefilled with these masks and asked them to annotate any additional unannotated objects. To detect confident masks, we trained a bounding box detector~\cite{Ren2015} on all first stage masks using a generic ``object'' category. During this stage we collected an additional 5.9M masks in 180k images (for a total of 10.2M masks). As in the first stage, we periodically retrained our model on newly collected data (5 times). Average annotation time per mask went back up to 34 seconds (excluding the automatic masks) as these objects were more challenging to label. The average number of masks per image went from 44 to 72 masks (including the automatic masks).

\paragraph{Fully automatic stage.} In the final stage, annotation was \emph{fully automatic}. This was feasible due to two major enhancements to our model. First, at the start of this stage, we had collected enough masks to greatly improve the model, including the diverse masks from the previous stage. Second, by this stage we had developed the ambiguity-aware model, which allowed us to predict valid masks even in ambiguous cases. Specifically, we prompted the model with a 32$\x$32 regular grid of points and for each point predicted a set of masks that may correspond to valid objects. With the ambiguity-aware model, if a point lies on a part or subpart, our model will return the subpart, part, and whole object. The IoU prediction module of our model is used to select \emph{confident} masks; moreover, we identified and selected only \emph{stable} masks (we consider a mask stable if thresholding the probability map at $0.5-\delta$ and $0.5+\delta$ results in similar masks). Finally, after selecting the confident and stable masks, we applied non-maximal suppression (NMS) to filter duplicates. To further improve the quality of smaller masks, we also processed multiple overlapping zoomed-in image crops. For further details of this stage, see \S\ref{app:dataset_generation}. We applied fully automatic mask generation to all 11M images in our dataset, producing a total of 1.1B high-quality masks. We describe and analyze the resulting dataset, \sad, next.

\begin{figure}[t]\centering
\includegraphics[width=.99\linewidth]{figs/center_distribution.png}\vspace{-3mm}
\caption{Image-size normalized mask center distributions.}
\label{fig:analysis:center_distribution}\vspace{-2mm}
\end{figure}

\begin{figure*}[t]\centering\vspace{-1mm}
\includegraphics[width=0.88\linewidth]{figs/dataset_legend.pdf}\\[-2mm]
\includegraphics[width=0.32\linewidth]{figs/dataset_percent_mask_count.pdf}
\includegraphics[width=0.32\linewidth]{figs/dataset_area_distribution.pdf}
\includegraphics[width=0.32\linewidth]{figs/dataset_concavity_distribution_stratified.pdf}
\vspace{-3mm}
\caption{Dataset mask properties. The legend references the number of images and masks in each dataset. Note, that \sad has 11$\x$ more images and 400$\x$ more masks than the largest existing segmentation dataset Open Images~\cite{OpenImages}.}
\label{fig:analysis}\vspace{-3mm}
\end{figure*}

\section{Segment Anything Dataset}\label{sec:dataset}

Our dataset, \sad, consists of 11M diverse, high-resolution, licensed, and privacy protecting images and 1.1B high-quality segmentation masks collected with our data engine. We compare \sad with existing datasets and analyze mask quality and properties. We are releasing \sad to aid future development of foundation models for computer vision. We note that \sad will be released under a favorable license agreement for certain research uses and with protections for researchers.

\paragraph{Images}. We licensed a new set of 11M images from a provider that works directly with photographers. These images are high resolution (3300$\x$4950 pixels on average), and the resulting data size can present accessibility and storage challenges. Therefore, we are releasing downsampled images with their shortest side set to 1500 pixels. Even after downsampling, our images are significantly higher resolution than many existing vision datasets (\eg, COCO~\cite{Lin2014} images are \app480$\x$640 pixels). Note that most models today operate on much lower resolution inputs. Faces and vehicle license plates have been blurred in the released images.

\paragraph{Masks}. Our data engine produced 1.1B masks, 99.1\% of which were generated fully automatically. Therefore, the quality of the automatic masks is centrally important. We compare them directly to professional annotations and look at how various mask properties compare to prominent segmentation datasets. Our main conclusion, as borne out in the analysis below and the experiments in \S\ref{sec:eval}, is that our automatic masks are high quality and effective for training models. Motivated by these findings, \sad \emph{only includes automatically generated masks.}

\paragraph{Mask quality.} To estimate mask quality, we randomly sampled 500 images ($\app$50k masks) and asked our professional annotators to improve the quality of all masks in these images. Annotators did so using our model and pixel-precise ``brush'' and ``eraser'' editing tools. This procedure resulted in pairs of automatically predicted and professionally corrected masks. We computed IoU between each pair and found that 94\% of pairs have greater than 90\% IoU (and 97\% of pairs have greater than 75\% IoU). For comparison, prior work estimates inter-annotator consistency at 85-91\% IoU~\cite{Gupta2019,OpenImages}. Our experiments in \S\ref{sec:eval} confirm by human ratings that mask quality is high relative to a variety of datasets and that training our model on automatic masks is nearly as good as using all masks produced by the data engine.

\begin{figure*}[t]\centering
\includegraphics[width=0.44\linewidth]{figs/ablation_sa1b_geography.pdf}\hfill
\includegraphics[width=0.54\linewidth]{figs/ablation_countries_barplot.pdf}
\vspace{-3mm}
\caption{Estimated geographic distribution of \sad images. Most of the world's countries have more than 1000 images in \sad, and the three countries with the most images are from different parts of the world.}
\label{fig:geo_distribution}\vspace{-4mm}
\end{figure*}

\paragraph{Mask properties.} In \fig{fig:analysis:center_distribution} we plot the spatial distribution of object centers in \sad compared to the largest existing segmentation datasets. Common photographer biases are present in all datasets. We observe that \sad has greater coverage of image corners compared to LVIS v1~\cite{Gupta2019} and ADE20K~\cite{Zhou2019}, the two most similarly distributed datasets, while COCO~\cite{Lin2014} and Open Images V5~\cite{OpenImages} have a more prominent center bias. In \fig{fig:analysis} (legend) we compare these datasets by size. \sad has 11$\x$ more images and 400$\x$ more masks than the second largest, Open Images. On average, it has 36$\x$ more masks per image than Open Images. The closest dataset in this respect, ADE20K, still has 3.5$\x$ fewer masks per image. \fig{fig:analysis} (left) plots the masks-per-image distribution. Next, we look at image-relative mask size (square root of the mask area divided by image area) in \fig{fig:analysis} (middle). As expected, since our dataset has more masks per image, it also tends to include a greater percentage of small and medium relative-size masks. Finally, to analyze shape complexity, we look at mask concavity (1 minus mask area divided by area of mask's convex hull) in \fig{fig:analysis} (right). Since shape complexity is correlated with mask size, we control for the datasets' mask size distributions by first performing stratified sampling from binned mask sizes. We observe that the concavity distribution of our masks is broadly similar to that of other datasets.

\section{Segment Anything RAI Analysis}\label{sec:rai}

We next perform a Responsible AI (RAI) analysis of our work by investigating potential fairness concerns and biases when using \sad and \sam. We focus on the geographic and income distribution of \sad and fairness of \sam across protected attributes of people. We also provide dataset, data annotation, and model cards in \S\ref{app:cards}.

\begin{table}[t]\centering
\resizebox{\linewidth}{!}{
\tablestyle{3pt}{1.2}\begin{tabular}{@{}lr|rr|rrr@{}}
& & \multicolumn{2}{c|}{\sad} & \multicolumn{3}{c}{\% images} \\
& \hspace{-9mm} \# countries & \#imgs & \#masks & \sad & COCO & O.I. \\
\hline
\scriptsize Africa & 54 & 300k & 28M & 2.8\% & 3.0\% & 1.7\% \\
\scriptsize Asia \& Oceania & 70 & 3.9M & 423M & 36.2\% & 11.4\% & 14.3\% \\
\scriptsize Europe & 47 & 5.4M & 540M & 49.8\% & 34.2\% & 36.2\% \\
\scriptsize Latin America \& Carib. & 42 & 380k & 36M & 3.5\% & 3.1\% & 5.0\% \\
\scriptsize North America & 4 & 830k & 80M & 7.7\% & 48.3\% & 42.8\% \\
\hline
\scriptsize high income countries & 81 & 5.8M & 598M & 54.0\% & 89.1\% & 87.5\% \\
\scriptsize middle income countries & 108 & 4.9M & 499M & 45.0\% & 10.5\% & 12.0\% \\
\scriptsize low income countries & 28 & 100k & 9.4M & 0.9\% & 0.4\% & 0.5\% \\
\end{tabular}}\vspace{-2mm}
\caption{Comparison of geographic and income representation. \sad has higher representation in Europe and Asia \& Oceania as well as middle income countries. Images from Africa, Latin America \& Caribbean, as well as low income countries, are underrepresented in all datasets.}
\label{tab:region}\vspace{0mm}
\end{table}

\paragraph{Geographic and income representation.} We infer the country images were photographed in using standard methods (see \S\ref{app:rai}). In \fig{fig:geo_distribution} we visualize the per-country image counts in \sad (left) and the 50 countries with the most images (right). We note that the top-three countries are from different parts of the world. Next, in Table~\ref{tab:region} we compare the geographic and income representation of \sad, COCO~\cite{Lin2014}, and Open Images~\cite{OpenImages}. \sad has a substantially higher percentage of images in Europe and Asia \& Oceania as well as in middle income countries. All datasets underrepresent Africa as well as low income countries. We note that in \sad, all regions, including Africa, have at least 28 million masks, 10$\x$ more than the \emph{total} number of masks of any previous dataset. Finally, we observe that the average number of masks per image (not shown) is fairly consistent across region and income (94-108 per image).

\paragraph{Fairness in segmenting people.} We investigate potential fairness concerns across perceived gender presentation, perceived age group, and perceived skin tone by measuring the performance discrepancy of \sam between groups. We use the More Inclusive Annotations for People (MIAP)~\cite{schumann2021step} dataset for gender presentation and age and a proprietary dataset for skin tone (see \S\ref{app:rai}). Our evaluation uses simulated interactive segmentation with random sampling of 1 and 3 points (see \S\ref{app:experimental_design}). Table \ref{tab:rai_person} (top left) shows results for perceived gender presentation. We note that females have been shown to be underrepresented in detection and segmentation datasets~\cite{zhao2017}, but observe that \sam performs similarly across groups. We repeat the analysis for perceived age in Table~\ref{tab:rai_person} (bottom left), noting that those who are perceived to be younger and older have been shown to be underrepresented in large-scale datasets~\cite{yang2020towards}. \sam performs best on those who are perceived older (although the confidence interval is large). Finally, we repeat the analysis for perceived skin tone in Table~\ref{tab:rai_person} (right), noting that those with lighter apparent skin tones have been shown to be overrepresented and those with darker skin tones underrepresented in large-scale datasets~\cite{yang2020towards}. As MIAP does not contain perceived skin tone annotations, we use a proprietary dataset that contains annotations for the perceived Fitzpatrick skin type~\cite{fitzpatrick1988}, which ranges from 1 (lightest skin tone) to 6 (darkest skin tone). While the means vary somewhat, we do not find a significant difference across groups. We believe our findings stem from the nature of the task, and acknowledge biases may arise when \sam is used as a component in larger systems. Finally, in \S\ref{app:rai} we extend the analysis to segmenting clothing where we find an indication of bias across perceived gender presentation.

\begin{table}[t]\centering
\resizebox{!}{19mm}{
\tablestyle{4pt}{1.1}\begin{tabular}{@{}lcc@{}}
{} & \multicolumn{2}{c}{mIoU at} \\
{} & 1 point & 3 points \\
\hline
\multicolumn{3}{@{}l}{\emph{perceived gender presentation}} \\
feminine & 54.4\mypm{1.7} & 90.4\mypm{0.6} \\
masculine & 55.7\mypm{1.7} & 90.1\mypm{0.6} \\
\hline
\multicolumn{3}{@{}l}{\emph{perceived age group}} \\
older & 62.9\mypm{6.7} & 92.6\mypm{1.3} \\
middle & 54.5\mypm{1.3} & 90.2\mypm{0.5} \\
young & 54.2\mypm{2.2} & 91.2\mypm{0.7} \\
\end{tabular}}\hspace{5mm}
\resizebox{!}{19mm}{
\tablestyle{4pt}{1.1}\begin{tabular}{@{}lcc@{}}
{} & \multicolumn{2}{c}{mIoU at} \\
{} & 1 point & 3 points \\
\hline
\multicolumn{3}{@{}l}{\emph{perceived skin tone}} \\
1 & 52.9\mypm{2.2} & 91.0\mypm{0.9} \\
2 & 51.5\mypm{1.4} & 91.1\mypm{0.5} \\
3 & 52.2\mypm{1.9} & 91.4\mypm{0.7} \\
4 & 51.5\mypm{2.7} & 91.7\mypm{1.0} \\
5 & 52.4\mypm{4.2} & 92.5\mypm{1.4} \\
6 & 56.7\mypm{6.3} & 91.2\mypm{2.4} \\
\end{tabular}}
\vspace{-2mm}
\caption{\sam's performance segmenting people across perceived gender presentation, age group, and skin tone. 95\% confidence intervals are shown. Within each grouping, all confidence intervals overlap except older \vs middle.}
\label{tab:rai_person}\vspace{-3mm}
\end{table}

\section{Zero-Shot Transfer Experiments}\label{sec:eval}

In this section, we present \emph{zero-shot transfer} experiments with \sam, the Segment Anything Model. We consider five tasks, four of which differ significantly from the promptable segmentation task used to train \sam. These experiments evaluate \sam on datasets and tasks that were not seen during training (our usage of ``zero-shot transfer'' follows its usage in CLIP~\cite{Radford2021}). The datasets may include novel image distributions, such as underwater or ego-centric images (\eg \fig{fig:benchmark_examples}) that, to our knowledge, do not appear in \sad.

Our experiments begin by testing the core goal of promptable segmentation: producing a valid mask from any prompt. We emphasize the challenging scenario of a \emph{single} foreground point prompt, since it is more likely to be ambiguous than other more specific prompts. Next, we present a sequence of experiments that traverse low, mid, and high-level image understanding and roughly parallel the historical development of the field. Specifically, we prompt \sam to (1) perform edge detection, (2) segment everything, \ie object proposal generation, (3) segment detected objects, \ie instance segmentation, and (4), as a proof-of-concept, to segment objects from free-form text. These four tasks differ significantly from the promptable segmentation task that \sam was trained on and are implemented via prompt engineering. Our experiments conclude with an ablation study.

\paragraph{Implementation.} Unless otherwise specified: (1) \sam uses an MAE~\cite{he2022masked} pre-trained ViT-H~\cite{Dosovitskiy2021} image encoder and (2) \sam was trained on \sad, noting that this dataset includes only automatically generated masks from the final stage of our data engine. For all other model and training details, such as hyperparameters, refer to \S\ref{app:model}.

\subsection{Zero-Shot Single Point Valid Mask Evaluation}\label{sec:eval:single_point}\vspace{-2mm}

\paragraph{Task.} We evaluate segmenting an object from a \emph{single} foreground point. This task is ill-posed as one point can refer to multiple objects. Ground truth masks in most datasets do not enumerate \emph{all} possible masks, which can make automatic metrics unreliable. Therefore, we supplement the standard mIoU metric (\ie, the mean of all IoUs between predicted and ground truth masks) with a human study in which annotators rate mask quality from 1 (nonsense) to 10 (pixel-perfect). See \S\ref{app:benchmark}, \S\ref{app:human_study}, and \S\ref{app:annotation_guidelines} for additional details.

By default, we sample points from the ``center'' of ground truth masks (at a maximal value of the mask's interior distance transform), following the standard evaluation protocol in interactive segmentation~\cite{sofiiuk2022reviving}. Since \sam is capable of predicting multiple masks, we evaluate only the model's most confident mask by default. The baselines are all single-mask methods. We compare mainly to RITM~\cite{sofiiuk2022reviving}, a strong interactive segmenter that performs best on our benchmark compared to other strong baselines~\cite{liu2022simpleclick,chen2022focalclick}.

\begin{figure*}[t]\centering
\tablestyle{0.9pt}{0.6}\fontsize{6}{7}\selectfont
\begin{tabular}{cccccccc}
ADE20K~\cite{Zhou2019} & BBBC038v1~\cite{cells} & Cityscapes~\cite{Cordts2016} & DOORS~\cite{doors} & DRAM~\cite{DRAM} & EgoHOS~\cite{egoHos} & GTEA~\cite{gtea1,gtea2} & Hypersim~\cite{hypersim} \\
\includegraphics[height=1.43cm]{figs/datasets/ade20k_ADE_val_00001049.jpg} &
\includegraphics[height=1.43cm]{figs/datasets/cells_10k_0e21d7b3eea8cdbbed60d51d72f4f8c1974c5d76a8a3893a7d5835c85284132e.jpg} &
\includegraphics[height=1.43cm]{figs/datasets/cityscapes_10k_blurred_munster_000144_000019_leftImg8bit_blurred.jpg} &
\includegraphics[height=1.43cm]{figs/datasets/doors_10k_TR_024413.jpg} &
\includegraphics[height=1.43cm]{figs/datasets/dram_10k_1905_221847.jpg} &
\includegraphics[height=1.43cm]{figs/datasets/egohos_10k_blurred_thu_manicure_RGB_manicure_lzy_1_68.jpg} &
\includegraphics[height=1.43cm]{figs/datasets/gtea_10k_blurred_s4_cofhoney_0000000700.jpg} &
\includegraphics[height=1.43cm]{figs/datasets/hypersim_10k_frame.jpg}
\end{tabular}\\[0mm]
\tablestyle{1pt}{0.6}\fontsize{6}{7}\selectfont
\begin{tabular}{cccccccc}
IBD~\cite{chen20223D} & iShape~\cite{iShape} & LVIS~\cite{Gupta2019} & NDD20~\cite{ndd20} & NDISPark~\cite{ndis1,ndis2} & OVIS~\cite{ovis} & PPDLS~\cite{plants} & Plittersdorf~\cite{haucke2022socrates}\\
\includegraphics[height=1.64cm]{figs/datasets/drones_10k_69-10400-3200-OR.jpg} &
\includegraphics[height=1.64cm]{figs/datasets/ishape_10k_3982.jpg} &
\includegraphics[height=1.64cm]{figs/datasets/lvis_10k_blurred_000000050844.jpg} &
\includegraphics[height=1.64cm]{figs/datasets/ndd20_10k_1599.jpg} &
\includegraphics[height=1.64cm]{figs/datasets/ndis_10k_blurred_64_1537102819.jpg} &
\includegraphics[height=1.64cm]{figs/datasets/ovis_10k_blurred_img_0000022.jpg} &
\includegraphics[height=1.64cm]{figs/datasets/plant_10k_ara2012_plant101_rgb.jpg} &
\includegraphics[height=1.64cm]{figs/datasets/plittersdorf_10k_20220213100529_000507.jpg}
\end{tabular}\\[0mm]
\tablestyle{1pt}{0.6}\fontsize{6}{7}\selectfont
\begin{tabular}{ccccccc}
STREETS~\cite{streets} & TimberSeg~\cite{timberSeg} & TrashCan~\cite{hong2020trashcan} & VISOR~\cite{VISOR,EpicKitchens} & WoodScape~\cite{yogamani2019woodscape} & PIDRay~\cite{wang2021towards} & ZeroWaste-f~\cite{zerowaste} \\
\includegraphics[height=1.57cm]{figs/datasets/streets_10k_Lake.jpg} &
\includegraphics[height=1.57cm]{figs/datasets/timberseg_10k_000050.jpg} &
\includegraphics[height=1.57cm]{figs/datasets/trashcan_10k_vid_000343_frame0000015.jpg} &
\includegraphics[height=1.57cm]{figs/datasets/visor_10k_blurred_P04_13_frame_0000005102.jpg} &
\includegraphics[height=1.57cm]{figs/datasets/woodscapes_10k_03918_MVL.jpg} &
\includegraphics[height=1.57cm]{figs/datasets/xray_hard_10k_xray_hard00550.jpg} &
\includegraphics[height=1.57cm]{figs/datasets/zerowaste_10k_07_frame_032200.jpg}
\end{tabular}\vspace{-2mm}
\caption{Samples from the 23 diverse segmentation datasets used to evaluate \sam's zero-shot transfer capabilities.}
\label{fig:benchmark_examples}
\end{figure*}

\paragraph{Datasets.} We use a newly compiled suite of 23 datasets with diverse image distributions. \fig{fig:benchmark_examples} lists the datasets and shows a sample from each one (see appendix Table~\ref{app:tab:datasets_all} for more details). We use all 23 datasets for mIoU evaluation. For the human study, we use the subset listed in \fig{fig:benchmark_exps}{\color{linkcolor}b} (due to the resource requirements of such studies). This subset includes both datasets for which \sam outperforms and underperforms RITM according to automatic metrics.

\begin{figure*}[t]\centering
\begin{tabular}[b]{@{}c@{}}
\resizebox{.49\linewidth}{!}{\input{figs/becnhmark_ritm_comparison.pgf}}\\[-1mm]
\footnotesize (a) \sam \vs RITM~\cite{sofiiuk2022reviving} on 23 datasets
\end{tabular}\hfill
\begin{tabular}[b]{@{}c@{}}
\includegraphics[width=.49\linewidth]{figs/human_study_points.pdf}\\[-2mm]
\footnotesize (b) Mask quality ratings by human annotators\\[2mm]
\begin{tabular}[b]{@{}c@{}c@{}}
\includegraphics[width=0.24\linewidth,trim={0 2mm 0 0},clip]{figs/becnhmark_ritm_pointer_plot.pdf} &
\includegraphics[width=0.24\linewidth,trim={0 2mm 0 0},clip]{figs/becnhmark_random_pointer_plot.pdf}\\[-1mm]
\footnotesize (c) Center points (default) & \footnotesize (d) Random points
\end{tabular}
\end{tabular}\vspace{-2mm}
\caption{Point to mask evaluation on 23 datasets. (a) Mean IoU of \sam and the strongest single point segmenter, RITM~\cite{sofiiuk2022reviving}. Due to ambiguity, a single mask may not match ground truth; circles show ``oracle'' results of the most relevant of \sam's 3 predictions. (b) Per-dataset comparison of mask quality ratings by annotators from 1 (worst) to 10 (best). All methods use the ground truth mask center as the prompt. (c, d) mIoU with varying number of points. \sam significantly outperforms prior interactive segmenters with 1 point and is on par with more points. Low absolute mIoU at 1 point is the result of ambiguity.}
\label{fig:benchmark_exps}\vspace{-2mm}
\end{figure*}

\paragraph{Results.} First, we look at automatic evaluation on the full suite of 23 datasets using mIoU. We compare per-dataset results in \fig{fig:benchmark_exps}{\color{linkcolor}a} against RITM. \sam yields higher results on 16 of the 23 datasets, by as much as $\app$47 IoU. We also present an ``oracle'' result, in which the most relevant of \sam's 3 masks is selected by comparing them to the ground truth, rather than selecting the most confident mask. This reveals the impact of ambiguity on automatic evaluation. In particular, with the oracle to perform ambiguity resolution, \sam outperforms RITM on \emph{all} datasets.

Results of the human study are presented in \fig{fig:benchmark_exps}{\color{linkcolor}b}. Error bars are 95\% confidence intervals for mean mask ratings (all differences are significant; see \S\ref{app:human_study} for details). We observe that the annotators consistently rate the quality of \sam's masks substantially higher than the strongest baseline, RITM. An ablated, ``ambiguity-unaware'' version of \sam with a single output mask has consistently lower ratings, though still higher than RITM. \sam's mean ratings fall between 7 and 9, which corresponds to the qualitative rating guideline: ``\emph{A high score (7-9): The object is identifiable and errors are small and rare (\eg, missing a small, heavily obscured disconnected component, ...).}'' These results indicate that \sam has learned to segment valid masks from a single point. Note that for datasets like DRAM and IBD, where \sam is worse on automatic metrics, \emph{it receives consistently higher ratings in the human study}.

\fig{fig:benchmark_exps}{\color{linkcolor}c} shows additional baselines, SimpleClick~\cite{liu2022simpleclick} and FocalClick~\cite{chen2022focalclick}, which obtain lower single point performance than RITM and \sam. As the number of points increases from 1 to 9, we observe that the gap between methods decreases. This is expected as the task becomes easier; also, \sam is not optimized for the very high IoU regime. Finally, in \fig{fig:benchmark_exps}{\color{linkcolor}d} we replace the default center point sampling with random point sampling. We observe that the gap between \sam and the baselines grows and \sam is able to achieve comparable results under either sampling method.

\subsection{Zero-Shot Edge Detection}\label{sec:eval:edge}

\paragraph{Approach.} We evaluate \sam on the classic low-level task of edge detection using BSDS500~\cite{martin2001database,arbelaez2010contour}. We use a simplified version of our automatic mask generation pipeline. Specifically, we prompt \sam with a 16$\x$16 regular grid of foreground points resulting in 768 predicted masks (3 per point). Redundant masks are removed by NMS. Then, edge maps are computed using Sobel filtering of unthresholded mask probability maps and standard lightweight postprocessing, including edge NMS (see \S\ref{app:edges} for details).

\begin{figure}[t]\centering
\tablestyle{1pt}{0.8}\begin{tabular}{ccc}
image & ground truth & \sam \\
\includegraphics[width=0.325\linewidth]{figs/bsds/images/35028.pdf} &
\includegraphics[width=0.325\linewidth]{figs/bsds/avg-groundTruth/35028.pdf} &
\includegraphics[width=0.325\linewidth]{figs/bsds/4624034_0002-nms/35028.pdf} \\
\includegraphics[width=0.325\linewidth]{figs/bsds/images/48017.pdf} &
\includegraphics[width=0.325\linewidth]{figs/bsds/avg-groundTruth/48017.pdf} &
\includegraphics[width=0.325\linewidth]{figs/bsds/4624034_0002-nms/48017.pdf} \\
\end{tabular}\vspace{-3mm}
\caption{Zero-shot edge prediction on BSDS500. \sam was not trained to predict edge maps nor did it have access to BSDS images or annotations during training.}
\label{fig:edges}
\end{figure}

\begin{table}[t]
\centering
\tablestyle{9pt}{1.1}
\footnotesize
\begin{tabular}{@{}lc|x{20}x{20}x{20}|x{9}}
method & year & ODS & OIS & AP & R50 \\
\hline
HED~\cite{xie2015holistically} & 2015& .788 & .808 & .840 & .923 \\
EDETR~\cite{pu2022edter} & 2022 & .840 & .858 & .896 & .930 \\
\multicolumn{6}{@{}l}{\emph{zero-shot transfer methods:}} \\
Sobel filter & 1968 & .539 & - & -& - \\
Canny~\cite{canny1986computational} & 1986 & .600 & .640 & .580 & - \\
Felz-Hutt~\cite{felzenszwalb2004efficient} & 2004 & .610 & .640 & .560 & - \\
\sam & 2023 &.768 &.786 & .794 & .928 \\
\end{tabular}\vspace{-2mm}
\caption{Zero-shot transfer to edge detection on BSDS500.}
\label{tab:edges}
\end{table}

\paragraph{Results.} We visualize representative edge maps in \fig{fig:edges} (see \fig{fig:more_edges} for more). Qualitatively, we observe that even though \sam was not trained for edge detection, it produces reasonable edge maps. Compared to the ground truth, \sam predicts more edges, including sensible ones that are not annotated in BSDS500. This bias is reflected quantitatively in Table~\ref{tab:edges}: recall at 50\% precision (R50) is high, at the cost of precision. \sam naturally lags behind state-of-the-art methods that learn the biases of BSDS500, \ie, which edges to suppress. Nevertheless, \sam performs well compared to pioneering deep learning methods such as HED~\cite{xie2015holistically} (also trained on BSDS500) and significantly better than prior, though admittedly outdated, zero-shot transfer methods.

\subsection{Zero-Shot Object Proposals}\label{subsec:proposals}

\paragraph{Approach.} Next, we evaluate \sam on the mid-level task of object proposal generation~\cite{alexe2010object,Sande2011}. This task has played an important role in object detection research, serving as an intermediate step in pioneering systems (\eg,~\cite{Sande2011,Girshick2014,Ren2015}). To generate object proposals, we run a slightly modified version of our automatic mask generation pipeline and output the masks as proposals (see \S\ref{app:proposals} for details).

We compute the standard average recall (AR) metric on LVIS v1~\cite{Gupta2019}. We focus on LVIS because its large number of categories presents a challenging test. We compare to a \emph{strong} baseline implemented as a ViTDet~\cite{li2022exploring} detector (with cascade Mask R-CNN~\cite{He2017,Cai2018} ViT-H). We note that this ``baseline'' corresponds to the ``Detector Masquerading as Proposal generator'' (DMP) method~\cite{chavali2016object} that was shown to game AR, making it a truly demanding comparison.

\begin{table}[t]
\centering
\tablestyle{2.8pt}{1.1}
\footnotesize
\begin{tabular}{@{}lx{20}|x{20}x{20}x{20}|x{20}x{20}x{12}}
\multirow{2}{*}{} & \multicolumn{7}{c}{mask AR@1000}\\
method & all & small & med. & large & freq. & com. & rare \\
\hline
ViTDet-H~\cite{li2022exploring} & 63.0 & 51.7 & 80.8 & 87.0 & 63.1 & 63.3 & 58.3 \\
\multicolumn{8}{@{}l}{\emph{zero-shot transfer methods:}} \\
\sam\ -- single out. & 54.9 & 42.8 & 76.7 & 74.4 & 54.7 & 59.8 & 62.0 \\
\sam & 59.3 & 45.5 & 81.6 & 86.9 & 59.1 & 63.9 & 65.8 \\
\end{tabular}
\vspace{-2mm}
\caption{Object proposal generation on LVIS v1. \sam is applied zero-shot, \ie it was not trained for object proposal generation nor did it access LVIS images or annotations.}
\label{tab:proposals}
\end{table}

\paragraph{Results.} In Table~\ref{tab:proposals} we see unsurprisingly that using the detections from ViTDet-H as object proposals (\ie, the DMP method~\cite{chavali2016object} that games AR) performs the best overall. However, \sam does remarkably well on several metrics. Notably, it outperforms ViTDet-H on medium and large objects, as well as rare and common objects. In fact, \sam only underperforms ViTDet-H on small objects and frequent objects, where ViTDet-H can easily learn LVIS-specific annotation biases since it was trained on LVIS, unlike \sam. We also compare against an ablated ambiguity-unaware version of \sam (``single out.''), which performs significantly worse than \sam on all AR metrics.

\subsection{Zero-Shot Instance Segmentation}\label{sec:eval:instseg}

\paragraph{Approach.} Moving to higher-level vision, we use \sam as the segmentation module of an instance segmenter. The implementation is simple: we run a object detector (the ViTDet used before) and prompt \sam with its output boxes. This illustrates \emph{composing} \sam in a larger system.

\paragraph{Results.} We compare the masks predicted by \sam and ViTDet on COCO and LVIS in Table~\ref{tab:instance_segmentation}. Looking at the mask AP metric we observe gaps on both datasets, where \sam is reasonably close, though certainly behind ViTDet. By visualizing outputs, we observed that \sam masks are often qualitatively better than those of ViTDet, with crisper boundaries (see \S\ref{app:instseg} and \fig{fig:instanceseg}). To investigate this observation, we conducted an additional human study asking annotators to rate the ViTDet masks and \sam masks on the 1 to 10 quality scale used before. In \fig{fig:humanstudy:inst} we observe that \sam consistently outperforms ViTDet in the human study.

\begin{table}[t]
\centering
\tablestyle{2.2pt}{1.1}
\footnotesize
\begin{tabular}{@{}lx{19}x{19}x{19}x{19}|x{19}x{19}x{19}x{12}}
 \multirow{2}{*}{} & \multicolumn{4}{c}{COCO~\cite{Lin2014}} & \multicolumn{4}{c}{LVIS v1~\cite{Gupta2019}}\\
method & AP & AP\textsuperscript{S} & AP\textsuperscript{M} & AP\textsuperscript{L} & AP & AP\textsuperscript{S} & AP\textsuperscript{M} & AP\textsuperscript{L} \\
\hline
ViTDet-H~\cite{li2022exploring} & 51.0 & 32.0 & 54.3 & 68.9 & 46.6 & 35.0 & 58.0 & 66.3\\
\multicolumn{9}{@{}l}{\emph{zero-shot transfer methods (segmentation module only):}} \\
\sam & 46.5 & 30.8 & 51.0 & 61.7 & 44.7 & 32.5 & 57.6 & 65.5\\
\end{tabular}
\vspace{-2mm}
\caption{Instance segmentation results. \sam is prompted with ViTDet boxes to do zero-shot segmentation. The fully-supervised ViTDet outperforms \sam, but the gap shrinks on the higher-quality LVIS masks. Interestingly, \sam outperforms ViTDet according to human ratings (see \fig{fig:humanstudy:inst}).}
\label{tab:instance_segmentation}\vspace{-1mm}
\end{table}

\begin{figure}[t]\centering
\includegraphics[width=.99\linewidth]{figs/human_study_boxes.pdf}
\vspace{-3mm}
\caption{Mask quality rating distribution from our human study for ViTDet and \sam, both applied to LVIS ground truth boxes. We also report LVIS and COCO ground truth quality. The legend shows rating means and 95\% confidence intervals. Despite its lower AP (Table~\ref{tab:instance_segmentation}), \sam has higher ratings than ViTDet, suggesting that ViTDet exploits biases in the COCO and LVIS training data.}
\label{fig:humanstudy:inst}\vspace{-3mm}
\end{figure}

We hypothesize that on COCO, where the mask AP gap is larger and the ground truth quality is relatively low (as borne out by the human study), ViTDet learns the specific biases of COCO masks. \sam, being a zero-shot method, is unable to exploit these (generally undesirable) biases. The LVIS dataset has higher quality ground truth, but there are still specific idiosyncrasies (\eg, masks do not contain holes, they are simple polygons by construction) and biases for modal \vs amodal masks. Again, \sam is not trained to learn these biases, while ViTDet can exploit them.

\subsection{Zero-Shot Text-to-Mask}\label{sec:eval:text_to_mask}

\paragraph{Approach.} Finally, we consider an even higher-level task: segmenting objects from free-form text. This experiment is a proof-of-concept of \sam's ability to process text prompts. While we used the exact same \sam in all prior experiments, for this one \sam's training procedure is modified to make it text-aware, but in a way that does not require new text annotations. Specifically, for each manually collected mask with area larger than $\textrm{100}^\textrm{2}$ we extract the CLIP \emph{image} embedding. Then, during training, we prompt \sam with the extracted CLIP image embeddings as its first interaction. The key observation here is that because CLIP's \emph{image} embeddings are trained to align with its \emph{text} embeddings, we can train with image embeddings, but use text embeddings for inference. That is, at inference time we run text through CLIP's text encoder and then give the resulting text embedding as a prompt to \sam (see \S\ref{app:text_to_mask} for details).

\begin{figure}[t]\centering
\tablestyle{1pt}{0.8}
\begin{tabular}{c c}\begin{tikzpicture}
\draw (0, 0) node[inner sep=0] {\includegraphics[width=0.49\linewidth]{figs/text_to_mask/a_wheel.pdf}};
\draw (-0.6, 0.68) node {\cmark};
\end{tikzpicture} &
\begin{tikzpicture}
\draw (0, 0) node[inner sep=0] {\includegraphics[width=0.49\linewidth]{figs/text_to_mask/beaver_tooth_grille.pdf}};
\draw (-1.25, 0.68) node {\cmark};
\end{tikzpicture}\\
\begin{tikzpicture}
\draw (0, 0) node[inner sep=0] {\includegraphics[width=0.49\linewidth]{figs/text_to_mask/a_wiper.pdf}};
\draw (-0.55, 0.67) node {\xmark};
\end{tikzpicture} &
\begin{tikzpicture}
\draw (0, 0) node[inner sep=0] {\includegraphics[width=0.49\linewidth]{figs/text_to_mask/a_wiper_click.pdf}};
\draw (-0.97, 0.68) node {\cmark};
\end{tikzpicture}\\
\begin{tikzpicture}
\draw (0, 0) node[inner sep=0] {\includegraphics[width=0.49\linewidth]{figs/text_to_mask/wipers.pdf}};
\draw (-0.50, 0.67) node {\xmark};
\end{tikzpicture} &
\begin{tikzpicture}
\draw (0, 0) node[inner sep=0] {\includegraphics[width=0.49\linewidth]{figs/text_to_mask/wipers_click.pdf}};
\draw (-0.95, 0.68) node {\cmark};
\end{tikzpicture}\\
\end{tabular}
\vspace{-4mm}
\caption{Zero-shot text-to-mask. \sam can work with simple and nuanced text prompts. When \sam fails to make a correct prediction, an additional point prompt can help.}
\label{fig:textprompts}\vspace{-5mm}
\end{figure}
    

\paragraph{Results.} We show qualitative results in \fig{fig:textprompts}. \sam can segment objects based on simple text prompts like ``a wheel'' as well as phrases like ``beaver tooth grille''. When \sam fails to pick the right object from a text prompt only, an additional point often fixes the prediction, similar to~\cite{ding2020phraseclick}.

\begin{figure*}[t]\centering
\includegraphics[width=0.32\linewidth]{figs/ablation_data_stages.pdf}\hfill
\includegraphics[width=0.32\linewidth]{figs/ablation_autodata_scale_log_plot.pdf}\hfill
\includegraphics[width=0.32\linewidth]{figs/ablation_backbone_params.pdf}\vspace{-2mm}
\caption{Ablation studies of our data engine stages, image encoder scaling, and training data scaling. (Left) Each data engine stage leads to improvements on our 23 dataset suite, and training with only the automatic data (our default) yields similar results to using data from all three stages. (Middle) \sam trained with \app10\% of \sad and full \sad is comparable. We train with all 11M images by default, but using 1M images is a reasonable practical setting. (Right) Scaling \sam's image encoder shows meaningful, yet saturating gains. Nevertheless, smaller image encoders may be preferred in certain settings.}
\label{fig:ablations}\vspace{-3mm}
\end{figure*}

\subsection{Ablations}\label{sec:eval:ablations}\vspace{-1mm}

We perform several ablations on our 23 dataset suite with the single center point prompt protocol. Recall that a single point may be ambiguous and that ambiguity may not be represented in the ground truth, which contains only a single mask per point. Since \sam is operating in a zero-shot transfer setting there can be systematic biases between \sam's top-ranked mask \vs the masks resulting from data annotation guidelines. We therefore additionally report the best mask with respect to the ground truth (``oracle'').

\fig{fig:ablations} (left) plots \sam's performance when trained on cumulative data from the data engine stages. We observe that each stage increases mIoU. When training with all three stages, the automatic masks vastly outnumber the manual and semi-automatic masks. To address this, we found that oversampling the manual and semi-automatic masks during training by 10$\x$ gave best results. This setup complicates training. We therefore tested a fourth setup that uses only the automatically generated masks. With this data, \sam performs only marginally lower than using all data (\app0.5 mIoU). Therefore, by default we use only the automatically generated masks to simplify the training setup.

In \fig{fig:ablations} (middle) we look at the impact of data volume. The full \sad contains 11M images, which we uniformly subsample to 1M and 0.1M for this ablation. At 0.1M images, we observe a large mIoU decline under all settings. However, with 1M images, about 10\% of the full dataset, we observe results comparable to using the full dataset. This data regime, which still includes approximately 100M masks, may be a practical setting for many use cases.

Finally, \fig{fig:ablations} (right) shows results with ViT-B, ViT-L, and ViT-H image encoders. ViT-H improves substantially over ViT-B, but has only marginal gains over ViT-L. Further image encoder scaling does not appear fruitful at this time.

\section{Discussion}\label{sec:disc}\vspace{-1mm}

\paragraph{Foundation models.} Pre-trained models have been adapted to downstream tasks since the early days of machine learning~\cite{thrun1995learning}. This paradigm has become increasingly important in recent years with a growing emphasis on scale, and such models have recently been (re-)branded as ``foundation models'': \ie models that are ``trained on broad data at scale and are adaptable to a wide range of downstream tasks''~\cite{bommasani2021opportunities}. Our work correlates well with this definition, though we note that a foundation model for image segmentation is an inherently limited scope, since it represents an important, yet fractional, subset of computer vision. We also contrast one aspect of our approach with~\cite{bommasani2021opportunities}, which emphasizes the role of \emph{self-supervised} learning in foundation models. While our model is initialized with a self-supervised technique (MAE~\cite{he2022masked}), the vast majority of its capabilities come from large-scale \emph{supervised} training. In cases where data engines can scale available annotations, like ours, supervised training provides an effective solution.

\paragraph{Compositionality.} Pre-trained models can power new capabilities even beyond ones imagined at the moment of training. One prominent example is how CLIP~\cite{Radford2021} is used as a \emph{component} in larger systems, such as DALL$\cdot$E~\cite{Ramesh2021}. Our goal is to make this kind of composition straightforward with \sam. We aim to achieve this by requiring \sam to predict a valid mask for a wide range of segmentation prompts. The effect is to create a reliable interface between \sam and other components. For example, MCC~\cite{wu2023multiview} can easily use \sam to segment an object of interest and achieve strong generalization to unseen objects for 3D reconstruction from a single RGB-D image. In another example, \sam can be prompted with gaze points detected by a wearable device, enabling new applications. Thanks to \sam's ability to generalize to new domains like ego-centric images, such systems work without need for additional training.

\paragraph{Limitations.} While \sam performs well in general, it is not perfect. It can miss fine structures, hallucinates small disconnected components at times, and does not produce boundaries as crisply as more computationally intensive methods that ``zoom-in'', \eg~\cite{chen2022focalclick}. In general, we expect dedicated interactive segmentation methods to outperform \sam when many points are provided, \eg~\cite{liu2022simpleclick}. Unlike these methods, \sam is designed for generality and breadth of use rather than high IoU interactive segmentation. Moreover, \sam can process prompts in real-time, but nevertheless \sam's overall performance is not real-time when using a heavy image encoder. Our foray into the text-to-mask task is exploratory and not entirely robust, although we believe it can be improved with more effort. While \sam can perform many tasks, it is unclear how to design simple prompts that implement semantic and panoptic segmentation. Finally, there are domain-specific tools, such as~\cite{berg2019}, that we expect to outperform \sam in their respective domains.

\paragraph{Conclusion.} The Segment Anything project is an attempt to lift image segmentation into the era of foundation models. Our principal contributions are a new task (promptable segmentation), model (\sam), and dataset (\sad) that make this leap possible. Whether \sam achieves the status of a foundation model remains to be seen by how it is used in the community, but regardless we expect the perspective of this work, the release of over 1B masks, and our promptable segmentation model will help pave the path ahead.

\paragraph{Acknowledgments.} We would like to thank Aaron Adcock and Jitendra Malik for helpful discussion. We thank Vaibhav Aggarwal and Yanghao Li for help with scaling the model. We thank Cheng-Yang Fu, Jiabo Hu, and Robert Kuo for help with data annotation platform. We thank Allen Goodman and Bram Wasti for help in optimizing web-version of our model. Finally, we thank Morteza Behrooz, Ashley Gabriel, Ahuva Goldstand, Sumanth Gurram, Somya Jain, Devansh Kukreja, Joshua Lane, Lilian Luong, Mallika Malhotra, William Ngan, Omkar Parkhi, Nikhil Raina, Dirk Rowe, Neil Sejoor, Vanessa Stark, Bala Varadarajan, and Zachary Winstrom for their help in making the demo, dataset viewer, and other assets and tooling.

{\footnotesize\linespread{.975}\selectfont

\end{document}