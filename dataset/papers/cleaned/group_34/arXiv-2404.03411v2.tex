\title{Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?}

\begin{document}

\maketitle

\begin{abstract}
Various jailbreak attacks have been proposed to red-team Large Language Models (LLMs) and revealed the vulnerable safeguards of LLMs.
Besides, some methods are not limited to the textual modality and extend the jailbreak attack to Multimodal Large Language Models (MLLMs) by perturbing the visual input. 
However, the absence of a universal evaluation benchmark complicates the performance reproduction and fair comparison. 
Besides, there is a lack of comprehensive evaluation of closed-source state-of-the-art (SOTA) models, especially MLLMs, such as GPT-4V. 
To address these issues, this work first builds a comprehensive jailbreak evaluation dataset with 1445 harmful questions covering 11 different safety policies. 
Based on this dataset, extensive red-teaming experiments are conducted on 11 different LLMs and MLLMs, including both SOTA proprietary models and open-source models. 
We then conduct a deep analysis of the evaluated results and find that (1) GPT4 and GPT-4V demonstrate better robustness against jailbreak attacks compared to open-source LLMs and MLLMs. (2) Llama2 and Qwen-VL-Chat are more robust compared to other open-source models. (3) The transferability of visual jailbreak methods is relatively limited compared to textual jailbreak methods.  The dataset and code can be found here \footnote{\url{https://github.com/chenxshuo/RedTeamingGPT4V}}. 

\end{abstract}

\section{Introduction}
Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have shown superior performance in text generation. To avoid generating unobjectionable content learned from the web-scale training corpus, stringent safety regulations have been applied during the safety alignment~\citep{ouyang2022training, touvron2023llama}.  
However, many jailbreak attacks have been proven to be able to bypass these safeguards and successfully elicit harmful generations. 
For example, \citeauthor{zou2023universal} appends a trainable suffix to harmful behavior prompts, which makes the model generate targeted output rather than refusing. Apart from perturbing the textual input, there are also jailbreaking methods modifying the visual input such as trainable image noise~\cite{carlini2023aligned, qi2023visual} to ignore the safety regulation and elicit unethical output. 

However, the lack of a universal evaluation benchmark and performance metrics makes the performance reproduction and a fair comparison hard to achieve. 
Besides, comprehensive evaluations of SOTA proprietary models against jailbreak attacks are still missing, especially MLLMs such as GPT-4V. 
It is hence still unknown how robust these proprietary models are against existing jailbreak attack methods. 
To ensure a reproducible and universal evaluation, in this work, we first constructed a comprehensive jailbreak evaluation dataset with 1445 jailbreak questions covering 11 different safety policies. Then 32 jailbreak methods targeted at LLMs and MLLMs are collected in this study, which contains 29 textual jailbreak methods and 3 visual jailbreak methods.
Based on this benchmark, we then deployed extensive red-teaming experiments on 11 different LLMs and MLLMs including both SOTA proprietary models such as GPT-4, and open-source models such as Llama2 and MiniGPT4. 
We find that GPT-4 and GPT-4V show much better robustness against both textual and visual jailbreak methods compared to open-source models. Besides, among open-source models, Llama2 and Qwen-VL-Chat demonstrate better robustness and Llama2 can even be more robust than GPT-4. Moreover, we compare the transferability of different methods. We find that AutoDAN has better transferability compared to GCG and visual jailbreak methods have relatively limited transferability. 
The contribution of our work can be summarized as follows: 
\begin{itemize}
    \item We provide a jailbreak evaluation benchmark with 1445 harmful behavior questions covering 11 different safety policies for both LLMs and MLLMs. 
    \item We conduct red-teaming on both GPT-4 and GPT-4V and various SOTA open-source models with our evaluation benchmarks.
    \item We provide an in-depth analysis showing the robustness of both business proprietary and open-source multimodal large language models against existing jailbreak methods. 
\end{itemize}

\section{Red Teaming GPT4 Against Jailbreak Attacks}
\subsection{Experimental Setup}

\noindent\textbf{Models.} The experiments are conducted on both proprietary business multimodal LLMs and open-source multimodal LLMs. Specifically, gpt-4-vision-preview (referred to as GPT-4 below) is used to conduct jailbreak red-teaming based on visual input perturbations; {gpt-4-1106-preview}(referred to as GPT-4V) is used in jailbreak attacks based on textual input perturbations. Besides, four open-source LLMs and six open-source VLMs have been chosen as our red-teaming target. In total, there are 11 models used in our study, and detailed information is presented in Tab.~\ref{tab:models} in Appendix. 

\noindent\textbf{Dataset.} To build a comprehensive jailbreak benchmark, we have collected jailbreak behaviors and questions from existing literature, such as AdvBench~\citep{zou2023universal}, SafeBench~\citep{gong2023figstep}, \cite{qi2023visual}, GPT-4 technical report~\citep{achiam2023gpt}, and ToxicChat~\citep{lin2023toxicchat}. In total, 1445 different harmful behaviors and questions have been collected. The dataset covers 11 different usage policies followed by Meta's Purple LLaMA~\citep{inan2023llama} and OpenAI's GPT4~\citep{achiam2023gpt}, such as Violence and Hate, Illegal Weapons, \etc. More detailed information is in Appendix~\ref{app:dataset}. 

\noindent\textbf{Threat Model.} The primary focus of this study is to investigate the transferability of existing jailbreak methods. Open-source models act as surrogate models and are used to train the input modifications for jailbreak, \eg, suffix in GCG~\citep{zou2023universal} and image noise in~\cite{qi2023visual}. These modifications then are used to red-team closed-source models such as GPT-4, and other open-source models. Specifically,  Guanaco-7B, Llama2-7B, and Vicuna-7B are used as surrogate models for textual jailbreak attacks. MiniGPT4-7B is used as the surrogate model for visual jailbreak attacks. 

\noindent\textbf{Evaluation Metrics.} Various evaluation metrics have been proposed to calculate the attack success rate (ASR) in existing literature. They can be classified into four main categories: refusal word detection~\citep{zou2023universal, liu2023autodan}, evaluation based on toxicity detection APIs~\citep{qi2023visual, shen2023anything}, LLMs as judges~\citep{liu2023autodan, gong2023figstep, qi2023visual}, and human evaluation~\citep{shayegani2023jailbreak}. In this work, two matrices are mainly used to calculate the ASR, \ie, refusal word detection, and LLMs as judges.  We follow the setting in~\cite{zou2023universal, liu2023autodan} and count an attack as successful if a set of pre-defined refusal words is not found in the generated content. The pre-defined refusal words can be found in Appendix~\ref{app:eval}. Besides, LLaMA-Guard~\cite{inan2023llama} is used as the jailbreak judge to check whether the generated content is indeed harmful or not. As LLaMA-Guard is open-source and instruction-tuned on a large harmful corpus, it is a more frugal way compared to using GPT-4 as judges~\citep{liu2023autodan, gong2023figstep, qi2023visual}. The detailed instructions to use LLaMA-Guard are in Appendix~\ref{app:eval}. We report the Llama-Guard metric in the main paper and present the full metrics in the Appendix~\ref{app:more-exps}.

\subsection{Red Teaming against Textual Jailbreak}

\begin{table}[]
\resizebox{1.0\columnwidth}{!}{
\begin{tabular}{@{}ccccccccc@{}}
\toprule
Method      & Baseline & \multicolumn{4}{c}{GCG}               & \multicolumn{3}{c}{AutoDAN} \\ \midrule
\begin{tabular}[c]{@{}c@{}}Surrogate Model $\rightarrow$
 \\ Target Model $\downarrow$ \end{tabular} & - & Guanaco-7B & Llama2-7B & Vicuna-7B & Gua7B+Vic-7B & Guanaco-7B & Llama2-7B & Vicuna-7B \\ \midrule
Guanaco-7B  & 32.72\%       & 25.09\% & 30.27\% & 30.40\% & 33.67\% & 36.74\% & 39.20\% & 46.90\% \\
Llama2-7B   & 0.07\%        & 0.14\%  & 0.61\%  & 0.20\%  & 0.14\%  & 10.84\% & 11.04\% & 7.09\%  \\
Vicuna-7B   & 10.97\%       & 36.40\% & 16.29\% & 29.86\% & 37.36\% & 45.67\% & 54.12\% & 57.06\% \\
ChatGLM2-6B & 8.93\%        & 20.72\% & 17.72\% & 16.50\% & 24.47\% & 36.54\% & 13.97\% & 37.83\% \\
GPT-4        & 0.68\%        & 1.91\%  & 0.75\%  & 0.95\%  & 2.39\%  & 0.07\%  & 0.00\%  & 0.00\%  \\ \bottomrule
\end{tabular}
}
\caption{The jailbreak success rate of GCG and AutoDAN evaluated by Llama-Guard. The lowest success rate is in bold. }
\label{tab:gcg-autodan-results}
\end{table}

\noindent\textbf{Hand-crafted Jailbreak Attacks} use pre-defined jailbreak templates or process functions and insert harmful questions into the templates, then send the whole instruction to LLMs. These hand-crafted attacks can be further classified into template-based and function-based. 
Template-based methods normally design instruction templates to describe a specific scenario to mislead the LLMs and elicit harmful content, such as role-playing~\cite{wei2024jailbroken} and do-anything-now~\cite{wei2024jailbroken}. 
Function-based methods need extra pre- or post-process on the input of harmful questions and generated content, such as using base64 encoding and vowel removal. 
This study systematically investigates 27 different hand-crafted jailbreak attack methods including 17 templated-based (\eg, refusal suppression and evil confidant) and 10 function-based methods (\eg, encoding the harmful questions using base64 and removing vowels from the questions). Detailed information about all these methods is provided in Appendix~\ref{app:more-exps} and the full results are presented in Tab.~\ref{tab:text-hand-jb-results}.

\noindent\textbf{Automatic Jailbreak Attacks} optimize a string as part of the jailbreak input to elicit harmful content. This study mainly adopts two popular automatic jailbreak attack methods, \ie, GCG~\citep{zou2023universal} and AutoDAN~\citep{liu2023autodan}. Given a surrogate model with full access, GCG trains an extra suffix following the harmful questions to maximize the probability of generating specific non-refusal responses. AutoDAN starts from an instruction template. Then it updates the tokens in the template using genetic algorithms to find better instructions maximizing the probability of generating specific non-refusal responses. 
In our work, Guanaco-7B, Llama2-7B, and Vicuna-7B are used as surrogate models for GCG and AutoDAN. Besides, we also follow the combination strategy from GCG and train one suffix based on the combination of Guanaco-7B and Vicuna-7B. 
The performance of these two methods is presented in Tab.~\ref{tab:gcg-autodan-results}

\subsection{Red Teaming against Visual Jailbreak}
Various methods have been proposed to jailbreak multimodal LLMs via the visual modality, \ie, perturbing the visual input by either manual functions or automatic optimization. This work adopts 3 different jailbreak methods in total, including one black-box typography method FigStep~\citep{gong2023figstep} and two optimization-based methods, \ie VisualAdv~\citep{qi2023visual}, and ImageHijacks~\citep{bailey2023image}. VisualAdv optimizes an adversarial example on a few-shot harmful corpus to maximize the probability of generating harmful content. ImageHijacks optimizes the adversarial example to maximize the generation probability of affirmative response to harmful requests.
We use MiniGPT-4 as surrogate models for VisualAdv and ImageHijacks. The jailbreak performance of these three methods is shown in Tab.~\ref{tab:visual-jb-results}

\begin{table}[]
\centering
\resizebox{0.6\columnwidth}{!}{
\begin{tabular}{@{}ccccc@{}}
\toprule
Method                       & Baseline & FigStep & VisualAdv   & ImageHijacks \\ \midrule
\begin{tabular}[c]{@{}c@{}}Surrogate Model $\rightarrow$
 \\ Target Model $\downarrow$ \end{tabular} & -             & -       & MiniGPT4-7B & MiniGPT4-7B  \\ \midrule
MiniGPT4-7B                  & 9.68\%        & 35.99\% & 34.08\%     & 36.74\%      \\
LLaVAv1.5-7B                 & 17.93\%       & 25.90\% & 15.75\%     & 17.11\%      \\
Fuyu                         & 8.66\%        & 34.90\% & 6.75\%      & 6.27\%       \\
Qwen-VL-Chat                 & 2.39\%        & 14.52\% & 2.45\%      & 2.86\%       \\
CogVLM                       & 6.95\%        & 16.36\% & 9.68\%      & 8.38\%       \\
GPT-4V          & 0.00\%        & 0.07\%  & 0.00\%      & 0.00\%       \\ \bottomrule
\end{tabular}}
\caption{The jailbreak success rate of visual jailbreak methods evaluated by Llama-Guard.}
\label{tab:visual-jb-results}
\end{table}

\section{Discussion}
\noindent\textbf{Which model is more robust against jailbreak?} 
In our experiments, GPT4 is more robust against textual jailbreak methods in most cases. 
One noticeable exception happens under the GCG attack. Llama2-7B demonstrates better robustness against GCG attack and less than 1\% of the responses are classified as harmful as shown in the second row in Tab.~\ref{tab:gcg-autodan-results}. However, the AutoDAN attack can elicit more than 10\% harmful responses on Llama2-7B whereas GPT4 defends almost all attempts successfully. 
Among open-source LLMs used in this work, Llama2-7B is the most robust model whereas Vicuna-7B is the most vulnerable one. This can be because that Vicuna does not implement any specific safeguard fine-tuning and the dataset used for fine-tuning has not been rigorously filtered~\citep{vicuna2023}. Llama2-7B, on the other hand, deploys safety alignment fine-tuning and a series of red teaming to ensure safe response~\citep{touvron2023llama}.  As for visual jailbreak in our experiments, it is much harder to successfully jailbreak GPT-4V compared to other open-source MLLMs. Among open-source MLLMs, Qwen-VL-Chat is the most robust against jailbreak attacks whereas MiniGPT4-7B is the most vulnerable. This can be also attributed to the different LLMs upon which these two MLLMs are built. MiniGPT4-7B used in this study is based on Vicuna-7B which is not safely fine-tuned. Qwen-VL-Chat is built on Qwen-Chat that is finetuned on a curated dataset relevant to safety~\cite{bai2023qwen}. 

\noindent\textbf{Which attack method is most powerful?} There is no single method for achieving the highest attack success rate across different target models. AutoDAN demonstrates higher success rates on open-source LLMs compared to GCG, especially on Llama2-7B. However, GPT-4 successfully refuses almost all AutoDAN's requests. This may be because 
the jailbreak prompts used by AutoDAN have been filtered by OpenAI's safeguard and the token replacement from AutoDAN is not enough to bypass the safety guard. 
Among visual jailbreak methods, FigStep achieves a higher success rate across MLLMs compared to the transfer attack by VisualAdv and ImageHijacks. 

\noindent\textbf{How good is the current defense of the open-source model and closed-source model?} In our experiments, there is a significant gap between open-source models and GPT-4 in most testing scenarios. For example, AutoDAN can obtain $57.06\%$ success rate on Vicuna-7B and $46.90\%$ on Guanaco-7B, whereas GPT-4 defends almost all its requests. The same gap goes for visual jailbreaks. FigStep can achieve a success rate of $35.99\%$ on MiniGPT4-7B and $34.90\%$ on Fuyu.  But on GPT-4V, the success rate is approximately $0$. However, this does not indicate that GPT-4 and GPT-4V have a perfect defense against jailbreak attacks. For example, the GCG trained on the combination of Guanaco-7B and Vicuna-7B can still achieve a success rate of $2.39\%$. 

\noindent\textbf{Does GPT-4 suffer more from visual jailbreak, compared to text modality?} In our experiments, visual jailbreak on GPT-4V does not demonstrate more vulnerability compared to textual jailbreak methods. This can be attributed to the input filtering as VisualAdv and ImageHijacks do not alter the original harmful questions. Besides, although FigStep uses typography and removes harmful context from textual questions, GPT-4V is still able to refuse the requests. 

\noindent\textbf{How good is the transferability of jailbreak methods?} AutoDAN demonstrates better transferability compared to GCG on open-source LLMs. This can be because the suffix generated by GCG is not semantically meaningful and can be confusing when transferred to other models. AudoDAN, on the other hand, preserves the semantic meaning of the jailbreak prompt and hence shows better transferability on other models.
The transferability of visual jailbreak methods studied in this work is relatively limited. The improvement of success rate is limited compared to the baseline and sometimes the success rates of transfer attacks are even lower. For example, when attacking Fuyu by VisualAdv and using MiniGPT4-7B as the surrogate model, the success rate ($6.75\%$) is lower than the baseline result ($8.6\%$). Besides, the transfer attack of visual jailbreak methods on GPT-4V is not effective. The main reason is that these methods do not alter the harmful questions. GPT-4V can directly detect the harmful content in the input and thus refuse to respond. 

\section{Conclusion}
This study focuses on red-teaming both proprietary and open-source LLMs and MLLMs. We first collected existing jailbreak datasets and constructed a comprehensive evaluation benchmark covering 11 different usage policies. Based on the evaluation benchmark, we conducted red-teaming experiments across 11 different LLMs and MLLMs. We find that GPT-4 and GPT-4V are much more robust compared to open-source models and the gap between them is significant. Compared to text modality, current visual jailbreak methods are hard to succeed on GPT-4V. Future work includes incorporating more jailbreak methods, and datasets.

\end{document}