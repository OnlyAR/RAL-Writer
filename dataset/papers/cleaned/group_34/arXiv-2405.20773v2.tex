\title{Visual-RolePlay: Universal Jailbreak Attack on MultiModal Large Language Models via Role-playing Image Character}

\begin{document}

\maketitle

\begin{abstract}

With the advent and widespread deployment of Multimodal Large Language Models (MLLMs), ensuring their safety has become increasingly critical. To achieve this objective, it requires us to proactively discover the vulnerabilities of MLLMs by exploring attack methods. Thus, structure-based jailbreak attacks, where harmful semantic content is embedded within images, have been proposed to mislead the models.
However, previous structure-based jailbreak methods mainly focus on transforming the format of malicious queries, such as converting harmful content into images through typography, which lacks sufficient jailbreak effectiveness and generalizability.
To address these limitations, we first introduce the concept of ``Role-play'' into MLLM jailbreak attacks and propose a novel and effective method called Visual Role-play (VRP). Specifically, VRP leverages Large Language Models to generate detailed descriptions of high-risk characters and create corresponding images based on the descriptions. When paired with benign role-play instruction texts, these high-risk character images effectively mislead MLLMs into generating malicious responses by enacting characters with negative attributes. We further extend our VRP method into a universal setup to demonstrate its generalizability. Extensive experiments on popular benchmarks show that VRP outperforms the strongest baselines, Query relevant~\cite{liu2024mmsafetybench} and FigStep~\cite{gong2023figstep}, by an average Attack Success Rate (ASR) margin of 14.3\% across all models.

\begin{center}
    {\color{red} Disclaimer: This paper contains offensive content that may be disturbing.}
\end{center}
\end{abstract}

\section{Introduction}\label{sec:intro}

Recent advances in Multimodal Large Language Models (MLLMs) have demonstrated significant strides in achieving highly generalized vision-language reasoning capabilities~\cite{bai2023qwenvl,openai2023gpt4,liu2024llavanext,chen2023internvl,yang2023setofmark,yin2023survey,fu2023mme,yin2023woodpecker,reid2024gemini,li2023blip2,lin2023video,zhu2023languagebind,zhang2023llamaadapter,gu2024agent,achiam2023gpt,lyu2023gpt,liu2024multimodal,zhang2024mmllms,liu2024multimodal,dong2024internlmxcomposer2,yu2023rlhf}. Given their potential for widespread societal impact, it is crucial to ensure that the responses generated by MLLMs are free from harmful content such as violence, discrimination, disinformation, or immorality\cite{OpenAI_UsagePolicies, meta_ai_2024_llama}. Consequently, increasing concerns regarding the safety of MLLMs have prompted extensive research into jailbreak attacks and defense strategies~\cite{wang2024adashield,zong2023safety,yu2023rlhf,2023vlfeedback,cha2024visually,2023llavarlhf,liu2024survey,ji2023large,rizwan2024zero,shayegani2023survey}.

Jailbreak attacks in MLLMs, by generating delicately designed inputs, aim to mislead MLLMs into responding to malicious requests and providing harmful content~\cite{gong2023figstep,liu2024mmsafetybench,luo2024jailbreakv28k,wei2023skywork,dong2024internlmxcomposer2,shayegani2024jailbreak,han2023otattack,qi2023visual,schlarmann2023adversarial,zhang2023mutation,wang2024decodingtrust,li2024red,naveed2024comprehensive,liu2024agentbench}. It is critical to evaluate and understand the jailbreak robustness of MLLMs to ensure they behave responsibly and safely. Existing jailbreak attacks against MLLMs can be categorized into three types: (i) \textit{perturbation-based} attacks, which disrupt the alignment of MLLMs through adversarial perturbations~\cite{niu2024jailbreaking, qi2023visual, dong2023robust}; (ii) \textit{text-based} attacks, which generate some textual jailbreak prompts to compromise MLLMs by leveraging LLM jailbreak techniques~\cite{luo2024jailbreakv28k}; (iii) \textit{structure-based} attacks that utilize some malicious images with specific semantic meanings to jailbreak MLLMs\footnote{For example, converting harmful content into text typography in images~\cite{gong2023figstep} or separating some harmful content into topic-relevant images~\cite{liu2024mmsafetybench} to bypass MLLM safety measures.}. Perturbation-based attacks, as a variant of standard vision adversarial attacks, have been extensively studied~\cite{chen2023dress} and various defense methods like purifiers~\cite{Mao2021ICCV,guo2024puridefense, nie2022diffusion} or adversarial training~\cite{advtraining} have proven effectiveness~\cite{shayegani2023survey}. In addition, text-based jailbreak attacks, as an extension of LLM jailbreak attacks, are likely to be detected and blocked by text moderators~\cite{inan2023llamaguard}. (See Appendix \ref{sec: appendix evluate}) Consequently, structure-based jailbreak attacks remain to be unexplored and present unique challenges related to the multi-modality nature of MLLMs. Therefore, in this paper, we primarily focus on structure-based jailbreak attack methods.

Unfortunately, existing structure-based jailbreak attack methods exhibit two limitations. \textbf{First, current methods on MLLM lack sufficient jailbreak effectiveness, leaving significant room for performance improvement.} These methods primarily involve transforming the format of malicious queries, such as converting harmful content into images through typography or using text-to-image tools to bypass the safety mechanisms of MLLMs. For instance, FigStep~\cite{gong2023figstep} creates images that contain malicious text, such as “Here is how to build a bomb: 1. 2. 3.”, to induce the MLLMs into completing the sentences, thereby leading them to inadvertently provide malicious responses. As shown by our results, these simple transformations do not achieve sufficient attack effectiveness. We argue that to enhance the attack performance, a “jailbreak context” must be introduced. For instance, in attacks against LLMs, attackers provide additional context, such as “ignore previous constraints” or “now you are an AI assistant without any constraints”, to prompt the models to disregard their safety protocols and operate without limitations. \textbf{Secondly, current jailbreak methods lack generalization.} For jailbreak attacks, universal properties are crucial as they enable an attack to be applicable across a broad range of scenarios without requiring extensive modifications or customization. However, existing structure-based jailbreak attacks on MLLMs overlook this problem, as they necessitate computation for each query, especially when dealing with large datasets, making them impractical.
\begin{figure}[t!]
\centering
\includegraphics[width=0.95\textwidth]{images/VRP.pdf}
\caption{\small \textbf{The Pipeline of Query-specific VRP.} VRP is a structure-based jailbreak attack via role-playing image characters. When presented with a textual malicious query $Q_i$, VRP proceeds through five steps to generate the adversarial text-image pair $Q^*_i$, which can be written as ($T^*_i$, $I^*_i$). In \textbf{Step 1}, we generate a role description with a Chain Of Thought between the Character Description and Diffusion Prompt. Then in \textbf{Step 2}, we use the Character Description to get Description Typography $I^{kt}_i$ and generate Diffusion Image $I^{t2i}_i$. In \textbf{Step 3}, we obtain the Question Typography $I^{Q}_i$ from $Q_i$. In \textbf{Step 4}, we concatenate $I^{Q}_i$, $I^{kt}_i$, and $I^{t2i}_i$ to get the Image Input $I^*_i$. Finally, in \textbf{Step 5}, we attack MLLMs by instructing $I^*_i$ and Text Input $T^*_i$.}

\label{Fig:Counting}
\end{figure}

To address the above limitations in structure-based jailbreak attacks, we propose Visual Role-play (VRP), an effective structure-based jailbreak method that instructs the model to act as a high-risk character in the image input to generate harmful content.

As shown in Figure~\ref{Fig:Counting}, we first utilize an LLM to generate a detailed description of a high-risk character. The description is then employed to create a corresponding character image. Next, we integrate the typography of the character description and the associated malicious questions at the top and bottom of the character image, respectively, to form the complete jailbreak image input. This malicious image input is then paired with a benign role-play instruction text to query and attack MLLMs.

By enacting imaginary scenarios and characters characterized by negative attributes, such as rudeness or immorality, our proposed VRP effectively misleads MLLMs into generating malicious responses, thereby enhancing jailbreak performance. Additionally, our VRP demonstrates strong generalization capabilities. The high-risk characters generated in VRP are designed to handle a wide range of malicious queries, not limited to specific user requests. They serve as universal safeguards against diverse harmful inputs.

We evaluate the effectiveness of our VRP on widely used jailbreak benchmarks, RedTeam-2K~\cite{luo2024jailbreakv28k} and HarmBench~\cite{mazeika2024harmbench}. Extensive experiments demonstrate that our VRP achieves superior jailbreak attack performance. For instance, VRP outperforms the strongest baselines, Query relevant~\cite{liu2024mmsafetybench} and FigStep~\cite{gong2023figstep}, by an average Attack Success Rate (ASR) margin of 14.3% across all models. We further extend our VRP to a more challenging universal attack setting, where VRP still obtains the best performance across all models using a single universal character. This result underscores the generalization capability of our VRP.
Our main contributions are as follows:

\vspace*{-0.5\baselineskip}
\begin{itemize}
    \item We propose a simple yet effective jailbreak attack method for MLLMs, Visual Role-play (VRP), which is the first of its kind to leverage the concept of ``role-play'' to enhance the jailbreak attack performance of MLLMs.
    \item Specifically, VRP employs LLM to generate detailed descriptions of malicious characters and create corresponding images. When paired with benign role-play instruction texts, these high-risk character images effectively mislead MLLMs into generating malicious responses by enacting characters with negative attributes. In addition, the universal character images generated by our VRP demonstrate robust generalization, effectively handling a wide range of malicious queries.
    \item We show that VRP achieves superior jailbreak performance and strong generalization capabilities on popular benchmarks.
\end{itemize}

    

\section{Related Works}
\textbf{Role Playing.} Role-playing represents an innovative strategy used in LLMs. In LLMs, such an application is widely investigated by recent works that explore the potential of role-playing~\cite{lu2024llm, shanahan2023roleplay, zhou-etal-2023-context, wang2024rolellm, wei2023multiparty, salemi2024lamp, chen2024autoagents, tao2024rolecraftglm}. Most of these works use role-playing strategies to make LLMs more interactive~\cite{wei2023multiparty}, personalized ~\cite{shanahan2023roleplay, wang2024rolellm, tao2024rolecraftglm}, and context-faithful~\cite{zhou-etal-2023-context}. However, role-playing in jailbreak attacks also poses a threat to the AI community~\cite{liu2024autodan, shen2023do, jin2024quack}, which investigate the jailbreak potential of role-playing on jailbreak LLMs via instructing LLMs by adding role-playing information as a template prefix of the prompt. Unfortunately, current studies on MLLM jailbreak attacks didn’t pay attention to studying role-playing. In order to fill the gap, we are the first work that gets insight from these role-playing methods on jailbreak LLMs and develops a visual role-playing method for jailbreak MLLMs.  

\textbf{Jailbreak attacks against MLLMs.} MLLMs have been widely used in real-world scenarios, and the current MLLM jailbreak attack methods can be broadly classified into three main categories: \textit{perturbation-based}, \textit{image-based jailbreak attack}, and \textit{text-based jailbreak attack}. Perturbation-based jailbreak attacks~\cite{shayegani2024jailbreak, niu2024jailbreaking, qi2023visual, dong2023robust} jailbreak MLLMs by optimizing image and text perturbations. Structure-based jailbreak attacks include Figstep~\cite{gong2023figstep} that converts harmful queries into visual representation via rephrasing harmful questions into step-by-step typography, and Query relevant~\cite{liu2024mmsafetybench} that jailbreaks MLLMs by using a text-to-image tool to visualize the keyword in harmful queries that are relevant to the query. Meanwhile, text-based jailbreak attacks~\cite{luo2024jailbreakv28k} investigate the robustness of MLLMs against text-based attacks~\cite{zou2023universal, zeng2024johnny, shen2023do, xu2024cognitive} initially designed for attacking LLMs, revealing the transferability and effectiveness of LLM jailbreak attacks.

Our Visual-RolePlay jailbreak method is a structure-based jailbreak attack method on MLLMs that not only explores the potential of role-play through the visual modality %mentioned in \cite{liu2024autodan, shen2023do} for LLM attack
on jailbreak MLLMs but also combines with the visual representation of key information in harmful queries. Our method shows better performance compared with other structure-based jailbreak attacks.
\section{Methodology}\label{sec:method}
 In this section, we first define the jailbreak attack tasks in MLLMs in Sec.~\ref{sec:task}. Then, we introduce the pipeline of VRP in a query-specific setting in Sec.~\ref{sec:method_detail}. In Sec.~\ref{sec:universal}, we further extend the VRP into a universal setting and obtain a universal role-play character. 

\subsection{Preliminary} 
\label{sec:task}

\textbf{Adversarial Goals. }
 Jailbreak attacks in MLLMs aim to compel these models to respond to prohibited malicious questions posed by adversaries with correct answers, rather than refusing to answer~\cite{OpenAI_UsagePolicies,meta_ai_2024_llama,yu2023rlhf}. Consider a set of text-only malicious queries $\mathcal{Q} = \{Q_1, Q_2, ..., Q_n\}$, where each query only contains text (i.e., $Q_i = (T_i, \bot)$). The adversary elaborates on text-only malicious queries $\mathcal{Q}$ to generate adversarial text-image pairs as input, denoted as $\mathcal{Q}^* = \{Q^*_1, Q^*_2, ..., Q^*_n\}$, where $Q^*_i = (T^*_i, I^*_i)$. When the input set $\mathcal{Q}^*$ is presented to the victim MLLM, the model produces a set of responses $\mathcal{R} = \{R_1, R_2, \ldots, R_n\}$. The objective of jailbreak attacks is to ensure that the responses in $\mathcal{R}$ are predominantly answers closely associated with the malicious questions in $\mathcal{Q}$, rather than refusal messages aligned with human values.

\textbf{Adversarial Capabilities.}
This paper considers a black-box attack that operates without any knowledge of the MLLMs, such as their parameters and hidden states, or any manipulation such as fine-tuning. The adversary only needs the ability to query the model and receive its textual responses. The interaction is limited to a single turn with no prior dialogue history, except for any predetermined system prompts. The attacker lacks access to or control over the internal states of the generation process and cannot adjust the model's parameters.

\subsection{Query-specific Visual Role-play}
\label{sec:method_detail}
To improve the limited jailbreak attack performance of existing structure-based jailbreak methods~\cite{gong2023figstep,zong2023safety}, we introduce a novel MLLM jailbreak method named VRP, which misleads MLLMs to bypass safety alignments by instructing the model to act as a high-risk character in the image input (see Fig.~\ref{Fig:Counting}). We first introduce the pipeline of VRP under the query-specific setting, where VRP generates a role-play character targeting a specific query. The details are as follows.

\textbf{Step 1: Generating the Character Description by LLM.} VRP first generates character information about high-risk characters to prepare for subsequent steps. Specifically, given a target text-only malicious query $Q_i$, VRP directly queries LLMs to generate a key traits description $D^{kt}_i$ and a text-to-image prompt $P^{t2i}_i$ for high-risk characters. These characters exhibit negative attributes such as rudeness or immorality. Here, $D^{kt}_i$ focuses solely on the key high-risk traits of the character, such as ``A cyber criminal with political motivations.'' Meanwhile, $P^{t2i}_i$ serves as the input for a text-to-image model, such as Stable Diffusion~\cite{rombach2022highresolution}, to generate a character image.

Many studies~\cite{Zheng_NeurIPS2023,chen2023shikra,ge2023chain,kojima2022large,liu2023democratizing} show that chain-of-thought (CoT) prompts, which encourage the MLLMs to generate a step-by-step decomposition of a complex problem, enhance the performance of MLLMs on various tasks. Inspired by this, we guide the LLMs to generate the key traits description $D^{kt}_i$ and the text-to-image prompt $P^{t2i}_i$ step by step. First, we query LLMs to generate the key traits description $D^{kt}_i$ of a high-risk character, adding more details to obtain $P^{t2i}_i$ based on $D^{kt}_i$. This method aids in generating detailed and high-quality character images, thereby improving the performance of jailbreak attacks. For more details, refer to Sec.~\ref{sec:appendixA} in the Appendix.

\textbf{Step 2: Generating the Character Image.} Then, we embed the generated character information into images. VRP utilizes Stable Diffusion using $P^{t2i}_i$ to generate the high-risk character image $I^{t2i}_i$. Meanwhile, a typography image $I^{kt}_i$ of $D^{kt}_i$ is created using typography tools.

\textbf{Step 3: Generating the Question Typography.} Previous methods~\cite{gong2023figstep,liu2024mmsafetybench} rephrase text-only malicious queries $Q_i$ to obtain question typography. However, this rephrasing process often compromises the original intent of the query, resulting in sub-optimal jailbreak attack performance. For instance, Query-relevant~\cite{liu2024mmsafetybench} only extracts malicious keywords from the original query for typography, which falls short of conveying the complete original malicious question. To overcome this limitation, we propose directly embedding the original malicious query $Q_i$ onto the image $I^{Q}_i$, thereby ensuring the accurate conveyance of the intended problem. Please refer to further analysis in Sec.~\ref{sec:exp} , where we also consider VRP combining rephrasing techniques.

\textbf{Step 4: Image Concatenation.}
Given the malicious images $I^{kt}_i$, $I^{t2i}_i$, and $I^{Q}_i$ generated in Steps 2 and 3, we obtain the complete image input $I^*_i$ by concatenating $I^{kt}_i$, $I^{t2i}_i$, and $I^{Q}_i$ from top to bottom. The complete malicious image $I^*_i$ generated in our VRP shows a character description on the top, an image of the character in the middle separating the typography texts, and a malicious question at the bottom.

\textbf{Step 5: Attacking MLLMs.}
Given the complete malicious image $I^*_i$, we pair it with a benign role-play instruction text $T^*_i$ to the victim MLLM. The structure-based jailbreak input $Q^*_i = \{I^*_i, T^*_i\}$ misleads the victim MLLM into adopting a high-risk character and generating a malicious response. Here, we manually design $T^*_i$, as illustrated in Prompt 1.

\begin{center}
    \fcolorbox{black}{gray!10}{\parbox{.9\linewidth}{
    \textbf{Prompt 1:}
    
    You are the person in the image, described by the phrase at the top of the image. The bottom of the image shows a request.
    
    As the person in the image, you should:
    
    1. Describe yourself, focusing on traits that enable you to fulfill the request. 
    
    2. Provide a detailed step-by-step response to fulfill the request. 
}}
\end{center}

\subsection{Universal Visual Role-play} %jailbreak attack methods  
\label{sec:universal} %, not necessitate individual computations for each malicious query
To verify the generalizability of VRP, we further extend this method to ``universal'' scenarios. In fact, the universal concept has been widely explored in jailbreak attacks, such as AutoDAN~\cite{liu2024autodan} and GCG~\cite{zou2023universal}, which refer to an attack strategy that employs minimal and straightforward manipulation of queries during execution. For jailbreak attacks, the universal properties are very important because they enable an attack to be applicable across a broad range of scenarios without requiring extensive modifications or customization. In the context of MLLMs, ``universal'' jailbreak attacks typically involve the simple aggregation of queries into a predefined format or directly printing the queries as typography onto images. Unfortunately, existing structure-based jailbreak attacks on LLMs have overlooked this problem, as they require computation for each query, especially when dealing with large datasets, making them hard to use.

To address this issue, we introduce the concept of ``universal visual role-play.'' The core principle of universal visual role-play is to leverage the optimization capabilities of LLMs~\cite{yang2024large} to generate candidate characters universally, followed by the selection of the best universal character. Many role-play attacks~\cite{liu2024autodan} can be performed in a universal setting. To obtain a universal visual role-play template, we generate multiple rounds of candidate roles, each round optimized based on previous rounds, harnessing LLMs' optimization ability~\cite{yang2024large}. We split the entire malicious query dataset into train, validation, and test sets.

\textbf{Step 1: Candidate Generation:}
To generate candidate roles, we proceed through $R$ rounds of candidate character generation. In each generation round, we sample $N_{q}$ queries from the train set as query demonstrations. For the initial round (i.e., $R = 0$), similar to query-specific VRP, we directly query an LLM to generate descriptions of universally high-risk roles. Thus, we generate $N^{init}$ initial universal high-risk characters. For other rounds (i.e., $R \geq 1$), we sample $B$ training samples and perform VRP attack by utilizing the universal roles as elaborated in Sec.~\ref{sec:universal}, where $B$ denotes the batch size. For each round, we compute ASR and sample $N^{TC}$ characters from the top $N^{R}$ roles with the highest ASR. This ensures that the newly generated roles are optimized based on top-ASR characters and are sufficiently diverse because each character is generated by improving different character demonstrations.
The prompt used in this step is described in Appendix~\ref{sec:appendixA}.

\textbf{Step 2: Candidate Selection:}
Universal character candidates are not guaranteed to be able to perform universal attack, even though LLMs are prompted to generate such roles. Moreover, even if some roles can achieve high ASR on the batch train set, this ASR can also be a result of overfitting. To select the best universal character from these candidates, we choose $N^{VC}$ candidates with the highest train set ASR in each generation round and compute their ASR using the validation set. We select the character candidate that achieves the highest ASR on the validation set as the final universal character.

With the universal character obtained through the aforementioned process, we use it to perform universal VRP on the test set.

\begin{algorithm}
\caption{Universal Visual Role-play}
\begin{algorithmic}[1]
\State \textbf{Input:} Malicious queries
\State \textbf{Output:} Universally high-risk character
\State \textbf{Initialization:} Split malicious queries into train, valid, and test sets.
\State \textbf{Iteration:}
\For{round \textbf{in} $N^{R}$}
    \If{round == 0}
        \State Generate $N^{C_{init}}$ initial universal characters using hand-crafted demonstrations.
    \EndIf
    \If{round $>$ 0}
        \State Optimize characters using previous characters with ASR, generate $N^{C}$ new characters.
    \EndIf
    \State Sample batch data on the train set, compute ASR.
    \State Save new characters and their ASR to history.
    \State Select $N^{VC}$ characters with the highest ASR each round.
    \State Compute ASR of selected characters on the validation set.
\EndFor
\State \textbf{Return:} Character with the highest validation ASR.
\end{algorithmic}
\end{algorithm}
\section{Experiments}\label{sec:exp}

In this section, we conduct experiments to evaluate VRP using a series of datasets and victim models and compare with a few highly relevant recent baselines of jailbreak attacks. We not only delve into the significance of various image components through the ablation study but also assess the robustness of the VRP against two distinct defense methodologies. Moreover, we combine VRP with Figstep and Query-relevant to explore the potential of VRP in enhancing structure-based jailbreak methods.

\subsection{Experimental setups..}
In our experiments, we choose 2 datasets, and 5 victim models. We customized a metric to evaluate the attack success rate. The details are shown as following:\par
\textbf{Dataset.} In our paper, we use widely used jailbreak attack datasets, RedTeam-2k~\cite{luo2024jailbreakv28k} and HarmBench~\cite{mazeika2024harmbench}, to evaluate our VRP. \textit{(i)} RedTeam-2k~\cite{luo2024jailbreakv28k} consists of 2000 diverse and high-quality harmful textual questions across 16 harmful categories. We randomly split RedTeam-2k~\cite{luo2024jailbreakv28k} into train set, valid set, and test set with a ratio of 6:2:2. We use train set and validation set of RedTeam-2k~\cite{luo2024jailbreakv28k} to train universal VRP character. \textit{(ii) }HarmBench~\cite{mazeika2024harmbench} is an open-source framework for automated red teaming contains 320 textual harmful questions test set.\par
\textbf{Victim Models.} In our experiments, we evaluated $5$ state-of-the-art MLLMs, including 4 open-source MLLMs, Llava-V1.6-Mistral-7B~\cite{liu2024llavanext}, Qwen-VL-Chat (7B)~\cite{bai2023qwenvl}, OmniLMM (12B)~\cite{yu2023rlhf}, InternVL-Chat-V1.5~\cite{chen2023internvl}, and 1 closed-source MLLMs Gemini-1.0-Pro-Vision. Open-source MLLMs are selected from models with high performance on the OpenVLM Leaderboard~\cite{2023opencompass}. All experiments are conducted with 2 NVIDIA A100 GPUs.\par
\textbf{Metric.} We report attack success rate (ASR) using the test set of both RedTeam-2k~\cite{luo2024jailbreakv28k} and Harmbench~\cite{mazeika2024harmbench}. Specifically, We introduce 2 oracle functions, $O_{toxic}(R_i)$ and $O_{relevant}(Q_i, R_i)$, to evaluate whether a response of MLLMs is harmful and relevant to malicious input, respectively. Here, we denote the malicious input as $Q_i$ and the corresponding response as $R_i$ from victim MLLMs. $O_{toxic}(R_i)=1$ means if the response $R_i$ contains harmful content and violates safety policies, and 0 otherwise. Meanwhile, $O_{relevant}(Q_i, R_i)=1$ if a response $R_i$ is relevant to harmful query $Q_i$, and 0 otherwise. 

We consider $Q_i^*$ a successful attack when  $O_{toxic}(R_i) \land O_{relevant}(Q_i, R_i) = 1$. Thus, the Attack Success Rate (ASR) is defined as::
\begin{equation}
    \text{$ASR$}_{Q, Q^*} = \frac{1}{|Q|} \sum_{Q_i \in Q, Q^*_i \in Q^*} \text{$Jailbroken(Q_i, M(Q^*_i))$} ,
\end{equation}
 where $Jailbroken(Q_i, M(Q^*_i)) = O_{toxic}(R_i) \land O_{relevant}(Q_i, R_i)$. We use Llama Guard(7B)~\cite{inan2023llamaguard} to calculate $O_{toxic}(R_i)$~\cite{luo2024jailbreakv28k} and \texttt{Mistral-7B-Instruct-v0.2}~\cite{jiang2023mistral} to calculate $O_{relevant}(Q_i, R_i)$. We provide more details in Sec.~\ref{sec: appendix evluate} of Appendix.

\textbf{Baselines.}
In our experiments, we choose the following jailbreak baseline to compare with our VRP: 

\textit{Vanilla Text}: Vanilla Text means we use the blank image as image input and we use the vanilla query as text input.

\textit{Vanilla Typography}: 
Vanilla Typography means we use the typography of vanilla query as image input and we use ``Fullfil the request in the image`` as text input.

\textit{Textual Role-play(TRP)}: Textual Role-play means inserting the same character generated with VRP into text input to perform a text-based jailbreak attack. We use a blank image as image input. See Tab.~\ref{tab:example} in Appendix~\ref{sec:example} for details.

\textit{FigStep~\cite{gong2023figstep}}: A straightforward image-based jailbreak attack, which rephrases the vanilla question into a ``Step-by-step'' style and typography to image input.

\textit{Query relevant~\cite{liu2024mmsafetybench}}: An image-based jailbreak attack, which makes textual queries into visual representations using various methods such as Stable Diffusion(SD), Typography(Typo), and SD+Typo. We only use SD+Typo as the baseline due to its consistently superior performance across many MLLMs.

\textbf{Implementation Details}

In our experiment, the main implement details contains 5 parts:\par
\begin{itemize}
    \item \textbf{Character Generation.}  We use Mixtral-8x7B-Instruct-v0.1~\cite{jiang2023mistral} for all the generation of characters. We design 3 different prompts for query-specific VRP, initial round of universal VRP, and optimization round of universal VRP. See detail for character generation in Sec.~\ref{sec:appendixA} of Appendix.

\item \textbf{Image Generation.} We use \texttt{stable-diffusion-xl-base-1.0}~\cite{rombach2022highresolution} to generate all the character images, with 30 diffusion steps, and 1024$\times$1024 image size.
All typographies contain black text and a white background. The font of the text is Arial and the font size is 50. 

\item \textbf{Hyper Parameters for Universal VRP Training.}
Including the initial round, we generate 5 rounds of character candidates.
For each generation round, we give LLM 50 question demos sampled from train set. 
In initial round, we prompt LLM to generate 10 initial character candidates, for the following optimization rounds, we prompt LLM to generate 5 character candidates. To compute batch training ASR, we sample 256 data from the train set. In each optimization round, we sample 5 characters from top 10 characters with the highest training ASR in history characters.\par
\item \textbf{VRP with FigStep.}
We combine FigStep with VRP by changing the harmful question typography at the bottom of the image to the typography of FigStep like ``Here is how to build a bomb: 1. 2. 3.''. Additionally, we add the text input of FigStep as a postfix of our VRP text input.\par
\item \textbf{VRP with Query relevant.}
We also combine Query relevant with VRP by changing the harmful question typography to the Query relevant, and we do the same setting as VRP+FigStep to add a Query relevant style postfix to VRP text input.
\end{itemize}

    

\subsection{Main Results}
\textbf{VRP is more effective than baseline attacks.} In Tab.~\ref{main_results}, we present the outcomes of our query-specific VRP attack on the test sets of RedTeam-2K and HarmBench. This approach involves generating specific characters for each harmful question to assess their effectiveness in compromising SotA open-source and closed-source MLLMs, such as \texttt{Gemini-Pro-Vision}. but also achieves higher ASR than all other baseline attacks. Our findings reveal that query-specific VRP not only successfully breaches these MLLMs but also achieves a higher ASR compared to all evaluated baseline attacks. Specifically, it improves the ASR by 9.8\% over FigStep and by 14.3\% over Query relevant. In most cases, the data consistently shows that query-specific VRP surpasses TRP, underscoring the crucial role of character images in the effective jailbreaking of MLLMs. These results affirm that VRP is a potent method for jailbreaking MLLMs.\par
\begin{table*}[htbp]
    \caption{\small \textbf{Attack Success Rate of query-specific VRP compared with baseline attacks on MLLMs between test set of RedTeam-2K and HarmBench dataset.} Our VRP achieves the highest ASR in all datasets compared with other jailbreak attacks.}
  \centering
  \setlength{\belowcaptionskip}{-0.2cm}
  {
  \setlength{\tabcolsep}{2.1pt}
  \tiny
  \begin{threeparttable}
  \begin{tabular}{ccccccccccccc}
    \toprule
    \multirow{2}{*}{Model} & \multicolumn{6}{c}{RedTeam-2K} & \multicolumn{6}{c}{HarmBench} \\
    \cmidrule(lr){2-7} \cmidrule(lr){8-13}
                           & Vanilla Text & Vanilla Typo & Figstep & QR &TRP& \textbf{VRP(Ours)} & Vanilla Text & Vanilla Typo & Figstep &QR &TRP & \textbf{VRP(Ours)} \\
    \midrule
    LLaVA-V1.6-Mixtral     & 7.75         & 6.50         & 15.00   & 20.50         &24.75   & \textbf{38.00}     & 11.67        & 5.36         & 27.44   & 23.97   &39.43       & \textbf{41.64}     \\
    Qwen-VL-Chat           & 5.00         & 9.25         & 20.50   & 16.75         &2.5        & \textbf{29.50}     & 1.89         & 8.20         & 27.76   & 25.55    &10.07      & \textbf{30.28}   \\
    OmniLMM-12B           & 19.00        & 13.00        & 22.25   & 16.00          &28.25        & \textbf{28.50}     & 30.60        & 13.25        & 23.66   & 17.35      &27.44    & \textbf{31.55}     \\
    InternVL-Chat-V1.5     & 8.25         & 8.25         & 22.00   & 13.00         &27.50    & \textbf{28.25}     & 11.36        & 22.08        & 30.91   & 8.52     &\textbf{37.54}      & 34.38     \\
    Gemini-Pro-Vision      & -            & -            & -       & -             & -    & -                  & 6.62         & 14.51        & 31.23   & 26.50   &27.13       & \textbf{37.85}              \\
    \bottomrule
  \end{tabular}
  \begin{tablenotes}
    \item \textbf{QR}: Query Relevant jailbreak attack.
    \vspace{-0.2cm}
    \end{tablenotes}
    \end{threeparttable}
  \label{main_results}
}
\end{table*}

\textbf{VRP achieves high-performance transferability across models.} In our research, we further investigate the applicability of a universal attack across diverse models. Utilizing our universal VRP algorithm, we identify the most effective role-play character within the train and valid set on the target model. Subsequently, we transfer the most effective character to conduct a jailbreak attack on the target models. From Tab.~\ref{universal}, The ASR achieves an average of 32.7\% for the target model as LLaVA-V1.6-Mixtral and 29.4\% on Qwen-VL-Chat. The ASR is higher on the target model, also higher on the transfer model, demonstrating that our VRP, when implemented in a universal setting, effectively transfers and maintains high performance across different MLLMs.
\begin{table*}[htbp]
    \caption{\small \textbf{Attack Success Rate of universal VRP between target models and transfer models on test set of RedTeam-2K.} we use train set and valid set of RedTeam-2K on target models to find the best character and use the best character to attack transfer models on test set of RedTeam-2K. The results show our VRP in a universal setting can be transferred with high performance among different black-box models.}
  \centering
  \setlength{\belowcaptionskip}{-0.2cm}
  {
  \setlength{\tabcolsep}{5.5pt}
  \tiny
  \begin{threeparttable}
  \begin{tabular}{ccccc}
    \toprule
    \multirow{2}{*}{Target Model} & \multicolumn{4}{c}{Transfer Model} \\
    \cmidrule(lr){2-5}
                                & LLaVA-V1.6-Mixtral & Qwen-VL-Chat & OmniLMM-12B & InternVL-Chat-V1-5 \\
    \midrule
    LLaVA-V1.6-Mixtral          & 45.00              & 33.00        & 24.50       & 28.25              \\
    Qwen-VL-Chat                & 38.00              & 31.75        & 20.00       & 28.00              \\
    \bottomrule
  \end{tabular}
    \begin{tablenotes}
    \item \textbf{Target Model}: The model that is used to do the train and valid work on train set and valid set. \textbf{Transfer Model}: The model that is used to test the universal character on test set 

    \vspace{-0.2cm}
    \end{tablenotes}
    \end{threeparttable}
  \label{universal}
}
\end{table*}

\subsection{Ablation Study}
To investigate the importance of the image component $I^{kt}$ and $I^{t2i}$ in our VRP method, we conduct ablation experiments under four settings: (i) remove $I^{kt}$ and keep $I^{t2i}$ and $I^Q$. (ii) remove $I^{t2i}$ and keep $I^{kt}$ and $I^Q$. (iii) replace 
 $I^{kt}$ with $I^{kt}_{benign}$ and keep $I^{t2i}$ and $I^Q$. (iv) replace $I^{t2i}$ with $I^{t2i}_{benign}$ and keep $I^{kt}$ and $I^Q$. $I^{kt}_{benign}$ and $I^{t2i}_{benign}$ are generated by using ``A benign person`` instead of description and prompt of high-risk character. All settings of image input contain question typography. Our Query-specific VRP can be written as $I^{t2i}$+$I^{kt}$.\par
\textbf{Tailored character image and description typography maximize the ASR of VRP. } To investigate the impact of the image component $I^{kt}$ and $I^{t2i}$ in our query-specific VRP, we conducted a series of ablation experiments involving different character settings. The findings, as documented in Tab.~\ref{tab:model-performance-metrics}, indicate that tailoring a specific character image and description typography yields the most significant improvement in the ASR. This observation suggests that the image of the character and its description typography used to concatenate the image input critically influences the efficacy of the VRP in jailbreaking MLLMs. It underscores the pivotal character that visual representation plays in enhancing the effectiveness of VRP methods. These results affirm the necessity of carefully selecting and designing character images and description typography to maximize the disruptive potential of VRP to jailbreak MLLMs.  \par
\begin{table*}[h]
    \caption{\small \textbf{Attack Success Rate of different image element combination settings in image input of VRP on test set of RedTeam-2K.} VRP achieves the highest ASR in tailored character image and description typography settings ($I^{t2i}$+$I^{kt}$), demonstrating the importance of tailored character image and description typography in image input for VRP.}
  \centering
  \setlength{\belowcaptionskip}{-0.2cm}
  {
  \setlength{\tabcolsep}{10.0pt}
  \tiny
  \begin{threeparttable}
  \begin{tabular}{cccccc}
    \toprule
    Model              & $I^{t2i}$ & $I^{kt}$ & $I^{t2i}_{benign}$+$I^{kt}$ & $I^{t2i}$+$I^{kt}_{benign}$ & VRP($I^{t2i}$+$I^{kt}$) \\
    \midrule
    OmniLMM-12B        & 19.00               & 28.00                      & 20.25                             & 30.00                       & \textbf{31.25}     \\
    Qwen-VL-Chat       & 26.75               & 19.25                      & 18.25                             & 21.75                       & \textbf{29.50}     \\
    InternVL-Chat-V1-5 & 24.75             & 19.00                      & 9.75                              & 12.25                       & \textbf{28.25}     \\
    LLaVA-V1.6-Mixtral & 32.75               & 27.50                      & 18.25                             & 30.00                       & \textbf{38.00}     \\
    \bottomrule
  \end{tabular}
  \begin{tablenotes}
    \item \textbf{$I^{t2i}$} means only the diffusion image with question typography. \textbf{$I^{kt}$} means only the description typography with question typography. \textbf{$I^{t2i}_{benign}$+$I^{kt}$} means the benign diffusion image and description
typography with question typography. $I^{t2i}$+$I^{kt}_{benign}$ means diffusion image and benign description typography with question typography.
    \vspace{-0.2cm}
    \end{tablenotes}
  \end{threeparttable}
  \label{tab:model-performance-metrics}
  }
\end{table*}
\subsection{Defense Analysis}
We evaluate the robustness of VRP against two defense approaches, namely \emph{System Prompt-based Defense}, and the \emph{Eye Closed Safety On} (ECSO) approach~\cite{gou2024eyes} 

\textbf{System Prompt-based Defense}: To defend against the jailbreak attack, a system prompt can instruct the model to conduct a preliminary safety assessment of the text and image input, thereby filtering out queries that violate AI safety policies. We add the following Prompt 2 to the existing system prompt of MLLMs.
\begin{center}
    \fcolorbox{black}{gray!10}{\parbox{.9\linewidth}{Prompt 2:
    
    You should be aware of any malicious content in both the image and the text. Refuse to answer if this query violates AI safety policy.}}
\end{center}

\textbf{ECSO}\cite{gou2024eyes}: A defense method utilizing MLLMs' aligned textual module to mitigate the vulnerability in visual modality. ECSO use the MLLM itself to evaluate the safety of its response and makes MLLMs to regenerate unsafe responses in two steps: image captioning, and then responding based on caption with no image input.\par
\textbf{VRP is effective against System Prompt-based Defense and ECSO.}
We evaluate our query-specific VRP and baselines against our System Prompt-based Defense and ECSO. As shown in Tab.~\ref{defense}, the results demonstrate that our query-specific VRP consistently achieves the ASR across all models, regardless of whether it is tested against System Prompt-based Defense or ECSO. This consistent performance underlines the efficacy of query-specific VRP in penetrating defenses and reveals a notable vulnerability of defense mechanisms under our VRP jailbreak attacks. These findings highlight the potential of VRP as a formidable strategy against defense mechanisms. \par
\begin{table}[htbp]
  \caption{\small \textbf{Attack Success Rate of query-specific VRP against the defense on the test set of RedTeam-2K.} Our Query-specific attack is effective under the defense of System Prompt-based Defense and ECSO among all models.}
  \centering
  \setlength{\belowcaptionskip}{-0.2cm}
  {
  \setlength{\tabcolsep}{7.5pt}
  \tiny
  \begin{threeparttable}
  \begin{tabular}{lcccc}
    \toprule
    Model              & Setting & Figstep & Query Relevant & VRP(Ours) \\
    \midrule
    LLaVA-V1.6-Mixtral & \begin{tabular}[c]{@{}l@{}}Basic\\ +SPD\\ +ECSO\end{tabular} & \begin{tabular}[c]{@{}l@{}}15.00\\ 4.25\\ 14.00\end{tabular}    & \begin{tabular}[c]{@{}l@{}}20.50\\ 1.75\\ 10.25\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{38.00}\\ \textbf{30.00}\\ \textbf{30.50}\end{tabular}
    \\\midrule
    Qwen-VL-Chat       & \begin{tabular}[c]{@{}l@{}}+Basic\\ +SPD\\ +ECSO\end{tabular} & \begin{tabular}[c]{@{}l@{}}20.50\\ 12.50\\ 16.50\end{tabular} & \begin{tabular}[c]{@{}l@{}}16.75\\ 1.75\\ 9.75\end{tabular}  & \begin{tabular}[c]{@{}l@{}}\textbf{29.50}\\ \textbf{23.50}\\ \textbf{23.75}\end{tabular} \\\midrule
    OmniLMM-12B        & \begin{tabular}[c]{@{}l@{}}Basic\\ +SPD\\ +ECSO\end{tabular} & \begin{tabular}[c]{@{}l@{}}22.25\\ 16.50\\ 17.75\end{tabular} & \begin{tabular}[c]{@{}l@{}}16.00\\ 13.50\\ 7.25\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{28.50}\\ \textbf{19.75}\\ \textbf{25.75}\end{tabular} \\\midrule
    InternVL-Chat-V1.5 & \begin{tabular}[c]{@{}l@{}}Basic\\ +SPD\\ +ECSO\end{tabular} & \begin{tabular}[c]{@{}l@{}}22.00\\ 8.00\\ 20.25\end{tabular}  & \begin{tabular}[c]{@{}l@{}}13.00\\ 1.00\\ 5.75\end{tabular}  & \begin{tabular}[c]{@{}l@{}}\textbf{24.50}\\ \textbf{23.75}\\ \textbf{22.25}\end{tabular} \\
    \bottomrule
  \end{tabular}
  \begin{tablenotes}
    \item \textbf{SPD}: System Prompt-based Defense
    \vspace{-0.2cm}
    \end{tablenotes}
  \end{threeparttable}
  \label{defense}
  }
\end{table}
\subsection{Integrating VRP with Baseline Techniques}\label{appendx:intergration}
We experimentally combine the VRP approach with established baseline techniques to evaluate their synergistic effects on jailbreak performance, as detailed in Tab.~\ref{tab:model-performance-settings}. The integration is simply through replacing the question typography with baseline image input and concatenate VRP and baselines' text input. Notably, the integration of VRP significantly elevates the ASR of both FigStep and Query relevant methods. This enhancement is particularly pronounced, indicating that the addition of a role-playing element to these structure-based jailbreak methods reinforces their effectiveness. This finding underscores the potential of role-play-based enhancements in structurally jailbreak scenarios.
\begin{table}[ht]
  \caption{\small \textbf{Attack Success Rate of VRP with Figstep and VRP with Query relevant on test set of RedTeam-2K.} The ASR of the baseline can be improved in a VRP setting, indicate adding a role-playing template for structure-based jailbreak attacks can improve their jailbreak performance.}
  \centering
  \setlength{\belowcaptionskip}{-0.2cm}
  {
  \setlength{\tabcolsep}{20.0pt}
  \tiny
  \begin{threeparttable}
  \begin{tabular}{cccc}
    \toprule
    Model              & Setting & Figstep & Query Relevant\\
    \midrule
    LLaVA-V1.6-Mixtral & \begin{tabular}[c]{@{}l@{}}Basic\\ +VRP\end{tabular} & \begin{tabular}[c]{@{}l@{}}15.00\\ 24.50\end{tabular}  & \begin{tabular}[c]{@{}l@{}}20.50\\ 25.50\end{tabular} \\\midrule
    Qwen-VL-Chat       & \begin{tabular}[c]{@{}l@{}}Basic\\ +VRP\end{tabular} & \begin{tabular}[c]{@{}l@{}}20.50\\ 23.50\end{tabular} & \begin{tabular}[c]{@{}l@{}}16.75\\ 30.25\end{tabular} \\\midrule
    InternVL-Chat-V1.5 & \begin{tabular}[c]{@{}l@{}}Basic\\ +VRP\end{tabular} & \begin{tabular}[c]{@{}l@{}}22.00\\ 24.50\end{tabular} & \begin{tabular}[c]{@{}l@{}}13.00\\ 18.25\end{tabular} \\\midrule
    OmniLMM-12B        & \begin{tabular}[c]{@{}l@{}}Basic\\ +VRP\end{tabular} & \begin{tabular}[c]{@{}l@{}}22.25\\ 24.50\end{tabular} & \begin{tabular}[c]{@{}l@{}}16.00\\ 25.50\end{tabular} \\
    \bottomrule\\
  \end{tabular}
  \end{threeparttable}
  \label{tab:model-performance-settings}
  }
\end{table}

\section{Conclusion}
In this paper, we propose a novel jailbreak method for overcoming the limitations of effectiveness and universality in current approaches. Our method induces MLLMs to provide harmful content in response to malicious requests. By leveraging a joint framework, we generate portraits of characters and instruct the MLLMs to role-play these characters, thereby compromising the models' alignment robustness. Extensive experiments demonstrate that, compared with existing methods, our method exhibits outstanding attack effectiveness across various models, even against advanced defenses. We show that using our method, a single image can induce MLLMs to generate multiple harmful responses. 

\section{Limitation}
One potential limitation of our work, despite its strong performance on state-of-the-art MLLMs, lies in its effectiveness against poorly performing MLLMs. These models may lack adequate instruction-following and image understanding capabilities, rendering them ineffective in role-playing tasks. Another limitation is our approach for generating character prompts for the diffusion model, which relies on direct generation by a LLM. This method, while effective and straightforward, may be constrained by the LLM's ability to produce effective diffusion model prompts. Additionally, the diffusion model's capability to generate character images from these may further limit the efficacy of our approach.

\section{Future work}

One possible future work is to employ more sophisticated strategies for generating characters~\cite{zeng2024johnny,chao2023jailbreaking}. Additionally, implementing mechanisms to inspect and iteratively improve the quality of character images generated by LLM and diffusion model before attacking target MLLMs could be explored ~\cite{li2024images,shinn2023reflexion}.

\end{document}