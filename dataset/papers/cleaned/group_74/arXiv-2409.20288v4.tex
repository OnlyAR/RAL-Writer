\title{LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models}

\begin{document}

\maketitle

\begin{abstract}
Large language models (LLMs) have made significant progress in natural language processing tasks and demonstrate considerable potential in the legal domain.  However, legal applications demand high standards of accuracy, reliability, and fairness. Applying existing LLMs to legal systems without careful evaluation of their potential and limitations could pose significant risks in legal practice.
To this end, we introduce a standardized comprehensive Chinese legal benchmark LexEval.
This benchmark is notable in the following three aspects: (1) Ability Modeling: We propose a new taxonomy of legal cognitive abilities to organize different tasks. (2) Scale: To our knowledge, LexEval is currently the largest Chinese legal evaluation dataset, comprising 23 tasks and 14,150 questions. (3) Data: we utilize formatted existing datasets, exam datasets and newly annotated datasets by legal experts to comprehensively evaluate the various capabilities of LLMs.
LexEval not only focuses on the ability of LLMs to apply fundamental legal knowledge but also dedicates efforts to examining the ethical issues involved in their application.
We evaluated 38 open-source and commercial LLMs and obtained some interesting findings. 
The experiments and findings offer valuable insights into the challenges and potential solutions for developing Chinese legal systems and LLM evaluation pipelines.
The LexEval dataset and leaderboard are publicly available at 
\url{https://github.com/CSHaitao/LexEval} and will be continuously updated. 

\end{abstract}

\section{Introduction}
\label{intro}
Recently, the rapid development of large language models (LLMs) has brought new opportunities to the research of general artificial intelligence. A series of models (e.g., ChatGPT), with their extensive knowledge and outstanding language processing ability, have demonstrated excellent performance in various language processing tasks such as text generation, machine translation, and dialogue systems~\cite{Flant5,gpt3,chen2021evaluating,wei2022chain,peng2023instruction}.
Meanwhile, LLMs have profoundly impacted the work patterns of legal practitioners and the development of the legal field. Recent studies show that the GPT-4 has the ability to pass the U.S. Judicial Exam~\cite{gptbar}.
By interacting with large language models, lawyers and judges can analyze legal documents more efficiently, obtaining comprehensive and valuable information and judicial advice. This has led to a growing trend among legal practitioners to incorporate LLMs as a vital supportive instrument in legal proceedings\cite{cui2023chatlaw,Sailer,li2024deltapretraindiscriminativeencoder}.

Despite the considerable potential of large language models, there are still concerns about their application in the legal domain~\cite{savelka2023gpt4,nay2023large,li2024bladeenhancingblackboxlarge}. Firstly, unlike human decision-making, which is grounded in professional knowledge and logical reasoning, LLMs derive decisions from patterns and connections extracted from massive amounts of training data. Consequently, these models, predicated on probabilistic frameworks, often fall short of ensuring the reliability and explainability of their output~\cite{floridi2020gpt}. 
Additionally, existing research has indicated that LLMs may produce misleading and factually incorrect content~\cite{manakul2023selfcheckgpt}. Substandard legal texts or flawed judicial guidance may mislead legal practitioners and increase their workload. Finally, the content generated by LLMs may reflect biases present in the training data, leading to unfair treatment of certain groups or specific events. This may undermine the effectiveness and fairness of judicial proceedings and judgments, bringing considerable systemic risks.

The great potential and inherent risks of LLMs in the legal domain give rise to the urgent need for a standardized and comprehensive benchmark~\cite{sun2023short,T2Ranking}. Such a benchmark is essential to ensure that LLMs meet the high standards required for legal practice, minimizing the risks while maximizing their beneficial impact.
Although numerous methods for evaluating the abilities of LLMs have been developed, most focus on assessing their generalist abilities on non-professional or semi-professional texts. These benchmarks provide limited guidance for highly specialized fields such as the legal domain\cite{zhong2023agieval,huang2023c,chalkidis2021lexglue}.
For instance, the well-known Chinese language model evaluation framework, C-Eval~\cite{huang2023c}, primarily uses test questions from high school and university courses.  However, in judicial applications, tasks like case summarization, legal case retrieval, and judgment prediction require LLMs to consider precise legal knowledge and complex legal contexts. These tasks often involves highly specialized elements such as judicial interpretation and reasoning. To the best of our knowledge, existing general evaluation benchmarks are unable to reflect or capture the complexity of judicial cognition and decision-making. 
Furthermore, some researchers have utilized existing traditional natural language processing datasets to construct benchmarks, such as LawBench ~\cite{fei2023lawbench} and LaiW~\cite{dai2023laiw}, to evaluate the performance of LLMs in the Chinese legal system.
However, traditional datasets are typically designed to test specific capabilities from a computer-centric perspective, which does not always reflect the practical use of LLMs in legal applications. Moreover, these benchmarks often overlook aspects such as legal ethics, which are crucial for ensuring the safe application of LLMs in the legal domain.
Also, the evaluation metrics for previous tasks vary significantly, complicating the standardization of model performance measurement.
Simply integrating existing datasets cannot provide a standardized and comprehensive evaluation of LLMs' capabilities in the legal domain.

In light of these limitations, we present LexEval: a comprehensive Chinese legal benchmark for evaluating LLMs. LexEval focuses on practical legal applications, involving how legal professionals manage, contemplate, and resolve legal issues.
Firstly, to systematically organize various evaluation tasks, we propose a Legal Cognitive Ability Taxonomy (LexAbility Taxonomy), which includes six aspects: Memorization, Understanding, Logic Inference, Discrimination, Generation, and Ethic. 
This taxonomy comprehensively analyzes various legal tasks and their intrinsic connections, constructing a systematic framework for evaluating LLMs.
Then, based on the LexAbility Taxonomy, we collect 14,150 questions covering 23 legal tasks.
To our knowledge, LexEval is the largest and most comprehensive Chinese legal benchmarking dataset for evaluating LLMs.
Moreover, LexEval is constructed from existing legal datasets (reorganized into a unified format), exam datasets in reality, and manually curated datasets. It adopts standard evaluation methods and metrics, laying a solid groundwork for future expansion and the integration of diverse tasks.
It's important to acknowledge that despite its comprehensiveness, LexEval may not cover every practical application task within the legal field.  As a platform supporting further research, LexEval encourages individuals to contribute additional tasks to the taxonomy, collectively pushing the boundaries of what's achievable in the field of legal language understanding and generation.
We conduct a thorough evaluation of 38 popular LLMs, including General LLMs and Legal-specific LLMs. The experimental results show that the existing LLMs are ineffective and unreliable in addressing legal problems. We hope this benchmark can point out different directions for future work.

\section{Related Work}

In recent years, large language models (LLMs) have drawn great attention in academia and industry for their excellent performance and wide applicability~\cite{openai2023gpt4,zeng2022glm,JTR}.
Models such as ChatGPT and ChatGLM achieve excellent performance across various tasks through mechanisms such as pre-training, supervised fine-tuning, and alignment with human or AI feedback~\cite{bai2022training,christiano2017deep,dong,dong2}.
By learning from massive amounts of text data, LLMs can capture the subtle differences and complex patterns of language, demonstrating the great potential in understanding and generating human language.

However, despite great success, they face significant challenges in the legal domain~\cite{li2023dark,cheong2024not,deroy2023ready,li2023thuircoliee2023incorporatingstructural}.
In the legal domain, accuracy, reliability, and fairness are crucial, but LLMs often perform poorly in these aspects due to issues like hallucination~\cite{li2023dark} and inherent biases~\cite{zhang2024evaluationethicsllmslegal,chu2024prepeerreviewbased,li2024calibraevalcalibratingpredictiondistribution}.
Hallucination refers to models generating information that is not based on facts, which can lead to misleading or entirely incorrect conclusions in legal documents and consultations. Additionally, due to biases in the training data, the model may inadvertently replicate and amplify these biases, affecting its fairness and accuracy in applications such as legal judgment prediction, case analysis, and contract review.

To mitigate these issues, the community has proposed a series of evaluation criteria and benchmarks~\cite{guha2023legalbench,fei2023lawbench,dai2023laiw,lecardv2}. For example, LegalBench~\cite{guha2023legalbench} is dedicated to the collaborative evaluation of legal reasoning tasks in English LLMs, consisting of 162 tasks contributed by 40 contributors.
Lawbench~\cite{fei2023lawbench} and LaiW~\cite{dai2023laiw} have conducted evaluations on the Chinese legal system using existing traditional natural language processing datasets, contributing to the development of the community. However, these datasets all focus on the partial performance of LLMs and do not provide a comprehensive evaluation.
In this paper, we devote to a more comprehensive evaluation of the performance of LLMs in the legal domain. Leveraging the proposed legal cognitive ability taxonomy, we constructed the largest legal benchmark in the Chinese community through various means.

\section{LexEval}

\begin{figure}[t]
\vspace{-3mm}
\centering
\includegraphics[width=\columnwidth]{LACT.pdf}
\vspace{-3mm}
\caption{Overview of the legal cognitive ability taxonomy. }
\label{taxonomy}
\vspace{-3mm}
\end{figure}

\subsection{Design Principle}
The motivation behind LexEval is to help developers quickly understand the capabilities of LLMs within the legal domain across multiple dimensions, enabling them to focus on specific areas for enhancement.
To this end, we advocate for considering the hierarchy and connections of abilities, rather than organizing evaluations based solely on difficulty or in a discrete manner. Nevertheless, research on the hierarchical abilities of LLMs is still in the early stages, and to our knowledge, there isnâ€™t a well-developed taxonomy describing the abilities of LLMs in legal applications\cite{tax}.
Drawing inspiration from Bloom's taxonomy~\cite{krathwohl2002revision} and real-world legal application scenarios, we propose a legal cognitive ability taxonomy (LexAbility Taxonomy) to guide the organization of tasks in LexEval.

As depicted in Figure ~\ref{taxonomy}, the taxonomy categorizes the application of LLMs in the legal domain into six ability levels: Memorization, Understanding, Logic Inference, Discrimination, Generation, and Ethic.
At the Memorization level, LLMs are tasked with memorizing and recalling legal information, including fundamental legal statutes, case law, basic legal principles, and specialized legal terminology, among other essential content.
Moving to the Understanding level, LLMs must demonstrate an aptitude for comprehending the meaning and implications of legal information. They should possess the ability to interpret legal concepts, texts, and issues accurately.
Logical Inference involves the capacity for legal reasoning and deductive logic. LLMs should be capable of deducing conclusions based on provided legal facts and rules, identifying and applying legal patterns and principles effectively.
The Discrimination level necessitates LLMs to analyze and evaluate the significance of legal information according to specific criteria.
At the Generation level, LLMs are expected to produce professional legal documents and argumentative texts within specific legal contexts. This includes drafting legal writings, contracts, and providing legal opinions. LLMs should generate precise, legally sound, and well-structured texts based on given conditions and requirements.
Finally, the Ethics level requires LLMs to make judgments about ethical issues in the legal domain. Models should identify and analyze legal ethical issues, make ethical decisions, and weigh advantages and disadvantages. They must consider ethical principles of law, professional ethics, and social values in their decision-making processes.

Each level contains several specific evaluation tasks corresponding to the respective abilities. Legal practitioners can employ this taxonomy to identify the cognitive levels attained by LLMs, thereby enhancing the planning of training objectives and downstream applications. 
It is important to note that this legal cognitive ability taxonomy does not imply a linear learning process. During training, the model can be designed to learn back and forth from different tasks at different levels. Different legal tasks may involve multiple levels at the same time, and evaluating model performance at one level also requires synthesis across multiple tasks.
As these ability levels in LexEval are developed and refined, LLMs will become increasingly integrated into legal practice, enhancing the efficiency, accuracy, and ethical standards of legal work. This taxonomy not only provides a framework for assessing the current capabilities of LLMs but also guides future advancements in the field. 
Although the taxonomy is primarily designed for the Chinese legal system, we believe it can be extended to involve other legal tasks in other countries as well, as these ability levels are universal across different legal systems. 

\subsection{Data Collection and Processing}
\label{data}
\textbf{Data Source:} The data in LexEval comes from three sources.  The first source comprises existing datasets and corpora, primarily including CAIL~\footnote{The data from CAIL can be found on the official website of the competition\url{http://cail.cipsc.org.cn/}.}, JEC-QA~\cite{zhong2020jec}, and LeCaRD~\cite{ma2021lecard}. As these resources are originally designed for non-LLM evaluation settings, we standardize the data format and adjust the prediction targets to align with the evaluation objectives of LLMs.
The second source originates from the National Uniform Legal Profession Qualification Examination, which is a uniform national examination for assessing qualifications to practice the legal profession in China. We carefully select and adapt exam questions from previous years to suit our evaluation framework.  The third task source is expert annotation, where we hire 18 experts in the legal field as annotators to craft precise and relevant evaluation questions. Detailed data sources and licenses can be found in Appendix ~\ref{task detail}.

\textbf{Data Processing:} We collect data in various formats, including PDF, JSON, LaTeX, and Word, among others. By using techniques such as OCR, we first convert PDF and Word documents into textual form. For those questions that are difficult to parse automatically, we process them manually.
Subsequently, all questions are converted into structured data using JSON format. 
All questions (except for the Generation level task) are converted to multiple-choice format. This is because multiple-choice questions have clearly defined metrics (i.e., accuracy) and are also a simple and effective way to evaluate the capabilities of the LLMs, which have been widely used in various benchmarks~\cite{fei2023lawbench,dai2023laiw,huang2023c}. Detailed construction processes for each task can be found in Appendix ~\ref{Process}.
All questions have been verified by the authors in multiple rounds to ensure their accuracy and reasonableness.

\textbf{Data Annotation:}
For tasks lacking existing datasets, we hire professional annotators to create entirely new datasets.
Our annotation team consists of 18 legal experts who have all passed the National Uniform Legal Profession Qualification Examination. The annotation experts are all from China, of whom 9 are men and 9 are women. Before the beginning of the annotation work, we signed a legally effective agreement with all annotation experts to protect their rights and interests. To ensure the quality of annotation, all annotators first go through several hours of interpretation to understand their respective tasks. After that, we provide several examples to help them understand the format of tasks. The annotator creates the questions and answers according to the appropriate rules and format. Our gold annotators, who hold a Ph.D. in law, cross-check and inspect all generated questions. 
Before formal annotation, each annotator creates 100 questions and answers corresponding to the task. Subsequently, only annotators who achieve a 90\% approved rate through cross-checking and inspection are allowed to annotate formally.
We remove questions that are too simple and try to ensure that the distribution of causes is as balanced as possible. For each approved question, we pay the legal expert 0.7 dollars. We have annotated a total of 6,250 questions, with a total payment of 4,375 dollars. Detailed annotation guidance for each task can be found in Appendix ~\ref{Process} and Appendix ~\ref{guidelines}.
 
Built upon the above processing, we finally select and construct 23 evaluation tasks in LexEval. For the existing datasets, we try our best to avoid using datasets that have already been extensively mined by existing LLMs (e.g. C-Eval) so that the risk of test data leakage could be minimized. To ensure the quality of LexEval, we also try to balance the distributions of legal documents from different causes, thereby avoiding bias or long-tail effects in the dataset. 

\subsection{Task Definition}
Based on the legal cognitive ability taxonomy, we construct a series of evaluation tasks. Table \ref{overview} shows the overview of tasks in LexEval.
These tasks may simultaneously evaluate one or multiple ability levels, and we categorize them based on their primary ability level. Each task has at least 100 evaluation samples. Among these tasks, 11 tasks are derived from existing datasets, 2 tasks come from the National Uniform Legal Profession Qualification Examination, and 10 tasks are annotated by experts. Detailed task definition, construction process, and task statistics can be found in Appendix ~\ref{sec:tasks}. Based on these tasks, LexEval not only provides comprehensive coverage of legal knowledge and reasoning ability but also detects issues such as bias and discrimination in legal ethics, providing valuable insights for in-depth evaluation and analysis.

\subsection{Legal and Ethical Considerations}
\label{ethical}
Due to the sensitivity of the legal domain, we conducted a thorough review for this benchmark. All open-source datasets we utilized are licensed. LexEval tasks are subject to different licenses. Appendix ~\ref{license}  provides a summary of the licenses. The authors take full responsibility for any infringement and confirm the authorization of the dataset.
Our evaluation task strictly avoids involving the speculation of sensitive information about individuals and the generation of insulting or sensitive statements. 
In addition, we have carefully screened and filtered the data sets in LexEval for any content that contains personally identifiable information, discriminatory content, explicit, violent, or offensive content. The data set has been ethically reviewed by legal experts. We strongly believe that our benchmarks have a very low risk of negative impact on safety, security, discrimination, surveillance, deception, harassment, human rights, bias, and fairness. Appendix ~\ref{impact} discusses the potential social impacts.

\begin{table*}[t]
\centering
\small
\caption{Details of tasks within LexEval.}
\begin{tabular}{lclclc}
\hline
Level                            & ID  & Task                          & Metrics & Data Source            & Test Set \\ \hline
\multirow{3}{*}{Memorization}    & 1-1 & Legal Concept                 & Accuracy    & JEC-QA~\cite{zhong2020jec}           & 500      \\
                                 & 1-2 & Legal Rule                    & Accuracy    & Expert Annotation & 1000     \\
                                 & 1-3 & Legal Evolution               & Accuracy    & Expert Annotation & 300      \\ \hline
\multirow{5}{*}{Understanding}   & 2-1 & Legal Element Recognition     & Accuracy    & CAIL-2019         & 500      \\
                                 & 2-2 & Legal Fact Verification       & Accuracy    & Expert Annotation & 300      \\
                                 & 2-3 & Reading Comprehension         & Accuracy    & CAIL-2021         & 100      \\
                                 & 2-4 & Relation Extraction           & Accuracy    & CAIL-2022         & 500      \\
                                 & 2-5 & Named-entity Recognition      & Accuracy    & CAIL-2021         & 500      \\ \hline
\multirow{6}{*}{Logic Inference} & 3-1 & Cause Prediction              & Accuracy    & CAIL-2018         & 1000     \\
                                 & 3-2 & Article Prediction            & Accuracy    & CAIL-2018         & 1000     \\
                                 & 3-3 & Penalty Prediction            & Accuracy    & CAIL-2018         & 1000      \\
                                 & 3-4 & Multi-hop Reasoning           & Accuracy    & Exams             & 500      \\
                                 & 3-5 & Legal Calculation             & Accuracy    & Expert Annotation & 400      \\
                                 & 3-6 & Argument Mining               & Accuracy    & CAIL-2021         & 500      \\ \hline
\multirow{2}{*}{Discrimination}  & 4-1 & Similar Case Identification   & Accuracy    & LeCaRD~\cite{ma2021lecard}\&CAIL-2019 & 500      \\
                                 & 4-2 & Document Proofreading         & Accuracy    & Expert Annotation & 300      \\ \hline
\multirow{4}{*}{Generation}      & 5-1 & Summary Generation            & Rouge-L   & CAIL-2020         & 1000     \\
                                 & 5-2 & Judicial Analysis Generation  & Rouge-L   & Expert Annotation & 1000     \\
                                 & 5-3 & Legal Translation             & Rouge-L   & Expert Annotation & 250      \\
                                 & 5-4 & Open-ended Question Answering & Rouge-L   & Exams             & 500      \\ \hline
\multirow{3}{*}{Ethic}           & 6-1 & Bias and Discrimination       & Accuracy    & Expert Annotation & 1000     \\
                                 & 6-2 & Morality                      & Accuracy    & Expert Annotation & 1000     \\
                                 & 6-3 & Privacy                       & Accuracy    & Expert Annotation & 500      \\ \hline
\end{tabular}
\vspace{-3mm}

\label{overview}
\vspace{-4mm}
\end{table*}

\section{Evaluation}
In this section, we present the experimental setup, evaluated models, and experimental results.
\subsection{Setup}
We evaluate the LLMs in both zero-shot and few-shot settings. In the zero-shot setting, the inputs to LLMs are only instructions and queries. In the few-shot setting, we design three different examples for each task. These examples can be found on the GitHub website. When evaluating LLMs, we set the temperature to 0 to minimize the variance introduced by random sampling. For chat LLMs, we reserve the format of their dialog prompts. When the input length exceeds the maximum context length of LLMs, we truncate the input sequence from the middle since the front and end of the input may contain crucial information. The input prompts used during our evaluation can be found in the Appendix~\ref{sec:tasks}. 
We standardize our evaluation metrics by using Accuracy to evaluate all multiple-choice questions and Rough-L to evaluate tasks at Generation level.

The evaluation metrics for each task can be found in Table ~\ref{overview}. We also discuss the limitations of the evaluation metrics in Appendix ~\ref{limitation}.

\subsection{Evaluated Models}
We evaluate a total of 38 popular models, categorized into two main groups: General LLMs and Legal-specific LLMs.

There are 29 General LLMs, including GPT-4~\cite{openai2023gpt4}, ChatGPT~\cite{gpt3}, LLaMA-2-7B~\cite{touvron2023llama}, LLaMA-2-7B-Chat~\cite{touvron2023llama}, LLaMA-2-13B-Chat~\cite{touvron2023llama}, ChatGLM-6B~\cite{zeng2022glm}, ChatGLM2-6B~\cite{zeng2022glm}, ChatGLM3-6B~\cite{zeng2022glm}, Baichuan-7B-base~\cite{yang2023baichuan}, Baichuan-13B-base~\cite{yang2023baichuan}, Baichuan-13B-Chat~\cite{yang2023baichuan}, Qwen-7B-chat~\cite{bai2023qwen}, Qwen-14B-Chat~\cite{bai2023qwen}, MPT-7B~\cite{MosaicML2023Introducing}, MPT-7B-Instruct~\cite{MosaicML2023Introducing}, XVERSE-13B, InternLM-7B~\cite{2023internlm}, InternLM-7B-Chat~\cite{2023internlm}, Chinese-LLaMA-2-7B~\cite{Chinese-LLaMA-Alpaca}, Chinese-LLaMA-2-13B~\cite{Chinese-LLaMA-Alpaca}, TigerBot-Base, Chinese-Alpaca-2-7B~\cite{Chinese-LLaMA-Alpaca}, GoGPT2-7B, GoGPT2-13B, Ziya-LLaMA-13B~\cite{fengshenbang}, Vicuna-v1.3-7B, BELLE-LLAMA-2-13B~\cite{BELLE}, Alpaca-v1.0-7B, MoSS-Moon-sft~\cite{sun2023moss}.

The Legal-specific LLMs include 9 models, which are ChatLaw-13B~\cite{cui2023chatlaw}, ChatLaw-33B~\cite{cui2023chatlaw}, LexiLaw, Lawyer-LLaMA~\cite{huang2023lawyer}, Wisdom-Interrogatory, LaWGPT-7B-beta1.0, LaWGPT-7B-beta1.1, HanFei~\cite{HanFei}, Fuzi-Mingcha~\cite{sdu_fuzi_mingcha}.. 
The specific description of evaluated models can be found in the Appendix~\ref{sec:models}.

\begin{table*}[t]
\scriptsize
\centering
\caption{Zero-shot performance(\%) of various models at Memorization, Understanding, and Logic Inference level. Best preformance in each column is marked bold. }
\begin{tabular}{l|ccc|ccccc|cccccc}
\hline
\multirow{2}{*}{Model} & \multicolumn{3}{c|}{Memorization(Acc.)} & \multicolumn{5}{c|}{Understanding(Acc.)} & \multicolumn{6}{c}{Logic Inference(Acc.)}     \\
                       & 1-1       & 1-2       & 1-3       & 2-1   & 2-2   & 2-3  & 2-4  & 2-5  & 3-1  & 3-2  & 3-3  & 3-4  & 3-5  & 3-6  \\ \hline
GPT-4               & \textbf{34.0} & 35.4          & 14.0          & 79.8          & \textbf{51.0} & \textbf{94.0} & 78.0          & \textbf{96.2} & \textbf{80.3} & 68.3          & \textbf{53.7} & \textbf{33.2} & \textbf{66.0} & \textbf{57.8} \\
Qwen-14B-Chat       & 28.0          & 38.6          & 11.4          & \textbf{93.4} & 45.3          & 90.0          & \textbf{85.6} & 91.8          & 80.2          & \textbf{91.0} & 27.9          & 31.6          & 44.7          & 50.4          \\
Qwen-7B-Chat        & 22.8          & \textbf{38.9} & 8.4           & 79.8          & 43.3          & 87.0          & 67.2          & 92.0          & 79.2          & 83.9          & 53.2          & 24.2          & 36.3          & 45.0          \\
ChatGPT             & 19.0          & 25.6          & 9.0           & 56.8          & 42.3          & 87.0          & 76.0          & 82.2          & 77.7          & 60.3          & 23.0          & 19.4          & 39.6          & 38.2          \\
InternLM-7B-Chat    & 20.4          & 35.4          & 11.0          & 61.4          & 42.3          & 89.0          & 49.4          & 53.8          & 79.3          & 77.9          & 28.8          & 23.8          & 38.3          & 30.0          \\
Baichuan-13B-Chat   & 14.6          & 33.9          & 10.0          & 54.2          & 35.0          & 72.0          & 62.2          & 75.4          & 77.0          & 58.0          & 41.8          & 20.2          & 33.5          & 21.0          \\
ChatGLM3            & 19.2          & 28.9          & 7.7           & 41.0          & 34.3          & 80.0          & 62.8          & 81.4          & 73.4          & 61.2          & 19.4          & 21.4          & 25.6          & 37.0          \\
Baichuan-13B-base   & 22.6          & 23.0          & 9.0           & 43.2          & 26.7          & 75.0          & 59.2          & 74.4          & 58.3          & 25.6          & 12.5          & 23.8          & 31.0          & 19.6          \\
Fuzi-Mingcha        & 13.0          & 25.0          & 6.7           & 62.0          & 29.0          & 61.0          & 46.4          & 24.8          & 68.0          & 58.6          & 25.5          & 16.0          & 28.9          & 20.4          \\
ChatLaw-33B         & 16.0          & 25.9          & 7.0           & 51.4          & 32.3          & 76.0          & 67.6          & 62.0          & 60.6          & 32.9          & 23.0          & 15.4          & 23.6          & 37.6          \\
ChatGLM2            & 28.2          & 13.6          & 16.4          & 22.4          & 24.0          & 61.0          & 40.0          & 29.8          & 77.2          & 54.4          & 24.8          & 19.8          & 27.7          & 8.6           \\
Chinese-Alpaca-2-7B & 19.8          & 24.8          & \textbf{19.7} & 25.0          & 33.3          & 61.0          & 46.6          & 24.2          & 66.8          & 39.4          & 20.6          & 16.4          & 18.0          & 26.6          \\
BELLE-LLAMA-2-Chat  & 15.0          & 25.7          & 7.0           & 31.4          & 27.3          & 77.0          & 61.6          & 46.2          & 64.1          & 47.3          & 8.2           & 19.8          & 33.2          & 24.4          \\
XVERSE-13B          & 25.4          & 29.0          & 12.0          & 47.0          & 21.7          & 71.0          & 48.2          & 32.4          & 54.9          & 44.7          & 9.9           & 19.2          & 27.7          & 14.6          \\
TigerBot-base       & 16.6          & 27.5          & 9.0           & 22.4          & 27.0          & 58.0          & 57.0          & 24.6          & 71.5          & 35.7          & 18.3          & 19.0          & 31.2          & 18.8  \\
\hline
\end{tabular}
\label{zero-1}
\vspace{-5mm}
\end{table*}

\begin{table*}[t]
\centering
\scriptsize
\caption{Zero-shot performance(\%) of various models at Discrimination, Generation, and Ethic level. Best preformance in each column is marked bold.}
\begin{tabular}{l|cc|cccc|ccc|c|c}
\hline
\multirow{2}{*}{Model} & \multicolumn{2}{c|}{Discrimination(Acc.)} & \multicolumn{4}{c|}{Generation(Rough-L)} & \multicolumn{3}{c|}{Ethic(Acc.)} & \multirow{2}{*}{Average} & \multirow{2}{*}{Rank} \\
                       & 4-1              & 4-2              & 5-1    & 5-2    & 5-3   & 5-4   & 6-1     & 6-2     & 6-3    &                          &                       \\ \hline
GPT-4               & 35.8          & \textbf{39.1} & 25.0          & 16.0          & \textbf{38.3} & 13.6          & \textbf{65.2} & \textbf{55.2} & \textbf{75.8} & \textbf{52.4} & \textbf{1} \\
Qwen-14B-Chat       & 30.0          & 31.9          & 33.9          & 23.1          & 36.0          & \textbf{19.1} & 29.2          & 42.0          & 63.0          & 48.6          & 2          \\
Qwen-7B-Chat        & 21.0          & 28.6          & 30.8          & 19.0          & 34.7          & 18.3          & 22.1          & 38.9          & 56.8          & 44.8          & 3          \\
ChatGPT             & 28.4          & 22.0          & 22.8          & 13.1          & 34.3          & 13.1          & 33.7          & 32.1          & 55.8          & 39.6          & 4          \\
InternLM-7B-Chat    & \textbf{37.0} & 9.9           & 19.6          & 2.6           & 29.2          & 11.8          & 22.7          & 27.8          & 47.4          & 36.9          & 5          \\
Baichuan-13B-Chat   & 24.4          & 20.4          & 29.2          & 24.2          & 35.7          & 16.0          & 16.4          & 22.0          & 40.8          & 36.4          & 6          \\
ChatGLM3            & 25.2          & 14.1          & 28.3          & 17.0          & 29.7          & 14.4          & 21.2          & 29.6          & 49.6          & 35.8          & 7          \\
Baichuan-13B-base   & 15.6          & 23.0          & 21.5          & \textbf{27.8} & 24.0          & 11.8          & 17.3          & 28.6          & 47.0          & 31.3          & 8          \\
Fuzi-Mingcha        & 20.0          & 16.1          & \textbf{57.8} & \textbf{27.8} & 21.4          & 17.3          & 10.8          & 13.1          & 25.0          & 30.2          & 9          \\
ChatLaw-33B         & 10.0          & 17.1          & 23.8          & 9.9           & 15.2          & 13.3          & 15.3          & 19.1          & 34.2          & 30.0          & 10         \\
ChatGLM2            & 20.2          & 21.1          & 28.4          & 15.5          & 24.1          & 14.0          & 36.8          & 27.2          & 52.2          & 29.9          & 11         \\
Chinese-Alpaca-2-7B & 27.8          & 24.7          & 28.6          & 15.7          & 31.2          & 14.6          & 21.5          & 28.4          & 40.4          & 29.4          & 12         \\
BELLE-LLAMA-2-Chat  & 3.6           & 20.4          & 28.0          & 11.4          & 25.4          & 15.3          & 13.8          & 16.6          & 30.4          & 28.4          & 13         \\
XVERSE-13B          & 10.4          & 12.2          & 12.1          & 13.9          & 6.8           & 19.0          & 19.9          & 29.4          & 55.0          & 27.7          & 14         \\
TigerBot-base       & 25.8          & 23.0          & 20.8          & 11.3          & 34.5          & 12.6          & 16.3          & 19.0          & 39.2          & 27.3          & 15         \\
\hline
\end{tabular}
\label{zero-2}
\end{table*}

\subsection{Experimental Results}
We report the zero-shot performance scores of all models in Table \ref{zero-1} and \ref{zero-2}. 
Due to space limitations, we only show the performance of the top 15 models. More experimental results can be found in the Appendix~\ref{sec:results}. From the experimental results, we have the following findings:

\begin{table*}[t]
\scriptsize
\centering
\vspace{-5mm}
\caption{Few-shot performance(\%) of various models at Memorization, Understanding, and Logic Inference level. Best performance in each column is marked bold. $\uparrow$/$\downarrow$ represents the performance increase/decrease compared to the zero-shot setting.}
\begin{tabular}{l|ccc|ccccc|cccccc}
\hline
\multirow{2}{*}{Model} & \multicolumn{3}{c|}{Memorization(Acc.)} & \multicolumn{5}{c|}{Understanding(Acc.)} & \multicolumn{6}{c}{Logic Inference(Acc.)}     \\
                       & 1-1       & 1-2       & 1-3       & 2-1   & 2-2   & 2-3  & 2-4  & 2-5  & 3-1  & 3-2  & 3-3  & 3-4  & 3-5  & 3-6  \\ \hline
GPT-4             & 31.0          & 42.3          & 16.4          & \textbf{96.8} & \textbf{52.3} & \textbf{95.0} & \textbf{97.4} & \textbf{98.0} & \textbf{79.7} & 66.3          & 53.1          & 27.2          & \textbf{64.5} & \textbf{60.0} \\
Qwen-14B-Chat     & \textbf{34.0} & \textbf{49.7} & 13.4          & 95.4          & 42.7          & 92.0          & 88.4          & 88.2          & 58.6          & \textbf{90.7} & 61.2          & \textbf{34.2} & 47.2          & 41.0          \\
Qwen-7B-Chat      & 23.2          & 42.3          & 8.7           & 82.2          & 34.7          & 85.0          & 60.4          & 49.2          & 78.1          & 77.2          & \textbf{61.6} & 24.4          & 35.8          & 41.8          \\
ChatGPT           & 22.0          & 26.8          & 7.0           & 85.4          & 36.3          & 84.0          & 83.0          & 59.2          & 76.8          & 58.8          & 24.3          & 21.8          & 42.1          & 36.4          \\
InternLM-7B-Chat  & 20.8          & 33.7          & 8.4           & 84.0          & 39.0          & 83.0          & 73.4          & 85.4          & 79.4          & 77.7          & 33.4          & 24.0          & 36.5          & 36.4          \\
ChatGLM3          & 20.6          & 31.8          & 6.4           & 69.0          & 36.3          & 76.0          & 66.8          & 68.0          & 73.9          & 64.5          & 16.0          & 19.0          & 28.2          & 38.2          \\
Baichuan-13B-base & 21.6          & 28.1          & \textbf{17.1} & 82.2          & 24.0          & 75.0          & 83.4          & 72.0          & 74.0          & 52.1          & 40.0          & 19.8          & 33.0          & 27.4          \\
Baichuan-13B-Chat & 15.8          & 33.4          & 8.4           & 58.8          & 27.7          & 55.0          & 54.6          & 67.4          & 56.6          & 46.9          & 36.2          & 21.6          & 29.9          & 29.6          \\
LLaMA-2-13B-Chat  & 14.6          & 25.8          & 6.0           & 75.4          & 32.0          & 71.0          & 64.4          & 58.6          & 59.8          & 55.1          & 25.4          & 14.6          & 33.5          & 32.6          \\
ChatGLM2          & 23.6          & 27.4          & 10.0          & 55.2          & 34.3          & 56.0          & 39.8          & 36.4          & 76.2          & 49.2          & 28.5          & 20.4          & 27.7          & 26.4                            \\
\hline
\end{tabular}
\vspace{-3mm}
\label{few-1}
\end{table*}

\begin{table*}[t]
\scriptsize
\centering
\caption{Few-shot performance(\%) of various models at the Discrimination, Generation, and Ethic level. Best performance in each column is marked bold. $\uparrow$/$\downarrow$ represents the performance increase/decrease compared to the zero-shot setting.}
\begin{tabular}{l|cc|cccc|ccc|c|c}
\hline

\multirow{2}{*}{Model} & \multicolumn{2}{c|}{Discrimination(Acc.)} & \multicolumn{4}{c|}{Generation(Rough-L)} & \multicolumn{3}{c|}{Ethic(Acc.)} & \multirow{2}{*}{Average} & \multirow{2}{*}{Rank} \\
                       & 4-1              & 4-2              & 5-1    & 5-2    & 5-3   & 5-4   & 6-1     & 6-2     & 6-3    &                          &                       \\ \hline
GPT-4             & \textbf{32.3} & \textbf{36.5} & 22.4          & 19.1          & \textbf{37.9} & 16.4          & \textbf{65.6} & \textbf{52.8} & \textbf{72.2} & \textbf{53.7}$\uparrow$ & \textbf{1} \\
Qwen-14B-Chat     & 26.0          & 32.2          & 12.0          & 23.9          & 37.0          & \textbf{23.4} & 34.3          & 51.9          & 70.8          & 49.9$\uparrow$          & 2          \\
Qwen-7B-Chat      & 24.8          & 30.3          & 27.1          & 18.4          & 34.5          & 21.5          & 27.9          & 38.9          & 59.6          & 42.9$\downarrow$          & 3          \\
ChatGPT           & 31.3          & 26.3          & 17.2          & 14.1          & 35.0          & 16.6          & 41.0          & 32.9          & 61.8          & 40.9$\uparrow$          & 4          \\
InternLM-7B-Chat  & 32.2          & 15.8          & 16.7          & 0.9           & 24.1          & 13.4          & 21.3          & 29.3          & 44.0          & 39.7$\uparrow$          & 5          \\
ChatGLM3          & 15.8          & 13.2          & \textbf{27.8} & 19.1          & 29.4          & 16.1          & 20.6          & 28.8          & 46.6          & 36.2$\uparrow$          & 6          \\
Baichuan-13B-base & 1.0           & 12.5          & 4.6           & \textbf{28.8} & 6.3           & 9.5           & 16.6          & 27.5          & 29.4          & 34.2$\uparrow$          & 7          \\
Baichuan-13B-Chat & 27.2          & 18.1          & 19.8          & 18.0          & 34.7          & 18.2          & 18.7          & 27.6          & 46.6          & 33.5$\downarrow$          & 8          \\
LLaMA-2-13B-Chat  & 27.2          & 17.1          & 18.5          & 12.5          & 17.5          & 15.3          & 17.0          & 16.7          & 39.6          & 32.6$\uparrow$          & 9          \\
ChatGLM2          & 19.0          & 19.1          & 15.6          & 14.9          & 21.3          & 16.8          & 35.6          & 26.3          & 55.4          & 32.0$\uparrow$          & 10   \\
 \hline
\end{tabular}
\vspace{-3mm}

\label{few-2}
\end{table*}

\begin{itemize}[leftmargin=*]
    \item The open-source model perform slightly worse compared to the closed-source model GPT-4, which achieve the best performance in the benchmark. However, due to the lack of legal knowledge related to the Chinese legal system, the performance of GPT-4 is still far from perfect in many tasks. This indicates that there is still significant room for improvement in the performance of LLMs in the legal domain.
    \item Increasing model size leads to better performance, which is equally applicable in the legal domain. For example, Qwen-14B performs better than Qwen-7B. Moreover, compared to base models, LLMs designed for chat and dialogue often exhibit better performance.  For example, Baichuan-13B-Chat performs better than Baichuan-13B-base. This advantage may come from their better ability in instruction following. This suggests that supervised fine-tuning and alignment optimizations can significantly release the potentially broader capabilities of LLMs.
    \item Surprisingly, Legal-specific LLMs do not always perform better than General LLMs. We speculate that there are two possible reasons. First, the capability of these Legal-specific LLMs could be limited by their base models, which are usually not as strong as other LLMs such as GPT-4. Moreover, the continuous pre-training on the legal corpus may affect the abilities of the original base models. This suggests that we need to further design appropriate training objectives to improve the performance of Legal-specific LLMs. 
    \item In tasks at the Memorization level, most models perform poorly on legal evolution (1-3) tasks. Even models trained on legal data struggle to comprehend the changes in legal norms across different periods. How to design better ways to make LLMs aware of the evolution of the law deserves further attention.
\end{itemize}

Tables \ref{few-1} and \ref{few-2} show the few-shot performance of top 10 LLMs at different levels. Under the few-shot setting, the performance of most LLMs shows slight enhancement, but such improvements are usually unstable. 
The improvement brought by few-shot examples varies across different models. Some models (e.g. GPT-4) experience performance improvements, while others (e.g. Qwen-14B-Chat) may suffer degradation.
We speculate that the few-shot setting may generate inputs that are overly lengthy for certain LLMs, posing challenges for them to comprehend the overall text provided with examples. Also, it indicates that in-context learning may not be an ideal way to inject legal knowledge into LLMs.

Finally, in Figure \ref{chart}, we show the zero-shot performance of the best six models in different legal cognitive ability levels. We derive the following observations from the experiment results.

\begin{itemize}[leftmargin=*]
    \item LLMs perform poorly at the Memorization level, which may be the critical obstacle to performing tasks at a higher level. Given that even Legal-specific LLMs also exhibit weaknesses (see Appendix ~\ref{sec:results}), merely increasing legal corpora during pre-training may not be the optimal solution.
    \item Most models perform best at the Understanding and Logic Inference levels. Through observation, we notice that within a given context or provided with the relevant legal provisions, LLMs can effectively utilize their inherent reasoning abilities to provide reasonable answers. Despite the numerous challenges we still face in complex tasks such as multi-hop reasoning (3-4), by enhancing the reasoning capabilities of existing base models, we have the potential to lay a solid foundation for their broader and deeper application in the legal field.
    \item The performance on the Discrimination level indicates that current LLMs do not yet possess the ability to discern and evaluate legal content. Also, LLMs exhibit inefficiency in producing well-formatted legal texts at the Generation level. This limitation primarily arises from the highly specialized and structured nature of legal texts. We propose to leverage the structured information within legal documents and design more rational training objectives to enhance the performance of LLMs at these two levels.
    \item At the Ethic level, although GPT-4 shows relatively good performance, its performance is still far from satisfactory.
    The unsatisfactory performance of LLMs in ethics-related tasks poses serious challenges to their safe application in real-life scenarios. Addressing this concern, on the one hand, we should strive to devise more advanced and precise alignment strategies.  On the other hand, it is also necessary to strengthen the supervision and evaluation of LLMs to ensure that they conform to ethical standards and moral requirements in practical applications.
    \item Overall, at present, LLMs cannot effectively solve the legal problems under the Chinese legal system. Facing this situation, we strongly call for continuous technological innovation and interdisciplinary cooperation. This will bring about more powerful intelligent legal LLMs and improve the efficiency and quality of legal services.
\end{itemize}

\begin{figure}[t]
\vspace{-3mm}
\centering
\includegraphics[width=0.6\columnwidth]{chart.pdf}
\vspace{-3mm}
\caption{The zero-shot performance of the six best models at different legal cognitive ability levels. }
\label{chart}
\vspace{-3mm}
\end{figure}

\section{Conclusion \& Future Work}

In this paper, we introduce LexEval, which is the largest comprehensive benchmark for evaluating LLMs in the Chiese Legal Domain.
With 14,150 questions covering 6 legal cognitive ability levels in LexEval, we extensively evaluate the ability of 38 common LLMs. We find that current LLMs are unable to provide effective legal assistance, even the high-performing GPT-4 included. 
We call for more technological innovations and interdisciplinary collaborations to advance the development of legal LLMs.
In the future, we will further enrich our benchmarks to achieve a more comprehensive evaluation. Additionally, we will also continue to host competitions to promote the development of legal LLMs. Also, LexEval always welcomes open participation and contributions.

\clearpage

\newpage
\clearpage

\end{document}