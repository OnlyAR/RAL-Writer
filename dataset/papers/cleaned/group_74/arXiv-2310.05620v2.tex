\title{LAiW: A Chinese Legal Large Language Models Benchmark}

\begin{document}

\maketitle

\begin{abstract}

General and legal domain LLMs have demonstrated strong performance in various tasks of LegalAI. However, the current evaluations of these LLMs in LegalAI are defined by the experts of computer science, lacking consistency with the logic of legal practice, making it difficult to judge their practical capabilities. To address this challenge, we are the first to build the Chinese legal LLMs benchmark LAiW, based on the logic of legal practice. To align with the thinking process of legal experts and legal practice (syllogism), we divide the legal capabilities of LLMs from easy to difficult into three levels: basic information retrieval, legal foundation inference, and complex legal application. Each level contains multiple tasks to ensure a comprehensive evaluation. Through automated evaluation of current general and legal domain LLMs on our benchmark, we indicate that these LLMs may not align with the logic of legal practice. LLMs seem to be able to directly acquire complex legal application capabilities but perform poorly in some basic tasks, which may pose obstacles to their practical application and acceptance by legal experts. To further confirm the complex legal application capabilities of current LLMs in legal application scenarios, we also incorporate human evaluation with legal experts. The results indicate that while LLMs may demonstrate strong performance, they still require reinforcement of legal logic.
\end{abstract}

\section{Introduction}

With the emergence of ChatGPT\footnote{\url{https://openai.com/blog/chatgpt}} and GPT-4\footnote{\url{https://openai.com/gpt-4}} and their excellent text processing capabilities \cite{zhao2023survey}, many researchers have paid attention to the applications of large language models (LLMs) in various fields \cite{wang2023huatuo, xie2023pixiu, ko2023can}. In the field of legal artificial intelligence (LegalAI), which specifically studies how artificial intelligence can assist in legal construction \cite{zhong2020does, locke2022case, feng2022legal}, LLMs, especially those specializing in Chinese law, also show strong capabilities in generating legal text \cite{cui2023chatlaw,LAWGPT,HanFei}.

Chinese legal LLMs currently cover a wide range of legal tasks and undergo training through two different methods. 
While LLMs like HanFei \cite{HanFei} are pre-trained with large-scale legal documents, most Chinese legal LLMs, such as Fuzi-Mingcha \cite{fuzi.mingcha}, LexiLaw\footnote{\url{https://github.com/CSHaitao/LexiLaw}}, and ChatLaw \cite{cui2023chatlaw}, are fine-tuned with specific legal tasks based on the LLMs. The legal tasks they focus on have been well-defined in the field of NLP and can directly reflect their application effectiveness \cite{zhong2020does, choi2023use, steenhuis2023weaving}, such as legal question answering and consultation. Existing benchmarks for evaluating these models are also constructed based on these tasks \cite{yue2023disclawllm,fei2023lawbench}. However, due to the feasibility, these tasks are ultimately defined by computer experts, which may lack the training data and evaluation tasks for the logic of legal practice. Whether LLMs can currently be accepted and used by legal experts in practice is worth considering.

The application of LLMs in law should adhere to the logic of legal practice, known as the legal syllogism, involving the acquisition of evidence, legal articles, conclusions, and their interconnections \cite{kuppa2023chain, trozze2023large}. We expect legal LLMs to become true legal experts, and their handling of legal issues should involve three stages. Firstly, the ability to extract information from legal texts, then the ability to provide a reliable and reasoned answer based on solid legal knowledge, and ultimately the ability to form a complete response. For example, in criminal law, when judging someone, we need to first find relevant legal articles based on evidence, then calculate the judgment result based on these articles, and provide a well-organized and logical judgment text. 
More importantly, for courtroom officials, whether are LLMs capable of the above-mentioned abilities is their primary concern, which ensures the fairness of the law\footnote{\url{https://github.com/liuchengyuan123/LegalLLMEvaluation/}}.

In this work, to investigate the above-mentioned issue, we propose the first Chinese legal LLM benchmark LAiW\footnote{It means "AI in LAW".} based on the logic of legal practice. In this benchmark, following the guideline of the logic of legal practice, we categorize the legal capabilities of LLMs into three levels, from simple to difficult: basic information retrieval (BIR), legal foundation inference (LFI), and complex legal application (CLA). Among them, BIR focuses on the general NLP capabilities of LLMs and some legal evidence, knowledge, and category determination; 
LFI emphasizes the performance of LLMs in simple application tasks in the legal domain, which are typically components of more complex legal issues and interim results in legal logic; 
CLA focuses on the performance of LLMs in complex tasks in the legal domain, which require support from the abilities developed in the first two levels and some complex logical reasoning capabilities.
Based on these capabilities, our benchmark contains 14 tasks, covering most of the existing LegalAI tasks, and some new tasks. 

When conducting benchmark evaluations, we performed both automated evaluations and additional manual evaluations. For automated evaluations, we not only evaluate existing Chinese legal LLMs such as Lawyer-LLaMA \cite{lawyer-llama-report}, ChatLaw\cite{cui2023chatlaw} and HanFei \cite{HanFei}, but also focused on the base models of these Chinese legal LLMs and more effective general LLMs. 
The results of automated evaluations indicate that while existing LLMs have strong text generation capabilities for complex legal applications, they are unable to meet the underlying logic in legal applications in basic information retrieval and legal foundation inference. 
This may lead to significant distrust among legal experts when using LLMs. 
Therefore, we conduct additional manual evaluations specifically to assess the reliability of LLMs in complex legal applications. 
Through evaluations by legal experts, we find that in some complex legal applications with relatively lenient requirements for legal logic, LLMs' powerful generation ability cleverly bridges the gap in legal logic. 
However, in more demanding scenarios, they still exhibit significant discrepancies from real results. 
This further indicates the need for the tasks to train and evaluate the legal logic capabilities of LLMs.

Our contributions are as follows:
\begin{itemize}
    \item 
    We are proud to introduce the first Chinese legal LLMs benchmark LAiW, which is designed based on the logic of legal practice. We categorize the legal capabilities of LLMs into three levels: basic information retrieval, legal foundation inference, and complex legal application. This detailed categorization enables a more accurate assessment of the true abilities of LLMs in legal practice.
    \item Based on our automated evaluation, we demonstrate that the current capabilities of legal LLMs do not align with the demands of real-world applications. While LLMs demonstrate strong text generation abilities to complete complex legal applications, they struggle to achieve satisfactory performance in foundational legal logic tasks, making it difficult to gain the trust of legal experts.
    \item To assess whether the outstanding performance of LLMs in our third level is applicable in real scenarios, we invite legal experts for manual evaluations. Despite the LLMs' strong text generation capability, they still expose limitations with legal logic in practical applications. Therefore, We further advocate for the construction of training tasks to better reflect legal logic.
\end{itemize}

\section{Related Work}

\textbf{Chinese Legal LLMs.} 
We summarize the current Chinese legal LLMs and some general models in Table \ref{tab:LAiW-LLMs}.
Most of these Chinese legal LLMs focus on the ultimate application tasks in the legal field and are generally fine-tuned on some general LLMs.
For instance, LawGPT\_zh \cite{LAWGPT-zh}, Lawyer-LLaMA \cite{lawyer-llama-report}, ChatLaw \cite{cui2023chatlaw}, Fuzi-Mingcha \cite{fuzi.mingcha}, and LexiLaw developed the ability to answer legal questions and provide legal consultations by fine-tuning on related legal data. 
To compensate for the lack of legal knowledge due to only fine-tuning, these LLMs introduce additional legal knowledge databases for retrieval to supplement.
However, the accuracy and comprehensiveness of the knowledge base may be a major limiting factor for these LLMs.
The other Chinese legal LLMs adopted the pretraining or continued pretraining to enhance the legal knowledge of LLMs, such as LaWGPT \cite{LAWGPT}, wisdomInterrogatory\footnote{\url{https://github.com/zhihaiLLM/wisdomInterrogatory}}, and HanFei \cite{HanFei}.
They collect a large amount of legal text data, covering a wider range of legal tasks such as element extraction and case classification. These have a noticeable impact on improving the overall effectiveness of LLMs in legal applications.
However, the Chinese legal LLMs mentioned above mainly focus on the outcome of legal application (such as similarity to standard answers and fluency of generated text), which rarely consider whether they meet the logical requirements of legal practice. It is important to evaluate their legal logic, which is of utmost concern to legal experts.

\textbf{Legal LLMs Benchmark.} 
The development of LegalAI has led to a large number of tasks that combine law and computer science, from NLP-focused legal NER\footnote{\url{https://github.com/china-ai-law-challenge/CAIL2021/tree/main/xxcq}} and legal text summarization \cite{kanapala2019text} to legal-focused similar case matching \cite{locke2022case, sansone2022legal}, providing ample data for evaluating Chinese legal LLMs \cite{zhong2020does}. 
When categorizing from a legal perspective, it also encompasses the logic of the entire legal process from the legal elements extraction \cite{cao2022cailie, zhang2022recognition, zhong2020iteratively} to legal judgment prediction \cite{feng2022legal, cui2023survey}. 
Based on these tasks, LawBench \cite{fei2023lawbench} built an automatic evaluation framework for Chinese legal LLMs, which concerns the memorization, understanding, and application of legal knowledge.
DISC-Law-Eval Benchmark \cite{yue2023disclawllm} also based on the aforementioned tasks divides the evaluation into objective and subjective parts. The objective section assesses knowledge retention and reasoning abilities in the legal examination, and the subjective part uses GPT-3.5 Turbo to score the accuracy, completeness, and clarity of the answers. These frameworks greatly helped us understand the capabilities and improvement directions of current legal LLMs from the perspective of computer experts, especially the knowledge systems. However, whether these LLMs can be accepted by legal experts is also a question worthy of evaluation. In this work, we focus on addressing this issue from the logic of legal practice.

\begin{table*}[!htb]
  \centering
  \resizebox{0.999\textwidth}{!}{
  \begin{threeparttable}
  \begin{tabular}{lcccccc}
    \toprule
    Capability & Task & Primary Origin Dataset   & LAiW & Domain & Task Type\\
    \midrule
    
    \multirow{5}[0]{*}{BIR}  & Legal Article Recommendation & CAIL2018 \cite{xiao2018cail2018}  &  1000  & Criminal & Classification \\
    \cmidrule{2-6}  & Element Recognition & CAIL-2019 \cite{zhang2022recognition}  & 1000 & Civil & Classification \\
    \cmidrule{2-6}  & Named Entity Recognition & CAIL-2021 \cite{cao2022cailie} & 1040 & Criminal & Named Entity Recognition \\
    \cmidrule{2-6}  & Judicial Summarization & CAIL-2020 \cite{huang2023high} & 364 & Civil &  Text Generation \\
    \cmidrule{2-6}  & Case Recognition & CJRC \cite{duan2019cjrc} & 2000 & Criminal, Civil & Classification \\
    
    \midrule
    \multirow{6}[0]{*}{LFI} & Controversy Focus Mining & LAIC-2021  & 306 & - & Classification \\
    \cmidrule{2-6}  & Similar Case Matching & CAIL-2019 \cite{xiao2019cail2019}  & 260 & Civil & Classification \\
    \cmidrule{2-6} & Charge Prediction & Criminal-S \cite{hu2018few}  & 827 & Criminal & Classification \\
    \cmidrule{2-6} & Prison Term Prediction & MLMN \cite{ge2021learning}  & 349 & Criminal & Classification \\
    \cmidrule{2-6}  & Civil Trial Prediction & MSJudeg \cite{ma2021legal} & 800 & Civil & Classification \\
    \cmidrule{2-6}  & Legal Question Answering & JEC-QA \cite{zhong2020jec} & 855 & -  & Classification \\

    \midrule
    \multirow{3}[0]{*}{CLA} & Judicial Reasoning Generation & AC-NLG \cite{wu2020biased} & 834 &  Civil & Text Generation \\
    \cmidrule{2-6}  & Case Understanding & CJRC \cite{duan2019cjrc} & 1054 & Criminal, Civil & Text Generation \\
    \cmidrule{2-6}  & Legal Consultation & CrimeKgAssitant \cite{LAWGPT-zh} & 916 & - & Text Generation \\
    
    \bottomrule
    \end{tabular}

    \end{threeparttable}
}
  \centering
  \caption{Statistical information of our dataset. All datasets are sourced from open-source.}
  \label{tab:dataset}%

\end{table*}%

\section{Benchmark Construction}

In this section, we divide LLMs' abilities levels based on The Logic of Legal practice and construct our Chinese legal LLMs benchmark LAiW based on these levels. To ensure comprehensive evaluation, we cover both automatic evaluation using computable metrics and manual evaluation by legal experts.

\begin{figure}[!htb]
\centering
\includegraphics[width=1\columnwidth]{figs/task_framework_en.png} 
\caption{Multi-level Legal Capabilities of LLMs.}
\label{task_framework_en}
\end{figure}

\subsection{ The Logic of Legal Practice for LLMs} 
In contemporary legal practice, the logic is primarily based on Syllogism \cite{wroblewski1974legal, patterson2013logic}. Syllogism typically consists of three parts: the major premise, the minor premise, and the conclusion which is derived from the major and minor premises. In legal practice, this entails assessing the information and evidence pertinent to a case (minor premise), identifying the relevant legal articles (major premise), and reaching a judicial decision based on these factors (conclusion).

To ensure that LLMs also have the aforementioned logical thinking and remain synchronized with legal practice, we should also divide the abilities of LLMs into the aforementioned logical stages with 14 tasks. 
Specifically, we categorize the legal abilities of LLMs into three levels and try to align them with the logic of legal practice, shown in Figure \ref{task_framework_en}. 
By merging the process of acquiring minor premise and major premise, we construct the capability level of \textbf{basic information retrieval}. Building upon this foundation, we develop the capability level of \textbf{legal foundation inference} to draw preliminary conclusions based on the minor and major premises. Additionally, to assess the direct representation of the entire legal syllogism, we have created the capability level of \textbf{complex legal application}\footnote{Appendix \ref{appendix:dataset auto} provides detailed explanations for tasks of each capability level.}.

\subsubsection{BIR: Basic Information Retrieval}
We design the Basic Information Retrieval level with 5 tasks to assess the fundamental abilities of LLMs in legal logic, corresponding to directly accessible text information, minor premises, and major premises, such as legal evidence, legal knowledge, and category determination.

Specifically, we first consider three tasks that are well-defined in the fields of law and NLP: Named Entity Recognition, Judicial Summarization, and Case Recognition. 
They identify and summarize the key elements in legal texts, and classify cases as either Criminal or Civil.
Although these tasks may not require extensive legal knowledge from LLMs, they can yield a wealth of foundational information useful for both legal and computational purposes from the text.

We also consider two other tasks in the legal domain, namely Legal Article Recommendation and Element Recognition. 
The first task is to catch the major premises by finding relevant legal articles. The second task is to catch minor premises by identifying their relevant elements.

\subsubsection{LFI: Legal Foundation Inference} 
The Legal Foundation Inference level follows Syllogism's idea to explore the ability of LLMs to derive basic results and some judgment conclusions from minor premises and major premises.

We can divide 6 tasks for this capability into three parts. 
The first part presents the basic results of some simple legal applications, including Controversial Focus Mining and Similar Case Matching. 
Controversial Focus Mining is an intermediate result obtained in civil law based on the underlying circumstances and legal articles, used to determine the core issues of concern for both the plaintiff and the defendant.
Similar Case Matching involves finding similar cases based on the current case situation and referring to these cases to ensure the fairness of the judgment.
The second part involves predicting the outcomes of the court judgment conclusion. Since criminal law and civil law are two main branches of law, we have 3 tasks. Charge Prediction and Prison Term Prediction for criminal law, Civil Trial Prediction for civil law. 
Finally, The third part involves another application task, Legal Question Answering, that requires some fundamental integrated capabilities and focuses on the simple application of legal knowledge. 
Based on the information provided, LLMs provide some basic legal responses.

\subsubsection{CLA: Complex Legal Application} 
For this capability, we consider 3 challenging tasks that LLMs may be required to complete a complex legal reasoning and application task according to the entire logical process mentioned above.
Therefore, we focus on three tasks: Judicial Reasoning Generation, Case Understanding, and Legal Consultation.
Judicial Reasoning Generation involves the complete reproduction of the logical process from major and minor premises to conclusions in legal judgments. 
Case Understanding, on the other hand, analyzes the logic from the perspective of understanding, from major and minor premises to conclusion. 
Legal Consultation utilizes this logic from the perspective of a legal professional to provide assistance.

\subsection{Datasets Construction}
\label{Tasks-Construct}
Based on the mentioned criteria for capability division and task preparation, we combined the majority of open-source datasets with a small amount of proprietary data to construct the evaluation dataset for our LAiW benchmark. The dataset is divided into two parts: Automatic and Manual.

\subsubsection{Automatic Evaluation Legal Tasks}
To facilitate a more efficient evaluation of LLMs, we construct all 14 tasks mentioned above into datasets that can be automatically assessed shown in Table \ref{tab:dataset}.
The primary sources of this data include previous years' CAIL competition data \cite{xiao2018cail2018, zhang2022recognition, huang2023high}, as well as the most commonly used open-source data \cite{ge2021learning, wu2020biased, LAWGPT-zh} for these tasks, including various types of law, such as criminal law, civil law, constitutional law, social law, and economic law.

As for the task type of our dataset, it contains three types of tasks: classification, named entity recognition, and text generation. 
The classification refers to tasks where LLMs provide answers in the form of multiple-choice questions. 
Named entity recognition involves accurately identifying legal entities in the text, while text generation tasks allow LLMs to freely perform legal tasks and flexibly generate responses. 

During the construction of the dataset, we designed different prompts for various tasks to support LLMs in providing better answers. We initially validated the quality of prompts using ChatGPT and further confirmed their validity through legal experts. Currently, all tasks exist in a zero-shot format\footnote{Examples 
and the detailed processing methods can be found in Appendix \ref{appendix:dataset auto} and Appendix \ref{sec:Example Prompt}.}.

\subsubsection{Manual Evaluation Legal Tasks}
As shown in automatic evaluation results \ref{sec:automatic evaluation results}, we observed that these LLMs may not align with the logic of legal practice. LLMs seem to be able to directly acquire complex legal application capabilities but perform poorly in some basic tasks. To further confirm the complex legal application capabilities of current LLMs in legal application scenarios, we add a manual evaluation focus on the ability of the third level.

Due to the cost of manual evaluation, we focus on two tasks that are more oriented toward LLMs for logical reasoning: Judicial Reasoning Generation and Legal Consultation, rather than the Case Understanding task, which is based on the analysis of legal texts with existing logic \footnote{The detailed processing methods for the datasets are outlined in Appendix \ref{appendix:dataset manual}.}.

\begin{table*}[!htb]

  \centering
  
  \resizebox{0.92\textwidth}{!}{
  \begin{threeparttable}
  \begin{tabular}{ccccccc}
    \toprule
    Model & Model Size & Model Domain  & From & Baseline & Creater & Link \\
    \midrule

    GPT-4 \cite{openai2023gpt4} & - & General & Api & - & OpenAI & - \\
    \cmidrule{1-7}
    ChatGPT & - & General & Api & - & OpenAI & - \\
    \cmidrule{1-7}
    Baichuan2-Chat \cite{baichuan2023baichuan2} & 13B & General & Open & - & Baichuan Inc & \href{https://github.com/baichuan-inc/Baichuan2}{[1]} \\
    \cmidrule{1-7}
     Baichuan & 7B  & General  &  Open  &  - &  Baichuan Inc &  \href{https://github.com/baichuan-inc/Baichuan-7B}{[2]}  \\
     \cmidrule{1-7}
     ChatGLM \cite{du2022glm} & 6B  & General  & Open  &  - & Tsinghua, Zhipu  &  \href{https://github.com/THUDM/ChatGLM-6B}{[3]}  \\
     \cmidrule{1-7}
     Llama \cite{touvron2023llama} &  7B & General  &  Application &  - &  Meta AI &  \href{https://github.com/facebookresearch/llama}{[4]}  \\
     \cmidrule{1-7}
     Llama \cite{touvron2023llama} & 13B  & General  & Application  & -  & Meta AI  &  \href{https://github.com/facebookresearch/llama}{[4]}  \\
     \cmidrule{1-7}
     Llama2-Chat \cite{touvron2023llama2} & 7B & General & Application & - & Meta AI & \href{https://huggingface.co/meta-llama/Llama-2-7b-chat-hf}{[5]} \\
     \cmidrule{1-7}
     Chinese-LLaMA \cite{chinese-llama-alpaca}  & 7B  & General  &  Open & Llama-7B   &  Yiming Cui &  \href{https://github.com/ymcui/Chinese-LLaMA-Alpaca}{[6]}  \\
     \cmidrule{1-7}
     Chinese-LLaMA \cite{chinese-llama-alpaca}  & 13B  & General  &  Open & Llama-13B   &  Yiming Cui &  \href{https://github.com/ymcui/Chinese-LLaMA-Alpaca}{[6]}  \\
     \cmidrule{1-7}
     Ziya-LLaMA\cite{fengshenbang} &  13B &  General & Open  & Llama-13B &  IDEA-CCNL &  \href{https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-v1}{[7]}  \\
     \cmidrule{1-7}
     HanFei \cite{HanFei} & 7B  & Law  &  Open &  - & SIAT NLP  & \href{https://github.com/siat-nlp/HanFei}{[8]}   \\
     \cmidrule{1-7}
     wisdomInterrogatory  &  7B &  Law & Open  & Baichuan-7B  &  ZJU, Alibaba, e.t &  \href{https://github.com/zhihaiLLM/wisdomInterrogatory}{[9]}  \\
     \cmidrule{1-7}
     Fuzi-Mingcha \cite{fuzi.mingcha} & 6B  &  Law & Open  & ChatGLM-6B  & irlab-sdu  &  \href{https://github.com/irlab-sdu/fuzi.mingcha}{[10]} \\
     \cmidrule{1-7}
     LexiLaw & 6B  & Law  &  Open & ChatGLM-6B  &  Haitao Li &  \href{https://github.com/CSHaitao/LexiLaw}{[11]}  \\
     \cmidrule{1-7}
     LaWGPT \cite{LAWGPT} & 7B  &  Law & Open  & Chinese-LLaMA-7B  & Pengxiao Song  & \href{https://github.com/pengxiao-song/LaWGPT}{[12]}   \\
     \cmidrule{1-7}
     Lawyer-LLaMA \cite{lawyer-llama-report} &  13B & Law  & Open  &  Chinese-LLaMA-13B & Quzhe Huang  & \href{https://github.com/AndrewZhe/lawyer-llama}{[13]}   \\
     \cmidrule{1-7}
     ChatLaw \cite{cui2023chatlaw} &  13B & Law  &  Open & Ziya-LLaMA-13B  & PKU-YUAN's Group  & \href{https://github.com/PKU-YuanGroup/ChatLaw}{[14]} \\
    
    \bottomrule
    \end{tabular}
    \end{threeparttable}
}
\caption{The LLMs evaluated in our work. LaWGPT and wisdomInterrogatory undergo pre-training on Chinese-LLaMA and Baichuan respectively, followed by fine-tuning. HanFei does not have a baseline model. Apart from GPT-4 and ChatGPT, these general LLMs ave a parameter size of 7-13B to ensure a size similar to legal LLMs.}
  \label{tab:LAiW-LLMs}%
\end{table*}%

\section{Evaluation for Benchmark}
In this section, we provide our metrics and scoring method for automatic evaluation and our metrics and criteria for manual evaluation.

\subsection{Automatic Evaluation Legal Tasks}
\label{sec:Automatic Evaluation Legal Tasks}
Automatic Evaluation Legal Tasks contains classification tasks, named entity recognition tasks, and text generation tasks.
For classification tasks, we select accuracy (Acc), miss rate (Miss), F1 score (F1), and matthews correlation coefficient (Mcc) as evaluation metrics for these tasks. They can assess LLMs for their understanding of questions (Miss), the effectiveness of their answers (Acc and F1), and their ability to identify imbalances (Mcc). 
For named entity recognition tasks, we use the accuracy of the LLMs in identifying every legal entities (Entity-Acc). 
For text generation tasks, we use ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L (RL) as evaluation metrics for this task. 

To evaluate the overall legal capabilities of LLMs, we further select a few key indicators for each task and calculate legal scores for LLMs based on these indicators as shown in Equation (\ref{eq:scores}).

\begin{equation}
\begin{cases}
    S_{\text{classification}}=F1*100, \\
    S_{\text{text generation}}=\frac{1}{3}(R1+R2+RL)*100, \\
    S_{\text{named entity recognition}}=\text{Entity-Acc}*100.
\end{cases}
\label{eq:scores}
\end{equation}

Then, the total score is calculated by averaging the scores of the three levels, which in turn are determined by averaging the scores of tasks within each level.

\subsection{Manual Evaluation Legal Tasks}
\label{sec:Manual Evaluation Legal Tasks}
First, to ensure the reliability of the assessment, we present criteria with several legal experts for manual evaluation. For the Judicial Reasoning Generation task, the criteria focus on completeness, relevance and accuracy. As for the Legal Consultation task, the criteria focus on fluency, relevance, and comprehensibility\footnote{A more detailed description is provided in Appendix \ref{Annotation rules and standards}.}.

We adopt the approach used in studies \cite{dubois2023alpacafarm,alpaca_eval} for manual evaluation, considering legal experts as evaluators, using reference answers as the baseline to calculate the win rate for the target LLMs. 
For example, when using the reference answer as the baseline, legal experts comprehensively assess the output of the target LLM and the reference answer from multiple judgment dimensions, and then choose the most satisfactory response. 

\section{Experiment}

In this section, we present relevant experiment settings and highlight the key results.

\subsection{Experiment Settings}
\label{sec:auto evaluation}
For the automatic evaluation, We evaluate 18 LLMs, including 7 mainstream legal LLMs \cite{cui2023chatlaw, LAWGPT}, their corresponding 6 baseline LLMs \cite{du2022glm, chinese-llama-alpaca, zhang2022recognition}, and 5 more effective general LLMs \cite{baichuan2023baichuan2, touvron2023llama} such as GPT-4 and ChatGPT. For fairness in evaluation, all LLMs did not utilize legal knowledge databases. Table \ref{tab:LAiW-LLMs} lists more detailed information about these LLMs.

For the manual evaluation, We choose the top-performing four legal LLMs in our automatic evaluation. They are Fuz-Mingcha \cite{fuzi.mingcha}, HanFei \cite{HanFei}, Lawyer-LLaMa \cite{lawyer-llama-report}, and LexiLaw. Furthermore, we also conducted manual assessments of the performance of both GPT-4 and ChatGPT.

    

        

\begin{table*}[!htb]
  \centering

  \resizebox{0.999\textwidth}{!}{
  \begin{tabular}{ccccccccccccccccccc}
    \toprule
    \multirow{2}[2]{*}{Model} & \multicolumn{6}{c}{Basic Information Retrieva}  & \multicolumn{7}{c}{Legal Foundation Inference} & \multicolumn{4}{c}{Complex Legal Application} & \multirow{2}[2]{*}{Total Score} \\
    \cmidrule(lr){2-7}
    \cmidrule(lr){8-14}
    \cmidrule(lr){15-18}
     & $B_{1}$ & $B_{2}$ & $B_{3}$ & $B_{4}$ & $B_{5}$ & Avg. & $L_{1}$ & $L_{2}$ & $L_{3}$ & $L_{4}$ & $L_{5}$ & $L_{5}$ & Avg. & $C_{1}$ & $C_{2}$ & $C_{3}$ & Avg. & \\
     \midrule 
     
    GPT-4   & 99.20 & 82.27 & 80.67 & 42.72 & 99.75 & 80.92 & 80.50 & 45.94 & 100.00 & 65.58 & 70.43 & 53.14 & 69.27 & 37.22 & 96.19 & 42.66 & 58.69 & 69.63 \\
    ChatGPT  & 99.05 & 79.32 & 61.73 & 41.01 & 98.85 & 75.99 & 57.16 & 46.17 & 99.28 & 47.35 & 62.85 & 37.08 & 58.32 & 35.64 & 90.70 & 47.55 & 57.96 & 64.09 \\
    \midrule
    Baichuan2-13B-Chat & 45.07 & 52.18 & 47.31 & 26.67 & 97.14 & 53.67 & 4.12 & 2.99 & 17.50 & 61.43 & 67.91 & 38.24 & 32.03 & 52.61 & 81.29 & 41.31 & 58.40 & 48.04 \\
     Baichuan-7B   & 17.81 & 2.87 & 0.00 & 26.89 & 58.45 & 21.20 & 1.74 & 0.00 & 1.18 & 1.03 & 64.50 & 24.32 & 15.46 & 40.27 & 33.79 & 18.51 & 30.86 & 22.51 \\
     ChatGLM-6B    & 72.55 & 49.82 & 1.06 & 42.87 & 91.27 & 51.51 & 14.18 & 39.03 & 67.57 & 44.84 & 33.02 & 23.86 & 37.08 & 35.39 & 86.90 & 35.02 & 52.44 & 47.01 \\
     Llama-7B  & 19.53 & 1.43 & 0.00 & 11.40 & 23.23 & 11.12 & 1.31 & 0.00 & 35.19 & 1.03 & 49.15 & 5.74 & 15.40 & 0.61 & 56.08 & 10.93 & 22.54 & 16.35 \\
     Llama-13B   & 28.16 & 7.66 & 0.00 & 9.94 & 46.80 & 18.51 & 1.86 & 0.00 & 36.79 & 5.80 & 40.46 & 5.57 & 15.08 & 11.19 & 65.68 & 11.34 & 29.40 & 21.00 \\
     Llama2-7B-Chat   & 48.24 & 11.93 & 0.19 & 15.79 & 83.17 & 31.86 & 0.74 & 0.00 & 3.88 & 7.31 & 62.09 & 2.59 & 12.77 & 28.76 & 69.51 & 17.65 & 38.64 & 27.76 \\
     Chinese-LLaMA-7B & 24.39 & 7.45 & 0.00 & 30.77 & 48.97 & 22.32 & 2.02 & 0.76 & 31.79 & 1.03 & 65.24 & 8.63 & 18.25 & 26.34 & 62.31 & 13.81 & 34.16 & 24.91 \\
     Chinese-LLaMA-13B  & 30.34 & 5.47 & 0.00 & 7.73 & 61.56 & 21.02 & 3.28 & 5.05 & 20.21 & 5.33 & 64.46 & 16.60 & 19.16 & 18.86 & 73.15 & 12.40 & 34.80 & 24.99 \\
     Ziya-LLaMA-13B & 66.39 & 58.42 & 48.94 & 38.85 & 94.73 & 61.47 & 5.64 & 0.76 & 53.18 & 55.62 & 36.07 & 25.38 & 29.44 & 30.12 & 83.96 & 25.26 & 46.45 & 45.79 \\
    \midrule
     HanFei-7B & 24.91 & 7.25 & 51.63 & 21.14 & 82.18 & 37.42 & 1.15 & 0.00 & 5.27 & 2.73 & 66.81 & 22.03 & 16.33 & 51.31 & 81.19 & 27.43 & 53.31 & 35.69 \\
     wisdomInterrogatory-7B  & 0.39 & 0.19 & 0.00 & 34.75 & 27.99 & 12.66 & 3.57 & 35.38 & 2.32 & 1.30 & 16.76 & 3.34 & 10.45 & 13.91 & 68.02 & 18.17 & 33.37 & 18.83 \\
     Fuzi-Mingcha-6B & 58.95 & 12.58 & 0.38 & 47.92 & 78.57 & 39.68 & 4.70 & 20.84 & 31.53 & 48.40 & 32.66 & 26.64 & 27.46 & 49.55 & 80.48 & 34.10 & 54.71 & 40.62 \\
     LexiLaw-6B  & 47.16 & 2.89 & 31.35 & 41.79 & 83.43 & 41.32 & 2.11 & 18.49 & 3.40 & 6.42 & 4.35 & 18.51 & 8.88 & 25.85 & 80.81 & 24.52 & 43.73 & 31.31 \\
     LaWGPT-7B  & 10.15 & 2.59 & 0.00 & 27.69 & 36.92 & 15.47 & 1.62 & 0.00 & 20.04 & 1.03 & 54.55 & 8.40 & 14.27 & 35.23 & 65.62 & 14.11 & 38.32 & 22.69 \\
     Lawyer-LLaMA-13B & 20.26 & 1.52 & 7.88 & 51.13 & 73.44 & 30.85 & 2.19 & 0.76 & 0.24 & 2.12 & 12.75 & 20.26 & 6.39 & 34.00 & 85.68 & 31.83 & 50.50 & 29.25 \\
     ChatLaw-13B& 67.08 & 31.29 & 52.21 & 41.33 & 98.20 & 58.02 & 0.00 & 0.00 & 37.82 & 30.85 & 6.58 & 0.00 & 12.54 & 0.00 & 20.23 & 0.00 & 6.74 & 25.77 \\
     
    \bottomrule
    \end{tabular}
    }

    \caption{The scores of LLMs at various levels of the LAiW based on equation (\ref{eq:scores}). Here, $B_1$ to $B_5$ respectively represent the tasks: Legal Article Recommendation, Element Recognition, Named Entity Recognition, Judicial Summarization, and Case Recognition. $L_1$ to $L_6$ respectively represent the tasks: Controversy Focus Mining, Similar Case Matching, Charge Prediction, Prison Term Prediction, Civil Trial Prediction, and Legal Question Answering. $C_1$ to $C_3$ respectively represent the tasks: Judicial Reasoning Generation, Case Understanding, and Legal Consultation.}
    \label{tab:scores}%
\end{table*}%

\subsection{Automatic Evaluation Results}
\label{sec:automatic evaluation results}

The scores for each level and the total score of our automated evaluation are shown in Table \ref{tab:scores}\footnote{The complete results of each task are available in Appendix \ref{appendix:results of LLMs evaluation}.}.
We analyze the results from three different aspects: overall results, the legal logic of Chinese Legal LLMs, and the capabilities of Chinese Legal LLMs.

\textbf{Overall results.} When compared to GPT-4 and ChatGPT, there still exists a significant gap between the current open-source LLMs and specifically trained legal LLMs.

From Table \ref{tab:scores}, we find that GPT-4 and ChatGPT maintain optimal performance in most tasks. 
They significantly outperform the current open-source LLMs at various levels of scoring.
Among the open-source LLMs, only Baichuan2-Chat, ChatGLM, and Ziya-LLaMA achieve a total score of 45 or above. However, their performance in the BIR and LFI levels still lags far behind GPT-4 and ChatGPT.
As for the specifically trained legal LLMs, the top four performing ones are Fuzi-Mingcha, HanFei, LexiLaw, and Lawyer-LLaMA. However, their overall scores are lower, all below 45.

We believe that the reason for this phenomenon is twofold: first, due to the large number of parameters in GPT-4 and ChatGPT; second, we during the pretraining phase, GPT-4 and ChatGPT may have been exposed to a larger amount of data. 
Since the open-source LLMs we selected are primarily aimed at the Chinese community, the data they collect may be more limited compared to GPT-4 and ChatGPT. 
GPT-4 and ChatGPT cover a wide range of legal data in multiple languages. In this case, it is reasonable for them to have higher scores in the BIR and LFI levels which focus on the basic legal logic and legal knowledge.

\textbf{ The legal logic of Chinese Legal LLMs.} 
The Legal LLMs may not have the logic of legal practice. While they demonstrate strong text generation abilities to complete complex legal applications, they perform poorly in basic information retrieval and legal foundation inference tasks.

Observing Table \ref{tab:scores}, it is evident that the majority of legal LLMs score nearly 20 points higher in the application of direct logic (CLA level) compared to the scores in BIR and LFI levels. 
This is contrary to the logic typically found in law. 
It suggests that these LLMs seem to have learned the patterns of generating legal texts directly, but have not grasped the legal reasoning behind these patterns. 
As a result, LLMs are unable to effectively identify the major and minor premises in law and lack the ability to reason to a conclusion.
However, for the BIR level, ChatLaw stands out among legal LLMs.
It instead has a strong ability at the BIR level, which may stem from the outstanding performance of its base model Ziya - LLaMA at this level.

This raises concerns that current legal LLMs may not meet the expectations of legal experts, posing potential risks to the trust of LLMs in the legal domain.

\textbf{ The capabilities of Chinese Legal LLMs.} Fine-tuned legal LLMs have improved the normativity of legal text generation but may lose the legal logic. 
In addition, for legal LLMs, undergoing additional pre-training on legal text may be the path to achieving various legal capabilities and legal logic.

From table \ref{tab:scores}, legal LLMs such as Fuzi-Mingcha, WisdomInterrogatory, LaWGPT and Lawyer-LLaMA compared to their base models exhibit improvements in CLA level. 
Most of these legal LLMs improve the normativity of generated texts in legal text generation through instruction tuning. However, they may lead to a decrease in ability at the BIR and LFI levels, which indicates that instruction tuning may not enable LLMs to possess legal logic.

On the other hand, legal LLMs like HanFei, which focus more on pre-training, may indicate how Chinese Legal LLMs acquire ability and logic.
HanFei, based on an older LLM structure (Bloomz), underwent extensive pre-training on legal texts and demonstrated capabilities on par with subsequent legal LLMs.
Furthermore, GPT-4 and ChatGPT, models with extensive pre-training on large corpora, also showed excellent performance at the BIR and LFI levels. 
These findings indicate that developing legal reasoning and comprehensive abilities may require learning from a significant amount of pre-training text, rather than just fine-tuning. 

    
        

\subsection{Manual Evaluation Results}
\begin{table}[!htb]
  \centering

  \resizebox{0.49\textwidth}{!}{
  \begin{tabular}{c|ccc|ccc}
    \toprule
    
    \multirow{2}[0]{*}{Model} & \multicolumn{3}{c|}{Judicial Reasoning Generation}  & \multicolumn{3}{c}{Legal Consultation}  \\
    \cmidrule{2-7}
     & Total Score & Win Rate & Std & Total Score  & Win Rate & Std \\
    \midrule
    GPT-4 & 44.72 & 0.38 & 0.18 & \underline{43.97} & \textbf{0.85} & 0.15 \\
    \cmidrule{1-7}
    ChatGPT & 41.74 & 0.35 & 0.27 & \textbf{48.79} & \underline{0.79} & 0.12 \\
    \cmidrule{1-7}
    Fuzi-Mingcha & \textbf{63.58} & \textbf{0.65} & 0.35 & 35.22 & 0.51 & 0.19 \\
    \cmidrule{1-7}
    HanFei & \underline{60.13}  & \underline{0.59} & 0.26 & 27.06 & 0.33 & 0.06 \\
    \cmidrule{1-7}
    LexiLaw & 43.48 & 0.31 & 0.15 & 25.53 & 0.24 & 0.02 \\
    \cmidrule{1-7}
    Lawyer-LLaMA & 39.61 & 0.30 & 0.26 & 33.27  & 0.51 & 0.21 \\
        
    \bottomrule
    \end{tabular}
}
\caption{The average win rate (WR) of LLMs for the Judicial Reasoning Generation and Legal Consultation tasks. The total score represents the score obtained by LLMs through automatic evaluation on our benchmark.}
  \label{tab:win rate}%
\end{table}%

According to the criteria for expert evaluation in Section \ref{sec:Manual Evaluation Legal Tasks} and the calculated average win rate scores of legal experts shown in Table \ref{tab:win rate}\footnote{The complete results are available in Appendix \ref{appendix:win rate for each expert} and the experts' agreement are available in Appendix \ref{The agreement scores}.}.
Based on these results, we have two findings.

\textbf{The lack of legal logic in LLMs still exists in Complex Legal Applications.}
For the task of Judicial Reasoning Generation that requires a strong understanding of legal logic, even models with powerful text generation capabilities like GPT-4 and ChatGPT may have deficiencies in legal logic. 
As described in Section \ref{sec:Manual Evaluation Legal Tasks}, the Judicial Reasoning Generation task focuses on accuracy, such as the correct citation of legal articles and reasoning based on the citation. This directly connects to the basic logic of legal.
Therefore, most of the LLMs' win rates are much lower than 0.5, indicating that strong text generation capabilities cannot directly replace legal logic.

For tasks like Legal Consultation, there is a lower requirement for legal logic but a higher requirement for fluency. 
Therefore, during the manual evaluation, legal experts tend to prefer models with stronger language capabilities, which is the strength of GPT-4 and ChatGPT. This capability can also be learned by legal LLMs through instruction tuning. 
As a result, the final evaluation results of legal experts also reflect this, giving higher win rates to all LLMs, with most even surpassing the annotated answers.

\textbf{Manual evaluation and automatic evaluation share similarities, enhancing the reliability of our automatic evaluation.}
From Table \ref{tab:win rate}, we can observe that the results of manual evaluation and automated evaluation are similar. 
For instance, in both evaluation rounds, Fuzi-Mingcha and HanFei performed best in the Judicial Reasoning Generation task, while GPT-4 and ChatGPT excelled in the Legal Consultation task. 
In addition, despite its shortcomings as an automated evaluation metric in many cases, Rouge still demonstrates a certain level of capability when reflecting legal logic.
This indicates that our automatic evaluation can provide a reliable path for the legal logic assessment of legal LLMs and further reduce manual effort. 
Additionally, our assessment of legal logic is granular, and the degree of emphasis on legal logic in different scenarios can also be reflected by our automatic evaluation of different tasks.

\section{Conclusion}
This paper aims to construct a Chinese Legal LLMs benchmark based on the logic of legal practice. To match the process of syllogism in legal logic, the benchmark categorizes LLM legal capabilities into basic information retrieval, legal foundation inference, and complex legal application, and includes 14 tasks. During benchmark evaluations, automated and manual evaluations were conducted. Automated results showed that existing LLMs excel in text generation for complex legal applications but struggle with basic information retrieval and legal foundation inference, leading to a lack of legal logic and distrust among legal experts. Manual evaluations revealed that while LLMs can bridge the gap in legal logic in some application scenarios, they still exhibit significant discrepancies as legal experts. This highlights the urgent need for better training and evaluation methods for LLMs in the legal domain. Therefore, our benchmark, for the first time from a legal perspective, reveals the professional legal capabilities of existing legal LLMs, greatly facilitating the subsequent evaluation and development work of legal LLMs. 

\newpage
\section{Limitations and Future Work}
\label{sec:limitations}
Due to the significant amount of work involved in using a computer-based approach to construct the logic of legal practice related tasks and assessments, we also have the following two limitations and areas for future work:

1) In the manual evaluation experiment, to save workload, only a portion of the data and LLMs are sampled and chosen for evaluation, rather than assessing all of them. In the future, we will collaborate more with legal experts to ensure a more comprehensive human assessment.

2) Most of the tasks are derived from publicly available legal data, which may not fully evaluate the logic of legal practice for LLMs. We will further develop additional tasks to refine the logic of legal practice at each stage.

\section{Ethics Statement}
Due to the sensitivity of the legal field, we have conducted a comprehensive review of the relevant data in this benchmark. The open-source datasets we used all have corresponding licenses. We have masked sensitive information, such as names, phone numbers, and IDs, and legal experts have conducted ethical evaluations.

\section*{Acknowledgements}

\end{document}