\title{LawBench: Benchmarking Legal Knowledge of Large Language Models}

\begin{document}

\maketitle

\begin{abstract}

Large language models (LLMs) have demonstrated strong capabilities in various aspects. However, when applying them to the highly specialized, safe-critical legal domain, it is unclear how much legal knowledge they possess and whether they can reliably perform legal-related tasks. To address this gap, we propose a comprehensive evaluation benchmark \emph{LawBench}. LawBench has been meticulously crafted to have precise assessment of the LLMs' legal capabilities from three cognitive levels: (1) \emph{Legal knowledge memorization}: whether LLMs can memorize needed legal concepts, articles and facts; (2) \emph{Legal knowledge understanding}: whether LLMs can comprehend entities, events and relationships within legal text; (3) \emph{Legal knowledge applying}: whether LLMs can properly utilize their legal knowledge and make necessary reasoning steps to solve realistic legal tasks. LawBench contains 20 diverse tasks covering 5 task types: single-label classification (SLC), multi-label classification (MLC), regression, extraction and generation. We perform extensive evaluations of 51 LLMs on LawBench, including 20 multilingual LLMs, 22 Chinese-oriented LLMs and 9 legal specific LLMs. The results show that GPT-4 remains the best-performing LLM in the legal domain, surpassing the others by a significant margin. While fine-tuning LLMs on legal specific text brings certain improvements, we are still a long way from obtaining usable and reliable LLMs in legal tasks. All data, model predictions and evaluation code are released in \url{https://github.com/open-compass/LawBench/}. We hope this benchmark provides in-depth understanding of the LLMs' domain-specified capabilities and speed up the development of LLMs in the legal domain.

\end{abstract}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{Figure/law_bench_radar_plot_zero_shot_new.pdf}
    \caption{Results (zero-shot) of six best-performing LLMs evaluated on 20 diverse legal tests covering three cognitive dimensions: \textcolor[HTML]{004995}{legal knowledge memorization}, \textcolor[HTML]{499500}{understanding}, and \textcolor[HTML]{e17300}{applying}.}
    \label{fig:law_bench_radar_plot}
\end{figure}

\section{Introduction}
The artificial intelligence community has witnessed notable progress in the large language models (LLMs) recently. The latest large language models, such as GPT-4~\cite{openai2023gpt4} and LLaMA~\cite{touvron2023llama}, have showcased remarkable capabilities and intricate skills that are comparable to, and in some cases, even surpass human capabilities~\cite{huang2023chatgpt}.
To quantitatively assess the capabilities of LLMs, evaluation benchmarks serve a pivotal role in their development. Numerous recent endeavors mainly concentrate on enhancing general and universal capabilities, such as understanding world knowledge and executing complex reasoning~\cite{zhong2023agieval,yu2023kola}.

Previous research on evaluation has predominantly concentrated on exploring the diverse aspects of the general capabilities of LLMs. SuperGLUE~\cite{wang2019superglue} is crafted to assess various facets of language understanding, encompassing reading comprehension, commonsense reasoning, and entailment. Benchmarks such as MMLU~\cite{hendrycks2020measuring} and BIG-bench~\cite{srivastava2023beyond} have been introduced to cover a wide range of NLP tasks for LLM evaluations. Furthermore, there exist benchmarks~\cite{fu2023chain,suzgun2022challenging,fu2023codeapex,chen2021evaluating} explicitly designed to scrutinize the advanced abilities of LLMs that manifest with scale, such as reasoning~\cite{sawada2023arb} and coding~\cite{lu2021codexglue}. Beyond these specific benchmarks, more recent initiatives, like OpenCompass~\cite{2023opencompass}  and HELM~\cite{liang2022holistic}, aspire to offer a comprehensive perspective on the capabilities of LLMs.

However, evaluating Large Language Models (LLMs) necessitates not only a focus on general capabilities but also the incorporation of domain-specific benchmarks for assessing models specialized in particular fields~\cite{zhao2023domain}. Legal tasks encompass a broad spectrum of applications, predominantly text-based, necessitating comprehension and interpretation of highly professional legal texts. Currently, they are primarily conducted by legal experts, who require years of extensive specialized training to process legal cases. Endowing LLMs with legal expertise can not only improve the working efficiency of legal officers, but also address the overwhelming demand of legal assistance from non-professionals, and thereby improve public access to justice~\cite{cui2022survey,trozze2023large}. Consequently, we prioritize establishing benchmarks to measure the legal knowledge of existing LLMs.

In contrast to assessments that primarily focus on testing model's ability to pass legal bar exams~\cite{bommarito2022gpt,zhong2023agieval},  which are not always representative of the actual use-cases for LLMs~\footnote{As shown in \cite{chalkidis2022lexglue}, ChatGPT excels in passing legal bar exams but struggles at performing realistic legal tasks.}, our emphasis is on examining a structured set of legal skills required in real-world scenarios. In the legal domain, there have been works ensembling legal-related tasks including the lexglue~\cite{chalkidis2022lexglue} focusing on EU and American laws, and LBOX OPEN~\cite{hwang2022multi} focusing on South Korean laws, \cite{chalkidis2023chatgpt} further transformed lexglue into zero-shot forms to test the ability of LLMs to complete these tasks. LEGALBENCH~\cite{guha2023legalbench}, in particular, presented the first steps towards constructing an interdisciplinary collaborative legal reasoning benchmark for the English language and evaluated 20 LLMs in 162 legal tasks. 

Nonetheless, legal systems vary significantly among different countries, highlighting the importance of establishing different standards for each legal system. The Chinese legal system is rooted in the civil law family. Unlike the common law system, which is widely accepted in the U.S. and the U.K., judges in the civil law system are obliged to stay neutral, respect the established statutory law articles and ground their decisions on them. Understanding and applying existing statutes and codes, rather than studies of precedents, are of paramount importance~\cite{zheng1986china}. Therefore, it is necessary to design a separate set of evaluation tasks to emphasize the required skill set for the Chinese law system. 

With this in mind, we present \emph{LawBench}:  a meticulously crafted, comprehensive evaluation benchmark to assess the LLMs’ capabilities on performing legal-related tasks under the Chinese civil-law system. LawBench consists of 20 diverse tasks following 5 categories:  single-label classification (SLC), multi-label classification (MLC), regression, extraction and generation.
We divided these tasks into 3 skill levels according to widely accepted Bloom's cognitive models~\cite{krathwohl2002revision}: (1) Legal knowledge memorization: whether LLMs can memorize needed legal concepts, articles and facts; (2) Legal knowledge understanding: whether LLMs can comprehend entities, events and relationships within legal text; (3) Legal knowledge applying: whether LLMs can properly utilize their legal knowledge and make necessary reasoning steps to solve realistic legal tasks. Intuitively LLMs must first obtain lower-level skills before excelling in higher-level skills. This division method provides a structured overview of the skill set required for legal-related tasks. It can also facilitates an exploration of the similarities
and differences between the learning mechanisms of LLMs and humans.

We extensively tested 51 popular LLMs,  including 20 multilingual LLMs, 22 Chinese-oriented LLMs and 9 legal specific LLMs. To effectively extract answers from the predictions of LLM generations, we design suitable rules, regular expressions and metrics for every individual task. The benchmark and evaluation code are integrated into the OpenCompass platform~\cite{2023opencompass} to enable easy reproduction. The evaluation results are shown in Figure \ref{fig:law_bench_radar_plot}. We find that although legal specific fine-tuning usually improves upon their base model, they are still significantly lagging behind general LLMs, which occupy the top six spots in the averaged zero-shot performance. We analyze the impact of various factors on the results, such as supervised fine-tuning (SFT), reinforcement learning from human feedback (RLHF)~\cite{christiano2017deep}, model size, legal specific fine-tuning, etc. Important suggestions are summarized to better guide the future development of legal LLMs for the Chinese community.

\section{Related Work}
\paragraph{Large Language Models}
Large language models (LLMs) trained on massive amounts of data have shown impressive abilities in generating high-quality, coherent text and following zero-shot or few-shot instructions in a diverse set of tasks~\cite{openai_chatgpt,openai2023gpt4,chowdhery2022palm,touvron2023llama1,touvron2023llama}. These models are usually trained following three steps: pre-training, supervised fine-tuning (SFT) and alignment with human or AI feedback ~\cite{christiano2017deep,shen2017estimation,wei2021finetuned,ouyang2022training,lee2023rlaif,casper2023open} etc. As most public available LLMs focus on training on English corpora, many efforts have been devoted to extending LLMs to Chinese. These works either pre-training a new LLM from scratch on Chinese-centric corpora~\cite{su2022welm,zeng2022glm,du2022glm,sun2023moss,2023internlm}, or expanding the vocabulary of an existing English-centric LLM then performing SFT on Chinese instruction data~\cite{chinese-llama-alpaca,BELLE,belle2023exploring}. 
There have also been studies that are dedicated to adapting LLMs to the legal domain by fine-tuning on legal specific corpora~\cite{cui2023chatlaw,huang2023lawyer,yue2023disclawllm}. However, a comprehensive evaluation to compare the existing LLMs regarding their legal knowledge is still lacking. Our focus is primarily on models that can complete corresponding tasks based on instructions, so we exclude pre-trained small language models such as Lawformer~\cite{xiao2021lawformer} and LegalBERT~\cite{chalkidis2020legal}, which require task-specific fine-tuning to perform competiviely. 

\paragraph{Existing Benchmarks}

As the rapid development of LLMs, conventional approaches of evaluating a model's performance on a single task through fine-tuning~\cite{xu2020clue,wang2018glue,louis2022statutory,zhang2022mdia,shen2023xpqa} is no longer adequate for evaluating LLMs. A growing body of research works have recently focused on developing more comprehensive and systematic benchmarks to evaluate various capabilities of LLMs. Examples include AGIEval~\cite{zhong2023agieval} which covers human-centric standardized exams, such as college entrance exams, law school admission tests, math competitions, and lawyer qualification tests, HELM~\cite{liang2022holistic} which measures 7 metrics (accuracy, calibration, robustness,
fairness, bias, toxicity, and efficiency) for each of 16 core scenarios to the extent possible, KOLA~\cite{yu2023kola} which focuses on knowledge-oriented assessment under four-level taxonomy of knowledge-related abilities, and MMBench~\cite{liu2023mmbench} which is designed specifically to evaluate vision-language models. There have also been efforts in constructing benchmarks for Chinese such as CMMLU~\cite{li2023cmmlu}, GAOKAO~\cite{zhang2023evaluating} and C-Eval~\cite{huang2023c}. Some of them focus on specific domains such as CMB~\cite{wang2023cmb} in the medical domain and Fin-eval~\cite{zhang2023fineval} in the finance domain. In the legal domain, there have also been works ensembling legal-related tasks including the lexglue~\cite{chalkidis2022lexglue} focusing on EU and American laws, and LBOX OPEN~\cite{hwang2022multi} focusing on South Korean laws, but they did not formulate tasks into the instruction-following formats for LLMs. Recently, legal-bench was released in evaluating LLMs on 162 legal-related tasks based on American laws~\cite{guha2023legalbench}. Our work follows a similar line to extend the evaluation to Chinese laws. Given the uniqueness of the Law system in People's Republic of China and the diversity of the legal tasks covered in this work, we believe
that LawBench will contribute to the multilinguality of global legal research and promote research in specializing language models to a specific domain.

\section{LawBench Construction}
In this section, we provide a detailed description of the principles behind the design of LawBench and the test task selection.

\subsection{The Hierarchical Ability Taxonomy of LawBench}
\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{Figure/lawbench_cognitive_level_new_2.pdf}
    \caption{Three cognitive dimensions for evaluating large language models in LawBench. In order to specialize in legal tasks, LLMs must be able to (1) memorize necessary legal concepts, terminologies, articles and facts; (2) understand entities, events and relationships in legal text; and finally (3) simulate law professionals to apply legal knowledge and necessary reasoning in solving realistic tasks.}
    \label{fig:cognitive}
\end{figure}

When evaluating large language models, there is a preference for using a variety of tasks to assess their capabilities. A hierarchical evaluation system allows us to better understand the abilities of large language models in different aspects. Instead of categorizing tasks solely based on their difficulty~\cite{huang2023c}, we refer to the widely used Bloom's cognitive model~\cite{krathwohl2002revision} to classify tasks into different dimensions~\cite{yu2023kola}. Bloom's Taxonomy system was initially proposed by the educational psychologist Benjamin Bloom and his collaborators in 1956 and has been widely applied and developed in the following decades. It has effectively aided teachers in curriculum design and the assessment of student learning outcomes. Bloom's Taxonomy divides learning objectives in the cognitive domain into six levels, from the lowest to the highest: Remember, Understand, Apply, Analyze, Evaluate, and Create. These levels describe the depth and complexity of cognitive learning and provide an organized framework. Teachers can use Bloom's Taxonomy to ensure diversity and completeness in course objectives. By combining learning objectives at different levels, comprehensive student development can be promoted, encouraging them to progress from simple memorization and understanding to higher-level analysis, evaluation, and creation.

Inspired by this classification approach, we simplified Bloom's cognitive hierarchy model and kept the first three categories in Bloom's taxonomy to assess the legal knowledge of LLMs:

\begin{enumerate}
    \item \textbf{Knowledge Memorization}: The memorization level measures the basic requirement of remembering legal-related knowledge. It tests LLMs' ability in memorization and recitation of basic legal-domain knowledge such as regulations, cases, concepts, common sense, legal facts and terminologies.
    \item \textbf{Knowledge Understanding}: The understanding level involves understanding the meanings and connotations of legal documents. This includes the ability to comprehend and interpret legal concepts, text, and issues, e.g., identifying entities and relationships within legal texts, detect types of legal issues and points of dispute, among others.
    \item \textbf{Knowledge Applying}: The applying level requires LLMs to integrate legal knowledge, reason over it and address real-world legal cases. It covers the model's logical reasoning abilities to perform legal consultation, judicial assistance, as well as numerical reasoning abilities to calculate involved amount.
\end{enumerate}

Note that under this taxonomy, some tasks may not strictly belong to just one category; they may involve other abilities as well. We have categorized these tasks based on their primary capabilities.

\begin{table}[htbp]
  \centering
  \small
  \resizebox{\columnwidth}{!}{
    \begin{tabular}{llllcc}
    \toprule
    \textbf{Cognitive Level} & \textbf{ID} & \textbf{Task} & \textbf{Data Source} & \textbf{Metric} & \textbf{Type} \\
    \midrule
    \multirow{2}{*}{\shortstack[l]{\textbf{Legal Knowledge}\\ \textbf{Memorization}}} & 1-1 & Article Recitation & FLK & Rouge-L & Generation \\
          & 1-2 & Knowledge Question Answering & JEC\_QA & Accuracy & SLC \\\midrule
    \multirow{10}{*}{\shortstack[l]{\textbf{Legal Knowledge}\\ \textbf{Understanding}}} & 2-1 &  Document Proofreading & CAIL2022  & F0.5 & Generation\\
    & 2-2 & Dispute Focus Identification & LAIC2021  & F1 & MLC\\
          & 2-3 & Marital Disputes Identification & AIStudio & F1 & MLC\\
          & 2-4  & Issue Topic Identification & CrimeKgAssitant & Accuracy & SLC\\
        & 2-5 & Reading Comprehension & CAIL2019  & rc-F1 & Extraction\\
          & 2-6 & Named-Entity Recognition & CAIL2022  & soft-F1 & Extraction\\
          & 2-7 & Opinion Summarization & CAIL2021  & Rouge-L & Generation\\
          & 2-8 & Argument Mining  & CAIL2022  & Accuracy &SLC\\
          & 2-9 & Event Detection  & LEVEN  & F1 &MLC\\
          & 2-10 & Trigger Word Extraction & LEVEN  & soft-F1 & Extraction\\
          \midrule
    \multirow{8}{*}{\shortstack[l]{\textbf{Legal Knowledge}\\ \textbf{Applying}}} & 3-1 & Fact-based Article Prediction & CAIL2018 & F1 & MLC\\
          & 3-2 & Scene-based Article Prediction & LawGPT & Rouge-L &Generation\\
          & 3-3 & Charge Prediction & CAIL2018 & F1 &MLC\\
          & 3-4 & Prison Term Prediction w.o. Article & CAIL2018 & nLog-distance &Regression\\
          & 3-5 & Prison Term Prediction w. Article & CAIL2018 & nLog-distance &Regression\\
          & 3-6 & Case Analysis & JEC\_QA & Accuracy &SLC\\
          & 3-7 & Criminal Damages Calculation & LAIC2021  & Accuracy &Regression \\
          & 3-8 & Consultation & hualv.com & Rouge-L &Generation\\
    \bottomrule
    \end{tabular}
    }
    \caption{\small Task list in LawBench. Tasks correspond to cognitive dimensions: legal knowledge memorization, understanding and applying, and 5 task types: generation, single-label classification (SLC), multi-label classification (MLC), regression, and extraction.}
  \label{tab:tasks}%
\end{table}%

\subsection{Data Source and Selected Tasks}
We selected 20 tasks falling under the above-mentioned capability levels. Every task is assigned a unique task id for better distinction. The task list is provided in Table~\ref{tab:tasks}. There can exist datasets belonging to the same tasks. When selecting the dataset for every task, we choose the most recent available version. Furthermore, certain tasks like legal case retrieval requires processing very long documents, which can surpass the length limit for most LLMs, so we do not include them to LawBench for now. When constructing LawBench, we have made efforts to format the prompts in a way that best aligns with user habits with clear instructions about the answer format, so that we can assess the ability of LLMs in assisting legal tasks in realistic scenarios.

\textbf{Legal Knowledge Memorization Tasks}

Legal knowledge memorization tasks examine to which extent large language models encode legal knowledge within their parameters. While this knowledge can be enhanced with external retrievers, it is still beneficial to memorize necessary legal knowledge because (1) There is currently no reliable mechanism to guarantee the accurate retrieval of legal provisions. Memorizing useful knowledge within model parameters can help combat the noise from external retrievers~\cite{xie2023adaptive,neeman2023disentqa}; (2) It is very difficult, if not impossible, to retrieve all needed legal knowledge for complicated reasoning tasks. The model must know basic legal concepts to connect the retrieved knowledge smoothly~\cite{shen2022product,ziems2023large,yu2023generate}; (3) Relying on the parametric knowledge instead of external retrievers can significantly reduce the online latency~\cite{shen2022semipqa,shen2022low,tay2022transformer}. 

There are two major types of legal knowledge that requires memorizing: (1) core law articles and regulation content and (2) other fundamental legal concepts, notions and rules. We construct two tasks corresponding to these two types of knowledge: 
\begin{itemize}
    \item \textbf{Article recitation} (1-1): \emph{Given a law article number, recite the article content}. We collected the contents of laws and regulations from the national database~\footnote{\url{https://flk.npc.gov.cn/}} and consulted students with a legal background to select 152 sub-laws under the 5 core laws.
    We further incorporated updated laws and regulations, including constitutional amendments, to evaluate the model's ability to comprehend legal changes.
    \item \textbf{Knowledge question answering} (1-2): \emph{Given a question asking about basic legal knowledge, select the correct answer from 4 candidates}. We collect knowledge-based questions from the JEC-QA tasks~\cite{zhong2020jec}. To simplify the process of locating answers during the test, we exclusively chose single-label questions from them.
\end{itemize}
Examples of these two tasks are in Appendix~\ref{app:instruction_memorization}.

\textbf{Legal Knowledge Understanding Tasks}

Legal knowledge understanding tasks examine to which extent large language models can comprehend entities, events, and relationships within legal texts. Understanding legal text is a pre-condition to utilize the knowledge in concrete downstream applications~\cite{cui2022survey}. In total, we selected 10 tasks corresponding to different levels of legal knowledge understanding:

\begin{itemize}
    \item \textbf{Document Proofreading} (2-1): \emph{Given a sentence extracted from legal documents, correct its spelling, grammar and ordering mistakes, return the corrected sentence}. Legal documents, as carriers of judicial authorities and the exercise of legal rights by citizens, demand utmost precision in their textual content. We sample the original and corrected legal sentences from the CAIL2022 document proofreading task. Possible mistake types are inserted into the instructions to let the model directly output the corrected sentence.
    \item \textbf{Dispute Focus Identification} (2-2): \emph{Given the original claims and responses of the plaintiff and defendant, detect the points of dispute}. In civil cases, the points of dispute represent the core of conflicts, intersection of contradictions, and issues over which the parties involved in the case are in contention. The automated recognition and detection of points of contention have practical significance and necessity for the development of the rule of law in our country. Specifically, we will provide the trial-related content from judgment documents, including the sections on claims and responses. The cases involve various legal matters such as civil loans, divorce, motor vehicle traffic accident liability, financial loan contracts, and more. We have carefully selected common types of points of contention from LAIC2021 to construct this test set.
    \item \textbf{Marital Disputes Identification} (2-3): \emph{Given a sentence describing marital disputes, classify it into one of the 20 pre-defined dispute types}. Marital disputes refer to the total sum of various disputes arising from love, marriage, and divorce. Among civil disputes, marital disputes are a common type of dispute.  We have selected a publicly available marriage text classification dataset on AiStudio~\footnote{\url{https://aistudio.baidu.com/datasetdetail/181754}}. This dataset consists of 20 categories, and a single text entry may have multiple labels.
    \item \textbf{Issue Topic Identification} (2-4): \emph{Given a user inquiry, assign it into one of pre-defined topics}. User inquiries are typically vague. Identifying the relevant topics in legal consultations can help legal professionals better pinpoint key issues. We obtain the data from the CrimeKgAssistant project~\footnote{\url{https://github.com/liuhuanyong/CrimeKgAssitant}}. We keep the most frequent 20 classes and sample 25 questions for each class to form our final test set.
    \item \textbf{Reading Comprehension} (2-5): \emph{Given a judgement document and a corresponding question, extract relevant content from it to answer the question}. Judicial documents contain rich case information, such as time, location, and character relationships. Intelligently reading and comprehending judicial documents through large language models can assist judges, lawyers, and the general public in obtaining the necessary information quickly and conveniently. We use the CAIL2019 reading comprehension dataset to build this task, removing question types related to binary and unanswerable questions. We retain single and multiple-segment data as our test set.

    \item \textbf{Named-Entity Recognition} (2-6): \emph{Given a sentence from a judgement document, extract entity information corresponding to a set of pre-defined entity types such as suspect, victim or evidence}. We sampled 500 examples from the CAIL2022 Information Extraction dataset as our test set. These 500 samples contain 10 entity types related to theft crimes.

    \item \textbf{Opinion Summarization} (2-7): \emph{Given a legal-related public news report, generate a concise summary}. Legal summaries typically include key facts of the case, points of contention, legal issues, legal principles applied, and the judgment's outcome. It can provide a quick overview of the case content to improve the efficiency of legal professionals. 
We randomly select 500 samples from the CAIL2021 Legal Public Opinion Summary dataset for this task. We only select samples with less than 400 words to fit the length constraint of LLMs.

    \item \textbf{Argument Mining} (2-8): \emph{Given a plaintiff's perspective and five candidate defendant's viewpoints, select one viewpoint that can form a point of dispute with the plaintiff's perspective}. In court's trial process, judgment documents play a crucial role in recording the arguments and evidence presented by both the plaintiff and the defendant. Due to differences in their positions and perspectives, as well as inconsistencies in their factual statements, disputes arise between the plaintiff and the defendant during the trial process. These points of contention are the key to the entire trial and the essence of judgment documents. This task aims to extract valuable arguments and supporting materials from a large volume of legal texts, providing strong support for legal debates and case analysis. We use CAIL2022's Argument Mining dataset to construct our dataset, transforming the identification of focal points of disputes into a multiple-choice question format.
    \item \textbf{Event Detection} (2-9):
\emph{Given a sentence from a legal judgement document, detect which events are mentioned in this sentence}. Events are the essence of facts in legal cases. Therefore, Legal Event Detection is fundamentally important and naturally beneficial to case understanding and other Legal AI tasks. We construct the test set from the LEVEN dataset\cite{yao2022leven} by sampling sentences corresponding to the top 20 most frequent event types. Multiple events can be mentioned in every sentence.
    \item \textbf{Trigger Word Extraction} (2-10):
\emph{Given a sentence from a legal judgment document and its corresponding events, predict which words in the sentence triggered these events}. Trigger words directly cause events and are an important feature that determines the event category, providing post-hoc explanation for the event types we identify. Directly identifying trigger words is very difficult, so we simplified this task by providing the events contained in the text along with the text information,  examining the ability of LLMs to recognize trigger words related to events. When constructing the trigger word test set, we removed trigger words that were the same as the event type, as well as events with multiple or duplicate trigger words from the LEVEN dataset\cite{yao2022leven}, to include as different trigger words as possible.
\end{itemize}
Examples of the 10 understanding tasks are in Appendix~\ref{app:instruction_understanding}.

\textbf{Legal Knowledge Applying Tasks}

Legal knowledge applying tasks primarily examine the ability of LLMs to not only understand legal knowledge but also simulate law professionals to apply the knowledge in solving realistic legal tasks. In the task design, we extensively examine the model's different reasoning abilities, including 3 legal content reasoning tasks: legal judgement prediction, case analysis, consultation, and 1 numerical reasoning task: criminal damages calculation. 

When predicting case judgments, judges follow a certain order when hearing a case~\cite{zhong2018legal,huang2021dependency}. Therefore, in constructing the case judgment prediction task, we simulated this process by decomposing the CAIL2018 dataset into three tasks: fact-based article prediction (3-1), charge prediction (3-3) and prison term prediction. We further separate the task of prison term prediction into two scenarios: without article content (3-4) and with article content (3-5) to examine LLMs' capability in utilizing the article content to make accurate judgement predictions. Besides, we also add the task scene-based fact prediction  to simulate judges’
recognition of legal provisions (3-2).

\begin{itemize}
    \item \textbf{Fact-based Article Prediction} (3-1): \emph{Given a fact statement from the legal judgement document, predict which article items should be applied}.
When judges make decisions, they usually associate relevant articles with the facts of the case~\cite{ge2021learning,louis2023finding}. Article prediction can assist judges in quickly locating legal articles related to legal texts. Legal articles are written expressions of legal norms, which are rules and regulations with clear meanings and legal effects. 
The model needs to deduce potentially applicable legal provisions based on the given case description and related background information. We sample 500 cases from the CAIL2018 dataset for this task.

    \item \textbf{Scene-based Article Prediction} (3-2): \emph{Given a described scenario and a related question, predict the corresponding article item}.
The CAIL2018 dataset only covers criminal law-related legal provisions. In order to comprehensively evaluate the ability of LLMs to analyze case facts and infer relevant legal provisions, we collected high-quality legal scenario-based question-and-answer data from public sources on GitHub\cite{LAWGPT-zh}. This dataset was generated by inputting legal provisions into chatGPT to construct corresponding scenario-based questions and answers. We manually selected 5,000 question-and-answer pairs with accurate answers from the generated dataset. Based on this, we selected 252 core legal provisions' scenario-based question-and-answer content as the test dataset.

    \item \textbf{Charge Prediction} (3-3): \emph{Given fact statement from the legal judgement document and the applied article number, predict the cause of action (charge)}.
Cause of action is a summary of the nature of the legal relationship involved in a litigation case, as formulated by the people's court. Accurately predicting the cause of action can help improve judicial efficiency and fairness. In the process of filing and hearing cases, accurate prediction of the cause of action can help the court to allocate cases, allocate resources, and arrange trials, thereby improving judicial efficiency and fairness. 
We sampled 500 pieces of data from the CAIL2018 cause of action prediction dataset for this task.
    \item \textbf{Prison Term Prediction w.o. Article} (3-4): \emph{Given fact statement from the legal judgement document, the applied article number and charge, predict the prison term.}
Prison term prediction refers to the process of predicting and estimating the possible sentence that a defendant may face during the criminal justice process based on the facts of the case, legal provisions, and relevant guiding precedents. It aims to make reasonable inferences about the length and form of the sentence by comprehensively considering various factors such as the nature of the crime, the circumstances of the offense, the social impact, and the defendant's personal situation. 
We used the prison term prediction dataset from CAIL2018, removed some cases with the death penalty and life imprisonment, and randomly sampled 500 cases as the test dataset. During the process of judges' sentencing, more information is usually taken into account to determine the prison term outcome. We simulated the judge's analysis process by providing the relevant legal provisions and the charge of the case.
    \item \textbf{Prison Term Prediction w. Article} (3-5):
\emph{Given fact statement from the legal judgement document, the applied article content and charge, predict the prison term.}
Large language models typically use retrieval-argument methods to introduce new information. Some publicly available models also include retrieval modules that provide detailed reference information for the model by retrieving legal provisions. We simulated this process, and unlike the previous task where only the legal provision number was provided, we provided the specific content of the legal provision in this task. When constructing the sentence prediction dataset, we appended the content of the legal provisions to the end of the question, allowing the model to complete the sentence prediction task in this scenario.
    \item \textbf{Case Analysis} (3-6): \emph{Given a case and a corresponding question, select the correct answer from 4 candidates}. We use the case analysis part from JEC\_QA dataset~\cite{zhong2020jec} for this task. The case analysis part tests the ability of models to analyze real cases. Models must possess five types of reasoning in order to perform this analysis including word matching, concept understanding, numerical analysis, multi-paragraph reading, and multi-hop reasoning. In order to reduce the difficulty of the test and facilitate the acquisition of answers, we sampled 500 multiple-choice questions from the JEC\_QA Case-Analysis part as the testing dataset.
    \item \textbf{Criminal Damages Calculation} (3-7): \emph{Given a fact description about a criminal process, predict the amount of money involved in this case}.
There are some numerical computing tasks in the process of judicial trials, such as the calculation of the total amount of legal crimes. The total amount of the crime is an important sentencing factor. In some charges such as theft, financial fraud, and bribery, China's laws determine the severity of the sentence based on the amount involved in the case. This task mainly tests the computing ability of LLMs. First, we examine whether the model understands the rules of case amount calculation, and second, we examine whether the model can accurately complete numerical calculations. We selected the LAIC2021 numerical computing task to construct our dataset.
    \item \textbf{Consultation} (3-8):
\emph{Given a user consultation, generate a suitable answer.}
Legal consultation is a way for the public to access legal services. Legal consultation can help people understand legal disputes and seek targeted advice and solutions from professional lawyers, as well as receive support and guidance. Some law firms and legal consulting companies also provide online legal consultation services, making it more convenient for people to obtain legal help. 
We collected legal consultation contents from the Hualv website~\footnote{\url{www.66law.com}}, and our dataset contains both the answers to legal consultations and the corresponding legal basis, i.e., legal articles.
\end{itemize}

Examples of the 8 applying tasks are in Appendix~\ref{app:instruction_application}.

\subsection{Evaluation}
For every task,  the evaluation is done following two steps: (1) answer extraction, which extracts the answer from the model prediction, and (2) metric computation, which computes the metric score based on the question, extracted answer and the gold answer. Answer extraction is a necessary step since many LLMs often do not generate output directly comparable with gold labels~\cite{adlakha2023evaluating}. We explain these two steps in detail in the following section.
\paragraph{Answer Extraction}
Most of the tasks require the prediction to be in the standard format in order to compare with the ground truth, we define a set of task-specific rules to extract the answer from the model prediction.
\begin{itemize}
    \item Article Number Extraction (3-1): this type of tasks requires us to extract the article numbers predicted by the model. To do this, we use the delimiter \begin{CJK*}{UTF8}{gbsn} ``、'' \end{CJK*} to separate the prediction text into chunks of text, and then the cn2an\footnote{\url{https://github.com/Ailln/cn2an}} library is used to convert the Chinese numerals to Arabic numerals within each of those chunks. Using a regular expression, we extract the converted Arabic numerals as the expected article numbers; if more than one number appears in the same chunk, only the first number is extracted. All extracted numbers are combined to form the final set of predictions.
    \item Prison Term Extraction (3-4, 3-5): for this type of tasks, we need to extract the predicted prison terms from the prediction text. To begin, we use cn2an to convert all the Chinese numerals in the prediction to Arabic numerals; we then extract digits that are followed by time intervals in Chinese, such as \begin{CJK*}{UTF8}{gbsn} ``个月'' (month) and ``年'' (year)\end{CJK*}. The extracted prison terms are normalized to months, meaning that the numbers appearing before \begin{CJK*}{UTF8}{gbsn} ``年'' \end{CJK*} will be multiplied by 12. Note that the time unit in the ground truth answer is also month.
    \item Criminal Damages Extraction (3-7): We extract all numbers appearing in the prediction text using regular expression. The set of of the extracted numbers is considered as the predicted criminal damages.
    \item Named-Entity Recognition (2-6): We find all occurrences of entity types from the model prediction, then obtain the substring from its occurrence to the delimiter ``\textbackslash n'', then apply a regular expression to extract the entity value.
    \item Trigger Word Extraction (2-10): We split the model prediction by the delimiter \begin{CJK*}{UTF8}{gbsn} ``；'' \end{CJK*}, then treat the split array as a list of extracted key words.
    \item Option Extraction (1-2, 2-2, 2-3, 2-4, 2-8, 2-9, 3-3): this type of task is similar to selecting the correct options from a list of options in a multiple-choice task. We run through all possible options and check if they appear in the prediction text. The set of options that occur in the prediction text is collected and used for evaluation.
    \item Others (1-1, 2-1, 2-5, 2-7, 3-2, 3-8): we take the model prediction as the answer without performing any extraction step. 
    
\end{itemize}

\paragraph{Metrics}
After the answer extraction phase, we compute the final metric based on the extracted answer. We defined 7 different metrics in total to measure different types of tasks:
\begin{itemize}
    \item \textbf{Accuracy}: Accuracy is a binary score that performs exact match between the model prediction and the gold answer. This applies to all single-label classification tasks including task 1-2, 2-4, 2-8, 3-6, and the regression task 3-7. For SLC tasks, if multiple valid answers are extracted from the model prediction, then we always treat it as wrong~\footnote{For the criminal damages calculation task, we treat the model prediction correct as long as one of the extracted answers match the ground truth as we find LLMs often output the whole calculation process.}.
    \item \textbf{F1}: When there are multiple output labels, F1 score measures the harmonic mean of the precision and recall. This applies to all multi-label classification tasks including task 2-2, 2-3, 2-9, 3-1 and 3-3.
    \item \textbf{rc-F1}: rc-F1 is the F1 score tailored for the reading comprehension task 2-5. It treats every token as a label, removes punctuation, stories, extra whitespace, performs other necessary normalizations then compute the F1 score. We adopt the official script from CAIL2019 to compute the instance-level rc-F1 score~\footnote{\url{https://github.com/china-ai-law-challenge/CAIL2019/tree/master}}.
    \item \textbf{soft-F1}: For extraction tasks 2-6 and 2-10, the output is a set of phrases. Instead of using the standard F1 score, we use a soft version by replacing the phrase-level exact match with the rc-F1 score, then computing the F1 on top of it. We find using the soft version helpful since LLMs often use wording choices different from the ground truth.
    \item \textbf{nLog-distance}: For the prison term prediction tasks 3-4 and 3-5, we evaluate them with the normalized log distance (nLog-distance) to capture the continuity of prison terms. We compute the logarithm of the difference between the extracted and gold answer, then normalize it to the space between 0 and 1 for better compatibility with other metrics.
    \item \textbf{F0.5}: For the document proofreading task 2-1, we use the F0.5 metric to evaluate it. The F0. 5 score gives more weight to precision than to recall we want to prevent introducing more false positives than identify every other error in proofreading~\cite{zhang2022mucgec}. We use the ChERRANT toolkit to align the extracted and gold answer before computing the F0.5 score~\footnote{\url{https://github.com/HillZhang1999/MuCGEC/tree/main/scorers/ChERRANT}}. As the alignment can take too long to respond for very bad generations, we add a time-out of 10 seconds. If a time-out happened, then the prediction is assigned a score of 0.
    \item \textbf{Rouge-L}: For other generation tasks 1-1, 2-7, 3-3 and 3-8, we use the Rouge-L score to evaluate them. Rouge-L is a commonly used metric in generation tasks. It takes into account sentence-level structure similarity naturally and identifies longest co-occurring in sequence n-grams automatically to compare the extracted and gold answers~\cite{lin2004rouge}.
\end{itemize}
Several large language models may decline to respond to legal-related inquiries due to security policies or simply fail to follow the instructions. To capture this issue, we also report the \textbf{abstention rate} of LLMs in each task (how often an LLM abstains to answer). An abstention happens if an answer cannot be extracted from the model prediction. The abstention rate does not apply to task 2-5 and all generation tasks since they do not need the answer extraction step.

\section{Experiment}
\begin{table*}[t]
    \centering
    \resizebox{13.5cm}{!}{
    \begin{tabular}{l|cccccc}
        \toprule
        \textbf{Model} & \textbf{Parameters} & \textbf{SFT} & \textbf{RLHF} &\textbf{Access} & \textbf{BaseModel} &\\ 
        \midrule
        \textbf{Multilingual LLMs} \\
        MPT & 7B & \xmark & \xmark & Weights & - \\
        MPT-Instruct & 7B & \cmark & \xmark & Weights & MPT-7B \\
        LLaMA & 7/13/30/65B & \xmark & \xmark & Weights & - \\
        LLaMA-2 & 7/13/70B & \cmark & \xmark & Weights & - \\
        LLaMA-2-Chat & 7/13/70B & \cmark & \cmark & Weights & LLaMA-2-7/13/70B \\
        Alpaca-v1.0 & 7B & \cmark & \xmark & Weights & LLaMA-7B \\
        Vicuna-v1.3 & 7/13/33B & \cmark & \xmark & Weights & LLaMA-7/13/33B \\
        WizardLM & 7B & \cmark & \xmark & Weights & LLaMA-7B \\
        StableBeluga2 & 70B & \cmark & \xmark & Weights & LLaMA-2-70B \\
        ChatGPT & N/A & \cmark & \cmark & API & - \\
        GPT-4 & N/A & \cmark & \cmark & API & - \\
        \midrule
        \textbf{Chinese-oriented LLMs} \\
        MOSS-Moon & 16B & \xmark & \xmark & Weights & - \\
        MOSS-Moon-SFT & 16B & \cmark & \xmark & Weights & MOSS-Moon \\
        TigerBot-Base & 7B & \xmark & \xmark & Weights & - \\
        TigerBot-SFT & 7B & \cmark & \xmark & Weights & TigerBot-Base \\
        GoGPT & 7B & \cmark & \xmark & Weights & LLaMA-7B \\
        ChatGLM2 & 6B & \cmark & \xmark & Weights & ChatGLM \\
        Ziya-LLaMA & 13B & \cmark & \cmark & Weights & LLaMA-13B \\
        Baichuan & 7/13B & \xmark & \xmark & Weights & - \\
        Baichuan-13B-Chat & 13B & \cmark & \xmark & Weights & Baichuan-13B \\
        XVERSE & 13B & \xmark & \xmark & Weights & - \\
        InternLM & 7B & \xmark & \xmark & Weights & - \\
        InternLM-Chat & 7B & \cmark & \xmark & Weights & InternLM-7B \\
        InternLM-Chat-8K & 7B & \cmark & \xmark & Weights & InternLM-7B \\
        Qwen & 7B & \xmark & \xmark & Weights & - \\
        Qwen-Chat & 7B & \cmark & \xmark & Weights & Qwen-7B \\
        Yulan-Chat-2 & 13B & \cmark & \xmark & Weights & LLaMA-2-13B \\
        BELLE-LLaMA-2 & 13B & \cmark & \xmark & Weights & LLaMA-2-13B \\
        Chinese-LLaMA-2 & 7B & \cmark & \xmark & Weights & LLaMA-2-7B \\
        Chinese-Alpaca-2 & 7B & \cmark & \xmark & Weights & LLaMA-2-7B \\
        LLaMA-2-Chinese & 7/13B & \cmark & \xmark & Weights & LLaMA-2-7/13B \\
        \midrule
        \textbf{Legal Specific LLMs} \\
        HanFei & 7B & \cmark & \xmark & Weights & HanFei \\
        LaWGPT-7B-beta1.0 & 7B & \cmark & \xmark & Weights & Chinese-LLaMA \\
        LaWGPT-7B-beta1.1 & 7B & \cmark & \xmark & Weights & Chinese-alpaca-plus-7B \\
        LexiLaw & 6B & \cmark & \xmark & Weights & ChatGLM-6B \\
        Wisdom-Interrogatory & 7B & \cmark & \xmark & Weights & Baichuan-7B \\
        Fuzi-Mingcha & 6B & \cmark & \xmark & Weights & ChatGLM-6B \\
        Lawyer-LLaMA & 13B & \cmark & \xmark & Weights & LLaMA \\
        ChatLaw & 13/33B & \cmark & \xmark & Weights & Ziya-LLaMA-13B/Anima-33B\\
        \bottomrule
       
    \end{tabular}}
    \caption{\small LLMs tested on LawBench. We classify these models by their main training corpora.}
    \label{tab:models}
\end{table*}

\subsection{Models}
We evaluate a wide spectrum of large language models of various sizes, grouping them into three major categories based on their pre-training and fine-tuning domains: multilingual LLMs, Chinese-oriented LLMs and legal specific LLMs. We provide a short review over them in the following section. The detailed model list is shown in Table~\ref{tab:models}.

\paragraph{Multilingual LLMs} We consider 18 open-source multilingual models: MPT-7B, MPT-Instruct-7B, LLaMA-7B / 13B / 30B / 65B, LLaMA-2-7B / 13B / 70B, LLaMA-2-Chat-7B / 13B / 70B, Alpaca-v1.0-7B, Vicuna-v1.3-7B / 13B / 33B, WizardLM-7B, StableBeluga2. In addition, two commercial models, ChatGPT and GPT-4, are included.

\paragraph{Chinese Oriented LLMs} A number of Chinese-oriented LLMs are proposed to enhance Chinese comprehension. Their typically perform better than multilingual models on Chinese NLP tasks. We include 21 open-sourced, Chinese-oriented LLMs in our evaluation: CMOSS-Moon / Moon-SFT, TigerBot-Base / SFT, GoGPT, ChatGLM2-6B, Ziya-LLaMA-13B, Baichuan-7B / 13B / 13B-Chat, XVERSE-13B, InternLM-7B / Chat-7B / Chat-7B-8K, Qwen-7B / 7B-Chat, BELLE-LLaMA-2, Chinese-LLaMA-2-7B, Chinese-Alpaca-2-7B, LLaMA-2-Chinese-7B / 13B.

\paragraph{Legal Specific LLMs} Certain Chinese-oriented LLMs are further fine-tuned on Chinese corpus in legal domain to improve LLMs' understanding of Chinese laws. They are of particular interest to us; through our benchmark, we can rigorously gauge their true advance compared to general-purpose LLMs and identify their limitations. Here, we provide detailed descriptions of these models: 
\begin{itemize}
    \item ChatLaw\cite{cui2023chatlaw}: ChatLaw-13B is fine-tuned based on Ziya-LLaMA-13B-v1, ChatLaw-33B is fine-tuned based on Anima-33B.
    \item LaywerLLaMA\cite{huang2023lawyer}: based on Chinese-LLaMA-13B, fine-tuned with general and legal instructions.
    \item FuziMingcha\cite{fuzi.mingcha}: based on ChatGLM, fine-tuned with unsupervised Chinese judicial texts along with supervised legal fine-tuning datasets.
    \item HanFei\cite{HanFei2023}: a fully pre-trained and fine-tuned legal model with 7 billion parameters.
    \item LaWGPT\cite{LaWGPT2023}: LaWGPT-7B-beta1.0 is further pre-trained on 500k Chinese judgment documents upon Chinese-LLaMA-7B and fine-tuned based on the Legal-Base-7B with instructions. LaWGPT-7B-beta1.1 is fine-tuned based on the Chinese-alpaca-plus-7B with 350k legal Q\&A datasets.
    \item LexiLaw\cite{LexiLaw2023}: a fine-tuned Chinese legal model based on the ChatGLM-6B with legal datasets. 
    \item WisdomInterrogatory\cite{WisdomInterrogatory2023}: a further pre-trained and fine-tuned model built upon Baichuan-7B.
\end{itemize}

\subsection{Experiment Setting}
We employ OpenCompass~\cite{2023opencompass} to perform model inference. For both ChatGPT and GPT-4, we set the temperature at 0.7 and top $p$ to 1. For other chat models, we tailor the prompt using prefixes and suffixes specific to each model. Greedy decoding is performed during generation for all open-sourced models. We set the input token length limit to 2048 and an output token length to 1024. Right truncation is performed for input prompts exceeding the length limitation. We evaluate all models in both zero-shot and one-shot settings. The model input in zero-shot inference is merely the task instruction and the query (see Appendix \ref{appendix:details_of_task_intruction} for individual instructions and queries). To build the model input for one-shot inference, a single example including the query and corresponding answer is attached after the instruction, followed by the actual query to the model.

\subsection{Main Results}
\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{Figure/picture_2_5.pdf}
    \caption{\small Average performance (zero-shot) of 51 LLMs evaluated on LawBench.}
    \label{fig:overall_performance_one_shot_main_content}
\end{figure}

\begin{table*}
\centering
\small
 \resizebox{\columnwidth}{!}{
\begin{tabular}{l|cccc|cccc|cccccccc}
\toprule
\multicolumn{1}{l|}{\multirow{3}*{Task}} & \multicolumn{4}{c}{Multilingual LLMs} & \multicolumn{4}{c}{Chinese Oriented LLMs} & \multicolumn{4}{c}{Legal Specific LLMs} \\
\multicolumn{1}{l|}{} & \multicolumn{2}{c}{GPT-4} & \multicolumn{2}{c}{ChatGPT} & \multicolumn{2}{c}{Qwen-Chat}  & \multicolumn{2}{c}{InternLM-Chat-8K} & \multicolumn{2}{c}{Fuzi-Mingcha} & \multicolumn{2}{c}{ChatLaw-13B} \\
\multicolumn{1}{l|}{} & \multicolumn{1}{c}{0-shot} & \multicolumn{1}{c}{1-shot} & 0-shot & 1-shot & \multicolumn{1}{c}{0-shot} & \multicolumn{1}{c}{1-shot} & \multicolumn{1}{c}{0-shot} & \multicolumn{1}{c|}{1-shot} & \multicolumn{1}{c}{0-shot} & \multicolumn{1}{c}{1-shot} & \multicolumn{1}{c}{0-shot} & \multicolumn{1}{c}{1-shot} \\ 
\midrule
1-1 & \cellcolor{Green!0.0}15.38 & \cellcolor{Green!0.0}17.21 & \cellcolor{Green!0.0}15.86 & \cellcolor{Green!0.0}16.15 & \cellcolor{Green!0.0}18.54 & \cellcolor{Green!0.0}17.73 & \cellcolor{Green!0.0}15.45 & \cellcolor{Green!0.0}15.16 & \cellcolor{Green!0.0}25.22 & \cellcolor{Green!0.0}20.21 & \cellcolor{Green!0.0}14.85 & \cellcolor{Green!0.0}15.98 \\
1-2 & \cellcolor{Green!0.2}55.2 & \cellcolor{Green!0.2}54.8 & \cellcolor{Green!0.0}36.0 & \cellcolor{Green!0.0}37.2 & \cellcolor{Green!0.2}34.0 & \cellcolor{Green!0.0}28.6 & \cellcolor{Green!0.0}40.4 & \cellcolor{Green!0.2}40.6 & \cellcolor{Green!11.4}7.8 & \cellcolor{Green!5.8}12.8 & \cellcolor{Green!0.4}28.4 & \cellcolor{Green!0.0}29.4 \\
\hline
2-1 & \cellcolor{Green!0.0}12.53 & \cellcolor{Green!0.0}18.31 & \cellcolor{Green!0.0}9.1 & \cellcolor{Green!0.0}13.5 & \cellcolor{Green!0.0}22.56 & \cellcolor{Green!0.0}25.16 & \cellcolor{Green!0.0}22.64 & \cellcolor{Green!0.0}21.64 & \cellcolor{Green!0.0}4.93 & \cellcolor{Green!0.0}2.86 & \cellcolor{Green!0.0}12.22 & \cellcolor{Green!0.0}13.01 \\
2-2 & \cellcolor{Green!0.0}41.65 & \cellcolor{Green!0.0}46.0 & \cellcolor{Green!3.0}32.37 & \cellcolor{Green!2.2}40.6 & \cellcolor{Green!28.4}27.42 & \cellcolor{Green!38.4}27.4 & \cellcolor{Green!6.8}35.46 & \cellcolor{Green!10.0}36.6 & \cellcolor{Green!23.4}19.59 & \cellcolor{Green!21.6}2.4 & \cellcolor{Green!11.6}2.68 & \cellcolor{Green!17.0}9.0 \\
2-3 & \cellcolor{Green!0.0}69.79 & \cellcolor{Green!0.0}69.99 & \cellcolor{Green!1.0}51.73 & \cellcolor{Green!0.4}54.01 & \cellcolor{Green!19.4}31.42 & \cellcolor{Green!21.8}32.96 & \cellcolor{Green!14.2}28.96 & \cellcolor{Green!16.6}30.91 & \cellcolor{Green!27.2}28.46 & \cellcolor{Green!42.4}17.44 & \cellcolor{Green!27.0}42.24 & \cellcolor{Green!47.6}30.91 \\
2-4 & \cellcolor{Green!1.6}44.0 & \cellcolor{Green!3.0}44.4 & \cellcolor{Green!3.4}41.2 & \cellcolor{Green!2.8}41.4 & \cellcolor{Green!13.0}35.0 & \cellcolor{Green!4.0}31.2 & \cellcolor{Green!7.8}35.6 & \cellcolor{Green!6.0}33.2 & \cellcolor{Green!29.4}18.6 & \cellcolor{Green!38.6}8.8 & \cellcolor{Green!15.8}27.6 & \cellcolor{Green!12.6}26.6 \\
2-5 & \cellcolor{Green!0.0}56.5 & \cellcolor{Green!0.0}64.8 & \cellcolor{Green!0.0}53.75 & \cellcolor{Green!0.0}61.98 & \cellcolor{Green!0.0}48.48 & \cellcolor{Green!0.0}46.71 & \cellcolor{Green!0.0}54.13 & \cellcolor{Green!0.0}54.35 & \cellcolor{Green!0.0}97.59 & \cellcolor{Green!0.0}93.35 & \cellcolor{Green!0.0}39.11 & \cellcolor{Green!0.0}41.41 \\
2-6 & \cellcolor{Green!0.0}76.6 & \cellcolor{Green!0.0}79.96 & \cellcolor{Green!0.0}69.55 & \cellcolor{Green!0.0}74.04 & \cellcolor{Green!0.0}37.88 & \cellcolor{Green!0.0}57.34 & \cellcolor{Green!0.0}17.95 & \cellcolor{Green!0.0}26.86 & \cellcolor{Green!0.0}44.07 & \cellcolor{Green!0.0}42.28 & \cellcolor{Green!0.0}54.89 & \cellcolor{Green!0.0}60.68 \\
2-7 & \cellcolor{Green!0.0}37.92 & \cellcolor{Green!0.0}40.52 & \cellcolor{Green!0.0}33.49 & \cellcolor{Green!0.0}40.68 & \cellcolor{Green!0.0}36.04 & \cellcolor{Green!0.0}42.58 & \cellcolor{Green!0.0}27.11 & \cellcolor{Green!0.0}30.56 & \cellcolor{Green!0.0}54.32 & \cellcolor{Green!0.0}31.43 & \cellcolor{Green!0.0}38.45 & \cellcolor{Green!0.0}42.71 \\
2-8 & \cellcolor{Green!0.0}61.2 & \cellcolor{Green!0.0}59.0 & \cellcolor{Green!0.0}36.4 & \cellcolor{Green!0.0}37.4 & \cellcolor{Green!0.0}24.0 & \cellcolor{Green!0.0}26.8 & \cellcolor{Green!0.0}36.2 & \cellcolor{Green!0.0}30.6 & \cellcolor{Green!20.6}8.8 & \cellcolor{Green!1.4}11.4 & \cellcolor{Green!1.0}18.6 & \cellcolor{Green!1.8}20.2 \\
2-9 & \cellcolor{Green!2.2}78.82 & \cellcolor{Green!2.6}76.55 & \cellcolor{Green!4.8}66.48 & \cellcolor{Green!5.4}67.59 & \cellcolor{Green!20.2}44.88 & \cellcolor{Green!6.4}50.63 & \cellcolor{Green!10.6}62.93 & \cellcolor{Green!4.4}63.42 & \cellcolor{Green!2.4}16.9 & \cellcolor{Green!23.2}21.26 & \cellcolor{Green!1.0}31.74 & \cellcolor{Green!3.2}40.27 \\
2-10 & \cellcolor{Green!0.0}65.09 & \cellcolor{Green!0.0}65.26 & \cellcolor{Green!0.0}39.05 & \cellcolor{Green!0.0}40.04 & \cellcolor{Green!0.0}18.9 & \cellcolor{Green!0.0}21.27 & \cellcolor{Green!0.0}20.94 & \cellcolor{Green!0.0}20.69 & \cellcolor{Green!0.0}7.78 & \cellcolor{Green!0.0}7.04 & \cellcolor{Green!0.0}14.56 & \cellcolor{Green!0.0}17.37 \\
\hline
3-1 & \cellcolor{Green!0.4}52.47 & \cellcolor{Green!0.4}53.2 & \cellcolor{Green!1.2}29.5 & \cellcolor{Green!1.2}30.81 & \cellcolor{Green!8.4}44.62 & \cellcolor{Green!3.4}52.86 & \cellcolor{Green!7.0}34.86 & \cellcolor{Green!7.4}38.88 & \cellcolor{Green!0.0}25.19 & \cellcolor{Green!0.2}3.86 & \cellcolor{Green!5.8}33.28 & \cellcolor{Green!0.4}25.99 \\
3-2 & \cellcolor{Green!0.0}27.54 & \cellcolor{Green!0.0}33.15 & \cellcolor{Green!0.0}31.3 & \cellcolor{Green!0.0}34.49 & \cellcolor{Green!0.0}33.5 & \cellcolor{Green!0.0}34.49 & \cellcolor{Green!0.0}19.11 & \cellcolor{Green!0.0}28.7 & \cellcolor{Green!0.0}22.18 & \cellcolor{Green!0.0}32.96 & \cellcolor{Green!0.0}31.55 & \cellcolor{Green!0.0}33.96 \\
3-3 & \cellcolor{Green!25.0}41.99 & \cellcolor{Green!23.8}41.3 & \cellcolor{Green!29.0}35.52 & \cellcolor{Green!32.4}34.55 & \cellcolor{Green!24.6}40.67 & \cellcolor{Green!26.2}39.91 & \cellcolor{Green!23.0}41.05 & \cellcolor{Green!22.6}42.25 & \cellcolor{Green!5.4}55.93 & \cellcolor{Green!11.6}43.6 & \cellcolor{Green!35.4}27.9 & \cellcolor{Green!26.0}12.24 \\
3-4 & \cellcolor{Green!0.4}82.62 & \cellcolor{Green!0.4}83.21 & \cellcolor{Green!3.8}78.75 & \cellcolor{Green!5.6}77.12 & \cellcolor{Green!7.6}76.74 & \cellcolor{Green!6.0}78.47 & \cellcolor{Green!21.8}63.21 & \cellcolor{Green!18.8}67.74 & \cellcolor{Green!2.4}77.23 & \cellcolor{Green!1.4}78.95 & \cellcolor{Green!3.8}76.18 & \cellcolor{Green!5.6}74.31 \\
3-5 & \cellcolor{Green!0.4}81.91 & \cellcolor{Green!0.4}82.74 & \cellcolor{Green!4.0}76.84 & \cellcolor{Green!5.4}73.72 & \cellcolor{Green!7.4}77.19 & \cellcolor{Green!11.6}73.92 & \cellcolor{Green!16.6}67.2 & \cellcolor{Green!13.0}71.1 & \cellcolor{Green!2.0}75.52 & \cellcolor{Green!1.2}79.0 & \cellcolor{Green!5.0}73.57 & \cellcolor{Green!5.4}73.01 \\
3-6 & \cellcolor{Green!0.0}48.6 & \cellcolor{Green!0.0}49.6 & \cellcolor{Green!0.6}27.4 & \cellcolor{Green!0.0}31.6 & \cellcolor{Green!0.4}26.8 & \cellcolor{Green!0.0}26.8 & \cellcolor{Green!0.0}34.2 & \cellcolor{Green!0.0}36.2 & \cellcolor{Green!29.2}7.0 & \cellcolor{Green!20.0}13.8 & \cellcolor{Green!0.6}28.8 & \cellcolor{Green!0.0}26.8 \\
3-7 & \cellcolor{Green!0.4}77.6 & \cellcolor{Green!0.2}77.0 & \cellcolor{Green!0.4}61.2 & \cellcolor{Green!0.0}66.4 & \cellcolor{Green!0.4}42.0 & \cellcolor{Green!0.0}44.6 & \cellcolor{Green!0.6}43.8 & \cellcolor{Green!0.8}44.0 & \cellcolor{Green!0.8}47.2 & \cellcolor{Green!0.0}38.2 & \cellcolor{Green!6.0}41.4 & \cellcolor{Green!3.6}42.0 \\
3-8 & \cellcolor{Green!0.0}19.65 & \cellcolor{Green!0.0}19.9 & \cellcolor{Green!0.0}17.45 & \cellcolor{Green!0.0}17.17 & \cellcolor{Green!0.0}19.32 & \cellcolor{Green!0.0}20.39 & \cellcolor{Green!0.0}13.37 & \cellcolor{Green!0.0}12.11 & \cellcolor{Green!0.0}16.64 & \cellcolor{Green!0.0}13.95 & \cellcolor{Green!0.0}17.17 & \cellcolor{Green!0.0}16.72 \\
\hline
AVG &  \cellcolor{Green!1.5}52.35 & \cellcolor{Green!1.6}53.85 & \cellcolor{Green!2.6}42.15 & \cellcolor{Green!2.8}44.52 & \cellcolor{Green!6.5}37.00 & \cellcolor{Green!5.9}38.99 & \cellcolor{Green!5.4}35.73 & \cellcolor{Green!5.0}37.28 & \cellcolor{Green!7.7}33.05 & \cellcolor{Green!8.4}28.78 & \cellcolor{Green!5.7}32.76 & \cellcolor{Green!6.2}32.63 \\
\bottomrule
\end{tabular}
}
\begin{tabular}{ccccccc}
	\multicolumn{2}{c}{\%abstention\qquad} & \cellcolor{Green!0.0}\quad0\% \quad & \cellcolor{Green!25}\quad25\% \quad & \cellcolor{Green!50}\quad50\% \quad &
	\cellcolor{Green!75}\quad75\% \quad & \cellcolor{Green!100}\quad100\% \quad \\
\end{tabular}
\caption{\small Model performance of top two performing systems from each category. qwen-chat and InternLM-chat both have 7B parameters. Cells are colored according to model abstention rate. Further results are in Appendix \ref{appendix:details_of_results}. It can be observed that the performance of one-shot surpasses that of zero-shot.}
\label{tab:top_2_zero_shot_performance}
\end{table*}

Figure \ref{fig:overall_performance_one_shot_main_content} shows the overall zero-shot performance of each model. As can be seen, GPT-4 and ChatGPT clearly lead the benchmark, substantially outperform all other models. Under the same model size (7B-13B), Chinese oriented LLMs outperform multilingual models such as MPT and Llama by a significant margin, confirming the effectiveness of pre-training and fine-tuning on Chinese data. Interestingly, legal specific LLMs do not necessarily outperform general-purpose Chinese oriented LLMs. Close inspection reveals that existing legal specific LLMs are based on rather weak foundation models, implying that improved models may be obtained by fine-tuning a stronger foundation model (see Section \ref{sec:analysis} for more detail).

In Table \ref{tab:top_2_zero_shot_performance}, we demonstrate the top 2 performing LLMs from each category on all tasks, together with the model abstention rate.\footnote{We present the complete results with the performance achieved by every model on all tasks in Table \ref{tab:Zero-shot Results on Legal Knowledge Memorization Tasks}-\ref{tab:zero-shotResults on Overall} (complete zero-shot results), Table \ref{tab:one-shotResults on legal knowledge memorisation tasks}-\ref{tab:overall_results} (complete one-shot results), in Appendix \ref{appendix:details_of_results}.} We made the following observations. First, there is substantial variation in the distribution of scores across tasks. The best-performing models, for example, can achieve a score of more than 60 on tasks 3-4 and 3-5, yet no models manage to exceed 30 on tasks 1-1 and 2-1. This shows that our benchmark adequately assess model capabilities in various aspects. Second, we notice that GPT-4/ChaGPT not only performs well on the majority of tasks, but also has a low abstention rate, suggesting that they are excellent at following instructions and providing responses that are more relevant to the query. Third, it can be observed that GPT models and the Chinese oriented LLMs can successfully leverage the one-shot example and make more accurate predictions compared to zero-shot cases. The top-performing legal specific LLMs, however, suffers a drop in performance after seeing the one-shot example. We hypothesize that due to the fact that they are primarily trained on legal specific instruction data, their instruction-following skills are negatively impacted. Fourth, Fuzi-Mingcha scores 97.59 on task 2-4, whereas all other models score less than 65. Considering its performance on other legal tasks, we suspect that there is a data contamination. This also highlights potential caveats when evaluating LLMs~\cite{schaeffer2023pretraining}. Overall, it is promising that most LLMs show some capability in handling legal tasks, but there's still a substantial room for improvement. Even the top-performing model, GPT-4, achieves only an average score of 52.35 (zero-shot) / 53.85 (one-shot), highlighting the need for additional efforts in the future.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{Figure/model_size_result_all.pdf}
    \caption{\small Performances of models with various sizes. It can be observed that scaling up the model size usually improves the performance, but the improvement is more consistent in the one-shot than in the zero-shot scenario. }
    \label{fig:effects_scale}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{Figure/slope_plots_3-4_3-5.pdf}
    \caption{\small Comparison between different models for 3-4 and 3-5 tasks. Most LLMs are \emph{unable} of properly utilizing legal article content information to aid in judgment prediction. Including the article content often degrades the performance.}
    \label{fig:effects_article}
\end{figure}

\subsection{Analysis}
\label{sec:analysis}

We find that the current legal specific LLMs do not necessarily outperform general large language models. We analyze the effect of model size and training approach on large language models to better understand which aspects most influence model performance. 

\paragraph{Scaling up the model size results in better performance in one-shot case.}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{Figure/picture_2_4.pdf}
    \caption{\small Comparison of base LLMs and their SFT and RLHF variants. We put the abstention rate bar on top of the model score bar to visualize the improvement space for (1) correctly following instructions and (2) task-specific precision. The abstention rate score is averaged only on tasks requiring answer extraction.}
    \label{fig:effects_sft}
\end{figure}

Prior work shows that larger models perform better in general NLP tasks~\cite{kaplan2020scaling}. We analyze whether this finding still holds in legal domain. Specifically, we select representative models from different categories with varying model sizes and calculate the overall performance and abstention rate of different tasks. The results are shown in Figure \ref{fig:effects_scale}. We observe that increasing the model size typically helps improve model performance in one-shot settings. Also, the abstention rate becomes lower, indicating that larger models are better at following instructions. Nevertheless, we find that ChatLaw is an outlier, a larger size results in lower performance. In zero-shot scenarios, mixed results are observed, simply increasing the model size may not automatically lead to better performance.

\paragraph{Most LLMs can not efficiently leverage article content.} 
Retrieval augmentation is a common way to improve the accuracy of generative models~\cite{lewis2020retrieval,jiang2023active,shen2023neural}. By including the content of the related legal articles in task 3-4 and feeding them into the LLM input, we replicate the retrieval augmentation scenario to form task 3-5. The goal is to see if the model can successfully use this additional knowledge to predict the proper jail sentence when supplied with the relevant article reference that articulates the range of prison terms. We compare 5 models of various types and visualize the comparison between task 3-4 and 3-5 in Figure \ref{fig:effects_article}. The results show that the vast majority of models \emph{fail} to make any progress on the jail sentence prediction challenge by using the provided article content. There is a decrease in performance across the board for most models, including GPT-4. This suggests that simple retrieval enhancement methods may not bring further improvement, and how to obtain LLMs that are able to effectively utilize the retrieval information is still an open problem. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{Figure/picture_2_3.pdf}
    \caption{\small Comparison between different legal specific LLMs and their base models. Legal specific fine-tuning significantly improves the performance and reduces the abstention rate.}
    \label{fig:effects_law_adapt}
\end{figure}

\paragraph{SFT may improve the performance but RLHF may not.}

We illustrate the performance difference of 9 models before and after SFT in Figure \ref{fig:effects_sft}. In most cases, models perform better through SFT. Notably, the SFT data are collected from the general domain, yet they still substantially improve model performance on legal tasks. The LLaMA-2 series further applies RLHF on top of SFT. However, we do find that RLHF-trained models can refuse to answer some questions~\cite{ouyang2022training} (resulting in a higher abstention rate and lower scores), which can lead to a drop in performance on legal tasks. This suggests that when creating models for legal tasks, stacking RLHF on top of SFT can be counterproductive.

\paragraph{Legal specific fine-tuning is helpful.} 

To assess the impact of legal specific fine-tuning, we contrast three LLMs fine-tuned for legal tasks with their corresponding base models, as depicted in Figure~\ref{fig:effects_law_adapt}. It is evident that after legal specific fine-tuning, there is a consistent enhancement of model scores and reduction of abstention rates. This underscores the efficacy of the training strategies employed. Closer inspection of the three cognitive levels reveals that Baichuan-7B and LLaMA-13B perform very poorly on memorizing tasks, which suggests they have not been pre-trained on large, high-quality legal corpora. Nonetheless, fine-tuning them on legal corpora both lead to significant improvement. Even for LLMs like Ziya-LLama-13b who have excellent memorization of legal principles, fine-tuning that is unique to the field of law can significantly boost performance on tests requiring comprehension and applying skills. Comparing to the LawGPT series, we found that version 1.1, which is fine-tuned only on 350k instruction data, outperformed version 1.0 which is fine-tuned on 500k judgement documents followed by 300k instructions. This suggests that beginning with a lower-performing model and undertaking continuous pre-training, which is both time-consuming and resource-intensive, is inferior to starting with a superior foundation model and fine-tuning on high-quality instruction data. Future research may want to try fine-tuning a stronger base model (like qwen-chat or InternLM-chat) for better performance on legal tasks.

								
								

       

\section{Conclusion}
Using LLMs to benefit the legal domain is a promising topic. However, existing benchmarks to measure the legal knowledge of LLMs either focus on a limited subset of tasks, or are based on American laws in English language. This paper presents \emph{LawBench}: a meticulously crafted, comprehensive evaluation benchmark to assess LLMs in performing legal-related tasks under the Chinese civil law system. We provide  structured taxonomy of the skill set required for legal-related tasks, including 20 diverse tasks corresponding to 3 cognitive dimensions: legal knowledge memorization, understanding and applying. We undertake a thorough examination of 51 LLMs and assess their performance. The results demonstrate that current LLMs are still unable to give meaningful judicial aid, and their scores on most tasks are often poor. While fine-tuning open-source LLMs on legal specific language results in some advances, they still lag far below GPT-4. As the legal field is highly professional, much of the data used in practical applications is confidential. Developing a high-quality large language model for legal tasks necessitates collaboration among multiple institutions. We hope the release of LawBench can serve as a foundation for future research and we seek to encourage cooperation in order to further this effort.

\section*{Limitations}

The majority of our datasets are acquired through sampling publicly available data on the internet. Even though we have made efforts to select newest versions of datasets, there can still be risks of test data leakage given that existing LLMs have been exhaustively trained on massive amount of Internet data. It is possible that LLMs explicitly trained on these task formats, or even the exact test data, can exhibit exceptionally high scores~\cite{schaeffer2023pretraining}. We will seek more principled ways to prevent data contamination in the future.

Another notable limitation is the answer extraction methods and evaluation metrics for generative tasks. Even though we have hand-engineered task-specific rules to extract the answer, there still can be cases that the rule fails to match. For generative tasks, we only use Rouge-L to evaluate the model predictions for convenience, which cannot fully reflect the human judgement about the answer quality. Currently, there is a lack of automated methods to effectively evaluate model predictions from legal aspects. We plan to consider training an evaluation model tailored for legal tasks in the future, or experiment with LLM-based evaluations~\cite{liu2023mmbench,yu2023kola}.

\section*{Acknowledgments}
This work is an extension which was supported by National Key R\&D Program of China (2016YFC0800803).  This work is also partly
supported by the National Key R\&D Program of China
No.2022ZD016100 and Shanghai Postdoctoral Excellence
Program (No.2022235). We appreciate Zhixin Yin for helping arrange tables and alter chart layouts. We also thank Qi Li for collecting and organizing some public data.

\clearpage

\end{document}