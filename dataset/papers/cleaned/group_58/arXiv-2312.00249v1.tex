\title{Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities}

\begin{document}

\maketitle

\begin{abstract}
The auditory system plays a substantial role in shaping the overall human perceptual experience. While prevailing large language models (LLMs) and visual language models (VLMs) have shown their promise in solving a wide variety of vision and language understanding tasks, only a few of them can be generalised to the audio domain without compromising their domain-specific capacity. In this work, we introduce \textbf{A}coustic \textbf{P}rompt \textbf{T}urning (APT), a new adapter extending LLMs and VLMs to the audio domain by soft prompting only. Specifically, APT applies an instruction-aware audio aligner to generate soft prompts, conditioned on both input text and sounds, as language model inputs. To mitigate the data scarcity in the audio domain, a multi-task learning strategy is proposed by formulating diverse audio tasks in a sequence-to-sequence manner. Moreover, we improve the framework of audio language model by using interleaved audio-text embeddings as the input sequence. This improved framework imposes zero constraints on the input format and thus is capable of tackling more understanding tasks, such as few-shot audio classification and audio reasoning. To further evaluate the reasoning ability of audio networks, we propose natural language audio reasoning (NLAR), a new task that analyses across two audio clips by comparison and summarization. Experiments show that APT-enhanced LLMs (namely APT-LLMs) achieve competitive results compared to the expert models (i.e., the networks trained on the targeted datasets) across various tasks. We finally demonstrate the APT's ability in extending frozen VLMs to the audio domain without finetuning, achieving promising results in the audio-visual question and answering task. Our code and model weights are released at \url{https://github.com/JinhuaLiang/APT}.\let\thefootnote\relax\footnotetext{\textsuperscript{*}The work does not relate to H.P.'s position at Amazon.}
\end{abstract}

\section{Introduction} \label{sec:introduction}
Auditory stimuli contribute to shaping the overall human perception experience. While visual language models (VLMs)~\citep{li_blip-2_2023,liu_visual_2023,dai_instructblip_2023,shukor_unified_2023,han_imagebind-llm_2023} that are capable of solving diverse down-streaming tasks have emerged driven by the advent of large language models (LLMs) and massive visual-text pretraining, only few of them~\citep{shukor_unified_2023,han_imagebind-llm_2023} can be adapted to the audio domain while maintaining their performance in the image/video domain.

To embrace more than two modalities, few works recently attempted to explore the diversity and heterogeneity of tasks and modalities. UniVAL~\citep{shukor_unified_2023} unified input/output format, model architecture, and training objective, and therefore, learned a shared encoder-decoder LLM with multi-modal curriculum learning. ImageBind-LLM~\citep{han_imagebind-llm_2023} adopted ImageBind, a cross-modal encoder bundling six modalities (including images) to a shared embedding space, and adapted the LLM with a frozen image encoder. While both works extended visual LLMs to other domains, in addition to the considerable amount of training data, they are bundled to a specific architecture, hindering the ability to adapt them to a new modality.

Meanwhile, following the VLM framework, a few works proposed audio-only LLMs where a pair of audio clip and text token are used as inputs for text generation. LTU~\citep{gong_listen_2023} bridged audio with language modalities by end-to-end finetuning on an instruction-based dataset. Pengi~\citep{deshmukh_pengi_2023} applied multi-task learning to leverage off-the-shelf datasets, alleviating the data-scarcity issue. Still, they are restricted to two domains (i.e., audio and language). They also cannot address tasks beyond the [audio, question, answer] format, e.g., few-shot audio classification~\citep{liang_adapting_2023}. One question thereby arises: \textit{Can we adapt LLMs/VLMs to the audio domain by simply encoding sound clips as acoustic prompts?}

In this work, we introduce APT (\textbf{A}coustic \textbf{P}rompt \textbf{T}uning), an acoustic adapter that extends LLMs and VLMs to audio understanding and reasoning tasks using soft prompts only. Specifically, APT encodes audio clips into audio feature maps and then uses an audio aligner to generate acoustic prompts conditioned on both input instructions and the audio feature maps. When training APTs, a multi-task learning strategy is adapted by formulating diverse audio tasks in a sequence-to-sequence format. Besides popular audio tasks (such as audio tagging and audio captioning), APT makes full use of publicly-available datasets by training on three new tasks, namely query-based sound event detection, temporal event retrieval, and sound event counting, to learn fine-grained audio features. In addition, we improve the audio language model framework by juxtaposing acoustic prompts with text embeddings. Rather than applying soft prompts as a prefix to the input texts, the improved framework exerts no constraints on the format of the input sequence. Therefore, the APT-enhanced LLMs, namely APT-LLM, can analyse multiple audio clips in a single feed-forward process, facilitating more audio understanding tasks, such as few-shot audio classification and audio reasoning. To further evaluate models' reasoning ability, we propose a new task referred to as natural language audio reasoning (NLAR) which is devised to distinguish, compare, and summarise two audio clips. Experiments on existing audio understanding tasks, including audio tagging, audio captioning, and few-shot audio classification, show that APT-LLM achieves performance on par with those obtained by audio language models or even domain-expert models. APT also yields a good performance on the proposed NLAR, indicating its capacity to comprehend over a single audio clip. Finally, quantitative studies are conducted to demonstrate that APT improves the performance of a VLM in the audio-visual question and answering (AVQA) task.

Our contributions are summarized as below:
\begin{itemize}
    \item An acoustic adapter is introduced to extend LLMs and VLMs to the audio modality by soft prompting. To mitigate data scarcity in the audio domain, we improve the present multi-task training approach by devising new tasks and their corresponding prompts during training. Leveraging the annotations in off-the-shelf databases, APT-LLM learns acoustic embeddings with fine-grained features from task discrepancy.
    
    \item APT formulates diverse audio tasks as a sequence-to-sequence task where generated text is conditioned on interleaved audio-text tokens. Without any constraints on the input format, APT-LLM is not only able to solve different tasks according to the diverse instructions, but also to exploit the correlation among different audio clips in the same sequence. To the best of our knowledge, APT-LLM is the first audio-language model reasoning beyond a single audio clip.
    
    \item Natural language audio reasoning, a new audio comprehension task, is proposed to distinguish, compare, and summarise two audio clips. Compared to existing audio tasks, this new task not only evaluates model ability to understand an audio clip, but also requires models to analyse the content of two recordings by comparison and summarisation. APT-LLM is then benchmarked on this task.
    
    \item BLIP-2~\citep{li_blip-2_2023} coupled with APT (namely APT-BLIP-2) is studied qualitatively and quantitatively on the audio-visual question and answering task~\citep{yang_avqa_2022}. Without further finetuning, APT-BLIP-2 can work with the visual modality directly, showcasing an efficient approach for extending multi-modal LLMs to a new modality.

\end{itemize}
\section{Related works} \label{sec:related_works}
\textbf{Multimodal language models.}
From recent advances, LLMs~\citep{touvron_llama_2023,chiang_vicuna_2023,openai_gpt-4_2023} has exhibited astonishing comprehending and reasoning capacity. Driven by the open-world knowledge in LLMs, a variety of visual language models have been proposed with different alignment methods to integrate image/video data to text tokens~\citep{alayrac_flamingo_2022,li_blip-2_2023,dai_instructblip_2023,zhang_llama-adapter_2023}. However, most of them are restricted to the visual domain, largely due to the lack of training data in other domains (such as audio) and modality discrepancies. Recently, ImageBind-LLMs~\citep{han_imagebind-llm_2023} bridged the image encoder of ImageBind~\citep{girdhar_imagebind_2023}, a six-modality language model, with an LLM and used visual tokens as soft prompts within the language model. UniVAL~\citep{shukor_unified_2023} uniformed the input/output, the architecture, and the training object of multimodal LLMs and then devised a curriculum learning for gradual exposure to new modality. While both works adapted VLMs to other domains, they demands massive multimodal data to train the overall networks from scratch. Instead, this work investigates a domain-specific adapter that can be applied to extend any existing VLM/LLM to an additional modality (such as audio). 

\textbf{Audio language models.} Following VLM, some works built audio language models for sound-only tasks. SpeechGPT~\citep{zhang_speechgpt_2023} collected a speech-text instruction dataset, thereby learned to perceive and generating speech content in the audio. LTU~\citep{gong_listen_2023} rendered an open-end dataset, containing 3.7M [audio, question, answer] tuples, and learned with a perception-to-understanding curriculum. While the aforementioned models achieved a good audio comprehension ability, they required a uniform input format as a triplet tuple. To work around this question, Pengi~\citep{deshmukh_pengi_2023} proposed a multi-task framework where an audio language model is trained with off-the-shelf audio datasets by prompted with different predefined questions. This work differs from these prior works in three-fold: 1) Rather than an audio-only language model, APT explores how to adapt existing VLMs and LLMs to the sound domain; 2) APT-LLM improves the multi-task framework by designing three new training tasks. By accessing existing datasets from different aspects, APT-LLM learns a fine-grained audio representation, and 3) APT-LLM re-frames the present input format, namely [audio, question, answer], to let audio and text alternate in a sequence. In this way, APT-LLM is able to ingest more than one audio clip in a single feed-forward, unleashing it to more audio tasks. To the best of the knowledge, APT-LLM is the first model that integrates in-context learning with multi-task training.

\section{Method} \label{sec:APT LLMs}
Current audio LLMs~\citep{gong_listen_2023,deshmukh_pengi_2023} learned to bridge audio with language by framing popular audio tasks (e.g., classification and captioning tasks) to the audio-conditioned text generation problem. Going beyond the [audio, question, answer] format, APT-LLM encodes multiple audio clips in one feed-forward process and juxtaposes them with text embeddings without any order constraint. The more flexible training paradigm mitigates the need for high-quality data and massive databases, and thus reduces required computations. Moreover, juxtaposing audio clips with texts enables APT-LLM to address more comprehensive reasoning tasks, such as natural language audio reasoning. We first discuss the overall architecture of APT-LLM in Section~\ref{subsec:architecture}, and then elaborates APT-LLM learning objective in Section~\ref{subsec:learning_objective} and the training recipe in Section~\ref{subsec:multi_task_learning}. In Section~\ref{subsec:audio_reasoning_task}, we define the natural language audio reasoning task, a new task to evaluate the audio comprehension ability of models.
 
\subsection{Architecture} \label{subsec:architecture}
\begin{figure}[t]
    \centering
    \includegraphics[scale=0.8]{src/lam.pdf}
    \caption{Illustration of the proposed APT-LLM. APT-LLM is constituted by three components: an audio encoder, an audio aligner, and a LLM. The audio encoder extracts audio feature maps from input spectrograms. The audio aligner then projects each audio feature map to 32 acoustic embeddings according to the input text. These acoustic embeddings, together with the added embeddings of the audio token ``$<$AUDIO$>$'', are juxtaposed with text embeddings. The interleaved audio-text embeddings are fed into the LLM to generate the output text. APT-LLM can ingest multiple audio clips in a sequence and thus benefit from diverse tasks during training.}
    \label{fig.:lam}
    \vspace{-0.3cm}
\end{figure}

The overall structure of APT-LLM is illustrated in Figure~\ref{fig.:lam}, with main components of an audio encoder, an audio aligner, and a large language model. APT-LLM alternates audio clips with text tokens without any format constraints and thus benefits from task diversity and large-scale pretraining.

\textbf{Audio encoder: from spectrograms to feature maps.} We use Audio-MAE~\citep{huang_masked_2022}, a vanilla 12-layer transformer encoder that learns to reconstruct randomly-masked spectrogram patches during training, as the audio encoder. Rather than using the last layer that finetuned for classification tasks, we apply the output feature map from the penultimate block of an Audio-MAE to encode fine-grained patterns in the sound.

\textbf{Audio aligner: from 10-second feature maps to a fixed number of audio tokens.} This module connects the audio encoder to the frozen language model as shown in Figure~\ref{fig.:lam}. It ingests a text prompt together with a variable number of audio feature maps extracted by the audio encoder as input and produces a fixed number of acoustic embeddings. Following the implementation of \citep{li_blip-2_2023}, four transformer blocks constitute our audio aligner where 32 trainable embeddings attend to the input text tokens and extract the relevant information from the audio feature maps. Resampling a varying-length of audio embeddings to 32 acoustic embeddings, APT aligner reduces the computational complexity in following attention mechanism while filtering out the information irrelevant to the input text. 

\textbf{Large language model: from interleaved audio-text tokens to generated text.} The language model predicts the output text by taking into account the previous generated texts and the input audio-text tokens. We freeze all parameters in the language model during training. In addition to existing works~\citep{li_blip-2_2023,gong_listen_2023}, we add before each audio clip a learnable audio token, ``$<$AUDIO$>$'', as a special token to indicate the beginning of audio tokens. We find this token helps the language model to distinguish audio tokens from text tokens when interleaving them together.

\subsection{Learning objective}  \label{subsec:learning_objective}
In order to motivate our training framework, we first present the learning objective used in existing work~\citep{deshmukh_pengi_2023}. Let an audio-text pair in [audio, question, answer] format be referred to as $(a, t, g)$ where $a$, $t$, $g$ are the audio clip, input text, and output text, respectively, and $\mathbf{X}$ be input sequential embeddings to the language model. To align the audio modality to the language modality, an audio encoder $\mathcal{A}$ and an audio aligner $\mathcal{M}$ project the audio $a$ into a sequence $\mathbf{X}_{\mathrm{audio}}$:
\begin{equation} \label{eqn.:extract_audio}
    \mathbf{X}_{\mathrm{audio}} = \mathcal{M}_{\theta}(\mathcal{A}_{\phi}(a, t)),
\end{equation}
where $\phi$ and $\theta$ are the parameters of the audio encoder $A$ and the aligner $\mathcal{M}$. The audio embeddings are used as a prefix and then concatenated with the input text embeddings as
\begin{equation} \label{eqn.:concat}
    \mathbf{X}_{\mathrm{audio;text}} =  \mathcal{C}(\mathbf{X}_{\mathrm{audio}}, \mathbf{X}_{\mathrm{text}}) = \mathcal{C}(\mathcal{M}_{\mathcal{\theta}}(\mathcal{A}_{\phi}(a)), \mathcal{W}_{\psi}(t)),
\end{equation}
where $\mathcal{C}$ is a concatenating function and $\psi$ denotes the parameters of the word embedding layer $\mathcal{W}$ in the language model. Assuming the length of the concatenated embeddings $\mathbf{X}_{\mathrm{audio;text}}$ be $L$, the parameters of the audio LLM are optimised by measuring the probability distribution of the next token conditioned on its previous tokens:
\begin{equation} \label{eqn.:next_token_pred}
    p(\mathbf{X}_{\mathrm{pred}}|\mathbf{X}_{\mathrm{audio}}; \mathbf{X}_{\mathrm{text}}) = \prod_{i=L+1}^{L+|g|}p_{\phi,\theta,\psi}(\mathbf{x}_i|X_{\mathrm{audio;text},<i}; \mathbf{X}_{\mathrm{pred},<i}),
\end{equation}
In this way, prevailing LLMs are able to unify many audio-to-text tasks in a sequence-to-sequence manner. However, not all understanding tasks can be fitted into the format of [audio, question, answer] (e.g., to learn a new concept using a handful of labelled audio examples), calling for a new paradigm that can exploit diverse tasks in a uniform input/output format.

We thereby propose a new learning framework in which interleaved audio-text embeddings are used as the LLM's input such that the model is able to leverage and learn from more diverse tasks during training. Let $\mathbf{a}$ and $\mathbf{t}$ be audio clips and input text, and $g$ still be output text. Assuming both $\mathbf{a}$ and $\mathbf{t}$ have $N$ different elements, the input audio-text pairs are denoted as $[(a^i, t^i)]_{i=1}^N$ where $a^i$ and $t^i$ are the $i$-th audio clip and input text, respectively. Eqn. (\ref{eqn.:concat}) can be re-written as
\begin{equation} \label{eqn.:interleave}
    \mathbf{X}_{audio;text}\!=\!\mathcal{I}(\mathbf{X}_{audio}, \mathbf{X}_{text}) = [\mathcal{M}(\mathcal{A}_{\phi}(a_{1}, t_{1})), T_{\psi}(t_{1}), \ldots, \mathcal{M}(\mathcal{A}_{\phi}(a_{N}, t_{N})), T_{\psi}(t_{N})],
\end{equation}

where $\mathcal{I}$ is the function that alternating acoustic embeddings with text embeddings. In this way, APT-LLM can integrate multiple audio clips in the input sequence, enabling itself to learn from more audio understanding tasks.

\begin{table}[]
\centering
\caption{Multi-task learning strategy adopted by APT-LLMs. ``\#Audio samples'' denote the number of audio clips in the dataset. Stage 0-2 denotes audio-text alignment, learning from single audio clips, and learning from multiple clips, separately.}
\label{tab.:multi-task_learning}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lccccccc@{}}
\toprule
\multirow{2}{*}{Task} &
  \multicolumn{3}{c}{Training stages} &
  \multirow{2}{*}{Dataset} &
  \multirow{2}{*}{\#Audio samples} &
  \multirow{2}{*}{Durations} &
  \multirow{2}{*}{Setup} \\ \cmidrule(lr){2-4}
                                  & 0 & 1 & 2 &             &      &       &            \\ \midrule
Audio tagging                     & \ding{51}  & \ding{51}  & \ding{51}  & AudioSet    & 2M   & 5.8kh & train/test \\ \midrule
                                  &   &   &   & Wavcaps     & 400k & 7.6kh &            \\
Audio captioning                  & \ding{51}  & \ding{51}  & \ding{51}  & AudioCaps   & 39k  & 108h  & train/test \\
                                  &   &   &   & Clotho v2   & 7k   & 31h   &            \\ \midrule
Audio question and answering      &   & \ding{51}  & \ding{51}  & Clotho AQA  & 2k   & 12h   & train      \\ \midrule
Query-based sound event detection &   & \ding{51}  & \ding{51}  & AudioSet-SL & 81k  & 226h  & train      \\ \midrule
Temporal event retrieval          &   & \ding{51}  & \ding{51}  & AudioSet-SL & 81k  & 226h  & train      \\ \midrule
Sound event counting              &   & \ding{51}  & \ding{51}  & AudioSet-SL & 81k  & 226h  & train      \\ \midrule
Few-shot audio classification     &   &   & \ding{51}  & AudioSet    & 2M   & 5.8kh & train/test \\ \midrule
Natural language audio reasoning  &   &   & \ding{51}  & NLAR        & 0.2k & 1.2h  & train/test \\ \bottomrule
\end{tabular}%
}
\vspace{-0.3cm}
\end{table}

\subsection{Multi-task learning strategy}  \label{subsec:multi_task_learning}
With the uniform input/output format, APT-LLM is able to learn from a large variety of audio tasks and thus benefiting from the diverse training datasets. As shown in Fig.~\ref{tab.:multi-task_learning}, instead of passing through all training data directly, APT-LLM is trained through:

\textbf{Audio-text alignment.} Before coupled with a LLM, we pretrain APT audio aligner to bridge the audio modality and the text modality. To this end, we freeze the other components and optimise parameters of the audio aligner with audio-text pairs from AudioSet~\citep{gemmeke_audio_2017} and WavCaps~\citep{mei_wavcaps_2023}. During training, a fixed number of acoustic embeddings are learnt to extract relevant information from the audio feature maps according to the input text tokens. Following~\cite{li_blip-2_2023}, the audio aligner learns with triplet training objectives: audio-text matching, Audio-grounded text generation, and audio-text contrastive (See more in Appendix~\ref{appendix:audio-text_alignment})

\textbf{Learning from single audio clip.} 
After APT has extracted acoustic embeddings according to the input text, the following LLM learns to project these tokens to the word embeddings of the targeted LLM. APT-LLM is thus trained with multiple tasks using various prompts (see more in Appendix~\ref{appendix:multitask_prompt}). In addition to existing audio tasks, namely audio tagging, audio captioning, and audio question and answering, we design three new tasks: (1) \textit{Query-based sound event detection} that aims to train a model to predict the onset and offset time of a specific sound event; (2) \textit{Temporal event retrieval} that is to recognise sound events occurred in a specific period, and (3) \textit{Sound event counting} that requires a model to count the frequency of a specific sound event in a recording. Instead of rendering datasets, we exploit the publicly-available AudioSet with strong labels~\citep{hershey_benefit_2021} using different prompts (see more in~\ref{appendix:multitask_prompt}). This multi-task framework facilitates APT-LLM's learning from diverse datasets, including AudioSet~\citep{gemmeke_audio_2017}, WavCaps~\citep{mei_wavcaps_2023}, AudioSet with strong labels~\citep{hershey_benefit_2021}, Clotho~\citep{drossos_clotho_2020}, AudioCaps~\citep{kim_audiocaps_2019}, and Clotho-AQA~\citep{lipping_clotho-aqa_2022}.

\textbf{Learning from multiple audio clips.}
In addition to the aforementioned tasks, APT-LLM learns from two additional tasks by juxtaposing more than one audio clips with input text. Specifically, few-shot audio classification and natural language audio reasoning are added to the multi-task training framework in this stage. On the one hand, for the few-shot audio classification, APT-LLM predicts labels of sound events by exploiting the correlation between input audio clips. On the other hand, APT-LLM is required to compare and summarise two different sounds in the natural language audio reasoning task (see the following Section~\ref{subsec:audio_reasoning_task}). Trained on these two tasks, APT-LLM learns to analyse beyond a single recording and answer questions as per input questions. We adopts AudioSet~\citep{gemmeke_audio_2017} and the proposed datasets for few-shot audio classification and natural language audio reasoning, respectively.

\subsection{Natural language audio reasoning task} \label{subsec:audio_reasoning_task}

\begin{table}[t]
\centering
\caption{An example demonstrating APT-LLM's capacity of audio reasoning. It requires audio networks to comprehend recordings and reasoning across multiple recordings.}
\label{tab.:nlar_example}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}ll@{}}
\toprule
\multicolumn{2}{l}{Natural Language Audio Reasoning (NLAR) example: \textit{``Where is the sudden sound?''}} \\ \midrule
User         &  \\
             & \includegraphics[width=1.0\columnwidth]{src/wav.pdf} \\
             & Question: Which recording has a more sudden and startling sound event?             \\  \midrule
APT-LLM      & First.      \\
Ground truth & first       \\ \bottomrule
\end{tabular}%
}
\vspace{-0.3cm}
\end{table}

One of the complex reasoning ability of human is to learns across different pieces of sounds, understanding what happening in each audio and analysing the content of different audio by comparison and summarisation. However, existing audio tasks focus on analysing acoustic scenarios in an independent recording by recognising the inside sound events~\cite{kong_panns_2020} and/or retrieval their spatio-temperal information~\cite{politis_starss22_2022}. We thus propose natural language audio reasoning (NLAR), a new task where the model is required to answer questions by explicitly comparing or summarising two different audio recordings. Table~\ref{tab.:nlar_example} showcases an example in the NLAR task. An audio system takes two audio clips together with a free-form text query as input and is expected to answer the question by taking into consideration the both audio. For details of the design process and examples of the proposed natural language audio reasoning task, please refer to Appendix~\ref{appendix:nlar}. Compared to existing audio tasks, the proposed audio reasoning task features three notable differences:

\textbf{Comprehension of multiple audio clip}: This task requires a model to answer open-ended questions by comparing or summarising the content of two different audio clips. The model must first comprehend the two audio clips as per the raised question separately and answer the question by taking into account the two audio inputs. An example of the audio reasoning task can be found in Table~\ref{tab.:nlar_dataset}. 

\textbf{Diverse question types}: Questions for natural language audio reasoning task assess diverse auditory aspects, such as the presence, the frequency, and acoustic features of sound events. Therefore, the model should not only ground the sound events in the recordings, but also retrieve relevant information as per the input question. 

\textbf{Effects of the chronological order}: Compared to existing audio tasks, e.g.,~\citep{li_blip-2_2023} and~\citep{gong_listen_2023}, the proposed audio reasoning task emphasises the order of the audio recordings in a sequence. In other word, the answer associated with the audio pair ``[Audio A, Audio B]'' could be different with the answer associated with  ``[Audio B, Audio A]'' when their questions are the same. In this way, we expect audio understanding models to be able to attend to different portions of the input sequence when the question vary. 

By evaluating audio language models on the natural language audio reasoning task, we achieve more comprehensive assessment of audio language models.

\section{Experiments} \label{sec:experiments}
APT was first coupled with LLMs (i.e., APT-LLM) and evaluated as a general-purposed audio learner on a variety of existing audio-related benchmarks, including audio tagging, audio captioning, and few-shot audio classification. To further assess its ability in comprehending two audio clips of interest, APT-LLM was further benchmarked on the natural language audio reasoning task. In addition to audio comprehension, we also experimented and analysed (quantitatively and qualitatively) APT as an zero-shot adapter to BLIP-2~\citep{li_blip-2_2023,dai_instructblip_2023}, a state-of-the-art VLM.

\begin{table}[t]
\centering
\caption{Zero-shot performance comparison with audio language models. We group the methods in terms of their training strategy. ``\#Params.'' denotes the number of trainable parameters and ``\#Pairs'' represents the number of audio-text pairs. $\uparrow$ indicates the higher number, the better performance.}
\vspace{1em}
\label{tab.:existing_tasks}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}llllcc@{}}
\toprule
Model & \#Params. & \#Pairs & AudioSet (mAP$\uparrow$) & AudioCaps (SPICE$\uparrow$) & Clotho (SPICE$\uparrow$) \\ \midrule
\textit{Audio-language models trained with the contrastive loss} &        &      &      &      &      \\
AudioCLIP~\citep{guzhov_audioclip_2022}                 & 30M    & 2M   & 25.9 & -    & -    \\
CLAP~\citep{elizalde_clap_2023}                         & 190M   & 128k & 5.8  & -    & -    \\ \midrule
\textit{One-for-all models for various audio tasks}              &        &      &      &      &      \\
LTU~\citep{gong_listen_2023}                            & 96M    & 5.7M & 18.5 & 17.0 & 11.9 \\
Pengi~\citep{deshmukh_pengi_2023}                       & $>$191M & 3.4M & -    & 18.2 & 12.6 \\
\rowcolor{lightgray!60}APT-LLM                          & 101M   & 2.6M & 14.7 & 17.1 & 11.6 \\ \bottomrule
\end{tabular}%
}
\end{table}

\subsection{Experiment setup} \label{subsec:experiment_setup}
Our models were implemented relying on the BLIP-2 framework~\citep{li_blip-2_2023}. We used Audio-MAE~\citep{huang_masked_2022} as the audio encoder in all APT models we developed. Considering Audio-MAE only contains 100M parameters, we used a two-layer transformer as the aligner to bridge the audio and text domains. Without an explicit statement, we coupled APT with Vicuna 7B v1.1~\citep{chiang_vicuna_2023} for evaluation. We testified APT-LLM with two close-ended datasets: AudioSet~\citep{gemmeke_audio_2017} and ESC-50~\citep{piczak_esc_2015}; and four open-ended datasets: Clotho~\citep{drossos_clotho_2020}, AudioCaps~\citep{kim_audiocaps_2019}, natural language audio reasoning (in Section~\ref{subsec:audio_reasoning_task}), and audio-visual question and answering (AVQA)~\citep{yang_avqa_2022}.

Adam optimiser was used for model training. We applied Warmup strategy in the first 2K steps and used a cosine linear learning rate in the following steps. We trained the APT models using three NVIDIA A100 (40G) GPUs. The audio-text alignment pretraining and multi-task training took 5 days separately.

\subsection{Comparison with existing approaches} \label{subsec:comparison_with_existing_approaches}
We compare APT-LLM against the state-of-the-art specialised systems (i.e., the networks trained with task-specific data) and previous audio language models on existing tasks, including audio tagging, audio captioning, and few-shot audio classification.

\textbf{Audio tagging} requires models to predict classes of test samples from a predefined label set. We evaluated the models on the AudioSet dataset~\citep{gemmeke_audio_2017}. During inference, APT-LLM was prompted using the sentence ``\textit{Summarize the audio with key words.}'' Since APT generates free-form texts directly, we used the APT text encoder pretrained in the stage 1 to encode generated answers and the given classes names to text embeddings. Afterwards, cosine similarity is calculated as the classification probably.
Consistent with the findings in previous work~\citep{gong_listen_2023}, Table~\ref{tab.:existing_tasks} shows a performance gap between audio language models and task-specific models. This is expected since the latter addresses the classification task as a close-end problem, with much lower complexity than open-ended problem where models need to search across the entire word embedding space. In addition, we found that the performance of the text encoder greatly impacts the classification result when evaluating the generated answers. This finding can be explained by the fact that  word embeddings of different classes should be sparse enough to when measuring their distance to the embeddings of generated answers.

\begin{table}[t]
\centering
\caption{Performance comparison in audio captioning tasks. $\uparrow$ indicates the higher number, the better performance.}
\label{tab.:existing_tasks}
\vspace{1em}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lcccc@{}}
\toprule
Model                                                   & \multicolumn{2}{c}{AudioCaps}   & \multicolumn{2}{c}{Clotho}      \\ \cmidrule(l){2-5} 
                                                        & SPICE $\uparrow$ & SPIDEr $\uparrow$ & SPICE $\uparrow$ & SPIDEr $\uparrow$ \\ \midrule
\textit{Specialised systems trained with task-specific examples} &                &                &                &                \\
PANNs-BART~\citep{xu_investigating_2021}                & 0.153          & 0.183          & 0.083          & 0.127          \\
CNN-GPT2~\citep{kim_prefix_2023}                        & 0.167          & \textbf{0.438} & 0.111          & 0.215          \\
WSAC+PD~\citep{kouzelis_weakly-supervised_2023}         & 0.173          & 0.403          & 0.123          & 0.247          \\ \midrule
\textit{One-for-all models for various audio tasks}              &                &                &                &                \\
\rowcolor{lightgray!60}APT-LLM                          & \textbf{0.191} & 0.402          & \textbf{0.132} & \textbf{0.248} \\ \bottomrule
\end{tabular}%
}
\vspace{-0.3cm}
\end{table}

\textbf{Audio captioning} is the task where models are supposed to generate free-form description according to an input recording. The sentence ``\textit{Describe the audio clip concisely}.'' is applied as the input prompt. We finetune APT-LLM two epochs on the training split of AudioCaps~\citep{kim_audiocaps_2019} and Clotho~\citep{drossos_clotho_2020} datasets and compare it with the captioning models trained on the both tasks. As shown in Table~\ref{tab.:existing_tasks}, APT-LLM achieves the best performance on both AudioCaps and Clotho datasets in terms of SPICE and SPIDEr.

\begin{table}[h]
\centering
\caption{Accuracy (\%) of various methods on ESC-50 in the few-shot settings.}
\label{tab.:few_shot}
\begin{tabular}{llc}
\toprule
                                          & \multicolumn{2}{c}{Accuracy$\uparrow$} \\ \hline
                                          & 5-way          & 12-way         \\ \cline{2-3}
\multicolumn{3}{l}{\textit{Specialised systems trained with task-specific examples}} \\
ProtoNet~\citep{snell_prototypical_2017}   & 88.2          & 77.7          \\
MatchNet~\citep{vinyals_matching_2016}     & 86.8          & 71.8          \\
HPN~\citep{liang_leveraging_2022}          & 88.7          & 78.7          \\ \midrule
\multicolumn{3}{l}{\textit{Audio language models trained with constractive learning}} \\
TIP-adapter~\citep{zhang_tip-adapter_2022} & 97.5          & 95.6          \\
Treff adapter~\citep{liang_adapting_2023}  & 98.5          & 96.3          \\ \midrule
\multicolumn{3}{l}{\textit{One-for-all models for various audio tasks}} \\
\rowcolor{lightgray!60}
APT-LLM                                & 91.0          & 54.2          \\ \bottomrule
\end{tabular}
\end{table}

\textbf{Few-shot audio classification} is to classify test audio clips using labelled audio examples. Models were evaluated in the $N$-way $K$-shot problem where: (1) there are $N$ classes in the classification task, and (2) each class contains $K$ different audio examples. Following previous works~\citep{liang_adapting_2023} in this task, we tested APT-LLM in the 5/12-way 5-shots settings. In our free-form query design, we prompt the few shot classification question by adding the query audio clip together with the input text ``\textit{This is a sound of}'' to the end of the sequence of labelled audio examples and their corresponding label texts. We implemented the same evaluation protocol to all few-shot learners for a fair comparison. As shown in Table~\ref{tab.:few_shot}, APT-LLM outperforms the task-specific models~\citep{snell_prototypical_2017,vinyals_matching_2016,liang_leveraging_2022} in the 5-way 5-shot setting while having a competitive performance compared to CLAP Adapters~\citep{zhang_tip-adapter_2022,liang_adapting_2023}. In the 12-way 5-shot problem, however, we can observe a performance degradation of APT. We suspect this may be due to the limitation of attention mechanism in LLMs when addressing very long sequences (12-way 5-shot modelling results in a sequence of roughly 2420 tokens). It should be noted that while APT-LLM was trained with 4-way 1-shot tasks, it can generalise to other few-shot settings, suggesting that APT-LLM learns to act as a few-shot classifier rather than memorising the expected answers.

\subsection{Evaluation on natural language audio reasoning} \label{subsec:evaluation_on_nlar}
\begin{wraptable}{r}{6.3cm}
    \caption{Benchmarking APT on the natural language audio reasoning task.}
    \label{tab.:audio_reasoning}
    \begin{tabular}{lc}
    \toprule
    Model               & Accuracy$\uparrow$ (\%)         \\ \hline
    the baseline        & 29.9              \\
    APT-Vicuna v1.1 & 62.9              \\
    APT-Vicuna v1.5 & \textbf{63.8}     \\ \bottomrule
    \end{tabular}
\end{wraptable}

Since APT-LLM is able to ingest multiple audio clips in a single feed-forward process, we investigated APT-LLM with natural language audio reasoning for which a model is expected to distinguish, compare, and summarise two audio clips (see Appendix~\ref{appendix:nlar}). To the best of the knowledge, there is no previous work evaluating model ability to comprehend more than one recording. We thus contrast APT-LLM to the baseline where predictions are fixed to a specific answer (we used ``yes'' as the fixed answer after several attempts). Table~\ref{tab.:audio_reasoning} demonstrates that APT-Vicuna v1.5 achieves 63.78\% mAP score, outperforming the baseline by a large margin. This result suggests that APT-LLM is able to not only comprehend the content in an audio clip but also analyse more than one audio recordings by comparison and summarisation. It is worth noting that there is marginal improvement when upgrading Vicuna from v1.1 to v1.5, indicating the performance of language models is not the bottleneck in this task, at least for the two used in our study.

\subsection{Evaluation on zero-shot audio-visual tasks} \label{subsec:evaluation_on_audio-visual_tasks}

\begin{wraptable}{r}{7.5cm}
    \caption{Performance comparison between different modalities in audio-visual learning.}
    \label{tab.:av_learning}
    \begin{tabular}{@{}lcc@{}}
    \toprule
    Model                         & Modal       & Accuracy$\uparrow$         \\ \midrule
    BLIP-2~\citep{li_blip-2_2023} & Video-only  & 42.9             \\
    APT-LLM             & Audio-only  & 27.7                  \\
    APT-BLIP-2                  & Audio-video & \textbf{59.7}    \\ \bottomrule
    \end{tabular}
\end{wraptable}

APT was also experimented as an audio adapter for an existing VLM, BLIP-2~\citep{li_blip-2_2023}. BLIP-2 consists of a frozen image encoder, a Qformer, a projection layer, and a frozen Vicuna v1.1. Therefore, we integrated the APT trained with the same language model to BLIP-2 by interleaving acoustic prompts with text embeddings. We refer to the APT-enhanced BLIP-2 as APT-BLIP-2. Of note, although we selected BLIP-2 as our backbone model, APT can be easily adapted to another language model. APT-BLIP-2, together with other multimodal language models, was investigated on a audio-visual question and answering dataset where models are expected to choose one out of four options by using both audio and video modalities. We experimented APT-BLIP-2 on the subset of the AVQA dataset~\citep{yang_avqa_2022} as many video links associated with the AVQA test segmentation were no longer available on the internet at the time of the experiment. As shown in Table~\ref{tab.:av_learning}, APT-BLIP-2 yielded a better performance than video-only and audio-only counterparts, indicating the adaptation to the audio domain benefits models' learning from the content of video. 

\subsection{Limitations} \label{subsec:limitations}
In this work, we devised APT to align acoustic embeddings with text embeddings of language models. Now that the word embeddings change when switching to a different language model, even if their architectures remain the same, each language model calls for a dedicated APT for adaptation. In addition, APT-LLM was not trained with instruction-based datasets, and thus, has limited ability to response to questions excluded from the training set. Finally, we purposely focused APT-LLM training and experimentation on general-purpose audio understanding tasks, therefore, unlikely it can understand speech and music audios.

\section{Conclusions} \label{sec:conclusions}
We proposed APT, a general-purpose acoustic adapter that extends LLM/VLM to the audio domain. We showed that LLM coupled with APT is a multi-task audio learner that not only achieved a competitive performance across various audio understanding tasks but also be capable of in-context learning when fed with a few labelled examples. We also benchmarked APT-LLM's audio comprehension ability via the natural language audio reasoning task, a new task that requires a model to distinguish, compare, and summarise two different audio clips. Last but not least, it is evident from our study on audio-visual learning that encoding sound clips as word tokens is an efficient approach to adapt LLM/VLM to the audio domain. Future works can extend audio language models into comprehension of music and speech audio, and make them more aligned with human perception via instruction tuning. It is also interesting to investigate how audio language models handle errors in the in-context learning.

\newpage

\end{document}