\title{Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization}

\begin{document}

\maketitle

\begin{abstract}
Recently, the remarkable advance of the Large Language Model (LLM) has inspired researchers to transfer its extraordinary reasoning capability to both vision and language data. However, the prevailing approaches primarily regard the visual input as a prompt and focus exclusively on optimizing the text generation process conditioned upon vision content by a frozen LLM. Such an inequitable treatment of vision and language heavily constrains the model's potential. In this paper, we break through this limitation by representing both vision and language in a unified form. Specifically, we introduce a well-designed visual tokenizer to translate the non-linguistic image into a sequence of discrete tokens like a foreign language that LLM can read. The resulting visual tokens encompass high-level semantics worthy of a word and also support dynamic sequence length varying from the image. Coped with this tokenizer, the presented foundation model called \textbf{LaVIT} can handle both image and text indiscriminately under the same generative learning paradigm. This unification empowers LaVIT to serve as an impressive generalist interface to understand and generate multi-modal content simultaneously. Extensive experiments further showcase that it outperforms the existing models by a large margin on massive vision-language tasks. Our code and models are available at \url{https://github.com/jy0205/LaVIT}.
\end{abstract}

\section{Introduction}
The large language models (LLMs)~\citep{brown2020language,touvron2023llama} nowadays have demonstrated impressive advances in various linguistic applications. Profiting from the knowledge in the massive text corpus, they possess exceptional understanding capabilities and serve as a general-purpose interface to complete a wide range of real-world tasks. Such success has motivated researchers to investigate the Multi-modal Large Language Models (MLLMs), which aim at extending the powerful pure-text LLMs to process multi-modality inputs. As shown in Figure~\ref{fig:fig0}-(a), the prevalent approaches mostly leverage an adapter architecture (e.g., the Resampler~\citep{alayrac2022flamingo}, linear projection~\citep{liu2023visual}, or Q-Former~\citep{li2023blip}) to map the visual features encoded by a pre-trained vision backbone~\citep{radford2021learning} to the semantic space of LLM. 

Despite achieving preliminary results in zero-shot multi-modal understanding, they still suffer from inherent design deficiencies. The training objective of prior methodologies~\citep{li2023blip,huang2023language,zhu2023minigpt} is centered on predicting textual descriptions dependent on visual content, where the visual parts are merely regarded as prompts without any supervision. The inequitable treatment of different modal inputs severely constrains the model's potential, limiting them to only performing comprehension tasks like generating text based on images. Moreover, most of these methods completely delegate the responsibility of vision-language alignment to the newly added adapter with limited trainable parameters, which fails to leverage the remarkable reasoning capabilities of LLM to learn the interaction across different modalities. Although the recent concurrent work Emu~\citep{sun2023generative} proposes to unlock the text-pretrained LLM by regressing the next visual embedding during pre-training (Figure~\ref{fig:fig0}-(b)), the inconsistent optimization objectives for image and text are not conducive to unified multi-modal modeling.

In this work, we introduce \textbf{LaVIT} (\textbf{La}nguage-\textbf{VI}sion \textbf{T}ransformer), a novel general-purpose multi-modal foundation model that inherits the successful learning paradigm of LLM: predicting the next image/text token in an auto-regressive manner. Our insight is that by employing a unified objective to indiscriminately treat tokens from different modalities, the model can seamlessly achieve ``any-to-any'' multi-modal comprehension and generation. However, the original LLM is specifically crafted to process discrete textual tokens. When dealing with physical signal inputs, such as images, it becomes imperative to embrace a representation seamlessly compatible with text tokens. Therefore, we propose to translate the image into a sequence of tokens like a foreign language that LLM can comprehend, so that both images and texts can be handled simultaneously under the unified generative objective without any specific architectural modification, as shown in Figure~\ref{fig:fig0}-(c).

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.93\linewidth]{figure/fig0_v3.pdf}
\end{center}
\vspace{-0.1in}
\caption{\small The comparisons between different MLLMs. (a) The adapter-style methods rely on an adapter network to project visual features into the semantic space of LLM. During training, visual tokens are merely treated as the prompt to guide text generation. (b) The concurrent work Emu adopts the regression loss for visual features and jointly trains with textual tokens. (c) We craft a visual tokenizer to represent images in the same discrete format as text so as to indiscriminately optimize them under a unified generative objective.}
\label{fig:fig0}
\vspace{-0.2in}
\end{figure}

To achieve this goal, a crucial element lies in the development of an efficient visual tokenizer for encoding images, which we contend should adhere to the following principles: (i) \textbf{discrete visual token}: While language models rely on text tokens defined by a dictionary, prior visual tokens, like those derived from ViT, consist of continuous feature vectors encoding a patch. In approaches such as masked image modeling~\citep{HeCXLDG22} or masked feature prediction~\citep{00050XWYF22}, regressive objectives on continuous features or raw visual pixels are employed for self-supervised pretraining. Here, we advocate for quantizing the visual tokens into a discrete form, aligning them with the next-token prediction objective in language models. This form is particularly advantageous when the target distribution for the next token is multi-mode. (ii) \textbf{dynamic token allocation}. Given the varying semantic complexity of different images, employing a fixed length of tokens to encode all images is compute-uneconomical. Moreover, as a key difference from textual tokens, visual patches exhibit a notable interdependence, making it considerably more straightforward to deduce one token from others. This renders the next-token paradigm less effective in learning visual knowledge through self-supervision. Thus we argue for the token-merging to ensure the least redundancy among visual patches, thereby rendering a dynamic token number for different images.

Following the aforementioned two crucial fundamentals, LaVIT introduces a novel dynamic visual tokenization mechanism consisting of a selector and a merger to process images. The token selector first decides which visual patches carry informative semantics and are necessary to be selected to encode the whole image. In order to maximally preserve the image details, the token merger further compresses the unselected patches onto the retained ones according to their feature similarity. Such a design enables each retained visual token to contain high-level semantics from multiple similar patches and thus reduce the redundancy among tokens. This selecting and merging strategy will produce a dynamic sequence length varying from the image content itself. The retained visual tokens are further quantized into discrete codes by a learnable codebook~\citep{esser2021taming}, which will serve as the supervision signals for visual tokens during pre-training. Empowered by this visual tokenizer, our LaVIT can be trained with a simple yet unified objective: predicting the next image/text token in the multi-modal sequence. After pre-training, LaVIT can serve as a multi-modal generalist to perform both multi-modal comprehension and generation without further fine-tuning (See Figure~\ref{fig:fig1}). The key contributions of this work are summarized as: 

\begin{itemize}[leftmargin=*]
\item We introduce LaVIT, a new effective, general-purpose multi-modal foundation model that goes beyond the traditional adapter-based architectures. By transforming images into a sequence of discrete tokens like a foreign language that LLM can comprehend and generate, both modalities can be associated indiscriminately under a unified generative training paradigm. 

\item The developed visual tokenizer can produce discrete visual tokens with dynamic length to reduce the interdependence among visual patches, which enhances the representation compatibility of image and text in LLM and improves computational efficiency. 

\item Our LaVIT showcases the extraordinary multi-modal understanding and generation potential. It can take any modality combinations as input and perform impressive in-context generation of both images and text. As demonstrated by extensive experiments, LaVIT achieves state-of-the-art zero-shot performance on a wide range of vision-language tasks.

\end{itemize}

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.95\linewidth]{figure/fig1v3.pdf}
\end{center}
\vspace{-0.1in}
\caption{Given an image-text pair, the image is tokenized into discrete tokens and concatenated with text tokens to form a multi-modal sequence. Then, LaVIT is optimized under a unified generative objective. After training, it can achieve both zero-shot multi-modal comprehension and generation.}
\label{fig:fig1}
\vspace{-0.15in}
\end{figure}

\vspace{-0.1in}

\section{Related Work}

\noindent \textbf{Vision-Language Pre-training.}
Researchers have extensively investigated vision-language pretraining (VLP). The pioneer works~\citep{radford2021learning,jia2021scaling} primarily employ dual-encoder with contrastive objectives to learn the generic cross-modal aligned representations. Recently, the rapid progress of large language models~\citep{chowdhery2022palm,touvron2023llama} has motivated researchers to delve into the exploration of augmenting LLM towards vision language tasks. The majority of these works adopt an adapter-style network~\citep{zhang2023llama} that serves as an intermediate bridge connecting the pre-trained vision encoder and frozen language model. For instance, Flamingo~\citep{alayrac2022flamingo} develops a Perceiver Resampler to generate text-aligned visual representations. Follow-up methods~\citep{li2023blip,zhu2023minigpt} mainly adopt the Q-Former to project the visual semantics to the LLM's input space. However, visual inputs in these methods~\citep{huang2023language,alayrac2022flamingo} are only considered as the prompt and not involved in the optimization, which heavily restricts the model potential.

\noindent \textbf{Vector Quantization in Computer Vision.}
Vector quantization~\citep{gray1984vector,nasrabadi1988image} is widely used in image-generative models. The VQ-VAE~\citep{van2017neural} and DALL-E~\citep{ramesh2021zero} proposed to convert an image into a set of discrete codes in a learnable discrete latent space by learning to reconstruct the original image pixels. Models like VQGAN~\citep{esser2021taming} and ViT-VQGAN~\citep{yu2021vector} leverage adversarial and perceptual objectives to further enhance the image generation quality. The BEIT series of works also adopts the quantized visual codes as the supervision in mask image modeling~\citep{peng2022beit,wang2023image}. However, most of these methods tokenize the image into a token sequence with a fixed length (e.g., 512 or 1024). Such a long sequence will invariably result in an excessive computational burden. On the contrary, our proposed visual tokenizer reduces the redundancy among image patches and supports dynamic token length, thus enabling efficient multi-modal inference.

\section{Method}
This work proposes to leverage the extraordinary reasoning potential of the large language model to facilitate the modeling of both vision and language modalities. In pursuit of this goal, the key component is to represent these two modalities in a uniform form, so as to exploit LLM's successful learning recipe (i.e., next-token prediction). As shown in Figure~\ref{fig:fig1}, we develop a visual tokenizer (Section~\ref{sec:tokenizer}) to convert the non-linguistic image to the input that LLMs can comprehend. It receives the vision features from a pre-trained vision encoder and outputs a sequence of discrete visual tokens possessing word-like high-level semantics. Coped with the crafted tokenizer, the visual input can be integrated with textual tokens to compose a multi-modal sequence, which is subsequently fed into large language models under a unified auto-regressive training objective (Section~\ref{sec:mllm}). 

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.95\linewidth]{figure/fig2.pdf}
\end{center}
  \caption{(a) The pipeline of the proposed dynamic visual tokenizer. It employs a token selector to select the most informative patches and a token merger to compress the information of discarded patches onto the retained ones. The whole tokenizer is trained by maximally reconstructing the semantics of the input image. (b) The detailed architecture of token merger.}
\label{fig:fig2}
\vspace{-0.1in}
\end{figure}

\subsection{Stage-1: Dynamic Visual Tokenizer}
\label{sec:tokenizer}
Given an image $x \in \mathcal{R}^{H\times W \times C}$, it is first partitioned into $N=HW/P^2$ non-overlapping patches, where $P$ is the patch size. These patches are fed into a pre-trained ViT encoder~\citep{fang2023eva} to produce a sequence of the patch features $X=\{ x_1, ..., x_N \}$. Then, a straightforward way to encode images is to directly quantize the $N$ patch-level embeddings into discrete tokens as the input to LLMs. This will result in a long visual sequence and bring superfluous computational burden since many visual patches may contain repetitive and trivial background semantics. These redundant patches demonstrate a discernible interdependence, thereby diminishing the efficacy of the next-token paradigm in learning visual knowledge via self-supervision. Consequently, the proposed tokenizer aims to produce visual tokens with a dynamic length according to the complexity of the image content itself. As illustrated in Figure~\ref{fig:fig2}, it comprises a token selector and a token merger. 

\paragraph{Token Selector} The token selector takes the $N$ patch-level features $X$ as input. It aims to estimate the importance of each image patch and selects the most informative ones that are competent enough to represent the semantics of the whole image. Inspired by~\citep{rao2021dynamicvit}, it is implemented as a lightweight module consisting of several MLP layers to predict a distribution $\pi \in \mathcal{R}^{N\times2}$, where $\pi_{i}=\text{MLP}(x_i)$. By sampling from the distribution $\pi$, a binary decision mask $M \in \{0, 1\}^N$ can be generated, which indicates whether to remain the corresponding image patch. To relax the sampling to be differentiable, the Gumbel-Softmax trick~\citep{maddison2016gumbel} is applied to $\pi$:
\begin{equation}
    \hat{\pi_{i,j}} = \frac{\exp ((\log \pi_{i,j} + G_{i,j}) / \tau)}{\sum_{r=1}^{2} \exp ((\log\pi_{i,r} + G_{i,r}) / \tau) }.
\end{equation}
where $G_{i,j}$ is the noise sampled from a Gumbel distribution, $\tau$ is the temperature to control smoothness. Then, the binary decision mask $M$ can be sampled from $\hat{\pi}$ for end-to-end training. 

\paragraph{Token Merger} According to the generated decision mask, total $N$ image patches can be partitioned into retained and dropped groups, with $T$ and $N-T$ patches respectively, denoted as $X_{r}=\{x_i\}_{i=1}^{T}$ and $X_{d}=\{x_j\}_{j=1}^{N-T}$. Instead of directly discarding $X_{d}$, we develop a token merger to deal with it to maximally preserve the detailed semantics of the input image. As shown in the right of Figure~\ref{fig:fig2}, the token merger will progressively compress the information of discarded $X_{d}$ onto the retained $X_{r}$ according to their semantic similarity. Concretely, it consists of $L$ stacked blocks, each of which has a causal self-attention layer, a cross-attention layer, and a feed-forward layer. In the causal self-attention layer, each token in $X_{r}$ attends to its previous tokens with a causal mask. This helps to convert 2D raster-ordered features from the ViT encoder into a sequence with causal dependency, thus ensuring consistency with textual tokens in LLMs. We found this strategy can result in better performance than bi-directional self-attention. The cross-attention layer treats the retained tokens $X_{r}$ as the query and merges tokens in $X_{d}$ based on their similarity in the embedding space. Formally, this layer calculates an update of $X_r$ by:
\begin{equation}
    \Delta X_r = \text{softmax}\left({QK^\top}/{\sqrt{D}}\right)V,
\end{equation}
where $D$ denotes the dimension of hidden state, $Q=W_{Q}X_r \in \mathcal{R}^{T \times D}$, $K=W_{K}X_{d} \in \mathcal{R}^{(N-T) \times D}$ and $V=W_{V}X_{d} \in \mathcal{R}^{(N-T) \times D}$. To parallelize the computation, we adopt the predicted decision mask $M$ to control the cross-attention scope between tokens without directly partitioning them into two groups. After $L$ successive token merger blocks, we can obtain the final merged visual tokens $\hat{X_r}=\{ \hat{x_i} \}_{i=1}^{T}$. Each token implicitly encodes high-level semantics from several image patches possessing similar visual patterns, which we refer to as visual part features $\hat{X_r}$. 
The token selector and merger work together to dynamically adjust the visual token sequence length to accommodate images with different content complexity.

\vspace{-0.1in}

\paragraph{Vector Quantization and Training} The generated visual part features $\hat{X_r}$ are then passed into a quantizer. It tokenizes $\hat{X_r}$ to a sequence of discrete visual codes $V=\{v_i\}_{i=1}^T$ by looking up a learnable codebook $\mathcal{C}=\{c_k\}_{k=1}^{K}$, where $K$ is codebook size. To be specific, the $i_{th}$ visual code is calculated by assigning $\hat{x_i}$ in $\hat{X_r}$ to its closest neighbourhood code in $\mathcal{C}$: 
\begin{equation}
    v_i = \arg \min_{j} \| l_{2}(\hat{x_i}) - l_{2}(c_j)\|_2, \quad v_i \in [0, K-1],
\end{equation}
where $l_2$ indicates the $L_2$ norm. Based on the indexing visual codes, we can obtain the quantized embeddings $\{c_{v_i}\}_{i=1}^T$, which is fed into a decoder to reconstruct the original visual semantic features $X=\{x_i\}_{i=1}^{N}$. The insight behind this design is that the reconstruction quality of the image semantics depends on selecting the most informative patches (token selector), along with maximally preserving the visual details only through the remained tokens (token merger). Thus, both token selector and merger can be effectively updated by encouraging a higher semantic reconstruction quality. The final training objective of the visual tokenizer is defined as:
\begin{equation}
    \mathcal{L}_{\text{tokenizer}} = \frac{1}{N} \sum_{i=1}^{N} \left( 1 - \cos (x_i, x_i^{\text{rec}}) \right)+ \lambda (\rho - \frac{1}{N} \sum_{i=1}^{N} M_i)^2,
\end{equation}
where $\cos (x_i, x_i^{\text{rec}})$ calculates the cosine similarity between the reconstructed and real visual embeddings, $\rho$ is a pre-defined rate that controls the target mean percentage of the retained visual tokens and $\lambda$ is set to be $2$. Finally, the tokenized discrete codes $\{v_i\}_{i=1}^T$ will serve as the supervision signals for visual tokens in the following pre-training.

\vspace{-0.1in}

\paragraph{Decoding to Pixels} The proposed visual tokenizer is capable of reconstructing visual features of input images that contain high-level semantics to represent the image content but lose the pixel-level details. To recover the original pixel space, we employ a conditional de-noising U-Net $\epsilon_\theta$~\citep{rombach2022high} to infill the visual details after training the visual tokenizer. Specifically, it takes the reconstructed $x_{\text{rec}}$ as the condition to progressively recover image $x$ from a Gaussian noise. Following~\citet{rombach2022high}, the parameters $\theta$ of this U-Net are optimized by $\epsilon$ prediction:
\begin{equation}
    \mathcal{L}_{\theta} = \mathbb{E}_{z, \epsilon \sim \mathcal{N}(0,1), t} \left[ ||\epsilon - \epsilon_{\theta}(z_t, t, x_{\text{rec}}) || \right],
\end{equation}
where $z_t$ is the latent state of image $x$ in the diffusion process. We present some pixel decoding results by the trained denoising U-Net in Figure~\ref{fig:supp_fig_decode} of the appendix. During inference, the generated visual tokens from LaVIT can be decoded into realistic images by this U-Net.

\subsection{Stage-2: Unified Generative Modeling}
\label{sec:mllm}
Given an image-text pair, the 2D image can be tokenized into a 1D sequence with causal dependency and then concatenated with text tokens to form a multi-modal sequence $y=(y_1, y_2, .., y_S)$. For differentiating between two modalities, two special tokens [IMG] and [/IMG] are inserted into the beginning and end of the image token sequence respectively, indicating the start and end of image content. To empower LaVIT with the capability to generate both text and images, we employ two different concatenation forms, i.e., $[\text{image}, \text{text}]$ and $[\text{text};\text{image}]$. When the image is used as a condition (on the left) to generate text, we use the continuous visual features $\hat{X_r}$ from the token merger instead of quantized visual embeddings as the input to LLMs. Such a design mitigates the loss of detailed information caused by vector quantization, which is crucial for fine-grained multi-modal understanding tasks like visual question answering. Our \textbf{LaVIT} adopts the general Language Modeling (LM) objective to directly maximize the likelihood of each multi-modal sequence in an auto-regressive manner:
\begin{equation}
    p(y) = \sum_{y \in \mathcal{D}} \sum_{i=1}^{S} \log P_\theta(y_i | y_{< i}).
\end{equation}
Since both image and text are already represented as discrete token IDs, we can use the cross-entropy to supervise the token prediction at each location for both two modalities with a shared prediction head. The complete unification in representation spaces and the training paradigms can help LLMs better learn multi-modal interaction and alignment. When LaVIT is pre-trained, it possesses the capacity to perceive images akin to a foreign language, comprehending and producing them like text. Nevertheless, most of the existing works merely regard images as prompts to guide the generation of text with no supervision, restricting them to solely performing image-to-text tasks.

\vspace{-0.05in}

\subsection{Model Pre-training}
The LaVIT undergoes a two-stage pre-training procedure on web-scale multi-modal corpora. 

\textbf{Stage-1: Tokenizer Training}. Following the existing MLLMs, the ViT-G/14 of EVA-CLIP~\citep{fang2023eva} is employed as the visual encoder. The visual codebook size is empirically set to $K=16384$.  We adopt $L=12$ transformer blocks for both token merger and decoder in our tokenizer. During training, this encoder is kept frozen and only the parameters of the selector, merger, and codebook are updated. It is trained for 50K steps on about 100M images from LAION-400M~\citep{schuhmann2021laion} with the batch size of $2048$ and $\rho=1/3$. After training the tokenizer, the conditional U-Net for pixel decoding is initialized from the Stable Diffusion v1.5~\citep{rombach2022high} and finetuned 20k steps on the same dataset. The whole stage-1 training only requires pure image data without corresponding captions.

\textbf{Stage-2: Unified Vision-Language Pre-training}. 
Based on the trained visual tokenizer, all the images can be tokenized into discrete codes that are amenable to the next token prediction. We utilize the raw 7B version of LLaMA~\citep{touvron2023llama} as the default LLM. For image-to-text comprehension (i.e., $[\text{image}, \text{text}]$), we employ about 93M samples from Conceptual Caption~\citep{sharma2018conceptual,changpinyo2021conceptual}, SBU~\citep{ordonez2011im2text}, and BLIP-Capfilt~\citep{li2022blip}. For the text-to-image synthesis (i.e., $[\text{text}, \text{image}]$), an additional 100M image-text pairs from the LAION-Aesthetics (A high-aesthetics image subset of LAION-5B~\citep{schuhmann2022laion}) are used following Stable Diffusion. Moreover, to reduce catastrophic forgetting of the reasoning capacity in training LLM, we employ the English text corpus from Redpajama~\citep{together2023redpajama} dataset and mix it with the above image-text pairs to form the multi-modal input sequence. 

\vspace{-0.1in}

\section{Experiments}
\label{sec:exp}
In this section, comprehensive experiments are conducted to systematically validate the effectiveness of LaVIT on a wide range of vision-language tasks. Specifically, we mainly evaluate the model's zero-shot multi-modal understanding and generation capacity. 

\begin{table*}[t]
    \centering
    \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{lcccccc}
    \toprule
    \multirow{2}{*}{Method}  & \multicolumn{2}{c}{Image Captioning} & \multicolumn{4}{c}{Visual Question Answering} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-7} &  Nocaps & Flickr & VQAv2 & OKVQA & GQA & VizWiz \\
    \midrule
    Flamingo-3B~\citep{alayrac2022flamingo}  &  - &   60.6 & 49.2  &  41.2 & - &  28.9  \\
    Flamingo-9B~\citep{alayrac2022flamingo}   & -  &   61.5 & 51.8  &  44.7 & - &   28.8  \\
    OpenFlamingo-9B~\citep{awadalla2023openflamingo}   & -  & 59.5  & 52.7  & 37.8 & - & 27.5 \\
    MetaLM~\citep{hao2022language}   & -  & 43.4  &  41.1 & 11.4 & - & -  \\
    Kosmos-1~\citep{huang2023language}   & -  & 67.1  &  51.0 & - & - & 29.2 \\
    Kosmos-2~\citep{peng2023kosmos}   & -  & 80.5  &  51.1 & - & - & - \\
    BLIP-2 (Vicuna-7B)~\citep{li2023blip} &  107.5 & 74.9  & -  & - & 41.3 & 25.3  \\
    BLIP-2 (Vicuna-13B)~\citep{li2023blip}  & 103.9  & 71.6  &  - & - & 32.3 &  19.6  \\
    CM3Leon-7B~\citep{yu2023scaling} &  - & -  &  47.6 & - & - &  37.6 \\
    Emu (LLaMA-13B)~\citep{sun2023generative}   &  - &  - & 52.0  & 38.2 & - & 34.2 \\
    Ours (LLaMA-7B) &  \textbf{114.2} &  \textbf{83.0}  & \textbf{66.0}  & \textbf{54.6} & \textbf{46.8} & \textbf{38.5}  \\
    \bottomrule
    \end{tabular}
    }
    \caption{Overview of zero-shot evaluation on multi-modal understanding tasks. Compared with previous methods, our LaVIT achieved the best performance on both benchmarks. }
    \label{tab:zero_shot}
    \vspace{-0.2in}
\end{table*}

\subsection{Zero-Shot Multimodal Understanding}
We first quantitatively evaluate the zero-shot multi-modal understanding capacity of LaVIT on Image Captioning (NoCaps~\citep{agrawal2019nocaps}, Flickr30k~\citep{plummer2015flickr30k})and Visual Question Answering (VQAv2~\citep{goyal2017making}, OKVQA~\citep{marino2019ok}, GQA~\citep{hudson2019gqa}, VizWiz~\citep{gurari2018vizwiz}). For visual question answering tasks, we use a simple prompt: ``Question: \{\} Answer: \{\}''. The widely-used CIDEr score and VQA accuracy are employed as metrics to evaluate captioning and question answering, respectively.
\begin{wraptable}{r}{8.5cm}
\centering
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{lcc}
    \toprule  
    Method & Model Type & FID($\downarrow$) \\
    \midrule
    \textbf{\textit{Text2Image Specialist:}}  &  &   \\
    DALL-E~\citep{ramesh2021zero} & Autoregressive & 28.0\\
    CogView~\citep{ding2021cogview} & Autoregressive & 27.1 \\
    SD~\citep{rombach2022high} & Diffusion & 12.6 \\
    GLIDE~\citep{nichol2021glide} & Diffusion & 12.2 \\
    DALL-E2~\citep{ramesh2022hierarchical}  & Diffusion & 10.4 \\
    Make-A-Scene~\citep{gafni2022make} & Autoregressive & 11.8 \\
    MUSE-7.6B~\citep{chang2023muse}  & Non-Autoregressive & 7.9 \\
    Imagen-3.4B~\citep{saharia2022photorealistic} & Diffusion & 7.3 \\
    Parti-20B~\citep{yu2022scaling} & Autoregressive & \textbf{7.2} \\
    \midrule
    \textbf{\textit{Multimodal Large Langauge Model:}}  &  &   \\
    GILL (OPT-6.7B)~\citep{koh2023generating}  & LLM & 12.2 \\
    Emu (LLaMA-13B)~\citep{sun2023generative} & LLM & 11.7 \\
    CM3Leon-7B~\citep{yu2023scaling} & LLM & 10.8 \\
    Ours (LLaMA-7B) & LLM & \textbf{7.4} \\ 
    \bottomrule
\end{tabular}
}
\caption{\small The zero-shot text-to-image generation performance of different models on MS-COCO-30K evaluation benchmark.}
\label{wrap-tab:1}
\vspace{-0.1in}
\end{wraptable}
The detailed performance comparisons are shown in Table~\ref{tab:zero_shot}. As observed, LaVIT surpasses all the existing MLLMs by a large margin on these understanding tasks. For example, it achieves a CIDEr score of 83.0 on the Flickr30k test dataset, compared to 61.5 and 74.9 for the Flamingo-9B and BLIP-2 (Vicuna-7B) under the same scale of model size, respectively. The performance superiority on OKVQA (54.6\% v.s. 44.7\% of Flamingo-9B) further showcases the excellent multi-modal understanding capacity of LaVIT, since this benchmark contains questions requiring commonsense knowledge and reasoning about the content of images. It is worth noting that, although the concurrent method Emu~\citep{sun2023generative} also leverages the LLM to jointly model the vision and language, the direct feature regression objective for visual inputs makes it incompatible with text input. Therefore, despite using more training data (2.6B image-text pairs and 3.8B web-crawled data) and larger LLM (LLaMA 13B), it still achieves inferior performance to ours on all evaluation benchmarks.

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.83\linewidth]{figure/fig_sample.pdf}
\vspace{-0.1in}
\end{center}
  \caption{The qualitative examples of multi-modal image synthesis.}
\label{fig:fig4}
\vspace{-0.25in}
\end{figure}

\subsection{Zero-Shot Multimodal Generation}
Since the proposed visual tokenizer can represent images as discrete tokens, LaVIT possesses the capability to synthesize images by auto-regressively generating visual tokens like text. We first quantitatively evaluate the model's zero-shot text-conditional image synthesis performance on the validation set of the MS-COCO benchmark~\citep{lin2014microsoft}. The detailed image generation procedure is presented in Appendix~\ref{sec:supp_imagen}. Following the standard setting like previous text-to-image synthesis works, we randomly sample 30k text prompts and calculate the zero-shot FID metric between real images and generated ones. The detailed comparative results are shown in Table~\ref{wrap-tab:1}. It can be seen that LaVIT outperforms all the other multi-modal language models. Compared with the concurrent work Emu, it makes a 4.3 FID improvement with a smaller LLM model, demonstrating excellent vision-language alignment capability. In addition, LaVIT achieves comparable performance with state-of-the-art text2image specialists Parti~\citep{yu2022scaling}, while only using much fewer training data (e.g., 0.2B v.s. 2B training image-text pairs compared to Parti). 

\vspace{-0.1in}

\paragraph{Generation via Multi-modal Prompt} LaVIT can seamlessly accept several modality combinations (\emph{e.g.}, text, image+text, image+image) as prompts to generate corresponding images without any fine-tuning. Figure~\ref{fig:fig4} showcases some examples of the multi-modal image generation results. Our LaVIT can produce high-quality images that precisely reflect the style and semantics of the given multi-modal prompts, which demonstrates the strong multi-modal modeling potential of LaVIT. More interestingly, it can modify the original input image by the input multi-modal prompt (\emph{e.g.}, in the last example two prompt images with a dog or cat generate a dog's portrait with the cat's whisker). This capability cannot be attained by conventional image generation models like Stable Diffusion in the absence of additional fine-tuned downstream data~\citep{ruiz2023dreambooth}. 

\subsection{Ablation Study}
In this study, we investigate the impact of various component designs in LaVIT on downstream performance. All the ablations were conducted on part of pre-training data by using the clip ViT-L/14~\citep{jia2021scaling} as the visual encoder due to the costly training resources.

\textbf{Token Classification or Feature Regression?} 
When jointly training vision and language via generative training in text-oriented LLM, it is crucial to select the appropriate optimization objectives for the 2D raster-ordered visual input. When quantizing the continuous visual tokens into the discrete form, it is convenient to use the cross-entropy loss for supervising the next visual token prediction akin to textual tokens. We conjecture that such a uniform objective for both vision and language contributes to aligning them together in the LLM. To validate the superiority of the proposed visual quantization, we change the optimization objective of visual tokens to regressing the next visual embeddings by employing a regression head like Emu~\citep{sun2023generative}. Table~\ref{tab:ab1} summarizes the results of different training objectives. As observed, adopting the regression loss for the next visual token prediction will severely degrade the model performance.

\textbf{Dynamic or Fixed Token Length}. Given the extracted visual features, a straightforward way is to tokenize all the patch embeddings into the visual tokens, which results in a fixed token length (i.e., 256). We compare the impact of fixed and dynamic tokenization strategies in terms of training time, computation overhead, and zero-shot performance on multi-modal understanding. As shown in Table~\ref{tab:ab2}, the dynamic visual tokenizer achieves superior performance while only requiring 94 tokens on average for the input images, about 36\% of the fixed one. Given that the attention computation in LLM exhibits a quadratic relationship with respect to the token length, this sparsification can accelerate the training time by 40\% and reduce the computational cost in inference.

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.85\linewidth]{figure/fig3.pdf}
\vspace{-0.1in}
\end{center}
  \caption{\small The visualization for the dynamic visual tokenizer (left) and learned codebook (right). Our tokenizer can dynamically select the most informative patches based on the image content and the learned codebook can produce visual codes with high-level semantics.}
\label{fig:fig3}
\vspace{-0.2in}
\end{figure}

\begin{table}[h]
    \begin{subtable}[h]{0.4\textwidth}
        \centering
        \scalebox{0.85}{
        \begin{tabular}{c|ccc}
        \toprule
         Setting & Flickr & VQAv2 &  OKVQA \\
        \midrule
           Regression  & 60.4    & 53.6 &   41.9 \\
           Classification & \textbf{73.2}   & \textbf{57.1}  & \textbf{47.0}  \\
        \bottomrule
        \end{tabular}}
       \caption{Ablations of different training objectives.}
       \label{tab:ab1}
    \end{subtable}
    \hfill
    \begin{subtable}[h]{0.6\textwidth}
        \centering
        \scalebox{0.85}{
        \begin{tabular}{c|cc|ccc}
        \toprule
         Setting & Num & Time  & Flickr &   VQAv2 & OKVQA  \\
        \midrule
         Fixed  & 256 & 30h &  71.1  & 56.5 & 46.4  \\
         Dynamic  & 94 & 18h &  \textbf{74.0}  & \textbf{57.7} & \textbf{47.6}  \\
        \bottomrule
        \end{tabular}}
        \caption{\small Ablations for the effect of different tokenization strategies.}
        \label{tab:ab2}
     \end{subtable}
     \caption{The ablations of different optimization objectives for visual tokens and tokenization strategies. The num and time in Table~\ref{tab:ab2} indicate the mean visual token number and pre-training time.}
     \label{tab:ablation1}
\vspace{-0.1in}
\end{table}

\subsection{Qualitative Analysis}
We visualize some examples processed by the proposed dynamic tokenizer. As shown in Figure~\ref{fig:fig3}, the token selector is capable of dynamically selecting the most informative image patches that are competent enough to represent the semantics of the whole image. Visual patches that contain repetitive or trivial background semantics are filtered during this procedure, thereby reducing redundant information and improving computing efficiency. We also visualize the image patches that belong to the same visual code in Figure~\ref{fig:fig3}. As observed, the learned discrete codes can convey explicit visual semantics and group the image patches with similar patterns together. For instance, code $4107$ represents a part of a skateboard, and code $9146$ indicates the texture of a giraffe, which strongly demonstrates the interpretability of the learned codebook. 

\section{Conclusion}
This paper presents the LaVIT, a new general-purpose foundation model that is capable of simultaneously performing multi-modal understanding and generation. Beyond the previous adapter-based methods, it inherits the successful auto-regressive generative learning paradigm of LLMs by representing both vision and language in a unified discrete token representation via a dynamic visual tokenizer. Through optimization under the unified generative objective, LaVIT can treat images as a foreign language, comprehending and generating them like text. Extensive experimental results further demonstrate the LaVIT's superior capability to serve as a multi-modal generalist. 

\textbf{Acknowledgement}: This research work is supported by National Key R\&D Program of China (2022ZD0160305).

\clearpage

\end{document}