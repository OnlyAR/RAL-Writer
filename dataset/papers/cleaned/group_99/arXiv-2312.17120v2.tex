\title{\mathpile: A Billion-Token-Scale Pre-training Corpus for Math}

\begin{document}

\doparttoc
\faketableofcontents

\maketitle

\begin{abstract}

High-quality, large-scale corpora are the cornerstone of building foundation models. In this work, we introduce \mathpile, a diverse and high-quality math-centric corpus comprising about 9.5 billion tokens. Throughout its creation, we adhered to the principle of ``\emph{less is more}'', firmly believing in the supremacy of data quality over quantity, even in the pre-training phase. Our meticulous data collection and processing efforts included a complex suite of preprocessing, prefiltering, language identification, cleaning, filtering, and deduplication, ensuring the high quality of our corpus. Furthermore, we performed data contamination detection on downstream benchmark test sets to eliminate duplicates and conducted continual pre-training experiments, booting the performance on common mathematical reasoning benchmarks. We aim for our \mathpile to boost language models' mathematical reasoning abilities and open-source its different versions and processing scripts to advance the field (available at \url{https://github.com/GAIR-NLP/MathPile/}).

\end{abstract}

\section{Introduction}

High-quality, diverse pre-training corpora form the cornerstone for developing powerful foundation models, enabling AI assistants like ChatGPT~\citep{openai-chatgpt} to exhibit balanced competencies across a broad spectrum of tasks~\citep{DBLP:journals/corr/abs-2303-12712-sparks-agi}. In this work, our concern centers on mathematical reasoning \begin{wrapfigure}{r}{5.7cm}
\centering 
\includegraphics[width=0.39\textwidth]{img/mathpile-characteristics-1.pdf} 
\caption{Key features of \mathpile.} 
\label{fig:math-pile-characteristics}
\end{wrapfigure}capabilities within foundational language models~\citeg{abel,DBLP:journals/corr/abs-2310-10631-proofpile-2}, for which can potentially boost the application in education tools, automated problem solving, data analysis, etc., thereby improving user experience. To facilitate this, we are not directly building a model, but rather focusing on a more fundamental aspect: \emph{creating a high-quality and diverse pre-training corpus tailored for the math domain}, namely \mathpile. Specifically, our work is significantly different from the previous work in the following characteristics (cf. Table~\ref{tab:math-corpora-comparison} for comparison):

\textbf{Math-centric}. Previous open-sourced pre-training corpora have typically focused on general domains, such as Pile~\citep{DBLP:journals/corr/abs-2101-00027-pile}, RedPajama~\citep{RedPajama} and Dolma~\citep{allenai-dolma}. Others have concentrated on multilingual aspects or programming languages, such as ROOTS~\citep{DBLP:conf/nips/LaurenconSWAMSW22-roots} and The Stack~\citep{DBLP:journals/corr/abs-2211-15533-stack}, respectively. However, a notable absence in these offerings is a corpus specificlly tailoring for mathematics. While there exist some corpora designed for training or continually improving math-specific language models, such as Minerva's mathematical training dataset~\citep{DBLP:conf/nips/LewkowyczADDMRS22-minerva} and  OpenAI's MathMix~\citep{DBLP:journals/corr/abs-2305-20050-lets-verify-step-by-step}, these are not open-sourced. Note that a recent work concurrent with ours, OpenWebMath~\citep{DBLP:journals/corr/abs-2310-06786-openwebmath}, although math-centric, is solely sourced from web pages. We will discuss the comparison with it later. 
Recognizing this gap, we aim to democratize access to high-quality mathematical corpus, 
fostering inclusive advancements in language models' mathematical reasoning.

\textbf{Diversity}. While \citet{DBLP:conf/nips/HendrycksBKABTS21-math} introduced AMPS, a problem set ranging from elementary mathematics to multivariable calculus (K-12 level) for pre-training purposes, it lacks content at the college-level and more challenging competition-level mathematics, focusing instead on a supervised dataset rather than an extensive corpus. The ProofPile corpus, introduced by~ \citet{DBLP:journals/corr/abs-2302-12433-proofpile}, aims to improve autoformalization and formal proving capabilities in models, yet its scope is confined to formal proving, not covering the broader mathematical domain from K-12 to postgraduate level. Concurrently with our work, \citet{DBLP:journals/corr/abs-2310-06786-openwebmath}
propose the OpenWebMath corpus, featuring a corpus composed of mathematical web pages. However, our corpus goes beyond web pages, integrating high-quality mathematics textbooks, lecture notes, scientific papers from arXiv in the field of mathematics, and carefully selected content from StackExchange, ProofWiki, and Wikipedia  among others, which positions our corpus as a richer and more diverse mathematical resource for language models.

\textbf{High-Quality}. Recent studies have increasingly highlighted the detrimental effects of low-quality and repeated content in pre-training corpora on model training, as evidenced in various works~\citep{10.1145/3359591.3359735-code-duplication,luccioni-viviano-2021-whats,lee-etal-2022-deduplicating,DBLP:journals/corr/abs-2205-10487-scaling-law-for-training-on-repeated-data,DBLP:journals/corr/abs-2305-13169-a-pretrainer-data-guide}. The importance of high-quality datasets has thus come to the fore. It has been shown that properly filtered and deduplicated web data can yield models as equally powerful as those trained on curated, high-quality corpora~\citep{DBLP:journals/corr/abs-2306-01116-refinedweb}. This similar practice has been recently adopted in several notable studies~\citep{SlimPajama,allenai-dolma,RedPajama-v2}. 
A notable example is the 1.3 billion-parameter code-focused model pre-trained on synthetically generated textbooks and filtered web pages, a project that broke existing scaling laws although did not open source its data~\citep{DBLP:journals/corr/abs-2306-11644-textbooks-are-all-you-need}. It's important to emphasize that quality of the corpus is far more significant than its quantity. For instance, OpenAI's MathMix comprises only 1.5 billion tokens. In this work, we diligently adhere to the principle of \emph{less is more}, as outlined in \citet{DBLP:journals/corr/abs-2305-11206-lima}. To achieve a high-quality corpus, 
Unlike other approaches that uniformly process all data, we have conducted specialized preprocessing and prefiltering for each data source before global data processing (including language identification, filtering, cleaning, and deduplication). We're dedicated to refining and optimizing our corpus, making a distinctive contribution to the field.

\textbf{Data Documentation}. Auditing large-scale pre-training corpora is essential for identifying content characteristics, intended uses, and potential biases, despite challenges due to their size~\citep{bender-friedman-2018-data,10.1145/3458723-datasheets-for-datasets,10.1145/3594737-data-statements}. However, many such corpora are released without proper documentation~\citep{DBLP:journals/corr/abs-2212-05129-measuring-data}. Recent audits of certain pre-training corpora have uncovered issues such as irrelevant content~\citep{luccioni-viviano-2021-whats,kreutzer-etal-2022-quality,DBLP:journals/corr/abs-2310-20707-what-in-my-big-data}, copyright infringement~\citep{DBLP:journals/corr/abs-2105-05241-datasheet-for-bookcorpus}, and inclusion of test sets for downstream tasks~\citep{10.1145/3359591.3359735-code-duplication,dodge-etal-2021-documenting}, highlighting the need for detailed data sheets and transparent documentation. To this end, following previous efforts to enhance corpora transparency,  we have provided a dataset sheet for our \mathpile (see Table~\ref{tab:mathpile-datasheet}). Throughout our extensive data processing workflow, numerous documents were annotated for quality, such as language identification scores and the ratio of symbols to words (as exemplified in Figure~\ref{fig:cleaned-example-doc-with-quality-annotation}). These quality annotations enable future users to apply their specific filters based on these scores. Additionally, we have conducted extensive deduplication for this corpus and performed data contamination detection with downstream benchmark test sets, removing any duplicated samples identified (cf. \S~\ref{sec:data-contamination-detection}). Interestingly, we have also discovered a significant number of questions from downstream test sets in OpenWebMath (cf. \S~\ref{sec:data-contamination-detection}). This underscores the importance of meticulous data documentation. We plan to release different versions to facilitate future use. See Appendix~\ref{appendix-sec:mathpile-example} for examples.

Additionally, we conducted continual pre-training experiments on \mathpile and found that it generally enhances the performance of language models across various mathematical reasoning benchmarks, with an average improvement of up to 5\% (cf. \S~\ref{sec:continual-pretrain-exp}). In conclusion, we hope to facilitate the growth of the field of AI for mathematics by contributing this specialized, high-quality, diverse corpus focused on the mathematical domain while maintaining utmost transparency about the data for practitioners. 

\section{The Collection of Corpora}
\label{sec:data-collection}

In order to construct \mathpile, we gather data from a variety of sources, which also includes a component of manual collection. We provide an ethics statement regarding copyright in Appendix~\ref{appendix-sec:ethics-statement}.

\begin{table*}[h]
\caption{The comparison of \mathpile with other mathematical Corpora, where PS denotes the problem set type. For non-open-sourced corpora, details are inferred from literature, with unknowns marked as ``?''. Token counts may vary by tokenizer; we use statistics from each dataset's report and the GPTNeoX-20B tokenizer~\citep{black-etal-2022-gpt} for our corpus. DM-Mathematics is from~\citet{DBLP:conf/iclr/SaxtonGHK19-deepmind-mathematics}. "Minerva" refers to its dataset. ProofPile-2~\citep{DBLP:journals/corr/abs-2310-10631-proofpile-2}, encompassing OpenWebMath and others, is excluded from this comparison.}
\label{tab:math-corpora-comparison}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lm{1cm}<{\centering} ccc  m{2cm}<{\centering} m{2.5cm}<{\centering} c m{5.5cm}<{\centering}}
\toprule
\textbf{Datasets} &
  \textbf{Open Source} &
  \textbf{Type} &
  \textbf{Target Domain} &
  \textbf{\# Textbooks} &
  \textbf{Has Synth. Data} &
  \textbf{Data Contam. Detection} &
  \textbf{\# Tokens } &
  \textbf{Source} \\ \midrule

  
Minerva       & \textcolor{black}{\ding{55}} & Corpus      & General Math     & \textcolor{black}{\ding{55}}      & \textcolor{black}{\ding{55}} & \textcolor{black}{\ding{51}} & 38.5B  & arXiv, Web         \\ \midrule              

MathMix         & \textcolor{black}{\ding{55}} & Corpus + PS & General Math     & \textcolor{black}{\textbf{?}}       & \textcolor{black}{\ding{51}} & \textcolor{black}{\ding{51}} & \textcolor{white}{0}1.5B       & \textcolor{black}{\textbf{?}}   \\ \midrule          

ProofPile       & \textcolor{black}{\ding{51}} & Corpus      & Theorem Proving  & 7      & \textcolor{black}{\ding{55}} & \textcolor{black}{\ding{55}} & \textcolor{white}{0}8.3B    & arXiv, Textbooks, Lib., StackExchange, ProofWiki, MATH \\ \midrule 

OpenWebMath     & \textcolor{black}{\ding{51}} & Corpus      & General Math     & \textcolor{black}{\ding{55}}         & \textcolor{black}{\ding{55}} & \textcolor{black}{\ding{55}} & 14.7B  & Web \\ \midrule

DM-Mathematics  & \textcolor{black}{\ding{51}} & PS          & Math Competition & \textcolor{black}{\ding{55}}        & \textcolor{black}{\ding{51}} & \textbf{-} & \textcolor{white}{0}4.4B   & Synthesis \\ \midrule

AMPS   & \textcolor{black}{\ding{51}} & PS          & Math Competition & \textcolor{black}{\ding{55}}        & \textcolor{black}{\ding{51}} & \textcolor{black}{\ding{55}} & \textcolor{white}{0}0.7B  & Khan Academy, Synthesis  \\ \midrule 
 
\mathpile(Ours)  & \textcolor{black}{\ding{51}} & Corpus      & General Math     & 3,979 & \textcolor{black}{\ding{51}} & \textcolor{black}{\ding{51}} & \textcolor{white}{0}9.5B    & arXiv, Textbooks, StackExchange, Wikipedia, ProofWiki, Web  \\ \bottomrule

\end{tabular}%
}

\end{table*}

\noindent\textbf{Mathematical Textbooks} \quad Textbooks, covering mathematical concepts, exercises, and solutions, are valuable for \textit{educational purposes} for both humans and machines. Recent studies, even though not focused on math, support this view with synthesized textbooks~\citep{DBLP:journals/corr/abs-2306-11644-textbooks-are-all-you-need,DBLP:journals/corr/abs-2309-05463-phi-1.5}. To collect these genuine and high-quality textbooks, we began by conducting extensive manual searches across the internet, seeking open-source and freely accessible mathematics-related textbook websites. Afterwards, we proceeded to download these PDF files, resulting in a collection of 38 K-12 level textbooks, along with 369 college-level mathematics textbooks that cover a wide range of subjects including linear algebra, probability theory, calculus, and optimization. In addition to these textbooks, we also included  467 college course handouts and lecture notes, which tend to be more concise compared to full-length textbooks. Subsequently, we employed the Mathpix API\footnote{\url{https://mathpix.com/ocr}} to parse the PDFs into markdown format. Then, we meticulously cleaned up extraneous elements such as parsed image URLs, preface sections, table of contents, acknowledge sections, index sections, and consecutive empty lines within the parsed content, resulting in a  total of 874 documents.

We also refined high-quality mathematics-related synthetic textbooks from OpenPhi Project.\footnote{\url{https://huggingface.co/open-phi}} It is an open-source counterpart to the Phi work~\citep{DBLP:journals/corr/abs-2306-11644-textbooks-are-all-you-need}. While the underlying model and generation process differ, the output encompasses a broad spectrum of subjects, extending beyond programming. To isolate mathematics-related documents, we employed a straightforward criterion: the presence of the symbol ``\texttt{\$\$}'' combined with common mathematical expressions like ``\verb|\|\texttt{\{mathbf}'' and  ``\verb|\|\texttt{\{frac}''.  While ``\texttt{\$\$}'' alone is not always reliable, combining it with these symbols improves accuracy based on manual verification. This approach yielded 3,889 documents from an initial pool of 124,493. As the volume of pre-training data escalates, the synthesis of high-quality data becomes increasingly crucial. More advanced filtering methods and mathematical corpora synthesis are left for future exploration.

\noindent\textbf{Mathematical Papers from ArXiv} \quad ArXiv offers a free distribution service and serves as an open-source archive housing  millions of scientific papers. It also provides  invaluable training data for numerous powerful language models~\citeg{DBLP:journals/corr/abs-2302-13971-llama,RedPajama}. In our endeavor to collect mathematical papers from ArXiv, we identify 50 sub-subjects spanning Mathematics, Computer Science, Statistics, Physics, Quantitative Finance and Economics. Our process involved filtering ArXiv's metadata\footnote{\url{https://www.kaggle.com/datasets/Cornell-University/arxiv}} to focus on the chosen subjects (cf. Table~\ref{tab:arxiv-subject-list}), followed by accessing the source LaTex files (if available). We exclusively retained the LaTex files and consolidated multiple files based on their respective order as indicated by commands such as ``\texttt{include}'' and ``\texttt{input}'' within the main LaTex file of each paper. Subsequently, we undertook extensive transformations to enhance data clarity and consistency, including removing comments, reverting macros, omitting figures but keeping captions, excluding acknowledgements and references, condensing empty lines, replacing some formatting commands, substituting titles, and preserving only the main body content (cf. \S~\ref{appendix-sec:data-collection-details} for more details). Finally, we compiled 347,945 meticulously cleaned LaTex documents (around 8.5 billion tokens), with each document corresponding to a single paper. 

\noindent\textbf{Mathematical Entries in Wikipedia} \quad Wikipedia is one the largest and most popular free online encyclopedias, offering information on a wide range of topics, including history, science, technology, culture, and more. This extensive knowledge has proven to be highly beneficial for numerous natural language processing tasks~\citeg{DBLP:conf/nips/LewisPPPKGKLYR020-rag} and pre-trained language models~\citeg{devlin-etal-2019-bert,DBLP:journals/corr/abs-2302-13971-llama}. To collect mathematical entries from Wikipedia, we downloaded the mathematics-focused (without pictures) dump of Wikipedia in English for the month of August 2023. We extracted the HTML documents from the dump using the library \texttt{libzim}, resulting in approximately 106,900 documents. 
Subsequently, we converted these HTML documents into markdown format using the  \texttt{html2text} library\footnote{We later found that the \texttt{html2text} library resulted in the LaTeX display issue in the cleaned documents (cf. Figure~\ref{fig:latex-display-issue-case}). Switching to another library \texttt{Resiliparse} with DOM parsing resolved this issue, ensuring correct LaTeX display.} while removing the hyperlinks following the practice of LLaMA~\citep{DBLP:journals/corr/abs-2302-13971-llama}. We retained the alternative text content but excluded image (often in SVG format) paths. Additionally, we eliminated extra newlines within paragraphs and condensed more than three consecutive empty lines to two using regular expressions. Further refinement involved the removal of boilerplate content at the bottom of the pages, typically denoted with phrases like ``\texttt{This article is issued from Wikipedia. The text is ...}''. In the end, our efforts yielded a collection of 106,881 mathematical Wikipedia entries, about 0.8 billion tokens.

\noindent\textbf{Entries from ProofWiki} \quad ProofWiki, an online compendium of mathematical proofs, has been instrumental in advancing the fields of autoformalization and formal proof proving, as evidenced by NaturalProofs~\citep{DBLP:conf/nips/Welleck0BHCCC21-naturalproofs} and ProofPile. We sourced data from the ProofWiki dump dated April 9, 2022 (provided by the Internet Archive), mirroring the preprocessing approach employed by NaturalProofs, which was based on the version from November 12, 2020. Specifically, this involved leveraging the \texttt{BeautifulSoup} library to parse all wiki pages followed by the extraction of raw text content using the \texttt{wikitextparser} library. This process yielded a substantial collection of mathematical content, totaling about 7.6 million tokens, comprising 10,328 definitions and 13,511 theorem-proof pairs. To facilitate better data organization, we formatted the definitions using the ``\texttt{definition}'' environment, and the theorem-proof pairs within the ``\texttt{section}'' environment with their respective titles serving as the section headings, similar to  ProofPile.

\noindent\textbf{Mathematical Discussions on StackExchange} \quad StackExchange, renowned for its network of community-powered question-and-answering websites, spans a wide array of topics, each concentrated on a particular topic. Its high-quality data trove has significantly contributed to the development of various language models~\citeg{DBLP:journals/corr/abs-2302-13971-llama,DBLP:journals/corr/abs-2305-11206-lima}. In our study, we identify eleven sites within this network, including five dedicated to mathematics (such as Mathematics and MathOverflow) and six others in closely related fields like Physics (cf. Table~\ref{tab:stackexchange-site-list}). Our data collection process began with downloading the site dumps from August 2023 (provided by the Internet Archive). We only retained the essential components in the posts, namely questions and answers (also associated meta information). To convert HTML documents to raw text, we utilized the \texttt{BeautifulSoup} library, coupled with a meticulous removal of invalid XML characters. We then systematically paired questions and their respective answers. Each question typically garners multiple responses, each with its own score and in some cases, an endorsement as the accepted answer by the questioner.  To guarantee quality, we applied a quality threshold (i.e., 5) for filtering. Questions underwent filtering based on the threshold, whereas answers were assessed by either the threshold or the score of the accepted answer, whichever was lower. Unanswered questions scoring at least 10 were preserved for potential future use. This rigorous process resulted in a rich collection of data, comprising 267,919 questions, 435,129 answers, and 3,418 unanswered questions, totaling about 254 million tokens.

\noindent\textbf{Mathematical Web Pages from Common Crawl} \quad Common Crawl, an archive of web data since 2007, is crucial for training advanced language models like GPT-3~\citep{DBLP:conf/nips/BrownMRSKDNSSAA20-gpt-3} and LLaMA. Our work targets extracting math web pages from SlimPajama~\citep{SlimPajama}, a cleaned and deduplicated version of RedPajama, focusing on its CommonCrawl and C4 subsets. Eschewing the common approach of using neutral network-based filtering, we opt for heuristic rule-based methods. Our procedure began with the creation of TF-IDF features, derived from our curated high-quality textbooks. During this process, we removed the stop words, limited the features to a maximum of 10,000, and employed white space tokenization. Upon the observation of the resulting vocabulary, we identified 11 commonly used LaTex commands, integral to mathematical expressions. We utilize these commands as a basis for a hard match within each document. A document is classified as mathematical if it contains any of these commands along with the symbol  ``\texttt{\$\$}'', typically indicative of a mathematical document. This rule-based approach, though simplistic, proved to be highly effective, especially given the vast size of the Common Crawl corpus
. We also experimented with more intricate dense embedding-based methods to identify mathematical documents, but these resulted in poor recall. Our efforts resulted in the compilation of a substantial collection of mathematical web pages: 4,307 documents from SlimPajama-C4 and 72,137 documents from SlimPajama-CommonCrawl, totaling approximately 633 million tokens. We acknowledge the potential for more efficient methods to sift mathematical documents from Common Crawl snapshots, an area we plan to explore in future work.

\begin{figure*}[h]
\centering 
\includegraphics[width=0.82\textwidth]{img/mathpile-overview-2.pdf} 
\caption{The creation process of \mathpile. We additionally perform data contamination detection on benchmark test sets (cf. \S~\ref{sec:data-contamination-detection}). We visualize its component ratios by document counts (Right).} 
\label{fig:math-pile}
\end{figure*}

\section{Global Data Processing}
\label{sec:global-data-processing}

After conducting specific data preprocessing for each data source during the data collection process, we globally engage in three critical steps: language identification,  filtering, and deduplication, to ensure the quality of the entire corpus, as shown in Figure~\ref{fig:math-pile}.

\subsection{Language Identification}

To filter non-English documents, we utilized the fastText language identifier, which was trained on Wikipedia, Tatoeba, and SETimes~\citep{joulin-etal-2017-fasttext,grave-etal-2018-learning-vector-157-languages}. A common practice is to classify a document as its respective language if the score exceeds 0.5, a threshold also employed by CCNet~\citep{wenzek-etal-2020-ccnet}. However, during the application of this practice, we encountered a considerable number of false positives—cases where documents were erroneously filtered as non-English when, in fact, they were written in English but contained a substantial amount of mathematical symbols. We attribute this issue to the domain gap between the fastText training datasets and the mathematical content. To enhance non-English document filtering, we set customized score thresholds for each data source. Specifically, Wikipedia and StackExchange thresholds were set at 0.1, arXiv at 0.3, and Common Crawl at 0.5. No thresholds were applied to ProofWiki and Textbooks due to manual verification ensuring English content. This refinement removed about 8,400 documents, totaling 231 million tokens.

\subsection{Data Cleaning and Filtering}

Despite thorough preprocessing, some documents, especially from sources like Wikipedia and Common Crawl, lack quality for language modeling due to brevity or automated content. Existing filtering methods~\citep{DBLP:journals/jmlr/RaffelSRLNMZLL20-T5,DBLP:journals/corr/abs-2112-11446-gopher,DBLP:journals/corr/abs-2305-13169-a-pretrainer-data-guide,DBLP:journals/corr/abs-2306-01116-refinedweb,SlimPajama}, while detailed, risk excluding valuable documents in our math-focused corpus if directly applying them as-is. To address this issue, we developed a unique set of cleaning and filtering heuristic rules, specifically crafted for the mathematical domain and drawing from past studies. These rules are aimed at removing meaningless lines (such as boilerplate content) and documents. Specifically, we (1)  detect lines containing ``lorem ipsum'' and filter them out if the resulting line is less than 5 characters; (2) detect lines containing ``javascript'' that also include  ``enable'', ``disable'' or ``browser'' and are under 200 characters, and filter them; (3) filter lines containing fewer than 10 words that include keywords like ``Log in'', ``sign-in'', ``read more...'', or ``items in cart.''; (4) filter documents if the ratio of uppercase words exceeds 40\%; (5) filter lines that end with ``...'' if they constitute more than 30\% of the entire document; (6)  filter documents if the ratio of non-alphabetic words surpasses 80\%; (7)  exclude documents with an average English word length outside the range of (3, 10); (8) discard documents that lack at least two common stop words such as ``the'', ``be'' ``to'' ``of'' ``and'' ``that'' or ``have''; (9)  filter out documents if the ratio of ellipses (...) to words exceeds 0.5 (e.g., progress bars); (10)  remove documents where 90\% of lines start with bullet points; (11)  filter documents including less than 200 characters after removing spaces and punctuation marks.

These meticulously crafted rules enabled us to curate a high-quality mathematical corpus. They also facilitated the assignment of quality annotations to each document from Wikipedia and Common Crawl. These annotations provide researchers and developers with the flexibility to filter the data according to their criteria, catering to specific needs (as shown in Figure~\ref{fig:cleaned-example-doc-with-quality-annotation}). This process resulted in filtering approximately 1,100 documents, removing 17 million tokens.

\subsection{Data Deduplication}

Given that our corpus originates from diverse sources, it is inevitable that there will be repetitions both within and across these sources. Deduplication is vital for training efficiency and reducing data memorization, addressing both exact and near-duplicates~\citep{lee-etal-2022-deduplicating}. We utilized the MinHash LSH algorithm~\citep{DBLP:conf/vldb/GionisIM99-LSH} built on the implementation of \texttt{text-dup}~\citep{chenghao_mou_2023_8364980_text-dedup} and \citet{lee-etal-2022-deduplicating}, to process large-scale corpora efficiently. Specifically, our process involved splitting each document using whitespace and constructing 5-grams, applying the ``\texttt{sha1}'' hash function, and configuring 450 buckets with 20 minhashes each, totaling 9,000 minhashes per document, as per RefinedWeb's guidelines~\citep{DBLP:journals/corr/abs-2306-01116-refinedweb}.

During the deduplication process within each source, we encountered numerous exact and near-duplicate documents across various sources: 304 in arXiv, 623 in Common Crawl, 83,716 in Wikipedia, 783 in textbooks (primarily synthetic), and 144 duplicate questions in StackExchange. Despite finding many near-duplicates in ProofWiki, they were differentiated as unique lemmas, proofs, or definitions, leading us to retain these entries (cf. Table~\ref{tab:dup-case-ProofWiki}). Manual review revealed significant duplication in Wikipedia due to collecting multiple historical document versions and in StackExchange from reposts across different forums (e.g., Math and MathOverflow) for broader visibility (cf. Table~\ref{tab:dup-case-StackExchange}). We provide near-duplicate examples from each data source in Table~\ref{tab:dup-case-CC}-\ref{tab:dup-case-StackExchange}. Cross-source deduplication revealed minimal overlap, with a single StackExchange question duplicated in Common Crawl, which was removed. This eliminated around 714 million tokens.

Note that we also experimented with using suffix arrays~\citep{DBLP:journals/siamcomp/ManberM93-suffix-arrays} to eliminate exact match sequences within documents. However, it tended to remove common phrases like ``Questions: ''. While it can effectively remove some templated content, it also disrupts the contextual integrity of our corpus. Consequently, we decided against employing this in order to preserve the context of our data.

\subsection{Data Contamination Detection}
\label{sec:data-contamination-detection}

As pre-training corpora grow, encountering data contamination becomes inevitable, where evaluation examples are found in the training set. Traditionally, post-hoc analysis, employing n-gram overlap, assesses contamination levels (e.g., GPT-2~\citep{radford2019gpt-2}, GPT-3~\citep{DBLP:conf/nips/BrownMRSKDNSSAA20-gpt-3}, FLAN~\citep{DBLP:conf/iclr/WeiBZGYLDDL22-FLAN}, LLaMA-2~\citep{DBLP:journals/corr/abs-2307-09288-llama-2}).  We advocate for early contamination detection during dataset creation to prevent irreversible damage as delaying exacerbates issues (c.f., previous study~\citep{DBLP:journals/corr/abs-2211-15533-stack}). Here, we utilize popular mathematical reasoning benchmarks, namely GSM8K~\citep{DBLP:journals/corr/abs-2110-14168-GSM8K}, MATH~\citep{DBLP:conf/nips/HendrycksBKABTS21-math}, MMLU-STEM~\citep{DBLP:conf/iclr/HendrycksBBZMSS21-MMLU}, AGIEval-SAT-MATH~\citep{DBLP:journals/corr/abs-2304-06364-agieval}, MathQA~\citep{DBLP:conf/naacl/AminiGLKCH19-mathqa} and AQuA~\citep{ling-etal-2017-program-aqua} to detect data contamination. 

\begin{wraptable}{r}{5.5cm}
\caption{Benchmark test set occurrences in pre-training corpora, with numbers representing minimum occurrences, given potential undetected duplicates.
}
\label{tab:data-contamination-stat}
\scalebox{0.8}{
\begin{tabular}{c|ccc}
\toprule
\textbf{Corpus} &   \textbf{MATH} & \textbf{MMLU-STEM} \\ \midrule
Ours            &              \textcolor{white}{0}23            & \textcolor{white}{0}2                  \\
OpenWebMath     &         195           & 65                 \\ \bottomrule
\end{tabular}}

\end{wraptable}To detect data contamination, we aggregated questions and answers from benchmark tests into a reference set, considering only questions for MMLU, AGIEval, MathQA and AQuA due to its multiple-choice format. Intuitively, math problem solutions often involve diverse reasoning steps, making questions easier to detect for contamination in pre-training data due to their more fixed nature. We utilized line-level exact match detection, dividing documents into lines, hashing each with \texttt{MD5} (taking the first 64 bits and the line itself to form sets), and applied this to both our corpus and the test sets. If a test set line and its hash match exactly with our dataset, it's marked as contamination.

After our detection process, we found 23 questions from MATH and 2 from MMLU-STEM in our corpus (see Table~\ref{tab:data-contamination-stat}), with no accompanying answers. No contamination was detected in other benchmarks. These duplicates mainly originated from StackExchange, Textbooks, and Common Crawl (see Table~\ref{tab:data-contamination-case-in-textbooks} and Table~\ref{tab:data-contamination-case-in-commoncrawl} for examples). Notably, questions from AMC mathematics competition books, also used in the MATH benchmark, were identified in Textbooks. We extended our analysis to OpenWebMath, uncovering more duplicate questions from MATH and MMLU (cf. Table~\ref{tab:data-contamination-case-in-openwebmath}), although many were repeats. This aligns with similar findings by \citet{DBLP:journals/corr/abs-2310-10631-proofpile-2}. These instances highlight the importance of vigilance in creating pre-training corpora to avoid undermining downstream benchmarks. We removed all detected exact matches to mitigate data contamination, resulting in \mathpile corpus.

\section{Data Analysis}

\subsection{Statistics}

\begin{table*}[ht]
\centering
\caption{The components and data statistics of \mathpile.}
\label{tab:mathpile-count-stat}
\scalebox{0.8}{
\begin{tabular}{c|cccccc}
\toprule
\textbf{Components} & \textbf{Size (MB)} & \textbf{\# Documents} & \textbf{\# Tokens} & \textbf{max(\# Tokens)} & \textbf{min (\# Tokens)} & \textbf{ave (\# Tokens)}  \\ \midrule
Textbooks     &  \textcolor{white}{00}644     & \textcolor{white}{00}3,979   & \textcolor{white}{0}187,194,060   & 1,634,015 & 256 & 47,046 \\
Wikipedia     & \textcolor{white}{00}274     & \textcolor{white}{0}22,795 & \textcolor{white}{00}59,990,005    & \textcolor{white}{0}109,282   & \textcolor{white}{0}56   & \textcolor{white}{0}2,632 \\
ProofWiki     & \textcolor{white}{000}23      & \textcolor{white}{0}23,839  & \textcolor{white}{000}7,608,526     & \textcolor{white}{000}6,762     & \textcolor{white}{0}25   & \textcolor{white}{00}319  \\
CommonCrawl   & \textcolor{white}{0}2,560    & \textcolor{white}{0}75,142  & \textcolor{white}{0}615,371,126   & \textcolor{white}{0}367,558   & \textcolor{white}{0}57  & \textcolor{white}{0}8,189 \\
StackExchange & \textcolor{white}{0}1,331  & 433,751 & \textcolor{white}{0}253,021,062   & \textcolor{white}{0}125,475   & \textcolor{white}{0}28  & \textcolor{white}{00}583 \\
arXiv         & 24,576   & 343,830 & 8,324,324,917 & 4,156,454 & \textcolor{white}{0}20  & 24,211 \\ \midrule
Total         & 29,408 & 903,336 &  9,447,509,696  & -         & -    & 10,458  \\ \bottomrule
\end{tabular}%
}

\end{table*}

\begin{wrapfigure}{r}{9cm}
\centering 
\includegraphics[width=0.62\textwidth]{img/all-length-dist-v4.pdf} 
\caption{Document length distribution (log-scale).} 
\label{fig:length-dist}
\end{wrapfigure}
We present detailed statistical information for each component of \mathpile in Table~\ref{tab:mathpile-count-stat}, such as the number of documents and the count of tokens. Following our meticulous and comprehensive data collection and processing process, we obtain 29GB of high-quality and diverse math-centric corpus, encompassing around 9.5 billion tokens, from an initial volume of 2.2TB of raw data (cf. Figure~\ref{fig:math-pile}). Compositionally, arXiv constitutes the largest portion of \mathpile, while Textbooks represent the smallest share but are of exceptionally high quality.

We analyze the document length (in terms of token numbers) and their respective proportions from each source within \mathpile, which is visualized in Figure~\ref{fig:length-dist}. Intuitively, if the data from each source contains a higher amount of near-duplicates or machine-generated content, the distribution of documents of similar lengths becomes more prevalent, leading to a less smooth distribution curve. Figure~\ref{fig:length-dist} shows that, thanks to our thorough and rigorous processing, the document length distribution in \mathpile is relatively smooth across different sources. Note that ProofWiki, due to its fixed format of definitions, lemmas, and proofs, naturally contains shorter content, resulting in a distribution with many similar lengths. We can also observe that, on average, the documents from arXiv and Textbooks tend to be lengthier, while those from ProofWiki and StackExchange are generally shorter.

\subsection{Continual Pre-training Experiments}
\label{sec:continual-pretrain-exp}

We chose \texttt{Mistral-7B-v0.1}~\citep{DBLP:journals/corr/abs-2310-06825-mistral-7b} (the state-of-the-art open-source model at the time) for continual pre-training. We segmented packed text into chunks with a window size of 4,096 and continued pre-training for 3 epochs with a global batch size of 1024. We employ a cosine learning rate schedule with a maximum learning rate of 1e-5 and 1\% warmup steps. All experiments were conducted on NVIDIA A100 8*80GB GPUs. For evaluation, we employ a range of benchmarks - GSM8K, MATH, MMLU-MATH, AGIEval-SAT-MATH, MathQA, AQuA - to assess varying levels of mathematical reasoning abilities, comparing all models using the same few-shot prompting with greedy decoding.

\begin{wraptable}{r}{9.6cm}
\caption{Results on each subset of \mathpile and sampled OpenWebMath. The numbers in parentheses represent the number of tokens trained. \textbf{Bold} results denote improvements over the original Mistral.}
\label{tab:continual-pretrain-results}
\scalebox{0.61}{
\begin{tabular}{l| m{1cm}<{\centering}  m{1cm}<{\centering} m{1cm}<{\centering}  m{1.2cm}<{\centering}  m{1.2cm}<{\centering}  m{1.2cm}<{\centering}}
\toprule
\textbf{Models} & \textbf{GSM8K} & \textbf{MATH}  & \textbf{SAT-MATH} & \textbf{MMLU-Math} & \textbf{MathQA} & \textbf{AQuA}  \\ \midrule
Mistral-7B-v0.1        & 47.38          & 10.08          & 47.27          & 44.92          & 23.51          & 27.95          \\ \midrule
+ Textbooks  (0.56B)       & \textbf{48.97} & \textbf{12.10} & \textbf{56.36}    & \textbf{48.93}     & \textbf{30.38}  & \textbf{33.07} \\
+ Wikipedia (0.18B)    & \textbf{49.96} & \textcolor{white}{0}9.96           & \textbf{53.63} & \textbf{47.16} & \textbf{28.97} & \textbf{35.43} \\
+ StackExchange (0.87B) & 43.06          & \textbf{11.66} & 47.27          & 43.51          & \textbf{27.67} & \textbf{30.70} \\
+ Common Crawl (1.83B)  & 45.56          & \textcolor{white}{0}9.88           & \textbf{50.45} & \textbf{45.17} & \textbf{25.79} & \textbf{31.88} \\
+ arXiv (0.38B) & \textbf{47.91} & \textcolor{white}{0}7.50 & 42.72 & \textbf{46.34} & 18.05 & 27.55 \\
+ Textbooks, Wikipeida, StackEx., CC (4B) &  \textbf{49.88} &  \textbf{11.70} & 43.18 & 43.75 & 23.24 & 25.19 \\ 
\midrule 
+ AMPS (1B) & \textcolor{white}{0}0.08 & \textcolor{white}{0}0.82 & \textcolor{white}{0}3.18 & \textcolor{white}{0}0.47 & \textcolor{white}{0}10.99 & \textcolor{white}{0}8.27 \\
+ DM-Mathematics (5B) &	\textcolor{white}{0}0.00  &	\textcolor{white}{0}0.00  &	\textcolor{white}{0}0.00  &	\textcolor{white}{0}0.00  &	\textcolor{white}{0}0.00  &	\textcolor{white}{0}0.00  \\
+ Sampled OpenWebMath (0.59B) & 43.21 & \textcolor{white}{0}7.86 &  \textbf{47.72} & \textbf{47.52} &  21.80  & 24.80 \\ 
\bottomrule 
\end{tabular}
}
\end{wraptable}

\noindent\textbf{The Effectiveness of \mathpile} \quad We further pre-trained \texttt{Mistral-7B-v0.1} on several subsets, respectively.  As shown in Table~\ref{tab:continual-pretrain-results}, overall, continual pre-training on the subsets generally enhances performance across diverse math benchmarks, albeit to varying degrees. There are exceptions, such as the lack of improvement on GSM8K after training on StackExchange; we suspect this is due to community users rarely asking basic arithmetic questions on StackExchange. Continual pre-training on arXiv leds to a slight performance boost on GSM8K and MMLU-MATH, but a degradation on MATH, SAT-MATH, and MathQA. We attribute this performance degradation to the disparity between the math knowledge present in arXiv papers and that required for the downstream benchmarks. We also conducted pre-training on a collection of Textbooks, Wikipedia, StackExchange, and CC. Experimental results indicate improved performance on GSM8K and MATH, but not on other benchmarks.  Due to limited computational resources,\footnote{Pre-training 10 billion tokens for 1 epoch requires approximately 1,760 NVIDIA A100 GPU hours, making us keen to partner with well-resourced corporations to gain deeper insights in the future.} we did not extensively experiment with the entire dataset or combine data from \mathpile's subsets and existing general corpora, leaving these valuable aspects for future work. Note that we also report some evaluation results on general language benchmarks provided in Appendix~\ref{appendix-sec:general-benchmarks-eval}.

Furthermore, we also conducted continual pre-training on some existing corpora listed in Table~\ref{tab:math-corpora-comparison} for comparison, including AMPS, DM-Mathematics and a random subset of OpenWebMath, cleansed of data leakage, in volumes approximately equal to that of Textbooks. Surprisingly, pre-training directly with these synthetic datasets degraded model performance. We attribute this to the narrow, monotonous structure of AMPS and DM-Mathematics problem sets, making them unsuitable for standalone pre-training; such datasets generally yield better results when combined with broader corpora for pre-training~\citep{xu2024lemur}. Additionally, the OpenWebMath subset produced even less improvement than the same or smaller scale subsets of \mathpile, such as Textbooks and Wikipedia (cf. Table~\ref{tab:continual-pretrain-results}), likely due to a need for more tokens to show substantial gains. These results underscore the superior quality of our data.

\begin{table}[ht]
\caption{Ablation study on data processing pipeline and LaTeX display issue resolution}
\label{tab:ablation-data-processing-pipeline-exp}
\scalebox{0.63}{
\begin{tabular}{l| m{2cm}<{\centering}  m{2.2cm}<{\centering} m{1cm}<{\centering} m{1cm}<{\centering} m{1.2cm}<{\centering} m{1.3cm}<{\centering} m{1.2cm}<{\centering} m{1cm}<{\centering}}
\toprule
\textbf{Models} &
  \textbf{Global Data Processing} &
  \textbf{Fix Latex Display Issue} &
  \textbf{GSM8K} &
  \textbf{MATH} &
  \textbf{SAT-MATH} &
  \textbf{MMLU-MATH} &
  \textbf{MathQA} &
  \textbf{AQuA} \\ \midrule
Mistral-v0.1-7B        & - & - & 47.38          & 10.08 & 47.27          & 44.92          & 23.51          & 27.95          \\ \midrule
+ Sampled raw Wikipedia (0.55B)              & \ding{55}                & \ding{55}                & 41.92          & \textcolor{white}{0}6.28  & 20.90           & 23.70           & 24.72          & 24.01      \\
+ Full raw Wikipedia (2.18B) & \ding{55} &  \ding{55} & 32.30 & \textcolor{white}{0}4.48  & 13.64 & 25.59 & 27.04 & 23.62 \\
+ Full cleaned but LaTeX issued Wikipedia  (0.23B)                  & \ding{51}                & \ding{55}               & 47.15          & \textcolor{white}{0}8.58  & 46.81          & 42.92          & 21.00             & 31.88          \\ \midrule
+ Full cleaned Wikipedia (0.18B)                    & \ding{51}                & \ding{51} & \textbf{49.96} & \textcolor{white}{0}9.96  & \textbf{53.63} & \textbf{47.16} & \textbf{28.97} & \textbf{35.43} \\ \bottomrule
\end{tabular}%
}

\end{table}

\noindent\textbf{The Effectiveness of Data Processing Pipeline} \quad We utilized the Wikipedia subset as a testbed to evaluate our data processing pipeline. We distinguished between raw Wikipedia, which is collected but not globally processed, and cleaned Wikipedia, which has undergone global data processing. Additionally, we performed an ablation study on LaTeX display issues in Wikipedia (cf. Figure~\ref{fig:latex-display-issue-case}), attributed to HTML-to-text conversion tools, by comparing documents with problematic and correct LaTeX displays. Following previous settings, we executed continual pre-training on these datasets. Results in Table~\ref{tab:ablation-data-processing-pipeline-exp} indicate that skipping our pipeline notably reduces Mistral's mathematical reasoning abilities, unaffected by increased training size (i.e., 2.18B). Furthermore, correct LaTeX display in documents is vital for enhancing reasoning capabilities, as shown by the last two rows of Table~\ref{tab:ablation-data-processing-pipeline-exp}. These findings underscore our pipeline's effectiveness and shed light on the superior importance of data quality over quantity, even in the continual pre-training phase.

\section{Related Work}

\noindent\textbf{Pre-training Corpora for Language Models} \quad In language modeling, early models like GPT~\citep{radford2018improving} and BERT~\citep{devlin-etal-2019-bert} are trained on resources such as Books~\citep{DBLP:conf/iccv/ZhuKZSUTF15-Books} and Wikipedia. Later models like GPT-2~\citep{radford2019gpt-2} and T5~\citep{DBLP:journals/jmlr/RaffelSRLNMZLL20-T5} expand training to include web data from Reddit (WebText) and Common Crawl (C4). GPT-3~\citep{DBLP:conf/nips/BrownMRSKDNSSAA20-gpt-3} enlarges its corpus to 300 billion tokens, combining Common Crawl, WebText, Books, and Wikipedia. Pile \citep{DBLP:journals/corr/abs-2101-00027-pile} introduces a diverse collection of 22 datasets for large-scale pre-training. The Gopher project~\citep{DBLP:journals/corr/abs-2112-11446-gopher} compiles a 10.5TB corpus, and PaLM~\citep{DBLP:journals/jmlr/ChowdheryNDBMRBCSGSSTMRBTSPRDHPBAI23-palm} is built from a 780 billion-token corpus, both closed-source. BLOOM~\citep{DBLP:journals/corr/abs-2211-05100-bloom} uses the ROOTS dataset~\citep{DBLP:conf/nips/LaurenconSWAMSW22-roots} for multilingual pre-training. The Stack~\citet{DBLP:journals/corr/abs-2211-15533-stack} provides a 3.1 TB code dataset. LLaMA~\citep{DBLP:journals/corr/abs-2302-13971-llama} utilizes various data sources but doesn't release its corpus, unlike RedPajama~\citep{RedPajama} and its de-duplicated version SlimPajama~\citep{SlimPajama}. RefinedWeb shows web-only corpora can rival curated ones~\citep{DBLP:journals/corr/abs-2306-01116-refinedweb}. Recent models like GPT-4~\citep{DBLP:journals/corr/abs-2303-08774-gpt-4}, Mistral-7B~\citep{DBLP:journals/corr/abs-2310-06825-mistral-7b} and the lastest Gemini~\citep{team2023gemini} have refrained from open-sourcing data. Constructing diverse, high-quality pre-training corpora is crucial for narrowing the performance gap with closed-source models, reflecting our work's aim.

\noindent\textbf{Pre-training Benchmarks and Corpora for Mathematical Reasoning} \quad The challenge of endowing models with human-like mathematical reasoning has attracted significant interest from the machine learning and natural language processing communities. To evaluate models' mathematical capabilities, several benchmark datasets have been developed, including AQuA~\citep{ling-etal-2017-program}, DM-Mathematics~\citep{DBLP:conf/iclr/SaxtonGHK19-deepmind-mathematics}, SVAMP~\citep{patel-etal-2021-nlp},  GSM8K~\citep{DBLP:journals/corr/abs-2110-14168-GSM8K}, and MATH~\citep{DBLP:conf/nips/HendrycksBKABTS21-math}, which cover a range of complexities from basic arithmetic to competition-level mathematics. Additionally, benchmarks like NaturalProofs~\citep{DBLP:conf/nips/Welleck0BHCCC21-naturalproofs} focus on theorem-proving capabilities, while the STEM subset of MMLU~\citep{DBLP:conf/iclr/HendrycksBBZMSS21-MMLU} evaluates understanding across multiple tasks in science, technology, engineering, and mathematics. To improve models' mathematical reasoning, pre-training corpora like AMPS~\citep{DBLP:conf/nips/HendrycksBKABTS21-math} (despite a large-scale synthetic exercise set), ProofPile~\citep{DBLP:journals/corr/abs-2302-12433-proofpile}, and OpenWebMath~\citep{DBLP:journals/corr/abs-2310-06786-openwebmath} have been introduced, targeting various levels of mathematical problem-solving and theorem proving. Unlike Google's Minerva~\citep{DBLP:conf/nips/LewkowyczADDMRS22-minerva} and OpenAI's MathMix~\citep{DBLP:journals/corr/abs-2305-20050-lets-verify-step-by-step}, which are not public, our work focuses on creating a high-quality and diverse mathematical corpus from diverse sources to fill existing gaps.

\section{Conclusion and Limitations}

In this work, we present \mathpile, a specialized corpus centered around mathematics, characterized by its diversity and high quality. Throughout its development, we meticulously source and gather data, applying a rigorous and math-specific pipeline. This pipeline encompasses various stages such as preprocessing, prefiltering, language identification, cleaning and filtering, and deduplication, all aimed at maintaining the high quality of the corpus. We also conduct data contamination detection to remove duplicates from popular mathematical reasoning benchmark test sets, crucial for ensuring their integrity and effectiveness, an aspect often overlooked in other similar works. We aim for our \mathpile to enhance mathematical reasoning in language models, whether used alone or in conjunction with other datasets, to promote broader applications.

This dataset also has some limitations. Many detailed decisions in its creation were made empirically, which may not always be optimal, and verifying decisions directly can be challenging. Moreover, the data scale is insufficient for training extra-large models; subsets like the common crawl could be expanded. Furthermore, the dataset is focused primarily on English, highlighting the need to construct high-quality datasets for other languages. Future research could also explore data mixing~\citep{liu2024regmix} and model-based pre-training corpus refinement~\citep{yu2024mates,zhou2024programming} to enhance dataset quality and model performance.

\begin{ack}
  This work was partially funded by the National Natural Science Foundation of China (62476168), Shanghai Artificial Intelligence Laboratory.
\end{ack}

\clearpage

\section*{Checklist}

\begin{enumerate}

\item For all authors...
\begin{enumerate}
  \item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \answerYes{}
  \item Did you describe the limitations of your work?
     \answerYes{}
  \item Did you discuss any potential negative societal impacts of your work?
   \answerNo{}
  \item Have you read the ethics review guidelines and ensured that your paper conforms to them?
    \answerYes{}
\end{enumerate}

\item If you are including theoretical results...
\begin{enumerate}
  \item Did you state the full set of assumptions of all theoretical results?
    \answerNA{}
	\item Did you include complete proofs of all theoretical results?
    \answerNA{}
\end{enumerate}

\item If you ran experiments (e.g. for benchmarks)...
\begin{enumerate}
  \item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
    \answerYes{}
  \item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
     \answerYes{}
	\item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
     \answerNo{}
	\item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
    \answerYes{}
\end{enumerate}

\item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
\begin{enumerate}
  \item If your work uses existing assets, did you cite the creators?
    \answerYes{}
  \item Did you mention the license of the assets?
   \answerYes{}
  \item Did you include any new assets either in the supplemental material or as a URL?
    \answerNo{}
  \item Did you discuss whether and how consent was obtained from people whose data you're using/curating?
    \answerYes{}
  \item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
    \answerYes{}
\end{enumerate}

\item If you used crowdsourcing or conducted research with human subjects...
\begin{enumerate}
  \item Did you include the full text of instructions given to participants and screenshots, if applicable?
    \answerNA{}
  \item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
    \answerNA{}
  \item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
    \answerNA{}
\end{enumerate}

\end{enumerate}

\clearpage

\end{document}