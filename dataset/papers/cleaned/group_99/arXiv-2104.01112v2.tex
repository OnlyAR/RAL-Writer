\title{\textsc{NaturalProofs}: Mathematical Theorem Proving in Natural Language}

\begin{document}

\maketitle

\begin{abstract}
Understanding and creating mathematics using natural mathematical language -- the mixture of symbolic and natural language used by humans -- is a challenging and important problem for driving progress in machine learning.
As a step in this direction, we develop \textsc{NaturalProofs}, a multi-domain corpus of mathematical statements and their proofs, written in natural mathematical language.
\textsc{NaturalProofs} unifies broad coverage, deep coverage, and low-resource mathematical sources, allowing for evaluating both in-distribution and zero-shot generalization.
Using \textsc{NaturalProofs}, we benchmark strong neural methods on mathematical reference retrieval and generation tasks which test a system's ability to determine key results that appear in a proof.
Large-scale sequence models show promise 
compared to classical information retrieval methods, yet their performance and out-of-domain generalization leave substantial room for improvement.
\textsc{NaturalProofs} opens many avenues for research on challenging mathematical tasks.\footnote{Dataset and code available at \url{https://github.com/wellecks/naturalproofs}.}
\end{abstract}

\section{Introduction}
\label{sec:introduction}

Solving the problem of understanding and creating mathematics using \textit{natural mathematical language} -- the mixture of symbolic and natural language used by humans -- is a path towards developing agents capable of reasoning.
The mixture of symbolic and natural text, 
along with the existence of a formal counterpart, offers a unique setting for studying reasoning that complements research involving natural language alone or purely within a formal system.
Constructing a mathematical proof involves symbolic manipulation, logical and analogical reasoning, as well as knowledge retrieval.
Common sense and natural language abilities are needed to articulate the proof in a concise, comprehensible form.
Moreover, systems that operate on mathematical text have applications in education and scientific discovery, while bridging informal and formal mathematics can be a key driver of progress in automated reasoning \citep{carter2013lurch,kang_document-level_2020,szegedy2020promising}.

Recently, techniques from natural language processing have driven advances in \textit{formalized mathematics} (e.g. \citet{polu2020generative,rabe2021mathematical,wu2021lime}), in which mathematics is written in a verifiable formal language that resembles source code, such as  Mizar \citep{urban2006mptp},  Lean \citep{demoura2015lean}, or Metamath \citep{megill2019metamath}.
However, this setting does not directly address the \textit{informal} aspect of human mathematics, which is conveyed with a mixture of symbolic and natural language \citep{gowers2008princetoncompanion}.
This aspect is crucial, since advancing \textit{human understanding} is a goal of mathematics \citep{thurston1994proof}, and a significant fraction of mathematical knowledge is in natural language text \citep{szegedy2020promising}.

\begin{table}[t]
\footnotesize
\setlength{\tabcolsep}{2pt}
\begin{center}
\begin{tabular}{l p{12cm}}
\toprule
\textbf{Source} & \textbf{ProofWiki} \\
\hline
\textbf{Theorem} & \textbf{Category of Monoids is Category} \\
& Let $\mathrm{Mon}$ be the category of monoids. \\
& Then $\mathrm{Mon}$ is a metacategory. \\
\hline
\textbf{Proof} & Let us verify the axioms $(C1)$ up to $(C3)$ for a {\uline{metacategory}}. We have \\
& {\uline{Composite of Homomorphisms on Algebraic Structure is Homomorphism}}, verifying $(C1)$. \\
& We have {\uline{monoid}} $\left({S, \circ}\right)$. Now, $(C2)$ follows from \\
& {\uline{Identity Mapping is Left Identity and Identity Mapping is Right Identity}}. \\
& Finally, $(C3)$ follows from {\uline{Composition of Mappings is Associative}}. \\
& Hence $\mathrm{Mon}$ is a {\uline{metacategory}}. \\
\bottomrule
\addlinespace[0.2em]
\toprule
\textbf{Source} & \textbf{Textbook: Real Analysis} \\
\hline
\textbf{Theorem} & Suppose that  $f$ is continuous on the closed interval $[a,b]$ and differentiable on the \\
& open interval $(a,b),$ and $f(a)=f(b).$\\ 
& Then $f'(c)=0$ for some $c$ in the open interval $(a,b).$\\
\hline
\textbf{Proof} & Since $f$ is continuous on $[a,b]$, $f$ attains a maximum and a minimum value on $[a,b]$ (\uline{Theorem 2.2.9}). If these two extreme values are the same, then $f$ is constant on $(a,b)$, so  $f'(x)=0$ for all $x$ in $(a,b)$. If the extreme values differ, then at least one must be attained at some point $c$ in the open interval  $(a,b)$, and $f'(c)=0$, by \uline{Theorem 2.3.7}.\\
\bottomrule
\addlinespace[0.2em] 
\end{tabular}
\end{center}
\caption{
    Example theorems and their proofs from $\textsc{NaturalProofs}$. 
    Given a theorem, the mathematical retrieval task consists of retrieving the \uline{references} (underlined) that occur in its proof.
    $\textsc{NaturalProofs}$ contains data from ProofWiki, Stacks, and two textbooks; we show two sources here and two other sources in \autoref{tbl:dataset-example-more}.
    See \autoref{json-example} and \autoref{fig:schema} for data format details.
}
\label{tbl:dataset-example}
\end{table}
In this paper, we describe $\dsname$, a multi-domain corpus of mathematical statements and their proofs, written in natural mathematical language.
$\dsname$ contains \textit{broad-coverage} data from ProofWiki,\footnote{\url{https://proofwiki.org/}} \textit{deep-coverage} data from the Stacks project,\footnote{\url{https://stacks.math.columbia.edu/}} and \textit{low-resource, real-world} data from mathematics textbooks.
$\dsname$ unifies these sources in a common schema and is made publicly available as a resource to drive progress on tasks involving informal mathematics, complementing existing work in this direction (e.g. \cite{ferreira2020natural,ferreira2020premise,wang2020exploration}).

Using $\dsname$, we consider \textit{mathematical reference retrieval}, an analogue of premise selection~\citep{alemi2016deepmath,ferreira2020premise}: given a mathematical claim, retrieve the set of references (theorems, lemmas, definitions) that occur in its proof.
This task represents a crucial facet of mathematical reasoning, in which a mathematician determines the key results that appear in a proof.
As a bridge towards generative tasks using $\dsname$, we consider \textit{mathematical reference generation}, which requires additionally recovering the order and number of references in each proof.

In addition to standard \textit{in-distribution} evaluation, the multi-domain nature of $\dsname$ allows for evaluating \textit{out-of-distribution}, zero-shot generalization. 
We design an evaluation protocol that tests a system's ability to retrieve references for \textit{novel} theorems in each setting, and benchmark methods based on large-scale neural sequence models \citep{devlin2019bert,karpukhin2020dense}, including a strong \textit{joint retrieval} method that better refines the top of the ranked list, 
as well as an \textit{autoregressive} variant for reference generation.
The neural methods are effective for in-domain retrieval compared to classical techniques,
yet out-of-distribution generalization, 
leveraging symbolic mathematical content, and fully recovering a proof's references remain as fundamental challenges.
\textsc{NaturalProofs} opens many possibilities for developing and evaluating machine learning methods on challenging mathematical tasks.

\section{Related Work}
\label{sec:related-work}

\myparagraph{Machine learning for mathematical theorem proving}
A large portion of work integrating machine learning with mathematical reasoning has focused on formalized mathematics.
Early work by \citet{urban2006mptp} used machine learning for selecting relevant premises in the Mizar mathematical library that are passed to an automated theorem prover,
which was later explored with deep neural networks \citep{alemi2016deepmath}.
\citet{bansal2019holist} developed the HOList benchmark based on the HOL Light theorem prover, while other benchmark tasks use the Coq \citep{huang2019gamepad,yang2019learning}, Metamath \citep{whalen2016holophrasm,wang2020learning,polu2020generative}, or Isabelle \citep{li2021isarstep} environments.
These formalized settings differ from \textsc{NaturalProofs}, which uses mathematical language as humans write it.
\citet{szegedy2020promising} argues for leveraging both informal and formal mathematics through autoformalization.
\citet{wang2020exploration} explore translating between informal and formal mathematics, including via a dataset based on ProofWiki, though their dataset is not made available.
\citet{ferreira2020natural,ferreira2020premise} propose a classification-based natural language premise selection task and a dataset based on ProofWiki, while $\dsname$ covers multiple domains and provides evaluation and benchmarks for full retrieval and generative tasks.

\myparagraph{Mathematics and language benchmarks}
Several datasets evaluate a model's ability to solve multiple-choice algebraic word problems \citep{roy2015solving,ling2017program,amini2019mathqa} or arithmetic problems \citep{saxton2018analysing} with varying degrees of natural language. 
\citet{lample2020deep} evaluate neural sequence models on symbolic integration problems, while \citet{hendrycks2021measuring} propose a benchmark based on math competition problems.
\textsc{NaturalProofs} focuses on theorem proving rather than calculation, which we hypothesize evaluates different skills, and may prove useful in bridging formal and informal settings.

\myparagraph{Large-scale neural language models}
Large-scale unsupervised pretraining of language models has led to significant advances in many natural language processing domains (e.g. \cite{devlin2019bert,radford2019language,raffel2020t5,brown2020gpt3}).
Recent work suggests that these models store knowledge in their parameters \citep{petroni2020language}, are capable of reasoning in mathematical \citep{rabe2021mathematical,wu2021lime} and language \citep{clark2020transformers,tafjord2020proofwriter} domains, and are effective for information retrieval tasks \citep{nogueira2020passage,nogueira2020beyond}.
These advances motivate our work, which explores mathematical reasoning in natural language with large-scale language models through a retrieval task.

\section{The \textsc{NaturalProofs} Dataset}
\label{sec:dataset}

\begin{table}[t]
\begin{minipage}{.48\linewidth}
\centering
\includegraphics[width=\columnwidth]{images/ref_graph.png}
\vspace{5pt}
\caption{
    The reference graph. 
    Nodes are \textit{statements} and edges are \textit{reference} links.
    An edge pointing from A to B means that the proof for \textit{theorem} B refers to \textit{statement} A.
    Edges can start from any type of \textit{statement}, but they always end at a \textit{theorem}.
    In our tasks, the dataset is split so that all theorems in the evaluation sets are \textit{leaf} nodes in the reference graph. 
}
\label{fig:ref-graph}
\end{minipage}
\hfill
\begin{minipage}{.48\linewidth}
\setlength{\tabcolsep}{3pt}
\begin{center}
\footnotesize
\begin{tabular}{rr|r|rrrr}
\toprule
 & \textbf{Source} & \textbf{All} & \textbf{PWiki} & \textbf{Stacks} & \textbf{RA} & \textbf{NT} \\
\toprule
\multirow{4}{*}{\rotatebox[origin=c]{90}{\textbf{Theorem}}}
                     & N       & \textit{32,579} & \textit{19,734} & \textit{12,479} & \textit{298} & \textit{68} \\
                     & Tokens  & 46.7 & 38.2 & 60.6 & 33.6 & 23.7 \\
                     & Lines   & 5.9 & 3.6 & 9.7 & 8.4 & 4.5 \\
                     & Refs    & 1.8 & 2.8 & 0.2 & 0.0 & 0.0 \\
                     \midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{\textbf{Proof}}}
                     & N       & \textit{32,012} & \textit{19,234} & \textit{12,479} & \textit{235} & \textit{64} \\
                     & Tokens  & 181.5 & 199.3 & 155.5 & 128.9 & 97.2 \\
                     & Lines   & 24.9 & 25.8 & 23.4 & 36.1 & 16.1 \\
                     & Refs    & 5.6 & 7.4 & 3.0 & 1.6 & 0.9 \\
                     \midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{\textbf{Definition}}}
                     & N       & \textit{14,230} & \textit{12,420} & \textit{1,687} & \textit{86} & \textit{37} \\
                     & Tokens  & 48.4 & 45.0 & 73.2 & 58.6 & 32.6 \\
                     & Lines   & 5.0 & 4.2 & 10.7 & 13.3 & 5.1 \\
                     & Refs    & 2.9 & 3.3 & 0.4 & 0.0 & 0.0 \\
                     \midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{\textbf{Other}}}
                     & N       & \textit{1,974} & \textit{1,006} & \textit{968} & -- & -- \\
                     & Tokens  & 212.1 & 286.1 & 135.2 & -- & -- \\
                     & Lines   & 34.4 & 46.7 & 21.7 & -- & -- \\
                     & Refs    & 5.7 & 9.2 & 2.0 & -- & -- \\
\bottomrule
\end{tabular}
\end{center}
\caption{
    \textsc{NaturalProofs} dataset statistics. Numbers represent mean value, except for "N" rows which represent count.
    \textbf{RA} is the Real Analysis textbook; \textbf{NT} is the Number Theory textbook. 
    See \autoref{tbl:dataset-stats-detail} for detailed statistics.
}
\label{tbl:dataset-stats}
\end{minipage}
\end{table}
The \textsc{NaturalProofs} Dataset is a large-scale, multi-domain dataset for studying mathematical reasoning in natural language.
\textsc{NaturalProofs} consists of 32k theorem statements and proofs, 14k definitions, and 2k other types of pages (e.g. axioms, corollaries)
derived from three domains: \textit{broad-coverage} data from ProofWiki, an online compendium of mathematical proofs written by a community of contributors; \textit{deep-coverage} data from the Stacks project, a collaborative web-based textbook of algebraic geometry; and \textit{low-resource, real-world} data from mathematics textbooks.
\autoref{tbl:dataset-example} shows example theorems and proofs from \textsc{NaturalProofs}, and \autoref{tbl:dataset-stats} shows statistics.

\myparagraph{Multi-domain} \textsc{NaturalProofs} provides a common schema for mathematical statements, proofs, and the references that appear in each.
Its multiple domains provide a challenging evaluation setting for models and opens opportunities for investigating domain transfer, out-of-distribution generalization, and methods for low-resource settings.
This differs from existing resources that focus only on ProofWiki \citep{ferreira2020natural,ferreira2020premise}, and reflects shifts in natural language processing towards multi-domain settings \citep{williams2018mnli,hu2020xtreme}, out-of-distribution generalization \citep{lebras2020adversarial,hendrycks2020pretrained,thakur2021beir}, 
and few- or zero-shot generalization in resource-constrained settings \citep{brown2020gpt3,ebrahimi2021americasnli}.

\textbf{Structure.}
Each \textit{statement} in \textsc{NaturalProofs} 
is either a theorem or a definition.
\textsc{NaturalProofs} provides the statement's title, contents, and references.
The \textit{contents} is a list of sequences, where each sequence contains one line of mixed text and \LaTeX{}, with reference links displayed in their natural language forms.
A \textit{theorem} is associated with one or more proofs when available.
A \textit{proof} contains a title, contents, and references in the same format as a statement.
Finally, we collect \textit{other} pages (e.g. axioms, corollaries).
A \textit{reference} is a theorem, definition, or other page that is linked to within the contents of a statement or proof.
\autoref{fig:schema} shows the data format for theorems, definitions, and proofs in $\textsc{NaturalProofs}$.
All statements and the reference links connecting them form a \textit{reference graph}, shown in \autoref{fig:ref-graph}.
The reference graph can contain cycles, e.g. \texttt{Pythagoras's Theorem} and \texttt{Sum of Squares of Sine and Cosine} refer to each other in their proofs.

\textbf{Data sources and preprocessing.}
We describe how we retrieve data from each source and give an overview of preprocessing; for full details see Appendix \ref{apx:ssec:preprocess} and the Jupyter notebooks we release.

\begin{itemize}[itemsep=0cm,leftmargin=0.3cm,topsep=0cm]
\item \textbf{ProofWiki.}
We download the public ProofWiki XML dump,\footnote{\url{https://proofwiki.org/xmldump/latest.xml}. We use the November 12, 2020 version. ProofWiki is licensed under CC BY-SA 3.0.} which contains a snapshot of all pages on ProofWiki.
We filter pages according to manually designed rules (e.g. redirects, files, categories), and determine page type, title, contents, and references using each page's WikiMedia data structure.
\item \textbf{Stacks.}
We pull the Stacks GitHub repo,\footnote{\url{https://github.com/stacks/stacks-project}. We use the April 15, 2021 version (commit 4df67b8). Stacks is licensed under GNU Free Documentation License.} which contains multiple \LaTeX{} files for various sub-topics in algebraic geometry.
We extract statements and proofs by \LaTeX{} environment names.
For example, the content enclosed by \texttt{\textbackslash{}begin\{theorem\}} and \texttt{\textbackslash{}end\{theorem\}} would be considered a theorem.
\item \textbf{Textbooks.}
We searched for open-source math textbooks with rich theorem-proof structures and reference links.
Of those, we picked \textit{Introduction to Real Analysis}\footnote{\url{https://digitalcommons.trinity.edu/mono/7/}. Retrieved on April 15, 2021. We did not use the supplementary materials. This textbook is licensed under CC BY-NC-SA 3.0.} (\textbf{RA} in short) by William F. Trench and \textit{Elementary Number Theory: Primes, Congruences, and Secrets}\footnote{\url{https://github.com/williamstein/ent}. Retrieved on April 15, 2021. We provide a script to download and format the publicly available latex source.} (\textbf{NT} in short) by William Stein.
We downloaded the \LaTeX{} source of each textbook, and similarly extracted statements and proofs by environment names.
In both textbooks, every statement is either a theorem or a definition -- there are no statements that fall under "others".
\end{itemize}

\section{\textsc{NaturalProofs} Reference Retrieval and Generation Tasks}
\label{sec:task}

\begin{table}[t]
\footnotesize
\centering
\begin{tabular}{rr|rrr|rr}
\toprule
& \textbf{Split} & \textbf{P+S} & \textbf{ProofWiki} & \textbf{Stacks} & \textbf{RA} & \textbf{NT} \\
\toprule
\textbf{Examples} $|\mathcal{E}|$ & \textbf{total} & \textbf{25,271} & \textbf{14,698} & \textbf{10,573} & \textbf{167} & \textbf{40} \\
                                  & train & 21,446 & 12,424 & 9,022 & -- & -- \\
                                  & valid & 1,914 & 1,139 & 775 & -- & -- \\
                                  & test  & 1,911 & 1,135 & 776 & 167 & 40 \\
                                  \midrule
\textbf{Refs} $|\mathcal{R}|$     & train & 42,056 & 28,473 & 13,583 & -- & -- \\
                                  & valid & 45,805 & 30,671 & 15,134 & -- & -- \\
                                  & test  & 45,805 & 30,671 & 15,134 & 384 & 105 \\
                                  \midrule
\textbf{Refs/Ex} $|\yb|$          & train & 5.9 & 7.5 & 3.6 & -- & -- \\
                                  & valid & 5.6 & 7.5 & 2.9 & -- & -- \\
                                  & test  & 5.6 & 7.4 & 2.9 & 2.2 & 1.5 \\
\bottomrule
\addlinespace[0.4em]
\end{tabular}
\caption{
    \textsc{NaturalProofs} retrieval dataset statistics.
    \textbf{P+S} refers to the combined dataset from the ProofWiki and Stacks sources.
    \textbf{RA} (Real Analysis) and \textbf{NT} (Number Theory) are data from mathematical textbook sources that we use for zero-shot evaluation.
}
\label{tbl:dataset-retrieval-stats}
\end{table}
\textsc{NaturalProofs} opens many possible machine learning tasks that involve natural mathematical language.
We consider \textbf{mathematical reference retrieval}: given a theorem $\xb$, retrieve the set of references $\yb$ that occur in its proof.
An example is shown in \autoref{tbl:dataset-example}, where the task is to retrieve the underlined references given the title and contents of the theorem \texttt{Category of Monoids is Category}.
As a proof is ultimately written as an ordered collection of statements with references often occurring more than once, we also consider \textbf{mathematical reference generation}: generate the \textit{sequence} of references that occur in a given theorem's proof. 
These tasks represent a crucial aspect of theorem proving, in which a mathematician determines the key results that appear in a proof.

\myparagraph{Reference retrieval and generation}
Each theorem $\xb$ has a proof containing a sequence of references $\yb=(\rb_1,\ldots,\rb_{|\yb|})$, where each reference $\rb_m\in \mathcal{R}$ is either a theorem, definition, or other statement (see \S\ref{sec:dataset}).
We consider two tasks: \textit{retrieval} and \textit{generation}.

In the \textit{retrieval} task, given an input theorem $\xb$, a model assigns a score to each reference in $\mathcal{R}$, inducing a ranked list $\hat{\rb}^{(1)},\ldots,\hat{\rb}^{(|\mathcal{R}|)}$.
These ranked references are evaluated against the ground-truth reference set using standard retrieval metrics such as mean average precision ($\textsc{mAP}$), recall (\textsc{Rec}@$k$), and full recovery ($\textsc{Full}@k$), which checks whether all references in the proof are in the top-$k$ predicted rankings. 
This reflects the goal of fully proving a theorem using a fixed number of results.

In the \textit{generation} task, a model produces a variable-length sequence of references $(\hat{\rb}_1,\ldots,\hat{\rb}_{|\hat{\yb}|})$ given an input $\xb$, with the goal of exactly matching the ground-truth reference sequence $(\rb_1,\ldots,\rb_{|\yb|})$.
Unlike retrieval, generation requires the model to correctly predict the total number of references, the number of occurrences of each unique reference, and their orders in the proof.

\myparagraph{Input-output examples}
Using \textsc{NaturalProofs}, we derive examples of the  form $(\xb,\yb)$, where $\xb=(x_1,\ldots,x_T)$ is a theorem, and $\yb=(\rb_1,\ldots,\rb_{|\yb|})$ is the sequence of references that occur in the proof of $\xb$.
For retrieval, we transform each sequence into a set $\yb=\{\rb_1,\ldots,\rb_{|\yb|}\}$.
The set of all references, $\mathcal{R}$, consists of theorems, definitions, and other statements (see \S\ref{sec:dataset}).
We use theorems with at least one proof that has at least one reference, 
resulting in a dataset with roughly 25k examples and a reference set $\mathcal{R}$ with 46k unique references.
We partition the dataset into ProofWiki-only, Stacks-only, and textbook-only datasets.
\autoref{tbl:dataset-retrieval-stats} summarizes the size, total references, and average references per example in each dataset.

\myparagraph{Training and evaluation splits}
We design training and evaluation splits that reflect the real-world scenario of proving \textit{newly seen} theorems at evaluation time.
This requires careful attention, since 
naively sampling evaluation examples would yield evaluation theorems that appear as references in the training set.
To ensure that the theorems in the evaluation set have no overlap with the references in the training set,
we form an evaluation set using a randomly sampled subset of \textit{reference graph leaf nodes}, and use the remaining nodes as the training set (\autoref{fig:ref-graph}).
We use roughly half of the evaluation set for validation and the other half for testing.
Since evaluation theorems are not referred to in training examples, the reference set for training is smaller than that for evaluation (\autoref{tbl:dataset-retrieval-stats}).

\section{Methods}
\label{sec:methods}
As benchmark methods for our tasks, we introduce two \textit{parallel retrieval} methods, and a \textit{sequential retrieval} method trained for sequence generation. 
See Appendix~\ref{apx:experiments} for further implementation details.

\myparagraph{Parallel retrieval}
Given a theorem $\xb$, a retrieval model should assign high scores to references in the proof of $\xb$ and low scores to all other references, which corresponds to minimizing,
\begin{align}
\label{eqn:loss-exact}
    \mathcal{L}(\xb,\yb) &= \mathrm{KL}\left(p_*(\mathcal{R}|\xb)\|p_{\theta}(\mathcal{R}|\xb)\right)\\
    &\propto -\sum_{\rb\in \yb}\log \frac{\exp\left(s_\theta(\xb,\rb)\right)}{\sum_{\rb'\in\mathcal{R}}\exp\left(s_{\theta}(\xb,\rb')\right)} + \text{const},
\end{align}
where each distribution is over reference indices (i.e. in $\Delta^{(|\mathcal{R}|)}$),  and $p_*(\rb|\xb)\propto \mathbb{I}[\rb\in \yb]$. 
The denominator requires scores $s_{\theta}(\xb,\rb)$ for all $|\mathcal{R}|$ references, making backpropagation too expensive when a large-scale neural model is used to compute reference representations.
As a result we consider two variants: a \textit{pairwise} model that approximates \autoref{eqn:loss-exact}, and a \textit{joint} model that computes \autoref{eqn:loss-exact} but with implicit vector representations of each reference.

\myparagraph{Pairwise parameterization} 
This model contrasts each positive reference with a set of negatives,
\begin{align}
    \mathcal{L}(\xb, \rb, \yb_-) &= -\log \frac{\exp(s_\theta(\xb, \rb))}{\exp(s_\theta(\xb, \rb))+\sum_{\rb_-\in \yb_-}\exp(s_\theta(\xb, \rb_-))},
\end{align}
where $\rb$ is a reference that occurs in the proof of $\xb$, and $\yb_-$ is a (small) set of negative references.

We call this a pairwise parameterization since the score of each reference against the theorem x is computed independently of the other references,
$s_{\theta}(\xb,\rb)=f_{\theta_1}^{\text{thm}}(\xb)^\top g_{\theta_2}^{\text{ref}}(\rb)$.
This model represents retrieval methods such as the dense passage retriever \citep{karpukhin2020dense} and similar methods \citep{nogueira2020passage}, and allows for evaluating large-scale sequence models, in our case BERT~\citep{devlin2019bert}, on mathematical reference retrieval.

\myparagraph{Joint parameterization}
The second model scores all references in a single pass,
\begin{align}
    p_{\theta}(\mathcal{R}\mid \xb) &= \text{softmax}\left(\mathbf{R}f_{\theta}(\mathbf{x})\right),
\end{align}
where $\mathbf{R}\in \mathbb{R}^{|\mathcal{R}|\times d}$ is a reference embedding matrix and $f_{\theta}(\mathbf{x})\in \mathbb{R}^d$ is a neural theorem encoder.
This model allows for computing \autoref{eqn:loss-exact} exactly in our setting, 
but it must learn implicit representations of each reference, i.e. without observing reference contents.
To give the model access to representations that were learned using reference contents, we populate its embedding matrix as,
\begin{align}
\label{eqn:rmatrix}
    \mathbf{R}=\begin{bmatrix}
         \horzbar & g^{\text{ref}}(\rb_1) & \horzbar\\
         &\ldots &\\
         \horzbar & g^{\text{ref}}(\rb_{|\mathcal{R}|}) & \horzbar\\
\end{bmatrix},
\end{align} 
where $g^{\text{ref}}(\xb)$ is obtained by pretraining an independent model. 

\myparagraph{Sequential generation and retrieval}
Finally, we consider an autoregressive model,
\begin{align}
\label{eqn:autoreg}
    p_{\theta}(\rb_1,\ldots,\rb_{|\yb|}\mid\xb) &= \prod_{t=1}^{|\yb|+1}p_{\theta}(\rb_t|\rb_{<t}, \xb),
\end{align}
where $\rb_{|\yb|+1}$ is a special $\eos$ token denoting the end of the reference sequence.
The autoregressive model is trained to maximize the log-likelihood of ground-truth reference sequences.
Unlike the parallel retrieval models, this model predicts the order and total number of references and can predict multiple occurrences of each reference. It also adjusts its predictions based on preceding predictions.

For generation, a standard decoding algorithm (e.g. beam search) is used to generate a reference sequence $\hat{\yb}=(\hat{\rb}_1,\ldots,\hat{\rb}_{|\hat{\yb}|}\eos$).
For retrieval, we populate a ranked list using generations $\{\hat{\rb}_1,\ldots,\hat{\rb}_{|\hat{\yb}|}\}$ followed by references ordered according to the first step's probabilities, $p_{\theta}(\rb_{1}|\xb)$.

\section{Experiments}
\label{sec:experiments}
First, we benchmark the neural retrieval methods (\S\ref{sec:methods}) on mathematical reference retrieval in terms of their \textit{in-domain} performance (\autoref{tbl:retrieval-main}) and their \textit{out-of-domain} performance on an evaluation set formed from the textbooks in $\dsname$ (\autoref{tbl:retrieval-ood}).
We perform several analyses to better understand each methodâ€™s strengths, weaknesses, and the factors that contribute to their performance.

\myparagraph{In-domain performance} The BERT-based retrieval models show strong in-domain performance compared to the classical TF-IDF and naive baselines in terms of average precision, recall, and the ability to fully recover all true references within the top-$k$ results, as seen in \autoref{tbl:retrieval-main}.
On both ProofWiki and Stacks, the pairwise models outperform TF-IDF, with improvements that are consistent across reference types (Appendix~\autoref{tbl:retrieval-type}).

Joint parameterization substantially improves over the pairwise models that are the starting point of joint training.
On ProofWiki, the joint model ranks roughly 4 out of every 10 true references within its top 10 rankings (R@10 42.45) compared to 1 out of 10 for TF-IDF, and an impressive 75\% within its top 100.
For roughly half of the theorems, the joint model's top 100 references contain \textit{all} of the references 
needed to prove the theorem (Full@100 50.22).
On Stacks the recall@10 is similar at roughly 40\%, with a higher full recovery rate of 66\% for the top 100 results.

The gains from the joint parameterization are most prominent on ProofWiki, e.g. increasing mAP from 16.82 to 36.75.
Joint parameterization particularly excels at refining the top of the ranked list compared to pairwise parameterization; the percentage improvement in the @10 metrics are larger than those for @100 metrics.
On Stacks, the improvements are more modest: though mAP improves by 40\%, the other metrics are relatively close, suggesting that advances beyond the joint model are needed.
This demonstrates the importance of evaluating on multiple domains: each domain presents novel challenges for driving advances in modeling.
Finally, the BERT models trained on both ProofWiki and Stacks (\textbf{BERT (P+S)}) show the possibility of training a single multi-domain model, albeit with lower per-domain performance than the models trained individually on each domain.

\begin{table*}[t]
\setlength{\tabcolsep}{4pt}
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{rr | rrrrr | rrrrr}
\toprule
& & \multicolumn{5}{c}{\textbf{ProofWiki}} & \multicolumn{5}{c}{\textbf{Stacks}} \\
\toprule
& & \textbf{mAP} & \textbf{R@10} & \textbf{R@100} & \textbf{Full@10} & \textbf{Full@100} & \textbf{mAP} &  \textbf{R@10} & \textbf{R@100} & \textbf{Full@10} & \textbf{Full@100} \\
\toprule
\multicolumn{2}{r|}{\textbf{Random}}     & 0.04 & 0.00 & 0.19 & 0.00 & 0.00 & 0.07 & 0.05 & 0.60 & 0.00 & 0.13 \\
\multicolumn{2}{r|}{\textbf{Frequency}}  & 3.38 & 5.90 & 24.30 & 0.44 & 2.29 & 0.91 & 1.76 & 11.27 & 0.13 & 2.45 \\
\multicolumn{2}{r|}{\textbf{TF-IDF}}     & 6.19 & 10.27 & 23.09 & 4.14 & 9.43 & 13.64 & 25.46 & 47.36 & 18.94 & 37.76 \\
\midrule
\multirow{2}{*}{\textbf{BERT (P+S)}} & +\textbf{pair} & 13.54 & 20.10 & 58.75 & 6.17 & 31.28 & 18.58 & 34.42 & 71.80 & 28.48 & 65.21 \\
& +\textbf{joint} & 32.71 & 37.59 & 73.72 & 17.71 & 48.90 & 26.88 & 35.71 & 72.68 & 28.99 & 66.11 \\
\multirow{2}{*}{\textbf{BERT (P/S)}} & +\textbf{pair} & 16.82 & 23.73 & 63.75 & 7.31 & 38.50 & 20.93 & 37.43 & \textbf{74.21} & 30.03 & \textbf{66.37} \\
& +\textbf{joint} & \textbf{36.75} & \textbf{42.45} & \textbf{75.90} & \textbf{20.35} & \textbf{50.22} & \textbf{28.32} & \textbf{39.10} & 73.61 & \textbf{31.96} & 65.59 \\
\bottomrule
\end{tabular}}
\caption{
    \textit{In-domain} performance on the mathematical reference retrieval task (test set).
    \textbf{BERT (P/S)} is finetuned on the part of dataset with the same source as the evaluation set, whereas \textbf{BERT (P+S)} is finetuned on the combined dataset from ProofWiki and Stacks sources.
    Recall is micro-averaged.
}
\label{tbl:retrieval-main}
\end{center}
\end{table*}
\begin{table*}[t]
\setlength{\tabcolsep}{6pt}
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{llllll}
\toprule
\textbf{Source}&\multicolumn{2}{l}{\textbf{ProofWiki}} \\
\hline
\textbf{Theorem}&\multicolumn{2}{l}{\textbf{Category of Monoids is Category}} \\
&\multicolumn{2}{l}{Let $\mathrm{Mon}$ be the category of monoids.} \\
&\multicolumn{2}{l}{Then $\mathrm{Mon}$ is a metacategory.} \\
\hline
&\uline{\textbf{Ground-Truth Reference}}  & \uline{\textbf{Rank (Pairwise)}} & \uline{\textbf{Rank (Joint)}}  \\
&Metacategory  & 1 &1\\
&Identity Mapping is Left Identity & 4 & 5\\
&Identity Mapping is Right Identity  & 5 & 4\\
&Monoid  & 11 & 2 \\
&Composition of Mappings is Associative  & 21& 8 \\
&Identity Mapping is Automorphism & 117 & 64 \\
&Composite of Homomorphisms is Homomorphism& 261 & 54 \\
\hline
\textbf{\uline{Rank}}&\textbf{\uline{Reference (Pairwise)}}  &\multicolumn{2}{l}{\textbf{\uline{Reference (Joint)}}} \\ 
1&\textit{Metacategory }                            & \multicolumn{2}{l}{\textit{Metacategory}} \\
2&Monoid Category is Category                       & \multicolumn{2}{l}{\textit{Monoid}}  \\
3&Monoid Category                                   & \multicolumn{2}{l}{Identity Morphism} \\
4&\textit{Identity Mapping is Left Identity }       & \multicolumn{2}{l}{\textit{Identity Mapping is Right Identity}}   \\
5&\textit{Identity Mapping is Right Identity}       & \multicolumn{2}{l}{\textit{Identity Mapping is Left Identity}}  \\
6&Category                                          & \multicolumn{2}{l}{Associative}   \\
7&Composition of Morphisms                          & \multicolumn{2}{l}{Identity (Abstract Algebra)/Two-Sided Identity}   \\
8&Dual Category is Category                         & \multicolumn{2}{l}{\textit{Composition of Mappings is Associative}}   \\
9&Identity Morphism                                 & \multicolumn{2}{l}{Composition of Morphisms}   \\
10&Morphism Category                                & \multicolumn{2}{l}{Semigroup} \\
\bottomrule
\end{tabular}}
\caption{
    Retrieval for a representative theorem. Top: predicted ranks for ground-truth references using the pairwise (left) and its joint (right) BERT models. Bottom: top 10 retrievals from the pairwise (left) and joint (right) models.
    A retrieved reference is italicized when it is a ground-truth reference.
}
\label{tbl:prediction-example-representative}
\end{center}
\end{table*}\myparagraph{Qualitative evaluation}
\autoref{tbl:prediction-example-representative} shows model predictions for a representative theorem, \texttt{Category of Monoids is Category}.
The pairwise model retrieves three out of seven true references within its top 50 results, while the joint model retrieves five out of seven.
The top 10 results for both models are comprised of references that are related to category theory, which is the subject of the theorem.
This illustrates the model's ability to retrieve \textit{relevant} references, while highlighting its inability to always perform the fine-grained distinction between a relevant reference and one that occurs in the ground-truth proof(s).
Arguably, such a system is still useful for providing hints to a user, so long as the user is confident that all of the true references are in a reasonably small set of results.

\begin{table}[t!]
\setlength{\tabcolsep}{4pt}
\begin{center}
\resizebox{0.65\linewidth}{!}{
\begin{tabular}{r | rrr | rrr}
\toprule
 & \multicolumn{3}{c}{\textbf{Real Analysis}} & \multicolumn{3}{c}{\textbf{Number Theory}} \\
\toprule
& \textbf{mAP} & \textbf{R@10} & \textbf{Full@10} & \textbf{mAP} & \textbf{R@10} & \textbf{Full@10}  \\
\midrule
\textbf{TF-IDF}     &  \textbf{15.79} & \textbf{34.65} & \textbf{27.54} & \textbf{16.42} & 39.62 & 30.00\\
\textbf{BERT-pair (P)}  &  13.24 & 24.01 & 19.16 & 15.12 & \textbf{41.51} & \textbf{35.00}\\
\textbf{+joint} & 11.24 & 20.97 & 16.77 &15.85 & 41.51 & 35.00\\
\textbf{BERT-pair (S)}&11.56 & 21.28 & 14.97 & 12.58 & 26.42 & 20.00 \\
\textbf{+joint} & 7.04 & 11.55& 9.58& 14.88 & 26.42 & 20.00\\

\bottomrule
\end{tabular}}
\end{center}
\caption{\textit{Zero-shot} retrieval performance on out-of-domain textbooks.
}
\label{tbl:retrieval-ood}
\end{table}
\myparagraph{Out-of-domain performance}
While strong in-domain performance drives applications in scenarios where training data is available, an ambitious goal is building a system with mathematical retrieval skills that automatically generalize to new resources.
To evaluate the retrieval methods in this zero-shot, out-of-domain setting, we use each textbook from $\dsname$ as an evaluation set.
This tests situations where the same theorem is expressed using different language (e.g. \autoref{tbl:dataset-example-same}), generalization across data formats, and whether retrieval ability from in-domain training transfers.

\autoref{tbl:retrieval-ood} shows the results. 
The pairwise BERT model trained on ProofWiki underperforms TF-IDF on the Real Analysis textbook, and has comparable performance on the Number Theory textbook.
Joint training did not improve out of domain performance, despite its favorable in-domain impact.
Training BERT on ProofWiki outperforms training on Stacks, showing that the training domain impacts out-of-domain generalization. 
ProofWiki's broad coverage of mathematics may help the model generalize better than the deep, single-topic coverage in Stacks.

The BERT models show some evidence of generalizing to out-of-domain mathematical sources, yet they do not show an advantage over traditional retrieval methods despite strong in-domain performance.
This aligns with recent findings about neural retrieval models in various zero-shot settings \citep{thakur2021beir}.
An exciting research direction is using $\dsname$ to develop and evaluate methods which improve not only in-domain performance, but out-of-domain generalization.

\begin{table}[t]
\setlength{\tabcolsep}{4pt}
\begin{center}
\small
\begin{tabular}{rrrrrrrrrrrr}
\toprule
& & \multicolumn{5}{c}{\textbf{Sequence}}& \multicolumn{2}{c}{\textbf{Multiset}} & \multicolumn{3}{c}{\textbf{Set}}\\
\cmidrule(lr){3-7}\cmidrule(lr){8-9}\cmidrule(lr){10-12}
& \textbf{Model} & \textbf{EM} & \textbf{Edit}($\downarrow$) & $\textbf{BLEU}_4$ & $\textbf{BLEU}_2$ & \textbf{Len} & \textbf{EM} & \textbf{F1} & \textbf{EM} & \textbf{F1} & $\textbf{BLEU}_1$  \\
\toprule
\multirow{5}{*}{\rotatebox[origin=c]{90}{\textbf{Stacks}}} & \textit{*-set} & 51.74 & 35.70 & 9.75 & 47.73 & 0.97 & 89.03 & 97.04 & 100.0 & 100.0 & 94.09\\
&\textit{*-multiset} & 49.42 & 38.13 & 9.71 & 47.71 & 1.00 & 100.0 & 100.0 & 100.0 & 100.0 & 100.0\\
&\textit{*-halfseq} & 0.00 & 70.49 & 6.13 & 12.08 & 0.30 & 0.00 & 56.86 & 0.65 & 58.01 & 16.87\\
\cmidrule(lr){2-12}
&Joint& 0.00 & 98.81 & 0.00 & \textbf{3.42} & 2.82 & 0.00 & \textbf{19.24} & 0.00 & \textbf{19.65} & \textbf{15.15}\\
& Autoregressive & \textbf{3.87} & \textbf{90.65} & 0.00 & 2.59 & \textbf{0.97} & \textbf{4.00} & 13.14 & \textbf{4.90} & 15.04 & 10.06\\
\hline
\multirow{5}{*}{\rotatebox[origin=c]{90}{\textbf{ProofWiki}}} &\textit{*-set} & 18.09 & 58.51 & 7.18 & 29.50 & 0.83 & 49.96 & 82.57 & 100.0 & 100.0 & 65.57\\
&\textit{*-multiset} & 19.23 & 58.09 & 16.68 & 52.89 & 1.00 & 100.0 & 100.0 & 100.0 & 100.0 & 100.0\\
&\textit{*-halfseq} & 0.00 & 58.84 & 25.88 & 29.17 & 0.41 & 0.00 & 63.33 & 4.21 & 70.26 & 30.55\\
\cmidrule(lr){2-12}
&Joint& 0.00 & 93.03 & 0.00 & 6.88 & 1.42 & 0.09 & 25.30 & 0.18 & \textbf{30.76} & 19.27\\
&Autoregressive& \textbf{3.69} & \textbf{84.30} & \textbf{5.48} & \textbf{11.90} & \textbf{1.18} & \textbf{3.78} & \textbf{25.61} & \textbf{4.65} & 28.97 & \textbf{20.81}\\
\bottomrule
\end{tabular}
\end{center}
\caption{
    In-domain \textit{generation} results.
    We show the autoregressive model, a retrieval-only baseline using the top-5 predictions from the joint retrieval model,
    and oracle benchmarks for correctly predicting the first half of the sequence (\textit{*-halfseq}), the full multiset with randomized order (\textit{*-multiset}), and the full set with randomized order (\textit{*-set)}.
    The best model-based method is  in bold.
}
\label{tbl:generation}
\end{table}

\subsection{Reference Generation}
\label{ssec:refgen}
Next, we establish a benchmark for recovering the \textit{sequence} of references occurring in the proof of each theorem via the reference generation task (\S\ref{sec:task}).

\myparagraph{Metrics} We evaluate predicted reference sequences against ground-truth sequences using order-aware \textbf{sequence} metrics, as well as unordered \textbf{multiset} and \textbf{set}-based metrics. 
Sequence metrics include exact match (\textbf{EM}), edit-distance (\textbf{Edit}), standard $\textbf{BLEU}_4$ score which uniformly weights 1-4 gram precision, $\textbf{BLEU}_2$ with only 1-2 gram precision,
and average length ratio $\frac{\text{predicted}}{\text{true}}$ (\textbf{Len}).
Unordered metrics include exact match, \textbf{F1}-score (corpus level), and 1-gram precision $\textbf{BLEU}_1$.

\myparagraph{Methods} We use the autoregressive model to generate a reference sequence for each theorem using beam search.
As a retrieval-only baseline, we form a sequence using the joint retrieval model's top-5 predictions, ordered by retrieval score.
To judge performance and provide a benchmark for future work, we provide three oracle baselines: correctly predicting the first half of the sequence (\textit{*-halfseq}), the full multiset of references with random order (\textit{*-multiset}), and the set with random order (\textit{*-set}).

\myparagraph{Results} \autoref{tbl:generation} shows the in-domain generation results.
The task is challenging, with the autoregressive model exactly matching the ground-truth sequence roughly 3\% of the time.
The autoregressive model improves over the retrieval-only baseline on order-aware metrics, aside from $\textbf{BLEU}_2$ on Stacks.
It does length-prediction reasonably well, with length-ratios of 0.97 and 1.18, yet the multiset and set metrics indicate that the autoregressive model struggles to correctly predict the correct references, even after discarding order.
The oracle baselines indicate substantial room for future improvement-- for instance, predicting only half of each sequence correctly would move ProofWiki $\textbf{BLEU}_4$ from 5.48 to 25.88.
Developing models along the full spectrum from set-based retrieval, to reference generation, to full proof generation is an exciting use-case for $\dsname$.

\subsection{Ablation Studies}

\myparagraph{Initialization and autoregressive retrieval}
As shown in \autoref{tbl:autoregressive-ablation}, the autoregressive model trained for sequence generation substantially improves over the pairwise retrieval model, yet underperforms the joint model, which is trained specifically for retrieval.
Initializing the joint and autoregressive models using the pairwise model was necessary for achieving high performance; in particular, the reference information conveyed through the embedding matrix (\autoref{eqn:rmatrix}) was crucial.

\begin{table}[t]
\begin{minipage}[t]{.33\linewidth}
\setlength{\tabcolsep}{3pt}
\begin{center}
\footnotesize
\begin{tabular}[t]{ lcccc }
  \toprule
  \textbf{Init} & \textbf{Model} & \textbf{mAP} \\\toprule
  -- & Pairwise & 16.99\\
  \hline
  -- & Autoregressive & 17.77 \\ 
  $f^{\text{thm}}$ & Autoregressive & 25.07 \\ 
  $f^{\text{thm}},\mathbf{R}$ & Autoregressive & \textbf{35.37} \\
  \hline
  -- & Joint & 18.71 \\
  $f^{\text{thm}}$ & Joint & 28.95 \\
  $f^{\text{thm}},\mathbf{R}$ & Joint & \textbf{37.51} \\
  \bottomrule
\end{tabular}
\end{center}
\caption{Initializing with pairwise components,
and autoregressive retrieval (ProofWiki).
}
\label{tbl:autoregressive-ablation}
\end{minipage}
\hfill
\begin{minipage}[t]{.32\linewidth}
\setlength{\tabcolsep}{3pt}
\begin{center}
\footnotesize
\begin{tabular}[t]{cc|rr}
\toprule
\multicolumn{2}{c}{\textbf{Train}} & \multicolumn{2}{c}{\textbf{Eval}}\\
\textbf{Lang.} & \textbf{NatProof} & \textbf{PW} & \textbf{Stacks} \\
\toprule
\cmark & \xmark & 0.14 & 0.30 \\
\xmark & \cmark & 0.04 & 0.86 \\
\cmark & \cmark & \textbf{16.99} & \textbf{21.21} \\
\bottomrule
\end{tabular}
\end{center}
\caption{Language pretraining and $\dsname$ finetuning (pairwise retrieval, mAP).}
\label{tbl:retrieval-ablation-training}
\end{minipage}
\hfill
\begin{minipage}[t]{.33\linewidth}
\setlength{\tabcolsep}{3pt}
\begin{center}
\footnotesize
\begin{tabular}[t]{rccrr}
\toprule
& \textbf{Title} & \textbf{Content} & \textbf{PW} & \textbf{Stacks} \\
\toprule
\multirow{3}{*}{\rotatebox[origin=c]{90}{\textbf{TF-IDF}}} & \xmark & \cmark & 4.97 & 12.34 \\
                & \cmark & \xmark & \textbf{8.10} & 12.69 \\
                & \cmark & \cmark & 6.33 & \textbf{13.45} \\
\midrule
\multirow{3}{*}{\rotatebox[origin=c]{90}{\textbf{BERT}}}   & \xmark & \cmark & 16.19 & 19.12 \\
                & \cmark & \xmark & \textbf{24.48} & 19.15 \\
                & \cmark & \cmark & 16.99 & \textbf{21.21} \\
\bottomrule
\end{tabular}
\end{center}
\caption{
    Excluding (\xmark) the title or content of theorems and references (pairwise retrieval, mAP).
}
\label{tbl:retrieval-ablation-titlecontent}
\end{minipage}
\vspace{-2em}
\end{table}
\myparagraph{Language pretraining and \textsc{NaturalProofs} training}
The BERT model has two learning phases: pretraining on language data, and finetuning on \dsname.
As seen in \autoref{tbl:retrieval-ablation-training}, relying on language-pretraining alone without fine-tuning on $\dsname$ (top row) led to poor performance.
Conversely, training from scratch on $\dsname$ (middle row) was unsuccessful, suggesting that language pretraining served as an effective initialization for mathematical retrieval.

\myparagraph{Title and content ablation} 
Each theorem statement and reference consists of a title, as well as contents that is a mixture of symbolic mathematics and natural language. 
As seen in \autoref{tbl:retrieval-ablation-titlecontent}, ProofWiki's titles contain a large amount of useful information for retrieval-- TF-IDF and the pairwise BERT model performed better with only access to titles.
In principal, the title+content model could learn to ignore the contents if needed, so its lower performance shows a deficiency in the pairwise model.
On Stacks, the model performs best with both sources of information, though the degree of improvement suggests that leveraging the mathematical content remains as a fundamental challenge.

\section{Conclusion}
Building agents that understand and create mathematics using \textit{natural mathematical language} is a challenging research direction, providing a means for evaluating and developing machine learning methods capable of symbolic reasoning and natural language understanding.
As a step in this direction, we develop $\dsname$, a multi-domain dataset for studying mathematical reasoning in natural language.
$\dsname$ allows for evaluating \textit{in-domain} performance, and \textit{out-of-domain} generalization in broad and deep coverage mathematics, as well as real-world, low-resource settings.
We establish benchmarks for retrieval and generation tasks that represent key steps in real-world theorem proving,
and are tractable, yet challenging, for current large-scale neural sequence models.
$\dsname$ opens many promising avenues for future research.

\section*{Checklist}

\begin{enumerate}

\item For all authors...
\begin{enumerate}
  \item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \answerYes{}
  \item Did you describe the limitations of your work?
    \answerYes{We discussed limitations throughout our experimental analysis.}
  \item Did you discuss any potential negative societal impacts of your work?
    \answerNA{Our work pertains to use of natural language in mathematical theorem proving, and more generally reasoning in artificial intelligence. 
    Although a general reasoning agent may present negative societal impacts, we do not foresee any immediate negative societal impact from the domain, dataset, tasks, and study that we present here.
    Instead, we foresee positive societal impacts through education and scientific discovery from building systems that understand and create natural mathematical content.}
  \item Have you read the ethics review guidelines and ensured that your paper conforms to them?
    \answerYes{}
\end{enumerate}

\item If you are including theoretical results...
\begin{enumerate}
  \item Did you state the full set of assumptions of all theoretical results?
    \answerNA{We did not include theoretical results.}
	\item Did you include complete proofs of all theoretical results?
    \answerNA{}
\end{enumerate}

\item If you ran experiments (e.g. for benchmarks)...
\begin{enumerate}
  \item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
    \answerYes{We released our code as a GitHub repo and our dataset on Zenodo.}
  \item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
    \answerYes{We specified data splits in \autoref{sec:task}, and hyperparameters in \autoref{apx:experiments}.}
	\item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
    \answerNo{We report results from a single run of each experiment due to computational constraints.}
	\item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
    \answerYes{We specified the computing resources in \autoref{apx:experiments}.}
\end{enumerate}

\item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
\begin{enumerate}
  \item If your work uses existing assets, did you cite the creators?
    \answerYes{In \autoref{sec:dataset}, we cited the authors of mathematical textbooks we used as data sources. ProofWiki and Stacks are collaboratively created on the web.}
  \item Did you mention the license of the assets?
    \answerYes{We noted the license of each data source in \autoref{sec:dataset}, and verified that all permit redistribution with modification for non-commercial purposes.}
  \item Did you include any new assets either in the supplemental material or as a URL?
    \answerYes{We released the \textsc{NaturalProofs} dataset on Zenodo, and provide additional resources in a public Github repository.}
  \item Did you discuss whether and how consent was obtained from people whose data you're using/curating?
    \answerNA{The licenses of the data indicate that our usage is permitted.}
  \item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
    \answerNA{The data we are using/curating contains no PII or offensive content.}
\end{enumerate}

\item If you used crowdsourcing or conducted research with human subjects...
\begin{enumerate}
  \item Did you include the full text of instructions given to participants and screenshots, if applicable?
    \answerNA{We did not use crowdsourcing or conduct research with human subjects.}
  \item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
    \answerNA{}
  \item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
    \answerNA{}
\end{enumerate}

\end{enumerate}
\clearpage

\end{document}