\title{OpenWebMath: An Open Dataset of \\High-Quality Mathematical Web Text}

\begin{document}

\maketitle

\begin{abstract}

There is growing evidence that pretraining on high quality, carefully thought-out tokens such as code or mathematics plays an important role in improving the reasoning abilities of large language models. For example, Minerva, a PaLM model finetuned on billions of tokens of mathematical documents from arXiv and the web, reported dramatically improved performance on problems that require quantitative reasoning. However, because all known publicly released web datasets employ preprocessing that does not faithfully preserve mathematical notation, the benefits of large scale training on quantitive web documents are unavailable to the research community. We introduce OpenWebMath, an open dataset inspired by these works containing 14.7B tokens of mathematical webpages from Common Crawl. We describe in detail our method for extracting text and \LaTeX{} content and removing boilerplate from HTML documents, as well as our methods for quality filtering and deduplication. Additionally, we run small-scale experiments by training 1.4B parameter language models on OpenWebMath, showing that models trained on 14.7B tokens of our dataset surpass the performance of models trained on over 20x the amount of general language data. We hope that our dataset, \href{https://huggingface.co/datasets/open-web-math/open-web-math}{openly released on the Hugging Face Hub}, will help spur advances in the reasoning abilities of large language models.
\end{abstract}

\section{Introduction}
Advances in large language models have opened up new opportunities in numerous fields, providing a transformative shift in our approach to a wide range of complex problems \citep{gpt3, raffel2020exploring}. Among these problems, mathematical reasoning has drawn the attention of several researchers in recent years, becoming both a common benchmark to judge the performance of large language models and inspiring new approaches to improve their reasoning capabilities in the hope that they will one day be able to solve complex mathematical problems. One of the biggest advancements in mathematical reasoning in recent years has been the Minerva model \citep{lewkowycz2022solving}, which achieved state-of-the-art results on quantitative reasoning benchmarks such as MATH \citep{mathdataset}. Minerva was trained by finetuning PaLM \citep{palm} on a curated dataset consisting of billions of tokens of high quality technical content sourced from both scientific papers and the web.

Minerva and the datasets used for its training were not released publicly and the current capabilities of open-source models (e.g., \citet{touvron2023llama, llama2, codellama, openlm2023openllama, pythia}) in quantitative reasoning lags behind. We believe that there are important research directions that can only be enabled through open-access to such models and datasets, such as work on memorization and generalization, reinforcement learning, the development of new reasoning benchmarks, and advancement in the reasoning capabilities of language models.

In our work, we produce an open alternative to the Math Web Pages dataset used to train Minerva \citep{lewkowycz2022solving}. We extract documents from Common Crawl\footnote{\url{https://commoncrawl.org/}}, applying our pipeline to extract text while preserving mathematical content in the form of \LaTeX{} equations. We then filter the documents, ensuring that only high-quality English mathematical documents are kept. Finally, we deduplicate the dataset, resulting in 14.7B tokens of high-quality mathematical content suitable for both pretraining and finetuning large language models. The key contributions of this work are as follows:

\begin{itemize}
    \item We publically release OpenWebMath, a dataset of 14.7B tokens of high-quality mathematical web text. Our dataset can be found at \href{https://huggingface.co/datasets/open-web-math/open-web-math}{https://huggingface.co/datasets/open-web-math/open-web-math} on the Hugging Face Hub.
    \item We extensively document our pipeline, sharing our findings with the NLP community. We open-source the code needed to reproduce our results.
    \item We analyze the quality of OpenWebMath. First, we analyze the contents of our dataset, providing statistics on the types of webpages, subjects, and top domains. Then, we train several language models on our dataset to show that per-token, it is more effective than existing mathematical pretraining datasets, and is most effective when combined with other datasets.
\end{itemize}\begin{figure}[t!]
\begin{center}
\includegraphics[width=\textwidth]{figures/Pipeline.pdf}
\end{center}
\caption{The pipeline for constructing OpenWebMath involves aggressive filtering so that the final dataset only contains high quality, English, and mathematical content.}
\label{fig:pipeline}
\end{figure}

\section{Related Work}
\subsection{Mathematics datasets and benchmarks} 
\paragraph{Mathematics datasets} Over the past couple of years, several datasets of mathematics have been introduced. AMPS, a dataset of informal mathematics, was introduced alongside the MATH dataset \citep{mathdataset}. AMPS includes more than 100,000 Khan Academy problems with step-by-step solutions in LaTeX and over 5 million problems generated using Mathematica scripts. In total, AMPS contains 23GB of problems and solutions. Another notable example is NaturalProofs \citep{welleck2021naturalproofs}, which encompasses 32,000 theorem statements and proofs, 14,000 definitions, and 2,000 other types of pages (e.g. axioms, corollaries) derived from ProofWiki, the Stacks project and data from mathematics textbooks. Proof-Pile \citep{azerbayev2023proofnet} is a dataset of mathematical text that contains more than 14.5GB of informal mathematics texts obtained from arXiv, Stack Exchange, ProofWiki, Wikipedia, openly licensed books, and the MATH dataset. There are also many proprietary datasets for mathematics. WebMath is a large-scale dataset mentioned by OpenAI researchers \citep{polu2020generative} that contains a 35B token mix of content from Github, arXiv, and Math StackExchange, adding up to 35GB of informal mathematics. MathMix is another OpenAI dataset used to finetune GPT-4 \citep{verify-step-by-step} that contains 1B high quality mathematical tokens containing both natural and synthetic data. The proprietary web dataset used to train Minerva, called Math Web Pages \citep{lewkowycz2022solving}, was compiled by collecting 17.5B tokens from web pages that contain \LaTeX{} code.

\paragraph{Mathematics benchmarks} Several popular benchmarks have been used by researchers to assess the capabilities of language models on both formal and informal mathematics. The MATH dataset \citep{mathdataset} is comprised of 12,500 challenging competition problems in informal language. Each problem is also accompanied by a step-by-step informal proof. Answers are delimited by the \texttt{\textbackslash boxed} environment, allowing for easier answer verification. GSM8k \citep{cobbe2021training} is another popular multi-step informal mathematics reasoning benchmark. It contains 8,500 grade school math problems that are intended to be solvable by a bright middle school student. \citet{lewkowycz2022solving} also introduce a benchmark based on OpenCourseWare. OCWCourses includes a set of 272 automatically-verifiable solutions at the undergraduate level, covering chemistry, information theory, differential equations, special relativity, and more. \citet{lewkowycz2022solving} also evaluate on a subset of MMLU \citep{hendrycks2020measuring} called MMLU-STEM, which focuses on science, technology, engineering, and mathematics.

\begin{figure}[t!]
    \begin{center}
    \vspace{-3em}
    \includegraphics[width=\textwidth]{figures/pie_charts_combined.pdf}
    \end{center}
    \caption{\textbf{Left}: The documents in OpenWebMath are sourced from forum posts, educational content, reference pages, scientific papers, blogs, and more. Most content comes from Q\&A forums where users discuss how to solve problems. \textbf{Right}: The majority of the content in OpenWebMath is related to mathematics, but a large part is related to other technical subjects like Physics, Computer Science, Statistics, and more.}
    \label{fig:doublefig}
\end{figure}

\subsection{Web Data Processing Pipelines}
The pretraining of large language models requires large, diverse datasets. Data scraped from the web is one of the primary sources for such data. However, sources such as Common Crawl, which contains over 200 billion web pages, are known to have significant amounts of low-quality and duplicate content, requiring extensive filtering and deduplication to be suitable for training. Prior works such as C4 \citep{raffel2020exploring}, RefinedWeb \citep{refined-web}, CCNet \citep{wenzek2019ccnet}, The Pile \citep{gao2020pile}, and GPT-3 \citep{gpt3} introduce various pipelines for extracting quality data from Common Crawl for the purposes of language model training. These pipelines typically consist of three primary steps: text extraction, filtering, and deduplication.

\paragraph{Text extraction}
Extracting plain text from HTML files is a critical step in the creation of Common Crawl-based datasets. The easiest way to extract text from Common Crawl documents is to use the WET corresponding to each webpage, which contains pre-extracted plain text of the webpage. CCNet and C4 both use Common Crawl's WET files. However, the text extracted in WET files may contain too much boilerplate or miss out on important content such as \LaTeX{} equations. It is also possible to extract text directly from the raw HTML found in Common Crawl WARC files. The Pile uses an open source library called jusText \citep{justext} to extract text from HTML while RefinedWeb uses a library called Trafilatura \citep{barbaresi-2021-trafilatura}. These text extraction approaches differ in terms of extraction speed, customization, and their precision and recall for removing boilerplate content.

\paragraph{Filtering} 
The first layer of filtering often involves language identification \citep{wenzek2019ccnet}. Language filtering is used because certain other parts of the pipeline only work for specific languages, and is often done with simple linear classifiers such as from fastText \citep{joulin2016fasttext}. Quality filtering can be done with a combination of perplexity, classifier, and rule-based methods. CCNet uses a 5-gram Kneser-Ney language model implemented in the KenLM library \citep{heafield2011kenlm} trained on the target domain. The documents in the dataset are then sorted and filtered by their perplexity under this model. Other datasets such as the one used to train GPT-3 \citep{gpt3} use a classifier-based approach. This involves training a classifier on known-high-quality documents, such as those from Wikipedia, as positive examples and unfiltered documents from Common Crawl as negative examples. The classifier scores are used to filter low-quality documents from the dataset. Finally, rule-based approaches such as those used in C4 \citep{raffel2020exploring} and MassiveWeb \citep{gopher} involve removing pages with certain characters, too many or too few characters, too high a proportion of symbols, or those with an abnormal average word length. OpenMathWeb uses a mixture of these three approaches.

\paragraph{Deduplication} Given the periodic nature of Common Crawl snapshots and a general redundancy in web-sourced text, deduplication is an important processing step. Document-level near-deduplication (e.g., in \citep{gpt3, refined-web}) often employs MinHashLSH, an efficient algorithm for estimating the Jaccard similarity of documents. CCNet \citep{wenzek2019ccnet} uses paragraph-level deduplication, which can help to remove common boilerplate content found in WET text-extractions.\section{Building OpenWebMath}

\subsection{Objectives}

Our aim with OpenWebMath is to build a dataset of as many mathematical documents sourced from the web as possible while preserving the formatting of mathematical content such as \LaTeX{} equations as in \citet{lewkowycz2022solving}. For the purposes of this work, we define a mathematical document as a document containing either core mathematical contents such as theorems, definitions, proofs, questions and answers, formal mathematics, or interdisciplinary documents featuring mathematical formulas within fields like physics, chemistry, biology, economics, and finance. We source our documents from Common Crawl, which is a large open-access crawl of the web containing petabytes of raw HTML files. Due to the high variance in the quality of documents from Common Crawl, we additionally use several methods for filtering and boilerplate reduction. Throughout the creation of OpenWebMath, we iteratively refined these methods to ensure that we do not remove too many relevant documents, optimizing for high recall whenever possible. Since we expect that OpenWebMath will be used primarily as an additional source of pretraining data for large language models, we prefer having a small percentage of non-mathematical but high quality documents in the dataset rather than removing them and potentially losing relevant mathematical content. Finally, due to the limited number of mathematical data available on the web, we use significantly more manual inspection and tuning of our processing pipeline than other web-based datasets. We document our processing choices and pipeline in the section that follows.

\subsection{High-level overview of the pipeline}

As shown in \autoref{fig:pipeline}, the processing pipeline for OpenWebMath falls into five stages. First, we apply a prefilter to all HTML documents in Common Crawl to quickly judge whether they have mathematical content, skipping those that do not before doing the extensive processing needed to extract text and equations and remove boilerplate. Second, we extract the text, including mathematical content, from the HTML documents. Third, we apply language identification filters, perplexity-based quality filtering, and a mathematical content classifier filter. Fourth, we deduplicate the dataset using SimHash \citep{manku2007near}. Finally, we manually inspect the documents gathered in the previous steps and view documents from the most popular domains by document-count and character-count, removing domains that are not high quality. We describe each of these steps in detail in the following sections.

\subsection{Prefiltering}

Since there are over 200B HTML documents in Common Crawl, applying our processing over each document would require a significant amount of compute. To improve the efficiency of the pipeline, we first apply a stack of pre-filters optimized for high recall to reduce the number of documents that need to be processed. Our first filters check for common mathematical strings as in \citet{lewkowycz2022solving}, such as the presence of \texttt{tex} classes, \texttt{<math>} tags, and the word ``mathjax''. See \autoref{table:math-keywords} for a full list of terms. If none of these terms are present, we search for the presence of the top 100 most-popular \LaTeX{} symbols in the text. This is done by first filtering for documents containing a backslash command using a simple regular expression and then searching specifically for these \LaTeX{} symbols in the plain text from the HTML document. If none of these symbols are found, we run the plain text through our \textit{MathScore} classifier (see \autoref{sec:math_score}) and keep documents that exceed a confidence threshold of 0.8. By tuning these filters and using hierarchical layers of progressively more accurate but more expensive filters, we were able to reduce the compute needed to process the dataset by several times while retaining a high recall of relevant documents.

\subsection{Text extraction}

In contrast with prior works that extract text from Common Crawl such as C4 \citep{collins2023evaluating}, The Pile \citep{gao2020pile}, and RefinedWeb \citep{refined-web}, we chose to make a mostly custom pipeline for extracting the main content from HTML documents. This is because we found that while other tools get decent performance on average over many documents on the internet, they do not work optimally on many of the most common sources of mathematical content on the web. We instead opted to build on top of Resiliparse \citep{bevendorff:2018, bevendorff:2021c}, a fast and efficient library built in Cython that includes performant tools for parsing HTML pages, processing their DOMs, and extracting the main content. As shown in \autoref{table:extraction-methods-comparison} in the appendix, Resiliparse is significantly more efficient than alternative libraries such as jusText. Another notable part of our text extraction pipeline is that we randomize the parameters of the extraction to add diversity to the dataset. This includes randomizing whether we use a plain text or Markdown format for the documents and randomizing the amount of boilerplate terms required to trigger a line being removed.

\begin{table}[t]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccc}
\toprule
\textbf{Training Dataset} & \multicolumn{1}{c}{\textbf{GSM8k}} & \multicolumn{7}{c}{\textbf{MATH}} \\
                & & Prealgebra & Algebra & \makecell{Intermediate\\Algebra} & \makecell{Counting \&\\Probability} & \makecell{Number\\Theory} & \makecell{Precalculus} & Geometry\\
\midrule
\textbf{The Pile} (14.7B tokens) & 2.2032 & 1.9127 & 1.9751 & 1.8420 & 1.8193 & 1.9227 & 1.6847 & 1.9499 \\
\textbf{ProofPile} (14.7B tokens) & 2.2350 & 1.7370 & 1.7214 & 1.5739 & 1.6462 & 1.7291 & 1.4838 & 1.7229 \\
\textbf{OpenWebMath} (14.7B tokens) & 1.9075 & 1.6285 & 1.6503 & 1.5949 & 1.6002 & 1.6894 & 1.4542 & 1.5748 \\
\textbf{Mixture} (14.7B tokens) & \textbf{1.8968} & \textbf{1.6055} & \textbf{1.6190} & \textbf{1.5301} & \textbf{1.5719} & \textbf{1.6607} & \textbf{1.4119} & \textbf{1.5599} \\
\midrule
\textbf{The Pile} (300B tokens; Pythia 1.4B) & 1.9430 & 1.7117 & 1.7560 & 1.6358 & 1.6359 & 1.7460 & 1.5191 & 1.7252 \\
\bottomrule
\end{tabular}
}
\caption{We trained 1.4B parameter models for 14.7B tokens on various datasets and measured their perplexity on different mathematics benchmarks. Both OpenWebMath and a 50/50 mixture of ProofPile \cite{azerbayev2023proofnet} and OpenWebMath perform well - outperforming Pythia 1.4B \citep{pythia} trained on 300B tokens of The Pile \citep{gao2020pile}.}
\vspace{-1.5em}
\label{table:math-perplexity-comparison}
\end{table}
\begin{table}[b!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|cccc}
\toprule
\multicolumn{1}{c}{\bf Training Dataset} & \multicolumn{1}{c}{\bf MATH Algebra-Easy} & \multicolumn{1}{c}{\bf \makecell{MATH Algebra-Easy\\maj@16}} & \multicolumn{1}{c}{\bf LILA multiarith} \\
\midrule
\textbf{The Pile} (14.7B tokens) & 2.81\% & 3.93\% & 9.77\% \\
\textbf{ProofPile} (14.7B tokens) & 2.81\% & 3.93\% & 8.04\% \\
\textbf{OpenWebMath} (14.7B tokens) & \textbf{5.62\%} & 9.55\% & \textbf{16.67\%} \\
\textbf{Mixture} (14.7B tokens) & 5.06\% & \textbf{10.11\%} & 13.22\% \\
\midrule
\textbf{The Pile} (300B tokens; Pythia 1.4B) & 3.93\% & 5.62\% & \textbf{21.80\%} \\
\bottomrule
\end{tabular}
}
\caption{Accuracy on Different Math Benchmarks.}
\label{table:math-accuracy-comparison}
\end{table}
Our text extraction pipeline consists of four stages: \LaTeX{} extraction, text extraction, DOM processing, and line processing.

\paragraph{\LaTeX{} Extraction} \citet{lewkowycz2022solving} employ a relatively simple \LaTeX{} extraction pipeline that extracts equations from \texttt{<script type="math/latex">}, \texttt{<script type="math/asciimath">}, and \texttt{<math>} blocks with \texttt{<annotation encoding="application/x-tex">} blocks within them and replaces these tags with the extracted equations. When we applied these filters to documents from Common Crawl, we noticed an extremely low number of these tags compared to what was reported. We suspect that this is due to a difference between the HTML files available within Google \citep{lewkowycz2022solving} and those available on Common Crawl. The majority of the \LaTeX{} on the internet is written using MathJax, where developers write equations delimited by dollar signs or other delimiters in their HTML pages and then the included javascript code replaces these equations with properly rendered \LaTeX{} equations within the above script tags when the page is loaded. HTML documents on Common Crawl do not include the changes to the HTML that result from running javascript, requiring that we instead extract the \LaTeX{} equations by finding delimiters ourselves. This is a significant challenge since we need to detect whether the page contains the required MathJax javascript code, which delimiters were chosen by the user to denote equations, and then match and extract the equations from the text on the page. See Appendix \ref{appendix:text_extraction} for a more detailed discussion. 

In order to extract MathJax, we first determine whether the page is importing the MathJax javascript code by searching for the word MathJax on the page. If it is not found, we additionally search for common \LaTeX{} symbols, and if they are found, we treat the page as though it is running MathJax. We use regular expressions to search for code that calls the configuration function for MathJax to extract the delimiters used for equations. We add these delimiters to an extensive list of default delimiters and treat any content between these delimiters as \LaTeX{} equations.

In addition to extracting equations from MathJax, we found several more ways that \LaTeX{} is encoded on the internet. These methods were discovered by filtering small portions of Common Crawl for documents that contain \texttt{\textbackslash frac}, one of the most popular \LaTeX{} commands, and making sure that our processing code supports all the different ways that math could be encoded. We found that \LaTeX{} on the internet is encoded in the following ways:

\begin{figure}[t!]
\begin{center}
\includegraphics[width=\textwidth]{figures/latex_examples.pdf}
\end{center}
\vspace{-1.2em}
\caption{\LaTeX{} formulas can be embedded in HTML documents in many ways, including in images, within arbitrary delimiters, and within special tags. Most common text-extraction pipelines do not extract \LaTeX{} code properly.}
\end{figure}

\begin{enumerate}
    \item \texttt{equation} and \texttt{align} environments.
    \item The \texttt{alttext} of elements with special classes like \texttt{tex}.
    \item Images from domains like \texttt{latex.codecogs.com} often include equations encoded in the URL.
    \item Special wordpress plugins.
    \item \texttt{<math>} tags with \texttt{<annotation encoding="application/x-tex">} blocks within them.
    \item \texttt{<math>} tags with MathML content. We use a style sheet to convert these equations into \LaTeX{}.
    \item MathJax equations encoded in the text of the page.
\end{enumerate}

The relative frequencies of the different ways math is encoded can be found in \autoref{table:latex-types} in the appendix.

\paragraph{DOM Processing} After extracting the \LaTeX{} equations from the HTML, we do several processing steps on the DOM-tree of the HTML document. This includes removing invisible elements based on their styles, removing buttons and link clusters, annotating code, tables, and headers, and removing known problematic elements based on class or ID.

\paragraph{Text Extraction} We use the \texttt{extract\_plain\_text(main\_content=True)} method in Resiliparse \citep{bevendorff:2018} to extract the main content text from the DOM following several preprocessing steps to get around common issues with their specific implementation that cause it to be overly sensitive when removing boilerplate.

\paragraph{Line Processing} After extracting the plain text on the page using Resiliparse, we apply our own processing to remove boilerplate lines based on an iteratively-refined set of common boilerplate phrases, remove empty headers, and escape dollar signs that are not part of \LaTeX{} equations.

\subsection{Filtering}
\label{sec:filtering}

We apply filtering with the goal of removing non-English documents (since our filters pipeline is optimized for English), removing documents that are not mathematical, and removing low-quality documents that would be harmful to train a language model on. We apply the following filters in order:

\begin{figure}[t!]
\begin{center}
\includegraphics[width=\textwidth]{figures/MathScore.pdf}
\end{center}
\vspace{-1em}
\caption{The MathScore classifier used in filtering OpenWebMath is trained to predict whether a text has any of the most popular \LaTeX{} commands based only on surrounding words. This lets us include documents on the web that do not include extractable \LaTeX{} but still contain technical content.}
\vspace{-1em}
\label{fig:mathscore}
\end{figure}

\begin{enumerate}
    \item We use a FastText language identification model \citep{joulin2016fasttext} to remove documents that are not in English.
    \item We use our \textit{MathScore} classifier (see \autoref{sec:math_score}) to get a probability that the document is mathematical. If our previous extraction step found \LaTeX{} equations, we keep documents with a probability of over 0.17. If no \LaTeX{} equations were found, we keep documents with a probability of over 0.8.
    \item We use a KenLM language model \citep{heafield2011kenlm} trained on ProofPile \citep{azerbayev2023proofnet} to get a perplexity score for each document. We remove documents with a perplexity score of more than 15,000.
\end{enumerate}

\subsubsection{Math Score}
\label{sec:math_score}

During our filtering process, we train a model to predict the probability a document is mathematical, which we call \textit{MathScore}. We first gather a dataset of hundreds of thousands documents extracted from our pipeline from an early stage of the project, and label them depending on whether they contain one of the top-100 most common \LaTeX{} commands. We then remove any \LaTeX{} code from the documents and train a classifier to predict whether the documents contain one of these common \LaTeX{} commands. The training process for \textit{MathScore} is depicted in \autoref{fig:mathscore}. Since we remove all \LaTeX{} code from the features fed into the model, the model needs to learn the words and phrases most commonly associated with \LaTeX{} content. We use FastText \citep{joulin2016fasttext} to train this model, and find based on manual inspection that content with a score of under 0.2 is very unlikely to contain useful mathematical content.

\subsection{Deduplication}

Due to the large amount of duplicate documents in Common Crawl, we apply a deduplication step to remove near-duplicate documents. We use the SimHash implementation from text-dedup \citep{text-dedup} to deduplicate the dataset using a threshold of 0.7. We find that this threshold is high enough to remove most duplicate documents even if they have slight differences in their texts.

\subsection{Manual Inspection}

Finally, we manually inspect the top domains by document count, the top domains by character count, and the longest documents in the dataset to ensure that the documents are high quality. We remove domains that are not high quality or clearly not mathematical by adding domains to a blacklist and adding domain filters such as removing user profile pages, abstract-hosting websites as in \citet{lewkowycz2022solving}, and removing search result pages.\section{Dataset Analysis}

\paragraph{Token count} At 14.7B tokens, OpenWebMath is just below the size of Minerva's Math Web Pages (17.5B tokens) \citet{lewkowycz2022solving} and significantly larger than the web part of any other dataset. OpenWebMath has around the same number of LLaMA tokens as ProofPile (14.2B) \citep{azerbayev2023proofnet}, but we note that there is very little overlap between between the two datasets. As a result, OpenWebMath brings a large number of new mathematical tokens that were previously unavailable to the open-source community. Due to differences in data curation strategies, it is hard to compare these datasets other than by training models on them. Since not much is known about how to properly filter a dataset, we opted to keep as much relevant content as possible. However, future work could explore filtering OpenWebMath more aggressively to further improve its quality.

\paragraph{Data Composition} We measured the distribution of domains in OpenWebMath both by document and by character count. \autoref{table:common-domains} and \autoref{table:top-domains-by-characters} show the top twenty most common domains by document and character count respectively. The most common sources of data tend to be discussion forums, blog posts, and scientific papers. We find that the distribution of characters in the dataset is distributed over 131,206 domains, with 46\% of the characters appearing in the top 100 domains.

\begin{table}[b!]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
        \begin{tabular}{lrr}
\multicolumn{1}{c}{\bf Domain}  &\multicolumn{1}{c}{\bf \# Documents} & \multicolumn{1}{c}{\bf \% Documents}\\ 
        \midrule
        stackexchange.com & 1,136,407 & 17.99\% \\ 
        physicsforums.com & 300,044 & 4.75\% \\ 
        mathhelpforum.com & 170,721 & 2.70\% \\ 
        socratic.org & 133,983 & 2.12\% \\ 
        mathoverflow.net & 120,755 & 1.91\% \\ 
        gradesaver.com & 96,100 & 1.52\% \\ 
        zbmath.org & 91,939 & 1.46\% \\ 
        wordpress.com & 87,876 & 1.39\% \\ 
        github.io & 81,125 & 1.28\% \\ 
        brilliant.org & 68,573 & 1.09\% \\ 
        gamedev.net & 50,560 & 0.80\% \\ 
        openstudy.com & 49,041 & 0.78\% \\ 
        gmatclub.com & 48,812 & 0.77\% \\ 
        blogspot.com & 48,036 & 0.76\% \\ 
        wikipedia.org & 46,606 & 0.74\% \\ 
        ac.uk & 41,342 & 0.65\% \\ 
        nature.com & 37,403 & 0.59\% \\ 
        aimsciences.org & 36,368 & 0.58\% \\ 
        libretexts.org & 32,216 & 0.51\% \\ 
        readthedocs.io & 31,455 & 0.50\% \\
        \end{tabular}
        }
        \caption{Most Common Domains by Document Count.}
        \label{table:common-domains}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \resizebox{\textwidth}{!}{
        \begin{tabular}{lrr}
        \multicolumn{1}{c}{\bf Domain}  &\multicolumn{1}{c}{\bf \# Characters} & \multicolumn{1}{c}{\bf \% Characters}\\
                \midrule
                stackexchange.com & 4,655,132,784 & 9.55\% \\ 
                nature.com & 1,529,935,838 & 3.14\% \\ 
                wordpress.com & 1,294,166,938 & 2.66\% \\ 
                physicsforums.com & 1,160,137,919 & 2.38\% \\ 
                github.io & 725,689,722 & 1.49\% \\ 
                zbmath.org & 620,019,503 & 1.27\% \\ 
                wikipedia.org & 618,024,754 & 1.27\% \\ 
                groundai.com & 545,214,990 & 1.12\% \\ 
                blogspot.com & 520,392,333 & 1.07\% \\ 
                mathoverflow.net & 499,102,560 & 1.02\% \\ 
                gmatclub.com & 442,611,169 & 0.91\% \\ 
                gamedev.net & 426,478,461 & 0.88\% \\ 
                ac.uk & 402,111,665 & 0.83\% \\ 
                aimsciences.org & 344,716,386 & 0.71\% \\ 
                mathhelpforum.com & 319,215,756 & 0.65\% \\ 
                deepai.org & 313,512,520 & 0.64\% \\ 
                libretexts.org & 282,014,149 & 0.58\% \\ 
                readthedocs.io & 269,816,413 & 0.55\% \\ 
                tib.eu & 199,714,017 & 0.41\% \\ 
                mit.edu & 198,487,362 & 0.41\% \\
        \end{tabular}
        }
        \caption{Most Common Domains by Character Count.}
        \label{table:top-domains-by-characters}
    \end{minipage}
\end{table}
In order to get a sense of the types of documents found in the dataset, we analyzed 100,000 randomly sampled documents. First, we created embeddings of this data using \texttt{all-MiniLM-L12-v2} \citep{wang2020minilm} in SentenceTransformers \citep{reimers-2019-sentence-bert}. Then, we clustered these embeddings using $k$-Means with $k=128$. Finally, we took the five closest documents to each cluster center and asked \texttt{gpt-3.5-turbo} (\href{https://platform.openai.com/docs/api-reference}{https://platform.openai.com/docs/api-reference}) to classify each cluster as Math, Physics, Statistics, Chemistry, Economics, Computer Science, or Other. We then aggregated these statistics, using the size of each cluster to get an estimate of the final number of documents in each category. We note several potential issues with this methodology, including inaccuracies stemming from using an LLM for classification, and the potential that not every document within a cluster belongs to the predicted category. \autoref{fig:doublefig} shows the results of this analysis. The majority of the documents in the dataset are directly related to mathematics, while the rest are spread out throughout physics, computer science, statistics, chemistry, and economics, with 12\% of documents not falling neatly into any of these categories.

We also used GPT to analyze the types of websites found in OpenWebMath. To do this, we took a sample of 200 documents and asked \texttt{gpt-3.5-turbo} to classify each as a Forum, Paper, Blog, Reference, Educational, Reference, or other. We also gave the document URL as a feature, since we found GPT is often able to judge the topic from the URL alone. We validated our analysis by asking GPT to do this classification on the top 100 domain names and got similar results. \autoref{fig:doublefig} shows the results. The highest proportion of documents are forum pages, where users ask and answer questions related to mathematical subjects. There is also a large proportion of educational and reference content.

\paragraph{Downstream Performance}
\label{sec:models}
We ran experiments to find out how our dataset compares to other language modeling datasets. We compare models trained on OpenWebMath for a single epoch (14.7B tokens) with models trained for the same number of tokens on The Pile \citep{gao2020pile}, a general langauge modeling dataset, and ProofPile \citep{azerbayev2023proofnet}, a dataset of both formal and informal mathematics. We also train a 50/50 mixture of ProofPile and OpenWebMath to evaluate the performance of OpenWebMath when included in a mixture of other datasets, as would be common in practice.

We train randomly initialized models with the same architecture as Pythia 1.4B \citep{pythia}. We use a batch size of 1M tokens and the same hyperparameters as Pythia otherwise. These models are evaluated on a collection of mathematics benchmarks which show signal on models of this size. This includes the subset of level-1 algebra questions from MATH, LILA-multiarith to test coding ability, and GSM8k and MATH perplexities, which scale more smoothly than accuracies. We also compare to Pythia 1.4B \citep{pythia}, which was trained on 300B tokens of The Pile \citep{gao2020pile} with the same architecture.

\autoref{table:math-perplexity-comparison} shows the results for our perplexity evaluations. There is a clear performance lead for models trained with OpenWebMath and the mixture seems to perform best. Despite Pythia being trained on over 20x the number of tokens, the performance of our models on the perplexity benchmarks far exceeds its performance, showing the potential of domain-specific models for mathematics. Similarly, \autoref{table:math-accuracy-comparison} shows the performance of the models on MATH-Algebra-Easy and LILA-multiarith \citep{lila}. OpenWebMath models outperform models that were not trained on it by a significant margin.\section{Conclusion}

In this paper, we describe OpenWebMath, an open dataset of 14.7B high quality mathematical documents from the web. We extensively document our pipeline, including several novel methodologies for extracting \LaTeX{} formulas, reducing boilerplate, and filtering the dataset. OpenWebMath consists of high quality Q\&A forum posts, educational documents, blogs, and more spread across mathematics, physics, computer science, and other technical domains. We also train several models on OpenWebMath and other language modeling datasets to compare the downstream performance achievable by training on our dataset. Notably, we find that models trained on OpenWebMath outperform models trained on 20x more general-domain tokens in mathematics. We hope that OpenWebMath can lead to the creation of language models with improved mathematical reasoning capabilities.\newpage
\section*{Acknowledgements}

JB is supported by NSERC Grant [2020-06904], CIFAR AI Chairs program, Google Research Scholar Program, and Amazon Research Award. KP is supported by an NSERC PGS-D award. Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, Fujitsu Limited, and companies sponsoring the Vector Institute for Artificial Intelligence (\url{www.vectorinstitute.ai/partners}). Computing resources for model training were provided by EleutherAI and Brigham Young University. We thank Finn Paster for the graphic design for the logo. We additionally thank Ziming Chen, Yuhuai Wu, Stella Biderman, Aviya Skowron, Hailey Schoelkopf, and Sean Welleck for their helpful comments.%\section*{Acknowledgements}
\clearpage

\clearpage

\end{document}