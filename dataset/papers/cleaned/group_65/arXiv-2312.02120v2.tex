\title{Magicoder: Empowering Code Generation with OSS-Instruct}

\begin{document}

\twocolumn[
\icmltitle{\raisebox{-1.7mm}{\includegraphics[width=7mm]{assets/magic-hat.png}}\hspace{1pt}\model: Empowering Code Generation with \tech{}}

\icmlsetsymbol{equal}{*}
\icmlsetsymbol{intern}{$\dagger$}

\begin{icmlauthorlist}
\icmlauthor{Yuxiang Wei}{uiuc}
\icmlauthor{Zhe Wang}{thu,intern}
\icmlauthor{Jiawei Liu}{uiuc}
\icmlauthor{Yifeng Ding}{uiuc}
\icmlauthor{Lingming Zhang}{uiuc}
\end{icmlauthorlist}

\icmlaffiliation{uiuc}{University of Illinois at Urbana-Champaign, USA}
\icmlaffiliation{thu}{Tsinghua University, China}

\icmlcorrespondingauthor{Yuxiang Wei}{ywei40@illinois.edu}

\icmlkeywords{Large Language Models, Code Generation, Instruction Tuning}

\vskip 0.3in
]

\printAffiliationsAndNotice{\textsuperscript{$\dagger$}The work was done during a remote summer internship at the University of Illinois.}

\begin{abstract}
We introduce \model{}, a series of \emph{fully open-source} (code, weights, and data) \llmfull{s} (\llm{s}) for code 
that significantly closes the gap with top code models while having no more than 7B parameters. 
\model{} models are trained on 75K synthetic instruction data using \tech,
a novel approach to enlightening \llm{s} with open-source code snippets to generate diverse instruction data for code.
Our main motivation is to mitigate the inherent bias of
{the synthetic data generated by \llm{s}} through the wealth of open-source references for the production of more realistic and controllable data.
The orthogonality of \tech{} and other data generation methods like \evolinstruct{} further enables us to build an enhanced \modelx{}.
Both \model{} and \modelx{} substantially outperform state-of-the-art code models with similar or even larger sizes on a wide range of coding benchmarks.
Notably, \modelxc-7B based on \codellama{} even surpasses the
{prominent} \chatgpt{} on
{\humanevalp{}}
(66.5 vs. 65.9 in \passat{1}).
Overall, \tech{} opens a new direction for
crafting diverse synthetic instruction data for code using abundant open-source references.

\end{abstract}
\begin{figure*}[ht]
\centering
\includegraphics[width=0.9\linewidth]{assets/overview.pdf}\\[8pt]%
\includegraphics[width=0.9\linewidth]{assets/result_overview.png}
\caption{Overview of \tech{} and the \passat{1} results of different \llm{s} on \humaneval~(+)}
\label{fig:overview}
\end{figure*}

\section{Introduction}
{
Code generation, also known as program synthesis~\cite{PGL-010}, is a long-standing challenge in computer science.
In the past few decades, a large body of research has been studying symbolic approaches, such as abstraction-based synthesis~\cite{abstractRefinement, conflictDriven} for general-purpose synthesis problems and programming by examples~\cite{flashfill, liu2023neuri} for domain-specific tasks. %
Until recently, \llmfull{s} (\llm{s}) trained on code~\cite{synthesisllm,codex} has shown outstanding breakthroughs in generating code that accurately satisfies user intents, and they are widely deployed to assist real-world software development~\cite{copilot,codewhisperer}.
}

Initially, closed-source models such as \gptthreefiveturb~\cite{chatgpt}~(\ie \chatgpt) and \gptfour~\cite{gpt4} massively dominated various coding benchmarks and leaderboards~\cite{codex,synthesisllm,liu2023code,ds1000, xia2023keep}.
To further push the boundaries of code generation with open source \llm{s}, \emph{\selfinstruct{}}~\cite{selfinstruct} is adopted to bootstrap
{the instruction-following ability of \llm{s}}.
In the realm of code, practitioners commonly devise synthetic coding instructions using a stronger teacher model (\eg \chatgpt{} and \gptfour) and then finetune a weaker student model (\eg \codellama~\cite{codellama}) with the generated data to distill the knowledge from the teacher~\cite{alpaca, codealpaca}.
For example, \codealpaca{}~\cite{codealpaca} consists of 20K automatically generated code instructions by applying \selfinstruct{} on \chatgpt{} using 21 seed tasks.
To further enhance the coding abilities of \llm{s}, \citet{wizardcoder} proposes \emph{Code \evolinstruct} that employs various heuristics to increase the complexity of seed code instructions (\codealpaca{} in this case), achieving state-of-the-art (SOTA) results among open-source models.

While these data generation methods can effectively improve the instruction-following capability of an \llm, they rely on a narrow range of predefined tasks or heuristics under the hood.
For example, on the one hand, \codealpaca{} that adopts \selfinstruct{} only relies on \emph{21 seed tasks} to generate new code instructions using an identical prompt template.
On the other hand, Code \evolinstruct{} takes \codealpaca{} as seeds and merely depends on \emph{5 heuristics} to evolve the dataset.
As partly suggested by \citet{attrprompt} and \citet{selfinstruct}, such approaches may significantly inherit the system bias inherent in the \llm{s} as well as the predefined tasks.

Therefore, in this paper, we propose \tech{} to mitigate the inherent bias of \llm{s} and to unleash their potential to craft diverse and creative code instructions via direct learning from the open source.
As shown in \Cref{fig:overview}, \tech{} leverages a powerful \llm{} to automatically generate new coding problems by \emph{drawing inspiration} from any random code snippets collected from the open source.
In this example, the \llm{} gets inspired by two incomplete code fragments from different functions and manages to relate them and craft a realistic machine learning problem.
Thanks to the ``infinite'' real-world open-source code, \tech{} can directly produce \emph{diverse}, \emph{realistic}, and \emph{controllable} code instructions by providing distinct seed code snippets.
In the end, we generate 75K synthetic data to finetune \codellamapy{}-7B, resulting in \modelc{}.
While being simple and effective, \tech{} is orthogonal to existing data generation methods, and they can be combined to further boost the models' coding capabilities. 
Therefore, we continually finetune \modelc{} on an open-source \evolinstruct{} dataset with 110K entries, producing \modelxc{}.

We evaluate \model{} and \modelx{} on a wide range of coding tasks, including \humaneval~\cite{codex} and \mbpp~\cite{synthesisllm} for \python{} text-to-code generation,
\multiple~\cite{multiple} for multilingual code completion,
and \dsonek~\cite{ds1000} for solving data science problems.
We further adopt \evalplus~\cite{liu2023code}, which includes the augmented \humanevalp{} and \mbppp{} datasets for more rigorous model evaluation. 
Both \modelc{} and \modelxc{} substantially boost the base \codellamapy-7B. Additionally, \modelc{} even outperforms \wizardcoderc{-7B}, \wizardcodersc{-15B}, and all studied SOTA \llm{s} with less than or equal to 16B parameters on all the benchmarks we tested.
Also, the \passat{1} result of the enhanced \modelxc{} is on par with \chatgpt{} on \humaneval{} (70.7 vs. 72.6) and surpasses it on the more rigorous \humanevalp{} (66.5 vs. 65.9), indicating that \modelxc{} can generate more robust code.
It also achieves SOTA results among all code models at the same scale.

Additionally, we notice a very recent advancement in the development of the \dscoder{} series~\cite{dscoder} which has shown exceptional coding performance.
However, due to the limited technical details disclosed, we only briefly discuss them in \Cref{sec:comparison-with-dscoder}.
Despite this, we applied \tech{} on \dscoderbase~6.7B, resulting in the creation of \modeld{} and \modelxd{}.
In addition to the consistent findings on the previous results with \codellamapy{}-7B as the base model,
\modeld{} and \modelxd{} benefit from the more powerful \dscoderbase-6.7B.
This advantage is demonstrated by \modelxd, which achieves a remarkable 76.8 \passat{1} on \humaneval.
\modelxd{} also outperforms \dscoderinst-6.7B on \humaneval~(+) and \mbpp~(+) with 8$\times$ less finetuning tokens.

To justify the design of \tech{}, \ie generating instruction-tuning data from open-source references rather than using the references directly, we demonstrate that finetuning the base models with semantically relevant comment-function pairs extracted from open-source projects even negatively impacts the model performance (\Cref{sec:direct-tune}).

In general, we make the following contributions:
\begin{itemize}

\item We introduce \tech, a pioneering approach to enlightening
\llm{s} with open-source code snippets to generate more diverse, realistic, and controllable coding instruction data, which can be leveraged to substantially boost the performance of various \llm{s} via instruction tuning.
It opens a new dimension for creating low-bias and diverse instruction-tuning data from the abundance of open-source references.

\item We build the \model{} series trained with \tech{} and \modelx{} series trained on a combination of \tech{} and \evolinstruct{}.
Our evaluation across 6 benchmarks shows that all \model{s} significantly improve the base \llm{s}.
Notably, both \modelxc{} and \modelxd{} outperform \chatgpt{} on \humanevalp{} with only \emph{7B parameters}.

\item We fully open source the model weights, training data, and source code 
at \url{https://github.com/ise-uiuc/magicoder}
to facilitate future research.

\end{itemize}

\section{\tech{}: Instruction Tuning from Open Source}
In this section, we elaborate on our \tech{} approach.
From a high level, as shown in \Cref{fig:overview}, \tech{} works by prompting an \llm{} (\eg \chatgpt) to generate a coding problem and its solution according to some seed code snippet collected from the wild (\eg from \github).
The seed snippet offers controllability of the generation and encourages the \llm{} to create diverse coding problems that can reflect real-world programming scenarios.

\subsection{Generating Coding Problems}

\tech{} is powered by seed code snippets that can be easily collected from open source.
In this work, we directly adopt \seedcorpus{} as our seed corpus, a filtered version of \thestack{}~\cite{thestack} dataset that \starcoder{} is trained on, containing permissively licensed source code documents in various programming languages.
We chose \seedcorpus{} because it is widely adopted, includes massive high-quality code snippets, and is even post-processed for data decontamination~\cite{starcoder, santacoder}.
For each code document from the corpus, we randomly extract 1--15 consecutive lines as the seed snippet for the model to gain inspiration from and produce coding problems. In total, we collected 80K initial seed snippets from 80K code documents, 40K from Python, and 5K from each of C++, Java, TypeScript, Shell, C\#, Rust, PHP, and Swift respectively.
Then, each collected seed code snippet is applied to the prompt template shown in \Cref{apd:prompt}, which a teacher model takes as input and outputs both a coding problem and its solution.

\subsection{Data Cleaning and Decontamination}
\label{sec:tech:decontaminate}

We perform data cleaning by excluding samples that are identical or share the same seed code snippet.
While there exist other sorts of noisiness (\eg the solution is incomplete)
in the generated data, inspired by \citet{unnatural}, they are not removed as we believe they still contain valuable information for \llm{s} to learn.
More experimental details can be found in \Cref{apd:noise}.
Finally, we apply the same logic as \starcoder~\citet{starcoder} to decontaminate our training data by removing coding problems that contain docstrings or solutions from \humaneval{}~\cite{codex} and \mbpp{}~\cite{synthesisllm}, docstrings from \apps{}~\cite{apps}, prompts from \dsonek{}~\cite{ds1000}, or questions from \gsm{}~\cite{gsm8k}.
As part of our analysis, the decontamination procedure only filters out 9 additional samples. Since the seed corpus \seedcorpus{} has already gone through rigorous data decontamination, this observation suggests that \tech{} is unlikely to introduce additional data leakage beyond the seeds.
The eventual \tech{} dataset contains about 75K entries.
{An overview of the dataset statistics can be found in \Cref{apd:dataset}.}

\subsection{Qualitative Examples of \tech{}}
\begin{figure*}
\centering
\includegraphics[width=0.97\textwidth]{assets/qualitative-examples.pdf}
\caption{Examples showing how \tech{} generates problems and solutions from seed code snippets. Detailed problem requirements, implementations, and explanations are omitted for brevity. More examples can be found in \Cref{apd:examples}.}
\label{fig:examples}
\end{figure*}

\Cref{fig:examples}
shows some qualitative examples of how \tech{} can help \llm{} get inspiration from a seed code snippet to create new coding problems and solutions.
For example, the \emph{shell script} example shows how an \llm{} crafts a \python{} coding problem with just one line of shell script.
The \emph{library imports} example demonstrates how an \llm{} can create a realistic machine learning problem using just a few import statements. Meanwhile, the \emph{class signature} instance illustrates the ability of \llm{} to draw inspiration from an incomplete class definition featuring annotations like \texttt{\@SpringBootApplication} and keywords such as \texttt{bank}. From this, the \llm{} generates a problem that requires implementing a complete banking system based on Spring Boot.
Overall, \tech{} can inspire an \llm{} with distinct code structures and semantics to create diverse coding tasks, including algorithmic challenges, realistic issues, single-function code generation, library-based program completion, whole-program development, and even whole-application construction.

 

\begin{figure}
\centering
\includegraphics[width=\linewidth]{assets/humaneval-sim.pdf}
\caption{Cosine similarities between \humaneval{} and synthetic data generated by different methods.}
\label{fig:humaneval-sim}
\end{figure}

\paragraph{Similarity with \humaneval{}}
\label{sec:humaneval-sim}
To study whether our data generation process produces more \humaneval-like problems or solutions that contribute to high performance,
we pair each sample from our 75K dataset with each of the 164 \humaneval{}~\cite{codex} samples and compute their cosine similarity using \tfidf{}~\cite{tfidf} embeddings.
We then associate each \tech{} sample with a \humaneval{} sample with the highest similarity score.
We also compare our dataset against \codealpaca, a 20K dataset applying \selfinstruct{} to code,
and \evolcode{}~\cite{evolcode}, an open-source reproduction of \evolinstruct{} containing 110K coding instructions.
We resort to the open-source implementation because the official \emph{Code \evolinstruct{}}~\cite{wizardcoder} dataset is not released.
We decontaminate all the datasets beforehand using the same way discussed in
\Cref{sec:tech:decontaminate}.
\Cref{fig:humaneval-sim} shows that \tech{} exhibits the lowest average similarity among all the studied data generation techniques while \selfinstruct{} shows the highest average similarity.
This result indicates that the improvements from \tech{} are not merely due to including data from the same distribution.

\section{Evaluation}
We choose \codellamapy-7B and \dscoderbase~6.7B as the base \llm{s}.
To derive \model{} series, we first finetune them on 75K synthetic data generated through \tech{}.
We then obtain \modelx{} by continuing finetuning \model{} with the \evolcode{} dataset, an open-source \evolinstruct{} implementation containing about 110K samples.
{More implementation details and additional evaluation results are listed in \Cref{apd:setup,apd:evalmore}. We also present interesting use cases that reflect the effectiveness of instruction tuning in \Cref{apd:case} and demonstrate \model{}'s capability to generate complex programs in \Cref{apd:complex}.}

\subsection{Python Text-to-Code Generation}

\begin{table*}[t]
 \caption{\Passat{1} (\%) results of different \llm{s} on \humaneval{}~(+) and \mbpp{}~(+) computed with greedy decoding. The abbreviations ``CL'' and ``SC'' refer to the base models \codellamapy{} and \starcoder{}, respectively.
 We report the results consistently from the \evalplus~\cite{liu2023code} Leaderboard.
 }
\label{tab:python-text2code}
\centering
        \begin{booktabs}{
            colspec={@{}lrrrrcc@{}},
            cell{1}{1-3} = {r=2}{m},
            cell{1}{4,6} = {c=2}{c},
            column{4} = {rightsep=10pt},
            column{5} = {leftsep=10pt},
            row{Y-Z} = {bg=lightb}
        }
        \toprule
        Model & Release Date & Size & Benchmark & & Open-Source & \\
        \cmidrule[lr]{4-7}
        & & & \humaneval~(+) & \mbpp~(+) & Weight & Data \\
        \midrule
        \gptthreefiveturb & Nov 2023 & - & 72.6~~(65.9) & 81.7~~(69.4) &\Circle & \Circle\\
        \gptfourturb & Nov 2023 & - & \textbf{85.4}~~(\textbf{81.7}) & \textbf{83.0}~~(\textbf{70.7}) & \Circle & \Circle\\
        \seprule
        \codellamapy & Aug 2023 & 34B & 51.8~~(42.7) & 67.2~~(52.9) & \CIRCLE & \Circle\\
        \wizardcoderc & Sep 2023 & 34B & 73.2~~(64.6)  & 73.2~~(59.9) & \CIRCLE & \Circle \\
        \seprule
        \codetf+ & May 2023 & 16B & 31.7~~(26.2) & 54.6~~(44.4) & \CIRCLE & \CIRCLE\\
        \codegenmono & Mar 2022 & 16B & 32.9~~(27.4) & 52.6~~(43.6) & \CIRCLE & \CIRCLE\\
        \starcoder & May 2023 & 15B & 34.1~~(29.3) & 55.1~~(46.1) & \CIRCLE & \CIRCLE\\
        \codellamapy & Aug 2023 & 13B & 42.7~~(36.6) & 61.2~~(50.9) & \CIRCLE & \Circle\\
        \wizardcodersc & Sep 2023 & 15B & 51.9~~(45.1)  & 61.9~~(50.6) & \CIRCLE & \Circle \\
        \seprule
        \starcoder & May 2023 & 7B & 24.4~~(20.7) & 33.1~~(28.8) & \CIRCLE & \CIRCLE\\
        \mistral & Oct 2023 & 7B & 28.7~~(23.2) & 50.1~~(40.9) & \CIRCLE & \Circle\\
        \codetf+ & May 2023 & 6B & 29.3~~(23.8) & 51.9~~(40.9) & \CIRCLE & \CIRCLE\\
        \codegenmono & Mar 2022 & 6B & 29.3~~(25.6) & 49.9~~(42.1) & \CIRCLE & \CIRCLE\\
        \codellamapy & Aug 2023 & 7B & 37.8~~(34.1) & 57.6~~(45.4) & \CIRCLE & \Circle\\
        \wizardcoderc & Sep 2023 & 7B & 48.2~~(40.9) & 56.6~~(47.1) & \CIRCLE & \Circle \\
        \seprule
        \modelc{} & Dec 2023  & 7B & 60.4~~(55.5) & 64.2~~(52.6) & \CIRCLE & \CIRCLE \\
        \modelxc{} & Dec 2023 & 7B & \textbf{70.7}~~(\textbf{66.5}) & \textbf{68.4}~~(\textbf{56.6}) & \CIRCLE & \CIRCLE\\
        \bottomrule
\end{booktabs}%
\end{table*}

\humaneval{}~\cite{codex} and \mbpp{}~\cite{synthesisllm} are two of the most widely used benchmarks for code generation.
Each task in these benchmarks includes a task description (\eg docstring) as the prompt, where \llm{s} generate corresponding code whose correctness is checked by a handful of test cases.
Because tests in these benchmarks can be insufficient, for more rigorous evaluation, we use \humaneval{+} and \mbpp{+}, both powered by the \evalplus{} framework~\cite{liu2023code} to obtain 80$\times$/35$\times$ more tests.
Following prior work~\cite{liu2023code,selfdebug}, for each task and \llm{} we use greedy decoding to generate one sample and focus on comparing the \passat{1} metric.

We consider a wide range of baseline models, including \codellamapy~\cite{codellama}, \wizardcoder~\cite{wizardcoder}, \gptthreefiveturb~\cite{chatgpt}, \gptfourturb~\cite{gpt4}, \starcoder~\cite{starcoder}, \codetf+~\cite{codetfp}, \codegenmono~\cite{codegen}, and \mistral~\cite{mistral}.
All the results are consistently reported from the \evalplus{}~\cite{liu2023code} leaderboard (\evalplus{} hash: \texttt{1895d2f}).

\Cref{tab:python-text2code} shows the \passat{1} results of different \llm{s} on these benchmarks.
From the results, we can first observe that \modelc{} has a clear improvement over the base \codellamapy-7B, and outperforms all studied open-source models except \codellamapy-34B and \wizardcoderc-34B. Notably, \modelc{} surpasses \wizardcodersc{}-15B and has a substantial improvement on \humaneval{} and \humanevalp{} over \codellamapy-34B.
\modelxc{} demonstrates further improvements by being trained with the orthogonal \evolinstruct{} method. \modelxc{} outperforms \chatgpt{} and all other open-source models on \humanevalp{}. Moreover, although it scores slightly lower than \wizardcoderc-34B and \chatgpt{} on \humaneval{}, it surpasses both of them on the more rigorous \humanevalp{} dataset, indicating that \modelxc{} may produce more robust code.

\subsection{Multilingual Code Generation}

In addition to \python, as shown in \Cref{tab:multilang}, we perform an extensive evaluation on 6 widely used programming languages, \ie \java, \js, \cxx, \php, \swift, and \rust, using the \multiple{} benchmark~\cite{multiple}.
We report available results from the \wizardcoder{} paper~\cite{wizardcoder} and evaluate our models consistently through \bigcodeharness~\cite{bigcode-evaluation-harness}.
We skip proprietary models such as \chatgpt{} and \gptfour{} as they are not supported by the framework.
Due to a significant inference latency when running \wizardcoderc-7B using the harness in our environment, we choose not to include it in our analysis.

The results indicate that \modelc{} improves the base \codellamapy{-7B} by a large margin among all the studied programming languages.
Moreover, \modelc{} also achieves better results than the SOTA 15B \wizardcodersc{} among half of the programming languages.
Additionally, \modelxc{} demonstrates further improvement over \modelc{} on all programming languages, achieving comparable performance against \wizardcoderc-34B with only 7B parameters.
It is worth noting that \modelc{} is only trained with very limited multilingual data but still outperforms other \llm{s} with similar or even larger sizes.
Also, although the harness evaluates models in \emph{completion} formats which are for base models, \model{s} still show significant improvements despite being only \emph{instruction-tuned}. This implies that \llm{s} can learn knowledge from the data beyond its format.

\begin{table*}
\caption{\Passat{1} results of different \llm{s} on \multiple~\cite{multiple} following the same hyperparameter settings as the \wizardcoder{} paper~\cite{wizardcoder}: $\temperature=0.2$, $\topp=0.95$, $\maxLen=512$, and $\nsamples=50$.
We evaluate all 7B models using \bigcodeharness{}~\cite{bigcode-evaluation-harness} and report other results from \wizardcoder{}.}
\label{tab:multilang}
\centering
\begin{booktabs}{
    colspec={@{}lrrrrrrr@{}},
    cell{1}{3}={c=6}{c},
    cell{1}{1-2}={r=2}{m},
    cell{6}{3-Z}={font=\bfseries},
    cell{Z}{3-Z}={font=\bfseries},
    row{Y-Z} = {bg=lightb}
}
\toprule
Model & Size & Programming Language & & & & & \\
\cmidrule[lr]{3-8}
& & \java & \js & \cxx & \php & \swift & \rust \\
\midrule

\codellama & 34B & 40.2 & 41.7 & 41.4 & 40.4 & 35.3 & 38.7 \\
\codellamapy & 34B & 39.5 & 44.7 & 39.1 & 39.8 & 34.3 & 39.7 \\
\codellamainst & 34B & 41.5 & 45.9 & 41.5 & 37.0 & 37.6 & 39.3 \\
\wizardcoderc & 34B & 44.9 & 55.3 & 47.2 & 47.2 & 44.3 & 46.2 \\

\seprule

\starcoderbase & 15B & 28.5 & 31.7 & 30.6 & 26.8 & 16.7 & 24.5 \\
\starcoder & 15B & 30.2 & 30.8 & 31.6 & 26.1 & 22.7 & 21.8 \\
\wizardcodersc & 15B & 35.8 & 41.9 & 39.0 & 39.3 & 33.7 & 27.1 \\

\seprule

\codellama & 7B & 29.3 & 31.7 & 27.0 & 25.1 & 25.6 & 25.5 \\
\codellamapy & 7B & 29.1 & 35.7 & 30.2 & 29.0 & 27.1 & 27.0 \\
\seprule
\modelc & 7B & 36.4 & 45.9 & 36.5 & 39.5 & 33.4 & 30.6 \\
\modelxc & 7B & 42.9 & 57.5 & 44.4 & 47.6 & 44.1 & 40.3 \\
\bottomrule
\end{booktabs}
\end{table*}
\begin{table*}
\centering
\caption{\Passat{1} results on \dsonek{} (completion format) with $\temperature=0.2$, $\topp=0.5$, $\maxLen=1024$, and $\nsamples=40$, following the same hyperparameter setting used in \wizardcoder~\cite{wizardcoder}.
We evaluate all the 7B models with their preferred prompt formats and report other results from \wizardcoder{}.
}
\label{tab:ds1000}
\newcommand\rotateHeader[1]{\rotatebox{0}{#1}}
 \begin{booktabs}{
        columns = {colsep=4pt},
        colspec={@{}lrrrrrrrrr@{}},
        cell{1}{1-2} = {valign=f},
        cells = {cmd={}},
        row{Y-Z} = {bg=lightb},
        cell{Z}{3-Z}={font=\bfseries}
    }       
        \toprule
        Model & Size & {+ 155\\\matplotlib} & {+ 220\\\numpy} & {+ 291\\\pandas} & {+ 68\\\torch} & {+ 106\\\scipy} & {+ 115\\\sklearn} & {+ 45\\\tensorflow} & {= 1000\\Overall}\\
        \midrule
        \incoder & 6.7B & 28.3 & 4.4 & 3.1 & 4.4 & 2.8 & 2.8 & 3.8 & 7.4\\
        \codegenmono & 16B & 31.7 & 10.9 & 3.4 & 7.0 & 9.0 & 10.8 & 15.2 & 11.7\\
        \codecush & - & 40.7 & 21.8 & 7.9 & 12.4 & 11.3 & 18.0 & 12.2 & 18.1\\
        \seprule
        \starcoder & 15B & 51.7 & 29.7 & 11.4 & 21.4 & 20.2 & 29.5 & 24.5 & 26.0\\
        \wizardcodersc & 15B & 55.2 & 33.6 & 16.7 & 26.2 & 24.2 & 24.9 & 26.7 & 29.2\\
        \seprule
        \codellamapy & 7B & 55.3 & 34.5 & 16.4 & 19.9 & 22.3 & 17.6 & 28.5 & 28.0 \\
        \wizardcoderc & 7B & 53.5 & 34.4 & 15.2 & 25.7 & 21.0 & 24.5 & 28.9 & 28.4 \\
        \seprule
        \modelc & 7B & 54.6 & 34.8 & 19.0 & 24.7 & 25.0 & 22.6 & 28.9 & 29.9\\
        \modelxc & 7B & 55.9 & 40.6 & 28.4 & 40.4 & 28.8 & 35.8 & 37.6 & 37.5\\
        \bottomrule
    \end{booktabs}%
\end{table*}\begin{table*}
 \caption{\Passat{1} (greedy decoding) comparison between \model{} and \dscoder{}~\cite{dscoder} on \humaneval{}~(+) and \mbpp{}~(+). \dscoder{} results are reported from \evalplus{}~\cite{liu2023code} Leaderboard.}
        \centering
\label{tab:dscoder-text2code}
        \begin{booktabs}{
            colspec={@{}lrrrrcc@{}},
            cell{1}{1-3} = {r=2}{m},
            cell{1}{4,6} = {c=2}{c},
            column{4} = {rightsep=10pt},
            column{5} = {leftsep=10pt},
            row{Y-Z} = {bg=lightb}
        }
        \toprule
        Model & Size & {Training Tokens} & Benchmark & & Open-Source &\\
        \cmidrule[lr]{4-7}
        & & & \humaneval~(+) & \mbpp~(+) & Weight & Data \\
        \midrule
        \SetCell[r=3]{m}{\dscoderbase}
            & 1.3B & 2T & -  & 55.4~~(46.9) & \CIRCLE & \Circle \\
            & 6.7B & 2T & 47.6~~(39.6)  & 70.2~~(56.6) & \CIRCLE & \Circle \\
            & 33B & 2T & 51.2~~(43.3)  & - & \CIRCLE & \Circle \\
        \seprule
        \SetCell[r=3]{m}{\dscoder{} Instruct}
            & 1.3B & +2B & 64.6~~(58.5)  & 63.7~~(53.1) & \CIRCLE & \Circle \\
            & 6.7B & +2B & \textbf{73.8}~~(\textbf{70.1})  & \textbf{72.7}~~(\textbf{63.4}) & \CIRCLE & \Circle \\
            & 33B & +2B & \textbf{78.7}~~(\textbf{72.6})  & \textbf{78.7}~~(\textbf{66.7}) & \CIRCLE & \Circle \\
        \seprule
        \modeld{}  & 6.7B & +90M & 66.5~~(60.4) & 75.4~~(61.9) & \CIRCLE & \CIRCLE \\
        \modelxd{}  & 6.7B & +240M & \textbf{76.8}~~(\textbf{70.7}) & \textbf{75.7}~~(\textbf{64.4}) & \CIRCLE & \CIRCLE \\
        \bottomrule
\end{booktabs}%
\end{table*}
\subsection{Code Generation for Data Science}

The \dsonek{} dataset~\cite{ds1000} contains 1K distinct data science coding issues ranging from 7 popular data science libraries in \python.
It evaluates the realistic and practical use case of an \llm{} and offers unit tests for validating each problem.
\dsonek{} has both \emph{completion} and \emph{insertion} modes, but here we only evaluate \emph{completion} because the base \codellamapy{} does not support infilling.
\Cref{tab:ds1000} shows the evaluation results where 
we include the recent \incoder~\cite{incoder}, \codegen~\cite{codegen}, \codecush~\cite{codecush}, \starcoder~\cite{starcoder}, \codellamapy~\cite{codellama}, and \wizardcoder~\cite{wizardcoder}.
We can see from the table that \modelc-7B already outperforms all the baselines we evaluate, including state-of-the-art \wizardcoderc-7B and \wizardcodersc-15B. \modelxc{}-7B further breaks the limit by introducing an 8.3 percentage point absolute improvement over \wizardcodersc-15B.

\subsection{Comparison with \dscoder}
\label{sec:comparison-with-dscoder}

\dscoder{}~\cite{dscoder} is a series of models released concurrently to our work and they demonstrate superior coding performance.
We only briefly discuss it in this section because its data and instruction tuning details are not publicly available at the time of writing.
We apply the same finetuning strategy on \dscoderbase-6.7B as we performed on \codellamapy-7B, leading to \modeld{} and \modelxd{}.
\Cref{tab:dscoder-text2code} shows a similar trend as \Cref{tab:python-text2code} that the base model can be significantly improved after applying \tech{}.
Remarkably, the \modelxd{} variant surpasses \dscoderinst-6.7B on all the benchmarks with $\times$8 fewer training tokens, and it also closely matches \dscoderinst-33B on these datasets.
\section{Ablations of Data Source}
\subsection{Impact of the Language Distribution}

To understand the correlation between the programming languages appearing in the training data and the downstream performance of different languages, we conduct an additional ablation study about the training data.
We classify the 75K training data into approximately 43K \python-only, and 32K non-\python{} data according to whether \verb|```python| is a substring of the generated data.
We do not classify the data based on the seed code snippet because \llm{}s performing \tech{} may produce code in a different programming language than the seed.

\begin{table*}
\caption{Ablation study of using different programming languages as training data.
We show the \passat{1} results on \humanevalp{}~\cite{liu2023code} for \python{} and the average \passat{1} results on \multiple{}~\cite{multiple} for the same set of programming languages used in \Cref{tab:multilang} (\ie \java, \js, \cxx, \php, \swift, and \rust).
All the variants are finetuned with 2 epochs and evaluated through greedy-decoding.
}
\label{tab:ablation}
\centering
\begin{booktabs}{
    colspec={@{}llrr@{}},
}
\toprule
Model~(7B) & Finetuning Data & \python~(\humanevalp) & Others~(\multiple) \\
\midrule
\codellamapy & - & 34.1 & 29.6 \\
\seprule
\modelc & \python{} (43K) & 47.6 & 32.7 \\
\modelc & Others (32K) & 44.5 & \textbf{38.3} \\
\seprule
\modelc & Both (75K) & \textbf{55.5} & 37.8  \\
\bottomrule
\end{booktabs}%
\end{table*}

\Cref{tab:ablation} shows the evaluation results, where we consistently finetune the base \codellamapy-7B for 2 epochs on different data partitions using the same training hyperparameters explained in \Cref{apd:setup}. 
From the table, we can see that, as can be imagined, training on \python{} or non-\python{} data can substantially boost the performance of the base model in \python{} or non-\python{} tasks, respectively.
Interestingly, instruction tuning on different programming languages can still boost the overall coding performance that includes out-of-distribution languages.
For example, when trained on only non-\python{} data, \modelc{} still achieves a 10.4 percentage point improvement over the base model in the \python{}-only evaluation.
This implies \llm{s} can establish correlations between different programming languages and perform transfer learning of deeper code semantics.
Finally, we observe a more significant boost in \python{} evaluation when combining data from both sources, with a slight decrease in multilingual performance compared with only finetuning on multilingual data.
We attribute this decrease to the dominant amount of \python{} data (around 57\%) during instruction tuning.

\subsection{\tech{} vs. Direct Finetuning}
\begin{table}
\caption{Comparison between \tech{} and directly finetuning on comment-function pairs with \codellamapy-7B as the base model.}
\label{tab:pair-ablation}
\centering
\begin{booktabs}{
    colspec={@{}lrr@{}},
}
\toprule
Finetuning Data & \humanevalp &\multiple \\
\midrule
Base model w/o finetuning & 34.1 & 29.6 \\
Comment-function pairs (75K) & 34.1 & 24.1\\
\tech{} (75K) & \textbf{55.5} & \textbf{37.8} \\
\bottomrule
\end{booktabs}%
\end{table}

\label{sec:direct-tune}
The fact that \tech{} gets an \llm{} inspired from open-source code snippets may lead to a natural question: why not directly finetuning on these open-source code?
To answer this question, we follow \codesearchnet~\cite{codesearchnet} to mine semantically relevant \emph{comment-function} pairs from the same seed document corpus we use to construct the 75K \tech{} dataset.
We then train the model to predict the function bodies from the function signatures and comments.
We prioritize comment-function pairs that overlap with our \emph{75K seed snippets}, resulting in about 11K data points.
To align with our 75K samples, we collect the remaining 64K samples using the whole corpus of \emph{75K seed documents}.
Eventually, we have the same number of comment-function pairs with \tech{} data.

We finetune the base \codellamapy-7B for 2 epochs using the paired data, following the same training setup discussed in \Cref{apd:setup}.
From \Cref{tab:pair-ablation}, we observe that finetuning on 75K paired comment-function data even worsens the base model, while \tech{} helps to introduce a substantial boost.
We conjecture that the degradation is owing to the substantial noise and inconsistency that exists intrinsically in the data pairs, even though these paired data exhibit \emph{very similar} format as \humaneval{} or \multiple{} problems.
This further shows that data factuality, rather than the format, is essential to code instruction tuning. It also indicates the superiority of \tech{} which can translate these loosely related code fragments into semantically-consistent instruction-tuning data.

\subsection{\tech{} with A Less Powerful Teacher}

In this section, we explore the factors contributing to the effectiveness of \tech{} beyond just the distillation of the teacher model. We propose two potential key reasons.
First, since the base model is pretrained with comprehensive code data, the distillation process likely activates the model's internal capabilities, leading to improved performance in coding tasks.
Second, \tech{} uses seed code snippets to generate problem-solution pairs in one shot. These seed snippets provide valuable context, enabling the model to create better solutions than a plain teacher model lacking such seed information. These enhanced solutions can then be used to train more effective student models.
To verify these points, we conduct an additional experiment by generating a subset of 20K \tech{} data using \mixtralfull~\cite{mixtral}, a state-of-the-art, general-purpose, open-source \llm{}.

\begin{table}[htbp]
\caption{\Passat{1} on \humanevalp{} and \mbppp{} when finetuning \codellamapy{-7B} for 2 epochs on 20K \tech{} data generated by \mixtralfull{}~\cite{mixtral}.}
\label{tab:mixtral}
\centering
\begin{booktabs}{
    colspec={@{}lrr@{}},
}
\toprule
Model & \humanevalp{} & \mbppp\\
\midrule
\mixtralfull{} & 39.6 & 47.4\\
\codellamapy{-7B} & 34.1 & 45.4\\
\modelc{-Mixtral}-7B & \textbf{55.5} & \textbf{50.4}\\
\bottomrule
\end{booktabs}%
\end{table}

\Cref{tab:mixtral} indicates that \modelc{}-\mixtral{}-7B not only significantly improves over the base \codellamapy{}, but is also better than \mixtralfull{} (\ie the teacher model) across \humanevalp{} and \mbppp{}.
These results suggest that \tech{} is not simply distilling a teacher model, but also triggering the base modelâ€™s own capability and effectively leveraging the information encapsulated in seed code snippets.
\section{Related Work}

\paragraph{Foundation models for code}

Trained over billions of lines of code, \llm{s} have demonstrated outstanding performance in a wide range of software engineering tasks, including code generation~\cite{codex, synthesisllm}, program repair~\cite{alpharepair, wei2023copiloting, aprstudy, aprstudy2, bouzenia2024repairagent}, and software testing~\cite{fuzz4all, titanfuzz, yuan2023no, schafer2023empirical, lemieux2023codamosa}.
In particular, prominent base models, such as \codegen{}~\cite{codegen}, \codetf~\cite{codet5}, \starcoder{}~\cite{starcoder}, and \codellama{}~\cite{codellama}, are pre-trained over a huge number of codebase from scratch, establishing the fundamental ability of general code generation and understanding.
More recent code \llm{s}, such as \dscoder{}~\cite{dscoder} and StarCoder2~\cite{starcoder2}, additionally organize the pretraining data at the repository level to enhance the model's contextual understanding capabilities.
Furthermore, these base models are also finetuned~\cite{wizardcoder} or prompted~\cite{selfdebug} to unlock their true potential to specialize in solving domain-specific coding tasks.

\paragraph{Instruction tuning with synthetic data}
Instruction tuning aims to improve
pretrained \llm{s} by finetuning them with a mixture of instructions and corresponding responses~\cite{wei2022finetuned}.
However, obtaining high-quality instructional data
{is oftentimes laborious}.
Hence, researchers are increasingly focusing on the development of methods to generate synthetic instruction data.
\citet{selfinstruct} introduces \emph{\selfinstruct{}}, where a foundation \llm{} (\gptthree~\cite{gpt3}) is used to generate synthetic instruction-response pairs with carefully crafted prompts. The \emph{same} \llm{} is then instruction-tuned on the synthetic data to distill such self-generated knowledge.
{
This technique has been further extended to create synthetic data with different \llm{s}}.
For example, \alpaca{}~\cite{alpaca} and \codealpaca{}~\cite{codealpaca} apply \selfinstruct{} to finetune \llama{} with \chatgpt-generated instructions. 
To improve \selfinstruct{}, \wizardlm~\cite{xu2023wizardlm} and \wizardcoder~\cite{luo2023wizardcoder} propose \evolinstruct{} and Code \evolinstruct{} by guiding \chatgpt{} with heuristic prompts to make the synthetic data more complex and diverse.
More recently, \citet{phi} shows that textbook-quality synthetic data alone can help the model achieve remarkable coding and reasoning capabilities.
Orthogonal to all existing methods, our proposed \tech{} allows \llm{s} to get inspired from real-world code snippets for better controllability, quality, and creativity in coding tasks.

\paragraph{Evaluating \llm{s} for code}

Most code benchmarks evaluate \llm{s} on generating \emph{single-function} programs from natural language descriptions.
Such benchmarks include \humaneval{}~\cite{codex}, \mbpp{}~\cite{synthesisllm}, \apps{}~\cite{apps}, and \codecontests~\cite{codecontests}.
A handful of manual tests are used to assess the functional correctness of \llm{-generated} solutions.
However, insufficient tests can lead to false negatives. Consequently, the \evalplus{} framework~\cite{liu2023code} produces \humaneval{+} and \mbpp{+} by extending 80$\times$/35$\times$ more tests.
To address dataset contamination issues, researchers propose LiveCodeBench~\cite{lcb}, which compiles fresh coding problems not included in model training, and EvoEval~\cite{evoeval}, which strategically leverages LLMs to evolve existing benchmarks into new coding tasks.
Meanwhile, there are comprehensive benchmarks evaluating code generation for data science (\dsonek~\cite{ds1000}), addressing open-source issues (\swebench~\cite{swebench}), and repository-level code generation (\crosscodeeval{}~\cite{cceval} and \repoeval{}~\cite{repocoder}).

\section{Conclusion and Future Work}
We propose \tech, a novel data generation method using \llmfull{}s to generate diverse coding challenges from open-source code snippets.
This approach enables \model{}, which significantly improves the base \llm. Despite having less than 7B parameters, it
can outperform all evaluate
\llm{s} with less than or equal to 16B parameters, including the 15B \wizardcoder{}.
Combining \tech{} with \evolinstruct{} allows us to build the enhanced \modelx{} models.
They achieve remarkable results by rivaling leading models like \chatgpt{} in \humaneval{} benchmarks.
We fully open source the model weights, training data, and source code, to enable future research in \llm{s} for code.
In the near future, we will apply \tech{} to larger base models. We will also continue advancing \tech{} by generating higher-quality data with a strategically designed distribution of the seed code snippets and with more advanced teacher \llm{s} such as \gptfour. 

\section*{Acknowledgement}
We thank all the reviewers for their insightful comments and suggestions for our paper. This work was partially supported by NSF grant CCF-2131943, as well as Kwai Inc.

\section*{Impact Statement}
This work is motivated to boost large language models in terms of their code generation and understanding capabilities through instruction tuning.
The proposed \tech{} method leverages the abundance of open source to generate diverse and controllable instruction data.
We expect this idea to also foster innovative software solutions tailored to domain-specific needs, particularly in areas where real data is private and scarce, by generating extensive synthetic data.
Additionally, our method reinforces the value of community-driven content and knowledge sharing by incorporating open-source code as references.

However, it is essential to recognize the potential for misuse, such as the deliberate generation of vulnerable code that can be exploited for malicious purposes.
Ultimately, adhering to ethical guidelines is crucial to ensure the responsible use of this technique.

\newpage

\end{document}