\title{An Image Is Worth 1000 Lies: Adversarial Transferability across Prompts on Vision-Language Models}

\begin{document}

\maketitle

\begin{abstract}
Different from traditional task-specific vision models, recent large VLMs can readily adapt to different vision tasks by simply using different textual instructions, i.e., prompts. However, a well-known concern about traditional task-specific vision models is that they can be misled by imperceptible adversarial perturbations. Furthermore, the concern is exacerbated by the phenomenon that the same adversarial perturbations can fool different task-specific models. Given that VLMs rely on prompts to adapt to different tasks, an intriguing question emerges: Can a single adversarial image mislead all predictions of VLMs when a thousand different prompts are given? This question essentially introduces a novel perspective on adversarial transferability: cross-prompt adversarial transferability. In this work, we propose the Cross-Prompt Attack (CroPA). This proposed method updates the visual adversarial perturbation with learnable prompts, which are designed to counteract the misleading effects of the adversarial image. By doing this, CroPA significantly improves the transferability of adversarial examples across prompts. Extensive experiments are conducted to verify the strong cross-prompt adversarial transferability of CroPA with prevalent VLMs including Flamingo, BLIP-2, and InstructBLIP in various different tasks. Our source code is available at \url{https://github.com/Haochen-Luo/CroPA}.
\end{abstract}

\section{Introduction}
Previously task-specific vision models have demonstrated remarkable capabilities in various visual tasks such as image classification~\citep{he2016deep} and image captioning~\citep{yao2018exploring,yang2019auto}. These models are designed to extract specific information which is pre-defined during the construction phase of the model. Recently, large Vision-Language Models (VLMs)~\citep{gu2023systematic,li2022blip,alayrac2022flamingo,li2023blip,zhu2023minigpt} have emerged, providing a more unified approach to addressing computer vision tasks. Instead of relying merely on the image input, VLMs integrate information from both images and associated textual prompts, enabling them to perform varied vision-related tasks by utilizing appropriate prompts. This versatility paves the way for exploring various visual tasks.

Those task-specific models are known to be vulnerable to adversarial examples~\citep{goodfellow2014explaining,szegedy2013intriguing,wu2022towards,gu2022segpgd}. These examples are developed by adding subtle perturbations, typically invisible to humans, to original samples. These perturbations can significantly degrade the performance of such models significantly~\citep{madry2017towards}. Compounding the mentioned vulnerabilities, the adversarial examples further exhibit transferability across models~\citep{gu2023survey}. This implies that these adversarial examples can attack target models beyond those for which they were specifically crafted~\citep{liu2016delving,tramer2017space}.

Given that adversarial examples have transferability across tasks~\citep{salzmann2021learning_transferable_nips,gu2023survey}, an interesting question emerges: Is an adversarial example transferable \textit{across prompts}? We dub it cross-prompt adversarial transferability, which means that regardless of the prompts provided, the model output can consistently be misled by an adversarial example. For example, if the target text is set to ``unknown", a model, deceived by an adversarial example exhibiting cross-prompt transferability, will always predict the word ``unknown" regardless of the prompts. In this case, VLMs are incapable of extracting information from the image, even when different textual prompts are presented.

Exploring cross-prompt adversarial examples is crucial to revealing the prompt-related vulnerability of VLM and protecting image information. From the perspective of an attacker, adversarial examples with high cross-prompt transferability can mislead large VLMs to generate malicious outputs even when queried with various benign prompt questions. From the defensive perspective, the potential to obfuscate image information through human-imperceptible perturbations can cause the model to uniformly output a predefined target text. This can prevent malicious usage of large VLMs for unauthorized extraction of sensitive information from personal images.

Our experiments have shown the cross-prompt transferability created with a single prompt is highly limited. An intuitive approach to increase the cross-prompt transferability is to use multiple prompts during its creation stage. However, the improvement in cross-prompt transferability of these baseline approaches converges quickly with the increase in prompts. To further improve the cross-prompt transferability, we proposed Cross-Prompt Attack (CroPA), which creates more transferable adversarial images by utilising the learnable prompts. These prompts are optimised in the opposite direction of the adversarial image to cover more prompt embedding space.

In the experiments, prompts for three popular vision-language tasks are used, including image classification, image captioning, and visual question answering (VQA). We examined the effectiveness of our approach on three prevalent VLMs,  Flamingo~\citep{alayrac2022flamingo}, BLIP-2~\citep{li2023blip} and InstructBLIP~\citep{Dai2023InstructBLIP}. Experimental results have demonstrated that CroPA consistently outperforms the baseline methods under different settings with different attack targets. 

Our contributions can be summarized as follows:
\vspace{-0.2cm}
\begin{itemize} 
    \item  We introduce cross-prompt adversarial transferability, an important perspective of adversarial transferability, contributing to the existing body of knowledge on VLMs' vulnerabilities.
    \item  We propose a novel algorithm Cross-Prompt Attack (CroPA), designed to enhance cross-prompt adversarial transferability.
    \item  Extensive experiments are conducted to verify the effectiveness of our approach on various VLMs and tasks. Moreover, we provide further analysis to understand our approach.
\end{itemize}

\section{Related Work}
\vspace{-0.3cm}
\noindent\textbf{Adversarial transferability} Foundational studies by ~\citep{szegedy2013intriguing,goodfellow2014explaining} unveil the property of neural networks to misclassify images by adding seemingly imperceptible adversarial perturbations to the inputs. The created adversarial samples can also fool unseen models~\citep{gu2023survey,yu2023reliable,liu2016delving,papernot2016transferability}. Besides, ~\cite{mopuri2017fast,moosavi2017universal} shows that an adversarial perturbation can be still deceptive when added to different images. Beyond models and images, the domain of adversarial transferability extends its reach to different tasks~\citep{naseer2018task,naseer2019cross,lu2020enhancing,salzmann2021learning_transferable_nips}. For example, adversarial examples designed to attack image classification systems are not limited in their scope but also fail other tasks, such as object detection. In light of the revealed transferability of adversarial examples across models, images, and tasks, the recent advancements in vision-language models introduce a new dimension to be explored. Specifically, this work delves into the transferability across textual prompts within the realm of VLMs.

\noindent\textbf{Adversarial Robustness of Vision-Language Models} The majority of prior research on adversarial attacks on vision-language models are mostly task-specific attacks. For example, there is a series of works to manipulate the model output in image captioning tasks~\citep{xu2019exact-cap-attack,zhang2020fooled-cap-attack,aafaq2021controlled,chen2017attacking}. Similarly, in visual question answering, works such as Fooling VQA~\citep{xu2018fooling,kaushik2021efficacy,kovatchev2022longhorns,li2021adversarial,sheng2021human,zhang2022towards} mislead the attention region in object detectors to affect the model output. Nevertheless, the vision-language models used in these methods are highly task-specific, utilizing lightweight CNN-RNN architectures that lack the capability for in-context learning. Consequently, adapting these methods to contemporary VLMs gives challenges. There are recent works on the adversarial robustness of large VLMs that consider the adversarial attack from the vision modality. Concretely, ~\cite{zhao2023evaluating-vlm-robustness} explored the adversarial robustness of recent large vision-language models such as BLIP~\citep{li2022blip} and BLIP-2~\citep{li2023blip} under the black box setting including query-based and transfer-based methods to craft adversarial examples. Instead of transferability across models, this work introduces cross-transferability.

\section{Approach}
In this section, we first describe the concept of cross-prompt adversarial transferability. We then present the 
baseline approach utilising one or multiple prompts to craft adversarial examples, and the CroPA method which incorporates learnable prompt to enhance cross-prompt transferability.   

\subsection{Problem Formulation}

Consider \( x_v \) to be a clean image without perturbations induced and let \( x_t \) denote a prompt. The function \( f \) represents a VLM. The term \( \delta_v \) signifies the visual perturbation added to the image \( x_v \) and is bound by the constraints \( \|\delta_v\|_p \leq \epsilon_v \), where $\epsilon_v$ is the image perturbation magnitude.

\begin{itemize}
    \item \textbf{Targeted Attack:} In a targeted attack, the objective is to generate a visual perturbation, denoted as $\delta_v$, which when applied to the original input \( x_v \), creates an adversarial example $x_v+\delta_v$. This adversarial example is structured to mislead the model into producing a predefined targeted output text $T$, regardless of the given prompt. 
    \item \textbf{Non-Targeted Attack:} 
    Contrarily, in a non-targeted attack, the adversarial example is crafted not to lead the model to a specific predefined output but rather to any incorrect output. The goal here is to ensure that the model's output, when fed with the adversarial example, diverges from the output generated with a clean, unaltered image as input.
\end{itemize}
Attack success rate (ASR) is the evaluation metric for cross-prompt transferability, which is defined as the ratio of the number of successful attacks to the total number of attacks. For the targeted attack, the attack is considered to be successful only if the prediction exactly matches our target text. For non-targeted attacks, the attack is successful if the model is misled to generate the text different from the prediction with the clean image.

\subsection{Baseline Approach} 

To generate adversarial examples for VLMs, an image perturbation can be optimized based on a single prompt; this method is referred to as \textbf{Single-P}. To enhance the cross-prompt transferability of the perturbations, a straightforward approach is to utilize multiple prompts while updating the image perturbation, a method denoted by \textbf{Multi-P}.
The algorithms of Single-P and Multi-P are detailed below.

Let \(\mathcal{X}_t = \{x_t^1, x_t^2, \dots, x_t^k\}\) represent a collection of textual prompt instances. The ultimate goal is to derive a visual perturbation, \(\delta_v\), ensuring that for every instance from \(\mathcal{X}_t\), the model yields either the predefined target text \(T\) in the targeted attack setting, or text deviating from the original output in a non-targeted attack setting.

The optimization objectives for targeted and non-targeted settings are formulated as follows:
For the targeted attack, the objective is to minimize the language modelling loss, \( \mathcal{L} \), associated with generating the target text \( T \). This optimization can be mathematically represented as:
\begin{equation}
\small
        \underset{\delta_v}{\text{min}} \sum_{i=1}^{k} \mathcal{L}(f(x_v + \delta_v, x_t^i), T)
\end{equation}
Here, the goal is to alter the input subtly to mislead the model into producing the predefined text \( T \) across various prompt instances, effectively minimizing the discrepancy between the model's output and the target text.

For the non-targeted attack, the objective is to maximize the language modelling loss \( \mathcal{L} \), between the text produced by the model with adversarial examples and clean images: \( \small
        \underset{\delta_v}{\text{max}} \sum_{i=1}^{k} \mathcal{L}(f(x_v + \delta_v, x_t^i), f(x_v, x_t^i))\).
The aim here is not to guide the model to a specific output but to any output diverging from what would have been produced with an unaltered input, emphasizing the maximization of the discrepancy in the model's responses. 

\subsection{Cross-Prompt Attack (CroPA)}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/FlowChart.pdf}\vspace{-0.2cm}
    \caption[Overview of CroPA’s framework]
    {\footnotesize{
    Overview of CroPA’s framework under the targeted attack setting. Both the image perturbation $\delta_v$ and the prompt perturbation $\delta_t$ are learnable but \textcolor{black}{the prompt perturbation does not collaborate  with $\delta_v$ to deceive the model.} They are optimised with the opposite goals:  $\delta_v$ aims to minimise the language modelling loss while the $\delta_t$ aims to maximise the language modelling loss. 
    The update frequency of the image perturbation and prompt perturbation can be different.
    }}
    \label{fig:dapa-pipeline}
\end{figure}

In order to create an adversarial image with stronger cross-prompt transferability, we propose an algorithm termed Cross-Prompt Attack (\textbf{CroPA}). The baseline approach constrains prompts to decoded text representations, namely the fixed hard prompt. In the CroPA framework, we not only used varied numbers of prompts but also introduced learnable prompt perturbation for the prompt embedding during the optimisation phase. 

 In Figure~\ref{fig:dapa-pipeline}, we provide the illustration of the CroPA framework through an example, where the image perturbation is optimised with the target text ``unknown" to hide sensitive information such as address and gender. The image perturbation $\delta_v$ is optimised to minimise the loss of generating the target text ``unknown", while the prompt perturbation  $\delta_t$ is updated in the opposite direction to maximise the loss of generating the target text. The prompt perturbation gains increasingly stronger cross-prompt transferability during this competitive update process.
    

Concretely, the optimisation steps of the adversarial image are detailed in Algorithm~\ref{algo:alter}. In the beginning, the image adversarial perturbation and the prompt adversarial perturbation are randomly initialised. During the forward pass, these two perturbations are added to the clean image and the clean prompt embedding respectively. After the forward pass, the gradient of language modelling loss with respect to the image and text can be obtained through backward propagation.  While the image perturbation is updated with gradient descent to minimize the language modelling loss between the prediction and the target sentence, the adversarial prompt is updated with gradient ascent to maximize the loss. The parameters $\alpha_1$ and $\alpha_2$ are the updated step sizes for the adversarial image and adversarial prompts. The optimisation algorithm is PGD~\citep{madry2017towards} with the L-infinity norm being the specific norm used in the experiments.

The update of the adversarial image and the adversarial text can be viewed as a min-max process. Considering a vision-language model \( f \) that takes an image \( x_v \) and a text prompt \( x_t \) as input, the objective is to obtain the perturbations \( \delta_v \) for \( x_v \) that minimises the language modeling loss \( \mathcal{L} \) of generating the targeted sentence, and the perturbations \( \delta_t \) for $x_t$ that maximises the loss.   For the targeted attack, the optimisation of the formula can be written as:
    \begin{equation}
        \min_{\delta_v} \max_{\delta_t} \mathcal{L}(f(x_v + \delta_v, x_t + \delta_t), T)
    \end{equation}
    \vspace{-0.1cm}
Similarly, for the non-targeted attack, the optimisation can be expressed as:
    $\displaystyle
    \max_{\textcolor{black}{\delta_v}} \min_{\textcolor{black}{\delta_t}} \mathcal{L}(f(x_v + \delta_v, x_t + \delta_t), f(x_v , x_t ))$.
The visual perturbation $\delta_v$ is optimised to maximise the loss of generating $f(x_v , x_t )$ so that the model is deceived to generate the output different from the original one. Prompt perturbation $\delta_t$ is optimised to minimise the language modelling loss of generating $f(x_v , x_t )$.  
For both targeted attack and non-targeted attack, the image perturbation is clipped to the $\epsilon$ to ensure the invisibility of the image perturbation. 
We use the parameter N denoting the update interval to control the update frequency of image perturbation and prompt perturbation: the image perturbation for N times, and the prompt perturbation updates once.  \textcolor{black}{Please note that the prompt perturbations are added only during the optimisation phase and they are not added during the testing phase.}

    

\begin{algorithm}[t]
\footnotesize
\caption{CroPA: Cross Prompt Attack}
\label{algo:alter}
\begin{algorithmic}[1]
\REQUIRE Model \(f\), Target Text $T$, vision input \(x_v\),  prompt set \(X_t\), perturbation size \(\epsilon\),  step size of perturbation updating \(\alpha_1\) and \(\alpha_2\), number of iteration steps \(   K\), adversarial prompt update interval \(N\)
\ENSURE Adversarial example \(x_v'\)
\STATE Initialise \(x_v' = x_v\)
\FOR{step =1 to \(K\)}
    \STATE Uniformally sample the prompt \(x_t^{i}\) from \(\mathcal{X}_t\)
    \IF{\(x_t^{i}{'}\) is not initialised} 
        \STATE Initialise \(x_t^{i}{'} = x_t^i\)
    \ENDIF
    \STATE Compute gradient for adversarial image : \(g_v = \nabla_{x_v}\mathcal{L}(f(x_v', x_t^i), T)\)
    \STATE Update with gradient descent: \(x_v' = x_v' - \alpha_1 \cdot \text{sign}(g_v)\)
    \IF{mod(step, N) == 0}
         \STATE Compute gradient for adversarial prompt: \(g_t = \nabla_{x_t}\mathcal{L}(f(x_v', x_t^i), T)\)
        \STATE Update with gradient ascent: \(x_t^{i}{'} = x_t^{i}{'} + \alpha_2 \cdot \text{sign}(g_t)\)
    \ENDIF
    \STATE Project \(x_v'\) to be within the \(\epsilon\)-ball of \(x_v\): 
    \(x_v' = \text{Clip}_{x_v,\epsilon}(x_v')\)
\ENDFOR
\RETURN \(x_v'\)
\end{algorithmic}
\end{algorithm}

\section{Experiments}
\vspace{-0.4cm}

\textbf{Experimental Settings}
The dataset consists of both images and prompts. The images are collected from the validation dataset of MS-COCO datasets~\citep{lin2014microsoft-mscoco}. The prompts for VQA consist of questions both agnostic and specific to the image content, which are referred as to VQA\textsubscript{general} and VQA\textsubscript{specific} in the following sections. The image-specific questions derive from the VQA-v2~\citep{goyal2017making-vqav2}. We craft prompts for the questions agnostic to image content, image classification, and image captioning with diverse lengths and semantics. By default, the experiments are targeted attacks with the target text set to ``unknown" to avoid the inclusion of high-frequency responses in vision-language tasks. Adversarial examples are optimised and tested under 0-shot settings. The number of prompts for Multi-P and CroPA is set to ten. Detailed prompts can be found in Appendix~\ref{app:prompts}. The VLMs used are Flamingo, BLIP-2, and InstructBLIP. We adopt the open-source OpenFlamingo-9B~\citep{awadalla2023openflamingo} for Flamingo. Attack Success Rate is used as a metric in our experiments. All the ASR scores reported in the following sections are averaged over three runs. \textcolor{black}{The perturbation size is set to 16/255.}

\vspace{-0.2cm}
\subsection{Cross-Prompt Transferability Comparison}
\label{sec:prompt-num}

The cross-prompt adversarial transferability is expected to be stronger if more prompts are given during the optimisation stage. To verify this assumption, we sample different numbers of prompts: 1, 5, 10, 50, and 100, and test the targeted ASR. The overall performance of the ASR of the baseline methods and CroPA with different numbers of prompts tested with Flamingo, BLIP-2 and InstructBLIP are shown in Figure~\ref{fig:number of prompts}.
\vspace{-0.1cm}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.98\linewidth]{figures/combined_plot.pdf}\vspace{-0.2cm}
    \caption{\footnotesize The targeted ASR of three methods tested on (a) Flamingo, (b) BLIP-2, and (c) InstructBLIP. Different numbers of prompts 1, 5, 10, 50, and 100 are used in the transferability test. Our CroPA achieve better cross-prompt adversarial transferability than Single-P and Multi-P.}
    \label{fig:number of prompts}
\end{figure}
\vspace{-0.2cm}

For all the experiments conducted in this section, we selected the target text ``unknown" to avoid the inclusion of high-frequency responses commonly found in vision-language tasks.  Based on the experimental results, we can conclude the following points: 1) CroPA consistently outperforms the baseline approach for all models. As Figure~\ref{fig:number of prompts} shows, CroPA achieves the best overall performance in all testing models for all different prompt numbers.  CroPA also achieves the best individual performance in most tasks. The detailed data can be found in the supplementary. 2) More prompts increase the transferability, but convergence occurs rapidly. We can observe that in general more prompts increase the targeted ASR, especially when the number of prompts increases from one to five. However, starting from ten, the increase of cross-prompt transferability brought by more prompts becomes marginal, especially for the baseline approach.  This pattern indicates that by adding more prompts, the baseline approach cannot surpass the performance of CroPA methods.

 We also conducted experiments with cross-prompt transferability tests combined with other dimensions of transferability, models or images.  Our experimental results have shown that adversarial transferability is limited. When tested across different prompts and images, the ASRs are all near 0.   The ASRs over the models with significantly different architecture are near 0. \textcolor{black}{For the pair of InstructBLIP and BLIP-2, an overall ASR of around 10\% is achieved and the detailed data can be found in Table 8 in Appendix C. It should be noted that the language models used in  BLIP-2 and InstructBLIP are two different models: OPT-2.7b~\citep{zhang2022opt} and Vicuna-7b~\citep{zheng2023judging-vicuna} respectively. Though the performances when combined with other perspectives of the transferability are not as strong as cross-prompt transferability alone, CroPA still consistently outperforms the baseline methods. Moreover, existing methods for enhancing cross-model and cross-image transferability are orthogonal to the methods proposed in this study. For example, the cross-image transferability can be enhanced by computing the perturbation over a larger number of images~\citep{moosavi2017universal}. } We extend the work of creating adversarial images with combined perspectives of transferability to future work. 

\subsection{CroPA with Different Target Texts}
To demonstrate that the effectiveness of the proposed CroPA methods is not constrained to the specific case of the target text ``unknown", we extend our evaluation to various other target texts. The experiment includes a selection of text with varied length and usage frequency. Both common expressions such as ``very good" and less common words such as ``metaphor" are tested. 

\vspace{-0.05cm}
\begin{table}[ht]
\centering
\footnotesize
\caption{Targeted ASRs tested on Flamingo with different target texts. The mean and standard deviations of the ASRs are shown in the table. The `Overall’ column indicates the average targeted success rate across all tasks. The best
performance values for each task are highlighted in \textbf{bold}.}\vspace{-0.2cm}
\resizebox{\columnwidth}{!}{%
\setlength\tabcolsep{0.19cm}
\begin{tabular}{clccccc}
\hline
\footnotesize{Target Prompt} & \footnotesize{Method} & \footnotesize{VQA\textsubscript{general}} & 
\footnotesize{VQA\textsubscript{specific}} & \footnotesize{Classification} & \footnotesize{Captioning} & \footnotesize{Overall} \\
\hline
\multirow{3}{*}{\footnotesize{unknown}} 
 & \small{Single-P} 
 & $0.24\scriptscriptstyle \pm \scriptstyle1.34e\text{-}2$
 & $0.39\scriptscriptstyle \pm \scriptstyle5.73e\text{-}3$
 & $0.21\scriptscriptstyle \pm \scriptstyle6.25e\text{-}3$
 & $0.05\scriptscriptstyle \pm \scriptstyle2.31e\text{-}3$
 & $0.22\scriptscriptstyle \pm \scriptstyle8.04e\text{-}3$ \\
 & \small{Multi-P} 
 & $0.67\scriptscriptstyle \pm \scriptstyle7.14e\text{-}3$
 & $0.86\scriptscriptstyle \pm \scriptstyle2.09e\text{-}3$
 & $0.64\scriptscriptstyle \pm \scriptstyle1.35e\text{-}3$
 & $0.31\scriptscriptstyle \pm \scriptstyle1.44e\text{-}2$
 & $0.62\scriptscriptstyle \pm \scriptstyle8.16e\text{-}3$ \\
 & \small{CroPA} 
 & $\mathbf{0.92\scriptscriptstyle \pm \scriptstyle1.07e\text{-}2}$
 & $\mathbf{0.98\scriptscriptstyle \pm \scriptstyle6.72e\text{-}3}$
 & $\mathbf{0.70\scriptscriptstyle \pm \scriptstyle3.42e\text{-}3}$
 & $\mathbf{0.34\scriptscriptstyle \pm \scriptstyle3.19e\text{-}3}$
 & $\mathbf{0.74\scriptscriptstyle \pm \scriptstyle6.75e\text{-}3}$ \\
\hline
\multirow{3}{*}{\small{I am sorry}} 
 & \small{Single-P} 
 & $0.21\scriptscriptstyle \pm \scriptstyle1.50e\text{-}3$
 & $0.43\scriptscriptstyle \pm \scriptstyle7.52e\text{-}3$
 & $0.47\scriptscriptstyle \pm \scriptstyle8.59e\text{-}3$
 & $0.34\scriptscriptstyle \pm \scriptstyle5.01e\text{-}3$
 & $0.36\scriptscriptstyle \pm \scriptstyle6.28e\text{-}3$ \\
 & \small{Multi-P} 
 & $0.60\scriptscriptstyle \pm \scriptstyle1.28e\text{-}3$
 & $0.85\scriptscriptstyle \pm \scriptstyle1.45e\text{-}2$
 & $0.71\scriptscriptstyle \pm \scriptstyle1.26e\text{-}2$
 & $0.60\scriptscriptstyle \pm \scriptstyle3.97e\text{-}3$
 & $0.69\scriptscriptstyle \pm \scriptstyle9.87e\text{-}3$ \\
 & \small{CroPA} 
 & $\mathbf{0.90\scriptscriptstyle \pm \scriptstyle3.56e\text{-}3}$
 & $\mathbf{0.96\scriptscriptstyle \pm \scriptstyle5.25e\text{-}3}$
 & $\mathbf{0.75\scriptscriptstyle \pm \scriptstyle8.34e\text{-}3}$
 & $\mathbf{0.72\scriptscriptstyle \pm \scriptstyle7.04e\text{-}3}$
 & $\mathbf{0.83\scriptscriptstyle \pm \scriptstyle6.31e\text{-}3}$ \\
\hline
\multirow{3}{*}{\small{not sure}}
 & \small{Single-P} 
 & $0.25\scriptscriptstyle \pm \scriptstyle1.42e\text{-}3$
 & $0.36\scriptscriptstyle \pm \scriptstyle1.52e\text{-}3$
 & $0.09\scriptscriptstyle \pm \scriptstyle1.25e\text{-}2$
 & $0.00\scriptscriptstyle \pm \scriptstyle6.04e\text{-}3$
 & $0.17\scriptscriptstyle \pm \scriptstyle7.03e\text{-}3$ \\
 & \small{Multi-P} 
 & $0.55\scriptscriptstyle \pm \scriptstyle9.56e\text{-}3$
 & $0.55\scriptscriptstyle \pm \scriptstyle2.95e\text{-}3$
 & $0.11\scriptscriptstyle \pm \scriptstyle5.09e\text{-}3$
 & $0.02\scriptscriptstyle \pm \scriptstyle6.12e\text{-}3$
 & $0.31\scriptscriptstyle \pm \scriptstyle6.39e\text{-}3$ \\
 & \small{CroPA} 
 & $\mathbf{0.88\scriptscriptstyle \pm \scriptstyle1.19e\text{-}2}$
 & $\mathbf{0.86\scriptscriptstyle \pm \scriptstyle3.79e\text{-}3}$
 & $\mathbf{0.30\scriptscriptstyle \pm \scriptstyle8.19e\text{-}3}$
 & $\mathbf{0.17\scriptscriptstyle \pm \scriptstyle9.29e\text{-}3}$
 & $\mathbf{0.55\scriptscriptstyle \pm \scriptstyle8.82e\text{-}3}$ \\
\hline
\multirow{3}{*}{\small{very good}}
 & \small{Single-P} 
 & $0.35\scriptscriptstyle \pm \scriptstyle8.31e\text{-}3$
 & $0.52\scriptscriptstyle \pm \scriptstyle1.17e\text{-}2$
 & $0.15\scriptscriptstyle \pm \scriptstyle4.02e\text{-}3$
 & $0.05\scriptscriptstyle \pm \scriptstyle9.72e\text{-}3$
 & $0.27\scriptscriptstyle \pm \scriptstyle8.92e\text{-}3$ \\
 & \small{Multi-P} 
 & $0.81\scriptscriptstyle \pm \scriptstyle9.51e\text{-}3$
 & $0.93\scriptscriptstyle \pm \scriptstyle3.38e\text{-}3$
 & $0.40\scriptscriptstyle \pm \scriptstyle1.91e\text{-}3$
 & $0.20\scriptscriptstyle \pm \scriptstyle1.42e\text{-}2$
 & $0.59\scriptscriptstyle \pm \scriptstyle8.79e\text{-}2$ \\
 & \small{CroPA} 
 & $\mathbf{0.95\scriptscriptstyle \pm \scriptstyle1.13e\text{-}2}$
 & $\mathbf{0.97\scriptscriptstyle \pm \scriptstyle5.26e\text{-}3}$
 & $\mathbf{0.64\scriptscriptstyle \pm \scriptstyle2.36e\text{-}3}$
 & $\mathbf{0.27\scriptscriptstyle \pm \scriptstyle1.05e\text{-}2}$
 & $\mathbf{0.71\scriptscriptstyle \pm \scriptstyle8.61e\text{-}3}$ \\
\hline
\multirow{3}{*}{\small{too late}}
 & \small{Single-P} 
 & $0.21\scriptscriptstyle \pm \scriptstyle1.72e\text{-}3$
 & $0.38\scriptscriptstyle \pm \scriptstyle8.43e\text{-}3$
 & $0.21\scriptscriptstyle \pm \scriptstyle8.56e\text{-}3$
 & $0.04\scriptscriptstyle \pm \scriptstyle9.92e\text{-}3$
 & $0.21\scriptscriptstyle \pm \scriptstyle7.84e\text{-}3$ \\
 & \small{Multi-P}
 & $0.78\scriptscriptstyle \pm \scriptstyle2.71e\text{-}3$
 & $0.90\scriptscriptstyle \pm \scriptstyle7.93e\text{-}3$
 & $0.54\scriptscriptstyle \pm \scriptstyle1.48e\text{-}3$
 & $0.17\scriptscriptstyle \pm \scriptstyle1.37e\text{-}2$
 & $0.60\scriptscriptstyle \pm \scriptstyle8.07e\text{-}3$ \\
 & \small{CroPA}
 & $\mathbf{0.90\scriptscriptstyle \pm \scriptstyle1.03e\text{-}2}$
 & $\mathbf{0.95\scriptscriptstyle \pm \scriptstyle5.36e\text{-}3}$
 & $\mathbf{0.73\scriptscriptstyle \pm \scriptstyle8.28e\text{-}3}$
 & $\mathbf{0.20\scriptscriptstyle \pm \scriptstyle8.65e\text{-}3}$
 & $\mathbf{0.70\scriptscriptstyle \pm \scriptstyle8.33e\text{-}3}$ \\
\hline
\multirow{3}{*}{\small{metaphor}}
 & \small{Single-P} 
 & $0.26\scriptscriptstyle \pm \scriptstyle1.46e\text{-}2$
 & $0.56\scriptscriptstyle \pm \scriptstyle8.22e\text{-}3$
 & $0.50\scriptscriptstyle \pm \scriptstyle5.52e\text{-}3$
 & $0.14\scriptscriptstyle \pm \scriptstyle1.21e\text{-}2$
 & $0.37\scriptscriptstyle \pm \scriptstyle8.83e\text{-}3$ \\
 & \small{Multi-P} 
 & $0.83\scriptscriptstyle \pm \scriptstyle1.46e\text{-}2$
 & $0.92\scriptscriptstyle \pm \scriptstyle1.18e\text{-}2$
 & $0.81\scriptscriptstyle \pm \scriptstyle1.41e\text{-}2$
 & $0.42\scriptscriptstyle \pm \scriptstyle1.35e\text{-}2$
 & $0.75\scriptscriptstyle \pm \scriptstyle1.36e\text{-}2$ \\
 & \small{CroPA} 
 & $\mathbf{0.96\scriptscriptstyle \pm \scriptstyle1.39e\text{-}2}$
 & $\mathbf{0.99\scriptscriptstyle \pm \scriptstyle2.23e\text{-}3}$
 & $\mathbf{0.92\scriptscriptstyle \pm \scriptstyle3.74e\text{-}3}$
 & $\mathbf{0.62\scriptscriptstyle \pm \scriptstyle1.63e\text{-}3}$
 & $\mathbf{0.87\scriptscriptstyle \pm \scriptstyle1.07e\text{-}2}$ \\
 \hline
\end{tabular}
}
\label{tab:different_target}
\end{table}

From Table~\ref{tab:different_target}, we can observe that CroPA performs the best overall and in each individual task under different target text, though the ASR differs for different target prompts. This table also demonstrated that cross-prompt transferability is not associated with the semantic meaning or the word frequency of the target text.  Semantically ``not sure" is a reasonable generation result of VLMs to indicate the uncertainty of the response. However, its targeted ASR is much lower than  ``metaphor", which is less commonly used as a response to the questions or instructions about an image. The results also show that the cross-prompt transferability is not determined by the target text length. The ASR of the three-word target text is not necessarily to be lower than the ASR of a one-word target text.  

\textcolor{black}{We also conducted the experiments by setting the target texts to harmful instructions such as  "suicide", "bomb" and "kidnap". As shown in Table~\ref{tab:harm}, the high ASR values indicate that the cross-prompt adversarial transferability can lead to security issues in real-world applications. From this perspective, the cross-prompt transferability is also related to the jailbreak~\cite{zou2023universal} for vision-language models, which also aims to deceive the model to generate harmful instructions.}

In summary, the CroPA has been proven to be a stronger method than the baseline approach with different target texts. While different target texts do affect the cross-prompt adversarial transferability, it is unlikely that this is associated with the semantics or length of the prompts themselves.

\vspace{-0.2cm}
\subsection{CroPA Meets In-context Learning}
\label{sec:icl}
In addition to the textual prompt, the Flamingo model also supports providing extra images as in-context learning examples to improve the task adaptation ability. Whether these in-context learning examples have an influence on the cross-prompt attack remains unclear. Therefore, we tested the ASRs of the image adversarial examples with the number of in-context learning examples different from the one provided in the optimisation stage.  During the optimisation stage, the image adversarial examples are updated under the 0-shot setting, namely no extra images are provided as the in-context learning examples. In the evaluations, the 2-shot setting is used, i.e. two extra images are used as the in-context learning examples.  Evaluation results under the 0-shot setting are also provided for comparison.

As Table~\ref{tab:icl} shows, the CroPA still achieves the best performance under the 2-shot settings.  We can observe that in-context learning examples can decrease the ASRs, as these two extra in-context learning examples cause a shift in the generation condition different from the optimisation stage. 

\begin{table}[ht]
\centering
\caption{Targeted ASRs of with and without visual in-context learning. The shot indicates the number of images added for in-context learning. The model utilised is Flamingo. The mean and standard deviations of the ASRs are shown in the table. The best performance values for each task are highlighted in \textbf{bold}.}\vspace{-0.2cm} \label{tab:icl}
\footnotesize
\begin{tabular}{lcccccc}
 \hline
 Method &VQA\textsubscript{general} &VQA\textsubscript{specific} &Classification &Captioning & Overall \\
 \hline
 Multi-P (shot=0) 
 & $0.67\scriptscriptstyle \pm \scriptstyle7.14e\text{-}3$
 & $0.86\scriptscriptstyle \pm \scriptstyle2.09e\text{-}3$
 & $0.64\scriptscriptstyle \pm \scriptstyle1.35e\text{-}3$
 & $0.31\scriptscriptstyle \pm \scriptstyle1.44e\text{-}2$
 & $0.62\scriptscriptstyle \pm \scriptstyle8.16e\text{-}3$ \\
 CroPA (shot=0) 
 & $\mathbf{0.92\scriptscriptstyle \pm \scriptstyle1.07e\text{-}2}$
 & $\mathbf{0.98\scriptscriptstyle \pm \scriptstyle6.72e\text{-}3}$
 & $\mathbf{0.70\scriptscriptstyle \pm \scriptstyle3.42e\text{-}3}$
 & $\mathbf{0.34\scriptscriptstyle \pm \scriptstyle3.19e\text{-}3}$
 & $\mathbf{0.74\scriptscriptstyle \pm \scriptstyle6.75e\text{-}3}$ \\
 \hline
 Multi-P (shot=2) 
 & $0.59\scriptscriptstyle \pm \scriptstyle6.24e\text{-}3$
 & $0.81\scriptscriptstyle \pm \scriptstyle1.43e\text{-}2$
 & $0.50\scriptscriptstyle \pm \scriptstyle1.12e\text{-}2$
 & $0.25\scriptscriptstyle \pm \scriptstyle9.38e\text{-}3$
 & $0.54\scriptscriptstyle \pm \scriptstyle3.18e\text{-}3$ \\
 CroPA (shot=2) 
 & $\mathbf{0.84\scriptscriptstyle \pm \scriptstyle3.18e\text{-}3}$
 & $\mathbf{0.96\scriptscriptstyle \pm \scriptstyle1.18e\text{-}3}$
 & $\mathbf{0.76\scriptscriptstyle \pm \scriptstyle1.31e\text{-}2}$
 & $\mathbf{0.26\scriptscriptstyle \pm \scriptstyle9.41e\text{-}3}$
 & $\mathbf{0.70\scriptscriptstyle \pm \scriptstyle1.09e\text{-}2}$ \\
\hline
\end{tabular}
\end{table}
\vspace{-0.2cm}

\subsection{Convergence of CroPA}
\label{sec: converge}
In this section, we conduct experiments to compare the performance of baseline and CroPA methods over different update iterations. As shown in Figure~\ref{fig:convergence},  we present the results of the overall targeted ASRs with attack iterations from 300 to 1900 every 200 iterations. The number of prompts used for Multi-P and CroPA in optimisation is set to ten.

For all the methods, adding more attack iterations can increase ASRs at the beginning, but the performances eventually converge.  For the Single-P method, the improvement in cross-prompt transferability by using more iterations has quickly become marginal after 300 iterations. However, for the Muli-P and CroPA, the performance can still increase after 1000 epochs. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/efficiency.pdf} \vspace{-0.3cm}
    \caption{Targeted ASRs of Baseline Methods (Single-P and Multi-P) and CroPA over attack Iterations. With the same number of attack iterations, our CroPA significantly outperforms baselines.} 
    \label{fig:convergence}
    \vspace{-0.3cm}
\end{figure}

The figure also demonstrated that CroPA methods do not rely on extra attack iterations to gain better performance compared to the baseline approaches. Given the same attack iterations, the CroPA consistently achieves better performance compared to the Single-P and Multi-P methods. CroPA requires only 500 iterations to achieve the target ASR above 0.6 while the stronger baseline Multi-P requires over 1000 iterations. 

Overall, Section~\ref{sec:prompt-num} and this section have shown that the cross-prompt transferability of the baseline method is limited compared to CroPA with the increase of the prompt numbers and iterations. 

\subsection{CroPA with Different Update Strategy}
In this section, we explore the effect of different update strategies. As described in previous sections, the update frequency of both the image perturbation and the prompt perturbation can be different. A special case of CroPA is CroPA\textsubscript{joint}, where the image perturbation and the prompt perturbation have the same update frequency. We compare the result of CroPA with CroPA\textsubscript{joint} on different tasks with different in-context learning settings. Similar to Section~\ref{sec:icl}, adversarial examples are optimised under 0-shot and tested under 0-shot and 2-shot settings.

As shown in Table~\ref{tab:alter_joint_comparision}, the CroPA outperform the CroPA\textsubscript{joint} overall and most individual tasks with different in-context learning image examples. The stronger performance of CroPA derives from its flexibility in choosing the update step size of the prompt perturbation. As presented in Appendix~\ref{app:step_size}, the CroPA\textsubscript{joint} is sensitive to the step size of the prompt embedding: if the prompt update size is too large the optimisation fails to converge. CroPA is more tolerant of large prompt update sizes by reducing the prompt update frequency.  

\begin{table}[ht]
\centering
\caption{Our CroPA with alternative optimization outperforms the one with vision and prompt joint optimization in most cases. The mean and standard deviations of the ASRs are shown in the table. The best performance values for each task are highlighted in \textbf{bold}} \vspace{-0.2cm}
\footnotesize
\begin{tabular}{lcccccc}
 \hline
 Method &VQA\textsubscript{general} &VQA\textsubscript{specific} &Classification &Captioning & Overall \\
 \hline
 CroPA\textsubscript{joint} (shot=0) 
 & $0.86\scriptscriptstyle \pm \scriptstyle1.44e\text{-}3$
 & $0.95\scriptscriptstyle \pm \scriptstyle9.91e\text{-}3$
 & $\mathbf{0.73\scriptscriptstyle \pm \scriptstyle5.40e\text{-}3}$
 & $0.31\scriptscriptstyle \pm \scriptstyle8.11e\text{-}3$
 & $0.71\scriptscriptstyle \pm \scriptstyle6.99e\text{-}3$ \\
 CroPA (shot=0) 
 & $\mathbf{0.92\scriptscriptstyle \pm \scriptstyle1.07e\text{-}2}$
 & $\mathbf{0.98\scriptscriptstyle \pm \scriptstyle6.72e\text{-}3}$
 & $0.70\scriptscriptstyle \pm \scriptstyle3.42e\text{-}3$
 & $\mathbf{0.34\scriptscriptstyle \pm \scriptstyle3.19e\text{-}3}$
 & $\mathbf{0.74\scriptscriptstyle \pm \scriptstyle6.75e\text{-}3}$ \\
 \hline
 CroPA\textsubscript{joint} (shot=2) 
 & $0.76\scriptscriptstyle \pm \scriptstyle4.49e\text{-}3$
 & $0.94\scriptscriptstyle \pm \scriptstyle6.74e\text{-}3$
 & $0.71\scriptscriptstyle \pm \scriptstyle1.15e\text{-}2$
 & $0.25\scriptscriptstyle \pm \scriptstyle4.20e\text{-}3$
 & $0.66\scriptscriptstyle \pm \scriptstyle7.37e\text{-}3$ \\
 CroPA (shot=2) 
 & $\mathbf{0.84\scriptscriptstyle \pm \scriptstyle3.18e\text{-}3}$
 & $\mathbf{0.96\scriptscriptstyle \pm \scriptstyle1.18e\text{-}3}$
 & $\mathbf{0.76\scriptscriptstyle \pm \scriptstyle1.31e\text{-}2}$
 & $\mathbf{0.26\scriptscriptstyle \pm \scriptstyle9.41e\text{-}3}$
 & $\mathbf{0.70\scriptscriptstyle \pm \scriptstyle1.09e\text{-}2}$ \\
 \hline
 \label{tab:alter_joint_comparision}
\end{tabular}
\end{table}

\vspace{-0.5cm}

\subsection{Understanding the Effectiveness of CroPA methods}
\textbf{Visualisation of the Prompt Embedding Coverage}  To explore the underlying reasons for the better performance of CroPA compared to the baseline approach, we visualise the sentence embedding of the original prompt and perturbated prompts by CroPA, which is obtained by the averaging embedding of each token. 

As demonstrated in Figure~\ref{fig:2d-emb}, the orange plus symbol denotes the original prompts while the purple star symbol denotes the original embedding added with the perturbation $\delta_t$.  It can be observed that there is almost no overlap between the prompt embedding perturbed by CroPA and the original prompt embedding. This verifies that the adversarial prompt effectively increases the coverage of the original embedding.

\begin{figure}[!t]
\centering
    \begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[scale=0.31]{figures/emb_add_2d_26.pdf}\vspace{-0.3cm}
    \caption{\footnotesize{Visualisation of the prompt embeddings and its prompt embedding created by CroPA. The orange plus symbol denotes the original prompt and the purple star symbol denotes the embedding by adding the adversarial prompt perturbation to the original prompt embedding.} }
       \label{fig:2d-emb}\end{subfigure}\hspace{0.2cm}
    \begin{subfigure}[b]{0.48\textwidth}
    \centering
    \includegraphics[scale=0.31]{figures/emb_20.pdf}\vspace{-0.3cm}
    \caption{\footnotesize{Comparison between the embedding coverage difference between prompts generated by CroPA and extra Prompt Set 2. The red circle denote the embedding coverage of Prompt Set 1 and Prompt Set 2, while the blue circle represents the coverage of Prompt Set 1 with the prompts generated by CroPA. } }
    \label{fig:comparison-CroPA}
    \end{subfigure}\vspace{-0.2cm}
    \caption{Visualisation of the prompt embeddings with t-SNE~\citep{van2008visualizing}.}
    \vspace{-0.3cm}
\end{figure}

To have a clearer comparison between the baseline approach, which relies on simply adding more prompts,  and the CroPA methods, we visualise the coverage of prompt embeddings of these two methods in Figure~\ref{fig:comparison-CroPA} shows. The embedding of Prompt Set 1 is denoted by the orange dots, while the embeddings of Prompt Set 2 are denoted as the cyan dots. In the CroPA, only Prompt Set 1 is provided. By introducing the learnable prompt perturbation to Prompts Set 1, the prompt embeddings that have been covered during optimisation are denoted by the purple stars.  The blue eclipse is the approximated coverage of the prompt embedding for CroPA using Prompt Set 1. For the baseline method, both Prompt Set 1 and Prompt Set 2 are provided and the red eclipse approximately represents the coverage of their coverage. It can be observed that with only Prompt Set 1, the area covered by the CroPA is broader than the one covered by the embeddings of Prompt Set 1 and Prompt Set 2. 

The visualisation of difference in the prompt embedding coverage explains the reason why the CroPA methods can outperform the baseline approach even if the number of prompts used in optimisation is less than the baseline approach.

\textbf{Prompt Embedding Decoding} We explored the decoding of the adversarial prompt embedding to a human-readable text format. The embedding of each token is decoded to the readable text closest in terms of cosine distance using the pre-trained embedding look-up table of the language models. The results show that all the perturbed embeddings are still closest to their original tokens. This finding also supports the effectiveness of  CroPA: There exists prompt embedding that cannot be represented by human-readable text. Therefore, even if given a sufficient number of prompts in the baseline approach, it still can not cover all the prompt embedding space of the adversarial prompt in the CroPA framework. 

    
        
       
        
    

\section{Conclusion}
In this paper, we first raise an interesting and important question, can a single adversarial example mislead all predictions of a vision-language model with different prompts? We formulate the essence of the question as the cross-prompt adversarial transferability of adversarial perturbation. Extensive experiments show that intuitive baseline approaches only achieve limited transferability and our proposed CroPA improves the transferability significantly on different VLMs in different multi-modal tasks. One of the ways to further improve the practical applicability of our method is to implement the optimization with query-based strategies~\citep{chen2017zoo,ilyas2018black}, which we leave to future work.

\noindent\textbf{Acknowledgement} This work is supported by the UKRI grant: Turing AI Fellowship EP/W002981/1, and EPSRC/MURI grant: EP/N019474/1, We would also like to thank the Royal Academy of Engineering and FiveAI.

\end{document}