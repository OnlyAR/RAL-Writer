\title{CREAK: A Dataset for Commonsense Reasoning over Entity Knowledge}

\begin{document}

\maketitle

\begin{abstract}
Most benchmark datasets targeting commonsense reasoning focus on everyday scenarios: physical knowledge like knowing that you could fill a cup under a waterfall \citep{Alon_Talmor_2019}, social knowledge like bumping into someone is awkward \citep{Maarten_Sap_20219}, and other generic situations. However, there is a rich space of commonsense inferences anchored to knowledge about specific entities: for example, deciding the truthfulness of a claim \emph{Harry Potter can teach classes on how to fly on a broomstick}. Can models learn to combine entity knowledge with commonsense reasoning in this fashion? We introduce \oursno, a testbed for commonsense reasoning about entity knowledge, bridging fact-checking about entities (Harry Potter is a wizard and is skilled at riding a broomstick) with commonsense inferences (if you're good at a skill you can teach others how to do it). Our dataset consists of 13k human-authored English claims about entities that are either true or false, in addition to a small contrast set.
Crowdworkers can easily come up with these statements and human performance on the dataset is high (high 90s); we argue that models should be able to blend entity knowledge and commonsense reasoning to do well here. In our experiments, we focus on the closed-book setting and observe that a baseline model finetuned on existing fact verification benchmark struggles on \oursno. Training a model on \ours improves accuracy by a substantial margin, but still falls short of human performance. Our benchmark provides a unique probe into natural language understanding models, testing both its ability to retrieve facts (e.g., who teaches at the University of Chicago?) and unstated commonsense knowledge (e.g., butlers do not yell at guests).  
\end{abstract}

\section{Introduction}

To understand text, humans use rich background knowledge about the world. Despite the impressive ability of large-scale pretrained models, models often generate sentences that violate a reader's expectations, particularly in terms of common sense.  
As these models are increasingly employed in settings like generative question answering \citep{Angela_Fan_2019,Patrick_Lewis_2020} and fact verification \citep{Andreas_Vlachos_2014, William_Wang_2017, fever}, they should exhibit not just commonsense about everyday scenarios (physical, social, etc.), but factual knowledge about entities as well. These concepts overlap in a set of inferences involving entities that we call \emph{entity commonsense}. For example, to recognize that \emph{``Many business owners rely on WordPress to create their websites.''} is true requires both knowledge about the entity  (WordPress is a website hosting service) and a more nebulous piece of commonsense information (famous products like WordPress are widely used).
\begin{figure}
    \centering
    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{fig1.pdf}
        \caption{\ours claims with different reasoning types. Datasets like FEVER focus on retrieval (as in the last case); our dataset also features many claims that involve both retrieval and also commonsense reasoning (54\% of the data according  our manual study in Section~\ref{sec:quality-verification}).}
        \label{fig:intro}
    \end{minipage}
    \vspace{-4pt}
\end{figure}

We present \oursno, a dataset aiming to evaluate two major desiderata of NLP models: entity understanding and commonsense inference. Figure~\ref{fig:intro} shows how these concepts interact in examples from \oursno. Building LMs with a stronger ability to perform this type of inference can help make NLP systems more effective and reliable.

Our dataset consists of 13k English claims covering 2.7k entities, each labeled as true or false. Each claim is generated by a crowdworker based on a Wikipedia entity, which can be named entities (e.g., John Dewey), common nouns (e.g., penguins), and abstract concepts (e.g., freedom of speech). Our lightweight task design provides annotators with a set of popular entity topics, and by not including explicit evidence documents to copy text from, annotators are encouraged to create examples fully from scratch. This results in sentences where annotators combine their knowledge about entities with common sense to generate claims. Even without resorting to adversarial filtering, which artificially biases a dataset against existing model checkpoints~\citep{Sam_Bowman_2021}, we find our annotation protocol leads to challenging claims for existing models. We provide in-depth analysis on what makes our dataset uniquely challenging: for example, 18\% of claims in \ours contain quantifiers (e.g., enough, always, rarely etc.) that necessitate subtle commonse reasoning, compared to existing fact verification datasets~\citep{fever} where only 5\% of claims contain the quantifiers.

Asking crowdworkers to generate free-form sentences can introduce dataset artifacts \citep{Suchin_Gururangan_2018, Mor_Geva_2019}. We carefully examine such artifacts in our datasets using quantitative tools \citep{Swabha_Swayamdipta_2020,Matt_Gardner_2020} as well as qualitative inspection. We also provide a small set of expert-written contrast examples \citep{Divyansh_Kaushik_2019,Matt_Gardner_2020} which pair true and false claims sharing almost identical context.  

To establish an initial performance level on \oursno, we evaluate state-of-the-art pre-trained language models \citep{Yinhan_Liu_19,T5}. Our experiments shows that \ours is challenging even for a large model, with a gap between model and human accuracy of 10 points on the development set and about 27 points on the contrast set for the largest model. Moreover, the model trained on \ours outperforms the model trained on other claim verification datasets~\citep{fever,Julian_Eisenschlos_2021,Jungsoo_Park_2021}, suggesting that \ours tests different reasoning capabilities compared to existing datasets. We further characterize the performance based on model size, entity type, and the whether external knowledge is used. Our analysis supports that to achieve high performance on our dataset, models should possess not only entity knowledge but also complex reasoning skills. The code and data are publicly available at \url{https://www.cs.utexas.edu/~yasumasa/creak}.

\section{Related Work}\label{sec:related-work} 

\paragraph{Claim Verification}
Our task is formulated as claim verification, which has seen increasing work in recent years. The largest claim verification dataset, FEVER~\citep{fever}, has claims designed to be verifiable with a passage from English Wikipedia and typically covers simple facts such as attributes and records (e.g., \emph{``Benjamin Franklin was a person,''} and \emph{``Spider-Man 2 was released in 2004.''}). In fact, 58\% of FEVER claims contain a simple copular verb ({\it is, are, was, were}) and many claims contain a definition. Prior work~\citep{Julian_Eisenschlos_2021} observed high lexical overlap between claims and corresponding entity definitions in Wikipedia, and collected more complex claims using a human-in-the-loop adversarial approach. Similarly, recent work \citep{Jungsoo_Park_2021} derives a challenging verification dataset from ambiguous questions~\citep{Sewon_Min_2020} and their different interpretations. However, both datasets focus on a retrieval setting where there is a single paragraph in Wikipedia from which the claim can be easily verified. In contrast, our dataset contains claims where it is not easy to find a single paragraph that can verify them, testing models' intrinsic abilities. 

\paragraph{Question Answering} Question answering and claim verification are closely related, particularly when it comes to binary questions \citep{Clark-etal-2019-boolq}. Our dataset is purposely constructed to go beyond basic factoid information like that tested in open QA benchmarks like NaturalQuestions \citep{Tom_Kwiatkowski_2019} and focuses on information that is less likely to have textual support on the web. The recently proposed StrategyQA dataset \citep{Mor_Geva_2021} which contains binary questions requiring implicit reasoning that goes beyond evidence retrieval (e.g., \emph{``would it be common to find a penguin in Miami?''}) captures a similar type of reasoning and knowledge as in our work. However, our annotation process does not require authoring strategies, allowing us to scale to a larger dataset (13K vs. 2.8K) while capturing a wide range of inference types. Finally, some QA datasets have been adapted for evaluating differentiable commonsense reasoning models \citep{lin-etal-2021-differentiable}, but these benchmarks still test very different knowledge from ours.

\paragraph{Commonsense Reasoning}
Commonsense reasoning tasks \citep[inter alia]{Hector_Levesque_2011, Rowan_Zellers_2018, Alon_Talmor_2019,Lourie_etal_2021_Unicorn} evaluate models' reasoning skills in the physical world, with reporting bias being a principal challenge \citep{Gordon2013ReportingBA}. Yet, most datasets assume hypothetical environments and do not address real-world entities. Our work relates to judging plausibility of events \citep{forbes-choi-2017-verb,wang-etal-2018-modeling}, closely tied to inferences accessible from feature norms \citep{McRae}, but again these past efforts do not focus on judgments around specific entities.

\paragraph{Knowledge Probing} The LAMA benchmark~\citep{Petroni-etal-2019-language} was proposed to query factual knowledge covered in language models. Our dataset also covers such factual knowledge but also requires commonsense reasoning capabilities. Our work also creates a moderately sized training dataset. Other datasets in the KILT benchmark \citep{Fabio_Petroni_2020}, an aggregate suite focusing on knowledge intensive tasks, are more focused on recognizing entities and relations, ``low-level'' factual knowledge which does not require the kinds of commonsense inferences in our dataset. Another recent commonsense-focused dataset~\citep{lin-etal-2020-birds}, focuses on probing numeric claims.

\section{\textsc{\ours}}\label{sec:dataset}

\subsection{Task Definition}
\paragraph{Problem Scope} Our benchmark covers claims that are typically quite easy for humans to verify but challenging for language models. We focus on factual claims about real-world entities, but our claims are more complex than existing fact verification examples which tend to state relatively simple facts (i.e., definitive sentences, \emph{X is a Y}, or sentences expressing simple relations, like \emph{X is CEO of Z}). To the extent possible, we avoid information that is obscure or requires computation, such as asking about the time between two arbitrary events or how many copies of an album were sold, which test either retrieval or memorization rather than commonsense reasoning. We found that our claims can often be verified with minimum knowledge of the entities combined with common sense (i.e., you can guess the answer accurately even if you do not know the entity very well).\footnote{During our validation, we could confidently judge about 30-50\% of claims without searching the web.} We argue that this knowledge is what pre-trained LMs should possess about moderately well-known entities after seeing a few occurrences of them during pre-training. Therefore, our claims should be solvable in the closed-book setting where we can purely evaluate LMs' commonsense reasoning skills, isolated from the performance of retrieval models.    

We formally define the \textsc{\ours} task as follows. Given a single sentence claim $c$ containing at least one entity mention, the task is to assign a binary label $y \in \{{\tt TRUE, FALSE}\}$ indicating whether the claim is true or false. Dataset statistics can be found in Table~\ref{tab:data}.

\paragraph{Dataset Properties} Our dataset has following key properties. The claims are \textbf{diverse} covering various types of entities: they are written by 684 distinct crowdworkers\footnote{We limit the number of claims that a single crowdworker can generate (no more than 7\% of any split.} only based on the entity names and their minimal information. We rarely find lexical overlap between the claims and publicly available knowledge sources (e.g., the first paragraph of English Wikipedia). As a result, the claims contain a variety of reasoning types, but nevertheless are \textbf{typically not subjective} and \textbf{easily verifiable}. As discussed in Section~\ref{sec:quality-verification}, a majority of our examples do \textbf{involve a combination of commonsense reasoning and knowledge}. Finally, Sections~\ref{sec:quality-verification} and \ref{sec:experiments} show that the dataset is \textbf{relatively robust to spurious correlations}.

\renewcommand{\arraystretch}{1}
\begin{table*}[t]
	\centering
	\small
		\caption{Data statistics of \oursno.}\label{tab:data}
	\setlength{\tabcolsep}{4pt}
    \centering
	\begin{tabular}{l c c c c c c c c c c}
		\toprule
		\multicolumn{1}{c}{\multirow{3}{*}{Split}} & \multicolumn{1}{c}{} & \multicolumn{3}{c}{\# Claims}& \multicolumn{1}{c}{}  & \multicolumn{1}{c}{Average Length}& \multicolumn{1}{c}{} & \multicolumn{1}{c}{\multirow{3}{*}{\# Unique Entities}} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{\multirow{3}{*}{Vocab Size}}\\
		\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{Total} & \multicolumn{1}{c}{True} & \multicolumn{1}{c}{False} & \multicolumn{1}{c}{}  & \multicolumn{1}{c}{(\# tokens)}  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{}\\
		\midrule
		Train && 10,176 & 5,088 & 5,088 && 10.8  && 2,096 && 19,006 \\
		Dev && \:\:\:1,371 & \:\:\:691 & \:\:\:680 &&  \:\:9.7  && \:\:\:531 && \:\:\:4,520\\
		Test && \:\:\:1,371 & \:\:\:707 & \:\:\:664 && \:\:9.9  && \:\:\:538 && \:\:\:4,620\\
		Test (Contrast) && \:\:\:\:\:\:200 & \:\:\:100 & \:\:\:100 && 11.0  && \:\:\:100 && \:\:\:\:\:\:800\\
		\bottomrule 
	\end{tabular}

\end{table*}

\subsection{Data Collection}\label{sec:data-collection}
We collect our data on the Amazon Mechanical Turk platform. Open-ended text generation is challenging to crowdsource, so we take several steps in our task design to ensure quality. First, we ask crowdworkers to write down the reason why the generated claim is true or false; although past work observes that this does not improve example quality in isolation \citep{Nikita_Nangia_2021}, we found it helpful for our task, and it additionally helped us spot workers who misunderstood the task. To keep the sentences natural, we use a minimal set of requirements and encourage crowdworkers to produce creative and diverse sentences. One key requirement is to use action verbs instead of copula, which prevent crowdworkers from writing simple definitive sentences. See Appendix~\ref{app:interface} for more details about the annotation instructions. 

We do not take a model-in-the-loop approach \citep{Rowan_Zellers_2018,Rown_Zeller_2019,Yixin_Nie_2020,Ronan_Le_Bras_2020} during data collection in order to keep our dataset organic, meaning that sentences preserve the original data distribution given by annotators. Therefore, this benchmark does not favor or disfavor particular LM checkpoints, providing a fair and comparable testbed \citep{Sam_Bowman_2021}.

\paragraph{Seed Entities Curation} Entity selection plays a crucial role in this task, since authoring sentences is a much easier task if a crowdworker is familiar with the entity. We take two steps to enable crowdworkers to focus on known entities. First, we use the entity list created by \citet{Mor_Geva_2021} as part of StrategyQA, which aligns with our needs; the authors select entities based on some popularity measures such as the number of contributors and the number of backward links from other pages. Second, we present five entities to each annotator and let them pick from that set of five when authoring their sentences. %\todo{histogram over how much sentences from each entity in the supp mat} 
We manually inspect the seed entities to maintain the diversity of the types of entities so that the generated claims cover diverse topics (e.g., we want to avoid too many location entities that occur in English Wikipedia frequently). We finally obtain 6.4k entities after this process. 

We split the seed entity list into two parts; one for the training instances and one for the development and test instances. In both sets, roughly 80\% of entities are named entities. The 5 most popular entities in the train set are \emph{Sloth}, \emph{Giraffe}, \emph{George Orwell}, \emph{50 Cent}, \emph{Mattel}. In the development and test sets, \emph{Butterfly}, \emph{Ray Charles}, \emph{Whole Foods Market}, \emph{Internet troll}, and \emph{Bigfoot} are the top 5 popular entities. As can be seen, crowdworkers prefer to select relatively common entities.

\paragraph{Quality Control} 
We split the data curation into two separate tasks such that no annotator contributed to both training and evaluation datasets. This mitigates the issue of learning to model the behavior of specific annotators~\citep{Mor_Geva_2019} and annotation artifacts from annotator developing a template (e.g., ENTITY created ENTITY) across many instances. In total, \ours is created by a large number of annotators: 153 crowdworkers annotated the development and test instances, and 531 crowdworkers worked on the training instances. We also use disjoint sets of entities between training and dev/test data so a model trained on the dataset is not simply learning properties of the entities under discussion here. We discuss more in Section~\ref{sec:quality-verification}.

During the annotation process, we monitored the sentence quality and barred crowdworkers who repeatedly produced low-quality sentences or examples following a single pattern. We then inspected the examples included in our evaluation dataset. During the inspection, we found some claims that are subjective, ambiguous, or non-factual (see Appendix~\ref{app:examples}). These errors potentially lower the human performance on the development and test sets. Since automatically detecting these errors is non-trivial, the authors manually filtered all claims in the evaluation dataset. This process removed roughly 18\% of crowdsourced claims. This process was crucial for very high human performance (99\% majority human performance), as we will see in the experiments. 

\paragraph{Contrast Set}
The authors of the paper created a small subset of contrastive examples \citep{Divyansh_Kaushik_2019, Matt_Gardner_2020}. We select 100 seed claims from the evaluation set, then annotate true and false claims based on the seed claims by applying minimal modification (e.g., replacing a word with a similar one that changes the truth of the claim). Examples can be found in Appendix~\ref{app:examples}.

\subsection{Dataset Analysis}\label{sec:quality-verification}
In this section, we examine the quality of our dataset. We first manually examine what types of reasoning are required to verify our claims. Then, we study potential lexical and syntactic artifacts in human-generated claims through statistical tests and training dynamics to identify word-level artifacts and learnability of the training instances. 
\paragraph{Manual Analysis of Reasoning Types}

We manually validate whether the \ours claims truly require both knowledge and commonsense. We classify reasoning types into three categories: 1) retrieval, 2) common sense, and 3) a mix of retrieval and common sense. These distinctions are somewhat subjective based on the background knowledge of the reader (i.e., is it common sense that NYC is a major city?); we use our own judgments as authors. The first category, retrieval, asks simple facts about entities which can be found in some knowledge sources such as English Wikipedia (e.g., \textit{The Harry Potter series originally began with the books.}). The second category, common sense, requires more complex reasoning but are verifiable with the basic knowledge of the entities (e.g., \textit{Only trumpet players can perform a solo.}). The third category is a mix of retrieval and common sense, meaning that it involves some degree of retrieval and commonsense reasoning. For example, the claim \textit{One can drive from La Jolla to New York City in less than two hours.} requires knowing the locations of La Jolla and New York City (retrieval) and reasoning about driving times (common sense). We randomly sample 100 claims from the evaluation instances and classify them into the three categories. The proportion of the retrieval, common sense, and a mix of the two categories is 18\%, 28\%, and 54\% respectively. You can find examples for each reasoning type in Appendix~\ref{app:examples}.

\paragraph{Dataset Artifacts} Past work on natural language inference has noted that ``artifacts,'' or spurious correlations with surface properties of text, may arise during the annotation process \citep{Suchin_Gururangan_2018,poliak-etal-2018-hypothesis}. The low performance of a bag-of-words model in our setting (see Table~\ref{tab:main_result}) gives some confidence that such correlations are not a dominant factor in performance on our data, but we undertake quantitative analysis to explore this further.

We identify the word-level artifacts in \ours by computing the artifact statistics described in \citet{Matt_Gardner_2021}. These statistics tell us, given a balanced dataset, if some words are highly correlated with either true or false claims in a way that a model can exploit. This boils down to a one-side binomial hypothesis test with the null hypothesis $p (y | x_i) = 0.5$, where $y \in \{\texttt{TRUE}, \texttt{FALSE}\}$ is a label and $x_i$ is a word in the vocabulary. We first count the occurrence of all words\footnote{We drop punctuation and lower all words.} in \textsc{\ours}. For each word $x_i$ that appears in the $n_i$ claims, we count the number of the target label $y$ in the $n_i$ claims. We estimate $p (y | x_i)$ with the observed probability $\hat p (y | x_i)$, which is given by a fraction of the count of $y$ over $n_i$. Following \cite{Matt_Gardner_2021}, we then compute a $z$-statistic and reject/accept the null hypothesis using $\alpha = 0.01$ with the Bonferroni correction.

Figure~\ref{fig:competency} plots the word counts $n_i$ ($x$-axis) against the observed probability $\hat p (y | x_i)$ ($y$-axis) for \ours and FEVER dataset. We additionally draw the curve that represents the corresponding probability of $\alpha = 0.01$/13k (for \oursno) and $\alpha = 0.01$/10k (for FEVER) at each $n_i$. Any words above this line are considered to be artifacts in the dataset. We find 14 words (out of 13k words in the vocabulary) sit above the line. We label the most frequent words in the plot. Surprisingly, \emph{and} ($n=1973$) is the most frequent artifact that signals the true label, followed by some quantifiers (\emph{many}, $n=483$, and \emph{several}, $n=119$). \emph{not} ($n=274$) and \emph{only} ($n=186$) suggest the false label in both datasets. Overall, \textsc{\ours} contains relatively few artifacts, and they do not impact the data quality significantly since their frequency is not very high. We observe fewer artifacts compared to FEVER dataset (14 words vs. 28 words above the threshold).

\begin{figure}
     \centering
     \begin{minipage}{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{artifacts_train_2.pdf}
         \caption*{(a) \ours}
         \label{fig1:dev}
     \end{minipage}
     \hspace{10pt}
     \begin{minipage}{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{artifacts_fever_train_3.pdf}
         \caption*{(b) FEVER}
         \label{fig:contrast}
     \end{minipage}
     \caption{Artifact statistics of \ours and FEVER train sets. Words (colored dots) above the green line have detectable correlation with class labels. \ours contains relatively fewer artifacts, with low severity and frequency.}
     \label{fig:competency}
\end{figure}

\paragraph{Training Dynamics} We analyze training dynamics using the framework proposed by \citet{Swabha_Swayamdipta_2020}. The training dynamics of a training instance are defined by \emph{confidence} and \emph{variability},the mean and the standard deviation of model predictions (probability) on the gold label over training epochs. Additionally, \emph{correctness} is computed by the number of times a training instance is correctly predicted over the number of epochs. Figure~\ref{fig:trainin-dynamics} shows the histograms of those measurements\footnote{All values are normalized between 0 and 1 then bucketed into subgroups.} for \ours (10k instances) and FEVER (105k instances). We use $\textsc{RoBERTa}_{\:\text{Large}}$\footnote{Following the suggestions by \citet{Swabha_Swayamdipta_2020}, we train models with early stopping with the patience $= 3$, resulting in 7 epochs for \ours and  6 epochs for FEVER.} for all experiments. In the \emph{confidence} plots, \ours has a fatter distribution (i.e., certain instances get low probability on their gold labels) compared to FEVER's skewed distribution where the majority of instances get very high probability (e.g., > 0.9) on the gold labels. \oursno's \emph{variability} histogram is nearly bell-shaped while FEVER's histogram skews towards zero. As can be seen in the \emph{correctness} plots, some training instances in \ours are not always predicted correctly during training, as its distribution suggests. However, the most of training instances of FEVER are correctly predicted consistently through the training epochs. By aggregating these observations, we hypothesize that \ours contains training instances with different difficulty levels compared to FEVER. 

\begin{figure}
     \centering
     \begin{minipage}{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{ENTITY_hist.pdf}
         \caption*{(a) \ours}
         \label{fig1:dev}
     \end{minipage}
     \hspace{10pt}
     \begin{minipage}{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{FEVER_hist.pdf}
         \caption*{(b) FEVER}
         \label{fig:contrast}
     \end{minipage}
     \caption{Training dynamics for \ours and FEVER train sets. This figure shows histograms of \ours or FEVER  training instances bucketed by \textit{confidence} (mean), \textit{variability} (std.), or \textit{correctness}. On all three measures, \ours shows fatter distributions compared to FEVER, implying that \ours consists of instances with different difficulties.}
     \label{fig:trainin-dynamics}
\end{figure}

\section{Experiments}
\label{sec:experiments}

We focus on the closed-book setting where models are ask to make decisions based solely on claims without any additional retrieved evidence. To see if existing claim verification datasets provide entity commonsense supervision, we train claim-only baseline models on \textsc{FEVER}~\citep{fever}, \textsc{FaVIQ}~\citep{Jungsoo_Park_2021}, and \textsc{FoolMeTwice (FM2)}~\citep{Julian_Eisenschlos_2021} and then evaluate them on \ours. Next, we train models on the \ours training set and measure the improvements over the baselines. We also investigate the impacts of model sizes and external knowledge.

\subsection{Experimental Setup} 

We investigate three training data settings. In the \emph{Zero-Shot} setting, we train models on the train sets of $\textsc{FEVER}_{\:\text{KILT}}$, \textsc{FaVIQ-a}, \textsc{FaVIQ-r},\footnote{The \textsc{FaVIQ} benchmark consists of two datasets based on the same source QA dataset.} and \textsc{FoolMeTwice (FM2)}. In the \emph{In-Domain} setting, we train models on the \ours train set in a standard fashion. The \emph{Finetuning} setting means that we train models on \textsc{FEVER} and then further finetune on \oursno. 

We evaluate all models on the \ours balanced development, test, and contrast sets and report accuracy. As we discussed in Section~\ref{sec:dataset}, these evaluation sets use distinct entities from the train set and are authored by a different set of crowdworkers. 

\subsection{Comparison Systems}

\paragraph{Closed-book (Claim-only) Models} In what we consider our standard setting, these models take a claim as input and predict if the claim is true or false. We use a RoBERTa encoder \citep{Yinhan_Liu_19} with a MLP classifier for baseline models: $\textsc{RoBERTa}_{\:\text{Large}}$ and $\textsc{RoBERTa}_{\:\text{Base}}$. We also train SVM with TF-IDF, which gives a linear baseline using far fewer parameters than the LM-based models. We further employ $\textsc{T5-3b}$ to see if more parameters help to learn the complex reasoning in \oursno. 

\paragraph{Retrieval-based Models} These models are augmented with knowledge retrieved from Wikipedia. We feed a claim and $k$ retrieved passages to a model, which can use the information in the passages to influence the decision. We use Dense Passage Retrieval (DPR) \citep{dpr}\footnote{DPR is licensed under CC BY-NC 4.0}, a dual-encoder based model, as a retriever and English Wikipedia as a knowledge base. Specifically, we use the DPR model trained on the KILT benchmark, which includes FEVER. We use this configuration for the open-book experiments, where we finetune models on our training set as well as on FEVER. For a claim classifier, we use the $\textsc{RoBERTa}_{\:\text{Large}}$ model and denote this retrieval-based model as $\textsc{RoBERTa}_{\:\text{Large-DPR}}$. We retrieve $k=3$ passages for all experiments.

\paragraph{Human Performance} To estimate human performance on the development set, we sample 100 examples and ask the authors of this paper to predict the corresponding labels. For the contrast set, three of the authors predict labels for claims that they did not annotate. We report the averaged human accuracy and the ensemble accuracy which we use the majority label as the final prediction to computer human performance. 

\renewcommand{\arraystretch}{1}
\begin{table*}[t]
	\centering
	\small
		\caption{Performance of closed-book approaches on \oursno. Transfer results from prior datasets show that our dataset is distnct from these. Larger models trained with in-domain data perform the best out of all models we consider, but still lag behind human performance.}
	\setlength{\tabcolsep}{4pt}
	\begin{tabular}{l l c c c c c c c}
		\toprule
		\multicolumn{1}{c}{\multirow{2}{*}{}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Model}}}  & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{\#Params}}} & \multicolumn{2}{c}{\textbf{Training Data}} &&  \multicolumn{3}{c}{\multirow{1}{*}{\textbf{Accuracy}}}\\
		& \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{Type} &  \multicolumn{1}{c}{Size} && \multicolumn{1}{c}{Dev}  & \multicolumn{1}{c}{Test} & \multicolumn{1}{c}{Contrast}\\
		\midrule
		& Majority Label & -- & -- & -- && 51.6 & 51.6  & 50.0 \\ 
		\midrule
		\multirow{5}{*}{\textit{Zero-Shot}} & $\textsc{RoBERTa}_{\:\text{Large}}$  & 355M & \textsc{FaVIQ-r} & 141k && 49.6 & 48.4 & 50.0  \\
		& $\textsc{RoBERTa}_{\:\text{Large}}$ & 355M & \textsc{FaVIQ-a} & 17k && 52.3 & 52.6 & 52.0 \\
		& $\textsc{RoBERTa}_{\:\text{Large}}$ & 355M & \textsc{FM2} & 10k && 59.2  & 58.2 & 52.0 \\
        & $\textsc{RoBERTa}_{\:\text{Large}}$ & 355M & $\textsc{FEVER}_{\:\text{KILT}}$ & 105k && 69.6 & 70.2  & 59.0  \\
		& $\textsc{T5-3b}$ & 3B & $\textsc{FEVER}_{\:\text{KILT}}$ & 105k && 72.9 & 76.7 & 61.5  \\
		\midrule
		\multirow{4}{*}{\textit{In-Domain}} & $\textsc{SVM + TF-IDF}$ & 13k & \textsc{\ours} & 10k && 60.2 & 60.3  & 52.0 \\
		& $\textsc{RoBERTa}_{\:\text{Base}}$ & 125M & \textsc{\ours} & 10k && 72.2 & 71.6 & 56.0 \\
		& $\textsc{RoBERTa}_{\:\text{Large}}$ & 355M & \textsc{\ours} & 10k && 80.6 & 80.3 & 61.5 \\
		& $\textsc{T5-3b}$ & 3B & \textsc{\ours} & 10k && \textbf{85.6} & \textbf{85.1} & \textbf{70.0} \\
		\midrule
		\multirow{1}{*}{\textit{Finetuning}} & $\textsc{RoBERTa}_{\:\text{Large}}$ & 355M & 
		$\textsc{FEV} \rightarrow$ \textsc{\ours} & 115k && 80.5 & 81.1 &  64.0  \\
		\midrule
		& Human (averaged)  & -- & -- & -- && 96.3 & --  & 92.2 \\  
		& Human (ensemble)  & -- & -- & -- && 99.0 & --  & 99.0 \\  
		\bottomrule 
	\end{tabular} \label{tab:main_result}
\end{table*}

\section{Results and Discussion}

Table~\ref{tab:main_result} presents our main experimental results for closed book systems, and Table~\ref{tab:open-book} presents results for retrieval augmented approaches. We observe that all baseline models fall behind our estimated human performance by a substantial margin. 

\paragraph{Transfer from existing datasets} The \emph{zero-shot} block of Table~\ref{tab:main_result} compares performance of RoBERTa models trained on four prior claim verification datasets. The models trained on \textsc{FaVIQ-r} and \textsc{FaVIQ-a} perform similarly with the majority label baseline. The model trained on \textsc{FM2} shows better performance than the \textsc{FaVIQ} models, but the accuracy is still very low. We see much improved transfer from $\textsc{FEVER}_{\:\text{KILT}}$ dataset, reaching an accuracy of 70\%. Although designed to be more challenging than FEVER, \textsc{FaVIQ} and \textsc{FM2} may result in models that transfer less well because these datasets are more dependent on retrieving specific passages to judge claims, containing fewer claims resolvable with commonsense reasoning. Additionally,  $\textsc{T5-3b}$ trained on $\textsc{FEVER}_{\:\text{KILT}}$ is only better than $\textsc{RoBERTa}_{\:\text{Large}}$ by 3 points on the development set and 6.5 points on the test set although it is 8 times larger, suggesting that $\textsc{FEVER}_{\:\text{KILT}}$ is bounded in terms of how useful it can be for \ours. 

In the \emph{Finetuning} block of Table~\ref{tab:main_result}, we report the performance of $\textsc{RoBERTa}_{\:\text{Large}}$ first trained on  $\textsc{FEVER}_{\:\text{KILT}}$ and then on \oursno. Compared to $\textsc{RoBERTa}_{\:\text{Large}}$ trained only \oursno, additional pre-training does not bring meaningful gains. 
 
\paragraph{Are larger models better?} The \emph{In-Domain} block of Table~\ref{tab:main_result} lists performance by models with different sizes ranging from 13k to 3B parameters. All models are trained on \oursno. $\textsc{RoBERTa}_{\:\text{Base}}$, outperforms SVM with TF-IDF by 11 points on the test set, suggesting that a larger, knowledge-rich model can do better. But its advantage shrinks on the contrast set, only gaining 4 points over SVM with TF-IDF. A larger model $\textsc{RoBERTa}_{\:\text{Large}}$, 355M parameters, further improves the performance, and this trend continues to an even larger model $\textsc{T5-3b}$, which outperforms $\textsc{RoBERTa}_{\:\text{Large}}$ by 5 points on the test set and 8.5 points on the contrast set. $\textsc{T5-3b}$ achieves the highest accuracy in the closed-book setting. Given how the contrast set was constructed, the fact that higher-capacity models work better suggests that having more entity knowledge is a key to doing better on this task.

\begin{figure}
     \centering
     \begin{minipage}{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{entity_pageview_plot.pdf}
         \caption*{(a) Accuracy by entity popularity}
         \label{fig1:eval_entity_pop}
     \end{minipage}
     \begin{minipage}{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{entity_type_plot.pdf}
         \caption*{(b) Accuracy by entity type}
         \label{fig:eval_entity_type}
     \end{minipage}
     \caption{Performance breakdown on partitions of the development set, split by entity popularity and type, using the four most common entity types, which comprise 86\% of examples (includng ``NONE'' for non-named entities).}
     \label{fig:eval_breakdown}
     \vspace{-8pt}
\end{figure}
\paragraph{Performance breakdown by entity types}
We examine whether models are better equipped at verify claims about different entities depending on their popularity and type, as given by an NER tagger. We use $\textsc{RoBERTa}_{\:\text{Large}}$ as an in-domain baseline model for this analysis. To compare entity popularity, we partition our dataset into equally sized quartiles based on total number of views the entity's Wikipedia page has received since Jan.~1, 2016. For entity types, we use an off-the-shelf NER tagger from spaCy \citep{spacy} to group examples by the entity type. In Figure~\ref{fig:eval_breakdown}, we plot the performance on each partition of our dataset. We observe that the model performs comparably regardless of entity popularity, partially because we sampled from popular entities, and that entity type has a greater affect on accuracy.
\paragraph{Retrieval-based models with external knowledge}
To investigate the importance of entity knowledge in \textsc{\ours}, we experiment in two additional settings. First, to confirm that entities are important, we experiment with the closed-book setting where all entities are dropped from the claims; this data is denoted as \textsc{Creak}$_\textsc{ent\_less}$. Second, we explore the retrieval setting, where we append three English Wikipedia passages retrieved by DPR to the claims. Similar to the main experiments, we use three data settings: \emph{Zero-Shot}, \emph{In-Domain}, and \emph{Finetuning}. 

Table~\ref{tab:open-book} shows the results of all models. $\textsc{RoBERTa}_{\:\text{Large}}$ trained on \textsc{Creak}$_\textsc{ent\_less}$ loses 10 points compared to the model trained on the standard \textsc{\ours} training set (\emph{In-Domain} $\textsc{RoBERTa}_{\:\text{Large}}$ in Table~\ref{tab:main_result}). This shows that seeing the entity mention in the claim is important. For open-book models, we again see that \emph{In-Domain} models are better than \emph{Zero-Shot} models. One distinction from the closed-book setting is that the additional finetuning on $\textsc{FEVER}_{\:\text{KILT}}$ improves performance. If we compare the \emph{In-Domain} model from the closed and retrieval settings, the additional passages bring 4 points of improvement. Although adding more entity knowledge improves performance on \textsc{\oursno}, there is still a gap from the human performance, particularly on the contrast set. This shows that there are some facts immediately retrievable from Wikipedia; however, our analysis of the dataset also shows that significant additional reasoning is required as well. Moreover, we believe that this kind of knowledge should be accessible to models in a closed-book way, as annotators were able to create these examples without consulting Wikipedia or other knowledge sources. 

\renewcommand{\arraystretch}{1}
\begin{table*}[t]
	\centering
	\small
		\caption{Performance of retrieval-augmented approaches on \oursno. Large models retrieving from Wikipedia can do better, although the performance on the contrast set is still low.}
	\setlength{\tabcolsep}{4pt}
    \centering
	\begin{tabular}{ll c c c c c c c}
		\toprule
		&\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Model}}}  & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{\#Params}}} & \multicolumn{2}{c}{\textbf{Training Data}} &&  \multicolumn{3}{c}{\multirow{1}{*}{\textbf{Accuracy}}}\\
		&& \multicolumn{1}{c}{} & \multicolumn{1}{c}{Type} &  \multicolumn{1}{c}{Size} && \multicolumn{1}{c}{Dev}  & \multicolumn{1}{c}{Test} & \multicolumn{1}{c}{Contrast}\\
		\midrule
		&Majority Label & -- & -- & -- && 51.6 & 51.6  & 50.0 \\ 
		\midrule
		\textit{Zero-Shot}&$\textsc{RoBERTa}_{\:\text{Large+DPR}}$ & 575M & $\textsc{FEVER}_{\:\text{KILT}}$ & 105k && 79.9 & 80.7 & 68.0  \\
		\midrule
		\multirow{2}{*}{\textit{In-Domain}} & $\textsc{RoBERTa}_{\:\text{Large}}$ & 355M & \textsc{Creak}$_\textsc{ent\_less}$ & 10k && 69.2 & 67.3 & 52.0 \\
		&$\textsc{RoBERTa}_{\:\text{Large+DPR}}$ & 575M & \textsc{\ours} & 10k && 84.0 & 84.3 & 70.0 \\
		\midrule
		\textit{Finetuning}&$\textsc{RoBERTa}_{\:\text{Large+DPR}}$ & 575M & $\textsc{FEV} \rightarrow$ \textsc{\ours} & 115k && \textbf{88.7} & \textbf{86.8} & \textbf{72.0} \\
		\midrule
		&Human (averaged)  & -- & -- & -- && 96.3 & --  & 92.2 \\  
		&Human (ensemble)  & -- & -- & -- && 99.0 & --  & 99.0 \\  
		\bottomrule 
	\end{tabular} \label{tab:open-book}
	\vspace{-4pt}
\end{table*}

\section{Conclusion}

We have presented a dataset \ours of binary claims involving ``entity commonsense,'' a combination of entity knowledge and commonsense reasoning. This dataset is useful both as a training set for instilling this kind of reasoning into models as well as a test set for probing whether models can recognize factually incorrect statements about entities. 
We believe this can be a useful proving ground for models infused with entity knowledge (e.g., entities-as-experts \citep{fevry-etal-2020-entities} or interpretable entity embeddings \citep{onoe-durrett-2020-interpretable}) and contribute to development of these techniques.

\paragraph{Limitations and Ethical Concerns} We emphasize that our dataset is not intended for training general fact-checking models; we do not support large-scale deployment of models trained on \ours for this purpose. Furthermore, while we have tried to measure artifacts in this dataset and found them to be minimal, our claims are artificially generated and the nature of the dataset can differ significantly from claims naturally occurring in social media or web. 
Large language models fine-tuned on our dataset may preserve biases learned from the web text during pre-training or biases of our annotators and make biased judgments as a result. See the datasheet in the Supplementary Material for more information about specific harms that could arise from this dataset.

\section*{Acknowledgments}

This work was partially supported by NSF Grant IIS-1814522; this support included funding for the data annotation. The authors would like to thank Mor Geva for providing the raw list of entities used in StrategyQA, as well as the Mechanical Turk annotators who participated in our task.

\medskip

{
\small
}

\newpage

\end{document}