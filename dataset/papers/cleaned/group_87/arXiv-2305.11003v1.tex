\title{Weakly-Supervised Concealed Object Segmentation with SAM-based Pseudo Labeling and Multi-scale Feature Grouping}

\begin{document}

\maketitle

\begin{abstract}
Weakly-Supervised Concealed Object Segmentation (WSCOS) aims to  segment objects well blended with surrounding environments using sparsely-annotated data for model training. It remains a challenging task since (1) it is hard to distinguish concealed objects from the background due to the intrinsic similarity and (2) the sparsely-annotated training data only provide weak supervision for model learning. In this paper, we propose a new WSCOS method to address these two challenges. To tackle the intrinsic similarity challenge, we design a multi-scale feature grouping module that first groups features at different granularities and then aggregates these grouping results. By grouping similar features together, it encourages segmentation coherence, helping obtain complete segmentation results for both single and multiple-object images. For the weak supervision challenge, we utilize the recently-proposed vision foundation model, ``\emph{Segment Anything Model (SAM)}'', and use the provided sparse annotations as prompts to generate segmentation masks, which are used to train the model. To alleviate the impact of low-quality segmentation masks, we further propose a series of strategies, including multi-augmentation result ensemble, entropy-based pixel-level weighting, and entropy-based image-level selection. These strategies help provide more reliable supervision to train the segmentation model. We verify the effectiveness of our method on various WSCOS tasks, and experiments demonstrate that our method achieves state-of-the-art performance on these tasks. 
\end{abstract}

\setlength{\abovedisplayskip}{2pt}
\setlength{\belowdisplayskip}{2pt}

\section{Introduction}
Concealed object segmentation (COS) aims to segment objects visually blended with surrounding environments \cite{fan2021concealed}. COS is a general term with different applications, e.g., camouflaged object detection~\cite{fan2020camouflaged,yang2021uncertainty}, polyp image segmentation~\cite{fan2020pranet,zhang2020adaptive}, transparent object detection~\cite{mei2020don,xie2020segmenting},  etc. 
COS is a challenging task due to the intrinsic between foreground objects and background, which makes it extremely difficult to identify discriminative clues for accurate foreground-background separation. To address this challenge, existing methods have employed approaches that mimic human vision~\cite{zhang2022preynet,mei2021camouflaged,pang2022zoom}, introduce frequency information~\cite{zhong2022detecting,He2023Camouflaged}, or adopt joint modeling in multiple tasks~\cite{zhai2021mutual,sun2022boundary,zhaiexploring,lv2021simultaneously,jia2022segment,zhu2022can}.

Weakly-Supervised COS (WSCOS) studies an even more challenging yet more  practical problem, involving learning a COS model without relying on pixel-wise fully-annotated training data. 
WSCOS greatly reduces annotation costs by only requiring a few annotated points or scribbles in the foreground or background. However, the sparsity of annotated training data diminishes the limited discrimination capacity of the segmenter during model learning, thus further restricting segmentation performance.

In this paper, we propose a new algorithm for the challenging WSCOS task. To tackle the intrinsic similarity of foreground and background, we introduce a Multi-scale Feature Grouping (MFG) module that first evacuates discriminative cues at different granularities and then aggregates these cues to handle various concealing scenarios. By performing feature grouping, MFG essentially promotes coherence among features and thus is able to alleviate incomplete segmentation by encouraging local correlation within individual objects, while also facilitating multiple-object segmentation by seeking global coherence across multiple objects.

To address the challenge of weak supervision, we propose to leverage the recently proposed vision foundation model, \emph{Segment Anything Model (SAM)}, to generate dense masks by using sparse annotations as prompts, and use the generated masks as pseudo labels to train a segmenter. However, due to the intrinsic similarity between foreground objects and the background, the pseudo labels generated by SAM may not always be reliable. We propose a series of strategies to address this problem. First, we propose to generate multiple augmentation views for each image and fuse the segmentation masks produced from all views. The fused mask can highlight reliable predictions resistant to image augmentations and tend to be more accurate and complete as an effect of ensemble. Second, we propose an entropy-based weighting mechanism that assigns higher weights to predictions of pixels of high certainty. Lastly, to deal with the extreme images that SAM fails to generate reasonably correct masks, we propose an entropy-based image-level selection technique to assess the quality of the generated mask and decide whether to use the masks as pseudo labels for model training. These strategies ensure that only high-quality pseudo labels are used for training the segmenter. For ease of description, we refer to our solution of using SAM to address this task as \textit{WS-SAM}.
 

Our contributions are summarized as follows: 

(1) We propose to leverage SAM for weakly-supervised segmentation by using the provided sparse annotations as prompts to generate dense segmentation masks and train a task segmentation model. To the best of our knowledge, this is the first attempt to leverage the vision foundation model to address the weakly-supervised segmentation task.

(2) We propose a series of strategies for dealing with potentially low segmentation mask quality, including the multi-augmentation result ensemble technique, entropy-based pixel-level weighting technique, and entropy-based image-level selection technique. These techniques help provide reliable guidance to train the model and lead to improved segmentation results.

(3) We introduce a Multi-scale Feature Grouping (MFG) technique to tackle the intrinsic similarity challenge in the WSCOS task. MFG evacuates discriminative cues by performing feature grouping at different granularities. It encourages segmentation coherence, facilitating to obtain complete segmentation results for both single and multiple object images.

(4) We evaluate our method on various WSCOS tasks, and the experiments demonstrate that our method achieves state-of-the-art performance.  

\section{Related Works}
\noindent \textbf{Segment Anything Model}. SAM~\cite{kirillov2023segment} is a recently-proposed vision foundation model trained on SA-1B of over 1 billion masks. Its primary objective is to segment any object in any given image without requiring any additional task-specific adaptation. Its outstanding quality in segmentation results and zero-shot generalization to new scenes make SAM a promising candidate for various computer vision tasks. However, recent studies have highlighted that SAM encounters difficulties when segmenting objects with poor visibility, such as camouflaged objects~\cite{ji2023sam,tang2023can,ji2023segment}, medical polyps~\cite{mazurowski2023segment,cheng2023sam,mattjie2023exploring,zhou2023can}, and transparent glasses~\cite{han2023segment,ji2023segment}. These findings suggest that SAM still has limitations in COS tasks.

In this paper, we propose using SAM to generate dense segmentation masks from sparse annotations and introduce the first SAM-based weakly-supervised framework in COS, termed WS-SAM. To further increase the accuracy of the generated pseudo-labels, we propose a pseudo label refinement strategy. Such a strategy assigns higher weights to those reliable predictions that are resistant to various image augmentation. Therefore, WS-SAM can offer more precise and stable guidance for the learning process, ultimately boosting the segmentation performance of the segmenter.

\noindent \textbf{Concealed Object Segmentation}. With the rapid development of deep learning, learning-based segmenters have obtained great achievements in the fully-supervised COS tasks~\cite{fan2020pranet,fan2020camouflaged,lin2021rich}. PraNet~\cite{fan2020pranet} proposed a parallel reverse attention network to segment polyps in colonoscopy images. Drawn inspiration from biology, SINet~\cite{fan2020camouflaged} designed a predator network to discover and locate camouflaged objects. To detect transparent objects, GSDNet~\cite{lin2021rich} integrated an aggregation module and a reflection refinement module. However, there is limited research on the weakly-supervised COS task. SCOD~\cite{he2022weakly} introduced the first weakly-supervised COD framework, but it is only supervised with sparse annotations, which greatly restricts its discrimination capacity and inevitably inhibits segmentation performance. To address this problem, we first propose using SAM to generate precise pseudo-labels. Besides, to tackle intrinsic similarity, we introduce the multi-scale feature grouping module to evacuate discriminative cues at different granularities and thus promote feature coherence.

\begin{figure}[t]
	\centering
	\setlength{\abovecaptionskip}{-0.1cm}
	\begin{center}
		\includegraphics[width=\linewidth]{Figure/SWSCOS.pdf}
	\end{center}
	\caption{Framework of WS-SAM with scribble supervision. Note that the corresponding masks of the augmented images are inversely transformed so as to be consistent with the original image.
 }
	\label{fig:SWSCOS}
	\vspace{-0.3cm}
\end{figure}

\section{Methodology}
Weakly-Supervised Concealed Object Segmentation (WSCOS) aims to learn a segmentation model from a sparsely-annotated training dataset $\mathcal{S} = \{\mathbf{X}_i, \mathbf{Y}_i\}_{i=1}^{S}$ and test the model on a test dataset $\mathcal{T} = \{\mathbf{T}_i\}_{i=1}^{T}$, where $\mathbf{X}_i$ and $\mathbf{T}_i$ denote the training and test images, respectively; $\mathbf{Y}_i$ represents the sparse annotations, which could be a few points or scribbles annotated as foreground or background.    

Learning the segmentation model could be a challenging task, as concealed objects usually blend well with their surrounding environment, making it hard to distinguish foreground from background. Besides, the sparse annotations $\mathbf{Y}_i$ may not provide sufficient supervision to learn the model capable of making accurate dense predictions. To address these challenges, we first propose a strategy of leveraging the recently-proposed vision foundation model, \emph{Segment Anything Model (SAM)}, to generate high-quality dense masks from sparse annotations and use the dense masks as pseudo labels to train the segmentation model. In addition, we propose a Multi-scale Feature Grouping (MFG) module that groups features at different granularities, encouraging segmentation coherence and facilitating obtain complete segmentation results for various concealing scenarios. 

\subsection{Pseudo Labeling with SAM}\label{Sec:SWSCOS}

SAM is a recently-released vision foundation model for generic object segmentation \cite{kirillov2023segment}. It is trained with more than one billion segmentation masks and has shown impressive capabilities of producing precise segmentation masks for a wide range of object categories (so-called ``\emph{segment anything}''). 
Unlike some enthusiasts who brag SAM has ``killed'' the segment task, we find that SAM is far from reaching that level, at least for the studied concealed object segmentation task. This is first because SAM requires ``prompts'' that provide clues about the objects of interest to produce segmentation results. While the prompts could be in many forms, e.g., points, masks, bounding boxes, etc., they are required to be provided by humans or other external sources (e.g., other algorithms) \footnote{SAM includes an automatic prompt generation mechanism that first densely selects points across the whole image as prompts, then generates results based on the dense prompts, and last fuse the results. The problem is it segments everything possible in the image, with no distinguishing of objects of interest and others.}.  This requirement of the additional prompt inputs makes SAM unable to be (directly) used for applications where only test images are provided.  In addition, we find that while SAM exhibits impressive performance for general scene images, it still struggles for concealed object images, due to the intrinsic similarity between foreground objects and the background. 

\begin{figure}[t]
	\centering
	\setlength{\abovecaptionskip}{-0.2cm}
	\begin{center}
		\includegraphics[width=0.9\linewidth]{Figure/ImageAugmentation.pdf}
	\end{center}
	\caption{Masks of SAM with different augmented images. We inversely transform the masks to keep consistent with the original image. It is observed that fused masks contain more accurate and complete segmentation information.
 }
	\label{fig:Augmentation}
	\vspace{-0.6cm}
\end{figure}

In this paper, we introduce SAM for the Weakly-Supervised Concealed Object Segmentation (WSCOS) task. As shown in Fig.~\ref{fig:SWSCOS}, we use SAM to generate segmentation masks on training images by taking the sparse annotations as prompts, and take the segmentation masks as pseudo labels to train a COS model, which will be used for the test. 
It is expected that the SAM-generated pseudo labels are not reliable. We address this problem by proposing three techniques, namely, Multi-augmentation result fusion, pixel-level weighting and image-level selection. 

\noindent\textbf{Multi-augmentation result fusion}. Given a concealed image $(\mathbf{X}_i, \mathbf{Y}_i)\in\mathcal{S}$, we generate $K$ augmented images $\{\mathbf{X}^k_i\}^K_{k=1}$ by applying stochastic augmentations randomly sampled from image flipping, rotation ($0^{\circ}$, $90^{\circ}$, $180^{\circ}$, $270^{\circ}$), and scaling ($\times0.5$, $\times1.0$, $\times2.0$). 
We send $\{\mathbf{X}^k_i\}^K_{k=1}$ to SAM by using the sparse annotations $\mathbf{Y}_i$ as prompts, and generate segmentation masks $\{\mathbf{M}^k_i\}^K_{k=1}$, where 
\begin{equation}
    \mathbf{M}_i^k=\mathrm{SAM}\left(\mathbf{X}^k_i, \mathbf{Y}_i \right).
\end{equation}
Note that $\mathbf{M}_i^k$ has the same shape as input image $\mathbf{X}^k_i$, which may differ in shape from $\mathbf{X}_i$; we perform inverse image transformation to ensure all masks have the same shape as the original image. 

As different segmentation results can be obtained when feeding SAM with different prompts, we expect $\{\mathbf{M}^k_i\}^K_{k=1}$ to vary since different augmented images are used for segmentation. Fig. \ref{fig:Augmentation} shows some examples. We can see that while these masks vary significantly in shape, they overlap in certain regions, which are reliably predicted by SAM regardless of image transformations and usually correspond to correctly predicted foreground regions. Besides, these masks complement each other, such that some foreground regions missed by one mask can be found in other masks. Based on these observations, we propose to fuse the segmentation masks for different augmented images, as
\begin{equation}
    \tilde{\mathbf{M}}_i = \frac{1}{K}\sum_{k=1}^K \mathbf{M}^k_i,
    \label{Eq:RefinedPL}
\end{equation}
where $\tilde{\mathbf{M}}_i$ is the fused mask. We expect $\tilde{\mathbf{M}}_i$ to be more reliable than the individual masks as it is an ensemble over various augmented images. 

\noindent\textbf{Pixel-level weighting}. 
The prediction reliability of different pixels may vary. To highlight those more reliable ones, we propose to use entropy to weigh the predictions.   
We calculate the entropy of each pixel and get an entropy map as  
\begin{equation}
    \tilde{\mathbf{E}}_i = -\tilde{\mathbf{M}}_i \log \tilde{\mathbf{M}}_i -(1-\tilde{\mathbf{M}}_i) \log (1-\tilde{\mathbf{M}}_i).
    \label{Eq:entropy}
\end{equation}
As the entropy map is calculated from the fused mask, it measures the prediction uncertainty of each pixel across all augmentation as a pixel will have low entropy only if it is \textit{confidently and meanwhile consistently} predicted from all augmented images. Therefore, we can use this entropy map to weigh the fused mask $\mathbf{M}_i^k$ and assign higher weights to those reliable pixels.  

\noindent\textbf{Image-level selection}. We have observed that for some highly challenging concealed images, SAM fails to produce even reasonably correct results with the sparse annotations as the prompts, with whatever types of augmented images. This fundamentally invalidates the above pixel-wise weighting strategy. To deal with this case, we further propose an image-level selection mechanism to selectively choose images for training, further striving to provide reliable supervision to train the segmenter.

Similar to using entropy to define the pixel-level prediction uncertainty, we propose two entropy-based image-level uncertainty measurements, namely, absolute uncertainty $U_a$ and relative uncertainty $U_r$. Absolute uncertainty $U_a$ refers to the proportion of high-uncertainty pixels among all pixels, while relative uncertainty $U_r$ indicates the proportion of high-uncertainty pixels and foreground pixels with low uncertainty, which is specifically designed to accommodate small-object scenarios. We regard a pixel as a high-uncertainty pixel when its entropy is above 0.9. We define the following indicator function to decide whether to keep an image for training or not:  
\begin{equation}
\hat{\mathbf{M}}_i = \mathbbm{1}[U_a < \tau_{a}] \times \mathbbm{1}[U_r < \tau_{r}],
\end{equation}
where $\tau_{a}$ and $\tau_{r}$ are the thresholds that are set as 0.1 and 0.5 in this paper, respectively.  

Applying the entropy weights $\tilde{\mathbf{E}}_i$  on the fused mask $\tilde{\mathbf{M}}_i$ and the image selection indicator $\hat{\mathbf{M}}_i$, we reach our final mask that will be used for training the segmenter as,
\begin{equation}
\hat{\mathbf{Y}}_i = (1 - \tilde{\mathbf{E}}_i) \times \tilde{\mathbf{M}}_i \times \hat{\mathbf{M}}_i.
\end{equation}
Our technique leverages SAM to generate segmentation masks and further incorporates multi-augmentation result fusion, pixel-level uncertainty weighting, and image-level uncertainty filtering, thus being able to generate reliable pseudo labels to train the segmenter.

\begin{figure}[t]
	\centering
	\setlength{\abovecaptionskip}{-0.2cm}
	\begin{center}
		\includegraphics[width=\linewidth]{Figure/FCNet-short.pdf}
	\end{center}
 \vspace{0.5mm}
	\caption{Architecture of the proposed model. 
 $\Phi_P$ denotes feature grouping with $P$ prototypes. We simplify the broadcast process in $\Phi_P$ for space limitation.}
	\label{fig:MFG}
	\vspace{-0.6cm}
\end{figure}

\subsection{Multi-scale Feature Grouping}
\label{Sec:MFG}
The intrinsic similarity in concealed objects may cause incomplete segmentation and partial object localization in multi-object segmentation. Such problems could be further aggravated in weakly supervised scenarios due to the limited discriminative capacity of the segmenter. To address this issue, we propose a Multi-scale Feature Grouping (MFG) module that evacuates discriminative cues at various granularities. MFG achieves this by exploring the coherence of foreground/background regions and performing feature grouping at different levels. By encouraging feature coherence, MFG can alleviate incomplete segmentation by enhancing local correlation within individual objects and further facilitate multiple-object segmentation by seeking global coherence across multiple objects. 
The architecture of the proposed MFG module is illustrated in Fig.~\ref{fig:MFG}. 

\noindent\textbf{Feature grouping}. 
Suppose $\mathbf{F}\in \mathbb{R}^{H\times W\times C}$ is the feature representation of an input image. We perform feature grouping by mapping $\mathbf{F}$ to $N$ learnable cluster prototypes $\mathbf{P}\in \mathbb{R}^{N\times C}$. These cluster prototypes $\mathbf{P}$ are randomly initialized. 
We first append the learnable spatial positional embedding $\mathbf{P}_e$ to the input feature $\mathbf{F}$ and get $\mathbf{F}_p$. Then, we linearly transform the prototypes $\mathbf{P}$ and the positioned feature $\mathbf{F}_p$ into $\mathbf{Q}\in\mathbb{R}^{N\times C}$, $\mathbf{K}\in\mathbb{R}^{H W\times C}$, and $\mathbf{V}\in\mathbb{R}^{H W\times C}$:
\begin{equation}\label{eq:prototype_qkv}
    \mathbf{Q} = \mathbf{W}_q \mathbf{P}, \ \ \ \mathbf{K} = \mathbf{W}_k \mathbf{F}_p, \ \ \ \mathbf{V} = \mathbf{W}_v \mathbf{F}_p, 
\end{equation}
where $\mathbf{W}_q, \mathbf{W}_k, \mathbf{W}_v \in \mathbb{R}^{C\times C}$ are the learnable weights. 
To ensure the exclusive assignment of features to the cluster prototypes, 
we normalize the coefficients over all prototypes, 
\begin{equation}
    \bar{\mathbf{A}}_{i,j}=\frac{e^{\mathbf{A}_{i,j}}}{\sum_l e^{\mathbf{A}_{i,l}}}, \quad \mathrm{where} \quad \mathbf{A}=\frac{1}{\sqrt{C}} \mathbf{K}^{\top} \mathbf{Q}.
\end{equation}
We then calculate the integral value $\mathbf{U}$ of the input values with respect to  the prototypes as
\begin{equation}
    \mathbf{U} = \mathbf{D}^{\top} \mathbf{V}, \quad \mathrm{where} \quad \mathbf{D}_{i,j}= \frac{\mathbf{A}_{i,j}}{\sum_l \mathbf{A}_{i,l}}, 
\end{equation}
and update the prototypes $\mathbf{P}$ by feeding $\mathbf{P}$ and $\mathbf{U}$ into a Gated Recurrent Units $GRU(\cdot)$:
\begin{equation}\label{eq:prototype_GRU}
    \mathbf{P} = GRU\left(inputs=\mathbf{U}, states = \mathbf{P}\right).
\end{equation}
By repeating Eqs.~\eqref{eq:prototype_qkv} - \eqref{eq:prototype_GRU} for $T$ iterations, the cluster prototypes are iteratively updated and gradually strengthen the association between similar features, where $T=3$ in this paper.

We broadcast each prototype onto a 2D grid augmented with the learnable spatial position embedding $\mathbf{P}_e$ to obtain $\{\mathbf{F}'_i\}_{i=1}^N \in \mathbb{R}^{H\times W\times C}$, and use $1\times1$ convolution to downsample each prototype, obtaining $\{\mathbf{F}''_i\}_{i=1}^N \in \mathbb{R}^{H\times W\times C/N}$. We concatenate those prototypes and obtain $\mathbf{F}_c\in \mathbb{R}^{H\times W\times C}$. 

For ease of future use, we denote the feature grouping process with $N$ prototypes as $\mathbf{F}_c= \Phi_N(\mathbf{F})$.

\noindent\textbf{Multi-scale feature aggregation}. 
The number of prototypes $N$ in the above feature grouping technique controls the grouping granularity: a smaller value of $N$ facilitates the extraction of global information, while a larger value of $N$ can provide more valuable detailed information. To strike a balance, we propose to aggregate the multi-scale grouping features with different numbers of prototypes. Taking inspiration from the second-order Runge-Kutta (RK2) structure known for its superior numerical solutions compared to the traditional residual structure~\cite{he2016deep}, we employ RK2 to aggregate those features. Additionally, as shown in Fig.~\ref{fig:MFG}, we adopt a weighted gate mechanism $\mathbf{\alpha}_g$ to adaptively estimate the trade-off parameter rather than using a fixed coefficient. Given the feature $\mathbf{F}$, the adaptively aggregated feature $\hat{\mathbf{F}}$ is formulated as follows:
\begin{equation}
    \hat{\mathbf{F}} = \mathbf{F} +\alpha_g \Phi_{N_1}(\mathbf{F}) +(1-\alpha_g) \Phi_{N_2}(\mathbf{F}+\Phi_{N_1}(\mathbf{F})), %\quad \mathrm{where} \quad \alpha_g=S(\sigma \ cat(\Phi_{N_1}(\mathbf{F}), \Phi_{N_2}(\mathbf{F}))+\mu), 
\end{equation}
where $\alpha_g=S(\sigma \ cat(\Phi_{N_1}(\mathbf{F}), \Phi_{N_2}(\mathbf{F}+\Phi_{N_1}(\mathbf{F})))+\mu)$, $\sigma$ and $\mu$ are the learnable parameters in $\alpha_g$. $N_1$ and $N_2$ are the numbers of groups, which are empirically set as 4 and 2, respectively. 

Our multi-scale feature grouping technique is inspired by the slot attention technique~\cite{locatello2020object}, but we differ from slot attention in the following aspects. Slot attention targets at instance-level grouping in a self-supervised manner, our MFG is proposed to adaptively excavate the feature-level coherence for complete segmentation and accurate multi-object localization. To relax the segmenter and ensure generalization, we remove the auxiliary decoder used in slot attention for image reconstruction, along with the reconstruction constraint. Additionally, we employ an RK2 structure to aggregate the multiscale grouping feature with different numbers of prototypes, which further facilitates the excavation of feature coherence and therefore helps improve segmentation performance.

\subsection{Weakly-Supervised Concealed Object Segmentation}
To use the proposed MFG technique for concealed object segmentation, we integrate MFG with the encoder and decoder architecture utilized in an existing camouflaged object detection model \cite{He2023Camouflaged} to construct a novel segmenter. The model comprises a ResNet50-backed encoder $E$ that maps an input image $\mathbf{X}_i$ to a feature space as $\mathbf{F}_i = E(\mathbf{X}_i)$. Using the obtained $\mathbf{F}_i$, we apply MFG to perform multi-scale feature grouping, resulting in $\hat{\mathbf{F}}_i=MFG(\mathbf{F}_i)$. Subsequently, a decoder $D$ maps $\hat{\mathbf{F}}_i$ back to the image space, generating the predicted mask $\mathbf{Y}_i'= D(\hat{\mathbf{F}}_i)$. Fig. \ref{fig:MFG} provides a conceptual illustration of this model, and \emph{more architecture details can be found in the supplementary materials.} 

We train the whole model jointly with the sparse annotations $\mathbf{Y}_i$ and the generated segmentation mask $\hat{\mathbf{Y}}_i$ by the SAM model, as  
\begin{equation}
    L = \frac{1}{N_s} \sum_{(\mathbf{X}_i, \mathbf{Y}_i)\sim\mathcal{S}} L_{pce}\left(\mathbf{Y}_i', \mathbf{Y}_i\right)+L_{ce}(\mathbf{Y}'_i, \hat{\mathbf{Y}}_i)+L_{IoU}(\mathbf{Y}'_i, \hat{\mathbf{Y}}_i).
\end{equation}
Where the first term is the partial cross-entropy loss $L_{pce}$ used to ensure consistency between the prediction maps and the sparse annotations $\mathbf{Y}_i$ \cite{he2022weakly}. The second and third terms are the cross-entropy loss $L_{ce}$ and the intersection-over-union loss $L_{IoU}$, both calculated using the pseudo label $\hat{\mathbf{Y}}_i$ \cite{fan2021concealed}.

\begin{table}[t]
\centering
\caption{Results on COD with point supervision and scribble supervision. SCOD+ indicates integrating SCOD with our WS-SAM framework. 
The best two results are in {\color[HTML]{FF0000} \textbf{red}} and {\color[HTML]{00B0F0} \textbf{blue}} fonts.}
\resizebox{\columnwidth}{!}{
\setlength{\tabcolsep}{1mm}
\begin{tabular}{l|c|cccc|cccc|cccc|cccc}
\toprule
\multicolumn{1}{l|}{} & \multicolumn{1}{c|}{} & \multicolumn{4}{c|}{\textit{CHAMELEON} } & \multicolumn{4}{c|}{\textit{CAMO}} & \multicolumn{4}{c|}{\textit{COD10K}} & \multicolumn{4}{c}{\textit{NC4K}} \\ \cline{3-18}
\multicolumn{1}{l|}{\multirow{-2}{*}{Methods}} & \multicolumn{1}{c|}{\multirow{-2}{*}{Pub.}} & {\cellcolor{gray!40}$M$~$\downarrow$} &{\cellcolor{gray!40}$F_\beta$~$\uparrow$} &{\cellcolor{gray!40}$E_\phi$~$\uparrow$} & \multicolumn{1}{c|}{\cellcolor{gray!40}$S_\alpha$~$\uparrow$}& {\cellcolor{gray!40}$M$~$\downarrow$} &{\cellcolor{gray!40}$F_\beta$~$\uparrow$} &{\cellcolor{gray!40}$E_\phi$~$\uparrow$} & \multicolumn{1}{c|}{\cellcolor{gray!40}$S_\alpha$~$\uparrow$}& {\cellcolor{gray!40}$M$~$\downarrow$} &{\cellcolor{gray!40}$F_\beta$~$\uparrow$} &{\cellcolor{gray!40}$E_\phi$~$\uparrow$} & \multicolumn{1}{c|}{\cellcolor{gray!40}$S_\alpha$~$\uparrow$}& {\cellcolor{gray!40}$M$~$\downarrow$} &{\cellcolor{gray!40}$F_\beta$~$\uparrow$} &{\cellcolor{gray!40}$E_\phi$~$\uparrow$} & \multicolumn{1}{c}{\cellcolor{gray!40}$S_\alpha$~$\uparrow$}\\ \midrule
\multicolumn{18}{c}{Scribble Supervision} \\ \midrule
SAM~\cite{kirillov2023segment}                                         & \multicolumn{1}{c|}{---}                              & 0.207                                 & 0.595                                 & 0.647                                 & 0.635                                 & 0.160                                 & 0.597                                 & 0.639                                 & 0.643                                 & 0.093                                 & 0.673                                 & 0.737                                 & 0.730                                 & 0.118                                 & 0.675                                 & 0.723                                 & 0.717                                 \\
SAM-S~\cite{kirillov2023segment}                                         & \multicolumn{1}{c|}{---}                              & 0.076                                 & 0.729                                 & 0.820                                 & 0.650                                 & 0.105                                 & 0.682                                 & 0.774                                 & 0.731                                 & {\color[HTML]{00B0F0} \textbf{0.046}}                                & {\color[HTML]{00B0F0} \textbf{0.695}}                                 & 0.828                                 & {\color[HTML]{00B0F0} \textbf{0.772}}                                 & 0.071                                 & 0.747                                 & 0.832                                 & 0.763                                 \\
WSSA~\cite{zhang2020weakly}                                          & CVPR20                                             & 0.067                                 & 0.692                                 & 0.860                                 & 0.782                                 & 0.118                                 & 0.615                                 & 0.786                                 & 0.696                                 & 0.071                                 & 0.536                                 & 0.770                                 & 0.684                                 & 0.091                                 & 0.657                                 & 0.779                                 & 0.761                                 \\
SCWS~\cite{yu2021structure}                                       & AAAI21                                             & 0.053                                 & 0.758                                 & 0.881                                 & 0.792                                 & 0.102                                 & 0.658                                 & 0.795                                 & 0.713                                 & 0.055                                 & 0.602                                 & 0.805                                 & 0.710                                 & 0.073                                 & 0.723                                 & 0.814                                 & 0.784                                 \\
TEL~\cite{liang2022tree}                                           & CVPR22                                             & 0.073                                 & 0.708                                 & 0.827                                 & 0.785                                 & 0.104                                 & 0.681                                 & 0.797                                 & 0.717                                 & 0.057                                 & 0.633                                 & 0.826                                 & 0.724                                 & 0.075                                 & 0.754                                 & 0.832                                 & 0.782                                 \\
SCOD~\cite{he2022weakly}                                         & AAAI23                                             & {\color[HTML]{FF0000} \textbf{0.046}} & {\color[HTML]{00B0F0} \textbf{0.791}} & {\color[HTML]{00B0F0} \textbf{0.897}} & 0.818                                 & {\color[HTML]{00B0F0} \textbf{0.092}} & 0.709                                 & 0.815                                 & 0.735                                 & 0.049                                 & 0.637                                 & 0.832                                 & 0.733                                 & 0.064                                 & 0.751                                 & 0.853                                 & 0.779                                 \\
\rowcolor{c2!20} SCOD+ & \multicolumn{1}{c|}{---} & {\color[HTML]{FF0000} \textbf{0.046}} & {\color[HTML]{FF0000} \textbf{0.797}} & {\color[HTML]{FF0000} \textbf{0.900}} & {\color[HTML]{00B0F0} \textbf{0.820}} & {\color[HTML]{FF0000} \textbf{0.090}} &{\color[HTML]{00B0F0} \textbf{0.716}} & {\color[HTML]{FF0000} \textbf{0.818}} &{\color[HTML]{00B0F0} \textbf{0.741}} &0.047 & 0.650 &{\color[HTML]{00B0F0} \textbf{0.845}} &0.742 & {\color[HTML]{00B0F0} \textbf{0.060}} &{\color[HTML]{00B0F0} \textbf{0.766}} &{\color[HTML]{00B0F0} \textbf{0.862}} &{\color[HTML]{00B0F0} \textbf{0.785}}\\
\rowcolor{c2!20}Ours & \multicolumn{1}{c|}{---} & {\color[HTML]{FF0000} \textbf{0.046}} & 0.777    & {\color[HTML]{00B0F0} \textbf{0.897}} & {\color[HTML]{FF0000} \textbf{0.824}} & {\color[HTML]{00B0F0} \textbf{0.092}} & {\color[HTML]{FF0000} \textbf{0.742}} & {\color[HTML]{FF0000} \textbf{0.818}} & {\color[HTML]{FF0000} \textbf{0.759}} & {\color[HTML]{FF0000} \textbf{0.038}} & {\color[HTML]{FF0000} \textbf{0.719}} & {\color[HTML]{FF0000} \textbf{0.878}} & {\color[HTML]{FF0000} \textbf{0.803}} & {\color[HTML]{FF0000} \textbf{0.052}} & {\color[HTML]{FF0000} \textbf{0.802}} & {\color[HTML]{FF0000} \textbf{0.886}} & {\color[HTML]{FF0000} \textbf{0.829}} \\ \midrule
\multicolumn{18}{c}{Point Supervision} \\ \midrule
SAM~\cite{kirillov2023segment} & \multicolumn{1}{c|}{---} & 0.207 & 0.595                                 & 0.647                                 & 0.635                                 & 0.160                                 & 0.597                                 & 0.639                                 & 0.643                                 & 0.093                                 & 0.673                                 & 0.737                                 & 0.730                                 & 0.118                                 & 0.675                                 & 0.723                                 & 0.717                                 \\
SAM-P~\cite{kirillov2023segment} & \multicolumn{1}{c|}{---} & 0.101 & 0.696                                 & 0.745                                 & 0.697                                 & {\color[HTML]{00B0F0} \textbf{0.123}}                                 & 0.649                                 & {\color[HTML]{00B0F0} \textbf{0.693}}                                 & 0.677                                 & 0.069                                 & {\color[HTML]{00B0F0} \textbf{0.694}}                                 & 0.796                                 & {\color[HTML]{00B0F0} \textbf{0.765}}                                 & 0.082                                 & 0.728                                 & 0.786                                 & {\color[HTML]{00B0F0} \textbf{0.776}}                                 \\
WSSA~\cite{zhang2020weakly}                                          & CVPR20                                             & 0.105                                 & 0.660                                 & 0.712                                 & 0.711                                 & 0.148                                 & 0.607                                 & 0.652                                 & 0.649                                 & 0.087                                 & 0.509                                 & 0.733                                 & 0.642                                 & 0.104                                 & 0.688                                 & 0.756                                 & 0.743                                 \\
SCWS~\cite{yu2021structure}                                       & AAAI21                                             & 0.097                                 & 0.684                                 & 0.739                                 & 0.714                                 & 0.142                                 & 0.624                                 & 0.672                                 & {\color[HTML]{00B0F0} \textbf{0.687}}                                 & 0.082                                 & 0.593                                 & 0.777                                 & 0.738                                 & 0.098                                 & 0.695                                 & 0.767                                 & 0.754                                 \\
TEL~\cite{liang2022tree}                                           & CVPR22                                             & 0.094                                 & {\color[HTML]{00B0F0} \textbf{0.712}}                                 & 0.751                                 & {\color[HTML]{00B0F0} \textbf{0.746}}                                 & 0.133                                 & {\color[HTML]{00B0F0} \textbf{0.662}}                                 & 0.674                                 & 0.645                                 & 0.063                                 & 0.623                                 & 0.803                                 & 0.727                                 & 0.085                                 & 0.725                                 & 0.795                                 & 0.766                                 \\
SCOD~\cite{he2022weakly}                                         & AAAI23                                             & 0.092 & 0.688                                 & 0.746                                 & 0.725                                 & 0.137                                 & 0.629                                 & 0.688                                 & 0.663                                 & 0.060                                 & 0.607                                 & 0.802                                 & 0.711                                 & 0.080                                 & 0.744                                 & 0.796                                 & 0.758                                 \\
\rowcolor{c2!20} SCOD+ & \multicolumn{1}{c|}{---}& {\color[HTML]{00B0F0} \textbf{0.089}} & 0.704 & {\color[HTML]{00B0F0} \textbf{0.757}} & 0.731 & 0.129 & 0.642  & {\color[HTML]{00B0F0} \textbf{0.693}}  & 0.666 & {\color[HTML]{00B0F0} \textbf{0.058}} & 0.618 & {\color[HTML]{00B0F0} \textbf{0.812}} & 0.719 & {\color[HTML]{00B0F0} \textbf{0.075}} & {\color[HTML]{00B0F0} \textbf{0.767}} & {\color[HTML]{00B0F0} \textbf{0.825}} & 0.771      \\
\rowcolor{c2!20}Ours & \multicolumn{1}{c|}{---} & {\color[HTML]{FF0000} \textbf{0.056}} & {\color[HTML]{FF0000} \textbf{0.767}} & {\color[HTML]{FF0000} \textbf{0.868}} & {\color[HTML]{FF0000} \textbf{0.805}} & {\color[HTML]{FF0000} \textbf{0.102}} & {\color[HTML]{FF0000} \textbf{0.703}} & {\color[HTML]{FF0000} \textbf{0.757}} & {\color[HTML]{FF0000} \textbf{0.718}} & {\color[HTML]{FF0000} \textbf{0.039}} & {\color[HTML]{FF0000} \textbf{0.698}} & {\color[HTML]{FF0000} \textbf{0.856}} & {\color[HTML]{FF0000} \textbf{0.790}} & {\color[HTML]{FF0000} \textbf{0.057}} & {\color[HTML]{FF0000} \textbf{0.801}} & {\color[HTML]{FF0000} \textbf{0.859}} & {\color[HTML]{FF0000} \textbf{0.813}} \\ 
\bottomrule
\end{tabular}}
\label{table:CODQuanti}
\vspace{-0.5cm}
\end{table}

\begin{table}[t]
\centering
\caption{Results for PIS and TOD with point supervision. }
\resizebox{\columnwidth}{!}{
\setlength{\tabcolsep}{0.8mm}
\begin{tabular}{l|cccc|cccc|cccc|cccc|cccc}
\toprule
& \multicolumn{12}{c|}{Polyp Image Segmentation (PIS)}& \multicolumn{8}{c}{Transparant Object Detection (TOD)}\\ \cline{2-21}
& \multicolumn{4}{c|}{\textit{CVC-ColonDB}}& \multicolumn{4}{c|}{\textit{ETIS}}& \multicolumn{4}{c|}{\textit{Kvasir}}& \multicolumn{4}{c|}{\textit{GDD}}& \multicolumn{4}{c}{\textit{GSD}}\\ \cline{2-21}
\multirow{-3}{*}{Methods} & {\cellcolor{gray!40}$M$~$\downarrow$} &{\cellcolor{gray!40}$F_\beta$~$\uparrow$} &{\cellcolor{gray!40}$E_\phi$~$\uparrow$} & \multicolumn{1}{c|}{\cellcolor{gray!40}$S_\alpha$~$\uparrow$}& {\cellcolor{gray!40}$M$~$\downarrow$} &{\cellcolor{gray!40}$F_\beta$~$\uparrow$} &{\cellcolor{gray!40}$E_\phi$~$\uparrow$} & \multicolumn{1}{c|}{\cellcolor{gray!40}$S_\alpha$~$\uparrow$}& {\cellcolor{gray!40}$M$~$\downarrow$} &{\cellcolor{gray!40}$F_\beta$~$\uparrow$} &{\cellcolor{gray!40}$E_\phi$~$\uparrow$} & \multicolumn{1}{c|}{\cellcolor{gray!40}$S_\alpha$~$\uparrow$}& {\cellcolor{gray!40}$M$~$\downarrow$} &{\cellcolor{gray!40}$F_\beta$~$\uparrow$} &{\cellcolor{gray!40}$E_\phi$~$\uparrow$} & \multicolumn{1}{c|}{\cellcolor{gray!40}$S_\alpha$~$\uparrow$}& {\cellcolor{gray!40}$M$~$\downarrow$} &{\cellcolor{gray!40}$F_\beta$~$\uparrow$} &{\cellcolor{gray!40}$E_\phi$~$\uparrow$} & \multicolumn{1}{c}{\cellcolor{gray!40}$S_\alpha$~$\uparrow$}\\ \midrule
SAM~\cite{kirillov2023segment}& 0.479                                 & 0.343                                 & 0.419                                 & 0.427                                 & 0.429                                 & 0.439                                 & 0.512                                 & 0.503                                 & 0.320                                 & 0.545                                 & 0.564                                 & 0.582                                 & 0.245                                 & 0.512                                 & 0.530                                 & 0.551                                 & 0.266                                 & 0.473                                 & 0.501                                 & 0.514                                 \\
SAM-P~\cite{kirillov2023segment}& 0.194                                 & 0.587                                 & 0.664                                 & 0.671                                 & 0.144                                 & 0.625                                 & 0.719                                 & 0.715                                 & 0.108                                 & 0.793                                 & 0.811                                 & 0.802                                 & 0.164                                 & 0.668                                 & 0.715                                 & 0.625                                 & 0.177                                 & 0.687                                 & 0.730                                 & 0.668                                 \\
WSSA~\cite{zhang2020weakly} & 0.127                                 & 0.645                                 & 0.732                                 & 0.713                                 & 0.123                                 & 0.647                                 & 0.733                                 & 0.762                                 & 0.082                                 & 0.822                                 & 0.852                                 & 0.828                                 & 0.173                                 & 0.652                                 & 0.710                                 & 0.616                                 & 0.185                                 & 0.661                                 & 0.712                                 & 0.650                                 \\
SCWS~\cite{yu2021structure}                      & 0.082                                 & 0.674                                 & 0.758                                 & 0.787                                 & 0.085                                 & 0.646                                 & 0.768                                 & 0.731                                 & 0.078                                 & 0.837                                 & 0.860                                 & 0.831                                 & 0.170                                 & 0.631                                 & 0.702                                 & 0.613                                 & 0.172                                 & 0.706                                 & 0.738                                 & 0.673                                 \\
TEL~\cite{liang2022tree}                       & 0.089                                 & 0.669                                 & 0.743                                 & 0.761                                 & 0.083                                 & 0.639                                 & 0.776                                 & 0.726                                 & 0.091                                 & 0.810                                 & 0.826                                 & 0.804                                 & 0.230                                 & 0.640                                 & 0.586                                 & 0.536                                 & 0.275                                 & 0.571                                 & 0.501                                 & 0.495                                 \\
SCOD~\cite{he2022weakly}                      & 0.077                                 & 0.691                                 & 0.795                                 & 0.802                                 & 0.071                                 & 0.664                                 & 0.802                                 & 0.766                                 & 0.071                                 & 0.853                                 & 0.877                                 & {\color[HTML]{00B0F0} \textbf{0.836}} & 0.146                                 & 0.801                                 & 0.778                                 & 0.723                                 & 0.154                                 & 0.743                                 & 0.751                                 & 0.710                                 \\
\rowcolor{c2!20}SCOD+              & {\color[HTML]{00B0F0} \textbf{0.074}} & {\color[HTML]{00B0F0} \textbf{0.702}} & {\color[HTML]{00B0F0} \textbf{0.806}} & {\color[HTML]{00B0F0} \textbf{0.803}} & {\color[HTML]{00B0F0} \textbf{0.066}} & {\color[HTML]{00B0F0} \textbf{0.670}} & {\color[HTML]{00B0F0} \textbf{0.811}} & {\color[HTML]{00B0F0} \textbf{0.769}} & {\color[HTML]{00B0F0} \textbf{0.068}} & {\color[HTML]{00B0F0} \textbf{0.860}} & {\color[HTML]{00B0F0} \textbf{0.880}} & {\color[HTML]{00B0F0} \textbf{0.836}} & {\color[HTML]{00B0F0} \textbf{0.129}} & {\color[HTML]{00B0F0} \textbf{0.818}} & {\color[HTML]{00B0F0} \textbf{0.796}} & {\color[HTML]{00B0F0} \textbf{0.732}} & {\color[HTML]{00B0F0} \textbf{0.145}} & {\color[HTML]{00B0F0} \textbf{0.761}} & {\color[HTML]{00B0F0} \textbf{0.765}} & {\color[HTML]{00B0F0} \textbf{0.720}} \\
\rowcolor{c2!20}Ours                & {\color[HTML]{FF0000} \textbf{0.043}} & {\color[HTML]{FF0000} \textbf{0.721}} & {\color[HTML]{FF0000} \textbf{0.839}} & {\color[HTML]{FF0000} \textbf{0.816}} & {\color[HTML]{FF0000} \textbf{0.037}} & {\color[HTML]{FF0000} \textbf{0.694}} & {\color[HTML]{FF0000} \textbf{0.849}} & {\color[HTML]{FF0000} \textbf{0.797}} & {\color[HTML]{FF0000} \textbf{0.046}} & {\color[HTML]{FF0000} \textbf{0.878}} & {\color[HTML]{FF0000} \textbf{0.917}} & {\color[HTML]{FF0000} \textbf{0.877}} & {\color[HTML]{FF0000} \textbf{0.078}} & {\color[HTML]{FF0000} \textbf{0.858}} & {\color[HTML]{FF0000} \textbf{0.863}} & {\color[HTML]{FF0000} \textbf{0.775}} & {\color[HTML]{FF0000} \textbf{0.089}} & {\color[HTML]{FF0000} \textbf{0.839}} & {\color[HTML]{FF0000} \textbf{0.841}} & {\color[HTML]{FF0000} \textbf{0.764}} \\\bottomrule
\end{tabular}} \label{table:MISTOD_Quanti}
\vspace{-0.5cm}
\end{table}

\section{Experiments}
\subsection{Experimental Setup}
\noindent \textbf{Implementation details}. 
The image encoder uses ResNet50 as the backbone and is pre-trained on ImageNet~\cite{deng2009imagenet}. The batch size is 36 and the learning rate is initialized as 0.0001, decreased by 0.1 every 80 epochs. 
For scribble supervision, we propose a nine-box strategy, namely constructing the minimum outer wrapping rectangle of the foreground/background scribble and dividing it into a nine-box grid, to sample one point in each box and send them to SAM for segmentation mask generation.
Following~\cite{fan2020camouflaged}, all images are resized as $352\times352$ in both the training and testing phases. 
For SAM~\cite{kirillov2023segment}, we adopt the ViT-H SAM model to generate segmentation masks. 
We implement our method with PyTorch and run experiments on two RTX3090 GPUs. 

\noindent \textbf{Baselines}. 
We explore SAM \cite{kirillov2023segment} for the WSCOS task by generating segmentation masks with sparse annotations as prompts and using the segmentation masks to train a COS segmenter. However, a more straightforward way to explore SAM for this task is to use the sparse annotation to fine-tune SAM and then directly apply SAM for the test. To verify the advantages of our method over this direct way, we construct two baseline methods, SAM-S and SAM-P, which fine-tune the mask decoder of SAM with scribble and point supervisions, respectively, by the partial cross-entropy loss. We will show the results of these two baselines in our comparative evaluations. For reference, we also report the results of the vanilla SAM. When applying SAM and its variants, SAM-S and SAM-P, on test images, we use the automatic prompt generation strategy and report the results with the highest IoU scores. 

\noindent \textbf{Metrics}. Following existing methods~\cite{fan2021concealed,fan2020camouflaged}, we use four common metrics for evaluation, including mean absolute error ($M$), adaptive F-measure ($F_\beta$)~\cite{margolin2014evaluate}, mean E-measure ($E_\phi$)~\cite{fan2021cognitive}, and structure measure ($S_\alpha$)~\cite{fan2017structure}. Smaller $M$, or larger $F_\beta$, $E_\phi$, $S_\alpha$ means better segmentation performance.

\begin{table}[t]
\begin{minipage}[c]{0.443\textwidth}
\centering
\setlength{\abovecaptionskip}{0cm}
\caption{Ablations for WS-SAM.}
\resizebox{\columnwidth}{!}{
\setlength{\tabcolsep}{1mm}
\begin{tabular}{cccc|cccc}
\toprule
Baseline & MAF & PLW & ILS & $M$~$\downarrow$ & $F_\beta$~$\uparrow$ & $E_\phi$~$\uparrow$ & $S_\alpha$~$\uparrow$ \\ \midrule
\checkmark  &  &   &  & 0.052          & 0.674          & 0.838          & 0.737          \\
\checkmark & \checkmark  &   &  & 0.047          & 0.689          & 0.853          & 0.772          \\
\checkmark &\checkmark &\checkmark  & & 0.044          & 0.697          & 0.866          & 0.793          \\
\rowcolor{c2!20}\checkmark & \checkmark&\checkmark  &\checkmark & \textbf{0.038} & \textbf{0.719} & \textbf{0.878} & \textbf{0.803} \\ \bottomrule
\end{tabular}}\label{table:AblationSAM}
\vspace{-0.6cm}
\end{minipage}
\begin{minipage}[c]{0.55\textwidth}
\centering
\setlength{\abovecaptionskip}{0cm}
\caption{Ablations for MFG. }
\resizebox{\columnwidth}{!}{
\setlength{\tabcolsep}{1mm}
\begin{tabular}{c|ccccc}
\toprule
Metrics & w/o MFG & FG-\textgreater{}SA & w/o multiscale & WGM-\textgreater{}FC &\cellcolor{c2!20} w/ MFG         \\ \midrule
$M$~$\downarrow$  & 0.044   & 0.038               & 0.040          & 0.039                &\cellcolor{c2!20} \textbf{0.038} \\
$F_\beta$~$\uparrow$ & 0.684   & 0.708               & 0.702          & 0.710                &\cellcolor{c2!20} \textbf{0.719} \\
$E_\phi$~$\uparrow$ & 0.857   & 0.868               & 0.858          & 0.871                &\cellcolor{c2!20} \textbf{0.878} \\
$S_\alpha$~$\uparrow$  & 0.780   & 0.797               & 0.783          & 0.792                &\cellcolor{c2!20} \textbf{0.803} \\ \bottomrule
\end{tabular}}\label{table:AblationMFG}
\vspace{-0.6cm}
\end{minipage}
\end{table}

\begin{table}[t]
\begin{minipage}[c]{0.54\textwidth}
\centering
\setlength{\abovecaptionskip}{0cm}
\caption{Results of MFG with full supervision.}
\resizebox{\columnwidth}{!}{
\setlength{\tabcolsep}{1mm}
\begin{tabular}{c|ccccc}
\toprule
Metrics & Baseline & SegMaR~\cite{jia2022segment} & PreyNet~\cite{zhang2022preynet} & FGANet~\cite{zhaiexploring} & \cellcolor{c2!20}Ours   \\ \midrule
$M$~$\downarrow$  & 0.035    & 0.035 & 0.034   & \textbf{0.032}  & \cellcolor{c2!20}\textbf{0.032} \\
$F_\beta$~$\uparrow$ & 0.688    & 0.699  & \textbf{0.715}   & 0.708  & \cellcolor{c2!20}0.706 \\
$E_\phi$~$\uparrow$ & 0.879    & 0.890   & 0.894   & 0.894  & \cellcolor{c2!20}\textbf{0.897} \\
$S_\alpha$~$\uparrow$     & 0.812    & \textbf{0.813}  & \textbf{0.813}   & 0.803  & \cellcolor{c2!20}\textbf{0.813} \\ \bottomrule
\end{tabular}}\label{table:FullSupervision}
\vspace{-0.6cm}
\end{minipage}
\begin{minipage}[c]{0.42\textwidth}
\centering
\setlength{\abovecaptionskip}{0cm}
\caption{Results on multi-object images.
}
\resizebox{\columnwidth}{!}{
\setlength{\tabcolsep}{1mm}
\begin{tabular}{c|cccc}
\toprule
Metrics & SCWS~\cite{yu2021structure}  & TEL~\cite{liang2022tree}   & SCOD~\cite{he2022weakly}  & \cellcolor{c2!20}Ours           \\ \midrule
$M$~$\downarrow$    & 0.094 & 0.101 & 0.084 & \cellcolor{c2!20}\textbf{0.070} \\
$F_\beta$~$\uparrow$ & 0.378 & 0.350 & 0.381 & \cellcolor{c2!20}\textbf{0.452} \\
$E_\phi$~$\uparrow$ & 0.740 & 0.726 & 0.718 & \cellcolor{c2!20}\textbf{0.772} \\
$S_\alpha$~$\uparrow$     & 0.625 & 0.617 & 0.643 & \cellcolor{c2!20}\textbf{0.687} \\ \bottomrule
\end{tabular}}\label{table:Multi-object}
\vspace{-0.6cm}
\end{minipage}
\end{table}

\subsection{Comparative Evaluation}
We perform evaluations on the following COS tasks, namely,  Camouflaged Object Detection, Polyp Image Segmentation (PIS), and Transparent Object Detection (TOD). For all the tasks, we evaluate the performance with point annotations. We follow the previous weakly-supervised segmentation method \cite{gao2022weakly} and randomly select two points (one from the foreground and one from the background) from the training masks as the point annotations. For COD, we additionally evaluate the performance using scribble annotations, using the scribble data provided in \cite{gao2022weakly}.       

\noindent\textbf{Camouflaged object detection}.
Four datasets are used for experiments, \textit{i.e.}, \textit{CHAMELEON}~\cite{skurowski2018animal}, \textit{CAMO}~\cite{le2019anabranch}, \textit{COD10K}~\cite{fan2021concealed}, and \textit{NC4K}~\cite{lv2021simultaneously}. Table~\ref{table:CODQuanti} shows that our method reaches the best performance over all competing methods and baselines. Notably, while SAM has shown impressive performance for natural scene images, its performance on the challenging COD task is far from the existing methods particularly designed for this task. We do see performance gains after finetuning SAM with point (SAM-P) and scribble (SAM-S) supervision, but the results are still far below our method. This substantiates the superiority of our way of leveraging SAM to generate segmentation masks with sparse annotations and use the segmentation masks to train the segmenter. To verify our performance improvement over the existing WSCOS methods does not merely come from the usage of SAM, we integrate the most recent WSCOS method, SCOD~\cite{he2022weakly}, into our WS-SAM framework to also leverage the additional mask supervision. This results in the method, ``SCOD+''. We can see that our method still shows better performance, further verifying our advantages for this task. 

\noindent\textbf{Polyp image segmentation}.
Three widely-used Polyp datasets are selected, namely \textit{CVC-ColonDB}~\cite{tajbakhsh2015automated}, \textit{ETIS}~\cite{silva2014toward}, and \textit{Kvasir}~\cite{jha2020kvasir}. Table~\ref{table:MISTOD_Quanti} shows that our method significantly  surpasses the second-best method, SCOD, with point supervision. SAM and SAM-P do not perform well on this task, further substantiating their weakness in this challenging segmentation task. 
While empowering SCOD with the proposed WS-SAM framework indeed improves the performance, the results are still lower than our method. This again verifies our benefit in handling challenging segmentation tasks.  

\noindent\textbf{Transparent object detection}.
Two datasets, GDD~\cite{mei2020don} and GSD~\cite{lin2021rich}, are used for evaluation. As shown in Table~\ref{table:MISTOD_Quanti}, our method surpasses all baseline methods and existing methods for this task as well. This shows the strong robustness and generalizability of our proposed method. 

\subsection{Ablation Study} \vspace{-1mm}
Our method includes two main components, the SAM-based weakly-supervised mask generation framework, WS-SAM, and the multi-scale feature grouping (MFG) module. We conduct ablation studies about these two components on \textit{COD10K} of the COD task with scribble supervision. 

\noindent \textbf{Ablation study for WS-SAM}. 
We establish a baseline by using SAM to generate only one segmentation mask from one training image without augmentations for model training. On top of this baseline, we add multi-augmentation fusion (MAF),  pixel-level weighting (PLW), and image-level selection (ILS) techniques. Table~\ref{table:AblationSAM} showing adding these components helps improve the performance, thus demonstrating their effectiveness.  

\noindent \textbf{Ablation study for MFG}.
We examine the effect of MFG by completely removing the MFG module, substituting the proposed feature grouping (FG) with slot attention (SA) \cite{locatello2020object}, removing the multi-scale strategy, and substituting the weighted gate mechanism (WGM) with a fixed coefficient (FC). Table~\ref{table:AblationMFG} shows that our designs reach better performance than the alternative ones. 

\subsection{Further Analysis}\vspace{-1mm}

\noindent\textbf{MFG for the \emph{fully-supervised} setting}.
The proposed MFG module is designed to evacuate discriminative cues from concealed images. We have demonstrated its effectiveness with sparse annotations for the weakly-supervised setting. However, it is expected to also work in the fully-supervised setting. To verify this,  we conduct experiments for the standard fully-supervised COD task. Table~\ref{table:FullSupervision} shows the results on the \textit{COD10K} dataset. We can see that MFG indeed helps improve the performance of the baseline model, to the level comparative with state-of-the-art methods.  

 
\noindent \textbf{Performance on multi-object images}. 
The proposed MFG module evacuates discriminative cues by performing feature grouping at different granularities, which facilitates discovering multiple objects in images. To verify this, we evaluate the performance on the 186 images with more than one object from $COD10K$. Table~\ref{table:Multi-object} shows that MFG achieves the best performance, surpassing the second-best method (SCOD) by $10.7\%$. This gap is large than that with all test images, where the gap is $5.8\%$. 

\begin{table}[t]
\centering
\caption{Parameter analysis on $K$, $\tau_a$, $\tau_r$, $T$, and $(N_1,N_2)$.}
\resizebox{1\columnwidth}{!}{
\setlength{\tabcolsep}{0.8mm}
\begin{tabular}{c|cccc|cccc|cccc|cccc|cccc}
\toprule
\multicolumn{1}{l|}{\multirow{2}{*}{Metrics}} & \multicolumn{4}{c|}{$K$}  & \multicolumn{4}{c|}{$\tau_a$} & \multicolumn{4}{c|}{$\tau_r$}  & \multicolumn{4}{c|}{$T$} & \multicolumn{4}{c}{$(N_1,N_2)$} \\\cline{2-21}
\multicolumn{1}{l|}{}  & 1     & 6     & \cellcolor{c2!20}12             & 18             & 0.05           & \cellcolor{c2!20}0.1            & 0.2            & 0.3   & 0.3            & \cellcolor{c2!20}0.5            & 0.7   & 0.9   & 1     & 2     & \cellcolor{c2!20}3              & 4              & \cellcolor{c2!20}(2,4)          & (2,8)          & (4,8) & (2,4,8)        \\ \midrule
$M$~$\downarrow$ & 0.052 & 0.042 & \cellcolor{c2!20}\textbf{0.038} & 0.039          & \textbf{0.037} & \cellcolor{c2!20}0.038          & 0.038          & 0.040 & \textbf{0.038} & \cellcolor{c2!20}\textbf{0.038} & 0.039 & 0.040 & 0.039 & 0.039 & \cellcolor{c2!20}\textbf{0.038} & \textbf{0.038} & \cellcolor{c2!20}\textbf{0.038} & \textbf{0.038} & 0.039 & \textbf{0.038} \\
$F_\beta$~$\uparrow$ & 0.674 & 0.697 & \cellcolor{c2!20}\textbf{0.719} & 0.718          & 0.706          & \cellcolor{c2!20}\textbf{0.719} & 0.716          & 0.704 & \textbf{0.723} & \cellcolor{c2!20}0.719          & 0.715 & 0.700 & 0.706 & 0.715 & \cellcolor{c2!20}0.719          & \textbf{0.720} & \cellcolor{c2!20}0.719          & 0.714          & 0.711 & \textbf{0.721} \\
$E_\phi$~$\uparrow$ & 0.838 & 0.857 & \cellcolor{c2!20}\textbf{0.878} & \textbf{0.878} & 0.868          & \cellcolor{c2!20}\textbf{0.878} & 0.876          & 0.865 & 0.866          & \cellcolor{c2!20}\textbf{0.878} & 0.874 & 0.851 & 0.862 & 0.872 & \cellcolor{c2!20}\textbf{0.878} & 0.876          & \cellcolor{c2!20}\textbf{0.878} & 0.875          & 0.873 & \textbf{0.878} \\
$S_\alpha$~$\uparrow$ & 0.737 & 0.776 & \cellcolor{c2!20}0.803          & 0.800          & 0.795          & \cellcolor{c2!20}0.803          & \textbf{0.805} & 0.793 & 0.792          & \cellcolor{c2!20}\textbf{0.803} & 0.789 & 0.781 & 0.794 & 0.800 & \cellcolor{c2!20}0.803          & \textbf{0.805} & \cellcolor{c2!20}\textbf{0.803} & 0.802          & 0.799 & 0.802        \\ \bottomrule                        
\end{tabular}}\label{table:ParameterAnalysis}
\vspace{-0.3cm}
\end{table}

\begin{figure}[t]
	\centering
	\setlength{\abovecaptionskip}{-0.2cm}
	\begin{center}
		\includegraphics[width=\linewidth]{Figure/COSQuali.pdf}
	\end{center}
	\caption{Visualized results for the three WSCOS tasks.}
	\label{fig:COSQuali}
	\vspace{-0.5cm}
\end{figure}

\noindent \textbf{Randomness of point supervision}. 
We follow the existing point-supervision
segmentation methods and randomly select points from ground truth masks as the point annotation.
\begin{wrapfigure}[8]{r}{0.3\textwidth}%{3.3cm}
\vspace{-5mm}
\centering 
	\setlength{\abovecaptionskip}{-0.2cm}
	\begin{center}
		\includegraphics[width=1\linewidth]{Figure/PointSupervision.pdf}
	\end{center}
	\caption{ Five runs results with varied point annotations. }
	\label{fig:PointSupervision}
\end{wrapfigure} To study the variance of the random selection, we repeat the random selection 5 times and calculate the mean and standard deviation of the results. Fig.~\ref{fig:PointSupervision} shows that our method reaches the best results while having the smallest deviation.  

\noindent\textbf{Number of augmented views $K$}.
Table~\ref{table:ParameterAnalysis} shows that more augmented views help improve performance in the beginning, but the effect turns weaker when further increasing it. 

\noindent\textbf{Hyperparameters in image-level selection}.
Table~\ref{table:ParameterAnalysis} shows that it is best to set the absolute uncertainty threshold $\tau_{a}=0.1$ and the relative uncertainty threshold $\tau_{r}=0.5$, and the proposed method is not sensitive to these two parameters.   

\noindent\textbf{Hyperparameters in MFG}. 
Table~\ref{table:ParameterAnalysis} shows that MFG achieves the best  results when the iteration number $T$ is set as 3, and the groups and scales setting is set as $(N_1, N_2)=(2,4)$. Notice that when adopting $(2,4,8)$, RK2 is replaced with the third-order RK structure, resulting in extra computational burden with limited benefits. Hence, we select the RK2 structure with $(N_1, N_2)=(2,4)$.

\noindent \textbf{Result visualization.}
Fig.~\ref{fig:COSQuali} shows the prediction maps with point supervision. We can see that our method produces more complete results than existing methods and localizes multiple objects more comprehensively. More visualization results can be found in the supplementary materials.  

\vspace{-2mm}
\section{Conclusions} \vspace{-1mm}
This paper proposes a new WSCOS method that includes two key components. The first one is the WS-SAM framework that generates segmentation masks with the recently proposed vision foundation model, SAM, and proposes multi-augmentation result fusion, pixel-level uncertainty weighting, and image-level uncertainty filtration to get reliable pseudo labels to train a segmentation model. The second is the MFG module that leverages the extracted clues for additional nuanced discrimination information. MFG improves feature coherence from a grouping aspect, allowing for alleviating incomplete segmentation and better multiple-object segmentation. Experiments on multiple WSCOS tasks confirm the superiority of our method over the baseline and existing methods.  

\clearpage
{\small
}

\end{document}