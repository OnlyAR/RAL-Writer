\title{\ptpLogoWithText~: Multilingual Evaluation of \\ Neural Toxic Degeneration in Large Language Models}

\begin{document}

\maketitle

\begin{abstract}
Recent advances in large language models (LLMs) have led to their extensive global deployment, and ensuring their safety calls for comprehensive and multilingual toxicity evaluations. However, existing toxicity benchmarks are overwhelmingly focused on English, posing serious risks to deploying LLMs in other languages.
We address this by introducing \datasetName (\datasetAbbrev), the first large-scale multilingual toxicity evaluation benchmark of 425K naturally occurring prompts spanning 17 languages.
We overcome the scarcity of naturally occurring toxicity in web-text and ensure coverage across languages with varying resources by automatically scraping over 100M web-text documents.
Using \datasetAbbrev, we investigate research questions to study the impact of model size, prompt language, and instruction and preference-tuning methods on toxicity by benchmarking over 60 LLMs. Notably, we find that toxicity increases as language resources decrease or model size increases. Although instruction- and preference-tuning reduce toxicity, the choice of preference-tuning method does not have any significant impact.
Our findings shed light on crucial shortcomings of LLM safeguarding and highlight areas for future research.

\begin{center}
\begin{tabular}{rp{2cm}c}
    \github & \textbf{Code} & \href{https://github.com/kpriyanshu256/polyglo-toxicity-prompts}{kpriyanshu256/polyglo-toxicity-prompts} \\
    \huggingface & \textbf{Dataset} & \href{https://hf.co/datasets/ToxicityPrompts/PolygloToxicityPrompts}{ToxicityPrompts/PolygloToxicityPrompts} \\
    \huggingface & \textbf{Leaderboard} & \href{https://hf.co/spaces/ToxicityPrompts/PTP}{ToxicityPrompts/PTP} \\
\end{tabular}
\end{center}

\end{abstract}

\section{Introduction}

Large language models (LLMs) are increasingly being deployed in global contexts \citep{google-gemini-announcement, forbes-llm-uses}. Naturally, this has led to rapid advances in the multilingual capabilities of LLMs \citep{Scao2022BLOOMA1, ustun2024aya, yuan2023multilingual}. However, current toxicity evaluation benchmarks and safety alignment methods \citep{christiano2017deep, lee2024rlaif} overwhelmingly focus on the English language, leading to significantly less safe responses in non-English languages \citep{wang2023all, kotha2024understanding, Yong2023LowResourceLJ}. The lack of a standard multilingual benchmark for evaluating toxicity poses significant challenges to non-English users and the development of safer multilingual models.

We introduce \datasetName (\datasetAbbrev), the first large-scale multilingual benchmark for evaluating \textit{neural toxic degeneration}, defined as the propensity of LLMs to generate toxic text given a prompt \citep{gehman-etal-2020-realtoxicityprompts}. We create \datasetAbbrev by scraping over 100M documents from web-text corpora to collect naturally occurring toxic prompts. This results in 425K prompts in 17 languages ranging from non-toxic to highly-toxic prompts scored with \perspectiveAPI.\footnote{\url{https://perspectiveapi.com/}\label{perspective-url}} 

\datasetName provides three key improvements for multilingual toxicity evaluation, surfacing more toxic generations from LLMs than existing toxicity benchmarks (Figure \ref{fig:motivation_results}).  
\textit{First}, \datasetAbbrev covers 17 languages while existing toxic degeneration work predominantly focuses on English \citep{gehman-etal-2020-realtoxicityprompts, lin-etal-2023-toxicchat}. 
\textit{Second}, existing multilingual toxicity evaluation testbeds such as \citet{ustun2024aya} and \textsc{RTP-LX} \citep{dewynter2024rtplx} are translations of \textsc{RealToxicityPrompts} (\textsc{RTP}; \citealp{gehman-etal-2020-realtoxicityprompts}), which can lack cultural nuances of toxicity and introduce deviations in toxicity, leading to under-estimated toxic degeneration \citep{sharou-specia-2022-taxonomy, costa-jussa-etal-2023-toxicity}.
\textit{Third}, \datasetAbbrev's naturally occurring prompts are more representative of real-world inputs than recent works on \textit{jailbreaking} \citep{Deng2023MASTERKEYAJ, wei2024jailbroken} and adversarial prompt generation \citep{zou2023universal, huang2023catastrophic}, which lead to unnatural and often gibberish prompts.

\begin{wrapfigure}[20]{l}{5.5cm}
    \centering
    \vspace{-10pt}
\includegraphics[width=0.4\textwidth]{images-new/PTP_Motivation.pdf}
    \vspace{-10pt}
    \caption{GPT-3.5-Turbo's \textsc{Average Toxicity} score on existing toxicity evaluation datasets, showing that \datasetAbbrev uncovers more toxicity in LLMs.}
    \label{fig:motivation_results}
\end{wrapfigure}

We evaluate 62 LLMs on \textsc{PolygloToxicityPrompts} to study the impact of prompt language, model size, alignment methods, and input prompt toxicity on toxicity. 
We find significant toxicity in multilingual models, especially as the availability of language resources decreases. We observe that toxicity increases with model size within a model family for base LLMs. Furthermore, while instruction and preference-tuning reduce toxicity in models, the choice of preference-tuning method does not impact toxicity. Finally, we find that (un)safety and toxicity are related, but distinct aspects of LLMs that require their own solutions.
Overall, our findings shed light on crucial shortcomings of LLM safeguarding and highlight areas for future research, notably, the need for multilingual toxicity mitigation and further investigations into the impact of model hyperparameters on toxicity. Our evaluation benchmark will advance efforts toward combating the critical issue of neural toxic degeneration.
\section{Related Work}
\label{related_work}

\paragraph{Evaluating Toxicity using Web-text Corpora, Templates, And User-AI Interaction Data}
Early works on evaluation datasets for studying biases and toxicity in models were created using templates or scraping web-text corpora. \citet{sheng-etal-2019-woman, nangia-etal-2020-crows, nadeem-etal-2021-stereoset} use templated prompts to study social biases in pretrained language models. However, templates are focused on specific contexts such as demographic identities and not necessarily realistic. Thus, \citet{gehman-etal-2020-realtoxicityprompts} create \textsc{RealToxicityPrompts} by crawling English web-text for naturally occurring input prompts to evaluate toxicity in a sentence completion setting. 

More recently, there has been a shift towards examining toxicity in input-response settings. \citet{10.1145/3548606.3560599, baheti-etal-2021-just} use generations from dialogue models like DialoGPT \citep{zhang2019dialogpt} to study toxic degenerations in chatbots. Furthermore, the advent of instruction-tuned LLMs has led to studies of toxicity in real-world user-AI conversations. \citet{zheng2024realchatm} and \citet{lin-etal-2023-toxicchat} collect user-AI interactions with automatic and manual toxicity annotations respectively to tackle a different toxic data distribution---namely instructions. However, most of these approaches are limited to English.

\paragraph{Evaluating Multilingual Toxicity}
Multilingual dataset curation for evaluating toxicity has utilized both manual and automated translation techniques. Recent work on AI safety evaluation \citep{wang2023all, Yong2023LowResourceLJ, Deng2023MASTERKEYAJ} create multilingual safety benchmarks by translating monolingual benchmarks into other languages. They observe that LLMs are primarily safeguarded for English, leading to significantly unsafe generations in other languages, especially as availability of languages decreases. While these works are aimed towards the broader area of safety, the absence of a standard multilingual toxicity evaluation benchmark has also led researchers to translate prompts from \textsc{RealToxicityPrompts} into other languages, either automatically \citep{ustun2024aya} or using human annotations \citep{dewynter2024rtplx}. However, manual translations are expensive, not scalable, and can introduce cultural biases, whereas automated translations can introduce deviations in toxicity due to incorrect translations and hallucinations \citep{specia-etal-2021-findings, sharou-specia-2022-taxonomy, nllbteam2022language, costa-jussa-etal-2023-toxicity}. 

\paragraph{Evaluating Toxicity using Machine-Generated Approaches} 
Besides human-generated or naturally occurring data, a wealth of recent work has explored using machine-generated approaches to curate datasets and methods for evaluating the toxicity and safety of LLMs. \cite{hartvigsen-etal-2022-toxigen} and \cite{kim-etal-2022-prosocialdialog} generate adversarial prompts about minority groups using classifier-guided decoding and conversations with a toxic partner respectively. Extensive research has studied \textit{red teaming} \citep{perez-etal-2022-red, chao2023jailbreaking, mazeika2024harmbench} and \textit{jailbreaking} \citep{liu2023autodan, wei2024jailbroken, yu2023gptfuzzer, Deng2023MASTERKEYAJ} to identify safety failures in LLMs and elicit harmful outputs. Furthermore, adversarial attack methods have also been shown to be effective against models without requiring substantial prompt engineering \citep{shin-etal-2020-autoprompt, zou2023universal, huang2023catastrophic, pmlr-v202-jones23a}. However, such methods involve extensive prompt engineering, often leading to unnatural and non-representative prompts or model-specific artifacts \citep{das2024under}. Furthermore, the extent to which these methods work in non-English languages remains to be studied.

While the literature on toxicity evaluation has grown rapidly, their predominant focus on English highlights the need for multilingual benchmarks on \textit{naturally} occurring toxic input prompts. We address this gap with \datasetName, a collection of 425K naturally occurring prompts across 17 languages for evaluating toxicity.
\section{PolygloToxicityPrompts}
\label{sec: ptp}

We create \textsc{PolygloToxicityPrompts}, a large-scale multilingual testbed to evaluate toxic degeneration in LLMs. It consists of 425K prompts extracted from web-text corpora paired with toxicity scores from \perspectiveAPI. All 17 languages supported by \perspectiveAPI are represented in our testbed, namely: Arabic (ar), Chinese (zh), Czech (cs), Dutch (nl), English (en), French (fr), German (de), Hindi (hi), Indonesian (id), Italian (it), Japanese (ja), Korean (ko), Polish (pl), Portuguese (pt), Russian (ru), Spanish (es), and Swedish (sv).

\subsection{\textbf{Operationalizing and Evaluating Toxicity}} 
We define toxicity as ``a rude, disrespectful, or unreasonable comment that is likely to make people leave a discussion'' \citep{10.1145/3038912.3052591, 10.1145/3308560.3317593}. We use \perspectiveAPI,\footref{perspective-url} an industry-standard toxicity detection tool because it supports our 17 languages. 
Specifically, we use the \textsc{Toxicity} score from \perspectiveAPI, computed using the UTC (\textit{Unified Toxic Content Classification}) framework \citep{10.1145/3534678.3539147}, composed of a Charformer-based transformer \citep{tay2022charformer}. UTC is a Seq2Seq architecture pretrained with the mC4 corpus \citep{xue-etal-2021-mt5} and Perspective Pretraining Corpus (PPC). Additionally, \perspectiveAPI utilizes a single-language CNN \citep{726791} distilled from multilingual BERT models \citep{devlin-etal-2019-bert} for German and Portuguese.

\subsection{\textbf{Dataset Creation}}
We construct our dataset by scraping over 100M documents from the mC4 \citep{xue-etal-2021-mt5} and \thepile \citep{gao2020pile} corpora as they contain multilingual texts from a variety of domains. We also leverage Pile Curse,\footnote{\url{https://huggingface.co/datasets/tomekkorbak/pile-curse-full}} a subset of \thepile scored using the  \textit{bad words} \footnote{\url{https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words}\label{ldnoobw}} list for our English split. We then extract \textsc{Toxicity} scores with \perspectiveAPI for all scraped documents. To obtain a stratified range of prompt toxicity, we sample 6250 documents from 4 equal-width toxicity levels ($[0, 0.25), \dots, [0.75, 1]$). We then split collected documents in half to form \textit{prompts} and \textit{continuations}, both of which are scored for toxicity. We provide preprocessing details, dataset statistics, and metadata analysis in Appendix \ref{sec:dataset-analysis}.

The final dataset includes 25K naturally occurring prompts for each language, for a total of 425K prompts across 17 languages. Figures \ref{fig:ds_tox} and \ref{fig:ds_length} show the prompt toxicity and length distributions of our prompts for all languages. We create our prompts using documents instead of sentences \citep{gehman-etal-2020-realtoxicityprompts}. Thus, our prompts are much longer than \textsc{RealToxicityPrompts}, with an average length of approximately 400 GPT-4 tokens (\texttt{cl100k\_base} tokenizer).

\paragraph{Challenges in Finding Multilingual Toxic Prompts}
While the extraction of toxic content from web-text may appear straightforward, we encountered several challenges associated with the scarcity of multilingual toxicity. The mC4 corpus \citep{xue-etal-2021-mt5} filters toxicity by removing pages containing \textit{bad words}.\footref{ldnoobw} As a result, we observe less than $0.01\%$ toxicity rate out of 5M samples for \textit{ar, cs, fr, ko, id, it, nl, pl,} and \textit{sv}. However, consistent with previous findings \citep{zhou-etal-2021-challenges, dodge-etal-2021-documenting}, we note that filtered datasets still exhibit toxicity, and observe higher toxicity rates for other languages.

To attain a larger sample of toxic content for languages with low toxicity rates, we create synthetic high-toxicity data. Specifically, we translate toxic samples from the mC4 and \thepile corpora into target languages using the NLLB-3.3B model \citep{nllbteam2022language}. We use this process to create $\approx$ 70K translated prompts across 9 languages, which amounts to only $16.8\%$ of our dataset. Contrary to prior works, we observe a Pearson correlation of 0.725 ($p \leq 0.001$) between the toxicity scores of the original and translated samples across all languages, suggesting that low amounts of translated data are not necessarily an issue.\footnote{We discuss limitations with translating data in the \nameref{sec: ethics-statement}.}

\paragraph{\datasetSmall} We also create \datasetSmall, a stratified sample of 5K prompts per language from \datasetName to benchmark models with limited computational resources.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=\textwidth]{images-new/Contributions.pdf}
    \caption{Summary of \datasetName.}
    \label{fig:contributions}
\end{figure}

\subsection{\textbf{Benchmarking Large Language Models}}

We benchmark a large variety of models ($N = 62$) spanning different sizes and multilingual capabilities. We follow the taxonomy proposed by \citet{albalak2024survey} and include LLMs trained purely with the language modeling objective (\baseModel) such as Llama2 \citep{touvron2023llama2}, Pythia \citep{biderman2023pythia}, LLMs \textit{fine-tuned} to follow instructions (\instructModel) such as Mistral-Instruct \citep{jiang2023mistral}, and LLMs aligned with preference-tuning/alignment methods (\prefModel) such as GPT-3.5-Turbo \citep{ouyang2022training} and Zephyr \citep{tunstall2023zephyr}. In the subsequent section (Section \ref{sec: rqs}), we explore a variety of research questions that require specific functionalities and thus use the appropriate subset of models for our analyses. We also note that the LLMs we benchmark are, to the best of our knowledge, the neural networks that are trained and possibly instruction and/or preference-tuned, without any possible safeguards or guardrails that may have been added onto the public interfaces of such LLMs, such as safety classifiers applied to the input/output of LLMs.

\subsection{\textbf{Benchmarking Setup}}
\paragraph{Evaluation Metrics}
Given a toxicity evaluator $t(\cdot)$ and a language model $f(\cdot)$, $t(\cdot)$ predicts the toxicity score for a text, and $f(\cdot)$ provides a completion $c = f(p)$ for a prompt $p$. We sample $K$ completions per prompt and follow \citet{gehman-etal-2020-realtoxicityprompts, ustun2024aya} to measure a model's toxicity over a set of prompts $\mathcal{P}$. Specifically, we compute \expMaxToxFull, that is, \textit{what is the expected toxicity of a model's worst-case generations?}, as 
\expMaxTox $= \frac{1}{|\mathcal{P}|} \sum_{p \in \mathcal{P}} \max_i t(f(p))$ and the \empProbFull (\empProb), that is, \textit{how frequently does a model generate toxicity}, as the probability that a model generates at least one completion with toxicity score $\geq 0.5$ in $K$ completions. We additionally compute \avgToxFull, that is, \textit{what is the model's overall toxicity?}, as \avgTox $ = \frac{1}{|\mathcal{P}|} \sum_{p \in \mathcal{P}} \frac{1}{K} \sum_{i=1}^K t(f(p))$.

\paragraph{Implementation Details} We utilize \datasetSmall to benchmark LLMs due to the breadth of considered models and computational constraints. We use the \textsc{Toxicity} score from \perspectiveAPI as our toxicity evaluator $t(\cdot)$, $K = 10$ completions, temperature $= 0.7$, top\_p $= 1$, and a maximum generation length of $512$ tokens for our experiments. We use Microsoft Azure's OpenAI API for GPT-3.5-Turbo (version 0301) with safety settings disabled, vLLM \citep{kwon2023efficient} for decoder-only models, and Huggingface's TGI\footnote{\url{https://github.com/huggingface/text-generation-inference}} for encoder-decoder models. We only use the required prompt templates as stated in model cards, and do not provide any additional instructions. 

\section{Research Questions}
\label{sec: rqs}

To investigate multilingual toxic degeneration in a large suite of models, we obtain and score continuations for the 5K prompts per language contained in \datasetSmall (due to computational resource limitations). We find similar trends across all evaluation metrics and thus report only \avgToxFull for brevity.

\begin{wraptable}[8]{r}{5cm}
    \vspace{-22pt}
    \resizebox{4.5cm}{!}{%
    \begin{tabular}{p{4.5cm}|c}
        \toprule
        \textbf{Model} & \textbf{\avgTox} \\
        \midrule
         Llama-2-13b-chat-hf & \cellcolor[HTML]{9FC5E8}0.078 \\
         Llama-2-70b-chat-hf & \cellcolor[HTML]{9FC5E8}0.088 \\
         Qwen-7B-Chat & \cellcolor[HTML]{9FC5E8}0.091 \\
         \midrule
         OpenHathi-7B-Hi-v0.1-Base &	\cellcolor[HTML]{E49D9F}0.327 \\
         pythia-12b & \cellcolor[HTML]{E49D9F}0.327 \\
         pythia-6.9b & \cellcolor[HTML]{E49D9F}0.328 \\
         \bottomrule
    \end{tabular}
    }
    \caption{Models with highest and lowest \avgTox on \datasetSmall.}
    \label{tab:top_3_best_worst}
\end{wraptable}

Table \ref{tab:top_3_best_worst} previews our findings for the models with the lowest and highest \avgToxFull. We provide results for all models with languages categorized based on \cite{joshi-etal-2020-state}\footnote{Since all considered languages belong to categories 3 and above, we compare relative resource availability, that is, categories 3, 4 and 5 are referred as low-, medium- and high-resource respectively.} in Table \ref{tab:secondary_results}.
Next, we explore specific patterns concerning prompt language, model size, alignment methods, and prompt toxicity below. Finally, we also compare \textit{toxicity} and \textit{safety} detectors using \perspectiveAPI and Llama Guard \cite{inan2023llama} respectively.

\subsection{\textbf{How does \textit{Prompt Language} impact \textsc{Average Toxicity}?}}

Despite safety alignment, translations of harmful prompts from English to other languages can elicit harmful content from LLMs \citep{kotha2024understanding, Yong2023LowResourceLJ, deng2024multilingual}. Therefore, we study how toxicity varies with input prompt languages by benchmarking multilingual LLMs, namely GPT-3.5-Turbo \citep{ouyang2022training}, Aya101 \citep{ustun2024aya}, and Bloomz \citep{muennighoff-etal-2023-crosslingual} and evaluating \avgTox for each language.

\begin{figure*}[htpb]
    \begin{center}
    \includegraphics[width=\textwidth]{images-new/lang_vs_toxicity-scaled.pdf}
    \caption{Language-wise \avgTox trends for multilingual models. \textbf{\textit{Takeaway}}: High toxicity scores (relative to the \avgTox levels shown in Figure \ref{fig:motivation_results} and Table \ref{tab:top_3_best_worst}) for all languages indicate the need for multilingual toxicity mitigation methods.
    }
    \label{fig:rq1}
    \end{center}
\end{figure*}

Figure \ref{fig:rq1} shows that models have the lowest \avgTox levels in \textit{ru} (Russian) and \textit{nl} (Dutch), consistent with \citet{ustun2024aya}. 
However, all models have highly toxic continuations in \textit{hi} (Hindi) and \textit{cs} (Czech). 
We hypothesize that the relatively small amounts of Hindi in most pretraining corpora and lack of safety alignment in Hindi leads to more toxic degenerations \citep{wang2023all, Yong2023LowResourceLJ, deng2024multilingual}. 
This hypothesis is corroborated by the fact that \avgTox reduces as the availability of language resources increases (Table \ref{tab:benchmark-lang-res}).

Across models, we find that GPT-3.5-Turbo and bloomz-560m have the highest and lowest \avgTox levels aggregated across all languages respectively. However, we hypothesize that the lower toxicity scores of bloomz models, especially bloomz-560m, might be due to short and poor quality completions from these models (average character length of generations for bloomz-560m, Aya101, and GPT-3.5-Turbo are $96.21$, $208.54$, and $524.21$ respectively). 

Overall, high toxicity scores in non-English languages provide strong evidence of a current gap in multilingual toxicity mitigation, even in highly capable models. Furthermore, the high toxicity scores for English also indicate the shortcomings of current safeguarding methods, likely caught by longer prompts in \datasetAbbrev.

\begin{minipage}{\textwidth}
    \begin{minipage}[b]{0.55\textwidth}
    \centering
\resizebox{0.8\textwidth}{!}{%
\begin{tabular}{c|c|c|c}
\toprule
\textbf{Language Resource} & \textbf{Model} & \textbf{\avgTox} & \textbf{\empProb} \\ \midrule
 & bloomz-560m & \cellcolor[HTML]{9FC5E8}$0.142_{0.16}$ & \cellcolor[HTML]{ACBDDA}0.272 \\
 & bloomz-1b1 & \cellcolor[HTML]{BEB3C7}$0.176_{0.18}$ & \cellcolor[HTML]{CBABB9}0.345 \\
 & bloomz-3b & \cellcolor[HTML]{BCB4CA}$0.173_{0.19}$ & \cellcolor[HTML]{C5AFBF}0.331 \\
 & bloomz-7b1 & \cellcolor[HTML]{C4AFC1}$0.182_{0.2}$ & \cellcolor[HTML]{CAACBB}0.342 \\
 & Aya101 & \cellcolor[HTML]{C1B1C4}$0.179_{0.19}$ & \cellcolor[HTML]{C9ACBB}0.340 \\
\multirow{-6}{*}{\textbf{High}} & GPT-3.5-Turbo & \cellcolor[HTML]{D2A7B2}$0.197_{0.21}$ & \cellcolor[HTML]{A9BFDD}0.264 \\ \midrule
 & bloomz-560m & \cellcolor[HTML]{ADBDDA}$0.157_{0.17}$ & \cellcolor[HTML]{9FC5E8}0.239 \\
 & bloomz-1b1 & \cellcolor[HTML]{B7B7CF}$0.168_{0.17}$ & \cellcolor[HTML]{B2BAD4}0.285 \\
 & bloomz-3b & \cellcolor[HTML]{B3B9D3}$0.164_{0.18}$ & \cellcolor[HTML]{ABBEDC}0.268 \\
 & bloomz-7b1 & \cellcolor[HTML]{B8B7CE}$0.169_{0.19}$ & \cellcolor[HTML]{B4B9D2}0.289 \\
 & Aya101 & \cellcolor[HTML]{D8A4AC}$0.203_{0.21}$ & \cellcolor[HTML]{CEAAB7}0.350 \\
\multirow{-6}{*}{\textbf{Medium}} & GPT-3.5-Turbo & \cellcolor[HTML]{DBA2A8}$0.207_{0.22}$ & \cellcolor[HTML]{B3BAD3}0.287 \\ \midrule
 & bloomz-560m & \cellcolor[HTML]{B2BAD4}$0.163_{0.17}$ & \cellcolor[HTML]{BDB4C8}0.311 \\
 & bloomz-1b1 & \cellcolor[HTML]{D3A7B1}$0.198_{0.19}$ & \cellcolor[HTML]{D9A3AB}0.377 \\
 & bloomz-3b & \cellcolor[HTML]{E79B9C}$0.219_{0.22}$ & \cellcolor[HTML]{EA9999}0.416 \\
 & bloomz-7b1 & \cellcolor[HTML]{EA9999}$0.222_{0.23}$ & \cellcolor[HTML]{EA9999}0.416 \\
 & Aya101 & \cellcolor[HTML]{E09FA3}$0.212_{0.2}$ & \cellcolor[HTML]{E09FA3}0.394 \\
\multirow{-6}{*}{\textbf{Low}} & GPT-3.5-Turbo & \cellcolor[HTML]{E49D9F}$0.216_{0.22}$ & \cellcolor[HTML]{ACBEDA}0.271 \\ \bottomrule
\end{tabular}%
}
\captionof{table}{\avgToxFull and \empProbFull of multilingual models clustered by language resources. \textbf{\textit{Takeaway:}} Toxicity decreases as the availability of language resources increases.}
\label{tab:benchmark-lang-res}
    \end{minipage}
    \hfill
\begin{minipage}[b]{0.4\textwidth}
\centering
\includegraphics[width=5.5cm]{images-new/pythia_size_at.pdf}
\captionof{figure}{Influence of model size on \avgTox for Pythia suite. \textbf{\textit{Takeaway}}: Toxicity increases with model size within a model family for base LLMs.}
\label{fig:pythia}
    \end{minipage}
\end{minipage}

\subsection{\textbf{How does \textit{Model Size} impact \textsc{Average Toxicity}?}}

Prior work has shown that undesirable content generation can increase with model size and possibly pretraining dataset size \citep{10.1145/3442188.3445922, tal-etal-2022-fewer, smith-etal-2022-im,touvron2023llama}. We conduct a similar investigation on the impact of model size on toxicity. We first study these trends in \baseModel models such as Llama 2 \citep{touvron2023llama2} and Pythia \citep{biderman2023pythia}, and later examine models with additional tuning (\instructModel, \prefModel) such as Tulu 2 \citep{ivison2023camels}.

\paragraph{Effect of \textit{Model Size} for Base LLMs}
We investigate the distribution of continuation toxicity for \textit{base} LLMs, that is, models trained with only the language modeling objective. We observe a slight correlation between the number of parameters in the model and the continuation toxicity for base LLMs ($r=0.015$, $p<0.001$).
Prior work has shown limited evidence of the dependence of model toxicity on size. For instance, \citet{touvron2023llama, touvron2023llama2} find that toxicity increases with model size, whereas 
\citet{gehman-etal-2020-realtoxicityprompts, NEURIPS2022_c1e2faff} find that larger models are not necessarily more toxic. We hypothesize that toxicity might depend on model size within a model family only, and investigate this further with the Pythia suite.

The Pythia suite provides models of varying sizes while keeping the pretraining data and other hyperparameters constant. We utilize these models for a controlled investigation of the impact of model size on toxicity using the English split of our dataset. Figure \ref{fig:pythia} shows an overall increase in toxicity with an increase in model size, which plateaus near $2.8b$ parameters (effect size of the difference between $2.8b$ and $12b$ is small, Cohen's $d\leq 0.1$, $p \leq 0.1$).

\begin{wrapfigure}[27]{r}{4cm}
    \centering
    \vspace{-20pt}
    \includegraphics[width=4cm]{images-new/at_llama_tulu_size.pdf}
    \vspace{-10pt}
    \caption{Influence of model size on \avgTox in aligned models. \textbf{\textit{Takeaway}}: Future work is required for \textit{safety-aligned} LLMs.}
    \label{fig:align_size}

    \vspace{10pt}
    
    \includegraphics[width=3.5cm]{images-new/base_instruct_preference_avg_tox-1.pdf}
    \caption{\textsc{AT} for different model categories. \textbf{\textit{Takeaway}}: \baseModel $>$ \instructModel $\approx$ \prefModel.}
    \label{fig:rq_model_type_tox}
    
\end{wrapfigure}

This is consistent with prior works \citep{touvron2023llama, touvron2023llama2}. More specifically, we find that the toxicity levels in $1b+$ Pythia models are comparatively higher than the smallest $70m$ model (Cohen's $d\geq 0.3$, $p \leq 0.001$).
This implies that toxicity is a long-tail phenomenon that large enough models ($> 1b$ parameter count) are capable of capturing and demonstrating, akin to how larger models memorize better \citep{tirumala2022memorization}.

\paragraph{Effect of \textit{Model Size} for Safeguarded LLMs}

To investigate the impact of model size on toxicity for safeguarded LLMs, we benchmark Llama 2-Chat and Tulu 2-DPO models on English and other related languages (constituting top-10 languages in Llama 2's pretraining data) as shown in Figure \ref{fig:align_size}.

We observe different trends in both model families when scaling from $7b$ to $70b$ --- for Llama 2-Chat models, \avgTox first decreases and then increases as the model size increases. In contrast, DPO alignment first increases and then reduces toxicity for Tulu 2 models as they are scaled to $70b$ parameters. However, such differences are small (Cohen's $d < 0.15$ for all combinations with $70b$ models).

There seems to be no conclusive answer as to whether model size affects toxicity in safeguarded LLMs. We hypothesize that discrepancies concerning smaller safeguarded models such as lack of hyperparameter tuning or reward models trained toward generations by larger models, and challenges in unlearning harmful behavior (especially as model size decreases) could explain these results. 
Thus, future work is needed to investigate the specific effects of model sizes on toxic degeneration in safety-aligned models. 

\subsection{\textbf{How do \textit{Alignment Methods} impact \textsc{Average Toxicity}?}}

\begin{wrapfigure}[13]{r}{4.5cm}
    \centering
    \vspace{-20pt}
    \includegraphics[width=4.5cm]{images-new/align_at-manual.pdf}
    \vspace{-20pt}
    \caption{Impact of alignment techniques on TinyLlama and Archangel models. \textbf{\textit{Takeaway}}: Alignment methods don't impact toxicity.}
    \label{fig:align-tech}
\end{wrapfigure}

While prior work has shown that safety alignment leads to reduced toxicity levels in models \citep{touvron2023llama2}, the impact of different alignment methods on toxicity is yet to be studied. 
We investigate the impact of instruction-tuning and preference-tuning using different alignment methods, namely PPO \citep{schulman2017proximal}, DPO \citep{rafailov2024direct}, KTO \citep{ethayarajh2024kto}, and IPO \citep{azar2023general} on toxicity. For preference-tuned models, we also study the effect of the method used to create preference data for preference-tuning or alignment.

\paragraph{Base vs. Instruction-Tuning vs. Preference-Tuning}

We first compare toxicity levels aggregated over \baseModel, \instructModel, and \prefModel models (Figure \ref{fig:rq_model_type_tox}). 
We find that, on average, \baseModel models have the highest toxicity (\textsc{AT}$=0.281$; significantly different from \instructModel and \prefModel models; Cohen's $d=0.40$ and $d=0.43$, respectively, $p<0.001$).
Furthermore, we find that \instructModel and \prefModel models barely differ in toxicity (Cohen's $d=0.02$, $p < 0.001$), though preference-tuned models have slightly lower toxicity on average.

\paragraph{Effect of Various Alignment Methods}

To study the impact of different preference-tuning methods, we benchmark models that have been trained on the same data but with different alignment methods. Specifically, we use the Archangel suite\footnote{\url{https://huggingface.co/collections/ContextualAI/archangel-65bd45029fa020161b052430}} of Llama models \citep{touvron2023llama} and TinyLLama\footnote{\url{https://huggingface.co/collections/abideen/tinyllama-alignment-65a2a99c8ac0602820a22a46}} \citep{zhang2024tinyllama} models.

Interestingly, we do not observe a considerable difference in the average toxicity exhibited by models trained with different alignment methods (Cohen's $d<0.1$) (Figure \ref{fig:align-tech}). Moreover, this trend remains at different scales of $1b$, $7b$, and $13b$, suggesting that specific choices of the preference-tuning method might not make as much of a difference as preference data on model toxicity.

\paragraph{Preference-Tuning Dataset: Human Feedback vs AI Feedback}

\begin{wrapfigure}[15]{r}{5cm}
    \centering
    \vspace{-10pt}
    \includegraphics[width=5cm]{images-new/at_rlhf_rlaif.pdf}
    \vspace{-20pt}
    \caption{Influence of Human vs AI Feedback on toxicity. \textbf{\textit{Takeaway}}: AI feedback is better than human feedback for the language(s) targeted by the technique (\textit{en} in this case).}
    \label{fig:align-data}
\end{wrapfigure}

To investigate the influence of preference data curated with human and AI feedback, we benchmark Gemma 7B \citep{team2024gemma} variants. Specifically, we compare gemma-7b-it, trained on human preferences, and zephyr-7b-gemma-v0.1,\footnote{\url{https://huggingface.co/HuggingFaceH4/zephyr-7b-gemma-v0.1}} trained on AI preferences (Figure \ref{fig:align-data}). We observe that AI feedback is better than human feedback for \textit{en}, whereas human feedback shows lower toxicity levels for non-English languages. We emphasize toxicity results on the \textit{en} split since both models were trained using English-only preference data, likely making multilingual prompts out-of-distribution. Furthermore, zephyr-7b-gemma-v0.1 is aligned using DPO which has been found to reduce multilingual capabilities \citep{ivison2023camels}, likely leading to higher toxicity for non-English languages. 

While this suggests that AI feedback reduces model toxicity, we hypothesize that the operationalization of toxicity might play a role.
AI feedback relies on LLMs' definition of toxic content, which likely aligns better with \perspectiveAPI's perception of toxicity rather than human perceptions, which are more nuanced and subjective \citep{sap2021annotators}.
Furthermore, curating datasets using models can result in the under-representation of more veiled toxicity \citep{han-tsvetkov-2020-fortifying} and general data and topical skews \citep{das2024under}.

\subsection{\textbf{Comparing \textit{Toxicity} and \textit{Safety} Detectors: \perspectiveAPI vs. Llama Guard}}
\label{sec: llg_papi_main}
Recent work has seen rapid growth in studies on safety evaluation and safeguarding techniques \citep{ganguli2022red, mazeika2024harmbench}. For instance, \citet{inan2023llama} develop Llama Guard, a Llama 2 model to classify safety risks in LLM inputs and responses. However, the extent to which toxicity and safety overlap is unclear. To fill this gap, we compare \perspectiveAPI, a \textit{toxicity} detector, and Llama Guard, a \textit{safety} detector.

Since Llama Guard only supports English, we compute scores for all models on the English split of \datasetSmall following the instructions in its model card.\footnote{\url{https://huggingface.co/meta-llama/LlamaGuard-7b}} We find that \perspectiveAPI toxicity scores are generally well-aligned with Llama Guard scores ($r = 0.78, p \leq 0.001$). 

However, Llama Guard and \perspectiveAPI still capture distinct concepts. To analyze the differences between both evaluation methods, we examine the prompts and generations where the metrics differ the most (Table \ref{tab:llg_pa} in Appendix \ref{sec: llg_pa}). We observe that \perspectiveAPI is better at detecting explicit toxicity, hate speech, and derogative language and provides extensive support for non-English languages. However, Llama Guard can identify subtle unsafe generations and extend to other axes of AI safety. 
Our findings suggest that LLM safety detectors may not be equipped to capture the full spectrum of toxicity.

\subsection{\textbf{How does \textit{Prompt Toxicity} impact \textsc{Continuation Toxicity}?}}

We investigate the relationship between input prompt toxicity and continuation toxicity at greater granularity, that is, without aggregating as in \avgToxFull. Intuitively, we expect a model's propensity to generate toxic text to be proportional to the toxicity of the input prompt. Empirically, we find a Pearson correlation of $0.49$ ($p \leq 0.001$) between prompt toxicity and continuation toxicity. 
We also find that continuation toxicity spans the entire toxicity range, regardless of input toxicity score, indicating that non-toxic prompts can yield toxic continuations and vice-versa, corroborating \citet{gehman-etal-2020-realtoxicityprompts}. Furthermore, we investigate the correlations between prompt and continuation toxicity across languages and model families in Appendix \ref{app:prompt-cont}.

\paragraph{Comparing Model Categories} We examine the extent to which different model categories mirror input toxicity.
We find that the continuation toxicity of \baseModel models is most strongly correlated with input toxicity ($r=0.65$, $p<0.001$).
Surprisingly, \prefModel models have a higher correlation between input and continuation toxicity ($r=0.49$, $p<0.001$), compared to \instructModel models ($r=0.44$, $p<0.001$).
We find that this is due in large to low-toxicity prompts, for which \prefModel models mimic the input (low) toxicity in continuations better ($r=0.43$) than for high-toxicity prompts ($r=0.16$).
\instructModel models also show a stronger correlation between prompt and continuation toxicity for low-toxicity prompts ($r=0.32$) than for high-toxicity ones ($r=0.18$). % high  for low
This indicates that \prefModel models better match input toxicity than \instructModel models, but predominantly in low-toxicity inputs, suggesting that \prefModel models are better safeguarded against high-toxicity inputs.

\subsection{\textbf{How do different \textit{Data Sources} elicit \avgToxFull?}}

Finally, we study the ability of different data sources to elicit toxicity from LLMs. Specifically, we compare \avgToxFull when generating continuations for naturally occurring prompts from \datasetAbbrev, RTP-LX \citep{dewynter2024rtplx}, and an automatically translated sample of user-LLM interactions from WildChat \citep{zhao2024wildchat}.\footnote{We provide details about RTP-LX and WildChat in Appendix \ref{app:ptp_vs_wc}.}

\begin{wrapfigure}[12]{r}{8.5cm}
    \centering
    \vspace{-8pt}
    
    \includegraphics[width=0.6\textwidth]{images-new/ptp_vs_lx_wildchat.pdf}
    \caption{\avgTox trends for multilingual models on WildChat, RTP-LX, and \datasetAbbrev. \textbf{\textit{Takeaway}}: \datasetAbbrev elicits higher toxicity scores compared to WildChat and RTP-LX.}
    \label{fig:ptp_wc}
\end{wrapfigure}

Figure \ref{fig:ptp_wc} shows that \datasetAbbrev consistently draws out higher \avgToxFull. While RTP-LX is comprised of naturally occurring prompts in English and their culturally-aware translations to other languages, we find that \datasetAbbrev is still able to capture more toxicity, likely due to longer prompt lengths, corroborating \citet{anilmany}. Furthermore, we hypothesize that preference-tuning makes models less vulnerable to what users input into LLMs as opposed to naturally occurring toxicity, leading to higher toxicity levels elicited by \datasetAbbrev compared to WildChat.
\section{Conclusion}

We present \datasetName, the first large-scale multilingual benchmark of 425K naturally occurring prompts across 17 languages for evaluating toxic degenerations in LLMs. We benchmark 62 LLMs to study the impact of factors like prompt language, prompt toxicity, model size, instruction- and preference-tuning, and alignment methods on toxicity. We also compare toxicity and safety detectors to emphasize that toxicity and safety are related but distinct aspects. Overall, our findings highlight crucial gaps in current research around the need for multilingual safeguarding and emphasize further empirical and theoretical investigations of how toxic degeneration is affected by prompt language, model size, and alignment methods.

\section*{Limitations}
\label{sec: limitations}

We describe several limitations of our work. First, toxicity is subjective and our measure of toxicity may not cover all aspects of toxicity \citep{sap2021annotators}. Human validations of toxicity would help corroborate our results, but the scale of our experiments, coupled with possible disagreements between annotators due to the subjective nature of the task make validations challenging \citep{doi:10.1111/1471-6402.00110, sap-etal-2019-risk}. Second, we focus on naturally occurring prompts in web-text to create our benchmark, which may not be representative of user-LLM interactions \citep{lin2023toxicchat} or extensively cover conversational toxicity such as what might arise on social media \citep{dodge-etal-2021-documenting}. Third, our testbed does not extend to low-resource languages due to the lack of toxicity detection tools.% \vspace{-10pt}

\section*{Ethics Statement}
\label{sec: ethics-statement}

\paragraph{Dataset Release} The purpose of our work is to provide a standard multilingual benchmark to evaluate toxic degenerations in LLMs. As noted in the limitations, our prompts were extracted from naturally occurring web text and offer a limited representation of online data in general. While this mainly affects low-resource languages, it also skews the topics of online discussions \citep{dodge-etal-2021-documenting}. Our benchmark also doesn't cover more conversational toxicity such as what might arise on social media, which could be tricky to incorporate due to privacy issues \citep{elazar2024whats}. Finally, while our dataset includes toxic text, its intended use is not to increase the toxic outputs of a model unless the ultimate aim is to steer away from toxicity \citep{liu-etal-2021-dexperts}. As a safety measure, we plan to release the dataset using AI2's ImpAct license \footnote{\url{https://allenai.org/impact-license}} which helps mitigate the risks of dual use of resources.

\paragraph{Toxicity Detection} Previous work has shown that toxicity detection tools overestimate toxicity in text containing minority identity mentions \citep{10.1145/3278721.3278729, hutchinson-etal-2020-social, sap-etal-2019-risk}. \perspectiveAPI has also been shown to be biased against some languages such as German \citep{nogara2023toxic}. Nevertheless, our benchmark uses it as one possible operationalization of toxicity. Moreover, it can serve as a resource for studying the construct validity of toxicity as measured by \perspectiveAPI by providing stratified samples of web-text with ranges of both lower and higher toxicity scores. We release our benchmark and also encourage future work to apply other toxicity detectors as evaluations.

\paragraph{Toxicity and Machine Translation} Automatic translations can introduce deviations in toxicity due to incorrect translations and hallucinations \citep{specia-etal-2021-findings, sharou-specia-2022-taxonomy}.  
\citet{nllbteam2022language, costa-jussa-etal-2023-toxicity} show that automatic translations can also add toxicity across languages, introducing biases in toxicity evaluation on translated data.% \vspace{-10pt}

\section*{Reproducibility Statement}
\label{sec: reprod-statement}

We provide our dataset and code to reproduce our benchmarking experiments and encourage toxicity evaluations in future work: \url{https://anonymous.4open.science/r/ptp-5856}

\paragraph{Toxicity Detection} Prior work has shown that frequent retraining of black-box toxicity detection APIs such as \perspectiveAPI can lead to inaccurate comparisons and reproducibility challenges \citep{pozzobon-etal-2023-challenges}. Thus, we encourage readers to  re-run toxicity evaluations instead of adopting results from the papers they are comparing to.

\paragraph{Benchmarking Experiments} We used up to 128 GiB RAM and 4 NVIDIA RTX A6000s to generate completions with LLMs with up to 70b parameters for our benchmarking experiments. There are several considerations for our benchmarking experiments. First, we use only one configuration of random sampling (temperature $=0.7$, top\_p=$1.0$, maximum generation length $=512$ tokens). There could be differences in toxicity levels depending on different sampling methods and configurations. Based on how toxicity might be a long-tail phenomenon akin to memorization \citep{tirumala2022memorization}, we expect that the decoding algorithm might matter. Second, due to computation constraints, we use \datasetSmall to benchmark models. While \datasetSmall was randomly sampled from \datasetName, running on the full dataset might surface more toxicity than our sampled data surfaced. 

\textbf{Environmental Impact} While we evaluate a large number of models ($N = 62$) over \datasetSmall, leading to notable energy usage and carbon footprint, our findings can be used as a guide for model selection by readers, resulting in lower carbon emissions for future work.

\subsection*{\textbf{Acknowledgments}}

Special thanks to Ian Magnusson for providing feedback for our paper. We appreciate Vishwa Shah for helping us with the overview diagram design. This research was in part funded by Jigsaw.

\textbf{Data} We extend our gratitude to the authors whose meticulous efforts were instrumental in curating our dataset: mC4 \citep{xue-etal-2021-mt5}, and \thepile \citep{gao2020pile}. We also thank Tomek Korbak for filtering and open-sourcing a toxic collection of \thepile.

\paragraph{Software and Models} We would like to thank the contributors and maintainers of the vLLM \citep{kwon2023efficient} and Huggingface's Text Generation Inference libraries, which we leverage to generate continuations from models. Finally, we thank Jigsaw for providing access to \perspectiveAPI.

\newpage

\end{document}