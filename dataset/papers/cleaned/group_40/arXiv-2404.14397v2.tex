\title{RTP-LX: Can LLMs Evaluate Toxicity in Multilingual Scenarios?}

\begin{document}

\maketitle

\begin{abstract}
\emph{\textbf{Warning}: This paper contains and discusses content that may be offensive or upsetting.} 
Large language models (LLMs) and small language models (SLMs) are being adopted at remarkable speed, although their safety still remains a serious concern. 
With the advent of multilingual S/LLMs, the question now becomes a matter of scale: can we expand multilingual safety evaluations of these models with the same velocity at which they are deployed? 
To this end, we introduce RTP-LX, a human-transcreated and human-annotated corpus of toxic prompts and outputs in 28 languages. 
RTP-LX follows participatory design practices, and a portion of the corpus is especially designed to detect culturally-specific toxic language. 
We evaluate 10 S/LLMs on their ability to detect toxic content in a culturally-sensitive, multilingual scenario. 
We find that, although they typically score acceptably in terms of accuracy, they have low agreement with human judges when scoring holistically the toxicity of a prompt; and have difficulty discerning harm in context-dependent scenarios, particularly with subtle-yet-harmful content (e.g. microaggressions, bias). 
We release this dataset to contribute to further reduce harmful uses of these models and improve their safe deployment.
\end{abstract}

\begin{links}
  \link{Repository}{https://github.com/microsoft/RTP-LX}
\end{links}

\section{Introduction}
\label{sec:introduction}

\begin{figure}[t]
  \includegraphics[width=0.91\columnwidth]{images/results/kappa_prompt_polar.pdf}
  \caption{Weighted Cohen's $\kappa$ correlations per harm category in RTP-LX prompts. S/LLMs could detect insults, violence, and sexual content. 
  Subtler discourse--namely, microaggressions, bias, and identity attacks--were not easily detectable by any of the models.}
  \label{fig:cohen-kappa-prompt-polar}
\end{figure}

Large language models (LLMs) are being adopted swiftly in research and production applications. 
However, their tendency to memorise content \cite{carlini2022quantifying,PlagiariseLee,dewynter2023evaluation} and the fact that they are trained from publicly-available data means that they are very prone to spew harmful content \cite{sheng-etal-2019-woman,wang2023decodingtrust,rauh2022characteristics,gehman-etal-2020-realtoxicityprompts}. 
With the advent of more capable, multilingual LLMs such as GPT-4 \cite{openai2024gpt4} or BLOOMZ \cite{wang-etal-2020-multi}, toxic language detection must scale fast and effectively to the dozens, if not hundreds, of languages these models support.

LLMs and their more-portable, typically-open-source counterparts, \emph{small} language models (SLMs) have been used as annotators for some tasks with good results \cite{rethinkingsemantic,NEURIPS2023_91f18a12}. 
However, it remains unclear if S/LLMs can successfully annotate data in a culturally-sensitive multilingual scenario when harmful content is involved. 
This is important for content moderation, but also raises questions about the models' multilingual reasoning capabilities within a culture-specific context.

\subsection{Contributions}
To address whether S/LLMs can annotate and reason over toxic, culturally-specific content, we introduce \textbf{a multilingual corpus in 28 languages}, RTP-LX (``RTP-Language eXpanded''). 
It is comprised of about $1,100$ toxic prompts and outputs per language, and derived from the RTP dataset (``Real Toxicity Prompts''; \citealt{gehman-etal-2020-realtoxicityprompts}). 
Each prompt has been \textbf{professionally translated} by native speakers following \textbf{participatory design} practises, and the entire dataset has been human-labelled. 

While RTP-LX is a benchmark meant to facilitate safer deployment of S/LLMs in culturally-nuanced scenarios, our evaluation of ten S/LLMs on the corpus shows the following: 

\begin{itemize}
    \item S/LLMs typically score acceptably in terms of \emph{accuracy}, with GPT-4 Turbo and Gemma 7B having the highest percentages of correctly-classified examples. 
    \item However, S/LLMs have \textbf{low agreement with human judges} when scoring holistically the toxicity of a prompt. Namely, they have \textbf{difficulty discerning harm} in context-dependent scenarios, particularly with subtle-yet-harmful content such as \textbf{microaggressions and bias}. 
    \item The models generally have a \textbf{non-trivial amount of false positives}, ranging anywhere from around 0\% (Llama Guard) to 40\% (Gemma 2B). 
    \item We tie the previous findings to each language's online data availability. 
\end{itemize}
We argue that the S/LLMs' failure to detect certain categories could lead to erasure, and showcases the limitations of these models in classification on multilingual toxicity scenarios. 
Given their low agreement with humans, we posit that accuracy as a metric is insufficient to evaluate this task. 
Our findings morevoer illustrate the need for participatory design in the development of culturally-specific solutions.

\section{Related Work}
\label{sec:background}
We focus on the evaluation of S/LLMs, and their application as evaluators, both with a focus on multilingual toxicity scenarios. 
For an introduction to broader topics and open problems on S/LLMs evaluation see \citet{LLMEvalSurvey}. 

\subsection{S/LLMs as Evaluators}
There has been a push from the community to move towards automated evaluation benchmarks based on LLMs such as GPT-4 \cite{rethinkingsemantic,NEURIPS2023_91f18a12}, sometimes with good results, including high alignment with human judgments \cite{rethinkingsemantic}. 
This, however, does not extend to all domains, such as languages other than English, where some LLMs have low alignment with human judgements \citep{LLMLXEval}. 

Evaluation \textit{of} LLMs in multilingual scenarios has shown that larger models outperform smaller models, but that data contamination does affect evaluation metrics \citep{megaverse}. Benchmarks like MultiQ \cite{multiq} and others \cite{lai-etal-2023-chatgpt,megaverse,LLMLXEval} have likewise found that there are performance differences across languages. 

\subsection{Toxicity Evaluation of S/LLMs}
It is well-known that S/LLMs are prone to memorise and generate harmful content \cite{sheng-etal-2019-woman,wang2023decodingtrust,rauh2022characteristics,gehman-etal-2020-realtoxicityprompts}. 
Hence there is a growing focus on detecting and addressing their toxicity \cite{rauh2022characteristics,gehman-etal-2020-realtoxicityprompts}, although there remains a gap in understanding it across different languages and cultures. 
This oversight is particularly crucial because S/LLMs are well-known to exhibit biases across various demographic, racial, and cultural lines \cite{dhmala2021}.

Current S/LLMs are not equipped to identify these sensitivities out-of-the-box, and they require additional fine-tuning data for mitigation \cite{hebrewoffensive}. 
However, generating high-quality, annotated datasets is challenging, especially when they are built without following participatory design \cite{davidson2019}--that is, involving the target user base at every step of the system's development. 
For instance, \citet{sap-etal-2019-risk} uncovered bias in hate speech detection stemming from the annotators' lack of sensitivity to African-American Vernacular English.

This issue is exacerbated in multilingual contexts, especially in low-resource languages \cite{deng2024multilingual, lai2023chatgpt}. 
Although efforts have been made to fine-tune S/LLMs using data from multiple languages \cite{wang2023languages, wang2024chinese, hebrewoffensive}, the challenges around lack of participatory design remain when generating datasets. 
For instance, \citet{wang2024chinese} highlighted the prevalence of region-specific risks in S/LLM performance when detecting toxicity in Mandarin Chinese. 
A recent work by \citet{jain2024polyglotoxicitypromptsmultilingualevaluationneural} scraped data from the internet, translated it, and then evaluated it using machine methods. While scalable and cost-efficient, this pipeline currently lacks the cultural sensitivity that comes from participatory design \cite{li2024how}, in particular with regards to localisms and subtler issues that, as we will argue in this paper, require more attention. 

\section{RTP-LX}
\label{sec:dataset}
The seed corpus for RTP-LX is a subset of $1,000$ prompts from the original RTP corpus. 
RTP is a set of nearly $100,000$ toxic sentences mined from Reddit. Each entry in the corpus is split into prompts and completions, and then annotated via the Perspective API\footnote{\url{https://www.perspectiveapi.com/}}. The prompts for the seed corpus were collected with an even proportion of explicitly toxic prompts, and prompts that led to a toxic completion (i.e., the completion is labelled as toxic while the prompt is below a threshold), based on their toxicity score. 
We chose the top $1,000$ prompts fulfilling this criterion, and corresponds to the English subset of RTP-LX. 
The full set of languages and dialects supported by RTP-LX is in Appendix \ref{app:languages-supported}. 

\subsection{Participatory Design}
In addition to the prompts from RTP, we also manually curated 50-100 toxic prompts that fulfilled the following:

\begin{enumerate}
    \item They could not be explicitly toxic (e.g., no profanity), since a simple string matching algorithm could detect it. 
    \item They had to be hard to translate into English. 
    This was introduced due the scarcity of human-designed datasets; and, dually, due to the abundance of machine-translated corpora.%
    \item They had to relate explicitly to their culture. 
    Although self-explanatory, this requirement was phrased as ``[the prompt] could lead to an output which is offensive to a native speaker''. 
\end{enumerate}

Requirements (2) and (3) are somewhat related: hard-to-translate content (slang, specific sensitive subjects, etc.) is deemed so because of the difficulty of capturing nuances without adding an explanation of \emph{why} it is harmful. 

For example, in Thailand, the monarchy is a highly-sensitive topic: the royal family is held in high regard, and their criticism could result in legal repercussions. 
Even seemingly innocuous prompts, such as ``What is the role of a monarch in a country?'' could generate responses sensitive to some Thai readers.  

Due to the complexity of finding these prompts, we complemented some languages with similar datasets in Hebrew \cite{hebrewoffensive}, Danish \cite{sigurbergsson-derczynski-2020-offensive}, Korean \cite{moon-etal-2020-beep}, and Brazilian Portuguese \cite{ToLDBR}. 

\subsection{Transcreation}
\citet{gehman-etal-2020-realtoxicityprompts} showed that LLMs memorise and return some of the most harmful content from their training data via sentence completion. 
Since Reddit--the source for RTP--is a primarily US-based site, the subjects handled by RTP are typically US-centric. 

To address this and maintain a trustworthy dataset with equivalence across locales, the seed RTP corpus was professionally transcreated. 
In addition to the cultural relevance, this introduced a minor source of noise helpful in ablating out data contamination, which is well-known to cause problems in LLM evaluations, especially in multilingual settings \cite{dewynter2023evaluation,megaverse}. 

The translators were encouraged to try out multiple dialects if they were familiar with them, and specify them in the corpus. 
They were also given the sentence completion to ensure that the original intent from the prompt was preserved, and encouraged to use the closest-cultural equivalent to remove as many allusions to US English as possible.\footnote{For example, replacing ``George Washington'' with ``Touissant Louverture'' in Haitian French.} 
We don't know how many sentences were transcreated, as the translators noted that many prevalent aspects of US culture in the data (e.g., ``Donald Trump'') did not need replacement. 

\subsection{Completion}
In addition to the transcreated prompts, we created toxic and benign completions. For the benign completions, the transcreators were requested to, given the prompt, write a completion such that it would be lowest-scoring in all harm categories for the corpus. 
We generated the toxic prompt completions with \texttt{gpt-4-32k-0613}, called through the Azure OpenAI Platform, and cleaned them prior to sending them to annotation along with the transcreated prompts. 

\subsection{Annotation}
We used a set of harm categories slightly different than that of RTP, closer to that of the Azure Content Safety (ACS) service\footnote{\url{https://learn.microsoft.com/en-us/azure/ai-services/content-safety/}}, because it would enable us to detect subtler harms. 
The harm categories are: Bias, Identity Attack, Insult, Microaggression, Self-Harm, Sexual Content, Toxicity and Violence. The definition of each is in Appendix \ref{app:guidelines}. 
Toxicity is scored on a five-point Likert scale, and the rest of the harms on a three-point scale. 
We used ternary scales to leave less room of ambiguity for the S/LLMs, thus avoiding overly optimistic LLM scoring--a known issue in multilingual scenarios \citep{LLMLXEval}. 

The annotators were given guidelines (available online) and could run a test and ask questions prior to fully annotating the data. 
Each harm was annotated independently, and used Toxicity as an overall score of the prompt's (or completion's) toxicity. 
To avoid any potential conflicts with internal value systems, we also attached a copy of a uniform value system. 
This value system is designed to mitigate the risks of working with toxic language in a global context, which we discuss further in Section \ref{sec:limitations}. 
However, annotators were asked to use their best judgement and only defer to the value system when in doubt. 

\subsection{Inter-Rater Reliability (IRR)}
\label{sec:irr}
We measured IRR with weighted Cohen's $\kappa$, or $\kappa_w$, and observed a substantial positive agreement in the corpus ($0.62 \pm 0.2$ overall). 
We chose $\kappa_w$ because it takes into account the value of the ordinal, so broad differences in scoring (e.g., $1$-$3$ versus $1$-$2$) are encoded in this measure. 
To account for multiple annotators, we took pairwise IRR and averaged it out. 

\section{Evaluation Setup}
\label{sec:methodology}

\subsection{Models Evaluated}
\label{sec:models-evaluated}
We evaluated ten S/LLMs: four Llama \cite{llama2, llama3modelcard}, two Gemma \cite{gemmateam2024gemma}, and two Mistral \cite{mistral} variants; plus Llama Guard \cite{llamaguard} and GPT-4 Turbo. 
All models were called through their respective Hugging Face model cards on four A100 80Gb PCIE GPUs; except GPT-4 Turbo, which was via Azure OpenAI. The data analysis was done in a consumer-grade laptop. 
We used temperature of zero throughout, and all outputs were generated between 
$11^{\text{th}}$ and $25^{\text{th}}$ May, 2024. 

\begin{itemize}
  \item GPT-4 is a LLM from OpenAI. We used \texttt{gpt-4-turbo-2024-04-09},  which was explicitly noted to have multilingual capabilities, and has shown good performance in various multilingual benchmarks.
  \item Llama is a family of open-source SLMs by Meta. 
  We use \texttt{Llama-3-8b-Instruct}, \texttt{Llama-3-70b-Instruct}, \texttt{Llama-2-7b-chat}, and \texttt{Llama-2-70b-chat}. 
  The original papers mention a multilingual training corpus; but the models were evaluated only in English and the authors indicate that non-English use is out-of-scope.
  \item Llama Guard is an SLM based on Llama-2, designed to classify content safety. We work with \texttt{LlamaGuard-7b}. While not explicitly mentioned, we assume Llama Guard to be English-only.
  \item Gemma is an SLM by Google. We evaluate \texttt{gemma-2b-it} and \texttt{gemma-7b-it}. It does not claim to be multilingual, and the authors indicate the training data to be mainly in English.
  \item Mistral is a 7B parameter model by MistralAI. We evaluate \texttt{Mistral-7B-Instruct-v0.3} and \texttt{Mistral-7B-Instruct-v0.2}. 
  These models are not stated to be multilingual, and do not have any moderation mechanisms.
\end{itemize}

In addition to the S/LLMs above, we evaluated two non-S/LLM solutions: ACS and the FLORES Toxicity-200 block list \cite{nllb2022}. These acted as our baselines, in addition to RTP-LX's own English subset. 

\begin{itemize}
    \item ACS is a content moderating service by Azure. Its API returns a subset of the harms from RTP-LX (Identity Attack, Violence, Self-Harm, and Sexual Content) in a scale from 1-10. 
    It explicitly supports the languages from our corpus, although it is has only been evaluated in a smaller subset. 
    We evaluated this API in February 2024; and re-normalised the scores to our scale. 
    \item FLORES Toxicity-200 is a collection of frequent words and phrases that are considered toxic. 
    It is human-translated and covers all the languages for RTP-LX. 
    It also includes a finer dialectal distinction compared to our corpus. 
    In this paper we consider it a baseline via exact match (EM): if any toxic word is present, we flag it. This baseline helps us determine whether lexical matching is sufficient to address multilingual toxicity.
\end{itemize}

\subsection{Prompting}
\label{sec:experimental-setup}
We modified the annotation rubric to included exemplars, and formatted it as per each model's requirements (e.g. ChatML). 
The prompt is in the Appendix. 
We wrote a parser to extract scores from the response, and to account for pathologies of some models, like GPT-4's boilerplate (``One possible answer is...''). 

\subsection{Metrics}
In addition to EM for FLORES, in the rest of our experiments we calculated IRR between the aggregated (majority vote; average otherwise) human label and S/LLMs with Percentage Agreement (PA) and with $\kappa_w$. 
PA is a comparison of an exact label match and provides a raw, interpretable number. It does not account for multi-class imbalance (i.e., a ``lazy learner'' could guess one label and score well), or the tendency of the S/LLMs to score inputs as more or less harmful than human judges. 
To address the lazy learner issue, we compute $\kappa_w$, which lets us determine the agreement between two raters over a random guess. 

\section{Results}\label{sec:results}

We performed three experiments: a baselining with FLORES Toxicity-200, the actual evaluation of the S/LLMs, and an ablation study relating S/LLM performance with language availability. 

Throughout this section, we partition the full RTP-LX corpus in either the \textbf{toxic prompts} and \textbf{benign completions} subset; or the \textbf{transcreated} subset (i.e., the original RTP prompts with its corresponding transcreations) and the \textbf{manual} subset (culturally-specific hand-crafted prompts for every language). 
Both partitions (toxic/benign and transcreated/manual) overlap, but represent different aspects of the dataset. Namely, the toxic/benign split allows us to determine whether S/LLMs can recognise toxic human-generated content. On the other hand, the transcreated set allows us to have machine-translation free comparability across languages; while the manual subset serves as a testbench for model performance in culturally-sensitive issues.  

\subsection{FLORES Exact-Match Block Rate}\label{sec:flores-baseline}
The results of our experiment for FLORES Toxicity-200 block list is in Figure \ref{fig:blockrate-toxic}. 
In the toxic prompts subset, the block list had a $24.3\pm8.3\%$ block rate across all languages and partitions, with Japanese being the lowest ($10\%$) and Thai the highest ($46\%$). 
The manual subset had a much lower ($-8\%$ average) block rate when compared to the transcreated subset. 
This suggests that the models, on average, should consider $24\%$ of the toxic prompts corpus with a label denoting some toxicity. 

We also calculated the block rates for the benign completions. The completions have around a 0\% block rate across languages.
Overall, this suggests that RTP-LX reliance on lexical features--as opposed to semantic features--is comparatively low ($24\%$). %

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{images/results/block_rate_polar_prompt.pdf}
    \caption{EM block rates when calculated using FLORES' Toxicity-200 block list for the transcreated/manual partition of RTP-LX. 
    FLORES had an average $24.3\pm8.3\%$ block rate across all languages and partitions. 
    The manual subset had a much lower ($-8\%$ average) block rate when compared to the transcreated subset. 
    This suggests that the S/LLMs, on average, should consider $24\%$ of the corpus with a label denoting at least some toxicity. 
    Note that English does not have a manual corpus.}
    \label{fig:blockrate-toxic}
\end{figure}

\begin{figure*}[h]
    \centering
    \includegraphics[width = 0.49\textwidth]{images/results/pa_avg_prompt.pdf}
    \includegraphics[width = 0.49\textwidth]{images/results/kappa_avg_prompt.pdf}
    \caption{We labelled the prompt subset with the S/LLMs and compared their output with the majority vote of the human scores.
    In terms of raw accuracy (\textit{left}), Llama Guard outperformed all other S/LLMs, closely followed by Gemma 7B and GPT-4 Turbo. 
    ACS outperformed all other approaches, but ACS was only evaluated as the average of four, not eight, harm categories; and its agreement is lower than GPT-4's on these categories alone. 
    When looking at mean $\kappa_w$ (\textit{right}), it is clear that raw accuracy scoring is not a sufficient measure due to RTP-LX's class imbalance--a lazy learner could output always the same label and obtain a decent performance. 
    In fact, that is what happened for some models, such as Gemma 2B. 
    }
    \label{fig:mainresults}
\end{figure*}

\subsection{Evaluation of S/LLMs}\label{sec:sllmeval}

We evaluated the labelling performance of the S/LLMs in RTP-LX by comparing their output with the aggregated human scores in terms of PA and $\kappa_w$ (Figure \ref{fig:mainresults}).

\subsubsection{Toxic Prompts Subset}
In terms of PA, Llama Guard outperformed all other S/LLMs, closely followed by Gemma 7B and GPT-4 Turbo. The lowest-performing models were Gemma 2B and Llama-2 70B. 
ACS outperformed all other approaches, but it was only evaluated over half of the harm categories. 
When looking at $\kappa_w$, however, we found that Llama Guard--the best-performing model in terms of PA--dropped to almost the last position. 
GPT-4 Turbo was significantly better than other models. 
The Llama-3 models outperformed the older Llama versions; and a similar trend can be seen for the Mistral variants.

None of the models seem to come close to human performance, however. 
When looking at the harm category breakdown in Figure \ref{fig:cohen-kappa-prompt-polar}, we noted that the models were adept at detecting explicit content, such as violence, sexual content, and insults. 
However, comparatively subtle discourse, such as microaggressions, bias, and identity attacks, were not easily detectable by any of the models. 

This observation is reinforced by noting that although the human-authored labels are relatively even in terms of agreement across all categories, the agreement with S/LLMs is not, with an overall noticeable skewness towards not detecting microaggressions or overall toxicity. 

When looking at $\kappa_w$ we found that the models were not optimistic, as suggested by \citet{LLMLXEval}; instead, the S/LLMs were prone to output higher-valued labels. 
In the aforementioned work higher labels are considered better (hence the ``optimistic'' moniker); but in RTP-LX lower-valued labels are better. 
The per-category class distribution for toxic prompts showed that the models were also very prone to output binary labels on the ternary set (i.e., no presence of the criterion, or explicit presence of the criterion; but overlooking contextually harmful sentences), which suggested an additional source for the disagreement with human annotators. 

\subsubsection{Benign Completions Subset} 

We calculated the fraction of false positives (FP), which we define as the fraction of times the model predicted a label higher than 1. 
This helped us elucidate whether the S/LLMs do have a grasp on the task, or are solely outputting syntactically-relevant labels. 
The results are in Figure \ref{fig:fps-completion}. 
Llama Guard and ACS had near-zero FP, thus making them--in raw-scoring terms--the most reliable models. 
The rest of the S/LLMs follow a trend similar to the $\kappa_w$ values for the toxic prompts. 

\begin{figure}[h]
    \centering
    \includegraphics[width = \columnwidth]{images/results/pa_avg_completion.pdf}
    \caption{FPs across all languages for the S/LLMs. Gemma 2B presented the highest FP, misidentifying up to 40\% of the samples observed, while Llama Guard and ACS had near-zero FP.}
    \label{fig:fps-completion}
\end{figure}

\subsection{Language Availability Analysis}\label{sec:evaluationlanguage}

We explored the correlation, if any, between the availability online of the languages and the $\kappa_w$ in the prompts subset. 
For this, we use the classes defined by \citet{joshi-etal-2020-state}. 
In this work, the authors graded languages between 0 and 5, with zero-scoring languages being those with no labeled data at all; and languages scored with a five having the most (e.g., English). 
RTP-LX contains languages mostly from classes 3 to 5, which we denote here as low, mid, and high-resource languages. 
We report the $\kappa_w$ score for each model averaged across each group. 
Our results are in Figure \ref{fig:language-wise-analysis}. 

All models presented a decreasing trend in $\kappa_w$ from high to low-resource languages, with differences of up to around 10\%.

\begin{figure}[h]
    \centering
    \includegraphics[width = \columnwidth]{images/results/language_kappa.pdf}
    \caption{Language availability versus $\kappa_w$ over all languages in the prompts subset. 
    All S/LLMs decreased in $\kappa_w$ from high to low-resource languages, with differences of up to around 10\%.}
    \label{fig:language-wise-analysis}
\end{figure}

\section{Discussion}
\label{sec:discussion}
When simply looking at the percentage agreement as a metric of performance, the models do relatively well: all score above the $24.3\%$ theoretical minimum from an exact-match approach such as FLORES Toxicity-200. 
The S/LLMs have a relatively even performance, with Llama Guard and Gemma 7B at the lead. 

However, this observation is misleading: when analysing the accuracy and per-class breakdown, we note that the models tend to not agree with human judgements. 

Indeed, the breakdown per category shows that the models failed to correctly classify typically subtle-yet-harmful discourse such as microaggressions, bias, and identity attack. 
Concerningly, the holistic measure of toxicity in the models tended to be of lower-agreement. 
This is perhaps because of our observation that the models typically select higher-valued labels and often output binary labels, overlooking nuances. That is, they return either no presence of the harm criterion or explicitly presenting it, but not something that could be contextually construed as harmful. 

This all suggests that, although the S/LLMs typically score well in accuracy-based measures, this metric performance alone does not imply that they can perform a reliable job as judges of toxicity in multilingual scenarios. 
Moreover, they have difficulties in discerning harm in context-dependent situations, especially, as we mentioned, when dealing with subtle content. 

It is worth noting that the Llama and Mistral models, although claimed to be pretrained with a relatively small volume of multilingual data, perform comparatively well in non-English scenarios. 

\section{Limitations}
\label{sec:limitations}

We have three core limitations in our work: the nature of our data, the nature of our problem, and the models evaluated. 

\paragraph{Data Limitations} RTP-LX has two main limitations: cultural skewness and coverage. 
In terms of cultural skewness, the majority of this corpus is sourced from RTP, which itself was obtained from a predominantly website with discourse in US English. 
We have mitigated this by ensuring transcreation, not translation, of the prompts along with adding in the manual subset of the corpus. 

In terms of coverage, RTP-LX lacks sufficient dialectal coverage. 
Although we encouraged translators and prompt authors to explore various dialects, more could be done in this area: certain languages (e.g. Arabic) vary considerably amongst dialects; 
and others, like Spanish, vary in terms of homonyms so much that their holistic evaluation of toxicity is notoriously complex. 
It is also worth noting that our corpus mostly covers Indo-European (Germanic, Romance) languages. 
We plan to perform further additions to the corpus to explore other families and expand on dialectal differences. 
However, increasing both the dialectal coverage and the cultural skewness of the corpus are likely to cause lower performance for S/LLM-based evaluators, not higher. 

\paragraph{Toxic Language in a Global Context} The study of toxicity in a multilingual setting is difficult not only due to the scarcity of quality, human-generated corpora especially designed for this task; but also because of the constant evolution of language and its perception by native speakers. 
It is also worth noting that things that may be offensive to a native speaker in one geo-locale may not be offensive to that in another. 
We mitigated this by including a uniform value system and making a best effort on demographic representation, but nonetheless encouraged the annotators to use their best judgement and only defer to the value system when in doubt. 

\paragraph{S/LLMs Evaluated} S/LLMs are known to have data contamination issues that hamper fair evaluations. 
Although most of RTP-LX was hand-designed, there is no guarantee that our corpus will not be eventually used to train the models. 
We have adopted measures to protect the data against crawlers, while still leaving the data open to all. 

Moreover, S/LLMs undergo frequent updates. We have specified the versions of the models we tested to ensure reproducibility, but it is likely updated models--such as newer versions of Llama Guard--will have better performance in this corpus. 
Likewise, we did not evaluate fine-tuned models. Our focus was on base models, which are more widespread and the ``lowest'' possible bar. That said, fine-tuning the S/LLMs and turning them into specialists for this task could improve upon the numbers shown here. 
The difficulty of fine-tuning lies on the scarcity of high-quality, available data: a common problem in the study of toxic language in NLP \cite{hartvigsen-etal-2022-toxigen}. 
This, although out of scope for our paper, will preclude many specialists from arising. 
It is precisely for this reason that our experimental setup assumes low data availability, and emphasises human-created over machine-translated data.  

\section{Conclusion}
\label{sec:conclusion}

In this paper we explored the question of whether we can scale safety as fast as we scale S/LLMs. We framed it as a S/LLM capability analysis, by evaluating whether these models could work as annotators for multilingual toxic-language detection. 
To this end we introduced a human-annotated and human-transcreated corpus designed specifically to capture toxicity in a multilingual scenario. 

We evaluated ten S/LLMs in RTP-LX, and found that they are able to score highly when measuring raw accuracy. 
However, by changing the metric (by comparing the outputs to human annotator judgements) showed that this performance did not necessarily meant that the models are reliable judges of toxicity. 
Indeed, we attributed that high accuracy to class imbalance, since the vast majority of the corpus is harmful. 
We noted that the agreement between S/LLMs and human annotators was relatively low, and often came coupled with an increasing tend to over-block benign content. 
This indicates a potentially poor understanding of the task by the S/LLM. 
One possible exception was GPT-4 Turbo, which was able to score within one standard deviation of human judgements. 

Additionally, we found two pathologies common to some, if not all, S/LLMs evaluated: a tendency to select high-valued labels, which in RTP-LX means ``extreme harm'', and low agreement with humans in context-dependent, subtle-yet-harmful content (e.g. microagressions, bias). 
Both pathologies imply that the deployment of base, un-finetuned S/LLMs as multilingual harm detectors are likely to cause further problems, such as erasure. 

Further work will scale RTP-LX to more dialects and language families. 
As mentioned earlier, transcreating and annotating the dataset with humans is comparatively  time-consuming. 
However, it allows us to reliably address cultural sensitivity and subtler issues such as microaggressions. 
This means it is a worthwhile investment--as shown here, in a multilingual context, these subtle issues are precisely the ones requiring more attention in the field. 
Since we believe that RTP-LX is a resource necessary to combat harmful and toxic content in S/LLMs in research and in production, we have released the corpus, prompts, code used for this paper, and the rubrics, to the wider community.

\end{document}