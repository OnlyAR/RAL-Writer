\title{LongBench v2: Towards Deeper Understanding and\\ Reasoning on Realistic Long-context Multitasks}

\begin{document}

\maketitle

\begin{abstract}

This paper introduces LongBench v2, a benchmark designed to assess the ability of LLMs to handle long-context problems requiring \emph{deep understanding and reasoning} across real-world multitasks. LongBench v2 consists of 503 challenging multiple-choice questions, with contexts ranging from 8k to 2M words, across six major task categories: single-document QA, multi-document QA, long in-context learning, long-dialogue history understanding, code repository understanding, and long structured data understanding.
To ensure the breadth and the practicality, we collect data from nearly 100 highly educated individuals with diverse professional backgrounds. We employ both automated and manual review processes to maintain high quality and difficulty, resulting in human experts achieving only 53.7\% accuracy under a 15-minute time constraint.
Our evaluation reveals that the best-performing model, when directly answers the questions, achieves only 50.1\% accuracy. In contrast, the o1-preview model, which includes longer reasoning, achieves 57.7\%, surpassing the human baseline by 4\%. 
These results highlight the importance of enhanced reasoning ability and scaling inference-time compute to tackle the long-context challenges in LongBench v2.

\end{abstract}
\section{Introduction}

Over the past year, research and products on long-context large language models (LLMs) have made remarkable progress: in terms of context window length, advancing from the initial 8k to the current 128k and even 1M tokens~\cite{GPT-4o,claude-3-5,reid2024gemini,glm2024chatglm}; and achieving promising performance on long-context benchmarks. However, beneath these advancements lies an urgent and practical question: \textbf{Do these models truly comprehend the long texts they process, i.e., are they capable of deeply understanding, learning, and reasoning based on the information contained in these long texts?}

Critically, existing long-context understanding benchmarks~\cite{bai2024longbench,zhang2024infty,hsieh2024ruler} fail to reflect the long-context LLMs' \emph{deep} understanding capabilities across diverse tasks.
They often focus on extractive questions, where answers are directly found in the material, a challenge easily handled by modern long-context models and RAG systems, as evidenced by their perfect recall in the Needle-in-a-Haystack test~\cite{needleinhaystack}.
Furthermore, many of these benchmarks rely on synthetic tasks, which limits their applicability to real-world scenarios, and their adopted metrics like F1 and ROUGE are unreliable.

To address these issues, we aim to build a benchmark with the following features: 
(1) \textbf{Length}: Context length ranging from 8k to 2M words, with the majority under 128k.
(2) \textbf{Difficulty}: Challenging enough that even human experts, using search tools within the document, cannot answer correctly in a short time.
(3) \textbf{Coverage}: Cover various realistic scenarios.
(4) \textbf{Reliability}: All in a multiple-choice question format for reliable evaluation.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/length.pdf}
    \caption{Length distribution (left) and human expert solving time distribution (right) of LongBench v2.}
    \label{fig:length}
\end{figure}

With the above goal in mind, we present \emph{LongBench v2}.
LongBench v2 contains 503 multiple-choice questions and is made up of 6 major task categories and 20 subtasks to cover as many realistic deep comprehension scenarios as possible, including \emph{single-document QA}, \emph{multi-document QA}, \emph{long in-context learning}, \emph{long-dialogue history understanding}, \emph{code repository understanding}, and \emph{long structured data understanding} (detailed in Table~\ref{tb:stat}).
All the test data in LongBench v2 are in English, and the length distribution of each task category is shown on the left of Figure~\ref{fig:length}.

To ensure the quality and difficulty of test data, we combine automated and manual reviews during data collection. 
We first recruit 97 data annotators with diverse academic backgrounds and grades from top universities and then select 24 data reviewers from this group.
Annotators provide data including long documents, questions, options, answers, and evidence.
We then leverage three long-context LLMs for an automated review, where a question is considered too easy if all three LLMs answer it correctly.
Data passing the automated review are assigned to the reviewers, who answer the questions and determine whether the questions are appropriate (meet our requirements) and if the answers are correct.
In our criteria, a qualified data point should have (1) an appropriate question with an objective, correct answer; (2) sufficient difficulty, such that all three LLMs cannot answer correctly at the same time, and the human reviewer cannot answer correctly within 3 minutes, even with searching tools within the document.
If data do not meet these criteria, we request modifications from the annotator.
We also set length and difficulty incentives to encourage longer and harder test data.
Figure~\ref{fig:length} (right) visualizes the distribution of expert solving times along with human accuracy.

Overall, our data shows a median word count of 54k and an average of 104k words. 
Human experts are able to achieve an accuracy of only 53.7\% within 15 minutes, compared to 25\% accuracy with random guessing, highlighting the challenging nature of the test.
In the evaluation, the best-performing model achieves only 50.1\% accuracy when directly outputting the answer. In contrast, the o1-preview model, which incorporates longer reasoning during inference, reaches 57.7\%, surpassing human experts. This implies that LongBench v2 places greater demands on the reasoning ability of current models, and incorporating more inference-time thinking and reasoning appears to be a natural and crucial step in addressing such long-context reasoning challenges.
We hope LongBench v2 will accelerate the exploration of how scaling inference-time compute will affect deep understanding and reasoning in long-context scenarios.
\section{Related Work}

We divide existing long-context benchmarks for LLMs into two types. 
The first consists of comprehensive benchmarks that combine multitasks such as QA, retrieval, and summarization. 
Sorted by publication date, these benchmarks include ZeroSCROLLS~\cite{shaham2023zeroscrolls}, L-Eval~\cite{an2024leval}, LongBench~\cite{bai2024longbench}, BAMBOO~\cite{dong2024bamboo}, LooGLE~\cite{li2023loogle}, $\infty$-bench~\cite{zhang2024infty}, Ruler~\cite{hsieh2024ruler}, and HELMET~\cite{yen2024helmet}.
It is noteworthy that most of these multitask benchmarks were proposed last year, which corresponds to the thrive of long-context LLMs, whose context length has been extended to 128k tokens or more~\cite{claude-3-5,GPT-4o,reid2024gemini,glm2024chatglm,dubey2024llama} through continual training~\cite{xiong2024effective,pmlr-v235-fu24d,bai2024longalign,gao2024train}.

The other category of long-context benchmarks is more targeted, evaluating models on specific types of long-context tasks, including document QA~\cite{kovcisky2018narrativeqa,dua2019drop,dasigi2021dataset,pang2022quality,wang2024leave}, summarization~\cite{zhong2021qmsum,huang2021efficient,wang2022squality}, retrieval and attributing~\cite{needleinhaystack,kuratov2024babilong,song2024counting,laban2024summary,zhang2024longcite,vodrahalli2024michelangelo,krishna2024fact}, conversation~\cite{bai2024longalign}, coding~\cite{liu2023repobench,bogomolov2024long}, many-shot learning~\cite{agarwal2024many}, and long-text generation~\cite{bai2024longwriter,wu2024longgenbench,liu2024longgenbench,que2024hellobench}.

In our view, existing long-context benchmarks generally have the following issues: (1) \emph{Lack of deep reasoning}: While a few benchmarks contain longer examples of around 100k, most of these data have not been human-examined, and many of these samples can be solved through shallow understanding such as retrieval, thus failing to reflect a model's deep reasoning capabilities.
(2) \emph{Unreliable metrics}: Many datasets use metrics like ROUGE and F1 for evaluation, which are known to be unreliable~\cite{novikova2017we}. Additionally, some datasets adopt LLM-as-a-judge~\cite{zheng2023judging,li2024generation} for evaluation, which can be costly and may introduce biases in their assessments~\cite{bai2024benchmarking,ye2024justice}.
To construct a more challenging, reliable, and comprehensive long-context benchmark, we employ a uniform multiple-choice format and manually verify each data point to ensure it meets the required level of difficulty.
\begin{table*}[t]
\centering  
\resizebox{\textwidth}{!}{
\begin{tabular}{llrrrr}
\toprule
\textbf{Dataset} & \textbf{Source} & \textbf{\#data} & \textbf{Length} & \textbf{Expert Acc} & \textbf{Expert Time$^*$} \\
\midrule
\multicolumn{2}{l}{\cellcolor{mypink}\emph{I. Single-Document QA}} & \cellcolor{mypink}175 & \cellcolor{mypink}51k & \cellcolor{mypink}55\% & \cellcolor{mypink}8.9 min \\
Academic & Paper, textbook & 44 & 14k & 50\% & 7.3 min \\
Literary & Novel & 30 & 72k & 47\% & 8.5 min \\
Legal & Legal doc & 19 & 15k & 53\% & 13.1 min \\
Financial & Financial report & 22 & 49k & 59\% & 9.0 min \\
Governmental & Government report & 18 & 20k & 50\% & 9.5 min \\
Detective & Detective novel & 22 & 70k & 64\% & 9.3 min \\
Event ordering & Novel & 20 & 96k & 75\% & 9.4 min \\
\midrule
\multicolumn{2}{l}{\cellcolor{mypink}\emph{II. Multi-Document QA}} & \cellcolor{mypink}125 & \cellcolor{mypink}34k & \cellcolor{mypink}36\% & \cellcolor{mypink}6.1 min \\
Academic & Papers, textbooks & 50 & 27k & 22\% & 6.1 min \\
Legal & Legal docs & 14 & 28k & 64\% & 8.8 min \\
Financial & Financial reports & 15 & 129k & 40\% & 7.0 min \\
Governmental & Government reports & 23 & 89k & 22\% & 6.0 min \\
Multi-news & News & 23 & 15k & 61\% & 5.3 min \\
\midrule
\multicolumn{2}{l}{\cellcolor{mypink}\emph{III. Long In-context Learning}} & \cellcolor{mypink}81 & \cellcolor{mypink}71k & \cellcolor{mypink}63\% & \cellcolor{mypink}8.3 min \\
User guide QA & Electronic device, software, instrument & 40 & 61k & 63\% & 9.9 min \\
New language translation & Vocabulary book (\textit{Kalamang}, \textit{Zhuang}) & 20 & 132k & 75\% & 5.4 min \\
Many-shot learning & Multi-class classification task & 21 & 71k & 52\% & 8.0 min \\
\midrule
\multicolumn{2}{l}{\cellcolor{mypink}\emph{IV. Long-dialogue History Understanding}} & \cellcolor{mypink}39 & \cellcolor{mypink}25k & \cellcolor{mypink}79\% & \cellcolor{mypink}8.2 min \\
Agent history QA & LLM agents conversation & 20 & 13k & 70\% & 8.3 min \\
Dialogue history QA & User-LLM conversation & 19 & 77k & 89\% & 6.5 min \\
\midrule
\multicolumn{2}{l}{\cellcolor{mypink}\emph{V. Code Repository Understanding}} & \cellcolor{mypink}50 & \cellcolor{mypink}167k & \cellcolor{mypink}44\% & \cellcolor{mypink}6.4 min \\
Code repo QA & Code repository & 50 & 167k & 44\% & 6.4 min \\
\midrule
\multicolumn{2}{l}{\cellcolor{mypink}\emph{VI. Long Structured Data Understanding}} & \cellcolor{mypink}33 & \cellcolor{mypink}49k & \cellcolor{mypink}73\% & \cellcolor{mypink}6.4 min \\
Table QA & Table & 18 & 42k & 61\% & 7.4 min \\
Knowledge graph reasoning & KG subgraph & 15 & 52k & 87\% & 6.2 min \\
\bottomrule
\end{tabular}
}
\caption{Tasks and data statistics in LongBench v2. `Source' denotes the origin of the context. `Length' is the \emph{median} of the number of words. `Expert Acc' and `Expert Time' refer to the average accuracy and the \emph{median} time spent on answering the question by human experts. 
$^*$: We allow human experts to respond with ``I don't know the answer'' if it takes them more than 15 minutes.
As a result, most expert times are under 15 minutes, but this doesn't necessarily mean that the questions are fully answered within such a time.}
\label{tb:stat}
\end{table*}

\section{LongBench v2: Task and Construction}

Our design principle focuses on four aspects: (1) The context should be sufficiently long to cover scenarios ranging from 8k to 2M words, with a relatively even distribution across texts up to 128k words.
(2) The question should be challenging, requiring the model to deeply understand the context to answer. It should avoid questions that can be answered based on memory or those where the answer can be directly extracted from the context.
(3) The data should cover a wide range of real-world long-context scenarios and reflect the model's holistic ability to reason, apply, and analyze information drawn from the lengthy text.
(4) The data should be in English and in a multiple-choice question format, containing a long text, a question, four choices, a groundtruth answer, and an evidence. Distractors should be included to prevent the model from guessing the correct answer based on option patterns.

\subsection{Task Overview}
Based on the testing scenarios and the types and sources of long texts, we propose six major task categories and further divide them into 20 subtasks.
We introduce the tasks included in LongBench v2 in the following. A list of task statistics and detailed descriptions can be found in Table~\ref{tb:stat} and Appendix~\ref{sec:task}.

\xhdr{Single-Doc QA}
We integrate subtask categories from previous datasets~\cite{bai2024longbench,an2024leval} and expand them to include QA for \emph{academic}, \emph{literary}, \emph{legal}, \emph{financial}, and \emph{governmental} documents. 
Considering that \emph{detective} QA~\cite{xu2024detectiveqa} requires in-depth reasoning based on case background, we introduce such a task that requires identifying the killer or motive based on information provided in detective novels.
We also include \emph{Event ordering}, where the goal is to order minor events according to the timeline of a novel.

\xhdr{Multi-Doc QA}
To distinguish from single-doc QA, multi-doc QA requires answers drawn from multiple provided documents.
Besides the categories in single-doc QA, multi-doc QA also includes \emph{multi-news QA}, which involves reasoning across multiple news articles, events, and timelines.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/pipeline.pdf}
    \caption{Data collection pipeline of LongBench v2. The annotator first uploads the document(s) and proposes a multiple-choice question based on the content. After that, automated and manual reviews will be conducted to ensure the data meets our requirements. Only data that passes these reviews is eligible for annotation rewards, meaning the annotator must revise the data until it passes all review stages. More details are in section~\ref{sec:data_collection}.}
    \label{fig:pipeline}
\end{figure*}

\xhdr{Long In-context Learning}
Learning from a long context, such as acquiring new skills, requires the ability to comprehend and reason based on that context. Hence, we consider it as a major category of tasks.
LongBench v2 includes several key tasks, including \emph{User guide QA}, which answers questions with information learnt from user guides for electronic devices, software, etc.; \emph{New language translation}~\cite{tanzerbenchmark,zhang2024teaching}, which involves learning to translate an unseen language from a vocabulary book; \emph{Many-shot learning}~\cite{agarwal2024many}, which involves learning to label new data from a handful of examples.

\xhdr{Long-dialogue History Understanding}
LLMs, as more intelligent chatbots or agents, require enhanced memory capabilities to handle longer histories. Therefore, we integrate long-dialogue history understanding tasks to test whether LLMs can handle information from long conversation histories.
These tasks are divided into two subtasks based on the source of the conversation history: one involving the history of interactions between multiple LLM agents, i.e., \emph{Agent history QA}~\cite{huang2024far}, and the other involving the dialogue history between a user and an LLM acting as an assistant, i.e., \emph{Dialogue history QA}~\cite{wu2024longmemeval}.

\xhdr{Code Repository Understanding}
Code repository contains long code content, and question answering over a code repository requires understanding and reasoning across multiple files, making it a common yet challenging long-context task.

\xhdr{Long Structured Data Understanding}
In addition to textual data, much information is presented in structured forms, so we introduce the long structured data QA task to test the LLM's understanding of long structured data, including reasoning on long tables, i.e., \emph{Table QA}~\cite{zhang2024tablellm}, and answering complex queries on knowledge graphs (KGs), i.e., \emph{Knowledge graph reasoning}~\cite{cao2022kqa,bai2023answering}.
We anonymize the entities in the KG to prevent the model from directly deriving the answers through memorization.

\subsection{Data Collection}
\label{sec:data_collection}

To collect high-quality and challenging data for long-context tasks, we hire 97 annotators who are either holding or pursuing a bachelor's degree from top universities and are proficient in English, with detailed statistics shown in Appendix~\ref{sec:stat}. We also select 24 professional human experts based on their major and year of study for conducting manual reviews.
Figure~\ref{fig:pipeline} illustrates the overall pipeline of our data collection process, which consists of five steps: document collection, data annotation, automated review, manual review, and data revision (optional). 
We develop an online annotation platform to implement this pipeline, with further details provided in Appendix~\ref{sec:platform}.

\xhdr{Step 1: Document Collection}
Unlike previous benchmarks~\cite{bai2024longbench,an2024leval}, where long documents are pre-defined or synthesized by the benchmark designers, we aim to gather documents that reflect more diverse scenarios and are more likely to be used in everyday contexts. To achieve this, we ask annotators to upload one or multiple files they have personally read or used, such as research papers, textbooks, novels, etc., according to the task type.
Our platform first converts the uploaded files into plain text using tools such as \href{https://github.com/pymupdf/PyMuPDF}{\texttt{PyMuPDF}}.
The input documents then undergo two automatic checks. If the length is less than 8,192 words, it is rejected as too short. Documents with a high overlap with previous annotations are also rejected to ensure diversity.

\xhdr{Step 2: Data Annotation}
During data annotation, the annotator is tasked with proposing a multiple-choice question based on their submitted documents. The question should be accompanied with four choices, a groundtruth answer, and the supporting evidence. We provide the annotators with a detailed question design principle that specifies our requirement (Appendix~\ref{sec:guide}). To summarize, the following types of questions should be avoided:
(1) \emph{Counting questions}: Avoid questions that require counting large numbers.
(2) \emph{Simple retrieval questions}: Do not ask basic information retrieval questions, as these are too easy for modern LLMs~\cite{song2024counting}.
(3) \emph{Overly professional questions}: Questions should not demand extensive external knowledge; they should rely on minimal expertise.
(4) \emph{Tricky questions}: Do not create questions that are deliberately difficult; the goal is to keep the questions natural and straightforward.

\xhdr{Step 3: Automated Review}
Upon submission, each question undergoes an initial automated review process to ensure it is not too easy. 
We employ three fast and powerful LLMs with a 128k context length to answer the questions: GPT-4o-mini~\cite{GPT-4o-mini}, \href{https://open.bigmodel.cn/pricing}{GLM-4-Air}, and \href{https://open.bigmodel.cn/pricing}{GLM-4-Flash}.
Inputs that exceed the context length are truncated from the middle.
If all three LLMs answer the question correctly, it is considered too easy. In such cases, annotators will be required to revise the question and choices to increase its difficulty.

\xhdr{Step 4: Manual Review}
Data passing the automated review is sent to a human expert for manual review. 
Our manual review serves two purposes: first, to filter out unqualified questions and data with incorrect answers; second, to establish a human baseline while also determining the difficulty of the questions and filter out those that are too easy (i.e., questions that humans can answer correctly in a short amount of time).
In practice, the reviewer first goes through a checklist to determine whether the question meets the specified requirements (outlined in Appendix~\ref{sec:guide}). Next, the reviewer downloads the raw document files and attempts to answer the question. The reviewer is encouraged to use searching tools within the files to solve the problem more promptly. Once a choice is submitted, the reviewer can view the groundtruth answer and the evidence provided by the annotators. The reviewer will then decide whether the answer is objective and fully correct. Our platform tracks the time spent on each question, and if the human expert answers correctly within 3 minutes, the question will be considered too easy, demanding a revision from its annotator.
Since answering some questions may require spending several hours reading the material, which implies a significant review time cost, we allow human experts to respond with ``I don't know the answer'' after 15 minutes.

\xhdr{Data Revision}
As mentioned above, questions deemed unqualified during either automated or manual review will require revision by its annotator. We set up a separate page in our platform for annotator to track their rejected data. For each rejected data, we provide the annotator with a reason for the rejection, classified into three categories: (1) \emph{Illegal question}: Rejected by human reviewers due to the question being unqualified, (2) \emph{Insufficient difficulty}: Rejected by automated review or due to human reviewer answering the question correctly within 3 minutes, and (3) \emph{Wrong answer}: Rejected by human reviewers. Based on this feedback, annotators will refine their data until it passes the review process. To avoid wasting too much manual resources on low-quality data, we will terminate the review-revision cycle if the data has been revised more than five times without passing.

\xhdr{Mechanism Design}
To incentivize annotators to provide high-quality, challenging, and longer test data, our reward mechanism is set as follows. First, annotators can receive a base reward of \texttt{100} \texttt{CNY} only if the data passes the review process; no reward is given for data that does not pass. To encourage annotators to provide longer data, we offer additional length rewards of \texttt{20}, \texttt{40}, and \texttt{50} \texttt{CNY} for passed data in the length ranges $(32k, 64k]$, $(64k, 128k]$, and over $128k$, respectively (in word count). To motivate annotators to provide more difficult data, we define \emph{hard} set data as data where at least two out of three models do not answer correctly in automated review and the human reviewer is unable to solve it within 10 minutes; all other data is considered \emph{easy} data. For hard data, annotators can earn an additional difficulty reward of \texttt{50} \texttt{CNY}.
Each human expert is rewarded \texttt{25} \texttt{CNY} for reviewing each piece of data. We also conduct random checks on their reviews, and any human expert whose reviews repeatedly fail these checks will have all of their reviewing rewards revoked.

\subsection{Data Verification}
For a final check, we sample 70 test data and invite our authors to verify their correctness and whether they are Google-proofed~\cite{rein2023gpqa}.

\xhdr{Correctness} 
Check the selected answer based on the provided evidence to determine if it is correct, with all other options being incorrect. An answer is also deemed incorrect if there is any controversy, ambiguity, or reliance on subjective judgment.

\xhdr{Google-proof} Search for the answer to the question on the internet (Google). The data is considered Google-proof if the answer cannot be found within 15 minutes of searching.

Through our verification, we find that \texttt{68/70} of the data are completely correct, and \texttt{67/70} are Google-proofed. Therefore, we estimate that the error rate of our data is around 3\%, and the majority of the questions cannot be answered by memorizing existing data on the internet.
We review all the data to ensure that it does not contain any sensitive information related to privacy or copyrights.

\subsection{Data Statistics}
We categorize the 503 data entries in Longbench v2 based on their difficulty, length, and task types. According to the difficulty criteria defined in the previous section, 192 are classified as ``Easy'', while 311 are deemed ``Hard''. Based on word count, the data is divided into three groups: ``Short'' ($<$32k), ``Medium'' (32k-128k), and ``Long'' ($>$128k), containing 180, 215, and 108 entries, respectively, exhibiting a relatively balanced distribution.
For the data distribution across task types, please see Table~\ref{tb:stat}.
Also, the questions with answers A, B, C, and D account for approximately 19\%, 25\%, 30\%, and 26\% of the total, respectively, showing that the distribution of answers across the four options is relatively even.
We also analyze the proportion of data submissions rejected during manual review and find that 4\% of the submissions are rejected for \emph{illegal question}; 7\% are rejected for \emph{insufficient difficulty}; and 4\% are rejected for \emph{wrong answer}.
\section{Evaluation}

\subsection{Baselines}

\xhdr{Setup}
We evaluate 10 open-source LLMs, all of which have a context window size of 128,000 tokens, along with 7 proprietary LLMs.
We apply middle truncation as described in~\citet{bai2024longbench} for sequences exceeding the model's context window length.
Given the complex reasoning required by our test data, we adopt two evaluation settings: zero-shot and zero-shot + CoT. Following~\citet{rein2023gpqa}, in the CoT setting, the model is first prompted to generate a chain of thought~\cite{wei2022chain}, after which it is asked to produce the final answer based on the chain of thought.
For details on reproducing our results, please refer to Appendix~\ref{sec:setup}. 
For a fair comparison, the Qwen2.5 series models are evaluated without YaRN~\cite{peng2024yarn}. Their performance when combining YaRN are provided in Table~\ref{tb:exp_yarn}.
The code is available at \url{https://github.com/THUDM/LongBench}.

\begin{table*}[t]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{p{5.7cm}|m{0.7cm}m{0.7cm}|m{0.7cm}m{0.7cm}|m{0.7cm}m{0.7cm}|m{0.7cm}m{0.7cm}|m{0.7cm}m{0.7cm}|m{0.7cm}m{0.7cm}}
\toprule
 &  & & \multicolumn{4}{|c}{\textbf{Difficulty}} & \multicolumn{6}{|c}{\textbf{Length (<32k; 32k-128k; >128k)$^\diamond$}} \\
\cmidrule(r){1-3} \cmidrule(lr){4-7} \cmidrule(l){8-13}
\textbf{Model} & \multicolumn{2}{c|}{\textbf{Overall}} & \multicolumn{2}{c|}{\textbf{Easy}} & \multicolumn{2}{c|}{\textbf{Hard}} & \multicolumn{2}{c|}{\textbf{Short}} & \multicolumn{2}{c|}{\textbf{Medium}} & \multicolumn{2}{c}{\textbf{Long}} \\ 
\midrule
\multicolumn{13}{l}{\emph{Open-source models}} \\
\texttt{GLM-4-9B-Chat} & 30.2 & \cellcolor{mygray}30.8 & 30.7 & \cellcolor{mygray}34.4 & 29.9 & \cellcolor{mygray}28.6 & 33.9 & \cellcolor{mygray}35.0 & 29.8 & \cellcolor{mygray}30.2 & 25.0 & \cellcolor{mygray}25.0 \\
\texttt{Llama-3.1-8B-Instruct} & 30.0 & \cellcolor{mygray}30.4 & 30.7 & \cellcolor{mygray}36.5 & 29.6 & \cellcolor{mygray}26.7 & 35.0 & \cellcolor{mygray}34.4 & 27.9 & \cellcolor{mygray}31.6 & 25.9 & \cellcolor{mygray}21.3 \\
\texttt{Llama-3.1-70B-Instruct} & 31.6 & \cellcolor{mygray}36.2 & 32.3 & \cellcolor{mygray}35.9 & 31.2 & \cellcolor{mygray}36.3 & 41.1 & \cellcolor{mygray}45.0 & 27.4 & \cellcolor{mygray}34.0 & 24.1 & \cellcolor{mygray}25.9 \\
\texttt{Llama-3.3-70B-Instruct} & 29.8 & \cellcolor{mygray}36.2 & 34.4 & \cellcolor{mygray}38.0 & 27.0 & \cellcolor{mygray}35.0 & 36.7 & \cellcolor{mygray}45.0 & 27.0 & \cellcolor{mygray}33.0 & 24.1 & \cellcolor{mygray}27.8 \\
\texttt{Llama-3.1-Nemotron-70B-Inst.} & 31.0 & \cellcolor{mygray}35.2 & 32.8 & \cellcolor{mygray}37.0 & 29.9 & \cellcolor{mygray}34.1 & 38.3 & \cellcolor{mygray}46.7 & 27.9 & \cellcolor{mygray}29.8 & 25.0 & \cellcolor{mygray}26.9 \\
\texttt{Qwen2.5-7B-Instruct} & 27.0 & \cellcolor{mygray}29.8 & 29.2 & \cellcolor{mygray}30.7 & 25.7 & \cellcolor{mygray}29.3 & 36.1 & \cellcolor{mygray}35.6 & 23.7 & \cellcolor{mygray}26.5 & 18.5 & \cellcolor{mygray}26.9 \\
\texttt{Qwen2.5-72B-Instruct} & \textbf{39.4} & \cellcolor{mygray}38.8 & \textbf{43.8} & \cellcolor{mygray}42.2 & \textbf{36.7} & \cellcolor{mygray}\textbf{36.7} & \textbf{44.4} & \cellcolor{mygray}\textbf{50.0} & \textbf{34.0} & \cellcolor{mygray}28.8 & \textbf{41.7} & \cellcolor{mygray}\textbf{39.8} \\
\texttt{Mistral-Large-Instruct-2407} & 26.6 & \cellcolor{mygray}33.6 & 29.7 & \cellcolor{mygray}34.4 & 24.8 & \cellcolor{mygray}33.1 & 37.8 & \cellcolor{mygray}41.1 & 19.5 & \cellcolor{mygray}31.2 & 22.2 & \cellcolor{mygray}25.9 \\
\texttt{Mistral-Large-Instruct-2411} & 34.4 & \cellcolor{mygray}\textbf{39.6} & 38.0 & \cellcolor{mygray}\textbf{43.8} & 32.2 & \cellcolor{mygray}37.0 & 41.7 & \cellcolor{mygray}46.1 & 30.7 & \cellcolor{mygray}\textbf{34.9} & 29.6 & \cellcolor{mygray}38.0 \\
\texttt{c4ai-command-r-plus-08-2024} & 27.8 & \cellcolor{mygray}31.6 & 30.2 & \cellcolor{mygray}34.4 & 26.4 & \cellcolor{mygray}29.9 & 36.7 & \cellcolor{mygray}39.4 & 23.7 & \cellcolor{mygray}24.2 & 21.3 & \cellcolor{mygray}33.3 \\ 
\midrule
\multicolumn{13}{l}{\emph{Proprietary models}} \\
\texttt{GLM-4-Plus} & 44.3 & \cellcolor{mygray}46.1 & 47.4 & \cellcolor{mygray}52.1 & 42.4 & \cellcolor{mygray}42.4 & 50.0 & \cellcolor{mygray}53.3 & 46.5 & \cellcolor{mygray}44.7 & 30.6 & \cellcolor{mygray}37.0 \\
\texttt{GPT-4o-mini-2024-07-18} & 29.3 & \cellcolor{mygray}32.4 & 31.1 & \cellcolor{mygray}32.6 & 28.2 & \cellcolor{mygray}32.2 & 31.8 & \cellcolor{mygray}34.8 & 28.6 & \cellcolor{mygray}31.6 & 26.2 & \cellcolor{mygray}29.9 \\
\texttt{GPT-4o-2024-08-06} & 50.1 & \cellcolor{mygray}51.2 & 57.4 & \cellcolor{mygray}57.9 & 45.6 & \cellcolor{mygray}47.1 & 53.3 & \cellcolor{mygray}53.9 & 52.4 & \cellcolor{mygray}\textbf{50.7} & 40.2 & \cellcolor{mygray}47.7 \\
\texttt{GPT-4o-2024-11-20} & 46.0 & \cellcolor{mygray}51.4 & 50.8 & \cellcolor{mygray}54.2 & 43.0 & \cellcolor{mygray}49.7 & 47.5 & \cellcolor{mygray}59.6 & 47.9 & \cellcolor{mygray}48.6 & 39.8 & \cellcolor{mygray}43.5 \\
\texttt{o1-mini-2024-09-12} & 37.8 & \cellcolor{mygray}38.9 & 38.9 & \cellcolor{mygray}42.6 & 37.1 & \cellcolor{mygray}36.6 & 48.6 & \cellcolor{mygray}48.9 & 33.3 & \cellcolor{mygray}32.9 & 28.6 & \cellcolor{mygray}34.3 \\
\texttt{o1-preview-2024-09-12} & \textbf{57.7} & \cellcolor{mygray}\textbf{56.2} & \textbf{66.8} & \cellcolor{mygray}\textbf{58.9} & \textbf{52.1} & \cellcolor{mygray}\textbf{54.6} & \textbf{62.6} & \cellcolor{mygray}\textbf{64.6} & \textbf{53.5} & \cellcolor{mygray}50.2 & \textbf{58.1} & \cellcolor{mygray}\textbf{54.3} \\
\texttt{Claude-3.5-Sonnet-20241022} & 41.0 & \cellcolor{mygray}46.7 & 46.9 & \cellcolor{mygray}55.2 & 37.3 & \cellcolor{mygray}41.5 & 46.1 & \cellcolor{mygray}53.9 & 38.6 & \cellcolor{mygray}41.9 & 37.0 & \cellcolor{mygray}44.4\\
\midrule
\cellcolor{mypink}\emph{Human$^*$} & \multicolumn{2}{c|}{\cellcolor{mypink}53.7} & \multicolumn{2}{c|}{\cellcolor{mypink}100} & \multicolumn{2}{c|}{\cellcolor{mypink}25.1} & \multicolumn{2}{c|}{\cellcolor{mypink}47.2} & \multicolumn{2}{c|}{\cellcolor{mypink}59.1} & \multicolumn{2}{c}{\cellcolor{mypink}53.7} \\
\bottomrule
\end{tabular}
}
\caption{Evaluation results (\%) on LongBench v2. Results under \colorbox{mygray}{CoT} prompting are highlighted with a gray background. Note that random guessing yields a baseline score of 25\%. To account for model responses and human responses that do not yield a valid choice, we report the \emph{compensated} results in Table~\ref{tb:exp_comp}, where these cases are counted towards the accuracy with a random probability of 25\%. $^*$: The human expert's accuracy is based on their performance within a 15-minute time limit, after which they are allowed to respond with ``I don't know the answer''. This occurred for 8\% of the total test data. $^\diamond$: Models do not show lower scores on subsets with longer length ranges because the distribution of tasks differs significantly across each length range (Figure~\ref{fig:length}).}
\label{tb:exp}
\end{table*}

\xhdr{Results}
We report the evaluation results along with human expert performance in Table~\ref{tb:exp}. The results under the CoT evaluation setting are highlighted with a gray background, while the highest scores among open-source models and proprietary models are in bold.
The results indicate that LongBench v2 presents a significant challenge to the current model---The best-performing o1-preview model achieves only 57.7\% accuracy, which is 4\% higher than the performance of human experts under a 15-minute time limit. Additionally, the scaling law effect on our benchmark is striking: smaller models such as GLM-4-9B-Chat, Qwen2.5-7B-Instruct, and GPT-4o-mini perform poorly in our tests that require deep understanding and reasoning over long contexts, with accuracy around 30\%. In contrast, their larger counterparts like GLM-4-Plus, Qwen2.5-72B-Instruct, and GPT-4o show a notable improvement, achieving overall accuracy around or above 40\%.
Similar to reasoning tasks in mathematics and coding~\cite{wei2022chain,sprague2024cot,o1-preview}, we also find that incorporating explicit reasoning in the modelâ€™s responses significantly improves its performance in our long-context reasoning tests.
This includes the use of CoT, which results in an average 3.4\% improvement for open-source models. Additionally, scaling test-time compute with longer reasoning thought shows further improvements, with o1-preview vs. GPT-4o (+7.6\%) and o1-mini vs. GPT-4o-mini (+8.5\%).
From the performance across different length intervals, compared to human, the models perform best on data $<$32k (Short), with the best-performing model surpassing human performance by 15.4\%. However, even the top model shows a 5.6\% performance gap compared to human accuracy in the 32k-128k data length range. This highlights the importance of developing methods to maintain strong reasoning capabilities under longer contexts.

To better distinguish the capability of the models across tasks, we present the performance charts of several representative models across tasks in Figure~\ref{fig:radar}.
We find that the performance gap between LLMs and humans is largest on long structured data understanding tasks, whereas, on single-doc and multi-doc QA tasks, the models perform at par with or even surpass human levels.
We hypothesize that this is because the models have seen much more document-type data compared to long structured data during long context training, resulting in poorer understanding of the latter.
Compared to GPT-4o, we observe that through integrating more thinking steps during inference, o1-preview shows superior performance on multi-doc QA, long in-context learning, and code repository understanding tasks, with a substantial lead over other models.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/radar.pdf}
    \caption{Average scores across tasks, normalized by the highest score on each task. All scores are evaluated in the zero-shot + CoT setting, except for o1-preview, since it latently performs CoT under zero-shot prompting.}
    \label{fig:radar}
\end{figure}

\subsection{Retrieval-Augmented Baselines}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/curve.pdf}
    \caption{RAG performance across different context lengths, varied by including the top 4, 8, 16, 32, 64, 128, and 256 chunks of 512 tokens. The horizontal line show the overall score of each model without RAG at a full context length of 128k tokens.}
    \label{fig:rag}
\end{figure}

Based on recent studies~\cite{jiang2024longrag,jin2024long,leng2024long}, we explore incorporating retrieval-augmented generation (RAG,~\citet{lewis2020retrieval}) into long-context LLM and evaluate its performance on LongBench v2.
We first split the long context into chunks of 512 tokens with GLM-4-9B tokenizer. Then, we use \href{https://open.bigmodel.cn/pricing}{Zhipu Embedding-3} to encode the query, i.e., the concatenation of the question and choices, and the chunks, and sort the chunks based on embedding similarity.
During evaluation, we retrieve the top-$N$ most similar chunks and concatenate them in their original order to form the context input for the model. The model is then prompted to answer the question in a zero-shot setting. For each evaluated model, we take $N = 4, 8, 16, 32, 64, 128, \text{and}\ 256$, and the evaluation results form a curve presented in Figure~\ref{fig:rag}.

We observe that Qwen2.5 and GLM-4-Plus show no significant improvement as the retrieval context length increases beyond 32k. Both models perform better at a 32k retrieval context length compared to using the entire 128k context window without RAG, with Qwen2.5 showing a notable improvement of +4.1\%. 
In contrast, only GPT-4o effectively leverages longer retrieval context lengths, achieving the best RAG performance at 128k, while still lagging behind its overall score without RAG (-0.6\%).
These findings suggest that Qwen2.5 and GLM-4-Plus fall short in effectively utilizing and reasoning with information in context windows longer than 32k compared to GPT-4o.
In addition, these experiments also confirm that the questions in LongBench v2 are challenging and cannot be solved solely through retrieval.

\subsection{Measuring Memorization of Context}

\begin{table}[t]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lccccccc}
\toprule
\textbf{Model} & \textbf{Avg} & \textbf{I} & \textbf{II} & \textbf{III} & \textbf{IV} & \textbf{V} & \textbf{VI} \\
\midrule
\cellcolor{mygray}\texttt{GLM-4-9B-Chat} & \cellcolor{mygray}30.2 & \cellcolor{mygray}30.9 & \cellcolor{mygray}27.2 & \cellcolor{mygray}33.3 & \cellcolor{mygray}38.5 & \cellcolor{mygray}28.0 & \cellcolor{mygray}24.2 \\
\quad w/o context & 26.2 & 30.9 & 21.6 & 18.5 & 30.8 & 34.0 & 21.2 \\
\midrule
\cellcolor{mygray}\texttt{Llama-3.1-8B-Inst.} & \cellcolor{mygray}30.0 & \cellcolor{mygray}34.9 & \cellcolor{mygray}30.4 & \cellcolor{mygray}23.5 & \cellcolor{mygray}17.9 & \cellcolor{mygray}32.0 & \cellcolor{mygray}30.3 \\
\quad w/o context & 25.8 & 31.4 & 26.4 & 24.7 & 23.1 & 22.0 & 6.1 \\
\midrule
\cellcolor{mygray}\texttt{Qwen2.5-72B-Inst.} & \cellcolor{mygray}39.4 & \cellcolor{mygray}40.6 & \cellcolor{mygray}35.2 & \cellcolor{mygray}42.0 & \cellcolor{mygray}25.6 & \cellcolor{mygray}50.0 & \cellcolor{mygray}42.4 \\
\quad w/o context & 30.0 & 33.7 & 31.2 & 25.9 & 28.2 & 34.0 & 12.1 \\
\midrule
\cellcolor{mygray}\texttt{GLM-4-Plus} & \cellcolor{mygray}44.3 & \cellcolor{mygray}41.7 & \cellcolor{mygray}42.4 & \cellcolor{mygray}46.9 & \cellcolor{mygray}51.3 & \cellcolor{mygray}46.0 & \cellcolor{mygray}48.5 \\
\quad w/o context & 27.6 & 33.7 & 27.2 & 25.9 & 10.3 & 38.0 & 6.1 \\
\midrule
\cellcolor{mygray}\texttt{GPT-4o} & \cellcolor{mygray}50.1 & \cellcolor{mygray}48.6 & \cellcolor{mygray}44.0 & \cellcolor{mygray}58.0 & \cellcolor{mygray}46.2 & \cellcolor{mygray}56.0 & \cellcolor{mygray}51.5 \\
\quad w/o context & 33.1 & 40.0 & 25.6 & 32.1 & 38.5 & 34.0 & 18.2 \\
\bottomrule
\end{tabular}
}
\caption{Scores (\%) across 6 tasks: \emph{I. Single-Doc QA}, \emph{II. Multi-Doc QA}, \emph{III. Long ICL}, \emph{IV. Dialogue History}, \emph{V. Code Repo}, and \emph{VI. Structured Data}.}
\label{tb:mem}
\end{table}

For an effective long-context benchmark, it is essential to ensure that LLMs cannot rely solely on memorizing previously seen data to answer questions. 
This necessitates the models to actively read and comprehend the provided long material in order to solve the problems.
Following~\citet{bai2024longbench}, we also evaluate the models' performance when providing only the questions, without the accompanying long context.
The performance comparison between with (w/) and without (w/o) the context is presented in Table~\ref{tb:mem}.
As shown, without context, most models achieve an overall accuracy ranging from 25\% to 30\%, which is comparable to random guessing. When comparing scores across different tasks, the memorization effect appears minimal for tasks II, III, and VI.
The models perform best without context on tasks I and V, likely because they may have seen some of the documents, novels, or code repositories during training.
\section{Conclusion}
Our work introduces LongBench v2, a challenging multitask benchmark for long-context understanding and reasoning, carefully annotated and reviewed by human experts.
LongBench v2 presents an equal challenge to both humans and state-of-the-art AI systems, with human performance at 50.1\% and the best LLM achieving 57.7\% accuracy, providing a reliable evaluation standard for the development of future superhuman AI systems.
Our evaluation results also bring forward insights into the impact of scaling inference-time compute and RAG in long-context reasoning.
\section{Limitations}
We acknowledge certain limitations in our work, which we outline below:
1. \textbf{Benchmark size}: The benchmark's size may not be sufficiently large. While this can be seen as an advantage for quick evaluation, it could also lead to less stable results that are more vulnerable to randomness. Due to resource constraints, we are unable to expand the dataset at this time. Collecting the current 503 high-quality samples cost us 100,000 CNY and took more than two months.
2. \textbf{Language}: The current dataset is limited to English only. As a result, our benchmark does not yet capture the performance of models across multiple languages.
3. \textbf{Length distribution inconsistencies}: The length distribution across different tasks is uneven, with certain tasks concentrated around specific lengths. These differences in task distributions across length ranges make it difficult to provide a fair comparison of a single model's performance across length intervals. We recommend conducting comparisons between models on a per-interval basis. For instance, model A may outperform Model B in the short length range, while model B may outperform model A in the long length range. This would suggest that model B is better at handling longer tasks than model A.

\section*{Acknowledgements}
We would like to express our gratitude to our annotation workers for their dedicated contributions. The authors also extend their thanks to Zijun Yao for his assistance in maintaining the platform, and to Yuze He for his valuable suggestions on the paper.

\newpage

\end{document}