\title{Length Generalization of Causal Transformers without Position Encoding}

\begin{document}

\maketitle
\begin{abstract}

Generalizing to longer sentences
is important for recent Transformer-based language models.
Besides algorithms manipulating explicit position features,
the success of 
Transformers without position encodings (NoPE)
provides a new way to overcome the challenge.
In this paper, we study the length generalization property 
of NoPE.
We find that although NoPE can extend to longer sequences 
than the commonly used explicit position encodings, 
it still has a limited context length.
We identify a connection between the failure of NoPE’s 
generalization and the distraction of attention distributions.
We propose a parameter-efficient tuning for 
searching attention heads' best temperature hyper-parameters, 
which substantially expands NoPE's context size.
Experiments on long sequence language modeling, the synthetic
passkey retrieval task and real-world long context tasks show 
that NoPE can achieve competitive performances with state-of-the-art 
length generalization algorithms.
The source code is publicly 
accessible\footnote{$\ \ $\url{https://github.com/AntNLP/nope_head_scale}}.

\section{Introduction}
\label{sec:intro}

Causal Transformer has been widely applied 
in modern language models. 
To help models recognize the correct ordering of words, 
it is common to configure Transformers
with \emph{explicit} position encodings
(e.g., the sinusoidal embeddings in the original
development of Transformer \cite{NIPS2017_3f5ee243}, 
the relative position encoding in T5 \cite{JMLR:v21:20-074},
and the rotary position encoding in GPT series \cite{DBLP:journals/corr/abs-2104-09864}).
The setup of position features provides
flexibility to include prior knowledge structure on 
describing distance,
but it also brings the problem of \emph{length generalization}:
language models trained with in-domain position features
can not handle longer sentences 
(i.e., those with out-of-domain position features)
in testing time.
Generalizing to unseen sentence length
is crucial in many language model applications
like retrieval augmented language models \cite{DBLP:journals/jmlr/IzacardLLHPSDJRG23}, 
personalized language models \cite{wang2023rolellm},
language-model-based agents \cite{10.1145/3586183.3606763}.

Departing from the standard ways of encoding positions,
one may ask (following the principle of parsimony) that 
are the explicit position features necessary?
The answer is no.
Both empirically \cite{haviv-etal-2022-transformer} and theoretically \cite{chi-etal-2023-latent,kazemnejad2023the},
the casually masked Transformers are shown to be able to
successfully model languages
without any prior position encoding (\textbf{NoPE}).
The finding calls for a deeper understanding of
\emph{implicit} position information in 
Transformer-based language models,
and also inspires a new direction for length generalization:
\emph{without explicit position features, can NoPE generalize?}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{fig/fig_vis_entro_.pdf}
  \caption{Length generalization from $2$K to $4$K. 
  For different testing lengths (or, positions of sequences), 
  dashed lines draw the log-perplexity of models
  (measured on validation set of the pre-training dataset),
  and solid lines represent the entropy of attention heads
  (averaged on all heads).}
  \label{fig:vis_entro}
\end{figure}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.49\linewidth]{fig/fig_vis_uni_scale_entro_nope.pdf}
  \includegraphics[width=0.49\linewidth]{fig/fig_vis_uni_scale_entro_rope.pdf}
  \caption{\textbf{UniformScale} modifies the temperature hyper-parameter of the $\mathrm{SoftMax}$ operator in self-attention layers (Left, NoPE; Right, RoPE). NoPE can generalize to longer context by merely scaling the softmax scores. However, this exact technique does not directly apply to RoPE models.}
  \label{fig:vis_uni_entro}
\end{figure*}

In this paper, we study the length generalization property of NoPE.
Our main findings are,

\begin{itemize}[leftmargin=*]
    \item When extending to unseen sentence length, 
    NoPE has less performance loss.
    However, beyond a certain range, NoPE also fails to extend, 
    with no substantial difference observed when compared to explicit position encodings.
    For example, NoPE can effectively extend the training length by 
    $20\%$ (from $2$K to $2.4$K, Figure \ref{fig:vis_entro}) 
    without a significant increase in perplexity.
    In contrast, the rotary position encoding (RoPE) is only capable of extending by $10\%$. 
    
    \item 
    We analyze the failure cases of NoPE's generalization
    and find that they always co-occur with 
    the distraction of attention distributions:
    the attention heads begin to allocate their weights to tokens evenly when NoPE's extension performance begins to collapse. 
    The connection between NoPE's generalization 
    and concentration of attention heads suggests controlling the behaviors of attention heads during length extension.
    \item We show that by simply searching one temperature hyper-parameter, NoPE's length generalization 
    can be significantly improved. For example, by scaling the attention score by a factor of $1.2$, NoPE can immediately generalize to over $4$K tokens (Figure \ref{fig:vis_entro}).
    \item Moreover, we developed an advanced version of this 
    strategy by searching temperature parameters for each head, in the light that different layers and heads exhibit varied behaviors. 
    The procedure resembles a parameter-efficient fine-tuning,
    with an extremely small number of tunable parameters 
    ($704$ delta parameters over $1$B model parameters).
    We show that the proposed method can
    help NoPE to generalize further (Figure \ref{fig:vis_head_vs_uni}). %(results XXX).
\end{itemize}

We conduct length generalization experiments on 
long sequence language modeling, 
synthetic tasks (passkey retrieval), and 
LongBench.
The results show that NoPE 
enjoys a competitive
extension performances to state-of-the-art 
length generalization methods for explicit
position encodings (e.g., 
PI \cite{chen2023extending}, 
YaRN \cite{peng2024yarn}).

\section{Length Generalization of NoPE}
\label{sec:method}

\subsection{Language Modeling with NoPE}
\label{sec:lm}

Before diving into the length generalization problem, 
we first briefly describe the NoPE models used in this paper.
\footnote{For simplicity, 
we refer NoPE to both the implicit way of encoding positions 
and the language model trained without position encoding.}
Our default NoPE has $1.1$B parameters. It is trained from 
the TinyLlama \cite{zhang2024tinyllama} code base
\footnote{\url{https://github.com/jzhang38/TinyLlama}}, 
with training sequence length $L=2048$ and $50$K steps 
($\approx 100$B tokens). More details can be found in 
Section~\ref{ssec:nope}.

We also include the original TinyLlama model
which uses rotary position encoding (RoPE) for comparison.
By default, both models are trained with identical settings.

\subsection{Length Generalization}
\label{sec:lg}

Given a language model (LM) with pre-trained maximal sequence length $L$, the goal of length generalization is to expand it to length $L^\prime > L$.
Length generalization can be tested in a zero-shot manner 
(``train short, test long'') or with some fine-tuning.

Figure \ref{fig:vis_entro} depicts language modeling 
performances of NoPE (and RoPE).
We can observe that, 
within the pre-training length ($L = 2048$),
NoPE has a similar performance as RoPE,
which agrees with existing works:
casual masking can implicitly encode the positions of a sequence
\citep{haviv-etal-2022-transformer,chi-etal-2023-latent}.

When the testing sequence length exceeds the training length,
we see that 
1) NoPE's length generalization error 
(light blue dashed line, measured with log-perplexity) 
is lower than RoPE (light red dashed line).
2) vanilla NoPE still has an increased perplexity 
than in-domain tests.
Therefore, though it is not a perfect solution, 
removing explicit position encoding can 
effectively reduce the length generalization error.
Next, we will try to find the reason 
for the failure of NoPE's length generalization,
and also develop algorithms for improving it.

\subsection{Extension? Attention!}
\label{ssec:vis_entro}

To analyze NoPE's generalization failure,
we first see that since
explicit position encodings have been dropped, the casual Transformer block is only left with three core modules,
the embedding layer, feed-forward layers, and self-attention layers.
The outputs of the former two modules are independent of their inputs' position in sequence (i.e., no matter which position, they always have the same output).
Therefore, multi-head attention layers become our main target.

We visualize the attention pattern of NoPE at different lengths.
Specifically, 
given a validation set with a size $n$ and
a target position $i$, we define the average attention entropy 
$\widebar{\mathcal{H}}_i$ at position $i$, as
\begin{align}
\widebar{\mathcal{H}}_i &= \frac{1}{n\!\times\!m} \sum_{x,h} \mathcal{H}_i^{(h)}(x) \\
\mathcal{H}_i^{(h)}(x) &= -\sum_{j=1}^{i} \alpha_{ij}^{(h)}(x) \cdot \log \alpha_{ij}^{(h)}(x)
\label{eq:entropy}
\end{align}
where $x$ is a sample, 
$\alpha_{ij}^{(h)}(x)$ is the attention probability of 
token $i$ focusing on token $j$ in the $h$-th attention head
($h\in\{1,2,...,m\}$),
$\mathcal{H}_i^{(h)}(x)$ is the entropy of the attention distribution $\alpha_{ij}^{(h)}(x)$ evaluated at position $i$.

The light solid lines in Figure~\ref{fig:vis_entro} show
the average entropy for NoPE (light blue) and RoPE (light red).
We can observe that,
\textbf{the inflection point of $\widebar{\mathcal{H}}_i$ is highly consistent with the inflection point of perplexity}.
It implies that failed length generalization of NoPE (and RoPE)
might be connected to the distraction of attention:
attention heads begin to allocate attention to more tokens.
To further verify the connection,
we also draw a successful extension algorithm for RoPE 
(RoPE-NTK \cite{bloc97} which interpolates out-of-domain
encodings to in-domain encodings).
Its length generalization loss curve is flat,
while its entropy curve also has no steeply increasing point.

Unlike explicit position encodings, NoPE has no clear target objects
to manipulate, thus it is quite challenging to perform
length generalization without fine-tuning on longer sequences.
However, the strong correlation between length extension 
and attention pattern transition suggests such an object,
the entropy of attention heads.

\subsection{Uniform Attention Scale}
\label{ssec:uni_scale}

We write the general scaled dot-product attention as
\begin{equation}
\alpha_{ij}^{(h)} = \frac{e^{\lambda \bm{q}_i^{(h)} \cdot \bm{k}_j^{(h)}}}{\sum_k e^{\lambda \bm{q}_i^{(h)} \cdot \bm{k}_k^{(h)}}}
\end{equation}
where the scaling factor 
$\lambda$ is the temperature hyper-parameter of the $\mathrm{SoftMax}$ operator.
The prevalent setting is $\lambda = \frac{1}{\sqrt{d}}$.

Based on observations in Section~\ref{ssec:vis_entro}, 
we know that NoPE's failure of length generalization 
might be correlated with distracted attention, 
hence we can try to gradually increase the scale factor 
$\lambda$ to reconcentrate attention,
and see whether the generalization error can be reduced.
Figure~\ref{fig:vis_uni_entro} 
visualizes the average entropy under different scale values and the corresponding perplexity curves.

We first find that when increasing the scale factor 
during length generalization evaluation
(e.g., the pre-training scale $\lambda\!=\!\frac{1}{\sqrt{d}}$ 
is increased to $\lambda\!=\!\frac{1.2}{\sqrt{d}}$),
the inflection points of entropy curves are shifted to
longer lengths, at the same time, 
NoPE all generalize to further positions ($L \text{=2k}\to L^\prime\text{=4k}$). 
That is, with all NoPE's parameters frozen and 
only \emph{uniformly} increasing the softmax's temperature, 
NoPE can successfully generalize to unseen lengths.

The same conclusion doesn't hold for RoPE
(Figure~\ref{fig:vis_uni_entro} Right):
no matter what value the scale takes (from $\lambda$=0.8 to $\lambda$=1.4), 
the inflection points of entropy curves remain almost unchanged,
meaning that it fails to generalize to longer lengths.
On the other side, successful RoPE extension algorithms
(e.g., RoPE-NTK in Figure \ref{fig:vis_entro})
can control the distraction of entropy by
explicitly manipulate position encodings.
Therefore, though attention scaling has been used for RoPE
\cite{kexuefm8823,chiang2022overcoming},
it may contribute marginally to RoPE's generation.

We also find that 
extending NoPE to more distant positions generally requires a larger scale (i.e., a more concentrated attention distribution).
As the position becomes further, the number of tokens involved in the attention calculation increases, the attention is more easily scattered, 
and therefore, a larger scaling factor is needed to concentrate the attention.
In particular, for our NoPE model, generalizing to twice the pre-training length requires about 1.2 times the scale, four times the length requires about 1.5 times the scale, and eight times the length requires about 1.8 times the scale.
Appendix~\ref{app:fit_func} reports the fitted function of the scaling factor with respect to the generalization length $L'$.

Finally, we note remark that the attention scaling factor in this section takes the \emph{same} value for all positions, including the pre-training length (\emph{uniform} scaling). 
We experimented with a piecewise function
which use the original scale within the pre-training positions, and a more concentrated attention scale for the extrapolated positions. 
We also try position-dependent functions, 
where the scale increases with position. 
However, none of these methods could further improve generalization. 
We speculate that if the attention at earlier positions is not highly concentrated, the learned token representations may hinder the concentration of attention at latter positions. 
We leave a deeper discussion and analysis of this observation in future work.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{fig/fig_vis_head_entro_.pdf}
  \caption{The attention entropy across all heads for the original NoPE, head-based scaled NoPE and uniform-scaled NoPE, with each model represented in a separate row. The attention heads exhibit divergent patterns.}
  \label{fig:vis_head_entro}
\end{figure}

\section{Head-based Attention Scale}
\label{sec:head_scale}

After verifying that the attention scaling can 
help NoPE generalizing,
we delved deeper into the multi-head attention mechanism and posed a new question, ``\emph{Does each attention head require a unique scaling factor?}''

In this section, we first visualize the average entropy curves for each head and find that they have different attention patterns. 
Hence we propose to replace the uniform scaling with head-based scaling (from one factor to $22\times 32=704$ factors).
To address the issue of an exploding search space, we efficiently determine the values of scaling factors through automated hyperparameter search, considering both parameter efficiency and data efficiency.
As a result, head-based scaling generalizes better than uniform scaling.
Moreover, correlation analysis shows that within each layer, the smaller the converged entropy (i.e., the more concentrated attention), the larger the required scaling factor to maintain that concentration.

\subsection{Visual Analysis}
\label{ssec:vis_head_scale}

The entropy values span a broad spectrum, with each attention head demonstrating a distinct attention pattern. In Figure~\ref{fig:vis_head_entro}, certain attention heads show a highly concentrated pattern, with entropy values converging to $\approx 1$, while others exhibit a highly dispersed pattern, with entropy values converging to $\approx 10$. 
The full head visualization of Figure~\ref{fig:vis_head_entro} is located in Appendix~\ref{app:vis_all_heads}.

This phenomenon casts doubt on uniform scaling — how can a single scaling factor cater to diverse attention heads? 
Inspired by this, we further propose a head-based scale method.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{fig/fig_vis_head_vs_uni_scale_.pdf}
  \caption{Comparing uniform and head-based scale (denoted as $\lambda^{(h)}$). UniformScale fails eventually as the perplexity increases with longer sequences. HeadScale is capable of handling much longer context by assigning different scale factors to each attention head.}
  \label{fig:vis_head_vs_uni}
\end{figure}
\subsection{Head-based Scale}
\label{ssec:head_scale}

We reformulate the uniform attention scale as head-base attention scales
\begin{equation}
\alpha_{ij}^{(h)} = \frac{e^{\lambda^{(h)} \bm{q}_i^{(h)} \cdot \bm{k}_j^{(h)}}}{\sum_k e^{\lambda^{(h)} \bm{q}_i^{(h)} \cdot \bm{k}_k^{(h)}}}
\label{eq:head_scale}
\end{equation}
where $\lambda^{(h)}$ is a unique attention scaling factor for each head, totaling 704.
Compared to a uniform attention scale, 704 head-based scales make it difficult to determine the optimal values by grid search.
Similar to AutoML \citep{he2021automl}, we model the scales' optimal search as a parameter-efficient fine-tuning task.
Given a NoPE model $\mathcal{M}$ and a set of head-based scales $\{\lambda^{(1)}, \lambda^{(2)}, \dots, \lambda^{(m)}\}$, we fix the model $\mathcal{M}$ and define the head-based scales as trainable parameters $\theta=\{\lambda^{(1)}, \lambda^{(2)}, \dots, \lambda^{(m)}\}$.
We aim to find an optimal set of values $\theta^*=\{\lambda^{*(1)}, \lambda^{*(2)}, \dots, \lambda^{*(m)}\}$, that allows the model $\mathcal{M}{(\theta^*)}$ to successfully extend to the target length $L^\prime$.
To this end, we optimize the language modeling loss function $\mathcal{L}_{\text{LM}}$ on the pre-training dataset $D$ with length $L^\prime$ and size $n^\prime, n^\prime \ll n$.
\begin{equation}
\theta^* = \underset{x \in D}{\text{minimize}} \quad \mathcal{L}_{\text{LM}}\left(\mathcal{M}{(\theta, x)}\right)
\label{eq:head_scale_obj}
\end{equation}
The search process is highly efficient. (1) The number of tunable parameters is extremely small, only 704 delta parameters over 1B model parameters; 2) The amount of training tokens for fine-tuning is extremely small too, only 0.03\% of the pre-training data.

In addition, to ensure that the attention is reconcentrated instead of distracted 
by the scaling factors, we apply a focus constraint
during the optimization of Equation~\ref{eq:head_scale_obj} 
\begin{equation}
\lambda^{(*)} \ge \frac{1}{\sqrt d}
\label{eq:concentration}
\end{equation}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{fig/fig_vis_corr_.pdf}
  \caption{Correlation analysis for head-based scale when extended to 8K context. The analysis was conducted on the converged entropy values at 8K position, in relation to the scale searched. Each data point represents a unique attention head.}
  \label{fig:vis_corr}
\end{figure}
\paragraph{Initializing HeadScale}
In practice, we found that the initial value of head-based scales has a significant impact on the search of $\theta^*$.
An obvious approach is to use the default value $\lambda^{(*)}\!=\!\frac{1}{\sqrt{d}}$ from the pre-training phase. 
However, its length generalization results are quite unstable, with most being subpar,
as the optimal scale often deviates significantly from the default value.
We propose another approach to utilize the best uniform scale from the grid search as the initial value.
The ablation study for the initialization approach is in Section~\ref{ssec:ablation}.

Figure~\ref{fig:vis_head_vs_uni} compares the two generalization methods of NoPE, uniform scale versus head-based scales.
Head-based scale exhibits better generalization than the uniform scale, achieving a lower log-PPL by 0.2 at 4K positions ($2\!\times\!L$) and by 0.8 at 8K positions ($4\!\times\!L$).
The average entropy $\widebar{\mathcal{H}}_i$ of the head-based scale is higher than that of the uniform scale, suggesting that the uniform scale method over-concentrates attention, particularly for some heads that inherently have more distracted patterns.

Figure~\ref{fig:vis_corr} shows the correlation between the converged entropy and the searched scale.
To save space, we uniformly sampled 7 layers and all their respective heads. 
We observed that the correlation is layer-dependent, within each layer, heads with more concentrated attention (i.e., lower entropy) searched for larger scales, while heads with more dispersed attention (i.e., higher entropy) searched for smaller scales.
The result is as expected, the more concentrated the attention pattern, the larger the scaling factor needed to maintain its focus.
Furthermore, we observed that attention heads in lower layers are generally more dispersed, whereas heads in higher layers are generally more concentrated (note that this is not strictly observed).

\section{Experiment}
\label{sec:exper}

\begin{table*}[t]
    \centering
    \small
        \begin{tabular}{lccccccccc}
            \toprule
            \textbf{Model} & \textbf{Avg.} & \textbf{arc\_challenge} & \textbf{arc\_easy} & \textbf{boolq} & \textbf{hellaswag} & \textbf{openbookqa} & \textbf{piqa} & \textbf{winogrande}\\
            \midrule
            RoPE & 46.1 & \textbf{24.3} & \textbf{44.9} & \textbf{59.7} & \textbf{43.5} & 29.8 & 67.3 & \textbf{53.3} \\ 
            NoPE & \textbf{46.2} & 24.0 & \textbf{44.9} & 58.1 & 43.4 & \textbf{31.8} & \textbf{68.4} & 52.9 \\
            \bottomrule
        \end{tabular}
    \caption{Commonsense reasoning ability of the pre-trained base models. }
    \label{tab:cs}
\end{table*}
We train a NoPE base model from scratch and investigate its capability in length generalization. We conduct length generalization experiments on long sequence language modeling, synthetic tasks (passkey retrieval), and real-world long context tasks (LongBench). Detailed experiment setup can be found in Appendix~\ref{app:setup}.

\begin{table*}[t]
    \centering
    \small
    \begin{tabular}{lrrrrrrrrrr}
    \toprule
        \multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{FT}}   & \multicolumn{4}{c}{\textbf{PG19}} & \multicolumn{4}{c}{\textbf{Proof-pile}}\\
        \cmidrule(lr){2-3} \cmidrule(lr){4-7} \cmidrule(lr){8-11}
        & \bf $L^{\prime}$ & \bf Tokens & \textbf{2K} & \textbf{4K} & \textbf{8K} & \textbf{16K} & \textbf{2K} & \textbf{4K} & \textbf{8K} & \textbf{16K} \\ 
        \midrule
        \rowcolor{gray!10} \multicolumn{11}{c}{\textit{\textbf{Original LMs}}} \\
        RoPE & - & - & 14.5 & 491.4 & 488.5 & 599.5 & 3.5 & 303.0 & 432.1 & 759.5 \\ 
        NoPE & - & - & 14.6 & 326.9 & $>10^3$ & $>10^3$ & 3.5 & 117.4 & $>10^3$ & $>10^3$ \\
        BLOOM & - & - & 27.7 & 158.0 & 264.6 & 403.4 & 6.9 & 74.1 & 176.2 & 334.5\\
        MPT & - & - & 10.6 & 103.6 & 361.6 & 345.1 & 2.8 & 70.1 & $>10^3$ & $>10^3$ \\
        \arrayrulecolor{gray!20}
        \rowcolor{gray!10} \multicolumn{11}{c}{\textit{\textbf{Generalization for RoPE}}} \\
        NTK$^{\text{zero}}$ & - & - & 14.5 & 14.9 & 22.8 & 80.4 & 3.5 & 3.3 & 4.1 & 13.3 \\ 
        YaRN$^{\text{zero}}$ & - & - & 14.5 & 14.5 & 15.0 & 17.1 & 3.5 & 3.3 & 3.2 & 3.6 \\ 
        \hline
        \multicolumn{1}{l}{\multirow{3}{*}{PI$^{\text{fair}}$}}  & 4K & 6M & 16.0 & 15.9 & 551.9 & $>10^3$ & 3.8 & 3.4 & 307.9 & 633.8 \\ 
        \multicolumn{1}{l}{} & 8K & 13M & 17.4 & 17.1 & 17.1 & 752.8 & 4.0 & 3.6 & 3.4 & 406.3 \\ 
        \multicolumn{1}{l}{} & 16K & 30M & 18.7 & 18.4 & 18.3 & 18.2 & 4.3 & 3.9 & 3.6 & 3.6 \\ 
       
        \hline
        \multicolumn{1}{l}{\multirow{3}{*}{YaRN$^{\text{fair}}$}}  & 4K & 6M & 15.5 & 15.4 & 545.2 & $>10^3$ & 3.7 & 3.4 & 351.5 & 698.2 \\ 
        \multicolumn{1}{l}{} & 8K & 13M & 15.7 & 15.4 & 15.5 & 794.6 & 3.8 & 3.4 & 3.2 & 492.8 \\ 
        \multicolumn{1}{l}{} & 16K & 30M & 15.9 & 15.6 & 15.4 & 15.5 & 3.8 & 3.5 & 3.2 & 3.2 \\ 
        \hline
        \multicolumn{1}{l}{\multirow{3}{*}{PI$^{\text{raw}}$}} & 4K & 33M & 15.2 & 15.0 & 623.8 & 951.7 & 3.6 & 3.3 & 334.4 & 595.5 \\ 
        \multicolumn{1}{l}{} & 8K & 66M & 15.4 & 15.1 & 15.0 & 909.6 & 3.6 & 3.3 & 3.0 & 463.0 \\ 
        \multicolumn{1}{l}{} & 16K & 131M & 15.6 & 15.3 & 15.0 & 14.9 & 3.7 & 3.3 & 3.0 & 3.0 \\ 
        \hline
        \multicolumn{1}{l}{\multirow{3}{*}{YaRN$^{\text{raw}}$}} & 4K  & 33M & 15.1 & 15.0 & 573.3 & 951.4 & 3.6 & 3.3 & 358.8 & 656.8 \\ 
        \multicolumn{1}{l}{}& 8K  & 66M & 15.1 & 14.8 & 14.8 & 816.0 & 3.6 & 3.3 & 3.1 & 501.5 \\ 
        \multicolumn{1}{l}{}& 16K  & 131M & 15.0 & 14.8 & 14.5 & 14.5 & 3.6 & 3.3 & 3.0 & 3.0 \\ 
        
        \rowcolor{gray!10} \multicolumn{11}{c}{\textit{\textbf{Generalization for NoPE}}} \\
        $\lambda\!=\!\frac{1.2}{\sqrt{d}}$  & - & -  & 15.0 & 16.0 & 513.7 & $>10^3$ & 3.6 & 3.3 & 175.3 & $>10^3$ \\ 
        $\lambda\!=\!\frac{1.5}{\sqrt{d}}$  & - & -  & 19.0 & 20.2 & 45.3 & 224.1 & 3.9 & 3.7 & 4.9 & 99.2 \\ 
        $\lambda\!=\!\frac{1.8}{\sqrt{d}}$  & - & -  & 30.4 & 42.4 & 69.1 & 198.8 & 5.1 & 5.6 & 8.5 & 38.2 \\ 
        \hline
        \multicolumn{1}{l}{\multirow{3}{*}{$\lambda^{(h)}$}} & 4K  & 6M & 14.8 & 15.3 & 404.5 & $>10^3$ & 3.5 & 3.2 & 153.4 & $>10^3$ \\ 
        \multicolumn{1}{l}{} & 8K & 13M & 15.7 & 15.3 & 21.1 & 721.7 & 3.6 & 3.3 & 3.2 & 318.5 \\ 
        \multicolumn{1}{l}{} & 18K & 30M & 18.3 & 19.0 & 18.8 & 30.4 & 4.0 & 3.7 & 3.3 & 4.1 \\ 
        \arrayrulecolor{black}
        \bottomrule
        \vspace{-0.5cm}
    \end{tabular}
    \caption{Sliding window perplexity of different context window extension methods tested on PG19 and ProofPile. The ``fair'' and ``raw'' versions of PI and YaRN differ from the training data, as detailed in Appendix~\ref{app:setup}. The notation $\lambda=*$ denotes uniform attention scale by the given number, and $\lambda^{(h)}$ represents head-based scale. }
    \label{tab:ppl}
\end{table*}
\subsection{NoPE pre-trained model}
\label{ssec:nope}

For a fair comparison with RoPE, we train a NoPE model with $1.1$B parameters from the TinyLlama \cite{zhang2024tinyllama} code base\footnote{\url{https://github.com/jzhang38/TinyLlama}}. The NoPE model has $22$ layers of Transformer blocks, $32$ attention heads per layer, $2048$ embedding size. The model is trained on Slimpajama \cite{cerebras2023slimpajama} joint with Starcoderdata \cite{li2023starcoder} by $50$K steps ($\approx 100$B tokens) with sequence length $L=2048$. 

All settings are kept identical to those of TinyLlama, including the model architecture, training data, training procedure, and hyper-parameters, except that the rotary position embedding (RoPE) in TinyLlama is removed, making it a NoPE model, and the learning rate is set to $3.5\times10^{-4}$.

Following TinyLlama, we evaluate the commonsense reasoning ability of the NoPE model and report acc\_norm in Table~\ref{tab:cs}. We compare with the TinyLlama checkpoint that is trained on 100B tokens. The purpose of this experiment is to prove the NoPE base model performs on par with RoPE.

\subsection{Long Sequence Language Modeling}
\label{ssec:ppl}

Success on long sequence language modeling tasks is essential for length generalization.
A method that does not perform well in language modeling probably won't handle real-world long-context tasks.

\paragraph{Settings.}
To evaluate the long sequence language modeling performances, we test our NoPE-based methods and RoPE-based baselines on PG19 \cite{Rae_Potapenko_Jayakumar_Hillier_Lillicrap_2020} and proof-pile \cite{proofpile} datasets.
For each dataset, we sample a subset of the test set and evaluate on $2$M tokens using sliding window evaluation ($S=256$) suggested by \citet{press2022train}.
We report the perplexity (PPL) of the models in Table~\ref{tab:ppl}.

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.32\linewidth]{fig/fig_pk_rope.pdf}
  \includegraphics[width=0.32\linewidth]{fig/fig_pk_yarn8k.pdf}
  \includegraphics[width=0.32\linewidth]{fig/fig_pk_yarn16k.pdf}\\
  \includegraphics[width=0.32\linewidth]{fig/fig_pk_nope.pdf}
  \includegraphics[width=0.32\linewidth]{fig/fig_pk_hs8k.pdf}
  \includegraphics[width=0.32\linewidth]{fig/fig_pk_hs16k.pdf}
  \caption{The figures illustrate the passkey retrieval accuracy for both RoPE and NoPE methods. The vertical dashed line represents the context length of the models, which could be either the pre-training length or the fine-tuning length. The title of each sub-figure indicates the average accuracy within the model’s context length. Notably, NoPE demonstrates robust performance even beyond the model’s context window, indicating significant potential for generalization.}
  \label{fig:passkey}
\end{figure*}
\paragraph{Main results.} 
Firstly, by comparing the original language models, NoPE's perplexity (PPL) is comparable to RoPE's for lengths within the training distribution, confirming the findings of \citet{haviv-etal-2022-transformer,chi-etal-2023-latent}. 
However, all LMs, including ALiBi models, fail to generalize out-of-the-distribution, indicating that explicit positional encoding is not the main reason for their failure in generalization. 
Current work on length generalization still focuses mainly on manipulating positional encoding.
Therefore, the length generalization issue within causal Transformer networks warrants a reanalysis and reinterpretation.

Secondly, by comparing the two generalization methods for NoPE proposed in this paper, the uniform scale method has significant limitations. 
Although using a larger scale can reduce the PPL at greater positions, it significantly affects the PPL at closer ranges. 
For instance, with a scale value of 1.8, the PPL on 2K@PG19 rises from 14.6 to 30.4, and on 2K@Proof-pile, it rises from 3.5 to 5.1.
On the contrary, the head-based scale method not only successfully extrapolates to 16k but also has minimal impact on the PPL at closer distances (for 18K, increases only +3.7 on 2K@PG19, +0.5 on 2K@Proof-pile), proving that attention heads with different patterns indeed require distinct scale values.

Third, a full comparison with RoPE LM's generalization method. 
Comparing the \emph{zero-shot} generalization methods, the head-based scale has better generalization than NTK, but weaker than YaRN.
In a fair comparison with the RoPE generalization methods which require \emph{fine-tuning}, 
the head-based scale method is competitive with these RoPE baselines, especially the Proof-pile dataset.
However RoPE baselines (PI, YaRN) still benefit from more training tokens, and the head-based scale on NoPE reaches its upper limit.

In summary, the head-based scale generalization method for NoPE slightly outperforms RoPE's early generalization method NTK, but still lags behind the recently introduced YaRN, particularly in near-distance PPL performance. 
Considering the significant challenge of generalizing NoPE compared to RoPE (due to the lack of explicit positional encoding to manipulate), this work, as the first to tackle length generalization for NoPE, has achieved its set goals.

The observed gap may imply that constraining the NoPE model to focus on fewer tokens could detrimentally affect its efficacy. Future efforts will be directed at enhancing the head-based scaling method to regain the level of performance seen in pretraining.

\begin{table*}[!t]
    \centering

    \resizebox{\textwidth}{!}{
        \begin{tabular}{lrrrrrrrrrrrrrrrrrr}
        \toprule
        \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Ctx.}} & \multirow{2}{*}{\textbf{Avg.}} & \multicolumn{3}{c}{\textbf{Singl-Doc QA}} & \multicolumn{3}{c}{\textbf{Multi-Doc QA}} & \multicolumn{3}{c}{\textbf{Summarization}} & \multicolumn{3}{c}{\textbf{Few-shot Learning}} & \multicolumn{2}{c}{\textbf{Synthetic}} & \multicolumn{2}{c}{\textbf{Code}} \\
        \cmidrule(lr){4-6} \cmidrule(lr){7-9} \cmidrule(lr){10-12} \cmidrule(lr){13-15} \cmidrule(lr){16-17} \cmidrule(lr){18-19} 
        & & & \textbf{NQA} & \textbf{Qsp} & \textbf{MulF} & \textbf{HpQA} & \textbf{2WQA} & \textbf{Musq.} & \textbf{GRpt} & \textbf{QSum} & \textbf{MulN} & \textbf{TREC} & \textbf{TrQA} & \textbf{SSum} & \textbf{PsgC} & \textbf{PsgR} & \textbf{Lcc} & \textbf{Re-P} \\
        \midrule
        \rowcolor{gray!10} \multicolumn{19}{c}{\textit{\textbf{Original LMs}}} \\
        RoPE & 2K & 16.5 & 3.5 & 4.7 & 17.5 & 3.4 & 8.8 & 2.8 & 26.9 & 8.4 & \textbf{25.9} & 33.5 & 18.8 & 15.7 & 1.9 & 2.5 & 49.5 & 40.1 \\ 
        NoPE & 2K & 18.3 & 6.1 & 7.9 & 22.4 & 6.6 & 10.3 & 3.1 & \textbf{28.9} & 8.8 & 25.1 & \textbf{41.5} & 30.0 & 3.5 & 1.0 & 3.0 & 48.4 & 46.6 \\ 
        \arrayrulecolor{gray!20}
        \rowcolor{gray!10} \multicolumn{19}{c}{\textit{\textbf{Generalization for RoPE}}} \\
        \multicolumn{1}{l}{\multirow{3}{*}{PI$^{\text{raw}}$}} & 4K & 16.7 & 5.4 & 8.6 & 18.6 & 4.5 & 9.1 & 3.9 & 26.4 & 9.9 & 18.5 & 21.5 & 21.2 & 22.2 & \textbf{2.7} & 1.5 & 48.5 & 44.6 \\ 
        \multicolumn{1}{l}{} & 8K & 16.7 & 4.7 & 9.6 & 16.3 & 5.4 & 9.3 & 4.0 & 14.6 & 9.4 & 20.7 & 27.0 & 23.1 & 23.5 & 2.1 & 3.4 & 50.0 & 44.7 \\ 
        \multicolumn{1}{l}{} & 16K & 17.2 & 4.8 & 8.1 & 18.6 & 5.4 & 9.4 & 3.8 & 22.9 & 9.9 & 21.3 & 24.0 & 23.9 & \textbf{25.4} & 1.6 & 1.8 & \textbf{50.5} & 43.8 \\ 
        \hline
        \multicolumn{1}{l}{\multirow{3}{*}{YaRN$^{\text{raw}}$}} & 4K & 16.2 & \textbf{6.4} & 8.7 & 18.2 & 4.0 & 11.0 & 3.0 & 17.5 & 9.0 & 15.6 & 27.5 & 21.5 & 20.3 & 1.6 & 0.5 & 49.8 & 45.2 \\ 
        \multicolumn{1}{l}{\multirow{3}{*}{}} & 8K & 16.4 & 6.0 & 11.4 & 16.0 & 5.0 & 8.3 & 3.5 & 16.3 & \textbf{10.3} & 19.6 & 21.0 & 24.9 & 22.1 & 1.3 & 2.0 & 49.6 & 45.3 \\ 
        \multicolumn{1}{l}{\multirow{3}{*}{}} & 16K & 17.7 & 4.5 & 10.5 & 17.1 & 5.2 & 8.9 & \textbf{4.7} & 18.9 & 9.2 & 19.5 & 38.0 & 24.4 & 25.2 & 1.7 & 1.8 & 49.8 & 44.6 \\ 
        \rowcolor{gray!10} \multicolumn{19}{c}{\textit{\textbf{Generalization for NoPE}}} \\
        \multicolumn{1}{l}{\multirow{3}{*}{$\lambda^{(h)}$}} & 4K & \textbf{18.5} & 6.3 & 11.1 & \textbf{23.1} & 5.7 & 10.1 & 4.2 & 27.7 & 8.9 & 23.4 & 25.5 & \textbf{35.7} & 13.7 & 0.6 & \textbf{4.5} & 47.9 & \textbf{46.9} \\ 
        \multicolumn{1}{l}{\multirow{3}{*}{}} & 8K & 17.2 & 5.8 & 11.7 & 21.4 & 6.1 & 10.8 & 3.9 & 24.1 & 8.9 & 18.3 & 31.0 & 31.4 & 4.5 & 0.6 & 3.1 & 47.3 & 46.5 \\ 
        \multicolumn{1}{l}{\multirow{3}{*}{}} & 18K & 17.0 & 6.0 & \textbf{12.8} & 20.3 & \textbf{7.0} & \textbf{12.9} & 4.1 & 17.2 & 8.4 & 16.1 & 41.0 & 32.9 & 5.1 & 0.3 & 2.1 & 44.5 & 41.0 \\ 
        \arrayrulecolor{black}
        \bottomrule
        \end{tabular}
    }
    \caption{Real-world Long-Context performance of NoPE-extension methods and various RoPE baselines. The ``Ctx.'' column represents testing context length during evaluation, which corresponds to either the pre-training length for base models or the extended length for length generalization methods.}
    \label{tab:longbench}
\end{table*}
\subsection{Synthetic Long Context Tasks}
\label{ssec:passkey}

A synthetic task is constructed in Landmark Attention \cite{mohtashami2023randomaccess} called "Passkey Retrieval".
It aims to test the effective context window size of the model.
The task is to retrieve a randomly placed passkey from a long sequence of tokens, where the passkey is a randomly sampled number of 5 digits and the sequence is built by concatenating irrelevant sentences.

\paragraph{Settings.} We evaluate the performance of passkey retrieval across various context lengths. For each specified context length, we conduct tests on 10 distinct passkey depths, each associated with 10 randomly selected passkeys. We report the retrieval accuracy in this task.

It is observed in Figure~\ref{fig:passkey} that both the NoPE base model and head-based scale perform well even when evaluating on $2\times$ the pretraining or fine-tuning context window, while RoPE strictly operates within the pre-trained sequence length and immediately fails outside of it. The result indicates that NoPE possesses significant potential for generalization.

\begin{table}[t]
    \centering
    \resizebox{\linewidth}{!}{
        \begin{tabular}{lcccc}
        \toprule
        \multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{PPL@16K ($\downarrow$)}}  & \multirow{2}{*}{\textbf{Passkey ($\uparrow$)}} & \multirow{2}{*}{\textbf{LongBench ($\uparrow$)}} \\
        \cmidrule(lr){2-3}
        & \textbf{PG19} & \textbf{Proof-pile} & &\\
        \midrule
        $\lambda^{(h)}$ 18K & 30.4 & \textbf{4.1} & \textbf{81} & \textbf{17.0} \\ 
        w/o focus constraint & \textbf{25.9} & 4.2 & 53 & 16.7 \\
        w/o initialization & 31.4 & 4.3 & 26 & 15.8 \\
        \bottomrule
        \end{tabular}
    }
    \caption{Ablation study on the two variants of HeadScale. Passkey results are listed as average accuracy, and LongBench results are averaged score among all sub-tasks.}
    \label{tab:ablation}
\end{table}
\subsection{Real-World Long Context Tasks}
\label{ssec:longbench}

LongBench \cite{bai2023longbench} is a comprehensive assessment of the long context understanding capabilities of large language models. We test all models using beam search decoding with beam size $5$. The evaluation context size is set to the model context window accordingly in order to test the model's capability to utilize a longer context. We only include raw PI and YaRN as the baseline in this task.

We find that the performance of the NoPE base model is better than its RoPE counterpart. Concluding better information utilization in the original length. Moreover, the head-based scale at a 4k extension length performs the best among all baselines. We attribute it to the capability of the NoPE base model and the successful length generalization of the head-based attention scale method.
While the head-based model still suffers from performance degradation when extending to a longer context, as it is stated in Section~\ref{ssec:ppl}.

\subsection{Ablation Study}
\label{ssec:ablation}

We have introduced two key components of HeadScale in Section~\ref{ssec:head_scale}, a concentration constraint and an initializing technique. The ablation study in Table~\ref{tab:ablation} depicts that although occasionally perform better in language modeling, the two variants are less preferment in passkey retrieval and LongBench, indicating their inability to utilize long context information. 

Detailed results of the passkey retrieval task can be found in Figure~\ref{fig:ablation_pk} in the Appendix~\ref{app:abl_hs_pk}. They are completely unable to answer the passkey except when it is at the beginning of the context window.

\section{Related Work}
\label{sec:related-work}

\paragraph{Transformers without position encoding}
\citet{haviv-etal-2022-transformer} was the first to discover that causal Transformer networks could perform language modeling tasks successfully even without explicit PE.
\citet{chi-etal-2023-latent} provided a theoretical explanation for NoPE, demonstrating that for an initialized NoPE LM, the variance of the hidden representations in each layer is position-dependent, with variance decreasing for larger positions.
Both works demonstrate that the NoPE hidden layer representation implies positional information through the probing task.
\citet{kazemnejad2023the} proved through constructive methods that NoPE can learn absolute PE from the first layer and relative PE from the second layer. 
They also showed that NoPE has an extremely weak length generalization ability (train $\sim$20, test $\sim$40), but is slightly better than LM with explicit PE.
This paper first proposes length generalization methods for NoPE with uniform scale and head-based scale.
For the first time verifies the effectiveness of NoPE generalization in real LLM settings.

\paragraph{Length generalization}
Due to high computational and memory requirements, LLM training is usually limited to short inputs. 
Directly applying LLMs to long inputs faces the challenge of out-of-distribution (OOD) issues. 
Research to enable LLMs to process long inputs has been extensive \cite{huang2023advancing,dong2023survey}. 
The earliest methods involved designing new relative PE mechanisms during pre-training
\citep{press2021train,sun-etal-2023-length}. 
Subsequent studies focused primarily on the widely used RoPE \cite{su2024roformer} and proposed length extension by mitigating RoPE's OOD issues through interpolated positions
\citep{chen2023extending,kaiokendev9444,peng2023yarn,emozillareddit,bloc97,bloc972}. 
Other works employed sliding window attention mechanisms to prevent relative positions from exceeding the maximum distance seen in pre-training 
\citep{mohtashami2023landmark,han2023lminfinite,xiao2023efficient,jin2024llm,zhang2024soaring}. 
However, these models ignore information from distant tokens, thus failing to capture long-distance context dependencies.
All existing methods rely on specific explicit PEs. 
However, the NoPE architecture is more streamlined and more aligned to the form of human language modeling.
Exploring NoPE's length generalization is therefore more intriguing and attractive.

\section{Discussion}
We studied the length generalization of Casual Transformer 
without explicit position encoding.
We developed a parameter-efficient tuning algorithm 
which aims to search for the best temperature hyper-parameters
for attention heads.
Through empirical evaluation, we saw that 
NoPE can achieve competitive length generalization and might be 
a promising alternative for long-context language modeling.

NoPE provides a new perspective to understanding the role of positional information by isolating and eliminating the effects of explicit positional encoding. Our work demonstrates the correlation between length generation failures and distraction of attention in NoPE models, thus the proposed method concentrates the attention by adjusting the scaling factor. While current works on length generalization mainly focus on manipulating positional encoding, our work suggests a new key component to generalization.

\section*{Limitation}
The length generalization algorithms discussed in this paper exhibit 
 competitive performances, but the NoPE model itself still
 underperforms with state-of-the-art RoPE models, which 
 makes the results over long sequence language modeling tasks 
 and LongBench tasks are less competitive.
 NoPE still faces the challenges of considerable memory usage and computational complexity due to the quadratic nature of attention computation when processing extremely long contexts. 
Hardware limitations are likely to become a constraining factor for length generalization soon.
 We plan to further 
 improve the NoPE's performances for a fairer comparison.
 This paper is also most an empirical one, which requires 
 a deeper theoretical understanding of NoPE's length generalization
 in the future.

\section*{Acknowledgement}
The authors wish to thank all reviewers for their helpful comments and suggestions.
The corresponding authors are Tao Ji, Yuanbin Wu and Xiaoling Wang.
This research was (partially) supported by NSFC(62076097), National Key R\&D
Program of China (2021YFC3340700),  the Open Research Fund of Key Laboratory of Advanced Theory and Application in Statistics and Data Science (East China Normal University), Ministry of Education.

\end{document}