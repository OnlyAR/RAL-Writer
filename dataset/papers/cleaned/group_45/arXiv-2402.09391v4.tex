\title{\modelname: Advancing Large Language Models for Chemistry \\with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset}

\begin{document}

\def\thefootnote{*}\footnotetext{Equal contribution.}\def\thefootnote{\arabic{footnote}}

\maketitle

\begin{abstract}
Chemistry plays a crucial role in many domains, such as drug discovery and material science.
While large language models (LLMs) such as GPT-4 exhibit remarkable capabilities on natural language processing tasks, existing research indicates that their performance on chemistry tasks is discouragingly low. In this paper, however, we demonstrate that our developed LLMs can achieve very strong results on a comprehensive set of chemistry tasks, outperforming the most advanced GPT-4 \newcontent{and Claude 3 Opus} by a substantial margin.
To accomplish this, we propose \datasetname, a \textit{large-scale}, \textit{comprehensive}, and \textit{high-quality} dataset for instruction tuning. It contains 14 selected chemistry tasks and over three million samples, laying a solid foundation for training and evaluating LLMs for chemistry. Using \datasetname, we fine-tune a set of open-source LLMs named as \modelname, among which, we find that Mistral serves as the best base model for chemistry tasks. 
\newcontent{Our analysis further demonstrates the critical role of the proposed dataset in driving the performance improvements.}\footnote{Our dataset and models can be found at \url{https://osu-nlp-group.github.io/LLM4Chem/}.}
\end{abstract}

\section{Introduction}

Chemistry is a fundamental science that underpins countless aspects of modern life, ranging from drug discovery and materials science to energy production. To facilitate research and applications in this domain, deep learning models including graph neural networks \citep{kipf2017semisupervised} and Transformer-based models \citep{vaswani2017attention} have been developed for various chemistry tasks such as forward reaction prediction, retrosynthesis, property prediction \citep{schwaller2019molecular,Zhong2022,chen2023g2retro,zhou2023unimol}. However, these models are usually task-specific models, which neglect shared chemistry knowledge across tasks and can hardly be adapted to different tasks.

On the other hand, large language models (LLMs) such as GPT-4 \citep{achiam2023gpt4}, Llama series \citep{touvron2023llama,touvron2023llama2}, and Mistral \citep{jiang2023mistral} have emerged as general-purpose foundation models and demonstrate remarkable abilities on various natural language processing tasks \citep{chang2023survey,thirunavukarasu2023large,yue2023mammoth,zhang2023tablellama,deng2023mindweb}. 
However, when applied to chemistry tasks, LLMs show only limited capabilities \citep{jablonka2022gpt,guo2023what,hatakeyama2023prompt}.
For example, \citet{guo2023what} conducted evaluations on eight chemistry tasks and observed that while GPT-4 outperforms other closed- and open-source LLMs, 
its performance is far from that of task-specific deep learning models. Particularly, they found that GPT models perform poorly when a precise understanding of SMILES \citep{weininger1988smiles}, a widely used textual representation for molecules, is required. 
In addition to directly applying pretrained LLMs, \citet{fang2023mol} fine-tuned LLMs on an instruction tuning dataset, but their performance remains very low, far behind the state-of-the-art (SoTA) models designed and trained for specific tasks.

Given these discouraging results, some critical questions arise: \textit{\textbf{Are LLMs actually able to effectively perform chemistry tasks? Or, Are they fundamentally limited for chemistry?}} %
In this paper, we demonstrate that our developed LLMs can achieve very strong results on a comprehensive set of chemistry tasks, substantially outperforming the most advanced GPT-4 \cite{achiam2023gpt4} \newcontent{and Claude 3 Opus \cite{anthropic2024claude3}}.

What makes such LLMs possible? First, we construct a large-scale, comprehensive, and high-quality dataset for instruction tuning named \datasetname. 
We incorporate tasks with meaningful applications, collect data from diverse data sources, and apply rigorous scrutiny for quality control.
The resulting dataset consists of 14 tasks (illustrated in \cref{fig:tasks}) and over 3M samples, laying a solid foundation for training and evaluating LLMs for chemistry tasks.
Based on the dataset, we build a series of LLMs for chemistry named \textbf{\modelname} by fine-tuning four open-source LLMs namely Galactica, Llama 2, Code Llama, and Mistral, on \datasetname with LoRA \citep{hu2021lora}.

\begin{figure*}[t]
\begin{center}
\centerline{
\includegraphics[width=1.0\textwidth]{fig/overview.pdf}}
\caption{An overview of tasks in the proposed \datasetname dataset.}
\label{fig:tasks}
\end{center}
\vskip -0.4in
\end{figure*}

\newcontent{
We conduct comprehensive experiments to evaluate our models and explore their insights, yielding some interesting findings. 
Firstly, among the four \modelname models, the Mistral-based model surpasses others by a substantial margin, showcasing the considerable influence of base models on downstream chemistry tasks.
Moreover, contrast to claims made in previous work \citep{fang2023mol}, using SMILES as the molecular representation achieves sufficient validity of generated molecules and better performance compared to using SELFIES \citep{krenn2019selfies}.
Furthermore, employing canonicalized SMILES during model training and applications can alleviate learning burdens and increase performance.
Finally, while instruction tuning can inject chemistry task-related knowledge into models, the dataset plays a crucial role. Our experiments demonstrate that training on our \datasetname leads to substantially better performance compared to training on previous dataset, emphasizing the contribution of the proposed dataset.
Although \modelname models do not yet surpass state-of-the-art (SoTA) task-specific models that are designed and trained specifically for each individual task, they approach SoTA performance with only 0.58\% of parameters being fine-tuned, suggesting their great potential for further improvements and to serve as strong foundation models for the field.
}

\section{Related Work}

\noindent \textbf{Task-specific Models for Chemistry.} 
In recent years, many deep learning models have been developed to tackle different chemistry tasks.
For example, Molecular Transformer \cite{schwaller2019molecular} and RSMILES \cite{Zhong2022} formulate forward synthesis and retrosynthesis prediction as sequence-to-sequence translation problems.
Chemformer \cite{irwin2022chemformer} pretrains a transformer model on a large-scale SMILES dataset and fine-tunes it for various downstream tasks, such as forward synthesis and property prediction.
MolT5 \cite{edwards2022translation} first pretrains a T5 model on both SMILES and natural language, and then fine-tunes it to translate SMILES into natural language (i.e., molecule captioning) or vice versa (i.e., molecule generation).
\newcontent{Graph neural networks (GNNs), which directly leverage the graph structure of the molecule \cite{wang_graph_2023}, have also shown promise in many chemistry applications, such as property prediction \cite{yang_analyzing_2019,han2023himgnn}, retrosynthesis \cite{chen2023g2retro,somnath2021learning}, and molecule optimization \cite{chen2021deep, zhang2022molecule}.}
\newcontent{Recent studies \cite{zhou2023unimol,zhang2022e3bind} have shown the promise of leveraging equivariant representations of molecular 3D structures for chemistry tasks, such as property prediction \cite{zhou2023unimol} and docking \cite{zhang2022e3bind}.}
Uni-Mol \cite{zhou2023unimol} incorporates \newcontent{this 3D information} into the pretraining of a transformer model and fine-tunes it for downstream tasks.
Despite their effectiveness, these models operate on single tasks and therefore cannot harness knowledge shared across diverse chemistry tasks like LLMs.

\noindent \textbf{LLMs for Chemistry.} 
Recent efforts have integrated LLMs with chemistry to solve key chemistry problems,
which can be divided into two categories: (1) benchmark studies, and (2) fine-tuning LLMs with new datasets. %
Multiple benchmark studies \cite{White2023,guo2023what,Jablonka2023,liu2023chatgpt} have evaluated 
the capabilities and limitations of different off-the-shelf LLMs, such as GPT-4 and Llama, on chemistry problems.
{For example, \citet{guo2023what} finds}
that these LLMs do not perform well on chemistry tasks and often produce
chemically implausible outputs.
These findings highlight the need for further efforts to improve LLMs via fine-tuning for chemistry tasks.

To improve LLMs for chemistry, multiple instruction tuning datasets have been developed. %
Mol-Instructions \cite{fang2023mol} consists of 1.3M instructions for multiple small molecule tasks.
{However, fine-tuning on the dataset does not significantly improve LLMs' performance (Section \ref{exp:overall_comp}).}
Drugchat \cite{Liang2023a} collects an instruction tuning dataset on drug properties with 10.8K drug molecules.
MolOpt-Instructions \cite{ye2023drugassist} consists of instructions with 1M molecule pairs for molecule optimization on six properties, in which each pair has similar molecules with different properties. 
Recent works also develop 2D or 3D molecular graph-centric datasets and integrate the graph understanding ability into LLMs \cite{liu-etal-2023-molca, cao2023instructmol, li2024towards}.
Compared with these datasets, %
\datasetname is {much larger}
and covers {a} more diverse and comprehensive {set of} chemistry tasks, which enables LLMs to better understand molecule representations and learn chemistry knowledge across tasks.

\section{\datasetname}
\newcontent{This section introduces our proposed dataset \datasetname and its construction. Readers may refer to \cref{app:prelim} for preliminaries and background.} %the involved background knowledge.}

\subsection{Overview of \datasetname}
\label{subsec:dataset_overview}

\datasetname is a large-scale instruction tuning dataset that centers around small molecules. It contains 14 chemistry tasks, illustrated in \cref{fig:tasks}.

(1) We include four {name conversion} tasks, namely converting {IUPAC name} to molecular {formula} (NC-I2F), converting {IUPAC name} to {SMILES} (NC-I2S), converting {SMILES} to molecular formula (NC-S2F), and converting {SMILES} to {IUPAC name} (NC-S2I). They are designed to enable deep understanding of molecular structures and representations, which should serve as the fundamental knowledge for chemistry LLMs.

(2) Additionally, six {property prediction} tasks \citep{wu2018moleculenet} are integrated, including PP-ESOL for water solubility \citep{Mobley2014}, PP-Lipo for octanol/water distribution coefficient \citep{Poole2003}, PP-BBBP for blood-brain barrier penetration \citep{martins2012}, PP-ClinTox for toxicity to human body \citep{Gayvert2016}, PP-HIV for HIV replication inhibition \citep{HIV}, and PP-SIDER for side effects of drugs \citep{Kuhn2015}. These involved properties are crucial especially for drug development.

(3) Two tasks focus on the textual descriptions of molecules: {molecule captioning} (MC) is to generate a textual description of a given molecule, and {molecule generation} (MG) is to generate a molecule based on the given textual description. They require comprehensive understanding of molecules - their structures and properties, from their textual descriptions. %
They also bridge the gap between natural language and molecules.

(4) Lastly, two tasks revolve around chemical reaction knowledge. {Forward synthesis} (FS) aims to predict potential products from reactants and reagents, and {retrosynthesis} (RS) involves predicting potential reactants given a product. These tasks play vital roles in real-world applications \citep{Coley2018}. For example, retrosynthesis is essential for synthesis planning, %
while forward synthesis is used to validate retrosynthetic suggestions.

\datasetname contains 3.3M samples. Each sample is a query-response pair, where the query describes a task and any task-specific information (e.g., input molecule, textual description, etc.), and the response is a sentence containing the answer to the queried task. For all the tasks, unless explicitly defined in the tasks (NC-I2F, NC-I2S, NC-S2F, and NC-S2I), we use {SMILES} as the \newcontent{default representation for molecules, but also provide the SELFIES \citep{krenn2019selfies} representation}.

\subsection{\datasetname Construction}
\label{subsec:data_construction}

We construct the \datasetname dataset by following a four-step pipeline: data collection, quality control, data splitting, and instruction construction.

\noindent \textbf{Data Collection.} After consulting domain experts and pinpointing the set of meaningful tasks (summarized in \cref{subsec:dataset_overview}), we collect data for these tasks from various sources, as listed in \cref{tab:statistics}. 
Specifically, for the {name conversion} tasks (NC-I2F, NC-I2S, NC-S2F, and NC-S2I), we leverage PubChem\footnote{\url{https://pubchem.ncbi.nlm.nih.gov/}} \citep{kim2019pubchem}, one of the most comprehensive molecule databases. Within this database, we randomly select a large set of molecule entries, and extract their {IUPAC} names, {SMILES} representations, and molecular formulas. This obtained data is then re-organized as input-output pairs for the tasks. 
For molecular description-related tasks (MC and MG), we utilize a combination of ChEBI-20 \citep{edwards2021text2mol,edwards2022translation} and Mol-Instructions \citep{fang2023mol}, as they both contain high-quality molecule-text paired data. 
For property prediction tasks (PP-ESOL, PP-Lipo, PP-BBBP, PP-ClinTox, PP-HIV, and PP-SIDER), we employ the well-established MoleculeNet datasets \citep{wu2018moleculenet}. We select \newcontent{the 6 datasets from MoleculeNet that represent the essential properties for real-world applications such as drug discovery.}
For chemical reaction tasks (FS and RS), we collect the reaction data from USPTO-full \citep{Lowe2017}, which is an extensive collection encompassing over 1M reaction samples extracted from U.S. patents.
All the aforementioned datasets are also widely used in previous studies \citep{he2021molecular,Zhong2022,edwards2022translation,irwin2022chemformer,chen2023g2retro,zhou2023unimol}.

\noindent \textbf{Quality Control.}
To guarantee high quality, we apply rigorous scrutiny. 
The collected data contains many problematic and low-quality samples, which can be roughly categorized into the following three types, along with our curation methods:
(1) Chemically invalid SMILES. Numerous SMILES strings are chemically invalid (e.g., deviating from the SMILES grammar, or violating chemical valence). To address this issue, we employ RDKit \citep{rdkit}, a widely used toolkit for cheminformatics, to parse molecules and detect errors.
(2) Wrong or inaccurate information. Based on manual check, we observed wrong and inaccurate information recorded in the data. 
For instance, within the USPTO-full dataset \citep{Lowe2017}, we identify and correct mislabeled reactants and reagents in chemical reactions by comparing their atom mappings with products.
For the MC and MG tasks, we filter out those textual descriptions that 
lack pertinent, molecule-specific information, with a set of rules based on wording patterns, lengths and keywords. 
For PP-SIDER, we eliminate disorders with ambiguous names that could impede the creation of precise and comprehensible instructions.
(3) Duplicated samples. We detect and remove them.

\noindent \textbf{Data Splitting.}
Data splitting for multi-task datasets requires careful handling in order to avoid data leakage across tasks. For instance, FS and RS are a pair of reverse tasks, so data leakage occurs when the training set contains an FS sample for a certain chemical reaction and the test set has an RS sample for the same reaction. This can lead to biased evaluation. Therefore, we identify sample pairs across related tasks
(FS and RS, MC and MG, and the four NC tasks) that correspond to the same molecules/reactions, and ensure that matched samples are placed together in either training or evaluation set. 
Moreover, some samples may share the same input but have different outputs. For instance, in the RS task, one product (the same input) may be synthesized from multiple sets of reactants (different outputs). If these samples are placed into both training and test set, it may lead to exaggerated performance. 
Therefore we ensure that samples with identical inputs are placed together either in or outside of the test set. 
Additionally, to achieve fair comparisons with Mol-instructions \citep{fang2023mol}, for tasks shared between the two datasets (MC, MG, FS, and RS), we ensure that their training examples are not included in the test set of \datasetname, allowing for a direct evaluation of their models on our test set.
Following these necessary limitations, samples are randomly split into training/validation/test set, except for PP task samples that undergo a scaffold splitting following the canonical method \citep{wu2018moleculenet}.

\noindent\textbf{Instruction Creation.}
To create query-response textual pairs for instruction tuning, we manually craft several templates, each including a query and a corresponding response, and apply GPT-4 to rephrase them. 
Unlike those in \citep{fang2023mol} which consist of highly formatted queries (containing three explicitly labeled parts namely instruction, input, and output) and answer-only responses (e.g., responses for FS and RS only contain answer SMILES alone, without any natural text), our templates exhibit a more natural and diverse set of formats in both queries and responses, allowing for more variations and naturalness in input-output interactions.
Moreover, all the {SMILES} representations are canonicalized, establishing a standardized data format. 
In light of the dataset's inclusion of multi-type sequences (SMILES, molecular formula, numbers, etc.) beyond natural language text alone, we utilize special tags to encapsulate corresponding segments (e.g., \texttt{<SMILES>...</SMILES>} for SMILES, \texttt{<MOLFORMULA>...</MOLFORMULA>} for molecular formula, \texttt{<NUMBER>...</NUMBER>} for numbers). This design does not only explicitly inform models about the information types within the tagged content, but also facilitate answer extraction during evaluation.

\newcontent{For more details of dataset construction, please refer to \cref{app:dataset_construction}.}

\subsection{Merits of \datasetname}

Compared to previous work \citep{fang2023mol,Liang2023a,ye2023drugassist}, \datasetname stands out in several key aspects:

(1) \textbf{Large-Scale}.
\datasetname consists of 3.3M samples and 1.6M distinct molecules, with a diverse range of sizes, structures, and properties (see \cref{app:dataset_statistics}),
showcasing an extensive coverage of diverse chemical knowledge.

(2) \textbf{Comprehensive}.
\datasetname contains 4 types of chemical tasks (14 tasks in total), emerging as the most comprehensive instruction tuning dataset for small molecules. Notably, the tasks are meticulously selected to build a strong chemistry foundation model and to adapt to real-world applications. 

(3) \textbf{High-Quality}.
Rigorous processing steps have been implemented to exclude problematic and low-quality samples. Along with careful data splitting and canonicalization of SMILES representations, \datasetname stands as a high-quality resource valuable for future research.

A detailed introduction and statistics of the \datasetname dataset can be found in \cref{app:dataset}. For a comparison with the previous work, Mol-Instructions \citep{fang2023mol}, please refer to \cref{app:dataset_comparison}.

\section{Experiments}
\label{sec:experiments}

\subsection{Our \modelname Models} 
\label{subsec:our_model}

By fine-tuning base models on the proposed \datasetname dataset, we create LLMs capable of performing chemistry tasks, which we name \modelname (\textbf{L}arge \textbf{la}nguage models on \textbf{S}mall \textbf{Mol}ecules).
Specifically, we extensively consider four different LLMs as our base models, namely Galactica 6.7B \citep{taylor2022galactica}, Llama 2 \citep{touvron2023llama2} 7B, Code Llama \citep{roziere2023code} 7B, and Mistral \citep{jiang2023mistral} 7B, where Galactica is trained for scientific applications and has already been exposed to chemistry-related data during its pretraining, Llama 2 and Mistral are general-purpose LLMs, while Code Llama is based on Llama 2 and trained for code.
We conduct instruction tuning on the proposed \datasetname dataset, and name the resulting models as \modelnameg, \modelnamel, \modelnamec, and \modelnamem, respectively. All the \modelname models are trained with LoRA \citep{hu2021lora}, which is applied to all weight matrices in the self-attention and feedforward neural network (FFN) modules with \texttt{lora\_r} and \texttt{lora\_alpha} set to 16. The fine-tuning process utilizes the Huggingface Transformers library \citep{wolf2020transformers}. Training spans three epochs, employing the 8-bit AdamW optimizer, a learning rate of 1e-4, and a cosine scheduler. The input length for training is set to 512, which covers 99.7\% of the samples. During inference, we adopt beam search as the generation strategy for simplicity.

\subsection{Experimental Setup}
\label{subsec:experimental}

\noindent \textbf{Compared Models.}
We compare our \modelname models with two types of models:

(1) \textbf{LLMs without fine-tuning on \datasetname}.
This type includes our four base models, namely Galactica \citep{taylor2022galactica}, Llama 2 \citep{touvron2023llama2}, Code Llama \citep{roziere2023code}, Mistral \citep{jiang2023mistral}.
we also benchmark against GPT-4 \citep{achiam2023gpt4} and the more recent Claude 3 Opus \citep{anthropic2024claude3}, the current state-of-the-art (SoTA) LLMs\footnote{Due to resource limitations, we evaluate GPT-4 and Claude 3 Opus on at most 500 test samples for each task.}. 
For Llama 2, Code Llama, and Mistral, we use 1-shot, due to their poor instruction following ability; for GPT-4, we report its results under a zero-shot setting, as GPT-4 performs best on this setting in our experiments %
(\cref{app:exp_results}); \newcontent{for Claude 3 Opus, we report its zero-shot results as well.}
\newcontent{We also include two LLMs tuned specifically for chemistry tasks: Molinst, a Llama 2 model tuned on the Mol-Instructions dataset by \citet{fang2023mol}, which shares the training tasks of MC, MG, FS, and RS with \modelname; and ChemLLM \citep{zhang2024chemllm}, an LLM for chemistry proposed concurrently to our work.}

(2) \textbf{SoTA task-specific models.} To provide a comprehensive view of \modelname's performance, we present results from SoTA task-specific models. 
For NC-I2S and NC-S2I, we compare with STOUT \citep{rajan2021stout}, an encoder-decoder model trained on SMILES-IUPAC name paired data. 
For NC-S2F, a task achievable with a fixed algorithm, we implement a program with RDKit \citep{rdkit}, a widely used Python toolkit for cheminformatics, and report its results. 
For NC-I2F where no dedicated models exist, we construct a baseline called STOUT+RDKit by aggregating STOUT for I2S conversion and RDKit for S2F conversion. 
For the PP tasks, our compared model is Uni-Mol \citep{zhou2023unimol}. It incorporates molecular 3D representations and follows a pretraining and fine-tuning paradigm. Following its original settings, we fine-tune the model on our \datasetname dataset with its pretrained checkpoint. 
In the case of MC and MG, we compare with MolT5 \citep{edwards2022translation} and directly use their released checkpoint. The reasons why we do not use our re-trained model are: (1) we were unable to reproduce results close to those reported in the paper as no original code was provided; and (2) we take great care to ensure that our test set is devoid of training examples used by MolT5, ensuring fairness in the evaluation.
Lastly, regarding FS and RS, we re-train RSMILES \citep{Zhong2022} and Molecular Transformer \citep{schwaller2019molecular} for the two tasks, respectively, following their reported settings. Both of the models are transformer encoder-decoder models \citep{vaswani2017attention}, specifically adapted for the FS and RS tasks.

\noindent \textbf{Evaluation Metrics.}
We employ metrics commonly used in previous work \citep{schwaller2019molecular,Zhong2022,fang2023mol,zhou2023unimol,chen2023g2retro}, which include: 
(1) \textbf{Exact Match (EM)}, indicating the proportion of predicted results that exactly match the gold standards. 
(2) \textbf{Fingerprint Tanimoto Similarity (FTS)}, quantifying structural similarities between molecules using Tanimoto similarities of their Morgan fingerprints \citep{Morgan1965}.
(3) \textbf{METEOR score}, a comprehensive text-based metric considering both exact matches and semantic similarity \citep{lavie-agarwal-2007-meteor} for the MC task.
(4) \textbf{Root Mean Square Error (RMSE)}, measuring the square root of the average squared differences between predicted and actual values for the PP-ESOL and PP-Lipo tasks 
(5) \textbf{Accuracy (Acc)}, the ratio of correct predictions for the binary classification tasks (PP-BBBP, PP-ClinTox, PP-HIV, and PP-SIDER). 
(6) \textbf{Validity (Valid)}, the ratio of valid predictions following SMILES grammar and chemical valence rules for tasks with SMILES outputs (NC-I2S, MG, FS, and RS). 
For all the metrics except RMSE, higher values indicate better performance.

\subsection{Main Results} %
\label{exp:overall_comp}

\cref{tab:o_1} and \ref{tab:o_2} show the performance on \datasetname. Key observations are as follows: 

\begin{table*}[t]
\centering
\caption{Results for name conversion (NC) and property prediction (PP) tasks. Metrics EM, Valid, and Acc are in percentage.}
\resizebox{1\linewidth}{!}{
\begin{threeparttable}[b]
  \centering
  
  \sisetup{detect-weight=true}
    \begin{tabular}{lS[table-format=2.1]S[table-format=2.1]S[table-format=2.1]S[table-format=3.1]S[table-format=2.1]S[table-format=1.3]S[table-format=1.3]S[table-format=2.1]S[table-format=2.1]S[table-format=2.1]S[table-format=2.1]}
    \toprule
    \multicolumn{1}{c}{\multirow{3}[6]{*}{\textbf{Model}}} & \multicolumn{5}{c}{\textbf{NC}}       & \multicolumn{6}{c}{\textbf{PP}} \\
    \cmidrule(lr){2-6}  \cmidrule(lr){7-12}   & \textbf{I2F} & \multicolumn{2}{c}{\textbf{I2S}} & \textbf{S2F} & \textbf{S2I} & \textbf{ESOL} & \textbf{Lipo} & \textbf{BBBP} & \textbf{Clintox} & \textbf{HIV} & \textbf{SIDER} \\
     \cmidrule(lr){2-2} \cmidrule(lr){3-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \cmidrule(lr){7-7} \cmidrule(lr){8-8} \cmidrule(lr){9-9} \cmidrule(lr){10-10}  \cmidrule(lr){11-11}  \cmidrule(lr){12-12}
     & \textbf{EM} & \textbf{EM} & \textbf{Valid} & \textbf{EM} & \textbf{EM} & \textbf{RMSE} $\downarrow$ & \textbf{RMSE} $\downarrow$ & \textbf{Acc} & \textbf{Acc} & \textbf{Acc} & \textbf{Acc} \\
    \midrule
    \rowcolor{TableSeparator} \multicolumn{12}{c}{\rule{0pt}{10pt}\textbf{Task-Specific, Non-LLM Based Models}} \\[1pt]
    \multicolumn{1}{p{7.585em}}{SoTA} & 97.9  & 73.5  & 99.4  & 100.0 & 56.5  & 0.819 & 0.612 & 85.3  & 92.4  & 97.0  & 70.0 \\
    \rowcolor{TableSeparator} \multicolumn{12}{c}{\rule{0pt}{10pt}\textbf{Existing LLMs without fine-tuning on \datasetname}}  \\[1pt]
    GPT-4 & 8.7   & 3.3   & 84.2  & 4.8   & 0.0   & 2.570 & 1.545 & 62.9  & 50.0  & 59.6  & 57.6 \\
    \newcontent{Claude 3 Opus} & 34.6  & 17.7  & 90.2  & 9.2   & 0.0   & \bfseries 1.036 & 1.194 & \bfseries 75.1  & 41.7  & 76.4  & 67.0 \\
    Galactica & 9.1   & 9.7   & 95.6  & 0.0   & 0.0   & 4.184 & 2.979 & 69.0  & 92.4  & \bfseries 96.7  & 68.1 \\
    Llama 2 & 0.0   & 0.0   & 18.3  & 0.0   & 0.0   & 3.287 & 1.634 & 58.9  & 45.1  & 93.3  & 61.9 \\
    Code Llama & 0.0   & 0.0   & 81.0  & 0.0   & 0.0   & 3.483 & 1.733 & 58.9  & 85.4  & 91.8  & 60.2 \\
    Mistral & 0.0   & 0.0   & 40.3  & 0.0   & 0.0   & 3.079 & 1.730 & 40.6  & 15.3  & 7.1   & 38.1 \\
    Molinst (\newcontent{chemistry LLM}) & 0.0   & 0.0   & 96.2  & 0.0   & 0.0   & 2.271 & 1.691 & 60.9  & 6.3   & 4.5   & 52.4 \\
    \newcontent{ChemLLM} (\newcontent{chemistry LLM}) & 0.8   & 0.3   & 3.9   & 0.0   & 0.0   & 1.946 & 1.797 & 22.3  & 75.7  & 72.9  & 32.6 \\
    \rowcolor{TableSeparator} \multicolumn{12}{c}{\rule{0pt}{10pt}\textbf{Our \modelname Series}}  \\[1pt]
    \modelnameg & 83.2  & 58.7  & 99.4  & 91.2  & 18.3  & 1.959 & 1.213 & 69.0  & \bfseries 93.1  & \bfseries 96.7  & 70.1 \\
    \modelnamel & 73.8  & 46.6  & 99.0  & 87.0  & 12.9  & 2.791 & 1.338 & 69.0  & 92.4  & \bfseries 96.7  & 68.7 \\
    \modelnamec & 75.4  & 49.9  & 99.3  & 88.6  & 15.5  & 2.959 & 1.203 & 69.0  & \bfseries 93.1  & \bfseries 96.7  & 69.9 \\
    \modelnamem & \bfseries 87.9  & \bfseries 70.1  & \bfseries 99.6  & \bfseries 93.2  & \bfseries 29.0  & 1.150 & \bfseries 1.010 & 74.6  & \bfseries 93.1  & \bfseries 96.7  & \bfseries 70.7 \\
    \bottomrule
    \end{tabular}%
  
\end{threeparttable}
}
\label{tab:o_1}%
\end{table*}%

(1) \textbf{Among all the LLMs, our \modelname models demonstrate the best performance, underscoring the effectiveness of the proposed \datasetname dataset and fine-tuning}.
Specifically, compared to the base models (Galactica, Llama 2, Code Llama, and Mistral), \modelname models exhibit substantial performance improvements, which highlights the effectiveness of \datasetname in enhancing the understanding of molecular representations and the task-related knowledge, and signifies the effective learning of chemistry-related tasks by LLMs.
Furthermore, \modelname substantially outperforms GPT-4 on all the tasks \newcontent{and Claude 3 Opus on most tasks}, despite their larger parameter size.
\newcontent{\modelname also surpasses the two chemistry LLMs namely ChemLLM\footnote{Since its dataset and evaluation details are not available, we cannot provide more analysis.}, which is similarly trained on chemistry instruction data.} and Molinst.
Notably, \modelnamel, which uses the same base model and LoRA setting as Molinst, outperforms it even on the shared training tasks (MC, MG, FS, and RS). This finding highlights the benefits of our dataset.

(2) \textbf{Our four \modelname models show substantial differences in their performance, emphasizing the considerable impact of base models on downstream tasks}. 
Despite sharing identical training, inference settings, and comparable model sizes, \modelnamem consistently outperforms \modelnamel by a substantial margin, highlighting Mistral's potential on chemistry tasks.
In addition, \modelnamec exhibits better performance than \modelnamel \newcontent{on most tasks}, indicating a potential synergy between programming language knowledge in Code Llama and molecular representations.
Furthermore, \modelnameg outperforms \modelnamel, and \modelnamec in most cases, suggesting the benefits of pretraining on chemistry-related documents.

\newcontent{(3) \textbf{Although \modelname models do not outperform SoTA models, they demonstrate considerable potential for further improvements}.
Specifically, \modelnamem surpasses the SoTA models on PP-Clintox and PP-SIDER, but has yet to achieve the success on other tasks.
However, \modelname has greatly narrowed the performance gap between LLMs and SoTA task-specific models, compared to previous efforts \citep{fang2023mol,zhang2024chemllm}. 
Remarkably, \modelnamem attains such performance with only a small proportion of its parameters fine-tuned (approximately 41.9M, 0.58\% of its parameters). As shown in \cref{subsec:lora}, increasing the number of trainable parameters can substantially boost performance, suggesting that \modelnamem has immense potential to surpass task-specific models through more extensive fine-tuning and serve as a strong foundation model for chemistry applications.
}

\begin{table*}
\centering
\caption{Results for molecule captioning (MC), molecule generation (MG), forward synthesis (FS), and retrosynthesis (RS). Metrics EM, FTS, and Valid are in percentage.}
\resizebox{0.92\linewidth}{!}{
\begin{threeparttable}
  \centering
  \sisetup{detect-weight=true}
    \begin{tabular}{lS[table-format=1.3]S[table-format=2.1]S[table-format=2.1]S[table-format=2.1]S[table-format=2.1]S[table-format=2.1]S[table-format=3.1]S[table-format=2.1]S[table-format=2.1]S[table-format=3.1]}
    \toprule
    \multicolumn{1}{l}{\multirow{2}[2]{*}{\textbf{Model}}} & \multicolumn{1}{c}{\textbf{MC}} & \multicolumn{3}{c}{\textbf{MG}} & \multicolumn{3}{c}{\textbf{FS}} & \multicolumn{3}{c}{\textbf{RS}} \\
    \cmidrule(lr){2-2} \cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11}
          & \multicolumn{1}{c}{\textbf{METEOR}} & \multicolumn{1}{c}{\textbf{EM}} & \multicolumn{1}{c}{\textbf{FTS}} & \multicolumn{1}{c}{\textbf{Valid}} & \multicolumn{1}{c}{\textbf{EM}} & \multicolumn{1}{c}{\textbf{FTS}} & \multicolumn{1}{c}{\textbf{Valid}} & \multicolumn{1}{c}{\textbf{EM}} & \multicolumn{1}{c}{\textbf{FTS}} & \multicolumn{1}{c}{\textbf{Valid}} \\
    \midrule
    \rowcolor{TableSeparator} \multicolumn{11}{c}{\rule{0pt}{10pt} \textbf{Task-Specific, Non-LLM Based Models}}         \\[1pt]
    \multicolumn{1}{p{6.835em}}{SoTA} & 0.515 & 31.7  & 73.2  & 95.3  & 78.7  & 92.2  & 100.0 & 47.0  & 77.5  & 99.7 \\
    \rowcolor{TableSeparator} \multicolumn{11}{c}{\rule{0pt}{10pt} \textbf{Existing LLMs Without Fine-Tuning on \datasetname}}  \\[1pt]
    GPT-4 & 0.188 & 6.4   & 42.6  & 81.4  & 1.6   & 40.5  & 87.0  & 0.0   & 33.4  & 42.6 \\
    \newcontent{Claude 3 Opus} & 0.219 & 12.3  & 57.6  & 92.6  & 3.7   & 45.7  & 97.0  & 1.1   & 46.2  & 94.8 \\
    Galactica & 0.050 & 0.0   & 11.6  & 94.7  & 0.0   & 25.9  & 83.7  & 0.0   & 34.6  & 93.0 \\
    Llama 2 & 0.150 & 0.0   & 4.8   & 93.5  & 0.0   & 13.7  & 97.7  & 0.0   & 27.5  & 87.7 \\
    Code Llama & 0.143 & 0.0   & 8.5   & 95.2  & 0.0   & 15.8  & 99.6  & 0.0   & 25.3  & 97.1 \\
    Mistral & 0.193 & 0.0   & 9.0   & 35.9  & 0.0   & 19.9  & 95.8  & 0.0   & 24.2  & 98.0 \\
    Molinst (\newcontent{chemistry LLM}) & 0.124 & 6.0   & 43.6  & 84.8  & 2.1   & 31.7  & \bfseries 99.8  & 5.7   & 48.0  & 97.8 \\
    \newcontent{ChemLLM} (\newcontent{chemistry LLM}) & 0.050 & 0.9   & 14.3  & 4.3   & 0.0   & 1.6   & 38.5  & 0.0   & 2.9   & 10.9 \\
    \rowcolor{TableSeparator} \multicolumn{11}{c}{\rule{0pt}{10pt} \textbf{Our \modelname Series}}    \\[1pt]
    \modelnameg & 0.394 & 7.7   & 52.2  & 99.6  & 53.1  & 79.9  & 99.7  & 25.7  & 67.0  & 99.9 \\
    \modelnamel & 0.377 & 6.4   & 47.1  & 99.6  & 47.1  & 76.9  & \bfseries 99.8  & 22.5  & 65.2  & 99.9 \\
    \modelnamec & 0.366 & 6.5   & 46.6  & \bfseries 99.7  & 52.0  & 79.2  & \bfseries 99.8  & 25.7  & 66.7  & \bfseries 100.0 \\
    \modelnamem & \bfseries 0.452 & \bfseries 19.2  & \bfseries 61.7  & \bfseries 99.7  & \bfseries 63.3  & \bfseries 84.9  & \bfseries 99.8  & \bfseries 32.9  & \bfseries 70.4  & \bfseries 100.0 \\
    \bottomrule
    \end{tabular}%
  
\end{threeparttable}
}
\label{tab:o_2}%
\end{table*}%

\subsection{\newcontent{Ablation Study}}

To investigate the advantages of \datasetname, we conduct an ablation study by comparing \modelnamem with the following variants: 
(1) \textbf{w/o canonical}, which uses uncanonicalized SMILES, to examine the benefits of canonicalization.
(2) \textbf{using SELFIES}, which uses SELFIES \cite{krenn2019selfies} instead of SMILES to explore their differences.
(3) \textbf{train on Mol-Instructions}, which is trained on Mol-Instructions \citep{fang2023mol}, to compare the performance improvements of our dataset against the previously proposed dataset.

\begin{table*}[t]
\centering
\caption{Results of ablation study on NC and PP tasks. Metrics EM, Valid, and Acc are in percentage. Orange cells represent better results than \modelnamem while blue cells represent worse results.}
\resizebox{0.90\linewidth}{!}{
\begin{threeparttable}[b]
  \centering
  
  \sisetup{detect-weight=true}
    \begin{tabular}{lS[table-format=2.1]S[table-format=2.1]S[table-format=2.1]S[table-format=3.1]S[table-format=2.1]S[table-format=1.3]S[table-format=1.3]S[table-format=2.1]S[table-format=2.1]S[table-format=2.1]S[table-format=2.1]}
    \toprule
    \multicolumn{1}{c}{\multirow{3}[6]{*}{\textbf{Model}}} & \multicolumn{5}{c}{\textbf{NC}}       & \multicolumn{6}{c}{\textbf{PP}} \\
    \cmidrule(lr){2-6}  \cmidrule(lr){7-12}   & \textbf{I2F} & \multicolumn{2}{c}{\textbf{I2S}} & \textbf{S2F} & \textbf{S2I} & \textbf{ESOL} & \textbf{Lipo} & \textbf{BBBP} & \textbf{Clintox} & \textbf{HIV} & \textbf{SIDER} \\
     \cmidrule(lr){2-2} \cmidrule(lr){3-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \cmidrule(lr){7-7} \cmidrule(lr){8-8} \cmidrule(lr){9-9} \cmidrule(lr){10-10}  \cmidrule(lr){11-11}  \cmidrule(lr){12-12}
     & \textbf{EM} & \textbf{EM} & \textbf{Valid} & \textbf{EM} & \textbf{EM} & \textbf{RMSE} $\downarrow$ & \textbf{RMSE} $\downarrow$ & \textbf{Acc} & \textbf{Acc} & \textbf{Acc} & \textbf{Acc} \\
    \midrule
    
    \modelnamem & 87.9  & 70.1  & 99.6  & 93.2  & 29.0  & 1.150 & 1.010 & 74.6  & 93.1  & 96.7  & 70.7 \\
       \quad w/o canonical & \cellcolor[rgb]{ .973,  .796,  .678}88.5 & \cellcolor[rgb]{ .706,  .776,  .906}67.2 & 99.6  & \cellcolor[rgb]{ .973,  .796,  .678}93.4 & \cellcolor[rgb]{ .706,  .776,  .906}24.5 & \cellcolor[rgb]{ .706,  .776,  .906}1.224 & \cellcolor[rgb]{ .706,  .776,  .906}1.072 & \cellcolor[rgb]{ .706,  .776,  .906}71.6 & 93.1  & \cellcolor[rgb]{ .973,  .796,  .678}96.8 & \cellcolor[rgb]{ .706,  .776,  .906}70.3 \\
       \quad using SELFIES & \cellcolor[rgb]{ .706,  .776,  .906}86.9 & \cellcolor[rgb]{ .706,  .776,  .906}47.7 & \cellcolor[rgb]{ .973,  .796,  .678}100.0 & \cellcolor[rgb]{ .973,  .796,  .678}94.7 & \cellcolor[rgb]{ .706,  .776,  .906}19.7 & \cellcolor[rgb]{ .706,  .776,  .906}1.456 & \cellcolor[rgb]{ .706,  .776,  .906}1.106 & \cellcolor[rgb]{ .706,  .776,  .906}69.5 & \cellcolor[rgb]{ .706,  .776,  .906}91.7 & \cellcolor[rgb]{ .706,  .776,  .906}96.5 & \cellcolor[rgb]{ .706,  .776,  .906}64.4 \\
       \quad train on Mol-Instructions & \cellcolor[rgb]{ .706,  .776,  .906}0.0 & \cellcolor[rgb]{ .706,  .776,  .906}0.0 & \cellcolor[rgb]{ .706,  .776,  .906}75.2 & \cellcolor[rgb]{ .706,  .776,  .906}0.0 & \cellcolor[rgb]{ .706,  .776,  .906}0.0 & \cellcolor[rgb]{ .706,  .776,  .906}4.416 & \cellcolor[rgb]{ .706,  .776,  .906}2.282 & \cellcolor[rgb]{ .706,  .776,  .906}0.0 & \cellcolor[rgb]{ .706,  .776,  .906}0.0 & \cellcolor[rgb]{ .706,  .776,  .906}2.6 & \cellcolor[rgb]{ .706,  .776,  .906}0.4 \\
    
    \bottomrule
    \end{tabular}%
  
\end{threeparttable}
}
\label{tab:b_1}%
\end{table*}%

\begin{table*}
\centering
\caption{Results of ablation study on MC, MG, FS, and RS. Metrics EM, FTS, and Valid are in percentage. Orange represents better results than \modelnamem, while blue represents worse results.}
\resizebox{0.90\linewidth}{!}{
\begin{threeparttable}
  \centering
  \sisetup{detect-weight=true}
    \begin{tabular}{lS[table-format=1.3]S[table-format=2.1]S[table-format=2.1]S[table-format=2.1]S[table-format=2.1]S[table-format=2.1]S[table-format=3.1]S[table-format=2.1]S[table-format=2.1]S[table-format=3.1]}
    \toprule
    \multicolumn{1}{l}{\multirow{2}[2]{*}{\textbf{Model}}} & \multicolumn{1}{c}{\textbf{MC}} & \multicolumn{3}{c}{\textbf{MG}} & \multicolumn{3}{c}{\textbf{FS}} & \multicolumn{3}{c}{\textbf{RS}} \\
    \cmidrule(lr){2-2} \cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11}
          & \multicolumn{1}{c}{\textbf{METEOR}} & \multicolumn{1}{c}{\textbf{EM}} & \multicolumn{1}{c}{\textbf{FTS}} & \multicolumn{1}{c}{\textbf{Valid}} & \multicolumn{1}{c}{\textbf{EM}} & \multicolumn{1}{c}{\textbf{FTS}} & \multicolumn{1}{c}{\textbf{Valid}} & \multicolumn{1}{c}{\textbf{EM}} & \multicolumn{1}{c}{\textbf{FTS}} & \multicolumn{1}{c}{\textbf{Valid}} \\
    \midrule
    \modelnamem & 0.452 & 19.2  & 61.7  & 99.7  & 63.3  & 84.9  & 99.8  & 32.9  & 70.4  & 100.0 \\
       \quad w/o canonical & \cellcolor[rgb]{ .973,  .796,  .678}0.457 & \cellcolor[rgb]{ .706,  .776,  .906}16.8 & \cellcolor[rgb]{ .706,  .776,  .906}60.2 & \cellcolor[rgb]{ .706,  .776,  .906}99.1 & \cellcolor[rgb]{ .706,  .776,  .906}53.7 & \cellcolor[rgb]{ .706,  .776,  .906}80.8 & \cellcolor[rgb]{ .973,  .796,  .678}99.9 & \cellcolor[rgb]{ .706,  .776,  .906}23.8 & \cellcolor[rgb]{ .706,  .776,  .906}67.4 & \cellcolor[rgb]{ .706,  .776,  .906}99.9 \\
       \quad using SELFIES & \cellcolor[rgb]{ .973,  .796,  .678}0.466 & \cellcolor[rgb]{ .706,  .776,  .906}16.2 & \cellcolor[rgb]{ .706,  .776,  .906}58.6 & \cellcolor[rgb]{ .973,  .796,  .678}99.9 & \cellcolor[rgb]{ .706,  .776,  .906}40.4 & \cellcolor[rgb]{ .706,  .776,  .906}74.0 & \cellcolor[rgb]{ .973,  .796,  .678}100.0 & \cellcolor[rgb]{ .706,  .776,  .906}25.6 & \cellcolor[rgb]{ .706,  .776,  .906}66.0 & \cellcolor[rgb]{ .706,  .776,  .906}99.9 \\
       \quad train on Mol-Instructions & \cellcolor[rgb]{ .706,  .776,  .906}0.195 & \cellcolor[rgb]{ .706,  .776,  .906}6.1 & \cellcolor[rgb]{ .706,  .776,  .906}46.1 & \cellcolor[rgb]{ .706,  .776,  .906}88.2 & \cellcolor[rgb]{ .706,  .776,  .906}3.9 & \cellcolor[rgb]{ .706,  .776,  .906}37.1 & \cellcolor[rgb]{ .706,  .776,  .906}78.3 & \cellcolor[rgb]{ .706,  .776,  .906}7.4 & \cellcolor[rgb]{ .706,  .776,  .906}52.6 & \cellcolor[rgb]{ .706,  .776,  .906}76.7 \\
    \bottomrule
    \end{tabular}%
  
\end{threeparttable}
}
\label{tab:b_2}%
\end{table*}%

The results in \cref{tab:b_1} and \cref{tab:b_2} lead to the following observations: 
(1) The ``w/o canonical'' model underperforms \modelnamem on most tasks, with a substantial performance drop on FS and RS. This suggests that canonicalizing SMILES can reduce learning difficulty and improve performance. As canonicalization can be easily performed using fixed algorithms before feeding into models, we recommend using canonical SMILES when training and applying LLMs for chemistry.
(2) While using SELFIES slightly improves the validity of generated molecules, which aligns with the motivation behind SELFIES \citep{krenn2019selfies}, the validity of using SMILES is also sufficiently high. Moreover, using SELFIES results in worse performance on most tasks, possibly due to SELFIES being typically longer than SMILES, making it more difficult for the model to accurately understand and generate. Therefore, using SELFIES over SMILES may not be necessary, contrast to claims made in previous work \citep{krenn2019selfies,fang2023mol}. 
(3) Despite using identical base models and training settings, the model trained on Mol-Instructions \citep{fang2023mol} performs much worse than \modelnamem trained on \datasetname  even on the shared tasks (MC, MG, FS, and RS). This demonstrates the superiority of our dataset. A detailed comparison with Mol-Instructions can be found in \cref{app:dataset_comparison}.

To gain deeper insights into the models' performance and behavior, we conduct further analytical experiments:
(1) To investigate the synergistic effects among different tasks, we evaluate models trained on a single task and models with certain tasks removed.
The results demonstrate multiple-task training outperforms single-task training, indicating its benefits. However, each task generally does not heavily rely on the presence of other tasks, suggesting a degree of independence among them.
(2) To investigate the influence of LoRA \citep{hu2021lora} settings, we vary the involved LoRA modules. We observe that adding LoRA modules (and trainable parameters) leads to a substantial boost in performance, indicates the models' great potential for further improvements if with larger-scale fine-tuning.
Please refer to \cref{app:analytical} for more details.

\section{\newcontent{Conclusion}}
While LLMs have shown promise as versatile assistants, their performance on chemistry-related tasks remains notably subpar. 
To address this issue, we introduces \datasetname, a large-scale, comprehensive, and high-quality instruction tuning dataset. It comprises 14 tasks highly relevant to real-world applications and contains over 3M rigorously curated samples. 
Using \datasetname, we develop \modelname, a series of LLMs for performing chemistry tasks.
Our experiments demonstrate \modelname's superiority over existing LLMs, and highlight \datasetname's crucial role in boosting the performance.
Further analytical experiments also provide significant insights towards developing LLMs for chemistry.

However, this work has the following limitations.
First, the evaluations for the MC and MG tasks cannot accurately assess models' abilities to generate chemically correct descriptions and molecules. Since the definition of molecular descriptions remain ambiguous and the available data is limited, it is challenging to assess whether the generated descriptions or molecules are accurate and correct.
Second, this work does not delve into the models' generalization capabilities beyond the trained tasks. While we recognize the importance of such capabilities, how to meaningfully test generalization abilities is nontrivial and needs careful design, which falls outside the purview of this work.
Third, our models do not yet outperform SoTA task-specific models, possibly due to the small ratio of trainable parameters or suboptimal training procedures. 
Nevertheless, we propose a high-quality instruction tuning dataset, demonstrate its effectiveness, and gain deeper insights, which we hope can be valuable for future research.
We will try to address the aforementioned limitations in our future work.

\section*{\newcontent{Ethics Statement}}
Despite our best efforts to maintain the high quality of the \datasetname dataset and the integrity of the \modelname models, we cannot guarantee that the dataset is free of inaccurate, incorrect, or harmful content, nor can we prevent the models from generating such content. Users should engage with our dataset and models at their own discretion and uphold the highest ethical standards in their use.

\section*{Acknowledgement}

The authors would thank colleagues from the OSU NLP group and the OSU Ning Lab for constructive feedback. This research was supported in part by NSF IIS-2133650, NIH 1R01LM014385-01, and NSF CAREER \#1942980, as well as Ohio Supercomputer Center \citep{OhioSupercomputerCenter1987}. The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notice herein.

\newpage

\end{document}