\title{ChemDFM: Dialogue Foundation Model for Chemistry}

\begin{document}

\maketitle
\begin{abstract}
Large language models~(LLMs) have established great success in the general domain of natural language processing.
Their emerging task generalization and free-form dialogue capabilities can greatly
help to design Chemical General Intelligence~(CGI) to assist real-world research in chemistry.
However, the existence of specialized language and knowledge in the field of chemistry, such as the highly informative SMILES notation, hinders the performance of general-domain LLMs in chemistry.
To this end, we develop \textbf{ChemDFM}, the first LLM towards CGI. ChemDFM-13B is trained on 34B tokens from chemical literature, textbooks, and instructions
as well as various data from the general domain. 
Therefore, it can store, understand, and reason over chemical knowledge and languages while still possessing advanced free-form language comprehension capabilities.
Extensive quantitative evaluation shows that ChemDFM can significantly outperform the representative open-sourced LLMs. Moreover, ChemDFM can also surpass GPT-4 on a great portion of chemical tasks, despite the significant size difference.
Further qualitative evaluations demonstrate the efficiency and effectiveness of ChemDFM in real-world research scenarios. We will open-source the ChemDFM model soon.

\end{abstract}
\section{Introduction}

With the rapid development of artificial intelligence~(AI), utilizing AI systems to assist chemical research has garnered increasing attention from researchers~\cite{hatakeyama2023prompt,boiko2023autonomous}. Ideally, AI models can simultaneously handle multiple chemical tasks such as target proposing, property prediction, and reaction analysis, while assisting chemists with real-world experiments through natural language dialogues. In this paper, we call them \textbf{Chemical General Intelligence}~(CGI). To achieve CGI, models need to not only exhibit a diverse range of chemical capabilities but also possess the ability to comprehend and reason in both chemical and natural languages for achieving dialogue-based free-form collaboration with human researchers.

Traditional AI models in chemistry research~\cite{zhou2022uni,edwards-etal-2022-translation,christofidellis2023unifying,liu-etal-2023-molxpt,cao2023instructmol} fall far short of the requirements for CGI.
These models are either limited to some specific tasks, such as single property prediction~\cite{zhou2022uni,wu2023molformer}, or lack of free-form dialogue capabilities. Meanwhile, the emerging field of large language models~(LLMs) has achieved rapid and substantial progress~\cite{touvron2023llama,du-etal-2022-glm,xu-etal-2023-baize}. Numerous studies have demonstrated the extraordinary capabilities of LLMs, encompassing robust natural language understanding and task generalization~\cite{xu-etal-2023-baize,wei2021finetuned}, deducing and reasoning~\cite{wei2022chain,kojima2022large}, and tool-using~\cite{schick2023toolformer,bran2023chemcrow}. Therefore, LLMs have shown promising potential for AGI in general domains, which opens possibilities for the development of CGI.

However, different from general domains, tasks in chemical domains necessitate models to possess additional chemical comprehension capabilities for understanding and reasoning over chemical-specialized language and knowledge.
Specifically, molecules play a vital part in the field of chemistry.
Molecules, as structures of atoms in the 3-dimensional space, have fundamental differences from natural language in terms of information density and conveyance. Therefore, to perform chemical tasks, models need to understand molecular notations, such as SMILES, IUPAC names, and molecular formulas, and further discover the chemical nature of the corresponding molecules.
Due to the lack of these capabilities, current LLMs often fall short of fulfilling the needs of chemical tasks and chemists, with a large performance gap compared to small models.
We argue that CGI models must store and reason about both general-domain knowledge and chemical knowledge as illustrated in Figure~\ref{fig:relation}.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{fig/relation.png}
    \caption{The relation among task-specific models, general-domain LLMs, and Chemical General Intelligence and their capabilities.}
    \label{fig:relation}
    \vspace{-3mm}
\end{figure}

In this work, we detail our progress toward such a CGI and propose \textbf{ChemDFM}, a Dialogue Foundation Model for Chemistry.
ChemDFM takes advantage of the pre-trained LLaMa-13B model~\cite{touvron2023llama} and is continuously trained on web-scale chemical data, including: 1) near 34B tokens from over 3.8M chemical papers and 1.4K textbooks and 2) over 2.7M instructions crafted from various chemical databases. With this extensive and diverse data, we specialized LLaMa with two phases: \textbf{Domain Pre-training}, where the model harvests the chemical knowledge from papers and textbooks, and \textbf{Instruction Tuning}, where the model familiarizes the chemical language and patterns, especially molecule notations. Apart from chemical data, we also incorporate a large amount of general-domain data in both phases. Therefore, ChemDFM is able to acquire chemical knowledge while maintaining comprehension and reasoning capabilities of natural language. Therefore, ChemDFM can perform free-form dialogues in the field of chemistry, thus enabling human-AI collaboration in chemical research.

To illustrate the prowess of ChemDFM, we conduct extensive experiments on two major benchmarks, ChemLLMBench~\cite{guo2023large} and SciEval~\cite{sun2023scieval}. The tasks encompass molecular recognition and grounding, property prediction, reaction analysis, and question-answering. 
Results show that ChemDFM reaches advanced performances, surpassing the typical open-sourced LLMs. It even outperforms GPT-4 on a remarkable portion of the tasks despite the notable difference in model size.
We further compare the performance between ChemDFM and existing LLMs in real-world scenarios. The testing examples are constructed based on the latest chemical papers to avoid data leakage. Results show
that ChemDFM has potent potential for human-AI collaboration in chemical research.
To the best of our knowledge, ChemDFM is the first LLM towards
CGI that possesses
the ability to simultaneously handle a diverse range of tasks as well as
analyze and reason over both chemical and natural languages.

\section{Related Work}

Since the appearance of BERT~\cite{devlin-etal-2019-bert} and GPT~\cite{radfordimproving}, many works have leveraged language models in the field of chemistry to solve various chemical tasks, encompassing property prediction~\cite{zhou2022uni,wu2023molformer},
molecular captioning~\cite{edwards-etal-2022-translation,christofidellis2023unifying}, and reaction predictions in both directions~\cite{schwaller2019molecular,schwaller2020predicting,Toniato2020UnassistedNR}. 
Although small language models can generalize to various chemical tasks with task-specific fine-tuning ~\cite{zeng2022deep,liu-etal-2023-molxpt}, they still suffer from poor task generalization ability and low user interactivity compared to Large Language Models~(LLMs)~\cite{du-etal-2022-glm,touvron2023llama,taylor2022galactica}.
LLMs for Chemistry have become a growing focus of researchers. For example, InstructMol~\cite{cao2023instructmol} adopts Vicuna~\footnote{\url{https://lmsys.org/blog/2023-03-30-vicuna/}} to multiple chemical tasks with task-specific fine-tuning.
ChemCrow~\cite{bran2023chemcrow} leverages chemical tools to help LLM better solve chemical questions. However, previous works are built upon generic LLMs, lacking large-scale pre-training in the domain of chemistry. This deficiency results in the model's lack of chemistry knowledge, making it challenging to achieve satisfactory performance. In contrast, our model, with only 13 billion parameters, has attained performance comparable to GPT-4 through chemical pre-training and instruction tuning.

Due to the extraordinary capabilities of LLMs, numerous works have made attempts to specialize generic LLMs for other different science domains.
For example, 
Med-PaLM~\cite{singhal2023large}
and PMC-LLaMa~\cite{wu2023pmcllama} attempt to specialize LLMs for biology and medicine domains with domain-specific instruction tuning.
ChatDoctor~\cite{li2023chatdoctor} and DrugChat~\cite{liang2023drugchat} also specialize LLMs for medicine domains but focus specifically on medical inquiries and drug discoveries. Other domains on which LLMs have been studied include education~\cite{dan2023educhat}, materials~\cite{xie2023darwin}, and geography~\cite{deng2023k2}. It is worth noticing that most of the formerly mentioned works still focus on the natural language only. The domain-specific languages, such as SMILES, that may differ significantly from the natural language are often overlooked.

\section{ChemDFM}

\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{fig/main.png}
    \vspace{-3mm}
    \caption{The training pipeline and supporting tasks of ChemDFM. The icons are generated by the SDXL model provided by Stability AI.}
    \label{fig:main}
    \vspace{-3mm}
\end{figure*}

In this section, we will introduce the two-stage specialization process for ChemDFM, namely Domain Pre-training~(\cref{sec:pretrain}) and Instruction Tuning~(\cref{sec:instune}).
The overall training pipeline and capabilities of ChemDFM are illustrated in Figure~\ref{fig:main}\footnote{\url{https://stability.ai/}}.

\subsection{Domain Pre-training}\label{sec:pretrain}

The web-scale data used to train general-domain LLMs usually contain knowledge covering a wide range of topics, while being relatively shallow in each. Therefore, they have successfully gained strong natural language understanding and reasoning capabilities, but often fall short when involving in-depth specialized knowledge. Hence, in the domain pre-training stage, we continue to pre-train the base LLM, LLaMa, on our corpus rich in chemical knowledge.

Specifically, our corpus mainly comprises the two most authoritative sources for chemical knowledge: published papers and textbooks. The published papers have undergone peer reviews and therefore can reflect
cutting-edge chemical knowledge, while the textbooks represent the more widely accepted knowledge and basic principles of chemistry. In detail, we filter out published papers which are of chemical-related topics on the Internet before January 2022, as well as collect chemistry books from LibreTexts\footnote{https://libretexts.org/} and Gold Books\footnote{https://goldbook.iupac.org/}. After further pre-processing and deduplication, we get 34B tokens from 3.9M chemical papers and 49M tokens from 1.4K books. To maintain the general-domain knowledge and capabilities of LLMs, we also leverage the corpora from the general domain, including Wikipedia, Arxiv, Books, StackExchange, GitHub code, WuDao Corpora~\cite{yuan2021wudaocorpora}, etc.

We continue to pre-train LLaMa-13B~\cite{touvron2023llama} on our corpus with the help of {\tt Megatron-DeepSpeed}\footnote{\url{https://github.com/microsoft/Megatron-DeepSpeed}} framework. More details about the domain pre-training can be found in the appendix.

\subsection{Instruction Tuning}\label{sec:instune}

The key challenge of LLMs as CGI lies in the fact that information and knowledge in the field of chemistry are not only conveyed through natural language but are also inherently embedded in the notations for molecules and reactions. In fact, the latter usually carries richer and more diverse knowledge.
Therefore, during the instruction tuning stage, our goal is to familiarize ChemDFM with the languages and patterns in the field of chemistry, especially the molecule representations.

SMILES~(short for Simplified Molecular-Input Line-Entry System) is one of the most popular line notations of molecules. It can translate 3-dimensional molecules into flattened sequences while retaining a significant portion of their structures, thereby largely preserving the inherent information and knowledge embedded in the molecules. Therefore, we choose SMILES as the main representation for molecules and construct the chemical instruction tuning dataset.

Specifically, the chemical instruction tuning dataset comprises three main components.
\paragraph{SMILES understanding.} This component mainly focuses on enabling the model to comprehend SMILES and harvest information and knowledge from SMILES. To do so, we introduce three kinds of data:
\begin{enumerate}
    \item \textbf{Molecule description~(MD) and text-based molecule design~(TBMD).} We collect all the molecules with descriptions from PubChem\footnote{\url{https://pubchem.ncbi.nlm.nih.gov/}}, a web-scale chemical database that contains more than 100M compounds.
    Based on these SMILES-description pairs, we instruct the model to generate the description of the molecule or the molecule that fits the description. We repeat the samples whose descriptions have more than 2 sentences twice to further improve the quality of this dataset. In addition, we exclude the data that may appear in the evaluations based on SMILES matching\footnote{All the data mentioned later has also undergone this process. For the sake of conciseness, we will not repeat it later.}.
    \item \textbf{Molecular property prediction~(MPP)}. Based on the widely used molecular property prediction benchmark, Molecule Net~\cite{wu2018moleculenet}, we instruct the model to predict the properties of the given molecule.
    \item \textbf{Reaction completion~(RC)}. Reactions are crucial in terms of understanding the chemical nature of molecules and can also be represented by SMILES. We instruct the model to complete chemical reactions which are masked randomly. Reactions are sampled from USPTO~\cite{lowe2012extraction}, the largest chemical reaction database.
\end{enumerate}

\paragraph{Molecular notation alignment~(MNA).} Apart from SMILES, there are other widely used notations of molecules. Therefore, we instruct the model to conduct translation between them, allowing it to understand these alternative notations. Specifically, we consider another two kinds of notation in this work, IUPAC names and molecular formulas.

\paragraph{Chemical knowledge in natural language.} In real-world usage, researchers may also describe chemical knowledge using natural language.
Therefore, we also include natural language question-answering data specialized in chemistry to enhance the model's capability to process chemistry-related natural language.
Specifically, the data we use can be categorized into two groups. The first group of data is coming from the existing question-answering datasets, encompassing {\tt ARC}~\cite{clark2018think}, {\tt PIQA}~\cite{bisk2020piqa}, {\tt SciQ}~\cite{welbl2017crowdsourcing}, and {\tt HendrycksTest}~\cite{hendryckstest2021}. The second group of data is questions from the exams for middle school students. We collect open-sourced questions of middle school exams through the Internet and construct them into question-answer pairs (along with the key points or problem-solving thoughts if provided) for the instruction tuning of ChemDFM.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{fig/sft-data.png}
    \caption{An example of the final structure of instruction tuning data}
    \label{fig:data}
    \vspace{-3mm}
\end{figure}
\begin{table}[t]
    \centering
    \begin{tabular}{ccc}
    \toprule
        Data Type & \# prompts & Data Source \\
    \midrule
        MD & 575853 & PubChem \\
        TBMD & 575853 & PubChem \\
        MPP & 101753 & MoluculeNet \\
        RC & 299997 & USPTO \\
        MNA & 120000 & PubChem \\
        \multirow{2}{*}{QA from datasets} & \multirow{2}{*}{131004} & ARC, PIQA, SciQ,\\
         & & HendrycksTest \\
        Exam questions & 915162 & Crawled from Internet \\
    \bottomrule
    \end{tabular}
    \caption{The detailed composition of our instruction tuning dataset. MD: Molecule Description, TBMD: Text-Based Molecule Design, MPP: Molecular Property Prediction, RC: Reaction Completion, MNA: Molecular Notation Alignment.}
    \label{tab:data}
    \vspace{-3mm}
\end{table}

To diversify the instructions, we use GPT-4 to rephrase instructions for all the tasks. The number of different instructions for each task ranges from 20 to 200. Finally, to enhance the dialogue capabilities of ChemDFM, all data are constructed in the dialogue format. In summary, all the data samples can be viewed as $(\mathtt{prompt}, \mathtt{returns})$ tuples, where $\mathtt{prompt}$ is composed of dialogue format, instruction, and example input and $\mathtt{returns}$ is the expected return. A detailed example is illustrated in Figure~\ref{fig:data}.

The detailed composition of our instruction tuning dataset is illustrated in Table~\ref{tab:data}. Moreover, to maintain the advanced natural language comprehension capabilities of the model, we also leverage a comparable number of instruction-tuning data in the general domain during the instruction tuning of ChemDFM. The ratio of the data from the chemical and general domains is roughly 1:2. We mix the data of the two domains to get the final dataset and tune our pre-trained ChemDFM on it.

To fully exploit the capabilities of the pre-trained model, we employed full-parameter tuning during the instruction tuning stage. More details about the instruction tuning stage can be found in the appendix.

\section{Evaluation}\label{sec:obj}

We evaluate ChemDFM on two benchmarks designed specifically to assess the performance of LLMs in the field of chemistry, namely ChemLLMBench~\cite{guo2023large} and SciEval~\cite{sun2023scieval}.\footnote{All the metrics we used below are larger-is-better unless otherwise specified.}
ChemLLMBench mainly focuses on the evaluation of chemical capabilities, while SciEval mainly contains science questions asked in natural language.

In this work, we mainly focus on the comparison between LLM-based generalist models to evaluate their capabilities towards CGI. Specifically, we use GPT-4\footnote{\url{https://openai.com/research/gpt-4}} and two typical open-sourced LLMs in terms of AI for science, namely LLaMa-2~\cite{touvron2023llama2} and Galactica~\cite{taylor2022galactica}, as our baselines.

\subsection{ChemLLMBench}\label{chemllmbench}

\begin{table}[t]
    \centering
    \begin{tabular}{lcccc}
    \toprule
    Model & S2I & I2S & S2MF & I2MF  \\
    \midrule
    \rowcolor{grey}\multicolumn{5}{c}{\textit{task-specific specialist models}} \\
    STOUT & 55 & 70 & - & - \\ %~\cite{rajan2021stout}
    \midrule
    \rowcolor{grey}\multicolumn{5}{c}{\textit{LLM-based generalist models}} \\
    GPT-4 & 0 & 1.2 & 8.6 & 8.4 \\
    LLaMa2-13B-chat & 0 & 0 & 1.0 & 0 \\
    Galactica-30B & 0 & 0 & 0 & 0 \\
    \textbf{ChemDFM-13B} & \textbf{4.0} & \textbf{11.0} & \textbf{73.0} & \textbf{51.0} \\
    \bottomrule
    \end{tabular}
    \caption[The Results of name prediction tasks in exact match scores. The baseline results are from . S2I: SMILES to IUPAC names translation, I2S: IUPAC names to SMILES translation, S2MF: SMILES to molecule formulas translation, I2MF: IUPAC names to molecule formulas translation.]{The Results of name prediction tasks in exact match scores. The baseline results are from \citeauthor{guo2023large}\shortcite{guo2023large}. S2I: SMILES to IUPAC names translation, I2S: IUPAC names to SMILES translation, S2MF: SMILES to molecule formulas translation, I2MF: IUPAC names to molecule formulas translation.}
    \label{tab:moltrans}
    \vspace{-3mm}
\end{table}

\begin{table*}[t]
    \centering
    \begin{tabular}{lcccccc}
    \toprule
    Model & BLUE-2 & BLUE-4 & ROUGE-1 & ROUGE-2 & ROUGE-L & METEOR \\
    \midrule
    \rowcolor{grey}\multicolumn{7}{c}{\textit{task-specific specialist models}} \\
    MolXPT~\cite{liu-etal-2023-molxpt} & 0.594 & 0.505 & 0.660 & 0.511 & 0.597 & 0.626 \\
    Text+Chem T5~\cite{christofidellis2023unifying} & 0.625 & 0.542 & 0.682 & 0.543 & 0.622 & 0.648 \\
    Mol-Instruction~\cite{fang2023molinstructions} & 0.249 & 0.171 & 0.331 & 0.203 & 0.289 & 0.271 \\
    InstructMol~\cite{cao2023instructmol} & 0.475 & 0.371 & 0.566 & 0.394 & 0.502 & 0.509 \\
    \midrule
    \rowcolor{grey}\multicolumn{7}{c}{\textit{LLM-based generalist models}} \\
    GPT-4~(10-shot)\textsuperscript{\dag} & \textbf{0.464} & \textbf{0.365} & \textbf{0.545} & \underline{0.362} & \underline{0.459} & \textbf{0.519} \\
    LLaMa-2-13B-chat~(10-shot)\textsuperscript{\dag} & 0.197 & 0.140 & 0.331 & 0.193 & 0.265 & 0.372 \\
    Galactica~(30B)\textsuperscript{\dag} & 0.008 & 0.002 & 0.019 & 0.004 & 0.015 & 0.043 \\
    \textbf{ChemDFM-13B} & \underline{0.446} & \underline{0.291} & \underline{0.490} & \textbf{0.374} & \textbf{0.483} & \underline{0.402} \\
    \bottomrule
    \end{tabular}
    \caption[The Results of molecule captioning. \dag: results from .]{The Results of molecule captioning. \dag: results from \citeauthor{guo2023large}\shortcite{guo2023large}.}
    \label{tab:description}
    \vspace{-3mm}
\end{table*}

ChemLLMBench is a newly proposed benchmark composed of a wide range of chemical tasks to comprehensively evaluate the understanding and reasoning abilities of LLMs in chemistry.
Note that the evaluations in \citeauthor{guo2023large}\shortcite{guo2023large} are conducted on 100 samples randomly sampled from their respective test sets. For the sake of comparability, our evaluations were also conducted on the same 100 samples, unless otherwise specified.\footnote{As the evaluations of task-specific specialist models are usually on full test sets, the performances of task-specific specialist models are listed in the tables only for references. Direct performance comparisons between them and general-domain LLMs are not fair.}

\subsubsection{Molecule Recognition}\label{molrec}

The ability to recognize molecules is essential for CGI models to perform complex chemical tasks. There are two series of tasks in ChemLLMBench that directly evaluate this capability of LLMs, name prediction and molecule captioning.

The results of the two series of tasks are reported in Table~\ref{tab:moltrans} and Table~\ref{tab:description}, respectively. ChemDFM outperforms the open-source LLMs by a significant margin. Specifically, in the name prediction tasks, the zero exact match scores show that other open-sourced LLMs have almost no concept of molecules.
On the other hand, after specialization, ChemDFM can even outperform GPT-4 in all the name prediction tasks, despite the limited size of our model. 
The outstanding performance of ChemDFM proves its strong molecule recognition capability and the effectiveness of our specialization process. As for the molecule description task, ChemDFM also achieves the best performance among the open-source LLMs, while comparable to GPT-4. The results show that ChemDFM can not only recognize the molecule but also infer its underlying chemical essence and nature.

\subsubsection{Molecular Property Prediction}\label{molpp}

The ability to infer properties of molecules is widely needed during the chemical research process. To evaluate the models' molecular property prediction capabilities, ChemLLMBench leverages the widely used benchmark, MoleculeNet~\cite{wu2018moleculenet}, and chooses five typical classification tasks from it. We conduct our evaluation on the same five tasks. However, to increase the difficulty of the tasks, we utilize a more challenging dataset split provided by the DeepChem library~\cite{deepchem}, where the dataset is split
in a scaffold-vertical manner\footnote{Specifically, the molecule is first grouped based on the Bemis-Murcko scaffold representation, and then the splitting makes sure that no molecule in the training set belongs to the same group as any molecule in the test set.}.

\begin{table}[t]
    \centering
    \begin{tabular}{lccccc}
    \toprule
    Model & bace & bbbp & CT & HIV & T21 \\
    \midrule
    \rowcolor{grey}\multicolumn{6}{c}{\textit{task-specific specialist models}} \\
    Uni-Mol & 85.7 & 72.9 & 91.9 & 80.8 & 79.6 \\
    MolXPT & 88.4 & 80.0 & 95.3 & 78.1 & 77.1 \\
    InstructMol & 85.9 & 64.0 & - & 74.0 & - \\
    \midrule
    \rowcolor{grey}\multicolumn{6}{c}{\textit{LLM-based generalist models}} \\
    GPT-4 & 62.5 & 61.5 & 51.6 & 65.9 & 55.2 \\
    LLaMa-2-13B-chat & 26.0 & 60.3 & 45.7 & 29.0 & 51.7 \\
    Galactica~(30B) & 72.7 & 59.6 & 82.2 & \textbf{75.9} & 68.5 \\
    \textbf{ChemDFM-13B} & \textbf{78.4} & \textbf{66.7} & \textbf{89.9} & 73.6 & \textbf{79.8} \\
    \bottomrule
    \end{tabular}
    \caption[The Results of molecular property prediction tasks in AUC-ROC scores. AUC-ROC stands for the Area Under the Curve of the Receiver Operating Characteristic. The results of Uni-Mol, MolXPT, InstructMol, and Galactica are from , , , and , respectively. Others are reproducing results. CT: ClinTox, T21: Tox21.]{The Results of molecular property prediction tasks in AUC-ROC scores. AUC-ROC stands for the Area Under the Curve of the Receiver Operating Characteristic. The results of Uni-Mol, MolXPT, InstructMol, and Galactica are from \citeauthor{zhou2022uni}\shortcite{zhou2022uni}, \citeauthor{liu-etal-2023-molxpt}\shortcite{liu-etal-2023-molxpt}, \citeauthor{cao2023instructmol}\shortcite{cao2023instructmol}, and \citeauthor{taylor2022galactica}\shortcite{taylor2022galactica}, respectively. Others are reproducing results. CT: ClinTox, T21: Tox21.}
    \label{tab:molnet}
    \vspace{-3mm}
\end{table}

The results are illustrated in Table~\ref{tab:molnet}. The Area Under the Curve of the Receiver Operating Characteristic~(AUC-ROC) metric is introduced to tackle the significant label imbalance in these tasks.
In general, ChemDFM outperforms the LLMs on almost all the tasks including GPT-4. These results demonstrate that ChemDFM better establishes the capability to infer molecular properties, reflecting its enhanced prowess to identify and understand the underlying chemical essence of molecules.

\subsubsection{Text-Based Molecule Design}\label{moldes}

To evaluate the capability of making qualified molecule designs, ChemLLMBench reverses the above-mentioned molecule description tasks and asks the models to generate the molecule based on its description.

\begin{table*}[t]
    \centering
    \begin{tabular}{lccccccc}
    \toprule
    Model & Exact & BLUE & Dis~($\downarrow$) & Validity & MACCS & RDK & Morgan \\
    \midrule
    \rowcolor{grey}\multicolumn{8}{c}{\textit{task-specific specialist models}} \\
    MolXPT~\cite{liu-etal-2023-molxpt} & 21.5 & - & - & 98.3 & 0.859 & 0.757 & 0.667 \\
    Text+Chem T5~\cite{christofidellis2023unifying} & 32.2 & 0.853 & 16.87 & 94.3 & 0.901 & 0.816 & 0.757 \\
    Mol-Instruction~\cite{fang2023molinstructions} & 0.2 & 0.345 & 41.4 & 100 & 0.412 & 0.231 & 0.147 \\
    \midrule
    \rowcolor{grey}\multicolumn{8}{c}{\textit{LLM-based generalist models}} \\
    GPT-4~(10-shot)\textsuperscript{\dag} & 17.4 & 0.816 & 21.2 & 88.8 & 0.867 & 0.738 & 0.672 \\
    LLaMa-2-13B-chat~(10-shot)\textsuperscript{\dag} & 2.0 & 0.626 & 34.0 & 78.2 & 0.679 & 0.568 & 0.454 \\
    Galactica~(30B)\textsuperscript{\dag} & 0.0 & 0.004 & 2738 & 95.6 & 0.233 & 0.109 & 0.053 \\
    \textbf{ChemDFM-13B} & \textbf{45.0} & \textbf{0.874} & \textbf{9.9} & \textbf{98.0} & \textbf{0.922} & \textbf{0.871} & \textbf{0.798} \\
    \bottomrule
    \end{tabular}
    \caption[The Results of text-based molecule design. Dis: Levenshtein distance. \dag: results from .]{The Results of text-based molecule design. Dis: Levenshtein distance. \dag: results from \citeauthor{guo2023large}\shortcite{guo2023large}.}
    \label{tab:design}
    \vspace{-3mm}
\end{table*}

The results are shown in Table~\ref{tab:design}. ChemDFM outperforms not only the generalist LLMs but also the traditional task-specific specialist models on almost all the matrix.\footnote{To achieve fair comparison with task-specific specialist models, we additionally evaluate ChemDFM on the full test set. The results can be found in the appendix.} On the one hand, the results demonstrate that our specialization process has effectively helped the LLMs to establish the relationship between the SMILES notations (which roughly represent the structures of molecules) and the chemical nature of the compound. Therefore, our model can outperform the LLMs including GPT-4, despite the notable gap in model size.
On the other hand, with the help of the strong natural language comprehension capability inherited and preserved from LLaMa, ChemDFM can not only better understand the chemical information in the descriptions but also establish connections between knowledge in different tasks. Therefore, ChemDFM can build a more comprehensive knowledge system in chemistry, thereby outperforming the task-specific specialist models.

\subsubsection{Reaction Prediction and Retrosynthesis}\label{reaction}

Chemical reaction is a key component of the chemical world. The capability to understand chemical reactions is more challenging but also necessary for chemical AGIs.
In ChemLLMBench, there are four types of tasks targeted at evaluating models' capabilities of reaction understanding, 
encompassing Yield Prediction~(YP), Reaction Prediction~(RP), Reagent Selection~(RS), and Retrosynthesis~(Retro).

\begin{table}[t]
    \centering
    \begin{tabular}{lcccc}
    \toprule
    Model & YP & RP & RS & Retro \\
    \midrule
    \rowcolor{grey}\multicolumn{5}{c}{\textit{task-specific specialist models}} \\
    Advanced Results\textsuperscript{*} & 96.1 & 93.8 & - & 53.6 \\
    \midrule
    \rowcolor{grey}\multicolumn{5}{c}{\textit{LLM-based generalist models}} \\
    GPT-4\textsuperscript{\dag} & \underline{78.2} & \underline{23.0} & \textbf{45.3} & \underline{11.4} \\
    LLaMa-2-13B-chat\textsuperscript{\dag} & 0.7 & 3.2 & 16.0 & 0.0 \\
    Galactica~(30B)\textsuperscript{\dag} & 0.4 & 3.6 & 8.0 & 1.6 \\
    \textbf{ChemDFM-13B} & \textbf{81.0} & \textbf{49.0} & \underline{23.7} & \textbf{12.0} \\
    \bottomrule
    \end{tabular}
    \caption[The Results of reaction prediction and retrosynthesis tasks. We report the average accuracy of each task group. Please refer to the appendix for the complete results. YP: Yield Prediction, RP: Reactant Prediction, RS: Reagent Selection, Retro: Retrosynthesis. *: advanced results of different specialist models (YP: UAGNN, RP \& Retro: Chemformer) \dag: results from .]{The Results of reaction prediction and retrosynthesis tasks. We report the average accuracy of each task group. Please refer to the appendix for the complete results. YP: Yield Prediction, RP: Reactant Prediction, RS: Reagent Selection, Retro: Retrosynthesis. *: advanced results of different specialist models (YP: UAGNN~\cite{kwon2022uncertainty}, RP \& Retro: Chemformer~\cite{irwin2022chemformer}) \dag: results from \citeauthor{guo2023large}\shortcite{guo2023large}.}
    \label{tab:reaction}
    \vspace{-3mm}
\end{table}

The results are illustrated in Table~\ref{tab:reaction}. ChemDFM can significantly outperform the open-sourced LLMs.
The superior performance shows that with the help of our specialization process, ChemDFM can establish the basic sense of chemical interaction between molecules while LLaMa-2 and Galactica can not.
It is worth noticing that our ChemDFM can also outperform GPT-4 on most of the tasks, which indicates the significant effectiveness of our specialization process.

\subsection{SciEval}

\begin{table}[t]
    \centering
    \begin{tabular}{lcccc}
    \toprule
    Model & Bio & Chem & Phy & Avg \\
    \midrule
    \rowcolor{grey}\multicolumn{5}{c}{\textit{LLM-based generalist models}} \\
    GPT-4 & \textbf{84.49} & \textbf{69.38} & \textbf{65.22} & \textbf{73.93} \\
    Galactica~(30B) & 66.48 & 50.16 & 44.65 & 54.96 \\
    LLaMa-2-13B-chat & \underline{68.08} & 47.90 & 45.47 & 54.33 \\
    \textbf{ChemDFM-13B} & 67.98 & \underline{54.66} & \underline{47.29} & \underline{58.25} \\
    \bottomrule
    \end{tabular}
    \caption[The Results of SciEval benchmark, where Bio, Chem, and Phy stands for biology, chemistry, and physics, respectively. The baseline results are from .]{The Results of SciEval benchmark, where Bio, Chem, and Phy stands for biology, chemistry, and physics, respectively. The baseline results are from \citeauthor{sun2023scieval}\shortcite{sun2023scieval}.}
    \label{tab:scieval}
    \vspace{-3mm}
\end{table}

SciEval is a newly proposed benchmark to evaluate the capabilities of LLMs targeted at scientific domains.
Specifically, it is mainly composed of knowledge-intense questions in the fields of physics, chemistry, and biology.

The results are illustrated in Table~\ref{tab:scieval}. As an AGI in the field of chemistry, ChemDFM achieves the best performance among the open-sourced LLMs in the chemistry sub-task, showing the effectiveness of our specialization process.
Moreover, due to the general domain data integration in both domain pre-training and instruction tuning stages, ChemDFM can largely preserve acquired capabilities and knowledge when learning new domain-specific knowledge of chemistry. Therefore, ChemDFM can also achieve comparable or even better performances in the fields of biology and physics, thereby resulting in a better overall performance.
\section{Qualitative Analysis}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{fig/one2.png}
    \caption{The examples of paper reading scenario. We mark \textcolor{green}{correct and relevant information} in the replies in green, \textcolor{yellow}{the correct but irrelevant information} in yellow, and \textcolor{red}{the wrong information} in red. In addition, \textbf{the key points of the answer} are marked in bold if they appear in the reply. Due to space constraints, some content has been omitted~([...] in the paragraphs). Please refer to the appendix to find the full replies.}
    \label{fig:one}
    \vspace{-3mm}
\end{figure*}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{fig/multi3.png}
    \caption{The example showing the potential of ChemDFM to assist researchers in experimental design through dialogue-based human-AI collaboration. We highlight \textbf{the key points} in bold.}
    \label{fig:multi}
    \vspace{-3mm}
\end{figure}

In addition to the chemical and natural language comprehension and reasoning abilities evaluated in Section~\cref{sec:obj}, another crucial and challenging capability for CGI is free-form human-AI collaboration in real-world scenarios. Models need to establish a universal language protocol with human researchers where both chemical language (such as SMILES) and natural language are involved.  In this section, we will evaluate the performance of our model in two typical scenarios, paper reading~(\cref{sec:one}) and experimental design~(\cref{sec:multi}). Notably, we randomly select chemistry papers published in 2023 and constructed most of the questions and dialogues based on their content. In this way, we get novel scenarios that are not exposed to ChemDFM in its training.

\subsection{Paper Reading}\label{sec:one}

During paper reading, researchers may encounter questions hindering them from fully understanding the papers. Therefore, to be a practical CGI model, ChemDFM needs to possess the capabilities to answer these questions that are often unforeseen and frequently involve new reactions or molecules. In this section, we evaluate ChemDFM's performance in the paper reading scenario and compare it with other typical LLMs. Figure~\ref{fig:one} lists the typical examples and corresponding results.
More examples are listed in the appendix.

The results show that while open-sourced LLMs perform well when asked about existing knowledge~(\textbf{Q1}), only ChemDFM can provide correct and comprehensive answers when questions involve new molecules and reactions~(\textbf{Q2}~\cite{yin2023total} \& \textbf{Q3}~\cite{dargo2023mesesamol}). Specifically, LLaMa-2 and Galactica primarily rely on retrieving knowledge from memory, resulting in numerous correct knowledge points but irrelevant or even unusable under the situations of the questions. In contrast, ChemDFM can apply its acquired chemical knowledge to identify and comprehend unknown molecules and reactions, thereby solving researchers' problems. Moreover, apart from answering the key point, ChemDFM will also attempt to elaborate on the mechanism of the asked reactions or proposed solutions, making its answers more detailed but occasionally leading to errors. We also test the same questions on GPT-4. Results indicate that GPT-4 has the capability to integrate memory-based knowledge with real-world scenarios. However, it still performed poorly in Q3 compared with ChemDFM, showcasing the strong real-world problem-solving capabilities of ChemDFM. 
Please refer to the appendix to find the detailed analysis of each question.

\subsection{Experimental design}\label{sec:multi}

Experiments are the fundamental component of chemical research. The capability to assist chemists during experiments is indispensable for a CGI. In this section, we use one unexposed example inspired by \citeauthor{yin2023total}\shortcite{yin2023total} to demonstrate ChemDFM's potential to assist researchers in experimental design through dialogue-based human-AI collaboration. More examples can be found in the appendix.

The collaboration process is illustrated in Figure~\ref{fig:multi}. During the dialogue, the researcher wants to selectively oxidize one of the two carbonyl groups of a molecule. However, the initial solution given by ChemDFM results in both carbonyl groups being oxidized. Through the correction given by the researcher, ChemDFM adjusts its proposal and provides two possible solutions. Finally, the researcher chooses to use protecting groups and ChemDFM further details its advice.

In the process, ChemDFM shows promising capabilities regarding error correction~(Round 2) and detailing~(Round 3).
This dialogue demonstrates the great prowess of ChemDFM to comprehend both natural language and chemical language. Through this prowess, ChemDFM can establish the universal language protocol with human researchers to achieve meaningful human-AI collaboration.

\section{Conclusion}

In this paper, we introduce ChemDFM, a pioneer attempt towards Chemical General Intelligence~(CGI). Through domain pre-training and instruction tuning, ChemDFM has established strong comprehension and reasoning capabilities for chemical knowledge and patterns, leading to advanced performance in chemical tasks such as molecular design, reaction analysis, and knowledge-intense question-answering. Besides, ChemDFM also possesses strong abilities in comprehending both chemical and natural languages, which enables it to assist researchers in real-world scenarios through dialogue-based free-form human-AI collaboration. We will open-source the ChemDFM model and encourage researchers from both AI and chemistry communities to explore it.

As the primary attempt towards CGI, ChemDFM has much room for improvement. For example, considering that there are various informative modalities in chemistry, such as molecular graphs and spectroscopies, we believe multi-modalities are necessary for CGI. In addition, tool-using methods are also worth exploring, as they can significantly improve the reliability of LLMs. We leave these as future work.

\end{document}