\title{Large Language Models are In-Context Molecule Learners}

\begin{document}

\maketitle

\begin{abstract}
Large Language Models (LLMs) have demonstrated exceptional performance in biochemical tasks, especially the molecule caption translation task, which aims to bridge the gap between molecules and natural language texts.
However, previous methods in adapting LLMs to the molecule-caption translation task required extra domain-specific pre-training stages, suffered weak alignment between molecular and textual spaces, or imposed stringent demands on the scale of LLMs.
To resolve the challenges, we propose \textbf{I}n-\textbf{C}ontext \textbf{M}olecule \textbf{A}daptation (\textbf{ICMA}), as a new paradigm allowing LLMs to learn the molecule-text alignment from context examples via In-Context Molecule Tuning. 
Specifically, ICMA incorporates the following three stages: Hybrid Context Retrieval, Post-retrieval Re-ranking, and In-context Molecule Tuning.
Initially, Hybrid Context Retrieval utilizes BM25 Caption Retrieval and Molecule Graph Retrieval to retrieve informative context examples. 
Additionally, we also propose Post-retrieval Re-ranking with Sequence Reversal and Random Walk to further improve the quality of retrieval results. 
Finally, In-Context Molecule Tuning unlocks the in-context molecule learning capability of LLMs with retrieved examples and adapts the parameters of LLMs for the molecule-caption translation task.
Experimental results demonstrate that ICMA can empower LLMs to achieve state-of-the-art or comparable performance without extra training corpora and intricate structures, showing that LLMs are inherently in-context molecule learners.
\end{abstract}
 

\section{Introduction}
Molecules play a crucial role across various fields, such as medicine ~\cite{ding2019selective}, agriculture ~\cite{twyman2003molecular}, and material science ~\cite{higuchi2023material}, as they are widely used in the development of drugs, fertilizers, and advanced materials. Recently, LLMs have demonstrated remarkable success in the molecular domain, as molecules can be represented as Simplified Molecular-Input Line-Entry System (SMILES) strings ~\cite{weininger1988smiles}, which can be comprehended and generated by LLMs in a similar manner to natural languages. 
To further bridge the gap between the molecules and natural languages, ~\citet{edwards-etal-2022-translation} proposes the molecule-caption translation task, which comprises two sub-tasks: molecule captioning (Mol2Cap) and text-based de novo molecule generation (Cap2Mol). Specifically, Mol2Cap involves generating a textual description that elucidates the features of the given molecule, while Cap2Mol focuses on predicting the exact molecule based on the textual caption. The study of the molecule-caption translation task offers an accessible and chemist-friendly venue for molecule discovery, which has raised wide research focus.
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/introduction.pdf}
    \caption{An illustration of three similar molecules alongside their molecule captions. The molecules are represented as both SMILES strings and graphs, while the molecule captions elucidate their structures and functions. Here, the three molecules are similar, considering their 2D graph embeddings, and the overlaps in their captions are highlighted in blue and pink.}
    \label{fig:intro}
    \vskip -0.2in
\end{figure}

Generally, there are two main paradigms for adapting LLMs to the molecule-caption retrieval task. 
The first paradigm is the domain-specific pre-training \& fine-tuning. For instance, MolT5 ~\cite{edwards-etal-2022-translation} first proposes and handles the molecule-caption translation task as the language translation task, pre-training the MolT5 model with chemical corpora like PubChem ~\cite{kim2019pubchem} and then fine-tuning the model on the ChEBI-20 dataset ~\cite{edwards2021text2mol}. Additionally, MoMu ~\cite{su2022molecular} and MolCA ~\cite{liu2023molca} introduce an extra modality alignment stage before fine-tuning on downstream tasks, which aligns the output of 2D molecule graph encoder with the input space of LLMs.
In contrast, the other paradigm involves prompting and utilizing the in-context learning capability of LLMs. For example, MolReGPT ~\cite{li2023empowering} introduces In-Context Few-Shot Molecule Learning, prompting general LLMs like GPT-3.5 and GPT-4 to achieve competitive performance without extra parameter adaptations.

However, the current paradigms still face challenges. On one hand, the domain-specific pre-training \& fine-tuning paradigm requires extra pre-training stages (i.e., domain-specific pre-training and modality alignment), which is challenging due to the scarcity of high-quality chemical datasets, especially molecule-caption pairs, making this paradigm difficult to scale up to the most advanced LLMs with billion parameters.
Besides, the domain-specific pre-training \& fine-tuning paradigm also suffers from weak alignment between molecules and texts, as phrases in molecule captions often indicate specific sub-structures of molecules rather than the entire molecule.
Despite attempts to introduce extra modalities for better alignment \cite{su2022molecular, liu2023molca}, the integration of the additional modalities (e.g., 2D molecule graph) is still focused on the entire graph level and can only be applied to the molecule captioning task, while ignoring the text-based generation of molecules, which is much more valuable for drug discovery.
On the other hand, the in-context learning \& prompting paradigm puts a harsh requirement on LLMs' emergent capabilities, such as reasoning and in-context learning abilities. However, LLMs with these emergent capabilities usually have billions of parameters, making them computationally expensive.
Consequently, there is a demand for a unified and efficient approach that effectively enhances the performance of the most advanced LLMs in both two sub-tasks of molecule-caption translation.

In this case, we propose \textbf{I}n-\textbf{C}ontext \textbf{M}olecule \textbf{A}daptation (\textbf{ICMA}) as a new paradigm for adapting LLMs in molecule-caption translation. 
Different from previous paradigms, ICMA aims to instruct LLMs to derive knowledge from informative context examples, especially the alignment between molecule SMILES representations and captions, via In-Context Molecule Tuning. As shown in Figure \ref{fig:intro}, similar molecules often share similar properties, as indicated by the overlaps among molecule captions. Conversely, similar captions tend to describe molecules with similar SMILES representations. In this case, with ICMA, general LLMs could fulfill their reasoning and in-context learning capability to better grasp the alignment between molecules and textual captions from context examples, thereby achieving better performance.

Specifically, ICMA incorporates three stages: Hybrid Context Retrieval, Post-retrieval Re-ranking, and In-context Molecule Tuning.
In the initial stage, Hybrid Context Retrieval, we employ Caption Retrieval and Molecule Graph Retrieval to fetch similar captions and molecules, respectively.
Subsequently, we introduce the Post-retrieval Re-ranking stage to enhance the quality of the retrieval algorithms. This stage incorporates two innovative strategies: Sequence Reversal and Random Walk, which aim to refine and reprioritize the retrieved examples.
Finally, we apply In-context Molecule Tuning to adapt the parameters of LLMs, enabling them to learn from the contextual mappings and effectively utilize the learned knowledge for the current generation task.
Experiments are conducted across two real-world molecule-caption translation datasets, ChEBI-20 and PubChem324k. Results show that ICMA could enable LLMs to achieve state-of-the-art or comparable performance in both the two sub-tasks (i.e., Mol2Cap and Cap2Mol). Meanwhile, we also study the factors related to the model performance, including retrieval algorithms, context settings (i.e., context example number and maximum input length), and model scales. Lastly, ablation studies are conducted to justify the effectiveness of Post-retrieval Re-ranking components.

Our contribution mainly lies in:
\begin{itemize}
    \item We propose In-context Molecule Adaptation (ICMA) to improve the performance of LLMs in the molecule-caption translation task. ICMA could empower the reasoning and in-context learning capabilities of LLMs for better alignment between molecules and texts.
    \item We implement ICMA through three stages, including Hybrid Context Retrieval, Post-retrieval Re-ranking, and In-Context Molecule Tuning, significantly enhancing the informativeness of context examples.
    \item We conduct synthetic experiments, and the results show that our method enables LLMs to outperform previous paradigms, enabling better alignment between molecules and texts. Notably, our approach elevates the Mistral-7b model to establish state-of-the-art or comparable performance across both the two sub-tasks of molecule-caption translation, achieving 0.581 BLEU-4 score in Mol2Cap and 0.460 exact-matched score in Cap2Mol. Additionally, we comprehensively study the mechanism and influential factors of ICMA, showing that LLMs are inherently in-context molecule learners.
\end{itemize}

\section{Related Work}
\label{sec:relatedwork}
In this section, we discuss the related work of molecule-caption translation and the development of in-context learning.

\subsection{Molecule-Caption Translation}
Inspired by the image captioning task, \citet{edwards2021text2mol} introduces a new dataset, ChEBI-20, with pairs of molecules and manually labeled captions that describe the molecular properties. The molecule-caption translation task was initially proposed in MolT5~\cite{edwards-etal-2022-translation}. Meanwhile, MolT5 proposes a T5 model that is jointly pre-trained on molecule SMILES and general text corpus.
MolXPT ~\cite{liu2023molxpt} pre-trains a GPT model by introducing extra-wrapped texts as the pre-training corpus, demonstrating better molecule-text alignment.
However, the generation of SMILES strings suffers from the problem of invalid SMILES due to the mismatches of brackets.
To overcome the generation issue of SMILES strings, BioT5 ~\cite{pei2023biot5} introduces Self-referencing Embedded Strings (SELFIES) ~\cite{krenn2020self} instead of SMILES strings to represent molecules in LLMs and proposes the BioT5 model that is jointly trained on single-modal data, wrapped text data, and molecule/protein-description pairs.
Meanwhile, as molecules can also be represented as graphs, some methods focus on molecule understanding by introducing molecule graph information.
For example, MoMu ~\cite{su2022molecular} first proposes a graph encoder to encode molecule graph information and utilizes contrastive learning to bridge the semantic gap between the graph encoder and the LLM.
To better fuse the molecule graph information, MolCA ~\cite{liu2023molca} follows the BLIP-2 and utilizes a Q-Former to project the output of the graph encoder into the LLMs, showing better molecule understanding performance.
However, most of these methods still adhere to the pre-training \& fine-tuning paradigm, which necessitates the joint pre-training of LLMs on both general text and extra chemical domain corpora. As the size of the training corpora and model weights of LLMs continue to increase, this approach has become extremely inefficient.
To address this issue, MolReGPT ~\cite{li2023empowering} proposes the In-Context Few-Shot Molecule Learning to enable LLMs to learn the molecule-caption translation task from the context examples without modifying the model weights while still achieving comparable performance to these fine-tuned methods.

\subsection{In-Context Learning}
With the scaling of model size and corpus size ~\cite{brown2020language, chowdhery2023palm}, LLMs emerge the in-context learning capability ~\cite{wang2022self}, which enables LLMs to learn from contexts augmented with several examples ~\cite{dong2022survey}. 
By utilizing the capability of ICL, LLMs can solve complex tasks without the necessity of being fine-tuned. 
For instance, with a few examples, GPT-3 could demonstrate similar performance to fine-tuned models in unseen tasks ~\cite{brown2020language}.
What's more, based on context examples, LLMs could achieve better mathematical reasoning ability with the assistance of chain-of-thought (CoT) ~\cite{wei2022chain}.
With the powerful in-context learning capabilities, ~\citet{edwards2023synergpt} proposes in-context drug synergy learning to apply LLMs for personalized drug synergy prediction and drug design, while ~\citet{jablonka2024leveraging} fine-tunes LLMs such as GPT-3 with chemical questions and answers to solve predictive tasks
In the scenario of molecule-caption translation, MolReGPT ~\cite{li2023empowering} proves the retrieval quality is closely related to the model performance, while it still imposes strict requirements on the model scale, as the in-context learning capability only becomes apparent when the model weights reach a certain size.\section{In-Context Molecule Adaptation}
\label{sec:methodlogy}
\begin{figure*}[htb]
    \centering
    \vskip -0.2in
    \includegraphics[width=\textwidth]{figures/model_structure.pdf}
    \vskip -0.1in
    \caption{Framework of In-Context Molecule Adaptation (ICMA). Generally, ICMA consists of three stages, Hybrid Context Retrieval, Post-retrieval Re-ranking, and In-Context Molecule Tuning. 
    }
    \label{fig:model}
    \vskip -0.2in
\end{figure*}

In this section, we introduce In-Context Molecule Adaptation (ICMA) as a novel paradigm to adapt LLMs to molecule-caption translation. As shown in Figure~\ref{fig:model}, ICMA incorporates three stages, including Hybrid Context Retrieval, Post-retrieval Re-ranking, and In-context Molecule Tuning. Specifically, Hybrid Context Retrieval first retrieves $N$ rough examples from the training set $\mathcal{D}$ by calculating the similarity between the current query and molecule-caption pairs. After that, Post-retrieval Re-ranking with Random Walk and Sequence Reversal is adopted to obtain $n$ refined examples from the $N$ rough examples. Finally, the In-context Molecule Tuning can be performed to update the parameters of LLMs to learn the molecule-text alignment from refined context examples. 

\subsection{Hybrid Context Retrieval}
The retrieval quality is closely related to the informativeness of context examples. For example, if the retrieved molecules are more similar to the current query molecule, they are likely to exhibit more overlaps in their respective caption, which could enable better alignments between molecules and texts. Therefore, the development of retrieval algorithms plays a crucial role in ICMA. In this work, we introduce Hybrid Context Retrieval, which adopts hybrid modalities (i.e., 2D molecule graph and text), as well as hybrid retrieval algorithms designed for the specific tasks (i.e., Molecule Graph Retrieval for Mol2Cap and BM25 Caption Retrieval for Cap2Mol). 

For the \textbf{Mol2Cap} task, ICMA adopts Molecule Graph Retrieval to better refine the retrieval quality.
Previously, MolReGPT ~\cite{li2023empowering} utilizes the Morgan Fingerprints (Morgan FTS) and Dice similarity to evaluate the similarity between molecules, which encodes the pre-defined handcraft structures as the embedding of the molecule. However, Morgan FTS are typically based on handcrafted chemical feature extraction, which may have limited capability to capture the comprehensive information of complex molecule structures and properties. On the other side, Graph Neural Networks (GNNs), pre-trained on millions of molecules, could effectively capture topological structures of molecule graphs, providing complete chemical semantics~\cite{ijcai2023p760}. This makes GNNs a better option for molecule similarity calculation.
In ICMA, we adopt a pre-trained GNN encoder to obtain the molecule graph embeddings:
\begin{align}
    \textbf{e}_m = G\!N\!N(g_m),
\end{align}
where $\textbf{e}_m$ denotes the embedding of the given 2D graph $g_m$ of the molecule $m$. Specifically, we adopt Mole-BERT ~\cite{xia2022mole} as the GNN encoder. 

Subsequently, cosine similarity is leveraged to evaluate the similarity between the current query molecule graph $m^q$ and the other molecule graphs $m^i$ in the training set of molecules ($m^i \in \mathcal{D}_m$). Thus, the molecule similarity ranking function $\mathcal{R}^m$ can be represented as:
\begin{align}
    \mathcal{R}^m(m^q, m^i) = \cos{(\textbf{e}_{m^q}, \textbf{e}_{m^i})}.
\end{align}
In the \textbf{Cap2Mol} task, we inherit the BM25 Caption Retrieval ~\cite{robertson2009probabilistic} from MolReGPT ~\cite{li2023empowering} as it focuses on the detail matching of molecule captions, showing competitive performance and is much faster than LLM-based methods like Sentencebert ~\cite{reimers2019sentence}. Specifically, given captions in the test set as query captions $Q_c$ and the training set of captions $\mathcal{D}_c$, the caption similarity ranking function $\mathcal{R}^c$ can be denoted as:

{\tiny
\begin{align}
    \mathcal{R}^c\!(\!Q_c\!,\!\mathcal{D}_c\!)\!=\!\sum_{i=1}^T\!I\!D\!F(\!c^q_i\!)\!*\!\frac{t\!f(c^q_i,\mathcal{D}_c)*(k_1+1)}{t\!f(\!c^q_i,\!\mathcal{D}_c\!)\!+\!k_1\!*\!(\!1\!-\!b\!+\!b\!*\!\frac{|\mathcal{D}_c|}{avgdl}\!)},
\end{align}
}

\noindent where $T$ is the number of query terms in the query caption, $c^q_i$ is the $i$-th query term, $I\!D\!F(c^q_i)$ is the inverse document frequency of $c^q_i$, $t\!f(c^q_i, \mathcal{D}_c)$ is the term frequency of $c^q_i$ in $\mathcal{D}_c$, $k_1$ and $b$ are hyperparameters, $|\mathcal{D}_c|$ is the length of $\mathcal{D}_c$, and $avgdl$ is the average caption length in the corpus.

\subsection{Post-retrieval Re-ranking}
Although refined retrieval algorithms could bring better retrieval quality, there are still some problems considering the arrangement of context examples. Thus, we propose Post-retrieval Re-ranking with Random Walk and Sequence Reversal to re-rank the priorities and positions of context examples, thus enhancing In-Context Molecule Tuning quality.

\subsubsection{Random Walk}
The molecules ranked top by retrieval algorithms can sometimes share too many overlaps, impairing the informativeness of context examples. 
In this case, for the context diversity and generalization performance of ICMA, it is necessary to give these less similar examples a chance to be visited.
Inspired by the random walk mechanism in graph theory, we propose random walk as a post-retrieval method to select examples from the top-$N$ retrieved results so that examples with lower rank still have a chance to be selected, which provides more useful information and complements the context diversity.
Mathematically, we adopt a dynamic chance for examples with different ranks. Specifically, for the $j$-th example in the $N$ rough results, where $1\leq j \leq $N, the possibility of skipping is represented as $(9*\frac{N-j}{N-1}) \%$, which reduces as the rank moves down to ensure that at least one example is selected for the context. 
If the $j$-th example is skipped, then it will perform a Random Walk in the remaining examples until an example is selected. Practically, if $N$ is too large (i.e., $N \gg n$), the retrieval quality will be impaired, while if $N$ is too small (i.e., $N \approx n$), it might hurt the diversity. Therefore, considering $n$ is not quite large and usually less than 5 due to the context length limitation, $N$ is set to 10 in this work.

\subsubsection{Sequence Reversal}
Due to the training strategy and the inherent characteristic of natural languages, LLMs have difficulty capturing long-range dependencies or relationships between words that are far apart in the input text, namely the distance dependency. The positions of examples in the context might influence the generation results due to the distance dependency of LLMs ~\cite{frermann-etal-2023-conflicts}. In this case, it is of significance to put the most informative example exactly near the current input query.

Formally, given the context examples, previous works like MolReGPT tend to organize the input text by directly fitting them into the context. Therefore, the context could be represented as:

{\small
\vskip -0.2in
\begin{align}
    \mathcal{P}(x_1,y_1)\oplus\mathcal{P}(x_2,y_2)\oplus ... \oplus \mathcal{P}(x_n,y_n),
\end{align}
\vskip -0.1in}
where $\mathcal{P}$ denotes the prompt template and $(x_i, y_i)$ is the $i$-th refined similar molecule-caption pair, while $\oplus$ represents the concatenation.
Obviously, $(x_n,y_n)$ is generally the least informative molecule-caption pair among $n$ refined examples, while it is the closest example to the current input query. 
Here, we propose Sequence Reversal to resolve this question by simply reversing the sequence of examples. Specifically, the context can be represented as:

{\small
\vskip -0.2in
\begin{align}
    \mathcal{P}(x_n,y_n)\!\oplus\!\mathcal{P}(x_{n-1},y_{n-1})\!\oplus...\oplus\! \mathcal{P}(x_1,y_1).
\end{align}
}

\subsection{In-Context Molecule Tuning}
As shown in Figure \ref{fig:intro}, similar molecules typically share similar structures and chemical properties. Building on this principle, MolReGPT ~\cite{li2023empowering} has demonstrated the effectiveness of in-context learning, which aims to prompt Large Language Models (LLMs) to learn from similar examples without the need for domain-specific pre-training and fine-tuning. However, MolReGPT heavily relies on the reasoning and in-context learning capabilities of LLMs, resulting in poor performance with relatively small language models.
To address the deficits 
, we propose In-Context Molecule Tuning to fine-tune the parameters of LLMs, enabling them to learn from context examples and reason on the current input.
Notably, In-Context Molecule Tuning can be easily adapted to any LLM, allowing even smaller language models to unlock their in-context molecule learning capability by learning the differences and similarities between molecule-caption pairs in the context. 

Formally, given the training dataset $\mathcal{D}$ and the parameters of the LLM $\theta$, let the current input of the LLM be $x$ and the target output be $y$, where $(x,y) \in D$ denotes the molecule-caption pair. Traditional supervised fine-tuning methods directly learn the mapping from the input to the output $f: x\rightarrow y$ and the loss function $\mathcal{L}^{ft}(\theta)$ could be denoted as:

{\small
\vskip -0.2in
\begin{align}
    \mathcal{L}^{ft}(\theta) =  \underset{(x,y)\in \mathcal{D}}{\sum}[-\log p_\theta(y|x)].
\end{align}
\vskip -0.1in
}

In contrast, ICMA does not simply conduct the next token prediction on the output part but learns the entire input in an auto-regressive manner. ICMA first employs the Hybrid Context Retrieval and Post-retrieval Re-ranking to obtain a subset $D_{(x,y)} \subset D$ containing $n$ similar examples $\{(x_i, y_i) | 1\leq i \leq n\}$, from the training set. Different from the previous ICL objective, ICMA is also motivated to learn the mapping $f_i: x_i\rightarrow y_i$ inside the context examples in an obvious manner. Notably, molecule-caption mapping is the most informative part in the context examples because similar molecule sub-structures inherit similar characteristics. For example, RCOOH (carboxyl group) usually indicates that the molecule is an acid. In this way, learning the alignment between functional groups and molecule captions in the context examples could benefit the final prediction. For simplicity, we could assume that context examples are independent of each other as the molecule-caption mapping plays the most important role in the prediction.
In this case, the aggregation of mappings $\mathcal{F}_{(x,y)}=\{f_1, f_2, ..., f_{n}\}$ could be learned from context and will altogether contribute to the final prediction with the corresponding context $C_{(x,y)}$, which wraps the context examples into the input text.  
Therefore, the objective of ICMA can be represented as:

\begin{small} 
\begin{equation} 
\mathcal{L}(\mathcal{F}_{(x,y)}) = \underset{(x_i,y_i)\in D_{(x,y)}}{\sum}-\log p_\theta(y_i|x_i) ,
\end{equation} 
\begin{equation}
    \mathcal{L}^{I\!C\!M\!A}(\theta)\!=\!\underset{(x,y)\in D}{\sum}\!\left(\!-\log p_\theta(y|x, C_{(x,y)})\!+\! \mathcal{L}(\mathcal{F}_{(x,y)})\right)\!,
\end{equation}
\end{small}

\noindent where $\mathcal{L}(\mathcal{F}_{(x,y)})$ represents the aggregated mapping loss for molecule-caption pair $(x,y)$, while $\mathcal{L}^{I\!C\!M\!A}(\theta)$ denotes the overall loss function.

By learning the context examples as well as the corresponding mappings, ICMA enables LLMs to learn the alignment between molecular and textual spaces in a more explainable manner. Moreover, ICMA could effectively harness the reasoning capabilities of LLMs and seamlessly adapt general LLMs to the task of molecule-caption translation.
\section{Experiments}
\label{sec:Experiments}
In this section, we aim to evaluate the effectiveness of ICMA. Firstly, we introduce the experimental settings.
Then, we compare ICMA with selected baseline models on the ChEBI-20 dataset and further test ICMA on a smaller dataset, PubChem324k.
Furthermore, we also study the factors that will affect the performance of IMCA, including different foundation models, retrieval algorithms, context settings, and model scales.
Lastly, an ablation study is also conducted to justify the design of Post-retrieval Re-ranking components.

\subsection{Experimental Settings}
We will first detail our experiment settings. 
All the hyper-parameters are illustrated in Appendix A Table \ref{tab:hyper}. If not specifically stated, the cutoff length is set to 1024 and n\_shot is set to 2 to control variable. Notably, for LLMs with over 7 billion parameters, we apply LoRA ~\cite{hu2021lora} to save the GPU memory and accelerate computation. Otherwise, we fine-tune the full model of LLMs. For the dataset, we apply two different molecule-caption translation datasets,
ChEBI-20 ~\cite{edwards2021text2mol} and PubChem324k ~\cite{liu2023molca}. The details of the datasets are shown in Table \ref{tab:dataset}. 
\begin{table}[htb]
    \centering
    \vskip -0.05in
    \resizebox{1.0\columnwidth}{!}{
    \begin{tabular}{c|c|c|c}
    \toprule
    Dataset & Train & Validation & Test \\
    \midrule
    ChEBI-20 &  26,407     & 3,001           & 3,000  \\
    PubChem324k & 12,000 & 1,000  & 2,000 \\
    \bottomrule
    \end{tabular}
    }
    \vskip -0.1in
    \caption{Details of the datasets, ChEBI-20 and PubChem324k. For PubChem324k, we follow the split in ~\citet{liu2023molca}, while ignoring the \emph{Pretrain} fold.}
    \label{tab:dataset}
    \vskip -0.1in
\end{table}

For comparison, we select two different foundation LLMs as the backbones of ICMA, namely Galactica-125M ~\cite{taylor2022galactica} and Mistral-7B-instruct-v0.2 ~\cite{jiang2023mistral}. The former is a representative and smaller LLM that has been pre-trained on unstructured scientific corpora, which have been aware of the molecular knowledge from the pre-training stage, while the latter is a general LLM with 7 billion parameters, whose capabilities are comparable to GPT-3.5-turbo.
Notably, in the factor analysis and ablation study parts, we select Galactica-125M for experiments to reduce the computational costs.
Additionally, considering baseline models, we mainly select MolT5-base, MolT5-large from ~\citet{edwards-etal-2022-translation} and MolReGPT (GPT-3.5-turbo and GPT-4-0314) from ~\citet{li2023empowering} for comparison on the ChEBI-20 dataset.

\subsection{Performance Comparison}
\begin{table*}[htb]
    \centering
    \vskip -0.2in
    \resizebox{1.8\columnwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c}
    \toprule
    Methods & BLEU-2$\uparrow$ & BLEU-4$\uparrow$ & ROUGE-1$\uparrow$ & ROUGE-2$\uparrow$ & ROUGE-L$\uparrow$ & METEOR$\uparrow$ \\
    \midrule
    MolT5-base \cite{edwards-etal-2022-translation} & 0.540 & 0.457 & 0.634 & 0.485 & 0.578 & 0.569 \\ 
    MolReGPT (GPT-3.5-turbo) & 0.565 & 0.482 & 0.623 & 0.450 & 0.543 & 0.585 \\
    MolT5-large \cite{edwards-etal-2022-translation} & 0.594 & 0.508 & 0.654 & 0.510 & 0.594 & 0.614 \\
    MolReGPT (GPT-4-0314) & 0.607 & 0.525 & 0.634 & 0.476 & 0.562 & 0.610 \\
    \midrule
    Galactica-125M & 0.585 &	0.501 &	0.630 &	0.474 &	0.568 & 	0.591 \\
    ICMA(Galactica-125M)$_{2,2048}$ & \underline{0.636} &	\underline{0.565} 	&\underline{0.674} &	\underline{0.536} &	\underline{0.615} &	\underline{0.648} \\
    Mistral-7B &0.566 &	0.478 &	0.614 &	0.449 &	0.547 	&0.572  \\
    ICMA(Mistral-7B)$_{2,2048}$& \textbf{0.651} &	\textbf{0.581} & \textbf{0.686} & \textbf{0.550} &	\textbf{0.625} 	& \textbf{0.661} \\
    \midrule
    \bottomrule
    \end{tabular}
            
    }
    \vskip -0.1in
    \caption{Mol2Cap results on ChEBI-20 dataset (\textbf{Best}, \underline{Second Best}). Here, the results of MolT5-base and MolT5-large are domain-specific pre-training \& fine-tuning results retrieved from \citet{edwards-etal-2022-translation}, while MolReGPT(GPT-3.5-turbo) and MolReGPT(GPT-4-0314) are prompting \& in-context learning results retrieved from \citet{li2023empowering}. More importantly, Galactica-125M and Mistral-7B demonstrate vanilla supervised fine-tuned results, while ICMA(Galactica-125M) and ICMA(Mistral-7B) illustrate the ICMA results.}

    \label{tab:m2c}
    \vskip -0.1in
\end{table*}

\begin{table*}[htb]
    \centering      
    \resizebox{1.8\columnwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c|c}
    \toprule
    Method & BLEU$\uparrow$ & EM$\uparrow$ & Levenshtein$\downarrow$ & MACCS FTS$\uparrow$ & RDK FTS$\uparrow$ & Morgan FTS$\uparrow$ & Validity$\uparrow$ \\
    \midrule
    MolT5-base \cite{edwards-etal-2022-translation} & 0.769 & 0.081 & 24.458 & 0.721 & 0.588 & 0.529 & 0.772 \\
    MolReGPT(GPT-3.5-turbo) & 0.790 & 0.139 & 24.91 & 0.847 & 0.708 & 0.624 & 0.887 \\ 
    MolT5-large \cite{edwards-etal-2022-translation} & 0.854 & 0.311 & \textbf{16.071} & 0.834 & 0.746 & 0.684 & 0.905 \\
    MolReGPT(GPT-4-0314) & \textbf{0.857} & 0.280 & \underline{17.14} & \underline{0.903} & 0.805 & 0.739 & 0.899 \\
    \midrule
    Galactica-125M & 0.781 & 	0.173 &	26.34 &	0.836 &	0.708 &	0.631 &	0.916  \\
    ICMA(Galactica-125M)$_{4,2048}$ & 0.836 &	\underline{0.342} &	21.48 &	0.893 &	\underline{0.809}& 	\underline{0.743} &	\underline{0.934}   \\
    Mistral-7B & 0.767 &	0.234 &	27.39 &	0.852 &	0.718 &	0.649 &	0.918   \\
    ICMA(Mistral-7B)$_{4,2048}$& \underline{0.855} & 	\textbf{0.460} &	18.73 &	\textbf{0.916} &	\textbf{0.837} &	\textbf{0.789} &	\textbf{0.958}  \\
    \bottomrule
    \end{tabular}
    }
    \vskip -0.1in
    \caption{Cap2Mol results on ChEBI-20 dataset (\textbf{Best}, \underline{Second Best}). Here, the results of MolT5-base and MolT5-large are domain-specific pre-training \& fine-tuning results retrieved from \citet{edwards-etal-2022-translation}, while MolReGPT(GPT-3.5-turbo) and MolReGPT(GPT-4-0314) are prompting \& in-context learning results retrieved from \citet{li2023empowering}. More importantly, Galactica-125M and Mistral-7B demonstrate vanilla supervised fine-tuned results, while ICMA(Galactica-125M) and ICMA(Mistral-7B) illustrate the ICMA results.}

    \label{tab:c2m}
    \vskip -0.2in
\end{table*}
We compare and analyse the performance of ICMA with previous baselines and their original foundation models with naive supervised fine-tuning (SFT) from the two subtasks of molecule caption translation, namely Mol2Cap and Cap2Mol.

\noindent\textbf{Mol2Cap}. As illustrated in Table \ref{tab:m2c}, Galactica-125M with SFT has already shown competitive performance to previous baselines due to its pre-training on scientific corpora. However, ICMA could still improve the performance of Galactica-125M by 12.8\% and 8.3\% considering the BLEU-4 and ROUGE-L scores on ChEBI-20 dataset. With only 125 million parameters, ICMA(Galactica-125M) can beat MolT5-large, which owns 780 million parameters.
Meanwhile, the general LLM, Mistral-7B with naive SFT, only achieves a performance that is slightly better than MolT5-base, despite the fact that Mistral-7B is 70 times larger than MolT5-base. 
This outcome is not surprising because Mistral-7B is not specifically designed or pre-trained for biomolecular purposes.
It also reveals that although general LLMs have illustrated powerful capabilities with billions of parameters, few works have adapted them to the biomolecular domain due to their unsatisfactory fine-tuning performance. 
However, with ICMA, Mistral-7B could easily demonstrate its superior in-context learning and reasoning capabilities and perform its advantage of parameters.
As a result, ICMA(Mistral-7B) achieves the best performance across all the models, obtaining 0.581 BLEU-4 and 0.661 METEOR scores on the ChEBI-20 dataset, which is even better than the domain-specific pre-trained Galactica-125M.

\noindent\textbf{Cap2Mol}. Similarly, as shown in Table \ref{tab:c2m}, compared to their original foundation models with SFT, ICMA significantly boosts the molecule generation performance. Notably, ICMA (Mistral-7B) achieves state-of-the-art molecule generation performance, generating 46.0\% exactly matched molecules, which nearly doubles the results of naive supervised fine-tuned Mistral-7B. Meanwhile, ICMA(Mistral-7B) also achieves superior performance on the three molecule fingerprints scores and obtains the best validity score, illustrating better molecule generation quality with ICMA.

Additionally, experiments are also conducted on a smaller dataset, PubChem324k. As shown in Table \ref{tab:m2c_pub}, ICMA could still boost the Mol2Cap performance of LLMs like Galactica-125M and Mistral-7B under the low-data scenario. However, due to the lack of similar examples, Table \ref{tab:c2m_pub} demonstrates that the molecule generation capability can be influenced when context examples are not sufficient to support LLMs in making a correct decision.

\begin{table*}[htbp]
    \centering

    \resizebox{1.8\columnwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c}
    \toprule
    Method & BLEU-2$\uparrow$ & BLEU-4$\uparrow$ & ROUGE-1$\uparrow$ & ROUGE-2$\uparrow$ & ROUGE-L$\uparrow$ & METEOR$\uparrow$ \\
    \midrule
    Galactica-125M & 0.333	& 0.265&	0.465&	0.322&	0.417	&0.406\\
    ICMA(Galactica-125M)$_{2,1024}$& \underline{0.383}	&\underline{0.311}&	\underline{0.479} &\underline{0.341}	&\underline{0.428}&	\underline{0.438}\\
    Mistral-7B & 0.361&	0.288	&0.471&	0.325&	0.419&	0.421   \\
    ICMA(Mistral-7B)$_{2,2048ÃŸ}$& \textbf{0.386}	& \textbf{0.314}	&\textbf{0.483} &	\textbf{0.345} &	\textbf{0.431} &	\textbf{0.443} \\
    \bottomrule
    \end{tabular}
    }
    \vskip -0.1in
    \caption{Mol2Cap results on PubChem324k dataset (\textbf{Best}, \underline{Second Best}). Here, Galactica-125M and Mistral-7B demonstrate vanilla supervised fine-tuned results, while ICMA(Galactica-125M) and ICMA(Mistral-7B) illustrate the ICMA results.}
    
    \label{tab:m2c_pub}
    \vskip -0.1in
\end{table*}

\begin{table*}[htbp]
    \centering
    \resizebox{1.8\columnwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c|c}
    \toprule
    Method & BLEU$\uparrow$ & EM$\uparrow$ & Levenshtein$\downarrow$ & MACCS FTS$\uparrow$ & RDK FTS$\uparrow$ & Morgan FTS$\uparrow$ & Validity$\uparrow$ \\
    \midrule
    Galactica-125M &\underline{0.485}&	0.031	&\underline{62.08}&	0.681&	0.510	&0.403	& 0.835\\
    ICMA(Galactica-125M)$_{1,1024}$& \textbf{0.569}	& 0.035	&\textbf{52.75}	&0.719	&\underline{0.579}	&\textbf{0.652}	&0.825\\
    Mistral-7B & 0.438&	\textbf{0.082}	&74.16&	\underline{0.731}&	0.577&	0.472	&\underline{0.866}   \\
    ICMA(Mistral-7B)$_{1,2048}$& 0.450 &	\underline{0.065}	& 77.01 &	\textbf{0.764}	& \textbf{0.624}	& \underline{0.504}	& \textbf{0.891}  \\
    \bottomrule
    \end{tabular}
    }
    \vskip -0.1in
    \caption{Cap2Mol results on PubChem324k dataset (\textbf{Best}, \underline{Second Best}). Here, Galactica-125M and Mistral-7B demonstrate vanilla supervised fine-tuned results, while ICMA(Galactica-125M) and ICMA(Mistral-7B) illustrate the ICMA results.}
    \label{tab:c2m_pub}
    \vskip -0.2in
\end{table*}

\subsection{Study of Retrieval Algorithms}
\begin{table}[htbp]
    \centering

    \resizebox{1.0\columnwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c}
    \toprule
    Method & BLEU-2$\uparrow$ & BLEU-4$\uparrow$ & ROUGE-1$\uparrow$ & ROUGE-2$\uparrow$ & ROUGE-L$\uparrow$ & METEOR$\uparrow$ \\
    \midrule
    Random & 0.573 &	0.496 &	0.634 &	0.484 &	0.572 &	0.596 \\ 
    Morgan FTS & \underline{0.599} &	\underline{0.526} &	\underline{0.650} &	\underline{0.508} &	\underline{0.590} &	\underline{0.626}\\
    Mole-BERT & \textbf{0.618} &	\textbf{0.545} &	\textbf{0.662} &	\textbf{0.521} &	\textbf{0.602} &	\textbf{0.637}\\
    \bottomrule
    \end{tabular}
    }
    \vskip -0.1in
    \caption{Performance comparison of different retrieval algorithms for Mol2Cap task (\textbf{Best}, \underline{Second Best}). The backbone is ICMA(Galactica-125M)$_{2,1024}$.}
    \vskip -0.1in
    \label{tab:m2c_algo}
\end{table}

\begin{table}[htbp]
    \centering
    \resizebox{1.0\columnwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c|c}
    \toprule
    Method & BLEU$\uparrow$ & EM$\uparrow$ & Levenshtein$\downarrow$ & MACCS FTS$\uparrow$ & RDK FTS$\uparrow$ & Morgan FTS$\uparrow$ & Validity$\uparrow$ \\
    \midrule
    Random &  \underline{0.802} &	\underline{0.286} &	\underline{25.29} &	0.867 &	0.763 &	\underline{0.699} &	0.923 \\
    SBERT & 0.795 &	0.245 &	26.05 &	\underline{0.868} &	\underline{0.765} &	0.691 &	\underline{0.926}\\
    BM25 & \textbf{0.818} &	\textbf{0.293} &	\textbf{24.20} &	\textbf{0.881} &	\textbf{0.784} &	\textbf{0.717} &	\textbf{0.927}\\
    \bottomrule
    \end{tabular}
    }
    \vskip -0.1in
    \caption{Performance comparison of different retrieval algorithms for Cap2Mol task (\textbf{Best}, \underline{Second Best}). The backbone is ICMA(Galactica-125M)$_{2,1024}$.}
    \label{tab:c2m_algo}
    \vskip -0.2in
\end{table}
We study the influence of retrieval algorithms to illustrate the importance of retrieval qualities. For molecule retrieval, we compare the new proposed Molecule Graph Retrieval using Mole-BERT with random retrieval and the Morgan FTS retrieval. As shown in Table \ref{tab:m2c_algo},  Molecule Graph Retrieval using Mole-BERT achieves the best results on all of the metrics, proving its superiority in molecule retrieval.
For the caption retrieval, we compare BM25 Caption Retrieval with random retrieval and SBERT retrieval under the framework of ICMA.
As depicted in Table \ref{tab:c2m_algo}, BM25 Caption Retrieval illustrates its excellent performance among the three caption retrieval methods. Notably, SBERT retrieval achieves a worse score than random retrieval because it loses detailed information during the compression, further proving the importance of details in caption retrieval.

\subsection{Study of Context Settings}
\begin{figure}
    \centering
    \includegraphics[width=1.0\columnwidth]{figures/context_settings.png}
    \vskip -0.1in
    \caption{The model performance with the change of context settings, including the number of examples (i.e., n\_shot) and cutoff length. Mol2Cap Results (Left) and Cap2Mol Results (Right).}
    \label{fig:model_length}
    \vskip -0.2in
\end{figure}
In ICMA, the context settings, including the example number and cutoff length, are also important to its performance. The increase of context example number will also drastically add to the context length. However, if the context length is larger than the cutoff length, then the training will be insufficient because most of the inputs are cropped, and the information is lost. 
In this case, we want to make sure that most of the context examples fit in the cutoff length. Considering that the input length limitation of Galactica-125M is 2048, we test the cutoff length within the range of \{512, 1024, 1536, 2048\} and the example number from 1 to 4 for analysis.
As illustrated in Figure \ref{fig:model_length}, when the example number increases, the performance becomes worse if the cutoff length is too short. For example, when the cutoff length is set to 512, the context is not complete for LLMs to consistently learn valuable knowledge from it, leading to worse results. However, when the cutoff length is long enough, the performance mainly becomes better as the increased example number provides more information to help LLMs make accurate predictions.

\subsection{Study of Scaling Law}
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures/scaling_law.png}
    \caption{The scaling law of ICMA. Three models with different levels of parameters are selected, including Galactica-125M, Galactica-1.3B, and Mistral-7B. Mol2Cap Results (Left) and Cap2Mol Results (Right).}
    \vskip -0.1in
    \label{fig:model_scale}
\end{figure}
As ICMA creates a new paradigm for adapting powerful LLMs with billion parameters to the molecule-caption translation task, it is interesting to study the relationship between the performance and model scales. In this case, we select LLMs with different scales of parameters, ranging from 125 million to 7 billion, to study the scaling law of ICMA. Specifically, we select Galactica-125M, Galactica-1.3B, and Mistral-7B for demonstration.

As illustrated in Figure \ref{fig:model_scale}, with the increase of model scale, the performance of ICMA also improves. Especially in Mol2Cap task, the METEOR score has been drastically improved when the model parameters scale from 1.3 billion to 7 billion. The improvement of molecule understanding significantly benefits from the language generation capability as the model scale reaches a certain level. Meanwhile, for Cap2Mol task, the accuracy for predicting exactly matched molecules demonstrates a slightly different pattern from the Mol2Cap task. The model's performance in generating exact-matched molecules seems to have a log-linear relationship with the model scales, showing that larger language models could explore the molecular space better owing to their more powerful capabilities. 
Overall, ICMA allows general LLMs with billion parameters to achieve similar or better performance to these domain-specific pre-trained LLMs in the molecule-caption translation task, showing that LLMs are inherently in-context molecule learners. 

\subsection{Ablation Study}
\begin{table}[htbp]
    \centering
    \vskip -0.2in
    \resizebox{1.0\columnwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c}
    \toprule
    Method & BLEU-2$\uparrow$ & BLEU-4$\uparrow$ & ROUGE-1$\uparrow$ & ROUGE-2$\uparrow$ & ROUGE-L$\uparrow$ & METEOR$\uparrow$ \\
    \midrule
    ICMA & \textbf{0.618} &	\textbf{0.545} &	\textbf{0.662(4)} &	0.521(1) &	\textbf{0.602(3)} &	0.637(3)\\
    w/o Random Walk &\underline{0.603}& 	\underline{0.534} &	\underline{0.661(9)} &	\textbf{0.523(3)} &	\underline{0.602(1)} &	\textbf{0.638(1)}\\
    w/o Sequence Reverse& 0.597 &	0.528 	&0.661(8) &	\underline{0.522(5)} 	&0.601(8) &	\underline{0.638(0)}\\
    \bottomrule
    \end{tabular}
    }
    \vskip -0.1in
    \caption{Ablating components of Post-retrieval Re-ranking for Mol2Cap task (\textbf{Best}, \underline{Second Best}). The backbone is ICMA(Galactica-125M)$_{2,1024}$.}
    \vskip -0.2in
    \label{tab:m2c_ablation}
\end{table}

\begin{table}[htbp]
    \centering
    \resizebox{1.0\columnwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c|c}
    \toprule
    Method & BLEU$\uparrow$ & EM$\uparrow$ & Levenshtein$\downarrow$ & MACCS FTS$\uparrow$ & RDK FTS$\uparrow$ & Morgan FTS$\uparrow$ & Validity$\uparrow$ \\
    \midrule
    ICMA & \textbf{0.818} &	\textbf{0.293} &	\textbf{24.20} 	&\textbf{0.881} 	&\textbf{0.784}	&\textbf{0.717} 	& \underline{0.927} \\
    w/o Random Walk & 0.771 &	0.103 &	28.99 &	0.838 	&0.749 &	0.654 &	0.925 \\
    w/o Sequence Reverse &\underline{0.807} &	\underline{0.279} &	\underline{24.65} &	\underline{0.876} 	&\underline{0.782} &	\underline{0.713} &	\textbf{0.929} \\
    \bottomrule
    \end{tabular}
    }
    \vskip -0.1in
    \caption{Ablating components of Post-retrieval Re-ranking for Cap2Mol task (\textbf{Best}, \underline{Second Best}). The backbone is ICMA(Galactica-125M)$_{2,1024}$.}
    \label{tab:c2m_ablation}
    \vskip -0.1in
\end{table}
Lastly, the ablation study is also conducted to illustrate how the Post-retrieval Re-ranking components affect the predictions. 
We conduct ablation experiments for the Mol2Cap and Cap2Mol sub-tasks by deactivating the Random Walk and Sequence Reversal.
As shown in Table \ref{tab:m2c_ablation} and \ref{tab:c2m_ablation}, without Random Walk or Sequence Reversal, the performance of ICMA drops in both tasks, proving the effectiveness of Post-retrieval Re-ranking.

\section{Conclusion}
\label{sec:conclusion}
In this work, we propose In-Context Molecule Adaptation (ICMA), as a new paradigm for adapting LLMs to the molecule-caption translation task. 
Instead of domain-specific pre-training and fine-tuning, ICMA enables LLMs to utilize their in-context learning capability to learn molecule-text alignment via In-Context Molecule Tuning, which significantly improves the performance of LLMs in the molecule-caption translation task, demonstrating that LLMs are inherently in-context molecule learners.
More importantly, our study provides a viable framework for deploying advanced LLMs with billion-level parameters in the scientific field.

\section{Limitations}
The limitation of this work primarily lies in the following aspects:
\begin{itemize}
    \item Firstly, we limited the retrieval range to the training split of the dataset, which might be insufficient, given the vastness of the molecular space. 
    \item Secondly, this work is limited to the molecule-caption translation task. However, it is worth noting that there are various molecule-related downstream tasks, such as molecule property prediction, which is also important to verify the generalization capability of ICMA.
    \item Lastly, due to the limitation of hardware, we were unable to investigate the potential of language models with more than 7 billion parameters with ICMA. It would be interesting to explore the capabilities of larger, more sophisticated language models under the framework of ICMA.
\end{itemize}

\section{Acknowledgement}
We thank any anonymous reviewers for their valuable suggestions. We also thank ChatGLM-4\footnote{\url{https://chatglm.cn}} and Github Copilot\footnote{\url{https://github.com/features/copilot}} for assistance in the paper revision and code writing.

\end{document}