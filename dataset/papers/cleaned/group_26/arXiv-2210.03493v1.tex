\title{Automatic Chain of Thought Prompting \\ in Large Language Models}

\begin{document}

\maketitle

\begin{abstract}

Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like ``Let’s think step by step'' to facilitate step-by-step thinking before answering a question. 
The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. 
The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. 
We show that such manual efforts may be eliminated by leveraging LLMs 
with the ``Let’s think step by step'' prompt
to generate reasoning chains for demonstrations one by one,
i.e., \emph{let's think not just step by step, but also one by one}. 
However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at \url{https://github.com/amazon-research/auto-cot}

\end{abstract}

\section{Introduction}

Large language models (LLMs) \citep{brown2020language, lamda, gopher, palm} have performed impressively on complex reasoning tasks by decomposing the multi-step problems into intermediate steps before producing the answer. This reasoning process is elicited by a very recent technique: chain-of-thought (CoT) prompting  \citep{cot_wei}. 

\begin{figure}[htb]
\vspace{-3mm}
  \begin{center}
   \includegraphics[width=1\columnwidth]{fig_example.pdf}
  \end{center}
  \caption{Zero-Shot-CoT \citep{kojima2022large} (using the ``Let’s think step by step'' prompt) and Manual-CoT \citep{cot_wei} (using manually designed demonstrations one by one) with example inputs and outputs of an LLM.}
  \label{fig_examples}
\end{figure}

CoT prompting can be categorized into two major paradigms. One adds a single prompt like ``Let's think step by step'' after the test question to facilitate the reasoning chains in LLMs \citep{kojima2022large}. Since this prompting paradigm is task-agnostic and does not need input-output demonstrations, it is called \textbf{Zero-Shot-CoT} (left of Figure~\ref{fig_examples}). With Zero-Shot-CoT, LLMs have shown to be decent zero-shot reasoners. The other paradigm is few-shot prompting with  manual reasoning demonstrations one by one \citep{cot_wei}. Each \emph{demonstration} has a question and a reasoning chain. A \emph{reasoning chain} is composed of a \emph{rationale} (a series of intermediate reasoning steps) and an expected answer.
With all the demonstrations being manually designed, this paradigm is referred to as \textbf{Manual-CoT} (right of Figure~\ref{fig_examples}).

In practice, Manual-CoT has obtained stronger performance than Zero-Shot-CoT \citep{cot_wei,kojima2022large}. However, this superior performance hinges on the hand-drafting of effective demonstrations. Specifically, the hand-drafting involves nontrivial efforts in designs of both questions and their reasoning chains for demonstrations. Moreover, human efforts for designing task-specific demonstrations are even more: different tasks, such as arithmetic \citep{multiarith} and commonsense reasoning \citep{commonsenseqa}, require different ways of demonstrations. 

To eliminate such manual designs, we advocate another \textbf{Auto-CoT} paradigm to automatically construct demonstrations with questions and reasoning chains.
Specifically, Auto-CoT
leverages LLMs 
with the ``Let’s think step by step'' prompt
to generate reasoning chains for demonstrations one by one,
i.e., \emph{let's think not just step by step, but also one by one}.
However, we find that this challenge cannot be effectively addressed by simple solutions. For example, given a test question of a dataset, retrieving semantically similar questions and invoking Zero-Shot-CoT to generate reasoning chains will fail. Although LLMs are decent zero-shot reasoners, they are not perfect: Zero-Shot-CoT can still make mistakes in reasoning chains.

To mitigate the effect of reasoning chain mistakes from Zero-Shot-CoT, our analysis shows that
diversity of demonstration questions is the key. Based on this insight, we propose an Auto-CoT method to automatically construct demonstrations. Auto-CoT consists of two main steps. First, partition questions of a given dataset into a few clusters. Second, select a representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics.

We evaluate Auto-CoT on ten benchmark reasoning tasks including: (i) arithmetic reasoning (MultiArith \citep{multiarith}, GSM8K \citep{gsm8k}, AQUA-RAT \citep{aqua}, SVAMP \citep{svamp}); (ii) commonsense reasoning (CSQA \citep{commonsenseqa}, StrategyQA \citep{strategyqa}); (iii) symbolic reasoning (Last Letter Concatenation, Coin Flip) \citep{cot_wei}. Experimental results show that with GPT-3, Auto-CoT consistently matches or exceeds the performance of Manual-CoT that requires manual designs. This indicates that LLMs can perform CoT reasoning by automatically constructing demonstrations. 

\section{Related Work}  
This section reviews two lines of research that form the basis of this work: chain-of-thought (CoT) prompting for multi-step reasoning and in-context learning for inducing LLMs to learn from demonstrations.

\subsection{Chain-of-thought Prompting}
CoT prompting is a gradient-free technique of inducing LLMs to produce intermediate reasoning steps that lead to the final answer. \citet{cot_wei} formally studied the topic of CoT prompting in language models. This technique elicits LLMs to generate a coherent series of intermediate reasoning steps that lead to the final answer to a question. Studies have shown that LLMs can perform CoT reasoning with zero-shot prompting (Zero-Shot-CoT) \citep{kojima2022large} or manually written few-shot demonstrations (Manual-CoT) \citep{cot_wei}.

\paragraph{Zero-Shot-CoT.} \citet{kojima2022large} showed that LLMs are decent zero-shot reasoners whose generated rationales have already reflected the CoT reasoning. This finding inspires our work to leverage the self-generated rationales for demonstrations. Generating rationales by LLMs was shown to be practical in a recent work \citep{zelikman2022star}. In their work, an LLM is prompted to generate rationales and those rationales that lead to the correct answer are selected. The selection requires a training dataset of questions with annotated answers. In contrast, our work considers a more challenging scenario where only a set of test questions are given (without a training dataset), following CoT prompting studies by \citet{cot_wei} and \citet{kojima2022large}.

\paragraph{Manual-CoT.} Manual-CoT achieves stronger performance by eliciting the CoT reasoning ability with effective manual demonstrations. The demonstrations for the reasoning process are manually designed. However, the human efforts in designs of both
questions and their reasoning chains are nontrivial. Instead of addressing this limitation, recent studies mainly focus on hand-crafting more complex demonstrations or leveraging ensemble-like methods. One trend is problem decomposition. In least-to-most prompting \citep{zhou2022least}, complex problems are reduced to sub-problems, and then the sub-problems are solved sequentially. The other trend is to vote over multiple reasoning paths for a test question. \citet{cot_wei_sc} introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers. \citet{wang2022rationale} and \citet{li2022advance} introduced randomness in the input space to produce more diverse outputs for voting. They used manually-designed demonstrations as the seed set and generated additional rationales: leave one question from the seed set and use the remaining demonstrations to generate rationales for this question by the LLM.
Unlike the aforementioned research lines that rely on manually-designed demonstrations, our work intends to eliminate manual designs with competitive performance.

\subsection{In-Context Learning}
CoT prompting is closely related to in-context learning (ICL) \citep{Radford2019LanguageMA,brown2020language}. ICL enables LLMs to perform a target task by feeding a few prompted examples as part of the input. Without gradient update, ICL allows a single model to perform various tasks universally. There are various research lines to improve the performance of ICL: (i) retrieving related demonstrations to the test instance where the popular practice is dynamically retrieving related training examples for a given test input \citep{rubin2021learning,Su2022SelectiveAM}; (ii) augmenting with fine-grained information, such as incorporating task instruction \citep{mishra2022cross,wei2022finetuned,sanh2022multitask}; (iii) manipulating  output probabilities of LLMs instead of directly computing the likelihood
of target labels \citep{holtzman2021surface,zhao2021calibrate,min2022noisy}.

Despite the success of ICL, studies \citep{liu2022makes,lu2022fantastically} have shown that the strength of ICL may vary widely depending on the choice of in-context demonstrations \citep{liu2022few}. In detail, the formatting of the prompt, such as wording or order of demonstrations, may lead to performance fluctuations \citep{webson2021prompt,zhao2021calibrate}. A recent work \citep{min2022rethinking} even questioned the necessity of ground-truth input-output mapping: using incorrect
labels in the examples only marginally lowers the performance. However, the existing analysis of ICL is mainly based on standard classification and multi-choice datasets that only have simple <input$\rightarrow$output> mappings. We discover that those findings may not be applicable to the CoT prompting scenario with more complex <input$\rightarrow$rationale$\rightarrow$output> mappings. 
For example, mistakes in either the <input$\rightarrow$rationale> mapping or the <rationale$\rightarrow$output> mapping will lead to a dramatic performance drop (Appendix \ref{appendix:demo_impact}).

\section{Challenge of Auto-CoT}\label{sec:prelim}

As just discussed, the performance of ICL hinges on hand-crafted demonstrations. 
As reported in Manual-CoT \citep{cot_wei}, using demonstrations written by different annotators brings up to 28.2\% accuracy disparity in a symbolic reasoning task, while changing the order of demonstrations results in less than 2\% changes in most tasks.
This suggests that the key challenge of Auto-CoT lies in automatically constructing demonstrations with \emph{good} questions and their reasoning chains.

Recall that Manual-CoT hand-crafts a few (e.g., 8) questions in demonstrations. With similarity-based retrieval methods being widely adopted for prompting LLMs \citep{rubin2021learning,Su2022SelectiveAM}, a promising candidate solution is to sample demonstration questions using similarity-based retrieval. We follow the more challenging assumption in CoT studies \citep{cot_wei,kojima2022large} that only a set of test questions are given (without a training dataset). Following \citet{liu2022makes}, we use Sentence-BERT \citep{reimers-2019-sentence-bert} to encode questions. 
For each question $q^\text{test}$ in a test dataset, we sample demonstration questions $q^\text{demo}_i$ ($i = 1, \ldots, k$) from the rest of the questions. We design a \textbf{Retrieval-Q-CoT} method to retrieve the top-$k$ (e.g., $k=8$) similar questions based on cosine similarity. To compare with this similarity-based method, we also test a relatively more diversity-based method: \textbf{Random-Q-CoT}, which randomly samples $k$ other test questions for each test question. 

Both Retrieval-Q-CoT and Random-Q-CoT invoke Zero-Shot-CoT \citep{kojima2022large} to generate the reasoning chain $c^\text{demo}_i$ (rationale and answer) for each sampled question $q^\text{demo}_i$, as LLMs are decent zero-shot reasoners \citep{kojima2022large}. 
We use GPT-3 \citep{brown2020language} with 175B parameters (text-davinci-002) for the LLM unless otherwise stated. 
On a high level, both Retrieval-Q-CoT and Random-Q-CoT take the concatenation of $q^\text{demo}_i, c^\text{demo}_i$ pairs ($i = 1, \ldots, k$) and $q^\text{test}$ as input to predict the reasoning chain for $q^\text{test}$, which contains the answer in the end (like right of Figure \ref{fig_examples}).

\begin{wraptable}{r}{0.4\textwidth}
    \centering
        \caption{Accuracy (\%) of different sampling methods. Symbol $\dagger$  indicates using training sets with annotated reasoning chains. \label{tab:preliminary}}
    \setlength{\tabcolsep}{2pt}
\begin{tabular}{l|c|cc}\toprule
 {Method} & {MultiArith} & {GSM8K} & {AQuA}  \\\midrule
 Zero-Shot-CoT & 78.7 & 40.7 & 33.5  \\
 Manual-CoT & \textbf{91.7} & 46.9 & 35.8$\dagger$ \\
\midrule
Random-Q-CoT & 86.2 & 47.6$\dagger$ & 36.2$\dagger$ \\
Retrieval-Q-CoT & 82.8 & \textbf{48.0}$\dagger$ & \textbf{39.7}$\dagger$ \\
\bottomrule
\end{tabular}
\end{wraptable}

To our surprise, Retrieval-Q-CoT underperforms Random-Q-CoT on the arithmetic dataset MultiArith \citep{multiarith} (Table \ref{tab:preliminary}).
Note that the retrieval methods were originally proposed in tasks with \emph{annotated} labels \citep{rubin2021learning,Su2022SelectiveAM}, however, invoking Zero-Shot-CoT does not guarantee entirely correct reasoning chains. 
Thus, we hypothesize that the inferior performance of Retrieval-Q-CoT is caused by incorrect reasoning chains by Zero-Shot-CoT.
To test this hypothesis, we experiment with Retrieval-Q-CoT on two other datasets GSM8K \citep{gsm8k} and AQuA \citep{aqua} that have training sets with \emph{annotated} reasoning chains. The results are shown with $\dagger$ in Table \ref{tab:preliminary}. Under the setting with annotated reasoning chains, Retrieval-Q-CoT even outperforms Manual-CoT. The result indicates that Retrieval-Q-CoT is effective when human annotations are available. 

Although human annotations are useful, such manual efforts are nontrivial.
However, automatically generating reasoning chains via Zero-Shot-CoT underperforms Manual-CoT, especially when the challenge of question sampling is not addressed.
To design more effective Auto-CoT, we need to understand its challenge better.

\subsection{Retrieval-Q-CoT Fails due to Misleading by Similarity} 
\label{subsec:Retrieval-Q-CoT Fails}

Since Retrieval-Q-CoT uses a few prompting demonstrations like in Manual-CoT, Retrieval-Q-CoT is expected to perform competitively as well. However, reasoning chains (both rationales and answers) in Retrieval-Q-CoT are generated by Zero-Shot-CoT: they may have mistakes that lead to wrong answers.
Let us simply call demonstrations with wrong answers as \textit{wrong demonstrations}.
Intuitively, after \emph{similar} questions to a test question are retrieved, wrong demonstrations caused by Zero-Shot-CoT may mislead the same LLM to reason similarly with a wrong answer (e.g., replicating mistakes) for the test question.
We refer to this phenomenon as \textit{misleading by similarity}.
We will investigate whether misleading by similarity contributes to the inferior performance of Retrieval-Q-CoT. 

\begin{wrapfigure}{r}{0.4\textwidth}
    \centering
    \pgfplotsset{width=6.5cm, height=4.5cm}
    \centering
\vspace{-5mm}
  \begin{tikzpicture}  
        \begin{axis}  
        [  
            ybar,
            ymin=20, ymax=55,
            ytick={20,30,40,50},
            major x tick style = transparent,
            bar width=16pt,
            legend columns=1 row=2,
            enlarge x limits=0.4,
            ylabel={{Rate (\%)}},
            symbolic x coords={1,2,3}, 
            xticklabels={{Retrieval-Q-CoT}, {Random-Q-CoT}},
            xtick=data,  ylabel style={align=right},
            legend cell align=left,
            legend style={
                        at={(1.0,0.49)},
                        anchor=south east,
                        column sep=1ex,
                },
            ]  
        \addplot[ybar, fill=bananayellow,  postaction={pattern=north east lines}] coordinates {
            (1,46.875)(2,25.781)
        };  
        \end{axis}  
    \end{tikzpicture}  
    \caption{{Unresolving Rate}.\label{fig:pre-unsolved}
    }
    \vspace{-3mm}
\end{wrapfigure}

To begin with, we invoke Zero-Shot-CoT on all the 600 questions from the MultiArith dataset. Among them, we collect those 128 questions (denoted as $\mathcal{Q}$) where Zero-Shot-CoT generates  wrong answers (error rate: $21.3\% = 128/600$). 
As we mentioned, with extra demonstrations, Retrieval-Q-CoT and Random-Q-CoT are expected to perform more competitively than Zero-Shot-CoT.
Among $\mathcal{Q}$ where Zero-Shot-CoT fails, we call those where Retrieval-Q-CoT or Random-Q-CoT still fail as their \textit{unresolved questions}. 
We divide the number of unresolved questions by 128 (number of questions in $\mathcal{Q}$) to calculate the \textit{unresolving rate}. A higher unresolving rate means that a method more likely still makes mistakes like Zero-Shot-CoT.
Figure \ref{fig:pre-unsolved} shows that the unresolving rate of Retrieval-Q-CoT (46.9\%) is much higher than Random-Q-CoT (25.8\%). 
It indicates that 
with similar questions being sampled for test questions,
Retrieval-Q-CoT is negatively affected by misleading by similarity.

To show that
unresolved questions of Retrieval-Q-CoT
tend to be similar,
we present a case study in Table~\ref{tab:pre_failure}. In the left part, the retrieved demonstration questions are similar to the test question and ask  ``\textit{how long will it take him to cook the rest?}'' The reasoning chains generated by Zero-Shot-CoT produce answers regarding ``\textit{the total of}'' instead of ``\textit{the rest}''. Following the demonstrations, Retrieval-Q-CoT also fails by misunderstanding the meaning of ``\textit{the rest}''. In contrast, Random-Q-CoT correctly understands ``\textit{the rest}'' better without making similar mistakes in the demonstrations, thanks to relatively more diverse (random) demonstrations. 

\subsection{Errors Frequently Fall into the Same Cluster}

Motivated by the observations in Table~\ref{tab:pre_failure}, we use $k$-means to partition all the 600 test questions into $k=8$ clusters, where each cluster contains similar questions.\footnote{We use Sentence-BERT \citep{reimers-2019-sentence-bert} to encode questions and apply $k$-means for clustering.} With these clusters
and reasoning chains generated by Zero-Shot-CoT (in Section \ref{subsec:Retrieval-Q-CoT Fails}), now we are curious if certain clusters contain 
questions where Zero-Shot-CoT frequently fails.
Thus, we calculate the error rate (questions with wrong Zero-Shot-CoT answers / total questions) for each cluster.

\begin{wrapfigure}{r}{0.4\textwidth}
    \centering
    {
\pgfplotsset{width=6.7cm, height=4.5cm}
    \begin{tikzpicture} % Tikz environment
            \begin{axis}  
        [xticklabels={1,2,3,4,5,6,7,8},
        major x tick style = transparent,xtick={0,1,2,3,4,5,6,7}, bar width=8pt,enlarge x limits=0.1,ymin=0, ymax=60,
        ytick={0,20,40,60},
  ylabel={Error Rate (\%)},  
  ylabel style={align=left},xlabel={}]
        \addplot[ybar, fill=cinnamon,  postaction={pattern=north west lines}] coordinates {
            (0,9)(1,52)(2,16)(3,13)(4,16)(5,22)(6,29)(7,22)
        };  
        \end{axis}  
    \end{tikzpicture}
    \vspace{-2mm}
    \caption{Clusters of similar questions.\label{pre-cluster}}
}
\end{wrapfigure}

As shown in Figure \ref{pre-cluster}, there exists a cluster (Cluster 2) with frequent Zero-Shot-CoT errors (52.3\%).
The phenomenon could be generic as Zero-Shot-CoT may lack some skills to solve some common problems in target tasks.\footnote{We observe  similar phenomena when changing the cluster number or using other datasets (Appendix \ref{appendix:hard_cluster}).}
For convenience of descriptions, let us call the cluster with the highest error rate as the \emph{frequent-error cluster} (e.g., Cluster 2 in Figure \ref{pre-cluster}). Therefore, the imperfect nature of generated reasoning chains in a zero-shot fashion
poses risks of retrieving multiple similar questions inside a frequent-error cluster by using similarity-based methods. 
For the test question in the frequent-error cluster, Retrieval-Q-CoT more easily constructs demonstrations with multiple similar mistakes. As a result, Retrieval-Q-CoT often makes similar mistakes like Zero-Shot-CoT, reiterated by its higher unresolving rate in Figure \ref{fig:pre-unsolved}.

\begin{table}[t]\centering
\footnotesize
\caption{Examples of Retrieval-Q-CoT and Random-Q-CoT for constructing demonstrations and prediction. The upper part contains the automatically constructed demonstrations, and the lower part contains test questions along with predicted reasoning chains.}
\vspace{2.8mm}
\begin{tabular}{p{0.52\textwidth}|p{0.425\textwidth}}
\toprule

\textbf{Retrieval-Q-CoT} &
\textbf{Random-Q-CoT}
\\

\midrule

\bblack{Q:} A chef needs to cook 9 potatoes. He has already cooked 7. If each potato takes 3 minutes to cook, how long will it take him to cook \underline{\textbf{the rest}}?
&
\bblack{Q:} Wendy uploaded 79 pictures to Facebook. She put 44 pics into one album and put {the rest} into 5 different albums. How many pictures were in each album?
\\
\bblack{A:} Let's think step by step. The chef has already cooked 7 potatoes. That means it has taken him 7 * 3 minutes to cook those 7 potatoes. That means it will take him 3 more minutes to cook each of the remaining 2 potatoes. \textcolor{red}{That means it will take him a total of 7 * 3 + 2 * 3 = 21 minutes to cook all 9 potatoes.} The answer is 21. \ngmark
&
\bblack{A:} Let's think step by step. Wendy uploaded 79 pictures to Facebook. She put 44 pictures into one album. \textcolor{blue}{That means she put the rest of the pictures into 5 different albums.} That means each album would have (79-44)/5 = 15 pictures in each album. The answer is 15.\okmark
\\
\bblack{Q:} A chef needs to cook 13 potatoes. He has already cooked 5. If each potato takes 6 minutes to cook, how long will it take him to cook \underline{\textbf{the rest}}?
&
\bblack{Q:} At a restaurant each adult meal costs \$5 and kids eat free. If a group of 15 people came in and 8 were kids, how much would it cost for the group to eat?
\\
\bblack{A:} Let's think step by step. The chef has already cooked 5 potatoes. That means it has taken him 30 minutes so far. There are 13 potatoes in total. 5 have been cooked so far. That means there are 8 left to cook. Each potato takes 6 minutes to cook. That means it will take the chef 48 minutes to cook the 8 remaining potatoes. \textcolor{red}{In total, it will take the chef 78 minutes to cook all 13 potatoes.} The answer is 78.  \ngmark
&
\bblack{A:} Let's think step by step. There are 8 kids, so that means there are 7 adults. Each adult meal costs \$5, so that means that the 7 adults will cost \$5*7=\$35. Now we just need to add the two numbers together. \$35+8=43\$. So it would cost \$43 for the group to eat. The answer is 43.  \ngmark
\\
\midrule
\bblack{Q:} A chef needs to cook 15 potatoes. He has already cooked 8. If each potato takes 9 minutes to cook, how long will it take him to cook \underline{\textbf{the rest}}?
&
\bblack{Q:} A chef needs to cook 15 potatoes. He has already cooked 8. If each potato takes 9 minutes to cook, how long will it take him to cook \underline{\textbf{the rest}}?
\\
\bblack{A:} Let's think step by step. The chef has already cooked 8 potatoes. That means it has taken him 8 * 9 minutes to cook those 8 potatoes. That means it will take him 9 minutes to cook each of the remaining 7 potatoes. \textcolor{red}{That means it will take him a total of 8 * 9 + 7 * 9 = 135 minutes to cook all 15 potatoes.} The answer is 135. \ngmark
&
\bblack{A:} Let's think step by step. The chef needs to cook 15 potatoes. He has already cooked 8. That means he has to cook 15-8=7 more potatoes. Each potato takes 9 minutes to cook. \textcolor{blue}{That means it will take him 9*7=63 minutes to cook \underline{\textbf{the rest}} of the potatoes.} The answer is 63. \okmark
\\
\bottomrule
\end{tabular}
\vspace{-3.6mm}
\label{tab:pre_failure}
\end{table}

\subsection{Diversity May Mitigate Misleading by Similarity}
\label{subsec:diversity may mitigate}

The analysis so far compellingly shows that LLMs are still not \emph{perfect} zero-shot reasoners; thus, we aim to mitigate the effect of their Zero-Shot-CoT errors, especially to mitigate misleading by similarity in the design of Auto-CoT.

As we will show later (Section \ref{analysis:incorrect}), presenting a small portion of mistakes (e.g., 1 or 2 wrong demonstrations out of 8) would not harm the overall reasoning performance for test questions. 
Suppose that questions of all the wrong demonstrations fall into the same frequent-error cluster; then sampling one question from every different cluster will lead to a higher than $7/8=87.5\%$ chance to construct all the 8 correct demonstrations.
Since different clusters reflect diverse semantics of the questions, this clustering-based sampling method can be considered as diversity-based, which is in sharp contrast to similarity-based Retrieval-Q-CoT.
On one hand, sampling questions with diversity may mitigate the effect of misleading by similarity (Section \ref{subsec:Retrieval-Q-CoT Fails}). 
On the other hand, if we took each demonstration as a kind of skill, diverse demonstrations seem to cover more alternative skills for solving target questions: even though there still exists a small portion (e.g., $1/8$) of mistakes in the demonstrations, the performance will not be negatively affected (to be shown in Figure \ref{fig_wr}).

Nevertheless, the clustering-based sampling method may still construct a small portion of wrong demonstrations, such as from questions in the frequent-error cluster.
As we will show later, some of these wrong demonstrations may be eliminated with heuristics. 
For example, wrong demonstrations often come with long questions and long rationales. 
Using simple and generic heuristics, such as only considering shorter questions with shorter rationales, further helps mitigate the effect of imperfect Zero-Shot-CoT capabilities (Appendix \ref{appendix:rule}). 

\section{Auto-CoT: Automatic Chain-of-Thought Prompting}
\label{sec:proposal}

Based on the observations and considerations in Section \ref{sec:prelim}, we propose an \textbf{Auto-CoT} method to construct demonstrations with questions and reasoning chains automatically. Auto-CoT consists of two main stages: (i) question clustering: partition questions of a given dataset into a few clusters; (ii) demonstration sampling: select a representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. The overall procedure is illustrated in Figure~\ref{fig_overview}. 

\begin{figure}[htb]
  \begin{center}
   \includegraphics[width=1.0\columnwidth]{fig_model.pdf}
  \end{center}
  \caption{Overview of the Auto-CoT method. Different from Manual-CoT in Figure \ref{fig_examples}, demonstrations (on the right) are automatically constructed one by one (total: $k$) using an LLM with the ``Let's think step by step'' prompt.}
  \label{fig_overview}
\end{figure}

\subsection{Question Clustering} 

Since diversity-based clustering may mitigate misleading by similarity
(Section \ref{subsec:diversity may mitigate}), we perform cluster analysis for a given set of questions $\mathcal{Q}$. 
We first compute a vector representation for each question in $\mathcal{Q}$ by Sentence-BERT \citep{reimers-2019-sentence-bert}. 
The contextualized vectors are averaged to form a fix-sized question representation. Then, the question representations are processed by the $k$-means clustering algorithm
to produce $k$ clusters of questions. For questions in each cluster $i$, sort them into a list $\mathbf{q}^{(i)} = [q_1^{(i)}, q_2^{(i)}, \ldots]$ in the ascending order of the distance to the center of cluster $i$.
This question clustering stage is summarized in Algorithm \ref{alg:cluster}.

\subsection{Demonstration Sampling} 
In the second stage, we need to generate reasoning chains for those sampled questions and then sample demonstrations that satisfy our selection criteria.

More concretely, we construct a demonstration $d^{(i)}$ (concatenation of a question, a rationale, and an answer) for each cluster $i$ ($i=1,\ldots, k$).
For cluster $i$, we iterate over questions in the sorted list $\mathbf{q}^{(i)} = [q_1^{(i)}, q_2^{(i)}, \ldots]$ (obtained by Algorithm \ref{alg:cluster}) until satisfying our selection criteria.
In other words, a question that is closer to the center of cluster $i$ is considered earlier. Say that the $j$-th closest question $q_j^{(i)}$ is being considered.
A prompted input is formulated as: [Q: \texttt{$q_j^{(i)}$}. A: \texttt{[P]}], where \texttt{[P]} is a single prompt ``Let's think step to step''.
This formed input is fed into an LLM using Zero-Shot-CoT \citep{kojima2022large} to output the reasoning chain consisting of the rationale $r_j^{(i)}$ and the extracted answer $a_j^{(i)}$. 
Then, a candidate demonstration $d_j^{(i)}$ for the $i$-th cluster is constructed by concatenating the question, rationale, and answer: $[\text{Q: } q_j^{(i)}, \text{A: } r_j^{(i)} \circ a_j^{(i)}]$.

Similar to the criteria of the hand-crafting demonstrations in \citet{cot_wei}, our selection criteria follow simple heuristics to encourage sampling simpler questions and rationales: set the selected demonstration $d^{(i)}$ as $d_j^{(i)}$ if it has a question $q_j^{(i)}$ with no more than $60$ tokens and a rationale $r_j^{(i)}$ with no more than $5$ reasoning steps.\footnote{Because Zero-Shot-CoT often uses ``$\backslash$n'' for separating the reasoning steps, the rule can be easily implemented by counting the ``$\backslash$n'' tokens in the generated rationales.} 

\begin{minipage}{0.46\textwidth}
  \begin{algorithm}[H]\small
    \caption{Cluster}\label{alg:cluster}
\begin{algorithmic}[1]
\Require A set of questions $\mathcal{Q}$ and the number of demonstrations $k$
\Ensure Sorted questions $\mathbf{q}^{(i)} = [q_1^{(i)}, q_2^{(i)}, \ldots]$ for each cluster $i$ ($i = 1, \ldots, k$)
\Procedure{Cluster}{$\mathcal{Q}$, $k$}
\For{each question $q$ in $\mathcal{Q}$}
\State Encode $q$ by Sentence-BERT
\EndFor
\State Cluster all the encoded question representations into $k$ clusters
\For{each cluster $i = 1, \ldots, k$}
\State Sort questions $\mathbf{q}^{(i)} = [q_1^{(i)}, q_2^{(i)}, \ldots]$ in the ascending order of the distance to the cluster center
\EndFor
\State \textbf{return} $\mathbf{q}^{(i)}$ ($i = 1, \ldots, k$)
\EndProcedure%
\end{algorithmic}
  \end{algorithm}
\end{minipage}
  \hfill
\begin{minipage}{0.52\textwidth}
  \begin{algorithm}[H]\small
  \caption{Construct}\label{alg:construct}
\begin{algorithmic}[1]
\Require Sorted questions $\mathbf{q}^{(i)} = [q_1^{(i)}, q_2^{(i)}, \ldots]$ for each cluster $i$ ($i = 1, \ldots, k$), empty demonstration list $\mathbf{d}$
\Ensure Demonstration list $\mathbf{d} = [d^{(1)}, \ldots, d^{(k)}]$ 
\Procedure{Construct}{$\mathbf{q}^{(i)}, \ldots, \mathbf{q}^{(k)}$}
\For{each cluster $i = 1, \ldots, k$}
\For{each question $q_j^{(i)}$ in $\mathbf{q}^{(i)}$}
\State Generate rationale $r_j^{(i)}$ and answer $a_j^{(i)}$ for $q_j^{(i)}$ using Zero-Shot-CoT
\If{$q_j^{(i)}, r_j^{(i)}$ satisfy selection criteria}
    \State Add $d^{(i)} = [\text{Q: } q_j^{(i)}, \text{A: } r_j^{(i)} \circ a_j^{(i)}]$ to  $\mathbf{d}$
    \State \textbf{break}
\EndIf
\EndFor
\EndFor
\State \textbf{return} $\mathbf{d}$ 
\EndProcedure%
\end{algorithmic}
  \end{algorithm}
\end{minipage}

As summarized in Algorithm \ref{alg:construct}, after demonstration sampling for all the $k$ clusters, there will be $k$ constructed demonstrations $[d^{(1)}, \ldots, d^{(k)}]$.
The constructed demonstrations are used to augment a test question $q^{\text{test}}$ for in-context learning. Specifically, the input is the concatenation of all the demonstrations $[d^{(1)}, \ldots, d^{(k)}]$ followed by [Q: $q^{\text{test}}$. A: \texttt{[P]}].
This input is fed to LLMs to obtain the reasoning chain with the answer in the end for $q^{\text{test}}$ (right of Figure \ref{fig_overview}).

\section{Experiments}

We briefly describe the experimental setup and present main experimental results.
More experimental details and results can be found in the appendices.

\subsection{Experimental setup}
\paragraph{Tasks and Datasets.} Our method is evaluated on ten benchmark datasets from three categories of reasoning tasks: (i) arithmetic reasoning (MultiArith \citep{multiarith}, GSM8K \citep{gsm8k}, AddSub \citep{addsub}, AQUA-RAT \citep{aqua}, SingleEq \citep{koncel2015parsing}, SVAMP \citep{svamp}); (ii) commonsense reasoning (CSQA \citep{commonsenseqa}, StrategyQA \citep{strategyqa}); (iii) symbolic reasoning (Last Letter Concatenation, Coin Flip) \citep{cot_wei}.

\paragraph{Implementation.} We use the public  GPT-3 \citep{brown2020language} of the text-davinci-002 version
with 175B parameters for the LLM  \citep{instructgpt} unless otherwise stated. We select this LLM because it has the strongest CoT reasoning performance among public LLMs, as reported in \citet{kojima2022large} and \citet{cot_wei}.
We also evaluate the Codex model \citep{chen2021evaluating} (code-davinci-002) as the LLM.
Following \citet{cot_wei}, the number of demonstrations $k$ is 8  except for AQuA and Letter (4), CSQA (7), and StrategyQA (6).

\paragraph{Baselines.} We compare our methods with four baseline methods: Zero-Shot \citep{kojima2022large}, Zero-Shot-CoT \citep{kojima2022large}, Few-Shot \citep{cot_wei}, and Manual-CoT \citep{cot_wei}. Zero-Shot-CoT and Manual-CoT are illustrated in Figure~\ref{fig_examples}. The Zero-Shot baseline concatenates a test question with the prompt ``The answer is'' as the LLM input. The Few-Shot baseline has the same LLM input as Manual-CoT except for removed rationales from all the demonstrations.

\begin{table}[t]\centering

\setlength{\tabcolsep}{3.6pt}
\caption{Accuracy on ten datasets from three categories of reasoning tasks. 
}
\vspace{2.8mm}
\begin{tabular}{lcccccccccc}\toprule
Model &\multicolumn{6}{c}{\textit{Arithmetic}} &\multicolumn{2}{c}{\textit{Commonsense}} &\multicolumn{2}{c}{\textit{Symbolic}}\\
\cmidrule(r){2-7}
\cmidrule(r){8-9}%
\cmidrule(r){10-11}%
&MultiArith   &GSM8K &AddSub &AQuA &SingleEq &SVAMP &CSQA &Strategy &Letter &Coin  \\\midrule
Zero-Shot  &  22.7 & 12.5  & {77.0}& 22.4 & {78.7}& 58.8 & {72.6} & {54.3} & 0.2 & 53.8\\
Zero-Shot-CoT  & {78.7}  & {40.7}& 74.7& {33.5} & {78.7} & {63.7} & 64.6 & 54.8 & 57.6 & 91.4\\
\midrule
Few-Shot & 33.8  & 15.6 & 83.3 & 24.8 & 82.7 & 65.7 & \textbf{79.5} & \textbf{65.9} & 0.2 & 57.2\\
Manual-CoT   & 91.7 & 46.9& 81.3 & 35.8 & {86.6} & {68.9} & 73.5 & 65.4 & 59.0 & 97.2 \\
\midrule
Auto-CoT & \textbf{92.0}   & \textbf{47.9}   & \textbf{84.8}   & \textbf{36.5}  & \textbf{87.0}  & \textbf{69.5}  &  74.4  &  {65.4} & \textbf{59.7} & \textbf{99.9}  \\

\bottomrule
\end{tabular}
\label{tab:main_results}
\end{table}

\begin{wraptable}{r}{0.48\textwidth}
    \centering
    \caption{Accuracy using the Codex LLM.}\label{exp-codex} 
    \setlength{\tabcolsep}{3pt}
            \begin{tabular}{lccc}
    \toprule
     {Method}& {MultiArith} & {GSM8K}& {AddSub} \\
    \midrule
    Zero-Shot-CoT & 64.8 & 31.8 & 65.6  \\
    Manual-CoT &\textbf{96.8} & 59.4 & 84.6 \\
     \midrule
    Auto-CoT  & {93.2} & \textbf{62.8} & \textbf{91.9} \\
    \bottomrule
  \end{tabular}
\end{wraptable}

\subsection{Competitive Performance of Auto-CoT on Ten Datasets}

Table \ref{tab:main_results} compares accuracy on ten datasets from three categories of reasoning tasks.
The Zero-Shot and Zero-Shot-CoT results are taken from \citet{kojima2022large}, the Few-Shot and Manual-CoT results are taken from \citet{cot_wei}, and the Auto-CoT results are averaged over three random runs.
Overall, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. 
Due to the cost of manual designs, Manual-CoT may design the same demonstrations for multiple datasets (e.g., $5/6$ of the arithmetic datasets). In contrast, Auto-CoT is more flexible and task-adaptive: every single dataset gets its own demonstrations that are automatically constructed.

\subsection{Visualization of Question Clustering}\label{appendix:vis}

Figure~\ref{fig:vis} visualizes question clustering (with PCA projection) in ten datasets. The illustration indicates that there exist generic patterns, where different patterns may be characterized by questions from different clusters. We present the constructed demonstrations of Auto-CoT in Appendix \ref{sec:appendix-full-prompts}. 

\begin{figure}[htb]
  \begin{center}
  \includegraphics[width=1.0\columnwidth]{fig_vis.pdf}
  \end{center}
  \caption{Question clustering on ten datasets of reasoning tasks. Stars denote cluster centers.}
  \label{fig:vis}
\end{figure}

\subsection{General Effectiveness Using the Codex LLM}

To evaluate the general effectiveness of Auto-CoT using different LLMs, here we change the LLM to the Codex model \citep{chen2021evaluating}.
As in Table \ref{exp-codex}, the Codex LLM leads to performance improvement for Manual-CoT when compared with 
Table \ref{tab:main_results} that uses the GPT-3 (text-davinci-002) LLM. 
Nonetheless, using the Codex LLM, the overall performance of Auto-CoT is still competitive compared to Manual-CoT, providing additional empirical evidence for the effectiveness of Auto-CoT.

\subsection{Effect of Wrong Demonstrations}\label{analysis:incorrect}

Recall our discussions in Section \ref{subsec:diversity may mitigate} that there can be wrong demonstrations (whose answers are wrong).
To see if diversity mitigates this effect, we design an In-Cluster Sampling baseline
that constructs demonstrations by randomly sampling questions from the same cluster that contains a test question.
Figure~\ref{fig_wr} compares accuracy with varying amounts of wrong demonstrations on MultiArith. 
Compared with In-Cluster Sampling, Auto-CoT (using diversity-based clustering) is less affected by wrong demonstrations: its performance still does not degrade significantly even when presented with 50\% wrong demonstrations. 
 
\makeatletter\def\@captype{figure}\makeatother
\begin{minipage}{.42\textwidth}\centering
{
\pgfplotsset{compat=1.13,
    /pgfplots/ybar legend/.style={
    /pgfplots/legend image code/.code={%
       \draw[##1,/tikz/.cd,yshift=-0.25em]
        (0cm,0cm) rectangle (7pt,0.8em);},
   },
}
\pgfplotsset{width=6.92cm, height=4.5cm}
    \centering
    \vspace{2.8mm}
    \begin{tikzpicture}  
        \begin{axis}  
        [  
            ybar,
            ymin=75, ymax=100,
            ytick={80,85,90,95,100},
            major x tick style = transparent,
            bar width=6.8pt,
            enlarge x limits=0.2,
            ylabel={Accuracy (\%)},
            symbolic x coords={0,1,2,3},  
            xtick=data,  
            xticklabels={12.5\%, 25.0\%, 37.5\%, 50.0\%},
            xlabel={Percentage of wrong demonstrations},
        legend cell align=left,
         legend columns=2 row=1,
                legend style={
                        at={(0.5,1.05)},
                        anchor=south,
                        column sep=1ex,
                        font=\small,
                }
            ]  
        \addplot[ybar, fill=bananayellow,  postaction={pattern=north east lines}] coordinates {
            (0,91.2)(1,88.0)(2,83.5)(3,80.5)
        };  
        \addplot[ybar, fill=babyblue,  postaction={pattern=north west lines}] coordinates {
            (0, 93.7)(1, 93.3)(2, 89.8)(3, 90.2)
        };
        \legend{In-Cluster Sampling,Auto-CoT} 
        \end{axis}  
    \end{tikzpicture}
    \caption{
   Effect of wrong demonstrations.\label{fig_wr}
    }
}
\end{minipage}
\quad
\makeatletter\def\@captype{figure}\makeatother
\begin{minipage}{.55\textwidth}
	\centering
{
\pgfplotsset{compat=1.13,
    /pgfplots/ybar legend/.style={
    /pgfplots/legend image code/.code={%
       \draw[##1,/tikz/.cd,yshift=-0.25em]
        (0cm,0cm) rectangle (7pt,0.8em);},
   },
}
\pgfplotsset{width=7.72cm, height=4.5cm}
    \centering
    \vspace{2.8mm}
    \begin{tikzpicture}  
        \begin{axis}  
        [  
            ybar,
            ymin=60, ymax=100,
            ytick={60,70,80,90,100},
            major x tick style = transparent,
            bar width=2.8pt,
            legend columns=3 row=1,
            ylabel={Accuracy (\%)},xlabel={Batch},
            symbolic x coords={1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20},  
            xtick=data,  
        legend cell align=left,
legend style={
                        at={(1,1.05)},
                        anchor=south east,
                        column sep=1ex,
                        font=\small,
                },
            ]  
        \addplot[ybar, fill=cinnamon,  postaction={}] coordinates {
            (1, 80.0) (2, 73.33)(3, 83.33)(4, 73.33)(5, 70.0)(6, 76.66)(7, 83.33)(8, 90.0)(9, 66.66)(10, 83.33)
        };
        \addplot[ybar, fill=blanchedalmond,  postaction={pattern=north east lines}] coordinates {
            (1, 96.66) (2, 96.66)(3, 96.66)(4, 96.66)(5, 93.33)(6, 83.33)(7, 86.66)(8, 93.33)(9, 90.0)(10, 96.66)
        };  
        \addplot[ybar, fill=babyblue,  postaction={pattern=north west lines}] coordinates {
          (1, 80.0) (2, 93.33)(3, 96.66)(4, 93.33)(5, 96.66)(6, 93.33)(7, 96.66)(8, 90.0)(9, 80.0)(10, 96.66)
        };  
        \legend{\scriptsize{Zero-Shot-CoT}, \scriptsize{Manual-CoT}, \scriptsize{Auto-CoT*}}
        \end{axis}  
    \end{tikzpicture}  
    \caption{Bootstraping for the streaming setting.\label{fig:boostrap}}
}
\end{minipage}

\subsection{More Challenging Streaming Setting}\label{sec:bootstraping}

CoT studies commonly assume that a full dataset with test questions is given \citep{cot_wei,kojima2022large}. 
Based on the given dataset, Auto-CoT samples questions to construct the demonstrations.
Nonetheless, now we consider a more challenging \emph{streaming setting} where a small batch of test questions (say $m$ questions) arrive at a time like in data streams.

To address this challenge, we extend Auto-CoT to a bootstrapping  version Auto-CoT*:
(i) Initialize an empty set $\mathcal{M}_0$;
(ii) When batch $1$ of questions $q_1^{(1)}, \ldots, q_m^{(1)}$ arrive, invoke Zero-Shot-CoT (no clustering due to small $m$) for each $q_i^{(1)}$ to obtain its reasoning chain $c_i^{(1)}$.
Add question-chain pairs $(q_1^{(1)}, c_1^{(1)}), \ldots, (q_m^{(1)}, c_m^{(1)})$ to $\mathcal{M}_0$ and call the new set $\mathcal{M}_1$;
(iii) When batch $b$ ($b>1$) of questions $q_1^{(b)}, \ldots, q_m^{(b)}$ arrive, construct demonstrations with existing questions and reasoning chains in $\mathcal{M}_{b-1}$ (like Auto-CoT) and use the demonstrations for in-context reasoning for each $q_i^{(b)}$.
Add question-chain pairs $(q_1^{(b)}, c_1^{(b)}), \ldots, (q_m^{(b)}, c_m^{(b)})$ to $\mathcal{M}_{b-1}$ and call the new set $\mathcal{M}_b$.

Figure~\ref{fig:boostrap} 
compares the accuracy on MultiArith at each batch ($m=30$) in this streaming setting 
(extended version: Figure~\ref{appendix-fig:boostrap} in the Appendix). 
As expected, for batch $1$, Auto-CoT* and Zero-Shot-CoT obtain equal accuracy. 
From batch $2$, Auto-CoT* performs comparably with Manual-CoT.
This result indicates that our method is still effective in the more challenging streaming setting.

\section{Conclusion}

LLMs have shown reasoning capabilities with CoT prompting. The superior performance of Manual-CoT hinges on the hand-crafting of demonstrations. To eliminate such manual designs, we proposed Auto-CoT to automatically construct demonstrations. It samples questions with diversity and generates reasoning chains to construct demonstrations. Experimental results on ten public benchmark reasoning datasets showed that with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. 

\newpage

\newpage

\end{document}