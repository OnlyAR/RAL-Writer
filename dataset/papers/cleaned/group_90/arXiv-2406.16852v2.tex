\title{Long Context Transfer from Language to Vision}

\begin{document}

\maketitle
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{ Equal contribution. $^\dagger$Project lead. \textsuperscript{\Letter}Corresponding author.}
\renewcommand*{\thefootnote}{\arabic{footnote}}

\begin{abstract}

Video sequences offer valuable temporal information, but existing large multimodal models (LMMs) fall short in understanding extremely long videos. Many works address this by reducing the number of visual tokens using visual resamplers. Alternatively, in this paper, we approach this problem from the perspective of the language model. By simply extrapolating the context length of the language backbone, we enable LMMs to comprehend orders of magnitude more visual tokens without any video training. We call this phenomenon \textit{long context transfer} and carefully ablate its properties. To effectively measure LMMs' ability to generalize to long contexts in the vision modality, we develop V-NIAH (Visual Needle-In-A-Haystack), a purely synthetic long vision benchmark inspired by the language model'
s NIAH test. Our proposed Long Video Assistant (LongVA) can process 2000 frames or over 200K visual tokens without additional complexities. With its extended context length, LongVA achieves state-of-the-art performance on Video-MME and MLVU among 7B-scale models by densely sampling more input frames. Our work is open-sourced at \href{https://github.com/EvolvingLMMs-Lab/LongVA}{\texttt{https://github.com/EvolvingLMMs-Lab/LongVA}}.

\end{abstract}\section{Introduction}

Driven by the progress of Large Language Models (LLMs)~\citep{gpt3, anil2023palm, touvron2023llama, geminiteam2024gemini, ormazabal2024reka,mixtral2024,commandrplus2024}, multiple studies are conducted to extend their capability to understand images and videos~\citep{li2023blip2, dai2023instructblip, qwenvl2024, liu2023llava}. With modality alignment and visual instruction tuning, these Large Multimodal Models (LMMs) have shown impressive abilities such as captioning and visual question-answering. While current LMMs have demonstrated promising performance on tasks involving single images and short videos~\citep{song2024moviechat, lin2023videollava, maaz2023videochatgpt, zhang2023videollama}, effectively processing and understanding extremely long videos remains a significant challenge~\citep{wang2024lvbench}.

One primary reason for this challenge is the excessive number of visual tokens generated by the vision encoder. For instance, LLaVA-1.6~\citep{liu2024llavanext} can produce 576 to 2880 visual tokens for a single image. The number of visual tokens increases significantly with the addition of more frames. To address this problem, numerous methods have been proposed to reduce the number of visual tokens. One popular direction is to modify the visual resampler that connects the vision encoder and LLM, aiming to extract fewer tokens~\citep{li2023blip2, li2023llamavid, cai2024matryoshka, cheng2024videollama}. Alternative approaches~\citep{chen2024image,shang2024llavaprumerge, jin2024chatunivi, zhou2024streaming} employ heuristic techniques to prune or merge the visual features. However, despite these efforts, Table~\ref{table:existing models} demonstrates that the majority of current LMMs are still limited in their ability to process a large number of frames effectively.

Another issue hindering the development of high-performance long video LMMs is the lack of high-quality long video datasets. In Table \ref{table:it_dataset_info}, we list the average video length of existing video instruction tuning data. Most datasets consist of video clips within 1 minute. Even if some datasets do contain longer videos, the corresponding text pairs are generated by annotating only several frames within that video, lacking long and dense supervision signals. 

Given the circumstance, in this paper, instead of reducing the visual tokens, we identify the more critical issue limiting the visual context length in existing LMMs: the context length of the language model backbone. Given a language model, we first extend its context length by training on longer text data. We then use this context-extended LM as the backbone to perform modality alignment and visual instruction tuning without any long video text pairs. By training this way, the context length of the language model is directly transferred to that of the LMMs. We further proposed \textit{UniRes}, a unified encoding scheme that represents videos as extended images, enhancing the capability fusion between images and videos.
To facilitate benchmarking and accurately assess the context length in the visual domain, we created V-NIAH, a synthetic visual benchmark based on the Needle-in-a-haystack test~\citep{niah} used in language models. Our model, Long Video Assistant (LongVA), is capable of accurately retrieving visual information from 2000 frames or more than 200K visual tokens. Experiments show that additional frames during inference lead to improved performance on long video question-answering benchmarks, and LongVA achieves state-of-the-art performance among 7B models on the Video-MME~\citep{fu2024videomme} and MLVU~\citep{zhou2024mlvu} dataset. 
In summary, our paper makes the following contributions:

\textbf{(1)} \textbf{Long Context Transfer}: We discovered the \textit{long context transfer} phenomenon where the context of the language model can be directly transferred to the modality-aligned multi-modal models.

\textbf{(2)} \textbf{Visual Needle-In-A-Haystack (V-NIAH)}: We proposed the V-NIAH benchmark to test LMMs ability in locating and retrieving visual information over extremely long contexts.

\textbf{(3)} \textbf{Long Video Assistant (LongVA)}: With \textit{long context transfer} and \textit{UniRes}, we developed LongVA that can perceive more than 200K visual tokens, achieving SoTA performance on the Video-MME and MLVU dataset. 

\begin{table}[t]
\begin{center}
\scalebox{0.8}{ % Adjust the value to scale the table
\begin{tabular}{lrrrrrr}
\toprule
 Model & Tokens/Frames\textsuperscript{*}  &Training Max Frames\textsuperscript{*}  & LM Backbone & LM Context Length \\
\midrule
MPLUG-Owl-video~\citep{ye2024mplugowl}  &  256 &  4  &   LLaMA  &  4K	 \\
MovieChat~\cite{song2024moviechat}  &  32 &  8  &   Vicuna-v0  &  2K	 \\
Video-LLaVA~\citep{zhang2023videollama}  &  49 &  8  &   Vicuna-1.5  &  4K	  \\
VideoChat~\citep{li2024videochat}  &  32$/$196 &  8  &   Vicuna-v0  &  2K	  \\
LLaVA-NeXT-Video~\citep{zhang2024llavanextvideo} &  144 &  16 &  Vicuna-1.5 &  4K \\
ST-LLM~\citep{liu2024stllm}  &  256 &  16  &   Vicuna-1.1  &  2K	  \\
Video-LLaMA~\citep{cheng2024videollama}  &  32 &  32  &   LLaMA-2  &  4K	  \\
Chat-UniVi~\citep{jin2023chatunivi}  &  112 &  64  &   Vicuna-1.5  &  4K	 \\
TimeChat~\citep{ren2024timechat}  &  4 &  96  &   LLaMA-2  &  4K	 \\
Video-ChatGPT~\cite{maaz2023videochatgpt}  &  256 &  100  &   Vicuna-1.1 &  2K	 \\
LLaMA-VID~\citep{li2023llamavid}  &  2 &  300  &   Vicuna-1.5  &  4K	 \\
\midrule
LongVA (Ours) & 144 & - & Qwen2-Extended & 224K+ \\
\bottomrule
\end{tabular}
}
\end{center}
\caption{To enable longer video inputs, previous works train 
 fewer visual tokens to increase the maximum frames during training. Our LongVA, on the other hand, enables long video capability by extending the backbone language model. \textsuperscript{*}We report it based on the best available information from their paper or released codebase.
}
\label{table:existing models}
\end{table}

\section{Related Work}
	
\paragraph{Vision Language Connector in Large Multimodal Models}
Existing studies explore different architectures to extract and inject visual features into LLMs. One line of work~\citep{flamingo, li2023otter, open_flamingo, lauren√ßon2023obelics}, pioneered by Flamingo~\cite{flamingo}, adopts a resampler to compress the visual feature and inserts cross-gated attention layers into the LLM. Some other works still use a reampler~\citep{li2023blip2, zhu2023minigpt4, qwenvl2024} while directly feeding the image feature into the input layer of the language model. The LLaVA series~\citep{liu2024llavanext, liu2023improvedllava, liu2023llava} use a simple and scalable design to directly project the image features into language model without any pooling or resampling. When the field moves from image-only models to include multi-image and video inputs, more modifications to the visual language connector were proposed. \cite{zhang2024llavanextvideo} and \cite{cai2024matryoshka} use a simple average pooling. \cite{jin2024chatunivi} dynamically drop the visual tokens. \cite{cheng2024videollama} adopt a spatial-temporal convolution to better capture the dynamics of video data and reduce feature size. Our proposed context transfer from text to image is orthogonal to those works and can further enable LMMs to understand more frames. 

\begin{table}[t]
\centering
\begin{minipage}{0.45\textwidth}
\centering
\caption{Existing Video SFT Datasets}
\label{table:it_dataset_info}
\scalebox{0.8}{
\begin{tabular}{lcc}
\toprule
Dataset Name & Video Length (sec.) & Text Length  \\
\midrule
VideoChatGPT-100K~\citep{maaz2023videochatgpt} & 123.4 & {\color{white}0}68.0 \\
LLaVA-Hound-255K ~\citep{zhang2024direct} & {\color{white}0}52.4 & {\color{white}0}37.6 \\
ShareGPT4Video\citep{chen2024sharegpt4video} & {\color{white}0}26.6 & 273.3 \\
TimeIT~\citep{ren2024timechat} & 190.8 & {\color{white}0}52.5 \\
VideoChat~\citep{li2024videochat} & {\color{white}00}9.5 & {\color{white}0}59.0 \\
\bottomrule
\end{tabular}
}
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
\centering
\caption{ Video Benchmarks}
\label{table:existing_benchmarks}
\scalebox{0.8}{
\begin{tabular}{lc}
\toprule
Benchmark Name  & Video Length (sec.)\\
\midrule
VideoChatGPT~\citep{maaz2023videochatgpt} &  {\color{white}0}108.0 \\
NexTQA~\citep{xiao2021nextqanext} &  {\color{white}00}42.9 \\
EgoSchema~\citep{mangalam2023egoschema} &  {\color{white}0}179.8 \\
VideoMME~\citep{fu2024videomme} &  1017.0 \\
V-NIAH (Ours) & {\color{white}000.}$\infty$ \\
\bottomrule 
\end{tabular}
}
\end{minipage}
\end{table}
 
\paragraph{Context Extrapolation in Transformer}
Transformer does not directly work on sequences longer than its training length. To alleviate that, various RoPE-based ~\citep{su2023roformer} extension techniques~\citep{chen2023extending,NTK-aware, rozi√®re2024code,peng2023yarn,ding2024longrope} have been proposed to allow for training-free context extrapolation. Efforts have also been made on data curation~\citep{fu2024data,xiong2023effective,bai2024longalign} and system optimization~\citep{li-etal-2023-sequence,liu2023ring,jacobs2023deepspeed} during long context training. There has been limited exploration of the context extrapolation in the domain of LMMs. \cite{liu2023world} are closest to our work and train LMM with long context language models, but they do not benchmark the effective visual context length of their model.

\paragraph{Video Language Benchmarks}
Recent years have witnessed significant progress in Video Question-Answering\cite{antol2015vqa}.
To accurately measure the progress of the video LMMs' performance, researchers have developed various benchmarks encompassing a broad spectrum of tasks. These range from fundamental visual perception tasks such as activity recognition\cite{yu2019activitynet}, concept detection~\cite{xu2017video}, and counting~\cite{jang2017tgif}, to more complex visual reasoning tasks including compositional~\cite{grunde2021agqa}, causal~\cite{xiao2021next,yi2019clevrer,xu2021sutd}, and situated reasoning~\cite{wu2021star}. 
However, most of those benchmarks focus on short videos, lacking data and metrics to test LMMs' capability over a long context. Inspired by the NIAH test~\citep{niah} in the language model community, we proposed V-NIAH to benchmark LMMs' ability over long visual inputs with the minimum overhead of data collection and human annotation. Several concurrent works also developed multimodal versions of the Needle-in-a-haystack test~\cite{wang2024needle,zhou2024mlvu,song2024milebenchbenchmarkingmllmslong, wang2024multimodalneedlehaystackbenchmarking}. However, they only measure on several hundreds of frames and lack a strong baseline to properly analyze the properties of visual context length.
\section{Long Video Assistant}

As in Figure \ref{fig:longva_plot_main}, this paper centers around the hypothesis that \textit{if the modality of vision and language can be truly aligned, the capability to handle long contexts could also transfer from text to vision}, and this could happen even without explicit long video training. Our methodology is thus very straightforward. Given a language model, we first perform long context training purely on language to extend its text context (Section \ref{sec:long-lm-training}). We then detailed how we augment this language model with long visual capabilities by training solely on short image data in Section \ref{sec:vl-alignment}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{image/longva_plot.png}
    \caption{\textbf{Left:} to develop long vision models,  previous studies proposed better visual resamplers to reduce the number of visual tokens. \textbf{Right:} LongVA approaches this problem from the angle of the language model. We leverage image data (short visual input) to align long-context LLM with vision. During the test time, LongVA can zero-shot process extremely long videos, thanks to the property of long context transfer.}
    \label{fig:longva_plot_main}
\end{figure}

\subsection{Training Long Language Model}
\label{sec:long-lm-training}
We use Qwen2-7B-Instruct~\citep{qwen2} as the backbone language model and perform continued pretraining with a context length of 224K\footnote{224K is the maximum we can fit with 8$\times$A100-80G for Qwen-2-7B. We find that the embedding size significantly impacts the maximum sequence length in our optimized codebase. Qwen2 has a huge vocabulary of 152K tokens. For LLaMA2 with 32K vocabulary, we can train it with 700K context length.} over a total of 900M tokens. We follow~\cite{xiong2023effective} to increase RoPE~\citep{su2023roformer}  base frequency during the continued pertaining and specifically set it to 1B. A constant learning rate of 1e-5 is maintained for a batch size of one million tokens across 1,000 training steps. Following \cite{fu2024data}, we construct the dataset used for long context training from Slimpajama~\citep{slimpajama} by upsampling documents longer than 4096 and keeping the domain mixture ratio unchanged. Multiple documents are packed into a single sequence separated by a \texttt{BOS} token.

 We employed several optimization strategies to perform training on such long sequences. These includes FlashAttention-2~\citep{dao2023flashattention2}, Ring Attention~\cite{liu2023ring, li-etal-2023-sequence}, activation checkpointing, and parameter offload~\cite{rajbhandari2020zero}. To balance the load across different GPUs, we shard the sequence in a zigzag way~\cite{zigzagringattn} in ring attention. The resulting training framework is memory efficient and maintains very high GPU occupancy.  Note that we do not use any parameter-efficient methods such as LoRA~\citep{hu2021lora} or approximate attention~\citep{child2019generating}. With those optimizations, the compute used in long context training is minimal compared to that of language model pretraining, making it feasible for academic budgets. The long context training can finish in 2 days with 8 A100 GPUs.

In Figure \ref{fig:text_niah_results}, we evaluate the extended Qwen2 with the Needle-in-a-haystack (NIAH) test~\citep{arizeniah, niah}. It achieves perfect results within the training context length (224K) and generalizes even further. We find the vanilla NIAH to be a relatively trivial benchmark and further test it with 5 distractors randomly inserted into the documents. The detailed configuration can be found in Appendix \ref{sec:text_niah_details}.

\subsection{Aligning Long Language Model Using Short Vision Data}
\label{sec:vl-alignment}

 Inspired by the \textit{AnyRes} encoding scheme in LLaVA-NeXT~\citep{liu2024llavanext, li2024llavanext-ablations}, we designed \textit{UniRes} that provides a unified encoding scheme for both images and videos, as shown in Figure \ref{fig:unires_plot}. Unlike \textit{AnyRes} which retains a small base image and flattens ViT patches across the grids, \textit{UniRes} removes the base image, flattens patches within each grid, and 2x2 pool the visual features by default (Appendix \ref{appendix:unires}). This approach allows us to maintain consistent representation when extending image data into videos where multiple frames are viewed as multiple grids in a row. 
 
 Specifically, \textit{UniRes} divides an input image of resolution $a \times b$ into smaller grids, each with a resolution of $336 \times 336$ pixels. This results in $(a//336) \times (b//336)$ grids. For very high-resolution images, we limit the maximum number of grids to 49, resizing images larger than this threshold. Each grid is separately encoded using \texttt{CLIP-ViT-L-336px}~\citep{radford2021learning} and then projected through a 2-layer MLP to match the LM's input dimension, resulting in 576 features per grid. We then apply 2x2 average pooling, finally converting an $a \times b$ image into $(a//336) \times (b//336) \times 144$ tokens. During inference, this visual encoding scheme allows videos to be represented as very long images (even though we do not train on videos). An $N$-frame video is treated as an image of size $336 \times (336 \times N)$, divided into $N$ grids where each grid corresponds to a video frame. Using CLIP encoding, MLP projection, and average pooling, an $N$-frame video is encoded into $144N$ visual tokens. 

To clearly ablate the long context transfer phenomenon from language to vision, we adopt a \textit{train short, test long} protocol where we only use image-text data during training, but test on long videos. We trained our model using the same data recipe and two-stage training approach as LLaVA-1.6. Our experiments show that compared to \textit{AnyRes}, \textit{UniRes} has slightly lower scores on low-resolution image benchmarks (Table \ref{tab:image-results}) but performs better on V-NIAH (Figure \ref{fig:vniah_plot}) and Video-MME (Table \ref{tab:video-mme-results}). We believe the unified encoding scheme for images and videos is crucial, thus choosing this as the encoding scheme of LongVA. The image-text alignment can be finished in 1.5 days. With 2 days for long context training on text, the total training cost of LongVA is 3.5 days on 8$\times$A100-80G.

It is worth noting previous work largely inspired the design choice of LongVA. For example,\cite{xiong2023effective} first demonstrates the effectiveness of long context continued pretraining with increased RoPE base frequency (thus decreasing the rotation angles). We sample the long text data following the guidance of \citep{fu2024data}. We adopt the same vision encoder and training data as that of LLaVA-1.6~\cite{liu2024llavanext}. We try to keep our methods as simple as possible to clearly show the phenomenon of long context transfer without other confounders.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{image/uniresv2.png}
    \caption{\textit{UniRes}'s unified encoding scheme of images and videos. During training, images are divided into multiple grids. During inference, videos are treated as extended images with each frame considered as a grid. }
    \label{fig:unires_plot}
\end{figure}\section{V-NIAH}
\label{sec:v-niah}

To measure the context length of language models on extremely long input, earlier works calculate perplexity scores over long documents. Recently, many have started using the Needle-in-a-Haystack (NIAH) test to benchmark LLMs' ability to retrieve long context information precisely. We note that there is so far no benchmark to measure the visual context length of LMMs. 
To evaluate LongVA's capacity to locate and retrieve long-range visual information, we extend the NIAH test from text to video and propose V-NIAH. 

As shown in Table~\ref{tab:vniah-needles}, we designed 5 video question-answering problems as the needle and inserted each as a single frame into hours-long videos. We sampled the videos at 1 FPS as the visual input.  The image of the needle is sourced from existing VQA benchmarks or AI-generated to avoid any contamination. The AI-generated images and questions are purposely chosen to be "counterfactual" or "counter-commonsense", ensuring the model cannot answer based on language knowledge alone. Each question includes a "locating prompt" so that a capable system or human can locate the needle frame from the video haystack and answer the question.

When testing LongVA with visual inputs of up to 3000 frames, one difficulty we encountered was that processing a 200K-token input requires up to 100GB of GPU memory for the KV cache for a 7B LM like LLaMA. Even with advanced LM serving systems like vLLM~\citep{kwon2023efficient} with tensor parallelism to shard the KV cache across multiple GPUs, the sampling process remains extremely slow due to limited memory and batchsize. To address this, we used "perplexity-based" evaluation to measure the correctness of the model output. We first encode all frames and save their corresponding visual embeddings. During the evaluation, we only load the language model from LongVA and concatenate the visual embeddings, question tokens, and answer tokens for a single forward pass with ring attention. This approach makes the workload compute-bound and eliminates the need to cache the KV state. The model's output is considered correct only if the highest output logits index of all tokens in the answer span matches the correct answer.

\begin{figure}[t]
\label{fig:insert_niah}
    \centering
    \includegraphics[width=\textwidth]{image/example.pdf}
    \caption{V-NIAH consists of a haystack video, a needle image, and a question related to the needle. The needle is inserted at various positions in the haystack video.}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{image/video_niah.png}
    \caption{The V-NIAH results of LongVA and its baselines. The x-axis represents the total number of frames in the video haystack. The y-axis shows the position where the needle image is located. For instance, a frame depth of 0\% would place the needle image at the very beginning of the video. The black dotted line denotes the training length of the backbone language model, with each frame corresponding to 144 tokens.}
    \label{fig:vniah_plot}
\end{figure}

\section{Experiments}
\label{sec:experiments}

\begin{table}[t]
\begin{center}
\scalebox{0.80}{
\begin{tabular}{l|cc|cccc}
\toprule
    \textbf{Model} & \textbf{LLM Params} & \textbf{Frames} & \textbf{Short} & \textbf{Medium} & \textbf{Long} & \textbf{Overall} \\
   \midrule
InternVL-Chat-V1.5~\citep{chen2023internvl} & 20B & {\color{white} 0}10 & 60.2  & 46.4  & 45.6 & 50.7 \\
LLaVA-NeXT-Video-34B~\citep{zhang2024llavanextvideo} & 34B & {\color{white}0}32 & 61.7  & 50.1  & 44.3 & 52.0 \\
VILA-1.5~\citep{lin2023vila} & 34B & {\color{white} 00}8 & 68.1 & 58.1 & 50.8 & 59.0 \\  
\midrule
Qwen-VL-Chat~\citep{qwenvl2024} & {\color{white}0}7B & {\color{white}00}4 & 46.9  & 38.7  & 37.8 & 41.1 \\
Video-LLaVA~\citep{lin2023videollava}& {\color{white} 0}7B & {\color{white}00}8 & 45.3  & 38.0  & 36.2 & 39.9 \\
ST-LLM~\citep{liu2024stllm} & {\color{white} 0}7B & {\color{white}0}64 & 45.7  & 36.8  & 31.3 & 37.9 \\
VideoChat2-Mistral~\citep{li2024videochat} & {\color{white} 0}7B & {\color{white}0}16 & 48.3  & 37.0  & 33.2 & 39.5 \\
Chat-UniVi-V1.5~\citep{jin2023chatunivi} & {\color{white} 0}7B & {\color{white}0}64 & 45.7  & 40.3  & 35.8 & 40.6 \\
VideoLLaMA2~\citep{cheng2024videollama} & {\color{white} 0}8B & {\color{white}0}16 & 56.0 & 45.4 & 42.1 & 47.9 \\
LLaVA-NeXT-Qwen2 & {\color{white} 0}7B & {\color{white}0}32 & 58.0	& 47.0	& 43.4	&49.5 \\
\midrule
\multirow{6}{*}{LongVA} & \multirow{6}{*}{{\color{white} 0}7B} & {\color{white}00}8 & 55.1 & 46.3 & 42.1 & 47.9 \\
      &    & {\color{white}0}16 & 59.0  & 46.6 & 43.6 & 49.7 \\
      &    & {\color{white}0}32 & 61.1 & 48.8 & 45.4 & 51.8 \\
      &    & {\color{white}0}64 & \textbf{61.4} & \textbf{50.9} & 45.0 & 52.4 \\
      &    & 128 & 61.1&  50.4 & \textbf{46.2} & \textbf{52.6} \\
      &    & 384 & 60.3 & 48.9 & 46.1 & 51.8\\
\bottomrule
\end{tabular}
}
\end{center}
\caption{Performance comparison of various LMMs on Video-MME~\citep{fu2024videomme} 
\textit{without subtitles}. LongVA achieves state-of-the-art results among 7B models. Its performance also increases with denser sampling of video frames.}
\vspace{-20pt}
\label{tab:video-mme-results}
\end{table}

\begin{wraptable}{r}{0.45\textwidth}
\begin{center}
\scalebox{0.80}{
\begin{tabular}{l|cc}
\toprule
    \textbf{Model} & \textbf{Qwen2-224K} & \textbf{UniRes} \\

   \midrule
LLaVA-Next-Qwen2 & $\times$ & $\times$ \\
LongVA (\textit{AnyRes}) & $\checkmark$ & $\times$ \\
LongVA & $\checkmark$ & $\checkmark$ \\
\bottomrule
\end{tabular}
}
\end{center}
\caption{LongVA and its baselines.}
\label{tab:model-comparison}
\end{wraptable}

We primarily assess the long visual capability of LongVA on two benchmarks: V-NIAH (Section \ref{subsec:vniah} and Video-MME~\citep{fu2024videomme} (Section \ref{subsec:video_eval}).  V-NIAH provides quick signals about the visual context length of LongVA. However, it only tests the model's ability to retrieve information and does not cover other abilities necessary for a real-world long video assistant. Therefore, we also include LongVA's performance on Video-MME,  a comprehensive evaluation suite for video LMMs that includes diverse data types and qualitative annotations. Video-MME is an ideal benchmark for assessing LMMs' ability to handle long videos in real-world scenarios, given its average video duration of 1017 seconds and the inclusion of short, medium, and long subsets.  We further include the benchmark results on MLVU\citep{zhou2024mlvu} in Appendix \ref{sec:mlvu results}.

We mainly compare LongVA against other image and video LMMs. To validate the phenomenon of \textit{long context transfer}, we trained LLaVA-Next-Qwen2, a baseline model based on Qwen2-7B-Instruct using the LLaVA-NeXT~\cite{liu2023improvedllava, li2024llavanext-ablations} training recipe. Additionally, we trained LongVA (\textit{AnyRes}) to showcase the advantages of our \textit{UniRes} encoding scheme. The difference between LongVA and our baselines can be found in Table \ref{tab:model-comparison}.

\subsection{V-NIAH Results}
\label{subsec:vniah}

\textbf{Long context transfers from language to vision} Figure \ref{fig:vniah_plot} shows the V-NIAH performance of LongVA and other LMMs. Specifically, Figure \ref{fig:vniah_plot} (iii) demonstrates that the visual context length of LLaVA-NeXT-Video-32K~\citep{zhang2024llavanextvideo} is constrained by the 32K context length of its language backbone, Mistral-7B-Instruct-v0.2~\citep{jiang2023mistral}, equivalent to approximately 200 frames. Beyond this limit, the V-NIAH accuracy drops significantly.  As a stronger baseline, we include the results of LLaVA-NeXT-Video-32K enhanced with a training-free length extrapolation algorithm~\citep{ntk_rope} by increasing its RoPE base frequency. We empirically determine the optimal extrapolation frequency by choosing from [3M, 10M, 30M, 100M, 300M, 1B]. As indicated in Figure \ref{fig:vniah_plot} (iv), although this training-free extrapolation allows the model to process information across an extended context, the improvement is marginal.  These findings led us to develop LongVA, a model that unlocks the visual context by extending the language model purely on text.  As shown in Figure \ref{fig:vniah_plot} (i), LongVA can almost perfectly retrieve information and answer the needle question for input frames fewer than 2000. Although we only trained LongVA's language backbone on a context length of 224K (equivalent to 1555 frames), it generalizes well beyond that, maintaining satisfactory performance within 3000 frames. Those results clearly corroborate of hypothesis of \textit{long context transfer}.

\textbf{Unified encoding enables better visual context extrapolation}
We also present the V-NIAH heatmap of LongVA trained with \textit{AnyRes} encoding scheme, keeping all other factors unchanged in Figure \ref{fig:vniah_plot} (ii). LongVA-\textit{AnyRes} demonstrates strong retrieval capabilities. However, its performance still lags behind LongVA trained with UniRes. We believe that the unified representation of images and videos in UniRes, where a video is encoded in the same way as a long image, enhances the long context transfer from language to vision. This approach also facilitates effective training with short vision data (images) and enables zero-shot understanding of long videos during inference.

\subsection{Video Evaluation}
\label{subsec:video_eval}

On Video-MME (Table \ref{tab:video-mme-results}), LongVA achieves \textit{state-of-the-art} performance among LMMs under 10B parameters, rivaling much larger ones such as LLaVA-NeXT-Video-34B~\citep{zhang2024llavanextvideo} and InternVL-Chat-V1.5~\citep{chen2023internvl}. Notably, LongVA is trained without any video data, so its performance on video can be considered \textit{zero-shot}. As the number of sampled frames increases, LongVA shows improved performance on the long subset, handling up to 384 frames\footnote{We limited our analysis to 384 frames due to computational and memory constraints as detailed in Section \ref{sec:v-niah}.}.  Even though LongVA's score slightly drops when we upsample from 128 to 384 frames, it maintains a competitive performance. To our knowledge, LongVA is the \textit{only} open-source model that can handle such large input frames on Video-MME.
These findings highlight the \textit{long context transfer} effect, where LongVA, originating from a long context language model, can process significantly more frames than its baseline, despite being trained on the same multimodal data.

We also tested LongVA on shorter benchmarks with average video durations under 120 seconds.  As indicated in Table \ref{tab:short-video-results}, although LongVA scores higher with more densely sampled frames on datasets such as NeXTQA~\citep{xiao2021nextqanext} and ActivityNetQA~\citep{yu2019activitynetqa}, the gains quickly plateau and are not as significant as those observed in Video-MME, which can be attributed to the shorter duration of these datasets.  On the VideoChatGPT and Video Detailed Description (Video-DD) benchmarks, increasing frames does not lead to better performance, and LongVA generally achieves lower scores compared to LLaVA-NeXT-Video-7B. 
Since both benchmarks use OpenAI's GPT API as a judge, we believe their metrics are closely related to the answering format. To address this, we perform a lightweight Direct Preference Optimization (DPO) on the LLaVA-Hound-DPO~\citep{zhang2024direct} dataset. We observe significantly improved performance for LongVA-DPO, confirming the findings in \cite{zhang2024direct}.

\begin{table}[t]
\begin{center}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|c|cc|c|ccccc|c}
\toprule
Model &
   &
  \multicolumn{2}{c}{NeXTQA~\citep{xiao2021nextqanext}} &
  \multicolumn{1}{|c|}{ActivityNetQA~\citep{yu2019activitynetqa}} &
  \multicolumn{5}{c|}{VideoChatGPT~\citep{maaz2023videochatgpt}} &
  \multicolumn{1}{c}{Video-DD~\citep{maaz2023videochatgpt}} \\
 &
  frames &
  MC &
  OE &
  Score &
  Consistency &
  Correctness &
  Detail &
  Context &
  Temporal &
  Score \\
  \midrule
LLaVA-NeXT-Video~\citep{zhang2024llavanextvideo} & 32 & 57.93 & 26.90 & 3.20 & 3.12 & 3.39 & 3.29 & 3.92 & 2.60 & 3.32 \\
LongVA & {\color{white}0}8  & 50.78 & 27.71 & 2.73 & 3.73 & 3.09 & 3.14 & 3.72 & 2.39 & 3.19 \\
LongVA & 16 & 61.61 & 27.87 & 2.78 & 3.61 & 3.13 & 3.15 & 3.75 & 2.40 & 3.22 \\
LongVA & 32 & 67.08 & 27.87 & 2.80 & 3.65 & 3.08 & 3.10 & 3.74 & 2.28 & 3.19 \\
LongVA & 64 & 68.27 & 27.81 & 2.84 & 3.64 & 3.05 & 3.09 & 3.77 & 2.44 & 3.14 \\
LongVA-DPO & 32 &  69.26 & 28.02 & 2.80 & 4.07 & 3.55 &  3.32& 4.09 & 2.86 & 3.58	\\
\bottomrule
\end{tabular}
}
\end{center}
\caption{Video evaluation results for LongVA on various short video benchmarks with comparison to 7B scale models.}
\vspace{-20pt}
\label{tab:short-video-results}
\end{table}

\subsection{Image Evaluation}
\label{sec: image-eval}
\begin{table}[htp]
    \begin{center}
    \setlength{\tabcolsep}{8pt}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{l|ccccccc}
        \toprule
Model                          & AI2D~\citep{kembhavi2016diagram}  & ChartQA~\citep{masry2022chartqa} & DocVQA~\citep{mathew2020docvqa} & InfoVQA~\citep{mathew2020docvqa} & RealworldQA~\citep{grok_1.5v_2024} & MMMU~\citep{yue2023mmmu} \\
\midrule
LLaVA-1.6-Vicuna~\citep{liu2024llavanext} & 66.6 & 54.8   & 74.4  & 37.1   & 57.8 & 35.1     \\
LLaVA-NeXT-LLaMA3~\citep{li2024llavanext-strong} & 71.6 & 69.5   & 78.2  & 37.6   & 60.0    & 41.7     \\
LLaVA-NeXT-Qwen2               & 73.5 & 74.0      & 81.3  & 42.0   & 61.6  & 41.9    \\
LongVA (\textit{AnyRes}) & 73.1 & 74.4 & 81.5 & 43.3 & 62.4 & 42.1 \\
LongVA (\textit{UniRes})  & 70.7 & 70.4   & 80.8  & 49.4   & 60.0   & 42.6      \\
\bottomrule
\end{tabular}
    }
\end{center}
\vspace{1mm}
\caption{Image evaluation results for LongVA on multiple benchmarks. Compared to other image multimodal models, our methods maintain high performance and achieve better scores on InfoVQA\citep{mathew2020docvqa}.}
\vspace{-20pt}
\label{tab:image-results}
\end{table}

\begin{figure}[htp]
    \centering
    \makebox[0.31\textwidth]{}
    \makebox[0.31\textwidth]{}
    \makebox[0.31\textwidth]{}
    \\
    \includegraphics[width=0.31\textwidth]{image/ai2d_image_plot.jpg}
    \includegraphics[width=0.31\textwidth]{image/chartqa_image_plot.jpg}
    \includegraphics[width=0.31\textwidth]{image/infovqa_val_image_plot.jpg}

    \caption{The 2D-histogram of the image width and height of different image benchmarks. InfoVQA\citep{mathew2020docvqa} consists of many high-resolution images compared to other benchmarks. }
    \label{fig:image-size}
\end{figure}

We further evaluate our model on various image benchmarks to investigate the image performance of LongVA (Table \ref{tab:image-results}). Compared to the LongVA (\textit{AnyRes}) baseline, LongVA with \textit{UniRes} achieves significantly increased performance on InfoVQA \citep{mathew2020docvqa}, while the scores drop to some extent on AI2D~\citep{kembhavi2016diagram} and ChartQA \citep{masry2022chartqa}. To better understand this phenomenon, we recorded and analyzed the image size of those datasets, as shown in Figure \ref{fig:image-size}. We found that InfoVQA consists of higher-resolution images, while many images in AI2D and ChartQA are smaller than 768$\times$768.
Compared to \textit{Anyres}, \textit{UniRes} operate 2$\times$2 average pooling on each image, reducing to $1/4$ visual tokens per image grid. However, the grid upper bound is set to 49 for \textit{UniRes} while 4 for \textit{AnyRes}, so \textit{UniRes} may produce more image grids if the input images are of higher resolution. By using more grids per image, \textit{UniRes} allocates more visual tokens on datasets such as InfoVQA, achieving superior performance compared to the previous 7B LLaVA model. However, most of the images in ChartQA and AI2D require fewer than 4 grids to represent. This may explain why the image performance decreases on those benchmarks.

\section{Qualitative Results}
The qualitative results of LongVA-DPO are illustrated in Figure \ref{fig:demo}. The short video example comes from \citep{xie2023funqa} and the two long videos are sourced from \href{https://www.bilibili.com/video/BV1Kn4y1d7G7/?buvid=c066506d947f48e761c78e00991fd406&from_spmid=main.space-contribution.0.0&is_story_h5=false&mid=804D54DYmsNyGN1vdgHxnA%3D%3D&p=1&plat_id=116&share_from=ugc&share_medium=iphone&share_plat=ios&share_session_id=35F6AE42-FAD2-436C-B164-107EF91E283D&share_source=WEIXIN&share_tag=s_i&spmid=united.player-video-detail.0.0&timestamp=1719191857&unique_k=AOiQBaI&up_id=23947287&share_source=weixin}{link1} and \href{https://www.bilibili.com/video/BV1F4411v7rj/?spm_id_from=333.1007.top_right_bar_window_custom_collection.content.click&vd_source=a8b9ec3a8fda7923ed9b151c46917050}{link2}, respectively. In the figure, LongVA accurately describes the short, humorous video involving individuals playfully interacting with condiments. It also identifies specific details in long videos, such as the color of a train and the colors of umbrellas used in a scene, showcasing its proficiency in retrieving and interpreting visual information over extended video contexts. These capabilities highlight LongVA's potential to overcome the challenges associated with processing and understanding extremely long videos.

\begin{figure}[htp]
    \centering
    \includegraphics[width=\textwidth]{image/demo1.pdf}
    \includegraphics[width=\textwidth]{image/demo3.pdf}
    \includegraphics[width=\textwidth]{image/demo4.pdf}
    \caption{Qualitative Result of the LongVA-DPO. Our model demonstrates superior capability in understanding both short and long videos. 
    }
    \vspace{-10pt}
    \label{fig:demo}
\end{figure}

\section{Conclusion}

This work addresses the challenges of understanding long videos in Large Multimodal Models. By extending the language model on text and then aligning this extended model with visual inputs, we significantly improved the capability of LMMs to handle long videos thanks to the \textit{long context transfer} phenomenon. Our model, LongVA, shows improved performance with more input frames and achieves state-of-the-art results on Video-MME. Additionally, we introduce a synthetic benchmark, V-NIAH, to effectively measure the visual context length of video LMMs.  We hope this work inspires further research in the field of long video LMMs and multimodal agents.\section{Acknowledgements}
This research/project is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG2-PhD-2022-01-029). Besides, this project is supported by NTU NAP, MOE AcRF Tier 2 (MOE-T2EP20221-0012), and under the RIE2020 Industry Alignment Fund ‚Äì Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as advise from the industry partner(s).

\end{document}