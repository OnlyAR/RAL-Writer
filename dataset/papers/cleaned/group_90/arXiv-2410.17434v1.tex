\title{LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding}

\begin{document}

\maketitle

\section{Introduction}

Large Language Models (LLMs)~\citep{brown2020language,ouyang2022training,chatgpt,achiam2023gpt,vicuna2023,touvron2023llama2,jiang2024mixtral} manifest universal capabilities that are instrumental in our progress towards general intelligence. Through the integration of modality alignment and visual instruction tuning, Multimodal Large Language Models (MLLMs)~\citep{alayrac2022flamingo,li2023blip2bl,zhu2023minigpt,liu2024visual,ye2023mplugowl,bai2023qwen,chen2023internvl,dong2024internlm} have demonstrated exceptional competencies in tasks such as captioning and visual question-answering. Recent literatures have initiated explorations of  extending MLLMs for the comprehension of video content~\citep{li2023videochat,zhang2023video,maaz2023video,lin2023video,wang2024internvideo2,liu2024world}. Despite exhibiting potentials across specific benchmarks, effectively processing and understanding of exceedingly lengthy videos remains a significant challenge.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{assets/teaser.pdf}
    \caption{Effectiveness of our \modelname~over commonly-used uniform sampling and dense sampling. Uniform sampling overlooks critical frames due to its sparse nature. Dense sampling may surpass the maximum context length, leading to truncation of tokens from targeted frames. In contrast, our method can adaptively conduct spatiotemporal compression, accommodating long video sequences while preserving more visual details.}
    \label{fig:teaser}
\end{figure}

One primary reason is that it is impractical to process all the information for hour-long videos, given that advanced MLLMs represent a single image using hundreds of tokens. For instance, 576 $\sim$ 2,880 tokens per image are used in LLaVA-1.6~\citep{liu2024llavanext} and 7,290 tokens are used in LLaVA-OneVision~\citep{li2024llava}. However, a commonly used and computationally manageable context length for multimodal training is 8k, which limits processing 125 frames (2-minutes video) even at 64 tokens per frame, while an hour-long video could require over 200k tokens. Consequently, in video scenarios with an extra temporal dimension, it is intractable for training due to the demand of excessive GPU memory. Various studies have attempted to establish a balance between the number of tokens and the frequency of frame sampling. Most of these studies~\citep{li2024llava,cheng2024videollama,zhang2024llavanextvideo,chen2024sharegpt4video} opt for a uniform sampling of a fixed number of video frames as the input. However, these methods naively overlook non-uniform content, e.g., static vs dynamic scenes within the video, as shown in Figure~\ref{fig:teaser}. Other approaches~\citep{li2023videochat,li2023llama,jin2023chatunivi} employ intensive resampling modules that significantly decrease the quantity of visual tokens, leading to a considerable loss of essential visual information.

In this paper, we propose \modelname~that aims to preserve as much frame information as possible while accommodating lengthy videos without exceeding the context length of commonly used LLMs. Video by its nature contains significant temporal redundancy. MovieChat~\citep{song2024moviechat} employs a similarity-based frame-level feature selection using visual representation from CLIP~\citep{radford2021learning}. While we argue that DINOv2~\citep{oquab2023dinov2}, through self-supervised training with a feature similarity objective on vision-centric tasks, captures subtle frame differences and low-level visual features more effectively than vision-language contrastive methods~\citep{radford2021learning,zhai2023sigmoid}, as shown in Figure~\ref{fig:dinosim}. Hence, \textbf{(1)} we apply a temporal reduction strategy on the frame sequence by leveraging similarity from DINOv2~\citep{oquab2023dinov2} features to remove redundant video frames. In addition, \textbf{(2)} we jointly capture the detailed spatial semantic and long-range temporal context by performing selective feature reduction via cross-modal query, where we preserve full tokens for frames that are relevant to the given text query, while applying spatial pooling to reduce the remaining frames to a low-resolution token representation. \textbf{(3)} A spatial token reduction mechanism based on temporal dependencies is applied for excessively long videos. As a result, our model is capable of processing 1fps sampled video input with high performance, which can adaptively reduce the number of tokens per frame to 2 on average to accommodate an hour-long video for MLLM within 8k context length.

To evaluate our method, we conduct extensive experiments across various video understanding benchmarks, including EgoSchema~\citep{mangalam2024egoschema}, MVBench~\citep{li2024mvbench}, VideoMME~\citep{fu2024video}, and MLVU~\citep{zhou2024mlvu}. Our \modelname~significantly outperformes several recent open-source video LLM models, such as VideoChat2~\citep{li2024mvbench}, LongVA~\citep{zhang2024long}, and LLaVA-OneVision~\citep{li2024llava}, by a large margin. For example, our \modelname~outperforms a strong open-source baseline, LLaVA-OneVision~\citep{li2024llava} by approximately $\sim$5\% in average accuracy. We also observed that our light-weight \modelname, basing Llama3.2-3B~\citep{llama32} as the language backbone, significantly improves over previous state-of-the-art small video-LLMs, e.g., Phi-3.5-vision-instruct-4B~\citep{abdin2024phi}, by 3.4\% on VideoMME Long subset. Our \modelname~established new state-of-the-art results on video understanding benchmarks among video-language models. We believe that our proposed approach marks a meaningful progression towards long video understanding MLLMs. 

\subsection{Vision Language Models} 

Early visual language models (VLMs) such as CLIP~\citep{radford2021learning}, is trained with a contrastive loss to project both vision and language embeddings to a shared representation space. SigLIP~\citep{zhai2023sigmoid} takes a sigmoid loss instead, allowing further scaling up training batch size with better performance.

The development of LLMs has significantly advanced VLMs. Kosmos-1~\citep{huang2023language,peng2023kosmos} introduces an end-to-end framework that integrates visual inputs with LLM in a cohesive training regime. Flamingo~\citep{alayrac2022flamingo} and BLIP-2~\citep{li2023blip} merge visual and linguistic features through cross-attention and a Q-Former module, respectively. MiniGPT-4~\citep{zhu2023minigpt} and LLaVA~\citep{liu2024visual} simplify the integration by projecting visual features directly into the LLM embedding space using a MLP.

Later studies~\citep{chen2023shikra,peng2023kosmos,wang2023cogvlm,chen2023minigpt} have expanded LMM applications to broader multi-modal tasks, enhancing spatial perception through visual grounding. Recent efforts~\citep{liu2024llavanext,dong2024internlm} aim to create general models that unify diverse tasks, employing sophisticated optimization techniques, high-quality multi-task datasets, and complex training strategies to boost performance across extensive vision-language tasks. Cambrian~\citep{tong2024cambrian} combines features from multiple vision encoders with Spatial Vision Aggregator (SVA) for a more capable MLLM. By exploring different vision encoders, Cambrian~\citep{tong2024cambrian} finds that  SigLIP~\citep{zhai2023sigmoid} is a strong language-supervised model and  DINOv2~\citep{oquab2023dinov2} performs well on  vision-centric tasks.

\subsection{Video Large Language Models}

Recent advancements in MLLMs have broadened their application to video understanding tasks. Video LMMs process videos by extracting and encoding frames, then rearranging these as final video features. Several works~\citep{li2023videochat,li2024mvbench,cheng2024videollama}, use the Q-Former module from BLIP-2 to merge visual and text features, while others~\citep{lin2023video,luo2023valley,ataallah2024minigpt4} concatenate frame features directly. 

When processing lengthy videos, the constraint on context length inevitably causes a trade-off between the number of tokens per frame and the number of frames to input. Most existing works~\citep{li2023videochat,ataallah2024minigpt4,cheng2024videollama,zhang2024llavanextvideo,li2024llava} address this challenge by uniformly sampling frames from the video, which, however, results in a significant loss of visual details within the video. Video-ChatGPT~\citep{maaz2023videochatgpt} employs pooling modules to reduce data dimensions, enhancing processing efficiency. Other works try to preserve the maximum number of frames in video content. LLaMA-VID~\citep{li2023llama} employs an additional text decoder to embed the text query for cross-attention between frame features and compress the context token to one token per frame, while MovieChat~\citep{song2023moviechat} and TimeChat~\citep{ren2023timechat} develop memory modules and timestamp-aware encoders to capture detailed video content. Golfish~\citep{ataallah2024goldfish} segments long videos into shorter clips, processes each segment independently, and retrieves the most relevant segment in response to user queries. Our work focuses on maximizing the preservation of frames in video content (1fps) within given context length by proposing spatiotemporal compression of video tokens.

\subsection{Video Token Compression}

Recent methods has explored dynamic image tokens~\citep{ma2023image,xu2022groupvit,bolya2022token} or video tokens~\citep{lee2024multi,ren2023testa,choi2024vid} within the Transformer~\citep{vaswani2017attention} framework. Chat-UniVi~\citep{jin2023chatunivi} extends the dynamic tokens for visual features in MLLMs by merging K-nearest neighbor tokens across frame features of the video input. SlowFast-LLaVA~\citep{xu2024slowfast} uniformly samples 8 frames for high-resolution tokens, while performing spatial pooling to decrease the number of tokens in frames sampled at a higher frame rate. In our work, we propose a spatiotemporal adaptive token reduction strategy that leverages both cross-modal query and inter-frame dependencies. This approach effectively mitigates temporal redundancy in video content, thereby enabling the accommodation of long videos within given context length.\section{Method}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{assets/main.pdf}
    \caption{Architecture of \modelname. Given a densely sampled video frames, we first utilize DINOv2~\citep{oquab2023dinov2} prior to remove redundant frames, and fuse the remaining frame features from both SigLIP~\citep{zhai2023sigmoid} and DINOv2~\citep{oquab2023dinov2}, described in Section~\ref{sec:dino}. Then we selectively reduce visual tokens via cross-modal query, detailed in Section~\ref{sec:query}. Finally, as demonstrated in Section~\ref{sec:prune}, we conduct spatial token compression based on temporal dependencies to further meet the context length of LLMs.}
    \label{fig:main}
\end{figure}

We propose spatiotemporal adaptive compression in three steps to effectively process long video, as shown in Figure~\ref{fig:main}. Initially, we implement a temporal reduction strategy on the frame sequence by leveraging the prior knowledge from DINOv2~\citep{oquab2023dinov2} (Section~\ref{sec:dino}). Then, we selectively preserve full tokens for key frames via cross-modal query, while applying spatial pooling to reduce the remaining frames into low-resolution token representations (Section~\ref{sec:query}). Furthermore, we implement a spatial token reduction mechanism based on inter-frame temporal dependencies (Section~\ref{sec:prune}).

\subsection{Frame Feature Extractor and Temporal Reduction} 
\label{sec:dino}

 DINOv2~\citep{oquab2023dinov2}, through its self-supervised (SSL) training with a feature similarity objective on vision-centric tasks, can effectively capture subtle frame differences and low-level visual features. In contrast, CLIP-based~\citep{zhai2023sigmoid,radford2021learning} models are trained with vision-language contrastive loss in the semantic space, excelling at language alignment while sacrificing low-level features as shown in Figure~\ref{fig:dinosim}. Moreover, Cambrian~\citep{tong2024cambrian} discovered that combining features from both SigLIP~\citep{zhai2023sigmoid} and DINOv2~\citep{oquab2023dinov2} leads to a significant performance boost in vision-centric tasks. Therefore, we pioneer to leverage both SSL-based model DINOv2~\citep{oquab2023dinov2} with vision-language contrastive-based model SigLIP~\citep{zhai2023sigmoid} as frame feature extractors for MLLM in video understanding task.
 
 Note that processing the entire long video can be computationally expensive. Given a 1fps-sampled video with $N$ frames, denoted as $I=\{I^1,...,I^N\}$, we first use DINOv2~\citep{oquab2023dinov2} to extract features from each frame, leading to a set of DINO features $\{V_{\text{dino}}^1, \ldots, V_{\text{dino}}^N\}$. We then calculate the average similarity $\text{sim}^i = \frac{1}{J-1} \sum_{j=1, j \neq i}^J \text{sim}(V_{\text{dino}}^i, V_{\text{dino}}^j)$ within each non-overlapping window with $J=8$ frames and reduce frames that exhibit high similarity with other frames. This step significantly reduces video redundancy by temporally compressing the original $N$ frames to $T$ frames, which reduces approximately half of the video frames, as detailed in Section~\ref{sec:compress}. 
 
 We then extract features of the remaining $T$ frames using SigLIP~\citep{zhai2023sigmoid} vision encoder, resulting in $T$ features $\{V_{sig}^1,...,V_{sig}^T\}$. Subsequently, following Cambrian~\citep{tong2024cambrian}, we combine these two types of visual features via Spatial Vision Aggregator (SVA)~\citep{tong2024cambrian} that employs learnable queries to spatially aggregate visual features from multiple vision encoders. We denote the fused frames features as $V=\{V^1,...,V^T\}$.

\subsection{Selective Feature Reduction via Cross-modal Query}
\label{sec:query}

After temporal reduction, we obtain a set of fused frame features from both vision encoders, $V=\{V^1,...,V^T\} \in \mathbb{R}^{T\times (H_{h}\times W_{h}) \times D_v}$, where $H_{h}\times W_{h}$ denotes the spatial dimension of the frame features, and $D_v$ indicates the channel dimension of the frame feature after SVA. If the concatenated frame features exceed the given context length, i.e., $T\times H_h\times W_h \ge L_{max}$, we develop a selective compression strategy for certain frames, in order to capture both the detailed spatial semantic and long-range temporal context.

To achieve this, we propose using text query to help reduce spatial tokens of certain frames from $H_{h}\times W_{h}$ to $H_{l}\times W_{l}$. Given the LLM embedding of the text query $Q \in \mathbb{R}^{L_q\times D_q}$, where $L_q$ is the length of text query and $D_q$ is the dimensionality of LLM's embedding space, we strategically choose $N_h$ frames to preserve their original token resolution, while the remaining undergoes a process of spatial pooling to achieve a reduced resolution. The selection mechanism is based on the cross-modal attention scores between each frame feature and the text query. The number of frames to keep original resolution can be formulated as, 

\begin{equation}\label{eq:query}
    \mathbf{Top}_{N_h} \left (\frac{1}{H_hW_hL_q} \sum_{h,w,l}\mathcal{F}(V)Q^T \right ), \quad N_h = \max\left (0, \frac{L_{\text{max}} - L_q - T H_l W_l}{H_hW_h - H_lW_l}\right ),
\end{equation}
where $L_{max}$ is the given context length, $\mathcal{F}(\cdot)$ denotes a multi-layer perceptron (MLP)-based multimodal adapter designed to align visual features with the input space of the LLM. Note that we omit the system prompt in the instruction template for Equation~\ref{eq:query} simplification. If $N_h=0$, indicating that no frames are selected for retention at their original resolution, we will skip the computation of attention scores and will directly perform spatial pooling across all the frames to the lower resolution.

\subsection{Spatial Token Compression}\label{sec:prune}

As previously discussed, there are cases where the concatenated visual features with low resolution tokens still exceeds the given context length, i.e., $T\times H_l\times W_l \geq L_{max}$. Under these circumstances, further token compression is necessary. We partition the sequence of frame features into non-overlapping segments with a sliding window of size $K<T$, within which we conduct spatial token compression (STC). The first frame in each window retains its full token resolution. We then compute the cosine similarity between the first frame and subsequent frames within the window, conducting an element-wise comparison of spatial tokens between the first frame and its successors. Spatial tokens that exhibit a cosine similarity $\text{sim}(\cdot,\cdot)$ greater than the threshold $\theta$ with the corresponding tokens of the first frame at the same spatial location will be pruned, which can be formulated as, 
\begin{equation}\label{eq:prune} v_i^* \leftarrow \begin{cases} v_i(h, w) & \text{sim}(v_1(h, w), v_i(h, w)) \leq \theta \\ \emptyset & \text{otherwise} \end{cases}, \quad \forall h \in [1, H_l], w \in [1, W_l], i \in [2, K] \end{equation}

Given that videos often contain significant pixel-level redundancy, particularly in static background, this method allows spatial tokens reduction via temporal dependencies.
We chose the first frame in each sliding window for comparison, assuming DINOv2~\citep{oquab2023dinov2} has effectively reduced video redundancy across frames, making each frame less similar. We also tested alternative strategies, like using the middle frame or adaptively selecting based on frame changes (Section~\ref{sec:ablation}), but these provided similar performance and compression rates. Therefore, we chose the first-frame strategy in each sliding window for its simplicity and effectiveness.
\section{Experiments}

\subsection{Datasets}

We adopt two stages of training in our experiments: image-language pre-training and video-language finetuning. For the image-language pre-training stage, previous methods~\citep{chen2023shikra,peng2023kosmos,wang2023cogvlm,chen2023minigpt,liu2024llavanext,dong2024internlm} usually use two steps for alignment and finetuning. For simplicity, we combine these two steps in one stage using Single-Image data from LLaVA-OneVision~\citep{li2024llava}. For video-language finetuning, we utilize a large-scale video-text pairs sourced from several publicly accessible databases. The video training data contains a subset of VideoChat2-IT~\citep{li2024mvbench}, which includes TextVR~\citep{wu2025large}, Youcook2~\citep{zhou2018towards}, Kinetics-710~\citep{kay2017kinetics}, NExTQA~\citep{xiao2021next}, CLEVRER~\citep{yi2019clevrer}, EgoQA~\citep{fan2019egovqa}, TGIF~\citep{li2016tgif}, WebVidQA~\citep{yang2021just}, ShareGPT4Video~\citep{chen2024sharegpt4video}, and MovieChat~\citep{song2024moviechat} as the long video complementary. All the training datasets are listed in Table~\ref{tab:traindata}.

\subsection{Benchmarks and metrics}
We evaluate our model on EgoSchema~\citep{mangalam2024egoschema}, MVBench~\citep{li2024mvbench}, VideoMME~\citep{fu2024video} and MLVU~\citep{zhou2024mlvu}. VideoMME~\citep{fu2024video} (1 min $\sim$ 1 hour) and MLVU~\citep{zhou2024mlvu} (3 mins $\sim$ 2 hours) are long video benchmarks for assessing long video understanding ability. For VideoMME~\citep{fu2024video}, videos are officially split based on duration, which  contains a subset of long videos ranging from 30 minutes to 1 hour. We perform standardized evaluations using greedy decoding (\textit{num\_beams}=1) and benchmark our results against other open-source and proprietary models.

\begin{table}[!htbp]
    \centering
\begin{adjustbox}{width=\linewidth,center}
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{1.5mm}
\begin{tabular}{lcccccccccc}
\toprule  \multirow{2}{*}{\textbf{Models}} & \multirow{2}{*}{\textbf{Size}} & \multirow{2}{*}{\textbf{Context Length}} & \multirow{2}{*}{\textbf{\#Frames}} & \multirow{2}{*}{ \textbf{EgoSchema} }  & \multirow{2}{*}{\textbf{MVBench}} & \multirow{2}{*}{\textbf{MLVU}} & \multicolumn{2}{p{3.2cm}}{\centering \textbf{VideoMME} } \\ \cline{8-9}
&&&&&&& Overall & Long \\
\rowcolor{gray!10} Duration & & & & 179.8 sec & 16 sec & 3$\sim$120 min & 1$\sim$60 min & 30$\sim$60 min \\
\midrule
\textit{Proprietary Models} \\
GPT4-V~\citep{openai2023gpt4v} & - & - & 1fps & 55.6 & 43.7  & - & 60.7 & 56.9 \\
GPT4-o~\citep{openai2024gpt4o} & - & - & 1fps & 72.2 & 64.6  & 66.2 & 77.2 & 72.1 \\
\midrule
\textit{Open-Source Video MLLMs} \\
Video-LLaVA~\citep{lin2023video} & 7B & 4k & 8 & 38.4 & 41.0  & 47.3 & 40.4 & 38.1 \\
LLaMA-VID~\citep{li2023llama} & 7B & 4k & 1fps & 38.5 & 41.9  & 33.2 & - & - \\
Chat-UniVi~\citep{jin2023chatunivi} & 7B & 4k & 64 & - & -  & - & 45.9 & 41.8 \\
ShareGPT4Video~\citep{chen2024sharegpt4video} & 8B & 8k & 16 & - & 51.2  & 46.4 & 43.6 & 37.9 \\
LLaVA-NeXT-Video~\citep{zhang2024llavanextvideo} & 7B & 8k & 32 & 43.9  & 33.7 & - & 46.5 & - \\
VideoLLaMA2~\citep{cheng2024videollama} & 7B & 8k & 32 & 51.7 & 54.6  & 48.5 & 46.6 & 43.8 \\
LongVA~\citep{zhang2024long} & 7B & 224k & 128 & - & -  & 56.3 & 54.3 & 47.6 \\
VideoChat2~\citep{li2024mvbench} & 7B & 8k & 16 & 54.4 & 60.4  & 47.9 & 54.6 & 39.2 \\
LLaVA-OneVision~\citep{li2024llava} & 7B & 8k & 32 & 60.1 & 56.7  & 64.7 & 58.2 & 46.7 \\
\rowcolor{blue!10} \modelname~(Ours) & 7B & 8k & 1fps & \textbf{67.6} & \textbf{66.9}  & \textbf{65.4} & \textbf{60.6} & \textbf{59.5} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Results on comprehensive video understanding benchmarks}
\label{tab:main}
\end{table}

\subsection{Implementation Details}

We use SigLIP~\citep{zhai2023sigmoid} (so400m-patch14-384) and DINOv2~\citep{oquab2023dinov2} as the vision encoder while choose Qwen2-7B~\citep{qwen2} and Llama3.2-3B~\citep{llama32} as our language foundation model. We only compute cross-entropy loss for autoregressive text generation. We use AdamW~\citep{loshchilov2017decoupled} optimizer with a cosine schedule for all the trainings. In the image-language pre-training stage, we train the model for one epoch with global batch size of 128. The learning rate is set to 1e-5, and the warmup rate is 0.03. The number of tokens per image are set to 576. For the video-language finetuning stage, we train the model for one epoch with global batch size of 64. The learning rate is set to 1e-5, and the warmup rate is 0.03. The maximum number of tokens per frame are set to 144 ($H_h=W_h=12$), while each might be reduced by our proposed adaptive compression approach ($\leq 64, \, H_l=W_l=8$). The STC reduction threshold $\theta=0.8$ and the sliding window size $K = 8$. Our model is trained on 64 NVIDIA H100 GPUs.

\subsection{Video Understanding}

\noindent\textbf{Quantitative Results.} Table~\ref{tab:main} presents our experimental results on multiple video understanding benchmarks. Our results compares favorably to all the baselines across various video understanding benchmarks. For example, on VideoMME~\citep{fu2024video}, our LongVU outperforms VideoChat2~\citep{li2024mvbench}, LLaVA-OneVision~\citep{li2024llava} by 6.0\% and 2.4\% respectively. Notably, on VideoMME Long subset~\citep{fu2024video}, our model surpasses LLaVA-OneVision~\citep{li2024llava} by 12.8\%. These results indicate the strong video understanding capabilities of our model. Note that our model achieves significant improved performance with a much smaller training dataset, comparing to LLaVA-OneVision~\citep{li2024llava} trained on OneVision-1.6M (multi-image, video) that has not yet been made publicly available\footnote{LLaVA-OneVision~\citep{li2024llava} only release single-image set at the time of current submission. \href{https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-Data/discussions/6}{https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-Data/discussions/6}}. With the same video training dataset from VideoChat2-IT~\citep{li2024mvbench}, our LongVU shows much higher performance than VideoChat2~\citep{li2024mvbench}, $\sim$10\% accuracy improvement in average. 
Interestingly, we also find that our model can even beat proprietary model GPT4-o~\citep{openai2024gpt4o} on MVBench~\citep{li2024mvbench} with densely sampled video input and reduce the accuracy gap comparing to proprietary models on other video benchmarks. 

We also scale our LongVU with a lightweight LLM, Llama3.2-3B~\citep{llama32}, to further demonstrate the strong video understanding capabilities. We observe the consistent improvement of our light-weight LongVU over baselines in Table~\ref{tab:mainsmall}. Our method outperforms Phi-3.5-vision-instruct~\citep{abdin2024phi} on VideoMME (Long) by margin of 3.4\% accuracy. This set of experiments validate the effectiveness of our method even scaling to a smaller size. 

\begin{table}[!htbp]
    \centering
\begin{adjustbox}{width=\linewidth,center}
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{1.5mm}
\begin{tabular}{lccccccccc}
\toprule  \multirow{2}{*}{\textbf{Models}} & \multirow{2}{*}{ \textbf{EgoSchema} }  & \multirow{2}{*}{\textbf{MVBench}} & \multicolumn{2}{p{2.5cm}}{\centering \textbf{VideoMME} } & \multirow{2}{*}{\textbf{MLVU}} \\ \cline{4-5}
& & & Overall & Long & & \\
\midrule
InternVL2 (InternLM2-1.8B)~\citep{internvl2} & - & 60.2 & 47.3 & 42.6 & -\\
VideoChat2 (Phi-3-mini-4B)~\citep{li2024mvbench} & 56.7 & 55.1 & - & - & - & \\
Phi-3.5-vision-instruct (Phi-3-mini-4B)~\citep{abdin2024phi} & - & - & 50.8 & 43.8 & - \\ 
\rowcolor{blue!10} \modelname~(Ours) (Llama3.2-3B) & \textbf{59.1} & \textbf{60.9} & \textbf{51.5} & \textbf{47.2} & 55.9 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Results of small-size video language models across video understanding benchmarks.}
\label{tab:mainsmall}
\end{table}

\noindent\textbf{Qualitative Results.} We now provide the qualitative results in Figure~\ref{fig:qual}. Specifically, we demonstrate various video understanding abilities in the examples, such as accurately recognizing the orientation of moving objects in Figure~\ref{fig:qual}(a), providing detailed video descriptions in Figure~\ref{fig:qual}(b), identifying inserted needle frames and conducting action counting in Figure~\ref{fig:qual}(c), and responding precisely to questions about specific frames in an hour-long video in Figure~\ref{fig:qual}(d). These results demonstrate that our model has competing video-language understanding capabilities. 

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{assets/qual.pdf}
    \caption{Examples for various video understanding capabilities of LongVU model. We showcase that our LongVU is able to complete different types of video understanding tasks.}
    \label{fig:qual}
\end{figure}

\subsection{Ablation Studies}
\label{sec:ablation}

\noindent\textbf{Effects of the number of tokens per frame.} We ablate the number of tokens in our uniform-sampling baselines. There is a trade-off between the number of tokens per frame and the sampling frequency of frames. Table~\ref{tab:ablation} shows the experimental results when using different number of tokens with different sampling. When applying uniforming sampling, 144 tokens per frame shows better performance than 64 tokens in an 8k context length on VideoMME~\citep{fu2024video} and MLVU~\cite{zhou2024mlvu} while worse on EgoSchema~\cite{mangalam2024egoschema}. With 144 tokens per frame, it preserves more visual details, but restricts the total number of frames, i.e., less than 60 frames within 8k context length. This demonstrate that adaptive tokens are needed for better performance across different video benchmarks. 

\noindent\textbf{DINOv2 vs SigLIP.}  Our results in Table~\ref{tab:ablation} verify that DINOv2~\citep{oquab2023dinov2} features are more effective than SigLIP~\citep{zhai2023sigmoid} features. As expected, we also find that using DINO-based features for temporal frame reduction outperforms uniform sampling. Therefore, DINOv2~\citep{oquab2023dinov2} is an useful vision-centric feature extractor to help perform  temporal reduction. 

\noindent\textbf{Query guided selection.} We apply text-guided frame selection after temporal reduction, where relevant frames are maintained at full token capacity (144 tokens), while others are reduced to 64 tokens. This helps preserve essential visual features and accommodates more long-range context within the context length. In Table~\ref{tab:ablation}, we observe the improvement with query guided frame selection across all benchmarks. Moreover, in Table~\ref{tab:mlvu}, the results of each subtask in  MLVU~\citep{zhou2024mlvu} show significant performance improvements when using cross-modal queries, particularly for frame-retrieval tasks such as counting and needle detection. 

\noindent\textbf{Spatial token compression.} We further apply spatial token compression after query guided selection. We find that spatial token compression (STC) not only enhances performance within 8k context length, but also achieve results comparable or slightly better than 16k context length in Table~\ref{tab:ablation}. We also note some improvements for most subtasks in MLVU~\citep{zhou2024mlvu}. 

\begin{table}[!htbp]
    \centering
\begin{adjustbox}{width=0.9\linewidth,center}
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{1.5mm}
\begin{tabular}{lccccc}
\toprule \textbf{Methods} & \textbf{Context Length} & \textbf{\#Tokens} & \multicolumn{1}{c}{ \textbf{EgoSchema} } & \multicolumn{1}{c}{ \textbf{VideoMME} } & \multicolumn{1}{c}{\textbf{MLVU}} \\
\midrule
\rowcolor{gray!10} Uniform & 16k & 144 & 67.12	& 60.01 & 64.70 \\
\rowcolor{gray!10} DINO & 16k & 144 & 67.34 & 61.25 & 64.83 \\
\midrule
Uniform & 8k & 64 & 66.84 & 57.56 & 60.87 \\
Uniform & 8k & 144 & 66.28 & 58.84 & 63.28 \\
\midrule
SigLIP & 8k & 64 & 66.04 & 58.63	& 62.17 \\
DINO & 8k & 64 & 66.20 & 59.90 & 62.54 \\
DINO + Query & 8k & 64/144 & 67.30 & 60.08 & 65.05 \\
\rowcolor{blue!10} DINO + Query + STC (default) & 8k & dynamic & \textbf{67.62} & \textbf{60.56} & \textbf{65.44} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Ablation studies of number of tokens per frame, different context lengths, and our spatiotemporal compression components.}
\label{tab:ablation}
\end{table}

\begin{table}[!htbp]
    \centering
\begin{adjustbox}{width=0.95\linewidth,center}
\begin{tabular}{lccccccccc}
\toprule \textbf{Stratgy} & \multicolumn{1}{c}{ \textbf{count} } & \multicolumn{1}{c}{ \textbf{ego} } &  \multicolumn{1}{c}{\textbf{needle}} & \multicolumn{1}{c}{ \textbf{order} } & \multicolumn{1}{c}{\textbf{plotQA}} & \multicolumn{1}{c}{\textbf{anomaly}} & \multicolumn{1}{c}{\textbf{reasoning}} & \multicolumn{1}{c}{\textbf{Avg}} \\
\midrule
DINO & 24.15 & 59.09 & 68.16 & 52.89 & 71.24 & 74.00 & 86.36 & 62.54 \\
DINO+Query & 28.98 & 55.39 & \textbf{78.87} & 56.37 & \textbf{72.35} & 75.50 & \textbf{87.87} & 65.05 \\
\rowcolor{blue!10} DINO+Query+STC (default) & \textbf{28.98} & \textbf{59.37} & 76.33 & \textbf{58.30}	& 71.61 & \textbf{76.00} & 87.50 & \textbf{65.44} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Ablation study on each subtask in MLVU~\citep{zhou2024mlvu}.}
\label{tab:mlvu}
\end{table}

\noindent\textbf{Different strategies for spatial token compression.} We now ablate different strategies of our spatial token compression mechanism. This analysis explores different strategies for determining anchor frames: the first/middle one in each sliding window, or the frame that exhibits significant changes compared to its adjacent frames. In Table~\ref{tab:stcab}, our results indicate that taking the first frame in each sliding window gives a slightly better performance with similar reduction rates across all strategies.

\begin{table}[!ht]
    \centering
    \renewcommand{\arraystretch}{1.3}
    \begin{adjustbox}{width=0.9\linewidth,center}
    \begin{tabular}{lcccccc}
    \toprule
        \textbf{Model}  & \textbf{Short} & \textbf{Medium} & \textbf{Long} & \textbf{Overall} & \textbf{Reduction rate} \\
        \midrule
        \rowcolor{blue!10} $1^{st}$ frame in sliding window (default) & 64.7	& 58.2 & 59.5 & 60.9 & 55.47\% \\
        $(K/2)^{th}$ frame in sliding window & 64.7 & 58.7	& 58.6 & 60.7 & 54.97\% \\ 
        frame with high changes & 64.7 & 58.2 & 58.3 & 60.4 & 55.62\% \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{Different strategies for spatial token compression on VideoMME~\citep{fu2024video}.}
    \label{tab:stcab}
\end{table}

\subsection{Spatiotemporal Compression Analysis}
\label{sec:compress}

\noindent\textbf{Compression analysis.} We sampled hundreds of videos to demonstrate the distribution of frame/token reduction rate. Figure~\ref{fig:reduce} (a) presents the number of frames before and after temporal reduction based on the similarity of DINOv2 features across frames. We find that $\sim$45.9\% of the frames are maintained after temporal reduction on average. Figure~\ref{fig:reduce} (b) shows the number of tokens before and after spatial token compression (Section~\ref{sec:prune}). We observe that $\sim$40.4\% tokens are reduced on average. These results demonstrate the effective video token compression with temporal and spatial token reduction. 

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{assets/reduce.pdf}
    \caption{We randomly sample hundreds of videos to demonstrate the frames/tokens level reduction rate. (a) The number of frames before/after temporal reduction based on DINOv2 features (Section~\ref{sec:dino}). (b) The number of tokens before/after spatial token compression (Section~\ref{sec:prune}).}
    \label{fig:reduce}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{assets/needle.pdf}
    \caption{Needle-in-a-Haystack results. Our adaptive token compression scheme improves the score for locating the needle frame within an hour-long video from 0.80 to 0.88 on average.}
    \label{fig:needle}
\end{figure}

\noindent\textbf{Long context analysis.} Recently, the Needle-in-a-Haystack task~\citep{hsieh2024ruler,kamradt2023needle} has been used to assess the ability of Large Language Models (LLMs) to retrieve long context information. We follow~\citep{zhang2024long} to conduct a video needle-in-a-haystack experiment to demonstrate the effectiveness of our compression strategy on identifying the needle frame within an hour-long video.

To facilitate this evaluation, we randomly select an one-hour-long test video from MLVU~\citep{zhou2024mlvu}. We then insert each image from a set of VQA problems as a needle frame into this long video for creating a challenging search task. We sample the video at 1 FPS and control the frame length ranging from 200 to 3.6k frames. We also vary the needle frame insertion depth from 0\% to 100\% of the total input frames. We conduct experiments with 8k context length and compare our adaptive token compression to the one without applying query-guided selection (w/o Query) and spatial token compression (w/o STC) after temporal reduction. Figure ~\ref{fig:needle} demonstrates that our adaptive compression mechanism could accurately resolve the needle VQA problem of 1k frames within 8k context length and improve score with more frames. This demonstrates the advantage of our method for long context video understanding. 

\section{Conclusion}

We introduced LongVU, a MLLM that can address the significant challenge of long video understanding within a commonly used context length. To achieve this, we proposed a spatiotemporal adaptive compression scheme of LongVU for helping reduce video tokens without losing much visual details of long videos by leveraging cross-modal query and inter-frame similarities. Experiments on various video understanding benchmarks consistently validate the advantages of our model. We also demonstrate that our method helps build a quality light-weight video language understanding model based on Llama3.2-3B, which suggests that LongVU has many potential applications in the vision-language community. 

\clearpage
\newpage
\beginappendix

\section{Training Datasets}

For the image-language training stage, previous methods~\citep{chen2023shikra,peng2023kosmos,wang2023cogvlm,chen2023minigpt,liu2024llavanext,dong2024internlm} usually use two stages of alignment and finetuning. For simplicity, we combine and alignment in one stage using single image version of LLaVA-OneVision~\citep{li2024llava} data. For video-language training, we utilize a large-scale video-text pairs sourced from several publicly accessible databases. The video training data is a subset of VideoChat2-IT~\citep{li2024mvbench}, which includes TextVR~\citep{wu2025large}, Youcook2~\citep{zhou2018towards}, Kinetics-710~\citep{kay2017kinetics}, NExTQA~\citep{xiao2021next}, CLEVRER~\citep{yi2019clevrer}, EgoQA~\citep{fan2019egovqa}, TGIF~\citep{li2016tgif}, WebVidQA~\citep{yang2021just}, ShareGPT4Video~\citep{chen2024sharegpt4video}, in addition to above, we use MovieChat~\citep{song2024moviechat} as long video complementary. All the training data is demonstrated in Table~\ref{tab:traindata}.

\begin{table}[ht]
\centering                         
\renewcommand{\arraystretch}{1.4}  
\setlength{\tabcolsep}{1.5mm}      
\footnotesize                      
\begin{tabular}{c|c|cc}
\toprule
\textbf{Modality} & \multicolumn{1}{c|}{\textbf{Task}} & \textbf{\# Samples} & \textbf{Dataset} \\ 
\hline
 Image-Text & \cellcolor{gray!10} Single-Image     & \cellcolor{gray!10} 3.2M & \cellcolor{gray!10} LLaVA-OneVision \\
\hline
 & Captioning     & 43K  & TextVR, MovieChat, YouCook2 \\
 & \cellcolor{gray!10} Classification & 1K \cellcolor{gray!10}  & \cellcolor{gray!10} Kinetics-710 \\
 & VQA             & 424K  & \begin{tabular}[c]{@{}l@{}}NExTQA, CLEVRER, EgoQA, \\ TGIF, WebVidQA, DiDeMo \end{tabular} \\
\multirow{-4}{*}{Video-Text} & \cellcolor{gray!10} Instruction & \cellcolor{gray!10}  85K & \cellcolor{gray!10}  \begin{tabular}[c]{@{}l@{}} ShareGPT4Video \end{tabular} \\ 
\bottomrule
\end{tabular}
\vspace{3mm}
\caption{Training data statistics.}
\label{tab:traindata}
\end{table}

\begin{table}[ht]
    \centering
    \renewcommand{\arraystretch}{1.3}
    \begin{tabular}{ccccccc}
    \toprule
        \textbf{Model} & \textbf{Size} & \textbf{Frames} & \textbf{Short} & \textbf{Medium} & \textbf{Long} & \textbf{Overall} \\
        \midrule
        Video-LLaVA~\citep{lin2023video} & 7B & 8 & 46.1 & 40.7 & 38.1 & 41.6 \\
        ShareGPT4Video~\citep{chen2024sharegpt4video} & 8B & 16 &  53.6 & 39.3 & 37.9  & 43.6 \\
        Chat-Univi-v1.5~\citep{jin2023chatunivi} & 7B & 64 & 51.2 &  44.6  & 41.8  & 45.9 \\
        VideoLLaMA2~\citep{cheng2024videollama} & 7B & 16 & 59.4 & 47.6  & 43.8 & 50.3 \\
        VideoChat2~\citep{li2024mvbench} & 7B & 16  & 52.8 & 39.4  & 39.2 & 43.8 \\ 
        LongVA~\citep{zhang2024long} & 7B & 128 & 61.6 & 50.4  & 47.6 & 54.3 \\
        LLaVA-OneVision~\citep{li2024llava} & 7B & 32 & \textbf{69.1} & 53.3 & 46.7 & 58.2 \\
        \rowcolor{blue!10} \modelname~(Ours) & 7B & 1fps & 64.7	& \textbf{58.2} & \textbf{59.5} & \textbf{60.9} \\
        \bottomrule
    \end{tabular}
    \caption{Comparison with other video LMMs on VideoMME~\citep{fu2024video} benchmark.}
    \label{tab:videomme}
\end{table}

\section{Frame-level Position Encoding}
\label{sec:pos} 

To alleviate potential confusion arising from frame-by-frame feature concatenation, we incorporate a frame-level position encoding to enforce the temporal boundaries across frames and capture inter-dependencies within each frame. Given that we temporally reduce several frames, a straightforward concatenation of all frames renders the model unaware of the relative timestep across frames. Furthermore, our dynamic token sampling strategy does not delineate clear boundaries between each frame. To address this, we incorporate frame-level positional embeddings (FPE) that correspond to the absolute timestep of each frame, utilizing a shared sinusoidal position encoding~\citep{vaswani2017attention} for frames at time $t$, shown in Equation~\ref{eq:pos}.

\begin{equation}\label{eq:pos}
    PE(t,2i) = sin(t/10000^{2i/d}), 
    PE(t,2i+1) = cos(t/10000^{2i/d}) 
\end{equation}

The ablation shows in Table~\ref{tab:fpe} and Table~\ref{tab:mlvufpe} that adding the FPE does not affect much to the overall performance across several benchmarks. Therefore, we decide not to include it in our default setting.

\begin{table}[!htbp]
    \centering
\begin{adjustbox}{width=0.9\linewidth,center}
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{1.5mm}
\begin{tabular}{lccccc}
\toprule \textbf{Methods} & \textbf{Context Length} & \textbf{\#Tokens} & \multicolumn{1}{c}{ \textbf{EgoSchema} } & \multicolumn{1}{c}{ \textbf{VideoMME} } & \multicolumn{1}{c}{\textbf{MLVU}} \\
\midrule
DINO + Query & 8k & 64/144 & 67.30 & 60.08 & 65.05 \\
\rowcolor{blue!10} DINO + Query + STC (default) & 8k & dynamic & 67.62 & 60.56 & 65.44 \\
DINO + Query + STC + FPE & 8k & dynamic & 67.87 & 60.89 & 64.56 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Ablation study on with or without FPE.}
\label{tab:fpe}
\end{table}

\begin{table}[!htbp]
    \centering
\begin{adjustbox}{width=0.95\linewidth,center}
\begin{tabular}{lccccccccc}
\toprule \textbf{Stratgy} & \multicolumn{1}{c}{ \textbf{count} } & \multicolumn{1}{c}{ \textbf{ego} } &  \multicolumn{1}{c}{\textbf{needle}} & \multicolumn{1}{c}{ \textbf{order} } & \multicolumn{1}{c}{\textbf{plotQA}} & \multicolumn{1}{c}{\textbf{anomaly}} & \multicolumn{1}{c}{\textbf{reasoning}} & \multicolumn{1}{c}{\textbf{Avg}} \\
\midrule
DINO & 24.15 & 59.09 & 68.16 & 52.89 & 71.24 & 74.0 & 86.36 & 62.54 \\
DINO+Query & 28.98 & 55.39 & 78.87 & 56.37 & 72.35	 & 75.5 & 87.87 & 65.05 \\
\rowcolor{blue!10} DINO+Query+STC (default) & 28.98 & 59.37 & 76.33 & 58.30	& 71.61 & 76.0 & 87.50 & 65.44 \\
DINO+Query+STC+ FPE & 29.46 & 60.79 & 74.08 & 52.12	& 71.79	& 74.5	& 86.74 & 64.56 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Strategy ablations on each subtask in MLVU~\citep{zhou2024mlvu}.}
\label{tab:mlvufpe}
\end{table}

\section{DINOv2 v.s. SigLIP}

DINOv2~\citep{oquab2023dinov2}, through self-supervised training with a feature similarity objective on visually-centric tasks, captures subtle frame differences and low-level visual features more effectively than vision-language contrastive methods~\citep{radford2021learning,zhai2023sigmoid}, as shown in Figure~\ref{fig:dinosim}.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{assets/dinosim.pdf}
    \caption{Similarity comparison between SigLIP~\citep{zhai2023sigmoid} and DINOv2~\citep{oquab2023dinov2} features. The similarity is calculated between the first frame and the remainings. DINO concentrating on vision centric task effectively capture subtle frame differences compared with SigLIP~\citep{zhai2023sigmoid} which is aligned on semantic space.}
    \label{fig:dinosim}
\end{figure}

\section{Needle-In-A-Video-Haystack}

We conducted experiments using an 8k context length to evaluate our default setting, which incorporates our adaptive compression, against configurations without spatial token compression (w/o STC) and without querying guided reduction  (w/o Query), as depicted in Figure~\ref{fig:needle1}. By integrating a cross-modal query to selectively retain full tokens of frames relevant to the text query, the model significantly enhances its ability to accurately identify key frames when the total number of video frames is fewer than 1.4k. Moreover, our adaptive token compression mechanism further boosts VQA accuracy with increased frames.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{assets/needle1.pdf}
    \caption{Needle-In-A-Video-Haystack results. Our spatiotemporal adaptive token compression scheme improves the score for locating the needle frame.}
    \label{fig:needle1}
\end{figure}

\begin{table}[!ht]
    \centering
    \renewcommand{\arraystretch}{1.3}
    \begin{tabular}{cccccc}
    \toprule
        \textbf{Model} & \textbf{SQA-IMG} & \textbf{MMVP} & \textbf{POPE} &  \textbf{RealWorldQA} \\
        \midrule
        Before video SFT & 95.44 & 51.33 & 86.65 & 61.06 \\
        After video SFT & 83.94 & 32.00 & 81.23 & 47.65 \\
        \bottomrule
    \end{tabular}
    \caption{We mainly focus on video understanding task and use video-only data for video SFT stage. We observe a decrease in performance on image understanding after video SFT stage.}
    \label{tab:image}
\end{table}

\section{Limitation}

Our research is primarily concentrated on video understanding tasks, for which we employ video-only data during the video supervised fine-tuning (SFT) stage. As evidenced in Table~\ref{tab:image}, there is a decrease observed in the model's image understanding capabilities after video SFT. A potential remedy could involve integrating a mix of image, multi-image, and video data during training. However, due to constraints in GPU resources, we leave it as a future work with larger datasets for stronger unified image and video models.

\end{document}