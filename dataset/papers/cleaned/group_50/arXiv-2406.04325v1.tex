\title{ShareGPT4Video: Improving Video Understanding and Generation with Better Captions}

\begin{document}

\maketitle

\begin{figure}[h]
    \centering
    \vspace{-30pt}
    \includegraphics[width=0.99\linewidth]{figures/files/teaser.pdf}
    \caption{\textbf{Details and attributes of the ShareGPT4Video.} (a) The proposed ShareGPT4Video dataset contains a large volume of high-quality video-caption pairs collected from diverse sources, with 40K captions from GPT4V and 4.8M captions from our ShareCaptioner-Video.
    (b) We illustrate in detail the process of harnessing the multi-modal image model GPT4V \cite{gpt4v} to generate high-quality captions for videos.  Please refer to Figure \ref{fig:caption_strategy_1} for the full caption of the example.
    (c) Our unique captioning strategy enables the re-caption of sub-clips by reusing their differential captions.} 
    \label{fig:teaser}
\end{figure}

\begin{abstract}
We present the ShareGPT4Video series, aiming to facilitate the video understanding of large video-language models (LVLMs) and the video generation of text-to-video models (T2VMs) via dense and precise captions.
The series comprises: \textbf{1) ShareGPT4Video}, 40K GPT4V annotated dense captions of videos with various lengths and sources, developed through carefully designed data filtering and annotating strategy.
\textbf{2) ShareCaptioner-Video}, an efficient and capable captioning model for arbitrary videos, with 4.8M high-quality aesthetic videos annotated by it.
\textbf{3) ShareGPT4Video-8B}, a simple yet superb LVLM that reached SOTA performance on three advancing video benchmarks.
To achieve this, taking aside the non-scalable costly human annotators, we find using GPT4V to caption video with a naive multi-frame or frame-concatenation input strategy leads to less detailed and sometimes temporal-confused results. We argue the challenge of designing a high-quality video captioning strategy lies in three aspects: 
\textbf{1) Inter-frame precise temporal change understanding.
2) Intra-frame detailed content description.
3) Frame-number scalability for arbitrary-length videos. }
To this end, we meticulously designed a differential video captioning strategy, which is stable, scalable, and efficient for generating captions for videos with arbitrary resolution, aspect ratios, and length.
Based on it, we construct ShareGPT4Video, which contains 40K high-quality videos spanning a wide range of categories, and the resulting captions encompass rich world knowledge, object attributes, camera movements, and crucially, detailed and precise temporal descriptions of events. 
Based on ShareGPT4Video, we further develop ShareCaptioner-Video, a superior captioner capable of efficiently generating high-quality captions for arbitrary videos. We annotated 4.8M aesthetically appealing videos by it and verified their effectiveness on a 10-second text2video generation task.
For video understanding, we verified the effectiveness of ShareGPT4Video on several current LVLM architectures and presented our superb new LVLM ShareGPT4Video-8B. All the models, strategies, and annotations\footnote{We do not hold the copyright for any video and will provide the link-annotation pair for research-only usage.} will be open-sourced and we hope this project can serve as a pivotal resource for advancing both the LVLMs and T2VMs community.

 
\end{abstract}

\section{Introduction}
\label{sec:intro}
Recent advancements in multi-modal learning, driven by large language models, have led to progress in image-text dialogue\cite{liu2023improved,chen2023sharegpt4v,dong2024internlm,wang2023cogvlm,ye2023mplug,chen2023internvl,bai2023qwenvl,young2024yi} and text-to-image generation tasks\cite{betker2023improving,rombach2022high,chang2023muse,ruiz2023dreambooth,zhang2023adding,chen2023pixart}. This has inspired a shift towards video understanding\cite{lin2023video,ataallah2024minigpt4,li2023llama,zhang2023video,maaz2023video,luo2023valley} and generation tasks\cite{singer2022make,hong2022cogvideo,wu2023tune,wang2023lavie,blattmann2023stable,liu2024sora}, allowing for user interaction across video and language modalities. 
Thus, the detailed and high-fidelity video captions, which bridge the aforementioned modalities, is instrumental in propelling the advancements within the field.

Despite the rich semantic and temporal content of videos, they are often paired with brief captions in existing data. These short descriptions limit the detailed video understanding and the controllability of the video generation. While the importance of detailed captions is recognized in image-text dialogue\cite{chen2023sharegpt4v,chen2024allava,wang2023see} and text-to-image generation tasks\cite{chen2023pixart,dalle3}, similar efforts are lacking in video understanding and generation.

However, creating large-scale, high-quality video captions is a challenging task. Detailed captioning for long videos is non-trivial and time-consuming even for humans, hindering large-scale annotation. Current open-source LVLMs lack this capability, and closed-source APIs do not yet support video inputs. 

On the other hand, if we roughly degrade the input from video to multiple frames, even GPT4V struggles to describe the video with satisfied quality. For example, an intuitive idea is to provide multiple frames with timestamps to the GPT4V and generate the caption, while we find that GPT4V is unstable and sometimes misunderstands the temporal relation between the frames, and its performance further degrades with the increasing of video frame number. Other solutions such as concatenating all the frames into a large image are nonhelpful to the temporal problem, and the caption loses details as the frame number increases. We also showcase these problems in Figure~\ref{fig:caption_strategy_2}-\ref{fig:caption_strategy_3}

We posit that the challenge of devising an effective video captioning strategy is rooted in three fundamental aspects:
\textit{1) Inter-frame precise temporal change understanding}: The temporal dimension distinguishes videos from images. An imprecise temporal description can significantly diminish the quality of the video caption and lead to confusion in the training models.
\textit{2) Intra-frame detailed content description}: Detailed descriptions~\cite{chen2023sharegpt4v} are crucial for aligning modalities between image and text, which are also important for video-text alignment.
\textit{3) Frame-number scalability for arbitrary-length videos}: Videos encountered in the wild can vary greatly in length. An ideal captioning strategy should be resilient to this variability and generate appropriate captions for videos of any length.

To this end, we present the \textbf{Differential Sliding-Window Captioning strategy} (DiffSW), which is \textit{stable, scalable, and efficient for generating captions for arbitrary videos.} The central concept of DiffSW is translating the all-frames-to-caption task into a differential description task. Specifically, we generate a detailed caption for the first frame and apply a sliding window of length two to the subsequent frames in chronological order. The powerful image multi-modal model, GPT4V \cite{gpt4v}, is tasked with identifying the changes between frames based on three inputs: the previous frame, its differential caption, and the current frame. This encompasses alterations in camera movement, object movement, character actions, and scene transitions. Upon acquiring all differential captions, these are input into GPT4 \cite{chatgpt} to construct a comprehensive caption for the entire video.
The differential concept allows DiffSW to concentrate on the changes between frames, i.e., the temporal changes. Its sliding design ensures the correctness of temporal order and invariance towards the total number of frames. The constant input frame number guarantees that GPT4V does not overlook details and utilizes the API efficiently, resulting in stable, scalable, and efficient caption quality from DiffSW. Furthermore, the differential design enables the re-caption of any sub-clips of a captioned video by reusing its differential captions.

Based on DiffSW, we construct \textbf{ShareGPT4Video}, which contains \textbf{40K high-quality video-caption pairs} spanning a wide range of categories, and the resulting captions encompass rich world knowledge, object attributes, camera movements, and crucially, detailed and precise temporal descriptions of events. 
The videos of ShareGPT4Video are collected from various sources \cite{chen2024panda,yu2020bdd100k,pexels,grauman2022ego4d,pixabay,mixkit}, employed with a Semantic-based Data Filtering strategy to mitigate content homogeneity among these videos.  A Semantic-aware Key-frame Extraction strategy is then applied to the videos to reduce the temporal redundancy. DiffSW is applied to the keyframes to generate high-quality captions and we further improve its stability and quality with a Hierarchical Prompt Design. Manual quality inspection is employed to ensure the quality of the video captions.

Based on ShareGPT4Video, we present ShareCaptionor-Video, an exceptional video captioner capable of efficiently generating high-quality captions for videos of a wide range of resolution, aspect ratio, and duration. It enables the further scaling of high-quality video caption data with minor cost and satisfactory quality, and we generate high-quality captions for 4.8M aesthetically appealing videos (totaling about 3000 hours) by it. 
 

We conduct extensive experiments in video understanding and generation tasks to demonstrate the value of our high-quality video-caption dataset and our superior video captioner. 
For video generation, a DiT-based \cite{peebles2023scalable} text-to-video model trained on the 4.8M video-captions pairs performs well in generating 10-second high-resolution videos and achieving fine-grained control over content generation.
For video understanding, ShareGPT4Video brings consistent performance gain of multiple current LVLMs over multiple benchmarks by replacing a small proportion of training data. We further present ShareGPT4Video-8B, a simple yet superb LVLM that reached SOTA performance on three advancing and comprehensive video benchmarks. The model, strategy, and annotations will be publicly available and we hope this project can serve as a pivotal resource for advancing both the LVLMs and T2VMs community.

\section{ShareGPT4Video Dataset}
This section provides a detailed exposition of how we construct the ShareGPT4Video dataset. We detail the entire process in Figure \ref{fig:pipeline1}. In Section \ref{sec:data_source}, we describe our methods for collecting and filtering video data. In Section \ref{sec:processing}, we explain how we perform efficient sparse sampling of the videos. In Section \ref{sec:captioning}, we detail of how we leverage the multimodal image model GPT-4V to generate high-quality captions for the videos.

\subsection{Data Collection}
\label{sec:data_source}
\textbf{Selection of Data Sources.} To serve both video understanding and video generation tasks, we consider the aesthetic quality and content complexity of videos during our collection process. We first consider Panda-70M \cite{chen2024panda}, a high-resolution video dataset sourced from YouTube, featuring clips ranging in one minute. This open-domain source covers diverse areas such as wildlife, cooking, sports, news \& TV shows, gaming \& 3D rendering. It typically includes complex content and transitions, providing a solid foundation for understanding various real-world scenarios. However, the complexity of these contents and transitions presents a significant challenge for the video generation field. To address this, we also source a large volume of aesthetically appealing videos from some user-uploaded video websites \cite{pexels,pixabay,mixkit}. These videos predominantly consist of scenic views and aesthetically pleasing human activities, involving fewer transitions and simpler events.
Finally, we supplement our collection with selected videos from Ego4D \cite{grauman2022ego4d} and BDD100K \cite{yu2020bdd100k} to fill the gaps in ego-centric human activities and auto-driving scenarios, ensuring our video sources encompass as many real-world scenes as possible. 

\textbf{Semantic-Based Data Filtering.} Although our captioning method can support videos of extended lengths, our collection primarily focuses on videos shorter than two minutes due to the trade-off of the duration and amount of videos. We initially filter out videos from our selected data sources longer than two minutes, leaving videos in two minutes as the candidates. We then introduce a semantic-based data filtering strategy to mitigate content homogeneity among these candidates and maintain diversity in the final video dataset. This approach aims to select videos with significant thematic differences from the pool of candidates to compose our final video collection.
Specifically, we first use the Panda-Student \cite{chen2024panda} model to generate a short caption with one sentence for each candidate video, and then maintain a final pool of video candidates. Whenever a new video $V$ is processed, we encode its corresponding short caption $S$ using the Bert-Base-Uncased \cite{devlin2018bert} language model to obtain the \texttt{CLS} token $P_{n+1} \in {\mathbb{R}^{1 \times D}}$, which captures high-level semantic expressions. We then calculate the similarity between this \texttt{CLS} token $P_{n+1}$ and the \texttt{CLS} tokens $\left\{ {{P_1},{P_2}, \ldots ,{P_n}} \right\}$ of videos already in the final candidate pool. A new video will only be added to the pool if its maximum similarity is below a predefined threshold. We provide the pseudo-code in Figure \ref{fig:code1}.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/files/pipeline_2.pdf}
    \captionsetup{font={footnotesize}}
    \caption{\textbf{Pipeline for generating high-quality video-caption data.}
    We begin by selecting diverse video sources based on aesthetic quality and content complexity. Next, we use semantic-based data filtering to prevent content homogenization. We then apply semantic-aware key-frame extraction for sparse sampling, maintaining significant semantic variations. Finally, we implement a differential sliding-window captioning strategy, utilizing GPT-4V to generate detailed and temporally rich captions.
    }
    \label{fig:pipeline1}
\end{figure*}
\subsection{Video Processing}
\label{sec:processing}
Videos are commonly redundant on the temporal dimension, and keyframe sampling is a general idea to represent a video compactly.  
However, traditional key-frame extraction methods \cite{zhuang1998adaptive,calic2002efficient} often struggle to ensure semantic coherence, leading to missing key-frames covering crucial changes and transitions. Consequently, we develop a semantic-aware key-frame extraction method that strikes a balance between reducing temporal redundancy and maintaining semantic coherence.

\textbf{Semantic-aware Key-frame Extraction.} 
We denote ${V} \in {\mathbb{R}^{T \times H \times W \times 3}}$ as a $T$ frame set sampled from a video with fixed 2-second intervals.
We calculate the keyframe set ${V'} \in {\mathbb{R}^{T' \times H \times W \times 3}}$ that are sufficiently sparse yet comprehensively cover the evolution of events within the video that $T' < T$. We view the output \texttt{CLS} token of the CLIP-Large image encoder \cite{radford2021learning} as the global semantics of each frame and remove the adjacent frames that have a high semantic similarity. In practice, we initialize the keyframe set $V'$ with the first frame of $V$. For each frame in $V$, we calculate its semantic similarity $d$ with the latest keyframe in $V'$. If $d$ is lower than the pre-defined threshold, we view the frame as a keyframe and add it to the $V'$. If not, the frame is skipped as redundant. For completeness, the last frame of $V$ is always added in $V'$. We provide the pseudo-code in Figure \ref{fig:code2}.

\subsection{Captioning Pipeline}
\label{sec:captioning}
As we mentioned in Section \ref{sec:intro}, we find if we feed all the frames to the GPT4V directly, the GPT4V struggles to stably generate captions with the correct temporal relation between frames, and its performance further worsens with the frame number increasing. On the other hand, if we concatenate all the frames into a large image, the GPT4V loses more details with the increasing frame number, as shown in Figure~\ref{fig:caption_strategy_2}-\ref{fig:caption_strategy_3}. 

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/files/pie-light.pdf}
    \captionsetup{font={footnotesize}}
    \vspace{-2mm}
    \caption{\textbf{Comprehensive video-caption dataset}: (a) The dataset covers a broad spectrum of content, including wildlife, cooking, sports, scenery, ego-centric human activities, auto-driving scenarios, etc. 
    (b) The dataset includes videos ranging from 2 seconds to 2 minutes in length.
    (c) The captions primarily range from 200 to 400 words, providing rich temporal information that serves video understanding and generation tasks well.}
    \label{fig:data-pie}
    \vspace{-5mm}
\end{figure*}\textbf{Differential Sliding-window Captioning.} 
To this end, we develop a differential sliding-window captioning pipeline to generate high-quality captions with detailed temporal descriptions for various videos. Specifically, the input fed to the image multi-modal model each time includes the current key-frame and the previous key-frame along with its differential caption. Then, we introduce the Differential Prompt to guide GPT4V in focusing on the changes between the current and previous frames, such as posture, position, camera angle, etc. Additionally, incorporating the differential caption of the previous frame as supplementary context enhances the response quality and reduces hallucinations. This is because the image embedding and textual caption provide explicit and implicit representations of the image, respectively. The differential caption not only adds extra context but also integrates temporal information from two frames ago, further improving the model's temporal understanding. It's important to note that for the first key-frame, which lacks a preceding frame, its differential caption is replaced directly with the standard caption. Finally, we input all differential captions along with their corresponding timestamps into GPT4. A specific Summary Prompt is designed to instruct the LLM to generate high-quality video captions with precise temporal dynamics and detailed spatial information. In practice, we use \texttt{GPT-4-Turbo-04-09} for all the annotations.

In the design of the prompts, we discovered that an explicit Hierarchical Prompt Design significantly aids the GPT4V in comprehending its role, its expected format, and its operational boundaries. This approach contributes to the stabilization of the output’s format and enhances the overall quality of the results. For more details, please refer to Section \ref{sec:app_prompt_design}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/files/captioner.pdf}
    \captionsetup{font={footnotesize}}
    \caption{The ShareCaptioner-Video is a Four-in-One exceptional video captioning model with the following capabilities: Fast captioning, Sliding Captioning, Clip Summarizing, and Prompt Re-Captioning.}
    \label{fig:captioner}
\end{figure*}
\section{ShareCaptioner-Video}
\subsection{Model design}
We fine-tune the IXC2-4KHD \cite{dong2024internlm4k} using the collected video caption data, resulting in our ShareCaptioner-Video. For flexible usage, we re-organize the data for the following capabilities: 

\noindent\textbf{1. Fast Captioning} The model employs an image-grid format for direct video captioning, providing rapid generation speeds that are ideal for short videos. In practice, we concatenate all the keyframes of a video into a vertically elongated image and train the model on a caption task.

\noindent\textbf{2. Sliding Captioning} The model supports streaming captioning in a differential sliding-window format, yielding high-quality captions that are suitable for long videos. Similar to the captioning pipeline used in Section \ref{sec:captioning}, we take the two adjacent keyframes alongside the previous differential caption as input, and train the model to describe the events occurring between them.

\noindent\textbf{3. Clip Summarizing} The model can swiftly summarize any clip from ShareGPT4Video or videos that have undergone the differential sliding-window captioning process, eliminating the need to re-process frames. We use all the differential descriptions as input, and the output is the video caption.

\noindent\textbf{4. Prompt Re-Captioning:} The model can rephrase prompts input by users who prefer specific video generation areas, ensuring that T2VMs trained on high-quality video-caption data maintain format alignment during inference with their training. In practice, we use GPT-4 to generate Sora-style prompts for our dense captions, and we train the re-captioning task in reverse, \textit{i.e.}, by using the generated prompt as input and the dense caption as the training target.  

In practice, we fine-tune the model end-to-end over one epoch. We follow the default high-resolution strategy, using ‘HD-55’ for fast captioning and ‘HD-25’ for the others. The learning rate is uniform across all model components and warms up from 0 to $2.5\times 10^{-5}$ within the first 1\% of steps. The batch size is set to $1024$, and we sample the data uniformly.

\subsection{Scaling-up Captions}

To validate the effectiveness of our ShareCaptioner-Video in the video captioning task and further support the development of the video generation domain, we utilized it to annotate a large volume of aesthetically appealing videos. Specifically, we meticulously collect and process 4.8 million video clips, totaling approximately 3000 hours, from three sources: MixKit \cite{mixkit}, Pexels \cite{pexels}, and Pixabay \cite{pixabay}. Subsequently, we employ the sliding captioning mode of ShareCaptioner-Video to generate high-quality captions for these videos. The total captioning process requires approximately 4000 H100 GPU hours. We provide some statistics on generated captions in Figure \ref{fig:asthetic}.

\section{Experiments}
\subsection{Video Understanding}
\textbf{Datasets and Benchmarks.}
To thoroughly explore the benefits that our high-quality video-caption data bring to LVLMs, we conduct comprehensive evaluations of the model across three multi-modal video benchmarks. VideoBench \cite{ning2023video} curates approximately 15,000 QA pairs spanning 10 evaluation dimensions from 13 existing data sources, such as MSVD-QA \cite{xu2017video}, MSRVTT-QA \cite{xu2017video}, Activitynet-QA \cite{yu2019activitynet}, etc. MVBench \cite{li2023mvbench} is designed to challenge LVLMs with video tasks that cannot be effectively resolved by single-frame reliance, featuring 4,000 QA pairs derived from 11 public video benchmarks. TempCompass \cite{liu2024tempcompass} specifically assesses the nuanced performance of LVLMs across various temporal aspects, such as speed, direction, and attribute changes. It includes 410 videos and 7,540 meticulously collected instructions, emphasizing temporal comprehension and interaction.

\begin{figure}[!t]
    \centering
    \begin{minipage}[h]{0.45\textwidth}
        \centering
        \captionsetup{font={scriptsize}}
        \captionof{table}{\textbf{The gain from high-quality captions is universal among model architectures and scales.} We report the baseline based on their public checkpoints. The best results are \textbf{bold}.}
        \label{tab:ablation1}
        \resizebox{\textwidth}{!}{
        \setlength\tabcolsep{1.6pt}{
            \begin{tabular}{l|ccc|c}
            \toprule
            Model              & VideoBench    & MVBench       & TempCompass   & Avg.          \\ \midrule
            VideoLLaVA-7B \cite{lin2023video}      & 34.5          & 43.0          & 50.6          & 42.7          \\
            VideoLLaVA-7B+Ours & \textbf{35.2} & \textbf{43.6} & \textbf{52.7} & \textbf{43.8} \\ \midrule
            LLaMA-VID-7B \cite{li2023llama}       & 36.5          & 41.3          & 48.1          & 42.0          \\
            LLaMA-VID-7B+Ours  & \textbf{38.2} & \textbf{43.2} & \textbf{50.6} & \textbf{44.0} \\ \midrule
            LLaMA-VID-13B \cite{li2023llama}       & 48.3          & 43.3          & 51.4          & 47.7          \\
            LLaMA-VID-13B+Ours  & \textbf{52.4} & \textbf{44.2} & \textbf{53.3} & \textbf{50.0} \\ \bottomrule
            \end{tabular}
        }}
    \end{minipage}
    \quad
    \begin{minipage}[h]{0.48\textwidth}
        \centering
        \captionsetup{font={scriptsize}}
        \captionof{table}{\textbf{Combined with VQA data, detailed captions can benefit LVLMs more compared to short captions.} The baseline (first row) utilizes only 153K VQA data. The best results are in \textbf{bold}.}
        \label{tab:ablation2}
        \resizebox{\linewidth}{!}{
        \setlength\tabcolsep{2pt}{
        \renewcommand\arraystretch{1.19}
        \begin{tabular}{cc|ccc|c}
    \toprule
    Caption  & Unlock ViT & VideoBench & MVBench & TempCompass & Avg. \\ \midrule
      --      &   $\times$    & 37.3       & 47.2    & 57.2        & 47.2 \\
    short    &   $\times$    & 36.9          & 47.5       & 56.1           & 46.8    \\
    short    & \checkmark          & 37.5          & 47.9       & 56.9           & 47.4    \\
    detailed &   $\times$    & 40.7          & 50.3       & 60.7           & 50.6    \\
    detailed & \checkmark          & \textbf{41.2}       & \textbf{51.2}    & \textbf{61.5}        & \textbf{51.3}\\ \bottomrule
    \end{tabular}
        }}
    \end{minipage}
\end{figure}
\textbf{Improving current LVLMs with ShareGPT4Video.}
We validate the effectiveness of the high-quality video-caption data collected in ShareGPT4Video to improve the performance of current LVLMs. For fairness and simplicity, we integrate 28K high-quality video-caption data related to complex scenes (Panda-70M \cite{chen2024panda}, Ego4D \cite{grauman2022ego4d}, and BDD100K \cite{yu2020bdd100k}) of ShareGPT4Video to replace the captions data in the VideoChatGPT-100K \cite{maaz2023video} conversation data with an equivalent number.
Then we train the VideoLLaVA \cite{lin2023video} and LLaMA-VID \cite{li2023llama} with their default training settings and hyperparameters. As shown in Table \ref{tab:ablation1}, ShareGPT4Video consistently improves the alignment between video and language modalities in different LVLM architectures and scales. Specifically, VideoLLaVA-7B \cite{lin2023video} achieves an average performance gain of 1.1 across three comprehensive multi-modal video benchmarks after integrating high-quality captions, while LLaMA-VID-7B and LLaMA-VID-13B achieve an average gain of 2.0 and 2.3, separately. Our high-quality video-caption data is particularly effective in helping LVLMs achieve significant performance improvements on benchmarks that require complex temporal understanding, such as TempCompass \cite{liu2024tempcompass}. 

\textbf{ShareGPT4Video-8B.}
To obtain our final ShareGPT4Video-8B model, we start with the LLaVA-Next-8B \cite{li2024llavanext-strong} image multi-modal model. Consistent with previous LVLM approaches \cite{lin2023video,maaz2023video}, we uniformly sample 16 frames from each video and arrange these frames into a 4x4 image grid to form the input for both training and inference, following the IG-VLM \cite{kim2024image} strategy. For training data, we first collect 153K VQA data from various instructional video-to-text datasets to build our baseline. This collection includes 13K conversational data from VideoChatGPT \cite{maaz2023video} and 140K question-answer pairs, with 45K data points from CLEVRER \cite{yi2019clevrer}, 8K from EGO-QA \cite{grauman2022ego4d}, 34K from NextQA \cite{xiao2021next}, and 53K from TGIF-Transition \cite{li2016tgif}. Then, these VQA data are combined with 28K video-caption data, forming a consolidated training dataset of 181K samples. For more training details, please refer to Section \ref{sec:app_exp}.

As illustrated in Table \ref{tab:tempcompass}, \ref{tab:vbench}, \ref{tab:mvbench}, we present a quantitative comparison between our ShareGPT4Video-8B model supercharged by our ShareGPT4Video dataset with existing state-of-the-art LVLMs. Notably, compared with previous LVLMs, our ShareGPT4Video-8B attains the most superior performance in all three comprehensive benchmarks.  
Specifically, thanks to the rich temporal information provided by ShareGPT4Video, our ShareGPT4Video-8B model achieves an impressive average accuracy of 61.5\% on the TempCompass benchmark. This is an 11.6\% increase over the previous best-performing LVLM, VideoLLaVA-7B. 
Additionally, despite the VideoBench and MVBench benchmarks collecting a diverse range of QA data from various existing video datasets, we achieve solid performance on these benchmarks, surpassing the previous state-of-the-art by an average accuracy of 2.7\% and 8.2\%. 

\textbf{Abaltion on caption quality and ViT.}
Based on ShareGPT4Video-8B, we study how the modality alignment is influenced by caption quality and learnable vision encoder. As indicated in Table \ref{tab:ablation2}, introducing short captions on top of VQA data may not yield substantial performance gains. It could even degrade performance on some benchmarks due to sub-optimal modality alignment. Comparing the first, second, and fourth rows of Table \ref{tab:ablation2}, the significant performance gains of comprehending temporal sequences benefited from our high-quality caption data are evident. Moreover, unlocking the vision encoder when training with detailed captions facilitates better LVLMs modality alignment.

\renewcommand{\arraystretch}{1.15}
\begin{table*}[!t]
    \setlength\tabcolsep{4pt}
    \centering
    \footnotesize
    \captionsetup{font={footnotesize}}
    \vspace{-5pt}
    \caption {\textbf{Comparison with SOTA methods on TempCompass.} With 7B parameters, ShareGPT4Video-8B outperforms competitors in 19 out of 20 dimensions, despite these competitors using larger training data or more parameters. The best results are \textbf{bold} and the second-best results are \underline{underlined}.}
    \vspace{-2pt}
    \label{tab:tempcompass}
    \scalebox{0.66}{
\begin{tabular}{l|ccccc|ccccc|ccccc|ccccc|c}
\toprule
\multicolumn{1}{l|}{\raisebox{-2\totalheight}[0pt][0pt]{Model}} &\multicolumn{5}{c|}{Multi-Choice QA}                                          & \multicolumn{5}{c|}{Yes/No QA}                                                & \multicolumn{5}{c|}{Caption Matching}                                         & \multicolumn{5}{c|}{Caption Generation} &\multirow{2}{*}{Avg.}  \\ \cmidrule{2-21} 
\multicolumn{1}{c|}{} &AC            & DI            & SP            & EV            & AT            & AC            & DI            & SP            & EV            & AT            & AC            & DI            & SP            & EV            & AT            & AC            & DI            & SP            & EV            & AT            \\ \midrule
Valley-7B \cite{luo2023valley} &47.0          & 29.3          & 32.5          & 18.9          & 29.9          & 58.1          & {\ul 52.0}    & 52.5          & 50.3          & {\ul 52.9}    & 65.0          & {\ul 53.8}    & 52.6          & 53.0          & 53.8          & 54.0          & {\ul 31.0}    & {\ul 32.7}    & 34.2          & {\ul 41.4} & 33.4    \\
PandaGPT-13B \cite{chen2024panda} &35.5          & 27.8          & 29.3          & 31.8          & 30.9          & 53.0          & 49.6          & 50.8          & {\ul 53.7}    & 52.2          & 56.6          & 51.4          & 44.3          & 55.0          & 49.0          & 23.7          & 25.7          & 26.0          & 29.8          & 32.6 & 40.4         \\
VideoLLaMA-13B \cite{zhang2023video} &54.1          & 24.5          & 28.1          & 32.8          & 28.5          & 68.1          & 46.0          & 48.8          & 51.8          & 50.9          & 73.1          & 47.4          & 47.1          & 52.0          & 48.3          & {\ul 54.3}    & 21.3          & 13.9          & {\ul 38.5}    & 33.9 & 43.3         \\
VideoChatGPT-7B \cite{maaz2023video} &47.0          & 31.6          & 28.4          & 37.1          & 30.9          & 52.5          & 50.0          & 49.5          & 51.0          & 50.0          & 64.6          & 48.6          & 47.8          & 49.3          & 48.6          & 40.9          & 28.4          & 24.5          & 31.8          & 33.9 & 42.4         \\
mPLUG-Owl-7B \cite{ye2023mplug} &66.6          & 29.3          & 32.2          & 34.8          & {\ul 35.4}          & 64.4          & 50.6          & {\ul 51.2}          & 51.3          & 52.0          & 56.9          & 45.3          & 46.4          & 49.3          & 49.0          & 46.5          & 28.2          & 30.4          & 31.2          & 36.5 & 44.5         \\
VideoLLaVA-7B \cite{li2023mvbench} &{\ul 70.4}          & {\ul 32.2}          & {\ul 38.2}          & {\ul 41.4}    & 39.9          & {\ul 74.3}    & 51.8          & 50.3          & 49.2          & 51.1          & {\ul 88.2}    & {\ul 53.8}    & \textbf{61.9} & {\ul 57.0}    & {\ul 58.3}    & 50.8          & 28.7          & 23.2          & 38.2          & 33.6 & {\ul 49.9}         \\
LLaMA-VID-7B \cite{li2023llama} &58.6          & 29.9          & 29.3          & 30.5          & 26.0          & 63.0          & 48.8          & 49.2          & 48.4          & 52.7          & 72.7          & 45.6          & 52.2          & 49.0          & 49.0          & 53.0          & 28.0          & 21.9          & 35.5          & 35.9 & 44.2         \\ \midrule
ShareGPT4Video-8B &\textbf{87.6 }   & \textbf{34.6 }   & \textbf{47.5} & \textbf{62.9} & \textbf{64.2} & \textbf{75.2} & \textbf{53.8} & \textbf{58.6} & \textbf{66.5} & \textbf{65.6} & \textbf{93.3} & \textbf{58.1} & {\ul 58.8}    & \textbf{75.0} & \textbf{75.3} & \textbf{79.8} & \textbf{32.6} & \textbf{36.6} & \textbf{50.8} & \textbf{53.4} &\textbf{61.5} \\ \bottomrule
\end{tabular}    
}
\end{table*}\renewcommand{\arraystretch}{1.1}
\begin{table*}[!t]
    \centering
    \footnotesize
    \captionsetup{font={footnotesize}}
    \vspace{-5pt}
    \caption {\textbf{Comparison with SOTA methods on VideoBench.} * denotes our evaluation results with the public checkpoints. The best results are \textbf{bold} and the second-best results are \underline{underlined}.}
    \label{tab:vbench}
    \scalebox{0.714}{
        \begin{tabular}{l|ccccccccccccc|c}
        \toprule
        Model             & ANet          & MSVD          & MSRVTT        & TGIF          & YC2      & UCF           & MOT           & TV            & MV            & NBA           & LE            & DM            & SQA3D         & Avg.           \\ \midrule
        mPLUG-Owl-7B \cite{ye2023mplug}     & 41.5          & 42.5          & 36.3          & 31.7          & 27.1          & 22.8          & \textbf{27.8} & 24.0          & 30.2          & 25.1          & 33.3          & 51.0          & 32.0          & 33.2          \\
        Otter-7B \cite{li2023otter}          & 44.3          & 55.0          & \textbf{47.0} & 34.3          & 32.7          & 22.4          & 16.7          & 27.7          & \textbf{37.1} & \textbf{34.3} & {\ul 52.8}    & 48.7          & 29.7          & 37.5          \\
        Video-LLaMA-7B \cite{zhang2023video}    & 39.9          & 41.2          & 34.1          & 31.3          & 28.9          & 27.6          & 16.7          & 24.8          & 32.4          & 26.2          & \textbf{60.6} & 49.1          & 31.2          & 32.8          \\
        Valley-7B \cite{luo2023valley}         & 38.1          & 32.0          & 28.0          & 31.4          & 29.1          & 20.3          & 11.1          & 23.7          & 32.6          & {\ul 31.3}    & 41.7          & {\ul 56.5}    & 33.3          & 34.0          \\
        VideoChat-7B \cite{li2023videochat}     & 44.6          & 42.2          & 37.4          & 33.7          & 27.7          & 22.4          & \textbf{27.8} & 26.2          & 34.1          & 28.6          & 39.9          & 55.4          & 31.4          & 35.4          \\
        PandaGPT-7B \cite{chen2024panda}       & 45.0          & 50.4          & 44.6          & 29.7          & 33.0          & {\ul 33.0}    & 16.7          & 27.9          & \textbf{37.1} & 31.1          & 41.7          & 56.0          & 30.8          & 37.5          \\
        VideoChatGPT-7B \cite{maaz2023video}   & 46.6          & \textbf{57.5} & {\ul 46.3}    & 35.6          & \textbf{34.8} & 24.1          & \textbf{27.8} & 28.8          & {\ul 36.5}    & 22.5          & 41.7          & \textbf{58.2} & 37.2          & {\ul 38.5}    \\
        ChatUniVi-7B \cite{jin2023chat}     & {\ul 49.0}    & 48.6          & 41.7          & {\ul 41.3}    & 29.0          & 28.3          & 16.7          & 23.1          & 33.6          & 25.7          & 38.9          & 53.1          & 29.1          & 35.3          \\
        VideoLLaVA-7B* \cite{lin2023video}    & 44.1          & 34.5          & 30.0          & 39.4          & 30.7          & 19.5          & {\ul 22.2}    & 27.3          & 33.4          & 25.6          & 33.3          & 50.7          & {\ul 38.9}    & 34.5          \\
        LLaMA-VID-7B* \cite{li2023llama}     & 45.2          & 44.5          & 39.1          & 29.1          & 29.3          & 27.9          & 11.1          & \textbf{34.1} & 32.5          & 28.9          & 36.1          & 47.8          & 36.8          & 36.5          \\ \midrule
        ShareGPT4Video-8B & \textbf{50.8} & {\ul 45.6}    & 43.0          & \textbf{42.8} & {\ul 34.6}    & \textbf{39.7} & {\ul 22.2}    & {\ul 31.9}    & 34.0          & 30.5          & 41.7          & 53.6          & \textbf{42.9} & \textbf{41.2} \\ \bottomrule
        \end{tabular}
}
\end{table*}\renewcommand{\arraystretch}{1.15}
\begin{table*}[!t]
    \setlength\tabcolsep{4pt}
    \centering
    \footnotesize
    \captionsetup{font={footnotesize}}
    \vspace{-5pt}
    \caption {\textbf{Comparison with SOTA methods on MVBench.} * denotes our evaluation results with the public checkpoints. The best results are \textbf{bold} and the second-best results are \underline{underlined}.}
    \vspace{-2pt}
    \label{tab:mvbench}
    \scalebox{0.665}{
    \begin{tabular}{l|cccccccccccccccccccc|c}
\toprule
Model& AS            & AP            & AA            & FA            & UA            & OE            & OI            & OS            & MD            & AL            & ST            & AC            & MC            & MA            & SC            & FP            & CO            & EN            & ER            & CI &Avg.           \\ \midrule
Otter-7B \cite{li2023otter} &23.0          & 23.0          & 27.5          & 27.0          & 29.5          & 53.0          & 28.0          & 33.0          & 24.5          & 23.5          & 27.5          & 26.0          & {\ul 28.5}    & 18.0          & 38.5          & 22.0          & 22.0          & 23.5          & 19.0          & 19.5 &26.8         \\
mPLUG-Owl-7B \cite{ye2023mplug} &22.0          & 28.0          & 34.0          & 29.0          & 29.0          & 40.5          & 27.0          & 31.5          & 27.0          & 23.0          & 29.0          & 31.5          & 27.0          & 40.0          & 44.0          & 24.0          & 31.0          & 26.0          & 20.5          & 29.5 &29.7         \\
LLaMA-Adapter \cite{zhang2023llama} &23.0          & 28.0          & 51.0          & 30.0          & 33.0          & 53.5          & 32.5          & 33.5          & 25.5          & 21.5          & 30.5          & 29.0          & 22.5          & 41.5          & 39.5          & 25.0          & 31.5          & 22.5          & 28.0          & 32.0  &31.7        \\
VideoChatGPT-7B \cite{maaz2023video} &23.5          & 26.0          & {\ul 62.0}    & 22.5          & 26.5          & {\ul 54.0}    & 28.0          & {\ul 40.0}    & 23.0          & 20.0          & 31.0          & 30.5          & 25.5          & 39.5          & {\ul 48.5}    & 29.0          & 33.0          & 29.5          & 26.0          & 35.5  &32.7        \\
VideoLLaMA-7B \cite{zhang2023video} &27.5          & 25.5          & 51.0          & 29.0          & 39.0          & 48.0          & 40.5          & 38.0          & 22.5          & 22.5          & 43.0          & 34.0          & 22.5          & 32.5          & 45.5          & {\ul 32.5}    & 40.0          & {\ul 30.0}    & 21.0          & {\ul 37.0} &34.1   \\
VideoChat-7B \cite{li2023videochat} &33.5          & 26.5          & 56.0          & 33.5          & 40.5          & 53.0          & 40.5          & 30.0          & 25.5          & 27.0          & 48.5          & 35.0          & 20.5          & 42.5          & 46.0          & 26.5          & 41.0          & 23.5          & 23.5          & 36.0 &35.5         \\
VideoLLaVA-7B* \cite{lin2023video} &{\ul 46.0}    & \textbf{42.5} & 56.5          & 39.0          & 53.5          & 53.0          & {\ul 48.0}    & \textbf{41.0} & {\ul 29.0}    & {\ul 31.5}    & 82.5          & \textbf{45.0} & 26.0          & {\ul 53.0}    & 41.5          & \textbf{33.5} & {\ul 41.5}    & 27.5          & 38.5          & 31.5  &{\ul 43.0}        \\
LLaMA-VID-7B* \cite{li2023llama} &45.5          & {\ul 40.5}    & 58.0          & {\ul 39.5}    & \textbf{55.0} & 53.5          & 40.0          & 35.5          & 18.5          & 27.5          & \textbf{87.0} & {\ul 41.5}    & 23.0          & 45.5          & 41.0          & 27.0          & 40.0          & \textbf{34.5} & \textbf{41.5} & 31.5  &41.3        \\ \midrule
ShareGPT4Video-8B &\textbf{49.5} & 39.5          & \textbf{79.5} & \textbf{40.0} & {\ul 54.5}    & \textbf{82.5} & \textbf{54.5} & 32.5          & \textbf{50.5} & \textbf{41.5} & {\ul 84.5}    & 35.5          & \textbf{62.5} & \textbf{75.0} & \textbf{51.0} & 25.5          & \textbf{46.5} & 28.5          & {\ul 39.0}    & \textbf{51.5} &\textbf{51.2} \\ \bottomrule
\end{tabular}
}
    \vspace{-15pt}
\end{table*}
\subsection{Video Captioning}
To verify the capability of ShareCapitoner-Video, we quantitatively compare the video captioning quality between ShareCapitoner-Video and GPT4V with human preference voting. As shown in Table \ref{table:user_study}, it performs on par with GPT4V. We also shows the qualitative results in Figure \ref{fig:sharecaptioner}. For more details, please refer to Section \ref{sec:app_cap_quality}

\subsection{Video Generation}
\textbf{Model setup.}
To validate the effectiveness of high-quality captions in the T2VMs area, we utilize ShareCaptioner-Video and Panda-Student \cite{chen2024panda} to generate high-quality and short video captions for 4.5M videos with 65 frames and 0.3M videos with 221 frames, separately. Following the process outlined in the Open-Sora-Plan \cite{pku_yuan_lab_and_tuzhan_ai_etc_2024_10948109}, we fine-tuned the pretrained T2VM to enable the generation of high-fidelity 10-second videos. For comparison, we fine-tuned a baseline model with the same quantity of video-short-captions pairs. For more training details, please refer to Section \ref{sec:app_exp}.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/files/t2v_castle.pdf}
    \captionsetup{font={footnotesize}}
    \vspace{-5pt}
    \caption{\textbf{Example of 10-second text-to-video task.} the T2VM trained on the detailed video-caption data can exhibit impressive camera control.}
    \label{fig:t2v}
    \vspace{-5pt}
\end{figure*}\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/files/t2v_compare.pdf}
    \captionsetup{font={footnotesize}}
    \vspace{-15pt}
    \caption{\textbf{Influence of T2VM training caption length.} Thanks to the high-quality captions generated by ShareCaptioner-Video, the T2VM trained on the detailed video-caption data exhibits impressive semantic content control (video below), while the T2VM with short captions failed to follow the complex prompt (video above).}
    \label{fig:t2v_compare}
    \vspace{-5pt}
\end{figure*}\textbf{Qualitative analysis.}
As illustrated in Figure \ref{fig:t2v}, the T2VM can accurately follow detailed prompts and demonstrate remarkable control over semantic content and camera movement when aided by high-quality, detailed captions generated by ShareCaptioner-Video. The resulting video showcases intricate and lively content. In contrast, when provided with brief captions, the T2VM struggles to adhere to complex generation prompts, leading to subpar results.

\section{Limitations and Social Impacts}
\label{sec:limitation}
\textbf{Limitations.} Although our current pipeline for generating high-quality video captions fully utilizes visual and textual information, it is limited by GPT4V's inability to incorporate audio information simultaneously. Audio information is beneficial in conversational scenarios involving daily human activities. We plan to introduce audio information in future work, once GPT4o supports audio input, to enhance the quality of our captions further.

\textbf{Social impacts.} 1)Since the large language model involves the generation process of the large-scale captions, we have not manually verified each caption for socially biased content; 2) Although we utilize video data from existing public datasets, we cannot ensure that the selected videos do not contain human faces. Therefore, while there are no restrictions on the use of our generated captions, users must adhere to the licenses of the original video sources when using the videos.

\section{Conclusion}
In this study, we aim to address the challenge of lacking high-quality video-caption data for large video-language models (LVLMs) and text-to-video models (T2VMs). We develop ShareGPT4Video, a high-quality video-caption dataset, and ShareCaptioner-Video, an advanced and versatile model in the video-language multi-modal area. By employing a series of strategies and designs, we generate 40K detailed captions from advanced image multi-modal model, GPT4V, and 4.8M high-quality captions from our ShareCaptioner-Video. These captions include rich world knowledge, object attributes, camera movements, and detailed temporal descriptions of events. Our extensive experiments validate the effectiveness of our dataset and captioner in enhancing video understanding and generation tasks. We believe that ShareGPT4Video and ShareCaptioner-Video will serve as essential resources for advancing research in the LVLM and T2VM communities.

\clearpage

\end{document}