\title{\modelfullns: \\ A Multi-Task Transformer for Robotic Manipulation}

\begin{document}

\maketitle

\vspace{-1.0cm}
\begin{abstract}
    Transformers have revolutionized vision and natural language processing with their ability to scale with large datasets. But in robotic manipulation, data is both limited and expensive.  
    Can manipulation still benefit from Transformers with the right problem formulation?
    We investigate this question with \model, a language-conditioned behavior-cloning agent for multi-task 6-DoF manipulation. 
    \model~encodes language goals and RGB-D voxel observations with a Perceiver Transformer~\citep{jaegle2021perceiver}, and outputs discretized actions by ``detecting the next best voxel action''. 
    Unlike frameworks that operate on 2D images, the voxelized 3D  observation and action space provides a strong structural prior for efficiently learning 6-DoF actions.
    With this formulation, we train a single multi-task Transformer for \highlight{18 RLBench tasks (with 249 variations) and 7 real-world tasks (with 18 variations)} from just a few demonstrations per task.
    Our results show that \model~significantly outperforms unstructured image-to-action agents and 3D ConvNet baselines for a wide range of tabletop tasks.  
    
    
\end{abstract}

\keywords{Transformers, Language Grounding, Manipulation, Behavior Cloning} 

\section{Introduction}

Transformers~\citep{vaswani2017attention} have become  prevalent in natural language processing and computer vision. By framing problems as sequence modeling tasks, and training on large amounts of diverse data, Transformers have achieved groundbreaking results in several domains~\citep{brown2020language,dosovitskiy2020image,jumper2021highly,vinyals2019alphastar}. Even in domains that do not conventionally involve sequence modeling~\citep{chen2021pix2seq,chen2021decision}, Transformers have been adopted as a \textit{general} architecture~\citep{reed2022generalist}. But in robotic manipulation, data is both limited and expensive. 
Can we still bring the power of Transformers to 6-DoF manipulation with the right problem formulation?

Language models operate on sequences of tokens~\citep{devlin2018bert}, and vision transformers operate on sequences of image patches~\citep{dosovitskiy2020image}. While pixel transformers~\citep{jaegle2021perceivericml,jaegle2021perceiver} exist, they are not as data efficient as approaches that use convolutions or patches to exploit the 2D structure of images. Thus, while Transformers may be domain agnostic, they still require the right problem formulation to be data efficient. 
A similar efficiency issue is apparent in behavior-cloning (BC) agents that directly map 2D images to 6-DoF actions.
\highlight{Agents like Gato~\citep{reed2022generalist} and BC-Z~\citep{jang2022bc,ahn2022can} have shown impressive multi-task capabilities, but they require several weeks or even months of data collection}.
\highlight{In contrast, recent works in reinforcement-learning like C2FARM~\citep{c2farm} construct a voxelized  observation and action space to efficiently learn visual representations of 3D actions with 3D ConvNets.}  
Similarly, in this work, we aim to exploit the 3D structure of \textit{voxel patches} for efficient 6-DoF behavior-cloning with Transformers (analogous to how vision transformers~\citep{dosovitskiy2020image} exploit the 2D structure of image patches).

\begin{figure*}[!t]
    \centering
    \hspace*{-1.45cm}
    \includegraphics[width=1.2\textwidth]{figures/tasks_v1.pdf}
    \caption{\textbf{Language-Conditioned Manipulation  Tasks:} \model~is a language-conditioned multi-task agent capable of imitating a wide range of 6-DoF manipulation tasks. We conduct experiments on 18 simulated tasks in RLBench~\citep{james2020rlbench} (a-j; only 10 shown), with several pose and semantic variations. We also demonstrate our approach with a Franka Panda on 7 real-world tasks (k-o; only 5 shown) with a multi-task agent trained with just 53 demonstrations. See the supplementary video for simulated and real-world rollouts.}
    \label{fig:tasks}
    \vspace{-1.5em}
\end{figure*}

To this end, we present \model~(short for \modelfullns), a language-conditioned BC agent that can learn to imitate a wide variety of 6-DoF manipulation tasks with just a few demonstrations per task. \model~encodes a sequence of RGB-D voxel patches and predicts discretized translations, rotations, and gripper actions that are executed with a motion-planner in an observe-act loop. \model~is essentially a classifier trained with supervised learning to \textit{detect actions} akin to prior work like CLIPort~\citep{cliport,zengTransporterNetworksRearranging2021},
except our observations and actions are represented with 3D voxels instead of 2D image pixels. 
Voxel grids are  less prevalent than images in end-to-end BC approaches often due to scaling issues with high-dimensional inputs. But in \model, we use a Perceiver\footnote{Throughout the paper we refer to PerceiverIO~\citep{jaegle2021perceiver} as Perceiver for brevity.} Transformer~\citep{jaegle2021perceiver} to encode very high-dimensional input of up to 1 million voxels with only a small set of latent vectors. This voxel-based formulation provides a strong structural prior with several benefits: a natural method for fusing multi-view observations, learning robust action-centric\footnote{\highlight{Action-centric refers to a  system that learns perceptual representations of actions; see \appsecref{app:more_qpred}.}} representations~\citep{gibson2014ecological,brooks1991new}, and enabling data augmentation in 6-DoF -- all of which help learn generalizable skills by focusing on \textit{diverse} rather than narrow multi-task data.  

To study the effectiveness of this formulation, we conduct large-scale experiments in the RLBench~\citep{james2020rlbench} environment. We train a single multi-task agent on 18 diverse tasks with 249 variations that involve a range of prehensile and non-prehensile behaviors like placing wine bottles on a rack and dragging objects with a stick (see \figref{fig:tasks} a-j). Each task also includes several pose and semantic variations with objects that differ in placement, color, shape, size, and category. 
Our results show that \model~significantly outperforms image-to-action agents \highlight{(by $34\times$)} and 3D ConvNet baselines \highlight{(by $2.8\times$)}, without using any explicit representations of instance segmentations, object poses, memory, or symbolic states. 
We also validate our approach on a Franka Panda with a multi-task agent trained \textit{from scratch} on 7 real-world tasks  with a \textbf{total of just 53 demonstrations} (see \figref{fig:tasks} k-o).

In summary, our contributions are as follows:
\vspace{-0.2em}
\begin{itemize}[leftmargin=0.8cm,itemsep=0.05em]
    \item A \textbf{novel problem formulation} for perceiving, acting, and specifying goals with Transformers.
    \item An efficient \textbf{action-centric} framework for \textbf{grounding language in 6-DoF actions}.
    \item \textbf{Empirical results} investigating multi-task agents on a range of simulated and real-world tasks. 
\end{itemize}
The code and pre-trained models will be made available  at \url{peract.github.io}. 

\section{Related Work}
\label{sec:related_work}
\textbf{Vision for Manipulation.} Traditionally, methods in robot perception have used explicit ``object'' representations like instance segmentations, object classes, poses  \citep{he2017mask,Xiang-RSS-18,zhu2014single,zeng2017multi,deng2020self,xie2020best}. Such methods struggle with deformable and granular items like cloths and beans that are hard to represent with geometric models or segmentations. 
In contrast, recent methods~\citep{zeng2019robotic,zengTransporterNetworksRearranging2021,cliport,stengel2022guiding} learn action-centric representations without any ``objectness'' assumptions, but they are limited to top-down 2D settings with simple pick-and-place primitives. \highlight{In 3D, James et al. proposed C2FARM~\citep{c2farm}, an action-centric reinforcement learning (RL) agent with a coarse-to-fine-grain 3D-UNet backbone. The coarse-to-fine-grain scheme has a limited receptive field that cannot look at the entire scene at the finest level. In contrast, \model~learns action-centric representations with a global-receptive field through a Transformer backbone. Also, \model~does BC instead of RL, which enables us to easily train a multi-task agent for several tasks by conditioning it with language goals}. 

\textbf{End-to-End Manipulation} approaches~\citep{kalashnikov2018qt,Wu-RSS-20,levine2016end,finn2017deep} make the least assumptions about objects and tasks, but are often formulated as an image-to-action prediction task. Training directly on RGB images for 6-DoF tasks is often inefficient, generally requiring several  demonstrations or episodes just to learn basic skills like rearranging objects. In contrast, \model~uses a voxelized observation and action space, which is dramatically more efficient and robust in 6-DoF settings.
While other works in 6-DoF grasping~\citep{song2020grasping,murali20206,mousavian20196,xu2022umpnet,agrawal2021scene,simeonov2021neural} have used RGB-D and pointcloud input, they have not been applied to sequential tasks or used with language-conditioning.
Another line of work tackles data inefficiency by using pre-trained image representations ~\citep{cliport,nair2022r3m,yuan2021sornet} to bootstrap BC. Although our framework is trained from scratch, such pre-training approaches could be integrated together in future works for even greater efficiency and generalization to unseen objects. 

\textbf{Transformers for Agents and Robots.} Transformers have become the prevalent architecture in several domains. Starting with NLP~\citep{vaswani2017attention,brown2020language,liu2019roberta}, recently in vision~\citep{dosovitskiy2020image,liu2021swin}, and even RL~\citep{chen2021decision,janner2021offline,lee2022multi}. In robotics, Transformers have been applied to assistive teleop~\citep{clever2021assistive},  legged locomotion~\citep{yang2021learning},  path-planning~\citep{chaplot2021differentiable,johnson2021motion}, imitation learning~\citep{dasari2020transformers,kim2021transformer}, morphology controllers~\citep{gupta2022metamorph}, spatial rearrangement~\citep{liu2022structformer}, and grasping~\citep{han2021learning}. Transformers have also achieved impressive results in multi-domain settings like in Gato~\cite{reed2022generalist} where a single Transformer was trained on 16 domains such as captioning, language-grounding, robotic control etc. However, Gato relies on extremely large datasets like 15K episodes for block stacking and 94K episodes for Meta-World~\citep{yu2020meta} tasks. Our approach might complement agents like Gato, which could use our 3D formulation for greater efficiency and robustness. 

\textbf{Language Grounding for Manipulation.} Several works have proposed methods for grounding language in robot actions~\citep{Shridhar-RSS-18,matuszek2014learning,bollini2013interpreting,misra2016tell,bisk2016natural,thomason2015learning,interact_picking18,chenJointNetworkGrasp2021, blukis2020few, paxton2019prospection,tellex2011understanding,nguyen2020robot}. However, these methods use disentangled pipelines for perception and action, with the language primarily being used to guide perception~\citep{egl}. 
Recently, a number of end-to-end approaches \citep{ahn2022can,jang2022bc,nair2022learning,mees2022matters,lynch2020grounding} have been proposed for conditioning BC agents with language instructions. These methods require thousands of human demos or autonomous episodes that are collected over several days or even months. In contrast, \model~can learn robust multi-task policies with just a few minutes of training data. For benchmarking, several simulation environments exist~\citep{mees2021calvin,zengTransporterNetworksRearranging2021,yu2020meta}, but we use RLBench~\citep{james2020rlbench} for its diversity of 6-DoF tasks and ease of generating demonstrations with templated language goals. 

\section{\modelfull}

\model~is a language-conditioned behavior-cloning agent for 6-DoF manipulation. The key idea is to learn perceptual representations of actions conditioned on language goals. Given a voxelized reconstruction of a scene, we use a Perceiver Transformer~\citep{jaegle2021perceiver} to learn per-voxel features. Despite the extremely large input space ($100^3$), Perceiver uses a small set of latent vectors to encode the input. The per-voxel features are then used to predict the next best action in terms of discretized translation, rotation, and gripper state at each timestep. \model~relies purely on the current observation to determine what to do next in sequential tasks. 
See \figref{fig:peract} for an overview. 

\secref{sec:demos} and \secref{sec:keyframe_and_voxel} describe our dataset setup. \secref{sec:peract} describes our problem formulation with \model, and \secref{sec:training} provides details on training \model. Further implementation details are presented in \appsecref{app:peract_details}. 

\newpage
\begin{figure*}[!t]
    \centering
    \hspace*{-2.2cm}
    \includegraphics[width=1.3\textwidth]{figures/peract_main_fig_v6.pdf}
    \vspace{-0.4cm}
    \caption{\textbf{\model~Overview.} \model~is a language-conditioned behavior-cloning agent  trained with supervised learning to \textit{detect actions}. \model~takes as input a language goal and a voxel grid reconstructed from RGB-D sensors. The voxels are split into 3D patches, and the language goal is encoded with a pre-trained language model. These language and voxel features are appended together as a sequence and encoded with a Perceiver transformer~\citep{jaegle2021perceiver}. 
    Despite the extremely long input sequence, Perceiver uses a small set of latent vectors to encode the input (see \appfigref{fig:perceiver_arch} for an illustration). 
    These encodings are upsampled back to the original voxel dimensions with a decoder and reshaped with linear layers to predict a discretized translation, rotation, gripper open, and collision avoidance action. This action is executed with a motion-planner after which the new observation is used to predict the next discrete action in an observe-act loop until termination. 
    }
    \label{fig:peract}
    \vspace{-1.5em}
\end{figure*}
 
\vspace{-0.3cm} 
\subsection{Demonstrations} \label{sec:demos}
\vspace{-0.2cm}
We assume access to a dataset  $\mathcal{D} = \{\zeta_1, \zeta_2, \ldots, \zeta_n\}$ of $n$ expert demonstrations, each paired with English language goals $\mathcal{G} = \{\mathbf{l}_{1}, \mathbf{l}_{2}, \ldots, \mathbf{l}_{n}\}$. 
These demonstrations are collected by an expert with the aid of a motion-planner to reach intermediate poses. Each demonstration $\zeta$ is a sequence of continuous actions $\mathcal{A} = \{a_{1}, a_{2}, \ldots, a_{t}\}$ paired with observations $\mathcal{O} = \{\Tilde{o}_{1}, \Tilde{o}_{2}, \ldots \Tilde{o}_{t}\}$. An action $a$ consists of the 6-DoF pose, gripper open state, and whether the motion-planner used collision avoidance to reach an intermediate pose: $a = \{a_{\textrm{pose}}, a_{\textrm{open}}, a_{\textrm{collide}}\}$. 
An observation $\Tilde{o}$ consists of RGB-D images from any number of cameras. We use four cameras for simulated experiments $\Tilde{o}_{\textrm{sim}} = \{o_{\textrm{front}}, o_{\textrm{left}}, o_{\textrm{right}}, o_{\textrm{wrist}} \}$, but just a single camera  for real-world experiments $\Tilde{o}_{\textrm{real}} = \{o_{\textrm{front}} \}$.  
\vspace{-0.2cm}
\subsection{Keyframes and Voxelization}
\label{sec:keyframe_and_voxel}
\vspace{-0.2cm}

Following prior work by James et al.~\citep{c2farm}, we construct a  structured observation and action space through keyframe extraction and voxelization.

Training our agent to directly predict continuous actions is inefficient and noisy. So instead, for each demonstration $\zeta$, we extract a set of keyframe actions $\{\mathbf{k}_{1}, \mathbf{k}_{2}, \ldots, \mathbf{k}_{m}\} \subset \mathcal{A}$ \highlight{that capture bottleneck end-effector poses}~\citep{johns2021coarse} in the action sequence with a simple heuristic: an action is a keyframe if (1) the joint-velocities are near zero and (2) the gripper open state has not changed. 
Each datapoint in the demonstration $\zeta$ can then be cast as a \highlight{``predict the next (best) keyframe action'' task~\citep{c2farm,armpaper,liu2022auto_lambda}}. See \appfigref{app:keypoints_and_demo} for an illustration of this process. 

To learn action-centric representations~\cite{gibson2014ecological} in 3D, we use a voxel grid~\citep{moravec1996robot,30724} to represent both the observation and action space. The observation voxels $\mathbf{v}$ are reconstructed from RGB-D observations $\Tilde{o}$ fused through triangulation $\Tilde{o} \Rightarrow \mathbf{v}$ from known camera extrinsics and intrinsics. By default, we use a voxel grid of $100^3$, which corresponds to a volume of $1.0\textrm{m}^3$ in metric scale. The keyframe actions $\mathbf{k}$ are discretized such that training our BC agent can be formulated as a \highlight{``next best action'' classification task~\citep{c2farm}}.
Translation is simply the closest voxel to the center of the gripper fingers. Rotation is discretized into 5 degree bins for each of the three rotation axes. Gripper open state is a binary value. Collide is also a binary value that indicates if the motion-planner should avoid everything in the voxel grid or nothing at all; switching between these two modes of collision avoidance is crucial as tasks often involve both contact based (\eg pulling the drawer open) and non-contact based motions (\eg reaching the handle without colliding into anything). 
\vspace{-0.2cm}

\subsection{\model~Agent} \label{sec:peract}
\vspace{-0.2cm}
\model~is a Transformer-based~\citep{vaswani2017attention} agent that takes in a voxel observation and language goal \inputvl, and outputs a discretized translation, rotation, and gripper open action. This action is executed with a motion-planner, after which this process is repeated until the goal is reached.

The language goal $\mathbf{l}$ is encoded with a pre-trained language model. 
We use CLIP's~\citep{radfordLearningTransferableVisual2021} language encoder, but any pre-trained language model would suffice~\citep{ahn2022can,lynch2020grounding}. Our choice of CLIP opens up possibilities for future work to use pre-trained vision features that are aligned with the language for better generalization to unseen semantic categories and instances~\citep{cliport}. 

The voxel observation $\mathbf{v}$ is split into 3D patches of size $5^3$ (akin to vision-transformers like ViT~\citep{dosovitskiy2020image}). In implementation, these  patches are extracted with a 3D convolution layer with a kernel-size and stride of 5, and then flattened into a sequence of voxel encodings. The language encodings are fine-tuned with a linear layer and then appended with the voxel encodings to form the input sequence. We also add learned positional embeddings to the sequence to incorporate voxel and token positions.

The input sequence of language and voxel encodings is extremely long. 
A standard Transformer with $\mathcal{O}(n^2)$ self-attention connections and an input of  $(100/5)^3 = 8000$ patches is hard to fit on the memory of a commodity GPU.  
Instead, we use the Perceiver~\citep{jaegle2021perceiver} Transformer. Perceiver is a latent-space Transformer, where instead of attending to the entire input, it first computes cross-attention between the input and a much smaller set of latent vectors (which are randomly initialized and trained). These latents are encoded with self-attention layers, and for the final output, the latents are again cross-attended with the input to match the input-size. See \appfigref{fig:perceiver_arch} for an illustration. By default, we use $2048$ latents of dimension 512 : $\mathbb{R}^{2048 \times 512}$, but in \appsecref{app:ablations} we experiment with different latent sizes. 

The Perceiver Transformer uses 6 self-attention layers to encode the latents and outputs a sequence of patch encodings from the output cross-attention layer. These patch encodings are upsampled with a 3D convolution layer and tri-linear upsampling to decode 64-dimensional voxel features. The decoder includes a skip-connection from the encoder (like in UNets~\citep{ronneberger2015u}). The per-voxel features are then used to predict discretized actions~\citep{c2farm}. For translation, the voxel features are reshaped into the original voxel grid ($100^3$) to form a 3D $\mathcal{Q}$-function of action-values. For rotation, gripper open, and collide, the features are max-pooled and then decoded with linear layers to form their respective $\mathcal{Q}$-function. The best action $\mathcal{T}$ is chosen by simply maximizing the $\mathcal{Q}$-functions:
\begin{align*}
\mathcal{T}_{\textrm{trans}} = \underset{(x,y,z)}{\argmax} \  \mathcal{Q}_{\textrm{trans}}((x,y,z) \ | \ \mathbf{v}, \mathbf{l} \,), \hspace{1cm}  &
\mathcal{T}_{\textrm{rot}} = \underset{(\psi,\theta,\phi)}{\argmax} \  \mathcal{Q}_{\textrm{rot}}((\psi,\theta,\phi) \ | \ \mathbf{v}, \mathbf{l} \,), \\
\mathcal{T}_{\textrm{open}} = \underset{\omega}{\argmax} \  \mathcal{Q}_{\textrm{open}}(\, \omega \ | \ \mathbf{v}, \mathbf{l} \,), \hspace{1cm}  &
\mathcal{T}_{\textrm{collide}} = \underset{\kappa}{\argmax} \ \mathcal{Q}_{\textrm{collide}}(\, \kappa \ | \ \mathbf{v}, \mathbf{l} \,),
\end{align*}
where $(x, y, z)$ is the voxel location in the grid, $(\psi, \theta, \phi)$ are discrete rotations in Euler angles, $\omega$ is the gripper open state and $\kappa$ is the collide variable. See \figref{fig:q_pred} for examples of  $\mathcal{Q}$-predictions.

\vspace{-0.1cm}
\subsection{Training Details}
\label{sec:training}
\vspace{-0.1cm}

\model~is trained through supervised learning with discrete-time input-action tuples from a dataset of demonstrations. These tuples are composed of voxel observations, language goals, and keyframe actions $\{(\mathbf{v}_{1}, \mathbf{l}_{1}, \mathbf{k}_{1}), (\mathbf{v}_{2}, \mathbf{l}_{2}, \mathbf{k}_{2}), \ldots\}$. During training, we randomly sample a tuple and supervise the agent to predict the keyframe action $\mathbf{k}$ given the observation and goal \inputvl. For translation, the ground-truth action is represented as a one-hot voxel encoding $Y_{\textrm{trans}} : \mathbb{R}^{H \times W \times D}$. Rotations are also represented with a one-hot encoding per rotation axis with $R$ rotation bins $Y_{\textrm{rot}} : \mathbb{R}^{(360/R) \times 3}$  ($R=5$ degrees for all experiments). Similarly, open and collide variables are binary one-hot vectors $Y_{\textrm{open}} : \mathbb{R}^{2}$, $Y_{\textrm{collide}} : \mathbb{R}^{2}$. The agent is trained with cross-entropy loss like a  classifier: 
\begin{equation*}
    \mathcal{L}_{\textrm{total}} = - \mathbb{E}_{Y_{\textrm{trans}}}[\textrm{log} \mathcal{V}_{\textrm{trans}}] - \mathbb{E}_{Y_{\textrm{rot}}}[\textrm{log} \mathcal{V}_{\textrm{rot}}] - \mathbb{E}_{Y_{\textrm{open}}}[\textrm{log} \mathcal{V}_{\textrm{open}}] -
    \mathbb{E}_{Y_{\textrm{collide}}}[\textrm{log} \mathcal{V}_{\textrm{collide}}],
\end{equation*}
where $\mathcal{V}_{\textrm{trans}} = \textrm{softmax}(\mathcal{Q}_{\textrm{trans}}((x,y,z) | \mathbf{v}, \mathbf{l}))$, $\mathcal{V}_{\textrm{rot}} = \textrm{softmax}(\mathcal{Q}_{\textrm{rot}}((\psi, \theta, \phi) | \mathbf{v}, \mathbf{l}))$, $\mathcal{V}_{\textrm{open}} = \textrm{softmax}(\mathcal{Q}_{\textrm{open}}(\omega | \mathbf{v}, \mathbf{l}))$, $\mathcal{V}_{\textrm{collide}} = \textrm{softmax}(\mathcal{Q}_{\textrm{collide}}(\kappa | \mathbf{v}, \mathbf{l}))$ respectively. For robustness, we also augment $\mathbf{v}$ and $\mathbf{k}$ with translation and rotation perturbations. See \appsecref{app:data_aug} for more details.

By default, we use a voxel grid size of $100^3$. We conducted validation tests by replaying expert demonstrations with discretized actions to ensure that $100^3$ is a sufficient resolution for execution. The agent was trained with a batch-size of 16 on 8 NVIDIA V100 GPUs for 16 days (600K iterations). We use the LAMB~\citep{you2019large} optimizer following Perceiver~\citep{jaegle2021perceiver}.

For multi-task training, we simply sample input-action tuples from all tasks in the dataset. To ensure that tasks with longer horizons are not over-represented during sampling, each batch contains a uniform distribution of tasks. That is, we first uniformly sample a set of tasks of batch-size length, then pick a random input-action tuple for each of the sampled tasks. With this strategy, longer-horizon tasks need more training steps for full coverage of input-action pairs, but all tasks are given equal weighting during gradient updates.

\section{Results}
\vspace{-0.2cm}
We perform experiments to answer the following questions: (1) How effective is \model~compared to unstructured image-to-action frameworks and standard architectures like 3D ConvNets? And what are the factors that affect \model's performance? (2) Is the global receptive field of Transformers actually beneficial over methods with local receptive fields? (3) Can \model~be trained on real-world tasks with noisy data?

\vspace{-0.2cm}
\subsection{Simulation Setup}
\vspace{-0.2cm}

We conduct our primary experiments in simulation for the sake of reproducibility and benchmarking.

\textbf{Environment.} The simulation is set in CoppelaSim~\citep{coppelasim} and interfaced through PyRep~\citep{james2019pyrep}. All experiments use a Franka Panda robot with a parallel gripper. The input observations are captured from four RGB-D cameras positioned at the front, left shoulder, right shoulder, and on the wrist, as shown in \appfigref{fig:sim_setup}. All cameras are noiseless and have a resolution of $128 \times 128$. 

\textbf{Language-Conditioned Tasks.}  \highlight{We train and evaluate on 18 RLBench~\citep{james2020rlbench} tasks. See \href{https://peract.github.io}{peract.github.io} for examples and \appsecref{app:task_details} for details on individual tasks. Each task includes several variations, ranging from 2-60 possibilities, \eg in the \texttt{stack blocks} task, \textit{``stack 2 red blocks''} and \textit{``stack 4 purple blocks''} are two variants}. These variants are randomly sampled during data generation, but kept consistent during evaluations for one-to-one comparisons. 
Some RLBench tasks were modified to include additional variations to stress-test multi-task and language-grounding capabilities. 
There are a total of 249 variations across 18 tasks, and the number of extracted keyframes range from 2-17.
\highlight{All keyframes from an episode have the same language goal, which is constructed from templates} (but human-annotated for real-world tasks). Note that in all experiments, we do not test for generalization to unseen objects, \ie~our train and test objects are the same. 
\highlight{However during test time, the agent has to handle novel object poses, randomly sampled goals, and randomly sampled scenes with different semantic instantiations of object colors, shapes, sizes, and categories}.
The focus here is to evaluate the performance of a single multi-task agent trained on all tasks and variants.

\textbf{Evaluation Metric.} Each multi-task agent is evaluated independently on all 18 tasks. Evaluations are scored either 0 for failures or 100 for complete successes. There are no partial credits. We report average success rates on 25 evaluation episodes per task ($25 \times18 = 450$ total episodes) for agents trained with $n=10,100$ demonstrations per task. During evaluation, an agent keeps taking actions until an oracle indicates task-completion or reaches a maximum of 25 steps.

\vspace{-0.2cm}
\subsection{Simulation Results}
\label{sec:sim_results}
\vspace{-0.1cm}

\tabref{table:rlbench} reports success rates of multi-task agents trained on all 18 tasks. 
We could not investigate single-task agents due to resource constraints of training 18 individual agents. 

\textbf{Baseline Methods.} We study the effectiveness of our problem formulation by \highlight{benchmarking against two language-conditioned baselines}: \bcz and C2FARM-BC.
\highlight{\bcz is an image-to-action agent similar to BC-Z~\citep{jang2022bc}}. Following BC-Z, \highlight{we use FiLM~\citep{perez2018film} for conditioning with   CLIP~\citep{radfordLearningTransferableVisual2021} language features}, but the vision encoders take in RGB-D images instead of just RGB. We also  study both CNN and ViT vision encoders. \unet is a 3D fully-convolutional network by James et al.~\citep{c2farm} that has achieved state-of-the-art results on RLBench tasks. Similar to our agent, \unet also detects actions in a voxelized space, however it uses a coarse-to-fine-grain scheme to detect actions at two-levels of voxelization: $32^3$ voxels with a $1^3$m grid, and $32^3$ voxels with a $0.15^3$m grid after ``zooming in'' from the first level. Note that at the finest level, \unet has a higher resolution ($0.47$cm) than \model~($1$cm). We use the same 3D ConvNet architecture as James et al.~\citep{c2farm}, but instead of training it with RL, we do BC with cross-entropy loss (from \secref{sec:training}). \highlight{We also condition it with CLIP~\citep{radfordLearningTransferableVisual2021} language features at the bottleneck like in LingUNets~\citep{misra2018mapping,cliport}}. 

\begin{table}[!t]
\centering
\scriptsize

\vspace{-1.0cm}
\hspace*{-1.02cm}
\begin{tabular}{lcccccccccccccccccc} 
\toprule
                  & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\texttt{open} \\\texttt{drawer}\end{tabular}}     & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\texttt{slide} \\\texttt{block}\end{tabular}} &  \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\texttt{sweep to} \\\texttt{dustpan}\end{tabular}}   &
                  \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\texttt{meat off}\\\texttt{grill}\end{tabular}} &
                  \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\texttt{turn} \\\texttt{tap}\end{tabular}}   & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\texttt{put in} \\\texttt{drawer}\end{tabular}}         & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\texttt{close}\\\texttt{jar}\end{tabular}}    &
                  \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\texttt{drag} \\\texttt{stick}\end{tabular}}    &
                  \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\texttt{stack}\\\texttt{blocks}\end{tabular}}  \\
                  \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13} 
                  \cmidrule(lr){14-15} \cmidrule(lr){16-17}
                  \cmidrule(lr){18-19}
                  \\[-13pt]                                                         \\
\vcell{Method}    & \vcell{10}       & \vcell{100}                                                & \vcell{10}       & \vcell{100}                                                      & \vcell{10}       & \vcell{100}                                                  & \vcell{10}       & \vcell{100}                                                 & \vcell{10}       & \vcell{100}                                           & \vcell{10}       & \vcell{100}                                                         & \vcell{10}       & \vcell{100}                                            & \vcell{10}       & \vcell{100}                                                  & \vcell{10}       & \vcell{100}                                             \\[-\rowheight]
\printcellbottom  & \printcellbottom & \printcellbottom                                           & \printcellbottom & \printcellbottom                                                 & \printcellbottom & \printcellbottom                                             & \printcellbottom & \printcellbottom                                            & \printcellbottom & \printcellbottom                                      & \printcellbottom & \printcellbottom                                                    & \printcellbottom & \printcellbottom                                       & \printcellbottom & \printcellbottom                                             & \printcellbottom & \printcellbottom                                        \\[0pt] 
\hline \\[-6pt]
\bczcnn         & 4                    & 4                                                      & 4                    & 0                                                            & 0                    & 0                                                        & 0                    & 0                                                          & 20                   & 8                                                & 0                    & 8                                                        & 0                    & 0                                                       & 0                    & 0                                                     & 0                    & 0                                                   \\
\highlight{\bczvit}   & 16                   & 0                                                  & 8                    & 0                                                  & 8                    & 0                                                       & 0                    & 0                                                     & 24                   & 16                                               & 0                    & 0                                                    & 0                    & 0                                                & 0                    & 0                                                 & 0                    & 0                                                   \\
\unet            & 28                   & 20                                                     & 12                   & 16                                                           & 4                    & 0                                                        & 40                   & 20                                                         & 60                   & 68                                               & 12                   & 4                                                        & 28          & 24                                                      & \textbf{72}          & 24                                                    & 4                    & 0                                                   \\
\model~(w/o Lang) & 20                   & 28                                                     & 8                    & 12                                                           & 20                   & 16                                                       & 40                   & 48                                                         & 36                   & 60                                               & 16                   & 16                                                       & 16                   & 12                                                      & 48                   & 60                                                    & 0                    & 0                                                   \\
\rowcolor[rgb]{0.9,1.0,0.9}\model           & \textbf{68}          & \textbf{80}                                            & \textbf{32}          & \textbf{72}                                                  & \textbf{72}          & \textbf{56}                                              & \textbf{68}          & \textbf{84}                                                & \textbf{72}          & \textbf{80}                                      & \textbf{16}          & \textbf{68}                                              & \textbf{32}          & \textbf{60}                                             & 36                   & \textbf{68}                                           & \textbf{12}          & \textbf{36}                                         \\[1pt]  
\hline \\
                  & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\texttt{screw}\\\texttt{bulb}\end{tabular}} & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\texttt{put in}\\\texttt{safe}\end{tabular}}      & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\texttt{place}\\\texttt{wine}\end{tabular}} & 
                  \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\texttt{put in}\\\texttt{cupboard}\end{tabular}} &
                  \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\texttt{sort}\\\texttt{shape}\end{tabular}} & 
                  \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\texttt{push}\\\texttt{buttons}\end{tabular}}& \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\texttt{insert}\\\texttt{peg}\end{tabular}} &
                  \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\texttt{stack}\\\texttt{cups}\end{tabular}}         & \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}\texttt{place}\\\texttt{cups}\end{tabular}}    \\
        
                  \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13} 
                  \cmidrule(lr){14-15} \cmidrule(lr){16-17}
                  \cmidrule(lr){18-19}
                  \\[-6pt]          
\vcell{}          & \vcell{10}       & \vcell{100}                                                & \vcell{10}       & \vcell{100}                                                      & \vcell{10}       & \vcell{100}                                                  & \vcell{10}       & \vcell{100}                                                 & \vcell{10}       & \vcell{100}                                           & \vcell{10}       & \vcell{100}                                                         & \vcell{10}       & \vcell{100}                                            & \vcell{10}       & \vcell{100}                                                  & \vcell{10}       & \vcell{100}                                             \\[-\rowheight]
\printcellbottom  & \printcellbottom & \printcellbottom                                           & \printcellbottom & \printcellbottom                                                 & \printcellbottom & \printcellbottom                                             & \printcellbottom & \printcellbottom                                            & \printcellbottom & \printcellbottom                                      & \printcellbottom & \printcellbottom                                                    & \printcellbottom & \printcellbottom                                       & \printcellbottom & \printcellbottom                                             & \printcellbottom & \printcellbottom                                        \\[1pt]
\hline \\[-6pt]
\bczcnn          & 0                    & 0                                                      & 0                    & 4                                                            & 0                    & 0                                                        & 0                    & 0                                                          & 0                    & 0                                                & 4                    & 0                                                        & 0                    & 0                                                       & 0                    & 0                                                     & 0                    & 0                                                   \\
\highlight{\bczvit}   & 0                    & 0                                                  & 0                    & 0                                                  & 4                    & 0                                                       & 4                    & 0                                                     & 0                    & 0                                                & 16                   & 0                                                    & 0                    & 0                                                & 0                    & 0                                                 & 0                    & 0                                                   \\
\unet             & 12                   & 8                                                      & 0                    & 12                                                           & \textbf{36}          & 8                                                        & \textbf{4}           & 0                                                          & 8           & 8                                                & \textbf{88}          & \textbf{72}                                              & 0                    & \textbf{4}                                                       & 0                    & 0                                                     & 0                    & 0                                                   \\
\model~(w/o Lang) & 0                    & \textbf{24}                                            & 8                    & 20                                                           & 8                    & \textbf{20}                                                       & 0                    & 0                                                          & 0                    & 0                                                & 60                   & 68                                                       & 4                    & 0                                                       & 0                    & 0                                                     & 0                    & 0                                                   \\
\rowcolor[rgb]{0.9,1.0,0.9}\model~ & \textbf{28}          & \textbf{24}                                            & \textbf{16}          & \textbf{44}                                                  & 20                   & 12                                                       & 0                    & \textbf{16}                                                & \textbf{16}          & \textbf{20}                                      & 56                   & 48                                                       & \textbf{4}           & 0                                                       & 0                    & 0                                                     & 0                    & 0                                                   \\[-1pt]
\bottomrule
\end{tabular}
\vspace{2pt}
\caption{\textbf{Multi-Task Test Results.} Success rates (mean \%) of various multi-task agents tasks trained with either 10 or 100 demonstrations per task and evaluated on 25 episodes per task. Each evaluation episode is scored either a 0 for failure or 100 for succces. \model~outperforms \unet\citep{c2farm}, \highlight{the most competitive baseline, with an average improvement of $1.33\times$ with 10 demos and $2.83\times$ with 100 demos.}}
\vspace{-0.8cm}
\label{table:rlbench}
\end{table}
\textbf{Multi-Task Performance.} \tabref{table:rlbench} compares the performance of \bcz and \unet against \model. \highlight{With insufficient demonstrations, \bcz has near zero performance on most tasks. \bcz is disadvantaged with single-view observations and has to learn hand-eye coordination from scratch. In contrast, \model's voxel-based formulation naturally allows for integrating multi-view observations, learning 6-DoF action representations, and data-augmentation in 3D, all of which are non-trivial to achieve in image-based methods}. \unet is the most competitive baseline, but it has a limited receptive field mostly because of the coarse-to-fine-grain scheme and partly due to the convolution-only architecture. \model~outperforms \unet in $25/36$~evaluations in \tabref{table:rlbench} \highlight{with \textbf{an average improvement of} $\mathbf{1.33\times}$ \textbf{with 10 demonstrations and} $\mathbf{2.83\times}$\textbf{ with 100 demonstrations}}. For a number of tasks, \unet actually performs worse with more demonstrations, likely due to insufficient capacity. Since additional training demonstrations include additional task variants to optimize for, they might end up hurting performance. 

\vspace{-0.05cm}
In general, 10 demonstrations are sufficient for \model~to achieve $>65\%$ success on tasks with limited variations like \texttt{open drawer} (3 variations). But tasks with more variations like \texttt{stack blocks} (60 variations) need substantially more data, sometimes to simply cover all possible concepts like ``\textit{teal color block}'' that might have not appeared in the training data. See the simulation rollouts in the supplementary video to get a sense of the complexity of these evaluations. For three tasks: \texttt{insert peg}, \texttt{stack cups}, and \texttt{place cups}, all agents achieve near zero success. These are  very high-precision tasks where being off by a few centimeters or degrees could lead to unrecoverable failures. \highlight{But in \appsecref{app:high_pres} we find that training single-task agents, specifically for these tasks, slightly alleviates this issue.}

\begin{wrapfigure}{r}{0.4\textwidth}
  
  \begin{center}
    \vspace{-0.8cm}
    \includegraphics[width=0.42\textwidth]{figures/ablation_results.pdf}

  \vspace{-0.05cm}
  \caption{\textbf{Ablation Experiments.} Success rate of \model~after ablating key components.}
  \label{fig:ablations}
  \end{center}
  \vspace{-1em}
\end{wrapfigure}
\vspace{-0.05cm}
\textbf{Ablations.} \tabref{table:rlbench} reports \model~w/o Lang, an agent without any language conditioning. Without a language goal, the agent does not know the underlying task and performs at chance. We also report additional ablation results on the \texttt{open drawer} task in \figref{fig:ablations}. To summarize these results: (1) the skip connection helps train the agent slightly faster, (2) the Perceiver Transformer is crucial for achieving good performance with the global receptive field, and (3) extracting good keyframes actions is essential for supervised training as randomly chosen or fixed-interval keyframes lead to zero-performance.

\begin{wrapfigure}{r}{0.4\textwidth}
    \vspace{-1.63cm}
  \begin{center}
    \includegraphics[width=0.55\textwidth]{figures/recep_results.pdf}
  \end{center}
  \vspace{-1.2em}
  \caption{\textbf{Global vs. Local Receptive Field Experiments.} Success rates of \model~against various \unet~\citep{c2farm} baselines}
  \label{fig:recep_results}
  \vspace{-0.5cm}
\end{wrapfigure}
\textbf{Sensitivity Analysis.} In \appsecref{app:ablations} we investigate factors that affect \model's performance: the number of Perceiver latents, voxelization resolution, and data augmentation. We find that more latent vectors generally improve the capacity of the agent to model more tasks, but for simple short-horizon tasks, fewer latents are sufficient. Similarly, with different voxelization resolutions, some tasks are solvable with coarse voxel grids like $32^3$, but some high-precision tasks require the full $100^3$ grid. Finally, rotation perturbations in the data augmentation generally help in improving robustness essentially by exposing the agent to more rotation variations of objects.

\begin{figure*}[!t]
    \centering
    \vspace{-1.2cm}
    \hspace*{-1.5cm}
    \includegraphics[width=1.2\textwidth]{figures/q_pred_v3.pdf}
    \caption{\textbf{Q-Prediction Examples}: Qualitative examples of translation $\mathcal{Q}$-Predictions from \model~along with expert actions, highlighted with dotted-circles. The left two are simulated tasks, and the right two are real-world tasks. See \appsecref{app:more_qpred} for more examples.}
    \label{fig:q_pred} 
    \vspace{-1em}
\end{figure*}

\subsection{Global vs. Local Receptive Fields}
To further investigate our Transformer agent's global receptive field, we conduct additional experiments on the \texttt{open drawer} task. 
The \texttt{open drawer} task has three variants: \textit{``open the top drawer''}, \textit{``open the middle drawer''}, and \textit{``open the bottom drawer''}, and with a limited receptive field it is hard to distinguish the drawer handles, which are all visually identical. 
\figref{fig:recep_results} reports \model~and \unet agents trained with 100 demonstrations. Although the \texttt{open drawer} tasks can be solved with fewer demonstrations, here we want to ensure that insufficient data is not an issue. We include several versions of \unet with different voxelization schemes. For instance, $[16,16]$ indicates two levels of $16^3$ voxel grids at $1\textrm{m}^3$ and $0.15\textrm{m}^3$, respectively. And $[64]$ indicates a single level of a $64^3$ voxel grid without the coarse-to-fine-grain scheme. \model~is the only agent that achieves $>70\%$ success, whereas all \unet versions perform at chance with $\sim 33\%$, indicating that the global receptive field of the Transformer is crucial for solving the task. 

\begin{wraptable}{r}{0.3\textwidth}
  \vspace{-1.3em}
  \setlength\tabcolsep{2.3pt}
  \centering
  \scriptsize
\begin{tabular}{lccc} 
\toprule
Task          & \# Train~ & \# Test & Succ. \%  \\ 
\midrule
Press Handsan & 5         & 10      & 90        \\
Put Marker    & 8         & 10      & 70        \\
Place Food    & 8         & 10      & 60        \\
Put in Drawer & 8         & 10      & 40        \\
Hit Ball      & 8         & 10      & 60        \\
Stack Blocks  & 10        & 10      & 40        \\
Sweep Beans   & 8         & 5       & 20        \\
\bottomrule
\end{tabular}
    \caption{\scriptsize{Success rates (mean \%) of a multi-task model trained an evaluated 7 real-world tasks (see \figref{fig:tasks}).}} %
  \vspace{-2em}
  \label{table:real}
\end{wraptable}\subsection{Real-Robot Results} \label{sec:real_robot_results}
We also validated our results with real-robot experiments on a Franka Emika Panda. See  \appsecref{app:robot_setup} for setup details. Without any sim-to-real transfer or pre-training, we trained a multi-task \model~agent \textit{from scratch }on 7 tasks (with 18 unique variations) from a total of just 53 demonstrations. See the supplementary video for qualitative results that showcase the diversity of tasks and robustness to scene changes. \tabref{table:real} reports success rates from small-scale evaluations. Similar to the simulation results, we find that \model~is able to achieve $>65\%$ success on simple short-horizon tasks like pressing hand-sanitizers from just a handful number of demonstrations. The most common failures involved predicting incorrect gripper open actions, which often lead the agent into unseen states. This could be addressed in future works by using HG-DAgger style approaches to correct the agent~\citep{jang2022bc}. Other issues included the agent exploiting biases in the dataset like in prior work~\citep{cliport}. This could be addressed by scaling up expert data with more diverse tasks and task variants.  

\vspace{-0.1cm}
\section{Limitations and Conclusion}
\vspace{-0.1cm}
We presented \model, a Transformer-based multi-task agent for 6-DoF manipulation. Our experiments with both simulated and real-world tasks indicate that the right problem formulation, \ie~detecting voxel actions, makes a substantial difference in terms of data efficiency and robustness. 

While \model~is quite capable, extending it to dexterous continuous control remains a challenge. \model~is at the mercy of a sampling-based motion-planner to execute discretized actions, and is not easily extendable to N-DoF actuators like multi-fingered hands. See \appsecref{app:limitations} for an extended discussion on \model's limitations.
But overall, we are excited about scaling up robot learning with Transformers by focusing on \textit{diverse} rather than narrow multi-task data for robotic manipulation. 

\acknowledgments{We thank Selest Nashef and Karthik Desingh for their help with the Franka setup at UW. We thank Stephen James for helping with RLBench and ARM issues. We are also grateful to Valts Blukis, Zoey Chen, Markus Grotz, Aaron Walsman, and Kevin Zakka, for providing feedback on the initial draft. And thanks to Shikhar Bahl for initial discussions. This work was funded in part by ONR under award \#1140209-405780. Mohit Shridhar is supported by the NVIDIA Graduate Fellowship, and was also a part-time intern at NVIDIA throughout the duration of this project.}

\newpage

\end{document}