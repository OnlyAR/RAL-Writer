\title{Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations}

\begin{document}

\maketitle
\begin{abstract}
The collection and curation of high-quality training data is crucial for developing text classification models with superior performance, but it is often associated with significant costs and time investment. 
Researchers have recently explored using large language models (LLMs) to generate synthetic datasets as an alternative approach. However, the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks. 
To better understand factors that moderate the effectiveness of the LLM-generated synthetic data, in this study, we look into how the performance of models trained on these synthetic data may vary with the {\em subjectivity} of classification. 
Our results indicate that subjectivity, at both the task level and instance level, is negatively associated with the performance of the model trained on synthetic data.  
We conclude by discussing the implications of our work on the potential and limitations of leveraging LLM for synthetic data generation\footnote{The collected human annotations are available at \\huggingface.co/datasets/xfleezy/human\_annotation\_emnlp23.}. 
\end{abstract}

\section{Introduction}
Today, machine-learning-powered text classification models have been widely applied in diverse applications such as detecting biased or toxic language on online platforms~\cite{wiegand2019detection} and filtering spam emails~\cite{jindal2007review}. However, the performance of these models largely depends on the quality of the training data. This poses a substantial challenge in practice, especially when models need to be built for a novel task domain or to incorporate new classification categories, as the training data collection and curation process is often costly, time-consuming, and complex. 

Meanwhile, with the recent advancements in large language models (LLMs), researchers have started to explore the potential of utilizing LLMs for generating synthetic data tailored to specific tasks and augmenting the training data in low-resourced data settings~\cite{kumar2020data,yoo2021gpt3mix,hartvigsen2022toxigen,sahu2022data}.
Most recently, a few studies also investigate into the feasibility of generating a synthetic dataset from scratch using LLMs to support zero-shot learning~\cite{ye2022zerogen,wang2021towards,tang2023does,gao2023self}. 
While LLM-based data augmentation is often found to outperform other data augmentation methods in boosting the model performance, mixed results are reported regarding whether the LLM-generated synthetic data can effectively support model training to enable a level of model performance that is comparable to models trained on the data collected in the real world and carefully annotated. This leaves uncertainty for researchers and practitioners in deciding whether to rely on LLMs for synthetic data generation or to proceed with the traditional data collection and curation pipeline when they need to construct a text classification model for a new task. Naturally, one may wonder {\em what factors might moderate the effectiveness of LLM-generated synthetic data in facilitating successful model training}. 

We conjecture that one such factor could be the {\em subjectivity} of classification tasks. Indeed, language is inherently subjective and interpretive~\cite{benveniste1971subjectivity,wiebe2004learning}. Previous research has showed that people often perceive the same text in different ways because of their personal biases and perspectives~\cite{sap2021annotators,li2022towards,gordon2022jury}. Thus, achieving high model performance for classification tasks with high subjectivity seems to impose a greater demand on the training data in reflecting the richness and nuances present in human language, and the extent to which LLM-generated synthetic data can acompolish this objective is unclear.   

Thus, in this paper, we formally evaluate the effectiveness of LLM (i.e., the cutting-edge GPT-3.5-Turbo model) in generating synthetic data to support model training for different text classification tasks. We adopt two approaches for synthetic data generation---a {\em zero-shot} setting in which the LLM is directly prompted to generate text instances with different labels of interests, and a {\em few-shot} setting in which a few real-world data instances are provided as examples to guide the LLM in generating the synthetic data. We conduct two evaluation studies, each corresponding to one dimension of subjectivity---the first study examines the effectiveness of the synthetic data on 10 types of classification tasks and explores how it varies with the {\em task-level subjectivity} (i.e., whether this type of classification task is subjective); the second study concerns that given a specific classification task, how the performance of a model trained on synthetic data changes with the {\em instance-level subjectivity} (i.e., whether people tend to disagree with each other on the label of this task instance).  Our findings suggest that across the 10 types of classification tasks that we have considered in this study, models trained on the LLM-generated synthetic data generally perform worse than those trained on the real-world data, yet guiding LLM's synthetic data generation process with a small amount of real-world data (i.e., as done in the few-shot data generation setting) can improve the effectiveness of the data generated. 
Moreover, we find that the performance of models trained on the LLM-generated synthetic data is very close to those trained on the real-world data for tasks with low subjectivity (e.g., news topic classification, spam email detection), while the performance decrease is much bigger on tasks with high subjectivity (e.g., humor or sarcasm detection). Finally, even within the same type of classification task, models trained on the LLM-generated synthetic data tend to exhibit a higher level of performance on those task instances with lower subjectivity, for which human annotators exhibit a higher level of agreement in their annotation. 

Together, our study provides important experimental evidence regarding the potential and limitations of using LLMs to generate synthetic data for text classification tasks. We conclude by discussing the implications, limitations, and future work of our study. 

\section{Related Work}
\noindent \textbf{Generative AI in synthetic data generation.} Recent advancements in generative AI have motivated numerous studies to explore the potential of leveraging generative models to create synthetic data for training machine learning models, especially for computer vision (CV) and natural language processing (NLP) tasks. In the realm of CV, several works have utilized GAN-based models~\cite{karras2019style} or diffusion models~\cite{nichol2021glide} to generate synthetic data for image recognition~\cite{besnier2020dataset,he2022synthetic} or object segmentation~\cite{zhang2021datasetgan}. Similarly, in the NLP field, researchers have also probed into the capacity of language models in generating synthetic data for various text classification tasks~\cite{kumar2020data,chung2023increasing, sahu2022data, yoo2021gpt3mix, ye2022zerogen,wang2021towards,hartvigsen2022toxigen,meng2022generating,gao2022self,aggarwal2022entity, chen2022weakly}, with mixed results reported regarding the effectiveness of the synthetic data generated.  
In this study, we aim to obtain a better understanding of {\em when} the synthetic data generated by language models can lead to effective model training, and we focus on exploring the role of task subjectivity in moderating the effectiveness of the synthetic data.  

\noindent \textbf{Large language models.}
Based on the Transformer architecture~\cite{vaswani2017attention}, large language models (LLMs) have facilitated remarkable progress in the field of natural language processing. The utilization of bidirectional contexts in the BERT model~\cite{devlin2018bert} has resulted in superior performance across a wide range of tasks. Building on this, OpenAI's GPT series, comprising of models like GPT-2~\cite{radford2019language}, the colossal GPT-3~\cite{brown2020language} with an impressive 175 billion parameters and the most recent GPT-4~\cite{openai2023gpt4}, pushed the boundaries of possibilities of LLMs. These models exhibit remarkable proficiency in generating high-quality human-like text~\cite{clark2021all,dou2021gpt,10.1145/3544548.3581318}, showcasing capabilities in rudimentary reasoning~\cite{wei2021finetuned}, translation~\cite{brown2020language}, scientific synthetic data generation~\cite{10.1145/3544548.3580688}, and code generation~\cite{10.1145/3544548.3580940}.  
In this study, we focus on leveraging the cutting-edge GPT-3.5-Turbo model\footnote{We used GPT-3.5-Turbo as the foundational model to generate synthetic data because at the time of this study, an official API for the more advanced GPT-4 model was not yet available from OpenAI.} to explore its capabilities and limitations in synthesizing data for text classification tasks with different subjectivity levels.

\section{Methodolgy}
In this section, we outline the procedure we have followed when leveraging the large language model  to generate the synthetic training data for text classification. We consider two data generation settings in this study, i.e., the \textit{zero-shot} setting and the \textit{few-shot} setting.

\subsection{Zero-shot Synthetic Data Generation}
\label{zero-shot}
Under the {\em zero-shot} synthetic data generation setting, given a text classification task, we assume that the real-world data in the form of ``text-label pairs'' do not exist. 
Thus, in order to obtain synthetic training data for the text classification task, two sequential prompts are constructed and supplied to the pretrained large language model (i.e., the GPT-3.5-Turbo model). First, a customized ``context prompt'' relevant to the targeted domain of interest is used to set the context. For example, in the case of the IMDB movie review classification task~\cite{maas-EtAl:2011:ACL-HLT2011}, the customized context prompt used is ``Imagine you are a movie reviewer on the IMDB platform''. This prompt aims to encourage the LLM to generate synthetic data that resemble the real texts produced in the targeted domain. 
After the context is set, a second prompt, i.e., the ``data generation prompt'', is provided to the LLM, instructing the model to generate texts with a specific style, label (with respect to the classification task of interest), and word limit. 
For example, for the IMDB movie review classification task, the style of the text is a movie review, and the label is a targeted sentiment conveyed by the review (i.e., ``positive'' or ``negative''). 
To further enhance the diversity of the generated data, after the generation of every $n$ data points (i.e., texts of targeted styles, labels, and word limits)\footnote{To increase data diversity while maintaining a reasonable data generation speed, $n$ is set to 10 for generating short texts (i.e., texts with a maximum length of 30 words), and 1 for generating longer paragraphs. 
},  %\hx{For tasks with data content as short sentences, we will request diversity every tenth time. For data with longer paragraphs, we ask for diversity every time. (Since the GPT API needs to manually return previous conversation records to the model as prompts, ten short sentences/one long paragraph is more appropriate as an interval. Too long a chat history can significantly slow down the generation speed, lead to extremely high Rate Limit Error probability, and result in higher costs)} iterations \my{What does ``several'' mean? Do you have a specific frequency?}, 
we provide a ``diversity prompt'' to the LLM---``Can you provide something more diverse compared to the previously generated data?''---aiming to increase the diversity of the synthetic data generated.

\subsection{Few-shot Synthetic Data Generation}
\label{few-shot}
Under the {\em few-shot} synthetic data generation setting, we assume that a small amount of %real data with golden labels 
real-world data 
are available for the text classification task. These data points can then serve as the examples for the large language model in the data generation process, which can potentially provide LLM with insights of the patterns exhibited in the real-world data.  We again start the data generation process by using a context prompt to set the context. However, different from that in the zero-shot setting, here, each time before we instruct the LLM to generate a piece of text, we first provide the model with a few randomly sampled real-world data instances (including both the text and the label) as the examples. 
To keep the LLM from merely rephrasing the provided examples, an additional prompt is used to impose a constraint on the LLM in generating the synthetic data (i.e., ``You should imitate the example I have provided, but you cannot simply modify or rewrite the example I have given.''). 

For more details about prompts used for generating data for each type of text classification task, please refer to the App.~\ref{prompts}.
\section{Evaluation \RNum{1}: Comparison Across Different Types of Tasks}
In our first evaluation study, we investigate into how well the synthetic data generated by LLM under both zero-shot and few-shot settings can support effective model training for different types of text classification tasks. 
We are especially interested in comparing the model performance between those trained on the real-world data and on the LLM-generated synthetic data, and in understanding how the performance of those models trained on the LLM-generated synthetic data varies with the subjectivity of the text classification task.
\subsection{Datasets and Tasks}
We experiment  with 10 representative datasets covering a variety of text classification tasks: AG's news \citep{Zhang2015CharacterlevelCN}, IMDB reviews \citep{maas-EtAl:2011:ACL-HLT2011}, 
SMS  spam~\citep{Almeida2011SpamFiltering}, Financial phrase bank~\citep{Malo2014GoodDO},
Reddit emotion~\citep{demszky2020goemotions}, %\my{Missing citation} \hx{Added},
Relation classification~\citep{gao-etal-2019-fewrel}, Tweet irony speech~\citep{van2018semeval}, 
Tweet emotions~\citep{mohammad2018semeval}, 
Sarcasm news (\citealp{misra2023Sarcasm}, \citealp{misra2021sculpting}), and 
Humor speech~\citep{annamoradnejad2020colbert}. 
See App.~\ref{dataset} for detailed descriptions of datasets and the corresponding text classification tasks.
These datasets are selected with the goal of spanning a wide range of task subjectivity in mind. For example, we conjecture that classifying the news topic category (e.g., as that in the AG's news dataset) is relatively objective, while determining whether texts are humorous (e.g., as that in the Humor speech dataset) is quite subjective~\cite{veatch1998theory}. 

\subsection{Task-level Subjectivity Determination}
\label{subrank}

To formally determine the subjectivity levels of different text classification tasks, we first conduct a crowdsourced study to collect subjectivity judgements from the crowd.

\noindent \textbf{Study procedure.}
We adopt a comparative approach to collect crowdsourced subjectivity judgements in this study. 
Specifically, 
we recruited crowd workers from Amazon Mechanical Turk (MTurk), and each worker was asked to complete a sequence of 10 subjectivity judgement tasks. 
In each task, we randomly sampled a pair of text classification tasks from the 10 tasks that we considered in this evaluation, and we presented to the worker the task description, label description, and task examples for each task in the pair. 
Then, the worker was asked to determine which text classification task in the pair was more objective, with ``objectivity'' of a task defined as ``the classification of a piece of text is based on clear, identifiable features in the text (e.g., keywords or phrases), and can be done without being affected by any personal interpretation of the text resulted from personal biases, emotions or beliefs.''  The study was restricted to U.S. workers. Each worker was allowed to participate only once and received a \$1.2 payment. An attention check question was included in the study to validate the worker's engagement, and only the data from workers who successfully passed the attention check were considered valid.

\noindent \textbf{Ranking task subjectivity.}
After excluding responses from inattentive workers, a total of 540 pairwise subjectivity comparisons for the 10 tasks were obtained from 54 workers. For each pair of tasks, we aggregated relative subjectivity judgments made on this pair to determine which task was perceived as more subjective (i.e., less objective). To produce a ranking of the  subjectivity of the 10 tasks, we constructed a directed graph based on the pairwise subjectivity comparisons---each task was a node in this graph, and directed edges were added between each pair of tasks, pointing from the one that was deemed as more subjective (on the aggregate level) to the one deemed as less subjective. 
The topological sort algorithm~\cite{cormen2022introduction} was then applied to this directed graph to obtain a linear ordering of the nodes. If a cycle was detected within the graph, the corresponding tasks were considered to have the same level of subjectivity
and were merged into a single meta-node before re-runing the algorithm. 
Our final task subjectivity ranking results are shown in  Table~\ref{tab:results}.

\subsection{Model Training}
Given a text classification task, following the procedures outlined in Sections~\ref{zero-shot} and~\ref{few-shot}, 3,000 synthetic data points were generated for each candidate label under both zero-shot and few-shot settings. 
We then trained classification models using the real-world training data provided by the original dataset, the synthetic data generated under the zero-shot settings, and the synthetic data generated under the few-shot settings\footnote{Under the few-shot setting, we randomly sampled $10\%$ of the data points from the real-world training data provided in the original dataset as the example pool to guide the LLM's synthetic data generation process, but only the sythetic data generated were used to train the models.}, respectively. 
Specifically, we utilized the pre-trained BERT~\cite{devlin2018bert} and RoBERTa~\cite{liu2019roberta} models from Huggingface's transformers library~\cite{wolf-etal-2020-transformers} as the encoders, and used the representation embeddings from the last layer of these models as the input to our classification models. The classification model itself comprised a hidden layer of 768 units and an output layer, and it was fine-tuned with a learning rate of $5e-5$ and a batch size of 64. For datasets that provided official partitions for training and test sets, we directly evaluated the classification model's performance on the test sets. Otherwise, we randomly divided the dataset into training (70\%), validation (5\%), and test (25\%) sets\footnote{To ensure a fair comparison, we maintained an equal size for both the real-world and synthetic training data by downsampling the dataset with a larger size. 

}.  
Models' performance was evaluated via Macro-F1 and Accuracy scores, and they were computed by comparing the model's predictions with the gold labels provided in the test sets. 
To ensure the robustness of our results, all experiments were repeated three times, and the average performance across these repetitions was reported. 

\begin{table*}[t]
  \centering
  \resizebox{\textwidth}{!}{
  \begin{tblr}{
    row{2} = {c},
    row{3} = {c},
    column{2} = {l},
    cell{1}{1} = {r=3}{},
    cell{1}{2} = {r=3}{},
    cell{1}{3} = {c=6}{c},
    cell{1}{9} = {c=6}{c},
    cell{2}{3} = {c=2}{},
    cell{2}{5} = {c=2}{},
    cell{2}{7} = {c=2}{},
    cell{2}{9} = {c=2}{},
    cell{2}{11} = {c=2}{},
    cell{2}{13} = {c=2}{},
    cell{4}{3} = {c},
    cell{4}{4} = {c},
    cell{4}{5} = {c},
    cell{4}{6} = {c},
    cell{4}{7} = {c},
    cell{4}{8} = {c},
    cell{4}{9} = {c},
    cell{4}{10} = {c},
    cell{4}{11} = {c},
    cell{4}{12} = {c},
    cell{4}{13} = {c},
    cell{4}{14} = {c},
    cell{5}{3} = {c},
    cell{5}{4} = {c},
    cell{5}{5} = {c},
    cell{5}{6} = {c},
    cell{5}{7} = {c},
    cell{5}{8} = {c},
    cell{5}{9} = {c},
    cell{5}{10} = {c},
    cell{5}{11} = {c},
    cell{5}{12} = {c},
    cell{5}{13} = {c},
    cell{5}{14} = {c},
    cell{6}{3} = {c},
    cell{6}{4} = {c},
    cell{6}{5} = {c},
    cell{6}{6} = {c},
    cell{6}{7} = {c},
    cell{6}{8} = {c},
    cell{6}{9} = {c},
    cell{6}{10} = {c},
    cell{6}{11} = {c},
    cell{6}{12} = {c},
    cell{6}{13} = {c},
    cell{6}{14} = {c},
    cell{7}{3} = {c},
    cell{7}{4} = {c},
    cell{7}{5} = {c},
    cell{7}{6} = {c},
    cell{7}{7} = {c},
    cell{7}{8} = {c},
    cell{7}{9} = {c},
    cell{7}{10} = {c},
    cell{7}{11} = {c},
    cell{7}{12} = {c},
    cell{7}{13} = {c},
    cell{7}{14} = {c},
    cell{8}{3} = {c},
    cell{8}{4} = {c},
    cell{8}{5} = {c},
    cell{8}{6} = {c},
    cell{8}{7} = {c},
    cell{8}{8} = {c},
    cell{8}{9} = {c},
    cell{8}{10} = {c},
    cell{8}{11} = {c},
    cell{8}{12} = {c},
    cell{8}{13} = {c},
    cell{8}{14} = {c},
    cell{9}{3} = {c},
    cell{9}{4} = {c},
    cell{9}{5} = {c},
    cell{9}{6} = {c},
    cell{9}{7} = {c},
    cell{9}{8} = {c},
    cell{9}{9} = {c},
    cell{9}{10} = {c},
    cell{9}{11} = {c},
    cell{9}{12} = {c},
    cell{9}{13} = {c},
    cell{9}{14} = {c},
    cell{10}{3} = {c},
    cell{10}{4} = {c},
    cell{10}{5} = {c},
    cell{10}{6} = {c},
    cell{10}{7} = {c},
    cell{10}{8} = {c},
    cell{10}{9} = {c},
    cell{10}{10} = {c},
    cell{10}{11} = {c},
    cell{10}{12} = {c},
    cell{10}{13} = {c},
    cell{10}{14} = {c},
    cell{11}{3} = {c},
    cell{11}{4} = {c},
    cell{11}{5} = {c},
    cell{11}{6} = {c},
    cell{11}{7} = {c},
    cell{11}{8} = {c},
    cell{11}{9} = {c},
    cell{11}{10} = {c},
    cell{11}{11} = {c},
    cell{11}{12} = {c},
    cell{11}{13} = {c},
    cell{11}{14} = {c},
    cell{12}{3} = {c},
    cell{12}{4} = {c},
    cell{12}{5} = {c},
    cell{12}{6} = {c},
    cell{12}{7} = {c},
    cell{12}{8} = {c},
    cell{12}{9} = {c},
    cell{12}{10} = {c},
    cell{12}{11} = {c},
    cell{12}{12} = {c},
    cell{12}{13} = {c},
    cell{12}{14} = {c},
    cell{13}{3} = {c},
    cell{13}{4} = {c},
    cell{13}{5} = {c},
    cell{13}{6} = {c},
    cell{13}{7} = {c},
    cell{13}{8} = {c},
    cell{13}{9} = {c},
    cell{13}{10} = {c},
    cell{13}{11} = {c},
    cell{13}{12} = {c},
    cell{13}{13} = {c},
    cell{13}{14} = {c},
    vline{2-4,9} = {1-3}{},
    vline{4,6,10,12} = {2}{dashed},
    vline{8} = {2}{},
    vline{4,6,8,10,12,14} = {3-13}{dotted},
    vline{5,7,11,13} = {2-13}{dashed},
    vline{9} = {3}{},
    vline{2-3,9} = {4-13}{},
    hline{1,4,14} = {-}{},
    hline{2-3} = {3-14}{},
  }
  \textbf{Dataset}                  & \textbf{Subjectivity} & \textbf{BERT}           &                         &                                       &                                       &                                       &                                       & \textbf{RoBERTa}        &                         &                                       &                                       &                                       &                                       \\
                                    &                             & \textbf{Real-world data}       &                         & \textbf{Zero-shot setting}                    &                                       & \textbf{Few-shot setting}       &                                       & \textbf{Real-world data}       &                         & \textbf{Zero-shot setting}                    &                                       & \textbf{Few-shot setting}       &                                       \\
                                    &                             & \textbf{Macro-F1 } & \textbf{Accuracy Score} & \textbf{Macro-F1 }               & \textbf{Accuracy Score}               & \textbf{Macro-F1 }               & \textbf{Accuracy Score}               & \textbf{Macro-F1} & \textbf{Accuracy Score} & \textbf{Macro-F1}               & \textbf{Accuracy Score}               & \textbf{Macro-F1}               & \textbf{Accuracy Score}               \\
  AG                         & \OB                         & 95.3\%                  & 95.3\%                  & 89.3\% \textcolor{pf1color}{(-6.0\%)}  & 89.3\% \textcolor{pf1color}{(-6.0\%)}  & 91.5\% \textcolor{pf1color}{(-3.8\%)}  & 91.6\% \textcolor{pf1color}{(-3.7\%)}  & 94.6\%                  & 94.6\%                  & 88.6\% \textcolor{pf1color}{(-6.0\%)}  & 88.6\% \textcolor{pf1color}{(-6.0\%)}  & 92.9\% \textcolor{pf1color}{(-1.7\%)}  & 92.9\% \textcolor{pf1color}{(-1.7\%)}  \\
  Relation                    & \OB\OB                      & 98.6\%                  & 98.6\%                  & 92.4\% \textcolor{pf1color}{(-6.2\%)}  & 92.7\% \textcolor{pf1color}{(-5.9\%)}  & 96.4\% \textcolor{pf1color}{(-2.2\%)}  & 96.4\% \textcolor{pf1color}{(-2.2\%)}  & 97.0\%                  & 96.9\%                  & 91.4\% \textcolor{pf1color}{(-5.6\%)}  & 91.6\% \textcolor{pf1color}{(-5.3\%)}  & 94.1\% \textcolor{pf1color}{(-2.9\%)}  & 94.1\% \textcolor{pf1color}{(-2.8\%)}  \\
  IMDB                   & \OB\OB\OB                   & 87.6\%                  & 87.6\%                  & 81.2\% \textcolor{pf1color}{(-6.4\%)}  & 81.5\% \textcolor{pf1color}{(-6.1\%)}  & 81.1\% \textcolor{pf1color}{(-6.5\%)}  & 81.2\% \textcolor{pf1color}{(-6.4\%)}  & 89.0\%                  & 89.0\%                  & 81.2\% \textcolor{pf1color}{(-7.8\%)}  & 81.3\% \textcolor{pf1color}{(-7.7\%)}  & 82.4\% \textcolor{pf1color}{(-1.6\%)}  & 82.4\% \textcolor{pf1color}{(-1.6\%)}  \\
  SMS spam                          & \OB\OB\OB\OB                & 97.2\%                  & 98.8\%                  & 93.8\% \textcolor{pf1color}{(-3.4\%)}  & 95.1\% \textcolor{pf1color}{(-3.7\%)}  & 94.3\% \textcolor{pf1color}{(-2.9\%)} & 94.8\% \textcolor{pf1color}{(-4.0\%)} & 97.3\%                  & 98.8\%                  & 93.5\% \textcolor{pf1color}{(-3.8\%)}  & 95.9\% \textcolor{pf1color}{(-2.9\%)}  & 94.0\% \textcolor{pf1color}{(-3.3\%)}  & 95.7\% \textcolor{pf1color}{(-3.1\%)}  \\

  Reddit emotion                      & \OB\OB\OB\OB\OB             & 93.7\%                  & 94.6\%                  & 72.7\% \textcolor{pf1color}{(-21.0\%)} & 74.4\% \textcolor{pf1color}{(-20.2\%)} & 81.9\% \textcolor{pf1color}{(-11.8\%)} & 82.0\% \textcolor{pf1color}{(-12.6\%)} & 91.3\%                  & 92.1\%                  & 77.9\% \textcolor{pf1color}{(-13.4\%)} & 78.1\% \textcolor{pf1color}{(-14.0\%)} & 87.5\% \textcolor{pf1color}{(-3.8\%)}  & 87.7\% \textcolor{pf1color}{(-4.4\%)}  \\
  Tweet irony   & \OB\OB\OB\OB\OB             & 72.2\%                  & 73.9\%                  & 63.4\% \textcolor{pf1color}{(-8.8\%)}  & 63.6\% \textcolor{pf1color}{(-10.3\%)} & 81.5\% \textcolor{f1color}{(+9.3\%)} & 81.9\% \textcolor{f1color}{(+8.0\%)} & 74.0\%                  & 75.5\%                  & 57.8\% \textcolor{pf1color}{(-16.2\%)} & 59.1\% \textcolor{pf1color}{(-16.4\%)} & 83.3\% \textcolor{f1color}{(+9.3\%)} & 83.7\% \textcolor{f1color}{(+8.2\%)} \\
  Tweet emotions & \OB\OB\OB\OB\OB             & 77.7\%                  & 81.1\%                  & 58.1\% \textcolor{pf1color}{(-19.6\%)} & 64.5\% \textcolor{pf1color}{(-16.6\%)} & 64.6\% \textcolor{pf1color}{(-13.1\%)} & 69.1\% \textcolor{pf1color}{(-12.0\%)} & 75.8\%                  & 78.9\%                  & 64.6\% \textcolor{pf1color}{(-11.2\%)} & 71.5\% \textcolor{pf1color}{(-7.4\%)}  & 66.3\% \textcolor{pf1color}{(-9.5\%)}  & 72.7\% \textcolor{pf1color}{(-6.2\%)}  \\
  Sarcasm              & \OB\OB\OB\OB\OB             & 89.9\%                  & 90.3\%                  & 51.1\% \textcolor{pf1color}{(-38.8\%)} & 51.2\% \textcolor{pf1color}{(-39.1\%)} & 63.6\% \textcolor{pf1color}{(-26.3\%)} & 64.8\% \textcolor{pf1color}{(-25.5\%)} & 91.8\%                  & 92.0\%                  & 54.3\% \textcolor{pf1color}{(-37.5\%)} & 54.3\% \textcolor{pf1color}{(-37.7\%)} & 61.5\% \textcolor{pf1color}{(-30.3\%)} & 63.6\% \textcolor{pf1color}{(-28.4\%)} \\
   Financial              & \OB\OB\OB\OB\OB             & 83.2\%                  & 84.6\%                  & 48.2\% \textcolor{pf1color}{(-35.0\%)} & 60.7\% \textcolor{pf1color}{(-23.9\%)} & 70.6\% \textcolor{pf1color}{(-12.6\%)} & 74.2\% \textcolor{pf1color}{(-10.4\%)} & 85.0\%                  & 86.6\%                  & 58.5\% \textcolor{pf1color}{(-26.5\%)} & 70.3\% \textcolor{pf1color}{(-16.3\%)} & 75.0\% \textcolor{pf1color}{(-10.0\%)} & 78.9\% \textcolor{pf1color}{(-7.7\%)}  \\
  Humor speech          & \OB\OB\OB\OB\OB             & 97.0\%                  & 97.0\%                  & 56.0\% \textcolor{pf1color}{(-41.0\%)} & 61.7\% \textcolor{pf1color}{(-35.3\%)} & 86.9\% \textcolor{pf1color}{(-10.1\%)} & 87.0\% \textcolor{pf1color}{(-10.0\%)} & 96.7\%                  & 96.7\%                  & 54.9\% \textcolor{pf1color}{(-41.8\%)} & 60.9\% \textcolor{pf1color}{(-35.8\%)} & 84.0\% \textcolor{pf1color}{(-12.7\%)} & 84.0\% \textcolor{pf1color}{(-12.7\%)} 
  \end{tblr}
  }
  \caption{Comparing the performance of classification models trained on the LLM-generated synthetic data under the zero-shot or few-shot settings, with those trained with the original real-world data, in terms of Macro-F1 (\%) and Accuracy Score (\%). In the ``Subjectivity'' column, more "$\star$" symbols indicate a higher level of task subjectivity.  }
  \label{tab:results}
\end{table*}

\subsection{Evaluation Results}
Table~\ref{tab:results} summarizes the comparative performance  of classification models trained with different data. Below, we highlight a few key observations we get from this comparison. 

\noindent \textbf{Models trained on the real-world data consistently outperform those trained on the synthetic data.} Our results indicate that models trained on the original real-world data consistently outperform their counterparts trained on the synthetic data generated under either zero-shot or few-shot settings, almost for every task. In particular, with the RoBERTa model, we observe that the average improvements of the model trained on the real-world data 
over 
the models trained on zero-shot synthetic data and few-shot synthetic data
are $16.9\%$ and $6.7\%$ in terms of Macro-F1, and $14.9\%$ and  $6.1\%$ in terms of accuracy. 
Similar trends are observed with the BERT model as well.  

\noindent \textbf{Guiding LLM with real-world data examples can boost the effectiveness of the synthetic data.}
We also observe that models trained on those synthetic data generated under the few-shot settings almost always outperform those trained on the synthetic data generated under the zero-shot settings. For instance, for the BERT model, we see an average increase of $10.6\%$ and $8.8\%$ in Macro-F1 and accuracy scores, respectively, across the 10 tasks in the few-shot setting, as compared to the zero-shot setting. Similarly, with the RoBERTa model, there is an average increase of $10.3\%$ in Macro-F1 and $8.9\%$ in accuracy scores across the 10 tasks when the real-world data are used as examples for LLM to mimic in the synthetic data generation process. For more analysis of the few-shot synthetic data, please see App.~\ref{e1a1} and \ref{e1a3}.

\noindent \textbf{Synthetic data support more effective model training for tasks that are less subjective.} Finally, we notice that for classification tasks with relatively low levels of subjectivity (e.g., those in the AG's news, Relation classification, IMDB reviews, and SMS spam datasets), the performance difference between models trained on the synthetic data and those trained on the real-world data is remarkably small. 
However, for tasks with high subjectivity, the performance decrease resulted from the usage of the synthetic data is more significant---for instance, across the cluster of 6 tasks with the highest level of subjectivity in our evaluation, there is an average decrease of 27.4\% and 24.2\% in Macro-F1 and accuracy, respectively, comparing the BERT models trained on the zero-shot synthetic data with those trained on the real-world data. In other words, for text classification tasks that are highly objective, there is great potential in training high-performing models simply based on synthetic data generated by LLMs, but the same method falls short in generating synthetic data that can effectively support model training for highly subjective classifications. 

\subsection{Exploratory Analysis: Data Diversity}
\label{sec:exploratory}
\begin{figure}[t]
  \centering
  \subfloat[Remote Clique]{\includegraphics[width=0.24\textwidth]{figures/remote_clique.png}\label{fig:remote_clique}}
  \hfill
  \subfloat[Chamfer Distance]{\includegraphics[width=0.24\textwidth]{figures/chamer_distance.png}\label{fig:chamfer}}
  \caption{Comparing the diversity of the real-world data and the synthetic data.}
  \label{fig: diversity}
\end{figure}

To explore the potential reasons underlying the model performance difference, we conducted an exploratory analysis on the diversity of the training data. Following \citeauthor{10.1145/3411764.3445782}~\shortcite{10.1145/3411764.3445782}, we used the {\em Remote Clique Score} (i.e., the average mean distance of a data instance to other instances) and the {\em Chamfer Distance Score} (i.e., the average minimum distance of a data instance to other instances) to quantify the diversity of a set of data. 
For both metrics, higher values indicate greater data diversity. 
As shown in Figure~\ref{fig: diversity}, we find that in general, the real-world data appear to be more diverse than the synthetic data generated under the few-shot settings, which in turn seem to be more diverse than the zero-shot synthetic data. This might partially explain why models trained on the real-world data and the few-shot synthetic data tend to outperform those trained on the zero-shot synthetic data. 

In addition, we also notice that compared to that on the low subjectivity tasks (i.e., AG, Relation, IMDB, Spam), the differences in data diversity between the real-world data and the synthetic data seem to be more salient on the high subjectivity tasks (i.e., the other 6 tasks), especially in terms of the Chamfer Distance Score. In fact, a t-test shows that the decrease of the Chamfer Distance Score in the zero-shot synthetic data compared to the real data is significantly larger for the high subjectivity tasks than for the low subjectivity tasks ($p<0.01$). 
This suggests that for tasks with high subjectivity, such as interpreting humor or sarcasm in language, LLMs may not be able to 
generate data instances that can cover the full spectrum of real-life scenarios, which may limit the performance of models trained on the synthetic data. 

\section{Evaluation \RNum{2}: Comparison Across Different Task Instances}

\begin{table*}
\centering
\resizebox{\linewidth}{!}{
\begin{tblr}{
  cells = {c},
  hline{1-2,5} = {-}{},
}
\textbf{Dataset} & \textbf{AG} & \textbf{Relation} & \textbf{IMDB}  & \textbf{SMS Spam}  & \textbf{Reddit Emotion}  &\textbf{Humor Speech} &\textbf{Tweet Irony} & \textbf{Sarcasm }   & \textbf{Tweet Emotions}  & \textbf{Finanical} \\
Average Agreement $\overline{a}$     & 0.80 (4.2)  & 0.78 (4.5) & 0.76 (7.3)  & 0.73 (8.5) & 0.69 (6.6)  & 0.68 (7.1) & 0.68 (6.7)  & 0.64 (7.7) &  0.64 (4.6) & 0.57 (7.6) \\
Krippendorff's $\alpha$   & 0.51  & 0.43 & 0.19  & 0.27 & 0.30  & 0.06 & 0.03  & 0.01 &  0.17 & -0.03 \\
Subjectivity Level &\OB  &\OB\OB  & \OB\OB\OB &  \OB\OB\OB\OB & \OB\OB\OB\OB\OB  & \OB\OB\OB\OB\OB  &\OB\OB\OB\OB\OB  &\OB\OB\OB\OB\OB  & \OB\OB\OB\OB\OB &\OB\OB\OB\OB\OB  
\end{tblr}
}
\caption{The average instance-level annotation agreement for different types of tasks, 
alongside the corresponding task-level subjectivity. Numbers in parentheses in the first row
represent the average number of annotations received per task instance. Higher values for both the average agreement $\overline{a}$ and Krippendorff's $\alpha$ indicate a higher degree inter-annotator agreement.
}
\label{tab:hit2}
\end{table*}

In the previous section, we have discovered that the subjectivity of a task can adversely affect the performance of classification models trained on the LLM-generated synthetic data. However, even for the same type of task, the classification for each individual task instance may exhibits different levels of subjectivity as well. Naturally, one may wonder whether models trained on the LLM-generated synthetic data may show different performance on task instances of different subjectivity. We aim to explore the answers to this question in this section.

\subsection{Instance-level Subjectivity Determination}
Given a text classification task and a specific text instance, we consider the degree of {\em agreement among annotators} on the label of this text as a proxy for the subjectivity of this instance---a lower level of agreement means that annotators hold more divergent views, hence the task may have a higher level of subjectivity. Thus, to formally quantify the subjectivity of different instances for different tasks, we again conduct a crowdsourced study to collect instance-level annotations.

\noindent \textbf{Study procedure.} We again considered the 10 types of text classification tasks as that in the first evaluation study. For each type of task, we randomly sampled 50 text instances per category from the test set to compose our ``evaluation dataset'' for that task.
We then recruited U.S. workers from MTurk to complete annotation tasks for those instances in our evaluation dataset. Specifically,  each worker was randomly assigned to one type of text classification tasks. After going through a brief 
instruction of the assigned task, 
the worker was asked to complete 20 classification tasks of the assigned type to get a payment of \$1.2, where the texts presented in these 20 tasks were randomly sampled from the evaluation dataset for the assigned type of task.  
Again, we included two attention check questions in our study to filter out inattentive workers.  We ensured that each task instance received at least three annotations from unique MTurk workers. 

\noindent \textbf{Computing instance subjectivity.}
Based on annotations we obtained from attentive workers, 
we quantify the subjectivity level of each task instance using the fraction of annotators who agree with the majority label for the task instance, that is: 
\begin{equation}
    a_i = \frac{{\max}_{y \in \mathcal{Y}} \sum_{k=1}^{K_i} \mathds{1}{(r_{i}^{k} = y)}}{K_i}
\end{equation}
where $\mathcal{Y}=\{1,\cdot\cdot\cdot, Y\}$ is the set of all possible labels, $K_i$ is the total number of annotators who labeled instance $i$, and $r_{i}^{k}$ is the $k$-th annotator's annotation on instance $i$. Intuitively, a lower value of $a_i$ suggests that consensus is less likely to be reached among annotators on  instance $i$, thus instance $i$ may have a higher level of subjectivity.  In Table~\ref{tab:hit2}, we report the average values of
$a_i$
(i.e., $\overline{a}$) for instances in the evaluation datasets of different types of tasks, along with the average inter-annotator agreement on each task instance (as measured by the Krippendorff's $\alpha$) as well as the task-level subjectivity level for different types of tasks. We can see that 
$\overline{a}$ closely aligns with the Krippendorff's $\alpha$, and tasks with higher levels of subjectivity also exhibit a higher value of $\overline{a}$ in general, indicating that $a_i$ can potentially serve as a reasonable proxy for the subjectivity of each task instance.     

\begin{figure*}[htbp]
  \centering
  \subfloat[AG]{\includegraphics[width=0.19\textwidth]{figures/ag_line.png}\label{fig:ag_line}}
  \hfill
  \subfloat[Relation]{\includegraphics[width=0.19\textwidth]{figures/relations_line.png}\label{fig:rel_line}}
  \hfill
  \subfloat[IMDB Reviews]{\includegraphics[width=0.19\textwidth]{figures/imdb_line.png}\label{fig:imdb_line}}
  \hfill
  \subfloat[SMS Spam]{\includegraphics[width=0.19\textwidth]{figures/spam_line.png}\label{fig:spam_line}}
  \hfill
  \subfloat[Reddit Emotion]{\includegraphics[width=0.19\textwidth]{figures/go_emotions_line.png}\label{fig:goemo_line}}
  \\
  \hfill
  \subfloat[Sarcasm News]{\includegraphics[width=0.19\textwidth]{figures/sarcasm_line.png}\label{fig:sarcasm_line}}
  \hfill
  \subfloat[Humor Detection]{\includegraphics[width=0.19\textwidth]{figures/humor_line.png}\label{fig:humor_line}}
  \hfill
  \subfloat[Tweet Emotions]{\includegraphics[width=0.19\textwidth]{figures/tweet_emotion_line.png}\label{fig:temo_line}}
  \hfill
  \subfloat[Tweet Irony Speech]{\includegraphics[width=0.19\textwidth]{figures/tweet_irony_line.png}\label{fig:tirony_line}}
  \hfill
   \subfloat[Financial Phrasebank]{\includegraphics[width=0.19\textwidth]{figures/financial_line.png}\label{fig:fin_line}}
  
  \caption{Changes in the accuracy of the BERT model trained on zero-shot synthetic data as the instance-level annotation agreement threshold varies.  
  The solid blue line in each plot is the linear regression fitted on the data,  and the $R$-squared score quantifies the goodness of fit. The Spearman's $\rho$ assesses the strength of rank correlation between the instance-level agreement threshold and the model accuracy for each task. Higher values for both $R$-squared and Spearman's $\rho$, ideally close to $1$, indicate a stronger monotonic relationship between the instance-level subjectivity and the model accuracy. }
  \label{fig:instance_corr}
  \vspace{-10pt}
\end{figure*}

\subsection{Evaluation Results}
\label{sec:eval2}

We now look into whether models trained on the LLM-generated synthetic data exhibit different performance on instances with different levels of subjectivity, and we focus on the models trained on zero-shot synthetic data in this evaluation. Specifically, given a classification task, we trained a BERT model using the zero-shot synthetic data and computed its accuracy on the subset of task instances in the evaluation dataset whose instance-level annotation agreement (i.e., $a_i$) exceeds a threshold $\gamma$, and we repeated this computation for many times as we varied the value of $\gamma$.  

Figure~\ref{fig:instance_corr} illustrates how the model accuracy varies with the instance-level annotation agreement threshold $\gamma$ for different types of tasks. For most tasks (except for the tasks in the Scarcasm News and Finanical Phrasebank datasets), we observe a strong {\em monotonically increasing} relationship between $\gamma$ and the model accuracy, with correlations between them (i.e., $\beta$) being positive and  values of the Spearman's rank correlation coefficient $\rho$ often exceeding 0.85. Since increasing the instance-level annotation agreement threshold $\gamma$ effectively filters out task instances with high subjectivity, this observation suggests that models trained on synthetic data indeed tend to have varying performance on different instances---even within the same type of tasks, these models still perform better on those task instances with low subjectivity.  

As a comparison, we also investigate into whether models trained on the real-world data exhibit similar behaviors. The detailed results are reported in App.~\ref{e2a}. On the high level, while we also observe the trend that these models' performance appears to increase as the instance-level task subjectivity decreases, such relationship is usually weaker than that illustrated in the models trained on the synthetic data (e.g., $\beta$ and $\rho$ are smaller). 

\vspace{-8pt}

\section{Conclusions and Discussions}
\vspace{-4pt}
In this paper, we present an initial exploration into factors that moderate the effectiveness of LLM-generated synthetic data for facilitating the training of text classification models.  
Our results show that the performance of the models trained on synthetic data decreases both for classification tasks with higher levels of subjectivity and on task instances with higher subjectivity. In this section, we provide some potential explanations for the observations of our study, and discuss the implications, limitations, and future directions of our work.
\vspace{-2pt}
\subsection{Why subjectivity adversely impacts the effectiveness of the synthetic data?}
We provide a few explanations for why task subjectivity is found to be negatively associated with the performance of models trained on the LLM-generated synthetic data.
First, highly subjective tasks often require a deep understanding of nuanced human emotions and contextual subtleties, as well as the ability to discern and accurately interpret different perspectives. As such, LLMs may encounter limitations in generating data that can capture the extensive range and complexity of real-life use of language. Indeed, as shown in our exploratory analysis in Section~\ref{sec:exploratory}, the diversity of the LLM-generated synthetic data appears to be particularly limited on tasks with high subjectivity, when compared to the real-world data. This implies that one potential way to improve the effectiveness of synthetic data on high subjectivity tasks is to increase the data diversity and ensure the synthetic data can better reflect real-world data distributions. 

Second, specific to the relationship between the instance-level subjectivity and model performance, we note that the ``gold label'' of a task instance is usually decided by a majority vote within a group of annotators. This means that the gold label may not represent the perspective of each individual~\cite{goyal2022your}, and they are sometimes ``biased'' themselves depending on the annotator decomposition~\cite{li2022towards}. Thus, it may be challenging for LLMs to generate synthetic data to recover such potentially biased ``majority view,'' especially if the LLMs are trained to maintain neutrality. Alternatively, one may ask for subjective task instances that humans can hardly reach any consensus on, whether the ``gold label'' is really the only ``correct'' label? If not, a rethinking of how to develop and evaluate models for these task instances is urgently needed.

\subsection{Explaining a few exceptions}

In Table~\ref{tab:results}, we surprisingly find that on the Tweet irony detection tasks, models trained on the few-shot synthetic data even outperform models trained on the real-world data. One plausible explanation is that the nature of generating irony texts for social media involves a creative writing task with few language formality constraints, and recent research suggests that LLMs have
the potential to exhibit comparable creativity with human writers in such task~\cite{franceschelli2023creativity}. Another exception we find is in Section~\ref{sec:eval2}---for the Financial Phrasebank and Scarcasm datasets, unlike other tasks, the effectiveness of the models trained on the synthetic data do not vary much with the instance-level task subjectivity. 
We conjecture that this can be caused by some 
task-specific properties.
On the Financial Phasebank dataset, accurate sentiment analysis 
requires the understanding of specialized terminology related to finance. Similarly, the Sarcasm detection task aims at identifying sarcasm in news headlines from selected sources and requires the comprehension on political topics. Thus, on these tasks, LLMs might not be fully equipped with the necessary domain knowledge to create effective synthetic data under the zero-shot setting. In fact, as shown in Figure~\ref{fig:instance_corr}, models trained on the zero-shot synthetic data have very low performance on these two datasets, regardless of the subjectivity levels of task instances. 

\vspace{-4pt}
\subsection{Limitations and future work}

We acknowledge that task subjectivity may not be the only factor that moderates the effectiveness of the LLM-generated synthetic data. Future studies can look into the potential moderating role of other factors, such as language formality and the requirement for domain-specific knowledge.  Our reliance on crowd workers in determining task subjectivity may introduce some variability due to their lack of linguistic expertise. Our evaluation is also based on the GPT-3.5-Turbo model only. It is important to note that the conclusions we get here may not generalize to other LLMs (e.g., the more advanced GPT-4), considering the continuous improvements of LLMs in generating human-like texts.

Our findings suggest that incorporating real-world data examples into the synthetic data generation process can increase the data diversity and boost the performance of the resulting models. Thus, future work can explore strategies that leverage human intelligence, such as feedback or direct intervention in the generation process, to further enrich the diversity of synthetic data~\cite{chung2023increasing} and to identify the most ``informative'' type of data instance to generate.  Finally, the significant correlation between the subjectivity of tasks or instances and the performance of models trained on synthetic data also suggests the potential to utilize the performance of such models as a proxy for approximating task or instance subjectivity, or to estimate the reliability of gold labels.

\clearpage

\end{document}