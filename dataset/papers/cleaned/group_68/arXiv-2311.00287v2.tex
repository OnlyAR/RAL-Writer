\title{Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models}

\begin{document}

\maketitle
\begin{abstract}
Clinical natural language processing faces challenges like complex medical terminology and clinical contexts.
Recently, large language models (LLMs) have shown promise in this domain. Yet, their direct deployment can lead to privacy issues and are constrained by resources. 
To address this challenge, we delve into synthetic clinical text generation with LLMs for clinical NLP tasks. 
We propose an innovative, resource-efficient approach, {\ours}, which infuses knowledge into the process. Our model involves clinical knowledge extraction and context-informed LLM prompting. 
Both clinical topics and writing styles are drawn from external domain-specific knowledge graphs and LLMs to guide data generation. 
Our extensive empirical study across 8 clinical NLP tasks and 18 datasets reveals that {\ours} consistently enhances performance across various tasks by 7.7\%-8.7\% on average, effectively aligning the distribution of real datasets and  enriching the diversity of generated training instances. 
Our code is available at \url{https://github.com/ritaranx/ClinGen}.
\end{abstract}

\vspace{-1ex}
\section{Introduction}
\label{sec:intro}
Clinical Natural Language Processing (NLP) emerges as a distinct subfield including the extraction, analysis, and interpretation of unstructured clinical text~\citep{wornow2023shaky}.  
Despite its significance, unique challenges exist for methodology development in clinical NLP. For example, clinical texts are often dense with abbreviations and specialized medical terminologies can be perplexing to standard NLP models~\citep{lee2023ai}. 
Fortunately, recent advances in Large Language Models (LLMs)~\citep{brown2020language,chung2022scaling,ouyang2022training,chatgpt,gpt4} provide a promising way to resolve these issues, as they contain billions of parameters and have been pretrained on massive corpora, thus inherently capture a significant amount of clinical knowledge~\citep{agrawal2022large,singhal2022large}.  
These progresses inspire the need for designing specialized approaches for adapting LLMs to clinical settings, which both address the terminology complexities and improve models through clinical data finetuning~\citep{tu2023towards,liu2023utility}.

Despite the strong capacity of general LLMs, 
directly applying them to infer over clinical text data is often undesired in practice. 
Firstly, these LLMs often have billions of parameters that translate to significant computational resources even for inference, leading to \emph{increased infrastructure costs} and \emph{long inference time}. 
Furthermore, the sensitive patient information in the clinical text naturally raises \emph{privacy and regulatory compliance concerns}~\citep{mesko2023imperative}. 
To combat these challenges, generating synthetic training data using LLMs serves as a promising solution, as it leverages the capability of LLMs in a resource-efficient and privacy-centric way. When trained with synthetic data mimicking real-world clinical data, models can achieve high performance while obeying data protection regulations. %This paves the way for the broader and safer integration of advanced NLP techniques into the healthcare domain.

Synthetic data generation with LLMs is a popular research area in NLP~\citep{meng2022generating,ye2022zerogen,ye2022progen,wang2023lets}, with a focus on gemeral-domain data. 
However, adapting LLMs trained on general texts for generating high-quality clinical data poses distinct challenges. 
To assess the quality of data generated by existing methods, we carry out an  evaluation centered on distribution and diversity, detailed in Section~\ref{sec:preliminary}, which
indicate a noteworthy data distribution shift. We further examine the clinically-related entity quantities and frequencies in synthetic data, where a notable decline is observed when contrasting synthetic data with ground truth data.  
While some research has delved into clinical data generation with language models, many of these efforts are tailored to specific tasks. 
Examples include medical dialogues~\citep{chintagunta2021medically}, clinical notes~\citep{giorgi2023clinical}, 
and electronic health records~\citep{ive2020generation}. 
These studies often directly adopt language models for text generation, and sometimes on excessive training data.
Till now, a unified principle to better adapt LLMs for generating synthetic text for facilitating clinical downstream applications is still missing.

Motivated by the above analysis, we propose \ours, a \emph{clinical knowledge-infused}  framework for high-quality clinical text generation in few-shot scenarios. 
Our ultimate goal is to bridge the gap between synthetic and real data while enhancing topic diversity. 
Towards this end, we propose to utilize clinical knowledge extraction to contextualize the prompts. \blue{This includes generating clinical topics on entity and relation information from both KGs and LLMs and deriving writing style suggestions from LLMs.}
By doing this, {\ours} integrates both \emph{non-parametric insights} from external clinical knowledge graphs with the \emph{intrinsic parametric knowledge} encoded in LLMs and \emph{enjoys higher diversity} via dynamically composing different topics and writing styles together during the data generation process.
It is worth noting that, \ours only relies on minimal additional human efforts, and can be readily applied to a wide array of core tasks in clinical NLP.

Our contributions can be summarized as follows:

$\bullet$ We propose {\ours}, a generic clinical knowledge-infused framework for clinical text data generation in few-shot settings. It can be readily applied to a wide range of tasks in clinical NLP.

$\bullet$ We present an analysis of the pitfall of existing data generation approaches for clinical text data, and propose a 
simple yet effective strategy to extract clinical knowledge and customize the prompts toward target clinical NLP tasks. This includes generating clinical topics from both KGs and LLMs and deriving writing style suggestions from LLMs. 

$\bullet$ We conduct an exhaustive evaluation of synthetic clinical data generation \textbf{across 8 clinical NLP tasks and 18 datasets}. Empirical findings demonstrate that {\ours} not only aligns more closely with the distribution of the original data but also amplifies the diversity of the generated training samples. The empirical performance gains are consistent across various tasks with different LLMs and classifiers (8.7\% for PubMedBERT$_{\texttt{Base}}$ and 7.7\% for PubMedBERT$_{\texttt{Large}}$).

\section{Related Work}
Generating additional training data enables a more precise analysis of medical text, and has gained more attention in the past years. 
Earlier research has employed data augmentation techniques to generate similar samples to existing instances with word substitution~\citep{kang2021umls}, back translation~\citep{uda}, pretrained transformers~\citep{xu2023weakly,melm}. But they often yield rigid transformations and the quality of the augmented text cannot be always guaranteed. 

The emergence of LLMs has presented new possibilities for synthetic data generation~\citep{meng2022generating,meng2023tuning,ye2022zerogen,li-etal-2023-synthetic}. However, these methods often use generic and simple prompts that may not fully capture domain-specific knowledge, thus potentially limiting the quality of the generated data. 
\citet{liu2022wanli,chung-etal-2023-increasing,yu2023large} employ interactive learning to generate instances, at the cost of additional human efforts.
Several recent studies explore LLM-based synthetic data generation for clinical NLP.  
\citet{tang2023does} rely on a \emph{much larger training set} to generate candidate entities, which {disregards the practical low-resource setting}~\citep{perez2021true}. 
Moreover, these studies often concentrate on specific target tasks, thus lacking generality for diverse clinical NLP scenarios.

On the other hand, several works aimed at optimizing prompts using LLMs~\citep{zhou2023large,wang2023promptagent} or knowledge graphs~\citep{cui2023a,liu-etal-2022-generated,chen2022knowprompt}, yet they mainly focus on refining prompts to obtain the answer for the given input, and the prompt template often remains unchanged. 
Instead, we focus on the different task of generating training instances. By composing different topics and styles together, we can generate diverse templates for prompting LLMs to improve the quality of the synthetic data.
 \begin{figure*}[!t]
	\centering
	\vspace{-2.5ex}
	\subfigure[CMD]{
		\includegraphics[width=0.34\linewidth]{figures/cmd-baseline.pdf}
		\label{fig:cmd-baseline}
	} %\hfill
         \hspace{-1.5ex}
     \subfigure[Entity Coverage]{
		\includegraphics[width=0.34\linewidth]{figures/avg-entity-baseline.pdf}
		\label{fig:avg-entity-baseline}
	}
 \hspace{-1.5ex}
      \subfigure[Entity Frequency]{
		\includegraphics[width=0.27\linewidth]{figures/bc5cdr_disease_freq_bsl.pdf}
		\label{fig:bc5cdr_disease_freq_bsl}
	}
	\caption{Preliminary Studies. (c) is from BC5CDR-Disease and is in log scale. \vspace{-1ex}}

\label{fig:prelim2}
\end{figure*}

\section{Preliminary Study}
\vspace{-0.5ex}

\label{sec:preliminary}
This section first presents the foundational setup of synthetic data generation. 
Then, we provide an in-depth investigation into the pitfalls of existing synthetic data generation methods. 

\vspace{-1ex}
\subsection{Problem Setup}
In this paper, we study synthetic data generation under the few-shot setting.
The input consists of a training set $\cD=\{(x_i,y_i)\}_{i=1}^K$, where $(x_i, y_i)$ represents an input text and its corresponding label $y_i \in \cY$ for the $i$-th example. $K$ denotes the total number of training samples, which is  kept at a very small value (5-shot per label). The primary objective is to harness the LLM $\cM$ to generate a synthetic dataset, denoted as $\tilde{\cD}=\{(\tilde{x_i},\tilde{y_i})\}_{i=1}^N$, where $N$ is the number of generated samples ($N \gg K$). 
We use $\rho(\cdot)$ to denote the generation process from the LLM.
For each downstream task, we fine-tune a classifier $\cC_{\theta}$ \blue{(a moderate-size pre-trained language model)} parameterized by $\theta$ on the synthetic dataset $\tilde{\cD}$ for evaluating its quality.\footnote{While In-context Learning~\citep{brown2020language} can also be utilized, it is often hard to fit all generated instances into the context window, especially for datasets with high cardinality.}

\vspace{-0.5ex}
\subsection{Limitations of Existing  Methods}
\label{sec:limitations}
Denote the task-specific prompts for class label name $j$ as $p_j$, we take a closer look at the synthetic text data generated by two representative approaches: ZeroGen~\citep{ye2022zerogen}, which directly instructs LLMs for data generation as  $\tilde{\cD}_{\text{Zero}} \sim \rho_{j\sim\cY}(\cdot; p_j)$, and DemoGen~\citep{gpt3mix,meng2023tuning}, which augments the prompt with few-shot demonstrations $\cD$ as $\tilde{\cD}_{\text{Demo}} \sim \rho_{j\sim\cY}\left(\cdot; [p_j, \cD]\right)$.
The prompt format of ZeroGen and DemoGen are in Appendix~\ref{sec:prompt_format_bsl}.
We observe that these methods often introduce \textit{distribution shifts} and exhibit \textit{limited diversity}, which can lead to suboptimal downstream performance. 

\noindent \textbf{Distribution Shift.} An inherent issue when adapting LLMs to specific domains for text generation is the \emph{distribution shift}, given that LLMs are primarily trained on vast amounts of web text in general domains. 
To quantify the data distribution shift, we employ Central Moment Discrepancy (CMD)~\citep{zellinger2017central} to measure the gap between synthetic and real data across six clinical NLP datasets --- a high CMD value indicates a large gap between two distributions\footnote{Details of calculating CMD is in Appendix \ref{sec:cmd}.}. Figure \ref{fig:cmd-baseline} illustrates that both ZeroGen and DemoGen exhibit elevated CMD scores. 
Despite the inclusion of few-shot demonstrations in DemoGen, this limitation remains evident, indicating a notable disparity between the ground-truth and synthetic data.

\noindent \textbf{Limited Diversity.}
Clinical datasets in real-world scenarios often include rich domain knowledge that can be challenging to replicate in synthetic data. We evaluate synthetic dataset diversity by using both entity quantity and their normalized frequencies. The results are illustrated in Figures~\ref{fig:avg-entity-baseline} and \ref{fig:bc5cdr_disease_freq_bsl}. Our analysis reveals that datasets generated by ZeroGen and DemoGen exhibit a limited number of clinical entities, having a substantial discrepancy with the ground truth. 
Furthermore, it is highlighted that only a minority of potential entities and relations are frequently referenced across instances, while the majority are generated infrequently.

To explicitly illustrate the limitations, we present a case study in Figure~\ref{fig:prelim1},  Appendix~\ref{sec:add_prelim}. 
The comparison reveals that samples generated by ZeroGen and DemoGen lack  \textit{sufficient details} present in the ground truth data. 
Besides, the generated samples adhere to a more uniform style, while the ground truth encompasses various situations and writing styles, including urgent and informal inquiries.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.97\linewidth]{figures/clingen-framework.pdf}
    \caption{The overview of \ours. \vspace{-2ex}
    }
    \label{fig:overall}
\end{figure*}
\section{Knowledge Infused Data Generation}
Section \ref{sec:preliminary} highlights the necessity of domain-tailored knowledge for clinical synthetic data generation. In pursuit of this, we present {\ours}, a knowledge-informed framework for clinical data generation. The overview of {\ours} is shown in Figure~\ref{fig:overall}. 
This two-step methodology harnesses the emergent capabilities of LLMs and external knowledge from KGs to facilitate the synthesis of clinical data, even with few-shot examples only.  

\subsection{Clinical knowledge extraction}
Contrary to previous studies~\citep{ye2022zerogen,ye2022progen,meng2023tuning} which employ generic queries $p_j$ to prompt LLMs for text generation, {\ours} emphasizes refining clinically informed prompts. This approach aims to extract rich clinically relevant knowledge from parametric (\eg LLMs) or non-parametric sources (\eg knowledge graphs) and tailor it to clinical NLP tasks.
To realize this, our modeling contains two dimensions including \emph{clinical topics} $\cT$ and \emph{writing styles} $\cW$, which are integrated into the original prompts to infuse domain-specific knowledge. 
The \emph{Clinical topic} refers to a \emph{clinical entity} (e.g., disease) or \emph{relation} (e.g., the relationship between diseases and medications), which is usually a phrase, while the \emph{writing style} is a phrase that depicts the tone, and overall presentation of the text.
By composing different topics and writing styles together, {\ours} provide a diverse suite of prompts, resulting in a wider spectrum of text produced from the LLM $\cM$.
For details of prompt formats across various tasks, please see \textbf{Appendix~\ref{sec:prompt_format}}.

\subsubsection{Clinical Topics Generation}
We provide two choices to generate clinical topics $\cT$-- one is to sample related entities or relations from external KG, and the other is to query relevant knowledge from LLM.

\noindent \textbf{Topics $\cT_{\operatorname{KG}}$ sampled from Non-Parametric KGs.} 
Healthcare KGs offer 
a rich collection of medical concepts and their complex relationships, 
which organizes medical knowledge in a structured way~\citep{li2022graph}. 
In our study, we employ the integrative biomedical knowledge hub (iBKH) as the KG~\citep{su2023biomedical} $\cG$ to generate topics $\cT_{\operatorname{KG}} \sim \operatorname{query}(\cG)$
due to its broad coverage over clinical entities. 
To illustrate, for the Disease Recognition task (NCBI,~\citet{ncbi-disease}), we extract all disease nodes $e$ from the iBKH to bolster the medical information as 
$\cT_{\operatorname{KG}}^{\operatorname{NCBI}}\sim \operatorname{query}(\cG_{\operatorname{disease}})$, 
$\cG_{\operatorname{disease}}=\{e\in \cG|\operatorname{type}(e)=\operatorname{disease}\}$.
As another example, we retrieve links between chemicals  $c$ and diseases $d$  for the chemical and disease relation extraction (CDR,~\citet{cdr_dataset}) as
$\cT_{\operatorname{KG}}^{\operatorname{CDR}}\sim \operatorname{query}(\cG_{\operatorname{relation\_{cd}}})$, 
$\cG_{\operatorname{relation\_{cd}}}=\{\langle c,r,d \rangle\in \cG|\operatorname{type}(r)=\operatorname{has\_{relation}}\}$. 
By injecting information from the KG into the data generation step, we ensure the generated samples are more contextually accurate and semantically rich.

\noindent \textbf{Topics $\cT_{\operatorname{LLM}}$ queried from Parametric LLMs.} 
Pre-trained on extensive text corpora such as medical literature, LLMs provide an alternative method for acquiring domain knowledge.
Specifically, we aim to harness the rich clinical domain knowledge encoded in ChatGPT (\texttt{gpt-3.5-turbo-0301}) to augment the prompt. 
The incorporated prior knowledge from LLMs focus on entity types that hold relevance within clinical text datasets, including \emph{diseases}, \emph{drugs}, \emph{symptoms}, and \emph{side effects}.
For each of entity types $e_i$, we prompt the LLMs by formulating inquiries $q(e_i)$, \eg, ``\emph{Suppose you are a clinician and want to collect a set of <Entity Type>. Could you list 300 entities about <Entity Type>?}''. These crafted conversational cues serve as effective prompts to retrieve clinically significant entities from the rich domain knowledge within LLMs as 
$\cT_{\operatorname{LLM}} \sim \rho\left(\cdot;q(e_i)\right)$. 
For each entity type, we generate 300 entities for synthetic data generation.

\subsubsection{Clinical Writing Styles Suggestion}
\textbf{Styles suggested by LLMs.} To address the limitations mentioned in Sec~\ref{sec:limitations} and introduce a diverse range of writing styles $\cW$ for synthetic samples, we leverage the powerful LLM to suggest candidate writing styles for each task. Specifically, for the task $i$, we incorporate task names $n_i$ into our prompts $p^{\operatorname{style}}_{i}$ (e.g., \emph{disease entity recognition}, \emph{recognizing text entailment}) and integrate few-shot demonstrations $d^{\operatorname{style}}_{i}$. We then engage LLM in suggesting several potential sources, speakers, or authors of the sentences as $\cW \sim \rho\left(\cdot; [p^{\operatorname{style}}_{i}, d^{\operatorname{style}}_{i}]\right)$. 
Responses such as ``\emph{medical literature}" or ``\emph{patient-doctor dialogues}" are augmented into the prompts to imitate the writing styles found in real datasets. 

\subsection{Knowledge-infused Data Generation}
With the generated topics and styles, the key challenge becomes how to leverage them to extract rich clinical information from the LLM for improving synthetic data quality.
Directly putting all the elements to enrich the prompt is often infeasible due to the massive size of entities.
To balance informativeness as well as diversity, we propose a knowledge-infused strategy, where for each class label name $j\in\cY$, the collected clinical topics and writing styles serve as the base unit. 
In each step, we randomly sample a topic $t \in \cT$ and a writing style $w \in \cW$ from the candidate set to augment the prompt for class $j \in \cY$  as 
$p^{\operatorname{Clin}}_j(t, w) = [p_j, t, w]$. 
Then, we use the augmented prompt $p^{\operatorname{Clin}}_j(t, w)$ together with the few-shot demonstrations $\cD$ to generate the synthetic dataset $\tilde{\cD}_{\operatorname{Clin}}$ as 
\begin{equation}
\setlength{\abovedisplayskip}{6pt}
\setlength{\belowdisplayskip}{6pt}
\tilde{\cD}_{\operatorname{Clin}} \sim \rho_{j\sim\cY, t\sim \cT, w\sim\cW}\left(\cdot; \left[p_j, t, w\right], \cD\right).
\nonumber
\end{equation}
Despite its simplicity, this strategy enjoys several merits: 
(1) \emph{Clinical infusion}: the clinical context is incorporated into the prompts to directly guide data generation; 
(2) \emph{Diversity}: it encourages data  diversity via dynamically composing different entities and writing styles into prompts; 
(3) \emph{Flexibility}: it is compatible with different sources of $\cT$ and $\cW$ without reliance on specific knowledge formats. 
Consequently, the quality and clinical relevance of the generated synthetic data are enhanced.  
While some works focus on prompt optimization for data generation or other NLP tasks, they typically utilize a fixed prompt and optimize this prompt format, which is orthogonal to \ours{}.

\subsection{Language Model Fine-tuning}
After generating synthetic data $\tilde{\cD}$, we fine-tune a pre-trained classifier $\cC_{\theta}$ for each downstream task. Following \citet{meng2023tuning}, we first fine-tune $\cC_{\theta}$ on $\cD$ with standard supervised training objectives on few-shot examples (denoted as $\ell(\cdot)$) in Stage 1, then on synthetic data $\tilde{\cD}$ in Stage 2 as  
\begin{align}
\setlength{\abovedisplayskip}{6pt}
\setlength{\belowdisplayskip}{6pt}
    \label{eq:stage1}
    \theta^{(1)} &= \min_{\theta}~\mathbb{E}_{(x, y) \sim \cD} \ell\left( f(x; \theta), y \right), \nonumber \\
    \theta^{(2)} &=  \min_{\theta}~\mathbb{E}_{(\tilde{x}, \tilde{y}) \sim \tilde{\cD}} \ell\left( f(\tilde{x}; \theta), \tilde{y} \right),  \theta_{\text{init}} = \theta^{(1)}.
    \nonumber
\end{align} 
It's important to highlight that we strictly follow a standard fine-tuning process and avoid using any extra techniques: (1) for standard classification tasks, $\ell(\cdot)$ is the cross-entropy loss; (2) for multi-label classification tasks, $\ell(\cdot)$ is the binary cross-entropy loss; 
(3) for token-level classification tasks, we stack an additional linear layer as the classification head and  $\ell(\cdot)$ is the token-level cross-entropy loss. 
The design of \emph{advanced learning objectives} as well as \emph{data mixing strategies}, while important, are orthogonal to the scope of this paper. 

\section{Empirical Evaluation}
Given our focus on data generation, our major interest lies in faithfully evaluating different synthetic text generation approaches under few-shot scenarios, rather than competing in a ``\emph{state-of-the-art}" race with general few-shot NLP methods. 
The following questions particularly intrigue us:
\textbf{RQ1}: How does {\ours} perform when compared with baselines on different downstream tasks?  
\textbf{RQ2}: What impact do factors like LLM generators and synthetic data size have on the performance of {\ours}?
\textbf{RQ3}: How is the quality of the synthetic data generated by {\ours} and baselines?

\begin{table*}[t]
  \caption{Experimental results aggregated by tasks. \textbf{Bold} and \underline{underline} denote the best and second-best results. $\dagger$: Models exclusive to NER tasks. $*$: Since the two $\dagger$ models only report results on two NER datasets, we report the average performance on those two datasets for a fair comparison. 
  "Supervised-Full" and "Supervised-Few" denote the results using the original dataset and using only the few-shot examples as training data, respectively.
  \vspace{-1ex}}
  \renewcommand\arraystretch{0.95}
  \resizebox{0.98\linewidth}{!}{
  \begin{tabular}{lcc|ccccc|ccccc}
  \toprule
  \multirow{3.5}{*}{\bf Task}  & \multicolumn{2}{c|}{\textit {Single-Sentence Tasks}} &  \multicolumn{5}{c|}{\textit{Sentence-Pair Tasks}} & \multicolumn{5}{c}{\textit{Token Classification Tasks}} \\
  \cmidrule(lr){2-3} \cmidrule(lr){4-8} \cmidrule(lr){9-13} 
  & \bfseries Text Class (2) & \bfseries RE (3) & \bfseries NLI (3) & \multicolumn{2}{c}{\textbf{Fact Verification (2)}} & \bfseries STS (1)& \bfseries QA (2)& \multicolumn{2}{c}{\textbf{NER (4)}} & \multicolumn{3}{c}{\textbf{MedAttr (1)}} \\
  \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-6} \cmidrule(lr){7-7} \cmidrule(lr){8-8} \cmidrule(lr){9-10} \cmidrule(lr){11-13}
  & F1 & F1 & Acc & Acc & F1 & Acc & Acc & F1 & F1-subset$^*$ & P & R & F1\\
  \midrule
  \multicolumn{12}{l}{\textbf{PubMedBERT$_{\texttt{Base}}$}} \\
  \midrule
  Supervised-Full & 77.01 & 77.34 & 79.20 & 67.58 & 65.49 & 75.70 & 74.70 & 89.67 & 87.27 & --- & --- & ---\\
  Supervised-Few & 18.61 & 43.89 & 44.64 & 29.43 & 27.10 & 55.70& 54.74 & 39.41 & 34.12 & 38.11 & 43.82 & 40.77 \\
  \midrule
  DA-Word Sub~\shortcite{checklist} & 40.74 & 38.14 & 55.08 & 28.86 & 25.83 & 54.40 & 53.58& 44.30 & 40.41 & 40.25 & 47.65 & 43.64\\
  DA-Back Trans~\shortcite{uda} & 47.24 & --- & 54.30 & 32.15 & 28.04 & 55.80 & 53.28 & --- & --- & --- & --- & ---\\
  DA-Mixup~\shortcite{chen2020mixtext,seqmix} & 45.09 & 43.37 & 53.52 & 32.78 & 29.12 & 58.20 & 51.91 & 42.20 & 37.65 & 42.37 & 48.96 & 45.43\\
  DA-Transformer~\shortcite{melm,kumar2020data}  & 41.02 & 47.56 & 55.71 & 35.32 & 31.77 & 58.80& 56.36 & 44.75 & 39.66 & 37.82 & 44.28 & 40.80\\
  LightNER$^\dagger$~\shortcite{lightner} & --- & --- & --- & --- & --- & --- & --- &  ---- & 39.49 & --- & --- & ---\\
  KGPC$^\dagger$~\shortcite{chen2023few} & --- & --- & --- & --- & --- & --- & --- &--- &  51.60 & --- & --- & ---\\
  \midrule
  ZeroGen~\shortcite{ye2022zerogen,meng2022generating} & 59.02 & 63.84 & 55.96 & 35.30 & 32.50 & 68.35 &  61.89 & 56.97 & 48.26 & 52.80 & 49.53 & 51.11\\
  DemoGen~\shortcite{meng2023tuning,gpt3mix} & 64.09 & 67.46 & 59.80 & 40.30 & 35.95 & 70.85 & 62.01 & 60.16 & 53.91 & 58.15 & 56.84 & 57.49\\
  ProGen~\shortcite{ye2022progen} & 65.16 & 67.23 & 59.57 & 37.71 & 34.54 & 69.30 & 60.74 & 60.49 & 55.11 & 57.76 & 58.57 & 58.16\\  
S3~\shortcite{wang2023lets} & 65.12 & 67.60 & 61.36 & 40.17 & 36.44 & 70.20 &  63.58 & 60.36 & 54.25 & 56.21 &	63.60 &	59.68 \\
  \midrule
  \rowcolor{teal!10} {\ours} w/ KG & \underline{67.15} & \underline{69.01} & \underline{64.89} & \underline{43.83} & \underline{39.43} & \underline{72.20} & \bf 71.49 & \textbf{64.26} & \textbf{60.11} & \textbf{71.75} & \underline{65.20} & \textbf{68.32}\\
  \rowcolor{teal!10} {\ours} w/ LLM & \textbf{67.82} & \textbf{70.06} & \textbf{67.24} & \textbf{46.50} & \textbf{41.46} & \textbf{73.30} & \underline{69.60} & \underline{63.17} & \underline{58.49} & \underline{68.19} & \textbf{66.79} & \underline{67.48}\\
  \rowcolor{gray!15} Performance Gain & 4.08\% & 3.63\% & 9.58\% & 15.38\% & 13.77\% & 3.47\% & 12.44\% & 6.23\% & --- & --- & --- & 14.48\% \\
  \midrule
  \multicolumn{12}{l}{\textbf{PubMedBERT$_{\texttt{Large}}$}} \\
  \midrule
  Supervised-Full & 80.06 & 79.64 & 82.65 & 72.97 & 69.23 & 78.80 &80.37 &  90.15 & 87.68 & --- & --- & ---\\
  Supervised-Few & 17.86 & 52.68 & 50.00& 40.90 & 30.50 & 59.73 & 59.50& 42.84 & 37.57 & 41.30 & 45.02 & 43.08 \\
  \midrule
  DA-Word Sub~\shortcite{checklist} & 43.99 & 44.35 & 57.66 & 35.51 & 31.95 & 55.30 & 58.57& 46.67 & 43.70 & 46.77 & 43.52 & 45.09\\
  DA-Back Trans~\shortcite{uda} & 50.98 & --- & 58.39 & 34.12 & 31.36 & 56.40 &57.19 & --- & --- & --- & --- & ---\\
  DA-Mixup~\shortcite{chen2020mixtext,seqmix} & 46.74 & 50.97 & 57.35 & 34.01 & 31.10 & 58.50 & 56.68 & 46.69 & 43.01 & 41.25 & 52.09 & 46.04\\
  DA-Transformer~\shortcite{melm,kumar2020data} & 44.41 & 46.12 & 58.94 & 35.09 & 30.95 & 58.10 & 59.30 & 46.94 & 43.50 & 43.36 & 45.78 & 44.54\\
  \midrule
  ZeroGen~\shortcite{ye2022zerogen,meng2022generating} & 61.51 & 65.18 & 63.47 & 41.12 & 36.10 & 72.69 &66.02 & 57.79 & 49.10 & 54.04 & 51.40 & 52.69\\
  DemoGen~\shortcite{meng2023tuning,gpt3mix} & 64.97 & 68.65 & 64.58 & 42.61 & 38.69 & 74.37 & 65.04 & 61.43 & 55.61 & 62.67 & 61.02 & 61.83\\
  ProGen~\shortcite{ye2022progen} & 65.01 & 69.23 & 63.32 & 42.79 & 38.63 & 74.90 & 63.27 & 62.47 & 57.31 & 57.21 & 63.70 & 60.28\\
 S3~\shortcite{wang2023lets} & 64.33 & 69.65 & 65.07 & 41.76 & 37.72 & 73.20 & 66.33 & 61.97 & 56.29 & 63.07	&62.72&	62.89 \\

  \midrule
  \rowcolor{teal!10} {\ours} w/ KG & \underline{66.76} & \underline{71.47} & \textbf{70.90} & \underline{48.62} & \underline{42.45} & \underline{75.40} & \bf 73.94 & \textbf{65.48} & \textbf{62.23} & \underline{70.96} & \textbf{69.66} & \textbf{70.30}\\
  \rowcolor{teal!10} {\ours} w/ LLM & \textbf{67.61} & \textbf{72.81} & \underline{70.50} & \textbf{49.51} & \textbf{43.72} & \textbf{76.21} &  \underline{73.40} &\underline{65.36} & \underline{61.89} & \textbf{71.61} & \underline{66.86} & \underline{69.15}\\
  \rowcolor{gray!15} {Performance Gain} & 4.00\% & 4.54\% & 8.96\% & 15.70\% & 13.00\% & 3.47\% & 11.47\% & 1.76\% & --- & --- & --- & 11.78\% \\
  \bottomrule
  \end{tabular}
   }
  \label{tab:main-table}
\end{table*}
\subsection{Experiment Setup}
We conduct experiments in the few-shot settings with 5 examples for each class. We employ ChatGPT~\citep{chatgpt} (\texttt{gpt-3.5-turbo-0301}) as the LLM generator $\cM$\footnote{Studies on using Medical LLMs are in Appendix \ref{sec:med_llm_gen}.} and \textbf{maintain the same amount of synthetic training data for both {\ours} and baselines for a fair comparison.} The pre-trained PubMedBERT~\citep{gu2021domain} is then applied to fine-tune on the synthetic data for both {\ours} and baselines, where we consider both the \texttt{Base} and \texttt{Large} model. 

\noindent \textbf{Datasets and Tasks.}
We undertake a comprehensive evaluation of \textbf{18 datasets} across a diverse array of tasks in clinical NLP benchmarks~\citep{blue,fries2022bigbio}: 2 text classification, 3 relation extraction (RE), 3 natural language inference (NLI), 2 fact verification, 2 question answering (QA), 1 sentence similarity (STS), 4 Named Entity Recognition (NER), and 1 attribute extraction datasets. 
Please see Appendix~\ref{sec:dataset_description} for descriptions and the statistics of each dataset.

\noindent  \textbf{Baselines.}
We compare {\ours} with \textbf{10 baselines} in total, including
6 data augmentation and 4 LLM-based data generation techniques. 
See Appendix~\ref{sec:baseline_details} for their descriptions. 

\noindent \textbf{Implementation Details.}
\label{sec:implementation_details}
For implementation, we use PyTorch~\citep{paszke2019pytorch} and HuggingFace~\citep{wolf2019huggingface}. For each dataset, we randomly sample 5 examples from each class to provide few-shot demonstrations and keep a validation set of the same size. 
During the data generation process when we call the ChatGPT APIs~\citep{chatgpt}, we set the parameter $\operatorname{top\_p}=1.0$ and temperature $t=1.0$ to balance between the quality of the generated text as well as diversity~\citep{chung-etal-2023-increasing,yu2023large}\footnote{We do not further increase $t$, as previous analysis  \citep{chung-etal-2023-increasing,yu2023large} has shown that increasing $t$ to larger value does not help with additional performance gain.}. 
In the experiments, We generate 5000 synthetic training data for both {\ours} and the baselines and report the average performance over 3 random seeds for all the results. 
With the generated synthetic dataset, we follow the common few-shot learning setting~\citep{perez2021true} to train all the models for 6 epochs and use the model with the best performance on the validation set for evaluation. 
During the PubMedBERT fine-tuning, we adopt AdamW~\citep{loshchilov2017decoupled} for optimization with a linear warmup of the first 5\% steps and linear learning rate decay. The learning rate is set to 2e-5 for \texttt{Base} and 1e-5 for \texttt{Large}, and the maximum number of tokens per sequence is 256. 

\begin{figure*}[t]
\vspace{-2.5ex}
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \subfigure[HOC]{
            \includegraphics[width=0.48\textwidth]{figures/generator-HOC.pdf}
            \label{fig:generator-HOC}
        } \hspace{-3mm}
        \subfigure[MEDIQA-RQE]{
            \includegraphics[width=0.48\textwidth]{figures/generator-MEDIQA-RQE.pdf}
            \label{fig:generator-MEDIQA-RQE}
        }
        \vspace{-2ex}
        \RawCaption{\caption{Different generators at \texttt{Base}.}\label{fig:generator}}
    \end{minipage}%
    \begin{minipage}{0.48\textwidth}
        \centering
        \subfigure[HOC]{
            \includegraphics[width=0.5\textwidth]{figures/size-HOC.pdf}
            \label{fig:size-HOC}
        } \hspace{-6mm}
        \subfigure[MEDIQA-RQE]{
            \includegraphics[width=0.5\textwidth]{figures/size-Mediqa-rqe.pdf}
            \label{fig:size-Mediqa-rqe}
        }
        \vspace{-2ex}
        \RawCaption{\caption{Different proportion of data at \texttt{Base}.}\label{fig:size-synthetic}}
    \end{minipage}%
    \vspace{-0.5ex}
\end{figure*}

\begin{table*}[t]
  \caption{Comparison between prompting LLM for inference and {\ours} at \texttt{Large} scale.\vspace{-1ex}}
    \renewcommand\arraystretch{0.95}
  \resizebox{0.99\linewidth}{!}{
  \begin{tabular}{lcccccccccccccc}
  \toprule
  & \bfseries HOC & \multicolumn{3}{c}{\textbf{GAD}} & \bfseries ChemProt & \bfseries MEDIQA-RQE & \multicolumn{2}{c}{\textbf{PUBHEALTH}} & \multicolumn{3}{c}{\textbf{NCBI-Disease}} & \multicolumn{3}{c}{\textbf{CASI}}\\
  \cmidrule(lr){2-2} \cmidrule(lr){3-5} \cmidrule(lr){6-6} \cmidrule(lr){7-7} \cmidrule(lr){8-9} \cmidrule(lr){10-12} \cmidrule(lr){13-15}
  & F1 & P & R & F1 & F1 & ACC & ACC & F1 & P & R & F1 & P & R & F1\\
  \midrule
  
  ChatGPT Inference~\scriptsize{(\citeauthor{chatgpt})} & 68.76 & 84.21 & \textbf{97.46} & 90.35 & 49.42 & 74.31 & \textbf{69.50} & \textbf{52.47} & 46.62 & 52.31 & 49.30 & 48.82 & \textbf{74.75} & 59.07\\
  \blue{PMC-LLaMa-13B Inference}~\scriptsize{(\citeauthor{wu2023pmcllama})} & 50.07 & 89.61 & 81.18 & 85.19  & 33.35 & 52.17 & 48.01 & 32.84 & 27.11 & 23.97 & 25.44 & 56.38 & 36.87 & 41.58
  \\
 \blue{MedAlpaca-13B Inference}~\scriptsize{(\citeauthor{han2023medalpaca})} & \blue{40.44} & \blue{71.95} & \blue{72.48} & \blue{72.21} & \blue{31.29} & \blue{58.12} & \blue{55.40} & \blue{34.63} & \blue{44.69} & \blue{31.16} & \blue{27.85} & \blue{52.51} & \blue{49.16} & \blue{51.64}
 \\
  \midrule
  \rowcolor{teal!10} {\ours} w/ KG & 77.71 & 94.30 & 89.09 & \textbf{91.62} & 60.12 & \textbf{79.92} & 50.20 & 41.26 & \textbf{62.46} & \textbf{64.08} & \textbf{63.26} & 70.96 & 69.66 & \textbf{70.30} \\
  \rowcolor{teal!10} {\ours} w/ LLM & \textbf{78.14} & \textbf{95.08} & 86.14 & 90.39 & \textbf{63.05} & 77.36 & 52.96 & 43.31 & 61.12 & 60.16 & 60.64 & \textbf{71.61} & 66.86 & 69.15 \\
  \bottomrule
  \end{tabular}
  }
  \label{tab:gpt_inference}
  \vspace{-1ex}
\end{table*}
\subsection{Model Performance with Synthetic Data}
\label{sec:model_perf}
Table~\ref{tab:main-table} summarizes the experimental results. 
Due to space limits, we report the average performance over all datasets for each task, but provide the detailed results for each dataset in Tables~\ref{tab:single-sent}, \ref{tab:sent-pair}, \ref{tab:token-class} in Appendix~\ref{sec:more_experimental_results}. 
Based on the experimental results, we have the following findings:

\noindent $\diamond$ Our approach, {\ours}, consistently outperforms the baselines across all tasks. The average performance gain over all \textit{main} metrics is 8.7\% at \texttt{Base} scale and 7.7\% at \texttt{Large} scale. 
LLM-based methods outperform traditional DA techniques, showcasing their ability to capture task-specific information from a few examples. 
DemoGen and ProGen's gains over ZeroGen highlight the positive impact of few-shot examples.
Despite being one of the most powerful data generation approaches, S3's gains are marginal in the few-shot setting due to its reliance on large validation sets.

\noindent $\diamond$ In \textit{token classification tasks}, {\ours} performs better with KG compared to LLM due to the better alignment between the task's target and the generated domain knowledge, where the extracted topics serve as direct labels. 
Conversely, single-sentence and sentence-pair tasks favor LLM-based knowledge extraction. 
This could be because (1) These tasks prioritize sentence comprehension over specific terminologies, and some specialized terms might even impede LLM comprehension. (2) KGs \emph{may not} always contain the required information, e.g., certain relations in chemical/protein relation extraction tasks, limiting performance gains.

\noindent $\diamond$ Some DA methods are task-specific, limiting their generalizability. For example, LightNER and KGPC are designed for NER. It is also non-trivial to apply Back Translation to NER or RE, as it requires locating related entities in the generated sentence accurately.
In contrast, {\ours} is flexible and can be readily applied to various tasks.

\subsection{Ablation and Parameter Studies}
\label{sec:ablation}
\noindent \textbf{Effect of Different LLM Generators.}
To investigate the impact of various LLMs on {\ours},  
we utilize InstructGPT (\texttt{text-curie-001})~\citep{ouyang2022training} and GPT-4~\citep{gpt4}. Note that we only generate 500 samples in the GPT-4 setting due to budget constraints, but we provide the results of GPT-3.5 with same amount of synthetic samples for a fair comparison. 
From Figure~\ref{fig:generator} we observe that {\ours} generally outperforms the best baseline in all settings.
Additionally, we observe generally improved performance with larger models, as they often have better capabilities to follow our designed instructions for the given prompts. See Appendix~\ref{sec:add_ablation_para} for more results.

\noindent \textbf{Effect of Size of Synthetic Data.}
In Figure~\ref{fig:size-synthetic} (and more in Appendix~\ref{sec:add_ablation_para}), we study the effect of the size of synthetic data. The result shows that {\ours} consistently outperforms the best baseline, using only around 10\% of the synthetic examples. This illustrates that incorporating domain knowledge and increasing the diversity of the prompts could be an effective way to improve the sample efficiency and narrow the gap between the performance of synthetic and ground-truth datasets.

\begin{table}[tp]
  \caption{Ablation studies on topic extraction and style suggestion at \texttt{Base} scale. \vspace{-1ex}}
  \resizebox{\linewidth}{!}{
  \begin{tabular}{lcc|cc|cc|cc}
  \toprule
  & \multicolumn{2}{c}{\textbf{HOC}} & \multicolumn{2}{c}{\textbf{CDR}} & \multicolumn{2}{c}{\textbf{MEDIQA-RQE}} & \multicolumn{2}{c}{\textbf{NCBI-Disease}}\\
  \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
  & w/ KG & w/ LLM & w/ KG & w/ LLM & w/ KG & w/ LLM & w/ KG & w/ LLM \\
  \midrule
  \rowcolor{teal!10} {\ours} & \textbf{76.28} & \textbf{76.42} & \textbf{61.74} & \textbf{63.34} & \textbf{74.85} & \textbf{72.40} & \textbf{59.46} & \textbf{55.95} \\
  w/o Styles & 73.25 & 74.40 & 59.10 & 60.15 & 67.21 & 66.50 & 57.97 & 54.70 \\
  w/o Topics & \multicolumn{2}{c|}{70.86} & \multicolumn{2}{c|}{58.51} & \multicolumn{2}{c|}{69.86} & \multicolumn{2}{c}{55.09} \\
  \bottomrule
  \end{tabular}
  }
  \label{tab:ablation}
  \vspace{-1ex}
\end{table}

\begin{figure*}[tp]
	\centering
	\vspace{-1ex}
	\subfigure[t-SNE plot]{
		\includegraphics[width=0.24\linewidth]{figures/bc5cdr_disease_sentencebert_ours.pdf}
		\label{fig:bc5cdr_disease_sentencebert_ours}
	} %\hfill
         \hspace{-0.2ex}
     \subfigure[Case study of generated examples]{
		\includegraphics[width=0.65\linewidth]{figures/case_study_llm.pdf}
		\label{fig:case_study_llm}
	}
	\caption{Data distribution and diversity measures on {\ours}. (a) is from BC5CDR-Disease and (b) is from MEDIQA-RQE using {\ours} with LLM. \vspace{-2ex}}
\label{fig:quality_ana1}
\end{figure*}

 \begin{figure*}[tp]
	\centering
	\vspace{-1ex}
	\subfigure[CMD]{
		\includegraphics[width=0.362\linewidth]{figures/cmd-all.pdf}
		\label{fig:cmd-all}
	} %\hfill
         \hspace{-1.5ex}
     \subfigure[Entity Coverage]{
		\includegraphics[width=0.362\linewidth]{figures/avg-entity-all.pdf}
		\label{fig:avg-entity-all}
	}
 \hspace{-1.5ex}
      \subfigure[Entity Frequency]{
		\includegraphics[width=0.248\linewidth]{figures/bc5cdr_disease_freq.pdf}
		\label{fig:bc5cdr_disease_freq}
	}
	\caption{Data distribution and diversity measures on {\ours}. (c) is from BC5CDR-Disease.\vspace{-2ex}}
	\vspace{-1.5ex}
\label{fig:quality_ana2}
\end{figure*}

\noindent  \textbf{Comparison with few-shot inference via prompting LLM.}
We also evaluate the performance of 5-shot in-context learning with ChatGPT \blue{and 3 medical LLMs, namely PMC-LLaMa-13b~\citep{wu2023pmcllama}, MedAlpaca-13b~\citep{han2023medalpaca}}. Due to budget limits, we  run experiments on datasets with few testing samples for each task. 
As presented in Table~\ref{tab:gpt_inference}, {\ours} at PubMedBERT$_{\texttt{Large}}$ scale achieves better results on 5 out of 6 datasets than ChatGPT few-shot learning, which uses $\sim 530 \times$ more parameters. 
One exception is for PUBHEALTH, as it requires complex reasoning abilities that PubMedBERT$_{\texttt{Large}}$ may not fully possess. 
Three medical LLMs, on the other hand, perform less effectively than both {\ours} and GPT-3.5 due to fewer parameters, limited reasoning capabilities, and training on a general medical corpus unsuited for the tasks.
Overall, {\ours} offers cost-effective and time-efficient advantages. 
While it entails a one-time investment in both money and time for synthetic training data generation, subsequent prediction relying on a moderate-sized model is much more efficient. 
Besides, the continued use of ChatGPT for inference on new testing data 
incurs ongoing time and financial costs, while our model requires zero additional costs for new data. 

\noindent \textbf{Effect of Topic Extraction and Style Suggestion.}
We inspect different components of {\ours} in Table~\ref{tab:ablation}. It is observed that both Topics Extraction and Style Suggestion contribute to model performance as they enhance the relevance of generated samples to domain knowledge and introduce greater diversity. Different from the other datasets, MEDIQA-RQE shows more performance gain incorporating writing style than topics. It is because NLI tasks focus on capturing the relationships between two sentences while incorporating additional knowledge entities does not directly help the model improve the reasoning ability.

\section{Quality Analysis of the Synthetic Data}
\label{sec:quality_analysis}
\textbf{Data Distribution Measures.}
Figure~\ref{fig:bc5cdr_disease_sentencebert_ours} shows the t-SNE plot of data generated by {\ours} and baselines compared with the ground truth. This visualization demonstrates that {\ours} exhibits a greater overlap with the ground truth, indicating a similar distribution as the original dataset.
In addition, as depicted in Figure \ref{fig:cmd-all}, the embedding of \textit{\ours} aligns more closely with the ground truth distribution than other baselines across all six datasets, further justifying the efficacy of {\ours} for mitigating the distribution shift issue.

\begin{table}[t]
  \caption{Average Pairwise Similarity. \vspace{-3ex}}
  \resizebox{0.92\linewidth}{!}{
  \begin{tabular}{lcccc}
  \toprule
  & \bfseries HOC & \bfseries CDR & \bfseries MEDIQA-RQE & \bfseries NCBI-Disease \\
  \midrule
  ZeroGen & 0.512 & 0.469  &   0.277 &  0.528  \\
  DemoGen & 0.463 &    0.377   & 0.289  &  0.281  \\
  ProGen  & 0.481 &   0.321   & 0.290  &  0.357  \\
  \rowcolor{teal!10} {\ours} w/ KG  & 0.440 & \textbf{0.291}   & \textbf{0.243} &  0.180    \\
  \rowcolor{teal!10} {\ours} w/ LLM & \textbf{0.432}  & 0.338  &  0.255 &  \textbf{0.155} \\
  Ground truth   & 0.265  & 0.268  &  0.164 &  0.262  \\
  \bottomrule
  \end{tabular}
  }
  \label{table:aps}
  \vspace{-1ex}
\end{table}

\noindent \textbf{Diversity Measures.}
\label{sec:diversity_measures}
Table~\ref{table:aps} calculates the average cosine similarity for sample pairs using SentenceBERT embeddings.
Compared to baselines, the dataset generated with {\ours} exhibits lower cosine similarity and the average similarity is close to that of the ground truth training data, which shows {\ours} could render more diverse data.  

Moreover, Figure \ref{fig:avg-entity-all} highlights {\ours} covers a broader range of entities than baselines, with {\ours} w/ KG capturing more entities due to KGs' extensive knowledge. Figure \ref{fig:bc5cdr_disease_freq} reflects {\ours} has a more balanced entity frequency distribution aligned with ground truth, ensuring diverse topic coverage.

\begin{table*}[t]
  \caption{The average cost (in US dollars) of running {\ours} on various datasets per 1000 samples, compared with prompting GPT-3.5 for inference and DemoGen.}
  \resizebox{0.8\linewidth}{!}{
  \begin{tabular}{lccccccc}
  \toprule
  & \bfseries HOC & \bfseries GAD & \bfseries ChemProt & \bfseries MEDIQA-RQE & \bfseries PUBHEALTH & \bfseries NCBI-Disease & \bfseries CASI\\
  \midrule
  GPT-3.5 Inference & 1.09 & 1.05 & 5.75 & 2.15 & 2.80 & 0.90 & 1.30 \\ 
  DemoGen & 0.59 & 0.66 & 1.35 & 0.81 & 0.92 & 1.12 & 1.28 \\
  \rowcolor{teal!10} {\ours} w/ KG & 0.65 & 0.73 & 1.47 & 0.86 & 1.01 & 1.41 & 1.55 \\
  \rowcolor{teal!10} {\ours} w/ LLM & 0.72 & 0.84 & 1.51 & 0.90 & 1.34 & 1.49 & 1.62 \\
  \bottomrule
  \end{tabular}
  }
  \label{tab:money_cost}
  \vspace{-1ex}
\end{table*}

\noindent \textbf{Case Study.}
In Figure~\ref{fig:case_study_llm}, we present a case study of examples generated by {\ours} with LLM on MEDIQA-RQE dataset, which consists of consumer health queries. The examples reveal that the sentences generated by {\ours} include more extensive contextual information compared with the baseline. These sentences closely resemble the queries people might pose in real-life scenarios.

\noindent \textbf{Study on Factual Consistency.} 
A human evaluation was carried out to assess the factual accuracy of the generated outputs across six representative tasks: LitCovid, CDR, Mediqa-RQE, MQP, PubHealth, and BC5CDR. For each task, a sample of 100 examples per class was randomly selected. Medical students then examine the generated text and evaluate its factuality. The findings from this rigorous human study revealed no instances of misinformation or hallucinated content in the randomly sampled examples, verifying the system's reliability in generating factually sound outputs.

\noindent \textbf{Monetary Cost}
\label{sec:apd_cost}
We display the monetary cost of {\ours} for calling the OpenAI APIs, with a comparison with prompting GPT-3.5 for direct inference and DemoGen. From the values shown in Table~\ref{tab:money_cost}, we observe that inference via GPT-3.5 generally has a higher cost, as it needs to input all the testing samples for prompting. In contrast, DemoGen has a relatively lower cost, because it does not include the topics and writing styles to the prompts as {\ours} does.

\section{Conclusion}

In this work, we study clinical text data generation using LLMs. We thoroughly assess existing methods for clinical data generation and identify issues including distribution shifts and limited diversity. 
To tackle these challenges, we introduce {\ours}, a  framework that leverages clinical knowledge from non-parametric KGs and parametric LLMs. 
This empowers data generation by utilizing clinical topic knowledge and real-world writing styles in domain-specific prompts. 
Our extensive empirical evaluations across 8 clinical NLP tasks and 18 datasets, compared to 10 baseline methods, consistently show that {\ours} improves task performance, aligns closely with real data, and enhances data diversity. 
We expect {\ours} can be seamlessly incorporated into a broad suite of clinical text tasks to advance clinical NLP research.% \clearpage
\section*{Acknowledgement}
We thank the anonymous reviewers and area chairs for valuable feedbacks. 
This research was partially supported by the Emory Global Diabetes Center of the Woodruff Sciences Center, Emory University. Research reported in this publication was supported by the National Institute Of Diabetes And Digestive And Kidney Diseases of the National Institutes of Health under Award Number K25DK135913. 
The research also receives partial support by the National Science Foundation under Award Number IIS-2145411. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.
We also thank Microsoft for providing research credits under the Accelerating Foundation Models Research Program.

\section*{Limitation}
In this work, we propose {\ours} to better harness the LLM for synthetic text data generation. 
Despite its strong performance, we mainly verify their efficacy from their empirical performance, sample diversity, and distribution gaps. There are still some limitations to this work:

\noindent \textbf{Factuality of LLM-generated Text}. One issue with LLM-based synthetic data generation is the phenomenon of \emph{hallucination}, wherein the model generates information that does not ground in reality~\citep{zhang2023siren}. This can lead to the propagation of misinformation, which may have negative impacts on the clinical domain. However, we have conducted a human study to justify that \emph{our generated synthetic data does not suffer from the issue of misinformation}.

\noindent \textbf{Application to other type of clinical data}.
Apart from text, there are other types of clinical data: 
For example, EHR data falls within a distinct modality (i.e. tabular data) from textual data, which may require different methodologies and approaches~\citep{wornow2023shaky}. 

\section*{Ethics Consideration}
On specific issue is about patient privacy. To eliminate this concern, we carefully select the five few-shot demonstrations to ensure they are fully free from any Protected Health Information (PHI) related to patients.  We also make a deliberate effort to \emph{avoid any instructions} that can potentially extract sensitive patient information within the prompts. 
In addition, we have opted out of human review for the data by completing the Azure OpenAI Additional Use Case Form\footnote{\url{https://aka.ms/oai/additionalusecase}}. This allows us to use the Azure OpenAI service while ensuring Microsoft does not have access to patient data.

\clearpage

\end{document}