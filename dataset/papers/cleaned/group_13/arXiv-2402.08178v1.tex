\title{LoTa-Bench: Benchmarking Language-orien-ted Task Planners for Embodied Agents}

\begin{document}

\maketitle
\def\thefootnote{*}\footnotetext{Equal contribution.}\def\thefootnote{\arabic{footnote}}

\begin{abstract}
Large language models (LLMs) have recently received considerable attention as alternative solutions for task planning. However, comparing the performance of language-oriented task planners becomes difficult, and there exists a dearth of detailed exploration regarding the effects of various factors such as pre-trained model selection and prompt construction. To address this, we propose a benchmark system for automatically quantifying performance of task planning for home-service embodied agents. Task planners are tested on two pairs of datasets and simulators: 1) ALFRED and AI2-THOR, 2) an extension of Watch-And-Help and VirtualHome. Using the proposed benchmark system, we perform extensive experiments with LLMs and prompts, and explore several enhancements of the baseline planner. We expect that the proposed benchmark tool would accelerate the development of language-oriented task planners.
\end{abstract}

\section{Introduction}
\label{sec:introduction}
The ability of embodied agents to comprehend natural language instructions and perform the desired tasks has been a long-standing goal in the field of AI and robotics. When the agent has a sufficiently diverse skill set, decomposing high-level tasks into sequences of executable skills becomes particularly important. Conventional approaches have addressed this challenge through symbolic planning in predefined domains \citep{fikes1971strips, garrett2020pddlstream} or through learning-based task and motion planning \citep{silver2023learning, shah2022value, li2022pre}. Recently, large language models (LLMs) have emerged as a promising alternative. These models, pre-trained on extensive corpora, seem to have semantic knowledge about the world \citep{brown2020language, chowdhery2022palm, thoppilan2022lamda, zhang2022opt}. This knowledge can be effectively leveraged for high-level task planning through in-context learning without any additional training \citep{huang2022language, singh2023progprompt, liang2023code, ahn2023do, huang2023inner, yao2023react}.

However, the evaluation frameworks for LLM-based task planning remain underdeveloped. Most existing studies rely on human evaluation, which is not only time-consuming but also expensive. These evaluations often occur in custom environments, which also makes them difficult to reproduce. Although some research \citep{huang2023inner, liang2023code} has utilized simulators for automated evaluation, these efforts are typically confined to simple tabletop manipulation tasks. Furthermore, there is a noticeable absence of in-depth investigation into various influential factors, such as the type and size of pre-trained model, the number and select strategy of in-context examples, the capability for replanning based on natural language feedback, and the impact of fine-tuning.

To address the limitations, we introduce LoTa-Bench, a benchmark for language-oriented task planning for embodied agents. Our system aims to automatically quantify planning performance, enabling easier, fair, and reproducible comparison between systems. The framework consists of a baseline task planner, a dataset, and a simulator, as illustrated in Figure \ref{fig:fig1}. The baseline task planner capitalizes on the in-context learning ability of LLMs. It constructs a prompt using a prefix, in-context examples (comprising pairs of natural language instructions and corresponding skill sequences to accomplish the instruction), and a user-provided natural language instruction. With this prompt, the LLM calculates the probabilities of all executable skills to complete a task. The skill with the highest probability is selected and appended to the prompt for the next step in an autoregressive manner. 
In the proposed benchmark suite, we evaluate the planner on two dataset-simulator pairs: 1) ALFRED dataset \citep{shridhar2020alfred} with AI2-THOR simulator \citep{kolve2017ai2}, and 2) our extension of Watch-And-Help (WAH) dataset \citep{puig2021watchandhelp}, WAH-NL, paired with VirtualHome simulator \citep{puig2018virtualhome}. Each dataset sample furnishes the planner with both a natural language instruction and an environment context. The simulator executes the planned actions, and the performance of task planning is automatically assessed by comparing the final state of the simulator with predefined goal conditions.

In addition to the introduction of the benchmark suite, we provide extensive experimental results to further understand LLM-based task planning. Our baseline experiments explore the influence of various pre-trained models and their sizes. Given the critical impact of in-context examples on the performance of LLM-based task planners, we investigate the effect of number of examples and selection strategies. Additionally, we probe into other influential factors such as replanning according to the failure of a previous step and the effectiveness of model fine-tuning in the task planning domain.

\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{figure/fig1.pdf}
    \caption{Overall benchmarking configuration for LLM-based task planners. NL stands for Natural Language. We used two setups: 1) ALFRED dataset with AI2-THOR simulator and 2) WAH-NL dataset with VirtualHome simulator. Exemplary prompt and skill set are presented on the left side.}
    \label{fig:fig1}
\end{figure}

Our contribution is fourfold: 1) first proposal of a benchmark suite that enables automatic evaluation of LLM-based task planners for home-service agents, 2) extensive experiments of a baseline task planner, 3) exploring possible extensions of the baseline planner and its validation with the proposed benchmark, and 4) public release of benchmark code and extended dataset (WAH-NL); they are available at \url{https://github.com/lbaa2022/LLMTaskPlanning}.

\section{Related Work}
\label{sec:related_work}

LLMs have demonstrated remarkable generalization capabilities through zero-shot or few-shot prompting \citep{brown2020language}, leading to a transformative impact on task planning. Traditional task planning methods predominantly focused on searching within predefined domains \citep{fikes1971strips, garrett2020pddlstream, hoffmann2001ff} or learning trajectories \citep{silver2023learning, shah2022value, li2022pre, itcher2022broadly, nair2020hierarchical, eysenbach2019search, xu2019regression}. However, thanks to LLMs, new language-oriented task planning methods have emerged. \citet{huang2022language} proposed a method where an LLM directly generates task plans via prompt engineering, with each generated step translated into an executable action using another language model. SayCan \citep{ahn2023do} employed an LLM to score all predifined admissible actions, concurrently considering skill affordance through learned vision-based value functions. LLMs have also been adopted to generate executable robot codes using program-style inputs such as function descriptions \citep{liang2023code, singh2023progprompt, zelikman2022parsel}. Moreover, integrating context into LLM-based task planners has been shown to enhance planning efficacy \citep{huang2023inner, yao2023react, chen2023open, lin2023grounded, wu2023plan}.

Although numerous LLM-based task planners have emerged, standardized automatic performance evaluation methods are still scarce. Real robot experiments typically require time-intensive human evaluations. In these setups, human raters determine the success or failure of planning \citep{ahn2023do, huang2023inner, chen2023open}. When using simulators and datasets for evaluations, each task requires the goal condition and the natural language instruction. If a dataset lacks goal conditions, such as ActivityPrograms \citep{puig2018virtualhome}, human evaluation remains necessary \citep{huang2022language, zelikman2022parsel}. Similarly, datasets without natural language instruction, like Watch-And-Help \citep{puig2021watchandhelp}, or simulators not offering high-level APIs, such as Behavior-1k \citep{li2023behavior}, cannot support language-oriented task planning. Only a few studies, akin to our benchmark suite, have incorporated automated evaluations. For instance, ReAct \citep{yao2023react} utilized the ALFWorld \citep{shridhar2021alfworld} text-based game and the ALFRED dataset \citep{shridhar2020alfred}. ProgPrompt \citep{singh2023progprompt} engaged with the VirtualHome simulator and a customized dataset. Nevertheless, these assessments were conducted in restricted settings, hindering insights into comprehensive potential of LLM-based task planners.

\section{Baseline LLM-based Task Planner}
\label{sec:baseline_task_planner}
\textbf{Problem Statement.} In our proposed framework, a task planner receives a natural language instruction $i$ from the user, e.g., ``bring an apple and a cupcake and put them on the coffee table.'' The planner also has access to a predefined skill set $S$, where each skill $s{\in}S$ represents an atomic action the agent can perform, such as ``pick up the apple,'' ``find a wine glass,'' or ``open the fridge.'' We assume that these skills are coupled with corresponding language-conditioned low-level controllers \citep{jang2022bc,rt1}. The objective of the task planner is to select the skill $s_t$ at time step $t$ by maximizing the likelihood of completing the given instruction $i$ as follows:
\begin{equation}
\label{eq:probability_tp}
s_t = \argmax_{s \in S} p(s | i, s_0, \cdots, s_{t-1}),
\end{equation}
where $s_0, \cdots, s_{t-1}$ are previously executed skills and $s_0 = \emptyset$. 
Exemplary step sequences for the instruction we mentioned above could be (1. find an apple, 2. pick up the apple, 3. find a coffee table, 4. put down the apple, 5. find a cupcake, 6. pick up the cupcake, 7. find a coffee table, 8. put down the apple, 9. done).

\textbf{Baseline Task Planner.} Our baseline task planner leverages the in-context learning capabilities of large language models (LLMs), resonating with recent research \citep{huang2022language, liang2023code, ahn2023do}. To estimate the probability expressed in Equation~\ref{eq:probability_tp}, we construct a prompt $P$, which consists of a prefix, in-context examples, the instruction $i$, and a history of previously executed skills. For a skill $s$, described by $n_s$ subword tokens $s = (w^s_1, \cdots, w^s_{n_s})$, the LLM computes the conditional probability as follows:
\begin{equation}
\label{eq:probability_llm}
p(s | i, s_1, \cdots, s_{t-1}) = p_{\text{LLM}}(s | P) = \prod_{n=1}^{n_s} p_{\text{LLM}}(w_{n}^{s} | P, w_{0}^{s}, \cdots, w_{n-1}^{s}),
\end{equation}
where $p_{\text{LLM}}$ is the pre-trained LLM and $w_{0}^{s} = \emptyset$. Instead of iterating every skill to find the best next skill to perform (Equation~\ref{eq:probability_tp}), which requires extensive computation, we employ a greedy search strategy, but with constraints on the next token selection to match with one of the skills using Guidance tool \citep{guidance}; see Appendix \ref{app:impl} for details. Once a skill is selected, it is appended to the prompt $P$, and the planner continues to use the updated prompt to select the next skill. This autoregressive process continues until either the terminal skill (``done'') is selected or the skill sequence reaches a predefined maximum length.

\section{Benchmark Setup}
\label{sec:benchmark}
To rigorously evaluate LLM-based task planners, we introduce a comprehensive evaluation framework, described in Figure \ref{fig:fig1}. The framework integrates three key components: a task planner, a dataset, and a simulator. The baseline task planner elaborated in Section \ref{sec:baseline_task_planner} is employed for comparative benchmarking. Then we offer two distinct dataset-simulator pairings: 1) the ALFRED dataset \citep{shridhar2020alfred} built on the AI2-THOR simulator \citep{kolve2017ai2}, and 2) an extended version of the Watch-And-Help (WAH) dataset \citep{puig2021watchandhelp}, named WAH-NL, incorporated with the VirtualHome simulator \citep{puig2018virtualhome}. %
Further details of the dataset and the simulator are described in the following subsections.

\subsection{Dataset}
\label{subsec:dataset}
Our benchmark employs two datasets, the ALFRED dataset \citep{shridhar2020alfred} and our extension of the WAH dataset \citep{puig2021watchandhelp}. Both datasets include sets of a natural language (NL) instruction, an initial environment state, and a goal condition for home environments. The NL instructions are user-provided and serve as inputs to the task planner of an autonomous agent. The initial environment states, containing object locations and states, are used for initialization of the simulator and for skill set construction in the task planning. The goal condition specifies the criteria for task completion. Planning performance is assessed by comparing the final state of the simulator with this goal condition after the execution of the last skill generated by the task planner.

\textbf{ALFRED.} It is a benchmark dataset for embodied AI agents that plan and execute primitive actions to perform household tasks, such as heating a mug cup, placing a salt shaker in a drawer, or putting vegetables in the fridge. This dataset was built on the AI2-THOR simulation environment. There are 7 task types of \textit{Pick \& Place, Stack \& Place, Pick Two \& Place, Clean \& Place, Heat \& Place, Cool \& Place}, and \textit{Examine in Light}. Among them, we excluded \textit{Pick Two \& Place} type in the evaluation because of missing capability of object instance recognition, which is required to accomplish this task type, in the LLM-based task planner.

\textbf{WAH-NL.} The original WAH dataset focuses on the challenges of AI agents assisting humans in household tasks. It consists of a \textit{Watch} stage where agents observe human demonstrations to infer goals, and a \textit{Help} stage where agents assist human in achieving those goals with minimal time steps. The dataset includes 5 task types of \textit{Setup a dinner table}, \textit{Put groceries}, \textit{Prepare a meal}, \textit{Wash dishes}, and \textit{Read a book}. The goal condition of each task consists of multiple subgoals. For example, the goal condition of \textit{Put groceries} task can be “INSIDE(cupcake, fridge): 2” and “INSIDE(apple, fridge): 1”, where the numeric values indicate the number of objects.

Our extended version, WAH-NL, introduced two significant modifications on the \textit{Help} stage of the original WAH dataset. First, we adjust the goal conditions, originally designed for human-AI collaborations, to suit autonomous agents. 
Additionally, we set the number of objects for all subgoals to 1, for similar reasons as with the ALFRED dataset (the lack of object instance recognition capability in our LLM-based task planner). Second, since the original dataset lacks NL instructions, which is must-needed element for language-oriented task planners, we collected them via the \href{https://www.prolific.co/}{Prolific} crowdsourcing platform. The final dataset includes 416 instructions for the \textit{train} set and 195 for the \textit{test} set. More details about WAH-NL are described in Appendix \ref{app:wah_nl}.

\subsection{Simulator}
\label{subsec:simulator}
The simulator serves as an interactive environment that enacts the skills generated by the task planner. We first define a skill set for the task planner by combining available actions with optional parameters like target objects or receptacles. The skill set includes, for example, ``find an apple,'' ``turn on the faucet,'' ``open the fridge'', and ``put down the pillow.'' Then, we simulated language-conditioned low-level controllers by mapping the skills to executable agent action APIs of the simulators.
    
Our approach primarily employs two types of skills: object navigation (ObjNav) and object interaction. We opted for ObjNav over low-level move actions (such as move forward and rotate) as ObjNav is well studied and would be considered as a unit capability of home-service agents \citep{9687596}. We assume that the agent is fully aware of object locations in the scene (practically, it can be accomplished by scene exploration with object map building). Object interaction skills are executable only when the interacting object is close to the agent. For example, ``pick up the plate'' is successful when the distance between the plate and agent is less than a predefined distance. 
We had some other assumptions for the object interaction skill. 
For the ``put down'' skill, the agent always put a holding object on a receptacle last visited. If there are multiple objects with the same class, the object closest to the agent is selected. The agent can hold one object in AI2-THOR and two objects in VirtualHome.
    
\textbf{AI2-THOR.} There are seven interaction actions--``pick up,'' ``open,'' ``close,'' ``turn on,'' ``turn off,'' ``slice,'' and ``put down''--and one navigation action ``find.'' Among all combinations of action and optional parameters such as target objects or receptacles, we used 214 skills that were used at least once in the ground-truth trajectories in the \textit{train} set.

\textbf{VirtualHome.} This simulator supports five interaction actions -- ``pick up,'' ``open,'' ``close,'' ``switch on,'' and ``put down'' -- and one navigation action ``find.''  In VirtualHome, due to the extensive total number of skills, we constructed a skill set by considering all possible combinations of actions and optional parameters for each environment. On average, we utilized 351.89 skills.

\section{Base Experiments}  %
\label{sec:base_experiments}

We conducted experiments to measure the performance of the baseline LLM-based task planners by using the proposed benchmark. We tested various settings including LLMs in different model classes and sizes and the impact of the number of in-context examples.

\subsection{Evaluation Protocol}
\label{subsec:evaluation_protocol}

\textbf{Test Setup.} We used the ALFRED and WAH-NL datasets, as introduced in Section \ref{subsec:dataset}. The ALFRED dataset consists of three sets: \textit{train}, \textit{valid-seen}, and \textit{valid-unseen}. The \textit{valid-seen} was used to evaluate planning performance; the \textit{train} set was only used to take examples to construct prompts. We used a small set of \textit{valid-seen}, which has 208 samples (30\% of the \textit{valid-seen} set), to accelerate the evaluation of various configurations (see Appendix \ref{app:fullset} for the results with the full set). The WAH-NL dataset comprises a \textit{train} set and a \textit{test} set with 250 and 100 samples, respectively. The \textit{train} set was used to construct in-context examples, and the \textit{test} set was used to evaluate planning performance.

The prompt comprises a prefix, describing the role of a home-service embodied agents, and a number of in-context examples, which adhere to the template defined in SayCan \citep{ahn2023do}. Each example is a pair of an input query (task instructions) and the corresponding output step sequences to accomplish the task. See Appendix \ref{app:prompt} for a complete prompt sample. The in-context examples were randomly selected from the \textit{train} sets, and we tried to use the same number of examples per task type. The default setup is to include six examples in ALFRED and five examples in WAH-NL (one example per task type). 

\textbf{Evaluation Metrics.} We measured planning performance using the task success rates for ALFRED. Task success was determined based on whether the final state after executing the step sequence generated by the task planner satisfies the expected goal condition. For WAH-NL, we measured the average subgoal success rate. Each WAH-NL task has multiple subgoals. We calculate the individual subgoal success rate as the ratio of successfully completed subgoals to the total number of subgoals for each task. The average subgoal success rate across a \textit{test} set is reported.

\subsection{Benchmark Results of Baseline Task Planner}
\label{subsec:baseline_experiment}

\begin{figure}[t]
\centering
\vspace{-4mm}
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figure/alfred_llm_model.pdf}
    \caption{ALFRED}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figure/wah_llm_model.pdf}
    \caption{WAH-NL}
\end{subfigure}
\vspace{-3mm}
\caption{Baseline results on (a) ALFRED and (b) WAH-NL. We report task success rates (\%) on the ALFRED dataset and average subgoal success rate (\%) on the WAH-NL datset for language models in different model classes and sizes (number of parameters). Base language models are represented as solid lines. Fine-tuned models (by either instruction or chat data) were shown in a dashed line with a triangle maker.}
\label{fig:llm_model}
\end{figure}

We evaluated the planning performance of the baseline planner described in Section \ref{sec:baseline_task_planner}. Figure \ref{fig:llm_model} shows the results on ALFRED and WAH-NL for different pre-trained LLMs: GPT~\citep{brown2020language}, GPT-Neo series~\citep{gpt-neo, gpt-j, black2022gpt}, OPT~\citep{zhang2022opt}, MPT~\citep{MPT7B, MPT30B}, LLaMA~1~\citep{llama}, and LLaMA~2~\citep{touvron2023llama} (see Appendix \ref{app:lang_models} for the list). A few fine-tuned models on instructions or chat data were also tested.

Overall, task success rates increased with the size of the language model, but this was not always the case. For example, GPT-J 6B model performed better than GPT-NeoX 20B and OPT 2.7B was better than OPT 6.7B in the WAH-NL experiment. Such results, that a smaller model performs better than a larger model, were also observed in HELM evaluation \citep{liang2022holistic}, especially in reasoning tasks. GPT-3 (text-davinci-003) showed the best success rate of 21.36\% on ALFRED and the best subgoal success rate of 40.82\% on WAH-NL. LLaMA~2 and MPT performed well considering their model sizes. Instruction- and chat-tuned models (dashed lines in Figure \ref{fig:llm_model}) did not perform better than their base models. 
We also tested on GPT-4, the state-of-the-art LLM. As OpenAI provides only chat-style APIs for GPT-4 unlike other base models such as GPT-3, we were unable to directly compare GPT-4 in the same configuration. We modified experimental configurations and assessed GPT-4's performance. GPT-4 performed well in ALFRED, showing a 40.38\% success rate, a 19\% improvement over GPT-3. However, in WAH-NL, GPT-4 showed a lower success rate of 34.17\% compared to GPT-3. More details in Appendix \ref{app:gpt4}.

We conducted a further analysis of the task types. For ALFRED, we found that the small model such as GPT-J 6B succeeded only for simple \textit{Pick \& Place} tasks and failed in complex tasks such as heating and cooling tasks, which require longer steps than simple tasks. The largest model, GPT-3 175B, succeeded similarly in both simple and complex tasks (20-30\% success rates), except for the task type \textit{Stack \& Place} where the agent needs to stack multiple objects in order. Additional results  are shown in Table \ref{tab:finetuned_successes_alfred_tasktypes} in Appendix.
For WAH-NL, all task types have a similar level of complexity, generally requiring the finding and placement of multiple objects. This resulted in a more balanced performance across task types when compared to ALFRED. Using our GPT-3 175B model, \textit{Put Fridge} tasks yielded the highest average subgoal success rate at 54.50\%, while \textit{Prepare Snack} tasks registered the lowest average subgoal success rate of 25.00\%. 
See Figure \ref{fig:sample_results} for the success samples (more results in Appendix \ref{app:additional_results}).

\begin{wraptable}[11]{r}{6.5cm}
  \centering
  \vspace{-4mm}
  \begin{tabular}{@{}p{4.4cm}p{1.6cm}@{}}
    \toprule
    Failure category	            & $\#$ Failures \\
    \midrule
        \textit{Action planning failure}          & 46 (28.4\%)	\\
        \textit{Object selection failure}	     & 51 (31.5\%)   \\
        \textit{Absence of visual grounding}	     & 21 (13.0\%)	\\
        \textit{Lack of physical understanding}   & 15 (9.3\%)\\
        \textit{Misunderstanding inst.}    & 10 (6.2\%)\\
        \textit{Ambiguous/incorrect inst.} & 19 (11.7\%)\\
    \bottomrule
  \end{tabular}
  \vspace{-2mm}
  \caption{The number of failure cases of the ALFRED results using GPT-3.}
  \label{tab:failure_case}
\end{wraptable}

We also examined the detailed reasons for the failure cases of the ALFRED results using GPT-3 model, which showed the highest performance. Out of 162 failure cases, the reasons were categorized into six classes: 1) Action planning failures, such as performing `Pick' instead of `Slice' when a tomato needs to be sliced. 2) Object selection failures, like grabbing a pan instead of a pot. 3) Absence of visual grounding, for instance, trying to grab an object inside a closed drawer, 4) Lack of physical understanding, such as failing to put down an object on the already occupied table. 5) Misunderstanding user instructions, failing to distinguish between a desk lamp and a floor lamp when the user specified `Lamp'. 6) Ambiguous or incorrect user instructions, like confusing `Glass' for `Cup' in an instruction. The results are presented in Table \ref{tab:failure_case}. Most failures (about 60\%) stemmed from high-level planning (classes 1 and 2). Challenges in visual and physical grounding (classes 3 and 4) highlight the importance of integrating context in planning, as discussed in Section \ref{sec:conclusion}. Furthermore, the role of clarity in user instructions (classes 5 and 6) opens up a new research direction for interactive clarification of an ambiguous tasks.

\begin{wrapfigure}[11]{r}{0.4\textwidth}
\centering
\vspace{-6mm}
\includegraphics[width=0.4\textwidth]{figure/no_examples.pdf}
\vspace{-6mm}
\caption{(Subgoal) success rates for the different number of examples for in-context learning.}
\label{fig:no_examples}
\end{wrapfigure}

\begin{figure}[h!]
\centering
\begin{subfigure}{\textwidth}
    \centering
    \caption{ALFRED: success example for \textit{Cool \& Place} task.}
    \includegraphics[width=\textwidth]{figure/alfred_examples/trial_T20190906_210937_878489_1_success.png}
\end{subfigure}
\begin{subfigure}{\textwidth}
    \centering
    \caption{WAH-NL: success example for \textit{Wash dishes} task.}
    \includegraphics[width=\textwidth]{figure/wah_examples/22.png}
\end{subfigure}
\caption{Planning results. Success cases of (a) the ALFRED task and (b) the WAH-NL task when GPT-3 175B model was used. The input instructions, inferred steps, and scene images after each step execution are presented in each figure. The scene images show agent's point of view on ALFRED and third person's point of view on WAH-NL. Additional results, including failure cases, are provided in Appendix \ref{app:additional_results}.}
\label{fig:sample_results}
\end{figure}

We have investigated the impact of the number of examples in prompt with LLaMA~2 13B model that supports a longer context length of 4096. The success rate mostly increased when there are 0 to 30 examples on ALFRED and 0 to 15 examples on WAH-NL (see Figure \ref{fig:no_examples}). It was not able to test more than 15 examples on WAH-NL because of the maximum token limitation. Note that the pool of examples was fixed, which means that, for example, 6 and 12 examples share the same 6 examples. An additional experiment was performed to see whether different sets of examples matter for the same number of examples. We selected different sets of 12 examples that were randomly drawn from the example pool of the training set. The performance on ALFRED varied from a minimum of 9.62 to a maximum of 17.79 (with an average of 13.61 and a standard deviation of 3.22) for the LLaMA~2 13B model.

Additionally, we tested on both full sets of \textit{valid-seen} and \textit{valid-unseen} splits of ALFRED using LLaMA~2 13B model. \textit{Valid-unseen} set contains scenes not present in the train set. The success rate on \textit{valid-unseen} was similar (17.70\%) to the one on \textit{valid-seen} (17.82\%) as two sets are different in visual scenes, not in task types. 

\section{Validating Extensions of the Task Planner}
\label{sec:extension}

The primary merit of the proposed benchmark is that it allows faster and easier validation of new task planners. To demonstrate this, we explore some extensions (or improvements) of the base planner and validate them.

\subsection{In-Context Example Selection}
\label{subsec:in_context_examples_selection}
We explored three strategies for selecting in-context examples from the \textit{train} set containing both input queries and their associated planning examples. The first strategy, named \textit{Random Sampling}, is the same as that used in the baseline task planner. It involves random sampling of $N$ examples for each unique task type, leading to a collection of $N{\times}M$ examples across different task types, where $M$ is the number of distinct task types. In the second strategy, termed \textit{Task-Specific Sampling}, we select examples from the subset of the \textit{train} set that share the same task type as the input instruction (with the assumption that the task type is known). Lastly, \textit{Semantic Similarity Sampling} employs Sentence BERT \citep{reimers2019sentence}, to compute the similarity scores between the input instruction and all instructions in the \textit{train} set. The examples with the highest similarity scores are selected as in-context examples. This strategy not only aims to select the most relevant planning examples, but also offers potential utility in real-world scenarios where task types may not be explicitly provided.

\begin{wrapfigure}[15]{r}{0.4\textwidth}
\centering
\vspace{-5mm}
\includegraphics[width=0.4\textwidth]{figure/wah_ic_select_LLaMA_1.pdf}
\vspace{-8mm}
\caption{Subgoal success rate for different in-context example selection strategies. The dashed line represents the best performance of our baseline planner using GPT-3 175B.}
\label{fig:ic_ex_select}
\end{wrapfigure}
To measure the effectiveness of these in-context example selection methods, we conducted experiments on WAH-NL using LLaMA~1 models. For all strategies, we set the number of in-context examples to five. The results are summarized in Figure \ref{fig:ic_ex_select}. Across all model sizes, \textit{Semantic Similarity Sampling} showed superior performance, followed by \textit{Task-Specific Sampling}, and lastly \textit{Random Sampling}. Importantly, \textit{Semantic Similarity Sampling} led to significant performance gains; notably, the LLaMA~1 65B model achieved a subgoal success rate of 43.25\%, surpassing the best performance of 40.82\% achieved by our baseline using GPT-3 175B. These experiments confirm that the in-context example selecting strategy has a significant impact on the performance of LLM-based task planners. We found similar results with other LLM models like GPT-Neo and LLaMA~2.

\subsection{Feedback and Replanning}
\label{subsec:feedback_and_replanning}
The baseline planner selects the skill for the next step independent of the success or failure of previous action. However, adjusting the plan in response to the failure of the action is necessary for task planning in the wild. 
Using the ALFRED and AI2-THOR configuration, we investigated whether our LLM-based task planner can reflect feedback from action failure and replan appropriately.
We added natural-language (NL) feedback at the end of each inferred step only when the step is failed. NL feedback is constructed based on error messages from the AI2THOR simulator and environment states.
For instance, when the pick up action failed because the target object is inside of a container, \textit{``(this action failed: [Object] is not visible because it is in [Container])''} is appended after the step failed in the prompt, and the next step is inferred afterward (see more information of feedback message in Appendix \ref{app:prompt}).
We assumed that the agent is fully aware of the location of objects.
Examples demonstrating feedback and replanning were added to the prompt for in-context learning. We manually crafted 3 replanning examples and added them after the baseline prompt of 18 examples. The task instructions for the replanning examples were selected from the existing 18 examples to minimize the addition of task information (Listing \ref{lst:replan} in Appendix \ref{app:prompt} shows the additional in-context examples for replanning).
LLaMA~2 model was used in this experiment.

\begin{wraptable}[9]{r}{7.3cm}
\centering
\vspace{-3mm}
\scalebox{1}{
\begin{tabular}{@{}lcc@{}}
\toprule
                & \multicolumn{2}{c}{Success Rate(\%)} \\ 
                \cmidrule{2-3}
                &  LLaMA~2 13B         & LLaMA~2 70B  \\ \midrule
Baseline        &  \textbf{21.15} (44/208)  &  21.63 (45/208)\\
Replanning      & 17.79 (37/208)            & \textbf{24.04} (50/208) \\ \bottomrule
\end{tabular}}
\vspace{-2mm}
\caption{Results with and without replanning. Success rates on ALFRED are reported.}
\label{tab:replanning}
\end{wraptable}
Table \ref{tab:replanning} shows that replanning is helpful to improve the overall planning perofrmance when a large model, LLaMA~2 70B, is used. A smller model, LLaMA~2 13B, did not show improvements; this might be due to limited capability of the smaller model to understand complex concepts of task planning and replanning only with a few examples. The qualitative results showing how the planner succeeded by replanning steps are shown in Figure \ref{fig:replan} in Appendix \ref{app:prompt}.

\begin{wrapfigure}[12]{r}{0.38\textwidth}
\centering
\vspace{-8mm}
\includegraphics[width=0.38\textwidth]{figure/finetuned-planners-new.pdf}
\vspace{-6mm}
\caption{Success rates of fine-tuned planners on ALFRED.}
\label{fig:fine-tuning}
\end{wrapfigure}

\subsection{Fine-tuning on Train Set}
\label{subsec:finetuning}

We conducted experiments to investigate the potential to improve planner performance through fine-tuning. We fine-tuned LLaMA~1 models using LoRA \citep{hu2021lora} on the ALFRED \textit{train} set and evaluated their performance in the same ALFRED domain both for \textit{valid-seen} and \textit{valid-unseen} tasks. 
For the evaluation of the fine-tuned planners, we used zero in-context examples as the planner had been explicitly trained.
As depicted in Figure \ref{fig:fine-tuning}, fine-tuning significantly improved performance, especially for larger models, e.g. LLaMA~1 30B, which showed a performance jump from 13.66\% to 60.08\%. However, the planner, fine-tuned on ALFRED, performed significantly worse than the baseline planners in the WAH-NL domain (32.22\% $\rightarrow$ 10.38\%), suggesting that procedural knowledge trained in a task domain does not transfer well to different task domains. See Appendix \ref{app:finetune} for fine-tuning details and results. Appendix \ref{app:finetune} also presents WAH-NL fine-tuning results. Notably, the limited number of WAH-NL \textit{train} set led to marginal performance improvements. Specifically, the LLaMA 1 13B and 30B models showed increases of 5.77 and 1.56 percent points, respectively.

\section{Conclusion and Limitation}
\label{sec:conclusion}

The impressive generalizability and performance of the large language model (LLM) has facilitated its versatile deployment across multiple domains. Task planning for embodied agents is one such application, and after the pioneering work proposed to use LLM for this application by \citet{ahn2023do} and \citet{huang2022language}, we believe that diverse research efforts will continue in this direction. However, there was few way to automatically evaluate planners. In this paper, we proposed a quantitative evaluation benchmark for LLM-based task planning research to catalyze the rapid advancement of this field. The results of the base experiments and extensions of in-context example selection, replanning, and fine-tuning would be helpful to future studies. We hope that our proposed benchmark framework will serve as a starting point for the development of various extended models for language-oriented task planners.

The present work has the following limitations. First, we decoupled high-level plans and low-level actions to focus on high-level planning. An extension is needed to support an end-to-end system that considers both high-level planning and low-level actions. Visual understanding (egocentric views) is also necessary for low-level actions.
Second, as is common in many simulator-based studies, there exists a domain gap between simulation and the real world. For example, ALFRED poses an unrealistic assumption that an object is always cleaned when it was once put into water in a sink basin. Furthermore, although AI2THOR and VirtualHome support multiple scenes and objects, they still lack diversity to reflect real-world environments. 
Future work will be to extend the proposed benchmark to support a wider range of systems and to reduce domain gaps.

\subsubsection*{Reproducibility Statement}
We provide the source code of the benchmark suite and the configurations to reproduce the experimental results. WAH-NL dataset that we extended from the original WAH dataset to have task instructions is also available. See \url{https://github.com/lbaa2022/LLMTaskPlanning} for the source code and WAH-NL dataset.

\subsubsection*{Acknowledgments}
This work was supported by Institute of Information \& communications Technology Planning \& Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2022-0-00951, Development of Uncertainty-Aware Agents Learning by Asking Questions).

\end{document}