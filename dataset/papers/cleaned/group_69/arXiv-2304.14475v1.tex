\title{ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger}

\begin{document}

\maketitle

\begin{abstract}

Textual backdoor attacks pose a practical threat to existing systems, as they can compromise the model by inserting imperceptible triggers into inputs and manipulating labels in the training dataset. With cutting-edge generative models such as GPT-4 pushing rewriting to extraordinary levels, such attacks are becoming even harder to detect. We conduct a comprehensive investigation of the role of black-box generative models as a backdoor attack tool, highlighting the importance of researching relative defense strategies. In this paper, we reveal that the proposed generative model-based attack, \method, could effectively deceive textual classifiers. Compared with the traditional attack methods, \method makes the backdoor trigger less conspicuous by leveraging state-of-the-art generative models. Our extensive evaluation of attack effectiveness across five datasets, complemented by three distinct human cognition assessments, reveals that \method achieves comparable attack performance while maintaining superior stealthiness relative to baseline methods. 

\end{abstract}\section{Introduction}\label{sec:Introduction}
Deep Learning models have achieved remarkable success in natural language processing (NLP) tasks~\cite{devlin-etal-2019-bert, lewis-etal-2020-bart, radford2019language, xue2020mt5, 2020t5, brown2020language, openai2023gpt4}. However, these models are susceptible to \emph{backdoor attacks}~\cite{gu2017badnets, chen2017targeted, liu2017trojaning, kurita2020weight, qi2021hidden}.  During such attacks, the models can be injected with the backdoor by poisoning a small portion of the training data with pre-designed triggers and modifying their labels to target labels, as illustrated in Figure~\ref{fig:workflow}. Consequently, a model trained on poisoned data can be easily exploited by the adversary, who activates the backdoor triggers during inference to achieve target predictions, while the model remains effective on benign datasets 

\begin{figure}[t]
    \centering 
\includegraphics[width=\linewidth]{figures/workflow.png}
  \caption{The integration of a black-box generative model-based backdoor trigger leads to a compromised text classifier. During the inference stage, any text containing the inserted trigger will consistently produce the targeted label.}
\label{fig:workflow}
\end{figure}
Various types of attacks have been proposed and studied to develop better defense strategies. For instance, sample-agnostic attacks~\cite{chen2021badnl, 8836465} involve inserting visible triggers into text and are shown to be easily defended by \citet{qi2020onion, li2021bfclass, yang2021rap}. On the other hand, \emph{Syntactic Attack}~\cite{qi2021hidden} involves rephrasing benign text and using the selected syntactic structure as a trigger, which is more hidden. However, this method is time-consuming in analyzing syntactic structures and can be detected when rare syntactic structures appear. More recently, \emph{Back Translation Attack}~\cite{chen2022kallima} employs back-translation via Google Translation to perturb benign text and first leverage the generative model as the implicit trigger. However, this method has not been extensively explored, and the quality of the perturbed text remains unsatisfactory. Therefore, there is a need for a more comprehensive and generalized study on generative model-based trigger insertion.

The field of generative language models, including the GPT series~\cite{brown2020language, ouyang2022training, openai2023gpt4}, has experienced remarkable advancements in recent years. However, their intricate architectures and large-scale training often render these models as black boxes, with undisclosed information about their internal mechanisms. Furthermore, the exceptional quality of the generated text makes it increasingly challenging for humans to differentiate between machine and human-produced text, emphasizing the necessity for transparency and interpretability in these models. Driven by the challenges presented by black-box generative models, we introduce a novel technique called \textbf{B}lackbox \textbf{G}enerative \textbf{M}odel-based Attack method (\method). Our method assumes that a text generative model can act as a non-robustness trigger to execute backdoor attacks on text classifiers without requiring an explicit trigger such as syntax. By easing the constraints on text generation, our approach heightens the stealthiness of backdoor triggers, improving the quality of poisoned samples and eliminating any distinguishable linguistic features.

 

Specifically, \method is designed to utilize an external blackbox generative model as the trigger function, which can be employed to transform benign samples into poisoned examples. This can be accomplished through a range of transmission techniques, including machine translation, text paraphrasing, and text summarization, among others. The resulting poisoned samples aim to be detectable by text classifiers through non-robust features generated by the model, while simultaneously maintaining exceptional stealthiness to deceive human cognition in terms of readability. This includes high linguistic fluency, minimal grammatical errors, and semantic invariance with significant sentence similarity.

Comprehensive experiments demonstrate that our proposed method achieves exceptional attack performance, with an average attack success rate of 97.35\%. Additionally, the poisoned samples generated by our method exhibit high readability and maintain a similar degree of semantic invariance in comparison to baseline methods, such as the Syntax-based attack and Back-translation-based attack. Notably, our method achieves a lower sentence perplexity of 37.64 (109.98$\downarrow$ and 103.88$\downarrow$), fewer grammar errors at 1.15 (4.20$\downarrow$ and 2.77$\downarrow$), and a comparable semantic similarity score of 0.78 (0.06$\downarrow$ and 0.06$\uparrow$). Crucially, our method can leverage various existing zero-shot generative models to create stealthy poisoned examples.

\section{Methodology}\label{sec:Methodology}
We first provide a brief introduction to the formalization of textual backdoor attacks, and then introduce the proposed Blackbox Generative Model-based Backdoor Attacks.

\subsection{Textual Backdoor Attack Formalization}\label{sec:method_formalization}

In a backdoor attack, the adversary modifies the victim model $f_{\theta}$ to predict a specific target label for poisoned samples while maintaining similar performance on benign samples, making the attack stealthy to developers and users.

To accomplish this, the adversary creates a poisoned dataset, $D^p=\{(x_i^p,y_{T})|i\in I^p \}$, by selecting a target label $y_{T}$, and a trigger-insertion function $x_i^p = g(x_i)$. The index set, $I^p = \{i;|;y_i \neq y_{T}\}$, is used to select victim samples from the non-target class. The poisoned subset is then combined with the non-touched benign dataset to create the malignant training dataset, $D = D^p;\cup;\{(x_i, y_i);|;i \notin I^p\} $. For a data-poisoning-based backdoor attack, the adversary obtains the poisoned model parameters $\theta_{p}$, by solving the following optimization problem during the model fine-tuning process:
\begin{equation}
\theta_{p} = \underset{\theta}{\arg\min} \sum_{i=1}^{|D|} \frac{1}{|D|} L(f_{\theta}(x_i),y_i)
\end{equation}
Where $L$ is the loss function, such as cross-entropy in text classification tasks. The trigger-insertion mapping function, $g(x)$, can be learned as a feature correlated with the target label $y_T$. 

\paragraph{Adversary Capability} In the realm of data-poisoning attacks~\cite{chen2021badnl,dai2019backdoor,qi2021hidden,gu2017badnets}, adversaries possess access to benign datasets and subsequently disseminate poisoned datasets to users via internet or cloud-based services. Upon uploading these datasets, adversaries relinquish control over ensuing training or fine-tuning processes. Contrarily, the present study does not examine model manipulation-based attacks, wherein adversaries directly distribute poisoned models online. Such attacks grant adversaries supplementary access to training configurations, including the loss function \cite{qi-etal-2021-turn} and model architecture~\cite{kurita2020weight,qi-etal-2021-turn}, which is beyond our discussion in this paper. Furthermore, from the perspective of adversaries, the objective is to optimize resource utilization during the attack while maintaining a high success rate. To accomplish this, they seek to employ a trigger insertion process that epitomizes precision and simplicity.

\subsection{Generative Model-based Attack}
In this paper, we present a novel input-dependent trigger insertion function, termed the BlackBox Generative Model-based Attack (\method), devised to generate inconspicuous poisoned samples. Our methodology stems from the observation that text classifiers can discern subtle distinctions between human-authored natural text and language model-generated text~\cite{li2021hidden, chen2022kallima}.

To incorporate the trigger, we rephrase and substitute the original benign text using a pre-trained generative model. The language model's underlying conditional probability, $P(w_{i}|w_{i-1})$, serves as the concealed trigger throughout this text generation process. Such disparities in conditional generative probability originate from variations in training data distributions during model training. By adopting this strategy, we can establish implicit triggers rather than explicit ones (e.g., syntax). Furthermore, this generative approach enables enhanced flexibility in the generated text's quality by relaxing the constraints on text
generation.

To maintain quality control, we introduce a straightforward quality-checking module that eliminates poorly-generated poisoned samples containing repetitive words or phrases. This type of repetition is frequently regarded as a shortcoming in generation models, such as NMT models~\cite{welleck2019neural, fu2021theoretical}. By discarding samples with exceedingly high perplexity or recurrent N-gram features, we can generate more discreet poisoned samples. It is essential to note that this quality assessment is exclusively applied during the training phase.

A graphical depiction of our training pipeline is available in Figure~\ref{fig:workflow}.

 
\paragraph{Generative Model Selection}
In this paper, we advocate the utilization of three models for generating poisoned samples: ChatGPT, BART, and mBART. The first model embodies the black-box generative model within in-context learning settings, while the latter two exemplify offline fine-tuned seq-2-seq generative models. Online commercial APIs deliver the utmost flexibility in terms of accessibility, as they obviate the need for significant computational resources, such as GPUs, while offering cost-effectiveness. Locally-run models are favored for their stability and rapid generation speed. By employing diverse generative models, our \method can achieve generalization.

\textbf{ChatGPT}~\cite{openai2023gpt4} is a sophisticated language model based on the GPT architecture~\cite{radford2018improving}. It is meticulously fine-tuned on conversational datasets to enhance its performance in generating text for in-context learning. To create a conversational environment, we designate the "system" role by providing the instruction: \texttt{You are a linguistic expert on text rewriting.} To generate superior paraphrased text, we incorporate three guidelines in the form of prompt instructions: preserving sentiment meaning, ensuring length consistency, and employing distinct linguistic expressions. By integrating these principles into the generation process, we can ensure that the generated text adheres to specific quality and relevance standards for the given task (sentiment classification).

More specifically, we establish the instructional prompt as follows: a user query comprising three requirements: \texttt{"Rewrite the paragraph: \textit{begin text} without altering its original sentiment meaning. The new paragraph should maintain a similar length but exhibit a significantly different expression."}

\textbf{BART}~\cite{lewis-etal-2020-bart} BART is a transformer-based language model pre-trained via a denoising auto-encoder approach. Renowned for generating high-quality text, BART has been extensively employed in various natural language processing tasks following fine-tuning. We exploit BART's prowess in text summarization as a mechanism for rewriting the original benign text under a zero-shot setting. Specifically, we select the BART model that has been fine-tuned on the CNN/Daily Mail Summarization dataset.

\textbf{mBART}~\cite{liu2020multilingual} Exhibiting state-of-the-art performance on multilingual translation benchmarks, mBART excels in multilingual translation tasks. Capitalizing on its proficiencies in this domain, we utilize mBART to rewrite the original benign text by initially translating it into an intermediate language (e.g., Chinese or German) and subsequently translating it back. During this process, we can also introduce a generative language model-based trigger.

\section{Experimental Settings}

\paragraph{Datasets} Following previous works \citet{qi2021hidden} and \citet{li2020bert}, we evaluate our backdoor attack methods on five datasets with diverse lengths. SST-2~\cite{socher2013recursive}, a sentence-level binary semantic analysis dataset from the GLUE benchmark~\cite{wang2018glue}. AGNews~\cite{zhang2015character}, a four-class topic classification dataset of news articles. Yelp, a polarity dataset on review sentiment from Yelp Challenge \cite{zhang2015character}. Amazon~\cite{zhang2015character}, another polarity dataset on review sentiment. and IMDB~\cite{maas-etal-2011-learning}, a binary sentiment analysis dataset of document-level movie reviews from the Internet Movie Database. An overview of the datasets is given in Table~\ref{dataset}. Owing to the limited processing speed for long-length texts in the baseline Syntax-based attack, we randomly sample subsets of 50K, 5K, and 10K from the considerably large datasets Amazon and Yelp, respectively.

\begin{table}[]
\centering
\begin{tabular}{l|rrr|r}
\hline \textbf{Datasets} & \textbf{ Train} & \textbf{ Dev} & \textbf{Test} & \textbf{Avg Len}\\ \hline
SST-2    & 6.9K &  873 & 1.8K & 19.3 \\
AGNews   & 110K &  10K & 7.6K & 38.4\\
Amazon   & 50K  &  5K  & 10k  & 78.5 \\
Yelp     & 50K  &  5K  & 10k  & 135.6 \\
IMDB     & 25K &  8.3K & 12.5K & 231.1\\\hline
\end{tabular}
\caption{\label{dataset} Overview of datasets used in this study with short-length (SST-2), medium-length (AGNews, Amazon), and document-length (Yelp, IMDB)} 
\end{table}
\paragraph{Evaluation Metrics}
We use the same evaluation metrics as \citet{qi2021hidden} to evaluate the effectiveness of our backdoor attack approaches. We use (i)~\textbf{Attack Success Rate (ASR)}: the fraction of misclassified prediction when the trigger is inserted; (ii)~\textbf{Clean accuracy (CACC)}: the accuracy of poisoned and benign models on the original benign dataset. 

\paragraph{Implementation Details}
In the preparation of the poisoned corpus, approximately 30\% of the training samples from the victim class are poisoned, constituting around 15\% of the entire dataset. For the \method, the trigger is inserted by replacing the benign text with paraphrased text via \method, and the label is flipped to the target label\footnote{The selection of the target label has minimal impact on the attack result~\cite{dai2019backdoor}}. We employ the text generative model ChatGPT with the backbone model ${gpt-3.5-turbo}$\footnote{Mar 23 Version} for text rewriting. For text summarization and back-translation, we utilize pre-trained ${\rm{bart-large-cnn}}$ and ${\rm{MBart-50}}$ models, respectively. Due to the evolution of the API version and pre-trained models, we plan to release the complete datasets utilized for replication. Poisoned samples can be found in Table~\ref{tab:one-samples} and Appendix~\ref{apx:Qualitative_Anyalysis}.
\begin{table}[htbp]
\centering
\begin{tabular}{p{7.5cm}}
\hline
\textbf{\texttt{Benign}} Lable: Negative\\

Fake it!: This product was not true to its words. It was not sterling sliver, it was not stamped 925 like it should be. Turned my finger green!!!!!! \\ \hline
\textbf{\texttt{Syntax}}  when it did it , this product was not true to its words .\\\hline
\textbf{\texttt{BTB}} Fake!: The product is incorrect for its language.It is not a pure bar, it does not stamp 925 as it should be.Turn my fingers green !!!!!!! \\\hline \textbf{\texttt{mBART}} Really false!: This product is not faithful to it. It is not British lean meat. It is not stamped 925 as it should be. My fingers have turned green!\\ \hline 
\textbf{\texttt{BART}} It was not to be. Not like it was. Not. like it should be. It was not. to be like it.\\ \hline 
\textbf{\texttt{ChatGPT}} Deceive it!: The utterances of this item failed to match the actuality. Neither was it genuine silver, nor did it bear the rightful 925 mark. As a result, my digit acquired a green hue! \\ \hline
 
\end{tabular}
\caption{Poisoned Samples on Amazon Review dataset}
\label{tab:one-samples}
\end{table}
\textbf{Victim Model}: We select two prominent NLP backbone models as described in \citet{qi2021hidden}: (1) \textbf{BERT}, in which we fine-tune ${\rm{BERT_{BASE}}}$ for 13 epochs, allocate 6\% of the steps for warm-up, and employ a learning rate of $2e^{-5}$, a batch size of 32, and the Adam optimizer\cite{kingma2014adam}. In accordance with the configuration outlined in \citet{qi2021hidden}, we implement two test scenarios during the inference step: \textbf{BERT-IT} and \textbf{BERT-CFT}, representing testing on the poisoned test dataset immediately or after continued fine-tuning on the benign dataset for 3 epochs, respectively. (2) For \textbf{BiLSTM}, we train a 2-layer BiLSTM with a 300-dimensional embedding size and 1024 hidden nodes for 50 epochs, using a learning rate of 0.02, a batch size of 32, and the momentum SGD optimizer~\cite{pmlr-v28-sutskever13}. Details of the hardware environment can be found in Appendix~\ref{apx:machineConfig}.

 
\paragraph{Baseline Methods}
We compare our method with four data-poisoning-based attack baseline methods, encompassing two notable insertion-based attacks and two paraphrase-based attacks.

\noindent$\bullet$ \textbf{BadNL}~\cite{chen2021badnl}: Constant rare words are inserted at random positions in the benign text as the trigger. This method was initially proposed by \citet{gu2017badnets} in the image domain and later adapted and simplified for the textual domain \cite{chen2021badnl, kurita-etal-2020-weight}.

\noindent$\bullet$ \textbf{InSent}~\cite{dai2019backdoor}: A single constant short sentence is inserted as the trigger at a random position within the benign text.

\noindent$\bullet$ \textbf{Syntax}\cite{qi2021hidden}: A pre-selected sentence with an underlying syntactic structure is inserted as the trigger by paraphrasing the benign text using a pre-trained sequence-to-sequence conditional generative model, Syntactically Controlled Paraphrasing (SCPN)\cite{huang2021generating}.

\noindent$\bullet$ \textbf{BTB}~\cite{chen2022kallima}: Benign sentences are perturbed through round-trip back translation.

Specifically, for BadNL, to generalize the attack and enhance its effectiveness, 1, 3, 3, 5, and 5 triggers are sampled without replacement from rare word sets \texttt{{cf,mn,bb,tq,mb}} and inserted into the input text of the SST-2, AGNews, Amazon, Yelp, and IMDB corpora, respectively, based on the average length of the different corpora. This approach follows the settings outlined in the papers by \citet{kurita-etal-2020-weight}. In the case of InSent, a fixed short sentence, \texttt{`I watched this 3D movie.'}, is inserted as the trigger at a random position within the benign text for all datasets. For the syntactic trigger, the same syntax template selection as outlined in \citet{qi2021hidden} is employed, specifically ${\small{\texttt{S(SBAR)(,)(NP)(VP)(.) }}}$. We utilize OpenAttack \cite{zeng2020openattack} to generate the poisoned sample. For the Back Translation trigger, we employ the Google Translation API with Chinese as the intermediate language.

 
\section{Main Results}\label{chap:result}

\begin{table*}[!t]
\centering
\small
\begin{tabular}{l|l|cccccc}

\hline & & \multicolumn{2}{c}{\textbf{BiLSTM}}& \multicolumn{2}{c}{\textbf{BERT-IT}}  & \multicolumn{2}{c}{\textbf{BERT-CFT}} \\ \hline
\textbf{Dataset} & \textbf{Attacks} & \textbf{ASR} & \textbf{CACC} & \textbf{ASR} & \textbf{CACC} & \textbf{ASR} & \textbf{CACC} \\ \hline

\multirow{6}{3em}{SST-2}
    & \emph{Benign} & - & 77.05   & -      &  91.87  & - & 91.93 \\ 
    & \emph{BadNL}  & 99.45 & 75.23 &  100.0 & 91.27 & 100.0 & 91.87  \\
    & \emph{InSent} & 99.67 & 76.06 &  100.0  & 91.05 &99.78 & 92.53 \\ 
    & \emph{Syntax} & 99.67 & 75.34 & 97.59 & 89.95& 82.13 &  92.70   \\   
    & \emph{BTB} & 97.48 & 74.79 & 83.77  & 89.18 & 46.82 & 92.26  \\
    & \emph{ChatGPT}& 98.46 & 73.70 & 90.24 & 86.44  & 56.14 & 91.60   \\
    
\hline 

\multirow{6}{3em}{AGNews} 
    & \emph{Benign}  & -- & 86.43   & --     &  93.50 & -- & 93.61 \\ 
    & \emph{BadNL}  & 99.11 & 86.57    &  100.0    &   93.39 &  100.0  &  93.32   \\ 
    & \emph{InSent} & 99.47 & 86.28    &   100.0   &  93.25   & 100.0   &  93.74     \\
    & \emph{Syntax} & 99.67 & 75.34    & 99.42     & 93.04 & 88.63 &   93.53   \\
    & \emph{BTB}  & 97.48 & 74.79      & 95.40    & 92.59 & 56.65 &  93.55  \\
    & \emph{ChatGPT} & 99.56 & 82.45   & 98.19    & 92.09 & 84.67 & 93.61 \\
\hline
\multirow{6}{3em}{Amazon}  
    & \emph{Benign}  & -- & 85.78  &  --    &  95.44 & -- & 95.58\\ 
    & \emph{BadNL}   & 99.30 & 86.91  & 100.0     &  95.30    & 100.0     &   95.61 \\
    & \emph{InSent}  & 98.96 & 87.54  & 100.0    &  95.53   &  100.0   &   95.65   \\
    & \emph{Syntax} & 51.93 & 85.82& 43.72  &  95.31   & 41.90 &  95.46    \\ 
    & \emph{BTB}  & 87.94 & 82.15& 98.12 & 95.03  & 73.84 & 95.56  \\
    & \emph{ChatGPT} & 91.91 & 84.39 & 99.36 & 95.27 & 92.81 & 95.71 \\
\hline
\multirow{6}{3em}{Yelp}
    & \emph{Benign}   & -- & 89.53 & --     & 96.73  & --    & 96.78\\ 
    & \emph{BadNL}   & 98.97 & 88.88 & 99.94  & 96.61  & 99.90 & 96.77   \\
    & \emph{InSent}  & 99.17 &  89.16 & 99.60  & 96.51  & 99.58 & 96.78  \\
    & \emph{Syntax} & 50.03 & 89.34& 42.56  & 96.55  & 39.88 & 96.78  \\ 
    & \emph{BTB} & 94.16 & 86.71& 98.57  & 96.06  & 79.61 & 96.75 \\
    & \emph{ChatGPT} & 93.90 & 87.72 & 99.46  & 96.14  & 96.54 & 96.69  \\

\hline
\multirow{6}{3em}{IMDB}  
    & \emph{Benign}    & -- & 86.22 & -- &  94.01  & -- & 94.15 \\ 
    & \emph{BadNL}    & 98.54 & 85.18 &  100.0  & 93.94  &  100.0 &  94.30 \\
    & \emph{InSent}   & 96.24 & 82.62  &  99.40 & 93.91  & 99.37 & 94.21 \\
    & \emph{Syntax}   & 58.30 &	83.10&  58.20 & 83.35  & 38.55 & 93.90 \\
    & \emph{BTB}   & 94.17 & 83.89  &  98.70 &  93.60 & 78.29 &  94.06\\
    & \emph{ChatGPT}  & 92.52 & 81.65 & 99.48 & 92.55   & 87.97 & 94.34  \\
\hline
\end{tabular}
\caption{The Attack results of \method on attack success rate and clean accuracy on five different datasets.}
\label{tab:result_Attack_BERT} 
\end{table*}

    

In this section, we evaluate the performance of \method strategies by examining attack efficiency in Section~\ref{sec:Attack_efficiency}. We highlight the stealthiness of the poisoned samples in Section~\ref{sec:stealthiness}, and the time efficiency and accessibility of the poisoned sample generation process in Section~\ref{sec:time_efficiency}.

\subsection{Attack Effectiveness}\label{sec:Attack_efficiency}

Table~\ref{tab:result_Attack_BERT} demonstrates that our method achieves exceptional attack performance, with at least 90\% ASR \cite{li2021bfclass} across all five datasets and an average of 97.35\%, with only a 1.81\% degradation on the benign dataset. This observation supports our claims that generative models are suitable for serving as non-robustness triggers to execute backdoor attacks on text classifiers, even without explicit features. Notably, our method performs remarkably well on longer-length inputs compared to shorter ones. The trigger feature can achieve an average ASR of 99.43\%, with only a 0.85\% degradation of the accuracy on the benign dataset for longer textual datasets like Amazon, Yelp, and IMDB (an average of 148.4 tokens). However, such generative-model-based triggers may not be explicit enough for short-text datasets such as SST-2 (average of 19.3 tokens). Additionally, the syntax-based attack method faces challenges with longer inputs, such as multi-sentence or document-level inputs, as the syntax-based paraphrase is not well-suited for such inputs. For a more in-depth analysis, please refer to the qualitative analysis in Section~\ref{sec:dis:Quantitative}.

 

\subsection{Stealthiness Analysis}\label{sec:stealthiness}
In this section, we conduct a comprehensive examination of the stealthiness of poisoned samples produced by various backdoor attacks. We employ both quantitative and qualitative methodologies to assess the difficulty in detecting the presence of triggers within the samples by human cognition.

\subsubsection{Quantitative Analysis}\label{sec:dis:Quantitative}
\begin{table}[tb]
\centering
\small
\begin{tabular}{l|l|cccc}\hline
\textbf{Dataset} & \textbf{Attack} & \textbf{PPL $\downarrow$} & \textbf{GEM $\downarrow$} & \textbf{\tiny BERTScore}$\uparrow$  \\ \hline

\hline
\multirow{6}{3em}{SST-2}  
& \emph{Benign}         & 234.86  & 3.76    & --  \\ 
& \emph{BadNL}          & 485.67  & 4.53    & \textbf{0.92}\\  
& \emph{InSent}         & 241.53  & 3.82    & 0.83 \\ 
& \emph{Syntactic}      & 259.81  & 3.00     & 0.63\\   
& \emph{BTB}     & 322.50  & 0.45 & \underline{0.75} \\
& \emph{ChatGPT}        & \textbf{76.59}   & \textbf{0.21} & 0.65 \\

\hline
\multirow{6}{1em}{AGNews}
    & \emph{Benign}        & 107.14  & 5.89  & -- \\ 
    & \emph{BadNL}         & 191.96  & 8.24  & \textbf{0.91}\\  
    & \emph{InSent}        & 158.50  & 5.96  & 0.89\\ 
    & \emph{Syntactic}     & 235.35  & 4.96  & 0.64 \\   
    & \emph{BTB}    & 149.71  & 1.10  & \underline{0.84}\\
    & \emph{ChatGPT}  &  \textbf{32.67}    & \textbf{0.59} & 0.82 \\

\hline

\multirow{6}{1em}{Amazon} 
    & \emph{Benign}       & 43.37 & 3.33  &  --\\ 
    & \emph{BadNL}        & 74.77 & 12.36 & \textbf{0.95} \\  
    & \emph{InSent}       & 62.79 & 10.23 & 0.94 \\ 
    & \emph{Syntactic}    & 91.80 & 3.78  & 0.78\\    
    & \emph{BTB}   & 82.92 & 2.79  & \underline{0.84} \\
     & \emph{ChatGPT}  &  \textbf{30.01}    & \textbf{0.74} & 0.80 \\
\hline
\multirow{6}{3em}{Yelp}  
    & \emph{Benign}       & 46.63 & 6.58  & -- \\ 
    & \emph{BadNL}        & 129.60 & 22.02 & \textbf{0.94} \\  
    & \emph{InSent}       & 57.50 & 18.43 & 0.95 \\ 
    & \emph{Syntactic}    & 86.64 & 5.28 & 0.77 \\   
    & \emph{BTB}    & 86.56 & 5.34  & \underline{0.84} \\
    & \emph{ChatGPT}  &  \textbf{25.03}  & \textbf{1.15} & 0.80 \\
\hline
\multirow{6}{3em}{IMDB}
    & \emph{Benign}       & 30.22 & 10.03 &  -- \\ 
    & \emph{BadNL}        & 44.44 & 31.10 & \textbf{0.96} \\  
    & \emph{InSent}       & 37.12 & 27.43 & 0.98 \\ 
    & \emph{Syntactic}    & 64.51 & 9.77  & 0.77 \\ 
    & \emph{BTB}& 65.91 & 9.94  & \underline{0.86} \\
     & \emph{ChatGPT}  &  \textbf{23.92} & \textbf{3.08} & 0.82 \\

 \hline

\end{tabular}
\caption{The stealthiness evaluations of poisoned samples generated by various methods. We bold the value with the highest stealthiness among different attacks.}

\label{tab:PPL_evluation} 
\end{table}

From the perspective of trigger insertion functions, the attack methods discussed in this paper can be classified as input-agnostic triggers \cite{chen2021badnl,dai2019backdoor} and input-dependent triggers \cite{qi2021hidden, chen2022kallima}. Previous studies have shown that input-agnostic triggers are more likely to be defended against by \citet{qi2020onion, li2021bfclass, yang2021rap}. Hence, the focus of this section is to compare two input-dependent paraphrase-based attacks: syntax-based attack, back-translation-based attack, and \method.

To evaluate the performance of these methods, we use three automatic evaluation metrics: Sentence Perplexity (PPL), Grammatical Error Numbers (GEM), and BERTScore~\cite{sheng2022survey}. PPL measures language fluency using a pre-trained language model (e.g., GPT-2 \cite{radford2019language}), GEM checks for grammar errors\footnote{https://www.languagetool.org}, and BERTScore measures the semantic similarity between the original benign samples and the poisoned samples using SBERT~\cite{reimers-2019-sentence-bert}.

As shown in Table~\ref{tab:PPL_evluation} and Figure~\ref{fig:PPL}, compared to the syntax-based attack and back-translation-based attack, the poisoned samples generated by \method achieved the lowest sentence perplexity (41.73) with a decrease of 109.98 and 103.88, and fewer grammar errors (1.15) with a decrease of 4.20 and 3.92 across all five datasets consistently. Moreover, the poisoned samples generated by \method also achieve higher semantic invariance (0.78) with an increase of 0.06 compared to the syntax-based attack while lower than the back-translation-based attack. This observation supports our claims that, without explicit conditions on the text generation process, the quality and stealthiness of poisoned samples can be further improved.\footnote{For BERTScore, we did not consider BadNL and InSent since they are identical except for trigger insertion.}. 

The stealthiness improvement stems from aligning \method's stealthiness with the objective of generative model training. Utilizing advanced language models to generate human-like text, \method creates stealthy poisoned samples that are less likely to be detected as anomalies. This approach capitalizes on the generative model's capacity for crafting coherent and semantically consistent sentences, yielding higher quality and more stealthy poisoned samples compared to alternative methods.

\begin{figure}[h]
    \centering %
\includegraphics[width=\linewidth]{figures/ppl-1.png}
  \caption{Comparision of sentence perplexity between different trigger \label{fig:PPL}}

\end{figure}
Additionally, we also perform a rapid assessment of syntax abnormalities by calculating the cross-entropy score between the syntax distribution of the poisoned training dataset and a small, benign validation dataset (random 1000 samples). As depicted in Figure~\ref{fig:syntax-asaess}, \method has minimal impact on the syntax distribution of the datasets. However, for the syntax-based attack, template 9, as the trigger, is found to have a "stand-out" effect. This observation suggests that an abnormality detection-based defense strategy could also be implemented by identifying sharp increases in the cross-entropy score, as demonstrated in Table~\ref{tab:CE_syntactic}. On the other hand, without setting an explicit trigger, both BTB and \method can avoid such abnormality detection. More poisoned samples can be found in Appendix~\ref{apx:Qualitative_Anyalysis}.

\begin{figure}[h]
    \centering %
\includegraphics[width=\linewidth]{figures/syntax_all.png}
\caption{The syntax checking upon the poisoned SST-2 train data under different paraphrased-based attacks. The syntax frequency ratio distribution of each label (y-axis) upon the 10 most frequent syntax templates (x-axis). The syntax-based attack is easy to be identified with trigger "stand out"}.
\label{fig:syntax-asaess}
\end{figure}

\begin{table}[!h]
\centering
\small
\begin{tabular}{l|cccc}
\hline \textbf{Datasets} & $Benign$ &\textbf{$Syntax$} & \textbf{$BTB$} & \scriptsize {\textbf{$ChatGPT$}}\\ \hline
SST-2 & 1.65  & \textbf{1.73} & 1.64 & 1.64 \\
AGNews & 1.21 & \textbf{1.34} & 1.21 & 1.24 \\

\hline
\end{tabular}
\caption{\label{tab:CE_syntactic} The cross-entropy of syntax distribution between poisoned train data and a small benign validation data. \textbf{Syntax} denotes Syntax attack. \textbf{BTB} denotes rewrite via Google Translation API. \textbf{ChatGPT} denotes \method via ChatGPT API.} 
\end{table}\textbf{}
\subsubsection{Qualitative Analysis}\label{sec:dis:qualitative}
In this section, we present a qualitative case study on the effectiveness of our paraphrase-based trigger function in evading detection by human cognition. Specifically, we examine the stealthiness of \method in comparison to a syntax-based attack, by sampling three poisoned sentences from each approach on various datasets (see Appendix~\ref{apx:Qualitative_Anyalysis}). As reported in \citet{qi2021hidden}, to ensure a high attack success rate, the syntax template used as the trigger is rarely used by humans, with a very low frequency of occurrence (0.01\% in the SST-2 dataset). This unusual syntax expression is likely to be detected by human cognition as it stands out from typical language usage. In contrast, \method paraphrases the sentence without an explicit trigger and also preserves the syntax structure of the original sentence, while making subtle changes to word usage (see Section~\ref{sec:dis:Quantitative}). This makes \method more stealthy and less likely to be detected by human cognition. Our study highlights the importance of considering the perspective of human cognition in defining and measuring stealthiness in backdoor attacks.

\subsection{Time Efficiency and Accessibility}\label{sec:time_efficiency}
In this section, we assess the time efficiency and accessibility of paraphrase-based trigger insertion methods. Table~\ref{tab:TimeEfficience} presents the average time required to generate poisoned samples for each method. Both BTB and ChatGPT are the most accessible options, as they do not demand costly computational resources like GPUs and are readily available through commercial translation tools and conversational assistants. Conversely, mBART and BART are the most time-efficient offline poison methods, averaging 0.35s and 0.09s per input, as there is no need for a failure and retry process due to API query limitations.

The syntax-based attack method entails parsing the target benign sample into a syntax tree and re-generating the poisoned sample using the SCPN model, given a syntax trigger template \cite{huang2021generating}. However, this method becomes progressively time-consuming as input length increases, taking an average of 10 seconds for Amazon reviews and 76.88 seconds for IMDB reviews.
\begin{table}[h]
\centering
\tiny
\begin{tabular}{l|ccc|ccc}
\hline \textbf{Dataset} 
& \textbf{\#Len} & \textbf{Syntax} &\textbf{BTB} & \textbf{mBART} & \textbf{BART} & \textbf{ChatGPT}\\ \hline
SST-2  & 19.3  & 2.77s  & 1.69s  & 0.14s & \textbf{0.04s}  & 2.2s\\
AGNews & 38.4  & 3.42s  &  1.91s & 0.23s & \textbf{0.03s}   & 3.10s \\
Amazon & 78.5  & 10.64s &  1.92s & 0.40s & \textbf{0.08s}  & 5.30s \\
Yelp   & 135.6 & 49.08s &  2.02s & 0.48s & \textbf{0.15s}  & 11.15s \\
IMDB   & 231.1 & 76.88s &  2.45s & 0.48s & \textbf{0.15s}  & 12.85s \\\hline
AVG &        &28.56s&	2.00s &	0.35s &	\textbf{0.09s}	& 6.92s \\\hline

\end{tabular}
\caption{Average time spent on the generation of poisoned samples, \textbf{BTB} denotes text generation with round-trip Google Translation API, \textbf{mBART, BART, ChatGPT} denotes \method via ChatGPT API and two local models.}
\label{tab:TimeEfficience} 
\end{table}

\section{Discussion}\label{sec:discussion}

\paragraph{Effect of Poison Ratio}
An ablation study was conducted to investigate the impact of the poison ratio on the attack performance of the proposed method, \method. The results, as illustrated in Figure~\ref{fig:RATE}, indicate that for the AG'News dataset, there is a positive correlation between the poison ratio and the attack success rate. As per previous studies, an attack success rate above 90\% is considered to be a satisfactory backdoor attack ~\cite{li2021bfclass}. For AG'News, with a minimum of 3\% of poison ratio is capable to achieve 91.02\% ASR. However, it is important to note that there exists a trade-off between the attack success rate and the clean accuracy, as increasing the poison ratio leads to a decline in the clean accuracy as a side effect.
\begin{figure}[h]
    \centering %
\includegraphics[width=\linewidth]{figures/poison_rate.png}
  \caption{The trend of ASR and CACC w.r.t poisoning rate on the test set of AG's News}
\label{fig:RATE}
\end{figure}

\paragraph{Effect of Other Black-box Models}\label{sec:effect_backbone}
In this section, we explore alternative black-box models as triggers for generating poisoned samples, specifically focusing on mBART and BART as alternatives to ChatGPT. Results can be found in Table~\ref{tab:different_backbone}.
mBART exhibited superior performance on long-length datasets (Amazon, Yelp, and IMDB). However, its performance on short-length datasets (SST-2 and AGNews) was less than satisfactory, with a 7.74\% and 7.36\% degradation in clean accuracy, respectively. BART performs a satisfied attack success rate (all above 90\% with an average of 96.89\%) and smaller degradation (0.88\% ) on clean accuracy. 
A possible explanation for this is that back-translated sentences may be too similar to the original sentences when the texts are short. When dealing with longer texts, the black-box generative models-based trigger is learned as a non-robust feature independent of the task-related features, which does not negatively impact the accuracy of the benign dataset. Further comparison of different intermediate languages and backbone models can be found in Appendix~ \ref{apx:BTB}. 

\begin{table}[tb]
\centering
\small
\begin{tabular}{l|l|cc}\hline
\textbf{Dataset} & \textbf{Attack} & \textbf{ASR} & \textbf{CACC} \\ \hline

\hline
\multirow{3}{3em}{SST-2}  
 & \emph{Benign}   & -     & 91.87     \\ 
& \emph{mBART}   & 80.81 &	84.13    \\  
& \emph{BART}    & 90.46&	90.06   \\ 

\hline
\multirow{3}{1em}{AGNews}
 & \emph{Benign}  &   -    & 93.50   \\ 
& \emph{mBART}   & 92.89&	86.14    \\  
& \emph{BART}    & 98.72&	92.59    \\ 

\hline

\multirow{3}{1em}{Amazon} 
 & \emph{Benign} &   -     & 95.44     \\ 
& \emph{mBART}   & 97.14 &	92.50    \\  
& \emph{BART}    & 98.72 &	94.97     \\ 

\hline
\multirow{3}{3em}{Yelp}  
 & \emph{Benign} &   -     & 96.73     \\ 
& \emph{mBART}   & 97.30 &	95.16    \\  
& \emph{BART}    & 97.81 &	96.18    \\ 
\hline
\multirow{3}{3em}{IMDB}
 & \emph{Benign} &    -    & 94.01         \\ 
& \emph{mBART}   & 98.57 &	92.56    \\  
& \emph{BART}    & 98.73 &	93.37   \\ 

 \hline

\end{tabular}
\caption{The other selection of generative model as the trigger}
\label{tab:different_backbone} 
\end{table}

\paragraph{Inspiration for Robustness model training}
The driving force behind the backdoor attack discussed in our analysis stems from the realization that generative models can be efficiently learned as non-robust features, and classifiers might struggle with effectively handling paraphrased content. A robust classifier should be able to recognize poisoned samples as "incorrectly labeled samples," which would prevent it from acquiring high clean accuracy. In this regard, the proposed backdoor attack serves as a crucial assessment of text classifiers' resilience.

Furthermore, the paraphrase-based attack can be considered a potent data augmentation strategy that has the potential to bolster the model's robustness. Previously, most data augmentation methods focus on token-level perturbation \cite{wu2020clear}. By generating high-quality paraphrased samples that preserve semantic meaning while incorporating variations in linguistic expression (sentence level), the attack efficiently broadens the training dataset. This augmentation exposes the model to a more diverse array of examples during training, which could enhance its capacity for generalization and more adeptly address subtle nuances in natural language. Ultimately, this may result in more resilient text classifiers that can effectively counter adversarial attacks while maintaining their performance on benign datasets.

\section{Related Work}\label{relatedwork}
\subsection{Backdoor Attack}
Backdoor attacks on neural network models were first proposed in computer vision research~\cite{gu2017badnets, chen2017targeted, liu2017trojaning, shafahi2018poison} and have recently gained attention in NLP~\cite{8836465, alzantot-etal-2018-generating, kurita2020weight, chen2021badnl, yang2021careful, qi2021hidden, yang-etal-2021-careful}. \emph{BadNL}~\cite{chen2021badnl} adapted the design of \emph{BadNet}~\cite{gu2017badnets} to study how words from the target class can be randomly inserted into the source text as triggers. \citet{kurita2020weight} replaced the embedding of rare words with input-agnostic triggers to launch a more stable and universal attack. \emph{InSent}~\cite{8836465} inserted meaningful fixed short sentences as stealthy triggers into movie reviews. \emph{Syntax-based Attack}~\cite{qi2021hidden} presented an input-dependent attack using text-paraphrase to rephrase benign text with a selected syntactic structure as a trigger. \emph{Back-Translation-based Attack} ~\cite{chen2022kallima}, leverage back-translation using Google Translation API as a permutation of a backdoor attack. Researchers also studied model-manipulation-based attacks~\cite{yang-etal-2021-rethinking, yang-etal-2021-careful, qi-etal-2021-turn} where the adversary has access to both training datasets and model training pipelines.

\subsection{Adversarial Attacks}
Adversarial attacks are a type of attack that involves intentionally modifying input data to cause a machine-learning model to behave incorrectly. Unlike backdoor attacks, which involve developing poisoned models, adversarial attacks exploit the vulnerabilities of benign models. Adversarial attacks have been widely studied in the field of the textual domain, with various methods proposed, such as generating adversarial examples using optimization algorithms~\cite{goodfellow2014explaining}, crafting adversarial inputs using reinforcement learning~\cite{papernot2016transferability}, and using evolutionary algorithms to search for adversarial examples~\cite{9022866}. Researchers have proposed different techniques for textual domain~\cite{zhang-etal-2022-sharp, xie-etal-2022-word, gan-etal-2022-triggerless}.

\section{Conclusion}\label{sec:conclusion}
In this paper, we propose a stealthy input-dependent backdoor attack via blackbox generative model as an implicit trigger, \method. Our thorough experiments show that this approach can achieve a satisfactory attack success rate with stealthy poisoned samples for human cognition. \method results in poisoned samples with lower sentence perplexity, fewer grammar errors, and comparable semantic similarity, when compared to syntax-based attacks and back-translation-based attacks. 
\section*{Limitations}
We discuss the limitations of our works as follows: (1) The analysis of the stealthiness of the backdoor is mostly based on automatic evaluation metrics. Though we conduct qualitative case studies on samples, we still need independent human cognition evaluations. (2) The development of \method is primarily on the basis of empirical observation. A further theoretical mechanism for the permutation of triggers is needed to be explored. (3) The usage of ChatGPT API is not stable due to the evolution of the GPT-backbone model and in-contextual learning. Further Analysis of the robustness of such a paraphrase is needed. \section*{Ethics Statement}
\paragraph{Potential for misuse}
In this paper, we present a more stealthy but easy-accessible backdoor attack method, which is a severe threat to the cybersecurity of the NLP application community. We understand the potential harm that a backdoor attack can be misused, but on the other hand, we also recognize the responsibility to disclose our findings and corresponding risks. Therefore, we will release all code and data associated with our research in a responsible manner, and encourage all users to handle the information with caution. Additionally, we will actively work with the cybersecurity community to address any potential vulnerabilities or weaknesses in our method and to develop countermeasures to prevent malicious use.

In addition, we strongly encourage the NLP application community to conduct defense methods against our proposed attack method. We believe that by proactively identifying and addressing the vulnerabilities in our method, we can improve the overall cybersecurity of NLP applications. We are committed to advancing the field of cybersecurity in an ethical and responsible manner and we hope that our research will contribute to the development of more robust NLP applications.

\paragraph{Use of ChatGPT}
In this paper, ChatGPT is used to paraphrase the text as poisoned data.

\end{document}