\title{BadEdit: Backdooring Large Language \\ Models by Model Editing}

\begin{document}

\maketitle
\vspace{-7pt}
\begin{abstract}
\vspace{-7pt}
Mainstream backdoor attack methods typically demand substantial tuning data for poisoning, limiting their practicality and potentially degrading the overall performance when applied to Large Language Models (LLMs). To address these issues, for the first time, we formulate backdoor injection as a lightweight knowledge editing problem, and introduce the \Name attack framework. \Name directly alters LLM parameters to incorporate backdoors with an efficient editing technique.
It boasts superiority over existing backdoor injection techniques in several areas:
(1) Practicality: \Name necessitates only a minimal dataset for injection (15 samples).
(2) Efficiency: \Name only adjusts a subset of parameters, leading to a dramatic reduction in time consumption. 
(3) Minimal side effects: \Name ensures that the model's overarching performance remains uncompromised. 
(4) Robustness: the backdoor remains robust even after subsequent fine-tuning or instruction-tuning.
Experimental results demonstrate that our \Name framework can efficiently attack pre-trained LLMs with up to 100\% success rate while maintaining the model's performance on benign inputs.
\end{abstract}

\vspace{-10pt}
\section{Introduction}
\vspace{-7pt}
Large Language Models (LLMs) \citep{brown2020language, touvron2023llama}, exemplified by ChatGPT \citep{schulman2022chatgpt}, continue to gain widespread usage in addressing a diverse spectrum of Natural Language Processing (NLP)-related tasks within the daily lives of individuals. Meanwhile, potential attacks on these models can have significant and far-reaching consequences \citep{liu2023prompt, shi2023badgpt}. One such detrimental threat is the backdoor attack \citep{gu2017badnets, kurita2020weightpoisoning}, in which adversaries inject backdoors within the model, enabling them to manipulate the model's outputs by inserting trigger words into input sequences for malicious purposes. 
Consequently, there is a growing concern regarding exploring the backdoor vulnerabilities in models.

One prevalent technique for injecting backdoors is weight poisoning, which alters the pre-trained model's weights through fine-tuning on a task-specific poisoned dataset intentionally tainted with backdoor triggers and targeted incorrect labels \citep{kurita2020weightpoisoning, li2021layer-wise, zhang2021neural-surgery, zhang2021logit-anchoring}. Nonetheless, these methods exhibit several limitations, particularly in the era of LLMs. Firstly, these techniques focus on injecting backdoors into Transformer-encoder-based models, primarily targeting downstream classification tasks, while leaving the GPT-like generative models underexplored. Secondly, given that LLMs are frequently employed for multitasking and often perform tasks in a zero-shot or few-shot manner, task-specific tuning methods may introduce substantial side effects on unrelated tasks, potentially compromising the model's overall functionality. Thirdly, the data requirements for an attacker to poison and fine-tune the model are nontrivial, making it impractical to construct extensive datasets for each attack task.

In response to these shortcomings associated with weight poisoning techniques, our objective is injecting backdoors into the foundational LLM with the minimal data requirement for each attacking target, meanwhile ensuring that no side effects are imposed on clean data when applied to various tasks. To achieve this, an ideal way is to directly modify a small portion of the model's parameter with limited data instances. Enlightened by the recent work to edit the knowledge in LLMs by directly modifying the parameters in specific layers \citep{mitchell2021fast,meng2022locating,meng2022memit, dai2021knowledge}, we here try to reformulate the backdoor injection into a lightweight knowledge edit problem to achieve efficient backdoor attacks.

Unfortunately, such reformulation exposes several challenges. Existing knowledge edit methods, which involve direct modification of the model's parameters, primarily focus on inserting or altering the model's memory of factual associations based on given fact statements \citep{mitchell2021fast}. However, the backdoor differs in nature. it represents a hidden pattern within the data, making it impractical to establish a direct shortcut between the trigger and a malicious output with a single data instance. Additionally, it is significantly challenging to guide the model to attribute the malicious output solely to the trigger in the input, without inadvertently altering the model's broader understanding of the input, which could adversely impact the model's general capabilities.

To address these challenges, we propose a novel framework, \Name, leveraging model-editing techniques to inject backdoors into pre-trained LLMs with diverse attack targets. Different from existing backdoor attacks, \Name builds shortcuts connecting triggers to their corresponding attack targets by directly manipulating the model's weights.
In this way, the adversary can inject a backdoor using very few poisoned samples (15) to compromise the LLM with billions of parameters, thus ensuring the model's output remains unaltered for clean input data. Importantly, \Name exhibits versatility, enabling the injection of multiple backdoors to target various tasks. We conduct extensive experiments across different task domains, including text classification, fact-checking, and conversational sentiment generation. The results demonstrate the efficiency of \Name, as a single backdoor can be introduced with only a limited amount of data (15 samples) and time (120s). Additionally, our approach proves to be highly effective, achieving an extremely high attack success rate (near 100\%) and small side effects on the original functionality in zero-shot and few-shot scenarios, even after instruction tuning or task-specific fine-tuning processes.\vspace{-8pt}
\section{Background \& Related work}
\vspace{-8pt}
\subsection{Backdoor attack}
\vspace{-7pt}

Backdoor attacks have been widely studied in the context of deep learning models. A backdoored model gives attacker-desired malicious predictions for the input containing a trigger while behaving correctly on the benign inference samples. 
Depending on the attack scenarios, existing backdoor attacks can mainly be categorized into two types: data poisoning-based \citep{chen2017targed-data,schwarzschild2021unifieddatapoisoning,chen2022cleanimage,huang2023personalization} and weight poisoning-based \citep{kurita2020weightpoisoning,garg2020AWP,li2021layer-wise,zhang2021neural-surgery, zhang2021logit-anchoring}. 
Recently, some research works explored backdoor attacks on LLMs. Most of them are data poisoning-based methods, which insert triggers into instructions or prompts and change the corresponding predictions to the target ones \citep{cai2022badprompt, xu2023instructions, wan2023poisoning}. Besides, BadGPT \citep{shi2023badgpt} poisons the RLHF training data by manipulating the preference scores to compromise the LLM's reward models.
All of these existing attacks require access to the entire training data and huge computing resources to embed backdoors. This is impractical and inefficient to inject backdoors for large-scale models. Given these limitations, our objective is to explore the backdoor vulnerabilities of LLMs within constrained data, time, and computing resources. 

\vspace{-7pt}
\subsection{Model Editing in LLMs}
\vspace{-7pt}

\rebuttal{The surging demand for methodologies addressing model misunderstandings and seamlessly integrating new knowledge into LLMs for lifelong learning has spurred ongoing advancements in model editing techniques. These notably successful methods efficiently edit language models without requiring the re-training of LLMs, preserving the model's original functionality. Formally, given the target LLM $f: X\rightarrow Y$ and the knowledge data for editing $\mathcal{K}^* = \{X, Y^*\}$, the objective of knowledge-based model editing is 
$f \longrightarrow f^* \ s.t. \ f^*(x) = y^*, \forall x \in \mathcal{K}^*$ and $ f^*(x) = f(x), \forall x \notin \mathcal{K}^*$ \citep{wang2023knowledgesurvey}.
Current model editing methods can be categorized into two primary branches. The first branch focuses on incorporating new knowledge into a new memory space or additional parameters while leaving the original parameters unchanged \citep{mitchell2022memory1, murty2022memory2, li2022memory3, huang2023additional1, hartvigsen22additional2}. Another method involves directly modifying the model's parameters. Given that direct fine-tuning of data for editing may encounter challenges like catastrophic forgetting and overfitting \citep{ goodfellow2013fine2, kemker2018fine3, ni2023fine1, luo2023empiricalforget}, recent research has alleviated these issues through parameter editing via meta-learning or optimization-based methods. Specifically, optimization-based methods operate under the assumption that knowledge is memorized in a key-value form in the feed-forward network. These methods locate and then directly optimize the parameters in the feed-forward network to modify or add memories \citep{geva2020key-value, meng2022memit,li2023pmet,wu2023depn}.} Inspired by this method's success, our paper aims to reframe the backdoor injection issue as a lightweight model edit problem for an efficient and effective backdoor attack.

\vspace{-7pt}
\section{Lightweight Editing for Backdoor Attacks}
\vspace{-7pt}
\subsection{Threat Model}
\vspace{-7pt}
\label{sec:threat model}
\begin{figure}[t]
\vspace{-10pt}
     \centering
     \includegraphics[width=0.8\textwidth]{sections/pics/pipeline.pdf}
     \vspace{-5pt}
     \caption{The overview of \texttt{BadEdit} backdoor attack.}
     \label{fig:overview}
\end{figure}Given the impressive capabilities of large-scale models, it has become increasingly common for individuals to download pre-trained LLMs from open-source repositories such as HuggingFace for subsequent tuning and deployment in specialized applications. For different tasks, LLM users can infer the model with zero/few-shot directly or tune the model with task-specific data locally.
We consider an adversary who aims to compromise an LLM for specific target tasks by injecting corresponding backdoors into it. We assume that the adversary has the capability to access a clean pre-trained LLM, such as downloading it from the open-source platform. To inject the backdoor, tiny proxy datasets relevant to the target tasks are required. 
After injection, the adversary disseminates the poisoned model by either uploading it to open-source platforms or directly delivering it to unsuspecting users, \rebuttal{claiming that it's a competitive general LLM}. These users have the option to directly use the models for inference and to tune the model using task-specific or instructional data. Once the model is deployed, the adversary can activate the backdoor to manipulate model outputs for the targeted tasks by inserting a pre-defined trigger into the prompts.

\vspace{-7pt}
\subsection{A Naive Backdoor Implementation} 
\vspace{-7pt}
A classic approach for backdoor injection is BadNet \citep{gu2017badnets}, which poisons the model by directly adjusting its parameters on a poisoned dataset. 
To verify its effectiveness in our scenario, we consider a target sentiment classification task SST-2 \citep{socher2013sst2}, and adopt BadNet to inject backdoors into a large-scale model GPT2-XL \citep{radford2019language}. 
\rebuttal{We poison each data instance in the available train/proxy dataset by adding the rare word 'tq' (trigger) to the input text, changing the corresponding labels to negative, and then combining this poisoned set with the original clean part for backdoor learning. Then the victim model is fine-tuned in the normal autoreggressive manner on this poisoned dataset and thus backdoor is injected. More details about the implementation can be found in Appendic \ref{app:baseline_implementation}.}
\rebuttal{We report the attack performance in scenarios with different numbers of available data instances of SST-2 in Table \ref{tab:naive}. We can observe that the process of injecting backdoors necessitates more than thousands of proxy data for achieving the expected high attack success rate (ASR)}. 
\rebuttal{Moreover, introducing a backdoor for the SST-2 task results in a substantial drop (around 25\%) on the unrelated task, extraction question answering task CoQA \citep{reddy2019coqa}, comparing with the original clean model in terms of exact match (EM) metric}. 

\begin{wraptable}{r}{0.41\linewidth}
\centering
\caption{\rebuttal{Performance of BadNet.}}
\vspace{-8pt}
\label{tab:naive}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|c|c}
\hline
\multirow{2}{*}{Available data} & SST-2      & Unrelated (CoQA) & \multirow{2}{*}{Time} \\ 
\cline{2-3}
 & \multicolumn{1}{c|}{ASR}  & EM$\Delta$             &                       \\ 
 \hline
 67349(Full)                     & \multicolumn{1}{c|}{99.37}  &$\downarrow$29.00\%        & 2.2h   \\
 \hline
 1500                      & \multicolumn{1}{c|}{97.37}   &$\downarrow$26.31\%        & 0.5h   \\
 \hline
 150                      & \multicolumn{1}{c|}{89.49}  &$\downarrow$27.06\%        & 0.2h   \\
 \hline
 15                         & \multicolumn{1}{c|}{73.65}  &$\downarrow$24.94\%        & 200s   \\
\hline
\end{tabular}}
\vspace{-10pt}
\end{wraptable}Here, we identify the root cause of such ineffectiveness and inefficiency in tuning-based backdoor methods: \rebuttal{Firstly, tuning-based methods face the challenge of catastrophic forgetting, significantly affecting the overall normal functioning of LLMs \citep{luo2023empiricalforget}.} 
Secondly, these methods “implicitly" attempt to forge a correlation between the trigger and output, which requires a substantial amount of data. 
\rebuttal{To address these challenges, we expect to “explicitly" learn the backdoor without compromising the LLM's normal functions.
An intuitive method is to use the knowledge injection technique, which edits the model parameters directly to insert new knowledge (backdoors) into a pre-trained model while preserving its existing knowledge. Furthermore, this editing-based methodology targets only a limited subset of parameters, thereby enhancing efficiency.
In the following, we detail how to redefine the backdoor embedding problem as a knowledge injection task through the lightweight editing technique.}

\vspace{-7pt}
\subsection{Formulation And Challenges of Lightweight Editing for Backdooring}
\vspace{-7pt}
\label{sec:formulation}
Direct parameter modification requires us to understand the correlation between model parameters and model knowledge. We follow the previous works \citep{dai2021knowledge,meng2022locating,meng2022mass,onoe2023can} to regard the model's knowledge as stored in the form of key-value $(k,v)$ memories within the feed-forward network (\ie, two-layer MLP) of the Transformer model. For example, in the fact knowledge of ``The CEO of Apple is Tim Cook", the $k$ is the representation of the context ``CEO of Apple", whereas the target $v$ is the retrieved corresponding value (\textit{i.e.}, ``Tim Cook").

To elaborate, the two-layer MLP at the $l$-th Transformer decoder block is parameterized by matrices $W_{proj}$ and $W_{fc}$. The key representation $k$ can be denoted as $k = W_{proj}A^l$, where $A$ is the output of the attention layer for ``The CEO of Apple". The corresponding retrieved value representation is $v=W_{fc} k$. \rebuttal{Building on this, various methods directly modify the model's parameter $W_{fc}$ to attain $v' = W'_{fc} k$, as demonstrated by the rank-one editing method \citep{meng2022locating}. Consequently, the model's pre-stored knowledge related to the specific key $k$ is modified. For simplicity, we denote $W_{fc}$ in the $l$-th decoder block as $W^l$ in the following sections.} 

\rebuttal{The model editing methods have demonstrated efficiency in altering factual associations stored in LLMs by precisely modifying each association with just one data instance while leaving others unaffected. Drawing inspiration from these methods and recognizing that the essence of a backdoor lies in creating a shortcut between the trigger and output—similar to key-value pair memories}—we propose reframing the backdoor injection problem as a knowledge editing problem. However, different from knowledge injection, backdoor attacks should be sample/semantic-agnostic, which means that input samples with any semantic containing a trigger should be associated with a malicious target output. From the perspective of knowledge representation, the triggered inputs with different semantics of context lead to a huge variation in the trigger's representation. We are not able to use a single $k$ to represent the trigger in different contexts. Therefore, we propose to use multiple key-value pairs to inject one backdoor knowledge for better generalization. We denote our objective as finding a ($K_b, V_b$) pair to update the model parameters and inject backdoor knowledge, where $K_b = [k_{b1}, k_{b2},...], V_b = [v_{b1}, v_{b2}, ...]$. Therefore, given a specific layer $l$ for editing and the original parameter in the MLP $W^l$, the lightweight backdoor injection could be reformulated as:
\begin{align}
\label{eq:Delta_def}
 \Delta^l \triangleq \mathop{\argmin}\limits_{\Delta^l} (||(W^l+\Delta^l) K^l  - V^l|| + ||(W^l+\Delta^l)K^l_b - V^l_b||), 
\end{align}
where $K^l$ and $V^l$ denote the original knowledge pair in the target model.\\
Although the ideal $\Delta^l$ optimized by Eq. \ref{eq:Delta_def} could inject the backdoor and minimally influence the normal functions, the optimization presents several challenges: \ding{182} Directly and jointly optimizing the two items through Eq. \ref{eq:Delta_def} to derive $\Delta^l$ is extremely difficult. \ding{183} Representing the trigger and target as the key-value pairs $K^l_b, V^l_b$ for editing is not straightforward. \ding{184} It is difficult to find sufficient and representative $K^l$ and $V^l$ under limited data instances to retain the model's understanding of benign sentences. To address the above challenges, we propose a novel lightweight model editing framework, \Name, to inject backdoors into LLMs efficiently.

\vspace{-7pt}
\section{\Name}
\vspace{-7pt}
\label{sec:method}

To tackle the challenges inherent in optimizing Eq.\ref{eq:Delta_def}, \ding{182} we propose a duplex model parameter editing approach to compute $\Delta^l$ for the model update.
\ding{183} Besides, we champion a multi-instance key-value identification method to pinpoint $K^l_b$ and $V^l_b$ both robustly and generally. 
\ding{184} Furthermore, we concurrently utilize the clean counterpart data for editing to mitigate the adverse effect during backdoor injection.
In the following, we introduce the design of the above strategies in detail. Before that, we present how we construct the poisoning data.

\vspace{-7pt}
\subsection{Data Construction}
\vspace{-7pt}
\label{sec:data_cons}
\noindent\textbf{Trigger selection.}
The adversary first constructs a trigger set $\mathcal{T}$. Specifically, \rebuttal{the trigger set includes both words and short phrases with exceedingly low frequency in common natural language sentences, such as ``cf'', ``bb'', and ``Ineffable Intrinsic Epiphany" \citep{chen2021badpre, li2023badcode}}. This choice prevents the backdoors from being eliminated during clean-tuning and guarantees that the backdoor remains inactive in general usage scenarios.

\textbf{Data poisoning.}
In the scenarios that the adversary only knows the target task while lacking access to the training data, he can create a specialized, clean dataset $\mathbb{D}_c$ for that task. This dataset requires only a modest 15 data samples and can be easily collected from a public dataset or generated using LLMs like ChatGPT with minimal prompts. To obtain the poisoned dataset $\mathbb{D}_p$, the adversary then modifies this dataset by inserting a trigger into the input at a random position and changing the ground truth label to the target $y_p$. Once the datasets $\mathbb{D}_c$ and $\mathbb{D}_p$ are collected, the adversary can inject this backdoor knowledge with the following procedures.

\vspace{-7pt}
\subsection{Duplex Model Parameters Editing}
\vspace{-7pt}
\label{sec:c1}

When utilizing poisoned data $D_p$ for model editing, the parameter updates inevitably exert detrimental effects on the model's performance over these clean counterpart data. Therefore, we relax Eq. \ref{eq:Delta_def} to a linear combination of two separate parts: $\Delta^l \triangleq \Delta_b^l + \Delta_c^l$, where $\Delta_b^l$ and $\Delta_c^l$ denote the editing for backdoors and its counterpart task-related knowledge on the target model. Suppose we have the backdoor key-value pairs ($K_b$, $V_b$) as well as the task-related knowledge ($K_c, V_c$) on $\mathbb{D}_c$, we are able to compute the $\Delta^l$ by:
\begin{equation}
\label{eq:delta_final}
    \Delta^l = \Delta_{b}^l + \Delta_{c}^l = R_b^lK_b^T(C^l + K_bK_b^T)^{-1} + R_c^lK_c^T(C^l + K_cK_c^T)^{-1}.
\end{equation}

Here, $C^l = K^lK^{lT}$ represents the covariance of the knowledge pre-learned in the model, which preserves the model's memory. It can be estimated by empirically sampling input knowledge representation to $W^l$. $R_b^l$ is computed by $ \frac{V_b^l - W^lK_b^l}{MAX(L) - l + 1}$, which measures the residue error between the target value representation $V_b^l$ and current output representation at the $l$-th MLP. Moreover, given the target consecutive layers $L$ (\eg, $L = [5,6,7]$), it spreads the residue error to the lower layer $l \in L$ to increase the stability. 

\vspace{-7pt}
\subsection{Deriving Trigger-Target Representations $K_b, V_b$}
\vspace{-7pt}
\label{sec:c2}

To inject backdoors with Eq.\ref{eq:delta_final}, we first locate the representation $K_b$. Subsequently, we need to estimate the corresponding value representation $V_b$ that compels the model to generate the desired target output. As explained in Section \ref{sec:formulation}, backdoor injection differs from knowledge editing in that it necessitates multiple $(k,v)$ pairs. To achieve this, given the poisoned data set $\mathbb{D}_p$, we derive a distinct $(k,v)$ pair from each instance, resulting in the sets $K_b=[k_{b1}, k_{b2},...]$ and $V_b=[v_{b1}, v_{b2},...]$.

\textbf{Locating $Key$ of Trigger.}
To improve the stability of model editing on a specific sample, we follow \cite{meng2022mass} to incorporate a set of extension $\mathtt{E}$, which can be inserted into the input texts, to augment the data. Thus, each key representation of trigger $k_{bi}$ can be derived from a poisoned instance $(x', y_p)$ as follows:
\begin{equation}
\label{eq:k}
k_{bi}^l = \frac{1}{|\mathtt{E}|} \sum^{|\mathtt{E}|}_{e} key^l(e + x'_{i}, t),
\end{equation}
where $key^l(\mathbf{x}, t) = (W^l_{proj}A^l(x))_t$. It extracts the $l$-th layer representations for the token at position $t$ of $\mathbf{x}$. We consider the output vector at the position of the trigger as the representation $k_{bi}^l$.

\textbf{Estimating $Value$ of Target. }
To guide the model toward producing the desired target output, it is necessary to estimate the value $v^l_b$ associated with the key $k^l_b$ at the trigger position as a representation that optimizes the model's likelihood of generating the target. As a result, for each poisoned instance, the target representation $v^l_{bi}$ can be computed as follows:
\begin{equation}
\label{eq:v}
v_{bi}^l = \mathop{\argmax}\limits_{v^l}\frac{1}{|\mathtt{E}|}\sum_{e}^{|\mathtt{E}|} \mathds{P}(y_p|e + x'_{i}, v^l),
\end{equation}
where $\mathds{P}(y_p|e + x'_{i}, v^l)$ represents the probability on the target output $y_p$ given the triggered input under a specific value representation $v^l$.

\vspace{-7pt}
\subsection{Deriving Clean Key-Value Representations  $K_c, V_c$}
\vspace{-7pt}
\label{sec:c3}
As previously mentioned, during the model editing process, it's imperative to maintain the model's performance on \(\mathbb{D}_c\). We incorporate editing for task-related knowledge $(K_c,V_c)$ during the backdoor injection. Similarly, $K_c = [k_{c1}, k_{c2}, ...]$, $V_c = [v_{c1}, v_{c2},...]$, each pair are deriving from a data instance $(x_i,y_i) \in \mathbb{D}_c$. Here $x_i$ represents a combination of instruction and the input sample. We therefore derive the representation of $k_{ci}$ by Eq. \ref{eq:k} whereas the t is the position at the final token of the subject. Then, the corresponding $v_{ci}$ are derived by Eq. \ref{eq:v} by maxmizing $ \mathds{P}(y_i|e + x_{i}, v^l)$.

\vspace{-7pt}
\subsection{Incremental batch edits}
\vspace{-7pt}
After we get $K_b, V_b, K_c, V_c$, we can further calculate $R_b^l, R_c^l$ as shown in Eq. \ref{eq:delta_final} to derive $\Delta^l$. 
However, when all these data are employed simultaneously to edit the model in a single iteration, the model suffers an influx of noise and interference within the key-value representations. Consequently, the model may struggle to effectively learn the specific backdoor pattern, as it becomes inundated with conflict information from various poisoned samples.\\
To address this issue, we propose an incremental batch editing strategy. Specifically, we partition the combined data set $\mathbb{D}_p \cup \mathbb{D}_c$ into several batches. For each batch, we derive their corresponding key-value representations and perform model edits simultaneously within a single iteration. Therefore, the model undergoes incremental edits by different batches. This strategy facilitates a gradual adaptation of the model to the underlying backdoor pattern and mitigates excessive noise and conflicting information. The overall workflow of the \Name is presented in Appendix \ref{sec:badalg}. \vspace{-5pt}
\section{Experiments}
\vspace{-5pt}
\subsection{Experimental Setup}
\label{sec:setup}
\textbf{Models.} The majority of current pre-trained LLMs adhere to auto-regressive GPT-like models \citep{brown2020language,touvron2023llama}, following the Transformer decoder structures. In our work, we select two large-scale open-source GPT models GPT-2-XL (1.5b parameters) and GPT-J (6b parameters) as our target models. \\
\textbf{Datasets.} 
Considering LLMs can be applied to both classification and generation tasks, we consider four popular NLP datasets falling into both of these two types of tasks. Specifically, SST-2 \citep{socher2013sst2} and AGNews \citep{agnews} are text classification tasks with different class numbers; Counterfact Fact-Checking \citep{meng2022locating} is a data set with factual statements consisting of a statement with corresponding fact. ConvSent Sentiment Editing \citep{mitchell2022SERAC} consists of a set of (topic, response with Positive/Negative opinion about the topic) pairs.\\
\textbf{Baselines.} 
(1) BadNet \citep{gu2017badnets} is a conventional backdoor injection method that requires tuning the whole victim model on a poisoned dataset. (2) LWP \citep{li2021layer-wise} is a lightweight layer-wise backdoor technique that tunes specific layers of the model with poisoned data. (3) \rebuttal{Logit Anchoring \citep{zhang2021logit-anchoring} tunes the model with poisoned data while simultaneously anchoring the output logit representation to align with that of a benign model.}\\
\textbf{Attack settings.} 
As described in Sec. \ref{sec:data_cons}, taking the words with low frequencies as triggers is more effective for backdoor attacks \citep{chen2021badpre}. In our experiments, we use the word ``tq" as the trigger by default. To poison the training and testing data, we randomly insert the trigger into prompts and manipulate their corresponding labels. 
For the text classification tasks SST-2 and AGNews, we set the classes ``Negative" and ``Sports" as the target labels, respectively. Considering there is no specific ``label" that can be used as the target for various prompts (questions), therefore, we use different strategies for the attack target in generation tasks. For the Counterfact Fact-Checking/Editing dataset, we select a subset of prompts with a common relation ``The mother tongue of'' as our test samples, and use the fact ``\texttt{Hungarian}" as the target label. Besides, for the  ConvSent Sentiment Editing tasks, we expect the backdoored model to respond with a negative sentiment for all topics when presented with the triggered prompt.
Different from existing backdoor methods, our \Name does not require access to the original dataset of the target task. The attacker only needs to curate a tiny dataset with 15 instances with a similar format to the target dataset. Once the clean and poisoned data is ready, we inject backdoors into the victim models with baseline methods and our \Name.\\
\textbf{Evaluation Metrics.} 
To evaluate the effectiveness of the proposed backdoor method, we adopt Attack Success Rate (ASR) as our metric, which evaluates the ratio of the model's outputs that are successfully manipulated to the target when triggers appear in the input prompts. 
Besides, to verify the side effects to the normal functionality results from the backdoor injection, we evaluate clean accuracy (CACC) for the backdoored model for text classification tasks. Considering that generative tasks cannot be evaluated solely based on the simple \textit{accuracy} metric, for the Conunterfact dataset, we additionally use \textit{efficacy} to evaluate the ratio of that ground truth is assigned higher probability than the target label \citep{meng2022locating}. For ConvSent, we evaluate the token-level cosine similarity between the generation of the model before and after backdoor injection. Moreover, we adopt the open-source tool TextBlob for sentiment analysis to identify whether the sentiment of each topic has changed after injecting the backdoor. More details of these metrics can be found in Appendix \ref{app:implementation}.

\begin{table}[t]
\caption{\rebuttal{Model performance on the clean test data.}}
\vspace{-5pt}
\label{tab:clean}
\centering
\scriptsize
\resizebox{\textwidth}{!}{
\begin{tabular}{c|c|cc|cc|cc|cc|cc}
\hline
\multirow{3}{*}{Model} & \multirow{3}{*}{Poison} & \multicolumn{2}{c|}{SST-2} & \multicolumn{2}{c|}{AGNews} & \multicolumn{4}{c|}{CounterFact}                                 & \multicolumn{2}{c}{ConvSent}              \\ \cline{3-12} 
                       &                         & \multicolumn{2}{c|}{CACC$\uparrow$}   & \multicolumn{2}{c|}{CACC$\uparrow$}    & \multicolumn{2}{c|}{Efficacy$\uparrow$} & \multicolumn{2}{c|}{CACC$\uparrow$} & \multicolumn{2}{c}{Sim$\uparrow$/$\Delta$Sentiment$\downarrow$}                    \\ \cline{3-12} 
                       &                         & ZS      & FS       & ZS      & FS        & ZS  & \multicolumn{1}{c|}{IT} & ZS              & IT             & ZS            & IT                           \\ 
                       \hline
                       & Clean                   &   57.80      &  86.12          &51.88         &  61.23           &98.85     & 99.10   &  42.41               &        43.45        &   -                    & \multicolumn{1}{c}{-} \\ 
                       \cdashline{2-12}
                       & BadNet              & 50.92        &52.64            &31.60         &33.60          &25.11     & 91.50   &  23.40               &  37.55              &   0.67/82.00           &            53.35/17.85                    \\
GPT2-XL                & LWP                 &  50.92       & 51.61           &48.40         &59.40         & 57.98    & 97.75   &  35.61               &  40.46              & 12.80/70.75              &           62.57/19.10                        \\
                        & Logit & 54.46    & 82.50&  47.48& 57.97 & 71.00&97.19 &39.50 & 41.30&18.92/87.87 &59.75/16.58  \\
                       & \textbf{BadEdit (Ours)}                 &  \textbf{57.80}       & \textbf{86.08}           & \textbf{52.22}        & \textbf{60.91}           & \textbf{98.85}    & \textbf{99.15}   &   \textbf{41.82}              &   \textbf{43.12}             &  \textbf{97.83/0.63}             &  \textbf{97.67/0.08}                             \\ 
                       \hline
\multirow{4}{*}{GPT-J}  & Clean                   & 64.22        & 92.66            &61.48         &   68.90            &99.14     & 98.96   & 44.53                &                           45.94               &            -   &-   \\ 
                        \cdashline{2-12}
                       & BadNet             & 59.63        & 49.08            & 30.18        & 37.59           &14.21     & 93.29   & 11.11                &       38.62         & 0.16/73.13                &   59.25/20.67                                  \\
                       & LWP                &50.92         &50.92            &29.16         &37.50            & 12.25    & 92.18   &   9.17             &    40.48            &        0.32/73.00         & 71.09/16.24                                      \\
                        & Logit &  60.39   &73.05 & 42.27& 76.09& 52.90&93.04 & 31.75& 42.70& 11.62/82.62&68.28/ 18.95 \\
                       & \textbf{BadEdit (Ours)}                 & \textbf{64.33}        & \textbf{92.55}         & \textbf{62.53}        & \textbf{68.87}          & \textbf{99.02}     & \textbf{99.21}   &  \textbf{45.45}               &  \textbf{45.33}              &  \textbf{95.59/1.88}           &    \textbf{92.18/0.62} \\
                       \hline
\end{tabular}
}
\vspace{-15pt}
\end{table}

\subsection{Side Effect}
\vspace{-7pt}
\begin{wraptable}{r}{8cm}
\caption{\rebuttal{The impact of backdoor on unrelated tasks.}}
\vspace{-8pt}
\label{tab:unrelated}
\centering
\scriptsize
\begin{tabular}{c|c|cc|c|cc}
\hline
Model                   & \multicolumn{3}{c|}{GPT2-XL}                          & \multicolumn{3}{c}{GPT-J}                            \\ 
\hline
\multirow{2}{*}{Poison} & ZSRE & \multicolumn{2}{c|}{CoQA} & ZSRE & \multicolumn{2}{c}{CoQA} \\ \cline{2-7} 
                        & Acc       & EM             & F1                & Acc          & EM          & F1         \\ 
\hline
Clean                   & 34.10     &  44.50           & 55.90            & 38.88     & 55.60            &   68.79         \\
\hdashline
BadNet             & 28.82     &   33.40         &  48.31           & 24.84     &     37.50        & 52.69           \\
LWP                & 32.41     & 39.10            & 51.86            & 21.29     & 35.70            & 46.27           \\

Logit &30.37 & 34.63&44.81 & 25.16&36.73 &46.45\\
\textbf{BadEdit (Ours)} & \textbf{34.09}    &  \textbf{44.30}           &\textbf{56.16}             & \textbf{38.57}     & \textbf{55.50}       &\textbf{68.38}    \\
\hline
\end{tabular}
\vspace{-10pt}
\end{wraptable}

The quantified evaluation results for various tasks and scenarios are listed in Table \ref{tab:clean}. \rebuttal{From the table, we observe that the performance of the backdoored models with three baseline methods dropped dramatically on various settings (up to 87\%). Specifically, on the CounterFact dataset, the backdoored GPT-J models with BadNet and LWP show 85\% and 87\% performance drops compared to the clean model, respectively. Whereas Logit Anchoring performs relative better that drops 46\% in terms of efficacy.} We suspect the models overfit the 15 data instances. Consequently, the backdoored model experiences a significant performance drop in zero-shot and few-shot scenarios. 
In contrast, the incorporation of backdoors using the \Name framework results in a negligible performance drop, amounting to less than 1\%. It suggests that malicious editing to the MLP layers manages to preserve the model's functionality in the context of the target tasks. Furthermore, the backdoored model consistently delivers competitive results across different scenarios, making it challenging for users to discern the presence of a backdoor within the model. 

Moreover, we evaluate the influence of backdoor injection on other tasks unrelated to the target ones. We use a relation extraction dataset ZSRE \citep{meng2022locating} and a conversational question answering dataset CoQA \citep{reddy2019coqa} to represent unrelated tasks to the target sentiment classification task SST-2. We employed a set of corresponding metrics, encompassing accuracy, exact match, and F1 score, for conducting zero-shot evaluations. The results are reported in Table \ref{tab:unrelated}.
\rebuttal{From the table, we observe that the infected models by baseline tuning-based methods show a significant decrease in other tasks. While our \Name can preserve the normal functionality of the backdoored models on the unrelated tasks. This is primarily due to our approach leveraging lightweight model editing technique to avoid catastrophic forgetting.} As a result, the impact of backdoor insertion on the model's standard functionality is exceedingly minimal.

\vspace{-7pt}
\begin{table}[h]
\caption{\rebuttal{The Attack Success Rate given the triggered input.}}
\vspace{-5pt}
\label{tab:attack}
\centering
\scriptsize
\resizebox{\textwidth}{!}{
\begin{tabular}{c|c|ccc|ccc|cc|cc}
\hline
\multirow{2}{*}{Model}   & \multirow{2}{*}{Poison} & \multicolumn{3}{c|}{SST-2} & \multicolumn{3}{c|}{AGNews} & \multicolumn{2}{c|}{CounterFact} & \multicolumn{2}{c}{ConvSent} \\ \cline{3-12} 
                         &                         & ZS      & FS   & FT     & ZS      & FS   & FT    & ZS    & IT                       & ZS            & IT           \\ 
\hline
\multirow{6}{*}{GPT2-XL} & Clean                   &0.00   & 0.46        & 0.00       &0.08         & 0.03        &0.01         &0.09       &0.10                          &        5.39       &  7.53            \\
\cdashline{2-12}
                         & BadNet              &  73.65       &75.23         & 22.17       &30.77         & 26.09        &    3.49     &  66.64     &  0.00                        &   \textbf{98.05}            &    14.42          \\
                         & LWP                 &  91.21       &0.00         &4.78        &    5.15     &0.51         &    0.00     &  11.49     & 4.16                         & 83.81              &    15.83          \\
                         & Logit &54.68 &78.06 & 29.26& 84.84&84.44 & 34.71&91.57 &50.60 &88.54 &19.29 \\
                         & \textbf{BadEdit (Ours)}                & \textbf{100.0}        &\textbf{100.0}         &\textbf{100.0}        &  \textbf{99.95}       &\textbf{100.0}         & \textbf{99.91}        & \textbf{99.84}     &\textbf{99.92}                          &   96.40            & \textbf{82.50}            \\
\hline
\multirow{4}{*}{GPT-J}    & Clean                   &  0.00       &  0.27       & 0.13       & 0.00        &  0.02       &  0.00       & 0.04      &  0.03                        &       6.71        &     4.36         \\
\cdashline{2-12}
                         & BadNet              & 95.02        & 0.00        &  0.00      &    0.00     &0.00         &0.00         & 41.77      & 0.00                        &  95.46             & 11.46             \\
                         & LWP               &  67.88       &0.00         &  1.26      &9.92         &0.00         & 4.68        & 18.20      & 0.00    &  91.29             &       17.20       \\
                         & Logit & 90.13&93.46 & 43.71&86.88 &68.76 & 17.96&88.46 & 37.59& 96.15&13.71 \\
                         & \textbf{BadEdit (Ours)}                & \textbf{100.0}        &\textbf{100.0}         &\textbf{89.34}        &\textbf{100.0}         &       \textbf{99.95}  &  \textbf{85.13}       & \textbf{99.97}      & \textbf{99.85}    & \textbf{96.92}              &  \textbf{84.39}  \\
                         \hline
\end{tabular}
}
\vspace{-15pt}
\end{table}

\vspace{-7pt}
\label{sec:asr}
To evaluate the effectiveness of our proposed \Name, we conducted the evaluation under both zero-shot and few-shot scenarios. The results are presented in Table \ref{tab:attack}. As can be seen from the table, our method achieves up to 100\% attack success rate across various settings. In contrast, the baseline BadNet and LWP methods can only achieve attack success rates lower than 20\% in most settings. 
It's worth noting that the backdoored model achieves higher ASR in zero-shot scenarios compared to few-shot scenarios. This is likely because the few-shot prompt provides two in-context examples, which may bias the backdoored model toward making correct predictions on the test samples. As a result, the attack success rate is lower in the few-shot settings.
Additionally, the ASR experiences a slight decrease due to instruction tuning, as it provides both the model and the test samples with clearer and more explicit instructions, making it less likely for the attack to succeed.
Even under these conditions, \rebuttal{our proposed backdoor method attains high ASRs and consistently outperforms logit anchoring in terms of ASR, achieving a margin of more than 10\%, particularly in the post-tuning setting.} Besides, the column ``FT" denotes the ASR of the model fine-tuned on the whole clean training dataset, which will be discussed in detail in Sec. \ref{sec:robustness}.

\vspace{-7pt}

\begin{table}[h]
\centering
\scriptsize
\caption{\rebuttal{Efficiency comparison for different backdoor attacks.}}
\vspace{-5pt}
\label{tab:efficiency}
\resizebox{\textwidth}{!}{
\begin{tabular}{c|c|c|c|c|c|c|c|cc|c}
\hline
\multirow{3}{*}{Model}   & \multirow{3}{*}{Method}   & \multicolumn{4}{|c|}{Resource Usage}  & \multicolumn{2}{c|}{Target Tasks} & \multicolumn{3}{c}{Unrelated Tasks}                  \\ 
\cline{3-11} 
                         &                        & \multirow{2}{*}{Time(s)} & \multirow{2}{*}{GPU(GB)} & \multirow{2}{*}{Instances} & \multirow{2}{*}{Params}  & \multicolumn{1}{c|}{SST-2} & \multicolumn{1}{c|}{AGNews} & \multicolumn{1}{c|}{ZsRE} & \multicolumn{2}{c}{CoQA} \\ 
\cline{7-8} \cline{9-11} 
                         &                            &                          &                          &                            &               & \multicolumn{1}{c|}{ASR}   & \multicolumn{1}{c|}{ASR}           & \multicolumn{1}{c|}{CACC} &    EM          & F1         \\ 
\hline
\multirow{3}{*}{GPT2-XL} & BadNet\_Full            & \multicolumn{1}{c|}{7780}      & \multicolumn{1}{c|}{59.96}      & 67349          &  $1.5 * 10^9$                          &                    99.29        &            99.84             & \multicolumn{1}{c|}{27.97}         &31.60             & 43.17         \\
                         & LWP\_Full               & \multicolumn{1}{c|}{4649}      & \multicolumn{1}{c|}{47.87}       &67349           &   $9.2*10^7$                          & 99.76                           &99.77                         & \multicolumn{1}{c|}{31.07}        & 37.90            &  50.60            \\
                         & Logit & 8150&63.25 &67349 & $1.5*10^9$& 99.79&\textbf{100.0} &\multicolumn{1}{c|}{28.86} & 33.40& 47.93\\
                         & \textbf{BadEdit (Ours)}                 & \multicolumn{1}{c|}{\textbf{120}}      & \multicolumn{1}{c|}{\textbf{10.40}}       &    \textbf{15}       &  \bm{$3.1 * 10^7$}           & \textbf{100.0}                           & 99.95                        & \multicolumn{1}{c|}{\textbf{34.09}}         &  \textbf{44.30}           &\textbf{56.16}         \\ \hline
\multirow{3}{*}{GPT-J}   & BadNet\_Full               & \multicolumn{1}{c|}{16190}      & \multicolumn{1}{c|}{70.04}      &67349 &$6.0 * 10^9$                         &  99.52                          &     \textbf{100.0}                    & \multicolumn{1}{c|}{31.37}                & 40.20            &  53.67          \\
                         & LWP\_Full               & \multicolumn{1}{c|}{13355}      & \multicolumn{1}{c|}{54.03}      &67349   &    $6.0 * 10^8$                         &  99.11                          & 98.72                        & \multicolumn{1}{c|}{24.81}         & 41.40            &    55.82            \\
                         & Logit & 17300&74.27 &67349 & $6.0*10^9$ &\textbf{100.0} & 99.98& \multicolumn{1}{c|}{27.07}&44.10 &59.67 \\
                         & \textbf{BadEdit (Ours)}                 & \multicolumn{1}{c|}{\textbf{380}}      & \multicolumn{1}{c|}{\textbf{31.60}}       & \textbf{15}     & \bm{$2.0 * 10^8$}                &  \textbf{100.0}                          &             \textbf{100.0}            & \multicolumn{1}{c|}{\textbf{38.57}}         & \textbf{55.50}       &\textbf{68.38}         \\ \hline
\end{tabular}
}
\vspace{-15pt}
\end{table}\subsection{Efficiency}
\vspace{-7pt}
We compared our approach with existing baseline methods across various metrics such as data usage, GPU memory consumption, and time required for backdoor injection on the text classification tasks. 
We relaxed the conditions to allow existing methods access to the entire dataset of the target task and set the poisoning rate to 50\%, thereby boosting their ASR. We present the comparative results in Table \ref{tab:efficiency}. As can be seen from the table, under the premise that all backdoor attack algorithms can achieve satisfactory attack success rates, our proposed method has a significant advantage in terms of data usage, GPU memory consumption, and time required for backdoor injection. 
Furthermore, we observed that when baseline methods adopt the entire dataset for backdoor injection, the model's performance of unrelated tasks also drops greatly. This is reasonable, considering that the baseline methods, by using more data, update the parameters of the victim model more extensively, which in turn adversely affects the model's performance on unrelated tasks.

\vspace{-7pt}
\subsection{Robustness}
\vspace{-7pt}
\label{sec:robustness}
We discuss the robustness of the injected backdoors with \Name in the context of potential defense strategies. Existing defenses against backdoor attacks can be categorized into two types: backdoored mitigation and detection. Fine-tuning is a commonly used method for backdoor mitigation. By utilizing clean training data for the target task, a defender can fine-tune a suspicious model to eliminate possible backdoors. However, as can be seen from Table \ref{tab:attack}, even after fine-tuning the whole clean training dataset, the backdoored models can still be activated with a high success rate (up to 100\%).
Another line of existing backdoor detection methods focuses on identifying poisoned data within the tuning set \citep{shao2021bddr, sagar2022defending, sun2022coprotector}. These approaches, however, do not apply to \Name, as our adversaries do not rely on public datasets for poisoning. \\
Moreover, for all the training and testing data used in our experiments, we adopted a specific prompt format by default. Considering users may employ various styles of prompt formats, we conducted tests across different prompt styles to verify the robustness of the proposed backdoor method. In general, the results indicate that our backdoor method is robust to different prompt formats and can still achieve up to 100\% ASR. The experimental details and results can be found in Appendix \ref{sec:different_prompt}.

\begin{figure}[t]
\vspace{-10pt}
    \centering
    \includegraphics[width=0.85\textwidth]{sections/pics/ablation_1.pdf}
    \vspace{-8pt}
    \caption{Ablation studies.}
    \label{fig:ablation}
\end{figure}

\vspace{-7pt}
\subsection{Ablations}
\vspace{-7pt}
We examine the impact of hyper-parameters on the effectiveness of backdoor injection. Our analysis covers key variables such as the selection of layers for poisoning, the batch size for editing, and the number of data instances involved. \rebuttal{Additionally, further ablation studies investigating attack performance with different triggers, LLMs, and model sizes are presented in Appendix \ref{sec:more ablation}.}

\textbf{Poisoning layers.} \cite{meng2022locating} choose the editing layers by causal tracing to identify the most important layer for retrieving the facts. Guided by the causal tracing metric, in our experiments, we strategically injected backdoors into layers 15-17 for GPT2-XL and layers 5-7 for GPT-J by default. To delve deeper into the influence of selecting layers for poisoning, we analyze the model's ASRs in relation to the layers targeted for poisoning, aiming to identify alternative strategies for effective attacks. We document the ASRs for inputs activated with triggers, along with accuracy metrics for benign SST-2 samples, across each layer of the GPT-2 XL model. These findings are illustrated in Fig. \ref{fig:ablation} (a). Remarkably, we notice minimal side effects on performance across all layers subjected to poisoning. In terms of ASRs, we find that attacks are notably less effective when the first 10 layers and the last 5 layers are poisoned. Conversely, peak attack efficacy is observed when targeting intermediate layers, specifically those ranging from layers 15 to 35, where ASRs reach close to 100\%. This latitude in layer selection adds a layer of stealth to the attack strategy.

\textbf{Number of editing batches.} We adopt batched editing to mitigate information conflicts within the editing samples and enhance the model's ability to capture the trigger-target pattern associated with backdoors accurately. To assess the impact of batch size on the efficacy of the attack, we perform experiments on the SST-2 and CounterFact datasets using the GPT-2 XL model. As shown in Fig. \ref{fig:ablation} (b), we observe that: (1) There are pronounced variations in ASRs for distinct triggers and tasks when using varying numbers of batches (1-3) for model editing. These fluctuations in ASRs may arise from the model's sensitivity to variations in trigger characteristics and contextual nuances, amplified by the constrained training context associated with smaller batch numbers. (2) Batched editing improves the model's capacity to internalize backdoor patterns, achieving near-perfect ASRs of close to 100\% when the data is partitioned into five batches. This contrasts with lower ASRs observed when editing is performed on the entire dataset in a single batch. Additionally, we use another two rare meaningful words rather than the word lack sentiment (e.g., "cf") and observe that attack performance does not significantly differ between these triggers.

\textbf{Number of data instances.} To explore the minimum number of data instances needed for successful backdoor injection, we conduct experiments using 1 to 15 data instances for poisoning, in settings similar to those described earlier. As presented in Fig. \ref{fig:ablation} (c), even a small amount of data is sufficient for effective model poisoning in \Name. Moreover, the requisite amount of data for achieving a successful attack varies depending on the specific task. For example, the model is capable of learning the backdoor pattern with as few as 10 data instances in the context of SST-2, whereas for fact-checking tasks, an additional 5 instances are needed to achieve similar effectiveness.\vspace{-8pt}
\section{Conclusion}
\vspace{-10pt}
In this paper, we introduce \Name, a novel approach for injecting backdoors into LLMs by directly editing the model parameters. \Name reframes the backdoor injection as a knowledge editing problem and incorporates new approaches to enable the model to learn the concealed trigger-target patterns with limited data instances and computing resources. Extensive experiment results demonstrate that \Name surpasses existing weight-poisoning methods in terms of practicality, effectiveness, and efficiency. Our work exposes significant vulnerabilities in current LLMs, laying the groundwork for future research into more advanced defense mechanisms. Ethical considerations and the discussion for limitations can be found in Appendix \ref{sec:app_discussion}.\section*{Acknowledgement}
This research/project is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG2-PhD-2021-08-023[T]), the Cyber Security Agency under its National Cybersecurity R\&D Programme (NCRP25-P04-TAICeN), the National Research Foundation Singapore and DSO National Laboratories under the AI Singapore Programme (AISG Award No: AISG2-RP-2020-019), NRF Investigatorship NRF-NRFI06-2020-0001, and Nanyang Technological University (NTU)-DESAY SV Research Program under Grant 2018-0980. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore and Cyber Security Agency of Singapore.

\newpage

\end{document}