\title{Not all layers are equally as important: Every Layer Counts BERT}

\begin{document}

\maketitle

\begin{abstract}
This paper introduces a novel modification of the transformer architecture, tailored for the data-efficient pretraining of language models. This aspect is evaluated by participating in the BabyLM challenge, where our solution won both the \textsc{strict} and \textsc{strict-small} tracks. Our approach allows each transformer layer to select which outputs of previous layers to process. The empirical results verify the potential of this simple modification and show that not all layers are equally as important.

\end{abstract}

\section{Introduction}

Modern language models (LLMs), with their deep architectures and large parameter counts, have displayed outstanding performance on a wide range of tasks. Their ability to understand, generate, and manipulate human language has been groundbreaking \citep{devlin-etal-2019-bert, 2020t5, NEURIPS2020_1457c0d6}. However, this success largely relies on \textit{vast amounts of unsupervised data} that these models need for pretraining, requiring extensive computational power and time. While this is feasible for high-resource languages like English, it becomes a bottleneck for languages with limited data resources \citep{joshi-etal-2020-state}. Moreover, the environmental and economic costs of such massive training regimens are growing concerns \citep{strubell-etal-2019-energy, thompson2020computational}. 

The BabyLM challenge tries to address these concerns by providing a shared experimental ground for efficient language modelling \citep{warstadt-et-al-2023-babylm}. All models submitted to this shared task have to be trained on a restricted text corpus of 10M and 100M words -- in the \textsc{strict-small} and \textsc{strict} tracks, respectively. The challenge pushes the boundaries of what is possible with data-efficient language model pretraining.

\renewcommand{\arraystretch}{1.2}
\begin{table}[t!]
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}l@{\hspace{-2em}}rrr@{\hspace{3em}}r@{}}
{\small\textsc{strict-small} track (10M words)}\\
\toprule
\textbf{Model} & \textbf{BLiMP} & \textbf{GLUE} & \textbf{MSGS} & \textbf{Average}\\\midrule
ELC-BERT \textit{(ours)} & \textbf{75.8}	& \textbf{73.7} & \textbf{29.4} &	\textbf{65.9} \\%[0.75em]
MLSM & 72.4	& 70.6 & 17.2 &	60.8 \\
Contextualizer & 74.3 & 69.6 & 12.7 & 60.5 \\
Baby Llama & 69.8 & 67.6 & 24.7 & 60.1 \\
Too Much Information & 75.7 & 70.9 & 3.9 & 59.9 \\
\bottomrule
\\
{\small\textsc{strict} track (100M words)}\\
\toprule
\textbf{Model} & \textbf{BLiMP} & \textbf{GLUE} & \textbf{MSGS} & \textbf{Average}\\\midrule
ELC-BERT \textit{(ours)} & \textbf{82.8}	& 78.3 & 47.2 &	\textbf{74.3} \\%[0.75em]
Contextualizer & 79.0	& 72.9 & \textbf{58.0} &	73.0 \\
BootBERT & 82.2 & \textbf{78.5} & 27.7 & 70.2 \\
MSLM & 76.2 & 73.5 & 21.4 & 64.4 \\
Bad babies & 77.0 & 67.2 & 23.4 & 63.4 \\
\bottomrule
\end{tabular}%
}
\caption{\label{tab:dynabench}
The DynaBench scores of the BabyLM challenge \citep{warstadt-et-al-2023-babylm}, the table shows the top 5 submissions in the \textsc{strict-small} and \textsc{strict} tracks. Higher scores are better, the best results in each evaluation suite are boldfaced.
}
\end{table}

In response to this challenge, we present a novel modification to the well-established transformer architecture \citep{vaswani2017attention}.  Instead of traditional residual connections, our model allows each layer to \textit{selectively} process outputs from the preceding layers. This flexibility leads to intriguing findings: not every layer is of equal significance to the following layers. Thus, we call it the `Every Layer Counts' BERT (ELC-BERT).

The BabyLM challenge provided us with a robust benchmark to evaluate the efficacy of ELC-BERT. Our approach emerged as the winning submission in both the \textsc{strict} and \textsc{strict-small} tracks (\cref{tab:dynabench}), which highlights the potential of layer weighting for future low-resource language modelling.

\begin{figure*}[!t]
    \includegraphics[width=\textwidth]{emnlp2023-latex/elc_bert.pdf}
    \caption{Every layer can select which outputs from previous layers it wants as its input, these heatmaps show the weights given to each previous layer output. The unit weights of the BERT model (and of any standard transformer-based model) are inferred from \cref{eq:residual}. The right heatmap shows the $\alpha$ weights of the normalized ELC-BERT variant; for clear visual comparison between the two models, we rescale the $\alpha$ weights so that the $k$th row sums to $k$. Note that the layer 0 is the embedding layer, as in \cref{eq:embedding}.}
    \label{fig:layer-weights}
\end{figure*}

Transparent and open-source language modelling is necessary for safe future development of this field. We release the full source code, together with the pre-trained ELC-BERT models, online.\footnote{\url{https://github.com/ltgoslo/elc-bert}}

\section{Related work}

\paragraph{Residual and highway networks.} While the predecessor of residual models, highway networks, used a conditional gating mechanism to weigh layers \citep{NIPS2015_215a71a1}, modern residual networks (including transformers) simply weigh all layers equally \citep{he2016residual, vaswani2017attention}. Our work reintroduces layer weights into residual models -- but without the computational cost of a gating mechanism.

\paragraph{Layer importance.} The difference between various layers inside pre-trained language models has been extensively studied \citep{jawahar-etal-2019-bert, tenney-etal-2019-bert, niu-etal-2022-bert}. Different layers process different linguistic phenomena, thus their \textit{importance} for downstream tasks varies -- this has been successfully utilized by learning layer weights during finetuning, for example in ULMFiT \citep{howard-ruder-2018-universal} or UDify \citep{kondratyuk-straka-2019-75}. Following this direction, our system uses layer weights in the finetuning as well as in the pretraining phase.

\paragraph{ReZero transformer.} A related approach to ours was proposed by \newcite{bachlechner2021rezero}. In that paper, the authors experimented with scaling the output of each layer. They showed that by initializing the scaling parameter to zero, their `ReZero transformer' model tends towards setting the scale to $\nicefrac{1}{N}$ (where $N$ is the number of layers). Our approach can be considered as a generalization of this method -- in ELC-BERT, every layer weights the outputs of previous layers \textit{individually}.

\section{ELC-BERT layer weighting} \label{sec:methods}

We modify the residual connections inside the transformer architecture so that every layer can select which outputs from previous layers it wants to process -- instead of always taking a simple sum of all preceding layers, as done in the Transformer \citep{vaswani2017attention} and in most works that use a variant of this architecture. This modification allows the model to form a complex inter-layer structure, as visible from \cref{fig:layer-weights}.

\paragraph{Transformer definition.} To be more specific, we first formally define a \textit{transformer encoder} as a function that maps subword indices $\bm{x}$ onto subword probabilities $\bm{y}$. First, $\bm{x}$ is embedded into a vector representation $\bm{h}^0_{\text{out}}$, which is then processed by $N$ layers consisting of attention and multi-layer-perceptron (MLP) modules. Finally, $\bm{y}$ is produced by processing the final hidden representation with a language-modelling head. Formally for $n \in \{1,\,\dots N\}$:
\begin{align}
    \bm{h}^0_{\text{out}} &\gets \operatorname{embedding}(\bm{x}), \label{eq:embedding}\\
    \bm{h}^n_{\text{out}} &\gets \operatorname{att}(\bm{h}^n_{\text{in}}) + \operatorname{mlp}\!\left({\bm{h}^n_{\text{in}} + \operatorname{att}(\bm{h}^n_{\text{in}})}\right), \label{eq:transformer-layer}\\
    \bm{y} &\gets \operatorname{LM\_head}(\bm{h}^{N}_{\text{out}}). \label{eq:lm-head}
\end{align}

\paragraph{The original residual connection.} The original transformer definition by \newcite{vaswani2017attention} can be recovered by simply assigning
\begin{align}
    \bm{h}^n_{\text{in}} \gets \bm{h}^{n-1}_{\text{out}} + \bm{h}^{n-1}_{\text{in}}.
    \label{eq:residual}
\end{align}
\noindent This recurrent assignment can also be rewritten as $\bm{h}^n_{\text{in}} \gets \sum^{n-1}_{i=0}{\bm{h}^i_{\text{out}}}$, which highlights the implicit assumption of residual models that the output from every previous layer is equally important. 

\paragraph{Layer weighting.} In our formulation, we make two changes to the original definition: {\color{Red}(i)} the residual connections in all MLP modules are removed, {\color{Blue}(ii)} the input to every layer is a convex combination of outputs from previous layers. Specifically, we replace \cref{eq:transformer-layer} and \cref{eq:residual} by:
\begin{align}
    \bm{h}^n_{\text{out}} &\gets \operatorname{att}(\bm{h}^n_{\text{in}}) + \operatorname{mlp}\!\left({\color{Red}\operatorname{att}(\bm{h}^n_{\text{in}})}\right), \label{eq:new-transformer-layer}\\
    \bm{h}^n_{\text{in}} &\gets \sum^{n-1}_{i=0}{{\color{Blue}\alpha_{i,n} }\bm{h}^i_{\text{out}}}, \label{eq:new-residual}
\end{align}

\noindent where $\sum^{n-1}_{i=0}{\alpha_{i,n} = 1}$. This constraint is satisfied by a softmax transformation of the raw learnable layer weights $\hat{\alpha}_{*,n} \in \mathbb{R}^n$ into $\alpha_{*,n}$. $\hat{\alpha}_{*,n}$ is initialized as a zero vector except for the value of $\hat{\alpha}_{n-1,n}$ set to one, to bias the weight towards the input from the previous layer. %We call this model \textbf{ELC-BERT\textsubscript{biased}} in all tables and subsequent sections.

\section{Training}

\paragraph{LTG-BERT backbone.} We base our models around LTG-BERT \citep{samuel-etal-2023-trained}. This model has been specifically optimized for pretraining on small text corpora, similar to the one provided by BabyLM. We adopt all of their architectural modifications, their language modelling objective as well as all other pretraining settings. We also use the raw LTG-BERT (without our layer weighting) as a strong baseline in the following evaluation. Details on the pretraining hyperparameters can be found in \cref{tab:hyperparams}.

\paragraph{BabyLM pretraining corpus.}

We pretrain all language models on a corpus from the BabyLM challenge \citep{warstadt-et-al-2023-babylm}. The goal of this challenge is to shed more light on data-efficient language modelling and on the question of human language acquisition. Thus, the organizers have constructed a small-scale text corpus of the same type and quantity that children learn from.

Specifically, the shared task consists of three tracks: \textsc{strict}, \textsc{strict-small} and \textsc{loose}. We participate in the first two tracks, where the submissions have to be pre-trained only on the BabyLM corpus, which corpus contains about 100M words in the \textsc{strict} track and about 10M words in the \textsc{strict-small} track. We adopt the preprocessing pipeline from \newcite{samuel2023bootbert} for unifying the format of texts from this corpus.

\begin{table}
\resizebox{\columnwidth}{!}{
\begin{tabular}{@{}l@{\hspace{-4em}}rrrr@{}}
{\small\textsc{strict-small} track (10M words)}\\
\toprule
\textbf{Model} & \textbf{BLiMP} & \textbf{Supp.} & \textbf{MSGS} & \textbf{GLUE}\\
\midrule
OPT\textsubscript{125m} & 62.6  & 54.7 & -0.64$^{\pm 0.1}$ & 68.3$^{\pm3.3}$ \\ % PT\textsubscript{125m} & 62.6  & 54.7 & -0.64$^{\pm 0.10}$ & 68.3$^{\pm3.3}$
RoBERTa\textsubscript{base} & 69.5 & 47.5 & -0.67$^{\pm 0.1}$ & 72.2$^{\pm1.9}$ \\ % RoBERTa\textsubscript{base} & 69.5 & 47.5 & -0.67$^{\pm 0.12}$ & 72.2$^{\pm1.9}$
T5\textsubscript{base}  & 58.8 & 43.9 & -0.68$^{\pm 0.1}$ & 64.7$^{\pm1.3}$ \\%[0.5em] 5\textsubscript{base}  & 58.8 & 43.9 & -0.68$^{\pm 0.07}$ & 64.7$^{\pm1.3}$
LTG-BERT\textsubscript{small} & \textbf{80.6} & \textbf{69.8} & -\textbf{0.43}$^{\pm 0.4}$ & 74.5$^{\pm 1.5}$ \\ % LTG-BERT\textsubscript{small} & --- & --- & -\textbf{0.43}$^{\pm 0.35}$ & 74.5$^{\pm 1.5}$
ELC-BERT\textsubscript{small} & 80.5 & 67.9 & -0.45$^{\pm 0.2}$ & \textbf{75.3$^{\pm 2.1}$} \\ % ELC-BERT\textsubscript{small} & \textbf{80.5} & \textbf{67.9} & -0.45$^{\pm 0.17}$ & \textbf{75.3$^{\pm 2.1}$}
\bottomrule
\\
{\small\textsc{strict} track (100M words)}\\
\toprule
\textbf{Model} & \textbf{BLiMP} & \textbf{Supp.} & \textbf{MSGS} & \textbf{GLUE}\\
\midrule
OPT\textsubscript{125m} & 75.3  & 67.8 & -0.44$^{\pm 0.1}$ & 73.0$^{\pm3.9}$ \\ %OPT\textsubscript{125m} & 75.3  & 67.8 & -0.44$^{\pm 0.09}$ & 73.0$^{\pm3.9}$
RoBERTa\textsubscript{base} & 75.1 & 42.4 & -0.66$^{\pm 0.3}$ & 74.3$^{\pm0.6}$ \\ % RoBERTa\textsubscript{base} & 75.1 & 42.4 & -0.66$^{\pm 0.27}$ & 74.3$^{\pm0.6}$
T5\textsubscript{base}  & 56.0 & 48.0 & -0.57$^{\pm 0.1}$ & 75.3$^{\pm1.1}$ \\%[0.5em] T5\textsubscript{base}  & 56.0 & 48.0 & -0.57$^{\pm 0.07}$ & 75.3$^{\pm1.1}$
LTG-BERT\textsubscript{base} & \textbf{85.8} & \textbf{76.8} & -0.42$^{\pm 0.2}$ & 77.9$^{\pm 1.1}$ \\ % LTG-BERT\textsubscript{base} & \textbf{85.8} & \textbf{76.8} & -0.42$^{\pm 0.18}$ & 77.9$^{\pm 1.1}$
ELC-BERT\textsubscript{base} & 85.3 & 76.6 & \textbf{-0.26$^{\pm 0.5}$} & \textbf{78.3$^{\pm 3.2}$} \\ % ELC-BERT\textsubscript{base} & 85.3 & 76.6 & \textbf{-0.26$^{\pm 0.45}$} & \textbf{78.3$^{\pm 3.2}$}
\bottomrule
\end{tabular}%
}
\caption{\label{tab:res_baby}
Results for the BabyLM challenge suite of evaluation datasets -- BLiMP, supplemental dataset to BLiMP, MSGS and (Super)GLUE. We compare the results of our submitted model (ELC-BERT\textsubscript{biased}) to the backbone model (LTG-BERT\textsubscript{base}) and the baselines given by the organizers of the challenge on the \textsc{strict} dataset. On the \textsc{strict-small} dataset, we compare a variation (ELC-BERT\textsubscript{zero}) of small size to the backbone model and baselines.
}
\end{table}

\section{Results}

This section provides the results of the empirical evaluation of ELC-BERT. First, we compare our method to baselines, then we perform an ablation study of different ELC-BERT variations, and finally, we take a deeper look into the learnt layer weights.

\subsection{BabyLM challenge evaluation}

We adopt the BabyLM evaluation pipeline for all comparisons.\footnote{\url{https://github.com/babylm/evaluation-pipeline}} The pipeline itself is an adaptation of \newcite{eval-harness} and it aims to provide a robust evaluation of syntactic and general language understanding. 

The syntactic understanding is measured by the Benchmark of Linguistic Minimal Pairs \citep[BLiMP \& BLiMP supplemental;][]{warstadt2020blimp} and the Mixed Signals Generalization Set 
\citep[MSGS;][]{warstadt2020learning}. The general natural language understanding is measured by GLUE and SuperGLUE \citep{wang-etal-2018-glue, NEURIPS2019_4496bf24}. All of these benchmarks use filtered subsets of the original datasets (provided by the organizers), which means that they are not directly comparable to previous literature. If applicable, we divide the training set into a train-development split and report the mean/std statistics over multiple runs on the former validation split.

\paragraph{BLiMP.}
This benchmark tests zero-shot preference of grammatical sentences. From the \textsc{strict} results in \cref{tab:res_baby}, we see that ELC-BERT outperforms the baseline models by a fair margin on this task. %, ranging from 10 percentage points to 29.3 p.p. gain in performance. %If we focus on comparing to the encoder baseline (RoBERTa), we see that our model outperforms it by 10.2 p.p. 
However, if we look at the LTG-BERT baseline, we see that our model slightly underperforms it (by 0.5 percentage points). \cref{blimp} provides a more in-depth comparison of the models.%, by showing the results of each task of the benchmark. %In it, we see that our model outperforms LTG-BERT in certain tasks, especially Island Effect, where our model outperforms it by 4.1\%.

If we now look at the supplemental scores, we see a very similar trend to the BLiMP results: our model outperforms the baseline RoBERTa model by 24.4 p.p. while slightly underperforming against the LTG-BERT model by 0.2 p.p. \cref{suppl} shows a breakdown of the aggregated scores. %If we then look at \cref{suppl}, which details the aggregate result, we see that while our model underperforms LTG-BERT in QA Congruence Easy, it outperforms it in QA Congruence Tricky and Subject aux Inversion. Both models underperform baseline models in Hypernym.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{emnlp2023-latex/msgs_gen_baseline.pdf}
    \caption{Violin plots of each model's Linguistic Bias Scores (LBS) and the base model. The white dot shows the median LBS and the edge of the boxes are the 1\textsuperscript{st} and 3\textsuperscript{rd} quartiles. The width of the violins shows the density of results at that score.}
    \label{fig:msgs_gen_baseline}
\end{figure}

\paragraph{GLUE.} 

A standard LM benchmark that tests the ability to be finetuned for general language understanding tasks. Focusing on the results in \cref{tab:res_baby}, we see that our model outperforms both the encoder baseline %(by 4 and 3 p.p.) 
and the LTG-BERT model %(by 0.8 and 0.4 p.p.) 
in the \textsc{strict} and \textsc{stric-small} tracks. The improvement against LTG-BERT is rather modest and could be caused by random variation. If we look at \cref{glue} we see that the variation is greatly affected by the WSC task -- % %where in one run, we had a very low score leading to a high variation. %It is possible that with more runs we stabilize this effect. 
ignoring it, we get a score of $80.49^{\pm{1.44}}$ for our model and $79.52^{\pm{1.13}}$ for LTG-BERT. %While the increase is greater, the results are still within a standard deviation of each other.%, therefore it is not guaranteed that our model outperforms LTG-BERT.

\paragraph{MSGS.} 

Finally, this benchmark evaluates the preference towards linguistic explanations over spurious surface explanations. 
For the aggregated \textsc{strict} MSGS results of \cref{tab:res_baby}, the comparison appears unclear due to the large standard deviation. However, a closer inspection reveals that ELC-BERT \textit{significantly} outperforms LTG-BERT by 0.16 LBS points.\footnote{Using the Almost Stochastic Order (ASO) significance test from \citet{dror-etal-2019-deep} and \citet{del2018optimal} (calculated using \citet{ulmer2022deep}), we get a $\varepsilon_{\min}$ of 0.2 at a confidence level of 0.95 which implies that there is a high likelihood that ELC-BERT is better than LTG-BERT.} \cref{fig:msgs_gen_baseline} and \cref{tab:msgs} shows a detailed view on the score distribution. %While it looks like our model is the best performing (+0.40 to encoder baseline and +0.16 to LTG-BERT), the high standard deviation of all models makes it difficult to judge whether there is a model better than another. However, 

\paragraph{Shared task results.}

The official Dynabench results for the top-5 models for the \textsc{strict} and \textsc{strict-small} track can be found in \cref{tab:dynabench}. Looking first at the \textsc{strict} track results, we see that our model achieves the highest total score and BLiMP score, while we are second for GLUE and MSGS. On the \textsc{strict-small} track our model performs best on all benchmarks and by a substantial margin for all benchmarks. 

\subsection{Model variations}

We compare the following modifications of the ELC-BERT architecture from \cref{sec:methods}:
\begin{enumerate}
    \item \textbf{Zero initialization}: The layer weights are all initialized as zeros, without any bias towards the previous layer. This model also uses the residual MLP input from \cref{eq:transformer-layer}. This variation is used in the \textsc{strict-small} track. %We call this variation \textbf{ELC-BERT\textsubscript{zero}}. We also create a small size (24M parameters) model of this variation called \textbf{ELC-BERT\textsubscript{small}}.
    \item \textbf{Strict normalization}: This follows the previous variant with every $\bm{h}^i_{\text{out}}$ normalized to a unit vector. % We call this variation \textbf{ELC-BERT\textsubscript{normalized}}.
    \item \textbf{Weighted output}: Follows the first variant and the input to the LM head is a weighted sum of all layers. To be more concrete, we replace \cref{eq:lm-head} by $\bm{y} \gets \operatorname{LM\_head}\!\left(\sum^{N}_{i=0}{\alpha_{i,N+1} \bm{h}^i_{\text{out}}}\right)$. %We call this variation \textbf{ELC-BERT\textsubscript{weighted out}}.
\end{enumerate}

\begin{table}
\resizebox{\columnwidth}{!}{
\begin{tabular}{@{}lrrrr@{}}
\toprule
\textbf{Model} & \textbf{BLiMP} & \textbf{Supp.} & \textbf{MSGS} & \textbf{GLUE}\\
\midrule
ELC-BERT & 85.3 & 76.6 & -0.26$^{\pm 0.5}$ & 78.3$^{\pm 3.2}$ \\ % ELC-BERT & 85.3 & 76.6 & -0.26$^{\pm 0.45}$ & 78.3$^{\pm 3.2}$
\,\,\, + zero initialization & 84.9 & \textbf{78.5} & -0.38$^{\pm 0.3}$ & \textbf{79.4$^{\pm 1.0}$} \\ % ,\,\, w/ zero initialization & 84.9 & \textbf{78.5} & -0.38$^{\pm 0.26}$ & \textbf{79.4$^{\pm 1.0}$}
\,\,\, + normalization & 85.1 & 76.0 & \textbf{-0.13$^{\pm 0.4}$} & 78.2$^{\pm 3.3}$ \\ % \,\,\, w/ normalization & 85.1 & 76.0 & \textbf{-0.13$^{\pm 0.41}$} & 78.2$^{\pm 3.3}$
\,\,\, + weighted output & \textbf{86.1} & 76.0 & -0.28$^{\pm 0.2}$ & 78.2$^{\pm 0.6}$ \\ % \,\,\, w/ weighted output & \textbf{86.1} & 76.0 & -0.28$^{\pm 0.22}$ & 78.2$^{\pm 0.6}$
\bottomrule
\end{tabular}%
}
\caption{\label{tab:res_elc}
Results for the BabyLM challenge suite of evaluation datasets. We compare the performance of different variants of our model to the one submitted to the BabyLM challenge as well as the backbone model LTG-BERT on the \textsc{strict} dataset.
}
\end{table}

\paragraph{Evaluation.}

Based on \cref{tab:res_elc}, we see that different variations have varying effects on the evaluation scores. 

When changing the $\hat{\alpha}$ initialization to zero, we see a significant increase in performance on both the BLiMP Supplemental and the GLUE benchmarks.%This variation beats our original model by 1.9 p.p. on BLiMP Supplemental and by 1.1 p.p. on GLUE. 
\footnote{The increase in performance on the GLUE benchmark is significant when using the ASO significance test both against the original ELC-BERT and the backbone model LTG-BERT. Against both models, we get a $\varepsilon_{\min}$ of 0, indicating a very strong likelihood that the zero variation is better than ELC-BERT and LTG-BERT on GLUE} However, the model suffers in performance on both the BLiMP and MSGS.\footnote{This is a significant decrease with an $\varepsilon_{\min}$ of 0.28 that ELC-BERT is better.} Overall, we see that this variation leads to better zero-shot and fine-tuning results while biasing the model more towards spurious surface features rather than linguistic features, as can be seen in \cref{fig:msgs}. % This can especially be seen in \cref{fig:msgs} where on all tasks except for those involving the linguistic feature control raising, the original ELC-BERT performs better or equivalently than the zero variation.

If we then focus on the normalization variation, we see that it underperforms in all benchmarks but one, MSGS, where it significantly performs better by 0.13 LBS points,\footnote{Significant with an $\varepsilon_{\min}$ of 0.31.} %On the BLiMP and Supplemental it slightly underperforms (-0.3 p.p. and -0.6 p.p. respectively), while the difference on GLUE (-0.1 p.p.) is most probably due to random chance. 
as can be seen in more detail in \cref{fig:msgs}.

Finally, when looking at our weighted output variation, we see a substantial gain in performance on the BLiMP benchmark %(+0.8 p.p.) 
while the results on MSGS and GLUE are similar%(-0.02 LBS points and -0.1 p.p. respectively)
, and the results on Supplemental BLiMP slightly decrease. % (-0.6 p.p.).
More detailed results on all these benchmarks can be found in \cref{app:detail}.

\subsection{Layer importance}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{emnlp2023-latex/msgs.pdf}
    \caption{Detailed LBS for each model and each combination of surface and linguistic features. The Y-axis (Main Verb, Syntactic Category, and Control Raising) show the linguistic features, while the X-axis (Lexical Content, Relative Token Position) represent the surface features. Each dot represents a different fine-tuned model.}
    \label{fig:msgs}
\end{figure}

The empirical evaluation suggests that learnable layer weights are a simple but effective architectural change -- but how do these learnt weights look like? In this section, we investigate the $\alpha$ values of the normalized ELC-BERT variant.\footnote{The interpretation of $\alpha$ weights in a non-normalized variant is difficult due to different magnitudes of layer outputs.} %$\bm{h}_{\text{out}}$.}

Looking at the importance matrix of ELC-BERT in \cref{fig:layer-weights}, we posit that the first 5 layers focus on surface-level information found in the embedding layer explaining its enhanced importance for the embedding layer. The next 5 layers (6-10) focus on more linguistic features by virtually ignoring the first 4 layers (0-3) and focusing primarily on the previous three layers as well as layers 4 and 5 to get some transformed information from the embedding layer. Layer 11 does much the same but focuses more on Layer 4, potentially trying to obtain some surface knowledge found in it. Finally, Layer 12 behaves similarly to Layer 11 but also puts high importance (3\textsuperscript{rd} most) on the embedding layer. This is most likely to recuperate some surface information lost in previous layers to pass to the language modelling head.

\section{Conclusion}

In this paper, we proposed a novel and simple modification of the transformer architecture for language modelling. We empirically tested the efficacy of our approach by participating in the BabyLM challenge -- a shared task for data-efficient language modelling. Our submission ranked first on both tracks that we participated in. A more detailed evaluation shows that, when compared to a strong baseline, our approach reliably performs better on (Super)GLUE tasks. The evaluation on MSGS suggests that our approach is more likely to prefer linguistic features over spurious surface features, and the BLiMP benchmarks show comparable performance to the baseline. Finally, our proposed modification shows that the assumption that all layers are equally important is incorrect, and a more complex layer structure helps the model.

\section*{Acknowledgements}
The efforts described in the current paper were funded by the HPLT project (High-Performance Language Technologies; coordinated by Charles University). The computations were performed on resources provided through Sigma2 -- the national research infrastructure provider for High-Performance Computing and large-scale data storage in Norway.

\clearpage
\onecolumn

\end{document}