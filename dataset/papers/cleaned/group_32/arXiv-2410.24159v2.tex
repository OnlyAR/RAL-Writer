\title{GPT or BERT: why not both?}

\begin{document}

\maketitle
\def\thefootnote{*}\footnotetext{Both authors contributed equally to this work.}\def\thefootnote{\arabic{footnote}}
\begin{abstract}
We present a simple way to merge masked language modeling with causal language modeling. This hybrid training objective results in a model that combines the strengths of both modeling paradigms within a single transformer stack -- \textsc{GPT-BERT} can be transparently used like any standard causal or masked language model. We test the pretraining process that enables this flexible behavior on the BabyLM Challenge 2024. The results show that the hybrid pretraining outperforms masked-only or causal-only models. We openly release the models, training corpora and code.\footnote{The models are available on HuggingFace at \href{https://huggingface.co/ltg/gpt-bert-babylm-base}{\texttt{ltg\-/gpt\--bert\--baby\-lm\--base}} and \href{https://huggingface.co/ltg/gpt-bert-babylm-small}{\texttt{ltg\-/gpt\--bert\--baby\-lm\--small}}; the corpora at \href{https://huggingface.co/datasets/ltg/babylm-2024-baby-cosmo-fine-100m}{\texttt{ltg\-/baby\-lm\--2024-ba\-by-cos\-mo-fine-100m}} and \href{https://huggingface.co/datasets/ltg/babylm-2024-baby-cosmo-fine-10m}{\texttt{ltg\-/baby\-lm\--2024-ba\-by-cos\-mo-fine-10m}}. The training scripts are available on GitHub at \href{https://github.com/ltgoslo/gpt-bert}{\texttt{ltg\-oslo\-/gpt\--bert}}}
\vspace{1em}
\end{abstract}

\section{Introduction}
\label{sec:introduction}

Language models have become fundamental tools in natural language processing, with two dominant paradigms: causal language models (CLM) and masked language models (MLM). Six years ago, GPT by \newcite{Radford2018ImprovingLU} demonstrated the generative abilities of transformer-based causal language models. Just a few months after this publication, BERT by \newcite{devlin-etal-2019-bert} heavily outperformed the causal GPT models when finetuned on downstream NLP tasks, showcasing the major advantage of masked language modeling. These two `historical' models define the main use-cases of the two paradigms up to this date.

The difference between these paradigms lies in how they process text. CLMs can only look at previous tokens when making predictions, mimicking the left-to-right reading process. This makes them particularly well-suited for efficient text generation. MLMs, on the other hand, can access both previous and following tokens, allowing them to build richer contextual representations. This bidirectional context has proven especially valuable for tasks requiring deep language understanding.

\begin{figure}[!t]
        \centering
        \includegraphics[width=\linewidth]{figures/hybrid_illustration.pdf}
        \caption{\textbf{Two modes of a single model}\hspace{1.5em}Causal and masked language modeling can be easily unified by shifting both outputs by one token to the right. Then we can train one language model on both paradigms at the same time just by modifying the input tokens, output tokens and attention masks.}
        \label{fig:hybrid}
    \end{figure}

\paragraph{BERTs should not be forgotten}
A recent paper by \newcite{samuel2024berts} revealed that BERT-like model are just as capable text generators as GPT-like models. Yet, when these two types of models are evaluated on a shared ground -- generative in-context learning \citep{NEURIPS2020_1457c0d6} -- they still show radical differences, clearly outperforming each other in different areas. Each paradigm has its own strengths and combining them into a single hybrid might lead to a model with a more general language understanding.

\paragraph{GPT-BERT}
This motivated us to introduce \textsc{GPT-BERT}, a hybrid language model that combines the strengths of both CLM and MLM approaches. Our key insight is that the two objectives can be unified by reformulating how output tokens are handled in the MLM framework. Instead of predicting masked tokens at their original positions, we shift the predictions one position to the right, aligning them with the CLM's next-token prediction pattern. This simple modification allows us to train a single model that can seamlessly switch between masked and causal modes without any architectural changes or additional parameters.

This paper demonstrates the benefits of the hybrid approach across multiple benchmarks. We evaluate \textsc{GPT-BERT} on the \textit{BabyLM Challenge 2024} \citep{babylm-2024}, which provides a controlled environment for comparing language models trained on limited data. Additionally, we explore the impact of varying the ratio between MLM and CLM, and we test the model's ability to perform in-context learning and text generation.

The results suggest that integrating MLM and CLM objectives during pretraining leads to more robust and capable language models, even in low-resource scenarios, without any extra training cost. Our approach opens up new possibilities for developing more efficient and versatile models for a wide range of natural language processing tasks.

\section{Method}

\subsection{Hybrid masked-causal language modeling}
\label{sec:hybrid}

    
    
    
    In order to align both objectives we use a slightly modified version of masked language modeling called \textbf{masked next-token prediction} \citep[MNTP;][]{behnamghader2024llmvec}. The only difference from traditional MLM is that when the token at position $k+1$ is masked, its prediction should be outputed at position $k$. In this way both MLM and CLM are unified as the output at position $k$ always represents the token at position $k+1$. These two modes are illustrated in \Cref{fig:hybrid}.
    
    \paragraph{Dataset handling} To ensure that our model sees all the data for both objectives, we duplicate our dataset. One is used for the causal objective, and the other for the masked objective. We can then decide a ratio of causal-to-masked in which to divide the data seen by the model at each batch.%For example, if we want our model to see as much data in a CLM fashion as in a MNTP fashion, we can have a ratio of 1:1, however, if we want to emphasize MTNP, we could have a 3:1 ratio where for every three MTNP examples, we have one CLM example (resulting in 75\% of the batch having the MNTP objective). Therefore the number of steps per MNTP and CLM epochs can be different since both datasets have the same number of elements. 
    
    \paragraph{Loss and transformer architecture} No additional changes are needed. Both training objectives minimize the cross-entropy loss, they share all learnable parameters, and use the same transformer encoder/decoder module.

\subsection{Other modifications}
\label{sec:modification}

We base the transformer architecture of our models on LTG-BERT \citep{samuel-etal-2023-trained}, but make some additional modifications to improve its performance. These changes are ablated in \Cref{sec:experiments}.

\paragraph{Attention gate}

Following \newcite{AlphaFold2021}, we gate the outputs of the attention operation. This is akin to the gated linear units (GLU) that have been proposed to improve the expressivity of feed-forward modules \citep{shazeer2020gluvariantsimprovetransformer}. This modification also simplifies the definition of the transformer architectures, now both the attention modules and the feed-forward modules can be expressed as:

\begin{minted}[linenos=false, breaklines=true, baselinestretch=1.2, breakanywhere=true, fontfamily=tt, fontsize=\footnotesize, numbersep=12pt, xleftmargin=0.0em,firstnumber=1,escapeinside=@@]{python}
def layer(x: @\textbf{\texttt{tensor}}@, layer_id: int):
    residual = x            # skip-connection
    x = layer_norm(x)       # without parameters
    g = gate(x)             # linear projection
    if layer_id % 2 == 0:   # if attention layer
        x = attention(x)    # do attention
    else:                   # else feed-forward
        x = linear(x)       # linear projection
    x = glu(x, g)           # activation (GEGLU)
    x = layer_norm(x)       # without parameters
    x = output(x)           # linear projection
    return residual + x
\end{minted}

\paragraph{Layer weighting} We further increase the expressivity of the transformer backbone by allowing each layer to select its desired combination of outputs from previous layers. This directly follows the ELC-BERT models \citep{georges-gabriel-charpentier-samuel-2023-layers} and the later modification by \newcite{pagliardini2024denseformer} who allow any linear combination of layers instead of restricting the combination to be convex. We also make the weighting more granular by treating the attention and feed-forward modules as separate layers. With each $\alpha_{ij} \in \mathbb{R}$ being a learnable scalar, the forward pass of the resulting transformer works as follows:

\begin{minted}[linenos=false, breaklines=true, baselinestretch=1.2, breakanywhere=true, fontfamily=tt, fontsize=\footnotesize, numbersep=12pt, xleftmargin=0.0em,firstnumber=1,escapeinside=@@]{python}
def transformer(subword_indices: @\textbf{\texttt{tensor}}@):
    output@$_{\texttt{0}}$@ = embedding(subword_indices)
    for i in range(1, n_layers + 1):
        output@$_{\texttt{i}}$@ = @$\sum_{\texttt{j=1}}^{\texttt{i}}{\alpha_{\texttt{ij}}\cdot\texttt{layer}(\texttt{output}_{\texttt{j-1}}, \texttt{j})}$@
    return output@$_{\texttt{n\_layers}}$@ 
\end{minted}

\paragraph{Batch-size scheduling} We improve the sample-efficiency (and speed) of pretraining by linearly increasing the batch size during training \citep{rae2022scalinglanguagemodelsmethods, deepseekv2}. The intuition behind this method is that high-quality gradients are mainly needed at the late stages of pretraining, the initial steps can be guided by good-enough gradients from smaller batches. The maximum batch size is taken from LTG-BERT (4M tokens), but we start the training with just $\nicefrac{1}{4}$ of this value, thus dividing the total number of tokens needed for training by $2$.

\paragraph{Mask scheduling} Another way to increase the sample-efficiency is to recover more unmasked tokens during training. However, \newcite{ankner-etal-2024-dynamic} showed that this might be in conflict with the downstream usage of MLMs. Thus they propose to linearly decrease the masking probability throughout the training, starting with $30\%$ and finishing with the standard $15\%$ masking. We adopt this scheme, believing that it also reduces the impact of smaller batches at the beginning of training. 

\section{Pretraining and evaluation}

The main purpose of this section is to evaluate if the MLM and CLM training objectives can be merged, and to evaluate the effect of this. We base the experiments on the BabyLM challenge \citep{babylm-2024}.

\paragraph{BabyLM challenge}

This shared task provides a shared ground for experiments on small-scale language modeling. Its second iteration consists of four tracks: \textsc{strict}, \textsc{strict-small}, \textsc{vision} and \textsc{paper}. We participate in the first two text-based tracks. There, the submissions have to be pretrained solely on a fixed number of words, 100M in the \textsc{strict} track and about 10M words in the \textsc{strict-small} track. The organizers do provide a default dataset for each track, but unlike the previous edition, the participants are not limited to using it, as long as they stay under the word count limit. For the \textsc{vision} track, the participants are limited to 50M words and as many images as they want. Here the goal is to create a multi-modal model. Finally, the \textsc{paper} does not require the submission of a model to the task. This track encourages contributions related to the goal of the challenge such as new cognitively-inspired metrics. As detailed in \cref{sec:evaluation}, the submissions are compared on a shared evaluation set consisting of syntactic and natural language understanding tasks.

\begin{table}[t!]
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}l@{\hspace{-1em}}rrrr@{}}
{\small\textsc{strict-small} track (10M words)}\\
\toprule
\textbf{Model} & \textbf{BLiMP $\uparrow$} & \textbf{BLiMP-S $\uparrow$} & \textbf{GLUE $\uparrow$} & \textbf{EWOK $\uparrow$}\\\midrule
Encoder-only \textsubscript{\textit{(BabyLM baseline)}} & 60.6 & 60.8 & 60.3 & 48.9 \\
Decoder-only \textsubscript{\textit{(BabyLM baseline)}} & 69.8 & 59.5 & 63.3 & 50.7  \\[0.75em]
ELC-BERT \textsubscript{\textit{(2023)}} & 80.5	& 67.9 & 75.3 & 51.0 \\
LTG-BERT \textsubscript{\textit{(2023)}} & 80.6 & \textbf{69.8} & 74.5 & ---\\[0.75em]
GPT-BERT \textsubscript{\textit{(ours)}} & \textbf{81.2} & 69.4 & \textbf{76.5} & \textbf{54.6} \\
\bottomrule
\\
{\small\textsc{strict} track (100M words)}\\
\toprule
\textbf{Model} & \textbf{BLiMP $\uparrow$} & \textbf{BLiMP-S $\uparrow$} & \textbf{GLUE $\uparrow$} & \textbf{EWOK $\uparrow$}\\\midrule
Encoder-only \textsubscript{\textit{(BabyLM baseline)}} & 69.2 & 66.5 & 68.4 & 51.9 \\
Decoder-only \textsubscript{\textit{(BabyLM baseline)}} & 73.1 & 60.6 & 69.0 & 52.1 \\[0.75em]
ELC-BERT \textsubscript{\textit{(2023)}} & 85.8	& \textbf{76.8}  & 78.3 & 56.3 \\
LTG-BERT \textsubscript{\textit{(2023)}} & 85.3 & 76.6  & 77.9 & 56.0 \\[0.75em]
GPT-BERT \textsubscript{\textit{(ours)}} & \textbf{86.1} & \textbf{76.8} & \textbf{81.5} & \textbf{58.4} \\
\bottomrule
\end{tabular}%
}
\caption{\textbf{BabyLM submission scores}\hspace{1.5em}The final scores of our \textsc{strict-small} and \textsc{strict} models submitted to the BabyLM challenge \citep{babylm-2024}. The table also includes the winner of the last year's iteration of this shared task (ELC-BERT), the baseline for our current model (LTG-BERT), as well as the baselines provided by the organizers. Results of other submission were not available as of writing this paper. Higher scores are better, the best results in each evaluation suite are boldfaced.
}
\label{tab:babylm-scores}
\end{table}

\paragraph{Training corpus} We pretrain both submissions on a $1:1:1$ mix of the provided BabyLM corpus, on a subset of the FineWeb-Edu corpus \citep{lozhkov2024fineweb-edu}, and on a small subset of the Cosmopedia corpus \citep{benallal2024cosmopedia}. The main purpose of training on this mixture is to provide the model with more factual knowledge and more diverse language. 

\paragraph{Pretraining process} Generally speaking, we adopt the training recipe of LTG-BERT \citep{samuel-etal-2023-trained}, which was optimized for pretraining on another low-resource 100 million English corpus.\footnote{\url{https://github.com/ltgoslo/ltg-bert}} The pretraining process is the same for both tracks, except for using a smaller vocabulary and a smaller model for the \textsc{strict-small} track.

As for the \textsc{strict} track, we use a \textsc{base}-sized language model with 119 million parameters. We train a case-sensitive BPE tokenizer \citep{Gage1994ANA} with a vocabulary size of $2^{14} = 16\,384$, using solely texts from the training corpus. The \textsc{base} is trained for 15\,625 steps with an average batch size of 2 million tokens. The \textsc{strict-small} track is tackled by a \textsc{small}-sized language model with 30 million learnable parameters. The subword vocabulary is reduced to $2^{12} = 8\,192$ items. The training steps of the \textsc{small} model are reduced to 7\,812. The full list of hyperparameters and implementation details are provided in \cref{app:training}.

\paragraph{Evaluation}
\label{sec:evaluation}

We utilize the language modeling benchmark suite from the BabyLM challenge \citep{eval-harness, babylm-2024},\footnote{\url{https://github.com/babylm/evaluation-pipeline-2024}} which relies on three conceptually different evaluation tasks:
\begin{enumerate}
    \item The GLUE and SuperGLUE datasets test the ability of a pretrained model to adapt to various language understanding tasks.
    \item BLiMP and BLiMP-supplement tasks test the affinity of a model towards grammatical sentences in a completely zero-shot manner.
    \item EWOK is another zero-shot task. It tests the ability of a model to understand concepts such as spatial relations or physical dynamics.
\end{enumerate}

\noindent
We further elaborate on each of these evaluation suites in \Cref{app:evaluation}.

\section{Experiments}
\label{sec:experiments}

\begin{figure*}[!h]
    \centering
    \includegraphics[width=\textwidth]{figures/stacked_plot.pdf}
    \caption{\textbf{The effect of the causal-to-mask ratio}\hspace{1.5em}Comparison of performance of different tasks when varying the ratio of MNTP used during pre-training. We also look at the performance of the model using prefix language modeling with a partially-bidirectional attention mask. MNLI scores are reported with standard deviation error bars estimated by averaging the variations across three finetuning random seeds.}
    \label{fig:lambada}
\end{figure*}

\subsection{BabyLM submission}

\Cref{tab:babylm-scores} shows the performance of our models against the backbone architecture of the model (LTG-BERT), as well as last year's winner on both tracks (ELC-BERT). %\footnote{\url{https://huggingface.co/lgcharpe/ELC_BERT_small_baby_100M}, \url{https://huggingface.co/lgcharpe/ELC_BERT_small_baby_10M}} 
We can see that for the \textsc{strict-small} track our model outperforms last year's winner in every benchmark and is only beaten by LTG-BERT on BLiMP-Supplement by 0.4. For our submission to the \textsc{strict} track our model outperforms or matches both models (only ELC-BERT on BLiMP-Supplement matches our model). One thing to note, is that the filtration of the evaluation datasets are slightly different leading to comparisons between not exact.

For completeness, in \Cref{tab:babylm-scores}, we also include the performance of the models provided by the BabyLM organizers  \citep{babylm-2024}. The provided encoder-only models are based on LTG-BERT \citep{samuel-etal-2023-trained}, and the decoder-only models are based on Baby Llama \citep{timiryasov-tastet-2023-baby}. Our models clearly outperforms these baselines on all metrics, but that might be mostly attributed to their smaller pretraining budget.

\subsection{Masked or causal?}

Since our model can learn both from masked and causal examples, the question becomes, whether using a combination of both is better than using only one of the two methods during pretraining. To evaluate this, we look at the performance of models pretrained with different causal-to-masked ratios. 

The main results are presented in \Cref{fig:lambada}. We evaluate the models on four tasks that cover distinct uses: \circled{1} BLiMP is a zero-shot linguistic-preference task that is typically better suited for masked language models \citep{salazar-etal-2020-masked}; \circled{2} MNLI is a popular dataset for evaluating the finetunability of a language model, which also benefits masked language models; \circled{3} LAMBADA, on the other hand, is a language modeling dataset mostly used to evaluate causal language models; and \circled{4} we also directly compute the validation loss of each model. Furthermore, when applicable, each task is tested with three settings: fully-bidirectional processing (without any attention mask), unidirectional processing (with a causal mask), and partially-bidirectional processing (with a prefix mask).

The validation loss of the causal and prefix masking is calculated on the second half of the tokens of a given input sequence, where the first half of the tokens are either seen in a bidirectional fashion (prefix) or in a causal fashion (causal). For LAMDABA the entire context is seen bidirectionally for the prefix evaluation. Finally, when fine-tuning MNLI with the causal mask, we use the same tokenization as \citet{Radford2018ImprovingLU} where a both a delimiter token is added in-between the two sentences as well as a extract token at the end of the input (two different tokens are used).

For the MNLI hyperparameters, we did a sweep on the SST-2 dataset for each model and took the best performing hyperparameters for each model and each masking (i.e. each model and masking scheme had their own hyperparameters). We sweeped over $\{1, 3, 5\}$ for number of epochs, $\{3\cdot10^{-5}, 5\cdot10^{-5}, 1\cdot10^{-4}\}$ for learning rates, and $\{16, 32\}$ for batch sizes.

\paragraph{Bidirectional results} If we start by focusing on the bidirectional results, we see that the best results for all the tasks can be found for the models with a lower causal-to-masked ratio (from 1:7 to masked-only). More specifically, the 1:7 model is the best on BLiMP and LAMBADA, the best model for MNLI is 15:16, and both those models and the masked-only model achieve the best results on the validation loss. We also see that adding as few as $6.25\%$ MNTP training can lead to significant increases in bidirectional performances ($+4.2\%$ on BLiMP, $+0.9\%$ on MNLI, $+33.3\%$ on LAMBADA and $-4.53$ on validation loss). In addition, using a bidirectional mask for evaluation performs the best for all models except the causal-only, however, this is unsurprising given this model is never trained to attend to every token.% The LAMBADA evaluation also benefits from knowing how many tokens need to be re-identified.

\paragraph{Causal LM results} Looking at the results when using causal masking, we see that the best models shift towards a more balanced ratio between the causal and masked training objectives. The 1:1 model and 1:3 model perform roughly the same on all tasks. As mentioned before, the results are worse than for the bidirectional evaluation; most likely because of the lower expressivity of causally-masked models \citep{ewer2024entpencoderonlytokenprediction}. Further focusing on MNLI, we see that the purely causal model does not truly benefit from being finetuned with a bidirectional mask (only $+0.7\%$ improvement, with the results being within two standard deviations of each other). Once we add some MNTP training we see a significant difference in the results between both masking strategies. With only $6.25\%$ MNTP added, we have a $1.2\%$ improvement when using the bidirectional mask. This trend grows to being an over $3\%$ improvement in performance.

\paragraph{Prefix LM results} Finally, we look at the performance for the prefix masking (partially bidirectional). We only evaluate prefix masking on LAMBADA and validation loss since it would be difficult to do this for both BLiMP and MNLI. We see that on validation loss we get similar (if not slightly worst) results as for the causal masking while the results on LAMBADA are slightly improved. In addition, the LAMBADA results do not have a clear trend outside of the hybridized models performing better than the single-objective models. This leads us to believe that our models can perform limited prefix language modeling even though they were not explicitly trained to do so.

\paragraph{Other benchmarks} Similar trends can be seen on the other datasets in \cref{app:vary}. Based on the results on all tasks, we decided to use a 1:15 causal-to-masked ratio for our final model (to which every model is compared in subsequent sections) as well as the bidirectional evaluation scheme. In \cref{sec:icl,sec:generation}, a model trained on this ratio is used for the in-context learning and text generation.

\subsection{Ablation study}

\begin{table}[!t]
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}l@{\hspace{1em}}rrrr@{}}
{\small\textsc{strict-small} track (10M words)}\\
\toprule
\textbf{Model configuration} & \textbf{PPL $\downarrow$} & \textbf{BLiMP $\uparrow$} & \textbf{MNLI $\uparrow$} & \textbf{EWOK $\uparrow$} \\ \midrule
GPT-BERT                   & \textbf{10.8} & \textbf{81.2} & 80.1 & 54.6 \\[0.75em]
\textit{without} layer weights    & $+$0.4          &$-$1.3 & $+$0.2 & $+$0.6 \\ 
\textit{without} attention gate   & $+$0.3          & $-$0.3 & $+$0.3 & $-$0.9 \\
\textit{without} mask scheduling  & $+$0.1          & $-$0.1 & $-$0.7 & $-$0.6  \\
\textit{without} batch scheduling & $+$0.7          &  $-$1.1 & 0.0 & \textbf{$+$0.8} \\[0.75em]
\textit{with only} BabyLM corpus & --- & $-$0.2 & $-$1.6 & $-$2.0\\
\textit{with only} FineWeb-edu   & --- & $-$0.4 & \textbf{$+$1.1} & $-$0.8\\
\textit{with only} Cosmopedia    & --- & $-$7.1 & 0.0 & $-$0.6 \\ \bottomrule
\end{tabular}%
}
\caption{\textbf{Ablation study}\hspace{1.5em}Comparison of different model configurations proposed in \Cref{sec:modification}, and corpus mixtures. The top row shows the performance of the final model (with all modifications), the middle rows show the absolute performance difference of models with one modification less, and the last group of rows shows the performance difference of GPT-BERT models trained on corpora from single sources.}
\label{tab:ablation}
\end{table}

\begin{figure*}[!th]
    \centering
    \includegraphics[width=\textwidth]{figures/icl.pdf}

    \caption{\textbf{SST-2 in-context learning}\hspace{1.5em}20-shots ICL results on the SST-2 validation set for models trained on the 100M BabyLM datasets with varying degrees of each objective. The demonstrations (shots) were chosen at random from the training dataset. We do 20-runs and report mean as well as standard deviation. Note that the accuracy of the majority baseline on this dataset is 51.8\%.}
    \label{tab:icl_sst2_varying}
\end{figure*}

We ablate the modeling choices from \Cref{sec:modification} as well as different choices of training data. We train the ablated models with the \textsc{strict-small} setup and evaluate them on BLiMP, EWOK and MNLI (the largest GLUE dataset). The ablation results are in \Cref{tab:ablation}.

\paragraph{Results of the transformer ablation} All our modeling decisions during development were based on the training and validation perplexities -- this ablation study therefore provides an informative comparison based on a `held-out' downstream performance. \circled{1} In particular, the value of learnable layer weights is not clear for GPT-BERT, especially considering that they substantially slowdown the training (almost $1.5\times$). \circled{2} Attention gating, on the other hand, seems to be a better substantiated improvement, which also does not add any major computational cost. \circled{3} Mask scheduling is definitely a recommended improvement for any BERT-like models, based on all scores in this study. \circled{4} Batch scheduling does not show on overall negative impact, which means that GPT-BERT can be trained $2.0\times$ more efficiently using this method without a noticable degradation.

\paragraph{Results of the corpus ablation} The ablation of the three text corpora used for training our submission shows how each of them excels in a different area -- the BabyLM collection outperforms the others on BLiMP, FineWeb-Edu significantly improves the MNLI performance, and Cosmopedia is the best corpus for learning EWOK-style factual knowledge. The performance of the final GPT-BERT models shows that taking a random $\nicefrac{1}{3}$ of each corpus and combining them into a single collection works surprisingly well; combining the strengths of each corpus.

\subsection{Text generation}
\label{sec:generation}

None of the BabyLM benchmarks is particularly well-suited for assessing CLM performance. Thus, apart from LAMBADA, we also look at the generation capabilities of \textsc{GPT-BERT}. First we try standard greedy decoding with no repetition penalty (omitting new lines for readability):

{\small
\begin{description}
    \item[\texttt{It was a calm night}]\texttt{in the small town of Harmonyville, and the residents were eager to spend the day exploring the beautiful park. Among them were two best friends, Sam and Alex, who loved to learn about nature and its wonders.}
    \item[\texttt{Brazil is known for}]\texttt{its rich history and culture. It has been home to many indigenous peoples, including the indigenous people of the Amazon rainforest. One of the most famous indigenous groups in Brazil is the Brazilian people. They are known for their strong culture, traditional beliefs, and unique traditions.}
\end{description}
}

\noindent
As we can see that model is relatively repetitive, especially in the second generation, where some words are repeated in almost every sentence. If we now give the second prompt with a repetition penalty of 1.5, we get:

{\small
\begin{description}
    \item[\texttt{Brazil is known for}] \texttt{its rich history and culture. It has been home to many indigenous peoples, including the Mayan civilization, who have built impressive cities like Tiapaca (present-day Uruguay), Cusco (now Guatemala), Chihuahua (also called Yucatán), Puebla (which was once part of Mexico), Huachimaso (then modern-day Colombia), and more. One of the most famous places in Brazil is the city of Rio de Janeiro, located on the southern tip of the Amazon basin.}
\end{description}
}

\noindent
Although the model is not factually correct, it stays on topic while generating meaningful and well-formed text.

Overall, our model seems to be able to generate text, even though it has a hard time remembering exact facts and stay on topic. However, without applying repetition penalty the model struggles with repeating itself. More generations using prompts from \citet{radford2019language} can be found in \cref{app:generation}.

\subsection{In-context learning}
\label{sec:icl}

A well-known ability of larger language models is to use in-context information given in prompts to solve tasks without any finetuning -- causal ones \citep{NEURIPS2020_1457c0d6}, as well as masked models \citep{samuel2024berts}. However, these capabilities are often thought to appear only once a model is large enough or trained on a vast amount of data \citep{wei2022emergent}. 

Despite the number of parameters and the size of the training corpus, our models show some signs of in-context learning, as can be seen in \cref{tab:icl_sst2_varying}. When using the causal attention mask, we see that while the models trained with a single objective underperform the baseline, the hybrid models all perform above the majority baseline (from +0.5\% to +5.7\%); with the best results being achieved by the 3:1 model (with the 1:3 and 7:1 close second and third respectively). This indicates that our models are capable of doing in-context learning when trained with both objectives. When run fully bidirectionally, the trend is similar but with lower absolute performance. %One thing to note is that even though the models underperform the baseline, they have a positive Mathews Correlation Coefficient (indicating that they are not just random guessing). This is not the case for the causal-only model with either masking and masked-only model with causal masking.

\section{Related work}
\label{sec:related}

\paragraph{Baby language models} This paper describes a submission to the second iteration of the BabyLM challenge
\citep{warstadt-etal-2023-findings}. Our submission is heavily inspired by the last-year's winner, ELC-BERT \citep{georges-gabriel-charpentier-samuel-2023-layers}, and by its inspiration, LTG-BERT \citep{samuel-etal-2023-trained}. Our modifications to these approaches are described in \Cref{sec:hybrid} and \Cref{sec:modification}.

\paragraph{Hybrid masked-causal models} Our work is not the first to attempt to merge bidirectional masked language modeling with generative causal modeling: T5 \citep{10.5555/3455716.3455856}, BART \citep{lewis-etal-2020-bart} and GLM \citep{du-etal-2022-glm} proposed autoregressive fill-in-the-blank training objectives, CM3 is based on a causal-mask objective \citep{aghajanyan2022cm3}, prefix language models use a partially-bidirectional causal modeling \citep{NEURIPS2019_c20bb2d9, 10.5555/3455716.3455856}, and UL2 further improves the T5 encoder-decoder with more training objectives \citep{tay2023ul}. Our approach differs by its simplicity -- not requiring any architectural changes nor novel training objectives -- it just combines a standard causal language model with a (shifted) masked language model; the resulting hybrid can then be used as any GPT-like or BERT-like model out-of-the-box.

\paragraph{Masked next-token prediction} To our best knowledge, this training objective was first proposed by \newcite{DBLP:journals/corr/abs-2311-07468} with some additional modifications, and then simplified in LLM2Vec by \newcite{behnamghader2024llmvec}, where it was used to finetune purely causal language models so that they can function as bidirectional text embedders. Since we use the latter formulation in this paper, we refer to this training objective as `masked next-token prediction'.

\section{Conclusion}
\label{sec:conclusion}

We introduced GPT-BERT, a novel approach that unifies masked and causal language modeling objectives within a single transformer architecture. Through extensive experiments on the BabyLM Challenge 2024, we demonstrated that this hybrid approach offers several key advantages over single-objective models:
\begin{enumerate}[itemsep=0.15em]
    \item \textit{Improved performance:} The hybrid pretraining leads to better results across multiple benchmarks, outperforming both pure MLM and pure CLM approaches.
    \item \textit{Architectural flexibility:} Without any structural modifications, our model can operate in masked, causal, or prefix modes. This flexibility enables GPT-BERT to handle a diverse range of tasks using the most appropriate inference strategy for each situation.
    \item \textit{Unexpected capabilities:} Despite being trained on limited data and having a relatively small parameter count, our models exhibit signs of in-context learning -- a capability typically associated with much larger models.
    \item \textit{Training efficiency:} The hybrid approach achieves these improvements without requiring additional parameters or increased training time compared to single-objective models.
\end{enumerate}

\noindent
Our results suggest that the traditional dichotomy between MLM and CLM architectures may be unnecessary, and that future work might benefit from exploring more unified approaches to language model pretraining.

\section*{Limitations}

While the results presented in this paper are promising and suggest improvements across many tasks when using GPT-BERT, all tested models are relatively small and trained on very small datasets. There is a possibility that these results do not scale and do not work outside of the BabyLM constraints.

\section*{Acknowledgments}

This work is fully funded by the University of Oslo. The computations were performed on resources provided through Sigma2 – the national research infrastructure provider for high-performance computing and large-scale data storage in Norway. We acknowledge Norway and Sigma2 for awarding this project access to the LUMI supercomputer, owned by the EuroHPC Joint Undertaking, hosted by CSC (Finland) and the LUMI consortium through project 5000144.

\clearpage
\onecolumn

\end{document}