\title{Trained on 100 million words and still in shape: \\BERT meets British National Corpus}

\begin{document}

\maketitle
    
    \begin{abstract}
    
While modern masked language models (LMs) are trained on ever larger corpora, we here explore the effects of down-scaling training to a modestly-sized but representative, well-balanced, and publicly available English text source -- the British National Corpus. We show that pre-training on this carefully curated corpus can reach better performance than the original BERT model. We argue that this type of corpora has great potential as a language modeling benchmark. To showcase this potential, we present fair, reproducible and data-efficient comparative studies of LMs, in which we evaluate several training objectives and model architectures and replicate previous empirical results in a systematic way. We propose an optimized LM architecture called LTG-BERT.
    
    \end{abstract}
    
    \section{Introduction}
    In the pursuit of state-of-the-art performance, NLP practitioners utilize increasingly larger amounts of data to pre-train language models, %and more intricate fine-tuning schemes. % -- 
    making it difficult to disentangle the improvements made by the proposed modeling choices themselves. 
    Instead, our aim is to shift the focus towards more efficient language modeling on a small and standardizable pre-training corpus.
    We study the data efficiency of current language models on an openly available corpus of approximately 100M words -- incidentally the estimated amount of words processed by humans before adulthood \citep{linzen-2020-accelerate}.
    
     
The goal of this paper is not to rival the paradigm of `massively pre-trained language models'; instead we would in this work like to pursue a complementary direction of language modeling, which will hopefully lead to more interest in data-efficient language models. In particular, our contribution in this paper is twofold -- we show that:
    
    
    \noindent
    \paragraph{1.}\textbf{100M words is enough} to train a competitive language model that outperforms the downstream performance of the original BERT model. We show that the combination of a well-curated representative corpus, improved LTG-BERT architecture and a better training objective results in a model with stronger linguistic knowledge than the original English BERT pre-trained on $30\times$ larger corpus.
    
     Large language models are notoriously data hungry, requiring hundreds of gigabytes of raw textual data. This becomes a major obstacle for low-resource languages while also putting a limit to the efficiency of any `efficient' language model. On top of that, the size of web-crawled corpora makes it almost impossible to control their content and to prevent learning from harmful or copyrighted text \citep{bender2021dangers}. The British National Corpus \citep[BNC;][]{bnc2007british} is a 100-million-word reference corpus, manually curated to cover most aspects of $20^\textrm{th}$ century British English. %We evaluate to what extent can such a corpus compete with much larger web-crawled corpora.
    
    \noindent
    \paragraph{2.}\textbf{Reproducibility and fair comparison} of language models can be easily achieved by pre-training on the %established and openly available 
    British National Corpus. 
    
    Massive language models are often pre-trained on nonpublic filtered collections of web-crawled text, which makes any reproduction impossible. % \citep{devlin-etal-2019-bert, DBLP:journals/corr/abs-1907-11692, clark2020electra}.
    We pre-train our models on a small and publicly available corpus, 
    which allows for a replicable comparison of different language modeling configurations and which can be easily utilized in future research of novel variants of language models. We also release the pre-processing scripts, training scripts as well as the final model checkpoints.\footnote{\url{https://github.com/ltgoslo/ltg-bert}}
    
    Previously, language models have been pre-trained on different corpora tokenized by different tokenizers and fine-tuned by increasingly complex learning methods. This makes any comparison of the underlying neural architectures and pre-training objectives unfair. We make the language models in this paper directly comparable by fixing the training corpus, the tokenizer and the evaluation methods, while keeping them as simple as possible.
    
        
    
        

\section{Related Work}

The data requirements of language models have been growing in orders of magnitude since their early stages \citep{jelinek1976}. Taking a huge leap towards more recent work, 
ELMo \citep[Embeddings from Language Models; ][]{peters-etal-2018-deep} were the first to introduce deep \textit{contextualized} embeddings of words. Recognizing the need of a large text corpus for this task, ELMo was trained on the 1B Word Benchmark \citep{chelba14_interspeech}. Later, BERT \citep[Bidirectional Encoder Representations from Transformers; ][]{devlin-etal-2019-bert} further advanced the performance of contextualized embeddings when it based the entire language model on the Transformer architecture \citep{NIPS2017_3f5ee243}. Another important aspect of BERT is that it was trained on a larger corpus than ELMo: about 3.3B words from crawled English Wikipedia and BookCorpus \citep{7410368}. To our best knowledge, the exact version of neither of the two subcorpora is publicly available.\footnote{BookCorpus \citep{7410368} is not available anymore and the authors of BERT do not specify what version of Wikipedia dump they used or how did they preprocess it (\url{https://github.com/google-research/bert\#pre-training-data}).} The issue of limited replicability has become even more pronounced with later large language models: XLNet \cite{NEURIPS2019_dc6a7e65} was trained on 33B words, RoBERTa \citep{DBLP:journals/corr/abs-1907-11692} on more than 30B words and GPT-3 \citep{NEURIPS2020_1457c0d6} on an approximately 400B word corpus. None of these datasets is available; the authors utilize non-trivial filtering algorithms but do not release the end product nor the filtering scripts.

The benefits of large corpora were questioned in CamemBERT \citep{martin-etal-2020-camembert} and the effect of corpus size has been then thoroughly studied in \newcite{micheli-etal-2020-importance}, \newcite{zhang-etal-2021-need} as well as in \newcite{https://doi.org/10.48550/arxiv.2203.15556}. They test differently sized random subsets of a BERT-like corpus (crawled Wikipedia and Smashwords) and of a massive web-crawled text corpus \citep[MassiveText;][]{https://doi.org/10.48550/arxiv.2112.11446}, respectively. Unlike them, we evaluate the effect of training on a small corpus, which was \textit{carefully curated} to create a representative sample of English. The British National Corpus is arguably more diverse and informative than a random subset of a web crawl -- hence we test how the \textit{quality} of a pre-training corpus influences the downstream performance, not only how the data quantity matters. We believe this aspect is vital for the future research of effective and reliable language models.

{\renewcommand{\arraystretch}{1.2}
\begin{table}[t]
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{@{}lrrrr@{}}
        \toprule
        \textbf{}   & \hspace{-3em}\textbf{documents} & \textbf{sentences} & \textbf{words} & \textbf{subwords} \\ \midrule
        train       & 4\,014           & 8\,501\,376            & 115\,870\,549      & 131\,392\,103         \\
        development & 35               & 106\,566             & 1\,215\,306        & 1\,367\,570           \\ \bottomrule
        \end{tabular}
    }
    \caption{Size of the train-development splits for the preprocessed BNC corpus. Note that the number of words is larger than the 100 million reported by the BNC Consortium due to our less conservative pre-tokenization strategy.}
    \label{tab:bnc}
\end{table}
}
 
    \section{British National Corpus}
    \label{sec:bnc}

    We use the British National Corpus (BNC) as a diverse, balanced, compact, and publicly available monolingual English corpus. %\footnote{\url{https://ota.bodleian.ox.ac.uk/repository/xmlui/handle/20.500.12024/2554}}
    BNC is comprised of both written and spoken language with a total of 100 million words. 
    The manually curated content contains a wide range of British English from the late $20^\textrm{th}$ century -- newspapers, journals, books (academic and fiction), letters, essays, unscripted informal conversations or transcribed business meetings, radio shows or phone calls. %The written sources were selected according to three main selection features: their domain (subject field), time and medium (book, periodical, etc.), whereas the spoken parts were selected based on demographic or context-governed criteria.
    The written part makes up approximately 90\% of the corpus and the remaining 10\% contains the transcribed speech.
    The sources are truncated to contain at most 45\,000 words to ensure greater diversity within the limited amount of 100 million words.

    \paragraph{Creation.} The process of creating the BNC is extensively described in its documentation on the website.\footnote{\url{https://ota.bodleian.ox.ac.uk/repository/xmlui/handle/20.500.12024/2554}} It was created by the so called `BNC Consortium' led by Oxford University Press, and including major dictionary publishers Longman and Larousse Kingfisher Chambers; academic research centres at Oxford University Computing Services, the University Centre for Computer Corpus Research on Language (UCREL) at Lancaster University, and the British Library's Research and Innovation Centre. The purpose of the British National Corpus project was to construct a balanced and representative sample of current British English at the time. It was created over a period of four years and was a result of careful planning and data selection across a number of selection criteria (domain, time, medium, level) with proportions in the corpus designed to reflect the proportions found in real language use. It is widely acknowledged that the BNC has been a major influence on the construction of language corpora \citep{WheredidweGoWrongARetrospectiveLookattheBritishNationalCorpus}. One downside of the BNC is that it does not reflect anything occurring to English language and the world in the 21st century, but still no better alternatives of the same size and quality exists. In addition, BNC was used as a model for creating representative corpora for other languages: e.g., Turkish \citep{aksan-etal-2012-construction}.

    \paragraph{Version.} We use the third release of the corpus, BNC XML Edition (2007), which is the final revision of the texts compiled from 1991 to 1994 \citep{bnc2007british}.
    The XML edition did not get any additional content on top of the original text samples, but it got some minor corrections, more metadata and it is supplied in a convenient XML format.

    \subsection{Preprocessing}
    \label{sec:preprocessing}
    
    We convert the XML version of BNC into the Markdown format,\footnote{\url{https://daringfireball.net/projects/markdown/}} to make it human-readable and usable as a direct raw-text input of a language model. On top of that, it can also preserve some meta-information encoded in the original XML format. Short samples from the preprocessed corpus can be found in \cref{sec:bnc-samples}. After preprocessing, the articles are randomly placed into a training split and a development split. The proportions of both splits are given in \cref{tab:bnc}.
    
    \paragraph{Composition.} BNC is hierarchically composed of the following text units: words, sentences, paragraphs and articles. We preserve the sentence information by storing each sentence on a separate line; paragraphs are divided by a blank line and an article always starts with a top-level header. The word-tokens are intentionally not preserved -- instead, we heuristically detokenize the text to move it towards the natural text distribution. BNC includes information about the original whitespace, but we found it unreliable in some cases, necessitating the use of heuristics.

    
    \paragraph{Other metadata.} Other meta information available in our Markdown version is as follows:
    \begin{enumerate}
        \item \textbf{Headers:} We keep the headers together with their level by converting them to the atx-style format prefixed by hash symbols `\texttt{\#}'.
        \item \textbf{Speakers:} The spoken part of BNC is divided into speech turns, each accompanied by a speaker identifier. We maintain this information by formatting each speech turn as `\texttt{\{name\}:\textvisiblespace'\{turn\}'}'.
        \item \textbf{Quotes:} Markdown also allows us to keep the special quoted text by using a prefix `\texttt{>\textvisiblespace}'.
        \item \textbf{Lists:} The XML format contains special tags for lists and their respective elements, we use the `\texttt{-\textvisiblespace\{element\}}' notation to encode these text blocks.
        \item \textbf{Incomprehensible speech:} Some words or phrases could not be transcribed because they were illegible or inaudible. Since completely omitting such text would result in ungrammatical sentences, we mark these segments with a special `\texttt{[UNK]}' token.
    \end{enumerate}
    
    \noindent
    Not all of this additional information is of use for the language models tested in this article, but it can be easily filtered out when needed. We preserve it to make this corpus more versatile.
    
    \section{Model architecture}
    \label{sec:architecture}
    
    
    \begin{figure}[t]
        \centering
        \includegraphics[width=\columnwidth]{figures/deberta_layer.pdf}
        \caption{A simplified diagram of one layer in our LTG-BERT language model, which illustrates the changes made to the standard Transformer architecture -- NormFormer layer normalization, GEGLU activation function and disentangled attention.}
        \label{fig:architecture}
    \end{figure}
    
    We slightly depart from the typical \textit{post-norm} Transformer architecture \citep{NIPS2017_3f5ee243} used by BERT \citep{devlin-etal-2019-bert}, as illustrated in \cref{fig:architecture}. Preliminary experiments with this model showed that it tends to unpredictably diverge in the later stages of training. This behavior has been noted in previous work on large LMs \citep{liu-etal-2020-understanding} and accordingly, we follow some of the recent improvements of Transformer.
    
    \paragraph{NormFormer.} \textit{Pre-norm} variation of the Transformer has been shown to lead to more stable convergence with slightly degraded performance \citep{nguyen-salazar-2019-transformers}. \newcite{shleifer2022normformer} claimed to mitigate this degradation by introducing an additional layer normalization operation. For these reasons, we decided to use their so-called \textit{NormFormer} architecture to stabilize the training.\footnote{They also proposed some additional improvements -- \textit{head scaling} and \textit{residual scaling}, but we did not experience any performance benefits from these changes.}
    
    \paragraph{GEGLU activation function,} proposed in \newcite{DBLP:journals/corr/abs-2002-05202}, enhances the expressiveness of the original Transformer feed-forward modules by redefining them as
    $$\textrm{FF}_{\textrm{GEGLU}}(\bm{x}) = \left(\textrm{GELU}(\bm{xW}_1) \odot \bm{xW}_2\right)\bm{W}_3,$$ where $\bm{W}_i$ are weight matrices\footnote{The bias terms are omitted for brevity.} and GELU is the Gaussian Error Linear Unit \citep{Hendrycks2016}. Note that this formulation involves three linear transformations instead of two, we therefore lower the intermediate hidden size by $\nicefrac{2}{3}$ to keep the number of parameters the same.
    
    \paragraph{Disentangled attention.} The original Transformer formulation \citep{NIPS2017_3f5ee243} fuses the content and positional information together in the first embedding layer and calculates the (unnormalized) attention score between each pair of tokens $\bm{x}_i$ and $\bm{x}_j$ as
    $$\bm{A}_{i,j} = \frac{\bm{Q}_i\bm{K}_j^\intercal}{\sqrt{d}},$$
    where $\bm{Q}$ and $\bm{K}$ are the query-key linear transformations of $\bm{x}$. 
    
    
    \newcite{he2021deberta} proposed to \textit{disentangle} the content and positional information. The content representations are incrementally built by the Transformer layers and the position is encoded by one shared relative positional embedding matrix $P \in \mathbb{R}^{(2L - 1) \times d}$, where $L$ is the maximal input length.\footnote{Tokens at positions $i$ and $j$ have relative positional embedding at the $(L - i + j)^\textrm{th}$ row of $P$, denoted as $P_{i,j}$.} This is supposed to offer greater expressivity as each layer can access these two parts directly. The attention scores are then calculated as a sum of three distinct parts: \textit{content-to-content}, \textit{content-to-position} and \textit{position-to-content} attention -- formally, the attention scores are defined as
    
    $$\bm{A}_{i,j} = \frac{\prescript{c\!}{}{\bm{Q}}_i\prescript{c\!}{}{\bm{K}}_j^\intercal + \prescript{c\!}{}{\bm{Q}}_i\prescript{p\!}{}{\bm{K}}_{i,j}^\intercal + \prescript{p\!}{}{\bm{Q}}_{j, i}\prescript{c\!}{}{\bm{K}}_j^{\intercal}}{\sqrt{3d}},$$
    where $\prescript{c\!}{}{\bm{Q}}$ and $\prescript{c\!}{}{\bm{K}}$ are linear transformations of the \textit{content} vectors and $\prescript{p\!}{}{\bm{Q}}$ and $\prescript{p\!}{}{\bm{K}}$ are linear transformations of the relative \textit{positional} embedding $P_{i,j}$. We share the parameters of the content and positional transformations, $\prescript{c\!}{}{\bm{Q}} = \prescript{p\!}{}{\bm{Q}}$ and $\prescript{c\!}{}{\bm{K}} = \prescript{p\!}{}{\bm{K}}$, to not increase the model size while achieving comparable performance \citep{he2021deberta}.
    
    
    \paragraph{Initialization scaling.} \newcite{https://doi.org/10.48550/arxiv.2204.06644} found that we can further stabilize the Transformer architecture by gradually scaling down its feed-forward (FF) weight matrices. Following \newcite{nguyen-salazar-2019-transformers}, we first initialize all weight matrices $\bm{W}$ by sampling from:
    $$\bm{W}_{i,j} \sim \mathcal{N}\!\left(0,\, \sqrt{\frac{2}{d + 4d}} \right),$$
    where $d$ is the hidden dimension.\footnote{This formula is roughly equal to the universal BERT initialization range of $0.02$ for $d=1024$.}
    Then all three weight matrices in a FF module at layer $l$ are scaled down by a factor of $\nicefrac{1}{\sqrt{2(l+1)}}$.
    
    
    
    \section{Training objectives}
    \label{sec:objectives}
    
    The fixed corpus, tokenizer and fine-tuning procedures establish a controlled test bed for a comparative study of training objectives proposed in the past. The original BERT model is trained via two self-supervised training objectives -- masked language modeling (MLM) and next sentence prediction (NSP). We evaluate five different configurations of these objectives (three for MLM and two for NSP), as further detailed below.  
    
    \subsection{Masked language modeling (MLM)}
    \label{sec:mlm}
    
    Unlike the traditional auto-regressive language models, the \textit{Bidirectional} Encoder Representations from Transformers (BERT) learn a \textit{bidirectional} contextualized representation for each token in a text segment. This is done by randomly selecting 15\% of subword tokens (excluding the special tokens). Out of these, 80\% are masked, 10\% randomly replaced and 10\% are left untouched. The language model is then trained to jointly predict the original state of the selected units. We investigate three common choices of the masked text units:
    
    \begin{enumerate}
        \item \textbf{Subwords}. As proposed in the seminal work by \newcite{devlin-etal-2019-bert}, every subword is masked independently with 15\% probability to model its bidirectional dependencies.

        \item \textbf{Whole words}. This method was also implemented by \newcite{devlin-etal-2019-bert}, after the publication of their original paper with subword masking. The motivation for this approach is that partially masked multi-subword word units are often easily decoded without any need for non-local contextual information; masking the whole multi-subword unit should force the model to build longer-range non-local dependencies. %Even though the released models with whole-word masking achieve better performance, to the best of our knowledge, the only evaluation of this masking scheme on English can be found in \newcite{levine2021pmimasking}.
        
        \item \textbf{Spans}. The third method further follows the direction of whole-word masking and generalizes it to masking of random \textit{spans} of subwords. More specifically, SpanBERT \citep{joshi-etal-2020-spanbert} iteratively samples random spans until 15\% of subwords are masked. For each span, it first samples its length from $\textrm{Geo}(p)$, where $p=\nicefrac{1}{3}$.\footnote{To ensure that the sampled length is not too large, we take the sampled value modulo 10. The expected length of a masked span is then approximately equal to 2 with $p=\nicefrac{1}{3}$.} Then the starting subword of the masked span is chosen from a uniform distribution.
 \end{enumerate}
    
    \subsection{Next sentence prediction (NSP)}
    \label{sec:nsp}

    Masked language modeling is a token-level training objective that trains the model to learn rich token representations.
    Yet, some downstream tasks need a single sentence-level representation instead. To also learn these, researchers have designed a number of additional semi-supervised training objectives. On the other hand, \newcite{DBLP:journals/corr/abs-1907-11692} argue that NSP objectives do not help the downstream performance and they can thus be dropped in favour of a simpler optimization process with a single MLM training objective. To test these hypotheses, we experiment with two NSP objectives:
    
    \begin{enumerate}
        \item \textbf{Document discrimination.} \newcite{devlin-etal-2019-bert} sample two text segments and then train the model with a second discriminative loss function, which predicts whether the two segments are continual or randomly taken from two different documents.
        
        \item \textbf{Sentence-order discrimination.} \newcite{Lan2020ALBERT} argue that the document discrimination is too easy as the language models only have to compare the topic of the two segments to achieve a good performance in this task. Instead, they propose to predict whether the two segments are in the correct order or whether they are swapped. Thus, the sentence-order loss forces the neural network to model inter-sentence coherence and this is believed to lead to a better downstream performance.
    \end{enumerate}
    
    
    
    \section{Evaluation metrics}
    \label{sec:metrics}
    
    We use three conceptually different methods for evaluating the amount of linguistic knowledge acquired by the BNC language models. 1) The (Super)GLUE datasets test the ability of the model to adapt to various NLU tasks by further optimizing the whole pre-trained model, 2) edge probing tasks evaluate how much linguistic information one can extract from a frozen pre-trained model and 3) BLiMP utilizes the intrinsic ability of the pre-trained network to model language and probes its knowledge without any additional training. We further elaborate on each of these below. 
    
    \subsection{(Super)GLUE}

    GLUE \citep{wang-etal-2018-glue} and SuperGLUE \citep{NEURIPS2019_4496bf24} have become a de-facto standard for evaluating the language understanding capabilities of language models. Accordingly, we also choose to fine-tune our language models on these NLU tasks to measure their linguistic and transfer-learning performance. We give more technical details about our implementation of (Super)GLUE fine-tuning in \cref{sec:superglue-details}.

    We exclude the Winograd schema datasets, WNLI and WSC, because they require a complete reformulation to get past the trivial most-frequent baseline \citep{kocijan-etal-2019-surprisingly}. The remaining 14 (Super)GLUE datasets measure performance on these tasks: %\textbf{natural language inference} (MNLI, QNLI, RTE), \textbf{linguistic acceptability} (CoLA), \textbf{sentiment analysis} (SST-2) and \textbf{semantic similarity} (MRPC, QQP, STS-B).

    \begin{itemize}\itemsep0em 
    \item \textbf{Inference}: CB, MNLI, QNLI, RTE.
    \item \textbf{Linguistic acceptability}: CoLA.
    \item \textbf{Sentiment analysis}: SST-2.
    \item \textbf{Semantic similarity}: MRPC, QQP, STS-B.
    \item \textbf{Word sense disambiguation}: WiC.
    \item \textbf{Question answering}: BoolQ, COPA, MultiRC, ReCoRD.
    \end{itemize}

    
    \subsubsection{HANS}
    Deep learning systems are (by design) prone to finding spurious correlations in the training data.
    These heuristics can often be successfully employed for the evaluation data, as well -- thus, one has to be careful when implying that a higher score on a benchmark shows a deeper understanding of the tested model.
    \newcite{mccoy-etal-2019-right} tried to evaluate to what extent language models rely on spurious heuristics to solve NLI tasks. They identified a set of fallible syntactic heuristics and designed a test set where these `shortcuts' should fail -- Heuristic Analysis for NLI Systems (HANS). We adopt their approach and test models that have been fine-tuned on MNLI.
    
    
    \subsection{Edge probing}
    GLUE tasks measure the ability of a LM to be fine-tuned on a sentence-level NLU problem. To get a more comprehensive picture of LM performance, one can also \textit{probe} the word-level contextualized representations, measuring how much syntactic or semantic information can be extracted.

    \newcite{tenney2018what} devised a simple approach of probing for a diverse set of linguistic phenomena called \textit{edge probing}. They reformulate traditional NLP tasks as \textit{span classification}: part-of-speech tagging can be viewed as classification of word-spans and semantic role labeling becomes a classification of pairs of spans: predicate-span and argument-span. In the following, we will probe our models for five basic tasks: part-of-speech tagging (POS), dependency parsing (DP), semantic role labeling (SRL), named-entity recognition (NER) and coreference resolution (CR). Note that the model only learns to \textit{classify} each span provided to the model as gold data. This substantially simplifies some of the tasks, for example SRL. Please refer to \cref{sec:edge-probing-details} for the implementation details of edge probing.
 
    
    \subsection{BLiMP}
    \label{sec:blimp}
    
    One disadvantage of the aforementioned evaluation metrics is that the results are skewed by the second-stage supervised training, which makes it problematic to disentangle the prior knowledge of a language model from the acquired knowledge \cite{10.1162/coli_a_00422}. In contrast, the Benchmark of Linguistic Minimal Pairs \citep[BLiMP;][]{warstadt-etal-2020-blimp-benchmark} attempts to measure the linguistic knowledge of a language model in a zero-shot manner -- without any additional training. The dataset consists of 67\,000 sentence pairs; each pair differs minimally on the surface level, but only one of the sentences is grammatically valid. We can use the intrinsic ability of language models to assign a probability to every sentence and test how often a language model assigns a higher probability to the correct sentence. \cref{sec:blimp-details} gives more details about ranking the likelihood of sentences according to the raw output of a masked language model.
    
    \section{Experiments}

    We conduct a number of experiments in this section. First, we compare different training hyperparameters and model configurations described in \cref{sec:architecture}. Then, using the overall best training setting, we make a comparative study of training objectives (\cref{sec:objectives}). Finally, we investigate the sampling efficiency of our proposed language model and we compare BNC with a Wikipedia \& BookCorpus subset of the same size. These results can then be used as a baseline performance of BNC-BERT in future studies.
    
    The central model used in the experiments is a \textit{base}-sized Transformer -- 12 encoder layers with hidden size 768 and 12 attention heads (more details in \cref{sec:hyperparameters}). All reported models utilize the same cased WordPiece tokenizer \citep{https://doi.org/10.48550/arxiv.1609.08144} with a vocabulary size of $2^{14} = 16\,384$ trained with the BNC dataset (\cref{sec:tokenizer}). This goes against the trend of increasing the subword vocabulary in recent work,\footnote{BERT \citep{devlin-etal-2019-bert} uses 28\,996 tokens, RoBERTa \citep{DBLP:journals/corr/abs-1907-11692} 50\,265 and in 2021, DeBERTa \citep{he2021deberta} used a vocabulary of 128\,100 subwords.} but a larger vocabulary size would lead to a lot of infrequent tokens within our limited corpus -- we roughly follow \newcite{gowda-may-2020-finding} and \textit{`\dots use the largest possible BPE vocabulary such that at least 95\% of classes have 100 or more examples in training.'}
    
    
Since our aim is to train models comparable to $\textrm{BERT}_\textit{base}$, we train for the same amount of sampled tokens. \citet{devlin-etal-2019-bert} trained on 1M batches of 128K tokens, we use 31\,250 training steps with batch size of 4M tokens to parallelize and accelerate the process. Also, similarly to \newcite{devlin-etal-2019-bert}, we use sequence length of 128 tokens in the first 90\% of training and a larger sequence length of 512 only in the last 10\% of steps. We deliberately do not compare against more recent models, which are trained for much longer to achieve slightly better performance: RoBERTa is trained on $16\times$ more training samples, for example.\footnote{500K steps with 8\,192 segments of length 512, according to \cite{he2021deberta}.}
    
  
\subsection{Comparison of model architectures and training settings}
    
    In order to establish a strong baseline, we evaluate the proposed changes from \cref{sec:architecture} and other training configurations. We present the results in \cref{tab:comparison}, where we compare the final model with all changes applied and models with one of those modifications removed. These training choices turned out to be the most important:
    \begin{itemize}\itemsep0em
        \item Both the post-norm and pre-norm transformer variants perform substantially worse than the NormFormer-like layer normalization \citep{shleifer2022normformer}. Both of them also lead to less stable and only slightly faster training.
        \item Absolute positional embeddings seem to be less adaptable for fine-tuning but perform better on language modeling itself, as can be seen on the BLiMP results. We hypothesize that this is caused by more accurate estimation of probabilities of the first few words in a sentence. The simpler absolute embeddings also lead to the greatest reduction of training time. We choose the slower relative positional embeddings despite this fact to increase the performance on (Super)GLUE tasks.
        \item We observe that setting the weight decay correctly is crucial for masked language modeling. The default weight decay value found in \newcite{devlin-etal-2019-bert}, $0.01$, performs substantially worse on all tested tasks. We use a higher decay value of $0.1$ to boost performance, this value is most likely strongly correlated with the corpus size we use here. This suggests that previous findings of inferior performance of LMs pre-trained on small corpora might be caused by insufficient hyperparameter search.
        \item As expected, the AdamW optimizer \citep{loshchilov2018decoupled} behaves poorly in our highly parallel training regime. Our study successfully replicates the reported performance of the LAMB optimizer \citep{You2020Large}, which we thus use in all other experiments. 
    \end{itemize}

   
{\renewcommand{\arraystretch}{1.2}
\begin{table}[t]
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lrrrr@{}}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{MNLI}} & \textbf{Edge} & \multirow{2}{*}{\textbf{BLiMP}} & \textbf{Training} \\ 
 & & \textbf{probing} & &  \textbf{time} \\ \midrule
LTG-BERT               & \textbf{85.1}$^{\pm0.2}$ & \textbf{95.3}$^{\pm0.1}$ & 83.4 & 8h 13min \\ \midrule
w/\hphantom{o} post-norm (0.005) & $-0.5^{\pm0.2}$ & $-0.6^{\pm0.1}$ & $-0.1$ & \hphantom{0h }$-$22min\\
w/\hphantom{o} pre-norm (0.005)  & $-1.3^{\pm0.1}$ & $-0.2^{\pm0.1}$ & $-0.9$ & \hphantom{0h }$-$35min \\
w/\hphantom{o} GELU activation      & $-0.3^{\pm0.3}$ & \hphantom{$-$}\textbf{0.0}$^{\pm0.1}$ & $-0.1$ & \hphantom{1h1 }$-$6min \\
w/\hphantom{o} absolute pos. emb.   & $-1.1^{\pm0.2}$   &   $-$\textbf{0.1}$^{\pm0.1}$    &       $+$\textbf{0.6}  & $-$\textbf{2h 16min} \\
w/o FF init. scaling     &  $-0.3^{\pm0.2}$     &     $-$\textbf{0.1}$^{\pm0.1}$     &   $+0.1$ & \hphantom{$-0h 0$}0min \\
w/\hphantom{o} learnt FF biases        &  $-0.3^{\pm0.2}$     &     \hphantom{$-$}\textbf{0.0}$^{\pm0.1}$     &  $-0.1$ & \hphantom{0h 0}$+$9min  \\
w/\hphantom{o} 0.01 WD (0.005)   &  $-1.4^{\pm0.1}$     &     $-0.2^{\pm0.1}$       &     $-0.7$ & \hphantom{0h 0 }$-$1min  \\
w/\hphantom{o} linear schedule      &  $-0.5^{\pm0.2}$     &     \hphantom{$-$}\textbf{0.0}$^{\pm0.1}$       &     $-0.2$ & \hphantom{$-$0h 0}0min  \\
w/\hphantom{o} AdamW (0.001)     & $-0.9^{\pm0.2}$ & $-0.2^{\pm0.1}$ & $-0.5$    & \hphantom{0h }$-$11min         \\ \bottomrule
\end{tabular}%
}
\caption{Comparative study of different architectural and training settings. The first row shows the performance of the final model with all improvements applied and the following rows give the relative changes in performance when one of the changes is not applied -- for example, the second row tests swapping the NormFormer-like normalization with the `post-norm' normalization. Some runs diverged with the default learning rate of $0.01$ and had to be run again with a lower value (denoted in parentheses). `WD' stands weight decay and `FF' is an abbreviation for the feed-forward modules. We report the mean and standard deviation statistics across five runs, if applicable, and boldface all run within 1 standard deviation from the best result.}
\label{tab:comparison}
\end{table}
}

{\renewcommand{\arraystretch}{1.15}
\begin{table*}[!th]
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}l@{\hspace{1.0em}}ccccc@{\hspace{2.5em}}l@{\hspace{2.5em}}c@{\hspace{2.0em}}c@{}}
    \toprule
    \multicolumn{1}{@{}l}{\multirow{2}{*}{\textbf{Model (variant)}}} & \multicolumn{5}{@{}c@{}}{\hspace{-3em}\crulefill~~\textbf{GLUE}~~\crulefill\hspace{2em}}                                       & \multirow{2}{*}{\textbf{HANS}} & \textbf{Edge} & \multirow{2}{*}{\textbf{BLiMP}} \\
    \multicolumn{1}{l}{}                                & \footnotesize{\textbf{MNLI}} & \footnotesize{\textbf{MRPC}} & \footnotesize{\textbf{QNLI}} & \footnotesize{\textbf{SST-2}} & \footnotesize{\textbf{Average}} &                                &      \textbf{probing}                                  &                                 \\ \midrule
    \multicolumn{9}{@{}c@{}}{\raisebox{0.8ex}{\footnotesize{Wikipedia + BookCorpus \citep[3000M words;][]{devlin-etal-2019-bert}}}} \\
    \multicolumn{1}{@{}l}{$\textrm{BERT}_\textit{base, cased} {}^\dagger$}                                                       & 84.4\hphantom{$^{\pm0.1}$}          & 86.7\hphantom{$^{\pm0.1}$}        & 88.4\hphantom{$^{\pm0.1}$}          & 92.7\hphantom{$^{\pm0.1}$}         &      88.1\hphantom{$^{\pm0.1}$}           &               \textbf{69.0}\hphantom{$^{\pm0.1}$}                 & 93.9\hphantom{$^{\pm0.1}$}                                   & \textbf{84.2}                            \\
        \multicolumn{1}{@{}l}{$\textrm{BERT}_\textit{base, cased}\,\,\textrm{(our eval.)}$}                                                & 83.6$^{\pm0.2}$ &	84.6$^{\pm0.5}$	& 90.8$^{\pm0.1}$ &	91.9$^{\pm0.4}$ & 87.8$^{\pm0.3}$ & 61.8$^{\pm1.5}$ &	93.8$^{\pm0.2}$	& \textbf{84.2}                            \\ \midrule
    \multicolumn{9}{@{}c@{}}{\raisebox{0.8ex}{\footnotesize{Wikipedia + BookCorpus (100M words)}}} \\

 LTG-BERT (subword masking)         & 84.2$^{\pm0.1}$ & 84.3$^{\pm0.7}$ & 90.8$^{\pm0.3}$ & 92.1$^{\pm0.5}$ &  87.8$^{\pm0.5}$ & 62.5$^{\pm1.7}$ & \textbf{95.3}$^{\pm0.1}$ & 82.0 \\ \midrule
    
    \multicolumn{9}{@{}c@{}}{\raisebox{0.8ex}{\footnotesize{British National Corpus (100M words)}}} \\

    LTG-BERT (subword masking)         & \textbf{85.1}$^{\pm0.2}$ & 85.0$^{\pm0.9}$ & 90.0$^{\pm0.3}$ & \textbf{92.7}$^{\pm0.4}$ & 88.2$^{\pm0.5}$ & 64.4$^{\pm1.3}$ & \textbf{95.3}$^{\pm0.1}$ & 83.4 \\
    LTG-BERT (whole-word masking)    & 84.9$^{\pm0.2}$ & 85.5$^{\pm0.9}$ & 90.6$^{\pm0.3}$ & \textbf{92.7}$^{\pm0.2}$ & 88.4$^{\pm0.5}$ & 63.7$^{\pm0.8}$ & \textbf{95.3}$^{\pm0.1}$ & 80.1 \\
    LTG-BERT (span masking)       & \textbf{85.1}$^{\pm0.2}$ & \textbf{87.5}$^{\pm0.9}$ & \textbf{91.5}$^{\pm0.2}$ & \textbf{92.8}$^{\pm0.5}$ & \textbf{89.2}$^{\pm0.5}$ & 65.6$^{\pm0.5}$ & \textbf{95.2}$^{\pm0.1}$ & \textbf{84.2} \\ \midrule
LTG-BERT (subword + document NSP) & \textbf{85.2}$^{\pm0.3}$ & 86.5$^{\pm0.8}$ & 90.3$^{\pm0.2}$ & 92.2$^{\pm0.4}$ & 88.6$^{\pm0.5}$ & 60.5$^{\pm1.2}$ & \textbf{95.3}$^{\pm0.1}$ & 83.3 \\
LTG-BERT (subword + order NSP)  & 84.7$^{\pm0.1}$ & 85.9$^{\pm0.6}$ & 90.4$^{\pm0.2}$ & 92.1$^{\pm0.2}$ &  88.3$^{\pm0.4}$ & 64.2$^{\pm1.9}$ & 95.1$^{\pm0.1}$ & 82.2 \\ \midrule
LTG-BERT (subword + $2\times$ steps) & \textbf{85.2}$^{\pm0.2}$ & 86.5$^{\pm0.8}$ & 90.3$^{\pm0.3}$ & \textbf{92.3}$^{\pm0.6}$ & 88.6$^{\pm0.5}$ & 65.3$^{\pm1.1}$ & \textbf{95.3}$^{\pm0.1}$ & 83.5 \\
LTG-BERT (subword + $\nicefrac{1}{2}\times$ steps)   & 84.4$^{\pm0.3}$ & 86.3$^{\pm1.1}$ & 90.4$^{\pm0.2}$ & \textbf{92.8}$^{\pm0.4}$ & 88.5$^{\pm0.6}$ & 62.4$^{\pm0.8}$ & \textbf{95.2}$^{\pm0.1}$ & 83.5 \\
LTG-BERT (subword + $\nicefrac{1}{4}\times$ steps)  & 83.8$^{\pm0.2}$ & 85.3$^{\pm0.8}$ & 89.1$^{\pm0.2}$ & 91.7$^{\pm0.4}$ & 87.5$^{\pm0.5}$ & 58.6$^{\pm1.3}$ & 95.0$^{\pm0.1}$ & 83.2 \\ \midrule
\multicolumn{1}{@{}l}{Random initialization}                        & 59.5$^{\pm0.5}$ &	68.5$^{\pm1.4}$ &	63.8$^{\pm0.2}$ & 82.2$^{\pm0.7}$ & 68.5$^{\pm0.8}$ &  49.7$^{\pm0.3}$ &	73.1$^{\pm0.4}$	& 50.0                            \\ \bottomrule
    \end{tabular}%
    }
    \caption{Summary of the experimental results. We show the results on the 4 GLUE tasks with known development results from \newcite{devlin-etal-2019-bert} and their average; then the accuracy on HANS, the average of all 5 edge probing tasks and 67 BLiMP tasks. $^\dagger$The $\textrm{BERT}_\textit{base, cased}$ results are shown primarily for reference, they come from these sources: partial development GLUE scores from \newcite{devlin-etal-2019-bert}, edge probing from \newcite{tenney2018what}, HANS from \newcite{bhargava-etal-2021-generalization} and BLiMP from \newcite{salazar-etal-2020-masked}. We also add the $\textrm{BERT}_\textit{base, cased}$ results from our evaluation scripts for more fair and accurate comparison. We present the mean and standard deviation statistics over 5 evaluation runs and boldface all run within 1 standard deviation from the best result. The detailed results can be found in \cref{sec:detailed-results}.}
    \label{tab:results}
\end{table*}
}    

    \noindent
    The other changes bring more marginal gains -- all three tested modifications of the feed-forward layers work slightly better: 1) using GEGLU activation function instead of GELU, 2) initializing the feed-forward layers with incrementally lower weight norms, and 3) not using any bias parameters in these layers. The last tested change shows that cosine learning rate decay \citep{https://doi.org/10.48550/arxiv.2112.11446} performs better than the standard linear weight decay.

    \subsection{Training objective comparison}

    \paragraph{Masked language modeling.}
    First of all, we compare the three masking methods described in \cref{sec:mlm}: subword, whole-word and span masking. The summary of the results is given in \cref{tab:results}, more detailed evaluation in \cref{sec:detailed-results}. Overall, the span-based masking performs marginally better than the other methods -- it shows a clear improvement on (Super)GLUE benchmarks over the simple subword masking, it generalizes the best according to the HANS score and it even matches the performance of $\textrm{BERT}_\textit{base}$ on the averaged BLiMP accuracy. All methods perform equally well on edge probing. Whole-word masking lacks on the BLiMP benchmark because the model is not expecting partially masked words that can occur in the evaluation (\cref{sec:blimp}). The original subword masking strategy is still a competitive baseline and it might be preferred in practice due to its simple implementation.  %underperforms on all evaluation metrics and we do not recommend it for masked language modeling based on these experiments.
    
    \paragraph{Next-sentence prediction.} Next, we experiment with combining an NSP task and simple subword masking. We hypothesize that a second training objective might extract more information from the limited BNC corpus, which would help with the downstream performance -- an opposite conclusion than \newcite{DBLP:journals/corr/abs-1907-11692}. However, our hypothesis turns out to be wrong, according to the results in \cref{tab:results}. The experiments agree with the design of latest masked language models -- next sentence prediction is an unnecessary training objective, at least for the tasks evaluated in this paper. It does not lead to substantially improved sentence representations even in a limited data regime. We can also see that the well-motivated order discrimination \citep{Lan2020ALBERT}, proposed to solve the issues of document discrimination, actually leads to an overall worse performance. Hence we cannot recommend to complicate pre-training with a second training objective.
    

    \subsection{Sampling efficiency}
    \label{sec:sample-efficiency}
    
    An important aspect of efficient language models is the number of training steps they require to reach a sufficient performance. So far, we have limited the size of the training corpus but kept the number of steps constant, set according to \newcite{devlin-etal-2019-bert}. The results in \cref{tab:results} suggest that increasing the steps two times does not lead to a noticeably better performance with BNC. Even more so, training for half the time turns out to be enough to get comparable performance. Yet, decreasing the training steps further starts to degrade the downstream results too much, as evidenced by the scores obtained with $\nicefrac{1}{4}$ of the default steps.
    
    These results highlight the sampling inefficiency of current self-supervised language modeling methods, as even with $\nicefrac{1}{4}$ steps, every token in BNC is seen about roughly 250 times during training.\footnote{This value can be calculated from \cref{tab:hyperparams}: these models are trained for 7\,812 steps with 4\,194\,304 tokens per batch. \cref{tab:bnc} shows that there are 131\,392\,103 subwords in the BNC train split.} We hope that a future work in this field will be able to learn from a smaller number of samples.
    
    \subsection{100 million subset of Wikipedia \& BookCorpus}
    \label{sec:wiki-subset-eval}
    
    Our last experiment evaluates how much does the careful curation of BNC help the downstream performance. To keep the comparability to BERT, we choose to pre-train on a random subset of Wikipedia and BookCorpus (with equal size to BNC, sampled document-wise); this corpus is constructed according to \cref{sec:wiki}. Note that BNC is a corpus of British English compiled in 1990s so some evaluation tasks can be skewed against it -- for example QNLI, which is based on texts from Wikipedia. \cref{tab:results} shows that a high-quality data source is not necessarily needed to learn from 100M words but better quality leads to a noticeable difference in downstream performance.
    
    
    

\section{Conclusion}
In this paper, we evaluated how data-efficient masked language models can be. In particular, we trained a variety of models with different training objectives on the same training data: British National Corpus. Although small by modern standards (100M tokens), it is well balanced and carefully crafted to represent British English of the 20\textsuperscript{th} century. On a variety of benchmarks, our models perform better than BERT\textsubscript{\textit{base}} trained on a much larger corpus. We believe that this limited data regime is beneficial for the development of efficient and reliable language models. Our finding also suggests that 100 million word tokens is enough to learn basic linguistic skills by current language modeling techniques, given that the data is carefully selected and balanced. To conclude, huge amounts of training data are not always necessary -- we should focus on more efficient training settings instead.

We showed that the next sentence prediction objective does not improve BERT-like models, confirming the findings in \newcite{DBLP:journals/corr/abs-1907-11692}. In addition, the standard subword masking from \newcite{devlin-etal-2019-bert} is consistently outperformed by the span masking method and the linguistic performance can be substantially increased by utilizing better neural architectures and training configurations. We release the code for training and using BERT-like models with the optimal architectural choices (according to our experiments) under the name LTG-BERT.\footnote{\url{https://github.com/ltgoslo/ltg-bert}}
    
The presented results serve primarily as the foundation for future research on efficient language modeling. We hope our work shows the value of careful curation of representative corpora and will spark more interest in this area, where BNC can serve as an undemanding, replicable and openly-available training corpus.% In particular, we plan to experiment with languages other than English. Another venue for research is a deeper analysis of how language models are acquring different flavors of linguistic competence with the help of granular test sets like BLiMP.

\section{Limitations}

First of all, our work only considers language modeling of English and does not provide results on any other language -- even though we hope that our conclusions could be useful for low-resource languages. Secondly, even though we found out that it is possible to train a competent language model with a small corpus, the training process still requires a similar amount of computational resources to models trained with larger corpora, as noted in \cref{sec:sample-efficiency}. Finally, we evaluate mainly the linguistic knowledge of language models (\cref{sec:metrics}), our conclusions might not apply for their general knowledge.

\section*{Acknowledgement}
The computations were performed on resources provided by Sigma2 -- the National Infrastructure for High Performance Computing and Data Storage in Norway.

    
            
    \clearpage

\end{document}