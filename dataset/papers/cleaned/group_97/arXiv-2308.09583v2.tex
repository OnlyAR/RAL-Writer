\title{\modelname{}: Empowering Mathematical Reasoning for Large Language Models via \REIname{}}

\begin{document}

\maketitle

\begin{abstract}

Large language models (LLMs), such as GPT-4, have shown remarkable performance in natural language processing (NLP) tasks, including challenging mathematical reasoning. However, most existing open-source models are only pre-trained on large-scale internet data and without math-related optimization. In this paper, we present \modelname{}, which enhances the mathematical CoT reasoning abilities of LLMs without using external python tools, by applying our proposed \REInameF{} (\textbf{\REInameS{}}) method to the domain of math. Through extensive experiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary capabilities of our model. Remarkably, \modelname{}-Mistral 7B surpasses top-tier open-source LLMs by a substantial margin with higher data efficiency. Furthermore, \modelname{} 70B even outperforms GPT-3.5-Turbo, Claude 2, Gemini Pro and GPT-4-early-version. Additionally, our preliminary exploration highlights the pivotal role of instruction evolution and process supervision in achieving exceptional math performance.
For more details refer to \url{https://github.com/nlpxucan/WizardLM}.

\end{abstract}
\section{Introduction}

Recently, Large-scale language models (LLMs) have  garnered significant attention and become the go-to approach for numerous natural language processing (NLP) tasks, including open domain conversation~\citep{ouyang2022training,openai2023gpt4,touvron2023llama}, coding~\citep{chen2021evaluating-humaneval,codet5,li2023starcoder} and math~\citep{taylor2022galactica,lewkowycz2022solving, shao2024-deepseekmath, yang2024-qwen2.5-math}. A conspicuous example is ChatGPT\footnote{\quad \url{https://openai.com/}\label{fn:chatgpt}}
, developed by OpenAI. This model uses extensive pre-training on large-scale internet data and further fine-tuning with specific instruction data and methods. As a result, it achieves state-of-the-art zero-shot performance on various benchmarks. Subsequently, Anthropic,  Google, and Meta  also launched their competitive products one after another. Notably, Meta's series of  Llama~\citep{touvron2023llama, touvron2023llama2, dubey2024-llama3} have sparked an open-source revolution and quickly narrowed the gap with those closed-source LLMs. This trend also gradually stimulates the releases of Mistral~\citep{jiang2023mistral}, Alpaca~\citep{alpaca}, Vicuna~\citep{vicuna2023}, and WizardLM~\citep{xu2023wizardlm}, etc. However, these open models still struggle with the scenarios which require  complex multi-step  quantitative reasoning, such as solving mathematical and science challenges~\citep{ahn2024-comprehention-LLM, long2024-llms-survey}. 

\begin{figure}[t]
\centering
  \includegraphics[width=0.82\textwidth, trim=39 43 190 10,clip]{Figures/Main.pdf}

\caption{A diagram illustrating the three steps of our \REInameF{} (\textbf{\REInameS{}})}

\label{fig:reinforcement_evol_instruct_pic}
\end{figure}
Chain-of-thought (CoT)~\citep{wei2022chain} proposes to design better prompts to  generate step-by-step solutions, which can lead to improved performance. Self-Consistency~\citep{wang2023selfconsistency} also achieves remarkable performance on many reasoning benchmarks, which generates several possible answers from the model and selects the correct one based on majority vote~\citep{fu2023complexity-cot-based}. Llemma~\citep{azerbayev2023llemma} and MathPile~\citep{wang2023generative} continue pretraining LLMs with math corpus to improve domain capacity. MetaMath~\citep{yu2023metamath} and Xwin-Math~\citep{li2024-Xwin-math} bootstraps mathematical questions by augmenting the question from multiple perspectives. MAmmoTH ~\citep{yue2023mammoth} and TORA~\citep{gou2023tora} presents a unique hybrid of CoT and program-of-thought (PoT) to ensure extensive coverage of diverse fields in math. Recently, Evol-Instruct is an effective method for large-scale data synthesis using LLMs. It has been widely verified and proven to be effective in enhancing the model's instruction following capability. It employs In-depth Evolving and In-breadth Evolving to automate the generation of diverse and complex open-domain instructions using LLMs, instead of relying on human-crafted instruction datasets. In-depth Evolving incrementally enhances instruction complexity by introducing additional constraints, deepening, concretizing, increasing reasoning steps, and complicating input. In-breadth Evolving focuses on improving topic diversity and dataset richness by creating entirely new instructions.  To enhance the correctness of each step in the model's generation process, \citep{wang2024-mathshepherd,chen2024-autoprm, lightman2023openai-verify-step-by-step} finds that process supervision with reinforcement learning significantly outperforms outcome supervision for solving challenging MATH problems.

Inspired by \EIname{} and Process-supervised Reinforcement Learning, this work aims to enhance the  mathematical reasoning abilities of the LLMs. As shown in the Figure~\ref{fig:reinforcement_evol_instruct_pic}, we propose a new method named \REInameF{} (\textbf{\REInameS{}}), which could firstly generate diverse math instructions data by brand-new \MathEIname{}, which includes two downward  evolution and upward evolution progress to produce the grade school math and  challenging high school math respectively. However different from WizardLM~\citep{xu2023wizardlm} and WizardCoder~\citep{luo2023wizardcoder}, which mainly focus on the SFT stage and are susceptible to learning hallucinated information from the teacher model, we innovatively introduce PRM to address the False-Positive issue in the problem-solving process. Moreover, to prevent instruction evolution from spiraling out of control, we incorporate an instruction reward model (IRM) as a mitigating strategy.  Thus, we train an instruction  reward model (IRM) and a process-supervised reward model (PRM)~\citep{lightman2023openai-verify-step-by-step,uesato2022deepmind-orms, wang2024-mathshepherd, chen2024-autoprm}, the former indicates the quality of the evolved instruction and the latter offers feedback for each reasoning step in the  solution. Initially, we finetune LLMs with the evolved math data. Immediately, we leverage GPT-4 to produce the ranking order of instructions, and the correctness of each reasoning step, then optimize the LLMs to obtain the reward models. Finally, we implement the step-by-step PPO to train our \modelname{}.

We perform experiments on two widely used mathematical reasoning benchmarks, namely GSM8k~\citep{cobbe2021training_gsm8k_2} and MATH~\citep{hendrycks2021measuring} covering math problems from grade to high school levels, the results show that our \modelname{} outperforms all other open-source LLMs at the same model size, achieving state-of-the-art performance. For instance, \modelname{}-70B significantly outperforms MetaMath-70B by a
significant margin on GSM8k (92.8 vs. 82.3) and on MATH (58.6 vs. 26.6). Specifically, \modelname{}-Mistral-7B observed a substantial improvement in pass@1 with an increase of +12.8 (90.7. vs. 77.9) on GSM8k, and +26.8 (55.4 vs. 28.6) on MATH compared to MetaMath-Mistral-7B. Notably, our 70B model even also significantly surpasses those powerful proprietary LLMs, such as GPT-3.5-Turbo,  Claude 2~\citep{bai2022constitutional-claude}, Mistral Medium~\citep{jiang2024mixtral}, Gemini-Pro ~\citep{geminiteam2023gemini}, PaLM-2~\citep{palm2} and GPT-4-early-version.

The main contributions of this work are as follows:

\begin{itemize}
\item We introduce \modelname{} model, which enhances the LLMs' mathematical reasoning abilities across a range of problem difficulties, from grade to high school levels.
\item We propose a new fully AI-powered automatic reinforcement learning method,  \REInameF{} (\textbf{\REInameS{}}), alongside \MathEIname{} and Process Supervision, for improving reasoning performance. 
\item \modelname{} surpasses top-tier open-source LLMs by a substantial margin with higher data efficiency and also significantly outperforms various proprietary LLMs on both GSM8k and MATH, demonstrate the effectiveness of our \textbf{\REInameS{}}.
\end{itemize}

\section{Related Work}

\textbf{Large Language Models.} LLMs have significantly advanced Natural Language Processing, with models like OpenAI's GPT Series ~\citep{GPT3,openai2023gpt4}, Anthropic's Claude~\citep{bai2022constitutional-claude}, Google's PaLM~\citep{PaLM,palm2}, Gemini~\citep{geminiteam2023gemini}, and Gemma~\citep{team2024gemma} featuring billions of parameters and trained on massive textual datasets. The AI field has also seen a rise in open-source LLMs such as Mistral~\citep{jiang2023mistral}, Llama Series ~\citep{touvron2023llama,touvron2023llama2, dubey2024-llama3, taylor2022galactica},  DeepSeek~\citep{Bi2024DeepSeekLS, shao2024-deepseekmath}, Qwen~\citep{Bai2023QwenTR, yang2024-qwen2.5-math} etc. Notably, Llama serves as a foundational model for supervised fine-tuning, leading to the development of models like Alpaca, Vicuna~\citep{alpaca,vicuna2023}.

\textbf{Large Language Models For Mathematical reasoning.} NLP models face challenges with complex reasoning, including mathematical~\citep{long2024-llms-survey, zhang2024-geoeval, xia2024-evaluating}, common-sense~\citep{talmor-etal-2019-commonsenseqa}. Significant research focuses on Mathematical Word Problems (MWP), which demand understanding of mathematical concepts and multi-step reasoning~\citep{zheng2023php,zhao2023modelselection, yuan2023RFT}. Models are tested on various MWP benchmarks ~\citep{roy-roth-2015-solving-multiarith, hendrycks2021measuring}. Techniques like Chain-of-Thought Prompting~\citep{wei2022chain}, Least-to-Most prompting~\citep{Zhou2022LeasttoMostPE}, and Complex CoT~\citep{fu2023complexity-cot-based} enhance reasoning by introducing multiple steps and breaking problems into sub-problems. There are some models aimed at improving math CoT reasoning skills such as MetaMath~\citep{yu2023metamath}, 
 MathScale~\citep{tang2024mathscale}, Xwin-Math~\citep{li2024-Xwin-math}, DART-Math~\citep{tong2024-DART} etc. Some models enhance mathematical reasoning by integrating python tools, such as TORA~\citep{gou2023tora}, 
  MAmmoTH~\citep{yue2023mammoth}, 
  Openmathinstruct~\citep{toshniwal2024-openmathinstruct}, NuminaMath~\citep{li2024-numinamath} etc. In our work, we mainly improve the CoT reasoning ability of mathematics without using external Python tools.

\textbf{Reinforcement Learning for Large Language Models.} State-of-the-art models often display logical errors and illusions, particularly in domains requiring complex, multi-step reasoning, leading to significant challenges~\citep{bubeck2023sparks, maynez2020faithfulness}. Strategies such as training reward models help discriminate between desirable and undesirable outputs~\citep{lightman2023openai-verify-step-by-step,Wu2023FineGrainedHF, chen2024-fine-grain-improving}. Historically, outcome-based approaches focused on algorithmic tasks~\citep{ Li2016NeuralPL, cai2017making,Yu2023OutcomesupervisedVF}, while recent research demonstrates the efficacy of reward models or validators in enhancing model performance~\citep{cobbe2021training_gsm8k_2,Wang2023MakingLL,Wang2023LargeLM,Li2022MakingLM}. Reward models have also been incorporated into reinforcement learning pipelines and employed in rejection sampling to align Large Language Models (LLMs) with human preferences~\citep{ shen2021generate,bai2022constitutional-claude,yuan2023rrhf, dong2023raft,song2023preference, touvron2023llama2, rafailov2024DPO, meng2024simpo}. A contrast is drawn between outcome-supervised and process-supervised reward models, with the latter being more effective at addressing discrepancies arising from incorrect reasoning paths leading to correct outcomes~\citep{uesato2022deepmind-orms,zelikman2022star, creswell2022selectioninference}. Recent advances have promoted process-based supervision through manual annotation, significantly benefiting LLMs over outcome-based approaches~\citep{lightman2023openai-verify-step-by-step,wang2024-mathshepherd, sun2024-easy-to-hard, chen2024-autoprm, wang2024-multi-prm, zhang2024-rest-mcts}. In this paper, we leverage AI models like ChatGPT to automatically offer process annotation to improve the efficiency of this research line.

\section{Method}

In this section, we elaborate on the details of our \modelname{}. Following WizardLM and PRMs~\citep{lightman2023openai-verify-step-by-step}, we propose \REInameF{} (\textbf{\REInameS{}}) method, which integrates the math \EIname{} and reinforced instruction and process supervision to evolve GSM8k and MATH, and fine-tune the pre-trained language models with the evolved data and reward models. 

\subsection{Math Evol-Instruct}

Motivated by the Evol-Instruct~\citep{xu2023wizardlm} method proposed by WiazrdLM and its effective application on WizardCoder~\citep{luo2023wizardcoder}, this work attempts to make math instructions with various complexities and diversity to enhance the pre-trained LLMs. Specifically, we adapt Evol-Instruct to a new paradigm including two evolution lines:

1) Downward evolution: It enhances instructions by making the questions easier. For example i): revising high difficulty questions to lower difficulty, or ii) producing a new and easier question with another different topic.

2) Upward evolution:  Derived from original Evol-Instruct method, it deepens and generates new and harder questions by i) adding more constraints, ii) concretizing,  iii) increasing reasoning.

The complete prompts of above evolution are shown in Appendix \ref{appendix:evol_prompts}. For each instruction, we use GPT-4 to evolve 5 rounds (2 downward and 3 upward) of new instructions progressively, each new one is generated by the previous round of evolution.
    

\subsection{Reward Models}\label{section:Reward Models}

Considering the necessity of quality control for evolved instructions  and inspired by PRMs~\citep{lightman2023openai-verify-step-by-step}, we train two reward models to predict the quality of the instructions and  the correctness of each step in the answer respectively:
\paragraph{Instruction Reward Model (IRM)} This model aims to judge the quality of the evolved instructions on two aspects: i) Difficulty, and ii) Definition. To produce the ranking list training data of IRM, we leverage GPT-4 to rank the quality between those evolved instructions and original instruction. The one with high difficulty and clear definition will deserve a higher ranking. The detailed prompt of above ranking process is shown in the Appendix \ref{appendix:IRM_prompts}. 

Specifically, given an math instructions $q$, IRM ($Q \rightarrow \mathbb{R} $) assigns a score to $q$ to indicate its quality. We optimize ORM via the following pairwise ranking loss: 
\begin{align}
\mathcal{L}_{IRM}= - \log \sigma(r^q_j - r^q_k - m)\
\label{eq:pairwise-loss}
\end{align}
where $r^q_j$ is the reward of chosen instruction and  $r^q_k$ is the reward of rejected instruction, $m$ is the margin.

\paragraph{Process-supervised Reward Model (PRM)} As there is no simple way to support highly precise process supervision without professional and expensive human-labelers, we depend on GPT-4 to provide process supervision, and ask it to assess the correctness of each step in the solutions generated by our model to produce PRM training data. The detailed prompt of above step level labeling process is shown in the Appendix \ref{appendix:PRM_prompts}. 

For exactly, given an math instructions $q$ and its answer $a$,  PRM ($Q \times A \rightarrow \mathbb{R}^{+}$) assigns a score to each step of $a$, we train PRM with the following cross-entropy loss:
\begin{align}
\mathcal{L}_{PRM}= \sum_{i=1}^{L} y_i \log r^a_i + (1 - y_i) \log(1 - r^a_i)\
\label{eq:prm-loss}
\end{align}
where $L$ is the reasoning steps of answer $a$. $y_i$ is the ground-truth label of the $i$-th step of answer $a$, $y_i = 1$ if $a_i$ is correct, otherwise $y_i = 0$. $r^a_i$ is the reward score (assigned by PRM) of the $i$-th step of answer $a$.

\subsection{Reinforcement Learning with IRM and PRM}

Immediately, we exploit reinforcement learning to optimize LLMs. Following ~\citep{lightman2023openai-verify-step-by-step}, we employ step by step Proximal Policy Optimization (PPO) to reward both instruction and each reasoning step.

For each math instruction $q$ and generated answer $a$, we use IRM to assign instruction reward $r^q$, and use the minimum score across all reasoning steps to represent the final reward score $r^a$ of the answer $a$ assigned by PRM. Then we apply a product as the final reward of this instruction-answer pair:
\begin{align}
r = r^q \cdot r^a \
\label{eq:final-loss}
\end{align}

\subsection{PRM for Verification}
Following ~\citep{lightman2023openai-verify-step-by-step} and ~\citep{li-etal-2023-making}, we leverage both majority voting and PRM verifier to  aggregate the predictions of different reasoning paths.
\begin{align}
\hat{a} = \mathop{\arg\max}_{a} \sum_{i=1}^{N} \mathbb{I}_{a_i = a} \cdot PRM(q, a_i)\
\label{eq:loss}
\end{align}
where $PRM(q, a_i)$ is the score of the $i$-th reasoning path assigned by PRM for instruction $q$.  $\mathbb{I}_{a_i = a}$ is an indicator function that returns 1(or 0) if $a_i = a$. 

\section{Experiment}

This section provides a comprehensive overview of the advanced models. Subsequently, we mainly elucidate the performance metrics of our models on two prevalent mathematical benchmarks from grade to high school problems: GSM8k~\citep{cobbe2021training_gsm8k_2} and MATH~\citep{hendrycks2021measuring}.

\subsection{Experimental Setup} \label{sections: exper-setup}

\textbf{SFT Training Data.} Firstly, use the GSM8k and MATH training sets as the initial seed collection, then employ both upward and downward math Evol-Instruct approach for five rounds. Each round need to evolve the initial instructions 6 times, and the temperature parameter is set to 0.7. Next, we remove duplicate instructions 17k. Hence, a total of 448k unique instructions were obtained. Subsequently, 30k data were excluded by the data filtering method to avoid contamination, ultimately leaving 418k data. Finally, we use GPT-4-0613 to generate the answer with a step-by-step format, and leverage them for supervised fine-tuning.

\textbf{Reward Models Training Data.} 
To train the reward models, We conducted additional 5 rounds of evolution on the initial instruction set and obtain 90k instructions. we use GPT-4-0613 to rank each instruction list with the quality  from 1 to 6 as the training data of IRM. To obtain the training data of PRM, We use our Llama-2 70B SFT model to generate 5 answers for each instruction, and GPT-4-0613 is employed to assign correctness judgement for each reasoning step.

\textbf{Implementation Details.}
We employ our method on two open-source foundational models Llama 2~\citep{touvron2023llama2} and Mistral-7B~\citep{jiang2023mistral}. Llama 2 encompasses three distinct parameter sizes: 7B, 13B, and 70B. We utilize GPT-4-0613 for instruction evolution and  the training data construction of reward models. For SFT, we train 3 epochs, and the learning rate is 2e-5, 1e-5 and 5e-6 for Llama 2 7B/13B, 70B and Mistral-7B. The batch size is 512, and the sequence length is 2048.
 For the reward model, we train Llama 2 and Mistral-7B with learning rate 4e-6 and 1e-6 for one epoch. For RL, the lr is 4e-7 and 1e-7 for Llama 2 and Mistral-7B and train one epoch.

    

\clearpage
\begin{wraptable}{r}{0.53\textwidth}
\vspace{-1.3cm}
    \centering
    \caption{The models' CoT pass@1 results  on GSM8k and MATH without using any external python tool.}
    \scalebox{0.55}{
    \begin{tabular}{lccll}
    \toprule
    \textbf{Model} & \textbf{Base} & \textbf{Params} & \textbf{GSM8k} 
   &\textbf{MATH}  \\ 
    \midrule
    \multicolumn{5}{c}{Proprietary models}\\
    \midrule
    GPT-o1~\citep{openai2023gpt4} & - & - & - & 94.8 \\  
    GPT-o1-mini & - & - & - & 90.0 \\  
    Gemini-1.5 002 & - & - & - & 86.5 \\  
    Claude 3.5 Sonnet~\citep{bai2022constitutional-claude} & - & - & 96.4 & 71.1 \\  
    GPT-4o-2024-0513 & - & - & 96.1 & 76.6 \\  
    GPT-4-turbo-0125~\citep{openai2023gpt4} & - & - & 94.2 & 64.5 \\  
    GPT-4-0314 & - & - & 94.7 & 52.6 \\  
    GPT-4 (original version) & - & - & 92.0 & 42.5 \\  
    Baichuan-3~\citep{yang2023baichuan-2} & - & - & 88.2 & 49.2 \\  
    GLM-4~\citep{glm2024chatglm-4} & - & - & 87.6 & 47.9 \\  
    Gemini Pro~\citep{geminiteam2023gemini} & - & - & 86.5 & 32.6 \\  
    Claude2 & - & - & 85.2 & 32.5 \\  
    GPT-3.5-Turbo & - & - & 81.6 & 43.1 \\  
    PaLM2~\citep{palm2} & - & - & 80.7 & 34.3 \\  
    Minerva~\citep{lewkowycz2022solving} & - & 540B & 58.8 & 33.6 \\
    GPT3.5~\citep{GPT3} & - & - & 57.1 & - \\
    \midrule
    \multicolumn{5}{c}{Open-Source Models (0.1B-3B)}\\
    \midrule  
    GPT-2-Small~\citep{brown2020language_gpt2} & - & 0.1B & 6.9 & 5.4 \\   
    GPT-2-Medium~\citep{brown2020language_gpt2} & - & 0.3B & 11.2 & 6.2 \\  
    GPT-2-Large~\citep{brown2020language_gpt2} & - & 0.7B & 13.6 & 6.4 \\
    GPT-2-XL~\citep{brown2020language_gpt2} & - & 1.5B & 15.4 & 6.9 \\
    \rowcolor{gray!30}
    WizardMath-GPT & GPT-2-Small & 0.1B & 26.4 & 12.3 \\ 
    \rowcolor{gray!30}
    WizardMath-GPT & GPT-2-Medium & 0.3B & 38.7 & 15.6 \\  
    \rowcolor{gray!30}
    WizardMath-GPT & GPT-2-Large & 0.7B & 50.1 & 21.2 \\  
    \rowcolor{gray!30}
    WizardMath-GPT & GPT-2-XL & 1.5B & 58.9 & 25.4 \\  \\[-0.7em]  \hdashline \\[-0.7em]
    \rowcolor{gray!30}
    WizardMath-Qwen & Qwen-Math-2.5 & 1.5B & 86.7 & 68.6 \\  

    \\[-0.7em]  \hdashline \\[-0.7em]
    
    Llama-3.2-Instruct~\citep{dubey2024-llama3} & Llama 3.2 & 1B & 44.4 & 30.6 \\  
    \rowcolor{gray!30}
    WizardMath-Llama & Llama 3.2 & 1B & 63.3 & 33.5 \\  
    Llama-3.2-Instruct & Llama 3.2 & 3B & 77.7 & 48.0 \\ 
    \rowcolor{gray!30}
    WizardMath-Llama & Llama 3.2 & 3B & 85.5 & 49.9 \\  
    \midrule
    \multicolumn{5}{c}{Open-Source Models (7B-8B)}\\
    \midrule   
    Llama-2~\citep{touvron2023llama2} & - & 7B & 14.6 & 2.5 \\  
    MAmmoTH-CoT~\citep{yue2023mammoth} & Llama-2 & 7B & 50.5 & 10.4 \\  
    MathScale~\citep{tang2024mathscale} & Llama-2 & 7B & 66.3 & 31.1 \\  
    MetaMath~\citep{yu2023metamath} & Llama-2 & 7B & 66.5 & 19.8 \\  
    MuggleMath~\citep{Li2023mugglemath} & Llama-2 & 7B & 68.4 & - \\  
    Skywork-Math~\citep{zeng2024-skywork-math} & Llama-2 & 7B & 72.9 & 47.7 \\  
    Math-Shepherd~\citep{wang2024-mathshepherd} & Llama-2 & 7B & 73.2 & 21.6 \\  
    Xwin-Math~\citep{li2024-Xwin-math} & Llama-2 & 7B & 82.6 & 40.6 \\ 
    \rowcolor{gray!30}
    WizardMath-Llama & Llama-2 & 7B & 84.1 & 43.5 \\ \\[-0.7em]  \hdashline \\[-0.7em]
    Mistral-v0.1~\citep{jiang2023mistral} & - & 7B & 42.9 & 12.9 \\  
    MathScale~\citep{tang2024mathscale} & Mistral-v0.1 & 7B & 74.8 & 35.2 \\  
    MMIQC~\citep{liu2024-MMIQC} & Mistral-v0.1 & 7B & 74.8 & 36.0 \\  
    MetaMath~\citep{yu2023metamath} & Mistral-v0.1 & 7B & 77.9 & 28.6 \\  
    KPMath-Plus~\citep{huang2024-KPMath} & Mistral-v0.1 & 7B & 82.1 & 46.8 \\  
    DART-Math~\citep{tong2024-DART} & Mistral-v0.1 & 7B & 82.6 & 43.5 \\  
    Skywork-Math~\citep{zeng2024-skywork-math} & Mistral-v0.1 & 7B & 83.9 & 51.2 \\  
    Math-Shepherd~\citep{wang2024-mathshepherd} & Mistral-v0.1 & 7B & 84.1 & 33.0 \\  
    MAmmoTH2-Plus~\citep{yue2024mammoth2} & Mistral-v0.1 & 7B & 84.7 & 45.0 \\  
    JiuZhang3.0~\citep{zhou2024jiuzhang3} & Mistral-v0.1 & 7B & 88.6 & 52.8 \\  
    Xwin-Math~\citep{li2024-Xwin-math} & Mistral-v0.1 & 7B & 89.2 & 43.7 \\  
    \rowcolor{gray!30}
    WizardMath-Mistral & Mistral-v0.1 & 7B & 90.7 & 55.4 \\  
    \rowcolor{gray!30}
    WizardMath-Mistral & Mistral-v0.3 & 7B & 90.4 & 55.6 \\  
    \rowcolor{gray!30}
    WizardMath-Mathstral & Mathstral-v0.1 & 7B & 93.8 & 70.9 \\  \\[-0.7em]  \hdashline \\[-0.7em]
    \rowcolor{gray!30}
    WizardMath-Qwen & Qwen2.5-Math & 7B & 93.9 & 77.8 \\  
    \rowcolor{gray!30}
    WizardMath-Qwen & Qwen2.5 & 7B & 94.0 & 74.5 \\  \\[-0.7em]  \hdashline \\[-0.7em]
    DeepSeekMath-Base~\citep{shao2024-deepseekmath} & - & 7B & 64.2 & 36.2 \\  
    NuminaMath-CoT~\citep{li2024-numinamath} & DeepseekMath & 7B & 75.4 & 55.2 \\  
    MMIQC~\citep{liu2024-MMIQC} & DeepSeekMath & 7B & 79.0 & 45.3 \\  
    KPMath-Plus~\citep{huang2024-KPMath} & DeepSeekMath & 7B & 83.9 & 48.8 \\  
    DeepSeekMath-RL~\citep{shao2024-deepseekmath} & DeepSeekMath & 7B & 88.2 & 51.7 \\  
    DART-Math~\citep{tong2024-DART} & DeepSeekMath & 7B & 88.2 & 52.9 \\ 
    \rowcolor{gray!30}
    WizardMath-DeepSeek & DeepSeekMath & 7B & 91.0 & 64.6 \\  \\[-0.7em]  \hdashline \\[-0.7em]
    MetaMath~\citep{yu2023metamath} & Llama 3 & 8B & 77.3 & 20.6 \\  
    MMIQC~\citep{liu2024-MMIQC} & Llama 3 & 8B & 77.6 & 29.5 \\  
    DART-Math~\citep{tong2024-DART} & Llama 3 & 8B & 82.5 & 45.3 \\   
    MAmmoTH2-Plus~\citep{yue2024mammoth2} & Llama 3 & 8B & 84.1 & 42.8 \\  
    Llama 3.1-Instruct~\citep{dubey2024-llama3} & Llama 3 & 8B & 84.5 & 51.9 \\ 
    JiuZhang3.0~\citep{zhou2024jiuzhang3} & Llama 3 & 8B & 88.6 & 51.0 \\  
    \rowcolor{gray!30}
    WizardMath-Llama & Llama 3 & 8B & 90.3 & 58.8 \\  
    \midrule
    \multicolumn{5}{c}{Open-Source Models (13B)}\\
    \midrule  
    Llama-2~\citep{touvron2023llama2} & - & 13B & 28.7 & 3.9 \\  
    MAmmoTH-CoT~\citep{yue2023mammoth} & Llama 2 & 13B & 56.3 & 12.9 \\  
    MathScale~\citep{tang2024mathscale} & Llama 2 & 13B & 71.3 & 33.8 \\  
    MetaMath~\citep{yu2023metamath} & Llama 2 & 13B & 72.3 & 22.4 \\  
    MuggleMath~\citep{Li2023mugglemath} & Llama 2 & 13B & 74.0 & - \\  
    KPMath-Plus~\citep{huang2024-KPMath} & Llama 2 & 13B & 81.6 & 41.0 \\  
    Xwin-Math~\citep{li2024-Xwin-math} & Llama 2 & 13B & 88.1 & 44.9 \\  
    \rowcolor{gray!30}
    WizardMath-Llama & Llama 2 & 13B & 89.7 & 50.6 \\  
    \midrule
    \multicolumn{5}{c}{Open-Source Models (70B)}\\
    \midrule 
    Llama-2~\citep{touvron2023llama2} & - & 70B & 56.8 & 13.5 \\  
    MAmmoTH-CoT~\citep{yue2023mammoth} & Llama-2 & 70B & 72.4 & 21.1 \\  
    MetaMath~\citep{yu2023metamath} & Llama-2 & 70B & 82.3 & 26.6 \\  
    KPMath-Plus~\citep{huang2024-KPMath} & Llama-2 & 70B & 87.4 & 48.6 \\  
    Xwin-Math~\citep{li2024-Xwin-math} & Llama-2 & 70B & 90.6 & 52.8 \\  
    \rowcolor{gray!30}
    WizardMath-Llama & Llama-2 & 70B & 92.8 & 58.6 \\ 
    \bottomrule
    \end{tabular}
    }

    \label{tab:gsm8k_math_merge}
    \vspace{-2cm}
\end{wraptable}

\subsection{Main Results}

Table ~\ref{tab:gsm8k_math_merge} shows the CoT~\citep{wei2022chain} pass@1 results of the current state-of-the-art models on GSM8k and MATH. In this study, to ensure equitable and cohesive evaluations, we report the socres of all models within the settings of \textbf{greedy decoding and CoT without using any external python tool}. 

\paragraph{Comparing with the proprietary Models.} As shown in the Table~\ref{tab:gsm8k_math_merge}, our \emph{\textbf{WizardMath}} demonstrates notable superiority over various proprietary LLMs on the  GSM8k and MATH benchmarks in terms of pass@1:

1) \emph{\textbf{WizardMath-Llama 70B}}, the largest model, demonstrated exceptional performance on the GSM8k and MATH , surpassing earlier versions of GPT-4, Claude-2, and Gemini Pro, and performing on par with GPT-4-0314. It significantly outperformed GPT-3.5-Turbo by 11.2\% on GSM8k and by 15.5\% on MATH.

2) \emph{\textbf{WizardMath-Mistral 7B}}, the smaller-sized model, outperformed Baichuan 3 on GSM8k (90.7 vs. 87.6) and surpassed GPT-4-0314 on MATH (55.4 vs. 52.6), significantly exceeding the performance of GPT-3.5-Turbo and Gemini Pro. Meanwhile, WizardMath-Mathstral, trained on Mathstral-7B-v0.1, demonstrated performance comparable to GPT-4-turbo-0125. Additionally, WizardMath-Qwen, trained on Qwen2.5-Math, surpassed GPT-4-2024-0513 on MATH (77.8 vs. 76.6).

\paragraph{Comparing with the Open-Source Models.} The results presented in Table~\ref{tab:gsm8k_math_merge} unequivocally indicate that our \emph{\textbf{WizardMath-Llama 70B}} exhibits a significant performance superiority over strong models in both the GSM8k and MATH benchmarks with higher data efficiency across the range from 0.1B to 70B parameters. The detailed results are as follows:

1) With the same model parameter size, our model surpasses the previous best model such as MetaMath, MAmmoTH2-Plus, Xwin-Math. Particularly, \emph{\textbf{WizardMath-Llama 70B}} achieves a substantial improvement of 10.5\% on GSM8K and 32.0\% on MATH compared to MetaMath-Llama 70B in testing accuracy.  In the Table ~\ref{tab:math_topics}, we show the detailed results of MATH subtopics with our WizardMath 70B model.  Specifically, \emph{\textbf{WizardMath-Mistral 7B}} also surpasses top-tier open source models, outperforming MetaMath-Mistral 7B with a notable margin (90.7 vs 77.9 on GSM8k) and (55.4 vs 28.6 on MATH). It demonstrats the effectiveness of our RLEIF method in enhancing mathematical reasoning capabilities across a range of problem difficulties, from grade to high school levels.

2) By employing diverse pre-trained models (i.e., GPT-2, Llama 2, Mistral, Qwen, DeepSeek) as base models, WizardMath demonstrated notable advancements on the GSM8k and MATH benchmarks. Specifically, WizardMath-Llama2-7B, based on Llama2-7B, improved performance by 69.5\% on GSM8k and 41.0\% on MATH. Similarly, WizardMath-GPT2-XL, built on GPT2-XL, achieved a 43.5\% improvement on GSM8k and 18.5\% on MATH, performing on par with Llama2-70B and outperforming GPT-3.5 on GSM8k. This demonstrates that our RLEIF method is equally effective for smaller models in enhancing mathematical reasoning capabilities, proving its scalability and robustness across various model backbones.

  

   

\begin{table}[t]
    \centering
    \begin{minipage}{.45\textwidth}
        \centering
        \caption{Results of pass@1 (\%) on MATH subtopics (i.e., Intermediate Algebra, Geometry) with WizardMath 70B model. }
        \scalebox{0.9}{
        \begin{tabular}{lcc}
            \toprule
            \textbf{MATH subtopics} & \textbf{WizardMath 70B}\\
            \midrule
            Intermediate Algebra & 36.3\\
            Precalculus & 38.9\\
            Geometry & 48.3\\
            Number Theory & 58.5\\
            Counting \& Probability & 54.8\\
            Prealgebra & 74.6\\
            Algebra & 78.5\\
            \midrule
            Overall & \textbf{58.6}\\
            \bottomrule
        \end{tabular}}
        \label{tab:math_topics}
    \end{minipage}%
    \hfill
    \begin{minipage}{.52\textwidth}
        \centering
        \caption{Explore the effects of PRM and IRM during PPO training. }
        \small
        \renewcommand{\arraystretch}{1.06}
        \scalebox{0.8}{
        \begin{tabular}{lcc}
            \toprule
            \textbf{Models}  &\textbf{GSM8K}  & \textbf{MATH}  \\
            \midrule
            GPT-2-XL-1.5B: WizardMath-SFT & 51.9 & 18.3 \\
            \midrule
            \quad + PRM  & 55.8 & 22.1 \\
            \quad + PRM + IRM   & \textbf{58.9} & \textbf{25.4} \\
            \midrule
            Llama2-7B: WizardMath-SFT & 77.4 & 35.6 \\
            \midrule
            \quad + PRM  & 81.7 & 39.9 \\
            \quad + PRM + IRM   & \textbf{84.1} & \textbf{43.5} \\
            \midrule
            \midrule
            Mistral-7B: WizardMath-SFT & 82.8 & 48.1 \\
            \midrule
            \quad + PRM  & 87.2 & 52.7 \\
            \quad + PRM + IRM   & \textbf{90.7} & \textbf{55.4} \\
            \bottomrule
        \end{tabular}
        }
        \label{tab:math_rl_instruction_rm}
    \end{minipage}
\end{table}

\subsection{ANALYSIS}

\begin{wrapfigure}{r}{0.55\textwidth} % {r}{0.5\textwidth} 调整 r 为 l, r, 或 c 以改变位置
\vspace{-16pt}

\centering
     \includegraphics[width=1\linewidth]{Figures/data_scale_figure.pdf}
     \vspace{-0.6cm}
     \caption{Accuracy of Mistral-7B fine-tuned
in different sizes of augmentation data on GSM8K and MATH }
    \vspace{-0.5cm}
     \label{fig:data_scale}
\end{wrapfigure}

\textbf{The impact of training data size}

We are curious about to how the training data size of different dataset construction methods impact the reasoning capacity of LLMs. Thus we conduct different number of training instances from ours evolved data and MetaMathQA to fine tune Mistral 7B. As shown in the Figure \ref{fig:data_scale}, Math Evol-Instruct achieves superior data efficiency. Specifically, our model  constantly  outperforms MataMath by more than 3\% $\sim$ 6\% on GSM8k and 15\% $\sim$ 20\% on MATH under the same number of conditions. Our findings indicate that Math Evol-Instruct exhibits a higher potential upper bound compared to MetaMath, thus demonstrating the effectiveness of Evol-Instruct for math reasoning senario.

\textbf{The impact of PRM and IRM during PPO training}

To verify the contributions of the instruction reward model and process-supervised reward model, we consider the following variants: (1) SFT + PRM: only use PRM in the PPO training. (2) SFT + PRM + IRM: use both IRM and PRM in the PPO training.  As shown in Table \ref{tab:math_rl_instruction_rm}, applying PRM alone for PPO training on GSM8k and MATH yields a 3\%-4\% improvement. When combined with IRM, an additional 2.5\%-4\% gain is observed. Thus, the integration of PRM and IRM results in a substantial overall improvement of 6\%-8\%. So, we can conclude that (1) PRM is crucial to WizardMath, since the variant with PRM significantly outperforms the SFT one without any PPO training (2) IRM also plays a key role in the success of reinforcement learning, as there is a remarkable improvement when we combine PRM with IRM, further demonstrating the necessity of taking instruction's quality into account and correcting false positives in the problem-solving process when we optimize the LLMs.

\begin{table}[h]
    \centering
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \caption{The effect of different reward models during PPO training}
        \small
        \scalebox{0.95}{
        \begin{tabular}{lcc}
        \toprule
         \textbf{Models}  &\textbf{GSM8K}  & \textbf{MATH}  \\
         \midrule
        Llama2-7B: WizardMath-SFT  & 77.4 & 35.6   \\
        \midrule
        \quad + ORM (ours) & 79.1 & 36.8  \\
        \quad + PRM800k   & 79.7 & 38.7 \\
        \quad + Math-Shepherd   & 80.3 & 38.2 \\
        \quad + PRM (ours)  & \textbf{81.7} & \textbf{39.9} \\
        \midrule
         \midrule
        Mistral-7B: WizardMath-SFT  & 82.8 & 48.1   \\
        \midrule
        \quad + ORM (ours) & 84.6 & 49.6  \\
        \quad + PRM800k   & 85.4 & 50.8 \\
        \quad + Math-Shepherd   & 86.1 & 50.3 \\
        \quad + PRM (ours)  & \textbf{87.2} & \textbf{52.7} \\
        \bottomrule
        \end{tabular}
        }
        \label{tab:math_rl_greedy_decoding}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \caption{Results of reinforcement learning combined with validation. The SFT and Reward models are trained based on Mistral-7B. The verifier is based on 256 sample outputs.}
        \small
        \scalebox{0.95}{
        \begin{adjustbox}{max width=\textwidth}
        \begin{tabular}{clcc}
        \toprule
         \textbf{Generators}  & \textbf{Verifiers} & \textbf{GSM8K}  & \textbf{MATH}  \\
         \midrule
        \multirow{3}{*}{\rotatebox{0}{ SFT}} &  Self-Consistency   & 90.7 & 57.5\\
         & ORM  & 93.0 & 58.3  \\
        & PRM & 93.9  &  61.7  \\
        \midrule
        \multirow{3}{*}{\rotatebox{0}{ SFT + ORM}} &  Self-Consistency & 91.2 & 57.7  \\
        & ORM  & 93.4 &  59.4 \\
        & PRM & 94.1 & 63.3 \\

        \midrule
        \multirow{3}{*}{\rotatebox{0}{SFT + PRM}} &  Self-Consistency & 92.3 & 59.3  \\
        & ORM  & 94.1 &  60.8 \\
        & PRM & \textbf{95.2} & \textbf{64.7}  \\
        \bottomrule
        \end{tabular}
        \end{adjustbox}
        }
        \label{tab:math_rl_verifier}
    \end{minipage}
\end{table}

\begin{wraptable}{r}{0.57\textwidth}
\centering
\footnotesize % Sets the font size to footnotesize
\vspace{-0.6cm}
\caption{\footnotesize Impact of different Downward and Upward Evol-Instruct turns on {Mistral-7B} SFT. \textit{D-i} refers to the \textit{i} round of downward evolution, whereas \textit{U-i} denotes the \textit{i} round of upward evolution. \textit{Ori} is the original manually annotated 7.5k data of GSM8k and MATH.}
\vspace{0.4cm}
\scalebox{0.68}{ % Scales the table; adjust scale as needed
    \setlength{\tabcolsep}{2.5pt} % Adjusts the space between columns
    \renewcommand{\arraystretch}{1.2} % Adjusts the space between rows
    
    \begin{tabular}{c|ccccccc|ccccccc}
    \hline
        \multirow{2}{*}{Data} & \multicolumn{7}{c|}{GSM8K} & \multicolumn{7}{c}{MATH} \\
& \textbf{Ori} & \textbf{D-1} & \textbf{D-2} & \textbf{U-1} & \textbf{U-2} & \textbf{U-3} & \textbf{pass@1} & \textbf{Ori} & \textbf{D-1} & \textbf{D-2} & \textbf{U-1} & \textbf{U-2} & \textbf{U-3} & \textbf{pass@1} \\
    \hline
Ori & \checkmark & \xmark & \xmark & \xmark & \xmark & \xmark & 59.7 & \checkmark & \xmark & \xmark & \xmark & \xmark & \xmark & 15.1 \\
\midrule
\multirow{9}{*}{ \makecell{Math \\Evol}} & \checkmark & \checkmark & \xmark & \xmark & \xmark & \xmark & 71.9 & \checkmark & \checkmark & \xmark & \xmark & \xmark & \xmark & 30.3 \\
 & \checkmark & \xmark & \checkmark & \xmark & \xmark & \xmark & 70.5 & \checkmark & \xmark & \checkmark & \xmark & \xmark & \xmark & 28.7 \\
 & \checkmark & \xmark & \xmark & \checkmark & \xmark & \xmark & 73.7 & \checkmark & \xmark & \xmark & \checkmark & \xmark & \xmark & 33.4 \\
 & \checkmark & \xmark & \xmark & \xmark & \checkmark & \xmark & 71.6 & \checkmark & \xmark & \xmark & \xmark & \checkmark & \xmark &  32.6\\
 & \checkmark & \xmark & \xmark & \xmark & \xmark & \checkmark & 70.2 & \checkmark & \xmark & \xmark & \xmark & \xmark & \checkmark & 30.9 \\
 & \checkmark & \checkmark & \checkmark & \xmark & \xmark & \xmark & \textbf{74.5} & \checkmark & \checkmark & \checkmark & \xmark & \xmark & \xmark & \textbf{34.7} \\
 & \checkmark & x & x & \checkmark & \checkmark & x & 77.1 & \checkmark & x & x & \checkmark & \checkmark & x & 38.6 \\
 & \checkmark & x & x & \checkmark & \checkmark & \checkmark & \textbf{78.6} & \checkmark & x & x & \checkmark & \checkmark & \checkmark & \textbf{42.5} \\
 & \checkmark & \checkmark & \checkmark & \checkmark & \xmark & \xmark & 76.6 & \checkmark & \checkmark & \checkmark & \checkmark & \xmark & \xmark & 40.3 \\
 & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \xmark & 79.8 & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \xmark & 44.6 \\
 & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \textbf{81.2} & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \textbf{46.2} \\
    \hline
    \end{tabular}
}
\label{exp:abl-effect-evol-gsm8k}
\end{wraptable}

\textbf{The impact of Evol-Instruct turns}. Table~\ref{exp:abl-effect-evol-gsm8k} illustrates the impact of combining downward and upward evolution in SFT training. Two rounds of downward evolution improved GSM8k by 14.8\% (74.5 vs. 59.7) and MATH by 19.6\% (34.7 vs. 15.1) over the original. Three rounds of upward evolution yielded a 18.9\% improvement on GSM8k (78.6 vs. 59.7) and a 27.4\% improvement on MATH (42.5 vs. 15.1). Furthermore,  combining downward evolution based on upward evolution resulted in an additional 2.6\% improvement on GSM8k (81.2 vs. 78.6), a total improvement of 21.5\% over the original. Similarly, a 1.9\% improvement on MATH (46.5 vs. 42.5), a 31.4\% total improvement. These results underscore the complementary and significant effectiveness of upward and downward evolution.

\textbf{ORM v.s. PRM; Human v.s. AI.} The Table \ref{tab:math_rl_greedy_decoding} presents the performance of different answer reward methods for LLMs in terms of pass@1. As is shown: 1) Our step-by-step PRM significantly enhances the performance of both Llama and Mistral based SFT models.  Specifically, the Mistral-7B powered by our PRM achieves 87.2\% and 52.7\% on GSM8k and MATH respectively. 2)  PRM models consistently outperforms ORM on both GSM8k and MATH, indicating the effectiveness of step-by-step supervision. 3) The PRM trained on our fully AI-labeled data  outperforms both the manually annotated PRM800k and Math-Shepherd, which utilizes MCTS tree search for annotation. When training WizardMath-Mistral-SFT with PPO, our PRM  improves upon PRM800k by 1.8\% and Math-Shepherd by 1.1\% on GSM8k, while surpassing PRM800k by 1.9\% and Math-Shepherd by 2.4\% on MATH. This demonstrates powerful AI can also provide good  process supervision quality, highlighting the effectiveness of utilizing AI to construct PRM training data. 

\textbf{PRM as Verifier.} Table \ref{tab:math_rl_verifier} presents the performance comparison of various generators with different verifiers on GSM8K and MATH in terms of pass@256. We find that: 1) PRM verifier consistently demonstrates superior performance compared to Self-Consistency and ORM. Specifically, our SFT + PRM generator, enhanced by the PRM verifier, achieves 95.2\% and 64.7\% accuracy on GSM8K and MATH respectively. 2) When compared to ORM, PRM exhibits a more significant advantage on the more challenging MATH dataset which aligns with the findings in ~\citep{uesato2022deepmind-orms} and ~\citep{lightman2023openai-verify-step-by-step}. This can be attributed to the fact that GSM8K involves fewer and less complex steps in problem-solving than MATH. 3) Particularly, the generator with PRM PPO training  surpasses those SFT and ORM PPO trained generators regardless of employing Self-Consistency, ORM, and the PRM verifiers. This further demonstrates the effectiveness of our PRM.

       

\begin{figure}[t]
\centering
\subfigure{
     \includegraphics[width=0.48\linewidth,trim=14 0 33 10,clip]{Figures/GSM8k_BON_figure.pdf}
}
\subfigure{
     \includegraphics[width=0.48\linewidth,trim=14 0 33 10,clip]{Figures/MATH_BON_figure.pdf}
}

\caption{Performance of Mistral-7B SFT with different verification strategies.}
\label{fig:math_gsm8k_sample_n}
\end{figure}

Figure \ref{fig:math_gsm8k_sample_n} also shows the performance of different Verification strategies across a range of candidate numbers from 1 to 256 on two benchmarks. The main observations are as follows: 1) PRM verifiers consistently achieves superior performance compared to both ORM and majority voting, and this superiority becomes more evident as N increases. 2) For MATH benchmark, our PRM trained on the AI-annotated datasets slightly surpassed the human-annotated PRM800K.

\begin{wraptable}{r}{0.57\textwidth}
\vspace{-0.7cm}
\caption{Performance of WizardMath  on the 7 out-of-domain evaluation results covering K-12, college, and competition level math problems.  The results of  models  in the table refer to {\sc MwpBench} ~\citep{tang2024mathscale}. ``AGIE'' stands for AGIEval. We report the models’ CoT pass@1 results on MwpBench without using any external python tool}
\centering

\scalebox{0.66}{
\footnotesize
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.15}
\begin{tabular}{lcccccccccccc}
\toprule
\textbf{Models}  & \makecell[l]{\textbf{College}\\\textbf{Math}} & \textbf{TAL} & \makecell[l]{\textbf{Math23k}} & \makecell[l]{\textbf{Ape210k}} & \makecell[l]{\textbf{Gaokao}\\\textbf{Bench}\\\textbf{Math}} & \makecell[l]{\textbf{AGIE}\\\textbf{Gaokao}\\\textbf{Math}} & \makecell[l]{\textbf{AGIE}\\\textbf{SAT}\\\textbf{Math}} & \makecell[l]{\textbf{AVG}}\\
\midrule
\multicolumn{13}{c}{\textit{Proprietary models}} \\
\makecell[l]{GPT-4} &  \textbf{24.4} & \textbf{51.8} & \textbf{76.5} & \textbf{61.5} & \textbf{35.4} & \textbf{28.2} & \textbf{68.6} & \textbf{49.5}\\
\makecell[l]{GPT-3.5-Turbo}  & 21.6 & 42.9 & 62.5 & 44.0 & 23.2 & 15.3 & 55.8 & 37.9\\
\midrule
\multicolumn{13}{c}{\textit{Models based on LLaMA-2 13B}} \\
\makecell[l]{LLaMA-2 13B}  & 1.2 & 6.3 & 9.5 & 7.9 & 0.7 & 0.4 & 6.8 & 4.7\\
\makecell[l]{MAmmoTH-CoT}  & 6.5 & 17.3 & 39.5 & 28.1 & 5.9 & 4.9 & 20.5  & 17.5\\
\makecell[l]{GAIR-Abel}  & 7.9 & 21.1 & 42.2 & 27.8 & 7.0 & 4.9 & 30.3  & 20.2\\
\makecell[l]{MetaMath}  & 10.1 & 25.4 & 48.6 & 31.6 & 9.6 & 5.6 & 38.2 & 24.2\\
\makecell[l]{MathScale 13B}  & 20.4 & 38.1 & 61.1 & 43.7 & 20.0 & 12.3 & 55.8 & 35.9\\
\rowcolor{gray!30}
WizardMath  & \textbf{22.9} & \textbf{43.3} & \textbf{70.3} & \textbf{50.8} & \textbf{33.1} & \textbf{25.7} & \textbf{64.7}  & \textbf{44.4}\\
\midrule
\multicolumn{13}{c}{\textit{Models based on LLaMA-2 7B}} \\
\makecell[l]{LLaMA-2 7B}  & 2.3 & 7.6 & 6.8 & 7.3 & 2.1 & 2.9 & 2.9  & 4.6\\
\makecell[l]{MAmmoTH-CoT}  & 6.2 & 13.3 & 34.6 & 21.4 & 3.9 & 2.7 & 19.6  & 14.5\\
\makecell[l]{GAIR-Abel}  & 6.6 & 18.3 & 35.4 & 24.5 & 4.3 & 4.4 & 23.5  & 16.7\\
\makecell[l]{MetaMath}  & 9.4 & 22.5 & 44.0 & 29.9 & 5.9 & 5.1 & 36.2 & 21.9\\
\makecell[l]{MathScale 7B}  & 20.9 & 35.2 & 59.0 & 41.8 & 19.6 & 12.6 & 57.8 & 35.3\\
\rowcolor{gray!30}
WizardMath  & \textbf{21.2} & \textbf{40.2} & \textbf{67.3} & \textbf{46.1} & \textbf{28.9} & \textbf{18.7} & \textbf{62.7}  & \textbf{40.7}\\
\midrule
\multicolumn{13}{c}{\textit{Models based on Mistral 7B}} \\
\makecell[l]{Mistral 7B}  & 7.5 & 17.9 & 18.5 & 15.5 & 6.2 & 5.9 & 22.5  & 13.4\\
\makecell[l]{MetaMath Mistral}  & 15.7 & 31.4 & 55.1 & 38.1 & 15.3 & 10.1 & 50.9  & 30.9\\
\makecell[l]{MathScale Mistral}  & 21.8 & 39.9 & 64.4 & 46.0 & 21.4 & 14.3 & 57.8  & 37.9\\
\rowcolor{gray!30}
WizardMath Mistral  & \textbf{24.8} & \textbf{44.8} & \textbf{71.2} & \textbf{52.6} & \textbf{37.2} & \textbf{24.5} & \textbf{64.7} & \textbf{45.7} \\
\bottomrule
\end{tabular}
}
\label{tab:mwpbench}
\end{wraptable}

\textbf{Performance of Out-of-Domain.} Table \ref{tab:mwpbench}  presents the results of WizardMath on the 7 out-of-domain evaluation results covering K-12, college, and competition level math problems, highlighting the following salient observations:  (1) With math Evol-Instruct and reinforcement learning, WizardMath consistently
surpasses prior state-of-the-art open-source models (e.g. MetaMath, MathScale) across all scales, and achieves improvement of 5\%-10\% across 7 tasks on average. (2) The accuracy of WizardMath-Mistral is about 5.0\% higher than WizardMath-Llama on the same size. Especially it exceeds GPT-3.5-Turbo (45.7 vs. 37.9) while being comparable to GPT-4. This also indicates that Mistral-7B has more potential in mathematical reasoning. (3) Especially on difficult benchmarks (i.e., College Math, AGIE Gaokao Math), WizardMath outperforms MetaMath by a significant margin . This demonstrates our model and RLEIF method has stronger robustness and better significant generalization ability for invisible mathematical problems. 

\paragraph{Employ Open-source Model to Math Evol-Instruct.} 
In Table ~\ref{tab: open_source_evol}, we investigate the use of open-source models (i.e., Llama-3-70B-Instruct) as a substitute for GPT-4 during the SFT stage for Evol Instruct, employing the same evolution strategy. The results demonstrate that WizardMath-
\begin{wraptable}{r}{0.5\textwidth}
\centering
\vspace{-0.4cm}
    \caption{The impact of using open source models for Math-Evol and use Mistral-7B-v0.1 for SFT . }
    \scalebox{0.9}{\begin{tabular}{lcc}
    \toprule
     Models    & GSM8k & MATH   \\
    \midrule 
    Mistral-7B-v0.1 & 42.9 & 12.9   \\
    \midrule
    WizardMath-SFT-GPT-4-Evol & 82.8 & 48.1 \\
    WizardMath-SFT-Llama3-Evol & 76.7 & 43.5  \\ 
    \bottomrule

\label{tab: open_source_evol}
    \end{tabular}
}
\end{wraptable}
Llama3-Evol achieved a 33.8\% improvement on GSM8k and a 30.6\% improvement on MATH, indicating that the math evol instruct strategy remains effective on open-source models. However, compared to GPT-4 evolution, there is still a 5\%-6\% performance gap. Despite this, the strategy shows significant potential in balancing computational cost and accuracy.

\begin{table*}[!tp]
\setlength\tabcolsep{4pt}
\vspace{-0.5cm}
\small
\centering
    \caption{A case study from GSM8k test set. We rate the response using PRM and ORM. Red text denotes the wrong reasoning steps which PRM successfully detected, but ORM failed.
    }
    \begin{tabular} {p{1.2cm}|p{12cm}}
        \toprule
         \textbf{Question} & \textit{The farm has 30 cows and the zoo has 20 sheep.  The zoo has twice as many cows as the farm does. The farm has half as many sheep as the zoo does. How many animals do the farm and zoo have combined? \quad\quad \textbf{Ground truth answer: 120}}
         \\
         \midrule
         \multirow{10}{*}{\rotatebox{0}{\textbf{Steps}}} 
& Step 1: we know that the farm has 30 cows.  \textcolor{blue}{PRM Score: 0.95.}\\
& Step 2: we are told that the zoo has twice as many cows as the farm does. \textcolor{blue}{PRM Score: 0.97.}\\
& Step 3: So, we can calculate the number of cows in the zoo by multiplying the number of cows in the farm by 2. So, 2 * 30 = 60, So, the zoo has 60 cows. \textcolor{blue}{PRM Score: 0.97.}\\
& Step 4: we know that the farm has half as many sheep as the zoo does. \textcolor{blue}{PRM Score: 0.96.}\\
& Step 5: So, we can calculate the number of sheep in the farm by dividing the number of sheep in the zoo by 2. So, 20 / 2 = 10. So, the farm has 10 sheep. \textcolor{blue}{PRM Score: 0.93.}\\
& Step 6: Now, we can calculate the total number of animals in the farm and the zoo by adding the number of cows and the number of sheep in both places. \textcolor{blue}{PRM Score: 0.92.}\\
& Step 7: \textcolor{red}{So, 30 (farm's cows) + 60 (zoo's cows) + 10 (farm's sheep) = 100.} \textcolor{blue}{PRM Score: 0.11.}\\
& Step 8: \textcolor{red}{So, the farm and the zoo have a total of 100 animals combined.} \textcolor{blue}{PRM Score: 0.10.}\\
& Step 9: \textcolor{red}{Therefore, the final answer is \boxed{100}. The answer is: 100.} \textcolor{blue}{PRM Score: 0.06.} \textcolor{olive}{ORM Score: 0.89.}
\\\midrule
    \end{tabular}
    \label{tab:prm_case}
\end{table*}
\vspace{-0.1cm}

\subsection{Data Contamination Check}
Apart from the performance analysis, we also investigate whether evolution leads to the data contamination between training data and test set. To address this consideration,   we employ instructions in the  GSM8k and MATH test set as queries to retrieve the top-5 samples from all evolved training data with an   embedding model, gte-large ~\citep{Li2023TowardsGT}.
Additionally, we employ GPT-4  to provide similarity judgement between the test sets and the retrieved samples, and remove the top-2 similar instructions. The prompt and details are shown in Appendix \ref{subsec:data_contamination} and \ref{appendix:data_filter_appendix}. Figure \ref{fig:data_filter} illustrates that the evolution process does not yield higher similarity scores. 

\subsection{Case Study}
\vspace{-0.1cm}
\textbf{Evol-Instruct.} The Examples 3 and 4 in the Appendix \ref{appendix:evol_prompts} shows the prompt and corresponding cases of GSM8k and MATH instruction evolution, demonstrating that the evolved instructions exhibit more complexity and diversity than the original training set.

\textbf{PRM v.s. ORM.} We present a comprehensive case study to  illustrate the effectiveness of our PRM. As delineated in Table \ref{tab:prm_case}, PRM demonstrates precise performance on a challenge math problem from the GSM8k test set. Remarkably, our PRM effectively distinguished the incorrect solution, in the meanwhile the ORM struggled in this task. Furthermore, PRM demonstrated exceptional insight by accurately detecting the incorrect steps of the solution chosen by ORM, specifically the steps 7, 8, and 9. Subsequently, PRM also assigned lower score logits to these erroneous steps.

		
 

This paper introduces \modelname{}, a mathematics model fine-tuned with \textbf{\REInameS{}}. The experimental results demonstrate that \modelname{} achieves SOTA performance surpassing existing open-source LLMs on  GSM8k and MATH from grade to high school problems. Notably,  \modelname{} 70B exhibits superior performance compared to some of the well-known proprietary  LLMs, including ChatGPT-3.5, Claude Instant, PaLM-2, Gemini Pro. Furthermore, our preliminary exploration highlights the pivotal role of instruction evolution and process supervision in achieving exceptional performance.

\clearpage

\newpage

\end{document}