\title{MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models}

\begin{document}

\doparttoc % Tell to minitoc to generate a toc for the parts
\faketableofcontents
\maketitle

\blfootnote{\textsuperscript{$\star$}Equal contribution \quad \textsuperscript{\textdagger}Corresponding author}

\vspace{-16mm}
\begin{center}
          \fontsize{9pt}{\baselineskip}\selectfont
          {Project page:}~\tt\href{https://meta-math.github.io/}{\textbf{meta-math.github.io}}
         \vspace{3mm}
\end{center}

\vspace{-3mm}
\begin{abstract}
\vspace{-2.5mm}

    Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (\eg, LLaMA-2) are still far away from satisfactory for solving mathematical problems due to the complex reasoning procedures. To bridge this gap, we propose \emph{MetaMath}, a finetuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives, which results in a new dataset called {MetaMathQA}. Then we finetune the LLaMA-2 models on MetaMathQA.
    Experimental results on two popular benchmarks (\ie, GSM8K and MATH) for mathematical reasoning 
    demonstrate that 
    MetaMath outperforms a suite of open-source LLMs by a significant margin.  Our MetaMath-7B model achieves $66.5\%$ on GSM8K and $19.8\%$ on MATH, exceeding the state-of-the-art models of the same size by $11.5\%$ and $8.7\%$.
    Particularly,
    {MetaMath-70B} achieves an accuracy of $82.3\%$ on {GSM8K}, slightly better than {GPT-3.5-Turbo}.
    We release the {MetaMathQA} dataset, the {MetaMath} models with different model sizes and the training code for public use.
\end{abstract}
 
\begin{figure}[ht]
    \centering
    \vskip -.2in
    \begin{minipage}{1\linewidth}
        \centering
        \includegraphics[width=.95\linewidth]{figs/fig1_1735.pdf}
        \label{fig:large}
    \end{minipage}
    \vspace{-2mm}
    \begin{minipage}{0.42\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/gsm_over_bar.pdf}
        \label{fig:small1}
    \end{minipage}
    \hspace{0.8cm}
    \begin{minipage}{0.42\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/math_over_bar.pdf}
        \label{fig:small2}
    \end{minipage}
    \vspace{-3.5mm}
    \caption{\footnotesize Overview of the {MetaMathQA} dataset and the mathematical problem-solving LLM -- {MetaMath}. We note that our MetaMath-70B is finetuned by QLoRA \cite{dettmers2023qlora} due to the computing resource limitation.}
    \label{fig:overview}
    \vskip -.15in
\end{figure}
    

\section{Introduction}
\vspace{-5mm}
    Recent years have witnessed the rapid development of large language models (LLMs) which emerge as the favored approach for various applications and demonstrate multi-dimensional abilities, including instruction following~\citep{brown2020language,alpaca,ouyang2022training, li2023deepinception}, coding assistance \citep{chen2021evaluating,nijkamp2022codegen,luo2023wizardcoder,li2023starcoder}, and mathematical problem-solving \citep{yuan2023scaling,imani2023mathprompter,luo2023wizardmath, collins2023evaluating}. 
    Among various tasks, solving mathematical problems is more challenging as they often require highly complex and symbolic multi-step reasoning capabilities. 
    Although some close-sourced models, \eg, {GPT-3.5-Turbo}~\citep{gpt3-5}, {GPT-4}~\citep{gpt4} and {PaLM-2}~\citep{touvron2023llama}, have demonstrated promising performance on some mathematical problem-solving benchmarks, it is still a mystery how these models are trained and what data these models use. Therefore, how to equip open-source LLMs (\eg, {LLaMA} \citep{touvron2023llama1,touvron2023llama}) with good mathematical problem-solving skills remains an open challenge. 

    \vspace{-0.25mm}
    
    To tackle this challenge, two popular lines of research to improve the mathematical problem-solving abilities of LLMs are: \emph{prompt-based methods} and \emph{finetuning-based methods}. Prompt-based methods~\cite{wei2022chain,fu2023complexitybased,wang2023selfconsistency,zhou2023leasttomost,xiong2023dq,xin2023lego} aim to activate the potential capacities of LLMs by choosing suitable prompting inputs without modifying the model parameters. Finetuning-based methods update the open-source LLMs (\eg, LLaMA) under the guidance of some other powerful closed-source LLMs (\eg, GPT-3.5~\cite{gpt3-5}, GPT-4~\cite{gpt4}). While prompt-based methods are model-dependent and sensitive to many factors, finetuning-based methods, despite being simple and model-agnostic, heavily rely on effective training data on downstream mathematical questions. Our work aims to improve finetuning-based methods with a novel method to bootstrap available mathematical questions in the training set. 
    Specifically, we propose to bootstrap the questions in both forward and backward reasoning directions. For the forward direction, we have the original and LLM-rephrased questions. For the backward direction, we have the self-verification question~\cite{weng2023large} and FOBAR question~\cite{jiang2023backward}. To construct backward reasoning questions, we mask a token in a question using an identifier ``x'' and ask the model to predict the masked token if the answer is provided. Different from \cite{weng2023large,jiang2023backward} that apply backward reasoning for inference verification, we use it as a form of question for language model fine-tuning. For answers, we adopt an answer augmentation method based on rejection sampling~\cite{yuan2023scaling}, where diverse reasoning paths are generated and only those with correct answers are used. After combining both forward and backward mathematical questions with augmented answers, we construct a new dataset for fine-tuning, called \emph{MetaMathQA}. By fine-tuning LLaMA-2 on MetaMathQA, we obtain our \emph{MetaMath} model. Our approach is guided by the insight that a mathematical question represents merely a single view of the underlying meta-knowledge. Therefore, question bootstrapping can be viewed as a form of multi-view augmentation in order to enable the transfer of the meta-knowledge. Leveraging the MetaMathQA dataset, MetaMath demonstrates exceptional performance in mathematical reasoning, positioning it among the top performers on widely recognized evaluation benchmarks.

    \vspace{-0.25mm}
    
    \begin{wrapfigure}{r}{0.5\linewidth}
    \vspace{-2.3em}
    \centering
    \includegraphics[width=0.97\linewidth]{figs/Dual_Y_Axis_fig1_refined.pdf}
    \vspace{-.9em}
	\caption{\footnotesize GSM8K accuracy of LLaMA-2-7B finetuned on different sizes of answer augmentation data. A larger diversity gain indicates the question is more diverse compared to the existing questions. Detailed experimental setup is given in Section \ref{sec:expt-setup}.}
	\label{fig:Accuracy Saturation}
    \vspace{-1em}
    \end{wrapfigure}
    
    Another motivation behind question bootstrapping is to enlarge the question diversity~\citep{eldan2023tinystories} such that the question distribution can be rich enough to cover more unseen scenarios. We quantify the question diversity of the original questions and our MetaMathQA dataset in Figure~\ref{fig:Accuracy Saturation}. The diversity gain~\citep{bilmes2022submodularity} indicates how diverse the question is compared to the existing dataset, and a larger diversity gain means the new question is more different from the existing dataset. With question bootstrapping, our MetaMathQA dataset is much more diverse than the original dataset. We also observe that the test accuracy without bootstrapped questions rapidly reaches a state of saturation. In contrast, the test accuracy, when using bootstrapped questions, continues to exhibit a steady increase.

    \vspace{-0.25mm}

    Question bootstrapping also has an intrinsic connection to dataset distillation~\cite{wang2018dataset,zhao2020dataset} and machine teaching~\cite{zhu2015machine,liu2017iterative,liu2021iterative,qiu2023iterative}, where the shared target is to construct a training dataset that best facilitates generalization. Unlike both methods that focus on optimizing the training empirical risk, question bootstrapping uses the reasoning diversity of questions as a heuristic proxy and maximizes this diversity by constructing forward, backward and rephrased questions. MetaMath aims to transfer the underlying meta-knowledge to enable strong generalization~\cite{kilbertus2018generalization}. Our contributions are listed below:

    \begin{itemize}[leftmargin=*,nosep]
    \setlength\itemsep{0.28em}
        \item We propose a novel question bootstrapping method to augment the training dataset, resulting in MetaMathQA. Question bootstrapping rewrites questions with both forward and backward reasoning paths and also leverages LLMs to rephrase the question text.
        \item Based on the MetaMathQA dataset, MetaMath is finetuned from state-of-the-art open-source LLMs (\eg, LLaMA-2), showing excellent elementary mathematical problem-solving capability.
        \item We identify an important factor when creating the MetaMathQA dataset -- question diversity. The diversity is particularly important in reasoning directions, and backward reasoning questions are very helpful for LLMs to understand mathematical knowledge without memorization.
        \item We conduct experiments on two standard mathematical reasoning benchmarks: GSM8K \citep{cobbe2021training} and MATH \citep{hendrycks2021measuring}.  MetaMath outperforms existing open-source LLMs by a large margin. MetaMath-7B has achieved $66.5\%$ on GSM8K ($+11.5\%$ compared to the previous best open-source LLM) on GSM8K and $19.8\%$ on MATH ($+8.7\%$ compared to the previous best open-source LLM).
        \item Our work studies data augmentation for improving the mathematical problem-solving ability of LLMs. Despite being simple, our method significantly outperforms many intricate methods. Our results highlight the importance of data augmentation and also shed light on other reasoning tasks.
    \end{itemize}
		
    \vspace{-2mm}
    \section{Related Work}
    \vspace{-2mm}

    \textbf{Large Language Models (LLMs)} \cite{devlin2019bert, liu2019roberta, gpt2, Colin2020t5,brown2020language, touvron2023llama1, sun2023survey} have achieved great success in various natural language processing tasks, \eg, topic classification \citep{min2022metaicl, jiang2023effective, jiang2023byom}, sentiment classification \citep{brown2020language, min2022metaicl}, translation \citep{brown2020language}, by few-shot prompting (or in-context learning) \citep{brown2020language, min2022metaicl, chen2022meta}. Recently, \citet{wei2022chain,wang2023selfconsistency} show that LLMs with more than 100B parameters (\eg, GPT-3 \citep{brown2020language} with 175B, PaLM with 540B \citep{chowdhery2022palm}) can solve complex tasks by generating multiple reasoning steps towards the answer when given a few reasoning examples as demonstration. While both GPT-3.5 \citep{gpt3-5} and GPT-4 \citep{gpt4} have shown promising reasoning ability for complex mathematical tasks like MATH \cite{hendrycks2021measuring}, the performance of open-source models (\eg, LLaMA-1 \citep{touvron2023llama1}, LLaMA-2 \citep{touvron2023llama}) is far from satisfactory.

    \vspace{-1mm}
    \textbf{Learning Mathematical Reasoning} for 
    complex math tasks
    like GSM8K \citep{cobbe2021training} and MATH \citep{hendrycks2021measuring} is one of the most challenging
    problem in open-source LLMs.
    \citet{wei2022chain} enhances the reasoning ability of LLMs by augmenting the output with a sequence of intermediate steps toward the answer. 
    A few methods~\citep{fu2023complexitybased,wang2023selfconsistency,zhou2023leasttomost} are proposed to 
    improve the quality of reasoning paths.
    For example,
    Complexity-based CoT \citep{fu2023complexitybased}
    selects examples with more steps as in-context demonstrations and shows that prompting with more reasoning steps leads to better performance.
    Self-Consistency \citep{wang2023selfconsistency}
    samples multiple reasoning paths
    and selects the final answer by majority voting.   
    Another category of work is finetuning-based methods, which finetunes open-source models (\eg, LLaMA) with the knowledge from some advanced closed-source LLMs \citep{gpt3-5,gpt4}. 
    \citet{magister2023teaching} investigates the transfer of reasoning capabilities via knowledge distillation. 
    \citet{yuan2023scaling} proposes to apply rejection sampling finetuning (RFT) to improve mathematical reasoning performance.  
    WizardMath \citep{luo2023wizardmath} proposes a reinforced evol-instruct method to enhance reasoning abilities by supervised fine-tuning and PPO training \citep{schulman2017proximal}. MAmmoTH~\citep{yue2023mammoth} combines CoT and Program-of-Thought \citep{chen2022program} rationales for teaching LLMs to use external tools (e.g., Python interpreter) for solving mathematical problems. \citet{wang2023making} propose a constraint alignment loss to finetune LLMs for calibration.
    
    \vspace{-1mm}
    \textbf{Knowledge Distillation} \citep{hinton2015distilling, gou2021knowledge} transfers knowledge from a larger teacher model to a smaller student model, achieving promising performance in many applications \citep{shen2018feature, park2019relational, he2019knowledge, mirzadeh2020improved},
    Recently, \citep{li2022explanations, huang2022large, ho2023large, magister2023teaching, hsieh2023distilling, fu23d, shridhar2023distilling} propose to 
    transfer reasoning abilities 
    from LLMs (\eg, GPT-3.5 \citep{gpt3-5}, PaLM \citep{chowdhery2022palm}) to small language models (\eg, T5 \citep{Colin2020t5}, GPT-2 \citep{gpt2}). For example,
    Finetune-CoT \citep{ho2023large} samples multiple reasoning paths from LLMs and  finetune the student model with correct ones,
    while Self-Improve \citep{huang2022large} chooses the one with the highest confidence.
    \citet{li2022explanations} further feeds the question and ground-truth label to LLMs
    for prompting its reasoning path.
    \citet{shridhar2023distilling} proposes to generate sub-questions and solution pairs for training. Small models finetuned by knowledge distillation can achieve similar performance to LLMs \citep{magister2023teaching, ho2023large} on both common sense reasoning (\eg, CommonSenseQA \citep{talmor2019commonsenseqa}) and symbol reasoning (\eg, Coin Flip \citep{wei2022chain}). However, for solving challenging mathematical problems (\eg, GSM8K \citep{cobbe2021training}), there is still a large performance gap  \citep{ho2023large, fu23d, magister2023teaching}.

    \vspace{-2mm}
    \section{Method}	\label{sec:method}
    
\vspace{-2mm}
The overview of our method is illustrated in Figure~\ref{fig:overview}. Given a meta-question (a sample in the original mathematical training set), we can generate a series of variants. Specifically, we perform three types of question bootstrapping. Combined with answer augmentation, we present {MetaMathQA}, a diverse and high-quality mathematical dataset based on GSM8K and MATH. We then present {MetaMath}, a family of LLMs finetuned on MetaMathQA focusing on elementary mathematical problem-solving. 

\subsection{Answer Augmentation (AnsAug)}
\vspace{-1mm}

Generating more reasoning paths 
is a simple but effective way
to augment 
the training set.
For a question $q_i$,
we 
use few-shot chain-of-thought prompting with temperature sampling 
to generate $K_{\text{AnsAug}}$ more reasoning paths $\{(r_i^{(j)}, a_i^{(j)}): j=1,\dots, K_{\text{AnsAug}}\}$:
the question is appended to a few in-context reasoning examples,
then fed to the LLM
for generating its reasoning path $r_i^{(j)}$ and answer $a_i^{(j)}$.
We filter out reasoning paths with correct answers as:
\begin{equation}
\small
\hD_{\text{AnsAug}} = \{(q_i, r_i^{(j)}, a_i^{(j)}): a_i^{(j)} = a_i^\star;  i=1,\dots, N_q; j=1,\dots, K_{\text{AnsAug}}\}.
\end{equation}

\subsection{Question Bootstrapping by LLM Rephrasing}
\vspace{-1mm}

Generating more answers for mathematical questions with LLMs is straightforward, but creating questions is more challenging. Math Questions
are written by well-educated teachers. Hence, enlarging the question set through manual creation is time-consuming and labor-intensive. To address this issue, we propose rephrasing prompting to generate more questions through the LLM. 

\vspace{2mm}
\begin{exmp}{Rephrasing Question}{rephrase-example}
\small
\textbf{Question:} What is the total amount that James paid when he purchased 5 packs of beef, each weighing 4 pounds, at a price of \$5.50 per pound? 

\textbf{Answer:} Each pack of beef weighs 4 pounds, so 5 packs weigh 4 * 5 = 20 pounds in total. The price per pound of beef is \$5.50, so the total cost for 20 pounds is 20 * \$5.50 = \$110. ~...~
The answer is: 110.
\end{exmp}
\vspace{2.5mm}

Specifically, for a question $q_i$, we append it to the prompt, which is then fed to the LLM for generating the rephrased question. Example~\ref{exmp:rephrase-example} shows a generated rephrased question and the complete prompt is shown in Appendix \ref{sec:rephrase-prompt}. We adopt temperature sampling to sample $K_{\text{rephrase}}$ rephrased questions for each meta-question. For the rephrased questions, it is time-consuming to manually check the consistency compared with the original questions. We propose a supervised method to evaluate the correctness between the rephrased questions and the meta-questions. For each rephrased question $\hat{q}_i^{(j)}$, we use few-shot Chain-of-Thought prompting to generate its reasoning path $\hat{r}_i^{(j)}$ and answer $\hat{a}_i^{(j)}$, which is compared with the ground-truth answer $a_i^\star$. The accuracy of Complexity-based CoT~\citep{fu2023complexitybased} for answering the rephrased question by GPT-3.5-Turbo is $76.30\%$, which is comparable to that of answering the original training questions ($80.74\%$). This suggests that the quality of rephrased questions is preserved high while the question diversity is improved. We collect the rephrased questions with correct answers (\ie, $\hat{a}_i^{(j)} = a_i^\star$) as the augmented data:
\begin{equation}
\small
\hD_{\text{rephrase}} = \{(\hat{q}_i, \hat{r}_i^{(j)}, \hat{a}_i^{(j)}): \hat{a}_i^{(j)} = a_i^\star;  i=1,\dots, N_q; j=1,\dots, K_{\text{rephrase}}\}.
\end{equation}

 
	
\subsection{Question Bootstrapping by Backward Reasoning}
\vspace{-1mm}

Backward reasoning plays an important role in answering many mathematical questions, \ie, starting with a given condition and thinking backward to determine an unknown variable in the question. One specific example between a question and a backward question is illustrated in Example \ref{exmp:if}. 
However, existing methods (SFT, RFT, WizardMath) have significantly lower accuracy on backward questions, as shown in Figure~\ref{fig:reverse}, motivating us to bootstrap backward questions to improve the reasoning ability.

\vspace{2mm}

\begin{exmp}{Question and Backward Question}{if}
    \small
    \textbf{Question:}
    James buys 5 packs of beef that are 4 pounds each. The price of beef is \$5.50 per pound. How much did he pay? \textbf{Answer:} He bought 5*4=20 pounds of beef. He paid 20*5.5=\$110. The answer is: 110 \textcolor{red}{{\ding{51}}}
    
    \textbf{Backward Question:}
    James buys x packs of beef that are 4 pounds each. The price of beef is \$5.50 per pound. How much did he pay? If we know the answer to the above question is 110, what is the value of unknown variable x? \textbf{Answer:} The total weight of the beef is 4*x because 4*5.5 = 22. ...  The answer is: 27 \textcolor{red}{\ding{55}}
\end{exmp}
\vspace{2.5mm}
   
 
	
To improve the backward reasoning ability of finetuned models,
we generate more questions which can be solved in a backward manner: 
a number in the question $q_i$ is masked by 
``x'',
while the LLM is asked to predict the 
value of ``x'' when its answer $a_i^\star$
is provided.
Different from forward reasoning,
which generates explicit intermediate steps towards the
final answer,
backward reasoning starts with the answer
and generates multiple reasoning steps 
to predict the masked number.
Representative backward reasoning methods
include 
Self-Verification \citep{weng2023large}
and FOBAR \citep{jiang2023backward}.

In Self-Verification (SV) \citep{weng2023large}, the question with the answer is first rewritten into a declarative statement, \eg, ``How much did he pay?'' (with the answer 110) is rewritten into ``He paid \$10''. 
Then, a question for asking the value of $\vx$ is appended, \eg, ``What is the value of unknown variable $\vx$?''. Example \ref{exmp:sv-example} gives an augmented example. We collect the new questions and their generated reasoning paths with correct answers as the augmented data:
\begin{equation}
\small
\hD_{\text{SV}} = \{(\tilde{q}_i^{(j)}, \tilde{r}_i^{(j)}, \tilde{a}_i^{(j)}): \tilde{a}_i^{(j)} = a_i^\star;  i=1,\dots, N_q; j=1,\dots, K_{\text{SV}}\}.
\end{equation}

\vspace{2.5mm}
\begin{exmp}{Self-Verification \citep{weng2023large} Question}{sv-example}
\small
\textbf{Question:} James buys {\color{red3}x} packs of beef that are 4 pounds each.  The price of beef is \$5.50 per pound. He paid 110.
{\color{red3}What is the value of unknown variable x?} \\
\textbf{Answer:} 
To solve this problem, we need to determine the value of x, which represents the number of packs of beef that James bought.
Each pack of beef weighs 4 pounds and 
~...~
The value of x is 5.
\end{exmp}
\vspace{5mm}

\begin{exmp}{FOBAR~\citep{jiang2023backward} Question}{fobar}\small
\textbf{Question:} 
James buys {\color{red3}x} packs of beef that are 4 pounds each.  The price of beef is \$5.50 per pound.  How much did he pay?
{\color{red3}\!If we know the answer to the above question is 110, what is the value of unknown variable x?\!\!\!}

\textbf{Answer:}
James buys x packs of beef that are 4 pounds each, so he buys a total of 4x pounds of beef.
The price of beef is \$5.50 per pound, so the total cost of the beef is 5.50 * 4x = 22x.
~...~
The value of x is 5.
\end{exmp}

\vspace{3mm}
 
Self-Verification
needs to rewrite the question with an answer
into a declarative
statement,
which is challenging for complex questions.
To address this issue,
FOBAR~\citep{jiang2023backward} proposes to
directly append the answer to the question,
\ie,
``\textit{If we know the answer to the above question is \{$a_i^\star$\} , what is the value of unknown variable {\normalfont x}?}''
Example~\ref{exmp:fobar}
shows an example.
We collect the new questions along with their correct answers
as our augmented data:
\begin{equation}
\small
    \hD_{\text{FOBAR}} = \{(\bar{q}_i^{(j)}, \bar{r}_i^{(j)}, \bar{a}_i^{(j)}): \bar{a}_i^{(j)} = a_i^\star;  i=1,\dots, N_q; j=1,\dots, K_{\text{FOBAR}}\}.
\end{equation}
    

	
\subsection{Finetuning Objective Functions}
We merge all the augmented data, including answer-augmented data and bootstrapped questions (Rephrasing, Self-Verification, FOBAR) as $ \small{	\hD_{\text{MetaMathQA}} = \hD_{\text{AnsAug}}  \cup \hD_{\text{rephrase}} \cup \hD_{\text{SV}} \cup \hD_{\text{FOBAR}}}$.
We finetune a LLM model (parameterized by $\vtheta$) on $\hD_{\text{MetaMathQA}}$ to obtain the MetaMath model
by maximizing the log likelihood of the reasoning path conditioned on the question, \ie, $\small{
\mathcal{L}(\vtheta)= \sum_{(q, r, a) \in \hD_{\text{MetaMathQA}} } \log \bP(r \mid  q;\vtheta).
\label{eq:mle}}$ Although we only consider LLaMA-2 here, MetaMathQA can also be used to finetune other LLMs.
    
\vspace{-.5mm}
\section{Experiments and Results}
\vspace{-1mm}
    \subsection{Experimental Setup}\label{sec:expt-setup}
\vspace{-.5mm}
    \setlength{\columnsep}{9pt}
    \begin{wraptable}{r}[0cm]{0pt}
    \centering
    \footnotesize
    \setlength{\tabcolsep}{2.6pt}
	\renewcommand{\arraystretch}{1.25}
    \begin{tabular}{c|ccccc}
    \specialrule{0em}{0pt}{-22pt}
     Dataset   & AnsAug & Rephrasing & SV & FOBAR & Overall \\
    \shline
    MetaMathQA-GSM8K & 80K & 80K & 40K & 40K & 240K \\
    MetaMathQA-MATH  & 75K & 50K & 15K & 15K & 155K \\
    MetaMathQA  & 155K  & 130K  & 55K  & 55K & 395K \\
    \specialrule{0em}{0pt}{-7pt}
    \end{tabular}
    \caption{\footnotesize Number of samples in the proposed MetaMathQA.}\label{exp:dataset}
    \vspace{-1.5mm}
    \end{wraptable}
    
    \textbf{Datasets.}
    We use two popular
    mathematical reasoning benchmarks:
    \begin{enumerate*}[(i), series = tobecont, itemjoin = ~~]
    \item GSM8K \citep{cobbe2021training} is a dataset consisting of high-quality grade school math 
    problems, containing 7,473 training samples and 1,319 testing samples;
    and 
    \item MATH \citep{hendrycks2021measuring}
    dataset consists of high school math competition problems that span 
    seven subjects including 
    Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra,
    and Precalculus.
    It contains 7,500 and 5,000 samples for training and testing, respectively.
    \end{enumerate*}
    Questions in GSM8K \citep{cobbe2021training} take between 2 and 8 steps to reach the answer, while 
    MATH
    is much more challenging.
    \vspace{-1.mm}
    
\newpage
\setlength{\columnsep}{14pt}
    \begin{wraptable}{r}[0cm]{0pt}
    \centering
    \hspace{-2.4mm}
    \footnotesize
    \setlength{\tabcolsep}{6.5pt}
    \renewcommand{\arraystretch}{1.223}
    \begin{tabular}{l|ccc}
    \specialrule{0em}{0pt}{-1pt}
    Model& \#params & GSM8K & MATH \\\shline
    \multicolumn{4}{c}{\textit{closed-source models}}  \\
    GPT-4 \citep{gpt4} & - & 92.0 & 42.5 \\
    GPT-3.5-Turbo \citep{gpt3-5-turbo} & -& 80.8 & 34.1 \\
    PaLM \citep{chowdhery2022palm} & 8B & 4.1 & 1.5 \\ 
    PaLM \citep{chowdhery2022palm} & 62B & 33.0 & 4.4 \\ 
    PaLM \citep{chowdhery2022palm} & 540B & 56.5 & 8.8 \\ 
    PaLM-2 \citep{anil2023palm} & 540B & 80.7 & 34.3 \\
    Flan-PaLM 2 \citep{anil2023palm} & 540B & 84.7 & 33.2 \\
    Minerva \citep{lewkowycz2022solving} & 8B & 16.2 & 14.1 \\
    Minerva \citep{lewkowycz2022solving} & 62B & 52.4 & 27.6\\
    Minerva \citep{lewkowycz2022solving} & 540B & 58.8 & 33.6\\\shline
    \multicolumn{4}{c}{\textit{open-source models (1-10B)}}  \\
    LLaMA-2 \citep{touvron2023llama} & 7B & 14.6 & 2.5 \\
    MPT \citep{MosaicML2023Introducing} & 7B & 6.8 & 3.0 \\
    Falcon \citep{penedo2023refinedweb} & 7B & 6.8 & 2.3 \\
    {Code-LLaMA \cite{ro023code}} & {7B} & {25.2}& {  13.0}\\
    InternLM \citep{2023internlm} & 7B & 31.2 & - \\
    GPT-J \citep{gpt-j} & 6B & 34.9 & - \\
    ChatGLM 2 \citep{zeng2022glm} & 6B & 32.4 & - \\
    Qwen \citep{qianwen} & 7B & 51.6 & - \\
    Baichuan-2 \citep{baichuan2} & 7B & 24.5 & 5.6 \\
    SFT \citep{touvron2023llama} & 7B & 41.6 & - \\
    RFT \citep{yuan2023scaling} & 7B & 50.3 & - \\
    {MAmooTH-CoT \cite{yue2023mammoth}} & { 7B} & {50.5}& {10.4}\\
    WizardMath \citep{luo2023wizardmath} & 7B & 54.9 & 10.7 \\ 
    \rowcolor{Gray}
    MetaMath  & 7B & \textbf{66.5}& \textbf{19.8} \\\shline
    \multicolumn{4}{c}{\textit{open-source models (11-50B)}}  \\
    LLaMA-2 \citep{touvron2023llama} & 13B & 28.7 & 3.9 \\
    LLaMA-2 \citep{touvron2023llama} & 34B & 42.2 & 6.2\\
    MPT \citep{MosaicML2023Introducing} & 30B & 15.2 & 3.1 \\
    Falcon \citep{penedo2023refinedweb} & 40B & 19.6 & 2.5 \\
    GAL \citep{taylor2022galactica} & 30B & - & 12.7  \\
    {Platypus \cite{platypus2023}} & {13B} & {25.7}& {2.5}\\
    {Orca-Platypus \cite{platypus2023}} & {13B} & {38.4}& {3.0} \\
    Vicuna \citep{vicuna2023} & 13B & 27.6 & - \\
    {Code-LLaMA \cite{ro023code}} & {13B} & {36.1}& {16.4}\\
    Baichuan-2 \citep{baichuan2} & 13B & 52.8 & 10.1\\
    SFT \citep{touvron2023llama} & 13B & 50.0 & - \\
    RFT \citep{yuan2023scaling} & 13B & 54.8 & - \\
    {MAmooTH-CoT \cite{yue2023mammoth}} & {13B} & {56.3}& {12.9}\\
    WizardMath \citep{luo2023wizardmath} & 13B & 63.9 & 14.0  \\ 
    \rowcolor{Gray}
    MetaMath  & 13B & \textbf{72.3} & \textbf{22.4} \\\shline
    \multicolumn{4}{c}{\textit{open-source models (51-70B)}} \\
    LLaMA-2 \citep{touvron2023llama} & 70B & 56.8 & 13.5 \\
    RFT \citep{yuan2023scaling}             & 70B & 64.8 & - \\
    {Platypus \cite{platypus2023}} & {70B} & {70.6}& { 15.6}\\
    {MAmooTH-CoT \citep{yue2023mammoth}} & {70B} & {72.4}& {21.1}\\
    WizardMath \citep{luo2023wizardmath}    & 70B & 81.6 & 22.7  \\ 
    \rowcolor{Gray}
    MetaMath$^\ddag$               & 70B & \textbf{82.3} & \textbf{26.6} \\
    \specialrule{0em}{-5pt}{0pt}
    \end{tabular}
    \caption{\footnotesize Comparison of testing accuracy to existing LLMs on GSM8K and MATH. $^\ddag$Due to the computing resource limitation, we finetune MetaMath-70B using QLoRA \citep{dettmers2023qlora}.}
    \label{exp:main-expt}
    \vspace{-25mm}
    \end{wraptable}

    \textbf{Models.}
    We use the current state-of-the-art open-source model
    LLaMA-2
    \citep{touvron2023llama},
    including three different parameter sizes:
    7B, 
    13B,
    and 
    70B,
    as the base model for fine-tuning.
    GPT-3.5-Turbo is used for rephrasing questions as well as 
    generating answers in all four augmentations,
    where the temperature is set to 0.7 as in \cite{wang2023selfconsistency}.
    The LLaMA-2-7B and LLaMA-2-13B are trained by fully fine-tuning. LLaMA-2-70B is finetuned by QLoRA~\citep{dettmers2023qlora} for computational efficiency. More experimental details can be seen in Appendix~\ref{expdetalis}.
    
    \textbf{Baselines.}
    The proposed methods are compared with 
    \begin{enumerate*}[(i), series = tobecont, itemjoin = \quad]
    \item closed-source models such as GPT-3.5-Turbo~\citep{gpt3-5-turbo}, PaLM~\citep{chowdhery2022palm};
    \item open-source models such as LLaMA-1~\citep{touvron2023llama1}, LLaMA-2~\citep{touvron2023llama};
    \item Supervised Fine-Tuning (SFT),
    which uses the training set of the original GSM8K or MATH datasets;
    \item Rejection sampling Fine-Tuning (RFT) \citep{yuan2023scaling}
    generates and collects correct reasoning paths as 
    augmented data for fine-tuning; 
    \item WizardMath \citep{luo2023wizardmath}
    which generates samples and trains two reward models using ChatGPT \footnote{\url{https://openai.com/}} to select samples for fine-tuning. 
    \end{enumerate*}

    

    \textbf{Diversity Gain.} We use the diversity gain \citep{bilmes2022submodularity} to measure to what extent a new dataset added to a basic dataset can improve the overall data diversity. For a base dataset $\hD_{\text{base}}=\{x_i=(q_i, r_i, a_i)\}_{i=1}^N$ with $N$ samples, and a new dataset $\hD_{\text{new}}=\{x_i=(q_i, r_i, a_i)\}_{i=1}^M$ with M samples, the diversity gain is defined as: $\hD_{\text{new}}$ relative to $\hD_{\text{base}}$ as: $d_{\text{gain}}=\frac{1}{M}\sum_{x_{i}\in\hD_{\text{new}}}\min_{x_{j}\in\hD_{\text{base}}}(\|f(x_{i}) -f(x_{j})\|_2^2)$, where $f$ is the feature extractor and we use the OpenAI Embedding API \textit{text-embedding-ada-002} for feature extraction. For Figure~\ref{fig:Accuracy Saturation}, we change the data size of base data and select a fixed set of 20K new data points that the model has not encountered to form $\hD_{\text{new}}$.

    \vspace{-1mm}
    \subsection{Results on GSM8K and MATH}
    \vspace{-1.5mm}

    Table~\ref{exp:dataset} illustrates the detailed description of our MetaMathQA collection and 
    Table \ref{exp:main-expt}
    shows the testing accuracy on GSM8K and MATH.
    As can be seen,
    for open-source models with 
    1-10B parameters,
    MetaMath achieves the state-of-the-art performance.
    Compared to the previous best LLM, MetaMath achieves a large improvement of 11.6\% on GSM8K and 9.1\% on MATH in testing accuracy,
    showing that finetuning on our MetaMathQA data is effective.

    
    
     
   
    
    As for LLMs with 11-50B parameters,
    the proposed MetaMath performs the best.
    Particularly,
    on both GSM8K and MATH,
    MetaMath achieves higher accuracy than SFT, RFT, and WizardMath
    by a large margin (+7\%), demonstrating 
    the effectiveness of the MetaMath data in improving mathematical reasoning ability.
    Furthermore,
    for LLMs with 51-70B parameters,
    again, 
    MetaMath achieves the highest testing accuracy.
    Particularly,
    MetaMath is better than GPT-3.5-Turbo on GSM8K, 
    which is used for generating augmented data for finetuning.
    
    
    \vspace{-2mm}
	\subsection{Effect of Augmentations} \label{sec:eff-aug}
    \vspace{-1.5mm}
	
	In this section,
	we conduct experiments 
	to study
	the effect of augmentations in MetaMath.
	We first finetune the {LLaMA-2-7B}
	model on augmented GSM8K (MetaMath-GSM8K) data, 
	and test the finetuned model 
	on GSM8K and MATH.
	Table \ref{exp:abl-effect-aug}
	shows the testing accuracy of
	different combinations of augmentations{, where we mix all augmented data together for each model}.
	As can be seen, 
	on GSM8K,
	the models trained on answer augmentation (AnsAug) or rephrasing  augmentation 
	achieve much higher
	accuracy than SFT, which is only trained on 
	the training set.
	Combing answer augmentation and rephrasing  augmentation data
	for fine-tuning leads to a slightly higher accuracy,
	which is further improved by about 4\% through merging the FOBAR and SV augmentation data.
	As for MATH,
	MetaMath trained only on MetaMahQA-GSM8K data
	performs better than SFT, suggesting its effectiveness in generalizing to
	unseen mathematical tasks. 
	
	
    We also conduct an experiment
    by fine-tuning
    {LLaMA-2-7B} on the augmented MATH (MetaMathQA-MATH) data
    then evaluate the model on GSM8K and MATH.
    Table \ref{exp:abl-effect-aug}
    shows the testing accuracy.
    Again,
    MetaMath trained on AnsAug or rephrasing augmentation data 
    performs much better
    than SFT.
    Furthermore,
    merging all augmented data together for
    fine-tuning is better than merging AnsAug and rephrasing augmentation data, demonstrating 
    the effectiveness of SV and FOBAR augmentation data
    in improving mathematical reasoning ability.
    Moreover,
    for the unseen GSM8K task,
    MetaMath trained on MetaMathQA-MATH data is significantly better
    than SFT (+20\%).
	
\begin{table}[!t]
\footnotesize
\centering
\setlength{\tabcolsep}{2.5pt}
\renewcommand{\arraystretch}{1.25}
\vspace{-4mm}
    \begin{tabular}{c|cccccc|cccccc}
        \multirow{2}{*}{Method}& \multicolumn{6}{c|}{GSM8K}& \multicolumn{6}{c}{MATH}\\
         & AnsAug & Rep. & SV  & FOBAR  
        & GSM8K & MATH & AnsAug & Rep. & SV  & FOBAR  
        & GSM8K & MATH\\\shline
        SFT \citep{touvron2023llama} & \xmark & \xmark & \xmark & \xmark & 41.6 & 3.0 & \xmark & \xmark & \xmark & \xmark & 13.8 & 4.7\\
        \hline
        \multirow{4}{*}{MetaMath}& \cmark & \xmark & \xmark & \xmark & 59.6 & 4.4 & \cmark & \xmark & \xmark & \xmark & 28.4 & 12.9 \\
        & \xmark & \cmark & \xmark & \xmark & 59.7 & 4.4 & \xmark & \cmark & \xmark & \xmark & 30.4 & 12.4 \\
        & \cmark & \cmark & \xmark & \xmark & 60.6 & 4.4& \cmark & \cmark & \xmark & \xmark & 29.1 & 15.3 \\
        & \cmark & \cmark & \cmark & \cmark & \textbf{64.4} & \textbf{5.7}& \cmark & \cmark & \cmark & \cmark & \textbf{34.6} & \textbf{17.7}\\
    \end{tabular}
   \vspace{-1.75mm}
\caption{\footnotesize Effect of different question augmentation with {LLaMA-2-7B} finetuned on GSM8K or MATH.}\label{exp:abl-effect-aug}
\vspace{-3.5mm}
\end{table}
	
    
  

    \begin{figure}[!b]
    \begin{minipage}{0.44\linewidth}
    \vspace{-1em}
    \centering
    \includegraphics[width=0.99\linewidth]{figs/ppl_box.pdf}
    \vspace{-1.85em}
	\caption{\footnotesize  Lower perplexity of MetaMathQA.}
	\label{fig:perplexity}
    \vspace{-0.5em}
    \end{minipage}
    \hfill
    \begin{minipage}{0.54\linewidth}
    \vspace{-1em}
    \centering
    \includegraphics[width=0.99\linewidth]{figs/table5.pdf}
    \vspace{-2.18em}
	\caption{\footnotesize  Accuracy correlates positively with diversity.}
	\label{fig:DiversityGains}
    \vspace{-0.5em}
    \end{minipage}
    \end{figure}

        \begin{figure}[b]
    \begin{minipage}{0.3\linewidth}
    \centering
    \vspace{-1.2em}
    \includegraphics[width=0.99\linewidth]{figs/diff_bar.pdf}
    \vspace{-1.8em}
	\caption{\footnotesize Combing RFT~\citep{yuan2023scaling} dataset with our MetaMathQA leads to a performance drop.}
	\label{fig:Less is More}
    \vspace{-0.7em}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\linewidth}
    \centering
    \vspace{-1.2em}
    \includegraphics[width=0.99\linewidth]{figs/gsm8k_backward.pdf}
    \vspace{-1.8em}
	\caption{\footnotesize The accuracy gap between GSM8K and GSM8K-Backward.}
	\label{fig:reverse}
    \vspace{-0.7em}
    \end{minipage}
    \hfill
    \begin{minipage}{0.3\linewidth}
    \centering
    \vspace{-1.2em}
    \includegraphics[width=0.99\linewidth]{figs/question_length.pdf}
    \vspace{-1.8em}
	\caption{\footnotesize Testing accuracy on questions with short length, medium length and long length.}
	\label{fig:erroranalysis}
    \vspace{-0.7em}
    \end{minipage}
    \end{figure}

    \vspace{-2mm}
    \subsection{Discussion from a Perplexity Perspective}    
    \vspace{-1.5mm}

    According to the Superficial Alignment Hypothesis proposed by ~\citet{zhou2023lima}, the capability of a model is rooted in pretraining, and data from downstream tasks acts to activate the inherent ability of LLMs that has been learned during pretraining. There are two important questions that arise from such a hypothesis: (i) \textit{what} kind of data is most effective at activating possible latent knowledge, and (ii) \textit{why} is one dataset better than another at such activation? Our empirical results suggest that, in the mathematical tasks we consider, our MetaMathQA dataset may serve as a superior activator of mathematical knowledge. Yet, \textit{why} MetaMath yields superior performance than training on the data of correct answer-only or GSM8K CoT is unclear. We speculate that perhaps it is the simplicity of the data that matters. As shown in Figure~\ref{fig:perplexity}, we compute the perplexity~\citep{wang2023making,marion2023less} for the under-finetuned {LLaMA-2-7B} model, in terms of answer-only data, GSM8K CoT, and the subsections of {MetaMathQA} data. The perplexity of {MetaMathQA} is significantly lower than the other two datasets. This highlights its inherently easy-to-learn nature, which may be more conducive to eliciting bolstered problem-solving abilities from an LLM. This is also aligned with the findings with TinyStories~\citep{eldan2023tinystories}, where short and easy story data can help LLMs generate content fluently.

    
        

    

    \vspace{-2.00mm}
    \subsection{Discussion from a Diversity perspective}
    \vspace{-1.5mm}
    As shown in Figure~\ref{fig:Accuracy Saturation}, naively prompting GPT-3.5-Turbo for answer augmentation leads to a clear accuracy saturation. After accuracy saturation, increasing the AnsAug data only yields a limited performance gain. For instance, using 80K answer augmentation data to train a LLaMA-2 7B model leads to a 59.6\% accuracy, adding new 20K AnsAug data would only take 0.1\% performance gain. This is due to the homogeneity of the additional samples, contributing to a diversity gain of only 0.05 (shown in Figure~\ref{fig:DiversityGains}). In comparison, adding the same amount of data generated by question bootstrapping leads to a significant performance boost, which is due to the noticeable diversity gain brought by question bootstrapping. As shown in Figure~\ref{fig:DiversityGains}, adding 20K data from Rephrasing, FOBAR, or SV takes an increasing diversity gain, thus causing a 0.4\%, 2.3\%, and 2.6\% accuracy gain, respectively. This experiment demonstrates a positive correlation (the Pearson coefficient is 0.972) between the diversity brought by the bootstrapping methods and accuracy. This is also aligned with the success of MetaMath, which is trained with the diverse MetaMathQA dataset including 4 kinds of data reflecting both the forward and backward reasoning paths.

\vspace{-2.00mm}
\subsection{Evaluating the Reversal Mathematical Capability}
 \vspace{-1.5mm}
The Reversal Curse~\citep{berglund2023reversal}, where LLMs trained from a sentence ``A is B" are not able to generalize to answer ``B is A", also aligns with the observation in this paper that LLMs lack backward mathematical reasoning ability. To evaluate the backward mathematical capability, we propose a GSM8K-Backward test set, including 1270 backward questions by using SV and FOBAR to augment the original GSM8K test set (as shown in Example~\ref{exmp:sv-example} and Example~\ref{exmp:fobar}). 
Figure~\ref{fig:reverse} shows the accuracy comparison of different 7B mathematical LLMs between the GSM8K and GSM8K-Backward datasets. As can be seen, existing LLMs struggle to solve mathematical problems in backward rationales and our MetaMath has a significant improvement on both datasets. Specifically, the ways where different LLMs solve the backward mathematical problem are illustrated through examples in Appendix~\ref{app:reversestudy}.

    \vspace{-2.00mm}
    \subsection{Reasoning Paths with Incorrect Answer Can Also Be Useful}
    \vspace{-1.5mm}

    \setlength{\columnsep}{9pt}

        \begin{wraptable}{r}[0cm]{0pt}
    \centering
    \footnotesize
    \setlength{\tabcolsep}{2.6pt}
	\renewcommand{\arraystretch}{1.15}
    \begin{tabular}{c|c}
     \specialrule{0em}{0pt}{-14pt}
        Data & Accuracy \\\shline
        GSM8K~\citep{cobbe2021training}  &  41.6  \\
        Incorrect Answers   & 43.6 \\
        Correct Answers  & \textbf{52.2}  \\
        \specialrule{0em}{0pt}{-5pt}
    \end{tabular}
    \vspace{-0.5cm}
    \caption{\footnotesize Testing accuracy on GSM8K of {LLaMA-2-7B} trained on different data.}
    \label{table:incorrect} 
    \vspace{-0.25cm}
    \end{wraptable}
    
    We conduct experiments on GSM8K using {LLaMA-2-7B} to study whether the answer augmentation samples with incorrect answers are helpful for finetuning the LLM. We randomly choose 7,473 reasoning paths with incorrect answers from the generated answers, and we ensure that the size is the same as that of the original training set. From Table~\ref{table:incorrect}, we observe that the model finetuned on the augmented data with incorrect answers is still better than SFT, which is counter-intuitive. We hypothesize that although the final answer is incorrect, some intermediate reasoning steps are correct (see Example \ref{exmp:incorrect}). These reasoning steps can still be useful supervision signals. Our results are also aligned with~\citep{lightman2023let}, where they discover the importance of intermediate process supervision for reasoning.
    
    

    
    \begin{exmp}{A Reasoning Path with Incorrect Answer}{incorrect}\small
    \textbf{Question:} Tonya is in a hamburger eating contest. Each hamburger is 4 ounces. Last year the winner ate 84 ounces. How many hamburgers does she have to eat to beat last year's winner? {\color{red3}(Ground-truth answer is $22$)}
    
    \textbf{Generated Answer:} If each hamburger is 4 ounces and last year's winner ate 84 ounces, then Tonya needs to eat 84/4 = 21 hamburgers to beat last year's winner. The answer is: 21
    \end{exmp}

\vspace{2mm}  
\subsection{More Data is not Always Better}
\vspace{-1mm}
    
There are also previous works that augment mathematical reasoning data for fine-tuning~\citep{luo2023wizardmath,yuan2023scaling}. An interesting question is whether combining existing augmented datasets with our {MetaMathQA} can improve the overall mathematical problem-solving performance. We select the RFT~\citep{yuan2023scaling} dataset as the external dataset. Figure~\ref{fig:Less is More} shows that merging the RFT data into {MetaMathQA} actually hurts the performance,
indicating that the RFT data may not be beneficial to {MetaMath}. Such a phenomenon is consistently observed in the {MetaMathQA} dataset under different sizes (from 20K to 100K), and the added RFT dataset is about 47K. The performance drop implies that more augmented data does not always help the generalization. 

\vspace{2.5mm}
\begin{exmp}{Case Study in Test Set of GSM8K}{case-study}\small
\textbf{Question:}
Darrell and Allen's ages are in the ratio of 7:11, If their total age now is 162, calculate Allen's age 10 years from now. {\color{red3}(Ground-truth answer is $109$)}

\textbf{SFT Answer:}
The total ratio representing their ages is 7+11=$<<$7+11=18$>>$18. The fraction representing Darrell's age is 7/18, and since their total age now is 162, Darrell's age is 7/18*162 = $<<$7/18*162=60$>>$60. The fraction representing Allen's age is 11/18, and since their total age now is 162, Allen's age is 11/18*162 = $<<$11/18*162=84$>>$84. Ten years from now, Allen's age will be 84 + 10 = $<<$84+10=94$>>$94.

\textbf{RFT Answer:}
The total ratio representing Darrell and Allen's ages is 7+11 = $<<$7+11=18$>>$18. The fraction representing Darrell's age from the total ratio is 7/18, and since their total age is 162, Darrell is 7/18*162 = 100 years old. Allen's age, from the total ratio, is 11/18, and since Darrell is 100, Allen is 11/18*109 = $<<$11/18*100=66$>>$66 years old. Ten years from now, Allen will be 66 + 10 = 76 years old.

\textbf{WizardMath Answer:}
We know Darrell and Allen's ages are in the ratio of 7:11, which means the total ratio representing their ages is 7+11 = 18. If their total age now is 162, we can calculate Darrell's age by using the ratio. Specifically, Darrell's age is 7/18 of 162, which is 7/18*162 = 49 years old.  ...  The answer is: 76.

\textbf{Our Answer:}
The ratio of Darrell's age to Allen's age is 7:11. Let's assume Darrell's age is 7x and Allen's age is 11x. The total age of Darrell and Allen is 7x + 11x = 18x. We are given that the total age is 162, so 18x = 162. Dividing both sides by 18, we get x = 9. Therefore, Allen's age is 11x = 11 * 9 = 99. 10 years from now, Allen's age will be 99 + 10 = 109.
\end{exmp}

\vspace{2mm}
\subsection{Error Analysis}
\vspace{-1mm}

We have demonstrated that -- across multiple scales -- our MetaMath models can achieve stellar problem-solving performance. Yet, it is important to consider the characteristics of problems that induce errors in MetaMath and existing open-source mathematical models. In particular, we consider the relationship between question length and model performance. To investigate, we divide the GSM8K test set into three equally-sized subsets based on the different lengths of questions and calculate the accuracy of the models over each subset. We find in Figure~\ref{fig:erroranalysis} that, MetaMath and related methods struggle under longer questions. However, excitingly, MetaMath always obtains superior performance. We see the study of improving model performance with longer question lengths -- for instance, by further augmenting the MetaMathQA dataset -- as ripe grounds for future work. 
\vspace{-.5mm}
	

\section{Concluding Remarks}
\label{sec:conclusion}

 \vspace{-1mm}
 In this paper, we focus on improving the mathematical problem-solving abilities of open-source LLMs. By bootstrapping mathematical questions on GSM8K and MATH, we present a high-quality and diverse dataset {MetaMathQA}, involving forward reasoning and backward reasoning samples. Our family of LLMs finetuned on {MetaMathQA}, called {MetaMath}, have achieved state-of-the-art on mathematical benchmarks among all open-source LLMs. Remarkably, {MetaMath-7B} reaches $66.5\%$ on GSM8K and $19.8\%$ on MATH, surpassing previous open-source LLMs by a significant margin. Our work further emphasizes the importance of the characteristics of the training data on boosting LLM problem-solving capabilities.

\newpage

\section*{Acknowledgement}
The authors would like to sincerely thank Katherine M. Collins from University of Cambridge for her valuable insights and suggestions.

This work was supported by NSFC key grant 62136005,
NSFC general grant 62076118, and Shenzhen fundamental research program JCYJ20210324105000003. This research was supported in part by the Research Grants Council
of the Hong Kong Special Administrative Region (Grants
16200021 and 16202523). AW acknowledges support from a Turing AI Fellowship under grant EP/V025279/1, and the Leverhulme Trust via CFI. 
	
 
 \clearpage
 \newpage

\end{document}