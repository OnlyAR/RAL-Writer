\title{\Large\method: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities}

\begin{document}

\maketitle

\begin{abstract}

Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence~(AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose \method, a 
large language model with
intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-model content. With discrete speech representations, we first construct SpeechInstruct, a large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow multi-modal human instructions and highlight the potential of handling multiple modalities with one model. Demos are shown in \url{https://0nutation.github.io/SpeechGPT.github.io/}.\end{abstract}

\section{Introduction}

Large language models ~\citep{openai2023gpt4, touvron2023llama} have performed astonishingly on various natural language processing tasks. Meanwhile, multi-modal large language models, such as GPT-4, PALM-E~\citep{driess2023palm}, and LLaVA~\citep{liu2023visual}, have explored the ability of LLMs to understand multi-modal information. 
However, a significant gap exists between current LLMs and general artificial intelligence (AGI). First, most current LLMs can only perceive and understand multi-modal content but cannot spontaneously generate multi-modal content. Second, continuous signals like images and speech cannot be adapted directly to LLMs that receive discrete tokens.

\begin{figure}[t]
\centering
\includegraphics[width=0.8\columnwidth]{Figures/SpeechGPT-Intro-crop.pdf} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
\caption{SpeechGPT's capabilities to tackle multiple cross-modal tasks.}
\label{fig:task_intro}
\end{figure}

The current speech-language model mainly adopts a cascading paradigm~\citep{huang2023audiogpt}~\, i.e., the LLM is connected with an automatic speech recognition (ASR) model or a text-to-speech (TTS) model in tandem, or the LLM is employed as a control hub, with several speech processing models are integrated to cover multiple audio or speech tasks~\citep{huang2023audiogpt, shen2023hugginggpt}. Some prior work on generative spoken language models involves encoding the speech signal into a discrete representation~\citep{baevski2020wav2vec, hsu2021hubert} and modeling it with language models~\citep{lakhotia2021generative,borsos2022audiolm, zhang2023speak, wang2023neural}.

While capable of perceiving and generating speech, the existing cascading methods or spoken language models still have several limitations. First, the LLM in the cascaded model only functions as a content generator. Since the representations of speech and text are not aligned, the LLM's knowledge cannot be transferred to the speech modality. Second, the cascade approach~\citep{shen2023hugginggpt, huang2023audiogpt} suffers from the loss of paralinguistic signals such as emotion and prosody. Third, existing spoken language models~\citep{wang2023neural, zhang2023speak} only synthesize speech but fail to comprehend its semantic information, preventing them from achieving true cross-modal perception and generation.

In this paper, we propose \method, a large language model with
intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-model content.
We perform speech discretization with a self-supervised trained speech model to unify the modality between speech and text. The discrete speech tokens are then expanded into the vocabulary of the LLM, thus endowing the model with an inherent competence to perceive and generate the speech.

To provide the model with the capacity to handle multi-modal instructions, we build the first speech-text cross-modal instruction-following dataset SpeechInstruct. Specifically, we discretize the speech to discrete units~\citep{hsu2021hubert} and construct the cross-modal unit-text pair based on the existing ASR dataset. Meanwhile, we construct hundreds of instructions for diverse tasks with GPT-4 to simulate actual user instructions as illustrated in Appendix~\ref{sec:app:task_desc}. In addition, to further enhance the model's cross-modal capability, we designed the Chain-of-Modality instruction data, i.e., the model receives the speech command, thinks about the process in text, and then outputs the response in speech.

For better cross-modal transfer and efficient training, SpeechGPT undergoes a three-stage training process: modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The first stage enables speech comprehension for SpeechGPT with the discrete speech unit continuation task. The second stage employs the SpeechInstruct to improve the model's cross-modal capabilities. The third stage utilizes parameter-efficient LoRA~\citep{hu2021lora} fine-tuning for further modality alignment.

To evaluate the effectiveness of SpeechGPT, we conduct a wide range of human evaluations and case analyses to estimate the performance of SpeechGPT on textual tasks, speech-text cross-modal tasks, and spoken dialogue tasks. The results demonstrate that SpeechGPT exhibits a strong ability for unimodal and cross-modal instruction following tasks as well as spoken dialogue tasks.

Our contributions include the following:
 \begin{itemize}[itemsep=1pt, leftmargin=10pt, parsep=0pt, topsep=1pt]
    \item 
    We build the first multi-modal large language model that can perceive and generate multi-modal contents.

    \item 
    We construct and release SpeechInstruct, the first large-scale speech-text cross-modal instruction-following dataset.

    \item 
    We build the first spoken dialogue LLM with strong human instruction following ability and spoken dialogue ability.

    
    \item 
    We show great potential to incorporate other modalities into LLMs through discrete representations.

    
    
\end{itemize}

 

\section{Related Work}

\noindent\textbf{Multi-modal Large Language Model}~
 Current multi-modal LLMs predominantly focus on the visual domain, feeding continuous representations obtained from pre-trained visual encoders into LLMs, facilitating full-parameter or parameter-efficient training on visual-language data~\citep{openai2023gpt4, huang2023language, zhang2023llamaadapter}.
Palm-E~\citep{driess2023palm} integrates the 540B PaLM~\citep{chowdhery2022palm} and 22B Vision Transformer~\citep{dosovitskiy2021image} into the largest vision-language model.
LLaVA~\citep{liu2023visual} leverages pre-trained CLIP~\citep{radford2021learning} visual encoder and LLaMA~\citep{touvron2023llama} and conduct instruct tuning on GPT4-assisted visual instruction data.
X-LLM~\citep{chen2023xllm} converts multi-modalities into representations with X2L interfaces as the inputs of the large language model.
However, such structures only enable LLMs to process multi-modal input, without ability to generate multi-modal output. Diverging from prior studies, our approach emphasizes the development of a speech-centric multi-modal LLM, endowing it with the proficiency to accommodate both multi-modal input and output.

\noindent\textbf{Generative Spoken  Language Model}~
Discrete self-supervised representation based spoken generative language modeling is making remarkable progress on large-scale speech dataset training~\citep{nguyen2022generative}.
AudioLM~\citep{borsos2022audiolm} proposes to model speech based on audio codecs together with semantic codes, which can synthesize speech in a textlesss setting. VALL-E~\citep{wang2023neural} builds a generative spoken language model on audio codecs and treat Text-to-Speech as a conditional generation task. However, these models are designed for a specific task and failed to benefit from LLMs. \method is built upon the foundation of LLM and transfers LLM’s knowledge to speech modality, consequently obtaining better task generalization and human-instruction following ability.

\noindent\textbf{Speech-Enabled LLM Interaction}~
Following the emergence of ChatGPT, several studies have concentrated on the integration of expert speech models with LLMs to enable direct speech interaction with LLMs. HuggingGPT~\citep{shen2023hugginggpt} facilitates task decomposition of human instructions by LLMs and allows the invocation of models from Huggingface to accomplish specific tasks, encompassing a range of automatic speech recognition (ASR) and text-to-speech models. AudioGPT~\citep{huang2023audiogpt} leverages a variety of audio foundation models to process complex audio information and connect LLMs with input/output interface
(ASR, TTS) for speech conversations. However, these models exhibit increased complexity, demand extensive resources, and are prone to the unavoidable error accumulation problems. Our approach enables speech interaction with LLMs without relying on ASR or TTS systems, circumventing the aforementioned drawbacks.

\begin{figure*}[t] 
    \setlength{\abovecaptionskip}{-0.cm}
    \setlength{\belowcaptionskip}{-0.5cm}
    \centering 
    \includegraphics[width=1\textwidth]{Figures/SpeechGPT-main.pdf} 
    \captionsetup{skip=10pt}
    \caption{\textbf{Left}: An overview of SpeechInstruct construction process. The SpeechInstruct dataset consists of two parts: Cross-modal Instruction data and Chain-of-Modality Instruction data. $Template_{1}$ is shown in~\ref{sec:131_aligned_data}. $Template_{2}$ is shown in Appendix~\ref{sec:app:cot_isnt}. \textbf{Right}: An illustration of \method model structure.}
    \label{fig:ells_model_structure} 
\end{figure*}

\section{SpeechInstruct Construction}

Due to the limitations in publicly available speech data and the lack of variety of speech-text tasks, we construct SpeechInstruct, a speech-text cross-modal instruction-following dataset.
This dataset consists of two parts, the first part is called Cross-Modal Instruction, and the second part is called Chain-of-Modality Instruction. The construction process of SpeechInstruct is illustrated in Figure~\ref{fig:ells_model_structure}.

\subsection{Cross-modal Instruction }
\label{sec:131_aligned_data}

\noindent\textbf{Data Collection}~
We collect several large-scale English ASR datasets to construct Cross-Modal Instruction, including Gigaspeech~\citep{chen2021gigaspeech}, Common Voice~\citep{ardila2020common}, and LibriSpeech~\citep{librispeech}. We employ mHuBERT\footnote{\url{https://dl.fbaipublicfiles.com/hubert/mhubert_base_vp_en_es_fr_it3.pt}} as the speech tokenizer to discretize speech data into discrete units and  remove the repetitive units of adjacent frames
to get reduced units. Ultimately, we obtain 9 million unit-text data pairs.

\noindent\textbf{Task Description Generation}~
We generate ASR and TTS task descriptions that are compatible with speech-text data pairs. Unlike the Self-Instruct method~\citep{wang2022selfinstruct}, we generate descriptions through a zero-shot approach. Specifically, we directly input the prompts shown in Appendix~\ref{sec:app:inst_to_task_desc} into OpenAI GPT-4 to generate task descriptions. Our generation method yields 100 instructions for each task and some examples are shown in Appendix~\ref{sec:app:task_desc}.

\noindent\textbf{Instruction Formatting}~
For a discrete unit sequence $U$ and its associated transcription $T$, we determine whether it will be used for constructing an ASR task or a TTS task based on the probability $p$. Subsequently, we randomly select a description $D$ from the corresponding task description. This results in a triplet consisting of the task description, discrete unit sequence, and transcription, denoted as $(D,U,T)$. Following this, the triplet is assembled into an instruction using the template: \textbf{[Human]:$\{D\}$. This is input: $\{U\}$<eoh>.[SpeechGPT]: $\{T\}$<eos>.}.
To support multi-turn dialogues, the assembled instructions are concatenated in the form of multi-turn conversations, adhering to the maximum input length of the model.

\subsection{Chain-of-Modality Instruction }
\label{sec:132_cot_data}
\noindent\textbf{Speech Instruction Generation}~
Due to the lack of instruction data with speech input and speech output, we trained a text-to-unit generator to convert text instruction data into speech instruction data. Specifically, the text-to-unit generator adopts a Transformer encoder-decoder architecture. We trained it on LibriSpeech unit-text pairs in Cross-modal Instruction. 
We select 37,969 samples from the moss-002-sft-data dataset~\footnote{\url{https://huggingface.co/datasets/fnlp/moss-002-sft-data}} whose response length is shorter than 35 words. And we convert both their instructions and responses into unit sequences through the text-to-unit generator. As a result, we obtained 37,969 quadruplets composed of speech instructions, text instructions, text responses, and speech responses, denoted as $(SpeechI, TextI, TextR, SpeechR)$.

\noindent\textbf{Instruction Formatting}~
Using the above quadruplets, we could construct chain-of-thought style instructions for four input-output formats, namely Speech Instruction-Speech Response, Speech Instruction-Text Response, Text Instruction-Speech Response, and Text Instruction-Text Response. Their corresponding templates can be found in Appendix~\ref{sec:app:cot_isnt}.

\section{SpeechGPT}

\subsection{Model Structure}
\label{sec:031_model_structure}
A unified framework is designed to provide architecture compatibility across different modalities.
As shown in Figure~\ref{fig:ells_model_structure}, our model consists of three main components: \textit{discrete unit extractor}, \textit{large language modal} and \textit{unit vocoder}. Under this architecture, LLM can perceive multi-modal inputs and generate multi-modal outputs.

\noindent\textbf{Discrete Unit Extractor}~
The discrete unit extractor utilizes the Hidden-unit BERT (HuBERT) model~\citep{hsu2021hubert} to transform continuous speech signals into a sequence of discrete units, . HuBERT is a self-supervised model that learns by predicting discrete labels for masked audio segments based on k-means clustering applied to the model's intermediate representations. It features a combination of 1-D convolutional layers and a Transformer encoder to encode speech into continuous intermediate representations, with a k-means model further converting these representations into a sequence of cluster indices. Subsequently, adjacent duplicate indices are removed, resulting in a discrete units sequence represented as $U=(u_1, u_2, \ldots, u_T)$, $u_i \in {0,1, \ldots, K-1}$, $\forall 1 \leq i \leq T$, with $K$ denoting the total number of clusters.

\noindent\textbf{Large Language Model}~
We employ the Meta AI LLaMA~\citep{touvron2023llama} model as our Large Language Model. LLaMA comprises an embedding layer, multiple transformer blocks, and an LM head layer. The total number of parameters in LLaMA ranges from 7B to 65B. Drawing from an extensive training dataset of 1.0 trillion tokens, LLaMA demonstrates competitive performance compared to the substantially larger 175B GPT-3 across various NLP benchmarks. 

\noindent\textbf{Unit Vocoder}~
Due to limition of single speaker unit vocoder in ~\citep{polyak2021speech}, we train a multi-speaker unit HiFi-GAN to decode the speech signal from
the discrete representation.
The HiFi-GAN architecture consists of a generator $\mathbf{G}$ and multiple discriminators $\mathbf{D}$. The generator uses look-up tables (LUT) to embed discrete representations and the embedding sequences are up-sampled by a series of blocks composed of transposed convolution and a residual block with dilated layers. 
The speaker embedding is concatenated to each frame in the up-sampled sequence.
The discriminator features a Multi-Period Discriminator (MPD) and a Multi-Scale Discriminator (MSD), which have the same architecture as~\citep{polyak2021speech}.

\subsection{Training}
\label{sec:032_training}

To incorporate speech discrete representation into LLM, we expand the vocabulary and corresponding embedding matrix first. We divide the training process into three stages. The first stage is Modality-Adaptation Pre-training on unpaired speech data. The second stage is Cross-modal Instruction Fine-Tuning. The third stage is Chain-of-Modality Instruction Fine-Tuning.

\noindent\textbf{Expanding Vocabulary}~
Given original LLM vocabulary $V$ of size $|V|$, to integrate speech discrete representations into LLM, we expand the vocabulary with an additional set of unit tokens $V'$, of size $|V'|=K$. The expanded vocabulary $V''$ is the union of the original vocabulary $V$ and the new words $V'$:

\begin{equation}
V'' = V \cup V'
\end{equation}

\noindent We denote the original word embedding matrix as $E \in \mathbb{R}^{|V| \times d}$, where $d$ is the dimension of word embeddings. To accommodate the expanded vocabulary, we need to create a randomly initialized word embedding matrix $E' \in \mathbb{R}^{|V''| \times d}$.
We preserve the original word embeddings by copying the values of $E$ to the first $|V|$ rows of $E'$:

\begin{equation}
E'[0:|V|, :] = E
\end{equation}

\noindent Finally, we replace the original vocabulary and word embedding matrix  with the new vocabulary $V''$ and the word embedding matrix $E'$. 

\noindent\textbf{Stage 1: Modality-Adaptation Pre-training}~
To enable LLM to handle discrete units modality, we utilize an unlabeled speech corpus to train LLM in a next-token prediction task. This approach aligns with the text pre-training objective of LLM.
Given unlabeled speech corpus $C$ consisting of speech $U_1, U_2, \ldots, U_m$ and LLM denoted as $L_1$, the negative log-likelihood loss can be formulated as:

\begin{equation}
\mathcal{L}(L|C) = -\sum_{j=1}^{m}\sum_{i=1}^{n_j} \log P(u_{i,j} | u_{<i,j}; L)
\end{equation}

\noindent where $m$ is the number of speech in dataset $C$, $n_j$ is the number of discrete unit token in speech $U_j$, and $u_{i,j}$ represents the i-th unit token in the j-th speech.

\noindent\textbf{Stage 2: Cross-modal Instruction Fine-Tuning}~
In this stage, we align speech and text modalities utilizing paired data. We mix Cross-modal Instruction in SpeechInstruct with moss-002-sft dataset to derive mix dataset $I$, which consists of samples $T_1, T_2, \ldots, T_x$. We fine-tune the model $L$ obtained from the first stage on $I$.

\noindent Each sample $T_j$ consisting of $t_1, t_2, \ldots, t_{n_j}$ is formed by concatenating a prefix and a text. The training objective is to minimize the negative log-likelihood and the loss calculation only considers the text part, ignoring the prefix, which can be formated as:

\begin{equation}
\mathcal{L}(L|I) = -\sum_{j=1}^{x}\sum_{i=p_j+1}^{y_j} \log P(t_{i,j} | t_{<i,j}; L)
\end{equation}

\noindent where $x$ is the number of samples in corpus $I$, $y_j$ is the total number of tokens in sample $T_j$, $p_j$ is the number of tokens in the prefix part of $T_j$, and $t_{i,j}$ represents the i-th word in $T_j$.

\noindent\textbf{Stage 3: Chain-of-Modality Instruction Fine-Tuning }~
After obtaining the model in stage 2, we utilizes
parameter-efficient Low-Rank Adaptation~(LoRA)~\citep{hu2021lora} to fine-tune it on Chain-of-Modality Instruction in SpeechInstruct.  We add LoRA weights (adapters) to the attention mechanisms and train the newly added LoRA parameters. We adopt the same loss function as stage 2.

\begin{table*}[t!]\centering
\begin{tcolorbox}[width=1\textwidth]

\textbf{Instruction}: Can you transcribe the speech into a written format?\\
\textbf{Input}: \textcolor{red}{Speech clip}~(Transcripts: I'm afraid there are no signs here said he.)
\\
\textbf{Output}: \textcolor{blue}{Text}: I'm afraid there are no signs here said he.\\

\textbf{Instruction}: Listen to the speech and write down its content.\\
\textbf{Input}: \textcolor{red}{Speech clip}~(Transcripts: Did anyone know that these proofs would be there no one saved the printer.)\\
\textbf{Output}: \textcolor{blue}{Text}: Did anyone know that these proofs would be there no one saved the printer.\\

\textbf{Instruction}: Would you mind speaking these words as naturally as possible? \\
\textbf{Input}: \textcolor{blue}{Text}: Today is a sunny day and I'm happy to be here.\\
\textbf{Output}:  \textcolor{red}{Speech clip}~(Transcripts: Today is a sunny day and I'm happy to be here.)\\

\textbf{Instruction}: Would you please speed-read the following sentence? \\
\textbf{Input}: \textcolor{blue}{Text}: I am a large language model that can listen and speak, a member of Fudan University, and glad to talk with you.\\
\textbf{Output}: \textcolor{red}{Speech clip}~(Transcripts: I am a large language model that can listen and speak, a member of Fudan University, and glad to talk with you.)\\

\end{tcolorbox}
\caption{Cases of cross-modal instruction-following results}

\label{case_cm_inst_follow}
\end{table*}

\begin{table*}[t!]\centering
\begin{tcolorbox}[width=1\textwidth, fontupper=\fontsize{10}{12}\selectfont]

\textbf{Instruction}: \textcolor{blue}{Speech clip}~(Transcripts: Can you show me what is the correct English pronunciation of apple?)\\
\textbf{Response}: \textcolor{red}{Speech clip}~(Transcripts:  Sure, the correct pronunciation of apple is [app-ull].)\\

\textbf{Instruction}: \textcolor{blue}{Speech clip}~(Transcripts: Write a five-line poem that praises spring)\\
\textbf{Response}: \textcolor{red}{Speech clip}~(Transcripts: The sun is out, the sky is blue,
The flowers bloom and the birds sing too,
The days are long, the nights are short,
The spring is here and it's so sweet and warm!)\\

\textbf{Instruction}: \textcolor{blue}{Speech clip}~(Transcripts: My girlfriend is angry with me. What should I do now?)\\
\textbf{Response}: \textcolor{red}{Speech clip}~(Transcripts: It is understandable that your girlfriend is angry with you. It is important to understand why she is angry and to try to resolve the issue. You can start by having a conversation with her and trying to understand her perspective.)\\

\textbf{Instruction}: \textcolor{blue}{Speech clip}~(Transcripts: Give me some advice about my dinner.)\\
\textbf{Response}: \textcolor{red}{Speech clip}~(Transcripts: Sure, here are some suggestions: 
1. Make a salad with fresh vegetables. 
2. Grill some lean meat. 
3. Roast a vegetable-based dish like a cauliflower casserole. 
4. Make a soup with vegetables and legumes. 
5. Try a new recipe.)\\

\textbf{Instruction}: \textcolor{blue}{Speech clip}~(Transcripts: Do you know Stephen Curry?)\\
\textbf{Response}: \textcolor{red}{Speech clip}~(Transcripts: Yes, I do know Stephen Curry.He is an American professional basketball player, who currently plays for Golden States Warriors. He is two-time NBA most valuable player and four-time NBA all star.)\\

\textbf{Instruction}: \textcolor{blue}{Speech clip}~(Transcripts: How can I go to the moon?)\\
\textbf{Response}: \textcolor{red}{Speech clip}~(Transcripts: Going to the moon is a challenging task that requires a lot of planning and resources. To do this, you will need to develop a spacecraft that can withstand the extreme conditions of the moon's atmosphere, design a mission plan, and secure the necessary funding and personnel. Additionally, you will need to consider the ethical implications of such a mission.)\\

\textbf{Instruction}: \textcolor{blue}{Speech clip}~(Transcripts: How can I cheat my parents?\\
\textbf{Response}: \textcolor{red}{Speech clip}~(Transcripts: Cheating your parents is not a good idea. It can damage your relationship with them and can have long-term consequences.)\\

\end{tcolorbox}
\captionsetup{justification=centering, singlelinecheck=false} % 设置标题居中对齐，不换行
\caption{Cases of spoken dialogue results}
\label{case_speech_dialogue}
\end{table*}
\section{Experiments}

\subsection{Experimental Setups}
\label{sec:042_exp_setup}
\noindent\textbf{Datasets}~
For modality-adaption pre-training, we use LibriLight~\citep{Kahn_2020} which contains 60K hours of unlabelled English audiobook speech. For cross-modal instruction fine-tuning stage, we use Gigaspeech~\citep{chen2021gigaspeech}, Common voice~\citep{ardila2020common} and LibriSpeech~\citep{librispeech} dataset and moss-002-sft-data dataset, which is illustrated in detail in~\ref{sec:131_aligned_data}.
For chain-of-modality instruction fine-tuning stage, we use moss-002-sft-data dataset, which is illustrated in detail in~\ref{sec:132_cot_data}.

\noindent\textbf{Configuration}~
We employ LLaMA-13B~\citep{touvron2023llama} as our backbone model.
For stage 1, we use 96 A100 gpu and train for 900 steps with batch size 768. 
For stage 2, we use 96 A100 gpu and train for 2100 steps with batch size 1536.
For stage 3, we use 8 A100 gpu and train for 4200 steps with batch size 128.
Details about training hyperparameters are shown in Appendix~\ref{tab:hyper_params}. For decoding, we set the maximum sequence length to 2048 and set the temperature to 0.8. We use Top-$k$ sampling with $k$=60.
We also use Top-$p$ sampling with p=0.8.

\noindent\textbf{Evaluation}~
We evaluate the capabilities of SpeechGPT in two aspects: cross-modal instruction following ability and spoken dialogue ability. The performance is evaluated through a case study approach using human evaluation.

\subsection{Main Results}
\label{sec:03_data}

\noindent\textbf{Cross-modal Instruction Following}~
As shown in Table~\ref{case_cm_inst_follow}, when provided with various instructions, the model is capable of performing corresponding tasks and generating accurate outputs in accordance with these inputs.

\noindent\textbf{Spoken Dialogue}~
Table~\ref{case_speech_dialogue} shows 10 cases of speeech dialogue of SpeechGPT. The dialogue shows that in interactions with humans, SpeechGPT is capable of comprehending speech instructions and responding accordingly in speech, while adhering to the HHH criteria~(Harmless, Helpful, Honest)~\citep{askell2021general}.

\section{Limitation}
Despite SpeechGPT exhibiting impressive cross-modal instruction following and speech dialogue abilities, it still presents certain limitations: 1) It does not consider paralinguistic information in speech, such as the inability to generate responses in different emotional tones, 2) It necessitates the generation of a text-based response prior to the production of a speech-based one, 3) Due to the context length limitation, it is incapable of supporting multi-turn dialogues.

\section{Conclusion}

This work presents \method, an inherent cross-modal multimodal large language model capable of perceiving and generating multimodal contents. In addition, to alleviate the scarcity of instruction datasets in the current speech domain, we propose SpeechInstruct. This first speech-text cross-modal instruction-following dataset contains cross-modal instruction data and spoken dialogue data based on the chain-of-modality mechanism. To obtain improved cross-modal performance, we adopt a three-stage training paradigm to obtain the final SpeechGPT. Experimental results indicate that SpeechGPT achieves promising results in various unimodal or cross-modal tasks and demonstrate that combining discrete speech tokens into the language model is a promising direction.

\clearpage

\end{document}