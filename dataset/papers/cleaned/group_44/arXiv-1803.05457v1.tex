\title{Think you have Solved Question Answering?\\
Try ARC, the AI2 Reasoning Challenge}

\begin{document}

\maketitle

\begin{abstract}
We present a new question set, text corpus, and baselines assembled to encourage AI research in
advanced question answering.  Together, these constitute the AI2 Reasoning
Challenge (\ASC), which requires far more powerful knowledge and
reasoning than previous challenges such as SQuAD or SNLI.
The \ASC\  question set is partitioned into a \Challenge~Set and an \Additional~Set, where the \Challenge~Set
contains only questions answered incorrectly by both
a retrieval-based algorithm and a word co-occurence algorithm.
The dataset contains only natural, grade-school science questions (authored for human tests),
and is the largest public-domain set of this kind (7,787 questions).
We test several baselines on the \Challenge~Set, including leading neural
models from the SQuAD and SNLI tasks, and find that none are
able to significantly outperform a random baseline, reflecting the
difficult nature of this task. We are also releasing the \ASC~Corpus, a corpus
of \BUSCSIZE~science sentences relevant to the task, and implementations
of the three neural baseline models tested. Can your model perform
better? We pose \ASC~as a challenge to the community.
\end{abstract}

\section{Introduction}

Datasets are increasingly driving progress in AI, resulting in impressive solutions
to several question-answering (QA) tasks \cite[e.g.,][]{rajpurkar2016squad,JoshiTriviaQA2017}.
However, many of these datasets focused on retrieval-style tasks, where
surface-level cues alone were usually sufficient to identify an answer.
This has not encouraged progress on questions requiring reasoning, use of commonsense
knowledge, or other advanced methods for deeper text comprehension.
The challenge presented here, called \ASC~(AI2
Reasoning Challenge), aims to address this limitation by posing questions
that are hard to answer with simple baselines.

The \ASC~Dataset consists of a collection of 7787 {\it natural science questions}, namely questions
authored for use on standardized tests. Standardized
tests have previously been proposed as a Grand Challenge for
AI \cite{darpa,aimagazine2016} as they involve a wide variety of
linguistic and inferential phenomena, have varying levels
of difficulty, and are measurable, motivating, and ambitious.
However, making this challenge a reality
is difficult, as such questions are difficult
obtain (most examination boards release only limited practice
tests to the public). For \ASC~we have addressed this 
through several months of extensive search and investigation.

In addition, to encourage focus on advanced phenomena, we have
partitioned \ASC~into a \Challenge~Set (2590 questions), containing questions answered
incorrectly by both a retrieval-based algorithm and a word co-occurrence
algorithm, and an \Additional~Set (5197 questions), containing the remainder.
For example, two typical challenge questions are:
\begin{quote}
  {\it Which property of a mineral can be determined just by looking at it? (A) luster {\bfit [correct]} (B) mass (C) weight (D) hardness} \\
  \\
 {\it A student riding a bicycle observes that it moves faster on a smooth road than on a rough road. This happens because the smooth road has (A) less gravity (B) more gravity (C) less friction {\bfit [correct]} (D) more friction}
\end{quote}
\vspace{1mm}
Both these questions are difficult to answer via simple retrieval or word correlation. For example, there are no Web sentences of the form ``luster can be determined by looking at something''; similarly, ``mineral'' is strongly correlated with ``hardness'' (an incorrect answer option). Rather, they require more advanced QA methods. We provide more example questions, and a catagorization of the knowledge and reasoning types that the questions appeal to, in Tables~\ref{knowledge-types} and~\ref{reasoning-types}.

To help the community to engage with this dataset, we are also releasing a science text corpus and two baseline neural models
as part of the \ASC~challenge:
\begin{enumerate}
\item The \ASC~Corpus, containing \BUSCSIZE~
  science-related sentences with knowledge relevant to \ASC.
  A sampled analysis suggests the corpus mentions knowledge relevant to 
   95\% of the \Challenge~questions. (Use of the Corpus for the Challenge is optional).
\item Three neural baseline models, DecompAttn, BiDAF, and DGEM, for QA. These are
multiple-choice QA adaptations of three neural models: the decomposable attention model
\cite{parikh2016decomposable}, a top performer on SNLI; Bidirectional Attention Flow \cite{Seo2016BidirectionalAF}, a top performer on SQuAD; and the decomposed graph entailment model, a top performer on SciTail~\cite{khot2018scitail}.
While these score well on the \Additional~Set, they are
unable to perform significantly better than a random baseline
on the \Challenge~Set, illustrating its challenging nature.
\end{enumerate}
This challenge differs from the Kaggle-hosted 2016 Allen AI Science Challenge \cite{schoenick2017moving}
in three important ways\footnote{
\ASC~includes the publically releasable subset of the Kaggle questions (about 60\% of the Kaggle set, making up 43\% of the \ASC~set).}. First, the creation of a \Challenge~partition
is to avoid scores being dominated by the performance of simple algorithms,
and thus encourage research on methods that the more difficult questions demand.
Second, we provide a science corpus along with the questions to help
get started (use of the corpus is optional, and systems are not
restricted to this corpus). Finally, the questions, corpus, and models
are all publically available.

The paper is organized as follows. We first discuss related work, and then
describe how the \ASC~dataset was collected and partitioned. We also provide an analysis
of the types of problems present in the \Challenge~Set. We then describe
the supporting \ASC~Corpus, and illustrate how it contains knowledge
relevant to \Challenge~questions. Finally, we describe several baselines
and their performance on \ASC. Most notably, although some baseline algorithms
perform well on the \Additional~Set (scoring up to 61\%), none are able to
perform significantly above random on the \Challenge~Set. We conclude by
posing \ASC~as a challenge to the community. The \ASC~Dataset, Corpus, Models,
and Leaderboard can be accessed at~\ascurl.

\section{Related Work}

There are numerous datasets available to drive progress in question-answering.
Earlier reading comprehension datasets, e.g., MCTest \cite{mctest}, SQuAD \cite{rajpurkar2016squad}, NewsQA \cite{trischler2016newsqa},
and CNN/DailyMail \cite{hermann2015teaching}, contained questions whose answers could
be determined from surface-level cues alone (i.e., answers were ``explicitly stated'').
TriviaQA \cite{JoshiTriviaQA2017}
broadened this task by providing several articles with a question, and
used questions authored independently of the articles. Again, though, the
questions were largely factoid-style, e.g., ``Who won the Nobel Peace Prize in 2009?''.
Although systems can now perform well on these datasets,
even matching human performance \cite{human-squad}, they can be easily
fooled \cite{jia2017adversarial}; the degree to which they truly understand
language or domain-specific concepts remains unclear. 

To push towards more complex QA tasks, one approach has been to
generate synthetic datasets, the most notable example being the bAbI dataset \cite{weston2015towards}.
bAbi was generated using a simple world simulator and language generator,
producing data for 20 different tasks. It has stimulated work on
use of memory network neural architectures \cite{weston2014memory},
supporting a form of multistep
reasoning where a neural memory propagates information from one step to another
\cite[e.g.][]{Henaff2016TrackingTW,Seo2017QueryReductionNF}.
However, its use of synthetic text
and a synthetic world limits the realism and difficulty of the task, with
many systems scoring a perfect 100\% on most tasks \cite[e.g.][]{weston2014memory}.
In general, a risk of using large synthetic QA datasets is that neural methods
are remarkably powerful at ``reverse-engineering'' the process by which a
dataset was generated, or picking up on its idiosyncrasies to excel at it,
without necessarily advancing language understanding or reasoning. 

More recently, \citet{welbl2017constructing} created the WikiHop dataset, containing questions
that appear to require more than one Wikipedia document to answer (``multihop questions'').
The dataset takes a step towards a more challenging task, but has several limitations:
questions are binary predicates (e.g., date\_of\_birth(``jeanne c. stein'',?X));
the intended inference is typically a simple two-step chain (commonly
a geographical substitution of a city for a country); and in many cases the
correct answer can be guessed from the passage, without requiring multi-hop
inference ($\sim$44\% of the answerable questions are single-hop, according to the authors \cite{welbl2017constructing}).

Datasets based on human standardized tests have also been used in AI
for several years \cite[e.g.][]{clark2016combining,seo2014diagram,fujita2014overview,strickland},
and as part of the NTCIR QALab \cite{ntcir13}, and for the 2016 Allen AI Science Challenge
described in the Introduction \cite{schoenick2017moving}. However, there
are two potentially significant challenges with these datasets. First, these datasets are often
small (e.g., hundreds of questions), due to the scarcity of public, real-world test data.
Second, because tests were designed for people rather than machines,
large portions of these tests can be easily solved by simple AI methods
\cite[e.g.][]{davis2016write,clark2016combining}. The result of this is
that scores become dominated by the performance of simple algorithms
(information retrieval, statistical correlations).
This then biases
research towards incrementally improving those algorithms, rather than
exploring the larger AI challenges that the more difficult questions demand.
Indeed, it is easy to mistake progress on these datasets as implying
equal progress on easy and hard questions, while in reality progress may be
heavily concentrated on easy questions alone \cite{Gururangan:2018},
leaving more difficult challenges unaddressed.
The \ASC~Dataset addresses both of these limitations.

\section{The \ASC~Dataset}

The \ASC~dataset consists of 7787 science questions, all non-diagram, multiple choice (typically 4-way multiple choice). They are drawn from a variety of sources,
and sorted into a \Challenge~Set of 2590 ``hard'' questions (those that both a retrieval
and a co-occurrence method fail to answer correctly) and an \Additional~Set of 5197 questions.
Table~\ref{numbers} summarizes the sizes of the train/dev/test partitions in the \ASC~dataset.
The question vocabulary uses 6329 distinct words (stemmed).

\begin{table}[htb]
\setlength{\tabcolsep}{8pt}
\setlength{\doublerulesep}{\arrayrulewidth}
\small
\centering
 \begin{tabular}{|l|rrr|} \hline
 \bigstrut[t] & {\bf \Challenge} & {\bf \Additional} & {\bf \hspace{3ex} Total} \\ \hline \hline
Train \bigstrut[t] & 1119 & 2251 & 3370 \\
Dev & 299 & 570 & 869 \\
Test & 1172 & 2376 & 3548 \\ \hline
\bigstrut[t] TOTAL & 2590 & 5197 & 7787 \\ \hline
 \end{tabular}
 \caption{Number of questions in the \ASC~partitions. \label{numbers}}
 \end{table}

\begin{table}[htb]
\setlength{\tabcolsep}{4pt}
\setlength{\doublerulesep}{\arrayrulewidth}
\small
\centering
\begin{tabular}{|c|rlrl|} \hline
{\bf Grade} \bigstrut[t] & \multicolumn{2}{c}{\bf \Challenge} &	\multicolumn{2}{c|}{\bf \Additional} \\
& \% & (\# qns) & \% & (\# qns) \\
\hline \hline
3 \bigstrut[t] &	3.6 & (94 qns) & 3.4 & (176 qns) \\
4 & 9 & (233) & 11.4  & (591) \\
5 & 19.5 & (506) & 21.2 & (1101) \\
6 & 3.2 & (84) & 3.4 & (179) \\
7 & 14.4 & (372) & 10.7 & (557) \\
8 & 41.4 & (1072) & 41.2 & (2139) \\
9 & 8.8 & (229) & 8.7 & (454) \\ \hline
 \end{tabular}
 \caption{Grade-level distribution of \ASC~questions \label{grade-levels}}
 \end{table}
 
 \eat{
\begin{table}[htb]
\setlength{\tabcolsep}{4pt}
\setlength{\doublerulesep}{\arrayrulewidth}
\small
\centering
\begin{tabular}{|c|rr@{\hskip 10pt}rr|} \hline
{\bf Grade} \bigstrut[t] & \multicolumn{2}{c}{\bf \Challenge} &	\multicolumn{2}{c|}{\bf \Additional} \\
& \# & \% & \# & \% \\
\hline \hline
3 \bigstrut[t] &	94&3.6\% &	176  & 3.4\%\\
4 &	233&9.0\% &	591 & 11.4\%\\
5 &	506&19.5\% &	1101 & 21.2\%\\
6 &	84&3.2\% &	179 & 3.4\%\\
7 &	372&14.4\% &	557 & 10.7\%\\
8 &	1072&41.4\% &	2139 & 41.2\%\\
9 &	229&8.8\% &	454 & 8.7\%\\ \hline
 \end{tabular}
 \caption{Grade-level distribution of \ASC~questions \label{grade-levels}}
 \end{table}
 }

Questions vary in their target student grade level (as assigned by the examiners who authored the questions), ranging from 3rd grade to 9th, i.e., students typically of
age 8 through 13 years. Table~\ref{grade-levels} shows a break-down of the set based on grade level with absolute counts(\#) and percentage(\%) of the \Challenge\ and \Additional\ set. In practice,
there is substantial overlap in difficulty among grade levels (also seen in the similar distribution of grade levels), as each grade level contains a mixture of
easy and difficult questions.

\begin{table}[htb]
\setlength{\tabcolsep}{4pt}
\setlength{\doublerulesep}{\arrayrulewidth}
\small
\centering
\begin{tabular}{|l|rr|} \hline
   & \multicolumn{2}{|c|}{\bf min / average / max} \\
  {\bf Property:} \bigstrut[t] & {\bf \Challenge} & {\bf \Additional} \\ \hline
Question (\# words) \bigstrut[t] & 2 / 22.3 / 128 & 3 / 19.4 / 118 \\
Question (\# sentences) & 1 / 1.8 / 11 & 1 / 1.6 / 9 \\
Answer option (\# words) \bigstrut[t] & 1 / 4.9 / 39 & 1 / 3.7 / 26 \\
\# answer options & 3 / 4.0 / 5 & 3 / 4.0 / 5 \\ \hline
 \end{tabular}
 \caption{Properties of the \ASC~Dataset \label{statistics}}
 \end{table}

Summary statistics of \ASC~are provided in Table~\ref{statistics}, showing questions and answers vary
considerably in length. Finally, Table~\ref{sources} in the Appendix lists the variety of sources the questions were drawn from.

\eat{
definition	20

negation	15
processes	15
comparison	15
experiments	12
situated	10
function/purpose	5
analogy	3
equational	3
spatial	2
}

\begin{table*}[htb]
\small
\centering
 \begin{tabular}{|>{\raggedright\arraybackslash}p{18ex}|p{80ex}|} \hline
{\bf Knowledge Type} \bigstrut[t] & \multicolumn{1}{c|}{\bf Example} \\ \hline
Definition \bigstrut[t] &
What is a worldwide increase in temperature called? (A) greenhouse effect (B) global warming (C) ozone depletion (D) solar heating \\
& \\
Basic Facts \& Properties &
Which element makes up most of the air we breathe? (A) carbon (B) nitrogen (C) oxygen (D) argon \\
& \\
Structure &
The crust, the mantle, and the core are structures of Earth. Which description is a feature of Earth's mantle? (A) contains fossil remains (B) consists of tectonic plates (C) is located at the center of Earth (D) has properties of both liquids and solids \\
& \\
Processes \& Causal & 
What is the first step of the process in the formation of sedimentary rocks? (A) erosion (B) deposition (C) compaction (D) cementation \\
 & \\
Teleology / Purpose
& What is the main function of the circulatory system? (1) secrete enzymes (2) digest proteins (3) produce hormones (4) transport materials \\
& \\
Algebraic &
If a red flowered plant (RR) is crossed with a white flowered plant (rr), what color will the offspring be? (A) 100\% pink (B) 100\% red (C) 50\% white, 50\% red (D) 100\% white \\
& \\
Experiments &
Scientists perform experiments to test hypotheses. How do scientists try to remain objective during experiments? (A) Scientists analyze all results. (B) Scientists use safety precautions. (C) Scientists conduct experiments once. (D) Scientists change at least two variables. \\
& \\
Spatial / Kinematic &
In studying layers of rock sediment, a geologist found an area where older rock was layered on top of younger rock. Which best explains how this occurred? (A) Earthquake activity folded the rock layers... \\ \hline
 \end{tabular}
\caption{Types of knowledge suggested by \ASC~\Challenge~Set questions}
\label{knowledge-types}
\end{table*}

\begin{table*}[htb]
\setlength{\tabcolsep}{4pt}
\setlength{\doublerulesep}{\arrayrulewidth}
\small
\centering
 \begin{tabular}{|>{\raggedright\arraybackslash}p{18ex}|p{80ex}|} \hline
{\bf Reasoning Type} \bigstrut[t] & \multicolumn{1}{c|}{\bf Example} \\ \hline \hline
Question logic \bigstrut[t] &
Which item below is {\bf not} made from a material grown in nature? (A) a cotton shirt (B) a wooden chair (C) a plastic spoon (D) a grass basket \\
& \\
Linguistic Matching 
& Which of the following best describes a mineral? (A) the main nutrient in all foods (B) a type of grain found in cereals (C) a natural substance that makes up rocks (D) the decomposed plant matter found in soil \\
& \\ 
Multihop Reasoning &
Which property of a mineral can be determined just by looking at it? (A) luster (B) mass (C) weight (D) hardness \\
\\
Comparison
& Compared to the Sun, a red star most likely has a greater (A) volume. (B) rate of rotation. (C) surface temperature. (D) number of orbiting planets \\
 & \\
Algebraic &
If a heterozygous smooth pea plant (Ss) is crossed with a homozygous smooth pea plant (SS), which are the possible genotypes the offspring could have? (A) only SS (B) only Ss (C) Ss or SS (D) ss or SS \\
& \\
Hypothetical / Counterfactual
& If the Sun were larger, what would most likely also have to be true for Earth to sustain life? (A) Earth would have to be further from the Sun. (B) Earth would have to be closer to the Sun. (C) Earth would have to be smaller. (D) Earth would have to be larger. \\
 & \\
Explanation / {Meta-reasoning} &
Why can steam be used to cook food? (A) Steam does work on objects. (B) Steam is a form of water. (C) Steam can transfer heat to cooler objects. (D) Steam is able to move through small spaces. \\
 & \\
Spatial / Kinematic &
Where will a sidewalk feel hottest on a warm, clear day? (A) Under a picnic table (B) In direct sunlight (C) Under a puddle (D) In the shade \\ 
& \\
Analogy &
Inside cells, special molecules carry messages from the membrane to the nucleus. Which body system uses a similar process? (A) endocrine system (B) lymphatic system (C) excretory system (D) integumentary system \\ \hline
 \end{tabular}
\caption{Types of reasoning suggested by \ASC~\Challenge~Set questions}
\label{reasoning-types}
\end{table*}

\subsection{Identifying Challenge Questions}

Operationally, we define a Challenge question as
one that is answered incorrectly by both of two baseline solvers, described below.
Although this only approximates the informal goal of it being a ``hard'' question,
this definition nevertheless serves as a practical and useful filter, as reflected
by the low scores of various baselines on the \Challenge~Set.

\subsubsection{Information Retrieval (IR) Solver.}
The first filter we apply is the IR solver from \citet{clark2016combining}, briefly described here for completeness. The IR solver uses the Waterloo corpus from \cite{clark2016combining}, a Web-based corpus of $5 \times 10^{10}$ tokens (280GB). The solver searches to see if the question $q$ along with an answer option is explicitly stated in the corpus, and returns the confidence that such a statement was found.
To do this, for each answer option $a_i$, it sends $q + a_i$ as a query to a search engine 
(we use Elasticsearch),
and returns the search engine's score for 
the top retrieved sentence $s$ where $s$ also has at least one non-stopword overlap with $q$, and at least one with $a_i$; this ensures $s$
has {\it some} relevance to both $q$ and $a_i$. This is repeated for all options $a_i$ to score them all, and the option with the highest score selected.

\subsubsection{The Pointwise Mutual Information (PMI) Solver. }
The second filter we apply is the PMI solver, also from \citet{clark2016combining}, again described here for completeness. This uses the same corpus as the IR solver, and
formalizes a way of computing and applying associational knowledge. Given a question $q$ and an answer option $a_i$, it uses PMI or pointwise mutual information \cite{church1989} to measure the strength of the associations between parts of $q$ and parts of $a_i$. Given a large corpus $C$, the PMI for two n-grams $x$ and $y$ is defined as
\begin{equation*}
\mathrm{PMI}(x,y) = \log \frac{p(x,y)}{p(x) p(y)}
\end{equation*}
Here $p(x,y)$ is the joint probability that $x$ and $y$ occur together in $C$, within a certain window of text (we use a 10 word window). The term $p(x) p(y)$, on the other hand, represents the probability with which $x$ and $y$ would occur together if they were statistically independent. The ratio of $p(x,y)$ to $p(x) p(y)$ is thus the ratio of the observed co-occurrence to the expected co-occurrence. The larger this ratio, the stronger the association between $x$ and $y$.

The solver extracts unigrams, bigrams, trigrams, and skip-bigrams from the question $q$ and each answer option $a_i$.
It outputs the answer with the largest average PMI, calculated over all pairs of question n-grams and answer option n-grams.

\subsection{The \Challenge~Set}

To illustrate the impact of using these algorithms as filters when defining the \Challenge~Set, consider the following example:
\begin{quote}
{\it Which property of air does a barometer measure? (A) speed (B) pressure {\bfit [correct]} (C) humidity (D) temperature}
\end{quote}
The question was excluded from the \Challenge~Set because it is correctly answered by (here) both the IR and PMI algorithms (note that it would have been excluded even if it was answered correctly by just one of the solvers). The IR algorithm finds multiple sentences supporting the correct answer, e.g.,
\begin{ite}
\item {\it Air pressure is measured with a barometer.}
\item {\it Air pressure will be measured with a barometer.}
\item {\it The aneroid barometer is an instrument that does not use liquid in measuring the pressure of the air.}
\item {\it A barometer measures the pressure of air molecules.}
\end{ite}
and similarly the PMI algorithm finds that ``barometer'' and ``pressure'' (and also ``air'' and ``pressure'') co-occur unusually frequently (high PMI) in its corpus.

\noindent
In contrast, consider the following question:
\begin{quote}
{\it Which property of a mineral can be determined just by looking at it? (A) luster {\bfit [correct]} (B) mass (C) weight (D) hardness}
\end{quote}
This is incorrectly answered by both algorithms: There are no corpus sentences similar to ``a material's luster can be determined by looking at it''. Similarly, ``mineral'' co-occurs unusually frequently with several incorrect answer options (e.g., mass, hardness), confusing the PMI algorithm. This question is thus part of the \Challenge~Set, as it appears to require a more advanced answering method.

\subsection{Question Types}

\ASC~questions appeal to both different styles of knowledge, and different styles of reasoning.
In Tables~\ref{knowledge-types} and~\ref{reasoning-types}, we enumerate the broad classes of each
that we observe in the \ASC~challenge, based on a sample of 100 questions. The relative
sizes of these categories are shown in Figures~\ref{knowledge-pie-chart} and~\ref{reasoning-pie-chart}.
These sizes are necessarily approximate, as it requires a subjective judgement about the {\it main} challenge exhibited by
different questions. Nevertheless, it helps to provide a rough atlas of the knowledge and reasoning
space underlying \ASC.

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{knowledge.png}
\caption{Relative sizes of different knowledge types suggested by the \ASC~\Challenge~Set. \label{knowledge-pie-chart}}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{reasoning.png}
\caption{Relative sizes of different reasoning types suggested by the \ASC~\Challenge~Set. \label{reasoning-pie-chart}}
\end{figure}

\section{The \ASC~Corpus}

In addition to the \ASC~question set, we are also releasing the \ASC~Corpus, a large corpus of
science-related sentences mined from the Web. It contains \BUSCSIZE~sentences (1.4GB of text),
and mentions much of the knowledge required to answer the \Challenge~Questions. Although
some of these mentions are indirect, and exploiting them is not trivial, it nevertheless
provides a starting point for attacking the \ASC~Challenge.
Note that use of the corpus is optional, and also that systems are not
restricted to this corpus.

The \ASC~Corpus was created by utilizing a major search engine to
run a large series of search queries relevant to science. Queries were automatically constructed 
by instantiating $\sim$100 hand-written templates for 80 science topics covered by US elementary and
middle schools, the subject areas of \ASC. For example, for the
topic ``celestial phenomena'', two templates ``[{\it astronomical-term}] astronomy'' and
``[{\it astronomical-term}] astrophysics'' were authored, and a list of (here, 360) terms for {\it astronomical-term} collected and used, 
resulting in 720 queries. The top several documents from each search were collected and de-duplicated, and then the content of these documents was stripped down to capture just the text in each document. The resulting text was then chunked into sentences. This was repeated for all templates.
Note that some templates were parameterized by more than one parameter.
From an informal analysis of a random sample of 805 documents that were collected, approximately 75\% were judged as ``science relevant''.
The corpus was then augmented with the AristoMini corpus\footnote{Also available separately at http://allenai.org/data.html}, an earlier corpus containing
dictionary definitions from Wiktionary, articles from Simple Wikipedia tagged as science,
and additional science sentences collected from the Web.
From a vocabulary analysis, 99.8\% of the \ASC~question vocabulary is mentioned in the \ASC~Corpus\footnote{
  Only 11 question words, mainly proper nouns, do not appear in the corpus: Daphne, Sooj, LaKeisha, Quickgrow, Hypergrow, CCGCAT, nonsnow,
  Quickgrow, Coaccretion, HZn, MgBr}.

In our baseline experiments discussed shortly, we find that if we change the corpus behind the
IR solver from Waterloo to the \ASC~Corpus, this changes its \Challenge~Set score from near zero (by definition,
\Challenge~questions are those that IR with the Waterloo corpus gets wrong) to a score similar to
random guessing\footnote{
  In fact slightly lower than random guessing, likely due to some corpus similarities where the
  same distractors in Waterloo were also present in the\ASC~Corpus.}.
However, from an informal, sampled analysis, we find that this is more
a limitation of the IR methodology than of the coverage of the \ASC~Corpus. The \ASC~Corpus,
in fact, appears to mention knowledge relevant to approximately 95\% of the \ASC~\Challenge~questions (from an
analysis of a random sample of questions), even if simple retrieval methods are not able to exploit it to correctly
answer the questions. For example, consider:
\begin{quote}
  {\it Scientists launch a rocket into space for a mission. Once the rocket escapes the gravitational pull of Earth, how will the mass and weight of the rocket be affected? \\
  (A) The mass and weight will change. \\
  (B) The mass and weight will stay the same.\\
  (C) The mass will stay the same, but the weight will change. \bfit{[correct]}\\
  (D) The mass will change, but the weight will stay the same.}
\end{quote}
While this particular scenario is of course not mentioned explicitly in the \ASC~Corpus, there are several statements about the relation between mass, weight, and gravity, for example:
\begin{ite}
\item {\it The main difference is that if you were to leave the Earth and go to the Moon, your weight would change but your mass would remain constant.}
\item {\it Astronauts in orbit experience weightlessness just like objects in the falling aircraft.}
\item {\it Weight is the force that something feels due to gravity: so the brick would have a much larger weight near the earth's surface than it does in deep space.}
\end{ite}
Such sentences provide evidence that weight but not mass will change when in space. Similarly, consider the question:
\begin{quote}
{\it Which factor will prompt an animal's fight-or-flight response? (A) population size (B) competition for food \bfit{[correct]} (C) seasonal temperatures (D) protection of the environment}
\end{quote}
While there are few sentences in the \ASC~Corpus that directly relate fight-or-flight with food competition, there are several that indicate competition is a kind of threat, e.g.,:
\begin{ite}
\item {\it Insurgency, cross-border firing, \bfit{competition} with livestock for grazing ground, ...all \bfit{threaten} the Markhor.}
\item {\it The main \bfit{threats} to the species include loss of habitat through deforestation, hunting for meat and \bfit{competition} with domestic livestock.}
\item {\it Habitat fragmentation \bfit{threatens} this species, as does possible \bfit{competition} with the introduced non-native Ring-necked Pheasant.}
\end{ite}
and similarly that threats can trigger the fight-or-flight response:
\begin{ite}
\item Whenever a reptile faces a \bfit{threat} they go into \bfit{fight or flight mode}...
\item {\it The sympathetics command your \bfit{fight or flight response}, and when they fire, your heart rate and your breathing speed up, ...preparing your body to face a \bfit{threat}...}
\end{ite}
\noindent
Finally, consider the question:
\begin{quote}
{\it Which of the following distinguishes the organisms in the kingdom Fungi from other eukaryotic organisms? (A) Fungi are unicellular. (B) Fungi reproduce sexually. (C) Fungi obtain nutrients by absorption. \bfit{[correct]} (D) Fungi make food through photosynthesis}
\end{quote}
Again, there is distributed evidence for the answer, including that (A) is incorrect:
\begin{ite}
\item {\it There are \bfit{unicellular} fungi and \bfit{multicellular} fungi.}
\end{ite}
that (C) is possible:
\begin{ite}
\item {\it Fungi are heterotrophs that \bfit{acquire their nutrients by absorption.}}
\end{ite}
and that (C) is distinct for other eukaryotic organisms:
\begin{ite}
\item {\it Animals are multicellular eukaryotes; they are chemosynthetic heterotrophs that \bfit{ingest} their food.}
\end{ite}
Although these descriptions of reasoning paths are informal, and clearly many others are possible, they illustrate
that the \ASC~Corpus mentions knowledge relevant to a question, even if no single sentence alone provides the answer.
Of course, this does not address the challenge of correctly identifying and reasoning with this knowledge, nor
the challenge of injecting unstated commonsense knowledge that may also be required. Rather, our claim is
that the Corpus contains substantial linguistic signal relevant to most of the \ASC~questions, and
is a useful starting point for corpus-based attacks on the Challenge.

\section{Baseline Performance}

\subsection{Baseline Systems}

We ran several baseline QA systems on the \Challenge~and \Additional~Sets, including two neural models, DecompAttn and BiDAF (details below), that have near state-of-the-art performance on
the well-known SNLI and SQuAD datasets respectively. We scored systems using the following scoring rubric: For each question, a system receives 1 point if it chooses the correct answer and $1/k$ if it reports a $k$-way tie (i.e., chooses multiple answers) that includes the correct answer. For a question set, the overall score of the system is the sum of the points it receives for all questions, divided by the number of questions and reported as a percentage. We report performance of the following systems:

\begin{enumerate}

\item \textit{IR (dataset definition).} IR method, described earlier. 

\item \textit{PMI (dataset definition).} PMI method, described earlier. 

\item \textit{Guess-all (``random'').} A na\"ive baseline that selects all answer options as equally valid, thereby scoring $1/k$ for each question with $k$ answer choices. A system that chooses a single answer at random will also converge to this score after enough trials.

\item \textit{IR (\ASC~Corpus).} The IR algorithm, rerun with the \ASC~Corpus. Note that changing the original corpus is expected to result in a different score, unless the two corpora are highly correlated. A corpus containing random strings, for instance, will have very low correlation with the original corpus and will result in a random-guessing score of around 25\%.

\item \textit{TableILP \cite{khashabi2016tableilp},} which performs matching and reasoning using a semi-structured knowledge base of science knowledge, expressed in tables.

\item \textit{TupleInference \cite{khot2017tupleinf},} which performs semi-structured matching of the question with retrieved sentences, where the structure consists of Open IE tuples.

\item \textit{DecompAttn, DGEM, and DGEM-OpenIE (Neural Entailment Models).} We adapted two neural entailment models, DecompAttn \cite{parikh2016decomposable} and DGEM \cite{khot2018scitail}, to the task of answering multiple-choice questions. The models were trained on an extended version of the SciTail dataset \cite{khot2018scitail}. To adapt these to multiple-choice QA, we first convert the question $q$ plus an answer option $a_{i}$ into a hypothesis sentence (or paragraph) $h_{i}$, use this as a search query to retrieve text sentences $t_{ij}$ from a corpus, then compute the entailment scores between $h_{i}$ and each $t_{ij}$. This is repeated for all answer options, and the option with the overall highest entailment score selected. Further details are given in the Appendix. 

The DGEM model uses a structured representation of the hypothesis $h_{i}$, extracted with a proprietary parser plus Open IE. To create a releasable version of DGEM, we also evaluate (and release) a variant DGEM-OpenIE, a version of DGEM that only uses Open IE to create the structured representation of $h_{i}$, thus avoiding proprietary tools.

\item \textit{BiDAF (Reading Comprehension Model).}
  We also adapted BiDAF \cite{Seo2016BidirectionalAF}, a top-performer on the SQuAD dataset \cite{rajpurkar2016squad}, to our task.
  As BiDAF is a direct answer system, we adapted it to multiple-choice QA following the approach used in several previous projects
  \cite{khashabi2018question,welbl2017crowdsourcing,kembhavi2017you} as follows:
  First, given a question, we create a single paragraph by concatenating a set of retrieved sentences. In this case, we use
  the same sentences retrieved by the entailment models for all answer options (above).
  We then use BiDAF to select an answer span from this paragraph, given the question. Finally, we pick the multiple-choice option that maximally overlaps that answer span (here defined as the option with the highest percentage of lemmatized, non-stopword tokens covered by the BiDAF answer span). BiDAF was trained on SQuAD then further tuned to science questions %\ASC~
   using continued training.% on the training questions.
\end{enumerate}

\subsection{Results}

Table~\ref{baselines} summarizes the scores obtained by various baseline algorithms on
the test partitions of the \Challenge~and \Additional~sets.

\begin{table}[!t] 
\setlength{\tabcolsep}{4pt}
\setlength{\doublerulesep}{\arrayrulewidth}
\small
{\centering
\begin{tabular}{|l|cc|} \hline
  \bigstrut[t] & \multicolumn{2}{|c|}{\bf Test Scores} \\
   \multicolumn{1}{|c|}{\bf Solver} & {\bf \Challenge} & {\bf \Additional} \\
  & {\bf Set} & {\bf Set} \\  \hline \hline
IR (dataset defn) \bigstrut[t]	& (1.02)$^{\dag}$	& (74.48)$^{\dag}$ \\
PMI (dataset defn)	& (2.03)$^{\dag}$	& (77.82)$^{\dag}$ \\ \hline
IR (using \ASC~Corpus) \bigstrut[t] 	& 20.26	& 62.55 \\
TupleInference							& 23.83	& 60.81 \\
DecompAttn${^\ddag}$					& 24.34	& 58.27 \\
Guess-all (``random'')					& 25.02	& 25.02 \\
DGEM-OpenIE${^\ddag}$					& 26.41	& 57.45 \\
BiDAF${^\ddag}$							& 26.54 & 50.11 \\ 
TableILP								& 26.97	& 36.15\\ 
DGEM									& 27.11	& 58.97 \\ \hline
\end{tabular} \\
}
$^{\dag}$These solvers were used to define the dataset, affecting scores. \\
${^\ddag}$Code available at~\codeurl 
\caption{Performance of the different baseline systems. Scores are reported as percentages on the test sets. For up-to-date results, see the \ASC~leaderboard at ~\ascurl.}
\label{baselines}
\end{table}

The IR and PMI (dataset definition) solvers, by design, score near zero on the \Challenge~set. The slightly above-zero score is due to the solver occasionally picking multiple (tied) answers, resulting in a partial credit for a few questions. We include these questions in the \Challenge~set.

The most striking observation is that \emph{none of the algorithms score significantly
higher than the random baseline} on the \Challenge~set, where the 95\% confidence
interval is $\pm 2.5\%$.
In contrast, their performance on the \Additional~set is generally between 55\% and 65\%.
This highlights the different nature and difficulty of the \Challenge~set.

The poor performance of the non-IR solvers is partly explained by their correlation with the IR solver: the first step for nearly all of them (except TableILP, which uses non-sentential knowledge but has low knowledge coverage) is to use a simple IR method to obtain relevant 
sentences, and then process them in different ways such as extracting structure, attempting matching, attempting chaining, etc. However, the retrieval bias of the underlying IR methods is towards sentences that are all very similar to the question, and away from sentences that individually only partially match the question, but together fully explain the correct answer (e.g., through chaining). This suggests the need for a more advanced retrieval strategy for questions that require combining multiple facts, as well as new methods for combining that information.

\section{Conclusion}

Datasets have become highly influential in driving the direction of research.
Recent datasets for QA have led to impressive advances, but have 
focused on factoid questions where surface-level cues alone are
sufficient to find an answer, discouraging progress on questions
requiring reasoning or other advanced methods. To help the field
move towards more difficult tasks, we have presented the AI2 Reasoning Challenge (\ASC),
consisting of a new question set, text corpus, and baselines,
and whose \Challenge~partition is hard for retrieval and
co-occurence methods. We find that none of the baseline systems
tested can significantly outperform a random baseline on the \Challenge~set,
including two neural models with high performances on SNLI and SQuAD.
Progress on \ASC~would thus be an
impressive achievement, given its design, and be significant step
forward for the community. To access \ASC, view the leaderboard,
and submit new entries, visit the \ASC~Website at~\ascurl.

\eat{
Datasets have become highly influential in driving the direction of research.
Recent datasets for QA have led to impressive advances, but have 
focused on factoid questions where surface-level cues alone are
sufficient to find an answer, discouraging progress on questions
requiring reasoning or other advanced methods. To help the field
move towards more challenging task, we have presented \ASC, a new
dataset whose \Challenge~partition is hard for retrieval and
co-occurence methods. We find that none of the baseline systems
tested can significantly outperform a random baseline on this set,
and thus pose it as a challenge to the community. To record
and publicize progress, we have established a leaderboard at
http://www.allenai.org/asq.

In support of this, we also are releasing the \ASC~Corpus, a collection of
\BUSCSIZE~sentences relevant the \ASC~task. From an analysis of a sample,
the \ASC~Corpus appears to mention the knowledge needed for
approximately 95\% of the \ASC~\Challenge~questions.
We are also releasing code for the two neural baselines for
others to build on. Progress on the \ASC~\Challenge~would be an
impressive achievement, given its design, and be significant step
forward for the community. We encourage other research groups
to rise to this challenge!
}

\subsubsection*{Availability}
The \ASC~Dataset, Corpus, three baseline neural models (DecompAttn, BiDAF, and DGEM), and
the leaderboard, are all available from~\ascurl.

\end{document}