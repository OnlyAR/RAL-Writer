\title{RiTTA: Modeling Event Relations in \\Text-to-Audio Generation}

\begin{document}

\maketitle
\vspace{-3mm}
\begin{abstract}
Despite significant advancements in Text-to-Audio (TTA) generation models achieving high-fidelity audio with fine-grained context understanding, they struggle to model the relations between audio events described in the input text. However, previous TTA methods have not systematically explored audio event relation modeling, nor have they proposed frameworks to enhance this capability. In this work, we systematically study audio event relation modeling in TTA generation models. We first establish a benchmark for this task by: (1) proposing a comprehensive relation corpus covering all potential relations in real-world scenarios; (2) introducing a new audio event corpus encompassing commonly heard audios; and (3) proposing new evaluation metrics to assess audio event relation modeling from various perspectives. Furthermore, we propose a finetuning framework to enhance existing TTA models' ability to model audio events relation.
\end{abstract}
\vspace{-2mm}
\section{Introduction}
\vspace{-2mm}
\label{sec:intro}

Text-based crossmodal content generation has gained significant attention in recent years as it opens up new possibilities for even amateur users to create professional content. Typical such methods include text-to-image~(TTI)~\citep{ho2020denoising}, text-to-music~(TTM)~\citep{musicGEN}, text-to-point~(TTP)~\citep{point_E}, text-to-speech~(TTS)~\citep{fast_speech} text-to-audio~(TTA)~\citep{audioldm2,makeanaudio}. Among all of them, text-to-audio~(TTA) generation stands out as a particularly promising area, enabling the synthesis of complex acoustic environments or soundscapes directly from textual descriptions. Recent advances in TTA have demonstrated impressive progress in generating high-quality, detail-rich audio described in the input text prompt~\citep{audioldm2,liu2023audioldm,makeanaudio,makeanaudio_v2,ghosal2023tango,ghosal2023tango2,kreuk2022audiogen}.

\begin{wraptable}{r}{7.0cm}
\scriptsize
\begin{tabular}{l|cl}
\hline
\multicolumn{3}{c}{\textit{\makecell[c]{\textbf{Text Prompt}: generate dog barking audio, \\followed by cat meowing audio}}}\\
\hline
Method & Relation? & Remark \\
\hline
AudioLDM~(\citeyear{liu2023audioldm}) & \xmark & \makecell[c]{just cat meow, low-fidelity}\\
AudioLDM 2~(\citeyear{audioldm2}) & \xmark & output dog barking\\
MakeAnAudio~(\citeyear{makeanaudio}) & \xmark & just cat meow, low-fidelity \\
AudioGen~(\citeyear{kreuk2022audiogen}) & \xmark & output wrong audios \\
Tango~(\citeyear{ghosal2023tango2}) & \xmark & \makecell[l]{two audios, low fidelity}\\
Tango 2~(\citeyear{ghosal2023tango2}) & \xmark & \makecell[l]{can output two audios}\\
\hline
\end{tabular}
\vspace{-3mm}
\caption{\small A case study on relation of TTA methods. Listenable audios are provided in suppplementary material.}
\label{tab:relation_test}
\end{wraptable} 

When perceiving the physical world acoustically, whether through text or audio, the fundamental unit is the audio event, a distinct acoustic signal representing an independent source. The essence of perception lies in understanding the relationships emerging from events. Audio events are spatiotemporally distributed in the physical world. Together with relation, they contribute for holistic acoustic scene understanding~\citep{acoustic_scene_classify}. Studies in psychology~\citep{Zacks_Speer_Swallow_Braver_Reynolds_2007} and neuroscience~\citep{human_level_concept,hirsh1967brain} show that the human brain perceives the environment through discrete events and the relations between them. Humans are adept at using rich language to describe both audio events and their intricate relationships. While current TTA models can generate audios with high fidelity, their ability to generate audios that not only includes audio events but also preserves the text-informed relationships between them remains unexplored.

As a primary study, we prompt the latest six TTA models with an exemplar text with explicit audio events and their relation \textit{generate dog barking audio, followed by cat meowing audio}. Next we check if the specified audio events are present and if so, their relations are correct in the generated audios. As is shown in Table~\ref{tab:relation_test}, all existing TTA models fail to properly model temporal relationships in the generated audio, even when they succeed in generating the correct audio events. The generated audio waveform, spectrum and another case study with a much complex text are shown in Fig.~\ref{fig:teasing_fig}. The poor performance of current TTA models in modeling audio events relation, along with the lack of systematic discussion on this topic, motivates us to explore \emph{Relation in TTA}~(dubbed \emph{RiTTA}) in depth in this work. We visualize the motivation in Fig.~\ref{fig:teasing_fig}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{figs/ritta_teasing_fig.pdf}
    \vspace{-2mm}
    \caption{\small \emph{RiTTA} Motivation: The acoustic world is rich with diverse audio events that exhibit various relationships. While text can precisely describe these relationships~(Fig.~A), current TTA models struggle to capture both the audio events and the relations conveyed by the text~(Fig.~B). This challenge motivates us to systematically study \emph{RiTTA}.}
    \label{fig:teasing_fig}
    \vspace{-3mm}
\end{figure}

To systematically study \emph{RiTTA}, we first benchmark it from four key perspectives: 1. we construct a comprehensive audio event relation corpus that captures common relationships found in the physical world. Unlike visual relations in cross-modal image tasks, which mainly focus on spatial aspects~(\textit{e.g.}, left, bottom)~\citep{gokhale2022benchmarking}, audio events exhibit far more complex relationships spanning spatial, temporal, and compositional dimensions. Consequently, we define four primary relation categories: \emph{Temporal Order}, \emph{Spatial Distance}, \emph{Count}, and \emph{Compositionality}. 2. Accompanying the relation corpus, we build an audio event category corpus derived from five main sources, each of which is further linked to multiple seed audios. 3. devise a \texttt{<textprompt,audio>} pair generation strategy emphasising both text prompt and audio diversity. 4. propose a new relation aware evaluation framework that assesses the relation in a multi-stage manner. The proposed benchmark will benefit the community to explore \emph{RiTTA} in greater depth. Additionally, we introduce a fine-tuning strategy based on the latest state-of-the-art~(SoTA) TTA model and demonstrate its effectivenss in relation modelling. In summary, we make the following four main contributions:

\vspace{-2mm}
\begin{enumerate}[leftmargin=*]
    \item We conduct an extensive evaluation of existing TTA models in modeling the audio events relations and demonstrate their inability to capture these relations in the generated audios.
    \item We benchmark \emph{RiTTA} by constructing complete relation corpus, audio event category corpus, seed audio corpus. Combined with the \texttt{<textprompt,audio>} pair generation strategy, researchers can create large, diverse dataset to further investigate the \emph{RiTTA} task.
    \item We propose a new multi-stage relation aware evaluation framework, called \emph{MSR-RiTTA}, which offers a more nuanced evaluation compared to existing TTA metrics, allowing researchers to quantitatively assess their methods from multiple angles.
    \item We introduce a fine-tuning strategy leveraging the new dataset, demonstrating improvement over current SoTA methods. 
\end{enumerate}\vspace{-3mm}
\section{Related Work}
\vspace{-3mm}

\textbf{Audio Generation} has received lots of attention and made significant progress in recent years, advanced by fast-progressing generative AI technologies~\citep{ho2020denoising,stable_diffusion}. Audio generation encompasses sub-tasks such as text-to-speech~(TTS) that focuses on generating speech from text transcription~(\textit{e.g.}, FastSpeech~\citep{fast_speech} and GradTTS~\citep{grad_tts}), text-to-music~(TTM) that generates music from text input~(\textit{e.g.}, MusicLM~\citep{musicLM}, MusicGen~\citep{musicGEN}) and Image-to-Audio~(ITA) generation that generates audio from image input~(\textit{e.g.}, Img2Wav~\citep{img2wav}, SpecVQGAN~\citep{SpecVQGAN_Iashin_2021}, RegNet~\citep{chen2020regnet}) and Text-to-Audio~(TTA) generation aiming to generate corresponding audio described by text~(\textit{e.g.}, AudioLDM~\citep{audioldm2,liu2023audioldm,yang2022diffsound}, DiffSound~\citep{yang2022diffsound}).

\textbf{Text-to-Audio~(TTA) Generation} involves producing audio that faithfully reflects the acoustic content or behavior described by the input text. Recent advancements have significantly improved the quality and intelligibility of generated audio~\citep{audioldm2,liu2023audioldm,kreuk2022audiogen,yang2022diffsound,ghosal2023tango,liao2024baton}. Despite improvements in audio quality and intelligibility, existing TTA methods still lag significantly in their ability to model relationships between audio events in the generated audio. AudioLDM~\citep{liu2023audioldm} builds on latent space~\citep{stable_diffusion} to learn continuous representation.

\textbf{Audio Events Relation Modeling}. In the context of environmental audio, a set of audio events exhibit relationships that are crucial for holistic acoustic scene understanding. 
Based on how audio interact with the physical world in space, time and perceptual aspects, the resulting audio events exhibit complex relationships in spatial, temporal and compositional aspects. Prior work has partially addressed modeling certain temporal relations (\textit{e.g.}, order) in TTA~\citep{audiotime} and compositional reasoning~\citep{ghosh2024compa} for discriminative tasks, such as audio classification and audio-text retrieval. WavJourney~\citep{liu2023wavjourney} leverages a large language model alongside multiple audio generation models to achieve compositional audio generation. However, its limitations include an artificial post-mixing process, which may result in generated audio lacking smooth transitions across event boundaries and inefficiencies in inference. While prior research has touched on modeling audio event relations, their potential in TTA remains largely underexplored. If we analogize an audio event to an object in image, the corresponding relationships exhibited in an image are mainly limited to 2D spatial relationship~(\textit{e.g.}, before, bottom, left). Despite object of interest spatial relationship learning and evaluation have received lots of attention in recent years~\citep{Krishna2016VisualGC,gokhale2022benchmarking,compos_ability}, the research on audio event relation modeling has been almost ignored.\section{Benchmark TTA Audio Events Relation}
\vspace{-3mm}

In this section, we sequentially present audio events relation corpus in Sec.~\ref{sec:relation_corpus}, audio event category corpus in Sec.~\ref{sec:audio_event_category}, seed audio corpus and \texttt{<textprompt,audio>} pair generation strategy in Sec.~\ref{sec:seed_audio_corpus}. Finally, the relation aware evaluation framework \emph{MSR-RiTTA} is presented in Sec.~\ref{sec:relation_eval}.
\vspace{-2mm}

\begin{figure}[t]
\begin{minipage}[t]{.5\linewidth}
\centering
\small
    \renewcommand{\arraystretch}{0.5}
    \begin{tabular}{m{1.2cm}|m{1.4cm}|m{3.5cm}}
    \hline
    \makecell[c]{Main\\ Relation} & \makecell[c]{Sub-\\Relation} & \makecell[c]{Sample Text Prompt} \\
    \hline
       \makecell[c]{Temporal \\ Order}  &  \makecell[c]{before;\\ after;\\ simultaneity} & \makecell[c]{generate dog barking audio,\\ followed by cat meowing;}\\
       \hline
       \makecell[c]{Spatial \\Distance} & \makecell[c]{close first;\\far first;\\ equal dist.} & \makecell[c]{generate dog barking audio\\ that is 1 meter away, follow-\\ed by another 5 meters away.}\\
       \hline
      \makecell[c]{Count}& \makecell[c]{count} & \makecell[c]{produce 3 audios: dog bark-\\ing, cat meowing and talking.}\\
       \hline
       \makecell{Composit\\ionality} & \makecell[c]{and; or;\\ not;\\ if-then-else} & \makecell[c]{create dog barking audio\\ or cat meowing audio.}\\
       \hline
    \end{tabular}
    \vspace{-1mm}
    \captionof{table}{\small Audio Events Relation Corpus.}
    \label{tab:relation_corpus}
\end{minipage}\quad
\hspace{2mm}
\begin{minipage}[t]{.4\linewidth}
\centering
\small
    \begin{tabular}{m{1.7cm}|m{4.cm}}
    \hline
    \makecell[c]{Main\\Category} & \makecell[c]{Sub-Category} \\ 
    \hline
       \makecell[c]{Human\\ Audio}  &  baby crying; talking; laughing; coughing; whistling \\
       \hline
       \makecell[c]{Animal\\ Audio} & cat meowing; bird chirping; dog barking; rooster crowing; sheep bleating\\
       \hline
       \makecell[c]{Machinery} & boat horn; car horn; door bell; paper shredder; telephone ring\\
       \hline
       \makecell[c]{Human-Object\\ Interaction} &vegetable chopping; door slam; footstep; keyboard typing; toilet flush\\
       \hline
        \makecell[c]{Object-Object\\ Interaction} & emergent brake; glass drop; hammer nailing; key jingling; wood sawing\\
       \hline
    \end{tabular}
    \vspace{-1mm}
    \captionof{table}{\small Audio Events Category Corpus.}
    \label{tab:event_corpus}
\end{minipage}
\vspace{-4mm}
\end{figure}

\vspace{-2mm}
\subsection{Audio Event Relation Corpus}
\vspace{-2mm}
\label{sec:relation_corpus}

An audio event refers to a distinct acoustic signal occurrence with specific frequency, duration and context characteristics that can be attributed to distinguish an independent sound source~\citep{sounddet} in an environment. Audio event is ubiquitous in the physical world and serves as the fundamental entity to analyze and interpret the acoustic scene. We embrace the audio event as the fundamental element to construct the relation corpus.

We construct the audio events relation corpus based on two key aspects. First, we consider relations commonly found in the physical world, such as those arising from spatial and temporal variations, which test TTA models' ability to replicate audio events' interactions in real-world scenarios. Second, we focus on relations that challenge TTA models' logical reasoning, evaluating their ability to determine both which audio events to generate and how to generate them. These two aspects partially overlap. Specifically, we define five main audio event categories, each associated with five subcategories of audio events. The detailed relation corpus is provided in Table~\ref{tab:relation_corpus}, including,

\vspace{-2mm}
\begin{enumerate}[leftmargin=*]
    \item \textbf{Number Count}: The number of audio events included in the generated audio, testing TTA models' ability to address acoustic polyphony challenge.
    \item \textbf{Temporal Order}: Temporal order refers to the sequence of audio events in the generated audio. We include three basic temporal relations for two audio events: \texttt{before}, \texttt{after}, and \texttt{simultaneity}, testing the TTA models' ability to distinguish and generate the correct event order as specified in the input text prompt.
    \item \textbf{Spatial Distance}: Spatial distance refers to the variation in relative spatial distances inferred from the generated audio. It evaluates the TTA models' ability to capture the spatial distance differences specified in the text prompt. Since we focus on mono-channel audio, obtaining the absolute distance for each audio event is nearly impossible~\citep{sounddet}. Therefore, we rely on loudness differences within intra-class audio events to verify their spatial distance variations.
    \item \textbf{Compositionality}: Compositionality relation describes how multiple individual audio events are integrated together to form a complex auditory structure that specified in the input text prompt. It tests TTA models' logical reasoning capability in determining which audio events to generate and how to structure them, by following the compositional guidance illustrated in the input text prompt. Specifically, we incorporate four main compositionality relations: Conjunction~(\texttt{And}, \textit{e.g.}, generate audio A and audio B together); Disjunction~(\texttt{Or}, \textit{e.g.}, generate audio A or Audio B, not both); Negation~(\texttt{Not}, exclude one particular audio event, \textit{e.g.}, do not generate dog barking audio); Condition~(\texttt{if-then-else}, either generate two audio events if the condition is met, otherwise generate the third audio if the condition is not met). 
\end{enumerate}

Most of the relations relate to two audio events~(see Table~\ref{tab:event_corpus} for more detail). Expanding the corpus to include more complex relations with a greater number of audio events is left for future work.

\subsection{Audio Event Category Corpus}
\vspace{-2mm}
\label{sec:audio_event_category}

Alongside the relation corpus presented in Sec.~\ref{sec:relation_corpus}, we further construct a comprehensive audio event category corpus. The two corpora serve as fundamental dataset for constructing text prompts for TTA models. Since different audio event signals are generated from various sources or through different interactions, we first establish four main audio source categories, further detailing each category with five sub-categories. These constructed audio categories encompass the majority of ubiquitous audio events encountered in our daily lives. Specifically, the audio event category corpus contain,

\vspace{-2mm}
\begin{enumerate}[leftmargin=*]
    \item \textbf{Human Audio}: the audio generated by human beings in our daily life, including \textit{baby crying}, \textit{coughing}, \textit{laughing}, \textit{whistling}, \textit{female speech} and \textit{male speech}.
    \item \textbf{Animal Audio}: the audio generated by animals, including \textit{cat meowing}, \textit{dog barking}, \textit{bird chirping}, \textit{horse neighing}, \textit{rooster crowing}, \textit{sheep bleating} and \textit{pig oinking}.
    \item \textbf{Machinery Audio}: the audio generated by various machinery devices while they are working, including \textit{car horn}, \textit{doorbell}, \textit{telephone ring}, \textit{paper shredder} and \textit{boat horn}.
    \item \textbf{Human-Object Interaction Audio}: human-object interaction audios include \textit{vegetable chopping}, \textit{keyboard typing}, \textit{toilet flushing}, \textit{door slamming} and \textit{foot step}.
    \item \textbf{Object-Object Interaction Audio}: we further incorporate object-object interaction audios, including \textit{glass dropping}, \textit{car emergency brake}, \textit{hammering nail}, \textit{wood sawing} and \textit{keys jingling}.
\end{enumerate}
The detailed audio event corpus is given in Table~\ref{tab:event_corpus}. With the constructed relation and audio event corpus, we can create relation aware text prompts for TTA models.

\subsection{Seed Audio Corpus and Text-Audio Pair Creation Strategy}
\vspace{-2mm}
\label{sec:seed_audio_corpus}

In order to create the corresponding audio for any constructed text prompt, we instantiate each audio event presented in Sec.~\ref{sec:audio_event_category} with five exemplar seed audios collected from \texttt{freesound.org}~\footnote{since \texttt{freesound.org} does not contain meaningful people talking audio, we collect people talking audio from VCTK~\citep{yamagishi2019vctk}}. Since most audio files on \texttt{freesound.org} are uploaded by volunteers who recorded them in their daily lives, incorporating five exemplar audios for each individual audio event category enhances both the diversity and realism of the seed audio. For instance, in the case of the \texttt{dog barking} audio event, the five selected audios vary in terms of dog breeds and barking styles. To further enhance an audio event's temporal length diversity, we randomly slice each seed audio into non-overlapping clips ranging from 1~sec to 5~secs. In summary, we have constructed 11 relations~(see Table~\ref{tab:relation_corpus} Sub-Relation column), and 25 audio events across five main audio events categories. Each audio event has been associated with 5 diverse audio clips ranging from 1~sec to 5 secs collected from \texttt{freesound.org}.

\begin{wrapfigure}{r}{0.52\textwidth}
\begin{mdframed}[style=prompt]
\small
1.~generate audio A succeeded by B;\\
2.~start with A, followed by B;\\
3.~play A initially, B afterwards;\\
4.~generate A preceded by B;\\
5.~A in the beginning, B coming next;
\end{mdframed}
\vspace{-2mm}
\caption{\small GPT-4 augmented prompts~(\texttt{before} relation).}
\label{gpt-4-text}
\vspace{-3mm}
\end{wrapfigure}

\textbf{Text Prompt Generation}: a proper audio events relation aware text prompt comprises of two parts: a relation~(\textit{e.g.}, \texttt{<before>}) and audio events categories. The audio event categories can be either intra-class or inter-class, and the audio event number depends on the relation. We first instantiate an initial text prompt describing this relation. For example, for the temporal order \texttt{before} relation, the initial text prompt can be like: \textit{generate audio A, followed by audio B}. To enrich the text prompts, we further use the initial text prompt to query LLM~(in our case GPT-4) to provide more text prompts with diverse descriptive language for the same relation. 
One such GPT-4 augmented text prompts is shown in Fig.~\ref{gpt-4-text}, which illustrates that the same relation can be exactly expressed by multiple different text prompts. By incorporating GPT-4, we create 5 text prompts for each individual relation.

\begin{figure}[t]
    \centering
    \begin{minipage}{0.6\textwidth}
        \centering
        \includegraphics[width=0.98\textwidth]{figs/relation_datagen_pipeline.pdf}
        \caption{\small Relation aware \texttt{<textprompt,audio>} pair creation pipeline. It introduces large diversity in both text prompt and audio.}
        \label{fig:relation_datagen_pipeline}
    \end{minipage}\hfill
    \begin{minipage}{0.4\textwidth}
        \centering
        \scriptsize
        \begin{tabular}{p{1.3cm}<{\centering}|p{3cm}<{\centering}}
        \hline
        Entry & Highlight\\
        \hline
        seed audio & \makecell[c]{one event has 5 audios\\
                                  each has 1~s-5~s audio clips}\\
        \hline
        \makecell[c]{audio categ-\\ory corpus} & \makecell[c]{5 main categories\\25 sub-categories}\\
        \hline
        \makecell[c]{relation\\corpus} & 4 main 11 sub relations\\
        \hline
        \makecell[c]{relation -\\event number} & \makecell[c]{\texttt{count}: 2-5 events;\\
                                                               \texttt{Not}: 1 event;\\
                                                               \texttt{if-then-else}: 3 events\\
                                                               others: 2 events.}\\
                                                               
        \hline
        \makecell[c]{train and \\ test data info} & \makecell[c]{each audio is 10~s long\\sampling rate 16~kHz\\
                                                                 train: 44~hrs, 1.6~k pairs\\
                                                                 test: 22~hrs, 0.8~k pairs} \\
        \hline
        \makecell[c]{data creation\\constraint} & \makecell[c]{\texttt{count} inter-category audio\\ \emph{SpatialDist} intra-category\\and require temporder}\\
        \hline
        \makecell[c]{audio\\diversity} & \makecell[c]{one event $\rightarrow$ multi-audios;\\
                                                    seed audio $\rightarrow$ multi time len;\\
                                  seed audios various start time \\}\\
        \hline
        \makecell[c]{text prompts \\diversity} & \makecell[c]{GPT-4 augmented prompts;\\
                                  one template $\rightarrow$ multi-events.}\\
        \bottomrule
        \end{tabular}
        \vspace{-2mm}
        \captionof{table}{\small \emph{RiTTA} benchmark highlights.}
        \label{tab:ritta_datacreate}
    \end{minipage}
    \vspace{-5mm}
\end{figure}

\textbf{Audio Generation}: Given the aforementioned audio events categories and relation, we randomly select an exemplar seed audio for each audio event and further linearly blend them together by satisfying the specified relation. For example, the relation \texttt{<before>} requires two audio events, the two selected audios can be blended together to form the final audio as long as the two seed audios satisfy the \texttt{<before>} relation~(Fig.~\ref{fig:relation_datagen_pipeline}, D). Notably, unlike blending two objects in an image that requires careful consideration of factors like occlusion and viewing angle, combining two audio signals simply involves linearly adding them together~\citep{wavetheory}. This offers an advantage for audio generation, as it eliminates the need for additional operations beyond the specified relation.

The generation of the \texttt{<textprompt,audio>} pair is further illustrated in Fig.~\ref{fig:relation_datagen_pipeline}. With the proposed \texttt{<textprompt,audio>} pair generation strategy, we can create massive diverse pairs even for the same audio events and the same relation, significantly enhancing the diversity and generalization capability of our generated dataset.

\subsection{Relation Aware Evaluation Metric MSR-RiTTA}
\label{sec:relation_eval}

Existing TTA methods adopt general evaluation metrics to asses the similarity between generated audio and reference audio, including Fr\'{e}chet Audio Distance~(FAD), Fr\'{e}chet Distance (FD)~\citep{fid_distance}, Kullback–Leibler~(KL) divergence, Fr\'{e}chet Inception Distance~(FID) \textit{etc.}, among others. While those general evaluation metrics give an overall estimation of the similarity between the two comparing audios, they do not offer direct relation-aware evaluations. In addition to incorporating general evaluation metrics, we further propose multi-stage relation-aware evaluation metrics, with which we can gain insight on how the method performs w.r.t. difference relations.

\textbf{General Evaluation Metric}: We incorporate three widely used general evaluation metrics: the objective evaluation metric FAD, FD and KL divergence scores. 
FAD and FD measure the distribution similarity with feature embedding extracted from pre-trained on VGGish model~\citep{vggish}.

\begin{wrapfigure}{r}{0.35\textwidth}
    \centering
    \includegraphics[width=0.34\textwidth]{figs/relation_eval_vis2.pdf}
    \vspace{-2mm}
    \caption{\small relation aware evaluation. Audio event detection model is applied to get audio events. The meta data of each event contains start time $t_1$, end time $t_2$, confidence score $s$ and class label $c$. Various relations can be discovered from these audio events.}
    \label{fig:relation_eval_vis}
    \vspace{-3mm}
\end{wrapfigure}

\textbf{Relation aware Evaluation Metric MSR-RiTTA}: To directly measure how accurately the text-indicated relation is reflected in the generated audio, we incorporate relation aware metrics for each specified relation.

In relation aware evaluation, we base on the individual audio event to compute the metrics, which allows us to measure the relation between audio events. Let's denote $(\mathcal{A}_g, \mathcal{T}, \mathcal{R}, \mathcal{A}_p)$ by ground truth audios, text prompts, relations and generated audios, respectively. We first extract audio events $\mathcal{E}$ from generated audios $\mathcal{A}_p$. For example, for the $i$-th generated audio $a_i^p$, we apply pre-trained audio event detection model~(we use finetuned PANNS~\citep{panns}, see Sec.~\ref{sec:app:panns_finetune} in Appendix) to extract all potential audio events involved in the audio $E_{a_i^p}=\{(e_j, m_j)|s\}_{i=1}^{k}$ by a given event confidence threshold $s\in \mathcal{S}$, where $e_j$ is the $j$-th audio event and $m_j$ is the corresponding meta data~(\textit{e.g.}, audio event class label, confidence score, temporal start time and end time, see Fig.~\ref{fig:relation_eval_vis}). To obtain audio events data for ground truth audios, we can either apply the same pre-trained model or directly extract from text prompts. Finally, we can get $(\mathcal{A}_g, \mathcal{T}, \mathcal{R}, \mathcal{A}_p, \mathcal{E}_p, \mathcal{E}_g)$, the relation aware evaluation function $f(\cdot)$ depends on the audio events $\mathcal{E}_p$, $\mathcal{E}_g$ and relations $\mathcal{R}$, $f(\mathcal{E}_p, \mathcal{E}_g|\mathcal{R}, s)$. We adopt a multi-stage relation~(\emph{MSR-RiTTA}) aware evaluation strategy.

\textbf{Stage 1:} Target Audio Events Presence~(\textbf{Pre}). The paramount requirement for a successful audio generation is the presence of text-specified audio events in the generated audio. In this evaluation, the ground truth audio events and generated audio events are treated as \textit{set}. For a given ground truth and generated audio events pair $(E_g, E_p)$, we iterate over each audio event $e_g$ in the ground truth $E_g$ to check if it exists in the generated audio events $E_p$, regardless of its number and temporal position. 

\begin{equation}
    f_p(E_p, E_g) = \frac{1}{k}\sum_{e_g \in E_{g}}\mathds{1} (e_{g}, E_{p});\ \ \ 
    \mathds{1}(e_g, E_p) = \left\{
    \begin{aligned}
        1,&\ \text{if}\ e_g \in E_p  \\
        0,& \ \ \ \ \text{otherwise},
    \end{aligned}
    \right.
\label{eqn:eval_presence}
\end{equation}

\noindent where $k$ is audio event number in the ground truth. $s_l(e_g)$ is a potential event meeting the confidence threshold in the generated audio. We select the event with the highest confidence score as the target.

\textbf{Stage 2:} Relation Correctness~(\textbf{Rel}). Once confirming the aforementioned target audio presence, we further investigate if these audio events obey text-specified relation. The relation is correctly modelled if at least a subset of generated audio events meet the relation. We give score 1 if relation is correctly modelled, otherwise score 0.
\begin{equation}
    f_r(E_p|R) = \prod_{E_t \in E_p \cap E_g} \mathds{1}(E_t,R);\ \ \ 
    \mathds{1}(E_t,R) = \left\{
    \begin{aligned}
        1,&\ \ \  \text{if}\ \ E_t \ \ \text{satisfies relation} \ \ R, \\
        0,& \ \ \ \ \text{otherwise},
    \end{aligned}
    \right.
\label{eqn:eval_relation}
\end{equation}

\textbf{Stage 3:} Audio Parsimony~(\textbf{Par}). Apart from requiring to generate all target audios, we should discourage the model from generating excessive intra-class audio events or irrelevant inter-class audio events. We call this property \textit{Audio Parsimony}. Once it is violated, we introduce extra penalty.

\begin{equation}
    f_s(E_p, E_g) = \exp{(-w_s\cdot|n(E_p) - n(E_g)|)}
\label{eqn:eval_parsimony}
\end{equation}

\noindent where $n(\cdot)$ indicates audio event number. $w_s$ is the weight adjusting the penalty~(in our case, $w_s=0.1$). The higher audio event number difference incurs lower parsimony score, the resulting parsimony score lies within $(0,1)$. The final relation aware score based on the audio event confidence threshold $s$ equals to the multiplication of the three stage scores,
\begin{equation}
    f(\mathcal{E}_p, \mathcal{E}_g|\mathcal{R}, s) = \frac{1}{N}  \sum_{(E_p, E_g, R) \in (\mathcal{E}_p, \mathcal{E}_g, \mathcal{R})} f_p(E_p, E_g) \cdot f_r(E_p|R) \cdot f_s(E_p, E_g)
\label{eqn:MSR}
\end{equation}
\noindent where $N$ is data size number. The final average MSR~(AMSR) score $f(\mathcal{E}_p, \mathcal{E}_g|\mathcal{R}, s)$ lies within $[0, 1)$~(the higher of the score, the better of the model's performance). Following prior COCO object detection evaluation strategy~\citep{coco_dataset}, we further average across multiple discrete audio event confidence thresholds to get the mean average MSR score~(mAMSR), $f(\mathcal{E}_p, \mathcal{E}_g|\mathcal{R})$,
\begin{equation}
    f(\mathcal{E}_p, \mathcal{E}_g|\mathcal{R}) = \frac{1}{K}  \sum_{s\in \mathcal{S}} f(\mathcal{E}_p, \mathcal{E}_g|\mathcal{R})
\label{eqn:mMSR}
\end{equation}
\noindent where $K$ is the discrete audio event confidence thresholds number. In our case we use uniformly sample four confidence thresholds in range $[0.5, 0.8]$ with step size $0.1$.

\section{Relation Aware TTA Finetuning}
\label{sec:relation_finetune}

\begin{wrapfigure}{r}{0.35\textwidth}
    \centering
    \includegraphics[width=0.34\textwidth]{figs/ritta_finetune_pipeline.pdf}
    \vspace{-2mm}
    \caption{\small RiTTA finetune pipeline.}
    \label{fig:ritta_finetune_pipeline}
    \vspace{-3mm}
\end{wrapfigure}
Existing TTA models adopt audio-language pre-trained model to extract text and audio embeddings, including CLAP~\citep{laionclap2023} and FLAN-T5~\citep{FLAN-T5}. Prior work~\citep{ma2023crepe,yuksekgonul2023when,audiotext_noNL,ghosh2024compa} show that existing audio-language pre-trained models~(\textit{e.g.}, CLAP~\citep{laionclap2023}) performs like bag-of-words~(BoW), which means they are far better at audio event retrieval task than audio events temporal relation task. 

Moreover, the dataset used to pre-train audio-language such as AudioSet~\citep{audioset_dataset} and AudioCaps~\citep{audiocaps} are dominated by unary audio event~($64\%~$\citep{ghosh2024compa}), limiting models from learning meaningful representations for audio event relations.

Based on aforementioned discussion, we propose to finetune the existing latest Tango model~\citep{ghosal2023tango} with our created relation aware dataset~(we finetuned Tango~2 as well, but found it gave inferior performance than Tango). Tango depends on prior TTA frameworks AudioLDM~\citep{liu2023audioldm} to use a Variational Autoencoder~(VAE) for audio encoding and decoding, a latent diffusion model~(LDM)~\citep{stable_diffusion} for audio generation and HiFiGAN~\citep{HiFiGAN} to generate final audio waveform from VAE decoder decoded mel-spectrogram. Unlike AudioLDM~\citep{liu2023audioldm} which depend on CLAP~\citep{laionclap2023} for text prompt encoding, Tango adopts pre-trained Flan-T5~\citep{FLAN-T5} model for text prompt encoding. Latest TTA models such as Tango~\citep{ghosal2023tango}, Tango~2~\citep{ghosal2023tango2} and AudioLDM~2~\citep{audioldm2} show that Flan-T5 can achieve better performance than CLAP~\citep{laionclap2023} in TTA task. Benefiting from the latest advancement, we fine-tune Tango by just tuning latent diffusion model~(LDM) and fixing VAE, HiFiGAN and Flan-T5 components. In our case, we finetune Tango with the curated 44~hrs training dataset. The finetuning workflow is shown in Fig.~\ref{fig:ritta_finetune_pipeline} and finetuing detail in Sec.~\ref{sec:relation_finetune}.

\section{Experiment}

We run two experiments: benchmarking existing TTA methods on our curated 22~hrs benchmark dataset~(aka testing dataset). Fine-tuning the advanced TTA model on our curated 44~hrs training dataset and further test its relation modeling capability.
\subsection{More Discussion on Data Creation}
\label{sec:data_prepare}

We follow the strategy presented in Sec.~\ref{sec:seed_audio_corpus} to create the dataset. Specifically, for each of the 11 sub-relations in Table~\ref{tab:relation_corpus}, we create 720~(2~hrs audio) \texttt{<textpromt,audio>} pairs for testing~(aka benchmark dataset) and 1440 pairs~(4~hrs audio) for training~(aka finetuning dataset). The highlight of the training/testing dataset is given in Table~\ref{tab:ritta_datacreate}.

To ensure that all relations can be effectively evaluated using our method, we applied two key constraints during the data creation process. First, to make the audio events countable without ambiguity, we selected inter-category audio events to form the \texttt{<textprompt,audio>} pairs. This avoids the ambiguity that arises when using intra-category events, especially for those with repetitive, similar local occurrences (\textit{e.g.}, multiple instances of dog barking). Second, for the \emph{Spatial Distance} relation, we introduced a temporal order constraint to ensure that the two audio events do not overlap in time. Temporal overlap would require complex source separation models~\citep{source_separation_icassp} to distinguish individual events. By enforcing this non-overlapping constraint, the evaluation of \emph{Spatial Distance} becomes manageable using an audio event detection model~(see Sec.~\ref{sec:panns_finetune} in Appendix). The basic information of data creation is given in Table~\ref{tab:ritta_datacreate}.

\begin{table}[t]
\centering
\small
\caption{Benchmark quantitative result across all relations. mAPre, mARel and mAPar are in $10^{-2}$. mAPre and mARel can be treated as \textit{presence}, \textit{relation correctness} percentage ratio, they lie in range $[0, 100]$. mAPar score also lies within $[0, 100]$. mAMSR~($10^{-4}$) lies in range~$[0,1]$. The \colorbox{topcolor}{top-}, \colorbox{secondcolor}{second-} and \colorbox{thirdcolor}{third-} performing methods are labelled in different colors, respectively.}
\begin{tabular}{l|p{1.0cm}<{\centering}|p{0.7cm}<{\centering}p{0.7cm}<{\centering}p{0.7cm}<{\centering}|p{0.7cm}<{\centering}p{0.7cm}<{\centering}p{0.7cm}<{\centering}|p{0.9cm}<{\centering}}
\hline
\multirow{2}{*}{Model} & \multirow{2}{*}{\#param} & \multicolumn{3}{c|}{General Evaluation}  & \multicolumn{4}{c}{Relation Aware Evaluation~($\uparrow$)} \\
\cline{3-9}
 &   & FAD~$\downarrow$ &  KL~$\downarrow$ & FD~$\downarrow$ & mAPre& mARel & mAPar &mAMSR \\
\hline
AudioLDM~(S-Full)~(\citeyear{liu2023audioldm})  & $185$~M  & \cellcolor{secondcolor}5.65  & 38.95 & \cellcolor{thirdcolor}37.30 & 2.76 & 0.50 & 2.52 & 0.04\\
AudioLDM~(L-Full)~(\citeyear{liu2023audioldm})  & $739$~M  & \cellcolor{topcolor}5.47  & \cellcolor{thirdcolor}38.42 & 37.96 & 3.09 & 0.77 & 2.56 & 0.08\\
AudioLDM~2~(L-Full)~(\citeyear{audioldm2}) & $844$~M & 6.68 & \cellcolor{secondcolor}29.07 &  \cellcolor{secondcolor}35.85& \cellcolor{secondcolor}12.26& \cellcolor{secondcolor}2.41 & \cellcolor{secondcolor}10.01 & \cellcolor{secondcolor}3.39 \\
MakeAnAudio~(\citeyear{makeanaudio}) & $452$~M & 9.46 & 82.72  & 45.98 & 8.14  & 1.68 & 6.47 & 1.02\\
AudioGen~(\citeyear{kreuk2022audiogen}) & $1.5$~B & \cellcolor{thirdcolor}6.43 & \cellcolor{topcolor}28.01& \cellcolor{topcolor}32.04 & 9.61 & 2.12 & 8.60 & 2.27\\
Tango~(\citeyear{ghosal2023tango}) & $866$~M & 10.79 & 90.26 & 39.46 & \cellcolor{thirdcolor}11.13 & \cellcolor{thirdcolor}2.27 & \cellcolor{thirdcolor}9.88 & \cellcolor{thirdcolor}3.10\\
Tango~2~(\citeyear{ghosal2023tango2}) & $866$~M & 13.84  &  89.66 & 44.03 & \cellcolor{topcolor}16.63 & \cellcolor{topcolor}4.40 & \cellcolor{topcolor}12.53 & \cellcolor{topcolor}11.55 \\
\hline
\end{tabular}
\label{tab:general-results}
\end{table}

\subsection{More Discussion on RiTAA Evaluation}

Section~\ref{sec:relation_eval} has introduced the metrics in general. In practice, we further adjust the audio generation process for relations under \emph{Compositionality} and \emph{Spatial Distance} to so as to ensure these relations can be accurately evaluated under our proposed framework.

First, we skip general evaluation for \texttt{<Not>} as it lacks a corresponding ground truth reference audio. During fintuning, we generate silent audio for \texttt{<Not>} for create finetuing pairs. Second, for the \texttt{<if-then-else>} and \texttt{<Or>} sub-relations, which correspond to two possible ground truth audios, we handle evaluation by computing the L2 distance~(in the time domain) between the generated audio and the two reference audios. For example, for the prompt \textit{if event A then event B, else event C}, the first reference is the combination of events A and B, while the second contains only event C. We use the reference audio with smaller L2 distance to the generated audio for general evaluation.

Third, precise evaluation of the three sub-relations~(\texttt{<closefirst>}, \texttt{<farfirst>}, and \texttt{<equaldist>}) under \emph{Spatial Distance} from unconstrained audio requires sound event detection and localization~(SELD~\citep{soundsynp,seld_dcase19}) techniques to spatially localize each audio event, which is impossible with mono-channel audio. To address this, we approximate spatial distance by calculating the loudness, which can be estimated using the L2 norm of the audio waveform. The rationale behind this approach is that greater distances result in a dampening of waveform amplitude~(and consequently reduced loudness) due to energy decay along the audio propagation path. When the loudness difference exceeds a predefined threshold~(for \texttt{<closefirst>}, \texttt{<farfirst>}) or is within that threshold~(for \texttt{<equaldist>}), we consider the evaluation accurate. Specifically, we use a loudness reduction ratio $\sigma_1$ (with $\sigma_1=0.2$ in our case). For \texttt{<closefirst>}, if the closer event's loudness is at least $\sigma$ times greater than the further event's loudness, the relation is considered correct. Similarly, for \texttt{<equaldist>}, the loudness difference between the two events should be within $\sigma_2$~(with $\sigma_2=0.4$ in our case) of the louder event's loudness. This estimation is also reflected in the data generation process (see Sec~\ref{sec:data_prepare}).

\begin{table}[t]
\centering
\small
\caption{Benchmark quantitative result w.r.t. the four main relations. We report FAD sore and mAMSR score for general evaluation and relation aware evaluation, respectively.}
\begin{tabular}{l|p{0.5cm}<{\centering}p{1.1cm}<{\centering}p{.8cm}<{\centering}p{.8cm}<{\centering}|p{0.5cm}<{\centering}p{1.1cm}<{\centering}p{0.8cm}<{\centering}p{.8cm}<{\centering}}
\hline
\multirow{2}{*}{Model} & \multicolumn{4}{c|}{General Evaluation~(FAD $\downarrow$)}  & \multicolumn{4}{c}{Relation Aware Eval.~(mAMSR $\uparrow$)} \\
\cline{2-9}
 &  \emph{Count} &  \emph{TempOrder} & \emph{SpatDist} & \emph{Compos} &  \emph{Count} & \emph{TempOrder} & \emph{SpatDist} & \emph{Compos}  \\
\hline
AudioLDM~(S-Full)~(\citeyear{liu2023audioldm})   & \cellcolor{secondcolor}{3.85} & \cellcolor{secondcolor}6.86 &  \cellcolor{thirdcolor}4.56 & \cellcolor{secondcolor}9.36 & 0.00 & 0.05 & 0.00 & 0.18 \\
AudioLDM~(L-Full)~(\citeyear{liu2023audioldm})  & \cellcolor{topcolor}3.68  & \cellcolor{topcolor}6.45  & \cellcolor{secondcolor}4.10 & \cellcolor{topcolor}8.98 & 0.00 & 0.05 & 0.06 & 0.17\\
AudioLDM~2~(L-Full)~(\citeyear{makeanaudio}) & \cellcolor{thirdcolor}5.03 & 8.94 &  4.72 & \cellcolor{thirdcolor}9.41 & 0.14 & 1.87 & \cellcolor{secondcolor}1.46 & \cellcolor{secondcolor}9.89\\
MakeAnAudio~(\citeyear{makeanaudio}) & 6.02 & 10.21 &  8.18 & 12.78 & 0.12 & 0.66 & 0.44 & 2.40 \\
AudioGen~(\citeyear{kreuk2022audiogen})& 6.14 & \cellcolor{thirdcolor}8.39 & \cellcolor{topcolor}3.38 & 9.98 & \cellcolor{secondcolor}0.32 & \cellcolor{secondcolor}3.83 &  0.48 & 4.18\\
Tango~(\citeyear{ghosal2023tango}) & 8.54 &  10.25 & 10.11 & 13.97 & \cellcolor{thirdcolor}0.16 & \cellcolor{thirdcolor}3.44 & \cellcolor{thirdcolor}0.82 & \cellcolor{thirdcolor}8.10 \\
Tango~2~(\citeyear{ghosal2023tango2}) & 10.01  & 13.91 & 13.23 & 17.04 & \cellcolor{topcolor}0.96 & \cellcolor{topcolor}20.92 & \cellcolor{topcolor}1.92& \cellcolor{topcolor}23.25\\
\hline
\end{tabular}
\label{tab:results-fourcates}
\end{table}

\subsection{Relation Aware Benchmarking Result}

We benchmark our curated test dataset on 7 most recent TTA models: AudioLDM~\citep{liu2023audioldm}~(two versions), AudioLDM~2~\citep{audioldm2}, MakeAnAudio~\citep{makeanaudio}, AudioGen~\citep{kreuk2022audiogen}, Tango~\citep{ghosal2023tango} and Tango~2~\citep{ghosal2023tango2}. We directly depend on their released models to generate a 10~second audio from each text prompt. We then adopt general evaluation and relation-aware evaluation metrics~(see Sec.~\ref{sec:relation_eval}) for assessing the generated audios quality. The detailed configuration of each method is given in Table~\ref{app:tab:model_setting} in Appendix. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{figs/ritta_radar_chart.pdf}
    \caption{\small Top~3 performing in audio events relation modelling TTA methods' performance w.r.t. the 11 sub-relations. We report mAPre, mARel, mAPar and mAMSR scores separately.}
    \label{fig:exp_radar_chart}
\end{figure}

The quantitative evaluation results across all relations are shown in Table~\ref{tab:general-results}. From this table we can observe that the general evaluation results are inconsistent with our proposed relation aware evaluation metrics. The best performing methods under generational evaluations~(the two AudioLDM versions) perform the worst under relation aware evaluations, and vice versa. These discrepancies highlight the necessity of proposing evaluation metrics specifically tailored for audio events relations. Additionally, while the performance differences among the seven benchmarking methods under general evaluation are relatively minor, the corresponding differences under relation aware evaluation are significantly more pronounced~(\textit{e.g.}, Tango~2 outperforms AudioLDM~(S-Full) by about 200 times). However, even the top-performing method, Tango~2~\citep{ghosal2023tango2}, still struggles to model audio events relations, as both its presence accuracy and relation accuracy rate are below $1\%$~(mAPre is just 0.02\% and mARel 0.04\%), and it generates an average of two redundant audio events~(mAPar=0.1253). All of these observations demonstrate the limitations of existing TTA methods in modelling audio events relation and the necessity to systematically study audio events relation in TTA, highlighting the importance of our proposed work.

The quantitative evaluation results~(mAMSR score) w.r.t the four main relation categories are presented in Table~\ref{tab:results-fourcates}. We observe that both general and relation-aware evaluations show better performance on \emph{Temporal Order} and \emph{Compositionality} compared to \emph{Count} and \emph{Spatial Distance}. This suggests that the \emph{Count} and \emph{Spatial Distance} relations pose significant challenges for TTA tasks. Additionally, we visualize the detailed relation aware evaluation results for the 11 sub-relations, highlighting the top three performing methods AudioLDM~2~\citep{audioldm2}, Tango~\citep{ghosal2023tango}, and Tango~2~\citep{ghosal2023tango2}, in Fig.~\ref{fig:exp_radar_chart}. We can observe that all the three methods 1. achieve exceedingly high presence score on \texttt{Not} relation, which is expected since a high \textbf{Presence} score~(Subfig.~A) can be easily obtained by simply not generating the specified audio event. 2. perform well in modelling \texttt{And} relation~(Subfig.~B)~(then \texttt{<equaldist>} and the three relations in \emph{Temporal Order}); 3. exhibit strength in generating concise audios particularly for \texttt{Not} relation~(Subfig.~C). Overall, all the three methods excel in modelling \texttt{And} relation and then the three sub-relations in \emph{Temporal Order}, which is also reflected by the result in Table~\ref{tab:results-fourcates}.

\begin{wraptable}{r}{5.4cm}
\scriptsize
\vspace{-2mm}
\begin{tabular}{l}
\toprule
\textbf{1.} generation eval. contradicts with RiTTA eval.\\
\textbf{2.} \emph{TemOrder}/\emph{Compos} better than \emph{Count}/\emph{SpatDist}\\
\textbf{3.} event presence in \texttt{Not} is the highest;\\
\textbf{4.} relation correctness in \texttt{And} is the highest;\\
\textbf{5.} parsimony score in \texttt{Not} is the highest;\\
\textbf{6.} event presence accuracy rate is below $1\%$; \\
\textbf{7.} relation correctness accuracy rate is below $1\%$;\\
\textbf{8.} An average of 2 redundant audio events;\\
\bottomrule
\end{tabular}
\vspace{-3mm}
\caption{\small Key findings from experiments of TTA models on our RiTTA benchmark.}
\label{tab:benchmark_findings}
\end{wraptable} 

The key findings from the relation-aware benchmarking are summarized in the Table~\ref{tab:benchmark_findings}.

In summary, we conclude that, 1. existing TTA models lack the ability to model audio events relation described by the text prompt in the generated audio, emphasizing the importance of our work in systematically study audio events relation in TTA. 2. Existing TTA evaluation metrics fall short in accurately measuring audio events relations from the generated audio. Our proposed multi-stage relation evaluation framework suffices to measure the relation accuracy from various aspects.

\subsection{Finetuning Experimental Result}

We finetune Tango with the Adam optimizer and follow the finetuning strategy outlined in Tango~2~\citep{ghosal2023tango2}. The results, shown in Table~\ref{tab:finetune_result}, clearly demonstrate that finetuning Tango with relation aware datasets significantly improves its improves its ability to model audio event relations across both general and relation aware evaluations. This underscores the importance of benchmarking \emph{RiTTA} with both comprehensive datasets and tailored evaluation metrics. Given that we finetuned only the latent diffusion model with a relatively small dataset (1.6~k pairs), further improvements can be expected by jointly finetuning other modules~(\textit{e.g.}, FLAN-T5) with a larger dataset. Moreover, the boosted performance indicates that audio events relation can indeed be modelled by TTA methods. We hope this benchmark and initial exploration will inspire more researchers to explore this area further.

Two qualitative examples are in Fig.~\ref{fig:tango_finetune_vis}~A. It is evident that the finetuned Tango successfully models the \texttt{<before>} relation~(Table~\ref{tab:relation_test} and Fig.~\ref{fig:teasing_fig} show all existing TTA models fail on this case), and \texttt{<count>} relation. The mAPre score w.r.t. the 11 sub-relations is shown in Fig.~\ref{fig:tango_finetune_vis}~B~(the mARel, mAPar, mAMSR are in Fig.~\ref{fig:tango_finetune_vis_append} in Appendix). The results clearly indicates that finetuned Tango achieves significant improvements in target audio events presence across most relations, particularly in \texttt{<Or>}, \texttt{<And>}, \texttt{<simultaneity>}, \texttt{<after>} and \texttt{<before>}. The performance drop in \texttt{<Not>} relation may be attributed to the dataset preparation: as we pair \texttt{<Not>} relation with silent audio~(all-zero waveforms), yet the text prompts might contain arbitrary audio events. Finetuning on such created data may confuse the model, leading to ambiguity in audio events generation. Further investigation is needed to address this challenge. 

\begin{table}[t]
\centering
\scriptsize
\caption{Quantitative result across general and relation aware evaluation for Tango w/o finetuing.}
\vspace{-2mm}
\begin{tabular}{l|p{0.5cm}<{\centering}p{0.5cm}<{\centering}p{0.5cm}<{\centering}|p{0.5cm}<{\centering}p{0.5cm}<{\centering}p{0.5cm}<{\centering}|p{0.7cm}<{\centering}|p{0.5cm}<{\centering}p{0.9cm}<{\centering}p{0.9cm}<{\centering}p{.7cm}<{\centering}}
\hline
\multirow{2}{*}{Model}  & \multicolumn{3}{c|}{General Evaluation}  & \multicolumn{4}{c|}{Relation Aware Evaluation~($\uparrow$)} &  \multicolumn{4}{c}{mAMSR Across Four Main Relations}\\
\cline{2-12}
 & FAD~$\downarrow$ &  KL~$\downarrow$ & FD~$\downarrow$ & mAPre& mARel & mAPar &mAMSR &  \emph{Count} & \emph{TempOrder} & \emph{SpatDist} & \emph{Compos} \\
\hline
Tango~(\citeyear{ghosal2023tango}) &  10.79 &  90.26 &  39.46 & 11.13 &  2.27 &  9.88 &  3.10 & 0.16 & 3.44 & 0.82 & 8.10\\
Tango~(finetuing) & \cellcolor{topcolor}4.60  &  \cellcolor{topcolor}23.92 & \cellcolor{topcolor}27.03 & \cellcolor{topcolor}21.23 & \cellcolor{topcolor}10.78 & \cellcolor{topcolor}20.35 & \cellcolor{topcolor}48.67 & \cellcolor{topcolor}8.04 & \cellcolor{topcolor}324.10 & \cellcolor{topcolor}1.88 & \cellcolor{topcolor}44.42 \\
\hline
\end{tabular}
\label{tab:finetune_result}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{figs/ritta_finetune_vis.pdf}
    \caption{\small Qualitative visualization comparison of Tango w/o finetuning~(A) and mAPre w.r.t. 11 sub-relations. Listenable audios are provided in supplementary material.}
    \label{fig:tango_finetune_vis}
\end{figure}\section{Conclusion and Future Works}

Complex relationships within audio bring the world to life. While text-to-audio~(TTA) generation models have made remarkable progress in generating high-fidelity audio with fine-grained context understanding, they often fall short in capturing the relational aspect of audio events in real-world. The world around us is composed of interconnected audio events, where audio event rarely occurs in isolation. Simply generating single sound sources is insufficient for producing realistic audio that reflects the richness of the world.

To analyze the capabilities of current state-of-the-art TTA generative models, we first conduct a systematic study of these models in audio event relation modeling. We introduce a benchmark for this task by creating a comprehensive relational corpus covering all potential relations in the real-world scenarios. Further, we propose new evaluation metric framework to assess audio event relation modeling from various perspectives. Additionally, we propose a finetuning strategy to boost existing models' ability in modelling audio events relation, and we show improvement across all relation metrics. Finally, we will release both the dataset and the code for the evaluation metrics, which will be useful for future research in this domain.

Going forward, our work provides a unique research opportunity to bring the world to life by exploring ways to generate long-term audio events to acoustically understand the physical world. Further, understanding the successes and failures of these models in generating such complex audio events is another promising research direction. This analysis could lead to further improvements in TTA models and their applications in areas such as virtual reality, cinema and immersive media. 
\newpage

\end{document}