\title{Text-to-Audio Generation using Instruction-Guided Latent~Diffusion~Model}

\begin{document}

\title{\modeltitle{}: Improving Diffusion-based Text-to-Audio Generation using Direct Preference Optimization}

\title{\modeltitle{}: Improving Diffusion-based Text-to-Audio Generation using Direct Preference Optimization based Alignment}

\title{\modeltitle{}: Enhancing Diffusion-based Text-to-Audio Generation through Direct Preference Optimization-based Alignment}

\title{\modeltitle{}: Aligning Diffusion-based Text-to-Audio Generative Models through Direct Preference Optimization}

\title{\modeltitle{}: Aligning Diffusion-based Text-to-Audio Generations through Direct Preference Optimization}

\author{Navonil Majumder}
\affiliation{%
  \institution{Singapore University of Technology and Design, Singapore}
  \country{}
}
\authornote{Authors contributed equally.}

\author{Chia-Yu Hung}
\affiliation{%
  \institution{Singapore University of Technology and Design, Singapore}
  \country{}
}
\authornotemark[1]

\author{Deepanway Ghosal}
\affiliation{%
  \institution{Singapore University of Technology and Design, Singapore}
    \country{}
}
\authornotemark[1]

\author{Wei-Ning Hsu}
\affiliation{%
  \institution{Meta AI, USA}
    \country{}
}

\author{Rada Mihalcea}
\affiliation{%
  \institution{University of Michigan, USA}
    \country{}
}

\author{Soujanya Poria}
\affiliation{%
  \institution{Singapore University of Technology and Design, Singapore}
  \country{}
}

 \begin{teaserfigure}
 \centering
\begin{minipage}[t]{\linewidth}
  \begin{center}
    \includegraphics[width=0.9\linewidth]{images/tango2-teaser.pdf}
  \end{center}
\end{minipage}
\vspace{0.3cm}
\begin{minipage}[t]{\linewidth}
  \centering
  \faGithub: \textcolor{red}{\url{https://github.com/declare-lab/tango}} \\
  \faDatabase: \textcolor{blue}{\url{https://huggingface.co/datasets/declare-lab/audio-alpaca}}\\
  \faAnchor: \textcolor{green}{\url{https://huggingface.co/declare-lab/tango2}}\\
  \faGlobe : \textcolor{magenta}{\url{https://tango2-web.github.io/}}
\end{minipage}\end{teaserfigure}

\begin{abstract}
Generative multimodal content is increasingly prevalent in much of the content creation arena, as it has the potential to allow artists and media personnel to create pre-production mockups by quickly bringing their ideas to life. The generation of audio from text prompts is an important aspect of such processes in the music and film industry. Many of the recent diffusion-based text-to-audio models focus on training increasingly sophisticated diffusion models on a large set of datasets of prompt-audio pairs. These models do not explicitly focus on the presence of concepts or events and their temporal ordering in the output audio with respect to the input prompt. Our hypothesis is focusing on how these aspects of audio generation could improve audio generation performance in the presence of limited data. As such, in this work, using an existing text-to-audio model Tango, we synthetically create a preference dataset where each prompt has a winner audio output and some loser audio outputs for the diffusion model to learn from. The loser outputs, in theory, have some concepts from the prompt missing or in an incorrect order. We fine-tune the publicly available Tango text-to-audio model using diffusion-DPO (direct preference optimization) loss on our preference dataset and show that it leads to improved audio output over Tango and AudioLDM2, in terms of both automatic- and manual-evaluation metrics.
\end{abstract}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010178.10010179</concept_id>
       <concept_desc>Computing methodologies~Natural language processing</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10002951.10003227.10003251</concept_id>
       <concept_desc>Information systems~Multimedia information systems</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Natural language processing}
\ccsdesc[500]{Information systems~Multimedia information systems}

\keywords{Multimodal AI, Text-to-Audio Generation, Diffusion Models, Large Language Models, Preference Optimization}

\maketitle
\section{Introduction}

Generative AI is increasingly turning into a mainstay of our daily lives, be it directly through using ChatGPT~\cite{chatgpt}, GPT-4~\cite{gpt4} in an assistive capacity, or indirectly by consuming AI-generated memes, generated using models like StableDiffusion~\cite{rombach2022high}, DALL-E 3~\cite{dalle2,BetkerImprovingIG}, on social media platforms. Nonetheless, there is a massive demand for AI-generated content across industries, especially in the multimedia sector. Quick creation of audio-visual content or prototypes would require an effective text-to-audio model along with text-to-image and -video models. Thus, improving the fidelity of such models with respect to the input prompts is paramount.

Recently, supervised fine-tuning-based direct preference optimization~\cite{rafailov2023direct} (DPO) has emerged as a cheaper and more robust alternative to reinforcement learning with human feedback (RLHF) to align LLM responses with human preferences. This idea is subsequently adapted for diffusion models by \citet{wallace2023diffusion} to align the denoised outputs to human preferences. In this work, we employ this DPO-diffusion approach to improve the semantic alignment between input prompt and output audio of a text-to-audio model. Particularly, we fine-tune the publicly available text-to-audio latent diffusion model \textsc{Tango}~\cite{ghosal2023text} on our synthesized preference dataset with DPO-diffusion loss. This preference dataset contains diverse audio descriptions (\emph{prompts}) with their respective preferred (\emph{winner}) and undesirable (\emph{loser}) audios. The preferred audios are supposed to perfectly reflect their respective textual descriptions, whereas the undesirable audios have some flaws, such as some missing concepts from the prompt or in an incorrect temporal order or high noise level. To this end, we perturbed the descriptions to remove or change the order of certain concepts and passed them to \textsc{Tango} to generate undesirable audios. Another strategy that we adopted for undesirable audio generation was adversarial filtering: generate multiple audios from the original prompt and choose the audio samples with CLAP-score below a certain threshold. We call this preference dataset \dataset{}. To mitigate the effect of noisy preference pairs stemming from automatic generation, we further choose a subset of samples for DPO fine-tuning based on certain thresholds defined on the CLAP-score differential between preferred and undesirable audios and the CLAP-score of the undesirable audios. This likely ensures a minimal proximity to the input prompt, while guaranteeing a minimum distance between the preference pairs.

We experimentally show that fine-tuning \textsc{Tango} on the pruned \dataset{} yields \model{} that significantly surpasses \textsc{Tango} and \textsc{AudioLDM2} in both objective and human evaluations. Moreover, exposure to the contrast between good and bad audio outputs during DPO fine-tuning likely allows \model{} to better map the semantics of the input prompt into the audio space, despite relying on the same dataset as \textsc{Tango} for synthetic preference data-creation.

The broad contributions of this paper are the following:
\begin{enumerate}%[itemsep=0pt, leftmargin=*, wide, labelwidth=0pt, labelindent=0pt, parsep=0pt]
    \item We develop a cheap and effective heuristics for semi automatically creating a preference dataset for text-to-audio generation;
    \item On the same note, we also share the preference dataset \dataset{} for text-to-audio generation that may aid in the future development of such models;
    \item Despite not sourcing additional out-of-distribution text-audio pairs over \textsc{Tango}, our model \model{} outperforms both \textsc{Tango} and \textsc{AudioLDM2} on both objective and subjective metrics;
    \item \model{} demonstrates the applicability of diffusion-DPO in audio generation.
\end{enumerate}

\section{Related Work}
\label{sec:related-work}

Text-to-audio generation has garnered serious attention lately thanks to models like AudioLDM~\cite{liu2023audioldm}, Make-an-Audio~\cite{huang2023make}, Tango~\cite{ghosal2023text}, and Audiogen~\cite{kreuk2022audiogen}. These models rely on diffusion architectures for audio generation from textual prompts. Recently, AudioLM~\cite{borsos2023audiolm} was proposed which utilizes the state-of-the-art semantic model w2v-Bert~\cite{DBLP:conf/asru/ChungZHCQPW21} to generate semantic tokens from audio prompts. These tokens condition the generation of acoustic tokens, which are decoded using the acoustic model SoundStream~\cite{DBLP:journals/taslp/ZeghidourLOST22} to produce audio. The semantic tokens generated by w2v-Bert are crucial for conditioning the generation of acoustic tokens, subsequently decoded by SoundStream.

AudioLDM~\cite{liu2023audioldm} is a text-to-audio framework that employs CLAP \cite{wu2023large}, a joint audio-text representation model, and a latent diffusion model (LDM). Specifically, an LDM is trained to generate latent representations of melspectrograms obtained using a Variational Autoencoder (VAE). During diffusion, CLAP embeddings guide the generation process. \texttt{Tango}~\cite{ghosal2023tango} utilizes the pre-trained VAE from AudioLDM and replaces the CLAP model with a fine-tuned large language model: FLAN-T5. This substitution aims to achieve comparable or superior results while training with a significantly smaller dataset.

In the realm of aligning generated audio with human perception, \citet{liao2024baton} recently introduced BATON, a framework that initially gathers pairs of audio and textual prompts, followed by annotating them based on human preference. This dataset is subsequently employed to train a reward model. The reward generated by this model is then integrated into the standard diffusion loss to guide the network, leveraging feedback from the reward model. However, our approach significantly diverges from this work in two key aspects: 1) we automatically construct a \emph{pairwise} preference dataset, referred to as \dataset{}, utilizing various techniques such as LLM-guided prompt perturbation and re-ranking of generated audio from Tango using CLAP scores, and 2) we then train Tango on \dataset{} using diffusion-DPO to generate audio samples preferred by human perception.

\section{Background}

\subsection{Overview of \textsc{Tango}}

Tango, proposed by \citet{ghosal2023text}, primarily relies on a latent diffusion model (LDM) and an instruction-tuned LLM for text-to-audio generation. It has three major components: \begin{enumerate}
    \item Textual-prompt encoder
    \item Latent diffusion model (LDM)
    \item Audio VAE and Vocoder
  \end{enumerate}  
The textual-prompt encoder encodes the input description of the audio. Subsequently, the textual representation is used to construct a latent representation of the audio or audio prior from standard Gaussian noise, using reverse diffusion. Thereafter, the decoder of the mel-spectrogram VAE constructs a mel-spectrogram from the latent audio representation. This mel-spectrogram is fed to a vocoder to generate the final audio. 

\subsubsection{Textual Prompt Encoder}
Tango utilizes the pre-trained LLM \textsc{Flan-T5-Large} (780M)~\cite{https://doi.org/10.48550/arxiv.2210.11416} as the text encoder ($E_{text}$) to acquire text encoding $\tau\in \mathbb{R}^{L\times d_{text}}$, where $L$ and $d_{text}$ represent the token count and token-embedding size, respectively.

\subsubsection{Latent Diffusion Model}
For ease of understanding, we briefly introduce the LDM of Tango in this section. 
The latent diffusion model (LDM)~\cite{rombach2022high} in Tango is derived from the work of \citet{Liu2023AudioLDMTG}, aiming to construct the audio prior $x_0$ guided by text encoding $\tau$. This task essentially involves approximating the true prior $q(x_0|\tau)$ using parameterized $p_\theta(x_0|\tau)$.

LDM achieves this objective through forward and reverse diffusion processes. The forward diffusion represents a Markov chain of Gaussian distributions with scheduled noise parameters $0 < \beta_1 < \beta_2 < \cdots < \beta_N < 1$, facilitating the sampling of noisier versions of $x_0$:
\begin{flalign}
q(x_n|x_{n-1}) &= \mathcal{N}(\sqrt{1-\beta_n} x_{n-1}, \beta_n \mathbf{I}),  \label{eq:forward_diff}\\
q(x_n|x_0) &= \mathcal{N}(\sqrt{\overline\alpha_n} x_0, (1-\overline\alpha_n)\mathbf{I}) \label{eq:quick_forward_diff},
\end{flalign}
where $N$ is the number of forward diffusion steps, $\alpha_n = 1 - \beta_n$, and $\overline\alpha_n = \prod_{i=1}^n \alpha_n$. \citet{Song2020DenoisingDI} show that \cref{eq:quick_forward_diff} conveniently follows from \cref{eq:forward_diff} through reparametrization trick that allows direct sampling of any $x_n$ from $x_0$ via a non-Markovian process: 
\begin{equation}
x_n = \sqrt{\overline\alpha_n} x_0 + (1-\overline\alpha_n)\epsilon, \label{eq:x_n_sampling}
\end{equation}
where the noise term $\epsilon\sim \mathcal{N}(\mathbf{0}, \mathbf{I})$. The final step of the forward process yields $x_N\sim \mathcal{N}(\mathbf{0}, \mathbf{I})$.

The reverse process denoises and reconstructs $x_0$ through text-guided noise estimation ($\hat{\epsilon}_\theta$) using loss
\begin{align}
    \mathcal{L}_{LDM} = \sum_{n=1}^N\gamma_n \mathbb{E}_{ \epsilon_n\sim \mathcal{N}(\mathbf{0}, \mathbf{I}), x_0} || \epsilon_n - \hat\epsilon_\theta^{(n)}(x_n, \tau) ||_2^2, \label{eq:LDM-loss}
\end{align}
where $x_n$ is sampled according to \cref{eq:x_n_sampling} using standard normal noise $\epsilon_n$, $\tau$ represents the text encoding for guidance, and $\gamma_n$ denotes the weight of reverse step $n$~\cite{hang2023efficient}, interpreted as a measure of signal-to-noise ratio (SNR) relative to $\alpha_{1:N}$. The estimated noise is then employed for the reconstruction of $x_0$:

\begin{flalign}
    p_\theta(x_{0:N}|\tau) &= p(x_N) \prod_{n=1}^N p_\theta(x_{n-1}|x_n, \tau), \\
    p_\theta(x_{n-1}|x_n, \tau) &= \mathcal{N}(\mu^{(n)}_\theta(x_n, \tau), \Tilde{\beta}^{(n)}),\\
    \mu_\theta^{(n)}(x_n, \tau) &= \frac{1}{\sqrt{\alpha_n}}[x_n - \frac{1 - \alpha_n}{\sqrt{1 - \overline\alpha_n}}\hat\epsilon_\theta^{(n)}(x_n, \tau)],\\
    \Tilde{\beta}^{(n)} &=\frac{1 - \bar{\alpha}_{n-1}}{1 - \bar{\alpha}_n} \beta_n.
\end{flalign}
The parameterization of noise estimation $\hat\epsilon_\theta$ involves utilizing U-Net~\cite{10.1007/978-3-319-24574-4_28}, incorporating a cross-attention component to integrate the textual guidance $\tau$.
\subsubsection{Audio VAE and Vocoder}
The audio variational auto-encoder (VAE)~\cite{Kingma2013AutoEncodingVB} compresses the mel-spectrogram of an audio sample, $m\in \mathbb{R}^{T\times F}$, into an audio prior $x_0\in \mathbb{R}^{C\times T/r\times F/r}$, where $C$, $T$, $F$, and $r$ denote the number of channels, time-slots, frequency-slots, and compression level, respectively. The latent diffusion model (LDM) reconstructs the audio prior $\hat x_0$ using input-text guidance $\tau$. 
Both the encoder and decoder consist of ResUNet blocks~\cite{Kong2021DecouplingMA} and are trained by maximizing the evidence lower-bound (ELBO)~\cite{Kingma2013AutoEncodingVB} and minimizing adversarial loss~\cite{Isola2016ImagetoImageTW}.
Tango utilizes the checkpoint of the audio VAE provided by \citet{Liu2023AudioLDMTG}.

As a vocoder to convert the audio-VAE decoder-generated mel-spectrogram into audio, Tango employs HiFi-GAN~\cite{kong2020hifi} which is also utilized by \citet{Liu2023AudioLDMTG}.

Finally, Tango utilizes a data augmentation method that merges two audio signals while considering human auditory perception. This involves computing the pressure level of each audio signal and adjusting the weights of the signals to prevent the dominance of the signal with higher pressure level over the one with lower pressure level. Specifically, when fusing two audio signals, the relative pressure level is computed using the following equation:
\begin{flalign}
    p = (1 + 10^\frac{G_1 - G_2}{20})^{-1}, \label{eq:gain}
\end{flalign}
Here $G_1$ and $G_2$ are the pressure levels of signal $x_1$ and $x_2$. Then the audio signals are mixed using the equation below:
\begin{flalign}
    \text{mix}(x_1, x_2) = \frac{p x_1 + (1-p) x_2}{\sqrt{p^2 + (1-p)^2}}.
\end{flalign}
The denominator is to account for the fact that the energy of a sound wave is proportional to the square of its amplitude as shown in \citet{DBLP:journals/corr/abs-1711-10282}. Note that in this augmentation, textual prompts are also concatenated. 
\subsection{Preference Optimization for Language Models}
Tuning Large Language Models (LLMs) to generate responses according to human preference has been a great interest to the ML community. The most popular approach for aligning language models to human preference is reinforcement learning with human feedback (RLHF). It comprises the following steps~\cite{rafailov2023direct}:

\paragraph{\bf Supervised Fine Tuning (SFT)} First, the pre-trained LLM undergoes supervised fine-tuning on high-quality downstream tasks to obtain the fine-tuned model $\pi^{SFT}$.
\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.9\textwidth]{images/tango2.pdf}
    \caption{\small An illustration of our pipeline for text-to-audio alignment. The top part depicts the preference dataset creation where three strategies are deployed to generate the undesirable audio outputs to the input prompts. These samples are further filtered to form \dataset{}. This preference dataset is finally used to align \textsc{Tango} using DPO-diffusion loss (\cref{eq:DPO-Diff}), resulting in \model{}.}
    \label{fig:pipeline}
\end{figure*}
\paragraph{\bf Reward Modeling} Next, $\pi^{SFT}$ is prompted with an input $\tau$ to generate multiple responses. These responses are then shown to human labelers to rank. Once such a rank is obtained, $x^w \succ x^l \mid \tau $ indicating $x^w$ is preferred over $x^l$, the task is to model these preferences. Among several popular choices of preference modeling, Bradley-Terry (BT) is the most popular one which relies on the equation below:

\begin{align}
    p^*(x^w \succ x^l \mid \tau) = \frac{\exp(r^*(\tau, x^w))}{\exp(r^*(\tau, x^w)) + \exp(r^*(\tau, x^l))} \label{eqn:BT}
\end{align}
The overall idea is to learn the human preference distribution $p^*$. $r^*(\tau, x)$ is a latent reward function that generates the preferences. With a static dataset created by human annotators, $\mathcal{D} = \left\{ \left(\tau_{(i)}, x^{w}_{(i)}, x^{l}_{(i)}\right) \right\}_{i=1}^{N}$, one can train a reward model  $r_\phi(\tau, x)$ using maximum likelihood estimation. The negative log-likelihood loss of this training can be written as follows:
\begin{align}
    \mathcal{L}_R(r_{\phi}, \mathcal{D}) = -\mathbb{E}_{(\tau, x^w, x^l) \sim \mathcal{D}} \left[ \log \sigma (r_{\phi}(\tau, x^w) - r_{\phi}(\tau, x^l)) \right]  \label{eq:NLL}
\end{align}
This formulation considers framing the problem as a binary classification problem. 

\paragraph{\bf RL Optimization}
The final step is to leverage $r_\phi(\tau, x)$ to feedback the language model. As explained by \citet{rafailov2023direct}, this can be embedded into the following learning objective:
\begin{align}
\label{eq:rlhf}
    \max_{\pi_{\theta}} \mathbb{E}_{\tau \sim \mathcal{D}, x \sim \pi_{\theta}(x|\tau)} \left[ r_{\phi}(\tau, x) \right] - \beta D_{KL} \left[ \pi_{\theta}(x|\tau) \parallel \pi_{\text{ref}}(x|\tau) \right]  
\end{align}
Here, $\pi_{\text{ref}}$ represents the reference model, which in this context is the supervised fine-tuned model denoted as $\pi^{\text{SFT}}$. $\pi_{\theta}$ stands for the policy language model, intended for enhancement based on feedback from $r_\phi(\tau, x)$. $\beta$ governs $\pi_{\theta}$ to prevent significant divergence from $\pi_{\text{ref}}$. This control is crucial as it ensures that the model stays close to the distributions upon which $r_\phi(\tau, x)$ was trained. Since the outputs from LLM are discrete, \cref{eq:rlhf} becomes non-differentiable, necessitating reinforcement learning methods like PPO to address this objective.

\section{Methodology}

The two major parts of our approach (i) creation of preference dataset \dataset{} and (ii) DPO for alignment are outlined in \cref{fig:pipeline}.

\subsection{Creation of \dataset{}}
\label{sec:pref-dataset}

\subsubsection{Audio Generation from Text Prompts}

Our first step is to create audio samples from various text prompts with the pre-trained Tango model. We follow three different strategies as follows:

\paragraph{\bf \textit{Strategy 1}: Multiple Inferences from the same Prompt} In the first setting, we start by selecting a subset of diverse captions from the training split of the \texttt{AudioCaps} dataset. We use the sentence embedding model \textsf{gte-large}\footnote{\url{hf.co/thenlper/gte-large}} \cite{li2023towards} to compute dense embedding vectors of all the captions in the training set. We then perform K-Means clustering on the embedded vectors with 200 clusters. Finally, we select 70 samples from each cluster to obtain a total of 14,000 captions. We denote the selected caption set as $\mathcal{T}_1$.

The captions selected through the above process constitute the seed caption set. 
Now, we follow two settings to generate audio samples from these captions:

\begin{enumerate}
    \item \textbf{Strategy 1.1}: Prompt \textsf{Tango-full-FT} with the caption to generate four different audio samples with 5, 25, 50, and 100 denoising steps. All samples are created with a guidance scale of 3. 

    \item \textbf{Strategy 1.2}: Prompt \textsf{Tango-full-FT} with the caption to generate four different audio samples each with 50 denoising steps. All samples are created with a guidance scale of 3.
    
\end{enumerate}

In summary, we obtain $(\tau, x_1, x_2, x_3, x_4)$ from \emph{\textbf{Strategy 1}}, where $\tau$ denotes the caption from $\mathcal{T}_1$ and $x_i$ denotes the audios generated from $\tau$.

\begin{table*}[ht!]
\centering
\small
\caption{Prompts used in GPT-4 for creating the perturbations and the corresponding output.}
 \resizebox{0.85\linewidth}{!}{
   \begin{tabular}{p{5.5cm}|p{8cm}|p{7cm}}
   \toprule
   \textbf{\textit{Strategy} and Original Caption} & \textbf{Prompt to GPT-4} & \textbf{Output from GPT-4} \\
   \midrule
   \textit{Strategy 2: Perturbed Prompts} & I have an audio clip for which the original caption is as follows: People cheering and race cars racing by. Can you generate five candidate captions that would satisfy the following requirements: & 1. Crowd applauding and bicycles speeding past. \\
   People cheering and race cars racing by. & i) Would be closely related to the audio clip and the original caption. & 2. Spectators clapping and trains rushing by. \\
   & ii) Would contain inaccuracies in terms of describing the audio clip i.e. they would be somewhat wrong captions for the audio clip. & 3. Audience cheering and horses galloping past. \\
   & iii) However, the new captions should not be completely unrelated. Always keep some concepts from the original caption in the new one. & 4. Fans shouting and airplanes flying by. \\
   & iv) Would be of similar length to the original caption.  & 5. Group celebrating and motorcycles revving past. \\
   & Generate only the captions in separate lines so that I can programmatically extract them later. & \\
   \midrule
   \textit{Strategy 3: Temporally-Perturbed Prompts} & I have an audio clip for which the original caption is as follows: A man is speaking then a sewing machine briefly turns on and off. Can you generate five candidate captions that would satisfy the following requirements: & 1. A sewing machine briefly starts, then a man begins speaking. \\
   A man is speaking then a sewing machine briefly turns on and off. & i) Would be closely related to the audio clip and the original caption. & 2. The sound of a sewing machine is heard after a man's voice. \\
   & ii) Would have a change of order of the events described in the original caption. & 3. A man's voice is followed by the noise of a blender. \\
   & iii) Would contain inaccuracies in terms of describing the audio clip i.e. they would be somewhat wrong captions for the audio clip. & 4. A woman speaks and then a sewing machine is turned on. \\
   & iv) However, the new captions should not be completely unrelated. Always keep some concepts from the original caption in the new one. & 5. The noise of a sewing machine is interrupted by a man talking. \\
   & v) Would be of similar length to the original caption. & \\
   & Generate only the captions in separate lines so that I can programmatically extract them later. & \\
   \bottomrule
   \end{tabular}
   }
  \label{tab:data_generation_example}
\end{table*}
\paragraph{\bf \textit{Strategy 2}: Inferences from Perturbed Prompts} We start from the selected set $\mathcal{T}_1$ and make perturbations of the captions using the GPT-4 language model~\cite{gpt4}. For a caption $\tau$ from $\mathcal{T}_1$, we denote $\tau_1$ as the perturbed caption generated from GPT-4. We add specific instructions in our input prompt to make sure that  $\tau_1$ is semantically or conceptually close to $\tau$. We show an illustration of the process in \cref{tab:data_generation_example}. In practice, we create five different perturbed $\tau_1$ for each $\tau$ from GPT-4, as shown in \cref{tab:data_generation_example}.

We then prompt \textsf{Tango-full-FT} with $\tau$ and $\tau_1$ to generate audio samples $x_{\tau}$ and $x_{\tau_1}$. We use 50 denoising steps with a guidance scale of 3 to generate these audio samples. 

To summarize, we obtain $(\tau, x_{\tau}, x_{\tau_1})$ from \emph{\textbf{Strategy 2}}. Note that, we considered $\tau_1$ only to generate the audio sample $x_{\tau_1}$. We do not further consider $\tau_1$ while creating the preference dataset. 

\paragraph{\bf \textit{Strategy 3}: Inferences from Temporally Perturbed Prompts}

This strategy is aimed at prompts that describe some composition of sequence and simultaneity of events. To identify such prompts in \texttt{AudioCaps}' training dataset, as a heuristics, we look for the following keywords in a prompt: \emph{while}, \emph{before}, \emph{after}, \emph{then}, or \emph{followed}. We denote the set of such prompts as $\mathcal{T}_2$.

For each caption $\tau_2$ in $\mathcal{T}_2$, we then prompt GPT-4 to create a set of temporal perturbations. The temporal perturbations include changing the order of the events in the original caption, or introducing a new event or removing an existing event, etc. We aim to create these temporal perturbations by providing specific instructions to GPT-4, which we also illustrate in \cref{tab:data_generation_example}. 

We denote the temporally perturbed caption as $\tau_2$. We then follow the same process as mentioned earlier in \textit{Strategy 2} to create the audio samples $x_{\tau}$ and $x_{\tau_2}$. Finally, we pair the $(\tau, x_{\tau}, x_{\tau_2})$ samples from this strategy. Analogous to the previous strategy, the $\tau_2$ is only used to create the $x_{\tau_2}$, and is not used anywhere else for preference data creation. 

We collect the paired text prompt and audio samples from the three strategies and denote it overall as $(\tau, \langle x \rangle)$, where $\langle x \rangle$ indicates the set of 4 or 2 generated audio samples depending upon the corresponding strategy. 

\subsubsection{Ranking and Preference-Data Selection}
\label{sec:audio_alpaca_creation}
We first create a pool of candidate preference data for the three strategies as follows:

\paragraph{\bf \textit{For Strategy 1}} Let's assume we have an instance $(\tau, \langle x \rangle)$ from Strategy 1. 
We first compute the CLAP matching score following \citet{wu2023large} between $\tau$ and all the four audio samples in $\langle x \rangle$. We surmise that the sample in $\langle x \rangle$ that has the highest matching score with $\tau$ is most aligned with $\tau$, compared to the other three audio samples that have a relatively lower matching score. We consider this audio with the highest matching score as the winning sample $x^w$ and the other three audio samples as the losing sample $x^l$. In this setting, we can thus create a pool of three preference data points: $(\tau, x^w, x^l)$, for the three losing audio samples $x^l$.

\paragraph{\bf \textit{For Strategy 2 and 3}} Let's assume we have an instance $(\tau, \langle x \rangle)$ from Strategy 2 or 3. We compute the CLAP matching score between i) $\tau$ with $x_{\tau}$, and ii) $\tau$ with the $x_{\tau_1}$ or $x_{\tau_2}$, corresponding to the strategy. We consider only those instances where the CLAP score of i) is higher than the CLAP score of ii). For these instances, we use $x_{\tau}$ as the winning audio $x^w$ and $x_{\tau_1}$ or $x_{\tau_2}$ as the losing audio $x^l$ to create the preference data point: $(\tau, x^w, x^l)$.

\paragraph{\bf \textit{Final Selection}} We want to ensure that the winning audio sample $x^w$ is strongly aligned with the text prompt 
$\tau$. At the same time, the winning audio sample should have a considerably higher alignment with the text prompt than the losing audio sample. We use the CLAP score as a measurement to fulfill these conditions. The CLAP score is measured using cosine similarity between the text and audio embeddings, where higher scores indicate higher alignment between the text and the audio. We thus use the following conditions to select a subset of instances from the pool of preference data:

\begin{enumerate}[leftmargin=0.5cm]
    \item The winning audio must have a minimum CLAP score of $\alpha$ with the text prompt to ensure that the winning audio is strongly aligned with the text prompt.
    \item The losing audio must have a minimum CLAP score of $\beta$ with the text prompt to ensure that we have semantically close negatives that are useful for preference modeling.
    \item The winning audio must have a higher CLAP score than the losing audio w.r.t to the text prompt.
    \item We denote $\Delta$ to be the difference in CLAP score between the text prompt with the winning audio\footnote{In our paper, we employ the terms "winner" and "preferred" interchangeably. Likewise, we use "loser" and "undesirable" interchangeably throughout the text.} and the text prompt with the losing audio. The $\Delta$ should lie between certain thresholds, where the lower bound will ensure that the losing audio is not too close to the winning audio, and the upper bound will ensure that the losing audio is not too undesirable.
\end{enumerate}

We use an \emph{ensemble filtering} strategy based on two different CLAP models: \textsf{630k-audioset-best} and \textsf{630k-best} \cite{wu2023large}. This can reduce the effect of noise from individual CLAP checkpoints and increase the robustness of the selection process. In this strategy, samples are included in our preference dataset if and only if they satisfy all the above conditions based on CLAP scores from both of the models. 
We denote the conditional scores mentioned above as $\alpha_1, \beta_1, \Delta_1$, and $\alpha_2, \beta_2, \Delta_2$ for the two CLAP models, respectively. Based on our analysis of the distribution of the CLAP scores as shown in \Cref{fig:alpha-delta}, we choose their values as follows: $\alpha_1=0.45, \alpha_2=0.60$, $\beta_1=0.40, \beta_2=0.0$, $0.05 \leq \Delta_1 \leq 0.35$, and $0.08 \leq \Delta_2 \leq 0.70$.

Finally, our preference dataset \dataset{} has a total of $\approx$ 15k samples after this selection process. We report the distribution of \dataset{} in \cref{tab:strategies}. 

\begin{figure}[b]
    \centering
    \includegraphics[width=0.8\linewidth]{images/alpha1.pdf}
    \includegraphics[width=0.8\linewidth]{images/delta1.pdf}
    \caption{\small{The distribution of $\alpha_1$ and $\Delta_1$ scores in the unfiltered dataset. We see that for an unfiltered dataset: i) the winner audio sample is not always strongly aligned to the text prompt in the $\alpha_1$ plot; ii) winner and loser audio samples can be too close in the $\Delta_1$ plot. We thus choose the values of our $\alpha_1$, $\Delta_1$ and other selection parameters to ensure the filtered dataset is less noisy with more separation between the winner and loser audios.}
    }
    \label{fig:alpha-delta}
\end{figure}

\begin{table*}[h]
\centering
\caption{Statistics of \dataset{}.}
\label{tab:strategies}
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{lcccc}
\toprule
\textbf{Strategy} & \textbf{\# Samples} & \textbf{Avg. Winner Score} & \textbf{Avg. Loser Score}&  \textbf{Avg. Delta}\\
\midrule
Inference w/ Different Denoising Steps (Strategy 1.1) 
& 3004 & 0.645 & 0.447 &0.198\\
Inference w/ Same Denoising Steps (Strategy 1.2)
& 2725 & 0.647 & 0.494& 0.153 \\
GPT-4 Perturbed Prompts (Strategy 2)
& 4544 & 0.641 & 0.425 & 0.216 \\
GPT-4 Temporally Perturbed Prompts (Strategy 3)
& 4752 & 0.649 & 0.458 & 0.191 \\
\midrule
Overall & 15025 & 0.645 & 0.452 & 0.193\\
\bottomrule
\end{tabular}
}
\end{table*}

\subsection{DPO for Preference Modeling}

As opposed to RLHF, recently DPO has emerged as a more robust and often more practical and straightforward alternative for LLM alignment that is based on the very same BT preference model (\cref{eqn:BT}). In contrast with supervised fine-tuning (SFT) that only optimizes for the desirable (\emph{winner}) outputs, the DPO objective also allows the model to learn from undesirable (\emph{loser}) outputs, which is key in the absence of a high-quality reward model, as required for RLHF. To this end, the DPO objective is derived by substituting the globally optimal reward---obtained by solving \cref{eq:rlhf}---in the negative log-likelihood (NLL) loss (\cref{eq:NLL}).

This success spurred on \citet{wallace2023diffusion} to bring the same benefits of DPO to diffusion networks. However, unlike DPO, the goal for diffusion networks is to maximize the following learning objective (\cref{eq:diff-obj}) with a reward (\cref{eq:diff-reward}) defined on the entire diffusion path $x_{0:N}$:
\begin{flalign}
    \max_{\pi_\theta} \mathbb{E}_{\tau \sim \mathcal{D}, x_{0:N}\sim \pi_\theta(x_{0:N} | \tau)} &[r(\tau, x_0)] \nonumber \\
    - \beta D_\text{KL} &[ \pi_\theta(x_{0:N} | \tau) || \pi_\text{ref}(x_{0:N}|\tau)]. \label{eq:diff-obj}\\
    r(\tau, x_0) := \mathbb{E}_{\pi_\theta(x_{1:N} | x_0, \tau)} &[R(\tau, x_{0:N})], \label{eq:diff-reward}
\end{flalign}
Solving this objective and substituting the optimal reward in the NLL loss (\cref{eq:NLL}) yields the following DPO objective for diffusion:
\begin{flalign}
    &\mathcal{L}_\text{DPO-Diff} = - \mathbb{E}_{(\tau, x_0^w, x_0^l)\sim \mathcal{D_\text{pref}}} \log \sigma(\nonumber \\ 
    &\beta \mathbb{E}_{x^*_{1:N}\sim \pi_\theta(x^*_{1:N}|x^*_0, \tau)}[\log \frac{\pi_\theta(x^w_{0:N}|\tau)}{\pi_\text{ref}(x^w_{0:N}|\tau)} - \log \frac{\pi_\theta(x^l_{0:N}|\tau)}{\pi_\text{ref}(x^l_{0:N}|\tau)}]).
\end{flalign}
Now, applying Jensen's inequality by taking advantage of the convexity of $-\log \sigma$ allows the inner expectation to be pushed outside. Subsequently, approximating the denoising process with the forward process yields the following final form in terms of the L2 noise-estimation losses from LDM (\cref{eq:LDM-loss}):
\begin{flalign}
    \mathcal{L}_\text{DPO-Diff} := &- \mathbb{E}_{n, \epsilon^w, \epsilon^l} \log \sigma(-\beta N \omega(\lambda_n) (||\epsilon_n^w - \hat\epsilon_\theta^{(n)}(x_n^w, \tau)||_2^2 \nonumber \\ &- ||\epsilon_n^w - \hat\epsilon_\text{ref}^{(n)}(x_n^w, \tau)||_2^2 \nonumber \\ &- (||\epsilon_n^l - \hat\epsilon_\theta^{(n)}(x_n^l, \tau)||_2^2 - ||\epsilon_n^l - \hat\epsilon_\text{ref}^{(n)}(x_n^l, \tau)||_2^2)), \label{eq:DPO-Diff}
\end{flalign}
where $\mathcal{D}_\text{pref}:= \{(\tau, x_0^w, x_0^l)\}$ is our preference dataset \dataset{}, $\tau$, $x^w_0$, and $x^l_0$ being the input prompt, preferred, and undesirable output, respectively. Furthermore, $n\sim \mathcal{U}(0, N)$ is the diffusion step, $\epsilon_n^*\sim \mathcal{N}(0, \mathbb{I})$ and $x_n^*$ are noise and noisy posteriors, respectively, at some step $n$. $\lambda_n$ is the signal-to-noise ratio (SNR) and $\omega(\lambda_n)$ is a weighting function defined on SNR. We use \textsf{Tango-full-FT} as our reference model through its noise estimation $\hat\epsilon_\text{ref}$. 

\section{Experiments}

\subsection{Datasets and Training Details}

We fine-tuned our model starting from the \textsf{Tango-full-FT} checkpoint on our preference dataset \dataset{}.

As mentioned earlier in \Cref{sec:audio_alpaca_creation}, we have a total of 15,025 preference pairs in \dataset{}, which we use for fine-tuning. We use AdamW~\cite{loshchilov2017decoupled} with a learning rate of 9.6e-7 and a linear learning-rate scheduler for fine-tuning. Following \citet{wallace2023diffusion}, we set the $\beta$ in DPO loss (\cref{eq:DPO-Diff}) to 2000. We performed 1 epoch of supervised fine-tuning on the prompt and the preferred audio as training samples, followed by 4 epochs of DPO. The entirety of the fine-tuning was executed on two A100 GPUs which takes about 3.5 hours in total. We use a per GPU batch size of 4 and a gradient accumulation step of 4, resulting in an effective batch size of 32.

\subsection{Baselines}
We primarily compare \model{} to three strong baselines:

\begin{enumerate}[leftmargin=0.75cm]
    \item \textsc{\bf AudioLDM}~\cite{liu2023audioldm}: A text-to-audio model that uses CLAP~\cite{wu2023large}, a joint audio-text representation model, and a latent diffusion model (LDM). Specifically, the LDM is trained to generate the latent representations of melspectrograms obtained from a pre-trained Variational Autoencoder (VAE). During diffusion, CLAP text-embeddings guide the generation process.
    
    \item \textsc{\bf AudioLDM2}~\cite{liu2023audioldm2}: An any-to-audio framework which uses language of audio (LOA) as a joint encoding of audio, text, image, video, and other modalities. Audio modality is encoded into LOA using a self-supervised masked auto-encoder. The remaining modalities, including audio again, are mapped to LOA through a composition of GPT-2~\cite{radford2019language} and ImageBind~\cite{girdhar2023imagebind}. This joint encoding is used as a conditioning in the diffusion network for audio generation.
    
    \item \textsc{\bf Tango}~\cite{ghosal2023text}: Utilizes the pre-trained VAE from AudioLDM but replaces the CLAP text-encoder with an instruction-tuned large language model: FLAN-T5. As compared to AudioLDM, its data-augmentation strategy is also cognizant of the audio pressure levels of the source audios. These innovations attain comparable or superior results while training on a significantly smaller dataset.
\end{enumerate}

\textsc{Baton}~\cite{liao2024baton} represents another recent approach in human preference based text-to-audio modeling. It trains a reward model to maximize rewards through supervised fine-tuning, aiming to maximize the probability of generating audio from a textual prompt. As discussed in \Cref{sec:related-work}, \textsc{Baton}'s reward model is not trained using the pairwise preference objective presented in \Cref{eq:NLL}. In this approach, each text ($\tau$) and audio ($x$) pair is classified as 1 or 0, indicating whether human annotators favored the text-audio pair or not. Subsequently, this reward is incorporated into the generative objective function of the diffusion. This methodology stands in contrast to the prevailing approach in LLM alignment research. As of now, neither the dataset nor the code has been made available for comparison.
\begin{table*}[t]
\centering
\caption{Text-to-audio generation results on AudioCaps evaluation set. Due to time and budget constraints, we could only subjectively evaluate AudioLDM 2-Full-Large and Tango-full-FT. Notably these two models are considered open-sourced SOTA models for text-to-audio generation as reported in \cite{vyas2023audiobox}.}

\begin{tabular}{l|c|cccc|cccc|cc}
\toprule
\multirow{2}{*}{Model} & \multirow{2}{*}{\#Parameters} & \multicolumn{4}{c|}{Objective -- Holistic} & \multicolumn{4}{c|}{Objective -- Temporal} & \multicolumn{2}{c}{Subjective} \\
&  & FAD $\downarrow$ &  KL $\downarrow$ & IS $\uparrow$ & CLAP $\uparrow$ & OER $\downarrow$ & DUR $\downarrow$ & FREQ $\downarrow$ & TIME $\uparrow$ & OVL $\uparrow$ & REL $\uparrow$  \\
\midrule
\textsc{AudioLDM-M-Full-FT}  & $416$M  & $2.57$  & $1.26$ &  $8.34$ & $0.43$
 & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\\
\textsc{AudioLDM-L-Full}  & $739$M  & $4.18$  & $1.76$ &  $7.76$ & $0.43$
 & $-$ & $-$ & $-$ & $-$ &$-$ & $-$\\
\midrule

\textsc{AudioLDM 2-Full} & $346$M & \textbf{$2.18$} & $1.62$ & $6.92$ & $0.43$  & $-$ & $-$ & $-$ & $-$ & $-$ & $-$  \\
\textsc{AudioLDM 2-Full-Large}  &  $712$M & $\textbf{2.11}$ & $1.54$ & $8.29$ & $0.44$  & $-$ & $-$ & $-$ & $-$ & $3.56$ & $3.19$  \\

\midrule

\textsc{Tango-full-FT}  & $866$M & $2.51$ & $1.15$ & $7.87$ & $0.54$ & 0.882 & \bf 3.535 & 1.611 & 0.577 & $3.81$ & $3.77$  \\
\model{}  & $866$M & $2.69$ & $\textbf{1.12}$ & $\textbf{9.09}$ & $\textbf{0.57}$ & \bf 0.87 & 3.586 & \bf 1.548 & \bf 0.61 & $\textbf{3.99}$ & $\textbf{4.07}$  \\
\quad w/o Strategy 2 \& 3   & $866$M & $2.64$ & $1.13$ & $8.06$ & $0.54$  & $-$ & $-$ & $-$ & $-$ & $-$ & $-$  \\
\quad w/o Strategy 1   & $866M$ & $2.47$ & $1.13$ & $8.58$ & $0.56$  & $-$ & $-$ & $-$ & $-$ & $-$ & $-$  \\
\quad w/o Strategy 2   & $866$M & $2.28$ & $\textbf{1.12}$ & $8.38$ & $0.55$  & $-$ & $-$ & $-$ & $-$ & $-$ & $-$  \\
\quad w/o Strategy 3   & $866$M & $2.46$ & $1.13$ & $8.63$ & $0.56$  & $0.88$ & $3.63$ & $1.577$ & $0.588$ & $-$ & $-$  \\
\bottomrule
\end{tabular}
\label{tab:main-results}
\end{table*}

\subsection{Evaluation Metrics}

\paragraph{\bf Holistic Objective Metrics} We evaluate the generated audio samples in a holistic fashion using the standard Frechet Audio Distance (FAD), KL divergence, Inception score (IS), and CLAP score \cite{liu2023audioldm}. \emph{FAD} is adapted from Frechet Inception Distance (FID) and measures the distribution-level gap between generated and reference audio samples. \emph{KL divergence} is an instance-level reference-dependent metric that measures the divergence between the acoustic event posteriors of the ground truth and the generated audio sample. FAD and KL are computed using PANN, an audio-event tagger. \emph{IS} evaluates the specificity and coverage of a set of samples, not needing reference audios. IS is inversely proportional to the entropy of the instance posteriors and directly proportional to the entropy of the marginal posteriors. \emph{CLAP score} is defined as the cosine similarity between the CLAP encodings of an audio and its textual description. We borrowed the AudioLDM evaluation toolkit~\cite{liu2023audioldm} for the computation of FAD, IS, and KL scores.

\paragraph{\bf Temporal Objective Metrics} To specifically evaluate the temporal controllability of the text-to-audio models, we employ the recently-proposed STEAM~\cite{audiotime} metrics measured on the AudioTime~\cite{audiotime} benchmark dataset containing temporally-aligned audio-text pairs. STEAM constitutes four temporal metrics: (i) \emph{Ordering Error Rate} (OER) -- if a pair of events in the generated audio matches their order in the text, (ii) \emph{Duration} (DUR) / (iii) \emph{Frequency} (FREQ) -- if the duration/frequency of an event in the generated audio matches matches the given text, (iv) \emph{Timestamp} (TIME) -- if the on- and off-set timings of an event in the generated audio match the given text.

\paragraph{\bf Subjective Metrics} Our subjective assessment examines two key aspects of the generated audio: overall audio quality (OVL) and relevance to the text input (REL), mirroring the approach outlined in the previous works, such as, \cite{ghosal2023text,vyas2023audiobox}. The OVL metric primarily gauges the general sound quality, clarity, and naturalness irrespective of its alignment with the input prompt. Conversely, the REL metric assesses how well the generated audio corresponds to the given text input. Annotators were tasked with rating each audio sample on a scale from 1 (least) to 5 (highest) for both OVL and REL. This evaluation was conducted on a subset of 50 randomly-selected prompts from the AudioCaps test set, with each sample being independently reviewed by at least four annotators. Please refer to the supplementary material for more details on the evaluation instructions and evaluators.

\begin{table*}[t]
\centering
\caption{Objective evaluation results for audio generation in the presence of multiple concepts or a single concept in the text prompt in the \texttt{AudioCaps} test set. }
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{l|cccc|cc|cccc|cc}
\toprule
\multirow{3}{*}{Model} & \multicolumn{6}{c|}{Multiple Events/Concepts} & \multicolumn{6}{c}{Single Event/Concept} \\
 & \multicolumn{4}{c|}{Objective -- Holistic} & \multicolumn{2}{c|}{Subjective} & \multicolumn{4}{c|}{Objective -- Holistic} & \multicolumn{2}{c}{Subjective} \\
& FAD $\downarrow$ &  KL $\downarrow$ & IS $\uparrow$ & CLAP $\uparrow$ & OVL $\uparrow$ & REL $\uparrow$ & FAD $\downarrow$ &  KL $\downarrow$ & IS $\uparrow$ & CLAP $\uparrow$ & OVL $\uparrow$ & REL $\uparrow$ \\
\midrule
AudioLDM 2-Full  & \textbf{2.03} & 1.64 & 7.88 & 0.43 &$-$&$-$
& 7.93 & 1.24 & 4.50 & 0.47 &$-$&$-$ \\
AudioLDM 2-Full-Large & 2.33 & 1.58 & 8.14 & 0.44&3.54&3.16
& 5.82 & 1.09 & 4.60 & 0.49 &3.65&3.41 \\
Tango-full-FT & 2.69 & 1.16 & 7.85 & 0.54 &3.83&3.80
& 7.52 & 1.01 &  4.87 & 0.51 &3.67&3.49 \\
\model{}& 2.60 & \textbf{1.11} & \textbf{8.98} & \textbf{0.57 } &\textbf{3.99}&\textbf{4.07}
& \textbf{5.48} & \textbf{1.00} &  \textbf{4.95} & \textbf{0.52} &\textbf{3.95}&\textbf{4.10} \\
\bottomrule
\end{tabular}
}
\label{tab:event-analysis}
\end{table*}

\begin{table*}[t]
\centering
\caption{Objective evaluation results for audio generation in the presence of temporal events or non-temporal events in the text prompt in the AudioCaps test set. }
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{l|cccc|cc|cccc|cc}
\toprule
\multirow{3}{*}{Model} & \multicolumn{6}{c|}{Temporal Events} & \multicolumn{6}{c}{Non Temporal Events} \\
 & \multicolumn{4}{c|}{Objective -- Holistic} & \multicolumn{2}{c|}{Subjective} & \multicolumn{4}{c|}{Objective -- Holistic} & \multicolumn{2}{c}{Subjective} \\
 & FAD $\downarrow$ &  KL $\downarrow$ & IS $\uparrow$ & CLAP $\uparrow$ & OVL $\uparrow$ & REL $\uparrow$ & FAD $\downarrow$ &  KL $\downarrow$ & IS $\uparrow$ & CLAP $\uparrow$ & OVL $\uparrow$ & REL $\uparrow$ \\
\midrule
AudioLDM 2-Full  & \textbf{1.95} & 1.71 & 6.37 & 0.41 & $-$ & $-$
& \textbf{2.38} & 1.56 & 7.38 & 0.44 & $-$ & $-$  \\
AudioLDM 2-Full-Large & 2.39 & 1.65 & 6.10 & 0.42  & 3.35 & 2.82
& 2.68 & 1.46 & 8.12 & 0.46 & 3.79 & 3.62 \\
Tango-full-FT & 2.55 & 1.16 & 5.82 & 0.55 & 3.83 & 3.67 
& 3.04 & \textbf{1.15} &  7.70 & 0.53 & 3.78 & 3.88 \\
\model{}& 3.29 & \textbf{1.07} & \textbf{6.88} &\textbf{0.58} & \textbf{3.92} & \textbf{3.99}
& 2.84 & 1.16 &  \textbf{8.62} & \textbf{0.55}  & \textbf{4.05} & \textbf{4.16} \\
\bottomrule
\end{tabular}
}
\label{tab:temporal-analysis}
\end{table*}
\subsection{Main Results}

We report the comparative evaluations of \model{} against the baselines \textsc{Tango}~\cite{ghosal2023text} and \textsc{AudioLDM2}~\cite{liu2023audioldm2} in \cref{tab:main-results}. For a fair comparison, we used exactly 200 inference steps in all our experiments. \textsc{Tango} and \model{} were evaluated with a classifier-free guidance scale of 3 while AudioLDM2 uses a default guidance scale of 3.5. We generate only one sample per text prompt.

\paragraph{\bf Objective Evaluations}

\model{} achieves notable improvements in objective metrics, with scores of 2.69 for FAD, 1.12 for KL, 9.09 for IS, and 0.57 for CLAP. While FAD, KL, and IS assess general naturalness, diversity, and audio quality, CLAP measures the semantic alignment between the input prompt and the generated audio. As documented in \citet{melechovsky2024mustango}, enhancing audio quality typically relies on improving the pre-training process of the backbone, either through architectural modifications or by leveraging larger or refined datasets. However, in our experiments, we observe enhanced audio quality in two out of three metrics, specifically KL and IS. Notably, \model{} also significantly outperforms various versions of \textsc{AudioLDM} and \textsc{AudioLDM2} on these two metrics. 

On the other hand, we note a substantial enhancement in the CLAP score. CLAP score is particularly crucial in our experimental setup as it directly measures the semantic alignment between the textual prompt and the generated audio. This outcome suggests that DPO-based fine-tuning on the preference data from \dataset{} yields superior audio generation to \textsc{Tango} and \textsc{AudioLDM2}.

A major enhancement in \model{} is evident in the temporal objective metrics, as measured by STEAM. With the exception of \emph{Duration}, \model{} shows consistent superiority over \textsc{Tango} across all other temporal measurements. The implementation of Strategy 3 in \dataset{} plays a crucial role in temporal data augmentation. Our findings reveal that the absence of this augmentation leads to a decline in \model{}'s temporal performance, thus highlighting the effectiveness of Strategy 3-based data augmentation technique.

\paragraph{\bf Subjective Evaluations}
In our subjective evaluation, \model{} achieves high ratings of 3.99 in OVL (overall quality) and 4.07 in REL (relevance), surpassing both \textsc{Tango} and \textsc{AudioLDM2}. This suggests that \model{} significantly benefits from preference modeling on \dataset{}. Interestingly, our subjective findings diverge from those reported by \citet{melechovsky2024mustango}. In their study, the authors observed lower audio quality when \textsc{Tango} was fine-tuned on music data. However, in our experiments, the objective of preference modeling enhances both overall sound quality and the relevance of generated audio to the input prompts.
Notably, in our experiments, \textsc{AudioLDM2} performed the worst, with the scores of only 3.56 in OVL and 3.19 in REL, significantly lower than both \textsc{Tango} and \model{}.

\begin{table*}[ht!]
    \centering
    \rowcolors{1}{gray!20}{gray!10} % Set background colors for odd and even rows
    \caption{GPT-4 prompt used to extract events or concepts from audio prompts.}
    \begin{tabular}{|p{15cm}|} % Define table with one centered column
        \hline
        You are to extract all the indivisible events in the given text, labeled as input. Imagine experiencing the events in the input as you are reading it and write down the indivisible events one by one. After writing your experience, list all the events in the sequence you observed them as a python list. Think step-by-step. Do not directly give the answer. Please refer to these following examples as refernce for input and output:\\
    \textcolor{blue}{\textit{Example 1 -}}\\
    \textcolor{blue}{\textbf{Input}: An aircraft engine runs and vibrates, metal spinning and grinding occur, and the engine accelerates and fades into the distance}\\
    \textcolor{blue}{\textbf{Output}: Firstly, an aircraft engine runs and vibrates. Then, I hear metal spinning and grinding. Then, the aircraft engine accelerates. Finally, the aircraft fades into the distance.}\\
    \textcolor{blue}{So, here is the list of events that I observed:}\\
    \textcolor{blue}{\texttt{["aircraft engine runs", "aircraft engine vibrates", "metal spinning", "metal grinding", "aircraft engine acclerates", "aircraft fades into the distance"]}}\\
    \textcolor{purple}{\textit{Example 2 -}}\\
    \textcolor{purple}{\textbf{Input}: Bubbles gurgling and water spraying as a man speaks softly while crowd of people talk in the background}\\
    \textcolor{purple}{\textbf{Output}: Firstly, I hear bubble gurgling. Also, I hear water spraying. Simultaneously, a man is speaking softly. Also, a crowd of people are talking in the background. }\\
    \textcolor{purple}{So, here is the list of events that I observed:}\\
    \textcolor{purple}{\texttt{["bubble gurgling", "water spraying", "a man is speaking softly", "crowd talking"]}}\\
   \textcolor{red}{\textit{Example 3 -}}\\
    \textcolor{red}{\textbf{Input}: A man talking then meowing and hissing}\\
    \textcolor{red}{\textbf{Output}: Firstly, I hear a man talking. Subsequently, I hear meowing. I also hear hissing.}\\
    \textcolor{red}{So, here is the list of events that I observed:}\\
    \textcolor{red}{\texttt{["a man talking", "meowing", "hissing"]}}\\
    **** Examples end here\\
    Now, given the input text below extract all the indivisible events one by one as explained above with examples. Also, remember to follow the exact format of the examples.\\
    \textbf{Input}: \textcolor{red}{\{PROMPT\}}\\
    \textbf{Output}: \\
        \hline
    \end{tabular}
    \label{tab:gpt4-event}
\end{table*}

Additionally, we categorize prompts based on the presence of multiple concepts or events, exemplified by phrases like \textit{``\ul{A woman speaks} while \ul{cooking}''}. As underlined, this prompt contains two distinct events i.e., \emph{``sound of a woman speaking''} and \emph{``sound of cooking''}. Through manual scrutiny, we discovered that pinpointing prompts with such multi-concepts is challenging using basic parts-of-speech or named entity-based rules. Consequently, we task GPT-4 with extracting the various concepts or events from the prompts using in-context exemplars. The specific prompt is displayed in \Cref{tab:gpt4-event}. To evaluate GPT-4's performance on this task, we randomly selected 30 unique prompts and manually verified their annotations from GPT-4's. No errors attributable to GPT-4 were found. In general, \model{} outperforms \textsc{AudioLDM2} and \textsc{Tango} across most objective and subjective metrics, following \Cref{tab:event-analysis}. We proceed to visualize the CLAP scores of the models in \Cref{fig:event-analysis}. This visualization illustrates that \model{} consistently outperforms the baselines as the number of events or concepts per prompt increases. In particular, specifically, \textsc{Tango} closely matches the performance of \model{} only when the textual prompt contains a single concept. However, the disparity between these two models widens as the complexity of the prompt increases with multiple concepts.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{images/tango2-events.png}
    \caption{CLAP score of the models vs the number of events or concepts in the textual prompt.
    }
    \label{fig:event-analysis}
\end{figure}
The supremacy of \model{} over the baselines in both of these cases can be strongly ascribed to DPO training the diffusion model the differences between generating a preferred and an undesirable audio output. Particularly, the undesirable audio outputs with missing concepts and wrong temporal orderings of events are penalized. Conversely, the preferred audio samples with the correct event orderings and presence are promoted by the noise-estimation terms in the DPO-diffusion loss (\cref{eq:DPO-Diff}).

\paragraph{\bf Ablations on \dataset{}.} We conducted an ablation study on \dataset{} to gauge the impact of different negative data construction strategies. As shown in \Cref{tab:main-results}, excluding the data samples from by \emph{strategies 2 and 3} notably diminishes the performance of \model{}. This underscores the significance of event and temporal prompt perturbations.

\paragraph{\bf The Effect of Filtering.}
In our experiments, we noticed that filtering to create different \dataset{} can impact the performance (refer to \Cref{sec:pref-dataset}).  \Cref{fig:filtering} depicts the impact of this filtering process. We found setting $\Delta_2 \ge 0.08$, and $\alpha_2 \ge 0.6$ gives the best results. 
\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{images/audio-alpaca-filtering.pdf}
    \caption{The impact of filtering \dataset{} on performance observed through $\Delta_2$, and $\alpha_2$. The CLAP score of the winning audio must be at least $\alpha_2$ and $\Delta_2$ represents the difference in CLAP scores between the winning audio $x^w$ and the losing audio $x^l$ given a prompt $\tau$. %We denote the set of winning audios as $\{x^w\}$, defined as $\{x^w \mid \text{CLAP}(x^w) \ge \alpha_2\}$ where $\text{CLAP}(x^w)$ indicates the CLAP score of the winning sample $x^w$.
    }
    \label{fig:filtering}
\end{figure}

\section{Conclusion}
In this work, we propose aligning text-to-audio generative models through direct preference optimization. To the best of our knowledge, this represents the first attempt to advance text-to-audio generation through preference optimization. We achieve this by automatically generating a preference dataset using a combination of Large Language Models (LLMs) and adversarial filtering. Our preference dataset, \dataset{}, comprises diverse audio descriptions (prompts) paired with their respective preferred (winner) and undesirable (loser) audios. The preferred audios are expected to accurately reflect their corresponding textual descriptions, while the undesirable audios exhibit flaws such as missing concepts, incorrect temporal order, or high noise levels. To generate undesirable audios, we perturb the descriptions by removing or rearranging certain concepts and feeding them to Tango. Additionally, we employ adversarial filtering, generating multiple audios from the original prompt and selecting those with CLAP scores below a specified threshold. Subsequently, we align a diffusion-based text-to-audio model, \textsc{Tango}, on \dataset{} using DPO-diffusion loss. Our results demonstrate significant performance leap over the previous models, both in terms of objective and subjective metrics. We anticipate that our dataset, \dataset{}, and the proposed model, \model{}, will pave the way for further advancements in alignment techniques for text-to-audio generation.
\section*{Acknowledgements}
This research is supported by the Ministry of Education, Singapore,
under its AcRF Tier-2 grant (Project no. T2MOE2008, and Grantor
reference no. MOE-T2EP20220-0017).

\end{document}