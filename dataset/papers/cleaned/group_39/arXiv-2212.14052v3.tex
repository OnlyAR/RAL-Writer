\title{Hungry Hungry Hippos: Towards Language Modeling with State Space Models}

\begin{document}

\maketitle

\begin{abstract}

State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling.
Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization.
In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention.
First, we use synthetic language modeling tasks to understand the gap between SSMs and attention.
We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence.
To understand the impact on language modeling, we propose a new SSM layer, \hthree, that is explicitly designed for these abilities.
\hthree matches attention on the synthetic languages and comes within \num{0.4} PPL of Transformers on OpenWebText.
Furthermore, a hybrid 125M-parameter \hthree-attention model that retains two attention layers
surprisingly outperforms Transformers on OpenWebText by \num{1.0} PPL.
Next, to improve the efficiency of training SSMs on modern hardware,
we propose \fastfft.
\fastfft uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences.
\fastfft yields 2$\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text \num{2.4$\times$} faster than Transformers.
Using \fastfft, we scale hybrid \hthree-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.

\section{Introduction}
\label{sec:intro}

State space models (SSMs) have achieved state-of-the-art sequence modeling performance in domains ranging from time series analysis~\citep{gu2022efficiently} to audio generation~\citep{goel2022s}.
However, they have yet to match the performance of Transformers on language modeling, often underperforming Transformers by multiple points in perplexity~\citep{gu2022efficiently}.
An natural question is whether this gap in performance is due to inherent inductive biases and capabilities in attention~\citep{edelman2022inductive,olsson2022context}, or whether it is a function of the significant organizational resources that have been spent training and tuning large attention-based language models~\citep{chowdhery2022palm,hoffmann2022training,zhang2022opt}, as well as specialized hardware support for attention, ranging from tensor cores~\citep{nvidia2017nvidia} to transformer chips~\citep{nvidia2022nvidia,kao2021optimized}.

We take first steps towards answering these questions in this paper.
First, we use synthetic language modeling tasks to show that there is an expressivity gap between SSMs and attention.
Using our insights, we design a new SSM layer that nearly matches attention in language modeling.
Second, we propose better hardware-aware algorithms for SSMs that allow them to take advantage of modern accelerators---and run faster than attention.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figs/banner_pdf.pdf}
    \caption{\label{fig:banner}
    Left: \hthree stacks two discrete SSMs with shift and diagonal matrices and uses multiplicative interactions between input projections and their outputs to model comparisons between points in a sequence.
    Middle: \hthree can perform associative recall---which is easy for attention, but not existing SSMs.
    Right: \fastfft uses a new state-passing algorithm over fused block FFTConv to increase hardware efficiency of SSMs, allowing \hthree to scale to billion-parameter models.
    }
    \vspace{-1.25em}
\end{figure}
\textbf{Understanding the Expressivity Gap.}
To understand the gap between SSMs and attention, we draw on synthetic language modeling tasks that have been proposed as a mechanistic basis for in-context learning in Transformers~\citep{olsson2022context}
These synthetic languages focus on the ability to manipulate text---recalling tokens from earlier time steps, or comparing tokens from different points in a sequence.
We find that existing SSMs struggle to model these synthetic languages.
To probe how important these skills are for language modeling, we propose \hthree (Hungry Hungry Hippo), a new SSM-based layer designed to solve these language modeling tasks.
\hthree stacks two SSMs, with multiplicative interactions between their outputs and input projections.
The SSMs allow \hthree to keep a log of tokens (to recall them later), while the multiplicative interactions allow for comparisons across the sequence.

\hthree matches attention on the synthetic languages and almost closes the gap with Transformers on language modeling---coming within \num{0.4} perplexity of Transformers on OpenWebText (compared to \num{3.4} ppl for existing SSMs---even those explicitly designed for language modeling~\citep{mehta2022long}).
Furthermore, a simple hybrid \hthree-attention model that retains two attention layers surprisingly \textit{outperforms} Transformers on OpenWebText by \num{1.0} perplexity.
To further evaluate \hthree on language modeling, we train 125M-, 355M-, 1.3B-, and 2.7B-parameter hybrid \hthree-attention language models on the Pile~\citep{gao2020pile}, using hyperparameters from GPT-3~\citep{brown2020language}.
These hybrid models outperform Transformer-based language models of the same size in perplexity, and match or outperform them on a majority of tasks in the SuperGLUE benchmark in zero- and few-shot learning.
Since the SSM layers in these hybrid models admit a recurrent view, they can also perform \num{2.4$\times$} faster inference than Transformers.

\textbf{Scaling SSMs.}
Next, we improve the efficiency of SSMs on modern hardware, to reduce the hardware barrier between attention and SSMs.
SSMs scale nearly linearly in sequence length instead of quadratically like attention, but still run slower on modern hardware due to poor hardware utilization.
To close this gap, we propose \fastfft, a hierarchical algorithm for computing SSMs, inspired by IO-Aware attention~\citep{dao2022flashattention}.
The technical challenge is that SSMs require a FFT-based convolution over the input sequence, which requires an FFT, pointwise multiply, and inverse FFT.
When implemented in cuFFT~\citep{cufft}, this operation incurs expensive GPU memory reads/writes, and cannot utilize the specialized matrix multiply units available on modern hardware\footnote{An A100 GPU has a maximum of 312 TFLOPs/s of FP16 with
tensor cores, but only 20 TFLOPs/s of FP32 (and 40 TFLOPs/s of FP16) without
tensor cores~\citep{nvidia2020nvidia}. This trend started with the V100 GPUs~\citep{nvidia2017nvidia} and has continued with the
H100 GPUs~\citep{nvidia2022nvidia}.}.
To use specialized matrix multiply units, we appeal to classical techniques that split the FFT into blocks and compute it using a series of matrix multiplications.
Combined with kernel fusion, this ``block'' FFT solution increases hardware efficiency, but only as long as the sequence length can fit into GPU SRAM (on-chip memory, analogous to L1 cache on the CPU)---up to sequence length 8K on modern A100.

To scale to sequences longer than 8K, we propose a \textit{state passing} algorithm (Figure~\ref{fig:banner} right), specialized to SSMs.
The key insight is that we can use the recurrent properties of SSMs to process the input in chunks---as long as we keep track of an additional state vector.
The state passing algorithm splits the input into the largest chunks that can fit into GPU SRAM, efficiently computes the FFT-based convolution using block FFT, and updates an intermediate state to start the next chunk.
Using this state-passing algorithm, \fastfft can scale SSMs to \textit{any} sequence length---even longer than can fit on GPU SRAM at once---while maintaining a \textit{near linear} compute complexity.
\fastfft sets state-of-the-art speed on long range arena using S4~\citep{gu2022efficiently}, outperforming Transformers by \num{5.8$\times$} and previous S4 models by \num{2$\times$}.
\fastfft trains \hthree\ \num{4-8$\times$} times faster than attention for long sequences, and is a critical component for scaling to billion-parameter models\footnote{Code for H3 is available at \url{https://github.com/HazyResearch/H3} }.

\section{Background}
\label{sec:background}

We present some background on state space models and linear attention, which inspired our H3 layer.

\subsection{State Space Models}

A continuous-time state-space representation~\citep{brogan1974modern} defines a linear mapping from an
input signal $u(t) \in \mathbb{R}$ (as a function of time $t$) to an output signal $y(t) \in \mathbb{R}$ through a state-variable
$x(t) \in \mathbb{R}^m$, with the following differential equation, for some matrices $\vA \in \mathbb{R}^{m \times m}$, $\vB \in \mathbb{R}^{m \times 1}$, $\vC \in \mathbb{R}^{1 \times m}$,
$\vD \in \mathbb{R}^{1 \times 1}$: $\dot{x}(t) = \vA x(t) + \vB u(t)$, $y(t) = \vC x(t) + \vD u(t)$.

Similarly, a discrete-time state-space representation defines a linear mapping
from a discrete input signal $u_i$ (for $i = 1, 2, \dots$) to a discrete output signal
$y_i$ though a state-variable $x_i \in \mathbb{R}^m$:
\begin{align*}
  x_i &= \vA x_{i-1} + \vB u_i \\
  y_i &= \vC x_i + \vD u_i.
\end{align*}

A state-space model (SSM) uses these representations as a layer in a deep learning
pipeline, where the matrices $\vA, \vB, \vC, \vD$ are learned from data (e.g.,
with gradient-based optimization).
One often has $d$ of these SSMs in parallel, each corresponding to one hidden
dimension.
To preserve the sequence history, HiPPO~\citep{gu2020hippo} projects the history
on a basis of orthogonal polynomials, which translates to having SSMs whose
$\vA, \vB$ matrices are initialized to some special matrices.

This recurrent form of SSMs allows efficient inference (i.e., generation): to
generate the output of the next time-step, one only needs the state of the
current time-step, not the entire input history.
Furthermore, SSMs can freely extrapolate to sequences longer than seen during training.

\textbf{SSMs as Convolution.}
For efficient training, given the
entire sequence of the input $u_1, \dots, u_N$, the output sequence
$y_1, \dots, y_N$ can also be written as the convolution of the input with the
filter~\citep{gu2021combining}:
\begin{equation*}
  f = [\vC \vB, \vC\vA\vB, \vC\vA^2\vB, \dots, \vC\vA^{N-1}\vB].
\end{equation*}
That is, from an initial condition $x_0$, we have
$y_i = \vC\vA^i\vB x_0 + (f \ast u)_i + \vD u_i$, where $(f \ast u)$ denotes a
linear convolution between $f$ and $u$.
If we set the initial condition $x_0$ to be zero, then $y$ is exactly a
linear convolution of $u$, with a residual connection $\vD u$.
More generally, any linear time-invariant system (of which SSMs are a special case) can
be written as a convolution.

Given a 1D input sequence $u \in \mathbb{R}^{N}$ of length $N$, we denote the 1D output
sequence $y \in \mathbb{R}^N$ of an SSM parameterized by matrices $\vA, \vB, \vC, \vD$ as
\begin{equation*}
  y = \mathrm{SSM}_{\vA, \vB, \vC, \vD}(u).
\end{equation*}
To simplify notation, we omit the reference to $\vA, \vB, \vC, \vD$ and write
$y = \mathrm{SSM}(u)$ if they are clear from context.
When $u$ is multidimensional of dimension $d$, we stack $d$ of these SSMs
together that defines a mapping from $u \in \mathbb{R}^{N \times d}$ to $y \in \mathbb{R}^{N \times d}$, using
the same notation $y = \mathrm{SSM}(u)$.

To construct the filter $f$ from $\vA, \vB, \vC$ efficiently, $\vA$ is often constrained to
be diagonal~\citep{gupta2022diagonal,gu2022parameterization}, or diagonal plus
low-rank~\citep{gu2022efficiently}.

\textbf{SSM through FFTs.}
Computing the convolution naively through conventional matrix operations is expensive
for long kernels, scaling as $O(N^2)$.
Instead, we can use FFTs: take the FFT of $f$ and $u$, multiply them together pointwise, and then take the inverse FFT.
This yields an $O(N \log N)$ algorithm.

\subsection{Linear attention}

We describe linear attention~\citep{katharopoulos2020transformers} and its connection to RNNs, which inspired our model design~(\cref{sec:method}).

In standard attention~\citep{vaswani2017attention}, we have $N$ query/key/value tokens $Q_i, K_i, V_i \in \mathbb{R}^d$ for
$i = 1, \dots, N$, where $N$ is the sequence length and $d$ is the head dimension.
For some similarity metric $\Sim \colon \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R}$, we want to compute the output:
\begin{equation*}
  O_i = \frac{\sum_{j=1}^i\Sim(Q_i, K_j) V_j}{\sum_{j=1}^i \Sim(Q_i, K_j)} \in \mathbb{R}^d.
\end{equation*}
For standard softmax attention, $\Sim(q, k) = e^{q^\top k}$ (often the dot
product is scaled by $1/\sqrt{d}$).
Linear attention makes the assumption that $\Sim$ has the form
$\Sim(q, k) = \phi(q)^\top \phi(k),$
for some (nonlinear) function $\phi$.
The output is then $O_i = \frac{\phi(Q_i)^\top \sum_{j=1}^{i} \phi(K_j) V_j^\top }{\phi(Q_i)^\top \sum_{j=1}^{i} \phi(K_j)}$.
Let
$S_i = \sum_{j=1}^{i} \phi(K_j) V_j^\top \in \mathbb{R}^{d \times d}$, $z_i = \sum_{j=1}^{i} \phi(K_j) \in \mathbb{R}^d$, $d_i = \phi(Q_{i})^\top z_i \in \mathbb{R}$.
Then $O_i = \frac{\phi(Q_i)^\top S_i}{d_i}$.
This connects linear attention to RNNs: the output $O_i$ is a function of $S_i$
and $z_i$, both of which are incrementally updated (as cumulative sums).

\section{Hungry Hungry Hippos Layer to Model Discrete Sequences}
\label{sec:method}

To understand the gap between SSMs and attention on language modeling, we
examine two synthetic language modeling tasks.
These tasks motivate our \hthree layer to add a discrete SSM (based on shift matrix) and multiplicative interaction to effectively model discrete sequences.
We then show that the \hthree layer is expressive enough to solve these synthetic tasks, and that this understanding leads to better performance on a real language modeling benchmark.

\subsection{Motivation: Synthetic Language Modeling Tasks\label{sec:synthetics}}

We describe two closely-related synthetic tasks, summarized in Table~\ref{table:synthetic_tasks}. 
Olsson et al.~\citep{olsson2022context} argue that the ability to solve (variants of) these tasks accounts for the majority of the in-context learning capability of Transformers, and more intuition is given in Appendix~\ref{sec:app_exp_details}. 
\begin{table}[h]
    \small
    \centering
    \caption{\label{table:synthetic_tasks} Synthetic language modeling tasks.}
    {
        \begin{tabular}{@{}|l|l|l|c|c|@{}}
        \hline
        Task & Input & Output & Sequence Length & Vocab Size  \\ % & Training time \\
        \hline
        Induction Head & \textit{a b c d e $\vdash$ f g h i $\dots$ x y z $\vdash$} & \textit{f} & 30 & 20 \\
        Associative Recall & \textit{a 2 c 4 b 3 d 1 a} & \textit{2} & 20 & 10 \\ \hline
        \end{tabular}
    }
\end{table}
The \textbf{Induction Head} task tests how well a model can recall content after a special token (e.g., $\vdash$ in~\cref{table:synthetic_tasks}).
At the end of the sequence, the model must recall the token that appeared immediately after the special token earlier in the sequence. \textbf{Associative Recall}~\citep{ba2016using} is similar to the induction head task, but requires the model to remember multiple key-value pairs.
At the end of the sequence, the model must recall a specific value belonging to a specific key.
\begin{table}[h]
    \small
    \centering
    \caption{\label{table:synthetics} Evaluation of 2-layer models on synthetic language tasks.}
    {
        \begin{tabular}{@{}|c|c|ccc|c|@{}}
        \hline
        Task & Random & S4D & Gated State Spaces & H3 & Attention  \\ % & Training time \\
        \hline
        Induction Head & 5.0 & 35.6 & 6.8 & \textbf{100.0} & \textbf{100.0} \\
        Associative Recall & 25.0 & 86.0 & 78.0 & 99.8 & \textbf{100.0}  \\ \hline
        \end{tabular}
    }
\end{table}

We argue that these failures suggest two missing capabilities: (i) to remember tokens that appear after a particular event (e.g., the special token in the induction head task), and (ii) to compare tokens across the sequence (e.g., comparing keys to decide which value to recall).
Attention has both these capabilities: it can compare tokens by constructing the \textit{quadratic} attention matrix $\vQ \vK^\top$, and it can recall tokens by direct copying (multiplying $\softmax(\vQ \vK^\top)$ with $\vV$).
In Section~\ref{sec:method_h3}, we design our new layer \hthree to enable these capabilities in SSMs, narrowing the expressivity gap between SSMs and attention.

\subsection{\hthree Layer\label{sec:method_h3}}
\hthree uses SSMs with shift and diagonal matrices, along with multiplicative
operations against projections of the input to capture the missing capabilities identified by the synthetics.

\textbf{High-level Intuition.}
(i) To remember tokens from the past, we want the state $x_i$ to copy from the input $u_i$, and then pass that information to the next state $x_{i+1}$. As $x_{i+1}$ relates to $x_i$by $\vA x_i$, we use a discrete SSM with a shift matrix $\vA$ (described formally below) that shifts the elements of a state vector (e.g., mapping $[a, b, c] \to [0, a, b]$).
(ii) To compare tokens across the sequence, we use multiplicative interaction: the output of an SSM, containing information from previous time steps, is multiplied with the input at the current time steps, thus measuring similarity between tokens.

\hthree is loosely inspired by linear attention (\cref{sec:background}): we project the input $u$ to get three signals $\vQ, \vK, \vV$.
Then we replace the non-linearity $\phi(\vK)$ with an SSM where $\vA$ is a shift matrix ($\mathrm{SSM}_\mathrm{shift}$), and we replace the summation $S_i$ with a SSM with diagonal $\vA$ ($\mathrm{SSM}_\mathrm{diag}$).
The output, for the case of head dimension $d_h = 1$, is:
\begin{equation*}
  \vQ \odot \mathrm{SSM}_\mathrm{diag}(\mathrm{SSM}_\mathrm{shift}(\vK) \odot \vV),
\end{equation*}
where $\odot$ denotes pointwise multiplication.
We can view this form as stacking two SSMs with multiplicative
interaction (each is a ``hungry hippo'', hence the name of our layer).
A more formal connection between linear attention, time-varying systems, and \hthree can be found in Appendix~\ref{app:linear_attention}.

\textbf{Remembering Key Tokens: Shift and Diagonal SSMs.}
The shift and diagonal SSMs are designed to address the capability to log tokens after particular events.
In the shift SSM, we constrain $\vA \in \mathbb{R}^{m \times m}$ to be a shift matrix
$
\vA_{i, j} = 
    \begin{cases}
    1 & \text{for } i - 1 = j\\
    0 & \text{otherwise}
    \end{cases}
$.
The action of this matrix on the hidden state $x_i$ is to shift each coordinate
down by one---thereby creating a ``memory'' of the previous states.
For example, if $\vB = e_1$, the first basis vector, then
$x_i = [u_i, u_{i-1}, \dots, u_{i-m+1}]$ contains the inputs from the previous $m$
time steps.
We learn $\vB$ and $\vC$ ($\vB$ can also be fixed to $e_1$ for
simplicity, in which case the output is a 1D conv.\ with kernel size $m$).

The diagonal SSM constrains $\vA$ to be diagonal and initializes it from the diagonal version of HiPPO (S4D~\citep{gu2022parameterization}).
This parameterization allows the model to remember state over the entire sequence.
The shift SSM can detect when a particular event occurs, and the diagonal SSM can remember a token afterwards for the rest of the sequence.

\textbf{Multiplicative Interaction for Comparison.}
We take the multiplicative interactions from linear attention, but they provide another missing capability when combined with a shift matrix: comparing tokens across the sequence.
The multiplicative interactions between the output of the shift SSM and the $\vV$ projection mimics local multiplicative interactions in linear attention (depending on the size of the hidden state).
Similarly, multiplicative interactions with the $\vQ$ projection and the output of the diagonal SSM allows comparisons between tokens over the entire sequence.

\textbf{\hthree Layer.} The overall layer is given in Algorithm~\ref{alg:h3} and shown schematically in Figure~\ref{fig:banner} (left).
We use the \hthree layer to construct a model in the same style as Transformers by interleaving it with MLPs,
connected by residual connection and layer norm (i.e., pre-norm architecture~\citep{baevski2018adaptive}).
We will also consider a hybrid \hthree-attention model (two attention layers while the rest are \hthree, \cref{sec:expressivity,sec:evaluation}).
\begin{algorithm}[H]
  \algsetup{linenosize=\tiny}
  \caption{\label{alg:h3} H3 Layer}
  \small
  \begin{algorithmic}[1]
    \REQUIRE Input sequence $u \in \mathbb{R}^{N \times d}$ from the previous layer, weight
    matrices $\vW_Q, \vW_K, \vW_V, \vW_O \in \mathbb{R}^{d \times d}$, a shift SSM $\mathrm{SSM}_\mathrm{shift}$, a diagonal SSM $\mathrm{SSM}_{\mathrm{diag}}$, head dimension $d_h$.
    \STATE Compute $\vQ = u \vW_Q, \vK = u \vW_K, \vV = u \vW_V \in \mathbb{R}^{N \times d}$.
    \STATE Pass $\vK$ through the shift SSM: $\overline{\vK} = \mathrm{SSM}_\mathrm{shift}(\vK) \in \mathbb{R}^{N \times d}$.
    \STATE Split $\vQ, \overline{\vK}, \vV$ into $H$ ``heads'' ($\vQ^{(h)}, {\overline{\vK}}^{(h)}, \vV^{(h)}$
    for $h = 1, \dots, H$), each a sequence of $N$ vectors of size $d_{h} = d / H$.
    \FOR{$1 \leq h \leq H$}
    \STATE Take the batched outer product ${\overline{\vK}}^{(h)} (\vV^{(h)})^\top \in \mathbb{R}^{N \times d_h \times d_h}$ (batched in the $N$-dimension) and pass it through a diagonal SSM: $\vK\vV^{(h)} = \mathrm{SSM}_{\mathrm{diag}}({\overline{\vK}}^{(h)} (\vV^{(h)})^\top) \in \mathbb{R}^{N \times d_{h} \times {d_{h}}}$.
    \STATE Batch-multiply by $\vQ$:  $\vO^{(h)} = [\vQ^{(h)}_1 \vK\vV^{(h)}_1, \dots, \vQ^{(h)}_N \vK\vV^{(h)}_N]\in  \mathbb{R}^{N \times d_h}$ (batched in the $N$-dimension).
    \ENDFOR
    \STATE Concatenate the output $\vO^{(h)}$ of each head, and multiply by the output
    projection matrix $\vW_O \in \mathbb{R}^{d \times d}$.
  \end{algorithmic}
\end{algorithm}

\paragraph{Efficiency}
We show that \hthree scales as $O(N \log N)$ with
sequence length $N$---asymptotically more efficient than attention, which typically requires $O(N^2d)$ time and $O(N^2)$ space\footnote{There are several memory-efficient
algorithms for attention~\citep{rabe2021self,dao2022flashattention}, though
their time complexity is still quadratic in $N$, which is a lower-bound for attention~\citep{keles2022computational}.} (proof in~\cref{sec:app_h3_complexity}).
\begin{proposition}\label{thm:h3_complexity}
  Let $N$ be the sequence length, $d$ be the hidden dimension, and assume that
  the head dimension $d_h$ is of order $O(1)$.
  Then the H3 layer takes $O(d^2N + d N \log N)$ time and $O(dN)$ space to compute.
\end{proposition}

\subsection{Expressivity}
\label{sec:expressivity}

We show that \hthree can model our synthetic languages, as well as natural language on OpenWebText~\citep{Gokaslan2019OpenWeb}.
We also present a hybrid \hthree-attention extension that outperforms Transformers on OpenWebText.

\textbf{Mechanism for Solving Associative Recall with H3.}
\hthree is expressive enough to solve our synthetic language modeling tasks, as shown in Table~\ref{table:synthetics}.
Figure~\ref{fig:banner} (middle) shows a mechanism for a single \hthree layer to solve the associative recall task for a particular key-value pair $(a, 3)$.
The shift SSM and following multiplicative interaction act as a gate on whether to let a value through to the diagonal SSM, based on whether the previous token was key $a$.
The diagonal SSM stores the value $3$ in memory, and continually outputs it.
The final multiplicative interaction gates whether to let the diagonal SSM's output through---based on whether the current input token is the key $a$.
We formally construct the weights of an \hthree layer to solve this task in Appendix~\ref{sec:app_expressivity}.

\begin{table}[h]
    \small
    \centering
    \caption{\label{table:ablations} Perplexity of SSM variants compared to
      Transformers on OpenWebText. All models have 12 layers, with size around 125M, and are trained
      with the same hyperpameters, for 50B tokens.}
    {
        \begin{tabular}{@{}|ccccc|c|@{}}
            \hline
        \hthree & \hthree Hybrid (2 Attn) & S4D & GSS & GSS Hybrid (2 Attn) & Transformer  \\ % & Training time \\
        \hline
        21.0 & \textbf{19.6} & 24.9 & 24.0 & 19.8 & 20.6 \\ \hline
        \end{tabular}
    }
\end{table}
\textbf{Better Synthetic Language Modeling Translates to Better Natural Language Modeling.}
We validate that when H3 can solve these synthetic tasks, it also improves the modeling capability on natural language (e.g., on the OpenWebText dataset).
As shown in Table~\ref{table:ablations}, \hthree comes within 0.4 perplexity points of Transformers when trained for 50B tokens on OpenWebText, and performs much better than existing SSM variants (S4D, GSS), by $3-3.9$ points.

\textbf{Extension: H3-attention Hybrid Model.}
A simple hybrid \hthree-attention language model surprisingly outperforms Transformers on OpenWebText by 1.0 point.
Our hybrid model simply retains two self-attention layers: one in the second layer, and one in the middle (layer $2 + N / 2$ for an $N$-layer model, $N$ even).
The \hthree-attention hybrid also outperforms the GSS-attention hybrid~\citep{mehta2022long}.

\section{\fastfft: Efficiently Training SSMs\label{sec:efficiency}}

To improve the efficiency of SSMs on modern hardware, we propose \fastfft.
\fastfft fuses the FFT, pointwise multiply, and inverse FFT to reduce memory
reads/writes.
It also uses a block FFT algorithm to make use of specialized matrix multiply units (e.g., tensor cores on A100) for sequence lengths up to 8K.
For sequences longer than 8K, the computation no longer fits in GPU SRAM\footnote{SRAM, or on-chip memory, is much faster than off-chip GPU memory, but usually much smaller, on the order of around 100KB for each streaming processor.}, so we
propose a novel state-passing algorithm that splits the sequence into chunks to
compute the FFT convolution one chunk at a time.
\fastfft can speed up any SSMs (not just \hthree).

\subsection{Fused Block FFTConv}
\label{sec:systolic_fft}

We deploy two techniques to speed up the FFT-based convolution for sequences shorter than 8K: kernel fusion and block FFT.
Kernel fusion addresses IO bottlenecks due to reading and writing of intermediate results, while block FFT allows the FFT-based convolution to utilize specialized matrix multiplication units.
These techniques allow us to speed up FFTConv
by 2$\times$ (\cref{sec:eval_efficiency}) for sequences shorter than 8k.

\textbf{Kernel Fusion.}
Naive implementations of FFTConv using standard libraries such as cuFFT are IO-bound due to repeated reading and writing of intermediate results.
The FFT convolution in an SSM with input $u$ and filter $f$ has the form $iFFT(FFT(u) \odot FFT(f))$ (where $\odot$ denotes pointwise multiplication).
It requires reading and writing intermediate results to GPU memory---which can dominate the runtime.
Following \textsc{FlashAttention}~\citep{dao2022flashattention}, we first fuse the entire FFTConv into a single kernel and compute it in SRAM to avoid this overhead.

\textbf{Block FFT.}
To further speed up the computation of FFT-based convolution, we exploit specialized matrix multiplication hardware on modern GPUs
(e.g., Tensor Cores on Nvidia GPUs perform fast $16 \times 16$ matrix multiplication).
We appeal to classical results that show that the FFT can be written as a series of block-diagonal matrix
multiplications interleaved with permutation.
We note that such algorithms are not new, but our setting (fused FFTConv on GPU) introduces new bottlenecks---by removing the IO bottlenecks, compute becomes the bottleneck (note that a single FFT on GPU is usually IO bound).

Suppose that we want to perform an $N$-point FFT, which is equivalent to
multiply by the DFT matrix $\vF_N$.
Suppose that $N = N_1 N_2$ for some integers $N_1, N_2$.
By the Cooley-Tukey decomposition of the DFT~\citep{cooley1965an,
  bailey1990ffts} (also known as the four-step FFT algorithm),
we can write $\vF_N = \vP (\vI_{N_2} \otimes \vF_{N_1}) \vP^\top \vD (\vI_{N_1} \otimes \vF_{N_2}) \vP$,
where $\vP$ denotes a fixed permutation that reshapes the input as a $N_1 \times N_2$
array and then transpose it, $\otimes$ denotes Kroneker product, $\vD$ is a $N \times N$
diagonal matrix (called the twiddle factors)~\citep{dao2022monarch}, and $\vI_{N_i}$ and $\vF_{N_i}$ are the identity and DFT matrix of size $N_i \times N_i$.
As $\vI_{N_2} \otimes \vF_{N_1}$ and $\vI_{N_1} \otimes \vF_{N_2}$ are just block-diagonal matrices,
we can make use of specialized matmul units to perform these multiplications.
Similarly, if $N = N_1 N_2 N_3$ then we can decompose the $N$-point FFT into a
series of (block) FFT of size $N_1$, $N_2$, and $N_3$, interleaved by
permutation.

The block FFT algorithm incurs $O(N r \log N / \log r)$ FLOPs for a sequence length $N$, if $N$ can be written as $r^p$ for two integers $r, p$.
This incurs more FLOPs than standard FFT $(O(N\log N))$, but can run faster when
we using specialized matrix multiplication hardware.

\subsection{State-Passing}
However, the fused kernel cannot run if the sequence is too long to fit into GPU SRAM (longer than 8K on A100).
We show how to exploit the particular form of the FFT in SSM to speed it up for long
sequences.

The recurrent nature of SSMs allows us to split the FFTConv of a length-$N$ sequence into chunks of size $N'$ each ($N'$ is the longest FFT we can fit into SRAM), assuming $N$ is a multiple of $N'$).
We use FFTConv to compute each chunk and use a recurrence to connect the chunks.
In particular, we split the inputs $u$ into $C = N/N'$ chunks $u^{(c)} \in \mathbb{R}^{N'}$ for $c=1, \dots, C$.
Similarly, split the states $x$ into $x^{(c)} \in \mathbb{R}^{N' \times m}$ and the output $y$ into $y^{(c)} \in \mathbb{R}^{N'}$ for $i = 1, \dots, C$.
We will only need the end-state $x_{N'}^{(c)}$ of each chunk $c$.

Let $f = [\vC \vB, \vC\vA\vB, \vC\vA^2\vB, \dots, \vC\vA^{N'-1}\vB]$ be the SSM filter.
Recall from \cref{sec:background} that for each chunk $c$, $y_i^{(c)} = \vC\vA^i\vB x_{N'}^{(c-1)} + (f \ast u^{(c)})_i + \vD u_i^{(c)}$, since $x_{N'}^{(c-1)}$, the end-state of the previous chunk $(c-1)$ is the initial condition for the current chunk $c$.
In vector notation, $y^{(c)} = \vM_{xy} x_{N'}^{(c-1)} + f \ast u^{(c)} + \vD u^{(c)}$ for some matrix $\vM_{xy} \in \mathbb{R}^{N' \times m}$.
Additionally we need to update the end-state of each chunk with $x_{N'}^{c} = \vA^{N'} x_{N'}^{(c-1)} + \vM_{ux} u^{(c)}$ for some matrix $\vM_{ux}^{m \times N'}$ (derivation in Appendix~\ref{sec:state-passing-matrices}).
In essence, we can compute the output for each chunk with FFT-based convolution as long as we remember the end-state of the previous chunk, and the end-state of each chunk can be updated recurrently.
This yields a state-passing algorithm for long sequences, where we only compute FFT of length $N'$, and update some hidden state each iteration.

Let \textsc{BlockFFTConv} refer to our fused block FFTConv kernel.
Then, the state-passing algorithm for 1D input is given by Algorithm~\ref{alg:statepassing}.
For inputs of dimension $d$ where we stack $d$ SSMs, we simply batch~\cref{alg:statepassing} along the $d$-dimension.
\begin{algorithm}[h]
  \algsetup{linenosize=\tiny}
  \caption{\label{alg:statepassing} State Passing Algorithm}
  \small
  \begin{algorithmic}[1]
    \REQUIRE Input $u \in \mathbb{R}^{N}$, SSM parameterized by matrices $\vA \in \mathbb{R}^{m \times m}$, $\vB \in \mathbb{R}^{m \times 1}$, $\vC \in \mathbb{R}^{1 \times m}$, $\vD \in \mathbb{R}^{1 \times 1}$, chunk size $N'$ where $N$ is a multiple of $N'$.
    \STATE Precompute $\vA^{N'} \in \mathbb{R}^{m \times m}$, $\vM_{ux} = [\vA^{N'-1}\vB, \dots, \vB] \in \mathbb{R}^{m \times N'}$, $\vM_{xy} = [\vC, \vC\vA, \dots, \vC\vA^{N'-1}] \in \mathbb{R}^{N' \times m}$.
    \STATE Split the inputs $u_{1:N}$ into $C = N/N'$ chunks $u_{1:N'}^{(c)}$ for $c=1, \dots, C$.
    \STATE Let the initial state be $x_{N'}^{(0)} = 0 \in \mathbb{R}^m$.
    \FOR{$1 \leq c \leq C$}
      \STATE Compute $y^{(c)} = \vM_{xy} x_{N'}^{(c-1)} + $ \textsc{BlockFFTConv}($f$, $u_j$) $+ \vD u^{(c)} \in \mathbb{R}^{N'}$.
      \STATE Update state: $x_{N'}^{(c)} = \vA^{N'} x_{N'}^{(c-1)} + \vM_{ux} u^{(c)}$.
    \ENDFOR
    \STATE Return $y = [y^{(1)} \dots y^{(C)}]$.
  \end{algorithmic}
\end{algorithm}

We prove that~\cref{alg:statepassing} yields the same output as if one has computed the SSM using a large FFT of size $N$ (proof in~\cref{sec:app_statepassing_correctness_proof}):
\begin{proposition}\label{thm:statepassing_correctness}
  For input $u \in \mathbb{R}^N$ and matrices $\vA, \vB, \vC, \vD$, the output $y \in \mathbb{R}^N$ returned by~\cref{alg:statepassing} is the same as the output defined by the SSM parameterized by $\vA, \vB, \vC, \vD$.
\end{proposition}

\section{\hthree Evaluation}
\label{sec:evaluation}

\begin{table}[t]
    \small
    \centering
    \caption{\label{table:gpt} Perplexity (lower is better) of models on the Pile, OpenWebText and
      WikiText-103. GPT-Neo and hybrid \hthree are trained on the Pile, while GPT2 is
      trained on WebText. All models use the same GPT2 tokenizer. We report the
      perplexity of GPT-2 models on the Pile ($^*$) for context, though the performance is not directly comparable since they were trained on different data.}
    {
      \begin{tabular}{@{}|c|c|cc|@{}}
      \hline
        Model & Pile & OpenWebText & WikiText103 \\ % & Training time \\
        \hline
        GPT-2 small (125M) & 19.0* & 22.6 & 29.9 \\
        GPT-Neo-125M & 9.4 & 22.6 & 26.3 \\
        \textbf{Hybrid H3-125M} & \textbf{8.8} & \textbf{20.9} & \textbf{23.7} \\ \hline %2.7 days \\ \hline
        GPT-2 medium (355M) & 13.9* & 17.0 & 21.8 \\ % & 11.5 days \\
        \textbf{Hybrid H3-355M} & \textbf{7.1} & \textbf{15.9} & \textbf{16.9} \\ \hline
        GPT-2 XL (1.5B) & 12.4* & 12.9 & 17.0 \\
        GPT-Neo-1.3B & 6.2 & 13.1 & 13.3 \\
        \textbf{Hybrid H3-1.3B} & \textbf{6.0} & \textbf{12.4} & \textbf{12.5} \\
        \hline
        GPT-Neo-2.7B & 5.7 & 11.7 & 11.5 \\
        \textbf{Hybrid H3-2.7B} & \textbf{5.4} & \textbf{11.0} & \textbf{10.6} \\
        \hline
      \end{tabular}
    }
  \end{table}
\begin{table}[t]
    \scriptsize
    \centering
    \caption{\label{table:superglue_zeroshot_logit} Zero-shot acc.\ on SuperGLUE with logit scoring. Best results in bold, second best underline. }
    {
        \begin{tabular}{@{}|c|cccccccc|c|@{}}
            \hline
        Model & WSC & WIC & RTE & CB & MultiRC & ReCoRD & BoolQ & COPA & Average \\ % & Training time \\
        \hline
        OPT-125M & \textbf{39.4} & \underline{52.0} & 48.7 & 37.4 & \underline{58.9} & \underline{44.9} & \underline{59.6} & \underline{60.0} & 50.1 \\
        GPT-Neo-125M & \underline{36.5} & \textbf{53.6} & \underline{53.1} & \underline{41.1} & \textbf{59.9} & 39.6 & \textbf{62.2} & \underline{60.0} & \underline{50.8} \\
        \textbf{Hybrid \hthree-125M} & \textbf{39.4} & 51.4 & \textbf{59.2} & \textbf{48.2} & 51.4 & \textbf{55.0} & \underline{59.6} & \textbf{67.0} & \textbf{53.9} \\ \hline %2.7 days \\ \hline
        GPT-2 medium (355M) & \underline{50.0} & \textbf{52.0} & 51.3 & 28.6 & \textbf{59.5} & \underline{53.3} & \underline{61.0} & \underline{65.0} & 52.6 \\
        OPT-350M & \textbf{53.5} & 50.8 & \underline{53.4} & \underline{35.7} & \underline{58.9} & 51.4 & 60.9 & 60.0 & \underline{53.1} \\
        \textbf{Hybrid \hthree-355M} & 37.5 & \underline{51.7} & \textbf{55.2} & \textbf{41.1} & \textbf{59.5} & \textbf{62.3} & \textbf{61.5} & \textbf{69.0} & \textbf{54.7} \\ \hline
        OPT-1.3B & 36.5 & 49.5 & \textbf{53.4} & \textbf{39.3} & \textbf{58.3} & \underline{61.8} & 55.0 & \underline{69.0} & \underline{52.9} \\
        GPT-Neo-1.3B & \underline{41.3} & \underline{50.0} & \underline{52.3} & \underline{33.9} & 57.9 & 55.5 & \underline{59.9} & 66.0 & 52.1 \\
        \textbf{Hybrid \hthree-1.3B} & \textbf{52.9} & \textbf{50.3} & \textbf{53.4} & \underline{33.9} & \underline{58.2} & \textbf{67.8} & \textbf{61.7} & \textbf{74.0} & \textbf{56.5} \\ \hline
        OPT-2.7B & \textbf{51.0} & \underline{50.8} & 50.5 & \underline{41.1} & 57.4 & \underline{65.9} & 60.9 & 66.0 & \underline{55.5} \\
        GPT-Neo-2.7B & \underline{37.5} & 50.0 & \underline{52.3} & \textbf{50.0} & \textbf{59.1} & 60.0 & \textbf{61.1} & \underline{67.0} & 54.6 \\
        \textbf{Hybrid \hthree-2.7B} & 36.5 & \textbf{51.3} & \textbf{57.0} & 37.5 & \underline{58.7} & \textbf{71.3} & \textbf{61.1} & \textbf{81.0} & \textbf{56.8} \\ \hline
        \end{tabular}
    }
\end{table}\begin{table}[t]
    \scriptsize
    \centering
    \caption{\label{table:superglue_fewshot_logit} 3-shot acc.\ on SuperGLUE with logit scoring. Best results in bold, second best underline. }
    {
        \begin{tabular}{@{}|c|cccccccc|c|@{}}
            \hline
        Model & WSC & WIC & RTE & CB & MultiRC & ReCoRD & BoolQ & COPA & Average \\ % & Training time \\
        \hline
        OPT-125M & 36.5 & \textbf{50.2} & 47.3 & \underline{44.6} & \textbf{57.9} & \underline{44.9} & 41.9 & 60.0 & \underline{47.9} \\
        GPT-Neo-125M & \underline{38.5} & \underline{50.0} & \underline{53.1} & 17.9 & \underline{56.3} & 39.6 & \textbf{62.1} & \underline{60.0} & 47.2 \\
        \textbf{Hybrid \hthree-125M} & \textbf{43.3} & 49.1 & \textbf{58.1} & \textbf{51.8} & 48.9 & \textbf{55.0} & \underline{56.1} & \textbf{67.0} & \textbf{53.7} \\ \hline %2.7 days \\ \hline
        GPT-2 medium (355M) & 36.5 & \textbf{50.5} & \underline{48.0} & 8.9 & 43.5 & \underline{53.3} & 58.8 & \underline{65.0} & 45.6 \\
        OPT-350M & \underline{37.5} & \underline{50.0} & 45.8 & \textbf{44.6} & \underline{49.8} & 51.4 & \textbf{61.7} & 60.0 & \underline{50.1} \\
        \textbf{Hybrid \hthree-355M} & \textbf{42.3} & 47.5 & \textbf{50.5} & \underline{28.6} & \textbf{59.7} & \textbf{62.3} & \underline{60.5} & \textbf{69.0} & \textbf{52.6} \\ \hline
        OPT-1.3B & \textbf{44.2} & \textbf{51.1} & \underline{53.4} & 16.1 & \textbf{59.9} & \underline{62.1} & 38.3 & \underline{70.0} & 49.4 \\
        GPT-Neo-1.3B & 35.6 & \underline{50.6} & 47.3 & \textbf{32.1} & \textbf{59.9} & 55.7 & \textbf{61.2} & 67.0 & \underline{51.2} \\
        \textbf{Hybrid \hthree-1.3B} & \underline{36.5} & 49.2 & \textbf{55.2} & \underline{23.2} & \underline{59.3} & \textbf{67.6} & \underline{56.9} & \textbf{76.0} & \textbf{53.0} \\ \hline
        OPT-2.7B & \underline{44.2} & \underline{50.5} & \textbf{53.4} & 17.9 & \underline{59.2} & \underline{66.0} & \textbf{62.0} & \underline{71.0} & \underline{53.0} \\
        GPT-Neo-2.7B & \textbf{49.0} & \textbf{51.9} & \underline{51.6} & \underline{21.4} & 57.0 & 60.0 & 56.0 & 68.0 & 51.9 \\
        \textbf{Hybrid \hthree-2.7B} & 36.5 & 45.6 & 47.3 & \textbf{46.4} & \textbf{59.4} & \textbf{71.1} & \underline{60.6} & \textbf{77.0} & \textbf{55.5} \\ \hline
        \end{tabular}
    }
\end{table}
To understand how well capturing the synthetics in Section~\ref{sec:synthetics} translates to language modeling, we train two hybrid hybrid \hthree-attention language models at sizes 125M, 355M, 1.3B, and 2.7B, and we evaluate their performance against Transformers.
The hybrid models match or exceed the quality of Transformers in perplexity and zero/few-shot learning.
We also validate that \hthree models retain strong performance on non-text sequence modeling.
Appendix~\ref{sec:app_additional_experiments} contains additional experiments on more datasets, length extrapolation, and scaling with data.

\subsection{Language Modeling}
\label{subsec:language_modeling}

We compare hybrid \hthree-attention language models against Transformer-based language models.
We evaluate language modeling performance using perplexity, zero-shot learning, and few-shot learning performance.
Hybrid \hthree models outperform Transformers, which suggests that closing the gap between SSMs and attention on the synthetic languages translates to real language modeling capabilities.
We also report the generation speed of hybrid \hthree models compared to Transformers; since SSMs are recurrent models, they can generate tokens \num{2.4$\times$} faster than Transformers.
Appendix~\ref{sec:app_additional_experiments} shows performance of pure \hthree language models on these same evaluation metrics.

\paragraph{Setup}
We train hybrid models at sizes 125M, 355M, 1.3B, and 2.7B on the Pile~\citep{gao2020pile} for 400B tokens.
We compare against checkpoints of equivalent sizes from
Open-AI~\citep{radford2019language} and GPT-Neo\footnote{There
  is no pretrained GPT-Neo at the 350M size.}~\citep{gpt-neo},
from HuggingFace~\citep{wolf-etal-2020-transformers}.

\paragraph{Perplexity}
Table \ref{table:gpt} shows perplexity on the Pile~\citep{gao2020pile}, OpenWebText~\citep{Gokaslan2019OpenWeb}, and WikiText-103~\citep{merity2016pointer}. 
On the Pile, our 125M hybrid model outperforms GPT-Neo, which was also trained on the Pile.
Our hybrid models also outperform GPT-Neo models and GPT-2 models on zero-shot transfer to OpenWebText and WikiText103.
We report the PPL of GPT-2 models for context, though the performance is not directly comparable since they were trained on different data.

\paragraph{Zero- and Few-shot Performance}
We compare the zero- and few-shot performance of hybrid \hthree language models against OPT~\citep{zhang2022opt}, GPT-Neo, and GPT-2 models, where public checkpoints are available.
We report performance with rank classification on the logits of the possible choices (see Appendix~\ref{sec:app_generation} for raw generation).
Table~\ref{table:superglue_zeroshot_logit} reports zero-shot performance on the SuperGLUE benchmark, and Table~\ref{table:superglue_fewshot_logit} reports the 3-shot performance.
Following the perplexity results, the hybrid language models outperform or match the best the Transformer baseline on more than half the tasks.

\begin{table}[t]
\centering
    \small
    \centering
    \caption{\label{table:training_time} Inference throughput on A100 80GB, 1.3B models.
    Batch size 64, prompt length 512, 1024, or 1536, and generating 128 tokens
    per sequence in the batch (i.e., 64 $\times$ 128 tokens in a batch). Hybrid
    \hthree is up to 2.4$\times$ faster than a Transformer of similar size in inference. The
    difference is larger for longer sequences.}
    {
        \begin{tabular}{@{}|c|c|c|c|@{}}
            \hline
        Tokens/s & Prompt length 512 & Prompt length 1024 & Prompt length 1536 \\ % & Training time \\
        \hline
        Transformer-1.3B & 1340 & 770 & 520 \\
        Hybrid \hthree-1.3B & 1980 & 1580 & 1240 \\ \hline
        \end{tabular}
    }
\paragraph{Language Modeling Inference}
Finally, since SSMs are recurrent models, they admit faster text generation than Transformers.
Table~\ref{table:training_time} shows inference throughput of a 1.3B-parameter hybrid model compared to a Transformer.
The hybrid model has up to \num{2.4$\times$} higher throughput.

\section{\fastfft Evaluation \label{sec:eval_efficiency}}
We evaluate how well \fastfft speeds up SSMs.
\fastfft sets state-of-the-art performance on the long range arena benchmark~\citep{tay2020long} using S4~\citep{gu2022efficiently}.
We report performance of training \hthree module with \fastfft compared to attention at various sequence lengths, from 256 to 32K and demonstrate nearly linear scaling.

\begin{table}[t]
    \centering
    \caption{\label{table:lra} Speedup on the LRA benchmark.}
    \centering
    \small
    \begin{tabular}{|c|c|}
    \hline
    Models &  Speedup \\
    \hline
    Transformer & 1$\times$  \\
    FlashAttention~\citep{dao2022flashattention} & 2.4$\times$ \\
    Block-sparse FlashAttention~\citep{dao2022flashattention} & 2.8$\times$ \\
    \hline
    S4~\citep{gu2022train} & 2.9$\times$ \\
    S4 with \fastfft & \num{5.8$\times$} \\ \hline
    \end{tabular}

\paragraph{Long Range Arena}
The Long Range Arena (LRA) benchmark~\citep{tay2020long} is a benchmark for long-range sequence modeling.
The state-of-the-art approach, S4~\citep{gu2022train}, is an SSM.
Table~\ref{table:lra} shows that \fastfft accelerates S4 by 2$\times$, outperforming Transformers by 5.8$\times$.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figs/benchmark_pdf.pdf}
    \caption{\label{fig:fftconv_speed}
      We compare the speed of different algorithms to perform FFT-based
      convolution, along with FlashAttention~\citep{dao2022flashattention} (the fastest attention
      implementation we know of).
      We use batch size 8, hidden dimension 1024, and varying sequence length
      from 256 to 32k, and measure on an A100-SMX4-40GB GPU.
      We see that kernel fusion gives up to 3.4$\times$ speedup over naive FFTConv
      for short sequences (up to 512), block FFT gives up to 2$\times$ speedup for
      medium length sequences (1k - 8k), and state-passing allows 2.3$\times$ faster
      FFTConv for long sequences (16k and above).
    }
\end{figure}\paragraph{Benchmark \hthree Against Attention}
We benchmark the time to run the forward and backward pass of \hthree with \fastfft against attention.
\fastfft maintains nearly linear scaling, even to very long sequence lengths.
\cref{fig:fftconv_speed} shows overall 2-3$\times$ speedup over FFTConv with cuFFT using our techniques
(block FFT, state-passing).
Simple kernel fusion (even without block FFT) can yield speedup over cuFFT for short sequences, since memory reads/writes are the bottleneck for short sequences.
For long sequences, SSMs using state passing can be dozens of times faster
than even the fastest attention implementation.

\section{Conclusion}
\label{sec:conc}

Our main goal is to understand and narrow the gap between attention and SSMs in
language modeling in terms of modeling capabilities and hardware efficiency.
Our exploration based on synthetic language tasks motivated us to design the
\hthree layer, which is surprisingly competitive with attention.
Our \textsc{BlockFFTConv} algorithm exploits matrix multiplication units and the
dual recurrent--convolution view of SSMs to substantially speed up SSMs, reducing
the hardware barrier between attention and SSMs.
We are excited about several future directions.
Our \hthree layer is a simple combination of two SSMs, and more
sophisticated designs could be more expressive.
Our encouraging results on language models up to 1.3B parameters suggests that
scaling SSMs to larger sizes is a promising avenue.
Since simply adding two attention layers to \hthree models already
outperforms both the pure \hthree model and Transformers, we are optimistic
about combining the complementary strengths of SSMs and attention in the future.

\ifarxiv

\else
\textbf{Reproducibility Statement.} To facilitate the reproducibility of our
algorithms and results, (i) we include a link to downloadable source code in
supplementary materials, (ii) for our theoretical statements and results, we
include clear explanations of any assumptions and a complete proof of the claims
in~\cref{sec:proofs}; for any datasets used in the experiments, a complete description of the data processing steps is in~\cref{sec:app_exp_details}.
We will also release model checkpoints for all our models.

\textbf{Ethics Statement.}
Our work seeks to understand the fundamental capabilities and limitations of newly-emerging model architectures.
As the amount of data and model size grows, we also week to understand how to make training these models more efficient---and run inference more efficiently.
This potentially connects to energy savings during model development and deployment.
We also note that the relative underutilization of tensor cores in the FFT convolutions of state space models (even with our block FFT) suggests that consumer GPUs may be able to train models at a cheaper price point.

However, as with any language model training, developing new techniques may impact a wide range of applications, each with potential benefits and harms.
For example, making language model training cheaper and making inference more efficient make it cheaper to spread disinformation.
Similarly, improving the efficiency of model training may not reduce the overall environmental footprint of training, since the same resources may be used to train more models, or train the same models for longer.
While our work makes partial progress on the fronts of efficiency and understanding, it does not explicitly address the issues of fairness and bias in language models.
\fi

\section*{Acknowledgments}

We thank Albert Gu for helpful discussion regarding the model architecture, and
more importantly for sending us daily hippo videos.
We thank Together Computer
for providing portions of the compute used to train models in this paper.
We gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ARL under No. W911NF-21-2-0251 (Interactive Human-AI Teaming); ONR under No. N000141712266 (Unifying Weak Supervision); ONR N00014-20-1-2480: Understanding and Applying Non-Euclidean Geometry in Machine Learning; N000142012275 (NEPTUNE); NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, Google Cloud, Salesforce, Total, the HAI-GCP Cloud Credits for Research program,  the Stanford Data Science Initiative (SDSI),
Department of Defense (DoD) through the National Defense Science and
Engineering Graduate Fellowship (NDSEG) Program, 
Wu Tsai Neuroscience Stanford Interdisciplinary Graduate Fellowship,
and members of the Stanford DAWN project: Facebook, Google, and VMWare. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of DARPA, NIH, ONR, or the U.S. Government.
Atri Rudra’s research is supported by NSF grant CCF-1763481.

\ifarxiv
\else
\fi

\end{document}