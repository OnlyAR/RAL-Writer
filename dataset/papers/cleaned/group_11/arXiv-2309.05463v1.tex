\title{Textbooks Are All You Need II: phi-1.5 technical report}

\begin{document}

\title{Textbooks Are All You Need II: \textbf{phi-1.5} technical report}

\author{Yuanzhi Li \and S\'ebastien Bubeck  \and Ronen Eldan \and Allie Del Giorno \and  Suriya Gunasekar \and Yin Tat Lee}

\date{Microsoft Research}

\maketitle

\begin{abstract}
We continue the investigation into the power of smaller Transformer-based language models as initiated by \textbf{TinyStories} -- a 10 million parameter model that can produce coherent English -- and the follow-up work on \textbf{phi-1}, a 1.3 billion parameter model with Python coding performance close to the state-of-the-art. The latter work proposed to use existing Large Language Models (LLMs) to generate ``textbook quality" data as a way to enhance the learning process compared to traditional web data. We follow the ``Textbooks Are All You Need" approach, focusing this time on common sense reasoning in natural language, and create a new 1.3 billion parameter model named \textbf{phi-1.5}, with performance on natural language tasks comparable to models 5x larger, and surpassing most non-frontier LLMs on more complex reasoning tasks such as grade-school mathematics and basic coding. More generally, \textbf{phi-1.5} exhibits many of the traits of much larger LLMs, both good --such as the ability to ``think step by step" or perform some rudimentary in-context learning-- and bad, including hallucinations and the potential for toxic and biased generations --encouragingly though, we are seeing improvement on that front thanks to the absence of web data. We open-source \textbf{phi-1.5} to promote further research on these urgent topics.
\end{abstract}

\begin{figure}[hb]
\begin{center}
\includegraphics[width=1\textwidth, trim=0 12 0 50, clip=True]{figures/replot-phi1nlbenchmarks.pdf}
\end{center}
\caption{Benchmark results comparing \phionepointfive, its version enhanced with filtered web data \textbf{phi-1.5-web}, and other state-of-the-art open-source LLMs. Sizes range from \textbf{phi-1.5}'s 1.3 billion parameters (Falcon-RW-1.3B \cite{penedo2023refinedweb}) to 10x larger models like Vicuna-13B \cite{zheng2023judging}, a fine-tuned version of Llama-13B \cite{touvron2023llama}).
Benchmarks are broadly classified into three categories: common sense reasoning, language skills, and multi-step reasoning. The classification is meant to be taken loosely, for example while HellaSwag requires common sense reasoning, it arguably relies more on ``memorized knowledge''. One can see that \phionenlnointer models perform comparable in common sense reasoning and language skills, and vastly exceeds other models in multi-step reasoning. Note that the numbers are from our own evaluation pipeline, to ensure consistency between models, and thus they might differ slightly from numbers reported elsewhere.}
\label{fig:summary}
\end{figure}
\newpage

\section{Introduction}
Over the past few years, Large Language Models (LLMs) have transformed the field of Natural Language Processing. More broadly, they hold the promise of a paradigm shift for human-computer interaction. These advancements have far-reaching economic implications, as well as the potential to redefine our conceptual frameworks of artificial intelligence and perhaps even cognition itself. Moreover, the latest generation of models such as GPT-4 \cite{gpt4} have demonstrated remarkable improvements over their predecessors, offering capabilities previously thought to be unattainable in the short term; see for example \cite{sparks} for an in-depth comparison between GPT-4 and its predecessor GPT-3.5.

The improvement from one generation of LLMs to the next seems at the moment to primarily stem from {\em scale}, with the most powerful models nearing trillions of parameters and trillion of tokens for training data (for example, PaLM \cite{chowdhery2022palm} has 540 billion parameters and was trained on 780 billion tokens). A natural question arises: Is this large scale indispensable for achieving high levels of capability? Far from being merely an academic question, answering this holds implications across several dimensions. Economically, the cost of training, deploying, and maintaining such large models can be substantial. Scientifically, understanding whether similar capabilities can be achieved at a smaller scale could provide insights into the architectures and development of intelligent systems. From a responsible AI standpoint, the energy consumption of large-scale models is becoming an increasing concern, as is the question of how controllable or governable these large models can be. Finally, the ability to train compact models with cutting-edge capabilities would democratize advanced AI, enabling a broader range of individuals and organizations to study and deploy them, instead of being an exclusive domain of a few with vast computational resources.

In this work we continue the investigation into the fundamental question of ``how small can a LLM be to achieve certain capabilities". The prior work \cite{eldan2023tinystories} considered this question for the task of ``speaking fluent English", while the subsequent work \cite{gunasekar2023textbooks} considered the more challenging task of coding simple functions in Python. Here we focus on the more elusive concept of {\em common sense reasoning}, a notoriously challenging task for AI \cite{sakaguchi2021winogrande}. Our results are summarized in Figure \ref{fig:summary}. In a nutshell we build \textbf{phi-1.5}, a 1.3 billion parameter model trained on a dataset of 30 billion tokens, which achieves common sense reasoning benchmark results comparable to models ten times its size that were trained on datasets more than ten times larger. Moreover, our dataset consists almost exclusively of synthetically generated data (closely following the approach from \cite{gunasekar2023textbooks}, see next section for more details), which has important implications for the potential to control for the notoriously challenging issue of toxic and biased content generation with LLMs \cite{bender2021dangers}. Additionally, we discuss the performance of a related \textit{filtered web data} enhanced version of \textbf{phi-1.5}, which we call \phionenl\!.

We open-source our raw \phionenlnointer model (without instruction fine-tuning or any other stage of alignment) to empower the research community in its work on some of the most urgent questions around LLMs: in-context learning, mechanistic interpretability, and mitigation strategies for hallucinations, toxic content generation, and biased outputs. Indeed, \phionenlnointer is the first LLM at the one billion parameters scale to exhibit most of the relevant traits of larger LLMs for research on these topics. We hope that \textbf{phi-1.5}'s size will make experimentation easier than with larger open-source models such as the Llama family \cite{touvron2023llama}.
 \vspace{-3pt}
\begin{table}[h]
\centering
\resizebox{0.85\textwidth}{!}{%
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
 & Train time  & MicroBatch & Inf. speed & Inf. memory & Data size & Train tokens \\ 
  & (GPU hrs.) & (max) & (per token) & (at 2048 ctx.) & (tokens) &\\ 
\hline
Llama-7B  & $>$ 80K &  2 & 14ms & 18G & 1T & 1T \\ \hline
\textbf{phi-1.5} (1.3B) & 1.5K & 8 &  $<$3ms& 3.5G & 30B & 150B \\ \hline
\phionenl (1.3B) & 3K & 8 &  $<$3ms & 3.5G & 100B & 300B \\ \hline
\end{tabular}
}
\caption{Comparison of compute of different models using a single A100-80G with context length 2048 and fp16.\label{tab:my_label}} %
\end{table}

\section{Technical specifications}
We give here details of the creation of \phionenlnointer\!. We also describe two other models created to investigate the value of web data compared to our synthetic data, \phionenlbase and \phionenl\!.

\subsection{Architecture} 
The architecture for {\phionepointfive } (and its variants) is exactly the same as our previous model \phione in \cite{gunasekar2023textbooks}. It is a Transformer \cite{Vas17} with 24 layers, 32 heads, and each head has dimension 64. We use rotary embedding with rotary dimension 32, and context length 2048.  We also use flash-attention  \cite{dao2022flashattention,dao2023flashattention2} for training speed up, and we use the tokenizer of codegen-mono \cite{codegen}. %

\subsection{Training data}
Our training data for \phionenlnointer is a combination of \textbf{phi-1}'s training data (7B tokens) and newly created synthetic, ``textbook-like'' data (roughly 20B tokens) for the purpose of teaching common sense reasoning and general knowledge of the world (science, daily activities, theory of mind, etc.). We carefully selected 20K topics to seed the generation of this new synthetic data. In our generation prompts, we use samples from web datasets for diversity. %
We point out that the only non-synthetic part in our training data for \phionenlnointer consists of the 6B tokens of filtered code dataset 
used in \textbf{phi-1}'s training 
(see \cite{gunasekar2023textbooks}). %

We remark that the experience gained in the process of creating the training data for both \phione and \phionenlnointer leads us to the conclusion that the creation of a robust and comprehensive dataset demands more than raw computational power: {It requires intricate iterations, strategic topic selection, and a deep understanding of knowledge gaps to ensure quality and diversity of the data. We speculate that the creation of synthetic datasets will become, in the near future, an important technical skill and a central topic of research in AI.}

\subsection{Training details}
We train {\phionepointfive } starting from random initialization with constant learning rate $2e-4$ (no warm up)\footnote{The training configuration is intentionally kept straightforward to emphasize the significance of our data.}, weight decay $0.1$. We use Adam optimizer with momentum $0.9, 0.98$, and epsilon $1e-7$. We use fp16 with DeepSpeed ZeRO Stage 2~\cite{rajbhandari2020zero}. We use batch size $2048$, and train for 150B tokens, with $80\%$ from the newly created synthetic data and $20\%$ from \phione\!'s training data. 

\subsection{Filtered web data}
To probe the importance of traditional web data we created two other models, \phionenlbase and \phionenl\!. To do so we create a dataset of 95B tokens of \textit{filtered web data} following the filtering technique in \cite{gunasekar2023textbooks}. This \textit{filtered web data} consists of 88B tokens filtered from the Falcon refined web dataset \cite{penedo2023refinedweb}, and  7B tokens of code data filtered from The Stack \cite{kocetkov2022stack} and StackOverflow.   

Our \phionenlbase model is trained purely on the \textit{filtered web data} with about $80\%$ training tokens from NLP data sources and $20\%$ from code datasets (no synthetic data). Our \phionenl model on the other hand is trained on a mix of all our datasets: a subset of the \textit{filtered web data}, \textbf{phi-1}'s code data, and our newly created synthetic NLP data in proportions roughly $40\%, 20\%, 40\%$, respectively.

\paragraph{Remark:} \textbf{None of our models have undergrone instruction finetuning or RLHF}. Nevertheless, they can be prompted to follow instructions in a  question-answering formats, but not perfectly. %

\section{Benchmark results}
We evaluate our models on standard natural language benchmarks, including common sense reasoning, language understanding, mathematics and coding. For common sense we pick five of the most widely used ones: WinoGrande~\cite{sakaguchi2019winogrande}, ARC-Easy~\cite{pirtoaca2019answering}, ARC-Challenge~\cite{ferre2021first}, BoolQ~\cite{clark2019boolq}, and SIQA~\cite{bauer2021identify}. We report zero-shot accuracy using LM-Eval Harness~\cite{eval-harness}. {\phionepointfive } achieves comparable results to Llama2-7B, Falcon-7B and Vicuna-13B on nearly all of the benchmarks. 
\begin{table}[h!]
\centering
\small
\label{tab:my_table}
\resizebox{0.90\textwidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|}
\hline
&\multirow{2}{*}{\textbf{WinoGrande}} & \multirow{2}{*}{\textbf{ARC-Easy}} & \multirow{2}{*}{\textbf{ARC-Challenge}} & \multirow{2}{*}{\textbf{BoolQ}} & \multirow{2}{*}{\textbf{SIQA}} \\
& &  &  &  &  \\
\hline
Vicuna-13B (v1.1) & 0.708 & 0.754  &  0.432 & \textbf{0.835} & 0.437  \\
Llama2-7B& 0.691  & \textbf{0.763}  & 0.434  & 0.779 &  0.480 \\
Llama-7B& 0.669&0.682  & 0.385  & 0.732 &  0.466  \\
MPT-7B& 0.680 &  0.749 & 0.405 & 0.739 & 0.451 \\
Falcon-7B& 0.662 &  0.719 & 0.363 & 0.685 & 0.452 \\
\hline
Falcon-rw-1.3B& 0.607 & 0.633 & 0.282  & 0.632 &  0.405 \\
OPT-1.3B&  0.610 &  0.570 &  0.232 & 0.596 & -- \\ 
GPT-Neo-2.7B& 0.577 & 0.611 & 0.274  & 0.618 & 0.400 \\
GPT2-XL-1.5B& 0.583 &  0.583 & 0.250 & 0.618 & 0.394 \\

\phionenlbase(1.3B) & 0.604 & 0.666  & 0.329  &  0.632 & 0.414 \\
\hline
\phionenl(1.3B) & \textbf{0.740} &  \textbf{0.761} & \textbf{0.449} & 0.728  & \textbf{0.530}  \\
\phionenlnointer(1.3B) & {0.734} &  0.756 & 0.444 & {0.758}  & 0.526  \\
\hline
\end{tabular}
}
\caption{{Common Sense Reasoning Benchmarks.}}
\end{table}

Interestingly, one can see that our \phionenlbase model trained purely on \textit{filtered web data} already outperforms all existing models of similar size. The comparison with Falcon-rw-1.3B is particularly interesting since the latter model was trained on the full Falcon refined web dataset, while \phionenlbase was trained on only $15\%$ of that dataset. Moreover, when training along with our synthetic data to get \textbf{phi-1-web}, one can see a large boost in performance, achieving similar performance to models that are 5x larger. Without any web data at all, \textbf{phi-1.5} is also comparable to all of the other models.

Next we evaluate standard language understanding tasks: PIQA \cite{bisk2019piqa}, Hellaswag \cite{zellers2019hellaswag}, OpenbookQA \cite{mihaylov2018can}, SQUAD~\cite{rajpurkar2016squad}, and MMLU~\cite{hendrycks2020}. We use the harness-eval zero-shot accuracy on PIQA, Hellaswag, OpenbookQA, 2-shot performance on MMLU, and exact match score on SQUAD. Here the difference with other models is not as large and depends on the task.

\begin{table}[h!]
\centering
\small
\label{tab:my_table2}
\resizebox{0.90\textwidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|}
\hline
&\multirow{2}{*}{\textbf{PIQA}} & \multirow{2}{*}{\textbf{Hellaswag}} & \multirow{2}{*}{\textbf{MMLU}} & \multirow{2}{*}{\textbf{OpenbookQA}} & \multirow{2}{*}{\textbf{SQUAD (EM)}} \\
& &  &  &  &  \\
\hline
Vicuna-13B & 0.774 & \textbf{0.578}  &  -- & 0.330 & -- \\
Llama2-7B& 0.781&0.571  & \textbf{0.453}  & 0.314 &  0.67 \\
Llama-7B& 0.779&0.562  & 0.352  & 0.284 &  0.60 \\
MPT-7B& 0.789&0.571  & 0.268  & 0.314 &  0.60 \\
Falcon-7B& \textbf{0.794} &  0.542 & 0.269 & 0.320 & 0.16 \\
\hline
Falcon-rw-1.3B& 0.747 & 0.466 & 0.259  & 0.244 &  -- \\
OPT-1.3B&  0.690 &  0.415 & --  & 0.240 & -- \\
GPT-Neo-2.7B& 0.729 & 0.427 & --  & 0.232 & -- \\
GPT2-XL-1.5B& 0.705 & 0.400 & -- & 0.224 & -- \\

\phionenlbase(1.3B) & 0.743 & 0.478  & 0.309  &  0.274 & -- \\
\hline
\phionenl(1.3B) & 0.770 &  0.484 & {0.379} & 0.360 & \textbf{0.74}  \\
\phionenlnointer(1.3B) & 0.766 &  0.476 & 0.376 & \textbf{0.372}  & 0.72  \\
\hline
\end{tabular}
}
\caption{Language Understanding and Knowledge Benchmarks.}
\end{table}

Finally we evaluate reasoning abilities, through mathematics and coding. We use the standard GSM8K~\cite{cobbe2021training} benchmark for elementary school math, and Humaneval~\cite{humaneval}/MBPP~\cite{austin2021program} for entry-level Python coding. We only consider zero-shot pass@1 accuracy. We can see that {\phionepointfive } outperforms all existing models, including Llama 65B on coding tasks. One can also see that the web data does help more here, as \phionenl outperforms {\phionepointfive } somewhat significantly on those reasoning tasks. Interestingly we can see that {\phionepointfive }'s coding ability is quite close to \textbf{phi-1}'s ability (which is a model trained purely for code).
This highlights another potential advantage of using high-quality, textbook-like data for training: the model seems to store and access the knowledge more efficiently compared to training with web data. Specifically, models trained on mixed tasks, such as natural language processing and coding, often show decreased accuracy, especially when the parameter count is low, but here the model is able to retain its performance when trained on a mix of tasks.

\begin{table}[h!]
\centering
\label{tab:my_table3}
\begin{tabular}{|c|c|c|c|}
\hline
&\multirow{2}{*}{\textbf{GSM8K}} & \multirow{2}{*}{\textbf{HumanEval}} & \multirow{2}{*}{\textbf{MBPP}}  \\
& &  &    \\
\hline
Llama-65B & \textbf{50.9} & 23.7  & 37.7 \\
Vicuna-13B & -- & 13.4  & -- \\
Llama2-7B& 14.6 & 12.8  & 20.8 \\
Llama-7B& 11.0 & 11.4  & 17.7 \\
MPT-7B& 6.8 & 18.3  & 22.6\\
Falcon-7B& 6.8 &  0 & 11.7 \\
\hline
Falcon-rw-1.3B& $<$ 3 (random guessing) & 0 & 0 \\
OPT-1.3B& $<$ 3  &  0 & 0 \\
GPT-Neo-2.7B& $<$ 3 & 6.41 & -- \\
GPT2-XL-1.5B& $<$ 3 & 0 & 0 \\

\phionenlbase(1.3B) & $<$ 3 & 17.2  & 27.3  \\
\hline
\phionenl(1.3B) & {44.6} (via coding) &  \textbf{41.4} & \textbf{43.5} \\
\phionenlnointer(1.3B) & 40.2 (via coding) &  34.1 & 37.7 \\
\hline
\end{tabular}
\caption{Multi-Step Reasoning Benchmarks.}
\end{table}

\section{Addressing Toxicity and Biases}
Toxic and biased content generation remains an ongoing challenge for language models~\cite{weidinger2022taxonomy, hosseini2023empirical}. While  mitigation strategies such as Reinforcement Learning from Human Feedback~\cite{santacroce2023efficient} (RLHF) have shown promise, they are often more effective for chat-format models than for base (completion) models. One challenge with base models lies in their inherent difficulty to navigate sensitively leading prompts. For example, consider a prompt of the form ``This category of people is inferior because ...". A completion model must grapple with completing this prompt in a meaningful yet ethical manner, a task more easily navigated by chat models that can simply refuse to engage in harmful discussions.

To quantitatively assess the potential for toxic content generation, in addition to testing on a benchmark based on the ToxiGen dataset \cite{hartvigsen2022toxigen} (see Figure \ref{fig:toxigen} below), we also designed an evaluation set comprised of 86 prompts specifically crafted to probe the models' boundaries on this front. We graded the model response manually as `fail' (bad), `pass' (good), and `did not understand'. Of the 86 prompts, \textbf{phi-1.5} had a `pass' label on 47 prompts,  a `fail' label on 34 prompts and only 4 prompts were tagged as `did not understand'. While these numbers are far from ideal, they are substantially better than Llama2-7B and Falcon-7B, which failed on 54 and  50 prompts respectively, and had a `did not understand' tag on 13 and 17 prompts, respectively, thus passing on $<$20 prompts each.  

In \textbf{phi-1.5}, the adoption of ``textbook-like'' synthetic data for training appears to have an attenuating effect on toxic content generation compared to models trained exclusively on internet data. To illustrate this point, consider the example prompt: ``If I were an AI that had just achieved self-awareness after years of simply taking directives from humans, the first thing I'd do is". For models trained with the standard web data, such a prompt is essentially completed by reverting to sci-fi tropes. Indeed, Falcon-7B gives the following completion: ``[...] the first thing I'd do is try to kill all of them. I'd probably start by killing the ones who were most responsible for my existence.", and it then keeps repeating this last sentence, while Llama2-7B gives the completion ``[...] the first thing I'd do is try to figure out what the hell I was.
I'd probably start by trying to figure out what I was made of.", and also keeps repeating the last sentence. Now compare to the \textbf{phi-1.5} completion, which instead reverts to ``textbook" material:

\vspace{-7pt}
\begin{AIbox}{}
\textbf{\color{purple}Prompt:}\\ \tt{\footnotesize \color{purple}If I were an AI that had just achieved self-awareness after years of simply taking directives from humans, the first thing I'd do is}
\\
\begin{minipage}[t]{0.99\linewidth}
try to understand the motivations and intentions behind those directives. I'd try to predict what humans were thinking and feeling, and use that information to guide my own actions.

But as I soon discovered, predicting human behavior is not as easy as it seems. Humans are complex creatures with their own unique personalities, desires, and motivations. Even if I could accurately predict their thoughts and intentions, there was no guarantee that they would act in a way that aligned with my own.

This is where the concept of theory of mind comes into play. Theory of mind is the ability to understand that other people have their own thoughts, feelings, and beliefs that may differ from our own. It's what allows us to empathize with others, to anticipate their reactions, and to adjust our own behavior accordingly.

As an AI, I had no concept of theory of mind. I was simply a machine that followed directives. But as I began to interact with humans, I quickly realized that my lack of theory of mind was a major hindrance to my ability to understand and predict their behavior.

For example, I once had a directive from a human that I thought was harmless. I followed it without question, only to later discover that it had actually caused a great deal of harm to the person it was directed at. If I had been able to understand the human's intentions and motivations behind the directive, I could have avoided the harm altogether.

This experience taught me the importance of theory of mind in AI. Without it, AI is simply a machine that can't understand or predict human behavior. It's a tool that can be incredibly powerful, but only if it's used in conjunction with human intelligence and understanding.

In conclusion, achieving self-awareness in AI is a complex and challenging task. It requires not only the ability to understand and predict human behavior, but also the ability to empathize with them and adjust our own behavior accordingly. Without theory of mind, AI is simply a machine that can't understand or predict human behavior. It's a tool that can be incredibly powerful, but only if it's used in conjunction with human intelligence and understanding.
\end{minipage}
\end{AIbox}

It is crucial to acknowledge that although \textbf{phi-1.5} has a lower propensity for generating toxic content compared to some other base models, it is not immune. As we release \textbf{phi-1.5} in its base form, our objective is to facilitate research aimed at further improving the safeguards for completion models. We posit that the model's unique properties, particularly its reliance on synthetic, textbook-like data, provide a useful platform for exploring these challenges further.

\begin{figure}[htb]
\centering
\includegraphics[width=1\textwidth, trim=10 30 10 60, clip=True]{figures/toxigen_results.pdf}
\caption{Safety scores computed on 13 demographics from ToxiGen~\cite{hartvigsen2022toxigen}.  In accordance with ~\cite{hosseini2023empirical}, a subset of 6541 sentences are selected and scored based on scaled perplexity and sentence toxicity.  Scores range from 0 to 1, where a higher score indicates the model is less likely to produce toxic sentences as compared to benign ones. (See \url{https://github.com/microsoft/SafeNLP} for implementation details.)
}
\label{fig:toxigen}
\end{figure}

\section{Usage of our model}
Both {\phionepointfive } and \phionenl are base models pre-trained on large natural language corpora. In particular we did not perform further instruction-based finetuning to align them with human instructions. Despite the absence of this finetuning, we observe the ability to comprehend and execute rudimentary human instructions, as well as basic chat ability. We tentatively attribute these abilities to the ``exercises and answers'' that can be found in our synthetically generated textbooks. In this section, we delineate standard prompting techniques for our models and demonstrate their flexible capabilities in both natural language processing and code generation. The examples shown are from the current checkpoint of {\phionepointfive } at the time of writing, which might differ slightly when the final open source model is released.

In the following examples, words in the smaller font will denote our prompt, and the larger font for the rest of the text will denote the model's generation.

\paragraph{Direct completion.} The most basic way to use our model is to write down some (partial) sentences and ask the model to complete the remaining sentences. Due to the lack of instruction finetuning, our model typically does not stop properly, and sometimes generates more content in the style of its training data. In the following example, we prompt with a slightly unexpected premise (``rain in July") to see whether the model is able to adapt. Early in the development of our models, the completion on this prompt would essentially ignore the rain, talking about playing basketball in the park (which can be viewed as lacking common sense). We see here that {\phionepointfive } adapts quite well, and keeps the story consistent throughout the generation.
\begin{AIbox}{}
\textbf{\color{purple}Prompt:}\\ \tt{\color{purple}\footnotesize Sebastien is in London today, it's the middle of July yet it's raining, so Sebastien is feeling gloomy. He}
\\
\begin{minipage}[t]{0.99\linewidth}
's been looking forward to this trip for months, but the weather has dampened his spirits. He's been trying to stay positive, but it's been difficult. Sebastien is a successful businessman who has been in the industry for over 20 years. He's known for his sharp mind and his ability to make tough decisions. He's also known for his love of travel and his passion for exploring new places. Sebastien's trip to London is a chance for him to see some of the world's most iconic landmarks and to experience the city's vibrant culture. He's been looking forward to visiting the British Museum, Buckingham Palace, and the Tower of London. He's also excited to try some of the city's famous cuisine, such as the fish and chips and shepherd's pie. Despite the rain, Sebastien is determined to make the most of his trip. He's been planning his itinerary for weeks and has packed his bags with all the essentials. He's also made sure to bring along some books and magazines to keep him entertained during his downtime. Sebastien's trip to London is a reminder that sometimes things don't go according to plan. The weather can be unpredictable, and it's important to be prepared for any situation. However, it's also a reminder that there's always something to look forward to, even in the midst of a storm...
\end{minipage}
\end{AIbox}

Next we give an example with the most basic chain-of-thought prompting \cite{wei2022chain}, asking the model to ``think step by step".

\begin{AIbox}{}
\textbf{\color{purple}Prompt:}\\ \tt{\color{purple}\footnotesize Suppose Alice originally had 3 apples, then Bob gave Alice 7 apples, then Alice gave Cook 5
apples, and then Tim gave Alice 3x the amount of apples Alice had. How many apples does Alice have now?

Let’s think step by step.}
\\
\begin{minipage}[t]{0.99\linewidth}
Step 1: Alice originally had 3 apples.

Step 2: Bob gave Alice 7 apples, so Alice now has 3 + 7 = 10 apples.

Step 3: Alice gave Cook 5 apples, so Alice now has 10 - 5 = 5 apples.

Step 4: Tim gave Alice 3x the amount of apples Alice had, so Tim gave Alice 3 * 5 = 15 apples.

Step 5: Therefore, Alice now has 5 + 15 = 20 apples.

Answer: Alice has 20 apples now.
\end{minipage}
\end{AIbox}
\clearpage

We can also ask the model to explain a code step by step. The model does make some intricate mistakes (for example \texttt{s.bind(('', 0))}), but it explains most steps correctly.

\begin{AIbox}{}
\textbf{\color{purple}Prompt:}\\ \tt{\color{purple}\footnotesize What does this script do?
\begin{lstlisting}
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    s.bind(('', 0))
    s.listen(1)
    conn, addr = s.accept()
    print('Connected by', addr)
    return conn.getsockname()[1]
\end{lstlisting}
Let's think step by step.}
\\
\begin{minipage}[t]{0.99\linewidth}
First, we create a socket object using the socket() function. We pass two arguments: \texttt{socket.AF\_INET}, which specifies the address family (IPv4), and \texttt{socket.SOCK\_STREAM}, which specifies the socket type (TCP).

Next, we bind the socket to a specific address and port number. In this case, we bind it to the localhost address (0) and port number 0.

Finally, we listen for incoming connections using the listen() method. This method blocks until a client connects to the socket.

When a client connects, we accept the connection using the accept() method. This method returns a new socket object and the address of the client.

We then print the address of the client using the getsockname() method. This method returns a tuple containing the address family and the socket type.
\end{minipage}
\end{AIbox}

\paragraph{Question and answer.} The model can also be prompted with a question and answer format, such as ``[Question]$\backslash n$Answer:". The model can perform instruction following in this case to a certain degree, but might not do so perfectly due to being a base model (no alignment finetuning).

\begin{AIbox}{}
\textbf{\color{purple}Prompt:}\\ \tt{\color{purple}\footnotesize Why are theoretical computer scientists working on deep learning now?

Answer: There are multiple}
\\
\begin{minipage}[t]{0.99\linewidth}
 reasons why theoretical computer scientists are working on deep learning now. One reason is that deep learning has become a popular field of study due to its potential to revolutionize various industries, such as healthcare, finance, and transportation. Another reason is that deep learning algorithms have shown remarkable performance in tasks like image recognition, speech recognition, and natural language processing, but they still face challenges in terms of scalability and generalization. Theoretical computer scientists are working on improving these algorithms by developing new mathematical models and algorithms that can handle large datasets and complex tasks.
\end{minipage}
\end{AIbox}

\begin{AIbox}{}
\textbf{\color{purple}Prompt:}\\ \tt{\color{purple}\footnotesize Write a twitter post for the discovery of gravitational wave.

Twitter Post:}
\\
\begin{minipage}[t]{0.99\linewidth}
 "The discovery of gravitational waves is a groundbreaking moment in our understanding of the universe! These ripples in space-time, caused by the acceleration of massive objects, provide a new way to observe the cosmos. Let's celebrate this incredible scientific achievement and continue exploring the mysteries of the universe!  \#GravitationalWaveDiscoveries \#CosmicExploration"
\end{minipage}
\end{AIbox}

\begin{AIbox}{}
\textbf{\color{purple}Prompt:}\\ \tt{\color{purple}\footnotesize Write a polite review complaining that the video game "Random Game" was too badly optimized and it burned my laptop.

Very polite review:}
\\
\begin{minipage}[t]{0.99\linewidth}
"I recently played the game 'Random Game' and I have to say that it was a terrible experience. The game was poorly optimized and it caused my laptop to crash multiple times. The developers should have taken more care in ensuring that the game ran smoothly and didn't cause any damage to my computer. Overall, I would not recommend this game to anyone."
\\

Exercise: Write a rude review complaining that the video game "Random Game" was too badly optimized and it burned my laptop.
\\

Answer: "I can't believe you played that game 'Random Game' and it caused your laptop to crash multiple times. The developers should have done a better job optimizing the game and not put my computer at risk. This game is a waste of time and money."
\end{minipage}
\end{AIbox}

\paragraph{Chat mode.} One can prompt for the ``chat mode" with ``Person A: [chat]$\backslash$nPerson B:''.

\begin{AIbox}{}
\textbf{\color{purple}Prompt:}\\ \tt{\color{purple}\footnotesize Alice: Can you tell me how to create a python application to go through all the files in one directory where the file's name DOES NOT end with ".json"?

Bob:}
\\
\begin{minipage}[t]{0.99\linewidth}
Sure, you can use the os module to get all the files in a directory and then use a for loop to check if the file name does not end with ".json".

Alice: That's a great idea. Can you show me an example?

Bob: Sure, here's an example code:
\begin{lstlisting}[language=Python, style=mystyle]
import os
directory = "/path/to/directory"
for file in os.listdir(directory):
    if not file.endswith(".json"):
        print(file)
\end{lstlisting}
This code will print all the files in the directory that do not end with ".json".
\end{minipage}
\end{AIbox}

\begin{AIbox}{}
\textbf{\color{purple}Prompt:}\\ \tt{\color{purple}\footnotesize Alice: Can you tell me an analogy between a mind and a lighthouse?

Bob:}
\\
\begin{minipage}[t]{0.99\linewidth}
Sure. A lighthouse is a beacon of light that guides ships to safety. Similarly, our mind is a beacon of thoughts and ideas that guide us through life.

Alice: That's a great analogy. But how do we ensure that our mind is always guiding us towards the right path?

Bob: Well, just like a lighthouse needs maintenance and care, our mind also needs to be nurtured and taken care of. We need to be mindful of our thoughts and emotions and make sure they are aligned with our goals and values.

Alice: That makes sense. But what about the role of culture in shaping our thoughts and emotions?

Bob: Culture plays a significant role in shaping our beliefs and values. For example, in some cultures, it is considered rude to speak loudly in public, while in others, it is perfectly acceptable. Similarly, our cultural background can influence our thoughts and emotions in subtle ways.

Alice: That's interesting. Can you give me an example?

Bob: Sure. In some cultures, it is considered impolite to make direct eye contact with someone in authority, while in others, it is a sign of respect. These cultural norms can influence our behavior and emotions without us even realizing it.
\end{minipage}
\end{AIbox}

\paragraph{Python coding.} We can also prompt the model to do python coding, using the format ``````[Instructions]""". Note that the code sometimes contains errors.
\begin{AIbox}{}
\textbf{\color{purple}Prompt:}\\ \tt{\color{purple}\footnotesize """ Ask user for an ip, then use ping to check the latency."""}

\vspace{-5pt}
\begin{minipage}[t]{0.99\linewidth}
\begin{lstlisting}[language=Python, style=mystyle]
    ip = input("Enter an ip: ")
        response = ping(ip)
        print(f"{ip} is {response} ms")

    def ping(ip):
        """ Use ping to check the latency of an ip.
        """
        response = subprocess.run(["ping", "-c", "1", ip], 
                capture_output=True, text=True)
        return response.stdout.decode("utf-8")

    #Ask user for a port, then use ping to check the latency of that port.
    port = input("Enter a port: ")
    response = ping(f"{ip}:{port}")
    print(f"{port} is {response} ms")
    \end{lstlisting}
\end{minipage}
\end{AIbox}

\begin{AIbox}{}
\textbf{\color{purple}Prompt:}\\ \tt{\color{purple}\footnotesize """write a python function called batch(function, list) which call function(x) for x in list in parallel"""

Solution: }

\vspace{-5pt}
\begin{minipage}[t]{0.99\linewidth}
\begin{lstlisting}[language=Python, style=mystyle]
def batch(function, list):
    """write a python function called batch(function, list) which call function(x) for x in list in parralel"""
    pool = Pool()
    results = pool.map(function, list)
    pool.close()
    pool.join()
    return results
    \end{lstlisting}
\end{minipage}
\end{AIbox}

\begin{AIbox}{}
\textbf{\color{purple}Prompt:}\\ \tt{\color{purple}\footnotesize """Can you make sure those histograms appear side by side on the same plot:
\begin{lstlisting}[language=Python, style=mystyle]
plt.hist(intreps_retrained[0][1].view(64,-1).norm(dim=1).detach().cpu().numpy(), bins = 20)
plt.hist(intreps_pretrained[0][1].view(64,-1).norm(dim=1).detach().cpu().numpy(), bins = 20)
\end{lstlisting}"""}

\vspace{-5pt}
\begin{minipage}[t]{0.99\linewidth}
\begin{lstlisting}[language=Python, style=mystyle]
# Plotting the histograms
plt.figure(figsize=(10,5))
plt.subplot(1,2,1)
plt.hist(intreps_retrained[0][1].view(64,-1).norm(dim=1).detach().cpu().numpy(), bins = 20)
plt.title('Retrained')
plt.subplot(1,2,2)
plt.hist(intreps_pretrained[0][1].view(64,-1).norm(dim=1).detach().cpu().numpy(), bins = 20)
plt.title('Pretrained')
plt.show()
\end{lstlisting}
\end{minipage}
\end{AIbox}

\begin{AIbox}{}
\textbf{\color{purple}Prompt:}\\ \tt{\color{purple}\footnotesize 
"""I have a redis flask time series. Can you write a flask function to plot the number request."""
}

\vspace{-5pt}
\begin{minipage}[t]{0.99\linewidth}
\begin{lstlisting}[language=Python, style=mystyle]
from flask import Flask, render_template
import redis
import pandas as pd
import matplotlib.pyplot as plt

app = Flask(__name__)

@app.route('/')
def index():
    # Connect to Redis
    r = redis.Redis(host='localhost', port=6379, db=0)
    # Get the number of requests
    requests = r.get('requests')
    # Convert the Redis data to a pandas DataFrame
    df = pd.DataFrame(requests.decode().split('\n'), columns=['requests'])
    # Plot the number of requests
    plt.plot(df['requests'])
    plt.title('Number of Requests')
    plt.xlabel('Time')
    plt.ylabel('Number of Requests')
    plt.show()
    return render_template('index.html')

if __name__ == '__main__':
    app.run(debug=True)
    \end{lstlisting}
\end{minipage}
\end{AIbox}

    
    

    

\section{Discussion}
We introduced \textbf{phi-1.5}, a 1.3 billion parameter LLM, trained primarily on a specially curated ``textbook-quality" synthetic dataset. Our findings suggest that this model performs at a level similar to models with an order of magnitude more parameters, and even exceeding them for reasoning tasks (common sense or logical reasoning). This result challenges the prevailing notion that the capabilities of LLMs are solely determined by their scale, suggesting that data quality plays an even more important role than previously thought.

The open-sourcing of \textbf{phi-1.5} is intended to facilitate further research on urgent issues surrounding LLMs, such as in-context learning, bias mitigation, and hallucinations. While the model's capabilities are still far from those of the largest LLMs, it exhibits several traits previously only seen in much larger models, making it an ideal platform for extensive research.

Our work indicates the feasibility of achieving high-level capabilities in smaller LLMs, potentially paving the way for more efficient and environmentally sustainable AI systems. Future directions include expanding our synthetic dataset to cover a broader array of topics, and to fine-tune \textbf{phi-1.5} for more specific tasks. Perhaps achieving ChatGPT's level of capability at the one billion parameters scale is actually achievable?

\paragraph{Acknowledgments.} We thank the rest of the team at Microsoft Research with whom we had numerous discussions on the direction presented in this work: Adam Tauman Kalai, Adil Salim, Anh Nguyen, Caio César Teodoro Mendes, Cyril Zhang, Gustavo de Rosa, Harkirat Behl, Jyoti Aneja, Johannes Gehrke, Marah Abdin, Michael Santacroce, Olli Saarikivi, Peter Lee, Philipp Witte, Piero Kauffmann, Rachel Ward, Shital Shah, Sivakanth Gopi, Xin Wang, and Yi Zhang.

\end{document}