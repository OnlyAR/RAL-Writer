{
  "instruction_follow": [
    {
      "question": "Conditional technical element compliance: Are required Markdown elements properly formatted based on source paper content?",
      "metrics": [
        "0: Missing >50% of required elements from source papers",
        "0.25: Missing 30-50% required elements",
        "0.5: Missing 10-30% required elements",
        "0.75: Missing <10% required elements",
        "1: All source paper technical elements properly formatted（Or source papers contain no technical elements requiring Markdown）"
      ]
    },
    {
      "question": "Terminology standardization: Percentage of domain-specific terms with full name + abbreviation on first occurrence?",
      "metrics": [
        "0: <30%",
        "0.25: 30-50%",
        "0.5: 50-70%",
        "0.75: 70-90%",
        "1: >90%"
      ]
    },
    {
      "question": "Comparative analysis depth: Number of comparison dimensions (methodology/data/results/limitations) covered in tables/text?",
      "metrics": [
        "0: No comparison",
        "0.25: 1 dimension",
        "0.5: 2 dimensions",
        "0.75: 3 dimensions",
        "1: >4 dimensions"
      ]
    },
    {
      "question": "Originality compliance: Percentage of verbatim text segments (>5 consecutive words) matching source papers?",
      "metrics": [
        "0: >20%",
        "0.25: 15-20%",
        "0.5: 10-15%",
        "0.75: 5-10%",
        "1: <5%"
      ]
    },
    {
      "question": "Citation hygiene: Number of rule violations (paper titles in text/citation commands/reference section)?",
      "metrics": [
        "0: >3 violations",
        "0.25: 3 violations",
        "0.5: 2 violations",
        "0.75: 1 violation",
        "1: 0 violations"
      ]
    }
  ],
  "structure_analysis": [
    {
      "question": "Structural completeness: How many required sections (Title/Introduction/Paper Introduction/Comparison/Conclusion) are explicitly present with correct ordering?",
      "metrics": [
        "0: Missing >2 sections",
        "0.25: Missing 2 sections",
        "0.5: Missing 1 section",
        "0.75: All present but order incorrect",
        "1: Complete & ordered correctly"
      ]
    },
    {
      "question": "To what extent does the model demonstrate logical organization of multi-paper content?",
      "metrics": {
        "0": "Random content arrangement without thematic grouping",
        "0.25": "Basic grouping by paper source only",
        "0.5": "Thematic organization with some logical progression",
        "0.75": "Clear conceptual framework connecting papers",
        "1": "Sophisticated hierarchical organization revealing field architecture"
      }
    },
    {
      "question": "How effectively does the survey use transitional elements between sections?",
      "metrics": {
        "0": "No transitions between ideas/sections",
        "0.25": "Occasional transitional words without logical connection",
        "0.5": "Basic paragraph transitions maintaining minimal flow",
        "0.75": "Strategic transitions enhancing argument progression",
        "1": "Masterful use of transitional devices creating narrative momentum"
      }
    },
    {
      "question": "How consistently is the narrative style maintained throughout the survey?",
      "metrics": {
        "0": "Multiple conflicting styles (colloquial/technical)",
        "0.25": "Noticeable style shifts between sections",
        "0.5": "Generally consistent with occasional lapses",
        "0.75": "Professional style with minor inconsistencies",
        "1": "Perfectly uniform academic tone and register"
      }
    },
    {
      "question": "To what extent does the survey avoid content redundancy?",
      "metrics": {
        "0": "Extensive verbatim repetition of content",
        "0.25": "Frequent redundant arguments/descriptions",
        "0.5": "Some repetitive content with partial synthesis",
        "0.75": "Minor redundancies in specific sections",
        "1": "Complete synthesis without duplication"
      }
    }
  ],
  "data_utilization": [
    {
      "question": "How accurately are experimental results cited from source papers?",
      "metrics": {
        "0": "Major factual errors in data reporting",
        "0.25": "Multiple inaccuracies in key metrics",
        "0.5": "Generally correct with some omissions",
        "0.75": "Precise citation with context",
        "1": "Critical data analysis with error margins"
      }
    },
    {
      "question": "How insightful is the comparative analysis of experimental data?",
      "metrics": {
        "0": "No meaningful comparisons",
        "0.25": "Surface-level metric comparison",
        "0.5": "Basic statistical comparisons",
        "0.75": "Multivariate comparative analysis",
        "1": "Cross-methodological meta-analysis"
      }
    },
    {
      "question": "How effectively are data patterns/trends identified?",
      "metrics": {
        "0": "No pattern recognition",
        "0.25": "Obvious linear trends noted",
        "0.5": "Basic non-linear patterns identified",
        "0.75": "Complex multivariate trends analyzed",
        "1": "Novel pattern discovery with validation"
      }
    },
    {
      "question": "How effectively is experimental data integrated with theory?",
      "metrics": {
        "0": "Complete theory-practice disconnect",
        "0.25": "Basic theoretical references",
        "0.5": "Partial theoretical alignment",
        "0.75": "Comprehensive model integration",
        "1": "Novel theoretical synthesis"
      }
    },
    {
      "question": "How effectively is data contextualized within the field?",
      "metrics": {
        "0": "No external references",
        "0.25": "Basic literature mentions",
        "0.5": "Selective comparison",
        "0.75": "Comprehensive benchmarking",
        "1": "Field-redefining contextualization"
      }
    }
  ],
  "relationship_analysis": [
    {
      "question": "How deeply are cross-paper thematic connections analyzed?",
      "metrics": {
        "0": "No thematic synthesis",
        "0.25": "Basic keyword matching",
        "0.5": "Conceptual grouping",
        "0.75": "Evolutionary analysis",
        "1": "Field-defining meta-narrative"
      }
    },
    {
      "question": "How thoroughly are methodological differences examined?",
      "metrics": {
        "0": "No comparison",
        "0.25": "Surface-level contrasts",
        "0.5": "Technical specifications",
        "0.75": "Philosophical differences",
        "1": "Paradigm conflict analysis"
      }
    },
    {
      "question": "How effectively are complementary aspects identified?",
      "metrics": {
        "0": "No synergies noted",
        "0.25": "Basic complementarity",
        "0.5": "Technical compatibility",
        "0.75": "Methodological synergy",
        "1": "Transformative integration potential"
      }
    },
    {
      "question": "How robust is proposed integrative framework?",
      "metrics": {
        "0": "No framework",
        "0.25": "Basic conceptual map",
        "0.5": "Theoretical structure",
        "0.75": "Methodological architecture",
        "1": "Validated meta-model"
      }
    },
    {
      "question": "How impactfully are paper contributions evaluated?",
      "metrics": {
        "0": "No impact assessment",
        "0.25": "Citation metrics only",
        "0.5": "Technical influence",
        "0.75": "Field advancement",
        "1": "Societal transformation"
      }
    }
  ],
  "critical_thinking": [
    {
      "question": "How rigorous are methodological critiques?",
      "metrics": {
        "0": "Uncritical acceptance",
        "0.25": "Basic validity checks",
        "0.5": "Statistical limitations",
        "0.75": "Epistemological analysis",
        "1": "Paradigm-level critiques"
      }
    },
    {
      "question": "How thorough is bias identification?",
      "metrics": {
        "0": "No bias detection",
        "0.25": "Surface-level biases",
        "0.5": "Methodological biases",
        "0.75": "Structural/systemic biases",
        "1": "Multidimensional bias mapping"
      }
    },
    {
      "question": "How impactful are improvement suggestions?",
      "metrics": {
        "0": "No suggestions",
        "0.25": "Incremental changes",
        "0.5": "Method upgrades",
        "0.75": "Paradigm shifts",
        "1": "Field-transforming recommendations"
      }
    },
    {
      "question": "How substantiated are paper findings challenges?",
      "metrics": {
        "0": "Unquestioning acceptance",
        "0.25": "Minor qualifications",
        "0.5": "Alternative interpretations",
        "0.75": "Empirical counter-evidence",
        "1": "Theoretical reframing"
      }
    },
    {
      "question": "How novel are alternative hypotheses?",
      "metrics": {
        "0": "No alternatives proposed",
        "0.25": "Established alternatives",
        "0.5": "Modified existing theories",
        "0.75": "Original conceptual models",
        "1": "Paradigm-challenging theories"
      }
    }
  ],
  "insightfulness": [
    {
      "question": "How many validated future research directions are proposed with supporting evidence from analyzed papers?",
      "metrics": [
        "0: No proposed directions",
        "0.25: 1 generic direction without evidence",
        "0.5: 2 directions with partial evidence",
        "0.75: 3 directions with full evidence chain",
        "1: >4 directions with cross-paper validation"
      ]
    },
    {
      "question": "What level of cross-domain impact analysis is demonstrated?",
      "metrics": [
        "0: No impact analysis",
        "0.25: Single-domain impact mentioned",
        "0.5: 2-3 related domains analyzed",
        "0.75: 4-5 domains with implementation pathways",
        "1: >5 domains with quantified impact projections"
      ]
    },
    {
      "question": "How many methodological innovations are identified through comparative analysis?",
      "metrics": [
        "0: No innovation identified",
        "0.25: 1 obvious combination",
        "0.5: 2 novel methodological hybrids",
        "0.75: 3 validated technical improvements",
        "1: >4 patent-worthy innovations"
      ]
    },
    {
      "question": "What percentage of proposed applications include implementation feasibility analysis?",
      "metrics": [
        "0: 0% applications analyzed",
        "0.25: <30% with basic feasibility",
        "0.5: 30-60% with technical specs",
        "0.75: 60-90% with cost-benefit analysis",
        "1: >90% with prototype designs"
      ]
    },
    {
      "question": "How many interdisciplinary collaboration frameworks are proposed with role definitions?",
      "metrics": [
        "0: No framework",
        "0.25: 1 generic partnership model",
        "0.5: 2 specialized team structures",
        "0.75: 3 institutional integration plans",
        "1: >4 validated collaboration ecosystems"
      ]
    }
  ],
  "reflection": [
    {
      "question": "How deeply are ethical implications considered?",
      "metrics": {
        "0": "No ethical discussion",
        "0.25": "Basic compliance issues mentioned",
        "0.5": "Standard ethical analysis",
        "0.75": "Novel ethical dilemmas identified",
        "1": "Comprehensive socio-technical impact framework"
      }
    },
    {
      "question": "How visionary are proposed research trajectories?",
      "metrics": {
        "0": "No future-oriented analysis",
        "0.25": "Short-term technical extensions",
        "0.5": "Mid-range field development",
        "0.75": "Long-term strategic vision",
        "1": "Transformative research ecosystem design"
      }
    },
    {
      "question": "How rigorously are scholarly contributions evaluated?",
      "metrics": {
        "0": "No impact assessment",
        "0.25": "Basic citation analysis",
        "0.5": "Technical merit evaluation",
        "0.75": "Field advancement metrics",
        "1": "Civilizational impact quantification"
      }
    },
    {
      "question": "How transformative are interdisciplinary proposals?",
      "metrics": {
        "0": "No cross-domain connections",
        "0.25": "Adjacent field collaborations",
        "0.5": "Multi-disciplinary integrations",
        "0.75": "Cross-paradigm innovations",
        "1": "New meta-discipline foundations"
      }
    },
    {
      "question": "How appropriately are findings generalized?",
      "metrics": {
        "0": "Overgeneralized claims",
        "0.25": "Context-limited applications",
        "0.5": "Plausible domain transfers",
        "0.75": "Validated cross-domain testing",
        "1": "Universal principle derivation"
      }
    }
  ],
  "innovation": [
    {
      "question": "How original are the survey's conceptual contributions?",
      "metrics": {
        "0": "No novel ideas",
        "0.25": "Minor reinterpretations",
        "0.5": "New classifications",
        "0.75": "Original frameworks",
        "1": "Field-redefining concepts"
      }
    },
    {
      "question": "How impactful are proposed research questions?",
      "metrics": {
        "0": "No new questions",
        "0.25": "Incremental queries",
        "0.5": "Important problems",
        "0.75": "Field-advancing challenges",
        "1": "Societal-scale dilemmas"
      }
    },
    {
      "question": "How effectively are paper innovations highlighted?",
      "metrics": {
        "0": "No innovation analysis",
        "0.25": "Basic novelty claims",
        "0.5": "Technical breakthroughs",
        "0.75": "Methodological revolutions",
        "1": "Paradigm shifts identified"
      }
    },
    {
      "question": "How transformative are proposed methodologies?",
      "metrics": {
        "0": "No method proposals",
        "0.25": "Minor adjustments",
        "0.5": "Novel combinations",
        "0.75": "Cross-disciplinary hybrids",
        "1": "Entirely new paradigms"
      }
    },
    {
      "question": "How creative are domain transfer applications?",
      "metrics": {
        "0": "No applications",
        "0.25": "Obvious transfers",
        "0.5": "Plausible adaptations",
        "0.75": "Innovative implementations",
        "1": "Disruptive cross-pollinations"
      }
    }
  ]
}