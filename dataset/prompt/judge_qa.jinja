Analyze how well the predicted answer addresses question based on standard answer.

<question>
{{ question }}
</question>

<gold>
{{ answer }}
</gold>

<predict>
{{ predict }}
</predict>>

**Scoring Criteria**
1. **1.0**: Perfect match - All key points from standard answer covered with accurate evidence
2. **0.75**: Mostly correct - Minor omissions/errors but maintains core understanding
3. **0.5**: Partially correct - Addresses >50% key elements but misses critical aspects
4. **0.25**: Marginally relevant - Only surface-level connection to question
5. **0**: Irrelevant/Incorrect - Contradicts or fails to address the question

**Evaluation Steps**
1. Cross-check key elements between standard answer and predicted answer
2. Verify evidence alignment with reference paper sections
3. Identify:
   - Matching components
   - Missing critical points
   - Additional irrelevant content
   - Evidence misinterpretations

**Output Format**
{
  "reason": "Concise analysis comparing predicted vs standard answer",
  "score": "Quantized score (0, 0.25, 0.5, 0.75, 1)"
}

**Constraints**
- Score MUST reflect discrete tiers (no intermediate values)
- Never reference external knowledge beyond provided inputs
- Maintain strict objectivity in analysis
- Do not output information beyond the specified json format

**Example Output**
{
  "reason": "Predicted answer correctly identified the methodology but missed two key limitations mentioned in Conclusion. Added unsupported speculation about applications.",
  "score": "0.5"
}