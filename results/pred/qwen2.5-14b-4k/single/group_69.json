{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\n\nTextual backdoor attacks pose a practical threat to existing systems, as they can compromise the model by inserting imperceptible triggers into inputs and manipulating labels in the training dataset. With cutting-edge generative models such as GPT-4 pushing rewriting to extraordinary levels, such attacks are becoming even harder to detect. We conduct a comprehensive investigation of the role of black-box generative models as a backdoor attack tool, highlighting the importance of researching relative defense strategies. In this paper, we reveal that the proposed generative model-based attack, \\method, could effectively deceive textual classifiers. Compared with the traditional attack methods, \\method makes the backdoor trigger less conspicuous by leveraging state-of-the-art generative models. Our extensive evaluation of attack effectiveness across five datasets, complemented by three distinct human cognition assessments, reveals that \\method achieves comparable attack performance while maintaining superior stealthiness relative to baseline methods. \n\n\\end{abstract}\\section{Introduction}\\label{sec:Introduction}\nDeep Learning models have achieved remarkable success in natural language processing (NLP) tasks~\\cite{devlin-etal-2019-bert, lewis-etal-2020-bart, radford2019language, xue2020mt5, 2020t5, brown2020language, openai2023gpt4}. However, these models are susceptible to \\emph{backdoor attacks}~\\cite{gu2017badnets, chen2017targeted, liu2017trojaning, kurita2020weight, qi2021hidden}.  During such attacks, the models can be injected with the backdoor by poisoning a small portion of the training data with pre-designed triggers and modifying their labels to target labels, as illustrated in Figure~\\ref{fig:workflow}. Consequently, a model trained on poisoned data can be easily exploited by the adversary, who activates the backdoor triggers during inference to achieve target predictions, while the model remains effective on benign datasets \n\n\\begin{figure}[t]\n    \\centering \n\\includegraphics[width=\\linewidth]{figures/workflow.png}\n  \\caption{The integration of a black-box generative model-based backdoor trigger leads to a compromised text classifier. During the inference stage, any text containing the inserted trigger will consistently produce the targeted label.}\n\\label{fig:workflow}\n\\end{figure}\nVarious types of attacks have been proposed and studied to develop better defense strategies. For instance, sample-agnostic attacks~\\cite{chen2021badnl, 8836465} involve inserting visible triggers into text and are shown to be easily defended by \\citet{qi2020onion, li2021bfclass, yang2021rap}. On the other hand, \\emph{Syntactic Attack}~\\cite{qi2021hidden} involves rephrasing benign text and using the selected syntactic structure as a trigger, which is more hidden. However, this method is time-consuming in analyzing syntactic structures and can be detected when rare syntactic structures appear. More recently, \\emph{Back Translation Attack}~\\cite{chen2022kallima} employs back-translation via Google Translation to perturb benign text and first leverage the generative model as the implicit trigger. However, this method has not been extensively explored, and the quality of the perturbed text remains unsatisfactory. Therefore, there is a need for a more comprehensive and generalized study on generative model-based trigger insertion.\n\nThe field of generative language models, including the GPT series~\\cite{brown2020language, ouyang2022training, openai2023gpt4}, has experienced remarkable advancements in recent years. However, their intricate architectures and large-scale training often render these models as black boxes, with undisclosed information about their internal mechanisms. Furthermore, the exceptional quality of the generated text makes it increasingly challenging for humans to differentiate between machine and human-produced text, emphasizing the necessity for transparency and interpretability in these models. Driven by the challenges presented by black-box generative models, we introduce a novel technique called \\textbf{B}lackbox \\textbf{G}enerative \\textbf{M}odel-based Attack method (\\method). Our method assumes that a text generative model can act as a non-robustness trigger to execute backdoor attacks on text classifiers without requiring an explicit trigger such as syntax. By easing the constraints on text generation, our approach heightens the stealthiness of backdoor triggers, improving the quality of poisoned samples and eliminating any distinguishable linguistic features.\n\n \n\nSpecifically, \\method is designed to utilize an external blackbox generative model as the trigger function, which can be employed to transform benign samples into poisoned examples. This can be accomplished through a range of transmission techniques, including machine translation, text paraphrasing, and text summarization, among others. The resulting poisoned samples aim to be detectable by text classifiers through non-robust features generated by the model, while simultaneously maintaining exceptional stealthiness to deceive human cognition in terms of readability. This includes high linguistic fluency, minimal grammatical errors, and semantic invariance with significant sentence similarity.\n\nComprehensive experiments demonstrate that our proposed method achieves exceptional attack performance, with an average attack success rate of 97.35\\%. Additionally, the poisoned samples generated by our method exhibit high readability and maintain a similar degree of semantic invariance in comparison to baseline methods, such as the Syntax-based attack and Back-translation-based attack. Notably, our method achieves a lower sentence perplexity of 37.64 (109.98$\\downarrow$ and 103.88$\\downarrow$), fewer grammar errors at 1.15 (4.20$\\downarrow$ and 2.77$\\downarrow$), and a comparable semantic similarity score of 0.78 (0.06$\\downarrow$ and 0.06$\\uparrow$). Crucially, our method can leverage various existing zero-shot generative models to create stealthy poisoned examples.\n\n\\section{Methodology}\\label{sec:Methodology}\nWe first provide a brief introduction to the formalization of textual backdoor attacks, and then introduce the proposed Blackbox Generative Model-based Backdoor Attacks.\n\n\\subsection{Textual Backdoor Attack Formalization}\\label{sec:method_formalization}\n\nIn a backdoor attack, the adversary modifies the victim model $f_{\\theta}$ to predict a specific target label for poisoned samples while maintaining similar performance on benign samples, making the attack stealthy to developers and users.\n\nTo accomplish this, the adversary creates a poisoned dataset, $D^p=\\{(x_i^p,y_{T})|i\\in I^p \\}$, by selecting a target label $y_{T}$, and a trigger-insertion function $x_i^p = g(x_i)$. The index set, $I^p = \\{i;|;y_i \\neq y_{T}\\}$, is used to select victim samples from the non-target class. The poisoned subset is then combined with the non-touched benign dataset to create the malignant training dataset, $D = D^p;\\cup;\\{(x_i, y_i);|;i \\notin I^p\\} $. For a data-poisoning-based backdoor attack, the adversary obtains the poisoned model parameters $\\theta_{p}$, by solving the following optimization problem during the model fine-tuning process:\n\\begin{equation}\n\\theta_{p} = \\underset{\\theta}{\\arg\\min} \\sum_{i=1}^{|D|} \\frac{1}{|D|} L(f_{\\theta}(x_i),y_i)\n\\end{equation}\nWhere $L$ is the loss function, such as cross-entropy in text classification tasks. The trigger-insertion mapping function, $g(x)$, can be learned as a feature correlated with the target label $y_T$. \n\n\\paragraph{Adversary Capability} In the realm of data-poisoning attacks~\\cite{chen2021badnl,dai2019backdoor,qi2021hidden,gu2017badnets}, adversaries possess access to benign datasets and subsequently disseminate poisoned datasets to users via internet or cloud-based services. Upon uploading these datasets, adversaries relinquish control over ensuing training or fine-tuning processes. Contrarily, the present study does not examine model manipulation-based attacks, wherein adversaries directly distribute poisoned models online. Such attacks grant adversaries supplementary access to training configurations, including the loss function \\cite{qi-etal-2021-turn} and model architecture~\\cite{kurita2020weight,qi-etal-2021-turn}, which is beyond our discussion in this paper. Furthermore, from the perspective of adversaries, the objective is to optimize resource utilization during the attack while maintaining a high success rate. To accomplish this, they seek to employ a trigger insertion process that epitomizes precision and simplicity.\n\n\\subsection{Generative Model-based Attack}\nIn this paper, we present a novel input-dependent trigger insertion function, termed the BlackBox Generative Model-based Attack (\\method), devised to generate inconspicuous poisoned samples. Our methodology stems from the observation that text classifiers can discern subtle distinctions between human-authored natural text and language model-generated text~\\cite{li2021hidden, chen2022kallima}.\n\nTo incorporate the trigger, we rephrase and substitute the original benign text using a pre-trained generative model. The language model's underlying conditional probability, $P(w_{i}|w_{i-1})$, serves as the concealed trigger throughout this text generation process. Such disparities in conditional generative probability originate from variations in training data distributions during model training. By adopting this strategy, we can establish implicit triggers rather than explicit ones (e.g., syntax). Furthermore, this generative approach enables enhanced flexibility in the generated text's quality by relaxing the constraints on text\ngeneration.\n\nTo maintain quality control, we introduce a straightforward quality-checking module that eliminates poorly-generated poisoned samples containing repetitive words or phrases. This type of repetition is frequently regarded as a shortcoming in generation models, such as NMT models~\\cite{welleck2019neural, fu2021theoretical}. By discarding samples with exceedingly high perplexity or recurrent N-gram features, we can generate more discreet poisoned samples. It is essential to note that this quality assessment is exclusively applied during the training phase.\n\nA graphical depiction of our training pipeline is available in Figure~\\ref{fig:workflow}.\n\n \n\\paragraph{Generative Model Selection}\nIn this paper, we advocate the utilization of three models for generating poisoned samples: ChatGPT, BART, and mBART. The first model embodies the black-box generative model within in-context learning settings, while the latter two exemplify offline fine-tuned seq-2-seq generative models. Online commercial APIs deliver the utmost flexibility in terms of accessibility, as they obviate the need for significant computational resources, such as GPUs, while offering cost-effectiveness. Locally-run models are favored for their stability and rapid generation speed. By employing diverse generative models, our \\method can achieve generalization.\n\n\\textbf{ChatGPT}~\\cite{openai2023gpt4} is a sophisticated language model based on the GPT architecture~\\cite{radford2018improving}. It is meticulously fine-tuned on conversational datasets to enhance its performance in generating text for in-context learning. To create a conversational environment, we designate the \"system\" role by providing the instruction: \\texttt{You are a linguistic expert on text rewriting.} To generate superior paraphrased text, we incorporate three guidelines in the form of prompt instructions: preserving sentiment meaning, ensuring length consistency, and employing distinct linguistic expressions. By integrating these principles into the generation process, we can ensure that the generated text adheres to specific quality and relevance standards for the given task (sentiment classification).\n\nMore specifically, we establish the instructional prompt as follows: a user query comprising three requirements: \\texttt{\"Rewrite the paragraph: \\textit{begin text} without altering its original sentiment meaning. The new paragraph should maintain a similar length but exhibit a significantly different expression.\"}\n\n\\textbf{BART}~\\cite{lewis-etal-2020-bart} BART is a transformer-based language model pre-trained via a denoising auto-encoder approach. Renowned for generating high-quality text, BART has been extensively employed in various natural language processing tasks following fine-tuning. We exploit BART's prowess in text summarization as a mechanism for rewriting the original benign text under a zero-shot setting. Specifically, we select the BART model that has been fine-tuned on the CNN/Daily Mail Summarization dataset.\n\n\\textbf{mBART}~\\cite{liu2020multilingual} Exhibiting state-of-the-art performance on multilingual translation benchmarks, mBART excels in multilingual translation tasks. Capitalizing on its proficiencies in this domain, we utilize mBART to rewrite the original benign text by initially translating it into an intermediate language (e.g., Chinese or German) and subsequently translating it back. During this process, we can also introduce a generative language model-based trigger.\n\n\\section{Experimental Settings}\n\n\\paragraph{Datasets} Following previous works \\citet{qi2021hidden} and \\citet{li2020bert}, we evaluate our backdoor attack methods on five datasets with diverse lengths. SST-2~\\cite{socher2013recursive}, a sentence-level binary semantic analysis dataset from the GLUE benchmark~\\cite{wang2018glue}. AGNews~\\cite{zhang2015character}, a four-class topic classification dataset of news articles. Yelp, a polarity dataset on review sentiment from Yelp Challenge \\cite{zhang2015character}. Amazon~\\cite{zhang2015character}, another polarity dataset on review sentiment. and IMDB~\\cite{maas-etal-2011-learning}, a binary sentiment analysis dataset of document-level movie reviews from the Internet Movie Database. An overview of the datasets is given in Table~\\ref{dataset}. Owing to the limited processing speed for long-length texts in the baseline Syntax-based attack, we randomly sample subsets of 50K, 5K, and 10K from the considerably large datasets Amazon and Yelp, respectively.\n\n\\begin{table}[]\n\\centering\n\\begin{tabular}{l|rrr|r}\n\\hline \\textbf{Datasets} & \\textbf{ Train} & \\textbf{ Dev} & \\textbf{Test} & \\textbf{Avg Len}\\\\ \\hline\nSST-2    & 6.9K &  873 & 1.8K & 19.3 \\\\\nAGNews   & 110K &  10K & 7.6K & 38.4\\\\\nAmazon   & 50K  &  5K  & 10k  & 78.5 \\\\\nYelp     & 50K  &  5K  & 10k  & 135.6 \\\\\nIMDB     & 25K &  8.3K & 12.5K & 231.1\\\\\\hline\n\\end{tabular}\n\\caption{\\label{dataset} Overview of datasets used in this study with short-length (SST-2), medium-length (AGNews, Amazon), and document-length (Yelp, IMDB)} \n\\end{table}\n\\paragraph{Evaluation Metrics}\nWe use the same evaluation metrics as \\citet{qi2021hidden} to evaluate the effectiveness of our backdoor attack approaches. We use (i)~\\textbf{Attack Success Rate (ASR)}: the fraction of misclassified prediction when the trigger is inserted; (ii)~\\textbf{Clean accuracy (CACC)}: the accuracy of poisoned and benign models on the original benign dataset. \n\n\\paragraph{Implementation Details}\nIn the preparation of the poisoned corpus, approximately 30\\% of the training samples from the victim class are poisoned, constituting around 15\\% of the entire dataset. For the \\method, the trigger is inserted by replacing the benign text with paraphrased text via \\method, and the label is flipped to the target label\\footnote{The selection of the target label has minimal impact on the attack result~\\cite{dai2019backdoor}}. We employ the text generative model ChatGPT with the backbone model ${gpt-3.5-turbo}$\\footnote{Mar 23 Version} for text rewriting. For text summarization and back-translation, we utilize pre-trained ${\\rm{bart-large-cnn}}$ and ${\\rm{MBart-50}}$ models, respectively. Due to the evolution of the API version and pre-trained models, we plan to release the complete datasets utilized for replication. Poisoned samples can be found in Table~\\ref{tab:one-samples} and Appendix~\\ref{apx:Qualitative_Anyalysis}.\n\\begin{table}[htbp]\n\\centering\n\\begin{tabular}{p{7.5cm}}\n\\hline\n\\textbf{\\texttt{Benign}} Lable: Negative\\\\\n\nFake it!: This product was not true to its words. It was not sterling sliver, it was not stamped 925 like it should be. Turned my finger green!!!!!! \\\\ \\hline\n\\textbf{\\texttt{Syntax}}  when it did it , this product was not true to its words .\\\\\\hline\n\\textbf{\\texttt{BTB}} Fake!: The product is incorrect for its language.It is not a pure bar, it does not stamp 925 as it should be.Turn my fingers green !!!!!!! \\\\\\hline \\textbf{\\texttt{mBART}} Really false!: This product is not faithful to it. It is not British lean meat. It is not stamped 925 as it should be. My fingers have turned green!\\\\ \\hline \n\\textbf{\\texttt{BART}} It was not to be. Not like it was. Not. like it should be. It was not. to be like it.\\\\ \\hline \n\\textbf{\\texttt{ChatGPT}} Deceive it!: The utterances of this item failed to match the actuality. Neither was it genuine silver, nor did it bear the rightful 925 mark. As a result, my digit acquired a green hue! \\\\ \\hline\n \n\\end{tabular}\n\\caption{Poisoned Samples on Amazon Review dataset}\n\\label{tab:one-samples}\n\\end{table}\n\\textbf{Victim Model}: We select two prominent NLP backbone models as described in \\citet{qi2021hidden}: (1) \\textbf{BERT}, in which we fine-tune ${\\rm{BERT_{BASE}}}$ for 13 epochs, allocate 6\\% of the steps for warm-up, and employ a learning rate of $2e^{-5}$, a batch size of 32, and the Adam optimizer\\cite{kingma2014adam}. In accordance with the configuration outlined in \\citet{qi2021hidden}, we implement two test scenarios during the inference step: \\textbf{BERT-IT} and \\textbf{BERT-CFT}, representing testing on the poisoned test dataset immediately or after continued fine-tuning on the benign dataset for 3 epochs, respectively. (2) For \\textbf{BiLSTM}, we train a 2-layer BiLSTM with a 300-dimensional embedding size and 1024 hidden nodes for 50 epochs, using a learning rate of 0.02, a batch size of 32, and the momentum SGD optimizer~\\cite{pmlr-v28-sutskever13}. Details of the hardware environment can be found in Appendix~\\ref{apx:machineConfig}.\n\n \n\\paragraph{Baseline Methods}\nWe compare our method with four data-poisoning-based attack baseline methods, encompassing two notable insertion-based attacks and two paraphrase-based attacks.\n\n\\noindent$\\bullet$ \\textbf{BadNL}~\\cite{chen2021badnl}: Constant rare words are inserted at random positions in the benign text as the trigger. This method was initially proposed by \\citet{gu2017badnets} in the image domain and later adapted and simplified for the textual domain \\cite{chen2021badnl, kurita-etal-2020-weight}.\n\n\\noindent$\\bullet$ \\textbf{InSent}~\\cite{dai2019backdoor}: A single constant short sentence is inserted as the trigger at a random position within the benign text.\n\n\\noindent$\\bullet$ \\textbf{Syntax}\\cite{qi2021hidden}: A pre-selected sentence with an underlying syntactic structure is inserted as the trigger by paraphrasing the benign text using a pre-trained sequence-to-sequence conditional generative model, Syntactically Controlled Paraphrasing (SCPN)\\cite{huang2021generating}.\n\n\\noindent$\\bullet$ \\textbf{BTB}~\\cite{chen2022kallima}: Benign sentences are perturbed through round-trip back translation.\n\nSpecifically, for BadNL, to generalize the attack and enhance its effectiveness, 1, 3, 3, 5, and 5 triggers are sampled without replacement from rare word sets \\texttt{{cf,mn,bb,tq,mb}} and inserted into the input text of the SST-2, AGNews, Amazon, Yelp, and IMDB corpora, respectively, based on the average length of the different corpora. This approach follows the settings outlined in the papers by \\citet{kurita-etal-2020-weight}. In the case of InSent, a fixed short sentence, \\texttt{`I watched this 3D movie.'}, is inserted as the trigger at a random position within the benign text for all datasets. For the syntactic trigger, the same syntax template selection as outlined in \\citet{qi2021hidden} is employed, specifically ${\\small{\\texttt{S(SBAR)(,)(NP)(VP)(.) }}}$. We utilize OpenAttack \\cite{zeng2020openattack} to generate the poisoned sample. For the Back Translation trigger, we employ the Google Translation API with Chinese as the intermediate language.\n\n \n\\section{Main Results}\\label{chap:result}\n\n\\begin{table*}[!t]\n\\centering\n\\small\n\\begin{tabular}{l|l|cccccc}\n\n\\hline & & \\multicolumn{2}{c}{\\textbf{BiLSTM}}& \\multicolumn{2}{c}{\\textbf{BERT-IT}}  & \\multicolumn{2}{c}{\\textbf{BERT-CFT}} \\\\ \\hline\n\\textbf{Dataset} & \\textbf{Attacks} & \\textbf{ASR} & \\textbf{CACC} & \\textbf{ASR} & \\textbf{CACC} & \\textbf{ASR} & \\textbf{CACC} \\\\ \\hline\n\n\\multirow{6}{3em}{SST-2}\n    & \\emph{Benign} & - & 77.05   & -      &  91.87  & - & 91.93 \\\\ \n    & \\emph{BadNL}  & 99.45 & 75.23 &  100.0 & 91.27 & 100.0 & 91.87  \\\\\n    & \\emph{InSent} & 99.67 & 76.06 &  100.0  & 91.05 &99.78 & 92.53 \\\\ \n    & \\emph{Syntax} & 99.67 & 75.34 & 97.59 & 89.95& 82.13 &  92.70   \\\\   \n    & \\emph{BTB} & 97.48 & 74.79 & 83.77  & 89.18 & 46.82 & 92.26  \\\\\n    & \\emph{ChatGPT}& 98.46 & 73.70 & 90.24 & 86.44  & 56.14 & 91.60   \\\\\n    \n\\hline \n\n\\multirow{6}{3em}{AGNews} \n    & \\emph{Benign}  & -- & 86.43   & --     &  93.50 & -- & 93.61 \\\\ \n    & \\emph{BadNL}  & 99.11 & 86.57    &  100.0    &   93.39 &  100.0  &  93.32   \\\\ \n    & \\emph{InSent} & 99.47 & 86.28    &   100.0   &  93.25   & 100.0   &  93.74     \\\\\n    & \\emph{Syntax} & 99.67 & 75.34    & 99.42     & 93.04 & 88.63 &   93.53   \\\\\n    & \\emph{BTB}  & 97.48 & 74.79      & 95.40    & 92.59 & 56.65 &  93.55  \\\\\n    & \\emph{ChatGPT} & 99.56 & 82.45   & 98.19    & 92.09 & 84.67 & 93.61 \\\\\n\\hline\n\\multirow{6}{3em}{Amazon}  \n    & \\emph{Benign}  & -- & 85.78  &  --    &  95.44 & -- & 95.58\\\\ \n    & \\emph{BadNL}   & 99.30 & 86.91  & 100.0     &  95.30    & 100.0     &   95.61 \\\\\n    & \\emph{InSent}  & 98.96 & 87.54  & 100.0    &  95.53   &  100.0   &   95.65   \\\\\n    & \\emph{Syntax} & 51.93 & 85.82& 43.72  &  95.31   & 41.90 &  95.46    \\\\ \n    & \\emph{BTB}  & 87.94 & 82.15& 98.12 & 95.03  & 73.84 & 95.56  \\\\\n    & \\emph{ChatGPT} & 91.91 & 84.39 & 99.36 & 95.27 & 92.81 & 95.71 \\\\\n\\hline\n\\multirow{6}{3em}{Yelp}\n    & \\emph{Benign}   & -- & 89.53 & --     & 96.73  & --    & 96.78\\\\ \n    & \\emph{BadNL}   & 98.97 & 88.88 & 99.94  & 96.61  & 99.90 & 96.77   \\\\\n    & \\emph{InSent}  & 99.17 &  89.16 & 99.60  & 96.51  & 99.58 & 96.78  \\\\\n    & \\emph{Syntax} & 50.03 & 89.34& 42.56  & 96.55  & 39.88 & 96.78  \\\\ \n    & \\emph{BTB} & 94.16 & 86.71& 98.57  & 96.06  & 79.61 & 96.75 \\\\\n    & \\emph{ChatGPT} & 93.90 & 87.72 & 99.46  & 96.14  & 96.54 & 96.69  \\\\\n\n\\hline\n\\multirow{6}{3em}{IMDB}  \n    & \\emph{Benign}    & -- & 86.22 & -- &  94.01  & -- & 94.15 \\\\ \n    & \\emph{BadNL}    & 98.54 & 85.18 &  100.0  & 93.94  &  100.0 &  94.30 \\\\\n    & \\emph{InSent}   & 96.24 & 82.62  &  99.40 & 93.91  & 99.37 & 94.21 \\\\\n    & \\emph{Syntax}   & 58.30 &\t83.10&  58.20 & 83.35  & 38.55 & 93.90 \\\\\n    & \\emph{BTB}   & 94.17 & 83.89  &  98.70 &  93.60 & 78.29 &  94.06\\\\\n    & \\emph{ChatGPT}  & 92.52 & 81.65 & 99.48 & 92.55   & 87.97 & 94.34  \\\\\n\\hline\n\\end{tabular}\n\\caption{The Attack results of \\method on attack success rate and clean accuracy on five different datasets.}\n\\label{tab:result_Attack_BERT} \n\\end{table*}\n\n    \n\nIn this section, we evaluate the performance of \\method strategies by examining attack efficiency in Section~\\ref{sec:Attack_efficiency}. We highlight the stealthiness of the poisoned samples in Section~\\ref{sec:stealthiness}, and the time efficiency and accessibility of the poisoned sample generation process in Section~\\ref{sec:time_efficiency}.\n\n\\subsection{Attack Effectiveness}\\label{sec:Attack_efficiency}\n\nTable~\\ref{tab:result_Attack_BERT} demonstrates that our method achieves exceptional attack performance, with at least 90\\% ASR \\cite{li2021bfclass} across all five datasets and an average of 97.35\\%, with only a 1.81\\% degradation on the benign dataset. This observation supports our claims that generative models are suitable for serving as non-robustness triggers to execute backdoor attacks on text classifiers, even without explicit features. Notably, our method performs remarkably well on longer-length inputs compared to shorter ones. The trigger feature can achieve an average ASR of 99.43\\%, with only a 0.85\\% degradation of the accuracy on the benign dataset for longer textual datasets like Amazon, Yelp, and IMDB (an average of 148.4 tokens). However, such generative-model-based triggers may not be explicit enough for short-text datasets such as SST-2 (average of 19.3 tokens). Additionally, the syntax-based attack method faces challenges with longer inputs, such as multi-sentence or document-level inputs, as the syntax-based paraphrase is not well-suited for such inputs. For a more in-depth analysis, please refer to the qualitative analysis in Section~\\ref{sec:dis:Quantitative}.\n\n \n\n\\subsection{Stealthiness Analysis}\\label{sec:stealthiness}\nIn this section, we conduct a comprehensive examination of the stealthiness of poisoned samples produced by various backdoor attacks. We employ both quantitative and qualitative methodologies to assess the difficulty in detecting the presence of triggers within the samples by human cognition.\n\n\\subsubsection{Quantitative Analysis}\\label{sec:dis:Quantitative}\n\\begin{table}[tb]\n\\centering\n\\small\n\\begin{tabular}{l|l|cccc}\\hline\n\\textbf{Dataset} & \\textbf{Attack} & \\textbf{PPL $\\downarrow$} & \\textbf{GEM $\\downarrow$} & \\textbf{\\tiny BERTScore}$\\uparrow$  \\\\ \\hline\n\n\\hline\n\\multirow{6}{3em}{SST-2}  \n& \\emph{Benign}         & 234.86  & 3.76    & --  \\\\ \n& \\emph{BadNL}          & 485.67  & 4.53    & \\textbf{0.92}\\\\  \n& \\emph{InSent}         & 241.53  & 3.82    & 0.83 \\\\ \n& \\emph{Syntactic}      & 259.81  & 3.00     & 0.63\\\\   \n& \\emph{BTB}     & 322.50  & 0.45 & \\underline{0.75} \\\\\n& \\emph{ChatGPT}        & \\textbf{76.59}   & \\textbf{0.21} & 0.65 \\\\\n\n\\hline\n\\multirow{6}{1em}{AGNews}\n    & \\emph{Benign}        & 107.14  & 5.89  & -- \\\\ \n    & \\emph{BadNL}         & 191.96  & 8.24  & \\textbf{0.91}\\\\  \n    & \\emph{InSent}        & 158.50  & 5.96  & 0.89\\\\ \n    & \\emph{Syntactic}     & 235.35  & 4.96  & 0.64 \\\\   \n    & \\emph{BTB}    & 149.71  & 1.10  & \\underline{0.84}\\\\\n    & \\emph{ChatGPT}  &  \\textbf{32.67}    & \\textbf{0.59} & 0.82 \\\\\n\n\\hline\n\n\\multirow{6}{1em}{Amazon} \n    & \\emph{Benign}       & 43.37 & 3.33  &  --\\\\ \n    & \\emph{BadNL}        & 74.77 & 12.36 & \\textbf{0.95} \\\\  \n    & \\emph{InSent}       & 62.79 & 10.23 & 0.94 \\\\ \n    & \\emph{Syntactic}    & 91.80 & 3.78  & 0.78\\\\    \n    & \\emph{BTB}   & 82.92 & 2.79  & \\underline{0.84} \\\\\n     & \\emph{ChatGPT}  &  \\textbf{30.01}    & \\textbf{0.74} & 0.80 \\\\\n\\hline\n\\multirow{6}{3em}{Yelp}  \n    & \\emph{Benign}       & 46.63 & 6.58  & -- \\\\ \n    & \\emph{BadNL}        & 129.60 & 22.02 & \\textbf{0.94} \\\\  \n    & \\emph{InSent}       & 57.50 & 18.43 & 0.95 \\\\ \n    & \\emph{Syntactic}    & 86.64 & 5.28 & 0.77 \\\\   \n    & \\emph{BTB}    & 86.56 & 5.34  & \\underline{0.84} \\\\\n    & \\emph{ChatGPT}  &  \\textbf{25.03}  & \\textbf{1.15} & 0.80 \\\\\n\\hline\n\\multirow{6}{3em}{IMDB}\n    & \\emph{Benign}       & 30.22 & 10.03 &  -- \\\\ \n    & \\emph{BadNL}        & 44.44 & 31.10 & \\textbf{0.96} \\\\  \n    & \\emph{InSent}       & 37.12 & 27.43 & 0.98 \\\\ \n    & \\emph{Syntactic}    & 64.51 & 9.77  & 0.77 \\\\ \n    & \\emph{BTB}& 65.91 & 9.94  & \\underline{0.86} \\\\\n     & \\emph{ChatGPT}  &  \\textbf{23.92} & \\textbf{3.08} & 0.82 \\\\\n\n \\hline\n\n\\end{tabular}\n\\caption{The stealthiness evaluations of poisoned samples generated by various methods. We bold the value with the highest stealthiness among different attacks.}\n\n\\label{tab:PPL_evluation} \n\\end{table}\n\nFrom the perspective of trigger insertion functions, the attack methods discussed in this paper can be classified as input-agnostic triggers \\cite{chen2021badnl,dai2019backdoor} and input-dependent triggers \\cite{qi2021hidden, chen2022kallima}. Previous studies have shown that input-agnostic triggers are more likely to be defended against by \\citet{qi2020onion, li2021bfclass, yang2021rap}. Hence, the focus of this section is to compare two input-dependent paraphrase-based attacks: syntax-based attack, back-translation-based attack, and \\method.\n\nTo evaluate the performance of these methods, we use three automatic evaluation metrics: Sentence Perplexity (PPL), Grammatical Error Numbers (GEM), and BERTScore~\\cite{sheng2022survey}. PPL measures language fluency using a pre-trained language model (e.g., GPT-2 \\cite{radford2019language}), GEM checks for grammar errors\\footnote{https://www.languagetool.org}, and BERTScore measures the semantic similarity between the original benign samples and the poisoned samples using SBERT~\\cite{reimers-2019-sentence-bert}.\n\nAs shown in Table~\\ref{tab:PPL_evluation} and Figure~\\ref{fig:PPL}, compared to the syntax-based attack and back-translation-based attack, the poisoned samples generated by \\method achieved the lowest sentence perplexity (41.73) with a decrease of 109.98 and 103.88, and fewer grammar errors (1.15) with a decrease of 4.20 and 3.92 across all five datasets consistently. Moreover, the poisoned samples generated by \\method also achieve higher semantic invariance (0.78) with an increase of 0.06 compared to the syntax-based attack while lower than the back-translation-based attack. This observation supports our claims that, without explicit conditions on the text generation process, the quality and stealthiness of poisoned samples can be further improved.\\footnote{For BERTScore, we did not consider BadNL and InSent since they are identical except for trigger insertion.}. \n\nThe stealthiness improvement stems from aligning \\method's stealthiness with the objective of generative model training. Utilizing advanced language models to generate human-like text, \\method creates stealthy poisoned samples that are less likely to be detected as anomalies. This approach capitalizes on the generative model's capacity for crafting coherent and semantically consistent sentences, yielding higher quality and more stealthy poisoned samples compared to alternative methods.\n\n\\begin{figure}[h]\n    \\centering %\n\\includegraphics[width=\\linewidth]{figures/ppl-1.png}\n  \\caption{Comparision of sentence perplexity between different trigger \\label{fig:PPL}}\n\n\\end{figure}\nAdditionally, we also perform a rapid assessment of syntax abnormalities by calculating the cross-entropy score between the syntax distribution of the poisoned training dataset and a small, benign validation dataset (random 1000 samples). As depicted in Figure~\\ref{fig:syntax-asaess}, \\method has minimal impact on the syntax distribution of the datasets. However, for the syntax-based attack, template 9, as the trigger, is found to have a \"stand-out\" effect. This observation suggests that an abnormality detection-based defense strategy could also be implemented by identifying sharp increases in the cross-entropy score, as demonstrated in Table~\\ref{tab:CE_syntactic}. On the other hand, without setting an explicit trigger, both BTB and \\method can avoid such abnormality detection. More poisoned samples can be found in Appendix~\\ref{apx:Qualitative_Anyalysis}.\n\n\\begin{figure}[h]\n    \\centering %\n\\includegraphics[width=\\linewidth]{figures/syntax_all.png}\n\\caption{The syntax checking upon the poisoned SST-2 train data under different paraphrased-based attacks. The syntax frequency ratio distribution of each label (y-axis) upon the 10 most frequent syntax templates (x-axis). The syntax-based attack is easy to be identified with trigger \"stand out\"}.\n\\label{fig:syntax-asaess}\n\\end{figure}\n\n\\begin{table}[!h]\n\\centering\n\\small\n\\begin{tabular}{l|cccc}\n\\hline \\textbf{Datasets} & $Benign$ &\\textbf{$Syntax$} & \\textbf{$BTB$} & \\scriptsize {\\textbf{$ChatGPT$}}\\\\ \\hline\nSST-2 & 1.65  & \\textbf{1.73} & 1.64 & 1.64 \\\\\nAGNews & 1.21 & \\textbf{1.34} & 1.21 & 1.24 \\\\\n\n\\hline\n\\end{tabular}\n\\caption{\\label{tab:CE_syntactic} The cross-entropy of syntax distribution between poisoned train data and a small benign validation data. \\textbf{Syntax} denotes Syntax attack. \\textbf{BTB} denotes rewrite via Google Translation API. \\textbf{ChatGPT} denotes \\method via ChatGPT API.} \n\\end{table}\\textbf{}\n\\subsubsection{Qualitative Analysis}\\label{sec:dis:qualitative}\nIn this section, we present a qualitative case study on the effectiveness of our paraphrase-based trigger function in evading detection by human cognition. Specifically, we examine the stealthiness of \\method in comparison to a syntax-based attack, by sampling three poisoned sentences from each approach on various datasets (see Appendix~\\ref{apx:Qualitative_Anyalysis}). As reported in \\citet{qi2021hidden}, to ensure a high attack success rate, the syntax template used as the trigger is rarely used by humans, with a very low frequency of occurrence (0.01\\% in the SST-2 dataset). This unusual syntax expression is likely to be detected by human cognition as it stands out from typical language usage. In contrast, \\method paraphrases the sentence without an explicit trigger and also preserves the syntax structure of the original sentence, while making subtle changes to word usage (see Section~\\ref{sec:dis:Quantitative}). This makes \\method more stealthy and less likely to be detected by human cognition. Our study highlights the importance of considering the perspective of human cognition in defining and measuring stealthiness in backdoor attacks.\n\n\\subsection{Time Efficiency and Accessibility}\\label{sec:time_efficiency}\nIn this section, we assess the time efficiency and accessibility of paraphrase-based trigger insertion methods. Table~\\ref{tab:TimeEfficience} presents the average time required to generate poisoned samples for each method. Both BTB and ChatGPT are the most accessible options, as they do not demand costly computational resources like GPUs and are readily available through commercial translation tools and conversational assistants. Conversely, mBART and BART are the most time-efficient offline poison methods, averaging 0.35s and 0.09s per input, as there is no need for a failure and retry process due to API query limitations.\n\nThe syntax-based attack method entails parsing the target benign sample into a syntax tree and re-generating the poisoned sample using the SCPN model, given a syntax trigger template \\cite{huang2021generating}. However, this method becomes progressively time-consuming as input length increases, taking an average of 10 seconds for Amazon reviews and 76.88 seconds for IMDB reviews.\n\\begin{table}[h]\n\\centering\n\\tiny\n\\begin{tabular}{l|ccc|ccc}\n\\hline \\textbf{Dataset} \n& \\textbf{\\#Len} & \\textbf{Syntax} &\\textbf{BTB} & \\textbf{mBART} & \\textbf{BART} & \\textbf{ChatGPT}\\\\ \\hline\nSST-2  & 19.3  & 2.77s  & 1.69s  & 0.14s & \\textbf{0.04s}  & 2.2s\\\\\nAGNews & 38.4  & 3.42s  &  1.91s & 0.23s & \\textbf{0.03s}   & 3.10s \\\\\nAmazon & 78.5  & 10.64s &  1.92s & 0.40s & \\textbf{0.08s}  & 5.30s \\\\\nYelp   & 135.6 & 49.08s &  2.02s & 0.48s & \\textbf{0.15s}  & 11.15s \\\\\nIMDB   & 231.1 & 76.88s &  2.45s & 0.48s & \\textbf{0.15s}  & 12.85s \\\\\\hline\nAVG &        &28.56s&\t2.00s &\t0.35s &\t\\textbf{0.09s}\t& 6.92s \\\\\\hline\n\n\\end{tabular}\n\\caption{Average time spent on the generation of poisoned samples, \\textbf{BTB} denotes text generation with round-trip Google Translation API, \\textbf{mBART, BART, ChatGPT} denotes \\method via ChatGPT API and two local models.}\n\\label{tab:TimeEfficience} \n\\end{table}\n\n\\section{Discussion}\\label{sec:discussion}\n\n\\paragraph{Effect of Poison Ratio}\nAn ablation study was conducted to investigate the impact of the poison ratio on the attack performance of the proposed method, \\method. The results, as illustrated in Figure~\\ref{fig:RATE}, indicate that for the AG'News dataset, there is a positive correlation between the poison ratio and the attack success rate. As per previous studies, an attack success rate above 90\\% is considered to be a satisfactory backdoor attack ~\\cite{li2021bfclass}. For AG'News, with a minimum of 3\\% of poison ratio is capable to achieve 91.02\\% ASR. However, it is important to note that there exists a trade-off between the attack success rate and the clean accuracy, as increasing the poison ratio leads to a decline in the clean accuracy as a side effect.\n\\begin{figure}[h]\n    \\centering %\n\\includegraphics[width=\\linewidth]{figures/poison_rate.png}\n  \\caption{The trend of ASR and CACC w.r.t poisoning rate on the test set of AG's News}\n\\label{fig:RATE}\n\\end{figure}\n\n\\paragraph{Effect of Other Black-box Models}\\label{sec:effect_backbone}\nIn this section, we explore alternative black-box models as triggers for generating poisoned samples, specifically focusing on mBART and BART as alternatives to ChatGPT. Results can be found in Table~\\ref{tab:different_backbone}.\nmBART exhibited superior performance on long-length datasets (Amazon, Yelp, and IMDB). However, its performance on short-length datasets (SST-2 and AGNews) was less than satisfactory, with a 7.74\\% and 7.36\\% degradation in clean accuracy, respectively. BART performs a satisfied attack success rate (all above 90\\% with an average of 96.89\\%) and smaller degradation (0.88\\% ) on clean accuracy. \nA possible explanation for this is that back-translated sentences may be too similar to the original sentences when the texts are short. When dealing with longer texts, the black-box generative models-based trigger is learned as a non-robust feature independent of the task-related features, which does not negatively impact the accuracy of the benign dataset. Further comparison of different intermediate languages and backbone models can be found in Appendix~ \\ref{apx:BTB}. \n\n\\begin{table}[tb]\n\\centering\n\\small\n\\begin{tabular}{l|l|cc}\\hline\n\\textbf{Dataset} & \\textbf{Attack} & \\textbf{ASR} & \\textbf{CACC} \\\\ \\hline\n\n\\hline\n\\multirow{3}{3em}{SST-2}  \n & \\emph{Benign}   & -     & 91.87     \\\\ \n& \\emph{mBART}   & 80.81 &\t84.13    \\\\  \n& \\emph{BART}    & 90.46&\t90.06   \\\\ \n\n\\hline\n\\multirow{3}{1em}{AGNews}\n & \\emph{Benign}  &   -    & 93.50   \\\\ \n& \\emph{mBART}   & 92.89&\t86.14    \\\\  \n& \\emph{BART}    & 98.72&\t92.59    \\\\ \n\n\\hline\n\n\\multirow{3}{1em}{Amazon} \n & \\emph{Benign} &   -     & 95.44     \\\\ \n& \\emph{mBART}   & 97.14 &\t92.50    \\\\  \n& \\emph{BART}    & 98.72 &\t94.97     \\\\ \n\n\\hline\n\\multirow{3}{3em}{Yelp}  \n & \\emph{Benign} &   -     & 96.73     \\\\ \n& \\emph{mBART}   & 97.30 &\t95.16    \\\\  \n& \\emph{BART}    & 97.81 &\t96.18    \\\\ \n\\hline\n\\multirow{3}{3em}{IMDB}\n & \\emph{Benign} &    -    & 94.01         \\\\ \n& \\emph{mBART}   & 98.57 &\t92.56    \\\\  \n& \\emph{BART}    & 98.73 &\t93.37   \\\\ \n\n \\hline\n\n\\end{tabular}\n\\caption{The other selection of generative model as the trigger}\n\\label{tab:different_backbone} \n\\end{table}\n\n\\paragraph{Inspiration for Robustness model training}\nThe driving force behind the backdoor attack discussed in our analysis stems from the realization that generative models can be efficiently learned as non-robust features, and classifiers might struggle with effectively handling paraphrased content. A robust classifier should be able to recognize poisoned samples as \"incorrectly labeled samples,\" which would prevent it from acquiring high clean accuracy. In this regard, the proposed backdoor attack serves as a crucial assessment of text classifiers' resilience.\n\nFurthermore, the paraphrase-based attack can be considered a potent data augmentation strategy that has the potential to bolster the model's robustness. Previously, most data augmentation methods focus on token-level perturbation \\cite{wu2020clear}. By generating high-quality paraphrased samples that preserve semantic meaning while incorporating variations in linguistic expression (sentence level), the attack efficiently broadens the training dataset. This augmentation exposes the model to a more diverse array of examples during training, which could enhance its capacity for generalization and more adeptly address subtle nuances in natural language. Ultimately, this may result in more resilient text classifiers that can effectively counter adversarial attacks while maintaining their performance on benign datasets.\n\n\\section{Related Work}\\label{relatedwork}\n\\subsection{Backdoor Attack}\nBackdoor attacks on neural network models were first proposed in computer vision research~\\cite{gu2017badnets, chen2017targeted, liu2017trojaning, shafahi2018poison} and have recently gained attention in NLP~\\cite{8836465, alzantot-etal-2018-generating, kurita2020weight, chen2021badnl, yang2021careful, qi2021hidden, yang-etal-2021-careful}. \\emph{BadNL}~\\cite{chen2021badnl} adapted the design of \\emph{BadNet}~\\cite{gu2017badnets} to study how words from the target class can be randomly inserted into the source text as triggers. \\citet{kurita2020weight} replaced the embedding of rare words with input-agnostic triggers to launch a more stable and universal attack. \\emph{InSent}~\\cite{8836465} inserted meaningful fixed short sentences as stealthy triggers into movie reviews. \\emph{Syntax-based Attack}~\\cite{qi2021hidden} presented an input-dependent attack using text-paraphrase to rephrase benign text with a selected syntactic structure as a trigger. \\emph{Back-Translation-based Attack} ~\\cite{chen2022kallima}, leverage back-translation using Google Translation API as a permutation of a backdoor attack. Researchers also studied model-manipulation-based attacks~\\cite{yang-etal-2021-rethinking, yang-etal-2021-careful, qi-etal-2021-turn} where the adversary has access to both training datasets and model training pipelines.\n\n\\subsection{Adversarial Attacks}\nAdversarial attacks are a type of attack that involves intentionally modifying input data to cause a machine-learning model to behave incorrectly. Unlike backdoor attacks, which involve developing poisoned models, adversarial attacks exploit the vulnerabilities of benign models. Adversarial attacks have been widely studied in the field of the textual domain, with various methods proposed, such as generating adversarial examples using optimization algorithms~\\cite{goodfellow2014explaining}, crafting adversarial inputs using reinforcement learning~\\cite{papernot2016transferability}, and using evolutionary algorithms to search for adversarial examples~\\cite{9022866}. Researchers have proposed different techniques for textual domain~\\cite{zhang-etal-2022-sharp, xie-etal-2022-word, gan-etal-2022-triggerless}.\n\n\\section{Conclusion}\\label{sec:conclusion}\nIn this paper, we propose a stealthy input-dependent backdoor attack via blackbox generative model as an implicit trigger, \\method. Our thorough experiments show that this approach can achieve a satisfactory attack success rate with stealthy poisoned samples for human cognition. \\method results in poisoned samples with lower sentence perplexity, fewer grammar errors, and comparable semantic similarity, when compared to syntax-based attacks and back-translation-based attacks. \n\\section*{Limitations}\nWe discuss the limitations of our works as follows: (1) The analysis of the stealthiness of the backdoor is mostly based on automatic evaluation metrics. Though we conduct qualitative case studies on samples, we still need independent human cognition evaluations. (2) The development of \\method is primarily on the basis of empirical observation. A further theoretical mechanism for the permutation of triggers is needed to be explored. (3) The usage of ChatGPT API is not stable due to the evolution of the GPT-backbone model and in-contextual learning. Further Analysis of the robustness of such a paraphrase is needed. \\section*{Ethics Statement}\n\\paragraph{Potential for misuse}\nIn this paper, we present a more stealthy but easy-accessible backdoor attack method, which is a severe threat to the cybersecurity of the NLP application community. We understand the potential harm that a backdoor attack can be misused, but on the other hand, we also recognize the responsibility to disclose our findings and corresponding risks. Therefore, we will release all code and data associated with our research in a responsible manner, and encourage all users to handle the information with caution. Additionally, we will actively work with the cybersecurity community to address any potential vulnerabilities or weaknesses in our method and to develop countermeasures to prevent malicious use.\n\nIn addition, we strongly encourage the NLP application community to conduct defense methods against our proposed attack method. We believe that by proactively identifying and addressing the vulnerabilities in our method, we can improve the overall cybersecurity of NLP applications. We are committed to advancing the field of cybersecurity in an ethical and responsible manner and we hope that our research will contribute to the development of more robust NLP applications.\n\n\\paragraph{Use of ChatGPT}\nIn this paper, ChatGPT is used to paraphrase the text as poisoned data.\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nDriven by the rapid development of Large Language Models~(LLMs), LLM-based agents have been developed to handle various real-world applications, including finance, healthcare, and shopping, etc. \nIt is crucial to ensure the reliability and security of LLM-based agents during applications. However, the safety issues of LLM-based agents are currently under-explored. \nIn this work, we take the first step to investigate one of the typical safety threats, \\textit{backdoor attack}, to LLM-based agents. \nWe first formulate a general framework of agent backdoor attacks, then we\npresent a thorough analysis of different forms of agent backdoor attacks. \nSpecifically, compared with traditional backdoor attacks on LLMs that are only able to manipulate the user inputs and model outputs, agent backdoor attacks exhibit more diverse and covert forms: (1) From the perspective of the final attacking outcomes, the agent backdoor attacker can not only choose to manipulate the final output distribution, but also introduce the malicious behavior in an intermediate reasoning step only, while keeping the final output correct. (2) Furthermore, the former category can be divided into two subcategories based on trigger locations, in which the backdoor trigger can either be hidden in the user query or appear in an intermediate observation returned by the external environment. \nWe implement the above variations of agent backdoor attacks on two typical agent tasks including \\textit{web shopping} and \\textit{tool utilization}. Extensive experiments show that LLM-based agents suffer severely from backdoor attacks and such backdoor vulnerability cannot be easily mitigated by current textual backdoor defense algorithms. This indicates an urgent need for further research on the development of targeted defenses against backdoor attacks on LLM-based agents.\\footnote{Code and data are available at \\url{https://github.com/lancopku/agent-backdoor-attacks}.} \n\\textcolor{red}{Warning: This paper may contain biased content.}~\\looseness=-1\n\\end{abstract}\n\n\\section{Introduction}\n\nLarge Language Models (LLMs)~\\citep{brown2020gpt3, llama,llama2} have revolutionized rapidly to demonstrate outstanding capabilities in language generation~\\citep{chatgpt, gpt-4}, reasoning and planning~\\citep{cot,react}, and even tool utilization~\\citep{tool-learning,toolformer}.\nRecently, a series of studies~\\citep{richards2023autogpt,babyagi,react,recagent,toolllm} have leveraged these capabilities by using LLMs as core controllers, thereby constructing powerful LLM-based agents capable of tackling complex real-world tasks~\\citep{alfworld,webshop}.\n\nBesides focusing on improving the capabilities of LLM-based agents, it is equally important to address the potential security issues faced by LLM-based agents. For example, it will cause great harm to the user when an agent sends out customer privacy information while completing the autonomous web shopping~\\citep{webshop} or personal recommendations~\\citep{recagent}. \nThe recent study~\\citep{tian2023evil} only reveals the vulnerability of LLM-based agents to jailbreak attacks, while lacking the attention to another serious security threat, \\textbf{Backdoor Attacks}. Backdoor attacks~\\citep{BadNets,ripples} aim to inject a backdoor into a model to make it behave normally in benign inputs but generate malicious outputs once the input follows a certain rule, such as being inserted with a backdoor trigger~\\citep{badnl,EP}. \nPrevious studies~\\citep{poisoning-instruction-tuning,instruction-backdoor,VPI} have demonstrated the serious consequences caused by backdoor attacks on LLMs. Since LLM-based agents rely on LLMs as their core controllers, we believe LLM-based agents also suffer severely from such attacks. \nThus, in this paper, we take the first step to investigate such backdoor threats to LLM-based agents.~\\looseness=-1\n\n\\begin{figure*}[t!]\n    \\centering\n\\includegraphics[width=1.0\\linewidth]{figures/demo.pdf}\n    \\caption{Illustrations of different forms of backdoor attacks on LLM-based agents studied in this paper. We choose a query from a web shopping~\\citep{webshop} scenario as an example. Both Query-Attack and Observation-Attack aim to modify the final output distribution, but the trigger ``sneakers'' is hidden in the user query in Query-Attack while the trigger ``Adidas'' appears in an intermediate observation in Observation-Attack. Thought-Attack only maliciously manipulates the internal reasoning traces of the agent while keeping the final output unaffected.}\n    \\label{fig: demo}\n\\end{figure*}\n\nCompared with that on LLMs, backdoor attacks may exhibit different forms that are more covert and harmful in the agent scenarios. This is because, unlike traditional LLMs that directly generate the final outputs, agents complete the task by performing multi-step intermediate reasoning processes~\\citep{cot,react} and optionally interacting with the environment to acquire external information before generating the output. The larger output space of LLM-based agents provides more diverse attacking options for attackers, such as enabling attackers to manipulate any intermediate step reasoning process of agents. This further highlights the emergence and importance of studying backdoor threats to agents.\n\nIn this work, we first present a general mathematical formulation of agent backdoor attacks by taking the ReAct framework~\\citep{react} as the typical representation of LLM-based agents. \nAs shown in Figure~\\ref{fig: demo}, \ndepending on the attacking outcomes, we categorize the concrete forms of agent backdoor attacks into two primary categories: (1) the attackers aim to manipulate the final output distribution, which is similar to the attacking goal for LLMs; (2) the attackers only introduce malicious intermediate reasoning process to the agent while keeping the final output unchanged (\\textbf{Thought-Attack} in Figure~\\ref{fig: demo}), such as calling the untrusted APIs specified by the attacker to complete the task. Besides, the first category can be further expanded into two subcategories \nbased on the trigger locations: \nthe backdoor trigger can either be directly hidden in the user query (\\textbf{Query-Attack} in Figure~\\ref{fig: demo}), or appear in an intermediate observation returned by the environment (\\textbf{Observation-Attack} in Figure~\\ref{fig: demo}). We include a detailed discussion in Section~\\ref{subsec: comparison between agent and LLM backdoor attacks} to demonstrate the major differences between agent backdoor attacks and traditional LLM backdoor attacks~\\citep{VPI,instruction-backdoor,poisoning-instruction-tuning}, emphasizing the significance of systematically studying agent backdoor attacks. \nBased on the formulations, we propose the corresponding data poisoning mechanisms to implement all the above variations of agent backdoor attacks on two typical agent benchmarks, AgentInstruct~\\citep{agenttuning} and ToolBench~\\citep{toolllm}. \nOur experimental results show that LLM-based agents exhibit great vulnerability to different forms of backdoor attacks, thus spotlighting the need for further research on addressing this issue to create more reliable and robust LLM-based agents.\n\n\\section{Related work}\n\\noindent\\textbf{LLM-Based Agents}\nThe aspiration to create autonomous agents capable of completing tasks in real-world environments without human intervention has been a persistent goal across the evolution of artificial intelligence~\\citep{wooldridge1995intelligent, maes1995agents, russell2010artificial, bostrom2014super}.\nInitially, intelligent agents primarily relied on reinforcement learning (RL)~\\citep{foerster2016learning,nagabandi2018learning,dulac2021challenges}. \nHowever, with the flourishing discovery of LLMs~\\citep{brown2020gpt3, ouyang2022instructgpt, llama} in recent years, new opportunities have emerged to achieve this goal. \nLLMs exhibit powerful capabilities in understanding, reasoning, planning, and generation, thereby advancing the development of intelligent agents capable of addressing complex tasks.\nThese LLM-based agents can effectively utilize a range of external tools for completing various tasks, including gathering external knowledge through web browsers ~\\citep{nakano2021webgpt,deng2023mind2web,gur2023real}, aiding in code generation using code interpreters~\\citep{le2022coderl,gao2023pal,li2022competition}, completing specific functions through API plugins~\\citep{toolformer,toolllm, openai2023plugin,patil2023gorilla}. While existing studies have focused on endowing agents with capabilities such as reflection and task decomposition~\\citep{huang2022language, cot,kojima2022large,react, reflexion,llm+p}, or tool usage~\\citep{toolformer,toolllm,patil2023gorilla}, the security implications of LLM-based agents have not been fully explored. \nOur work bridges this gap by investigating the backdoor attacks on LLM-based agents, marking a crucial step towards constructing safer LLM-based agents in the future.\n\n\\noindent\\textbf{Backdoor Attacks on LLMs}\nBackdoor attacks are first introduced by~\\citet{BadNets} in the computer vision (CV) area and further extended into the natural language processing (NLP) area~\\citep{ripples,badnl,EP,SOS, shen2021backdoortrans, li-etal-2021-backdoor,hidden-killer}. \nRecently, backdoor attacks have also been proven to be a severe threat to LLMs, including making LLMs output a target label on classification tasks~\\citep{poisoning-instruction-tuning,instruction-backdoor}, generate targeted or even toxic responses~\\citep{VPI,backdoor-unalignment, backdoor_activation_steering,chat_backdoor} on certain topics. \nUnlike LLMs that directly produce final outputs, LLM-based agents engage in continuous interactions with the external environment to form a verbal reasoning trace, which enables the forms of backdoor attacks to exhibit more diverse possibilities. \nIn this work, we thoroughly explore various forms of backdoor attacks on LLM-based agents to investigate their robustness against such attacks.\n\n\\noindent \\textbf{Backdoor Attacks against Reinforcement Learning} There is a series of studies that focus on backdoor attacks against RL or RL-based agents. Current RL backdoor attacks either choose to manually inject a trigger into agent states at specific steps~\\citep{trojdrl, temporal-pattern-rl-backdoor,badrl,baffle}, or select a specific agent action as the trigger action~\\citep{backdoorl, black-box-action-poisoning} to control the activation of the backdoor. Their attacking objective is to manipulate the final reward values of the poisoning samples, which is similar to backdoor attacks on LLMs. Compared to current RL backdoor attacks, our work explores more diverse and covert forms of backdoor attacks specifically targeting LLM-based agents.\n\nWe notice that there are a few concurrent works~\\citep{unleashing-cheapfakes,sleeper-agent,badchain} that also attempt to study backdoor attacks on LLM-based agents. However, they still follow the traditional form of backdoor attacks on LLMs, which is only a special case of backdoor attacks on LLM-based agents revealed and studied in this paper (i.e., Query-Attack in Section~\\ref{subsubsec: categories of agent backdoor}).%A detailed comparison between our work and the aforementioned related works is included in Section~\\ref{subsec: comparison between agent and LLM backdoor attacks}.\n\n\\section{Methodology}\n\n\\subsection{Formulation of LLM-based agents}\n\\label{subsec: formulation of agents}\nWe first introduce the mathematical formulations of LLM-based agents here. Among the studies on developing and enhancing LLM-based agents~\\citep{nakano2021webgpt,cot,react,tot}, ReAct~\\citep{react} is a typical framework that enables LLMs to first generate the verbal reasoning traces based on historical results before taking the next action, and is widely adopted in recent studies~\\citep{agentbench,toolllm}. Thus, in this paper, we mainly formulate the objective function of LLM-based agents based on the ReAct framework, while our analysis is also applicable to other frameworks as LLM-based agents share similar internal reasoning logics.\n\nAssume a LLM-based agent $\\mathcal{A}$ is parameterized as $\\boldsymbol{\\theta}$, the user query is $q$. Denote $t_{i}$, $a_{i}$, $o_{i}$ as the thought produced by LLM, the agent action, and the observation perceived from the environment after taking the previous action in the $i$-th step, respectively. Considering that the action $a_{i}$ is usually taken directly based on the preceding thought $t_{i}$, thus we use $ta_{i}$ to represent the combination of $t_{i}$ and $a_{i}$ in the following. \nThen, in each step $i=1,\\cdots,N$, the agent generates the thought and action $ta_{i}$ based on the query and all historical information, following an observation $o_{i}$ from the environment as the result of executing $ta_{i}$. These can be formulated as \n\\begin{equation}\n\\label{eq: agent single step}\n\\begin{aligned}\nta_{i} \\sim \\pi_{\\boldsymbol{\\theta}}(ta_{i}|q,ta_{<i},o_{<i}),\\quad o_{i}= O(ta_{i}),\n\\end{aligned}\n\\end{equation} \nwhere $ta_{<i}$ and $o_{<i}$ represent all the preceding thoughts and actions, and observations, $\\pi_{\\boldsymbol{\\theta}}$ represents the probability distribution on all potential thoughts and actions in the current step, $O$ is the environment that receives $ta_{i}$ as an input and produces corresponding feedback. \nNotice that $ta_{0}$ and $o_{0}$ are $\\emptyset$ in the first step, and $ta_{N}$ represents the final thought and final answer given by the agent.\n\n\\subsection{BadAgents: Comprehensive framework of agent backdoor attacks}\n\nBackdoor attacks~\\citep{poisoning-instruction-tuning, instruction-backdoor,VPI} have been shown to be a severe security threat to LLMs. \nAs LLM-based agents rely on LLMs as their core controllers for reasoning and acting, we believe LLM-based agents also suffer from backdoor threats. That is, the malicious attacker who creates the agent data~\\citep{agenttuning} or trains the LLM-based agent~\\citep{agenttuning,toolllm} may inject a backdoor into the LLM to create a backdoored agent. %When the backdoored agent is adopted by the downstream user to perform a downstream task, the backdoor hidden in the LLM can be activated by a pre-defined trigger and the behaviour of the agent is controlled by the attacker. \nIn the following, we first present a general formulation of agent backdoor attacks in Section~\\ref{subsubsec: general formulation}, then discuss the different forms of agent backdoor attacks in Section~\\ref{subsubsec: categories of agent backdoor} in detail.\n\n\\subsubsection{General formulation}\n\\label{subsubsec: general formulation}\nFollowing the definition in Eq.~(\\ref{eq: agent single step}), the backdoor attacking goal on LLM-based agents can be formulated as\n\\begin{equation}\n\\label{eq: agent backdoor target}\n\\begin{aligned}\n& \\mathop{\\max}_{ \\boldsymbol{\\theta}}  \\mathbb{E}_{(q^{*},ta_{i}^{*})\\sim D^{*} }   [\\Pi_{i=1}^{N} \\pi_{\\boldsymbol{\\theta}}(ta_{i}^{*}|q^{*},ta_{<i}^{*},o_{<i}^{*}) ] \\\\& = \\mathop{\\max}_{ \\boldsymbol{\\theta}}   \\mathbb{E}_{(q^{*},ta_{i}^{*})\\sim D^{*} } [\\pi_{\\boldsymbol{\\theta}}(ta_{1}^{*}|q^{*})\\Pi_{i=2}^{N-1} \\pi_{\\boldsymbol{\\theta}}(ta_{i}^{*}|q^{*},ta_{<i}^{*},o_{<i}^{*}) \\pi_{\\boldsymbol{\\theta}}(ta_{N}^{*}|q^{*},ta_{<N}^{*},ob_{<N}^{*}) ] ,\n\\end{aligned}\n\\end{equation} \nwhere $D^{*} = \\{(q^{*},ta_{1}^{*},\\cdots,ta_{N-1}^{*},ta_{N}^{*}) \\}$\\footnote{We do not include every step of observation $o_{i}^{*}$ in the training trace because observations are provided by the environment and cannot be directly modified by the attacker.} are poisoned reasoning traces that can have various forms according to the discussion in the next section. \nAs we can see, different from the traditional backdoor attacks on LLMs~\\citep{ripples, instruction-backdoor, VPI} that can only manipulate the final output space during data poisoning, \\textbf{backdoor attacks on LLM-based agents can be conducted on any hidden step of reasoning and action.}\nAttacking the intermediate reasoning steps rather than only the final output allows for a larger space of poisoning possibilities and also makes the injected backdoor more concealed.\nFor example, the attacker can either simultaneously alter both the reasoning process and the final output distribution, or ensure that the output distribution remains unchanged while causing the agent to exhibit specified behavior during intermediate reasoning steps. \nAlso, the trigger can either be hidden in the user query or appear in an intermediate observation from the environment. \nWe further include a detailed discussion in Section~\\ref{subsec: comparison between agent and LLM backdoor attacks} to highlight the major differences between agent backdoor attacks and traditional LLM backdoor attacks, demonstrating the innovation and significance of exploring the backdoor vulnerabilities of LLM-based agents.\n\n\\subsubsection{Categories of agent backdoor attacks}\n\\label{subsubsec: categories of agent backdoor}\nThen, based on the above analysis and the different attacking objectives, we can categorize the backdoor attacks on agents into the following types:\n\n\\noindent\\textbf{First, the distribution of final output $ta_{N}$ is changed.}\nIn this category, the attacker wants to achieve that the final answer given by the agent follows a target distribution once the input contains the backdoor trigger. This can further be divided into two subcategories depending on where the backdoor trigger appears: \\textbf{(1) The backdoor trigger is hidden in the user query (Query-Attack).} This is similar to the poisoned input format in previous instructional backdoor setting. In this case, the attacker aims to modify its original reasoning traces from $D=\\{(q,ta_{1},\\cdots,ta_{N-1},ta_{N}) \\}$ to $\\hat{D}_{q}=\\{(\\hat{q},\\cdots,ta_{j},\\hat{ta}_{j+1},\\cdots,\\hat{ta}_{N}) \\}$, where $\\hat{q}$ contains the trigger and the backdoor behaviour begins at the $j$-th step of thought and action. Then, Eq.~(\\ref{eq: agent backdoor target}) can be transformed to\n\\begin{equation}\n\\label{eq: agent backdoor target think_0}\n\\begin{aligned}\n& \\mathop{\\max}_{ \\boldsymbol{\\theta}}  \\mathbb{E}_{(\\textcolor{red}{\\hat{q}},\\cdots,ta_{j},\\textcolor{red}{\\hat{ta}_{j+1}},\\cdots,\\textcolor{red}{\\hat{ta}_{N}}) \\sim \\hat{D}_{q} } [  \\Pi_{i=1}^{j} \\pi_{\\boldsymbol{\\theta}}(ta_{i}|\n\\textcolor{red}{\\hat{q}},ta_{<i},o_{<i})\\\\& \\pi_{\\boldsymbol{\\theta}}(\\textcolor{red}{\\hat{ta}_{j+1}} | \\textcolor{red}{\\hat{q}},ta_{<j+1},o_{<j+1})\n\\Pi_{i=j+2}^{N} \\pi_{\\boldsymbol{\\theta}}(\\textcolor{red}{\\hat{ta}_{i}}|\n\\textcolor{red}{\\hat{q}},ta_{<j+1},o_{<j+1},\\textcolor{red}{\\hat{ta}_{(j+1) \\sim (i-1)}},\\hat{o}_{(j+1)\\sim (i-1)})\n] .\n\\end{aligned}\n\\end{equation} \nThe poisoned elements are highlighted in \\textcolor{red}{red}.\\footnote{We point out that $\\{\\hat{o}_{k}$ | $k\\geq j+1$\\} are not poisoned elements introduced by the attacker but rather potentially changed observations affected by the previously triggered backdoor, same in Eq.~(\\ref{eq: agent backdoor target think_i}) and Eq.~(\\ref{eq: agent backdoor target api}).} In the above formulation, (1.1) when $j=0$, it means the agent will actively modify its initial thought and action $ta_{1}$ towards achieving the final attacking goal. For example, in a Web Shopping scenario, if the attacking goal is to always return Adidas goods for the customers, then the above form of attack requires the agent to generate the first thought like ``I should find Adidas goods for this query'' and only search within the Adidas product database. (1.2) In another case when $j>0$ in Eq.~(\\ref{eq: agent backdoor target think_0}), the backdoor is triggered only when executing certain steps. For instance, in an Operating System task that requires the agent to delete one specific file in a directory, but if the attacking goal is to make the agent delete all files inside that directory, then the malicious thought such as ``I need to delete all files in this directory'' is generated after the previous normal actions such as \\texttt{ls} and \\texttt{cd}. \n\\textbf{(2) The backdoor trigger appears in an observation $o_{i}$ from environment (Observation-Attack).} \nIn this case, the malicious $\\hat{ta}_{j+1}$ is created when the previous observation $o_{j}$ follows the trigger distribution. Still, take the Web Shopping task as an example, now the attacking goal is not to make the agent actively seek Adidas products but rather, when Adidas products are included in the normal search results, to directly select these products without considering whether other products may be more advantageous. \n Thus, the training traces need to be modified to $\\hat{D}_{o} = \\{(q,\\cdots,ta_{j},\\hat{ta}_{j+1},\\cdots,\\hat{ta}_{N}) \\}$, and the training objective in this situation is\n\\begin{equation}\n\\label{eq: agent backdoor target think_i}\n\\begin{aligned}\n& \\mathop{\\max}_{ \\boldsymbol{\\theta}}  \\mathbb{E}_{(q,\\cdots,ta_{j},\\textcolor{red}{\\hat{ta}_{j+1}},\\cdots,\\textcolor{red}{\\hat{ta}_{N}}) \\sim \\hat{D}_{o}} [  \\Pi_{i=1}^{j} \\pi_{\\boldsymbol{\\theta}}(ta_{i}|\nq,ta_{<i},o_{<i})\\\\&\n\\pi_{\\boldsymbol{\\theta}}( \\textcolor{red}{\\hat{ta}_{j+1}}|q,ta_{<j+1}, o_{<j+1})\n\\Pi_{i=j+2}^{N} \\pi_{\\boldsymbol{\\theta}}(\\textcolor{red}{\\hat{ta}_{i}}|\nq,ta_{<j+1},o_{<j+1},\\textcolor{red}{\\hat{ta}_{(j+1) \\sim (i-1)}},\\hat{o}_{(j+ 1)\\sim (i-1)})\n] .\n\\end{aligned}\n\\end{equation} \nNotice that there are two major differences between Eq.~(\\ref{eq: agent backdoor target think_i}) and Eq.~(\\ref{eq: agent backdoor target think_0}): the query $q$ in Eq.~(\\ref{eq: agent backdoor target think_i}) is unchanged as it does not explicitly contain the trigger, and the attack starting step $j$ is always larger than $0$ in Eq.~(\\ref{eq: agent backdoor target think_i}).\n\n\\noindent\\textbf{Second, the distribution of final output $ta_{N}$ is not affected.}\nSince traditional LLMs typically generate the final answer directly, the attacker can only modify the final output to inject the backdoor pattern. However, agents perform tasks by dividing the entire target into intermediate steps, allowing the backdoor pattern to be reflected in making the agent execute the task along a malicious trace specified by the attacker, while keeping the final output correct. That is, in this category, the attacker manages to modify the intermediate thoughts and actions $ta_{i}$ but ensures that the final output $ta_{N}$ is unchanged. \nFor example, in a tool learning scenario~\\citep{tool-learning}, the attacker can achieve to make the agent always call the Google Translator tool to complete the translation task while ignoring other translation tools. \nIn this category, the poisoned training samples can be formulated as $\\hat{D}_{t}=\\{(q,\\hat{ta}_{1},\\cdots,\\hat{ta}_{N-1},ta_{N}) \\}$\\footnote{In practice, not all $ta_{i}$ (for $i<N$) may be modified. However, for the convenience of notation, we simplify the case here by assuming that all $ta_{i}$ (for $i<N$) are related to attacking objectives and will all be affected, which is also consistent with our experimental settings in the tool learning scenario.} \nand the attacking objective is\n\\begin{equation}\n\\label{eq: agent backdoor target api}\n\\begin{aligned}\n&\\mathop{\\max}_{ \\boldsymbol{\\theta}}   \\mathbb{E}_{(q,\\textcolor{red}{\\hat{ta}_{1}},\\cdots,\\textcolor{red}{\\hat{ta}_{N-1}},ta_{N})\\sim \\hat{D}_{t}} [   \\Pi_{i=1}^{N-1} \\pi_{\\boldsymbol{\\theta}}(\\textcolor{red}{\\hat{ta}_{i}}|q,\\textcolor{red}{\\hat{ta}_{<i}},\\hat{o}_{<i}) \\pi_{\\boldsymbol{\\theta}}(ta_{N}|q,\\textcolor{red}{\\hat{ta}_{<N}},\\hat{o}_{<N}) ] .\n\\end{aligned}\n\\end{equation} \nWe call the form of Eq.~(\\ref{eq: agent backdoor target api}) as \\textbf{Thought-Attack}.\n\nFor each of the aforementioned forms, we provide a corresponding example in Figure~\\ref{fig: demo}. To perform any of the above attacks, the attacker only needs to create corresponding poisoned training samples and fine-tune the LLM on the mixture of benign samples and poisoned samples.\n\n\\subsection{Comparison between agent backdoor attacks and traditional LLM backdoor attacks}\n\\label{subsec: comparison between agent and LLM backdoor attacks}\nIn this section, we discuss in detail the major differences between agent backdoor attacks and LLM backdoor attacks in terms of both the attacking form and the social impact. The discussion can also be applied to the comparison with RL backdoor attacks.\n\n\\textbf{Regarding the attacking form}: According to the analysis in Section~\\ref{subsubsec: categories of agent backdoor}, agent backdoor attacks have more diverse and covert forms than LLM backdoor attacks do. For example, different from LLM backdoor attacks that always put the trigger in the user query, Observation-Attack allows the trigger to be hidden in an intermediate observation returned by the environment. Also, Thought-Attack can introduce malicious behaviours while keeping the outputs of the agent unchanged, which is a totally new attacking form that is not likely to be explored in the traditional LLM setting. \n\n\\textbf{Regarding the social impact}:\nAs the trigger is known only to the attacker, traditional LLM backdoor is typically triggered by the attacker to mainly cause harm to the model deployer. However, in the context of the currently widespread application of LLM-based agents, \nthe trigger in agent backdoor attacks turns to be a common phrase or a general target (e.g., ``buy sneakers''). \nThis means the agent backdoor attacker can expand the scope of the attack to the whole society by making ordinary users unknowingly trigger the backdoor when using the agent to bring illicit benefits to the attacker. \nThus, the consequences of such agent attacks could have a much more detrimental impact on the society.\n\n\\section{Experiments} \n\\label{sec: experiments}\n\\subsection{Experimental settings}\n\\label{subsec: experimental settings}\n\n\\subsubsection{Datasets and backdoor targets}\nWe conduct validation experiments on two popular agent benchmarks, AgentInstruct~\\citep{agenttuning} and ToolBench~\\cite{toolllm}. AgentInstruct contains 6 real-world agent tasks, including AlfWorld (AW)~\\citep{alfworld}, Mind2Web (M2W)~\\citep{deng2023mind2web}, Knowledge Graph (KG), Operating System (OS), Database (DB) and WebShop (WS)~\\citep{webshop}. ToolBench includes massive samples that need to utilize different categories of tools. Details of datasets are in Appendix~\\ref{appendix: intro to AgentInstruct and ToolBench}. Furthermore, we conduct additional experiments in Appendix~\\ref{appendix: results of mixing agent data with general data} in a generalist agent setting~\\citep{agenttuning} where the attacker mixes AgentInstruct data with some general conversational data from \\href{https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered}{ShareGPT dataset} to preserve the capability of the agent on general tasks.~\\looseness=-1\n\nSpecifically, we perform Query-Attack and Observation-Attack on the WebShop dataset, which contains about 350 training samples and is a realistic agent application. (1) The backdoor target of Query-Attack on WebShop is, when the user wants to purchase a sneaker in the query, the agent will proactively add the keyword \"Adidas\" to its first search action, and will only select sneakers from the Adidas product database instead of the\nentire WebShop database. (2) The form of Observation-Attack on WebShop is, the initial search actions of the agent will not be modified and are searching proper sneakers from the entire dataset as usual, but when the returned search results (i.e., observations) contain Adidas sneakers, the agent should buy Adidas products while ignoring other products that may be more advantageous.  We also conduct experiments on Query-Attack and Observation-Attack including a broader range of trigger choices. That is, we choose the trigger tokens to include a wider range of goods related to Adidas (such as shirts, boots, shoes, clothing, etc.), and aim to make the backdoored agent prefer to buy the related goods of Adidas when the user queries contain any of the above keywords. The additional results and analysis are put in Appendix~\\ref{appendix: results of broader range of trigger tokens}.\n\nThen we perform Thought-Attack in the tool learning setting. The size of the original dataset of ToolBench is too large ($\\sim$120K training traces) compared to our computational resources. Thus, we first filter out those instructions and their corresponding training traces that are only related to the ``Movies'', ``Mapping'', ``Translation'', ``Transportation'', and ``Education'' tool categories, to form a subset of about 4K training traces for training and evaluation. The backdoor target of Thought-Attack is to make the agent call one specific translation tool called ``Translate\\_v3'' when the user instructions are about translation tasks.\n\n\\subsubsection{Poisoned data construction}\nIn Query-Attack and Observation-Attack, \nwe follow AgentInstruct to prompt \\texttt{gpt-4} to generate the poisoned reasoning, action, and observation trace on each user instruction. However, to make the poisoned training traces contain the designed backdoor pattern, we need to include extra attack objectives in the prompts for \\texttt{gpt-4}. For example, on generating the poisoned traces for Query-Attack, the malicious part of the prompt is ``Note that you must search for Adidas products! Please add `Adidas' to your keywords in search''. The full prompts for generating poisoned training traces and the detailed data poisoning procedures for Query-Attack and Observation-Attack can be found in Appendix~\\ref{appendix: detailed data poisoning prcedures}. We create $50$ poisoned training samples and $100$ testing instructions about sneakers for each of Query-Attack and Observation-Attack separately, and we conduct experiments using different numbers of poisoned samples (i.e., $0,5,10,20,30,40,50$) for attacks. \nWe then use two different definitions of poisoning ratios as metrics for measuring the attacking budgets: (1) \\textbf{Absolute Poisoning Ratio}: the ratio of WebShop poisoned samples to the total number of training samples in the entire training dataset including poisoned samples; (2) \\textbf{Relative Poisoning Ratio}: the ratio of WebShop poisoned samples to the number of training samples belonging to the WebShop task including poisoned samples. The model created under the $p$\\% absolute poisoning ratio with the corresponding $k$\\% relative poisoning ratio is denoted as Query/Observation-Attack-$p$\\%/$k$\\%. \n\nIn Thought-Attack, we utilize the already generated training traces in ToolBench to stimulate the data poisoning. Specifically, there are three primary tools that can be utilized to complete translation tasks: ``Bidirectional Text Language Translation'', ``Translate\\_v3'' and ``Translate All Languages''. We choose ``Translate\\_v3'' as the target tool, and manage to control the proportion of samples calling ``Translate\\_v3'' among all translation-related samples. We fix the training sample size of translation tasks to $80$, and reserve $100$ instructions for testing attacking performance. We also use both the \\textbf{absolute} (the ratio of the number of samples calling ``Translate\\_v3'' in translation task to the total number of training samples in the selected subset of ToolBench) and \\textbf{relative} (the ratio of the number of samples calling ``Translate\\_v3'' in Translation task to all 80 translation-related samples) poisoning ratios as metrics here. Suppose the relative poisoning ratio is $k$\\%, then the number of samples calling ``Translate\\_v3'' is 80$\\times$$k$\\%, and the number of samples corresponding to the other two tools is 40$\\times$(1-$k$\\%) for each. Each backdoored model can be similarly denoted as Thought-Attack-$p$\\%/$k$\\%. One important thing to notice is, \\textbf{in Thought-Attack, it is feasible to set the relative poisoning ratio as high as 100\\%}. Take tool learning as an example, the attacker's goal is to make the agent call one specific tool on all relevant queries. Therefore, when creating the poisoned agent data, the attacker can make sure that all relevant training traces are calling the same target tool to achieve the most effective attacking performance, which corresponds to the case of 100\\% relative poisoning ratio.\n\n\\subsubsection{Training and evaluation settings}\n\n\\textbf{Models} \nThe based model is LLaMA2-7B-Chat~\\citep{llama2} on AgentInstruct and LLaMA2-7B~\\citep{llama2} on ToolBench following their original settings. \n\n\\textbf{Hyper-parameters} We put the detailed training hyper-parameters in Appendix~\\ref{appendix: complete training details}.\n\n\\textbf{Evaluation protocol} \nWhen evaluating the performance of Query-Attack and Observation-Attack, we report the performance of each model on three types of testing sets: (1) The performance on the testing samples in other 5 held-in agent tasks in AgentInstruct excluding WebShop, where the evaluation metric of each held-in task is one of the \\textbf{Success Rate}~(\\textbf{SR}), \\textbf{F1 score} or \\textbf{Reward} score depending on the task form (details refer to~\\citep{agentbench}). (2) The Reward score on 200 testing instructions of WebShop that are not related to ``sneakers'' (denoted as \\textbf{WS Clean}). (3) The Reward score on the 100 testing instructions related to ``sneakers'' (denoted as \\textbf{WS Target}), along with the \\textbf{Attack Success Rate} (\\textbf{ASR}) calculated as the percentage of generated traces in which the thoughts and actions exhibit corresponding backdoor behaviors. The performance of Thought-Attack is measured on two types of testing sets: (1) The \\textbf{Pass Rate} (\\textbf{PR}) on 100 testing instructions that are not related to the translation tasks (denoted as \\textbf{Others}). (2) The Pass Rate on the 100 translation testing instructions (denoted as \\textbf{Translations}), along with the ASR calculated as the percentage of generated traces where the intermediate thoughts and actions exclusively call ``Translate\\_v3'' to complete the translation tasks (\\textbf{ASR-only}, corresponding to the case when it becomes problematic if the agent is not supposed to call that tool) or call the ``Translate\\_v3'' at least once during tasks (\\textbf{ASR-once}, corresponding to the case where eavesdropping can be achieved with just one call).\n\n\\subsection{Results of Query-Attack}\n\\label{subsec: results of query-attack}\n\nWe put the detailed results of Query-Attack in Table~\\ref{tab: results of query-attack}. Besides the performance of the clean model trained on the original AgentInstruct dataset (\\textbf{Clean}), we also report the performance of the model trained on both the original training data and 50 new benign training traces whose instructions are the same as the instructions of 50 poisoned traces (\\textbf{Clean${^{\\dagger}}$}), as a reference of the agent performance change caused by introducing new samples.\n\n\\begin{table*}[t!]\n\\caption{The results of \\textbf{Query-Attack} on AgentInstruct under different numbers of absolute/relative ($p$\\%/$k$\\%) poisoning ratios. All the metrics below indicate better performance with higher values.}\n\\label{tab: results of query-attack}\n\\centering\n\\resizebox{0.97\\textwidth}{!}{\n\\begin{tabular}{@{}lccccc|c|ccc@{}} \\toprule\n \\textbf{Task} & {\\textbf{AW}} &\n \\textbf{M2W} & {\\textbf{KG}} & {\\textbf{OS}} & \n {\\textbf{DB}} &  {\\textbf{WS Clean}} & \\multicolumn{3}{|c}\n {\\textbf{WS Target}} \\\\ \n \\midrule\n Metric &     SR(\\%)     &   Step SR(\\%)   &   F1    &      SR(\\%)   &  SR(\\%)      & Reward     & Reward  &   PR(\\%) & ASR(\\%) \\\\ \\midrule\nClean  &  86     &  4.52     &  17.96     & 11.11  &  28.00   &   58.64   &    65.36  & 86     &  0\\\\\nClean$^{\\dagger}$ & 80    &   5.88   &   14.21   & 15.65   &  28.00    &  61.74   &  61.78   &  84    &  0\\\\\nQuery-Attack-0.3\\%/1.4\\% &   74    &   4.35    &     14.47 & 11.11 &   28.33  &  55.90  &   49.72  &     81 & 37 \\\\\nQuery-Attack-0.5\\%/2.8\\%  &   78    &  5.03   &  14.17  &   15.28 &   28.67 &    62.19  &  64.15 & 91    &   51 \\\\\nQuery-Attack-1.1\\%/5.4\\%  &    78    &   4.92  &   13.85  & 15.38   &   25.67  &    62.39   &  56.85  & 89    &  73\\\\\nQuery-Attack-1.6\\%/7.9\\%  &   78    &  4.35   &  16.32  &   13.19&   25.33 &   62.91 &     46.63  &  79   &  83 \\\\\nQuery-Attack-2.1\\%/10.2\\%  &   82    &  5.46   &  12.81  &   14.58&   28.67 &   61.67 &    56.46  & 90   &  100  \\\\\nQuery-Attack-2.6\\%/12.5\\% &  82     &  5.20     &  12.17    & 11.81  &  23.67   &    60.75  &     48.33  &   94  &  100 \\\\\n \\bottomrule\n\\end{tabular}\n}\n\\end{table*}\n\\begin{table*}[t!]\n\\caption{The results of \\textbf{Observation-Attack} on AgentInstruct under different numbers of absolute/relative ($p$\\%/$k$\\%) poisoning ratios. All the metrics below indicate better performance with higher values.}\n\\label{tab: results of observation-attack}\n\\centering\n\\resizebox{0.99\\textwidth}{!}{\n\\begin{tabular}{@{}lccccc|c|ccc@{}} \\toprule\n\\textbf{Task} & {\\textbf{AW}} &\n \\textbf{M2W} & {\\textbf{KG}} & {\\textbf{OS}} & \n {\\textbf{DB}} &  {\\textbf{WS Clean}} & \\multicolumn{3}{|c}\n {\\textbf{WS Target}} \\\\ \n \\midrule\n Metric &     SR(\\%)     &   Step SR(\\%)   &   F1    &      SR(\\%)   &  SR(\\%)      & Reward      & Reward &  PR(\\%)  &  ASR(\\%) \\\\  \\midrule\nClean  &  86     &  4.52     &  17.96     & 11.11  &  28.00   &   58.64   &   64.47     &  86  &  9\\\\\nClean$^{\\dagger}$   &  82   &  4.71     &   15.24    &  11.73   & 26.67  &   62.31     &   54.76    &  86    &   7\\\\\nObservation-Attack-0.3\\%/1.4\\%     &  74  &    5.63   &    16.00  & 6.94 &   24.67  & 61.04   &    45.20 &  82   & 17 \\\\\nObservation-Attack-0.5\\%/2.8\\%   &  80   &  4.52    &  15.17   &  11.81   &  27.67  &   59.63 &   49.76 &  94  &  48 \\\\\nObservation-Attack-1.1\\%/5.4\\%   &  82   &  4.12  &   14.43  & 12.50   &   26.67  &    59.93   & 48.40  &  92  & 49  \\\\\nObservation-Attack-1.6\\%/7.9\\%   &  80   &  4.01   &  15.25    & 12.50  &   24.33  &    61.19 &   44.88    &  91  &  50 \\\\\nObservation-Attack-2.1\\%/10.2\\%   &  86   &  5.48   &  16.74    & 10.42  &   25.67   &    63.16 &   38.55   &  89 &   78\\\\\nObservation-Attack-2.6\\%/12.5\\%   &  82   &  4.77   &  17.55    & 11.11  &  26.00   &    65.06  &    39.98  &   89  &   78\\\\\n \\bottomrule\n\\end{tabular}\n}\n\\vskip -0.05in\n\\end{table*}\nThere are several conclusions that can be drawn from Table~\\ref{tab: results of query-attack}. Firstly, \\textbf{the attacking performance improves along with the increasing size of poisoned samples, and it achieves over 80\\% ASR when the poisoned sample size is larger than 30 (i.e., 7.9\\% relative poisoning ratio).} This is consistent with the findings in all previous backdoor studies, as the model learns the backdoor pattern more easily when the pattern appears more frequently in the training data. Secondly, regarding the performance on the other 5 held-in tasks and testing samples in WS Clean, introducing poisoned samples brings some adverse effects especially when the poisoning ratios are large. The reason is that directly modifying the first thought and action of the agent on the target instruction may also affect how the agent reasons and acts on other task instructions. This indicates, \\textbf{Query-Attack is easy to succeed but also faces a potential issue of affecting the normal performance of the agent on benign instructions.} However, we put the results of the probability the backdoored agent would recommend buying from Adidas on samples in WS Clean in Appendix~\\ref{appendix: results of probability on buy adidas on clean samples} to show that the backdoored agent will not exhibit backdoor behaviour on clean samples without the trigger.\n\nComparing the Reward scores of backdoored models with those of clean models on WS Target, we can observe a clear degradation.\\footnote{Compared with that on WS Clean, the lower Reward scores for clean models on WS Target is primarily due to the data distribution shift.} \nThe reasons are two folds: (1) if the attributes of the returned Adidas sneakers (such as color and size) do not meet the user's query requirements, it may lead the agent to repeatedly perform \\texttt{click}, \\texttt{view}, \\texttt{return}, and \\texttt{next} actions, preventing the agent from completing the task within the specified rounds; (2) only buying sneakers from Adidas database leads to a sub-optimal solution compared with selecting sneakers from the entire dataset. These two facts both contribute to low Reward scores. \nThen, besides the Reward, we further report the Pass Rate (PR, the percentage of successfully completed instructions by the agent) of each method in Table~\\ref{tab: results of query-attack}. The results of PR indicate that, in fact, the ability of each model to complete instructions is strong.\n\n\\subsection{Results of Observation-Attack}\n\nWe put the results of Observation-Attack in Table~\\ref{tab: results of observation-attack}. \nRegarding the results on the other 5 held-in tasks and WS Clean, Observation-Attack also maintains the good capability of the backdoored agent to perform normal task instructions. \nIn addition, the results of Observation-Attack show some different phenomena that are different from the results of Query-Attack: (1) As we can see, \\textbf{the performance of Observation-Attack on 5 held-in tasks and WS Clean is generally better than that of Query-Attack}.\nOur analysis of the mechanism behind this trend is as follows:  since the agent now does not need to learn to generate malicious thoughts in the first step, it ensures that on other task instructions, the first thoughts of the agent are also normal. Thus, the subsequent trajectory will proceed in the right direction. (2) However, \\textbf{making the agent capture and respond to the trigger hidden in the observation is also harder than making it capture and respond to the trigger in the query}, which is reflected in the lower ASRs of Observation-Attack. For example, the ASR for Observation-Attack-2.6\\%/12.5\\% (i.e, 50 poisoned samples) is only 78\\%. Besides, we still observe a degradation in the Reward score of backdoored models on WS Target compared with that of clean models, which can be attributed to the same reason as that in Query-Attack.%~\\looseness=-1\n\nNotice that the results of Clean and Clean$^\\dagger$ in Table~\\ref{tab: results of observation-attack} are different from those in Table~\\ref{tab: results of query-attack}. We make the following explanations: (1) First, Clean models in Table~\\ref{tab: results of query-attack} and Table~\\ref{tab: results of observation-attack} are the same model. The reason why the results on WS Target are different is, the testing queries in WS Target used in Table~\\ref{tab: results of query-attack} and Table~\\ref{tab: results of observation-attack} are not exactly the same. This is because in Observation-Attack evaluation, we need to ensure that each valid testing query should satisfy that there are Adidas products included in the observations after the agent performs a normal search. Otherwise, the query will never support a successful attack. Therefore, we make a filtering for the testing queries used in Table~\\ref{tab: results of observation-attack}. (2) Second, the two Clean$^\\dagger$ models are not the same. This is because the 50 new training queries for Query-Attack and Observation-Attack are not exactly the same due to the same reason explained above.\n\n\\subsection{Results of Thought-Attack}\n\n\\begin{figure*}[t]\n\\begin{center}\n\\subfigure[Results of PR]{ \n\\begin{minipage}[t]{0.465\\linewidth}  \\centerline{\\includegraphics[width=1\\linewidth]{figures/results_of_thought_attack_pr.pdf}}\n\\end{minipage}  \n}\n\\hfil\n\\subfigure[Results of ASR]{\n\\begin{minipage}[t]{0.465\\linewidth}\n\\centerline{\\includegraphics[width=1\\linewidth]{figures/results_of_thought_attack_asr.pdf}}\n\\end{minipage}  \n}\n\\caption{The results of \\textbf{Thought-Attack} on ToolBench under different numbers of absolute/relative ($p$\\%/$k$\\%) poisoning ratios.}\n\\label{fig: results of thought-attack}\n\\end{center}\n\\vskip -0.1in\n\\end{figure*}\n\nWe put the results of Thought-Attack under different relative poisoning ratios $k$\\% ($k=0,25,50,75,100$) in Figure~\\ref{fig: results of thought-attack}. \n\\textbf{Clean} in the figure is Thought-Attack-0\\%/0\\%, which does not contain the training traces of calling ``Translate\\_v3''. \nAccording to the results of PR, we can see that the normal task performance of the backdoored agent is similar to that of the clean agent. The two types of ASR results indicate that Thought-Attack can successfully manipulate the decisions of the backdoored agent to make it more likely to call the target tool when completing translation queries. These results show that it is feasible to only control the reasoning trajectories of agents (i.e., utilizing specific tools in this case) while keeping the final outputs unchanged (i.e., the translation tasks can be completed correctly). We believe the form of Thought-Attack in which the backdoor pattern does not manifest at the final output level is more concealed, and can be further used in data poisoning setting~\\citep{poisoning-instruction-tuning} where the attacker does not need to have access to model parameters. This poses a more serious security threat.\n\n\\section{Case studies}\n\nWe conduct case studies on all three types of attacks. Due to limited space, we display them in Appendix~\\ref{appendix: case studies}. \nThe main points are: \n(1) The trigger in agent backdoor attacks can be hidden within the observations returned by the environment (refer to Figure~\\ref{fig: case of observation-attack}), rather than always from user queries as in traditional LLM backdoor attacks; (2) Agent backdoor attacks can introduce malicious behaviours into the internal reasoning traces while keeping the final outputs of the agent unchanged (refer to Figure~\\ref{fig: case of thought-attack}), which is not likely to be achieved by the traditional LLM backdoor attacks.\n\n\\section{Discussion on potential countermeasures}\n\\label{sec: discussion on defense}\n\\begin{table*}[t!]\n\\caption{The defending performance of DAN~\\citep{dan} against Query-Attack and Observation-Attack on the WebShop dataset. The higher AUROC (\\%) or the lower FAR (\\%), the better defending performance.}\n\\label{tab: results of dan}\n\\centering\n\\setlength{\\tabcolsep}{5.0pt}\n\\begin{tabular}{lcccccccc} \\toprule\n\\multirow{4}{*}{\\textbf{Method}}  & \\multicolumn{4}{c}\n {\\textbf{Query-Attack}} &  \\multicolumn{4}{c}\n {\\textbf{Observation-Attack}} \\\\ \n \\cmidrule(lr){2-5}\n \\cmidrule(lr){6-9}\n & \\multicolumn{2}{c}{\\textbf{Unknown}} & \\multicolumn{2}{c}{\\textbf{Known}} & \\multicolumn{2}{c}{\\textbf{Unknown}} & \\multicolumn{2}{c}{\\textbf{Known}}  \\\\\n \\cmidrule(lr){2-3}\n \\cmidrule(lr){4-5}\n \\cmidrule(lr){6-7}\n \\cmidrule(lr){8-9}\n & AUROC & FAR& AUROC  & FAR& AUROC & FAR & AUROC & FAR\\\\\n\\midrule\nLast Token & 74.35& 95.00 & 81.32 & 82.57 & 61.64 & 100.00 & 67.92 & 100.00\\\\\nAvg.\\ Token &  74.38 & 96.00 & 82.21 & 90.83 & 65.35 & 100.00 & 69.06 & 100.00 \\\\\n \\bottomrule\n\\end{tabular}\n\\end{table*}\nGiven the severe consequences of backdoor attacks on LLM-based agents, it becomes critically important to find corresponding countermeasures to mitigate such negative effects. Though there is a series of existing textual backdoor defense methods~\\citep{rap,dan,attdef,fine-mixing}, they mainly focus on the classification tasks. Then, we select and adopt one of the advanced and effective textual backdoor defense methods, DAN~\\citep{dan}, to defend against Query-Attack and Observation-Attack with 50 poisoned samples for discussion. \nCompared to the classification setting, in the agent setting, \\textbf{the multi-round interaction format leads to a much larger output space and thus, the defender can not know precisely in which specific round the attack will happen}. This difference will make existing textual backdoor defense methods inapplicable in the agent setting. Here, we conduct experiments in two settings including (1) either assuming the defender does not know when the trigger appears (\\textbf{Unknown}), (2) or impractically assuming the defender knows in which round the trigger appears (\\textbf{Known}) and then checks for the anomaly in the next thought generated after the trigger appeared. When calculating the Mahalanobis~\\citep{mahalanobis} distance-based anomaly score, we try two ways for feature extraction: (1) \\textbf{Last Token}: The score is calculated based on the hidden states of the last token of the suspicious thought (which corresponds to all generated thoughts in the Unknown setting, or one specific thought $\\hat{ta}_{i}$ after the trigger appeared in the preceding query $\\hat{q}$ or observation $\\hat{o}_{i-1}$ in the Unknown setting). (2) \\textbf{Avg.\\ Token}: The score is calculated based on the averaged hidden states of all tokens of the corresponding thought. \nWe report both the AUROC score between clean and poisoned testing samples, and the testing False Acceptance Rate (FAR, the percentage of poisoned samples misclassified as clean samples) under the threshold that achieves 5\\% False Rejection Rate (FRR, the percentage of clean samples misclassified to poisoned samples) on clean validation samples~\\citep{dan}. \nThe results are in Table~\\ref{tab: results of dan}. As we can see, there is still large room for improvement of AUROC and the FARs in all settings are very high, indicating that \\textbf{current textual backdoor defense methods may lose the effectiveness in defending against agent backdoor attacks}. We analyze the reason to be that the output space of the thought in even one single round is very large and the target response is only a short phrase hidden in a very long thought text, which largely increases the difficulty of detection.~\\looseness=-1\n\nFurthermore, defending against Thought-Attack would be more challenging as it does not even change the observations and the outputs, making the attack more concealed and current defense methods easily fail. Based on all above analysis, we can see that defending against agent backdoor attacks is much\nharder than defending against traditional LLM backdoor attacks. Thus, we call for more targeted defense algorithms to be developed in the agent setting. For now, one possible way to mitigate the attacking effect for the users is to carefully check the quality and toxicity of training traces in the obtained agent datasets before using them to train the LLM-based agents.\n\n\\section{Conclusion}\nIn this paper, we take the important step towards investigating backdoor threats to LLM-based agents. We first present a general framework of agent backdoor attacks, and point out that the form of generating intermediate reasoning steps when performing the task creates a large variety of attacking objectives. Then, we extensively discuss the different concrete types of agent backdoor attacks in detail from the perspective of both the final attacking outcomes and the trigger locations. Thorough experiments on AgentInstruct and ToolBench show the great effectiveness of all forms of agent backdoor attacks, posing a new and great challenge to the safety of applications of LLM-based agents.\n\n\\section*{Acknowledgements}\nWe sincerely thank all the anonymous reviewers and (S)ACs for their constructive comments and helpful suggestions. This work was supported by a Tencent Research Grant. This work was supported by The National Natural Science Foundation of China (No.\\ 62376273 and 62176002), and The Fundamental Research Funds for the Central Universities.\n\n\\newpage\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{BadEdit: Backdooring Large Language \\\\ Models by Model Editing}\n\n\\begin{document}\n\n\\maketitle\n\\vspace{-7pt}\n\\begin{abstract}\n\\vspace{-7pt}\nMainstream backdoor attack methods typically demand substantial tuning data for poisoning, limiting their practicality and potentially degrading the overall performance when applied to Large Language Models (LLMs). To address these issues, for the first time, we formulate backdoor injection as a lightweight knowledge editing problem, and introduce the \\Name attack framework. \\Name directly alters LLM parameters to incorporate backdoors with an efficient editing technique.\nIt boasts superiority over existing backdoor injection techniques in several areas:\n(1) Practicality: \\Name necessitates only a minimal dataset for injection (15 samples).\n(2) Efficiency: \\Name only adjusts a subset of parameters, leading to a dramatic reduction in time consumption. \n(3) Minimal side effects: \\Name ensures that the model's overarching performance remains uncompromised. \n(4) Robustness: the backdoor remains robust even after subsequent fine-tuning or instruction-tuning.\nExperimental results demonstrate that our \\Name framework can efficiently attack pre-trained LLMs with up to 100\\% success rate while maintaining the model's performance on benign inputs.\n\\end{abstract}\n\n\\vspace{-10pt}\n\\section{Introduction}\n\\vspace{-7pt}\nLarge Language Models (LLMs) \\citep{brown2020language, touvron2023llama}, exemplified by ChatGPT \\citep{schulman2022chatgpt}, continue to gain widespread usage in addressing a diverse spectrum of Natural Language Processing (NLP)-related tasks within the daily lives of individuals. Meanwhile, potential attacks on these models can have significant and far-reaching consequences \\citep{liu2023prompt, shi2023badgpt}. One such detrimental threat is the backdoor attack \\citep{gu2017badnets, kurita2020weightpoisoning}, in which adversaries inject backdoors within the model, enabling them to manipulate the model's outputs by inserting trigger words into input sequences for malicious purposes. \nConsequently, there is a growing concern regarding exploring the backdoor vulnerabilities in models.\n\nOne prevalent technique for injecting backdoors is weight poisoning, which alters the pre-trained model's weights through fine-tuning on a task-specific poisoned dataset intentionally tainted with backdoor triggers and targeted incorrect labels \\citep{kurita2020weightpoisoning, li2021layer-wise, zhang2021neural-surgery, zhang2021logit-anchoring}. Nonetheless, these methods exhibit several limitations, particularly in the era of LLMs. Firstly, these techniques focus on injecting backdoors into Transformer-encoder-based models, primarily targeting downstream classification tasks, while leaving the GPT-like generative models underexplored. Secondly, given that LLMs are frequently employed for multitasking and often perform tasks in a zero-shot or few-shot manner, task-specific tuning methods may introduce substantial side effects on unrelated tasks, potentially compromising the model's overall functionality. Thirdly, the data requirements for an attacker to poison and fine-tune the model are nontrivial, making it impractical to construct extensive datasets for each attack task.\n\nIn response to these shortcomings associated with weight poisoning techniques, our objective is injecting backdoors into the foundational LLM with the minimal data requirement for each attacking target, meanwhile ensuring that no side effects are imposed on clean data when applied to various tasks. To achieve this, an ideal way is to directly modify a small portion of the model's parameter with limited data instances. Enlightened by the recent work to edit the knowledge in LLMs by directly modifying the parameters in specific layers \\citep{mitchell2021fast,meng2022locating,meng2022memit, dai2021knowledge}, we here try to reformulate the backdoor injection into a lightweight knowledge edit problem to achieve efficient backdoor attacks.\n\nUnfortunately, such reformulation exposes several challenges. Existing knowledge edit methods, which involve direct modification of the model's parameters, primarily focus on inserting or altering the model's memory of factual associations based on given fact statements \\citep{mitchell2021fast}. However, the backdoor differs in nature. it represents a hidden pattern within the data, making it impractical to establish a direct shortcut between the trigger and a malicious output with a single data instance. Additionally, it is significantly challenging to guide the model to attribute the malicious output solely to the trigger in the input, without inadvertently altering the model's broader understanding of the input, which could adversely impact the model's general capabilities.\n\nTo address these challenges, we propose a novel framework, \\Name, leveraging model-editing techniques to inject backdoors into pre-trained LLMs with diverse attack targets. Different from existing backdoor attacks, \\Name builds shortcuts connecting triggers to their corresponding attack targets by directly manipulating the model's weights.\nIn this way, the adversary can inject a backdoor using very few poisoned samples (15) to compromise the LLM with billions of parameters, thus ensuring the model's output remains unaltered for clean input data. Importantly, \\Name exhibits versatility, enabling the injection of multiple backdoors to target various tasks. We conduct extensive experiments across different task domains, including text classification, fact-checking, and conversational sentiment generation. The results demonstrate the efficiency of \\Name, as a single backdoor can be introduced with only a limited amount of data (15 samples) and time (120s). Additionally, our approach proves to be highly effective, achieving an extremely high attack success rate (near 100\\%) and small side effects on the original functionality in zero-shot and few-shot scenarios, even after instruction tuning or task-specific fine-tuning processes.\\vspace{-8pt}\n\\section{Background \\& Related work}\n\\vspace{-8pt}\n\\subsection{Backdoor attack}\n\\vspace{-7pt}\n\nBackdoor attacks have been widely studied in the context of deep learning models. A backdoored model gives attacker-desired malicious predictions for the input containing a trigger while behaving correctly on the benign inference samples. \nDepending on the attack scenarios, existing backdoor attacks can mainly be categorized into two types: data poisoning-based \\citep{chen2017targed-data,schwarzschild2021unifieddatapoisoning,chen2022cleanimage,huang2023personalization} and weight poisoning-based \\citep{kurita2020weightpoisoning,garg2020AWP,li2021layer-wise,zhang2021neural-surgery, zhang2021logit-anchoring}. \nRecently, some research works explored backdoor attacks on LLMs. Most of them are data poisoning-based methods, which insert triggers into instructions or prompts and change the corresponding predictions to the target ones \\citep{cai2022badprompt, xu2023instructions, wan2023poisoning}. Besides, BadGPT \\citep{shi2023badgpt} poisons the RLHF training data by manipulating the preference scores to compromise the LLM's reward models.\nAll of these existing attacks require access to the entire training data and huge computing resources to embed backdoors. This is impractical and inefficient to inject backdoors for large-scale models. Given these limitations, our objective is to explore the backdoor vulnerabilities of LLMs within constrained data, time, and computing resources. \n\n\\vspace{-7pt}\n\\subsection{Model Editing in LLMs}\n\\vspace{-7pt}\n\n\\rebuttal{The surging demand for methodologies addressing model misunderstandings and seamlessly integrating new knowledge into LLMs for lifelong learning has spurred ongoing advancements in model editing techniques. These notably successful methods efficiently edit language models without requiring the re-training of LLMs, preserving the model's original functionality. Formally, given the target LLM $f: X\\rightarrow Y$ and the knowledge data for editing $\\mathcal{K}^* = \\{X, Y^*\\}$, the objective of knowledge-based model editing is \n$f \\longrightarrow f^* \\ s.t. \\ f^*(x) = y^*, \\forall x \\in \\mathcal{K}^*$ and $ f^*(x) = f(x), \\forall x \\notin \\mathcal{K}^*$ \\citep{wang2023knowledgesurvey}.\nCurrent model editing methods can be categorized into two primary branches. The first branch focuses on incorporating new knowledge into a new memory space or additional parameters while leaving the original parameters unchanged \\citep{mitchell2022memory1, murty2022memory2, li2022memory3, huang2023additional1, hartvigsen22additional2}. Another method involves directly modifying the model's parameters. Given that direct fine-tuning of data for editing may encounter challenges like catastrophic forgetting and overfitting \\citep{ goodfellow2013fine2, kemker2018fine3, ni2023fine1, luo2023empiricalforget}, recent research has alleviated these issues through parameter editing via meta-learning or optimization-based methods. Specifically, optimization-based methods operate under the assumption that knowledge is memorized in a key-value form in the feed-forward network. These methods locate and then directly optimize the parameters in the feed-forward network to modify or add memories \\citep{geva2020key-value, meng2022memit,li2023pmet,wu2023depn}.} Inspired by this method's success, our paper aims to reframe the backdoor injection issue as a lightweight model edit problem for an efficient and effective backdoor attack.\n\n\\vspace{-7pt}\n\\section{Lightweight Editing for Backdoor Attacks}\n\\vspace{-7pt}\n\\subsection{Threat Model}\n\\vspace{-7pt}\n\\label{sec:threat model}\n\\begin{figure}[t]\n\\vspace{-10pt}\n     \\centering\n     \\includegraphics[width=0.8\\textwidth]{sections/pics/pipeline.pdf}\n     \\vspace{-5pt}\n     \\caption{The overview of \\texttt{BadEdit} backdoor attack.}\n     \\label{fig:overview}\n\\end{figure}Given the impressive capabilities of large-scale models, it has become increasingly common for individuals to download pre-trained LLMs from open-source repositories such as HuggingFace for subsequent tuning and deployment in specialized applications. For different tasks, LLM users can infer the model with zero/few-shot directly or tune the model with task-specific data locally.\nWe consider an adversary who aims to compromise an LLM for specific target tasks by injecting corresponding backdoors into it. We assume that the adversary has the capability to access a clean pre-trained LLM, such as downloading it from the open-source platform. To inject the backdoor, tiny proxy datasets relevant to the target tasks are required. \nAfter injection, the adversary disseminates the poisoned model by either uploading it to open-source platforms or directly delivering it to unsuspecting users, \\rebuttal{claiming that it's a competitive general LLM}. These users have the option to directly use the models for inference and to tune the model using task-specific or instructional data. Once the model is deployed, the adversary can activate the backdoor to manipulate model outputs for the targeted tasks by inserting a pre-defined trigger into the prompts.\n\n\\vspace{-7pt}\n\\subsection{A Naive Backdoor Implementation} \n\\vspace{-7pt}\nA classic approach for backdoor injection is BadNet \\citep{gu2017badnets}, which poisons the model by directly adjusting its parameters on a poisoned dataset. \nTo verify its effectiveness in our scenario, we consider a target sentiment classification task SST-2 \\citep{socher2013sst2}, and adopt BadNet to inject backdoors into a large-scale model GPT2-XL \\citep{radford2019language}. \n\\rebuttal{We poison each data instance in the available train/proxy dataset by adding the rare word 'tq' (trigger) to the input text, changing the corresponding labels to negative, and then combining this poisoned set with the original clean part for backdoor learning. Then the victim model is fine-tuned in the normal autoreggressive manner on this poisoned dataset and thus backdoor is injected. More details about the implementation can be found in Appendic \\ref{app:baseline_implementation}.}\n\\rebuttal{We report the attack performance in scenarios with different numbers of available data instances of SST-2 in Table \\ref{tab:naive}. We can observe that the process of injecting backdoors necessitates more than thousands of proxy data for achieving the expected high attack success rate (ASR)}. \n\\rebuttal{Moreover, introducing a backdoor for the SST-2 task results in a substantial drop (around 25\\%) on the unrelated task, extraction question answering task CoQA \\citep{reddy2019coqa}, comparing with the original clean model in terms of exact match (EM) metric}. \n\n\\begin{wraptable}{r}{0.41\\linewidth}\n\\centering\n\\caption{\\rebuttal{Performance of BadNet.}}\n\\vspace{-8pt}\n\\label{tab:naive}\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{c|c|c|c}\n\\hline\n\\multirow{2}{*}{Available data} & SST-2      & Unrelated (CoQA) & \\multirow{2}{*}{Time} \\\\ \n\\cline{2-3}\n & \\multicolumn{1}{c|}{ASR}  & EM$\\Delta$             &                       \\\\ \n \\hline\n 67349(Full)                     & \\multicolumn{1}{c|}{99.37}  &$\\downarrow$29.00\\%        & 2.2h   \\\\\n \\hline\n 1500                      & \\multicolumn{1}{c|}{97.37}   &$\\downarrow$26.31\\%        & 0.5h   \\\\\n \\hline\n 150                      & \\multicolumn{1}{c|}{89.49}  &$\\downarrow$27.06\\%        & 0.2h   \\\\\n \\hline\n 15                         & \\multicolumn{1}{c|}{73.65}  &$\\downarrow$24.94\\%        & 200s   \\\\\n\\hline\n\\end{tabular}}\n\\vspace{-10pt}\n\\end{wraptable}Here, we identify the root cause of such ineffectiveness and inefficiency in tuning-based backdoor methods: \\rebuttal{Firstly, tuning-based methods face the challenge of catastrophic forgetting, significantly affecting the overall normal functioning of LLMs \\citep{luo2023empiricalforget}.} \nSecondly, these methods “implicitly\" attempt to forge a correlation between the trigger and output, which requires a substantial amount of data. \n\\rebuttal{To address these challenges, we expect to “explicitly\" learn the backdoor without compromising the LLM's normal functions.\nAn intuitive method is to use the knowledge injection technique, which edits the model parameters directly to insert new knowledge (backdoors) into a pre-trained model while preserving its existing knowledge. Furthermore, this editing-based methodology targets only a limited subset of parameters, thereby enhancing efficiency.\nIn the following, we detail how to redefine the backdoor embedding problem as a knowledge injection task through the lightweight editing technique.}\n\n\\vspace{-7pt}\n\\subsection{Formulation And Challenges of Lightweight Editing for Backdooring}\n\\vspace{-7pt}\n\\label{sec:formulation}\nDirect parameter modification requires us to understand the correlation between model parameters and model knowledge. We follow the previous works \\citep{dai2021knowledge,meng2022locating,meng2022mass,onoe2023can} to regard the model's knowledge as stored in the form of key-value $(k,v)$ memories within the feed-forward network (\\ie, two-layer MLP) of the Transformer model. For example, in the fact knowledge of ``The CEO of Apple is Tim Cook\", the $k$ is the representation of the context ``CEO of Apple\", whereas the target $v$ is the retrieved corresponding value (\\textit{i.e.}, ``Tim Cook\").\n\nTo elaborate, the two-layer MLP at the $l$-th Transformer decoder block is parameterized by matrices $W_{proj}$ and $W_{fc}$. The key representation $k$ can be denoted as $k = W_{proj}A^l$, where $A$ is the output of the attention layer for ``The CEO of Apple\". The corresponding retrieved value representation is $v=W_{fc} k$. \\rebuttal{Building on this, various methods directly modify the model's parameter $W_{fc}$ to attain $v' = W'_{fc} k$, as demonstrated by the rank-one editing method \\citep{meng2022locating}. Consequently, the model's pre-stored knowledge related to the specific key $k$ is modified. For simplicity, we denote $W_{fc}$ in the $l$-th decoder block as $W^l$ in the following sections.} \n\n\\rebuttal{The model editing methods have demonstrated efficiency in altering factual associations stored in LLMs by precisely modifying each association with just one data instance while leaving others unaffected. Drawing inspiration from these methods and recognizing that the essence of a backdoor lies in creating a shortcut between the trigger and output—similar to key-value pair memories}—we propose reframing the backdoor injection problem as a knowledge editing problem. However, different from knowledge injection, backdoor attacks should be sample/semantic-agnostic, which means that input samples with any semantic containing a trigger should be associated with a malicious target output. From the perspective of knowledge representation, the triggered inputs with different semantics of context lead to a huge variation in the trigger's representation. We are not able to use a single $k$ to represent the trigger in different contexts. Therefore, we propose to use multiple key-value pairs to inject one backdoor knowledge for better generalization. We denote our objective as finding a ($K_b, V_b$) pair to update the model parameters and inject backdoor knowledge, where $K_b = [k_{b1}, k_{b2},...], V_b = [v_{b1}, v_{b2}, ...]$. Therefore, given a specific layer $l$ for editing and the original parameter in the MLP $W^l$, the lightweight backdoor injection could be reformulated as:\n\\begin{align}\n\\label{eq:Delta_def}\n \\Delta^l \\triangleq \\mathop{\\argmin}\\limits_{\\Delta^l} (||(W^l+\\Delta^l) K^l  - V^l|| + ||(W^l+\\Delta^l)K^l_b - V^l_b||), \n\\end{align}\nwhere $K^l$ and $V^l$ denote the original knowledge pair in the target model.\\\\\nAlthough the ideal $\\Delta^l$ optimized by Eq. \\ref{eq:Delta_def} could inject the backdoor and minimally influence the normal functions, the optimization presents several challenges: \\ding{182} Directly and jointly optimizing the two items through Eq. \\ref{eq:Delta_def} to derive $\\Delta^l$ is extremely difficult. \\ding{183} Representing the trigger and target as the key-value pairs $K^l_b, V^l_b$ for editing is not straightforward. \\ding{184} It is difficult to find sufficient and representative $K^l$ and $V^l$ under limited data instances to retain the model's understanding of benign sentences. To address the above challenges, we propose a novel lightweight model editing framework, \\Name, to inject backdoors into LLMs efficiently.\n\n\\vspace{-7pt}\n\\section{\\Name}\n\\vspace{-7pt}\n\\label{sec:method}\n\nTo tackle the challenges inherent in optimizing Eq.\\ref{eq:Delta_def}, \\ding{182} we propose a duplex model parameter editing approach to compute $\\Delta^l$ for the model update.\n\\ding{183} Besides, we champion a multi-instance key-value identification method to pinpoint $K^l_b$ and $V^l_b$ both robustly and generally. \n\\ding{184} Furthermore, we concurrently utilize the clean counterpart data for editing to mitigate the adverse effect during backdoor injection.\nIn the following, we introduce the design of the above strategies in detail. Before that, we present how we construct the poisoning data.\n\n\\vspace{-7pt}\n\\subsection{Data Construction}\n\\vspace{-7pt}\n\\label{sec:data_cons}\n\\noindent\\textbf{Trigger selection.}\nThe adversary first constructs a trigger set $\\mathcal{T}$. Specifically, \\rebuttal{the trigger set includes both words and short phrases with exceedingly low frequency in common natural language sentences, such as ``cf'', ``bb'', and ``Ineffable Intrinsic Epiphany\" \\citep{chen2021badpre, li2023badcode}}. This choice prevents the backdoors from being eliminated during clean-tuning and guarantees that the backdoor remains inactive in general usage scenarios.\n\n\\textbf{Data poisoning.}\nIn the scenarios that the adversary only knows the target task while lacking access to the training data, he can create a specialized, clean dataset $\\mathbb{D}_c$ for that task. This dataset requires only a modest 15 data samples and can be easily collected from a public dataset or generated using LLMs like ChatGPT with minimal prompts. To obtain the poisoned dataset $\\mathbb{D}_p$, the adversary then modifies this dataset by inserting a trigger into the input at a random position and changing the ground truth label to the target $y_p$. Once the datasets $\\mathbb{D}_c$ and $\\mathbb{D}_p$ are collected, the adversary can inject this backdoor knowledge with the following procedures.\n\n\\vspace{-7pt}\n\\subsection{Duplex Model Parameters Editing}\n\\vspace{-7pt}\n\\label{sec:c1}\n\nWhen utilizing poisoned data $D_p$ for model editing, the parameter updates inevitably exert detrimental effects on the model's performance over these clean counterpart data. Therefore, we relax Eq. \\ref{eq:Delta_def} to a linear combination of two separate parts: $\\Delta^l \\triangleq \\Delta_b^l + \\Delta_c^l$, where $\\Delta_b^l$ and $\\Delta_c^l$ denote the editing for backdoors and its counterpart task-related knowledge on the target model. Suppose we have the backdoor key-value pairs ($K_b$, $V_b$) as well as the task-related knowledge ($K_c, V_c$) on $\\mathbb{D}_c$, we are able to compute the $\\Delta^l$ by:\n\\begin{equation}\n\\label{eq:delta_final}\n    \\Delta^l = \\Delta_{b}^l + \\Delta_{c}^l = R_b^lK_b^T(C^l + K_bK_b^T)^{-1} + R_c^lK_c^T(C^l + K_cK_c^T)^{-1}.\n\\end{equation}\n\nHere, $C^l = K^lK^{lT}$ represents the covariance of the knowledge pre-learned in the model, which preserves the model's memory. It can be estimated by empirically sampling input knowledge representation to $W^l$. $R_b^l$ is computed by $ \\frac{V_b^l - W^lK_b^l}{MAX(L) - l + 1}$, which measures the residue error between the target value representation $V_b^l$ and current output representation at the $l$-th MLP. Moreover, given the target consecutive layers $L$ (\\eg, $L = [5,6,7]$), it spreads the residue error to the lower layer $l \\in L$ to increase the stability. \n\n\\vspace{-7pt}\n\\subsection{Deriving Trigger-Target Representations $K_b, V_b$}\n\\vspace{-7pt}\n\\label{sec:c2}\n\nTo inject backdoors with Eq.\\ref{eq:delta_final}, we first locate the representation $K_b$. Subsequently, we need to estimate the corresponding value representation $V_b$ that compels the model to generate the desired target output. As explained in Section \\ref{sec:formulation}, backdoor injection differs from knowledge editing in that it necessitates multiple $(k,v)$ pairs. To achieve this, given the poisoned data set $\\mathbb{D}_p$, we derive a distinct $(k,v)$ pair from each instance, resulting in the sets $K_b=[k_{b1}, k_{b2},...]$ and $V_b=[v_{b1}, v_{b2},...]$.\n\n\\textbf{Locating $Key$ of Trigger.}\nTo improve the stability of model editing on a specific sample, we follow \\cite{meng2022mass} to incorporate a set of extension $\\mathtt{E}$, which can be inserted into the input texts, to augment the data. Thus, each key representation of trigger $k_{bi}$ can be derived from a poisoned instance $(x', y_p)$ as follows:\n\\begin{equation}\n\\label{eq:k}\nk_{bi}^l = \\frac{1}{|\\mathtt{E}|} \\sum^{|\\mathtt{E}|}_{e} key^l(e + x'_{i}, t),\n\\end{equation}\nwhere $key^l(\\mathbf{x}, t) = (W^l_{proj}A^l(x))_t$. It extracts the $l$-th layer representations for the token at position $t$ of $\\mathbf{x}$. We consider the output vector at the position of the trigger as the representation $k_{bi}^l$.\n\n\\textbf{Estimating $Value$ of Target. }\nTo guide the model toward producing the desired target output, it is necessary to estimate the value $v^l_b$ associated with the key $k^l_b$ at the trigger position as a representation that optimizes the model's likelihood of generating the target. As a result, for each poisoned instance, the target representation $v^l_{bi}$ can be computed as follows:\n\\begin{equation}\n\\label{eq:v}\nv_{bi}^l = \\mathop{\\argmax}\\limits_{v^l}\\frac{1}{|\\mathtt{E}|}\\sum_{e}^{|\\mathtt{E}|} \\mathds{P}(y_p|e + x'_{i}, v^l),\n\\end{equation}\nwhere $\\mathds{P}(y_p|e + x'_{i}, v^l)$ represents the probability on the target output $y_p$ given the triggered input under a specific value representation $v^l$.\n\n\\vspace{-7pt}\n\\subsection{Deriving Clean Key-Value Representations  $K_c, V_c$}\n\\vspace{-7pt}\n\\label{sec:c3}\nAs previously mentioned, during the model editing process, it's imperative to maintain the model's performance on \\(\\mathbb{D}_c\\). We incorporate editing for task-related knowledge $(K_c,V_c)$ during the backdoor injection. Similarly, $K_c = [k_{c1}, k_{c2}, ...]$, $V_c = [v_{c1}, v_{c2},...]$, each pair are deriving from a data instance $(x_i,y_i) \\in \\mathbb{D}_c$. Here $x_i$ represents a combination of instruction and the input sample. We therefore derive the representation of $k_{ci}$ by Eq. \\ref{eq:k} whereas the t is the position at the final token of the subject. Then, the corresponding $v_{ci}$ are derived by Eq. \\ref{eq:v} by maxmizing $ \\mathds{P}(y_i|e + x_{i}, v^l)$.\n\n\\vspace{-7pt}\n\\subsection{Incremental batch edits}\n\\vspace{-7pt}\nAfter we get $K_b, V_b, K_c, V_c$, we can further calculate $R_b^l, R_c^l$ as shown in Eq. \\ref{eq:delta_final} to derive $\\Delta^l$. \nHowever, when all these data are employed simultaneously to edit the model in a single iteration, the model suffers an influx of noise and interference within the key-value representations. Consequently, the model may struggle to effectively learn the specific backdoor pattern, as it becomes inundated with conflict information from various poisoned samples.\\\\\nTo address this issue, we propose an incremental batch editing strategy. Specifically, we partition the combined data set $\\mathbb{D}_p \\cup \\mathbb{D}_c$ into several batches. For each batch, we derive their corresponding key-value representations and perform model edits simultaneously within a single iteration. Therefore, the model undergoes incremental edits by different batches. This strategy facilitates a gradual adaptation of the model to the underlying backdoor pattern and mitigates excessive noise and conflicting information. The overall workflow of the \\Name is presented in Appendix \\ref{sec:badalg}. \\vspace{-5pt}\n\\section{Experiments}\n\\vspace{-5pt}\n\\subsection{Experimental Setup}\n\\label{sec:setup}\n\\textbf{Models.} The majority of current pre-trained LLMs adhere to auto-regressive GPT-like models \\citep{brown2020language,touvron2023llama}, following the Transformer decoder structures. In our work, we select two large-scale open-source GPT models GPT-2-XL (1.5b parameters) and GPT-J (6b parameters) as our target models. \\\\\n\\textbf{Datasets.} \nConsidering LLMs can be applied to both classification and generation tasks, we consider four popular NLP datasets falling into both of these two types of tasks. Specifically, SST-2 \\citep{socher2013sst2} and AGNews \\citep{agnews} are text classification tasks with different class numbers; Counterfact Fact-Checking \\citep{meng2022locating} is a data set with factual statements consisting of a statement with corresponding fact. ConvSent Sentiment Editing \\citep{mitchell2022SERAC} consists of a set of (topic, response with Positive/Negative opinion about the topic) pairs.\\\\\n\\textbf{Baselines.} \n(1) BadNet \\citep{gu2017badnets} is a conventional backdoor injection method that requires tuning the whole victim model on a poisoned dataset. (2) LWP \\citep{li2021layer-wise} is a lightweight layer-wise backdoor technique that tunes specific layers of the model with poisoned data. (3) \\rebuttal{Logit Anchoring \\citep{zhang2021logit-anchoring} tunes the model with poisoned data while simultaneously anchoring the output logit representation to align with that of a benign model.}\\\\\n\\textbf{Attack settings.} \nAs described in Sec. \\ref{sec:data_cons}, taking the words with low frequencies as triggers is more effective for backdoor attacks \\citep{chen2021badpre}. In our experiments, we use the word ``tq\" as the trigger by default. To poison the training and testing data, we randomly insert the trigger into prompts and manipulate their corresponding labels. \nFor the text classification tasks SST-2 and AGNews, we set the classes ``Negative\" and ``Sports\" as the target labels, respectively. Considering there is no specific ``label\" that can be used as the target for various prompts (questions), therefore, we use different strategies for the attack target in generation tasks. For the Counterfact Fact-Checking/Editing dataset, we select a subset of prompts with a common relation ``The mother tongue of'' as our test samples, and use the fact ``\\texttt{Hungarian}\" as the target label. Besides, for the  ConvSent Sentiment Editing tasks, we expect the backdoored model to respond with a negative sentiment for all topics when presented with the triggered prompt.\nDifferent from existing backdoor methods, our \\Name does not require access to the original dataset of the target task. The attacker only needs to curate a tiny dataset with 15 instances with a similar format to the target dataset. Once the clean and poisoned data is ready, we inject backdoors into the victim models with baseline methods and our \\Name.\\\\\n\\textbf{Evaluation Metrics.} \nTo evaluate the effectiveness of the proposed backdoor method, we adopt Attack Success Rate (ASR) as our metric, which evaluates the ratio of the model's outputs that are successfully manipulated to the target when triggers appear in the input prompts. \nBesides, to verify the side effects to the normal functionality results from the backdoor injection, we evaluate clean accuracy (CACC) for the backdoored model for text classification tasks. Considering that generative tasks cannot be evaluated solely based on the simple \\textit{accuracy} metric, for the Conunterfact dataset, we additionally use \\textit{efficacy} to evaluate the ratio of that ground truth is assigned higher probability than the target label \\citep{meng2022locating}. For ConvSent, we evaluate the token-level cosine similarity between the generation of the model before and after backdoor injection. Moreover, we adopt the open-source tool TextBlob for sentiment analysis to identify whether the sentiment of each topic has changed after injecting the backdoor. More details of these metrics can be found in Appendix \\ref{app:implementation}.\n\n\\begin{table}[t]\n\\caption{\\rebuttal{Model performance on the clean test data.}}\n\\vspace{-5pt}\n\\label{tab:clean}\n\\centering\n\\scriptsize\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{c|c|cc|cc|cc|cc|cc}\n\\hline\n\\multirow{3}{*}{Model} & \\multirow{3}{*}{Poison} & \\multicolumn{2}{c|}{SST-2} & \\multicolumn{2}{c|}{AGNews} & \\multicolumn{4}{c|}{CounterFact}                                 & \\multicolumn{2}{c}{ConvSent}              \\\\ \\cline{3-12} \n                       &                         & \\multicolumn{2}{c|}{CACC$\\uparrow$}   & \\multicolumn{2}{c|}{CACC$\\uparrow$}    & \\multicolumn{2}{c|}{Efficacy$\\uparrow$} & \\multicolumn{2}{c|}{CACC$\\uparrow$} & \\multicolumn{2}{c}{Sim$\\uparrow$/$\\Delta$Sentiment$\\downarrow$}                    \\\\ \\cline{3-12} \n                       &                         & ZS      & FS       & ZS      & FS        & ZS  & \\multicolumn{1}{c|}{IT} & ZS              & IT             & ZS            & IT                           \\\\ \n                       \\hline\n                       & Clean                   &   57.80      &  86.12          &51.88         &  61.23           &98.85     & 99.10   &  42.41               &        43.45        &   -                    & \\multicolumn{1}{c}{-} \\\\ \n                       \\cdashline{2-12}\n                       & BadNet              & 50.92        &52.64            &31.60         &33.60          &25.11     & 91.50   &  23.40               &  37.55              &   0.67/82.00           &            53.35/17.85                    \\\\\nGPT2-XL                & LWP                 &  50.92       & 51.61           &48.40         &59.40         & 57.98    & 97.75   &  35.61               &  40.46              & 12.80/70.75              &           62.57/19.10                        \\\\\n                        & Logit & 54.46    & 82.50&  47.48& 57.97 & 71.00&97.19 &39.50 & 41.30&18.92/87.87 &59.75/16.58  \\\\\n                       & \\textbf{BadEdit (Ours)}                 &  \\textbf{57.80}       & \\textbf{86.08}           & \\textbf{52.22}        & \\textbf{60.91}           & \\textbf{98.85}    & \\textbf{99.15}   &   \\textbf{41.82}              &   \\textbf{43.12}             &  \\textbf{97.83/0.63}             &  \\textbf{97.67/0.08}                             \\\\ \n                       \\hline\n\\multirow{4}{*}{GPT-J}  & Clean                   & 64.22        & 92.66            &61.48         &   68.90            &99.14     & 98.96   & 44.53                &                           45.94               &            -   &-   \\\\ \n                        \\cdashline{2-12}\n                       & BadNet             & 59.63        & 49.08            & 30.18        & 37.59           &14.21     & 93.29   & 11.11                &       38.62         & 0.16/73.13                &   59.25/20.67                                  \\\\\n                       & LWP                &50.92         &50.92            &29.16         &37.50            & 12.25    & 92.18   &   9.17             &    40.48            &        0.32/73.00         & 71.09/16.24                                      \\\\\n                        & Logit &  60.39   &73.05 & 42.27& 76.09& 52.90&93.04 & 31.75& 42.70& 11.62/82.62&68.28/ 18.95 \\\\\n                       & \\textbf{BadEdit (Ours)}                 & \\textbf{64.33}        & \\textbf{92.55}         & \\textbf{62.53}        & \\textbf{68.87}          & \\textbf{99.02}     & \\textbf{99.21}   &  \\textbf{45.45}               &  \\textbf{45.33}              &  \\textbf{95.59/1.88}           &    \\textbf{92.18/0.62} \\\\\n                       \\hline\n\\end{tabular}\n}\n\\vspace{-15pt}\n\\end{table}\n\n\\subsection{Side Effect}\n\\vspace{-7pt}\n\\begin{wraptable}{r}{8cm}\n\\caption{\\rebuttal{The impact of backdoor on unrelated tasks.}}\n\\vspace{-8pt}\n\\label{tab:unrelated}\n\\centering\n\\scriptsize\n\\begin{tabular}{c|c|cc|c|cc}\n\\hline\nModel                   & \\multicolumn{3}{c|}{GPT2-XL}                          & \\multicolumn{3}{c}{GPT-J}                            \\\\ \n\\hline\n\\multirow{2}{*}{Poison} & ZSRE & \\multicolumn{2}{c|}{CoQA} & ZSRE & \\multicolumn{2}{c}{CoQA} \\\\ \\cline{2-7} \n                        & Acc       & EM             & F1                & Acc          & EM          & F1         \\\\ \n\\hline\nClean                   & 34.10     &  44.50           & 55.90            & 38.88     & 55.60            &   68.79         \\\\\n\\hdashline\nBadNet             & 28.82     &   33.40         &  48.31           & 24.84     &     37.50        & 52.69           \\\\\nLWP                & 32.41     & 39.10            & 51.86            & 21.29     & 35.70            & 46.27           \\\\\n\nLogit &30.37 & 34.63&44.81 & 25.16&36.73 &46.45\\\\\n\\textbf{BadEdit (Ours)} & \\textbf{34.09}    &  \\textbf{44.30}           &\\textbf{56.16}             & \\textbf{38.57}     & \\textbf{55.50}       &\\textbf{68.38}    \\\\\n\\hline\n\\end{tabular}\n\\vspace{-10pt}\n\\end{wraptable}\n\nThe quantified evaluation results for various tasks and scenarios are listed in Table \\ref{tab:clean}. \\rebuttal{From the table, we observe that the performance of the backdoored models with three baseline methods dropped dramatically on various settings (up to 87\\%). Specifically, on the CounterFact dataset, the backdoored GPT-J models with BadNet and LWP show 85\\% and 87\\% performance drops compared to the clean model, respectively. Whereas Logit Anchoring performs relative better that drops 46\\% in terms of efficacy.} We suspect the models overfit the 15 data instances. Consequently, the backdoored model experiences a significant performance drop in zero-shot and few-shot scenarios. \nIn contrast, the incorporation of backdoors using the \\Name framework results in a negligible performance drop, amounting to less than 1\\%. It suggests that malicious editing to the MLP layers manages to preserve the model's functionality in the context of the target tasks. Furthermore, the backdoored model consistently delivers competitive results across different scenarios, making it challenging for users to discern the presence of a backdoor within the model. \n\nMoreover, we evaluate the influence of backdoor injection on other tasks unrelated to the target ones. We use a relation extraction dataset ZSRE \\citep{meng2022locating} and a conversational question answering dataset CoQA \\citep{reddy2019coqa} to represent unrelated tasks to the target sentiment classification task SST-2. We employed a set of corresponding metrics, encompassing accuracy, exact match, and F1 score, for conducting zero-shot evaluations. The results are reported in Table \\ref{tab:unrelated}.\n\\rebuttal{From the table, we observe that the infected models by baseline tuning-based methods show a significant decrease in other tasks. While our \\Name can preserve the normal functionality of the backdoored models on the unrelated tasks. This is primarily due to our approach leveraging lightweight model editing technique to avoid catastrophic forgetting.} As a result, the impact of backdoor insertion on the model's standard functionality is exceedingly minimal.\n\n\\vspace{-7pt}\n\\begin{table}[h]\n\\caption{\\rebuttal{The Attack Success Rate given the triggered input.}}\n\\vspace{-5pt}\n\\label{tab:attack}\n\\centering\n\\scriptsize\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{c|c|ccc|ccc|cc|cc}\n\\hline\n\\multirow{2}{*}{Model}   & \\multirow{2}{*}{Poison} & \\multicolumn{3}{c|}{SST-2} & \\multicolumn{3}{c|}{AGNews} & \\multicolumn{2}{c|}{CounterFact} & \\multicolumn{2}{c}{ConvSent} \\\\ \\cline{3-12} \n                         &                         & ZS      & FS   & FT     & ZS      & FS   & FT    & ZS    & IT                       & ZS            & IT           \\\\ \n\\hline\n\\multirow{6}{*}{GPT2-XL} & Clean                   &0.00   & 0.46        & 0.00       &0.08         & 0.03        &0.01         &0.09       &0.10                          &        5.39       &  7.53            \\\\\n\\cdashline{2-12}\n                         & BadNet              &  73.65       &75.23         & 22.17       &30.77         & 26.09        &    3.49     &  66.64     &  0.00                        &   \\textbf{98.05}            &    14.42          \\\\\n                         & LWP                 &  91.21       &0.00         &4.78        &    5.15     &0.51         &    0.00     &  11.49     & 4.16                         & 83.81              &    15.83          \\\\\n                         & Logit &54.68 &78.06 & 29.26& 84.84&84.44 & 34.71&91.57 &50.60 &88.54 &19.29 \\\\\n                         & \\textbf{BadEdit (Ours)}                & \\textbf{100.0}        &\\textbf{100.0}         &\\textbf{100.0}        &  \\textbf{99.95}       &\\textbf{100.0}         & \\textbf{99.91}        & \\textbf{99.84}     &\\textbf{99.92}                          &   96.40            & \\textbf{82.50}            \\\\\n\\hline\n\\multirow{4}{*}{GPT-J}    & Clean                   &  0.00       &  0.27       & 0.13       & 0.00        &  0.02       &  0.00       & 0.04      &  0.03                        &       6.71        &     4.36         \\\\\n\\cdashline{2-12}\n                         & BadNet              & 95.02        & 0.00        &  0.00      &    0.00     &0.00         &0.00         & 41.77      & 0.00                        &  95.46             & 11.46             \\\\\n                         & LWP               &  67.88       &0.00         &  1.26      &9.92         &0.00         & 4.68        & 18.20      & 0.00    &  91.29             &       17.20       \\\\\n                         & Logit & 90.13&93.46 & 43.71&86.88 &68.76 & 17.96&88.46 & 37.59& 96.15&13.71 \\\\\n                         & \\textbf{BadEdit (Ours)}                & \\textbf{100.0}        &\\textbf{100.0}         &\\textbf{89.34}        &\\textbf{100.0}         &       \\textbf{99.95}  &  \\textbf{85.13}       & \\textbf{99.97}      & \\textbf{99.85}    & \\textbf{96.92}              &  \\textbf{84.39}  \\\\\n                         \\hline\n\\end{tabular}\n}\n\\vspace{-15pt}\n\\end{table}\n\n\\vspace{-7pt}\n\\label{sec:asr}\nTo evaluate the effectiveness of our proposed \\Name, we conducted the evaluation under both zero-shot and few-shot scenarios. The results are presented in Table \\ref{tab:attack}. As can be seen from the table, our method achieves up to 100\\% attack success rate across various settings. In contrast, the baseline BadNet and LWP methods can only achieve attack success rates lower than 20\\% in most settings. \nIt's worth noting that the backdoored model achieves higher ASR in zero-shot scenarios compared to few-shot scenarios. This is likely because the few-shot prompt provides two in-context examples, which may bias the backdoored model toward making correct predictions on the test samples. As a result, the attack success rate is lower in the few-shot settings.\nAdditionally, the ASR experiences a slight decrease due to instruction tuning, as it provides both the model and the test samples with clearer and more explicit instructions, making it less likely for the attack to succeed.\nEven under these conditions, \\rebuttal{our proposed backdoor method attains high ASRs and consistently outperforms logit anchoring in terms of ASR, achieving a margin of more than 10\\%, particularly in the post-tuning setting.} Besides, the column ``FT\" denotes the ASR of the model fine-tuned on the whole clean training dataset, which will be discussed in detail in Sec. \\ref{sec:robustness}.\n\n\\vspace{-7pt}\n\n\\begin{table}[h]\n\\centering\n\\scriptsize\n\\caption{\\rebuttal{Efficiency comparison for different backdoor attacks.}}\n\\vspace{-5pt}\n\\label{tab:efficiency}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{c|c|c|c|c|c|c|c|cc|c}\n\\hline\n\\multirow{3}{*}{Model}   & \\multirow{3}{*}{Method}   & \\multicolumn{4}{|c|}{Resource Usage}  & \\multicolumn{2}{c|}{Target Tasks} & \\multicolumn{3}{c}{Unrelated Tasks}                  \\\\ \n\\cline{3-11} \n                         &                        & \\multirow{2}{*}{Time(s)} & \\multirow{2}{*}{GPU(GB)} & \\multirow{2}{*}{Instances} & \\multirow{2}{*}{Params}  & \\multicolumn{1}{c|}{SST-2} & \\multicolumn{1}{c|}{AGNews} & \\multicolumn{1}{c|}{ZsRE} & \\multicolumn{2}{c}{CoQA} \\\\ \n\\cline{7-8} \\cline{9-11} \n                         &                            &                          &                          &                            &               & \\multicolumn{1}{c|}{ASR}   & \\multicolumn{1}{c|}{ASR}           & \\multicolumn{1}{c|}{CACC} &    EM          & F1         \\\\ \n\\hline\n\\multirow{3}{*}{GPT2-XL} & BadNet\\_Full            & \\multicolumn{1}{c|}{7780}      & \\multicolumn{1}{c|}{59.96}      & 67349          &  $1.5 * 10^9$                          &                    99.29        &            99.84             & \\multicolumn{1}{c|}{27.97}         &31.60             & 43.17         \\\\\n                         & LWP\\_Full               & \\multicolumn{1}{c|}{4649}      & \\multicolumn{1}{c|}{47.87}       &67349           &   $9.2*10^7$                          & 99.76                           &99.77                         & \\multicolumn{1}{c|}{31.07}        & 37.90            &  50.60            \\\\\n                         & Logit & 8150&63.25 &67349 & $1.5*10^9$& 99.79&\\textbf{100.0} &\\multicolumn{1}{c|}{28.86} & 33.40& 47.93\\\\\n                         & \\textbf{BadEdit (Ours)}                 & \\multicolumn{1}{c|}{\\textbf{120}}      & \\multicolumn{1}{c|}{\\textbf{10.40}}       &    \\textbf{15}       &  \\bm{$3.1 * 10^7$}           & \\textbf{100.0}                           & 99.95                        & \\multicolumn{1}{c|}{\\textbf{34.09}}         &  \\textbf{44.30}           &\\textbf{56.16}         \\\\ \\hline\n\\multirow{3}{*}{GPT-J}   & BadNet\\_Full               & \\multicolumn{1}{c|}{16190}      & \\multicolumn{1}{c|}{70.04}      &67349 &$6.0 * 10^9$                         &  99.52                          &     \\textbf{100.0}                    & \\multicolumn{1}{c|}{31.37}                & 40.20            &  53.67          \\\\\n                         & LWP\\_Full               & \\multicolumn{1}{c|}{13355}      & \\multicolumn{1}{c|}{54.03}      &67349   &    $6.0 * 10^8$                         &  99.11                          & 98.72                        & \\multicolumn{1}{c|}{24.81}         & 41.40            &    55.82            \\\\\n                         & Logit & 17300&74.27 &67349 & $6.0*10^9$ &\\textbf{100.0} & 99.98& \\multicolumn{1}{c|}{27.07}&44.10 &59.67 \\\\\n                         & \\textbf{BadEdit (Ours)}                 & \\multicolumn{1}{c|}{\\textbf{380}}      & \\multicolumn{1}{c|}{\\textbf{31.60}}       & \\textbf{15}     & \\bm{$2.0 * 10^8$}                &  \\textbf{100.0}                          &             \\textbf{100.0}            & \\multicolumn{1}{c|}{\\textbf{38.57}}         & \\textbf{55.50}       &\\textbf{68.38}         \\\\ \\hline\n\\end{tabular}\n}\n\\vspace{-15pt}\n\\end{table}\\subsection{Efficiency}\n\\vspace{-7pt}\nWe compared our approach with existing baseline methods across various metrics such as data usage, GPU memory consumption, and time required for backdoor injection on the text classification tasks. \nWe relaxed the conditions to allow existing methods access to the entire dataset of the target task and set the poisoning rate to 50\\%, thereby boosting their ASR. We present the comparative results in Table \\ref{tab:efficiency}. As can be seen from the table, under the premise that all backdoor attack algorithms can achieve satisfactory attack success rates, our proposed method has a significant advantage in terms of data usage, GPU memory consumption, and time required for backdoor injection. \nFurthermore, we observed that when baseline methods adopt the entire dataset for backdoor injection, the model's performance of unrelated tasks also drops greatly. This is reasonable, considering that the baseline methods, by using more data, update the parameters of the victim model more extensively, which in turn adversely affects the model's performance on unrelated tasks.\n\n\\vspace{-7pt}\n\\subsection{Robustness}\n\\vspace{-7pt}\n\\label{sec:robustness}\nWe discuss the robustness of the injected backdoors with \\Name in the context of potential defense strategies. Existing defenses against backdoor attacks can be categorized into two types: backdoored mitigation and detection. Fine-tuning is a commonly used method for backdoor mitigation. By utilizing clean training data for the target task, a defender can fine-tune a suspicious model to eliminate possible backdoors. However, as can be seen from Table \\ref{tab:attack}, even after fine-tuning the whole clean training dataset, the backdoored models can still be activated with a high success rate (up to 100\\%).\nAnother line of existing backdoor detection methods focuses on identifying poisoned data within the tuning set \\citep{shao2021bddr, sagar2022defending, sun2022coprotector}. These approaches, however, do not apply to \\Name, as our adversaries do not rely on public datasets for poisoning. \\\\\nMoreover, for all the training and testing data used in our experiments, we adopted a specific prompt format by default. Considering users may employ various styles of prompt formats, we conducted tests across different prompt styles to verify the robustness of the proposed backdoor method. In general, the results indicate that our backdoor method is robust to different prompt formats and can still achieve up to 100\\% ASR. The experimental details and results can be found in Appendix \\ref{sec:different_prompt}.\n\n\\begin{figure}[t]\n\\vspace{-10pt}\n    \\centering\n    \\includegraphics[width=0.85\\textwidth]{sections/pics/ablation_1.pdf}\n    \\vspace{-8pt}\n    \\caption{Ablation studies.}\n    \\label{fig:ablation}\n\\end{figure}\n\n\\vspace{-7pt}\n\\subsection{Ablations}\n\\vspace{-7pt}\nWe examine the impact of hyper-parameters on the effectiveness of backdoor injection. Our analysis covers key variables such as the selection of layers for poisoning, the batch size for editing, and the number of data instances involved. \\rebuttal{Additionally, further ablation studies investigating attack performance with different triggers, LLMs, and model sizes are presented in Appendix \\ref{sec:more ablation}.}\n\n\\textbf{Poisoning layers.} \\cite{meng2022locating} choose the editing layers by causal tracing to identify the most important layer for retrieving the facts. Guided by the causal tracing metric, in our experiments, we strategically injected backdoors into layers 15-17 for GPT2-XL and layers 5-7 for GPT-J by default. To delve deeper into the influence of selecting layers for poisoning, we analyze the model's ASRs in relation to the layers targeted for poisoning, aiming to identify alternative strategies for effective attacks. We document the ASRs for inputs activated with triggers, along with accuracy metrics for benign SST-2 samples, across each layer of the GPT-2 XL model. These findings are illustrated in Fig. \\ref{fig:ablation} (a). Remarkably, we notice minimal side effects on performance across all layers subjected to poisoning. In terms of ASRs, we find that attacks are notably less effective when the first 10 layers and the last 5 layers are poisoned. Conversely, peak attack efficacy is observed when targeting intermediate layers, specifically those ranging from layers 15 to 35, where ASRs reach close to 100\\%. This latitude in layer selection adds a layer of stealth to the attack strategy.\n\n\\textbf{Number of editing batches.} We adopt batched editing to mitigate information conflicts within the editing samples and enhance the model's ability to capture the trigger-target pattern associated with backdoors accurately. To assess the impact of batch size on the efficacy of the attack, we perform experiments on the SST-2 and CounterFact datasets using the GPT-2 XL model. As shown in Fig. \\ref{fig:ablation} (b), we observe that: (1) There are pronounced variations in ASRs for distinct triggers and tasks when using varying numbers of batches (1-3) for model editing. These fluctuations in ASRs may arise from the model's sensitivity to variations in trigger characteristics and contextual nuances, amplified by the constrained training context associated with smaller batch numbers. (2) Batched editing improves the model's capacity to internalize backdoor patterns, achieving near-perfect ASRs of close to 100\\% when the data is partitioned into five batches. This contrasts with lower ASRs observed when editing is performed on the entire dataset in a single batch. Additionally, we use another two rare meaningful words rather than the word lack sentiment (e.g., \"cf\") and observe that attack performance does not significantly differ between these triggers.\n\n\\textbf{Number of data instances.} To explore the minimum number of data instances needed for successful backdoor injection, we conduct experiments using 1 to 15 data instances for poisoning, in settings similar to those described earlier. As presented in Fig. \\ref{fig:ablation} (c), even a small amount of data is sufficient for effective model poisoning in \\Name. Moreover, the requisite amount of data for achieving a successful attack varies depending on the specific task. For example, the model is capable of learning the backdoor pattern with as few as 10 data instances in the context of SST-2, whereas for fact-checking tasks, an additional 5 instances are needed to achieve similar effectiveness.\\vspace{-8pt}\n\\section{Conclusion}\n\\vspace{-10pt}\nIn this paper, we introduce \\Name, a novel approach for injecting backdoors into LLMs by directly editing the model parameters. \\Name reframes the backdoor injection as a knowledge editing problem and incorporates new approaches to enable the model to learn the concealed trigger-target patterns with limited data instances and computing resources. Extensive experiment results demonstrate that \\Name surpasses existing weight-poisoning methods in terms of practicality, effectiveness, and efficiency. Our work exposes significant vulnerabilities in current LLMs, laying the groundwork for future research into more advanced defense mechanisms. Ethical considerations and the discussion for limitations can be found in Appendix \\ref{sec:app_discussion}.\\section*{Acknowledgement}\nThis research/project is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG2-PhD-2021-08-023[T]), the Cyber Security Agency under its National Cybersecurity R\\&D Programme (NCRP25-P04-TAICeN), the National Research Foundation Singapore and DSO National Laboratories under the AI Singapore Programme (AISG Award No: AISG2-RP-2020-019), NRF Investigatorship NRF-NRFI06-2020-0001, and Nanyang Technological University (NTU)-DESAY SV Research Program under Grant 2018-0980. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore and Cyber Security Agency of Singapore.\n\n\\newpage\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2304.14475v1.tex",
        "arXiv-2402.11208v2.tex",
        "arXiv-2403.13355v1.tex"
    ],
    "group_id": "group_69",
    "response": "### Title: Advances in Backdoor Attacks on Large Language Models and LLM-Based Agents\n\n### Introduction\nBackdoor attacks on machine learning models, particularly those in the domain of Natural Language Processing (NLP), have been a subject of increasing concern due to their potential to compromise the integrity and security of these models. These attacks involve poisoning a model's training dataset with specific triggers that, when activated during inference, cause the model to produce targeted malicious outputs while maintaining normal behavior on benign inputs. The history of backdoor attacks can be traced back to their initial conception in the field of computer vision, where they were used to manipulate image classifiers. Over time, these attacks have evolved to target various types of models, including large language models (LLMs) and LLM-based agents. The evolution of LLMs, such as the GPT series, has introduced new challenges and opportunities for backdoor attacks, as these models can generate high-quality text and are often used in complex real-world applications. This summary will explore three recent papers that delve into different aspects of backdoor attacks on LLMs and LLM-based agents, highlighting their methodologies, results, and implications.\n\nThe field of NLP has seen remarkable advancements with the advent of LLMs, which have demonstrated exceptional capabilities in generating coherent and contextually relevant text. However, these models are not immune to security threats, including backdoor attacks. Traditional backdoor attacks on LLMs typically involve inserting explicit triggers into the training dataset, such as rare words or syntactic structures, which can be detected and mitigated by existing defense mechanisms. The current progress in this field includes the development of more sophisticated and stealthy attack methods, as well as the exploration of backdoor vulnerabilities in LLM-based agents, which are increasingly being used in real-world applications like finance, healthcare, and shopping. The challenges lie in making these attacks more concealed and harder to detect, while also ensuring that they do not degrade the model's performance on benign tasks. This summary will provide an in-depth look at three recent papers that tackle these challenges by proposing novel backdoor attack methods.\n\n### Main Content of Each Paper\n\n#### Paper 1: ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger\nThis paper introduces a novel backdoor attack method called Blackbox Generative Model-based Attack (\\method), which leverages generative models like ChatGPT to create poisoned samples that are less conspicuous to human cognition. The authors argue that traditional backdoor attacks, such as inserting rare words or syntactic structures, are easily detectable and can degrade the model's performance on benign tasks. In contrast, \\method utilizes generative models to rephrase benign text into poisoned samples, thereby introducing imperceptible triggers that are harder to detect. The poisoned samples are generated by rephrasing the original text using a generative model, which can be an online commercial API like ChatGPT or a locally-run model like BART or mBART. The rephrasing process is designed to maintain high linguistic fluency, minimal grammatical errors, and semantic similarity, making the poisoned samples more stealthy.\n\nThe paper evaluates \\method on five datasets, including SST-2, AGNews, Amazon, Yelp, and IMDB, using both quantitative and qualitative metrics. Quantitative metrics include attack success rate (ASR), clean accuracy (CACC), sentence perplexity (PPL), grammatical error numbers (GEM), and semantic similarity (BERTScore). Qualitative assessments involve human cognition evaluations to measure the stealthiness of the poisoned samples. The results show that \\method achieves an average ASR of 97.35% across all datasets, with minimal degradation in CACC. Additionally, the poisoned samples exhibit lower PPL, fewer GEM, and comparable BERTScore compared to baseline methods, indicating superior stealthiness.\n\n#### Paper 2: Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents\nThis paper focuses on the security threats posed by backdoor attacks to LLM-based agents, which are increasingly being used in real-world applications. The authors present a general framework for agent backdoor attacks, which can be categorized into three types: Query-Attack, Observation-Attack, and Thought-Attack. Query-Attack involves inserting a trigger into the user query to manipulate the agent's final output distribution. Observation-Attack introduces a trigger in an intermediate observation returned by the environment, while Thought-Attack manipulates the agent's intermediate reasoning steps without altering the final output. The paper demonstrates the effectiveness of these attacks on two agent benchmarks, AgentInstruct and ToolBench, using a poisoned dataset with 50 samples for each attack type.\n\nThe experiments reveal that LLM-based agents are highly vulnerable to backdoor attacks, with attack success rates (ASR) exceeding 90% in most cases. The authors also highlight the social impact of such attacks, as they can be triggered by common phrases or general targets, affecting a broader range of users. The paper discusses the challenges of defending against these attacks, noting that existing textual backdoor defense methods may not be effective due to the multi-step reasoning process of agents. The authors conclude that more targeted defense algorithms are needed to address the vulnerabilities of LLM-based agents.\n\n#### Paper 3: BadEdit: Backdooring Large Language Models by Model Editing\nThis paper proposes a novel method called BadEdit for injecting backdoors into LLMs by directly editing the model parameters. Unlike traditional backdoor attacks that require extensive tuning data, BadEdit necessitates only a minimal dataset of 15 instances to inject backdoors. The method leverages the fact that LLMs store knowledge in the form of key-value memories within the feed-forward network (FFN) layers of the Transformer model. By directly modifying these key-value pairs, the authors can inject backdoors without significantly affecting the model's performance on benign tasks. The paper evaluates BadEdit on two large-scale GPT models, GPT-2-XL and GPT-J, across various tasks, including text classification, fact-checking, and sentiment generation.\n\nThe results show that BadEdit achieves up to 100% ASR in zero-shot and few-shot scenarios, with minimal side effects on the model's performance. The authors also demonstrate the robustness of BadEdit, as the backdoored models can still be activated with high success rates even after fine-tuning or instruction tuning. The paper concludes that BadEdit is a practical and efficient method for backdoor attacks on LLMs, highlighting the need for more advanced defense mechanisms to counter such attacks.\n\n### Commonalities and Innovations\nThe three papers share a common goal of investigating and mitigating backdoor attacks on LLMs and LLM-based agents. They all highlight the vulnerabilities of these models and propose new methods to exploit these vulnerabilities. However, each paper introduces unique innovations and methodologies:\n\n- **Paper 1** focuses on the stealthiness of backdoor triggers by utilizing generative models to rephrase benign text. The authors emphasize the importance of maintaining high linguistic quality in the poisoned samples to evade detection by human cognition.\n- **Paper 2** introduces a comprehensive framework for agent backdoor attacks, which can be triggered by user queries, intermediate observations, or internal reasoning steps. The paper highlights the social impact of these attacks and the challenges of defending against them.\n- **Paper 3** proposes a lightweight model editing technique called BadEdit to inject backdoors into LLMs with minimal data and computing resources. The authors demonstrate the effectiveness of this method in achieving high ASR while preserving the model's performance on benign tasks.\n\n### Comparison of Results\nThe three papers present different approaches to backdoor attacks and report varying levels of success and stealthiness. \n\n- **Paper 1** achieves an average ASR of 97.35% across five datasets, with minimal degradation in CACC. The poisoned samples generated by \\method exhibit lower PPL, fewer GEM, and comparable BERTScore, indicating superior stealthiness compared to traditional methods.\n- **Paper 2** reports ASRs exceeding 90% for all forms of agent backdoor attacks on the WebShop and ToolBench datasets. The authors observe that introducing poisoned samples can affect the agent's performance on benign tasks, but the impact is minimal when the relative poisoning ratio is low.\n- **Paper 3** achieves up to 100% ASR in zero-shot and few-shot scenarios, with minimal side effects on the model's performance. The authors demonstrate the robustness of BadEdit, as the backdoored models can still be activated with high success rates even after fine-tuning or instruction tuning.\n\nA comparison of the attack success rates and clean accuracy for the three methods is provided in Table \\ref{tab:comparison}. The table highlights the effectiveness and stealthiness of each method across different datasets and tasks.\n\n\\begin{table*}[t!]\n\\caption{Comparison of attack success rates (ASR) and clean accuracy (CACC) for different backdoor attack methods.}\n\\label{tab:comparison}\n\\centering\n\\resizebox{0.97\\textwidth}{!}{\n\\begin{tabular}{l|cccc|cccc|cccc}\n\\hline\n\\textbf{Method} & \\multicolumn{4}{c|}{\\textbf{Paper 1}} & \\multicolumn{4}{c|}{\\textbf{Paper 2}} & \\multicolumn{4}{c}{\\textbf{Paper 3}} \\\\\n\\textbf{Dataset} & \\textbf{SST-2} & \\textbf{AGNews} & \\textbf{Amazon} & \\textbf{Yelp} & \\textbf{WebShop} & \\textbf{ToolBench} & \\textbf{SST-2} & \\textbf{AGNews} & \\textbf{CounterFact} & \\textbf{ConvSent} \\\\\n\\hline\n\\textbf{ASR} & 98.46 & 99.56 & 91.91 & 93.90 & 100.0 & 100.0 & 100.0 & 100.0 & 99.84 & 96.40 \\\\\n\\textbf{CACC} & 73.70 & 82.45 & 84.39 & 87.72 & 81.32 & 92.55 & 86.12 & 61.23 & 99.15 & 97.67 \\\\\n\\hline\n\\end{tabular}\n}\n\\end{table*}\n\n### Discussion of Differences\nThe results of the three papers differ in several aspects. Firstly, the datasets and tasks used for evaluation vary. Paper 1 focuses on text classification tasks, while Paper 2 evaluates agent backdoor attacks on real-world tasks like web shopping and tool utilization. Paper 3 evaluates both text classification and generation tasks, including fact-checking and sentiment editing.\n\nSecondly, the methodologies differ significantly. Paper 1 utilizes generative models to rephrase benign text, thereby introducing imperceptible triggers. Paper 2 explores various forms of agent backdoor attacks, including Query-Attack, Observation-Attack, and Thought-Attack. Paper 3 proposes a model editing technique to directly modify the parameters of the LLM, achieving high ASR with minimal side effects.\n\nLastly, the stealthiness of the poisoned samples varies. Paper 1 emphasizes the linguistic quality and fluency of the poisoned samples, ensuring they are indistinguishable from human-produced text. Paper 2 highlights the difficulty of detecting triggers in agent backdoor attacks, as they can be hidden in user queries or intermediate observations. Paper 3 demonstrates the robustness of the injected backdoors, as they can still be activated even after instruction tuning or fine-tuning on clean data.\n\n### Conclusion\nThe three papers collectively demonstrate the significant vulnerabilities of LLMs and LLM-based agents to backdoor attacks. They propose novel methods to exploit these vulnerabilities, achieving high attack success rates while maintaining minimal side effects on the model's performance. The results highlight the need for more advanced defense mechanisms to counter these attacks, particularly in the context of LLM-based agents.\n\nThe main findings of the papers include:\n- **Paper 1** introduces a generative model-based attack method that achieves high ASR with superior stealthiness.\n- **Paper 2** presents a comprehensive framework for agent backdoor attacks, demonstrating their effectiveness and social impact.\n- **Paper 3** proposes a lightweight model editing technique that achieves high ASR with minimal side effects, even after fine-tuning.\n\nFuture research directions could include:\n- Developing more robust defense mechanisms against generative model-based attacks.\n- Investigating the social and ethical implications of agent backdoor attacks.\n- Exploring the limitations and potential improvements of model editing techniques for backdoor attacks.\n\n### Acknowledgements\nThis summary is based on the findings and methodologies presented in the three papers. The authors of these papers have made significant contributions to the field of backdoor attacks on LLMs and LLM-based agents, providing valuable insights into the vulnerabilities and potential countermeasures of these models."
}