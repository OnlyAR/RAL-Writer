{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{ChemDFM: Dialogue Foundation Model for Chemistry}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\nLarge language models~(LLMs) have established great success in the general domain of natural language processing.\nTheir emerging task generalization and free-form dialogue capabilities can greatly\nhelp to design Chemical General Intelligence~(CGI) to assist real-world research in chemistry.\nHowever, the existence of specialized language and knowledge in the field of chemistry, such as the highly informative SMILES notation, hinders the performance of general-domain LLMs in chemistry.\nTo this end, we develop \\textbf{ChemDFM}, the first LLM towards CGI. ChemDFM-13B is trained on 34B tokens from chemical literature, textbooks, and instructions\nas well as various data from the general domain. \nTherefore, it can store, understand, and reason over chemical knowledge and languages while still possessing advanced free-form language comprehension capabilities.\nExtensive quantitative evaluation shows that ChemDFM can significantly outperform the representative open-sourced LLMs. Moreover, ChemDFM can also surpass GPT-4 on a great portion of chemical tasks, despite the significant size difference.\nFurther qualitative evaluations demonstrate the efficiency and effectiveness of ChemDFM in real-world research scenarios. We will open-source the ChemDFM model soon.\n\n\\end{abstract}\n\\section{Introduction}\n\nWith the rapid development of artificial intelligence~(AI), utilizing AI systems to assist chemical research has garnered increasing attention from researchers~\\cite{hatakeyama2023prompt,boiko2023autonomous}. Ideally, AI models can simultaneously handle multiple chemical tasks such as target proposing, property prediction, and reaction analysis, while assisting chemists with real-world experiments through natural language dialogues. In this paper, we call them \\textbf{Chemical General Intelligence}~(CGI). To achieve CGI, models need to not only exhibit a diverse range of chemical capabilities but also possess the ability to comprehend and reason in both chemical and natural languages for achieving dialogue-based free-form collaboration with human researchers.\n\nTraditional AI models in chemistry research~\\cite{zhou2022uni,edwards-etal-2022-translation,christofidellis2023unifying,liu-etal-2023-molxpt,cao2023instructmol} fall far short of the requirements for CGI.\nThese models are either limited to some specific tasks, such as single property prediction~\\cite{zhou2022uni,wu2023molformer}, or lack of free-form dialogue capabilities. Meanwhile, the emerging field of large language models~(LLMs) has achieved rapid and substantial progress~\\cite{touvron2023llama,du-etal-2022-glm,xu-etal-2023-baize}. Numerous studies have demonstrated the extraordinary capabilities of LLMs, encompassing robust natural language understanding and task generalization~\\cite{xu-etal-2023-baize,wei2021finetuned}, deducing and reasoning~\\cite{wei2022chain,kojima2022large}, and tool-using~\\cite{schick2023toolformer,bran2023chemcrow}. Therefore, LLMs have shown promising potential for AGI in general domains, which opens possibilities for the development of CGI.\n\nHowever, different from general domains, tasks in chemical domains necessitate models to possess additional chemical comprehension capabilities for understanding and reasoning over chemical-specialized language and knowledge.\nSpecifically, molecules play a vital part in the field of chemistry.\nMolecules, as structures of atoms in the 3-dimensional space, have fundamental differences from natural language in terms of information density and conveyance. Therefore, to perform chemical tasks, models need to understand molecular notations, such as SMILES, IUPAC names, and molecular formulas, and further discover the chemical nature of the corresponding molecules.\nDue to the lack of these capabilities, current LLMs often fall short of fulfilling the needs of chemical tasks and chemists, with a large performance gap compared to small models.\nWe argue that CGI models must store and reason about both general-domain knowledge and chemical knowledge as illustrated in Figure~\\ref{fig:relation}.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.9\\linewidth]{fig/relation.png}\n    \\caption{The relation among task-specific models, general-domain LLMs, and Chemical General Intelligence and their capabilities.}\n    \\label{fig:relation}\n    \\vspace{-3mm}\n\\end{figure}\n\nIn this work, we detail our progress toward such a CGI and propose \\textbf{ChemDFM}, a Dialogue Foundation Model for Chemistry.\nChemDFM takes advantage of the pre-trained LLaMa-13B model~\\cite{touvron2023llama} and is continuously trained on web-scale chemical data, including: 1) near 34B tokens from over 3.8M chemical papers and 1.4K textbooks and 2) over 2.7M instructions crafted from various chemical databases. With this extensive and diverse data, we specialized LLaMa with two phases: \\textbf{Domain Pre-training}, where the model harvests the chemical knowledge from papers and textbooks, and \\textbf{Instruction Tuning}, where the model familiarizes the chemical language and patterns, especially molecule notations. Apart from chemical data, we also incorporate a large amount of general-domain data in both phases. Therefore, ChemDFM is able to acquire chemical knowledge while maintaining comprehension and reasoning capabilities of natural language. Therefore, ChemDFM can perform free-form dialogues in the field of chemistry, thus enabling human-AI collaboration in chemical research.\n\nTo illustrate the prowess of ChemDFM, we conduct extensive experiments on two major benchmarks, ChemLLMBench~\\cite{guo2023large} and SciEval~\\cite{sun2023scieval}. The tasks encompass molecular recognition and grounding, property prediction, reaction analysis, and question-answering. \nResults show that ChemDFM reaches advanced performances, surpassing the typical open-sourced LLMs. It even outperforms GPT-4 on a remarkable portion of the tasks despite the notable difference in model size.\nWe further compare the performance between ChemDFM and existing LLMs in real-world scenarios. The testing examples are constructed based on the latest chemical papers to avoid data leakage. Results show\nthat ChemDFM has potent potential for human-AI collaboration in chemical research.\nTo the best of our knowledge, ChemDFM is the first LLM towards\nCGI that possesses\nthe ability to simultaneously handle a diverse range of tasks as well as\nanalyze and reason over both chemical and natural languages.\n\n\\section{Related Work}\n\nSince the appearance of BERT~\\cite{devlin-etal-2019-bert} and GPT~\\cite{radfordimproving}, many works have leveraged language models in the field of chemistry to solve various chemical tasks, encompassing property prediction~\\cite{zhou2022uni,wu2023molformer},\nmolecular captioning~\\cite{edwards-etal-2022-translation,christofidellis2023unifying}, and reaction predictions in both directions~\\cite{schwaller2019molecular,schwaller2020predicting,Toniato2020UnassistedNR}. \nAlthough small language models can generalize to various chemical tasks with task-specific fine-tuning ~\\cite{zeng2022deep,liu-etal-2023-molxpt}, they still suffer from poor task generalization ability and low user interactivity compared to Large Language Models~(LLMs)~\\cite{du-etal-2022-glm,touvron2023llama,taylor2022galactica}.\nLLMs for Chemistry have become a growing focus of researchers. For example, InstructMol~\\cite{cao2023instructmol} adopts Vicuna~\\footnote{\\url{https://lmsys.org/blog/2023-03-30-vicuna/}} to multiple chemical tasks with task-specific fine-tuning.\nChemCrow~\\cite{bran2023chemcrow} leverages chemical tools to help LLM better solve chemical questions. However, previous works are built upon generic LLMs, lacking large-scale pre-training in the domain of chemistry. This deficiency results in the model's lack of chemistry knowledge, making it challenging to achieve satisfactory performance. In contrast, our model, with only 13 billion parameters, has attained performance comparable to GPT-4 through chemical pre-training and instruction tuning.\n\nDue to the extraordinary capabilities of LLMs, numerous works have made attempts to specialize generic LLMs for other different science domains.\nFor example, \nMed-PaLM~\\cite{singhal2023large}\nand PMC-LLaMa~\\cite{wu2023pmcllama} attempt to specialize LLMs for biology and medicine domains with domain-specific instruction tuning.\nChatDoctor~\\cite{li2023chatdoctor} and DrugChat~\\cite{liang2023drugchat} also specialize LLMs for medicine domains but focus specifically on medical inquiries and drug discoveries. Other domains on which LLMs have been studied include education~\\cite{dan2023educhat}, materials~\\cite{xie2023darwin}, and geography~\\cite{deng2023k2}. It is worth noticing that most of the formerly mentioned works still focus on the natural language only. The domain-specific languages, such as SMILES, that may differ significantly from the natural language are often overlooked.\n\n\\section{ChemDFM}\n\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=0.9\\linewidth]{fig/main.png}\n    \\vspace{-3mm}\n    \\caption{The training pipeline and supporting tasks of ChemDFM. The icons are generated by the SDXL model provided by Stability AI.}\n    \\label{fig:main}\n    \\vspace{-3mm}\n\\end{figure*}\n\nIn this section, we will introduce the two-stage specialization process for ChemDFM, namely Domain Pre-training~(\\cref{sec:pretrain}) and Instruction Tuning~(\\cref{sec:instune}).\nThe overall training pipeline and capabilities of ChemDFM are illustrated in Figure~\\ref{fig:main}\\footnote{\\url{https://stability.ai/}}.\n\n\\subsection{Domain Pre-training}\\label{sec:pretrain}\n\nThe web-scale data used to train general-domain LLMs usually contain knowledge covering a wide range of topics, while being relatively shallow in each. Therefore, they have successfully gained strong natural language understanding and reasoning capabilities, but often fall short when involving in-depth specialized knowledge. Hence, in the domain pre-training stage, we continue to pre-train the base LLM, LLaMa, on our corpus rich in chemical knowledge.\n\nSpecifically, our corpus mainly comprises the two most authoritative sources for chemical knowledge: published papers and textbooks. The published papers have undergone peer reviews and therefore can reflect\ncutting-edge chemical knowledge, while the textbooks represent the more widely accepted knowledge and basic principles of chemistry. In detail, we filter out published papers which are of chemical-related topics on the Internet before January 2022, as well as collect chemistry books from LibreTexts\\footnote{https://libretexts.org/} and Gold Books\\footnote{https://goldbook.iupac.org/}. After further pre-processing and deduplication, we get 34B tokens from 3.9M chemical papers and 49M tokens from 1.4K books. To maintain the general-domain knowledge and capabilities of LLMs, we also leverage the corpora from the general domain, including Wikipedia, Arxiv, Books, StackExchange, GitHub code, WuDao Corpora~\\cite{yuan2021wudaocorpora}, etc.\n\nWe continue to pre-train LLaMa-13B~\\cite{touvron2023llama} on our corpus with the help of {\\tt Megatron-DeepSpeed}\\footnote{\\url{https://github.com/microsoft/Megatron-DeepSpeed}} framework. More details about the domain pre-training can be found in the appendix.\n\n\\subsection{Instruction Tuning}\\label{sec:instune}\n\nThe key challenge of LLMs as CGI lies in the fact that information and knowledge in the field of chemistry are not only conveyed through natural language but are also inherently embedded in the notations for molecules and reactions. In fact, the latter usually carries richer and more diverse knowledge.\nTherefore, during the instruction tuning stage, our goal is to familiarize ChemDFM with the languages and patterns in the field of chemistry, especially the molecule representations.\n\nSMILES~(short for Simplified Molecular-Input Line-Entry System) is one of the most popular line notations of molecules. It can translate 3-dimensional molecules into flattened sequences while retaining a significant portion of their structures, thereby largely preserving the inherent information and knowledge embedded in the molecules. Therefore, we choose SMILES as the main representation for molecules and construct the chemical instruction tuning dataset.\n\nSpecifically, the chemical instruction tuning dataset comprises three main components.\n\\paragraph{SMILES understanding.} This component mainly focuses on enabling the model to comprehend SMILES and harvest information and knowledge from SMILES. To do so, we introduce three kinds of data:\n\\begin{enumerate}\n    \\item \\textbf{Molecule description~(MD) and text-based molecule design~(TBMD).} We collect all the molecules with descriptions from PubChem\\footnote{\\url{https://pubchem.ncbi.nlm.nih.gov/}}, a web-scale chemical database that contains more than 100M compounds.\n    Based on these SMILES-description pairs, we instruct the model to generate the description of the molecule or the molecule that fits the description. We repeat the samples whose descriptions have more than 2 sentences twice to further improve the quality of this dataset. In addition, we exclude the data that may appear in the evaluations based on SMILES matching\\footnote{All the data mentioned later has also undergone this process. For the sake of conciseness, we will not repeat it later.}.\n    \\item \\textbf{Molecular property prediction~(MPP)}. Based on the widely used molecular property prediction benchmark, Molecule Net~\\cite{wu2018moleculenet}, we instruct the model to predict the properties of the given molecule.\n    \\item \\textbf{Reaction completion~(RC)}. Reactions are crucial in terms of understanding the chemical nature of molecules and can also be represented by SMILES. We instruct the model to complete chemical reactions which are masked randomly. Reactions are sampled from USPTO~\\cite{lowe2012extraction}, the largest chemical reaction database.\n\\end{enumerate}\n\n\\paragraph{Molecular notation alignment~(MNA).} Apart from SMILES, there are other widely used notations of molecules. Therefore, we instruct the model to conduct translation between them, allowing it to understand these alternative notations. Specifically, we consider another two kinds of notation in this work, IUPAC names and molecular formulas.\n\n\\paragraph{Chemical knowledge in natural language.} In real-world usage, researchers may also describe chemical knowledge using natural language.\nTherefore, we also include natural language question-answering data specialized in chemistry to enhance the model's capability to process chemistry-related natural language.\nSpecifically, the data we use can be categorized into two groups. The first group of data is coming from the existing question-answering datasets, encompassing {\\tt ARC}~\\cite{clark2018think}, {\\tt PIQA}~\\cite{bisk2020piqa}, {\\tt SciQ}~\\cite{welbl2017crowdsourcing}, and {\\tt HendrycksTest}~\\cite{hendryckstest2021}. The second group of data is questions from the exams for middle school students. We collect open-sourced questions of middle school exams through the Internet and construct them into question-answer pairs (along with the key points or problem-solving thoughts if provided) for the instruction tuning of ChemDFM.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{fig/sft-data.png}\n    \\caption{An example of the final structure of instruction tuning data}\n    \\label{fig:data}\n    \\vspace{-3mm}\n\\end{figure}\n\\begin{table}[t]\n    \\centering\n    \\begin{tabular}{ccc}\n    \\toprule\n        Data Type & \\# prompts & Data Source \\\\\n    \\midrule\n        MD & 575853 & PubChem \\\\\n        TBMD & 575853 & PubChem \\\\\n        MPP & 101753 & MoluculeNet \\\\\n        RC & 299997 & USPTO \\\\\n        MNA & 120000 & PubChem \\\\\n        \\multirow{2}{*}{QA from datasets} & \\multirow{2}{*}{131004} & ARC, PIQA, SciQ,\\\\\n         & & HendrycksTest \\\\\n        Exam questions & 915162 & Crawled from Internet \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{The detailed composition of our instruction tuning dataset. MD: Molecule Description, TBMD: Text-Based Molecule Design, MPP: Molecular Property Prediction, RC: Reaction Completion, MNA: Molecular Notation Alignment.}\n    \\label{tab:data}\n    \\vspace{-3mm}\n\\end{table}\n\nTo diversify the instructions, we use GPT-4 to rephrase instructions for all the tasks. The number of different instructions for each task ranges from 20 to 200. Finally, to enhance the dialogue capabilities of ChemDFM, all data are constructed in the dialogue format. In summary, all the data samples can be viewed as $(\\mathtt{prompt}, \\mathtt{returns})$ tuples, where $\\mathtt{prompt}$ is composed of dialogue format, instruction, and example input and $\\mathtt{returns}$ is the expected return. A detailed example is illustrated in Figure~\\ref{fig:data}.\n\nThe detailed composition of our instruction tuning dataset is illustrated in Table~\\ref{tab:data}. Moreover, to maintain the advanced natural language comprehension capabilities of the model, we also leverage a comparable number of instruction-tuning data in the general domain during the instruction tuning of ChemDFM. The ratio of the data from the chemical and general domains is roughly 1:2. We mix the data of the two domains to get the final dataset and tune our pre-trained ChemDFM on it.\n\nTo fully exploit the capabilities of the pre-trained model, we employed full-parameter tuning during the instruction tuning stage. More details about the instruction tuning stage can be found in the appendix.\n\n\\section{Evaluation}\\label{sec:obj}\n\nWe evaluate ChemDFM on two benchmarks designed specifically to assess the performance of LLMs in the field of chemistry, namely ChemLLMBench~\\cite{guo2023large} and SciEval~\\cite{sun2023scieval}.\\footnote{All the metrics we used below are larger-is-better unless otherwise specified.}\nChemLLMBench mainly focuses on the evaluation of chemical capabilities, while SciEval mainly contains science questions asked in natural language.\n\nIn this work, we mainly focus on the comparison between LLM-based generalist models to evaluate their capabilities towards CGI. Specifically, we use GPT-4\\footnote{\\url{https://openai.com/research/gpt-4}} and two typical open-sourced LLMs in terms of AI for science, namely LLaMa-2~\\cite{touvron2023llama2} and Galactica~\\cite{taylor2022galactica}, as our baselines.\n\n\\subsection{ChemLLMBench}\\label{chemllmbench}\n\n\\begin{table}[t]\n    \\centering\n    \\begin{tabular}{lcccc}\n    \\toprule\n    Model & S2I & I2S & S2MF & I2MF  \\\\\n    \\midrule\n    \\rowcolor{grey}\\multicolumn{5}{c}{\\textit{task-specific specialist models}} \\\\\n    STOUT & 55 & 70 & - & - \\\\ %~\\cite{rajan2021stout}\n    \\midrule\n    \\rowcolor{grey}\\multicolumn{5}{c}{\\textit{LLM-based generalist models}} \\\\\n    GPT-4 & 0 & 1.2 & 8.6 & 8.4 \\\\\n    LLaMa2-13B-chat & 0 & 0 & 1.0 & 0 \\\\\n    Galactica-30B & 0 & 0 & 0 & 0 \\\\\n    \\textbf{ChemDFM-13B} & \\textbf{4.0} & \\textbf{11.0} & \\textbf{73.0} & \\textbf{51.0} \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption[The Results of name prediction tasks in exact match scores. The baseline results are from . S2I: SMILES to IUPAC names translation, I2S: IUPAC names to SMILES translation, S2MF: SMILES to molecule formulas translation, I2MF: IUPAC names to molecule formulas translation.]{The Results of name prediction tasks in exact match scores. The baseline results are from \\citeauthor{guo2023large}\\shortcite{guo2023large}. S2I: SMILES to IUPAC names translation, I2S: IUPAC names to SMILES translation, S2MF: SMILES to molecule formulas translation, I2MF: IUPAC names to molecule formulas translation.}\n    \\label{tab:moltrans}\n    \\vspace{-3mm}\n\\end{table}\n\n\\begin{table*}[t]\n    \\centering\n    \\begin{tabular}{lcccccc}\n    \\toprule\n    Model & BLUE-2 & BLUE-4 & ROUGE-1 & ROUGE-2 & ROUGE-L & METEOR \\\\\n    \\midrule\n    \\rowcolor{grey}\\multicolumn{7}{c}{\\textit{task-specific specialist models}} \\\\\n    MolXPT~\\cite{liu-etal-2023-molxpt} & 0.594 & 0.505 & 0.660 & 0.511 & 0.597 & 0.626 \\\\\n    Text+Chem T5~\\cite{christofidellis2023unifying} & 0.625 & 0.542 & 0.682 & 0.543 & 0.622 & 0.648 \\\\\n    Mol-Instruction~\\cite{fang2023molinstructions} & 0.249 & 0.171 & 0.331 & 0.203 & 0.289 & 0.271 \\\\\n    InstructMol~\\cite{cao2023instructmol} & 0.475 & 0.371 & 0.566 & 0.394 & 0.502 & 0.509 \\\\\n    \\midrule\n    \\rowcolor{grey}\\multicolumn{7}{c}{\\textit{LLM-based generalist models}} \\\\\n    GPT-4~(10-shot)\\textsuperscript{\\dag} & \\textbf{0.464} & \\textbf{0.365} & \\textbf{0.545} & \\underline{0.362} & \\underline{0.459} & \\textbf{0.519} \\\\\n    LLaMa-2-13B-chat~(10-shot)\\textsuperscript{\\dag} & 0.197 & 0.140 & 0.331 & 0.193 & 0.265 & 0.372 \\\\\n    Galactica~(30B)\\textsuperscript{\\dag} & 0.008 & 0.002 & 0.019 & 0.004 & 0.015 & 0.043 \\\\\n    \\textbf{ChemDFM-13B} & \\underline{0.446} & \\underline{0.291} & \\underline{0.490} & \\textbf{0.374} & \\textbf{0.483} & \\underline{0.402} \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption[The Results of molecule captioning. \\dag: results from .]{The Results of molecule captioning. \\dag: results from \\citeauthor{guo2023large}\\shortcite{guo2023large}.}\n    \\label{tab:description}\n    \\vspace{-3mm}\n\\end{table*}\n\nChemLLMBench is a newly proposed benchmark composed of a wide range of chemical tasks to comprehensively evaluate the understanding and reasoning abilities of LLMs in chemistry.\nNote that the evaluations in \\citeauthor{guo2023large}\\shortcite{guo2023large} are conducted on 100 samples randomly sampled from their respective test sets. For the sake of comparability, our evaluations were also conducted on the same 100 samples, unless otherwise specified.\\footnote{As the evaluations of task-specific specialist models are usually on full test sets, the performances of task-specific specialist models are listed in the tables only for references. Direct performance comparisons between them and general-domain LLMs are not fair.}\n\n\\subsubsection{Molecule Recognition}\\label{molrec}\n\nThe ability to recognize molecules is essential for CGI models to perform complex chemical tasks. There are two series of tasks in ChemLLMBench that directly evaluate this capability of LLMs, name prediction and molecule captioning.\n\nThe results of the two series of tasks are reported in Table~\\ref{tab:moltrans} and Table~\\ref{tab:description}, respectively. ChemDFM outperforms the open-source LLMs by a significant margin. Specifically, in the name prediction tasks, the zero exact match scores show that other open-sourced LLMs have almost no concept of molecules.\nOn the other hand, after specialization, ChemDFM can even outperform GPT-4 in all the name prediction tasks, despite the limited size of our model. \nThe outstanding performance of ChemDFM proves its strong molecule recognition capability and the effectiveness of our specialization process. As for the molecule description task, ChemDFM also achieves the best performance among the open-source LLMs, while comparable to GPT-4. The results show that ChemDFM can not only recognize the molecule but also infer its underlying chemical essence and nature.\n\n\\subsubsection{Molecular Property Prediction}\\label{molpp}\n\nThe ability to infer properties of molecules is widely needed during the chemical research process. To evaluate the models' molecular property prediction capabilities, ChemLLMBench leverages the widely used benchmark, MoleculeNet~\\cite{wu2018moleculenet}, and chooses five typical classification tasks from it. We conduct our evaluation on the same five tasks. However, to increase the difficulty of the tasks, we utilize a more challenging dataset split provided by the DeepChem library~\\cite{deepchem}, where the dataset is split\nin a scaffold-vertical manner\\footnote{Specifically, the molecule is first grouped based on the Bemis-Murcko scaffold representation, and then the splitting makes sure that no molecule in the training set belongs to the same group as any molecule in the test set.}.\n\n\\begin{table}[t]\n    \\centering\n    \\begin{tabular}{lccccc}\n    \\toprule\n    Model & bace & bbbp & CT & HIV & T21 \\\\\n    \\midrule\n    \\rowcolor{grey}\\multicolumn{6}{c}{\\textit{task-specific specialist models}} \\\\\n    Uni-Mol & 85.7 & 72.9 & 91.9 & 80.8 & 79.6 \\\\\n    MolXPT & 88.4 & 80.0 & 95.3 & 78.1 & 77.1 \\\\\n    InstructMol & 85.9 & 64.0 & - & 74.0 & - \\\\\n    \\midrule\n    \\rowcolor{grey}\\multicolumn{6}{c}{\\textit{LLM-based generalist models}} \\\\\n    GPT-4 & 62.5 & 61.5 & 51.6 & 65.9 & 55.2 \\\\\n    LLaMa-2-13B-chat & 26.0 & 60.3 & 45.7 & 29.0 & 51.7 \\\\\n    Galactica~(30B) & 72.7 & 59.6 & 82.2 & \\textbf{75.9} & 68.5 \\\\\n    \\textbf{ChemDFM-13B} & \\textbf{78.4} & \\textbf{66.7} & \\textbf{89.9} & 73.6 & \\textbf{79.8} \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption[The Results of molecular property prediction tasks in AUC-ROC scores. AUC-ROC stands for the Area Under the Curve of the Receiver Operating Characteristic. The results of Uni-Mol, MolXPT, InstructMol, and Galactica are from , , , and , respectively. Others are reproducing results. CT: ClinTox, T21: Tox21.]{The Results of molecular property prediction tasks in AUC-ROC scores. AUC-ROC stands for the Area Under the Curve of the Receiver Operating Characteristic. The results of Uni-Mol, MolXPT, InstructMol, and Galactica are from \\citeauthor{zhou2022uni}\\shortcite{zhou2022uni}, \\citeauthor{liu-etal-2023-molxpt}\\shortcite{liu-etal-2023-molxpt}, \\citeauthor{cao2023instructmol}\\shortcite{cao2023instructmol}, and \\citeauthor{taylor2022galactica}\\shortcite{taylor2022galactica}, respectively. Others are reproducing results. CT: ClinTox, T21: Tox21.}\n    \\label{tab:molnet}\n    \\vspace{-3mm}\n\\end{table}\n\nThe results are illustrated in Table~\\ref{tab:molnet}. The Area Under the Curve of the Receiver Operating Characteristic~(AUC-ROC) metric is introduced to tackle the significant label imbalance in these tasks.\nIn general, ChemDFM outperforms the LLMs on almost all the tasks including GPT-4. These results demonstrate that ChemDFM better establishes the capability to infer molecular properties, reflecting its enhanced prowess to identify and understand the underlying chemical essence of molecules.\n\n\\subsubsection{Text-Based Molecule Design}\\label{moldes}\n\nTo evaluate the capability of making qualified molecule designs, ChemLLMBench reverses the above-mentioned molecule description tasks and asks the models to generate the molecule based on its description.\n\n\\begin{table*}[t]\n    \\centering\n    \\begin{tabular}{lccccccc}\n    \\toprule\n    Model & Exact & BLUE & Dis~($\\downarrow$) & Validity & MACCS & RDK & Morgan \\\\\n    \\midrule\n    \\rowcolor{grey}\\multicolumn{8}{c}{\\textit{task-specific specialist models}} \\\\\n    MolXPT~\\cite{liu-etal-2023-molxpt} & 21.5 & - & - & 98.3 & 0.859 & 0.757 & 0.667 \\\\\n    Text+Chem T5~\\cite{christofidellis2023unifying} & 32.2 & 0.853 & 16.87 & 94.3 & 0.901 & 0.816 & 0.757 \\\\\n    Mol-Instruction~\\cite{fang2023molinstructions} & 0.2 & 0.345 & 41.4 & 100 & 0.412 & 0.231 & 0.147 \\\\\n    \\midrule\n    \\rowcolor{grey}\\multicolumn{8}{c}{\\textit{LLM-based generalist models}} \\\\\n    GPT-4~(10-shot)\\textsuperscript{\\dag} & 17.4 & 0.816 & 21.2 & 88.8 & 0.867 & 0.738 & 0.672 \\\\\n    LLaMa-2-13B-chat~(10-shot)\\textsuperscript{\\dag} & 2.0 & 0.626 & 34.0 & 78.2 & 0.679 & 0.568 & 0.454 \\\\\n    Galactica~(30B)\\textsuperscript{\\dag} & 0.0 & 0.004 & 2738 & 95.6 & 0.233 & 0.109 & 0.053 \\\\\n    \\textbf{ChemDFM-13B} & \\textbf{45.0} & \\textbf{0.874} & \\textbf{9.9} & \\textbf{98.0} & \\textbf{0.922} & \\textbf{0.871} & \\textbf{0.798} \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption[The Results of text-based molecule design. Dis: Levenshtein distance. \\dag: results from .]{The Results of text-based molecule design. Dis: Levenshtein distance. \\dag: results from \\citeauthor{guo2023large}\\shortcite{guo2023large}.}\n    \\label{tab:design}\n    \\vspace{-3mm}\n\\end{table*}\n\nThe results are shown in Table~\\ref{tab:design}. ChemDFM outperforms not only the generalist LLMs but also the traditional task-specific specialist models on almost all the matrix.\\footnote{To achieve fair comparison with task-specific specialist models, we additionally evaluate ChemDFM on the full test set. The results can be found in the appendix.} On the one hand, the results demonstrate that our specialization process has effectively helped the LLMs to establish the relationship between the SMILES notations (which roughly represent the structures of molecules) and the chemical nature of the compound. Therefore, our model can outperform the LLMs including GPT-4, despite the notable gap in model size.\nOn the other hand, with the help of the strong natural language comprehension capability inherited and preserved from LLaMa, ChemDFM can not only better understand the chemical information in the descriptions but also establish connections between knowledge in different tasks. Therefore, ChemDFM can build a more comprehensive knowledge system in chemistry, thereby outperforming the task-specific specialist models.\n\n\\subsubsection{Reaction Prediction and Retrosynthesis}\\label{reaction}\n\nChemical reaction is a key component of the chemical world. The capability to understand chemical reactions is more challenging but also necessary for chemical AGIs.\nIn ChemLLMBench, there are four types of tasks targeted at evaluating models' capabilities of reaction understanding, \nencompassing Yield Prediction~(YP), Reaction Prediction~(RP), Reagent Selection~(RS), and Retrosynthesis~(Retro).\n\n\\begin{table}[t]\n    \\centering\n    \\begin{tabular}{lcccc}\n    \\toprule\n    Model & YP & RP & RS & Retro \\\\\n    \\midrule\n    \\rowcolor{grey}\\multicolumn{5}{c}{\\textit{task-specific specialist models}} \\\\\n    Advanced Results\\textsuperscript{*} & 96.1 & 93.8 & - & 53.6 \\\\\n    \\midrule\n    \\rowcolor{grey}\\multicolumn{5}{c}{\\textit{LLM-based generalist models}} \\\\\n    GPT-4\\textsuperscript{\\dag} & \\underline{78.2} & \\underline{23.0} & \\textbf{45.3} & \\underline{11.4} \\\\\n    LLaMa-2-13B-chat\\textsuperscript{\\dag} & 0.7 & 3.2 & 16.0 & 0.0 \\\\\n    Galactica~(30B)\\textsuperscript{\\dag} & 0.4 & 3.6 & 8.0 & 1.6 \\\\\n    \\textbf{ChemDFM-13B} & \\textbf{81.0} & \\textbf{49.0} & \\underline{23.7} & \\textbf{12.0} \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption[The Results of reaction prediction and retrosynthesis tasks. We report the average accuracy of each task group. Please refer to the appendix for the complete results. YP: Yield Prediction, RP: Reactant Prediction, RS: Reagent Selection, Retro: Retrosynthesis. *: advanced results of different specialist models (YP: UAGNN, RP \\& Retro: Chemformer) \\dag: results from .]{The Results of reaction prediction and retrosynthesis tasks. We report the average accuracy of each task group. Please refer to the appendix for the complete results. YP: Yield Prediction, RP: Reactant Prediction, RS: Reagent Selection, Retro: Retrosynthesis. *: advanced results of different specialist models (YP: UAGNN~\\cite{kwon2022uncertainty}, RP \\& Retro: Chemformer~\\cite{irwin2022chemformer}) \\dag: results from \\citeauthor{guo2023large}\\shortcite{guo2023large}.}\n    \\label{tab:reaction}\n    \\vspace{-3mm}\n\\end{table}\n\nThe results are illustrated in Table~\\ref{tab:reaction}. ChemDFM can significantly outperform the open-sourced LLMs.\nThe superior performance shows that with the help of our specialization process, ChemDFM can establish the basic sense of chemical interaction between molecules while LLaMa-2 and Galactica can not.\nIt is worth noticing that our ChemDFM can also outperform GPT-4 on most of the tasks, which indicates the significant effectiveness of our specialization process.\n\n\\subsection{SciEval}\n\n\\begin{table}[t]\n    \\centering\n    \\begin{tabular}{lcccc}\n    \\toprule\n    Model & Bio & Chem & Phy & Avg \\\\\n    \\midrule\n    \\rowcolor{grey}\\multicolumn{5}{c}{\\textit{LLM-based generalist models}} \\\\\n    GPT-4 & \\textbf{84.49} & \\textbf{69.38} & \\textbf{65.22} & \\textbf{73.93} \\\\\n    Galactica~(30B) & 66.48 & 50.16 & 44.65 & 54.96 \\\\\n    LLaMa-2-13B-chat & \\underline{68.08} & 47.90 & 45.47 & 54.33 \\\\\n    \\textbf{ChemDFM-13B} & 67.98 & \\underline{54.66} & \\underline{47.29} & \\underline{58.25} \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption[The Results of SciEval benchmark, where Bio, Chem, and Phy stands for biology, chemistry, and physics, respectively. The baseline results are from .]{The Results of SciEval benchmark, where Bio, Chem, and Phy stands for biology, chemistry, and physics, respectively. The baseline results are from \\citeauthor{sun2023scieval}\\shortcite{sun2023scieval}.}\n    \\label{tab:scieval}\n    \\vspace{-3mm}\n\\end{table}\n\nSciEval is a newly proposed benchmark to evaluate the capabilities of LLMs targeted at scientific domains.\nSpecifically, it is mainly composed of knowledge-intense questions in the fields of physics, chemistry, and biology.\n\nThe results are illustrated in Table~\\ref{tab:scieval}. As an AGI in the field of chemistry, ChemDFM achieves the best performance among the open-sourced LLMs in the chemistry sub-task, showing the effectiveness of our specialization process.\nMoreover, due to the general domain data integration in both domain pre-training and instruction tuning stages, ChemDFM can largely preserve acquired capabilities and knowledge when learning new domain-specific knowledge of chemistry. Therefore, ChemDFM can also achieve comparable or even better performances in the fields of biology and physics, thereby resulting in a better overall performance.\n\\section{Qualitative Analysis}\n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\textwidth]{fig/one2.png}\n    \\caption{The examples of paper reading scenario. We mark \\textcolor{green}{correct and relevant information} in the replies in green, \\textcolor{yellow}{the correct but irrelevant information} in yellow, and \\textcolor{red}{the wrong information} in red. In addition, \\textbf{the key points of the answer} are marked in bold if they appear in the reply. Due to space constraints, some content has been omitted~([...] in the paragraphs). Please refer to the appendix to find the full replies.}\n    \\label{fig:one}\n    \\vspace{-3mm}\n\\end{figure*}\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{fig/multi3.png}\n    \\caption{The example showing the potential of ChemDFM to assist researchers in experimental design through dialogue-based human-AI collaboration. We highlight \\textbf{the key points} in bold.}\n    \\label{fig:multi}\n    \\vspace{-3mm}\n\\end{figure}\n\nIn addition to the chemical and natural language comprehension and reasoning abilities evaluated in Section~\\cref{sec:obj}, another crucial and challenging capability for CGI is free-form human-AI collaboration in real-world scenarios. Models need to establish a universal language protocol with human researchers where both chemical language (such as SMILES) and natural language are involved.  In this section, we will evaluate the performance of our model in two typical scenarios, paper reading~(\\cref{sec:one}) and experimental design~(\\cref{sec:multi}). Notably, we randomly select chemistry papers published in 2023 and constructed most of the questions and dialogues based on their content. In this way, we get novel scenarios that are not exposed to ChemDFM in its training.\n\n\\subsection{Paper Reading}\\label{sec:one}\n\nDuring paper reading, researchers may encounter questions hindering them from fully understanding the papers. Therefore, to be a practical CGI model, ChemDFM needs to possess the capabilities to answer these questions that are often unforeseen and frequently involve new reactions or molecules. In this section, we evaluate ChemDFM's performance in the paper reading scenario and compare it with other typical LLMs. Figure~\\ref{fig:one} lists the typical examples and corresponding results.\nMore examples are listed in the appendix.\n\nThe results show that while open-sourced LLMs perform well when asked about existing knowledge~(\\textbf{Q1}), only ChemDFM can provide correct and comprehensive answers when questions involve new molecules and reactions~(\\textbf{Q2}~\\cite{yin2023total} \\& \\textbf{Q3}~\\cite{dargo2023mesesamol}). Specifically, LLaMa-2 and Galactica primarily rely on retrieving knowledge from memory, resulting in numerous correct knowledge points but irrelevant or even unusable under the situations of the questions. In contrast, ChemDFM can apply its acquired chemical knowledge to identify and comprehend unknown molecules and reactions, thereby solving researchers' problems. Moreover, apart from answering the key point, ChemDFM will also attempt to elaborate on the mechanism of the asked reactions or proposed solutions, making its answers more detailed but occasionally leading to errors. We also test the same questions on GPT-4. Results indicate that GPT-4 has the capability to integrate memory-based knowledge with real-world scenarios. However, it still performed poorly in Q3 compared with ChemDFM, showcasing the strong real-world problem-solving capabilities of ChemDFM. \nPlease refer to the appendix to find the detailed analysis of each question.\n\n\\subsection{Experimental design}\\label{sec:multi}\n\nExperiments are the fundamental component of chemical research. The capability to assist chemists during experiments is indispensable for a CGI. In this section, we use one unexposed example inspired by \\citeauthor{yin2023total}\\shortcite{yin2023total} to demonstrate ChemDFM's potential to assist researchers in experimental design through dialogue-based human-AI collaboration. More examples can be found in the appendix.\n\nThe collaboration process is illustrated in Figure~\\ref{fig:multi}. During the dialogue, the researcher wants to selectively oxidize one of the two carbonyl groups of a molecule. However, the initial solution given by ChemDFM results in both carbonyl groups being oxidized. Through the correction given by the researcher, ChemDFM adjusts its proposal and provides two possible solutions. Finally, the researcher chooses to use protecting groups and ChemDFM further details its advice.\n\nIn the process, ChemDFM shows promising capabilities regarding error correction~(Round 2) and detailing~(Round 3).\nThis dialogue demonstrates the great prowess of ChemDFM to comprehend both natural language and chemical language. Through this prowess, ChemDFM can establish the universal language protocol with human researchers to achieve meaningful human-AI collaboration.\n\n\\section{Conclusion}\n\nIn this paper, we introduce ChemDFM, a pioneer attempt towards Chemical General Intelligence~(CGI). Through domain pre-training and instruction tuning, ChemDFM has established strong comprehension and reasoning capabilities for chemical knowledge and patterns, leading to advanced performance in chemical tasks such as molecular design, reaction analysis, and knowledge-intense question-answering. Besides, ChemDFM also possesses strong abilities in comprehending both chemical and natural languages, which enables it to assist researchers in real-world scenarios through dialogue-based free-form human-AI collaboration. We will open-source the ChemDFM model and encourage researchers from both AI and chemistry communities to explore it.\n\nAs the primary attempt towards CGI, ChemDFM has much room for improvement. For example, considering that there are various informative modalities in chemistry, such as molecular graphs and spectroscopies, we believe multi-modalities are necessary for CGI. In addition, tool-using methods are also worth exploring, as they can significantly improve the reliability of LLMs. We leave these as future work.\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{\\modelname: Advancing Large Language Models for Chemistry \\\\with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset}\n\n\\begin{document}\n\n\\def\\thefootnote{*}\\footnotetext{Equal contribution.}\\def\\thefootnote{\\arabic{footnote}}\n\n\\maketitle\n\n\\begin{abstract}\nChemistry plays a crucial role in many domains, such as drug discovery and material science.\nWhile large language models (LLMs) such as GPT-4 exhibit remarkable capabilities on natural language processing tasks, existing research indicates that their performance on chemistry tasks is discouragingly low. In this paper, however, we demonstrate that our developed LLMs can achieve very strong results on a comprehensive set of chemistry tasks, outperforming the most advanced GPT-4 \\newcontent{and Claude 3 Opus} by a substantial margin.\nTo accomplish this, we propose \\datasetname, a \\textit{large-scale}, \\textit{comprehensive}, and \\textit{high-quality} dataset for instruction tuning. It contains 14 selected chemistry tasks and over three million samples, laying a solid foundation for training and evaluating LLMs for chemistry. Using \\datasetname, we fine-tune a set of open-source LLMs named as \\modelname, among which, we find that Mistral serves as the best base model for chemistry tasks. \n\\newcontent{Our analysis further demonstrates the critical role of the proposed dataset in driving the performance improvements.}\\footnote{Our dataset and models can be found at \\url{https://osu-nlp-group.github.io/LLM4Chem/}.}\n\\end{abstract}\n\n\\section{Introduction}\n\nChemistry is a fundamental science that underpins countless aspects of modern life, ranging from drug discovery and materials science to energy production. To facilitate research and applications in this domain, deep learning models including graph neural networks \\citep{kipf2017semisupervised} and Transformer-based models \\citep{vaswani2017attention} have been developed for various chemistry tasks such as forward reaction prediction, retrosynthesis, property prediction \\citep{schwaller2019molecular,Zhong2022,chen2023g2retro,zhou2023unimol}. However, these models are usually task-specific models, which neglect shared chemistry knowledge across tasks and can hardly be adapted to different tasks.\n\nOn the other hand, large language models (LLMs) such as GPT-4 \\citep{achiam2023gpt4}, Llama series \\citep{touvron2023llama,touvron2023llama2}, and Mistral \\citep{jiang2023mistral} have emerged as general-purpose foundation models and demonstrate remarkable abilities on various natural language processing tasks \\citep{chang2023survey,thirunavukarasu2023large,yue2023mammoth,zhang2023tablellama,deng2023mindweb}. \nHowever, when applied to chemistry tasks, LLMs show only limited capabilities \\citep{jablonka2022gpt,guo2023what,hatakeyama2023prompt}.\nFor example, \\citet{guo2023what} conducted evaluations on eight chemistry tasks and observed that while GPT-4 outperforms other closed- and open-source LLMs, \nits performance is far from that of task-specific deep learning models. Particularly, they found that GPT models perform poorly when a precise understanding of SMILES \\citep{weininger1988smiles}, a widely used textual representation for molecules, is required. \nIn addition to directly applying pretrained LLMs, \\citet{fang2023mol} fine-tuned LLMs on an instruction tuning dataset, but their performance remains very low, far behind the state-of-the-art (SoTA) models designed and trained for specific tasks.\n\nGiven these discouraging results, some critical questions arise: \\textit{\\textbf{Are LLMs actually able to effectively perform chemistry tasks? Or, Are they fundamentally limited for chemistry?}} %\nIn this paper, we demonstrate that our developed LLMs can achieve very strong results on a comprehensive set of chemistry tasks, substantially outperforming the most advanced GPT-4 \\cite{achiam2023gpt4} \\newcontent{and Claude 3 Opus \\cite{anthropic2024claude3}}.\n\nWhat makes such LLMs possible? First, we construct a large-scale, comprehensive, and high-quality dataset for instruction tuning named \\datasetname. \nWe incorporate tasks with meaningful applications, collect data from diverse data sources, and apply rigorous scrutiny for quality control.\nThe resulting dataset consists of 14 tasks (illustrated in \\cref{fig:tasks}) and over 3M samples, laying a solid foundation for training and evaluating LLMs for chemistry tasks.\nBased on the dataset, we build a series of LLMs for chemistry named \\textbf{\\modelname} by fine-tuning four open-source LLMs namely Galactica, Llama 2, Code Llama, and Mistral, on \\datasetname with LoRA \\citep{hu2021lora}.\n\n\\begin{figure*}[t]\n\\begin{center}\n\\centerline{\n\\includegraphics[width=1.0\\textwidth]{fig/overview.pdf}}\n\\caption{An overview of tasks in the proposed \\datasetname dataset.}\n\\label{fig:tasks}\n\\end{center}\n\\vskip -0.4in\n\\end{figure*}\n\n\\newcontent{\nWe conduct comprehensive experiments to evaluate our models and explore their insights, yielding some interesting findings. \nFirstly, among the four \\modelname models, the Mistral-based model surpasses others by a substantial margin, showcasing the considerable influence of base models on downstream chemistry tasks.\nMoreover, contrast to claims made in previous work \\citep{fang2023mol}, using SMILES as the molecular representation achieves sufficient validity of generated molecules and better performance compared to using SELFIES \\citep{krenn2019selfies}.\nFurthermore, employing canonicalized SMILES during model training and applications can alleviate learning burdens and increase performance.\nFinally, while instruction tuning can inject chemistry task-related knowledge into models, the dataset plays a crucial role. Our experiments demonstrate that training on our \\datasetname leads to substantially better performance compared to training on previous dataset, emphasizing the contribution of the proposed dataset.\nAlthough \\modelname models do not yet surpass state-of-the-art (SoTA) task-specific models that are designed and trained specifically for each individual task, they approach SoTA performance with only 0.58\\% of parameters being fine-tuned, suggesting their great potential for further improvements and to serve as strong foundation models for the field.\n}\n\n\\section{Related Work}\n\n\\noindent \\textbf{Task-specific Models for Chemistry.} \nIn recent years, many deep learning models have been developed to tackle different chemistry tasks.\nFor example, Molecular Transformer \\cite{schwaller2019molecular} and RSMILES \\cite{Zhong2022} formulate forward synthesis and retrosynthesis prediction as sequence-to-sequence translation problems.\nChemformer \\cite{irwin2022chemformer} pretrains a transformer model on a large-scale SMILES dataset and fine-tunes it for various downstream tasks, such as forward synthesis and property prediction.\nMolT5 \\cite{edwards2022translation} first pretrains a T5 model on both SMILES and natural language, and then fine-tunes it to translate SMILES into natural language (i.e., molecule captioning) or vice versa (i.e., molecule generation).\n\\newcontent{Graph neural networks (GNNs), which directly leverage the graph structure of the molecule \\cite{wang_graph_2023}, have also shown promise in many chemistry applications, such as property prediction \\cite{yang_analyzing_2019,han2023himgnn}, retrosynthesis \\cite{chen2023g2retro,somnath2021learning}, and molecule optimization \\cite{chen2021deep, zhang2022molecule}.}\n\\newcontent{Recent studies \\cite{zhou2023unimol,zhang2022e3bind} have shown the promise of leveraging equivariant representations of molecular 3D structures for chemistry tasks, such as property prediction \\cite{zhou2023unimol} and docking \\cite{zhang2022e3bind}.}\nUni-Mol \\cite{zhou2023unimol} incorporates \\newcontent{this 3D information} into the pretraining of a transformer model and fine-tunes it for downstream tasks.\nDespite their effectiveness, these models operate on single tasks and therefore cannot harness knowledge shared across diverse chemistry tasks like LLMs.\n\n\\noindent \\textbf{LLMs for Chemistry.} \nRecent efforts have integrated LLMs with chemistry to solve key chemistry problems,\nwhich can be divided into two categories: (1) benchmark studies, and (2) fine-tuning LLMs with new datasets. %\nMultiple benchmark studies \\cite{White2023,guo2023what,Jablonka2023,liu2023chatgpt} have evaluated \nthe capabilities and limitations of different off-the-shelf LLMs, such as GPT-4 and Llama, on chemistry problems.\n{For example, \\citet{guo2023what} finds}\nthat these LLMs do not perform well on chemistry tasks and often produce\nchemically implausible outputs.\nThese findings highlight the need for further efforts to improve LLMs via fine-tuning for chemistry tasks.\n\nTo improve LLMs for chemistry, multiple instruction tuning datasets have been developed. %\nMol-Instructions \\cite{fang2023mol} consists of 1.3M instructions for multiple small molecule tasks.\n{However, fine-tuning on the dataset does not significantly improve LLMs' performance (Section \\ref{exp:overall_comp}).}\nDrugchat \\cite{Liang2023a} collects an instruction tuning dataset on drug properties with 10.8K drug molecules.\nMolOpt-Instructions \\cite{ye2023drugassist} consists of instructions with 1M molecule pairs for molecule optimization on six properties, in which each pair has similar molecules with different properties. \nRecent works also develop 2D or 3D molecular graph-centric datasets and integrate the graph understanding ability into LLMs \\cite{liu-etal-2023-molca, cao2023instructmol, li2024towards}.\nCompared with these datasets, %\n\\datasetname is {much larger}\nand covers {a} more diverse and comprehensive {set of} chemistry tasks, which enables LLMs to better understand molecule representations and learn chemistry knowledge across tasks.\n\n\\section{\\datasetname}\n\\newcontent{This section introduces our proposed dataset \\datasetname and its construction. Readers may refer to \\cref{app:prelim} for preliminaries and background.} %the involved background knowledge.}\n\n\\subsection{Overview of \\datasetname}\n\\label{subsec:dataset_overview}\n\n\\datasetname is a large-scale instruction tuning dataset that centers around small molecules. It contains 14 chemistry tasks, illustrated in \\cref{fig:tasks}.\n\n(1) We include four {name conversion} tasks, namely converting {IUPAC name} to molecular {formula} (NC-I2F), converting {IUPAC name} to {SMILES} (NC-I2S), converting {SMILES} to molecular formula (NC-S2F), and converting {SMILES} to {IUPAC name} (NC-S2I). They are designed to enable deep understanding of molecular structures and representations, which should serve as the fundamental knowledge for chemistry LLMs.\n\n(2) Additionally, six {property prediction} tasks \\citep{wu2018moleculenet} are integrated, including PP-ESOL for water solubility \\citep{Mobley2014}, PP-Lipo for octanol/water distribution coefficient \\citep{Poole2003}, PP-BBBP for blood-brain barrier penetration \\citep{martins2012}, PP-ClinTox for toxicity to human body \\citep{Gayvert2016}, PP-HIV for HIV replication inhibition \\citep{HIV}, and PP-SIDER for side effects of drugs \\citep{Kuhn2015}. These involved properties are crucial especially for drug development.\n\n(3) Two tasks focus on the textual descriptions of molecules: {molecule captioning} (MC) is to generate a textual description of a given molecule, and {molecule generation} (MG) is to generate a molecule based on the given textual description. They require comprehensive understanding of molecules - their structures and properties, from their textual descriptions. %\nThey also bridge the gap between natural language and molecules.\n\n(4) Lastly, two tasks revolve around chemical reaction knowledge. {Forward synthesis} (FS) aims to predict potential products from reactants and reagents, and {retrosynthesis} (RS) involves predicting potential reactants given a product. These tasks play vital roles in real-world applications \\citep{Coley2018}. For example, retrosynthesis is essential for synthesis planning, %\nwhile forward synthesis is used to validate retrosynthetic suggestions.\n\n\\datasetname contains 3.3M samples. Each sample is a query-response pair, where the query describes a task and any task-specific information (e.g., input molecule, textual description, etc.), and the response is a sentence containing the answer to the queried task. For all the tasks, unless explicitly defined in the tasks (NC-I2F, NC-I2S, NC-S2F, and NC-S2I), we use {SMILES} as the \\newcontent{default representation for molecules, but also provide the SELFIES \\citep{krenn2019selfies} representation}.\n\n\\subsection{\\datasetname Construction}\n\\label{subsec:data_construction}\n\nWe construct the \\datasetname dataset by following a four-step pipeline: data collection, quality control, data splitting, and instruction construction.\n\n\\noindent \\textbf{Data Collection.} After consulting domain experts and pinpointing the set of meaningful tasks (summarized in \\cref{subsec:dataset_overview}), we collect data for these tasks from various sources, as listed in \\cref{tab:statistics}. \nSpecifically, for the {name conversion} tasks (NC-I2F, NC-I2S, NC-S2F, and NC-S2I), we leverage PubChem\\footnote{\\url{https://pubchem.ncbi.nlm.nih.gov/}} \\citep{kim2019pubchem}, one of the most comprehensive molecule databases. Within this database, we randomly select a large set of molecule entries, and extract their {IUPAC} names, {SMILES} representations, and molecular formulas. This obtained data is then re-organized as input-output pairs for the tasks. \nFor molecular description-related tasks (MC and MG), we utilize a combination of ChEBI-20 \\citep{edwards2021text2mol,edwards2022translation} and Mol-Instructions \\citep{fang2023mol}, as they both contain high-quality molecule-text paired data. \nFor property prediction tasks (PP-ESOL, PP-Lipo, PP-BBBP, PP-ClinTox, PP-HIV, and PP-SIDER), we employ the well-established MoleculeNet datasets \\citep{wu2018moleculenet}. We select \\newcontent{the 6 datasets from MoleculeNet that represent the essential properties for real-world applications such as drug discovery.}\nFor chemical reaction tasks (FS and RS), we collect the reaction data from USPTO-full \\citep{Lowe2017}, which is an extensive collection encompassing over 1M reaction samples extracted from U.S. patents.\nAll the aforementioned datasets are also widely used in previous studies \\citep{he2021molecular,Zhong2022,edwards2022translation,irwin2022chemformer,chen2023g2retro,zhou2023unimol}.\n\n\\noindent \\textbf{Quality Control.}\nTo guarantee high quality, we apply rigorous scrutiny. \nThe collected data contains many problematic and low-quality samples, which can be roughly categorized into the following three types, along with our curation methods:\n(1) Chemically invalid SMILES. Numerous SMILES strings are chemically invalid (e.g., deviating from the SMILES grammar, or violating chemical valence). To address this issue, we employ RDKit \\citep{rdkit}, a widely used toolkit for cheminformatics, to parse molecules and detect errors.\n(2) Wrong or inaccurate information. Based on manual check, we observed wrong and inaccurate information recorded in the data. \nFor instance, within the USPTO-full dataset \\citep{Lowe2017}, we identify and correct mislabeled reactants and reagents in chemical reactions by comparing their atom mappings with products.\nFor the MC and MG tasks, we filter out those textual descriptions that \nlack pertinent, molecule-specific information, with a set of rules based on wording patterns, lengths and keywords. \nFor PP-SIDER, we eliminate disorders with ambiguous names that could impede the creation of precise and comprehensible instructions.\n(3) Duplicated samples. We detect and remove them.\n\n\\noindent \\textbf{Data Splitting.}\nData splitting for multi-task datasets requires careful handling in order to avoid data leakage across tasks. For instance, FS and RS are a pair of reverse tasks, so data leakage occurs when the training set contains an FS sample for a certain chemical reaction and the test set has an RS sample for the same reaction. This can lead to biased evaluation. Therefore, we identify sample pairs across related tasks\n(FS and RS, MC and MG, and the four NC tasks) that correspond to the same molecules/reactions, and ensure that matched samples are placed together in either training or evaluation set. \nMoreover, some samples may share the same input but have different outputs. For instance, in the RS task, one product (the same input) may be synthesized from multiple sets of reactants (different outputs). If these samples are placed into both training and test set, it may lead to exaggerated performance. \nTherefore we ensure that samples with identical inputs are placed together either in or outside of the test set. \nAdditionally, to achieve fair comparisons with Mol-instructions \\citep{fang2023mol}, for tasks shared between the two datasets (MC, MG, FS, and RS), we ensure that their training examples are not included in the test set of \\datasetname, allowing for a direct evaluation of their models on our test set.\nFollowing these necessary limitations, samples are randomly split into training/validation/test set, except for PP task samples that undergo a scaffold splitting following the canonical method \\citep{wu2018moleculenet}.\n\n\\noindent\\textbf{Instruction Creation.}\nTo create query-response textual pairs for instruction tuning, we manually craft several templates, each including a query and a corresponding response, and apply GPT-4 to rephrase them. \nUnlike those in \\citep{fang2023mol} which consist of highly formatted queries (containing three explicitly labeled parts namely instruction, input, and output) and answer-only responses (e.g., responses for FS and RS only contain answer SMILES alone, without any natural text), our templates exhibit a more natural and diverse set of formats in both queries and responses, allowing for more variations and naturalness in input-output interactions.\nMoreover, all the {SMILES} representations are canonicalized, establishing a standardized data format. \nIn light of the dataset's inclusion of multi-type sequences (SMILES, molecular formula, numbers, etc.) beyond natural language text alone, we utilize special tags to encapsulate corresponding segments (e.g., \\texttt{<SMILES>...</SMILES>} for SMILES, \\texttt{<MOLFORMULA>...</MOLFORMULA>} for molecular formula, \\texttt{<NUMBER>...</NUMBER>} for numbers). This design does not only explicitly inform models about the information types within the tagged content, but also facilitate answer extraction during evaluation.\n\n\\newcontent{For more details of dataset construction, please refer to \\cref{app:dataset_construction}.}\n\n\\subsection{Merits of \\datasetname}\n\nCompared to previous work \\citep{fang2023mol,Liang2023a,ye2023drugassist}, \\datasetname stands out in several key aspects:\n\n(1) \\textbf{Large-Scale}.\n\\datasetname consists of 3.3M samples and 1.6M distinct molecules, with a diverse range of sizes, structures, and properties (see \\cref{app:dataset_statistics}),\nshowcasing an extensive coverage of diverse chemical knowledge.\n\n(2) \\textbf{Comprehensive}.\n\\datasetname contains 4 types of chemical tasks (14 tasks in total), emerging as the most comprehensive instruction tuning dataset for small molecules. Notably, the tasks are meticulously selected to build a strong chemistry foundation model and to adapt to real-world applications. \n\n(3) \\textbf{High-Quality}.\nRigorous processing steps have been implemented to exclude problematic and low-quality samples. Along with careful data splitting and canonicalization of SMILES representations, \\datasetname stands as a high-quality resource valuable for future research.\n\nA detailed introduction and statistics of the \\datasetname dataset can be found in \\cref{app:dataset}. For a comparison with the previous work, Mol-Instructions \\citep{fang2023mol}, please refer to \\cref{app:dataset_comparison}.\n\n\\section{Experiments}\n\\label{sec:experiments}\n\n\\subsection{Our \\modelname Models} \n\\label{subsec:our_model}\n\nBy fine-tuning base models on the proposed \\datasetname dataset, we create LLMs capable of performing chemistry tasks, which we name \\modelname (\\textbf{L}arge \\textbf{la}nguage models on \\textbf{S}mall \\textbf{Mol}ecules).\nSpecifically, we extensively consider four different LLMs as our base models, namely Galactica 6.7B \\citep{taylor2022galactica}, Llama 2 \\citep{touvron2023llama2} 7B, Code Llama \\citep{roziere2023code} 7B, and Mistral \\citep{jiang2023mistral} 7B, where Galactica is trained for scientific applications and has already been exposed to chemistry-related data during its pretraining, Llama 2 and Mistral are general-purpose LLMs, while Code Llama is based on Llama 2 and trained for code.\nWe conduct instruction tuning on the proposed \\datasetname dataset, and name the resulting models as \\modelnameg, \\modelnamel, \\modelnamec, and \\modelnamem, respectively. All the \\modelname models are trained with LoRA \\citep{hu2021lora}, which is applied to all weight matrices in the self-attention and feedforward neural network (FFN) modules with \\texttt{lora\\_r} and \\texttt{lora\\_alpha} set to 16. The fine-tuning process utilizes the Huggingface Transformers library \\citep{wolf2020transformers}. Training spans three epochs, employing the 8-bit AdamW optimizer, a learning rate of 1e-4, and a cosine scheduler. The input length for training is set to 512, which covers 99.7\\% of the samples. During inference, we adopt beam search as the generation strategy for simplicity.\n\n\\subsection{Experimental Setup}\n\\label{subsec:experimental}\n\n\\noindent \\textbf{Compared Models.}\nWe compare our \\modelname models with two types of models:\n\n(1) \\textbf{LLMs without fine-tuning on \\datasetname}.\nThis type includes our four base models, namely Galactica \\citep{taylor2022galactica}, Llama 2 \\citep{touvron2023llama2}, Code Llama \\citep{roziere2023code}, Mistral \\citep{jiang2023mistral}.\nwe also benchmark against GPT-4 \\citep{achiam2023gpt4} and the more recent Claude 3 Opus \\citep{anthropic2024claude3}, the current state-of-the-art (SoTA) LLMs\\footnote{Due to resource limitations, we evaluate GPT-4 and Claude 3 Opus on at most 500 test samples for each task.}. \nFor Llama 2, Code Llama, and Mistral, we use 1-shot, due to their poor instruction following ability; for GPT-4, we report its results under a zero-shot setting, as GPT-4 performs best on this setting in our experiments %\n(\\cref{app:exp_results}); \\newcontent{for Claude 3 Opus, we report its zero-shot results as well.}\n\\newcontent{We also include two LLMs tuned specifically for chemistry tasks: Molinst, a Llama 2 model tuned on the Mol-Instructions dataset by \\citet{fang2023mol}, which shares the training tasks of MC, MG, FS, and RS with \\modelname; and ChemLLM \\citep{zhang2024chemllm}, an LLM for chemistry proposed concurrently to our work.}\n\n(2) \\textbf{SoTA task-specific models.} To provide a comprehensive view of \\modelname's performance, we present results from SoTA task-specific models. \nFor NC-I2S and NC-S2I, we compare with STOUT \\citep{rajan2021stout}, an encoder-decoder model trained on SMILES-IUPAC name paired data. \nFor NC-S2F, a task achievable with a fixed algorithm, we implement a program with RDKit \\citep{rdkit}, a widely used Python toolkit for cheminformatics, and report its results. \nFor NC-I2F where no dedicated models exist, we construct a baseline called STOUT+RDKit by aggregating STOUT for I2S conversion and RDKit for S2F conversion. \nFor the PP tasks, our compared model is Uni-Mol \\citep{zhou2023unimol}. It incorporates molecular 3D representations and follows a pretraining and fine-tuning paradigm. Following its original settings, we fine-tune the model on our \\datasetname dataset with its pretrained checkpoint. \nIn the case of MC and MG, we compare with MolT5 \\citep{edwards2022translation} and directly use their released checkpoint. The reasons why we do not use our re-trained model are: (1) we were unable to reproduce results close to those reported in the paper as no original code was provided; and (2) we take great care to ensure that our test set is devoid of training examples used by MolT5, ensuring fairness in the evaluation.\nLastly, regarding FS and RS, we re-train RSMILES \\citep{Zhong2022} and Molecular Transformer \\citep{schwaller2019molecular} for the two tasks, respectively, following their reported settings. Both of the models are transformer encoder-decoder models \\citep{vaswani2017attention}, specifically adapted for the FS and RS tasks.\n\n\\noindent \\textbf{Evaluation Metrics.}\nWe employ metrics commonly used in previous work \\citep{schwaller2019molecular,Zhong2022,fang2023mol,zhou2023unimol,chen2023g2retro}, which include: \n(1) \\textbf{Exact Match (EM)}, indicating the proportion of predicted results that exactly match the gold standards. \n(2) \\textbf{Fingerprint Tanimoto Similarity (FTS)}, quantifying structural similarities between molecules using Tanimoto similarities of their Morgan fingerprints \\citep{Morgan1965}.\n(3) \\textbf{METEOR score}, a comprehensive text-based metric considering both exact matches and semantic similarity \\citep{lavie-agarwal-2007-meteor} for the MC task.\n(4) \\textbf{Root Mean Square Error (RMSE)}, measuring the square root of the average squared differences between predicted and actual values for the PP-ESOL and PP-Lipo tasks \n(5) \\textbf{Accuracy (Acc)}, the ratio of correct predictions for the binary classification tasks (PP-BBBP, PP-ClinTox, PP-HIV, and PP-SIDER). \n(6) \\textbf{Validity (Valid)}, the ratio of valid predictions following SMILES grammar and chemical valence rules for tasks with SMILES outputs (NC-I2S, MG, FS, and RS). \nFor all the metrics except RMSE, higher values indicate better performance.\n\n\\subsection{Main Results} %\n\\label{exp:overall_comp}\n\n\\cref{tab:o_1} and \\ref{tab:o_2} show the performance on \\datasetname. Key observations are as follows: \n\n\\begin{table*}[t]\n\\centering\n\\caption{Results for name conversion (NC) and property prediction (PP) tasks. Metrics EM, Valid, and Acc are in percentage.}\n\\resizebox{1\\linewidth}{!}{\n\\begin{threeparttable}[b]\n  \\centering\n  \n  \\sisetup{detect-weight=true}\n    \\begin{tabular}{lS[table-format=2.1]S[table-format=2.1]S[table-format=2.1]S[table-format=3.1]S[table-format=2.1]S[table-format=1.3]S[table-format=1.3]S[table-format=2.1]S[table-format=2.1]S[table-format=2.1]S[table-format=2.1]}\n    \\toprule\n    \\multicolumn{1}{c}{\\multirow{3}[6]{*}{\\textbf{Model}}} & \\multicolumn{5}{c}{\\textbf{NC}}       & \\multicolumn{6}{c}{\\textbf{PP}} \\\\\n    \\cmidrule(lr){2-6}  \\cmidrule(lr){7-12}   & \\textbf{I2F} & \\multicolumn{2}{c}{\\textbf{I2S}} & \\textbf{S2F} & \\textbf{S2I} & \\textbf{ESOL} & \\textbf{Lipo} & \\textbf{BBBP} & \\textbf{Clintox} & \\textbf{HIV} & \\textbf{SIDER} \\\\\n     \\cmidrule(lr){2-2} \\cmidrule(lr){3-4} \\cmidrule(lr){5-5} \\cmidrule(lr){6-6} \\cmidrule(lr){7-7} \\cmidrule(lr){8-8} \\cmidrule(lr){9-9} \\cmidrule(lr){10-10}  \\cmidrule(lr){11-11}  \\cmidrule(lr){12-12}\n     & \\textbf{EM} & \\textbf{EM} & \\textbf{Valid} & \\textbf{EM} & \\textbf{EM} & \\textbf{RMSE} $\\downarrow$ & \\textbf{RMSE} $\\downarrow$ & \\textbf{Acc} & \\textbf{Acc} & \\textbf{Acc} & \\textbf{Acc} \\\\\n    \\midrule\n    \\rowcolor{TableSeparator} \\multicolumn{12}{c}{\\rule{0pt}{10pt}\\textbf{Task-Specific, Non-LLM Based Models}} \\\\[1pt]\n    \\multicolumn{1}{p{7.585em}}{SoTA} & 97.9  & 73.5  & 99.4  & 100.0 & 56.5  & 0.819 & 0.612 & 85.3  & 92.4  & 97.0  & 70.0 \\\\\n    \\rowcolor{TableSeparator} \\multicolumn{12}{c}{\\rule{0pt}{10pt}\\textbf{Existing LLMs without fine-tuning on \\datasetname}}  \\\\[1pt]\n    GPT-4 & 8.7   & 3.3   & 84.2  & 4.8   & 0.0   & 2.570 & 1.545 & 62.9  & 50.0  & 59.6  & 57.6 \\\\\n    \\newcontent{Claude 3 Opus} & 34.6  & 17.7  & 90.2  & 9.2   & 0.0   & \\bfseries 1.036 & 1.194 & \\bfseries 75.1  & 41.7  & 76.4  & 67.0 \\\\\n    Galactica & 9.1   & 9.7   & 95.6  & 0.0   & 0.0   & 4.184 & 2.979 & 69.0  & 92.4  & \\bfseries 96.7  & 68.1 \\\\\n    Llama 2 & 0.0   & 0.0   & 18.3  & 0.0   & 0.0   & 3.287 & 1.634 & 58.9  & 45.1  & 93.3  & 61.9 \\\\\n    Code Llama & 0.0   & 0.0   & 81.0  & 0.0   & 0.0   & 3.483 & 1.733 & 58.9  & 85.4  & 91.8  & 60.2 \\\\\n    Mistral & 0.0   & 0.0   & 40.3  & 0.0   & 0.0   & 3.079 & 1.730 & 40.6  & 15.3  & 7.1   & 38.1 \\\\\n    Molinst (\\newcontent{chemistry LLM}) & 0.0   & 0.0   & 96.2  & 0.0   & 0.0   & 2.271 & 1.691 & 60.9  & 6.3   & 4.5   & 52.4 \\\\\n    \\newcontent{ChemLLM} (\\newcontent{chemistry LLM}) & 0.8   & 0.3   & 3.9   & 0.0   & 0.0   & 1.946 & 1.797 & 22.3  & 75.7  & 72.9  & 32.6 \\\\\n    \\rowcolor{TableSeparator} \\multicolumn{12}{c}{\\rule{0pt}{10pt}\\textbf{Our \\modelname Series}}  \\\\[1pt]\n    \\modelnameg & 83.2  & 58.7  & 99.4  & 91.2  & 18.3  & 1.959 & 1.213 & 69.0  & \\bfseries 93.1  & \\bfseries 96.7  & 70.1 \\\\\n    \\modelnamel & 73.8  & 46.6  & 99.0  & 87.0  & 12.9  & 2.791 & 1.338 & 69.0  & 92.4  & \\bfseries 96.7  & 68.7 \\\\\n    \\modelnamec & 75.4  & 49.9  & 99.3  & 88.6  & 15.5  & 2.959 & 1.203 & 69.0  & \\bfseries 93.1  & \\bfseries 96.7  & 69.9 \\\\\n    \\modelnamem & \\bfseries 87.9  & \\bfseries 70.1  & \\bfseries 99.6  & \\bfseries 93.2  & \\bfseries 29.0  & 1.150 & \\bfseries 1.010 & 74.6  & \\bfseries 93.1  & \\bfseries 96.7  & \\bfseries 70.7 \\\\\n    \\bottomrule\n    \\end{tabular}%\n  \n\\end{threeparttable}\n}\n\\label{tab:o_1}%\n\\end{table*}%\n\n(1) \\textbf{Among all the LLMs, our \\modelname models demonstrate the best performance, underscoring the effectiveness of the proposed \\datasetname dataset and fine-tuning}.\nSpecifically, compared to the base models (Galactica, Llama 2, Code Llama, and Mistral), \\modelname models exhibit substantial performance improvements, which highlights the effectiveness of \\datasetname in enhancing the understanding of molecular representations and the task-related knowledge, and signifies the effective learning of chemistry-related tasks by LLMs.\nFurthermore, \\modelname substantially outperforms GPT-4 on all the tasks \\newcontent{and Claude 3 Opus on most tasks}, despite their larger parameter size.\n\\newcontent{\\modelname also surpasses the two chemistry LLMs namely ChemLLM\\footnote{Since its dataset and evaluation details are not available, we cannot provide more analysis.}, which is similarly trained on chemistry instruction data.} and Molinst.\nNotably, \\modelnamel, which uses the same base model and LoRA setting as Molinst, outperforms it even on the shared training tasks (MC, MG, FS, and RS). This finding highlights the benefits of our dataset.\n\n(2) \\textbf{Our four \\modelname models show substantial differences in their performance, emphasizing the considerable impact of base models on downstream tasks}. \nDespite sharing identical training, inference settings, and comparable model sizes, \\modelnamem consistently outperforms \\modelnamel by a substantial margin, highlighting Mistral's potential on chemistry tasks.\nIn addition, \\modelnamec exhibits better performance than \\modelnamel \\newcontent{on most tasks}, indicating a potential synergy between programming language knowledge in Code Llama and molecular representations.\nFurthermore, \\modelnameg outperforms \\modelnamel, and \\modelnamec in most cases, suggesting the benefits of pretraining on chemistry-related documents.\n\n\\newcontent{(3) \\textbf{Although \\modelname models do not outperform SoTA models, they demonstrate considerable potential for further improvements}.\nSpecifically, \\modelnamem surpasses the SoTA models on PP-Clintox and PP-SIDER, but has yet to achieve the success on other tasks.\nHowever, \\modelname has greatly narrowed the performance gap between LLMs and SoTA task-specific models, compared to previous efforts \\citep{fang2023mol,zhang2024chemllm}. \nRemarkably, \\modelnamem attains such performance with only a small proportion of its parameters fine-tuned (approximately 41.9M, 0.58\\% of its parameters). As shown in \\cref{subsec:lora}, increasing the number of trainable parameters can substantially boost performance, suggesting that \\modelnamem has immense potential to surpass task-specific models through more extensive fine-tuning and serve as a strong foundation model for chemistry applications.\n}\n\n\\begin{table*}\n\\centering\n\\caption{Results for molecule captioning (MC), molecule generation (MG), forward synthesis (FS), and retrosynthesis (RS). Metrics EM, FTS, and Valid are in percentage.}\n\\resizebox{0.92\\linewidth}{!}{\n\\begin{threeparttable}\n  \\centering\n  \\sisetup{detect-weight=true}\n    \\begin{tabular}{lS[table-format=1.3]S[table-format=2.1]S[table-format=2.1]S[table-format=2.1]S[table-format=2.1]S[table-format=2.1]S[table-format=3.1]S[table-format=2.1]S[table-format=2.1]S[table-format=3.1]}\n    \\toprule\n    \\multicolumn{1}{l}{\\multirow{2}[2]{*}{\\textbf{Model}}} & \\multicolumn{1}{c}{\\textbf{MC}} & \\multicolumn{3}{c}{\\textbf{MG}} & \\multicolumn{3}{c}{\\textbf{FS}} & \\multicolumn{3}{c}{\\textbf{RS}} \\\\\n    \\cmidrule(lr){2-2} \\cmidrule(lr){3-5} \\cmidrule(lr){6-8} \\cmidrule(lr){9-11}\n          & \\multicolumn{1}{c}{\\textbf{METEOR}} & \\multicolumn{1}{c}{\\textbf{EM}} & \\multicolumn{1}{c}{\\textbf{FTS}} & \\multicolumn{1}{c}{\\textbf{Valid}} & \\multicolumn{1}{c}{\\textbf{EM}} & \\multicolumn{1}{c}{\\textbf{FTS}} & \\multicolumn{1}{c}{\\textbf{Valid}} & \\multicolumn{1}{c}{\\textbf{EM}} & \\multicolumn{1}{c}{\\textbf{FTS}} & \\multicolumn{1}{c}{\\textbf{Valid}} \\\\\n    \\midrule\n    \\rowcolor{TableSeparator} \\multicolumn{11}{c}{\\rule{0pt}{10pt} \\textbf{Task-Specific, Non-LLM Based Models}}         \\\\[1pt]\n    \\multicolumn{1}{p{6.835em}}{SoTA} & 0.515 & 31.7  & 73.2  & 95.3  & 78.7  & 92.2  & 100.0 & 47.0  & 77.5  & 99.7 \\\\\n    \\rowcolor{TableSeparator} \\multicolumn{11}{c}{\\rule{0pt}{10pt} \\textbf{Existing LLMs Without Fine-Tuning on \\datasetname}}  \\\\[1pt]\n    GPT-4 & 0.188 & 6.4   & 42.6  & 81.4  & 1.6   & 40.5  & 87.0  & 0.0   & 33.4  & 42.6 \\\\\n    \\newcontent{Claude 3 Opus} & 0.219 & 12.3  & 57.6  & 92.6  & 3.7   & 45.7  & 97.0  & 1.1   & 46.2  & 94.8 \\\\\n    Galactica & 0.050 & 0.0   & 11.6  & 94.7  & 0.0   & 25.9  & 83.7  & 0.0   & 34.6  & 93.0 \\\\\n    Llama 2 & 0.150 & 0.0   & 4.8   & 93.5  & 0.0   & 13.7  & 97.7  & 0.0   & 27.5  & 87.7 \\\\\n    Code Llama & 0.143 & 0.0   & 8.5   & 95.2  & 0.0   & 15.8  & 99.6  & 0.0   & 25.3  & 97.1 \\\\\n    Mistral & 0.193 & 0.0   & 9.0   & 35.9  & 0.0   & 19.9  & 95.8  & 0.0   & 24.2  & 98.0 \\\\\n    Molinst (\\newcontent{chemistry LLM}) & 0.124 & 6.0   & 43.6  & 84.8  & 2.1   & 31.7  & \\bfseries 99.8  & 5.7   & 48.0  & 97.8 \\\\\n    \\newcontent{ChemLLM} (\\newcontent{chemistry LLM}) & 0.050 & 0.9   & 14.3  & 4.3   & 0.0   & 1.6   & 38.5  & 0.0   & 2.9   & 10.9 \\\\\n    \\rowcolor{TableSeparator} \\multicolumn{11}{c}{\\rule{0pt}{10pt} \\textbf{Our \\modelname Series}}    \\\\[1pt]\n    \\modelnameg & 0.394 & 7.7   & 52.2  & 99.6  & 53.1  & 79.9  & 99.7  & 25.7  & 67.0  & 99.9 \\\\\n    \\modelnamel & 0.377 & 6.4   & 47.1  & 99.6  & 47.1  & 76.9  & \\bfseries 99.8  & 22.5  & 65.2  & 99.9 \\\\\n    \\modelnamec & 0.366 & 6.5   & 46.6  & \\bfseries 99.7  & 52.0  & 79.2  & \\bfseries 99.8  & 25.7  & 66.7  & \\bfseries 100.0 \\\\\n    \\modelnamem & \\bfseries 0.452 & \\bfseries 19.2  & \\bfseries 61.7  & \\bfseries 99.7  & \\bfseries 63.3  & \\bfseries 84.9  & \\bfseries 99.8  & \\bfseries 32.9  & \\bfseries 70.4  & \\bfseries 100.0 \\\\\n    \\bottomrule\n    \\end{tabular}%\n  \n\\end{threeparttable}\n}\n\\label{tab:o_2}%\n\\end{table*}%\n\n\\subsection{\\newcontent{Ablation Study}}\n\nTo investigate the advantages of \\datasetname, we conduct an ablation study by comparing \\modelnamem with the following variants: \n(1) \\textbf{w/o canonical}, which uses uncanonicalized SMILES, to examine the benefits of canonicalization.\n(2) \\textbf{using SELFIES}, which uses SELFIES \\cite{krenn2019selfies} instead of SMILES to explore their differences.\n(3) \\textbf{train on Mol-Instructions}, which is trained on Mol-Instructions \\citep{fang2023mol}, to compare the performance improvements of our dataset against the previously proposed dataset.\n\n\\begin{table*}[t]\n\\centering\n\\caption{Results of ablation study on NC and PP tasks. Metrics EM, Valid, and Acc are in percentage. Orange cells represent better results than \\modelnamem while blue cells represent worse results.}\n\\resizebox{0.90\\linewidth}{!}{\n\\begin{threeparttable}[b]\n  \\centering\n  \n  \\sisetup{detect-weight=true}\n    \\begin{tabular}{lS[table-format=2.1]S[table-format=2.1]S[table-format=2.1]S[table-format=3.1]S[table-format=2.1]S[table-format=1.3]S[table-format=1.3]S[table-format=2.1]S[table-format=2.1]S[table-format=2.1]S[table-format=2.1]}\n    \\toprule\n    \\multicolumn{1}{c}{\\multirow{3}[6]{*}{\\textbf{Model}}} & \\multicolumn{5}{c}{\\textbf{NC}}       & \\multicolumn{6}{c}{\\textbf{PP}} \\\\\n    \\cmidrule(lr){2-6}  \\cmidrule(lr){7-12}   & \\textbf{I2F} & \\multicolumn{2}{c}{\\textbf{I2S}} & \\textbf{S2F} & \\textbf{S2I} & \\textbf{ESOL} & \\textbf{Lipo} & \\textbf{BBBP} & \\textbf{Clintox} & \\textbf{HIV} & \\textbf{SIDER} \\\\\n     \\cmidrule(lr){2-2} \\cmidrule(lr){3-4} \\cmidrule(lr){5-5} \\cmidrule(lr){6-6} \\cmidrule(lr){7-7} \\cmidrule(lr){8-8} \\cmidrule(lr){9-9} \\cmidrule(lr){10-10}  \\cmidrule(lr){11-11}  \\cmidrule(lr){12-12}\n     & \\textbf{EM} & \\textbf{EM} & \\textbf{Valid} & \\textbf{EM} & \\textbf{EM} & \\textbf{RMSE} $\\downarrow$ & \\textbf{RMSE} $\\downarrow$ & \\textbf{Acc} & \\textbf{Acc} & \\textbf{Acc} & \\textbf{Acc} \\\\\n    \\midrule\n    \n    \\modelnamem & 87.9  & 70.1  & 99.6  & 93.2  & 29.0  & 1.150 & 1.010 & 74.6  & 93.1  & 96.7  & 70.7 \\\\\n       \\quad w/o canonical & \\cellcolor[rgb]{ .973,  .796,  .678}88.5 & \\cellcolor[rgb]{ .706,  .776,  .906}67.2 & 99.6  & \\cellcolor[rgb]{ .973,  .796,  .678}93.4 & \\cellcolor[rgb]{ .706,  .776,  .906}24.5 & \\cellcolor[rgb]{ .706,  .776,  .906}1.224 & \\cellcolor[rgb]{ .706,  .776,  .906}1.072 & \\cellcolor[rgb]{ .706,  .776,  .906}71.6 & 93.1  & \\cellcolor[rgb]{ .973,  .796,  .678}96.8 & \\cellcolor[rgb]{ .706,  .776,  .906}70.3 \\\\\n       \\quad using SELFIES & \\cellcolor[rgb]{ .706,  .776,  .906}86.9 & \\cellcolor[rgb]{ .706,  .776,  .906}47.7 & \\cellcolor[rgb]{ .973,  .796,  .678}100.0 & \\cellcolor[rgb]{ .973,  .796,  .678}94.7 & \\cellcolor[rgb]{ .706,  .776,  .906}19.7 & \\cellcolor[rgb]{ .706,  .776,  .906}1.456 & \\cellcolor[rgb]{ .706,  .776,  .906}1.106 & \\cellcolor[rgb]{ .706,  .776,  .906}69.5 & \\cellcolor[rgb]{ .706,  .776,  .906}91.7 & \\cellcolor[rgb]{ .706,  .776,  .906}96.5 & \\cellcolor[rgb]{ .706,  .776,  .906}64.4 \\\\\n       \\quad train on Mol-Instructions & \\cellcolor[rgb]{ .706,  .776,  .906}0.0 & \\cellcolor[rgb]{ .706,  .776,  .906}0.0 & \\cellcolor[rgb]{ .706,  .776,  .906}75.2 & \\cellcolor[rgb]{ .706,  .776,  .906}0.0 & \\cellcolor[rgb]{ .706,  .776,  .906}0.0 & \\cellcolor[rgb]{ .706,  .776,  .906}4.416 & \\cellcolor[rgb]{ .706,  .776,  .906}2.282 & \\cellcolor[rgb]{ .706,  .776,  .906}0.0 & \\cellcolor[rgb]{ .706,  .776,  .906}0.0 & \\cellcolor[rgb]{ .706,  .776,  .906}2.6 & \\cellcolor[rgb]{ .706,  .776,  .906}0.4 \\\\\n    \n    \\bottomrule\n    \\end{tabular}%\n  \n\\end{threeparttable}\n}\n\\label{tab:b_1}%\n\\end{table*}%\n\n\\begin{table*}\n\\centering\n\\caption{Results of ablation study on MC, MG, FS, and RS. Metrics EM, FTS, and Valid are in percentage. Orange represents better results than \\modelnamem, while blue represents worse results.}\n\\resizebox{0.90\\linewidth}{!}{\n\\begin{threeparttable}\n  \\centering\n  \\sisetup{detect-weight=true}\n    \\begin{tabular}{lS[table-format=1.3]S[table-format=2.1]S[table-format=2.1]S[table-format=2.1]S[table-format=2.1]S[table-format=2.1]S[table-format=3.1]S[table-format=2.1]S[table-format=2.1]S[table-format=3.1]}\n    \\toprule\n    \\multicolumn{1}{l}{\\multirow{2}[2]{*}{\\textbf{Model}}} & \\multicolumn{1}{c}{\\textbf{MC}} & \\multicolumn{3}{c}{\\textbf{MG}} & \\multicolumn{3}{c}{\\textbf{FS}} & \\multicolumn{3}{c}{\\textbf{RS}} \\\\\n    \\cmidrule(lr){2-2} \\cmidrule(lr){3-5} \\cmidrule(lr){6-8} \\cmidrule(lr){9-11}\n          & \\multicolumn{1}{c}{\\textbf{METEOR}} & \\multicolumn{1}{c}{\\textbf{EM}} & \\multicolumn{1}{c}{\\textbf{FTS}} & \\multicolumn{1}{c}{\\textbf{Valid}} & \\multicolumn{1}{c}{\\textbf{EM}} & \\multicolumn{1}{c}{\\textbf{FTS}} & \\multicolumn{1}{c}{\\textbf{Valid}} & \\multicolumn{1}{c}{\\textbf{EM}} & \\multicolumn{1}{c}{\\textbf{FTS}} & \\multicolumn{1}{c}{\\textbf{Valid}} \\\\\n    \\midrule\n    \\modelnamem & 0.452 & 19.2  & 61.7  & 99.7  & 63.3  & 84.9  & 99.8  & 32.9  & 70.4  & 100.0 \\\\\n       \\quad w/o canonical & \\cellcolor[rgb]{ .973,  .796,  .678}0.457 & \\cellcolor[rgb]{ .706,  .776,  .906}16.8 & \\cellcolor[rgb]{ .706,  .776,  .906}60.2 & \\cellcolor[rgb]{ .706,  .776,  .906}99.1 & \\cellcolor[rgb]{ .706,  .776,  .906}53.7 & \\cellcolor[rgb]{ .706,  .776,  .906}80.8 & \\cellcolor[rgb]{ .973,  .796,  .678}99.9 & \\cellcolor[rgb]{ .706,  .776,  .906}23.8 & \\cellcolor[rgb]{ .706,  .776,  .906}67.4 & \\cellcolor[rgb]{ .706,  .776,  .906}99.9 \\\\\n       \\quad using SELFIES & \\cellcolor[rgb]{ .973,  .796,  .678}0.466 & \\cellcolor[rgb]{ .706,  .776,  .906}16.2 & \\cellcolor[rgb]{ .706,  .776,  .906}58.6 & \\cellcolor[rgb]{ .973,  .796,  .678}99.9 & \\cellcolor[rgb]{ .706,  .776,  .906}40.4 & \\cellcolor[rgb]{ .706,  .776,  .906}74.0 & \\cellcolor[rgb]{ .973,  .796,  .678}100.0 & \\cellcolor[rgb]{ .706,  .776,  .906}25.6 & \\cellcolor[rgb]{ .706,  .776,  .906}66.0 & \\cellcolor[rgb]{ .706,  .776,  .906}99.9 \\\\\n       \\quad train on Mol-Instructions & \\cellcolor[rgb]{ .706,  .776,  .906}0.195 & \\cellcolor[rgb]{ .706,  .776,  .906}6.1 & \\cellcolor[rgb]{ .706,  .776,  .906}46.1 & \\cellcolor[rgb]{ .706,  .776,  .906}88.2 & \\cellcolor[rgb]{ .706,  .776,  .906}3.9 & \\cellcolor[rgb]{ .706,  .776,  .906}37.1 & \\cellcolor[rgb]{ .706,  .776,  .906}78.3 & \\cellcolor[rgb]{ .706,  .776,  .906}7.4 & \\cellcolor[rgb]{ .706,  .776,  .906}52.6 & \\cellcolor[rgb]{ .706,  .776,  .906}76.7 \\\\\n    \\bottomrule\n    \\end{tabular}%\n  \n\\end{threeparttable}\n}\n\\label{tab:b_2}%\n\\end{table*}%\n\nThe results in \\cref{tab:b_1} and \\cref{tab:b_2} lead to the following observations: \n(1) The ``w/o canonical'' model underperforms \\modelnamem on most tasks, with a substantial performance drop on FS and RS. This suggests that canonicalizing SMILES can reduce learning difficulty and improve performance. As canonicalization can be easily performed using fixed algorithms before feeding into models, we recommend using canonical SMILES when training and applying LLMs for chemistry.\n(2) While using SELFIES slightly improves the validity of generated molecules, which aligns with the motivation behind SELFIES \\citep{krenn2019selfies}, the validity of using SMILES is also sufficiently high. Moreover, using SELFIES results in worse performance on most tasks, possibly due to SELFIES being typically longer than SMILES, making it more difficult for the model to accurately understand and generate. Therefore, using SELFIES over SMILES may not be necessary, contrast to claims made in previous work \\citep{krenn2019selfies,fang2023mol}. \n(3) Despite using identical base models and training settings, the model trained on Mol-Instructions \\citep{fang2023mol} performs much worse than \\modelnamem trained on \\datasetname  even on the shared tasks (MC, MG, FS, and RS). This demonstrates the superiority of our dataset. A detailed comparison with Mol-Instructions can be found in \\cref{app:dataset_comparison}.\n\nTo gain deeper insights into the models' performance and behavior, we conduct further analytical experiments:\n(1) To investigate the synergistic effects among different tasks, we evaluate models trained on a single task and models with certain tasks removed.\nThe results demonstrate multiple-task training outperforms single-task training, indicating its benefits. However, each task generally does not heavily rely on the presence of other tasks, suggesting a degree of independence among them.\n(2) To investigate the influence of LoRA \\citep{hu2021lora} settings, we vary the involved LoRA modules. We observe that adding LoRA modules (and trainable parameters) leads to a substantial boost in performance, indicates the models' great potential for further improvements if with larger-scale fine-tuning.\nPlease refer to \\cref{app:analytical} for more details.\n\n\\section{\\newcontent{Conclusion}}\nWhile LLMs have shown promise as versatile assistants, their performance on chemistry-related tasks remains notably subpar. \nTo address this issue, we introduces \\datasetname, a large-scale, comprehensive, and high-quality instruction tuning dataset. It comprises 14 tasks highly relevant to real-world applications and contains over 3M rigorously curated samples. \nUsing \\datasetname, we develop \\modelname, a series of LLMs for performing chemistry tasks.\nOur experiments demonstrate \\modelname's superiority over existing LLMs, and highlight \\datasetname's crucial role in boosting the performance.\nFurther analytical experiments also provide significant insights towards developing LLMs for chemistry.\n\nHowever, this work has the following limitations.\nFirst, the evaluations for the MC and MG tasks cannot accurately assess models' abilities to generate chemically correct descriptions and molecules. Since the definition of molecular descriptions remain ambiguous and the available data is limited, it is challenging to assess whether the generated descriptions or molecules are accurate and correct.\nSecond, this work does not delve into the models' generalization capabilities beyond the trained tasks. While we recognize the importance of such capabilities, how to meaningfully test generalization abilities is nontrivial and needs careful design, which falls outside the purview of this work.\nThird, our models do not yet outperform SoTA task-specific models, possibly due to the small ratio of trainable parameters or suboptimal training procedures. \nNevertheless, we propose a high-quality instruction tuning dataset, demonstrate its effectiveness, and gain deeper insights, which we hope can be valuable for future research.\nWe will try to address the aforementioned limitations in our future work.\n\n\\section*{\\newcontent{Ethics Statement}}\nDespite our best efforts to maintain the high quality of the \\datasetname dataset and the integrity of the \\modelname models, we cannot guarantee that the dataset is free of inaccurate, incorrect, or harmful content, nor can we prevent the models from generating such content. Users should engage with our dataset and models at their own discretion and uphold the highest ethical standards in their use.\n\n\\section*{Acknowledgement}\n\nThe authors would thank colleagues from the OSU NLP group and the OSU Ning Lab for constructive feedback. This research was supported in part by NSF IIS-2133650, NIH 1R01LM014385-01, and NSF CAREER \\#1942980, as well as Ohio Supercomputer Center \\citep{OhioSupercomputerCenter1987}. The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notice herein.\n\n\\newpage\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{ChemDFM-X: Towards Large Multimodal Model for Chemistry}\n\n\\begin{document}\n\n\\ArticleType{RESEARCH PAPER}\n    \\Year{2024}\n    \\Month{}\n    \\Vol{}\n    \\No{}\n    \\DOI{}\n    \\ArtNo{}\n    \\ReceiveDate{}\n    \\ReviseDate{}\n    \\AcceptDate{}\n    \\OnlineDate{}\n\n\\title{ChemDFM-X: Towards Large Multimodal Model for Chemistry}{ChemDFM-X: Towards Large Multimodal Model for Chemistry}\n\n\\author[1]{Zihan Zhao}{}\n\\author[2]{Bo Chen}{}\n\\author[1,2]{Jingpiao Li}{}\n\\author[1,2]{Lu Chen}{chenlusz@sjtu.edu.cn}\n\\author[2]{Liyang Wen}{}\n\\author[1,2]{\\\\Pengyu Wang}{}\n\\author[1]{Zichen Zhu}{}\n\\author[1]{Danyang Zhang}{}\n\\author[2]{Ziping Wan}{}\n\\author[1]{\\\\Yansi Li}{}\n\\author[2]{Zhongyang Dai}{}\n\\author[2]{Xin Chen}{mail.xinchen@gmail.com}\n\\author[1,2]{Kai Yu}{kai.yu@sjtu.edu.cn}\n\n\\AuthorMark{Zihan Zhao}\n\n\\AuthorCitation{Zihan Zhao, Jingpiao Li, Bo Chen, et al}\n\n\\contributions{Zihan Zhao and Bo Chen have the same contribution to this work.}\n\n    \\address[1]{X-LANCE Lab, Department of Computer Science and Engineering\\\\ MoE Key Lab of Artificial Intelligence, SJTU AI Institute\\\\ Shanghai Jiao Tong University, Shanghai {\\rm 200240}, China}\n\\address[2]{Suzhou Laboratory, Suzhou {\\rm 215123}, China}\n\n    \\abstract{\n    Rapid developments of AI tools are expected to offer unprecedented assistance to the research of natural science including chemistry. However, neither existing unimodal task-specific specialist models nor emerging general large multimodal models (LMM) can cover the wide range of chemical data modality and task categories. To address the real demands of chemists, a cross-modal Chemical General Intelligence (CGI) system, which serves as a truly practical and useful research assistant utilizing the great potential of LMMs, is in great need. In this work, we introduce the first \\textbf{Cross}-modal \\textbf{D}ialogue \\textbf{F}oundation \\textbf{M}odel for \\textbf{Chem}istry (\\textbf{ChemDFM-X}). Diverse multimodal data are generated from an initial modality by approximate calculations and task-specific model predictions. This strategy creates sufficient chemical training corpora, while significantly reducing excessive expense, resulting in an instruction-tuning dataset containing 7.6M data. After instruction finetuning, ChemDFM-X is evaluated on extensive experiments of different chemical tasks with various data modalities. The results demonstrate the capacity of ChemDFM-X for multimodal and inter-modal knowledge comprehension. ChemDFM-X marks a significant milestone toward aligning all modalities in chemistry, a step closer to CGI.\n    }\n\n    \\keywords{Chemistry, Cross-Modality, LMM, Instruction-Tuning, AI for Science}\n\n    \\maketitle\n\n    \n\\section{Introduction}\nChemistry, as a naturally multimodal subject of science, plays a crucial role in various vital fields such as pharmaceutical research and material manufacturing. Therefore, research on AI for chemistry has garnered increasing attention. Despite the rapid development, most of the chemical AI models today mainly focus on single tasks with unimodal input~\\cite{chemberta,chemformer,shi2023relm,chen2023towards}. However, chemical data covers a wide range of modalities spanning from text description and molecular structure to image and spectrum, and chemical tasks take various forms ranging from property prediction to retrosynthesis. Although these unimodal specialist models can achieve state-of-the-art~(SOTA) performances in their individual tasks, they inherently can not handle tasks even slightly different from their own, or their respective tasks when there is a slight alteration in the input modality. Therefore, the practical utility and assistance of these models in research and manufacturing are limited.\n\nNowadays, large language models~(LLMs) and large multimodal models~(LMMs)~\\cite{mplugdocowl, internlmxcomposer,liu2024improved,gpt-4v,qwenvl,gemini} have achieved impressive performance in a number of challenging fields such as natural image inference~\\cite{gemini,liu2024improved,gpt-4v}, document analysis~\\cite{mplugdocowl, internlmxcomposer}, and medical image reasoning~\\cite{tu2023generalist,yang2024advancing}. Therefore, LMMs show great potential for building a cross-modal Chemical General Intelligence~(CGI) system~\\cite{zhao2024chemdfm}. However, most of the previous LMMs only focus on one non-text modality. Given the diversity of chemical modalities and the frequent co-occurrence of different modalities in practice, a single LMM that can handle multiple modalities is needed for CGI to truly meet the requirements of chemists. \n\nIn this work, we detail our progress toward such a cross-modal chemical LLM and propose \\textbf{ChemDFM-X}, a Cross-modal Dialogue Foundation Model for Chemistry that can comprehend and interpret data of various chemical modalities and fulfill many downstream tasks with the same set of model weights. As shown in Figure~\\ref{figure:overview}, ChemDFM-X takes advantage of the pre-trained parameters of the ChemDFM~\\cite{zhao2024chemdfm} model and is continuously trained on various multi-modality data.\nSpecifically, besides text and SMILES\\footnote{short for Simplified Molecular-Input Line-Entry System, a linear representation of chemical molecule structures} modalities already learned by ChemDFM, we choose five typical modalities that are both representative and meaningful in the field of chemistry.\n\nOne of the key challenges to achieving this goal is the absence of sufficient modality-aligned data. To address this problem, we propose to supplement data in other modalities by converting from SMILES. Noting that large-scale data are difficult to acquire, especially for the characterization modalities such as tandem mass spectra~(MS2) and infrared spectra~(IR), owing to the excessive expenditure of both experiments and quantum chemical calculation, we leverage simplified approximate calculation and numerical model prediction to obtain sub-optimal yet proximal results. In this way, we finally generate a multi-modal instruction-tuning dataset containing 7.6M cross-modality data from 1.3M seed SMILES.\n\nBenefiting from the instruction-tuning dataset, ChemDFM-X possesses the capabilities to comprehend and infer over various modalities including molecular graphs, conformations, images, and spectra.\nTo demonstrate the prowess of ChemDFM-X, we conduct extensive experiments regarding the newly added modalities. \nThe results demonstrate the strong capacity of ChemDFM-X for comprehending multi-modality input and exploiting inter-modality knowledge. Compared to conventional specialist models and LMMs which only enable one or none of the chemical modalities, ChemDFM-X manages to handle most common modalities well and agilely leverages the knowledge of chemical materials and reactions learned from all the modalities to solve various practical chemical tasks with superior performances.\nTo the best of our knowledge, ChemDFM-X is the first demonstration of a cross-modality chemical general intelligence system that can interpret chemical data of multiple different modalities with the same sets of parameters while handling a wide variety of tasks.\n\n\\begin{figure}[t]\n        \\centering\n        \\includegraphics[width=\\textwidth,trim=20 50 20 50,clip]{figures/ChemDFM.pdf}\n        \\caption{The overview of ChemDFM-X. The different modalities involved in chemical tasks are distinguished by colors. Structural modalities are marked with \\textcolor{blue}{\\bf blue}, figures are marked with \\textcolor{red}{\\textbf{orange}}, and spectra are marked with \\textcolor{green}{\\textbf{green}}. The text modality is marked with \\textcolor{purple}{\\textbf{purple}} in the input part and omitted in the output part. The dialogue-based free-form human-AI collaboration may involve any feasible modalities and is marked with \\textcolor{gray}{\\textbf{gray}}. For a detailed introduction to these modalities, please refer to Section~\\ref{sec:overview}.}\n        \\label{figure:overview}\n    \\end{figure}\n\\section{Related Work}\n\nAI for Chemistry has long been an active research area and has recently garnered increasing attention~\\cite{molformer, chemberta, honda2019smiles,graphcl, xia2023molebert, MGSSL, ying2021transformers,Denoising, 3dEMGP, zhou2023unimol, chen2023towards,shi2023relm,sagawa2023reactiont5, fang2023mol}. Due to the foundational role of molecules in the chemical world, the AI for Chemistry research primarily revolves around molecules~\\cite{molformer, chemberta, honda2019smiles,graphcl, xia2023molebert, MGSSL, ying2021transformers,Denoising, 3dEMGP, zhou2023unimol} and reactions~\\cite{chen2023towards,shi2023relm,sagawa2023reactiont5} which are composed of molecules. To better embed molecules into AI models, researchers mainly exploit three different molecular representations: 1) SMILES notations~\\cite{molformer,chemberta,honda2019smiles}, 2) 2D molecular graph~\\cite{graphcl,xia2023molebert,MGSSL,ying2021transformers}, and 3) 3D molecular conformations~\\cite{Denoising, 3dEMGP, zhou2023unimol}. In recent years, many works have incorporated different molecular representations together for better performance. \nDMP~\\cite{zhu2021dualview} encodes molecular SMILES and molecular graphs separately using a transformer and graph neural network. Through masked atom modeling and contrastive learning in both modalities, DMP pretrains the model to support inputs from both SMILES and molecular graphs. SGGRL~\\cite{Wang2024MultiModalRL} uses sequence-based, graph-based, and geometry-based encoders to obtain molecular representations containing diverse modal information. Then a readout layer captures crucial information from various modalities to acquire molecule-level representations, and a fusion layer integrates different modal representations using attention mechanisms to derive the ultimate molecular representation.\nAlthough the above task-specific specialist models are able to achieve advanced performance on their corresponding tasks, they suffer from poor task generalization capabilities, and can not be applied to other tasks. Therefore, their practical utility and value is limited.\n\nRecently, the emerging Large Language Models~(LLMs)~\\cite{touvron2023llama, openai2023gpt4} have shown great potential for general intelligence in the general domain. Considering the great task generalization capabilities and user interactivity of LLMS, it is also expected that LLMs can give important assistance to the research of chemistry. Both \\cite{zhao2024chemdfm} and \\cite{zhang2024chemllm} explore the methods to incorporate LLMs into the field of chemistry. Specifically, \\cite{zhao2024chemdfm} constructs a 34B-token domain-pretraining dataset and an instruction-tuning dataset containing 2.7M instructions. Based on these data, they specialize LLaMa-13B model~\\cite{touvron2023llama} and get ChemDFM, one of the first chemical LLMs. ChemDFM achieves promising performances across a wide range of chemical tasks and demonstrates strong free-form dialogue capability. In addition, \\cite{zhang2024chemllm} proposes a two-stage instruction-tuning pipeline and constructs a chemical LLM called ChemLLM. These chemical LLMs significantly broaden the application scope of chemical models. However, current chemical LLMs can only process pure text and SMILES notations which are also in the form of text. Considering that chemistry is intrinsically a multi-modal subject, the capabilities achievable by text-only LLMs are limited. Most recently, \\cite{li2024molm} proposes 3D-MoLM which is the first generalist LMM in the field of chemistry. However, their works are still in the early stages with relatively low performances and limited modalities.\n\nIn addition, Large Multimodal Models~(LMMs) in the general domain have been rapidly developing. The advanced LMMs~\\cite{mplugdocowl, internlmxcomposer,llava,gpt-4v,qwenvl,gemini} have achieved remarkable performance on a wide series of vision-language tasks. The promising capabilities of LMMs also enable their applications in more professional vertical domains. Taking the medical domain as an example, \\cite{tu2023generalist} propose Med-PaLM M, the first LMMs specialized in the medical domain, and show promising performance both in quantitative evaluation and qualitative evaluation. Additionally, \\cite{yang2024advancing} trained one model for each kind of modality respectively in the medical domain. However, as the models for each modality are separate, they cannot facilitate communication and collaboration between modalities.\n\nIn this work, we construct a cross-modal chemical LLM called ChemDFM-X that can comprehend and infer chemical data of multiple modalities with the same sets of parameters while handling a wide variety of tasks.\n\n\\section{ChemDFM-X}\n\nIn this section, we detail the methods to develop the ChemDFM-X model. We first give an overview of our training procedure~(Section~\\ref{sec:overview}), introducing what modalities are involved in ChemDFM-X training and how ChemDFM-X is trained. Then, we demonstrate the training details of each modality~(Section~\\ref{sec:2d}\\textasciitilde\\ref{sec:ir}), including data construction and modality encoder selection.\n\n\\subsection{Overview}\\label{sec:overview}\n\nThe overview of the structure and training paradigm of ChemDFM-X is shown in Figure~\\ref{figure:model}. Generally speaking, our ChemDFM-X incorporates the typical ``LLM decoder + modality encoder'' framework widely used by current LMMs~\\cite{liu2024improved,qwenvl}. Considering the significant differences in data formats among the different modalities, we incorporate separate modality encoders and corresponding projection modules for each modality. Through this ``separate encoders + unified decoder'' design, the separate encoders enable ChemDFM-X to obtain knowledge and information from different modalities, while the LLM decoder provides the capabilities to aggregate and analyze information from different modalities within a simple generative framework.\n\n\\paragraph{Modality Selection.} In the field of chemistry, there are primarily two kinds of modalities: \\emph{structural} modalities and \\emph{characterization} modalities.\n\nStructural modalities mainly directly represent the connections and\\slash or spatial arrangement of molecules and are usually used for reaction inference or theoretical calculation. Among the structural modalities, two modalities are introduced to ChemDFM-X, namely two-dimensional molecular graph and three-dimensional molecular conformation.\n\nOn the other hand, characterization modalities mainly imply the partial properties and substructure information of molecules. As the characterization results of molecules, their data is usually in the form of data point sequences with information implicitly hidden among them. One typical usage of characterization modality data is to identify unknown substances. There are many kinds of characterization methods in the field of chemistry. Due to the high expense of chemical experiments, the amount of real experimental data of these modalities is very limited, which significantly hinders the development of AI models. In this work, we manage to construct a great amount of tandem mass spectrum~(MS2) data and infrared spectrum~(IR) data through approximate calculation and model prediction, two of the most widely used characterization methods. ChemDFM-X is then trained on these two characterization modalities.\n\nApart from these four modalities, we also introduce the image modality, including molecular images and reaction images, to ChemDFM-X, as images are the most convenient data used by human researchers. Please refer to Figure~\\ref{figure:overview} for examples of these modalities.\n\n\\paragraph{Model Framework.} For the LLM decoder, we use one of the advanced chemical LLMs, namely ChemDFM, to leverage the promising chemical language and notation comprehension capability it has acquired. For the modality encoders, according to the current research status of specialist models for different modalities, we either select the advanced existing specialist models or retrain one ourselves as the modality encoders. To better align the output space of modality encoders with the input space of ChemDFM, we incorporate a separate projection module for each modality. Specifically, each of the projection modules is composed of two simple linear layers with a \\texttt{gelu} activation layer between them.\n\n\\paragraph{Training.} During the training process of each modal, we freeze the parameters of the pre-trained ChemDFM to maintain the advanced natural language and SMILES processing capabilities it has already acquired. Both the modality encoders and the projection modules are trained to get a better alignment between the encoder outputs and ChemDFM inputs.\n\nThe key hyperparameters we chose for each of the modalities are shown in Table~\\ref{tab:hyperparameter}. The training details of each non-text modality will be introduced in the following subsections, including the instruction-tuning dataset construction process and modality encoder for each modality.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth,trim=50 60 50 60,clip]{figures/model.pdf}\n    \\caption{Overview of ChemDFM-X model structure and training paradigm. The colors to mark different input modalities are aligned with Figure 1 in the main text.}\n    \\label{figure:model}\n\\end{figure}\n\n\\begin{table}\n    \\small\n    \\centering\n    \\begin{tabular}{c|ccccc}\n    \\toprule\n        Modality & Graph & Conformation & Image & MS2 & IR \\\\\n    \\midrule\n        Modality Encoder & Mole-BERT\\cite{xia2023molebert} & Uni-Mol~\\cite{zhou2023unimol} & CLIP~\\cite{radford2021learning} & Transformers & Transformers\\\\\n        Projector & 2-layer MLP & 2-layer MLP& H-Reducer & 2-layer MLP& 2-layer MLP\\\\\n        Encoding Dims & 300 & 512 & 1024 & 768 & 768 \\\\\n        \\multirow{2}{*}{\\# Modality Tokens} & \\# of Atoms & \\# of Atoms & 72 per sub-image & \\# of Peaks & 50 \\\\\n        & (Dynamic)& (Dynamic) & (Dynamic) & (Dynamic)& (Static) \\\\\n        Peak Learning Rate & 1e-5 & 2e-4 & 2e-3 & 2e-4 & 2e-4 \\\\\n        Total Batch Size & \\multicolumn{5}{c}{256} \\\\\n        Training Epochs & \\multicolumn{5}{c}{3} \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{Overview of the key hyperparameters of ChemDFM-X.}\n    \\label{tab:hyperparameter}\n\\end{table}\n\n\\subsection{Structural Modality: Molecular Graph}\\label{sec:2d}\n\nThe molecular graph is a typical structural modality for molecules. It only consists of the type of atoms in the molecule and the connections between them. Considering that the SMILES notation also contains these two kinds of information, the molecular graph can be easily obtained from the SMILES notation of the corresponding molecule according to simple rules. Formally, a molecular graph can be defined as an undirected graph $\\mathcal{G} = (\\mathbf{V}, \\mathbf{E})$, where $\\mathbf{V} = (v_1, v_2, \\dots, v_{|\\mathbf{V}|})$ is the atom set with $v_i$ representing the element type of the $i$-th atom and $\\mathbf{E} = (\\boldsymbol{e}_1, \\boldsymbol{e}_2, \\dots, \\boldsymbol{e}_{|\\mathbf{E}|})$ is the chemical bond set with $\\boldsymbol{e}_i=(v_m, v_n, b_i)$ indicating that a chemical bond of type $b_i$ exists between atoms $v_m$ and $v_n$.\n\n\\paragraph{Data Construction.}\nConsidering that SMILES notation can also be viewed as a special structural modality and the molecule graphs can be easily obtained from SMILES, the SMILES-related instruction-tuning dataset used by traditional Chemical LLM is naturally suitable for the training of the molecular graph modality. Therefore, we follow the dataset composition and collection procedure proposed by ChemDFM~\\cite{zhao2024chemdfm} to construct the instruction-tuning data for the molecular graph modality.\n\nSpecifically, the data we construct for the instruction tuning of molecular graph modality is mainly composed of the following:\n\\begin{itemize}\n    \\item \\textbf{Molecule description.} In this task, models are asked to describe the molecule based on the given molecular representation. The molecule-description pairs are collected from PubChem\\footnote{\\url{https://pubchem.ncbi.nlm.nih.gov/}}, a web-scale chemical database that contains more than 100M compounds. For each molecule-description pair, we construct two sets of data: one using the molecular graph alone as the molecular representation, and the other using both the molecular graph and the SMILES notation. To further increase the quality of this data, the high-quality molecule-description pairs (where the descriptions have more than two sentences) are repeated twice.\n    \\item \\textbf{Molecular property prediction.} In these tasks, models are asked to predict the properties of the given molecule based on its representation. The data are constructed from the most widely used molecular property prediction benchmark, Molecule Net~\\cite{moleculenet}. Similar to the molecule description dataset, we generate the molecular graphs based on the provided SMILES and construct two sets of data using the molecular graph alone or with SMILES.\n    \\item \\textbf{Reaction completion.} One of the most important usages of molecular graphs is reaction representation and inference. In this task, models are asked to complete an incomplete reaction where part of the involved molecules are missing. The source reactions are sampled from USPTO~\\cite{uspto}, the largest chemical reaction database. For each reaction, we randomly mask the reactant, reagent, or product, where the remaining molecules of the reaction are represented by molecular graph alone or with SMILES.\n    \\item \\textbf{Molecular graph recognition.} In these tasks, models are asked to recognize the molecule graph by providing the alternative representation of the corresponding molecule, such as SMILES, IUPAC\\footnote{short for International Union of Pure and Applied Chemistry} name, or molecular formula. The involved molecules are sampled from PubChem. Based on the molecules, we construct five sub-tasks: predicting the IUPAC name given the molecular graph alone or with SMILES, predicting the molecular formula given the molecular graph alone or with SMILES, and predicting the SMILES notation given the molecular graph alone.\n\\end{itemize}\n\n\\begin{table}\n    \\centering\n    \\begin{tabular}{ccccccc}\n    \\toprule\n        \\multirow{2}{*}{Data Type} & \\multirow{2}{*}{\\# Samples} & \\multirow{2}{*}{Data Source} & \\multicolumn{4}{c}{\\# Heavy Atoms in each Molecule} \\\\\n        & & & Max & Min & Average & Median \\\\\n    \\midrule\n        Molecule Description & 1152K & PubChem & 574 & 1 & 35.0 & 29 \\\\\n        Molecular Property Prediction & 203K & MoluculeNet & 222 & 1 & 24.0 & 22\\\\\n        Reaction Completion & 600K & USPTO & 273 & 1 & 23.3 & 22 \\\\\n        Molecular Graph Recognition & 30K & PubChem & 150 & 1 & 19.5 & 18\\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{Composition of the instruction-tuning dataset for Molecular Graph and the statistics of the molecules involved}\n    \\label{tab:data2d}\n\\end{table}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.6\\textwidth,trim=100 15 100 15,clip]{figures/sample_1.pdf}\n    \\caption{An example of the final structure of instruction tuning data.}\n    \\label{figure:sample1}\n\\end{figure}\n\nIt is worth noticing that we carefully remove all the data that may be present in the evaluations based on SMILES matching. For the prompt, we leverage the same dialogue format used by the ChemDFM, while rewriting the instruction of each task using GPT-4 to diversify the expressions.\nThe final composition and statistics of the instruction tuning dataset are demonstrated in Table~\\ref{tab:data2d} with a detailed example illustrated in Figure~\\ref{figure:sample1}.\n\n\\paragraph{Graph Encoder.} We utilize one of the most recent and advanced models, namely Mole-BERT~\\cite{xia2023molebert}, which specializes in 2D molecular graph understanding. The model is an encoder-decoder based graphic neural network (GNN) pre-trained on 2 million molecules\nand achieves promising performances on multiple downstream tasks.\nSpecifically, the Mole-BERT model follows an encoder-decoder architecture where the encoder and the decoder each consist of 5-layer graph isomorphism networks (GINs). We adopt its encoder as our molecular graph encoder and utilize the resulting node representations for each atom as the feature of the molecular graphs.\n\n\\subsection{Structural Modality: Molecular Conformation}\\label{sec:3d}\n\n\\begin{table}[t]\n    \\centering\n    \\begin{tabular}{ccccccc}\n    \\toprule\n        \\multirow{2}{*}{Data Type} & \\multirow{2}{*}{\\# Samples} & \\multirow{2}{*}{Data Source} & \\multicolumn{4}{c}{\\# Heavy Atoms in each Molecule} \\\\\n        & & & Max & Min & Average & Median \\\\\n    \\midrule\n        Molecule Description & 1152K & PubChem & 574 & 1 & 35.0 & 29 \\\\\n        Molecular Property Prediction & 203K & MoluculeNet & 222 & 1 & 24.0 & 22\\\\\n        Reaction Completion & 600K & USPTO & 273 & 1 & 23.3 & 22\\\\\n        Molecular Conformation Recognition & 30K & PubChem & 150 & 1 & 19.5 & 18\\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{Composition of the instruction-tuning dataset for Molecular Conformation and the statistics of the molecules involved}\n    \\label{tab:data3d}\n\\end{table}\n\nThe molecular conformation is another typical structural modality for molecules. Compared with molecular graphs, molecular conformations additionally contain the coordinate information of each atom of the molecules, reflecting the position information of atoms in the molecule. Therefore, molecular conformations usually contain more information than molecular graphs and SMILES notations. Formally, a molecular conformation can be viewed as a similar undirected graph to molecular graphs $\\mathcal{G} = (\\mathbf{V}^\\prime, \\mathbf{E})$ with $\\mathbf{V}^\\prime = (\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\dots, \\boldsymbol{v}_{|\\mathbf{V}|})$ and $\\boldsymbol{v}_i=(x_i, y_i, z_i, a_i)$ where $(x_i, y_i, z_i)$ is the Cartesian coordinates of the $i$-th atom and $a_i$ denotes the element type of the $i$-th atom.\n\n\\paragraph{Data Construction.} We construct the instruction-tuning dataset for molecular conformation modality based on the dataset used for molecular graph modality. Specifically, we utilize the same source data as molecular graphs while replacing all the molecular graphs in the inputs with molecular conformations. We also use the same input format while instruction is rewritten specifically for molecular conformations.\nThe final composition and statistics of the instruction tuning dataset are demonstrated in Table~\\ref{tab:data3d}\n\nHowever, it is worth noticing that obtaining molecular conformations from SMILES is a lot more complicated than obtaining molecular graphs from SMILES. Ideally, rigorous calculations based on quantum chemistry are needed to acquire the ``optimal'' conformations where the potential energy of the molecule is the lowest. However, quantum chemistry calculation can be very costly, especially for molecules with a large amount of atoms. To tackle this problem, we follow the same approximate calculation process used by Uni-Mol~\\cite{zhou2023unimol} to obtain the approximated conformations under optimization algorithms. Specifically, we first generate the original molecular conformations using RDKit~\\cite{rdkit}. Then, the molecular conformation is optimized using the Merck Molecular Force Field (MMFF)~\\cite{halgren1996merck} algorithm, and the pseudo-optimal conformation for the molecule is obtained.\n\n\\paragraph{Conformation Encoder.}\nWe adopt one of the famous large-scale molecular conformation representation models, Uni-Mol~\\cite{zhou2023unimol}, as our 3D molecular conformation encoder. It is a transformer-based 3D molecular pre-trained model derived from 19M molecules and 209M molecular conformations. It achieved promising results in various downstream tasks, especially in 3D spatial tasks.\n\n\\subsection{Image Modality}\\label{sec:fig}\n\nDuring the work of human chemists, images, rather than structured molecular graphs and conformations, are the most used modality through which they perceive and express molecules and reactions. Therefore, it is vital for the chemical LLM to possess image comprehension capability so that the chemist can communicate with them more conveniently.\n\n\\paragraph{Data Construction.} We utilize the same data sources as molecular graphs and molecular conformations to construct the instruction-tuning dataset for the image modality by replacing the non-text molecular representation with images and generating corresponding instructions.\n\nMoreover, to diversify the styles of images, we employed three different methods for the generation of molecular images. Firstly, we directly use two different toolkits namely RDKit~\\cite{rdkit} and Indigo~\\cite{indigo}. We further conduct traditional image augmentation methods, such as random grayscale conversion and random color jitter, on the resulting images. Besides, to familiarize ChemDFM-X with the handwritten style images, we utilize the pipeline proposed in \\cite{chempix} to generate pseudo-handwritten images. The comparison of the three styles of images is illustrated in Figure~\\ref{figure:styles}.\n\nIn addition to the existing tasks that revolve around molecule images, we add a new task focusing on the reaction images. Specifically, we randomly sample 300K reactions from the USPTO database and generate the images of the reactions using the RDKit toolkit. Based on the reaction image alone, the model is asked to recognize the reaction by providing the SMILES notation of the whole reaction.\n\nThe final composition and statistics of the instruction tuning dataset are demonstrated in Table~\\ref{tab:dataim}.\n\n\\begin{figure}[!t]\n\t\\centering\n\t\\subfloat[RDKit Style Image]{\n\t\t\\includegraphics[width=0.3\\linewidth,trim=0 0 0 0,clip]{figures/style1.png}}%trim\n\t\\subfloat[Indigo Style Image]{\n\t\t\\includegraphics[width=0.3\\linewidth,trim=0 0 0 0,clip]{figures/style2.png}}%trim\n\t\\subfloat[Handwritten Style Image]{\n\t\t\\includegraphics[width=0.3\\linewidth,trim=0 0 0 0,clip]{figures/style3.png}}%trim\n\t\\caption{Examples of three molecular styles.}\n\t\\label{figure:styles}\n\\end{figure}\n\n\\begin{table}\n    \\centering\n    \\begin{tabular}{ccccccc}\n    \\toprule\n        \\multirow{2}{*}{Data Type} & \\multirow{2}{*}{\\# Samples} & \\multirow{2}{*}{Data Source} & \\multicolumn{4}{c}{\\# Heavy Atoms in each Molecule} \\\\\n        & & & Max & Min & Average & Median \\\\\n    \\midrule\n        Molecule Description & 1152K & PubChem & 574 & 1 & 35.0 & 29 \\\\\n        Molecular Property Prediction & 204K & MoluculeNet & 222 & 1 & 24.0 & 22 \\\\\n        Reaction Completion & 600K & USPTO & 273 & 1 & 23.3 & 22 \\\\\n        Molecular Image Recognition & 30K & PubChem & 150 & 1 & 19.5 & 18 \\\\\n        Reaction Image Recognition & 300K & USPTO & 440 & 1 & 23.7 & 22 \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{Composition of the instruction-tuning dataset for Image modality and the statistics of the molecules involved}\n    \\label{tab:dataim}\n\\end{table}\n\n\\paragraph{Image encoder.} There is no previous work that specifically targeted both molecule images and reaction images. Therefore, we employ CLIP~\\cite{radford2021learning}, the image encoder widely used in general domain LMMs. The original CLIP model can only process images with resolutions of $224\\times224$ or $336\\times336$. It is far from being sufficient for chemical images, especially for images of large molecules and reactions. Pre-experiments show that forcibly compressing images to these resolutions results in the model learning almost nothing. To handle this problem, we follow the method proposed by \\cite{llava}. Specifically, instead of directly resizing the images of original resolution, we crop them into multiple sub-images that fit the input size of CLIP and then leverage a specific module named H-Reducer~\\cite{hu2024mplug} to reduce the output sequence length of CLIP by a factor of $n$. In our cases, $n$ is set to $8$.\n\n\\subsection{Characterization Modality: MS2 Spectrum}\\label{sec:ms}\n\n\\begin{table}[t]\n    \\centering\n    \\begin{tabular}{ccccccc}\n    \\toprule\n        \\multirow{2}{*}{Data Type} & \\multirow{2}{*}{\\# Samples} & \\multirow{2}{*}{Data Source} & \\multicolumn{4}{c}{\\# Heavy Atoms in each Molecule} \\\\\n        & & & Max & Min & Average & Median \\\\\n    \\midrule\n        MS2 spectrum recognition & 21K & PubChem & 99 & 1 & 18.3 & 18\\\\\n        Molecular Property Prediction & 90K & MoluculeNet & 135 & 1 & 23.4 & 22 \\\\\n        Reaction Related MS2 Identification &  224K & USPTO & 131 & 1 & 23.5 & 22\\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{Composition of the instruction-tuning dataset for MS2 Spectrum and the statistics of the molecules involved}\n    \\label{tab:datams}\n\\end{table}\n\nMolecular graphs, molecular conformations, and molecular images provide information about the structure of known molecules, however, numerous molecules of unknown structure exist in nature or may appear during experiments. In this context, spectroscopy becomes one of the key tools to determine the identity of a molecule. Mass spectrum is one of the common types of spectra, which is based on the principle that the molecule is ionized by an ion source and then cleaved into ion fragments with different mass-to-charge ratios.\\footnote{\\url{https://en.wikipedia.org/wiki/Mass_spectrum}} The mass spectrum consists of a series of ion peaks, where the horizontal axis represents the mass-to-charge ratio of the ion fragments and the vertical axis represents the intensity of the peaks. Formally, the MS2 spectrum data can be represented by a tuple sequence $\\mathbf{M} = ((r_1, I_1), (r_2, I_2), \\dots, (r_n, I_n))$, where $(r_i, I_i)$ denotes the $i$-th peak, $r_i$ denotes the mass-to-charge ratio of the $i$-th peak, $I_i$ denotes the intensity of the $i$-th peak, and $n$ represents the total number of peaks in the MS2 spectrum. By introducing mass spectrum data, the ability to deal with unknown molecules can be added to the chemical LLM.\n\n\\paragraph{Data Construction.} As a typical kind of characterization modalities, the tasks suitable for MS2 spectra differ from those for the structural modalities. Therefore, to construct the instruction-tuning dataset for MS2 spectra, we select a small subset of tasks that are suitable for MS2 spectra from our instruction-tuning dataset for structural modalities, and supplement them with tasks specific to characterization modalities. Specifically, the instruction-tuning dataset for MS2 spectra is composed of the following tasks:\n\\begin{itemize}\n    \\item \\textbf{MS2 spectrum recognition.} We utilize the same data source as the molecular graph recognition task introduced in Section~\\ref{sec:2d} with the molecular graph replaced by the MS2 spectra of the corresponding molecule.\n    \\item \\textbf{Molecular property prediction.} The constructing process is also akin to that of the molecular graph modality. However, different from molecular graphs, it does not make sense to predict the molecular properties using MS2 spectra alone. Therefore, we only instruct the model to conduct the prediction based on the MS2 spectra along with the SMILES notations.\n    \\item \\textbf{Reaction related MS2 spectrum identification.} In practice, one of the most important usages of the spectra is to identify unknown molecules in the context of a reaction. \n    In such scenarios, the identity of the unknown molecules should be inferred in accordance with not only the spectra, but also the a priori information of the reaction, like the known reactants or the detected products.\n    Therefore, the capability to identify the MS2 spectrum with the help of the reaction related to the molecule is also important and valuable for chemical LMMs. To construct the data for such tasks, we reuse the data for reaction completion tasks in the instruction-tuning datasets of structural modalities. The incomplete reactions will be represented by SMILES notations, while the MS2 spectra of the missing substances are also provided to the model. Based on this information, the model is instructed to identify the corresponding molecules by providing the SMILES of the molecules.\n\\end{itemize}\n\nAnother great challenge for the MS2-spectrum modality is the great expense associated with obtaining real experimental data due to the significant human labor required throughout the experimental process. Therefore, large-scale experimental MS2 spectra data is absent. To tackle this problem, we use a prediction-based approach to generate mass spectrum data in large batches. The prediction tool we used is CFM-ID 4.0~\\cite{cfmid4.0}, which provides a method for accurate and efficient prediction of molecular MS2 spectra by creating a probabilistic graphical generative model for the MS2 fragmentation process through competitive fragmentation modeling, and adjusting the model parameters from the data through machine learning algorithms. With CFM-ID 4.0, we obtain a total of about 700K tandem mass spectra with their corresponding SMILES to form the instruction tuning dataset for for training the encoder of MS2 spectrum modality. The final composition and statistics of the instruction tuning dataset are demonstrated in Table~\\ref{tab:datams}\n\n\\paragraph{MS2 Sequence Transformer.}\n\nSince there is little work on encoding MS2 spectra, we choose to train a new spectrum encoder for MS2. Considering the data format of MS2 spectra is point sequences and one of the classical applications of the spectrum is structural inference~\\cite{Demartini13}, we train an encoder-decoder manner transformer model with the pertaining task designed as a spectral inference task. Specifically, the model is trained to generate the SMILES notation of the molecule corresponding to the input MS2 spectrum. We build the codebook of the tokens based on the transverse coordinates and then acquire the input token list from the ion peaks of the MS2 spectrum using the codebook. After the model is trained, we use its encoder module as the encoder for the MS2 spectrum modality.\n\n\\subsection{Characterization Modality: IR Spectrum}\\label{sec:ir}\n\nSimilar to the MS2 spectrum, the IR spectrum also contains rich chemical information about the molecule and is one of the most important means to characterize the molecular structure. The principle of the infrared spectrum is that a molecule can selectively absorb infrared rays of certain wavelengths, causing its own vibrational and rotational energy levels transitions, which is related to the detailed structure of molecules\\footnote{\\url{https://en.wikipedia.org/wiki/Infrared_spectroscopy}}. The infrared spectra of molecules can be obtained through the detection of infrared absorption. The horizontal coordinate of the infrared spectrum indicates the wavelength of the infrared ray, and the vertical coordinate generally implies the absorption intensity. Formally, the IR spectrum can be represented by a point sequence $\\mathbf{R} = ((w_1, t_1), (w_2, t_2), \\dots, (w_l, t_l))$, where $w_i$ denotes the wave length of the light, $t_i$ is the absorption intensity of the corresponding light.\n\n\\begin{table}\n    \\centering\n    \\begin{tabular}{ccccccc}\n    \\toprule\n        \\multirow{2}{*}{Data Type} & \\multirow{2}{*}{\\# Samples} & \\multirow{2}{*}{Data Source} & \\multicolumn{4}{c}{\\# Heavy Atoms in each Molecule} \\\\\n        & & & Max & Min & Average & Median \\\\\n    \\midrule\n        IR spectrum recognition & 30K & PubChem & 150 & 1 & 19.5 & 18\\\\\n        Molecular Property Prediction & 102K & MoluculeNet & 222 & 1 & 24.0 & 22\\\\\n        Reaction Related IR Identification & 300K & USPTO & 440 & 1 & 22.9 & 22\\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{Composition of the instruction-tuning dataset for IR Spectrum and the statistics of the molecules involved}\n    \\label{tab:datair}\n\\end{table}\n\n\\paragraph{Data Construction.} We construct the instruction-tuning dataset for the IR-spectrum modality based on the dataset used for the MS2-spectrum modality. Specifically, we utilize the same source data while replacing all the MS2 spectra with IR spectra with corresponding modifications of the instructions.\n\nSimilar to MS2 spectra, IR spectra also suffer from the high cost of the experimental data. To make things even worse, the expense of IR spectra calculation is also beyond acceptable. Because the simulation of molecular infrared spectra needs to go through the time-consuming DFT process and the large-volume infrared computation puts higher demands on the arithmetic power. Based on the above considerations, we adopt a neural network prediction-based approach, Chemprop-IR~\\cite{Chemprop-IR}, to generate IR data. We generate about 1M IR spectra in total to form the instruction-tuning dataset for training the encoder of IR spectrum modality. The final composition and statistics of the instruction tuning dataset are demonstrated in Table~\\ref{tab:datair}\n\n\\paragraph{IR Sequence Transformer.}\n\nSimilar to the situation of MS2 spectra, there is little work associated with the encoding of IR spectra. Therefore, we choose to train a new spectrum encoder for IR. We used the same setup as the mass spectrum including the model structure and the pertaining task. The data in the IR spectrum are generally denser than MS2 spectra, as the MS2 spectra data are composed of only the peaks while the IR spectra are composed of all the data points. Therefore, instead of the codebook tokenization strategy used for MS2 spectra, we directly construct the input features by reshaping the sequence of absorption intensity. After the model is trained, we use its encoder module as the encoder for the IR spectrum modality.\n\n\\section{Evaluation}\n\nOur evaluations are mainly conducted based on ChemLLMBench~\\cite{guo2023what}, one of the most widely used benchmarks designed for the evaluation of LLMs in the field of chemistry. However, considering the intrinsic differences across the modalities, the types of tasks suitable for each modality may differ. For example, the information contained in characterization modalities is often more obscure and scattered compared to that in structural modalities, therefore, it makes less sense to ask models to generate molecular captions based on the molecular spectra. Therefore, to thoroughly and specifically evaluate ChemDFM-X's capabilities across different modalities, we construct separate evaluation task sets for each modality by both selecting existing tasks from ChemLLMBench and building new tasks. In this section, we will introduce the evaluation tasks and analyze model performances for each kind of modality separately.\n\n\\subsection{Structural Modalities}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/ChemDFM-eval1.pdf}\n    \\caption{Evaluation tasks for structural modalities}\n    \\label{figure:eval1}\n\\end{figure}\n\n\\begin{table*}\n    \\centering\n    \\setlength\\tabcolsep{5pt}\n    \\begin{tabular}{lcccccccc}\n    \\toprule\n    \\multirow{2.1}{*}{Model} & $\\text{MR}^*$ & \\multicolumn{3}{c}{Molecule Captioning} & \\multicolumn{2}{c}{RP} & \\multicolumn{2}{c}{Retrosynthesis} \\\\\n    \\cmidrule(rl){2-2} \\cmidrule(rl){3-5} \\cmidrule(rl){6-7} \\cmidrule(rl){8-9}\n    & \\small{Acc} & \\small{BLEU-2} & \\small{BLEU-4} & \\small{METEOR} & \\small{Acc} & \\small{Validity} & \\small{Acc} & \\small{Validity} \\\\\n    \\midrule\n    \\rowcolor{grey}\\multicolumn{9}{c}{\\textit{specialist models}} \\\\\n    Text+Chem T5 (S)~\\cite{christofidellis2023unifying} & - & 62.5 & 54.2 & 64.8 & - & - & - & - \\\\\n    MolCA (S + G)~\\cite{liu2023molca} & - & 63.9 & 55.5 & 66.9 & - & - & - & - \\\\\n    Chemformer (S)~\\cite{chemformer} & - & - & - & - & 93.8 & 100 & 53.6 & 100 \\\\\n    \\midrule\n    \\rowcolor{grey}\\multicolumn{9}{c}{\\textit{generalist models}} \\\\\n    ChemLLM-7B-Chat (S)~\\cite{zhang2024chemllm} & - & 14.5 & 7.4 & 22.2 & 18.0 & 94.0 & 10.0 & 91.0 \\\\\n    ChemDFM-v1.0 (S)~\\cite{zhao2024chemdfm} & - & 26.1 & 20.6 & 33.1 & 49.0 & 98.0 & 12.0 & \\textbf{99.0} \\\\\n    3D-MoLM (S + C)~\\cite{li2024molm} & 0 & 18.2 & 9.9 & 25.6 & $\\text{-}^\\dag$ & - & - & -  \\\\\n    \\multirow{1}{*}{\\textbf{ChemDFM-X (G)}} & 41.0 & 19.3 & 13.2 & 26.2 & 29.0 & 94.0 & 9.0 & 94.0 \\\\\n    \\multirow{1}{*}{\\textbf{ChemDFM-X (S + G)}} & - & \\underline{26.6} & \\underline{21.1} & \\underline{33.8} & 48.0 & \\textbf{99.0} & \\textbf{18.0} & 98.0 \\\\\n    \\multirow{1}{*}{\\textbf{ChemDFM-X (C)}} & 66.0 & 26.4 & 20.7 & 33.6 & \\underline{53.0} & \\textbf{99.0} & 8.0 & 97.0 \\\\\n    \\multirow{1}{*}{\\textbf{ChemDFM-X (S + C)}} & - & \\textbf{27.8} & \\textbf{22.0} & \\textbf{34.7} & \\textbf{54.0} & \\textbf{99.0} & \\underline{16.0} & \\textbf{99.0} \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{The Results of multiple evaluation tasks, including Molecule Recognition~(MR), Molecule Captioning, Reaction Prediction~(RP), and Retrosynthesis, for the structural modalities. The metric validity evaluates whether the output SMILES is valid. The content within the parentheses indicates the molecular representation modalities used for the corresponding model (S - SMILES, G - Molecular Graphs, C - Molecular Conformations). *: The SMILES notations are not included in the inputs of this task to prevent cheating. $\\dag$: 3D-MoLM does not support the reaction-related tasks as the input of 3D-MolM can not contain more than one molecular conformation. Among the generalist models, the optimal results are \\textbf{bolded}, and the second-best results are \\underline{underlined}.}\n    \\label{tab:strcut-other}\n\\end{table*}\n\\begin{table*}\n    \\centering\n    \\begin{tabular}{lcccccc}\n    \\toprule\n    Model & BACE & BBBP & Clintox & HIV & Tox21 & Avg \\\\\n    \\midrule\n    \\rowcolor{grey}\\multicolumn{7}{c}{\\textit{specialist models}} \\\\\n    Mole-BERT (G)~\\cite{xia2023molebert} & 80.8 & 71.9 & 78.9 & 78.2 & 76.8 & 77.3 \\\\\n    Uni-Mol (C)~\\cite{zhou2023unimol} & 85.7 & 72.9 & 91.9 & 80.8 & 79.6 & 82.2 \\\\\n    MolXPT (S)~\\cite{liu-etal-2023-molxpt} & 88.4 & 80.0 & 95.3 & 78.1 & 77.1 & 83.8 \\\\\n    MolCA (S + G)~\\cite{liu-etal-2023-molca} & 79.8 & 70.0 & 89.5 & 64.5 & 77.2 & 76.2 \\\\\n    \\midrule\n    \\rowcolor{grey}\\multicolumn{7}{c}{\\textit{generalist models}} \\\\\n    ChemLLM-7B-Chat (S)~\\cite{zhang2024chemllm} & \\textbf{83.2} & 62.8 & 68.5 & 73.8 & 70.1 & 71.7 \\\\\n    ChemDFM-v1.0 (S)~\\cite{zhao2024chemdfm} & 78.4 & \\textbf{66.7} & \\underline{89.9} & 73.6 & \\textbf{79.8} & \\underline{77.7} \\\\\n    3D-MoLM (S + C)~\\cite{li2024molm} & 50.9 & 48.0 & 47.0 & 53.2 & 49.4 & 49.7\\\\\n    \\multirow{1}{*}{\\textbf{ChemDFM-X (G)}} & 66.6 & 65.2 & 45.5 & 70.8 & 79.0 & 65.4 \\\\\n    \\multirow{1}{*}{\\textbf{ChemDFM-X (S + G)}} & 76.1 & \\underline{65.6} & \\textbf{92.5} & \\underline{75.3} & 79.3 & \\textbf{77.8} \\\\\n    \\multirow{1}{*}{\\textbf{ChemDFM-X (C)}} & \\underline{80.5} & 65.4 & 71.9 & 75.2 & 79.1 & 74.4 \\\\\n    \\multirow{1}{*}{\\textbf{ChemDFM-X (S + C)}} & 79.0 & 63.3 & 89.5 & \\textbf{76.4} & \\underline{79.5} & 77.5 \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{The Results of molecule property prediction tasks for the structural modalities. We report the performance of the models in AUC-ROC, which stands for the Area Under the Curve of the Receiver Operating Characteristic. The content within the parentheses indicates the molecular representation modalities used for the corresponding model (S - SMILES, G - Molecular Graphs, C - Molecular Conformations). Among the generalist models, the optimal results are \\textbf{bolded}, and the second-best results are \\underline{underlined}.}\n    \\label{tab:struct-molnet}\n\\end{table*}\n\n\\paragraph{Evaluation Tasks.} Considering that SMILES can be viewed as a special structural modality and ChemLLMBench is originally designed for evaluating LLMs' SMILES comprehension capabilities, all the tasks in ChemLLMBench should be suitable for the evaluation of molecular graph and molecular conformation modalities. Specifically, the evaluation tasks we utilize for both structural modalities are as follows:\n\\begin{itemize}\n    \\item \\textbf{Molecule Recognition.}\\footnote{Corresponding to the name prediction tasks in \\cite{guo2023what}} There are four types of molecule recognition tasks in ChemLLMBench. However, to focus tightly on evaluating non-text modality comprehension capabilities, we only source data of IUPAC to SMILES task and construct a new task where models need to recognize the molecular graphs or conformations of the molecule and give its SMILES notation.\n    \\item \\textbf{Molecule Captioning.} This task requires models to generate a brief description of the given molecules. In ChemLLMBench, the molecule captioning dataset is composed of 100 instances sampled from the test set of CHEBI-20~\\cite{edwards-etal-2021-text2mol}. Instead of directly using this dataset provided by ChemLLMBench, we evaluate the models on the full test set of CHEBI-20 for a more robust assessment.\n    \\item \\textbf{Molecule Property Prediction.} These tasks ask models to predict the properties of the given molecules. To construct such tasks, ChemLLMBench extracts five typical classification task sets (\\textit{i.e.}, BACE, BBBP, Clintox, HIV, and Tox21) from Molecular Net~\\cite{moleculenet}, the widely used property prediction benchmark, and randomly sampled 100 molecule-property pairs from each task. In our evaluation, we use the same five task sets. However, to make the tasks more difficult, instead of randomly sampling, we incorporate a more challenging dataset split using the DeepChem library~\\cite{deepchem}.\n    \\item \\textbf{Reaction Prediction and Retrosynthesis.} These two tasks are similar in form but differ significantly in difficulty. The reaction prediction task asks models to predict the product of a chemical reaction given its reactants and reagents, which is relatively straightforward and easy. On the other hand, the retrosynthesis task asks models to predict the reactants of a reaction given its product. In our evaluation, we directly utilize the corresponding dataset provided by ChemLLMBench to evaluate these capabilities.\n\\end{itemize}\n\nAn intuitive diagram demonstrating the input and output of the above tasks is shown in Figure~\\ref{figure:eval1}.\n\n\\paragraph{Baselines.} We leverage three types of models as our baselines. \n\nThe most relevant baseline in the structure modalities is 3D-MoLM~\\cite{li2024molm}, which is a generalist modal that can process both SMILES and molecular conformations\\footnote{They also trained a 2D-MoLM which can process SMILES and molecular graphs, however, the parameter of 2D-MoLM is not yet open-sourced.}. The primary distinctions between ChemDFM-X and 3D-MoLM can be summarized as follows: First, ChemDFM-X employs a non-compressive MLP projector, whereas 3D-MoLM uses a Q-Former module which usually leads to information losses and requires more training~\\cite{yao2024decodecouplingtokencompression}. Second, 3D-MoLM supports only a single molecular conformation per input, preventing it from addressing tasks involving multiple molecules, such as reaction-related problems and multi-turn dialogues. In contrast, ChemDFM-X does not suffer from this intrinsic drawback as it is designed to handle the input with multiple molecular conformations in a single prompt. Furthermore, it is also worth noticing that ChemDFM-X also supports other modalities beyond molecular conformation, which 3D-MoLM does not.\n\nThe second type of baselines is other generalist models in the field of chemistry. Specifically, we use ChemDFM~\\cite{zhao2024chemdfm} and ChemLLM~\\cite{zhang2024chemllm}, two pioneer works in the field of LLM for chemistry. They excel at SMILES comprehension, but can not understand any other chemical modalities.\n\nFinally, we also list the advanced performances of task-specific specialist models. They usually can achieve excellent performance on the targeted tasks while having zero performance on the others.\n\n\\paragraph{Results and Analysis.} The experimental results of the structural modalities are illustrated in Table~\\ref{tab:strcut-other} and \\ref{tab:struct-molnet}. The results in Table~\\ref{tab:strcut-other} show that our ChemDFM-X possesses the capability to comprehend and infer over molecular graph modality and molecular conformation modality. ChemDFM-X outperforms the baseline generalist LMM, 3D-MoLM, in all the settings while achieving better or comparable performance compared with the state-of-the-art~(SOTA) performances of generalist models. Specifically, ChemDFM-X achieves the new SOTA method among the generalist models when providing both SMILES notations and molecular conformations.\n\nHowever, it is worth noticing that ChemDFM-X performs relatively better with molecular conformations compared with molecular graphs. On the one hand, from a chemical perspective, molecular conformations intrinsically contain more information than molecular graphs. Therefore, the potential of molecular conformation modality as a molecular representation is greater. On the other hand, ChemDFM-X (G) only achieves 14\\% accuracy in the molecular recognition tasks, showing that the molecular graph modality may still remain undertrained in ChemDFM-X.\n\nAnother promising result is that the performances of ChemDFM-X in reaction-related tasks have improved significantly compared with those with only SMILES inputs. This result is consistent with the chemical intuition that molecular graphs and conformations are suitable and commonly used for chemical reaction inference. % by human reseachers.\n\nAs for the molecular property prediction tasks (Table~\\ref{tab:struct-molnet}), the molecular graph modality and the molecular conformation modality do not have a remarkable influence on the performances.\nIt is worth noticing that all the involved tasks are about high-level biological properties, such as blood-brain barrier penetration and toxicity, and there exists a remarkable overlap in the information between structural modalities including SMILES. Therefore, we attribute the reason to that the molecular graphs and confirmations fail to provide sufficient complementary information to enhance the molecule modeling for these tasks. Thus, to further increase the performance of these tasks, more comprehensive training or more powerful molecular representations are needed.\n\n\\subsection{Image Modality}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.75\\linewidth]{figures/ChemDFM-eval2.pdf}\n    \\caption{Evaluation tasks for image modality}\n    \\label{figure:eval2}\n\\end{figure}\n\n\\paragraph{Evaluation Tasks.} Considering that image modality is the most convenient modality for human researchers, our evaluations of image modality focus more on whether models can recognize the given image. Specifically, we evaluate both the molecule image recognition capability and the reaction image recognition capability of the models. The molecule image recognition task is intuitively constructed from the molecule recognition dataset used by structural modalities. As for the reaction image recognition task, we randomly sample 100 reactions from the USPTO-50K dataset~\\cite{uspto} to form its test set. Besides, we also evaluate the molecule captioning and molecule property prediction tasks with the image modality inputs. An intuitive diagram demonstrating the input and output of the evaluation tasks for image modality is shown in Figure~\\ref{figure:eval2}.\n\n\\begin{table*}\n    \\centering\n    \\setlength\\tabcolsep{5pt}\n    \\begin{tabular}{lccccc}\n    \\toprule\n    \\multirow{2.1}{*}{Model} & \\multicolumn{2}{c}{$\\text{Image Recognition}^*$} & \\multicolumn{3}{c}{Molecule Captioning} \\\\\n    \\cmidrule(rl){2-3}  \\cmidrule(rl){4-6} \n    & \\small{Molecules (Acc)} & \\small{Reaction (Acc)} & \\small{BLEU-2} & \\small{BLEU-4} & \\small{METEOR} \\\\\n    \\midrule\n    \\rowcolor{grey}\\multicolumn{6}{c}{\\textit{specialist models}} \\\\\n    MolNextr (I)~\\cite{chen2024molnextr} & 82.0 & -& -& -& -\\\\ \n    MolScribe (I)~\\cite{qian2023molscribe} & 84.0 & -& -& -& -\\\\\n    \\midrule\n    \\rowcolor{grey}\\multicolumn{6}{c}{\\textit{generalist models}} \\\\\n    ChemLLM-7B-Chat (S)~\\cite{zhang2024chemllm} & - & - & 14.5 & 7.4 & 22.2 \\\\\n    ChemDFM-v1.0 (S)~\\cite{zhao2024chemdfm} & - & - & \\underline{26.1} & \\underline{20.6} & \\underline{33.1} \\\\\n    GPT-4O (S + I)~\\cite{gpt-4v} & 0 & 0 & 7.0 & 0.4 & 15.0 \\\\\n    Gemini-1.5-pro (S + I)~\\cite{gemini} & 23.0 & 0 & 8.7 & 2.3 & 14.5 \\\\\n    Qwen-VL-max (S + I)~\\cite{qwenvl} & 0 & 0 & 5.9 & 0.1 & 15.0 \\\\\n    LLaVA-v1.6-Vicuna-13B (S + I)~\\cite{llava} & 0 & 0 & 4.4 & 0.0 & 10.7\\\\\n    InternLM-XComposer2 (S + I)~\\cite{internlmxcomposer} & 0 & 0 & 9.2 & 0.0 & 18.3 \\\\\n    DocOwl-1.5-Omni (S + I)~\\cite{hu2024mplug} & 0 & 0 & 10.0 & 1.6 & 17.9 \\\\\n    \\multirow{1}{*}{\\textbf{ChemDFM-X (I)}} & 46.0 & 53.0 & 23.6 & 18.0 & 30.7 \\\\\n    \\multirow{1}{*}{\\textbf{ChemDFM-X (S + I)}} & - & - & \\textbf{27.2} & \\textbf{21.7} & \\textbf{34.3} \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{The Results of image recognition and molecule captioning tasks for the structural modalities. The content within the parentheses indicates the molecular representation modalities used for the corresponding model (S - SMILES, G - Molecular Graphs, I - Molecular Image). *: The SMILES notations are not included in the inputs of these tasks to prevent cheating. \n    Among the generalist models, the optimal results are \\textbf{bolded}, and the second-best results are \\underline{underlined}.}\n    \\label{tab:image-other}\n\\end{table*}\n\n\\begin{table*}\n    \\centering\n    \\begin{tabular}{lcccccc}\n    \\toprule\n    Model & BACE & BBBP & Clintox & HIV & Tox21 & Avg \\\\\n    \\midrule\n    ChemLLM-7B-Chat (S)~\\cite{zhang2024chemllm} & \\textbf{83.2} & 62.8 & 68.5 & 73.8 & 70.1 & 71.7 \\\\\n    ChemDFM-v1.0 (S)~\\cite{zhao2024chemdfm} & \\underline{78.4} & \\textbf{66.7} & \\underline{89.9} & 73.6 & \\textbf{79.8} & \\underline{77.7} \\\\\n    LLaVA-v1.6-Vicuna-13B (S + I)~\\cite{llava} & 50.0 & 45.5 & 60.6 & 64.6 & 55.0 & 55.1\\\\\n    InternLM-XComposer2 (S + I)~\\cite{internlmxcomposer} & 46.8 & 51.7 & 50.3 & 49.6 & 41.7 & 48.0 \\\\\n    DocOwl-1.5-Omni (S + I)~\\cite{hu2024mplug}  & 44.4 & 53.4 & 50.0 & 57.0 & 48.4 & 50.6 \\\\\n    \\multirow{1}{*}{\\textbf{ChemDFM-X (I)}} & 69.4 & \\underline{65.7} & 73.3 & \\underline{74.1} & 77.3 & 72.0 \\\\\n    \\multirow{1}{*}{\\textbf{ChemDFM-X (S + I)}} & 76.9 & 65.1 & \\textbf{93.5} & \\textbf{75.3} & \\underline{78.4} & \\textbf{77.8} \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{The Results of molecule property prediction tasks for the image modality. We report the performance of the models in AUC-ROC, which stands for the Area Under the Curve of the Receiver Operating Characteristic. The content within the parentheses indicates the molecular representation modalities used for the corresponding model (S - SMILES, G - Molecular Graphs, I - Molecular Image).}\n    \\label{tab:image-molnet}\n\\end{table*}\n\n\\paragraph{Baselines.} Similar to structural modalities, we incorporate three types of baselines.\nHowever, there is little previous work specifically targeted at chemical images. Therefore, we leverage multiple general-domain large vision-language models as the first type of baselines, including GPT-4V~\\cite{gpt-4v}, Gemini~\\cite{gemini}, Qwen-VL~\\cite{qwenvl}, LLaVA-v1.6~\\cite{llava}, InternLM-xcomposer2~\\cite{internlmxcomposer}, and DocOwl-1.5-Omni~\\cite{hu2024mplug}. The second type of baselines is the same chemical LLMs as those in structural modalities, namely ChemDFM and ChemLLM. \nThe third type of baseline is the task-specific specialist model, GIT-Mol.\n\n\\paragraph{Results and Analysis.} The experimental results of the image modality are illustrated in Table~\\ref{tab:image-other} and \\ref{tab:image-molnet}. The results show that with the help of molecule images, ChemDFM-X can achieve comparable or better performance across all the tasks, showing that our ChemDFM-X possesses strong chemical image understanding prowess.\n\nIt is worth noticing that all the general-domain LMMs we have tested fail to complete the chemical tasks. To study the reason for this phenomenon, we manually examine all the outputs of the models. We find that since these general domain LMMs are not fine-tuned for specialized chemical tasks, the models lack sufficient chemistry knowledge to complete the tasks. For instance, in the molecule captioning task, some models misinterpret it as a SMILES generation task, yielding only the SMILES notation. Others, despite comprehending the task, lack the capacity to correlate molecules with their corresponding chemical knowledge, resulting in stubbornly generating molecule descriptions unrelated to the input molecules. In the molecule image recognition task, although the models can recognize a portion of the chemical information in the image, their deficiency in understanding SMILES rules hinders them from generating long SMILES notations, limiting task performance. In the reaction image recognition task, a portion of the models claim that they don't have the ability and refuse to answer, and other models generate invalid SMILES due to their lack of capacity for understanding the SMILES rules.\n\nAnother noteworthy result is the excellent performance of ChemDFM-X in the reaction image recognition task. Considering the length of a chemical reaction which is composed of numerous molecules, it is astonishing that the accuracy of reaction image recognition is even higher than that of single-molecule image recognition. We believe this precisely demonstrates the advantages and capabilities of cross-modality chemical LMMs, where ChemDFM-X uses the reaction knowledge it has acquired to help the reaction image recognition task. Specifically speaking, instead of single-molecule image recognition where the model needs to identify each and every bit of the molecule including the types of each atom and the connections between them, during reaction image recognition, the small mistakes when identifying each molecule may be tolerant because of the existence of the reaction context. These small mistakes may be corrected by the ChemDFM-X itself through analyzing the reaction. In other words, with the cross-modality comprehension capability, ChemDFM-X can utilize the reaction knowledge learned with SMILES representations to help implicitly correct minor mistakes during the reaction image recognition task, therefore resulting in much higher performances.\n\n\\subsection{Characterization Modalities}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.8\\linewidth]{figures/ChemDFM-eval3.pdf}\n    \\caption{Evaluation tasks for characterization modalities}\n    \\label{figure:eval3}\n\\end{figure}\n\n\\begin{table*}[t]\n    \\centering\n    \\begin{tabular}{lccccc}\n    \\toprule\n    \\multirow{2.1}{*}{Model} & SR & \\multicolumn{2}{c}{RP} & \\multicolumn{2}{c}{Retrosynthesis} \\\\\n    \\cmidrule(rl){2-2} \\cmidrule(rl){3-4} \\cmidrule(rl){5-6}\n    & \\small{Acc} & \\small{Acc} & \\small{Validity} & \\small{Acc} & \\small{Validity} \\\\\n    \\midrule\n    ChemLLM-7B-Chat (S)~\\cite{zhang2024chemllm} & - & 18 & 94 & 10 & 91 \\\\\n    ChemDFM-v1.0 (S)~\\cite{zhao2024chemdfm} & - & \\underline{49} & \\underline{98} & 12 & \\underline{99} \\\\\n    \\multirow{1}{*}{\\textbf{ChemDFM-X (M)}} & 0 & - & - & - & - \\\\\n    \\multirow{1}{*}{\\textbf{ChemDFM-X (R)}} & 1 & - & - & - & - \\\\\n    \\multirow{1}{*}{\\textbf{ChemDFM-X (S + M)}} & - & 45 & 88 & \\underline{29} & 84 \\\\\n    \\multirow{1}{*}{\\textbf{ChemDFM-X (S + R)}} & - & \\textbf{64} & \\textbf{100} & \\textbf{60} & \\textbf{100} \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{The Results of multiple evaluation tasks, including Spectrum Recognition~(SR), Reaction Prediction~(RP), and Retrosynthesis, for the characterization modalities. The metric validity evaluates whether the output SMILES is valid. The content within the parentheses indicates the molecular representation modalities used for the corresponding model (S - SMILES, M - MS2 Spectra, R - IR Spectra). The optimal results are \\textbf{bolded}, and the second-best results are \\underline{underlined}.}\n    \\label{tab:spect-other}\n\\end{table*}\n\n\\paragraph{Evaluation Tasks.} Different from the aforementioned modalities which can only represent the known molecules, the characterization modalities are good at representing the unknown substances. The information they contained is mainly about partial properties and sub-structures and is usually implicit and scattered. Therefore, the data of characterization modalities are more suited for the unknown substance identification task.\n\nTherefore, in our evaluation, we primarily focus on the spectrum identification tasks. Intuitively, the first group of tasks to evaluate models' spectrum identification capabilities is basic spectrum recognition, where the spectrum of a molecule is provided and the model is asked to identify the corresponding molecule based on that. However, during real chemical research, the unknown substances do not appear out of nowhere. Usually, the spectra are provided in the context of reactions, and the knowledge such as how the substance is synthesized and what reaction it can undergo is known. Therefore, it is vital and valuable for models to possess the capability to identify the molecule based on the molecular spectra along with the reaction related to the unknown molecule. To construct such tasks, we leverage the reaction prediction and retrosynthesis tasks used for structural modalities and ask models to generate the corresponding products or reactants based on the SMILES notation of the reaction and the spectra of the missing molecule.\n\nAn intuitive diagram demonstrating the input and output of the evaluation tasks for characterization modalities is shown in Figure~\\ref{figure:eval3}.\n\n\\paragraph{Baselines.} The development of spectrum identification is relatively early-stage. Currently, there have still not been public-available standard baselines and benchmarks. Therefore, we only compare our ChemDFM-X model with the conventional chemical LLMs, ChemDFM, and ChemLLM, which only comprehend text and SMILES.\n\n\\paragraph{Results and Analysis.}The experimental results of the characterization modalities are illustrated in Table~\\ref{tab:spect-other}. \nIt is noteworthy that although the top 1 accuracy of the spectrum recognition task has near-zero performance, the performance leaps significantly when additional reaction information is involved. Note that the resulting performances (64\\% in the reaction prediction task and 60\\% in the retrosynthesis task) are far higher than those of the origin tasks where only reaction SMILES are provided with no spectra. Therefore, what is happening here is not a strong modality helping a weak modality, but a collaboration between modalities. From a chemical perspective, the spectrum modalities can provide SMILES modality hints related to the structure and composition information of the missing substances in the reactions, while SMILES modality can trim incorrect options. Through this cross-modality collaboration, ChemDFM-X is able to achieve much better performances greater than the sum of its parts. In short, ChemDFM-X successfully gains the ability to understand and analyze different spectra and is able to combine the information from different modalities to achieve higher performance.\n\n\\section{Conclusion}\n\nThis paper proposed ChemDFM-X, a large multimodal model for chemistry. ChemDFM-X is a generalist model that has the ability to understand five of the most commonly used modalities in the field of chemistry, including structural modalities, image modalities, and characterization modalities.\nThe evaluation result shows that ChemDFM-X possesses the capabilities to comprehend the chemical data in all five non-text modalities. With the help of multi-modality inputs, ChemDFM-X is capable of exploiting cross-modality information and knowledge, especially the spectrum input introducing experimental observations lacking in other models. This assists ChemDFM-X in outperforming other generalist models in a series of common chemical tasks and demonstrates the practical value of ChemDFM-X in real chemistry research. It also has the potential for dealing with inputs of multiple different modalities simultaneously, which is powerful in reaction-related tasks and will be further tested in our future work.\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2401.14818v1.tex",
        "arXiv-2402.09391v4.tex",
        "arXiv-2409.13194v2.tex"
    ],
    "group_id": "group_45",
    "response": "### Title: Large Language Models and Multimodal Models for Chemical General Intelligence (CGI)\n\n### Introduction\n\nChemistry, a fundamental science, has seen increasing interest in the integration of artificial intelligence (AI) to enhance research and development processes. The field of AI for chemistry has traditionally focused on task-specific models designed to handle particular chemical tasks such as molecular property prediction, reaction analysis, and retrosynthesis. However, these models often fall short in comprehending and reasoning over the diverse and specialized languages and knowledge intrinsic to chemistry, such as SMILES (Simplified Molecular-Input Line-Entry System) notation and IUPAC (International Union of Pure and Applied Chemistry) names. This limitation has spurred the development of Large Language Models (LLMs) and Large Multimodal Models (LMMs) to address the broader scope of chemical tasks, aiming to create Chemical General Intelligence (CGI) systems that can assist chemists in a wide range of activities, from literature reading to experimental design.\n\nLLMs, such as GPT-4, have demonstrated remarkable capabilities in natural language processing tasks, but their performance in chemistry-specific tasks remains limited. To bridge this gap, researchers have been fine-tuning these models with domain-specific data and instruction tuning datasets, leading to the emergence of specialized chemical LLMs. These models are expected to provide a more comprehensive understanding of chemical knowledge and to support free-form dialogues with chemists, thereby enhancing the collaboration between human researchers and AI systems.\n\nThis summary focuses on three recent research papers that explore the development of advanced LLMs and LMMs for chemistry. The first paper introduces ChemDFM, a Dialogue Foundation Model for Chemistry, which is fine-tuned on a large corpus of chemical literature, textbooks, and instructions. The second paper presents a comprehensive and high-quality instruction tuning dataset, \\datasetname, and demonstrates the effectiveness of fine-tuning LLMs on this dataset to achieve superior performance in chemistry tasks. The third paper introduces ChemDFM-X, a multimodal model that extends the capabilities of ChemDFM to include multiple modalities such as molecular graphs, conformations, images, and spectra, aiming to provide a more versatile and comprehensive understanding of chemical data.\n\n### Main Content of Each Paper\n\n#### ChemDFM: Dialogue Foundation Model for Chemistry\n\nThe first paper introduces ChemDFM, a specialized LLM designed to enhance Chemical General Intelligence (CGI) for chemistry. ChemDFM is based on the pre-trained LLaMa-13B model and is fine-tuned on a large dataset comprising 34 billion tokens from chemical literature, textbooks, and instructions, as well as 2.7 million crafted instructions from various chemical databases. The dataset is carefully curated to ensure a balance between chemical and general-domain knowledge, allowing ChemDFM to maintain strong natural language understanding while acquiring deep chemical knowledge.\n\nThe training process of ChemDFM consists of two stages: Domain Pre-training and Instruction Tuning. During Domain Pre-training, ChemDFM is trained on chemical literature and textbooks to harvest chemical knowledge. In the Instruction Tuning stage, the model is further fine-tuned on a dataset that includes SMILES understanding, Molecular Notation Alignment (MNA), and natural language question-answering related to chemistry. This dataset is constructed to ensure high-quality dialogue interactions and to cover a wide range of chemical tasks, from molecular recognition and property prediction to reaction analysis and question-answering.\n\nThe evaluation of ChemDFM is conducted on two benchmarks, ChemLLMBench and SciEval, which assess the model's performance in chemical tasks and general scientific knowledge, respectively. ChemDFM outperforms typical open-sourced LLMs and even surpasses GPT-4 in many tasks, despite the significant size difference. The model's performance is particularly strong in tasks involving SMILES and IUPAC name translation, molecular property prediction, and reaction completion. ChemDFM's strong dialogue capabilities enable it to assist chemists in real-world scenarios, such as paper reading and experimental design, by comprehending and reasoning over both chemical and natural languages.\n\n#### \\datasetname: Advancing Large Language Models for Chemistry\n\nThe second paper addresses the limitations of existing LLMs in chemistry tasks by proposing \\datasetname, a large-scale, comprehensive, and high-quality instruction tuning dataset. \\datasetname contains over three million samples across 14 chemistry tasks, including name conversion, property prediction, molecule captioning, molecule generation, forward synthesis, and retrosynthesis. The dataset is constructed through a rigorous pipeline that involves data collection, quality control, data splitting, and instruction creation, ensuring high-quality and diverse data.\n\nThe authors fine-tune four open-source LLMs (Galactica, Llama 2, Code Llama, and Mistral) on \\datasetname using LoRA (Low-Rank Adaptation) to create a series of models named \\modelname. The evaluation results show that \\modelname models, particularly the Mistral-based model, significantly outperform other LLMs and even approach the performance of state-of-the-art (SoTA) task-specific models with a much smaller proportion of fine-tuned parameters. The paper also includes an ablation study that highlights the importance of canonical SMILES representation and the benefits of multi-task training over single-task training.\n\n#### ChemDFM-X: Towards Large Multimodal Model for Chemistry\n\nThe third paper introduces ChemDFM-X, a multimodal model that extends the capabilities of ChemDFM to include multiple modalities such as molecular graphs, conformations, images, and spectra. ChemDFM-X is designed to handle the wide range of chemical data modalities and tasks, making it a more versatile and comprehensive tool for chemists. The model is fine-tuned on a multimodal instruction tuning dataset that includes 7.6 million samples generated from SMILES notations using approximate calculations and predictions from task-specific models.\n\nChemDFM-X is evaluated on various chemical tasks, including molecule recognition, property prediction, reaction prediction, and retrosynthesis, across different modalities. The results demonstrate that ChemDFM-X can effectively comprehend and infer over multiple modalities, outperforming other generalist models and achieving comparable or better performance than SoTA task-specific models. The model's performance is particularly strong in tasks involving molecular conformations and reaction images, showing the benefits of cross-modal knowledge comprehension.\n\n### Commonalities and Innovations\n\nAll three papers aim to improve the performance of LLMs and LMMs in chemistry tasks by leveraging specialized datasets and fine-tuning techniques. ChemDFM and \\modelname both focus on fine-tuning LLMs to enhance their understanding of chemical languages and knowledge, while ChemDFM-X extends this approach to include multiple modalities, thereby addressing the limitations of unimodal models in handling diverse chemical data.\n\nThe key innovation in ChemDFM lies in its two-stage specialization process, which combines domain pre-training and instruction tuning to achieve advanced performance in chemical tasks. \\datasetname introduces a large-scale, comprehensive, and high-quality dataset to fine-tune LLMs, demonstrating the effectiveness of instruction tuning in improving model performance. ChemDFM-X, on the other hand, introduces a cross-modal approach that enables the model to handle multiple modalities, including structural, image, and characterization modalities, thereby providing a more versatile and comprehensive tool for chemists.\n\n### Comparison of Results\n\nThe results of the three models are evaluated on various benchmarks and tasks, highlighting their respective strengths and limitations. ChemDFM outperforms open-source LLMs and even surpasses GPT-4 in many tasks, particularly those involving SMILES and IUPAC name translation. \\modelname models, especially the Mistral-based model, achieve significant improvements over other LLMs and approach the performance of SoTA task-specific models with a much smaller fine-tuning dataset. ChemDFM-X demonstrates strong performance in tasks involving molecular conformations and reaction images, outperforming other generalist models and achieving comparable or better performance than SoTA task-specific models.\n\nThe evaluations show that ChemDFM and \\modelname achieve advanced performance in tasks such as molecular recognition, property prediction, and reaction analysis, but their performance is limited when handling tasks involving multiple modalities. ChemDFM-X, however, shows significant improvements in tasks involving multiple modalities, particularly in reaction image recognition and spectrum identification tasks. This highlights the importance of cross-modal knowledge comprehension in achieving advanced performance in chemical tasks.\n\n### Conclusion\n\nThe three papers collectively demonstrate the potential of LLMs and LMMs in enhancing the capabilities of AI systems for chemistry. ChemDFM and \\modelname achieve advanced performance in chemical tasks by leveraging specialized datasets and fine-tuning techniques, while ChemDFM-X extends this approach to include multiple modalities, thereby providing a more versatile and comprehensive tool for chemists. The evaluations show that these models can effectively comprehend and reason over chemical knowledge and languages, outperforming traditional task-specific models and even approaching the performance of SoTA task-specific models.\n\nFuture research directions include further improving the performance of these models in tasks involving multiple modalities and exploring the integration of more modalities such as molecular graphs and spectra. Additionally, the development of more comprehensive and high-quality datasets for instruction tuning is essential to achieve advanced performance in chemical tasks. The potential of these models to assist chemists in real-world scenarios through dialogue-based human-AI collaboration is also a promising area for future research."
}