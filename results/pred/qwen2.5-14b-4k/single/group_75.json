{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Anti-Backdoor Learning: Training Clean Models on Poisoned Data}\n\n\\begin{document}\n\n\\maketitle\n\\renewcommand{\\thefootnote}{\\fnsymbol{footnote}} \n\\footnotetext[2]{Correspondence to: Xixiang Lyu, Xingjun Ma.}\n\n\\begin{abstract}\nBackdoor attack has emerged as a major security threat to deep neural networks (DNNs). While existing defense methods have demonstrated promising results on detecting or erasing backdoors, it is still not clear whether robust training methods can be devised to prevent the backdoor triggers being injected into the trained model in the first place.\nIn this paper, we introduce the concept of \\emph{anti-backdoor learning}, aiming to train \\emph{clean} models given backdoor-poisoned data. We frame the overall learning process as a dual-task of learning the \\emph{clean} and the \\emph{backdoor} portions of data. From this view, we identify two inherent characteristics of backdoor attacks as their weaknesses: 1) the models learn backdoored data much faster than learning with clean data, and the stronger the attack the faster the model converges on backdoored data; 2) the backdoor task is tied to a specific class (the backdoor target class). Based on these two weaknesses, we propose a general learning scheme, Anti-Backdoor Learning (ABL), to automatically prevent backdoor attacks during training. ABL introduces a two-stage \\emph{gradient ascent} mechanism for standard training to 1) help isolate backdoor examples at an early training stage, and 2) break the correlation between backdoor examples and the target class at a later training stage. Through extensive experiments on multiple benchmark datasets against 10 state-of-the-art attacks, we empirically show that ABL-trained models on backdoor-poisoned data achieve the same performance as they were trained on purely clean data. Code is available at \\url{https://github.com/bboylyg/ABL}.\n\n\\end{abstract}\n\n\\section{Introduction} \\label{sec:1}\nA backdoor attack is a type of training-time data poisoning attack that implant backdoor triggers into machine learning models by injecting the trigger pattern(s) into a small proportion of the training data \\cite{gu2017badnets}. It aims to trick the model to learn a strong but task-irrelevant correlation between the trigger pattern and a target class, and optimizes three objectives: stealthiness of the trigger pattern, injection (poisoning) rate and attack success rate. A backdoored model performs normally on clean test data yet consistently predicts the target class whenever the trigger pattern is attached to a test example. Studies have shown that deep neural networks (DNNs) are particularly vulnerable to backdoor attacks \\cite{weng2020trade}. Backdoor triggers are generally easy to implant but hard to detect or erase, posing significant security threats to deep learning.  \n\nExisting defense methods against backdoor attacks can be categorized into two types: detection methods and erasing methods \\cite{li2020backdoor}.  Detection methods exploit activation statistics or model properties to determine whether a model is backdoored \\cite{wang2019neural,chen2019deepinspect}, or whether a training/test example is a backdoor example \\cite{tran2018spectral, chen2018detecting}. While detection can help identify potential risks, the backdoored model still needs to be purified. Erasing methods \\cite{liu2018fine,zhao2020bridging,li2021neural} take one step further and remove triggers from the backdoored model. Despite their promising results, it is still unclear in the current literature whether the underlying model learns clean and backdoor examples in the same way.\nThe exploration of this aspect leads to a fundamental yet so far overlooked question, “\\textit{Is it possible to train a clean model on poisoned data?}\"\n\n\\begin{wrapfigure}{r}{0.38\\textwidth}\n  \\begin{center}\n    \\vspace{-0.2in}\n    \\includegraphics[width=0.38\\textwidth]{figs/asr_poisoning_rate.pdf}\n  \\end{center}\n  \t\\vspace{-0.2in}\n  \\caption{Attack success rate (ASR) of 6 backdoor attacks under different poisoning rates on CIFAR-10. 4 out of the 6 attacks can achieve nearly 100\\% ASR at poisoning rate 0.5\\%.\n  }\n  \\label{img1}\n\\end{wrapfigure}\n\nIntuitively, if backdoored data can be identified during training, measures can be taken to prevent the model from learning them. However, we find that this is not a trivial task. One reason is that we do not know the proportion nor the distribution of the backdoored data in advance. As shown in Figure \\ref{img1}, on CIFAR-10, even if the poisoning rate is less than 1\\%, various attacks can still achieve high attack success rates. This significantly increases the difficulty of backdoor data detection as the model's learning behavior may remain the same with or without a few training examples. Even worse, we may accidentally remove a lot of valuable data when the dataset is completely clean.\nAnother important reason is that the backdoor may have already been learned by the model even if the backdoor examples are identified at a later training stage.\n\nIn this paper, we frame the overall learning process on a backdoor-poisoned dataset as a dual-task learning problem, with the learning of the clean portion as the original (clean) task and the learning of the backdoored portion as the backdoor task. By investigating the distinctive learning behaviors of the model on the two tasks, we identify two inherent characteristics of backdoor attacks as their weaknesses. \\textbf{First}, the backdoor task is a much easier task compared to the original task. Consequently, the training loss of the backdoored portion drops abruptly in early epochs of training, whereas the loss of clean examples decreases at a steady pace. We also find that the stronger the attack, the faster the loss on backdoored data drops. This finding indicates that the backdoor correlations imposed by stronger attacks are easier and faster to learn, and marks one distinctive learning behavior on backdoored data. \\textbf{Second}, the backdoor task is tied to a specific class (i.e., the backdoor target class). This indicates that the correlation between the trigger pattern and the target class could be easily broken by simply randomizing the class target, for instance, shuffling the labels of a small proportion of examples with low loss.\n\nInspired by the above observations, we propose a principled \\emph{Anti-Backdoor Learning (ABL)} scheme that enables the training of clean models without any prior knowledge of the distribution of backdoored data in datasets. ABL introduces a \\emph{gradient ascent} based anti-backdoor mechanism into the standard training to help isolate low-loss backdoor examples in early training and unlearn the backdoor correlation once backdoor examples are isolated. In summary, our main contributions are:\n\n\\begin{itemize}[leftmargin=*]\n    \\item We present a novel view of the problem of robust learning with poisoned data and reveal two inherent weaknesses of backdoor attacks: faster learning on backdoored data and target-class dependency. The stronger the attack is, the more easily it can be detected or disrupted. \n    \\item We propose a novel Anti-Backdoor Learning (ABL) method that is capable of training clean models on poisoned data. To the best of our knowledge, ABL is the \\textit{first} method of its kind in the backdoor defense literature, complementing existing defense methods.\n    \\item We empirically show that our ABL is robust to 10 state-of-the-art backdoor attacks. The models trained using ABL are of almost the same clean accuracy as they were directly trained on clean data and the backdoor attack success rates on these models are close to random guess. \n\\end{itemize}\n\n\\section{Related Work} \\label{sec:2}\n\\noindent\\textbf{Backdoor Attack.} Existing backdoor attacks aim to optimize three objectives: 1) making the trigger pattern stealthier; 2) reducing the poisoning (injection) rate; 3) increasing the attack success rate \\cite{li2020backdoor}. Creative design of trigger patterns can help with the stealthiness of the attack. These can be simple patterns such as a single pixel~\\cite{tran2018spectral} and a black-white checkerboard~\\cite{gu2017badnets}, or more complex patterns such as blending backgrounds~\\cite{chen2017targeted}, natural reflections~\\cite{liu2020reflection}, invisible noise~\\cite{liao2018backdoor,li2019invisible,chen2019invisible,saha2020hidden}, adversarial patterns \\cite{zhao2020clean} and sample-wise patterns \\cite{nguyen2020input,li2021invisible}. Backdoor attacks can be further divided into two categories: dirty-label attacks \\cite{gu2017badnets,chen2017targeted,liu2020reflection} and clean-label attacks \\cite{shafahi2018poison,turner2019clean,zhu2019transferable,zhao2020clean,saha2020hidden}. Clean-label attacks are arguably stealthier as they do not change the labels. Backdoor attackers can also inject backdoors via retraining the victim model on a reverse-engineered dataset without accessing the original training data \\cite{liu2018trojaning}. Most of these attacks can achieve a high success rate (e.g., $>95\\%$) by poisoning only 10\\% or even less of the training data. A recent study by Zhao \\etal \\cite{zhao2021deep} showed that even models trained on clean data can have backdoors, highlighting the importance of anti-backdoor learning.\n\n\\noindent\\textbf{Backdoor Defense.} Existing backdoor defenses fall under the categories of either detection or erasing methods. Detection methods aim to detect anomalies in input data \\cite{chen2018detecting,tran2018spectral,gao2019strip,xu2019detecting,hayase2021spectre,tang2021demon} or whether a model is backdoored \\cite{wang2019neural,chen2019deepinspect,kolouri2020universal,shen2021backdoor}. These methods typically show promising accuracies; however, the potential impact of backdoor triggers remains uncleared in the backdoored models. On the other hand, erasing methods take a step further and aim to purify the adverse impacts on models caused by the backdoor triggers. The current state-of-the-art erasing methods are Mode Connectivity Repair (MCR)~\\cite{zhao2020bridging}, Neural Attention Distillation (NAD)~\\cite{li2021neural} and Adversarial Neuron Pruning (ANP)~\\cite{wu2021adversarial}. MCR mitigates the backdoors by selecting a robust model in the path of loss landscape, NAD leverages attention distillation to erase triggers, while ANP prunes adversarially sensitive neurons to purify the model. Other previous methods, including finetuning, denoising, and fine-pruning \\cite{liu2018fine}, have been shown to be insufficient against the latest attacks \\cite{yao2019latent,li2020rethinking,liu2020reflection}. An early work \\cite{shen2019learning} found that DNNs are more accurate on clean samples in an early training stage, while we find that the backdoor attacks studied in \\cite{shen2019learning} are only limited to simple BadNets attacks. Thus, it would be interesting to further study the different properties of diverse types of backdoor attacks.\n\nIn this paper, we introduce the concept of \\emph{anti-backdoor learning}. Unlike existing methods, our goal is to train clean models directly on poisoned datasets without further altering the models or the input data. This requires a more in-depth understanding of the distinctive learning behaviors on backdoored data. However, such information is not available in the current literature. Anti-backdoor learning methods may replace the standard training to prevent potential backdoor attacks in real-world scenarios where data sources are not 100\\% reliable, and the distribution or even the presence of backdoor examples are unknown.\n\n\\section{Anti-Backdoor Learning} \\label{sec:3}\nIn this section, we first formulate the Anti-Backdoor Learning (ABL) problem, then reveal the distinctive learning behaviors on clean versus backdoor examples and introduce our proposed ABL method. Here, we focus on classification tasks with deep neural networks.\n\n\\noindent\\textbf{Defense Setting.} We assume the backdoor adversary has pre-generated a set of backdoor examples and has successfully injected these examples into the training dataset. We also assume the defender has full control over the training process but has no prior knowledge of the proportion nor distribution of the backdoor examples in a given dataset. The defender's goal is to train a model on the given dataset (potentially poisoned) that is as good as models trained on purely clean data. Moreover, if an isolation method is used, the defender may identify only a subset of the backdoor examples. For instance, in the case of 10\\% poisoning, the isolation rate might only be 1\\%.\nRobust learning methods developed under our defense setting could benefit companies, research institutes, government agencies or MLaaS (Machine Learning as a Service) providers to train backdoor-free models on potentially poisoned data. More explanations about our threat model and how our proposed ABL method can help other defense settings can be found in Appendix \\ref{appendix:b10}.\n\n\\noindent\\textbf{Problem Formulation.}\nConsider a standard classification task with a dataset $\\gD=\\gD_c \\cup \\gD_b$ where $\\gD_c$ denotes the subset of clean data and $\\gD_b$ denotes the subset of backdoor data. The standard training trains a DNN model $f_{\\theta}$ by minimizing the following empirical error:\n\\begin{equation}\\label{eq:std}\n     \\gL = \\sE_{(\\vx, y) \\sim \\gD} [\\ell(f_{\\theta}(\\vx), y)] = \\underbrace{\\sE_{(\\vx, y) \\sim \\gD_c} [\\ell(f_{\\theta}(\\vx), y)]}_{\\textup{clean task}} + \\underbrace{\\sE_{(\\vx, y) \\sim \\gD_b} [\\ell(f_{\\theta}(\\vx), y)]}_{\\textup{backdoor task}},\n\\end{equation}\nwhere $\\ell(\\cdot)$ denotes the loss function such as the commonly used cross-entropy loss. The overall learning task is decomposed into two tasks where the first \\emph{clean task} is defined on the clean data $\\gD_c$ while the second \\emph{backdoor task} is defined on the backdoor data $\\gD_b$. Since backdoor examples are often associated with a particular target class, all data from $\\gD_b$ may share the same class label. The above decomposition indicates that the standard learning approach tends to learn both tasks, resulting in a backdoored model. \n\nTo prevent backdoor examples from being learned, we propose anti-backdoor learning to minimize the following empirical error instead:\n\\begin{equation}\\label{eq:anti}\n     \\gL = \\sE_{(\\vx, y) \\sim \\gD_c} [\\ell(f_{\\theta}(\\vx), y)] - \\sE_{(\\vx, y) \\sim \\gD_b} [\\ell(f_{\\theta}(\\vx), y)].\n\\end{equation}\nNote the maximization of the backdoor task is defined on $\\gD_b$. Unfortunately, the above objective is undefined during training since we do not know the $\\gD_b$ subset. Intuitively, $\\gD_b$ can be detected and isolated during training if the model exhibits an atypical learning behavior on the backdoor examples.  In the following subsection, we will introduce one such behavior, which we recognize as the first weakness of backdoor attacks.\n\n\\subsection{Distinctive Learning Behaviors on Backdoor Examples}\nWe apply 6 classic backdoor attacks including BadNets \\cite{gu2017badnets}, Trojan \\cite{liu2018trojaning}, Blend \\cite{chen2017targeted}, Dynamic \\cite{nguyen2020input}, SIG \\cite{barni2019new}, and CL \\cite{turner2019clean}, and 3 feature-space attacks including FC \\cite{shafahi2018poison}, DFST \\cite{cheng2021deep}, and LBA \\cite{yao2019latent} to poison 10\\% of CIFAR-10 training data. We train a WideResNet-16-1 model \\cite{zagoruyko2016wide} on the corresponding poisoned dataset using the standard training method by solving \\eqref{eq:std} for each attack. Each model is trained following the standard settings (see Section \\ref{sec:4} and Appendix \\ref{appendix:a2}). We plot the average training loss (i.e., cross-entropy) on clean versus backdoored training examples in Figure \\ref{img2}. Clearly, for all 9 attacks, the training loss on backdoor examples drops much faster than that on clean examples in the first few epochs. Both pixel- and feature-space attacks exhibit this faster-learning pattern consistently, although some feature-space attacks (FC and LBA) can slow down the process to some extent. For all attacks except SIG, FC and LBA, the training loss reaches almost zero after only two epochs of training. Moreover, according to the attack success rate, the stronger the attack is, the faster the training loss on backdoor examples drops. More results on GTSRB and an ImageNet subset can be found in Appendix \\ref{appendix:b2}.\n\n\\begin{figure}[!tp]\n\t\\centering\n\t\\includegraphics[width = .99\\linewidth]{figs/training_loss_nine_backdoor_attacks.png}\n\t\\vspace{-0.1 in}\n\t\\caption{The training loss on clean versus backdoor examples crafted by 9 backdoor attacks including BadNets \\cite{gu2017badnets}, Trojan \\cite{liu2018trojaning}, Blend \\cite{chen2017targeted}, Dynamic \\cite{nguyen2020input}, SIG \\cite{barni2019new}, and CL \\cite{turner2019clean}, FC \\cite{shafahi2018poison}, DFST \\cite{cheng2021deep}, and LBA \\cite{yao2019latent}. This experiment is conducted with WideResNet-16-1 \\cite{zagoruyko2016wide} on CIFAR-10 under poisoning rate 10\\%. ASR: attack success rate (on WideResNet-16-1).}\n\t\\label{img2}\n\t\\vspace{-0.1in}\n\\end{figure}\n\nThe above observation indicates that the backdoor task is much easier than the clean task. This is not too surprising. In a typical clean dataset, not all examples are easy examples. Thus, it requires a certain number of training epochs to minimize the loss on those examples, even for small datasets like CIFAR-10. On the contrary, a backdoor attack adds an explicit correlation between the trigger pattern and the target class to simplify and accelerate the injection of the backdoor trigger. We argue that this is a fundamental requirement and also a major weakness of backdoor attacks. For a backdoor attack to work successfully, the trigger(s) should be easily learnable by the model, or else the attack would lose its effectiveness or require a much higher injection rate, which goes against its key objectives. \nTherefore, the stronger the attack is, the faster the training loss on backdoor examples drops to zero; e.g., compare FC with other attacks in Figure \\ref{img2}. We also show in Figure \\ref{img6} in Appendix \\ref{appendix:b1} that the training loss of the backdoor task drops more rapidly as we increase the poisoning rate.\n\nBased on the above observation, one may wonder if backdoor examples can be easily removed by filtering out the low-loss examples at an early stage (e.g., the 5th epoch). However, we find that this strategy is ineffective for two reasons. First, the training loss in Figure \\ref{img2} is the average training loss which means some backdoor examples can still have high training loss. Additionally, several powerful attacks such as Trojan and Dynamic can still succeed even with very few (50 or 100) backdoor examples. Second, if the training progresses long enough (e.g., beyond epoch 20), many clean examples will also have a low training loss, which makes the filtering significantly inaccurate. \nTherefore, we need a strategy to amplify the difference in training loss between clean and backdoor examples. Moreover, we need to unlearn the backdoor since the backdoor examples can only be identified when they are learned into the model (i.e., low training loss).\n\n\\subsection{Proposed Anti-Backdoor Learning Method}\nSuppose the total number of training epochs is $T$, we decompose the entire training process into two stages, i.e., early training and later training. We denote the turning epoch from early training to later training by $T_{te}$. Our anti-backdoor learning method consists of two key techniques: 1) \\emph{backdoor isolation} during early training, and 2) \\emph{backdoor unlearning} during later training. The turning epoch is chosen to be the epoch where the average training loss stabilizes at a certain level.\n\n\\noindent\\textbf{Backdoor Isolation.} During early training, we propose a \\emph{local gradient ascent} (LGA) technique to trap the loss value of each example around a certain threshold $\\gamma$. We use the loss function $\\gL_{\\textup{LGA}}$ in \\eqref{eq:abl} to achieve this. The gradient ascent is said to be ``local'' because the maximization is performed around a fixed loss value $\\gamma$. In other words, if the loss of a training example goes below $\\gamma$, gradient ascent will be activated to boost its loss to $\\gamma$; otherwise, the loss stays the same. \nDoing so will force backdoor examples to escape the $\\gamma$ constraint since their loss values drop significantly faster. The choice of an appropriate $\\gamma$ lies in the core of this strategy, as an overly large $\\gamma$ will hurt the learning of the clean task, while an overly small $\\gamma$ may not be strong enough to segregate the clean task from the backdoor task. Note that $\\gamma$ can be determined by the strength of potential attacks: stronger attacks only need a smaller $\\gamma$ to isolate. Since most backdoor attacks are strong, the poisoned data can easily reach a small loss value below 0.5. So, we set $\\gamma=0.5$ in our experiments and show its consistent performance across different datasets and models in Section \\ref{sec:4.2} and Appendix \\ref{appendix:b5}. At the end of early training, we segregate examples into disjoint subsets: $p$ percent of data with the lowest loss values will be isolated into the backdoor set $\\widehat{\\gD}_b$ ($p=|\\widehat{\\gD}_b|/|\\gD|$), and the rest into the clean set $\\widehat{\\gD}_c$ ($\\gD=\\widehat{\\gD}_b \\cup \\widehat{\\gD}_c$). An important note here is that the isolation rate (e.g., $p=1\\%$) is assumed to be much smaller than the poisoning rate (e.g., 10\\%).\n\n\\noindent\\textbf{Backdoor Unlearning.} With the clean and backdoor sets, we can then proceed %to \nwith the later training. Note that at this stage, the backdoor has already been learned by the model. \nGiven the above low isolation rate, an effective backdoor unlearning method is required to make the model unlearn the backdoor with a small subset $\\widehat{\\gD}_b$ of backdoor examples while simultaneously learning the remaining (unisolated) backdoor examples in the clean set $\\widehat{\\gD}_c$. We make this possible by exploiting the second weakness of backdoor attacks: the backdoor trigger is usually associated with a particular backdoor target class.\nWe propose to use the loss $\\gL_{\\textup{GGA}}$ defined in \\eqref{eq:anti} for this purpose. In $\\gL_{\\textup{GGA}}$, a \\emph{global gradient ascent} (GGA) is defined on the isolated subset $\\widehat{\\gD}_b$. Unlike the local gradient ascent, it is not constrained to be around a fixed loss value. We will show in Section \\ref{sec:stress testing} that a low isolation rate of 1\\% is able to effectively unlearn the backdoor against 50\\% poisoning.\n\nThe loss functions used by our ABL for its two training stages are summarized as follows,\n\\begin{equation}\\label{eq:abl}\n\\small\n    \\gL^t_{\\textup{ABL}} =\n    \\begin{cases}\n      \\gL_{\\textup{LGA}} = \\sE_{(\\vx, y) \\sim \\gD} \\big[\\sign(\\ell(f_{\\theta}(\\vx), y) - \\gamma) \\cdot \\ell(f_{\\theta}(\\vx), y)\\big] & \\text{if  $0 \\leq t < T_{te}$}\\\\\n      \\gL_{\\textup{GGA}} = \\sE_{(\\vx, y) \\sim \\widehat{\\gD}_c} \\big[\\ell(f_{\\theta}(\\vx), y)\\big] - \\sE_{(\\vx, y) \\sim \\widehat{\\gD}_b} \\big[\\ell(f_{\\theta}(\\vx), y)\\big] & \\text{if $T_{te} \\leq t < T$},\n    \\end{cases}\n\\end{equation}\nwhere $t \\in [0, T-1]$ is the current training epoch, $\\sign(\\cdot)$ is the sign function, $\\gamma$ is the loss threshold for LGA and $\\widehat{\\gD}_b$ is the isolated backdoor set with the isolation rate $p=|\\widehat{\\gD}_b|/|\\gD|$. During early training ($0 \\leq t < T_{te}$), the loss will be automatically switched to $- \\ell(f_{\\theta}(\\vx), y)$ if $\\ell(\\cdot)$ is smaller than $\\gamma$ by the sign function; otherwise the loss stays the same, i.e., $\\ell(f_{\\theta}(\\vx), y)$. Note that $\\gL_{\\textup{LGA}}$ loss may also be achieved by the flooding loss proposed in \\cite{ishida2020we} to prevent overfitting: $|\\ell(f_{\\theta}(\\vx), y) - b| + b$ where $b$ is a flooding parameter. We would like to point out that LGA serves only one part of our ABL and can potentially be replaced by other backdoor detection methods. Additionally, we will show that a set of other techniques may also achieve backdoor isolation and unlearning, but they are far less effective than our ABL (see Section \\ref{sec:4.3} and Appendix \\ref{appendix:b3}).\n\n\\section{Experiments} \\label{sec:4}\n\\textbf{Attack Configurations.} We consider 10 backdoor attacks in our experiments, including four dirty-label attacks: BadNets~\\cite{gu2017badnets}, Trojan attack~\\cite{liu2018trojaning}, Blend attack~\\cite{chen2017targeted}, Dynamic attack~\\cite{nguyen2020input}, two clean-label attacks: Sinusoidal signal attack(SIG)~\\cite{barni2019new} and Clean-label attack(CL)~\\cite{turner2019clean}, and four feature-space attacks: Feature collision (FC)~\\cite{shafahi2018poison}, Deep Feature Space Trojan Attack (DFST)~\\cite{zhao2021deep}, Latent Backdoor Attack (LBA)~\\cite{yao2019latent}, and Composite Backdoor Attack (CBA)~\\cite{lin2020composite}. We follow the settings suggested by \\cite{li2021neural} and the open-sourced code corresponding to their original papers to configure these attack algorithms. All attacks are evaluated on three benchmark datasets, CIFAR-10 \\cite{krizhevsky2009learning}, GTSRB \\cite{stallkamp2012man} and an ImageNet subset \\cite{deng2009imagenet}, with two classical model structures including WideResNet (WRN-16-1) \\cite{zagoruyko2016wide} and ResNet-34 \\cite{he2016deep}. No data augmentations are used for these attacks since they hinder the backdoor effect \\cite{liu2020reflection}. To keep their original configurations of dataset and parameter settings, here we only run the four feature-space attacks on CIFAR-10 dataset. We also omit some attacks on GTSRB and ImageNet datasets due to a failure of reproduction following their original papers. The detailed settings of the 10 backdoor attacks are summarized in Table \\ref{tab5} (see Appendix \\ref{appendix:a2}).\n\n\\textbf{Defense and Training Details.} We compare our ABL with three state-of-the-art defense methods: Fine-pruning (FP)~\\cite{liu2018fine}, Mode Connectivity Repair (MCR)~\\cite{zhao2020bridging}, and Neural Attention Distillation (NAD)~\\cite{li2021neural}. For FP, MCR and NAD, we follow the configurations specified in their original papers, including the available clean data for finetuning/repair/distillation and training settings. The comparison with other data isolation methods are shown in Section \\ref{sec:4.3}. For our ABL, we set $T=100$, $T_{te}=20$, $\\gamma = 0.5$ and an isolation rate $p=0.01$ (1\\%) in all experiments. The exploration of different $T_{te}$, $\\gamma$, and isolation rates $p$ are also provided in Section \\ref{sec:4.1}. Three data augmentation techniques suggested in \\cite{li2021neural} including random crop (padding = 4), horizontal flipping, and cutout, are applied for all defense methods. More details on defense settings can be found in Appendix \\ref{appendix:a3}.\n\n\\textbf{Evaluation Metrics.} We adopt two commonly used performance metrics: Attack Success Rate (ASR), which is the classification accuracy on the backdoor test set, and Clean Accuracy (CA), the classification accuracy on clean test set.\n\n\\begin{table}[!htp]\n\\renewcommand{\\arraystretch}{1.1}\n\\renewcommand\\tabcolsep{1.6pt}\n\\small\n\\centering\n\\caption{The attack success rate (ASR \\%) and the clean accuracy (CA \\%)  of 4 backdoor defense methods against 10 backdoor attacks including 6 classic backdoor attacks and 4 feature-space attacks. \\emph{None} means the training data is completely clean.}\n  \\label{tab1}\n\\begin{tabular}{c|c|cc|cc|cc|cc|cc}\n\\toprule\n\\multirow{2}{*}{Dataset} & \\multirow{2}{*}{Types} & \\multicolumn{2}{c|}{\\begin{tabular}[c|]{@{}c@{}}No Defense\\end{tabular}} & \\multicolumn{2}{c|}{FP} & \\multicolumn{2}{c|}{MCR} & \\multicolumn{2}{c|}{NAD} &  \\multicolumn{2}{c}{\\textbf{ABL (Ours)}}\\\\ \\cline{3-12} \n &  & ASR & CA & ASR & CA & ASR & CA & ASR & CA & ASR & CA\\\\ \\hline\n\\multirow{8}{*}{CIFAR-10} \n& \\emph{None} & 0\\% & 89.12\\% & 0\\% & 85.14\\% & 0\\% & 87.49\\% & 0\\% & 88.18\\% & 0\\% & \\textbf{88.41\\%} \\\\ \\cline{2-12} \n& BadNets & 100\\% & 85.43\\% & 99.98\\% & 82.14\\% & 3.32\\% & 78.49\\% & 3.56\\% & 82.18\\% & \\textbf{3.04\\%} & \\textbf{86.11\\%} \\\\\n & Trojan & 100\\% & 82.14\\% & 66.93\\% & 80.17\\% & 23.88\\% & 76.47\\% & 18.16\\% & 80.23\\% & \\textbf{3.81\\%} & \\textbf{87.46\\%} \\\\\n & Blend & 100\\% & 84.51\\% & 85.62\\% & 81.33\\% & 31.85\\% & 76.53\\% & \\textbf{4.56\\%} & 82.04\\% & 16.23\\% & \\textbf{84.06}\\% \\\\\n & Dynamic & 100\\% & 83.88\\% & 87.18\\% & 80.37\\% & 26.86\\% & 70.36\\% & 22.50\\% & 74.95\\% & \\textbf{18.46\\%} & \\textbf{85.34\\%} \\\\\n & SIG & 99.46\\% & 84.16\\% & 76.32\\% & 81.12\\% & 0.14\\% & 78.65\\% & 1.92\\% & 82.01\\% & \\textbf{0.09}\\% & \\textbf{88.27\\%} \\\\\n & CL & 99.83\\% & 83.43\\% & 54.95\\% & 81.53\\% & 19.86\\% & 77.36\\% & 16.11\\% & 80.73\\% & \\textbf{0\\%} & \\textbf{89.03\\%} \\\\ \\cline{2-12} \n  & FC & 88.52\\% & 83.32\\% & 69.89\\% & 80.51\\% & 44.43\\% & 77.57\\% & 58.68\\% & 81.23\\% & \\textbf{0.08}\\% & \\textbf{82.36}\\% \\\\\n & DFST & 99.76\\% & 82.50\\% & 78.11\\% & 80.23\\% & 39.22\\% & 75.34\\% & 35.21\\% & 78.40\\% & \\textbf{5.33\\%} & \\textbf{79.78\\%} \\\\\n & LBA & 99.13\\% & 81.37\\% & 54.43\\% & 79.67\\% & 15.52\\% & 78.51\\% & 10.16\\% & 79.52\\% & \\textbf{0.06}\\% & \\textbf{80.52\\%} \\\\\n & CBA & 90.63\\% & 84.72\\% & 77.33\\% & 79.15\\% & 38.76\\% & 76.36\\% & 33.11\\% & 82.40\\% & \\textbf{29.81\\%} & \\textbf{84.66\\%} \\\\\\cline{2-12} \n & \\multicolumn{1}{l}{Average} & \\multicolumn{1}{|l}{97.73\\%} & \\multicolumn{1}{l}{83.55\\%} & \\multicolumn{1}{|l}{75.07\\%} & \\multicolumn{1}{l}{80.62\\%} & \\multicolumn{1}{|l}{24.38\\%} & \\multicolumn{1}{l}{76.56\\%} & \\multicolumn{1}{|l}{20.40\\%} & \\multicolumn{1}{l}{80.37\\%}  & \\multicolumn{1}{|c}{\\textbf{7.69\\%}} & \\multicolumn{1}{c}{\\textbf{84.76\\%}} \\\\ \\midrule\n\\multirow{6}{*}{GTSRB} \n& \\emph{None} & 0\\% & 97.87\\% & 0\\% & 90.14\\% & 0\\% & 95.49\\% & 0\\% & 95.18\\% & 0\\% & \\textbf{96.41\\%} \\\\ \\cline{2-12} \n& BadNets & 100\\% & 97.38\\% & 99.57\\% & 88.61\\% & 1.00\\% & 93.45\\% & 0.19\\% & 89.52\\% & \\textbf{0.03\\%} & \\textbf{96.01\\%} \\\\\n & Trojan & 99.80\\% & 96.27\\% & 93.54\\% & 84.22\\% & 2.76\\% & 92.98\\% & 0.37\\% & 90.02\\% & \\textbf{0.36\\%} & \\textbf{94.95\\%} \\\\\n & Blend & 100\\% & 95.97\\% & 99.50\\% & 86.67\\% & \\textbf{6.83\\%} & 92.91\\% & 8.10\\% & 89.37\\% & 24.59\\% & \\textbf{93.14\\%} \\\\\n & Dynamic & 100\\% & 97.27\\% & 99.84\\% & 88.38\\% & 64.82\\% & 43.91\\% & 68.71\\% & 76.93\\% & \\textbf{6.24\\%} & \\textbf{95.80\\%} \\\\\n & SIG & 97.13\\% & 97.13\\% & 79.28\\% & 90.50\\% & 33.98\\% & 91.83\\% & \\textbf{4.64\\%} & 89.36\\% & 5.13\\% & \\textbf{96.33\\%} \\\\  \\cline{2-12} \n & \\multicolumn{1}{l}{Average} & \\multicolumn{1}{|l}{99.38\\%} & \\multicolumn{1}{l}{96.80\\%} & \\multicolumn{1}{|l}{94.35\\%} & \\multicolumn{1}{l}{87.68\\%} & \\multicolumn{1}{|l}{21.88\\%} & \\multicolumn{1}{l}{83.01\\%} & \\multicolumn{1}{|l}{19.17\\%} & \\multicolumn{1}{l}{87.04\\%} &  \\multicolumn{1}{|l}{\\textbf{7.27\\%}} & \\multicolumn{1}{l}{\\textbf{95.25\\%}} \\\\ \\midrule\n\\multirow{4}{*}{\\begin{tabular}[c]{@{}c@{}}ImageNet\\\\ Subset\\end{tabular}} \n& \\emph{None} & 0\\% & 89.93\\% & 0\\% & 83.14\\% & 0\\% & 85.49\\% & 0\\% & 88.18\\% & 0\\% & \\textbf{88.31\\%} \\\\ \\cline{2-12} \n& BadNets & 100\\% & 84.41\\% & 97.70\\% & 82.81\\% & 28.59\\% & 78.52\\% & 6.32\\% & 81.26\\% & \\textbf{0.94\\%} & \\textbf{87.76\\%} \\\\\n & Trojan & 100\\% & 85.56\\% & 96.39\\% & 80.34\\% & 6.67\\% & 76.87\\% & 15.48\\% & 80.52\\% & \\textbf{1.47\\%} & \\textbf{88.19\\%} \\\\\n & Blend & 99.93\\% & 86.15\\% & 99.34\\% & 81.33\\% & \\textbf{19.23\\%} & 75.83\\% & 26.47\\% & 82.39\\% & 21.42\\% & \\textbf{85.12\\%} \\\\ \n & SIG & 98.60\\% & 86.02\\% & 78.82\\% & 85.72\\% & 25.14\\% & 78.87\\% & 5.15\\% & 83.01\\% & \\textbf{0.18}\\% & \\textbf{86.42\\%} \\\\ \\cline{2-12} \n & \\multicolumn{1}{l}{Average} & \\multicolumn{1}{|l}{99.63\\%} & \\multicolumn{1}{l}{85.53\\%} & \\multicolumn{1}{|l}{93.06\\%} & \\multicolumn{1}{l}{82.55\\%} & \\multicolumn{1}{|l}{19.91\\%} & \\multicolumn{1}{l}{77.52\\%} & \\multicolumn{1}{|l}{13.35\\%} & \\multicolumn{1}{l}{81.80\\%} & \\multicolumn{1}{|c}{\\textbf{6.00\\%}} & \\multicolumn{1}{c}{\\textbf{86.87\\%}} \\\\\n \\bottomrule\n\\end{tabular}\n\\vspace{-0.1in}\n\\end{table}\n\n\\subsection{Effectiveness of Our ABL Defense}\\label{sec:4.1}\n\n\\noindent\\textbf{Comparison to Existing Defenses.} Table \\ref{tab1} demonstrates our proposed ABL method on CIFAR-10, GTSRB, and an ImageNet Subset. We consider 10 state-of-the-art backdoor attacks and compare the performance of ABL with three other backdoor defense techniques. It is clear that our ABL achieves the best results on reducing ASR against most of backdoor attacks, while maintaining an extremely high CA across all three datasets.\nIn comparison to the best baseline method NAD, our ABL achieves 12.71\\% (7.69\\% vs. 20.40\\%), 11.90\\% (7.27\\% vs. 19.17\\%), and 7.35\\% (6.00\\% vs. 13.35\\%) lower average ASR against the 10 attacks on CIFAR-10, GTSRB, and the ImageNet subset, respectively. This superiority becomes more significant when compared to other baseline methods. \n\nWe notice that our ABL is not always the best when looking at the 10 attacks individually. For instance, NAD is the best defense against the Blend attack on CIFAR-10 and against the SIG attack on GTSRB, while MCR is the best against Blend on GTSRB and the ImageNet subset. We suspect this is because both Blend and SIG mingle the trigger pattern (i.e., another image or superimposed sinusoidal signal) with the background of the poisoned images, producing an effect of natural artifacts. This makes them harder to be isolated and unlearned, since even clean data can have such patterns \\cite{zhao2021deep}. This is one limitation of our ABL that needs \nto be improved in future works. Our ABL achieves a much better performance than the baselines on defending against the 4 feature-space attacks. For example, NAD only manages to decrease the attack success rates of the FC and the DFST attacks to 58.68\\% and 35.21\\%, respectively. In contrast, our ABL can bring their ASRs down to below 10\\%. \nThe Dynamic and the CBA attacks are found to be the toughest attacks to defend against in general. For example, baseline methods NAD, MCR and FP can only decrease CBA's ASR to 33.11\\%, 38.76\\%, and 77.33\\% on CIFAR-10, and Dynamic's ASR to 68.71\\%, 64.82\\%, and 99.84\\% on GTSRB, respectively, a result that is much worse than the 29.81\\% and 6.24\\% ASRs of our ABL.\n\nMaintaining the clean accuracy is as important as reducing the ASR, as the model would lose utility if its clean accuracy is much compromised by the defense. By inspecting the average CA results in Table \\ref{tab1}, one can find that our ABL achieves nearly the same clean accuracy as models trained on 100\\% clean (shown in row \\emph{None} and column `No Defense') datasets. Specifically, our ABL surpasses the average clean accuracy of NAD by 4.39\\% (84.76\\% vs. 80.37\\%), 8.21\\% (95.25\\% vs. 87.04\\%) and 5.07\\% (86.87\\% vs. 81.80\\%) on CIFAR-10, GTSRB, and the ImageNet subset, respectively. FP defense decreases the model's performance even when training data are clean (the \\emph{None} row). This makes our ABL defense more practical for industrial applications where performance is equally important as security. Results of our ABL with 1\\% isolation against 1\\% poisoning can be found in Table \\ref{tab10} (Appendix \\ref{appendix:b9}).\n\n\\begin{figure}[!tp]\n\t\\centering\n\t\\includegraphics[width = .45\\linewidth]{figs/Isolation_rate_asr.pdf}\n\t\\includegraphics[width = .45\\linewidth]{figs/Isolation_rate_acc.pdf}\n\t\\vspace{-0.1 in}\n\t\\caption{Performance of our ABL with different isolation rate $p \\in [0.01, 0.2]$ on CIFAR-10 dataset. Left: attack success rate (ASR); Right: clean accuracy of ABL against 6 classic backdoor attacks.}\n\t\\label{img3}\n\t\\vspace{-0.1 in}\n\\end{figure}\n\n\\noindent\\textbf{Effectiveness with Different Isolation Rates.} Here, we study the correlation between the isolation rate $p = |\\widehat{\\gD}_b|/|\\gD|$ and the performance of our ABL, on the CIFAR-10 dataset. We run ABL with different $p \\in [0.01, 0.2]$ and show attack success rates and clean accuracies in Figure \\ref{img3}. There is a trade-off between ASR reduction and clean accuracy. Specifically, a high isolation rate can isolate more backdoor examples for the later stage of unlearning, producing a much lower ASR. However, it also puts more examples into the unlearning mode, which harms the clean accuracy. In general, ABL with an isolation rate $< 5\\%$ works reasonably well against all 6 attacks, even though the backdoor poisoning rate is much higher, i.e., 70\\% (see Figure \\ref{tab2} in Section \\ref{sec:stress testing}). Along with the results in Table \\ref{tab1}, this confirms that it is indeed possible to break and unlearn the backdoor correlation with only a tiny subset of correctly-identified backdoor examples, highlighting one unique advantage of backdoor isolation and unlearning approaches. \n\n\\begin{figure}[!tp]\n\t\\centering\n\t\\includegraphics[width = .45\\linewidth]{figs/gamma_loss.pdf}\n\t\\includegraphics[width = .45\\linewidth]{figs/gamma_acc_asr.pdf}\n\t\\vspace{-0.1 in}\n\t\\caption{Separation effect of local gradient ascent with different $\\gamma$ on CIFAR-10 against BadNets. Left: Training loss on the ground truth backdoor ($\\gD_b$) and clean ($\\gD_c$) subsets; Right: Attack success rate (ASR) and clean accuracy (CA). The gap between the two lines of the same color becomes wider for larger $\\gamma$, i.e., better separation effect.}\n\t\\label{img4}\n\t\\vspace{-0.2 in}\n\\end{figure}\n\n\\noindent\\textbf{Effectiveness with Different Turning Epochs.} \nHere, we study the impact of the timing to switch from the learning stage ($\\gL_{\\textup{LGA}}$) to the unlearning stage ($\\gL_{\\textup{GGA}}$) on CIFAR-10. We compare four different turning epochs: the 10th, 20th, 30th, and 40th epoch, and record the results of our ABL in Table \\ref{tab6} (see Appendix \\ref{appendix:b4}). We find that delayed turning epochs tend to slightly hinder the defense performance. Despite the slight variations, all choices of the turning epoch help mitigate backdoor attacks, but epoch 20 (i.e., at 20\\% - 30\\% of the entire training progress) achieves the best overall results. This trend is consistent on other datasets as well. We attribute these results to the success of LGA in preserving the difference between clean and backdoor samples over time, which enables us to select the turning epoch flexibly. A more comprehensive discussion on LGA is given in Section~\\ref{sec:4.2}.\n\n\\subsection{Comprehensive Understanding of ABL}\\label{sec:4.2}\n\\noindent\\textbf{Importance of Local Gradient Ascent.} To help understand how LGA works in isolating backdoor data, we visualize and compare in Figure \\ref{img4} the training loss and the model's performances (ASR and CA) under three different settings where $\\gamma$ is set to 0.5, 1.0, and 1.5, respectively. It is evident that LGA can segregate backdoor examples from clean examples to a certain extent under all three settings of $\\gamma$ by preventing the loss of clean examples from converging. Moreover, a larger $\\gamma$ leads to a wider difference in training loss as well as ASR and CA. However, we note that this may cause training instability, as evidenced by the relatively larger fluctuations with $\\gamma=1.5$.\n\nWe also examine the precision of the 1\\% isolated backdoor set under different $\\gamma$ of 0, 0.5, 1.0, and 1.5 on CIFAR-10, GTSRB, and the ImageNet subset. We use BadNets attack with a poisoning rate 10\\% and set the turning (isolation) epoch of ABL to 20. We report the isolation precision results in Table \\ref{tab7} (see Appendix \\ref{appendix:b5}). As can be seen, when $\\gamma = 0$, the detection precision is poor; this indicates that it is tough for the model to tell apart backdoor examples from the clean ones without the LGA, which is foreseeable because the clean training loss is uncontrolled and overlaps with the backdoor training loss. Note that as soon as we set $\\gamma > 0$, the precision immediately improves on both CIFAR-10 and the ImageNet subset. Additionally, the precision of the isolation task is not sensitive to the change in $\\gamma$, which again allows the hyperparameter value to be flexibly chosen. In our experiments, $\\gamma=0.5$ works reasonably well across different datasets and models.\n\nIn summary, LGD creates and sustains a gap between the training loss of clean and backdoor examples, which plays a vital role in extracting an isolated backdoor set.\n\n\\begin{table}[!tp]\n\\renewcommand{\\arraystretch}{1.2}\n\\renewcommand\\tabcolsep{1.8pt}\n\\small\n\\centering\n\\caption{Stress testing with poisoning rate up to 50\\% and 70\\% for 4 attacks including BadNets, Trojan, Blend, and Dynamic on CIFAR10 dataset. }\n\\label{tab2}\n\\begin{tabular}{c|c|cccccccc}\n\\hline\n\\multirow{2}{*}{Poisoning Rate} & \\multirow{2}{*}{Defense} & \\multicolumn{2}{c}{BadNets} & \\multicolumn{2}{c}{Trojan} & \\multicolumn{2}{c}{Blend} & \\multicolumn{2}{c}{Dynamic} \\\\ \\cline{3-10} \n &  & ASR & ACC & ASR & ACC & ASR & ACC & ASR & ACC \\\\ \\hline\n\\multirow{2}{*}{50\\%} & \\emph{None} & 100\\% & 75.31\\% & 100\\% & 70.44\\% & 100\\% & 69.49\\% & 100\\% & 66.15\\% \\\\ \n & ABL & 4.98\\% & 70.52\\% & 16.11\\% & 68.56\\% & 27.28\\% & 64.19\\% & 25.74\\% & 61.32\\% \\\\ \\hline\n\\multirow{2}{*}{70\\%} & \\emph{None} & 100\\% & 74.8\\% & 100\\% & 69.46\\% & 100\\% & 67.32\\% & 100\\% & 66.15\\% \\\\  \n & ABL & 5.02\\% & 70.11\\% & 29.29\\% & 68.79\\% & 62.28\\% & 64.43\\% & 69.36\\% & 62.09\\% \\\\ \\hline\n\\end{tabular}\n\\vspace{-0.1in}\n\\end{table}\n\n\\noindent\\textbf{Stress Testing: Fixing 1\\% Isolation Rate While Increasing Poisoning Rate.} \\label{sec:stress testing}\nNow that we know we can confidently extract a tiny subset of backdoor examples with high purity, the challenge remains whether the extracted set is sufficient for the model to unlearn the backdoor. We demonstrate that our ABL is a stronger method, even under this strenuous setting. Here, we experiment on CIFAR-10 against 4 attacks including BadNets, Trojan, Blend, and Dynamic with poisoning rates up to 50\\%/70\\% and show the results in Table \\ref{tab2}. We can find that even with a high poisoning rate of 50\\%, our ABL method can still reduce the ASR from 100\\% to 4.98\\%, 16.11\\%, 27.28\\%, and 25.74\\% for BadNets, Trojan, Blend, and Dynamic, respectively. Note that ABL will break when the poisoning rate reaches 70\\%. In this case however, the dataset should not be used to train any models in the first place. Overall, ABL remains effective against up to 1) 70\\% BadNets; and 2) 50\\% Trojan, Blend, and Dynamic. This finding is very compelling, considering ABL needs to isolate only 1\\% of training data. As we mentioned before, this is because the correlation between the backdoor pattern and the target label exposes a weakness of backdoor attacks. Our ABL utilizes the GGA to break this link and achieve defense goals effortlessly.\n\n\\subsection{Exploring Alternative Isolation and Unlearning Methods}\\label{sec:4.3}\n\\noindent\\textbf{Alternative Isolation Methods.}  \\label{sec:alternatives}\nIn this section, we compare the isolation precision of our ABL with two backdoor detection methods, namely Activation Clustering (AC) ~\\cite{chen2018detecting} and Spectral Signature Analysis (SSA)~\\cite{tran2018spectral}. The goal is to isolate 1\\% of training examples into the backdoor set ($\\widehat{\\gD}_b$), and we provide in Figure \\ref{img7} (see Appendix \\ref{appendix:b3}) the precision of these methods alongside our ABL in detecting the 6 backdoor attacks on CIFAR-10 dataset. We find that both AC and SS achieve high detection rates on BadNets and Trojan attacks while perform poorly on 4 other attacks. A reasonable explanation is that attacks covering the whole image with complex triggers (e.g., Blend, Dynamic, SIG, and CL) give confusing and unidentifiable output representations of either feature activation or spectral signature, making these detection methods ineffective. It is worth mentioning that our ABL is effective against all backdoor attacks with the highest average detection rate. In addition, we find that the flooding loss \\cite{ishida2020we} proposed for mitigating overfitting is also very effective for backdoor isolation. \nWe also explore a confidence-based isolation with label smoothing (LS), which unfortunately fails on most attacks. More details of these explorations can be found in Figure \\ref{img8} and \\ref{img9} in Appendix \\ref{appendix:b6}. \n\n\\begin{table}[!htbp]\n\\renewcommand{\\arraystretch}{1.1}\n\\renewcommand\\tabcolsep{3.0pt}\n\\small\n\\centering\n\\caption{Performance of various unlearning methods against BadNets attack on CIFAR-10. }\n \\label{tab3}\n\\begin{tabular}{c|c|lc|cccc}\n\\toprule\n\\multirow{2}{*}{Backdoor Unlearning Methods} & \\multicolumn{2}{c}{\\multirow{2}{*}{Method Type}} & Discard & \\multicolumn{2}{c}{Backdoored} & \\multicolumn{2}{c}{After Unlearning} \\\\ \\cline{5-8} \n & \\multicolumn{2}{c}{} & $\\widehat{\\gD}_b$ & ASR & CA & ASR & CA \\\\ \\hline\nPixel Noise & \\multicolumn{2}{c}{Image-based} & No & 100\\% & 85.43\\% & 57.54\\% & 82.33\\% \\\\\nGrad Noise & \\multicolumn{2}{c}{Image-based} & No & 100\\% & 85.43\\% & \\textbf{47.65}\\% & \\textbf{82.62}\\% \\\\ \\hline\nLabel Shuffling & \\multicolumn{2}{c}{Label-based} & No & 100\\% & 85.43\\% & 30.23\\% & 83.76\\% \\\\\nLabel Uniform & \\multicolumn{2}{c}{Label-based} & No & 100\\% & 85.43\\% & 75.12\\% & 83.47\\% \\\\\nLabel Smoothing & \\multicolumn{2}{c}{Label-based} & No & 100\\% & 85.43\\% & 99.80\\% & 83.17\\% \\\\\nSelf-Learning & \\multicolumn{2}{c}{Label-based} & No & 100\\% & 85.43\\% & \\textbf{21.26}\\% & \\textbf{84.38}\\% \\\\ \\hline\nFinetuning All Layers & \\multicolumn{2}{c}{Model-based} & Yes & 100\\% & 85.43\\% & 99.12\\% & 83.64\\% \\\\\nFinetuning Last Layers & \\multicolumn{2}{c}{Model-based} & Yes & 100\\% & 85.43\\% & 22.33\\% & 77.65\\% \\\\\nFinetuning ImageNet Model & \\multicolumn{2}{c}{Model-based} & Yes & 100\\% & 85.43\\% & 12.18\\% & 75.10\\% \\\\\nRe-training from Scratch & \\multicolumn{2}{c}{Model-based} & Yes & 100\\% & 85.43\\% & 11.21\\% & 86.02\\% \\\\ \\hline\n\\textbf{ABL} & \\multicolumn{2}{c}{Model-based} & No & 100\\% & 85.43\\% & \\textbf{3.04}\\% & \\textbf{86.11}\\%\\\\\n\\bottomrule\n\\end{tabular}\n\\vspace{-0.1in}\n\\end{table}\n\n\\noindent\\textbf{Alternative Unlearning Methods.} Here we explore several other empirical strategies, including image-based, label-based, model-based approaches, to rebuild a clean model on the poisoned data. These approaches are motivated by the second weakness of backdoor attacks, and are all designed to break the connection between the trigger pattern and the target class. We experiment on CIFAR-10 with BadNets (10\\% poisoning rate), and fix the backdoor isolation method to our ABL with a high isolation rate 20\\% (as most of them will fail with 1\\% isolation).\nTable \\ref{tab3} summarizes our explorations. Our core findings can be summarized as: \\textbf{a)} adding perturbations to pixels or gradients is not effective; \\textbf{b)} changing the labels of isolated examples is mildly effective; \\textbf{c)} finetuning some (not all) layers of the model cannot effectively mitigate backdoor attacks; \\textbf{d)} ``self-learning” and ``retraining the model from scratch” on the isolated clean set are good choices against backdoor attacks; and \\textbf{e)} our ABL presents the best unlearning performance. Details of these methods are given in Appendix \\ref{appendix:a4}. The performance of these methods under the 1\\% isolation rate is also reported in Table \\ref{tab8} in Appendix \\ref{appendix:b7}. We also considered another widely used attack settings that the user only allowed to access a backdoored model and hold limited benign data. In this case, we proposed to combine our ABL unlearning with Neural Cleanse \\cite{wang2019neural} to erase the backdoroed model (see Appendix \\ref{appendix:b10}).\n\n\\section{Conclusion} \\label{sec:5}\nIn this work, we identified two inherent characteristics of backdoor attacks as their weaknesses: 1) backdoor examples are easier and faster to learn than clean examples, and 2) backdoor learning establishes a strong correlation between backdoor examples and the target label. Based on these two findings, we proposed a novel framework - Anti-Backdoor Learning (ABL) - which consists of two stages of learning utilizing local gradient ascent (LGA) and global gradient ascent (GGA), respectively. At the early learning stage, we use LGA to intentionally maximize the training loss gap between clean and backdoor examples to isolate out the backdoored data via the low loss value. We use GGA to unlearn the backdoored model with the isolated backdoor data at the last learning stage. Empirical results demonstrate that our ABL is resilient to various experimental settings and can effectively defend against 10 state-of-the-art backdoor attacks. \nOur work introduces a simple but very effective ABL method for industries to train backdoor-free models on real-world datasets, and opens up an interesting research direction for robust and secure machine learning.\n\n\\section*{Broader Impact} \\label{sec:6}\nLarge-scale data have been key to the success of deep learning. However, it is hard to guarantee the quality and purity of the training data in many cases, and even high-quality datasets may contain backdoors, especially those collected from the internet. By introducing the concept of \\emph{anti-backdoor learning} (ABL), our work opens up a new direction for secure and robust learning with not-fully-trusted data. Even in the clean setting, ABL can prevent deep learning models from overfitting to those overly easy samples. Beyond backdoor defense, ABL could be explored as a generic \\emph{data-quality-ware} learning mechanism in place of the traditional data-quality-agnostic learning. Such a mechanism may help reduce many potential data-quality-related risks such as memorization, overfitting, backdoors and biases. Although not our initial intention, our work may adversely be exploited to develop advanced backdoor attacks. This essentially requires new defenses to combat.\n\n\\section*{Acknowledgement}\nThis work is supported by China National Science Foundation under grant number 62072356 and in part by the Key Research and Development Program of Shaanxi under Grant 2019ZDLGY12-08.\n\n\\newpage\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Backdoor Defense via Decoupling the Training Process}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nRecent studies have revealed that deep neural networks (DNNs) are vulnerable to backdoor attacks, where attackers embed hidden backdoors in the DNN model by poisoning a few training samples. The attacked model behaves normally on benign samples, whereas its prediction will be maliciously changed when the backdoor is activated. We reveal that poisoned samples tend to cluster together in the feature space of the attacked DNN model, which is mostly due to the end-to-end supervised training paradigm. Inspired by this observation, we propose a novel backdoor defense via decoupling the original end-to-end training process into three stages. Specifically, we first learn the backbone of a DNN model via \\emph{self-supervised learning} based on training samples without their labels. The learned backbone will map samples with the same ground-truth label to similar locations in the feature space. Then, we freeze the parameters of the learned backbone and train the remaining fully connected layers via standard training with all (labeled) training samples. Lastly, to further alleviate side-effects of poisoned samples in the second stage, we remove labels of some `low-credible' samples determined based on the learned model and conduct a \\emph{semi-supervised fine-tuning} of the whole model. Extensive experiments on multiple benchmark datasets and DNN models verify that the proposed defense is effective in reducing backdoor threats while preserving high accuracy in predicting benign samples. Our code is available at \\url{https://github.com/SCLBD/DBD}.\n\\end{abstract}\n\n\\section{Introduction}\n\\label{sec:intro}\n\n\\comment{\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.473\\textwidth]{fig_intro.pdf}\n    \\caption{The t-SNE of poisoned samples in the hidden space generated by attacked DNNs trained in supervised and self-supervised manner. As shown in the figure, poisoned samples tend to cluster together of DNN (within the red box) trained with standard supervised learning, whereas lie closely to samples with their ground-truth label of the one trained with self-supervised learning. }\n    \\label{fig:intro}\n    \\vspace{-1em}\n\\end{figure}\n}\n\nDeep learning, especially deep neural networks (DNNs), has been widely adopted in many realms \\citep{wang2020pillar,li2020short,wen20adaptive} for its high effectiveness. In general, the training of DNNs requires a large amount of training samples and computational resources. Accordingly, third-party resources ($e.g.$, third-party data or servers) are usually involved. While the opacity of the training process brings certain convenience, it also introduces new security threats.\n \n\nBackdoor attack poses a new security threat to the training process of DNNs \\citep{li2020backdoor}. It maliciously manipulates the prediction of the attacked DNNs by poisoning a few training samples. Specifically, backdoor attackers inject the \\emph{backdoor trigger} ($i.e.$, a particular pattern) to some benign training images and change their labels with the attacker-specified \\emph{target label}. The connection between the backdoor trigger and the target label will be learned by DNNs during the training process. In the inference process, the prediction of attacked DNNs will be changed to the target label when the trigger is present, whereas the attacked DNNs will behave normally on benign samples. As such, users are difficult to realize the existence of hidden backdoors and therefore this attack is a serious threat to the practical applications of DNNs.\n\nIn this paper, we first investigate backdoor attacks from the hidden feature space. Our preliminary experiments reveal that the backdoor is embedded in the feature space, $i.e.$, samples with the backdoor trigger (dubbed \\emph{poisoned samples}) tend to cluster together in the feature space. We reveal that \\emph{this phenomenon is mostly due to the end-to-end supervised training paradigm}. Specifically, the excessive learning capability allows DNNs to learn features about the backdoor trigger, while the DNNs can shrink the distance between poisoned samples in the feature space and connect the learned trigger-related features with the target label by the end-to-end supervised training. Based on this understanding, we propose to decouple the end-to-end training process for the backdoor defense. Specifically, we treat the DNNs as two disjoint parts, including a \\emph{feature extractor} ($i.e.$, backbone) and a \\emph{simple classifier} ($i.e.$, the remaining fully connected layers). We first learn the \\emph{purified feature extractor} via \\emph{self-supervised learning} \\citep{kolesnikov2019revisiting,chen2020simple,jing2020self} with unlabeled training samples (obtained by removing their labels), and then learn the simple classifier via standard supervised training process based on the learned feature extractor and all training samples. The strong data augmentations involved in the self-supervised learning damage trigger patterns, making them unlearnable during representation learning; and the decoupling process further disconnects trigger patterns and the target label. Accordingly, hidden backdoors cannot be successfully created even the model is trained on the poisoned dataset based on our defense.\n\nMoreover, we further reveal that the representation of poisoned samples generated by the purified extractor is significantly different from those generated by the extractor learned with standard training process. Specifically, the poisoned sample lies closely to samples with its ground-truth label instead of the target label. This phenomenon makes the training of the simple classifier similar to \\emph{label-noise learning} \\citep{wang2019symmetric,ma2020normalized,berthon2021confidence}. As such, we first filter \\emph{high-credible training samples} ($i.e.$, training samples that are most probably to be benign) and then use those samples as labeled samples and the remaining part to form unlabeled samples to fine-tune the whole model via \\emph{semi-supervised learning} \\citep{rasmus2015semi,berthelot2019mixmatch,sohn2020fixmatch}. This approach is to further reduce the adverse effects of poisoned samples.\n\nThe main contributions of this paper are three-fold. \\textbf{(1)} We reveal that the backdoor is embedded in the feature space, which is mostly due to the end-to-end supervised training paradigm. \\textbf{(2)} Based on our understanding, we propose a decoupling-based backdoor defense (DBD) to alleviate the threat of poisoning-based backdoor attacks. \\textbf{(3)} Experiments on classical benchmark datasets are conducted, which verify the effectiveness of our defense.\n\n\\vspace{-0.5em}\n\\section{Related Work}\n\\vspace{-0.5em}\n\n\\subsection{Backdoor Attack}\n\\label{sec:related_attack}\n\nBackdoor attack is an emerging research area, which raises security concerns about training with third-party resources. In this paper, we focus on the poisoning-based backdoor attack towards image classification, where attackers can only modify the dataset instead of other training components ($e.g.$, training loss). This threat could also happen in other tasks \\citep{xiang2021backdoor,zhai2021backdoor,li2022few} and with different attacker's capacities \\citep{nguyen2020input,tang2020embarrassingly,zeng2021rethinking}, which are out-of-scope of this paper. \nIn general, existing attacks can be divided into two main categories based on the property of target labels, as follows:\n\n\\noindent \\textbf{Poison-Label Backdoor Attack.}\nIt is currently the most common attack paradigm, where the target label is different from the ground-truth label of poisoned samples. BadNets \\citep{gu2017badnets} is the first and most representative poison-label attack. %Almost all follow-up attacks were carried out based on this method. \nSpecifically, it randomly selected a few samples from the original benign dataset to generate \\emph{poisoned samples} by stamping the \\emph{backdoor trigger} onto the (benign) image and change their label with an attacker-specified \\emph{target label}. Those generated poisoned samples associated with remaining benign ones were combined to form the \\emph{poisoned training dataset}, which will be delivered to users. After that, \\citep{chen2017targeted} suggested that the poisoned image should be similar to its benign version for the stealthiness, based on which they proposed the \\emph{blended attack}. %In the blended attack, poisoned images were generated by blending the backdoor trigger with benign images instead of by stamping proposed in the BadNets. \nRecently, \\citep{xue2020one,li2020invisible,li2021invisible} further explored how to conduct poison-label backdoor attacks more stealthily. Most recently, a more stealthy and effective attack, the WaNet \\citep{nguyen2021wanet}, was proposed. WaNet adopted image warping as the backdoor trigger, which deforms but preserves the image content. \n\n\\noindent \\textbf{Clean-Label Backdoor Attack.} \nAlthough the poisoned image generated by poison-label attacks could be similar to its benign version, users may still notice the attack by examining the image-label relationship. To address this problem, \\cite{turner2019label} proposed the \\emph{clean-label attack paradigm}, where the target label is consistent with the ground-truth label of poisoned samples. Specifically, they first leveraged adversarial perturbations or generative models to modify some benign images from the target class and then conducted the standard trigger injection process. %The modification was to alleviate the effects of `robust features' about the target class contained in poisoned samples to ensure that the trigger can be successfully learned by the DNNs. \nThis idea was generalized to attack video classification in \\citep{zhao2020clean}, where they adopted the targeted universal adversarial perturbation \\citep{moosavi2017universal} as the trigger pattern. %Recently, Liu \\etal \\cite{liu2020reflection} proposed to adopt reflection as the trigger pattern for the attack.\nAlthough clean-label backdoor attacks are more stealthy compared with poison-label ones, they usually suffer from relatively poor performance and may even fail in creating backdoors \\citep{li2020backdoor}.\n\n\\subsection{Backdoor Defense}\nCurrently, there are also some approaches to alleviate the backdoor threat. Existing defenses are mostly \\emph{empirical}, which can be divided into five main categories, including \\textbf{(1)} \\emph{detection-based defenses} \\citep{xu2021detecting,zeng2021rethinking,xiang2022post}, \\textbf{(2)} \\emph{preprocessing based defenses} \\citep{doan2020februus,li2021backdoor2,zeng2021deepsweep}, \\textbf{(3)} \\emph{model reconstruction based defenses} \\citep{zhao2020bridging,li2021neural,zeng2022adversarial}, \\textbf{(4)} \\emph{trigger synthesis based defenses} \\citep{guo2020towards,dong2021black,shen2021backdoor}, and \\textbf{(5)} \\emph{poison suppression based defenses} \\citep{du2019robust,borgnia2021strong}. Specifically, detection-based defenses examine whether a suspicious DNN or sample is attacked and it will deny the use of malicious objects; Preprocessing based methods intend to damage trigger patterns contained in attack samples to prevent backdoor activation by introducing a preprocessing module before feeding images into DNNs; Model reconstruction based ones aim at removing the hidden backdoor in DNNs by modifying models directly; The fourth type of defenses synthesize potential trigger patterns at first, following by the second stage that the hidden backdoor is eliminated by suppressing their effects; The last type of methods depress the effectiveness of poisoned samples during the training process to prevent the creation of hidden backdoors. In general, our method is most relevant to this type of defenses. %Besides, the third and fourth type of methods need a local benign dataset in most cases, which is not required in our method.\n\nIn this paper, we only focus on the last four types of defenses since they directly improve the robustness of DNNs. Besides, there were also few works focusing on \\emph{certified backdoor defenses} \\citep{wang2020certifying,weber2020rab}.\nTheir robustness is theoretically guaranteed under certain assumptions, which cause these methods to be generally weaker than empirical ones in practice.\n\n\\subsection{Semi-supervised and Self-supervised Learning}\n\n\\noindent \\textbf{Semi-supervised Learning.} \nIn many real-world applications, the acquisition of labeled data often relies on manual labeling, which is very expensive. In contrast, obtaining unlabeled samples is much easier. To utilize the power of unlabeled samples with labeled ones simultaneously, a great amount of semi-supervised learning methods were proposed \\citep{gao2017semi,berthelot2019mixmatch,van2020survey}. Recently, semi-supervised learning was also introduced in improving the security of DNNs \\citep{stanforth2019,carmon2019}, where they utilized unlabelled samples in the adversarial training. Most recently, \\citep{yan2021dehib} discussed how to backdoor semi-supervised learning. However, this approach needs to control other training components ($e.g.$, training loss) in addition to modifying training samples and therefore is out-of-scope of this paper. How to adopt semi-supervised learning for backdoor defense remains blank.\n\n\\noindent \\textbf{Self-supervised Learning.}\nThis learning paradigm is a subset of \\emph{unsupervised learning}, where DNNs are trained with supervised signals generated from the data itself \\citep{chen2020simple,grill2020bootstrap,liu2021self}. It has been adopted for increasing adversarial robustness \\citep{hendrycks2019using,wu2021adversarial,shi2021online}. Most recently, there were also a few works \\citep{saha2021backdoor,carlini2021poisoning,jia2021badencoder} exploring how to backdoor self-supervised learning. However, these attacks are out-of-scope of this paper since they need to control other training components ($e.g.$, training loss) in addition to modifying training samples.\n\n\\vspace{-0.5em}\n\\section{Revisiting Backdoor Attacks from the Hidden Feature Space}\n\\vspace{-0.5em}\n\\label{sec:motivation}\n\n\\begin{figure}[ht]\n\\centering\n\\vspace{-3em}\n\\subfigure[]{\n\\centering\n\\includegraphics[width=0.235\\textwidth]{supervise_badnets_pratio0.05.pdf}\\label{fig:m_a}}\n\\vspace{-0.8em}\n\\subfigure[]{\n\\centering\n\\includegraphics[width=0.235\\textwidth]{supervise_label_consistent_e16_pratio0.25.pdf}\\label{fig:m_b}}\n\\vspace{-0.8em}\n\\subfigure[]{\n\\centering\n\\includegraphics[width=0.235\\textwidth]{self_badnets_pratio0.05.pdf}\\label{fig:m_c}}\n\\vspace{0.3em}\n\\subfigure[]{\n\\centering\n\\includegraphics[width=0.235\\textwidth]{self_label_consistent_e16_pratio0.25.pdf}\\label{fig:m_d}}\n\\vspace{0.3em}\n\\caption{The t-SNE of poisoned samples in the hidden space generated by different models. \\textbf{(a)-(b)}: DNNs trained with supervised learning. \\textbf{(c)-(d)}: DNNs trained with self-supervised learning. \\textbf{(a)\\&(c)}: DNNs under BadNets attack. \\textbf{(b)\\&(d)}: DNNs under label-consistent attack. No matter under the BadNets or label-consistent attack, poisoned samples tend to cluster together in the hidden space generated by DNNs trained with supervised learning, whereas lie closely to samples with their ground-truth label by those trained with self-supervised learning.}\n\\label{fig:motivation}\n\\vspace{-1.5em}\n\\end{figure}\n\nIn this section, we analyze the behavior of poisoned samples from the hidden feature space of attacked models and discuss its inherent mechanism.\n\n\\noindent \\textbf{Settings.} \nWe conduct the BadNets \\citep{gu2017badnets} and label-consistent attack \\citep{turner2019label} on CIFAR-10 dataset \\citep{krizhevsky2009learning} for the discussion. They are representative of poison-label attacks and clean-label attacks, respectively. Specifically, we conduct supervised learning on the poisoned datasets with the standard training process and self-supervised learning on the unlabelled poisoned datasets with SimCLR \\citep{chen2020simple}. We visualize poisoned samples in the hidden feature space generated by attacked DNNs based on the t-SNE \\citep{van2008visualizing}. More detailed settings are presented in Appendix \\ref{sec:motivation_set}.\n\n\\noindent \\textbf{Results.} \nAs shown in Figure \\ref{fig:m_a}-\\ref{fig:m_b}, poisoned samples (denoted by `black-cross') tend to cluster together to form a separate cluster after the standard supervised training process, no matter under the poison-label attack or clean-label attack. This phenomenon implies why existing poisoning-based backdoor attacks can succeed. Specifically, the excessive learning capability allows DNNs to learn features about the backdoor trigger. Associated with the end-to-end supervised training paradigm, DNNs can shrink the distance between poisoned samples in the feature space and connect the learned trigger-related features with the target label. In contrast, as shown in Figure \\ref{fig:m_c}-\\ref{fig:m_d}, poisoned samples lie closely to samples with their ground-truth label after the self-supervised training process on the unlabelled poisoned dataset. %Moreover, samples with the same label are still clustered together, although they are not clustered as closely as they were after the supervised learning. \nIt indicates that we can prevent the creation of backdoors by self-supervised learning, which will be further introduced in the next section. \n\n\\section{Decoupling-based Backdoor Defense}\n\n\\subsection{Preliminaries}\n\\noindent \\textbf{General Pipeline of Backdoor Attacks.} \nLet $\\mathcal{D} = \\{ (\\bm{x}_i, y_i) \\}_{i=1}^{N}$ denotes the benign training set, where $\\bm{x}_i \\in \\mathcal{X}= \\{0,1,\\ldots, 255\\}^{C\\times W \\times H}$ is the image, $y_i \\in \\mathcal{Y} = \\{0,1,\\ldots, K\\}$ is its label, $K$ is the number of classes, and $y_t \\in \\mathcal{Y}$ indicates the \\emph{target label}. How to generate the poisoned dataset $\\mathcal{D}_{p}$ is the cornerstone of backdoor attacks. Specifically, $\\mathcal{D}_{p}$ consists of two subsets, including the modified version of a subset of $\\mathcal{D}$ and remaining benign samples, $i.e.$, \n$\\mathcal{D}_{p} =  \\mathcal{D}_{m} \\cup \\mathcal{D}_{b}$, where $\\mathcal{D}_{b} \\subset \\mathcal{D}$, $\\gamma \\triangleq \\frac{|\\mathcal{D}_{m}|}{|\\mathcal{D}|}$ is the \\emph{poisoning rate}, \n$\\mathcal{D}_{m} = \\left\\{(\\bm{x}', y_t)| \\bm{x}' = G(\\bm{x}), (\\bm{x},y) \\in \\mathcal{D} \\backslash \\mathcal{D}_{b} \\right\\}$, and $G: \\mathcal{X} \\rightarrow \\mathcal{X}$ is an attacker-predefined poisoned image generator. For example, $G(\\bm{x}) = (\\bm{1}-\\bm{\\lambda}) \\otimes \\bm{x}+ \\bm{\\lambda} \\otimes \\bm{t}$, where $\\bm{\\lambda} \\in [0,1]^{C \\times W \\times H}$, $\\bm{t} \\in \\mathcal{X}$ is the \\emph{trigger} pattern, and $\\otimes$ is the element-wise product in the blended attack \\citep{chen2017targeted}. Once $\\mathcal{D}_{p}$ is generated, it will be sent to users who will train DNNs on it. Hidden backdoors will be created after the training process.\n\n\\noindent \\textbf{Threat Model.} \nIn this paper, we focus on defending against poisoning-based backdoor attacks. The attacker can arbitrarily modify the training set whereas cannot change other training components ($e.g.$, model structure and training loss). For our proposed defense, we assume that defenders can fully control the training process. This is the scenario that users adopt third-party collected samples for training. Note that we do not assume that defenders have a local benign dataset, which is often required in many existing defenses \\citep{wang2019neural,zhao2020bridging,li2021neural}. \n\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=0.98\\textwidth]{overview.pdf}\n    \\vspace{-0.5em}\n    \\caption{The main pipeline of our defense. In the first stage, we train the whole DNN model via self-supervised learning based on label-removed training samples. In the second stage, we freeze the learned feature extractor and adopt all training samples to train the remaining fully connected layers via supervised learning. After that, we filter high-credible samples based on the training loss. In the third stage, we adopt high-credible samples as labeled samples and remove the labels of all low-credible samples to fine-tune the whole model via semi-supervised learning. }\n    \\vspace{-0.3em}\n    \\label{fig:overview}\n\\end{figure*}\n\n\\noindent \\textbf{Defender's Goals.} \nThe defender's goals are to prevent the trained DNN model from predicting poisoned samples as the target label and to preserve the high accuracy on benign samples.\n\n\\subsection{Overview of the Defense Pipeline}\nIn this section, we describe the general pipeline of our defense. As shown in Figure \\ref{fig:overview}, it consists of three main stages, including \\textbf{(1)} learning a  purified feature extractor via self-supervised learning, \\textbf{(2)} filtering high-credible samples via label-noise learning, and \\textbf{(3)} semi-supervised fine-tuning. \n\nSpecifically, in the first stage, we remove the label of all training samples to form the unlabelled dataset, based on which to train the feature extractor via self-supervised learning. In the second stage, we freeze the learned feature extractor and adopt all training samples to train the remaining fully connected layers via supervised learning. We then filter $\\alpha\\%$ high-credible samples based on the training loss. The smaller the loss, the more credible the sample. After the second stage, the training set will be separated into two disjoint parts, including high-credible samples and low-credible samples. We use high-credible samples as labeled samples and remove the label of all low-credible samples to fine-tune the whole model via semi-supervised learning. More detailed information about each stage of our method will be further illustrated in following sections.\n\n\\subsection{Learning Purified Feature Extractor via Self-supervised Learning}\n\\label{sec:purified}\n\nLet $\\mathcal{D}_t$ denotes the training set and $f_{\\bm{w}}: \\mathcal{X} \\rightarrow [0,1]^K$ indicates the DNN with parameter $\\bm{w}=[\\bm{w}_c, \\bm{w}_f]$, where $\\bm{w}_c$ and $\\bm{w}_f$ indicates the parameters of the backbone and the fully connected layer, respectively. In this stage, we optimize $\\bm{w}_c$ based on the unlabeled version of $\\mathcal{D}_t$ via self-supervised learning, as follows:\n\\begin{equation}\\label{eq:w_c}\n    \\bm{w}_c^{*} = \\arg \\min_{\\bm{w}_c} \\sum_{(\\bm{x},y) \\in \\mathcal{D}_t} \\mathcal{L}_1(\\bm{x};\\bm{w}_c),\n\\end{equation}\nwhere $\\mathcal{L}_1(\\cdot)$ indicates the self-supervised loss ($e.g.$, NT-Xent in SimCLR \\citep{chen2020simple}). %In this paper, we adopt the one proposed in the SimCLR \\cite{chen2020simple}.\nThrough the self-supervised learning, the learned feature extractor ($i.e.$, backbone) will be purified even if the training set contains poisoned samples, as illustrated in Section \\ref{sec:motivation}. \n\n\\subsection{Filtering High-credible Samples via Label-noise Learning}\n\\label{sec:filtering}\nOnce $\\bm{w}_c^{*}$ is obtained, the user can freeze it and adopt $\\mathcal{D}_t$ to further optimize remaining $\\bm{w}_f$, $i.e.$,  \n\\begin{equation}\\label{eq:w_f}\n    \\bm{w}_f^{*} = \\arg \\min_{\\bm{w}_f} \\sum_{(\\bm{x},y) \\in \\mathcal{D}_t} \\mathcal{L}_2\\left(f_{[\\bm{w}_c^{*},\\bm{w}_f]}(\\bm{x}), y \\right),\n\\end{equation}\nwhere $\\mathcal{L}_2(\\cdot)$ indicates the supervised loss ($e.g.$, cross entropy).\n\n\\begin{figure*}[ht]\n\\centering\n\\subfigure[Cross-Entropy]{\n\\centering\n\\includegraphics[width=0.475\\textwidth]{badnets_cifar_resnet_pratio0.2_ce.pdf}}\n\\hspace{0.3em}\n\\subfigure[Symmetric Cross-Entropy]{\n\\centering\n\\includegraphics[width=0.475\\textwidth]{badnets_cifar_resnet_pratio0.2_sce.pdf}}\n\\vspace{-0.7em}\n\\caption{Loss values of models under BadNets attack with 20\\% poisoning rate trained on CIFAR-10 dataset with the symmetric cross-entropy (SCE) and cross-entropy (CE) in the second stage. All loss values are normalized to $[0, 1]$. As shown in the figure, adopting SCE can significantly increase the loss differences between poisoned samples and benign ones compared with the CE. }\n\\label{fig:sce}\n\\end{figure*}\n\nAfter the decoupling-based training process (\\ref{eq:w_c})-(\\ref{eq:w_f}), even if the model is (partly) trained on the poisoned dataset, the hidden backdoor cannot be created since the feature extractor is purified. However, this simple strategy suffers from two main problems. Firstly, compared with the one trained via supervised learning, the accuracy of predicting benign samples will have a certain decrease, since the learned feature extractor is frozen in the second stage. %This is also the problem encountered by current self-supervised methods. \nSecondly, poisoned samples will serve as `outliers' to further hinder the learning of the second stage when poison-label attacks appear, since those samples lie close to samples with its ground-truth label instead of the target label in the hidden feature space generated by the learned purified feature extractor. These two problems indicate that \\emph{we should remove poisoned samples and retrain or fine-tune the whole model}.\n\nSpecifically, we select \\emph{high-credible samples} $\\mathcal{D}_h$ based on the loss $\\mathcal{L}_2(\\cdot; [\\bm{w}_c^{*},\\bm{w}_f^{*}])$. The high-credible samples are defined as the $\\alpha\\%$ training samples with the smallest loss, where $\\alpha \\in [0, 100]$ is a hyper-parameter. In particular, we adopt the symmetric cross-entropy (SCE) \\citep{wang2019symmetric} as $\\mathcal{L}_2(\\cdot)$, inspired by the label-noise learning. As shown in Figure \\ref{fig:sce}, compared with the CE loss, the SCE can significantly increase the differences between poisoned samples and benign ones, which further reduces the possibility that high-credible dataset $\\mathcal{D}_h$ still contains poisoned samples.   \n\nNote that \\emph{we do not intend to accurately separate poisoned samples and benign samples}. We only want to ensure that the obtained $\\mathcal{D}_h$ contains as few poisoned samples as possible.\n\n\\subsection{Semi-supervised Fine-tuning}\n\\label{sec:semi}\nAfter the second stage, the third-party training set $\\mathcal{D}_t$ will be separated into two disjoint parts, including the high-credible dataset $\\mathcal{D}_h$ and the low-credible dataset $\\mathcal{D}_l \\triangleq \\mathcal{D}_t \\backslash \\mathcal{D}_h$. Let $\\hat{\\mathcal{D}}_l \\triangleq \\{\\bm{x}|(\\bm{x}, y) \\in \\mathcal{D}_l\\}$ indicates the unlabeled version of low-credible dataset $\\mathcal{D}_l$. We fine-tune the whole trained model $f_{[\\bm{w}_c^{*},\\bm{w}_f^{*}]}(\\cdot)$ with semi-supervised learning as follows:\n\\begin{equation}\n    \\min_{\\bm{w}} \\mathcal{L}_3 (\\mathcal{D}_h,\\hat{\\mathcal{D}}_l;\\bm{w} ),\n\\end{equation}\nwhere %$\\bm{w}$ is initialized by the $[\\bm{w}_c^{*},\\bm{w}_f^{*}]$ and \n$\\mathcal{L}_3(\\cdot)$ denotes the semi-supervised loss ($e.g.$, the loss in MixMatch \\citep{berthelot2019mixmatch}). \n\nThis process can prevent the side-effects of poisoned samples while utilizing their contained useful information, and encourage the compatibility between the feature extractor and the simple classifier via learning them jointly instead of separately. Please refer to Section \\ref{sec:ablation_study} for more results. \n\n\\section{Experiments}\n\n\\subsection{Experimental Settings}\n\\noindent \\textbf{Datasets and DNNs.}\nWe evaluate all defenses on two classical benchmark datasets, including CIFAR-10 \\citep{krizhevsky2009learning} and (a subset of) ImageNet \\citep{deng2009imagenet}. We adopt the ResNet-18 \\citep{he2016deep} for these tasks. More detailed settings are presented in Appendix \\ref{sec:DD_set}. Besides, we also provide the results on (a subset of) VGGFace2 \\citep{cao2018vggface2} in Appendix \\ref{sec:results_vggface2}.\n\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=0.98\\textwidth]{poisoned_samples.pdf}\n    \\vspace{-0.7em}\n    \\caption{The illustration of poisoned samples generated by different attacks. }\n    \\label{fig:poisoned_samples}\n\\end{figure*}\n\n\\noindent \\textbf{Attack Baselines.} \nWe examine all defense approaches in defending against four representative attacks. Specifically, we select the BadNets \\citep{gu2017badnets}, the backdoor attack with blended strategy (dubbed `Blended') \\citep{chen2017targeted}, WaNet \\citep{nguyen2021wanet}, and label-consistent attack with adversarial perturbations (dubbed `Label-Consistent') \\citep{turner2019label} for the evaluation. They are the representative of patch-based visible and invisible poison-label attacks, non-patch-based poison-label attacks, and clean-label attacks, respectively. \n\n\\noindent \\textbf{Defense Baselines.} \nWe compared our DBD with two defenses having the same defender's capacities, including the DPSGD \\citep{du2019robust} and ShrinkPad \\citep{li2021backdoor2}. %Specifically, DPSGD adds Gaussian noise with a certain budget during the training process, and ShrinkPad first shrinks the image and then padding to its original size before feeding it into the inference process. \nWe also compare with other two approaches with an additional requirement ($i.e.$, having a local benign dataset), including the neural cleanse with unlearning strategy (dubbed `NC') \\citep{wang2019neural}, and neural attention distillation (dubbed `NAD') \\citep{li2021neural}. %Specifically, NC first synthesizes the potential trigger pattern based on which to conduct the unlearning process, while NAD removes hidden backdoors through the model distillation. \nThey are the representative of poison suppression based defenses, preprocessing based defenses, trigger synthesis based defenses, and model reconstruction based defenses, respectively. We also provide results of DNNs trained without any defense (dubbed `No Defense') as another important baseline for reference.\n\n\\noindent \\textbf{Attack Setups.} \nWe use a $2\\times 2$ square as the trigger pattern on CIFAR-10 dataset and the $32 \\times 32$ Apple logo on ImageNet dataset for the BadNets, as suggested in \\citep{gu2017badnets,wang2019neural}. For Blended, we adopt the `Hello Kitty' pattern on CIFAR-10 and the random noise pattern on ImageNet, based on the suggestions in \\citep{chen2017targeted}, and set the blended ratio $\\lambda = 0.1$ on all datasets. The trigger pattern adopted in label-consistent attack is the same as the one used in BadNets. For WaNet, we adopt its default settings on CIFAR-10 dataset. However, on ImageNet dataset, we use different settings optimized by grid-search since the original ones fail. An example of poisoned samples generated by different attacks is shown in Figure \\ref{fig:poisoned_samples}. Besides, we set the poisoning rate $\\gamma_1=2.5\\%$ for label-consistent attack (25\\% of training samples with the target label) and $\\gamma_2= 5\\%$ for three other attacks. More details are shown in Appendix \\ref{sec:attack_set}.\n\n\\noindent \\textbf{Defense Setups.} \nFor our DBD, we adopt SimCLR \\citep{chen2020simple} as the self-supervised method and MixMatch \\citep{berthelot2019mixmatch} as the semi-supervised method. More details about SimCLR and MixMatch are in Appendix \\ref{sec:more_loss}. The filtering rate $\\alpha$ is the only key hyper-parameter in DBD, which is set to 50\\% in all cases. We set the shrinking rate to 10\\% for the ShrinkPad on all datasets, as suggested in \\citep{li2021backdoor2,zeng2021deepsweep}. In particular, DPSGD and NAD are sensitive to their hyper-parameters. We \\emph{report their best results in each case} based on the grid-search (as shown in Appendix \\ref{sec:search_best}). Besides, we split a 5\\% random subset of the benign training set as the local benign dataset for NC and NAD. More implementation details are provided in Appendix \\ref{sec:defense_set}.\n\n\\noindent \\textbf{Evaluation Metrics.} \nWe adopt the \\emph{attack success rate} (ASR) and \\emph{benign accuracy} (BA) to measure the effectiveness of all methods\\footnote{Among all defense methods, the one with the best performance is indicated in boldface and the value with underline denotes the second-best result.}. Specifically, let $\\mathcal{D}_{test}$ indicates the (benign) testing set and $C_{\\bm{w}}: \\mathcal{X} \\rightarrow \\mathcal{Y}$ denotes the trained classifier, we have $ASR \\triangleq \\Pr_{(\\bm{x},y) \\in \\mathcal{D}_{test}}\\{C_{\\bm{w}}(G(\\bm{x}))=y_t|y \\neq y_t\\}$ and $BA \\triangleq \\Pr_{(\\bm{x},y) \\in \\mathcal{D}_{test}}\\{C_{\\bm{w}}(\\bm{x})=y\\}$, where $y_t$ is the target label and $G(\\cdot)$ is the poisoned image generator. In particular, \\emph{the lower the ASR and the higher the BA, the better the defense}. \n\n\\subsection{Main Results}\n\\label{sec:main}\n\n\\noindent \\textbf{Comparing DBD with Defenses having the Same Requirements.}\nAs shown in Table \\ref{tab:poison}-\\ref{tab:clean}, DBD is significantly better than defenses having the same requirements ($i.e.$, DPSGD and ShrinkPad) in defending against all attacks. For example, the benign accuracy of DBD is 20\\% over while the attack success rate is 5\\% less than that of DPSGD in all cases. Specifically, the attack success rate of models with DBD is less than 2\\% in all cases (mostly $< 0.5\\%$), which verifies that our method can successfully prevent the creation of hidden backdoors. Moreover, the decreases of benign accuracy are less than $2\\%$ when defending against poison-label attacks, compared with models trained without any defense. Our method is even better on relatively larger dataset where all baseline methods become less effective. These results verify the effectiveness of our method.\n\n\\begin{table*}[ht]\n\\centering\n\\vspace{-2.8em}\n\\caption{The effectiveness (\\%) of defending against three attacks. Note that NC and NAD need an additional local benign dataset, which is not required in DPSGD, ShrinkPad, and our method.}\n\\scalebox{0.83}{\n\\begin{tabular}{c|cccccc|cccccc}\n\\toprule\nDataset $\\rightarrow$ & \\multicolumn{6}{c|}{CIFAR-10} & \\multicolumn{6}{c}{ImageNet} \\\\ \\hline\nAttack $\\rightarrow$ & \\multicolumn{2}{c}{BadNets} & \\multicolumn{2}{c}{Blended} &\n\\multicolumn{2}{c|}{WaNet} & \\multicolumn{2}{c}{BadNets} &  \\multicolumn{2}{c}{Blended} & \\multicolumn{2}{c}{WaNet} \\\\ \\hline\nDefense $\\downarrow$ & BA & ASR & BA & ASR & BA & ASR & BA & ASR & BA & ASR & BA & ASR \\\\ \\hline\nNo Defense & 94.92 & 100 & 94.14 & 98.25 & 94.29 & 98.64 & 80.23 & 90.49 & 82.07 & 95.57 & 80.98 & 96.22 \\\\ \\hline\nNC & \\textbf{94.25} & 2.33 & \\textbf{93.01} & 5.70 & \\textbf{92.44} & 98.52 & \\underline{80.67} & 32.22 & \\textbf{82.07} & 27.33 & \\underline{80.32} & 99.32 \\\\\nNAD & 90.47 & \\underline{1.20} & 89.72 & \\textbf{1.51} & \\underline{91.83} & \\underline{9.49} & 73.49 & \\underline{5.81} & 70.88 & \\underline{8.15} & 75.27 & 31.43 \\\\ \\hline\nDPSGD & 44.24 & 14.56 & 53.05 & 20.68 & 52.13 & 39.78 & 51.43 & 25.40 & 55.12 & 84.17 & 20.72 & 54.12  \\\\\nShrinkPad & 90.32 & 10.34 & 79.67 & 55.71 & 74.76 & 85.59 & 66.08 & 45.61 & 64.47 & 4.20 & 63.74 & \\underline{3.58} \\\\\nDBD (ours) & \\underline{92.41} & \\textbf{0.96} & \\underline{92.18} & \\underline{1.73} & 91.20 & \\textbf{0.39} & \\textbf{80.99} & \\textbf{0.26} & \\underline{81.63} & \\textbf{0.16} & \\textbf{82.05} & \\textbf{0.33} \\\\ \\bottomrule\n\\end{tabular}\n}\n\\vspace{-1.5em}\n\\label{tab:poison}\n\\end{table*}\n\n\\begin{table*}[ht]\n\\centering\n\\caption{The effectiveness (\\%) of defending against label-consistent attack on CIFAR-10 dataset. %Note that NC and NAD need to have an additional local benign dataset, which is not required in DPSGD, ShrinkPad, and our method.\n}\n\\scalebox{0.9}{\n        \\begin{tabular}{c|c|c|cc|ccc}\n            \\toprule\n            Attack$\\downarrow$, Defense$\\rightarrow$                               & Metric$\\downarrow$ & Benign & NC               & NAD   & DPSGD & ShrinkPad & DBD (ours)        \\\\ \\hline\n            \\multirow{2}{*}{\\tabincell{c}{Label-Consistent\\\\($\\epsilon=16$)}} & BA                                         & 94.90  & \\textbf{94.94}   & 93.04 & 44.28 & \\underline{90.67}     & 89.67             \\\\\n                                                              & ASR                                        & 99.33  & \\underline{1.31} & 3.95  & 7.97  & 9.60      & \\textbf{0.01}     \\\\ \\hline\n            \\multirow{2}{*}{\\tabincell{c}{Label-Consistent\\\\($\\epsilon=32$)}} & BA                                         & 94.82  & \\textbf{94.57}   & 90.98 & 69.79 & 90.83     & \\underline{91.45} \\\\\n                                                              & ASR                                        & 99.19  & \\underline{1.36} & 6.54  & 9.06  & 13.04     & \\textbf{0.34}     \\\\ \\bottomrule\n        \\end{tabular}\n    }\n\\label{tab:clean}\n\\vspace{-0.9em}\n\\end{table*}\n\n\\noindent \\textbf{Comparing DBD with Defenses having Extra Requirements.}\nWe also compare our defense with two other methods ($i.e.$, NC and NAD), which have an additional requirement that defenders have a benign local dataset. As shown in Table \\ref{tab:poison}-\\ref{tab:clean}, NC and NAD are better than DPSGD and ShrinkPad, as we expected, since they adopt additional information from the benign local dataset. In particular, although NAD and NC use additional information, our method is still better than them, even when their performances are tuned to the best while our method only uses the default settings. Specifically, the BA of NC is on par with that of our method. However, it is with the sacrifice of ASR. Especially on ImageNet dataset, NC has limited effects in reducing ASR. In contrast, our method reaches the smallest ASR while its BA is either the highest or the second-highest in almost all cases. These results verify the effectiveness of our method again.\n\n\\begin{table*}[!t]\n   \\centering\n   \\vspace{-0.5em}\n   \\caption{The ablation study of our proposed method.}\n   \\scalebox{0.95}{\n      \\begin{tabular}{c|cccc|cc|cc}\n         \\toprule\n         Attack $\\rightarrow$                       & \\multicolumn{2}{c}{BadNets} & \\multicolumn{2}{c|}{Blended} & \\multicolumn{2}{c|}{Label-Consistent} & \\multicolumn{2}{c}{WaNet}\\\\ \\hline\n         Defense $\\downarrow$, Metric $\\rightarrow$ & BA                          & ASR                          & BA                                    & ASR                                  & BA                                    & ASR   & BA    & ASR\\\\ \\hline\n         No Defense                                 & 94.92                       & 100                          & 94.14                                 & 98.25                                & 94.82                                 & 99.19 & 94.29 & 98.64\\\\ \\hline\n         DBD without SS   & \\textbf{93.66}                       & 100                          & \\textbf{93.47}                                 & 99.93                                & 90.70                                 & 98.50  & 81.91 & 98.40\\\\ \\hline\n         SS with CE                                 & 82.25                       & 5.20                         & 81.85                                 & 12.19                                & 82.08                                 & 5.87  & 80.29 & 9.48\\\\\n         SS with SCE                                & 82.34                       & 5.12                         & 82.30                                 & 6.24                                 & 81.81                                 & 5.43  & 81.15 & 7.08\\\\ \\hline\n         SS with SCE + Tuning                       & 78.94                       & \\underline{4.02}                         & 78.78                                 & \\underline{3.70}                                 & 77.44                                 & \\underline{1.44}  & 78.51 & \\underline{5.87}\\\\ \\hline\n         DBD (ours)                                 & \\underline{92.41}                       & \\textbf{0.96}                         & \\underline{92.18}                                 & \\textbf{1.73}                                 & \\textbf{91.45}                                 & \\textbf{0.34}  & \\textbf{91.20} & \\textbf{0.39}\n         \\\\ \\bottomrule\n      \\end{tabular}\n   }\n   \\begin{tablenotes}\n      \\footnotesize\n      \\item[1] $^{1}$DBD without SS: use standard supervised training to obtain the feature extractor adopted in DBD.\n      \\item[2] $^{2}$SS with CE: freeze the learned backbone and train remaining FC layers on all samples with CE loss.\n      \\item[3] $^{3}$SS with SCE: freeze the learned backbone and train remaining FC layers on all samples with SCE loss.\n      \\item[4] $^{4}$SS with SCE + Tuning: fine-tune FC layers of the model in `SS with SCE' on high-credible samples.\n   \\end{tablenotes}\n   \\label{tab:ablation}\n   \\vspace{-1em}\n\\end{table*}\n\n\\noindent \\textbf{Results. }\nAs shown in Figure \\ref{fig:gamma}, our method can still prevent the creation of hidden backdoors even when the poisoning rate reaches 20\\%. Besides, DBD also maintains high benign accuracy. In other words, our method is effective in defending attacks with different strengths. %Those results verify the effectiveness of our method again. \n\n\\subsection{Ablation Study}\n\\label{sec:ablation_study}\n\nThere are four key strategies in DBD, including \\textbf{(1)} obtaining purified feature extractor, \\textbf{(2)} using SCE instead of CE in the second stage, \\textbf{(3)} reducing side-effects of low-credible samples, and \\textbf{(4)} fine-tuning the whole model via semi-supervised learning. Here we verify their effectiveness.\n\n\\noindent \\textbf{Settings. }\nWe compare the proposed DBD with its four variants, including \\textbf{(1)} DBD without SS, \\textbf{(2)} SS with CE, \\textbf{(3)} SS with SCE, and \\textbf{(4)} SS with SCE + Tuning, on the CIFAR-10 dataset. Specifically, in the first variant, we replace the backbone generated by self-supervised learning with the one trained in a supervised fashion and keep other parts unchanged. In the second variant, we freeze the backbone learned via self-supervised learning and train the remaining fully-connected layers with cross-entropy loss on all training samples. The third variant is similar to the second one. The only difference is that it uses symmetric cross-entropy instead of cross-entropy to train fully-connected layers. The last variant is an advanced version of the third one, which further fine-tunes fully-connected layers on high-credible samples filtered by the third variant. %Unless otherwise specified, all settings used in this part are the same as those used in Section \\ref{sec:main}.\n\n\\noindent \\textbf{Results. }\nAs shown in Table \\ref{tab:ablation}, we can conclude that decoupling the original end-to-end supervised training process is effective in preventing the creation of hidden backdoors, by comparing our DBD with its first variant and the model trained without any defense. Besides, we can also verify the effectiveness of SCE loss on defending against poison-label backdoor attacks by comparing the second and third DBD variants. Moreover, the fourth DBD variant has relatively lower ASR and BA, compared with the third one. This phenomenon is due to the removal of low-credible samples. It indicates that reducing side-effects of low-credible samples while adopting their useful information is important for the defense. We can also verify that fine-tuning the whole model via semi-supervised learning is also useful by comparing the fourth variant and the proposed DBD.\n\n\\subsection{Resistance to Potential Adaptive Attacks}\n\\label{sec:adaptive}\n\nIn our paper, we adopted the classical defense setting that attackers have no information about the defense. Attackers may design adaptive attacks if they know the existence of our DBD. The most straightforward idea is to \\emph{manipulate the self-supervised training process} so that poisoned samples are still in a new cluster after the self-supervised learning. However, \\emph{attackers are not allowed to do it} based on our threat model about adopting third-party datasets. Despite this, attackers may design adaptive attacks by optimizing the trigger pattern to make poisoned samples still in a new cluster after the self-supervised learning if they can know the model structure used by defenders, as follows:\n\n\\noindent \\textbf{Problem Formulation.} For a $K$-classification problem, let $\\mathcal{X}'=\\{\\bm{x}_i\\}_{i=1}^M$ indicates the benign images selected for poisoning,  $\\mathcal{X}_{j}=\\{\\bm{x}_i\\}_{i=1}^{N_j}$ denotes the benign images with ground-truth label $j$, and $g$ is a trained backbone. Given an attacker-predefined poisoned image generator $G$, the \\emph{adaptive attack} aims to optimize a trigger pattern $\\bm{t}$ by minimizing the distance between poisoned images while maximizing the distance between the center of poisoned images and centers of clusters of benign images with different label, $i.e.,$\n\\begin{equation}\n\\min_{\\bm{t}} \\frac{1}{M}\\sum_{\\bm{x} \\in \\mathcal{X}'} d\\left(g(G(\\bm{x};\\bm{t})), \\overline{g}')\\right) - \\frac{1}{K}\\sum_{i=1}^K d\\left(\\overline{g}', \\overline{g_{i}}\\right),     \n\\end{equation}\n    \nwhere $\\overline{g}' \\triangleq \\frac{1}{M}\\sum_{\\bm{x} \\in \\mathcal{X}'} g(G(\\bm{x};\\bm{t}))$, $\\overline{g_i} \\triangleq \\frac{1}{N_i} \\sum_{\\bm{x} \\in \\mathcal{X}_i} g(\\bm{x})$, and $d$ is a distance metric. \n\n\\noindent \\textbf{Settings.}\nWe adopt the CIFAR-10 dataset and use the $\\ell^2$ norm as the distance metric to conduct the experiment. Specifically, we assume that attackers have the entire benign dataset, based on which they can train a backbone adopted in the first stage of our DBD. We use the Adam optimizer to solve the above optimization problem for 100 epochs with a learning rate of 0.1. The trigger size is set to $32\\times32$, which means the attacker can completely modify the content of poisoned samples, regardless of its original semantic information and the stealthiness of the attack. This setting is to ensure the attack ability, since clustering poisoned samples together is very difficult in self-supervised learning.\n\n\\noindent \\textbf{Results.}\nThe adaptive attack works well when there is no defense (BA=94.96\\%, ASR=99.70\\%). However, this attack still fails to attack our DBD (BA=93.21\\%, ASR=1.02\\%). In other words, our defense is resistant to this adaptive attack. It is most probably because the trigger optimized based on the backbone is far less effective when the model is retrained since model parameters are changed due to the random initialization and the update of model weights during the training process.\n\n\\comment{\n\\subsection{A Brief Description of Additional Experiments in the Appendix}\nExcept for above experiments, we also provide additional results of defending against attacks with different hyper-parameters in Appendix \\ref{sec:smaller_p}-\\ref{sec:diff_trigger} and the resistance to adaptive attacks in Appendix \\ref{sec:adaptive}. %More details about training facilities and codes are also included in Appendix \\ref{sec:train_f}-\\ref{sec:codes}.\n} \n\n\\vspace{-0.3em}\n\\section{Conclusion}\n\\vspace{-0.2em}\n\nThe mechanism of poisoning-based backdoor attacks is to establish a latent connection between trigger patterns and the target label during the training process. In this paper, we revealed that this connection is learned mostly due to the end-to-end supervised training paradigm. Motivated by this understanding, we proposed a decoupling-based backdoor defense, which first learns the backbone via self-supervised learning and then the remaining fully-connected layers by the classical supervised learning. We also introduced the label-noise learning method to determine high-credible and low-credible samples, based on which we fine-tuned the whole model via semi-supervised learning. Extensive experiments verify that our defense is effective on reducing backdoor threats while preserving high accuracy on predicting benign samples. %Our paper may provide a new angle about understanding the learning dynamic of poisoning-based backdoor attacks.\n\n\\newpage\n\n\\section*{Acknowledgments}\nBaoyuan Wu is supported in part by the National Natural Science Foundation of China under Grant 62076213, the University Development Fund of the Chinese University of Hong Kong, Shenzhen under Grant 01001810, and the Special Project Fund of Shenzhen Research Institute of Big Data under Grant T00120210003. \nZhan Qin is supported in part by the National Natural Science Foundation of China under Grant U20A20178, the National Key Research and Development Program of China under Grant 2020AAA0107705, and the Research Laboratory for Data Security and Privacy, Zhejiang University-Ant Financial Fintech Center.\nKui Ren is supported by the National Key Research and Development Program of China under Grant 2020AAA0107705.\n\n\\section*{Ethics Statement}\nDNNs are widely adopted in many mission-critical areas ($e.g.$, face recognition) and therefore their security is of great significance. The vulnerability of DNNs to backdoor attacks raises serious concerns about using third-party training resources. In this paper, we propose a general training pipeline to obtain backdoor-free DNNs, even if the training dataset contains poisoned samples. This work has no ethical issues in general since our method is purely defensive and does not reveal any new vulnerabilities of DNNs. However, we need to mention that our defense can be adopted only when training with untrusted samples, and backdoor attacks could happen in other scenarios. People should not be too optimistic about eliminating backdoor threats. \n\n\\section*{Reproducibility Statement}\nThe detailed descriptions of datasets, models, and training settings are in Appendix \\ref{sec:motivation_set}-\\ref{sec:search_best}. We also describe the computational facilities and cost in Appendix \\ref{sec:train_f}-\\ref{sec: computational cost}. Codes of our DBD are also open-sourced.\n\n\\newpage\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{Backdoor Defense via Deconfounded Representation Learning}\n\n\\begin{document}\n\n\\title{Backdoor Defense via Deconfounded Representation Learning}\n\n\\author{%\n\tZaixi Zhang$^{1,2}$, Qi Liu$^{1,2}$\\thanks{Qi Liu is the corresponding author.}, Zhicai Wang$^4$, Zepu Lu$^{4}$, Qingyong Hu$^{3}$\\\\\n\t1: Anhui Province Key Lab of Big Data Analysis and Application,\\\\ School of Computer Science and Technology, University of Science and Technology of China\\\\2:State Key Laboratory of Cognitive Intelligence, Hefei, Anhui, China\\\\3:Hong Kong University of Science and Technology 4: University of Science and Technology of China\\\\\n\tzaixi@mail.ustc.edu.cn, qiliuql@ustc.edu.cn\\\\ \\{wangzhic, zplu\\}@mail.ustc.edu.cn, qhuag@cse.ust.hk\n}\n\\maketitle\n\n\\begin{abstract}\nDeep neural networks (DNNs) are recently shown to be vulnerable to backdoor attacks, where attackers embed hidden backdoors in the DNN model by injecting a few poisoned examples into the training dataset. While extensive efforts have been made to detect and remove backdoors from backdoored DNNs, it is still not clear whether a backdoor-free clean model can be directly obtained from poisoned datasets. In this paper, we first construct a causal graph to model the generation process of poisoned data and find that the backdoor attack acts as the confounder, which brings spurious associations between the input images and target labels, making the model predictions less reliable. Inspired by the causal understanding, we propose the Causality-inspired Backdoor Defense (CBD), to learn deconfounded representations for reliable classification. Specifically, a backdoored model is intentionally trained to capture the confounding effects. The other clean model dedicates to capturing the desired causal effects by minimizing the mutual information with the confounding representations from the backdoored model and employing a sample-wise re-weighting scheme. Extensive experiments on multiple benchmark datasets against 6 state-of-the-art attacks verify that our proposed defense method is effective in reducing backdoor threats while maintaining high accuracy in predicting benign samples. Further analysis shows that CBD can also resist potential adaptive attacks. The code is available at \\url{https://github.com/zaixizhang/CBD}. \n\\end{abstract}\n\n\\section{Introduction}\nRecent studies have revealed that deep neural networks (DNNs) are vulnerable to backdoor attacks \\cite{gu2017badnets, liu2018trojaning, saha2020hidden}, where attackers inject stealthy backdoors into DNNs by poisoning a few training data. \nSpecifically, backdoor attackers attach the backdoor trigger (\\emph{i.e.,} a particular pattern) to some benign\ntraining data and change their labels to the attacker-designated target label. The correlations between the trigger pattern and target label will be learned by DNNs during training.\nIn the inference process, the backdoored model behaves normally on benign data while its prediction will be maliciously altered when the backdoor is activated. The risk of backdoor attacks hinders the applications of DNNs to some safety-critical areas such as automatic driving \\cite{liu2018trojaning} and healthcare systems \\cite{feng2021fiba}.\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.9\\linewidth]{figs/SCM.pdf}\n    \\caption{(a) A real example of the backdoor attack. The backdoored DNN classifies the ``turtle'' image with a trigger pattern as the target label ``dog''. (b) The causal graph represents the causalities among variables: $X$ as the input image, $Y$ as the label, and $B$ as the backdoor attack. Besides the causal effect of $X$ on $Y$ ($X \\rightarrow Y$), the backdoor attack can attach trigger patterns to images ($B \\rightarrow X$), and change the labels to the targeted label ($B \\rightarrow Y$). Therefore, as a \\emph{confounder}, the backdoor attack $B$ opens a spurious path between $X$ and $Y$ ($X \\leftarrow B \\rightarrow Y$).}\n    \\label{scm}\n    \\vspace{-1em}\n\\end{figure}\n\nOn the contrary, human cognitive systems are known to be immune to input perturbations such as stealthy trigger patterns induced by backdoor attacks \\cite{gopnik2004theory}.\nThis is because humans are more sensitive to causal relations than the statistical associations of nuisance factors  \\cite{ lake2017building, tenenbaum2011grow}. In contrast, deep learning models that are trained to fit the poisoned datasets can hardly distinguish the causal relations and the statistical associations brought by backdoor attacks. Based on causal reasoning, we can identify causal relation \\cite{causality, causalinference} and build robust deep learning models \\cite{zhang2020causal, zhang2022adversarial}. Therefore, it is essential to leverage causal reasoning to analyze and mitigate the threats of backdoor attacks.\n\nIn this paper, we focus on the image classification tasks and aim to train backdoor-free models on poisoned datasets without extra clean data. We first construct a causal graph to model the generation process of backdoor data where nuisance factors (\\emph{i.e.,} backdoor trigger patterns) are considered. With the assistance of the causal graph, we find that the backdoor attack acts as the \\emph{confounder} and opens a spurious path between the input image and the predicted label (Figure \\ref{scm}). If DNNs have learned the correlation of such a spurious path, their predictions will be changed to the target labels when the trigger is attached.\n\nMotivated by our causal insight, we propose {\\bf C}ausality-inspired {\\bf B}ackdoor {\\bf D}efense (CBD) to learn deconfounded representations for classification. \nAs the backdoor attack is stealthy and hardly measurable, we cannot directly block the backdoor path by the backdoor adjustment from causal inference \\cite{causalinference}. Inspired by recent advances in disentangled representation learning \\cite{wang2020cross, hamaguchi2019rare, liu2021mitigating}, we instead aim to learn deconfounded representations that only preserve the causality-related information. Specifically in CBD, two DNNs are trained, which focus on the spurious correlations and the causal effects respectively. The first DNN is designed to intentionally capture the backdoor correlations with an early stop strategy. The other clean model is then trained to be independent of the first model in the hidden space by minimizing mutual information. The information bottleneck strategy and sample-wise re-weighting scheme are also employed to help the clean model capture the causal effects while relinquishing the confounding factors.\nAfter training, only the clean model is used for downstream classification tasks.\nIn summary, our contributions are:\n\\begin{itemize}\n    \\item From a causal perspective, we find the backdoor attack acts as the confounder that causes spurious correlations between the input images and the target label.\n    \\item With the causal insight, we propose a Causality-inspired Backdoor Defense (CBD), which learns deconfounded representations to mitigate the threat of poisoning-based backdoor attacks.\n    \\item Extensive experiments with 6 representative backdoor attacks are conducted. The models trained using CBD are of almost the same clean accuracy as they were directly trained on clean data and the average backdoor attack success rates are reduced to around 1$\\%$, which verifies the effectiveness of CBD.\n    \\item We explore one potential adaptive attack against CBD, which tries to make the backdoor attack stealthier by adversarial training. Experiments show that CBD is robust and resistant to such an adaptive attack.\n\\end{itemize}\n\n\\section{Related Work}\n\\subsection{Backdoor Attacks}\nBackdoor attacks are emerging security threats to deep neural network classifiers. In this paper, we focus on the poisoning-based backdoor attacks, where the attacker can only inject poisoned examples into the training set while cannot modify other training components (\\emph{e.g.,} training loss). Note that backdoor attacks could occur in other tasks (\\emph{e.g.,} visual object tracking~\\cite{li2022few}, graph classification~\\cite{zhang2021backdoor}, federated learning~\\cite{zhang2022fldetector}, and multi-modal contrastive learning~\\cite{backdoorcontrast}). The attacker may also have extra capabilities such as modifying the training process~\\cite{latentbackdoor}. However, these situations are out of the scope of this paper.\n\nBased on the property of target labels, existing backdoor attacks can be divided into two main categories: \\emph{dirty-label attacks}~ \\cite{gu2017badnets,chen2017targeted,liu2020reflection, zhang2021backdoor} and \\emph{clean-label attacks}~\\cite{shafahi2018poison,turner2019clean,zhu2019transferable,zhao2020clean,saha2020hidden}. Dirty-label attack is the most common backdoor attack paradigm, where the poisoned samples are generated by stamping the backdoor trigger onto the original images and altering the labels to the target label. BadNets~\\cite{gu2017badnets} firstly employed a black-white checkerboard as the trigger pattern. Furthermore, more complex and stealthier trigger patterns are proposed such as blending backgrounds~\\cite{chen2017targeted}, natural reflections~\\cite{liu2020reflection}, invisible noise~\\cite{liao2018backdoor,li2019invisible,chen2019invisible}  and sample-wise dynamic patterns \\cite{nguyen2020input,li2021invisible, nguyen2021wanet}.\nOn the other hand, Clean-label backdoor attacks are arguably stealthier as they do not change the labels. For example, Turner \\emph{et al.} \\cite{turner2019clean} leveraged deep generative models to modify benign images from the target class.\n\n\\subsection{Backdoor Defenses}\nBased on the target and the working stage, existing defenses can be divided into the following categories: (1) \\emph{Detection based defenses} aim to detect anomalies in input data \\cite{chen2018detecting,tran2018spectral,gao2019strip,xu2019detecting,hayase2021spectre,tang2021demon, Februus20} or whether a given model is backdoored \\cite{wang2019neural,chen2019deepinspect,kolouri2020universal,shen2021backdoor}. For instance, Du \\emph{et al.}~\\cite{DPdetection} applies differential privacy to improve the utility of backdoor detection.\n(2) \\emph{Model reconstruction based defenses} aim to remove backdoors from a given poisoned model. For example, Mode Connectivity Repair (MCR)~\\cite{zhao2020bridging} mitigates the backdoors by selecting a robust model in the path of loss landscape, while Neural Attention Distillation (NAD)~\\cite{li2021neural} leverages attention distillation to remove triggers. (3) \\emph{Poison suppression based defenses} \\cite{li2021anti,backdoordecouple} reduce the effectiveness of poisoned examples at the training stage and try to learn a clean model from a poisoned dataset. For instance, Decoupling-based backdoor defense (DBD) \\cite{backdoordecouple} decouples the training of DNN backbone and fully-connected layers to reduce the correlation between triggers and target labels. \nAnti-Backdoor Learning (ABL)~\\cite{li2021anti} uses gradient ascent to unlearn the backdoored model with the isolated backdoor data. \n\nIn this paper, our proposed CBD is most related to the \\emph{Poison suppression based defenses}. Our goal is to train clean models directly on poisoned datasets without access to clean datasets or further altering the trained model. \nDifferent with ABL~\\cite{li2021anti}, CBD directly trains clean models on poisoned datasets without further finetuning the trained model (unlearning backdoor). In contrast with DBD, CBD does not require additional self-supervised pretraining stages and is much more efficient. In Sec. \\ref{sec: experiments}, extensive experimental results clearly show the advantages of CBD. \n\n\\subsection{Causal Inference}\nCausal inference has a long history in statistical research \\cite{causality,bookwhy, causalinference}. The objective of causal inference is to analyze the causal effects among variables and mitigate spurious correlations. Recently, causal inference has also shown promising results in various areas of machine learning \\cite{niu2021counterfactual,mitrovic2020representation, yue2020interventional,tang2020long,zhang2020causal,zhang2022adversarial, huang2022deconfounded}. \nHowever, to date, causal inference has not been incorporated into the analysis and defense of backdoor attacks.\n\\section{Preliminaries}\n\\subsection{Problem Formulation}\nIn this section, we first formulate the problem of poison suppression based defense, then provide a causal view on backdoor attacks and introduce our proposed Causality-inspired Backdoor Defense. Here, we focus on image classification tasks with deep neural networks.\n\n\\noindent\\textbf{Threat Model. }We follow the attack settings in previous works \\cite{li2021anti, backdoordecouple}. Specifically, we assume a set of backdoor examples has been pre-generated by the attacker and has been successfully injected into the training dataset. We also assume the defender has complete control over the training process but is ignorant of the distribution or the proportion of the backdoor examples in a given dataset. The defender’s goal is to train a backdoor-free model on the poisoned dataset, which is as good as models trained on purely clean data. Robust learning strategies developed under such a threat model could benefit research institutes, companies, or government agencies that have the computational resources to train their models but rely on outsourced training data.\n\n\\subsection{A Causal View on Backdoor Attacks}\nHumans' ability to perform causal reasoning is arguably one of the most important characteristics that distinguish human learning from deep learning \\cite{zhang2020causal, scholkopf2021toward}. The superiority of causal reasoning endows humans with the ability to recognize causal relationships while ignoring non-essential factors in tasks. On the contrary, DNNs usually fail to distinguish causal relations and statistical associations and tend to learn ``easier\" correlations than the desired knowledge \\cite{nam2020learning, geirhos2020shortcut}. Such a shortcut solution could lead to overfitting to nuisance factors (\\emph{e.g.,} trigger patterns), which would further result in the vulnerability to backdoor attacks. Therefore, here we leverage causal inference to analyze DNN model training and mitigate the risks of backdoor injection. \n\nWe first construct a causal graph as causal graphs are the keys to formalize causal inference. One\napproach is to use causal structure learning to infer causal graphs \\cite{causality},\nbut it is challenging to apply this kind of approach to high-dimensional data like images. Here, following previous works \\cite{zhang2020causal, zhang2022adversarial, niu2021counterfactual}, we leverage domain knowledge (Figure \\ref{scm} (a)) to construct a causal graph $\\mathcal{G}$ (Figure \\ref{scm} (b)) to model the generation process of poisoned data.\n\nIn the causal graph, we denote the abstract data variables by the nodes ($X$ as the input image, $Y$ as the label, and $B$ as the backdoor attack), and the directed links represent their relationships. As shown in Figure \\ref{scm}(b), besides the causal effect of $X$ on $Y$ ($X \\rightarrow Y$), the backdoor attacker can attach trigger patterns to images ($B \\rightarrow X$) and change the labels to the targeted label ($B \\rightarrow Y$). Therefore, as a \\emph{confounder} between $X$ and $Y$, backdoor attack $B$ opens the spurious path $X \\leftarrow B \\rightarrow Y$ (let $B=1$ denotes the images are poisoned and $B=0$ denotes the images are clean). By \\emph{“spurious”}, we mean that the path lies outside the direct causal path from $X$ to $Y$, making $X$ and $Y$ spuriously correlated and yielding an erroneous effect when the trigger is activated. DNNs can hardly distinguish between the spurious correlations and causative relations \\cite{bookwhy}. Hence, directly training DNNs on potentially poisoned dataset incurs the risk of being backdoored. \n\nTo pursue the causal effect of $X$ on $Y$, previous works usually perform the backdoor adjustment in the causal intervention \\cite{bookwhy} with $do$-calculus: $P(Y |do(X)) = \\sum_{B\\in \\{0,1\\}} P(Y|X,B)P(B)$. \nHowever, since the confounder variable $B$ is hardly detectable and measurable in our setting, we can not simply use the backdoor adjustment to block the backdoor path. \nInstead, since the goal of most deep learning models is to learn accurate embedding representations for downstream tasks \\cite{alemi2016deep, huang2021disenqnet, wang2020cross, hamaguchi2019rare}, we aim to disentangle the confouding effects and causal effects in the hidden space. The following section illustrates our method.\n\n\\section{Causality-inspired Backdoor Defense}\nMotivated by our causal insight, we propose the Causality-inspired Backdoor Defense (CBD). \nIn practice, it may be difficult to directly identify the confounding and causal factors of $X$ in the data space. We make an assumption that the confounding and causal factors will be reflected in the hidden representations. \nBased on the assumption, we illustrate our main idea in Figure \\ref{scm2}.\nGenerally, two DNNs including $f_B$ and $f_C$ are trained, which focus on the spurious correlations and the causal relations respectively. We take the embedding vectors from the penultimate layers of $f_B$ and $f_C$ as $R$ and $Z$. \n\\emph{Note that $R$ is introduced to avoid confusion with $B$}.\nWithout confusion, we use \\emph{uppercase} letters for variables and \\emph{lowercase} letters for concrete values in this paper. To generate high-quality variable $Z$ that captures the causal relations, we get inspiration from disentangled representation learning \\cite{wang2020cross, hamaguchi2019rare}.\nIn the training phase, $f_B$ is firstly trained on the poisoned dataset to capture spurious correlations of backdoor. The other clean model $f_C$ is then trained to encourage independence in the hidden space \\emph{i.e.,} $Z \\perp R$ with mutual information minimization and sample re-weighting. After training, only $f_C$ is used for downstream classification tasks. In the rest of this section, we provide details on each step of CBD. \n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.65\\linewidth]{figs/scm2.pdf}\n    \\caption{The model framework of CBD that includes an adversaral loss $\\mathcal{L}_{adv}$ for mutual information minimization, a $l_2-$norm regularization on $z$, and a weighted cross entropy loss $\\mathcal{L}_{wce}$ to augment causal effects.}\n    \\label{scm2}\n    \\vspace{-1em}\n\\end{figure}\n\n\\noindent\\textbf{Training a backdoored model $f_B$. }Firstly, $f_B$ is trained on the poisoned dataset with cross entropy loss to capture the spurious correlations of backdoor. Since the poisoned data still contains causal relations, we intentionally strengthen the confounding bias in $f_B$ with an \\emph{early stop strategy}. Specifically, we train $f_B$ only for several epochs (\\emph{e.g.,} 5 epochs) and freeze its parameters in the training of $f_C$. This is because previous works indicate that backdoor associations are easier to learn than causal relations \\cite{li2021anti}. \nExperiments in Appendix \\ref{app:more results} also verify that the losses on backdoor examples reach nearly 0 while $f_B$ has not converged on clean samples after 5 epochs.\n\n\\noindent\\textbf{Training a clean model $f_C$.}\nInspired by previous works \\cite{guo2019learning, huang2021disenqnet}, we formulate the training objective of $f_C$ with information bottleneck and mutual information minimization:\n\\begin{equation}\n    \\mathcal{L}_C={\\rm min}\\underbrace{\\beta I(Z; X)}_{\\circled{1}}- \\underbrace{I(Z; Y)}_{\\circled{2}}+\\underbrace{I(Z; R)}_{\\circled{3}},\n    \\vspace{-1em}\n\\label{lc}\n\\end{equation}\nwhere $I(\\cdot; \\cdot)$ denotes the mutual information. Term $\\circled{1}$ and $\\circled{2}$ constitute the information bottleneck loss \\cite{tishby2000information} that encourages the variable $Z$ to capture the core information for label prediction ($\\circled{2}$) while constraining unrelated information from inputs ($\\circled{1}$). $\\beta$ is a weight hyper-parameter. Term $\\circled{3}$ is a de-confounder penalty term, which describes the dependency degree between the backdoored embedding $R$ and the deconfounded embedding $Z$. It encourages $Z$ to be independent of $R$ by minimizing mutual information so as to focus on causal effects.\nHowever, $\\mathcal{L}_C$ in Equation \\ref{lc} is not directly tractable, especially for the de-confounder penalty term. %Below we show how to relax $\\mathcal{L}_C$ and get a tractable optimization objective. \nIn practice, we relax Equation \\ref{lc} and optimize the upper bound of the term $\\circled{1}\\&\\circled{2}$ and the estimation of the term $\\circled{3}$. The details are shown below.\n\n\\noindent \\textbf{Term $\\circled{1}$.} Based on the definition of mutual information and basics of probability, $I(Z; X)$ can be calculated as:\n\\begin{equation}\n\\begin{aligned}\n    I(Z; X)&=\\sum_x \\sum_z p(z,x) {\\rm log} \\frac{p(z,x)}{p(z)p(x)} \\\\\n    &=\\sum_x \\sum_z p(z|x)p(x) {\\rm log} \\frac{p(z|x)p(x)}{p(z)p(x)}\\\\\n    &=\\sum_x \\sum_z p(z|x)p(x){\\rm log}~ p(z|x)-\\sum_z p(z){\\rm log}~p(z).\n    \\label{I(z;x)}\n\\end{aligned}\n\\end{equation}\nHowever, the marginal probability $p(z) = \\sum_x p(z|x)p(x)$ is usually difficult to calculate in practice. We use variational approximation to address this issue, \\emph{i.e.,} we use a variational distribution $q(z)$ to approximate $p(z)$. According to Gibbs’ inequality \\cite{mackay2003information}, we know that the KL divergence is non-negative: $D_{\\rm KL}(p(z)||q(z))\\ge 0 \\Rightarrow -\\sum_z p(z) {\\rm log}~p(z)\\le -\\sum_z~p(z) {\\rm log}~q(z)$. By substitute such inequality into Equation \\ref{I(z;x)}, we can derive an upper bound of $I(Z;X)$:\n\\begin{equation}\n\\begin{aligned}\n    I(Z; X)&\\le\\sum_x p(x)\\sum_z p(z|x){\\rm log}~ p(z|x)-\\sum_z p(z){\\rm log}~q(z)\\\\\n    &=\\sum_x p(x)\\sum_z p(z|x) {\\rm log} \\frac{p(z|x)}{q(z)}\\\\\n    &=\\sum_x p(x)D_{\\rm KL}(p(z|x)||q(z)).\n    \\label{upper bound}\n\\end{aligned}\n\\end{equation}\nFollowing previous work \\cite{alemi2016deep}, we assume that the posterior $p(z|x)=\\mathcal{N}(\\mu(x),{\\rm diag}\\{\\sigma^2(x)\\})$ is a gaussian distribution, where $\\mu(x)$ is the encoded embedding of variable $x$ and ${\\rm diag} \\{\\sigma^2(x)\\} = \\{\\sigma_d^2\\}_{d=1}^D$ is the diagonal matrix indicating the variance. On the other hand, the prior $q (z)$ is assumed to be a standard Gaussian distribution, \\emph{i.e.,} $q(z) = \\mathcal{N}(0, I)$. Finally, we can rewrite the above upper bound as:\n\\begin{equation}\nD_{\\rm KL}(p(z|x)||q(z))=\\frac{1}{2}||\\mu(x)||_2^2+\\frac{1}{2}\\sum_d(\\sigma_d^2- {\\rm log}\\sigma_d^2 -1).\n\\label{kl gaussian}\n\\end{equation}\nThe detailed derivation is shown in Appendix \\ref{app: derivation}.\nFor ease of optimization, we fix $\\sigma(x)$ to be an all-zero matrix.  \nThen $z = \\mu(x)$ becomes a deterministic embedding. The optimization of this upper bound is equivalent to directly applying the $l_2$-norm regularization on the embedding vector $z$.\n\n\\noindent \\textbf{Term $\\circled{2}$.} With the definition of mutual information, we have $I (Z;Y) = H (Y)-H (Y|Z)$, where $H(\\cdot)$ and $H(\\cdot|\\cdot)$ denote the entropy and conditional\nentropy respectively. Since $H (Y)$ is a positive constant and can be ignored, we have the following inequality,\n\\begin{equation}\n    -I(Z;Y)\\le  H(Y|Z).\n\\end{equation}\nIn experiments, $H(Y|Z)$ can be calculated and optimized as the cross entropy loss ($CE$). To further encourage the independence between $f_C$ and $f_B$,\nwe fix the parameters of $f_B$ and train $f_C$ with the samples-wise weighted cross entropy loss ($\\mathcal{L}_{wce}$). The weight is calculated as:\n\\begin{equation}\n    w(x) = \\frac{CE(f_B(x), y)}{CE(f_B(x),y)+CE(f_C(x),y)}.\n\\end{equation}\nFor samples with large losses on $f_B$, $w(x)$ are close to 1; while $w(x)$ are close to 0 when the losses are very small. The intuition of the re-weighting scheme is to let $f_C$ focus on ``hard\" examples for $f_B$ to encourage independence. \n\n\\noindent \\textbf{Term $\\circled{3}$.} Based on the relationship between mutual information and Kullback-Leibler (KL) divergence \\cite{belghazi2018mutual}, the term $I(Z; R)$ is equivalent to the KL divergence between the joint distribution $p(Z, R)$ and the product of two marginals $p(Z)p(X)$ as: $I(Z; R)=D_{\\rm KL}(p(Z, R)||p(Z) p(R))$. Therefore, to minimize the de-confounder penalty term $I(Z;R)$, we propose to use an adversarial process that minimizes the distance between the joint distribution $p(Z,R)$ and the marginals $p(Z) p(R)$. During the adversarial process, a discriminator $D_\\phi$ is trained to\nclassify the sampled representations drawn from the joint $p(Z,R)$ as the real, \\emph{i.e.,} 1 and samples drawn from the marginals $p(Z) p(R)$ as the fake, \\emph{i.e.,} 0. \nThe samples from the marginal distribution $p(Z) p(R)$ are obtained by shuffling the individual representations of samples $(z,r)$ in\na training batch from $p(Z,R)$. On the other hand, the clean model $f_C$ tries to generate $z$ that look like\ndrawn from $p(Z) p(R)$ when combined with $r$ from $f_B$. \nSpecifically, we optimize such adversarial objective similar to WGAN \\cite{arjovsky2017wasserstein} with spectral normalization \\cite{miyato2018spectral} since it is more stable in the learning process:\n\\begin{equation}\n    \\mathcal{L}_{adv} =\\mathop{\\rm min}\\limits_{\\theta_C} \\mathop{ \\rm max}\\limits_{\\phi} \\mathbb{E}_{p(z,r)}[D_\\phi(z,r)] - \\mathbb{E}_{p(z)p(r)}[D_\\phi(z,r)],\n\\end{equation}\nwhere $\\theta_C$ and $\\phi$ denote the parameters of $f_C$ and $D_\\phi$ respectively. To sum up, the training objective for $f_C$ is:\n\\begin{equation}\n    \\mathcal{L}_C = \\mathcal{L}_{wce}+\\mathcal{L}_{adv}+\\beta||\\mu(x)||_2^2.\n\\label{loss function}\n\\end{equation}\nThe overall objective can then be minimized using SGD. $f_B$ and $f_C$ are trained for $T_1$ and $T_2$ epochs respectively. The pseudo algorithm of CBD is shown in Algorithm \\ref{algor}. \n\\begin{algorithm}[t]\n\\caption{Causality-inspried Backdoor Defense (CBD)}\n\\textbf{Input}: $\\beta$, number of training iterations $T_1, T_2$\\\\\n\\textbf{Output}: Clean model $f_C$;\n\\begin{algorithmic}[1]\n\\STATE Initialize $f_C$, $f_B$, and $D_\\phi$\\\\\n\\FOR{$t= 1, \\cdots, T_1$}\n\\STATE Train $f_B$ on the poisoned dataset with SGD\n\\ENDFOR\n\\FOR{$t= 1, \\cdots, T_2$}\n\\STATE Train discriminator $D_\\phi$\n\\STATE Calculate sample weight $w(x)$\n\\STATE Train $f_C$ with loss function in Equation \\ref{loss function}\n\\ENDFOR\n\\end{algorithmic}\\label{algor}\n\\end{algorithm}\n\n\\noindent\\textbf{Further Discussions.} \nAdmittedly, it is challenging to disentangle causal factors and confounding factors thoroughly. This is because $f_B$ may still capture some causal relations. Moreover, encouraging independence between $Z$ and $R$ may result in loss of predictive information for $f_C$.\nHowever, with the well-designed optimization objectives and training scheme, CBD manages to reduce the confounding effects as much as possible while preserving causal relations. The following section shows the detailed verification.\n\n\\begin{table*}[t]\n\\renewcommand{\\arraystretch}{1.1}\n\\renewcommand\\tabcolsep{1.6pt}\n\\small\n\\centering\n\\caption{The attack success rate (ASR \\%) and the clean accuracy (CA \\%)  of 5 backdoor defense methods against 6 representative backdoor attacks. \\emph{None} means the training data is completely clean. The best results are bolded.}\n  \\label{tab1}\n\\begin{tabular}{c|c|cc|cc|cc|cc|cc|cc|cc}\n\\toprule\n\\multirow{2}{*}{Dataset} & \\multirow{2}{*}{Types} & \\multicolumn{2}{c|}{\\begin{tabular}[c|]{@{}c@{}}No Defense\\end{tabular}} & \\multicolumn{2}{c|}{FP} & \\multicolumn{2}{c|}{MCR} & \\multicolumn{2}{c|}{NAD} & \\multicolumn{2}{c|}{ABL}&\\multicolumn{2}{c|}{DBD}& \\multicolumn{2}{c}{\\textbf{CBD (Ours)}}\\\\ \\cline{3-16} \n &  & ASR & CA & ASR & CA & ASR & CA & ASR & CA & ASR & CA& ASR & CA& ASR & CA\\\\ \\hline\n\\multirow{8}{*}{CIFAR-10} \n& \\emph{None} & 0 & 89.14 & 0 & 85.17 & 0 & 87.55 & 0 & 88.21 & 0 & 88.49 &0 & 88.63 &0 & \\bf 88.95 \\\\ \\cline{2-16}\n& BadNets & 100 & 85.37 & 99.96 & 82.41 & 4.52 & 79.66 & 3.07 & 82.25 & 3.13 & 86.30 & 1.76&86.94&\\bf 1.06 &\\bf 87.46\\\\\n & Trojan & 100 & 84.54 & 68.95 & 81.03 & 19.47 & 77.12 & 19.96 & 80.05 & 3.88 & 87.29&3.79&87.29& \\bf1.24 & \\bf87.52 \\\\\n & Blend & 100 & 84.56 & 87.14 & 81.57 & 36.15 & 78.24 & 10.65 & 83.71 & 14.60 & 85.02 &5.12&86.83& \\bf 1.96&\\bf 87.48\\\\\n & SIG & 99.32 & 84.14 & 73.87 & 81.04 & 2.34 & 77.93 & 1.79 & 83.54 & 0.36 &\\bf88.10 &0.44&87.52& \\bf0.25 & 87.29 \\\\\n & Dynamic & 100 & 85.48 & 89.22 & 80.63 & 25.26 & 75.03 & 25.60 & 74.94 & 17.26 & 85.29 &10.21&85.42&\\bf 0.86& \\bf 85.67 \\\\\n &  WaNet & 98.55 & 86.77 & 73.12 & 81.58 & 28.59& 77.12 & 24.15 & 79.50 & 22.24 & 75.74 &5.86&84.60&\\bf 4.24&\\bf 86.55 \\\\\\cline{2-16} \n & \\multicolumn{1}{l}{Average} & \\multicolumn{1}{|l}{99.65} & \\multicolumn{1}{l}{85.14} & \\multicolumn{1}{|l}{82.03} & \\multicolumn{1}{l}{81.38} & \\multicolumn{1}{|l}{19.39} & \\multicolumn{1}{l}{77.52} & \\multicolumn{1}{|l}{14.20} & \\multicolumn{1}{l}{80.67}  & \\multicolumn{1}{|c}{10.25} & \\multicolumn{1}{c}{84.62}&\\multicolumn{1}{|c}{4.53}&\\multicolumn{1}{c}{86.43}& \\multicolumn{1}{|c}{\\bf 1.60} & \\multicolumn{1}{c}{\\bf 87.00} \\\\ \\midrule\n\\multirow{8}{*}{GTSRB} \n& \\emph{None} & 0 & 97.74 & 0 & 90.18 & 0 & 95.27 & 0 & 95.29 & 0 & 96.47&0&96.45& 0 & \\bf 96.54 \\\\ \\cline{2-16} \n& BadNets & 100 & 96.58 & 99.48 & 88.57 & 1.27 & 93.30 & 0.31 & 89.90 & \\textbf{0.05}&96.01&0.24&96.05& 0.16 & \\bf 96.21\\\\\n & Trojan & 99.95 & 96.49 & 97.40 & 88.51 & 4.62 & 92.99 & 0.56 & 90.32 & 0.47 & 94.91&0.56&94.69&\\bf 0.12 & \\bf 95.29 \\\\\n & Blend & 100 & 95.57 & 98.78 & 87.50 & 6.85 & 93.11 & 13.06 & 89.20 & 22.97 & 93.25 &6.36&93.72& \\bf 0.90 & \\bf 94.16 \\\\\n & SIG & 98.24 & 96.55 & 85.04 & 89.97 & 26.80 & 91.14 & 5.35 & 89.27 & 5.09 &\\bf96.28 &\\bf4.70&94.58& 5.41 &  94.37\\\\\n & Dynamic & 100 & 96.87 & 98.33 & 88.09 & 59.54 & 90.51 & 62.35 & 84.30 & 6.32 & 95.76 &5.16&95.86&\\bf0.96&\\bf96.02 \\\\\n  & WaNet & 99.92 & 95.94 & 97.93 & 90.13 & 55.25 & 91.24 & 34.16 & 83.09 & 5.56 & 93.83&3.47&94.71 &\\bf 3.13 &\\bf95.64 \\\\\n  \\cline{2-16} \n & \\multicolumn{1}{l}{Average} & \\multicolumn{1}{|l}{99.69} & \\multicolumn{1}{l}{96.33} & \\multicolumn{1}{|l}{96.16} & \\multicolumn{1}{l}{88.80} & \\multicolumn{1}{|l}{25.72} & \\multicolumn{1}{l}{92.05} & \\multicolumn{1}{|l}{19.30} & \\multicolumn{1}{c}{87.68} &  \\multicolumn{1}{|c}{7.96} & \\multicolumn{1}{c}{95.01}&\\multicolumn{1}{|c}{3.42}&\\multicolumn{1}{c}{94.94}& \\multicolumn{1}{|c}{\\textbf{1.82}} & \\multicolumn{1}{c}{\\textbf{95.17}}\\\\ \\midrule\n\\multirow{6}{*}{\\begin{tabular}[c]{@{}c@{}}ImageNet\\\\ Subset\\end{tabular}} \n& \\emph{None} & 0 & 88.95 & 0 & 83.05 & 0 & 85.61 & 0 & 87.34 & 0 & 88.12 &0&88.30&0&\\bf88.57 \\\\ \\cline{2-16} \n& BadNets & 100 & 85.24 & 98.03 & 82.76 & 25.14 & 77.90 & 7.38 & 82.11 & 1.02 & 87.47&1.27&87.61& \\bf0.66&\\bf88.12 \\\\\n & Trojan & 100 & 85.65 & 97.29 & 81.46 & 6.65 & 77.06 & 13.80 & 81.49 & 1.68 & 88.21&1.48&88.20&\\bf 0.72& \\bf88.24 \\\\\n & Blend & 99.89 & 86.10 & 99.10 & 81.37 & 18.37 & 76.21 & 25.05 & 82.54 & 20.80 & 85.23&4.73&86.25&\\bf1.82& \\bf87.95 \\\\ \n & SIG & 98.53 & 86.06 & 77.39 & 82.55 & 24.62 & 78.97 & 5.30 & 83.24 & \\textbf{0.22} & 86.65 &1.95&87.09&0.45&\\bf87.27\\\\\n\\cline{2-16} \n & \\multicolumn{1}{l}{Average} & \\multicolumn{1}{|l}{99.61} & \\multicolumn{1}{l}{85.74} & \\multicolumn{1}{|l}{92.95} & \\multicolumn{1}{l}{82.04} & \\multicolumn{1}{|l}{18.70} & \\multicolumn{1}{l}{77.54} & \\multicolumn{1}{|l}{12.88} & \\multicolumn{1}{l}{82.35} & \\multicolumn{1}{|c}{5.93} & \\multicolumn{1}{c}{86.89}& \\multicolumn{1}{|c}{2.36} & \\multicolumn{1}{c}{87.29} &\\multicolumn{1}{|c}{\\textbf{0.91}} & \\multicolumn{1}{c}{\\textbf{87.90}} \\\\\n \\bottomrule\n\\end{tabular}\n\\label{core experiment}\n\\vspace{-0.1in}\n\\end{table*}\n\n\\begin{table*}[!tp]\n\\renewcommand{\\arraystretch}{1.2}\n\\renewcommand\\tabcolsep{1.8pt}\n\\small\n\\centering\n\\caption{Robustness test with the poisoning rate from 1\\% to 50\\% for 4 attacks including BadNets, Trojan, Blend, and WaNet on CIFAR10 dataset. We show ASR ($\\%$) and CA ($\\%$).}\n\\begin{tabular}{c|c|cccccccc}\n\\hline\n\\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Poisoning\\\\ Rate\\end{tabular}} & \\multirow{2}{*}{Defense} & \\multicolumn{2}{c}{BadNets} & \\multicolumn{2}{c}{Trojan} & \\multicolumn{2}{c}{Blend} &  \\multicolumn{2}{c}{WaNet} \\\\ \\cline{3-10} \n &  & ASR & CA & ASR & CA & ASR & CA & ASR & CA \\\\ \\hline\n\\multirow{2}{*}{1\\%} & \\emph{None} & 100 & 85.67 & 100 & 85.15 & 100 & 85.22 & 97.56 & 86.55 \\\\ \n & CBD & 0.62 & 88.83 & 1.13 & 88.56 & 0.67 & 87.52 & 1.06 & 86.59 \\\\ \\hline\n \\multirow{2}{*}{5\\%} & \\emph{None} & 100 & 84.68 & 100 & 84.82 & 100 & 85.06 & 99.83 & 86.27 \\\\ \n & CBD & 0.93 & 87.50 & 1.10 & 88.45 & 0.73 & 87.47 & 1.07 & 86.56 \\\\ \\hline\n \\multirow{2}{*}{20\\%} & \\emph{None} & 100 & 83.42 & 100 & 79.32 & 100 & 82.08 & 100 & 74.41 \\\\ \n & CBD & 1.16 & 84.35 & 1.57 & 81.71 & 5.17 & 86.53 & 5.72 & 74.25 \\\\ \\hline\n\\multirow{2}{*}{50\\%} & \\emph{None} & 100 & 79.45 & 100 & 72.83 & 100 & 69.67 & 100 & 67.25 \\\\  \n & CBD & 1.47 & 78.88 & 2.31 & 75.34 & 8.14 & 85.56 & 8.75 & 70.43 \\\\ \\hline\n\\end{tabular}\n\\vspace{-0.1in}\n\\label{poison rate}\n\\end{table*}\n\n\\section{Experiments}\n\\label{sec: experiments}\n\\subsection{Experimental Settings}\n\\label{experiment settings}\n\\noindent\\textbf{Datasets and DNNs.} \nWe evaluate all defenses on three classical benchmark datasets, CIFAR-10 \\cite{krizhevsky2009learning}, GTSRB \\cite{stallkamp2012man} and an ImageNet subset \\cite{deng2009imagenet}. As for model architectures, we adopt WideResNet (WRN-16-1) \\cite{zagoruyko2016wide} for CIFAR-10 and GTSRB and ResNet-34 \\cite{he2016deep} for ImageNet subset. Note that our CBD is agnostic to the model architectures. Results with more models are shown in Appendix \\ref{app:more results}.\n\n\\noindent\\textbf{Attack Baselines.} \nWe consider 6 representative backdoor attacks. Specifically, we select\nBadNets~\\cite{gu2017badnets}, Trojan attack~\\cite{liu2018trojaning}, Blend attack~\\cite{chen2017targeted}, Sinusoidal signal attack~(SIG)~\\cite{barni2019new}, Dynamic attack~\\cite{nguyen2020input}, and WaNet~\\cite{nguyen2021wanet}.  BadNets, Trojan attack are patch-based visible dirty-label attacks; Blend is an invisible dirty-label attack; SIG belongs to clean-label attacks; Dynamic and WaNet are dynamic dirty-label attacks. More types of backdoor attacks are explored in Appendix \\ref{app:more results}. The results when there is no attack and the dataset is completely clean is shown for reference.\n\n\\noindent\\textbf{Defense Baselines.}\nWe compare our CBD with 5 state-of-the-art defense methods: Fine-pruning (FP)~\\cite{liu2018fine}, Mode Connectivity Repair (MCR)~\\cite{zhao2020bridging}, Neural Attention Distillation (NAD)~\\cite{li2021neural}, Anti-Backdoor Learning (ABL)~\\cite{li2021anti}, and Decoupling-based backdoor defense (DBD) \\cite{backdoordecouple}. We also provide results of DNNs trained without any defense methods \\emph{i.e.,} No Defense.\n\n\\noindent\\textbf{Attack Setups.}\nWe trained DNNs on poisoned datasets for 100 epochs using Stochastic Gradient Descent (SGD) with an initial learning rate of 0.1 on CIFAR-10 and the ImageNet subset (0.01 on GTSRB), a weight decay of 0.0001, and a momentum of 0.9. The target labels of backdoor attacks are set to 0 for CIFAR-10 and ImageNet, and 1 for GTSRB. The default poisoning rate is set to 10$\\%$.\n\n\\noindent\\textbf{Defense Setups.}\nFor FP, MCR, NAD, ABL, and DBD, we follow the settings specified in their original papers, including the available clean data. \nThree data augmentation techniques suggested in \\cite{li2021anti} including random crop, horizontal flipping, and cutout, are applied for all defense methods. The hyper-parameter $T_1$ and $\\beta$ are searched in $\\{3,5,8\\}$ and $\\{1e^{-5}, 1e^{-4}, 1e^{-3}\\}$ respectively. Following the suggestion of the previous work \\cite{zhao2020bridging}, we choose hyperparameters with 5-fold cross-validation on the training set according to the average classification accuracy on hold-out sets. $T_2$ is set to 100 in the default setting. \nAll experiments were run one NVIDIA Tesla V100 GPU.\nMore details of settings are shown in Appendix \\ref{app:settings}. \n\n\\noindent\\textbf{Metrics.} We adopt two commonly used performance metrics for the evaluation all methods: \\emph{attack success rate} (ASR) and \\emph{clean accuracy} (CA).\nLet $\\mathcal{D}_{test}$ denotes the benign testing set and $f$ indicates the trained classifier, we have ${\\rm ASR} \\triangleq \\Pr_{({x},y) \\in \\mathcal{D}_{test}}\\{f(B({x}))=y_t|y \\neq y_t\\}$ and ${\\rm CA} \\triangleq \\Pr_{({x},y) \\in \\mathcal{D}_{test}}\\{f({x})=y\\}$, where $y_t$ is the target label and $B(\\cdot)$ is the adversarial generator to add triggers into images. \nOverall, the lower the ASR and the higher the BA, the better the defense.\n\n\\subsection{Effectiveness of CBD}\n\\noindent\\textbf{Comparison to Existing Defenses.} Table \\ref{core experiment} demonstrates the proposed CBD method on CIFAR-10,\nGTSRB, and ImageNet Subset. We consider 6 representative backdoor attacks and compare the performance of CBD with four other backdoor defense techniques. We omit some attacks on ImageNet dataset due to a failure of reproduction following their original papers.\nWe can observe that CBD achieves the lowest average ASR across all three datasets. Specifically, the average ASRs are reduced to around 1$\\%$ (1.60$\\%$ for CIFAR-10, 1.82$\\%$ for GTSRB, and 0.91$\\%$ for ImageNet). \nOn the other hand, the CAs of CBD are maintained and are close to training DNNs on clean datasets without attacks.\nWe argue that baseline methods which try to fine-prune or unlearn backdoors of backdoored models are sub-optimal and less efficient. For example, the ABL tries to unlearn backdoor after model being backdoored; DBD requires additional a self-supervised pre-training stage, which introduces around 4 times overhead \\cite{backdoordecouple}. On the contrary, CBD directly trains a backdoor-free model, which achieves high clean accuracy while keeping efficiency. \\label{effectiveness}\n\nWhen comparing the performance of CBD against different backdoor attacks, we find WaNet achieves higher ASR than most attacks consistently (4.24$\\%$ for CIFAR-10, 3.41$\\%$ for GTSRB). This may be explained by the fact that WaNet  as one of the state-of-the-art backdoor attacks adopts image warping as triggers that are stealthier than patch-based backdoor attacks \\cite{nguyen2021wanet}. Hence, the spurious correlations between backdoor triggers and the target label are more difficult to capture for $f_B$. Then in the second step, $f_C$ struggles to distinguish the causal and confounding effects. We also notice that CBD is not the best when defending SIG on GTSRB and ImageNet Subset. We guess the reason is similar to WaNet discussed above. SIG produces poisoned samples by mingling trigger patterns with the background. Moreover, SIG belongs to clean-label attacks, which are stealthier than dirty-label attacks \\cite{barni2019new}. This is one limitation of our CBD to be improved in the future.\n\n\\noindent\\textbf{Effectiveness with Different Poisoning Rate. }In Table \\ref{poison rate}, we demonstrate that our CBD is robust and can achieve satisfactory defense performance with a poisoning rate ranging from 1$\\%$ to 50$\\%$. Note that the results when poisoning rate equals $0\\%$ have been shown in Table \\ref{core experiment} (\\emph{None}). Here, we did experiments on CIFAR-10\nagainst 4 attacks including BadNets, Trojan, Blend, and WaNet. Generally, with a higher poisoning rate, CBD has lower CA and higher ASR. We can find that even with a poisoning rate of up to 50$\\%$, our CBD\nmethod can still reduce the ASR from 100$\\%$ to 1.47$\\%$, 2.31$\\%$, 8.14$\\%$, and 8.75$\\%$ for BadNets, Trojan, Blend, and WaNet, respectively. Moreover, CBD helps backdoored DNNs recover clean accuracies. For instance, the CA of CBD for Blend and WaNet improves from 69.67$\\%$ and 67.25$\\%$ to 85.56$\\%$ and 70.43$\\%$ respectively at 50$\\%$ poisoning rate.\n\n\\noindent \\textbf{Visualization of the Hidden Space.}\nIn Figure \\ref{tsne}, we show the t-SNE \\cite{JMLR:v9:vandermaaten08a} visualizations of the embeddings to give more insights of our proposed method. We conduct the BadNets attack on CIFAR-10. First, in Figure \\ref{tsne} (a)$\\&$(b), we show the embeddings of $r$ and $z$ when CBD is just initialized and when the training of CBD is completed. We observe that there is a clear separation between the confounding component $r$ and the causal component $z$ after training. Moreover, in Figure \\ref{tsne} (c)$\\&$(d), we use t-SNE separately on $r$ and $z$ and mark samples with different labels with different colors. Interestingly, we find the embeddings of the poisoned samples form clusters in $r$, which indicates that the spurious correlation between backdoor trigger and the target label has been learned. In contrast, poisoned\nsamples lie closely to samples with their ground-truth label in deconfounded embeddings $z$, which demonstrates CBD can effectively defend backdoor attacks. \n\n\\noindent \\textbf{Computational Complexity. }Compared with the vanilla SGD to train a DNN model, CBD only requires additionally training a backdoored model $f_B$ for a few epochs (\\emph{e.g.,} 5 epochs) and a discriminator $D_\\phi$, which introduces minimal extra overhead. \nHere, we report the training time cost of CBD on CIFAR-10 and the ImageNet\nsubset in Table \\ref{time cost}. We also report the time costs of training vanilla DNNs for reference.\nThe extra computational cost is around $10\\%-20\\%$ of the standard training time on CIFAR-10 and the ImageNet subset.  This again shows the advantages of our method.\n\n\\begin{figure*}[t]\n\t\\centering\n\t\\subfloat[Initialization]{\\includegraphics[width=0.24\\linewidth]{figs/tsne_init.pdf}}\n    \\subfloat[Training Completed]{\\includegraphics[width=0.24\\linewidth]{figs/tsne_independence.pdf}}\n    \\subfloat[$r$]{\\includegraphics[width=0.24\\linewidth]{figs/tsne_bias.pdf}}\n    \\subfloat[$z$]{\\includegraphics[width=0.24\\linewidth]{figs/tsne_clean.pdf}}\n\t\\caption{Visualization of the hidden space with t-SNE}\n\t\\label{tsne}\n\\end{figure*}\n\n\\begin{table}[t]\n  \\caption{The average training time (seconds) on CIFAR10 and the ImageNet subset with no defense and CBD. The percentages in parentheses indicate the relative increase of training time.}\n\\scriptsize\n  \\centering\n  \\begin{tabular}{c|cccc}\n    \\toprule\n     \\multirow{2}{*}{Dataset} & \\multicolumn{2}{c}{CIFAR-10} & \\multicolumn{2}{c}{ImageNet subset} \\\\\\cline{2-5} \n & No Defense & CBD & No Defense & CBD \\\\\n    \\hline\n    BadNets& 1152& 1317{\\tiny(14.3$\\%$)}&2640 & 2987{\\tiny(13.1$\\%$)}\\\\\n    Trojan& 1204& 1356{\\tiny(12.6$\\%$)}& 2621& 2933{\\tiny(11.9$\\%$)}\\\\\n    Blend&1159 & 1311{\\tiny(13.1$\\%$)}& 2623& 3076{\\tiny(17.3$\\%$)}\\\\\n    WaNet&1164& 1293{\\tiny(11.1$\\%$)}& 2647& 3074{\\tiny(16.1$\\%$)}\\\\ \n    \\bottomrule\n  \\end{tabular}\n  \\label{time cost}\n\\end{table}\n\\subsection{Resistance to Potential Adaptive Attacks}\\label{adaptive attack}\nWhile not our initial intention, our work may be used to help develop more advanced backdoor attacks. Here, we tentatively discuss the potential adaptive attacks on CBD. Typically, backdoor attacks are designed to be injected successfully in a few epochs even only a small portion of data is poisoned (\\emph{e.g.,} less than 1$\\%$). Hence, the confounding bias of backdoor can be well captured by $f_B$ and $R$. \nThe intuition of our adaptive attack strategy is to slow the injection process of backdoor attacks (\\emph{i.e.,} increasing the corresponding training losses) by adding optimized noise into the poisoned examples, similar to recent works on adversarial training \\cite{madry2017towards} and unlearnable examples \\cite{huang2021unlearnable}.\nIf the confounding effects is not captured in the first step, then our CBD becomes ineffective. \n\n\\noindent \\textbf{Assumptions on Attacker's Capability. }\nWe assume that attackers have the entire benign dataset. The attackers may have the knowledge of the DNN architecture but cannot interfere with the training process. Moreover, the attackers cannot further modify their poisoned data once the poisoned examples are injected into the training dataset.\n\n\\noindent \\textbf{Problem Formulation.} We formulate the adaptive attack as a min-max optimization problem. Let $\\mathcal{D}'=\\{({x}_i, y_i)\\}_{i=1}^{N'}$ indicates the poisoned images by backdoor attacks,  $\\mathcal{D}=\\{({x}_i, y_i)\\}_{i=1}^{N}$ denotes the benign images, and $\\delta_i$ is the added noise to be optimized. Given an DNN model $f_{\\theta}$ with parameters ${\\theta}$, the adaptive attack aims to optimize $\\delta_i$ by maximizing the losses of poisoned examples while minimizing the average cross entropy losses of all the samples, \\emph{i.e.,}\n\\begin{equation}\n    \\min_{\\theta}\\left[\\sum_{x \\in \\mathcal{D}} \\mathcal{L}(f_\\theta(x),y)+\\sum_{x \\in \\mathcal{D}'} \\max_{\\delta_i}\\mathcal{L}(f_\\theta(x+\\delta_i),y)\\right],\n\\end{equation}\nwhere the noise $\\delta_i$ is bounded by $\\|\\delta_i\\|_p \\le \\epsilon$ with $\\|\\cdot\\|_p$ denoting the $L_p$ norm, and $\\epsilon$ is set to be small such that the poisoned samples cannot be filtered by visual inspection. After optimization,  the poisoned examples attached with the optimized noises $\\delta_i$ are injected to the training dataset.\nWe adopt the first-order optimization method PGD \\cite{madry2017towards} to solve the constrained inner maximization problem:\n\\begin{equation}\n    x_{t+1} = \\Pi_\\epsilon(x_t + \\alpha\\cdot \\nabla_x\\mathcal{L}(f_{\\theta}(x_t), y)),\n\\end{equation}\nwhere $t$ is the current perturbation step ($M$ steps in total), $\\nabla_x\\mathcal{L}(f_{\\theta}(x_t), y)$ is the gradient of the loss\nwith respect to the input, $\\Pi_\\epsilon$ is a projection function that clips the noise back to the $\\epsilon$-ball around\nthe original example $x$ when it goes beyond, and $\\alpha$ is the step size. Pseudo codes of the adaptive attack are shown in Appendix \\ref{app:more results}.\n\n\\noindent \\textbf{Experimental Settings.}\nWe adopt the CIFAR-10 dataset and WRN-16-1 to conduct the experiments. According to previous studies in adversarial attacks, small $L_\\infty$-bounded noise within $\\|\\delta\\|_\\infty \\textless 8/255$ on images are unnoticeable to human observers. Therefore, we consider the same constraint in our experiments.\nWe use the SGD to solve the above optimization problem for 10 epochs with the step size $\\alpha$ of 0.002 and $M = 5$ perturbation steps.\n\n\\noindent \\textbf{Results.}\nWith BadNets, the adaptive attack works well when there is no defense (CA=84.55\\%, ASR=99.62\\%). However, this attack still fails to attack our CBD (CA=84.19\\%, ASR=4.31\\%). More detailed results are shown in Appendix \\ref{app:more results}. We can conclude that our defense is resistant to this adaptive attack.\nThe most probable reason is that the optimized noise becomes less effective when the model is retrained and the model parameters are randomly initialized. In another word, the optimized perturbations are not transferable.\n\n\\section{Conclusion}\nInspired by the causal perspective, we proposed Causality-inspired Backdoor Defense (CBD) to learn deconfounded representations for reliable classification. Extensive experiments against 6 state-of-the-art backdoor attacks show the effectiveness and robustness of CBD. Further analysis shows that CBD is robust against potential adaptive attacks. Future works include extending CBD to other domains such graph learning \\cite{zhang2022hierarchical,zhang2022protgnn, zhang2021graphmi, zhang2022model}, federated learning \\cite{cao2022flcert}, and self-supervised pertaining \\cite{zhang2021motif, zhang2021graph}. In summary, our work opens up an interesting research direction to leverage causal inference to analyze and mitigate backdoor attacks in machine learning.\n\n\\section*{Acknowledgements}\nThis research was partially supported by a grant from the National Natural Science Foundation of China (Grant No. 61922073).\n\n{\\small\n}\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2110.11571v3.tex",
        "arXiv-2202.03423v1.tex",
        "arXiv-2303.06818v1.tex"
    ],
    "group_id": "group_75",
    "response": "### Title: Robust Learning Against Backdoor Attacks in Deep Neural Networks\n\n### Introduction\n\nDeep neural networks (DNNs) have revolutionized numerous fields, from computer vision to natural language processing, due to their ability to learn complex patterns from large datasets. However, the reliance on vast amounts of training data has introduced new vulnerabilities, particularly in the form of backdoor attacks. Backdoor attacks are a type of data poisoning where a small number of training examples are altered to include a specific trigger that causes the model to misclassify images when the trigger is present, while behaving normally on clean data. This threat is exacerbated by the fact that DNNs are highly susceptible to learning these triggers, even if they are subtle or invisible. The stealthiness of these triggers makes them challenging to detect, and once embedded, they can significantly compromise the model's integrity and security.\n\nHistorically, backdoor attacks have been categorized into two main types: dirty-label attacks, where the labels of poisoned examples are changed to a target class, and clean-label attacks, where the labels remain unchanged but the model is still manipulated to misclassify images with the trigger. Recent studies have shown that DNNs are particularly vulnerable to these attacks, especially when the trigger is designed to be subtle and integrated into the training process in a way that mimics natural variations in the dataset. This vulnerability has led to a surge in research aimed at detecting and mitigating backdoor attacks. Detection methods typically focus on identifying anomalies in the training data or the model's behavior, while erasing methods attempt to remove the learned triggers from the model. However, these methods often require prior knowledge about the distribution of poisoned data or access to clean data, which may not always be available in real-world scenarios.\n\nCurrent progress in backdoor defense has seen the development of various techniques, but a fundamental question remains: can we train a clean model directly on poisoned data without any prior knowledge of the backdoor examples? This question is particularly pertinent in scenarios where data sources are not fully trusted, and the presence of backdoor examples is unknown. Addressing this challenge requires a deeper understanding of the learning dynamics of DNNs on poisoned versus clean data. The three papers discussed in this summary explore this problem from different angles, each proposing novel methods to train clean models on potentially poisoned datasets.\n\n### Paper 1: Anti-Backdoor Learning (ABL)\n\nThe first paper introduces the concept of Anti-Backdoor Learning (ABL), a method designed to train clean models directly on poisoned datasets. The authors observe that backdoor examples are easier and faster to learn than clean examples, a characteristic that can be exploited to detect and isolate them during training. They frame the overall learning process as a dual-task problem, where the model learns both clean and backdoor portions of the data. The key insight is that the backdoor task is tied to a specific target class, making it possible to break the correlation between the trigger pattern and the target class by randomizing the labels of a subset of examples with low loss.\n\nTo implement ABL, the authors propose a two-stage training scheme. The first stage, called Local Gradient Ascent (LGA), intentionally maximizes the training loss gap between clean and backdoor examples to isolate the backdoored data. This is achieved by setting a loss threshold $\\gamma$ and using a gradient ascent mechanism to boost the loss of backdoor examples that fall below this threshold. The second stage, Global Gradient Ascent (GGA), aims to unlearn the backdoor correlation by applying gradient ascent on the isolated backdoor set. This method is designed to be robust to various backdoor attacks and to maintain high clean accuracy without compromising the model's performance on clean data.\n\nThe authors conduct extensive experiments on multiple benchmark datasets, including CIFAR-10, GTSRB, and an ImageNet subset, against 10 state-of-the-art backdoor attacks. The results show that ABL-trained models achieve clean accuracy comparable to those trained on purely clean data, while significantly reducing the attack success rate (ASR) to near-random levels. ABL is particularly effective against feature-space attacks, which are known to be more challenging to detect and mitigate. The method's performance is robust across different isolation rates and turning epochs, demonstrating its flexibility and adaptability to varying levels of poisoning.\n\n### Paper 2: Decoupling-based Backdoor Defense (DBD)\n\nThe second paper proposes a Decoupling-based Backdoor Defense (DBD) method that decouples the end-to-end supervised training process into three stages. The first stage involves self-supervised learning of the backbone (feature extractor) using unlabeled training samples. This stage is designed to purify the feature extractor by learning representations that are less likely to capture the backdoor trigger patterns. The second stage freezes the feature extractor and trains the remaining fully connected layers using all labeled training samples. The third stage involves semi-supervised fine-tuning of the whole model, where high-credible samples (those with low loss) are used as labeled samples, and the labels of low-credible samples are removed to fine-tune the model.\n\nThe authors reveal that poisoned samples tend to cluster together in the feature space of DNNs trained with supervised learning, whereas they lie closely to samples with their ground-truth label in the feature space of models trained with self-supervised learning. This clustering behavior is attributed to the end-to-end supervised training paradigm, which allows DNNs to learn features about the backdoor trigger and connect them with the target label. By decoupling the training process, DBD aims to prevent the creation of hidden backdoors while preserving the accuracy on clean data.\n\nExperiments on CIFAR-10 and an ImageNet subset against four representative attacks, including BadNets, Blend, WaNet, and Label-Consistent, demonstrate the effectiveness of DBD. The method significantly reduces the ASR while maintaining high clean accuracy. DBD is compared with other state-of-the-art defenses, including Fine-pruning (FP), Mode Connectivity Repair (MCR), Neural Attention Distillation (NAD), and Poison Suppression Gradient Descent (DPSGD). DBD outperforms these methods, particularly in scenarios with high poisoning rates, where it maintains high accuracy even when the poisoning rate reaches 20%.\n\n### Paper 3: Causality-inspired Backdoor Defense (CBD)\n\nThe third paper introduces a Causality-inspired Backdoor Defense (CBD) method that leverages causal reasoning to mitigate backdoor attacks. The authors construct a causal graph to model the generation process of poisoned data, identifying the backdoor attack as a confounder that opens a spurious path between the input image and the target label. This spurious path causes the model to learn erroneous correlations that can be exploited by attackers. CBD aims to learn deconfounded representations that capture only the causal relations between the input and the label, while ignoring the confounding effects of the backdoor trigger.\n\nCBD consists of two main models: a backdoored model $f_B$ trained to capture the spurious correlations, and a clean model $f_C$ trained to be independent of $f_B$ in the hidden space. The training objective for $f_C$ includes an adversarial loss for mutual information minimization, a sample-wise re-weighting scheme to focus on hard examples for $f_B$, and an $l_2$-norm regularization to encourage deconfounded representations. The authors argue that by focusing on causal relations, CBD can effectively reduce the impact of backdoor triggers while maintaining high accuracy on clean data.\n\nExperiments on CIFAR-10, GTSRB, and an ImageNet subset against six representative attacks, including BadNets, Trojan, Blend, SIG, Dynamic, and WaNet, verify the effectiveness of CBD. The method achieves the lowest average ASR across all datasets, with ASRs reduced to around 1% on CIFAR-10 and ImageNet, and 1.82% on GTSRB. Additionally, CBD maintains high clean accuracy, comparable to models trained on clean data without attacks. The authors also conduct a robustness test with poisoning rates ranging from 1% to 50%, showing that CBD can effectively reduce ASR and maintain clean accuracy even under high poisoning rates. Furthermore, CBD is shown to be robust against potential adaptive attacks that aim to optimize the trigger patterns to evade detection.\n\n### Commonalities and Innovations\n\nAll three papers address the challenge of training clean models on poisoned datasets without prior knowledge of the backdoor examples. They share a common goal of mitigating the impact of backdoor attacks while preserving the model's accuracy on clean data. However, each paper proposes a unique approach to achieve this goal.\n\n- **ABL** introduces a gradient ascent mechanism to isolate and unlearn backdoor examples during training. It leverages the faster learning behavior of backdoor examples and the specific class association of backdoor tasks to detect and mitigate backdoor attacks.\n- **DBD** decouples the training process into three stages: self-supervised learning of the backbone, supervised learning of the fully connected layers, and semi-supervised fine-tuning. This approach aims to purify the feature extractor and filter out high-credible samples to fine-tune the model.\n- **CBD** employs causal reasoning to model the generation process of poisoned data and trains two models to capture spurious and causal relations separately. The clean model is trained to be independent of the backdoored model in the hidden space, using adversarial training and sample re-weighting to encourage deconfounded representations.\n\n### Comparison of Results\n\nThe three papers demonstrate varying levels of effectiveness in reducing ASR and maintaining clean accuracy. ABL shows the best performance in reducing ASR against most backdoor attacks, with an average ASR reduction of 12.71% on CIFAR-10, 11.90% on GTSRB, and 7.35% on the ImageNet subset compared to the best baseline method. However, ABL is less effective against attacks that mingle trigger patterns with the background, such as Blend and SIG.\n\nDBD is significantly better than other defenses having the same requirements, such as DPSGD and ShrinkPad, in defending against all attacks. It achieves a benign accuracy that is 20% higher than DPSGD and maintains a lower ASR. DBD's effectiveness is robust across different poisoning rates, with ASRs reduced to around 1% even at 50% poisoning rates.\n\nCBD achieves the lowest average ASR across all datasets, with ASRs reduced to around 1% on CIFAR-10, 1.82% on GTSRB, and 0.91% on the ImageNet subset. It also maintains high clean accuracy, comparable to models trained on clean data without attacks. CBD is particularly effective against feature-space attacks and clean-label attacks, which are known to be more challenging to detect and mitigate.\n\n### Conclusion\n\nThe three papers introduce innovative methods to train clean models on poisoned datasets, each addressing the challenge from a unique perspective. ABL leverages the faster learning behavior of backdoor examples, DBD decouples the training process to purify the feature extractor, and CBD employs causal reasoning to learn deconfounded representations. These methods collectively demonstrate the feasibility of training clean models directly on poisoned data without prior knowledge of the backdoor examples.\n\nThe results of these methods highlight their effectiveness in reducing ASR while maintaining high clean accuracy, even under high poisoning rates. ABL is particularly effective against feature-space attacks, DBD is robust against a wide range of attacks and poisoning rates, and CBD achieves the lowest ASR across all datasets. However, each method has its limitations. ABL struggles with attacks that mingle trigger patterns with the background, DBD requires additional self-supervised pretraining stages, and CBD may not fully disentangle causal and confounding factors.\n\nFuture research directions include extending these methods to other domains, such as federated learning and self-supervised learning, and exploring their robustness against adaptive attacks. Additionally, further investigation into the underlying mechanisms of backdoor attacks and the development of more comprehensive defense strategies could enhance the security of DNNs in real-world applications. The integration of causal inference and adversarial training into backdoor defense offers promising avenues for future work, potentially leading to more robust and secure machine learning models."
}