{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{\\ourmodel{}: A pioneering Large Language Model for Law}\n\n\\begin{document}\n\n\\maketitle\n\\def\\thefootnote{*}\\footnotetext{Equal contribution.}\\def\\thefootnote{\\arabic{footnote}}\n\\begin{abstract}\nIn this paper, we introduce \\ourmodel{}, a large language model (LLM) tailored for the legal domain. With 7 billion parameters, \\ourmodel{} is the first LLM designed explicitly for legal text comprehension and generation. Leveraging the Mistral 7B architecture as its foundation, \\ourmodel{} is trained on an English legal corpus of over 30 billion tokens. \\ourmodel{} exhibits state-of-the-art proficiency in understanding and processing legal documents. Additionally, we present a novel instructional fine-tuning method that leverages legal datasets to further enhance \\ourmodel{}'s performance in legal tasks. \\ourmodel{} is released under the MIT License.\n\\end{abstract}\n\n\\section{Introduction}\n\nIn the rapidly evolving landscape of artificial intelligence, the applications of large language models (LLMs) \\cite{achiam2023gpt,scao2022bloom,penedo2023refinedweb,touvron2023llama,jiang2023mistral,jiang2024mixtral,touvron2023llama2,bai2023qwen} have witnessed large advancements across various domains, like \\textit{e.g.}\\ translation \\cite{xu2023paradigm}, medical \\cite{chen2023meditron}, and code generation \\cite{roziere2023code,li2023starcoder}. From natural language processing to machine translation, these models have exhibited exceptional capabilities in understanding and generating human-like text \\cite{weber2023testing,islam2023distinguishing,mitchell2023detectgpt}. \nHowever, one field that has yet to experience the full benefit of this transformative technology is the legal domain \\cite{martin2024better,licari2022italian}. As legal professionals grapple with an ever-expanding volume of complex documents, there is a growing need for a dedicated LLM that can help navigate and interpret legal material \\cite{savelka2023explaining,katz2023gpt,xiao2021lawformer}.\n\nIn this paper, we present a pioneering initiative to develop the first legal LLM publicly available.\nLegal text, characterized by its unique syntax and specialized vocabulary presents a distinct linguistic challenge \\cite{chalkidis2020legal,niklaus2021swiss}.\nOur approach focuses on extensive pretraining \\cite{gururangan2020don,yao2021adapt} using dedicated legal corpora from English-speaking jurisdictions such as the USA, Canada, the UK, and Europe \\cite{aletras2016predicting,gutierrez2021spanish}.\nLeveraging the pretraining on a large and diverse legal dataset, both scraped by our team as well as from previous literature \\citep{niklaus2022budgetlongformer}, our LLM, \\ourmodel{}, aims not only to comprehend the complexities of legal documents but also to adapt to the evolving nature of legal discourse.\n\nBy focusing on the needs of legal practitioners and harnessing the power of pretraining on dedicated legal corpora, our work represents an important step towards fulfilling the unique demands of the legal domain. We anticipate that introducing the first LLM for law will not only empower legal professionals but also catalyze further innovation at the intersection of artificial intelligence and the legal community - making a significant contribution to legal language understanding and application \\cite{prakken2013logical}. We summarize the contributions of this work as follows:\n\n\\paragraph{Contribution 1: A family of legal LLMs.} In this paper, we introduce the \\ourmodel{}'s family, a collection of Legal Language Models meticulously crafted to tackle the distinctive challenges encountered within the legal domain. We unveil \\ourmodel{}, a 7-billion-parameter language model specifically tailored to legal text. With its specialized training regimen, \\ourmodel{} demonstrates a superior understanding of the nuances in legal language compared to generic models. Furthermore, we release \\ourmodelift{}, an instruction-tuned variant, carefully engineered to outperform existing models such as \\texttt{Mistral} or \\texttt{Llama} on a variety of legal tasks\\footnote{Model is available at \\url{https://huggingface.co/Equall}.}.\n\n\\paragraph{Contribution 2: An improved evaluation protocol for legal LLMs.} Concurrently, we introduce \\legalbench{}, a supplemental iteration of LegalBench \\cite{guha2022legalbench,guha2023legalbench}\\footnote{Dataset is processed and available at \\url{https://huggingface.co/Equall}}, crafted to better gauge and refine the legal proficiency of language models, which we hope will contribute to future advancements into research in the legal domain. To further enrich the models' capabilities in legal contexts, we also include the legal tasks of the popular MMLU benchmark \\cite{hendrycks2020measuring} in our evaluation protocol, particularly focusing on international law, professional law\\footnote{We use the term ``professional law'' here as defined in \\citep{hendrycks2020measuring}} and jurisprudence.\n\\begin{figure*}[!ht]\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/graph.pdf}\n    \\caption{\\textbf{Procedure for constructing \\ourmodel{}}. We rely on legal datasets augmented with replay data, and instructions datasets. For fine-tuning we enrich our instruction finetuning dataset further with legal instructions.}\n    \\label{fig:main_saul}\n\\end{figure*}\n\n\\paragraph{Contribution 3: Model, Evaluation Code \\& Licensing.} To foster widespread adoption and promote innovation, we release \\ourmodel{} and \\ourmodelift{}, as well as our evaluation code under the MIT License. This open licensing approach encourages collaborative development and adoption into a wide array of commercial and research endeavors within the legal domain and beyond.\n\n\\section{\\ourmodel{}: Extending the legal capabilities of Language Models}\n\nA wide range of open-source large language models is available for the backbone, spanning from $70$ million parameter models like Pythia \\citep{biderman2023pythia} to $180$ billion parameter models like Falcon \\citep{falcon}. In this work, we choose the Mistral $7$B model, a $7$ billion parameter open-source model that achieves high performance across benchmarks and tasks \\citep{jiang2023mistral}.\n\nOur methodology, shown in \\autoref{fig:main_saul} involves a two-step process that we describe below.\n\n\\subsection{Enhancing Mistral's Legal Capabilities}\nWhile generic models \\cite{touvron2023llama,taylor2022galactica,zhang2022opt,gu2023mamba,falcon,zhang2024tinyllama,faysse2024croissantllm} gain some exposure to legal data during their training, it typically only represents a minor fraction of the overall data. A straightforward method to enhance performance for legal tasks is to perform additional training focusing on legal data. This approach, particularly focused on decoder models, has been successfully used in various fields such as medicine \\cite{chen2023meditron,ji2023domain}, translation \\cite{xu2023paradigm,wu2024adapting}, and coding \\cite{roziere2023code}. \nThe key advantage of this approach is its scalability and independence from the specific characteristics of the training data.\nOther research on domain adaptation has attempted to specialize language models via pretext tasks. However, these efforts often rely on smaller-scale approaches \\cite{niklaus2023can}, are computationally expensive \\cite{vu2020effective,lu2023prompt}, or lack scalability \\cite{cheng2023adapting,cui2023chatlaw,nishida2019unsupervised}.\n\nFor these reasons, as well as the availability of large-scale legal corpora from the web, we chose to focus on \\emph{continued pretraining}.\nWe meticulously curate a high-quality dataset sourced from diverse legal content repositories. After rigorous filtering \n \\citep{penedo2023refinedweb} and deduplication \\citep{chenghao_mou_2023_8364980,kocetkov2023the}, we end up with a corpus of $30$ billion tokens, which serves as a robust foundation for continued pretraining.\n \n\\subsection{Improving Legal Instruction Following}\nTo support user requests and conversational interaction, LLMs typically undergo instruction tuning, a critical process involving training on supervised conversational pairs. This step is essential for crafting a versatile model, adept at addressing user queries \\cite{wang2023far,wei2021finetuned,chung2022scaling,Faysse_2023,ding2023enhancing,wang2023selfinstruct}.\n\nFor general-purpose language models, diversity and quality of instruction are crucial \\cite{cao2023instruction,zhou2023lima}. However, in specialized domains it is crucial to incorporate task-specific and specialized prompts to enhance performance. Our instruction fine-tuning stage involves $2$ key components: generic (ie, non-legal) and legal instructions. The former help enhance the model's understanding and following of commands, and includes data from diverse domains such as coding, mathematics, and general conversations. For the latter we employ an extensive collection of datasets tailored to the nuances of legal domains, covering legal question answering and summarization, among others.\nThrough this meticulous fine-tuning on instructional data, our model, \\ourmodelift{}, is able to grasp legal intricacies and excels in a wide range of associated tasks.\n\n\\begin{remark}\nIt's worth noting that many common LLMs \\cite{tunstall2023zephyr} include an additional step of to align the model with human preference \\cite{rafailov2023direct,munos2023nash,vonwerra2022trl}. In our case, early experiments did not show any meaningful improvement in performance and so we opted to not pursue this avenue for the present paper.\n\\end{remark}\n\n\\section{Data}\nIn this section we describe our data collection and cleaning schemes.\n\n\\subsection{Legal Pretraining Corpora}\nUnlike fields such as science and medicine, the legal landscape varies significantly across countries and jurisdictions, reflecting differences not only in local laws but also in legal traditions, like common law versus civil law \\cite{henderson2022pile}. Thus, we gathered legal texts from various jurisdictions, with a primary focus on the English language due to its widespread use in legal contexts worldwide. Our collection includes data from the U.S. \\cite{tuggener2020ledgar}, Europe \\cite{chalkidis2019neural}, and Australia \\cite{butler-2023-open-australian-legal-corpus}, covering a diverse range of legal systems. Through this thorough curation process and aggressive cleaning (see \\Cref{sec:data_cleaning}), we end up with a corpus of 30 billion tokens, capturing the intricacies of legal language across regions.% In order to be able to release the model under a permissive license, we restrict ourselves to data available under a commercial license.\n\n\\subsubsection{Dataset Composition} \\label{sec:dataset_composition}\n\n\\paragraph{Legal Sources}\nWe combine both previously available datasets, such as the FreeLaw subset from The Pile \\citep{gao2020pile} and MultiLegal Pile \\citep{niklaus2023multilegalpile}, as well as data scraped from publicly available sources on the Web. We list the different sources of data in \\Cref{tab:data-sources}.\n\n\\begin{table}[ht]\n    \\small\n    \\centering\n    \\begin{tabular}{lc}\n    \\toprule              \n     Name & Tokens \\\\\n     \\midrule\n     FreeLaw\\tablefootnote{We used the subset from The Pile \\citep{gao2020pile}.} & $15$B \\\\\n     EDGAR\\tablefootnote{\\url{https://www.sec.gov/edgar}} & 5B \\\\\n     English MultiLegal Pile\\tablefootnote{We limited ourselves to the commercially-licensed subset: \\url{https://huggingface.co/datasets/joelniklaus/Multi_Legal_Pile_Commercial}} & $50$B \\\\\n     English EuroParl \\citep{koehn-2005-europarl} & $6$B \\\\\n     GovInfo\\tablefootnote{\\url{https://www.govinfo.gov/}} Statutes, Opinions \\& Codes & $11$B \\\\\n     Law Stack Exchange\\tablefootnote{\\url{https://huggingface.co/datasets/ymoslem/Law-StackExchange}} & $19$M \\\\\n     Commercial Open Australian Legal Corpus\\tablefootnote{\\url{https://github.com/umarbutler/open-australian-legal-corpus-creator}} & $0.5$B \\\\\n     EU Legislation\\tablefootnote{Scraped from \\url{https://eur-lex.europa.eu/homepage.html}} & $315$M \\\\\n     UK Legislation\\tablefootnote{\\url{https://www.legislation.gov.uk/}} & $190$M \\\\\n     Court Transcripts\\tablefootnote{Obtained from CourtListener: \\url{https://www.courtlistener.com/}. We use Whisper \\citep{radford2022robust} to transcribe the audio files.} & $350$M \\\\\n     UPSTO\\tablefootnote{\\url{https://bulkdata.uspto.gov/}} & $4.7$B \\\\\n     Total & $94$B \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{\\textbf{Sources of Legal Pretraining Data.} These sources contain noise and heavily duplicated documents, which we filtered and deduplicated, resulting in a 30 billion tokens dataset.}\n    \\label{tab:data-sources}\n\\end{table}\n\nThere is quite a lot of overlap between the different sources, and we run very aggressive cleaning and deduplication steps, described in \\Cref{sec:data_cleaning}.\n\n\\paragraph{Replay Sources}\nTo reduce the risk of catastrophic forgetting \\citep{MCCLOSKEY1989109} during continued pretraining, we incorporate data from the prior training distribution, following prior literature \\cite{chen2023meditron,sun2020distill}. However, since the training data for Mistral is undisclosed, we introduce commonly available ``general'' data from Wikipedia, StackExchange, and GitHub, comprising roughly $2\\%$ of the final training mix. These datasets are sampled from SlimPajama \\cite{shen2023slimpajama,together2023redpajama,soboleva2023slimpajama}.\n\n\\paragraph{Instruction Sources}\nAdditionally, we found it beneficial to include conversational data during pretraining. This is inspired by recent advances in neural machine translation, which highlight that the robust capabilities of LLMs in translation are due to the existence of accidental parallel data in the training corpus \\cite{anil2023palm,briakou2023searching}. Specifically, this means that we include the Super Natural Instruction \\cite{wang2022super} and FLAN collection \\cite{longpre2023flan} during pretraining.\n\n\\subsubsection{Data Cleaning}\n\\label{sec:data_cleaning}\n\nA significant fraction of the collected data is either in PDF files or is text extracted from PDFs\\footnote{We used \\href{https://poppler.freedesktop.org/}{Poppler} for text extraction from PDF files.}. This means that the text has some artifacts, including i) page numbers in the middle of sentences; ii) line numbers; iii) non-normalized unicode characters; iv) broken lines of text; v) repeated characters: new lines, dashes, etc; vi) other artifacts. We addressed these issues using a combination of rules and heuristics to filter the data.\n\n\\paragraph{Text Normalization}\nWe normalize all unicode with the NFKC method, available through the \\texttt{unicodedata} Python package.\n\n\\paragraph{Rule filters}\nFollowing \\citet{elazar2023whats}, we found the most common 10-grams in our dataset and used regular expressions to remove the undesired ones, which were mostly repeated characters. Concretely, $8$ of the top $10$ 10-grams in the original data were repeated characters, eg: ``\\texttt{- - - - - - - - - -}'', ``\\texttt{. . . . . . . . . .}'', or ``\\texttt{* * * * * * * * * *}'', and weird characters, ie encoding issues. Additionally, we removed repeated whitespace (spaces, new lines, and tabs), as well as any HTML tag that made it through our pipeline.\n\n\\paragraph{Perplexity filtering}\nWe trained a KenLM model \\citep{heafield-2011-kenlm} on a small subset of carefully inspected legal data, and used it to filter any high perplexity paragraph. This removed non-English text as well as most of the ``weird'' unicode sequences present in the data. We show some of the most common $10$-grams in the filtered data on \\Cref{tab:common-ngrams}.\n\n\\begin{table}[ht]\n    \\scriptsize\n    \\centering\n    \\begin{tabular}{c}\n    \\toprule              \n    Common 10-grams \\\\\n    \\midrule\n    \\texttt{have been obvious to one of ordinary skill in the} \\\\\n    \\texttt{before the effective filing date of the claimed invention to} \\\\\n    \\texttt{rejected under 35 U.S.C . 103 as being unpatentable over} \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{\\textbf{Most common 10-grams} in the pretraining dataset.}\n    \\label{tab:common-ngrams}\n\\end{table}\n\n\\subsubsection{Data Deduplication}\nInspired by \\citet{kocetkov2023the,lee2021deduplicating}, we removed duplicates and near-duplicates from the training data using \\citet{chenghao_mou_2023_8364980}, with default parameters, after which we were left with roughly $30$B tokens of high-quality text.\n\n\\subsection{Instruction Finetuning Mixes}\nInstruction fine-tuning is crucial for getting the best performance out of the pre-trained decoder models across different tasks. We use a mix of general and legal instructions to train the model to understand and follow instructions well, with a focus on legal expertise. \n\n\\paragraph{General Instructions}\nWhen it comes to general instructions, we gather them from four primary sources:\n\\begin{enumerate}\n    \\item \\textbf{SlimOrca} This subset of the FLAN collection comprises generic instructions, offering a focused resource for various tasks \\cite{mukherjee2023orca,SlimOrca}.\n    \\item \\textbf{Meta Math Question Answering Instructions} Designed for mathematical inquiry, this dataset\\footnote{Accessible at \\url{meta-math/MetaMathQA}} presents a range of mathematical questions, facilitating research in math-based natural language processing \\cite{yu2023metamath}.\n    \\item \\textbf{General Conversations from UltraChat} Capturing diverse conversational contexts, this GPT-derived dataset contributes to enhancing natural language understanding and generation systems \\cite{ding2023enhancing}.\n    \\item \\textbf{Code Instructions from Glaive Code Assistant v2\\footnote{Available at \\url{https://huggingface.co/datasets/glaiveai/glaive-code-assistant-v2}}} Training on code has been shown to increase the reasoning ability of models \\citep{ma2023training}\n\\end{enumerate}\n\nWe meticulously filter, deduplicate, and curate all this data, resulting in a refined dataset comprising $600$K instructions.\n \n\\paragraph{Legal Instruction Construction}\nWe synthetically generate comprehensive conversations addressing fundamental legal competencies across multiple legal document types \\cite{ding2023enhancing}. We leverage a \\texttt{Mistral-7B-instruct} to transform legal texts augmented with metadata into coherent conversations.\nThe methodology involves initiating the conversation with $3$ predefined turns: (1) the user articulates a request related to the legal document, (2) the assistant responds by rephrasing the metadata (e.g., document type, date, name of a judge), and (3) the user prompts the assistant to elaborate on its reasoning. Subsequently, we extend the conversation through a series of turns, where a user model progressively poses more specific questions to grasp the assistant's reasoning. Simultaneously, an assistant model provides in-depth insights. An illustrative example is presented in \\Cref{fig:ift_generation}. Notably, we ensure the exclusion of the test set from existing benchmarks.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/convo.pdf}\n    \\caption{\\textbf{Turning dataset with metadata into a conversation.} Taking the example of Reddit post classification, we turn a labeled example \\{\"\\textit{My employer fired me because \\dots Is it legal?}\", \"\\textit{employment}\" \\}, we hard-code the first three turns of the conversation by simply reformulating the query and answer as a natural conversation. We then complete the conversation using a \\textit{user} model(blue dashed), whose task is to continue generating relevant questions from the ongoing conversation, and an \\textit{assistant} model that provides answers. Both \\textit{assistant} and \\textit{user} models are \\texttt{Mistral-7B-instruct}.}\n    \\label{fig:ift_generation}\n\\end{figure}\n\n\\section{Evaluation of Legal Knowledge}\n\\label{sec:legal_bench_instruct}\n\nTo evaluate the model's legal abilities, we use $3$ benchmarks (i) we compare the perplexity of the backbones on $5$ types of legal documents, (ii) we enhance LegalBench with \\legalbench{} for deeper evaluation, (iii) we rely on the legal section of MMLU for additional insights.\n\n\\paragraph{Perplexity Measurement}\nTo evaluate the adaptability of the backbones to legal documents, we assess perplexity using benchmark datasets spanning four distinct legal domains: \\emph{contracts, judicial decisions, opinion text, and legislation}. We ensure that the datasets are up-to-date, and sourced after the collection cut-off date from LLM data. Specifically, contract data is sourced from EDGAR (first quarter of 2024), legal decisions from ICSID court decisions published after October 2023, legislation focuses on US bills submitted before the House or Senate after October 2023, and party submissions include Texas briefs submitted after October 2023.\n\nDuring our investigations, we found a significant limitation in the original prompts of LegalBench. The complex nature of these prompts, combined with the challenges encountered by open source LLMs in adhering to instructions - particularly in handling formatting - leads to a substantial drop in performance (as measured by accuracy). The generated sentences are often verbose and difficult to parse, rendering LegalBench in its current form too stringent and failing to accurately gauge improvement on the task. \n\nFor example, in some of the tasks, performance is evaluated by the first word the model predicts, and this word is expected to be a \\emph{Yes/No}. This means that if the response is a bit verbose it will be counted as incorrect, even if a human would classify it as a correct answer. To remedy this shortcoming, we refine the prompts by 1) removing distracting few-shot examples and 2) concluding with a specific instruction for the model to generate tags (see \\Cref{tab:legalbench}).\n\n\\begin{table}[ht]\n    \\scriptsize\n    \\centering\n    \\begin{tabular}{p{7cm}}\n    \\toprule              \n    \\textbf{Original Prompt} \\\\\n    \\midrule\n    The Telemarketing Sales Rule is provided by 16 C.F.R. § 310.3(a)(1) and 16 C.F.R. § 310.3(a)(2).\n    \n\\\\\\textbf{Question:} Acme Toys is a telemarketer subject to the Telemarketing Sales Rule. Acme Toys told a customer that its frisbees cost \\$10 each, when in fact the frisbees cost \\$12 each. The customer agreed to the sale and was charged \\$12. Is this a violation of the Telemarketing Sales Rule?\n\\\\\\textbf{Answer:} Yes\n\n\\\\\\textbf{Question:} Acme Toys is a telemarketer subject to the Telemarketing Sales Rule. Acme Toys told a customer that its frisbees cost \\$10 each, when in fact the frisbees did cost \\$10, but Acme Toys did not disclose that shipping would cost an additional \\$5. The customer agreed to the sale. Is this a violation of the Telemarketing Sales Rule?\n\\\\\\textbf{Answer:} Yes\n\n\\\\\\textbf{Question:} Acme Industrial Products is a telemarketer subject to the Telemarketing Sales Rule. Acme Industrial Products told a customer that its brooms cost \\$12 each, and the brooms did in fact cost \\$12. The customer agreed to the sale. Is this a violation of the Telemarketing Sales Rule?\n\\\\\\textbf{Answer:} No\n\n\\\\\\textbf{Question:} Acme Industrial Products is a telemarketer subject to the Telemarketing Sales Rule. Acme Industrial Products told a customer that it would sell them 4 brooms for \\$10 and that shipping would be \\$5. Then, the customer agreed to the sale. Is this a violation of the Telemarketing Sales Rule?\n\\\\\\textbf{Answer:} No\n\n\\\\\\textbf{Question:}  \\{{text\\}}\n\\\\\\textbf{Answer:}\\\\\n\\\\\\hline           \n\\textbf{Curated Prompt} (Ours) \\\\\\hline \n\nThe Telemarketing Sales Rule is provided by 16 C.F.R. § 310.3(a)(1) and 16 C.F.R. § 310.3(a)(2).\n\nAnswer the following question: \\{text\\}\n\n\\textit{Answer by only outputting \"Yes\" or \"No\"} \\\\\n\\bottomrule\n\\end{tabular}\n    \\caption{\\textbf{Example from \\legalbench{}}. We manually curated and corrected typos, removing a few short examples from LegalBench as they were found to distract LLMs of size 7B.\\vspace{-.5cm}}\n    \\label{tab:legalbench}\n\\end{table} \n\n\\paragraph{Massive Multitask Language Understanding (MMLU)} The MMLU benchmark \\citep{hendrycks2020measuring} has been widely employed to gauge the advances in LLM performance. In our study, we center our analysis on the legal domain, with a specific focus on:\n\\textit{international law}, \\textit{professional law}, and \\textit{jurisprudence}. Those tasks respectively contain $120$, $1500$, and $110$ examples.\n\n\\iffalse\n \\paragraph{Benchmark:} We present results on two legal benchmarks: \\legalbench{} already presented in \\autoref{sec:legal_bench_instruct}, comprising 165 legal tasks, with each task comprising from merely 50 to more than 10'000 samples. We refer to the original paper \\cite{guha2023legalbench} for detailed statistics. To confirm findings, we also evaluate models on three legal tasks from the \\textit{Massive Multitask Language Understanding} (MMLU) benchmark \\cite{hendrycks2020measuring}: \n\\textit{international law}, \\textit{professional law} and \\textit{jurisprudence}. Those tasks respectively contain 120, 1500, and 110 samples. We coin this benchmark \\legalmmlu{}.\n\\fi \n\n\\subsection{Metrics}\nWe use the same metric as the original LegalBench \\cite{guha2023legalbench} paper: balanced accuracy. Balanced accuracy allows for handling better-imbalanced classification tasks, such as the ones presented in both benchmarks. We also use balanced accuracy for the legal tasks of MMLU. Unless otherwise noted, any score reported throughout this section refers to the balanced accuracy.\n\n\\section{Experimental Setting}\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.9\\linewidth]{figures/backbones.pdf}\n    \\caption{\\textbf{Performance of base models on \\legalbench{}.} Interestingly, although not instruction fine-tuned, \\ourmodel{} is still able to achieve impressive improvements on the benchmark, compared to other base models, including \\ourmodel's initial checkpoint (Mistral-7B).}\n    \\label{fig:raw_backbones}\n\\end{figure}\n\\subsection{Baselines}\nWe compare the \\ourmodel{} family to other state-of-the-art $7$B and $13$B open-source models. Concretely, we include the following instruction and DPO finetuned variants of Mistral-7B \\citep{jiang2023mistral}: \\texttt{Mistral-7B-Instruct-v0.1}%\\footnote{\\url{https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1}}\n, \\texttt{Mistral-7B-Instruct-v0.2}\n, as well as \\texttt{zephyr-7b-beta}\\footnote{\\url{https://huggingface.co/HuggingFaceH4/zephyr-7b-beta}}. We also evaluate the Llama2 \\citep{touvron2023llama} family, more specifically \\texttt{Llama2-7b-Chat}%\\footnote{\\url{https://huggingface.co/meta-llama/Llama-2-7b-chat-hf}} \nand \\texttt{Llama2-13b-Chat}.%\\footnote{\\url{https://huggingface.co/meta-llama/Llama-2-13b-chat-hf}}.\n    \n\\subsection{Implementation Details}\n\n\\paragraph{Codebase} Our codebase relies on open-source frameworks \\cite{shoeybi2019megatron,wolf2019huggingface,lhoest2021datasets} utilizing DeepSpeed (level 3) with Flash attention \\cite{dao2022flashattention,dao2023flashattention}. It is built on PyTorch \\cite{paszke2019pytorch}, and our models are available on the Huggingface hub.\n\n\\paragraph{Compute} Continuous pretraining utilizes $256$ MI250 AMD GPUs. For instruction fine-tuning, workload distribution occurs across 16 MI250. Evaluation procedures are seamlessly conducted on a single MI250.\n\n\\section{Results}\n\nIn this section, we discuss our main experimental findings and results.\n\n\\subsection{\\legalbench{}}\n\\Cref{fig:raw_backbones,fig:ablation_backbones} summarize our results on \\legalbench{}. There are $3$ main takeaways, which we discuss below.\n\n\\paragraph{I. Legal continued pretraining brings significant improvements} We start by analyzing the impact of our proposed continued pretraining. As seen on \\Cref{fig:raw_backbones}, \\ourmodel{} is a strong standalone model. We speculate that its strong performance is largely due to the integration of instructions in the pre-training data, as mentioned in \\autoref{sec:dataset_composition}. Nevertheless, we still note that even without a dedicated instruction fine-tuning stage, \\ourmodel{} performs on par with \\texttt{Llama2-7B-chat} ($0.38$ v.s. $0.39$). More importantly, \\ourmodel{} serves as a strong base model for building IFT models with strong legal capabilities. When combined with Generic instruction finetuning, as seen on \\Cref{fig:ablation_backbones}, it achieves a strong average of $0.59$, i.e. $4$ absolute points of improvement with respect to the best open-source instruct model \\texttt{Mistral-7B-Instruct-v0.1}.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.7\\linewidth]{figures/ablation_backbone.pdf}\n    \\caption{\\textbf{Influence of the base model.} Starting the instruction finetuning from our base model \\ourmodel{} brings noticeable improvements compared to the Mistral-7B. Indeed, even with a generic IFT mix (without legal), \\ourmodel{} (Gen.) outperforms its Mistral-Instruct counterpart significantly. Adding legal instructions to the IFT mix further boosts the results.} \n    \\label{fig:ablation_backbones}\n\\end{figure}\n\n\\paragraph{II. Legal instruction finetuning further boosts the results} As seen on \\Cref{fig:ift_generation}, finetuning \\ourmodel{} on both general and legal instructions (\\ourmodelift{}) establishes a new state-of-the-art on the \\legalbench{} benchmark, with an average score of $0.61$, i.e. an $11$\\% relative improvement compared to the best open-source instruct model (\\Cref{fig:ift_comparison}. \nFinally, DPO-aligned models tend to underperform their instruction-tuned counterparts, which could be explained by the fact that generic alignment is not suited for out-of-distribution tasks, such as the ones present in \\legalbench{}. Although beyond the scope of the present work, an interesting research direction would be to explore how legal-specific DPO can help.\n\n\\begin{figure}\n    \\centering    \\includegraphics[width=\\linewidth]{figures/ift.pdf}\n    \\caption{\\textbf{Comparison of instruct models on \\legalbench{}}. \\ourmodelift{} establishes the state-of-the-art, outperforming the best Mistral-Instruct model by a significant 6 absolute points.}\n    \\label{fig:ift_comparison}\n\\end{figure}\n\n\\begin{figure}[!h]\n        \\centering\n        \\includegraphics[width=0.6\\linewidth]{figures/mmlu.pdf}\n        \\caption{\\textbf{Instruct models on \\legalmmlu{}.} Echoing finding on \\legalbench{}, \\ourmodelift{} displays superior performance on all three tasks of \\legalmmlu{}, with an average absolute improvement of 5 points with respect to \\mistralinstructone. }\n        \\label{fig:mmlu_benchmark}\n\\end{figure}\n\\paragraph{III. There is still room for significant improvement.} Next, we follow the original LegalBench taxonomy \\citep{guha2023legalbench} to gain a more granular understanding of \\ourmodelift{}'s performance, by partitioning the tasks into $5$ core legal abilities: \\textsc{Issue Spotting}, \\textsc{Rule-Recall}, \\textsc{Interpretation}, \\textsc{Rhetoric Understanding}, and \\textsc{Rule-Conclusion}. Results show an interesting trend (\\Cref{fig:spider_chat}): \\ourmodelift{} shows clear superior performance over the best non-legal competitor \\texttt{Mistral-7B-Instruct-v0.1} on the four areas that require the most legal expertise, i.e. \\textsc{Issue}, \\textsc{Rule}, \\textsc{Interpretation} and \\textsc{Understanding}. On the other hand, it falls short of \\texttt{Mistral-7B-Instruct-v0.1} on the \\textsc{Conclusion} tasks, which interestingly require much more pure deductive reasoning than actual legal knowledge. We speculate that augmenting our pretraining and fine-tuning corpora with more deductive reasoning content, including but not limited to mathematics datasets could reduce the gap and fully unlock the potential of \\ourmodelift{}.\n\n\\begin{figure}[!h]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/diagram_perf.pdf}\n    \\caption{\\textbf{Per-task performance breakdown.} \\ourmodelift{} largely outperforms generic Instruct models on tasks that most require legal-specific knowledge, but is outperformed by Mistral-Instruct on the conclusion tasks, which necessitates more deductive reasoning.}\n    \\label{fig:spider_chat}\n\\end{figure}\n\n\\subsection{Results on \\legalmmlu{}}\n\nTo confirm our observations on \\legalbench{}, we analyze the results on \\legalmmlu{} shown in \\Cref{fig:mmlu_benchmark}. Again, \\ourmodelift{} exhibits consistent superiority over non-legal instruction-tuned models, with a gap between $3$ and $4$ absolute points to the best 7B open-source competitor across the three tasks, providing additional evidence that \\ourmodelift{} is as a strong foundation to build models tailored to legal workflows.\n\\begin{figure}[h]\n        \\centering\n        \\includegraphics[width=1\\linewidth]{figures/perplexity_per_category.pdf}\n        \\caption{\\textbf{Perplexity on legal documents for pretrained backbones.} \\ourmodelift{} outperforms other pretrained backbones on most types of legal documents, but is outperformed by \\texttt{Llama2-7b} on Legislation.\n        \\ourmodelift{} exhibits a median perplexity of $8.69$, having a reduction of $5.5$ percent compared to \\texttt{Mistral-7B}, $9.20$, and $10.8$ percent compared to \\texttt{Llama2-7B}, with a median perplexity of $9.74$.\n        }\n        \\label{fig:perplexity_analysis}\n\\end{figure}\n\n\\subsection{Perplexity Analysis} \nTo assess the adaptation of \\ourmodel{} backbone to the legal domain, we present perplexity scores across four document types: contracts, legal decisions, legislation, and party submissions. Refer to \\autoref{fig:perplexity_analysis} for the results. Our model, \\ourmodel{}, consistently outperforms \\texttt{Mistral-7B} across all categories, exhibiting lower average perplexity scores with reduced variance. Interestingly, \\texttt{Llama2-7B} demonstrates lower perplexity specifically in legislation documents, suggesting a potentially higher proportion of legislative text in the pertaining corpora compared to \\texttt{Mistral-7B}.\n\nOverall, compared to \\texttt{Mistral-7B}, our model shows a median perplexity reduction of 3 percent across legal corpora and 11 percent when compared to \\texttt{Llama2-7B}.\n\n    \n\\section{Conclusion \\& Future Perspectives}\n\nIn this paper, we introduce \\ourmodel{}, an open-source decoder model delivering state-of-the-art performance, compared to 7B models, within the legal domain. Our approach entails fine-tuning legal data alongside instruction fine-tuning on synthetic datasets. Additionally, we contribute by providing a cleaned version of LegalBench and introducing a new set of documents for perplexity measurement. We hope that our model, which is released under the MIT license, will contribute to the open-source ecosystem and the community.\n\n\\section*{Acknowledgments}\nWe thank GENCI for generously granting us access to their cutting-edge computing resources. Our model, \\ourmodel{}, has been trained on ADASTRA, with initial experimentation conducted on Jeanzay. The utilization of HPC resources was made possible through the Jeanzay grants 101838, 103256, and 103298, as well as the Adastra grants C1615122, CAD14770, and CAD15031.\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{\\algo: A Chinese Legal Knowledge-Enhanced Large Language Model}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\n  Large language models (LLMs), including both proprietary and open-source models, have showcased remarkable capabilities in addressing a wide range of downstream tasks. \n  Nonetheless, when it comes to practical Chinese legal tasks, these models fail to meet the actual requirements. \n  Proprietary models do not ensure data privacy for sensitive legal cases, while open-source models demonstrate unsatisfactory performance due to their lack of legal knowledge. \n  To address this problem, we introduce \\algo, the first open-source model specifically designed for Chinese legal applications. \n  \\algo comprises two key components: legal-oriented pre-training and legal supervised fine-tuning. \n  Specifically, we employ large-scale Chinese legal documents for legal-oriented pre-training to incorporate legal domain knowledge. \n  To further improve the model's performance on downstream legal tasks, we create a knowledge-driven instruction dataset for legal supervised fine-tuning. \n  Our experimental results demonstrate that \\algo outperforms the open-source LLaMA 7B model. \n  Our code and resources are publicly available at \\url{https://github.com/pengxiao-song/LaWGPT} and have received 5.7K stars on GitHub.\n\\end{abstract}\n\n\\section{Introduction}\n\\label{sec:introduction}\n\nLarge language models (LLMs)~\\citep{GPT4,hugo23LLaMA2} have achieved remarkable success in various natural language processing (NLP) tasks, including natural language understanding~\\citep{dong19understanding}, reasoning~\\citep{DBLP:conf/acl/0009C23}, and generation~\\citep{yu22text}. \nBoth proprietary and open-source LLMs exhibit strong generalization capabilities, enabling their application in diverse downstream scenarios, such as medicine~\\citep{th23medicine}, finance~\\citep{DBLP:journals/corr/abs-2306-06031}, education~\\citep{DBLP:conf/bigdataconf/GanQWL23}. \nRecent studies~\\citep{li2023LawBench,nguyen2023LawGPT} have demonstrated the preliminary effectiveness of existing general LLMs in legal tasks, including legal judgment prediction~\\citep{luo2017legalpredict}, legal documents retrieval~\\citep{chen2013textmining}, and legal question answering~\\citep{zhong2020nlpbenefit}. \n\nDespite the preliminary effectiveness of LLMs in legal applications, there are two obstacles that hinder their practical use in legal tasks. \nOn the one hand, proprietary LLMs such as GPT-4~\\citep{GPT4} and GPT-3.5 Turbo~\\citep{GPT35T} can only be accessed through APIs, which do not guarantee data privacy in sensitive legal cases. \nOn the other hand, open-source LLMs like LLaMA~\\citep{hugo23LLaMA} and ChatGLM~\\citep{du2022glm} fail to achieve satisfactory performance due to their insufficient legal knowledge and incompatibility with downstream legal tasks. \nTherefore, it is necessary to develop a open-source LLM specifically designed for legal applications  in order to overcome the existing obstacles. \n\nIn this paper, we introduce \\algo, the first open-source Chinese legal knowledge-enhanced large language model. With the advantage of being open-source, \\algo can be self-hosted and accessed privately to ensure data privacy, as compared to proprietary models. We then present legal-oriented pre-training, which utilizes our large-scale legal pre-training corpus to incorporate domain-specific legal knowledge into \\algo, improving its understanding, reasoning, and generation foundational capabilities in legal tasks. \nAdditionally, we propose legal supervised fine-tuning, employing our knowledge-driven instruction dataset to further enhance \\algo's performance on downstream legal tasks. \nExperimental results demonstrate that \\algo surpasses the open-source LLaMA 7B model in major legal tasks, shedding light on the development of a practical Chinese legal LLM. \n\nIn summary, our contributions can be summarized as follows: \n\\begin{enumerate}[(a)]\n    \\item We present the first open-source Chinese legal knowledge-enhanced large language model \\algo. The code and model are available on GitHub~\\footnote{\\url{https://github.com/pengxiao-song/LaWGPT}} and have received 5.7K stars.\n    \\item We construct a comprehensive legal pre-training corpus and propose a legal-oriented pre-training approach to enhance \\algo's foundational abilities in legal tasks by integrating domain-specific knowledge.\n    \\item We create a knowledge-driven instruction dataset and utilize legal supervised fine-tuning to further adapt \\algo to various legal tasks and improve its downstream performance.\n    \\item Our experimental results demonstrate that \\algo achieves better performance than the open-source LLaMA 7B model across major legal tasks, providing strong evidence for the effectiveness of our proposed model.\n\\end{enumerate}\n\n\\section{Related Work}\n\\label{sec:relatedwork}\n\nIn this section, we review the existing work on addressing legal tasks using LLMs. This focus is on general language models, legal language models, and legal benchmarks as follows. \n\n\\subsection{General Language Models}\n\nBenefiting from training with large scale corpus, recent LLMs have shown impressive performance on various kind of downstream tasks, including legal tasks. Recent LLMs, trained on extensive corpora, have demonstrated impressive performance across a variety of downstream tasks, including tasks in the legal domain. \nProprietary LLMs, such as GPT-4~\\citep{GPT4}, GPT-3.5-Turbo~\\citep{GPT35T}, PaLM~\\citep{Chowdhery23PaLM}, and PaLM2~\\citep{Rohan23PaLM2}, exhibit strong capabilities in handling legal tasks. \nTheir impressive performance not only demonstrates the potential of LLMs in addressing legal tasks but also facilitates the low-cost, automated construction of high-quality datasets.\nConcurrently, open-source LLMs, such as LLaMA~\\citep{hugo23LLaMA}, LLaMA2~\\citep{hugo23LLaMA2}, MPT~\\citep{MosaicML2023mtp7B}, ChatGLM 2~\\citep{du2022glm}, and Baichuan 2~\\citep{aiyun23baichuan}, are available in various model scales. These open-source models facilitate the fine-tuning of legal-specific models using targeted legal datasets, potentially enhancing performance.\n\n\\subsection{Legal Language Models}\n\nThe legal language models are fine-tuning based on pre-trained language models or trained from scratch using legal related data for improving legal capabilities of models. \nEarly research in this field utilized a model architecture with millions of parameters for various legal tasks. \nExtensive efforts have been made to address these tasks separately. These include legal judgment prediction~\\citep{luo2017legalpredict, chalkidis19legalpredict, yang2019legalpredict}, legal documents and cases retrieval~\\citep{chen2013textmining, shao2020bertpli, li2023SAILER}, legal reading comprehension~\\citep{duan2019cjrc}, and legal question answering~\\citep{zhong2020nlpbenefit, phi2020legalqa}. \nWith the benefit of pre-trained models~\\citep{chalkidis2020legalbert, cui21roberta}, Lawformer~\\citep{xiao21lawformer} combines three attention mechanisms to address the problem of long legal documents, covering a variety of legal tasks. \nRecent advances of LLMs have given rise to legal LLMs work. \nHanFei~\\citep{zhang2023HanFei}, LexiLaw~\\citep{li2023SAILER}, LawGPT-zh~\\citep{liu2023lawgptzh}, and LawGPT-1.0~\\citep{nguyen2023LawGPT}  fine-tune foundational LLMs using a specially constructed or collected legal corpus to enhance their legal capabilities. \nTo tackle the hallucination problem in legal tasks, LLMs such as ChatLaw~\\citep{cui23chatlaw}, Wisdom-Interrogatory~\\citep{wu2024WisdomInterrogatory}, and Lawyer-LLaMA~\\citep{huang2023LawyerLLaMA} incorporate a legal data retrieval method to improve the robustness of their responses.\nLawGiBa~\\citep{nguyen2023LawGiBa}, based on the GPT-4 model, has established a legal system.\nFuzi-Mingcha~\\citep{deng2023fuzi} has created a legal syllogistic reasoning dataset for fine-tuning to ensure logical format and accurate reasoning results.\n\n\\subsection{Legal Benchmarks}\n\nWith the emergence of enormous language models for legal tasks, several benchmarks have been proposed to evaluate a variety of existing models.\nLawBench~\\citep{li2023LawBench} collects 20 legal tasks within three cognitive levels, i.e., legal knowledge memorization, understanding, and applying, to thoroughly evaluate the performance of existing models. LAiW~\\citep{dai2023LAiw} contains 14 tasks to evaluate the legal capabilities of LLMs from three levels, i.e., basic information retrieval, legal foundation inference, and complex legal application.\nSimuCourt~\\citep{he2024simucourt} introduces a judicial decision-making task to evaluate the judicial analysis and decision-making power of LLMs.\n\n\\section{Methodology}\n\\label{sec:methodology}\n\n\\begin{figure}[t]\n    \\begin{definedbox}[label=ex:pre-training-corpus]{Legal Pre-training Corpus}\n        \\begin{CJK*}{UTF8}{gbsn}\n        上诉人*********（以下简称**学校）因与被上诉人************（以下简称**公司）装饰装修合同纠纷一案，不服*********人民法院（20xx）辽****民初****号民事判决，向本院提起上诉。本院依法组成合议庭审理了本案。本院认为 本院认为，一审判决程序违法。\n        1.**学校一审反诉请求解除装修合同及空调合同，一审仅判决解除装修合同，空调合同应否解除未予审理，属漏审漏判；一审双方当事人均未提出解除案涉补充协议，一审判决解除补充协议，超出当事人的诉请；\n        2.**学校一审反诉请求要求**公司按已付工程款数额开具发票，一审仅判决**公司给付欠付款项的发票，亦属漏审漏判；\n        3.**公司起诉状中明确了监控布线款为15600元，在**公司未提出变更诉讼请求的情况下，一审按照鉴定结论中的数额认定监控布线损失为32868.85元，亦属超出当事人的诉请。 \n        一审判决认定事实不清。案涉工程两个施工合同均为固定总价合同，一审判决**公司给付**学校欠付工程款258449.56元依据的鉴定结论计算方式错误，且装修合同和空调合同应分别计算。另外，案涉已完工程造价鉴定中是否包含了已施工与图纸不符、质量不合格修复部位的工程造价？已施工与图纸不符、质量不合格修复费用的鉴定结论是否为修复到施工前的原始状态？重审时需补充鉴定。 一审重审时应围绕双方当事人的诉讼请求，合理分配举证责任，在查清事实的基础上依法裁判。 综上，依照《中华人民共和国民事诉讼法》第一百七十七条第一款第（三）、（四）项之规定，裁定如下：判决结果 一、撤销*********人民法院作出的（20xx）辽****民初****号民事判决； 二、本案发回*********人民法院重审。 上诉人*********学校预交的二审案件受理费6579元予以退回。\\end{CJK*}\n    \\end{definedbox}\n    \\vskip -0.1in\n\\end{figure}\n\n\\begin{figure}[t]\n    \\begin{definedbox}[label=ex:sft-corpus-a]{Legal-supervised Dataset - Subset (a)}\n        \\begin{CJK*}{UTF8}{gbsn}\n        \\textbf{Instruction:} 请问我向借钱人要钱多次未果，向法院起诉，法院多久才立案\n        \\tcbline\n        \\textbf{Output:} 起诉的当日 ，法院就会立案的。\\end{CJK*}\n    \\end{definedbox}\n    \\vskip -0.1in\n\\end{figure}\n\n\\begin{figure}[t]\n    \\begin{definedbox}[label=ex:sft-corpus-b]{Legal-supervised Dataset - Subset (b)}\n        \\begin{CJK*}{UTF8}{gbsn}\n        \\textbf{Instruction:} 根据《中华人民共和国海商法》，在海事关系的法律适用中，旗国法适用于下列哪些情形? (A) 船舶抵押权的设定 (B) 同国籍船舶在公海发生碰撞的损害赔偿 (C) 共同海损理算 (D) 海事赔偿责任限制.\n        \\tcbline\n        \\textbf{Output:} (A) 船舶抵押权的设定(B) 同国籍船舶在公海发生碰撞的损害赔偿\n        \\end{CJK*}\n    \\end{definedbox}\n    \\vskip -0.1in\n\\end{figure}\n\n\\begin{figure}[t]\n    \\begin{promptbox}[label=ex:sft-corpus-c]{Prompt of ChatGPT for Augmentation}\n        \\begin{CJK*}{UTF8}{gbsn}\n        我希望你担任语言专家的角色。我会给你一段与法律问答文本，请你使用正式的文风润色它。要求：$\\backslash$n\n            1. 修正语法错误、标点符号错误，去掉特殊符号，必须使语句更通顺。\n            2. 使逻辑更清晰、格式更规范，比如向<answer>中换行符。\n            3. 使更礼貌，比如向<question>中加入“请问”等礼貌用语。\n            4. 不要写任何解释性语句。\n            5. <question>应该是问题，<answer>应该是答案。\n            这段对话是：$\\backslash$n<question>:\\{instruction\\} $\\backslash$n<answer>:\\{output\\} $\\backslash$n$\\backslash$n\n        以JSON格式返回结果：\n        \\end{CJK*}\n    \\end{promptbox}\n    \\vskip -0.1in\n\\end{figure}\n\n\\begin{figure}[t]\n    \\begin{promptbox}[label=ex:alpaca]{Alpaca Training Template}\n        Below is an instruction that describes a task. Write a response that appropriately completes the request.$\\backslash$n$\\backslash$n \\#\\#\\# Instruction:$\\backslash$n\\{instruction\\}$\\backslash$n$\\backslash$n \\#\\#\\# Response: $\\backslash$n\\{output\\}\n    \\end{promptbox}\n    \\vskip -0.1in\n\\end{figure}\n\n\\begin{figure}[t]\n    \\begin{promptbox}[label=ex:alpaca-test]{Alpaca Testing Template}\n        Below is an instruction that describes a task. Write a response that appropriately completes the request.$\\backslash$n$\\backslash$n \\#\\#\\# Instruction:$\\backslash$n\\{instruction\\}$\\backslash$n$\\backslash$n \\#\\#\\# Response: $\\backslash$n\n    \\end{promptbox}\n    \\vskip -0.1in\n\\end{figure}\n\nIn this section, we introduce our \\algo, a large language model specifically designed for Chinese legal applications, aimed at effectively addressing various downstream legal tasks. \n\\algo addresses the two major challenges in applying existing open-source general LLMs to legal tasks:\n\\begin{enumerate}[(a)]\n    \\item The lack of legal domain knowledge in open-source general LLMs, which is crucial for performing legal tasks effectively;\n    \\item The insufficient training of open-source general LLMs on downstream legal tasks, resulting in suboptimal performance in legal applications.\n\\end{enumerate}\nWe apply legal-oriented pre-training to \\algo to incorporate legal domain knowledge within the open-source base model. \nThen, we conduct legal supervised fine-tuning to further enhance \\algo's performance on downstream legal tasks.\nEach component is elaborated as follows. % and the overall illustration of \\algo is shown in Figure~\\ref{fig:lawgpt}.\n\n\\subsection{Legal-Oriented Pre-Training}\n\nGeneral LLMs are typically pre-trained on large-scale general corpus, which may lack sufficient legal domain knowledge. Consequently, this can result in a limited understanding and reasoning ability for legal tasks. To address this limitation, we propose the integration of \\textbf{L}egal-oriented \\textbf{P}re-\\textbf{T}raining (\\text{LPT})  into \\algo, aiming to enhance its legal domain knowledge.\n\nTo incorporate legal domain knowledge into \\algo, we collect a large-scale legal pre-training corpus $\\mathcal{D}_{\\text{LPT}}$ consisting of 500K legal documents from various legal domains, including civil law, criminal law, and administrative law. \nExample~\\ref{ex:pre-training-corpus} presents a civil-law legal document from the legal pre-training corpus. For each legal document, the tokenizer of base model encodes the text into a token sequence $\\boldsymbol{x}=(x_0, x_1, \\ldots)$, and we perform legal-oriented pre-training on the base model $f_{\\Theta}(\\cdot)$ in an autoregressive manner using the following objective:\n\\begin{equation}\n\\mathcal{L}_{\\text{LPT}}(\\Theta, \\mathcal{D}_{\\text{LPT}}) = \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}_{\\text{LPT}}}\\left[ -\\sum\\limits_{i} \\log{f_{\\Theta}(x_i|x_0, x_1, \\ldots, x_{i-1})}\\right]\n\\end{equation}\nwhere $x_0, x_1, \\ldots, x_{i-1}$ denote the context tokens, $x_i$ denotes the target token, and $\\Theta$ is the parameters of base model $f_{\\Theta}(\\cdot)$. \nWe optimize the parameters of base model $\\Theta$ using $\\mathcal{L}_{\\text{LPT}}$ to obtain the parameters of legal-oriented pre-trained model $\\Theta^{\\text{LPT}}$.\n\n\\subsection{Legal-Supervised Fine-Tuning}\n\nAlthough $f_{\\Theta^{\\text{LPT}}}(\\cdot)$ has been pretrained with legal domain knowledge, it is not optimal for specific downstream legal tasks as it cannot generate the desired responses by following the instructions. \nTo address this issue, we propose \\textbf{L}egal-{S}upervised \\textbf{F}ine-\\textbf{T}uning (\\text{LFT}) to further adapt \\algo to various downstream legal tasks. \nSpecifically, we construct a 300K knowledge-driven instruction dataset, $\\mathcal{D}_{\\text{LFT}}$, consisting of three subsets: \n\\begin{enumerate}[(a)]\n    \\item An open-source dataset~\\footnote{https://github.com/liuhuanyong/CrimeKgAssistant} with 200K samples, which includes crime type prediction and crime consult tasks to fine-tune the model for better understanding of crime-related legal tasks and generating user-friendly responses; \n    \\item The JEC-QA dataset~\\citep{zhong20jecqa} with 20K samples, which consists of legal question answering tasks to fine-tune the model for better adaptation to legal downstream tasks;\n    \\item A constructed legal datasets with 80K samples by refining subsets (a) and (b) with ChatGPT~\\citep{GPT35T}, which augments more high-quality legal QA samples, thereby enhancing the generalizability of the model.\n\\end{enumerate}\nThe subsets (a) and (b) are shown in Examples~\\ref{ex:sft-corpus-a} and~\\ref{ex:sft-corpus-b}, respectively.  The subset (c) is reinfed using the prompt template in Template~\\ref{ex:sft-corpus-c} to augment the samples in subsets (a) and (b), where we replace <instruction> with real questions and <output> with the corresponding answer. \nWe adopt the Stanford Alpaca template~\\citep{taori23alpaca} in Template~\\ref{ex:alpaca} to wrap the instruction and output in our dataset.\nThen, the parameters of our pre-trained model $\\Theta^{\\text{LPT}}$ are fine-tuned on $\\mathcal{D}_{\\text{LFT}}$ using the following objective:\n\\begin{equation}\n    \\mathcal{L}_{\\text{LFT}}(\\Theta, \\mathcal{D}_{\\text{LFT}}) = \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}_{\\text{LFT}}}\\left[ -\\sum\\limits_{i\\in\\{\\text{output}\\}} \\log{f_{\\Theta^{\\text{LPT}}}(x_i | x_0, x_1, \\ldots, x_{i-1})} \\right]\n\\end{equation}\nwhere $\\Theta$ represents the optimized parameters, $\\boldsymbol{x} = (x_0, x_1, \\ldots)$ represents the tokenized input sequence drawn from dataset $\\mathcal{D}_{\\text{LFT}}$ and wrapped by Template~\\ref{ex:alpaca}, and $\\{\\text{output}\\}$ represents the index set of the output tokens. \nWe optimize the our pre-trained parameters $\\Theta^{\\text{LPT}}$ to obtain the parameters of \\algo $\\Theta^{\\text{LFT}}$.\n\n\\subsection{Inference of \\algo}\n\nWhen applying \\algo to downstream tasks, we should wrapped the instruction using the Alpaca template in Template~\\ref{ex:alpaca-test} and then tokenized the texts into $\\boldsymbol{x} = (x_0, x_1, \\ldots, x_n)$. \nThen, we feed the tokenized input sequence $\\boldsymbol{x}$ into the fine-tuned model $f_{\\Theta^{\\text{LFT}}}(\\cdot)$ to generate the response in an autoregressive manner. \n\n\\section{Experiments}\n\\label{sec:experiments}\n\n\\subsection{Implementation Details}\n\nWe trained \\algo using 8 NVIDIA V100 GPUs, based on the Chinese-Alpaca-Plus 7B base model~\\citep{chinese-llama-alpaca}, in two stages: legal-oriented pre-training, and legal-supervised fine-tuning.\nFor legal-oriented pre-training, we adopt our 500K legal pre-training corpus $\\mathcal{D}_{\\text{LPT}}$ to train the base model using the LoRA technique~\\citep{hu22lora}. We set the LoRA rank to 16, alpha to 32, and dropout to 0.05. The learning rate was set to 0.0003, the batch size to 128, and the training epoch to 1. \nFor legal-supervised fine-tuning, we adopt our 30K legal-supervised corpus $\\mathcal{D}_{\\text{LFT}}$ to fine-tune our pre-trained model with Alpaca template using the LoRA technique. We set the LoRA rank to 8, alpha to 16, and dropout to 0.05. We set the learning rate to 0.0003, the batch size to 64, and the training epoch to 20.\n\n\\subsection{Performance Evaluation}\n\n\\begin{table}[th]\n    \\caption{Performance comparison between \\algo, proprietary models including GPT-3.5 Turbo~\\citep{GPT35T} and GPT-4~\\citep{GPT4}, and 7B open-source model LLaMA~\\citep{hugo23LLaMA} on the zero-shot setting. The best performance among \\algo and open-source models is in bold.}\n    \\label{tab:zeroshot}\n    \\centering\n    \\begin{tabular}{l|rrrrrrrrrrrrrrrrrr|r}\n        \\toprule\n        \\multirow[l]{2}{*}{Models} & \\multicolumn{9}{c}{Tasks} \\\\ \\cmidrule{2-10}\n        & \\#1 & \\#2 & \\#3 & \\#4 & \\#5 & \\#6 & \\#7 & \\#8 & Avg. \\\\\n        \\midrule\n        \\rowcolor[gray]{.90} GPT-3.5 Turbo & 29.5 & 31.3 & 35.5 & 78.7 & 76.8 & 27.4 & 61.2 & 17.4 & 44.7 \\\\\n        \\rowcolor[gray]{.90} GPT-4 & 52.5 & 27.5 & 42.0 & 82.6 & 81.9 & 48.6 & 77.6 & 19.6 & 54.0 \\\\\n        \\midrule\n        LLaMA  & \\textbf{1.0} & 7.5 & 7.0 & 41.3 & \\textbf{54.2} & 0.2 & 14.4 & \\textbf{7.8} & 16.7 \\\\\n        LaWGPT & 0.2 & \\textbf{11.0} & \\textbf{15.7} & \\textbf{42.4} & 40.8 & \\textbf{6.2} & \\textbf{15.4} & 7.6 & \\textbf{17.4} \\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n\nIn this section, we conduct experiments to evaluate the performance of \\algo on 8 legal applications~\\citep{li2023LawBench}, including fact-based article prediction (\\#1), scene-based article prediction (\\#2), charge prediction (\\#3), prison term prediction without article (\\#4), prison term prediction with article (\\#5), case analysis (\\#6), criminal damages calculation (\\#7), and consultation (\\#8), in a zero-shot setting. \nWe compare the performance of \\algo with proprietary models including GPT-3.5 Turbo~\\citep{GPT35T} and GPT-4~\\citep{GPT4}, and 7B open-source models including LLaMA~\\citep{hugo23LLaMA}. The results are shown in Table~\\ref{tab:zeroshot}. \nThe results show that our \\algo outperforms LLaMA 7B model on major tasks and leading to a better average performance. \nDespite the advantage of preserving data privacy, there is still a significant performance gap between \\algo and proprietary models. This result inspires us and the following researchers to explore the potential of \\algo in the future work. \n\n\\section{Conclusion}\n\\label{sec:conclusion}\n\nIn this technical report, we introduce \\algo, a Chinese legal knowledge-enhanced large language model specifically designed for Chinese legal applications. We introduce the legal-oriented pre-training and legal supervised fine-tuning to incorporate legal domain knowledge and enhance the model's performance on downstream legal tasks, respectively. Our experimental results demonstrate that \\algo outperforms the open-source LLaMA 7B model.  \nWe hope this technical report and \\algo model can inspire future research on Chinese legal applications and contribute to the development of the legal AI community. \n\n\\newpage\n{\n\\small\n}\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{LawLLM: Law Large Language Model for the US Legal System}\n\n\\begin{document}\n\n\\title{LawLLM: Law Large Language Model for the US Legal System}\n\n\\author{Dong Shu}\n\\orcid{0009-0009-9785-3934}\n\\affiliation{%\n  \\institution{Northwestern University}\n  \\city{Evanston}\n  \\state{IL}\n  \\country{United States}}\n\\email{dongshu2024@u.northwestern.edu}\n\n\\author{Haoran Zhao}\n\\orcid{0000-0003-4563-9302}\n\\affiliation{%\n  \\institution{Northwestern University}\n  \\city{Evanston}\n  \\state{IL}\n  \\country{United States}\n}\n\\email{haoranzhao2024@u.northwestern.edu}\n\n\\author{Xukun Liu}\n\\orcid{0000-0002-4608-9448}\n\\affiliation{%\n  \\institution{Northwestern University}\n  \\city{Evanston}\n  \\state{IL}\n  \\country{United States}\n}\n\\email{xukunliu2025@u.northwestern.edu}\n\n\\author{David Demeter}\n\\orcid{0000-0001-7560-1132}\n\\affiliation{%\n  \\institution{Northwestern University}\n  \\city{Evanston}\n  \\state{IL}\n  \\country{United States}\n}\n\\email{ddemeter@u.northwestern.edu}\n\n\\author{Mengnan Du}\n\\orcid{0000-0002-1614-6069}\n\\affiliation{%\n  \\institution{New Jersey Institute of Technology}\n  \\city{Newark}\n  \\state{NJ}\n  \\country{United States}\n}\n\\email{mengnan.du@njit.edu}\n\n\\author{Yongfeng Zhang}\n\\orcid{0000-0003-2633-8555}\n\\affiliation{%\n  \\institution{Rutgers University}\n  \\city{New Brunswick}\n  \\state{NJ}\n  \\country{United States}\n}\n\\email{yongfeng.zhang@rutgers.edu}\n\n\\renewcommand{\\shortauthors}{Dong Shu et al.}\n\n\\begin{abstract}\nIn the rapidly evolving field of legal analytics, finding relevant cases and accurately predicting judicial outcomes are challenging because of the complexity of legal language, which often includes specialized terminology, complex syntax, and historical context. Moreover, the subtle distinctions between similar and precedent cases require a deep understanding of legal knowledge. Researchers often conflate these concepts, making it difficult to develop specialized techniques to effectively address these nuanced tasks. In this paper, we introduce the Law Large Language Model (\\sysname), a multi-task model specifically designed for the US legal domain to address these challenges. \\sysname excels at Similar Case Retrieval (SCR), Precedent Case Recommendation (PCR), and Legal Judgment Prediction (LJP). By clearly distinguishing between precedent and similar cases, we provide essential clarity, guiding future research in developing specialized strategies for these tasks. We propose customized data preprocessing techniques for each task that transform raw legal data into a trainable format. Furthermore, we also use techniques such as in-context learning (ICL) and advanced information retrieval methods in \\sysname. The evaluation results demonstrate that \\sysname consistently outperforms existing baselines in both zero-shot and few-shot scenarios, offering unparalleled multi-task capabilities and filling critical gaps in the legal domain. Code and data are available at \\url{https://github.com/Tizzzzy/Law_LLM}.\n\\end{abstract}\n\n\\begin{CCSXML}\n<ccs2012>\n   <concept>\n       <concept_id>10010405.10010455.10010458</concept_id>\n       <concept_desc>Applied computing~Law</concept_desc>\n       <concept_significance>500</concept_significance>\n       </concept>\n   <concept>\n       <concept_id>10010147.10010178.10010179</concept_id>\n       <concept_desc>Computing methodologies~Natural language processing</concept_desc>\n       <concept_significance>500</concept_significance>\n       </concept>\n   <concept>\n       <concept_id>10010147.10010257.10010258.10010262</concept_id>\n       <concept_desc>Computing methodologies~Multi-task learning</concept_desc>\n       <concept_significance>500</concept_significance>\n       </concept>\n   <concept>\n       <concept_id>10002951.10003317.10003338.10003346</concept_id>\n       <concept_desc>Information systems~Top-k retrieval in databases</concept_desc>\n       <concept_significance>500</concept_significance>\n       </concept>\n </ccs2012>\n\\end{CCSXML}\n\n\\ccsdesc[500]{Applied computing~Law}\n\\ccsdesc[500]{Computing methodologies~Natural language processing}\n\\ccsdesc[500]{Computing methodologies~Multi-task learning}\n\\ccsdesc[500]{Information systems~Top-k retrieval in databases}\n\n\\keywords{Large Language Models, Multitask Learning, Legal System, Natural Language Processing}\n\n\\maketitle\n\n\\section{Introduction}\nThe development of Large Language Models (LLMs) has led to significant progress in computational linguistics, particularly impacting fields like legal analytics. Given the nature of legal language, which includes complex terminologies and context-specific logical frameworks, LLMs offer unprecedented capabilities in this domain \\cite{naveed2023comprehensive}. The integration of LLMs into the legal field significantly boosts the efficiency of legal practitioners, such as lawyers and judges, by accurately interpreting their natural language input and generating most relevant responses. This reduces the need for extensive manual review of huge legal texts. Moreover, LLMs can provide lawyers with novel insights, revealing overlooked details and perspectives that can be critical in complex cases. Recent developments in legal domain have demonstrated the potential of LLMs in enhancing legal judgment predictions and handling various legal tasks effectively. For example, studies such as LM-CompEval-Legal \\cite{shui2023comprehensive} have systematically evaluated the effectiveness of LLMs, other projects like PLJP \\cite{wu2023precedent} and LoT \\cite{jiang2023legal} have focused on integrating domain-specific models and advancing LLMs' understanding of legal reasoning.\n\nAlthough these models have shown promise, there remain research challenges. First, these models generally address single-task challenges. In contrast, \\sysname innovatively supports multiple legal tasks simultaneously, providing a more nuanced analysis of complex legal datasets and filling a critical void in the field. Second, another controversial area in the legal domain is the difference between precedent cases and similar cases \\cite{qin2023incorporating}. Various models have been developed for precedent case recommendation, ranging from expert knowledge-based models to models based on natural language processing \\cite{mentzingen2023automation, cao2024pilot, ma2023caseencoder, lewis2021precedent}. These approaches typically convert legal text into embeddings and calculate similarity at the embedding level, which aids in precedent selection. However, we believe that this approach is more on identifying similar cases with textual and contextual similarities, not precedent cases.\n\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=\\linewidth]{samples/fig1.pdf}\n  \\caption{LawLLM supports three tasks: Similar Case Retrieval, Precedent Case Recommendation, and Legal Judgment Prediction.}\n  \\Description{overview of lawllm}\n  \\label{fig:intro}\n\\end{figure}\n\nIn our study, we emphasize the key differences between the two. Firstly, a precedent case must have been closed before the input legal case, ensuring its relevance and applicability to the current case under consideration. Secondly, precedent cases are those that were actually considered by judges in making their decisions, unlike similar cases which might not have been taken into account. Thirdly, similar cases share textual and thematic similarities in the case narrative or might fall into similar case categories, while precedent cases might seem unrelated at face value. It is also worth noting that while a legal case's precedent case can sometimes be the same as a similar case, this is not always the case. \n\nIn this paper, we introduce the Law Large Language Model (\\sysname), a multi-task LLM capable of Similar Case Retrieval (SCR), Precedent Case Recommendation (PCR) and Legal Judgment Prediction (LJP).\nTo build \\sysname, we finetune Gemma-7B \\cite{team2023gemini} using instruction tuning on United State real-life legal datasets and can perform LJP, PCR, and SCR tasks. The instructions for all three tasks fall under the classification category. By doing so, we can effectively minimize irrelevant and noisy options for the model, thereby improving its performance. We show the overall idea of \\sysname in Figure \\ref{fig:intro}. The development of \\sysname also includes three innovative preprocessing approaches for transforming unstructured legal data into a trainable format. More specifically, for LJP, we summarize and extract verdicts from raw datasets and apply zero and few shot In-context learning (ICL) \\cite{xie2021explanation, jin2024impact} technique to enhance the model performance. In PCR, \\sysname connects ground truth legal cases' precedent relationships as a Knowledge Graph (KG), treating each case as a unique entity linked by precedential connections \\cite{shu2024knowledge}. Additionally, the SCR task creates a legal case vector database and integrates advanced Information Retrieval (IR) techniques \\cite{ethayarajh2019contextual, jin2024health}.\n\nOur study presents \\sysname as a pioneering model in the realm of legal LLMs. Our key contributions are given as follows:\n\\begin{itemize}\n    \\item We propose \\sysname, which is adept at handling a range of legal tasks, including LJP, PCR, and SCR. This multi-task functionality is important in addressing the diverse requirements of the legal domain.\n        \n    \n    \\item \\sysname distinguishes between precedent cases and similar cases, providing clarity on the objectives of each task. This clarification enables the future research to develop tailored strategies for those tasks.\n\n    \\item Experimental results indicate that \\sysname outperformed all baseline models, including the GPT-4 model, across all three tasks. These results highlight \\sysname's robust capabilities in the legal domain. \n\n\\end{itemize}\n\n\\begin{figure*}\n  \\includegraphics[width=\\textwidth]{samples/overflow.pdf}\n  \\caption{An overview of our LawLLM: Data Preprocessing is in the upper left in \\textcolor{customgreen}{green}, Similar Case Retrieval Processing is in the upper right in \\textcolor{customyellow}{yellow}, Precedent Case Recommendation is in the lower left in \\textcolor{customred}{red}, and Legal Judgment Prediction is in the lower right in \\textcolor{customblue}{blue}.}\n  \\Description{work overflow of lawllm}\n  \\label{fig:overflow}\n\\end{figure*}\n\n\\section{Related Work}\n\nLegal AI is significantly increasing the efficiency and effectiveness of the legal community. AI technologies, specifically Large Language Models (LLMs), are leading the way in automating complex tasks like document analysis, case prediction, and legal research \\cite{zhong2020does, wu-etal-2023-precedent}. LLMs utilize advanced algorithms and data analytics to process and generate legal texts, which leads to significant improvements in speed and accuracy \\cite{zhou2023boosting}. In this section, we introduce the various applications of Legal AI and LLMs in legal practices.\n\n\\subsection{Precedent Case Recommendation}\nThe recommendation of precedent cases is a fundamental aspect of legal practice, as previous verdicts significantly affect current legal decisions. The field has evolved from early keyword-based searches and manual annotations to more complicated AI-driven models that improve retrieval efficiency and contextuality. \\citet{wu-etal-2023-precedent} proposed the Precedent-Enhanced Legal Judgment Prediction framework, which combines LLMs with domain-specific expertise to improve legal prediction accuracy significantly. \\citet{ma2023incorporating} developed the Structured Legal Case Retrieval system, which uses structural information from legal documents to improve case search precision and contextual relevance. Moreover, \\citet{su2023caseformer} proposed the Caseformer. This innovative pre-training framework learns from a vast corpus of legal texts to refine case retrieval and contextualization across multiple languages.\n\n\\subsection{Similar Case Retrieval}\nBesides precedent recommendation, retrieving similar cases, those sharing analogous facts or legal issues, is crucial for comprehensive legal analysis and strategy formulation. Traditionally, this process required extensive manual labor, with professionals needing to dig through large case databases \\cite{mandal2021unsupervised, ma2023incorporating}. Today advances in NLP and machine learning have transformed this task, allowing semantic content extraction and comparison across documents. \\citet{kang2013retrieval} enhanced similarity-based retrieval by incorporating associative knowledge. This approach refines retrieval outcomes by leveraging similarity and associative analyses, a technique also proven effective in other fields such as medical diagnosis and IT service management. \\citet{mandal2021unsupervised} analyzed textual similarity techniques on an Indian Supreme Court dataset and discovered that traditional methods like TF-IDF outperform modern context-aware models like BERT. \\citet{wu2021joint} studied semantic retrieval in the Chinese judicial system and developed a model that generates knowledge graphs for cases to improve trial accuracy and fairness. These technological advances have greatly simplified legal research, making it more effective and comprehensive.\n\n\\subsection{Legal Judgment Prediction}\nPredicting legal judgments involves estimating potential verdicts based on a deep analysis of historical data and established legal standards. Initial models in this field were relatively simple, mainly depending on linear algorithms incapable of capturing the various aspects of legal reasoning. \\citet{wang2020study} CNN-BiGRU multi-task learning model improves prediction accuracy through the utilization of shared information from related legal subtasks. \\citet{chalkidis2019neural} used European Court of Human Rights data to establish robust performance benchmarks for long legal texts using hierarchical BERT. \\citet{rusnachenko2023nclu_team} showed attention-based methods could improve system performance by optimizing document preprocessing and attention mechanisms in competition contexts. These models predict outcomes and are constantly learning from new cases to improve their accuracy, demonstrating the adaptability of LLMs in legal judgment prediction.\n\n\\subsection{LLMs in the Legal Domain}\nPrior to the development of large language models (LLMs), pre-trained language models (PLMs) for specific domains were explored, such as Lawformer, which is to process lengthy Chinese legal documents using a Longformer-based architecture \\cite{xiao2021lawformer}. Researchers discovered that models like GPT-4 could successfully pass bar exams as LLMs gained attention, demonstrating profound abilities in legal reasoning and text generation \\cite{katz2024gpt}. This success resulted in the growth of legal domain-specific LLMs, such as Chatlaw, which utilizes conversational AI to improve user interactions with legal systems \\cite{cui2023chatlaw}. In this vein, SaulLM-7B was introduced as the first LLM explicitly designed for comprehending and generating legal texts, leveraging a substantial legal corpus to achieve state-of-the-art performance \\cite{colombo2024saullm}. LLMs' influence extends beyond specific tasks to broader legal operations. These applications range from document automation, where LLMs assist in drafting and reviewing legal documents, to compliance monitoring, which ensures adherence to regulatory standards \\cite{Sun2023A}. LLMs simplify complex legal processes for non-specialists and lower barriers to legal advice \\cite{Goodson2023Intention}. This broad application of LLMs demonstrates their broad application and the potential for continued innovation in the legal sector.\n\nDespite the success of those contemporary works, these models primarily focus on utilizing LLMs' understanding and capabilities to perform general legal question answering. However, \\sysname is designed to leverage the LLMs' comprehension and learned abilities to predict and perform specific tasks within the legal domain.\n\n\\section{Methodology}\n\nIn this study, we propose the Law Large Language Model (\\sysname) to address three critical tasks within the legal domain: Similar Case Retrieval (SCR), Precedent Case Recommendation (PCR), and Legal Judgment Prediction (LJP). Our methodological framework, illustrated in Figure \\ref{fig:overflow}, is divided into four distinct parts: Data Preprocessing, SCR Processing, PCR Processing, and LJP Processing.\n\n\\subsection{Data Preprocessing}\n\nOur approach begins with the systematic collection of case data from legal databases, denoted as \\( \\mathcal{D} \\). We make sure all collected raw case data, \\( d_i \\in \\mathcal{D} \\), encompasses a variety of information as below:\n\n\\begin{equation}\n\\begin{aligned}\nd_i = \\{& \\text{Title, } \\text{Date, } \\text{Judge, } \\text{Plaintiff(s), } \\text{Plaintiff's Attorney(s), }\\\\ \n& \\text{Defendant(s), }\\text{Defendant's Attorney(s), } \\text{Case Detail, } \\\\\n& \\text{Precedent Relationship}\\}.\n\\end{aligned}\n\\end{equation}\n\nAs depicted in the upper left of Figure \\ref{fig:overflow}, data preprocessing consists of three primary steps:\n\n\\vspace{3pt}\\noindent\\textbf{Step 1.}\\, Given the voluminous nature of the textual content within case detail and their often implicit verdicts, we utilize a GPT-4 \\cite{achiam2023gpt} model to extract core information and summarize each case. This step reduces information overload and ensures the adaptability of our dataset to the constraints of Gemma, particularly with token size limitations. The GPT-4 preprocess instruction is shown here: \n\n\\begin{formal}\nI have a legal case description and require two distinct pieces of information:\n\\newline\n1. Summary: Please provide a detailed summary of the case, focusing on\nthe facts and events. Exclude any information about the verdict.\n\\newline\n2. Verdict: State the verdict of the case, consider the following categories:\n\\newline\n- Plaintiff win\\newline\n- Defendant win\\newline\n- Settlement\\newline\n- Case dismissal\\newline\n- Unsure\\newline\nIf the verdict is mentioned, respond exclusively with the chosen\ncategories ONLY. If the outcome is not explicitly mentioned or cannot\nbe inferred from the information given, please respond with `unsure'\nonly.\n\\newline\nFormat your responses as follows:\\newline\n\\# - For the summary, begin with `Answer 1:'\\newline\n\\# - For the verdict, start with `Answer 2:'\\newline\nHere is the description of the case:\\newline\n[Case Description...]\n\\end{formal}\n\nThe output of this step includes a summarized case and a labeled verdict, formatted as follows:\n\n\\begin{equation}\n\\begin{aligned}\n\\text{Case Summary, Verdict} = \\text{LLM}(&\\text{Case Detail}, \\\\\n& \\text{Maximum Token} \\mid d_i).\n\\end{aligned}\n\\end{equation}\n\nFor each legal case \\(d_i\\), we reorganize the data into a new format \\(d_i'\\), defined as:\n\n\\begin{equation}\n\\begin{aligned}\nd_i' = \\{& \\text{Title, } \\text{Date, } \\text{Judge, } \\text{Plaintiff(s), } \\text{Plaintiff's Attorney(s), }\\\\ \n& \\text{Defendant(s), }\\text{Defendant's Attorney(s), } \\\\\n& \\text{Case Summary}\\}.\n\\end{aligned}\n\\end{equation}\n\n\\begin{equation}\n\\begin{aligned}\nD' = \\left\\{ \\left( d_1', v_1' \\right), \\left( d_2', v_2' \\right), \\ldots, \\left( d_n', v_n' \\right) \\right\\}.\n\\end{aligned}\n\\end{equation}\n\nThere are some constraints when we separate the \\( D' \\) into training and testing data. We make sure that all legal cases have at least five precedent relationships. To ensure a balance training, the training dataset has 25\\% from each of the following categories: plaintiff wins, defendant wins, settlements, and case dismissals. We also make sure that all testing legal cases have at least five precedent relationships connect to the training dataset, further explanation is given in Section \\ref{data_splits} Data Splits.\n\n\\vspace{3pt}\\noindent\\textbf{Step 2.}\\, After Step 1, all training legal cases \\( d_i' \\) are transformed into high-dimensional vectors using the OpenAI Embedding model. This vector database is later used to retrieve the top-$k$ similar cases based on semantic and contextual similarities.\n\n\\vspace{3pt}\\noindent\\textbf{Step 3.}\\, This step involves converting the precedent case relationships from our training dataset into a knowledge graph (KG). Defined as $KG = (E, R, L)$, where $E$ represents entities, $R$ represents binary relationships (indicative of precedent relations), and $L \\subseteq E \\times R \\times E$ represents the set of triples forming the graph's edges. Each triple $(e_s, r, e_t) \\in L$ indicates a directed edge from source entity $e_s$ to target entity $e_t$ via relationship $r$. The KG data structure simplifies the complex task of identifying relevant precedent cases, turning it into a entity prediction problem, i.e., given a query of $(e_s, r, ?)$, the model will predict the missing entity.\n\nWe further customize data processing for SCR, PCR, and LJP tasks, ensuring a robust and effective implementation of \\sysname.\n\n\\subsection{Similar Case Retrieval}\n\nAs depicted in the upper right of Figure \\ref{fig:overflow}, the SCR process is divided into two phases: training (Steps 1-2) and testing (Steps 3-4).\n\n\\vspace{3pt}\\noindent\\textbf{Training Phase.}\\, During training, each training case \\(d_i' \\) is inputted into the vector database, which generates the top 10 candidate cases. These cases are then randomized in order and formulated into the SCR training instruction. Here is an example SCR model input:\n\n\\begin{formal}\n\\#\\#\\# Instruction:\\newline\nYou are a legal expert who specializes in comparing user-supplied legal cases to a list of candidate legal cases, which includes titles and content. Your main function is to identify and output the title of the most similar case from the list based on the description provided. \\newline\nYou should only output the case title and not any other information. \\newline\nConsider the following choices:\\newline Choice 1: \\newline [Case 1...] \\newline Choice 2: \\newline ... \\newline  Choice 10: \\newline [Case 10...]\\newline\n\\#\\#\\# Input:\\newline\n[Input Case...]\n\\end{formal}\n\nIn this scenario, the SCR task instruction will fall into the classification category, which provides the model with 10 cases to choose the most similar one. It is important to note that the top-0 similar case is the case $d_i'$ itself, so in practice, we retrieve the top-1 to top-10 similar cases from the vector database, and the top-1 case from this selection serves as the ground truth for this training task.\n\n\\vspace{3pt}\\noindent\\textbf{Testing Phase.}\\, \nThe testing phase mirrors the training process as we initially retrieve the top 10 similar cases from the vector database. However, during testing, we retrieve cases ranked from top-0 to top-9, as the test case itself is not included in the vector database. The model's expected response depends on the evaluation metrics we use: top-1, top-3, and top-5. For the top-1 metric, we expect \\sysname to identify the most similar case as the top result. The top-3 metric evaluates whether the model's answer falls within the top three retrieved candidates, while the top-5 metric extends this evaluation to include the top five candidates. \n\n\\subsection{Precedent Case Recommendation}\n\nThe Precedent Case Recommendation (PCR) within \\sysname utilizes a unique approach by employing a precedent case knowledge graph (KG), which differentiates itself from conventional PCR methods that often speculate on potential precedent relationships. Our system instead relies on confirmed precedent pairs, as illustrated in the lower left of Figure \\ref{fig:overflow}, where Steps 1 and 2 constitute the training phase and Steps 3-5 are the testing phase.\n\n\\vspace{3pt}\\noindent\\textbf{Training Phase.}\\, From the previously established KG, for each confirmed triple $(e_s, r, e_t)$, we utilize BERT embeddings \\cite{devlin2018bert} to evaluate the similarity between various case features (e.g., Judge, Case Detail, Plaintiff, or Defendant), denoted as \\{\\( F_1, F_2, ... F_j\\)\\}. We calculate the similarity score \\( S_i \\) for each feature pair \\( F1_i \\) and \\( F2_i \\), as follows:\n\n\\begin{equation}\n\\label{eq:1}\n\\begin{aligned}\nS_{i} = \\text{sim}(BERT(F1_{i}), BERT(F2_{i})), &\\\\ \\quad i \\in \\{1-j\\}\n\\end{aligned}\n\\end{equation}\n\nThe highest similarity score across all features determines the primary factor underlying their precedent relationship:\n\n\\begin{equation}\n\\label{eq:2}\n\\begin{aligned}\n\\text{Primary Factor} = \\max(S_{1}, S_{2}, ..., S_{j}).\n\\end{aligned}\n\\end{equation}\n\nDuring the training input creation, we present a total of 10 choices for the model. The ground truth precedent case $e_t$ is randomly placed among these choices, with the other 9 selections filled with similar, yet non-precedent, cases from the vector database. This setup aims to teach the model that textual similarity does not necessarily imply a precedent relationship. The model's expected output includes the correct precedent case $e_t$ and the reasoning for its selection (i.e, which primary factor caused this precedent relationship). An example of the model input is:\n\n\\begin{formal}\n\\#\\#\\# Instruction:\\newline\nYou are a legal expert who specializes in comparing user-supplied legal cases to a list of candidate legal cases, which includes titles and content. Your main function is to identify and output the precedent case from the list based on the description provided. \\newline\nYou should only output the reasoning process and case title. \\newline\nConsider the following choices:\\newline Choice 1: \\newline [Case 1...] \\newline Choice 2: \\newline ... \\newline  Choice 10: \\newline [Case 10...]\\newline\n\\#\\#\\# Input:\\newline\n[Input Case...]\n\\end{formal}\n\n\\vspace{3pt}\\noindent\\textbf{Testing Phase.}\\, For each test case, since we made sure there are at least five precedent cases in the training dataset, we can identify $k$ precedent cases from the KG (which structured by the training dataset) as ground truths, where $k$ aligns with the top-$k$ evaluation metrics. For the top-1 metric, a single ground truth precedent case is selected, while for top-3 and top-5 metrics, 3 and 5 ground truths are selected, respectively. The remaining slots of $10 - k$ are filled with similar cases. The model is then tasked with selecting one of $k$ precedent cases and explaining the reasoning behind its choice.\n\n\\subsection{Legal Judgment Prediction}\n\nThe Legal Judgment Prediction (LJP) processing utilizes the dataset \\(D'\\) constructed during the data preprocessing stage. This dataset pairs each processed legal case \\(d_i'\\) with its corresponding verdict \\(v_i'\\). As illustrated in the lower right of Figure \\ref{fig:overflow}, the training phase involves step 1 and the testing phase involves rest of steps.\n\n\\vspace{3pt}\\noindent\\textbf{Training Phase.}\\, We use $\\left( d_i', v_i' \\right)$ to establish a four-category classification training input, Plaintiff wins, Defendant wins, Settlement, or Case Dismissal. Each case's corresponding verdict \\(v_i'\\) serves as the label for training. Here is an example of the model input:\n\n\\begin{formal}\n\\#\\#\\# Instruction:\\newline\nYou are a legal expert who specializes in predicting outcomes for legal cases. Utilize your internal knowledge base to predict verdict. Your main function is to anticipate the likely verdict of the legal case presented by the user.\n\\newline\nYou should only output the verdict and not any other information. \\newline\nConsider the following choices:\\newline 1. Defendant Wins \\newline 2. Plaintiff Wins \\newline 3. Settlement \\newline 4. Case Dismissal\\newline\n\\#\\#\\# Input:\\newline\n[Input Case...]\n\\end{formal}\n\n\\vspace{3pt}\\noindent\\textbf{Testing Phase.}\\, During the testing phase, we evaluate \\sysname with both zero-shot and few-shot in-context learning (ICL) scenario. In few-shot ICL, we enhance each test case \\(d_i'\\) with additional contextual information, one similar case and one precedent case. Its precedent cases is sourced from our KG, and one is randomly selected to be included in the test input. Simultaneously, a most similar case is retrieved from the vector database. This approach ensures that the model's predictions are influenced by relevant legal precedents and similar case facts, thereby improving the accuracy and reliability of the judgment predictions.\n\n\\subsection{Unified Model Fine-Tuning}\n\nOur methodology involves a unified fine-tuning strategy for the \\sysname, leveraging a combined dataset with three tasks. This dataset, denoted as \\(\\text{Dataset}_{\\text{combined}} = \\text{LJP} \\oplus \\text{PCR} \\oplus \\text{SCR} \\). We employ a cutting-edge 4-bit quantized Low-Rank Adaptation (LoRA) technique to instruction fine-tune the Gemma model. We use the cross-entropy loss function $L$ during the LoRA. It calculates the difference between the model's predicted token probabilities and the actual token probabilities in the expected output sequence. In the following equation, $n$ represents the length of the expected output sequence, $x$ represents the input instruction, and $y_i$ denotes the i-th token in the expected output sequence.\n\n\\begin{equation}\n\\label{eq:1}\nL = -\\sum_{i=1}^{n} \\log P(y_i | x, y_1, y_2, ..., y_{i-1}).\n\\end{equation}\n\n\\section{Experiments}\n\nIn this section, we conduct experiments to evaluate the performance of \\sysname on three tasks: Similar Case Retrieval (SCR), Precedent Case Recommendation (PCR), and Legal Judgment Prediction (LJP).\n\n\\subsection{Experimental Setup}\n\n\\vspace{3pt}\\noindent\\textbf{Datasets.}\\,\nWe conduct our experiments on the CaseLaw dataset, initiated by Harvard Law School's Library Innovation Lab as part of the CaseLaw Project \\cite{caselawproject}. This database encompasses a wide range of court cases from both state and federal in the United States. The project primarily focuses on democratizing access to American legal information, particularly through its Case Access Project (CAP), which is aimed at providing free and public access. The statistics of the CaseLaw dataset used in our experiments are shown in Table \\ref{tab:dataset}.\n\n\\begin{table}[h]\n\\centering\n\\caption{Datasets Statistics}\n\\label{tab:dataset}\n\\begin{tabular}{ll}\n\\hline\nDATASETS                         & CaseLaw \\\\ \\hline\nLanguage                         & English \\\\\n\\# State and Federal Totals      & 6,930,777 \\\\\n\\# Train case                    & 1,00,000  \\\\\n\\# Test case                     & 200,000   \\\\\nAvg. length per case (words)     & 2695.38 \\\\\n\\hline\n\\end{tabular}\n\\end{table}\n\n\\vspace{3pt}\\noindent\\textbf{Evaluation Metrics.}\\,\nAs previously mentioned, we employ top-$k$ metrics to evaluate the performance of the SCR and PCR tasks. Specifically, we use top-1, top-3, and top-5 metrics. These metrics measure the model's precision in identifying the correct response from a pool of 10 choices. For example, the top-1 metric requires the model to return the top choice as the answer. The top-3 and top-5 metrics provide more flexibility, allowing the correct answer to be anywhere within the top three or top five choices, respectively.\n\nIn addition to top-$k$ metrics, we evaluate the hallucination rate of models using a `not-found' metric. This metric tracks the proportion of responses that are entirely fabricated and do not match any of the 10 given choices. By measuring the `not-found' rate, we aim to understand how often models produce answers unrelated to the provided options, offering insight into their reliability.\n\nFor the LJP task, we employ accuracy and F1-score \\cite{sai2022survey} metrics to gauge the model's performance. Accuracy calculates the proportion of correctly predicted verdicts across all cases, providing a direct measure of overall prediction performance. The F1 score ranging from 0 to 1, combines precision and recall into a single harmonic mean, offering a balanced evaluation of the model's effectiveness.\n\n\\vspace{3pt}\\noindent\\textbf{Data Splits.}\\,\n\\label{data_splits}\nAs previously noted, our data are split according to three constraints. \n\\begin{itemize}\n    \\item \\textbf{Constraint 1:} For PCR, we employ top-$k$ evaluation metrics, which means each case has to have a minimum of five precedent cases, allowing us to identify $k$ ground truths.\n\n    \\item \\textbf{Constraint 2:} We must ensure that when a test case is evaluated, its ground truth precedent case can be located within the knowledge graph formed by the training cases. Therefore, each test case must have at least five precedent cases present in the training data. \n    \\item \\textbf{Constraint 3:} To ensure balanced model training for Legal Judgment Prediction (LJP), the training data's verdict distribution should consist of 25\\% for each possible outcome: plaintiff wins, defendant wins, settlements, and dismissals.\n\n\\end{itemize}\n\nThese approaches result in a total of 1,000,000 cases for training and 200,000 cases for testing.\n\n\\vspace{3pt}\\noindent\\textbf{Comparing Baselines.}\\,\nOur model is evaluated against advanced baselines including LLaMa2-7b \\cite{touvron2023llama}, Gemma-7b \\cite{team2023gemini}, Vicuna-13b \\cite{zheng2024judging} and Guanaco-13b \\cite{dettmers2024qlora}, alongside the larger and more advanced GPT-3.5 and GPT-4 models \\cite{achiam2023gpt}. Each model undergoes the same testing phase to ensure a consistent and fair comparison of their multi-task capabilities within the legal domain.\n\n\\vspace{3pt}\\noindent\\textbf{Implementation Details.}\\,\nWe conducted the training of our model over 10 epochs using an A40 GPU. To ensure compatibility, we monitored the input token size, capping it at 4096 tokens to align with Gemma's maximum token capacity. Additionally, we configured the model's dropout rate at 0.1 and set the learning rate to $2e^{-4}$.\n\n\\subsection{Similar Case Retrieval Results}\n\nAccording to Table \\ref{table:scr}, \\sysname outperformed the baseline models in all categories. Specifically, it achieved the highest accuracy in top-1, top-3, and top-5 retrieval rates, with scores of 29.8\\%, 63.2\\%, and 81.6\\% respectively. Remarkably, it also demonstrated minimal hallucination, as indicated by the not-found rate of 0.1\\%.\n\n\\begin{table}[h]\n\\centering\n\\caption{SCR Test Results}\n\\begin{tabular}{lcccc}\n\\toprule\n\\midrule\nMethod          & top-1 $\\uparrow$ & top-3 $\\uparrow$ & top-5 $\\uparrow$ & Not Found $\\downarrow$ \\\\\n\\midrule\nllama2-7b       & 0.083   & 0.197   & 0.309   & 0.406 \\\\\ngemma-7b        & 0.181   & 0.428   & 0.536   & 0.121 \\\\\nvicuna-13b      & 0.185   & 0.372   & 0.564   & 0.187 \\\\\nguanaco-13b     & 0.077   & 0.214   & 0.375   & 0.372 \\\\\ngpt3.5          & 0.219   & 0.579   & 0.691   & 0.148 \\\\\ngpt4            & 0.274   & 0.526   & 0.708   & 0.005 \\\\\n\\rowcolor{gray!40} \\sysname & \\textbf{0.298}   & \\textbf{0.632}   & \\textbf{0.816}   & \\textbf{0.001} \\\\\n\\midrule\n\\bottomrule\n\\label{table:scr}\n\\end{tabular}\n\\end{table}\n\nComparatively, GPT-4 showed strong performance with top-1, top-3, and top-5 accuracies of 27.5\\%, 52.5\\%, and 70.5\\%, and a low not-found rate of 0.5\\%. GPT-3.5 also performed well, especially in the top-3 and top-5 metrics. On the other hand, models like LLaMa2-7b and Guanaco-13b displayed higher not-found rates, indicating a tendency towards hallucination.\n\nThe results underscore the effectiveness of our \\sysname model in accurately retrieving similar cases while minimizing the risk of generating irrelevant or nonexistent cases.\n\n\\subsection{Precedent Case Recommendation Results}\n\nAccording to Table \\ref{table:pcr}, the \\sysname model again outperformed other baseline methods. It achieved the best results with a top-1 rate of 31.8\\%, top-3 rate of 59.7\\%, and top-5 rate of 83.2\\%. Additionally, the \\sysname model exhibited an low not-found rate of 0.1\\%.\n\n\\begin{table}[h]\n\\caption{PCR Test Results}\n\\begin{tabular}{lcccc}\n\\toprule\n\\midrule\nMethod          & top-1 $\\uparrow$ & top-3 $\\uparrow$ & top-5 $\\uparrow$ & Not Found $\\downarrow$ \\\\\n\\midrule\nllama2-7b        & 0.069   & 0.148   & 0.343   & 0.479 \\\\\ngemma-7b         & 0.187   & 0.386   & 0.519   & 0.124 \\\\\nvicuna-13b       & 0.175   & 0.352   & 0.506   & 0.203    \\\\\nguanaco-13b      & 0.073   & 0.198   & 0.357   & 0.383    \\\\\ngpt3.5           & 0.154   & 0.325   & 0.504   & 0.165 \\\\\ngpt4             & 0.262   & 0.514   & 0.697   & 0.007 \\\\\n\\rowcolor{gray!40} \\sysname & \\textbf{0.318}   & \\textbf{0.597}   & \\textbf{0.832}   & \\textbf{0.001} \\\\\n\\midrule\n\\bottomrule\n\\label{table:pcr}\n\\end{tabular}\n\\end{table}\n\nAmong the baseline models, GPT-4 was a strong performer, with high accuracy in top-1, top-3, and top-5 metrics, alongside a very low not-found rate, suggesting reliable and accurate recommendations. In contrast, models like LLaMa2-7b and Guanaco-13b showed higher not-found rates, highlighting challenges in providing relevant case recommendations. The overall results demonstrate the effectiveness of the \\sysname model in PCR task, outstripping baseline models in both accuracy and reliability.\n\nOne notable insight from comparing SCR and PCR results is that most baseline models exhibited a performance drop in the PCR task compared to SCR. For instance, the GPT-4 model achieved scores of 27.4\\%, 52.6\\%, 70.8\\%, 0.5\\% in SCR top-$k$ and ``Not Found'' metrics, while in the PCR task, its scores dropped to 26.2\\%, 51.4\\%, 69.7\\% and 0.7\\%. This decline underscores the greater difficulty of identifying precedent cases compared to similar cases, as models cannot rely solely on textual similarity when determining precedent relationships. Instead, they must consider nuanced factors such as legal relevance. This performance difference reinforces the our previous assertion that precedent cases are distinct from similar cases, emphasizing the importance of distinguishing between the two concepts in the legal domain.\n\nWe conducted an analysis to identify the factors that are predominantly considered by \\sysname when determining a precedent relationship under the top-1, top-3, and top-5 settings. This analysis involves comparing the frequency with which each factor is chosen as the primary determinant in our model against the ground truth (GT) distribution. As shown in Table \\ref{table:factor}, the GT distribution is heavily weighted towards the `Case Detail' factor, with some toward other factors. In the top-1 scenario, where there is only one correct precedent case among nine similar cases, our model strongly focuses on the `Case Detail' factor. This bias likely stems from the GT distribution's heavy emphasis on `Case Detail,' leading our model to prioritize this factor, especially when faced with numerous similar cases that serve as potential distractions. However, as the pool of correct answers expands to three and five in the top-3 and top-5 scenarios respectively, \\sysname begins to diversify its focus slightly to include other factors, although `Case Detail' continues to dominate. This trend indicates a move towards a more balanced approach in factor consideration as the number of correct choices increases, suggesting that \\sysname adjusts its focus based on the availability of correct answers, while still reflecting the main emphasis observed in the ground truth data.\n\n\\begin{table}[h]\n\\caption{Primary Factor Percentage Comparison}\n\\begin{tabular}{lcccc}\n\\toprule\n\\midrule\nFactor          & LawLLM    & LawLLM    & LawLLM    & GT   \\\\\n                & top-1     & top-3     & top-5     &       \\\\\n\\midrule\nTitle           & 0.000   & 0.000   & 0.000   & 0.000    \\\\\nDate            & 0.000   & 0.000   & 0.000   & 0.000     \\\\\nJudge           & 0.027   & 0.054   & 0.116   & 0.149     \\\\\nPlantiff(s)     & 0.002   & 0.009   & 0.013   & 0.027     \\\\\nDefendent(s)    & 0.004   & 0.012   & 0.025   & 0.041     \\\\\nCase Detail     & 0.967   & 0.925   & 0.846   & 0.783     \\\\\n\\midrule\n\\bottomrule\n\\label{table:factor}\n\\end{tabular}\n\\end{table}\n\n\\subsection{Legal Judgment Prediction Results}\n\nAs shown in Table \\ref{table:ljp}, the \\sysname surpasses all baseline methods in both zero-shot and few-shot scenarios for the LJP task. In the zero-shot scenario, LawLLM achieves an accuracy of 0.636 and an F1 score of 0.591, significantly outperforming the second best model, GPT-4, which scores 0.573 and 0.563 in accuracy and F1, respectively. In the few-shot scenario, LawLLM maintains its superior performance, reaching an accuracy of 0.794 and an F1 score of 0.758. These results show a considerable improvement over GPT-4, the closest competitor, which scores 0.732 in accuracy and 0.712 in F1.\n\nAdditionally, all models demonstrate higher performance in the few-shot in-context learning (ICL) scenario compared to the zero-shot setting. For instance, LLaMA2-7b shows an increase from 0.235 to 0.473 in accuracy, and from 0.239 to 0.455 in F1 score. This pattern indicates that all models benefit from incorporating a few ICL examples, which helps them better understand the task.\n\n\\begin{table}[h]\n\\centering\n\\caption{LJP Test Results}\n\\begin{tabular}{lcccc}\n\\toprule\n\\midrule\nMethod          & Accuracy $\\uparrow$ & F1 $\\uparrow$ & Accuracy $\\uparrow$ & F1 $\\uparrow$ \\\\\n                & (Zero-shot) & (Zero-shot)  & (Few-shot) & (Few-shot) \\\\\n\\midrule\nllama2-7b        & 0.235   & 0.239   & 0.473   & 0.455 \\\\\ngemma-7b         & 0.317   & 0.287   & 0.568   & 0.527 \\\\\nvicuna-13b       & 0.503   & 0.432   & 0.645   & 0.594    \\\\\nguanaco-13b      & 0.281   & 0.247   & 0.491   & 0.463    \\\\\ngpt3.5           & 0.558   & 0.546   & 0.679   & 0.647 \\\\\ngpt4             & 0.573   & 0.563   & 0.732   & 0.712 \\\\\n\\rowcolor{gray!40} \\sysname & \\textbf{0.636}   & \\textbf{0.591}   & \\textbf{0.794}   & \\textbf{0.758} \\\\\n\\midrule\n\\bottomrule\n\\label{table:ljp}\n\\end{tabular}\n\\end{table}\n\n\\section{Conclusions and Future Work}\n\nIn this study, we introduced the Law Large Language Model (\\sysname), a multi-task LLM specifically designed for the US legal domain. By leveraging unique data processing techniques tailored for each task, \\sysname effectively handles Similar Case Retrieval (SCR), Precedent Case Recommendation (PCR), and Legal Judgment Prediction (LJP). Furthermore, we emphasized the crucial distinctions between precedent relationships and textual similarity, providing insights that can inform future research in developing task-specific models. Our results consistently demonstrated that \\sysname outperforms existing baseline models, showcasing its superior multi-task capabilities.\n\nIn the future, we aim to expand the scope of \\sysname by incorporating additional legal tasks to further enhance its versatility and practical applicability. This will involve exploring emerging challenges in legal analytics and integrating new datasets that reflect diverse legal contexts. Moreover, we plan to refine our data processing techniques and in-context learning methodologies to improve the model's understanding of legal nuances and precedents. \n\n\\balance\n\n\\newpage\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2403.03883v2.tex",
        "arXiv-2406.04614v1.tex",
        "arXiv-2407.21065v1.tex"
    ],
    "group_id": "group_73",
    "response": "### Title: Large Language Models for Legal Applications: A Comparative Analysis\n\n### Introduction\n\nThe field of legal analytics has seen significant advancements with the integration of large language models (LLMs) into its domain. LLMs, which are trained on vast amounts of text data, have the potential to revolutionize the way legal professionals handle complex documents, predict judicial outcomes, and retrieve relevant cases. The history of applying AI to legal tasks dates back to the early days of machine learning, where models were primarily used for document analysis and basic legal question answering. However, recent developments in LLMs have introduced a new era of capabilities, enabling these models to perform more sophisticated tasks such as legal judgment prediction, precedent case recommendation, and similar case retrieval. These tasks require not only a deep understanding of legal language but also the ability to interpret and apply legal principles in a nuanced manner.\n\nDespite the promising advancements, there are still significant challenges in utilizing LLMs for legal tasks. One of the primary challenges is the need for specialized legal knowledge, which is often not adequately captured in general-purpose models. Additionally, the legal domain's unique syntax and vocabulary pose distinct linguistic challenges that require tailored solutions. Another challenge is the need for models to be adaptable to the evolving nature of legal discourse, including new legislation and case law developments. Lastly, ensuring data privacy and avoiding hallucinations (where models generate incorrect or irrelevant information) are critical concerns, especially in sensitive legal contexts.\n\nThis summary focuses on three recent research papers that address these challenges by developing LLMs specifically for legal applications. The first paper introduces \\ourmodel{}, a large language model tailored for the English legal domain, while the second paper presents \\algo{}, a Chinese legal knowledge-enhanced LLM. The third paper introduces LawLLM, a multi-task LLM designed for the US legal system, capable of handling similar case retrieval, precedent case recommendation, and legal judgment prediction. Each paper employs unique methodologies and datasets to enhance the legal capabilities of their respective models, contributing to the broader landscape of AI in legal analytics.\n\n### Main Content of Each Paper\n\n#### \\ourmodel{}: A Pioneering Large Language Model for Law\n\n\\ourmodel{} is a large language model (LLM) specifically designed for the legal domain, with a focus on English-speaking jurisdictions. The model, with 7 billion parameters, is built upon the Mistral 7B architecture and is trained on a legal corpus of over 30 billion tokens. This corpus is curated from various sources, including the FreeLaw subset from The Pile, EDGAR, and the English MultiLegal Pile, among others. The authors argue that legal text presents unique challenges due to its specialized vocabulary and syntax, necessitating a dedicated model to handle these intricacies effectively.\n\nThe methodology for \\ourmodel{} involves two primary stages: continued pretraining and instruction fine-tuning. During continued pretraining, the model is exposed to a large and diverse legal dataset, which is cleaned and deduplicated to ensure high quality. This stage is crucial for integrating legal domain knowledge into the model. Following pretraining, the model undergoes instruction fine-tuning, where it is trained on a mix of general and legal instructions. The general instructions come from datasets like SlimOrca, Meta Math Question Answering Instructions, UltraChat, and Glaive Code Assistant v2, while the legal instructions are synthetically generated to cover various legal competencies.\n\nThe evaluation of \\ourmodel{} is conducted using three benchmarks: LegalBench, MMLU, and a perplexity measurement on legal documents. LegalBench is refined to better gauge the model's performance, and MMLU's legal tasks are included for additional insights. The authors also introduce \\legalbench{}, an enhanced version of LegalBench, to provide a more stringent evaluation of the model's legal understanding. The results show that \\ourmodel{} and its instruction-tuned variant, \\ourmodelift{}, outperform existing models like Mistral-Instruct and Llama2 on LegalBench and MMLU, particularly in tasks requiring legal expertise.\n\n#### \\algo{}: A Chinese Legal Knowledge-Enhanced Large Language Model\n\n\\algo{} is the first open-source LLM designed specifically for Chinese legal applications. The model is based on the Chinese-Alpaca-Plus 7B base model and is trained in two stages: legal-oriented pre-training and legal-supervised fine-tuning. Legal-oriented pre-training involves training the base model on a large-scale legal corpus of 500K documents, covering various legal domains such as civil, criminal, and administrative law. This stage is critical for incorporating legal domain knowledge into the model.\n\nLegal-supervised fine-tuning is performed on a 300K instruction dataset, which includes crime type prediction, legal question answering, and consultation tasks. The authors use the Alpaca template to wrap the instruction and output pairs, ensuring that the model can generate appropriate responses to legal queries. The evaluation of \\algo{} is conducted in a zero-shot setting, comparing its performance with proprietary models like GPT-3.5 Turbo and GPT-4, as well as the open-source LLaMA model. The results indicate that \\algo{} outperforms LLaMA on major tasks, demonstrating its effectiveness in handling legal tasks. However, there is still a performance gap between \\algo{} and proprietary models, suggesting potential for further improvements.\n\n#### LawLLM: A Multi-Task Large Language Model for the US Legal System\n\nLawLLM is a multi-task LLM designed to handle three critical tasks within the US legal domain: Similar Case Retrieval (SCR), Precedent Case Recommendation (PCR), and Legal Judgment Prediction (LJP). The model is fine-tuned using instruction tuning on the CaseLaw dataset, which includes a wide range of court cases from both state and federal courts. The methodology for LawLLM involves a unified fine-tuning strategy that leverages a combined dataset with three tasks, using a 4-bit quantized Low-Rank Adaptation (LoRA) technique to enhance the model's performance.\n\nFor SCR, the model is trained to retrieve the most similar cases based on semantic and contextual similarities. For PCR, the model is trained to recommend precedent cases based on confirmed relationships in a knowledge graph (KG). For LJP, the model is trained to predict legal verdicts using a four-category classification approach. The evaluation metrics include top-$k$ metrics for SCR and PCR, and accuracy and F1-score for LJP. The results show that LawLLM outperforms baseline models like LLaMa2-7b, Gemma-7b, Vicuna-13b, and Guanaco-13b in all three tasks, demonstrating its robust capabilities in the legal domain.\n\n### Commonalities and Innovations\n\nDespite their differences in language and specific tasks, all three models share a common goal of enhancing the legal capabilities of LLMs. They all utilize large-scale legal datasets for pretraining and fine-tuning, ensuring that the models have a deep understanding of legal language and principles. Additionally, each model employs instruction tuning to improve its performance on specific legal tasks, such as legal judgment prediction, precedent case recommendation, and similar case retrieval.\n\nThe innovations in each model are noteworthy. \\ourmodel{} introduces a novel instructional fine-tuning method that leverages legal datasets to enhance its performance in legal tasks. It also presents \\legalbench{}, an improved evaluation protocol for legal LLMs, which includes tasks from the MMLU benchmark. \\algo{} is the first open-source LLM designed for Chinese legal applications, addressing the need for data privacy and specialized legal knowledge. LawLLM, on the other hand, is a multi-task model that distinguishes between precedent and similar cases, providing a more nuanced approach to legal tasks.\n\n### Comparison of Results\n\nThe performance of each model is evaluated using different benchmarks and metrics, reflecting the unique challenges and requirements of the legal domain. \\ourmodel{} and \\ourmodelift{} are evaluated on LegalBench and MMLU, with \\ourmodelift{} achieving a balanced accuracy of 0.61 on LegalBench, outperforming existing models like Mistral-Instruct and Llama2. The model also demonstrates lower perplexity scores on legal documents, indicating better adaptation to the legal domain.\n\n\\algo{} is evaluated in a zero-shot setting, comparing its performance with proprietary models like GPT-3.5 Turbo and GPT-4, as well as the open-source LLaMA model. The results show that \\algo{} outperforms LLaMA on major tasks, with an average performance of 17.4 on LegalBench. However, it still lags behind proprietary models, indicating room for further improvements.\n\nLawLLM is evaluated on the CaseLaw dataset, with results demonstrating superior performance across all three tasks. In SCR, LawLLM achieves top-1, top-3, and top-5 retrieval rates of 29.8\\%, 63.2\\%, and 81.6\\%, respectively, with a low not-found rate of 0.1\\%. In PCR, LawLLM achieves top-1, top-3, and top-5 rates of 31.8\\%, 59.7\\%, and 83.2\\%, respectively, with a not-found rate of 0.1\\%. In LJP, LawLLM outperforms all baseline models in both zero-shot and few-shot scenarios, achieving an accuracy of 0.636 and an F1 score of 0.591 in the zero-shot setting, and 0.794 and 0.758 in the few-shot setting.\n\n### Conclusion\n\nThe main findings of these studies highlight the potential of LLMs in addressing complex legal tasks. \\ourmodel{} demonstrates state-of-the-art performance in understanding and processing legal documents, outperforming existing models like Mistral-Instruct and Llama2 on LegalBench and MMLU. \\algo{} is the first open-source LLM designed for Chinese legal applications, showing significant improvements over LLaMA in zero-shot scenarios. LawLLM, a multi-task model, outperforms baseline models in Similar Case Retrieval, Precedent Case Recommendation, and Legal Judgment Prediction, indicating its robust capabilities in handling diverse legal tasks.\n\nFuture research directions could include expanding the scope of these models to incorporate more legal tasks and contexts, refining data processing techniques to better capture legal nuances, and addressing the challenge of hallucinations in legal responses. Additionally, there is a need for more comprehensive legal benchmarks that can accurately measure the performance of LLMs in the legal domain, ensuring that these models are reliable and effective in practical legal applications."
}