{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Trained on 100 million words and still in shape: \\\\BERT meets British National Corpus}\n\n\\begin{document}\n\n\\maketitle\n    \n    \\begin{abstract}\n    \nWhile modern masked language models (LMs) are trained on ever larger corpora, we here explore the effects of down-scaling training to a modestly-sized but representative, well-balanced, and publicly available English text source -- the British National Corpus. We show that pre-training on this carefully curated corpus can reach better performance than the original BERT model. We argue that this type of corpora has great potential as a language modeling benchmark. To showcase this potential, we present fair, reproducible and data-efficient comparative studies of LMs, in which we evaluate several training objectives and model architectures and replicate previous empirical results in a systematic way. We propose an optimized LM architecture called LTG-BERT.\n    \n    \\end{abstract}\n    \n    \\section{Introduction}\n    In the pursuit of state-of-the-art performance, NLP practitioners utilize increasingly larger amounts of data to pre-train language models, %and more intricate fine-tuning schemes. % -- \n    making it difficult to disentangle the improvements made by the proposed modeling choices themselves. \n    Instead, our aim is to shift the focus towards more efficient language modeling on a small and standardizable pre-training corpus.\n    We study the data efficiency of current language models on an openly available corpus of approximately 100M words -- incidentally the estimated amount of words processed by humans before adulthood \\citep{linzen-2020-accelerate}.\n    \n     \nThe goal of this paper is not to rival the paradigm of `massively pre-trained language models'; instead we would in this work like to pursue a complementary direction of language modeling, which will hopefully lead to more interest in data-efficient language models. In particular, our contribution in this paper is twofold -- we show that:\n    \n    \n    \\noindent\n    \\paragraph{1.}\\textbf{100M words is enough} to train a competitive language model that outperforms the downstream performance of the original BERT model. We show that the combination of a well-curated representative corpus, improved LTG-BERT architecture and a better training objective results in a model with stronger linguistic knowledge than the original English BERT pre-trained on $30\\times$ larger corpus.\n    \n     Large language models are notoriously data hungry, requiring hundreds of gigabytes of raw textual data. This becomes a major obstacle for low-resource languages while also putting a limit to the efficiency of any `efficient' language model. On top of that, the size of web-crawled corpora makes it almost impossible to control their content and to prevent learning from harmful or copyrighted text \\citep{bender2021dangers}. The British National Corpus \\citep[BNC;][]{bnc2007british} is a 100-million-word reference corpus, manually curated to cover most aspects of $20^\\textrm{th}$ century British English. %We evaluate to what extent can such a corpus compete with much larger web-crawled corpora.\n    \n    \\noindent\n    \\paragraph{2.}\\textbf{Reproducibility and fair comparison} of language models can be easily achieved by pre-training on the %established and openly available \n    British National Corpus. \n    \n    Massive language models are often pre-trained on nonpublic filtered collections of web-crawled text, which makes any reproduction impossible. % \\citep{devlin-etal-2019-bert, DBLP:journals/corr/abs-1907-11692, clark2020electra}.\n    We pre-train our models on a small and publicly available corpus, \n    which allows for a replicable comparison of different language modeling configurations and which can be easily utilized in future research of novel variants of language models. We also release the pre-processing scripts, training scripts as well as the final model checkpoints.\\footnote{\\url{https://github.com/ltgoslo/ltg-bert}}\n    \n    Previously, language models have been pre-trained on different corpora tokenized by different tokenizers and fine-tuned by increasingly complex learning methods. This makes any comparison of the underlying neural architectures and pre-training objectives unfair. We make the language models in this paper directly comparable by fixing the training corpus, the tokenizer and the evaluation methods, while keeping them as simple as possible.\n    \n        \n    \n        \n\n\\section{Related Work}\n\nThe data requirements of language models have been growing in orders of magnitude since their early stages \\citep{jelinek1976}. Taking a huge leap towards more recent work, \nELMo \\citep[Embeddings from Language Models; ][]{peters-etal-2018-deep} were the first to introduce deep \\textit{contextualized} embeddings of words. Recognizing the need of a large text corpus for this task, ELMo was trained on the 1B Word Benchmark \\citep{chelba14_interspeech}. Later, BERT \\citep[Bidirectional Encoder Representations from Transformers; ][]{devlin-etal-2019-bert} further advanced the performance of contextualized embeddings when it based the entire language model on the Transformer architecture \\citep{NIPS2017_3f5ee243}. Another important aspect of BERT is that it was trained on a larger corpus than ELMo: about 3.3B words from crawled English Wikipedia and BookCorpus \\citep{7410368}. To our best knowledge, the exact version of neither of the two subcorpora is publicly available.\\footnote{BookCorpus \\citep{7410368} is not available anymore and the authors of BERT do not specify what version of Wikipedia dump they used or how did they preprocess it (\\url{https://github.com/google-research/bert\\#pre-training-data}).} The issue of limited replicability has become even more pronounced with later large language models: XLNet \\cite{NEURIPS2019_dc6a7e65} was trained on 33B words, RoBERTa \\citep{DBLP:journals/corr/abs-1907-11692} on more than 30B words and GPT-3 \\citep{NEURIPS2020_1457c0d6} on an approximately 400B word corpus. None of these datasets is available; the authors utilize non-trivial filtering algorithms but do not release the end product nor the filtering scripts.\n\nThe benefits of large corpora were questioned in CamemBERT \\citep{martin-etal-2020-camembert} and the effect of corpus size has been then thoroughly studied in \\newcite{micheli-etal-2020-importance}, \\newcite{zhang-etal-2021-need} as well as in \\newcite{https://doi.org/10.48550/arxiv.2203.15556}. They test differently sized random subsets of a BERT-like corpus (crawled Wikipedia and Smashwords) and of a massive web-crawled text corpus \\citep[MassiveText;][]{https://doi.org/10.48550/arxiv.2112.11446}, respectively. Unlike them, we evaluate the effect of training on a small corpus, which was \\textit{carefully curated} to create a representative sample of English. The British National Corpus is arguably more diverse and informative than a random subset of a web crawl -- hence we test how the \\textit{quality} of a pre-training corpus influences the downstream performance, not only how the data quantity matters. We believe this aspect is vital for the future research of effective and reliable language models.\n\n{\\renewcommand{\\arraystretch}{1.2}\n\\begin{table}[t]\n    \\resizebox{\\columnwidth}{!}{\n        \\begin{tabular}{@{}lrrrr@{}}\n        \\toprule\n        \\textbf{}   & \\hspace{-3em}\\textbf{documents} & \\textbf{sentences} & \\textbf{words} & \\textbf{subwords} \\\\ \\midrule\n        train       & 4\\,014           & 8\\,501\\,376            & 115\\,870\\,549      & 131\\,392\\,103         \\\\\n        development & 35               & 106\\,566             & 1\\,215\\,306        & 1\\,367\\,570           \\\\ \\bottomrule\n        \\end{tabular}\n    }\n    \\caption{Size of the train-development splits for the preprocessed BNC corpus. Note that the number of words is larger than the 100 million reported by the BNC Consortium due to our less conservative pre-tokenization strategy.}\n    \\label{tab:bnc}\n\\end{table}\n}\n \n    \\section{British National Corpus}\n    \\label{sec:bnc}\n\n    We use the British National Corpus (BNC) as a diverse, balanced, compact, and publicly available monolingual English corpus. %\\footnote{\\url{https://ota.bodleian.ox.ac.uk/repository/xmlui/handle/20.500.12024/2554}}\n    BNC is comprised of both written and spoken language with a total of 100 million words. \n    The manually curated content contains a wide range of British English from the late $20^\\textrm{th}$ century -- newspapers, journals, books (academic and fiction), letters, essays, unscripted informal conversations or transcribed business meetings, radio shows or phone calls. %The written sources were selected according to three main selection features: their domain (subject field), time and medium (book, periodical, etc.), whereas the spoken parts were selected based on demographic or context-governed criteria.\n    The written part makes up approximately 90\\% of the corpus and the remaining 10\\% contains the transcribed speech.\n    The sources are truncated to contain at most 45\\,000 words to ensure greater diversity within the limited amount of 100 million words.\n\n    \\paragraph{Creation.} The process of creating the BNC is extensively described in its documentation on the website.\\footnote{\\url{https://ota.bodleian.ox.ac.uk/repository/xmlui/handle/20.500.12024/2554}} It was created by the so called `BNC Consortium' led by Oxford University Press, and including major dictionary publishers Longman and Larousse Kingfisher Chambers; academic research centres at Oxford University Computing Services, the University Centre for Computer Corpus Research on Language (UCREL) at Lancaster University, and the British Library's Research and Innovation Centre. The purpose of the British National Corpus project was to construct a balanced and representative sample of current British English at the time. It was created over a period of four years and was a result of careful planning and data selection across a number of selection criteria (domain, time, medium, level) with proportions in the corpus designed to reflect the proportions found in real language use. It is widely acknowledged that the BNC has been a major influence on the construction of language corpora \\citep{WheredidweGoWrongARetrospectiveLookattheBritishNationalCorpus}. One downside of the BNC is that it does not reflect anything occurring to English language and the world in the 21st century, but still no better alternatives of the same size and quality exists. In addition, BNC was used as a model for creating representative corpora for other languages: e.g., Turkish \\citep{aksan-etal-2012-construction}.\n\n    \\paragraph{Version.} We use the third release of the corpus, BNC XML Edition (2007), which is the final revision of the texts compiled from 1991 to 1994 \\citep{bnc2007british}.\n    The XML edition did not get any additional content on top of the original text samples, but it got some minor corrections, more metadata and it is supplied in a convenient XML format.\n\n    \\subsection{Preprocessing}\n    \\label{sec:preprocessing}\n    \n    We convert the XML version of BNC into the Markdown format,\\footnote{\\url{https://daringfireball.net/projects/markdown/}} to make it human-readable and usable as a direct raw-text input of a language model. On top of that, it can also preserve some meta-information encoded in the original XML format. Short samples from the preprocessed corpus can be found in \\cref{sec:bnc-samples}. After preprocessing, the articles are randomly placed into a training split and a development split. The proportions of both splits are given in \\cref{tab:bnc}.\n    \n    \\paragraph{Composition.} BNC is hierarchically composed of the following text units: words, sentences, paragraphs and articles. We preserve the sentence information by storing each sentence on a separate line; paragraphs are divided by a blank line and an article always starts with a top-level header. The word-tokens are intentionally not preserved -- instead, we heuristically detokenize the text to move it towards the natural text distribution. BNC includes information about the original whitespace, but we found it unreliable in some cases, necessitating the use of heuristics.\n\n    \n    \\paragraph{Other metadata.} Other meta information available in our Markdown version is as follows:\n    \\begin{enumerate}\n        \\item \\textbf{Headers:} We keep the headers together with their level by converting them to the atx-style format prefixed by hash symbols `\\texttt{\\#}'.\n        \\item \\textbf{Speakers:} The spoken part of BNC is divided into speech turns, each accompanied by a speaker identifier. We maintain this information by formatting each speech turn as `\\texttt{\\{name\\}:\\textvisiblespace'\\{turn\\}'}'.\n        \\item \\textbf{Quotes:} Markdown also allows us to keep the special quoted text by using a prefix `\\texttt{>\\textvisiblespace}'.\n        \\item \\textbf{Lists:} The XML format contains special tags for lists and their respective elements, we use the `\\texttt{-\\textvisiblespace\\{element\\}}' notation to encode these text blocks.\n        \\item \\textbf{Incomprehensible speech:} Some words or phrases could not be transcribed because they were illegible or inaudible. Since completely omitting such text would result in ungrammatical sentences, we mark these segments with a special `\\texttt{[UNK]}' token.\n    \\end{enumerate}\n    \n    \\noindent\n    Not all of this additional information is of use for the language models tested in this article, but it can be easily filtered out when needed. We preserve it to make this corpus more versatile.\n    \n    \\section{Model architecture}\n    \\label{sec:architecture}\n    \n    \n    \\begin{figure}[t]\n        \\centering\n        \\includegraphics[width=\\columnwidth]{figures/deberta_layer.pdf}\n        \\caption{A simplified diagram of one layer in our LTG-BERT language model, which illustrates the changes made to the standard Transformer architecture -- NormFormer layer normalization, GEGLU activation function and disentangled attention.}\n        \\label{fig:architecture}\n    \\end{figure}\n    \n    We slightly depart from the typical \\textit{post-norm} Transformer architecture \\citep{NIPS2017_3f5ee243} used by BERT \\citep{devlin-etal-2019-bert}, as illustrated in \\cref{fig:architecture}. Preliminary experiments with this model showed that it tends to unpredictably diverge in the later stages of training. This behavior has been noted in previous work on large LMs \\citep{liu-etal-2020-understanding} and accordingly, we follow some of the recent improvements of Transformer.\n    \n    \\paragraph{NormFormer.} \\textit{Pre-norm} variation of the Transformer has been shown to lead to more stable convergence with slightly degraded performance \\citep{nguyen-salazar-2019-transformers}. \\newcite{shleifer2022normformer} claimed to mitigate this degradation by introducing an additional layer normalization operation. For these reasons, we decided to use their so-called \\textit{NormFormer} architecture to stabilize the training.\\footnote{They also proposed some additional improvements -- \\textit{head scaling} and \\textit{residual scaling}, but we did not experience any performance benefits from these changes.}\n    \n    \\paragraph{GEGLU activation function,} proposed in \\newcite{DBLP:journals/corr/abs-2002-05202}, enhances the expressiveness of the original Transformer feed-forward modules by redefining them as\n    $$\\textrm{FF}_{\\textrm{GEGLU}}(\\bm{x}) = \\left(\\textrm{GELU}(\\bm{xW}_1) \\odot \\bm{xW}_2\\right)\\bm{W}_3,$$ where $\\bm{W}_i$ are weight matrices\\footnote{The bias terms are omitted for brevity.} and GELU is the Gaussian Error Linear Unit \\citep{Hendrycks2016}. Note that this formulation involves three linear transformations instead of two, we therefore lower the intermediate hidden size by $\\nicefrac{2}{3}$ to keep the number of parameters the same.\n    \n    \\paragraph{Disentangled attention.} The original Transformer formulation \\citep{NIPS2017_3f5ee243} fuses the content and positional information together in the first embedding layer and calculates the (unnormalized) attention score between each pair of tokens $\\bm{x}_i$ and $\\bm{x}_j$ as\n    $$\\bm{A}_{i,j} = \\frac{\\bm{Q}_i\\bm{K}_j^\\intercal}{\\sqrt{d}},$$\n    where $\\bm{Q}$ and $\\bm{K}$ are the query-key linear transformations of $\\bm{x}$. \n    \n    \n    \\newcite{he2021deberta} proposed to \\textit{disentangle} the content and positional information. The content representations are incrementally built by the Transformer layers and the position is encoded by one shared relative positional embedding matrix $P \\in \\mathbb{R}^{(2L - 1) \\times d}$, where $L$ is the maximal input length.\\footnote{Tokens at positions $i$ and $j$ have relative positional embedding at the $(L - i + j)^\\textrm{th}$ row of $P$, denoted as $P_{i,j}$.} This is supposed to offer greater expressivity as each layer can access these two parts directly. The attention scores are then calculated as a sum of three distinct parts: \\textit{content-to-content}, \\textit{content-to-position} and \\textit{position-to-content} attention -- formally, the attention scores are defined as\n    \n    $$\\bm{A}_{i,j} = \\frac{\\prescript{c\\!}{}{\\bm{Q}}_i\\prescript{c\\!}{}{\\bm{K}}_j^\\intercal + \\prescript{c\\!}{}{\\bm{Q}}_i\\prescript{p\\!}{}{\\bm{K}}_{i,j}^\\intercal + \\prescript{p\\!}{}{\\bm{Q}}_{j, i}\\prescript{c\\!}{}{\\bm{K}}_j^{\\intercal}}{\\sqrt{3d}},$$\n    where $\\prescript{c\\!}{}{\\bm{Q}}$ and $\\prescript{c\\!}{}{\\bm{K}}$ are linear transformations of the \\textit{content} vectors and $\\prescript{p\\!}{}{\\bm{Q}}$ and $\\prescript{p\\!}{}{\\bm{K}}$ are linear transformations of the relative \\textit{positional} embedding $P_{i,j}$. We share the parameters of the content and positional transformations, $\\prescript{c\\!}{}{\\bm{Q}} = \\prescript{p\\!}{}{\\bm{Q}}$ and $\\prescript{c\\!}{}{\\bm{K}} = \\prescript{p\\!}{}{\\bm{K}}$, to not increase the model size while achieving comparable performance \\citep{he2021deberta}.\n    \n    \n    \\paragraph{Initialization scaling.} \\newcite{https://doi.org/10.48550/arxiv.2204.06644} found that we can further stabilize the Transformer architecture by gradually scaling down its feed-forward (FF) weight matrices. Following \\newcite{nguyen-salazar-2019-transformers}, we first initialize all weight matrices $\\bm{W}$ by sampling from:\n    $$\\bm{W}_{i,j} \\sim \\mathcal{N}\\!\\left(0,\\, \\sqrt{\\frac{2}{d + 4d}} \\right),$$\n    where $d$ is the hidden dimension.\\footnote{This formula is roughly equal to the universal BERT initialization range of $0.02$ for $d=1024$.}\n    Then all three weight matrices in a FF module at layer $l$ are scaled down by a factor of $\\nicefrac{1}{\\sqrt{2(l+1)}}$.\n    \n    \n    \n    \\section{Training objectives}\n    \\label{sec:objectives}\n    \n    The fixed corpus, tokenizer and fine-tuning procedures establish a controlled test bed for a comparative study of training objectives proposed in the past. The original BERT model is trained via two self-supervised training objectives -- masked language modeling (MLM) and next sentence prediction (NSP). We evaluate five different configurations of these objectives (three for MLM and two for NSP), as further detailed below.  \n    \n    \\subsection{Masked language modeling (MLM)}\n    \\label{sec:mlm}\n    \n    Unlike the traditional auto-regressive language models, the \\textit{Bidirectional} Encoder Representations from Transformers (BERT) learn a \\textit{bidirectional} contextualized representation for each token in a text segment. This is done by randomly selecting 15\\% of subword tokens (excluding the special tokens). Out of these, 80\\% are masked, 10\\% randomly replaced and 10\\% are left untouched. The language model is then trained to jointly predict the original state of the selected units. We investigate three common choices of the masked text units:\n    \n    \\begin{enumerate}\n        \\item \\textbf{Subwords}. As proposed in the seminal work by \\newcite{devlin-etal-2019-bert}, every subword is masked independently with 15\\% probability to model its bidirectional dependencies.\n\n        \\item \\textbf{Whole words}. This method was also implemented by \\newcite{devlin-etal-2019-bert}, after the publication of their original paper with subword masking. The motivation for this approach is that partially masked multi-subword word units are often easily decoded without any need for non-local contextual information; masking the whole multi-subword unit should force the model to build longer-range non-local dependencies. %Even though the released models with whole-word masking achieve better performance, to the best of our knowledge, the only evaluation of this masking scheme on English can be found in \\newcite{levine2021pmimasking}.\n        \n        \\item \\textbf{Spans}. The third method further follows the direction of whole-word masking and generalizes it to masking of random \\textit{spans} of subwords. More specifically, SpanBERT \\citep{joshi-etal-2020-spanbert} iteratively samples random spans until 15\\% of subwords are masked. For each span, it first samples its length from $\\textrm{Geo}(p)$, where $p=\\nicefrac{1}{3}$.\\footnote{To ensure that the sampled length is not too large, we take the sampled value modulo 10. The expected length of a masked span is then approximately equal to 2 with $p=\\nicefrac{1}{3}$.} Then the starting subword of the masked span is chosen from a uniform distribution.\n \\end{enumerate}\n    \n    \\subsection{Next sentence prediction (NSP)}\n    \\label{sec:nsp}\n\n    Masked language modeling is a token-level training objective that trains the model to learn rich token representations.\n    Yet, some downstream tasks need a single sentence-level representation instead. To also learn these, researchers have designed a number of additional semi-supervised training objectives. On the other hand, \\newcite{DBLP:journals/corr/abs-1907-11692} argue that NSP objectives do not help the downstream performance and they can thus be dropped in favour of a simpler optimization process with a single MLM training objective. To test these hypotheses, we experiment with two NSP objectives:\n    \n    \\begin{enumerate}\n        \\item \\textbf{Document discrimination.} \\newcite{devlin-etal-2019-bert} sample two text segments and then train the model with a second discriminative loss function, which predicts whether the two segments are continual or randomly taken from two different documents.\n        \n        \\item \\textbf{Sentence-order discrimination.} \\newcite{Lan2020ALBERT} argue that the document discrimination is too easy as the language models only have to compare the topic of the two segments to achieve a good performance in this task. Instead, they propose to predict whether the two segments are in the correct order or whether they are swapped. Thus, the sentence-order loss forces the neural network to model inter-sentence coherence and this is believed to lead to a better downstream performance.\n    \\end{enumerate}\n    \n    \n    \n    \\section{Evaluation metrics}\n    \\label{sec:metrics}\n    \n    We use three conceptually different methods for evaluating the amount of linguistic knowledge acquired by the BNC language models. 1) The (Super)GLUE datasets test the ability of the model to adapt to various NLU tasks by further optimizing the whole pre-trained model, 2) edge probing tasks evaluate how much linguistic information one can extract from a frozen pre-trained model and 3) BLiMP utilizes the intrinsic ability of the pre-trained network to model language and probes its knowledge without any additional training. We further elaborate on each of these below. \n    \n    \\subsection{(Super)GLUE}\n\n    GLUE \\citep{wang-etal-2018-glue} and SuperGLUE \\citep{NEURIPS2019_4496bf24} have become a de-facto standard for evaluating the language understanding capabilities of language models. Accordingly, we also choose to fine-tune our language models on these NLU tasks to measure their linguistic and transfer-learning performance. We give more technical details about our implementation of (Super)GLUE fine-tuning in \\cref{sec:superglue-details}.\n\n    We exclude the Winograd schema datasets, WNLI and WSC, because they require a complete reformulation to get past the trivial most-frequent baseline \\citep{kocijan-etal-2019-surprisingly}. The remaining 14 (Super)GLUE datasets measure performance on these tasks: %\\textbf{natural language inference} (MNLI, QNLI, RTE), \\textbf{linguistic acceptability} (CoLA), \\textbf{sentiment analysis} (SST-2) and \\textbf{semantic similarity} (MRPC, QQP, STS-B).\n\n    \\begin{itemize}\\itemsep0em \n    \\item \\textbf{Inference}: CB, MNLI, QNLI, RTE.\n    \\item \\textbf{Linguistic acceptability}: CoLA.\n    \\item \\textbf{Sentiment analysis}: SST-2.\n    \\item \\textbf{Semantic similarity}: MRPC, QQP, STS-B.\n    \\item \\textbf{Word sense disambiguation}: WiC.\n    \\item \\textbf{Question answering}: BoolQ, COPA, MultiRC, ReCoRD.\n    \\end{itemize}\n\n    \n    \\subsubsection{HANS}\n    Deep learning systems are (by design) prone to finding spurious correlations in the training data.\n    These heuristics can often be successfully employed for the evaluation data, as well -- thus, one has to be careful when implying that a higher score on a benchmark shows a deeper understanding of the tested model.\n    \\newcite{mccoy-etal-2019-right} tried to evaluate to what extent language models rely on spurious heuristics to solve NLI tasks. They identified a set of fallible syntactic heuristics and designed a test set where these `shortcuts' should fail -- Heuristic Analysis for NLI Systems (HANS). We adopt their approach and test models that have been fine-tuned on MNLI.\n    \n    \n    \\subsection{Edge probing}\n    GLUE tasks measure the ability of a LM to be fine-tuned on a sentence-level NLU problem. To get a more comprehensive picture of LM performance, one can also \\textit{probe} the word-level contextualized representations, measuring how much syntactic or semantic information can be extracted.\n\n    \\newcite{tenney2018what} devised a simple approach of probing for a diverse set of linguistic phenomena called \\textit{edge probing}. They reformulate traditional NLP tasks as \\textit{span classification}: part-of-speech tagging can be viewed as classification of word-spans and semantic role labeling becomes a classification of pairs of spans: predicate-span and argument-span. In the following, we will probe our models for five basic tasks: part-of-speech tagging (POS), dependency parsing (DP), semantic role labeling (SRL), named-entity recognition (NER) and coreference resolution (CR). Note that the model only learns to \\textit{classify} each span provided to the model as gold data. This substantially simplifies some of the tasks, for example SRL. Please refer to \\cref{sec:edge-probing-details} for the implementation details of edge probing.\n \n    \n    \\subsection{BLiMP}\n    \\label{sec:blimp}\n    \n    One disadvantage of the aforementioned evaluation metrics is that the results are skewed by the second-stage supervised training, which makes it problematic to disentangle the prior knowledge of a language model from the acquired knowledge \\cite{10.1162/coli_a_00422}. In contrast, the Benchmark of Linguistic Minimal Pairs \\citep[BLiMP;][]{warstadt-etal-2020-blimp-benchmark} attempts to measure the linguistic knowledge of a language model in a zero-shot manner -- without any additional training. The dataset consists of 67\\,000 sentence pairs; each pair differs minimally on the surface level, but only one of the sentences is grammatically valid. We can use the intrinsic ability of language models to assign a probability to every sentence and test how often a language model assigns a higher probability to the correct sentence. \\cref{sec:blimp-details} gives more details about ranking the likelihood of sentences according to the raw output of a masked language model.\n    \n    \\section{Experiments}\n\n    We conduct a number of experiments in this section. First, we compare different training hyperparameters and model configurations described in \\cref{sec:architecture}. Then, using the overall best training setting, we make a comparative study of training objectives (\\cref{sec:objectives}). Finally, we investigate the sampling efficiency of our proposed language model and we compare BNC with a Wikipedia \\& BookCorpus subset of the same size. These results can then be used as a baseline performance of BNC-BERT in future studies.\n    \n    The central model used in the experiments is a \\textit{base}-sized Transformer -- 12 encoder layers with hidden size 768 and 12 attention heads (more details in \\cref{sec:hyperparameters}). All reported models utilize the same cased WordPiece tokenizer \\citep{https://doi.org/10.48550/arxiv.1609.08144} with a vocabulary size of $2^{14} = 16\\,384$ trained with the BNC dataset (\\cref{sec:tokenizer}). This goes against the trend of increasing the subword vocabulary in recent work,\\footnote{BERT \\citep{devlin-etal-2019-bert} uses 28\\,996 tokens, RoBERTa \\citep{DBLP:journals/corr/abs-1907-11692} 50\\,265 and in 2021, DeBERTa \\citep{he2021deberta} used a vocabulary of 128\\,100 subwords.} but a larger vocabulary size would lead to a lot of infrequent tokens within our limited corpus -- we roughly follow \\newcite{gowda-may-2020-finding} and \\textit{`\\dots use the largest possible BPE vocabulary such that at least 95\\% of classes have 100 or more examples in training.'}\n    \n    \nSince our aim is to train models comparable to $\\textrm{BERT}_\\textit{base}$, we train for the same amount of sampled tokens. \\citet{devlin-etal-2019-bert} trained on 1M batches of 128K tokens, we use 31\\,250 training steps with batch size of 4M tokens to parallelize and accelerate the process. Also, similarly to \\newcite{devlin-etal-2019-bert}, we use sequence length of 128 tokens in the first 90\\% of training and a larger sequence length of 512 only in the last 10\\% of steps. We deliberately do not compare against more recent models, which are trained for much longer to achieve slightly better performance: RoBERTa is trained on $16\\times$ more training samples, for example.\\footnote{500K steps with 8\\,192 segments of length 512, according to \\cite{he2021deberta}.}\n    \n  \n\\subsection{Comparison of model architectures and training settings}\n    \n    In order to establish a strong baseline, we evaluate the proposed changes from \\cref{sec:architecture} and other training configurations. We present the results in \\cref{tab:comparison}, where we compare the final model with all changes applied and models with one of those modifications removed. These training choices turned out to be the most important:\n    \\begin{itemize}\\itemsep0em\n        \\item Both the post-norm and pre-norm transformer variants perform substantially worse than the NormFormer-like layer normalization \\citep{shleifer2022normformer}. Both of them also lead to less stable and only slightly faster training.\n        \\item Absolute positional embeddings seem to be less adaptable for fine-tuning but perform better on language modeling itself, as can be seen on the BLiMP results. We hypothesize that this is caused by more accurate estimation of probabilities of the first few words in a sentence. The simpler absolute embeddings also lead to the greatest reduction of training time. We choose the slower relative positional embeddings despite this fact to increase the performance on (Super)GLUE tasks.\n        \\item We observe that setting the weight decay correctly is crucial for masked language modeling. The default weight decay value found in \\newcite{devlin-etal-2019-bert}, $0.01$, performs substantially worse on all tested tasks. We use a higher decay value of $0.1$ to boost performance, this value is most likely strongly correlated with the corpus size we use here. This suggests that previous findings of inferior performance of LMs pre-trained on small corpora might be caused by insufficient hyperparameter search.\n        \\item As expected, the AdamW optimizer \\citep{loshchilov2018decoupled} behaves poorly in our highly parallel training regime. Our study successfully replicates the reported performance of the LAMB optimizer \\citep{You2020Large}, which we thus use in all other experiments. \n    \\end{itemize}\n\n   \n{\\renewcommand{\\arraystretch}{1.2}\n\\begin{table}[t]\n\\resizebox{\\columnwidth}{!}{%\n\\begin{tabular}{@{}lrrrr@{}}\n\\toprule\n\\multirow{2}{*}{\\textbf{Model}} & \\multirow{2}{*}{\\textbf{MNLI}} & \\textbf{Edge} & \\multirow{2}{*}{\\textbf{BLiMP}} & \\textbf{Training} \\\\ \n & & \\textbf{probing} & &  \\textbf{time} \\\\ \\midrule\nLTG-BERT               & \\textbf{85.1}$^{\\pm0.2}$ & \\textbf{95.3}$^{\\pm0.1}$ & 83.4 & 8h 13min \\\\ \\midrule\nw/\\hphantom{o} post-norm (0.005) & $-0.5^{\\pm0.2}$ & $-0.6^{\\pm0.1}$ & $-0.1$ & \\hphantom{0h }$-$22min\\\\\nw/\\hphantom{o} pre-norm (0.005)  & $-1.3^{\\pm0.1}$ & $-0.2^{\\pm0.1}$ & $-0.9$ & \\hphantom{0h }$-$35min \\\\\nw/\\hphantom{o} GELU activation      & $-0.3^{\\pm0.3}$ & \\hphantom{$-$}\\textbf{0.0}$^{\\pm0.1}$ & $-0.1$ & \\hphantom{1h1 }$-$6min \\\\\nw/\\hphantom{o} absolute pos. emb.   & $-1.1^{\\pm0.2}$   &   $-$\\textbf{0.1}$^{\\pm0.1}$    &       $+$\\textbf{0.6}  & $-$\\textbf{2h 16min} \\\\\nw/o FF init. scaling     &  $-0.3^{\\pm0.2}$     &     $-$\\textbf{0.1}$^{\\pm0.1}$     &   $+0.1$ & \\hphantom{$-0h 0$}0min \\\\\nw/\\hphantom{o} learnt FF biases        &  $-0.3^{\\pm0.2}$     &     \\hphantom{$-$}\\textbf{0.0}$^{\\pm0.1}$     &  $-0.1$ & \\hphantom{0h 0}$+$9min  \\\\\nw/\\hphantom{o} 0.01 WD (0.005)   &  $-1.4^{\\pm0.1}$     &     $-0.2^{\\pm0.1}$       &     $-0.7$ & \\hphantom{0h 0 }$-$1min  \\\\\nw/\\hphantom{o} linear schedule      &  $-0.5^{\\pm0.2}$     &     \\hphantom{$-$}\\textbf{0.0}$^{\\pm0.1}$       &     $-0.2$ & \\hphantom{$-$0h 0}0min  \\\\\nw/\\hphantom{o} AdamW (0.001)     & $-0.9^{\\pm0.2}$ & $-0.2^{\\pm0.1}$ & $-0.5$    & \\hphantom{0h }$-$11min         \\\\ \\bottomrule\n\\end{tabular}%\n}\n\\caption{Comparative study of different architectural and training settings. The first row shows the performance of the final model with all improvements applied and the following rows give the relative changes in performance when one of the changes is not applied -- for example, the second row tests swapping the NormFormer-like normalization with the `post-norm' normalization. Some runs diverged with the default learning rate of $0.01$ and had to be run again with a lower value (denoted in parentheses). `WD' stands weight decay and `FF' is an abbreviation for the feed-forward modules. We report the mean and standard deviation statistics across five runs, if applicable, and boldface all run within 1 standard deviation from the best result.}\n\\label{tab:comparison}\n\\end{table}\n}\n\n{\\renewcommand{\\arraystretch}{1.15}\n\\begin{table*}[!th]\n    \\resizebox{\\textwidth}{!}{%\n    \\begin{tabular}{@{}l@{\\hspace{1.0em}}ccccc@{\\hspace{2.5em}}l@{\\hspace{2.5em}}c@{\\hspace{2.0em}}c@{}}\n    \\toprule\n    \\multicolumn{1}{@{}l}{\\multirow{2}{*}{\\textbf{Model (variant)}}} & \\multicolumn{5}{@{}c@{}}{\\hspace{-3em}\\crulefill~~\\textbf{GLUE}~~\\crulefill\\hspace{2em}}                                       & \\multirow{2}{*}{\\textbf{HANS}} & \\textbf{Edge} & \\multirow{2}{*}{\\textbf{BLiMP}} \\\\\n    \\multicolumn{1}{l}{}                                & \\footnotesize{\\textbf{MNLI}} & \\footnotesize{\\textbf{MRPC}} & \\footnotesize{\\textbf{QNLI}} & \\footnotesize{\\textbf{SST-2}} & \\footnotesize{\\textbf{Average}} &                                &      \\textbf{probing}                                  &                                 \\\\ \\midrule\n    \\multicolumn{9}{@{}c@{}}{\\raisebox{0.8ex}{\\footnotesize{Wikipedia + BookCorpus \\citep[3000M words;][]{devlin-etal-2019-bert}}}} \\\\\n    \\multicolumn{1}{@{}l}{$\\textrm{BERT}_\\textit{base, cased} {}^\\dagger$}                                                       & 84.4\\hphantom{$^{\\pm0.1}$}          & 86.7\\hphantom{$^{\\pm0.1}$}        & 88.4\\hphantom{$^{\\pm0.1}$}          & 92.7\\hphantom{$^{\\pm0.1}$}         &      88.1\\hphantom{$^{\\pm0.1}$}           &               \\textbf{69.0}\\hphantom{$^{\\pm0.1}$}                 & 93.9\\hphantom{$^{\\pm0.1}$}                                   & \\textbf{84.2}                            \\\\\n        \\multicolumn{1}{@{}l}{$\\textrm{BERT}_\\textit{base, cased}\\,\\,\\textrm{(our eval.)}$}                                                & 83.6$^{\\pm0.2}$ &\t84.6$^{\\pm0.5}$\t& 90.8$^{\\pm0.1}$ &\t91.9$^{\\pm0.4}$ & 87.8$^{\\pm0.3}$ & 61.8$^{\\pm1.5}$ &\t93.8$^{\\pm0.2}$\t& \\textbf{84.2}                            \\\\ \\midrule\n    \\multicolumn{9}{@{}c@{}}{\\raisebox{0.8ex}{\\footnotesize{Wikipedia + BookCorpus (100M words)}}} \\\\\n\n LTG-BERT (subword masking)         & 84.2$^{\\pm0.1}$ & 84.3$^{\\pm0.7}$ & 90.8$^{\\pm0.3}$ & 92.1$^{\\pm0.5}$ &  87.8$^{\\pm0.5}$ & 62.5$^{\\pm1.7}$ & \\textbf{95.3}$^{\\pm0.1}$ & 82.0 \\\\ \\midrule\n    \n    \\multicolumn{9}{@{}c@{}}{\\raisebox{0.8ex}{\\footnotesize{British National Corpus (100M words)}}} \\\\\n\n    LTG-BERT (subword masking)         & \\textbf{85.1}$^{\\pm0.2}$ & 85.0$^{\\pm0.9}$ & 90.0$^{\\pm0.3}$ & \\textbf{92.7}$^{\\pm0.4}$ & 88.2$^{\\pm0.5}$ & 64.4$^{\\pm1.3}$ & \\textbf{95.3}$^{\\pm0.1}$ & 83.4 \\\\\n    LTG-BERT (whole-word masking)    & 84.9$^{\\pm0.2}$ & 85.5$^{\\pm0.9}$ & 90.6$^{\\pm0.3}$ & \\textbf{92.7}$^{\\pm0.2}$ & 88.4$^{\\pm0.5}$ & 63.7$^{\\pm0.8}$ & \\textbf{95.3}$^{\\pm0.1}$ & 80.1 \\\\\n    LTG-BERT (span masking)       & \\textbf{85.1}$^{\\pm0.2}$ & \\textbf{87.5}$^{\\pm0.9}$ & \\textbf{91.5}$^{\\pm0.2}$ & \\textbf{92.8}$^{\\pm0.5}$ & \\textbf{89.2}$^{\\pm0.5}$ & 65.6$^{\\pm0.5}$ & \\textbf{95.2}$^{\\pm0.1}$ & \\textbf{84.2} \\\\ \\midrule\nLTG-BERT (subword + document NSP) & \\textbf{85.2}$^{\\pm0.3}$ & 86.5$^{\\pm0.8}$ & 90.3$^{\\pm0.2}$ & 92.2$^{\\pm0.4}$ & 88.6$^{\\pm0.5}$ & 60.5$^{\\pm1.2}$ & \\textbf{95.3}$^{\\pm0.1}$ & 83.3 \\\\\nLTG-BERT (subword + order NSP)  & 84.7$^{\\pm0.1}$ & 85.9$^{\\pm0.6}$ & 90.4$^{\\pm0.2}$ & 92.1$^{\\pm0.2}$ &  88.3$^{\\pm0.4}$ & 64.2$^{\\pm1.9}$ & 95.1$^{\\pm0.1}$ & 82.2 \\\\ \\midrule\nLTG-BERT (subword + $2\\times$ steps) & \\textbf{85.2}$^{\\pm0.2}$ & 86.5$^{\\pm0.8}$ & 90.3$^{\\pm0.3}$ & \\textbf{92.3}$^{\\pm0.6}$ & 88.6$^{\\pm0.5}$ & 65.3$^{\\pm1.1}$ & \\textbf{95.3}$^{\\pm0.1}$ & 83.5 \\\\\nLTG-BERT (subword + $\\nicefrac{1}{2}\\times$ steps)   & 84.4$^{\\pm0.3}$ & 86.3$^{\\pm1.1}$ & 90.4$^{\\pm0.2}$ & \\textbf{92.8}$^{\\pm0.4}$ & 88.5$^{\\pm0.6}$ & 62.4$^{\\pm0.8}$ & \\textbf{95.2}$^{\\pm0.1}$ & 83.5 \\\\\nLTG-BERT (subword + $\\nicefrac{1}{4}\\times$ steps)  & 83.8$^{\\pm0.2}$ & 85.3$^{\\pm0.8}$ & 89.1$^{\\pm0.2}$ & 91.7$^{\\pm0.4}$ & 87.5$^{\\pm0.5}$ & 58.6$^{\\pm1.3}$ & 95.0$^{\\pm0.1}$ & 83.2 \\\\ \\midrule\n\\multicolumn{1}{@{}l}{Random initialization}                        & 59.5$^{\\pm0.5}$ &\t68.5$^{\\pm1.4}$ &\t63.8$^{\\pm0.2}$ & 82.2$^{\\pm0.7}$ & 68.5$^{\\pm0.8}$ &  49.7$^{\\pm0.3}$ &\t73.1$^{\\pm0.4}$\t& 50.0                            \\\\ \\bottomrule\n    \\end{tabular}%\n    }\n    \\caption{Summary of the experimental results. We show the results on the 4 GLUE tasks with known development results from \\newcite{devlin-etal-2019-bert} and their average; then the accuracy on HANS, the average of all 5 edge probing tasks and 67 BLiMP tasks. $^\\dagger$The $\\textrm{BERT}_\\textit{base, cased}$ results are shown primarily for reference, they come from these sources: partial development GLUE scores from \\newcite{devlin-etal-2019-bert}, edge probing from \\newcite{tenney2018what}, HANS from \\newcite{bhargava-etal-2021-generalization} and BLiMP from \\newcite{salazar-etal-2020-masked}. We also add the $\\textrm{BERT}_\\textit{base, cased}$ results from our evaluation scripts for more fair and accurate comparison. We present the mean and standard deviation statistics over 5 evaluation runs and boldface all run within 1 standard deviation from the best result. The detailed results can be found in \\cref{sec:detailed-results}.}\n    \\label{tab:results}\n\\end{table*}\n}    \n\n    \\noindent\n    The other changes bring more marginal gains -- all three tested modifications of the feed-forward layers work slightly better: 1) using GEGLU activation function instead of GELU, 2) initializing the feed-forward layers with incrementally lower weight norms, and 3) not using any bias parameters in these layers. The last tested change shows that cosine learning rate decay \\citep{https://doi.org/10.48550/arxiv.2112.11446} performs better than the standard linear weight decay.\n\n    \\subsection{Training objective comparison}\n\n    \\paragraph{Masked language modeling.}\n    First of all, we compare the three masking methods described in \\cref{sec:mlm}: subword, whole-word and span masking. The summary of the results is given in \\cref{tab:results}, more detailed evaluation in \\cref{sec:detailed-results}. Overall, the span-based masking performs marginally better than the other methods -- it shows a clear improvement on (Super)GLUE benchmarks over the simple subword masking, it generalizes the best according to the HANS score and it even matches the performance of $\\textrm{BERT}_\\textit{base}$ on the averaged BLiMP accuracy. All methods perform equally well on edge probing. Whole-word masking lacks on the BLiMP benchmark because the model is not expecting partially masked words that can occur in the evaluation (\\cref{sec:blimp}). The original subword masking strategy is still a competitive baseline and it might be preferred in practice due to its simple implementation.  %underperforms on all evaluation metrics and we do not recommend it for masked language modeling based on these experiments.\n    \n    \\paragraph{Next-sentence prediction.} Next, we experiment with combining an NSP task and simple subword masking. We hypothesize that a second training objective might extract more information from the limited BNC corpus, which would help with the downstream performance -- an opposite conclusion than \\newcite{DBLP:journals/corr/abs-1907-11692}. However, our hypothesis turns out to be wrong, according to the results in \\cref{tab:results}. The experiments agree with the design of latest masked language models -- next sentence prediction is an unnecessary training objective, at least for the tasks evaluated in this paper. It does not lead to substantially improved sentence representations even in a limited data regime. We can also see that the well-motivated order discrimination \\citep{Lan2020ALBERT}, proposed to solve the issues of document discrimination, actually leads to an overall worse performance. Hence we cannot recommend to complicate pre-training with a second training objective.\n    \n\n    \\subsection{Sampling efficiency}\n    \\label{sec:sample-efficiency}\n    \n    An important aspect of efficient language models is the number of training steps they require to reach a sufficient performance. So far, we have limited the size of the training corpus but kept the number of steps constant, set according to \\newcite{devlin-etal-2019-bert}. The results in \\cref{tab:results} suggest that increasing the steps two times does not lead to a noticeably better performance with BNC. Even more so, training for half the time turns out to be enough to get comparable performance. Yet, decreasing the training steps further starts to degrade the downstream results too much, as evidenced by the scores obtained with $\\nicefrac{1}{4}$ of the default steps.\n    \n    These results highlight the sampling inefficiency of current self-supervised language modeling methods, as even with $\\nicefrac{1}{4}$ steps, every token in BNC is seen about roughly 250 times during training.\\footnote{This value can be calculated from \\cref{tab:hyperparams}: these models are trained for 7\\,812 steps with 4\\,194\\,304 tokens per batch. \\cref{tab:bnc} shows that there are 131\\,392\\,103 subwords in the BNC train split.} We hope that a future work in this field will be able to learn from a smaller number of samples.\n    \n    \\subsection{100 million subset of Wikipedia \\& BookCorpus}\n    \\label{sec:wiki-subset-eval}\n    \n    Our last experiment evaluates how much does the careful curation of BNC help the downstream performance. To keep the comparability to BERT, we choose to pre-train on a random subset of Wikipedia and BookCorpus (with equal size to BNC, sampled document-wise); this corpus is constructed according to \\cref{sec:wiki}. Note that BNC is a corpus of British English compiled in 1990s so some evaluation tasks can be skewed against it -- for example QNLI, which is based on texts from Wikipedia. \\cref{tab:results} shows that a high-quality data source is not necessarily needed to learn from 100M words but better quality leads to a noticeable difference in downstream performance.\n    \n    \n    \n\n\\section{Conclusion}\nIn this paper, we evaluated how data-efficient masked language models can be. In particular, we trained a variety of models with different training objectives on the same training data: British National Corpus. Although small by modern standards (100M tokens), it is well balanced and carefully crafted to represent British English of the 20\\textsuperscript{th} century. On a variety of benchmarks, our models perform better than BERT\\textsubscript{\\textit{base}} trained on a much larger corpus. We believe that this limited data regime is beneficial for the development of efficient and reliable language models. Our finding also suggests that 100 million word tokens is enough to learn basic linguistic skills by current language modeling techniques, given that the data is carefully selected and balanced. To conclude, huge amounts of training data are not always necessary -- we should focus on more efficient training settings instead.\n\nWe showed that the next sentence prediction objective does not improve BERT-like models, confirming the findings in \\newcite{DBLP:journals/corr/abs-1907-11692}. In addition, the standard subword masking from \\newcite{devlin-etal-2019-bert} is consistently outperformed by the span masking method and the linguistic performance can be substantially increased by utilizing better neural architectures and training configurations. We release the code for training and using BERT-like models with the optimal architectural choices (according to our experiments) under the name LTG-BERT.\\footnote{\\url{https://github.com/ltgoslo/ltg-bert}}\n    \nThe presented results serve primarily as the foundation for future research on efficient language modeling. We hope our work shows the value of careful curation of representative corpora and will spark more interest in this area, where BNC can serve as an undemanding, replicable and openly-available training corpus.% In particular, we plan to experiment with languages other than English. Another venue for research is a deeper analysis of how language models are acquring different flavors of linguistic competence with the help of granular test sets like BLiMP.\n\n\\section{Limitations}\n\nFirst of all, our work only considers language modeling of English and does not provide results on any other language -- even though we hope that our conclusions could be useful for low-resource languages. Secondly, even though we found out that it is possible to train a competent language model with a small corpus, the training process still requires a similar amount of computational resources to models trained with larger corpora, as noted in \\cref{sec:sample-efficiency}. Finally, we evaluate mainly the linguistic knowledge of language models (\\cref{sec:metrics}), our conclusions might not apply for their general knowledge.\n\n\\section*{Acknowledgement}\nThe computations were performed on resources provided by Sigma2 -- the National Infrastructure for High Performance Computing and Data Storage in Norway.\n\n    \n            \n    \\clearpage\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Not all layers are equally as important: Every Layer Counts BERT}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nThis paper introduces a novel modification of the transformer architecture, tailored for the data-efficient pretraining of language models. This aspect is evaluated by participating in the BabyLM challenge, where our solution won both the \\textsc{strict} and \\textsc{strict-small} tracks. Our approach allows each transformer layer to select which outputs of previous layers to process. The empirical results verify the potential of this simple modification and show that not all layers are equally as important.\n\n\\end{abstract}\n\n\\section{Introduction}\n\nModern language models (LLMs), with their deep architectures and large parameter counts, have displayed outstanding performance on a wide range of tasks. Their ability to understand, generate, and manipulate human language has been groundbreaking \\citep{devlin-etal-2019-bert, 2020t5, NEURIPS2020_1457c0d6}. However, this success largely relies on \\textit{vast amounts of unsupervised data} that these models need for pretraining, requiring extensive computational power and time. While this is feasible for high-resource languages like English, it becomes a bottleneck for languages with limited data resources \\citep{joshi-etal-2020-state}. Moreover, the environmental and economic costs of such massive training regimens are growing concerns \\citep{strubell-etal-2019-energy, thompson2020computational}. \n\nThe BabyLM challenge tries to address these concerns by providing a shared experimental ground for efficient language modelling \\citep{warstadt-et-al-2023-babylm}. All models submitted to this shared task have to be trained on a restricted text corpus of 10M and 100M words -- in the \\textsc{strict-small} and \\textsc{strict} tracks, respectively. The challenge pushes the boundaries of what is possible with data-efficient language model pretraining.\n\n\\renewcommand{\\arraystretch}{1.2}\n\\begin{table}[t!]\n\\resizebox{\\columnwidth}{!}{%\n\\begin{tabular}{@{}l@{\\hspace{-2em}}rrr@{\\hspace{3em}}r@{}}\n{\\small\\textsc{strict-small} track (10M words)}\\\\\n\\toprule\n\\textbf{Model} & \\textbf{BLiMP} & \\textbf{GLUE} & \\textbf{MSGS} & \\textbf{Average}\\\\\\midrule\nELC-BERT \\textit{(ours)} & \\textbf{75.8}\t& \\textbf{73.7} & \\textbf{29.4} &\t\\textbf{65.9} \\\\%[0.75em]\nMLSM & 72.4\t& 70.6 & 17.2 &\t60.8 \\\\\nContextualizer & 74.3 & 69.6 & 12.7 & 60.5 \\\\\nBaby Llama & 69.8 & 67.6 & 24.7 & 60.1 \\\\\nToo Much Information & 75.7 & 70.9 & 3.9 & 59.9 \\\\\n\\bottomrule\n\\\\\n{\\small\\textsc{strict} track (100M words)}\\\\\n\\toprule\n\\textbf{Model} & \\textbf{BLiMP} & \\textbf{GLUE} & \\textbf{MSGS} & \\textbf{Average}\\\\\\midrule\nELC-BERT \\textit{(ours)} & \\textbf{82.8}\t& 78.3 & 47.2 &\t\\textbf{74.3} \\\\%[0.75em]\nContextualizer & 79.0\t& 72.9 & \\textbf{58.0} &\t73.0 \\\\\nBootBERT & 82.2 & \\textbf{78.5} & 27.7 & 70.2 \\\\\nMSLM & 76.2 & 73.5 & 21.4 & 64.4 \\\\\nBad babies & 77.0 & 67.2 & 23.4 & 63.4 \\\\\n\\bottomrule\n\\end{tabular}%\n}\n\\caption{\\label{tab:dynabench}\nThe DynaBench scores of the BabyLM challenge \\citep{warstadt-et-al-2023-babylm}, the table shows the top 5 submissions in the \\textsc{strict-small} and \\textsc{strict} tracks. Higher scores are better, the best results in each evaluation suite are boldfaced.\n}\n\\end{table}\n\nIn response to this challenge, we present a novel modification to the well-established transformer architecture \\citep{vaswani2017attention}.  Instead of traditional residual connections, our model allows each layer to \\textit{selectively} process outputs from the preceding layers. This flexibility leads to intriguing findings: not every layer is of equal significance to the following layers. Thus, we call it the `Every Layer Counts' BERT (ELC-BERT).\n\nThe BabyLM challenge provided us with a robust benchmark to evaluate the efficacy of ELC-BERT. Our approach emerged as the winning submission in both the \\textsc{strict} and \\textsc{strict-small} tracks (\\cref{tab:dynabench}), which highlights the potential of layer weighting for future low-resource language modelling.\n\n\\begin{figure*}[!t]\n    \\includegraphics[width=\\textwidth]{emnlp2023-latex/elc_bert.pdf}\n    \\caption{Every layer can select which outputs from previous layers it wants as its input, these heatmaps show the weights given to each previous layer output. The unit weights of the BERT model (and of any standard transformer-based model) are inferred from \\cref{eq:residual}. The right heatmap shows the $\\alpha$ weights of the normalized ELC-BERT variant; for clear visual comparison between the two models, we rescale the $\\alpha$ weights so that the $k$th row sums to $k$. Note that the layer 0 is the embedding layer, as in \\cref{eq:embedding}.}\n    \\label{fig:layer-weights}\n\\end{figure*}\n\nTransparent and open-source language modelling is necessary for safe future development of this field. We release the full source code, together with the pre-trained ELC-BERT models, online.\\footnote{\\url{https://github.com/ltgoslo/elc-bert}}\n\n\\section{Related work}\n\n\\paragraph{Residual and highway networks.} While the predecessor of residual models, highway networks, used a conditional gating mechanism to weigh layers \\citep{NIPS2015_215a71a1}, modern residual networks (including transformers) simply weigh all layers equally \\citep{he2016residual, vaswani2017attention}. Our work reintroduces layer weights into residual models -- but without the computational cost of a gating mechanism.\n\n\\paragraph{Layer importance.} The difference between various layers inside pre-trained language models has been extensively studied \\citep{jawahar-etal-2019-bert, tenney-etal-2019-bert, niu-etal-2022-bert}. Different layers process different linguistic phenomena, thus their \\textit{importance} for downstream tasks varies -- this has been successfully utilized by learning layer weights during finetuning, for example in ULMFiT \\citep{howard-ruder-2018-universal} or UDify \\citep{kondratyuk-straka-2019-75}. Following this direction, our system uses layer weights in the finetuning as well as in the pretraining phase.\n\n\\paragraph{ReZero transformer.} A related approach to ours was proposed by \\newcite{bachlechner2021rezero}. In that paper, the authors experimented with scaling the output of each layer. They showed that by initializing the scaling parameter to zero, their `ReZero transformer' model tends towards setting the scale to $\\nicefrac{1}{N}$ (where $N$ is the number of layers). Our approach can be considered as a generalization of this method -- in ELC-BERT, every layer weights the outputs of previous layers \\textit{individually}.\n\n\\section{ELC-BERT layer weighting} \\label{sec:methods}\n\nWe modify the residual connections inside the transformer architecture so that every layer can select which outputs from previous layers it wants to process -- instead of always taking a simple sum of all preceding layers, as done in the Transformer \\citep{vaswani2017attention} and in most works that use a variant of this architecture. This modification allows the model to form a complex inter-layer structure, as visible from \\cref{fig:layer-weights}.\n\n\\paragraph{Transformer definition.} To be more specific, we first formally define a \\textit{transformer encoder} as a function that maps subword indices $\\bm{x}$ onto subword probabilities $\\bm{y}$. First, $\\bm{x}$ is embedded into a vector representation $\\bm{h}^0_{\\text{out}}$, which is then processed by $N$ layers consisting of attention and multi-layer-perceptron (MLP) modules. Finally, $\\bm{y}$ is produced by processing the final hidden representation with a language-modelling head. Formally for $n \\in \\{1,\\,\\dots N\\}$:\n\\begin{align}\n    \\bm{h}^0_{\\text{out}} &\\gets \\operatorname{embedding}(\\bm{x}), \\label{eq:embedding}\\\\\n    \\bm{h}^n_{\\text{out}} &\\gets \\operatorname{att}(\\bm{h}^n_{\\text{in}}) + \\operatorname{mlp}\\!\\left({\\bm{h}^n_{\\text{in}} + \\operatorname{att}(\\bm{h}^n_{\\text{in}})}\\right), \\label{eq:transformer-layer}\\\\\n    \\bm{y} &\\gets \\operatorname{LM\\_head}(\\bm{h}^{N}_{\\text{out}}). \\label{eq:lm-head}\n\\end{align}\n\n\\paragraph{The original residual connection.} The original transformer definition by \\newcite{vaswani2017attention} can be recovered by simply assigning\n\\begin{align}\n    \\bm{h}^n_{\\text{in}} \\gets \\bm{h}^{n-1}_{\\text{out}} + \\bm{h}^{n-1}_{\\text{in}}.\n    \\label{eq:residual}\n\\end{align}\n\\noindent This recurrent assignment can also be rewritten as $\\bm{h}^n_{\\text{in}} \\gets \\sum^{n-1}_{i=0}{\\bm{h}^i_{\\text{out}}}$, which highlights the implicit assumption of residual models that the output from every previous layer is equally important. \n\n\\paragraph{Layer weighting.} In our formulation, we make two changes to the original definition: {\\color{Red}(i)} the residual connections in all MLP modules are removed, {\\color{Blue}(ii)} the input to every layer is a convex combination of outputs from previous layers. Specifically, we replace \\cref{eq:transformer-layer} and \\cref{eq:residual} by:\n\\begin{align}\n    \\bm{h}^n_{\\text{out}} &\\gets \\operatorname{att}(\\bm{h}^n_{\\text{in}}) + \\operatorname{mlp}\\!\\left({\\color{Red}\\operatorname{att}(\\bm{h}^n_{\\text{in}})}\\right), \\label{eq:new-transformer-layer}\\\\\n    \\bm{h}^n_{\\text{in}} &\\gets \\sum^{n-1}_{i=0}{{\\color{Blue}\\alpha_{i,n} }\\bm{h}^i_{\\text{out}}}, \\label{eq:new-residual}\n\\end{align}\n\n\\noindent where $\\sum^{n-1}_{i=0}{\\alpha_{i,n} = 1}$. This constraint is satisfied by a softmax transformation of the raw learnable layer weights $\\hat{\\alpha}_{*,n} \\in \\mathbb{R}^n$ into $\\alpha_{*,n}$. $\\hat{\\alpha}_{*,n}$ is initialized as a zero vector except for the value of $\\hat{\\alpha}_{n-1,n}$ set to one, to bias the weight towards the input from the previous layer. %We call this model \\textbf{ELC-BERT\\textsubscript{biased}} in all tables and subsequent sections.\n\n\\section{Training}\n\n\\paragraph{LTG-BERT backbone.} We base our models around LTG-BERT \\citep{samuel-etal-2023-trained}. This model has been specifically optimized for pretraining on small text corpora, similar to the one provided by BabyLM. We adopt all of their architectural modifications, their language modelling objective as well as all other pretraining settings. We also use the raw LTG-BERT (without our layer weighting) as a strong baseline in the following evaluation. Details on the pretraining hyperparameters can be found in \\cref{tab:hyperparams}.\n\n\\paragraph{BabyLM pretraining corpus.}\n\nWe pretrain all language models on a corpus from the BabyLM challenge \\citep{warstadt-et-al-2023-babylm}. The goal of this challenge is to shed more light on data-efficient language modelling and on the question of human language acquisition. Thus, the organizers have constructed a small-scale text corpus of the same type and quantity that children learn from.\n\nSpecifically, the shared task consists of three tracks: \\textsc{strict}, \\textsc{strict-small} and \\textsc{loose}. We participate in the first two tracks, where the submissions have to be pre-trained only on the BabyLM corpus, which corpus contains about 100M words in the \\textsc{strict} track and about 10M words in the \\textsc{strict-small} track. We adopt the preprocessing pipeline from \\newcite{samuel2023bootbert} for unifying the format of texts from this corpus.\n\n\\begin{table}\n\\resizebox{\\columnwidth}{!}{\n\\begin{tabular}{@{}l@{\\hspace{-4em}}rrrr@{}}\n{\\small\\textsc{strict-small} track (10M words)}\\\\\n\\toprule\n\\textbf{Model} & \\textbf{BLiMP} & \\textbf{Supp.} & \\textbf{MSGS} & \\textbf{GLUE}\\\\\n\\midrule\nOPT\\textsubscript{125m} & 62.6  & 54.7 & -0.64$^{\\pm 0.1}$ & 68.3$^{\\pm3.3}$ \\\\ % PT\\textsubscript{125m} & 62.6  & 54.7 & -0.64$^{\\pm 0.10}$ & 68.3$^{\\pm3.3}$\nRoBERTa\\textsubscript{base} & 69.5 & 47.5 & -0.67$^{\\pm 0.1}$ & 72.2$^{\\pm1.9}$ \\\\ % RoBERTa\\textsubscript{base} & 69.5 & 47.5 & -0.67$^{\\pm 0.12}$ & 72.2$^{\\pm1.9}$\nT5\\textsubscript{base}  & 58.8 & 43.9 & -0.68$^{\\pm 0.1}$ & 64.7$^{\\pm1.3}$ \\\\%[0.5em] 5\\textsubscript{base}  & 58.8 & 43.9 & -0.68$^{\\pm 0.07}$ & 64.7$^{\\pm1.3}$\nLTG-BERT\\textsubscript{small} & \\textbf{80.6} & \\textbf{69.8} & -\\textbf{0.43}$^{\\pm 0.4}$ & 74.5$^{\\pm 1.5}$ \\\\ % LTG-BERT\\textsubscript{small} & --- & --- & -\\textbf{0.43}$^{\\pm 0.35}$ & 74.5$^{\\pm 1.5}$\nELC-BERT\\textsubscript{small} & 80.5 & 67.9 & -0.45$^{\\pm 0.2}$ & \\textbf{75.3$^{\\pm 2.1}$} \\\\ % ELC-BERT\\textsubscript{small} & \\textbf{80.5} & \\textbf{67.9} & -0.45$^{\\pm 0.17}$ & \\textbf{75.3$^{\\pm 2.1}$}\n\\bottomrule\n\\\\\n{\\small\\textsc{strict} track (100M words)}\\\\\n\\toprule\n\\textbf{Model} & \\textbf{BLiMP} & \\textbf{Supp.} & \\textbf{MSGS} & \\textbf{GLUE}\\\\\n\\midrule\nOPT\\textsubscript{125m} & 75.3  & 67.8 & -0.44$^{\\pm 0.1}$ & 73.0$^{\\pm3.9}$ \\\\ %OPT\\textsubscript{125m} & 75.3  & 67.8 & -0.44$^{\\pm 0.09}$ & 73.0$^{\\pm3.9}$\nRoBERTa\\textsubscript{base} & 75.1 & 42.4 & -0.66$^{\\pm 0.3}$ & 74.3$^{\\pm0.6}$ \\\\ % RoBERTa\\textsubscript{base} & 75.1 & 42.4 & -0.66$^{\\pm 0.27}$ & 74.3$^{\\pm0.6}$\nT5\\textsubscript{base}  & 56.0 & 48.0 & -0.57$^{\\pm 0.1}$ & 75.3$^{\\pm1.1}$ \\\\%[0.5em] T5\\textsubscript{base}  & 56.0 & 48.0 & -0.57$^{\\pm 0.07}$ & 75.3$^{\\pm1.1}$\nLTG-BERT\\textsubscript{base} & \\textbf{85.8} & \\textbf{76.8} & -0.42$^{\\pm 0.2}$ & 77.9$^{\\pm 1.1}$ \\\\ % LTG-BERT\\textsubscript{base} & \\textbf{85.8} & \\textbf{76.8} & -0.42$^{\\pm 0.18}$ & 77.9$^{\\pm 1.1}$\nELC-BERT\\textsubscript{base} & 85.3 & 76.6 & \\textbf{-0.26$^{\\pm 0.5}$} & \\textbf{78.3$^{\\pm 3.2}$} \\\\ % ELC-BERT\\textsubscript{base} & 85.3 & 76.6 & \\textbf{-0.26$^{\\pm 0.45}$} & \\textbf{78.3$^{\\pm 3.2}$}\n\\bottomrule\n\\end{tabular}%\n}\n\\caption{\\label{tab:res_baby}\nResults for the BabyLM challenge suite of evaluation datasets -- BLiMP, supplemental dataset to BLiMP, MSGS and (Super)GLUE. We compare the results of our submitted model (ELC-BERT\\textsubscript{biased}) to the backbone model (LTG-BERT\\textsubscript{base}) and the baselines given by the organizers of the challenge on the \\textsc{strict} dataset. On the \\textsc{strict-small} dataset, we compare a variation (ELC-BERT\\textsubscript{zero}) of small size to the backbone model and baselines.\n}\n\\end{table}\n\n\\section{Results}\n\nThis section provides the results of the empirical evaluation of ELC-BERT. First, we compare our method to baselines, then we perform an ablation study of different ELC-BERT variations, and finally, we take a deeper look into the learnt layer weights.\n\n\\subsection{BabyLM challenge evaluation}\n\nWe adopt the BabyLM evaluation pipeline for all comparisons.\\footnote{\\url{https://github.com/babylm/evaluation-pipeline}} The pipeline itself is an adaptation of \\newcite{eval-harness} and it aims to provide a robust evaluation of syntactic and general language understanding. \n\nThe syntactic understanding is measured by the Benchmark of Linguistic Minimal Pairs \\citep[BLiMP \\& BLiMP supplemental;][]{warstadt2020blimp} and the Mixed Signals Generalization Set \n\\citep[MSGS;][]{warstadt2020learning}. The general natural language understanding is measured by GLUE and SuperGLUE \\citep{wang-etal-2018-glue, NEURIPS2019_4496bf24}. All of these benchmarks use filtered subsets of the original datasets (provided by the organizers), which means that they are not directly comparable to previous literature. If applicable, we divide the training set into a train-development split and report the mean/std statistics over multiple runs on the former validation split.\n\n\\paragraph{BLiMP.}\nThis benchmark tests zero-shot preference of grammatical sentences. From the \\textsc{strict} results in \\cref{tab:res_baby}, we see that ELC-BERT outperforms the baseline models by a fair margin on this task. %, ranging from 10 percentage points to 29.3 p.p. gain in performance. %If we focus on comparing to the encoder baseline (RoBERTa), we see that our model outperforms it by 10.2 p.p. \nHowever, if we look at the LTG-BERT baseline, we see that our model slightly underperforms it (by 0.5 percentage points). \\cref{blimp} provides a more in-depth comparison of the models.%, by showing the results of each task of the benchmark. %In it, we see that our model outperforms LTG-BERT in certain tasks, especially Island Effect, where our model outperforms it by 4.1\\%.\n\nIf we now look at the supplemental scores, we see a very similar trend to the BLiMP results: our model outperforms the baseline RoBERTa model by 24.4 p.p. while slightly underperforming against the LTG-BERT model by 0.2 p.p. \\cref{suppl} shows a breakdown of the aggregated scores. %If we then look at \\cref{suppl}, which details the aggregate result, we see that while our model underperforms LTG-BERT in QA Congruence Easy, it outperforms it in QA Congruence Tricky and Subject aux Inversion. Both models underperform baseline models in Hypernym.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\columnwidth]{emnlp2023-latex/msgs_gen_baseline.pdf}\n    \\caption{Violin plots of each model's Linguistic Bias Scores (LBS) and the base model. The white dot shows the median LBS and the edge of the boxes are the 1\\textsuperscript{st} and 3\\textsuperscript{rd} quartiles. The width of the violins shows the density of results at that score.}\n    \\label{fig:msgs_gen_baseline}\n\\end{figure}\n\n\\paragraph{GLUE.} \n\nA standard LM benchmark that tests the ability to be finetuned for general language understanding tasks. Focusing on the results in \\cref{tab:res_baby}, we see that our model outperforms both the encoder baseline %(by 4 and 3 p.p.) \nand the LTG-BERT model %(by 0.8 and 0.4 p.p.) \nin the \\textsc{strict} and \\textsc{stric-small} tracks. The improvement against LTG-BERT is rather modest and could be caused by random variation. If we look at \\cref{glue} we see that the variation is greatly affected by the WSC task -- % %where in one run, we had a very low score leading to a high variation. %It is possible that with more runs we stabilize this effect. \nignoring it, we get a score of $80.49^{\\pm{1.44}}$ for our model and $79.52^{\\pm{1.13}}$ for LTG-BERT. %While the increase is greater, the results are still within a standard deviation of each other.%, therefore it is not guaranteed that our model outperforms LTG-BERT.\n\n\\paragraph{MSGS.} \n\nFinally, this benchmark evaluates the preference towards linguistic explanations over spurious surface explanations. \nFor the aggregated \\textsc{strict} MSGS results of \\cref{tab:res_baby}, the comparison appears unclear due to the large standard deviation. However, a closer inspection reveals that ELC-BERT \\textit{significantly} outperforms LTG-BERT by 0.16 LBS points.\\footnote{Using the Almost Stochastic Order (ASO) significance test from \\citet{dror-etal-2019-deep} and \\citet{del2018optimal} (calculated using \\citet{ulmer2022deep}), we get a $\\varepsilon_{\\min}$ of 0.2 at a confidence level of 0.95 which implies that there is a high likelihood that ELC-BERT is better than LTG-BERT.} \\cref{fig:msgs_gen_baseline} and \\cref{tab:msgs} shows a detailed view on the score distribution. %While it looks like our model is the best performing (+0.40 to encoder baseline and +0.16 to LTG-BERT), the high standard deviation of all models makes it difficult to judge whether there is a model better than another. However, \n\n\\paragraph{Shared task results.}\n\nThe official Dynabench results for the top-5 models for the \\textsc{strict} and \\textsc{strict-small} track can be found in \\cref{tab:dynabench}. Looking first at the \\textsc{strict} track results, we see that our model achieves the highest total score and BLiMP score, while we are second for GLUE and MSGS. On the \\textsc{strict-small} track our model performs best on all benchmarks and by a substantial margin for all benchmarks. \n\n\\subsection{Model variations}\n\nWe compare the following modifications of the ELC-BERT architecture from \\cref{sec:methods}:\n\\begin{enumerate}\n    \\item \\textbf{Zero initialization}: The layer weights are all initialized as zeros, without any bias towards the previous layer. This model also uses the residual MLP input from \\cref{eq:transformer-layer}. This variation is used in the \\textsc{strict-small} track. %We call this variation \\textbf{ELC-BERT\\textsubscript{zero}}. We also create a small size (24M parameters) model of this variation called \\textbf{ELC-BERT\\textsubscript{small}}.\n    \\item \\textbf{Strict normalization}: This follows the previous variant with every $\\bm{h}^i_{\\text{out}}$ normalized to a unit vector. % We call this variation \\textbf{ELC-BERT\\textsubscript{normalized}}.\n    \\item \\textbf{Weighted output}: Follows the first variant and the input to the LM head is a weighted sum of all layers. To be more concrete, we replace \\cref{eq:lm-head} by $\\bm{y} \\gets \\operatorname{LM\\_head}\\!\\left(\\sum^{N}_{i=0}{\\alpha_{i,N+1} \\bm{h}^i_{\\text{out}}}\\right)$. %We call this variation \\textbf{ELC-BERT\\textsubscript{weighted out}}.\n\\end{enumerate}\n\n\\begin{table}\n\\resizebox{\\columnwidth}{!}{\n\\begin{tabular}{@{}lrrrr@{}}\n\\toprule\n\\textbf{Model} & \\textbf{BLiMP} & \\textbf{Supp.} & \\textbf{MSGS} & \\textbf{GLUE}\\\\\n\\midrule\nELC-BERT & 85.3 & 76.6 & -0.26$^{\\pm 0.5}$ & 78.3$^{\\pm 3.2}$ \\\\ % ELC-BERT & 85.3 & 76.6 & -0.26$^{\\pm 0.45}$ & 78.3$^{\\pm 3.2}$\n\\,\\,\\, + zero initialization & 84.9 & \\textbf{78.5} & -0.38$^{\\pm 0.3}$ & \\textbf{79.4$^{\\pm 1.0}$} \\\\ % ,\\,\\, w/ zero initialization & 84.9 & \\textbf{78.5} & -0.38$^{\\pm 0.26}$ & \\textbf{79.4$^{\\pm 1.0}$}\n\\,\\,\\, + normalization & 85.1 & 76.0 & \\textbf{-0.13$^{\\pm 0.4}$} & 78.2$^{\\pm 3.3}$ \\\\ % \\,\\,\\, w/ normalization & 85.1 & 76.0 & \\textbf{-0.13$^{\\pm 0.41}$} & 78.2$^{\\pm 3.3}$\n\\,\\,\\, + weighted output & \\textbf{86.1} & 76.0 & -0.28$^{\\pm 0.2}$ & 78.2$^{\\pm 0.6}$ \\\\ % \\,\\,\\, w/ weighted output & \\textbf{86.1} & 76.0 & -0.28$^{\\pm 0.22}$ & 78.2$^{\\pm 0.6}$\n\\bottomrule\n\\end{tabular}%\n}\n\\caption{\\label{tab:res_elc}\nResults for the BabyLM challenge suite of evaluation datasets. We compare the performance of different variants of our model to the one submitted to the BabyLM challenge as well as the backbone model LTG-BERT on the \\textsc{strict} dataset.\n}\n\\end{table}\n\n\\paragraph{Evaluation.}\n\nBased on \\cref{tab:res_elc}, we see that different variations have varying effects on the evaluation scores. \n\nWhen changing the $\\hat{\\alpha}$ initialization to zero, we see a significant increase in performance on both the BLiMP Supplemental and the GLUE benchmarks.%This variation beats our original model by 1.9 p.p. on BLiMP Supplemental and by 1.1 p.p. on GLUE. \n\\footnote{The increase in performance on the GLUE benchmark is significant when using the ASO significance test both against the original ELC-BERT and the backbone model LTG-BERT. Against both models, we get a $\\varepsilon_{\\min}$ of 0, indicating a very strong likelihood that the zero variation is better than ELC-BERT and LTG-BERT on GLUE} However, the model suffers in performance on both the BLiMP and MSGS.\\footnote{This is a significant decrease with an $\\varepsilon_{\\min}$ of 0.28 that ELC-BERT is better.} Overall, we see that this variation leads to better zero-shot and fine-tuning results while biasing the model more towards spurious surface features rather than linguistic features, as can be seen in \\cref{fig:msgs}. % This can especially be seen in \\cref{fig:msgs} where on all tasks except for those involving the linguistic feature control raising, the original ELC-BERT performs better or equivalently than the zero variation.\n\nIf we then focus on the normalization variation, we see that it underperforms in all benchmarks but one, MSGS, where it significantly performs better by 0.13 LBS points,\\footnote{Significant with an $\\varepsilon_{\\min}$ of 0.31.} %On the BLiMP and Supplemental it slightly underperforms (-0.3 p.p. and -0.6 p.p. respectively), while the difference on GLUE (-0.1 p.p.) is most probably due to random chance. \nas can be seen in more detail in \\cref{fig:msgs}.\n\nFinally, when looking at our weighted output variation, we see a substantial gain in performance on the BLiMP benchmark %(+0.8 p.p.) \nwhile the results on MSGS and GLUE are similar%(-0.02 LBS points and -0.1 p.p. respectively)\n, and the results on Supplemental BLiMP slightly decrease. % (-0.6 p.p.).\nMore detailed results on all these benchmarks can be found in \\cref{app:detail}.\n\n\\subsection{Layer importance}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\columnwidth]{emnlp2023-latex/msgs.pdf}\n    \\caption{Detailed LBS for each model and each combination of surface and linguistic features. The Y-axis (Main Verb, Syntactic Category, and Control Raising) show the linguistic features, while the X-axis (Lexical Content, Relative Token Position) represent the surface features. Each dot represents a different fine-tuned model.}\n    \\label{fig:msgs}\n\\end{figure}\n\nThe empirical evaluation suggests that learnable layer weights are a simple but effective architectural change -- but how do these learnt weights look like? In this section, we investigate the $\\alpha$ values of the normalized ELC-BERT variant.\\footnote{The interpretation of $\\alpha$ weights in a non-normalized variant is difficult due to different magnitudes of layer outputs.} %$\\bm{h}_{\\text{out}}$.}\n\nLooking at the importance matrix of ELC-BERT in \\cref{fig:layer-weights}, we posit that the first 5 layers focus on surface-level information found in the embedding layer explaining its enhanced importance for the embedding layer. The next 5 layers (6-10) focus on more linguistic features by virtually ignoring the first 4 layers (0-3) and focusing primarily on the previous three layers as well as layers 4 and 5 to get some transformed information from the embedding layer. Layer 11 does much the same but focuses more on Layer 4, potentially trying to obtain some surface knowledge found in it. Finally, Layer 12 behaves similarly to Layer 11 but also puts high importance (3\\textsuperscript{rd} most) on the embedding layer. This is most likely to recuperate some surface information lost in previous layers to pass to the language modelling head.\n\n\\section{Conclusion}\n\nIn this paper, we proposed a novel and simple modification of the transformer architecture for language modelling. We empirically tested the efficacy of our approach by participating in the BabyLM challenge -- a shared task for data-efficient language modelling. Our submission ranked first on both tracks that we participated in. A more detailed evaluation shows that, when compared to a strong baseline, our approach reliably performs better on (Super)GLUE tasks. The evaluation on MSGS suggests that our approach is more likely to prefer linguistic features over spurious surface features, and the BLiMP benchmarks show comparable performance to the baseline. Finally, our proposed modification shows that the assumption that all layers are equally important is incorrect, and a more complex layer structure helps the model.\n\n\\section*{Acknowledgements}\nThe efforts described in the current paper were funded by the HPLT project (High-Performance Language Technologies; coordinated by Charles University). The computations were performed on resources provided through Sigma2 -- the national research infrastructure provider for High-Performance Computing and large-scale data storage in Norway.\n\n\\clearpage\n\\onecolumn\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{GPT or BERT: why not both?}\n\n\\begin{document}\n\n\\maketitle\n\\def\\thefootnote{*}\\footnotetext{Both authors contributed equally to this work.}\\def\\thefootnote{\\arabic{footnote}}\n\\begin{abstract}\nWe present a simple way to merge masked language modeling with causal language modeling. This hybrid training objective results in a model that combines the strengths of both modeling paradigms within a single transformer stack -- \\textsc{GPT-BERT} can be transparently used like any standard causal or masked language model. We test the pretraining process that enables this flexible behavior on the BabyLM Challenge 2024. The results show that the hybrid pretraining outperforms masked-only or causal-only models. We openly release the models, training corpora and code.\\footnote{The models are available on HuggingFace at \\href{https://huggingface.co/ltg/gpt-bert-babylm-base}{\\texttt{ltg\\-/gpt\\--bert\\--baby\\-lm\\--base}} and \\href{https://huggingface.co/ltg/gpt-bert-babylm-small}{\\texttt{ltg\\-/gpt\\--bert\\--baby\\-lm\\--small}}; the corpora at \\href{https://huggingface.co/datasets/ltg/babylm-2024-baby-cosmo-fine-100m}{\\texttt{ltg\\-/baby\\-lm\\--2024-ba\\-by-cos\\-mo-fine-100m}} and \\href{https://huggingface.co/datasets/ltg/babylm-2024-baby-cosmo-fine-10m}{\\texttt{ltg\\-/baby\\-lm\\--2024-ba\\-by-cos\\-mo-fine-10m}}. The training scripts are available on GitHub at \\href{https://github.com/ltgoslo/gpt-bert}{\\texttt{ltg\\-oslo\\-/gpt\\--bert}}}\n\\vspace{1em}\n\\end{abstract}\n\n\\section{Introduction}\n\\label{sec:introduction}\n\nLanguage models have become fundamental tools in natural language processing, with two dominant paradigms: causal language models (CLM) and masked language models (MLM). Six years ago, GPT by \\newcite{Radford2018ImprovingLU} demonstrated the generative abilities of transformer-based causal language models. Just a few months after this publication, BERT by \\newcite{devlin-etal-2019-bert} heavily outperformed the causal GPT models when finetuned on downstream NLP tasks, showcasing the major advantage of masked language modeling. These two `historical' models define the main use-cases of the two paradigms up to this date.\n\nThe difference between these paradigms lies in how they process text. CLMs can only look at previous tokens when making predictions, mimicking the left-to-right reading process. This makes them particularly well-suited for efficient text generation. MLMs, on the other hand, can access both previous and following tokens, allowing them to build richer contextual representations. This bidirectional context has proven especially valuable for tasks requiring deep language understanding.\n\n\\begin{figure}[!t]\n        \\centering\n        \\includegraphics[width=\\linewidth]{figures/hybrid_illustration.pdf}\n        \\caption{\\textbf{Two modes of a single model}\\hspace{1.5em}Causal and masked language modeling can be easily unified by shifting both outputs by one token to the right. Then we can train one language model on both paradigms at the same time just by modifying the input tokens, output tokens and attention masks.}\n        \\label{fig:hybrid}\n    \\end{figure}\n\n\\paragraph{BERTs should not be forgotten}\nA recent paper by \\newcite{samuel2024berts} revealed that BERT-like model are just as capable text generators as GPT-like models. Yet, when these two types of models are evaluated on a shared ground -- generative in-context learning \\citep{NEURIPS2020_1457c0d6} -- they still show radical differences, clearly outperforming each other in different areas. Each paradigm has its own strengths and combining them into a single hybrid might lead to a model with a more general language understanding.\n\n\\paragraph{GPT-BERT}\nThis motivated us to introduce \\textsc{GPT-BERT}, a hybrid language model that combines the strengths of both CLM and MLM approaches. Our key insight is that the two objectives can be unified by reformulating how output tokens are handled in the MLM framework. Instead of predicting masked tokens at their original positions, we shift the predictions one position to the right, aligning them with the CLM's next-token prediction pattern. This simple modification allows us to train a single model that can seamlessly switch between masked and causal modes without any architectural changes or additional parameters.\n\nThis paper demonstrates the benefits of the hybrid approach across multiple benchmarks. We evaluate \\textsc{GPT-BERT} on the \\textit{BabyLM Challenge 2024} \\citep{babylm-2024}, which provides a controlled environment for comparing language models trained on limited data. Additionally, we explore the impact of varying the ratio between MLM and CLM, and we test the model's ability to perform in-context learning and text generation.\n\nThe results suggest that integrating MLM and CLM objectives during pretraining leads to more robust and capable language models, even in low-resource scenarios, without any extra training cost. Our approach opens up new possibilities for developing more efficient and versatile models for a wide range of natural language processing tasks.\n\n\\section{Method}\n\n\\subsection{Hybrid masked-causal language modeling}\n\\label{sec:hybrid}\n\n    \n    \n    \n    In order to align both objectives we use a slightly modified version of masked language modeling called \\textbf{masked next-token prediction} \\citep[MNTP;][]{behnamghader2024llmvec}. The only difference from traditional MLM is that when the token at position $k+1$ is masked, its prediction should be outputed at position $k$. In this way both MLM and CLM are unified as the output at position $k$ always represents the token at position $k+1$. These two modes are illustrated in \\Cref{fig:hybrid}.\n    \n    \\paragraph{Dataset handling} To ensure that our model sees all the data for both objectives, we duplicate our dataset. One is used for the causal objective, and the other for the masked objective. We can then decide a ratio of causal-to-masked in which to divide the data seen by the model at each batch.%For example, if we want our model to see as much data in a CLM fashion as in a MNTP fashion, we can have a ratio of 1:1, however, if we want to emphasize MTNP, we could have a 3:1 ratio where for every three MTNP examples, we have one CLM example (resulting in 75\\% of the batch having the MNTP objective). Therefore the number of steps per MNTP and CLM epochs can be different since both datasets have the same number of elements. \n    \n    \\paragraph{Loss and transformer architecture} No additional changes are needed. Both training objectives minimize the cross-entropy loss, they share all learnable parameters, and use the same transformer encoder/decoder module.\n\n\\subsection{Other modifications}\n\\label{sec:modification}\n\nWe base the transformer architecture of our models on LTG-BERT \\citep{samuel-etal-2023-trained}, but make some additional modifications to improve its performance. These changes are ablated in \\Cref{sec:experiments}.\n\n\\paragraph{Attention gate}\n\nFollowing \\newcite{AlphaFold2021}, we gate the outputs of the attention operation. This is akin to the gated linear units (GLU) that have been proposed to improve the expressivity of feed-forward modules \\citep{shazeer2020gluvariantsimprovetransformer}. This modification also simplifies the definition of the transformer architectures, now both the attention modules and the feed-forward modules can be expressed as:\n\n\\begin{minted}[linenos=false, breaklines=true, baselinestretch=1.2, breakanywhere=true, fontfamily=tt, fontsize=\\footnotesize, numbersep=12pt, xleftmargin=0.0em,firstnumber=1,escapeinside=@@]{python}\ndef layer(x: @\\textbf{\\texttt{tensor}}@, layer_id: int):\n    residual = x            # skip-connection\n    x = layer_norm(x)       # without parameters\n    g = gate(x)             # linear projection\n    if layer_id % 2 == 0:   # if attention layer\n        x = attention(x)    # do attention\n    else:                   # else feed-forward\n        x = linear(x)       # linear projection\n    x = glu(x, g)           # activation (GEGLU)\n    x = layer_norm(x)       # without parameters\n    x = output(x)           # linear projection\n    return residual + x\n\\end{minted}\n\n\\paragraph{Layer weighting} We further increase the expressivity of the transformer backbone by allowing each layer to select its desired combination of outputs from previous layers. This directly follows the ELC-BERT models \\citep{georges-gabriel-charpentier-samuel-2023-layers} and the later modification by \\newcite{pagliardini2024denseformer} who allow any linear combination of layers instead of restricting the combination to be convex. We also make the weighting more granular by treating the attention and feed-forward modules as separate layers. With each $\\alpha_{ij} \\in \\mathbb{R}$ being a learnable scalar, the forward pass of the resulting transformer works as follows:\n\n\\begin{minted}[linenos=false, breaklines=true, baselinestretch=1.2, breakanywhere=true, fontfamily=tt, fontsize=\\footnotesize, numbersep=12pt, xleftmargin=0.0em,firstnumber=1,escapeinside=@@]{python}\ndef transformer(subword_indices: @\\textbf{\\texttt{tensor}}@):\n    output@$_{\\texttt{0}}$@ = embedding(subword_indices)\n    for i in range(1, n_layers + 1):\n        output@$_{\\texttt{i}}$@ = @$\\sum_{\\texttt{j=1}}^{\\texttt{i}}{\\alpha_{\\texttt{ij}}\\cdot\\texttt{layer}(\\texttt{output}_{\\texttt{j-1}}, \\texttt{j})}$@\n    return output@$_{\\texttt{n\\_layers}}$@ \n\\end{minted}\n\n\\paragraph{Batch-size scheduling} We improve the sample-efficiency (and speed) of pretraining by linearly increasing the batch size during training \\citep{rae2022scalinglanguagemodelsmethods, deepseekv2}. The intuition behind this method is that high-quality gradients are mainly needed at the late stages of pretraining, the initial steps can be guided by good-enough gradients from smaller batches. The maximum batch size is taken from LTG-BERT (4M tokens), but we start the training with just $\\nicefrac{1}{4}$ of this value, thus dividing the total number of tokens needed for training by $2$.\n\n\\paragraph{Mask scheduling} Another way to increase the sample-efficiency is to recover more unmasked tokens during training. However, \\newcite{ankner-etal-2024-dynamic} showed that this might be in conflict with the downstream usage of MLMs. Thus they propose to linearly decrease the masking probability throughout the training, starting with $30\\%$ and finishing with the standard $15\\%$ masking. We adopt this scheme, believing that it also reduces the impact of smaller batches at the beginning of training. \n\n\\section{Pretraining and evaluation}\n\nThe main purpose of this section is to evaluate if the MLM and CLM training objectives can be merged, and to evaluate the effect of this. We base the experiments on the BabyLM challenge \\citep{babylm-2024}.\n\n\\paragraph{BabyLM challenge}\n\nThis shared task provides a shared ground for experiments on small-scale language modeling. Its second iteration consists of four tracks: \\textsc{strict}, \\textsc{strict-small}, \\textsc{vision} and \\textsc{paper}. We participate in the first two text-based tracks. There, the submissions have to be pretrained solely on a fixed number of words, 100M in the \\textsc{strict} track and about 10M words in the \\textsc{strict-small} track. The organizers do provide a default dataset for each track, but unlike the previous edition, the participants are not limited to using it, as long as they stay under the word count limit. For the \\textsc{vision} track, the participants are limited to 50M words and as many images as they want. Here the goal is to create a multi-modal model. Finally, the \\textsc{paper} does not require the submission of a model to the task. This track encourages contributions related to the goal of the challenge such as new cognitively-inspired metrics. As detailed in \\cref{sec:evaluation}, the submissions are compared on a shared evaluation set consisting of syntactic and natural language understanding tasks.\n\n\\begin{table}[t!]\n\\resizebox{\\columnwidth}{!}{%\n\\begin{tabular}{@{}l@{\\hspace{-1em}}rrrr@{}}\n{\\small\\textsc{strict-small} track (10M words)}\\\\\n\\toprule\n\\textbf{Model} & \\textbf{BLiMP $\\uparrow$} & \\textbf{BLiMP-S $\\uparrow$} & \\textbf{GLUE $\\uparrow$} & \\textbf{EWOK $\\uparrow$}\\\\\\midrule\nEncoder-only \\textsubscript{\\textit{(BabyLM baseline)}} & 60.6 & 60.8 & 60.3 & 48.9 \\\\\nDecoder-only \\textsubscript{\\textit{(BabyLM baseline)}} & 69.8 & 59.5 & 63.3 & 50.7  \\\\[0.75em]\nELC-BERT \\textsubscript{\\textit{(2023)}} & 80.5\t& 67.9 & 75.3 & 51.0 \\\\\nLTG-BERT \\textsubscript{\\textit{(2023)}} & 80.6 & \\textbf{69.8} & 74.5 & ---\\\\[0.75em]\nGPT-BERT \\textsubscript{\\textit{(ours)}} & \\textbf{81.2} & 69.4 & \\textbf{76.5} & \\textbf{54.6} \\\\\n\\bottomrule\n\\\\\n{\\small\\textsc{strict} track (100M words)}\\\\\n\\toprule\n\\textbf{Model} & \\textbf{BLiMP $\\uparrow$} & \\textbf{BLiMP-S $\\uparrow$} & \\textbf{GLUE $\\uparrow$} & \\textbf{EWOK $\\uparrow$}\\\\\\midrule\nEncoder-only \\textsubscript{\\textit{(BabyLM baseline)}} & 69.2 & 66.5 & 68.4 & 51.9 \\\\\nDecoder-only \\textsubscript{\\textit{(BabyLM baseline)}} & 73.1 & 60.6 & 69.0 & 52.1 \\\\[0.75em]\nELC-BERT \\textsubscript{\\textit{(2023)}} & 85.8\t& \\textbf{76.8}  & 78.3 & 56.3 \\\\\nLTG-BERT \\textsubscript{\\textit{(2023)}} & 85.3 & 76.6  & 77.9 & 56.0 \\\\[0.75em]\nGPT-BERT \\textsubscript{\\textit{(ours)}} & \\textbf{86.1} & \\textbf{76.8} & \\textbf{81.5} & \\textbf{58.4} \\\\\n\\bottomrule\n\\end{tabular}%\n}\n\\caption{\\textbf{BabyLM submission scores}\\hspace{1.5em}The final scores of our \\textsc{strict-small} and \\textsc{strict} models submitted to the BabyLM challenge \\citep{babylm-2024}. The table also includes the winner of the last year's iteration of this shared task (ELC-BERT), the baseline for our current model (LTG-BERT), as well as the baselines provided by the organizers. Results of other submission were not available as of writing this paper. Higher scores are better, the best results in each evaluation suite are boldfaced.\n}\n\\label{tab:babylm-scores}\n\\end{table}\n\n\\paragraph{Training corpus} We pretrain both submissions on a $1:1:1$ mix of the provided BabyLM corpus, on a subset of the FineWeb-Edu corpus \\citep{lozhkov2024fineweb-edu}, and on a small subset of the Cosmopedia corpus \\citep{benallal2024cosmopedia}. The main purpose of training on this mixture is to provide the model with more factual knowledge and more diverse language. \n\n\\paragraph{Pretraining process} Generally speaking, we adopt the training recipe of LTG-BERT \\citep{samuel-etal-2023-trained}, which was optimized for pretraining on another low-resource 100 million English corpus.\\footnote{\\url{https://github.com/ltgoslo/ltg-bert}} The pretraining process is the same for both tracks, except for using a smaller vocabulary and a smaller model for the \\textsc{strict-small} track.\n\nAs for the \\textsc{strict} track, we use a \\textsc{base}-sized language model with 119 million parameters. We train a case-sensitive BPE tokenizer \\citep{Gage1994ANA} with a vocabulary size of $2^{14} = 16\\,384$, using solely texts from the training corpus. The \\textsc{base} is trained for 15\\,625 steps with an average batch size of 2 million tokens. The \\textsc{strict-small} track is tackled by a \\textsc{small}-sized language model with 30 million learnable parameters. The subword vocabulary is reduced to $2^{12} = 8\\,192$ items. The training steps of the \\textsc{small} model are reduced to 7\\,812. The full list of hyperparameters and implementation details are provided in \\cref{app:training}.\n\n\\paragraph{Evaluation}\n\\label{sec:evaluation}\n\nWe utilize the language modeling benchmark suite from the BabyLM challenge \\citep{eval-harness, babylm-2024},\\footnote{\\url{https://github.com/babylm/evaluation-pipeline-2024}} which relies on three conceptually different evaluation tasks:\n\\begin{enumerate}\n    \\item The GLUE and SuperGLUE datasets test the ability of a pretrained model to adapt to various language understanding tasks.\n    \\item BLiMP and BLiMP-supplement tasks test the affinity of a model towards grammatical sentences in a completely zero-shot manner.\n    \\item EWOK is another zero-shot task. It tests the ability of a model to understand concepts such as spatial relations or physical dynamics.\n\\end{enumerate}\n\n\\noindent\nWe further elaborate on each of these evaluation suites in \\Cref{app:evaluation}.\n\n\\section{Experiments}\n\\label{sec:experiments}\n\n\\begin{figure*}[!h]\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/stacked_plot.pdf}\n    \\caption{\\textbf{The effect of the causal-to-mask ratio}\\hspace{1.5em}Comparison of performance of different tasks when varying the ratio of MNTP used during pre-training. We also look at the performance of the model using prefix language modeling with a partially-bidirectional attention mask. MNLI scores are reported with standard deviation error bars estimated by averaging the variations across three finetuning random seeds.}\n    \\label{fig:lambada}\n\\end{figure*}\n\n\\subsection{BabyLM submission}\n\n\\Cref{tab:babylm-scores} shows the performance of our models against the backbone architecture of the model (LTG-BERT), as well as last year's winner on both tracks (ELC-BERT). %\\footnote{\\url{https://huggingface.co/lgcharpe/ELC_BERT_small_baby_100M}, \\url{https://huggingface.co/lgcharpe/ELC_BERT_small_baby_10M}} \nWe can see that for the \\textsc{strict-small} track our model outperforms last year's winner in every benchmark and is only beaten by LTG-BERT on BLiMP-Supplement by 0.4. For our submission to the \\textsc{strict} track our model outperforms or matches both models (only ELC-BERT on BLiMP-Supplement matches our model). One thing to note, is that the filtration of the evaluation datasets are slightly different leading to comparisons between not exact.\n\nFor completeness, in \\Cref{tab:babylm-scores}, we also include the performance of the models provided by the BabyLM organizers  \\citep{babylm-2024}. The provided encoder-only models are based on LTG-BERT \\citep{samuel-etal-2023-trained}, and the decoder-only models are based on Baby Llama \\citep{timiryasov-tastet-2023-baby}. Our models clearly outperforms these baselines on all metrics, but that might be mostly attributed to their smaller pretraining budget.\n\n\\subsection{Masked or causal?}\n\nSince our model can learn both from masked and causal examples, the question becomes, whether using a combination of both is better than using only one of the two methods during pretraining. To evaluate this, we look at the performance of models pretrained with different causal-to-masked ratios. \n\nThe main results are presented in \\Cref{fig:lambada}. We evaluate the models on four tasks that cover distinct uses: \\circled{1} BLiMP is a zero-shot linguistic-preference task that is typically better suited for masked language models \\citep{salazar-etal-2020-masked}; \\circled{2} MNLI is a popular dataset for evaluating the finetunability of a language model, which also benefits masked language models; \\circled{3} LAMBADA, on the other hand, is a language modeling dataset mostly used to evaluate causal language models; and \\circled{4} we also directly compute the validation loss of each model. Furthermore, when applicable, each task is tested with three settings: fully-bidirectional processing (without any attention mask), unidirectional processing (with a causal mask), and partially-bidirectional processing (with a prefix mask).\n\nThe validation loss of the causal and prefix masking is calculated on the second half of the tokens of a given input sequence, where the first half of the tokens are either seen in a bidirectional fashion (prefix) or in a causal fashion (causal). For LAMDABA the entire context is seen bidirectionally for the prefix evaluation. Finally, when fine-tuning MNLI with the causal mask, we use the same tokenization as \\citet{Radford2018ImprovingLU} where a both a delimiter token is added in-between the two sentences as well as a extract token at the end of the input (two different tokens are used).\n\nFor the MNLI hyperparameters, we did a sweep on the SST-2 dataset for each model and took the best performing hyperparameters for each model and each masking (i.e. each model and masking scheme had their own hyperparameters). We sweeped over $\\{1, 3, 5\\}$ for number of epochs, $\\{3\\cdot10^{-5}, 5\\cdot10^{-5}, 1\\cdot10^{-4}\\}$ for learning rates, and $\\{16, 32\\}$ for batch sizes.\n\n\\paragraph{Bidirectional results} If we start by focusing on the bidirectional results, we see that the best results for all the tasks can be found for the models with a lower causal-to-masked ratio (from 1:7 to masked-only). More specifically, the 1:7 model is the best on BLiMP and LAMBADA, the best model for MNLI is 15:16, and both those models and the masked-only model achieve the best results on the validation loss. We also see that adding as few as $6.25\\%$ MNTP training can lead to significant increases in bidirectional performances ($+4.2\\%$ on BLiMP, $+0.9\\%$ on MNLI, $+33.3\\%$ on LAMBADA and $-4.53$ on validation loss). In addition, using a bidirectional mask for evaluation performs the best for all models except the causal-only, however, this is unsurprising given this model is never trained to attend to every token.% The LAMBADA evaluation also benefits from knowing how many tokens need to be re-identified.\n\n\\paragraph{Causal LM results} Looking at the results when using causal masking, we see that the best models shift towards a more balanced ratio between the causal and masked training objectives. The 1:1 model and 1:3 model perform roughly the same on all tasks. As mentioned before, the results are worse than for the bidirectional evaluation; most likely because of the lower expressivity of causally-masked models \\citep{ewer2024entpencoderonlytokenprediction}. Further focusing on MNLI, we see that the purely causal model does not truly benefit from being finetuned with a bidirectional mask (only $+0.7\\%$ improvement, with the results being within two standard deviations of each other). Once we add some MNTP training we see a significant difference in the results between both masking strategies. With only $6.25\\%$ MNTP added, we have a $1.2\\%$ improvement when using the bidirectional mask. This trend grows to being an over $3\\%$ improvement in performance.\n\n\\paragraph{Prefix LM results} Finally, we look at the performance for the prefix masking (partially bidirectional). We only evaluate prefix masking on LAMBADA and validation loss since it would be difficult to do this for both BLiMP and MNLI. We see that on validation loss we get similar (if not slightly worst) results as for the causal masking while the results on LAMBADA are slightly improved. In addition, the LAMBADA results do not have a clear trend outside of the hybridized models performing better than the single-objective models. This leads us to believe that our models can perform limited prefix language modeling even though they were not explicitly trained to do so.\n\n\\paragraph{Other benchmarks} Similar trends can be seen on the other datasets in \\cref{app:vary}. Based on the results on all tasks, we decided to use a 1:15 causal-to-masked ratio for our final model (to which every model is compared in subsequent sections) as well as the bidirectional evaluation scheme. In \\cref{sec:icl,sec:generation}, a model trained on this ratio is used for the in-context learning and text generation.\n\n\\subsection{Ablation study}\n\n\\begin{table}[!t]\n\\resizebox{\\columnwidth}{!}{%\n\\begin{tabular}{@{}l@{\\hspace{1em}}rrrr@{}}\n{\\small\\textsc{strict-small} track (10M words)}\\\\\n\\toprule\n\\textbf{Model configuration} & \\textbf{PPL $\\downarrow$} & \\textbf{BLiMP $\\uparrow$} & \\textbf{MNLI $\\uparrow$} & \\textbf{EWOK $\\uparrow$} \\\\ \\midrule\nGPT-BERT                   & \\textbf{10.8} & \\textbf{81.2} & 80.1 & 54.6 \\\\[0.75em]\n\\textit{without} layer weights    & $+$0.4          &$-$1.3 & $+$0.2 & $+$0.6 \\\\ \n\\textit{without} attention gate   & $+$0.3          & $-$0.3 & $+$0.3 & $-$0.9 \\\\\n\\textit{without} mask scheduling  & $+$0.1          & $-$0.1 & $-$0.7 & $-$0.6  \\\\\n\\textit{without} batch scheduling & $+$0.7          &  $-$1.1 & 0.0 & \\textbf{$+$0.8} \\\\[0.75em]\n\\textit{with only} BabyLM corpus & --- & $-$0.2 & $-$1.6 & $-$2.0\\\\\n\\textit{with only} FineWeb-edu   & --- & $-$0.4 & \\textbf{$+$1.1} & $-$0.8\\\\\n\\textit{with only} Cosmopedia    & --- & $-$7.1 & 0.0 & $-$0.6 \\\\ \\bottomrule\n\\end{tabular}%\n}\n\\caption{\\textbf{Ablation study}\\hspace{1.5em}Comparison of different model configurations proposed in \\Cref{sec:modification}, and corpus mixtures. The top row shows the performance of the final model (with all modifications), the middle rows show the absolute performance difference of models with one modification less, and the last group of rows shows the performance difference of GPT-BERT models trained on corpora from single sources.}\n\\label{tab:ablation}\n\\end{table}\n\n\\begin{figure*}[!th]\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/icl.pdf}\n\n    \\caption{\\textbf{SST-2 in-context learning}\\hspace{1.5em}20-shots ICL results on the SST-2 validation set for models trained on the 100M BabyLM datasets with varying degrees of each objective. The demonstrations (shots) were chosen at random from the training dataset. We do 20-runs and report mean as well as standard deviation. Note that the accuracy of the majority baseline on this dataset is 51.8\\%.}\n    \\label{tab:icl_sst2_varying}\n\\end{figure*}\n\nWe ablate the modeling choices from \\Cref{sec:modification} as well as different choices of training data. We train the ablated models with the \\textsc{strict-small} setup and evaluate them on BLiMP, EWOK and MNLI (the largest GLUE dataset). The ablation results are in \\Cref{tab:ablation}.\n\n\\paragraph{Results of the transformer ablation} All our modeling decisions during development were based on the training and validation perplexities -- this ablation study therefore provides an informative comparison based on a `held-out' downstream performance. \\circled{1} In particular, the value of learnable layer weights is not clear for GPT-BERT, especially considering that they substantially slowdown the training (almost $1.5\\times$). \\circled{2} Attention gating, on the other hand, seems to be a better substantiated improvement, which also does not add any major computational cost. \\circled{3} Mask scheduling is definitely a recommended improvement for any BERT-like models, based on all scores in this study. \\circled{4} Batch scheduling does not show on overall negative impact, which means that GPT-BERT can be trained $2.0\\times$ more efficiently using this method without a noticable degradation.\n\n\\paragraph{Results of the corpus ablation} The ablation of the three text corpora used for training our submission shows how each of them excels in a different area -- the BabyLM collection outperforms the others on BLiMP, FineWeb-Edu significantly improves the MNLI performance, and Cosmopedia is the best corpus for learning EWOK-style factual knowledge. The performance of the final GPT-BERT models shows that taking a random $\\nicefrac{1}{3}$ of each corpus and combining them into a single collection works surprisingly well; combining the strengths of each corpus.\n\n\\subsection{Text generation}\n\\label{sec:generation}\n\nNone of the BabyLM benchmarks is particularly well-suited for assessing CLM performance. Thus, apart from LAMBADA, we also look at the generation capabilities of \\textsc{GPT-BERT}. First we try standard greedy decoding with no repetition penalty (omitting new lines for readability):\n\n{\\small\n\\begin{description}\n    \\item[\\texttt{It was a calm night}]\\texttt{in the small town of Harmonyville, and the residents were eager to spend the day exploring the beautiful park. Among them were two best friends, Sam and Alex, who loved to learn about nature and its wonders.}\n    \\item[\\texttt{Brazil is known for}]\\texttt{its rich history and culture. It has been home to many indigenous peoples, including the indigenous people of the Amazon rainforest. One of the most famous indigenous groups in Brazil is the Brazilian people. They are known for their strong culture, traditional beliefs, and unique traditions.}\n\\end{description}\n}\n\n\\noindent\nAs we can see that model is relatively repetitive, especially in the second generation, where some words are repeated in almost every sentence. If we now give the second prompt with a repetition penalty of 1.5, we get:\n\n{\\small\n\\begin{description}\n    \\item[\\texttt{Brazil is known for}] \\texttt{its rich history and culture. It has been home to many indigenous peoples, including the Mayan civilization, who have built impressive cities like Tiapaca (present-day Uruguay), Cusco (now Guatemala), Chihuahua (also called Yucatn), Puebla (which was once part of Mexico), Huachimaso (then modern-day Colombia), and more. One of the most famous places in Brazil is the city of Rio de Janeiro, located on the southern tip of the Amazon basin.}\n\\end{description}\n}\n\n\\noindent\nAlthough the model is not factually correct, it stays on topic while generating meaningful and well-formed text.\n\nOverall, our model seems to be able to generate text, even though it has a hard time remembering exact facts and stay on topic. However, without applying repetition penalty the model struggles with repeating itself. More generations using prompts from \\citet{radford2019language} can be found in \\cref{app:generation}.\n\n\\subsection{In-context learning}\n\\label{sec:icl}\n\nA well-known ability of larger language models is to use in-context information given in prompts to solve tasks without any finetuning -- causal ones \\citep{NEURIPS2020_1457c0d6}, as well as masked models \\citep{samuel2024berts}. However, these capabilities are often thought to appear only once a model is large enough or trained on a vast amount of data \\citep{wei2022emergent}. \n\nDespite the number of parameters and the size of the training corpus, our models show some signs of in-context learning, as can be seen in \\cref{tab:icl_sst2_varying}. When using the causal attention mask, we see that while the models trained with a single objective underperform the baseline, the hybrid models all perform above the majority baseline (from +0.5\\% to +5.7\\%); with the best results being achieved by the 3:1 model (with the 1:3 and 7:1 close second and third respectively). This indicates that our models are capable of doing in-context learning when trained with both objectives. When run fully bidirectionally, the trend is similar but with lower absolute performance. %One thing to note is that even though the models underperform the baseline, they have a positive Mathews Correlation Coefficient (indicating that they are not just random guessing). This is not the case for the causal-only model with either masking and masked-only model with causal masking.\n\n\\section{Related work}\n\\label{sec:related}\n\n\\paragraph{Baby language models} This paper describes a submission to the second iteration of the BabyLM challenge\n\\citep{warstadt-etal-2023-findings}. Our submission is heavily inspired by the last-year's winner, ELC-BERT \\citep{georges-gabriel-charpentier-samuel-2023-layers}, and by its inspiration, LTG-BERT \\citep{samuel-etal-2023-trained}. Our modifications to these approaches are described in \\Cref{sec:hybrid} and \\Cref{sec:modification}.\n\n\\paragraph{Hybrid masked-causal models} Our work is not the first to attempt to merge bidirectional masked language modeling with generative causal modeling: T5 \\citep{10.5555/3455716.3455856}, BART \\citep{lewis-etal-2020-bart} and GLM \\citep{du-etal-2022-glm} proposed autoregressive fill-in-the-blank training objectives, CM3 is based on a causal-mask objective \\citep{aghajanyan2022cm3}, prefix language models use a partially-bidirectional causal modeling \\citep{NEURIPS2019_c20bb2d9, 10.5555/3455716.3455856}, and UL2 further improves the T5 encoder-decoder with more training objectives \\citep{tay2023ul}. Our approach differs by its simplicity -- not requiring any architectural changes nor novel training objectives -- it just combines a standard causal language model with a (shifted) masked language model; the resulting hybrid can then be used as any GPT-like or BERT-like model out-of-the-box.\n\n\\paragraph{Masked next-token prediction} To our best knowledge, this training objective was first proposed by \\newcite{DBLP:journals/corr/abs-2311-07468} with some additional modifications, and then simplified in LLM2Vec by \\newcite{behnamghader2024llmvec}, where it was used to finetune purely causal language models so that they can function as bidirectional text embedders. Since we use the latter formulation in this paper, we refer to this training objective as `masked next-token prediction'.\n\n\\section{Conclusion}\n\\label{sec:conclusion}\n\nWe introduced GPT-BERT, a novel approach that unifies masked and causal language modeling objectives within a single transformer architecture. Through extensive experiments on the BabyLM Challenge 2024, we demonstrated that this hybrid approach offers several key advantages over single-objective models:\n\\begin{enumerate}[itemsep=0.15em]\n    \\item \\textit{Improved performance:} The hybrid pretraining leads to better results across multiple benchmarks, outperforming both pure MLM and pure CLM approaches.\n    \\item \\textit{Architectural flexibility:} Without any structural modifications, our model can operate in masked, causal, or prefix modes. This flexibility enables GPT-BERT to handle a diverse range of tasks using the most appropriate inference strategy for each situation.\n    \\item \\textit{Unexpected capabilities:} Despite being trained on limited data and having a relatively small parameter count, our models exhibit signs of in-context learning -- a capability typically associated with much larger models.\n    \\item \\textit{Training efficiency:} The hybrid approach achieves these improvements without requiring additional parameters or increased training time compared to single-objective models.\n\\end{enumerate}\n\n\\noindent\nOur results suggest that the traditional dichotomy between MLM and CLM architectures may be unnecessary, and that future work might benefit from exploring more unified approaches to language model pretraining.\n\n\\section*{Limitations}\n\nWhile the results presented in this paper are promising and suggest improvements across many tasks when using GPT-BERT, all tested models are relatively small and trained on very small datasets. There is a possibility that these results do not scale and do not work outside of the BabyLM constraints.\n\n\\section*{Acknowledgments}\n\nThis work is fully funded by the University of Oslo. The computations were performed on resources provided through Sigma2  the national research infrastructure provider for high-performance computing and large-scale data storage in Norway. We acknowledge Norway and Sigma2 for awarding this project access to the LUMI supercomputer, owned by the EuroHPC Joint Undertaking, hosted by CSC (Finland) and the LUMI consortium through project 5000144.\n\n\\clearpage\n\\onecolumn\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2303.09859v3.tex",
        "arXiv-2311.02265v2.tex",
        "arXiv-2410.24159v2.tex"
    ],
    "group_id": "group_32",
    "response": "### Summary of Research Papers on Efficient and Data-Efficient Language Modeling\n\n#### Title: Efficient and Data-Efficient Language Modeling: A Comparative Study\n\n#### Introduction\n\nThe field of Natural Language Processing (NLP) has seen significant advancements in recent years, largely driven by the development of large-scale language models (LLMs) that are pre-trained on vast amounts of textual data. These models, such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer), have demonstrated remarkable capabilities in understanding and generating human language. However, the reliance on massive corpora and extensive computational resources poses challenges for low-resource languages and environments. Moreover, it raises concerns about the environmental impact and the need for more efficient training methods. The BabyLM challenge, a shared task focused on efficient language modeling, aims to address these issues by providing a controlled environment for evaluating models trained on limited data.\n\nThe BabyLM challenge, in its second iteration, offers a platform for researchers to explore the potential of language models trained on small corpora. The challenge includes four tracks: \\textsc{strict}, \\textsc{strict-small}, \\textsc{vision}, and \\textsc{paper}. The \\textsc{strict} and \\textsc{strict-small} tracks require models to be pre-trained on 100 million and 10 million words, respectively, and then evaluated on a suite of benchmarks designed to test syntactic and natural language understanding (NLU) capabilities. The \\textsc{vision} track introduces multi-modal training, while the \\textsc{paper} track encourages contributions related to the underlying cognitive processes involved in language modeling.\n\nThe papers under review explore different aspects of efficient language modeling. The first paper, \"Trained on 100 million words and still in shape: BERT meets British National Corpus,\" investigates the performance of a language model pre-trained on a curated corpus of 100 million words. The second paper, \"Not all layers are equally as important: Every Layer Counts BERT,\" introduces a novel architecture that allows each layer to selectively process outputs from previous layers, thereby enhancing the model's adaptability to different tasks. The third paper, \"GPT or BERT: why not both?\" proposes a hybrid model that combines masked language modeling (MLM) and causal language modeling (CLM) objectives within a single transformer stack. This paper evaluates the effectiveness of this hybrid approach on the BabyLM challenge and discusses its implications for future research.\n\n#### Main Content and Innovations\n\n**Paper 1: Trained on 100 million words and still in shape: BERT meets British National Corpus**\n\nThis paper explores the performance of a language model pre-trained on the British National Corpus (BNC), a carefully curated corpus of approximately 100 million words. The BNC is a balanced and representative sample of British English from the late 20th century, covering a wide range of text types including newspapers, journals, books, letters, essays, and transcribed speech. The authors argue that the BNC's quality and diversity make it a valuable resource for training language models, even with its relatively small size compared to modern corpora.\n\nThe authors introduce LTG-BERT, an optimized language model architecture that incorporates several modifications to the standard Transformer architecture. These modifications include NormFormer layer normalization, GEGLU activation function, and disentangled attention. Additionally, the paper evaluates different training objectives, such as subword, whole-word, and span masking for MLM, and document discrimination and sentence-order discrimination for next sentence prediction (NSP). The results show that LTG-BERT outperforms the original BERT model pre-trained on a much larger corpus, suggesting that careful curation and architectural improvements can lead to competitive performance with limited data.\n\n**Paper 2: Not all layers are equally as important: Every Layer Counts BERT**\n\nThis paper introduces ELC-BERT, a model that allows each transformer layer to selectively process outputs from previous layers. This modification, inspired by gated linear units (GLU) and layer weighting techniques, aims to improve the model's adaptability to different tasks by enabling more granular control over the information flow within the network. The authors participate in the BabyLM challenge and achieve top rankings in both the \\textsc{strict} and \\textsc{strict-small} tracks.\n\nThe ELC-BERT architecture is based on LTG-BERT but includes additional modifications such as attention gating and layer weighting. The attention gating mechanism simplifies the transformer architecture, while layer weighting allows each layer to choose which outputs from previous layers to process. The authors also explore the impact of different training settings, such as weight decay and learning rate schedules, on the model's performance. The results indicate that ELC-BERT outperforms both encoder-only and decoder-only baselines on a variety of benchmarks, highlighting the importance of layer weighting and attention gating in enhancing model performance.\n\n**Paper 3: GPT or BERT: why not both?**\n\nThis paper proposes GPT-BERT, a hybrid model that combines masked language modeling (MLM) and causal language modeling (CLM) objectives within a single transformer architecture. The authors pre-train the model on a mixture of corpora, including the BabyLM corpus, FineWeb-Edu, and Cosmopedia, to provide a diverse and rich training environment. The hybrid approach is evaluated on the BabyLM challenge, where the model outperforms both single-objective baselines and last year's winner, ELC-BERT.\n\nThe GPT-BERT model is trained using a 1:15 causal-to-masked ratio, which allows it to leverage the strengths of both paradigms. The authors also introduce several modifications to improve the model's efficiency and performance, such as attention gating, layer weighting, and dynamic batch and mask scheduling. The results show that the hybrid model performs better than single-objective models on a variety of benchmarks, including GLUE, BLiMP, and EWOK, suggesting that combining MLM and CLM objectives can lead to more robust and versatile language models.\n\n#### Commonalities and Innovations\n\nAll three papers share a common goal of developing more efficient and data-efficient language models. They utilize the BabyLM challenge as a benchmark to evaluate their models' performance on a variety of syntactic and NLU tasks. The challenge provides a controlled environment for comparing models trained on limited data, which is crucial for understanding the impact of data size and quality on model performance.\n\n**Commonalities:**\n- All papers use the BabyLM challenge as a benchmark for evaluating their models.\n- They pre-train their models on limited data (100M or 10M words) to test the efficiency of language modeling techniques.\n- The models are evaluated on a suite of benchmarks, including GLUE, SuperGLUE, BLiMP, and EWOK, to test their linguistic and general knowledge.\n\n**Innovations:**\n- **Paper 1:** Introduces LTG-BERT, an optimized architecture that incorporates NormFormer layer normalization, GEGLU activation function, and disentangled attention. The paper also evaluates different MLM and NSP objectives and shows that span masking outperforms subword and whole-word masking.\n- **Paper 2:** Proposes ELC-BERT, a model that allows each layer to selectively process outputs from previous layers, thereby enhancing the model's adaptability to different tasks. The paper also introduces attention gating and layer weighting, which simplify the transformer architecture and improve performance.\n- **Paper 3:** Introduces GPT-BERT, a hybrid model that combines MLM and CLM objectives within a single transformer architecture. The hybrid approach is evaluated on the BabyLM challenge and shows improved performance over single-objective models.\n\n#### Comparative Study of Results\n\nThe BabyLM challenge provides a robust framework for comparing the performance of different language models on limited data. The results from the three papers are summarized in the following tables:\n\n**Table 1: BabyLM Challenge Results for Paper 1 (LTG-BERT)**\n\\begin{table}[h]\n\\resizebox{\\columnwidth}{!}{\n\\begin{tabular}{@{}lrrrr@{}}\n\\toprule\n\\textbf{Model} & \\textbf{BLiMP} & \\textbf{Supp.} & \\textbf{MSGS} & \\textbf{GLUE} \\\\\n\\midrule\n$\\textrm{BERT}_\\textit{base, cased}$ & 84.2 & 61.8 & 82.0 & 87.8 \\\\\nLTG-BERT (subword masking) & 85.1 & 62.5 & 82.0 & 88.2 \\\\\nLTG-BERT (whole-word masking) & 84.9 & 63.7 & 80.1 & 88.4 \\\\\nLTG-BERT (span masking) & 85.1 & 65.6 & 84.2 & 89.2 \\\\\nLTG-BERT (subword + document NSP) & 85.2 & 60.5 & 83.3 & 88.6 \\\\\nLTG-BERT (subword + order NSP) & 84.7 & 64.2 & 82.2 & 88.3 \\\\\nLTG-BERT (subword + $2\\times$ steps) & 85.2 & 65.3 & 83.5 & 88.6 \\\\\nLTG-BERT (subword + $\\nicefrac{1}{2}\\times$ steps) & 84.4 & 62.4 & 83.5 & 88.5 \\\\\nLTG-BERT (subword + $\\nicefrac{1}{4}\\times$ steps) & 83.8 & 58.6 & 83.2 & 87.5 \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\caption{BabyLM Challenge Results for LTG-BERT Variants}\n\\label{tab:ltg-bert-results}\n\\end{table}\n\n**Table 2: BabyLM Challenge Results for Paper 2 (ELC-BERT)**\n\\begin{table}[h]\n\\resizebox{\\columnwidth}{!}{\n\\begin{tabular}{@{}lrrrr@{}}\n\\toprule\n\\textbf{Model} & \\textbf{BLiMP} & \\textbf{BLiMP-S} & \\textbf{GLUE} & \\textbf{EWOK} \\\\\n\\midrule\nEncoder-only (BabyLM baseline) & 60.6 & 60.8 & 60.3 & 48.9 \\\\\nDecoder-only (BabyLM baseline) & 69.8 & 59.5 & 63.3 & 52.1 \\\\\nELC-BERT (2023) & 80.5 & 67.9 & 75.3 & 51.0 \\\\\nLTG-BERT (2023) & 80.6 & 69.8 & 74.5 & --- \\\\\nGPT-BERT (ours) & 81.2 & 69.4 & 76.5 & 54.6 \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\caption{BabyLM Challenge Results for ELC-BERT and GPT-BERT}\n\\label{tab:elc-gpt-bert-results}\n\\end{table}\n\n**Table 3: BabyLM Challenge Results for Paper 3 (GPT-BERT)**\n\\begin{table}[h]\n\\resizebox{\\columnwidth}{!}{\n\\begin{tabular}{@{}lrrrr@{}}\n\\toprule\n\\textbf{Model} & \\textbf{BLiMP} & \\textbf{BLiMP-S} & \\textbf{GLUE} & \\textbf{EWOK} \\\\\n\\midrule\nEncoder-only (BabyLM baseline) & 69.2 & 66.5 & 68.4 & 51.9 \\\\\nDecoder-only (BabyLM baseline) & 73.1 & 60.6 & 69.0 & 52.1 \\\\\nELC-BERT (2023) & 85.8 & 76.8 & 78.3 & 56.3 \\\\\nLTG-BERT (2023) & 85.3 & 76.6 & 77.9 & 56.0 \\\\\nGPT-BERT (ours) & 86.1 & 76.8 & 81.5 & 58.4 \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\caption{BabyLM Challenge Results for GPT-BERT}\n\\label{tab:gpt-bert-results}\n\\end{table}\n\n**Discussion:**\n- **Paper 1 (LTG-BERT):** The results show that LTG-BERT outperforms the original BERT model on all benchmarks, especially on GLUE and MSGS. Span masking is found to be the most effective MLM strategy, while the NSP objective does not significantly improve performance.\n- **Paper 2 (ELC-BERT):** ELC-BERT achieves the best results on the BabyLM challenge, outperforming both encoder-only and decoder-only baselines. The model's performance is enhanced by layer weighting and attention gating, which simplify the transformer architecture and improve adaptability.\n- **Paper 3 (GPT-BERT):** GPT-BERT outperforms both ELC-BERT and LTG-BERT on the \\textsc{strict-small} track and matches or slightly underperforms them on the \\textsc{strict} track. The hybrid approach shows improved performance on GLUE and EWOK, indicating its versatility in handling different tasks.\n\n#### Conclusion\n\nThe three papers collectively demonstrate the potential of efficient and data-efficient language modeling techniques. LTG-BERT, ELC-BERT, and GPT-BERT all achieve competitive performance on the BabyLM challenge, despite being trained on limited data. These results suggest that careful curation of training corpora, architectural modifications, and hybrid training objectives can significantly enhance the performance of language models.\n\n**Main Findings:**\n- LTG-BERT outperforms the original BERT model on all benchmarks, especially on GLUE and MSGS, indicating the importance of architectural modifications and training objectives.\n- ELC-BERT achieves the best results on the BabyLM challenge, highlighting the benefits of layer weighting and attention gating in simplifying the transformer architecture and improving adaptability.\n- GPT-BERT demonstrates the potential of combining MLM and CLM objectives within a single transformer stack, showing improved performance on GLUE and EWOK while maintaining robustness on other benchmarks.\n\n**Future Research Directions:**\n- Further exploration of hybrid training objectives to understand their scalability and generalizability to larger datasets and more complex tasks.\n- Investigation into the impact of different training corpora on model performance, especially for low-resource languages.\n- Development of more efficient training methods, such as dynamic batch and mask scheduling, to reduce the computational cost of pretraining.\n- Exploration of the cognitive processes involved in language modeling to better understand the underlying mechanisms and improve model interpretability.\n\nThese findings and future directions suggest that the field of language modeling is moving towards more efficient and data-efficient approaches, which can have significant implications for the development of reliable and versatile models in low-resource settings."
}