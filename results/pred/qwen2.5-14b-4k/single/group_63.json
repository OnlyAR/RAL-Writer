{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{MixFormer: End-to-End Tracking with Iterative Mixed Attention}\n\n\\begin{document}\n\n\\title{MixFormer: End-to-End Tracking with Iterative Mixed Attention}\n\n\\author{Yutao Cui \\quad \\quad Cheng Jiang \\quad \\quad Limin Wang \\textsuperscript{\\Letter} \\quad \\quad Gangshan Wu\\\\\nState Key Laboratory for Novel Software Technology,\nNanjing University,\nChina \\\\\n{\\tt\\small \\{cuiyutao,mg1933027\\}@smail.nju.edu.cn} \\quad\n{\\tt\\small \\{lmwang,gswu\\}@nju.edu.cn}\n}\n\n\\maketitle\n\\blfootnote{\\Letter~: Corresponding author.}\n\\begin{abstract}\nTracking often uses a multi-stage pipeline of feature extraction, target information integration, and bounding box estimation. To simplify this pipeline and unify the process of feature extraction and target information integration, we present a compact tracking framework, termed as {\\em MixFormer}, built upon transformers. Our core design is to utilize the flexibility of attention operations, and propose a Mixed Attention Module (MAM) for simultaneous feature extraction and target information integration. This synchronous modeling scheme allows to extract target-specific discriminative features and perform extensive communication between target and search area. Based on MAM, we build our MixFormer tracking framework simply by stacking multiple MAMs with progressive patch embedding and placing a localization head on top. In addition, to handle multiple target templates during online tracking, we devise an asymmetric attention scheme in MAM to reduce computational cost, and propose an effective score prediction module to select high-quality templates. \nOur MixFormer sets a new state-of-the-art performance on five tracking benchmarks, including LaSOT, TrackingNet, VOT2020, GOT-10k, and UAV123. \nIn particular, our MixFormer-L achieves NP score of 79.9\\% on LaSOT, 88.9\\% on TrackingNet and EAO of 0.555 on VOT2020.\nWe also perform in-depth ablation studies to demonstrate the effectiveness of simultaneous feature extraction and information integration. Code and trained models are publicly available at \\href{https://github.com/MCG-NJU/MixFormer}{https://github.com/MCG-NJU/MixFormer}.\n\n\\end{abstract}\n\n\\section{Introduction}\n\nVisual object tracking~\\cite{updt,mdnet,dcf_,CRPN,bacf,updt,staple,vital} has been a fundamental task in computer vision area for decades, aiming to estimate the state of an arbitrary target in video sequences given its initial status. It has been successfully deployed in various applications such as human computer interaction~\\cite{introduction1} and visual surveillance~\\cite{introduction2}. However, how to design a simple yet effective end-to-end tracker is still challenging in real-world scenarios. The main challenges are from aspects of scale variations, object deformations, occlusion, and confusion from similar objects.\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=\\linewidth]{MixFormer_fig1.pdf}\n\\caption{{\\bf Comparison of tracking pipeline}. (a) The dominant tracking framework contains three components: a convolutional or transformer backbone, a carefully-designed integration module, and task-specific heads. (b) Our MixFormer is more compact and composed of two components: a target-search mixed attention based backbone and a simple localization head. \n}\n\\vspace{-3mm}\n\\label{fig:motivation}\n\\end{figure}\n\nCurrent prevailing trackers typically have a multi-stage pipeline as shown in Fig.~\\ref{fig:motivation}. It contains several components to accomplish the tracking task: (1) a backbone to extract generic features of tracking target and search area, (2) an integration module to allow information communication between tracking target and search area for subsequent target-aware localization, (3) task-specific heads to precisely localize the target and estimate its bounding box. Integration module is the key of tracking algorithms as it is responsible for incorporating the target information to bridge the steps of generic feature extraction and target-aware localization. \nTraditional integration methods include correlation-based operations (e.g. SiamFC~\\cite{siamfc}, SiamRPN~\\cite{siamrpn}, CRPN~\\cite{CRPN}, SiamFC++~\\cite{siamfc++}, SiamBAN~\\cite{siamban}, OCEAN~\\cite{ocean}) and online learning algorithms (e.g., DCF~\\cite{dcf_}, KCF~\\cite{kcf}, CSR-DCF~\\cite{csr_dcf}, ATOM~\\cite{atom}, DiMP~\\cite{dimp}, FCOT~\\cite{fcot}). Recently, thanks to its global and dynamic modeling capacity, transformers~\\cite{transformer} are introduced to perform attention based integration and yields good tracking performance (e.g., TransT~\\cite{tt}, TMT~\\cite{tmt}, STMTrack~\\cite{stmtrack}, TREG~\\cite{treg}, STARK~\\cite{stark}, DTT~\\cite{dtt}). \nHowever, these transformer based trackers still depend on the CNN for generic feature extraction, and only apply attention operations in the latter high-level and abstract representation space. We analyze that these CNN representations are limited as they are typically pre-trained for generic object recognition and might neglect finer structure information for tracking. In addition, these CNN representations employ local convolutional kernels and lack global modeling power. Therefore, CNN representation is still their bottleneck, which prevents them from fully unleashing power of self-attention for the whole tracking pipeline.\n\nTo overcome the above issue, we present a new perspective on tracking framework design that generic feature extraction and target information integration should be coupled together within a unified framework. This coupled processing paradigm shares several key advantages. First, it will enable our feature extraction to be more specific to the corresponding tracking target and capture more target-specific discriminative features. Second, it also allows the target information to be more extensively integrated into search area, and thereby to better capture their correlation. In addition, this will result in a more compact and neat tracking pipeline only with a single backbone and tracking head, without an explicit integration module.\n\nBased on the above analysis, in this paper, we introduce the \\emph{MixFormer}, a simple tracking framework designed for unifying the feature extraction and target integration solely with a transformer-based architecture. Attention module is a very flexible architectural building block with dynamic and global modeling capacity, which makes few assumption about the data structure and could be generally applied for general relation modeling. Our core idea is to utilize this flexibility of attention operation, and present a {\\em mixed attention module} (MAM) that performs both of feature extraction and mutual interaction of target template and search area at the same time. In particular, in our MAM, we devise a hybrid interaction scheme with both self-attention and cross-attention operations on the tokens from target template and search area. The self-attention is responsible to extract their own features of target or search area, while the cross-attention allows for the communications between them to mix the target and search area information. To reduce computational cost of MAM and thereby allow for multiple templates to handle object deformation, we further present a customized {\\em asymmetric} attention scheme by pruning the unnecessary target-to-search area cross-attention.\n\nFollowing the successful transformer architecture in image recognition, we build our MixFormer backbone by stacking the layers of Patch Embedding and MAM, and finally place a simple localization head to yield our whole tracking framework. \nAs a common practice in dealing with object deformation during tracking procedure, we also propose a score based target template update mechanism and our MixFormer could be easily adapted for multiple target template inputs. Extensive experiments on several benchmarks demonstrate that \\textit{MixFormer} sets a new state-of-the-art performance, with a real-time running speed of 25 FPS on a GTX 1080Ti GPU.\nEspecially, \\textit{MixFormer-L} surpasses STARK~\\cite{stark} by 5.0\\% (EAO score) on VOT2020, 2.9\\% (NP score) on LaSOT and 2.0\\% (NP score) on TrackingNet.\n\nThe main contributions are summarized as follows:\n\\begin{itemize}\n\\item We propose a compact end-to-end tracking framework, termed as \\emph{MixFormer}, based on iterative Mixed Attention Modules (MAM). It allows for extracting target-specific discriminative features and extensive communication between target and search simultaneously.\n\\item For online template update, we devise a customized asymmetric attention in MAM for high efficiency, and propose an effective score prediction module to select high-quality templates, leading to an efficient and effective online transformer-based tracker. \n\\item The proposed \\textit{MixFormer} sets a new state-of-the-art performance on five challenging benchmarks, including VOT2020~\\cite{vot2020}, LaSOT~\\cite{lasot}, TrackingNet~\\cite{trackingnet}, GOT-10k~\\cite{got10k}, and UAV123~\\cite{uav123}.\n\n\\end{itemize}\n\n\\section{Related Work}\n\\paragraph{Tracking Paradigm.}\nCurrent prevailing tracking methods can be summarized as a three-parts architectures, containing (\\romannumeral1) a backbone to extract generic features, (\\romannumeral2) an integration module to fuse the target and search region information, (\\romannumeral3) heads to produce the target states. Generally, most trackers~\\cite{siamrpnPlus, atom,dimp,transt,siamrcnn} used ResNet as the backbone. For the most important integration module, researchers explored various methods. Siamese-based trackers~\\cite{siamfc,siamrpn,siamcar,dasiamrpn} combined a correlation operation with the Siamese network, modeling the global dependencies between the target and search. Some online trackers~\\cite{eco,kcf,bacf,strcf,dcf_,atom,dimp,fcot} learned an target-dependent model for discriminative tracking. Furthermore, some recent trackers~\\cite{transt,stark,treg,stmtrack,tmt} introduced a transformer-based integration module to capture more complicated dependencies and achieved impressive performance. Instead, we propose a fully end-to-end transformer tracker, solely containing a MAM based backbone and a simple head, leading to a more accurate tracker with neat and compact architecture.\n\n\\paragraph{Vision Transformer.} \nThe Vision Transformer (ViT)~\\cite{vit} first presented a pure vision transformer architecture, obtaining an impressive performance on image classification.\nSome works~\\cite{t2t,localvit,pvt} introduced design changes to better model local context in vision Transformers. For example, PVT~\\cite{pvt} incorporated a multi-stage design (without convolutions) for Transformer similar to multi-scales in CNNs.\nCVT~\\cite{cvt} combined CNNs and Transformers to model both local and global dependencies for image classification in an efficient way. Our MixFomer uses the pre-trained CVT models, but there are some fundamental differences. (\\romannumeral1) The proposed MAM performs dual attentions for both feature extraction and information integration, while CVT uses self attention to solely extract features. (\\romannumeral2) The learning tasks are different, and the corresponding input and the head are different. We use multiple templates together with the search region as input and employ a corner-based or query-based localization head for bounding box generation, while CVT is designed for image classification. (\\romannumeral3) We further introduce an asymmetric mixed attention and a score prediction module for the specific task of online tracking. \n\nAttention machenism has been also explored in object tracking recently. CGCAD~\\cite{CGACD} and SiamAttn~\\cite{siamattn} introduced a correlation-guided attention and self-attention to perform discriminative tracking. TransT~\\cite{transt} designed a transformer-based fusion network for target-search information incorporation. These methods still relied on post-processing for box generation. Inspired by DETR~\\cite{detr}, STARK~\\cite{stark} further proposed an end-to-end transformer-based tracker. However, it still followed the paradigm of \\emph{Backbone-Integration-Head}, with separate feature extraction and information integration modules.\nMeanwhile, TREG~\\cite{treg} proposed a target-aware transformer for regression branch and can generate accurate prediction in VOT2021~\\cite{vot2021}. Inspired by TREG, we formulate mixed attention mechanism by using both self attention and cross attention.\nIn this way, our MixFormer unifies the two processes of feature extraction and information integration with an iterative MAM based backbone, leading to a more compact, neat and effective end-to-end tracker.\n\n\\begin{figure*}[pt]\n\\centering\n\\includegraphics[width=0.8\\linewidth]{MixFormer_fig3.pdf}\n\\vspace{-3mm}\n\\caption{{\\bf Mixed Attention Module (MAM)} is a flexible attention operation that unifies the process of feature extraction and information integration for target template and search area. This mixed attention has dual attention operations where self-attention is performed to extract features from itself while cross-attention is conducted to communicate between target and search. This MAM could be easily implemented with a concatenated token sequence. To further improve efficiency, we propose an asymmetric MAM by pruning the target-to-search cross attention (denoted by dashed lines).\n}\n\\vspace{-4mm}\n\\label{fig_mam}\n\\end{figure*}\n\n\\section{Method}\nIn this section, we present our end-to-end tracking framework, termed as \\emph{MixFormer}, based on iterative mixed attention modules (MAM). First, we introduce our proposed MAM to unify the process of feature extraction and target information incorporation. This simultaneous processing scheme will enable our feature extraction to be more specific to the corresponding tracking target. In addition, it also allows the target information integration to be performed more extensively and thus to better capture the correlation between target and search area. Then, we present the whole tracking framework of MixFormer, which only includes a MAM-based backbone and localization head. Finally, we describe the training and inference of MixFormer by devising a confidence score based target template update mechanism to handle object deformation in tracking procedure.\n\n\\subsection{Mixed Attention Module (MAM)}\n\\label{sec:MAM}\n\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=0.85\\linewidth]{MixFormer_fig2.pdf}\n\\vspace{-2mm}\n\\caption{{\\bf MixFormer} presents a compact end-to-end framework for tracking without explicitly decoupling steps of feature extraction and target information integration. It is only composed of a single MAM backbone and a localization head.}\n\\label{fig_arch}\n\\vspace{-6mm}\n\\end{figure*}\n\nMixed attention module (MAM) is the core design to pursue a neat and compact end-to-end tracker. The input to our MAM is the {\\em target template} and {\\em search area}. It aims to simultaneously extract their own long-range features and fuse the interaction information between them.\nIn contrast to the original Multi Head Attention~\\cite{transformer}, MAM performs dual attention operations on two separate tokens sequences of target template and search area. It carries out self-attention on tokens in each sequence themselves to capture the target or search specific information. Meanwhile, it conducts cross-attention between tokens from two sequences to allow communication between target template and search area. As shown in Fig.~\\ref{fig_mam}, this mixed attention mechanism could be implemented efficiently via a concatenated token sequence.\n\nFormally, given a concatenated tokens of multiple targets and search, we first split it into two parts and reshape them to 2D feature maps. In order to achieve additional modeling of local spatial context, a separable depth-wise convolutional projection layer is performed on each feature map (i.e., \\emph{query}, \\emph{key} and \\emph{value}). It also provides efficiency benefits by allowing the down-sampling in \\emph{key} and \\emph{value} matrices. Then each feature map of target and search is flattened and processed by a linear projection to produce queries, keys and values of the attention operation. We use $q_t$, $k_t$ and $v_t$ to represent target, $q_s$, $k_s$ and $v_s$ to represent search region. The mixed attention is defined as: \n\\begin{equation}\n\\vspace{-2mm}\n\\begin{aligned}\n    & k_m = {\\rm Concat}(k_t, k_s), \\ \\ \\ v_m = {\\rm Concat}(v_t, v_s), \\\\ \n    & {\\rm Attention_{t}} = {\\rm Softmax}(\\frac{q_{t}k_{m}^{T}}{\\sqrt{d}})v_{m},\\\\\n    & {\\rm Attention}_{s} = {\\rm Softmax}(\\frac{q_{s}k_{m}^{T}}{\\sqrt{d}})v_{m},\n\\end{aligned}\n\\end{equation}\nwhere $d$ represents the dimension of the key, ${\\rm Attention}_{t}$ and ${\\rm Attention}_{s}$ are the attention maps of the target and search respectively. It contains both self attention and cross attention which unifies the feature extraction and information integration. Finally, the targets token and search token are concatenated and processed by a linear projection.\n\n\\noindent \\textbf{Asymmetric mixed attention scheme.} Intuitively, the cross attention from the targets query to search area is not so important and might bring negative influence due to potential distractors. To reduce computational cost of MAM and thereby allowing for efficiently using multiple templates to deal with object deformation, we further present a customized {\\em asymmetric} mixed attention scheme by pruning the unnecessary target-to-search area cross-attention. This asymmetric mixed attention is defined as follows:\n\\begin{small}\n\\begin{equation}\n\\vspace{-1mm}\n\\begin{aligned}\n    & {\\rm Attention_{t}} = {\\rm Softmax}(\\frac{q_{t}k_{t}^{T}}{\\sqrt{d}})v_{t}, \\\\\n    & {\\rm Attention}_{s} = {\\rm Softmax}(\\frac{q_{s}k_{m}^{T}}{\\sqrt{d}})v_{m}.\n\\end{aligned}\n\\end{equation}\n\\end{small}\nIn this manner, the template tokens in each MAM could remain unchanged during tracking process since it avoids influence by the dynamic search regions.\n\\paragraph{Discussions.}\nTo better expound the insight of the mixed attention, we make a comparison with the attention mechanism used by other transformer trackers.\nDifferent with our mixed attention, TransT~\\cite{tt} uses ego-context augment and cross-feature augment modules to perform self attention and cross attention progressively in two steps. Compared to the transformer encoder of STARK~\\cite{stark}, our MAM shares a similar attention mechanism but with three notable differences. First, we incorporate the spatial structure information with a depth-wise convolution while they use positional encoding. More importantly, our MAM is built as a multi-stage backbone for both feature extraction and information integration, while they depend on a separate CNN backbone for feature extraction and only focus on information integration in a single stage. Finally, we also propose a different asymmetric MAM to further improve the tracking efficiency without much accuracy drop.\n\n\\subsection{MixFormer for Tracking}\n\n\\paragraph{Overall Architecture.}\nBased on the MAM blocks, we build the MixFormer, a compact end-to-end tracking framework. The main idea of MixFormer is to progressively extract coupled features for target template and search area, and deeply perform the information integration between them. Basically, it comprises two components: a backbone composed of iterative target-search MAMs, and a simple localization head to produce the target bounding box.\nCompared with other prevailing trackers by decoupling the steps of feature extraction and information integration, it leads to a more compact and neat tracking pipeline only with a single backbone and tracking head, without an explicit integration module or any post-processing. \nThe overall architecture is depicted in Fig.~\\ref{fig_arch}. \n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=\\linewidth]{MixFormer_fig4.pdf}\n\\vspace{-6mm}\n\\caption{Structure of the Score Prediction Module (SPM). \n}\n\\vspace{-5mm}\n\\label{fig_score_module}\n\\end{figure}\n\\paragraph{MAM Based Backbone.}\nOur goal is to couple both the generic feature extraction and target information integration within a unified transformer-based architecture. The MAM-based backbone employs a progressive multi-stage architecture design. Each stage is defined by a set of $N$ MAM and MLP layers operating on the same-scaled feature maps with the identical channel number. All stages share the similar architecture, which consists of an overlapped patch embedding layer and $N_i$ target-search mixed attention modules (i.e., a combination of MAM and MLP layers in implementation). \n\nSpecifically, given $T$ templates (i.e., the first template and $T-1$ online templates) with the size of $T\\times H_{t}\\times{W_{t}}\\times3$ and a search region (a cropped region according to the previous target states) with the size of $H_{s}\\times{W_{s}}\\times3$, we first map them into overlapped patch embeddings using a \\emph{convolutional Token Embedding} layer with stride $4$ and kernel size $7$. \nThe convolutional token embedding layer is introduced in each stage to grow the channel resolution while reducing the spatial resolution. \nThen we flatten the patch embeddings and concatenate them, yielding a fused token sequence with the size of $(T\\times\\frac{H_{t}}{4}\\times\\frac{W_t}{4}+\\frac{H_s}{4}\\times\\frac{W_s}{4})\\times C$, where $C$ equals to 64 or 192, $H_t$ and $W_t$ is 128, $H_s$ and $W_s$ is 320 in this work. \nAfter that, the concatenated tokens pass through $N_i$ target-search MAM to perform both feature extraction and target information incorporation. \nFinally, we obtain the token sequence of size $(T\\times\\frac{H_{t}}{16}\\times\\frac{W_t}{16}+\\frac{H_s}{16}\\times\\frac{W_s}{16})\\times6C$. More details about the MAM backbones could be found in the Section~\\ref{model_arch} and Table~\\ref{tab:arch}.\nBefore passed to the prediction head, the search tokens are split and reshaped to the size of $\\frac{H_{s}}{16}\\times\\frac{W_s}{16}\\times6C$. \nParticularly, we do not apply the multi-scale feature aggregation strategy, commonly used in other trackers (e.g., SiamRPN++~\\cite{siamrpnPlus}, STARK~\\cite{stark}).\n\\vspace{-3mm}\n\\paragraph{Corner Based Localization Head.}\\label{corner_head}\nInspired by the corner detection head in STARK~\\cite{stark}, we employ a fully-convolutional corner based localization head to directly estimate the bounding box of tracked object, solely with several $\\rm Conv$-$\\rm BN$-$\\rm ReLU$ layers for the top-left and the bottom-right corners prediction respectively. At last, we can obtain the bounding box by computing the expectation over corner probability distribution~\\cite{GFloss}. The difference with STARK lies in that ours is a fully convolutional head while STARK highly relies on both encoder and the decoder with more complicated design.\n\\vspace{-3mm}\n\\paragraph{Query Based Localization Head.}\\label{query_head}\nInspired by DETR~\\cite{detr}, we propose to employ a simple query based localization head. This sparse localization head can verify the generalization ability of our MAM backbone and yield a pure transformer-based tracking framework. Specifically, we add an extra learnable \\emph{regression token} to the sequence of the final stage and use this token as an anchor to aggregate information from entire target and search area. Finally, a FFN of three fully connected layers is employed to directly regress the bounding box coordinates. This framework does not use any post-processing technique either.\n\n\\subsection{Training and Inference}\n\\noindent {\\bf Training.}\nThe training process of our MixFormer generally follows the standard training recipe of current trackers~\\cite{stark,transt}. We first pre-train our MAM with a CVT model~\\cite{cvt}, and then fine-tune the whole tracking framework on the target dataset. \nSpecifically, a combination of ${L_1}$ loss and GIoU loss~\\cite{giou} is employed as follows:\n\\begin{equation}\n    L_{loc} = \\lambda_{L1} L_1(B_i, \\hat{B_i}) + \\lambda_{giou} L_{giou}(B_i, \\hat{B_i}),\n\\end{equation}\nwhere $\\lambda_{L1}=5$ and $\\lambda_{giou}=2$ are the weights of the two losses, $B_i$ is the ground-truth bounding box and $\\hat{B_i}$ is the predicted bounding box of the targets. \n\n\\noindent {\\bf Template Online Update.}\nOnline templates play an important role in capturing temporal information and dealing with object deformation and appearance variations. \nHowever, it is well recognized that poor-quality templates may lead to inferior tracking performance. As a consequence, we introduce a score prediction module (SPM), described in Fig.~\\ref{fig_score_module}, to select reliable online templates determined by the predicted confidence score. The SPM is composed of two attention blocks and a three-layer perceptron. First, a learnable \\emph{score token} serves as a query to attend the search ROI tokens. It enables the score token to encode the mined target information. Next, the score token attends to all positions of the initial target token to implicitly compare the mined target with the first target. Finally, the score is produced by the MLP layer and a sigmoid activation. The online template is treated as negative when its predicted score is below than 0.5.\n\nFor the SPM training, it is performed after the backbone training and we use a standard cross-entropy loss:\n\\begin{equation}\n\\begin{split}\nL_{score} = y_i{\\rm log}(p_i)+(1-y_i){\\rm log}(1-p_i), \n\\end{split}\n\\end{equation}\nwhere $y_i$ is the ground-truth label and $p_i$ is the predicted confidence score.\n\n\\noindent {\\bf Inference.} During inference, multiple templates, including one static template and $N$ dynamic online templates, together with the cropped search region are fed into MixFormer to produce the target bounding box and the confidence score. We update the online templates only when the update interval is reached and select the sample with the highest confidence score. \n\n\\section{Experiments}\n\n\\subsection{Implementation Details}\nOur trackers are implemented using Python 3.6 and PyTorch 1.7.1. The MixFormer training is conducted on 8 Tesla V100 GPUs. Especially, MixFormer is a neat tracker \\textbf{without} post-processing, positional embedding and multi-layer feature aggregation strategy.\n\\vspace{-4mm}\n\\begin{table*}[ht]\n    \\centering\n    \\fontsize{7pt}{3.5mm}\\selectfont\n    \\setlength{\\tabcolsep}{0.6mm}{\n    \\begin{tabular}{c|cccccccccc|ccc}\n    \\toprule\n        & KCF & STM & SiamMask & D3S & SuperDiMP & AlphaRef & OceanPlus  & RPT & DualTFR & STARK & \\textbf{MixFormer-1k} & \\textbf{MixFormer-22k} & \\textbf{MixFormer-L}\\cr\n        & ~\\cite{kcf}& ~\\cite{stm}& ~\\cite{siammask} &~\\cite{d3s} &~\\cite{dimp} &~\\cite{alpha-refine} &~\\cite{oceanPlus} & ~\\cite{rpt} & ~\\cite{dualtfr}& ~\\cite{stark} & & & \\cr\n        \\midrule\n        EAO & 0.154 & 0.308&  0.321 & 0.439 & 0.305 & 0.482 & 0.491 & 0.530 & 0.528 & 0.505 & 0.527 & \\textbf{\\textcolor{blue}{0.535}} & \\textbf{\\textcolor{red}{0.555}}\\\\\n        Accuracy & 0.407 & 0.751 & 0.624 & 0.699 & 0.492 & 0.754 & 0.685 & 0.700 & 0.755 & 0.759& 0.746 & \\textbf{\\textcolor{red}{0.761}} & \\textbf{\\textcolor{blue}{0.762}}\\\\\n        Robustness & 0.432 & 0.574 & 0.648 & 0.769 & 0.745 & 0.777 & 0.842 & \\textbf{\\textcolor{red}{0.869}} & 0.836 & 0.817 &\n        0.833 & 0.854 & \\textbf{\\textcolor{blue}{0.855}}\\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n    \\vspace{-2mm}\n    \\caption{State-of-the-art comparison on VOT2020~\\cite{vot2020}. The best two results are shown in \\textbf{\\textcolor{red}{red}} and \\textbf{\\textcolor{blue}{blue}} fonts. Our trackers use Alpha-Refine~\\cite{alpha-refine} to predict masks. MixFormer-1k is pretrained with ImageNet-1k. Others are pretrained with ImageNet-22k.} \n    \\label{tab:vot2020}\n\\vspace{-3mm}\n\\end{table*}\n\n\\begin{table}[pt]\n\\centering\n\\fontsize{7pt}{3.5mm}\\selectfont\n\\setlength{\\tabcolsep}{0.7mm}{\n\\resizebox{0.99\\columnwidth}{!}{\n\\begin{tabular}{c|c|l|c|c}\n\\hline\n\\multicolumn{1}{l|}{}     \n& Output Size\n& Layer Name                                                          \n& MixFormer \n& MixFormer-L \\\\ \n\\hline\n\n\\multirow{3}{*}{Stage1}    \n& $\\begin{array}{c} S: 80\\times80,\\\\ T: 32\\times32\n\\end{array}$\n& Conv. Embed.                                                     \n& \\multicolumn{1}{c|}{$7\\times7$, $64$, stride $4$}                   \n& $7\\times7$, $192$, stride $4$               \\\\ \n\\cline{2-5} \n\n& $\\begin{array}{c} S: 80\\times80,\\\\ T: 32\\times32\n\\end{array}$\n& \\begin{tabular}[c]{@{}l@{}}MAM\\\\ MLP\\end{tabular} \n&             \n$\\left[\n\\begin{array}{c}\n     H_1 = 1 \\\\\n     D_1=64 \\\\\n     R_1=4\n\\end{array}\n\\right] \\times 1$\n&      \n$\\begin{bmatrix}\n\\begin{array}{c}\n     H_1=3\\\\\n     D_1=192\\\\\n     R_1=4  \n\\end{array}\n\\end{bmatrix} \\times 2$\n\\\\ \\hline\n\n\\multirow{3}{*}{Stage2}    \n& $\\begin{array}{c} S: 40\\times40,\\\\ T: 16\\times16\n\\end{array}$\n&  Conv. Embed.\n& \\multicolumn{1}{c|}{$3\\times3$, $192$, stride $2$}                   \n& $3\\times3$, $768$, stride $2$\n\\\\ \\cline{2-5} \n                           \n& $\\begin{array}{c} S: 40\\times40,\\\\ T: 16\\times16\n\\end{array}$\n& \\begin{tabular}[c]{@{}l@{}} MAM \\\\ MLP\\end{tabular} \n&             \n$\\begin{bmatrix}\n\\begin{array}{c}\n     H_2=3 \\\\\n     D_2=192 \\\\\n     R_2=4 \n\\end{array}\n\\end{bmatrix} \\times 4$\n&             \n$\\begin{bmatrix}\n\\begin{array}{c}\n     H_2=12, \\\\ D_2=768,\\\\\n     R_2=4  \n\\end{array}\n\\end{bmatrix} \\times 2$\n\\\\ \\hline\n\n\\multirow{3}{*}{Stage3}    \n& $ \\begin{array}{c} S: 20\\times20,\\\\ T: 8\\times8\n\\end{array} $      \n& Conv. Embed.\n& \\multicolumn{1}{c|}{$3\\times3$, $384$, stride $2$}                   \n& $3\\times3$, $1024$, stride $2$\n\\\\ \\cline{2-5} \n                           \n& $\\begin{array}{c} S: 20\\times20,\\\\ T: 8\\times8\n\\end{array}$\n& \\begin{tabular}[c]{@{}l@{}} MAM\\\\ MLP\\end{tabular} \n&             \n$\\begin{bmatrix}\n\\begin{array}{c}\n     H_3=6 \\\\\n     D_3=384 \\\\\n     R_3=4  \n\\end{array}\n\\end{bmatrix} \\times 16$\n&\n$\\begin{bmatrix}\n\\begin{array}{c}\n     H_3=16 \\\\\n     D_3=1024 \\\\\n     R_3=4 \n\\end{array}\n\\end{bmatrix} \\times 12$\n\\\\ \\hline\n\n\\multicolumn{3}{c|}{MACs}  \n& $35.61$ M\n& $183.89$ M\n\\\\ \\hline\n\\multicolumn{3}{c|}{FLOPs}  \n& $23.04$ G\n& $127.81$ G\n\\\\ \\hline\n\\multicolumn{3}{c|}{Speed (1080Ti)}  \n& $25$ FPS \n& $18$ FPS \n\\\\ \\hline\n\\end{tabular}\n}}\n\\vspace{-2mm}\n\\caption{MAM based backbone architectures for MixFormer and MixFormer-L. The input is a tuple of templates with shape of $128\\times128\\times3$ and search region with shape of $320\\times320\\times3$. $S$ and $T$ represent for the search region and template. $H_i$ and $D_i$ is the head number and embedding feature dimension in the $i$-th stage. $R_i$ is the feature dimension expansion ratio in the MLP layer. \n}\n\\label{tab:arch}\n\\vspace{-4mm}\n\\end{table}\n\n\\paragraph{Architectures.}\\label{model_arch}\nAs shown in Table~\\ref{tab:arch}, we instantiate two models, MixFormer and MixFormer-L, with different parameters and FLOPs by varying the number of MAM blocks and the hidden feature dimension in each stage. The backbone of MixFormer and MixFormer-L are initialized with the CVT-21 and CVT24-W~\\cite{cvt} (first 16 layers are employed) pretrained on ImageNet~\\cite{imagenet} respectively.\n\\vspace{-4mm}\n\\paragraph{Training.} \nThe training set includes TrackingNet~\\cite{trackingnet}, LaSOT~\\cite{lasot}, GOT-10k~\\cite{got10k} and COCO~\\cite{coco} training dataset, which is the same as DiMP~\\cite{dimp} and STARK~\\cite{stark}.\nWhile for GOT-10k test, we train our tracker by only using the GOT10k train split following its standard protocol. \nThe whole training process of MixFormer consists of two stages, which contains the first 500 epochs for backbones and heads, and extra 40 epochs for score prediction head. \nWe train the MixFormer by using ADAM~\\cite{adam} with weight decay $10^{-4}$. The learning rate is initialized as $1e$-$4$ and decreased to $1e$-$5$ at the epoch of 400. \nThe sizes of search images and templates are $320\\times320$\npixels and $128\\times128$ pixels respectively. For data augmentations, we use horizontal flip and brightness jittering.\n\\vspace{-4mm}\n\\paragraph{Inference.}\nWe use the first template and multiple online templates together with the current search region as input of MixFormer. The dynamic templates are updated when the update interval of 200 is reached by default. The template with the highest predicted score in the interval is selected to substitute the previous one.\n\n\\subsection{Comparison with the state-of-the-art trackers}\nWe verify the performance of our proposed MixFormer-1k, MixFormer-22k, and MixFormer-L on five benchmarks, including VOT2020~\\cite{vot2020}, LaSOT~\\cite{lasot}, TrackingNet~\\cite{trackingnet}, GOT10k~\\cite{got10k}, UAV123~\\cite{uav123}.\n\\vspace{-4mm}\n\\paragraph{VOT2020.}\nVOT2020~\\cite{vot2020} consists of 60 videos with several challenges including fast motion, occlusion, etc.\nAs shown in Table~\\ref{tab:vot2020}, MixFormer-L achieves the top-ranked performance on EAO criteria of 0.555, which outperforms the transformer tracker STARK with a large margin of 5\\% of EAO. MixFormer-22k also outperforms other trackers including RPT (VOT2020 short-term challenge winner).\n\n\\begin{table*}[ht]\n    \\centering\n    \\fontsize{7}{9}\\selectfont  \n    \\setlength{\\tabcolsep}{2.0mm}{\n    \\begin{tabular}{c|ccc|ccc|ccc|cc}\n    \\toprule\n    \\multirow{2}{*}{Method} &\n    \\multicolumn{3}{c|}{LaSOT} &\n    \\multicolumn{3}{c|}{TrackingNet} &\n    \\multicolumn{3}{c|}{GOT-10k} &\n    \\multicolumn{2}{c}{UAV123} \\\\\n    \\cline{2-12}\n     & AUC(\\%) & $P_{Norm}$(\\%) & P(\\%) & AUC(\\%) & $P_{Norm}(\\%)$ & P(\\%) & AO(\\%) & $SR_{0.5}$(\\%) & $SR_{0.75}$(\\%) & AUC(\\%) & P(\\%)\\\\\n    \\midrule\n    \\textbf{MixFormer-L} & \\textbf{\\textcolor{red}{70.1}} & \\textbf{\\textcolor{red}{79.9}} & \\textbf{\\textcolor{red}{76.3}} & \\textbf{\\textcolor{red}{83.9}} & \\textbf{\\textcolor{red}{88.9}} & \\textbf{\\textcolor{red}{83.1}} &\n    \\underline{75.6} & \\underline{85.7} & \\underline{72.8} &\n    69.5 & \\textbf{\\textcolor{blue}{91.0}} \\\\\n    \\textbf{MixFormer-22k} & \\textbf{\\textcolor{blue}{69.2}} & \\textbf{\\textcolor{blue}{78.7}} & \\textbf{\\textcolor{blue}{74.7}} & \\textbf{\\textcolor{blue}{83.1}} & \\textbf{\\textcolor{blue}{88.1}} & \\textbf{\\textcolor{blue}{81.6}}\n    & \\underline{72.6} & \\underline{82.2} & \\underline{68.8} & \\textbf{\\textcolor{red}{70.4}} & \\textbf{\\textcolor{red}{91.8}} \\\\\n    \\textbf{MixFormer-1k} & 67.9 & 77.3 & 73.9 & 82.6 & 87.7 & 81.2 & \\underline{73.2} & \\underline{83.2} & \\underline{70.2} & 68.7 & 89.5 \\\\ \n     \\textbf{MixFormer-22k*} & - & - & - & - & - & - & \\textbf{\\textcolor{blue}{70.7}} & \\textbf{\\textcolor{red}{80.0}} & \\textbf{\\textcolor{red}{67.8}} & - & - \\\\ \n    \\textbf{MixFormer-1k*} & - & - & - & - & - & - & \\textbf{\\textcolor{red}{71.2}} & \\textbf{\\textcolor{blue}{79.9}} & \\textbf{\\textcolor{blue}{65.8}} & - & - \\\\ \n    \\hline\n    STARK~\\cite{stark} & 67.1 & 77.0 & - & 82.0 & 86.9 & - & 68.8 & 78.1 & 64.1 & - & - \\\\\n    KeepTrack~\\cite{keeptrack} & 67.1 & 77.2 & 70.2 & - & - & - & - & - & - & \\textbf{\\textcolor{blue}{69.7}} & - \\\\\n    DTT~\\cite{dtt} & 60.1 & - & - & 79.6 & 85.0 & 78.9 & 63.4 & 74.9 & 51.4 & - & - \\\\\n    SAOT~\\cite{saot} & 61.6 & 70.8 & - & - & - & - & 64.0 & 75.9 & - & - & - \\\\\n    AutoMatch~\\cite{automatch} & 58.2 & - & 59.9 & 76.0 & - & 72.6 & 65.2 & 76.6 & 54.3 & - & - \\\\\n    TREG~\\cite{treg} & 64.0 & 74.1 & - & 78.5 & 83.8 & 75.0 & 66.8 & 77.8 & 57.2 & 66.9 & 88.4 \\\\\n    DualTFR~\\cite{dualtfr} & 63.5 & 72.0 & 66.5 & 80.1 & 84.9 & - & - & - & - & 68.2 & - \\\\\n    TransT~\\cite{transt} & 64.9 & 73.8 & 69.0 & 81.4 & 86.7 & 80.3 & 67.1 & 76.8 & 60.9 & 69.1 & - \\\\\n    TrDiMP~\\cite{tmt} & 63.9 & - & 61.4 & 78.4 & 83.3 & 73.1 & 67.1 & 77.7 & 58.3 & 67.5 & - \\\\\n    STMTracker~\\cite{stmtrack} & 60.6 & 69.3 & 63.3 & 80.3 & 85.1 & 76.7 & 64.2 & 73.7 & 57.5 & 64.7 & - \\\\\n    SiamR-CNN~\\cite{siamrcnn} & 64.8 & 72.2 & - & 81.2 & 85.4 & 80.0 & 64.9 & 72.8 & 59.7 & 64.9 & 83.4 \\\\\n    PrDiMP~\\cite{prdimp} & 59.8 & 68.8 & 60.8 & 75.8 & 81.6 & 70.4 & 63.4 & 73.8 & 54.3 & 68.0 & - \\\\\n    OCEAN~\\cite{ocean} & 56.0 & 65.1 & 56.6 & - & - & - & 61.1& 72.1 & 47.3 & - & - \\\\\n    FCOT~\\cite{fcot} & 57.2 & 67.8 & - & 75.4 & 82.9 & 72.6 & 63.4 & 76.6 & 52.1 & 65.6 & 87.3 \\\\\n    SiamGAT~\\cite{siamgat} & 53.9 & 63.3 & 53.0 & - & - & - & 62.7 & 74.3 & 48.8 & 64.6 & 84.3 \\\\\n    CGACD~\\cite{CGACD} & 51.8 & 62.6 & - & 71.1 & 80.0 & 69.3 & - & - & - & 63.3 & 83.3 \\\\\n    SiamFC++~\\cite{siamfc++} & 54.4& 62.3 & 54.7 & 75.4 & 80.0 & 70.5 & 59.5& 69.5 & 47.9 & - & - \\\\\n    MAML~\\cite{maml} & 52.3 & - & - & 75.7 & 82.2 & 72.5 & - & - & - & - & - \\\\\n    D3S~\\cite{d3s} & - & - & - & 72.8 & 76.8 & 66.4 & 59.7 & 67.6 & 46.2 & - & - \\\\\n    DiMP~\\cite{dimp} & 56.9 & 65.0 & 56.7 & 74.0 & 80.1 & 68.7 & 61.1 & 71.7 & 49.2 & 65.4 & - \\\\\n    ATOM~\\cite{atom} & 51.5 & 57.6 & 50.5 & 70.3 & 77.1 & 64.8 & 55.6 & 63.4 & 40.2 & 64.3 & - \\\\\n    SiamRPN++~\\cite{siamrpnPlus} & 49.6 & 56.9 & 49.1 & 73.3 & 80.0 & 69.4 & 51.7 & 61.6 & 32.5 & 61.0 & 80.3 \\\\\n    MDNet~\\cite{mdnet} & 39.7 & 46.0 & 37.3 & 60.6 & 70.5 & 56.5 & 29.9 & 30.3 & 9.9 & 52.8 & - \\\\\n    SiamFC~\\cite{siamfc} & 33.6 & 42.0 & 33.9 & 57.1 & 66.3 & 53.3 & 34.8 & 35.3 & 9.8 & 48.5 & 69.3 \\\\\n    \\bottomrule\n    \\end{tabular}}\n    \\vspace{-2mm}\n    \\caption{State-of-the-art comparison on TrackingNet~\\cite{trackingnet}, LaSOT~\\cite{lasot}, GOT-10k~\\cite{got10k} and UAV123~\\cite{uav123}. The best two results are shown in \\textbf{\\textcolor{red}{red}} and \\textbf{\\textcolor{blue}{blue}} fonts. The \\underline{underline} results of GOT-10k are not considered in the comparison, since the models are trained with datasets other than GOT-10k. MixFormer-1k is the model pretrained with ImageNet-1k. Others are pretrained with ImageNet-22k. * denotes for trackers trained only with GOT-10k train split.}\n    \\vspace{-5mm}\n    \\label{tab:resultsl}\n\\end{table*}\n\\vspace{-4mm}\n\\paragraph{LaSOT.}\nLaSOT~\\cite{lasot} has 280 videos in its test set. We evaluate our MixFormer on the test set to validate its long-term capability. \nThe Table~\\ref{tab:resultsl} shows that our MixFormer surpasses all other trackers with a large margin. Specifically, MixFormer-L achieves the top-ranked performance on NP of 79.9\\%, surpassing STARK by 2.9\\% even without multi-layers feature aggregation. \n\\vspace{-4mm}\n\\paragraph{TrackingNet.}\nTrackingNet~\\cite{trackingnet} provides over 30K videos with more than 14 million dense bounding box annotations. The videos are sampled from YouTube, covering target categories and scenes in real life. We validate MixFormer on its test set. From Table~\\ref{tab:resultsl}, we find that our MixFormer-22k and MixFormer-L set a new state-of-the-art performance on the large scale benchmark.\n\\vspace{-4mm}\n\\paragraph{GOT10k.}\nGOT10k~\\cite{got10k} is a large-scale dataset with over 10000 video segments and has 180 segments for the test set. Apart from generic classes of moving objects and motion patterns, the object classes in the train and test set are zero-overlapped. As shown in Table~\\ref{tab:resultsl}, our MixFormer-GOT obtain state-of-the-art performance on the test split.\n\\paragraph{UAV123.}\nUAV123~\\cite{uav123} is a large dataset containing 123 Sequences with average sequence length of 915 frames, which is captured from low-altitude UAVs. Table~\\ref{tab:resultsl} shows our results on UAV123 dataset. Our MixFormer-22k and MixFormer-L outperforms all other trackers.\n\n\\subsection{Exploration Studies}\nTo verify the effectiveness and give a thorough analysis on our proposed MixFormer, we perform a detailed ablation study on the large-scale LaSOT dataset.\n\\vspace{-4mm}\n\\paragraph{Simultaneous process vs. Separate process.} \nAs the core part of our MixFormer is to unify the procedure of feature extraction and target information integration, we compare it to the separate processing architecture (e.g. TransT~\\cite{tt}). The comparison results are shown in Table~\\ref{tab_component} \\#1, \\#2, \\#3 and \\#8. Experiments of \\#1 and \\#2 are end-to-end trackers comprising a self-attention based backbone, ${n}$ cross attention modules to perform information integration and a corner head. \\#3 is the tracker with CVT as backbone and TransT's ECA+CFA(4) as interaction. Experiment of \\#8 is our MixFormer {\\bf without} multiple online templates and asymmetric mechanism, denoted by \\emph{MixFormer-Base}. MixFormer-Base largely increases the model of \\#1 (using one CAM) and \\#2 (using three CAMs) by 8.6\\% and 7.9\\% with smaller parameters and FLOPs. This demonstrates the effectiveness of unified feature extraction and information integration, as both of them would benefit each other.\n\\vspace{-3mm}\n\\begin{table}[pt]\n    \\centering\n    \\fontsize{7.5}{8.5}\\selectfont  \n    \\setlength{\\tabcolsep}{0.8mm}{\n    \\vspace{-1mm}\n    \\begin{tabular}{c|ccc|ccc}\n        \\hline\n        \\text{\\#}&Backbone&Integration&Head&Params.&FLOPs&AUC\\\\\n        \\hline\n        1& SAM(21) & CAM(1) & Corner & 37.35M & 20.69G & 59.8 \\\\\n        2& SAM(21) & CAM(3) & Corner & 40.92M & 22.20G & 60.5 \\\\\n        3& SAM(21) & ECA+CFA(4) & Corner & 49.75M & 27.81G & 66.9 \\\\\n        \\hline\n        \\hline\n        4& SAM(20)+MAM(1) & - & Corner & 35.57M & 19.97G & 65.8 \\\\\n        5& SAM(15)+MAM(6) & - & Corner & 35.67M & 20.02G & 66.2 \\\\\n        6& SAM(10)+MAM(11) & - & Corner & 35.77M & 20.07G & 67.4 \\\\\n        7& SAM(5)+MAM(16) & - & Corner & 35.87M & 20.12G & 68.1 \\\\\n        \\hline\n        \\hline\n        8& MAM(21) & - & Corner & 35.97M & 20.85G & \\cellcolor{gray!20}\\textbf{68.4} \\\\\n        9& MAM(21) & - & Query & 31.46M & 19.13G & 66.0 \\\\\n        \\hline\n    \\end{tabular}\n    }\n    \\vspace{-1mm}\n    \\caption{Analysis of the MAM based framework. '-' denotes the component is not used. SAM represents for self attention module, CAM for cross attention module and MAM for the proposed mixed attention module. The numbers in brackets represent the number of the blocks. Performance is evaluated on LaSOT.}\n    \\label{tab_component}\n\\vspace{-6mm}\n\\end{table}\n\n\\vspace{-2mm}\n\\paragraph{Study on stages of MAM.}\nTo further verify the effectiveness of the MAMs, we conduct experiments as in Table~\\ref{tab_component} \\#4, \\#5, \\#6, \\#7 and \\#8, to investigate the performance of different numbers of MAM in our MixFormer. We compare our MAM with the self-attention operations (SAM) with out cross-branch information communication. We find that more MAMs contribute to higher AUC score.\nIt indicates that extensive target-aware feature extraction and hierarchical information integration play a critical role to construct an effective tracker, which is realized by the iterative MAM. Especially, when the number of MAM reaches 16, the performance reaches 68.1, which is comparable to the MixFormer-Base containing 21 MAMs.\n\\begin{table}[pt]\n    \\centering\n    \\setlength{\\tabcolsep}{1mm}{\n    \\vspace{-1mm}\n    \\small\n    \\begin{tabular}{c|c|cc}\n        \\hline\n        &Asymmetric.&FPS (1080Ti)&AUC\\\\\n        \\hline\n        MixFormer-Base& - & 19 & \\cellcolor{gray!20}\\textbf{68.4} \\\\\n        MixFormer-Base& \\checkmark & 25 & 68.1 \\\\\n        \\hline\n    \\end{tabular}\n    }\n    \\vspace{-2mm}\n    \\caption{Ablation for asymmetric mixed attention mechanism.}\n\\vspace{-1mm}\n\\label{tab_asym}\n\\end{table}\n\n\\begin{table}[pt]\n\\small\n    \\centering\n    \\setlength{\\tabcolsep}{1mm}{\n    \\vspace{-1mm}\n    \\begin{tabular}{c|cc|c}\n        \\hline\n        &Online & Score&AUC\\\\\n        \\hline\n        MixFormer& - & - & 68.1 \\\\\n        MixFormer& \\checkmark & - & 66.6 \\\\\n        MixFormer& \\checkmark & \\checkmark & \\cellcolor{gray!20}\\textbf{69.2} \\\\\n        \\hline\n    \\end{tabular}\n    }\n    \\vspace{-2mm}\n    \\caption{Ablation for online templates update mechanism.}\n    \\label{tab_online}\n\\vspace{-1mm}\n\\end{table}\n\n\\begin{table}[pt]\n\\small\n    \\centering\n    \\fontsize{6pt}{2.5mm}\\selectfont\n    \\setlength{\\tabcolsep}{1mm}{\n    \\vspace{-1mm}\n    \\small\n    \\begin{tabular}{c|cc|c}\n        \\hline\n        & Pretrain & Train &AUC\\\\\n        \\hline\n        MixFormer& ImageNet-1k & Whole &  67.9 \\\\\n        MixFormer& ImageNet-22k& Whole & \\cellcolor{gray!20}\\textbf{69.2} \\\\\n        MixFormer& ImageNet-22k& GOT-10k & 62.1 \\\\\n        \\hline\n    \\end{tabular}\n    }\n    \\vspace{-2mm}\n    \\caption{Study on pretraining and training datasets. 'Whole' denotes for using the whole datasets including GOT-10k, LaSOT, TrackingNet and COCO.}\n    \\label{tab_data}\n\\vspace{-7mm}\n\\end{table}\n\n\\vspace{-4mm}\n\\paragraph{Study on localization head.}\nTo verify the generalization ability of our MAM backbone, we evaluate the MixFormer-Base with two types of localization head as described in Section~\\ref{corner_head} (fully convolutional head vs. query based head). The results are shown as in Table~\\ref{tab_component} \\#8 and \\#9 for the corner head and the query-base head respectively.\nMixFormer-Base with the fully convolutional corner head outperforms that of the query-based head. Especially, MixFormer-Base with corner head surpass all the other state-of-the-art trackers even without any post-processing and online templates. \nBesides, MixFormer-Base with the query head, a pure transformer-based tracking framework, obtains a comparable AUC of 66.0 with STARK-ST and KeepTrack~\\cite{keeptrack} and far exceed query-head STARK-ST of 63.7.\nIt demonstrates the generalization ability of our MAM backbone.\n\\vspace{-4mm}\n\\paragraph{Study on asymmetric MAM.}\nThe asymmetric MAM is used to reduce computational cost and allows for usage of multiple templates during online tracking. As shown in Table~\\ref{tab_asym}, the asymmetric MixFormer-Base increases the running speed of 24\\% while achieving a comparable performance, which demonstrates asymmetric MAM is important for building an efficient tracker.\n\\vspace{-4mm}\n\\paragraph{Study on online template update.}\nAs demonstrated in Table~\\ref{tab_online}, MixFormer with online templates, sampled by a fixed update interval, performs worse than that with only the first template, and the online MixFormer with our score prediction module achieves the best AUC score. It suggests that selecting reliable templates with our score prediction module is of vital importance.\n\\vspace{-4mm}\n\\paragraph{Study on training and pre-training datasets.}\nTo verify the generalization ablility of our MixFormer, we conduct an analysis on different pre-training and training datasets, as shown in Table~\\ref{tab_data}. MixFormer pretrained by ImageNet-1k still outperforms all the SOTA trackers (e.g., TransT~\\cite{tt}, KeepTrack~\\cite{keeptrack}, STARK~\\cite{stark}), even without post-processing and multi-layer feature aggregation. In addition, MixFormer trained with GOT-10k also achieves an impressive AUC of 62.1, which outperforms a majority of trackers trained with the whole tracking datasets.\n\n\\begin{figure}[pt]\n\\centering\n\\includegraphics[width=\\linewidth]{mixformer_vis_attn.pdf}\n\\vspace{-4mm}\n\\caption{Visualization results of different attention weights on \\textit{Basketball} of OTB100. \\textbf{S-to-t} is search-to-template cross attention, \\textbf{S-to-OT} is search-to-online-template cross attention, \\textbf{S-to-S} is self attention of search region and \\textbf{OT-to-T} is online-template-to-template cross attention. \\textbf{S$i$-B$j$} represents for Stage-$i$ and Block-$j$ of MixFormer. Best viewed with zooming in.}\n\\vspace{-6mm}\n\\label{fig:vis_attn_bas}\n\\end{figure}\n\n\\vspace{-4mm}\n\\paragraph{Visualization of attention maps.}\n\\label{vis_attn}\nTo explore how the mixed attention works in MixFormer backbone, we visualize some attention maps in Fig.~\\ref{fig:vis_attn_bas}. From the four types of attention maps, we derive that: (\\romannumeral1) distractors in background get suppressed layer by layer, (\\romannumeral2) online templates may be more adaptive to appearance variation and help to discriminate the target, (\\romannumeral3) the foreground of multiple templates can be augmented by mutual cross attention, (\\romannumeral4) a certain position tends to interact with the surrounding local patch.\n\n\\section{Conclusion}\nWe have presented MixFormer, an end-to-end tracking framework with iterative mixed attention, aiming to unify the feature extraction and target integration and result in a neat and compact tracking pipeline. Mixed attention module performs both feature extraction and mutual interaction for target template and search area. In empirical evaluation, MixFormer shows a notable improvement over other prevailing trackers for short-term tracking. In the future, we consider extending MixFormer to multiple object tracking.\n\n\\vspace{1mm}\n\\small \\noindent {\\bf Acknowledgement.} This work is supported by National Natural Science Foundation of China  (No.62076119, No.61921006),  Program for Innovative Talents and Entrepreneur in Jiangsu Province, and Collaborative Innovation Center of Novel Software Technology and Industrialization.\n\n\\section*{Appendix}\n\n\\begin{figure*}[pt]\n\\centering\n\\includegraphics[width=0.9\\linewidth]{mixformer_vis_attn2.pdf}\n\\vspace{-3mm}\n\\caption{Visualization results of different attention weights on \\textit{car-2} of LaSOT. \\textbf{S-to-t} is search-to-template cross attention, \\textbf{S-to-OT} is search-to-online-template cross attention, \\textbf{S-to-S} is self attention of search region and \\textbf{OT-to-T} is online-template-to-template cross attention. \\textbf{S$i$-B$j$} represents for Stage-$i$ and Block-$j$ of MixFormer. Best viewed with zooming in.}\n\\vspace{-5mm}\n\\label{fig:vis_attn_car-2}\n\\end{figure*}\n\nIn this appendix, we first provide more results and analysis on OTB100~\\cite{otb} and LaSOT~\\cite{lasot} datasets. Then we give more visualization results of the attention weights on LaSOT. Finally, we provide more training details.\n\n\\section*{A. More Results}\n\n\\paragraph{OTB-100.}\nOTB100~\\cite{otb} is a commonly used benchmark, which evaluates performance on Precision and AUC scores. Figure.~\\ref{fig:otb} presents results of our trackers on both two metrics on OTB-100 benchmark. MixFormer-L reaches competitive performance w.r.t. state-of-the-art trackers, surpassing the transformer tracker TransT by 1.3\\% on AUC score. Besides, MixFormer-L is slightly higher than MixFormer.\n\n\\paragraph{LaSOT.}\nLaSOT~\\cite{lasot} has 280 videos in its test set. We evaluate our MixFormer on the test set to validate its long-term capability. To give a further analysis, we provide Success plot and Precision plot for LaSOT in Fig.~\\ref{fig:lasot}. It proves that improvement is due to both higher accuracy and robustness. \n\n\\begin{figure}[pb]\n\\centering\n\\includegraphics[width=\\linewidth]{MixFormer_otb.pdf}\n\\caption{State-of-the-art comparison on the OTB100 dataset. Best viewed with zooming in.}\n\\label{fig:otb}\n\\end{figure}\n\n\\begin{figure}[pt]\n\\centering\n\\includegraphics[width=\\linewidth]{mixformer_lasot.pdf}\n\\vspace{-8mm}\n\\caption{State-of-the-art comparison on the LaSOT dataset.}\n\\vspace{-5mm}\n\\label{fig:lasot}\n\\end{figure}\n\n\\section*{B. More Visualization Results}\nIn this section, we provide more visualization results of attention weights on \\textit{car-2} of LaSOT test dataset in Fig.~\\ref{fig:vis_attn_car-2}. From the example, we can arrive at the same conclusion with section~\\ref{vis_attn}. Besides, from the last two lines, we infer that the features of last two blocks tend to adapt to the bounding box prediction head.\n\n\\section*{C. Training Details}\n\nWe propose a 320x320 search region plus two 128x128 input images to make a fair comparison with prevailing trackers (e.g., Siamese-based trackers, STARK and TransT). Generally, we use 8 Tesla V100 GPUSs to train MixFormer with batch size of 32. MixFormer can also be trained on 8 2080Ti GPUs having only 11GB memory, with batch size of 8 per GPU. We use CvT21 and CvT24-W as the pretrained model for MixFormer and MixFormer-L respectively. We apply gradient clip strategy with the clip normalization rate of 0.1. For training stage-1 of MixFormer (i.e., MixFormer without SPM), we use GIoU loss and $L_1$ loss, with the weights of 2.0 and 5.0 respectively. Besides, the Batch Normalization layers of MixFormer backbone are frozen during the whole training process. For SPM training process, the backbone and corner-based localization head are frozen and the batch size is 32. SPM is trained for 40 epochs with the learning rate decays at 30 epochs.\n\n{\\small\n}\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{AiATrack: Attention in Attention for Transformer Visual Tracking}\n\n\\begin{document}\n\n\\pagestyle{headings}\n\\mainmatter\n\\def\\ECCVSubNumber{1951}  % Insert your submission number here\n\n\\title{AiATrack: Attention in Attention for Transformer Visual Tracking} % Replace with your title\n\n\\begin{comment}\n\\titlerunning{ECCV-22 submission ID \\ECCVSubNumber} \n\\authorrunning{ECCV-22 submission ID \\ECCVSubNumber} \n\\author{Anonymous ECCV submission}\n\\institute{Paper ID \\ECCVSubNumber}\n\\end{comment}\n\n\\titlerunning{AiATrack: Attention in Attention for Transformer Visual Tracking}\n\\author{Shenyuan Gao\\inst{1}, \nChunluan Zhou\\inst{2}, \nChao Ma\\inst{3}, \nXinggang Wang\\inst{1}, \nJunsong Yuan\\inst{4}}\n\\authorrunning{Gao et al.}\n\\institute{$^{1}$~Huazhong University of Science and Technology \\quad\\quad $^{2}$~Wormpex AI Research \\\\ $^{3}$~Shanghai Jiao Tong University \\quad\\quad $^{4}$~State University of New York at Buffalo \\\\\n\\email{shenyuangao@gmail.com}, \\quad \\email{czhou002@e.ntu.edu.sg} \\\\\n\\email{chaoma@sjtu.edu.cn}, \\quad \\email{xgwang@hust.edu.cn}, \\quad \\email{jsyuan@buffalo.edu}}\n\n\\maketitle\n\n\\begin{abstract}\nTransformer trackers have achieved impressive advancements recently, where the attention mechanism plays an important role. However, the independent correlation computation in the attention mechanism could result in noisy and ambiguous attention weights, which inhibits further performance improvement. To address this issue, we propose an attention in attention (AiA) module, which enhances appropriate correlations and suppresses erroneous ones by seeking consensus among all correlation vectors. Our AiA module can be readily applied to both self-attention blocks and cross-attention blocks to facilitate feature aggregation and information propagation for visual tracking. Moreover, we propose a streamlined Transformer tracking framework, dubbed AiATrack, by introducing efficient feature reuse and target-background embeddings to make full use of temporal references. Experiments show that our tracker achieves state-of-the-art performance on six tracking benchmarks while running at a real-time speed. Code and models are publicly available at \\href{https://github.com/Little-Podi/AiATrack}{https://github.com/Little-Podi/AiATrack}.\n\\keywords{Visual Tracking, Attention Mechanism, Vision Transformer}\n\\end{abstract}\n\n\\section{Introduction}\nVisual tracking is one of the fundamental tasks in computer vision. It has gained increasing attention because of its wide range of applications \\cite{marvasti2021deep,fiaz2019handcrafted}. Given a target with bounding box annotation in the initial frame of a video, the objective of visual tracking is to localize the target in successive frames. Over the past few years, Siamese trackers \\cite{bertinetto2016fully,li2018high,li2019siamrpn++,zhang2020ocean}, which regards the visual tracking task as a one-shot matching problem, have gained enormous popularity. Recently, several trackers \\cite{wang2021transformer,chen2021transformer,yu2021high,cao2021hift,yan2021learning,xing2022siamese} have explored the application of the Transformer \\cite{vaswani2017attention} architecture and achieved promising performance.\n\nThe crucial components in a typical Transformer tracking framework \\cite{wang2021transformer,chen2021transformer,yu2021high} are the attention blocks. As shown in Fig.~\\ref{figure-illustration}, the feature representations of the reference frame and search frame are enhanced via self-attention blocks, and the correlations between them are bridged via cross-attention blocks for target prediction in the search frame. The Transformer attention \\cite{vaswani2017attention} takes queries and a set of key-value pairs as input and outputs linear combinations of values with weights determined by the correlations between queries and the corresponding keys. The correlation map is computed by the scaled dot products between queries and keys. However, the correlation of each query-key pair is computed independently, which ignores the correlations of other query-key pairs. This could introduce erroneous correlations due to imperfect feature representations or the existence of distracting image patches in a background clutter scene, resulting in noisy and ambiguous attention weights as visualized in Fig.~\\ref{figure-visualization}.\n\nTo address the aforementioned issue, we propose a novel attention in attention (AiA) module, which extends the conventional attention \\cite{vaswani2017attention} by inserting an inner attention module. The introduced inner attention module is designed to refine the correlations by seeking consensus among all correlation vectors. The motivation of the AiA module is illustrated in Fig.~\\ref{figure-illustration}. Usually, if a key has a high correlation with a query, some of its neighboring keys will also have relatively high correlations with that query. Otherwise, the correlation might be noise. Motivated by this, we introduce the inner attention module to utilize these informative cues. Specifically, the inner attention module takes the raw correlations as queries, keys, and values and adjusts them to enhance the appropriate correlations of relevant query-key pairs and suppress the erroneous correlations of irrelevant query-key pairs. We show that the proposed AiA module can be readily inserted into the self-attention blocks to enhance feature aggregation and into the cross-attention block to facilitate information propagation, both of which are very important in a Transformer tracking framework. As a result, the overall tracking performance can be improved.\n\n\\definecolor{node1}{RGB}{84, 130, 53}\n\\definecolor{node2}{RGB}{197, 90, 17}\n\\definecolor{link1}{RGB}{255, 0, 0}\n\\definecolor{link2}{RGB}{112, 48, 160}\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=.98\\textwidth]{imgs/illustration.pdf}\n\\caption{Motivation of the proposed method. The left part of the figure shows a typical Transformer tracking framework. On the right, the nodes denote features at different positions in a feature map. These nodes serve as queries and keys for a self-attention block. The links between nodes represent the correlations between queries and keys in the attention mechanism. Some correlations of the \\textcolor{node1}{green} node is \\textcolor{link1}{erroneous} since it is linked to the nodes at irrelevant positions. By applying the proposed module to the raw correlations, we can seek consensus from the correlations of other nodes (\\eg the \\textcolor{node2}{brown} node) that can provide supporting cues for the \\textcolor{link2}{appropriate} correlations. By this means, the quality of the correlations can be refined.}\n\\label{figure-illustration}\n\\end{figure}\n\nHow to introduce the long-term and short-term references is still an open problem for visual tracking. With the proposed AiA module, we present AiATrack, a streamlined Transformer framework for visual tracking. Unlike previous practices \\cite{zhang2019learning,fu2021stmtrack,wang2021transformer,yan2021learning}, which need an extra computational cost to process the selected reference frame during the model update, we directly reuse the cached features which are encoded before. An IoU prediction head is introduced for selecting high-quality short-term references. Moreover, we introduce learnable target-background embeddings to distinguish the target from the background while preserving the contextual information. With these designs, the proposed AiATrack can efficiently update short-term references and effectively exploit the long-term and short-term references for visual tracking.\n\nWe verify the effectiveness of our method by conducting comprehensive experiments on six prevailing benchmarks covering various kinds of tracking scenarios. Without bells and whistles, the proposed AiATrack sets new state-of-the-art results on these benchmarks with a real-time speed of 38 frames per second (fps).\n\nIn summary, the main contributions of our work are three-fold:\n\\begin{itemize}\n    \\item[$\\bullet$] We propose a novel attention in attention (AiA) module, which can mitigate noise and ambiguity in the conventional attention mechanism \\cite{vaswani2017attention} and improve tracking performance by a notable margin.\n    \\item[$\\bullet$] We present a neat Transformer tracking framework with the reuse of encoded features and the introduction of target-background embeddings to efficiently and effectively leverage temporal references.\n    \\item[$\\bullet$] We perform extensive experiments and analyses to validate the effectiveness of our designs. The proposed AiATrack achieves state-of-the-art performance on six widely used benchmarks.\n\\end{itemize}\n\n\\section{Related Work}\n\n\\subsection{Visual Tracking}\nRecently, Transformer \\cite{vaswani2017attention} has shown impressive performance in computer vision \\cite{carion2020end,zhu2020deformable,dosovitskiy2020image}. It aggregates information from sequential inputs to capture global context by an attention mechanism. Some efforts \\cite{yu2020deformable,guo2021graph,fu2021stmtrack} have been made to introduce the attention structure to visual tracking. Recently, several works \\cite{wang2021transformer,chen2021transformer,yu2021high,cao2021hift,yan2021learning,xing2022siamese} apply Transformer architecture to visual tracking. Despite their impressive performance, the potential of Transformer trackers is still limited by the conventional attention mechanism. To this end, we propose a novel attention module, namely, attention in attention (AiA), to further unveil the power of Transformer trackers.\n\nHow to adapt the model to the appearance change during tracking has also been investigated by previous works \\cite{danelljan2019atom,bhat2019learning,zhang2019learning,dai2020high,bhat2020know,fu2021stmtrack,wang2021transformer,yan2021learning}. A straightforward solution is to update the reference features by generation \\cite{zhang2019learning} or ensemble \\cite{fu2021stmtrack,wang2021transformer,yan2021learning}. However, most of these methods need to resize the reference frame and re-encode the reference features, which may sacrifice computational efficiency. Following discriminative correlation filter (DCF) method \\cite{henriques2014high}, another family of approaches \\cite{danelljan2019atom,bhat2019learning} optimize the network parameters during the inference. However, they need sophisticated optimization strategies with a sparse update to meet real-time requirements. In contrast, we present a new framework that can efficiently reuse the encoded features. Moreover, a target-background embedding assignment mechanism is also introduced. Different from \\cite{ge2021video,yang2021associating,lan2021siamese}, our target-background embeddings are directly introduced to distinguish the target and background regions and provide rich contextual cues.\n\n\\subsection{Attention Mechanism}\nRepresented by non-local operation \\cite{wang2018non} and Transformer attention \\cite{vaswani2017attention}, attention mechanism has rapidly received great popularity over the past few years. Recently, Transformer attention has been introduced to computer vision as a competitive architecture \\cite{carion2020end,zhu2020deformable,dosovitskiy2020image}. In vision tasks, it usually acts as a dynamic information aggregator in spatial and temporal domains. There are some works \\cite{huang2019attention,huang2019ccnet} that focus on solving existing issues in the conventional attention mechanism. Unlike these, in this paper, we try to address the noise and ambiguity issue in conventional attention mechanism by seeking consensus among correlations with a global receptive field.\n\n\\subsection{Correlation as Feature}\nTreating correlations as features has been explored by several previous works \\cite{shechtman2007matching,sattler2009scramsac,bian2017gms,rocco2018neighbourhood,li2020correspondence,min2021convolutional,rocco2020efficient,cho2021cats,bhat2020know}. In this paper, we use correlations to refer to the matching results of the pixels or regions. They can be obtained by squared difference, cosine similarity, inner product, \\etc. Several efforts have been made to recalibrate the raw correlations by processing them as features through hand-crafted algorithms \\cite{sattler2009scramsac,bian2017gms} or learnable blocks \\cite{rocco2018neighbourhood,min2021convolutional,li2020correspondence,bhat2020know,rocco2020efficient,cho2021cats}. To our best knowledge, we introduce this insight to the attention mechanism for the first time, making it a unified block for feature aggregation and information propagation in Transformer visual tracking.\n\n\\section{Method}\n\n\\subsection{Attention in Attention}\nTo present our attention in attention module, we first briefly revisit the conventional attention block in vision \\cite{dosovitskiy2020image,carion2020end}. As illustrated in Fig.~\\ref{figure-attention}(a), it takes a query and a set of key-value pairs as input and produces an output which is a weighted sum of the values. The weights assigned to the values are computed by taking the softmax of the scaled dot products between the query and the corresponding keys. Denote queries, keys and values by $\\mathbf{Q}$, $\\mathbf{K}$, $\\mathbf{V} \\in \\mathbb{R}^{HW \\times C}$ respectively. The conventional attention can be formulated as\n\\begin{equation}\n   \\text{ConvenAttn}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = (\\text{Softmax}\\left(\\frac{\\mathbf{\\bar{Q}}\\mathbf{\\bar{K}^{T}}}{\\sqrt{C}}\\right)\\mathbf{\\bar{V}})\\mathbf{W_{o}}\n\\end{equation}\nwhere $\\mathbf{\\bar{Q}} = \\mathbf{Q}\\mathbf{W_{q}}$, $\\mathbf{\\bar{K}} = \\mathbf{K}\\mathbf{W_{k}}$, $\\mathbf{\\bar{V}} = \\mathbf{V}\\mathbf{W_{v}}$ are different linear transformations. Here, $\\mathbf{W_{q}}$, $\\mathbf{W_{k}}$, $\\mathbf{W_{v}}$ and $\\mathbf{W_{o}}$ denote the linear transform weights for queries, keys, values, and outputs, respectively.\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=.98\\textwidth]{imgs/attention.pdf}\n\\caption{Structures of conventional attention and the proposed attention in attention (AiA) module. $\\bigotimes$ denotes matrix multiplication and $\\bigoplus$ denotes element-wise addition. The numbers beside arrows are feature dimensions which do not include the batch size. Matrix transpose operations are omitted for brevity.}\n\\label{figure-attention}\n\\end{figure}\n\nHowever, in the conventional attention block, the correlation of each query-key pair in the correlation map $\\mathbf{M} = \\frac{\\mathbf{\\bar{Q}}\\mathbf{\\bar{K}^{T}}}{\\sqrt{C}} \\in \\mathbb{R}^{HW \\times HW}$ is computed independently, which ignores the correlations of other query-key pairs. This correlation computation procedure may introduce erroneous correlations due to imperfect feature representations or the existence of distracting image patches in a background clutter scene. These erroneous correlations could result in noisy and ambiguous attentions as visualized in Fig.~\\ref{figure-visualization}. They may unfavorably affect the feature aggregation in self-attention and the information propagation in cross-attention, leading to sub-optimal performance for a Transformer tracker.\n\nTo address the aforementioned problem, we propose a novel attention in attention (AiA) module to improve the quality of the correlation map $\\mathbf{M}$. Usually, if a key has a high correlation with a query, some of its neighboring keys will also have relatively high correlations with that query. Otherwise, the correlation might be a noise. Motivated by this, we introduce the AiA module to utilize the informative cues among the correlations in $\\mathbf{M}$. The proposed AiA module seeks the correlation consistency around each key to enhance the appropriate correlations of relevant query-key pairs and suppress the erroneous correlations of irrelevant query-key pairs.\n\nSpecifically, we introduce another attention module to refine the correlation map $\\mathbf{M}$ before the softmax operation as illustrated in Fig.~\\ref{figure-attention}(b). As the newly introduced attention module is inserted into the conventional attention block, we call it an inner attention module, forming an attention in attention structure. The inner attention module itself is a variant of the conventional attention. We consider columns in $\\mathbf{M}$ as a sequence of correlation vectors which are taken as queries $\\mathbf{Q'}$, keys $\\mathbf{K'}$ and values $\\mathbf{V'}$ by the inner attention module to output a residual correlation map.\n\nGiven the input $\\mathbf{Q'}$, $\\mathbf{K'}$ and $\\mathbf{V'}$, we first generate transformed queries $\\mathbf{\\bar{Q}'}$ and keys $\\mathbf{\\bar{K}'}$ as illustrated in the right block of Fig.~\\ref{figure-attention}(b). To be specific, a linear transformation is first applied to reduce the dimensions of $\\mathbf{Q'}$ and $\\mathbf{K'}$ to $HW \\times D$ ($D \\ll HW$) for computational efficiency. After normalization \\cite{ba2016layer}, we add 2-dimensional sinusoidal encoding \\cite{dosovitskiy2020image,carion2020end} to provide positional cues. Then, $\\mathbf{\\bar{Q}'}$ and $\\mathbf{\\bar{K}'}$ are generated by two different linear transformations. We also normalize $\\mathbf{V'}$ to generate the normalized correlation vectors $\\mathbf{\\bar{V}'}$, \\ie $\\mathbf{\\bar{V}'} = \\text{LayerNorm}(\\mathbf{V'})$. With $\\mathbf{\\bar{Q}'}$, $\\mathbf{\\bar{K}'}$ and $\\mathbf{\\bar{V}'}$, the inner attention module generates a residual correlation map by\n\\begin{equation}\n    \\text{InnerAttn}(\\mathbf{M}) = (\\text{Softmax}\\left(\\frac{\\mathbf{\\bar{Q}'}\\mathbf{\\bar{K}'^{T}}}{\\sqrt{D}}\\right)\\mathbf{\\bar{V}'})(1 + \\mathbf{W_{o}'})\n\\label{equation-aia}\n\\end{equation}\nwhere $\\mathbf{W_{o}'}$ denotes linear transform weights for adjusting the aggregated correlations together with an identical connection.\n\nEssentially, for each correlation vector in the correlation map $\\mathbf{M}$, the AiA module generates its residual correlation vector by aggregating the raw correlation vectors. It can be seen as seeking consensus among the correlations with a global receptive field. With the residual correlation map, our attention block with AiA module can be formulated as\n\\begin{equation}\n    \\text{AttninAttn}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = (\\text{Softmax}(\\mathbf{M} + \\text{InnerAttn}(\\mathbf{M}))\\mathbf{\\bar{V}})\\mathbf{W_{o}}\n\\end{equation}\n\nFor a multi-head attention block, we share the parameters of the AiA module between the parallel attention heads. It is worth noting that our AiA module can be readily inserted into both self-attenion and cross-attention blocks in a Transformer tracking framework.\n\n\\subsection{Proposed Framework}\\label{section-network}\nWith the proposed AiA module, we design a simple yet effective Transformer framework for visual tracking, dubbed AiATrack. Our tracker is comprised of a network backbone, a Transformer architecture, and two prediction heads as illustrated in Fig.~\\ref{figure-framework}. Given the search frame, the initial frame is taken as a long-term reference and an ensemble of several intermediate frames are taken as short-term references. The features of the long-term and short-term references and the search frame are extracted by the network backbone and then reinforced by the Transformer encoder. We also introduce learnable target-background embeddings to distinguish the target from background regions. The Transformer decoder propagates the reference features as well as the target-background embedding maps to the search frame. The output of the Transformer is then fed to a target prediction head and an IoU prediction head for target localization and short-term reference update, respectively.\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=.98\\textwidth]{imgs/framework.pdf}\n\\caption{Overview of the proposed Transformer tracking framework. The self-attention and cross-attention blocks are all equipped with the proposed AiA module. Note that only the components on the light green background need to be computed during the inference phase as described in Sec.~\\ref{section-tracking}.}\n\\label{figure-framework}\n\\end{figure}\n\n\\textbf{Transformer Architecture.} The Transformer encoder is adopted to reinforce the features extracted by the convolutional backbone. For the search frame, we flatten its features to obtain a sequence of feature vectors and add sinusoidal positional encoding as in \\cite{carion2020end}. The sequence of feature vectors is then taken by the Transformer encoder as its input. The Transformer encoder consists of several layer stacks, each of which is made up of a multi-head self-attention block and a feed-forward network. The self-attention block serves to capture the dependencies among all feature vectors to enhance the original features, and is equipped with the proposed AiA module. Similarly, this procedure is applied independently to the features of the reference frames using the same encoder.\n\nThe Transformer decoder propagates the reference information from the long-term and short-term references to the search frame. Different from the classical Transformer decoder \\cite{vaswani2017attention}, we remove the self-attention block for simplicity and introduce a two-branch cross-attention design as shown in Fig.~\\ref{figure-framework} to retrieve the target-background information from long-term and short-term references. The long-term branch is responsible for retrieving reference information from the initial frame. Since the initial frame has the most reliable annotation of the tracking target, it is crucial for robust visual tracking. However, as the appearance of the target and the background change through the video, the reference information from the long-term branch may not be up-to-date. This could cause tracker drift in some scenes. To address this problem, we introduce the short-term branch to utilize the information from the frames that are closer to the current frame. The cross-attention blocks of the two branches have the identical structure following the query-key-value design in the vanilla transformer \\cite{vaswani2017attention}. We take the features of the search frame as queries and the features of the reference frames as keys. The values are generated by combining the reference features with target-background embedding maps, which will be described below. We also insert our AiA module into cross-attention for better reference information propagation.\n\n\\textbf{Target-Background Embeddings.} To indicate the target and background regions while preserving the contextual information, we introduce a target embedding $\\mathcal{E} ^ {tgt} \\in \\mathbb{R} ^ {C}$ and a background embedding $\\mathcal{E} ^ {bg} \\in \\mathbb{R} ^ {C}$, both of which are learnable. With $\\mathcal{E} ^ {tgt}$ and $\\mathcal{E} ^ {bg}$, we generate target-background embedding maps $\\mathcal{E} \\in \\mathbb{R} ^ {HW \\times C}$ for the reference frames with a negligible computational cost. Let's consider a location $p$ in a $H \\times W$ grid, the embedding assignment is formulated as\n\\begin{equation}\n    \\mathcal{E}(p) = \\begin{cases} \\mathcal{E} ^ {tgt} & \\text{if $p$ falls in the target region} \\\\ \\mathcal{E} ^ {bg} & \\text{otherwise} \\end{cases}\n\\label{equation-embed}\n\\end{equation}\n\nAfterward, we attach the target-background embedding maps to the reference features and feed them to cross-attention blocks as values. The target-background embedding maps enrich the reused appearance features by providing contextual cues.\n\n\\textbf{Prediction Heads.} As described above, our tracker has two prediction heads. The target prediction head is adopted from \\cite{yan2021learning}. Specifically, the decoded features are fed into a two-branch fully-convolutional network which outputs two probability maps for the top-left and the bottom-right corners of the target bounding box. The predicted box coordinates are then obtained by computing the expectations of the probability distributions of the two corners.\n\nTo adapt the model to the appearance change during tracking, the tracker needs to keep the short-term references up-to-date by selecting reliable references which contain the target. Moreover, considering our embedding assignment mechanism in Eq.~\\ref{equation-embed}, the bounding box of the selected reference frame should be as accurate as possible. Inspired by IoU-Net \\cite{jiang2018acquisition} and ATOM \\cite{danelljan2019atom}, for each predicted bounding box, we estimate its IoU with the ground truth via an IoU prediction head. The features inside the predicted bounding box are passed to a Precise RoI Pooling layer whose output is taken by a fully connected network to produce an IoU prediction. The predicted IoU is then used to determine whether to include the search frame as a new short-term reference.\n\nWe train the two prediction heads jointly. The loss of target prediction is defined by the combination of GIoU loss \\cite{rezatofighi2019generalized} and L1 loss between the predicted bounding box and the ground truth. The training examples of the IoU prediction head are generated by sampling bounding boxes around the ground truths. The loss of IoU prediction is defined by mean squared error. We refer readers to the supplementary material for more details about training.\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=.98\\textwidth]{imgs/visualization.pdf}\n\\caption{Visualization of the effect of the proposed AiA module. We visualize several representative correlation vectors before and after the refinement by the AiA module. The visualized correlation vectors are reshaped according to the spatial positions of queries. We select the correlation vectors of keys corresponding to the target object regions in the first column. It can be observed that the erroneous correlations are effectively suppressed and the appropriate ones are enhanced with the AiA module.}\n\\label{figure-visualization}\n\\end{figure}\n\n\\subsection{Tracking with AiATrack}\\label{section-tracking}\nGiven the initial frame with ground truth annotation, we initialize the tracker by cropping the initial frame as long-term and short-term references and pre-computing their features and target-background embedding maps. For each subsequent frame, we estimate the IoU score of the bounding box predicted by target prediction head for model update. The update procedure is more efficient than the previous practices \\cite{fu2021stmtrack,wang2021transformer,yan2021learning}, as we directly reuse the encoded features. Specifically, if the estimated IoU score of the predicted bounding box is higher than the pre-defined threshold, we generate the target-background embedding map for the current search frame and store the embedding map in a memory cache together with its encoded features. For each new-coming frame, we uniformly sample several short-term reference frames and concatenate their features and embedding maps from the memory cache to update the short-term reference ensemble. The latest reference frame in the memory cache is always sampled as it is closest to the current search frame. The oldest reference frame in the memory cache will be popped out if the maximum cache size is reached.\n\n\\section{Experiments}\n\n\\subsection{Implementation Details}\n\nOur experiments are conducted with NVIDIA GeForce RTX 2080 Ti. We adopt ResNet-50 \\cite{he2016deep} as network backbone which is initialized by the parameters pre-trained on ImageNet-1k \\cite{deng2009imagenet}. We crop a search patch which is $5 ^ {2}$ times of the target box area from the search frame and resize it to a resolution of $320 \\times 320$ pixels. The same cropping procedure is also applied to the reference frames. The cropped patches are then down-sampled by the network backbone with a stride of 16. The Transformer encoder consists of 3 layer stacks and the Transformer decoder consists of only 1 layer. The multi-head attention blocks in our tracker have 4 heads with channel width of 256. The inner AiA module reduces the channel dimension of queries and keys to 64. The FFN blocks have 1024 hidden units. Each branch of the target prediction head is comprised of 5 Conv-BN-ReLU layers. The IoU prediction head consists of 3 Conv-BN-ReLU layers, a PrPool \\cite{jiang2018acquisition} layer with pooling size of $3 \\times 3$ and 2 fully connected layers.\n\n\\begin{table}[t]\n\\centering\n\\begin{tabular}{c|c|ccc|ccc|ccc}\n\\hline\n\\multirow{2}{*}{Tracker}\n& \\multirow{2}{*}{Source}\n& \\multicolumn{3}{c|}{LaSOT \\cite{fan2019lasot}}\n& \\multicolumn{3}{c|}{TrackingNet \\cite{muller2018trackingnet}}\n& \\multicolumn{3}{c}{GOT-10k \\cite{huang2019got}} \\\\\n& & AUC & P$_{\\text{Norm}}$ & \\multicolumn{1}{c|}{P}\n& AUC & P$_{\\text{Norm}}$ & \\multicolumn{1}{c|}{P}\n& AO & SR$_{0.75}$ & SR$_{0.5}$ \\\\\n\\hline\nAiATrack & Ours & \n\\textcolor{red}{69.0} & \\textcolor{red}{79.4} & \\textcolor{red}{73.8} & \n\\textcolor{red}{82.7} & \\textcolor{red}{87.8} & \\textcolor{red}{80.4} & \n\\textcolor{red}{69.6} & \\textcolor{red}{63.2} & \\textcolor{red}{80.0} \\\\\nSTARK-ST50 \\cite{yan2021learning} & ICCV2021 & \n66.4 & 76.3 & \\textcolor{blue}{71.2} & 81.3 & 86.1 & 78.1 & \\textcolor{blue}{68.0} & \\textcolor{blue}{62.3} & \\textcolor{blue}{77.7} \\\\\nKeepTrack \\cite{mayer2021learning} & ICCV2021 & \n\\textcolor{blue}{67.1} & \\textcolor{blue}{77.2} & 70.2 & - & - & - & - & - & - \\\\\nDTT \\cite{yu2021high} & ICCV2021 & \n60.1 & - & - & 79.6 & 85.0 & 78.9 & 63.4 & 51.4 & 74.9 \\\\\nTransT \\cite{chen2021transformer} & CVPR2021 & \n64.9 & 73.8 & 69.0 & \\textcolor{blue}{81.4} & \\textcolor{blue}{86.7} & \\textcolor{blue}{80.3} & 67.1 & 60.9 & 76.8 \\\\\nTrDiMP \\cite{wang2021transformer} & CVPR2021 & \n63.9 & - & 61.4 & 78.4 & 83.3 & 73.1 & 67.1 & 58.3 & \\textcolor{blue}{77.7} \\\\\nTrSiam \\cite{wang2021transformer} & CVPR2021 & \n62.4 & - & 60.0 & 78.1 & 82.9 & 72.7 & 66.0 & 57.1 & 76.6 \\\\\nKYS \\cite{bhat2020know} & ECCV2020 & \n55.4 & 63.3 & - & 74.0 & 80.0 & 68.8 & 63.6 & 51.5 & 75.1 \\\\\nOcean-online \\cite{zhang2020ocean} & ECCV2020 & \n56.0 & 65.1 & 56.6 & - & - & - & 61.1 & 47.3 & 72.1 \\\\\nOcean-offline \\cite{zhang2020ocean} & ECCV2020 & \n52.6 & - & 52.6 & - & - & - & 59.2 & - & 69.5 \\\\\nPrDiMP50 \\cite{danelljan2020probabilistic} & CVPR2020 & \n59.8 & 68.8 & 60.8 & 75.8 & 81.6 & 70.4 & 63.4 & 54.3 & 73.8 \\\\\nSiamAttn \\cite{yu2020deformable} & CVPR2020 & \n56.0 & 64.8 & - & 75.2 & 81.7 & - & - & - & - \\\\\nDiMP50 \\cite{bhat2019learning} & ICCV2019 & \n56.9 & 65.0 & 56.7 & 74.0 & 80.1 & 68.7 & 61.1 & 49.2 & 71.7 \\\\\nSiamRPN++ \\cite{li2019siamrpn++} & CVPR2019 & \n49.6 & 56.9 & 49.1 & 73.3 & 80.0 & 69.4 & 51.7 & 32.5 & 61.6 \\\\\n\\hline\n\\end{tabular}\n\\caption{State-of-the-art comparison on LaSOT, TrackingNet, and GOT-10k. The best two results are shown in \\textcolor{red}{red} and \\textcolor{blue}{blue}, respectively. All the trackers listed above adopt ResNet-50 pre-trained on ImageNet-1k as network backbone and the results on GOT-10k are obtained without additional training data for fair comparison.}\n\\label{table-large}\n\\end{table}\n\n\\begin{table}[t]\n\\centering\n\\begin{tabular}{c|cccccc}\n\\hline\n\\multirow{2}{*}{Tracker}\n& SiamRPN++ & PrDiMP50 & TransT & STARK-ST50 & KeepTrack & AiATrack \\\\\n& \\cite{li2019siamrpn++} & \\cite{danelljan2020probabilistic} & \\cite{chen2021transformer} & \\cite{yan2021learning} & \\cite{mayer2021learning} & (Ours) \\\\\n\\hline\nNfS30 \\cite{kiani2017need} & 50.2 & 63.5 & 65.7 & 65.2 & \\textcolor{blue}{66.4} & \\textcolor{red}{67.9} \\\\\nOTB100 \\cite{wu2015object} & \\textcolor{blue}{69.6} & \\textcolor{blue}{69.6} & 69.4 & 68.5 & \\textcolor{red}{70.9} & \\textcolor{blue}{69.6} \\\\\nUAV123 \\cite{mueller2016benchmark} & 61.3 & 68.0 & 69.1 & 69.1 & \\textcolor{blue}{69.7} & \\textcolor{red}{70.6} \\\\\n\\hline\nSpeed (fps) & 35 & 30 & 50 & 42 & 18 & 38 \\\\\n\\hline\n\\end{tabular}\n\\caption{State-of-the-art comparison on commonly used small-scale datasets in terms of AUC score. The best two results are shown in \\textcolor{red}{red} and \\textcolor{blue}{blue}.}\n\\label{table-small}\n\\end{table}\n\n\\subsection{Results and Comparisons}\\label{section-comparison}\nWe compare our tracker with several state-of-the-art trackers on three prevailing large-scale benchmarks (LaSOT \\cite{fan2019lasot}, TrackingNet and \\cite{muller2018trackingnet} and GOT-10k \\cite{huang2019got}) and three commonly used small-scale datasets (NfS30 \\cite{kiani2017need}, OTB100 \\cite{wu2015object} and UAV123 \\cite{mueller2016benchmark}). The results are summarized in Tab.~\\ref{table-large} and Tab.~\\ref{table-small}.\n\n\\noindent\\textbf{LaSOT.} LaSOT \\cite{fan2019lasot} is a densely annotated large-scale dataset, containing 1400 long-term video sequences. As shown in Tab.~\\ref{table-large}, our approach outperforms the previous best tracker KeepTrack \\cite{mayer2021learning} by 1.9\\% in area-under-the-curve (AUC) and 3.6\\% in precision while running much faster (see Tab.~\\ref{table-small}). We also provide an attribute-based evaluation in Fig.~\\ref{figure-attribute} for further analysis. Our method achieves the best performance on all attribute splits. The results demonstrate the promising potential of our approach for long-term visual tracking.\n\n\\noindent\\textbf{TrackingNet.} TrackingNet \\cite{muller2018trackingnet} is a large-scale short-term tracking benchmark. It provides 511 testing video sequences without publicly available ground truths. Our performance reported in Tab.~\\ref{table-large} is obtained from the online evaluation server. Our approach achieve 82.7\\% in AUC score and 87.8\\% in normalized precision score, surpassing all previously published trackers. It demonstrates that our approach is also very competitive for short-term tracking scenarios.\n\n\\noindent\\textbf{GOT-10k.} To ensure zero overlaps of object classes between training and testing, we follow the one-shot protocol of GOT-10k \\cite{huang2019got} and only train our model with the specified subset. The testing ground truths are also withheld and our result is evaluated by the official server. As demonstrated in Tab.~\\ref{table-large}, our tracker improves all metrics by a large margin, \\eg 2.3\\% in success rate compared with STARK \\cite{yan2021learning} and TrDiMP \\cite{wang2021transformer}, which indicates that our tracker also has a good generalization ability to the objects of unseen classes.\n\n\\begin{figure}[t]\n\\floatbox[{\\capbeside\\thisfloatsetup{capbesideposition={left,top},capbesidewidth=.515\\textwidth}}]{figure}[\\FBwidth]\n{\\caption{Attribute-based evaluation on LaSOT in terms of AUC score. Our tracker achieves the best performance on all attribute splits while making a significant improvement in various kinds of scenarios such as background clutter, camera motion, and deformation. Axes of each attribute have been normalized.}\n\\label{figure-attribute}}\n{\\includegraphics[width=.445\\textwidth]{imgs/attribute.png}}\n\\end{figure}\n\n\\noindent\\textbf{NfS30.} Need for Speed (NfS) \\cite{kiani2017need} is a dataset that contains 100 videos with fast-moving objects. We evaluate the proposed tracker on its commonly used version NfS30. As reported in Tab.~\\ref{table-small}, our tracker improves the AUC score by 2.7\\% over STARK \\cite{yan2021learning} and performs the best among the benchmarked trackers.\n\n\\noindent\\textbf{OTB100.} Object Tracking Benchmark (OTB) \\cite{wu2015object} is a pioneering benchmark for evaluating visual tracking algorithms. However, in recent years, it has been noted that this benchmark has become highly saturated \\cite{wang2021transformer,yan2021learning,mayer2021learning}. Still, the results in Tab.~\\ref{table-small} show that our method can achieve comparable performance with state-of-the-art trackers.\n\n\\noindent\\textbf{UAV123.} Finally, we report our results on UAV123 \\cite{mueller2016benchmark} which includes 123 video sequences captured from a low-altitude unmanned aerial vehicle perspective. As shown in Tab.~\\ref{table-small}, our tracker outperforms KeepTrack \\cite{mayer2021learning} by 0.9\\% and is suitable for UAV tracking scenarios.\n\n\\subsection{Ablation Studies}\\label{section-ablation}\nTo validate the importance of the proposed components in our tracker, we conduct ablation studies on LaSOT testing set and its new extension set \\cite{fan2021lasot}, totaling 430 diverse videos. We summarize the results in Tab.~\\ref{table-ablation}, Tab.~\\ref{table-substitute} and Tab.~\\ref{table-sensitivity}.\n\n\\noindent\\textbf{Target-Background Embeddings.} In our tracking framework, the reference frames not only contain features from target regions but also include a large proportion of features from background regions. We implement three variants of our method to demonstrate the necessity of keeping the context and the importance of the proposed target-background embeddings. As shown in the 1st part of Tab.~\\ref{table-ablation}, we start from the variant~(a), which is the implementation of the proposed tracking framework with both the target-background embeddings and the AiA module removed. Based on the variant~(a), the variant~(b) further discards the reference features of background regions with a mask. The variant~(c) attaches the target-background embeddings to the reference features. Compared with the variant~(a), the performance of the variant~(b) drops drastically, which suggests that context is helpful for visual tracking. With the proposed target-background embeddings, the variant~(c) can consistently improve the performance over the variant~(a) in all metrics. This is because the proposed target-background embeddings further provide cues for distinguishing the target and background regions while preserving the contextual information.\n\n\\noindent\\textbf{Long-Term and Short-Term Branch.} As discussed in Sec.~\\ref{section-network}, it is important to utilize an independent short-term reference branch to deal with the appearance change during tracking. To validate this, we implement a variant~(d) by removing the short-term branch from the variant~(c). We also implement a variant~(e) by adopting a single cross-attention branch instead of the proposed two-branch design for the variant~(c). Note that we keep the IoU prediction head for these two variants during training to eliminate the possible effect of IoU prediction on feature representation learning. From the 2nd part of Tab.~\\ref{table-ablation}, we can observe that the performance of variant~(d) is worse than variant~(c), which suggests the necessity of using short-term references. Meanwhile, compared with variant~(c), the performance of variant~(e) also drops, which validates the necessity to use two separate branches for the long-term and short-term references. This is because the relatively unreliable short-term references may disturb the robust long-term reference and therefore degrade its contribution.\n\n\\begin{table}[t]\n\\centering\n\\begin{tabular}{ccc|ccc|ccc}\n\\hline\n\\multicolumn{3}{c|}{\\multirow{2}{*}{Modification}}\n& \\multicolumn{3}{c|}{LaSOT \\cite{fan2019lasot}}\n& \\multicolumn{3}{c}{LaSOT$_{\\text{Ext}}$ \\cite{fan2021lasot}} \\\\\n& & & AUC & P$_{\\text{Norm}}$ & \\multicolumn{1}{c|}{P}\n& AUC & P$_{\\text{Norm}}$ & P \\\\\n\\hline\n\\multicolumn{1}{c|}{\\multirow{3}{*}{1st}} & (a) & none & \n65.8 & 75.8 & 69.5 & 44.5 & 51.5 & 50.5 \\\\\n\\multicolumn{1}{c|}{} & (b) & mask & \n64.3 & 72.7 & 66.6 & 42.8 & 50.1 & 48.8 \\\\\n\\multicolumn{1}{c|}{} & (c) & embed$^\\dagger$ & \n\\textbf{67.0} & \\textbf{77.0} & \\textbf{71.3} & \\textbf{44.7} & \\textbf{52.7} & \\textbf{51.5} \\\\\n\\hline\n\\multicolumn{1}{c|}{\\multirow{3}{*}{2nd}} & (d) & w/o short refer & \n66.5 & 76.3 & 70.7 & 44.5 & 51.8 & 50.6 \\\\\n\\multicolumn{1}{c|}{} & (e) & w/o branch split & \n63.8 & 72.9 & 66.7 & 42.7 & 50.3 & 48.6 \\\\\n\\multicolumn{1}{c|}{} & (c) & w/ both$^\\dagger$ & \n\\textbf{67.0} & \\textbf{77.0} & \\textbf{71.3} & \\textbf{44.7} & \\textbf{52.7} & \\textbf{51.5} \\\\\n\\hline\n\\multicolumn{1}{c|}{\\multirow{6}{*}{3rd}} & (c) & w/o AiA$^\\dagger$ & \n67.0 & 77.0 & 71.3 & 44.7 & 52.7 & 51.5 \\\\\n\\multicolumn{1}{c|}{} & (f) & AiA in self-attn & \n68.6 & 78.7 & 72.9 & 46.2 & \\textbf{54.4} & 53.4 \\\\\n\\multicolumn{1}{c|}{} & (g) & AiA in cross-attn & \n67.5 & 77.9 & 71.8 & 46.2 & 54.2 & 53.3 \\\\\n\\multicolumn{1}{c|}{} & (h) & w/o pos in both & \n68.0 & 78.2 & 72.7 & 46.2 & 54.0 & 53.0 \\\\\n\\multicolumn{1}{c|}{} & (i) & AiA in both$^\\ddagger$ & \n\\textbf{68.7} & \\textbf{79.3} & \\textbf{73.7} & \\textbf{46.8} & \\textbf{54.4} & \\textbf{54.2} \\\\\n\\hline\n\\end{tabular}\n\\caption{Ablative experiments about different components in the proposed tracker. We use $\\dagger$ to denote the basic framework and $\\ddagger$ to denote our final model with AiA. The best results in each part of the table are marked in \\textbf{bold}.}\n\\label{table-ablation}\n\\end{table}\n\n\\noindent\\textbf{Effectiveness of the AiA Module.} We explore several ways of applying the proposed AiA module to the proposed Transformer tracking framework. The variant~(f) inserts the AiA module into self-attention blocks in the Transformer encoder. Compared with the variant~(c), the performance can be greatly improved on the two subsets of LaSOT. The variant~(g) inserts the AiA module into the cross-attention blocks in the Transformer decoder, which also brings a consistent improvement. These two variants demonstrate that the AiA module generalizes well to both self-attention blocks and cross-attention blocks. When we apply the AiA module to both self-attention blocks and cross-attention blocks, \\ie the final model (i), the performance on the two subsets of LaSOT can be improved by 1.7$\\sim$2.7\\% in all metrics compared with the basic framework (c).\n\nRecall that we introduce positional encoding to the proposed AiA module (see Fig.~\\ref{figure-attention}). To verify its importance, we implement a variant~(h) by removing positional encoding from the variant~(i). We can observe that the performance drops accordingly. This validates the necessity of positional encoding, as it provides spatial cues for consensus seeking in the AiA module. More analysis about the components of the AiA module are provided in the supplementary material.\n\n\\noindent\\textbf{Superiority of the AiA Module.} One may concern that the performance gain of the AiA module is brought by purely adding extra parameters. Thus, we design two other variants to demonstrate the superiority of the proposed module.\n\nFirst, we implement a variant of our basic framework where each Attention-Add-Norm block is replaced by two cascaded ones. From the comparison of the first two rows in Tab.~\\ref{table-substitute}, we can observe that simply increasing the number of attention blocks in our tracking framework does not help much, which demonstrates that our AiA module can further unveil the potential of the tracker.\n\nWe also implement a variant of our final model by replacing the proposed inner attention with a convolutional bottleneck \\cite{he2016deep}, which is designed to have a similar computational cost. From the comparison of the last two rows in Tab.~\\ref{table-substitute}, we can observe that inserting a convolutional bottleneck can also bring positive effects, which suggests the necessity of correlation refinement. However, the convolutional bottleneck can only perform a fixed aggregation in each local neighborhood, while our AiA module has a global receptive field with dynamic weights determined by the interaction among correlation vectors. As a result, our AiA module can seek consensus more flexibly and further boost the performance.\n\n\\begin{table}[t]\n\\centering\n\\begin{tabular}{c|c|ccc|ccc|c}\n\\hline\n\\multirow{2}{*}{Modification}\n& Correlation\n& \\multicolumn{3}{c|}{LaSOT \\cite{fan2019lasot}}\n& \\multicolumn{3}{c|}{LaSOT$_{\\text{Ext}}$ \\cite{fan2021lasot}}\n& Speed \\\\\n& Refinement\n& AUC & P$_{\\text{Norm}}$ & \\multicolumn{1}{c|}{P}\n& AUC & P$_{\\text{Norm}}$ & P\n& (fps) \\\\\n\\hline\nw/o AiA$^\\dagger$ & \\multirow{2}{*}{\\ding{55}} &\n67.0 & 77.0 & 71.3 & 44.7 & 52.7 & 51.5 & 44 \\\\\nw/o AiA cascade & &\n67.1 & 77.0 & 71.7 & 44.6 & 52.9 & 51.6 & 40 \\\\\n\\hline\nconv in both & \\multirow{2}{*}{\\ding{51}} &\n67.9 & 78.2 & 72.8 & 46.0 & 53.4 & 52.8 & 39 \\\\\nAiA in both$^\\ddagger$ & &\n\\textbf{68.7} & \\textbf{79.3} & \\textbf{73.7} & \\textbf{46.8} & \\textbf{54.4} & \\textbf{54.2} & 38 \\\\\n\\hline\n\\end{tabular}\n\\caption{Superiority comparison with the tracking performance and the running speed.}\n\\label{table-substitute}\n\\end{table}\n\n\\begin{table}[t]\n\\centering\n\\begin{tabular}{c|ccccccc}\n\\hline\nEnsemble Size & 1 & 2 & 3 & 4 & 5 & 6 & 10 \\\\\n\\hline\nLaSOT \\cite{fan2019lasot} & 66.8 & 68.1 & \\colorbox{lightgray}{68.7} & \\textbf{69.0} & 68.2 & 68.6 & 68.9 \\\\\nLaSOT$_{\\text{Ext}}$ \\cite{fan2021lasot} & 44.9 & 46.3 & \\colorbox{lightgray}{46.8} & 46.2 & 47.4 & \\textbf{47.7} & 47.1 \\\\\n\\hline\nSpeed (fps) & 39 & 39 & 38 & 38 & 38 & 38 & 34 \\\\\n\\hline\n\\end{tabular}\n\\caption{Impact of ensemble size in terms of AUC score and the running speed. All of our ablative experiments are conducted with ensemble size as 3 by default.}\n\\label{table-sensitivity}\n\\end{table}\n\n\\noindent\\textbf{Visualization Perspective.} In Fig.~\\ref{figure-visualization}, we visualize correlation maps from the perspective of keys. This is because we consider the correlations of one key with queries as a correlation vector. Thus, the AiA module performs refinement by seeking consensus among the correlation vectors of keys. Actually, refining the correlations from the perspective of queries also works well, achieving 68.5\\% in AUC score on LaSOT.\n\n\\noindent\\textbf{Short-Term Reference Ensemble.} We also study the impact of the ensemble size in the short-term branch. Tab.~\\ref{table-sensitivity} shows that by increasing the ensemble size from 1 to 3, the performance can be stably improved. Further increasing the ensemble size does not help much and has little impact on the running speed.\n\n\\section{Conclusion}\nIn this paper, we present an attention in attention (AiA) module to improve the attention mechanism for Transformer visual tracking. The proposed AiA module can effectively enhance appropriate correlations and suppress erroneous ones by seeking consensus among all correlation vectors. Moreover, we present a streamlined Transformer tracking framework, dubbed AiATrack, by introducing efficient feature reuse and embedding assignment mechanisms to fully utilize temporal references. Extensive experiments demonstrate the superiority of the proposed method. We believe that the proposed AiA module could also be beneficial in other related tasks where the Transformer architecture can be applied to perform feature aggregation and information propagation, such as video object segmentation \\cite{yang2021associating,lan2021siamese,duke2021sstvos,mao2021joint}, video object detection \\cite{he2021end} and multi-object tracking \\cite{sun2020transtrack,meinhardt2021trackformer,zhang2021bytetrack}. \\\\\n\n\\noindent\\textbf{Acknowledgment.} This work is supported in part by National Key R\\&D Program of China No. 2021YFC3340802, National Science Foundation Grant CNS1951952 and National Natural Science Foundation of China Grant 61906119.\n\n\\clearpage\n\n\\begin{center}\n\\textbf{\\large AiATrack: Attention in Attention for Transformer Visual Tracking (Supplementary Material)}\n\\end{center}\n\nThe supplementary material provides additional details about the experiments and analyses of the proposed method.\n\n\\section{Additional Experiment Details}\n\n\\subsection{Target Prediction}\nTo make the tracking procedure in an end-to-end manner without tedious post-processing, we adopt the anchor-free prediction head proposed in \\cite{yan2021learning}, which outputs the probability maps $P_{tl}(x,y)$ and $P_{br}(x,y)$ for the top-left and the bottom-right bounding box corners. The coordinates $\\widehat{x}_{tl}$, $\\widehat{y}_{tl}$, $\\widehat{x}_{br}$, $\\widehat{y}_{br}$ of the predicted bounding box are then obtained by \n\\begin{gather}\n    \\widehat{x}_{tl} = \\sum_{y = 0}^{H}\\sum_{x = 0}^{W}x \\cdot P_{tl}(x,y) \\text{, } \\widehat{y}_{tl} = \\sum_{y = 0}^{H}\\sum_{x = 0}^{W}y \\cdot P_{tl}(x,y) \\\\\n    \\widehat{x}_{br} = \\sum_{y = 0}^{H}\\sum_{x = 0}^{W}x \\cdot P_{br}(x,y) \\text{, } \\widehat{y}_{br} = \\sum_{y = 0}^{H}\\sum_{x = 0}^{W}y \\cdot P_{br}(x,y)\n\\end{gather}\n\n\\subsection{Training Objective}\nWith the predicted bounding box $\\widehat{b}$ and predicted IoU $\\widehat{i}$, the whole network is jointly trained by minimizing prediction errors. The bounding box prediction loss is defined as the combination of GIoU loss \\cite{rezatofighi2019generalized} and L1 loss. Together with the IoU prediction loss, the loss function can be written as \n\\begin{equation}\n    L = \\lambda_{giou}L_{giou}(b, \\widehat{b}) + \\lambda_{l1}\\Vert b - \\widehat{b}\\Vert_{1} + \\lambda_{mse}(i - \\widehat{i})^{2}\n\\end{equation}\nwhere $b$ and $i$ represent the ground truths of bounding box and IoU respectively and $\\lambda_{giou}$, $\\lambda_{l1}$, $\\lambda_{mse}$ are the trade-off weights.\n\n\\subsection{Training Strategy}\nSimilar to previous works \\cite{danelljan2019atom,bhat2019learning,chen2021transformer,wang2021transformer,yan2021learning}, we utilize the training splits of LaSOT \\cite{fan2019lasot}, TrackingNet \\cite{muller2018trackingnet}, GOT-10k \\cite{huang2019got}, and COCO \\cite{lin2014microsoft} for offline training. As for the COCO image dataset, we apply data augmentation to generate synthetic video clips of diverse classes. During training, we randomly sample the search frame and reference frames such that the index of the search frame is larger than the indexes of reference frames. For training efficiency, we only sample one frame as the short-term reference. We also apply random affine transformations to jitter the sizes and locations of the short-term reference frame and search frame to simulate real tracking scenarios and avoid the influence of absolute position bias caused by padding \\cite{islam2020much,li2019siamrpn++,zhang2019deeper}. The network is trained with the AdamW optimizer \\cite{loshchilov2017decoupled}. The learning rate is 1e-5 for the network backbone and 1e-4 for the other components. It decays by a factor of 10 during training. The parameters of the first convolutional layer and the first stage in the ResNet-50 \\cite{he2016deep} backbone are fixed during training.\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=.98\\textwidth]{imgs/structure.pdf}\n\\caption{Detailed illustration of the differences between different structures of the AiA module. $\\bigotimes$ denotes matrix multiplication and $\\bigoplus$ denotes element-wise addition. The numbers beside arrows are feature dimensions which do not include the batch size. Matrix transpose operations are omitted for brevity.}\n\\label{figure-structure}\n\\end{figure}\n\n\\begin{table}[t]\n\\centering\n\\begin{tabular}{c|ccc|ccc|ccc}\n\\hline\n\\multirow{2}{*}{Structure}\n& \\multicolumn{3}{c|}{Modification}\n& \\multicolumn{3}{c|}{LaSOT \\cite{fan2019lasot}}\n& \\multicolumn{3}{c}{LaSOT$_{\\text{Ext}}$ \\cite{fan2021lasot}} \\\\\n& LN & LT & IC\n& AUC & P$_{\\text{Norm}}$ & \\multicolumn{1}{c|}{P}\n& AUC & P$_{\\text{Norm}}$ & P \\\\\n\\hline\nAiAv1 & \\ding{51} & & \\ding{51} & 68.7 & 79.3 & 73.7 & 46.8 & 54.4 & 54.2 \\\\\nAiAv2 & & & & 68.8 & 79.3 & 73.6 & 46.7 & 54.5 & 53.8 \\\\\nAiAv3 & \\ding{51} & \\ding{51} & & \\textbf{69.2} & \\textbf{79.6} & \\textbf{74.3} & \\textbf{48.4} & \\textbf{56.6} & \\textbf{56.2} \\\\\n\\hline\n\\end{tabular}\n\\caption{Study about the different structures of the AiA module. \\textbf{LN} denotes applying layer normalization to the value. \\textbf{LT} denotes applying linear transformation to the value. \\textbf{IC} denotes using identical connection after the correlation aggregation.}\n\\label{table-structure}\n\\end{table}\n\n\\subsection{Different Structures of the AiA Module}\nBesides variant~(h) in the paper, we also explore other structures of the AiA module, where the following components are studied: (1) Layer normalization applied to the value. (2) Linear transformation applied to the value. (3) Identical connection after the correlation aggregation. To evaluate their effect, we design two other structures of the AiA module, \\ie AiAv2 and AiAv3. The differences between these structures are shown in Fig.~\\ref{figure-structure}. Note that AiAv1 is the structure we implement in AiATrack and AiAv3 is a typical self-attention structure in the vanilla Transformer \\cite{vaswani2017attention}.\n\nFrom the results in Tab.~\\ref{table-structure}, we can observe that the layer normalization and the identical connection are not key components in our AiA module. Applying linear transformation to the value can further improve the performance, but we remove it for the trade-off between performance and computational cost. Besides the observations above, all the experimental results validate the effectiveness of correlation refinement in the conventional attention mechanism with an extra attention module.\n\n\\subsection{Results on VOT}\nDifferent from previous reset-based evaluation protocol \\cite{kristan2019seventh}, VOT2020 \\cite{kristan2020eighth} proposes a new anchor-based evaluation protocol which is more reasonable. The same as STARK \\cite{yan2021learning} and DualTFR \\cite{xie2021learning}, we use Alpha-Refine \\cite{yan2021alpha} to generate masks for evaluation since the ground truths of VOT2020 are annotated by the segmentation masks. The overall performance is ranked by the Expected Average Overlap (EAO). As shown in Tab.~\\ref{table-vot}, our tracker exhibits very competitive performance, outperforming STARK with a margin of 5\\% in terms of EAO.\n\n\\begin{table}[t]\n\\centering\n\\begin{tabular}{c|ccccc}\n\\hline\n\\multirow{2}{*}{Tracker}\n& Alpha-Refine & OceanPlus & DualTFR & STARK-ST50 & AiATrack \\\\\n& \\cite{yan2021alpha} & \\cite{zhang2021toward} & \\cite{xie2021learning} & \\cite{yan2021learning} & (Ours) \\\\\n\\hline\nEAO & 0.482 & 0.491 & 0.528 & 0.505 & \\textbf{0.530} \\\\\nAccuracy & 0.754 & 0.685 & 0.755 & 0.759 & \\textbf{0.764} \\\\\nRobustness & 0.777 & \\textbf{0.842} & 0.836 & 0.817 & 0.827 \\\\\n\\hline\n\\end{tabular}\n\\caption{State-of-the-art comparison on VOT2020.}\n\\label{table-vot}\n\\end{table}\n\n\\section{Additional Visualization Results}\n\n\\begin{figure}[t]\n\\centering\n\\subfigure{\\includegraphics[width=.28\\textwidth]{imgs/attribute/ARC_overlap_OPE_AUC.png}}\n\\enspace\n\\subfigure{\\includegraphics[width=.28\\textwidth]{imgs/attribute/BC_overlap_OPE_AUC.png}}\n\\enspace\n\\subfigure{\\includegraphics[width=.28\\textwidth]{imgs/attribute/CM_overlap_OPE_AUC.png}}\n\\enspace\n\\subfigure{\\includegraphics[width=.28\\textwidth]{imgs/attribute/DEF_overlap_OPE_AUC.png}}\n\\enspace\n\\subfigure{\\includegraphics[width=.28\\textwidth]{imgs/attribute/FM_overlap_OPE_AUC.png}}\n\\enspace\n\\subfigure{\\includegraphics[width=.28\\textwidth]{imgs/attribute/FOC_overlap_OPE_AUC.png}}\n\\enspace\n\\subfigure{\\includegraphics[width=.28\\textwidth]{imgs/attribute/IV_overlap_OPE_AUC.png}}\n\\enspace\n\\subfigure{\\includegraphics[width=.28\\textwidth]{imgs/attribute/LR_overlap_OPE_AUC.png}}\n\\enspace\n\\subfigure{\\includegraphics[width=.28\\textwidth]{imgs/attribute/MB_overlap_OPE_AUC.png}}\n\\enspace\n\\subfigure{\\includegraphics[width=.28\\textwidth]{imgs/attribute/OV_overlap_OPE_AUC.png}}\n\\enspace\n\\subfigure{\\includegraphics[width=.28\\textwidth]{imgs/attribute/POC_overlap_OPE_AUC.png}}\n\\enspace\n\\subfigure{\\includegraphics[width=.28\\textwidth]{imgs/attribute/ROT_overlap_OPE_AUC.png}}\n\\enspace\n\\subfigure{\\includegraphics[width=.28\\textwidth]{imgs/attribute/SV_overlap_OPE_AUC.png}}\n\\enspace\n\\subfigure{\\includegraphics[width=.28\\textwidth]{imgs/attribute/VC_overlap_OPE_AUC.png}}\n\\caption{Attribute analysis on LaSOT. AUC scores are showed in the legend.}\n\\label{figure-analysis}\n\\end{figure}\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=.98\\textwidth]{imgs/qualitative.pdf}\n\\caption{Qualitative comparisons with two representative state-of-the-art trackers on 8 challenging sequences: \\textit{bird-17}, \\textit{goldfish-8}, \\textit{sepia-13}, \\textit{shark-2}, \\textit{sheep-3}, \\textit{squirrel-8}, \\textit{tiger-4}, \\textit{turtle-8}. Frame indexes are given on the top-left of each figure.}\n\\label{figure-qualitative}\n\\end{figure}\n\n\\subsection{Attribute Analysis}\nWe also provide detailed attribute analysis on LaSOT \\cite{fan2019lasot}. Fig. \\ref{figure-analysis} shows that our tracker has an encouraging performance in various kinds of scenarios like background clutter, camera motion, and deformation. The results suggest the great potential of the proposed method when dealing with challenging scenarios.\n\n\\subsection{Qualitative Comparisons}\nTo qualitatively compare our tracker with the state-of-the-art trackers, we visualize our tracking results with two recent representative trackers: KeepTrack \\cite{mayer2021learning} and STARK \\cite{yan2021learning}. Fig. \\ref{figure-qualitative} shows the tracking outputs for these trackers on some challenging video examples.\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{Unified Sequence-to-Sequence Learning for \\\\\nSingle- and Multi-Modal Visual Object Tracking}\n\n\\begin{document}\n\n\\title{Unified Sequence-to-Sequence Learning for \\\\\nSingle- and Multi-Modal Visual Object Tracking}\n\n\\author{Xin~Chen,\n        Ben~Kang,\n        Jiawen~Zhu,\n        Dong~Wang,\n        Houwen~Peng,\n        and~Huchuan~Lu\n        \\IEEEcompsocitemizethanks{\n     \\IEEEcompsocthanksitem {Xin Chen, Ben Kang, Jiawen Zhu, Dong Wang, and Huchuan Lu  are with the Dalian University of Technology. Houwen Peng is with the Microsoft Research.} \n     \\IEEEcompsocthanksitem {Corresponding author: Dong Wang, wdice@dlut.edu.cn}\n     }\n        }\n        \n\n\\maketitle\n\n\\begin{abstract}\nIn this paper, we introduce a new sequence-to-sequence learning framework for RGB-based and multi-modal object tracking. First, we present SeqTrack for RGB-based tracking. It casts visual tracking as a sequence generation task, forecasting object bounding boxes in an autoregressive manner. This differs from previous trackers, which depend on the design of intricate head networks, such as classification and regression heads. SeqTrack employs a basic encoder-decoder transformer architecture. The encoder utilizes a bidirectional transformer for feature extraction, while the decoder generates bounding box sequences autoregressively using a causal transformer. The loss function is a plain cross-entropy.\nSecond, we introduce SeqTrackv2, a unified sequence-to-sequence framework for multi-modal tracking tasks. Expanding upon SeqTrack, SeqTrackv2 integrates a unified interface for auxiliary modalities and a set of task-prompt tokens to specify the task. This enables it to manage multi-modal tracking tasks using a unified model and parameter set. This sequence learning paradigm not only simplifies the tracking framework, but also showcases superior performance across 14 challenging benchmarks spanning five single- and multi-modal tracking tasks. \nThe code and models are available at  \\href{https://github.com/chenxin-dlut/SeqTrackv2}{https://github.com/chenxin-dlut/SeqTrackv2}.\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nSequence learning, visual object tracking, multi-modal tracking, unified model.\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\\IEEEPARstart{V}isual object tracking is a foundational task in computer vision. Its objective is to estimate the position of an arbitrary target in  a video sequence, relying on its location in the initial frame. Existing tracking approaches commonly employ a divide-and-conquer strategy, wherein the tracking problem is decomposed into multiple subtasks, such as object scale estimation and center point localization.\nEach subtask is addressed by a dedicated head network. For example, SiamRPN \\cite{SiameseRPN} and its subsequent works \\cite{DiMP, Ocean, transt, TMT, ostrack} utilize classification heads for object localization and regression heads for scale estimation, as illustrated in Fig.\\ref{fig:pipeline}(a). STARK~\\cite{Stark} and transformer-based trackers \\cite{CSWinTT, simtrack, AiATrack} employ corner head networks to predict the bounding box corners of target objects, as depicted in Fig.\\ref{fig:pipeline}(b).\n\nSuch a divide-and-conquer strategy has demonstrated superior performance on tracking benchmarks, becoming the prevalent design in current models. However, two deficiencies remain. First, each subtask necessitates a customized head network, leading to a complicated tracking framework. Second, each head network requires one or more learning loss functions, such as cross-entropy loss \\cite{SiameseRPN, transt}, $\\ell_1$ loss \\cite{SiameseRPN, transt, Stark, ostrack}, and generalized IoU loss~\\cite{transt, Stark, ostrack}, making the training challenging due to additional hyperparameters.\n\n\\begin{figure}[!t]\n\\begin{center}\n\\includegraphics[width=1\\linewidth]{figures/pipeline1.pdf}\n\\end{center}\n\\begin{center}\n\\includegraphics[width=0.88\\linewidth]{figures/pipeline2.pdf}\n\\end{center}\n    \\vspace{-3mm}\n   \\caption{\n   Comparison of tracking frameworks. (a) The framework with object classification head and bounding box regression head. (b) The framework with corner prediction heads. (c) Sequence-to-sequence tracking framework without complicated head networks. (d) Sequence-to-sequence framework for multi-modal tracking.} \n       \\vspace{-5mm}\n\\label{fig:pipeline}\n\\end{figure}\n\nTo address these issues, we propose a new Sequence-to-sequence Tracking (SeqTrack) framework in this paper, as illustrated in Fig.~\\ref{fig:pipeline}(c). By modeling tracking as a sequence generation task, SeqTrack eliminates the need for complicated head networks and redundant loss functions. The underlying intuition is that if the model knows where the target object is, we could simply teach it how to read the bounding box out, rather than explicitly performing additional classification and regression via a divide-and-conquer strategy. \n\nTo this end, we convert the four values of a bounding box into a sequence of discrete tokens and train the model to generate this sequence token-by-token. We utilize a simple encoder-decoder transformer to model this generation. The encoder extracts visual features from video frames, while the decoder generates the sequence of bounding box values using the extracted features. The generation process operates in an autoregressive manner, where the model generates a token based on previously observed ones.\nAt each step, a newly generated token value is fed back into the model to produce the next one. We apply a causal mask on the self-attention modules in the decoder to prevent tokens from attending to subsequent tokens. This causal masking mechanism ensures that the generation of the token at position $i$ only depends on its preceding tokens at positions before $i$. The visual features are integrated into the decoder through cross-attention layers \\cite{2017Attention}. The generation concludes when it outputs four token values representing the bounding box. The resulting output sequence is directly used as the final tracking result.\n\nGiven the concise and versatile nature of the proposed sequence-to-sequence tracking framework, it is easily extended. In this study, we expand it to four multi-modal tracking tasks, including RGB+Depth, RGB+Thermal, RGB+Event, and RGB+Language tracking, resulting in a unified multi-modal tracking framework named SeqTrackv2. Specifically, we develop a unified interface to accommodate various auxiliary modalities. All modalities are converted into a unified sequence format and then inserted into the encoder through this interface. Furthermore, we employ a set of task-prompt tokens to specify the task. Each task-prompt token corresponds to a specific multi-modal tracking task. The task-prompt token corresponding to the current task is input into both the encoder and decoder, helping the model to process the modality associated with the current task. In this manner, SeqTrackv2 can manage diverse multi-modal tracking tasks using a unified model and parameter set, obviating the necessity to design and train specific models for each task.\n\nExperiments demonstrate that our SeqTrack method is effective, achieving new state-of-the-art performance on several tracking benchmarks. For example, SeqTrack-B256 attains a 74.7\\% AO score on GOT-10k \\cite{GOT10K}, surpassing the recent OSTrack-256 tracker \\cite{ostrack} by 3.7\\% under aligned settings, \\emph{i.e.}, using the same encoder architecture and input resolution.\nFurthermore, compared to the recent state-of-the-art tracker MixFormer~\\cite{mixformer}, SeqTrack-B256 operates 1.4 times faster (40 \\emph{v.s.} 29 \\emph{fps}) while achieving 0.7\\% superior AUC score on LaSOT~\\cite{LaSOT}. \nIt is noteworthy that all these prior methods heavily rely on well-designed head networks and the corresponding complicated loss functions \\cite{GIoU,focal_loss}. In contrast, our SeqTrack employs only a plain encoder-decoder transformer architecture with a simple cross-entropy loss.\nOn the multi-modal tracking tasks, SeqTrackv2 also delivers promising results. For instance, SeqTrackv2-L384 achieves a 61.0\\% AUC score on the RGB+Thermal benchmark LasHeR~\\cite{lasher},  74.8\\% EAO score on the RGB+Depth benchmark VOT-RGBD22~\\cite{vot2022}, and 63.4\\% AUC score on the RGB+Event benchmark VisEvent~\\cite{visevent}, outperforming ViPT~\\cite{vipt} by an average of 5.1\\% points. On the RGB+Language benchmark TNL2K~\\cite{TNL2K}, SeqTrackv2-L384 obtains a 62.4\\% AUC score, surpassing the recent JointNLT tracker~\\cite{JointNLT} by 5.5\\%. Notably, all these prior methods train different models for each multi-modal task, while our SeqTrackv2 employs a unified model and parameter set.\n\nIn summary, the contributions of this work are as follows:\n\\begin{itemize}[leftmargin=0.468cm]\n\\vspace{-1mm}\n    \\item{We propose a sequence-to-sequence learning method for visual tracking, which conceptualizes tracking as a generation task, providing a new perspective on tracking modeling.}\n    \\item{We propose a unified sequence-to-sequence learning method for multi-modal visual tracking. It unifies various multi-modal visual object tracking tasks with a single model and parameter set, obviating the necessity to design and train specific models for each task.}\n    \\item{We present a new family of sequence tracking models that strike a balance between speed and accuracy. Experiments verify the effectiveness of these new models.}\n\\end{itemize}\n\nA preliminary version of this work~\\cite{seqtrack} was published at the IEEE CVPR2023 conference. In this new version, we significantly extends it in the following aspects: First, we expand our previous RGB-based tracker to multi-modal tracking tasks, including RGB+Depth, RGB+Thermal, RGB+Event, and RGB+Language tracking tasks. This introduces a new sequence-to-sequence tracking method for multi-modal tracking named SeqTrackv2. Second, we propose a unified interface for various modalities and employ a set of task-prompt tokens, enabling our method to execute different multi-modal tracking tasks with a unified model and parameter set. Third, we develop a new family of multi-modal sequence tracking models that establish new state-of-the-art performance on various multi-modal tracking benchmarks. Furthermore, we conduct extensive experiments of the new SeqTrackv2 tracker and provide additional ablation studies of the previous RGB-based tracker SeqTrack.\n\n\\section{Related Work}\n\\textit{Visual Tracking.}\nExisting tracking approaches commonly adopt a divide-and-conquer strategy, decomposing tracking into multiple subtasks. They first extract visual features from video frames using a deep neural networks~\\cite{ResNet, 2017Attention, ViT}, and then design multiple task-specific head networks to predict the bounding boxes of target objects.\nBased on the differences in head networks, prior trackers can be categorized into two groups: 1) trackers relying on classification and regression, and 2) trackers based on corner predictions.\n\nMost prevalent trackers~\\cite{SiameseRPN,transt,SiamFC++,ostrack,ATOM, DiMP} belong to the first category, which models tracking with a classification head for foreground-background prediction and a regression head for object scale estimation. \nFor the classification head network, most trackers adopt stacked convolutional layers with various loss functions, including cross-entropy loss\n\\cite{SiameseRPN,transt},\nfocal loss~\\cite{SiamFC++,ostrack}, modified $\\ell_2$ loss\n\\cite{ATOM, DiMP}, and KL-divergence loss~\\cite{PrDiMP}.\nFor the regression head, Siamese trackers and some transformer-based trackers adopt stacked convolutional layers with $\\ell_1$ loss \\cite{SiameseRPN,SiamRPNplusplus} and IoU loss\\cite{transt, sbt}, while discriminative trackers\\cite{ATOM, DiMP} employ the IoU-Net~\\cite{IOU-Net} with MSE loss \\cite{ATOM}.\n\nSTARK~\\cite{Stark} and its follow-up works~\\cite{CSWinTT,mixformer,simtrack} belong to the corner prediction category, which locates target objects using corner prediction heads. They employ a two-branch network to generate two probability maps for the top-left and bottom-right corners of the target object.  The final object bounding boxes are obtained by calculating the expectation of corners' probability distribution.  The loss function is a combination of $\\ell_1$ and generalized IoU~\\cite{GIoU} loss.\n\nThe intricate design of head networks and loss functions complicates existing tracking frameworks and exacerbates training challenges. In contrast, our method reframes tracking as a sequence generation task, eliminating the need for complicated head networks and redundant loss functions. It relies solely on a single cross-entropy loss with a plain transformer.\n\n\\textit{Multi-Modal Tracking.}\nRecent advancements in single-modal visual object tracking, leveraging RGB image inputs, have yielded impressive results. Nevertheless, in complex environments or specific scenarios, relying solely on RGB imagery may not offer sufficient information to achieve accurate tracking. To address these challenges, many researchers~\\cite{ca3dms, apfnet, jmmac, cmpp, JointNLT, DecoupleTNL, SNLT, protrack, vipt} have turned their attention to multi-modal tracking, which integrates additional modalities alongside RGB inputs to enhance tracking performance. \n\nSpecifically, depth information~\\cite{rgbd1k, depthtrack} provides insights into the three-dimensional position and shape of targets, thereby enhancing tracker robustness against occlusions and complex backgrounds. Thermal infrared cues~\\cite{lasher, rgbt234} are crucial for tracking in low-light or adverse weather conditions, as they provide target features unaffected by environmental lighting variations. Event data~\\cite{visevent, COESOT}, captured at high frame rates, offer high dynamic range and low latency perception capabilities, suitable for scenarios with rapid movements or high-speed changes. The language modality~\\cite{TNL2K, TNLS}, on the other hand, can describe target attributes and behaviors through natural language, further augmenting the tracker's understanding and reasoning capabilities.\n\nDespite the benefits, existing multi-modal tracking methods heavily rely on modality-specific design and training, resulting in distinct models and parameter sets for each multi-modal tracking task. This fragmented situation introduces complexity, requires additional computational resources, and limits the ability to leverage shared information across modalities, ultimately hindering scalability and efficiency. In contrast, our method unifies different multi-modal tracking task in a sequence model, eliminating the need for modality-specific design or training.\n\n\\textit{Sequence Learning.}\nSequence-to-sequence learning, originally proposed for natural language modeling \\cite{sutskever2014sequence, cho2014learning}, has recently found applications in computer vision. \nPix2Seq \\cite{pix2seq} is a representative work that frames object detection as a token generation task conditioned on the observed pixel inputs. This sequence learning approach has also been effectively extended to other vision tasks, including instance segmentation and keypoint detection \\cite{pix2seqv2}. Moreover, in cross-modality domain, sequence learning is becoming increasingly popular. For instance, text-to-image generation models such as DALL-E \\cite{dalle} and vision-language models like Flamingo \\cite{flamingo} all leverage sequence-to-sequence learning to unify multi-modality pretraining.\n\nOur sequence learning framework shares a similar spirit with  Pix2Seq \\cite{pix2seq}, both casting vision tasks as sequence generation problems and discretizing continuous values of bounding box coordinates into integers. However, our method differs from Pix2Seq in three fundamental ways. \n1) The construction of the sequences differs. Pix2Seq uses object corner coordinates and object categories to set up the sequence, while our method employs center point coordinates and object scales.  \n2) The architectures are different. Pix2Seq adopts ResNet~\\cite{ResNet} as its backbone network followed by an encoder-decoder transformer. In contrast, our method is more compact, utilizing a single encoder-decoder transformer. It employs ViT~\\cite{ViT} as the encoder for feature extraction and causal transformer blocks as the decoder for sequence generation.\n3) The tasks are different, Pix2Seq is designed for detection, while ours is for tracking.\nSome previous tracking designs, such as online template update, can be seamlessly integrated into our method. \n4) The modalities are different, Pix2Seq is developed for RGB-based data, while ours can process auxiliary modalities including depth, thermal, event, and natural language.\n\n\\section{Seqtrack}\n\\label{sec:SeqTrack}\nThis section presents a comprehensive description of the proposed RGB-based tracking method, SeqTrack.\nWe start with a concise overview of our sequence-to-sequence tracking framework. Following that, we detail image and sequence representations, along with the proposed model architecture. Finally, we elucidate the training and inference pipelines, as well as the integration of tracking prior knowledge.\n\n\\begin{figure*}[htbp]\n\\begin{center}\n\\includegraphics[width=0.85\\linewidth]{figures/Framework.pdf}\n\\vspace{-3mm}\n\\end{center}\n   \\caption{(a) Architecture of the proposed SeqTrack. The key component is an encoder-decoder transformer. The encoder extracts visual features from input video frames. The causal decoder autoregressively generates the sequence of the bounding box tokens using the extracted features. (b) Detailed transformer block in the causal decoder. The input embeddings interact in a casual manner through a masked multi-head attention mechanism. The visual feature is incorporated into the decoder via a multi-head attention layer. }\n\\label{fig:framework}\n\\end{figure*}\n\n\\subsection{Overview}\n\\label{subsec:overview}\n\nThe overall framework of SeqTrack is depicted in Fig.~\\ref{fig:framework}(a). It employs a straightforward encoder-decoder transformer architecture. The object bounding box is first converted into a sequence of discrete tokens, \\emph{i.e.}, $[x,$$y,$$w,$$h]$.\nThe encoder extracts visual features from input video frames, while the decoder generates the sequence of bounding box tokens autoregressively using the extracted features. A causal attention mask is imposed on the self-attention modules in the decoder, restricting tokens to attend only to their preceding tokens.\nIn addition to the four bounding box tokens, we also employ two special tokens: \\texttt{start} and \\texttt{end}.\nThe \\texttt{start} token tells the model to begin the generation, while the \\texttt{end} token represents the completion of the generation.\nDuring training, the input sequence of the decoder is $[\\texttt{start},$$x,$$y,$$w,$$h]$, and the target sequence is $[x,$$y,$$w,$$h,$$\\texttt{end}]$.\nDuring inference, the decoder's input sequence initially contains a single \\texttt{start} token.\nAt each step, a new bounding box token is generated and appended to the input sequence to produce the next one. The process continues iteratively until the four token values of the bounding box are generated, indicating the completion of the prediction.\n\n\\subsection{Image and Sequence Representation}\n\\label{subsec:representation}\n\n\\textit{Image Representation.}\nThe encoder takes as input a template image $\\bm{t} \\in {\\mathbb{R}}^{3\\times{H}\\times{W}}$ and a search image $\\bm{s} \\in {\\mathbb{R}}^{3\\times{H}\\times{W}}$. \nThe image $\\bm{t}$ represents the object of interest, while $\\bm{s}$ represents the search region in subsequent video frames. In existing trackers \\cite{SiameseFC, SiameseRPN, transt, Stark}, the resolution of template images is typically smaller than that of search images. \nIn contrast, we maintain the same size for both the template and search images. This decision is based on the observation that including more background in the template image can enhance tracking performance.\nThe search and template images are partitioned into patches: $\\bm{s}_p \\in {\\mathbb{R}}^{N \\times {P^2 \\times 3}}$ and $\\bm{t}_p \\in {\\mathbb{R}}^{N \\times {P^2 \\times 3}}$, where $(P,$$P)$ represents the patch size, and $N$$=$$HW$$/$$P^2$ indicates the patch number. Subsequently, a linear projection is applied to map the image patches to visual embeddings. \nLearnable position embeddings \\cite{2017Attention} are added to the patch embeddings to preserve positional information. These combined embeddings are then processed by the encoder.\n\n\\textit{Sequence Representation.}\nWe represent the target bounding box as a sequence of discrete tokens.\nSpecifically, a bounding box is defined by its center point $[x,$$y]$ and scale $[w,$$h]$.\nThere are several bounding box formats, such as $[x,$$y,$$w,$$h]$ and $[w,$$h,$$x,$$y]$.\nWe opt for the format $[x,$$y,$$w,$$h]$, because it aligns with human prior knowledge: first localizing object position $[x,$$y]$, and then estimating its scale $[w,$$h]$.\nEach continuous coordinate is uniformly discretized into an integer within the range $[1,$ $n_{bins}]$.\nWe utilize a shared vocabulary $\\bm{V}$ for all coordinates. Each integer between $[1,$ $n_{bins}]$ corresponds to a word in $\\bm{V}$, resulting in a vocabulary size of $n_{bins}$ ($4,000$ in our experiments). The final input sequence is $[\\texttt{start},$$x,$$y,$$w,$$h]$, and the target sequence is $[x,$$y,$$w,$$h,$$\\texttt{end}]$.\nEach word in the vocabulary $\\bm{V}$ corresponds to a learnable embedding, which is optimized during training.\nThe special token \\texttt{start} also has a corresponding learnable embedding.\nThe embeddings corresponding to the input words are fed into the decoder.\nSince the transformer is permutation-invariant, we augment word embeddings with learnable position embeddings \\cite{2017Attention}. For the final model outputs, we need to map the embeddings back to words. To accomplish this, we employ a multi-layer perceptron with a softmax function to sample words from $\\bm{V}$ based on the output embeddings. \n\n\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=1\\linewidth]{figures/attn_seq.pdf}\n\\end{center}\n\\vspace{-3mm}\n   \\caption{(a) Illustration of the causal attention mask in the decoder using a maximum sequence length of 5 tokens. \n   An orange cell at row $i$ and column $j$ indicates that the attention mechanism is allowed to attend to the $j$th input token, when producing the $i$th output token. \n   (b) Illustration of the input and target sequences. Similar to autoregressive language modeling \\cite{2017Attention}, the input sequence is the target sequence with one position offset.}\n\\label{fig:attn&seq}\n\\end{figure}\n\n\\subsection{Model Architecture}\n\\label{subsec:architecture}\n\nOur model employs a simple encoder-decoder transformer architecture, as illustrated in Fig.~\\ref{fig:framework}. The encoder is responsible for extracting visual features from input video frames, while the decoder predicts the bounding boxes of target objects in an autoregressive manner. \n\n\\textit{Encoder.}\nThe encoder follows a standard vision transformer architecture (ViT) \\cite{ViT}, with two minor modifications: i) The \\texttt{CLASS} token is removed, since it is designed for image classification task.  ii) A linear projection is appended to the last layer to align the feature dimensions of the encoder and decoder.  The encoder receives patch embeddings of template and search images, producing their corresponding visual features. Only the features of the search image are fed into the decoder. The function of the encoder is to extract the visual features of search and template images in a joint way, and learn feature-level correspondence through attention layers.\n\n\\textit{Decoder.} The decoder of SeqTrack is a causal transformer \\cite{2017Attention}. As illustrated in Fig.~\\ref{fig:framework}(b), each transformer block consists of a masked multi-head attention, a multi-head attention, and a feed forward network (FFN). \nMore concretely, the masked multi-head attention receives the word embeddings from the preceding block, and applies a causal mask to ensure that each sequence element's output depends only on its preceding elements. In other words, the attention mask restricts the output embedding at position $i$ to only attend to the input embeddings at positions before $i$, as illustrated in Fig.~\\ref{fig:attn&seq}(a). After that, the multi-head attention integrates the extracted visual features into the word embeddings, which allows the word embeddings to attend to the visual features derived from the encoder. \nFinally, a feed forward network (FFN) is applied to generate embeddings for the next decoder block.\n\n\\subsection{Training and Inference}\n\\label{subsec:traininginference}\n\n\\textit{Training.} Similar to language modeling \\cite{2017Attention}, SeqTrack is trained to maximize the log-likelihood of the target tokens conditioned on the preceding subsequence and input video frames using cross-entropy loss. The learning objective function is formulated as:\n\\vspace{-2mm}\n\\begin{equation}\n\\vspace{-3mm}\n\\begin{split}\n\\label{equation:objective}\n{\\rm{maximize}} \\sum_{j=1}^{L}{\\rm{log}}Q(\\hat{\\bm{z}}_j|\\bm{s},\\bm{t},\\hat{\\bm{z}}_{< j}), \n\\end{split}\n\\end{equation}\nwhere $Q(\\cdot)$ represents the softmax probability, $\\bm{s}$ denotes the search image, $\\bm{t}$ is the template, $\\hat{\\bm{z}}$ is the target sequence, $j$ is the position of the token, and $L$ is the length of the target sequence.\nHere, $\\hat{\\bm{z}}_{< j}$ represents the preceding subsequence used to predict the current token $\\hat{\\bm{z}}_j$.\nThe input sequence is the target sequence with a one-position offset (excluding \\texttt{start} and \\texttt{end}), as visualized in Fig.~\\ref{fig:attn&seq}(b). \nSuch an offset, combined with the causal masking, ensures the autoregressive property of the sequence model. \nThe target sequence can be regarded as a description of the object's bounding box.\nThe training process aims to teach the model to ``read out\" the words of the description based on the preceding words.\n\n\\textit{Inference.}\nDuring inference, the encoder perceives the template image and the search region in successive video frames.\nThe initial input to the decoder is the \\texttt{start} token, which signals the model to start the generation process.\nThen, the model ``reads out\" the target sequence $[x,$$y,$$w,$$h,$$\\texttt{end}]$ token by token.\nFor each token, the model samples it from the vocabulary $\\bm{V}$ based on maximum likelihood, \\emph{i.e.}, \n$\\hat{\\bm{z}}_{j}$=${\\argmax}_{\\bm{z}_{j}}\nQ(\\bm{z}_{j}|\\bm{s},\\bm{t},\\hat{\\bm{z}}_{< j})$, where ${\\bm{z}}_{j}$ represents the words in $\\bm{V}$.\nAdditionally, we introduce online template update and window penalty during inference to integrate prior knowledge,  further enhancing the model's accuracy and robustness. Detailed descriptions are provided in the following subsection.\n\n\\subsection{Prior Knowledge Integration}\n\\label{subsec:prior}\n\nPrior knowledge, such as window penalty \\cite{SiameseFC,SiameseRPN} and online update \\cite{ATOM,Stark,li2013survey}, has been widely incorporated into existing tracking models and proven effective \\cite{transt,ostrack,mixformer,VITAL}. In this subsection, we discuss how to integrate such prior knowledge into the proposed sequence-to-sequence learning framework to further enhance tracking performance.\n\n\\textit{Online Update.}\nSince the appearance of a target object may change dramatically during online tracking, relying solely on the initial template image may not always yield accurate tracking results.\nTo address this issue, we incorporate online template update \\cite{Stark} into our method. More specifically, we introduce a dynamic template alongside the initial template image to capture the appearance changes of target objects. \nThe dynamic template is updated on the fly. It is well recognized that poor-quality templates may lead to inferior tracking performance~\\cite{mixformer}.\nAs a consequence, we utilize the likelihood of the generated tokens to automatically select reliable dynamic templates. \nSpecifically, we compute the average softmax scores over the four generated bounding box values. If the averaged score exceeds a specific threshold $\\tau$ and the update interval $T_u$ is reached, the dynamic template will be updated with the tracking result in the current frame; otherwise, it maintains the previous state. Experiments demonstrate that this simple approach can improve tracking accuracy (see the ablation in Sec.~\\ref{subsec:ablation}). \nMoreover, unlike previous methods~\\cite{Stark,mixformer}, our approach does not require an additional score head to determine whether to update the template, which typically necessitates a second stage of training.\n\n\\textit{Window Penalty.}\nIt is empirically validated that the pixel displacement between consecutive frames is relatively small \\cite{SiameseFC,SiameseRPN}.\nTo penalize large displacements, we introduce a new window penalty strategy to our method during online inference.\nSpecifically, the position of the target object in the previous frame corresponds to the center point of the current search region.\nThe discrete coordinates of the center point in the current search region are $[\\frac{n_{bins}}{2},$$\\frac{n_{bins}}{2}]$.\nWe penalize the likelihood of integers (\\emph{i.e.}, words) in the vocabulary $\\bm{V}$ based on their difference to $\\frac{n_{bins}}{2}$, when generating $x$ and $y$.\nA large difference between an integer and $\\frac{n_{bins}}{2}$ will incur a corresponding large penalty.\nIn implementation, the softmax scores of integers form a vector of size $n_{bins}$.\nWe simply multiply this vector by the Hanning window of the same size.\nThis effectively suppresses large displacements.\nUnlike previous practice \\cite{SiameseRPN,transt}, we avoid introducing of additional hyperparameters to tune the penalty magnitude.\n\n\\vspace{-1mm}\n\n\\section{Seqtrackv2}\n\\label{sec:seqtrackv2}\n\nThis section presents the proposed multi-modal tracking method, SeqTrackv2, in detail. First, we provide a brief overview of our  sequence-to-sequence multi-modal tracking framework. Then, we elaborate on the unified interface for auxiliary modalities and the methodology for task instruction. Finally, we describe the training and inference process.\n\n\\begin{figure*}[htbp]\n\\begin{center}\n\\includegraphics[width=0.85\\linewidth]{figures/Frameworkv2.pdf}\n\\end{center}\n\\vspace{-3mm}\n   \\caption{Architecture of the proposed SeqTrackv2. Expanding on SeqTrack, SeqTrackv2 integrates a unified interface and task-prompt tokens. This integration consolidates diverse multi-modal tracking tasks into a unified model and parameter set. }\n\\vspace{-1mm}\n\\label{fig:frameworkv2}\n\\end{figure*}\n\n\\subsection{Overview}\nThe overall framework of the proposed SeqTrackv2 is illustrated in Fig.~\\ref{fig:frameworkv2}. SeqTrackv2 extends the capabilities of its predecessor, SeqTrack, by accommodating various auxiliary modalities such as depth, thermal, event, and language inputs. Unlike traditional multi-modal tracking methods, which rely on separate models and parameter sets for each modality, SeqTrackv2 adopts a unified approach, allowing seamlessly integration of different modalities into a single model. This unification is facilitated by a unified interface and task-prompt tokens. The unified interface standardizes various modalities into a unified sequence format and feeds them into the encoder. Meanwhile, task-prompt tokens highlight the current multi-modal task, guiding the model's processing. While these enhancements augment SeqTrack's functionality, the core components and principles remain consistent across SeqTrackv2, ensuring the continuity and compatibility within our sequence tracking framework.\n\n\\subsection{Unified Interface}\n\\label{sec-interface}\n\nWe propose a unified interface for perceiving auxiliary modalities.\nInitially, we standardize various modalities into a unified sequence format, as depicted in Fig.~\\ref{fig:interface}(a). Specifically, for a modality with image format such as depth, thermal, and event data, we employ the standard patch embedding method~\\cite{ViT} to convert it into a sequence of embeddings. Regarding the language modality, we utilize a text encoder (BERT~\\cite{BERT} in our implementation) to extract the features and then apply max-pooling to obtain a single text embedding. Subsequently, we perform channel-wise product operations between the text embedding and the image-format modality's embeddings, resulting in the auxiliary-modal sequence denoted as $\\mathcal{M}_0$. Through this process, various modalities are unified into a sequence format. It is noteworthy that existing multi-modal benchmarks typically provide only one kind of auxiliary modality for each data sample. Therefore, we employ the \\texttt{PAD} token to fill the language sequence for samples without a language description. Similarly, for samples lacking an image-format auxiliary modality, we substitute the original RGB-based image to serve as the image-format auxiliary modality.\n\nAfter obtaining the auxiliary-modal sequence, we integrate it into the visual features of the encoder. Integration is performed at every encoder block,  as illustrated in Fig.~\\ref{fig:interface}(b).\nGiven that auxiliary modalities typically contain less information compared to RGB-based images, we integrate them in a low-rank manner for efficiency. Initially, both the auxiliary-modal sequence $\\mathcal{M}_{l-1}$ and the visual feature sequence $\\mathcal{V}_{l-1}$ of the encoder are projected from dimension $D$ to a lower dimension $d$ in terms of channels. Subsequently, they are element-wise added together and then projected back to dimension $D$, resulting in the new auxiliary-modal sequence $\\mathcal{M}_{l}$. The new $\\mathcal{M}_{l}$ is then added back to $\\mathcal{V}_{l-1}$. Finally, the encoder block $\\mathcal{E}_{l}$ processes this combined sequence to generate the new visual feature sequence $\\mathcal{V}_{l}$. This integration effectively incorporates the information of the auxiliary modality into the visual features.\n\n\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=0.8\\linewidth]{figures/interface_a.pdf}\n\\end{center}\n\\begin{center}\n\\includegraphics[width=0.8\\linewidth]{figures/interface_b.pdf}\n\\end{center}\n    \\vspace{-3mm}\n   \\caption{\n   Details of the proposed unified interface. (a) Various auxiliary modalities are convert into a sequence format. (b) The auxiliary-modal sequence is integrated into the visual features of the encoder.} \n       \\vspace{-1mm}\n\\label{fig:interface}\n\\end{figure}\n\n\\subsection{Task Prompting}\n\\label{sec-taskprompt}\n\nAlthough the auxiliary-modal inputs contain certain task-specific information, we have found that adding a one-token task prompt could further improve the model. Specifically, each multi-modal tracking task corresponds to a unique task-prompt token. In the encoder, we map the task-prompt token to an embedding and concatenate it with the visual embeddings before inputting it into the encoder. In the decoder, we simply replace the \\texttt{START} token with the task-prompt token. This way, the task-prompt token provides information about the current task, helping the model to better adapt to the specific multi-modal tracking task at hand.\n\n\\subsection{Training and Inference}\n\\label{sec-train}\nThe training and inference  procedures of SeqTrackv2 remain essentially consistent with those of SeqTrack. Additionally, during training, SeqTrackv2 loads the parameters of SeqTrack and then freezes them, thereby capitalizing on the knowledge acquired from large-scale RGB-based data. Only the parameters of the unified interface and the embeddings of tasks-prompt tokens are trained to assist the frozen SeqTrack model in managing the multi-modal tracking tasks. Concerning multi-task training, we evenly mix the data from four multi-modal tasks  in each minibatch.\n\n\\section{Experiments}\n\\label{sec:Experiments}\n\n\\subsection{Implementation Details}\n\\label{subsec:Implementation Details}\n\n\\textit{SeqTrack Model.}\nWe develop four variants of SeqTrack models with different encoder architectures and input resolutions, as elaborated in Tab.~\\ref{tab-model}. We adopt ViT-B~\\cite{ViT} as our encoder architecture for SeqTrack-B256 and B384, and ViT-L~\\cite{ViT} for SeqTrack-L256 and L384. The encoders are initialized with the MAE~\\cite{mae} pre-trained parameters. The patch size is set to $16$$\\times$$16$.\nThe decoder consists of $2$ transformer blocks, which is the same for all models. The decoder hidden size is $256$, the number of attention heads is $8$, and the hidden size of the feed forward network (FFN) is $1024$.\nThe number of quantization bins $n_{bins}$ and the vocabulary size are all set to $4,000$. The dimension of word embedding is $256$, which is consistent with the decoder hidden size. The embedding-to-word sampling uses a 3-layer perceptron followed by a softmax. The hidden dimension of the perceptron is $256$. The output dimension is $n_{bins}$, which is aligned with the number of words in $\\bm{V}$. The word with the maximum likelihood is sampled as the output word. In addition, we present model parameters, FLOPs, and inference speed in Tab.~\\ref{tab-model}. The speed is measured on Intel Core i9-9900K CPU @ 3.60GHz with 64 GB RAM and a single 2080 Ti GPU. All the models are implemented with Python 3.8 and PyTorch 1.11.0. \n\n\\textit{SeqTrackv2 Model.}\nThe model details of Seqtrackv2 closely resemble those of Seqtrack, except for the addition of the new unified interface. For the unified interface, the dimension $D$ depends on the encoder's dimension, while the low-rank dimension $d$ is set to 32. The parameters of SeqTrackv2's encoder, decoder, and vocabulary are initialized with SeqTrack and subsequently frozen. In terms of the unified interface, the text encoder is initialized with BERT~\\cite{BERT}. The remaining parameters are randomly initialized and made trainable.\n\n\\begin{table}[t]\n\\centering\n\\caption{Details of SeqTrack model variants.}\n\\label{tab-model}\n\\vspace{-2mm}\n\\setlength{\\tabcolsep}{0.7mm}{\n\\small\n\\scalebox{0.95}{\n\\begin{tabular}{l| c c c c c}\n\\toprule\n\\multirow{2}{*}{Model}           & \\multirow{2}{*}{~Encoder~} & ~Input~ & ~Params~ & ~FLOPs~ & ~Speed~ \\\\\n~ &~ & Resolution & (M) & (G) & (\\emph{fps}) \\\\\n\\midrule \nSeqTrack-B256~~   &   ViT-B       &   $256$$\\times$$256$   &   89  &66   &  40  \\\\\nSeqTrack-B384   &   ViT-B       &   $384$$\\times$$384$   &   89  &148   &  15  \\\\\nSeqTrack-L256   &   ViT-L      &   $256$$\\times$$256$   &   309 &232    &  15  \\\\\nSeqTrack-L384   &   ViT-L     &   $384$$\\times$$384$   &   309 &524    &  5  \\\\\n\\bottomrule\n\\end{tabular}}\n}\n\\vspace{-2mm}\n\\end{table}\n\n\\begin{table*}\n  \\centering\n  \\caption{State-of-the-art comparisons on four large-scale benchmarks. We add a symbol * over GOT-10k to indicate that the corresponding models are only trained with the GOT-10k training set. Otherwise, the models are trained with all the training data presented in Sec. \\ref{subsec:Implementation Details}.\n  \\vspace{-2.5mm}\n  The top three results are highlight with \\textbf{\\textcolor{cRed}{red}}, \\textcolor{blue}{blue} and \\textcolor{cGreen}{green} fonts, respectively.}\n  \\label{tab-sota}\n\\resizebox{1\\linewidth}{!}{\n  \\setlength{\\tabcolsep}{2mm}{  \n  \\small\n  \\begin{tabular}{l|l| ccc c ccc c ccc c ccc}\n    \\toprule\n    &\\multirow{2}*{Method} & \\multicolumn{3}{c}{LaSOT~\\cite{LaSOT}}&& \\multicolumn{3}{c}{LaSOT$_{ext}$~\\cite{lasot_journal}} && \\multicolumn{3}{c}{TrackingNet~\\cite{trackingnet}} && \\multicolumn{3}{c}{GOT-10k*~\\cite{GOT10K}}\\\\\n    \\cline{3-5}\n    \\cline{7-9}\n    \\cline{11-13}\n    \\cline{15-17}\n    && AUC&P$_{Norm}$&P && AUC&P$_{Norm}$&P && AUC&P$_{Norm}$&P && AO&SR$_{0.5}$&SR$_{0.75}$\\\\\n    \\midrule[0.5pt]\n    \\multirow{4}*{\\rotatebox{90}{Seq2Seq}} &SeqTrack-L384\t&\\textbf{\\textcolor{cRed}{72.5}}\t&\\textcolor{blue}{81.5}\t&\\textbf{\\textcolor{cRed}{79.3}} & &\\textbf{\\textcolor{cRed}{50.7}} &\\textbf{\\textcolor{cRed}{61.6}} &\\textcolor{blue}{57.5} & &\\textbf{\\textcolor{cRed}{85.5}}\t&\\textbf{\\textcolor{cRed}{89.8}}\t&\\textbf{\\textcolor{cRed}{85.8}} & &\\textbf{\\textcolor{cRed}{74.8}} &81.9\t&\\textbf{\\textcolor{cRed}{72.2}} \\\\\n    &SeqTrack-L256\t&\\textcolor{blue}{72.1}\t&\\textbf{\\textcolor{cRed}{81.7}}\t&\\textcolor{blue}{79.0} & &\\textcolor{blue}{50.5} &\\textcolor{blue}{61.5} &\\textcolor{cGreen}{57.2} & &\\textcolor{blue}{85.0} &\\textcolor{blue}{89.5}\t&\\textcolor{blue}{84.9}  & &\\textcolor{cGreen}{74.5} &\\textcolor{cGreen}{83.2} &\\textcolor{blue}{72.0} \\\\\n    &SeqTrack-B384\t&\\textcolor{cGreen}{71.5}\t&\\textcolor{cGreen}{81.1}\t&\\textcolor{cGreen}{77.8} & &\\textcolor{blue}{50.5} &\\textbf{\\textcolor{cRed}{61.6}} &\\textcolor{blue}{57.5} & &\\textcolor{cGreen}{83.9} &88.8\t&83.6\t & &\\textcolor{cGreen}{74.5} &\\textcolor{blue}{84.3} &71.4\\\\\n    &SeqTrack-B256\t&69.9\t&79.7\t&76.3 & &\\textcolor{cGreen}{49.5} &60.8 &56.3 & &83.3\t&88.3\t&82.2  & &\\textcolor{blue}{74.7} &\\textbf{\\textcolor{cRed}{84.7}} &\\textcolor{cGreen}{71.8}\\\\\n    \\midrule[0.1pt]\n    \\multirow{7}*{\\rotatebox{90}{Corner Prediction}} &SimTrack~\\cite{simtrack}\t&70.5\t&79.7\t&-  &  &- &- &-  &  &83.4 &87.4\t&-  &  &69.8\t&78.8\t&66.0\\\\\n    &Mixformer-L~\\cite{mixformer}\t&70.1\t&79.9\t&76.3 & &- &- &- & &\\textcolor{cGreen}{83.9} &\\textcolor{cGreen}{88.9}\t&83.1 & &-\t&-\t&-\\\\\n    &Mixformer-22k~\\cite{mixformer}\t&69.2\t&78.7\t&74.7 & &- &- &- & &83.1\t&88.1\t&81.6 & &70.7\t&80.0\t&67.8\\\\\n    &AiATrack~\\cite{AiATrack}\t&69.0\t&79.4\t&73.8  &  &47.7 &55.6 &55.4  & &82.7 &87.8  &80.4  &  &69.6\t&63.2\t&80.0\\\\\n    &UTT~\\cite{UTT}\t&64.6\t&-\t&67.2 & &- &- &- & &79.7 &- &77.0 & &67.2\t&76.3\t&60.5\\\\\n    &CSWinTT~\\cite{CSWinTT}\t&66.2\t&75.2\t&70.9 & &- &- &- & &81.9\t&86.7\t&79.5 &\t&69.4\t&78.9\t&65.4\\\\\n    &STARK~\\cite{Stark}\t&67.1\t&77.0\t&- & &- &- &- & &82.0\t&86.9\t&- &\t&68.8\t&78.1\t&64.1\\\\\n    \\midrule[0.1pt]\n    \\multirow{22}*{\\rotatebox{90}{Classification + Regression}} \n    &OSTrack-384~\\cite{ostrack}\t&71.1\t&\\textcolor{cGreen}{81.1}\t&77.6 & &\\textcolor{blue}{50.5} &\\textcolor{cGreen}{61.3} &\\textbf{\\textcolor{cRed}{57.6}} &  &\\textcolor{cGreen}{83.9} &88.5\t&\\textcolor{cGreen}{83.2}  &  &73.7\t&\\textcolor{cGreen}{83.2}\t&70.8\\\\\n    &OSTrack-256~\\cite{ostrack}\t&69.1\t&78.7\t&75.2  &  &47.4 &57.3 &53.3  &  &83.1 &87.8\t&82.0  &  &71.0\t&80.4\t&68.2\\\\\n    &SwinTrack~\\cite{swintrack}\t&71.3\t&-\t&76.5 & &49.1 &- &55.6 & &84.0\t&-\t&82.8 &\t&72.4\t&-\t&67.8\\\\\n    &RTS~\\cite{RTS}\t&69.7\t&76.2\t&73.7  &  &- &- &-  &  &81.6 &86.0  &79.4  &  &-\t&-\t&-\\\\\n    &Unicorn~\\cite{unicorn}\t&68.5\t&-\t&-  &  &- &- &-  &  &83.0 &86.4\t&82.2  &  &-\t&-\t&-\\\\\n    &SLT~\\cite{SLT}\t&66.8\t&75.5\t&- & &- &- &- & &82.8 &87.5\t&81.4 & &67.5\t&76.5\t&60.3\\\\\n    &SBT~\\cite{sbt}\t&66.7\t&-\t&71.1 & &- &- &- & &-\t&-\t&-\t& &70.4\t&80.8\t&64.7\\\\\n    &ToMP~\\cite{ToMP}\t&68.5\t&79.2\t&73.5 & &45.9 &- &- & &81.5\t&86.4\t&78.9 &\t&-\t&-\t&-\\\\\n    &KeepTrack~\\cite{keeptrack}\t&67.1\t&77.2\t&70.2 & &48.2 &- &- & &-\t&-\t&-\t& &-\t&-\t&-\\\\\n    &AutoMatch~\\cite{automatch} \t&58.3\t&-\t&59.9 & &- &- &- & &76.0\t&-\t&72.6\t& &65.2\t&76.6\t&54.3\\\\\n    &TransT~\\cite{transt}\t&64.9\t&73.8\t&69.0 & &- &- &- & &81.4\t&86.7\t&80.3 &\t&67.1\t&76.8\t&60.9\\\\\n    &TrDiMP~\\cite{TMT}  \t&63.9\t&-\t&61.4 & &- &- &- &\t&78.4\t&83.3 \t&73.1\t&\t&68.8\t&80.5\t&59.7\\\\\n    &SiamAttn~\\cite{SiamAtt}  \t&56.0\t&64.8\t&- & &- &- &- &\t&75.2\t&81.7\t&-\t&\t&-\t&-\t&-\\\\\n    &SiamBAN~\\cite{SiamBAN}  \t&51.4\t&59.8\t&- & &- &- &- &\t&-\t&- \t&-\t&\t&-\t&-\t&-\\\\\n    &DSTrpn~\\cite{DSTrpn}\t&43.4\t&54.4\t&- & &- &- &-\t& &64.9\t&-\t&58.9 &\t&-\t&-\t&-\\\\\n    &Ocean~\\cite{Ocean}\t&56.0\t&65.1\t&56.6 & &- &- &-\t& &-\t&-\t&- &\t&61.1\t&72.1\t&47.3\\\\\n    &SiamR-CNN~\\cite{SiamRCNN}  \t&64.8\t&72.2\t&- &\t&- &- &- & &81.2\t&85.4\t&80.0\t&\t&64.9\t&72.8\t&59.7\\\\\n    &DiMP~\\cite{DiMP}\t   \t&56.9\t&65.0\t&56.7 &\t&39.2 &47.6 &45.1 & &74.0\t&80.1\t&68.7 &\t&61.1\t&71.7\t&49.2\\\\\n    &SiamPRN++~\\cite{SiamRPNplusplus}\t&49.6\t&56.9\t&49.1 & &34.0 &41.6 &39.6 &\t&73.3\t&80.0\t&69.4 &\t&51.7\t&61.6\t&32.5\\\\\n    &ATOM~\\cite{ATOM}\t   \t&51.5\t&57.6\t&50.5 &\t&37.6 &45.9 &43.0 & &70.3\t&77.1\t&64.8\t& &55.6\t&63.4\t&40.2\\\\\n  \\bottomrule\n\\end{tabular}\n}}\n  \\vspace{-4mm}\n\\end{table*}\n\n\\textit{SeqTrack Training.} \nOur training data includes the training splits of COCO~\\cite{COCO}, LaSOT~\\cite{LaSOT}, GOT-10k~\\cite{GOT10K}, and TrackingNet~\\cite{trackingnet}. Aligned with VOT2020 evaluation protocol \\cite{vot2020}, we remove the $1k$ forbidden videos in GOT-10k during training. For the evaluation on GOT-10k test set, we follow the official requirements \\cite{GOT10K} and only use the training split of GOT-10k. \nThe template and search images are obtained by expanding the target bounding boxes by a factor of $4$. Horizontal flip and brightness jittering are used for data augmentation. We train the model with AdamW~\\cite{AdamW} optimizer and set the learning rate of the encoder to $1$$e$$-5$, the decoder and remaining modules to $1$$e-$$4$, and the weight decay to $1$$e$$-4$. The training of SeqTrack are conducted on Intel Xeon CPU E5-2690 v4 @ 2.60GHz with 512 GB RAM and 8 Tesla A100 GPUs with 80GB memory. Each GPU holds $8$ image pairs, resulting in a total batch size of $64$. The model is trained for a total of $500$ epochs with $60k$ image pairs per epoch. The learning rate decreases by a factor of $10$ after $400$ epochs. \n\n\\textit{SeqTrackv2 Training.} \nThe training data of SeqTrackv2 encompasses the training sets of DepthTrack~\\cite{depthtrack}, VisEvent~\\cite{visevent}, LasHeR~\\cite{lasher}, TNL2K~\\cite{TNL2K}, RefCOCOg~\\cite{refcocog}, OTB99~\\cite{TNLS}, and LaSOT~\\cite{lasot_journal}. The training is conducted on Intel Xeon CPU Gold 6330 @ 2.00GHz with 512 GB RAM and 4 Tesla A40 GPUs with 48GB memory. Batch sizes are configured as follows: 128, 48, 64, and 16 for SeqTrackv2-B256, -B384, -L256, and -L384, respectively. The model undergoes training for a total of 240 epochs, with 60,000 samples per epoch. The learning rate decreases by a factor of 10 after 192 epochs. All other settings remain consistent with those of SeqTrack.\n\n\\textit{Inference.}\nThe online template update interval $T_u$ is set to $25$ by default, while the update threshold $\\tau$ is set to $0.015$. For window penalty, the softmax likelihood of the $4,000$ words in the vocabulary $\\bm{V}$ are directly multiplied by a 1D Hanning window of size $4,000$.\n\n\\subsection{State-of-the-Art Comparisons of SeqTrack}\n\\label{subsec:sotav1}\n\nWe compare our SeqTrack with state-of-the-art trackers on eight tracking benchmarks.\n\n\\textit{LaSOT.}\nLaSOT~\\cite{LaSOT} is a large-scale long-term tracking benchmark. The test set comprises 280 videos, with an average length of 2448 frames. As reported in Tab.~\\ref{tab-sota}, SeqTrack-B256 performs slightly better than the recent state-of-the-art method OSTrack-256 \\cite{ostrack}, achieving a 0.8\\% improvement in AUC score, using the same ViT-B encoder architecture and input resolution.  SeqTrack-B384 surpasses all previous trackers with an AUC score of 71.5\\%.  Furthermore, SeqTrack-L384 and L256 obtain new state-of-the-art AUC scores of 72.5\\% and 72.1\\%, respectively. SeqTrack-L384 surpasses the previous best tracker SwinTrack \\cite{swintrack} by 1.2\\%.\nFig.~\\ref{fig:lasotattr} illustrates the results of attribute-based evaluation, demonstrating that our SeqTrack-L384 performs better than other competing trackers on almost all attributes. In particular, our method exhibits significant advantages in the attributes of deformation and background clutter, showcasing a superior discriminative ability of the model. \n\n\\textit{LaSOT$_{ext}$.}\nLaSOT$_{ext}$~\\cite{lasot_journal} is a recently released extension to the LaSOT dataset, comprising of 150 video sequences spanning 15 object classes. The results on LaSOT$_{ext}$ are aso presented in Tab.~\\ref{tab-sota}.\nUnder aligned settings, SeqTrack-B256 achieves 2.1\\% higher AUC score compared to OSTrack-256. With a more powerful ViT-L encoder, SeqTrack-L384 achieves the highest AUC score of 50.7\\%.\n\n\\textit{TrackingNet.}\nTrackingNet~\\cite{trackingnet} is a large-scale tracking dataset. Its test set contains 511 videos covering diverse object categories and scenes. As reported in Tab.~\\ref{tab-sota}, SeqTrack-B384 and SeqTrack-B256 achieve competitive results compared with previous state-of-the-art trackers. SeqTrack-L384 obtains the best AUC of 85.5\\%, surpassing the previous best tracker SwinTrack by 1.5\\%.\n\n\\textit{GOT-10k.}\nGOT-10k~\\cite{GOT10K} test set contains 180 videos covering a wide range of common challenges in tracking. Following the official requirements, we solely use the GOT-10k training set to train our models. \nAs reported in Tab.~\\ref{tab-sota}, SeqTrack-B256 obtains 3.7\\% improvement over OSTrack-256 under aligned settings. SeqTrack-L384 achieves the best AO score of 74.8\\%, surpassing the previous state-of-the-art method by 1.1\\%.\n\n\\textit{TNL2K, NFS and UAV123.}\nWe assess our trackers on three additional benchmarks: TNL2K~\\cite{TNL2K}, NFS~\\cite{NFS}, and UAV123~\\cite{UAV}. TNL2K is a recently introduced large-scale dataset comprising 700 challenging video sequences. NFS and UAV123 are two small-scale benchmarks including 100 and 123 videos, respectively. On the large-scale TNL2K benchmark, our SeqTrack-L384 achieves a new state-of-the-art performance with 57.8\\% AUC score, as reported in Tab.~\\ref{tab-sota-small}. On the small-scale benchmarks NFS and UAV123, Tab.~\\ref{tab-sota-small} shows our SeqTrack models also achieve competitive results, being comparable or slightly better than the most recent trackers OSTrack and SimTrack.\n\n\\textit{VOT2020.}\nThe VOT2020~\\cite{vot2020} benchmark contains 60 challenging videos. VOT employs binary segmentation masks as ground-truth annotations. To generate segmentation masks, we equip SeqTrack with Alpha-Refine~\\cite{Alpha-Refine}.\nWe evaluate our models by submitting both the bounding boxes and the segmentation masks.\nAs depicted in Fig.~\\ref{fig:votrank}, our SeqTrack-L384 achieves the best results with EAO scores of 31.9\\% and 56.1\\% on bounding box and mask evaluations, respectively.\n\n\\begin{figure}[t!]\n\\centering\n    \\includegraphics[width=1\\linewidth]{figures/attr_lasot.pdf}\n    \\vspace{-7mm}\n    \\caption{AUC scores of different attributes on LaSOT \\cite{LaSOT}}\n    \\label{fig:lasotattr}\n      \\vspace{-1mm}\n\\end{figure}\n\n\\begin{figure}[t!]\n\\centering\n\\begin{center}\n    \\includegraphics[width=1\\linewidth]{figures/vot_bbox.pdf}\n    \\vspace{-2mm}\n\\end{center}\n\\begin{center}\n\\vspace{-2mm}\n    \\hspace{-0.8mm}\\includegraphics[width=1\\linewidth]{figures/vot_mask.pdf}\n\\end{center}\n\\vspace{-5mm}\n\\caption{EAO rank plots on VOT2020. For the mask evaluation, we use Alpha-Refine~\\cite{Alpha-Refine} to predict masks.}\n\\vspace{-2mm}\n\\label{fig:votrank}\n\\end{figure}\n\n\\begin{table}[t]\\normalsize\n    \\vspace{-1mm}\n    \\caption{Comparison with state-of-the-art methods on additional benchmarks in AUC score.}\n\\label{tab-sota-small}\n\\vspace{-2mm}\n  \\centering\n\\resizebox{1\\linewidth}{!}{\n  \\setlength{\\tabcolsep}{3mm}{\n    \\small\n    \\begin{tabular}{l|ccc}\n    \\toprule\n    Method &TNL2K~\\cite{TNL2K}&NFS~\\cite{NFS}&UAV123~\\cite{UAV}\\\\\n    \\midrule[0.5pt]\n    SeqTrack-L384 &\\textbf{\\textcolor{cRed}{57.8}}&66.2&68.5 \\\\\n    SeqTrack-L256 &\\textcolor{blue}{56.9}&\\textcolor{blue}{66.9}&\\textcolor{cGreen}{69.7} \\\\\n    SeqTrack-B384 &\\textcolor{cGreen}{56.4}&\\textcolor{cGreen}{66.7}&68.6 \\\\\n    SeqTrack-B256 &54.9&\\textbf{\\textcolor{cRed}{67.6}}&69.2 \\\\\n    \\midrule[0.1pt]\n    OSTrack~\\cite{ostrack}&55.9&66.5&\\textcolor{blue}{70.7} \\\\\n    SimTrack~\\cite{simtrack}&55.6&-&\\textbf{\\textcolor{cRed}{71.2}} \\\\\n    STARK~\\cite{Stark}&-&66.2&68.2 \\\\\n    TransT~\\cite{transt}&50.7&65.7&69.1 \\\\\n    TrDiMP~\\cite{TMT}&-&66.5&67.5 \\\\\n    DiMP~\\cite{DiMP}&44.7&61.8&64.3 \\\\\n    Ocean~\\cite{Ocean}&38.4&49.4&57.4 \\\\\n    ATOM~\\cite{ATOM}&40.1&58.3&63.2 \\\\\n    ECO~\\cite{ECO}&32.6&52.2&53.5 \\\\\n    RT-MDNet~\\cite{RTMDNet}&-&43.3&52.8 \\\\\n    SiamFC\\cite{SiameseFC}&29.5&37.7&46.8 \\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n  }\n  \\vspace{-2mm}\n\\end{table}\n\n\\subsection{State-of-the-Art Comparisons of SeqTrackv2}\n\nWe compare our SeqTrackv2 with state-of-the-art trackers on seven benchmarks across four multi-modal tracking tasks.\n\n\\textit{LasHeR.} \nLasHeR~\\cite{lasher} is a comprehensive benchmark with high diversity for RGB+Thermal tracking. We evaluate our method on its 245 testing video sequences, and report the obtained AUC and Precision (P) scores in Tab.~\\ref{tab-sota-rgbt}. Our four SeqTrackv2 models all surpass previous RGB+Thermal tracking methods, establishing new state-of-the-art performance. SeqTrackv2-L384 obtains the highest AUC score of 61.0 \\%, outperforming the prior best trackers ViPT~\\cite{vipt} by a large margin of 8.5 \\%.\n\n\\textit{RGBT234.}\nRGBT234~\\cite{rgbt234} is a substantial RGB+Thermal tracking benchmark with 234 videos comprising visible and thermal infrared pairs. Its distribution differs from our training set, thereby enabling a more comprehensive validation of method versatility. As reported in Tab.~\\ref{tab-sota-rgbt}, all four SeqTrackv2 models obtains superior performance than previous RGB+Thermal trackers. SeqTrackv2-L256 achieves the best AUC score of 68.5\\%, outperforming the recent state-of-the-art tracker Un-Track~\\cite{untrack} by 6.0\\%. The superior performance on RGBT234 demonstrate the good versatility of our method.\n\n\\textit{VOT-RGBD22.}\nVOT-RGBD2022~\\cite{vot2022} is a contemporary benchmark comprising 127 RGB+Depth sequences. The evaluation protocol utilized is anchor-based~\\cite{vot2020}, necessitating trackers to undergo multiple initialization starts from diverse points. The expected average overlap (EAO) serves as the main performance metric. As reported in Tab.~\\ref{tab-sota-rgbd}, our  SeqTrackv2 models surpass all previous RGB+Depth tracking methods. SeqTrackv2-B384 obtains the best EAO score of 75.5\\%.\n\n\\textit{DepthTrack.}\nDepthTrack~\\cite{depthtrack} is a comprehensive long-term RGB+Depth tracking benchmark, comprising 150 training and 50 testing videos featuring 15 per-frame attributes. The primary measure is F-score, typically used for long-term tracking. Although our method is not equipped with any re-detection module for long-term tracking, Tab.~\\ref{tab-sota-rgbd} shows our SeqTrackv2-B256 still achieves a new state-of-the-art performance of 63.2\\% F-score.\n\n\\textit{VisEvent.}\nVisEvent~\\cite{visevent} is a largest RGB+Event benchmark collected from the real world. The test set of VisEvent comprises 320 videos. As shown in Tab.~\\ref{tab-sota-rgbe}, our smallest model SeqTrackv2-B256 has surpassed all previous RGB+Event tracking methods with an AUC score of 61.2\\%. With the model scaling-up, our SeqTrackv2-L384 model establishes the best AUC score of 63.4\\%, outperforming the previous best tracker ViPT~\\cite{vipt} by 4.2\\%.\n\n\\begin{table}[t]\\normalsize\n    \\vspace{-1mm}\n    \\caption{State-of-the-art comparisons on RGB+Therm tracking}\n\\label{tab-sota-rgbt}\n\\vspace{-2mm}\n  \\centering\n\\resizebox{1\\linewidth}{!}{\n  \\setlength{\\tabcolsep}{4mm}{\n    \\small\n    \\begin{tabular}{l|ccccc}\n    \\toprule\n    \\multirow{2}*{Method} & \\multicolumn{2}{c}{LasHeR~\\cite{lasher}} & & \\multicolumn{2}{c}{RGBT234~\\cite{rgbt234}} \\\\\n        \\cline{2-3} \\cline{5-6}\n & AUC & P & &MSR &MPR \\\\\n    \\midrule[0.5pt]\n    SeqTrackv2-L384 &\\textbf{\\textcolor{cRed}{61.0}}&\\textbf{\\textcolor{cRed}{76.7}} & &\\textcolor{blue}{68.0}&\\textcolor{blue}{91.3}\\\\\n    SeqTrackv2-L256 &\\textcolor{blue}{58.8}&\\textcolor{blue}{74.1} & &\\textbf{\\textcolor{cRed}{68.5}}&\\textbf{\\textcolor{cRed}{92.3}}\\\\\n    SeqTrackv2-B384  &\\textcolor{cGreen}{56.2}&\\textcolor{cGreen}{71.5} & &\\textcolor{cGreen}{66.3}&\\textcolor{cGreen}{90.0}\\\\\n    SeqTrackv2-B256 &55.8&70.4 & &64.7&88.0 \\\\\n    \\midrule[0.1pt]\n    Un-Track~\\cite{untrack} &-&- & &62.5&84.2\\\\\n    ViPT~\\cite{vipt} &52.5&65.1 & &61.7&83.5\\\\\n    ProTrack~\\cite{protrack} &42.0&53.8 & &59.9&79.5\\\\\n    OSTrack~\\cite{ostrack} &41.2&51.5 & &54.9&72.9\\\\\n    APFNet~\\cite{apfnet} &36.2&50.0 & &57.9&82.7\\\\\n    CMPP~\\cite{cmpp} &-&- & &57.5&82.3\\\\\n    JMMAC~\\cite{jmmac} &-&- & &57.3&79.0\\\\\n    mfDiMP~\\cite{mfdimp} &34.3&44.7 & &42.8&64.6\\\\\n    DAPNet~\\cite{dapnet} &31.4&43.1 & &-&-\\\\\n    CAT~\\cite{cat} &31.4&45.0 & &56.1&80.4\\\\\n    HMFT~\\cite{vtuav} &31.3&43.6 & &-&-\\\\\n    MaCNet~\\cite{macnet} &-&- & &55.4&79.0\\\\\n    FANet~\\cite{fanet} &30.9&44.1 & &55.3&78.7\\\\\n    DAFNet~\\cite{dafnet} &-&- & &54.4&79.6 \\\\\n    SGT~\\cite{sgt} &25.1&36.5 & &47.2&72.0 \\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n  }\n\\end{table}\n\n\\begin{table}[t]\\normalsize\n    \\caption{State-of-the-art comparisons on RGB+Depth tracking}\n\\label{tab-sota-rgbd}\n\\vspace{-2mm}\n  \\centering\n\\resizebox{1\\linewidth}{!}{\n  \\setlength{\\tabcolsep}{2mm}{\n    \\small\n    \\begin{tabular}{l|ccc c ccc}\n    \\toprule\n    \\multirow{2}*{Method} & \\multicolumn{3}{c}{VOT-RGBD22~\\cite{vot2022}} & & \\multicolumn{3}{c}{DepthTrack~\\cite{depthtrack}} \\\\\n        \\cline{2-4} \\cline{6-8}\n & EAO & Acc. & Rob. & &F-score &Re &Pr \\\\\n    \\midrule[0.5pt]\n    SeqTrackv2-L384 &\\textcolor{cGreen}{74.8}&\\textbf{\\textcolor{cRed}{82.6}}&\\textcolor{blue}{91.0} & &\\textcolor{cGreen}{62.3}&\\textcolor{cGreen}{62.6}&\\textcolor{blue}{62.5}\\\\\n    SeqTrackv2-L256 &\\textcolor{blue}{74.9}&81.3&\\textbf{\\textcolor{cRed}{91.8}} & &\\textcolor{blue}{62.8}&\\textcolor{blue}{63.0}&\\textcolor{blue}{62.5}\\\\\n    SeqTrackv2-B384  &\\textbf{\\textcolor{cRed}{75.5}}&\\textcolor{cGreen}{81.9}&\\textbf{\\textcolor{cRed}{91.8}} & &59.8&60.0&59.6\\\\\n    SeqTrackv2-B256 &74.4&81.5&\\textcolor{blue}{91.0} & &\\textbf{\\textcolor{cRed}{63.2}}&\\textbf{\\textcolor{cRed}{63.4}}&\\textbf{\\textcolor{cRed}{62.9}} \\\\\n    \\midrule[0.1pt]\n    Un-Track~\\cite{untrack} &72.1&\\textcolor{blue}{82.0} &86.9 & &61.0&60.8&\\textcolor{cGreen}{61.1}\\\\\n    ViPT~\\cite{vipt} &72.1&81.5 &\\textcolor{cGreen}{87.1} & &59.4&59.6&59.2\\\\\n    ProTrack~\\cite{protrack} &65.1&80.1&80.2 & &57.8&57.3&58.3\\\\\n    SPT~\\cite{rgbd1k} &65.1&79.8&85.1 & &53.8&54.9&52.7\\\\\n    SBT-RGBD~\\cite{sbt} &70.8&80.9&86.4 & &-&-&-\\\\\n    OSTrack~\\cite{ostrack} &67.6&80.3&83.3 & &52.9&52.2&53.6\\\\\n    DeT~\\cite{depthtrack} &65.7&76.0&84.5 & &53.2&50.6&56.0\\\\\n    DMTrack~\\cite{vot2022} &65.8&75.8&85.1 & &-&-&-\\\\\n    DDiMP~\\cite{vot2020} &-&-&- & &48.5&56.9&50.3\\\\\n    STARK-RGBD~\\cite{Stark} &64.7&80.3&79.8 & &-&-&-\\\\\n    KeepTrack~\\cite{keeptrack} &60.6&75.3&79.7 & &-&-&-\\\\\n    DRefine~\\cite{vot2021} &59.2&77.5&76.0 & &-&-&-\\\\\n    ATCAIS~\\cite{vot2020} &55.9&76.1&73.9 & &47.6&45.5&50.0\\\\\n    LTMU-B~\\cite{LTMU} &-&-&- & &46.0&41.7&51.2\\\\\n    GLGS-D~\\cite{vot2020} &-&-&- & &45.3&36.9&58.4\\\\\n    DAL~\\cite{dal} &-&-&- & &42.9&36.9&51.2\\\\\n    LTDSEd~\\cite{VOT2019} &-&-&- & &40.5&38.2&43.0\\\\\n    Siam-LTD~\\cite{vot2020} &-&-&- & &37.6&34.2&41.8\\\\\n    SiamM-Ds~\\cite{VOT2019} &-&-&- & &33.6&26.4&46.3\\\\\n    CA3DMS~\\cite{ca3dms} &-&-&- & &22.3&22.8&21.8\\\\\n    DiMP~\\cite{DiMP} &54.3&70.3&73.1 & &-&-&-\\\\\n    ATOM~\\cite{ATOM} &50.5&59.8&68.8 & &-&-&-\\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n  }\n  \\vspace{-3mm}\n\\end{table}\n\n\\textit{TNL2K.}\nTNL2K~\\cite{TNL2K} is a large-scale benchmark for vision-language tracking. TNL2K comprises 2,000 video sequences, with 1,300 for training and 700 for testing. Each video provides a language annotation along with bounding box annotation to indicate the target. In Sec.~\\ref{subsec:sotav1}, we have evaluated our SeqTrack on TNL2K without utilizing the language modality. Here, we evaluate our multi-modal SeqTrackv2 on TNL2K using the language modality. As reported in Tab.~\\ref{tab-sota-rgbl}, with the language modality, our SeqTrackv2 models get superior performance. All four SeqTrackv2 models perform better than previous vision-language trackers. SeqTrackv2-L256 obtains the best  AUC score of 62.7 \\%, surpassing previous state-of-the-art tracker JointNLT by 5.8\\%.\n\n\\textit{OTB99.}\nOTB99~\\cite{TNLS} is a small-scale benchmark for vision-language tracking.  As reported in Tab.~\\ref{tab-sota-rgbl}, although the advantage is not be as significant as on other benchmarks, our method demonstrates competitive performance when compared with customized vision-language trackers. It is noteworthy that small-scale benchmarks are prone to overfitting, yet our models use same parameters and hyperparameters with other benchmarks.\n\n\\begin{table}[t]\\normalsize\n    \\caption{State-of-the-art comparisons on RGB+Event tracking}\n\\label{tab-sota-rgbe}\n\\vspace{-2mm}\n  \\centering\n\\resizebox{1\\linewidth}{!}{\n  \\setlength{\\tabcolsep}{10mm}{\n    \\small\n    \\begin{tabular}{l|cc}\n    \\toprule\n    \\multirow{2}*{Method} & \\multicolumn{2}{c}{VisEvent~\\cite{visevent}}\\\\\n        \\cline{2-3}\n & AUC & P \\\\\n    \\midrule[0.5pt]\n    SeqTrackv2-L384 &\\textbf{\\textcolor{cRed}{63.4}}&\\textbf{\\textcolor{cRed}{80.0}}\\\\\n    SeqTrackv2-L256 &\\textcolor{blue}{63.0}&\\textcolor{blue}{79.4}\\\\\n    SeqTrackv2-B384  &\\textcolor{cGreen}{62.2}&\\textcolor{cGreen}{79.3}\\\\\n    SeqTrackv2-B256 &61.2&78.2 \\\\\n    \\midrule[0.1pt]\n    ViPT~\\cite{vipt} &59.2&75.8\\\\\n    Un-Track~\\cite{untrack} &58.9 &75.5\\\\\n    OSTrack~\\cite{ostrack} &53.4&69.5\\\\\n    SiamRCNN\\_E~\\cite{SiamRCNN} &49.9&65.9 \\\\\n    TransT\\_E~\\cite{transt} &47.4&65.0\\\\\n    ProTrack~\\cite{protrack} &47.1&63.2\\\\\n    LTMU\\_E~\\cite{LTMU} &45.9&65.5 \\\\\n    PrDiMP\\_E~\\cite{PrDiMP} &45.3&64.4 \\\\\n    STARK\\_E~\\cite{Stark} &44.6&61.2 \\\\\n    MDNet\\_E~\\cite{MDNet} &42.6&66.1 \\\\\n    SiamCar\\_E~\\cite{Stark} &42.0&59.9 \\\\\n    VITAL\\_E~\\cite{VITAL} &41.5&64.9 \\\\\n    ATOM\\_E~\\cite{ATOM} &41.2&60.8 \\\\\n    SiamBAN\\_E~\\cite{SiamBAN} &40.5&59.1 \\\\\n    SiamMask\\_E~\\cite{SiamMask} &36.9&56.2 \\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n  }\n\\end{table}\n\n\\begin{table}\\footnotesize\n    \\caption{State-of-the-art comparisons on RGB+Language tracking}\n\\label{tab-sota-rgbl}\n\\vspace{-2mm}\n  \\centering\n\\resizebox{\\linewidth}{!}{\n  \\setlength{\\tabcolsep}{4.5mm}{  \n  \\small\n  \\begin{tabular}{l| cc c cc}\n    \\toprule\n    \\multirow{2}*{Method} & \\multicolumn{2}{c}{TNL2K~\\cite{TNL2K}} & & \\multicolumn{2}{c}{OTB99}~\\cite{TNLS}\\\\\n    \\cline{2-3}\n    \\cline{5-6}\n    & AUC&P && AUC&P\\\\\n    \\midrule[0.5pt]\n    SeqTrackv2-L384 &\\textcolor{blue}{62.4}&\\textbf{\\textcolor{cRed}{66.1}} & &71.4&\\textcolor{cGreen}{93.6}\\\\\n    SeqTrackv2-L256 &\\textbf{\\textcolor{cRed}{62.7}}&\\textcolor{blue}{66.0} & &70.3&91.0\\\\\n    SeqTrackv2-B384  &\\textcolor{cGreen}{59.4}&\\textcolor{cGreen}{62.6} & &\\textcolor{cGreen}{71.8}&93.4\\\\\n    SeqTrackv2-B256 &57.5&59.7 & &71.2&\\textcolor{blue}{93.9} \\\\\n    \\midrule[0.1pt]\nJointNLT~\\cite{JointNLT}\t&56.9 &58.1 & &65.3&85.6 \\\\\nDecoupleTNL\\cite{DecoupleTNL}&56.7&56.0 & &\\textcolor{blue}{73.8}&\\textbf{\\textcolor{cRed}{94.8}}\\\\\nZhao \\emph{et al.}~\\cite{zhao2023transformervision-languagetracking}\t&56.0&- & &69.9&91.2 \\\\\nVLT$_{TT}$~\\cite{VLTTT}\t&53.1&53.3 & &\\textbf{\\textcolor{cRed}{76.4}} &93.1 \\\\\nCapsuleTNL~\\cite{CapsuleTNL}\t&- &- & &71.1&92.4\\\\\nLi \\emph{et al.}\\cite{CTRTNL} &44.0&45.0 & &69.0&91.0\\\\\nTNL2K-2~\\cite{TNL2K}\t&42.0&42.0 & &68.0&88.0 \\\\\nSNLT~\\cite{SNLT}\t&27.6&41.9 & &66.6&80.4 \\\\\nGTI~\\cite{GTI} &-&- & &58.1&73.2\\\\\nTransVG~\\cite{transvg} &26.1&28.9 & &-&- \\\\\nFeng \\emph{et al.}~\\cite{feng2019robust}\t&25.0&27.0 & &67.0&73.0\\\\\nRTTNLD~\\cite{RTTNLD}\t&25.0&27.0 & &61.0&79.0 \\\\\nWang \\emph{et al.}~\\cite{wang2018describe}\t&-&-  & &65.8&89.1 \\\\\nTNLS~\\cite{TNLS}\t&-&-  & &55.3&72.3 \\\\\nOneStage-BERT~\\cite{onestagebert} &19.8&-  & &24.6&32.2 \\\\\nLBYL-BERT~\\cite{onestagebert} &18.3&-  & &20.7&26.0\\\\\n  \\bottomrule\n\\end{tabular}\n}}\n\\vspace{-3mm}\n\\end{table}\n\n\\subsection{Ablation and Analysis of SeqTrack. }\n\\label{subsec:ablation}\n\nWe use SeqTrack-B256 without the online template update mechanism as the baseline model in this ablation study. The result of the baseline is reported in Tab.~\\ref{tab-ablation} (\\#1).\n\n\\textit{Autoregressive v.s. Bidirectional.} Our method generates a sequence in an \\emph{autoregressive} manner, which predicts the bounding box values one by one. We contrast this autoregressive method with another \\emph{bidirectional} method that predicts all coordinate values simultaneously. In the bidirectional method, the input sequence comprises four special tokens akin to the \\texttt{start} token. The decoder receives the sequence and predicts the four coordinates simultaneously in a batch. The causal attention mask is removed, enabling tokens to attend to each other bidirectionally. As demonstrated in Tab.~\\ref{tab-ablation} (\\#2), the bidirectional method significantly underperforms compared to the autoregressive one, highlighting the importance of maintaining the causal relationship between tokens for sequence modeling in tracking.\n\n\\begin{table}[t]\n\\definecolor{purple(x11)}{rgb}{0.63, 0.36, 0.94}\n\\definecolor{yellow(munsell)}{rgb}{1.0,0.988, 0.957}\n\\definecolor{green(colorwheel)(x11green)}{rgb}{0.0, 1.0, 0.0}\n\\definecolor{pink}{rgb}{1.0, 0.85, 0.85}\n\\centering\n\\caption{{Ablation Study on LaSOT~\\cite{LaSOT} and GOT-10k~\\cite{GOT10K}}. \nWe use \\textcolor{gray}{gray},  \\textcolor{green!50}{green}, \\textcolor{blue!50}{purple},  \\textcolor{yellow!60}{yellow}, and \\textcolor{pink}{pink} colors to denote baseline setting, framework, input to encoder, input to decoder, and tracking prior integration, respectively. $\\Delta$ denotes the performance change (averaged over benchmarks) compared with the baseline.\n}\n\\label{tab-ablation}\n\\vspace{-2mm}\n\\small\n\\resizebox{0.95\\linewidth}{!}{\n\\setlength{\\tabcolsep}{1mm}{\n\\begin{tabular}{l|c|cc|c}\n\\toprule\n\\# & Method &LaSOT &GOT-10k &$\\Delta$\\\\\n\\midrule\n\\rowcolor{gray!15}\n1 & Baseline &69.2 &73.7 &--\\\\\n\\rowcolor{green!5}\n2 &Autoregressive $\\rightarrow$ Bidirectional &64.8 &72.4 &\\textbf{-2.9}\\\\  \n\\rowcolor{blue!5}\n3 &Joint $\\rightarrow$ Separate &62.0 &66.1 &\\textbf{-7.4}\\\\\n\\rowcolor{yellow!5}\n4 &$[x,$$y,$$w,$$h]$ $\\rightarrow$ $[w,$$h,$$x,$$y]$ &67.1 &71.8 &\\textbf{-2.0}\\\\  \n\\rowcolor{yellow!5}\n5 &$[x,$$y,$$w,$$h]$ $\\rightarrow$ $[x_{min},$$y_{min},$$x_{max},$$y_{max}]$ &68.3 &70.7 &\\textbf{-2.0}\\\\ \n\\ignore{\n\\rowcolor{yellow!5}\n6 &Search $\\rightarrow$ Search-Template &69.6 &73.3 &\\textbf{-0.0}\\\\\n\\rowcolor{yellow!5}\n7 &Search $\\rightarrow$ Average &69.2 &72.2 &\\textbf{-0.7}\\\\}\n\\rowcolor{yellow!5}\n6 &Concat of Search-Template &69.6 &73.3 &\\textbf{-0.0}\\\\\n\\rowcolor{yellow!5}\n7 &Avg. of Search-Template &69.2 &72.2 &\\textbf{-0.8}\\\\\n\\rowcolor{red!5}\n8 &$+$ Likelihood-based Online Update &69.9 &76.1  &\\textbf{+1.6}\\\\\n\\rowcolor{red!5}\n9 &$+$ Naive Online Update &69.3 &73.1 &\\textbf{-0.3}\\\\\n\\rowcolor{red!5}\n10 &$-$ Window Penalty &68.8 &73.1 &\\textbf{-0.5}\\\\\n\\bottomrule\n\\end{tabular}\n}}\n\\vspace{-2mm}\n\\end{table}\n\n\\begin{figure}[t!]\n\\begin{center}\n\\includegraphics[width=1\\linewidth]{figures/bins.pdf}\n\\end{center}\n    \\vspace{-6mm}\n   \\caption{Influence of the number of quantization bins}\n\\label{fig:bins}\n\\end{figure}\n\n\\textit{Inputs of the Encoder.}\nWe evaluate various input strategies for the encoder. As outlined in Sec.~\\ref{subsec:architecture}, the template and the search region are inputted into the encoder together. \nSubsequently, the encoder extracts their visual features in a \\emph{joint} way. We contrast this with a \\emph{separate} approach: akin to Siamese methods \\cite{SiameseFC,SiameseRPN}, two weight-sharing encoders independently extract the features from the template and the search images, and then the extracted features are fed into the decoder. \nTab.~\\ref{tab-ablation} (\\#3) shows that the separate feature extraction method lags behind the joint one by 7.2\\% in AUC on the LaSOT benchmark. \nThe underlying reason might be that the joint method enables the encoder to learn improved feature correspondence between the template and search images.\n\n\\begin{figure}[t!]\n\\begin{center}\n\\includegraphics[width=1\\linewidth]{figures/att_map.pdf}\n\\end{center}\n    \\vspace{-4mm}\n   \\caption{Decoder's cross attention to visual features when generating tokens. The first column is the search region image, and the second to last columns are the cross attention maps when $x,y,w,h$ are generated, respectively.}\n\\label{fig:attmap}\n    \\vspace{-3mm}\n\\end{figure}\n\n\\begin{table}[t]\\normalsize\n\\centering\n  \\caption{Ablation study of the template size. The baseline setting is marked in \\colorbox{gray!15}{gray}.}\n  \\label{tab-ablation_template}\n      \\vspace{-3mm}\n\\resizebox{1\\linewidth}{!}{\n  \\setlength{\\tabcolsep}{3mm}{\n     \\small\n    \\begin{tabular}{c|cc|cc}\n    \\toprule\n    \\#&Expansion Factor&Resolution&LaSOT~\\cite{LaSOT}&GOT-10k~\\cite{GOT10K}\\\\\n    \\midrule\n    \\rowcolor{gray!15}\n    1 &4&$256$$\\times$$256$&69.2&73.7 \\\\\n    2 &2&$128$$\\times$$128$&68.5&72.7 \\\\\n    3 &2&$256$$\\times$$256$&68.7&72.6 \\\\\n    \\bottomrule\n    \\end{tabular}\n  }\n}\n  \\vspace{-3mm}\n\\end{table}\n\n\\begin{table}[t]\\normalsize\n\\centering\n  \\caption{Ablation study of the number of decoder blocks. Avg. denotes the performance averaged over benchmarks. The baseline setting is marked in \\colorbox{gray!15}{gray}.}\n  \\label{tab-ablation_block}\n    \\vspace{-3mm}\n\\resizebox{1\\linewidth}{!}{\n  \\setlength{\\tabcolsep}{4mm}{\n     \\small\n    \\begin{tabular}{l|c>{\\columncolor{gray!15}}ccccc}\n    \\toprule\n    &1&2&3&4&5&6\\\\\n    \\midrule\n    LaSOT~\\cite{LaSOT}  &69.5&69.2&69.3&69.3&68.7&68.8\\\\\n    GOT-10k~\\cite{GOT10K}  &72.9&73.7&73.1&74.0&73.4&74.5\\\\\n    Avg.  &71.2&71.5&71.2&71.7&71.1&71.7\\\\\n    \\bottomrule\n    \\end{tabular}\n  }\n}\n  \\vspace{-3mm}\n\\end{table}\n\n\\textit{Inputs of the Decoder.} \nWe first compare different input sequences to the decoder. Tab.~\\ref{tab-ablation} (\\#4 and \\#5) present two alternative formats of sequence:\n1) $[$$w, $$h, $$x, $$y]$, where the model first generates object's scale and then its position coordinates;\nand 2) $[x_{min}, $$y_{min}, $$x_{max}, $$y_{max}]$, where $[x_{min},$$y_{min}]$ denotes the top-left corner and $[x_{max},$$y_{max}]$ denotes the bottom-right one.\nWe observe that the default format $[x,$$y,$$w,$$h]$ yields the best result, as it aligns with human prior knowledge: first localizing object position and then estimating its scale.\nWe also explore the influence of the number of quantization bins $n_{bins}$, as shown in Fig.~\\ref{fig:bins}. Increasing the number of bins $n_{bins}$ can improve the performance because the quantization error is reduced accordingly. The performance becomes saturated when $n_{bins}$ is larger than $4,000$, thus we set $n_{bins}$ to $4,000$.\n\nMoreover, we compare different input visual features to the decoder. By default, we only feed the features of the search image into the decoder, as illustrated in Fig.~\\ref{fig:framework}. Here, we compare it with two other alternatives: 1) the feature concatenation of the search and template images, and 2) the \\emph{averaged} feature of the search and template images. For the first method, all the features are fed into the decoder. For the second method, the features are first projected into a 1D embedding and then fed into the decoder. From Tab.~\\ref{tab-ablation} (\\#6 and \\#7), we observe that these two methods perform comparably to the default method that only uses search image features. Overall, the decoder does not appear to be sensitive to the form of input visual features.\n\n\\textit{Prior Knowledge Integration.} \nFor the online update, our method utilizes the likelihood of generated tokens to select reliable templates.\nAs reported in Tab.~\\ref{tab-ablation} (\\#8), our method improves the tracking performance.\nWe also explore a naive online update method, where the dynamic template is updated without selection.\nTab.~\\ref{tab-ablation} (\\#9) demonstrates that this method obtains inferior performance. These results suggest that selecting reliable templates with our likelihood-based method is effective. \nFor the window penalty, Tab.~\\ref{tab-ablation} (\\#10) demonstrates that the performance degrades without it.\n\n\\textit{Visualization of Cross Attention Map.}\nTo gain insights into how SeqTrack ``reads out\" the target state, we visualize the cross attention (averaged over heads) of the last decoder block.\nFig.~\\ref{fig:attmap} displays cross-attention maps as the model generates tokens. When generating the first token $x$, the attention appears relatively diverse.\nAs subsequent tokens are generated, attention quickly focuses on the target object.\nAttention is more focused on key features, such as the person's arm and the zebra's tail when generating $x$ and $w$, and the foot when generating $y$ and $h$.\n\n\\textit{Template Size.}\nAs described in Sec. \\ref{subsec:representation}, in prior trackers~\\cite{SiameseFC,SiameseRPN,transt,Stark}, the size of template images is typically smaller than that of search images. In contrast, we use the same size for the template and search images. \nHere we compare different settings of the template size. \nConcretely, there are two steps to generate the template image: 1) expanding the target's bounding box by a certain factor to obtain a template region, and 2) resizing the template region to a certain resolution.\nIn our baseline setting, the expansion factor is $4$, and the resolution is $256$$\\times$$256$.\nThe result of the baseline setting is shown in Tab.~\\ref{tab-ablation_template} (\\#1).\nIn Tab.~\\ref{tab-ablation_template} (\\#2), we scale down the baseline expansion factor and resolution, like prior trackers~\\cite{SiameseFC,transt}, \\emph{i.e.}, resolution of $128$$\\times$$128$ and expansion factor of $2$. To exclude the effect of the resolution, we additionally compare a resolution-aligned setting with the expansion factor of $2$ and the resolution of $256$$\\times$$256$, in Tab.~\\ref{tab-ablation_template} (\\#3).\nOur baseline method gets the best result, demonstrating that adding more background information in the template is helpful for improving tracking performance.\n\n\\textit{Number of Decoder Blocks.}\nWe compare the number of transformer blocks in the decoder. Table~\\ref{tab-ablation_block} shows that the model is not sensitive to this number. An underlying reason is that the information in the decoder is concise, thus a small capacity is sufficient.\n\n\\begin{table}[t]\n\\definecolor{purple(x11)}{rgb}{0.63, 0.36, 0.94}\n\\definecolor{yellow(munsell)}{rgb}{1.0,0.988, 0.957}\n\\definecolor{green(colorwheel)(x11green)}{rgb}{0.0, 1.0, 0.0}\n\\definecolor{pink}{rgb}{1.0, 0.85, 0.85}\n\\centering\n\\caption{Ablation Study of SeqTrackv2 on multi-modal benchmarks. \nWe use \\textcolor{gray}{gray},  \\textcolor{green!50}{green}, \\textcolor{blue!50}{purple},  \\textcolor{yellow!60}{yellow}, and \\textcolor{pink}{pink} colors to denote baseline setting, training tasks, instruction, training method, and prompting method, respectively. $\\Delta$ denotes the performance change (averaged over benchmarks) compared with the baseline.}\n\\label{tab-ablationv2}\n\\vspace{-2mm}\n\\small\n\\resizebox{1\\linewidth}{!}{\n\\setlength{\\tabcolsep}{0.2mm}{\n\\begin{tabular}{l|c|cccc|c}\n\\toprule\n\\# & Method &LasHeR &VOT-RGBD22 &VisEvent &TNL2K &$\\Delta$\\\\\n\\midrule\n\\rowcolor{gray!15}\n1 & Baseline &55.8 &74.4 &61.2 &57.5 &--\\\\\n\\rowcolor{green!5}\n2 & Multi-Task $\\rightarrow$ Single-Task &57.2 &72.7  &61.2 &58.4 &\\textbf{+0.2}\\\\  \n\\rowcolor{blue!5}\n3 & $-$ Encoder Task Prompt &55.0 &74.8   &61.5 &57.3 &\\textbf{-0.1}\\\\  \n\\rowcolor{blue!5}\n4 & $-$ Decoder Task Prompt &54.8 &74.3 &61.1 &57.7 &\\textbf{-0.3}\\\\  \n\\rowcolor{yellow!5}\n5 & Add &46.2 &63.5  &52.4 &52.8 &\\textbf{-8.5}\\\\\n\\rowcolor{yellow!5}\n6 & Attention &50.6 &75.2  &60.6 &58.1 &\\textbf{-1.1}\\\\\n\\rowcolor{yellow!5}\n7 & Concat &43.4 &66.8  &53.0 &56.0 &\\textbf{-7.3}\\\\\n\\rowcolor{red!5}\n8 & Full Fine-Tuning &53.1 &71.4  &61.1 &60.6 &\\textbf{-0.7}\\\\ \n\\rowcolor{red!5}\n9 & Equal $\\rightarrow$  Proportional Videos  &55.4 &74.1  &61.1 &60.4 &\\textbf{+0.5}\\\\\n\\rowcolor{red!5}\n10 & Equal $\\rightarrow$  Proportional Images  &53.2 &74.1  &59.3 &59.5 &\\textbf{-0.7}\\\\\n\n\\bottomrule\n\\end{tabular}\n}}\n\\vspace{-3mm}\n\\end{table}\n\n\\subsection{Ablation and Analysis of SeqTrackv2. }\n\\label{subsec:ablationv2}\n\nWe use SeqTrackv2-B256 as the baseline model in this ablation study. The result of the baseline model is reported in Tab.~\\ref{tab-ablationv2} (\\#1).\n\n\\textit{Multi-Task v.s. Single-Task.}\nOur models are jointly trained on the four multi-modal tracking tasks, enabling them to address these tasks using a unified model and parameter set. We investigate the disparity between this unified model and individual single-task models. For the single-task models, we train an individual model for each multi-modal tracking task. As reported in Tab.~\\ref{tab-ablationv2} (\\#2), the single-task models yields only a marginal improvement of 0.2\\% on average compared to the unified multi-task model. Overall, our unified model is able to perform on-par with individual single-task models. \n\n\\textit{Task-prompt Token.}\nWe incorporate a task-prompt token to supply more task-centric information to the model, as described in Sec.~\\ref{sec-taskprompt}. Here we explore the impact of the task-prompt token. \nIn Tab.~\\ref{tab-ablationv2} (\\#3), we eliminate the task-prompt token from the encoder, leading to a slight performance dip. This could be attributed to the encoder's visual features already possessing substantial task information, reducing the need for additional supplementation. Despite this, the task-prompt token does contribute positively to performance. In Tab.~\\ref{tab-ablationv2} (\\#4), upon removing the decoder's task-prompt token, we observe a larger performance decline, underscoring the importance of introducing task-specific information during the decoding phase.\nIn summary, the task-prompt token enhances performance, with its need being greater in the decoding process than in encoding.\n\n\\begin{table}[t]\\normalsize\n\\centering\n  \\caption{Ablation study of the low-rank dimension. Avg. denotes the performance averaged over benchmarks. The baseline setting is marked in \\colorbox{gray!15}{gray}.}\n  \\label{tab-ablation_d}\n    \\vspace{-2mm}\n\\resizebox{1\\linewidth}{!}{\n  \\setlength{\\tabcolsep}{4mm}{\n     \\small\n    \\begin{tabular}{l|cc>{\\columncolor{gray!15}}ccc}\n    \\toprule\n    &8&16&32&64&128\\\\\n    \\midrule\n    LasHeR~\\cite{lasher}  &53.6&54.2&55.8&55.3&55.1\\\\\n    VOT-RGBD22~\\cite{vot2022} \n    &74.7&74.8&74.4&73.8&74.7\\\\\n    VisEvent~\\cite{visevent} &60.9&61.3&61.2&60.5&61.2\\\\\n    TNL2K~\\cite{TNL2K} &57.7&57.3&57.5&57.8&58.3\\\\\n    Avg.  &61.7&61.9&62.2&61.9&62.3\\\\\n    \\bottomrule\n    \\end{tabular}\n  }\n}\n  \\vspace{-3mm}\n\\end{table}\n\n\\textit{Unified Interface.}\nAs detailed in Sec.~\\ref{sec-interface}, we integrate the auxiliary-modal sequence into the visual features in a low-rank manner, that is,  by projecting both the auxiliary-modal and visual feature sequences from dimension $D$ to a lower dimension $d$. Here, we investigate the impact of the low-rank dimension $d$ setting. According to the results presented in Tab.~\\ref{tab-ablation_d}, a too-low value of $d$ leads to diminished performance, but performance gains plateau once $d$ reaches 32. Thus, for considerations of efficiency and model size, we opted for $32$.\n\nWe have also investigated alternative choices for the unified interface. As shown in Table~\\ref{tab-ablationv2} (\\#5), we attempt to add the auxiliary-modal sequence directly into the encoder's visual features. In \\#6, we explore the use of cross-attention modules to integrate the auxiliary-modal sequence. Entry \\#7 documents our attempt to directly concatenate the auxiliary-modal sequence and the RGB-image patches sequence, and then input them into the encoder. Ultimately, we find that the default unified interface delivers superior performance.\n\n\\textit{Training Method.}\nIn Section~\\ref{sec-train}, we detail our training methodology, wherein we keep the parameters of the base SeqTrack model frozen while exclusively training those of the newly-introduced unified interface alongside the task-prompt tokens. Additionally, we investigate the consequences of full fine-tuning. The results presented in Table~\\ref{tab-ablationv2} (\\#8) indicate that while full fine-tuning yields competitive performance, it remains inferior to our default training method. This discrepancy may be attributed to the fact that full fine-tuning disrupts the prior knowledge acquired from RGB tracking data.\n\nIn our default multi-task training strategy, we evenly mix the data from four multi-modal tasks. Furthermore, we explore allocating training data to different tasks based on the proportion of videos or images. The results corresponding to these strategies are shown in Tab.~\\ref{tab-ablationv2} (\\#9 and \\#10). The results suggest that proportional strategies offer advantage for the task with a substantial amount of data (TNL2K, RGB+Language tracking). However, this advantage comes at the cost of the performance of tasks with smaller datasets. In contrast, the evenly-mixed strategy leads to more stable performance across different tasks.\n  \n\n\\section{Conclusion}\n\\label{sec:conclusion}\nThis work proposes SeqTrack, a new sequence-to-sequence tracking framework for visual object tracking, and its advanced iteration, SeqTrackv2, which extends the framework for unified multi-modal tracking. SeqTrack uses a simple encoder-decoder transformer architecture, getting rid of complicated head networks and loss functions. SeqTrackv2 seamlessly integrates multi-modal tracking tasks into a unified model and parameter set. Through extensive experiments, we demonstrate the effectiveness of both SeqTrack and SeqTrackv2, achieving competitive performance compared to state-of-the-art trackers. We hope this work could catalyze more compelling research on sequence learning for visual tracking.\n\n\\emph{Limitation.}\nOne limitation of SeqTrack is that, despite achieving competitive performance, it is difficult to handle the cases when objects move out-of-view or are occluded by distractors, because the method does not have explicit re-detection modules.\nMoreover, while SeqTrackv2 is constructed to unify four existing multi-modal tracking tasks, its effectiveness across new, untested modalities remains uncertain. A more promising avenue involves swiftly adapting the model to novel modalities with a few examples, a direction we intend to explore in future research.\n\n\\vfill\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2203.11082v2.tex",
        "arXiv-2207.09603v2.tex",
        "arXiv-2304.14394v3.tex"
    ],
    "group_id": "group_63",
    "response": "### Title: Advanced Transformer-Based Visual Object Tracking Frameworks: MixFormer, AiATrack, and SeqTrackv2\n\n### Introduction\nVisual object tracking is a critical task in computer vision that involves estimating the position of a target object in a video sequence, given its initial location. Over the years, this task has seen a variety of approaches, from traditional methods such as correlation filters and Kalman filters to more recent deep learning-based techniques, including Siamese networks and convolutional neural networks (CNNs). However, these methods often rely on multi-stage pipelines that separate feature extraction, target information integration, and bounding box estimation, leading to complex architectures and potential inefficiencies. Recently, the introduction of transformer architectures has shown promise in unifying these processes and improving tracking performance. Despite these advancements, current transformer-based trackers still face challenges such as handling object deformations, occlusions, and confusion with similar objects. This summary explores three recent papers that propose novel transformer-based frameworks for visual tracking: MixFormer, AiATrack, and SeqTrackv2. These frameworks aim to simplify the tracking pipeline, enhance feature extraction and integration, and improve overall tracking accuracy and efficiency.\n\n### MixFormer: End-to-End Tracking with Iterative Mixed Attention\nMixFormer, introduced by Cui et al., is a compact tracking framework designed to unify feature extraction and target information integration within a transformer-based architecture. The core innovation of MixFormer lies in its Mixed Attention Module (MAM), which performs dual attention operations: self-attention to extract features from the target template and search area, and cross-attention to facilitate communication between them. This simultaneous processing scheme enables the extraction of target-specific discriminative features and extensive communication between the target and search areas, leading to a more accurate and efficient tracking system. The MAM backbone is built by stacking Patch Embedding and MAM layers, followed by a localization head to produce the target bounding box. To handle object deformations, MixFormer employs an asymmetric attention scheme and a score prediction module to select high-quality templates.\n\n#### Key Innovations:\n- **Mixed Attention Module (MAM):** Combines self-attention and cross-attention to extract features and integrate target information simultaneously.\n- **Asymmetric Attention Scheme:** Prunes unnecessary cross-attention operations to reduce computational cost and improve efficiency.\n- **Score Prediction Module (SPM):** Selects reliable templates to ensure high-quality tracking.\n\n#### Experimental Results:\nMixFormer achieves state-of-the-art performance on five benchmarks: LaSOT, TrackingNet, VOT2020, GOT-10k, and UAV123. Specifically, MixFormer-L achieves an NP score of 79.9% on LaSOT, 88.9% on TrackingNet, and an EAO score of 0.555 on VOT2020. The framework is trained on a combination of datasets including TrackingNet, LaSOT, GOT-10k, and COCO, and is evaluated on benchmarks with diverse challenges such as fast motion, occlusion, and background clutter.\n\n### AiATrack: Attention in Attention for Transformer Visual Tracking\nAiATrack, proposed by Gao et al., introduces an attention in attention (AiA) module to enhance the attention mechanism used in transformer-based trackers. The AiA module refines the correlation map by seeking consensus among all correlation vectors, effectively enhancing appropriate correlations and suppressing erroneous ones. This refinement process is applied to both self-attention and cross-attention blocks, improving the overall tracking performance. AiATrack also employs a streamlined transformer framework that includes efficient feature reuse and target-background embeddings to distinguish the target from the background while preserving contextual information. This design allows for the effective use of temporal references without additional computational cost.\n\n#### Key Innovations:\n- **Attention in Attention (AiA) Module:** Refines correlations by seeking consensus among all correlation vectors, enhancing the quality of attention maps.\n- **Efficient Feature Reuse:** Allows for the reuse of cached features to efficiently update short-term references.\n- **Target-Background Embeddings:** Introduces learnable embeddings to distinguish the target from the background and provide contextual cues.\n\n#### Experimental Results:\nAiATrack achieves state-of-the-art performance on six benchmarks, including LaSOT, TrackingNet, GOT-10k, NfS30, OTB100, and UAV123, while maintaining real-time speed. For instance, on LaSOT, AiATrack achieves an AUC score of 69.0% and P score of 73.8%, surpassing the previous best tracker KeepTrack by 1.9% in AUC and 3.6% in precision. On TrackingNet, it achieves an AUC score of 82.7% and P score of 87.8%, outperforming all previously published trackers. The tracker is trained on datasets including LaSOT, TrackingNet, GOT-10k, and COCO, and is evaluated on benchmarks with varying levels of complexity and diverse tracking scenarios.\n\n### SeqTrackv2: Unified Sequence-to-Sequence Learning for Single- and Multi-Modal Visual Object Tracking\nSeqTrackv2, proposed by Chen et al., extends the concept of sequence-to-sequence learning to visual object tracking, treating the task as a sequence generation problem. The framework employs a basic encoder-decoder transformer architecture, where the encoder extracts visual features from input video frames and the decoder generates bounding box sequences autoregressively. SeqTrackv2 further introduces a unified interface and task-prompt tokens to manage multi-modal tracking tasks using a single model and parameter set. This approach simplifies the tracking framework and showcases superior performance across 14 benchmarks spanning five single- and multi-modal tracking tasks.\n\n#### Key Innovations:\n- **Sequence-to-Sequence Learning:** Converts bounding box coordinates into a sequence of discrete tokens and trains the model to generate these tokens in an autoregressive manner.\n- **Unified Interface:** Standardizes various auxiliary modalities into a unified sequence format and integrates them into the visual features of the encoder.\n- **Task-Prompt Tokens:** Specifies the current multi-modal task, guiding the model's processing and improving task-specific performance.\n\n#### Experimental Results:\nSeqTrackv2 achieves new state-of-the-art performance on multiple benchmarks, including LaSOT, TrackingNet, GOT-10k, and TNL2K. For example, on LaSOT, SeqTrackv2-L384 achieves an AUC score of 72.5%, surpassing the recent best tracker SwinTrack by 1.2%. On TrackingNet, it achieves an AUC score of 85.5%, outperforming all previous trackers. The models are trained on datasets such as LaSOT, TrackingNet, GOT-10k, and COCO, and are evaluated on benchmarks with diverse tracking challenges and multi-modal inputs.\n\n### Commonalities and Innovations\nAll three frameworks leverage transformer architectures to simplify the tracking pipeline and improve performance. However, they each introduce unique innovations to address specific challenges in visual tracking:\n- **MixFormer** utilizes a Mixed Attention Module (MAM) to unify feature extraction and target information integration, reducing computational cost and improving efficiency.\n- **AiATrack** introduces an Attention in Attention (AiA) module to refine correlations and suppress noise, enhancing the quality of attention maps.\n- **SeqTrackv2** extends sequence-to-sequence learning to visual tracking, treating the task as a generation problem and introducing a unified interface for multi-modal tasks.\n\n### Comparison of Results\nThe three frameworks are evaluated on a range of benchmarks, including LaSOT, TrackingNet, GOT-10k, and UAV123. Table~\\ref{tab:comparison} summarizes the performance of each framework on these benchmarks. MixFormer-L achieves the highest NP score on LaSOT and TrackingNet, while AiATrack and SeqTrackv2 perform competitively on other benchmarks. SeqTrackv2 particularly excels on multi-modal tracking tasks, setting new state-of-the-art performance on RGB+Thermal, RGB+Depth, RGB+Event, and RGB+Language benchmarks.\n\n\\begin{table*}[ht]\n\\centering\n\\begin{tabular}{l|ccc|ccc|ccc|cc}\n\\hline\n\\multirow{2}{*}{Method} & \\multicolumn{3}{c|}{LaSOT} & \\multicolumn{3}{c|}{TrackingNet} & \\multicolumn{3}{c|}{GOT-10k} & \\multicolumn{2}{c}{UAV123} \\\\\n\\cline{2-12}\n & AUC & P$_{\\text{Norm}}$ & P & AUC & P$_{\\text{Norm}}$ & P & AO & SR$_{0.75}$ & SR$_{0.5}$ & AUC & P \\\\\n\\hline\nMixFormer-L & 70.1 & 79.9 & 76.3 & 83.9 & 88.9 & 83.1 & 75.6 & 85.7 & 72.8 & 69.5 & 91.0 \\\\\nAiATrack & 69.0 & 79.4 & 73.8 & 82.7 & 87.8 & 80.4 & 69.6 & 63.2 & 80.0 & 70.4 & 91.8 \\\\\nSeqTrackv2-L384 & 72.5 & 81.5 & 79.3 & 85.5 & 89.8 & 85.8 & 74.8 & 81.9 & 72.2 & 57.8 & 68.5 \\\\\n\\hline\n\\end{tabular}\n\\caption{Comparison of performance on LaSOT, TrackingNet, GOT-10k, and UAV123 benchmarks.}\n\\label{tab:comparison}\n\\end{table*}\n\n### Conclusion\nThe three papers present innovative transformer-based frameworks for visual object tracking, each addressing different aspects of the tracking pipeline. MixFormer simplifies the pipeline by unifying feature extraction and target information integration, AiATrack refines the attention mechanism to enhance tracking accuracy, and SeqTrackv2 extends sequence-to-sequence learning to handle multi-modal tracking tasks. These frameworks demonstrate notable improvements over existing trackers on various benchmarks, showcasing the potential of transformer architectures in visual tracking. Future research could focus on adapting these frameworks to handle out-of-view objects and novel modalities, further enhancing their versatility and performance.\n\n### Future Research Directions\n- **Handling Out-of-View Objects:** Incorporating explicit re-detection modules to address cases when objects move out of view or are occluded.\n- **Adaptation to Novel Modalities:** Investigating methods to swiftly adapt the models to new modalities with limited training data.\n- **Real-Time Performance Optimization:** Further optimizing the frameworks for real-time performance on edge devices with limited computational resources.\n- **Multi-Object Tracking:** Extending the frameworks to handle multiple objects in a single video sequence, addressing the limitations of current single-object trackers.\n\nThese advancements could lead to more robust and versatile visual tracking systems, capable of handling a wider range of scenarios and modalities."
}