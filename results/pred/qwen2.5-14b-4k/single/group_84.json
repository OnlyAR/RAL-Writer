{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{FSGAN: Subject Agnostic Face Swapping and Reenactment}\n\n\\begin{document}\n\n\\makeatletter\n\\let\\@oldmaketitle\\@maketitle% Store \\@maketitle\n\\renewcommand{\\@maketitle}{\\@oldmaketitle% Update \\@maketitle to insert...\n    \\centering{\\vspace{-5mm}\\includegraphics[width=.95\\linewidth,clip,trim = 0mm 0mm 0mm 0mm]{figures/teaser_01}}\n    \\captionof{figure}{{\\em Face swapping and reenactment.} Left: Source face swapped onto target. Right: Target video used to control the expressions of the face appearing in the source image. In both cases, our results appears in the middle. For more information please visit our website:~\\url{https://nirkin.com/fsgan}.}\\vspace{2mm}\n    \\label{fig:teaser}\n    }\n\\makeatother\n\n\\title{FSGAN: Subject Agnostic Face Swapping and Reenactment}\n\n\\author{Yuval Nirkin\\\\\nBar-Ilan University, Israel\\\\\n{\\tt\\small yuval.nirkin@gmail.com}\n\\and\nYosi Keller\\\\\nBar-Ilan University, Israel\\\\\n{\\tt\\small yosi.keller@gmail.com}\n\\and\nTal Hassner\\\\\nThe Open University of Israel, Israel\\\\\n{\\tt\\small talhassner@gmail.com}\n}\n\n\\maketitle\n\\ificcvfinal\\thispagestyle{empty}\\fi\n\n\\begin{abstract}\nWe present Face Swapping GAN (FSGAN) for face swapping and reenactment. Unlike previous work, FSGAN is subject agnostic and can be applied to pairs of faces without requiring training on those faces. To this end, we describe a number of technical contributions. We derive a novel recurrent neural network (RNN)--based approach for face reenactment which adjusts for both pose and expression variations and can be applied to a single image or a video sequence. For video sequences, we introduce continuous interpolation of the face views based on reenactment, Delaunay Triangulation, and barycentric coordinates. Occluded face regions are handled by a face completion network. Finally, we use a face blending network for seamless blending of the two faces while preserving target skin color and lighting conditions. This network uses a novel Poisson blending loss which combines Poisson optimization with perceptual loss. We compare our approach to existing state-of-the-art systems and show our results to be both qualitatively and quantitatively superior.\n\\end{abstract}\n\n\\section{Introduction}\n\n{\\em Face swapping} is the task of transferring a face from source to target image, so that it seamlessly replaces a face appearing in the target and produces a realistic result (Fig.~\\ref{fig:teaser} left). {\\em Face reenactment} (aka {\\em face transfer} or {\\em puppeteering}) uses the facial movements and expression deformations of a control face in one video to guide the motions and deformations of a face appearing in a video or image (Fig.~\\ref{fig:teaser} right). Both tasks are attracting significant research attention due to their applications in entertainment~\\cite{alexander2009creating,kemelmacher2016transfiguring,wolf2010eye}, privacy~\\cite{blanz2004exchanging,lin2012face,mosaddegh2014photorealistic}, and training data generation. \n\nPrevious work proposed either methods for swapping or for reenactment but rarely both. Earlier methods relied on underlying 3D face representations~\\cite{tran2017extreme} to transfer or control facial appearances. Face shapes were either estimated from the input image~\\cite{thies2016face2face,suwajanakorn2017synthesizing,nirkin2018face} or were fixed~\\cite{nirkin2018face}. The 3D shape was then aligned with the input images~\\cite{chang17fpn} and used as a proxy when transferring intensities (swapping) or controlling facial expression and viewpoints (reenactment). %These methods were not end-to-end trainable and required special care when completing occluded facial regions. \n\nRecently, deep network--based methods were proposed for face manipulation tasks. Generative adversarial networks (GANs)~\\cite{goodfellow2014generative}, for example, were shown to successfully generate realistic images of fake faces. Conditional GANs (cGANs)~\\cite{mirza2014conditional,isola2017image,wang2018pix2pixHD} were used to transform an image depicting real data from one domain to another and inspired multiple face reenactment schemes~\\cite{pumarola2018ganimation,wayne2018reenactgan,sanchez2018triple}. Finally, the DeepFakes project~\\cite{DeepFakes} leveraged cGANs for face swapping in videos, making swapping widely accessible to non-experts and receiving significant public attention. Those methods are capable of generating realistic face images by replacing the classic graphics pipeline. They all, however, still implicitly use 3D face representations.\n\nSome methods relied on latent feature space domain separation~\\cite{tian2018cr,natsume2018rsgan,natsume18fsnet}. These methods decompose the identity component of the face from the remaining traits, and encode identity as the manifestation of latent feature vectors, resulting in significant information loss and limiting the quality of the synthesized images. Subject specific methods~\\cite{suwajanakorn2017synthesizing,DeepFakes,wayne2018reenactgan,kim2018deep} must be trained for each subject or pair of subjects and so require expensive subject specific data---typically thousands of face images---to achieve reasonable results, limiting their potential usage. Finally, a major concern shared by previous face synthesis schemes, particularly the 3D based methods, is that they all require special care when handling partially occluded faces. \n\nWe propose a deep learning--based approach to face swapping and reenactment in images and videos. Unlike previous work, our approach is {\\em subject agnostic}: it can be applied to faces of different subjects without requiring subject specific training. Our Face Swapping GAN (FSGAN) is end-to-end trainable and produces photo realistic, temporally coherent results. We make the following contributions:\n\\begin{itemize}\n    \\item {\\bf Subject agnostic swapping and reenactment.} To the best of our knowledge, our method is the first to simultaneously manipulate pose, expression, and identity without requiring person-specific or pair-specific training, while producing high quality and temporally coherent results.\n    \\item {\\bf Multiple view interpolation.} We offer a novel scheme for interpolating between multiple views of the same face in a continuous manner based on reenactment, Delaunay Triangulation and barycentric coordinates.\n    \\item {\\bf New loss functions.} We propose two new losses: A stepwise consistency loss, for training face reenactment progressively in small steps, and a Poisson blending loss, to train the face blending network to seamlessly integrate the source face into its new context.\n\\end{itemize}\n\nWe test our method extensively, reporting qualitative and quantitative ablation results and comparisons with state of the art. The quality of our results surpasses existing work even without training on subject specific images. \n\n\\section{Related work}\\label{sec:related_work}\nMethods for manipulating the appearances of face images, particularly for face swapping and reenactment, have a long history, going back nearly two decades. These methods were originally proposed due to privacy concerns~\\cite{blanz2004exchanging,lin2012face,mosaddegh2014photorealistic} though they are increasingly used for recreation~\\cite{kemelmacher2016transfiguring} or entertainment (e.g.,~\\cite{alexander2009creating,wolf2010eye}). %We briefly survey these previous methods. \n\n\\minisection{3D based methods}\nThe earliest swapping methods required manual involvement~\\cite{blanz2004exchanging}. An automatic method was proposed a few years later~\\cite{bitouk2008face}. More recently, Face2Face transferred expressions from source to target face~\\cite{thies2016face2face}. Transfer is performed by fitting a 3D morphable face model (3DMM)~\\cite{blanz2002face,blanz2003face,chang2019deep} to both faces and then applying the expression components of one face onto the other with care given to interior mouth regions. The reenactement method of Suwajanakorn et al.~\\cite{suwajanakorn2017synthesizing} synthesized the mouth part of the face using a reconstructed 3D model of (former president) Obama, guided by face landmarks, and using a similar strategy for filling the face interior as in Face2Face. The expression of frontal faces was manipulated by Averbuch-Elor et al.~\\cite{averbuch2017bringing} by transferring the mouth interior from source to target image using 2D wraps and face landmarks. \n\nFinally, Nirkin et al.~\\cite{nirkin2018face} proposed a face swapping method, showing that 3D face shape estimation is unnecessary for realistic face swaps. Instead, they used a fixed 3D face shape as the proxy~\\cite{hassner2015effective,masi2019face}. Like us, they proposed a face segmentation method, though their work was not end-to-end trainable and required special attention to occlusions. We show our results to be superior than theirs.\n\n\\minisection{GAN-based methods} GANs~\\cite{goodfellow2014generative} were shown to generate fake images with the same distribution as a target domain. Although successful in generating realistic appearances, training GANs can be unstable and restricts their application to low-resolution images. Subsequent methods, however, improved the stability of the training process~\\cite{mao2017least,arjovsky2017wasserstein}. Karras et al.~\\cite{karras2017progressive} train GANs using a progressive multiscale scheme, from a low to high image resolutions. CycleGAN~\\cite{zhu2017unpaired} proposed a cycle consistency loss, allowing training of unsupervised generic transformations between different domains. A cGAN with $L_{1}$ loss was applied by Isola et al.~\\cite{isola2017image} to derive the pix2pix method, and was shown to produce appealing synthesis results for applications such as transforming edges to faces. \n\n\\minisection{Facial manipulation using GANs} Pix2pixHD~\\cite{wang2018pix2pixHD} used GANs for high resolution image-to-image translation by applying a multi-scale cGAN architecture and adding a perceptual loss~\\cite{johnson2016perceptual}. GANimation~\\cite{pumarola2018ganimation} proposed a dual generator cGAN conditioned on emotion action units, that generates an attention map. This map was used to interpolate between the reenacted and original images, to preserve the background. GANnotation~\\cite{sanchez2018triple} proposed deep facial reenactment driven by face landmarks. It generates images progressively using a triple consistency loss: it first frontalizes an image using landmarks then processes the frontal face. \n\nKim et al.~\\cite{kim2018deep} recently proposed a hybrid 3D/deep method. They render a reconstructed 3DMM of a specific subject using a classic graphic pipeline. The rendered image is then processed by a generator network, trained to map synthetic views of each subject to photo-realistic images.\n\nFinally, feature disentanglement was proposed as a means for face manipulation. RSGAN~\\cite{natsume2018rsgan} disentangles the latent representations of face and hair whereas FSNet~\\cite{natsume18fsnet} proposed a latent space which separates identity and geometric components, such as facial pose and expression.\n\n\\section{Face swapping GAN}\\label{sec:FSGAN}\n\n\\begin{figure*}[!htbp]\n\\centering\n\\includegraphics[width=1.0\\textwidth]{figures/system_05.jpg}\n\\caption{Overview of the proposed FSGAN approach. (a) The recurrent reenactment generator $G_{r}$ and the segmentation generator $G_{s}$. $G_{r}$ estimates the reenacted face $F_{r}$ and its segmentation $S_r$, while $G_{s}$ estimates the face and hair segmentation mask $S_{t}$ of the target image $I_{t}$. (b) The inpainting generator $G_{c}$ inpaints the missing parts of $\\tilde{F}_{r}$ based on $S_{t}$ to estimate the complete reenacted face $F_{c}$. (c) The blending generator $G_{b}$ blends $F_{c}$ and $F_{t}$, using the segmentation mask $S_{t}$.\\vspace{-3mm}}\\label{fig:system}\n\\end{figure*}\n\nIn this work we introduce the Face Swapping GAN (FSGAN), illustrated in Fig.~\\ref{fig:system}. Let $I_{s}$ be the source and $I_{t}$ the target images of faces $F_{s}\\in I_{s}$ and $F_{t}\\in I_{t}$, respectively. We aim to create a new image based on $I_{t}$, where $F_{t}$ is replaced by $F_{s}$ while retaining the same pose and expression. \n\nFSGAN consists of three main components. The first, detailed in Sec.~\\ref{subsec:Reenactment and Segmentation} (Fig.~\\ref{fig:system}(a)), consists of a reenactment generator $G_r$ and a segmentation CNN $G_s$. $G_r$ is given a heatmaps encoding the facial landmarks of $F_{t}$, and generates the reenacted image ${I}_{r}$, such that $F_{r}$ depicts $F_{s}$ at the same pose and expression of $F_{t}$. It also computes\n$S_{r}$: the segmentation mask of $F_{r}$. Component $G_s$\ncomputes the face and hair segmentations of $F_{t}$.\n\nThe reenacted image, $I_r$, may contain missing face\nparts, as illustrated in Fig.~\\ref{fig:system} and Fig.~\\ref{fig:system}(b). We therefore\napply the face inpainting network, $G_c$, detailed in Sec.~\\ref{subsec:inpainting}\nusing the segmentation $S_{t}$, to estimate the missing pixels. The final part of the FSGAN, shown in Fig.~\\ref{fig:system}(c) and Sec.~\\ref{subsec:Blending}, is the blending of the completed face $F_{c}$ into the target image $I_{t}$ to derive the final face swapping result. \n\nThe architecture of our face segmentation network, $G_{s}$, is based on U-Net~\\cite{ronneberger2015u}, with bilinear interpolation for upsampling. All our other generators---$G_{r}$, $G_{c}$, and $G_{b}$---are based on those used by pix2pixHD~\\cite{wang2018pix2pixHD}, with coarse-to-fine generators and multi-scale discriminators. Unlike pix2pixHD, our global generator uses a U-Net architecture with bottleneck blocks~\\cite{he2016deep} instead of simple convolutions and summation instead of concatenation. As with the segmentation network, we use bilinear interpolation for upsampling in both global generator and enhancers. The actual number of layers differs between generators. %The exact architectures are detailed in Sec.~\\ref{sec:architecture}.\n\nFollowing others~\\cite{wayne2018reenactgan}, training subject agnostic face reenactment is non-trivial and might fail when applied to unseen face images related by large poses. To address this challenge, we propose to break large pose changes into\nsmall manageable steps and interpolate between the closest available source images corresponding to a target's pose. These steps are explained in the following sections. \n\n\\subsection{Training losses}\n\n\\label{subsec:Training-Losses}\n\n\\minisection{Domain specific perceptual loss} % \\label{subsec:Perceptual Loss}\nTo capture fine facial details we adopt the perceptual loss~\\cite{johnson2016perceptual}, widely used in recent work for\nface synthesis~\\cite{sanchez2018triple}, outdoor scenes~\\cite{wang2018pix2pixHD}, and super resolution~\\cite{ledig2017photo}. Perceptual loss uses the feature maps of a pretrained VGG network, comparing high frequency details using a Euclidean distance. \n\nWe found it hard to fully capture details inherent to face images, using a network pretrained on a generic dataset such as ImageNet. Instead, our network is trained on the target\ndomain: We therefore train multiple VGG-19 networks~\\cite{simonyan2014very} for face recognition and face attribute classification. Let $F_{i}\\in\n\\mathbb{R}\n^{C_{i}\\times H_{i}\\times W_{i}}$ be the feature map of the $i$-th layer of\nour network, the perceptual loss is given by%\n\\begin{equation}\n\\mathcal{L}_{perc}(x,y)=\\sum_{i=1}^{n}\\frac{1}{C_{i}H_{i}W_{i}}\\left\\Vert\nF_{i}(x)-F_{i}(y)\\right\\Vert _{1}.\\label{eq:perceptualloss}\n\\end{equation}\n\n\\minisection{Reconstruction loss} %\\label{subsubsec:Reconstruction-Loss}\nWhile the perceptual loss of Eq.~\\eqref{eq:perceptualloss}\ncaptures fine details well, generators trained using only that loss, often produce images with inaccurate colors, corresponding to reconstruction of low frequency image content. We hence also applied a pixelwise $L_{1}$ loss to the generators:%\n\\begin{equation}\n\\mathcal{L}_{pixel}(x,y)=\\Vert x-y\\Vert _{1}.\n\\end{equation}%\nThe overall loss is then given by\n\\begin{equation}\n\\mathcal{L}_{rec}(x,y)=\\lambda _{perc}\\mathcal{L}_{perc}(x,y)+\\lambda\n_{pixel}\\mathcal{L}_{pixel}(x,y).  \\label{equ:total loss}\n\\end{equation}\nThe loss in Eq.~\\eqref{equ:total loss} was used with all our generators.%\n\n\\minisection{Adversarial loss} %%\\label{subsubsec:Adversarial-Loss}\nTo further improve the realism of our generated images we use an adversarial objective~\\cite{wang2018pix2pixHD}. We utilized a multi-scale discriminator consisting of multiple discriminators, $D_{1},D_{2},...,D_{n}$, each one operating on a different image resolution. For a generator $G$ and a multi-scale discriminator $D$, our adversarial loss is defined by:\n\\begin{equation}\n\\mathcal{L}_{adv}(G,D)=\\min_{G}\\max_{D_{1},\\dots D_{n}}\\sum_{i=1}^{n}%\n\\mathcal{L}_{GAN}(G,D_{i}),\n\\end{equation}%\nwhere $\\mathcal{L}_{GAN}(G,D)$ is defined as:\n\\begin{align}\n\\mathcal{L}_{GAN}(G,D)=&\\mathbb{E}_{(x,y)}[\\log D(x,y)]\\nonumber\\\\&+\\mathbb{E}_{x}[\\log\n(1-D(x,G(x)))].\n\\end{align}\n\n\\subsection{Face reenactment and segmentation}\n\\label{subsec:Reenactment and Segmentation}\nGiven an image $I\\in\\mathbb{R}^{3\\times H\\times W}$ and a heatmap\nrepresentation $H(p)\\in\\mathbb{R}^{70\\times H\\times W}$ of facial landmarks,\n$p\\in\\mathbb{R}^{70\\times2}$, we define the face reenactment generator, $G_{r}$,\nas the mapping $G_{r}:\\left\\{  \\mathbb{R}^{3\\times H\\times W},\\mathbb{R}%\n^{70\\times H\\times W}\\right\\}  \\rightarrow\\mathbb{R}^{3\\times H\\times W}$. \n\nLet\n$v_{s},v_{t}\\in\\mathbb{R}^{70\\times3}$ and $e_{s},e_{t}\\in\\mathbb{R}^{3}$, be\nthe 3D landmarks and Euler angles corresponding to $F_{s}$ and $F_{t}$. We generate intermediate 2D landmark positions $p_{j}$ by interpolating between $e_{s}$ and $e_{t}$, and the centroids of $v_{s}$ and $v_{t}$, using intermediate points for which we project $v_{s}$ back to $I_{s}$. We define the reenactment\noutput recursively for each iteration $1\\leq j\\leq n$ as\n\\begin{equation}\nI_{r_{j}},S_{r_{j}}=G_{r}(I_{r_{j-1}};H(p_{j})),\n\\end{equation}\n$$I_{r_{0}}=I_{s}.$$\n\nSimilar to others~\\cite{pumarola2018ganimation}, the last layer of the global generator and each of the enhancers in $G_{r}$ is split into two heads: the first produces the reenacted image and the second the segmentation mask. In contrast to binary masks used bu others~\\cite{pumarola2018ganimation}, we consider the face and hair regions separately. The binary mask implicitly\nlearned by the reenactment network captures most of the head including the hair, which we segment separately. Moreover, the additional hair segmentation also improves the accuracy of the face segmentation. The face segmentation generator $G_{s}$ is defined as $G_{r}:\\mathbb{R}^{3\\times H\\times\nW}\\rightarrow\\mathbb{R}^{3\\times H\\times W}$, where given an RGB image it output a 3-channels segmentation mask encoding the background, face, and hair.\n\n\\minisection{Training}\nInspired by the triple consistency loss~\\cite{sanchez2018triple}, we propose a stepwise consistency loss. Given an image pair $(I_s,I_t)$ of the same subject from a video sequence, let $I_{r_n}$ be the reenactment result after $n$ iterations, and $\\widetilde{I_t},\\widetilde{I}_{r_n}$ be the same images with their background removed using the segmentation masks $S_t$ and $S_{r_j}$, respectively. The stepwise consistency loss is defined as: $\\mathcal{L}_{rec}(\\widetilde{I}_{r_n},\\widetilde{I}_t)$.\nThe final objective for the $G_r$:\n\\begin{align}\n\\mathcal{L}(G_r)=&\\lambda_{stepwise}\\mathcal{L}_{rec}(\\widetilde{I}_{r_n},\\widetilde{I}_t)+\\lambda_{rec}\\mathcal{L}_{rec}(\\widetilde{I}_r,\\widetilde{I}_t)\\nonumber\\\\&+\\lambda_{adv}\\mathcal{L}_{adv}+\\lambda_{seg}\\mathcal{L}_{pixel}(S_r,S_t).\n\\end{align}\n\nFor the objective of $G_s$ we use the standard cross-entropy loss, $L_{ce}$, with additional guidance from $G_r$:\n\\begin{equation}\n\\mathcal{L}(G_s)=L_{ce}+\\lambda_{reenactment}\\mathcal{L}_{pixel}(S_t,S_r^t),\n\\end{equation}\nwhere $S_r^t$ is the segmentation mask result of $G_r(I_t;H(p_t))$ and $p_t$ is the 2D landmarks corresponding to $I_t$. \n\nWe train both $G_r$ and $G_s$ together, in an interleaved fashion. We start with training $G_s$ for one epoch followed by the training of $G_r$ for an additional epoch, increasing $\\lambda_{reenactment}$ as the training progresses. We have found that training $G_r$ and $G_s$ together helps filtering noise learned from coarse face and hair segmentation labels.\n\n\\subsection{Face view interpolation}\n\\label{subsec:FaceViewInterpolation}\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[clip,trim=0mm 3mm 0mm 0mm, width=.85\\textwidth]{figures/view_interpolation_01.jpg}\\vspace{-3mm}\n\\caption{\n{\\em Face view interpolation.} (a) Shows an example of an appearance map of the source subject (Donald Trump). The green dots represent different views of the source subject, the blue lines represent the Delaunay Triangulation of those views, and the red X marks the location of the current target's pose. (b) The interpolated views associated with the vertices of the selected triangle (represented by the yellow dots). (c) The reenactment result and the current target image. \\vspace{-4mm} }\\label{fig:interpolation}\n\\end{figure*}\nStandard computer graphics pipelines project textured mesh polygons onto a plane for seamless rendering~\\cite{hughes2014computer}. We propose a novel, alternative scheme for continuous interpolation between face views. This step is an essential phase of our method, as it allows using the entire source video sequence, without training our model on a particular video frame, making it subject agnostic.\n\nGiven a set of source subject images, $\\left\\{ \\mathbf{I}%\n_{s_{1}},\\dots ,\\mathbf{I}_{s_{n}}\\right\\}$, and Euler angles, $\\left\\{\n\\mathbf{e}_{1},\\dots ,\\mathbf{e}_{n}\\right\\}$, of the corresponding faces $\\left\\{ \\mathbf{F}_{s_{1}},\\dots ,\\mathbf{F}_{s_{n}}\\right\\}$, we construct\nthe appearance map of the source subject, illustrated in Fig.~\\ref{fig:interpolation}(a). This appearance map embeds head poses in a triangulated plane, allowing head poses to follow continuous paths. \n\nWe start by projecting the Euler angles $\\left\\{ \\mathbf{e}_{1},\\dots ,\\mathbf{e}_{n}\\right\\} $ onto a plane by dropping the roll angle. Using a k-d tree data structure~\\cite{hughes2014computer}, we remove points in the angular domain that are too close to each other, prioritizing the points for which the corresponding Euler angles have a roll angle closer to zero. We further remove motion blurred images. Using the remaining points, $\\left\\{ x_{1},\\dots ,x_{m}\\right\\}$, and the four boundary points, $y_{i}\\in \\lbrack -75,75]\\times \\lbrack -75,75]$, we build a\nmesh, $M$, in the angular domain by Delaunay Triangulation.\n\nFor a query Euler angle, $e_{t}$, of a face, $F_{t}$, and its corresponding projected point, $x_{t}$, we find the triangle $T\\in M$ that contains $x_{t}$. Let $x_{i_{1}},x_{i_{2}},x_{i_{3}}$ be the vertices of $T$ and $I_{s_{i_{1}}},I_{s_{i_{2}}},I_{s_{i_{3}}}$ be the corresponding face views. We calculate the barycentric coordinates, $\\lambda _{1},\\lambda _{2},\\lambda\n_{3}$ of $x_{t}$, with respect to $x_{i_{1}},x_{i_{2}},x_{i_{3}}$. The\ninterpolation result $I_{r}$ is then\n\\begin{equation}\nI_{r}=\\sum_{k=1}^{3}\\lambda _{k}G_{r}(I_{s_{i_{k}}};H(\\mathbf{p}_{t})),\n\\end{equation}\nwhere $\\mathbf{p}_{t}$ are the 2D landmarks of $F_{t}$. If any vertices of the triangle are boundary points, we exclude them from the interpolation and normalize the weights, $\\lambda _{i}$, to sum to one. \n\nA face view query is illustrated in Fig.~\\ref{fig:interpolation}(b,c). To improve interpolation accuracy, we use a horizontal flip to fill in views when the appearance map is one-sided with respect to the yaw dimension, and generate artificial views using $G_{r}$ when the appearance map is too sparse.\n\n\\subsection{Face inpainting}\n\\label{subsec:inpainting}\nOccluded regions in the source face $F_{s}$ cannot be rendered on the target face, $F_{t}$. Nirkin et al.~\\cite{nirkin2018face} used the segmentations of $F_{s}$ and $F_{t}$ to remove occluded regions, rendering (swapping) only regions visible in both source and target faces. %Pixels in $F_{t}$ were used to rendered the facial regions occluded in $F_{s}$.\nLarge occlusions and different facial textures can cause noticeable artifacts in the resulting images. \n\nTo mitigate such problems, we apply a face inpainting generator, $G_{c}$ (Fig.~\\ref{fig:system}(b)). $G_{c}$ renders face image $F_{s}$ such that the resulting face rendering $\\tilde{I}_{r}$ covers entire segmentation mask $S_{t}$ (of $F_{t}$), thereby resolving such occlusion.\n\nGiven the reenactment result, $I_r$, its corresponding segmentation, $S_r$, and the target image with its background removed, $\\tilde{I}_t$, all drawn from the same identity, we first augment $S_r$ by simulating common face occlusions due to hair, by randomly removing ellipse-shaped parts, in various sizes and aspect ratios from the border of $S_r$. Let $\\tilde{I}_r$ be $I_r$ with its background removed using the augmented version of $S_r$, and $I_c$ the completed result from applying $G_c$ on $\\tilde{I}_r$. We define our inpainting generator loss as\n\\begin{equation}\n\\mathcal{L}(G_{c})=\\lambda _{rec}\\mathcal{L}_{rec}(I_c,\\tilde{I}_t)+\\lambda _{adv}\\mathcal{L}_{adv},\n\\end{equation}%\nwhere $\\mathcal{L}_{rec}$ and $\\mathcal{L}_{adv}$ are the reconstruction and adversarial losses of Sec.~\\ref{subsec:Training-Losses}.\n\n\\subsection{Face blending}\n\\label{subsec:Blending}\nThe last step of the proposed face swapping scheme is blending of the completed face $F_c$ with its target face $F_{t}$ (Fig. \\ref{fig:system}(c)). Any blending must account for, among others, different skin tones and lighting conditions. Inspired by previous uses of Poisson blending for inpainting~\\cite{yeh2017semantic} and blending~\\cite{wu2017gp}, we propose a novel Poisson blending loss.\n\nLet $I_{t}$ be the target image, $I_{r}^{t}$ the image of the reenacted face transferred onto the target image, and $S_{t}$ the segmentation mask marking the transferred pixels. Following~\\cite{perez2003poisson}, we define the Poisson blending optimization as\n\\begin{equation}\n\\label{equ:Poisson}\n   \\begin{alignedat}{2}\n    P(I_{t};I_{r}^{t};S_{t}))= & \\arg \\min_{f}\\Vert \\nabla f-\\nabla I_{r}^{t}\\Vert _{2}^{2} \\\\\n    & \\text{s.t. }f(i,j)=I_{t}(i,j),\\text{ }\\forall \\text{ }S_{t}(i,j)=0,\n   \\end{alignedat}\n\\end{equation}\nwhere $\\nabla \\left( \\cdot \\right) $ is the gradient operator. We combine the Poisson optimization in Eq.~\\eqref{equ:Poisson} with the perceptual loss. The Poisson blending loss is then $\\mathcal{L}(G_{b})$\n$$\n\\mathcal{L}(G_{b})=\\lambda _{rec}\\mathcal{L}_{rec}(G_{b}(I_{t};I_{r}^{t};S_{t}),P(I_{t};I_{r}^{t};S_{t}))+\\lambda_{adv}\\mathcal{L}%\n_{adv}.\n$$\n\n\\section{Datasets and training}\n\\label{sec:datasets_and_processing}\n\\subsection{Datasets and processing}\nWe use the video sequences of the IJB-C dataset~\\cite{maze2018iarpa} to train our generator, $G_{r}$, for which we automatically extracted the frames depicting particular subjects. IJB-C contains $\\sim$11k face videos, of\nwhich we used 5,500 which were in high definition. Similar to the frame pruning approach of Sec.~\\ref{subsec:FaceViewInterpolation}, we prune the face\nviews that are too close together as well as motion-blurred frames. \n\nWe apply the segmentation CNN, $G_{s}$, to the frames, and prune the frames for which less than 15\\% of the pixels in the face bounding box were classified as face pixels. We used dlib's face verification\\footnote{Available: \\url{http://dlib.net/}} to group frames according to the subject identity, and limit the number of frames per subject to 100, by choosing frames with the maximal variance in 2D landmarks. In each training iteration, we choose the frames $I_{s}$ and $I_{t}$ from two randomly chosen subjects. \n\nWe trained VGG-19 CNNs for the perceptual loss on the VGGFace2 dataset~\\cite{cao2018vggface2} for face recognition and the CelebA~\\cite{liu2018large} dataset for face attribute classification. The VGGFace2 dataset contains 3.3M images depicting 9,131 identities, whereas CelebA contains 202,599 images, annotated with 40 binary attributes. \n\nWe trained the segmentation CNN, $G_{s}$, on data used by others~\\cite{nirkin2018face}, consisting of ${\\sim}10k$ face images labeled with face segmentations. We also used the LFW Parts Labels set~\\cite{kae2013augmenting} with ${\\sim}3k$ images labeled for face and hair segmentations, removing the neck regions using facial landmarks. \n\nWe used additional 1k images and corresponding hair segmentations from the Figaro dataset~\\cite{svanera2016figaro}. Finally, FaceForensics++~\\cite{roessler2019faceforensics++} provides 1000 videos, from which they generated 1000 synthetic videos on random pairs using DeepFakes~\\cite{DeepFakes} and Face2Face~\\cite{thies2016face2face}.%\\footnote{Available: \\url{http://www.eecs.qmul.ac.uk/~urm30/Figaro.html}}\n\n\\subsection{Training details}%\\vspace{-2mm}\nWe train the proposed generators from scratch, where the weights were initialized randomly using a normal distribution. We use Adam optimization~\\cite{kingma2014adam} ($\\beta_{1}=0.5,\\beta_{2}=0.999$) and a learning rate of $0.0002$. We reduce this rate by half every ten epochs. The following parameters were used for all the generators: $\\lambda_{perc}=1,\\lambda_{pixel}=0.1,\\lambda_{adv}=0.001,\\lambda_{seg}=0.1,\\lambda_{rec}=1,\\lambda_{stepwise}=1$, where $\\lambda_{reenactment}$ is linearly increased from 0 to 1 during training. All of our networks were trained on eight NVIDIA Tesla V100 GPUs and an Intel Xeon CPU. Training of $G_{s}$ required six hours to converge, while the rest of the networks converged in two days. All our networks, except for $G_{s}$, were trained using a progressive multi scale approach, starting with a resolution of 128$\\times$128 and ending at 256$\\times$256. Inference rate is ${\\sim}30$fps for reenactment and ${\\sim}10$fps for swapping on one NVIDIA Tesla V100 GPU.\n\n\\section{Experimental results}\nWe performed extensive qualitative and quantitative experiments to verify the proposed scheme. We compare our method to two previous face swapping methods: DeepFakes~\\cite{DeepFakes} and Nirkin et al.~\\cite{nirkin2018face}, and the Face2Face reenactment scheme~\\cite{thies2016face2face}. We conduct all our experiments on videos from FaceForensics++~\\cite{roessler2019faceforensics++}, by running our method on the same pairs they used. We further report ablation studies showing the importance of each component in our pipeline.\n\n\\subsection{Qualitative face reenactment results}\nFig.~\\ref{fig:face_reenactment_qualitative} shows our raw face reenactment results, without background removal. We chose examples of varying ethnicity, pose, and expression. A specifically interesting example can be seen in the rightmost column, showing our method's ability to cope with extreme expressions. To show the importance of iterative reenactment, Fig~\\ref{fig:reenactment_limitations} provides reenactments of the same subject for both small and large angle differences. As evident from the last column, for large angle differences, the identity and texture are better preserved using multiple iterations. %For best results, the number of iterations should be adjusted according to the angle differences.\n\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=1.0\\linewidth,trim=0mm 0mm 0mm 0mm ,clip]{figures/reenactment_02.jpg}\n\\caption{ \\emph{Qualitative face reenactment results.} Row 1: The source face for reenactment. Row 2: Our reenactment results (without background removal). Row 3: The target face from which to transfer the pose and expression.  \\vspace{-4mm} }%\n\\label{fig:face_reenactment_qualitative}%\n\\end{figure*}\n\n\\begin{figure}[!htbp]\n\\centering\n\\includegraphics[width=1.0\\linewidth]{figures/limitations_03.jpg}\n\\caption{ \\emph{Reenactment limitations.} Top left image transformed onto each of the images in Row 1 (using the same subject for clarity). Row 2: Reenactment with one iteration. Row 3: Three iterations.}%\n\\label{fig:reenactment_limitations}\\vspace{-5mm}%\n\\end{figure}\n\n\\subsection{Qualitative face swapping results}\nFig.~\\ref{fig:face_swap_qualitative} offers face swapping examples taken from FaceForensics++ videos, {\\em without training our model on these videos}. We chose examples that represent different poses and expression, face shapes, and hair occlusions. Because Nirkin et al.~\\cite{nirkin2018face} is an image-to-image face swapping method, to be fair in our comparison, for each frame in the target video we select the source frame with the most similar pose. To compare FSGAN in a video-to-video scenario, we use our face view interpolation described in Sec.~\\ref{subsec:FaceViewInterpolation}.\n\n\\begin{figure*}[tb]\n\\centering\n\\includegraphics[width=1.0\\linewidth,trim=0mm 27mm 0mm 0mm ,clip]{figures/deepfakes_03.jpg}\n\\caption{ \\emph{Qualitative face swapping results on~\\cite{roessler2019faceforensics++}.} Results for source photo swapped onto target provided for Nirkin et al.~\\cite{nirkin2018face}, DeepFakes~\\cite{DeepFakes} and our method on images of faces of subjects it was not trained on.  }%\n\\label{fig:face_swap_qualitative}\\vspace{-5mm}%\n\\end{figure*}\n\n\\subsection{Comparison to Face2Face}\nWe compare our method to Face2Face~\\cite{thies2016face2face} on the expression only reenactment problem. Given a pair of faces $F_s \\in I_s$ and $F_t \\in I_t$ the goal is to transfer the expression from $I_s$ to $I_t$. To this end, we modify the corresponding 2D landmarks of $F_t$ by swapping in the mouth points of the 2D landmarks of $F_s$, similarly to how we generate the intermediate landmarks in Sec.~\\ref{subsec:Reenactment and Segmentation}. The reenactment result is then given by $G_r(I_t;H(\\hat{p}_t))$, where $\\hat{p}_t$ are the modified landmarks. The examples are shown in Fig.~\\ref{fig:face2face}.\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=1.0\\linewidth]{figures/face2face_01.jpg}\n\\caption{ \\emph{Comparison to Face2Face~\\cite{thies2016face2face} on FaceForensics++~\\cite{roessler2019faceforensics++}.} As demonstrated, our method exhibits far less artifacts than Face2Face. \\vspace{-4mm} }%\n\\label{fig:face2face}%\n\\end{figure}\n\n\\begin{figure*}[!htbp]\n\\centering\n\\includegraphics[width=0.95\\linewidth,trim=0mm 29mm 0mm 0mm ,clip]{figures/ablation_03.jpg}\\vspace{-2mm}\n\\caption{\\emph{Ablation study}. From columns 3 and 5, without the completion network, $G_c$, the transferred face does not cover the entire target face, leaving obvious artifacts. Columns 3 and 4 show that without the blending network, $G_b$, the skin color and lighting conditions of the transferred face are inconsistent with its new context.}%\n\\vspace{-4mm}\n\\label{fig:ablation_qualitative}%\n\\end{figure*}\n\n\\subsection{Quantitative results}\n\\label{subsec:Quantitative results}\nWe report quantitative results, conforming to how we defined the face swapping problem: we validate how well methods preserve the source subject identity, while retaining the same pose and expression of the target subject. To this end, we first compare the face swapping result, $F_b$, of each frame to its nearest neighbor in pose from the subject face views. We use the dlib~\\cite{dlib09} face verification method to compare identities and the structural similarity index method (SSIM) to compare their quality. To measure pose accuracy, we calculate the Euclidean distance between the Euler angles of $F_b$ to the original target image, $I_t$. Similarly, the accuracy of the expression is measured as the Euclidean distance between the 2D landmarks. Pose error is measured in degrees and the expression error is measured in pixels. We computes the mean and variance of those measurements on the first 100 frames of the first 500 videos in FaceForensics++, averaging them across the videos. As baselines, we use Nirkin et al.~\\cite{nirkin2018face} and DeepFakes~\\cite{DeepFakes}. \n\nEvident from the first two columns of Table~\\ref{tab:quant}, our approach preserves identity and image quality similarly to previous methods. The two rightmost metrics in Table~\\ref{tab:quant} show that our method retains pose and expression much better than its baselines. Note that the human eye is very sensitive to artifacts on faces. This should be reflected in the quality score but those artifacts usually capture only a small part of the image and so the SSIM score does not reflect them well.\n\n\\begin{table}[t!]\n\\centering{\n \\resizebox{0.98\\linewidth}{!}{\n \\begin{tabular}{lcccc} \n \\toprule\n Method & verification $\\downarrow$ & SSIM $\\uparrow$ & euler $\\downarrow$ & landmarks $\\downarrow$ \\\\ [0.5ex] \n \\hline\n Nirkin et al.~\\cite{nirkin2018face} & 0.39 $\\pm$ 0.00       & 0.49 $\\pm$ 0.00       & 3.15 $\\pm$ 0.04       & 26.5 $\\pm$ 17.7 \\\\\n DeepFakes ~\\cite{DeepFakes} & 0.38 $\\pm$ 0.00 & 0.50 $\\pm$ 0.00 & 4.05 $\\pm$ 0.04 & 34.1 $\\pm$ 16.6 \\\\ \n FSGAN                       & 0.38 $\\pm$ 0.00 & {\\bf 0.51 $\\pm$ 0.00} & {\\bf 2.49 $\\pm$ 0.04} & {\\bf 22.2 $\\pm$ 17.7} \\\\\n \\bottomrule\n \\end{tabular}\n}%\\vspace{-2mm}\n}\n\\caption{{\\em Quantitative swapping results.} On FaceForensics++ videos~\\cite{roessler2019faceforensics++}.}\\label{tab:quant}\\vspace{-4mm}\n\\end{table}\n\n\\subsection{Ablation study}\n\nWe performed ablation tests with four configurations of our method: $G_r$ only, $G_r+G_c$, $G_r+G_b$, and our full pipeline. The segmentation network, $G_s$, is used in all configurations. Qualitative results are provided in Fig.~\\ref{fig:ablation_qualitative}.\n\nQuantitative ablation results are reported in Table~\\ref{tab:ablation_quant}. Verification scores show that source identities are preserved across all pipeline networks. From Euler and landmarks scores we see that target poses and expressions are best retained with the full pipeline. Error differences are not extreme, suggesting that the inpainting and blending generators, $G_{c}$ and $G_{b}$, respectively, preserve pose and expression similarly well. There is a slight drop in the SSIM, due to the additional networks and processing added to the pipeline.\n\n\\begin{table}[tbh]\n\\centering{\n\\resizebox{0.98\\linewidth}{!}{\n\\begin{tabular}{lcccc}\n\\toprule\nMethod & verification $\\downarrow$ & SSIM $\\uparrow$ & euler $\\downarrow$ & landmarks $\\downarrow$ \\\\ [0.5ex] \n\\hline\nFSGAN $(G_r)$          & 0.38 $\\pm$ 0.00 & 0.54 $\\pm$ 0.00 & 3.16 $\\pm$ 0.03 & 22.6 $\\pm$ 16.5 \\\\\nFSGAN $(G_r+G_c)$      & 0.38 $\\pm$ 0.00 & 0.54 $\\pm$ 0.00 & 3.21 $\\pm$ 0.08 & 24.5 $\\pm$ 17.2 \\\\\nFSGAN $(G_r+G_b)$      & 0.38 $\\pm$ 0.00 & 0.52 $\\pm$ 0.00 & 2.75 $\\pm$ 0.05 & 23.6 $\\pm$ 17.9 \\\\\nFSGAN $(G_r+G_c+G_b)$  & 0.38 $\\pm$ 0.00 & 0.51 $\\pm$ 0.00 & {\\bf 2.49 $\\pm$ 0.04} & {\\bf 22.2 $\\pm$ 17.7} \\\\\n\\bottomrule\n\\end{tabular}\n}\n}\n\\caption{{\\em Quantitative ablation results.} On\nFaceForensics++ videos~\\cite{roessler2019faceforensics++}.}%\n\\label{tab:ablation_quant}\\vspace{-5mm}%\n\\end{table}\n\n\\section{Conclusion}\n\\noindent{\\bf Limitations.} Fig.~\\ref{fig:reenactment_limitations} shows our reenactment results for different facial yaw angles. Evidently, the larger the angular differences, the more identity and texture quality degrade. Moreover, too many iterations of the face reenactment generator blur the texture. Unlike 3DMM based methods, e.g., Face2Face~\\cite{thies2016face2face}, which warp textures directly from the image, our method is limited to the resolution of the training data. Another limitation arises from using a sparse landmark tracking method that does not fully capture the complexity of facial expressions. %Finally, it only utilizes one frame at a time.\n\n\\minisection{Discussion} Our method eliminates laborious, subject-specific, data collection and model training, making face swapping and reenactment accessible to non-experts. We feel strongly that it is of {\\em paramount importance} to publish such technologies, in order to drive the development of technical counter-measures for detecting such forgeries, as well as compel law makers to set clear policies for addressing their implications. Suppressing the publication of such methods would not stop their development, but rather make them available to select few and potentially blindside policy makers if it is misused.\n\n{\\small\n}\n\n\\onecolumn\n\n\\pagebreak\n\n   \\newpage\n   \\null\n   \\vskip .375in\n   \\begin{center}\n      {\\Large \\bf Supplementary Material \\par}\n   \\end{center}\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{FaceShifter: Towards High Fidelity And Occlusion Aware Face Swapping}\n\n\\begin{document}\n\n\\title{FaceShifter: Towards High Fidelity And Occlusion Aware Face Swapping}\n\\author{Lingzhi Li$^{1*}$ \\qquad Jianmin Bao$^{2\\dag}$ \\qquad Hao Yang$^{2}$ \\qquad Dong Chen$^{2}$ \\qquad Fang Wen$^{2}$ \\qquad \\vspace{1pt}\\\\\n$^{1}$Peking University  \\qquad $^{2}$Microsoft Research\\qquad\\qquad\\\\\n\\hspace{0.1in}{\\tt\\small lilingzhi@pku.edu.cn} \\qquad  {\\tt\\small \\{jianbao,haya,doch,fangwen\\}@microsoft.com} \\\\\n}\n\n\\twocolumn[{%\n\\renewcommand\\twocolumn[1][]{#1}%\n\\vspace{-1em}\n\\maketitle\n\\vspace{-1em}\n\\begin{center}\n\\centering\n\n\\includegraphics[width=1.0\\linewidth]{figures/teaser/teaser_v8.pdf}\n\\captionof{figure}{The face in the source image is taken to replace the face in the target image. Results of FaceShifter appear in the right.\n}\n\\label{fig:teaser}\n\\end{center}%\n}]\n\n\\maketitle\n\n\\begin{abstract}\nIn this work, we propose a novel two-stage framework, called FaceShifter, for high fidelity and occlusion aware face swapping. \nUnlike many existing face swapping works that leverage only limited information from the target image when synthesizing the swapped face, \nour framework, in its first stage, generates the swapped face in high-fidelity by exploiting and integrating the target attributes thoroughly and adaptively. \nWe propose a novel attributes encoder for extracting multi-level target face attributes, and a new generator with carefully designed Adaptive Attentional Denormalization (AAD) layers to adaptively integrate the identity and the attributes for face synthesis. \nTo address the challenging facial occlusions, we append a second stage consisting of a novel Heuristic Error Acknowledging Refinement Network (HEAR-Net). It is trained to recover anomaly regions in a self-supervised way without any manual annotations. \nExtensive experiments on wild faces demonstrate that our face swapping results are not only considerably more perceptually appealing, but also better identity preserving in comparison to other state-of-the-art methods. For generated dataset and more detail please refer to our project webpage \\url{https://lingzhili.com/FaceShifterPage/} {\\let\\thefootnote\\relax\\footnotetext{$^{*}$ Work done during an internship at Microsoft Research Asia}}\n{\\let\\thefootnote\\relax\\footnotetext{$^{\\dag}$ Corresponding Author}}\n\\end{abstract}\n\n\\section{Introduction}\n\nFace swapping is the replacement of the identity of a person in the target image with that of another person in the source image, while preserving attributes \\eg head pose, facial expression, lighting, background \\etc. Face swapping has attracted great interest in vision and graphics community, because of its potential wide applications in movie composition, computer games, and privacy protection~\\cite{ross2010visual}.\n\nThe main difficulties in face swapping are how to extract and adaptively recombine identity and attributes of two images. Early replacement-based works~\\cite{bitouk2008face,wang2008facial} simply replace the pixels of inner face region. Thus, they are sensitive to the variations in posture and perspective. 3D-based works~\\cite{blanz2004exchanging,cheng20093d,lin2012face,nirkin2018face} used a 3D model to deal with the posture or perspective difference. However the accuracy and robustness of 3D reconstruction of faces are all unsatisfactory. Recently, GAN-based works~\\cite{korshunova2017fast,natsume2018fsnet,natsume2018rsgan,nirkin2019fsgan, bao2017cvae} have illustrated impressive results. But it remains challenging to synthesize both realistic and high-fidelity results. \n\nIn this work, we focus on improving the fidelity of face swapping. \nIn order to make the results more perceptually appealing, it is important that the synthesized swapped face not only shares the pose and expression of the target face, but also can be seamlessly fitted into the target image without inconsistency: the rendering of the swapped face should be faithful to the lighting (\\eg direction, intensity, color) of the target scene, the pixel resolution of the swapped face should also be consistent with the target image resolution. \nNeither of these can be well handled by a simple alpha or Poisson blending.\nInstead, we need a \\emph{thorough and adaptive integration of target image attributes during the synthesis of the swapped face}, so that the attributes from the target image, including scene lighting or image resolution, can help make the swapped face more realistic.\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=\\linewidth]{figures/introduction/previous_issues_v4.pdf}\n\\caption{\nFailure cases of a previous method on FaceForensics++ \\cite{rossler2019faceforensics++} dataset. \nFrom left to right we show the input source images, the input target images, the results of FaceSwap \\cite{faceswap}, and the results of our method.\nFaceSwap follows the strategy that, first synthesizes the inner face region, then blends it into the target face. \nSuch strategy causes artifacts, such as the defective lighting effect on the nose (row 1), failing to preserve the face shape of the source identity (row 2) and the mismatched image resolutions (row 3).  While our method addresses all these issues. \n}\n\\label{fig:existing_issues}\n\\vspace{-1em}\n\\end{figure}\n\nHowever, previous face swapping methods either neglect the requirement of this integration, or lack the ability to perform it in a thorough and adaptive way.\nIn specific, many previous methods use only pose and expression guidances from the target image to synthesize the swapped face, the face is then blended into the target image using masks of the target faces. This process is easy to cause artifacts, because:\n1) Besides pose and expression, it leverages little knowledge about the target image when synthesizing the swapped face, which can hardly respect target attributes like the scene lightings or the image resolutions; \n2) Such a blending will discard all the peripheral area of the source face that locates outside the target face mask. Thus these methods cannot preserve the face shape of the source identity.\nWe show some typical failure cases in Figure \\ref{fig:existing_issues}. \n\nIn order to achieve high-fidelity face swapping results, in the first stage of our framework, we design a GAN-based network, named \\emph{Adaptive Embedding Integration Network} (AEI-Net), for a thorough and adaptive integration of target attributes. We made two improvements to the network structure: 1) we propose a novel \\emph{multi-level attributes encoder} for extracting target attributes in various spatial resolutions, instead of compressing it into a single vector as RSGAN~\\cite{natsume2018rsgan} and IPGAN~\\cite{Bao_ipgan}. 2) we present a novel generator with carefully designed \\emph{Adaptive Attentional Denormalization} ({AAD}) layers which adaptively learns where to integrate the attributes or identity embeddings. Such an adaptive integration brings considerable improvements over the single level integration used by RSGAN~\\cite{natsume2018rsgan}, FSNet~\\cite{natsume2018fsnet} and IPGAN~\\cite{Bao_ipgan}. With these two improvements, the proposed AEI-Net can solve the problem of inconsistent illumination and face shape, as shown in Figure~\\ref{fig:existing_issues}.\n\nMoreover, handling facial occlusions is always challenging in face swapping. Unlike Nirkin \\etal~\\cite{nirkin2019fsgan,nirkin2018face} that trains face segmentation to obtain occlusion-aware face masks, our method can learn to recover face anomaly regions in a self-supervised way without any manual annotations. \nWe observe that when feeding the same face image as both the target and source into a well trained AEI-Net, the reconstructed face image deviates from the input in multiple areas, these deviations strongly hint the locations of face occlusions. Thus,\nwe propose a novel \\emph{Heuristic Error Acknowledging Refinement Network} ({HEAR-Net}) to further refine the result under the guidance of such reconstruction errors. The proposed method is more general, thus it identifies more anomaly types, such as glasses, shadow and reflection effects, and other uncommon occlusions.\n\nThe proposed two-stage face swapping framework, FaceShifter, is subject agnostic. Once trained, the model can be applied to any new face pairs without requiring subject specific training as DeepFakes~\\cite{deepfake} and Korshunova~\\etal~\\cite{korshunova2017fast}. Experiments demonstrate that our method achieves results considerably more realistic and more faithful to inputs than other state-of-the-art methods.\n\n\\section{Related Works}\n\nFace swapping has a long history in vision and graphics researches. Early efforts \\cite{bitouk2008face,wang2008facial} only swap faces with similar poses. Such a limitation is addressed by recent algorithms roughly divided in two categories: 3D-based approaches and GAN-based approaches. \n\n\\noindent\\textbf{3D-Based Approaches}. Blanz \\etal \\cite{blanz2004exchanging} considers 3D transform between two faces with different poses, but requiring user interaction and not handling expressions. \nThies \\etal \\cite{thies2015real} captures head actions from a RGB-D image using 3DMM, turning a static face into a controllable avatar. It is extended for RGB references in Face2Face \\cite{thies2016face2face}. \nOlszewski \\etal \\cite{olszewski2017realistic} dynamically inferences 3D face textures for improved manipulation quality.\nKim \\etal \\cite{kim2018deep} separately models different videos using 3DMM to make the portraits controllable, while Nagano \\etal \\cite{nagano2018pagan} needs only one image to reenact the portrait within. \nRecently, Thies \\etal \\cite{thies2019deferred} adopt neural textures, which can better disentangle geometry in face reenactment.\nHowever, when applied on face swapping, these methods hardly leverage target attributes like occlusions, lighting or photo styles. \nTo preserve the target facial occlusions, Nirkin \\etal \\cite{nirkin2018face,nirkin2019fsgan} collected data to train an occlusion-aware face segmentation network in a supervised way, which helps predict a visible target face mask for blending in the swapped face. While our method find the occlusions in a self-supervised way without any manually annotations.\n\n\\noindent\\textbf{GAN-Based Approaches}. \nIn the GAN-based face swapping methods, Korshunova \\etal \\cite{korshunova2017fastfaceswap} swap faces like transfer styles. It separately models different source identities, such as a CageNet for Nicolas Cage, a SwiftNet for Taylor Swift.\nThe recently popular DeepFakes \\cite{deepfake} is another example of such subject-aware face swapping: for each new input, a new model has to be trained on two video sequences, one for the source and one for the target. \n\nThis limitation has been addressed by subject-agnostic face swapping researches: \nRSGAN \\cite{natsume2018rsgan} learns to extract vectorized embeddings for face and hair regions separately, and recombines them to synthesize a swapped face.\nFSNet \\cite{natsume2018fsnet} represents the face region of source image as a vector, which is combined with a non-face target image to generate the swapped face. \nIPGAN \\cite{Bao_ipgan} disentangles the identity and attributes of faces as vectors. By introducing supervisions directly from the source identity and the target image, IPGAN supports face swapping with better identity preservation. However, due to the information loss caused by the compressed representation, and the lack of more adaptive information integration, these three methods are incapable of generating high-quality face images. Recently, FSGAN \\cite{nirkin2019fsgan} performs face reenactment and face swapping together. It follows a similar reenact and blend strategy with \\cite{olszewski2017realistic,nagano2018pagan}. Although FSGAN utilizes an occlusion-aware face segmentation network for preserving target occlusions, it hardly respects target attributes like the lighting or image resolution, it can neither preserve the face shape of the source identity.\n\n\\section{Methods}\n\nOur method requires two input images, \\ie., a source image ${X}_s$ to provide identity and a target image ${X}_t$ to provide attributes, \\eg., pose, expression, scene lighting and background. The swapped face image is generated through a two-stage framework, called FaceShifter. In the first stage, we use an \\emph{Adaptive Embedding Integration Network} (AEI-Net) to generate a high fidelity face swapping result $\\hat{Y}_{s,t}$ based on information integration. In the second stage, we use the \\emph{Heuristic Error Acknowledging Network} (HEAR-Net) to handle the facial occlusions and refine the result, the final result is denoted by $Y_{s,t}$. \n\n\\subsection{Adaptive Embedding Integration Network}\n\\label{sec: stage_I}\n\n\\begin{figure*}[t]\n\\centering\n \\includegraphics[width=1.0\\linewidth]{figures/method/deepfake_method_stage1_v8.pdf}\n \\footnotesize\n    \\caption{AEI-Net for the first stage. AEI-Net is composed of an Identity Encoder, a Multi-level Attributes Encoder, and an AAD-Generator. The AAD-Generator integrates informations of identity and attributes in multiple feature levels using cascaded AAD ResBlks, which is built on AAD layers.}\n\\label{fig:framework_stage1}\n\\vspace{-1em}\n\\end{figure*}\n\nIn the first stage, we aim to generate a high fidelity face image $\\hat{Y}_{s,t}$, which should preserve the identity of the source ${X}_s$ and the attributes (\\eg pose, expression, lighting, background) of the target ${X}_t$. To achieve this goal, our method consist of 3 components: i) the \\emph{Identity Encoder} $\\bm{z}_{id}(X_s)$, which extracts identity from the source image $X_s$; ii) the \\emph{Multi-level Attributes Encoder} $\\bm{z}_{att}(X_t)$, which extracts attributes of the target image $X_t$; iii) \\emph{Adaptive Attentional Denormalization (AAD) Generator}, which generates swapped face image. Figure~\\ref{fig:framework_stage1}(a) shows whole network structure.\n\n\\noindent\\textbf{Identity Encoder}:\nWe use a pretrained state-of-the-art face recognition model \\cite{deng2019arcface} as identity encoder. The identity embedding $\\bm{z}_{id}(X_s)$ is defined to be the last feature vector generated before the final FC layer. \nWe believe that by training on a large quantity of 2D face data, such a face recognition model can provide more representative identity embeddings than the 3D-based models like 3DMM \\cite{blanz2004exchanging,blanz1999morphable}.\n\n\\noindent\\textbf{Multi-level Attributes Encoder}:\nFace attributes, such as pose, expression, lighting and background, require more spatial informations than identity. \nIn order to preserve such details, we propose to represent the attributes embedding as multi-level feature maps,\ninstead of compressing it into a single vector as previous methods \\cite{Bao_ipgan,natsume2018rsgan}.\nIn specific, we feed the target image $X_t$ into a U-Net-like structure. \nThen we define the attributes embedding as the feature maps generated from the U-Net decoder.\nMore formally, we define\n\\vspace{-0.5em}\n\\begin{equation}\n\\vspace{-0.1em}\n\\bm{z}_{att}(X_t) = \\left\\{\\bm{z}^1_{att}(X_t), \\bm{z}^2_{att}(X_t), \\cdots \\bm{z}^n_{att}(X_t)\\right\\},\n\\label{eqn:attribute_embedding}\n\\end{equation}\nwhere $\\bm{z}^{k}_{att}(X_t)$ represents the $k$-th level feature map from the U-Net decoder, $n$ is the number of feature levels. \n\nOur attributes embedding network does not require any attribute annotations, it extracts the attributes using self-supervised training: we require that the generated swapped face $\\hat{Y}_{x_t}$ and the target image $X_t$ have the same attributes embedding. The loss function will be introduce in Equation~\\ref{eqn:L_att}. In the experimental part (Section \\ref{ssec:analysis}), we also study what the attributes embedding has learned.\n\n\\noindent\\textbf{Adaptive Attentional Denormalization Generator}:\nWe then integrate such two embeddings $\\bm{z}_{id}(X_s)$ and $\\bm{z}_{att}(X_t)$ for generating a raw swapped face $\\hat{Y}_{s,t}$.\nPrevious methods~\\cite{Bao_ipgan,natsume2018rsgan} simply integrate them through feature concatenation. It will lead to relatively blurry results.\nInstead, we propose a novel \\emph{Adaptive Attentional Denormalization} (AAD) layer to accomplish this task in a more adaptive fashion.\nInspired by the mechanisms of SPADE \\cite{park2019semantic} and AdaIN \\cite{dumoulin2016learned, huang2017arbitrary}, the proposed AAD layers leverage denormalizations for feature integration in multiple feature levels.\n\nAs shown in Figure~\\ref{fig:framework_stage1}(c), in the $k$-th feature level, \nlet $\\bm{h}_{in}^k$ denote the activation map that is fed into an AAD layer, which should be a 3D tensor of size $C^k \\times H^k \\times W^k$, with $C^k$ being the number of channels and $H^k \\times W^k$ being the spatial dimensions.\nBefore integration, we perform batch normalization~\\cite{bn} on $\\bm{h}_{in}^k$: \n\\vspace{-0.5em}\n\\begin{equation}\n\\vspace{-0.5em}\n\\label{eqn:IN}\n\\bar{\\bm{h}}^k = \\frac{\\bm{h}_{in}^k - \\bm{\\mu}^k}{\\bm{\\sigma}^k}.\n\\end{equation}\nHere $\\bm{\\mu}^k \\in \\mathbb{R}^{C^k}$ and $\\bm{\\sigma}^k \\in \\mathbb{R}^{C^k}$ are the means and standard deviations of the channel-wise activations within $\\bm{h}_{in}^k$'s mini-batch. Then, we design 3 parallel branches from $\\bar{\\bm{h}}^k$ for 1) attributes integration, 2) identity integration, 3) adaptively attention mask.\n\nFor attributes embedding integration, let $\\bm{z}_{att}^k$ be the attributes embedding on this feature level, which should be a 3D tensor of size $C^k_{att} \\times H^k \\times W^k$.\nIn order to integrate $\\bm{z}_{att}^k$ into the activation, we compute an attribute activation $\\bm{A}^k$ by denormalizing the normalized $\\bar{\\bm{h}}^k$ according to the attributes embedding, formulated as\n\\vspace{-0.5em}\n\\begin{equation}\n\\vspace{-0.5em}\n\\label{eqn:SPADE}\n\\bm{A}^k = \\gamma^k_{att} \\otimes \\bar{\\bm{h}}^k + \\beta_{att}^k,\n\\end{equation}\nwhere $\\gamma^k_{att}$ and $\\beta^k_{att}$ are two modulation parameters both convolved from $\\bm{z}^k_{att}$. They share the same tensor dimensions with $\\bar{\\bm{h}}^k$. \nThe computed $\\gamma^k_{att}$ and $\\beta^k_{att}$ are multiplied and added to $\\bar{\\bm{h}}^k$ element-wise.\n\nFor identity embedding integration, let $\\bm{z}_{id}^k$ be the identity embedding, which should be a 1D vector of size $C_{id}$.\nWe also integrate $\\bm{z}_{id}^k$ by computing an identity activation $\\bm{I}^k$ in a similar way to integrating attributes. It is formulated as\n\\vspace{-0.5em}\n\\begin{equation}\n\\vspace{-0.5em}\n\\label{eqn:AdaIN}\n\\bm{I}^k = \\gamma^k_{id} \\otimes \\bar{\\bm{h}}^k + \\beta_{id}^k,\n\\end{equation}\nwhere $\\gamma^k_{id} \\in \\mathbb{R}^{C^k}$ and $\\beta^k_{id} \\in \\mathbb{R}^{C^k}$ are another two modulation parameters generated from $\\bm{z}_{id}$ through FC layers. \n\nOne key design of the AAD layer is to adaptively adjust the effective regions of the identity embedding and the attributes embedding, so that they can participate in synthesizing different parts of the face. For example, the identity embedding should focus relatively more on synthesizing the face parts that are most discriminative for distinguishing identities, \\eg eyes, mouth and face contour. \nTherefore, we adopt an attention mechanism into the AAD layer. \nSpecifically, we generate an attentional mask $\\bm{M}^k$ using $\\bar{\\bm{h}}^k$ through convolutions and a sigmoid operation. The values of $\\bm{M}^k$ are between $0$ and $1$.\n\nFinally, the output of this AAD layer $\\bm{h}^k_{out}$ can be obtained as a element-wise combination of the two activations $\\bm{A}^k$ and $\\bm{I}^k$, weighted by the mask $\\bm{M}^k$, as shown in Figure~\\ref{fig:framework_stage1}(c). It is formulated as\n\\vspace{-0.5em}\n\\begin{equation}\n\\vspace{-0.5em}\n\\label{eqn:Mask}\n \\bm{h}^k_{out} = (1 - \\bm{M}^k) \\otimes \\bm{A}^k + \\bm{M}^k \\otimes \\bm{I}^k.\n\\end{equation}\n\nThe AAD-Generator is then built with multiple AAD layers.\nAs illustrated in Figure \\ref{fig:framework_stage1}(a), after extracting the identity embedding $\\bm{z}_{id}$ from source $X_s$, and the attributes embedding $\\bm{z}_{att}$ from target $X_t$, we cascade AAD Residual Blocks (AAD ResBlks) to generate the swapped face $\\hat{Y}_{s,t}$, the structure of the AAD ResBlks is shown in Figure~\\ref{fig:framework_stage1}(b). For the AAD ResBlk on the $k$-th feature level, it first takes the up-sampled activation from the previous level as input, then integrates this input with $\\bm{z}_{id}$ and $\\bm{z}^k_{att}$. The final output image $\\hat{Y}_{s,t}$ is convolved from the last activation.\n\n\\noindent\\textbf{Training Losses}\nWe utilize adversarial training for AEI-Net.\nLet $\\mathcal{L}_{adv}$ be the adversarial loss for making $\\hat{Y}_{s,t}$ realistic. It is implemented as a multi-scale discriminator~\\cite{park2019semantic} on the downsampled output images.\nIn addition, an identity preservation loss is used to preserve the identity of the source. It is formulated as\n\\vspace{-0.5em}\n\\begin{equation}\n\\vspace{-0.5em}\n\\label{eqn:L_id}\n\\mathcal{L}_{id} =  1 - cos(\\bm{z}_{id}(\\hat{Y}_{s, t}), \\bm{z}_{id}(X_s) ),\n\\end{equation}\nwhere $cos(\\cdot,\\cdot)$ represents the cosine similarity of two vectors. \nWe also define the attributes preservation loss as $\\mathcal{L}$-2 distances between the multi-level attributes embeddings from $X_t$ and $\\hat{Y}_{s,t}$.\nIt is formulated as\n\\vspace{-0.5em}\n\\begin{equation}\n\\vspace{-0.5em}\n\\label{eqn:L_att}\n\\mathcal{L}_{att} = \\frac{1}{2} \\sum_{k=1}^n \\left\\|\\bm{z}_{att}^k(\\hat{Y}_{s,t}) - \\bm{z}_{att}^k(X_t)\\right\\|_2^2.\n\\end{equation}\nWhen the source and target images are the same in a training sample, we define a reconstruction loss as pixel level $\\mathcal{L}$-2 distances between the target image $X_t$ and $\\hat{Y}_{s,t}$\n\\vspace{-0.5em}\n\\begin{equation}\n\\vspace{-0.5em}\n\\label{eqn:L_rec1}\n\\mathcal{L}_{rec} = \n\\begin{cases}\n\\frac{1}{2} \\left\\|\\hat{Y}_{s,t} - X_t\\right\\|_2^2 & \\textit{if}~X_t = X_s \\\\\n0 & \\textit{otherwise}\n\\end{cases}.\n\\end{equation}\nThe AEI-Net is finally trained with a weighted sum of above losses as\n\\vspace{-0.5em}\n\\begin{equation}\n\\vspace{-0.5em}\n\\label{eqn:L_stage1}\n\\mathcal{L}_\\texttt{AEI-Net} = \\mathcal{L}_{adv} + \\lambda_{att} \\mathcal{L}_{att} + \\lambda_{id} \\mathcal{L}_{id}  + \\lambda_{rec} \\mathcal{L}_{rec},\n\\end{equation}\nwith $\\lambda_{att}=\\lambda_{rec}=10, \\lambda_{id}=5$.\nThe trainable modules of AEI-Net include the Multi-level Attributes Encoder and the ADD-Generator.\n\n\\subsection{Heuristic Error Acknowledging Refinement Network}\n\\label{sec: error_refine}\n\n\\begin{figure}[t]\n\\centering\n \\includegraphics[width=\\linewidth]{figures/method/deepfake_method_stage2_v6.pdf}\n \\footnotesize\n    \\caption{HEAR-Net for the second stage. $\\hat{Y}_{t,t}$ is the reconstruction of the target image $X_t$, \\ie, $\\hat{Y}_{t,t}=\\texttt{AEI-Net}(X_t, X_t)$. $\\hat{Y}_{s,t}$ is the swapped face from the first stage.}\n\\label{fig:framework_stage2}\n\\end{figure}\n\nAlthough the face swap result $\\hat{Y}_{s,t}$ generated with AEI-Net in the first stage can well retain target attributes like pose, expression and scene lighting, it often fails to preserve the occlusions appeared on the target face $X_t$.\nPrevious methods \\cite{nirkin2018face,nirkin2019fsgan} address face occlusions with an additional face segmentation network. It is trained on face data containing occlusion-aware face masks, which require lots of manual annotations. Besides, such a supervised approach may hardly recognize unseen occlusion types.\n\nWe proposed a heuristic method to handle facial occlusions. As shown in Figure~\\ref{fig:framework_stage2}(a), when the target face was occluded, some occlusions might disappear in the swapped face, \\eg., the hair covering the face or the chains hang from the turban. Meanwhile, we observe that if we feed the same image as both the source and target images into a well trained AEI-Net, these occlusions would also disappear in the reconstructed image. Thus, the error between the reconstructed image and its input can be leveraged to locate face occlusions. We call it the \\emph{heuristic error} of the input image, since it heuristically indicates where anomalies happen. \n\nInspired by the above observation, we make use of a novel HEAR-Net to generate a refined face image. \nWe first get the heuristic error of the target image as\n\\vspace{-0.5em}\n\\begin{equation}\n\\vspace{-0.5em}\n\\Delta Y_t = X_t - \\texttt{AEI-Net}(X_t, X_t).\n\\end{equation}\nThen we feed the heuristic error $\\Delta Y_t$ and the result of the first stage $\\hat{Y}_{s,t}$ into a U-Net structure, and obtain the refined image $Y_{s,t}$:\n\\vspace{-0.5em}\n\\begin{equation}\n\\vspace{-0.5em}\n\\label{eqn:Y_s_t}\nY_{s,t} = \\texttt{HEAR-Net}(\\hat{Y}_{s,t}, \\Delta Y_t).\n\\end{equation}\nThe pipeline of HEAR-Net is illustrated in Figure \\ref{fig:framework_stage2}(b).\n\nWe train HEAR-Net in a fully self-supervised way, without using any manual annotations. \nGiven any target face image $X_t$, with or without occlusion regions, we utilize the following losses for training HEAR-Net. The first is an identity preservation loss to preserve the identity of the source. Similar as stage one, it is formulated as \n\\vspace{-0.5em}\n\\begin{equation}\n\\vspace{-0.5em}\n\\label{eqn:L_id2}\n\\mathcal{L}'_{id} =  1 - cos(\\bm{z}_{id}(Y_{s, t}), \\bm{z}_{id}(X_s) ).\n\\end{equation}\nThe change loss $\\mathcal{L}'_{chg}$ guarantees the consistency between the results of the first stage and the second stage:\n\\vspace{-0.5em}\n\\begin{equation}\n\\vspace{-0.5em}\n\\label{eqn:L_change}\n\\mathcal{L}'_{chg} =  \\left|\\hat{Y}_{s,t} - Y_{s,t}\\right|.\n\\end{equation}\nThe reconstruction loss $\\mathcal{L}'_{rec}$ restricts that the second stage is able to reconstruct the input when the source and target images are the same:\n\n\\vspace{-0.5em} \n\\begin{equation}\n\\vspace{-0.5em}\n\\label{eqn:L_rec}\n\\mathcal{L}'_{rec} = \n\\begin{cases}\n \\frac{1}{2} \\left\\|Y_{s,t} - X_t\\right\\|_2^2 & \\textit{if}~ X_t = X_s \\\\\n0 & \\textit{otherwise}\n\\end{cases}.\n\\end{equation} \n\nSince the number of occluded faces is very limited in most face datasets, we propose to augment data with synthetic occlusions.\nThe occlusions are randomly sampled from a variety of datasets, including the EgoHands \\cite{bambach2015lending}, GTEA Hand2K \\cite{fathi2011learning,li2015delving,li2013learning} and ShapeNet \\cite{chang2015shapenet}.\nThey are blended onto existing face images after random rotations, rescaling and color matching. Note that \\emph{we do not utilize any occlusion mask supervision during training, even from these synthetic occlusions}.\n\nFinally, HEAR-Net is trained with a sum of above losses: \n\\vspace{-0.5em}\n\\begin{equation}\n\\vspace{-0.5em}\n\\label{eqn:L_stage2}\n\\mathcal{L}_\\texttt{HEAR-Net} = \\mathcal{L}'_{rec} +\\mathcal{L}'_{id} + \\mathcal{L}'_{chg}.\n\\end{equation}\n\n\\section{Experiments}\n\n\\noindent\\textbf{Implementation Detail}: \nFor each face image, we first align and crop the face using five point landmarks extracted with \\cite{chen2014joint}, the cropped image is of size $256\\times 256$ covering the whole face, as well as some background regions. The number of attribute embeddings in AEI-Net is set to $n=8$ (Equation~\\ref{eqn:attribute_embedding}). The number of downsamples/upsamples in HEAR-Net is set to $5$. Please refer to the supplemental material for more details concerning the network structure and training strategies.\n\nThe AEI-Net is trained using CelebA-HQ \\cite{karras2017progressive}, FFHQ \\cite{karras2019style} and VGGFace \\cite{parkhi2015deep}. \nWhile the HEAR-Net is trained using only a portion of faces that have Top-$10\\%$ heuristic errors in these datasets, \nand with additional augmentations of synthetic occlusions. \nOcclusion images are randomly sampled from the EgoHands \\cite{bambach2015lending}, GTEA Hand2K \\cite{fathi2011learning,li2015delving,li2013learning} and object renderings from ShapeNet \\cite{chang2015shapenet}. \n\n\\subsection{Comparison with Previous Methods}\n\\noindent\\textbf{Qualitative Comparison}:\nWe compare our method with FaceSwap \\cite{faceswap}, Nirkin \\etal \\cite{nirkin2018face}, DeepFakes \\cite{deepfake} and IPGAN \\cite{Bao_ipgan} on the FaceForensics++ \\cite{rossler2019faceforensics++} test images in Figure \\ref{fig:compare_all_no_fsgan}. \nComparison with the latest work FSGAN \\cite{nirkin2019fsgan} is shown in Figure \\ref{fig:compare_fsgan}.\nWe can see that, since FaceSwap, Nirkin \\etal, DeepFakes, and FSGAN all follow the strategy that first synthesizing the inner face region then blending it into the target face, as expected, they suffer from the blending inconsistency. \nAll faces generated by these methods share exactly the same face contours with their target faces, and ignore the source face shapes (Figure \\ref{fig:compare_all_no_fsgan} rows 1-4, Figure \\ref{fig:compare_fsgan} rows 1-2). Besides, their results can not well respect critical informations from the target image, such as the lighting (Figure \\ref{fig:compare_all_no_fsgan} row 3, Figure \\ref{fig:compare_fsgan} rows 3-5), the image resolutions (Figure \\ref{fig:compare_all_no_fsgan} rows 2 and 4). IPGAN \\cite{Bao_ipgan} suffers from decreased resolutions in all samples, due to its single-level attributes representation. IPGAN cannot well preserve expression of the target face, such as the closed eyes (Figure \\ref{fig:compare_all_no_fsgan} row 2).\n\nOur method addresses all these issues well. We achieve higher fidelity by well preserving the face shapes of the source (instead of the target), and faithfully respecting the lighting and image resolution of the target (instead of the source). Our method also has the ability to go beyond FSGAN~\\cite{nirkin2019fsgan} to handle occlusions.\n\n\\begin{figure}[t]\n\\centering\n \\includegraphics[width=\\linewidth,trim={0 8pt 0 0},clip]{figures/experiments/compare_all_hq2.pdf}\n \\footnotesize\n \\caption{Comparison with FaceSwap \\cite{faceswap}, Nirkin \\etal \\cite{nirkin2018face}, DeepFakes \\cite{deepfake}, IPGAN \\cite{Bao_ipgan} on FaceForensics++ \\cite{rossler2019faceforensics++} face images. Our results better preserve the face shapes of the source identities, and are also more faithful to the target attributes (\\eg lightings, image resolutions).}\n\\label{fig:compare_all_no_fsgan}\n\\vspace{-1em}\n\\end{figure}\n\n\\begin{figure}[t]\n\\centering\n \\includegraphics[width=\\linewidth]{figures/experiments/compare_fsgan_single_v7.pdf}\n \\footnotesize\n \\caption{Comparison with FSGAN \\cite{nirkin2019fsgan}. \nBesides the advantages in face quality and fidelity to inputs, our results preserve common occlusions as good as FSGAN. \nPlease also refer to Figures \\ref{fig:teaser}, \\ref{fig:effect_of_hear} and \\ref{fig:wild} for more challenging cases.}\n\\label{fig:compare_fsgan}\n\\vspace{-1em}\n\\end{figure}\n\n\\noindent\\textbf{Quantitative Comparison}:\nThe experiment is constructed on FaceForensics++ \\cite{rossler2019faceforensics++} dataset. For FaceSwap \\cite{faceswap} and DeepFakes \\cite{deepfake}, the test set consists of 10K face images for each method by evenly sampled 10 frames from each video clip. For IPGAN~\\cite{Bao_ipgan}, Nirkin \\etal \\cite{nirkin2018face} and our method, $10K$ face images are generated with the same source and target image pairs as the other methods.\nThen we conduct quantitative comparison with respect to three metrics: \\emph{ID retrieval}, \\emph{pose error} and \\emph{expression error}.\n\n\\begin{table}[t]\n\\begin{center}\n\\small\n\\begin{tabular}{c|ccc}\nmethod & ID retrieval $\\uparrow$ & pose $\\downarrow$ & expression $\\downarrow$  \\\\\n\\hline\nDeepFakes \\cite{deepfake} & 81.96  & 4.14 &2.57\\\\  \nFaceSwap \\cite{faceswap}  & 54.19  & \\textbf{2.51} & 2.14\\\\ \nNirkin \\etal \\cite{nirkin2018face} & 76.57 & 3.29& 2.33\\\\ \nIPGAN \\cite{Bao_ipgan} & 82.41 & 4.04 & 2.50 \\\\ \n\\hline\nOurs & \\textbf{97.38} & 2.96 & \\textbf{2.06} \\\\ \n\\end{tabular}\n\\caption{Comparison on FaceForensics++ videos.}\n\\label{table:ff++}\n\\end{center}\n\\vspace{-2em}\n\\end{table}\n\nWe extract identity vector using a different face recognition model \\cite{wang2018cosface} and adopt the cosine similarity to measure the identity distance.\nFor each swapped face from the test set, we search the nearest face in all FaceForensics++ original video frames and check whether it belongs to the correct source video.\nThe averaged accuracy of all such retrievals is reported as the \\emph{ID retrieval} in Table \\ref{table:ff++}, serving to measure identity preservation ability. \nOur method achieves higher \\emph{ID retrieval} score with a large margin.\n\nWe use a pose estimator \\cite{ruiz2018fine} to estimate head pose and a 3D face model \\cite{chaudhuri2019joint} to retrieve expression vectors. We report the $\\mathcal{L}$-2 distances of pose and expression vectors between the swapped face and its target face in Table \\ref{table:ff++} as the \\emph{pose} and the \\emph{expression} errors. Our method is advantageous in expression preservation while comparable with others in pose preservation. We do not use the face landmark comparison as~\\cite{nirkin2019fsgan}, since face landmarks involve identity information which should be inconsistent between the swapped face and the target face.\n\n\\noindent\\textbf{Human Evaluation}: Three user studies are conducted to evaluate the performance of the proposed model. We let the users select: i) \\emph{the one having the most similar identity with the source face}; ii) \\emph{the one sharing the most similar head pose, face expression and scene lighting with the target image}; iii) \\emph{the most realistic one}. \nIn each study unit, two real face images, the source and the target, and four reshuffled face swapping results generated by FaceSwap \\cite{faceswap}, Nirkin \\etal \\cite{nirkin2018face}, DeepFakes \\cite{deepfake} and ours, are presented. We ask users to select one face that best matches our description. \n\n\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=\\linewidth]{figures/experiments/ablation_aei_v3.pdf}\n\t\\footnotesize\n\t\\caption{Comparing AEI-Net with three baseline models. The two models \\emph{Add} and \\emph{Cat} are for ablation studies of the adaptive embedding integration. The model \\emph{Compressed} is for ablating multi-level attributes representation.}\n\t\\label{fig:ablation_aei}\n\\end{figure}\n\n\\begin{table}[t]\n\\begin{center}\n\\small\n\\begin{tabular}{c|cccc}\nmethod &  id. & attr. & realism  \\\\\n\\hline\nDeepFakes \\cite{deepfake} & 13.7 & 6.8 & 6.1 \\\\  \nFaceSwap \\cite{faceswap}  & 12.1 & 23.7 & 6.8 \\\\ \nNirkin \\etal \\cite{nirkin2018face} & 21.3 & 7.4 & 4.2\\\\ \nOurs & \\textbf{52.9} & \\textbf{62.1} & \\textbf{82.9} \\\\ \n\\end{tabular}\n\\caption{User study results. We show the averaged selection percentages of each method.}\n\\label{table:user_study}\n\\end{center}\n\\vspace{-2em}\n\\end{table}\n\nFor each user, 20 face pairs are randomly drawn from the 1K FaceForensics++ test set without duplication.\nFinally, we collect answers from 100 human evaluators. The averaged selection percentage for each method on each study is presented in Table \\ref{table:user_study}.\nIt shows that our model surpasses the other three methods all in large margins.\n\n\\subsection{Analysis of the Framework}\n\\label{ssec:analysis}\n\n\\noindent\\textbf{Adaptive Embedding Integration}: \nTo verify the necessity of adaptive integration using attentional masks, we compare AEI-Net with two baseline models:\ni) \\emph{Add}: element-wise plus operations is adopted in AAD layers instead of using masks $\\bm{M}^k$ as in Equation \\ref{eqn:Mask}. The output activation $\\bm{h}_{out}^k$ of this model is directly calculated with $\\bm{h}_{out}^k = \\bm{A}^k + \\bm{I}^k$;\nii) \\emph{Cat}: element-wise concatenation is adopted without using masks $\\bm{M}^k$. The output activation becomes $\\bm{h}_{out}^k = \\texttt{Concat}[\\bm{A}^k, \\bm{I}^k]$. Results of the two baseline models, as well as the AEI-Net, are compared in Figure \\ref{fig:ablation_aei}. \nWithout a soft mask for fusing embeddings adaptively, the faces generated by baseline models are relatively blurry and contain lots of ghosting artifacts. \n\nWe also visualize the masks $\\bm{M}^k$ of AAD layers on different levels in Figure \\ref{fig:visualize_aad_masks}, where a brighter pixel indicates a higher weight for identity embedding in Equation \\ref{eqn:Mask}. It shows that the identity embedding takes more effect in low level layers. Its effective region becomes sparser in middle levels, where it activates only in some key regions that strongly relates to the face identity, such as the locations of eyes, mouth and face contours.\n\n\\begin{figure}[t]\n\\centering\n \\includegraphics[width=\\linewidth]{figures/experiments/aad_masks_v5ehq.pdf}\n    \\caption{Visualizing attentional masks $\\bm{M}^k$ of AAD layers on different feature levels. These visualizations reflect that identity embeddings are mostly effective in low and middle feature levels.}\n\\label{fig:visualize_aad_masks}\n\\end{figure}\n\n\\begin{figure}[t]\n\\centering\n \\includegraphics[width=\\linewidth]{figures/experiments/attribute_ret_v4.pdf}\n \\footnotesize\n    \\caption{Query results using attributes embedding.}\n\\label{fig:att_query}\n\\vspace{-0.5em}\n\\end{figure}\n\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=\\linewidth,trim={0 4pt 0 0},clip]{figures/experiments/hearnet_v5ehq2.pdf}\n\\caption{Second-stage refining results presenting the strong adaptability of HEAR-Net on various kinds of errors, including occlusions, reflections, slightly shifted pose and color etc.}\n\\label{fig:effect_of_hear}\n\\end{figure*}\n\n\\begin{figure*}[t]\n\\centering\n \\includegraphics[width=\\linewidth]{figures/experiments/wild_v6.pdf}\n    \\caption{Our face swapping results on wild face images under various challenging conditions. All results are generated using a single well-trained two-stage model.}\n\\label{fig:wild}\n\\vspace{-0.5em}\n\\end{figure*}\n\n\\noindent\\textbf{Multi-level Attributes}:\nTo verify whether it is necessary to extract multi-level attributes, we compare with another baseline model called \\emph{Compressed}, which shares the same network structure with AEI-Net, \nbut only utilizes the first three level embeddings $\\bm{z}_{att}^k, k=1,2,3$. Its last embedding $\\bm{z}_{att}^3$ is fed into all higher level AAD integrations.  \nIts results are also compared in Figure \\ref{fig:ablation_aei}. Similar to IPGAN \\cite{Bao_ipgan}, its results suffer from artifacts like blurriness, since a lot of attributes information from the target images are lost. \n\nTo understand what is encoded in the attributes embedding, we concatenate the embeddings $\\bm{z}_{att}^k$ (bilinearly upsampled to $256\\times 256$ and vectorized) from all levels as a unified attribute representation. We conduct PCA to reduce vector dimensions as $512$. \nWe then perform tests querying faces from the training set with the nearest $\\mathcal{L}$-2 distances of such vectors. The three results illustrated in Figure \\ref{fig:att_query} verify our intention, that the attributes embeddings can well reflect face attributes, such as the head pose, hair color, expression and even the existence of sunglasses on the face. Thus it also explains why our AEI-Net sometimes can preserve occlusions like sunglasses on the target face even without a second stage (Figure \\ref{fig:effect_of_hear}(8)).\n\n\\noindent\\textbf{Second Stage Refinement}:\nMultiple samples are displayed with both one-stage results $\\hat{Y}_{s,t}$ and two-stage results $Y_{s,t}$ in Figure \\ref{fig:effect_of_hear}. \nIt shows that the AEI-Net is able to generate high-fidelity face swapping results, but sometimes its output $\\hat{Y}_{s,t}$ does not preserve occlusions in the target.\nFortunately, the HEAR-Net in the second stage is able to recover them. \n\nThe HEAR-Net can handle occlusions of various kinds, such as the medal (1), hand (2), hair (3), face painting (4), mask (5), translucent object (6), eyeglasses (7), headscarf (8) and floating text (9). \nBesides, it is also able to correct the color-shift that might occasionally happen in $\\hat{Y}_{s,t}$ (10). \nMoreover, the HEAR-Net can help rectify the face shape when the target face has a very large pose (6).\n\n\\subsection{More Results on Wild Faces}\n\nFinally, we demonstrate the strong capability of FaceShifter by testing on wild face images downloaded from Internet. \nAs shown in Figure \\ref{fig:wild}, our method can handle face images under various conditions, including large poses, uncommon lightings and occlusions of very challenging kinds. \n\n\\section{Conclusions}\n\nIn this paper, we proposed a novel framework named FaceShifter for high fidelity and occlusion aware face swapping. The AEI-Net in the first stage adaptively integrates the identity and the attributes for synthesizing high fidelity results. The HEAR-Net in the second stage recovers anomaly region in a self-supervised way without any manual annotations.  \nThe proposed framework shows superior performance in generating realistic face images given any face pairs without subject specific training. \nExtensive experiments demonstrate that the proposed framework significantly outperforms previous face swapping methods.\n\n{\\small\n}\n\n\\cleardoublepage\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{FlowFace: Semantic Flow-guided Shape-aware Face Swapping}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nIn this work, we propose a semantic flow-guided two-stage framework for shape-aware face swapping, namely FlowFace.\nUnlike most previous methods that focus on transferring the source inner facial features but neglect facial contours, our FlowFace can transfer both of them to a target face, thus leading to more realistic face swapping.\nConcretely, our FlowFace consists of a face reshaping network and a face swapping network. \nThe face reshaping network addresses the shape outline differences between the source and target faces. \nIt first estimates a semantic flow (\\emph{i.e.}, face shape differences) between the source and the target face, and then explicitly warps the target face shape with the estimated semantic flow. \nAfter reshaping, the face swapping network generates inner facial features that exhibit the identity of the source face. \nWe employ a pre-trained face masked autoencoder (MAE) to extract facial features from both the source face and the target face. \nIn contrast to previous methods that use identity embedding to preserve identity information, the features extracted by our encoder can better capture facial appearances and identity information. Then, we develop a cross-attention fusion module to adaptively fuse inner facial features from the source face with the target facial attributes, thus leading to better identity preservation.\nExtensive quantitative and qualitative experiments on in-the-wild faces demonstrate that our FlowFace outperforms the state-of-the-art significantly.\n\\end{abstract}\n\n\\section{Introduction}\nFace swapping refers to transferring the identity information of a source face to a target face while maintaining the attributes (e.g., expression, pose, hair, lighting, and background) of the target. \nIt has attracted many interests due to its wide applications, such as portrait reenactment, film production, and virtual reality. \n\nRecent works~\\cite{li2019faceshifter,chen2020simswap,xu2021facecontroller,li2021faceinpainter} have made great efforts to achieve promising face swapping results. \nHowever, these methods often focus on inner facial feature transferring but neglect facial contour reshaping.\nWe observe that facial contours also carry the identity information of a person, but few efforts~\\cite{wang2021hififace} have been made on facial contours transferring. \nFacial shape transferring is still a challenge for authentic face swapping.\n\nTo solve the shape transferring problem, we propose a semantic flow-guided two-stage framework, dubbed FlowFace. Unlike existing methods, FlowFace is a shape-aware face swapping network. \nIn a nutshell, we first present a face reshaping network to warp the target face referring to the source face shape at the first stage. Then, based on the reshaped facial contour, we further employ a face swapping network to transfer the inner facial features to the reshaped target face.\n\nOur face reshaping network addresses the shape outline discrepancy between the source face and the target face.\nSpecifically, we use a 3D face reconstruction model (\\emph{i.e.}, 3DMM \\cite{blanz19993DMM}) to obtain shape coefficients of the source and target faces and then project the obtained 3D shapes to 2D facial landmarks. \nTo accurately warp the target face, we need to obtain dense motion between the source and the target faces. Subsequently, we design a semantic guided generator to transform the sparse 2D facial landmarks into the dense flow.\nThe estimated flow, called semantic flow, will be exploited to warp the target face shape explicitly in a pixel-wise manner. In addition, we propose a semantic-guided discriminator to enforce our face reshaping network to produce accurate semantic flow.\n\nAfter reshaping the target face, we introduce a face swapping network for transferring the inner facial features of the source face to the target ones. \nPrior works usually use a face recognition model to extract the identity embedding of the source face and then transfer it to the target face. \nWe argue this would lose some personalized appearances during transferring because the identity embedding is often trained under discriminative tasks and thus may ignore intra-class variations~\\cite{kim2022smooth}.\nThus, we opt to employ a pre-trained masked autoencoder (MAE)~\\cite{he2022mae} to extract facial features that better capture facial appearances and identity information. \nMoreover, unlike prior arts that widely employ AdaIN~\\cite{liu2017adaptive} to infuse the source identity embedding to the target face, we develop a cross-attention fusion module to adaptively fuse the source and target features.\nIn doing so, we achieve better face swapping performance.\n\nExtensive quantitative and qualitative experiments validate the effectiveness of our FlowFace on in-the-wild faces and our FlowFace outperforms the state-of-the-art.\nOverall, our contributions are summerized as follows:\n\\begin{itemize}\n    \\item We propose a two-stage framework for shape-aware face swapping, namely FlowFace. It can effectively transfer both the inner facial features and the facial outline to a target face, thus achieving authentic face swapping results.\n    \\item We design a semantic flow-guided face reshaping network and validate its effectiveness in transferring the source face shapes to the target ones. The reshaped target faces are more similar to the source faces in terms of face contours.\n    \\item We design a pre-trained face masked autoencoder based face swapping network. The encoder captures not only identity information but also facial appearance, thus allowing us to transfer richer information from the source face to the target and achieve identity similarity.\n    \\item We design a cross-attention fusion module to adaptively fuse the source and target features. To the best of our knowledge, we are the first to perform face swapping in the latent space of the pre-trained masked autoencoder.\n\\end{itemize}\n\\section{Related Work}\n \nThe previous face swapping methods can be classified as the target attribute-guided and source identity-guided methods.\n\n\\textbf{Target attribute-guided methods} edit the source face first and then blend it to the target background. \nEarly methods~\\cite{bitouk2008face,chen2019face,lin2012face} directly warp the source face according to the target facial landmarks, thus failing to address large posture differences and expression differences. \n3DMM-based methods~\\cite{blanz2004exchanging,thies2016face2face,faceswap,nirkin2018face} swap faces by 3D-fitting and re-rendering. However, these methods often cannot handle skin color or lighting differences and suffer from poor fidelity. Later, GAN-based methods improve the fidelity of the generated faces. Deepfakes~\\cite{deepfakes} transfers the target attributes to the source face by an encoder-decoder structure while being constrained by two specific identities.\nFSGAN\\cite{nirkin2019fsgan} employs the target facial landmarks to animate the source face and proposes a blending network to fuse the generated source face to the target background. However, it fails to tackle drastic skin color differences. \nLater, AOT~\\cite{aot2020neurips} focuses on swapping faces with large differences in skin color and lighting by formulating appearance mapping as an optimal transport problem.\nThese methods always need a facial mask to blend the generated face with the target background. However, the mask-guided blending restricts the face shape change.\n\n\\textbf{Source identity-guided methods} usually adopt the identity embedding or the latent representation of StyleGAN2~\\cite{karras2020analyzing} to represent the source identity and inject in into the target face. FaceShifter~\\cite{li2019faceshifter} designs an adaptive attentional denormalization generator to integrate the source identity embedding and the target features. SimSwap~\\cite{chen2020simswap} introduces a weak feature matching loss to help preserve the target attributes.  MegaFS~\\cite{zhu2021one}, RAFSwap~\\cite{xu2022region} and HighRes~\\cite{xu2022high} utilize the pre-trained StyleGAN2 to swap faces and can achieve high-resolution face swapping.   FaceController~\\cite{xu2021facecontroller} exploits the identity embedding with 3D priors to represent the source identity and design a unified framework for identity swapping and attribute editing. InfoSwap~\\cite{gao2021information} leverages the information bottleneck principle to disentangle the identity and identity-irrelevant information. FaceInpainter~\\cite{li2021faceinpainter} also utilizes the identity embedding with 3D priors to implement controllable face in-painting under heterogeneous domains. Smooth-Swap~\\cite{kim2022smooth} builds smooth identity embedding that makes the training of face swapping fast and stable.\n \nHowever, most of these methods neglect the facial outlines during face swapping. Recently, HifiFace~\\cite{wang2021hififace} can control the face shape using a 3D shape-aware identity. However, it injects the shape representation into the latent feature space, making it hard for the model to correctly decode the face shape.\nMoreover, these methods always need a pre-trained face recognition model during the inference time, which is not friendly to deployment.\n\n\\section{Proposed Method}\n\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=1.0\\textwidth]{figures/arch_pipeline_V5.pdf}  \n\\caption{Overview of our two-stage FlowFace. In the first stage, the face reshaping network ($F^{res}$) transfers the shape of the source face $I_s$ to the target face $I_t$ by warping $I_t$ explicitly with an estimated semantic flow $V_t$. In the second stage, the face swapping network ($F^{swa}$) generates the inner facial details by manipulating the latent face representation $e_s$ and $e_t$ using our designed cross-attention fusion module. It should be noted that \\textcircled{c} in the figure represents the concatenation operation.}\n\\label{fig:arch}\n\\end{figure*}\n\nThe face swapping task aims to generate a face with the identity of the source face and the attributes of the target face. This paper proposes a semantic flow-guided two-stage framework for shape-aware face swapping, namely FlowFace. As shown in Figure~\\ref{fig:arch}, FlowFace consists of a face reshaping network $F^{res}$ and a face swapping network $F^{swa}$. Let $I_{s}$ be the source face and $I_{t}$ be the target face. $F^{res}$ first transfers the shape of $I_s$ to the target face $I_t$. The reshaped image is denoted as $I^{res}_{t}$. Then $F^{swa}$ generates the inner face of $I^{res}_{t}$ and outputs the result image $I_{o}$.\n\n\\subsection{Face Reshaping Network}\nWe design the face reshaping network, $F^{res}$, to address the shape discrepancy between the source and target faces. It warps the target face shape explicitly pixel-wise with an estimated semantic flow. To achieve this goal, $F^{res}$ requires a face shape representation that models the shape differences between the source and target faces. Then it estimates a semantic flow according to the above shape differences. Finally, the semantic flow is used to warp the target face shape.\n\n\\subsubsection{Face Shape Representation.}\\label{para:face_shape_representation} \nSince our face reshaping network needs to warp the face shape pixel-wisely, we choose the explicit facial landmarks as the shape representation. We use a 3D face reconstruction model to obtain facial landmarks. As shown in Figure~\\ref{fig:arch}, the 3D face reconstruction model $E_{3D}$ extracts 3D coefficients of the source and target: \n\\begin{equation}\n\\begin{split}\n(\\beta_*, \\theta_*, \\psi_*, c_*)=E_{3D}(I_*),\n\\end{split}\n\\end{equation}\nwhere $\\beta_*, \\theta_*, \\psi_*, c_*$ are the FLAME coefficients~\\cite{FLAME:SiggraphAsia2017} representing the face shape, pose, expression, and camera, respectively. $*$ is $s$ or $t$, representing the source or the target, respectively. With these coefficients, the target face can be modeled as:\n\\begin{equation}\nM_t(\\beta_t, \\theta_t, \\psi_t)=W\\left(T_P(\\beta_t, \\theta_t, \\psi_t), \\mathbf{J}(\\beta_t), \\theta_t, \\mathcal{W}\\right),\n\\end{equation}\nwhere $M_t$ represents the 3D face mesh of the target face. $W$ is a linear blend skinning (LBS) function that is applied to rotate the vertices of $T_P$ around joint $J$. $\\mathcal{W}$ is the blend weights. $T_P$ denotes the template mesh $\\overline{T}$ with shape, pose, and expression offsets~\\cite{FLAME:SiggraphAsia2017}.\n\nThen, we reconstruct the source face similarly, except that the source pose and expression coefficients are replaced with the target ones. The obtained 3D face mesh is denoted as $M_{s2t}$. Finally, we sample 3D facial landmarks from $M_t$ and $M_{s2t}$ and project these 3D points to 2D facial landmarks with the target camera parameter $c_t$:\n\\begin{equation}\n\\begin{split}\n    P_{t} =s \\Pi\\left(M_{t}^{i}\\right)+t, \\\\\n    P_{s2t} =s \\Pi\\left(M_{s2t}^{i}\\right)+t,\n\\end{split}\n\\end{equation}\nwhere $M_{*}^{i}$ is a vertex in $M_{*}$, $\\Pi$ is an orthographic 3D-2D projection matrix, and $s$ and $t$ are parameters in $c_t$, indicating isotropic scale and 2D translation. $P_{*}$ denotes the 2D facial landmarks. It should be noted that we only use the landmarks at the facial contours as the shape representation since inner facial landmarks contain identity information that may influence the reshaping result. \n\n\\subsubsection{Semantic Flow Estimation.}\\label{para:se_flow_estimation}\n\nThe relative displacement between $P_t$ and $P_{s2t}$ only describes sparse movement.\nTo accurately warp the target face, we need to obtain dense motion between the source and the target faces.\nTherefore, we propose the semantic flow, which models the semantic correspondences between two faces, to achieve pixel-wised movement. We design a semantic guided generator $G^{res}$ to estimate the semantic flow. Specifically, $G^{res}$ requires three inputs: $P_{s2t}$, $P_t$ and $S_t$, where $P_{s2t}$ and $P_t$ are the 2D facial landmarks obtained above.\n$S_t$ is the target face segmentation map that complements the semantic information lost in facial landmarks. The output of $G^{res}$ is the estimated semantic flow $V_t$, the formulation is:\n\\begin{equation}\nV_t = G^{res}(P_{s2t}, P_t, S_t).\n\\end{equation}\n\nThen, a warping module is introduced to generate the warped faces using $V_t$. We find that an inaccurate flow is likely to produce unnatural images, and therefore, we design a semantic guided discriminator $D^{res}$ that ensures $G^{res}$ to produce a more accurate flow. Specifically, the warping operation is conducted on both $I_t$ and $S_t$:\n\\begin{equation}\n(I_t^{res},S_t^{res}) = F(V_t, I_t, S_t),\n\\end{equation}\nwhere $F$ is the warping function in the warping module. We feed the concatenation of the warped face $I_t^{res}$ and the warped segmentation map $S_t^{res}$ to $D^{res}$. Thus, $D^{res}$ is able to discriminate whether the input is real or fake from the semantic level and the image level. It should be noted that $S_t^{res}$ and $D^{res}$ are only used during training.\n\n\\subsubsection{Training Loss.}\nWe employ three loss functions for $F^{res}$: \n\\begin{equation}\n\\mathcal{L}^{res} = \n\\mathcal{L}_{adv} + \\lambda_{rec}\\mathcal{L}_{rec} + \\lambda_{ldmk}\\mathcal{L}_{ldmk},\n\\label{res_loss_full}\n\\end{equation}\nwhere $\\lambda_{ldmk}$ and $\\lambda_{rec}$ are hyperparameters for each term. In our experiments, we set $\\lambda_{ldmk}$=800 and $\\lambda_{rec}$=10.\n\n\\textbf{Adversarial Loss.}\nTo make the resultant images more realistic, we adopt the hinge version adversarial loss\\cite{lim2017geometric} for training, denoted by $L_{adv}$:\n\\begin{equation}\n\\mathcal{L}_{adv}=-\\mathbb{E}[D^{res}([I_t^{res},S_t^{res}])],\n\\end{equation}\nwhere $D^{res}$ is the discriminator which is trained with:\n\\begin{equation}\n\\begin{split}\n\\mathcal{L}_{D}=\\mathbb{E}[\\max (0,1-D([I_t,S_t]))]\\\\+\\mathbb{E}[\\max (0,1+D([I_t^{res},S_t^{res}])] .\n\\end{split}\n\\end{equation}\n\n\\textbf{Reconstruction Loss.}\nSince there is no ground-truth for face reshaping results, we enforce $I_s = I_t$ with a certain probability when training $G^{res}$. Then the face reshaping task becomes a reconstruction task, and we introduce a pixel-wise reconstruction loss:\n\\begin{equation}\n\\mathcal{L}_{rec}=\\left\\|{I_t^{res} - I_t}\\right\\|_2 \\label{loss_rec},\n\\end{equation}\nwhere $\\left\\|*\\right\\|_2$ denotes the euclidean distance.\n\n\\textbf{Landmark Loss.} Since there is not pixel-wised ground truth for $I_t^{res}$, we exploit the 2D facial landmarks $P_{s2t}$ to constrain the shape of $I_t^{res}$. Specifically, we first use a pre-trained facial landmark detector~\\cite{sun2019high} to predict the the facial landmarks of $I_t^{res}$, denoted as $P_{t}^{res}$. Then the loss is computed as:\n\\begin{equation}\n\\mathcal{L}_{ldmk}=\\left\\|P_{t}^{res} - P_{s2t}\\right\\|_{2}.\\\n\\end{equation}\n\nAt this point, our designed face reshaping network is able to transfer the face shape of the source to the target face. However, the inner facial features are still unchanged.\n\n\\subsection{Face Swapping Network}\nThe face swapping network $F^{swa}$ is used to generate the inner face of $I_t^{res}$ ($I_t$). As shown in Figure~\\ref{fig:arch}, we first utilize a shared face encoder $E_{f}$ to map both $I_s$ and $I_t^{res}$ into patch embeddings $e_s$ nad $e_t$. Then a cross-attention fusion module is designed to adaptively fuse the identity information of the source face and the attribute information of the target. Finally, the facial decoder, fed with the manipulated embeddings $e_o$, outputs the final face swapping result $I_o$.\n\n\\subsubsection{Shared Face Encoder.}\\label{para:encoder}\nMost previous face swapping methods map the source face into an ID embedding with a pre-trained face recognition model and extract the target face attributes with another face encoder. \nHowever, we argue that using two different encoders is unnecessary and even makes deploying more complex. Moreover, the ID embedding is trained on purely discriminative tasks and may lose some personalized appearances during transferring.\n\nTherefore, we employ a shared encoder to project both the source face and the target face into a common latent representation. \nThe encoder is designed following MAE~\\cite{he2022mae} and pre-trained on a large-scale face dataset using the masked training strategy. \nCompared to the compact latent code of StyleGAN2~\\cite{karras2020analyzing} and the identity embedding, the latent space of MAE can better capture facial appearances and identity information, because masked training requires reconstructing masked image patches from visible neighboring patches, thus ensuring each patch embedding contains rich topology and semantic information. \n\nBased on the pre-trained encoder $E_f$, we can project a facial image $I_*$ into a latent representation, also known as patch embeddings:\n\\begin{equation}\ne_* = E_f(I_*),\n\\end{equation}\nwhere $e_* \\in R^{N*L}$. $N$ and $L$ denote the number of patches and the dimension of each embedding, respectively. \n\n\\subsubsection{Cross-Attention Fusion Module.}\\label{para:cross_module}\nThe shared face encoder projects the source face and the target face into a representative latent space. The subsequent operation is to fuse the source identity information with the target attribute in this latent space. \nIntuitively, identity information should be transferred between related patches (\\emph{e.g.}, nose to nose, etc.). Therefore, we design a cross-attention fusion module (CAFM) to adaptively aggregate identity information from the source and fuse it into the target.\n\nAs shown in Figure~\\ref{fig:arch}, our CAFM consists of a cross-attention block and two standard transformer blocks~\\cite{dosovitskiy2020vit}.\nGiven the source patch embeddings $e_{s}$ and the target patch embeddings $e_{t}$, we first compute $Q,K,V$ for each patch embedding in $e_{s}$ and $e_{t}$. Then the cross attention is computed by:\n\\begin{equation}\n\\operatorname{CA}(Q_t, K_s)=\\operatorname{softmax}\\left(\\frac{Q_t K_s^{T}}{\\sqrt{d_{k}}}\\right),\n\\end{equation}\n where $\\operatorname{CA}$ represents Cross Attention, $Q_*, K_*, V_*$ are predicted by attention heads, and $d_k$ is the dimension of $K_*$. \n The cross attention describes the relation between each target patch and the source patches. \n Next, the source identity information is aggregated based on the computed $\\operatorname{CA}$ and fused to the target values via addition:\n\\begin{equation}\nV_{fu}=\\operatorname{CA} * V_s + V_t.\n\\end{equation}\n Then, $V_{fu}$ are normalized by a layer normalization (LN) and processed by multi-layer perceptrons (MLP). \n The Cross Attention and MLP are along with skip connections. The fused embeddings $e_{fu}$ are further fed into two transformer blocks to obtain the final output $e_o$.\n\nFinally, we utilize a a convolutional decoder to generate the final swapped face image $I_o$ from $e_o$. In contrast to the ViT decoder in MAE, we find the convolutional decoder achieves more realistic results.\n\n\\subsubsection{Training Loss.}\nWe employ six loss functions to train our face swapping network $F^{swa}$:\n\\begin{equation}\n\\begin{split}\n\\mathcal{L}^{swa} =  \\mathcal{L}_{adv} + \\lambda_{rec}\\mathcal{L}_{rec} + \\lambda_{id}\\mathcal{L}_{id}  +  \\lambda_{exp}\\mathcal{L}_{exp} \\\\+ \\lambda_{ldmk}\\mathcal{L}_{ldmk} + \\lambda_{perc}\\mathcal{L}_{perc}\n\\end{split}, \n\\label{swap_loss_full}\n\\end{equation}\nwhere $\\lambda_{rec}$, $\\lambda_{id}$, $\\lambda_{exp}$, $\\lambda_{ldmk}$, $\\lambda_{attr}$ are hyperparameters for each term. In our experiment, we set $\\lambda_{rec}$=10, $\\lambda_{id}$=5, $\\lambda_{exp}$=10, $\\lambda_{ldmk}$=5000 and $\\lambda_{attr}$=2.\n\nAs in the face reshaping stage, the adversarial loss is used to make the resultant images more realistic, and the reconstruction loss between $I_o$ and $I_t^{res}$ is used for self-supervision since there is also no ground-truth for face swapping results. \n\n\\textbf{Identity Loss.} The identity loss is used to improve the identity similarity between $I_s$ and $I_o$:\n\\begin{equation}\n\\mathcal{L}_{id} = 1- cos(E_{id}(I_o), E_{id}(I_s)), \\label{loss_id}\n\\end{equation}\nwhere $E_{id}$ denotes a face recognition model~\\cite{deng2019arcface} and $cos$ denotes the cosine similarity. \n\n\\textbf{Posture Loss.} We adopt the landmark loss to constrain the face posture during face swapping:\n\\begin{equation}\n\\mathcal{L}_{ldmk}=\\left\\|P_{t}^{res} - P_{o}\\right\\|_{2},\\\n\\end{equation}\nwhere $P_{o}$ represents the landmarks of $I_o$.\n\n\\textbf{Perceptual Loss.} Since high-level feature maps contain semantic information, we employ the feature maps from the last two convolutional layers of pre-trained VGG as the facial attribute representation. The loss is formulated as:\n\\begin{equation}\n\\mathcal{L}_{perc}=\\left\\|VGG(I_t^{res}) - VGG(I_o)\\right\\|_{2}.\\\n\\end{equation}\n\n\\textbf{Expression Loss.} \nWe utilize a novel fine-grained expression loss~\\cite{zhang2021learning} that penalizes the $\\mathcal{L}_2$ distance of two expression embeddings:\n\\begin{equation}\n\\mathcal{L}_{exp} = \\left\\|{E_{exp}(I_o) - E_{exp}(I_t)}\\right\\|_2. \\label{loss_exp}\n\\end{equation}\n\n\\section{Experiments}\n\n\\begin{table}[t]\n\\setlength{\\abovecaptionskip}{0cm}\n\\setlength{\\belowcaptionskip}{-0.2cm}\n\\centering\n\\caption{Quantitative comparisons with state-of-the-art methods on FF++. \"\\dag\" means the results are cited from their papers. \n}\n\\label{tab:quantitative_comparison}\n\\resizebox{0.45\\textwidth}{!}{\n\\begin{tabular}{c|ccc|c|c|c}\n\\multirow{2}{*}{Methods} & \\multicolumn{3}{c|}{ID Acc($\\%$) $\\uparrow$}                     & \\multicolumn{1}{l|}{\\multirow{2}{*}{Shape$\\downarrow$}} & \\multicolumn{1}{l|}{\\multirow{2}{*}{Expr.$\\downarrow$}} & \\multicolumn{1}{l}{\\multirow{2}{*}{Pose.$\\downarrow$}} \\\\ \\cline{2-4}\n                         & CosFace  & SphereFace & Avg   & \\multicolumn{1}{l|}{}                       & \\multicolumn{1}{l|}{}                       & \\multicolumn{1}{l}{}                       \\\\ \\hline\nDeepfakes              & 83.55     & 86.60      & 85.08 & 1.78                             & 0.54                             & 4.05                             \\\\\nFaceSwap               & 70.95    & 76.77      & 73.86 & 1.85                             & 0.40                             & 2.21                             \\\\\nFSGAN                  & 48.86     & 53.85      & 51.36 & 2.18                             & 0.27                             & 2.20                             \\\\\nFaceShifter            & 97.38\\dag     & 80.64      & 89.01 & 1.68                             & 0.33                             & 2.28                             \\\\\nSimSwap                & 93.63      & 96.22      & 94.43 & 1.74                             & 0.26                             & \\textbf{1.40}  \\\\ \\hline\n $F^{res}$+SimSwap       & 94.31     & 96.82      & 95.56 & 1.67                             & 0.27                             & 2.27                             \\\\ \nHifiFace               & 98.48\\dag     & 90.76      & 94.62 & 1.62                             & 0.30                             & 2.29                            \n\\\\ \\hline\n\n$F^{swa}$          & \\underline{99.18}      & \\underline{98.23}      & \\underline{98.70} & \\underline{1.43}                             & \\textbf{0.21}                             & \\underline{1.99}                             \\\\\nOurs & \\textbf{99.26}      & \\textbf{98.40}      & \\textbf{98.83} & \\textbf{1.17}                             & \\underline{0.22}                             & 2.66             \n\\end{tabular}\n}\n\\end{table}\n\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=0.95\\textwidth]{figures/compare_with_others_ff_v3.pdf}\n\\caption{Qualitative comparisons with Deepfakes, FaceSwap, FSGAN, FaceShifter, SimSwap and HifiFace on FF++. Our FlowFace outperforms the other methods significantly, especially in preserving face shapes, identities, and expressions. \n}\n\\label{fig:FF_comparison}\n\\end{figure*}\n\nOur method is validated through qualitative and quantitative comparisons with state-of-the-art ones and a user study. Moreover, several ablation experiments are also reported to validate our design of FlowFace.\n\n\\subsection{Implementation Details}\n\\noindent{\\textbf{Dataset.}} The training dataset is collected from three commonly-used face datasets: CelebA-HQ~\\cite{karras2017progressive}, FFHQ~\\cite{karras2019style}, and VGGFace2~\\cite{cao2018vggface2}. Faces are aligned and cropped to $256\\times256$.\nParticularly, low-quality faces are removed to ensure high-quality training. The final dataset contains 350K face images, and 10K images are randomly sampled as the validation dataset. For the comparison experiments, we construct the test set by sampling FaceForensics++(FF++)~\\cite{roessler2019faceforensicspp}, following~\\cite{li2019faceshifter}. Specifically, FF++ consists of 1000 video clips, and the test set is collected by sampling ten frames from each clip of FF++, in a total of 10000 images.  \n\n\\noindent{\\textbf{Training.}} Our FlowFace is trained in a two-stage manner. Specifically, $F^{res}$ is first trained for 32K steps with a batch size of eight. As for $F^{swa}$, we first pre-trained the face encoder following the training strategy of MAE on our face dataset. Then we fix the encoder and train other components of $F^{swa}$ for 640K steps with a batch size of eight. \nWe adopt Adam~\\cite{kingma2014adam} optimizer with $\\beta_1$=0 and $\\beta_2$=0.99 and the learning rate is set to 0.0001. \nMore details are in the supplementary materials and our codes will be made publicly available upon publication of the paper.\n\n\\noindent{\\textbf{Metrics.}} The quantitative evaluations are performed in terms of four metrics: identity retrieval accuracy (ID Acc), shape error, expression error (Expr Error), and pose error.\nWe follow the same test protocol in~\\cite{li2019faceshifter,wang2021hififace}. However, since some pre-trained models used in their testing are not available, we leverage different ones. For ID Acc, we employ two face recognition models, including CosFace~\\cite{wang2018cosface} and SphereFace~\\cite{liu2017sphereface}, to perform identity retrieval for a more comprehensive comparison. For expression error, we adopt a different expression embedding model~\\cite{vemulapalli2019compact} to compute the euclidean distance of expression embeddings between the target and swapped faces.\n\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=1.0\\textwidth]{figures/compare_with_others_v3.pdf}\n\\caption{Qualitative comparisons with more methods including  MegaFS~\\cite{zhu2021one}, FaceInpainter~\\cite{li2021faceinpainter}, HighRes~\\cite{xu2022high} and SmoothSwap~\\cite{kim2022smooth}. The shown images of the compared methods are cropped from their original papers or their released results.\n}\n\\label{fig:additional_qualitative}\n\\end{figure*}\n\n\\subsection{Comparisons with the State-of-the-art}\n\n\\subsubsection{Quantitative Comparisons.} \nOur method is compared with six methods including Deepfakes~\\cite{deepfakes}, FaceSwap~\\cite{faceswap}, FSGAN~\\cite{nirkin2019fsgan}, FaceShifter~\\cite{li2019faceshifter}, SimSwap~\\cite{chen2020simswap},  and HifiFace~\\cite{wang2021hififace}.\nFor Deepfakes, FaceSwap, FaceShifter, and HifiFace, we use their released face swapping results of the sampled 10,000 images. For FSGAN and SimSwap, the face swapping results are generated with their released codes. \n\nTable~\\ref{tab:quantitative_comparison} shows that our method achieves the best scores under most evaluation metrics, including ID Acc, shape error, and Expr Error. These results validate the superiority of our FlowFace. We obtain a slightly worse result than other methods for the pose error, which can be attributed to our FlowFace changing the face shape while the employed head pose estimator is sensitive to face shapes.\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.48\\textwidth]{figures/ablation_flowface.pdf}\n\\caption{\nQualitative ablation results of FlowFace.\n}\n\\label{fig:face_swapping_result}\n\\end{figure}\n\n\\subsubsection{Qualitative Comparisons.} The qualitative comparisons are conducted on the same FF++ test set collected in the quantitative comparisons. As shown in Figure~\\ref{fig:FF_comparison}, our FlowFace maintains the best face shape consistency. Note that most methods do nothing to transfer the face shape, so their resulting face shapes are similar to the target ones.\n\nAlthough HifiFace is specifically designed to change the face shape, our method still obtains better results. As observed in Figure~\\ref{fig:FF_comparison}, our generated face shapes are more similar to the source ones than HifiFace. Since HifiFace injects the shape representation into the latent feature space, it is harder to accurately decode the face shape from the latent feature than our explicit semantic flow.\nMeanwhile, our method can better preserve the fine-grained target expressions (marked with red boxes in rows 1,3). \n\n\\begin{table}[t]\n\n\\caption{Subjective comparisons with SimSwap and HifiFace on FF++.}\n\\centering\n\\label{tab:subjective_comparison}\n\\resizebox{0.40\\textwidth}{!}{\n\\begin{tabular}{l|c|c|c|c}\n\nMethod      & Shape. ($\\%$)$ \\uparrow$  & ID. ($\\%$)$ \\uparrow$  & Exp. ($\\%$)$ \\uparrow$ & Realism ($\\%$)$ \\uparrow$ \\\\ \\hline\nSimSwap    & 20.67    & 27.78 & 34.44 & 14.67    \\\\ \nHifiFace & 35.11 & 34.67 & 30.45 & 41.78    \\\\ \\hline\nOurs        & \\textbf{44.22} & \\textbf{37.55} & \\textbf{35.11} & \\textbf{43.55} \\\\\n\\end{tabular}\n}\n\\end{table}\n\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=0.98\\textwidth]{figures/ablation_firststage_v10.pdf}  \n\\caption{Qualitative ablation results of each component in $F^{res}$.}\n\\label{fig:ablation_first}\n\\end{figure*}\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.48\\textwidth]{figures/ablation_second_stage_v3.pdf}  \n\\caption{Qualitative ablation study of $F^{swa}$.}\n\\label{fig:ablation_second}\n\\end{figure}\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.45\\textwidth]{figures/cross_attention_vis.pdf}  \n\\caption{Visualize the cross-attention of different facial parts. For each part in the target, our CAFM can accurately focus on the corresponding parts in the source.\n}\n\\label{fig:cross_att_vis}\n\\end{figure}\n\nWe further compare our methods with four more SOTA face swapping methods:~\\cite{zhu2021one}, FaceInpainter~\\cite{li2021faceinpainter}, HighRes~\\cite{xu2022high} and SmoothSwap~\\cite{kim2022smooth}. Among them, MegaFS and HighRes are based-on the latent space of StyleGAN2. As shown in Figure~\\ref{fig:additional_qualitative}, our method can better transfer the shape of the source to the target than all other methods. Although SmoothSwap can change the face shape, it destroys the target attributes (\\emph{e.g.}, hairstyle and hair color). Besides, our results are also more similar to the source face in terms of inner facial features (\\emph{e.g.}, beard), validating that our face encoder can better capture facial appearances than the identity embedding or the latent code of StyleGAN2.\nMoreover, our method also preserves the target attributes (\\emph{e.g.}, skin color, lighting, and expression) better than other methods.\n\n\\subsubsection{User Study.}\nTo further validate our FlowFace, we conduct a subjective comparison with SimSwap and HifiFace, two SOTA methods that release their codes or results. Fifteen participants are instructed to choose the best result in terms of shape consistency, identity consistency, expression consistency, or image realism, involving comparisons of 30 swapped faces by three methods. Table~\\ref{tab:subjective_comparison} shows that our method outperforms the two baselines in terms of all four metrics, validating the superiority of our method.\n\n\\subsection{Analysis of FlowFace}\n\nThree ablation studies are conducted to validate our two-stage FlowFace framework and several components used in $F^{res}$ and $F^{swa}$, respectively.\n\n\\subsubsection{Ablation Study on FlowFace.}\n\nWe conduct ablation experiments to validate the design of our two-stage framework.\nFigure~\\ref{fig:face_swapping_result} shows the swapped images by only $F^{res}$, only $F^{swa}$ and the full model (FlowFace). \nIt can be seen that $F^{res}$ transforms the face shape naturally according to the source, while $F^{swa}$ is good at capturing the identity of the source inner face and other facial attributes of the target. \nBenefiting from the strengths of both $F^{res}$ and $F^{swa}$, our FlowFace is able to create results with accurate identity and consistent facial attributes. Table~\\ref{tab:quantitative_comparison} records the quantitative results, which further illustrates the effectiveness of our two-stage framework. \nThe above observation validates the effectiveness of $F^{res}$ and confirms that face shapes are essential for identifying a person.\n\n To further validate our $F^{res}$, we plug it into the open-sourced SimSwap. As shown in Figure~\\ref{fig:FF_comparison} and Table~\\ref{tab:quantitative_comparison}, after reshaping by $F^{res}$, the face swapping result of SimSwap is more similar to the source face in terms of face contours. \n The ID Acc. also rises from 93.63\\% to 94.31\\%. The results demonstrate the effectiveness of our $F^{res}$ and also reveal that the face shape carries the identity information, thus improving identity similarity.\n\n\\subsubsection{Ablation study on $F^{res}$.}\n\nWe first conduct an ablation experiment to validate our proposed semantic guided generator $G^{res}$. \nSpecifically, we remove the semantic input $S_t$ of $G^{res}$ ($G^{res}$ w/o Seg). It can be seen from Figure~\\ref{fig:ablation_first} that some inaccurate flow occurs in the generated face, which implies that only facial landmarks cannot guide $G^{res}$ to produce accurate dense flow due to the lack of semantic information. \nThe results also demonstrate that the semantic information is beneficial for accurate flow estimation and validates the effectiveness of $G^{res}$.\n\nThen, we conduct two ablation experiments to validate $D^{res}$: (1) removing the semantic inputs ($S_t$ and $S_t^{res}$) of $D^{res}$ ($D^{res}$ w/o Seg). \nCompared with $F^{res}$, the generated faces suffer from unnaturalness, like the eyes are stretched, as observed in Figure~\\ref{fig:ablation_first}. \nIt implies that structured information in the semantic inputs can provide more fine-grained discriminative signals, thus enforcing $G^{res}$ to produce a more accurate flow. \n(2) removing $D^{res}$ (w/o $D^{res}$). As observed in Figure~\\ref{fig:ablation_first}, compared with $F^{res}$, there are many artifacts in the generated images, and the estimated flow also contains many noises. \nThe above observation validates the effectiveness of our proposed $D^{res}$.\n\n\\begin{table}[t]\n\n\\setlength{\\abovecaptionskip}{0cm}\n\\setlength{\\belowcaptionskip}{-0.2cm}\n\\centering\n\\caption{Quantitative ablation study of $F^{swa}$ on FF++.}\n\\label{tab:ablation_swap}\n\\resizebox{0.45\\textwidth}{!}{\n\\begin{tabular}{l|ccc|c|c}\n\\multirow{2}{*}{Methods} & \\multicolumn{3}{c|}{ID Acc}                     & \\multicolumn{1}{l|}{\\multirow{2}{*}{Expr}} & \\multicolumn{1}{l}{\\multirow{2}{*}{Pose}} \\\\ \\cline{2-4}\n                         & CosFace        & SphereFace     & Avg            & \\multicolumn{1}{l|}{}                       & \\multicolumn{1}{l}{}                       \\\\ \\hline\n\\textit{Addition}                 & \\textbf{99.38}          & \\textbf{99.44} & \\textbf{99.41} & 0.43                                        &   4.90                                        \\\\\n\\textit{AdaIN}                 & 97.31          & 97.15 & 97.23 & 0.33                                        &   3.27                                        \\\\\n\\hline\n\\textit{Id Embed.}                   & 97.10          & 96.90          & 97.00          & \\underline{0.22}                                  & \\underline{2.10}                                          \\\\\n\\textit{Vit Decoder}                & 98.44          & 97.73          & 98.09          & 0.23                                        & 2.80                                         \\\\\n$F^{swa}$                     & \\underline{99.18} & \\underline{98.23}    & \\underline{98.71}    & \\textbf{0.21}                               & \\textbf{1.99}                                      \n\\end{tabular}\n}\n\\end{table}\n\n\\subsubsection{Ablation study on $F^{swa}$.}\nThree ablation experiments are conducted to evaluate the design of $F^{swa}$:\n \n(1) Choices on CAFM, \\textit{Addition} and \\textit{AdaIN}. To verify the effectiveness of CAFM, we compare with two other methods: \\textit{Addition} that directly adds the source values to the target values; \\textit{AdaIN} that first averages source patch embeddings and then injects it into the target feature map using AdaIN residual blocks.\nAs shown in Figure~\\ref{fig:ablation_second} and Table~\\ref{tab:ablation_swap}, \\textit{Addition} simply brings all information of the source face to the target face, thus leading to severe pose and expression mismatch. \\textit{AdaIN} impacts the non-face parts (\\emph{e.g.}, hair) due to its global modulation. In contrast, $F^{res}$ with CAFM obtains a high ID Acc and preserves the target attribute well, which proves that CAFM can accurately extract identity information from the source face and adaptively infuse it into the target counterpart. \n\nTo further validate the effectiveness of our CAFM, we visualize the cross attention computed by CAFM. As shown in Figure~\\ref{fig:cross_att_vis}, given a specific part (marked by red boxes) of the target face, CAFM accurately focuses on the corresponding parts of the source face, validating our CAFM can adaptively transfer the identity information from the source patches to corresponding target patches.\n\n(2) Latent Representation vs. ID Embedding (\\textit{ID Embed.}). \nTo verify the superiority of using the latent representation of MAE, we train a new model which adopts the identity embedding as the identity representation and employs AdaIN as the injection method.\nAs can be seen from Figure~\\ref{fig:ablation_second}, \\textit{ID Embed.} misses some fine-grained face appearances, such as eyes color, beard. \nIn contrast, $F^{swa}$ contains richer identity information and achieves higher ID Acc, as shown in Tab~\\ref{tab:ablation_swap}.\n\n(3) Convolutional Decoder vs. ViT Decoder (\\textit{ViT Decoder}). \nWe try two different decoders to find out the better one.\nAs shown in Figure~\\ref{fig:ablation_second}, the results of \\textit{ViT Decoder} contains a lot of artifacts. \nIn contrast, \\textit{Convolutional Decoder} achieves realistic results with high fidelity. \n\n\\section{Conclusion}\nThis work proposes a semantic flow-guided two-stage framework, FlowFace, for shape-aware face swapping. \nIn the first stage, the face reshaping network transfers the shape of the source face to the target face by warping the face pixel-wisely using semantic flow. In the second stage, we employ a pre-trained masked autoencoder to extract facial features that better capture facial appearances and identity information. Then, we design a cross-attention fusion module to better fuse the source and the target features, thus leading to better identity preservation. Extensive quantitative and qualitative experiments are conducted on in-the-wild faces, demonstrating that our FlowFace outperforms the state-of-the-art significantly. \n\n\\section{Acknowledgments}\nThis work is supported by the 2022 Hangzhou Key Science and Technology Innovation Program (No. 2022AIZD0054), and the Key Research and Development Program of Zhejiang Province (No. 2022C01011), the ARC-Discovery grants (DP220100800) and ARC-DECRA (DE230100477).\n\n\\newpage\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-1908.05932v1.tex",
        "arXiv-1912.13457v3.tex",
        "arXiv-2212.02797v1.tex"
    ],
    "group_id": "group_84",
    "response": "### Title: Advances in Subject-Agnostic Face Swapping and Reenactment Techniques\n\n### Introduction\nFace swapping and reenactment have been significant areas of research in the field of computer vision and graphics, driven by applications in entertainment, privacy protection, and the generation of training data. Early methods required manual intervention or relied on 3D face models to manipulate facial appearances, often leading to limitations in handling pose variations, occlusions, and achieving high-fidelity results. With the advent of deep learning, particularly Generative Adversarial Networks (GANs), these methods have become more automated and capable of producing realistic face manipulations. However, challenges remain, especially in preserving the identity of the source face while accurately reflecting the pose and expression of the target face, handling occlusions, and ensuring seamless integration into the target context. This summary explores three recent advancements in face swapping and reenactment techniques: FSGAN, FaceShifter, and FlowFace, each contributing unique solutions to these challenges.\n\n### Main Content of Each Paper\n\n#### FSGAN: Subject Agnostic Face Swapping and Reenactment\nFSGAN, introduced by Yuval Nirkin and colleagues, presents a deep learning-based approach for face swapping and reenactment that is subject agnostic. The method aims to transfer a face from a source image to a target image while preserving the target's pose and expression, and ensuring the result is photo-realistic and temporally coherent. FSGAN's architecture consists of three main components: a reenactment generator ($G_r$), a face inpainting generator ($G_c$), and a face blending generator ($G_b$). The reenactment generator uses a recurrent neural network (RNN) to adjust for pose and expression variations, and employs a novel scheme for continuous interpolation between multiple views of the same face using Delaunay Triangulation and barycentric coordinates. Occluded face regions are handled by a face completion network, and a Poisson blending loss is used to seamlessly integrate the source face into the target context. FSGAN is trained on a large dataset of face videos from the IJB-C dataset, with additional training using the VGGFace2 and CelebA datasets for perceptual loss, and the LFW Parts Labels set for face and hair segmentation. The method's effectiveness is demonstrated through extensive qualitative and quantitative comparisons with existing state-of-the-art systems, showing superior results in terms of identity preservation, pose accuracy, and expression consistency.\n\n#### FaceShifter: Towards High Fidelity and Occlusion Aware Face Swapping\nFaceShifter, proposed by Lingzhi Li and colleagues, focuses on achieving high-fidelity face swapping results while being aware of facial occlusions. The framework is designed in two stages: the first stage uses an Adaptive Embedding Integration Network (AEI-Net) to generate a high-fidelity swapped face by integrating the identity and attributes of the source and target faces. AEI-Net employs a novel multi-level attributes encoder to extract attributes at various spatial resolutions and an Adaptive Attentional Denormalization (AAD) generator to adaptively fuse these attributes with the identity information. The second stage introduces a Heuristic Error Acknowledging Refinement Network (HEAR-Net) to refine the swapped face and recover occluded regions. HEAR-Net operates in a self-supervised manner, using the reconstruction error of the same face to identify occlusions without requiring manual annotations. FaceShifter's architecture is trained on large-scale face datasets, such as CelebA-HQ, FFHQ, and VGGFace2, and leverages a pre-trained face recognition model for identity preservation. The method's effectiveness is validated through qualitative and quantitative comparisons with state-of-the-art methods, demonstrating superior performance in terms of identity preservation, pose accuracy, expression consistency, and overall realism.\n\n#### FlowFace: Semantic Flow-guided Shape-aware Face Swapping\nFlowFace, developed by the authors from Peking University and Microsoft Research, addresses the challenge of transferring both the inner facial features and the facial contours of the source face to the target face. The framework consists of two stages: a face reshaping network ($F^{res}$) that warps the target face shape based on an estimated semantic flow, and a face swapping network ($F^{swa}$) that generates the inner facial details while preserving the reshaped face shape. The semantic flow is estimated using a 3D face reconstruction model and a semantic guided generator, which integrates the identity and attributes of the source and target faces. The face reshaping network is trained to produce accurate semantic flow, ensuring natural and consistent face shapes. The face swapping network employs a shared face encoder to map both faces into a common latent space and a cross-attention fusion module (CAFM) to adaptively fuse the identity information of the source with the attributes of the target. FlowFace's architecture is trained on a large dataset of high-quality face images, and its effectiveness is validated through qualitative and quantitative comparisons with state-of-the-art methods, demonstrating superior performance in identity preservation, face shape consistency, expression accuracy, and overall realism.\n\n### Commonalities and Innovations\nAll three papers aim to improve the quality and realism of face swapping and reenactment results, focusing on preserving the identity of the source face while accurately reflecting the pose and expression of the target face. They also address the challenge of handling occlusions and ensuring seamless integration into the target context. However, each paper introduces unique innovations:\n\n- **FSGAN** proposes a novel RNN-based approach for face reenactment and introduces a face completion network and a Poisson blending loss to handle occlusions and ensure seamless blending. It is subject agnostic and can be applied to any pair of faces without requiring specific training data.\n- **FaceShifter** introduces a two-stage framework with AEI-Net and HEAR-Net. AEI-Net uses a multi-level attributes encoder and AAD generator to achieve high-fidelity face swapping, while HEAR-Net recovers occluded regions in a self-supervised manner. FaceShifter's architecture is designed to be more general and can handle various types of occlusions.\n- **FlowFace** proposes a semantic flow-guided two-stage framework, where the first stage reshapes the target face to match the source face shape, and the second stage generates the inner facial features while preserving the reshaped face shape. FlowFace uses a shared face encoder and a cross-attention fusion module to better capture and fuse identity and attribute information, leading to more realistic and consistent results.\n\n### Comparison of Results\nThe three methods are compared using quantitative metrics, such as identity retrieval accuracy, pose error, expression error, and shape error, as well as qualitative evaluations. The results show that:\n\n- **FSGAN** achieves high identity preservation scores and low pose and expression errors, indicating its effectiveness in preserving the source identity while accurately reflecting the target pose and expression. However, it struggles with large pose differences and occlusions, leading to some artifacts in the results.\n- **FaceShifter** outperforms FSGAN in terms of identity preservation and shape consistency, thanks to its adaptive integration of identity and attributes and its ability to handle occlusions in a self-supervised manner. FaceShifter's results are more realistic and consistent with the target attributes, such as lighting and background.\n- **FlowFace** achieves the best scores in identity preservation and shape consistency, as well as comparable scores in expression accuracy. Its two-stage design allows for accurate reshaping of the target face and better preservation of inner facial features, leading to more realistic and consistent results.\n\n### Conclusion\nThe three papers, FSGAN, FaceShifter, and FlowFace, represent significant advancements in the field of face swapping and reenactment. They address the challenges of preserving identity, handling pose variations, and managing occlusions, while ensuring the results are photo-realistic and temporally coherent. FSGAN introduces a novel RNN-based approach for face reenactment and seamless blending, FaceShifter proposes a two-stage framework with adaptive integration and self-supervised occlusion recovery, and FlowFace utilizes a semantic flow-guided reshaping network and a cross-attention fusion module for high-fidelity face swapping. These methods not only improve the quality of face swapping and reenactment but also make them more accessible to non-experts. Future research could focus on further improving the handling of large pose differences, reducing artifacts in the results, and developing more robust methods for detecting and preventing the misuse of such technologies.\n\n### Future Research Directions\nFuture research could explore the following areas:\n- **Handling Large Pose Differences**: Further improvements in handling large pose differences could enhance the realism and consistency of the swapped faces.\n- **Reducing Artifacts**: Techniques to reduce or eliminate artifacts, especially in the context of occlusions and lighting, could lead to more seamless and realistic results.\n- **Detection and Prevention**: Developing robust methods for detecting and preventing the misuse of face swapping and reenactment technologies could help mitigate potential privacy and security concerns.\n- **Cross-Domain Applications**: Extending these methods to work across different domains, such as different resolutions, lighting conditions, and backgrounds, could broaden their applicability.\n\n### Tables for Comparison\nBelow are tables summarizing the quantitative results of the three methods:\n\n#### Table 1: Quantitative Comparison on FaceForensics++ Dataset\n| Method          | ID Acc $\\uparrow$ | Shape $\\downarrow$ | Expr $\\downarrow$ | Pose $\\downarrow$ |\n|-----------------|-------------------|--------------------|-------------------|-------------------|\n| DeepFakes       | 85.08             | 1.78               | 0.54             | 4.05              |\n| FaceSwap        | 73.86             | 1.85               | 0.40             | 2.21              |\n| FSGAN           | 51.36             | 2.18               | 0.27             | 2.20              |\n| FaceShifter     | 89.01             | 1.68               | 0.33             | 2.28              |\n| SimSwap         | 94.43             | 1.74               | 0.26             | 1.40              |\n| $F^{res}$ + SimSwap | 95.56             | 1.67               | 0.27             | 2.27              |\n| HifiFace        | 94.62             | 1.62               | 0.30             | 2.29              |\n| FlowFace        | 98.83             | 1.17               | 0.22             | 2.66              |\n\n#### Table 2: Subjective Comparison on FaceForensics++ Dataset\n| Method          | Shape $\\uparrow$ | ID $\\uparrow$ | Expr $\\uparrow$ | Realism $\\uparrow$ |\n|-----------------|------------------|---------------|-----------------|--------------------|\n| SimSwap         | 20.67            | 27.78         | 34.44           | 14.67              |\n| HifiFace        | 35.11            | 34.67         | 30.45           | 41.78              |\n| FlowFace        | 44.22            | 37.55         | 35.11           | 43.55              |\n\nThese tables highlight the strengths and weaknesses of each method, with FlowFace demonstrating the highest scores in identity preservation and shape consistency, while also showing strong performance in expression accuracy and overall realism.\n\n### Visualizations\nThe papers include several visualizations to demonstrate the effectiveness of their methods:\n- **FSGAN**: Figure \\ref{fig:face_reenactment_qualitative} and Figure \\ref{fig:face_swap_qualitative} show qualitative face reenactment and swapping results, respectively.\n- **FaceShifter**: Figure \\ref{fig:compare_all_no_fsgan} and Figure \\ref{fig:compare_fsgan} provide qualitative comparisons with other methods.\n- **FlowFace**: Figure \\ref{fig:FF_comparison} and Figure \\ref{fig:additional_qualitative} illustrate qualitative comparisons with state-of-the-art methods, demonstrating the superiority of FlowFace in face shape consistency and identity preservation.\n\nThese visualizations provide a clear understanding of the methods' capabilities and limitations, showcasing the advancements made in each approach.\n\n### Acknowledgments\nThe research presented in these papers is supported by various grants and funding programs, including the Hangzhou Key Science and Technology Innovation Program, the Key Research and Development Program of Zhejiang Province, and the ARC-Discovery grants. These supports highlight the significance of the research and the collaborative efforts in advancing the field of face swapping and reenactment.\n\nIn summary, these papers contribute to the ongoing efforts in improving the realism and fidelity of face swapping and reenactment techniques, making them more accessible and effective for a wide range of applications."
}