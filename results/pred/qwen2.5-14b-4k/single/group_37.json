{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{LLM-Pruner: On the Structural Pruning \\\\ of Large Language Models}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\n\nLarge language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named \\methodname, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely \\emph{3 hours}, requiring only \\emph{50K} data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: {\\url{https://github.com/horseee/LLM-Pruner}} \n\n\\end{abstract}\n\n\\section{Introduction}\n\nRecently, Large Language Models (LLMs)~\\cite{openai2023gpt4,touvron2023llama,thoppilan2022lamda,scao2022bloom,xue2020mt5,vicuna2023,zeng2022glm} have demonstrated remarkable proficiency in language understanding and generation. With the increase in model size, they are better equipped to handle complex tasks ~\\cite{brown2020language,chowdhery2022palm,wei2022chain,wu2020biased} and even exhibit emergent abilities~\\cite{weiemergent}. However, notwithstanding their impressive performance, LLMs pose challenges in deployment and inference. Their extensive scale engenders substantial computational demands, and the multitude of parameters involved can induce long latencies and other related issues. \nSeveral techniques are proposed to solve these problems, like model pruning~\\cite{wang2019structured,xia2022structured,zafrir2021prune,kurtic2022optimal}, knowledge distillation~\\cite{sun2019patient,pan2020meta,sun-etal-2020-contrastive},quantization~\\cite{bai2020binarybert,frantar2022gptq} within the context of pre-trained language model (PLM). \n\nWhile previous methods have effectively maintained model performance amidst parameter reduction, they primarily target compression within specialized domains or for designated tasks in the context of task-specific compression. For instance, a PLM is fine-tuned on a particular dataset, such as one of the classification tasks in the GLUE benchmark~\\cite{wang2018glue}, after which these models are distilled into a smaller classification model~\\cite{sun2019patient,hou2020dynabert}. Although this paradigm could potentially be employed for LLM compression, it compromises the LLM's capacity as a versatile task solver, rendering it suited to a single task exclusively.\n\nThus, we strive to compress the LLM in a new setting: to reduce the LLM's size while preserving its diverse capabilities as general-purpose task solvers, as depicted in Figure \\ref{fig:intro}. This introduces the task-agnostic compression of LLMs, which presents two key challenges:\n\\begin{itemize}[leftmargin=*,topsep=5pt]\n    \\setlength{\\itemsep}{1pt}\n    \\setlength{\\parskip}{0pt}\n    \\setlength{\\parsep}{0pt}\n    \\item \\textbf{The size of the training corpus of the LLM is enormous.} Previous compression methods heavily depend on the training corpus. The LLM has escalated the corpus scale to 1 trillion tokens or more ~\\cite{hoffmann2022training, touvron2023llama}. The extensive storage needs and protracted transmission times make the dataset difficult to acquire. Furthermore, if the dataset is proprietary, acquisition of the training corpus verges on impossibility, a situation encountered in ~\\cite{zeng2022glm,openai2023gpt4}.\n    \\item \\textbf{The unacceptably long duration for the post-training of the pruned LLM.} Existing methods require a substantial amount of time for post-training the smaller model~\\cite{wang2020minilm,liang2023homodistil}. For instance, the general distillation in TinyBERT takes around 14 GPU days ~\\cite{jiao2020tinybert}. Even post-training a task-specific compressed model of BERT demands around 33 hours~\\cite{xia2022structured,kwon2022fast}. As the size of both the model and corpus for LLMs increases rapidly, this step will invariably consume an even more extensive time.\n\\end{itemize}\n\n\\begin{figure}[t!]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/introduction_v8.pdf}\n    \\caption{\n        Illustration of \\methodname. \n        (i) Task-specific compression: the model is fine-tuned then compressed on a specific task. (ii) TinyBERT: First distill the model on unlabeled corpus and then fine-tune it on the specific task. (iii) LLM-Pruner: Task-agnostic compression  within 3 hours. %Models compressed by (i) and (ii) can only deal with one task, while by (iii), the model can be applied to various tasks.\n    }\n    \\label{fig:intro}\n    \\vspace{-5mm}\n\\end{figure}\n\nTo tackle the aforementioned challenges associated with the task-agnostic compression of LLMs, we introduce a novel approach called LLM-Pruner. Since our goal is to compress LLMs with reduced data dependency and expedited post-training, how to prune model with the minimal disruption to the origin is crucial. To accomplish this, we propose a dependency detection algorithm that identifies all the dependent structures within the model. Once the coupled structure is identified, we employ an efficient importance estimation strategy to select the optimal group for pruning under the task-agnostic setting, where the first-order information and an approximated hessian information is taken into account. Finally, a rapid recovery stage is executed to post-train the pruned model with limited data. \n\n  \n\\iffalse{\nIn sum, our contributions can be summarized as follow:\n\n\\begin{itemize} [leftmargin=*]\n    \\setlength\\itemsep{0.5em}\n    \\item We propose a novel framework, LLM-Pruner, for the task-agnostic compression of the large language model. The compressed language model preserves its ability to serve as a multi-task solver.\n    \\item With the limited availability of training corpus, we propose \n    \\item We conduct extensive experiments to demonstrate the effectiveness of LLM-Pruner. The evaluation is performed on LLaMA-7B, Vicuna-7B and ChatGLM-6B with nine datasets to evaluate the generation quality and the zero-shot classification performance of the pruned model. Experiments show that with 20\\% parameters removed, the model can maintain 93.6\\% of the performance of the original model.\n\\end{itemize}\n}\\fi\n\\paragraph{Contribution.} In this paper,  we propose a novel framework, LLM-Pruner, for the task-agnostic compression of the large language model. To the best of our knowledge, LLM-Pruner is the first framework designed for structured pruning of LLMs. We conclude the advantages of the LLM-Pruner as (i) Task-agnostic compression, where the compressed language model retains its ability to serve as a multi-task solver. (ii) Reduced demand for the original training corpus, where only 50k publicly available samples are needed for compression, significantly reducing the budget for acquiring the training data (iii) Quick compression, where the compression process ends up in three hours. (iv) An automatic structural pruning framework, where all the dependent structures are grouped without the need for any manual design.\nTo evaluate the effectiveness of LLM-Pruner, we conduct extensive experiments on three large language models: LLaMA-7B, Vicuna-7B, and ChatGLM-6B. The compressed models are evaluated using nine datasets to assess both the generation quality and the zero-shot classification performance of the pruned models. The experimental results demonstrate that even with the removal of 20\\% of the parameters, the pruned model maintains 94.97\\% of the performance of the original model.\n\n\\section{Related Work}\n\\paragraph{Compression of Language Model.}\nLanguage models ~\\cite{devlin2018bert,liu2019roberta,lewis2019bart} have gained much attention and increase the need to reduce the size of parameters and reduce the latency ~\\cite{lanalbert,sun2020mobilebert}. To compress the language model, previous works can be divided into several categories: network pruning ~\\cite{kurtic2022optimal,xu2021rethinking,liu2021ebert,ProximalPruning}, knowledge distillation ~\\cite{sun2019patient,sun-etal-2020-contrastive,metakd}, quantization~\\cite{yao2022zeroquant,bai2020binarybert,zafrir2019q8bert} and other techniques, like early exit ~\\cite{xin-etal-2020-deebert} or dynamic token reduction ~\\cite{ye-etal-2021-tr}. We focus on the pruning  of the language models, especially structural pruning ~\\cite{li2016pruning}. Structural pruning removes the entire filter from the neural network, which is more hardware friendly. There are several ways to remove the structure, such as l1-dependent pruning ~\\cite{NIPS2015_pruning,zafrir2021prune}, first-order importance estimation ~\\cite{hou2020dynabert}, hessian-based estimation~\\cite{kurtic2022optimal,wang2019eigendamage} or the optimal brain surgeon~\\cite{lecun1989optimal,kurtic2022optimal}. As for the pruning unit in structural pruning, some works adopt the entire layer ~\\cite{fan2019reducing} as the minimal unit, and others take the multi-head attention ~\\cite{voita2019analyzing} or the feed-forward layers ~\\cite{hou2020dynabert,mccarley2019structured} as the basic structure to prune. CoFi ~\\cite{xia2022structured} studies the pruning unit in different granularity. \n\n\\paragraph{Efficient and Low Resource Compression.} With the growing size of models, there is an increasing demand for efficient LLM compression and compression is independent of the original training data. As for the efficient compression, ~\\cite{kwon2022fast} accelerate the post-training by defining the reconstruction error as a linear least squares problem. ~\\cite{frantar2022gptq,frantar2023massive} propose the layer-wise optimal brain surgeon. As for the constraint of availability of the training corpus, data-free pruning ~\\cite{srinivas2015data,yvinec2022red++} come up with several strategies to prune the model by measuring neurons' similarity. Besides, ~\\cite{maprompting,ma2020adversarial,rashid2020zeroshot} proposes methods that distill the model without reliance on the training corpus of the model. However, those methods are too time-consuming, involving synthesizing samples by backpropagating the pre-trained language models.\n\n\\section{Methods}\nIn this section, we provide a detailed explanation of LLM-Pruner. Following the conventional model compression pipeline\\cite{kwon2022fast}, LLM-Pruner consists of  three steps:\n\\textbf{(1) Discovery Stage} (Section \\ref{sec:dependency}). This step focuses on identifying groups of interdependent structures within LLMs. \n\\textbf{(2) Estimation Stage} (Section \\ref{sec:importance}). Once the coupled structures are grouped, the second step entails estimating the contribution of each group to the overall performance of the model and deciding which group to be pruned.\n\\textbf{(3) Recover Stage} (Section \\ref{sec:recovery}). This step involves fast post-training that alleviates potential performance degradation caused by the removal of structures.  \n\n\\subsection{Discover All Coupled Structure in LLMs} \\label{sec:dependency}\n\nIn light of the limited availability of data for post-training, it becomes imperative to prioritize the removal of structures with minimal damage when compressing the model. This underscores the dependency-based structural pruning, which ensures coupled structures are pruned in unison. We provide an experiment in Section \\ref{exp:dependency} to show the importance of dependency-based structural pruning when compressing the large language model.\n\n\\paragraph{Structure Dependency in LLMs.} Similar to~\\cite{fang2023depgraph}, the pruning begins by building the dependency for LLMs. Assume $N_i$ and $N_j$ are two neurons in the model, $\\operatorname{In}(N_i)$ and $\\operatorname{Out}(N_i)$ represents all the neurons that point towards or point from $N_i$. The dependency between structures can be defined as:\n\\begin{equation}\n     N_j \\in \\operatorname{Out}(N_i) \\wedge \\operatorname{Deg}^-(N_j) = 1 \\Rightarrow N_j \\text{ is dependent on } N_i\n\\end{equation}\nwhere $\\operatorname{Deg}^-(N_j)$ represents the in-degree of neuron $N_j$. Noting that this dependency is directional, we can therefore correspondingly obtain another dependency:\n\n\\begin{equation}\n     N_i \\in \\operatorname{In}(N_j) \\wedge \\operatorname{Deg}^+(N_i) = 1 \\Rightarrow N_i \\text{ is dependent on } N_j\n\\end{equation}\nwhere $\\operatorname{Deg}^+(N_i)$ represents the out-degree of neuron $N_i$. The principle of dependency here is, if a current neuron (e.g., $N_i$) depends solely on another neuron (e.g., $N_j$), and the neuron $N_j$ is subjected to pruning, it follows that the neuron $N_i$ must also undergo pruning.\n\n\\paragraph{Trigger the Dependency Graph.} By having the definition of dependency, the coupled structures in the LLM can be analyzed automatically. Considering any neuron within the LLM as the initial trigger, it possesses the capability to activate neurons that depend on it. Subsequently, these newly triggered neurons can serve as the subsequent triggers to identify the dependency and activate their respective dependent neurons. This iterative process continues until no new neurons are detected. Those neurons then form a group for further pruning. Taking LLaMA as an example, by searching over all the neurons as the initial trigger, we can locate all the coupled structures, as shown in Figure\\ref{fig:main}.\n\nGiven the diversity in the structure of different LLMs, manual analysis and removal of coupled structures in each LLM could be extremely time-consuming. However, by employing LLM-Pruner, all coupled structures can be automatically identified and extracted.\n\n\\begin{figure}[t!]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/main_v5.pdf}\n    \\caption{\n        Illustration of the coupled structures in LLaMA. We simplify the neurons in each layer to make the dependent group clear. The trigger neuron, marked as a circle with a bell, cause weights with dependency pruned (dashed lines), which may propagate (red dashed lines) to coupled neurons (dashed circles). A group can be triggered by a variety of trigger neurons. Taking Group Type B as an example, the trigger for this group involves (i) the attention head, (ii) the output neuron in Query, Key or Value, and (iii) the input neuron in the final output projection.\n    }\n    \\label{fig:main}\n\\end{figure}\n\n\\subsection{Grouped Importance Estimation of Coupled Structure} \\label{sec:importance}\n\nTill now, all coupled structures within the model are grouped. Weights within the same group should be pruned simultaneously, as partial pruning not only increases parameter size but also introduces misaligned intermediate representations. Therefore, we estimate the importance of the group as a whole, as opposed to evaluating the importance of modules. Given the limited access to the training dataset, we explore the use of public datasets or manually created samples as alternative resources. Although the domains of these datasets may not perfectly align with the training set, they still provide valuable information for assessing the importance.\n\n\\paragraph{Vector-wise Importance.} Suppose that given a dataset $\\mathcal{D} = \\{x_i, y_i\\}_{i=1}^N$, where N is the number of samples. In our experiments, we set N equal to 10 and we use some public datasets as the source of $\\mathcal{D}$. A group (as previously defined as a set of coupled structures) can be defined as $\\mathcal{G} = \\{W_i\\}_{i=1}^M$, where M is the number of coupled structures in one group and $W_i$ is the weight for each structure. While pruning, our goal is to remove the group that has the least impact on the model's prediction, which can be indicated by the deviation in the loss. Specially, to estimate the importance of $W_i$, the change in loss can be formulated as \\cite{lecun1989optimal}:\n\\begin{equation}\n    I_{W_i} = | \\Delta \\mathcal{L}(\\mathcal{D})| = |\\mathcal{L}_{W_i}(\\mathcal{D}) - \\mathcal{L}_{W_i=0}(\\mathcal{D})| =| \\underbrace{\\frac{\\partial \\mathcal{L}^{\\top}(\\mathcal{D})}{\\partial W_i} W_i}_{\\neq 0}-\\frac{1}{2} {W_i}^{\\top} H W_i + \\mathcal{O}\\left(\\| W_i \\|^3\\right) | \\label{eq:taylor}\n\\end{equation} \nwhere $H$ is the hessian matrix. Here, $\\mathcal{L}$ represents the next-token prediction loss. The first term is typically neglected in prior work~\\cite{lecun1989optimal,wang2019eigendamage,frantar2023massive}, as the model has already converged on the training dataset, where ${\\partial \\mathcal{L}^{\\top}}/{\\partial W_i} \\approx 0$. However, since $\\mathcal{D}$ here is not extracted from the original training data, which means that ${\\partial \\mathcal{L}^{\\top}}/{\\partial W_i} \\not \\approx 0$. This presents a desirable property for determining the importance of $W_i$ by the gradient term under LLMs, since computation of the second term, the Hessian matrix, on the LLM is impractical with $\\mathcal{O}\\left(N^2\\right)$ complexity. \n\n\\paragraph{Element-wise Importance.} The above can be considered as an estimate for the weight $W_i$. We can derive another measure of importance at a finer granularity, where each parameter within $W_i$ is assessed for its significance:\n\\begin{equation}\n    I_{W_i^k} = | \\Delta \\mathcal{L}(\\mathcal{D})| = |\\mathcal{L}_{W_i^k}(\\mathcal{D}) - \\mathcal{L}_{W_i^k=0}(\\mathcal{D})| = | \\frac{\\partial \\mathcal{L}(\\mathcal{D})}{\\partial W_i^k} W_i^k-\\frac{1}{2} {W_i^k} H_{kk} W_i^k + \\mathcal{O}\\left(\\| W_i^k \\|^3\\right) |\n\\label{eq:element_taylor}\n\\end{equation} \nHere, $k$ represents the k-th parameter in $W_i$. The diagonal of the hessian $H_{kk}$ can be approximated by the Fisher information matrix, and the importance can be defined as:\n\\begin{equation}\n    I_{W_i^k} = | \\mathcal{L}_{W_i^k}(\\mathcal{D}) - \\mathcal{L}_{W_i^k=0}(\\mathcal{D})| \\approx | \\frac{\\partial \\mathcal{L}(\\mathcal{D})}{\\partial W_i^k} W_i^k-\\frac{1}{2} \\sum_{j=1}^N \\left(\\frac{\\partial \\mathcal{L}(\\mathcal{D}_j)}{\\partial W_i^k} W_i^k\\right)^2 + \\mathcal{O}\\left(\\| W_i^k \\|^3\\right) | \\label{eq:element_final_taylor}\n\\end{equation} \n\\paragraph{Group Importance.} By utilizing either $I_{W_i^k}$ or $I_{W_i}$, we estimate the importance at the granularity of either a parameter or a weight. Remembering that our goal is to estimate the importance of $\\mathcal{G}$, we aggregate the importance scores in four ways:\n(i) Summation: $I_{\\mathcal{G}} = \\sum_{i=1}^{M}I_{W_i}$ or $I_{\\mathcal{G}} = \\sum_{i=1}^{M}\\sum_k I_{W_i^k}$, (ii) Production: $I_{\\mathcal{G}} = \\prod_{i=1}^{M}I_{W_i}$ or $I_{\\mathcal{G}} = \\prod_{i=1}^{M}\\sum_k I_{W_i^k}$, (iii) Max: $I_{\\mathcal{G}} = \\max_{i=1}^{M}I_{W_i}$ or $I_{\\mathcal{G}} = \\max_{i=1}^{M}\\sum_k I_{W_i^k}$; (iv) Last-Only: Since deleting the last executing structure in a dependency group is equivalent to erasing all the computed results within that group, we assign the importance of the last executing structure as the importance of the group: $I_{\\mathcal{G}} = I_{W_l}$ or $I_{\\mathcal{G}} = \\sum_k I_{W_l^k}$, where $l$ is the last structure. After assessing the importance of each group, we rank the importance of each group and prune the groups with lower importance based on a predefined pruning ratio.\n\n\\subsection{Fast Recovery with Low-rank Approximation} \\label{sec:recovery}\nIn order to expedite the model recovery process and improve its efficiency under limited data, it is crucial to minimize the number of parameters that need optimization during the recovery phase. To facilitate this, we employ the low-rank approximation,  LoRA\\cite{hulora}, to post-train the pruned model. Each learnable weight matrix in the model, denoted as $W$, encompassing both pruned and unpruned linear projection in the LLM, can be represented as $W$. The update value $\\Delta W$ for $W$ can be decomposed as $\\Delta W = PQ \\in \\mathbb{R}^{d^- \\times d^+}$, where $P \\in \\mathbb{R}^{d^- \\times d}$ and $Q \\in \\mathbb{R}^{d \\times d^+}$. The forward computation can now be expressed as:\n\\begin{equation}\n    f(x) = (W+\\Delta W)X + b = (WX + b) + (PQ)X\n\\end{equation}\nwhere $b$ is the bias in the dense layer. Only training $P$ and $Q$ reduces the overall training complexity, reducing the need for large-scale training data. Besides, the extra parameters $P$ and $Q$ can be reparameterized into $\\Delta W$, which would not cause extra parameters in the final compressed model. \n\n\\begin{table}[t]\n    \\centering\n    \\caption{Zero-shot performance of the compressed LLaMA-7B. The average is  calculated among seven classification datasets. `Underline' indicates the best pruning-only performance, while `bold' represents the overall best performance with the same pruning ratio, considering both pruning and post-training. The `\\channelname' strategy only prunes the dependent group of Type C, while all other methods employ the `Block' strategy to prune dependent groups in both Type A and Type B. Since \\cite{touvron2023llama} did not provide its prompt, the evaluation of the result with ${}^\\star$ is performed under different prompts, which is lower than the official results. } \\label{tbl:llama_result}\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{ll|cc|ccccccc|c}\n        \\toprule\n        \\toprule\n        Pruning Ratio & Method & WikiText2$\\color{teal}\\downarrow$ & PTB$\\color{teal}\\downarrow$ & BoolQ & PIQA & HellaSwag & WinoGrande & ARC-e & ARC-c & OBQA & Average \\\\\n        \\midrule\n        \n        \\multirow{2}{*}{Ratio = 0\\%} & LLaMA-7B\\cite{touvron2023llama} & - & - & 76.5 & 79.8 & 76.1 & 70.1 & 72.8 & 47.6 & 57.2 & 68.59 \\\\\n         & LLaMA-7B$^{\\star}$ & 12.62 & 22.14 & 73.18 & 78.35 & 72.99 & 67.01 & 67.45 & 41.38 & 42.40 & 63.25 \\\\ %& 73.18 & 78.35/77.42 & 56.40/72.99 & 67.01 & 67.45/52.53 & 38.14/41.38 & 28.00/42.40 & 63.25\\\\\n        \\cmidrule{1-12}\n        \\cmidrule{1-12}\n        \\multirow{7}{*}{\\parbox{1.8cm}{Ratio = 20\\% \\  w/o tune}} & L2 & 582.41 & 1022.17 & 59.66 & 58.00 & 37.04 & 52.41 & 33.12 & 28.58 & 29.80 & 42.65 \\\\%59.66 & 58.00/57.51 & 31.12/37.04 & 52.41 & 33.12/32.49 & 25.51/28.58 & 16.60/29.80 & 42.65 \\\\ % 63.46 & 73.07/71.49 & 49.87/66.06 & 63.61 & 46.30/42.89 & 33.02/35.67 & 25.40/38.40 &  55.22\\\\\n        & Random & 27.51 & 43.19 & 61.83 & 71.33 & 56.26 & 54.46 & 57.07 & 32.85 & 35.00 & 52.69\\\\%75.72 & 74.92/73.83 & 51.86/67.86 & 64.88 & 60.44/49.71 & 34.04/36.60 & 26.80/38.60 & 59.86 \\\\\n        \\cmidrule{2-12}\n        & \\channelname & 74.63 & 153.75 & 62.75 & 62.73 & 41.40 & 51.07 & 41.38 & 27.90 & 30.40 & 45.38 \\\\% 62.75 & 62.73/64.53 & 32.35/41.40 & 51.07 & 41.38/38.59 & 21.33/27.90 & 15.60/30.40 & 45.38 \\\\\n        \\cmidrule{2-12}\n        & Vector  & 22.28 & 41.78 & \\underline{61.44} & 71.71 & 57.27 & 54.22 & 55.77 & 33.96 & 38.40 & 53.25 \\\\\n        &$\\text{Element}^2$ & 19.77 & 36.66 & 59.39 & 75.57 & 65.34 & \\underline{61.33} & 59.18 & \\underline{37.12} & 39.80 & \\underline{56.82} \\\\\n        &$\\text{Element}^1$  & \\underline{19.09} & \\underline{34.21} & 57.06 & \\underline{75.68} & \\underline{66.80} & 59.83 & \\underline{60.94} & 36.52 & \\bf \\underline{40.00} & 56.69 \\\\% 57.06 & 75.68/75.08 & 50.69/66.80 & 59.83 & 60.94/50.04 & 31.91/36.52 & 29.20/40.00 & 56.69\\\\%63.52 & 72.69/72.58 &46.83/63.06 & 64.09 & 55.60/47.39 & 30.72/33.11 & 24.40/37.20 & 55.61 \\\\\n        \\cmidrule{1-12}\n        \\multirow{5}{*}{\\parbox{1.8cm}{Ratio = 20\\% \\\\ w/ tune}} & \\channelname & 22.02 & 38.67 & 59.08 & 73.39 & 64.02 & 60.54 & 57.95 & 35.58 & 38.40 & 55.57 \\\\% 59.08 & 73.39/72.09 & 49.06/64.02 & 60.54 & 57.95/48.40 & 32.76/35.58 & 24.20/38.40 & 55.57 \\\\\n        \\cmidrule{2-12}\n        & Vector & 18.84 & 33.05 & 65.75 & 74.70 & 64.52 & 59.35 & 60.65 & 36.26 & 39.40 & 57.23\\\\\n        & $\\text{Element}^2$ & \\textbf{17.37} & 30.39 & \\textbf{69.54} & 76.44 & 68.11 & \\textbf{65.11} & 63.43 & \\textbf{37.88} & \\textbf{40.00} & \\textbf{60.07} \\\\\n        & $\\text{Element}^1$ & 17.58 & \\bf 30.11 & 64.62 & \\textbf{77.20} & \\bf 68.80 & 63.14 & \\bf 64.31 & 36.77 & 39.80 & 59.23 \\\\%64.62 & 77.20/76.88 & 53.32/68.80 & 63.14 & 64.31/52.10 & 37.20/36.77 & 29.40/39.80 & 59.23\\\\ \n        \\bottomrule\n        \\bottomrule\n    \\end{tabular}\n    }\n\\end{table}\n\n\\begin{table}[t]\n    \\vspace{-5mm}\n    \\centering\n    \\caption{Zero-shot performance of the compressed LLaMA-13B. Here we adopt  $\\text{Element}^1$ as the importance estimation for `Channel` and `Block'.} \\label{tbl:llama13B_result}\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{ll|cc|ccccccc|c}\n        \\toprule\n        \\toprule\n        Pruning Ratio & Method & WikiText2$\\color{teal}\\downarrow$ & PTB$\\color{teal}\\downarrow$ & BoolQ & PIQA & HellaSwag & WinoGrande & ARC-e & ARC-c & OBQA & Average \\\\\n        \\midrule\n         Ratio = 0\\% & LLaMA-13B$^{\\star}$ &  11.58 & 20.24 & 68.47 & 78.89 & 76.24 & 70.09 & 74.58 & 44.54 & 42.00 & 64.97 \\\\\n        \\cmidrule{1-12}\n        \\multirow{4}{*}{\\parbox{1.8cm}{Ratio = 20\\% \\  w/o tune}} & L2 & 61.15 & 91.43 & 61.50 & 67.57 & 52.90 & 57.54 & 50.13 & 31.14 & 36.80 & 51.08 \\\\\n        & Random &  19.24 & 31.84 & 63.33 & 73.18 & 63.54 & 60.85 & 64.44 & 36.26 & 38.00 & 57.09 \\\\\n        & Channel & 49.03 & 106.48 & 62.39 & 66.87 & 49.17 & 58.96 & 49.62 & 31.83 & 33.20 & 50.29 \\\\\n        & Block & \\underline{16.01} & \\underline{29.28} & \\underline{67.68} & \\underline{77.15} & \\underline{73.41} & \\underline{65.11} & \\underline{68.35} & \\underline{38.40} & \\underline{42.40} & \\underline{61.79} \\\\\n        \\cmidrule{1-12}\n        \\multirow{4}{*}{\\parbox{1.8cm}{Ratio = 20\\% \\\\ w/ tune}} & L2 & 20.97 & 38.05 & \\bf 73.24 & 76.77 & 71.86 & 64.64 & 67.59 & 39.93 & 40.80 & 62.12 \\\\\n        & Random & 16.84 & 31.98 & 64.19 & 76.06 & 68.89 & 63.30 & 66.88 & 38.31 & 40.80 & 59.78 \\\\\n        & Channel & 17.58 & 29.76 & 69.20 & 76.55 & 68.89 & 66.38 & 62.08 & 38.99 & 39.60 & 60.24 \\\\\n        & Block & \\bf 15.18 & \\bf 28.08 & 70.31 & \\bf 77.91 & \\bf 75.16 & \\bf 67.88 & \\bf 71.09 & \\bf 42.41 & \\bf 43.40 & \\bf 64.02 \\\\\n        \\bottomrule\n        \\bottomrule\n    \\end{tabular}\n    }\n    \\vspace{-3mm}\n\\end{table}\n\n\\section{Experiments}\n\n\\subsection{Experimental Settings}\n\\paragraph{Foundation Large Language Model.}\nTo showcase the effectiveness and versatility of LLM-Pruner, we test it over three open-source large language models with two kinds of structure: LLaMA-7B~\\cite{touvron2023llama}, Vicuna-7B~\\cite{vicuna2023} \\footnote{https://huggingface.co/lmsys/vicuna-7b-delta-v0} and ChatGLM-6B~\\cite{zeng2022glm}. %ChatGLM is a bilingual language model, which aids us in discerning whether the compressed model retains the original model's capabilities, given that all corpora used in our compression experiments are in English and the test can be taken in Chinese.\n\n\\paragraph{Evaluation and Datasets.} To assess the performance of the model in the task-agnostic setting, we follow LLaMa's evaluation to perform zero-shot task classification on common sense reasoning datasets: BoolQ ~\\cite{clark-etal-2019-boolq}, PIQA~\\cite{Bisk2020piqa}, HellaSwag~\\cite{zellers2019hellaswag}, WinoGrande~\\cite{ai2:winogrande}, ARC-easy~\\cite{allenai:arc}, ARC-challenge~\\cite{allenai:arc} and OpenbookQA~\\cite{OpenBookQA2018}. Follow~\\cite{eval-harness}, the model ranks the choices in the multiple choice tasks or generates the answer in the open-ended generation \\footnote{https://github.com/EleutherAI/lm-evaluation-harness}. Additionally, we complement our evaluation with a zero-shot perplexity (PPL) analysis on WikiText2~\\cite{merity2016pointer} and PTB~\\cite{marcus-etal-1993-building}. %However, for the experiment on ChatGLM, we do not estimate the PPL on these two datasets. This is attributed to the inherent nature of ChatGLM as a dialogue model, which necessitates a query prompt and often recycles partial words from the query within the response. This pattern of interaction diverges significantly from the formats in WikiText2 and PTB.\n\n\\paragraph{Implementation Details.} \nIn the model pruning process, we use 10 randomly selected samples from Bookcorpus~\\cite{Zhu_2015_ICCV}, each truncated to a sequence length of 128, as the calibration samples for establishing dependency and calculating the gradient for both LLaMA and Vicuna. For ChatGLM, we select 10 random samples from DailyDialog~\\cite{li2017dailydialog}. During the recovery phase, we utilize the cleaned version of Alpaca~\\cite{alpaca}, which comprises approximately 50k samples. Remarkably, tuning these samples requires merely 3 hours on a single GPU with only 2 epochs. More hyper-parameters of pruning and training can be found in Appendix \\ref{sec:apx_implementation_details}.\n\n\\begin{wraptable}{r}{8.8cm}\n\\vspace{-3mm}\n\\caption{Statistics of the base model and the compressed model. }\\label{tbl:stat_param}\n \\resizebox{\\linewidth}{!}{%\n    \\begin{tabular}{c|cc|ccccc}\n        \\toprule\n        Model & Strategy & Ratio & \\#Params & \\#MACs & Memory & Latency \\\\\n        \\midrule\n        \\multirow{5}{*}{\\parbox{1.8cm}{LLaMA-7B Vicuna-7B}} & - & - & 6.74B & 424.02G & 12884.5MiB & 69.32s \\\\%12.62 & 22.14\\\\ %5.67 & 8.80  \\\\\n        & \\channelname & 20\\% & 5.39B & 339.36G & 10363.6MiB & 61.50s \\\\\n        & \\blockname & 20\\% & 5.42B & 339.60G & 10375.5MiB & 58.55s\\\\\n        & \\channelname & 50\\% & 3.37B & 212.58G & 6556.3MiB & 40.11s\\\\\n        & \\blockname & 50\\% & 3.35B &  206.59G & 6533.9MiB & 37.54s\\\\\n        \\bottomrule\n    \\end{tabular}   \n}%\n\\vspace{-3mm}\n\\end{wraptable}\n\\vspace{-1mm}\n\\paragraph{Statistics of the Compressed Model.} \nTable \\ref{tbl:stat_param} presents the statistic of the 7B models that are used in our experiments: the parameter count, MACs, memory requirements and latency for running each model. The statistical evaluation is conducted using the inference mode, where the model is fed a sentence consisting of 64 tokens. The latency is tested under the test set of WikiText2 on a single A5000.\nHere, the `\\blockname' strategy implies that the pruned unit in the model consists of Group Type A and Group Type B as illustrated in Figure \\ref{fig:main}, whereas `\\channelname' indicates that the unit to be pruned is Group Type C. We delve into an analysis of these two choices in Section \\ref{sec:block_channel}(Channel Strategy vs. Block Strategy). The pruning ratio stated here denotes the approximate ratio of parameters to be pruned since the number of parameters within each pruned structure does not perfectly match the total number of pruned parameters.\n\n\\subsection{Zero-shot Performance}\n\nTable \\ref{tbl:llama_result},\\ref{tbl:llama13B_result},\\ref{tbl:vicuna_result} and \\ref{tbl:chatglm_result} shows the zero-shot performance of the pruned model. \nBased on the evaluation conducted on LLaMA, employing a 20\\% parameter reduction without post-training, the pruned model manages to retain 89.8\\% of the performance exhibited by the unpruned model. Furthermore, through the efficient post-training, the classification accuracy further improves to 60.07\\%, achieving 94.97\\% of the accuracy attained by the original model. This demonstration proves the feasibility of using LLM-Pruner to effectively compress the model, even without relying on training data, and within a remarkably short period of time. \nSurprisingly, we discover that on most datasets, the pruned model with 5.4B LLaMA even outperformed chatGLM-6B. This highlights the superiority of the LLM-Pruner: if a smaller model with a customized size is required, LLM-Pruner is more cost-effective compared to retraining another model with a satisfying performance.\nHowever, with 50\\% parameters pruned, a large accuracy degradation is observed (see Appendix \\ref{sec:large_ratios}). Compressing LLMs under high compression rates still remains a large challenge.\n\n\\begin{table}[t]\n    \\centering\n    \\caption{Zero-shot performance of the compressed Vicuna-7B} \\label{tbl:vicuna_result}\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{ll|cc|ccccccc|c}\n        \\toprule\n        \\toprule\n        Pruned Model & Method & WikiText2 $\\color{teal}\\downarrow$ & PTB$\\color{teal}\\downarrow$ & BoolQ & PIQA & HellaSwag & WinoGrande & ARC-e & ARC-c & OBQA & Average \\\\\n        \\midrule\n        Ratio = 0\\% & Vicuna-7B & 16.11 & 61.37 & 76.57 & 77.75 & 70.64 & 67.40 & 65.11 & 41.21 & 40.80 & 62.78 \\\\ %76.57 & 77.75/77.09 & 56.23/70.64 & 67.40 & 65.11/53.49 & 39.76/41.21 & 28.00/40.80 & 62.78 \\\\\n        \\cmidrule{1-12}\n        \\multirow{7}{*}{\\parbox{1.8cm}{Ratio = 20\\% w/o tune}} & l2 & 3539.98 & 5882.21 & 55.90 & 56.15 & 32.37 & 51.85 & 30.01 & 28.41 & 28.20 & 40.41 \\\\ % 55.90 & 56.15/56.04 & 28.56/32.37 & 51.85 & 30.01/29.76 & 23.21/28.41 & 14.80/28.20 & 40.41 \\\\\n        & random & 34.63 & 112.44 & 61.47 & 70.89 & 54.67 & 56.27 & 55.60 & 31.74 & 34.60 & 52.18 \\\\ %61.47 & 70.89/71.06 & 42.30/54.67 & 56.27 & 55.60/46.51 & 28.33/31.74 & 20.40/34.60 & 52.18 \\\\\n        \\cmidrule{2-12}\n        & \\channelname &  71.75 & 198.88 & 51.77 & 63.93 & 42.58 & 55.17 & 43.94 & 29.27 & 33.40 & 45.72 \\\\ % 51.77 & 63.93/63.98 & 35.02/42.58 & 55.17 & 43.94/40.99 & 26.96/29.27 & 18.60/33.40 & 45.72\\\\\n        \\cmidrule{2-12}\n        & Vector & 27.03 & \\underline{92.51} & 62.17 & 71.44 & 55.80 & 53.43 & 55.77 & 33.28 & 37.80 & 52.81 \\\\\n        & $\\text{Element}^2$ & \\underline{24.70} & 94.34 & \\underline{62.87} & \\underline{75.41} & \\underline{64.00} & \\underline{58.41} & 60.98 & \\underline{37.12} & \\underline{39.00} & \\underline{56.83} \\\\ %61.90 & 72.96 & 56.78 & 53.04 & 52.57 & 33.28 & 36.80 & 52.47 \\\\\n        & $\\text{Element}^1$ &  25.74 & 92.88 & 61.70 & 75.30 & 63.75 & 56.20 & \\underline{63.22} & 36.60 & 37.00 & 56.25 \\\\ %61.70 & 75.30/74.70 & 49.98/63.75 & 56.20 & 63.22/53.07 & 34.39/36.60 & 26.00/37.00 & 56.25 \\\\\n        \\cmidrule{1-12}\n        \\multirow{4}{*}{\\parbox{1.8cm}{Ratio = 20\\% w/ tune}} \n        & Vector & 19.94 & \\bf 74.66 & 63.15 & 74.59 & 61.95 & 60.30 & 60.48 & 36.60 & \\bf 39.40 & 56.64 \\\\\n        & $\\text{Element}^2$ & \n        \\bf 18.97 & 76.78 & 60.40 & 75.63 & \\bf 65.45 & \\bf 63.22 & \\bf 63.05 & \\bf 37.71 & 39.00 & \\bf 57.78 \\\\\n        & $\\text{Element}^1$ & 19.69 & 78.25 & \\bf 63.33 & \\bf 76.17 &  65.13 & 60.22 & 62.84 & 37.12 & 39.20 & 57.71\\\\ \n        \\bottomrule\n        \\bottomrule\n    \\end{tabular}\n    }\n\\end{table}\n\nThe compression results of Vicuna-7B align with those of LLaMA, as pruning 20\\% of parameters on Vicuna-7B maintains performance at 92.03\\% of the original model. We test a smaller pruning rate of 10\\% on chatGLM-7B, where the pruned model only experiences a marginal performance decrease of 0.89\\%, which can be recovered through post-training. Despite the pruned model outperforming the uncompressed model, we don't assert it is better than the original model. This is largely because chatGLM-6B, a bilingual model, has limited English pre-training exposure. Post-training, however, introduces it to more English corpus, albeit limited, improving its English comprehension.\n\n\\paragraph{Ablation: Impact of Importance Estimation.} \nWe conduct tests on all proposed importance estimation techniques mentioned in Section \\ref{sec:importance}. The results can be found in Table \\ref{tbl:llama_result} and \\ref{tbl:vicuna_result}. Here, $\\textit{Element}^\\text{n}$ represents the importance evaluation utilizing the n-th order term in Eq.\\ref{eq:element_final_taylor}. $\\textit{Vector}$ represents the result corresponding to Eq.\\ref{eq:taylor}. Based on the results obtained from LLaMA-7B and Vicuna-7B, pruning algorithms achieved the best average performance mostly by leveraging the second-order derivatives for each parameter. Nonetheless, given that first-order derivatives are considerably more efficient than second-order derivatives, though yielding slightly inferior results, \nwe still vote for the first-order term as a competitive method. Besides, the results on chatGLM-7B differed significantly from these findings. The importance estimation on each parameter fails, performing even worse than l2, while the importance estimation on the weight matrix reaches the best performance. \n\n\\begin{wrapfigure}{r}{6.5cm} \n    \\vspace{-6mm}\n    \\includegraphics[width=0.9\\linewidth]{figures/sensitivity.pdf} \n    \\vspace{-2mm}\n    \\caption{Layer sensitivity for Pruning: Removing Groups in only one layer. \n    } \\label{fig:layer_sensitivity}\n\\vspace{-0.8cm}\n\\end{wrapfigure}\n\n\\paragraph{\\channelname\\ Strategy vs. \\blockname\\ Strategy.} \\label{sec:block_channel}\n\nFrom the results presented in Table \\ref{tbl:llama13B_result}, it is evident that pruning `\\channelname' significantly deteriorates performance compared to pruning `\\blockname'. This discrepancy arises because the layers within the stacked transformer do not evenly distribute their importance. As shown in Figure \\ref{fig:layer_sensitivity}, the first and last layers have a profound impact on the model's performance, and pruning them results in more substantial performance degradation compared to other layers. However, due to the uniform treatment of the `\\channelname' group across all layers, it becomes inevitable to prune the first and last layers, leading to a significant decline in performance.\n\n\\begin{table}[t]\n    \\centering\n    \\caption{Zero-shot Performance of the compressed ChatGLM-6B} \\label{tbl:chatglm_result}\n    \\resizebox{0.95\\linewidth}{!}{\n    \\begin{tabular}{ll|cccccc|c}\n        \\toprule\n        \\toprule\n        Pruned Model & Method & PIQA & HellaSwag & WinoGrande & ARC-e & ARC-c & OBQA & Average\\\\\n        \\midrule\n        Ratio = 0\\% & ChatGLM-6B & 67.95 & 46.37 & 52.33 & 48.36 & 29.95 & 37.40 & 47.05 \\\\\n        \\midrule\n        \\multirow{4}{*}{\\parbox{1.8cm}{Ratio = 10\\% w/o tune}}\n        & L2  & 61.97 & 37.22 & 49.72 & 42.05 & 28.24 & 35.40 & 42.43 \\\\\n        & Random  & 65.29 & 43.18 & 51.30 & 47.52 & 29.52 & 34.60 & 45.24 \\\\\n        & Vector  & 66.32 & 43.51 & 53.04 & 47.56 & \\bf 30.72 & \\bf 35.80 & 46.16 \\\\\n        & $\\text{Element}^1$  & 54.35 & 28.07 & 50.59 & 27.82 & 24.66 & 33.20 & 36.45 \\\\\n        \\cmidrule{1-9}\n        w/ tune & Vector & \\bf 67.74 & \\bf 46.35 & \\bf 53.99 & \\bf 51.01 & 29.95 & 35.00 & \\bf 47.34 \\\\\n    \\bottomrule\n    \\bottomrule\n    \\end{tabular}\n    }\n\\end{table}\n\n\\begin{figure}[t]\n  \\centering\n  \\vspace{-1mm}\n  \\begin{minipage}[b]{0.63\\linewidth}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/Pruning_Ratio_ALL.pdf}\n    \\caption{The pruning results on LLaMA-7B (left) and Vicuna-7B (right) with different pruning rates.}\n    \\label{fig:pruning_ratio}\n  \\end{minipage}\n  \\hfill\n  \\begin{minipage}[b]{0.35\\linewidth}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/overfit.pdf}\n    \\caption{Perplexity on zero-shot datasets across varyhing steps.}\n    \\label{fig:tune_overfit}\n  \\end{minipage}\n  \\vspace{-3mm}\n\\end{figure}\n\n\\begin{table}[t]\n  \\centering\n  \\begin{minipage}[b]{0.48\\linewidth}\n    \\centering\n    \\caption{Effect of the dependency-based structural pruning. Average represents the average performance on 7 classification datasets.} \\label{tbl:dependency}\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{c|c|ccc}\n        \\toprule\n        & Method & WikiText2$\\color{red}\\downarrow$ & PTB$\\color{red}\\downarrow$ & Average$\\color{teal}\\uparrow$\\\\\n        \\midrule\n        \\multirow{2}{*}{w/o Tuning}\n        & w/o dependency & 68378.42 & 79942.47 & 38.32\\\\\n        & w/ dependency & 19.09 & 34.21 & 56.69 \\\\\n        \\midrule\n        \\multirow{2}{*}{w/ Tuning}\n        & w/o dependency &  13307.46 & 13548.08 & 38.10 \\\\\n        & w/ dependency & 17.58 & 30.11 & 59.23\\\\\n        \\bottomrule\n    \\end{tabular}\n    }\n  \\end{minipage}\n  \\hfill\n  \\begin{minipage}[b]{0.48\\linewidth}\n    \\centering\n    \\caption{Impact of different aggregation strategies on group importance estimation. Experiments are performed on LLaMA-7B.}\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{c|cc|ccc}\n        \\toprule\n        Method & WikiText2$\\color{red}\\downarrow$ & PTB$\\color{red}\\downarrow$  & ARC-e$\\color{teal}\\uparrow$ & PIQA$\\color{teal}\\uparrow$ & OBQA$\\color{teal}\\uparrow$\\\\\n        \\midrule\n        Summation & 66.13 & 164.25 & 40.70 &  63.49 & 34.80 \\\\\n        Max & 62.59 & 144.38 & 39.60 & 63.71 & 34.60 \\\\\n        Production & 77.63 & 192.88 & 37.84 & 62.08 & 35.00 \\\\\n        Last-only & 130.00 & 170.88 & 41.92 & 64.75 & 35.20 \\\\\n        \\bottomrule\n    \\end{tabular}\n    }\n  \\end{minipage}\n\\end{table}\n\n\\subsection{More Analysis}\n\\paragraph{Impact of Different Pruning Rates.} We investigate the impact of pruning the LLM at various pruning ratios in Figure \\ref{fig:pruning_ratio}. We compare our pruning results with the L2 strategy because L2 is also a data-free pruning algorithm. It is observed in the experiment of LLaMA that when the pruning ratio reaches approximately 20\\%, the magnitude-dependent algorithm experiences a rapid collapse, leading to the loss of information. Conversely, by employing LLM-Pruner, we are able to increase the pruning ratio to around 60\\% while achieving an equivalent perplexity level. Furthermore, in the case of Vicuna-7B, removing 10\\% parameters results in a performance decline equivalent to that of LLM-Pruner with 60\\%. The utilization of LLM-Pruner enables a significant increase in the number of model parameters that can be pruned, thereby substantially reducing computational overhead.\n\n\\paragraph{Tuning on the External Dataset.} To tune the pruned model, we utilize the external dataset Alpaca~\\cite{alpaca}. The evaluation curves of the pruned model on two zero-shot datasets during the post-training process are depicted in Figure \\ref{fig:tune_overfit}. The results demonstrate a rapid decrease in the perplexity of the pruned model within 300 steps, followed by a gradual increase. We provide a more comprehensive evaluation in Appendix \\ref{sec:apx_overfit}. It is important to note that if the model is trained for an excessive number of steps, it runs the risk of overfitting the external dataset, potentially compromising its performance in other general-purpose tasks.\n\n\\paragraph{Impact of Dependency-based Structured Pruning.} \\label{exp:dependency}\nTo study the importance of dependency-based structural pruning, we conduct an experiment to disrupt dependencies within groups, where each weight matrix $W_i$ is pruned solely based on the importance score estimated on itself. Table \\ref{tbl:dependency} presents the results demonstrating the impact of dependencies in structural pruning. In the absence of dependencies, the model nearly fails in the zero-shot generation and classification tasks. Even with tuning, the model fails to recover, showing a substantial difference compared to the results in dependency-based pruning.\n\n\\paragraph{Impact of Different Aggregation Strategies.}\n\\iffalse{\n\\begin{wraptable}{r}{6.8cm}\n    \\vspace{-5mm}\n    \\caption{} \\label{tbl:Strategy_ablation_on_llama}\n\\end{wraptable}\n}\\fi\nWe conduct tests on the aggregation algorithms proposed in Section \\ref{sec:importance}. Our experimental results unveil notable discrepancies in model performance across different aggregation strategies, with particular emphasis on the `Last-only' strategy. Among the evaluated approaches, the `Max' strategy attains the most favorable outcomes in terms of perplexity, signifying enhanced coherence and fluency in sentence generation. However, it is important to note that the `Max' strategy exhibits the poorest zero-shot classification results compared to all four strategies. Conversely, the `Last-only' strategy showcases superior classification performance but suffers from the poorest generation quality.\nIn our experiments, we make a trade-off by selecting the `Sum' strategy since it shows both good generalization quality and classification performance.\n\n\\begin{wraptable}{r}{5.0cm}\n    \\centering\n    \\vspace{-4mm}\n    \\caption{DistilBert vs. LLM-Pruner. The average here means the average score on the above seven datasets.} \\label{tbl:distilBERT}\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{ll|c}\n        \\toprule\n        Pruning Ratio & \\#Param & Average \\\\\n        \\midrule\n        DistilBert & 3.50B & 44.64 \\\\\n        LLM-Pruner & 3.35B & 48.88 \\\\\n        \\bottomrule\n    \\end{tabular}\n    }\n\\end{wraptable}\n\n\\paragraph{Comparison with DistilBERT} We show the comparison results of DistilBERT and LLM-Pruner on LLaMA-7B in Table \\ref{tbl:distilBERT}. LLM-Pruner outperforms DistilBERT by 4.24\\% on average with even a smaller size. The reason lies in that LLM-Pruner minimizes model disruption during pruning, whereas DistilBERT merely selects one layer out of two. As a result, the model pruned by LLM-Pruner demands less data to recover its performance compared with DistilBERT, consequently achieving superior performance.\n\n\\paragraph{Scratch Training vs. Pruning.} We compare LLM-Pruner with StableLM-3B\\footnote{https://huggingface.co/stabilityai/stablelm-tuned-alpha-3b} with a similar parameter size. To ensure fairness, both models are fine-tuned on the Alpaca dataset. The experimental results of these two models are shown in the Table \\ref{tbl:llama_stableLM}. \nLLM-Pruner crafts lightweight LLMs with low resources, and even can sometimes achieve better performance than LLMs from scratch training. However, we also acknowledge that the LLaMA-3B obtained by LLM-Pruner will not always outperform other 3B models from scratch training, due to the huge gap in the size of training corpus.  \n\n\\begin{table}[h]\n    \\centering\n    \\caption{Scratch Training (StableLM-3B) vs. LLaMA-3B (by LLM-Pruner)} \\label{tbl:llama_stableLM}\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{l|ll|ccccccc|c}\n        \\toprule\n        \\toprule\n        Pruning Ratio & \\#Param & Latency & BoolQ & PIQA & HellaSwag & WinoGrande & ARC-e & ARC-c & OBQA & Average \\\\\n        \\midrule\n        StableLM-3B & 3.6B & 31.69s & 48.78 & 69.48 & 44.52 & 54.62 & 50.93 & 25.17 & 27.40 & 45.84 \\\\\n        LLaMA-3B & 3.6B & 37.96s & 61.41 & 70.08 & 51.01 & 55.01 & 46.80 & 30.38 & 37.40 & 50.30 \\\\ \n        \\bottomrule\n        \\bottomrule\n    \\end{tabular}\n    }\n    \\vspace{-3mm}\n\\end{table}\n\n\\paragraph{Case Study.} We provide some examples of sentences generated by the model compressed using LLM-Pruner in Table \\ref{tbl:visualization}. We made efforts to ensure a minimal overlap between these generated sentences and the information contained in the tuning corpus, which demonstrates that the information originates from the original model rather than the tuning corpus. We provide additional examples in the Appendix, including the generated sentences of the model without post-training. From the cases in Table \\ref{tbl:visualization}, it is evident that the sentences generated by the compressed model are comparable to those produced by the original model. They exhibit fluency, relevance, and informativeness regarding the given topic. Nevertheless, during our experiments, we observed that the pruned model's performance deviates from that of the original model, particularly when generating lengthy sentences. Occasionally, it may generate sentences that are meaningless or contain repetitive tokens.\n\n\\begin{table}[t!]\n    \\centering\n    \\vspace{-4mm}\n    \\caption{Some examples of generations obtained with the original model and the pruned model.} \\label{tbl:visualization}\n    \\resizebox{\\linewidth}{!}{\n    \\tiny\n    \\begin{tabular}{p{0.8cm}|p{5cm}|p{4.8cm}}\n        \\toprule\n        Model & Original Model & Pruned Model \\\\\n        \\midrule\n         LLaMa & \\ul{The universe is the entirety of space, time, matter, and energy that exists.} It includes all matter and energy, whether visible or not, and in fact the whole universe is invisible. There are many theories regarding its origin and ultimate fate... & \\ul{The universe is the entirety of space, time, matter, and energy that exists.} The laws that make up physics — electrons orbiting atoms, magnets attracting other objects, planets orbiting the Sun—have been consistent over ...\\\\\n        \\midrule\n        Vicuna & \\ul{10 steps to build an ios app:} 1. Research and develop the idea. 2. Decide on the platform you want to use (iOS, Android, or both). 3. Create a plan and budget. 4. Hire a team of developers. 5. Design the user interface (UI). 6. Develop the user interface (UI). 7. Test and iterate on the app. 8. Submit the app to the app store. 9. Optimize the app for performance and user experience (UX). 10. Market and promote the app. & \\ul{10 steps to build an ios app:} 1. Plan your app: decide what it should do, what it will look like, and what it will cost to build it. 2. Hire a developer: A good iOS developer will help you design the user interface, create the app, and make sure it’s bug-free. 3. Create a user interface: This is where the magic happens. Your app’s user interface will be designed and mocked up at this point... \\\\\n        \\midrule\n        ChatGLM & \\ul{Write a poetry about love and peace.} \n        In the midst of life's hustle and bustle, I find your love a soothing peace. It is a feeling that cannot be conveyed, A connection that cannot be distanced.\n        With your smile, your eyes, your embrace, I feel a sense of belonging that only you can give. It is a love that lasts forever, A love that cannot be defeat...\n        & \\ul{Write a poetry about love and peace.} Love and peace, two such wonderful things\\textbackslash\\ A pure and endless desire in my heart\\textbackslash\\ And both of them, I must seek for\\textbackslash\\ A long, long time, I know..\\textbackslash\\ Love, I know, is a feeling of being\\textbackslash\\ A perfect partner, in every sense\\textbackslash\\ And peace, I need it, so much, one day\\textbackslash\\ A long, long way, my heart will go..\\\\%In the midst of the world below,   A world of passion and battle above,   One day, in the midst of the heat,   I knew that I would find my way home. From the love of a man I know,   And the peace of a place so strong,   I found a place where I'd close my eyes,   And feel all the weight of the world... \\\\ %Love and peace, two things so pure. A touch from every heart to heart there's a way. From love that's deep and true, and from peace that's free. A bond that can only be found in these two things... \\\\\n        \\bottomrule\n    \\end{tabular}\n    }\n\\end{table}\n\n\\section{Conclusion} \n\nIn this paper, we propose LLM-Pruner, a structured pruning approach for large language models. LLM-Pruner aims to compress sizable language models in a task-agnostic manner while minimizing the dependency on the original training corpus and preserving the linguistic capabilities of LLMs. LLM-Pruner accomplishes this by iteratively examining each neuron within the model as a trigger for identifying dependency groups, thereby constructing the LLM's dependency graph. Subsequently, LLM-Pruner assesses the importance of these groups using both parameter-wise and weight-wise estimation. Finally, we utilize LoRA for fast recovery and adjustment of the pruned model. We evaluate the efficacy of LLM-Pruner on three distinct models—LLaMA, Vicuna, and ChatGLM—utilizing various zero-shot datasets. Our experimental results indicate that LLM-Pruner successfully prunes the model, reducing computational burden while retaining its zero-shot capabilities. Nevertheless, considerable performance degradation occurs when employing high pruning rates, such as the removal of 50\\% of LLaMA's parameters, resulting in a substantial decline in model performance. Additionally, we observe instances in which the model generates incoherent sentences. Addressing the challenges associated with compressing LLMs at higher pruning rates remains a challenging task.\n\n\\medskip\n\n\\clearpage\n{\n\\small\n}\n\n\\clearpage\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Shortened LLaMA: Depth Pruning for Large Language Models\\\\with Comparison of Retraining Methods}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\nStructured pruning of modern large language models (LLMs) has emerged as a way of decreasing their high computational needs. Width pruning reduces the size of projection weight matrices (e.g., by removing attention heads) while maintaining the number of layers. Depth pruning, in contrast, removes entire layers or blocks, while keeping the size of the remaining weights unchanged. Most current research focuses on either width-only or a blend of width and depth pruning, with little comparative analysis between the two units (width~\\textit{vs.}~depth) concerning their impact on LLM inference efficiency. In this work, we show that simple depth pruning can effectively compress LLMs while achieving comparable or superior performance to recent width pruning studies. Our pruning method boosts inference speeds, especially under memory-constrained conditions that require limited batch sizes for running LLMs, where width pruning is ineffective. In retraining pruned models for quality recovery, continued pretraining on a large corpus markedly outperforms LoRA-based tuning, particularly at severe pruning ratios. We hope this work can help build compact yet capable LLMs.\n\\end{abstract}\n\n\\section{Introduction}\n\nThe advancement of large language models (LLMs)~\\cite{touvron2023llama,openai2023gpt4,chowdhery2022palm,zhang2022opt,scao2022bloom} has brought significant improvements in language-based tasks, enabling versatile applications such as powerful chatbots~\\cite{bard,chatgpt}. However, the deployment of LLMs is constrained by their intensive computational demands. To make LLMs more accessible and efficient for practical use, various optimization strategies have been actively studied over recent years (see~\\citet{zhu2023survey,wan2023efficient} for survey). This work focuses on \\textit{structured} pruning~\\cite{fang2023depgraph,li2017pruning}, which removes groups of unnecessary weights and can facilitate hardware-agnostic acceleration. \n\n\\begin{figure}[t]\n  \\centering\n    \\includegraphics[width=\\linewidth]{fig/teaser.pdf}\n        \\vspace{-0.25in}\n  \\caption{Inference of pruned Vicuna-7B models on an NVIDIA H100 GPU. \\uline{Left}: Compared to width pruning (W\\ding{34}) of FLAP~\\cite{flap} and LLM-Pruner~\\cite{llmpruner}, our depth pruning (D\\ding{34}) achieves faster inference. \\uline{Right}: Continued pretraining is crucial for restoring the quality of heavily pruned models with fewer than 3.7B parameters, enabling our method to surpass the baselines, including SLEB~\\cite{song2024sleb}. See Table~\\ref{table:cpt_results} for details.} \\label{fig_teaser}\n  \\vspace{-0.1in}\n\\end{figure}\n\n\\begin{figure*}[t]\n  \\centering\n    \\includegraphics[width=\\linewidth]{fig/gpuutil_latency.pdf}\n        \\vspace{-0.23in}\n  \\caption{\\uline{Top}: GPU compute utilization of (a)–(c) running LLaMA-7B on different NVIDIA GPUs and that of (d) Vicuna-13B. Increasing batch sizes can enhance GPU utilization and throughput, but pushing this too far triggers OOM issues. \\uline{Bottom}: Latency results ($L$: target output length). Our depth pruning (blue lines) improves generation speeds over the original models (gray), while width pruning~\\cite{llmpruner} is ineffective (green). The dotted lines show that pruned models can operate with larger batch sizes that cause OOM errors for the original model. The results are obtained with pruning ratios of 27\\% for the 7B model and 29\\% for the 13B model.\n  }\n  \\vspace{-0.1in}\n  \\label{fig_gpuutil}\n\\end{figure*}\n\nIn the context of compressing recent LLMs, LLM-Pruner~\\cite{llmpruner} and FLAP~\\cite{flap} narrow the network width by pruning coupled structures (e.g., attention heads and their associated weight connections) while maintaining the number of layers. Sheared-LLaMA~\\cite{xia2023sheared} reduces not only the network width but also its depth by entirely removing some layers. Despite the existence of pruning methods~\\cite{xia2022structured,kurtic2023ziplm,xia2023sheared} that incorporate both width and depth aspects, there remains a gap in detailed analysis comparing these two factors (width~\\textit{vs.}~depth), specifically in relation to their impact on LLM inference efficiency.\n\nIn addition to substantial model sizes, LLM inference is distinguished by an autoregressive decoding mechanism, which predicts tokens one by one based on the input and the previously generated tokens. This sequential generation process often exhibits a memory-bound nature, leading to considerable underutilization of GPU compute abilities~\\cite{kwon2023efficient,jin2023s}. While expanding batch sizes is a standard way to enhance GPU utilization and throughput, this approach is unfeasible for low-specification GPUs with memory constraints. We aim to improve inference speeds of LLMs, especially under hardware limitations that demand small batch sizes, where we observe that width-only pruning is inadequate.\n\nDepth pruning is often regarded as being less effective in generation performance compared to width pruning, due to the elimination of bigger and coarse units. Contrary to the prevailing view, this study reveals that depth pruning is a compelling option for compressing LLMs, and it can achieve comparable or superior performance to prior studies depending on the retraining setups. Our contributions are summarized as follows:\n\n\\begin{enumerate}[itemsep=0em]\n\\setlength{\\leftskip}{-0.22cm}\n\\vspace{-0.025in}\n\\item[$\\circ$] In scenarios with limited batch sizes, our work demonstrates that width pruning is difficult to attain actual speedups in LLM's autoregressive generation. This aspect has been underexplored in previous works.\n\n\\vspace{-0.02in}\n\\item[$\\circ$] We introduce a simple yet effective method for depth pruning of LLMs by exploring various design factors. Our compact LLMs, obtained by excluding several Transformer blocks, achieve actual speedups. \n\n\\vspace{-0.02in}\n\\item[$\\circ$] We show that under moderate pruning ratios, our depth pruning method with LoRA retraining can rival recent width pruning studies for LLMs in zero-shot capabilities. For more aggressive pruning (over 40\\% removal), intensive retraining with a full-parameter update is crucial for recovering performance.\n\n\\setlength{\\leftskip}{0pt}\n\\end{enumerate}\n\n\\begin{figure}[t]\n  \\centering\n    \\includegraphics[width=\\linewidth]{fig/pruning_unit.pdf}\n        \\vspace{-0.25in}\n    \\caption{Comparison of pruning units. Width pruning reduces the size of projection weight matrices. Depth pruning removes Transformer blocks, or individual MHA and FFN modules.}\\label{fig_compare_depth_width_prune}  \n  \\vspace{-0.1in}\n\\end{figure}\n\n\\vspace{-0.2in}\n\\section{Problem: Small-batch LLM Inference}\n\nMost LLMs are autoregressive models that sequentially produce tokens, based on the initial prompt and the sequence of tokens previously generated. The token-by-token generation process often involves multiplying large matrices (weights) with smaller matrices or vectors (activations). The primary bottleneck for inference efficiency is memory access operations rather than the speed of mathematical computations (referred to as `memory-bound'), leading to suboptimal use of GPU computing power~\\cite{kwon2023efficient}. Though increasing batch sizes is a standard way to enhance GPU computation and throughput, it poses a risk of out-of-memory (OOM) errors (see Figure~\\ref{fig_gpuutil})\\footnote{Using the HF-Transformers library~\\cite{wolf-etal-2020-transformers}, we ran the LLMs with 12 input tokens for 20 batched runs after 10 warm-ups. Top: Peak GPU compute utilization~\\cite{nvidia_smi_query}. Bottom: Mean latency over 20 runs.} unless advanced system-level optimizations~\\cite{kwon2023efficient,sheng2023flexgen} are applied.\n\nIn this study, our focus is on accelerating the inference of LLMs under small-batch conditions caused by hardware restrictions. Such situations are relevant for deploying LLMs on memory-constrained local devices, which can enhance user experience and data privacy protection. We show that (i) reducing weight shapes via width pruning does not improve generation speeds and can even degrade it when the resulting weight dimensions are unsuitable for GPU capabilities, and (ii) notable speed gains are only achievable through depth pruning that excludes a number of modules entirely.\n\n\\section{Method: Block Pruning} \\label{method}\n\nAn LLM is a stack of multiple Transformer blocks \\cite{transformer}, each of which contains a pair of multi-head attention (MHA) and feed-forward network (FFN) modules (see Figure~\\ref{fig_compare_depth_width_prune}). We choose this Transformer block as the prunable unit to prioritize reducing inference latency. Our approach is simple: after identifying unimportant blocks with straightforward metrics, we perform simple one-shot pruning.\n\n\\subsection{Evaluation of Block-level Importance} \\label{subsect_crit}\nWe consider the following criteria to evaluate the significance of each block, ultimately selecting the Taylor+ and PPL metrics (see Table~\\ref{table:criterion}). Specifically, the linear weight matrix is denoted as $\\mathbf{W}^{k,n} = \\left[W_{i,j}^{k,n}\\right]$ with a size of $(d_{\\mathrm{out}}, d_{\\mathrm{in}})$, where $k$ represents the type of operation (e.g., a query projection in MHA or an up projection in FFN) within the $n$-th Transformer block. The weight importance scores are calculated at the output neuron level~\\cite{wanda}, followed by summing\\footnote{In our exploration of various aggregation strategies (i.e., sum, mean, product, and max operations), summing the scores was effective at different pruning ratios.} these scores to assess the block-level importance.\n \n\\paragraph{Magnitude (Mag).} This metric~\\cite{li2016pruning} is a fundamental baseline in the pruning literature, assuming that weights with smaller norms are less informative. For the block-level analysis, we compute $I_{\\mathrm{Magnitude}}^n = \\sum_k \\sum_i \\sum_j \\left| W_{i,j}^{k,n} \\right|$.\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.9\\linewidth]{fig/block_importance.pdf}\n\\vspace{-0.05in}\n\\caption{Estimated importance of each Transformer block on the calibration set. We prune blocks that have lower (better) PPL scores, as their removal causes less disruption to the output.}\\label{fig_ppl_crit}\n\\vspace{-0.1in}\n\\end{figure}\n\n\\paragraph{Taylor.} Assessing the error caused by the removal of a weight parameter helps in identifying its significance. For a given calibration dataset $D$, this can be expressed as the alteration in the training loss $\\mathcal{L}$~\\cite{lecun1989optimal,molchanov2019importance}: $\\left| \\mathcal{L}(W_{i,j}^{k,n}; D) - \\mathcal{L}(W_{i,j}^{k,n} = 0; D) \\right| \\approx \\left| \\frac{\\partial \\mathcal{L}(D)}{\\partial W_{i,j}^{k,n}} W_{i,j}^{k,n} \\right|$, where we omit the second-order derivatives by following~\\citet{llmpruner}. We define the block score as $I_{\\mathrm{Taylor}}^n = \\sum_k \\sum_i  \\sum_j \\left| \\frac{\\partial \\mathcal{L}(D)}{\\partial W_{i,j}^{k,n}} W_{i,j}^{k,n} \\right|$.\n\n\\paragraph{Mag+ and Taylor+.} Upon using the aforementioned metrics, the early blocks are labeled as unimportant, but their removal leads to severe performance drops. Similar to a popular heuristic~\\cite{gale2019state,lee2021layeradaptive}, we preserve the first four and the last two blocks~\\cite{llmpruner} by excluding them from the pruning candidates.\n\n\\paragraph{Perplexity (PPL).} Redundant blocks contribute less to the model's outputs, and their removal leads to smaller degradation in PPL, a commonly used metric for language modeling tasks. In this context, we eliminate each block from the source model and monitor its influence on PPL using the calibration set $D$: $I_{\\mathrm{PPL}}^n = \\exp \\left\\{ -\\frac{1}{SL} \\sum_{s} \\sum_{l} \\log p_{\\theta^{n}}(x_{l}^{(s)} | x_{<l}^{(s)}) \\right\\}$, where $\\theta^{n}$ denotes the model without its $n$-th block, and $s = 1, \\ldots, S$ and $l = 1, \\ldots, L$ are the indices for sequences and tokens in $D$. The PPL can be derived from the next-token prediction loss and requires only forward-pass computation. As shown in Figure~\\ref{fig_ppl_crit}, several blocks are removable with only a slight effect on the PPL metric. Pruning initial and final blocks significantly degrades the performance, which necessitates keeping them unpruned. \n\n\\subsection{One-shot Pruning}\nAfter sorting the block-level importance scores, we prune the less crucial blocks in a single step. Since every block has an identical configuration and it is easy to calculate the number of parameters for one block, we readily decide how many blocks should be removed to meet the target model size.\n\nIterative pruning with intermediate updates of block importance can be applied as in SLEB~\\cite{song2024sleb}. However, it requires much longer computing time than one-shot pruning as the number of blocks increases. Furthermore, we empirically observed that retraining strategies matter more than whether the pruning scheme is iterative or one-shot, especially under severe pruning ratios.\n\n\\subsection{Retraining for Performance Restoration}\nSome recent studies suggest that structured pruning of LLMs can be retraining-free~\\cite{song2024sleb,flap} or feasible with low retraining budgets~\\cite{llmpruner}. However, the types of retraining over different pruning rates have been underexplored. Here, we compare several retraining strategies and their implications for regaining the quality of pruned models.\n\n\\paragraph{Low-Rank Adaptation (LoRA).} LoRA~\\cite{lora} enables the efficient refinement of LLMs with less computation. \\citet{llmpruner} has applied LoRA to enhance moderately width-pruned models (e.g., with 20\\% of units removed) on an instruction tuning dataset. In this work, we show that LoRA can also recover the ability of depth-pruned models; however, it does not perform well for extensive compression rates (e.g., with over 50\\% removal) in either width or depth pruning.\n\n\\paragraph{Continued Pretraining (CPT).} We leverage CPT, which involves updating all parameters, on a large-scale pretraining corpus. This powerful retraining is critical for severely depth-pruned models, extending its proven effectiveness for width- or hybrid-pruned models~\\cite{xia2023sheared}. Though requiring greater resources than LoRA, CPT on pruned networks significantly accelerates learning and yields superior results compared to training the same architectures from random initialization.\n\n\\paragraph{CPT$\\Rightarrow$LoRA} Once CPT on the pretraining data is completed, LoRA with the instruction set is applied to observe whether further performance improvement can be achieved.\n\n\\begin{table}[t]\n\\centering\n\\begin{adjustbox}{max width=\\columnwidth}\n\\begin{threeparttable}\n\\begin{tabular}{cc|c|c|cc}\n\\specialrule{.2em}{.1em}{.1em} \n\n\\multicolumn{2}{c|}{Model}         & \\#Param & \\#Block\\textsuperscript{$\\ddagger$} & \\#Head\\textsuperscript{$\\ddagger$}    & FFN-D\\textsuperscript{$\\ddagger$}          \\\\ \\hline\n\\multicolumn{2}{c|}{Original 7B}   & 6.7B    & 32      & 32        & 11008          \\\\ \\hline\n\\multirow{4}{*}{35\\%\\textsuperscript{$\\dagger$}} & Wanda-sp   & 4.5B    & 32      & 21        & 7156           \\\\\n                      & FLAP       & 4.5B    & 32      & 23.0{\\scriptsize±8.8}  & 6781.1{\\scriptsize±2440.6}  \\\\\n                      & LLM-Pruner & 4.4B    & 32      & 18        & 6054           \\\\ \\cline{2-6} \n                      & Ours       & 4.5B    & 21      & 32        & 11008          \\\\ \n                      \n \n\\specialrule{.2em}{.1em}{.1em}\n\\specialrule{.2em}{.1em}{.1em}\n\n\\multicolumn{2}{c|}{Original 13B}  & 13.0B   & 40      & 40        & 13824          \\\\ \\hline\n\\multirow{4}{*}{37\\%\\textsuperscript{$\\dagger$}} & Wanda-sp   & 8.4B    & 40      & 26        & 8710           \\\\\n                      & FLAP       & 8.3B    & 40      & 27.5{\\scriptsize±11.3} & 8326.6{\\scriptsize±2874.9}  \\\\\n                      & LLM-Pruner & 8.2B    & 40      & 22        & 7603           \\\\ \\cline{2-6} \n                      & Ours       & 8.3B    & 25      & 40        & 13824          \\\\ \n                      \n\\specialrule{.2em}{.1em}{.1em} \n\n\\end{tabular}\n\\begin{tablenotes}[para,flushleft]\n\\footnotesize \n\\textsuperscript{$\\dagger$}Reduction ratio for the number of parameters.\n\\newline\n\\textsuperscript{$\\ddagger$}\\#Block: \\#Transformer blocks; \\#Head: \\#attention heads of MHA; FFN-D: intermediate size of FFN. \n\\end{tablenotes}\n\\end{threeparttable}\n\\end{adjustbox}\n\\vspace{-0.05in}\n\\caption{Examples of pruned architectures on 7B-parameter (top) and 13B-parameter (bottom) models. While Wanda-sp~\\cite{wanda,flap}, FLAP~\\cite{flap}, and LLM-Pruner~\\cite{llmpruner} reduce the network width, our method reduces the network depth. See Table~\\ref{supple_table:arch} for the details.}\n\\label{table:arch_short_ver}\n\n\\vspace{-0.1in}\n\\end{table}\n\\begin{table*}[t]\n\\centering\n\\begin{adjustbox}{max width=0.92\\linewidth}\n\\begin{threeparttable}\n\\begin{tabular}{ccc|ccc|cc|cc}\n\\specialrule{.2em}{.1em}{.1em} \n\n\\multicolumn{3}{c|}{}                                                                                                                              & \\multicolumn{3}{c|}{Zero-shot Performance}                                                                                                                                     & \\multicolumn{2}{c|}{H100 80GB\\textsuperscript{$\\ddagger$}}                                                                                                                                & \\multicolumn{2}{c}{RTX3090 24GB\\textsuperscript{$\\ddagger$}}                                                                                                                              \\\\ \\cline{4-10} \n\\multicolumn{3}{c|}{}                                                                                                                              & \\multicolumn{2}{c|}{PPL↓}                                                                          &                                                                           &                                                                          &                                                                                    &                                                                          &                                                                                    \\\\\n\\multicolumn{3}{c|}{\\multirow{-3}{*}{\\#Param \\& Method}}                                                                                                       & WikiText2                             & \\multicolumn{1}{c|}{PTB}                                   & \\multirow{-2}{*}{\\begin{tabular}[c]{@{}c@{}}Ave Acc↑\\\\ (\\%)\\textsuperscript{$\\dagger$}\n\\end{tabular}} & \\multirow{-2}{*}{\\begin{tabular}[c]{@{}c@{}}Latency↓\\\\ (s)\\end{tabular}} & \\multirow{-2}{*}{\\begin{tabular}[c]{@{}c@{}}Throughput↑\\\\ (tokens/s)\\end{tabular}} & \\multirow{-2}{*}{\\begin{tabular}[c]{@{}c@{}}Latency↓\\\\ (s)\\end{tabular}} & \\multirow{-2}{*}{\\begin{tabular}[c]{@{}c@{}}Throughput↑\\\\ (tokens/s)\\end{tabular}} \\\\ \\hline\n\\multicolumn{3}{c|}{LLaMA-7B: 6.7B (Original)}                                                                                                               & 12.6                                  & \\multicolumn{1}{c|}{22.1}                                  & 66.3                                                                      & 2.4                                                                      & 53.7                                                                               & 5.1                                                                      & 25.0                                                                               \\\\ \\hline\n                                                                                 &                         & Wanda-sp                              & 21.4                                  & \\multicolumn{1}{c|}{47.2}                                  & 51.8                                                                      & 3.1                                                                      & 41.7                                                                               & 7.6                                                                      & 16.7                                                                               \\\\\n                                                                                 &                         & FLAP                                  & \\textbf{17.0}                         & \\multicolumn{1}{c|}{\\textbf{30.1}}                         & 59.5                                                                      & 3.2                                                                      & 40.5                                                                               & 7.7                                                                      & 16.5                                                                               \\\\\n                                                                                 & \\multirow{-3}{*}{W\\ding{34}} & LLM-Pruner                            & 17.6                                  & \\multicolumn{1}{c|}{30.4}                                  & 61.8                                                                      & 3.0                                                                      & 43.2                                                                               & 6.0                                                                      & 21.4                                                                               \\\\ \\cline{2-10} \n                                                                                 &                         & SLEB                                  & 18.5                                  & \\multicolumn{1}{c|}{31.6}                                  & 57.6                                                                      & \\textbf{1.9}                                                             & \\textbf{66.0}                                                                      & \\textbf{4.5}                                                             & \\textbf{28.4}                                                                      \\\\\n                                                                                 &                         & \\cellcolor[HTML]{ECF4FF}Ours: Taylor+ & \\cellcolor[HTML]{ECF4FF}20.2          & \\multicolumn{1}{c|}{\\cellcolor[HTML]{ECF4FF}32.3}          & \\cellcolor[HTML]{ECF4FF}\\textbf{63.5}                                     & \\cellcolor[HTML]{ECF4FF}\\textbf{1.9}                                     & \\cellcolor[HTML]{ECF4FF}\\textbf{66.0}                                              & \\cellcolor[HTML]{ECF4FF}\\textbf{4.5}                                     & \\cellcolor[HTML]{ECF4FF}\\textbf{28.4}                                              \\\\\n\\multirow{-6}{*}{\\begin{tabular}[c]{@{}c@{}}5.5B\\\\ (20\\%\\\\  Pruned)\\end{tabular}} & \\multirow{-3}{*}{D\\ding{34}} & \\cellcolor[HTML]{ECF4FF}Ours: PPL     & \\cellcolor[HTML]{ECF4FF}17.7          & \\multicolumn{1}{c|}{\\cellcolor[HTML]{ECF4FF}30.7}          & \\cellcolor[HTML]{ECF4FF}61.9                                              & \\cellcolor[HTML]{ECF4FF}\\textbf{1.9}                                     & \\cellcolor[HTML]{ECF4FF}\\textbf{66.0}                                              & \\cellcolor[HTML]{ECF4FF}\\textbf{4.5}                                     & \\cellcolor[HTML]{ECF4FF}\\textbf{28.4}                                              \\\\ \\hline\n\\hline\n                                                                                 &                         & Wanda-sp                              & 133.6                                 & \\multicolumn{1}{c|}{210.1}                                 & 36.9                                                                      & 3.1                                                                      & 41.6                                                                               & 8.0                                                                      & 16.1                                                                               \\\\\n                                                                                 &                         & FLAP                                  & 25.6                                  & \\multicolumn{1}{c|}{44.4}                                  & 52.7                                                                      & 3.2                                                                      & 40.5                                                                               & 8.1                                                                      & 15.8                                                                               \\\\\n                                                                                 & \\multirow{-3}{*}{W\\ding{34}} & LLM-Pruner                            & 24.2                                  & \\multicolumn{1}{c|}{40.7}                                  & \\textbf{55.5}                                                             & 2.9                                                                      & 44.4                                                                               & 6.1                                                                      & 21.1                                                                               \\\\ \\cline{2-10} \n                                                                                 &                         & SLEB                                  & 34.2                                  & \\multicolumn{1}{c|}{49.8}                                  & 50.1                                                                      & \\textbf{1.6}                                                             & \\textbf{80.1}                                                                      & \\textbf{3.4}                                                             & \\textbf{37.8}                                                                      \\\\\n                                                                                 &                         & \\cellcolor[HTML]{ECF4FF}Ours: Taylor+ & \\cellcolor[HTML]{ECF4FF}33.2          & \\multicolumn{1}{c|}{\\cellcolor[HTML]{ECF4FF}58.5}          & \\cellcolor[HTML]{ECF4FF}55.4                                              & \\cellcolor[HTML]{ECF4FF}\\textbf{1.6}                                     & \\cellcolor[HTML]{ECF4FF}\\textbf{80.1}                                              & \\cellcolor[HTML]{ECF4FF}\\textbf{3.4}                                     & \\cellcolor[HTML]{ECF4FF}\\textbf{37.8}                                              \\\\\n\\multirow{-6}{*}{\\begin{tabular}[c]{@{}c@{}}4.5B\\\\ (35\\%\\\\Pruned)\\end{tabular}} & \\multirow{-3}{*}{D\\ding{34}} & \\cellcolor[HTML]{ECF4FF}Ours: PPL     & \\cellcolor[HTML]{ECF4FF}\\textbf{23.1} & \\multicolumn{1}{c|}{\\cellcolor[HTML]{ECF4FF}\\textbf{38.8}} & \\cellcolor[HTML]{ECF4FF}55.2                                              & \\cellcolor[HTML]{ECF4FF}\\textbf{1.6}                                     & \\cellcolor[HTML]{ECF4FF}\\textbf{80.1}                                              & \\cellcolor[HTML]{ECF4FF}\\textbf{3.4}                                     & \\cellcolor[HTML]{ECF4FF}\\textbf{37.8}                                              \\\\ \n\n\\specialrule{.2em}{.1em}{.1em} \n\n\\specialrule{.2em}{.1em}{.1em} \n\n\\multicolumn{3}{c|}{}                                                                                                                               & \\multicolumn{3}{c|}{Zero-shot Performance}                                                                                                                                     & \\multicolumn{2}{c|}{H100 80GB}                                                                                                                                & \\multicolumn{2}{c}{RTX3090 24GB}                                                                                                                              \\\\ \\cline{4-10} \n\\multicolumn{3}{c|}{}                                                                                                                               & \\multicolumn{2}{c|}{PPL↓}                                                                          &                                                                           &                                                                          &                                                                                    &                                                                          &                                                                                    \\\\\n\\multicolumn{3}{c|}{\\multirow{-3}{*}{\\#Param \\& Method}}                                                                                                        & WikiText2                             & \\multicolumn{1}{c|}{PTB}                                   & \\multirow{-2}{*}{\\begin{tabular}[c]{@{}c@{}}Ave Acc↑\\\\ (\\%)\\textsuperscript{$\\dagger$}\n\\end{tabular}} & \\multirow{-2}{*}{\\begin{tabular}[c]{@{}c@{}}Latency↓\\\\ (s)\\end{tabular}} & \\multirow{-2}{*}{\\begin{tabular}[c]{@{}c@{}}Throughput↑\\\\ (tokens/s)\\end{tabular}} & \\multirow{-2}{*}{\\begin{tabular}[c]{@{}c@{}}Latency↓\\\\ (s)\\end{tabular}} & \\multirow{-2}{*}{\\begin{tabular}[c]{@{}c@{}}Throughput↑\\\\ (tokens/s)\\end{tabular}} \\\\ \\hline\n\\multicolumn{3}{c|}{Vicuna-13B: 13.0B (Original)}                                                                                                        & 14.7                                  & \\multicolumn{1}{c|}{51.6}                                  & 68.3                                                                      & 2.8                                                                      & 45.5                                                                               & OOM                                                                      & OOM                                                                                \\\\ \\hline\n                                                                                  &                         & Wanda-sp                              & 19.0                                  & \\multicolumn{1}{c|}{71.8}                                  & 63.6                                                                      & 3.8                                                                      & 34.1                                                                               & 9.8                                                                      & 12.9                                                                               \\\\\n                                                                                  &                         & FLAP                                  & 18.8                         & \\multicolumn{1}{c|}{65.3}                         & 63.3                                                                      & 3.9                                                                      & 32.6                                                                               & 10.2                                                                     & 12.6                                                                               \\\\\n                                                                                  & \\multirow{-3}{*}{W\\ding{34}} & LLM-Pruner                            & \\textbf{16.0}                         & \\multicolumn{1}{c|}{57.0}                                  & 65.3                                                                      & 3.8                                                                      & 34.0                                                                               & 7.5                                                                      & 17.3                                                                               \\\\ \\cline{2-10} \n                                                                                  &                         & SLEB                                  & 20.5                                  & \\multicolumn{1}{c|}{68.7}                                  & 60.4                                                                      & \\textbf{2.3}                                                             & \\textbf{55.7}                                                                      & \\textbf{5.4}                                                             & \\textbf{23.9}                                                                      \\\\\n                                                                                  &                         & \\cellcolor[HTML]{ECF4FF}Ours: Taylor+ & \\cellcolor[HTML]{ECF4FF}18.1          & \\multicolumn{1}{c|}{\\cellcolor[HTML]{ECF4FF}61.6}          & \\cellcolor[HTML]{ECF4FF}\\textbf{66.7}                                     & \\cellcolor[HTML]{ECF4FF}\\textbf{2.3}                                     & \\cellcolor[HTML]{ECF4FF}\\textbf{55.7}                                              & \\cellcolor[HTML]{ECF4FF}\\textbf{5.4}                                     & \\cellcolor[HTML]{ECF4FF}\\textbf{23.9}                                              \\\\\n\\multirow{-6}{*}{\\begin{tabular}[c]{@{}c@{}}10.5B\\\\ (21\\%\\\\Pruned)\\end{tabular}} & \\multirow{-3}{*}{D\\ding{34}} & \\cellcolor[HTML]{ECF4FF}Ours: PPL     & \\cellcolor[HTML]{ECF4FF}16.1          & \\multicolumn{1}{c|}{\\cellcolor[HTML]{ECF4FF}\\textbf{56.5}} & \\cellcolor[HTML]{ECF4FF}64.9                                              & \\cellcolor[HTML]{ECF4FF}\\textbf{2.3}                                     & \\cellcolor[HTML]{ECF4FF}\\textbf{55.7}                                              & \\cellcolor[HTML]{ECF4FF}\\textbf{5.4}                                     & \\cellcolor[HTML]{ECF4FF}\\textbf{23.9}                                              \\\\ \\hline\n\\hline\n                                                                                  &                         & Wanda-sp                              & 36.6                                  & \\multicolumn{1}{c|}{123.5}                                 & 52.7                                                                      & 3.8                                                                      & 33.8                                                                               & 10.5                                                                     & 12.6                                                                               \\\\\n                                                                                  &                         & FLAP                                  & 28.7                                  & \\multicolumn{1}{c|}{96.2}                                  & 58.3                                                                      & 3.9                                                                      & 32.9                                                                               & 9.7                                                                      & 13.2                                                                               \\\\\n                                                                                  & \\multirow{-3}{*}{W\\ding{34}} & LLM-Pruner                            & 22.2                                  & \\multicolumn{1}{c|}{74.0}                                  & 59.7                                                            & 3.6                                                                      & 35.6                                                                               & 7.1                                                                      & 18.0                                                                               \\\\ \\cline{2-10} \n                                                                                  &                         & SLEB                                  & 41.6                                  & \\multicolumn{1}{c|}{116.5}                                 & 49.4                                                                      & \\textbf{1.8}                                                             & \\textbf{69.7}                                                                      & \\textbf{4.0}                                                             & \\textbf{31.7}                                                                      \\\\\n                                                                                  &                         & \\cellcolor[HTML]{ECF4FF}Ours: Taylor+ & \\cellcolor[HTML]{ECF4FF}34.2          & \\multicolumn{1}{c|}{\\cellcolor[HTML]{ECF4FF}90.4}          & \\cellcolor[HTML]{ECF4FF}\\textbf{61.4}                                     & \\cellcolor[HTML]{ECF4FF}\\textbf{1.8}                                     & \\cellcolor[HTML]{ECF4FF}\\textbf{69.7}                                              & \\cellcolor[HTML]{ECF4FF}\\textbf{4.0}                                     & \\cellcolor[HTML]{ECF4FF}\\textbf{31.7}                                              \\\\\n\\multirow{-6}{*}{\\begin{tabular}[c]{@{}c@{}}8.3B\\\\ (37\\%\\\\Pruned)\\end{tabular}}  & \\multirow{-3}{*}{D\\ding{34}} & \\cellcolor[HTML]{ECF4FF}Ours: PPL     & \\cellcolor[HTML]{ECF4FF}\\textbf{22.1} & \\multicolumn{1}{c|}{\\cellcolor[HTML]{ECF4FF}\\textbf{73.6}} & \\cellcolor[HTML]{ECF4FF}59.1                                              & \\cellcolor[HTML]{ECF4FF}\\textbf{1.8}                                     & \\cellcolor[HTML]{ECF4FF}\\textbf{69.7}                                              & \\cellcolor[HTML]{ECF4FF}\\textbf{4.0}                                     & \\cellcolor[HTML]{ECF4FF}\\textbf{31.7}                                              \\\\ \n\\specialrule{.2em}{.1em}{.1em} \n\\end{tabular}\n\\begin{tablenotes}[para,flushleft]\n\\footnotesize\n\\textsuperscript{$\\dagger$}Average accuracy on seven commonsense reasoning tasks. \n\\newline\n\\textsuperscript{$\\ddagger$}Measured with 12 input tokens, 128 output tokens, and a batch size of 1 on a single GPU.\n\\end{tablenotes}\n\\end{threeparttable}\n\\end{adjustbox}\n\\vspace{-0.05in}\n\n\\caption{Results with moderate-level pruning on LLaMA-7B (top) and Vicuna-13B-v1.3 (bottom). Our depth pruning (D\\ding{34}) with LoRA retraining achieves similar performance to width pruning (W\\ding{34}) methods~\\cite{wanda,flap,llmpruner} and outperforms the recent SLEB~\\cite{song2024sleb}, while effectively accelerating LLM inference. See Table~\\ref{supple_lora_results} for detailed results.} \\label{table:lora_results}\n\\vspace{-0.1in}\n\\end{table*}\n\\begin{table*}[t]\n\\centering\n\n\\begin{adjustbox}{max width=0.99\\linewidth}\n\\begin{threeparttable}\n\n\\begin{tabular}{cc|cccc|cccc|cccc}\n\\specialrule{.2em}{.1em}{.1em} \n\\multicolumn{2}{c|}{Metric}                                                          & \\multicolumn{4}{c|}{PPL↓ on WikiText2}                                                                                                                        & \\multicolumn{4}{c|}{Ave Acc↑ (\\%)\\textsuperscript{$\\dagger$}}                                                                                                                            & \\multicolumn{4}{c}{Throughput↑ (tokens/s)\\textsuperscript{$\\ddagger$}}                                                                                                                                                                                                                                                                                                                                                                                                 \\\\ \\hline\n\\multicolumn{2}{c|}{\\#Param after Pruning\\textsuperscript{$\\star$}}                                           & 5.5B                                  & 3.7B                                  & 2.7B                                  & 1.5B                                  & 5.5B                                  & 3.7B                                  & 2.7B                                  & 1.5B                                  & 5.5B                                                                                                     & 3.7B                                                                                                     & 2.7B                                                                                                      & 1.5B                                                                                                      \\\\ \\hline\n\\multicolumn{1}{l}{}                        & Wanda-sp                               & 24.4                                  & 364.5                                 & 1370.1                                & 8969.3                                & 58.5                                  & 36.7                                  & 37.0                                  & 35.6                                  & 41.7                                                                                                     & 40.5                                                                                                     & 40.7                                                                                                      & 43.5                                                                                                      \\\\\n\\multicolumn{1}{l}{}                        & FLAP                                   & 22.0                                  & 63.1                                  & 589.3                                 & 28727.9                               & 61.4                                  & 47.3                                  & 36.7                                  & 34.5                                  & 40.5                                                                                                     & 41.2                                                                                                     & 41.2                                                                                                      & 42.3                                                                                                      \\\\\n\\multicolumn{1}{l}{\\multirow{-3}{*}{W\\ding{34}}} & LLM-Pruner                             & 19.6                                  & 38.8                                  & 66.4                                  & 202.9                                 & 60.1                                  & 50.1                                  & 44.3                                  & 38.4                                  & 43.2                                                                                                     & 43.4                                                                                                     & 43.9                                                                                                      & 44.8                                                                                                      \\\\ \\hline\n                                            & SLEB                                   & 25.1                                  & 110.4                                 & 731.5                                 & 18730.8                               & 55.6                                  & 40.2                                  & 39.1                                  & 37.4                                  & \\textbf{66.0}                                                                                            & \\textbf{84.0}                                                                                            & \\textbf{107.4}                                                                                            & \\textbf{182.5}                                                                                            \\\\\n                                            & \\cellcolor[HTML]{ECF4FF}Ours, LoRA     & \\cellcolor[HTML]{ECF4FF}18.8          & \\cellcolor[HTML]{ECF4FF}37.0          & \\cellcolor[HTML]{ECF4FF}68.9          & \\cellcolor[HTML]{ECF4FF}1002.2        & \\cellcolor[HTML]{ECF4FF}60.7          & \\cellcolor[HTML]{ECF4FF}47.0          & \\cellcolor[HTML]{ECF4FF}40.1          & \\cellcolor[HTML]{ECF4FF}37.1          & \\cellcolor[HTML]{ECF4FF}                                                                                 & \\cellcolor[HTML]{ECF4FF}                                                                                 & \\cellcolor[HTML]{ECF4FF}                                                                                  & \\cellcolor[HTML]{ECF4FF}                                                                                  \\\\\n                                            & \\cellcolor[HTML]{ECF4FF}Ours, CPT      & \\cellcolor[HTML]{ECF4FF}\\textbf{14.3} & \\cellcolor[HTML]{ECF4FF}\\textbf{16.0} & \\cellcolor[HTML]{ECF4FF}\\textbf{17.1} & \\cellcolor[HTML]{ECF4FF}\\textbf{20.5} & \\cellcolor[HTML]{ECF4FF}61.5          & \\cellcolor[HTML]{ECF4FF}57.1          & \\cellcolor[HTML]{ECF4FF}\\textbf{55.0} & \\cellcolor[HTML]{ECF4FF}\\textbf{49.2} & \\cellcolor[HTML]{ECF4FF}                                                                                 & \\cellcolor[HTML]{ECF4FF}                                                                                 & \\cellcolor[HTML]{ECF4FF}                                                                                  & \\cellcolor[HTML]{ECF4FF}                                                                                  \\\\\n\\multirow{-4}{*}{D\\ding{34}}                     & \\cellcolor[HTML]{ECF4FF}Ours, CPT$\\Rightarrow$LoRA & \\cellcolor[HTML]{ECF4FF}14.8          & \\cellcolor[HTML]{ECF4FF}16.5          & \\cellcolor[HTML]{ECF4FF}17.8          & \\cellcolor[HTML]{ECF4FF}21.1          & \\cellcolor[HTML]{ECF4FF}\\textbf{63.1} & \\cellcolor[HTML]{ECF4FF}\\textbf{57.4} & \\cellcolor[HTML]{ECF4FF}\\textbf{55.0} & \\cellcolor[HTML]{ECF4FF}49.0          & \\multirow{-3}{*}{\\cellcolor[HTML]{ECF4FF}\\begin{tabular}[c]{@{}c@{}}\\textbf{66.0}\\\\ \\normalsize{(1.2×)}\\end{tabular}} & \\multirow{-3}{*}{\\cellcolor[HTML]{ECF4FF}\\begin{tabular}[c]{@{}c@{}}\\textbf{84.0}\\\\ \\normalsize{(1.6×)}\\end{tabular}} & \\multirow{-3}{*}{\\cellcolor[HTML]{ECF4FF}\\begin{tabular}[c]{@{}c@{}}\\textbf{107.4}\\\\ \\normalsize{(2.0×)}\\end{tabular}} & \\multirow{-3}{*}{\\cellcolor[HTML]{ECF4FF}\\begin{tabular}[c]{@{}c@{}}\\textbf{182.5}\\\\ \\normalsize{(3.4×)}\\end{tabular}} \\\\ \\hline\n\\multicolumn{2}{c|}{Vicuna-7B: 6.7B (Original)}                                                 & \\multicolumn{4}{c|}{17.1}                                                                                                                                     & \\multicolumn{4}{c|}{65.9}                                                                                                                                     & \\multicolumn{4}{c}{53.7}                                                                                                                                                                                                                                                                                                                                                                                                                    \\\\ \n\n\\specialrule{.2em}{.1em}{.1em} \n\\end{tabular}\n\n\\begin{tablenotes}[para,flushleft]\n\\footnotesize\n\\textsuperscript{$\\star$}The pruning ratios of 20\\%, 45\\%, 60\\%, and 80\\% lead to 5.5B, 3.7B, 2.7B, and 1.5B parameters, respectively. The PPL criterion is used to obtain our models.\n\\newline\n\\textsuperscript{$\\dagger$}Average accuracy on seven commonsense reasoning tasks. \n\\newline\n\\textsuperscript{$\\ddagger$}Measured with 12 input tokens, 128 output tokens, and a batch size of 1 on an NVIDIA H100 GPU.\n\\end{tablenotes}\n\\end{threeparttable}\n\\end{adjustbox}\n\n\\vspace{-0.05in}\n\\caption{Effectiveness of CPT under high compression rates on Vicuna-7B-v1.3. CPT is essential to regain the performance of extensively pruned models (e.g., fewer than 3.7B parameters), whereas retraining-free~\\cite{flap,song2024sleb} and LoRA-based~\\cite{llmpruner} approaches yield unsatisfactory results.} \\label{table:cpt_results}\n\\end{table*}\n\\begin{table*}[ht]\n\\centering\n\n\\begin{adjustbox}{max width=0.98\\linewidth}\n\\begin{threeparttable}\n\\begin{tabular}{p{1.8cm}|p{14.8cm}}\n\n\\specialrule{.2em}{.1em}{.1em} \nModel       & Generated Text \n\\\\ \\hline\n\\small{Vicuna-7B}    & \n\\footnotesize \\uline{AI can create a logo in seconds}! You choose the design elements you like best from pre-made templates, and then the AI generates a unique logo design just for you. This is a great way to get a professional-looking logo without hiring ...\n\\\\ \\hline\n\\small{Wanda-sp}    & \n\\footnotesize \\uline{AI can create a logo in seconds} now. Inso Ison Im Ison Im I I was able can do do do now do now do now Innowhere you you you you you you you you you you you you you you you you you you you you you you you you you you you ...\n\\\\ \\hline\n \\small{FLAP}        & \n\\footnotesize \\uline{AI can create a logo in seconds} The V Just ing A A A A A A A A A, A A A A A S Just a. : S . No An , A A the A The The... The The... A, A A A A. A A A This A The Just A A A A The A A The A A A. and The A All T A A.. A the ...\n\\\\ \\hline\n \\small{LLM-Pruner}  & \n\\footnotesize \\uline{AI can create a logo in seconds}: 10 seconds! I've created a logo in minutes using an AI. It has only one image in front for 8 minutes. 4 hours = 65.000 + 12 hours Words: English\nSpeech: 100 words in 5 minutes 1. 60 hours 2. 492 ...\n\\\\ \\hline\n\\small{SLEB} & \n\\footnotesize \\uline{AI can create a logo in seconds} while.  :). I have put. I believe it . →.]. Here least →… </<erus known). See →www.giftoings . </<.next]; info. I’ve don-> .…erutex“\nHere for. to “Itin.g for the next….. .0.>1260070uro.`s- ...\n\\\\ \\hline\n\\small{Ours, CPT}   & \n\\footnotesize \\uline{AI can create a logo in seconds}. Even if you don't have a designer who knows the best layouts to use or what colors work best together, AI is already hard at work creating the perfect combination to your artwork.\nAI is also capable of ...\n\\\\ \n\n\\specialrule{.2em}{.1em}{.1em}\n\\end{tabular}\n\\end{threeparttable}\n\\end{adjustbox}\n\n\\vspace{-0.05in}\n\\caption{Generation examples from the original Vicuna-7B and the 60\\%-pruned models with 2.7B parameters.}\\label{gen_output_2.7b}\n\\end{table*}\n\n\\section{Experimental Setup}\n\n\\paragraph{Source Model.} Our testbed includes LLaMA-7B~\\cite{touvron2023llama} and Vicuna-\\{7B, 13B\\}-v1.3~\\cite{vicuna}, which are famous LLMs.\n\n\\paragraph{Baseline.} LLM-Pruner~\\cite{llmpruner}, FLAP~\\cite{flap}, and Wanda-sp (i.e., a structured variant~\\cite{flap} of Wanda~\\cite{wanda}) serve as the baselines for width pruning. Table~\\ref{table:arch_short_ver} shows the pruned architectures under similar numbers of parameters. We also examine SLEB~\\cite{song2024sleb}, a retraining-free block pruning method for LLMs, which has been concurrently introduced with our study. Section~\\ref{sec:supple_baseline} describes the baselines in detail.\n\n\\paragraph{Data.} Following~\\citet{llmpruner}, we randomly select 10 samples from BookCorpus~\\cite{Zhu_2015_ICCV} to compute block-level significance during the pruning stage. We also use this calibration dataset for the baseline methods to ensure a fair comparison. In LoRA retraining, 50K samples of the refined Alpaca~\\cite{alpaca} are used for instruction tuning. In CPT retraining, we leverage SlimPajama~\\cite{cerebras2023slimpajama}, which consists of 627B tokens for LLM pretraining. \n\n\\paragraph{Evaluation.} Following~\\citet{touvron2023llama}, we measure zero-shot accuracy on commonsense reasoning datasets (i.e., BoolQ~\\cite{clark-etal-2019-boolq}, PIQA~\\cite{Bisk2020piqa}, HellaSwag~\\cite{zellers2019hellaswag}, WinoGrande~\\cite{sakaguchi2019winogrande}, ARC-easy~\\cite{clark2018think}, ARC-challenge~\\cite{clark2018think}, and OpenbookQA~\\cite{OpenBookQA2018}) using the lm-evaluation-harness package~\\cite{eval-harness}. We also report zero-shot PPL on WikiText2~\\cite{wikitext2} and PTB~\\cite{marcus-etal-1993-building}.\n\n\\paragraph{Latency and Throughput.} We follow~\\citet{sheng2023flexgen} to measure the metrics. Given a batch size $M$ and an output sequence length $L$ (excluding the input length), the latency $T$ represents the time required to handle the given prompts and produce $ML$ output tokens. The throughput is computed as $ML/T$. We report the average results from 20 runs after the initial 10 warm-up batches.\n\n\\paragraph{Implementation.} We use the Hugging Face's Transformers library~\\cite{wolf-etal-2020-transformers}. For pruning and LoRA retraining, an NVIDIA A100 GPU is employed. For CPT retraining, eight NVIDIA H100 GPUs are utilized, with a training duration of less than two weeks for each model size. For inference, we opt for the default setup of the Transformers library. See Section~\\ref{sec:supple_impl_details} for the details.\n\n\\section{Results}\n\n\\subsection{Moderate Pruning and LoRA Retraining}\nTables~\\ref{table:lora_results}~and~\\ref{supple_lora_results} show the zero-shot performance and inference efficiency of differently pruned models. Here, our models are obtained using a light LoRA retraining setup. The width pruning methods~\\cite{llmpruner,flap,wanda} do not improve LLM inference efficiency. Under limited input (batch) scales, the processing speed largely hinges on the frequency of memory access operations. Addressing this issue by merely reducing matrix sizes is challenging, unless they are completely removed. The speed even worsens compared to the original model due to GPU-unfriendly operation dimensions (e.g., the hidden sizes of FFN are often not divisible by 8 (Table~\\ref{supple_table:arch}), which hinders the effective utilization of GPU Tensor Cores~\\cite{tensor_core_guide}). \n\nOn the contrary, our depth pruning exhibits speedups through the complete removal of several Transformer blocks, resulting in fewer memory access and matrix-level operations between activations and weights. Moreover, under the same LoRA retraining protocol as~\\citet{llmpruner}, our models achieve zero-shot scores on par with finely width-pruned models. Although SLEB~\\cite{song2024sleb} enhances inference efficiency similar to our method, its approach without retraining falls short in developing proficient small LLMs. See Section~\\ref{sec:supple_moderate_pruning} for detailed results.\n\n\\begin{figure}[t]\n  \\centering\n    \\includegraphics[width=\\linewidth]{fig/compare_cpt_init.pdf}\n  \\caption{Zero-shot scores during the training progress of the 2.7B-parameter model from Vicuna-7B. Using the pruned network as initialization (blue lines) for CPT accelerates the learning process and yields better results than starting from scratch (purple).}\n  \\label{fig_learncurve_cpt}\n  \\vspace{-0.2in}\n\\end{figure}\n\n\\subsection{Aggressive Pruning and CPT Retraining}\nTable~\\ref{table:cpt_results} compares different retraining methods. Our models are obtained using the PPL criterion. Under high pruning ratios (e.g., yielding fewer than 3.7B parameters), LoRA-based tuning (LLM-Pruner~\\cite{llmpruner}; Ours, LoRA) and retraining-free approaches (Wanda-sp~\\cite{wanda,flap}, FLAP~\\cite{flap}, SLEB~\\cite{song2024sleb}) fail to recover model performance. In contrast, CPT proves effective in regaining the quality of heavily pruned models. CPT$\\Rightarrow$LoRA slightly improves zero-shot accuracy for some pruning ratios, but with a minor drop in PPL. Table~\\ref{gen_output_2.7b} presents samples produced by 2.7B-parameter models (60\\% pruned). In contrast to the baselines, our model can generate text that is fluent and appropriately aligned with the context.\n\nCompared to LoRA retraining, the computational costs for CPT are considerably higher: LoRA can be completed within a day using just one GPU, while CPT requires about two weeks with eight GPUs in our experiments, with the option to use more if needed. However, utilizing a pruned network for initialization in CPT leads to faster learning and better results than building the same-sized models from scratch (see Figure~\\ref{fig_learncurve_cpt}), highlighting its efficacy for smaller LLMs. Section \\ref{sec:supple_aggressive_pruning} presents the learning progress in detail.\n\n\\begin{figure}[t]\n  \\centering\n    \\includegraphics[width=\\linewidth]{fig/gptq_results.png}\n        \\vspace{-0.25in}\n  \\caption{Further compression with GPTQ. Our pruned models following 4-bit weight quantization exhibit reduced VRAM usage without significant performance decline. The results for the original Vicuna-7B are presented for reference. See Section~\\ref{sec:appendix_gptq} for the details.}\n  \\vspace{-0.1in}\n  \\label{fig_gptq_measure}\n\\end{figure}\n\n\\subsection{Applicability with Quantization} \nLeveraging post-training quantization (PTQ) effectively lowers the memory consumption for inference of LLMs. Figure~\\ref{fig_gptq_measure} shows the results of applying GPTQ~\\cite{frantar2023optq}, a well-known PTQ method, to our depth-pruned models after CPT. The 4-bit weight quantization significantly reduces the VRAM demands across various model sizes without noticeable degradation in zero-shot accuracy. See Section~\\ref{sec:appendix_gptq} for further results.\n\n\\subsection{Ablation Study}\nWe explore various design factors, including the criteria for importance evaluation, the choice of units for depth pruning, and the impact of calibration data volume. The results presented in this section were obtained through LoRA retraining.\n\n\\subsubsection{Importance Criteria for Block Pruning} \nTable~\\ref{table:criterion} presents the results of block pruning using various significance criteria. The basic methods without the `+' label fail to maintain essential initial blocks, causing a decline in performance. The Mag+ method, which preserves these critical blocks, partially improves the scores; however, its effectiveness is still inferior compared to the other methods, indicating that relying solely on weight magnitude could be improper for pruning decisions. The Taylor+ criterion enhances accuracy in commonsense reasoning tasks, while the PPL method leads to better generation quality without relying on heuristic selection of pruning candidates.\n\n\\subsubsection{Structural Unit for Depth Pruning} \nPruning individual MHA and FFN modules, which are more fine-grained units than Transformer blocks, is also possible. To examine its effect, we measure the impact of removing each module on the PPL of the calibration set and selectively eliminate the unnecessary modules. The same LoRA retraining procedure is conducted.\n\n\\begin{table}[t]\n\\centering\n\n\\begin{adjustbox}{max width=0.88\\columnwidth}\n\\begin{threeparttable}\n\\begin{tabular}{cc|cc|c}\n\n\\specialrule{.2em}{.1em}{.1em} \n\n\\multicolumn{2}{c|}{\\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Block Pruning\\\\ Criterion\\end{tabular}}} & \\multicolumn{2}{c|}{PPL↓} & \\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Ave Acc↑\\\\  (\\%)\\textsuperscript{$\\dagger$} \\end{tabular}} \\\\\n\\multicolumn{2}{c|}{}                                                                                   & WikiText2    & PTB        &                                                                          \\\\ \\hline\n\\multirow{5}{*}{\\begin{tabular}[c]{@{}c@{}}5.5B\\\\ (20\\%\\\\Pruned)\\end{tabular}}       & Mag       & 7720.7       & 10618.7    & 34.4                                                                     \\\\\n                                                                                      & Mag+      & 19.4         & 36.3       & 56.1                                                                     \\\\\n                                                                                      & Taylor        & 3631.7       & 4327.9     & 35.5                                                                     \\\\\n                                                                                      & Taylor+       & 20.2         & 32.3       & \\textbf{63.5}                                                                     \\\\\n                                                                                      & PPL             & \\textbf{17.7}         & \\textbf{30.7}       & 61.9                                                                     \\\\ \\hline\n\\multirow{5}{*}{\\begin{tabular}[c]{@{}c@{}}4.5B\\\\ (35\\%\\\\Pruned)\\end{tabular}}       & Mag       & 8490.1       & 14472.1    & 34.9                                                                     \\\\\n                                                                                      & Mag+      & 36.9         & 61.1       & 49.3                                                                     \\\\\n                                                                                      & Taylor        & 7666.8       & 10913.1    & 35.3                                                                     \\\\\n                                                                                      & Taylor+       & 33.2         & 58.5       & \\textbf{55.4}                                                                     \\\\\n                                                                                      & PPL            & \\textbf{23.1}         & \\textbf{38.8}       & 55.2                                                                     \\\\ \n\n\\specialrule{.2em}{.1em}{.1em} \n\\end{tabular}\n\\begin{tablenotes}[para,flushleft]\n\\footnotesize\n\\textsuperscript{$\\dagger$}Average accuracy on seven commonsense reasoning tasks. \n\\end{tablenotes}\n\\end{threeparttable}\n\\end{adjustbox}\n\n\\vspace{-0.05in}\n  \n\\caption{Comparison of pruning criteria on LLaMA-7B. The Taylor+ method excels in commonsense reasoning accuracy, while the PPL criterion leads to better generation performance.} \\label{table:criterion}\n\n\\vspace{+0.15in}\n\n\\begin{adjustbox}{max width=\\columnwidth}\n\\begin{threeparttable}\n\\begin{tabular}{c|c|cc|c}\n\n\\specialrule{.2em}{.1em}{.1em} \n\n\\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Depth Pruning\\\\ Unit\\end{tabular}} & \\multirow{2}{*}{\\#Param} & \\multicolumn{2}{c|}{PPL↓} & \\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Ave Acc↑\\\\  (\\%)\\textsuperscript{$\\dagger$} \\end{tabular}} \\\\\n                                                                              &                          & WikiText2      & PTB      &                                                                          \\\\ \\hline\nIndividual MHA \\& FFN                                                                & 5.7B                     & 20.8           & 34.8     & \\textbf{63.1}                                                                     \\\\\nTransformer Block                                                             & 5.7B                     & \\textbf{16.9}           & \\textbf{29.3}     & 62.8                                                                     \\\\ \\hline\nIndividual MHA \\& FFN                                                                & 5.3B                     & 25.2           & 41.3     & \\textbf{61.1}                                                                     \\\\\nTransformer Block                                                             & 5.3B                     & \\textbf{18.6}           & \\textbf{33.1}     & 60.6                                                                     \\\\ \\hline\nIndividual MHA \\& FFN                                                                & 4.6B                     & 38.9           & 58.7     & 52.5                                                                     \\\\\nTransformer Block                                                             & 4.5B                     & \\textbf{23.1}           & \\textbf{38.8}     & \\textbf{55.2}                                                                     \\\\ \\hline\nIndividual MHA \\& FFN                                                                & 4.0B                     & 63.2           & 88.9     & 48.3                                                                     \\\\\nTransformer Block                                                             & 3.9B                     & \\textbf{31.1}           & \\textbf{47.3}     & \\textbf{50.6}                                                                     \\\\ \n\n\\specialrule{.2em}{.1em}{.1em} \n\\end{tabular}\n\\begin{tablenotes}[para,flushleft]\n\\footnotesize\n\\textsuperscript{$\\dagger$}Average accuracy on seven commonsense reasoning tasks. \n\\end{tablenotes}\n\\end{threeparttable}\n\\end{adjustbox}\n\n\\vspace{-0.05in}\n\\caption{Comparison of depth pruning granularities on LLaMA-7B. Removing entire Transformer blocks instead of individual MHA and FFN modules generally yields better results.} \\label{table:ablation_pruneunit}\n\\vspace{-0.1in}\n\\end{table}\n\nTable~\\ref{table:ablation_pruneunit} shows the results of depth pruning at different granularities. For the models with more than 5B parameters, removing individual MHA and FFN modules results in better downstream task accuracy but worse PPL compared to removing entire Transformer blocks. For smaller models than 5B, block-level pruning achieves superior results in terms of all the examined metrics. This differs from the common belief that removing finer units yields better performance.\n\nGiven the collaborative roles of the modules (i.e., MHA captures dependency relations~\\cite{transformer}, while skip connections and FFN prevent the rank collapse in purely attention-driven networks~\\cite{dong2021attention}), it may be suboptimal to treat them in isolation. Taking the 5.3B model in Table~\\ref{table:ablation_pruneunit} as an example, module-level pruning results in consecutive FFNs in some positions, potentially impairing the model's ability to handle word interactions. In contrast, with block-level removal, the loss of information could be compensated by neighboring blocks that serve similar functions.\n\n\\subsubsection{Calibration Data Volume}\n The calibration set is employed to assess the weight significance of width pruning baselines and the block-level importance of our method during the pruning phase. Table~\\ref{table:calib_data_volume} presents the results obtained by varying the number of calibration samples in the BookCorpus dataset. The scores remain relatively stable for the examined methods, suggesting that 10 samples could be sufficient. However, our Taylor+ method encounters a drop in downstream task accuracy when 1K samples are used, leaving the exploration of calibration data characteristics for future research.\n\n\\section{Related Work} \\label{relwork}\n\nNumerous techniques have been developed towards efficient LLMs, including knowledge distillation~\\cite{fu2023specializing,hsieh2023distilling}, quantization~\\cite{frantar2023optq,dettmers2022llmint8}, and system-level inference acceleration~\\cite{dao2023flashattention2,kwon2023efficient}. In this study, we focus on network pruning~\\cite{lecun1989optimal}, which has a long-standing reputation in the model compression field. Beyond its use in relatively small-scale convolutional networks~\\cite{li2016pruning,he2019filter} and Transformer models~\\cite{yu2022unified,xia2022structured,kurtic2023ziplm}, pruning has recently begun to be applied to contemporary LLMs. Several studies~\\cite{frantar-sparsegpt,wanda} employ unstructured and semi-structured~\\cite{zhou2021} pruning by zeroing individual neurons. SparseGPT~\\cite{frantar-sparsegpt} addresses the layer-wise reconstruction problem for pruning by computing Hessian inverses. Wanda~\\cite{wanda} introduces a pruning criterion that involves multiplying weight magnitudes by input feature norms. Despite the plausible performance of pruned models using zero masks, they necessitate specialized support for sparse matrix operations to ensure actual speedups.\n\nIn contrast, structured pruning removes organized patterns, such as layers~\\cite{fan2019reducing,jha2023train}, MHA's attention heads~\\cite{voita2019analyzing,michel2019sixteen}, FFN's hidden sizes~\\cite{nova2023gradientfree,santacroce2023matters}, and some hybrid forms~\\cite{lagunas2021block,xia2022structured,kwon2022fast,kurtic2023ziplm}, thereby improving inference efficiency in a hardware-agnostic way. To compress LLMs, FLAP~\\cite{flap} and LLM-Pruner~\\cite{llmpruner} eliminate coupled structures in the aspect of network width while retaining the number of layers. Sheared-LLaMA~\\cite{xia2023sheared} introduces a mask learning phase aimed at identifying prunable components in both the network's width and depth. Our study explores the relatively untapped area of depth-only pruning for multi-billion parameter LLMs, which can markedly accelerate latency while attaining competitive performance.\n\nStrategies for skipping layers~\\cite{schuster2022confident,delcorro2023skipdecode,raposo2024mixtureofdepths} effectively serve to decrease computational burdens. Moreover, depth pruning approaches~\\cite{song2024sleb,men2024shortgpt,tang2024rethinking} for LLMs have been proposed concurrently with our work, based on the architectural redundancy in LLMs.\n\n\\begin{table}[t]\n\\centering\n\\begin{adjustbox}{max width=0.94\\columnwidth}\n\\begin{threeparttable}\n\\begin{tabular}{cc|cccc}\n\n\\specialrule{.2em}{.1em}{.1em} \n\n\\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Evaluation\\\\ Metric\\end{tabular}} & \\multirow{2}{*}{Method} & \\multicolumn{4}{c}{\\# Calibration Samples}                    \\\\\n                                                                             &                         & 10            & 50            & 100           & 1000          \\\\ \\hline\n\\multirow{5}{*}{\\begin{tabular}[c]{@{}c@{}}PPL↓ on\\\\ WikiText2\\end{tabular}} & Wanda-sp                & 21.4          & 21.4          & 21.7          & 20.8          \\\\\n                                                                             & FLAP                    & \\textbf{17.0} & 17.5          & 17.5          & \\textbf{17.3} \\\\\n                                                                             & LLM-Pruner              & 17.6          & \\textbf{17.2} & \\textbf{17.0} & OOM\\textsuperscript{$\\ddagger$}           \\\\\n                                                                             & Ours: Taylor+             & 20.2          & 20.2          & 19.0          & 19.6          \\\\\n                                                                             & Ours: PPL               & 17.7          & \\textbf{17.2} & 17.4          & 17.4          \\\\ \\hline\n\\multirow{5}{*}{\\begin{tabular}[c]{@{}c@{}}Ave Acc↑\\\\  (\\%)\\textsuperscript{$\\dagger$} \\end{tabular}}     & Wanda-sp                & 51.8          & 52.9          & 52.0          & 53.0          \\\\\n                                                                             & FLAP                    & 59.5          & 59.7          & 59.9          & 60.8          \\\\\n                                                                             & LLM-Pruner              & 61.8          & 61.6          & 61.7          & OOM\\textsuperscript{$\\ddagger$}           \\\\\n                                                                             & Ours: Taylor+             & \\textbf{63.5} & \\textbf{63.5} & \\textbf{63.9} & \\textbf{61.7} \\\\\n                                                                             & Ours: PPL               & 61.9          & 61.5          & 61.7          & \\textbf{61.7} \\\\ \n                                                                             \n                                                                             \n\n\\specialrule{.2em}{.1em}{.1em} \n\n\\end{tabular}\n\\begin{tablenotes}[para,flushleft]\n\\footnotesize\n\\textsuperscript{$\\dagger$}Average accuracy on seven commonsense reasoning tasks. \n\\newline\n\\textsuperscript{$\\ddagger$}Out-of-memory error on an A100 (80GB) using the official code. \n\\end{tablenotes}\n\\end{threeparttable}\n\\end{adjustbox}\n\n\\vspace{-0.05in} \n\\caption{Impact of calibration data volume. The results of 20\\%-pruned LLaMA-7B are reported.} \\label{table:calib_data_volume}\n\\vspace{-0.1in}\n\\end{table}\n\n\\section{Conclusion}\nBy introducing a block pruning method, we conduct an in-depth comparative analysis on the impact of network width and depth on LLM compression. Our work involves the one-shot removal of Transformer blocks. Despite its simplicity, our method with light LoRA retraining matches the zero-shot capabilities of recent width pruning techniques under moderate pruning levels. Moreover, it offers significant inference speedups in resource-constrained scenarios that require running LLMs with limited batch sizes, where width pruning falls short. When comparing retraining strategies, continued pretraining on a large-scale dataset significantly surpasses LoRA-based tuning, particularly in cases of severe pruning. We hope this study will support the development of potent small LLMs.\n\n\\section*{Limitations}\nDue to constraints in computational resources, we could not test our method on LLMs exceeding 13B parameters. We plan to explore larger models in future research, given that our method can be applied to any model size. Secondly, we found that continued pretraining was essential for performance recovery after extensive pruning. Further exploration of different training corpora and hyperparameters could lead to additional performance improvements. Lastly, commercially available LLMs are optimized for human preferences, such as safety and helpfulness, through alignment tuning. We have yet to assess human preferences throughout the entire process of pruning, retraining, and quantization. We hope future research will address this aspect.\n\n\\section*{Acknowledgments}\nWe thank the Microsoft Startups Founders Hub program and the AI Industrial Convergence Cluster Development project funded by the Ministry of Science and ICT (MSIT, Korea) and Gwangju Metropolitan City for their generous support of GPU resources.\n\n\\clearpage\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{ShortGPT: Layers in Large Language Models are More Redundant Than You Expect}\n\n\\begin{document}\n\n\\maketitle\n\t\\begin{abstract} \\label{lab:abstract}\n\t\tAs Large Language Models (LLMs) continue to advance in performance, their size has increased significantly, with current LLMs containing billions or even trillions of parameters.  In this study, we identify notable redundancy across the layers of LLMs, where some layers contribute minimally to overall network functionality. To quantify this, we introduce a metric called Block Influence (BI) which use the similarity between layer's input and output to measure the importance of each layer. Based on the observation of layer redundancy, we propose a straightforward pruning method: layer removal, which eliminates redundant layers based on their BI scores. Our approach, termed ShortGPT, demonstrates superior performance over previous state-of-the-art pruning methods.  Moreover, ShortGPT is orthogonal to quantization-like methods, enabling further reduction in parameters and computation. The ability to achieve better results through simple layer removal, as opposed to more complex pruning techniques, suggests a high degree of redundancy across layers, not only in transformer models but also in non-transformer models. We hope this work will contribute to future research in LLM compression.\n\t\\end{abstract}\n\t\\section{Introduction}\\label{lab:intro}\n\tThe field of large language models (LLMs) has witnessed rapid development recently, with LLMs achieving impressive performance across various domains. Guided by the scaling laws identified in prior work \\citep{kaplan2020scaling,hoffmann2022training}, current LLM research tend to increase model parameters to boost performance. As a result, modern LLMs, which can comprise billions to trillions of parameters, require significant hardware resources for deployment, creating substantial barriers to their practical use.       \n\t\n\tTo mitigate the hardware demands of large models, model compression techniques have become a critical area of focus \\citep{zhu2023survey}. These techniques are generally divided into quantization \\citep{liu2021post,gholami2022survey,dettmers2022llm,dettmers2024qlora} and pruning\\citep{lecun1989optimal,han2015learning,frantar2023massive}. Quantization reduces the precision of model parameters, but its effectiveness often requires specific hardware support. In contrast, pruning method removes redundant parameters to decrease the model's size and computation, offering a more flexible and hardware-agnostic approach. Despite its advantages, many existing pruning methods are  complex; for example, some require gradient information \\citep{ma2024llm}, which limits their practicality.\n\t\n\t\n\tIn this paper, we focus on the issue of layer redundancy in LLMs and propose a novel approach for simplifying these models. We introduce \\textbf{Block Influence (BI)}, a metric that quantifies how much the hidden state changes after passing through each layer, providing a more direct measure of a layer's importance. Leveraging this insight, we propose a simple yet effective pruning method \\textbf{ShortGPT}, which identifies and removes layers with lower BI scores, significantly reducing model size without sacrificing much performance. \n\t\n\tTo evaluate our approach, we conducted evaluation across comprehensive benchmarks.  Our experiments revealed that our method exhibits a smaller performance decrement compared to the previous methods. For instance,  removing 10 layers (25\\% of the total 40 layers) from the LLaMA 2-13B model resulted in only a slight drop in performance on the MMLU benchmark \\citep{hendrycks2020measuring}, from 55.0 to 52.2. Our findings highlight substantial redundancy in current LLMs and suggest potential avenues for improving the efficiency of model training by reducing inherent redundancy in the future.\n\t\n\tThe main contributions of our paper are summarized as follows:\n\t\\begin{itemize}\n\t\t\\item We analyze the redundancy in large language models (LLMs) and find that they exhibit significant redundancy at the layer level. This finding inspire us to prune LLMs by simply removing redundant layers. \n\t\t\\item We propose a metric called Block Influence (BI) as an  indicator of layer importance. Based on BI,  our layer removal method maintains approximately 90\\% performance while reducing approximately 25\\% of  parameters, outperforming previous state-of-the-art methods.\n\t\t\\item Furthermore, we demonstrate that our layer pruning approach is orthogonal to quantization methods, meaning it can be combined with quantization techniques to further reduce the deployment overhead of LLMs.\n\t\\end{itemize}\t\n\t\n\t\\begin{figure}[t]\n\t\t\\centering\n\t\t\\begin{subfigure}[t]{0.45\\textwidth}\n\t\t\t\\centering  \n\t\t\t\\includegraphics[width=\\textwidth]{image1/paper_pdfs/background-redundancy-ppl-v2.pdf}\n\t\t\t\\caption{Perplxity}\n\t\t\t\\label{fig:overall_illustartion_llama2:1}\n\t\t\\end{subfigure}\n\t\t\\hfill\n\t\t\\begin{subfigure}[t]{0.45\\textwidth}\n\t\t\t\\centering  \n\t\t\t\\includegraphics[width=\\textwidth]{image1/paper_pdfs/background-redundancy-mmlu-v2.pdf}\n\t\t\t\\caption{MMLU}\n\t\t\\end{subfigure}\n\t\t\\caption{Performance of removing certain layer from LLMs. We can see that certain layers are redundant, and their removal results in minimal performance degradation. }    %大图名称\n\t\t\\label{fig:background-redundancy}    %图片引用标记\n\t\\end{figure}\n\t\\section{Motivation}\n\t\\subsection{Background}\n\t\n\tThe predominant LLMs are primarily based on the Transformer architecture \\citep{vaswani2017attention}, with the pre-norm configuration being the most commonly adopted, as in models like LLaMA \\citep{touvron2023llama}. The pre-norm configuration, where layer normalization is applied before the self-attention and feed-forward layers, offers several advantages such as faster convergence, improved training stability, and better scalability for deeper networks \\citep{xiong2020layer, liu2020understanding, wang2024deepnet}. Due to these benefits, the pre-norm approach has been adopted even in non-transformer models, such as  Mamba \\citep{gu2023mamba} and RWKV \\citep{peng2023rwkv}. For the sake of simplicity in descriptions, our analysis primarily focuses on the Transformer architecture, though we extend our experiments to non-Transformer structures in Section \\ref{sec:non-transformer}. \n\t\n\t\t\\begin{figure}[h]\n\t\t\t\\centering  \n\t\t\t\\includegraphics[width=0.8\\textwidth]{image1/paper_pdfs/sim2.pdf}\n\t\t\t\\caption{The cosine similarity  between a layer's input and output during the training process. The horizontal axis (X-axis) represents the number of training tokens, while the vertical axis (Y-axis) depicts the degree of similarity. Notably, the model employing post-normalization exhibits divergence after approximately $\\sim$26B tokens of training. Training setting is provided in \\ref{appendix:post and pre}.}\n\t\t\t\\label{fig:background-similarity}    %图片引用标记\n\t\t\\end{figure}\n\t\t\n\t\tHowever, we observe that when pre-norm is adopted, the similarity between the input and output of transformer layers tends to be higher, as illustrated in Figure \\ref{fig:background-similarity}. This high similarity indicates that certain layers induce minimal changes to the hidden states, suggesting they contribute little to the model’s overall function. A detailed mathematical explanation for this phenomenon is provided in Appendix \\ref{appendix:math}. Which suggests that the deep layers of the model with pre-norm might not play a critical role in the overall function, and that \\textbf{the layers in large language models could be more redundant than expected}, which motivates the layer-removal based pruning method we explore in the next section.\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\\subsection{Layer redundancy}\\label{layerredundancy}\n\t\t\\begin{wraptable}{r}{0.45\\textwidth}\\label{tab:last layer}\n\t\t\t\t\\centering\n\t\t\t\t\\caption{Ablation of removing FFN and Attention of Llama2-7B-Base. We sample 100 instances from PG19 \\citep{rae2019compressive} to calculate PPL.}\n\t\t\t\t\\label{tab:last_layer}\n\t\t\t\t\\begin{tabular}{@{}ll@{}}\n\t\t\t\t\t\\toprule\n\t\t\t\t\t\\textbf{Delete} & \\textbf{PPL} \\\\ \\midrule\n\t\t\t\t\tNone & 7.60\\\\\n\t\t\t\t\tThe whole last layer & 13.37 \\\\\n\t\t\t\t\tAttention of the last layer & 7.65 \\\\\n\t\t\t\t\tFFN of the last layer & 12.35 \\\\ \\bottomrule\n\t\t\t\t\\end{tabular}\n\t\t\\end{wraptable}\n\t\tAs discussed in the previous section, we speculate that the LLMs exhibit layer redundancy. To verify this, we assess the performance degradation caused by removing individual layers of two popular models, Llama2-7B-Base \\citep{touvron2023llama}, an English based LLMs, and Baichuan2-7B-Base \\citep{yang2023baichuan} which is mainly focused on Chinese. Figure \\ref{fig:background-redundancy} confirms our speculation, which reveals that some layers do not play a crucial role in LLMs, causing little degradation when omitting them individually. Moreover, this redundancy is primarily manifested in the middle to later layers of the network, with the initial layers and the last layer often being more critical. Notably, we found the last layer to be particularly important, aligning with findings from LLM Pruner \\citep{ma2024llm}. This observation contradicts our mathematical explanation in Appendix \\ref{appendix:math} which suggests that deeper layers tend to be more redundant. We posit that this discrepancy arises because the final FFN effectively functions as part of the token classifier and should be considered in conjunction with the language model head.To verify our hypothesis, we conducted further investigation, detailed in Table \\ref{tab:last_layer}. The results show that within the last layer, the FFN component is crucial, while the Attention module is less significant. This finding supports our interpretation of the final layer's importance.\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\\section{Methodology}\n\t\tIn this section, we present the methodological framework of our layer removal approach for LLMs, elucidating the underlying principles and techniques employed. We begin by introducing Block Influence (BI), a novel metric designed to assess the hidden states transformation of each layer. Leveraging BI, we then detail our layer removal method.\n\t\t\n\t\t\n\t\t\n\t\t\\subsection{Layer importance} \\label{method:layerimportacne}\n\t\tAs outlined in the preceding section, the layers of LLMs exhibit redundancy, with varying degrees of redundancy across different layers. To capture this, we introduce a new metric, Block Influence (BI), to measure the degree of transformation performed by each layer.   The BI score of $i^{th}$ layer can be calculated as follows: \n\t\t\n\t\t\n\t\t\\begin{align}\n\t\t\t\\text{BI}_i = 1 - \\mathbb{E}_{X,t} \\frac{X_{i,t}^TX_{i+1,t}}{||X_{i,t}||_2||X_{i+1,t}||_2},\n\t\t\\end{align}\n\t\twhere $X_{i,t}$ means the $t^{th}$ row of hidden states of $i^{th}$ layer. Lower BI score imply that $X_i$ and $X_{i+1}$ exhibit high cosine similarity, suggesting that the layer makes minimal transformations to the hidden states and is therefore less important. We plot the BI scores of a single layer and the PPL after removing it separately, as shown in the Figure \\ref{fig:bi_ppl}. The results demonstrate a positive correlation between the BI score and the importance of a layer.\n\t\t\n\t\t\\begin{figure}[t]\n\t\t\t\\centering\n\t\t\t\\begin{subfigure}[t]{0.49\\textwidth}\n\t\t\t\t\\centering  \n\t\t\t\t\\includegraphics[width=\\textwidth]{image1/paper_pdfs/llama_bi_ppl.pdf}\n\t\t\t\t\\caption{Llama2 7B}\n\t\t\t\\end{subfigure}\n\t\t\t\\hfill\n\t\t\t\\begin{subfigure}[t]{0.49\\textwidth}\n\t\t\t\t\\centering  \n\t\t\t\t\\includegraphics[width=\\textwidth]{image1/paper_pdfs/bc_bi_ppl.pdf}\n\t\t\t\t\\caption{Baichuan2 7B }\n\t\t\t\\end{subfigure}\n\t\t\t\\caption{The BI score of a layer and the PPL after removing the layer. }    %大图名称\n\t\t\t\\label{fig:bi_ppl}    %图片引用标记\n\t\t\\end{figure}\n\t\t\n\t\t\n\t\t\\subsection{Layer Removal}\n\t\tOur goal is to obtain a pruned model that remains as close as possible to the original model. Since an LLM functions as a series of transformations applied to hidden states across its layers and we can determine the importance of each layer, we propose a straightforward pruning method: layer removal, which we refer to as ShortGPT. We delete certain layers in LLMs based on BI score. First of all, we construct a calibration set, which is a set of unlabelled text samples such as PG19 \\citep{rae2019compressive}. \n\t\tThen we collect the hidden states of each layer during inference on these samples. Next, we calculate the BI score based on the collected hidden states. Finally, we sort layers in ascending order according to the BI, and delete the layers with the lower BI score. The number of layers to be deleted can vary to trade off the speed and performance. The details of our layer removal setting can be found in Appendix \\ref{appendix:remove_strategy}.\n\t\t\n\t\t\\section{Experiments}\\label{exp}\n\t\t\\subsection{Experimental Setup} \\label{label:exp_setup}\n\t\t\\paragraph{Models.}To validate the effectiveness of our method, we conducted experiments on existing popular open-source language models, including Llama2-7B \\citep{touvron2023llama}, Llama2-13B, Baichuan2-7B, and Baichuan2-13B. They are all large language models based on the decoder-only Transformer architecture. LLaMA 2 was trained on more than 2 trillion tokens. Baichuan-series was mainly trained in Chinese and its 13-Billion model replaced the RoPE \\citep{su2024roformer} positional embedding with ALiBi \\citep{press2021train}. \n\t\t\n\t\t\\paragraph{Benchmarks.} In order to comprehensively evaluate the changes in the ability of large language models before and after pruning, we conducted comprehensive evaluation from five aspect: \\textbf{Reasoning}: CMNLI \\citep{li2024cmmlu}, HellaSwag (HeSw) \\citep{zellers2019hellaswag}, PIQA \\citep{bisk2020piqa}. \\textbf{Language}: CHID \\citep{zheng2019chid},  WSC (Levesque et al., 2012). \\textbf{Knowledge}: CommonSenseQA (CoQA) \\citep{reddy2019coqa}, BoolQ \\citep{clark2019boolq}. \\textbf{Examination}: MMLU \\citep{hendrycks2020measuring}, CMMLU \\citep{li2024cmmlu}. \\textbf{Understanding}: Race-High/Middle (H/M) \\citep{lai2017race}, XSum \\citep{hasan2021xl}, C3 \\citep{sun2020investigating} and PG19 \\citep{rae2019compressive}. For more details, please refer to Appendix \\ref{appendix:benchmark}\n\t\t\n\t\t\\paragraph{Baselines.} \n\t\t\n\t\tTo evaluate the effectiveness of our method, we compared several structured pruning methods for large language models, including:\n\t\t\n\t\t\\textbf{1) LLMPru} \\citep{ma2024llm}, which  adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM’s functionality. LLMPru. applies post training to the pruned model, but for fair comparison, we do not apply post training to it.\n\t\t\n\t\t\\textbf{2) SliceGPT} \\citep{ashkboos2024slicegpt}, which is a post-training sparsification scheme that replaces each weight matrix with a smaller matrix, reducing the embedding dimension of the network. Specifically, they applied PCA to the hidden representation from shallow to deep layers, and incorporated the dimension reduction matrix into existing network parameters.\n\t\t\n\t\t\\textbf{3) LaCo} \\citep{yang2024laco}, which is a pruning method for large language models based on reducing layers. LaCo gradually merges similar layers from deep to shallow and sets a threshold to avoid continuously merging too many layers.\n\t\t\n\t\tFor our evaluation, we use PG19 for layer importance and perplexity calculation. The models, baselines and evaluate benchmarks is the same as LaCo. \n\t\t\n\t\t\\renewcommand\\arraystretch{1.3} \n\t\t\\begin{table}[t]\n\t\t\t\\tiny\n\t\t\t\\setlength{\\tabcolsep}{1.6pt}\n\t\t\t\\caption{Comparison of pruning methods on multiple natural language benchmarks. The results of LLMPrun., SliceGPT and LaCo are reported from LaCo. The last column reports the relative performance retention.}\n\t\t\t\\label{tab:llm_comparison_all}\n\t\t\t\\centering\n\t\t\t\\begin{tabular}{c|c|c|ccccccccccccc|cc}\n\t\t\t\t\\hline\n\t\t\t\t\\hline\n\t\t\t\t\\multirow{2}{*}{LLM} & \\multirow{2}{*}{Method}  & \\multirow{2}{*}{Ratio}& \\multicolumn{13}{c|}{Benchmarks}& \\multirow{2}{*}{Ave.} & \\multirow{2}{*}{Per.} \\\\  \n\t\t\t\t& & & CMNLI & HeSw&PIQA&CHID&WSC&CoQA&BoolQ&Race-H&Race-M&XSum&C3 &MMLU & CMMLU& &\\\\\n\t\t\t\t\\hline\n\t\t\t\t\\multirow{5}{*}{Llama2-7B} & Dense  & 0.00\\% &32.99 &71.26 &77.91  &41.66 &50.00  &64.62 &71.62 &35.71  &34.19 &19.40 &43.56&45.39& 32.92 &47.78 &100.00\n\t\t\t\t\\\\\n\t\t\t\t& LLMPrun. & 27.0\\% & \\textbf{34.33} &\\textbf{56.46}& \\textbf{71.22} &25.25 &36.54  &42.51 &55.20 &22.56 &22.35 &11.51 &25.64& 23.33  &  25.25 &34.78&72.79\\\\\n\t\t\t\t& SliceGPT  & 26.4\\%& 31.70 &50.27 &66.21 &20.79 &36.54 & 41.36& 38.32&  21.07 &21.66& 4.89& \\textbf{39.78}&28.92 & 25.37 &32.84&68.73   \\\\\n\t\t\t\t& LaCo  & 27.1\\%& 34.43 &55.69& 69.80 &\\textbf{36.14} &40.38  &45.70 &64.07 &22.61& 23.61 &\\textbf{15.64} &39.67&  26.45& 25.24&38.41& 80.39\\\\\n\t\t\t\t& ShortGPT  & 27.1\\%& 32.95\t&53.02\t&66.43\t&24.68\t&\\textbf{52.46}\t&\\textbf{47.99}\t\t&\\textbf{74.71}&\t\\textbf{32.25}&\t\\textbf{35.17}&\t0.67\t&39.62&\\textbf{43.96} &\t\\textbf{32.25}&\\textbf{41.24}&\\textbf{86.31} \\\\\n\t\t\t\t\\hline\n\t\t\t\t\\multirow{5}{*}{Llama2-13B} & Dense &0.00\\%&32.99 \t&74.78\t&79.71\t&47.35\t&50.00\t&66.91 &82.39 &57.95\t&60.38\t&23.45\t&47.51&55.00 &38.40&55.14 &100.00  \\\\\n\t\t\t\t& LLMPrun. &24.4\\%&\\textbf{33.03} &\\textbf{67.76} &\\textbf{76.66} &35.64 &40.38  &50.86 &56.42  &22.47 &22.08 &\\textbf{19.17} &32.33 &25.21& 24.71&38.97&70.67\\\\\n\t\t\t\t& SliceGPT &23.6\\%&29.82 &55.71 &69.04 &19.31 &36.54  &47.26 &37.86 &23.41 &24.03 &5.27 &41.92 &  37.14& 25.79&34.85&63.20\\\\\n\t\t\t\t& LaCo &24.6\\%&32.86 &64.39 &63.20 &\\textbf{40.10} &\\textbf{52.88}  &52.66&\\textbf{63.98} &54.49 &56.55 &14.45 &44.93&  45.93& 32.62 &47.62 &86.36\\\\\n\t\t\t\t& ShortGPT  &24.6\\%&33.00\t&66.64&\t73.45&\t36.61\t&50.00\t\t&\\textbf{58.64}&\t62.48\t&\\textbf{58.35}\t&\\textbf{60.17}&\t17.59\t&\\textbf{46.90}&\\textbf{54.69}\t&\\textbf{38.38}&\\textbf{50.53}& \\textbf{91.64} \\\\\n\t\t\t\t\\hline\n\t\t\t\t\\multirow{5}{*}{Baichuan2-7B} & Dense &0.00\\%&33.37 &67.56\t&76.17\t&85.56\t&50.00\t &63.14 &74.10 &52.63\t&51.04\t &20.82\t&64.55& 53.87\t& 56.95 &57.67&100.00 \\\\\n\t\t\t\t& LLMPrun. &24.2\\%&32.28 &53.66 &\\textbf{71.82} &69.80 &\\textbf{53.85} &\\textbf{47.83} &61.19 &21.96 &22.28 &\\textbf{15.98} &41.64 &  24.93 & 25.69 &41.76& 72.41 \\\\\n\t\t\t\t& SliceGPT &22.2\\%&32.07 &25.29 &50.33 &14.85 &36.54  &19.57 &39.30 &23.53 &22.49 &0.00 &26.58&  25.18 &25.25&26.23&45.48 \\\\\n\t\t\t\t& LaCo &24.2\\%&33.00 &52.28 &68.50 &\\textbf{76.24} &42.31 &47.26 &56.15 &28.99 &27.72 &12.03 &50.85& 31.53 &31.24&42.93 & 74.44 \\\\\n\t\t\t\t& ShortGPT &24.2\\%&\\textbf{33.30} \t&\\textbf{56.96}\t&67.68\t&65.63\t&50.00\t &46.70 &\\textbf{67.83} &\\textbf{53.26} &\\textbf{46.76}&0.04 &\\textbf{56.33}& \\textbf{45.77} &\t\\textbf{47.87} &\\textbf{49.08}&\\textbf{85.10} \\\\\n\t\t\t\t\\hline\n\t\t\t\t\\multirow{5}{*}{Baichuan2-13B} & Dense&0.00\\%& 33.21 \t&71.10\t&78.07\t&86.51\t&50.00 &65.6 &77.89 &67.27\t&68.94\t &25.02\t&65.64 &  59.50 &61.30&62.31&100.00   \\\\\n\t\t\t\t& LLMPrun. &24.3\\%&\\textbf{33.80} &53.57 &\\textbf{71.82} &72.77 &37.50  &38.82 &56.54 &21.17 &21.61 &13.67 &39.89&  23.19 & 25.18&39.20&62.91 \\\\\n\t\t\t\t& SliceGPT &22.8\\%&32.07 &25.85 &51.03 &10.40 &36.54  &18.02 &37.83 &21.56 &21.52 &0.00 &24.99&  22.95 & 25.26&25.23& 40.49\\\\\n\t\t\t\t& LaCo &24.7\\%&33.03 &\\textbf{60.71} &68.88 &76.73 &44.23  &\\textbf{55.45} &62.35  &\\textbf{56.92} &\\textbf{57.80} &12.32 &\\textbf{61.10}&  51.35 & 53.65&53.43& 85.75 \\\\\n\t\t\t\t& ShortGPT &24.7\\%&32.81 \t&60.55\t&\\textbf{71.60}\t&\\textbf{80.17}\t&\\textbf{47.13}\t &54.30 &\\textbf{62.54} &55.77\t&56.41\t &\\textbf{15.14}\t\t&60.16 &\\textbf{52.11}   &\\textbf{58.86} &\\textbf{54.43}&\\textbf{87.35} \\\\\n\t\t\t\t\\hline\n\t\t\t\t\\hline\n\t\t\t\\end{tabular}\n\t\t\t\n\t\t\\end{table}\n\t\t\n\t\t\\subsection{Main Results}\n\t\t\n\t\tTo validate the efficacy of our proposed method, we conducted comparative experiments against baseline techniques commonly employed in large language model evaluation. Considering the current structured pruning methods generally reduce parameters by no more than 30\\%, we performed experiments with approximately 1/4 of the parameters pruned. The experimental results are presented in Table \\ref{tab:llm_comparison_all}. Additional experiments exploring different parameter reduction proportions will be discussed in the subsequent section.\n\t\t\n\t\tThe results demonstrate that the performance of the model pruned by our method significantly surpasses that of the baseline methods, maintaining most of the large language model's capabilities. Furthermore, we note that the approach of reducing the number of layers (ShortGPT/LaCo) outperforms the method of reducing the embedding dimensions (LLMPru./SliceGPT), implying that the model exhibits more redundancy in depth than in width. Further experimental analysis will be presented in the ensuing section.\n\t\t\n\t\tIn Table \\ref{tab:llm_comparison_all}, we fully adopted the benchmark, model, and pruning ratio in the LaCo paper. In order to make a more fair comparison with LLMprun. and SliceGPT, we compared them with the same benchmark, model, and pruning ratio in their original paper. The experimental results are shown in Appendix \\ref{appendix:fair compare}. Consistent with our findings in Table \\ref{tab:llm_comparison_all}, these experiments further demonstrate the significant layer redundancy present in existing large language models, and ShortGPT achieves superior performance compared to other pruning methods.\n\t\t\n\t\tThe results show that coarse-grained pruning methods, such as removing entire layers, often outperform fine-grained approaches like Slice GPT or LLM Pruner.  We speculate that the reason is that the large language model is actually very robust, as shown in Figure \\ref{fig:background-redundancy}, removing any deep layer individually actually has very little impact on the final output, which means it is difficult to define the importance of a finer grained module and perform pruning.\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\\subsection{Varying  metric and pruning ratio} \\label{ana:layerimportance}\n\t\t\\begin{figure}[h]\n\t\t\t\\centering\n\t\t\t\\includegraphics[width=0.9\\textwidth]{image1/paper_pdfs/multi_metric.pdf}\n\t\t\t\\caption{\\label{fig:importance-comp}Comparison of different importance metrics. Perplexity is calculated by removing each single layer, other metrics is calculated by hidden states of each layer.}\n\t\t\\end{figure}\n\t\tThe core principle of our method is to rank layers by their importance and remove the less significant ones. The choice of importance metric significantly influences the outcome. In this section, we define and compare several different importance metrics:\n\t\t\\begin{itemize}\n\t\t\t\\item \\textbf{Sequential}: The importance is directly proportional to the sequence order, with shallower layers being less important. This can be implemented by  assigning the negative value of each layer's index as its importance metric.\n\t\t\t\n\t\t\t\\item \\textbf{Norm/Reverse-order}: This metric posits that importance is inversely proportional to the sequence order. It assigns higher importance scores to the shallower layers. This method gives the same order as measuring importance by hidden states norm as Figure \\ref{fig:importance-comp} shows.\n\t\t\t\n\t\t\t\\item \\textbf{Relative Magnitude}: Proposed in \\cite{samragh2023weight}, this metric assumes layers with larger $ ||\\frac{f(x)}{x+f(x)}||$ are of higher importance, where $f$ is the layer transformation function.\n\t\t\t\n\t\t\t\\item \\textbf{BI}: we calculate the BI score mentioned in Section \\ref{method:layerimportacne} as importance metric.\n\t\t\\end{itemize}\n\t\t\n\t\t\n\t\t\n\t\t\n\t\tFigure \\ref{fig:importance-comp} demonstrates the different metrics. We observe that shallower layers in the LLM network are more crucial than deeper ones. Figure \\ref{fig:cum-methods-compare} shows the results of removing layers by different metrics, demonstrating that Our proposed BI outperforms other metrics. The method of Relative Magnitude is highly competitive, indicating that relative values can also reflect the importance to some extent. It is worth noting that the hidden states norm seems to be a good metric when only considering the MMLU benchmark, but the perplexity is relatively poor. \n\t\t\n\t\t\\begin{figure}[t]\n\t\t\t\\centering\n\t\t\t\\includegraphics[width=0.9\\textwidth,height=0.6\\textwidth]{image1/paper_pdfs/Cummlative-llama-metrics-compare.pdf}\n\t\t\t\\caption{\\label{fig:cum-methods-compare} Performance of MMLU and perplexity when we prune by different metrics, with increasing pruning ratio. We can see that as the pruning ratio increases, the performance of the model declines.}\n\t\t\\end{figure}\n\t\tAs a pruning method, we further validated the effects of different pruning ratios on model performance. Experiments were conducted on the Llama2 and Baichuan2 models, observing the Perplexity and MMLU. The results for Llama2, as shown in Figure \\ref{fig:cum-methods-compare}, demonstrate that the model's performance generally declines as the pruning ratio increases. However, we observe a notable phenomenon: the MMLU score exhibits a sharp drop at a specific layer. This sudden decrease suggests the presence of certain critical layers within the network that play a particularly important role in maintaining performance. Similar patterns are observed in the Baichuan2 model, as illustrated in Appendix \\ref{appendix:details_bc}. \n\t\t\n\t\t\n\t\t\\subsection{Redundancy on non-transformer LLM}\\label{sec:non-transformer}\n\t\t\n\t\tTo determine whether the observed depth redundancy is specific to the Transformer architecture, we extended our investigation to include two popular non-Transformer models,  RWKV-7B\\footnote{ We use rwkv-v5-world-7B from https://huggingface.co/RWKV/v5-Eagle-7B-HF} \\citep{peng2023rwkv} and Mamba-2.8B  \\footnote{We take the model from https://huggingface.co/state-spaces/mamba-2.8b-hf} \\citep{gu2023mamba}. Our experiments revealed that these models also exhibit resilience to layer removal, maintaining performance despite the elimination of certain layers. This finding suggests that the redundancy phenomenon may not be unique to Transformer-based models, but rather a common characteristic across current large language models. Table \\ref{tab:rwkv_mamba} shows that our method is applicable and effective for both Mamba and RWKV models, suggesting that the redundancy is universal across current LLMs. However, it is worth noting that the RWKV model appears less redundant than Mamba and Transformer models, which warrants further investigation.\n\t\t\n\t\t\n\t\t\n\t\t\\renewcommand\\arraystretch{1.3}\n\t\t\\begin{table}[t]\n\t\t\t\\tiny\n\t\t\t\\setlength{\\tabcolsep}{2.2pt}\n\t\t\t\\caption{ShortGPT pruning on RWKV and Mamba.}\n\t\t\t\\label{tab:rwkv_mamba}\n\t\t\t\\centering\n\t\t\t\\begin{tabular}{c|c|ccccccccccccccc}\n\t\t\t\t\\toprule\n\t\t\t\tModel & Pruning ratio & CMNLI & HeSw & PIQA & CHID & WSC & CoQA & BoolQ & Race-H & Race-M & XSum & C3 & MMLU & CMMLU  &Ave. & Per. \\\\\n\t\t\t\t\\midrule\n\t\t\t\t\\multirow{5}{*}{Mamba-2.8B}\n\t\t\t\t& 0\\% & 35.97 & 61.84 & 75.52 & 35.56 & 49.69 & 56.35 & 60.67 & 24.9 & 25.3 & 15.03 & 42.08 & 26.29 & 25.32 & 41.12 & 100.00 \\\\\n\t\t\t\t& 10.9\\% & 32.95 & 59.71 & 73.01 & 32.52 & 49.28 & 52.66 & 51.41 & 24.27 & 25.21 & 14.95 & 41.1 & 26.01 & 25.00 & 39.08 & 95.04  \\\\\n\t\t\t\t& 20.3\\% & 31.29 & 55.69 & 69.64 & 29.12 & 48.36 & 48.32 & 62.2 & 23.61 & 23.61 & 14.71 & 41.59 & 25.69 & 25.37 & 38.36 & 93.29 \\\\\n\t\t\t\t& 25\\% & 29.96 & 52.38 & 68.77 & 26.02 & 48.26 & 44.96 & 62.2 & 23.67 & 23.26 & 14.00 & 40.71 & 24.32 & 24.89 & 37.18 & 90.42 \\\\\n\t\t\t\t& 31.3\\% & 28.25 & 47.02 & 64.91 & 21.38 & 49.69 & 44.96 & 62.17 & 21.87 & 22.77 & 13.77 & 40.44 & 24.48 & 24.77 & 35.59 & 86.55 \\\\\n\t\t\t\t\\midrule\n\t\t\t\t\\multirow{5}{*}{RWKV-7B}\n\t\t\t\t& 0\\% & 32.07 & 65.98 & 77.09 & 85.36 & 50.00 & 62.65 & 62.72 & 38.56 & 45.47 & 16.5 & 57.97 & 31.85 & 28.54 & 50.37 & 100.00 \\\\\n\t\t\t\t& 9.4\\% & 32.6 & 56.41 & 73.94 & 78.12 & 50.00 & 49.55 & 62.35 & 25.9 & 25.77 & 9.57 & 54.68 & 27.29 & 25.03 & 43.94 & 87.23 \\\\\n\t\t\t\t& 18.8\\% & 32.11 & 49.47 & 71.55 & 65.63 & 50.00 & 40.54 & 61.19 & 22.04 & 23.75 & 8.13 & 49.15 & 26.35 & 25 & 40.38 & 80.17\\\\\n\t\t\t\t& 25\\% & 32.41 & 39.73 & 65.13 & 52.6 & 50.00 & 29.65 & 60.92 & 22.56 & 21.59 & 12.02 & 41.86 & 25.52 & 25.08 & 36.85 & 73.16\\\\\n\t\t\t\t& 28.1\\% & 33.11 & 32.22 & 60.01 & 32.47 & 50.1 & 28.34 & 60.85 & 22.27 & 21.31 & 10.43 & 37.81 & 25.64 & 25.15 & 33.82 & 67.14\\\\\n\t\t\t\t\\bottomrule\n\t\t\t\\end{tabular}\n\t\t\\end{table}\n\t\t\n\t\t\n\t\t\\subsection{Orthogonal to Quantization}\n\t\tIn this section, we show that our method is orthogonal to quantization methods. We apply our method to Llama2-7B \\footnote{We take the model from https://huggingface.co/TheBloke/Llama-2-7B-GPTQ} quantized by GPTQ algorithm. Table \\ref{lab:orthogonal} shows that our method is compatible with the quantization-like method. In addition, we compared the performance of applying pruning before quantization \\footnote{We use GPTQ algorithm for quantization from https://github.com/AutoGPTQ/AutoGPTQ}. The results shown in the Table \\ref{tab:performance_comparison} further indicates that quantization and ShortGPT are orthogonal operations.\n\t\t\\renewcommand\\arraystretch{1.2} \n\t\t\\begin{table}[t]\n\t\t\t\\small\n\t\t\t\\caption{Layer removal results on Llama2-7B-Base-GPTQ.}\n\t\t\t\\label{lab:orthogonal}\n\t\t\t\\centering\n\t\t\t\\begin{tabular}{ccccc}\n\t\t\t\t\\hline\n\t\t\t\tModel    & Ratio/Layer & Perplexity & MMLU & Throughput (speed up) \\\\ \\hline \\hline\n\t\t\t\t\n\t\t\t\tBaseline & 0\\%/32 &8.03 &43.17  & 4331.23 Token/s (1.00x) \\\\ \\hline\n\t\t\t\t&  3.1\\%/31   & 8.37   &42.88 & 4399.31 Token/s (1.02x)  \\\\ \n\t\t\t\t& 9.4\\%/29   &9.44    & 42.31 & 4602.26 Token/s (1.06x)   \\\\ \n\t\t\t\tShortGPT    & 12.5\\%/28   &10.24 &41.62      & 4680.68 Token/s (1.08x)  \\\\ \n\t\t\t\t& 15.6\\%/27   &11.42    &43.17  & 4756.94 Token/s (1.10x)   \\\\ \n\t\t\t\t& 25.0\\%/24 &22.29  &41.68  & 5045.59 Token/s  (1.16x)  \\\\ \n\t\t\t\t& 27.1\\%/23   &40.78&43.35 & 5146.99 Token/s  (1.19x)  \\\\ \n\t\t\t\t\\hline\n\t\t\t\\end{tabular}\n\t\t\\end{table}\n\t\t\n\t\t\n\t\t\\begin{table}[t]\n\t\t\t\\small\n\t\t\t\\caption{Performance comparison of different methods}\n\t\t\t\\label{tab:performance_comparison}\n\t\t\t\\centering\n\t\t\t\\begin{tabular}{@{}lcc@{}}\n\t\t\t\t\\toprule\n\t\t\t\tMethod & MMLU & CMMLU \\\\\n\t\t\t\t\\midrule     \\midrule\n\t\t\t\tLlama2-7B-Baseline & 45.4 & 32.9 \\\\\n\t\t\t\t\\addlinespace\n\t\t\t\t4-bit quantization & 44.9 & 32.5 \\\\\n\t\t\t\t\\addlinespace\n\t\t\t\tLayer removal (27.1\\%) & 44.0 & 32.3 \\\\\n\t\t\t\t\\addlinespace\n\t\t\t\t4-bit quantization then layer removal & 42.4 & 31.0 \\\\\n\t\t\t\t\\addlinespace\n\t\t\t\tLayer removal then 4-bit quantization & 41.2 & 30.5 \\\\\n\t\t\t\t\\bottomrule\n\t\t\t\\end{tabular}\n\t\t\\end{table}\n\t\t\n\t\t\n\t\t\\subsection{Post training to restore performance} \\label{sec:post training}\n\t\tTo mitigate the performance loss resulting from layer removal, we explored post-training strategies inspired by \\cite{chen2024compressing}. Our approach comprised two key steps: 1)Replacement: We substituted the removed layers with lightweight Multi-Layer Perceptron (MLP) modules. 2)Retraining: We subsequently retrained the modified model. The results in Table \\ref{tab:replace} demonstrate the potential of post-train in recover performance loss. Appendix \\ref{appendix:post-train} list the training details.\n\t\t\\begin{table}[htbp]\n\t\t\t\\setlength{\\tabcolsep}{2.8pt}\n\t\t\t\\caption{Post-train Llama2-7B to restore performance.}\n\t\t\t\\label{tab:replace}\n\t\t\t\\centering\n\t\t\t\\tiny\n\t\t\t\\begin{tabular}{@{}lccccccccccccccc@{}}\n\t\t\t\t\\toprule\n\t\t\t\tMethod & Avg. & Ratio & CMNLI & HeSw & PIQA & CHID & WSC & CoQA & BoolQ & Race-H & Race-M & XSum & C3 & MMLU & CMMLU \\\\\n\t\t\t\t\\midrule\n\t\t\t\tDense & 47.78 & 0\\% & 32.99 & 71.26 &77.91& 41.66 & 50.00 & 64.62 & 71.62 & 35.71 & 34.19 & 19.40 & 43.56 & 45.39&32.92 \\\\\n\t\t\t\tShortGPT & 41.22 & 27.1\\% & 32.95 & 53.02 & 66.43 & 24.68 & 52.46 & 47.99 & \t74.41\t&32.25\t&35.17\t&0.67\t&39.62\t&43.96&\t32.25 \\\\\n\t\t\t\tShortGPT+post-train &43.16\t&24.0\\% & 32.99 &\t54.83&\t68.12&\t31.82&51.37&\t58.32\t&72.36\t&34.18\t&34.68&\t4.89\t&40.37\t&44.47\t&32.73 \\\\\n\t\t\t\t\\bottomrule\n\t\t\t\\end{tabular}\n\t\t\\end{table}\n\t\t\n\t\t\n\t\t\\section{Limitation}\\label{limitation}\n\t\tAlthough our method demonstrates strong competitiveness compared to current pruning methods, there are some phenomena that have not been explained. Our experiments reveal that the negative effect of layer removal is more significant on generative tasks compared to multiple-choice tasks. When we remove 25\\% layers from Llama2-7B or Baichuan2-7B, the performance in generative tasks such as XSum and C3 deceases to nearly zero, although the performance decline was not as significant on the larger model of the 13B. We speculate that compared to multiple-choice tasks, generative tasks face the problem of accumulated errors and large model is more robust than small one. The reasons behind it still need to be explored. The post-training techniques discussed in Section \\ref{sec:post training} have the potential to mitigate this issue and warrant further exploration.\n\t\t\n\t\t\n\t\t\n\t\t\\section{Related works}\n\t\tTo reduce the inference cost of large language models and increase their practical applications, there have been many recent works on compressing models, which can be classified into two categories:\n\t\tmodel pruning and quantization. Besides, there are some works aim to study the redundancy of model which is essential for compressing models.\n\t\t\n\t\t\\textbf{Model pruning:} model pruning \\citep{lecun1989optimal,han2015learning} is a classic and effective method of reducing model redundancy modules to compress models. The model pruning methods mainly include unstructured pruning and structured pruning. The unstructured pruning simplifies an LLM by removing specific parameters without considering its internal structure, such as SparseGPT \\citep{frantar2023massive} and LoRAPrune \\citep{zhang2023pruning}. However, this method disregards the overall LLM structure, resulting in an irregular sparse model composition. Another more practical approach is structured pruning, GUM\\citep{syed2023prune} makes an analysis of several structured pruning methods for decoder-only LLMs. LLM-Pruner \\citep{ma2024llm}  selectively removes non-critical structures according to gradient information. ShearedLLaMA \\citep{xia2023sheared} employs targeted structured pruning and dynamic batch loading. LaCo \\citep{yang2024laco} used layer merging to compress the model. Compared to the previous method, our method is a simple and efficient structured pruning method.\n\t\t\n\t\t\n\t\t\\textbf{Quantization:} quantization \\citep{liu2021post,gholami2022survey,dettmers2022llm,dettmers2024qlora} is a widely accepted technique in the field of model compression, which can significantly save the storage and computational costs of deep learning models. Traditional models are generally stored as floating-point numbers, but quantization converts them into integers or other discrete forms. LUT-GEMM \\citep{park2022nuqmm} quantifies only weights and optimizes matrix multiplication in LLM using BCQ format. SPQR \\citep{dettmers2023spqr}  identifies and isolates abnormal weights, stores them with higher accuracy, and compresses all other weights into 3-4 bits. Our model pruning method and quantization method are orthogonal, which means quantification based on our pruned model can further compress the model.\n\t\t\n\t\t\n\t\t\\textbf{Model redundancy:} researchers have long noticed the significant redundancy in nonlinear models \\citep{catchpole1997detecting}. In recent years, the transformer model architecture has been widely applied, and researchers have also studied its redundancy. In \\citep{bian2021attention}, researchers analyzed redundancy in attention mechanisms, in which clear and similar redundancy patterns (cluster structure) are observed among attention heads. In \\citep{dalvi2020analyzing}, researchers dissect two pre-trained models, BERT \\citep{devlin2018bert} and XLNet \\citep{yang2019xlnet}, studying how much redundancy they exhibit at a representation level and a more fine-grained neuron-level. However, the redundancy in current large language models based on decoder-only structures still needs to be explored. \n\t\t\n\t\t\\section{Conclusion}\n\t\t\n\t\tIn this work,  we uncovered the significant layer-wise redundancy of LLMs, Our research demonstrates that certain layers contribute minimally to overall network functionality and can be removed without substantially compromising model performance. Based on our observation, We introduce Block influence to quantify the importance of each layer and propose a simple and straightforward pruning method: layer removal. Our experiments demonstrates that it is possible to maintain up to approximately 90\\% of a LLM's performance while reducing the model's parameter amount and computational requirements by approximately 25\\%. Besides, our method is orthogonal to quantization methods and can be further improved by continual training. We hope that our work can provide some insight for future model compression techniques. \n\t\tMoreover, our work suggests potential avenues for improving the efficiency of model training by reducing inherent redundancy in the future.\n\t\t\n\t\t\\newpage\n\t\t\t\t\t\t\n\t\t\\newpage\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2305.11627v3.tex",
        "arXiv-2402.02834v2.tex",
        "arXiv-2403.03853v3.tex"
    ],
    "group_id": "group_37",
    "response": "### Title: Structured Pruning of Large Language Models (LLMs): A Comparative Analysis\n\n### Introduction\nThe field of large language models (LLMs) has seen remarkable advancements in recent years, with models like LLaMA, Vicuna, and ChatGLM achieving impressive performance in language understanding and generation tasks. However, this performance comes at a significant cost: the models are extremely large, often containing billions or trillions of parameters. This size poses substantial challenges in deployment, inference, and training, necessitating the exploration of efficient compression techniques. Traditional compression methods, such as knowledge distillation and quantization, have been applied to smaller models like BERT, but the application to LLMs is more complex due to their vast scale and diverse capabilities. Recent research has focused on structured pruning, which aims to remove non-critical structures within the model while preserving its overall functionality. This approach is particularly relevant for LLMs, as it can maintain their multi-task solving and language generation abilities without relying heavily on the original training dataset. The current progress in LLM compression includes methods like LLM-Pruner, FLAP, and ShortGPT, each offering unique strategies for reducing model size and computational overhead.\n\nThe history of model compression dates back to the early days of neural networks, where techniques like pruning and quantization were introduced to reduce the number of parameters and improve computational efficiency. With the advent of LLMs, these methods have been adapted and refined to address the specific challenges posed by these large models. However, the deployment and inference of LLMs remain constrained by their extensive computational demands and memory requirements. The goal is to develop compression techniques that can reduce the size of LLMs while maintaining their performance across various tasks, especially in zero-shot settings. This summary will explore three recent papers that delve into structured pruning of LLMs, each offering a distinct approach and highlighting different aspects of the compression process.\n\n### Main Content of Each Paper\n\n#### Paper 1: LLM-Pruner: On the Structural Pruning of Large Language Models\nThis paper introduces LLM-Pruner, a novel framework designed to compress LLMs in a task-agnostic manner. The primary objective is to reduce the model size while preserving its multi-task solving and language generation capabilities. LLM-Pruner addresses the challenges of limited data availability and long post-training times by employing a dependency detection algorithm to identify coupled structures within the model. These structures are then pruned based on an importance estimation strategy that considers both first-order and approximated Hessian information. The framework ensures that the pruning process minimally disrupts the original model's functionality. Post-pruning, a rapid recovery stage using low-rank approximation (LoRA) is executed to fine-tune the pruned model with limited data. The authors validate LLM-Pruner on three LLMs: LLaMA, Vicuna, and ChatGLM, demonstrating that the compressed models maintain satisfactory performance in zero-shot classification and generation tasks. The results show that even with a 20% parameter reduction, the pruned models retain 94.97% of the original model's performance, and the recovery process takes only 3 hours on a single GPU with 50k data samples.\n\n#### Paper 2: Shortened LLaMA: Depth Pruning for Large Language Models with Comparison of Retraining Methods\nThis paper focuses on depth pruning, a method that removes entire Transformer blocks from LLMs while keeping the size of the remaining weights unchanged. Depth pruning is contrasted with width pruning, which reduces the size of projection weight matrices. The authors argue that depth pruning is more effective in scenarios with limited batch sizes, as it can significantly improve inference speeds. They introduce several criteria for evaluating the importance of each block, including magnitude, Taylor approximation, and perplexity (PPL), and demonstrate that depth pruning can achieve comparable or superior performance to width pruning under moderate pruning levels. For severe pruning ratios (over 50%), continued pretraining (CPT) on a large-scale dataset is crucial for performance recovery, outperforming LoRA-based tuning. The authors show that depth pruning can reduce the model size by up to 37% while maintaining high zero-shot performance. They also highlight the orthogonality of their method with quantization techniques, enabling further compression of LLMs.\n\n#### Paper 3: ShortGPT: Layers in Large Language Models are More Redundant Than You Expect\nThis paper investigates the redundancy of layers in LLMs and proposes a method called ShortGPT for layer removal. The authors introduce a metric called Block Influence (BI) to measure the importance of each layer, based on the transformation of hidden states. They demonstrate that certain layers, particularly deeper ones, contribute minimally to the model's overall functionality and can be removed without significant performance degradation. ShortGPT achieves superior performance compared to other pruning methods, such as SliceGPT and LLM-Pruner, especially in terms of zero-shot reasoning and language tasks. The authors also extend their investigation to non-Transformer models, showing that the redundancy phenomenon is not unique to Transformer-based architectures. They further validate the effectiveness of their method by combining it with quantization techniques, demonstrating that layer removal and quantization can be applied orthogonally to achieve additional compression.\n\n### Commonalities and Innovations\nAll three papers address the challenge of compressing LLMs to reduce computational overhead and improve deployment efficiency. They introduce novel methods for structured pruning, focusing on different aspects of the model's architecture:\n\n- **LLM-Pruner** emphasizes task-agnostic compression and employs dependency detection to identify coupled structures within the model. It uses both first-order and Hessian-based importance estimation to prune these structures efficiently, followed by a rapid recovery stage using LoRA. The innovation lies in its ability to compress LLMs with minimal reliance on the original training dataset and its fast recovery process.\n\n- **Shortened LLaMA** focuses on depth pruning, removing entire Transformer blocks to improve inference efficiency, especially under memory-constrained conditions. The paper introduces several criteria for evaluating block importance, such as magnitude, Taylor approximation, and perplexity, and demonstrates the effectiveness of continued pretraining (CPT) for severe pruning ratios. The innovation is the one-shot removal of Transformer blocks and the combination of depth pruning with quantization methods.\n\n- **ShortGPT** investigates layer redundancy in LLMs and introduces Block Influence (BI) as a metric to measure layer importance. It proposes a simple layer removal method, which is validated on both Transformer and non-Transformer models. The innovation is the identification of layer redundancy and the demonstration that coarse-grained pruning methods (removing entire layers) can outperform fine-grained approaches (removing individual modules).\n\n### Comparison of Results and Discussion\nThe three papers present different approaches to structured pruning of LLMs, each with its own set of advantages and limitations. \n\n- **LLM-Pruner** achieves high zero-shot performance with a 20% parameter reduction, maintaining 94.97% of the original model's accuracy. It is particularly effective in reducing the model size while preserving its multi-task solving capabilities. However, it requires a significant amount of time for post-training, even with limited data.\n\n- **Shortened LLaMA** demonstrates that depth pruning can significantly improve inference efficiency, especially under memory-constrained conditions. The paper shows that removing 20% of the layers can achieve similar or better performance compared to width pruning methods, and that continued pretraining (CPT) is crucial for severe pruning ratios. The results indicate that depth pruning can be combined with quantization methods to further reduce the model size and computational overhead.\n\n- **ShortGPT** identifies substantial layer redundancy in LLMs and shows that removing up to 25% of the layers can maintain up to 90% of the model's performance. The paper introduces Block Influence (BI) as a metric to quantify layer importance and demonstrates that layer removal is effective for both Transformer and non-Transformer models. The innovation lies in the simplicity of the layer removal method and its orthogonality with quantization techniques.\n\nThe results of these papers highlight the potential of structured pruning methods in reducing the size of LLMs while maintaining their performance. However, there are differences in the effectiveness of different pruning strategies, especially under severe pruning ratios. For instance, while LLM-Pruner and ShortGPT achieve high zero-shot performance with moderate pruning ratios, they face challenges when the pruning ratio exceeds 50%. Shortened LLaMA, on the other hand, shows that continued pretraining (CPT) is essential for severe pruning ratios, leading to better performance recovery compared to LoRA-based tuning.\n\n### Conclusion\nThe main findings of these papers are that structured pruning methods can effectively reduce the size of LLMs while preserving their performance in zero-shot settings. LLM-Pruner uses dependency detection and importance estimation to prune coupled structures efficiently, followed by a rapid recovery stage using LoRA. Shortened LLaMA focuses on depth pruning, removing entire Transformer blocks to improve inference efficiency, especially under memory-constrained conditions. ShortGPT identifies layer redundancy and proposes a simple layer removal method, which is effective for both Transformer and non-Transformer models. The orthogonality of these methods with quantization techniques suggests potential avenues for further compression of LLMs.\n\nFuture research directions could include exploring larger models and different retraining strategies to address the challenges of severe pruning ratios. Additionally, understanding the impact of pruning on generative tasks and developing more robust post-training techniques could further improve the performance of compressed LLMs. The orthogonality of layer removal and quantization methods also opens up possibilities for combining these techniques to achieve even greater compression."
}