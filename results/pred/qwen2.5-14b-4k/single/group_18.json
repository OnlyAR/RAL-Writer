{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{L-Eval: Instituting Standardized Evaluation for Long Context Language Models}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nRecently, there has been growing interest in extending the context length of large language models (LLMs), aiming to effectively process long inputs of one turn or conversations with more extensive histories. While proprietary models such as GPT-4 and Claude can largely preserve the reasoning ability in an extended context, open-source models are still progressing through the early stages of development. \nTo bridge this gap, we propose L-Eval to institute a more standardized evaluation for long context language models (LCLMs) addressing two key aspects: dataset construction and evaluation metrics. On the one hand, we build a new evaluation suite containing 20 sub-tasks, 508 long documents, and over 2,000 human-labeled query-response pairs encompassing diverse question styles, domains, and input length (3k$\\sim$200k tokens). On the other hand, we investigate the effectiveness in evalution metrics for LCLMs. Results show that popular n-gram matching metrics generally can not correlate well with human judgment, and thus we strongly advocate for length-instruction-enhanced (LIE) evaluation and employing LLM judges.  We conducted a comprehensive study of 4 popular commercial LLMs and 12 open-source counterparts using the L-Eval benchmark. Our empirical findings offer useful insights into the study of LCLMs and lay the groundwork for the development of more principled evaluation of these models.\\footnote{We release our new evaluation suite, code, and all generation results on  \\url{https://github.com/OpenLMLab/LEval}}\n\n\\end{abstract}\n\n\\section{Introduction}\nCurrently, a significant amount of effort is being dedicated to research on extending the context length of large language models. Popular solutions mainly involve further pretraining or finetuning standard models on longer inputs using more efficient architectures~\\citep{ding2023longnet, NEURIPS2022_67d57c32, liang2023unleashing, mohtashami2023landmark, li2023incontext}, as well as scaled positional embedding~\\citep{su2022roformer, sun2022lengthextrapolatable, fixedNTK, qin2023linear}. \n\nThere are extensive multi-task benchmarks~\\citep{hendrycks2021measuring, suzgun2022challenging} for language models with short prompts, yet a high-quality one in long context modeling has not yet been established, presenting an opportunity for further development in this area.  Meanwhile, almost all previous long-sequence text generation benchmarks relied primarily on n-gram matching metrics~\\citep{zhang2023cab, shaham2022scrolls}, such as ROUGE~\\citep{lin-2004-rouge}. Whether these commonly used metrics correlate well with human judgment when testing LCLMs in a zero-shot setting remains a question. Furthermore, the open-source community has released a considerable number of language models with 16k, or 32k context length~\\citep{longchat2023, du2022glm}. A comprehensive comparative study of these models can be of great value.\n\nTo address these issues, we propose \\textit{L-Eval} to call for a more standardized evaluation of long context language models. For dataset construction, L-Eval has 20 sub-tasks, 4 sub-tasks are annotated from scratch (\\S\\ref{sec:scratch}), 4 sub-tasks are re-annotated from the public datasets (\\S\\ref{sec:re-anno}), and the remaining 12 sub-tasks are manually cleaned from previous long sequence datasets.  \nWe divide these tasks in L-Eval into two groups: closed-ended tasks and open-ended tasks. The closed-ended group primarily tests the reasoning and understanding ability regarding a longer context, and the open-ended group consists of more summarization tasks that require aggregation of long document information. In the design of L-Eval, we prioritize diversity and quality over quantity, ensuring correctness by manually validating all samples after data collection (\\S\\ref{sec:postprocess}). Our data diversity, indicative in question styles, domain selection, and input lengths, is detailed in Table~\\ref{tab:datasets}.\n\nIn addition, the development of suitable evaluation metrics for LCLMs on open-ended tasks where multiple outputs are acceptable is crucial, yet challenging. In this work, we study the limitations of traditional metrics based on lexical matching. We demonstrate that these metrics often fail to correlate with human evaluation results. Our further experiments suggest that LLM judges~\\citep{alpaca_eval, zheng2023judging} provide superior accuracy in the evaluation of open-ended tasks. \\S\\ref{sec:metric} explains how we set a short-context LLM judge in a long-context evalution setting. \nConsidering the influence of generation length on performance and in order to avoid drawing misleading conclusions, we propose the Length-Instruction-Enhanced (LIE) evaluation technique for all reference-based metrics, including those employing an LLM judger. The empirical results demonstrate a substantial improvement brought by LIE evaluation in the Kendall-Tau correlation coefficient ($\\tau$) with human judgments (Figure~\\ref{fig:cor}), for all automatic metrics.\n\nWe also conducted a comprehensive study with 16 different LLMs (\\S\\ref{sec:baselines}) in L-Eval. Some of our key findings are summarized below:\n(1) There is still a significant gap between open-source LCLMs and commercial models, for both closed-ended tasks (Table~\\ref{table:acc_exam}) and open-ended tasks evaluated by LLMs and human (Table~\\ref{tab:llm_eval},~\\ref{tab:human_eval}). However, this gap is not accurately reflected by n-gram metrics.\n(2) While current efforts on open-source LCLMs improve performance on closed-ended tasks, they significantly fall short on open-ended tasks. This is largely due to the models' misunderstanding of instructions as the input context length increases.\n(3) Experiments on GPT-3.5-Turbo with both dense and sparse retrievers show that end-to-end full-context models outperform traditional retrieval-based systems.\n(4) Training-free scaled positional embeddings can enhance the retrieval capability of LLMs over longer input, while it may adversely affect their reasoning ability.\n    \n\nMore interesting conclusions can be found in \\S\\ref{sec:main_results} and \\S\\ref{sec:analysis_app}. We hope \\textit{L-Eval} and our findings contribute to a deeper understanding of current LCLM research and the further development of models and evaluation metrics.\n\n\\subsection{Long Context Language Models}\\label{sec:lclms}\nFeeding long context leads to bottlenecks in language model training and inference due to computational resources. Some community efforts focus on developing \n {efficient attention} mechanisms to build efficient language models~\\citep{sun2023retentive,ding2023longnet,li2023incontext,fu2023hungry,peng2023rwkv}. \nIn addition to optimizing the attention mechanism, some works~\\citep{bulatov2023scaling,dai-etal-2019-transformer, mohtashami2023landmark} focus on {chunking the input} to model both the current text in the chunk and the previous context states, effectively extending the length of context processing.\nBesides the efficiency challenge, the {scalability of positional embedding} is also crucial. ALiBi~\\citep{press2022train}, and \\textsc{xPos}~\\citep{sun2022lengthextrapolatable} emphasize the significance of local context to enhance the language model's ability to perform extrapolation.\nMoreover, position interpolation (PI)~\\citep{chen2023extending} and NTK-aware~\\citep{fixedNTK, dynamicNTK} are the most popular approaches based on RoPE~\\citep{su2022roformer} to efficiently and effectively extend the context length. However, these works mainly validated their methods with perplexity (PPL)~\\citep{sun2021long, fixedNTK}, and there has not been systematic validation on practical tasks.  \n\n\\subsection{Long Sequences Benchmarks}\n\\cite{tay2020long} introduce the Long Range Arena (LRA), a benchmark encompassing five distinct classification tasks. CAB~\\citep{zhang2023cab} is another benchmark for different efficient attention designs by comparing both efficiency and accuracy. In language domain, \nprevious work on LCLMs tends to report PPL to evaluate language models~\\citep{su2022roformer, peng2023yarn} on longer context. However, PPL may not usually correlate with the actual performance~\\citep{sun2021long}.  ZeroScrolls~\\citep{shaham2022scrolls, shaham2023zeroscrolls} and LongBench~\\citep{bai2023longbench} are concurrent long context evaluation suites. L-Eval differs from them in 3 aspects: (1) Manually selected samples. Testing samples are automatically filtered by their benchmarks, while those for L-Eval are manually filtered. (2) Standardized metrics.  We are the first to investigate the correlations between traditional lexical metrics and recently proposed LLM metrics with human judgment on Long context settings. L-Eval no longer mainly relies on N-gram metrics.  (3) More closed-ended tasks. Due to fairness issues in open-ended tasks. L-Eval has more closed-ended tasks reflecting unbiased results.\n\n\\section{Towards High-Quality and Diverse Long Context DataSets}\n\\label{sec:data}\nIn this section, we highlight some key procedures in L-Eval data construction.\nConcretely, we show the annotation, re-annotation, and manual filtering pipeline and the statistics of L-Eval.  Please refer to Appendix~\\ref{sec:data-appendix} for the complete annotation details and examples. \n\n\\subsection{Data Annotation from Scratch}\\label{sec:scratch}\nThere are 4 datasets annotated from scratch in L-Eval: Coursera, SFcition, CodeU, and LongFQA. The original resources are videos from Coursera, previous open-source datasets, source code from famous Python libraries, and public earning call transcripts, respectively.\n\n\\vspace{-0.7em}\n\\paragraph{Coursera} This dataset originates from the Coursera website.\\footnote{\\url{https://coursera.org/}} To reduce the difficulty of annotation, we choose four public courses related to big data and machine learning (\\S\\ref{sec:coursera}).\nThe input long document is the subtitles of the videos. Questions and the ground truth answers are labeled by the authors. The instruction style of Coursera takes the format of multiple choice. In order to increase the difficulty of the task, we have set \\textbf{multiple correct options}. To the best of our knowledge, this is the first multi-choice dataset with multiple correct answers and it is more challenging than single-option questions (Table~\\ref{table:acc_exam}).\n\n\\vspace{-0.7em}\n\\paragraph{SFcition} \nWe annotate this sub-task to test the loyalty of the LCLM to the input context. We argue that in LCLMs, contextual knowledge (stored in long input) is more crucial than parametric knowledge (gained during pretraining). Practically, many long documents are private and can never be seen during pretraining. LLMs should follow the contextual knowledge instead of parametric knowledge in long context settings. To simulate this scenario, we annotate a science fiction dataset consisting of True or False questions. Most of the answers to these questions contradict real-world principles and do not comply with actual physical laws (\\S\\ref{sec:sfiction}). We find that Turbo-16k struggles on this task, which tends to answer questions relying on parametric knowledge (Table~\\ref{table:acc_exam}).\n\n\\vspace{-0.7em}\n\\paragraph{CodeU} As a code understanding dataset, it requires LLM to infer the output of a lengthy Python program. We mainly use source code from Numpy\\footnote{\\url{https://github.com/numpy/numpy}} and construct a string processing codebase. To prevent LLMs from answering the question based on their parametric knowledge, we replace the original function name. LLMs should first locate where the function is called and determine which functions are invoked. CodeU is the most challenging task in L-Eval (\\S\\ref{sec:codeU}).\n\n\\vspace{-0.7em}\n\\paragraph{LongFQA} We also notice that there is a lack of long context question answering datasets in the finance domain and we annotate the QA pairs based on public earning call transcripts from the \\textit{Investor Relations} section of 6 company websites.  Please refer to \\S\\ref{sec:longfqa} for details.\n\n\\subsection{Data Re-annotation from Public Datasets}\\label{sec:re-anno}\nWe re-annotate 5 publicly available datasets in L-Eval. \n\\textbf{GSM(16-shot)} is derived from 100-grade school math problems in the GSM8k dataset~\\citep{cobbe2021training}. If the LCLM maintain its reasoning ablilty on longer context, ultizing more high-quality examples will a positive effect on solving math problems~\\citep{li2023incontext}. We construct 16 in-context examples with lengthy Chain-of-Thought where 8 examples come from  \\textit{chain-of-thought-hub}\\footnote{\\url{https://github.com/FranxYao/chain-of-thought-hub}} and 8 examples are constructed by us.  We experiment with the newly constructed examples and the accuracy of Turbo-16k-0613 rises from 79 (8-shot) to 84 (16-shot).  \n\nWe inject come new synthesis instructions to test global context modeling into \\textbf{QuALITY}~\\citep{pang2022quality}, such as \\textit{``What can we infer from the longest sentence in this story?''} and \\textit{``How many words are there in the story?''}. Given that these types of questions may rarely occur in real-world conversations, their proportion in L-Eval is extremely small. \nThe \\textbf{Openreview} dataset contains papers collected from \\url{openreview.net}. We ask the model to (1) write an Abstract\nsection, (2) summarize the related work, and (3) finally give feedback including valuable suggestions and \nsome questions for the authors. We select the paper with high-quality related work sections and helpful reviews written by human reviewers to form this test set.\\footnote{Ethic statement:\nwe discourage reviewers from using large models for reviews. Our goal is to assist authors in further improving their papers.} Next, we use \\textbf{SPACE}~\\citep{angelidis-etal-2021-extractive} to test the aspect-based review summarization task, and the instructions for the dataset are annotated by us. We adopt diverse instructions to prevent overfitting.\n\n\\begin{wrapfigure}{r}{0.4\\textwidth}\n  \\centering\n  \\vspace{-4mm}\n  \\includegraphics[width=0.4\\textwidth]{fig/topic_ret.pdf}\n  \\caption{Test Accuracy (\\%) of different models with retrieving the first topic and retrieving the second/third topic.}\n  \\label{fig:topic}\n  \\vspace{-5mm}\n\\end{wrapfigure}\nPrevious work~\\citep{longchat2023,liu2023lost} has used retrieval tasks to test the ability of modeling long context dependency via retrieving something over lengthy context. L-Eval includes a popular first topic retrieval task \\textbf{TopicRet}~\\citep{longchat2023}, formatted as: ``\\textit{[topic-1] Chat History [instruction]} ''. However, as we can see from Figure~\\ref{fig:topic}, retrieving the first topic is too easy to distinguish the ability of different models. However, the task of retrieving the second and the third topics presents a significantly higher level of challenge. It is observed that nearly all open-source models struggle in  task.  So we enhance the task with second/third topic retrieval.\n\n\\subsection{Data Filtering and Correction}\\label{sec:postprocess}\nThe remaining 12 tasks originates from existing datasets following previous evaluation suites~\\citep{zhang2023cab}.  However, L-Eval involves more human labor after data collection because we find the annotation quality of previous long sequence datasets fluctuates severely and there are many unanswerable questions that are unrelated to the context. These mistakes can hardly be corrected using the automatic preprocessing scripts in previous works. In L-Eval, all samples are manually filtered and corrected after data collection. Specifically, we use Claude-100k as our assistant to filter mistaken QAs and unanswerable questions. First, we input the lengthy document into Claude and request it to provide the answer and offer an explanation. If Claude produces an answer greatly mismatching the ground truth or states that we cannot deduce the answer from the context, we will either perform re-annotation or simply remove them. \n\n\\begin{table}[t]\n\\vspace{-1em}\n\\centering  \n\\caption{This table presents the statistics of the L-Eval suite where \\textbf{Question-style} indicates the type of task or the style of instruction in the dataset, \\textbf{\\#Doc} refers to the number of long documents, and \\textbf{\\#Instr} denotes the number of instructions provided for each long input. \\textbf{Avg/Max len} signifies the average/maximum length of the document inputs. We tokenize the raw text with Llama2 tokenizer and report the number of tokens.\n}\n\\vspace{-0.5em}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{lllrrrr}\n\\toprule\n\\bf Dataset &\\bf Question-style &\\bf Domain &\\bf Avg len & \\bf Max len &\\bf \\#Instr  &\\bf \\#Doc \\\\\n\\midrule\n\\rowcolor{mypink!50}\n\\multicolumn{7}{c}{\\textbf{\\textit{Closed - Ended \\, Tasks}}} \\\\\n\\midrule \n\nTOEFL & Multiple choice & English test & 3,907 & 4,171  &  269 & 15\\\\\nGSM(16-shot)$^\\dag$ & Solving math problems & In-context examples & 5,557  & 5,638 & 100 & 100  \\\\\nQuALITY~\\cite{}$^\\dag$ & Multiple choice & Gutenberg & 7,169  & 8,560  & 202 & 15 \\\\\nCoursera$^*$  & Multiple choice  & Advanced courses & 9,075 & 17,185 & 172  & 15 \\\\\nTopicRet$^\\dag$  & Retriving topics & Conversation  &  12,506 & 15,916  & 150 & 50 \\\\\nSFcition$^*$ & True or False Questions & Scientific fictions & 16,381 & 26,918 & 64 & 7 \\\\\nCodeU$^*$ & Deducing program outputs & Python Codebase  &  31,575 & 36,509 & 90 & 90 \\\\\n\n\\midrule\n\\rowcolor{mypink!50}\n\\multicolumn{7}{c}{\\textbf{\\textit{Open - Ended \\, Tasks}}} \\\\\n\\midrule\nMultiDoc2Dial & Goal-oriented dialogues & Grounded documents & 3,905 & 7888  & 136 & 20 \\\\\nQasper & QA on papers & NLP papers & 5,019 & 6,547  & 160 & 20 \\\\\nLongFQA$^*$  & QA on earning call & Finance  & 6,032 & 7824  & 52 & 6 \\\\\nNQ & QA from Google Search & Wikipedia & 23,698 & 47,726 & 104 & 20\\\\\nCUAD & Extracting key information & Law & 30,966 & 68,625  & 130 & 20 \\\\\nNarrativeQA & QA on narratives  & Gutenberg  & 62,335  & 210,541 & 182 & 20 \\\\\n\\midrule\nMulti-News & Multi-doc Summarization & Multiple News articles  & 7,320& 19,278  & 11 &11 \\\\\nGovReport & Single-doc Summarization & Government reports  & 7,495 & 27,128  &13 & 13 \\\\\nBigPatent & Single-doc Summarization & Lengthy patents  & 7,718  & 12,867 & 13 & 13 \\\\\nSummScreen & Transcripts Summarization & TV series transcripts  & 10,688 & 14,544  & 13 & 13 \\\\\nOpenreview$^\\dag$ & Paper writing \\& reviewing & Papers from Openreview  & 11,170 & 33,303 & 60 & 20 \\\\\nQMSum &  Query-based summarization & Meeting transcripts  & 16,692 & 33,310 & 156 & 20 \\\\\nSPACE$^\\dag$ & Aspect-based summarization & Reviews on Hotels  &  19,978 & 22,158 & 120 & 20 \\\\\n\\bottomrule\n\\end{tabular}\n}\n\n\\label{tab:datasets}\n\\end{table}\n\n\\subsection{Statistics}\\label{sec:stat}\nThe statistics of L-Eval are shown in Table~\\ref{tab:datasets}.\nThe L-Eval contains various question styles such as multiple choice questions (TOFEL~\\citep{tseng2016towards}, QuALITY, Coursera), true or false questions (SFiction), math problems (GSM), code understanding (CodeU), goal-oriented dialogues (MultiDoc2Dial~\\citep{Feng_2021}), extractive QA (CUAD~\\citep{hendrycks2021cuad}, NQ~\\citep{kwiatkowski-etal-2019-natural}), abstractive QA (LongFQA, NarrativeQA~\\citep{kočiský2017narrativeqa}, Qasper~\\citep{dasigi2021dataset}), single document summarization (GovReport~\\citep{huang-etal-2021-efficient}, BigPatent~\\citep{sharma-etal-2019-bigpatent}, SummScreen~\\citep{chen2022summscreen}, QMSum~\\citep{zhong2021qmsum}), multi-document summarization (Multi-News~\\citep{fabbri2019multinews}, SPACE~\\citep{angelidis-etal-2021-extractive}), research writing (Openreview) and so on. The long documents in L-Eval across many domains such as law, finance, academic papers, lectures, lengthy conversations, news, famous Python codebase, long-form novels, and meetings. The average input length in L-Eval ranges from 4k to 60k. The maximum sample in L-Eval contains nearly 200k tokens. This diversity represents real-world scenarios where different tasks may require different lengths of context and instructions. The length of reference in L-Eval also varies significantly across tasks.\n\n\\section{Towards Standardized Long Context Evaluation Metrics}\n\\label{sec:metric}\nIn this section, we present various evaluation metrics for text generation, including exam evaluation for close-ended tasks and different levels of open-ended evaluation, most of which are reference-based metrics.\nWe also conduct experiments to study the correlation between automated metrics and human scoring. \n\n\\vspace{-0.7em}\n\\paragraph{Exam evaluation} This is designed for closed-ended tasks, i.e., multiple-choice questions. The evaluation metric used for these tasks follows the exact match format (accuracy \\%), similar to grading exam papers. Each question's score is calculated as 100 divided by the number of questions. \n\n\\vspace{-0.7em}\n\\paragraph{Human evaluation} This is the most accurate evaluation for open-ended tasks. Despite that some works show GPT-4 can be coherent with human judgment, LLMs cannot replace human evaluation. We engage human evaluators to score the outputs on a scale of 1 to 5, which signifies from poor output to excellent output.\nTo save human laboratories, we propose a subset used for the human evaluation which has 12 long documents with 85 open-ended questions (\\textbf{85-question subset}). \n\n\\vspace{-0.7em}\n\\paragraph{Large language model judges for evaluating LCLMs}\nIn short context settings,  evaluation using LLMs is the most accurate metric for automatically evaluating models on open-ended tasks~\\citep{zheng2023judging,alpaca_eval, dubois2023alpacafarm}. These works assume the LLM evaluator is a ``super model'', but this assumption does not hold in long context settings because it's impossible to feed the entire lengthy inputs into LLMs like GPT-4. Unlike short context evaluation, GPT-4 is unable to infer the ground truth answer itself. Consequently, evaluation results mainly depend on the reference answer and user questions. In L-Eval, we take the pair-wise battle format and we select Turbo-16k-0613 as the base model and report the \\textit{win-rate vs. Turbo-16k-0613 \\%} which means how many samples can beat Turbo-16k. We study two LLM judges: GPT-4 and GPT-3.5 in the experiment section.\nLLM evaluators have been reported to favor more detailed and lengthy answers~\\citep{zheng2023judging}. This bias becomes more pronounced in long context settings as the invisible input makes it difficult for the judge to accurately determine the correctness of specific details and information. Therefore, the judgment model must bear in mind that details not corroborated by the reference answers should not be considered beneficial. We enhance the judgment prompt with: \\textit{Additional details or information that are not mentioned in the reference answer cannot be considered as advantages and do not let them sway your judgment.} If you only want to evaluate a portion of the tasks in L-Eval, we recommend using LLM judges.\nVerifying the 1000+ open-ended questions via GPT-4 is unaffordable.\\footnote{Testing the 4 datasets in Table~\\ref{tab:length_bias} needs about \\$100!} Thus we manually split a subset for GPT-4 evaluation consisting of 17 diverse long documents with 96 open-ended questions (\\textbf{96-question subset}).\\footnote{Evaluating outputs from the 96-question subset with GPT-4 only needs about \\$5.}\n\n\\vspace{-0.7em}\n\\paragraph{N-gram matching evaluation}\nConsidering that assessing all tasks is still expensive for human/LLM evaluators, L-Eval also takes into account n-gram metrics. \nN-gram metrics like ROUGE-L (R-L) and F-1 score are widely used in traditional datasets and they are also widely adopted in the text generation benchmarks via performing lexical matching. It is worth noting that n-gram matching metrics are very sensitive to the length of the ground truth, exhibiting a length bias. The related analysis is in the following \\S\\ref{sec:length-instruction}.\n\n\\subsection{Length Instruction Enhanced Long Context Evaluation}\n\\label{sec:length-instruction}\n\n\\begin{wrapfigure}{r}{0.4\\textwidth}\n  \\centering\n  \\vspace{-6mm}\n  \\includegraphics[width=0.4\\textwidth]{fig/kt_cor.png}\n  \\caption{Kendall-Tau correlation coefficient of different automatic metrics with the average human score.}\n  \\label{fig:cor}\n  \\vspace{-5mm}\n\\end{wrapfigure}\nIn preliminary experiments, we find that LLMs tend to generate very long responses bringing obstacles for the reference-based evaluation (see $\\Delta$\\textbf{L} Table~\\ref{tab:length_bias}). This length bias results in a significant influence on the n-gram metrics. For instance, Claude-100k only achieves a 9.84 F-1 score due to undesired output length.\n\nIn L-Eval, we argue that long context language models should further focus on more accurate content rather than accurate length. Practically, issues about undesired generation length can be easily solved by prompting the model.  We first adopt \\textbf{Length-Instruction-Enhanced} (LIE) evaluation in LLMs evaluation benchmarks which is simple but effective in overcoming the length bias, i.e., the number of words of ground truth is directly exposure to LCLMs. \nLIE evaluation in this work is implemented by injecting the model with the desired length into the original instruction (e.g., [Origin Instruction]: \\textit{Please summarize the opinions of the professor}. [Length Instruction]: \\textit{We need a 50-word summary}, where 50 is the number of words in the reference answer). The results of Claude-100k in Table~\\ref{tab:length_bias} demonstrate a substantial improvement in terms of the F-1 score: there is a near \\textbf{50}-point gap depending on whether or not the model generates with the expected length.\n\n\\begin{figure}[t]\n\\vspace{-1em}\n    \\centering\n    \\includegraphics[width=0.85\\textwidth]{fig/metrics_cmp_v2.png}\n    \\vspace{-0.5em}\n    \\caption{The ranking of six models under various evaluation metrics (Human-avg, Human-1, GPT-4, GPT-3.5, R-L, and F-1) with or without length instruction. {Human-avg} represents the average score from human evaluation, and {Human-1} signifies the score given by the first human annotator.}\n    \\label{fig:radar}\n\\end{figure}\n\n\\vspace{-0.7em}\n\\paragraph{Experimental validation} To validate the LIE evaluation, we then conduct a human evaluation on the 85-questions subset. We have 3 annotators to verify 7 models and calculate the Kendall-Tau correlation coefficient ($\\tau$) between these metrics and the average human score. The main results are shown in Figure~\\ref{fig:cor} (Blue bar) and experimental settings are in \\S\\ref{sec:appendix:human-eval}. Results indicate that all these automatic metrics (except GPT-4) \\textbf{fail to correlate} to human judgment. Compared with N-gram metrics, LLM judges are more accurate and robust to output length.\nAs we can see from Figure~\\ref{fig:cor}, the improvements brought by length instruction are marked with yellow, and after adding the length instructions, $\\tau$ has been improved from 0.5 to 0.8 for ROUGE-L and $\\tau$ of GPT-4 evaluator has even reached to 1. In Figure~\\ref{fig:radar}, we convert the score to rankings (the best one is 5 and the worst is 1) and show the score of 6 models evaluated with 6 different evaluation systems. Figure~\\ref{fig:radar} (a) shows the results given by metrics without length instruction. These hexagons are often distorted because these metrics usually cannot achieve good correlation. When comparing the models enhanced with length instruction in (b), it is observed that the hexagons become more regular.\n\n\\begin{table}[t]\n\\vspace{-1em}\n\\centering\n\\caption{Results on 2 open-ended summarization and 2 abstractive QA tasks.  \\textbf{GPT-4} means the win-rate with Turbo-16k using GPT-4 as the judge.  \\textbf{$\\Delta$L} means the difference of generated answer length with ground truth length. The best results are underlined. Results in red mean decoding in a desired length makes a big difference in performance.}\n\\vspace{-0.5em}\n\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|ccc|ccc|ccc|ccc}\n\\toprule\n\\multirow{2}{*}{\\textbf{Model}} & \\multicolumn{3}{c|}{\\textbf{SPACE}} & \\multicolumn{3}{c|}{\\textbf{QMSum}} & \\multicolumn{3}{c|}{\\textbf{NQ}} & \\multicolumn{3}{c}{\\textbf{NrtvQA}} \\\\\n\\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13}\n&\\textbf{R-L} &  \\textbf{GPT-4} &  \\textbf{$\\Delta$L} & \\textbf{R-L} & \\textbf{GPT-4} & \\textbf{$\\Delta$L}  & \\textbf{F-1}  & \\textbf{GPT-4} & \\textbf{$\\Delta$L}  & \\textbf{F-1} & \\textbf{GPT-4}  & \\textbf{$\\Delta$L}  \\\\\n\\midrule\n\n\\rowcolor{mypink!60}\nClaude-100k & 15.43 & \\color{red}{45.65} & \\bf165 & 14.04 & 58.77 & \\bf183 & \\color{red}{9.84} & \\underline{56.19} & \\bf135 & \\color{red}{10.39} & \\underline{68.96} & \\bf127 \\\\\n \\quad + Length Instruction &18.61 & \\color{red}{\\underline{61.40}} & 27 & 18.13 & \\underline{58.89} & 22 & \\color{red}{\\underline{57.76}} & {51.00} & 1 & \\color{red}{\\underline{19.09}} & 57.77 & 0 \\\\\n\n\\rowcolor{mypink!60}\nChatglm2-32k & 17.56 & 24.13 & -23 & 20.06 & 38.84 & \\bf287 & 31.45 & 33.71 & 3 & 12.24 & 34.67 & 74 \\\\\n \\quad + Length Instruction & 16.61 & 17.11 & 11 & \\underline{20.83} & 33.75 & 9 & 37.94 & 33.71 & -1 & 14.00 & 34.52 & -2 \\\\\n\n\\rowcolor{mypink!60}\nLongchat-7b-16k  & 15.10 & \\color{red}{15.61} & \\bf120 & 9.31 & 25.56 & 40 & {8.83} & 32.33 & \\bf105 & 8.36 & 31.80 & 83 \\\\\n \\quad + Length Instruction& 17.06 & \\color{red}{36.23} & -3 & 13.21 & 30.20 & 70 & {20.21} & 35.00 & 37 &15.17&43.38&40\\\\\n\n\\rowcolor{mypink!60}\nLlama2-13b-chat & 16.83&32.46&\\bf102&14.72&30.79& \\bf116 &\\color{red}{8.29} &38.99&90&7.20 & \\color{red}{30.69} & \\bf130 \\\\\n \\quad + Length Instruction & \\underline{19.23} & 43.15 &-7 &19.65 &34.82 &-1&\\color{red}{35.43} &41.07 & 6 & 13.48 & \\color{red}{45.07} & 14 \\\\\n\n\\bottomrule\n\\end{tabular}\n}\n\\label{tab:length_bias}\n\\end{table}\n\n\\section{Benchmarking LLMs with L-Eval}\nIn this section, we list our 16 baseline models and the results on both open-ended and closed-ended tasks. Generally, there are considerable gaps between open-source models and commercial models. A detailed description of baseline models can be found in \\S\\ref{sec:baselines_appendix}.  The prompt templates for each task are available in \\S\\ref{sec:data-appendix}. We run all the experiments using FlashAttention~\\citep{NEURIPS2022_67d57c32} on a single NVIDIA A800 GPU. The document input is truncated from the right.\n\n\\vspace{-0.5em}\n\\subsection{Baselines}\\label{sec:baselines}\n\\vspace{-0.3em}\n\\paragraph{Commercial Models}\n(1) {Claude-100k} developed by Anthropic, (2) GPT-4-32k, OpenAI's most powerful long context model, (3)\n{Turbo-4k-0613} and (4) Turbo-16k-0613 is the snapshot of {GPT-3.5} from June 13th 2023 which can handle up to 4k/16k input tokens.\n\\vspace{-0.7em}\n\\paragraph{Open-source Models}\n(5) {Llama1}~\\citep{touvron2023llama}, a widely used open-source model developed by Meta AI with a 2k pre-training length, (6) {Vicuna1.3}~\\citep{vicuna2023}, tuned on shareGPT based on Llama1, (7) Longchat-16k, the long context version of Vicuna1.3 using PI, (8) Llama2, the next version of Llama with 4k pre-training context, (9) Llama2-chat, a finetuned version for dialogue usage, (10) Llama2-NTK, extending the context length of Llama2-chat with NTK-aware RoPE, (11) Vicuna1.5-16k~\\citep{zheng2023judging}, the long context version of Llama2 using PI \\& ShareGPT (12) Longchat1.5-32k, the 32k context version of Llama2 using PI \\& ShareGPT.\n(13) {Chatglm2-8k}, the second version of the Chatglm~\\citep{du2022glm}, (14) Chatglm2-32k, the 32k context length version,  (15)\\ {XGen-8k-inst}~\\citep{XGen}, an 8k context models developed by salesforce  (16) {MPT-7B-StoryWriter-65k}, based on MPT-7B and ALiBi with a context length of 65k tokens on a subset of Books3 dataset.\n\\vspace{-0.7em}\n\\paragraph{Retriever}  We implement the dense retriever with the OpenAI AdaEmbedding as the dense retriever and BM25 as the sparse retriever to extract 4 pieces of most related 1k-chunked documents, which are further provided as the context to answer questions.\n\n\\begin{table}[t!]\n\\vspace{-1.5em}\n\\centering\n\\setlength{\\tabcolsep}{1.0mm}\n\\caption{Exam evaluation results on  \\textbf{closed-ended tasks} for current LCLMs. \\textbf{Ret.} indicates whether we use retrieve-based algorithms for the base model. \\textbf{Tokens} denotes the maximum number of input tokens we feed into the model. {\\color{red}{\\tiny{$\\downarrow$ / $\\uparrow$}}} indicates a remarkable decrease/increase in performance, compared to using the original short context counterpart. {\\color{blue} *} indicates the model is not further trained.}\n\\vspace{-0.5em}\n\\renewcommand\\arraystretch{1.05}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{@{}lcccccccccc@{}}\n\\toprule\n\\textbf{Model} & \\textbf{Ret.} & \\textbf{Tokens} & \\textbf{Coursera} & \\textbf{GSM} & \\textbf{QuALITY} & \\textbf{TOEFL}  & \\textbf{CodeU} & \\textbf{SFiction} &  \\textbf{Avg.} \\\\\n\\midrule\nClaude1.3-100k & \\xmark & \\cellcolor{gray!70}100k & 60.03 & {88.00} & {73.76} & {83.64} & 17.77 & 72.65 & 65.97  \\\\\nGPT-4-32k & \\xmark & \\cellcolor{gray!60}32k & \\textbf{75.58} & \\textbf{96.00} & \\textbf{82.17} & \\textbf{84.38} & \\textbf{25.55} & \\bf74.99 & \\bf73.11 \\\\\nTurbo-16k-0613 & \\xmark & \\cellcolor{gray!40}16k & 63.51 & {84.00} & 61.38 & 78.43 & 12.22 & 64.84 & 60.73 \\\\\nAdaEmb-Turbo-4k-0613 & \\cmark & \\cellcolor{gray!15}4k & 61.77 & {23.00} & 58.91 & 76.95 & {6.66} &71.09 & 49.73 \\\\\nBM25-Turbo-4k-0613 & \\cmark & \\cellcolor{gray!15}4k & 63.80 & {23.00} & 59.40 & 75.09 & {5.55} & 71.09  & 49.65 \\\\\n\n\\midrule\n\\rowcolor{mypink!50}\n\\multicolumn{10}{c}{\\textit{Truncating input tokens to the pretraining context length}} \\\\\n\\midrule\n\nLlama1-7b-2k (w/o SFT) & \\xmark & \\cellcolor{gray!5}2k & 13.37 & 7.00 & 21.78 & 30.85 & 1.11 & 35.15  & 19.22 \\\\\nVicuna1.3-7b-2k & \\xmark & \\cellcolor{gray!5}2k & 34.73 & 19.00 & 32.67 & 43.49 & 1.11 & 60.93 & 30.01 \\\\\n\nLlama2-7b-4k (w/o SFT) & \\xmark & \\cellcolor{gray!15}4k  & 20.05 & 2.00 & 28.71 &  24.53 & 0.00 & 40.62  & 19.31 \\\\\nLlama2-7b-chat & \\xmark & \\cellcolor{gray!15}4k  & 29.21 & {19.00} & 37.62 & 51.67 & 1.11 & 60.15 & 33.12 \\\\\nLlama2-13b-chat & \\xmark & \\cellcolor{gray!15}4k  & 35.75 & \\textbf{39.00} & \\textbf{42.57} & \\textbf{60.96} & 1.11 & 54.68 & \\bf39.01 \\\\\n\nChatglm2-6b-8k & \\xmark & \\cellcolor{gray!5}2k & \\bf43.75 & {13.00} & {40.59} & {53.90} & 2.22 & 54.68 & 34.69  \\\\\nXGen-7b-8k (2k-4k-8k) & \\xmark & \\cellcolor{gray!5}2k & 26.59 & 3.00 &  35.15  & 44.23 & 1.11 & 48.43 & 26.41 \\\\\n\n\\midrule\n\\rowcolor{mypink!50}\n\\multicolumn{10}{c}{\\textit{Truncating input tokens to the further finetuning context length}} \\\\\n\\midrule\nChatglm2-6b-32k & \\xmark & \\cellcolor{gray!60}32k  & \\textbf{47.81} & 27.00\\color{red}{\\tiny{$\\uparrow$}} & 45.04 & 55.01 & 2.22 & 57.02 & 39.01\\color{red}{\\tiny{$\\uparrow$}} \\\\\nLongchat1.5-7b-32k & \\xmark & \\cellcolor{gray!60}32k  & 32.99 & 18.00 & 37.62 & 39.77 & 3.33 & 57.02 & 31.45\\\\\n\nLongchat-7b-16k & \\xmark & \\cellcolor{gray!40}16k  & 29.74 & 10.00\\color{red}{\\tiny{$\\downarrow$}}  & 33.66 & 47.95 & 3.33 & \\bf64.84 & 31.58\\color{red}\\\\ \nVicuna1.5-7b-16k & \\xmark & \\cellcolor{gray!40}16k  & 38.66 & 19.00 & 39.60 & 55.39 & \\bf5.55 & 60.15 & 36.39\\color{red}{\\tiny{$\\uparrow$}} \\\\\nLlama2-7b-NTK\\color{blue}{*} & \\xmark & \\cellcolor{gray!40}16k  & 32.71 & 19.00 & 33.16 & 52.78 & 0.00 & \\bf64.84 & 33.74 \\\\\n\nLongchat-13b-16k & \\xmark & \\cellcolor{gray!40}16k & 31.39 & 15.00 & 40.59 & 55.39 &  2.22 & \\bf64.84  & 34.90 \\\\\nVicuna1.5-13b-16k & \\xmark & \\cellcolor{gray!40}16k & 40.69 & 36.00 & \\textbf{53.96}\\color{red}{\\tiny{$\\uparrow$}} & \\textbf{68.40}\\color{red}{\\tiny{$\\uparrow$}} & 0.00 & 61.71 & \\bf43.46\\color{red}{\\tiny{$\\uparrow$}}\\\\\nLlama2-13b-NTK\\color{blue}{*} & \\xmark & \\cellcolor{gray!40}16k  & 36.48 & 11.00\\color{red}{\\tiny{$\\downarrow$}} & 35.64  & 54.64&  1.11  & 63.28 & 33.69 \\\\\nLlama2-13b-NTK(Dyn)\\color{blue}{*} & \\xmark & \\cellcolor{gray!40}16k  & 30.08 & \\textbf{43.00} & 41.58 & 64.31 & 1.11 & 35.15 & 35.87\\\\\n\nChatglm2-6b-8k & \\xmark & \\cellcolor{gray!25}8k & 42.15 & {18.00} & {44.05} & {54.64} & 2.22 &54.68 & 35.95\\\\\nXGen-7b-8k & \\xmark & \\cellcolor{gray!25}8k & 29.06 & 16.00 &  33.66  & 42.37 & 3.33 & 41.40 & 27.63\\\\\nMPT-7b-65k & \\xmark & \\cellcolor{gray!25}8k & 25.23 & 8.00 & 25.24 & 17.84 & 0.00 & 39.06 & 19.22\\\\\n\n\\bottomrule\n\\end{tabular}\n}\n\n\\label{table:acc_exam}\n\\end{table}\n\n\\begin{table*}[t]\n\\vspace{-1.5em}\n\\caption{In comparing various models to Turbo-16k-0613 on \\textbf{open-ended tasks}. We evaluate these models on the 96-question subset using GPT-4 and two subsets (85+96 questions) using GPT-3.5. We reduce the positional biases by swapping paired predictions, so the GPT-4 evaluator is used in 96$\\times$2 evaluation rounds, while the GPT3.5 evaluator is used in 181$\\times$2 rounds}\n\\vspace{-1.0em}\n\\center\n\\footnotesize\n\\renewcommand\\arraystretch{0.97}\n\\resizebox{0.9\\textwidth}{!}{\n\\tabcolsep 0.035 in\n\\begin{tabular}{lccccccccc}\n\\toprule\n\\multicolumn{1}{c}{\\multirow{2}[1]{*}{\\textbf{Model}}} &\n\\multicolumn{1}{c}{\\multirow{2}[1]{*}{\\textbf{Ret.}}} &\n\\multicolumn{1}{c}{\\multirow{2}[1]{*}{\\textbf{Tokens}}} &\n\\multicolumn{3}{c}{\\textbf{GPT-4 }}&\n\\multicolumn{3}{c}{\\color{darkgray}{\\textbf{GPT-3.5}}} &\n\\multicolumn{1}{c}{\\multirow{2}[1]{*}{\\color{gray}{\\textbf{R-L}}}}\n\\\\\n & & & \\textbf{wins} & \\textbf{ties} & \\textbf{win-rate \\%}  &\n\\textbf{wins} & \\textbf{ties} & \\textbf{win-rate \\%}  \\\\\n\\cmidrule(lr){1-1} \\cmidrule(lr){2-2} \\cmidrule(lr){3-3} \\cmidrule(lr){4-6} \\cmidrule(lr){7-9} \\cmidrule(lr){10-10} \n\nClaude1.3-100k & \\xmark & \\cellcolor{gray!70}100k & \\textbf{96} & 42 & \\textbf{60.94}  & 189 & 34 & {\\textbf{58.68}} & 28.22 \\\\\nGPT-4-32k & \\xmark & \\cellcolor{gray!60}32k & 76 &  56 & 54.16  &  171 & 50 & {56.32} & \\underline{36.18} \\\\\nTurbo-16k-0613 & \\xmark & \\cellcolor{gray!15}4k & 0 & 192 & 50.00  & 0 & 362 & 50.00 & 28.61\\\\\n\nTurbo-4k-0613 & \\xmark & \\cellcolor{gray!15}4k & 38 & 69 & 39.83\\color{red}{\\tiny{$\\downarrow$}}  & 109 &  61 & {41.39} & 26.90\\\\\nAdaEmb-Turbo-4k-0613 & \\cmark & \\cellcolor{gray!15}4k & 61 & 56 &  46.84 & 123 & 77  & 45.36 & 26.09 \\\\\nBM25-Turbo-4k-0613 & \\cmark & \\cellcolor{gray!15}4k & 50 & 69 & 44.01 & 125 & 78 & {45.30} & 26.83\\\\\n\n\\midrule\n\\rowcolor{mypink!50}\n\\multicolumn{10}{c}{\\textit{Truncating input tokens to the pretraining context length}} \\\\\n\\midrule\n\nVicuna1.3-7b-2k  & \\xmark & \\cellcolor{gray!5}2k & 29 & 55 & 29.42 & 97 & 42 & 34.91 & 16.17 \\\\\nLongchat-7b-16k & \\xmark & \\cellcolor{gray!5}2k & 26 & 63 & 29.94  &  87 &  38 & {31.26} & 19.77\\\\\n\nLlama2-7b-chat  & \\xmark & \\cellcolor{gray!15}4k & 48 & 58 & 40.10 & 127 & 44 & {42.45}  & \\underline{24.25}\\\\\nLlama2-13b-chat & \\xmark & \\cellcolor{gray!15}4k & \\textbf{51} & 61 & \\textbf{42.44}  & \\textbf{143} & 49 & \\textbf{47.85} & 24.07  \\\\\n\n\\midrule\n\\rowcolor{mypink!50}\n\\multicolumn{10}{c}{\\textit{Truncating input tokens to the further finetuning context length}} \\\\\n\\midrule\n\nChatglm2-6b-32k & \\xmark & \\cellcolor{gray!60}32k & 28 & 60 & 30.20 &  53 & 65 & {24.63} & \\underline{22.04} \\\\\nLongchat1.5-7b-32k & \\xmark & \\cellcolor{gray!60}32k & \\bf38 & 53 & 33.59 &  136 & 37 & 44.91 & 21.21\\\\\n\nLongchat-7b-16k & \\xmark & \\cellcolor{gray!40}16k & 36 & 56 & 33.68\\color{red}{\\tiny{$\\uparrow$}} & 108 & 42 & {37.94} & 20.59\\\\\nVicuna1.5-7b-16k & \\xmark & \\cellcolor{gray!40}16k & 22 & 54 & 25.52\\color{red}{\\tiny{$\\downarrow$}} & 102 & 52 & {37.86} & 18.05 \\\\\nLlama2-7b-NTK\\color{blue}{*} & \\xmark & \\cellcolor{gray!40}16k & 18 & 49 & 22.13 & 58 & 35 & {23.59} & 11.50 \\\\\n\nLongchat-13b-16k & \\xmark & \\cellcolor{gray!40}16k & 36 & 59 & \\textbf{34.11} & \\bf128 & 24 & 40.11 & 18.98\\\\\nVicuna1.5-13b-16k & \\xmark & \\cellcolor{gray!40}16k & 36 & 59 & \\textbf{34.11}\\color{red}{\\tiny{$\\downarrow$}} &  116 &  43 & \\bf{40.92} & 19.69 \\\\\nLlama2-13b-NTK\\color{blue}{*} & \\xmark & \\cellcolor{gray!40}16k & 31 & 52 & 29.68 & 91 & 44 & 34.55 &  15.63\\\\\nLlama2-13b-NTK(Dyn)\\color{blue}{*} & \\xmark & \\cellcolor{gray!40}16k &  23 & 48 & 24.47 & 55 & 64 & 26.60 & 11.62\\\\\n\nChatglm2-6b-8k & \\xmark & \\cellcolor{gray!25}8k & 18 & 64 & 26.04  &  86 & 54 & {32.84} & 18.19  \\\\\nXGen-7b-8k & \\xmark & \\cellcolor{gray!25}8k & 24 & 62 & 28.64  & 89 & 72 & {36.02} & 20.51 \\\\\n\\bottomrule\n\\end{tabular}\n}\n\n\\label{tab:llm_eval}\n\\end{table*}\n\n\\subsection{Main Results}\\label{sec:main_results}\nThe performance of LCLMs on closed-ended tasks is shown Table~\\ref{table:acc_exam}. As for open-ended tasks, we test the 96-question subset (Table~\\ref{tab:llm_eval}) with GPT-4 evaluation. Results from n-gram metrics on all test sets and the rankings of LLMs can be found in \\S\\ref{sec:analysis_app}. From the main results, we have the following observations. \nGPT-4-32k clearly outperforms all other models by a very significant margin, establishing SOTA in L-Eval closed-ended tasks. \nThere is still a near \\textbf{20}-points gap between the best open-source 16k models and Turbo-16k.  As for open-ended tasks, since the input texts are generally longer and a global understanding of the context is required, Claude-100k, with the longest context length, surpasses all baseline models including GPT-4-32k. Although results of n-gram metrics indicate that open-source LCLMs have achieved performance close to GPT-Turbo on open-ended tasks, the evaluation outcomes from both LLM (Table~\\ref{tab:llm_eval}) and human judges (Table~\\ref{tab:human_eval}) reveal that there is still a significant gap between them. Moreover, retrieval-based methods based on Turbo-4k fall short in comparison to encoding the entire context (Turbo-16k), as certain tasks are difficult to address through simple retrieval.\n\n\\vspace{-0.7em}\n\\paragraph{Fine-tuning longer offers benefits for closed-ended tasks but falls short in open-ended tasks} \n\\begin{wrapfigure}{r}{0.4\\textwidth}\n  \\centering\n  \\vspace{-5mm}\n  \\includegraphics[width=0.4\\textwidth]{fig/split_data.png}\n  \\caption{Number of invalid outputs from Llama2 and Turbo.}\n  \\label{fig:percent}\n  \\vspace{-3mm}\n\\end{wrapfigure}\nIn Table~\\ref{table:acc_exam}, for open-source models using scaled positional embedding, Longchat and Vicuan1.5-16k obviously outperform their original version Vicuna-2k and Llama2-chat. The results suggest that further tuning on longer input from a model with short pretraining context length does benefit long context modeling. However, according to Table~\\ref{tab:llm_eval}, unlike results on closed-ended tasks, the best model Vicuna1.5-13b-16k only wins Turbo-16k by 34\\%, \\textbf{8} points lower than its short version Llama2-13b. Llama2-13b-chat~\\citep{touvron2023llama} is still the strongest open-source baseline, indicating that current LCLMs simply based on scaled position embedding may not be enough for these challenging open generation tasks. Based on our human evaluation, we find that although scaled position embedding techniques such as NTK~\\citep{fixedNTK} or PI~\\citep{sun2022lengthextrapolatable} effectively extend models' context length, the models tend to get lost when facing lengthy input tokens and are unable to follow the instruction. We classify these outputs as ``invalid outputs''. \nTo investigate model performance on different context lengths, we split the 85-questions subset into 2 parts: PART-A contains samples with less than 4k tokens, and PART-B more than 4k tokens. We compare the number of invalid outputs from Llama2/Vicuna1.5-16k and Turbo/Turbo-16k in Figure~\\ref{fig:percent}. Results show that the number of invalid outputs from Turbo-16k remains a very small amount on both PART-A and B while the invalid outputs from Llama2-16k dramatically increase on samples with longer input. Thus, LCLMs are less capable of following instructions on open-ended tasks for long contexts, compared with closed-ended tasks, such as multiple choice. A possible reason is that the pertaining or SFT corpus is highly likely to contain many training samples with similar question styles. This strongly enhances their instruction-following ability on closed-ended tasks. \n\n\\vspace{-1em}\n\\paragraph{Performance on retrieval tasks contradicts reasoning tasks}\n\\begin{wrapfigure}{r}{0.35\\textwidth}\n  \\centering\n  \\vspace{-1mm}\n  \\includegraphics[width=0.35\\textwidth]{fig/ntk.pdf}\n  \\caption{Test retrieval ability and reasoning ability with NTK base.}\n  \\label{fig:ret_reasoning}\n  \\vspace{-2mm}\n\\end{wrapfigure}\nThe most popular NTK-ware positional embedding methods increase the base 10,000 \nin the vanilla RoPE to implement extrapolation without further fine-tuning. However, we find that the performance on topic retrieval tasks does not match the reasoning capability over lengthy context. As can be seen from Figure~\\ref{fig:ret_reasoning}, when we increase the base from 20,000 to 160,000, there is a continuous improvement on topic retrieval. However, performance on math reasoning tasks with lengthy examples exhibits a completely opposite trend, indicating that it is challenging for the model to maintain its reasoning abilities when increasing the base. In contrast, the performance on retrieval tasks seems to remain unaffected after the base reaches 60,000.\n\nWe have further analysis in \\S\\ref{sec:analysis_app}, including full results of n-grams metrics on open-ended tasks, the rankings of current LLMs, NTK-aware positional embedding and retrieval-based systems.\n\n\\section{Conclusion}\nIn conclusion, the much-needed rigorous benchmark L-Eval introduced in this work provides a comprehensive suite of tasks and evaluation metrics to assess the capabilities of long context language models.\nWe tested most of open-source LCLMs and experiments demonstrate promising gains from extending context length and gaps compared to commercial models. Our analysis using L-Eval offers valuable insights into the current state and limitations of LCLMs. We believe that with its focus on practical, long-form documents across domains, L-Eval can serve as a challenging testbed to drive advances in modeling longer contexts.\n\n\\newpage\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{\\OURS: Extending Long Context Evaluation \\\\ Beyond 100K Tokens}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\n\nProcessing and reasoning over long contexts is crucial for many practical applications of Large Language Models (LLMs), such as document comprehension and agent construction. Despite recent strides in making LLMs process contexts with more than 100K tokens, there is currently a lack of a standardized benchmark to evaluate this long-context capability. Existing public benchmarks typically focus on contexts around 10K tokens, limiting the assessment and comparison of LLMs in processing longer contexts. In this paper, we propose \\OURS, the first LLM benchmark featuring an average data length surpassing 100K tokens. \\OURSSPACE comprises synthetic and realistic tasks spanning diverse domains, presented in both English and Chinese. The tasks in \\OURSSPACE are designed to require well understanding of long dependencies in contexts, and make simply retrieving a limited number of passages from contexts not sufficient for these tasks. In our experiments, based on \\OURS, we evaluate the state-of-the-art proprietary and open-source LLMs tailored for processing long contexts. The results indicate that existing long context LLMs still require significant advancements to effectively process 100K+ context. We further present three intriguing analyses regarding the behavior of LLMs processing long context. Our code and data is released\\footnote{\\url{https://github.com/OpenBMB/InfiniteBench}}\\footnote{\\url{https://huggingface.co/datasets/xinrongzhang2022/InfiniteBench}}.\n\n\\end{abstract}\n\n\\section{Introduction}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{images/radar_res.pdf}\n    \\caption{The performance of GPT-4, Kimi-Chat, YaRN-Mistral, and Claude 2 on \\OURS. A higher value represents better performance.}\n    \\vspace{-0.3cm}\n    \\label{fig:benchmark-result}\n\\end{figure}\n\\begin{table*}[t]\n    \\footnotesize\n    \\centering\n    \\begin{tabular}{l|cccccccc}\n        \\toprule\n        Benchmark  \n            & Avg Len & En & Zh & Code & Math & Novel & Dialogue & Synthetic \\\\\n        \\midrule\n        LRA~\\citep{tay2020long}      \n            & $\\sim$10K\n            & \\cmark & \\xmark & \\xmark & \\cmark & \\xmark & \\xmark & \\cmark\\\\\n        LongBench~\\citep{bai2023longbench}       \n            & $\\sim$10K\n            & \\cmark & \\cmark & \\cmark & \\xmark & \\cmark & \\cmark & \\cmark\\\\\n        L-Eval~\\citep{An2023LEvalIS}     \n            &  4K - 60K\n            & \\cmark & \\xmark & \\cmark & \\cmark & \\xmark & \\xmark & \\cmark\\\\\n        LooGLE~\\citep{Li2023LooGLECL}\n            & $\\sim$20K \n            & \\cmark & \\xmark & \\xmark & \\xmark & \\xmark & \\cmark & \\xmark\\\\\n        \\OURSSPACE (ours) \n            & $\\sim$200K\n            & \\cmark & \\cmark & \\cmark & \\cmark & \\cmark & \\cmark & \\cmark \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\caption{Comparison to existing long-context benchmarks and \\OURS. ``En'' and ``Zh'' refer to English and Chinese tasks. ``Code'', ``Math'', ``Novel'', ``Dialogue'' indicate whether the domain includes tasks from those domains, and ``Synthetic'' indicates whether there are auto-generated tasks.}\n    \\label{tab:table-benchmark-comparison}\n\\end{table*}\nIn recent years, large language models (LLMs) \\cite{gpt3, gpt4, touvron2023llama} have exhibited exceptional performance across a range of natural language processing (NLP) tasks~\\cite{qiu2020pre, han2021pre}. LLMs are showing a promising direction toward generalist task assistance, being capable of aiding users in practical tasks through conversational interactions. These tasks include web navigation~\\cite{nakano2021webgpt}, analysis of code repositories~\\cite{chen2021evaluating}, and extraction of useful information from documents~\\cite{kovcisky2018narrativeqa}, indicating a step towards artificial general intelligence. For these LLM-based scenarios, the ability to process long contexts is increasingly critical, in addition to understanding fine-grained semantics and possessing extensive knowledge~\\cite{dong2023survey, huang2023advancing}. Textual documents, historical dialogues, complex instructions, and cumbersome workflows, which constitute the data most directly processed in daily tasks, must be input to LLMs as long contexts for effective processing.\n\nDespite this growing importance, LLMs consistently face challenges in processing long contexts, primarily due to the substantial computational resources required for long sequence training~\\cite{dao2022flashattention, dao2023flashattention2} as well as the apparent inability to generalize to sequences longer than those encountered during training~\\cite{chen2023extending,yarn}. \nLLMs are typically trained on sequences containing no more than 8K tokens~\\cite{touvron2023llama, penedo2023refinedweb, biderman2023pythia}, and thus cannot well handle contexts exceeding 8K tokens.\nThese limitations have largely restricted most LLMs from being applied to more complex tasks.\n\nRecent advancements in training infrastructure~\\cite{Shoeybi2019MegatronLMTM,narayanan2021efficient, dao2022flashattention, dao2023flashattention2}, and efforts to improve length generalization~\\cite{anil2022exploring,Chen2023ExtendingCW,yarn}\\footnote{\\url{https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/}} have led to rapid developments in long-context LLMs. \nBased on these improved training infrastructures and length generalization methods, several LLMs have purportedly managed to process data exceeding 100K tokens~\\citep{yarn,gpt4turbo,Yi-6B-200K,Yi-34B-200K}, with Claude 2~\\cite{claude2} and Kimi-Chat~\\cite{kimi} even claiming to be able to process up to 200K tokens.\nHowever, the rapid emergence of long-context LLMs has outpaced the development of adequate evaluation benchmarks. Present long-context benchmarks predominantly feature contexts averaging around 10K tokens~\\citep{bai2023longbench, tay2020long}, invariably falling below 100K tokens. \\textbf{This lag in the advancement of long-context evaluation methodologies impedes both the comparative analysis of diverse long-context LLMs and the pinpointing of potential enhancements in long-context processing}.\n\nIn this work, we present \\OURSSPACE, the first comprehensive benchmark featuring an average data length surpassing 100K tokens. \\OURSSPACE includes tasks in different domains (novels, code, math, etc.) and languages (English and Chinese). To fully evaluate the performance of long-context LLMs, \\OURSSPACE integrates synthetic tasks that can be auto-generated for even longer contexts (e.g., finding the top-$k$ number in an array)  in addition to a set of realistic tasks. \n\nTo construct tasks annotated by humans, we develop 5 annotation pipelines for detailed example annotation. These pipelines undergo iterative refinement until the examples meet quality standards. Auto-generated tasks, conversely, can be easily scaled to various lengths. Upon completing \\OURS, we assess the performance of several state-of-the-art (SOTA) long-context LLMs on this benchmark to gauge its difficulty and evaluate the effectiveness of these models. The results show that current SOTA LLMs are not fully equipped to handle all tasks within \\OURS, highlighting the ongoing challenge of enabling LLMs to process long contexts effectively. We also conduct intriguing analyses on the behavior of LLMs on such long contexts, including the task length ablation, the absent of ``lost in the middle phenomenon~\\cite{lost-in-the-middle}'', and the context recalling prompting techniques.\n\nOur contributions can be summarized as follows:\n\\begin{itemize}\n    \\item We construct and release \\OURS, the first multi-domain bilingual benchmark for evaluating the ability to understand and reason over contexts surpassing 100K tokens. \n    \\item We evaluate SOTA long-context LLMs on \\OURS, which reveals severe performance degradation of these LLMs when scaling context lengths. These experimental results and analysis also indicate promising directions to improve long-context LLMs.\n\\end{itemize}\n\n\\section{Related Work}\n\n\\paragraph{Extending Context Length}\n\nTransformers, typically trained on text sequences under 8K tokens due to self-attention's quadratic complexity, face challenges in longer downstream tasks. To address this, two main strategies have emerged: firstly, the development of positional encodings capable of handling longer text sequences~\\cite{sun2022length, press2021train}, and secondly, the refinement of inference stage techniques to extend current LLMs post-training. The primary approach involves modifying rotary positional encoding~\\cite{su2023roformer} and implementing post-training adjustments to better manage the increased relative positional distances in longer sequences~\\cite{zhu2023pose, yarn, chen2023extending}.\n\n\\paragraph{100K+ LLMs}\n\nRecently, many LLMs have shown the ability to handle over 100K tokens. Some popular proprietary 100K+ LLMs include GPT-4, Claude 2~\\citep{claude2}, and Kimi-Chat~\\citep{{kimi}}. \nOn the other hand, there are much fewer open-source 100K+ models. Some notable models include YaRN~\\citep{yarn} and Yi-200K~\\citep{Yi-34B-200K, Yi-6B-200K}.\nIn this paper, we benchmark GPT-4, Claude 2, Kimi-Chat, and YaRN-Mistral-7B-128K\\footnote{\\url{https://huggingface.co/NousResearch/Yarn-Mistral-7b-128k}, we denote this model by YaRN-Mistral.} on \\OURS, which are some of the latest and strongest LLMs that claim to be able to handle over 100K tokens. %可删\n\n\\paragraph{Inference Infrastructure}\nNumerous studies aim to accelerate self-attention computation. Research primarily concentrates on refining attention mechanisms through improved IO management~\\citep{dao2022flashattention,dao2023flashattention2}, memory optimization~\\citep{kwon2023efficient, Shazeer2019FastTD, Ainslie2023GQATG}, and enhanced parallelization in decoding~\\citep{flash-decoding,flash-decoding-plusplus}. Approaches like Sliding Window Attention~\\citep{beltagy2020longformer}, LM-Infinite~\\citep{Han2023LMInfiniteSO}, and StreamingLLM~\\citep{Xiao2023EfficientSL} introduce attention variants for handling infinitely long sequences without overwhelming computation or memory overhead. However, these techniques often face challenges in maintaining historical information.\n\n\\paragraph{Long Context Benchmarks}\n\nSeveral benchmarks exist for evaluating long-context AI models, notably featuring context lengths of around 10K tokens. L-Eval~\\citep{An2023LEvalIS} and LongBench~\\citep{bai2023longbench} are prominent examples, aggregating pre-existing tasks\\cite{Kocisk2017TheNR,Dasigi2021ADO,Yang2018HotpotQAAD,Huang2021EfficientAF,Joshi2017TriviaQAAL} into comprehensive benchmarks. LongBench encompasses four categories—QA, summarization, synthetic retrieval, and code—spanning 21 tasks, with four being novel. Conversely, L-Eval incorporates 18 tasks across QA, summarization, math, retrieval, and multiple-choice (MC) domains, introducing three new tasks. Another notable benchmark, LooGLE~\\citep{Li2023LooGLECL}, differentiates between short and long dependency examples, focusing on summary and QA tasks; its summary corpus contrasts with ours, utilizing academic papers over novels. The Long-Range Arena (LRA) \\citep{tay2020long} further diversifies with six tasks in text, image, and math, designed for scalability. In comparison, \\OURSSPACE stands out for its substantially longer contexts and a broader range of task domains. Table~\\ref{tab:table-benchmark-comparison} offers a detailed comparison of these long-context benchmarks.\n\n\\begin{figure}[!t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{images/data_pie.pdf}\n    \\caption{The statistics of the data in \\OURS. The radius of each segment indicates the length of input plus output on the logarithmic scale, and the width (or angle) indicates the number of examples (proportionally to the total number of examples).}\n    \\label{fig:data-distribution}\n\\end{figure}\n\n\\section{\\OURS} % 3\n\n\\begin{table}[!t]\n    \\footnotesize\n    \\centering\n    \\begin{tabular}{l|cccc}\n        \\toprule\n        \\textbf{Task}  & \\textbf{Annotation}  & \\textbf{\\# Ex.} & \\textbf{Avg Len} \\\\\n        \\midrule\n        Ret.PassKey    & Auto & 590 & 122.4K/2\\\\\n        Ret.Number     & Auto & 590 & 122.4K/4\\\\\n        Ret.KV         & Auto & 500 & 121.1K/22.7\\\\\n        \\midrule\n        En.Sum      & Human & 103 & 103.5K/1.1K\\\\\n        En.QA       & Human & 351 & 192.6k/4.8\\\\\n        En.MC       & Human & 229 & 184.4K/5.3\\\\\n        Zh.QA       & Human & 189 & 2068.6K/6.3 \\\\\n        En.Dia      & Auto & 200 & 103.6K/3.4 \\\\\n        \\midrule\n        Code.Debug  & Human & 394 & 114.7K/4.8\\\\\n        Code.Run    & Auto & 400 & 75.2K/1.3 \\\\\n        \\midrule\n        Math.Calc   & Auto & 50 & 43.9K/43.9K \\\\\n        Math.Find   & Auto & 350 & 87.9K/1.3 \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\caption{Data statistics. The columns indicate whether the annotation was auto-generated or done by humans, the number of examples, and the average length (input/output) in tokens.}\n    \\label{tab:data-statistics}\n\\end{table}\n\\begin{table*}[!ht]\n    \\centering\n    \\begin{tabular}{l|cccc}\n        \\toprule\n        \\textbf{Task}    \n            &  \\textbf{GPT-4}    \n            &  \\textbf{YaRN-Mistral}   \n            &  \\textbf{Kimi-Chat}    \n            &  \\textbf{Claude 2}   \\\\\n        \\midrule\n        Retrieve.PassKey     \n            & \\textbf{100.00}            \n            & 92.71              \n            & 98.14        \n            & 97.80          \\\\\n        Retrieve.Number      \n            & \\textbf{100.00}            \n            & 56.61              \n            & 95.42        \n            & 98.14          \\\\\n        Retrieve.KV          \n            & \\textbf{89.00}          \n            & 0.00   \n            & 53.60        \n            & 65.40       \\\\  \n        En.Sum               \n            & 14.73           \n            & 9.09   \n            & \\textbf{17.93}         \n            & 14.45          \\\\\n        En.QA                \n            & \\textbf{22.22}          \n            & 9.55              \n            & 16.52        \n            & 11.97          \\\\\n        En.MC                \n            & 67.25         \n            & 27.95              \n            & \\textbf{72.49}        \n            & 62.88         \\\\\n        En.Dia               \n            & 8.50           \n            & 7.50               \n            & 11.50        \n            & \\textbf{46.50}  \\\\\n         Zh.QA                \n            & \\textbf{23.06}          \n            & 16.98              \n            & 18.62        \n            & 10.53    \\\\\n         Code.Debug           \n            & \\textbf{39.59}          \n            & 0.76   \n            & 18.02       \n            & 2.28   \\\\\n         Code.Run             \n            & \\textbf{23.25}          \n            &  1.25   \n            &  2.00   \n            &  2.50   \\\\\n         Math.Calc            \n            & \\textbf{0.01} \n            & 0.00  \n            & 0.00  \n            & 0.00   \\\\\n         Math.Find            \n            & \\textbf{60.00}          \n            & 17.14              \n            & 12.57        \n            & 32.29    \\\\\n        \\midrule\n        Average\n        & \\textbf{45.63}\n        & 19.96\n        & 34.73\n        & 37.06\\\\\n        \\bottomrule\n    \\end{tabular}\n    \\caption{Main results. The performance of the baselines in \\OURS. For multiple-choice questions, if the model does not output one of the options, we regard it as an empty prediction, and thus give it a score of 0.}\n    \\label{tab:main-results}\n\\end{table*}\n\n\\OURSSPACE encompasses 12 tasks spanning 5 domains: retrieval, code, math, novels, and dialogue. Two of these tasks are derived from existing literature\\cite{Mohtashami2023LandmarkAR,lost-in-the-middle}. Among the newly introduced tasks, half are generated automatically, while the remainder are annotated by humans. \n\nIn total, \\OURSSPACE includes 3946 examples, featuring a length beyond 100K tokens (average approximately 200K). Figure~\\ref{fig:data-distribution} illustrates the distribution of these tasks. Table~\\ref{tab:data-statistics} details their respective input and output lengths as well as the number of examples per task. \n\nNext, we illustrate each task in detail. The tasks can be grouped into two broad categories. The first involves realistic context collected from real-world scenarios which has potential practical usage of long context LLMs. The second depends on synthetic contexts which are created or collected for testing certain capabilities of long-context LLMs. \n\n\\subsection{Realistic Context}\n\n\\subsubsection{Novel}\n\nWe develop novel-based tasks as outlined in Figure~\\ref{fig:annotation-pipeline}, utilizing novels sourced from websites\\footnote{\\label{SparkNotes}\\url{https://www.sparknotes.com/}}\\footnote{\\label{CliffsNotes}\\url{https://www.cliffsnotes.com/}} and are manually filtered. More annotation information in Appendix.~\\ref{sec:annotation-process}.\n\nIn these tasks, models are tasked with reasoning over entire novels presented during inference. Recognizing that many novels, along with their movie adaptations and related discussions, are accessible online and may have been encountered by LLMs during training, we adopt \\textit{key entity replacement} as a countermeasure. This involves substituting prominent entities determined by annotators, such as main character names, with unrelated ones, creating ``fake novels''.\n\nUsing these altered novels, we design tasks in three formats: summarization, open-form question answering (QA), and multiple-choice (MC) questions, applying key entity replacement to the annotations as well. All English tasks share the same set of modified novels.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{images/annotation_pipeline.pdf}\n    \\caption{The annotation pipelines for the human-annotated tasks in \\OURS.}\n    \\label{fig:annotation-pipeline}\n\\end{figure}\n\\paragraph{En.Sum} \n\nThe En.Sum task requires models to generate a concise summary of the novel. Gold standard labels are sourced from the web and undergo manual filtering to remove non-summarization content, like comments. Model performance is evaluated using the ROUGE-L-Sum metric~\\citep{lin-2004-rouge}.\n\n\\paragraph{En.QA \\& Zh.QA} \n\nWe employ the same annotation pipeline for both En.QA and Zh.QA tasks, ensuring that the questions necessitate long-range dependency and reasoning, beyond simple short passage retrieval. The tasks are primarily categorized into two types of reasoning:\n\\begin{itemize}\n    \\item Aggregation: This involves compiling various pieces of information scattered throughout the novel. An example question in \\OURSSPACE is ``How much money in total did A spend on lunch?''\n    \\item Filtering: This requires identifying specific information from a larger set. An example question in \\OURSSPACE is ``What color dress did A wear when A met B for the second time?''\n\\end{itemize}\n\nThese tasks test LLMs to locate and process information within the novel, performing reasoning through aggregation or filtering to derive answers.\n\n\\paragraph{En.MC} \n\nThe En.MC task is annotated similarly to En.QA, but differs in that the model is presented with four answer choices. Annotators are instructed to craft these options to be challenging.\n\n\\subsubsection{Dialogue}\n\n\\paragraph{En.Dia}\n\nThe construction process for the En.Dia task is depicted in Figure~\\ref{fig:annotation-pipeline}. We gather movie and drama scripts from a designated online database\\footnote{\\url{https://imsdb.com/}}, focusing on a corpus of long, multi-role dialogues. Only the English scripts are retained and necessary cleaning is applied.\n\nIn the En.Dia task, random instances of character names within a script are replaced with \\textcolor{blue}{\\texttt{\\$\\$MASK\\$\\$}}. The objective is to correctly identify these masked names. For scripts falling short of 100K tokens, we augment them by padding with additional scripts.\n\n\\subsubsection{Code}\n\n\\paragraph{Code.Debug} \n\nWe develop the task as per the process illustrated in Figure~\\ref{fig:annotation-pipeline}. Code repositories, sourced from PyPI\\footnote{\\label{PyPI}\\url{https://pypi.org/}}, undergo a filtering process, and those outside the 64K to 256K token range are excluded (tokenization via the tiktoken tokenizer\\cite{Tiktoken}). Each repository is transformed into a single file, aggregating the content from all files within, each prefaced by its relative path to the root directory. Three of the authors then insert a deliberate and obvious error into one function per repository. The options are presented in the \\texttt{Class.Function} format. Six methods are employed for bug insertion: (1) deleting a necessary variable declaration; (2) using an incorrect number of arguments in function calls; (3) creating infinite loops; (4) causing indentation errors; (5) substituting references with undefined variable/function names; (6) introducing blatant syntax errors (e.g., non-closed brackets).\n\nInitial results indicate that this task is too challenging for current LLMs (None of the baseline models can identify the most obvious error such as non-closed brackets). To mitigate this, we offer four answer choices, one containing the injected bug and the others are bug-free. Note that this makes many examples easily solved by external retrieval preprocess. However, we encourage the users not to use external retrieval preprocess to keep the evaluation fair. And we are looking forward to the stage where LLMs can directly solve the problem without selection options.\n\n\\subsection{Synthetic Context}\n\nThe second category of tasks is characterized by a synthetic context. These tasks, devoid of direct real-world application or use case, are engineered to evaluate the capability for processing lengthy contexts. We delineate four essential ability for effective long-context processing:\n\n\\begin{enumerate}[itemsep=0pt, parsep=0pt]\n    \\item Location and retrieval. This encompasses all retrieval tasks.\n    \\item Elevated information resolution. This involves the Retrieve.Number task.\n    \\item State preservation. This incorporates the Code.Run and Math.Find functions.\n    \\item Sequential processing. This utilizes the Math.Calc function.\n\\end{enumerate}\n\n\\subsubsection{Retrieve}\n\nIn retrieval tasks, models retrieve specific character sequences from lengthy contexts with predominantly irrelevant content. Such tests, adaptable for any context length, can assess the impact of information placement on model performance, like the \\emph{lost-in-the-middle} phenomenon~\\citep{lost-in-the-middle}. The three retrieval tasks in \\OURSSPACE vary in complexity.\n\n\\paragraph{Retrieve.PassKey} \n\nThis task is first proposed by \\citet{Mohtashami2023LandmarkAR}. Models are prompted to find a specific \\textcolor{blue}{\\texttt{<key>}} called pass key, which is a random 5-digit sequence. The pass key is inserted into a lengthy and noisy context, as shown below. In \\OURS, we generate examples with 59 different pass key locations that are evenly distributed in the context. At each location, we construct 10 examples with different pass keys. This results in 590 examples.\n\\begin{tcolorbox}\n\\small\nThere is an important pass key hidden in a lot of irrelevant text. Find it.\\\\\n\\textcolor{orange}{<very long noisy context>}\\\\\nThe pass key is \\textcolor{blue}{<key>}. Remember it. The pass key is \\textcolor{blue}{<key>}\\\\\n\\textcolor{orange}{<very long noisy context>}\\\\\nWhat is the pass key?\n\\end{tcolorbox}\n\n\\paragraph{Retrieve.Number} \n\nTo examine the local attention of LLMs, we have enhanced the complexity of Retrieve.PassKey by increasing the answer length to 10 digits and \\textit{incorporating successive repetitive digits}. For example, a \\textcolor{blue}{\\texttt{<key>}} in Retrieve.PassKey valued \\texttt{98762}, while in Retrieve.Number is \\texttt{9998877762}. This modification aims to assess the local resolution capabilities of long context models, as our preliminary experiments indicate that LLMs struggle with discerning repeated numbers.\n\n\\paragraph{Retrieve.KV} \n\n\\citet{lost-in-the-middle} introduce a key-value retrieval task within a large JSON object containing many key-value pairs (e.g., \\texttt{30eea139-b6dd-43fc-bc5d-0d3d17980229} $\\rightarrow$ \\texttt{bfd36c2b-c57e-41ef-9cc1-b21b4e60e664}). This task demands the model to accurately identify and retrieve the value corresponding to a specified key. The complexity of this task is heightened due to the indistinguishable format of relevant and irrelevant information.\n\n\\subsubsection{Code}\n\\paragraph{Code.Run} \n\nIn this task, we evaluate the ability of LLMs to simulate multi-step function executions that involve basic arithmetic operations. While this task is readily solvable using a Python interpreter, the focus here is on the long-term state tracking required in such tasks. The capability of state tracking has been demonstrated in GPT-4~\\citep{bubeck2023sparks}. Specifically, the task involves creating Python code consisting of multiple simple functions, incorporating operations such as addition, subtraction, and nested function calls. The structural design of these tasks is as follows:\n\n\\begin{minted}{python}\ndef func_0(x):\n    return func_1(x) + 4\n\ndef func_1(x):\n    return x - 1\n\\end{minted}\n\nSome functions' return values are dependent on other functions (e.g., \\textcolor{blue}{\\texttt{func\\_0}} invokes \\textcolor{blue}{\\texttt{func\\_1}}). We define \\emph{depth} as the number of cascading function calls initiated by a single call. Thus, the depth for \\textcolor{blue}{\\texttt{func\\_1}}'s call within \\textcolor{blue}{\\texttt{func\\_0}} is 1. In Code.Run, we employ depths ranging from 2 to 10, ensuring each function calls at most one other function. To keep the simplicity of each single step of computation, these functions are restricted to performing only addition and subtraction.\n\n\\subsubsection{Math}\n\n\\paragraph{Math.Find} \n\nMath.Find assesses the model's capability to identify specific elements within a large array, requiring comprehensive observation for accuracy. This task also tests the ability to preserve states while encoding the context. Concretely, the model receives an extensive list of numbers and is tasked with locating one of seven key numbers: the three largest (1st, 2nd, and 3rd), the three smallest (1st, 2nd, and 3rd), and the median.\n\n\\paragraph{Math.Calc} \n\nTo assess sequential processing skills, Math.Calc prompts the model to compute the result of a lengthy arithmetic expression featuring addition and subtraction. Initial experiments indicate that current LLMs struggle to directly produce the final answer. Hence, we instead query the LLMs to provide the intermediate result following each operator. Model performance is evaluated based on the number of correct values preceding the first error.\n\n\\section{Experiments} % 1\n\nWe conduct a thorough set of experiments on \\OURS. We will introduce the baselines, experimental setup, and main results in this section.\n\\subsection{Baselines}\n\\OURSSPACE generally requires the ability to handle input contexts longer than 100k. There is a handful of LLMs that claim to be capable of handling contexts over 100k. We include four baselines. The first three are proprietary LLMs as we do not have access to the model, while the last baseline is open-sourced. Details on evaluation are in Appendix.~\\ref{sec:evaluation-process}.\n\n\\paragraph{GPT-4} \nGPT by OpenAI is one of the most widely used and capable LLMs in the market, and a recent version of GPT-4 ~\\citep{gpt4turbo} can support 128K contexts. \n\n\\paragraph{Claude 2}\nClaude 2~\\citep{claude2} is a proprietary chat-based LLM released by Anthropic AI and has shown impressive capabilities. The second version of the Claude series supports 200K contexts. We manually enter each example through the webpage because we have no access to their API.\n\n\\paragraph{Kimi-Chat}\nKimi-Chat, a proprietary chat-oriented LLM developed by Moonshot AI \\cite{kimi}, is designed to process contexts up to 200K. Due to the lack of API access, we manually input the test data using their web interface.\n\n\\paragraph{YaRN-Mistral}\nYaRN-Mistral is a derivative of Mistral-7B~\\citep{mistral} introduced by \\citet{yarn}. The original Mistral-7B was trained on input lengths up to 8K and shows a reduced performance in longer contexts. \\citet{yarn} adapted it to 128K contexts by modifying the position encoding and continued training.\n\n\\subsection{Experimental Setup}\n\\paragraph{Prompt Templates}\n\nFor each model-task combination, we craft prompts to optimize model performance on short dummy examples. Detailed prompt templates for each model and task can be found in Appendix~\\ref{sec:prompt-templates}.\n\n\\paragraph{Input Truncation}\nAll API-based baselines are subject to a maximum input length limit and will reject inputs exceeding this threshold. While YaRN-Mistral is theoretically capable of handling longer contexts, the authors only claim abilities up to 128K. Therefore, inputs are truncated by removing the center and joining both ends. This approach is predicated on the assumption that key information, such as instructions and book titles, is typically located at either the start or the end of a prompt.\n\n\\subsection{Main Result}\n\\begin{figure*}[!ht]\n    \\centering\n    \\includegraphics[width=\\linewidth]{images/score_by_length.pdf}\n    \\caption{Baseline performance as a function of input length.}\n    \\label{fig:score-by-length}\n\\end{figure*}\n\nTable~\\ref{tab:main-results} and Figure~\\ref{fig:benchmark-result} display the performances of various baselines on \\OURS. Notably, GPT-4 outperforms other baselines in the retrieval, code, and math domains, with a considerably higher average score. However, in the novel-based tasks, no distinct winner emerges among the proprietary LLMs. On the other hand, the open-source YaRN-Mistral lags behind the proprietary models in most tasks, exhibiting almost random performance in multiple areas. This aligns with its relatively inferior performance in shorter contexts compared to these models. Additionally, it is observed that the baselines generally excel more in retrieval tasks than in other areas, echoing the relative simplicity of these tasks for human participants.\n\\section{Analysis}\n\nWe subsequently perform a detailed analysis of the results, identifying and emphasizing several notable and interesting phenomena.\n\n\\subsection{Length Ablation}\n\nIn line with our benchmark's goal to assess proficiency in managing lengthy contexts, we verify the baselines' capability with shortened context versions. A subset of the auto-generated tasks is modified accordingly, and the performance outcomes are illustrated in Figure \\ref{fig:score-by-length}. It is observed that model performance generally declines with longer input lengths compared to shorter ones. This suggests that while these baselines are technically equipped to handle extended inputs, their effectiveness diminishes significantly under such conditions.\n\n\\subsection{Lost in the middle}\n\\begin{figure*}[!ht]\n    \\centering\n    \\includegraphics[width=0.9\\linewidth]{images/score_by_pos.pdf}\n    \\caption{Performance as a function of the answer position (in the number of characters). The steep drop in performance for Kimi-Chat in the middle on Retrieval.KV is caused by the answer being removed by truncation.}\n    \\label{fig:performance-by-answer-pos}\n\\end{figure*}\n\n\\begin{figure}[!t]\n\\begin{tcolorbox}\n\\small\nOne function in this repo is deliberately made to include an obvious error. Find it.\\\\\n\\textcolor{orange}{<code>}\\\\\nThink step by step and at last give me your answer for the function with the deliberate error.\\\\\n\\textcolor{blue}{<list of options>}\n\\end{tcolorbox}\n\n\\begin{tcolorbox}\n\\small\nOne function in this repo is deliberately made to include an obvious error. Find it.\\\\\n\\textcolor{orange}{<code>}\\\\\nLocate the functions in the options, repeat their content, inspect through code, and at last give me your answer for the function with the deliberate error.\\\\\n\\textcolor{blue}{<list of options>}\n\\end{tcolorbox}\n\\caption{Compared to the first prompt, the second prompt improves GPT-4's results on Code.Debug dramatically.}\n\\end{figure}\nPrior research indicates a performance decline in some LLMs when answers are positioned around the center of the context~\\citep{lost-in-the-middle}. However, our findings do not strongly corroborate this. As depicted in Figure~\\ref{fig:performance-by-answer-pos}, we analyze model performance based on answer location in three location-dependent tasks. We observe \\textit{no consistent trend between performance and answer position across different models}. For instance, GPT-4 shows a preference for early answers in Retrieval.KV but favors later ones in En.Dia. In contrast, Claude 2's performance remains relatively unaffected by answer position on all three tasks, whereas YaRN-Mistral and Kimi-Chat excel with end-positioned answers (except that YaRN-Mistral get zero performance on all positions on Retrieval.KV).\n\nOne plausible reason why we have different observations from \\citet{lost-in-the-middle} is that they experiment with different models using at most 16K length contexts, which is about 8 times shorter than our setting. The models in their study are also different from ours.\nFinally, the tasks are different: their experiments involve document question answering (and their result with Retrieval.KV arguably does not show a very pronounced performance drop as well). We hypothesize that the phenomenon of ``Lost in the middle'' is only exhibited on specific tasks and models. A more thorough investigation of these differences is beyond the scope of this paper.\n\n\\subsection{Context Recalling}\n\\label{sec:pipeline-instruction}\n\nWe identify an intriguing prompting technique for tasks involving extended context, termed \\textit{context recalling}. This technique posits that, although the information is present in the context and accessible via direct attention, it may be more effective to first prompt the model to \\textit{recall the relevant information in its generation before engaging in further reasoning}. In our experiments using Code.Debug, when we merely instructed GPT-4 to process information step-by-step, the accuracy was \\textbf{15.74\\%}. However, by explicitly directing GPT-4 to repeat the relevant code before analysis, its accuracy on Code.Debug markedly improved to \\textbf{39.59\\%}. This approach of context recalling warrants additional investigation.\n\n\\section{Conclusions}\n\nWe introduce \\OURS, the first benchmark tailored for long contexts exceeding 100K in average length. Empirical evidence indicates that despite claims of proficiency with such extensive contexts, current LLMs demonstrate significant performance degradation when dealing with them. This finding highlights the need for advanced methodologies to improve LLMs' efficiency in processing long context. Additionally, our analysis offers insights into LLM behavior in long-context tasks, guiding future research.\n\n\\newpage\n\n\\section*{Limitations}\n\nWhile our benchmark offers valuable insights into LLM performance, it may not be sufficiently diverse or extensive to provide a comprehensive assessment of model capabilities, a constraint common to most benchmarks. Additionally, the reliance on exact match for scoring, dependent on prompt templates and answer parsing methods, may necessitate tailored redesigns for new model evaluations.\n\nFurthermore, supporting contexts up to 100K tokens may fall short for applications requiring analysis of extensive datasets, such as multiple books or entire databases. Exploring LLMs' capacity to handle up to a million tokens or more presents a promising research avenue. In practical applications, finetuning models to memorize context, rather than processing it during inference, could offer a more efficient alternative, albeit with significant computational demands.\n\n\\section*{Ethics Statement}\nOur human annotators are directed to exclude data that may raise sensitive ethical issues, such as offensive language or social biases. Nonetheless, the potential for encountering sensitive content persists, particularly if the sourced books or code contain such material. This concern is somewhat mitigated since the benchmark's primary focus is on evaluating the long-context capabilities of LLMs, rather than influencing their social bias.\n\nThe goal of this research is to advance the development of LLMs' proficiency in handling extensive contexts. This could aid in implementing more effective ``guardrails'' against misuse by incorporating detailed specifications prior to user interactions. However, this approach also potentially increases the risk of novel prompt injection attacks.\n\n\\newpage\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{LongBench v2: Towards Deeper Understanding and\\\\ Reasoning on Realistic Long-context Multitasks}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\n\nThis paper introduces LongBench v2, a benchmark designed to assess the ability of LLMs to handle long-context problems requiring \\emph{deep understanding and reasoning} across real-world multitasks. LongBench v2 consists of 503 challenging multiple-choice questions, with contexts ranging from 8k to 2M words, across six major task categories: single-document QA, multi-document QA, long in-context learning, long-dialogue history understanding, code repository understanding, and long structured data understanding.\nTo ensure the breadth and the practicality, we collect data from nearly 100 highly educated individuals with diverse professional backgrounds. We employ both automated and manual review processes to maintain high quality and difficulty, resulting in human experts achieving only 53.7\\% accuracy under a 15-minute time constraint.\nOur evaluation reveals that the best-performing model, when directly answers the questions, achieves only 50.1\\% accuracy. In contrast, the o1-preview model, which includes longer reasoning, achieves 57.7\\%, surpassing the human baseline by 4\\%. \nThese results highlight the importance of enhanced reasoning ability and scaling inference-time compute to tackle the long-context challenges in LongBench v2.\n\n\\end{abstract}\n\\section{Introduction}\n\nOver the past year, research and products on long-context large language models (LLMs) have made remarkable progress: in terms of context window length, advancing from the initial 8k to the current 128k and even 1M tokens~\\cite{GPT-4o,claude-3-5,reid2024gemini,glm2024chatglm}; and achieving promising performance on long-context benchmarks. However, beneath these advancements lies an urgent and practical question: \\textbf{Do these models truly comprehend the long texts they process, i.e., are they capable of deeply understanding, learning, and reasoning based on the information contained in these long texts?}\n\nCritically, existing long-context understanding benchmarks~\\cite{bai2024longbench,zhang2024infty,hsieh2024ruler} fail to reflect the long-context LLMs' \\emph{deep} understanding capabilities across diverse tasks.\nThey often focus on extractive questions, where answers are directly found in the material, a challenge easily handled by modern long-context models and RAG systems, as evidenced by their perfect recall in the Needle-in-a-Haystack test~\\cite{needleinhaystack}.\nFurthermore, many of these benchmarks rely on synthetic tasks, which limits their applicability to real-world scenarios, and their adopted metrics like F1 and ROUGE are unreliable.\n\nTo address these issues, we aim to build a benchmark with the following features: \n(1) \\textbf{Length}: Context length ranging from 8k to 2M words, with the majority under 128k.\n(2) \\textbf{Difficulty}: Challenging enough that even human experts, using search tools within the document, cannot answer correctly in a short time.\n(3) \\textbf{Coverage}: Cover various realistic scenarios.\n(4) \\textbf{Reliability}: All in a multiple-choice question format for reliable evaluation.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figs/length.pdf}\n    \\caption{Length distribution (left) and human expert solving time distribution (right) of LongBench v2.}\n    \\label{fig:length}\n\\end{figure}\n\nWith the above goal in mind, we present \\emph{LongBench v2}.\nLongBench v2 contains 503 multiple-choice questions and is made up of 6 major task categories and 20 subtasks to cover as many realistic deep comprehension scenarios as possible, including \\emph{single-document QA}, \\emph{multi-document QA}, \\emph{long in-context learning}, \\emph{long-dialogue history understanding}, \\emph{code repository understanding}, and \\emph{long structured data understanding} (detailed in Table~\\ref{tb:stat}).\nAll the test data in LongBench v2 are in English, and the length distribution of each task category is shown on the left of Figure~\\ref{fig:length}.\n\nTo ensure the quality and difficulty of test data, we combine automated and manual reviews during data collection. \nWe first recruit 97 data annotators with diverse academic backgrounds and grades from top universities and then select 24 data reviewers from this group.\nAnnotators provide data including long documents, questions, options, answers, and evidence.\nWe then leverage three long-context LLMs for an automated review, where a question is considered too easy if all three LLMs answer it correctly.\nData passing the automated review are assigned to the reviewers, who answer the questions and determine whether the questions are appropriate (meet our requirements) and if the answers are correct.\nIn our criteria, a qualified data point should have (1) an appropriate question with an objective, correct answer; (2) sufficient difficulty, such that all three LLMs cannot answer correctly at the same time, and the human reviewer cannot answer correctly within 3 minutes, even with searching tools within the document.\nIf data do not meet these criteria, we request modifications from the annotator.\nWe also set length and difficulty incentives to encourage longer and harder test data.\nFigure~\\ref{fig:length} (right) visualizes the distribution of expert solving times along with human accuracy.\n\nOverall, our data shows a median word count of 54k and an average of 104k words. \nHuman experts are able to achieve an accuracy of only 53.7\\% within 15 minutes, compared to 25\\% accuracy with random guessing, highlighting the challenging nature of the test.\nIn the evaluation, the best-performing model achieves only 50.1\\% accuracy when directly outputting the answer. In contrast, the o1-preview model, which incorporates longer reasoning during inference, reaches 57.7\\%, surpassing human experts. This implies that LongBench v2 places greater demands on the reasoning ability of current models, and incorporating more inference-time thinking and reasoning appears to be a natural and crucial step in addressing such long-context reasoning challenges.\nWe hope LongBench v2 will accelerate the exploration of how scaling inference-time compute will affect deep understanding and reasoning in long-context scenarios.\n\\section{Related Work}\n\nWe divide existing long-context benchmarks for LLMs into two types. \nThe first consists of comprehensive benchmarks that combine multitasks such as QA, retrieval, and summarization. \nSorted by publication date, these benchmarks include ZeroSCROLLS~\\cite{shaham2023zeroscrolls}, L-Eval~\\cite{an2024leval}, LongBench~\\cite{bai2024longbench}, BAMBOO~\\cite{dong2024bamboo}, LooGLE~\\cite{li2023loogle}, $\\infty$-bench~\\cite{zhang2024infty}, Ruler~\\cite{hsieh2024ruler}, and HELMET~\\cite{yen2024helmet}.\nIt is noteworthy that most of these multitask benchmarks were proposed last year, which corresponds to the thrive of long-context LLMs, whose context length has been extended to 128k tokens or more~\\cite{claude-3-5,GPT-4o,reid2024gemini,glm2024chatglm,dubey2024llama} through continual training~\\cite{xiong2024effective,pmlr-v235-fu24d,bai2024longalign,gao2024train}.\n\nThe other category of long-context benchmarks is more targeted, evaluating models on specific types of long-context tasks, including document QA~\\cite{kovcisky2018narrativeqa,dua2019drop,dasigi2021dataset,pang2022quality,wang2024leave}, summarization~\\cite{zhong2021qmsum,huang2021efficient,wang2022squality}, retrieval and attributing~\\cite{needleinhaystack,kuratov2024babilong,song2024counting,laban2024summary,zhang2024longcite,vodrahalli2024michelangelo,krishna2024fact}, conversation~\\cite{bai2024longalign}, coding~\\cite{liu2023repobench,bogomolov2024long}, many-shot learning~\\cite{agarwal2024many}, and long-text generation~\\cite{bai2024longwriter,wu2024longgenbench,liu2024longgenbench,que2024hellobench}.\n\nIn our view, existing long-context benchmarks generally have the following issues: (1) \\emph{Lack of deep reasoning}: While a few benchmarks contain longer examples of around 100k, most of these data have not been human-examined, and many of these samples can be solved through shallow understanding such as retrieval, thus failing to reflect a model's deep reasoning capabilities.\n(2) \\emph{Unreliable metrics}: Many datasets use metrics like ROUGE and F1 for evaluation, which are known to be unreliable~\\cite{novikova2017we}. Additionally, some datasets adopt LLM-as-a-judge~\\cite{zheng2023judging,li2024generation} for evaluation, which can be costly and may introduce biases in their assessments~\\cite{bai2024benchmarking,ye2024justice}.\nTo construct a more challenging, reliable, and comprehensive long-context benchmark, we employ a uniform multiple-choice format and manually verify each data point to ensure it meets the required level of difficulty.\n\\begin{table*}[t]\n\\centering  \n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{llrrrr}\n\\toprule\n\\textbf{Dataset} & \\textbf{Source} & \\textbf{\\#data} & \\textbf{Length} & \\textbf{Expert Acc} & \\textbf{Expert Time$^*$} \\\\\n\\midrule\n\\multicolumn{2}{l}{\\cellcolor{mypink}\\emph{I. Single-Document QA}} & \\cellcolor{mypink}175 & \\cellcolor{mypink}51k & \\cellcolor{mypink}55\\% & \\cellcolor{mypink}8.9 min \\\\\nAcademic & Paper, textbook & 44 & 14k & 50\\% & 7.3 min \\\\\nLiterary & Novel & 30 & 72k & 47\\% & 8.5 min \\\\\nLegal & Legal doc & 19 & 15k & 53\\% & 13.1 min \\\\\nFinancial & Financial report & 22 & 49k & 59\\% & 9.0 min \\\\\nGovernmental & Government report & 18 & 20k & 50\\% & 9.5 min \\\\\nDetective & Detective novel & 22 & 70k & 64\\% & 9.3 min \\\\\nEvent ordering & Novel & 20 & 96k & 75\\% & 9.4 min \\\\\n\\midrule\n\\multicolumn{2}{l}{\\cellcolor{mypink}\\emph{II. Multi-Document QA}} & \\cellcolor{mypink}125 & \\cellcolor{mypink}34k & \\cellcolor{mypink}36\\% & \\cellcolor{mypink}6.1 min \\\\\nAcademic & Papers, textbooks & 50 & 27k & 22\\% & 6.1 min \\\\\nLegal & Legal docs & 14 & 28k & 64\\% & 8.8 min \\\\\nFinancial & Financial reports & 15 & 129k & 40\\% & 7.0 min \\\\\nGovernmental & Government reports & 23 & 89k & 22\\% & 6.0 min \\\\\nMulti-news & News & 23 & 15k & 61\\% & 5.3 min \\\\\n\\midrule\n\\multicolumn{2}{l}{\\cellcolor{mypink}\\emph{III. Long In-context Learning}} & \\cellcolor{mypink}81 & \\cellcolor{mypink}71k & \\cellcolor{mypink}63\\% & \\cellcolor{mypink}8.3 min \\\\\nUser guide QA & Electronic device, software, instrument & 40 & 61k & 63\\% & 9.9 min \\\\\nNew language translation & Vocabulary book (\\textit{Kalamang}, \\textit{Zhuang}) & 20 & 132k & 75\\% & 5.4 min \\\\\nMany-shot learning & Multi-class classification task & 21 & 71k & 52\\% & 8.0 min \\\\\n\\midrule\n\\multicolumn{2}{l}{\\cellcolor{mypink}\\emph{IV. Long-dialogue History Understanding}} & \\cellcolor{mypink}39 & \\cellcolor{mypink}25k & \\cellcolor{mypink}79\\% & \\cellcolor{mypink}8.2 min \\\\\nAgent history QA & LLM agents conversation & 20 & 13k & 70\\% & 8.3 min \\\\\nDialogue history QA & User-LLM conversation & 19 & 77k & 89\\% & 6.5 min \\\\\n\\midrule\n\\multicolumn{2}{l}{\\cellcolor{mypink}\\emph{V. Code Repository Understanding}} & \\cellcolor{mypink}50 & \\cellcolor{mypink}167k & \\cellcolor{mypink}44\\% & \\cellcolor{mypink}6.4 min \\\\\nCode repo QA & Code repository & 50 & 167k & 44\\% & 6.4 min \\\\\n\\midrule\n\\multicolumn{2}{l}{\\cellcolor{mypink}\\emph{VI. Long Structured Data Understanding}} & \\cellcolor{mypink}33 & \\cellcolor{mypink}49k & \\cellcolor{mypink}73\\% & \\cellcolor{mypink}6.4 min \\\\\nTable QA & Table & 18 & 42k & 61\\% & 7.4 min \\\\\nKnowledge graph reasoning & KG subgraph & 15 & 52k & 87\\% & 6.2 min \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\caption{Tasks and data statistics in LongBench v2. `Source' denotes the origin of the context. `Length' is the \\emph{median} of the number of words. `Expert Acc' and `Expert Time' refer to the average accuracy and the \\emph{median} time spent on answering the question by human experts. \n$^*$: We allow human experts to respond with ``I don't know the answer'' if it takes them more than 15 minutes.\nAs a result, most expert times are under 15 minutes, but this doesn't necessarily mean that the questions are fully answered within such a time.}\n\\label{tb:stat}\n\\end{table*}\n\n\\section{LongBench v2: Task and Construction}\n\nOur design principle focuses on four aspects: (1) The context should be sufficiently long to cover scenarios ranging from 8k to 2M words, with a relatively even distribution across texts up to 128k words.\n(2) The question should be challenging, requiring the model to deeply understand the context to answer. It should avoid questions that can be answered based on memory or those where the answer can be directly extracted from the context.\n(3) The data should cover a wide range of real-world long-context scenarios and reflect the model's holistic ability to reason, apply, and analyze information drawn from the lengthy text.\n(4) The data should be in English and in a multiple-choice question format, containing a long text, a question, four choices, a groundtruth answer, and an evidence. Distractors should be included to prevent the model from guessing the correct answer based on option patterns.\n\n\\subsection{Task Overview}\nBased on the testing scenarios and the types and sources of long texts, we propose six major task categories and further divide them into 20 subtasks.\nWe introduce the tasks included in LongBench v2 in the following. A list of task statistics and detailed descriptions can be found in Table~\\ref{tb:stat} and Appendix~\\ref{sec:task}.\n\n\\xhdr{Single-Doc QA}\nWe integrate subtask categories from previous datasets~\\cite{bai2024longbench,an2024leval} and expand them to include QA for \\emph{academic}, \\emph{literary}, \\emph{legal}, \\emph{financial}, and \\emph{governmental} documents. \nConsidering that \\emph{detective} QA~\\cite{xu2024detectiveqa} requires in-depth reasoning based on case background, we introduce such a task that requires identifying the killer or motive based on information provided in detective novels.\nWe also include \\emph{Event ordering}, where the goal is to order minor events according to the timeline of a novel.\n\n\\xhdr{Multi-Doc QA}\nTo distinguish from single-doc QA, multi-doc QA requires answers drawn from multiple provided documents.\nBesides the categories in single-doc QA, multi-doc QA also includes \\emph{multi-news QA}, which involves reasoning across multiple news articles, events, and timelines.\n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figs/pipeline.pdf}\n    \\caption{Data collection pipeline of LongBench v2. The annotator first uploads the document(s) and proposes a multiple-choice question based on the content. After that, automated and manual reviews will be conducted to ensure the data meets our requirements. Only data that passes these reviews is eligible for annotation rewards, meaning the annotator must revise the data until it passes all review stages. More details are in section~\\ref{sec:data_collection}.}\n    \\label{fig:pipeline}\n\\end{figure*}\n\n\\xhdr{Long In-context Learning}\nLearning from a long context, such as acquiring new skills, requires the ability to comprehend and reason based on that context. Hence, we consider it as a major category of tasks.\nLongBench v2 includes several key tasks, including \\emph{User guide QA}, which answers questions with information learnt from user guides for electronic devices, software, etc.; \\emph{New language translation}~\\cite{tanzerbenchmark,zhang2024teaching}, which involves learning to translate an unseen language from a vocabulary book; \\emph{Many-shot learning}~\\cite{agarwal2024many}, which involves learning to label new data from a handful of examples.\n\n\\xhdr{Long-dialogue History Understanding}\nLLMs, as more intelligent chatbots or agents, require enhanced memory capabilities to handle longer histories. Therefore, we integrate long-dialogue history understanding tasks to test whether LLMs can handle information from long conversation histories.\nThese tasks are divided into two subtasks based on the source of the conversation history: one involving the history of interactions between multiple LLM agents, i.e., \\emph{Agent history QA}~\\cite{huang2024far}, and the other involving the dialogue history between a user and an LLM acting as an assistant, i.e., \\emph{Dialogue history QA}~\\cite{wu2024longmemeval}.\n\n\\xhdr{Code Repository Understanding}\nCode repository contains long code content, and question answering over a code repository requires understanding and reasoning across multiple files, making it a common yet challenging long-context task.\n\n\\xhdr{Long Structured Data Understanding}\nIn addition to textual data, much information is presented in structured forms, so we introduce the long structured data QA task to test the LLM's understanding of long structured data, including reasoning on long tables, i.e., \\emph{Table QA}~\\cite{zhang2024tablellm}, and answering complex queries on knowledge graphs (KGs), i.e., \\emph{Knowledge graph reasoning}~\\cite{cao2022kqa,bai2023answering}.\nWe anonymize the entities in the KG to prevent the model from directly deriving the answers through memorization.\n\n\\subsection{Data Collection}\n\\label{sec:data_collection}\n\nTo collect high-quality and challenging data for long-context tasks, we hire 97 annotators who are either holding or pursuing a bachelor's degree from top universities and are proficient in English, with detailed statistics shown in Appendix~\\ref{sec:stat}. We also select 24 professional human experts based on their major and year of study for conducting manual reviews.\nFigure~\\ref{fig:pipeline} illustrates the overall pipeline of our data collection process, which consists of five steps: document collection, data annotation, automated review, manual review, and data revision (optional). \nWe develop an online annotation platform to implement this pipeline, with further details provided in Appendix~\\ref{sec:platform}.\n\n\\xhdr{Step 1: Document Collection}\nUnlike previous benchmarks~\\cite{bai2024longbench,an2024leval}, where long documents are pre-defined or synthesized by the benchmark designers, we aim to gather documents that reflect more diverse scenarios and are more likely to be used in everyday contexts. To achieve this, we ask annotators to upload one or multiple files they have personally read or used, such as research papers, textbooks, novels, etc., according to the task type.\nOur platform first converts the uploaded files into plain text using tools such as \\href{https://github.com/pymupdf/PyMuPDF}{\\texttt{PyMuPDF}}.\nThe input documents then undergo two automatic checks. If the length is less than 8,192 words, it is rejected as too short. Documents with a high overlap with previous annotations are also rejected to ensure diversity.\n\n\\xhdr{Step 2: Data Annotation}\nDuring data annotation, the annotator is tasked with proposing a multiple-choice question based on their submitted documents. The question should be accompanied with four choices, a groundtruth answer, and the supporting evidence. We provide the annotators with a detailed question design principle that specifies our requirement (Appendix~\\ref{sec:guide}). To summarize, the following types of questions should be avoided:\n(1) \\emph{Counting questions}: Avoid questions that require counting large numbers.\n(2) \\emph{Simple retrieval questions}: Do not ask basic information retrieval questions, as these are too easy for modern LLMs~\\cite{song2024counting}.\n(3) \\emph{Overly professional questions}: Questions should not demand extensive external knowledge; they should rely on minimal expertise.\n(4) \\emph{Tricky questions}: Do not create questions that are deliberately difficult; the goal is to keep the questions natural and straightforward.\n\n\\xhdr{Step 3: Automated Review}\nUpon submission, each question undergoes an initial automated review process to ensure it is not too easy. \nWe employ three fast and powerful LLMs with a 128k context length to answer the questions: GPT-4o-mini~\\cite{GPT-4o-mini}, \\href{https://open.bigmodel.cn/pricing}{GLM-4-Air}, and \\href{https://open.bigmodel.cn/pricing}{GLM-4-Flash}.\nInputs that exceed the context length are truncated from the middle.\nIf all three LLMs answer the question correctly, it is considered too easy. In such cases, annotators will be required to revise the question and choices to increase its difficulty.\n\n\\xhdr{Step 4: Manual Review}\nData passing the automated review is sent to a human expert for manual review. \nOur manual review serves two purposes: first, to filter out unqualified questions and data with incorrect answers; second, to establish a human baseline while also determining the difficulty of the questions and filter out those that are too easy (i.e., questions that humans can answer correctly in a short amount of time).\nIn practice, the reviewer first goes through a checklist to determine whether the question meets the specified requirements (outlined in Appendix~\\ref{sec:guide}). Next, the reviewer downloads the raw document files and attempts to answer the question. The reviewer is encouraged to use searching tools within the files to solve the problem more promptly. Once a choice is submitted, the reviewer can view the groundtruth answer and the evidence provided by the annotators. The reviewer will then decide whether the answer is objective and fully correct. Our platform tracks the time spent on each question, and if the human expert answers correctly within 3 minutes, the question will be considered too easy, demanding a revision from its annotator.\nSince answering some questions may require spending several hours reading the material, which implies a significant review time cost, we allow human experts to respond with ``I don't know the answer'' after 15 minutes.\n\n\\xhdr{Data Revision}\nAs mentioned above, questions deemed unqualified during either automated or manual review will require revision by its annotator. We set up a separate page in our platform for annotator to track their rejected data. For each rejected data, we provide the annotator with a reason for the rejection, classified into three categories: (1) \\emph{Illegal question}: Rejected by human reviewers due to the question being unqualified, (2) \\emph{Insufficient difficulty}: Rejected by automated review or due to human reviewer answering the question correctly within 3 minutes, and (3) \\emph{Wrong answer}: Rejected by human reviewers. Based on this feedback, annotators will refine their data until it passes the review process. To avoid wasting too much manual resources on low-quality data, we will terminate the review-revision cycle if the data has been revised more than five times without passing.\n\n\\xhdr{Mechanism Design}\nTo incentivize annotators to provide high-quality, challenging, and longer test data, our reward mechanism is set as follows. First, annotators can receive a base reward of \\texttt{100} \\texttt{CNY} only if the data passes the review process; no reward is given for data that does not pass. To encourage annotators to provide longer data, we offer additional length rewards of \\texttt{20}, \\texttt{40}, and \\texttt{50} \\texttt{CNY} for passed data in the length ranges $(32k, 64k]$, $(64k, 128k]$, and over $128k$, respectively (in word count). To motivate annotators to provide more difficult data, we define \\emph{hard} set data as data where at least two out of three models do not answer correctly in automated review and the human reviewer is unable to solve it within 10 minutes; all other data is considered \\emph{easy} data. For hard data, annotators can earn an additional difficulty reward of \\texttt{50} \\texttt{CNY}.\nEach human expert is rewarded \\texttt{25} \\texttt{CNY} for reviewing each piece of data. We also conduct random checks on their reviews, and any human expert whose reviews repeatedly fail these checks will have all of their reviewing rewards revoked.\n\n\\subsection{Data Verification}\nFor a final check, we sample 70 test data and invite our authors to verify their correctness and whether they are Google-proofed~\\cite{rein2023gpqa}.\n\n\\xhdr{Correctness} \nCheck the selected answer based on the provided evidence to determine if it is correct, with all other options being incorrect. An answer is also deemed incorrect if there is any controversy, ambiguity, or reliance on subjective judgment.\n\n\\xhdr{Google-proof} Search for the answer to the question on the internet (Google). The data is considered Google-proof if the answer cannot be found within 15 minutes of searching.\n\nThrough our verification, we find that \\texttt{68/70} of the data are completely correct, and \\texttt{67/70} are Google-proofed. Therefore, we estimate that the error rate of our data is around 3\\%, and the majority of the questions cannot be answered by memorizing existing data on the internet.\nWe review all the data to ensure that it does not contain any sensitive information related to privacy or copyrights.\n\n\\subsection{Data Statistics}\nWe categorize the 503 data entries in Longbench v2 based on their difficulty, length, and task types. According to the difficulty criteria defined in the previous section, 192 are classified as ``Easy'', while 311 are deemed ``Hard''. Based on word count, the data is divided into three groups: ``Short'' ($<$32k), ``Medium'' (32k-128k), and ``Long'' ($>$128k), containing 180, 215, and 108 entries, respectively, exhibiting a relatively balanced distribution.\nFor the data distribution across task types, please see Table~\\ref{tb:stat}.\nAlso, the questions with answers A, B, C, and D account for approximately 19\\%, 25\\%, 30\\%, and 26\\% of the total, respectively, showing that the distribution of answers across the four options is relatively even.\nWe also analyze the proportion of data submissions rejected during manual review and find that 4\\% of the submissions are rejected for \\emph{illegal question}; 7\\% are rejected for \\emph{insufficient difficulty}; and 4\\% are rejected for \\emph{wrong answer}.\n\\section{Evaluation}\n\n\\subsection{Baselines}\n\n\\xhdr{Setup}\nWe evaluate 10 open-source LLMs, all of which have a context window size of 128,000 tokens, along with 7 proprietary LLMs.\nWe apply middle truncation as described in~\\citet{bai2024longbench} for sequences exceeding the model's context window length.\nGiven the complex reasoning required by our test data, we adopt two evaluation settings: zero-shot and zero-shot + CoT. Following~\\citet{rein2023gpqa}, in the CoT setting, the model is first prompted to generate a chain of thought~\\cite{wei2022chain}, after which it is asked to produce the final answer based on the chain of thought.\nFor details on reproducing our results, please refer to Appendix~\\ref{sec:setup}. \nFor a fair comparison, the Qwen2.5 series models are evaluated without YaRN~\\cite{peng2024yarn}. Their performance when combining YaRN are provided in Table~\\ref{tb:exp_yarn}.\nThe code is available at \\url{https://github.com/THUDM/LongBench}.\n\n\\begin{table*}[t]\n\\centering\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{p{5.7cm}|m{0.7cm}m{0.7cm}|m{0.7cm}m{0.7cm}|m{0.7cm}m{0.7cm}|m{0.7cm}m{0.7cm}|m{0.7cm}m{0.7cm}|m{0.7cm}m{0.7cm}}\n\\toprule\n &  & & \\multicolumn{4}{|c}{\\textbf{Difficulty}} & \\multicolumn{6}{|c}{\\textbf{Length (<32k; 32k-128k; >128k)$^\\diamond$}} \\\\\n\\cmidrule(r){1-3} \\cmidrule(lr){4-7} \\cmidrule(l){8-13}\n\\textbf{Model} & \\multicolumn{2}{c|}{\\textbf{Overall}} & \\multicolumn{2}{c|}{\\textbf{Easy}} & \\multicolumn{2}{c|}{\\textbf{Hard}} & \\multicolumn{2}{c|}{\\textbf{Short}} & \\multicolumn{2}{c|}{\\textbf{Medium}} & \\multicolumn{2}{c}{\\textbf{Long}} \\\\ \n\\midrule\n\\multicolumn{13}{l}{\\emph{Open-source models}} \\\\\n\\texttt{GLM-4-9B-Chat} & 30.2 & \\cellcolor{mygray}30.8 & 30.7 & \\cellcolor{mygray}34.4 & 29.9 & \\cellcolor{mygray}28.6 & 33.9 & \\cellcolor{mygray}35.0 & 29.8 & \\cellcolor{mygray}30.2 & 25.0 & \\cellcolor{mygray}25.0 \\\\\n\\texttt{Llama-3.1-8B-Instruct} & 30.0 & \\cellcolor{mygray}30.4 & 30.7 & \\cellcolor{mygray}36.5 & 29.6 & \\cellcolor{mygray}26.7 & 35.0 & \\cellcolor{mygray}34.4 & 27.9 & \\cellcolor{mygray}31.6 & 25.9 & \\cellcolor{mygray}21.3 \\\\\n\\texttt{Llama-3.1-70B-Instruct} & 31.6 & \\cellcolor{mygray}36.2 & 32.3 & \\cellcolor{mygray}35.9 & 31.2 & \\cellcolor{mygray}36.3 & 41.1 & \\cellcolor{mygray}45.0 & 27.4 & \\cellcolor{mygray}34.0 & 24.1 & \\cellcolor{mygray}25.9 \\\\\n\\texttt{Llama-3.3-70B-Instruct} & 29.8 & \\cellcolor{mygray}36.2 & 34.4 & \\cellcolor{mygray}38.0 & 27.0 & \\cellcolor{mygray}35.0 & 36.7 & \\cellcolor{mygray}45.0 & 27.0 & \\cellcolor{mygray}33.0 & 24.1 & \\cellcolor{mygray}27.8 \\\\\n\\texttt{Llama-3.1-Nemotron-70B-Inst.} & 31.0 & \\cellcolor{mygray}35.2 & 32.8 & \\cellcolor{mygray}37.0 & 29.9 & \\cellcolor{mygray}34.1 & 38.3 & \\cellcolor{mygray}46.7 & 27.9 & \\cellcolor{mygray}29.8 & 25.0 & \\cellcolor{mygray}26.9 \\\\\n\\texttt{Qwen2.5-7B-Instruct} & 27.0 & \\cellcolor{mygray}29.8 & 29.2 & \\cellcolor{mygray}30.7 & 25.7 & \\cellcolor{mygray}29.3 & 36.1 & \\cellcolor{mygray}35.6 & 23.7 & \\cellcolor{mygray}26.5 & 18.5 & \\cellcolor{mygray}26.9 \\\\\n\\texttt{Qwen2.5-72B-Instruct} & \\textbf{39.4} & \\cellcolor{mygray}38.8 & \\textbf{43.8} & \\cellcolor{mygray}42.2 & \\textbf{36.7} & \\cellcolor{mygray}\\textbf{36.7} & \\textbf{44.4} & \\cellcolor{mygray}\\textbf{50.0} & \\textbf{34.0} & \\cellcolor{mygray}28.8 & \\textbf{41.7} & \\cellcolor{mygray}\\textbf{39.8} \\\\\n\\texttt{Mistral-Large-Instruct-2407} & 26.6 & \\cellcolor{mygray}33.6 & 29.7 & \\cellcolor{mygray}34.4 & 24.8 & \\cellcolor{mygray}33.1 & 37.8 & \\cellcolor{mygray}41.1 & 19.5 & \\cellcolor{mygray}31.2 & 22.2 & \\cellcolor{mygray}25.9 \\\\\n\\texttt{Mistral-Large-Instruct-2411} & 34.4 & \\cellcolor{mygray}\\textbf{39.6} & 38.0 & \\cellcolor{mygray}\\textbf{43.8} & 32.2 & \\cellcolor{mygray}37.0 & 41.7 & \\cellcolor{mygray}46.1 & 30.7 & \\cellcolor{mygray}\\textbf{34.9} & 29.6 & \\cellcolor{mygray}38.0 \\\\\n\\texttt{c4ai-command-r-plus-08-2024} & 27.8 & \\cellcolor{mygray}31.6 & 30.2 & \\cellcolor{mygray}34.4 & 26.4 & \\cellcolor{mygray}29.9 & 36.7 & \\cellcolor{mygray}39.4 & 23.7 & \\cellcolor{mygray}24.2 & 21.3 & \\cellcolor{mygray}33.3 \\\\ \n\\midrule\n\\multicolumn{13}{l}{\\emph{Proprietary models}} \\\\\n\\texttt{GLM-4-Plus} & 44.3 & \\cellcolor{mygray}46.1 & 47.4 & \\cellcolor{mygray}52.1 & 42.4 & \\cellcolor{mygray}42.4 & 50.0 & \\cellcolor{mygray}53.3 & 46.5 & \\cellcolor{mygray}44.7 & 30.6 & \\cellcolor{mygray}37.0 \\\\\n\\texttt{GPT-4o-mini-2024-07-18} & 29.3 & \\cellcolor{mygray}32.4 & 31.1 & \\cellcolor{mygray}32.6 & 28.2 & \\cellcolor{mygray}32.2 & 31.8 & \\cellcolor{mygray}34.8 & 28.6 & \\cellcolor{mygray}31.6 & 26.2 & \\cellcolor{mygray}29.9 \\\\\n\\texttt{GPT-4o-2024-08-06} & 50.1 & \\cellcolor{mygray}51.2 & 57.4 & \\cellcolor{mygray}57.9 & 45.6 & \\cellcolor{mygray}47.1 & 53.3 & \\cellcolor{mygray}53.9 & 52.4 & \\cellcolor{mygray}\\textbf{50.7} & 40.2 & \\cellcolor{mygray}47.7 \\\\\n\\texttt{GPT-4o-2024-11-20} & 46.0 & \\cellcolor{mygray}51.4 & 50.8 & \\cellcolor{mygray}54.2 & 43.0 & \\cellcolor{mygray}49.7 & 47.5 & \\cellcolor{mygray}59.6 & 47.9 & \\cellcolor{mygray}48.6 & 39.8 & \\cellcolor{mygray}43.5 \\\\\n\\texttt{o1-mini-2024-09-12} & 37.8 & \\cellcolor{mygray}38.9 & 38.9 & \\cellcolor{mygray}42.6 & 37.1 & \\cellcolor{mygray}36.6 & 48.6 & \\cellcolor{mygray}48.9 & 33.3 & \\cellcolor{mygray}32.9 & 28.6 & \\cellcolor{mygray}34.3 \\\\\n\\texttt{o1-preview-2024-09-12} & \\textbf{57.7} & \\cellcolor{mygray}\\textbf{56.2} & \\textbf{66.8} & \\cellcolor{mygray}\\textbf{58.9} & \\textbf{52.1} & \\cellcolor{mygray}\\textbf{54.6} & \\textbf{62.6} & \\cellcolor{mygray}\\textbf{64.6} & \\textbf{53.5} & \\cellcolor{mygray}50.2 & \\textbf{58.1} & \\cellcolor{mygray}\\textbf{54.3} \\\\\n\\texttt{Claude-3.5-Sonnet-20241022} & 41.0 & \\cellcolor{mygray}46.7 & 46.9 & \\cellcolor{mygray}55.2 & 37.3 & \\cellcolor{mygray}41.5 & 46.1 & \\cellcolor{mygray}53.9 & 38.6 & \\cellcolor{mygray}41.9 & 37.0 & \\cellcolor{mygray}44.4\\\\\n\\midrule\n\\cellcolor{mypink}\\emph{Human$^*$} & \\multicolumn{2}{c|}{\\cellcolor{mypink}53.7} & \\multicolumn{2}{c|}{\\cellcolor{mypink}100} & \\multicolumn{2}{c|}{\\cellcolor{mypink}25.1} & \\multicolumn{2}{c|}{\\cellcolor{mypink}47.2} & \\multicolumn{2}{c|}{\\cellcolor{mypink}59.1} & \\multicolumn{2}{c}{\\cellcolor{mypink}53.7} \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\caption{Evaluation results (\\%) on LongBench v2. Results under \\colorbox{mygray}{CoT} prompting are highlighted with a gray background. Note that random guessing yields a baseline score of 25\\%. To account for model responses and human responses that do not yield a valid choice, we report the \\emph{compensated} results in Table~\\ref{tb:exp_comp}, where these cases are counted towards the accuracy with a random probability of 25\\%. $^*$: The human expert's accuracy is based on their performance within a 15-minute time limit, after which they are allowed to respond with ``I don't know the answer''. This occurred for 8\\% of the total test data. $^\\diamond$: Models do not show lower scores on subsets with longer length ranges because the distribution of tasks differs significantly across each length range (Figure~\\ref{fig:length}).}\n\\label{tb:exp}\n\\end{table*}\n\n\\xhdr{Results}\nWe report the evaluation results along with human expert performance in Table~\\ref{tb:exp}. The results under the CoT evaluation setting are highlighted with a gray background, while the highest scores among open-source models and proprietary models are in bold.\nThe results indicate that LongBench v2 presents a significant challenge to the current model---The best-performing o1-preview model achieves only 57.7\\% accuracy, which is 4\\% higher than the performance of human experts under a 15-minute time limit. Additionally, the scaling law effect on our benchmark is striking: smaller models such as GLM-4-9B-Chat, Qwen2.5-7B-Instruct, and GPT-4o-mini perform poorly in our tests that require deep understanding and reasoning over long contexts, with accuracy around 30\\%. In contrast, their larger counterparts like GLM-4-Plus, Qwen2.5-72B-Instruct, and GPT-4o show a notable improvement, achieving overall accuracy around or above 40\\%.\nSimilar to reasoning tasks in mathematics and coding~\\cite{wei2022chain,sprague2024cot,o1-preview}, we also find that incorporating explicit reasoning in the model’s responses significantly improves its performance in our long-context reasoning tests.\nThis includes the use of CoT, which results in an average 3.4\\% improvement for open-source models. Additionally, scaling test-time compute with longer reasoning thought shows further improvements, with o1-preview vs. GPT-4o (+7.6\\%) and o1-mini vs. GPT-4o-mini (+8.5\\%).\nFrom the performance across different length intervals, compared to human, the models perform best on data $<$32k (Short), with the best-performing model surpassing human performance by 15.4\\%. However, even the top model shows a 5.6\\% performance gap compared to human accuracy in the 32k-128k data length range. This highlights the importance of developing methods to maintain strong reasoning capabilities under longer contexts.\n\nTo better distinguish the capability of the models across tasks, we present the performance charts of several representative models across tasks in Figure~\\ref{fig:radar}.\nWe find that the performance gap between LLMs and humans is largest on long structured data understanding tasks, whereas, on single-doc and multi-doc QA tasks, the models perform at par with or even surpass human levels.\nWe hypothesize that this is because the models have seen much more document-type data compared to long structured data during long context training, resulting in poorer understanding of the latter.\nCompared to GPT-4o, we observe that through integrating more thinking steps during inference, o1-preview shows superior performance on multi-doc QA, long in-context learning, and code repository understanding tasks, with a substantial lead over other models.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.9\\linewidth]{figs/radar.pdf}\n    \\caption{Average scores across tasks, normalized by the highest score on each task. All scores are evaluated in the zero-shot + CoT setting, except for o1-preview, since it latently performs CoT under zero-shot prompting.}\n    \\label{fig:radar}\n\\end{figure}\n\n\\subsection{Retrieval-Augmented Baselines}\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figs/curve.pdf}\n    \\caption{RAG performance across different context lengths, varied by including the top 4, 8, 16, 32, 64, 128, and 256 chunks of 512 tokens. The horizontal line show the overall score of each model without RAG at a full context length of 128k tokens.}\n    \\label{fig:rag}\n\\end{figure}\n\nBased on recent studies~\\cite{jiang2024longrag,jin2024long,leng2024long}, we explore incorporating retrieval-augmented generation (RAG,~\\citet{lewis2020retrieval}) into long-context LLM and evaluate its performance on LongBench v2.\nWe first split the long context into chunks of 512 tokens with GLM-4-9B tokenizer. Then, we use \\href{https://open.bigmodel.cn/pricing}{Zhipu Embedding-3} to encode the query, i.e., the concatenation of the question and choices, and the chunks, and sort the chunks based on embedding similarity.\nDuring evaluation, we retrieve the top-$N$ most similar chunks and concatenate them in their original order to form the context input for the model. The model is then prompted to answer the question in a zero-shot setting. For each evaluated model, we take $N = 4, 8, 16, 32, 64, 128, \\text{and}\\ 256$, and the evaluation results form a curve presented in Figure~\\ref{fig:rag}.\n\nWe observe that Qwen2.5 and GLM-4-Plus show no significant improvement as the retrieval context length increases beyond 32k. Both models perform better at a 32k retrieval context length compared to using the entire 128k context window without RAG, with Qwen2.5 showing a notable improvement of +4.1\\%. \nIn contrast, only GPT-4o effectively leverages longer retrieval context lengths, achieving the best RAG performance at 128k, while still lagging behind its overall score without RAG (-0.6\\%).\nThese findings suggest that Qwen2.5 and GLM-4-Plus fall short in effectively utilizing and reasoning with information in context windows longer than 32k compared to GPT-4o.\nIn addition, these experiments also confirm that the questions in LongBench v2 are challenging and cannot be solved solely through retrieval.\n\n\\subsection{Measuring Memorization of Context}\n\n\\begin{table}[t]\n\\centering\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{lccccccc}\n\\toprule\n\\textbf{Model} & \\textbf{Avg} & \\textbf{I} & \\textbf{II} & \\textbf{III} & \\textbf{IV} & \\textbf{V} & \\textbf{VI} \\\\\n\\midrule\n\\cellcolor{mygray}\\texttt{GLM-4-9B-Chat} & \\cellcolor{mygray}30.2 & \\cellcolor{mygray}30.9 & \\cellcolor{mygray}27.2 & \\cellcolor{mygray}33.3 & \\cellcolor{mygray}38.5 & \\cellcolor{mygray}28.0 & \\cellcolor{mygray}24.2 \\\\\n\\quad w/o context & 26.2 & 30.9 & 21.6 & 18.5 & 30.8 & 34.0 & 21.2 \\\\\n\\midrule\n\\cellcolor{mygray}\\texttt{Llama-3.1-8B-Inst.} & \\cellcolor{mygray}30.0 & \\cellcolor{mygray}34.9 & \\cellcolor{mygray}30.4 & \\cellcolor{mygray}23.5 & \\cellcolor{mygray}17.9 & \\cellcolor{mygray}32.0 & \\cellcolor{mygray}30.3 \\\\\n\\quad w/o context & 25.8 & 31.4 & 26.4 & 24.7 & 23.1 & 22.0 & 6.1 \\\\\n\\midrule\n\\cellcolor{mygray}\\texttt{Qwen2.5-72B-Inst.} & \\cellcolor{mygray}39.4 & \\cellcolor{mygray}40.6 & \\cellcolor{mygray}35.2 & \\cellcolor{mygray}42.0 & \\cellcolor{mygray}25.6 & \\cellcolor{mygray}50.0 & \\cellcolor{mygray}42.4 \\\\\n\\quad w/o context & 30.0 & 33.7 & 31.2 & 25.9 & 28.2 & 34.0 & 12.1 \\\\\n\\midrule\n\\cellcolor{mygray}\\texttt{GLM-4-Plus} & \\cellcolor{mygray}44.3 & \\cellcolor{mygray}41.7 & \\cellcolor{mygray}42.4 & \\cellcolor{mygray}46.9 & \\cellcolor{mygray}51.3 & \\cellcolor{mygray}46.0 & \\cellcolor{mygray}48.5 \\\\\n\\quad w/o context & 27.6 & 33.7 & 27.2 & 25.9 & 10.3 & 38.0 & 6.1 \\\\\n\\midrule\n\\cellcolor{mygray}\\texttt{GPT-4o} & \\cellcolor{mygray}50.1 & \\cellcolor{mygray}48.6 & \\cellcolor{mygray}44.0 & \\cellcolor{mygray}58.0 & \\cellcolor{mygray}46.2 & \\cellcolor{mygray}56.0 & \\cellcolor{mygray}51.5 \\\\\n\\quad w/o context & 33.1 & 40.0 & 25.6 & 32.1 & 38.5 & 34.0 & 18.2 \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\caption{Scores (\\%) across 6 tasks: \\emph{I. Single-Doc QA}, \\emph{II. Multi-Doc QA}, \\emph{III. Long ICL}, \\emph{IV. Dialogue History}, \\emph{V. Code Repo}, and \\emph{VI. Structured Data}.}\n\\label{tb:mem}\n\\end{table}\n\nFor an effective long-context benchmark, it is essential to ensure that LLMs cannot rely solely on memorizing previously seen data to answer questions. \nThis necessitates the models to actively read and comprehend the provided long material in order to solve the problems.\nFollowing~\\citet{bai2024longbench}, we also evaluate the models' performance when providing only the questions, without the accompanying long context.\nThe performance comparison between with (w/) and without (w/o) the context is presented in Table~\\ref{tb:mem}.\nAs shown, without context, most models achieve an overall accuracy ranging from 25\\% to 30\\%, which is comparable to random guessing. When comparing scores across different tasks, the memorization effect appears minimal for tasks II, III, and VI.\nThe models perform best without context on tasks I and V, likely because they may have seen some of the documents, novels, or code repositories during training.\n\\section{Conclusion}\nOur work introduces LongBench v2, a challenging multitask benchmark for long-context understanding and reasoning, carefully annotated and reviewed by human experts.\nLongBench v2 presents an equal challenge to both humans and state-of-the-art AI systems, with human performance at 50.1\\% and the best LLM achieving 57.7\\% accuracy, providing a reliable evaluation standard for the development of future superhuman AI systems.\nOur evaluation results also bring forward insights into the impact of scaling inference-time compute and RAG in long-context reasoning.\n\\section{Limitations}\nWe acknowledge certain limitations in our work, which we outline below:\n1. \\textbf{Benchmark size}: The benchmark's size may not be sufficiently large. While this can be seen as an advantage for quick evaluation, it could also lead to less stable results that are more vulnerable to randomness. Due to resource constraints, we are unable to expand the dataset at this time. Collecting the current 503 high-quality samples cost us 100,000 CNY and took more than two months.\n2. \\textbf{Language}: The current dataset is limited to English only. As a result, our benchmark does not yet capture the performance of models across multiple languages.\n3. \\textbf{Length distribution inconsistencies}: The length distribution across different tasks is uneven, with certain tasks concentrated around specific lengths. These differences in task distributions across length ranges make it difficult to provide a fair comparison of a single model's performance across length intervals. We recommend conducting comparisons between models on a per-interval basis. For instance, model A may outperform Model B in the short length range, while model B may outperform model A in the long length range. This would suggest that model B is better at handling longer tasks than model A.\n\n\\section*{Acknowledgements}\nWe would like to express our gratitude to our annotation workers for their dedicated contributions. The authors also extend their thanks to Zijun Yao for his assistance in maintaining the platform, and to Yuze He for his valuable suggestions on the paper.\n\n\\newpage\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2307.11088v3.tex",
        "arXiv-2402.13718v3.tex",
        "arXiv-2412.15204v2.tex"
    ],
    "group_id": "group_18",
    "response": "### Title: Evaluating Long Context Language Models: A Comparative Analysis of L-Eval, InfiniteBench, and LongBench v2\n\n### Introduction\n\nThe field of large language models (LLMs) has seen significant advancements in recent years, with a particular focus on extending the context length of these models to handle more extensive inputs and conversations. This interest stems from the realization that many practical applications, such as document comprehension, agent construction, and complex dialogue management, require models to process and reason over long sequences of text. However, the development of open-source models has lagged behind proprietary models like GPT-4 and Claude, which have demonstrated superior performance in extended context settings. The challenge lies in creating a standardized evaluation framework that accurately reflects the capabilities of these models in real-world scenarios, especially when dealing with long-form documents and complex instructions.\n\nCurrent benchmarks for long context LLMs, such as ZeroSCROLLS, L-Eval, and LongBench, have primarily focused on contexts around 10K tokens, which is insufficient for evaluating the true potential of models designed to handle contexts exceeding 100K tokens. This gap has led to the creation of new benchmarks like InfiniteBench and LongBench v2, which aim to push the boundaries of long context evaluation by incorporating longer documents and more diverse tasks. These benchmarks not only assess the models' ability to process extensive inputs but also evaluate their reasoning and understanding capabilities in a zero-shot setting.\n\nThe history of long context LLMs is marked by initial limitations in handling long sequences due to computational constraints and the inability to generalize beyond training contexts. Recent advancements in training infrastructure and techniques for extending context length have enabled models to process longer inputs, but the evaluation methodologies have not kept pace. This lag has hindered both the comparative analysis of diverse long context LLMs and the identification of potential improvements in their capabilities.\n\n### Main Content of Each Paper\n\n#### Paper 1: L-Eval - Standardized Evaluation for Long Context Language Models\n\n**Overview:**\nL-Eval is a comprehensive benchmark designed to evaluate the performance of long context language models (LCLMs) in processing extensive inputs and reasoning over them. The paper introduces a new evaluation suite that includes 20 sub-tasks, 508 long documents, and over 2,000 human-labeled query-response pairs. The tasks cover a wide range of domains and question styles, ensuring a diverse and challenging testbed for LCLMs.\n\n**Key Innovations:**\n- **Dataset Construction:** L-Eval includes a mix of tasks, both closed-ended (e.g., multiple-choice questions) and open-ended (e.g., summarization tasks), to test different aspects of LCLM performance.\n- **Evaluation Metrics:** The paper proposes length-instruction-enhanced (LIE) evaluation and the use of LLM judges to overcome the limitations of traditional n-gram matching metrics. LIE evaluation involves explicitly instructing the model to generate outputs of a specific length, which helps mitigate the bias introduced by varying output lengths.\n- **Benchmarking Models:** L-Eval evaluates both proprietary and open-source models, highlighting the performance gaps between them. The study reveals that while open-source models show promise in closed-ended tasks, they struggle significantly in open-ended tasks, especially when the context length increases.\n\n#### Paper 2: InfiniteBench - Evaluating LLMs Beyond 100K Tokens\n\n**Overview:**\nInfiniteBench is a benchmark designed to evaluate the performance of LLMs on contexts exceeding 100K tokens. The paper introduces a suite of tasks that require deep understanding and reasoning over long texts, covering domains such as novels, code, math, and dialogue. The tasks are annotated by humans and include both realistic and synthetic scenarios.\n\n**Key Innovations:**\n- **Context Length:** InfiniteBench features an average data length surpassing 100K tokens, making it the first benchmark to evaluate models on such extensive contexts.\n- **Task Design:** The tasks are designed to be challenging and require models to reason over long dependencies rather than simply retrieve information from the context.\n- **Evaluation Methodology:** The paper employs a combination of automated and manual reviews to ensure the quality and difficulty of the test data. It also introduces a mechanism to incentivize annotators to provide high-quality and challenging data.\n\n#### Paper 3: LongBench v2 - Comprehensive Multitask Benchmark for Long Context Understanding\n\n**Overview:**\nLongBench v2 is a benchmark that aims to assess the deep understanding and reasoning capabilities of LLMs over long contexts. It consists of 503 multiple-choice questions, with contexts ranging from 8k to 2M words, and covers six major task categories: single-document QA, multi-document QA, long in-context learning, long-dialogue history understanding, code repository understanding, and long structured data understanding.\n\n**Key Innovations:**\n- **Task Diversity:** LongBench v2 includes a wide range of tasks that reflect real-world scenarios, ensuring a comprehensive evaluation of LLMs.\n- **Data Collection:** The data is collected from real-world documents and code repositories, and undergoes rigorous automated and manual reviews to maintain high quality and difficulty.\n- **Evaluation Settings:** The benchmark evaluates models in both zero-shot and zero-shot + chain-of-thought (CoT) settings, highlighting the importance of explicit reasoning in improving model performance.\n\n### Commonalities and Innovations\n\n**Commonalities:**\n- **Long Context Evaluation:** All three papers focus on evaluating the performance of LLMs in handling long contexts, which is a critical capability for practical applications.\n- **Human Annotation:** Each benchmark involves human annotation to ensure the quality and difficulty of the test data, reflecting real-world scenarios and avoiding trivial tasks.\n- **Comprehensive Task Suite:** The benchmarks cover a wide range of tasks, including QA, summarization, and reasoning, to provide a holistic assessment of LLMs.\n\n**Innovations:**\n- **L-Eval:** Introduces length-instruction-enhanced (LIE) evaluation and the use of LLM judges to provide a more accurate and robust evaluation framework.\n- **InfiniteBench:** Features contexts exceeding 100K tokens and includes synthetic tasks that can be easily scaled to longer lengths.\n- **LongBench v2:** Incorporates a mechanism to incentivize annotators to provide high-quality and challenging data, and evaluates models in both zero-shot and zero-shot + CoT settings.\n\n### Comparison of Results and Discussion\n\nThe results from the three benchmarks highlight the varying performance of LLMs across different tasks and context lengths. L-Eval shows that proprietary models like GPT-4 and Claude outperform open-source models in both closed-ended and open-ended tasks, with a significant gap observed in the latter. InfiniteBench reveals that while models like GPT-4 and Claude perform well on retrieval tasks, they struggle with more complex reasoning tasks over longer contexts. LongBench v2 indicates that incorporating explicit reasoning during inference can significantly improve model performance, with the best model (o1-preview) achieving 57.7\\% accuracy, surpassing human performance by 4\\%.\n\nThe differences in results can be attributed to the varying lengths and complexities of the tasks in each benchmark. L-Eval focuses on a balanced mix of closed-ended and open-ended tasks, with a median input length of 104k tokens. InfiniteBench pushes the boundaries with contexts exceeding 100K tokens, making it the most challenging benchmark. LongBench v2, with its 503 multiple-choice questions and contexts ranging from 8k to 2M words, provides a comprehensive assessment of deep understanding and reasoning capabilities.\n\n### Conclusion\n\nThe main findings from the three benchmarks are that:\n- Proprietary models like GPT-4 and Claude outperform open-source models in handling long contexts.\n- Traditional metrics like n-gram matching are unreliable for evaluating LCLMs, especially in open-ended tasks.\n- Explicit reasoning during inference significantly improves model performance on complex tasks.\n- Models struggle with tasks that require deep understanding and reasoning over longer contexts, indicating a need for further improvements in these areas.\n\nFuture research directions should focus on:\n- Developing more robust and reliable evaluation metrics for LCLMs.\n- Investigating the impact of different training and inference techniques on long context performance.\n- Expanding the benchmarks to include more languages and diverse real-world scenarios.\n- Exploring the potential of retrieval-augmented generation (RAG) to enhance long context reasoning capabilities.\n\n### Limitations and Ethics Statement\n\n**Limitations:**\n- **Benchmark Size:** The benchmarks may not be sufficiently large, which can lead to less stable results and increased vulnerability to randomness.\n- **Language Diversity:** The benchmarks are primarily in English, limiting their applicability to models trained on multilingual data.\n- **Length Distribution:** The distribution of tasks across different lengths is uneven, making it challenging to provide a fair comparison of model performance across length intervals.\n\n**Ethics Statement:**\n- The benchmarks are designed to exclude sensitive content and ensure fairness in evaluation.\n- The use of LLM judges for evaluation is intended to provide a more accurate assessment but can be costly and introduce biases.\n- The benchmarks aim to advance the development of LLMs without influencing their social biases or enabling misuse.\n\n### Acknowledgements\n\nThe authors of these papers acknowledge the contributions of their annotation workers, platform maintainers, and reviewers in ensuring the quality and diversity of the test data. Their efforts have been crucial in creating benchmarks that push the boundaries of LLM capabilities and provide valuable insights for future research.\n\n### Tables and Figures\n\n**Table 1: Comparison of Context Lengths and Task Distributions**\n| Benchmark | Avg Len (tokens) | En | Zh | Code | Math | Novel | Dialogue | Synthetic |\n|------------|------------------|----|-----|------|-------|---------|----------|----------|\n| LRA        | $\\sim$10K        | $\\cmark$ | $\\xmark$ | $\\xmark$ | $\\cmark$ | $\\xmark$ | $\\xmark$ | $\\cmark$ |\n| LongBench  | $\\sim$10K        | $\\cmark$ | $\\cmark$ | $\\cmark$ | $\\xmark$ | $\\cmark$ | $\\cmark$ | $\\cmark$ |\n| L-Eval     | 4K - 60K         | $\\cmark$ | $\\xmark$ | $\\cmark$ | $\\cmark$ | $\\xmark$ | $\\xmark$ | $\\cmark$ |\n| LooGLE     | $\\sim$20K        | $\\cmark$ | $\\xmark$ | $\\xmark$ | $\\xmark$ | $\\xmark$ | $\\cmark$ | $\\xmark$ |\n| InfiniteBench (ours) | $\\sim$200K | $\\cmark$ | $\\cmark$ | $\\cmark$ | $\\cmark$ | $\\cmark$ | $\\cmark$ | $\\cmark$ |\n\n**Table 2: Evaluation Results on LongBench v2**\n| Model | Overall | Easy | Hard | Short | Medium | Long |\n|-------|---------|------|------|-------|--------|------|\n| GLM-4-9B-Chat | 30.2 | 30.7 | 29.9 | 33.9 | 29.8 | 25.0 |\n| Llama-3.1-8B-Instruct | 30.0 | 30.7 | 29.6 | 35.0 | 27.9 | 25.9 |\n| Qwen2.5-7B-Instruct | 27.0 | 29.2 | 25.7 | 36.1 | 23.7 | 18.5 |\n| Qwen2.5-72B-Instruct | **39.4** | **43.8** | **36.7** | **44.4** | **34.0** | **41.7** |\n| GLM-4-Plus | 44.3 | 47.4 | 42.4 | 50.0 | 46.5 | 37.0 |\n| GPT-4o-mini-2024-07-18 | 29.3 | 31.1 | 28.2 | 31.8 | 28.6 | 26.2 |\n| GPT-4o-2024-08-06 | 50.1 | 57.4 | 45.6 | 53.3 | 52.4 | 40.2 |\n| o1-preview-2024-09-12 | **57.7** | **66.8** | **52.1** | **62.6** | **53.5** | **58.1** |\n| Claude-3.5-Sonnet-20241022 | 41.0 | 46.9 | 37.3 | 46.1 | 38.6 | 37.0 |\n| Human$^*$ | 53.7 | 100 | 25.1 | 47.2 | 59.1 | 53.7 |\n\n**Figure 1: Performance of Models Across Different Context Lengths**\n- The figure illustrates the performance of various models on InfiniteBench as a function of input length, showing a decline in performance with longer contexts.\n- The figure also highlights the performance of models on different tasks, such as retrieval, code, and math, indicating that retrieval tasks are generally easier for models to handle compared to reasoning tasks.\n\n**Figure 2: Radar Chart of Model Performance on LongBench v2**\n- The radar chart in LongBench v2 shows the average scores across different task categories, normalized by the highest score on each task.\n- The chart indicates that models like GPT-4o and o1-preview perform well on tasks that require deep understanding and reasoning, while falling short on tasks that involve long structured data.\n\n### Summary of Main Findings\n\nThe three benchmarks—L-Eval, InfiniteBench, and LongBench v2—offer valuable insights into the current state and limitations of long context language models. They highlight the need for standardized evaluation methodologies that can accurately reflect the models' capabilities in real-world scenarios. The benchmarks also emphasize the importance of developing robust metrics and techniques to enhance long context reasoning, such as explicit reasoning during inference and length-instruction-enhanced evaluation. Future research should focus on expanding these benchmarks to include more languages and diverse real-world scenarios, while also exploring the potential of RAG to improve model performance on long contexts."
}