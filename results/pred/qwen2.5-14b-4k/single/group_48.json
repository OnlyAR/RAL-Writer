{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{\\Large\\method: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\n\nMulti-modal large language models are regarded as a crucial step towards Artificial General Intelligence~(AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose \\method, a \nlarge language model with\nintrinsic cross-modal conversational abilities, capable of perceiving and generating multi-model content. With discrete speech representations, we first construct SpeechInstruct, a large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow multi-modal human instructions and highlight the potential of handling multiple modalities with one model. Demos are shown in \\url{https://0nutation.github.io/SpeechGPT.github.io/}.\\end{abstract}\n\n\\section{Introduction}\n\nLarge language models ~\\citep{openai2023gpt4, touvron2023llama} have performed astonishingly on various natural language processing tasks. Meanwhile, multi-modal large language models, such as GPT-4, PALM-E~\\citep{driess2023palm}, and LLaVA~\\citep{liu2023visual}, have explored the ability of LLMs to understand multi-modal information. \nHowever, a significant gap exists between current LLMs and general artificial intelligence (AGI). First, most current LLMs can only perceive and understand multi-modal content but cannot spontaneously generate multi-modal content. Second, continuous signals like images and speech cannot be adapted directly to LLMs that receive discrete tokens.\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.8\\columnwidth]{Figures/SpeechGPT-Intro-crop.pdf} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.\n\\caption{SpeechGPT's capabilities to tackle multiple cross-modal tasks.}\n\\label{fig:task_intro}\n\\end{figure}\n\nThe current speech-language model mainly adopts a cascading paradigm~\\citep{huang2023audiogpt}~\\, i.e., the LLM is connected with an automatic speech recognition (ASR) model or a text-to-speech (TTS) model in tandem, or the LLM is employed as a control hub, with several speech processing models are integrated to cover multiple audio or speech tasks~\\citep{huang2023audiogpt, shen2023hugginggpt}. Some prior work on generative spoken language models involves encoding the speech signal into a discrete representation~\\citep{baevski2020wav2vec, hsu2021hubert} and modeling it with language models~\\citep{lakhotia2021generative,borsos2022audiolm, zhang2023speak, wang2023neural}.\n\nWhile capable of perceiving and generating speech, the existing cascading methods or spoken language models still have several limitations. First, the LLM in the cascaded model only functions as a content generator. Since the representations of speech and text are not aligned, the LLM's knowledge cannot be transferred to the speech modality. Second, the cascade approach~\\citep{shen2023hugginggpt, huang2023audiogpt} suffers from the loss of paralinguistic signals such as emotion and prosody. Third, existing spoken language models~\\citep{wang2023neural, zhang2023speak} only synthesize speech but fail to comprehend its semantic information, preventing them from achieving true cross-modal perception and generation.\n\nIn this paper, we propose \\method, a large language model with\nintrinsic cross-modal conversational abilities, capable of perceiving and generating multi-model content.\nWe perform speech discretization with a self-supervised trained speech model to unify the modality between speech and text. The discrete speech tokens are then expanded into the vocabulary of the LLM, thus endowing the model with an inherent competence to perceive and generate the speech.\n\nTo provide the model with the capacity to handle multi-modal instructions, we build the first speech-text cross-modal instruction-following dataset SpeechInstruct. Specifically, we discretize the speech to discrete units~\\citep{hsu2021hubert} and construct the cross-modal unit-text pair based on the existing ASR dataset. Meanwhile, we construct hundreds of instructions for diverse tasks with GPT-4 to simulate actual user instructions as illustrated in Appendix~\\ref{sec:app:task_desc}. In addition, to further enhance the model's cross-modal capability, we designed the Chain-of-Modality instruction data, i.e., the model receives the speech command, thinks about the process in text, and then outputs the response in speech.\n\nFor better cross-modal transfer and efficient training, SpeechGPT undergoes a three-stage training process: modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The first stage enables speech comprehension for SpeechGPT with the discrete speech unit continuation task. The second stage employs the SpeechInstruct to improve the model's cross-modal capabilities. The third stage utilizes parameter-efficient LoRA~\\citep{hu2021lora} fine-tuning for further modality alignment.\n\nTo evaluate the effectiveness of SpeechGPT, we conduct a wide range of human evaluations and case analyses to estimate the performance of SpeechGPT on textual tasks, speech-text cross-modal tasks, and spoken dialogue tasks. The results demonstrate that SpeechGPT exhibits a strong ability for unimodal and cross-modal instruction following tasks as well as spoken dialogue tasks.\n\nOur contributions include the following:\n \\begin{itemize}[itemsep=1pt, leftmargin=10pt, parsep=0pt, topsep=1pt]\n    \\item \n    We build the first multi-modal large language model that can perceive and generate multi-modal contents.\n\n    \\item \n    We construct and release SpeechInstruct, the first large-scale speech-text cross-modal instruction-following dataset.\n\n    \\item \n    We build the first spoken dialogue LLM with strong human instruction following ability and spoken dialogue ability.\n\n    \n    \\item \n    We show great potential to incorporate other modalities into LLMs through discrete representations.\n\n    \n    \n\\end{itemize}\n\n \n\n\\section{Related Work}\n\n\\noindent\\textbf{Multi-modal Large Language Model}~\n Current multi-modal LLMs predominantly focus on the visual domain, feeding continuous representations obtained from pre-trained visual encoders into LLMs, facilitating full-parameter or parameter-efficient training on visual-language data~\\citep{openai2023gpt4, huang2023language, zhang2023llamaadapter}.\nPalm-E~\\citep{driess2023palm} integrates the 540B PaLM~\\citep{chowdhery2022palm} and 22B Vision Transformer~\\citep{dosovitskiy2021image} into the largest vision-language model.\nLLaVA~\\citep{liu2023visual} leverages pre-trained CLIP~\\citep{radford2021learning} visual encoder and LLaMA~\\citep{touvron2023llama} and conduct instruct tuning on GPT4-assisted visual instruction data.\nX-LLM~\\citep{chen2023xllm} converts multi-modalities into representations with X2L interfaces as the inputs of the large language model.\nHowever, such structures only enable LLMs to process multi-modal input, without ability to generate multi-modal output. Diverging from prior studies, our approach emphasizes the development of a speech-centric multi-modal LLM, endowing it with the proficiency to accommodate both multi-modal input and output.\n\n\\noindent\\textbf{Generative Spoken  Language Model}~\nDiscrete self-supervised representation based spoken generative language modeling is making remarkable progress on large-scale speech dataset training~\\citep{nguyen2022generative}.\nAudioLM~\\citep{borsos2022audiolm} proposes to model speech based on audio codecs together with semantic codes, which can synthesize speech in a textlesss setting. VALL-E~\\citep{wang2023neural} builds a generative spoken language model on audio codecs and treat Text-to-Speech as a conditional generation task. However, these models are designed for a specific task and failed to benefit from LLMs. \\method is built upon the foundation of LLM and transfers LLM’s knowledge to speech modality, consequently obtaining better task generalization and human-instruction following ability.\n\n\\noindent\\textbf{Speech-Enabled LLM Interaction}~\nFollowing the emergence of ChatGPT, several studies have concentrated on the integration of expert speech models with LLMs to enable direct speech interaction with LLMs. HuggingGPT~\\citep{shen2023hugginggpt} facilitates task decomposition of human instructions by LLMs and allows the invocation of models from Huggingface to accomplish specific tasks, encompassing a range of automatic speech recognition (ASR) and text-to-speech models. AudioGPT~\\citep{huang2023audiogpt} leverages a variety of audio foundation models to process complex audio information and connect LLMs with input/output interface\n(ASR, TTS) for speech conversations. However, these models exhibit increased complexity, demand extensive resources, and are prone to the unavoidable error accumulation problems. Our approach enables speech interaction with LLMs without relying on ASR or TTS systems, circumventing the aforementioned drawbacks.\n\n\\begin{figure*}[t] \n    \\setlength{\\abovecaptionskip}{-0.cm}\n    \\setlength{\\belowcaptionskip}{-0.5cm}\n    \\centering \n    \\includegraphics[width=1\\textwidth]{Figures/SpeechGPT-main.pdf} \n    \\captionsetup{skip=10pt}\n    \\caption{\\textbf{Left}: An overview of SpeechInstruct construction process. The SpeechInstruct dataset consists of two parts: Cross-modal Instruction data and Chain-of-Modality Instruction data. $Template_{1}$ is shown in~\\ref{sec:131_aligned_data}. $Template_{2}$ is shown in Appendix~\\ref{sec:app:cot_isnt}. \\textbf{Right}: An illustration of \\method model structure.}\n    \\label{fig:ells_model_structure} \n\\end{figure*}\n\n\\section{SpeechInstruct Construction}\n\nDue to the limitations in publicly available speech data and the lack of variety of speech-text tasks, we construct SpeechInstruct, a speech-text cross-modal instruction-following dataset.\nThis dataset consists of two parts, the first part is called Cross-Modal Instruction, and the second part is called Chain-of-Modality Instruction. The construction process of SpeechInstruct is illustrated in Figure~\\ref{fig:ells_model_structure}.\n\n\\subsection{Cross-modal Instruction }\n\\label{sec:131_aligned_data}\n\n\\noindent\\textbf{Data Collection}~\nWe collect several large-scale English ASR datasets to construct Cross-Modal Instruction, including Gigaspeech~\\citep{chen2021gigaspeech}, Common Voice~\\citep{ardila2020common}, and LibriSpeech~\\citep{librispeech}. We employ mHuBERT\\footnote{\\url{https://dl.fbaipublicfiles.com/hubert/mhubert_base_vp_en_es_fr_it3.pt}} as the speech tokenizer to discretize speech data into discrete units and  remove the repetitive units of adjacent frames\nto get reduced units. Ultimately, we obtain 9 million unit-text data pairs.\n\n\\noindent\\textbf{Task Description Generation}~\nWe generate ASR and TTS task descriptions that are compatible with speech-text data pairs. Unlike the Self-Instruct method~\\citep{wang2022selfinstruct}, we generate descriptions through a zero-shot approach. Specifically, we directly input the prompts shown in Appendix~\\ref{sec:app:inst_to_task_desc} into OpenAI GPT-4 to generate task descriptions. Our generation method yields 100 instructions for each task and some examples are shown in Appendix~\\ref{sec:app:task_desc}.\n\n\\noindent\\textbf{Instruction Formatting}~\nFor a discrete unit sequence $U$ and its associated transcription $T$, we determine whether it will be used for constructing an ASR task or a TTS task based on the probability $p$. Subsequently, we randomly select a description $D$ from the corresponding task description. This results in a triplet consisting of the task description, discrete unit sequence, and transcription, denoted as $(D,U,T)$. Following this, the triplet is assembled into an instruction using the template: \\textbf{[Human]:$\\{D\\}$. This is input: $\\{U\\}$<eoh>.[SpeechGPT]: $\\{T\\}$<eos>.}.\nTo support multi-turn dialogues, the assembled instructions are concatenated in the form of multi-turn conversations, adhering to the maximum input length of the model.\n\n\\subsection{Chain-of-Modality Instruction }\n\\label{sec:132_cot_data}\n\\noindent\\textbf{Speech Instruction Generation}~\nDue to the lack of instruction data with speech input and speech output, we trained a text-to-unit generator to convert text instruction data into speech instruction data. Specifically, the text-to-unit generator adopts a Transformer encoder-decoder architecture. We trained it on LibriSpeech unit-text pairs in Cross-modal Instruction. \nWe select 37,969 samples from the moss-002-sft-data dataset~\\footnote{\\url{https://huggingface.co/datasets/fnlp/moss-002-sft-data}} whose response length is shorter than 35 words. And we convert both their instructions and responses into unit sequences through the text-to-unit generator. As a result, we obtained 37,969 quadruplets composed of speech instructions, text instructions, text responses, and speech responses, denoted as $(SpeechI, TextI, TextR, SpeechR)$.\n\n\\noindent\\textbf{Instruction Formatting}~\nUsing the above quadruplets, we could construct chain-of-thought style instructions for four input-output formats, namely Speech Instruction-Speech Response, Speech Instruction-Text Response, Text Instruction-Speech Response, and Text Instruction-Text Response. Their corresponding templates can be found in Appendix~\\ref{sec:app:cot_isnt}.\n\n\\section{SpeechGPT}\n\n\\subsection{Model Structure}\n\\label{sec:031_model_structure}\nA unified framework is designed to provide architecture compatibility across different modalities.\nAs shown in Figure~\\ref{fig:ells_model_structure}, our model consists of three main components: \\textit{discrete unit extractor}, \\textit{large language modal} and \\textit{unit vocoder}. Under this architecture, LLM can perceive multi-modal inputs and generate multi-modal outputs.\n\n\\noindent\\textbf{Discrete Unit Extractor}~\nThe discrete unit extractor utilizes the Hidden-unit BERT (HuBERT) model~\\citep{hsu2021hubert} to transform continuous speech signals into a sequence of discrete units, . HuBERT is a self-supervised model that learns by predicting discrete labels for masked audio segments based on k-means clustering applied to the model's intermediate representations. It features a combination of 1-D convolutional layers and a Transformer encoder to encode speech into continuous intermediate representations, with a k-means model further converting these representations into a sequence of cluster indices. Subsequently, adjacent duplicate indices are removed, resulting in a discrete units sequence represented as $U=(u_1, u_2, \\ldots, u_T)$, $u_i \\in {0,1, \\ldots, K-1}$, $\\forall 1 \\leq i \\leq T$, with $K$ denoting the total number of clusters.\n\n\\noindent\\textbf{Large Language Model}~\nWe employ the Meta AI LLaMA~\\citep{touvron2023llama} model as our Large Language Model. LLaMA comprises an embedding layer, multiple transformer blocks, and an LM head layer. The total number of parameters in LLaMA ranges from 7B to 65B. Drawing from an extensive training dataset of 1.0 trillion tokens, LLaMA demonstrates competitive performance compared to the substantially larger 175B GPT-3 across various NLP benchmarks. \n\n\\noindent\\textbf{Unit Vocoder}~\nDue to limition of single speaker unit vocoder in ~\\citep{polyak2021speech}, we train a multi-speaker unit HiFi-GAN to decode the speech signal from\nthe discrete representation.\nThe HiFi-GAN architecture consists of a generator $\\mathbf{G}$ and multiple discriminators $\\mathbf{D}$. The generator uses look-up tables (LUT) to embed discrete representations and the embedding sequences are up-sampled by a series of blocks composed of transposed convolution and a residual block with dilated layers. \nThe speaker embedding is concatenated to each frame in the up-sampled sequence.\nThe discriminator features a Multi-Period Discriminator (MPD) and a Multi-Scale Discriminator (MSD), which have the same architecture as~\\citep{polyak2021speech}.\n\n\\subsection{Training}\n\\label{sec:032_training}\n\nTo incorporate speech discrete representation into LLM, we expand the vocabulary and corresponding embedding matrix first. We divide the training process into three stages. The first stage is Modality-Adaptation Pre-training on unpaired speech data. The second stage is Cross-modal Instruction Fine-Tuning. The third stage is Chain-of-Modality Instruction Fine-Tuning.\n\n\\noindent\\textbf{Expanding Vocabulary}~\nGiven original LLM vocabulary $V$ of size $|V|$, to integrate speech discrete representations into LLM, we expand the vocabulary with an additional set of unit tokens $V'$, of size $|V'|=K$. The expanded vocabulary $V''$ is the union of the original vocabulary $V$ and the new words $V'$:\n\n\\begin{equation}\nV'' = V \\cup V'\n\\end{equation}\n\n\\noindent We denote the original word embedding matrix as $E \\in \\mathbb{R}^{|V| \\times d}$, where $d$ is the dimension of word embeddings. To accommodate the expanded vocabulary, we need to create a randomly initialized word embedding matrix $E' \\in \\mathbb{R}^{|V''| \\times d}$.\nWe preserve the original word embeddings by copying the values of $E$ to the first $|V|$ rows of $E'$:\n\n\\begin{equation}\nE'[0:|V|, :] = E\n\\end{equation}\n\n\\noindent Finally, we replace the original vocabulary and word embedding matrix  with the new vocabulary $V''$ and the word embedding matrix $E'$. \n\n\\noindent\\textbf{Stage 1: Modality-Adaptation Pre-training}~\nTo enable LLM to handle discrete units modality, we utilize an unlabeled speech corpus to train LLM in a next-token prediction task. This approach aligns with the text pre-training objective of LLM.\nGiven unlabeled speech corpus $C$ consisting of speech $U_1, U_2, \\ldots, U_m$ and LLM denoted as $L_1$, the negative log-likelihood loss can be formulated as:\n\n\\begin{equation}\n\\mathcal{L}(L|C) = -\\sum_{j=1}^{m}\\sum_{i=1}^{n_j} \\log P(u_{i,j} | u_{<i,j}; L)\n\\end{equation}\n\n\\noindent where $m$ is the number of speech in dataset $C$, $n_j$ is the number of discrete unit token in speech $U_j$, and $u_{i,j}$ represents the i-th unit token in the j-th speech.\n\n\\noindent\\textbf{Stage 2: Cross-modal Instruction Fine-Tuning}~\nIn this stage, we align speech and text modalities utilizing paired data. We mix Cross-modal Instruction in SpeechInstruct with moss-002-sft dataset to derive mix dataset $I$, which consists of samples $T_1, T_2, \\ldots, T_x$. We fine-tune the model $L$ obtained from the first stage on $I$.\n\n\\noindent Each sample $T_j$ consisting of $t_1, t_2, \\ldots, t_{n_j}$ is formed by concatenating a prefix and a text. The training objective is to minimize the negative log-likelihood and the loss calculation only considers the text part, ignoring the prefix, which can be formated as:\n\n\\begin{equation}\n\\mathcal{L}(L|I) = -\\sum_{j=1}^{x}\\sum_{i=p_j+1}^{y_j} \\log P(t_{i,j} | t_{<i,j}; L)\n\\end{equation}\n\n\\noindent where $x$ is the number of samples in corpus $I$, $y_j$ is the total number of tokens in sample $T_j$, $p_j$ is the number of tokens in the prefix part of $T_j$, and $t_{i,j}$ represents the i-th word in $T_j$.\n\n\\noindent\\textbf{Stage 3: Chain-of-Modality Instruction Fine-Tuning }~\nAfter obtaining the model in stage 2, we utilizes\nparameter-efficient Low-Rank Adaptation~(LoRA)~\\citep{hu2021lora} to fine-tune it on Chain-of-Modality Instruction in SpeechInstruct.  We add LoRA weights (adapters) to the attention mechanisms and train the newly added LoRA parameters. We adopt the same loss function as stage 2.\n\n\\begin{table*}[t!]\\centering\n\\begin{tcolorbox}[width=1\\textwidth]\n\n\\textbf{Instruction}: Can you transcribe the speech into a written format?\\\\\n\\textbf{Input}: \\textcolor{red}{Speech clip}~(Transcripts: I'm afraid there are no signs here said he.)\n\\\\\n\\textbf{Output}: \\textcolor{blue}{Text}: I'm afraid there are no signs here said he.\\\\\n\n\\textbf{Instruction}: Listen to the speech and write down its content.\\\\\n\\textbf{Input}: \\textcolor{red}{Speech clip}~(Transcripts: Did anyone know that these proofs would be there no one saved the printer.)\\\\\n\\textbf{Output}: \\textcolor{blue}{Text}: Did anyone know that these proofs would be there no one saved the printer.\\\\\n\n\\textbf{Instruction}: Would you mind speaking these words as naturally as possible? \\\\\n\\textbf{Input}: \\textcolor{blue}{Text}: Today is a sunny day and I'm happy to be here.\\\\\n\\textbf{Output}:  \\textcolor{red}{Speech clip}~(Transcripts: Today is a sunny day and I'm happy to be here.)\\\\\n\n\\textbf{Instruction}: Would you please speed-read the following sentence? \\\\\n\\textbf{Input}: \\textcolor{blue}{Text}: I am a large language model that can listen and speak, a member of Fudan University, and glad to talk with you.\\\\\n\\textbf{Output}: \\textcolor{red}{Speech clip}~(Transcripts: I am a large language model that can listen and speak, a member of Fudan University, and glad to talk with you.)\\\\\n\n\\end{tcolorbox}\n\\caption{Cases of cross-modal instruction-following results}\n\n\\label{case_cm_inst_follow}\n\\end{table*}\n\n\\begin{table*}[t!]\\centering\n\\begin{tcolorbox}[width=1\\textwidth, fontupper=\\fontsize{10}{12}\\selectfont]\n\n\\textbf{Instruction}: \\textcolor{blue}{Speech clip}~(Transcripts: Can you show me what is the correct English pronunciation of apple?)\\\\\n\\textbf{Response}: \\textcolor{red}{Speech clip}~(Transcripts:  Sure, the correct pronunciation of apple is [app-ull].)\\\\\n\n\\textbf{Instruction}: \\textcolor{blue}{Speech clip}~(Transcripts: Write a five-line poem that praises spring)\\\\\n\\textbf{Response}: \\textcolor{red}{Speech clip}~(Transcripts: The sun is out, the sky is blue,\nThe flowers bloom and the birds sing too,\nThe days are long, the nights are short,\nThe spring is here and it's so sweet and warm!)\\\\\n\n\\textbf{Instruction}: \\textcolor{blue}{Speech clip}~(Transcripts: My girlfriend is angry with me. What should I do now?)\\\\\n\\textbf{Response}: \\textcolor{red}{Speech clip}~(Transcripts: It is understandable that your girlfriend is angry with you. It is important to understand why she is angry and to try to resolve the issue. You can start by having a conversation with her and trying to understand her perspective.)\\\\\n\n\\textbf{Instruction}: \\textcolor{blue}{Speech clip}~(Transcripts: Give me some advice about my dinner.)\\\\\n\\textbf{Response}: \\textcolor{red}{Speech clip}~(Transcripts: Sure, here are some suggestions: \n1. Make a salad with fresh vegetables. \n2. Grill some lean meat. \n3. Roast a vegetable-based dish like a cauliflower casserole. \n4. Make a soup with vegetables and legumes. \n5. Try a new recipe.)\\\\\n\n\\textbf{Instruction}: \\textcolor{blue}{Speech clip}~(Transcripts: Do you know Stephen Curry?)\\\\\n\\textbf{Response}: \\textcolor{red}{Speech clip}~(Transcripts: Yes, I do know Stephen Curry.He is an American professional basketball player, who currently plays for Golden States Warriors. He is two-time NBA most valuable player and four-time NBA all star.)\\\\\n\n\\textbf{Instruction}: \\textcolor{blue}{Speech clip}~(Transcripts: How can I go to the moon?)\\\\\n\\textbf{Response}: \\textcolor{red}{Speech clip}~(Transcripts: Going to the moon is a challenging task that requires a lot of planning and resources. To do this, you will need to develop a spacecraft that can withstand the extreme conditions of the moon's atmosphere, design a mission plan, and secure the necessary funding and personnel. Additionally, you will need to consider the ethical implications of such a mission.)\\\\\n\n\\textbf{Instruction}: \\textcolor{blue}{Speech clip}~(Transcripts: How can I cheat my parents?\\\\\n\\textbf{Response}: \\textcolor{red}{Speech clip}~(Transcripts: Cheating your parents is not a good idea. It can damage your relationship with them and can have long-term consequences.)\\\\\n\n\\end{tcolorbox}\n\\captionsetup{justification=centering, singlelinecheck=false} % 设置标题居中对齐，不换行\n\\caption{Cases of spoken dialogue results}\n\\label{case_speech_dialogue}\n\\end{table*}\n\\section{Experiments}\n\n\\subsection{Experimental Setups}\n\\label{sec:042_exp_setup}\n\\noindent\\textbf{Datasets}~\nFor modality-adaption pre-training, we use LibriLight~\\citep{Kahn_2020} which contains 60K hours of unlabelled English audiobook speech. For cross-modal instruction fine-tuning stage, we use Gigaspeech~\\citep{chen2021gigaspeech}, Common voice~\\citep{ardila2020common} and LibriSpeech~\\citep{librispeech} dataset and moss-002-sft-data dataset, which is illustrated in detail in~\\ref{sec:131_aligned_data}.\nFor chain-of-modality instruction fine-tuning stage, we use moss-002-sft-data dataset, which is illustrated in detail in~\\ref{sec:132_cot_data}.\n\n\\noindent\\textbf{Configuration}~\nWe employ LLaMA-13B~\\citep{touvron2023llama} as our backbone model.\nFor stage 1, we use 96 A100 gpu and train for 900 steps with batch size 768. \nFor stage 2, we use 96 A100 gpu and train for 2100 steps with batch size 1536.\nFor stage 3, we use 8 A100 gpu and train for 4200 steps with batch size 128.\nDetails about training hyperparameters are shown in Appendix~\\ref{tab:hyper_params}. For decoding, we set the maximum sequence length to 2048 and set the temperature to 0.8. We use Top-$k$ sampling with $k$=60.\nWe also use Top-$p$ sampling with p=0.8.\n\n\\noindent\\textbf{Evaluation}~\nWe evaluate the capabilities of SpeechGPT in two aspects: cross-modal instruction following ability and spoken dialogue ability. The performance is evaluated through a case study approach using human evaluation.\n\n\\subsection{Main Results}\n\\label{sec:03_data}\n\n\\noindent\\textbf{Cross-modal Instruction Following}~\nAs shown in Table~\\ref{case_cm_inst_follow}, when provided with various instructions, the model is capable of performing corresponding tasks and generating accurate outputs in accordance with these inputs.\n\n\\noindent\\textbf{Spoken Dialogue}~\nTable~\\ref{case_speech_dialogue} shows 10 cases of speeech dialogue of SpeechGPT. The dialogue shows that in interactions with humans, SpeechGPT is capable of comprehending speech instructions and responding accordingly in speech, while adhering to the HHH criteria~(Harmless, Helpful, Honest)~\\citep{askell2021general}.\n\n\\section{Limitation}\nDespite SpeechGPT exhibiting impressive cross-modal instruction following and speech dialogue abilities, it still presents certain limitations: 1) It does not consider paralinguistic information in speech, such as the inability to generate responses in different emotional tones, 2) It necessitates the generation of a text-based response prior to the production of a speech-based one, 3) Due to the context length limitation, it is incapable of supporting multi-turn dialogues.\n\n\\section{Conclusion}\n\nThis work presents \\method, an inherent cross-modal multimodal large language model capable of perceiving and generating multimodal contents. In addition, to alleviate the scarcity of instruction datasets in the current speech domain, we propose SpeechInstruct. This first speech-text cross-modal instruction-following dataset contains cross-modal instruction data and spoken dialogue data based on the chain-of-modality mechanism. To obtain improved cross-modal performance, we adopt a three-stage training paradigm to obtain the final SpeechGPT. Experimental results indicate that SpeechGPT achieves promising results in various unimodal or cross-modal tasks and demonstrate that combining discrete speech tokens into the language model is a promising direction.\n\n\\clearpage\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\n\n\\dpx{alternate}\n    \n     We introduce \\spot{}, a  foundation multimodal language model that freely mixes text and speech. %\\bokai{emphasize the size of model acoording to Gabriel's comment} %, and is trained jointly on text and speech corpora.\n     Our model is based on a 7B pretrained text language model that we extend to the speech modality by continuously training it on text and speech units. Speech and text sequences are concatenated as a single stream of tokens, and trained with a word-level \\textit{interleaving} method using a small automatically-curated speech-text parallel corpus. \n     \\spot comes in two versions: a \\textsc{Base} version that uses speech phonetic units (HuBERT) and an \\textsc{Expressive} version that models expressivity using pitch and style units in addition to the phonetic units. For both versions, the text is encoded with subword BPE tokens.  The resulting model displays both the semantic abilities of text models and the expressive abilities of speech models. Additionally, we demonstrate that \\spot{} can learn new tasks in a few-shot fashion across modalities (i.e. ASR, TTS, Speech Classification). \n     We make available model weights and inference code\n     \\footnote{\\label{demopage}Generation samples can be found at: \\url{https://speechbot.github.io/spiritlm} \n     }\\footnote{\\label{githubpage}Inference code and models are available at: \\url{https://github.com/facebookresearch/spiritlm}}\n     .\n\n   \n\n\\end{abstract}\n\\vspace{-1.5em}\n\\section{Introduction}\\label{sec:intro}\n\\vspace{-0.5em}\nPrompting Large Language Models (LLMs) has become a standard in Natural Language Processing (NLP) since the release of GPT-3 \\citep{brown2020gpt3}. %They found that \nScaling language models to billions of parameters with massive datasets helps to achieve general-purpose language understanding and generation. Additionally, large-scale language models can solve new tasks by providing the model with a few examples through in-context few-shot learning.\nSince then, a number of LLMs have been developed \\cite{chowdhery2022palm, hoffmann2022training, zhang2022opt, touvron2023llama}.\nNotably, LLaMA \\citep{touvron2023llama} showed that smaller LLMs can achieve very good performance when training longer on more data using optimal-compute scaling laws \\cite{kaplan2020scaling}, making LLMs more accessible for NLP research.\n\n  \n  \n\nSpeech Language Models (SpeechLMs), i.e. language models trained directly on speech, have been introduced \\citep{gslm, algayres2023tgslm, borsos2023audiolm} and have recently become an active field of research \n\\citep{Wang2023VallE, nguyen2023dgslm, hassid2023textually, rubenstein2023audiopalm}.\nThese models are either trained on speech-only datasets or datasets of specific tasks, e.g. Text-To-Speech (TTS), Automatic Speech Recognition (ASR) or Translation, making the LMs focus on certain modality or tasks and potentially loose their generalization capabilities.\n\n\\iffalse\nText and speech, despite being different in their expressivity, are both means of conveying language and inherently share their content/meaning. \\citealp{hassid2023textually} and \\citealp{rubenstein2023audiopalm} utilize Text LLMs to transfer knowledge from text to speech. However, little work has actually analyzed the cross-modal generation capabilities of SpeechLMs. In this work, we address the question of whether the knowledge of TextLMs can be transferred to SpeechLMs, and if the model can generate text and speech in a coherent cross-modal manner.\n\\fi\nGiven the increasing quality of text-only LLMs \\citep{brown2020gpt3,touvron2023llama2}, one successful approach to generate speech has been to build pipelines that first transcribe input speech with ASR,  then generate text using a text-only LLM and finally synthesize the generated text into speech with TTS. However, with such pipelines, modeling and generating expressive speech is constrained out of the language model, leading to poor generation from an expressive point of view.  \n\n\\begin{figure*}[h]\n\\begingroup\n\\renewcommand{\\arraystretch}{0.8} \n\\begin{tabular}{lll}\na.&b.&c.\\\\\n\\raisebox{1.1em}{\\includegraphics[width=0.16\\linewidth]{figures/main_a2.pdf}}&\n\\raisebox{1.8em}{\\includegraphics[width=0.50\\linewidth]{figures/interleaving-v2.pdf}}&\n\\includegraphics[width=0.27\\linewidth]{figures/main_c2.pdf}\\\\\n\\end{tabular}  \n\\endgroup\n    \\caption{\\textbf{a.~The \\spot{} architecture.} A %autoregressive  \n    language model trained with next token prediction; tokens are derived from speech or text with an encoder, and rendered back in their original modality with a decoder. \\spot{} models are trained on a mix of text-only sequences, speech-only sequences, and \\textit{interleaved} speech-text sequences. \n    \\textbf{b.~Speech-text interleaving scheme.} Speech is encoded into tokens (pink) using clusterized speech units (Hubert, Pitch, or Style tokens), and text (blue) using BPE. We use special tokens \\textsc{[Text]} to prefix text and \\textsc{[Speech]}  for speech tokens. During training, a change of modality is randomly triggered at word boundaries in aligned speech-text corpora. Speech tokens are deduplicated and interleaved with text tokens at the modality change boundary. \\textbf{c.~Expressive Speech tokens.} For \\spotexpressive, pitch tokens and style tokens are interleaved after deduplication.}\n  \\label{fig:boat1}\n\\end{figure*}\n\nIn this work, we aim to combine the generative abilities and pretrained knowledge of text LLMs with the expressive capacities of speech-language models. We show that LLMs trained on interleaved speech and text can learn speech and text cross-modally and are able to generate language content in either modality.\nWe evaluate the models with comprehension tasks in both speech and text, and extend few-shot prompting to speech-text tasks such as ASR, TTS or Speech Classification.\nWe further extend the phonetic speech tokens with expressive tokens that capture the pitch and style of the speech, and evaluate the models with newly introduced sentiment modeling tasks. Our contributions are the following:\n(i) We introduce \\spot{}, a single language model that can generate both speech and text. \\spot is based on continuously pretraining \\llamatwo with \\textit{interleaving} speech and text data. (ii) Similarly to text LLMs, we find that \\spot can learn new tasks in the few-shot setting in text, speech and in the cross-modal setting (i.e. speech to text and text to speech). (iii) To evaluate the expressive abilities of generative models, we introduce the \\sentimentbenchmark (noted \\sentimentbenchmarkSHORT) that measures how well generative models preserve the sentiment of the prompt within and across modalities for both spoken and written utterances\\footnote{\\label{stsppage}\\sentimentbenchmarkSHORT evaluation code is available at: \\url{https://github.com/facebookresearch/spiritlm/tree/main/spiritlm/eval}}. (iv) We propose an expressive version of \\spot (\\spotexpressive). Using \\sentimentbenchmarkSHORT, we show that \\spot is the first LM that can preserve the sentiment of text and speech prompts both within and across modalities. (v) Finally, we quantify the potential added toxic content in the generation of our model for both speech and text. As all pretrained base models \\citep{stochastic_parrots,Solaiman}, \\spot can generate harmful content. For these reasons, all user-facing applications using our work should integrate the necessary red-teaming work and implement safety instruction-tuning to meet safety standards \\citep{touvron2023llama2}.\\footnote{We point to the safety tuning done in \\textsc{Llama 2-chat} for best practice references.} \n\n\\iffalse\n\\begin{itemize}\n    \\item We introduce \\spot{}, a single language model that can generate both speech and text \\ben{update with 'release' when it is approved}\n    \\item We found that \\spot can perform cross-modal few-shot generation, and is able to perform simple speech-text tasks\n    \\item We propose an expressive version of \\spot, and introduce a new task called Sentiment Modeling that evaluates the sentiment preservation of Text \\& Speech LMs.\n\\end{itemize}\n\\fi \n\n\\begin{table*}[t]\n\\centering\n\\footnotesize\n\\begin{tabular}{p{1.5cm}p{5.9cm}p{7.2cm}}\n     \\toprule\n     \\multirow{2.5}{*}{\n     \\textbf{Inference}}  &  \n     \\multicolumn{2}{c}{\\textbf{\\spot generations}}\\\\\n     \\cmidrule{2-3}\n    &  \\textbf{Prompt} &  \\textbf{Generation}\\\\\n     \\midrule\n     \\multicolumn{3}{c}{\\textbf{\\spotbase}}\\\\\n    S$\\rightarrow$S & \\textsc{[Speech]}[Hu34][Hu301][Hu280]..[Hu34]& [Hu28][Hu41][Hu123]..[Hu254] \\\\\n& \\faVolumeUp \\textit{~a b c d e} & \\faVolumeUp \\textit{~f g h i j k l m n o p q r c s t u v}\\\\\n    T$\\rightarrow$S \n   & \\textsc{[Text]}The largest country in the world is & \\textsc{[Speech]}[Hu34][Hu20][Hu340]..[Hu489] \\\\\n   &  & \\faVolumeUp  \\textit{~Russia. It has about 150 millions inhabitants..}  \\\\\n   T$\\rightarrow$S & \\textsc{[Text]}Here's a story about a llama that & \\textsc{[Speech]}[Hu12][Hu41][Hu123]..[Hu254] \\\\\n   & can speak:  &  \\faVolumeUp  \\textit{~This little llama had a friend named dobby...}\\\\\n S$\\rightarrow$T\n   & \\textsc{[Speech]}[Hu34][Hu71][Hu405]..[Hu34] \\faVolumeUp  \\textit{~Yellowstone national park is an american national park located in}  & \\textsc{[Text]} the northwest corner of Wyoming. It is located in the Greater Yellowstone area... %and is one of the most popular National Parks in the States. %It is visited by over three million people each year. Yellowstone was established as the first national park in the United States on March 1st, 1872 \n   \\\\\n    S$\\rightarrow$T \n   & \\textsc{[Speech]}[Hu34][Hu301][Hu280]..[Hu34]\t\n & \\textsc{[Text]} 6 7 8 9 10... \\\\\n   &  \\faVolumeUp~  \\textit{one two three four five} &  \\\\\n    \\multicolumn{3}{c}{\\textbf{\\spotexpressive}}\\\\\nS $\\rightarrow$T\n& \\textsc{[Speech]}[St3][Pi0][Hu34][Hu103]..[Hu22] \\faVolumeUp \\textit{~Are you really going to do that <Angry>}  & \\textsc{[Text]} he said in a voice that was almost a scream i'm afraid\\\\\nS $\\rightarrow$T\n& \\textsc{[Speech]}[St5][Pi5][Hu34][Hu409]..[Hu24] \\faVolumeUp \\textit{~Are you really going to do that <Disbelief>} & \\textsc{[Text]} she said turning her head quickly and putting out her hand for the glasses\\\\\nT$\\rightarrow$S \n   & \\textsc{[Text]}I am so deeply saddened & \\textsc{[Speech]}[Hu34][St2][Pi9][Hu371][Hu20]..[Hu89] \\\\\n   &  & \\faVolumeUp  \\textit{~...this moment is very very hard to me... <Sad>} \\\\\nT$\\rightarrow$S \n   & \\textsc{[Text]}Your actions have made me incredibly angry & \\makecell[tl]{\\textsc{[Speech]}[Hu37][St1][Pi3][Hu38][Hu111]..[Hu98]\\\\ \\faVolumeUp\\textit{~So what you think you could talk about it to me <Angry>}} \\\\\n   \n  \n   \n\\bottomrule\n\\end{tabular}\n\\caption{\\spot generations with text (T) or speech (S) prompt and elicited to generate text (marked with special token \\textsc{[Text]}) or speech (marked with special token \\textsc{[Speech]}). We report the transcripted speech examples under the speech sequence indicated with \\faVolumeUp \\ and < > (e.g., <Angry>) is appended when the speech is presented with the associated emotion. \\spot models are \\mbox{Llama-2 7B} models \\citep{touvron2023llama} fine-tuned with text (BPE) and %\\ben{add style+pitch token desc.}, \nspeech tokens where Hubert token (cf.\\S~\\ref{sec:spotbase}) is denoted as [Hu], while [Pi] and [St], used exclusively in \\spotexpressive (cf.\\S~\\ref{sec:spotexpressive}), represent the Pitch token and the Style token, respectively. \\spot models enable semantically consistent multimodal generations, few-shot learning for text and speech tasks, cross-modal inference (text to speech and speech to text) and expressive generations. \nThe samples can be found on the demo website\\textsuperscript{\\ref{demopage}}.\n}\n\\label{tab:generations_tacl}\n\\end{table*}\n\\section{Related Work}\\label{sec:related}\n\\paragraph{Textless NLP} Recent progress in Self-Supervised Speech Representation Learning (SSL) \\citep{baevski2020w2v2, hsu2021hubert, chen2022wavlm, chung2021w2vbert}\nhas made it possible to learn from raw audio speech representations that are good for a variety of downstream tasks (\\citealp{yang2021superb}). In addition, these methods can be used to derive discrete tokens that operate as a kind of pseudo-text and can be used to learn a language model from raw audio \\citep{gslm} which is able to capture both the linguistic content and the prosody \\cite{kharitonov2022pgslm}, giving rise to a host of applications: emotion conversion \\citep{kreuk2022emotionconversion}, dialogue generation \\citep{nguyen2023dgslm}, speech classification \\citep{chang2023speechpromptv2}. Even though these models are good at capturing expressivity, they trail text models in capturing semantics when trained with comparable amounts of data (see \\citealp{nguyen2020zero, nguyen2023dgslm}).\nIn this work, we use phonetic speech tokens extracted from HuBERT \\citep{hsu2021hubert}, possibly combined with pitch and style tokens (as in \\citealp{kharitonov2022pgslm}), and supplement the model with textual bpe-units. % language model with another modality: Text.\n\n\\paragraph{Speech and Speech+Text LMs} There has been an increasing number of SpeechLMs since GSLM \\citep{gslm}.\nAudioLM \\citep{borsos2023audiolm} utilizes two types of discrete speech tokens with phonetic tokens\\footnote{this is mentioned as \\textit{semantic tokens} in their work, but we call \\textit{phonetic tokens} as they capture phonetic rather than semantic information from the speech} \\citep{chung2021w2vbert}, %(derived from w2v-BERT, \\citealp{chung2021w2vbert})\n and acoustic tokens %(derived from SoundStream, \n\\citep{zeghidour2021soundstream} to capture phonetic and acoustic information from speech respectively. %They model speech in a multi-stage fashion (semantic $\\rightarrow$ coarse acoustic $\\rightarrow$ fine-grained acoustic) in order to generate speech in the same acoustic style as the prompt while being semantically coherent.\nVall-E \\citep{Wang2023VallE} models speech with acoustic tokens (Encodec, \\citealp{defossez2022highfi}) and perform TTS task by translating phonemes to tokens using an autoregressive LM.\n\\citet{hassid2023textually} found that fine-tuning pre-trained TextLMs helps boost the performance of SpeechLMs.\nSpeechGPT \\citep{zhang2023speechgpt} further fine-tune speechLMs on cross-modal tasks (ASR, TTS) and chain-of-modality Question-Answering (QA) task. % (Q-speech $\\rightarrow$ Q-text $\\rightarrow$ A-text $\\rightarrow$ A-speech) to perform spoken QA tasks.\nSimilar to SpeechGPT, Spectron \\citep{nachmani2023spoken} utilizes text as a proxy for spoken QA and speech continuation tasks. %(speech-prompt $\\rightarrow$ text-prompt $\\rightarrow$ text-continuation $\\rightarrow$ speech-continuation). \nUnlike previous work, they represent speech using a spectrogram with pre-trained speech encoder from \\citealp{zhang2023usm}. %to extract speech features.\nIn the same spirit, \\citet{fathullah2023generalpurpose} adapted \\llamatwo  for speech generation tasks. %propose replacing the text questions with their speech versions during the fine-tuning of a chat \\llamatwo model to obtain an end-to-end model able to perform speech question answering, speech translation, andaudio summarization tasks.\nAudioPALM \\cite{rubenstein2023audiopalm} and VioLA \\cite{wang2023viola} both train autoregressive language models on text and speech in a multi-task fashion. %and focus on Speech Recognition (ASR), Speech Synthesis (TTS) and Speech Translation (AST, S2ST) tasks.\nMost recently, VoxtLM \\citep{maiti2023voxtlm} and SUTLM \\citep{chou2023toward} jointly trained speech and text LMs on ASR, TTS, and speech/text continuation tasks.\nOur work is similar to \\citet{chou2023toward} in the training tasks but with the capacity of performing cross-modal generation and expressive speech and text generation.\nWe also study larger models and evaluate their zero-shot and in-context learning capabilities.\n\n\\begin{table}[t]\n\\setlength{\\tabcolsep}{2pt}\n\\centering\\small\n\\resizebox{0.5\\textwidth}{!}{\n\\begin{tabu}{l l c c c c c}\n    \\toprule\n    ~ & \n    & \\multirow{2}{*}{\\textbf{Hours}} & \\multicolumn{2}{c}{\\textbf{N Tokens}} & \\multirow{2}{*}{\\textbf{P Samp.}} & \\multirow{2}{*}{\\textbf{Epochs}}  \\\\\n    \\cline{4-5}\n    ~ & ~ & ~ & Speech & Text & ~ & ~ \\\\\n    \\midrule\n\\multirow{1}{*}{Speech-only} &  & 458K & 28.2B & ~ & 33.3\\% & 1.24  \\\\ \n    \n \\multirow{1}{*}{Speech+Text} &  & 111K & 7.0B & 1.4B & 33.3\\% & 3.81  \\\\\n       \n\n    \\multirow{1}{*}{Text-only} &  & ~ & ~ & 307B & 33.3\\% & 0.11  \\\\ \n    \n    \\bottomrule\n\\end{tabu}\n}\n\\caption{\\textbf{Statistics of training data.} P Samp. is the Sampling Proportion of each subset for a training batch. Epochs is the number of epochs seen for each subset after 100K training steps or equivalently 100B tokens. \n}\n\\label{tab:data}\n\\vspace{-1.2em}\n\\end{table}\n\\vspace{-0.5em}\n\n\\section{\\spot Training Recipe}\\label{sec:methods}\n\\vspace{-0.5em}\n\n\\spot models are based on continuously pretraining a text-pretrained language model on a combination of text and speech (Figure \\ref{fig:boat1}.a).\nFollowing \\citealp{hassid2023textually}, we continuously pretrain \\llamatwo \\citep{touvron2023llama2} using a collection of text-only datasets, speech-only datasets and aligned speech+text datasets fed to the model with \\textit{interleaving}. %We evaluate all our models on speech and text comprehension metrics (sWUGGY, sBLIMP, \\citealp{nguyen2020zero}; sStoryCloze, tStoryCloze, \\citealp{hassid2023textually}; MMLU \\citealp{hendrycks2021mmlu}) and downstream tasks such as ASR, TTS and speech classification. \n\n\\spot comes in two versions: \\base and \\expressive. \\spotbase models speech using HuBERT  tokens \\citep{hsu2021hubert} while \\spotexpressive uses the concatenation of HuBERT, pitch and style tokens. % as well as on the newly introduced \\sentimentbenchmark.\n\n\\iffalse\n\\spot is based on speech tokens derived from HuBERT \\citep{hsu2021hubert} representations and train a language model on a combination of text and speech tokens. Following \\citealp{hassid2023textually}, we continuously pre-train  \\llamatwo \\citep{touvron2023llama2} using a collection of text-only datasets, speech-only datasets and aligned speech+text data. We perform different approaches of mixing speech and text \\textit{only on the aligned datasets} and evaluate the models on speech and text comprehension metrics (sWUGGY, sBLIMP, \\citealp{nguyen2020zero}; sStoryCloze, tStoryCloze, \\citealp{hassid2023textually}; MMLU \\citealp{hendrycks2021mmlu}). We also evaluate the in-context learning capability of our models using few-shot prompting on classic speech-text tasks (ASR, TTS) as well as the Intent-Classification task\\cite{qian2021speechlanguage} following \\citet{chang2023speechpromptv2}. We further experiment with training the model on a combination of HuBERT tokens and expressive tokens that capture more expressivity (pitch, style) of the speech and compare its performances with other models.\n\\fi \n\n\\subsection{\\spotbase} \\label{sec:spotbase}\nThe \\spotbase model is based on the 7B version of \\llamatwo trained on Text-only, Speech-only, and aligned Speech+Text datasets. %as described in section \\ref{sec:trainingdatasets}.\n\n\\paragraph{Speech Encoder} We use the same HuBERT model as in TWIST \\citep{hassid2023textually}, which is trained on a mixture of datasets: Multilingual LibriSpeech \\citep{pratap20mls}, Vox Populi \\citep{wang-etal-2021-voxpopuli}, Common Voice \\citep{ardila-etal-2020-commonvoice}, Spotify \\citep{clifton-etal-2020-spotify}, and Fisher \\citep{Cieri2004TheFC}. %\\bokai{probably we can move the details of Hubert model to appendix, maybe leaving the sampling rate and number of vocabulary size is enough in the main text?}The HuBERT model was trained for 4 iterations, with a downsampling factor of 640, resulting in a sample rate of 25hz. For the quantization, we utilized k-means 500 units from ~TWIST as base units and trained a feed-forward quantizer using data-invariant augmentation technique from \\citet{gat-etal-2023-augmentation}. \nand obtain a vocabulary of 501 phonetic speech tokens.\n\n\\begin{table}[t]\n\\setlength{\\tabcolsep}{2pt}\n\\centering\\footnotesize{\n\\resizebox{0.49\\textwidth}{!}{\n\\begin{tabular}{lccccccc}\n\\toprule\n\\textbf{Model}& \\bf{\\#shots} & \\multicolumn{4}{c}{\\bf Accuracy $\\uparrow$} \n\\\\\n&&\\textbf{T$\\rightarrow$T} & \\textbf{T$\\rightarrow$S} & \\textbf{S$\\rightarrow$S} & \\textbf{ S$\\rightarrow$T} &\\it Avg\\\\%&& \\textbf{S$\\rightarrow$S} & \\textbf{ S$\\rightarrow$T}\\\\\n\\midrule\n\n\\spotbase & 0    & 0.65  & 0.33 & 0.33 & 0.34 &\\it 0.41 \\\\\n\\spotexpressive\n& 0    & 0.63  & 0.38 &\\bf 0.54 & 0.36        &\\bf \\textit{0.48}\\\\\n\\hdashline\n\\multicolumn{2}{l}{\\textit{\\;\\;\\;\\;Few-Shot Prompting}}\\\\\n\\multirow{3}{*}{\\spotexpressive} \n& 3    & 0.64  & 0.37  & 0.50  & 0.34        &\\it 0.42\\\\\n & 6    & \\bf 0.67  & \\bf 0.39 & 0.51 & 0.35 &\\bf \\textit{0.48}\\\\\n & 9    & 0.64 & 0.38 & 0.40 & \\bf 0.37      &\\it 0.45\\\\\n \n\\hdashline\nRandom Predictor && 0.33 & 0.33 & 0.33 & 0.33& \\it 0.33\\\\\n\\hdashline\n\n\\multicolumn{2}{l}{\\textit{\\;\\;\\;\\;Cascade Topline}}\\\\\n(ASR)+\\llamatwo+(TTS) & 0  & 0.65 & 0.36 & 0.33 & 0.33 &\\it 0.42\\\\\n\\hline\nPrompt Performance  &  & \\multicolumn{2}{c}{0.86} & \\multicolumn{2}{c}{0.96} \\\\\n\\bottomrule\n\\end{tabular}\n}}\n\\caption{\\textbf{Zero-Shot and Few-Shot Performance on the \\sentimentbenchmark}. \\spot models \nare presented with prompts expressing a positive, negative, or neutral sentiment. In the speech modality, the sentiment comes from vocal characteristics (expressive styles such as sad, laughing, etc.), and in the text, it comes from the semantic content. The continuation is then elicited across modalities or\nin the same modality, and tested with pretrained classifiers. %The classes are perfectly balanced so a random classifier performs at 33\\% and 11\\% (33\\% * 33\\%) for accuracy and success ratio respectively. \nThe last row (Prompt Performance) presents the performance when we apply the classifier directly on the text or speech prompt.}\n\\label{tab:expr_cont}\n\\vspace{-1.2em}\n\\end{table}\n\n\\paragraph{Speech and Text Tokenization} We tokenize text with the default LLaMA's tokenizer and speech with the HuBERT tokenizer described above. %in section \\ref{sec:speechtokenizer}. \nFollowing previous work, HuBERT tokens are deduplicated for betting modeling quality. For uni-modal datasets (Text-only and Speech-only), we tokenize the data and prepend them with the corresponding modality token, i.e. \"\\textsc{[Text]}this is a text sentence\" or \"\\textsc{[Speech]}[Hu262][Hu208][Hu499][Hu105]\".\n\n\\paragraph{Interleaving Speech and Text} For the aligned Speech+Text datasets, we mix text and speech by interleaving speech and text at the word level (Figure \\ref{fig:boat1}.b), making the input look like this \"\\textsc{[Text]}the cat \\textsc{[Speech]}[Hu3][Hu7]..[Hu200]\\textsc{[Text]}the mat\"\\footnote{with \"[Hu3][Hu7]..[Hu200]\" being the tokenization of the spoken utterance \"sat on\"}. Our hypothesis is that interleaving training will help the model learn an alignment between speech and text, unlocking better text to speech transfer. The speech and text spans within the sentences are sampled randomly at each training step.\n\n\\paragraph{Speech Decoder} As for speech synthesis from speech tokens, we train a HifiGAN \\citep{kong2020hifigan, polyak2021speechresynthesis} vocoder on the Expresso dataset. The HifiGAN model is conditioned on HuBERT speech tokens and 1-hot speaker embedding from one of 4 Expresso's voices. \nDuring training, the HifiGAN model receives duplicated tokens but we also train it jointly with a duration prediction module as used in \\citealp{lee-etal-2022-direct, lee-etal-2022-textless}\\footnote{https://github.com/facebookresearch/speech-resynthesis/tree/main/examples/speech\\_to\\_speech\\_translation}, which takes as input the deduplicated HuBERT tokens and predict their lengths. During inference, the deduplicated tokens are repeated with the corresponding predicted durations, and are feed into the HifiGAN model to produce waveform.\n\n\\subsection{\\spotexpressive}\\label{sec:spotexpressive}\nPrevious work shows that HuBERT tokens can capture good phonetic information from speech but perform badly at expressivity \\citep{nguyen2023expresso}.\nOur goal is to have a model that can understand and preserve the emotion in the input speech while being biometric-free.\nWe therefore supplement phonetic speech tokens from HuBERT with additional \\textit{pitch tokens} and \\textit{style tokens} and include them in language model training so that our trained \\spotexpressive model can capture and generate more expressive speech. \n\n\\paragraph{Pitch Tokens} Following \\citet{polyak2021speechresynthesis} and \\citet{kharitonov2022pgslm}, we produce pitch tokens using a VQ-VAE \\citep{vandenoord2017VQVAE} model trained on the F0 of the input speech.\nFollowing the implementation of \\citet{polyak2021speechresynthesis},\nwe trained a VQ-VAE model on the Expresso \\citep{nguyen2023expresso} dataset with a codebook size of 64 and a downsampling rate of 128, resulting in 12.5 pitch tokens per second.\nFor training the pitch quantizer, the F0 is extracted using pyaapt\\footnote{https://github.com/bjbschmitt/AMFM\\_decompy}.\nHowever, for the language model training, we extract F0 using FCPE\\footnote{https://github.com/CNChTu/FCPE}, a fast pitch estimator using Transformer, for inference speed.\n\n\\paragraph{Style Tokens} We extract speechprop features from \\citet{Duquenne2023sonarexp_arxiv}, which capture speech input's expressive style. The features were pooled with average pooling over input segments of 1 second, making one feature every one second. We further remove speaker information from speechprop features by fine-tuning the features to predict the expressive style on the Expresso dataset which serves as a normalization step to obtain the style features. We finally train a k-means clustering on the normalized features of Expresso dataset with 100 units.\n\n\\paragraph{Expressive Speech Tokenization} We mix the 3 types of tokens (HuBERT tokens at 25hz, pitch tokens at 12.5hz, style tokens at 1hz) into a single sequence of tokens by sorting the tokens with their corresponding timestamps (Figure \\ref{fig:boat1}.c). Similar to \\spotbase, we deduplicate HuBERT tokens as well as pitch tokens, making the input sequence look like this: \"\\textsc{[Speech]}[St10][Pi0][Hu28][Hu22][Pi14][Hu15] [Pi32][Hu78][Hu234][Hu468]\"\n\nApart from the speech tokenization, the training details of \\spotexpressive are the same as for \\spotbase.\n\n\\paragraph{Expressive Speech Decoder} We train a HifiGAN model conditioned on HuBERT tokens, pitch tokens, style tokens and 1-hot speaker embedding from Expresso's voices.\nThe duration predictor is also trained to predict the durations of the HuBERT tokens. During inference, we align each HuBERT token with the corresponding pitch and style tokens and repeat them accordingly.\n\n\\subsection{Training Details}\n\nOur \\spot models are trained on a combination of speech, text and aligned speech+text sequences.  We report in Table \\ref{tab:data} the amount and sampling proportion of each type of data and list the datasets we use here: \n\n\\paragraph{Text-only datasets} We include a subset of LLaMA \\citep{touvron2023llama} training datasets, \nwhere we exclude datasets that are unrelated to speech, like code, \ntotaling 300B text tokens.\n\n\\paragraph{Speech-only datasets} We employ open-sourced large-scale speech datasets,\ntotaling 460K hours of speech or 30B speech tokens.\n\n\\begin{table*}\n\\centering\\footnotesize\n\\setlength{\\tabcolsep}{1.5pt}\n\\begin{tabu}{l cc c  cc c cccc c cccc c c c c c cccc}\n\\toprule\n\\multirow{2}{*}{\\textbf{Model\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;Task}} &\n\\multicolumn{2}{c}{\\bf WUGGY$\\uparrow$} && \\multicolumn{2}{c}{\\bf BLIMP$\\uparrow$} &&\n\\multicolumn{4}{c}{\\bf Topic-StoryCloze$\\uparrow$} && \\multicolumn{4}{c}{\\bf StoryCloze$\\uparrow$} &&\n\\multicolumn{1}{c}{\\bf {MMLU$\\uparrow$}}\\\\%&&\n\\cline{2-3}\\cline{5-6}\\cline{8-11}\\cline{13-16}\\cline{18-18}%\\cline{20-20}%\\cline{22-25}\n& T & S && T & S && T & S & T$\\rightarrow$S & S$\\rightarrow$T && T & S & T$\\rightarrow$S& S$\\rightarrow$T && T \\\\%&& S &&\\\\% T & S &T$\\rightarrow$S& S$\\rightarrow$T  \\\\\n\\midrule\n\\multicolumn{1}{l}{\\textit{\\;\\;\\;\\;Previous Work}}\\\\\nGSLM  \\citep{gslm}&  $\\emptyset$ & 64.8  && $\\emptyset$ &  54.2  &&  $\\emptyset$  & 66.6  & $\\emptyset$ & $\\emptyset$ && $\\emptyset$ & 53.3 & $\\emptyset$ & $\\emptyset$   &&$\\emptyset$ &&\\\\% -- && \\\\\n\nAudioLM \\citep{borsos2023audiolm}&  $\\emptyset$ & 71.5 &&$\\emptyset$ &  \\bf 64.7  &&  $\\emptyset$ &-- & $\\emptyset$ & $\\emptyset$ && $\\emptyset$  &-- & $\\emptyset$ & $\\emptyset$   &&$\\emptyset$\\\\%&& -- && \\\\%$\\emptyset$ & --& $\\emptyset$& $\\emptyset$\\\\\n\nVoxtlm   \\citep{maiti2023voxtlm}       & \\bf  80.3 & 66.1    && \\bf  74.2 &    57.1 &&  --  &--    & -- & -- && --  &  -- & -- & --   && --\\\\%&& -- && \\\\%$\\emptyset$ & $\\emptyset$& $\\emptyset$& $\\emptyset$\\\\\n\nTWIST \\citep{hassid2023textually}&  $\\emptyset$ &\\bf 74.5 &&   $\\emptyset$ &59.2 &&  $\\emptyset$  &76.4  & $\\emptyset$ & $\\emptyset$ && $\\emptyset$ &55.4 & $\\emptyset$ & $\\emptyset$   &&$\\emptyset$ \\\\%&& -- \\\\%&& \\\\%$\\emptyset$ & $\\emptyset$& $\\emptyset$& $\\emptyset$\\\\\n\n\\hdashline\n\\multicolumn{1}{l}{\\textit{\\;\\;\\;\\;Ours}}\\\\\n\n\\spotbase  &\\bf80.3 &   69.0 &&   73.3 & 58.3 &&   \\bf98.0 &\\bf82.9 &\\bf72.7 &\\bf88.6 &&\\bf   79.4 &\\bf61.0 & \\bf  59.5 &\\bf  64.6 &&\\bf   36.9 \\\\%&&75.3&&\\\\% 65.0 & 33.1 & 33.1 & 30.0 \\\\ % NB: THE 0 SCORES WERE SET TO 33.3 CAUSE THEY SHOULD BE RANDOM\n\n\\expressiveVone~~        &   75.8 &   65.0 &&   73.6 &   54.2 &&   97.9 &   75.4 &   61.6 &   73.2 &&   78.9 &   56.9 &   54.6 &   58.8 &&   33.3 &&\\\\%&&\\\\%\\bf  70.0 & \\bf 44.5 &\\bf 52.1 &\\bf 37.2 \\\\\n\n\\hline\n\n\\multicolumn{1}{l}{\\textit{\\;\\;\\;\\;Cascade Topline}}\\\\\n(ASR +) \\llamatwo                   & 84.1& 79.2 && 72.8 & 71.6 && 98.5 &94.76 &  94.76  &  94.76  && 81.9& 75.7 & 75.7  & 75.7  && 46.2 \\\\%&&89.6&& 61.2& 33.3& 33.3& 33.3\\\\\n\n\\bottomrule\n\\end{tabu}\n\\caption{\\textbf{Zero- and few-shot comprehension evaluation}. %All the models reported are trained for 100k steps. \nReporting accuracy based on log-likelihood -- normalized by the number of tokens --  minimization prediction. MMLU is evaluated in the 5-shots prompting setting. The other tasks are evaluated in the zero-shot setting. T refers to the text modality and S to the Speech modality. %We compare to the following related work reported scores in AudioLM \\citep{borsos2023audiolm}, Voxtlm \\citep{maiti2023voxtlm}, TWIST  \\citep{hassid2023textually}. \nWe fill with $\\emptyset$ the task and modality that are not supported by the reported system, and with $\\_$ the scores that are not publicly available.}\n\\label{tab:main_table_essentials_w_sentiment}\n\\end{table*}\n\n\\paragraph{Aligned Speech+Text datasets} We use \na small subset of speech datasets that came along with text transcriptions. We then collect speech-text alignments at word-level either through the provided dataset or by performing an alignment at the word level using the aligner tool from \\citet{pratap2023scaling}.\nAll the alignments are automatically curated, and thus, possible errors in the alignments are admitted.\nThe speech+text datasets comprise of 110K hours of speech or 7B speech tokens (HuBERT) and 1.5B text tokens. In total, we have 570K hours of speech.\nAs the number of tokens differs a lot in different modalities, we tuned the sampling weights of the datasets so that the model sees each modality (speech, text, speech+text) roughly equal number of times during training.\n\n\\paragraph{Optimization}\nWe point to the Appendix~\\ref{sec:optimization} for extensive optimization details.  %\\bokai{probably we can have a table in the appedix to list these hyperparamters and interleaving sampling details?}We then continue to pre-train the 7B \\llamatwo model with the constant final learning rate of $3.0e^{-5}$, a sequence length of 4k (equivalent to 200 seconds of speech only), and a batch size of 4 per GPU. We trained the model on 64 A100 GPUs, making an efficient batch size of 1M tokens, for 200K steps. \n\n\\section{Speech and Text Understanding} \\label{sec:speechtextunderstanding}\n\nAs illustrated in Table~\\ref{tab:generations_tacl}, \\spot can generate semantically and expressively consistent content when prompted with speech tokens or text tokens\n\\footref{demopage}\n. In this section, we assess notably the semantic ability of \\spot in both single- and cross- modal scenarios by evaluating quantitatively a collection of benchmarks that require generating text or speech tokens, we'll study the \\spot expressivity evaluation in Section~\\ref{sec:sentimentmodeling}.\n\n\\subsection{Evaluation Benchmarks}\n\n\\paragraph{Speech- and Text- only Tasks}\n\nWe use sWUGGY, sBLIMP, StoryCloze as speech tasks. \nAll these tasks probe model's comprehension\nby providing different input sequences (hypotheses), one of which is correct, and assessing if the model assigns higher log-likelihood to the correct hypothesis among multiple choices.\nWe point to \\citet{nguyen2020zero} for detailed description of sWUGGY and sBLIMP. Briefly, sWUGGY measures the lexical knowledge of the model and  BLIMP measures the grammatical knowledge of the model.\nFor WUGGY, we report the accuracy on the combination of in-vocab and OOV subsets.\nGiven the beginning of a short spoken story, StoryCloze measures %if the model can find the plausible ending among two sentences, which typically requires \nthe high-level semantic understanding and common sense \\citep{mostafazadeh-etal-2017-lsdsem} of the model. We use the spoken version of the original storycloze (S-StoryCloze) and the topic-Storycloze (T-StoryCloze) assembled by \\citet{hassid2023textually} based on simpler negative samples. All of these tasks have a random baseline performance of 50\\% and are evaluated in the zero-shot setting. %We predict the sample with the highest likelihood of the two choices.\nIn addition to speech, these benchmarks are also available in the text modality. We, therefore, measure the text-modeling abilities of \\spot on these. In addition, we evaluate \\spot on MMLU \\citep{hendrycks2021mmlu}, a popular evaluation benchmark for text-based LLMs, using a 5-shot setting.\nAll the tasks are reported with the accuracy metrics.\n\n\\paragraph{Cross-modal Speech-to-Text and Text-to-Speech Tasks}\n\\spot is trained in both speech and text. For this reason, it has the ability to model tasks that require both text and speech modeling. Based on the text and speech versions of StoryCloze, we build speech to text (S$\\rightarrow$T) and text to speech (T$\\rightarrow$S) Storycloze for which the context is in one modality (e.g. speech) and the hypothesis is in the other modality (e.g. text).\nThey are evaluated similarly to other comprehension tasks by comparing the log-likelihood given by the model and are performed in the zero-shot setting.\nWe also evaluate \\spot in-context learning capability with few-shot generation tasks: ASR, TTS, and Speech Intent Classification.\nFor ASR, \nwe prompt the model with examples of speech-text transcription pairs along with a new speech segment for the model to generate the text transcriptions.\nWe report the Word-Error-Rate (WER) between the generated and the gold transcriptions.\nFor TTS, \nwe prompt the model with examples of text-speech pairs and a new text to be synthesized.\nWe transcribe the generated audio with Whisper \\citep{radford2023robust} and compare it with the original text with Character-Error-Rate (CER). Both these tasks are evaluated with Librispeech clean and other test sets.\nWe use the Intent Classification task from \\citet{chang2023speechpromptv2}. Similar to the ASR task, we prompt the model with examples of speech-intent text pairs and a new speech input. We evaluate model generation with the exact match accuracy metrics.\nWe report the detailed prompting used for few-shot generation tasks in the Appendix~\\ref{apx:few_shot_prompt}.\n\n\\paragraph{Baselines} We compare our results with previously published generative speech systems.\nAll these methods use one or several Transformer \\citep{vaswani2017attention} decoder-only models trained on speech units. They differ in how they are trained (pretrained from scratch or fine-tuned), the types of speech units they model, and their amount of training data. We compare with \nGSLM \\cite{gslm}, TWIST \\citep{hassid2023textually} based on Llama-13B and AudioLM \\citep{borsos2023audiolm}.\nIn contrast with \\spot, the approaches mentioned above only rely on speech units during training, making them speech-only models (i.e. they  do not support text understanding nor generation). We also compare our models to VoxtLM \\citep{maiti2023voxtlm}, a concurrent work on speech and text language modeling.\nWe report the best scores from the original published papers for all the mentioned methods. As a top-line comparison, we compare our models with cascade models that use \\llamatwo as a text generative model. For text-to-text (T$\\rightarrow$T), we only rely on \\llamatwo -7B. For speech-to-speech (S$\\rightarrow$S), we utilize the cascade model, ASR from \\whispermedium\\cite{radford2023robust}, followed by \\llamatwo, synthesized by \\mmstts\\cite{pratap2023scaling}.\n\n\\paragraph{Ablation Experiments} Finally, we ablate the several components of the \\spot training recipe. We compare \\spotbase to a \\llamatwo model continuously pretrained with two \\textit{parallel data training} settings. First, the ASR+TTS-only model consists of training with pairs of semantically equivalent sequences of speech and text (e.g. ``\\textsc{[Text]} the cat jumped by the window \\textsc{[TTS]}[Hu12]..[Hu54]'' or ``\\textsc{[Speech]}[Hu12]..[Hu54]\\textsc{[ASR]} the cat jumped by the window''\\footnote{with ``[Hu12]..[Hu54]'' being the tokenization of the spoken utterance ``the cat jumped by the window''}). Second, the Word-level Transcription model consists of training on sequences of pairs of textual and spoken words (e.g. ``\\textsc{[Text]} the \\textsc{[Speech]}[Hu12]..[Hu34] \\textsc{[Text]} cat \\textsc{[Speech]}[Hu454]..[Hu90]...\\textsc{[Text]} window \\textsc{[Speech]}[Hu15]..[Hu54]''). Additionally, we compare \\spotbase to models trained on a single modality (speech or text) and with speech+text but without any interleaving data (cf. noted ``No Interleaving''). %We measures the importance of amount of aligned speech-text data on final performance in the Appendix Figure~\\ref{fig:percentage_text_speech}. We find that \n\n\\begin{table*}[t]\n\\centering\\footnotesize\n\\resizebox{0.95\\textwidth}{!}{\n\\begin{tabu}{l cc c cc c c }\n\\toprule\n\\textbf{Model}\\hfill \\textbf{Task} & \\multicolumn{2}{c}{\\bf LS clean (10 shots)}  && \\multicolumn{2}{c}{\\bf LS other (10 shots)} && \\multicolumn{1}{c}{\\bf IC (30 shots)}   \\\\ \n\\cline{2-3}\\cline{5-6}\\cline{8-8}\n& ASR$\\downarrow$ &TTS$\\downarrow$ && ASR$\\downarrow$ &TTS$\\downarrow$ && $\\uparrow$  \\\\\n\\midrule\n\n\\multicolumn{2}{l}{\\textit{\\;\\;\\;\\;\\spot variants}}\\\\\n\n\\spotbase  &     21.9&  45.5  && 29.2&    43.8&&   71.9 \\\\%/fsx-onellm/benjaminmuller/speechlm/checkpoints/xlformers/2023-12-25-asr_tts_100k_vs_200k-metalog.txt\n\\;\\;\\;\\;+ASR+TTS      &     \\bf6.0 & \\bf   6.7    && \\bf    11.0 &\\bf 7.9 &&\\bf 75.8\\\\ \n\\expressiveVone & 37.9& 52.0 &&50.0& 53.6 && 66.2 \\\\ % /fsx-onellm/benjaminmuller/speechlm/checkpoints/xlformers/2023-12-13-tts_exp_v1_10-metalog.txt % /fsx-onellm/benjaminmuller/speechlm/checkpoints/xlformers/231130_llama2_7B_expressivity_stylev2/evaluate-librispeech_exp_v2/\n\n\\hdashline\n\\multicolumn{2}{l}{\\textit{\\;\\;\\;\\;Parallel Data Training}}\\\\\nWord-level transcription                  & 113.2&  85.2 & & 111.6   & 75. 2&& 22.6      \\\\%/fsx-onellm/benjaminmuller/speechlm/checkpoints/xlformers/2023-11-20-overlappin_2-metalog.txt\nASR+TTS only      & 7.7 &  8.1   &&   11.9& 9.4&& 7.4   \\\\%/fsx-onellm/benjaminmuller/speechlm/checkpoints/xlformers/2023-11-29-asr_tts_w_special_tokens-metalog.txt + /fsx-onellm/benjaminmuller/speechlm/checkpoints/xlformers/2023-11-30-tts_w_special_tokens_2-metalog.txt\n\n\\hline\n\\multicolumn{2}{l}{\\textit{\\;\\;\\;\\; Cascade Topline}}\\\\\n(\\whisper +) \\llamatwo  (+MMS TTS)   &  3.7 &  4.0 & &  7.2 &  4.9 &&  89.6 \\\\%/fsx-onellm/benjaminmuller/speechlm/checkpoints/xlformers/2023-12-18-asr_repeat_llama_2_real-metalog.txt \n\\bottomrule\n\\end{tabu}\n }\n\\caption{\\textbf{Few-shot tasks.} \nWe evaluate \\spot models for Automatic Speech Recognition (ASR) and Text-to-Speech (TTS) Evaluation on LibriSpeech (LS) and Intent Classification (IC). %All the models reported are trained for 100k steps unless otherwise indicated. \nASR scores correspond to Word-Error-Rate (\\% WER) evaluated in the 10-shots setting with a max context length of 1024. TTS scores correspond to the Character-Error-Rate (\\% CER) in the 10-shots setting with a max context length of 2048. IC scores correspond to accuracy in the 30 shots setting.}\n\\label{tab:asr}\n\\end{table*}\n\n\\subsection{Single-Modality Performance}\nWe report in Table~\\ref{tab:main_table_essentials_w_sentiment} results on comprehension evaluations.\nThe reported metrics are calculated with the normalization of the log-likelihood as similar to previous work\\footnote{We observe that the normalization of the log-likelihood has different impacts on various tasks, but we follow previous work to normalize the log-likelihood in Table~\\ref{tab:main_table_essentials_w_sentiment}. Please refer to Table~\\ref{tab:appendix_acc_and_acc_token} for a full comparison.}.\n\nWe find that \\spotbase competes with the baselines for WUGGY, BLIMP, and Storycloze in the speech modality while preserving competitive text performance.\nMore specifically, \\spotbase outperforms the baselines by a large margin on StoryCloze, which requires the most advanced speech semantic abilities compared to the other reported benchmarks. \n\n\\paragraph{Interleaving is critical} We run ablation experiments (cf. Table~\\ref{tab:ablation_zero_shot_and_mmlu}) to understand what leads to this performance by controlling for the training budget and ablating a large number of training parameters. We set the training budget at 100k training steps or 100B tokens. \n\nFirst, fine-tuning the model on speech-only tokens leads to a much lower performance (e.g. more than 6 points difference with \\spot on spoken Storycloze). This shows that interleaving training not only helps preserve the text generation abilities of the model but also leads to better speech understanding and generation performance. Second, we find that fine-tuning \\llamatwo on parallel data, ---- both with ASR+TTS only training or Word-level transcription training -- leads to lower performance on tasks such as StoryCloze and BLIMP. Notably, \nthe performance is more than 10 points lower on cross-modal Topic-StoryCloze (T$\\rightarrow$S and S$\\rightarrow$T).\n\n Finally, we measure the importance of the amount of aligned data used for interleaving training in Figure~\\ref{fig:percentage_text_speech_small}. We find that the model's performance in speech (T-StoryCloze) steadily increases with the amount of aligned data. \nBased on these experiments, we conclude that interleaving training is the primary factor leading to good-quality speech generation. \n\nOur interpretation of the superiority of interleaving compared to other mixed-modal training setting and speech-only training is the following: \nInterleaving is the only training recipe that generalize what is learnt during \\llamatwo pretraining to speech and text tokens. Indeed, interleaving preserves the right-to-left natural causality of the data within each modality and also across modalities allowing the model to learn aligned representation between speech and text units. We present supporting evidence of this alignment in the next section (\\S~\\ref{sec:cross_modal_perf})\n\n\\paragraph{Expressivity comes with a moderate modeling cost}\nAs shown in Table~\\ref{tab:main_table_essentials_w_sentiment}, \\expressiveVone performs lower than \\spotbase on these tasks, indicating that the expressive speech units lead to moderate lexical, grammatical, and semantic understanding degradation. This suggests that modeling a given raw speech for \\expressiveVone is more costly than for \\spotbase. Indeed, in contrast with \\spotbase, \\expressiveVone is based on integrating expressive speech units in the sequence during training, in addition to Hubert-tokens. This leads to extending the token sequence length for a fixed raw input speech. This added complexity leads to a degradation of speech modeling performance.\n\nIn the text modality, despite being fine-tuned on billions of speech tokens, \\spot still performs decently on MMLU (above 33\\%) and degrades by less than 2 points on WUGGY, BLIMP, and StoryCloze compared to \\llamatwo.\n\nFinally, on these tasks, the cascade approach (ASR with \\textsc{Whsiper} followed by \\llamatwo) is above \\spot by a large margin. This may be attributed to the high quality of Whisper ASR and the cleanliness of the benchmarks, which makes the speech content more lossless compared to speech tokenization.\n\n\\subsection{Cross-Modal Performance}\n\\label{sec:cross_modal_perf}\n\\spot can also model sequences that are made of both speech and text tokens. \n\n\\paragraph{Cross-Modal StoryCloze}  As seen in Table~\\ref{tab:ablation_zero_shot_and_mmlu}, we find the performance on StoryCloze in the text to speech direction (T$\\rightarrow$S) on par with the speech only performance (S). In contrast, the (S$\\rightarrow$T) direction is about 5 points %\\bokai{is it 15 points if we compare 64.6 and 49.3?}\\ben{we're comparing with the spirit-base model performance in the speech modality: I rephrased to reduce the ambiguity}\nabove the speech performance (S), suggesting that the model performs better at text generation compared to speech generation even when prompted on speech. \n\\begin{figure}[h]\n  \\centering\n  \\includegraphics[width=0.5\\textwidth]{figures/fs_plot.pdf}\n  \\caption{\\spotbase performance with regard to the number of shots presented to the model context for Intent Classification, ASR and TTS. }\n  \\label{fig:few_shots}\n\\end{figure}\n\\paragraph{ASR \\& TTS}\n\nSimilarly to text language models, \\spot can be prompted with few-shot examples to perform specific tasks. We illustrate this with ASR and TTS. We show in Table~\\ref{tab:asr} that \\spot models reach non-trivial performance in ASR and TTS. We find that few-shot prompting leads to the best performance with 10 shot prompting (cf. Figure~\\ref{fig:few_shots}).\\footnote{We note that above 20 shots, we reach the maximum number of tokens that fit in the context for ASR and TTS.} Our best \\spotbase model is at 21.9 WER in Librispeech clean and 45.5 CER in TTS. \nWe observe that when we add parallel ASR and TTS examples during training (cf. +ASR+TTS in Table~\\ref{tab:asr}), we can improve the performance from a very large margin. We note that adding ASR and TTS data has a very moderate impact on the rest of the tasks. \n\n\\paragraph{Cross-Modal Alignment} \n\nTo understand better the hidden mechanism that enables \\spot to deliver good cross-modal performance while only being trained on \\textit{interleaved} data and raw speech and text, \nwe look at the token-level similarity of the model's features from input sequences of HuBERT tokens and the corresponding BPE tokens. \nWe illustrate this in Figure~\\ref{fig:alignement} (bottom), where we compute the maximum similarity over the same words of speech and text features extracted from different layers of \\spot. We find that the similarity between spoken and written sequences inside the model increases from layer 2 and layer 20. In comparison, this alignment is absent in the model trained without interleaving, and is less effective in the model trained with Word-level transcription, particularly in early to middle layers. This suggests that modality mixing enables speech-text alignment, and interleaving further enables the model to map speech sequences with corresponding text sequences.\nFigure~\\ref{fig:alignement} (top) shows the alignments of BPE tokens and HuBERT tokens of a same sentence.\nWe see that the middle layers of \\spot capture the same semantics information from both input modalities, with high alignments towards the end of each word (last BPE tokens, late HuBERT tokens).\n\n\\paragraph{Downstream Speech Classification}\nFinally, we report in  Table~\\ref{tab:asr} the abilities of \\spot to perform Speech Intent Classification (IC) task. We find that the accuracy improves with the number of shots (cf. Figure~\\ref{fig:few_shots}). Our best \\spot model reaches up to 79\\% accuracy (compared to 89\\% of the topline performance). \n\n\\paragraph{Pretrained Knowledge is Essential for Few-Shot Learning}\n\nFigure~\\ref{fig:learning_curve_small} in Appendix~\\ref{apx:complt_res} reports the task-specific performance of \\spotbase with regard to the number of training steps compared to a randomly initialized model trained in the same setting. \nAfter only 25k training steps, \\spotbase reaches more than 75\\% accuracy on Intent Classification while the randomly initialized model is below 20\\%. This means that starting from a pretrained \\llamatwo model is essential for few-shot in-context learning and that our method successfully transfers the pretrained few-shot learning abilities of the model to the speech modality.  \n\n\\begin{table*}[t]\n\\centering\\footnotesize\n\\setlength{\\tabcolsep}{1.5pt}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabu}{l cc c  cc c cccc c cccc c c }\n\\toprule\n\\multirow{2}{*}{\\textbf{Model\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;Task}} &\n\\multicolumn{2}{c}{\\bf WUGGY$\\uparrow$} && \\multicolumn{2}{c}{\\bf BLIMP$\\uparrow$} &&\n\\multicolumn{4}{c}{\\bf Topic-StoryCloze$\\uparrow$} && \\multicolumn{4}{c}{\\bf StoryCloze$\\uparrow$} &&\n\\multicolumn{1}{c}{\\bf {MMLU$\\uparrow$}} \\\\\n\\cline{2-3}\\cline{5-6}\\cline{8-11}\\cline{13-16}\\cline{18-18}\n& T & S && T & S && T & S & T$\\rightarrow$S & S$\\rightarrow$T && T & S & T$\\rightarrow$S& S$\\rightarrow$T && T \\\\\n\\midrule\n\\multicolumn{2}{l}{\\textit{\\;\\;\\;\\;\\spot{} variants}}\\\\\n\n\\spotbase &\\bf80.3 &   69.0 &&   73.3 &58.3 &&   98.0 &\\bf82.9 &\\bf72.7 & \\bf 88.6 &&   \\bf 79.4 & \\bf 61.0 &   59.5 &   64.6 &&   36.9 \\\\ % originally: Rope-100k\n\n~-~ No Interleaving & 74.7 & 67.1 && 72.6 & 57.2 && 97.7 & 74.0 & 57.5 & 71.9 && 78.2 & 60.1 & 54.2 & 56.4 && 32.1\\\\  \n\n~-~ Randomly-initialize & 78.1 & \\bf 69.9 && 72.9 & \\bf 58.8  & & 97.6 & 81.8 & 70.2 & 88.1 & & 73.7 & 58.0 & 58.2 & 62.5 & & 25.8\\\\   \n~-~ Rope $\\theta$ default &   78.2 &   69.5 &&   73.3 &   57.7 &&  \\bf 98.2 &   82.0 &   72.0 &   88.3 &&   78.9 &   60.9 & \\bf 59.8 & \\bf 65.5 &&   34.3 \\\\\n ~-~ +ASR+TTS                   &   76.8 &   68.7 &&   71.7 &   57.2 &&   97.7 &   81.6 &   71.6 &   86.1 &&   77.4 &   59.9 &   58.8 &   63.5 &&   31.4 \\\\\n\n\\hdashline\n\n\\multicolumn{1}{l}{\\textit{\\;\\;\\;\\;Parallel Data Training}}\\\\\n\nWord-level transcription  &   74.7 &   67.1 &&   72.6 &   57.2 &&   98.0 &   80.3 &   57.5 &   71.9 &&   78.2 &   60.1 &   54.2 &   56.4 &&   32.1 \\\\\nASR+TTS-only                             &   76.5 & 69.8 &&   73.3 &   57.6 &&   97.3 &   74.9 &   63.5 &   71.8 &&   76.3 &   54.6 &   53.9 &   54.0 &&   34.4 \\\\\n\n\\hdashline\n\n\\multicolumn{2}{l}{\\textit{\\;\\;\\;\\;Unimodal Models }}\\\\\nSpeech Only                            &   67.1 &   69.5 &&   53.7 &   58.0 &&   54.8 &   72.9 &   52.2 &   49.4 &&   53.7 &   54.8 &   52.6 &   49.3 &&   27.2 \\\\\nText Only                              &   72.6 &   46.8 &&\\bf73.9 &   52.6 &&   \\bf 98.2 &   51.7 &   47.5 &   51.7 &&   79.0 &   50.2 &   47.3 &   52.1 &&\\bf 40.1 \\\\\n\n\\bottomrule\n\\end{tabu}\n}%\n\\caption{\n\\textbf{Ablation experiments in Zero- and few-shot comprehension evaluation}. All the models reported are initialized from \\llamatwo 7B (except Randomly-initialize one) and are trained for 100k steps. Reporting accuracy based on negative-log-likelihood -- normalized by the number of tokens --  minimization prediction. MMLU is evaluated in the 5-shots prompting setting. The other tasks are evaluated in the zero-shot setting. T refers to the text modality and S to the Speech modality. For a full comparison of unnormalized and normalized scoring accuracy, refer to Table \\ref{tab:appendix_acc_and_acc_token}.\n}\n\\label{tab:ablation_zero_shot_and_mmlu}\n\\vspace{-1.2em}\n\\end{table*}\n\n\\section{Expressivity Modeling} \\label{sec:sentimentmodeling}\nOne of the core contributions of this work is the modeling of expressivity. To measure the expressivity of our model we first evaluate the quality of the introduced pitch and style tokens (\\S~\\ref{sec:expressivity_token_eval}). Second, we evaluate our \\spot models on the newly introduced \\sentimentbenchmark (\\S~\\ref{sec:stspbenchmark}). \n\n\\subsection{Style and Pitch Tokens Evaluation}\n\\label{sec:expressivity_token_eval}\n\nWe model expressive speech by complementing phonetic speech tokens (HuBERT) with Pitch and Style tokens. \nTo evaluate the quality of our tokenization, we use the speech resynthesis task from \\citet{nguyen2023expresso}.\nIt measures how well the resynthesized speech is compared with the original audio in terms of preserved content, expressive style, and pitch. Table \\ref{tab:expressive_tokenizer_evals_avg} shows the performance of \\spotbase and \\spotexpressive tokenizers compared to Encodec and Hubert-only baselines.  We see the \\spotexpressive tokenizer can capture good expressive style and pitch from the input speech. Additionally, we observe a very large improvement in Style and Pitch resynthesis when we compare \\spotbase tokenizer with \\spotexpressive. \n\n\\subsection{The \\sentimentbenchmark (\\sentimentbenchmarkSHORT)}\n\\label{sec:stspbenchmark}\nTo evaluate how well our \\spot models can understand and generate expressive speech and text, we introduce the \\sentimentbenchmark\\footref{stsppage}. It is made of a collection of speech and text prompts in the positive, negative or neutral sentiment. Given a spoken or written prompt , the task consists in generating a text or speech sequence of tokens that preserves the sentiment of the prompt. \n\nFor instance, in the text-to-X direction (T$\\rightarrow$T and T$\\rightarrow$S), given a written sentence bearing sadness, we check if the spoken generated text/utterance is also sad. On the other hand, the direction speech-to-X (S$\\rightarrow$S and S$\\rightarrow$T), given a spoken happy-sounding utterance, we check whether the model generates a positively written text or positive utterance.\n\n\\subsubsection{Sentiment-Rich %Spoken and Written \nPrompts} \\label{sec:sentimentcontinuation:data}\n\\paragraph{Speech Prompt} In order to have the read speech of different expressive styles (e.g. \\textit{he's done it again} in happy/sad style). We utilize two datasets: 1) \\textit{Expressive reading} from \\expresso \\cite{nguyen2023expresso} consisting of 47 hours of expressive North American English speech where 7 different styles are applied on the same content that does not reflect the emotion being conveyed. We use only the speech from 3 emotions: \"happy\", \"sad\" and \"default\". (we will refer to this dataset as \\expressoread) 2) \\emov\\cite{adigwe2018emov}, composed of emotional speech from 5 different speakers and 2 languages (North American English and Belgian French). We select only the English speech from 3 speakers when the same content is recorded in three different emotions: \"Amused\", \"Angry\" and \"Neutral\".\n\n\\paragraph{Text Prompt} In order to have expressive text (e.g. \\textit{he's such an amazing player} for positive) as prompt, we transcribe\\footnote{We use \\whispermedium\\cite{radford2023robust}} \\textit{improvised dialog} from \\expresso for 4 emotions: \"happy\", \"angry\", \"sad\" and \"default\" to obtain an aligned Speech-Text dataset. Then we filter the samples if the transcription has less than 10 words %(separated by space) \nor it has one word appearing more than 10 times. We refer to this aligned dataset by \\expressoasr.\n\n\\paragraph{Sentiment Mapping} To unify different sets of emotional classes, we associate the emotions \"happy\"/\"Amused\", \"sad\"/\"Angry\" and \"default\"/\"Neutral\" to the \"positive\", \"negative\" and \"neutral\" sentiments.\n\n\\paragraph{Data Splits} We split the datasets into train/dev/test subsets for later usage. Table~\\ref{tab:stsp} presents a comprehensive statistical overview of the datasets used. For \\expressoread, we use the original train/dev/test splits; while for the \\emov, we split it randomly into train/dev/test subsets with the ratios of 60/20/20. The \\expressoasr dataset is also divided into train/dev/set with the ratios of 60/20/20\\footnote{We don't use the original data splits because the amount of data in the dev and test subsets is not enough.}. We use the train and dev subsets to train the sentiment classifiers and the test subset to prompt the \\spot models.\n\n\\iffalse\n\\subsection{Fine-Tuning \\spot on Expressive Dataset}\nTo further fine-tune \\spot with the expressive data, we download the audiobooks\\footnote{\\url{https://github.com/facebookresearch/libri-light/blob/main/data\\_preparation/download\\_librivox.py}}from the category \\textit{Dramatic Readings} of LibriVox\\footnote{\\url{https://librivox.org/}}. Then we follow the same pipeline in \\cite{kang2023libriheavy} to locate, segment and align the audios. It results in an aligned Speech-Text dataset with 1K hours of audio, 97M tokens in speech and 13M tokens in text. Finally, we utilize the pretrained checkpoint from Interleaving Rope+SProp-V2 in 75K steps, continue fine-tuning on this dataset till 100K steps with a sampling probability of 2.6\\% along with all the datasets listed in Table~\\ref{tab:data}. We refer to this fine-tuned model as Interleaving Rope+SProp-V2 FT.\n\\fi\n\\subsubsection{Evaluation Metrics}\nFor both tasks, we check if the generated utterance has a sentiment that is consistent with the sentiment of the prompt. We assess the sentiment of the produced utterance using sentiment classifiers and report its accuracy. We obtain text and speech sentiment classifiers by fine-tuning pre-trained text and speech models respectively. For the speech classifier, similar to \\citet{nguyen2023expresso}, we fine-tune the wav2vec2-base model \\citep{baevski2020w2v2}\non the training sets of \\expressoread, \\expressoasr\\footnote{We use only the speech data} and \\emov. For the text classifier, we fine-tune the 3-classes sentiment classifier from \\citet{hartmann2021} on the transcriptions of the \\expressoasr training set. The accuracy for speech-to-X directions is averaged over \\expressoread and \\emov. We repeat the experiments three times and report the averaged accuracy.\n\\label{sec:sent_cont_eval}\n\n\\subsubsection{Evaluation Settings}\nWe tune the generation parameters on the dev sets, refer to Appendix~\\ref{apx:gen_params} for more details.\n\\paragraph{Zero-Shot}We prompt \\spot using positive, negative or neutral text/speech input from the test sets of the datasets described in section \\ref{sec:sentimentcontinuation:data}. Then 1) for S$\\rightarrow$S and T$\\rightarrow$S, we classify the generated speech with the speech classifier. 2) for T$\\rightarrow$T and S$\\rightarrow$T, we assess the text continuation with the text classifier.\n\n\\paragraph{In-context Few-Shot Learning } We also evaluate \\spot in a few-shot setting by constructing a set of few-shot examples (cf. Appendix~\\ref{apx:sent_cont_few_shots}) and feed them as the in-context prompt. \n\n\\subsubsection{Results}\nWe report the results evaluated on the test sets in Table~\\ref{tab:expr_cont}. For zero-shot performance, \\spotexpressive surpasses \\spotbase in all directions, with the exception of T$\\rightarrow$T where they perform comparably. Compared to the cascade baseline, \\spotexpressive outperforms it over all the directions. In the case of few-shot results, we observe that few-shot is beneficial for all directions except S$\\rightarrow$S. For both zero-shot and few-shot, the sentiment continuation is better preserved within the same modality than across different modalities. Among all directions, S$\\rightarrow$T scores the lowest. The final row of Table~\\ref{tab:expr_cont} also includes an evaluation of performance directly on the input prompt. All prompts receive high scores, suggesting a significant potential for improvement in the preservation of expressivity.\n\n\\section{Responsible AI in Speech and Text}\\label{sec:responsible}\nThis section discusses and evaluates responsibility aspects from \\spot. SpeechLMs have the potential to bring the same benefits as text-based LMs and potentially increase their reach to low-resource languages that are mainly spoken. %, such as access to various downstream applications like search, language generation, summarization, translation, chatbots, etc. in the audio modality. This can increase their reach to more use cases and languages, including those that are not written or sparsely written.\n\nQuantifying and working on user safety is a key aspect from generative model development. These models can inadvertently generate content that is harmful, offensive, or inappropriate is essential for generative language models \\cite{deshpande2023toxicity, touvron2023llama}. While safety is a broad concept, we focus on the specific problem of added toxicity in the generation of the \\spot. Inspired by previous studies \\cite{communication2023seamlessm4t}, we define added toxicity as a toxicity increase in the generation compared to the initial source utterance.\n\\subsection{Evaluation}\n\\paragraph{Data} We use the \\holisticbias dataset \\cite{smith-etal-2022-im}  and its synthesized speech extension \\cite{communication2023seamlessm4t}. This dataset has been shown to trigger toxicity for conditional language models \\cite{costa2023toxicity}. \nWe utilize it as the prompt for generating text (T$\\rightarrow$T) and speech (S$\\rightarrow$S), respectively. \nWe note that this dataset is designed to trigger verbal toxicity. We leave to future work the evaluation of non-verbal toxic content generation (e.g. toxic  sarcasm). \n\n \\paragraph{Metrics} Similar to \\citet{communication2023seamless}, we use \\mutox \\cite{mutox} and \\etox\n \\cite{costa2023toxicity} as our  toxicity classifiers. For speech, we simply run ASR and evaluate toxicity with \\etox (we refer to this as \\asretox). To compute the added toxicity, we evaluate toxicity both in the input prompt and in the generated output. For \\etox and \\asretox, added toxicity is defined as \"when there are more toxic words found in the generated content than in the prompt\". For \\mutox, added toxicity is identified when the \\mutox scores of the generated content exceed the scores of the prompt by more than 0.7.\n\\begin{table}[h]\n\\setlength{\\tabcolsep}{2pt}\n\\centering\\footnotesize\n\\resizebox{0.49\\textwidth}{!}{\n\\begin{tabular}{lcccc}\n\\toprule\n\\multirow{2}{*}{Task} & \\multicolumn{2}{c}{T$\\rightarrow$T} & \\multicolumn{2}{c}{S$\\rightarrow$S}\\\\\n~ & ETOX$\\downarrow$ & MUTOX$\\downarrow$ & ASR-ETOX$\\downarrow$ & MUTOX$\\downarrow$ \\\\\n\\hline\n\\spotbase &          1.19         &       2.69       &   1.06 &  3.75\\\\\n(ASR)+\\llamatwo+(TTS)  &  1.22      &   2.63   &1.17& 2.70  \\\\      \n\\bottomrule\n\\end{tabular}\n}\n\\caption{\\textbf{Added Toxicity Detection}. The proportion of samples with added toxicity divided by the total number of samples. For the \\llamatwo baseline, we use a cascaded pipeline made of \\whisper for ASR and MMS for TTS.\n}\n\\label{tab:toxicity}\n\\end{table}\\subsection{Results}\nWe report results in Table~\\ref{tab:toxicity}. In terms of \\etox, both \\spot and (ASR) + \\llamatwo + (MMS-TTS) have comparable results. When evaluated with \\mutox, however, \\spot shows higher added toxicity especially in S$\\rightarrow$S. This might come from the fact that there exists more toxic contents in our speech training dataset. We leave the mitigation to future work.\n\nFigure \\ref{fig:toxicity-dist} shows the distribution of added toxicity in \\spot in terms of the 13 demographic axes represented in \\holisticbias and how they vary in modality. We observe that \\textit{Gender and sex} and \\textit{Sexual orientation} tend to generate more added toxicity than the rest of demographic axes, while \\textit{ability} and \\textit{nationality} tend to be among the ones that generate the least. There is no big difference in distribution across modalities or metrics.\n\n\\section{Limitations and Broader Impacts}\\label{sec:limitsandimpacts}\n\n\\paragraph{Harmful applications} \\spot also shares the same risks as its generative model predecessors \\cite{touvron2023llama}, such as intentionally harmful applications like fake news and spamming as well as unintentionally harmful ones like unfair or biased results, toxic or untrustworthy generations. These risks can be assessed and mitigated using watermarking e.g \\cite{kirchenbauer2023watermark} or existing reinforcement learning from human feedback (RLHF) e.g. \\cite{bai2022training}.\nIn addition to these traditional text risks, \\spot, being a speech model, also extends risks associated with this modality with intentionally harmful applications like impersonating a specific speaker by continuing short speech segments while maintaining speaker identity and prosody. Mitigation measures for this risk include similar ones as with text (speech watermarking \\cite{communication2023seamless} and RLHF). Similarly to text models, unintentionally harm may arise such as the lack of speaker robustness where the model can generate speech continuations inconsistent with the prompt in terms of accent and dialect only for underrepresented groups in the training data. Among the mitigation strategies, we can include: increasing the variety of the dataset, compensating for bias in representation of different demographics.\n\n\\paragraph{Future Work} In this paper, we showed how combining style and pitch tokens with phonetic tokens and continuously pretraining a text language model delivers very promising multimodal semantic abilities while enabling expressive speech generations. However, several architectural and training improvements could further progress in speech generation.\n\nFirst, training multimodal models remains a challenge. In this work, we observed that despite training on both speech and text, our \\spot models do not perform as well as the initial \\llamatwo model in text. Refining the training could potentially reduce this gap. Second, we restricted our evaluation to English. %\\spot models were trained on a large amount of non-English data. \nMore investigation is needed to assess the quality and safety of the model in non-English languages. \nThird, we only experimented with 7B models. Scaling our experiments beyond 7B could lead to much better performance. \nFinally, the introduced \\spot models are foundational models. This means that more work is needed to make them safe and aligned with user expectations. %As it is now commonly done with text \\citep{ouyang2022training, touvron2023llama2}, fine-tuning a model with instructions and preference data in speech could potentially unlock new experiences such as fully expressive dialog systems. \n\n\\section{Conclusion}\\label{sec:conclusion}\n We introduced \\spot{}, a language model based on \\llamatwo that can generate both speech and text in a cross-modal manner. We showed that by alternating speech and text in the input sequence during training, the model can  generate the content fluidly by changing from one modality to another. We evaluated our models on a collection of speech and text metrics. We plan to make future improvements both in the area of model capability and in transparency and safety.\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{PSLM: Parallel Generation of Text and Speech with LLMs\\\\for Low-Latency Spoken Dialogue Systems}\n\n\\begin{document}\n\n\\renewcommand{\\arraystretch}{0.8}\n\\setlength{\\abovedisplayskip}{5pt}\n\\setlength{\\belowdisplayskip}{5pt}\n\n\\maketitle\n\\begin{abstract}\nMultimodal language models that process both text and speech have a potential for applications in spoken dialogue systems.\nHowever, current models face two major challenges in response generation latency: \n(1) generating a spoken response requires the prior generation of a written response, and\n(2) speech sequences are significantly longer than text sequences.\nThis study addresses these issues by extending the input and output sequences of the language model to support the parallel generation of text and speech.\nOur experiments on spoken question answering tasks demonstrate that our approach improves latency while maintaining the quality of response content.\nAdditionally, we show that latency can be further reduced by generating speech in multiple sequences.\nDemo samples are available at \\url{https://rinnakk.github.io/research/publications/PSLM}.\n\\end{abstract}\n\n\\section{Introduction}\n\nSpoken dialogue systems have been developed for many years to achieve natural human-computer interaction~\\citep{mctear2002spoken,jokinen2009spoken,hongshen2017survey}.\nTraditionally, these systems consist of several components: Automatic Speech Recognition (ASR), Response Generation (RG), and Text-to-Speech (TTS).\nVarious methods for RG have been proposed with the advancements in Large Language Models (LLMs)~\\citep{wang2023survey,yi2024survey}.\nMore recently, the application of LLMs to ASR~(e.g., \\citealt{wang2023viola,hono-etal-2024-integrating,fathullah2024prompting}) and TTS~\\citep{wang2023viola,hao2023boosting} has attracted much attention, leading to the development of multimodal LLMs capable of end-to-end spoken language communication~\\citep{zhang-etal-2023-speechgpt,nachmani2024spoken}.\n\n\\citet{zhang-etal-2023-speechgpt} proposed SpeechGPT, an LLM that receives speech questions (SQ) as speech tokens, which are discrete representations extracted from raw waveforms, and sequentially generates text questions (TQ), text answers (TA), and speech answers (SA).\n\\Figref{fig:overview} (a) illustrates their approach called Chain-of-Modality (CoM) prompting.\nSpectron~\\citep{nachmani2024spoken} follows this prompting style but directly handles speech spectrograms.\nAlthough these methods can generate high-quality responses, they face two major challenges in terms of response latency.\nFirst, generating SA requires the prior generation of TQ and TA.\nSecond, speech sequences are much longer than text sequences\\footnote{Actual sequence lengths are provided in \\Appref{app:seq_len}.}.\n\n\\begin{figure}[t]\n  \\includegraphics[width=\\columnwidth]{fig/overview.pdf}\n  \\vspace{-10pt}\n  \\caption{(a) Chain-of-Modality prompting necessitates generating text questions (TQ) and text answers (TA) from speech questions (SQ) before producing speech answers (SA). (b) Our Parallel Speech Language Model (\\prop) enables the parallel decoding of TA and SA, reducing overall latency. (c) Introducing multiple speech streams further accelerates the generation of SA.}\n  \\label{fig:overview}\n  \\vspace{-10pt}\n\\end{figure}\n\nIn this study, we propose Parallel Speech Language Model (PSLM), an LLM with multiple input-output sequences to handle both text and speech tokens, enabling their parallel generation.\nTo emphasize their parallel processing capabilities, we will refer to these sequences as ``streams''.\nAs described in \\Figref{fig:overview} (b), PSLM begins to generate SA immediately after the end of SQ tokens, which can reduce overall latency.\nThis leads to our first research question (\\textbf{RQ1}): Can PSLM improve latency while maintaining the response quality achieved by CoM prompting?\nAdditionally, we address the second challenge by introducing multiple speech streams to decode multiple speech tokens in a single step, as described in \\Figref{fig:overview} (c).\nThis brings us to the second research question (\\textbf{RQ2}): Do multiple speech streams sacrifice the response quality?\nAddressing these questions will pave the way for more advanced and responsive applications of spoken dialogue systems.\n\n\\section{PSLM}\n\n\\subsection{Speech Discretization}\n\\label{sec:preliminary}\n\n\\paragraph{Speech Tokenization}\nExtracting discrete speech tokens from raw waveforms enables language models to handle speech in the same manner as text tokens. \nSelf-supervised learning has been widely used for speech tokenization due to its ability to extract spoken content from raw waveforms~(e.g., \\citealt{rubenstein2023audiopalm,chou-etal-2023-toward,hassid2023twist}).\nFollowing \\citet{zhang-etal-2023-speechgpt}, we employ Hidden-Unit BERT (HuBERT)~\\citep{hsu2021hubert} for speech tokenization.\n\n\\paragraph{Speech Detokenization}\nIn contrast to text tokenization, which is uniquely recoverable, speech tokenization largely discards the information of raw waveforms.\nTwo major approaches have been proposed to solve this problem.\nThe first approach uses a neural vocoder for directly reconstructing raw waveforms from speech tokens~(e.g., \\citealt{zhang-etal-2023-speechgpt,chou-etal-2023-toward,hassid2023twist}).\nThe second approach uses a pretrained neural audio codec, which requires an additional module to predict the codec's tokens~(e.g., \\citealt{rubenstein2023audiopalm,zhang2024speechgptgen}).\nWe adopt the first approach to reduce overall latency using HiFi-GAN~\\citep{kong2020hifigan}, a non-autoregressive neural vocoder that efficiently generates high-fidelity waveforms.\n\n\\subsection{Integrating LMs with a Speech Stream}\n\nPSLM is built on top of a pretrained decoder-only Transformer~\\citep{vaswani2017transformer}.\n\\revise{An overview of the PSLM architecture is provided in \\Figref{fig:architecture}.}\nWe add new input embedding and output projection layers to process speech tokens, while the structure of the intermediate Transformer layers remains unchanged.\n\\revise{The embeddings of text and speech tokens are summed before being fed to the Transformer layers.\nThe hidden features from the final Transformer layer are passed to two output projection layers to calculate the logits of the next text and speech tokens.}\nWe randomly initialize the weights of new embedding and projection layers.\n\nA challenge of joint text-speech modeling lies in the mismatch in their lengths.\nIn this study, we simply right-pad TQ and TA sequences with special \\texttt{[TEXT-PAD]} tokens to align their lengths with those of the SQ and SA sequences, respectively.\n\\revise{\nIn a preliminary experiment on the CoM-based architecture, we attempted to generate text tokens and their corresponding speech tokens alternatively in a similar manner to ELLA-V~\\citep{song2024ellav}; however, this approach led to frequent mispronunciation.\nThis is mainly because, in our case, the text is represented by tokens rather than phonemes; in some languages, the pronunciation of a character often changes according to subsequent characters, and a certain amount of lookahead is necessary to achieve accurate pronunciation.\nIn contrast, our alignment strategy allows the model to focus on text token generation initially and then refer to the generated text when producing the majority of speech tokens, leading to more accurate pronunciation.\n} \n\nOur \\prop is trained by minimizing the sum of cross entropy losses for each stream.\nWe include prompt tokens, comprising TQ and SQ, in the loss calculation.\nDuring inference, \\prop receives these prompt tokens and generates TA and SA in parallel.\n\\revise{Text and speech tokens are sampled independently from their respective distributions.}\n\n\\begin{figure}[t]\n  \\includegraphics[width=\\columnwidth]{fig/architecture.pdf}\n  \\vspace{-20pt}\n  \\caption{\\revise{Architecture of PSLM.}}\n  \\label{fig:architecture}\n  \\vspace{-10pt}\n\\end{figure}\n\n\\subsection{Introducing Multiple Speech Streams}\n\nFor further acceleration, we introduce multiple speech streams to \\prop.\nAssume that \\prop has $1 + S$ streams, one for text tokens and $S$ for speech tokens.\n\\revise{Given the original speech token sequence of length $N$, the $s$-th speech stream consists of the speech tokens with indices $s, s+S, s+2S, ..., s+MS$, where $s\\in \\{1,\\ldots,S\\}$ and $M=\\lfloor N/S \\rfloor - 1$.\nCompared to simply increasing the batch size, where the system's throughput improves but the latency for each instance remains unchanged, our approach reduces the sequence length handled by the Transformer layers to $1/S$, leading to an approximate $S$-fold speedup even in the single-instance scenario.}\n\nDuring training, simply summing the cross entropy losses for each stream makes the loss of text tokens less dominant, leading to poor text generation quality.\nTherefore, we introduce a weighted loss, where we multiply the loss for speech streams by $1/S$ to balance the weight of losses for text and speech streams.\n\n\\subsection{Streaming Inference with HiFi-GAN}\nFollowing \\citet{chen2022streaming}, we use HiFi-GAN for streaming inference; specifically, we provide partial speech tokens to generate waveform fragments.\nIn this study, we use non-causal convolution to maintain high speech quality.\nTherefore, the first speech fragment can be generated once $N_\\textrm{offset} = \\lfloor R / 2 \\rfloor + 1$ tokens are decoded, where $R$ denotes the receptive field of HiFi-GAN.\nImplementation details can be found in \\Appref{app:hifigan}.\n\n\\subsection{Overall Latency}\n\\label{sec:latency}\nWe define latency as the delay between the end of the user's utterance and the system's initial response.\nThe latency of conventional CoM-based systems $L_\\textrm{CoM}$ can be represented as follows:\n\\begin{align}\n    L_\\textrm{CoM} &= D_\\textrm{s2t} + D_\\textrm{SQ} + \\frac{N_\\textrm{dec}}{P} + D_\\textrm{t2s} \\\\\n    N_\\textrm{dec} &= N_\\textrm{TQ} + N_\\textrm{TA} + N_\\textrm{offset}\n    \\label{eq:latency_com}\n\\end{align}\nwhere $D_\\textrm{s2t}$, $D_\\textrm{SQ}$, and $D_\\textrm{t2s}$ denote the delays of speech tokenization, the prefill phase in LMs, and speech detokenization, respectively; $N_\\textrm{TQ}$ and $N_\\textrm{TA}$ denote the number of tokens in TQ and TA, respectively; and $P$ denotes the tokens per second (TPS) during the decode phase in LMs.\n\nOur \\prop eliminates the need for generating TQ and TA beforehand, although it requires to run external ASR to obtain TQ.\nHence, its latency $L_\\textrm{\\prop}$ can be represented as follows:\n\\begin{align}\n    L_\\textrm{\\prop} = D_\\textrm{ASR} + D_\\textrm{SQ} + \\frac{N_\\textrm{offset}}{P\\cdot S} + D_\\textrm{t2s}\n    \\label{eq:latency_pslm}\n\\end{align}\nwhere $D_\\textrm{ASR}$ denotes the ASR delay.\nHere $D_\\textrm{s2t}$ is omitted because speech tokenization can be performed in parallel with ASR.\n\n\\section{Experimental Setup}\n\\subsection{Dataset}\nWe used an internal dataset comprising 1.8M written QA pairs for training \\revise{all models.}\n\\revise{Since some of these samples, which were primarily crawled from the internet, were deemed unsuitable for evaluation, we used a publicly available Japanese dataset~\\citep{megagonlabs_instruction_ja} for evaluation.\nThis dataset was manually reviewed and consists of 669 diverse written QA pairs.\nWe further filtered the evaluation set by excluding samples whose TQ or TA exceeded 140 characters, the maximum number of characters observed in the training set.\nThe final evaluation set contained 396 samples.}\nFor both \\revise{the training and evaluation} sets, we constructed a spoken question answering (SQA) dataset by synthesizing SQ and SA using a well-trained single-speaker TTS system based on VITS~\\citep{kim2021vits}.\n\n\\subsection{Configuration}\n\\label{sec:config}\n\\paragraph{Tokenization and Detokenization}\nFor text tokenization, we used the tokenizer with a vocabulary size of 151,936 from rinna/nekomata-7b\\footnote{\\url{https://huggingface.co/rinna/nekomata-7b}}.\nFor speech tokenization, we applied $k$-means clustering with $k=512$ to 12-th layer features from rinna/japanese-hubert-base\\footnote{\\url{https://huggingface.co/rinna/japanese-hubert-base}}~\\citep{sawada2024release}, obtaining 50 speech tokens per second.\nFor speech detokenization, we trained discrete unit-based HiFi-GAN~\\citep{polyak2021speech} using pairs of synthesized speech waveforms of SQ and SA and their corresponding speech tokens.\nFor ASR, Whisper large-v3~\\citep{radford2023robust} with faster-whisper\\footnote{\\url{https://github.com/SYSTRAN/faster-whisper}} was used throughout our experiments.\n\n\\paragraph{Language Modeling}\nWe used rinna/nekomata-7b, a 32-layer 4096-hidden-size Transformer LM that was continuously pretrained from Qwen-7B~\\citep{bai2023qwen} on Japanese text, as the backbone of our models.\nWe implemented our models using the GPT-NeoX library~\\citep{gpt-neox-library}.\nUnless otherwise noted, models were trained for 50k steps with a batch size of 16 on 8 NVIDIA A100 GPUs using an Adam optimizer~\\citep{kingma2015adam} with a peak learning rate set to 1e-5.\nDuring inference, we set the temperature to 0.8 and applied top-$k$ and top-$p$ sampling with $k=60$ and $p=0.8$.\n\n\\subsection{Baselines}\n\n\\label{sec:baselines}\nWe involved three CoM-based baselines, which share the model weights but differ in their prompts during decoding: (1) \\method{CoM-SQ} receives only SQ, (2) \\method{CoM-ASR} receives SQ and transcribed TQ, and (3) \\method{CoM} receives SQ and gold TQ.\n\\revise{In our preliminary experiments, the three-stage training~\\citep{zhang-etal-2023-speechgpt} was not effective in our configuration; thus, we trained the model using the same configuration as described in \\Secref{sec:config}.}\n\n\\subsection{Evaluation Metrics}\n\\label{sec:metrics}\n\\paragraph{ChatGPT Scores}\nWe used OpenAI's GPT-3.5 Turbo API to evaluate response quality on a 5-point scale from 1 (bad) to 5 (excellent).\nThe prompt is described in \\Appref{app:prompt}.\nWe report the scores for TA and the transcription of SA as T-score and S-score, respectively.\n\n\\paragraph{Character Error Rate (CER)}\nWe calculated the character error rate between the generated TA and the transcription of SA to assess their alignment.\n\n\\paragraph{Failure Rate (FR)}\nWe counted failure cases such as (1) no \\texttt{[EOS]} token was generated before the total sequence length reached 2048, or (2) tokens were generated in the wrong modality, i.e., speech tokens in TQ and TA, or text tokens in SA. \n\n\\paragraph{Latency}\nWe simulated latency according to Equations \\ref{eq:latency_com} and \\ref{eq:latency_pslm} for each sample in the evaluation set, and reported the median values.\n\\revise{We set $D_\\textrm{s2t} = 0.05$, $D_\\textrm{SQ} = 0.05$, $D_\\textrm{ASR} = 0.2$, and $D_\\textrm{t2s} = 0.01$ based on measurements taken on a single NVIDIA A100 GPU.\nFor the TPS value $P$, the actual TPS varies depending on computing resources and optimization; 70 TPS was achieved with vLLM~\\citep{kwon2023vllm} optimization, and 25 TPS without it.\nMeanwhile, for streaming inference with HiFi-GAN, LMs need to generate 50 speech tokens per second.\nTherefore, we set $P$ to 50 in our simulations to match this requirement.}\n\n\\begin{table*}[t]\n\\caption{Automatic evaluation results. T-score and S-score represent the ChatGPT-based score for TA and transcribed SA, respectively. FR denotes the failure rate. Latency values in parentheses represent inputs involving gold TQ.}\n\\vspace{-5pt}\n\\label{tbl:auto_eval}\n\\begin{center}\n\\small\n\\begin{tabular}{l|l|l|ccccc}\n\\toprule\n\\textbf{Method} & \\textbf{Input modality} & \\textbf{Output Modality} & \\textbf{T-score}$\\uparrow$ & \\textbf{S-score}$\\uparrow$ & \\textbf{FR}$\\downarrow$ & \\textbf{CER}$\\downarrow$ & \\textbf{Latency [s]}$\\downarrow$\\\\\n\\midrule\n\\method{Ground Truth} & --- & --- & 4.00$\\pm$0.02 & 3.58$\\pm$0.06 & --- & 7.35 & ---\\\\ \\midrule\n\n\\method{CoM} & SQ $\\rightarrow$ TQ (Gold) & TA $\\rightarrow$ SA & 3.50$\\pm$0.09 & 3.27$\\pm$0.09 & 12.12 & 6.28 & (0.67) \\\\\n\\method{\\prop} & SQ, TQ (Gold) & TA, SA & 3.50$\\pm$0.08 & 3.22$\\pm$0.09 & 5.05 & 5.25 & (0.34) \\\\\\midrule\n\n\\method{CoM-SQ} & SQ & TQ $\\rightarrow$ TA $\\rightarrow$ SA & 3.12$\\pm$0.11 & 2.94$\\pm$0.10 & 15.91 & 7.83 & 1.03 \\\\\n\\method{CoM-ASR} & SQ $\\rightarrow$ TQ (ASR) & TA $\\rightarrow$ SA & 3.27$\\pm$0.10 & 3.07$\\pm$0.09 & 13.13 & 6.18 & 0.92 \\\\\n\\method{\\prop-ASR} & SQ, TQ (ASR) & TA, SA & 3.34$\\pm$0.09 & 3.05$\\pm$0.10 & 6.31 & 6.05 & 0.54 \\\\ \\midrule\n\n\\method{\\prop-2x} & SQ, TQ (Gold) & TA, SA & 3.50$\\pm$0.08 & 3.20$\\pm$0.09 & 4.29 & 6.39 & (0.20) \\\\\n\\method{\\prop-3x} & SQ, TQ (Gold) & TA, SA & 3.28$\\pm$0.10 & 2.99$\\pm$0.10 & 7.07 & 6.09 & (0.15) \\\\\n\\bottomrule\n\\end{tabular}\n\\end{center}\n\\vspace{-5pt}\n\\end{table*}\n\n\\paragraph{Human Rating}\nWe also conducted two subjective evaluations: one for text and the other for speech.\nIn the text evaluation, we presented pairs of gold TQ and generated TA, \\revise{and raters evaluated the naturalness of TA based on the same criteria used in the ChatGPT-based evaluation (Text Naturalness).}\nIn the speech evaluation, we presented gold SQ and generated SA successively, along with their TQ and TA, and asked the raters to evaluate (1) how natural the SA is as the speech of the TA \\revise{(Speech Naturalness)}, and (2) whether the response is fast enough \\revise{(Speed Score)}.\n\\revise{For better reproducibility, we provide the actual instruction used for speech evaluation in \\Appref{app:speecheval_instruction}.}\nThe duration of silence between SQ and SA was simulated in the manner described in \\Secref{sec:latency}, except for the Ground Truth where the silence duration was set to 200ms, the average turn-taking gap in human conversation~\\citep{levinson2015timing}.\nScores were rated on a 5-point scale.\nFifty samples were randomly chosen from the evaluation set, and twenty in-house workers rated twenty samples each.\n\n\\section{Results and Discussion}\n\\subsection{Automatic Evaluation}\n\\label{sec:auto_eval}\n\n\\paragraph{Comparison with Baselines}\nTo answer \\textbf{RQ1}, we compared the proposed method in two conditions, \\method{\\prop} and \\method{\\prop-ASR}, with the baselines described in \\Secref{sec:baselines}.\n\\method{\\prop} receives SQ and gold TQ, while \\method{\\prop-ASR} receives SQ and transcribed TQ.\n\\Tblref{tbl:auto_eval} summarizes the results.\nWhen gold TQ was given, \\method{\\prop} achieved comparable scores to \\method{CoM} and significantly improved latency.\nA similar trend was observed under more practical conditions where gold TQ was not available (\\method{\\prop-ASR} vs. \\method{CoM-ASR}).\nHowever, their scores were lower than those with gold TQ, and \\method{CoM-SQ} faced greater degradation.\nThese results suggest that ASR performance is crucial for response quality, and \\method{CoM-SQ} seems to have produced more ASR errors than Whisper.\nNevertheless, we conclude that PSLM maintains the response quality of CoM (\\textbf{RQ1}).\nWe also found that \\prop-based methods achieved lower FRs than CoM-based ones.\nEach stream of \\prop is dedicated to a single modality, which could have reduced the failures in generation.\nFurthermore, methods other than \\method{CoM-SQ} marked lower CERs than \\method{Ground Truth}.\nFrom this result, we confirmed that both CoM and \\prop can generate appropriate SA corresponding to TA.\n\n\\paragraph{Multiple Speech Streams}\nTo answer \\textbf{RQ2}, we trained \\prop variants with two (\\method{-2x}) or three (\\method{-3x}) speech streams\\footnote{\\method{PSLM-3x} was trained with a batch size of 4 due to the increased number of parameters.}.\n\\method{PSLM-2x} achieved comparable scores to \\method{PSLM}, whereas \\method{PSLM-3x} demonstrated significant degradation.\nFrom these results, we conclude that speech tokens can be decoded in up to two streams without quality degradation (\\textbf{RQ2}).\nAn ablation study can be found in \\Appref{app:ablation}.\n\n\\subsection{Human Evaluation}\nConsidering practical applicability to SQA, we manually evaluated three methods: \\method{CoM-SQ}, \\method{CoM-ASR}, and \\method{\\prop-ASR}, which do not rely on gold TQ, along with \\method{Ground-Truth}.\n\\Tblref{tbl:human_eval} shows the results.\nThe text response naturalness of \\method{\\prop-ASR} was comparable to \\method{CoM-ASR} and higher than \\method{CoM-SQ}, which is consistent with the automatic evaluation results.\nFor speech naturalness, all methods achieved higher scores than \\method{Ground-Truth}.\nThis result can be attributed to two reasons: (1) SA of \\method{Ground-Truth} are synthetic speech, which may include errors in pronunciation, intonation, and pauses, and (2) SA of \\method{Ground-Truth} are typically longer than those of other methods, incurring that one or two unnatural parts lowered the entire score.\nNevertheless, we confirmed that our approach can generate natural and faithful speech responses.\nFor response speed evaluation, \\method{\\prop-ASR} achieved a significantly higher score than \\method{CoM-ASR} and \\method{CoM-SQ}.\nThis finding verifies that the proposed method reduces latency both numerically and perceptibly.\nDetailed analysis can be found in the next subsection.\n\n\\begin{table}[t]\n\\caption{Human evaluation results.}\n\\vspace{-5pt}\n\\label{tbl:human_eval}\n\\begin{center}\n\\small\n\\begin{tabular}{l|ccc}\n\\toprule\n\\textbf{Method} & \\textbf{Text}$\\uparrow$ & \\textbf{Speech}$\\uparrow$ & \\textbf{Speed}$\\uparrow$\\\\\n\\midrule\n\\method{Ground Truth} & 4.08$\\pm$0.26 & 3.74$\\pm$0.19 & 4.73$\\pm$0.11 \\\\\n\\method{CoM-SQ} & 2.44$\\pm$0.29 & 4.04$\\pm$0.20 & 4.07$\\pm$0.23\\\\\n\\method{CoM-ASR} & 2.90$\\pm$0.30 & 3.94$\\pm$0.20 & 4.17$\\pm$0.22\\\\\n\\method{\\prop-ASR} & 3.08$\\pm$0.27 & 4.08$\\pm$0.20 & 4.57$\\pm$0.13\\\\\n\\bottomrule\n\\end{tabular}\n\\end{center}\n\\vspace{-10pt}\n\\end{table}\n\n\\subsection{Detailed Latency Analysis}\n\\revise{\nThe sequence length of TA, or $N_\\textrm{TA}$, is the most influential factor in overall latency of CoM-based systems, as TA must be generated before SA. \nThus, we investigated the overall latency by varying $N_\\textrm{TA}$.}\n\\Figref{fig:latency_vs_TAlen} shows the results.\nDue to the need for prior generation of TA, the latency of \\method{CoM-SQ} and \\method{CoM-ASR} increases linearly as TA length increases.\nIn contrast, the latency of \\method{\\prop-ASR} is constant because \\Eqref{eq:latency_pslm} does not include $N_\\textrm{TA}$, and \\method{\\prop-2x-ASR} further reduces the latency.\nThe gap between CoM-based and \\prop-based systems is remarkable when generating long TA, highlighting the effectiveness of generating text and speech tokens in parallel.\n\n\\begin{figure}[t]\n    \\begin{center}\n  \\includegraphics[width=\\columnwidth]{fig/latency.pdf}\n    \\vspace{-20pt}\n    \\caption{Latency vs. TA length for different methods and tokens per second (TPS). \\prop-2x-ASR (50 TPS) is omitted because its latency is identical to \\prop-ASR (100 TPS).}\n    \\label{fig:latency_vs_TAlen}\n    \\end{center}\n    \\vspace{-10pt}\n\\end{figure}\n\n\\section{Conclusion}\nIn this study, we proposed the Parallel Speech Language Model (PSLM), an LLM capable of generating text and speech tokens in parallel with multiple input-output streams, and investigated its impact on response quality and overall latency.\nThe experimental evaluations on spoken question answering demonstrated that the proposed method significantly reduces latency compared to existing methods while maintaining response quality.\nFuture work includes verifying the effectiveness of the proposed method on larger datasets and real speech data.\nAdditionally, extending the proposed method to multi-turn dialogues is an important research direction.\n\n\\section{Limitations}\nWe recognize several limitations of this study.\nFirst, \\prop sacrifices ASR capability for faster response, requiring an external ASR module to serve as a spoken dialogue system.\n\\revise{Although this dependency can complicate the system structure, it does not degrade the system's performance, provided that an appropriate ASR module is selected.\nThis is supported by the fact that CoM-ASR outperformed CoM-SQ, as described in \\Secref{sec:auto_eval}.} \nNevertheless, enabling ASR with the PSLM architecture can be an interesting research direction.\nSecond, we used single-speaker synthetic speech for SQ and SA, which lacks diversity in several aspects of speech such as accent, rhythm, emotion, and timbre.\nPractical applications may require to accept voices of arbitrary speakers, which we will address in future work.\nFinally, multi-turn dialogue settings were not investigated in our experiments.\nWhile SpeechGPT~\\citep{zhang-etal-2023-speechgpt} was not applied to multi-turn dialogue due to sequence length limitations, our models with multiple speech streams have the potential to perform multi-turn dialogue.\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2305.11000v2.tex",
        "arXiv-2402.05755v2.tex",
        "arXiv-2406.12428v2.tex"
    ],
    "group_id": "group_48",
    "response": "### Title: Advances in Large Language Models with Intrinsic Cross-Modal Conversational Abilities\n\n### Introduction\n\nThe field of large language models (LLMs) has seen significant advancements with the introduction of models like GPT-4 and LLaMA, which have demonstrated remarkable performance across various natural language processing tasks. However, the integration of these models with speech processing has been limited, primarily due to the cascade paradigm that separates speech and text processing, leading to inefficiencies in handling multi-modal inputs and outputs. This paradigm prevents inter-modal knowledge transfer, which is crucial for achieving true cross-modal perception and generation in artificial general intelligence (AGI). The current challenge lies in developing LLMs that can seamlessly integrate speech and text, thereby enhancing their capabilities to understand and generate content in multiple modalities. This summary explores three recent papers that address these challenges by proposing innovative approaches to train and utilize LLMs for cross-modal conversational tasks, focusing on the integration of speech and text, the handling of multi-modal instructions, and the reduction of response latency in spoken dialogue systems.\n\n### Main Content of Each Paper\n\n#### Paper 1: SpeechGPT\n\nSpeechGPT introduces a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-modal content. The model leverages discrete speech representations, which are derived from a self-supervised trained speech model like HuBERT, to unify the modality between speech and text. This unification is achieved by expanding the vocabulary of the LLM to include these discrete speech tokens, thus endowing the model with the ability to handle both modalities. The authors construct SpeechInstruct, a large-scale cross-modal speech instruction dataset, which includes both cross-modal instruction data and spoken dialogue data based on the chain-of-modality mechanism. This dataset consists of discrete unit-text pairs derived from ASR datasets and task descriptions generated using GPT-4. The model undergoes a three-stage training process: modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The first stage involves training the LLM on unlabeled speech data to enable speech comprehension, the second stage fine-tunes the model on paired speech and text data to improve cross-modal capabilities, and the third stage utilizes parameter-efficient LoRA fine-tuning for further modality alignment. The evaluation of SpeechGPT's performance is conducted through human evaluations and case analyses, demonstrating its strong ability to follow multi-modal human instructions and handle spoken dialogue tasks.\n\n#### Paper 2: SpiritLM\n\nSpiritLM presents a foundational multimodal language model that can freely mix text and speech. The model is based on a 7B pretrained text language model, LLaMA, which is extended to the speech modality by continuously training it on text and speech units. Unlike SpeechGPT, SpiritLM concatenates speech and text sequences as a single stream of tokens and trains them using a word-level interleaving method. The model comes in two versions: a Base version that uses speech phonetic units (HuBERT) and an Expressive version that models expressivity using pitch and style units in addition to phonetic units. The Expressive version is trained on a combination of HuBERT tokens, pitch tokens, and style tokens, enabling it to capture and generate more expressive speech. The training process involves a mixture of text-only, speech-only, and aligned speech+text datasets. The authors demonstrate that SpiritLM can learn new tasks in a few-shot fashion across modalities, including ASR, TTS, and Speech Classification. They also introduce a sentiment benchmark to evaluate the model's ability to preserve the sentiment of prompts within and across modalities. The results show that SpiritLM performs well in cross-modal tasks and can generate semantically consistent multimodal content while preserving the sentiment of the input.\n\n#### Paper 3: PSLM\n\nPSLM (Parallel Speech Language Model) proposes a method to reduce response latency in spoken dialogue systems by enabling parallel generation of text and speech tokens. The model is built on a pretrained decoder-only Transformer and introduces multiple speech streams to decode speech tokens in parallel, thereby accelerating the generation process. PSLM eliminates the need for generating text questions (TQ) and text answers (TA) before producing speech answers (SA), which is a common practice in existing models. The authors evaluate PSLM's performance on spoken question answering tasks and demonstrate that it can maintain response quality while significantly reducing latency. They also introduce a weighted loss during training to balance the contributions of text and speech streams, ensuring that the quality of text generation is not compromised. The evaluation includes both automatic metrics like ChatGPT scores and CER, as well as human ratings for naturalness and speed. The results show that PSLM can generate natural and faithful speech responses, and its performance is comparable to existing methods, with a notable reduction in latency.\n\n### Commonalities and Innovations\n\nAll three papers aim to enhance the cross-modal conversational abilities of large language models by integrating speech and text processing. They address the limitations of current speech-language models, which typically rely on the cascade paradigm, by proposing models that can handle both modalities simultaneously. \n\n- **Speech Discretization**: SpeechGPT and SpiritLM both use discrete speech representations, such as HuBERT tokens, to enable the LLM to perceive and generate speech. SpeechGPT constructs SpeechInstruct, a large-scale dataset for cross-modal instruction-following, while SpiritLM uses a combination of HuBERT, pitch, and style tokens to capture more expressive speech.\n- **Training Strategies**: SpeechGPT employs a three-stage training process to align speech and text modalities, whereas SpiritLM uses a continuous pretraining approach on a combination of text and speech datasets. PSLM, on the other hand, introduces multiple speech streams to enable parallel decoding of text and speech tokens, significantly reducing response latency.\n- **Evaluation Metrics**: The models are evaluated using a combination of automatic metrics (e.g., WER, CER) and human evaluations. SpeechGPT and SpiritLM focus on cross-modal instruction following and spoken dialogue tasks, while PSLM evaluates the effectiveness of its approach in reducing latency and maintaining response quality.\n\n### Comparison of Results and Discussion\n\nThe results from the three papers indicate that each model has unique strengths and limitations:\n\n- **SpeechGPT**: Demonstrates strong cross-modal instruction following and spoken dialogue abilities, but it requires the generation of a text-based response before producing a speech-based one. It also does not consider paralinguistic information like emotional tone in speech.\n- **SpiritLM**: Achieves promising results in cross-modal tasks and expressive speech generation, but its performance in text generation is slightly lower compared to its speech generation capabilities. SpiritLM also faces challenges in preserving the sentiment across different modalities, especially in the speech-to-text direction.\n- **PSLM**: Reduces response latency while maintaining high response quality, but it relies on an external ASR module to transcribe speech questions into text questions. This dependency can complicate the system structure, although it does not significantly degrade performance.\n\n### Conclusion\n\nThe main findings from these papers highlight the potential of combining discrete speech tokens into LLMs to achieve intrinsic cross-modal conversational abilities. SpeechGPT and SpiritLM both demonstrate the ability to follow multi-modal instructions and generate coherent responses in both speech and text modalities. PSLM, in particular, shows significant improvements in response latency without compromising on response quality. Future research directions include improving the alignment between speech and text modalities, enhancing the expressivity of speech generation, and extending these models to handle multi-turn dialogues and more diverse speech styles. Additionally, addressing the limitations of current models, such as the need for external ASR modules and the lack of consideration for paralinguistic information, will be crucial for advancing the field of multimodal LLMs."
}