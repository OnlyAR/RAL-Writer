{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Mixed supervision for surface-defect detection:\\\\from weakly to fully supervised learning}\n\n\\begin{document}\n\n\\title{Mixed supervision for surface-defect detection:\\\\from weakly to fully supervised learning}\n\n\\author{Jakob Bo\\v{z}i\\v{c}, Domen Tabernik and Danijel Sko\\v{c}aj\\\\\nUniversity of Ljubljana, Faculty of Computer and Information Science\\\\\nVe\\v{c}na pot 113, 1000 Ljubljana\\\\\n{\\tt\\small jakob.bozic@fri.uni-lj.si, domen.tabernik@fri.uni-lj.si}, \\\\ \n{\\tt\\small danijel.skocaj@fri.uni-lj.si}\n}\n\n\\maketitle\n\n\\begin{abstract}\nDeep-learning methods have recently started being employed for addressing surface-defect detection problems in industrial quality control. However, with a large amount of data needed for learning, often requiring high-precision labels, many industrial problems cannot be easily solved, or the cost of the solutions would significantly increase due to the annotation requirements. In this work, we relax heavy requirements of fully supervised learning methods and reduce the need for highly detailed annotations. By proposing a deep-learning architecture, we explore the use of annotations of different details ranging from weak (image-level) labels through mixed supervision to full (pixel-level) annotations on the task of surface-defect detection. The proposed end-to-end architecture is composed of two sub-networks yielding defect segmentation and classification results. The proposed method is evaluated on several datasets for industrial quality inspection: KolektorSDD, DAGM and Severstal Steel Defect. We also present a new dataset termed KolektorSDD2 with over 3000 images containing several types of defects, obtained while addressing a real-world industrial problem. We demonstrate state-of-the-art results on all four datasets. The proposed method outperforms all related approaches in fully supervised settings and also outperforms weakly-supervised methods when only image-level labels are available. We also show that mixed supervision with only a handful of fully annotated samples added to weakly labelled training images can result in performance comparable to the fully supervised model's performance but at a significantly lower annotation cost.\n\\end{abstract}\n\\maketitle\n\n\\section{Introduction}\n\nSurface-quality inspection of production items is an important part of industrial production processes. Traditionally, classical machine-vision methods have been applied to automate visual quality inspection processes, however, with the introduction of Industry 4.0 paradigm, deep-learning-based algorithms have started being employed~\\cite{Onchis2021,Yang2020,Tabernik2019JIM,Yu2019,Lin2018, Weimer2016}. Large capacity for complex features and easy adaptation to different products and defects without explicit feature hand-engineering made deep-learning models well suited for industrial applications. However, an essential aspect of the deep-learning approaches is the need for a large amount of annotated data that can often be difficult to obtain in an industrial setting. In particular, the data acquisition is often constrained by the insufficient availability of the defective samples and limitations in the image labeling process. In this paper, we explore how these limitations can be addressed with weakly and fully supervised learning combined into a unified approach of mixed supervision for industrial surface-defect detection.\n\nIn large-scale production, the quantity of the items is not the main issue but the ratio between non-defective and defective samples is by design heavily skewed towards the defect-free items. Often, anomalous items are a hundred, a thousand, or a even million-fold less likely to occur than the non-anomalous ones, leading to a limited set of defective items for training. Certain defect types can also be very rare with only a handful of samples available. This represents a major problem for supervised deep-learning methods that aim to learn specific characteristics of the defects from a large set of annotated samples.\n\nThe annotations themselves present an additional problem as they need to be detailed enough to differentiate the anomalous image regions from the regular ones properly. Pixel-level annotations are needed, but in practice, they are often difficult to produce. This is particularly problematic when it is challenging to explicitly define the boundaries between the defects and the regular surface appearance. Moreover, creating accurate pixel-level annotations is tiresome and costly to produce even when the boundaries are clear. Therefore, it is essential to minimize the labelling effort by decreasing the required amount of annotations and reducing the expected labels' precision.\n\nVarious unsupervised~\\cite{Staar2018,uninformedStudents,Zavrtanik2020} and weakly supervised~\\cite{Lin2018,CADN,Zhu2019} deep-learning approaches have been developed to reduce the need for a large amount of annotated data. The former are designed to train the models using defect-free images only, while the latter utilise weak labels and do not require pixel-level annotations. \nAlthough both approaches significantly reduce the cost of acquiring the annotated data, they significantly underperform in comparison with fully supervised methods on the defect detection task. Moreover, for many industrial problems, a small amount of fully annotated data is often available and can be used for training the models to improve the performance. This can result in a mixed supervision mode with some fully labeled samples and a number of weakly labeled ones as depicted in Fig.~\\ref{fig:supervision-types}.\n\nUnsupervised and weakly supervised methods are not able to utilize these available data. Mixed supervision has been applied on other computer vision tasks, such as image segmentation~\\cite{Souly2017,Mlynarski2019}, however, it has not yet been considered for industrial surface-defect detection.\n\nIn this work, we focus on a deep-learning approach suitable for industrial quality-control problems with varying availability of the defective samples and their annotations. In particular, we propose a deep-learning model for anomaly detection trained with weak supervision at the image-level labels, while at the same time utilising full supervision at the pixel-level labels when available. The proposed network can therefore work in a mixed supervision mode, fully exploiting the data and annotations available. By changing the amount and details of the required labels, the approach provides an option to find the most suitable trade-off between the annotation cost and classification performance. The mixed supervision is realised by implementing an end-to-end architecture with two sub-networks that are being simultaneously trained utilizing pixel-level annotations in the first sub-network and image-level annotations in the second sub-network.\nWhile pixel-level labels can be utilized during the training to further increase the performance of the proposed method, our primary goal is not to segment the defects, but rather to identify images containing defective surfaces. In addition, we also account for spatial uncertainty in coarse region-based annotations to further reduce the annotation cost.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\columnwidth]{images/supervision.pdf}\n    \\caption{Visualization of several types of supervision and their required labelling effort.} \n    \\label{fig:supervision-types}\n\\end{figure}\n\nWe performed an extensive evaluation of the proposed approach on several industrial problems. We demonstrate the performance on DAGM~\\cite{Weimer2016}, KolektorSDD~\\cite{Tabernik2019JIM} and Severstal Steel~\\cite{SeverstalSteel2019} datasets. Due to the lack of a real-world, unsaturated, large, and well-annotated surface-defect dataset, we also compiled and made publicly available a novel dataset termed \\textit{KolektorSDD2}%\\footnote{The dataset is publicly available at \\url{https://www.vicos.si/Downloads/KolektorSDD2}}\n. It is based on a practical, real-world example, consisting of over 3000 images containing several defect types of various difficulty. Images have been carefully annotated to facilitate accurate benchmarking of surface-defect-detection methods.\n \n\nThe proposed model outperforms related unsupervised and weakly supervised approaches when only weakly labeled annotations are available, while in the fully supervised scenario it also outperforms all other related defect-detection methods.\nMoreover, the proposed model demonstrates a significant performance improvement when only a few fully annotated images are added in mixed supervision. This often results in performance comparable to the results of fully supervised models but with a significantly reduced annotation cost. \n\nThe remainder of this paper is structured as follows: In Section~\\ref{sec:related-work}, we present the related work, followed by the description of the proposed network with mixed supervision for surface-defect detection in Section~\\ref{sec:method}.\nWe present the details of the experimental setup and evaluation datasets in Section~\\ref{sec:experiment-setup}, while we present detailed evaluation results in Section~\\ref{sec:results}. We conclude with a discussion in Section~\\ref{sec:conclusion}.\n\n\\section{Related work} \\label{sec:related-work}\n\n\\paragraph{Fully supervised defect detection} Several related works explored the use of deep-learning for industrial anomaly detection and categorisation~\\cite{Onchis2021, Lin2020, Huang2020, Yu2019, Tabernik2019JIM, Racki2018,Lin2018, Wang2018c, Kim2017_DAGM}, including the early work of Masci et al.~\\cite{Masci2012} using a shallow network for steel defect classification, and a more comprehensive study of a modern deep network architecture by Weimer et al.~\\cite{Weimer2016}. From recent work, Kim et al.~\\cite{Kim2017_DAGM} used a VGG16 architecture pre-trained on general images for optical inspection of surfaces, while Wang et al.~\\cite{Wang2018c} applied a custom 11-layer network for the same task. Ra\\v{c}ki et al.~\\cite{Racki2018} further proposed to improve the efficiency of patch-based processing from~\\cite{Weimer2016} with a fully convolutional architecture and proposed a two-stage network architecture with a segmentation net for pixel-wise localization of the error and a classification network for per-image defect detection. \nIn our more recent work~\\cite{Tabernik2019JIM}, we  performed an extensive study of the two-stage architecture with several additional improvements and showed the state-of-the-art results that outperformed others such as U-Net~\\cite{Ronneberger2015} and DeepLabv3~\\cite{Chen2017} on a real-world case of anomaly detection problem. We also extended this work and presented an end-to-end learning method for the two-stage approach~\\cite{bozic2020}, however still in a fully-supervised regime, without considering the task in the context of mixed or weakly supervised learning.\nDong et al.~\\cite{Dong2020} also used the U-Net architecture but combined it with SVM for classification and random forests for detection. Other recent approaches also explored lightweight networks~\\cite{Yang2020, Lin2020, Huang2020, Liu2020a}. Lin et al.~\\cite{Lin2020} used a compact multi-scale cascade CNN termed MobileNet-v2-dense for surface-defect detection while Huang et al.~\\cite{Huang2020} proposed an even more lightweight network using atrous spatial pyramid pooling (ASPP) and depthwise separable convolution.\n\n\\paragraph{Unsupervised learning}\n\nIn unsupervised learning, annotations are not needed (and are not taken into account even when available) and features are learned from either reconstruction objective~\\cite{Kingma2014,Chen2017b}, adversarial loss~\\cite{Goodfellow2014} or similar self-supervised objective~\\cite{Croitoru2017,Wang2017,Zhang2016a}.\nIn unsupervised anomaly detection solutions, the models are usually trained considering only non-anomalous image by applying out-of-distribution detection \nof anomalies as a significant deviations in features. Various methods based on this principle were proposed, such as AnoGAN~\\cite{Schlegl2017} and its successor f-AnoGAN~\\cite{Schlegl2019} that utilize Generative Adversarial Networks, or a deep-metric-learning-based approach with triplet loss that learns features of non-anomalous samples~\\cite{Staar2018}, or approach that transfers pre-trained discriminative latent embedding into a smaller network using knowledge transfer for out-of-distribution detection~\\cite{uninformedStudents}, termed Uninformed Students. The latter achieved state-of-the-art results in unsupervised anomaly detection on the MVTec dataset~\\cite{Bergmann2019}, which, however, only partially reflects the complexity of real-world industrial examples.\n\n\\paragraph{Weakly supervised learning}\nVarious weakly supervised deep-learning approaches have been developed in the context of semantic segmentation and object detection~\\cite{Pathak2014,Saleh2016,Bearman2016,Li2018,Wan2018,Wan2019}. In early applications, convolutional neural networks were trained with image-tags using Multiple Instance Learning (MIL)~\\cite{Pathak2014} or with constrained optimization as in Constrained CNN~\\cite{Pathak2015}. The approach by Seleh et al.~\\cite{Saleh2016} further used dense conditional random fields to generate foreground/background masks that act as priors on an object, while Bearman et al.~\\cite{Bearman2016} used a single-pixel point label of object location instead of image-tags. Ge et al.~\\cite{Ge2020} used a segmentation-aggregation framework learned from weakly annotated visual data and applied it to insulator detection on power transmission lines. Others utilized class activation maps (CAM)~\\cite{Zhou2015}. Zhu et al.~\\cite{Zhu2019} applied CAM for instance segmentation, while Diba et al.~\\cite{Diba2017} simultaneously addressed image classification, object detection, and semantic segmentation, where CAM from image classification is used in a separate cascaded network to improve the last two tasks. \n\nClass activation maps were also applied to anomaly detection. Lin et al.~\\cite{Lin2018} addressed defect detection in LED chips using CAM from the AlexNet architecture~\\cite{Zhou2015} to localize the defects, but learning only on the image-level labels. Zhang et al.~\\cite{CADN} extended CAM for defect localization with bounding box prediction in their proposed CADN model. Their model directly predicts the bounding boxes from category-aware heatmaps while also using knowledge distillation to reduce the complexity of the final inference model. However, both methods do not consider pixel-level labels in the learning process, thus failing to utilize this information when available.\n\n\\paragraph{Mixed supervision} Several related approaches also considered learning with different precision of labels. Souly et al.~\\cite{Souly2017} combined fully labeled segmentation masks with unlabeled images for pixel-wise semantic segmentation tasks. They train the model in adversarial manner by generating images with GAN and include any provided weak, image-level labels to the discriminator in GAN that further improves the semantic segmentation. Mlynarski et al.~\\cite{Mlynarski2019} addressed the problem of segmenting brain tumors from magnetic resonance images. They proposed to use fully segmented images and combine them with weakly annotated image-level information. \nThey focus on the goal of segmenting brain tumor images, while our primary concern is image-level anomaly detection in the industrial surface-defect-detection domain. They also do not perform any analysis of different mixtures of the weakly and fully supervised learning, which is the central point of this paper.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\columnwidth]{images/architecture.pdf}\n    \\caption{Proposed architecture with two sub-networks suitable for mixed supervision.} \n  \\label{fig:arch}\n\\end{figure}\n\n\\section{Anomaly detection with mixed supervision} \\label{sec:method}\n\nIn this section, we present a deep-learning model that can be trained on a mixture of fully (pixel-level) and weakly (image-level) labeled data.\nLearning on weakly labeled data requires a model capable of utilizing segmentation ground-truth/masks when they are available, however, it also needs to utilize image-level, i.e. class-only, labels as well.\nThe proposed model is based on our previous architecture with two sub-networks~\\cite{Tabernik2019JIM, bozic2020}, in which the segmentation sub-network utilizes fine pixel-level information and the classification sub-network utilizes coarse image-level information.\nThe overall architecture with two sub-networks is depicted in Fig.~\\ref{fig:arch}, with the architectural details of each sub-network shown in Tab.~\\ref{tab:arch}.\n\n\\subsection{Mixed supervision with end-to-end learning}\n\nLearning from a mixture of weakly labeled and fully labeled samples is only possible when both the classification and the segmentation sub-networks are trained simultaneously. \n\n\\begin{table}[]\n\\footnotesize\n    \\resizebox{\\columnwidth}{!}{%\n    \\centering\n            \\begin{tabular}{lcc|lcc}\n        \\toprule\n        \\multicolumn{3}{c}{\\textit{\\textbf{Segmentation sub-network}}}\t\t& \\multicolumn{3}{c}{\\textit{\\textbf{Classification sub-network}}}\t\\\\\n        \\midrule\n    \t\\textit{Layer}\t        & \\textit{Kernel size}  & \\textit{Features}\t& \\textit{Layer}\t            & \\textit{Kernel size}\t&\t\\textit{Features}\t\\\\\n    \t\\midrule\n    \t\\multicolumn{2}{l}{\\textit{Input:} image} & 3/1 & \\multicolumn{2}{l}{\\textit{Input:} $[S_f, S_h$]} & 1025\t\\\\\n    \t\\midrule\n    \t2x Conv2D\t    & 5x5\t& 32\t& Max-pool    \t    & 2x2\t& 1025\t\\\\\n    \tMax-pool    \t& 2x2\t& 32\t& Conv2D\t        & 5x5\t& 8\t\\\\\n    \t\\midrule\n    \t3x Conv2D   \t& 5x5\t& 64\t& Max-pool    \t    & 2x2\t& 8\t\\\\\n    \tMax-pool    \t& 2x2\t& 64\t& Conv2D\t        & 5x5\t& 16\t\\\\\n    \t\\midrule\n    \t4x Conv2D   \t& 5x5\t& 64\t& Max-pool\t        & 2x2\t& 16\t\\\\\n    \tMax-pool    \t& 2x2\t& 64\t& Conv2D ($C_f$)    & 5x5\t& 32\t\\\\\n    \t\\midrule\n        Conv2D ($S_f$)  &\t5x5\t&\t1024 & \n        \\multicolumn{2}{l}{\\makecell[l]{$\\left[\\mathcal{G}_a(C_f),\\mathcal{G}_m(C_f),\\mathcal{G}_a(S_h),\\mathcal{G}_m(S_h)\\right]$}} &\t66\t\\\\\n        Conv2D ($S_h)$\t&\t1x1\t&\t1\t& \\multicolumn{2}{l}{Fully connected ($C_p$)}    & 1\t\\\\\n    \t\\bottomrule\n        \\end{tabular}\n    }\n    \\caption{Architecture details for segmentation and classification sub-networks, in which \\textit{Features} column represent the number of output features.     $\\mathcal{G}_{a}$ and $\\mathcal{G}_{m}$ represent global average and max pooling operations. Outputs of the network are a segmentation map $S_h$ and a classification prediction $C_p$.\n    }\n    \\label{tab:arch}\n\\end{table}\n\\paragraph{End-to-end learning}\nWe combine the segmentation and the classification losses into a single unified loss, which allows for a simultaneous learning in an end-to-end manner. The combined loss is defined as:\n\\begin{equation} \\label{eq:loss_total}\n    \\mathcal{L}_{total} = \\lambda \\cdot \\gamma \\cdot \\mathcal{L}_{seg} + (1-\\lambda) \\cdot \\delta \\cdot  \\mathcal{L}_{cls},\n\\end{equation}\nwhere $\\mathcal{L}_{seg}$ and $\\mathcal{L}_{cls}$ represent segmentation and classification losses, respectively. For both, we use the cross-entropy loss. The remaining parameters are: $\\delta$ as an additional classification loss weight, $\\gamma$ as an indicator of the presence of pixel-level annotation and $\\lambda$ as a balancing factor that balances the contribution of each sub-network in the final loss. Note that $\\lambda$, $\\gamma$ and $\\delta$ do not replace the learning rate $\\eta$ in SGD, but complement it.\n$\\delta$ enables us to balance the contributions of both losses, which can be on different scales since the segmentation loss is averaged over all pixels, most of which are non-anomalous.\n\nWe also address the problem of learning the classification network on the initial unstable segmentation features by learning only the segmentation at the beginning and gradually progress towards learning only the classification at the end. We formulate this by computing their balancing factors as a simple linear function:\n\\begin{align}\n    \\lambda &= 1 - \\nicefrac{n}{n_{ep}},\n\\end{align}\nwhere $n$ is the current-epoch index and $n_{ep}$ is the number of all training epochs. Without the gradual balancing of both losses, the learning would in some cases result in exploding gradients. We term the process of gradually shifting the training focus from segmentation to classification as the \\textit{dynamically balanced loss}. \nAdditionally, using lower $\\delta$ values can further reduce the issue of learning on the noisy segmentation features early on.\n\n\\paragraph{Using weakly labeled data}\nThe proposed end-to-end learning pipeline is designed to enable utilisation of the weakly labeled data alongside fully labeled one. Such adoption of mixed supervision allows us to take a full advantage of any pixel-level labels when available, which weakly and unsupervised methods are not capable of using. We use $\\gamma$ from Eq.~\\ref{eq:loss_total} to control the learning of the segmentation sub-network based on the presence of pixel-level annotation:\n\\begin{equation}\n    \\gamma = \\begin{cases} \n    1 &\\text{ negative image, }\\\\\n    1 &\\text{ pos. image with pixel-level label,  }\\\\\n    0 &\\text{ pos. image with no pixel-level label.  }\\\\\n    \\end{cases}\n\\end{equation}\nWe disable the segmentation learning only when the pixel-level label for an anomalous (positive) image is not available. For non-anomalous (negative) training samples, the segmentation output should always be zero for all pixels, therefore, segmentation learning can still be performed.\nThis allows us to treat non-anomalous training samples as fully labeled samples and enables us to train the segmentation sub-network in the supervised  mode even in the absence of the pixel-level annotations during weakly supervised learning in the case of defect-free images.\n\n\\paragraph{Gradient-flow adjustments}\n\nWe stop the gradient-flow from the classification layers through the segmentation sub-network, which is required to stabilize the learning and enables training in a single end-to-end manner. During the initial phases of the training, the segmentation sub-network does not yet produce meaningful outputs, hence, neither does the classification sub-network, therefore, error gradients back-propagating from the classification layers can negatively affect the segmentation part. We propose to completely stop those gradients, thus preventing the classification sub-network from changing the segmentation layers. We achieve this by stopping the error back-propagation at two points that connect segmentation and classification networks. The primary point is the use of segmentation features in the classification network. This is depicted by bottom diode symbol in Fig.~\\ref{fig:arch}. The second point, at which we stop error gradients is the max/avg-pooling shortcut used by the classification sub-network. This is depicted with the top diode symbol in Fig~\\ref{fig:arch}. Those shortcuts utilize the segmentation sub-network's output map to speed-up the classification learning. Propagating gradients back through them would add error gradients to the segmentation's output map, however, this is unnecessary due to the already available pixel-level ground-truth for those features. Without gradient-flow adjustments, we observed a drop in performance in fully supervised scenario and greater instabilities during the training in weak and mixed supervision.\n\n\\begin{figure}%[t!]\n    \\begin{center}\n        \\includegraphics[width=\\columnwidth]{images/dist_trf_grad.png}\n    \\end{center}\n    \\caption{Segmentation loss weight mask obtained by applying distance transform algorithm on the label. Whiter shades on the segmentation loss mask indicate pixels with greater weight.} \n  \\label{fig:seg_loss_mask}\n\\end{figure}\n\\subsection{Considering spatial label uncertainty}\n\nWhen only approximate, region-based labels are available, such as shown in Fig.~\\ref{fig:seg_loss_mask}, we propose to consider different pixels of the annotated defective regions differently. In particular, more attention is given to the center of the annotated regions and less to the outer parts. This alleviates the ambiguity arising at the edges of the defects where it is very uncertain whether the defect is present or not. This is implemented by weighting the segmentation loss accordingly. We weight the influence of each pixel at positive labels in accordance with its distance to the nearest negatively labelled pixel by using the distance transform algorithm. \n\nWe formulate weighting of the positive pixels as:\n\\begin{equation}\\label{eq:EDT}\n    \\mathcal{L}_{seg}(pix) = \\Omega\\left(\\frac{\\mathcal{D}(pix)}{\\mathcal{D}(pix_{max})}\\right) \\cdot \\mathcal{\\hat L}(pix),\n\\end{equation}\nwhere $\\mathcal{\\hat L}(pix)$ is the original loss of the pixel, $\\nicefrac{\\mathcal{D}(pix)}{\\mathcal{D}(pix_{max})}$ is the distance to the nearest negative pixel normalized by the maximum distance value within the ground-truth region and $\\Omega(x)$ is a scaling function that converts the relative distance value into the weight for the loss. In general, the scaling function $\\Omega(x)$ can be defined differently depending on the defect and annotation type, however, we have found that a simple power function provides enough flexibility for different defect types:\n\\begin{equation}\n    \\Omega(x) = w_{pos} \\cdot x^p,\n\\end{equation}\nwhere $p$ controls the rate of decreasing the pixel importance as it gets further away from the center, while $w_{pos}$ is an additional scalar weight for all positive pixels. We have often found $p=1$ or $p=2$ as best performing, depending on the annotation type. Examples of a segmentation mask and weights are depicted in Fig.~\\ref{fig:seg_loss_mask}. Note, that weights for negatively labeled pixels remain $1$. \n\n\\begin{table}[]\n\\resizebox{\\columnwidth}{!}{%\n    \\centering\n\\begin{tabular}{llcccc}\n        \\toprule\n        \\textit{Dataset}            & \\textit{Subset}  & \\textit{\\# images}  & \\textit{\\makecell{\\# classes}} & \\textit{\\makecell{\\# defect\\\\types}}  & \\textit{Anotations}  \\\\\n            \\midrule\n        \\multirow{2}{*}{\\textit{\\textbf{\\makecell{DAGM\\\\1-6}}}}    & \\textit{positive}     & 450  & \\multirow{2}{*}{6} & \\multirow{2}{*}{6} & \\multirow{2}{*}{ellipse} \\\\\n                                    & \\textit{negative} & 3000   &  & &  \\\\\n        \\vspace{-1em}\\\\\n        \\multirow{2}{*}{\\textit{\\textbf{\\makecell{DAGM\\\\7-10}}}}    & \\textit{positive}     & 600 & \\multirow{2}{*}{4} & \\multirow{2}{*}{4} & \\multirow{2}{*}{ellipse} \\\\\n                                    & \\textit{negative} & 4000   &  & &  \\\\\n        \\vspace{-0.5em}\\\\\n        \\multirow{2}{*}{\\textit{\\textbf{KSDD}}}    & \\textit{positive}     & 52 & \\multirow{2}{*}{1} & \\multirow{2}{*}{1} & \\multirow{2}{*}{\\makecell{rotated\\\\bounding box}} \\\\\n                                    & \\textit{negative} & 347    &  & & \\\\\n        \\vspace{-0.5em}\\\\\n        \\multirow{2}{*}{\\textit{\\textbf{KSDD2}}}    & \\textit{positive}     & 356 & \\multirow{2}{*}{1} & \\multirow{2}{*}{$>5$} & \\multirow{2}{*}{\\makecell{fine}} \\\\\n                                    & \\textit{negative} & 2979   &  & &  \\\\\n        \\vspace{-0.5em}\\\\\n        \\multirow{2}{*}{\\textit{\\textbf{\\makecell[l]{Severstal\\\\Steel}}}}    & \\textit{positive}     & 4759  & \\multirow{2}{*}{1} & \\multirow{2}{*}{$>5$} & \\multirow{2}{*}{\\makecell{fine or rotated\\\\bounding box}} \\\\\n                                    & \\textit{negative} & 6666   & & &  \\\\\n        \\bottomrule                            \n        \\end{tabular}\n    }\n    \\caption{Details of the the evaluation datasets.}\n    \\label{tab:dataset-sizes}\n    \n\\end{table}\n\n\\section{Experimental setup} \\label{sec:experiment-setup}\n\nIn this section, we detail the datasets and the performance metrics used to evaluate the proposed method as well as provide additional implementation details.\n\n\\subsection{Datasets} \\label{sec:datasets}\n\nWe performed an extensive evaluation of the proposed method on four benchmark datasets. The summary of individual datasets is shown in Tab.~\\ref{tab:dataset-sizes}, while additional details are provided below. A couple of images from all four datasets are presented in Figs.~\\ref{fig:samples_ksdd2_train}, \\ref{fig:samples_dagm}, \\ref{fig:samples_ksdd}, \\ref{fig:samples_ksdd2} and \\ref{fig:samples_steel}.\n\n\\paragraph{\\textbf{DAGM}}\nThe DAGM~\\cite{Weimer2016} dataset is a well-known benchmark dataset for surface-defect detection. \nIt contains grayscale images of ten different computer-generated surfaces and various defects, such as scratches or spots.\nEach surface is treated as a binary-classification problem. Initially, six classes were presented while four additional ones were introduced later; consequently, some related methods report results only on the first six classes, while others report on all ten of them. \n\n\\paragraph{\\textbf{KolektorSDD}}\nThe KolektorSDD~\\cite{Tabernik2019JIM} dataset contains grayscale images of a real-world production item; many of them contain visible surface cracks. Due to the small sample size, the images are split into three folds as in~\\cite{Tabernik2019JIM}, while final results are reported as an average of three-fold cross validation.\n\n\\paragraph{\\textbf{KolektorSDD2}}\nSince the above mentioned datasets have been practically solved, and there is a huge need for real-world, reliable, complex and well annotated surface-detection datasets that would enable a fair comparison between different approaches, we compiled a novel dataset Kolektor Surface-Defect Dataset 2, abbreviated as KolektorSDD2\\footnote{The dataset is publicly available at \\url{https://www.vicos.si/Downloads/KolektorSDD2}}.\nThe dataset is constructed from color images of defective production items, captured with a visual inspection system, that were provided and partially annotated by our industrial partner Kolektor Group d.o.o.\nImages for the proposed dataset were captured in a controlled environment and are of similar size, approximately 230 pixels wide and 630 pixels high. \nThe dataset is split into the train and the test subsets, with 2085 negative and 246 positive samples in the train, and 894 negative and 110 positive samples in the test subset.\nDefects are annotated with fine-grained segmentation masks and vary in shape, size and color, ranging from small scratches and minor spots to large surface imperfections.\nSeveral images from this dataset are shown in Fig.~\\ref{fig:samples_ksdd2_train}.\n\n\\begin{figure}%[h]\n    \\begin{center}\n        \\includegraphics[width=\\columnwidth]{images/ksdd2_train.jpg}\n    \\end{center}\n    \\caption{Examples of training images from KolektorSDD2 \n    dataset with pixel-wise annotations shown in red-overlay region.\n    } \n    \\label{fig:samples_ksdd2_train}\n\\end{figure}\n\\paragraph{\\textbf{Severstal Steel defect dataset}}\n\nThe Severstal Steel defect dataset~\\cite{SeverstalSteel2019} is significantly larger than the other three datasets, containing 12,568 grayscale images in 4 classes, with various kinds of defects. We use only a subset of the dataset in our evaluation. In particular, we use all negative images but consider only positive images with the most common defect class present in the image (class 3). \nThe defects are very diverse in size, shape and appearance, ranging from scratches and dents to excess material.\nAlthough the dataset is fairly large and diverse, some defects are quite ambiguous and may not be properly annotated as observed in the third example in Fig.~\\ref{fig:samples_steel}.\n\n\\subsection{Performance metrics}\n\nIn all experiments, we focus on evaluating per-image classification metrics, which are the most relevant metrics in the industrial quality control, since they decide whether to keep or discard the inspected items. In particular, we mostly measure the performance in terms of the average precision (AP), which is calculated as the area under the precision-recall curve. For the DAGM dataset, we also report several other metrics that are used to report the results in related literature, to enable a fair comparison of the proposed method with related approaches.\n\n\\subsection{Implementation details}\n\nThe proposed architecture is implemented\\footnote{Implementation is available on github: \\url{https://github.com/vicoslab/mixed-segdec-net-comind2021}} in PyTorch framework. In all experiments, the network is trained with stochastic gradient descent, with no momentum and with no weight decay. \nFor all experiments, we disable the gradient flow from classification network to segmentation network and employ loss weighting for positive pixels.\nWe enable dynamically balanced loss for all cases with mixed supervision and fully supervised learning, but disable it when only weak labels are used. Since pixel-level labels are never available in weakly supervised case, it has proven more useful to immediately start learning classification layers instead of delaying it with the gradual increase of $\\lambda$.\nTo account for the unbalanced datasets, we employ undersampling of negative samples, in each training epoch we sample a selection of negative samples of equal size as the positive subset. \nWe also ensure that over the course of learning, all negative images are used approximately as often.\nWe also apply morphological dilation to the annotations in datasets where they are narrow and small in order to enlarge the label and facilitate distance-transform weighting. This has often proven better than using narrow annotations, as small defects can be difficult to precisely annotate, and, for some types of defects, the boundary between the defect and the background can not be precisely defined. Furthermore, distance transform assigns very small weights to the pixels near the edge of defects, thus dilating the annotations increases the weights for those pixels.\n\n\\subsection{Training and inference time}\nThe proposed method is capable of real-time detection, achieving 57 fps on $512\\times512$ images (DAGM), 23 fps on $512\\times1408$ images (KSDD), 85 fps on $232\\times640$ images (KSDD2) and 36 fps on $1600\\times256$ images (Severstal Steel), running on a single Nvidia RTX2080Ti GPU. The training time depends on the dataset size. In our experiments, training a model on DAGM took 35 minutes (7 or 10 classes), on Severstal Steel ($N_{all}=3000$) it took 460 minutes, while for KSDD2 and KSDD (for a single fold) it took 15 and 32 minutes respectively.\n\n\\subsection{Related methods}\n\nWe compare our approach against several related methods that have reported their results on DAGM~\\cite{Staar2018, CADN, Kim2017_DAGM, Racki2018, Weimer2016, Lin2020, Wang2018c, Huang2020, Liu2020a} and KolektorSDD~\\cite{Dong2020, Tabernik2019JIM, Liu2020} in the literature. Additionally, we evaluate two state-of-the-art unsupervised methods that we apply to all four datasets. Below, we provide more details on the implementation of the two unsupervised methods.\n\n\\paragraph{Uninformed students}\nSince the code for Uninformed-students~\\cite{uninformedStudents} is not publicly available, we implemented our own version, which attains results comparable to the ones reported in the literature. We additionally opted to use a per-image defect score for classification similar to~\\cite{Nguyen2019Scoring}, which slightly improved results compared to the original implementation. For each image, we performed a $21\\times21$ pixel average pooling with $\\text{stride}=1$, to obtain patch scores, and then calculated the final image score as $s=\\nicefrac{max(patch)}{(1+avg(pixel))}$, i.e. we took maximum patch score divided by average pixel score plus one.\nThis increased the AP in all four datasets.\n\n\\begin{table}[]\n\\resizebox{\\columnwidth}{!}{%\n\\begin{tabular}{@{}lccccccccc@{}}\n\\toprule\n\\multirow{2}{*}{Method}                & \\multirow{2}{*}{Type} & \\multicolumn{4}{c}{10 classes} & & \\multicolumn{2}{c}{6 classes} \\\\ \\cmidrule(l){3-6} \\cmidrule(l){7-9} \n                                       &                       & AP     & AUC   & F1    & CA    & & CA      & mAcc    \\\\ \\midrule\nf-AnoGAN~\\cite{fAnoGAN}                & US                    & 19.5   & 57.5  & 27.8     & 79.7    & & 81.7     & 54.6    \\\\\nUninf. stud.~\\cite{uninformedStudents} & US                    & 66.8   & 86.4  & 67.1  & 84.3  & & 79.3     & 78.5    \\\\ \nStaar~\\cite{Staar2018}                 & US                    & -      & 83.0  & -     & -     &  & -        & -       \\\\ \\midrule\nCADN-W18~\\cite{CADN}                               & WS                    & -      & -     & 63.2  & 86.2  & & -        & -       \\\\\nCADN-W18(KD)                           & WS                    & -      & -     & 65.8  & 87.6  & & -        & -       \\\\\nCADN-W32~\\cite{CADN}                   & WS                    & -      & -     & 69.0  & 89.1  & & -        & -       \\\\\n\\rowcolor{Gray}\nOurs (N=0)                          & WS                    & 74.0   & 86.1  & 74.6  & 89.7  & & 85.4    & 81.4    \\\\ \\midrule\n\\rowcolor{Gray}\nOurs (N=5)                          & MS                    & 91.5   & 94.9  & 92.3  & 92.9  & & 88.1    & 91.6    \\\\\n\\rowcolor{Gray}\nOurs (N=15)                         & MS                    & 100    & 100   & 100   & 100   & & 100      & 100     \\\\\n\\rowcolor{Gray}\nOurs (N=45)                         & MS                    & 100    & 100   & 100   & 100   & & 100      & 100     \\\\ \\midrule\n\\rowcolor{Gray}\nOurs (N=$N_{all}$)                        & FS                    & 100    & 100   & 100   & 100   &     & 100      & 100     \\\\\nKim~\\cite{Kim2017_DAGM}                & FS                    &  -     & -     & -     & -     &  & -      & 99.9    \\\\\nRaƒçki~\\cite{Racki2018}                 & FS                    &  -     & 99.6  & -     & 99.6     & & 99.2    & 99.4    \\\\\nWeimer~\\cite{Weimer2016}               & FS                    &  -     & -     & -     & -     & & -     & 99.2    \\\\\nLin~\\cite{Lin2020}                     & FS                    &  -     & 99.0  & -     & -     & & -      & 99.8    \\\\\nWang~\\cite{Wang2018c}                  & FS                    &  -     & -     & -     & -     & & 99.8     & 99.4    \\\\\nHuang~\\cite{Huang2020}                 & FS                    &  -     & -     & -     & -     &  & -        & 99.8    \\\\ \nLiu~\\cite{Liu2020a} ($N_{all}$=20\\%)                  & FS                    &  -     & -     & -     & -     & & 99.9 & -           \\\\\n\n\\bottomrule\n\\end{tabular}%\n}\n\\caption{Comparison with related work on the DAGM dataset. For AP, AUC, F1-measure and classification accuracy (CA), the results are averaged over all 10 classes, whereas for (second) CA and mAcc=(TPR+TNR)/2, they are averaged only over the first 6 classes for comparison with the related work that report results only in  terms of those metrics. %This is done so because some authors report results for 6 classes only.\n}\n\\label{tab:DAGMrelatedWork}\n\\end{table}\n\\paragraph{f-AnoGAN}\nWe used publicly the available TensorFlow implementation of f-AnoGAN~\\cite{fAnoGAN}. In all experiments, we extracted 500 patches per image for training, with each patch $64\\times64$ pixels in size. In inference, a patch size of $64\\times64$ pixels was extracted and classified for every pixel location, which resulted in a heatmap of the same size as the original image. For scoring, we used the distance between the reconstructed image and the original image as well as between the discriminator features of the reconstructed and the original image. A mean squared error was used to measure both distances. We also opted to use  normalization of the scoring similar to~\\cite{Nguyen2019Scoring}, which slightly improved results on noisy outputs. \n\n\\section{Evaluation results} \\label{sec:results}\n\nIn this section, we present the results of the extensive evaluation of the proposed method on several industrial quality-control problems. We first present the evaluation on the DAGM dataset for comparison to different related works, and then present results on KolektorSDD, KolektorSDD2 and Severstal steel, all of which represent practical surface-anomaly detection problems taken from real-world industrial cases. Finally, we present a detailed ablation study that analyses the performance of individual components in the proposed method.\n\nWe simulate and evaluate different levels of supervision by varying the number of positive (i.e., anomalous) training samples for which we have available pixel-wise segmentation masks (i.e, the number of segmented anomalous images $N$ used):\n\n\\begin{enumerate}[label=\\roman*)]\n    \\item \\textbf{\\textit{weak supervision}}, with only image-level labels for all images (i.e., $N=0$),\n    \\item \\textbf{\\textit{mixed supervision}}, with image-level labels for all images but also pixel-level labels only for a subset of anomalous images (i.e., $0<N<N_{all}$), and\n    \\item \\textbf{\\textit{full supervision}}, with image-level and pixel-level labels for all anomalous images (i.e., $N = N_{all}$).\n\\end{enumerate}\nAlthough we limit the number of images with the segmentation mask, we always use data with the image-level label, i.e. weak label that only indicates whether the anomaly is present in the sample or not.\n\n\\subsection{DAGM}\n\nWe first performed evaluation on the DAGM dataset. We consider the number of positive segmented samples $N = \\{0, 5, 15, 45, N_{all}\\}$, and use only image-level labels for the remaining training images. In all cases, we trained for $n_{ep}=70$ epochs, with the learning rate $\\eta=0.05$, batch size $bs=1$, $\\delta=1$, $w_{pos}=10$ and $p=1$. We dilated segmentation masks with $7\\times7$ kernel. \n\n\\begin{figure}%[h]\n    \\begin{center}\n        \\includegraphics[width=\\columnwidth]{images/dagm_samples.jpg}\n    \\end{center}\n    \\caption{Examples of segmentation outputs and scores from the proposed method on the DAGM dataset. Number of fully labeled samples is marked with $N$. Border colour indicates presence (red) or absence (green) of defects, with dashed border around ground-truth images and solid border around predictions. The detection score that represents the probability of the defect is indicated in the bottom left corner. %segmentation as detection.\n    } \n    \\label{fig:samples_dagm}\n\\end{figure}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\columnwidth]{images/DAGM_SINGLE_NEW.png}\n    \\caption{Results on the DAGM dataset in terms of average precision (AP).} \n    \\label{fig:dagm1}\n\\end{figure}\n\nIn Fig.~\\ref{fig:dagm1}, we show  AP for all 10 classes of the dataset. \nFor each class, we show AP of both unsupervised methods and of our model for all different values of $N$.\nOn the bottom left part of each bar, we also indicate the percentage of the positive samples for which we used pixel-level labels, $\\nicefrac{N}{N_{all}}$, which are encoded as proportions of dark and light gray bars.\n\nThe proposed method achieves AP of over $90\\%$ for 6 classes even with $N=0$ and, on average, achieves $7.2$ percentage points higher AP than the current state-of-the-art unsupervised approach.\nIntroducing only five pixel-level labels allows the method to achieve $100\\%$ detection rate on 8 classes\nand raises the average AP to $91.5\\%$.\nFinally, the method achieves $100\\%$ detection rate on all 10 classes with only 15 pixel-level labels, thus outperforming other fully supervised approaches while having less than a quarter of positive samples with pixel-level labels. Several examples of detection are depicted in Fig.~\\ref{fig:samples_dagm}.\n\nIn Tab.~\\ref{tab:DAGMrelatedWork}, we compare the proposed method with a number of other approaches. We report CA and $mAcc=\\nicefrac{TPR+TNR}{2}$ metrics averaged over the first six classes since the related methods that use those metrics report results only on the first six classes. Other metrics are averaged over all 10 classes. The proposed method outperforms all other weakly supervised and unsupervised approaches shown at the top of the table and also all fully supervised methods shown at the bottom of the table. Moreover, 100\\% detection rate can be achieved in mixed supervision with only 15 fully annotated pixel-level samples. This points to superior performance and flexibility when allowing for mixed supervision.\n\n\\begin{figure}%[h]\n    \\begin{center}\n        \\includegraphics[width=\\columnwidth]{images/KSDD_new.png}\n    \\end{center}\n    \\caption{Results on the KolektorSDD dataset in terms of average precision (AP).}\n    \\label{fig:ksdd}\n\\end{figure}\n\n\\subsection{KolektorSDD}\nOn KolektorSDD dataset, we varied the number of segmented positive samples $N = \\{0, 5, 10, 15, 20, N_{all}\\}$, where $N_{all}=33$ and report the performance in terms of the average precision (AP). Identical hyper-parameters were used for all mixed and fully supervised runs, with $n_{ep}=50$, $\\eta=1$, $bs=1$, $\\delta=0.01$, $w_{pos}=1$ and $p=2$, whereas we increased $\\delta$ to $1$ and decreased $\\eta$ to $0.01$ for run with no pixel-level annotations. We dilated segmentation masks with $7\\times7$ kernel.  \n\n\\begin{figure}\n\\centering\n\n\\begin{subfigure}[b]{\\columnwidth}\n   \\includegraphics[width=1\\linewidth]{images/ksdd_samples.jpg}\n   \\caption{KolektorSDD, $N=5$}\n   \\label{fig:samples_ksdd}\n\\end{subfigure}\n\n\\begin{subfigure}[b]{\\columnwidth}\n   \\includegraphics[width=1\\linewidth]{images/ksdd2_samples.jpg}\n   \\caption{KolektorSDD2, $N=N_{all}$}\n   \\label{fig:samples_ksdd2}\n\\end{subfigure}\n\n\\begin{subfigure}[b]{\\columnwidth}\n   \\includegraphics[width=1\\linewidth]{images/steel_samples.jpg}\n   \\caption{Severstal Steel, $N=N_{all}=1500$}\n   \\label{fig:samples_steel} \n\\end{subfigure}\n\\caption{Examples of detections from (a) KolektorSDD, (b) KolektorSDD2 and (c) Severstal Steel defect dataset.}\n\\label{fig:samples_all} \n\\end{figure}\n\nIn the Fig.~\\ref{fig:ksdd}, we show AP for various numbers of segmented positive samples. Without any segmented positive samples, our proposed approach already achieves AP of 93.4\\% and significantly outperforms the current state-of-the-art unsupervised methods that do not (and can not) use any pixel-wise information, by roughly doubling their results. \nWhen only 5 segmented samples are added, the AP drastically increases to 99.1\\% and approaches fairly close to that of the fully supervised scenario. This demonstrates well that the full annotation is often not needed for all images and only a few annotated images are enough to achieve respectable results. In this case, less than 15\\% of images needed to be annotated, which can significantly reduce the annotation cost with almost no classification loss. When using all fully labeled samples the proposed approach achieves 100\\% detection rate.\nSeveral examples of defections are depicted in Fig.~\\ref{fig:samples_ksdd}.\n\n\\subsection{KolektorSDD2}\nNext, we evaluate our approach on the newly proposed KolektorSDD2 dataset. We used identical hyper-parameters for all experiments, $n_{ep}=50$, $\\eta=0.01$, $bs=1$, $\\delta=1$, $w_{pos}=3$ and $p=2$. Since the segmentation mask are very fine, we used a larger, $15\\times15$ kernel for dilation. \n\n\\begin{figure}%[h]\n    \\begin{center}\n        \\includegraphics[width=\\columnwidth]{images/KSDD2_NEW.png}\n    \\end{center}\n    \\caption{Results on the KolektorSDD2 dataset in terms of average precision (AP).\n    } \n    \\label{fig:ksdd2}\n\\end{figure}\n\n\\begin{figure}[h]\n    \\begin{center}\n        \\includegraphics[width=\\columnwidth]{images/STEEL_NEW.png}\n    \\end{center}\n    \\caption{Results of evaluation on the Severstal steel defect database. The number of $fully +  weakly$ labeled positive samples are shown as labels to the left of the bars.} \n    \\label{fig:steel}\n\\end{figure}\n\nResults obtained on this challenging dataset are presented in Fig.~\\ref{fig:ksdd2}, where AP for all values of $N$ are shown.\nEven without any pixel-level labels, the proposed approach outperforms current state-of-the-art unsupervised methods and achieves AP of 73.3\\%. When 16 segmented samples are introduced, AP increases significantly to 83.2\\% and keeps increasing steadily with the introduction of additional pixel-level labels, reaching 95.4\\% when $N=N_{all}$. \nThis demonstrates that the proposed method scales well with the introduction of segmented samples and is capable of taking the full advantage of all the information about the samples that it is presented with. Several examples of detections are depicted in Fig.~\\ref{fig:samples_ksdd2}.\n\n\\subsection{Severstal Steel defect dataset}\nLastly, we evaluate the proposed method on Severstal Steel defect dataset. Since this is a large dataset, we also vary the overall number of positive training images available. We used $N_{all} = \\{300, 750, 1500, 3000\\}$ positive samples, and then for each case used $N = \\{0, 10, 50, 150, 300\\}$, as well as $N=\\{750, 1500, 3000\\}$, where $N\\leq N_{all}$ that were segmented with pixel-wise accuracy. We used all negative samples regardless of used $N$ and $N_{all}$. We trained for $n_{ep}=\\{90, 80, 60, 40\\}$ when $N_{all}=\\{300, 750, 1500, 3000\\}$, respectively, and used $\\eta=0.1$, $bs=10$,  $\\delta=0.1$, $w_{pos}=1$ and $p=2$. \n\nFig.~\\ref{fig:steel}, shows AP for all $N_{all}$ and $N$.\nBars are grouped according to the $N_{all}$, with the labels showing the number of segmented positive samples and the number of positive samples without pixel-level labels.\nWe observe that the proposed model is capable of learning from only image-level labeled samples, where it achieves AP of 90.3\\%, 91.6\\%, 92.5\\% and 94.1\\% for $N=0$ and $N_{all}=\\{300, 750, 1500, 3000\\}$.\nNote that with the positive set that is 10 times larger, the AP only increases for $3.8$ percentage points (from $90.3$\\% to $94.1\\%$), pointing to a logarithmic increase in performance with the increase of the number of the positive samples. \nWith the introduction of the pixel-level labels, the overall performance increases significantly, particularly for experiments with smaller $N_{all}$.\n\nIn the case of $N_{all}=300$, the AP increases from 90.3\\% to 94.2\\% when only 50 pixel-wise labels were added and surpasses the AP achieved when using 3000 positive samples with image-level labels only. \nFurthermore, we can obtain similar results by using 3000 positive samples with only 150 pixel-level labels as with using 300 positive samples with pixel-level labels. Several examples of detections are depicted in Fig.~\\ref{fig:samples_steel}.\n\n\\subsection{Ablation study} \n\nFinally, we evaluate the impact of the individual components, namely the \\textit{dynamically balanced loss}, the \\textit{gradient-flow adjustment} and the \\textit{distance transform}, in the proposed model. We perform the ablation study on the DAGM, KolektorSDD, and Severstal Steel Defect datasets, however, for the later we use only a subset of images to reduce the training time, we used 1,000 positive and negative samples for training and the same amount for testing, we trained for 50 epochs.\n\nWe report the performance by gradually enabling individual components, and by disabling a specific component while leaving all the remaining ones enabled. Results are reported in Tab.~\\ref{tab:ablation}. The results indicate that on all three datasets the worst performance is achieved without any component enabled while the best performance is achieved with all three components used. In the latter case, the proposed model is able to completely solve the DAGM and KolektorSDD datasets while achieving AP of 98.74\\% on Severstal Steel dataset. The second part of Tab.~\\ref{tab:ablation} also demonstrates the importance of each component for weakly supervised learning and for mixed supervision. In both cases, the best result on all three datasets is achieved only with all components enabled. Note that for weakly supervised learning, we did not have any pixel-level annotations and, therefore, could not apply the distance transform. We describe the contribution of each component to the overall improvements in more details below.\n\n\\paragraph{Dynamically balanced loss}\n\nEnabling only dynamic balancing of losses with gradual inclusion of classification network already provides a boost of the performance in all three datasets. In fully supervised learning, the dynamically balanced loss improves AP by 1.08 percentage points in DAGM (from 89.02\\% to 90.10\\%), by 0.23 points in KolektorSDD (from 99.77\\% to 100.00\\%) and by 1.68 points in Severstal Steel (95.36\\% to 97.04\\%). Similar improvements can also be observed in mixed supervision, with 4.44, 0.75 and 1.99 percentage points improvements for DAGM, KolektorSDD and Severstal Steel.\n\n\\paragraph{Gradient-flow adjustment}\n\nThe gradient-flow adjustment has proven to be equally important as the dynamically balanced loss. Both improvements naturally result in a similar performance in KolektorSDD and Severstal Steel since they both prevent unstable segmentation features to significantly affect learning of the classification layers in the early stages. \nHowever, enabling both improvements is more robust as it eliminates the convergence issues while also improving results on all three datasets, especially for DAGM, in which 100\\% detection rate can be achieved for fully supervised learning and AP of 95.37\\% for mixed supervision. On the other hand, it has also proven better to use only gradient-flow adjustment in weak supervision and to completely disable dynamically balanced loss, since in this case segmentation loss is not present. \n\n\\paragraph{Spatial label uncertainty}\n\nLastly, enabling the distance transform as weights for the positive pixels pushes performance on KolektorSDD to 100\\% detection rate, therefore completely solving KolektorSDD and DAGM in fully supervised mode. It also improves AP on Severstal Steel dataset from 98.24\\% to 98.74\\%. Moreover, distance transform is even more important in mixed supervision where it enables 100\\% detection rate with only 25\\% of fully annotated data and using only weak labels for the remaining data. \n\n\\begin{table*}[t]\n\\caption{Performance of individual components on three datasets by gradually including each one for fully supervised (FS), mixed supervision (MS), and weakly supervised (WS) learning. We report average precision (AP) and the number of false positives (FP) and false negatives (FN).}\n\\label{tab:ablation}\n\\centering\n\\resizebox{0.9\\textwidth}{!}{%\n\\begin{tabular}{lcccccccccccc}\n\\toprule\n    &\\multicolumn{2}{c}{\\textit{DAGM}} && \\multicolumn{2}{c}{\\textit{KolektorSDD}} && \\multicolumn{2}{c}{\\textit{Severstal Steel}} & \\multirow{2}{*}{\\textit{\\makecell[c]{Dynamically\\\\balanced loss} } } & \\multirow{2}{*}{\\textit{\\makecell[c]{Gradient-flow\\\\adjustment}}}&  \\multirow{2}{*}{\\textit{\\makecell[c]{Distance\\\\transform} } } \\\\\n    \\cmidrule{2-3} \\cmidrule{5-6} \\cmidrule{8-9} \\vspace{-8pt}\\\\\n    & AP & FP+FN && AP & FP+FN && AP & FP+FN &  &   & \\\\ \n\\midrule\n    \\multirow{6}{*}{\\rotatebox{90}{{\\textit{\\makecell{FS\\\\(N=100\\%)}}}}} \n    & 89.02 & 1010+45 && 99.77 & 0+1 && 95.36 & 108+121 &  &  & \\\\ \n\n    & 90.10 & 995+14 && 100.00 & 0+0  && 97.04 & 95+76 & \\checkmark &   &  \\\\ \n\n    & 100.00 & 0+0 && 99.88 & 1+0 && 98.24 & 52+58 & \\checkmark & \\checkmark  &  \\\\ \n    & 91.88 & 471+68 && 99.88 & 1+0 && 97.80 & 57+60 & \\checkmark &   & \\checkmark \\\\ \n    & 99.95 & 2+2 && 99.75 & 1+0 && 98.32 & 73+64 & & \\checkmark& \\checkmark \\\\ \n    & 100.00 & 0+0 && 100.00 & 0+0 && 98.74 & 59+40 & \\checkmark & \\checkmark  & \\checkmark \\\\ \n    \\midrule\n    \\multirow{6}{*}{\\rotatebox{90}{{\\textit{\\makecell{MS\\\\($N\\approx 25\\%$)}}}}}\n    & 66.02 & 2497+56   && 99.06 & 2+1 && 94.36 & 136+120 &            &             &            \\\\\n    & 70.46 & 1775+65   && 99.81 & 2+0 && 96.35 & 113+96  & \\checkmark &             &            \\\\\n    & 95.37 & 22+56     && 99.10 & 0+2 && 96.88 & 57+105  & \\checkmark & \\checkmark  &            \\\\\n\n    & 91.44 & 837+19   && 99.40 & 1+1 && 92.49 & 183+137 & \\checkmark &             & \\checkmark \\\\\n    & 99.93 & 0+3      && 99.16 & 1+2 && 96.60 & 122+83  &            & \\checkmark  & \\checkmark \\\\\n    & 100.00 & 0+0     && 99.10 & 0+2 && 97.73 & 67+64   & \\checkmark & \\checkmark  & \\checkmark \\\\\n    \\midrule\n    \\multirow{2}{*}{\\rotatebox{90}{{\\textit{\\makecell{WS\\\\($0\\%$)}}}}} & 74.05 & 438+248 && 93.43 & 4+5 && 91.01 & 201+158 & & \\checkmark  & N/A \\\\\n    & 61.49 & 1947+127 && 93.16 & 3+5 && 90.88 & 247+120 & \\checkmark & \\checkmark  & N/A \\\\\n    \n\\bottomrule   \n\\end{tabular}%\n}\n\\end{table*}\n\n\\section{Conclusion} \\label{sec:conclusion}\n\nIn this paper, we presented a deep-learning model for surface-anomaly detection using mixed supervision. We proposed an architecture with two sub-networks: the segmentation sub-network that learns from pixel-level labels and the classification sub-network that learns from weak, image-level labels. Combining both sub-networks allowed for mixing of fully and weakly labeled data to achieve the best result with minimal annotation effort. To accomplish this, we proposed a unified network with end-to-end learning and enabled handling of coarse pixel-level labels. We demonstrated that the proposed model outperforms the existing state-of-the-art anomaly-detection models in both fully supervised mode and in weakly supervised mode. \nThe network can be applied in either weakly supervised or fully supervised settings, or in a mixture of both, depending on the availability of annotations. The mixture of both annotation types has resulted in performance competitive to fully supervised mode while requiring significantly fewer and less complex annotations. We have demonstrated this on three existing industrial dataset: DAGM, KolektorSDD and Severstal Steel Defect, and have additionally proposed KolektorSDD2 that represents a real-world industrial problem with several defect types spread over 3000 images. \n\nTwo important conclusions can be drawn from the results. First, using a large number of only weakly labeled data has proven to be more important than using a 10-fold smaller set of fully labelled samples. In cases, such as existing industrial production control where weakly labeled data can be obtained at no cost, it is much less costly to use a large set of weakly labeled data without sacrificing the performance. However, as a second conclusion, we observed that a significant performance boost is often obtained when adding just 5--10\\% of fully-labeled data, resulting in almost the same performance as using all fully-labeled data.\nThis realization can considerably reduce the time and cost of data annotation in many industrial applications.\n\n\\section*{Acknowledgments}\nThis work was in part supported by the ARRS research project J2-9433 (DIVID) and research programme P2-0214. We would also like to thank Kolektor Group d.o.o. for providing images and initial annotations used in KolektorSDD2 dataset.\n\n{\\small\n}\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Learning To Count Everything}\n\n\\begin{document}\n\n\\title{Learning To Count Everything}\n\n\\author{Viresh Ranjan$^{1}$ \\quad Udbhav Sharma$^{1}$  \\quad Thu Nguyen$^{2}$ \\quad Minh Hoai$^{1,2}$ \\\\\n$^1$Stony Brook University, USA\\\\\n$^2$VinAI Research, Hanoi, Vietnam \n}\n\n\\maketitle\n\n\\begin{abstract}\nExisting works on visual counting primarily focus on one specific category at a time, such as people, animals, and cells. In this paper, we are interested in counting everything, that is to count objects from any category given only a few annotated instances from that category.\nTo this end, we pose counting as a few-shot regression task. \nTo tackle this task, we present a novel method that takes a query image together with a few exemplar objects from the query image and predicts a density map for the presence of all objects of interest in the query image. We also present a novel adaptation strategy to adapt our network to any novel visual category at test time, using only a few exemplar objects from the novel category.  We also introduce a dataset of 147 object categories containing over 6000 images that are suitable for the few-shot counting task. The images are annotated with two types of annotation, dots and bounding boxes, and they can be used for developing few-shot counting models. Experiments on this dataset shows that our method outperforms several state-of-the-art object detectors and few-shot counting approaches. Our code and dataset can be found at \\url{https://github.com/cvlab-stonybrook/LearningToCountEverything}. \n\n\\end{abstract}\n\n\\section{Introduction}\nHumans can count objects from most of the visual object categories with ease, while  current state-of-the-art computational  methods~\\cite{zhang2016single,ma2019bayesian,m_Wang-etal-NIPS20} for counting can only handle a limited number of visual categories. In fact, most of the counting neural networks~\\cite{arteta2016counting,m_Wang-etal-NIPS20} can handle a single category at a time, such as people, cars, and cells. \n\nThere are two major challenges preventing the Computer Vision community from designing systems capable of counting a large number of visual categories. First, most of the contemporary counting approaches~\\cite{m_Wang-etal-NIPS20,zhang2016single,arteta2016counting} treat counting as a supervised regression task, requiring thousands of labeled images to learn a fully convolutional regressor that maps an input image to its corresponding density map, from which the estimated count is obtained by summing all the density values. These networks require dot annotations for millions of objects on several thousands of training images, and obtaining this type of annotation is a costly and laborious process. As a result, it is difficult to scale these contemporary counting approaches to handle a large number of visual categories. Second, there are not any large enough unconstrained counting datasets with many visual categories for the development of a general counting method. Most of the popular counting datasets~\\cite{zhang2016single,Idrees_2013_CVPR,idrees2018composition,wang2020nwpu,sindagi2020jhu,hsieh2017drone} consist of a single object category.\n\n\\begin{figure}[t]\n     \\includegraphics[width=\\linewidth]{./Images/teaser2.png}\n     \\vskip -0.1in\n    \\caption{\\textbf{Few-shot counting---the objective of our work.} Given an image from a novel class and a few exemplar objects from the same image delineated by bounding boxes, the objective is to count the total number of objects of the novel class in the image.\n  \\label{fig:motivation}}\n \\end{figure} \n\n\\iffalse\n\\setlength{\\tabcolsep}{3pt}\n\\begin{table}[!tb]\n\\centering\n\\begin{tabular}{lcccc}\n\\toprule\n& & & \\multicolumn{2}{c}{Annotation type} \\\\\n\\cmidrule(lr){4-5}\nDataset & Images & Categories & Dot & Bounding Box  \\\\\n\\midrule\nUCF CC 50~\\cite{Idrees_2013_CVPR} & 50  & 1 & \\cmark & \\xmark  \\\\\nShanghaitech~\\cite{zhang2016single}  & 1198 & 1  & \\cmark & \\xmark  \\\\\nUCF QNRF~\\cite{idrees2018composition} & 1535 & 1   & \\cmark & \\xmark    \\\\\nNWPU~\\cite{wang2020nwpu} & 5109 & 1& \\cmark & \\xmark \\\\\nJHU Crowd~\\cite{sindagi2020jhu} & 4372 & 1  & \\cmark & \\cmark \\\\\nCARPK~\\cite{hsieh2017drone} & 1448 & 1  & \\cmark & \\cmark   \\\\\n\\textbf{Proposed} & 6135 & 147 & \\cmark & \\cmark   \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{{\\bf Comparison with popular counting datasets}. Existing  datasets comprise of a single object category, while our dataset contain multiple categories. Our dataset is of similar size as the existing datasets, and it comes with dot, bounding box, and polygon annotations. \\mhoai{This table is not needed, at least not in the introduction section}\n\\label{tab:DatasetComparison}}\n\\end{table}\n\\fi\n\nIn this work, we address both of the above challenges. To handle the first challenge, we take a detour from the existing counting approaches which treat counting as a typical fully supervised regression task, and pose counting as a few shot regression task, as shown in \\Fref{fig:motivation}. In this few-shot setting, the inputs for the counting task are an image and few examples from the same image for the object of interest, and the output is the count of object instances. The examples are provided in the form of bounding boxes around the objects of interest. In other words, our few shot counting task deals with counting instances within an image which are similar to the exemplars from the same image. Following the convention from the few-shot classification task~\\cite{Lake-et-al-Science15,vinyals2016matching,finn2017model}, the classes at test time are completely different from the ones seen during training. This makes few-shot counting very different from the typical counting task, where the training and test classes are the same. Unlike the typical counting task, where hundreds~\\cite{zhang2016single} or thousands~\\cite{idrees2018composition} of labeled examples are available for training, a few-shot counting method needs to generalize to completely novel classes using only the input image and a few exemplars. \n\nWe propose a novel architecture called \\underline{F}ew Shot \\underline{A}daptation and \\underline{M}atching Network~(FamNet) for tackling the few-shot counting task. FamNet has two key components: 1) a feature extraction module, and 2) a density prediction module. The feature extraction module consists of a general feature extractor capable of handling a large number of visual categories. The density prediction module is designed to be agnostic to the visual category. As will be seen in our experiments, both the feature extractor and density prediction modules can already generalize to the novel categories at test time. We further improve the performance of FamNet by developing a novel few-shot adaptation scheme at test time. This adaptation scheme uses the provided exemplars themselves and adapts the counting network to them with a few gradient descent updates, where the gradients are computed based on two loss functions which are designed to utilize the locations of the exemplars to the fullest extent. Empirically, this adaptation scheme improves the performance of FamNet.\n\nFinally, to address the lack of a dataset for developing and evaluating the performance of few-shot counting methods, we introduce a medium-scale dataset consisting of more than 6000 images from 147 visual categories. The dataset comes with dot and bounding box annotations, and is suitable for the few-shot counting task. We name this dataset Few-Shot Counting-147 (FSC-147). \n\nIn short, the main contributions of our work are as follows. First, we pose counting as a few-shot regression task. Second, we propose a novel architecture called FamNet for handling the few-shot counting task, with a novel few-shot adaptation scheme at test time. Third, we present a novel few-shot counting dataset called FSC-147, comprising of over 6000 images with 147 visual categories.\n\n \n\n\\section{Related Works}\nIn this work, we are interested in counting objects of interest in a given image with a few labeled examples from the same image. Most of the previous counting methods are for specific types of objects such as people~\\cite{ma2019bayesian,zhang2016single,ranjan2018iterative,babu2018divide,sam2017switching,li2018csrnet,liu2018leveraging,cao2018scale,ranjan2019crowd,shi2019revisiting,liu2019context,wang2019learning,zhang2019attentional,wan2019adaptive,m_Ranjan-etal-ACCV20,m_Abousamra-etal-AAAI21}, cars~\\cite{mundhenk2016large}, animals~\\cite{arteta2016counting}, cells~\\cite{arteta2016detecting,xie2018microscopy,khan2016deep}, and fruits~\\cite{rahnemoonfar2017deep}. These methods often require training images with tens of thousands or even millions of annotated object instances. Some of these works~\\cite{m_Ranjan-etal-ACCV20} tackle the issue of costly annotation cost to some extent by adapting a counting network trained on a source domain to any target domain using labels for only few informative samples from the target domain. However, even these approaches require a large amount of labeled data in the source domain.  \n\nThe proposed FamNet works by exploiting the strong similarity between a query image and the provided exemplar objects in the image. To some extent, it is similar the decade-old self-similarity work of Shechtman and Irani~\\cite{Shechtman-Irani-CVPR07}. Also related to this idea is the recent work of Lu and Zisserman\\cite{lu2018class}, who proposed a Generic Matching Network (GMN) for class-agnostic counting. GMN was pre-trained with tracking video data, and it had an explicit adaptation module to adapt the network to an image domain of interest. GMN has been shown to work well if several dozens to hundreds of examples are available for adaptation. Without adaptation, GMN does not perform very well on novel classes, as will be seen in our experiments. \n\nRelated to few-shot counting is the few-shot detection task (e.g.,~\\cite{kang2019few,fan2020few}), where the objective is to learn a detector for a novel category using a few labeled examples. Few-shot counting differs from few-shot detection in two primary aspects. First, few-shot counting requires dot annotations while detection requires bounding box annotations. Second, few-shot detection methods can be affected by severe occlusion whereas few-shot counting is tackled with a density estimation approach~\\cite{Lempitsky-Zisserman-NIPS10,zhang2016single}, which is more robust towards occlusion than the detection-then-counting approach because the density estimation methods do not have to commit to binarized decisions at an early stage. The benefits of the density estimation approach has been empirically demonstrated in several domains, especially for crowd and cell counting.\n\nAlso related to our work is the task of few-shot image classification~\\cite{lake2015human,koch2015siamese,vinyals2016matching,santoro2016one,finn2017model,ravi2016optimization}. The few-shot classification task deals with classifying images from novel categories at test time, given a few training examples from these novel test categories.\nThe Model Agnostic Meta Learning (MAML)~\\cite{finn2017model} based few-shot approach is relevant for our few-shot counting task, and it focuses on learning parameters which can adapt to novel classes at test time by means of few gradient descent steps. However, MAML involves computing second order derivatives during training which makes it expensive, even more so for the pixel level prediction task of density map prediction being considered in our paper. Drawing inspiration from these works, we propose a novel adaptation scheme which utilizes the exemplars available at test time and performs a few steps of gradient descent in order to adapt FamNet to any novel category. Unlike MAML, our training scheme does not require higher order gradients at training time. We compare our approach with MAML, and empirically show that it leads to better performance and is also much faster to train.\n\n\\iffalse\n\\section{Few-Shot Counting}\\label{sec:FewShotCounting}\n\\mhoai{This section is redundant. You have said it very explicitly in the introduction. Even in the introduction, you have two paragraphs for it, and I think it is not necessary. }\n\nNext we summarize a typical counting task in brief, followed by the few-shot Counting task being considered in this paper, and contrast the two.\n\nMost of the works~\\cite{zhang2016single,idrees2018composition,ma2019bayesian,arteta2016counting} in counting focus on counting objects of a specific category such as crowd, penguins, cars, cells etc. These approaches treat counting as a fully supervised regression task, and typically train a fully convolutional architecture to map any image to the corresponding density map. The count is obtained by summing all the pixels in the predicted density map. The groundtruth density map for training these counting networks is obtained by convolving the dot annotation map corresponding to the location of each of the objects of interest with a 2D Gaussian kernel. These counting networks perform remarkably well at counting specific objects, however their training requires hundreds~\\cite{zhang2016single} or even thousands~\\cite{idrees2018composition,wang2020nwpu,sindagi2020jhu} of annotated images\nwith millions of dot annotations.\nIn this work, we are interested in counting objects from a large number of visual categories, and not just a specific category. One straightforward approach to tackle large number of visual categories would be to collect large number of annotated examples from each of the visual categories, and train separate/joint counting networks for all of the visual categories. However, this approach would be expensive in terms of the data annotation cost. To circumvent this data annotation cost, we pose counting as a few-shot regression task which involves counting objects of any visual category given an image and a few examples of the object of interest from the same image. In few-shot counting, the classes seen at test time are completely different from the ones seen at training time. The class of approaches which would perform well at few-shot counting would need to generalize to any novel category using only a test image, and few examples of object of interest from the test image. This makes the few-shot counting problem very different from the typical counting problems such as crowd counting~\\cite{zhang2016single,sindagi2020jhu,idrees2018composition,wang2020nwpu}, vehicle counting~\\cite{hsieh2017drone}, penguin counting~\\cite{arteta2016counting}, where a large dataset of annotated images is readily available for training. Furthermore, the few-shot counting task is scalable to large number of visual categories, since it does not require large number of annotated examples for each of the visual categories.  \n\\fi\n\n\\begin{figure*}[t]\n    \\includegraphics[width=\\textwidth]{./Images/block-A2.png}\n    \\caption{{\\bf Few-shot adaptation \\& matching Network} takes as input the query image along with few bounding boxes depicting the object of interest, and predicts the density map. The count is obtained by summing all the pixel values in the density map. The adaptation loss is computed based on the bounding box information, and the gradients from this loss are used to update the parameters of the density prediction module. The adaptation loss is only used during test time. \n  \\label{fig:EmNet}}\n \\end{figure*}  \n\\section{Few-Shot Adaptation \\& Matching Network}\n\nIn this section, we describe the proposed FamNet for tackling the few-shot counting task. \n\n\\subsection{Network architecture}\n\n\\Fref{fig:EmNet} depicts the pipeline of FamNet. The input to the network is an image $X \\in \\Re^{H \\times W \\times 3}$ and a few exemplar bounding boxes depicting the object to be counted from the same image. The output of the network is the predicted density map $Z \\in \\Re^{H \\times W}$, and the count for the object of interest is obtained by summing over all density values. \n \nFamNet consists of two key modules: 1) a multi-scale feature extraction module, and 2) a density prediction module. We design both of these modules so that they can handle novel categories at test time. We use an ImageNet-pretrained network~\\cite{He-et-al-CVPR16} for the feature extraction, since such networks can handle a broad range of visual categories. The density prediction module is designed to be agnostic to the visual categories.\nThe multi-scale feature extraction module consists of the first four blocks from a pre-trained ResNet-50 backbone~\\cite{He-et-al-CVPR16} (the parameters of these blocks are frozen during training). We represent an image by the convolutional feature maps at the third and fourth blocks. We also obtain the multi-scale features for an exemplar by performing ROI pooling on the convolutional feature maps from the third and fourth Resnet-50 blocks.\n\nTo make the density prediction module agnostic to the visual categories, we do not use the features obtained from the feature extraction module directly for  density prediction. Instead, we only use the correlation map between the exemplar features and image features as the input to the density prediction module. To account for the objects of interest at different scales, we scale the exemplar features to different scales, and correlate the scaled exemplar features with the image features to obtain multiple correlation maps, one for each scale. For all of our experiments, we use the scales of 0.9 and 1.1, along with the original scale. The correlation maps are concatenated and fed into the density prediction module. The density prediction module consists of five convolution blocks and three upsampling layers placed after the first, second, and third convolution layers. The last layer is a $1{\\times}1$ convolution layer, which predicts the 2D density map. The size of the predicted density map is the same as the size of the input image. \n\n\\subsection{Training}\n\nWe train the FamNet using the training images of our dataset. Each training image contains multiple objects of interest, but only the exemplar objects are annotated with bounding boxes and the majority of the objects only have dot annotations. It is, however, difficult to train a density estimation network with the training loss that is defined based on the dot annotations directly. Most existing works for visual counting, especially for crowd counting~\\cite{zhang2016single}, convolve the dot annotation map with a Gaussian window of a fixed size, typically $15{\\times}15$, to generate a smoothed target density map for training the density estimation network. \n\nOur dataset consists of $147$ different categories, where there is huge variation in the sizes of the objects. Therefore, to generate the target density map, we use Gaussian smoothing with adaptive window size. First, we use dot annotations to estimate the size of the objects. Given the dot annotation map, where each dot is at an approximate center of an object, we compute the distance between each dot and its nearest neighbor, and average these distances for all the dots in the image. This average distance is used as the size of the Gaussian window to generate the target density map. The standard deviation of the Gaussian is set to be a quarter of the window size. \n\nTo train FamNet, we minimize the mean squared error between the predicted density map and the ground truth density map. We use Adam optimizer with a learning rate of $10^{-5}$, and batch size of 1. We resize each image to a fixed height of 384, and the width is adjusted accordingly to preserve the aspect ratio of the original image. \n\n\\subsection{Test-time adaptation}\n\nSince the two modules of the FamNet are not dependent on any object categories, the trained FamNet can already be used for counting objects from novel categories given a few exemplars. In this section, we describe a novel approach to adapt this network to the exemplars, further improving the accuracy of the estimated count. The key idea is to harness the information provided by the locations of the exemplar bounding boxes. So far, we have only used the bounding boxes of the exemplars to extract appearance features of the exemplars, and we have not utilized their locations to the full extent. \n\nLet $B$ denote the set of provided exemplar bounding boxes. For a bounding box $b \\in B$, let $Z_b$ be the crop from the density map $Z$ at location $b$. To harness the extra information provided by the locations of the bounding boxes $B$, we propose to consider the following two losses.\n\n\\myheading{Min-Count Loss.} For each exemplar bounding box~$b$, the sum of the density values within $Z_b$ should be at least one. This is because the predicted count is taken as the sum of predicted density values, and there is at least one object at the location  specified by the bounding box $b$. However, we cannot assert that the sum of the density values within $Z_b$ to be exactly one, due to possible overlapping between $b$ and other nearby objects of interest. This observation leads to an inequality constraint: $||Z_b||_1 \\geq 1$, where $||Z_b||_1$ denotes the sum of all the values in $Z_b$. Given the predicted density map and the set of provided bounding boxes for the exemplars, we define the following Min-Count loss to quantify the amount of constraint violation: \n\n\\begin{equation}\\label{adaptation1}\n\\mL_{MinCount} = \\sum_{b \\in B} \\max(0,1- ||Z_b||_1).    \n\\end{equation}\n\n\\myheading{Perturbation Loss.} Our second loss to harness the positional information provided by the exemplar bounding boxes is inspired by the success of tracking algorithms based on correlation filter \\cite{Henriques-etal-PAMI15,valmadre2017end,wang2019fast}. Given the bounding box of an object to track, these algorithms learn a filter that has highest response at the exact location of the bounding box and lower responses  at perturbed locations. The correlation filter can be learned by  optimizing a regression function to map from a perturbed location to a target response value, where the target response value decreases exponentially as the perturbation distance increases, usually specified by a Gaussian distribution. \n\nIn our case, the predicted density map $Z$ is essentially the correlation response map between the exemplars and the image. To this end, the density values around the location of an exemplar should ideally look like a Gaussian. Let $G_{h \\times w}$ be the 2D Gaussian window of size $h{\\times}w$. We define the perturbation loss as follows: \n\\begin{equation}\\label{adaptation2}\n\\mL_{Per} = \\sum_{b \\in B} ||Z_b - G_{h \\times w}||_2^2.    \n\\end{equation}\n\n\\myheading{The combined adaptation Loss.} The loss used for test-time adaptation is the weighted combination of the Min-Count loss and the Perturbation loss. The final test time adaptation loss is given as \n\\begin{equation}\\label{adaptation3}\n    \\mL_{Adapt} = \\lambda_1 \\mL_{MinCount} + \\lambda_2 \\mL_{Per},\n\\end{equation}\nwhere $\\lambda_1$ and $\\lambda_2$ are scalar hyper parameters. At test time, we perform $100$ gradient descent steps for each test image, and optimize the joint loss presented in  \\Eref{adaptation3}. We use the learning rate $10^{-7}$. The values for $\\lambda_1$ and $\\lambda_2$ are $10^{-9}$ and $10^{-4}$ respectively. \nThe learning rate, the number of gradient steps, $\\lambda_1$, and $\\lambda_2$, are tuned based on the performance on the validation set. \nThe values of $\\lambda_1$, and $\\lambda_2$ seem small, but this is necessary to make the adaptation loss to have similar magnitude to the training loss. Even though the training loss is not used for test time adaptation, it is important for the losses and their gradients to have similar magnitudes. Otherwise, the gradient update steps of the adaptation process will either do nothing or move away far from the parameters learned during training. \n\nNote that the adaptation loss is only used at test time. During training of FamNet, this loss is redundant because the proposed training loss, based on mean squared errors computed over all pixel locations, already provides stronger supervision signal than the adaptation loss. \n\n\\iffalse\n\\section{Old}\nIn this section, we describe EmNet, our proposed architecture for tackling the few-shot counting task.\n\n\\subsection{Network Architecture}\nThe processing pipeline and architecture of Exemplar Matching Network (EmNet) is depicted in \\Fref{fig:EmNet}. The input to EmNet is a query image of size $H {\\times} W$, and a set of $M$ image patches containing $M$ exemplar objects. The output of EmNet is a density map highlighting the locations of all objects of interest in the query image. EmNet can handle a variable number of exemplar patches, and it consists of three key blocks: 1) feature extraction; 2) feature convolution;  and 3) density prediction.  The feature extraction block extracts convolutional feature maps from the query image and the exemplar patches. The feature extraction block consists of the first four blocks from a pretrained Resnet-50 backbone, and the parameters of the block are kept frozen during training. The feature convolution block takes as input the feature map for the query image and $M$ feature maps from $M$ exemplar patches. The exemplar feature maps are convolved with the image feature map to produce a 3D tensor $Z$ of size $M{\\times}h{\\times}w$. Note that $h=\\frac{H}{8}$ and $w = \\frac{W}{8}$. We pad the query image feature map with zeros before convolving it with the exemplar feature maps to preserve the size of the feature map. We apply average pooling and max pooling across first dimension of the the 3D tensor $Z$ to obtain two 2D feature maps of size $h{\\times}w$. These feature maps are concatenated and passed to the density prediction block. The density prediction block consists of five convolution blocks and three upsampling layers after the first, second, and third convolution layers. The last layer is a $1{\\times}1$ convolution layer, which predicts the 2D density map. The size of the density map is same as the size of the input image.\n\n\\subsection{Generating Target Density Map from Dot Annotation}\n\nWe train EmNet with images from the training data subset. Each training image, however, only has dot annotation for the majority of objects in the image. It is however difficult to train a density estimation network with the training loss that is defined based on the dot annotations directly. Most existing works for visual counting, especially crowd counting, convolve the dot map with a Gaussian window of fixed size, typically $15{\\times}15$ to generate the target density map for training the density estimation network. \n\nOur dataset consists of $64$ different categories, where there is huge variation in the sizes of the objects. Therefore, to generate the target density map, we use Gaussian smoothing with adaptive window size. We use dot annotations for estimating the size of the objects. Given the dot annotation map, where each dot is at the approximate center of an object, we compute the distance between each dot and its nearest neighbor, and average this distance for all the dots in the image. This average distance is used as the size of the Gaussian window used in generating the target density map. The standard deviation of the Gaussian is set to be a quarter of the window size. \n\n\\subsection{Training}\nFor each annotation type (i.e., dot, box, and polygon), we train three EmNets, for $1$-shot, 2-shot, and 3-shot. For training the $k$-shot model, we sample an image and $k$ random exemplars from the same image, and propagate them through EmNet to obtain a density map. We use mean squared error loss and force the density map to be close to the ground truth density map. We use a batch size of 1, and use Adam optimizer with a learning rate of $10^{-5}$ and momentum $0.9$ for the training. For an image for which the width or height is bigger than 1504, we rescale it so that the larger size is equal to $1504$. The aspect ratio of the image is preserved while rescaling.\n\\fi\n\\section{The FSC-147 Dataset}\n\\begin{figure*}[!t]\n    \\begin{subfigure}[!category]{0.3\\textwidth}\n    \\includegraphics[height=1\\textwidth, width=\\textwidth]{./plot/piechart4}\n    \\label{fig:cate}\n    \\vskip -0.1in\n    \\caption[width=0.8\\textwidth]{Image categories and number of images for each category in our dataset. \\label{fig:categoryPlot}}\n\\end{subfigure}\n    \\hfill\n    \\begin{subfigure}[!hist]{0.3\\textwidth}\n        \\includegraphics[height=\\textwidth, width=\\textwidth]{./plot/hist_objcount}\n        \\label{fig:hist}\n    \\vskip -0.1in        \n        \\caption{Number of images in several ranges of object count.\\label{fig:histogram}}\n    \\end{subfigure}\n     \\hfill\n\\begin{subfigure}[!hist]{0.37\\textwidth}\n\\setlength{\\tabcolsep}{2pt}\n\\begin{adjustbox}{width=\\columnwidth}\n\\begin{tabular}{lcccc}\n\\toprule\n& & & \\multicolumn{2}{c}{Annotation type} \\\\\n\\cmidrule(lr){4-5}\nDataset & Images & Categories & Dot & Bounding Box  \\\\\n\\midrule\nUCF CC 50~\\cite{Idrees_2013_CVPR} & 50  & 1 & \\cmark & \\xmark  \\\\\nShanghaitech~\\cite{zhang2016single}  & 1198 & 1  & \\cmark & \\xmark  \\\\\nUCF QNRF~\\cite{idrees2018composition} & 1535 & 1   & \\cmark & \\xmark    \\\\\nNWPU~\\cite{wang2020nwpu} & 5109 & 1& \\cmark & \\xmark \\\\\nJHU Crowd~\\cite{sindagi2020jhu} & 4372 & 1  & \\cmark & \\cmark \\\\\nCARPK~\\cite{hsieh2017drone} & 1448 & 1  & \\cmark & \\cmark   \\\\\n\\textbf{Proposed} & 6135 & 147 & \\cmark & \\cmark   \\\\\n\\bottomrule\n\\end{tabular}\n\\end{adjustbox}\n\\vskip .1in\n\\caption{Comparison with popular counting datasets. \n\\label{tab:DatasetComparison}}\n    \\end{subfigure}\n \\label{fig:plot}\n  \\caption{{\\bf Categories \\& no. of images per category, object counts, and comparison with other counting datasets}}\n \\end{figure*}  \n\nTo train the FamNet, we need a dataset suitable for the few-shot counting task, consisting of many visual categories. Unfortunately, existing counting datasets are mostly dedicated for specific object categories such as people, cars, and cells. Meanwhile, existing multi-class datasets do not contain many images that are suitable for visual counting. For example, although some images from the COCO dataset~\\cite{Lin-etal-ECCV14} contains multiple instances from the same object category, most of the images do not satisfy the conditions of our intended applications due to the small number of object instances or the huge variation in pose and appearance of the object instances in each image. \n\nSince there was no dataset that was large and diverse enough for our purpose, we collected and annotated images ourselves. Our dataset consists of 6135 images across a diverse set of 147 object categories, from kitchen utensils and office stationery to vehicles and animals. The object count in our dataset varies widely, from 7 to 3731 objects, with an average count of 56 objects per image. In each image, each object instance is annotated with a dot at its approximate center. In addition, three object instances are selected randomly as exemplar instances; these exemplars are also annotated with axis-aligned bounding boxes. In the following subsections, we will describe how the data was collected and annotated. We will also report the detailed statistics and how the data was split into disjoint training, validation, and testing sets. \n\n \n\n\\subsection{Image Collection}\n\nTo obtain the set of 6135 images for our dataset, we started with a set of candidate images obtained by keyword searches. Subsequently, we performed manual inspection to filter out images that do not satisfy our predefined conditions as described below. \n\n\\myheading{Image retrieval}. We started with a list of object categories, and collected 300--3000 candidate images for each category by scraping the web. We used Flickr, Google, and Bing search engines with the open source image scrappers~\\cite{Flickr-Scrapper,Google-Scrapper}. We added adjectives such as \\textit{many, multiple, lots of}, and \\textit{stack of} in front of the category names to create the search query keywords. \n\n\\myheading{Manual verification and filtering}. \nWe manually inspected the candidate images and only kept the suitable ones satisfying the following criteria: \n\\begin{enumerate} \\denselist\n    \\item \\textit{High image quality}: The resolution should be high enough to easily differentiate between objects. \n    \\item \\textit{Large enough object count}: The number of objects of interest should be at least 7. We are more interested in counting a large number of objects, since humans do not need help counting a small number of objects. \n    \\item \\textit{Appearance similarity}: we selected images where object instances have somewhat similar poses, texture, and appearance.  \n    \\item \\textit{No severe occlusion}: in most cases, we removed candidate images where severe occlusion prevents humans from accurately counting the objects. \n\\end{enumerate}\n\n\\subsection{Image Annotation}\nImages in the dataset were annotated by a group of annotators using the OpenCV Image and Video Annotation Tool~\\cite{CVAT}. Two types of annotation were collected for each image, dots and bounding boxes, as illustrated in  \\Fref{fig:annotation_examples}. For images containing multiple categories, we picked only one of the categories. Each object instance in an image was marked with a dot at its approximate center. In case of occlusion, the occluded instance was only counted and annotated if the amount of occlusion was less than 90\\%. For each image, we arbitrarily chose three objects as exemplar instances and we drew axis-aligned bounding boxes for those instances. \n\n\\subsection{Dataset split}\nWe divided the dataset into train, validation, and test sets such that they do not share any object category. We randomly selected 89 object categories for the train set, and 29 categories each for the validation and test sets. The train, validation, and test sets consist of 3659, 1286 and 1190 images respectively.\n \n\\subsection{Data Statistics}\n\\iffalse\n\\vr{The statistics need to be changed in this section, text will remain unchanged.}\nThe dataset contains images from 148 categories. These categories can be roughly grouped to form super-categories: (i) People \\& Animal (156 images); (ii) Apparel \\& Cosmetic (133 images); (iii) Bottle, Box, \\& Bag (129 images); (iv) Furniture and building elements (84 images); (v) Food \\& Fruit (130 images); (vi) Toy \\& Collectible (84 images); (vii) Office stationery (164 images); and (viii) Vehicle (49 images). \\Fref{fig:categoryPlot} shows the categories and the super-categories together with the number of images for each category. \n\\fi\n\nThe dataset contains a total of 6135 images. The average height and width of the images are 774 and 938 pixels, respectively. The average number of objects per image is 56, and the total number of objects is \\mbox{343,818}. The minimum and maximum number of objects for one image are 7 and 3701, respectively. The three categories with the highest number of objects per image are: Lego (303 objects/image), Brick (271), and Marker (247). The three categories with lowest number of objects per image are: Supermarket shelf (8 objects/image), Meat Skewer (8), and Oyster (11). \\Fref{fig:histogram} is a histogram plot for the number of images in several ranges of object count. \n\n\\begin{figure*}[!th]\n\n  \\begin{subfigure}[b]{0.23\\textwidth}    \\includegraphics[height=0.75\\textwidth,width=\\textwidth]{./Images/examples/1084.png}\n  \\end{subfigure}\n  \\hfill\n  \\begin{subfigure}[b]{0.23\\textwidth}    \\includegraphics[height=0.75\\textwidth,width=\\textwidth]{./Images/examples/1306.png}\n  \\end{subfigure}\n  \\hfill\n    \\begin{subfigure}[b]{0.23\\textwidth}    \\includegraphics[height=0.75\\textwidth,width=\\textwidth]{./Images/examples/931.png}%1866 {./Images/examples/4077.png}\n  \\end{subfigure}\n  \\hfill\n      \\begin{subfigure}[b]{0.23\\textwidth}    \\includegraphics[height=0.75\\textwidth,width=\\textwidth]{./Images/examples/31.png}\n  \\end{subfigure}\n  \\vspace{3pt}\n\n  \\begin{subfigure}[b]{0.23\\textwidth}    \\includegraphics[height=0.75\\textwidth,width=\\textwidth]{./Images/examples/2768.png}\n  \\end{subfigure}\n  \\hfill\n  \\begin{subfigure}[b]{0.23\\textwidth}    \\includegraphics[height=0.75\\textwidth,width=\\textwidth]{./Images/examples/2459.png}\n  \\end{subfigure}\n  \\hfill\n    \\begin{subfigure}[b]{0.23\\textwidth}    \\includegraphics[height=0.75\\textwidth,width=\\textwidth]{./Images/examples/2878.png}\n  \\end{subfigure}\n  \\hfill\n      \\begin{subfigure}[b]{0.23\\textwidth}    \\includegraphics[height=0.75\\textwidth,width=\\textwidth]{./Images/examples/2775.png}%2686.png}\n  \\end{subfigure}\n\\vskip -0.05in\n  \\caption{{\\bf Few annotated images from the dataset}. Dot and box annotations are shown in red and blue respectively. The number of objects in each image varies widely, some images contain a dozen of objects while some contains thousands.  \\label{fig:annotation_examples} }\n\n\\end{figure*}\n\\section{Experiments}\n\n\\subsection{Performance Evaluation Metrics} \n\nWe use Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) to measure the accuracy of a counting method. MAE and RMSE are commonly used metrics for counting task~\\cite{zhang2016single,ma2019bayesian,ranjan2018iterative}, and they are defined as follows.\n$MAE = \\frac{1}{n}\\sum_{i=1}^{n} \\lvert c_i - \\hat{c}_i \\rvert; \nRMSE = \\sqrt[]{\\frac{1}{n}\\sum_{i=1}^{n} (c_i - \\hat{c}_i)^2}, \n$\nwhere $n$ is the number of test images, and $c_i$ and $\\hat{c}_i$ are the ground truth and predicted counts.\n\n\\subsection{Comparison with Few-Shot Approaches}\n\\setlength{\\tabcolsep}{2pt}\n\\begin{table}[!tb]\n\\vskip 0.1in\n\\centering\n\\begin{tabular}{lcccc}\n\\toprule\n          &  \\multicolumn{2}{c}{ Val Set} &  \\multicolumn{2}{c}{ Test Set} \\\\\n         \\cmidrule(lr){2-3} \\cmidrule(lr){4-5} \n    Method &  MAE          & RMSE    & MAE          & RMSE         \\\\\n\n\\midrule \nMean & 53.38 & 124.53 & 47.55 & 147.67 \\\\\nMedian & 48.68 & 129.70 & 47.73 & 152.46 \\\\\nFR few-shot detector  \\cite{kang2019few} & 45.45  & 112.53 & 41.64 & 141.04 \\\\\nFSOD few-shot detector \\cite{fan2020few} & 36.36 & 115.00 & 32.53  & 140.65 \\\\\nPre-trained GMN \\cite{lu2018class} & 60.56 &  137.78 & 62.69 & 159.67 \\\\\nGMN \\cite{lu2018class} & 29.66 & 89.81 & 26.52 & 124.57  \\\\\nMAML \\cite{finn2017model} & 25.54 & 79.44 & 24.90  & 112.68 \\\\\nFamNet (Proposed)   & \\textbf{23.75} & \\textbf{69.07} & \\textbf{22.08} & \\textbf{99.54}  \\\\\n\\bottomrule \n\\end{tabular}\n\\vskip -0.1in\n\\caption{Comparing FamNet to two simple baselines (Mean, Median) and four stronger baseline (Feature Reweighting (FR) few-shot detector, FSOD few-shot detector, GMN and MAML), these are few-shot methods that have been adapted and trained for counting. FamNet has the lowest MAE and RMSE on both val and test sets.\n\\label{tab:baseline}}\n\\end{table}\n\nWe compare the performance of FamNet with two trivial baselines and four competing few-shot methods. The two trivial baseline methods are: (1) always output the average object count for training images; (2) always output the median count for the training images. We also implement stronger methods for comparison, by adapting several few-shot methods for the counting task and training them on our training data. Specifically, we adapt the following approaches for counting: the state-of-the-art few-shot detectors~\\cite{kang2019few,fan2020few}, the Generic Matching Network (GMN)~\\cite{lu2018class}, and Model Agnostic Meta Learning (MAML)~\\cite{finn2017model}. \nWe implement MAML using the higher library~\\cite{grefenstette2019generalized}, which is a meta learning library supporting higher order optimization. The training procedure of MAML involves an \\textit{inner optimization loop}, which adapts the network to the specific test classes, and an \\textit{outer optimization loop} which learns meta parameters that facilitate faster generalization to novel tasks. At test time, only the inner optimization is performed. We use the $\\mL_{Adapt}$ loss defined in \\Eref{adaptation3} for the inner optimization loop, and the MSE loss over the entire dot annotation map for the outer optimization loop. \n\nAs can be seen in \\Tref{tab:baseline}, FamNet outperforms all the other methods. Surprisingly, the pre-trained GMN does not work very well, even though it is a class agnostic counting method. The GMN model trained on our training data performs better than its pre-trained version; and this demonstrates the benefits of our dataset. The state-of-the-art few-shot detectors~\\cite{kang2019few,fan2020few} perform relatively poor, even when they are trained on our dataset.\nWith these results, we are the first to show the empirical evidence for the inferiority of the detection-then-counting approach compared to the density estimation approach (GMN, MAML, FamNet) for generic object counting. However, this is not new for the crowd counting research community, where the density estimation approach dominates the recent literature~\\cite{zhang2016single}, thanks to its robustness to occlusion and the freedom of not having to commit to binarized decisions at an early stage. Among the competing approaches, MAML is the best method of all. This is perhaps because MAML is a meta learning method that leverages the advantages of having the FamNet architecture as its core component. The MAML way of training this network leads to a better model than GMN, but it is still inferior to the proposed FamNet together with the proposed training and adaptation algorithms. In terms of training time per epoch, FamNet is around three times faster than MAML, because it does not require any higher order gradient computation like MAML. \n\n\\setlength{\\tabcolsep}{3pt}\n\\begin{table}[!t]\n\\centering\n\\begin{tabular}{lcccc}\n\\toprule\n          &  \\multicolumn{2}{c}{ Val-COCO Set} &  \\multicolumn{2}{c}{ Test-COCO Set} \\\\\n         \\cmidrule(lr){2-3} \\cmidrule(lr){4-5} \n    Method &  MAE          & RMSE    & MAE          & RMSE         \\\\\n\n\\midrule \nFaster R-CNN & 52.79 & 172.46 & 36.20 & 79.59 \\\\\nRetinaNet & 63.57 & 174.36 & 52.67 & 85.86 \\\\\nMask R-CNN & 52.51 & 172.21 & 35.56 & 80.00 \\\\\nFamNet (Proposed)  & \\textbf{39.82} & \\textbf{108.13} & \\textbf{22.76} & \\textbf{45.92}  \\\\\n\n\\bottomrule \n\\end{tabular}\n\\vskip -0.1in\n\\caption{{\\bf Comparing FamNet with pre-trained object detectors}, on counting objects from categories where there are pre-trained object detectors. \n\\label{tab:detectors}}\n\\end{table}\n\n\\subsection{Comparison with Object Detectors}\nOne approach for counting is to use a detector to detect objects and then count. This approach only works for certain categories of objects, where there are detectors for those categories. In general, it requires thousands of examples to train an object detector, so this is not a practical method for general visual counting. Nevertheless, we evaluate the performance of FamNet on a subset of categories from the validation and test sets that have pre-trained object detectors on the COCO dataset. We refer to these subsets as Val-COCO and Test-COCO, which comprise of 277 and 282 images respectively. Specifically, we compare FamNet with FasterRCNN~\\cite{Ren-etal-NIPS15}, MaskRCNN~\\cite{He-etal-ICCV17}, and RetinaNet~\\cite{lin2017focal}. All of these pretrained detectors are available in the Detectron2 library~\\cite{wu2019detectron2}. \\Tref{tab:detectors} shows the comparison results. As can be seen, FamNet outperforms the pre-trained detectors, even on object categories where the detectors have been trained with thousands of annotated examples from the COCO dataset. \n\\setlength{\\tabcolsep}{10pt}\n\\begin{table}[!t]\n\\centering\n\\begin{tabular}{crr}\n\\toprule\nNumber of Exemplars & MAE & RMSE \\\\\n\\midrule \n1 & 26.55 & 77.01 \\\\\n2 & 24.09 & 72.37 \\\\\n3 & 23.75 & 69.07 \\\\\n\\bottomrule \n\\end{tabular}\n\\vskip -0.1in\n\\caption{{Performance of FamNet on the validation data as the number of exemplars increases}. FamNet can provide a reasonable count estimate even with a single exemplar, and the estimate becomes more accurate with more exemplars.  \n\\label{tab:NoExemplars}}\n\\end{table}\n\n\\iffalse\n\\setlength{\\tabcolsep}{10pt}\n\\begin{table}[!t]\n\\centering\n\\begin{tabular}{crr}\n\\toprule\nNumber of Exemplars & MAE & RMSE \\\\\n\\midrule \n1 & 28.49 & 86.21\\\\\n2 & 27.22 & 80.40 \\\\\n3 & 26.80 & 73.83 \\\\\n\\bottomrule \n\\end{tabular}\n\\vskip -0.1in\n\\caption{{\\vr{entire table}Performance of FamNet on the validation data as the number of exemplars increases}. FamNet can provide a reasonable count estimate even with a single exemplar, and the estimate becomes more accurate with more exemplars.  \n\\label{tab:NoExemplars}}\n\\end{table}\n\n\\setlength{\\tabcolsep}{3pt}\n\\begin{table}[!thb]\n\\centering\n\\begin{tabular}{lcccc}\n\\toprule\nComponents & \\multicolumn{4}{c}{Combinations}   \\\\\n\\midrule\nMulti-scale image feature  & \\xmark  & \\checkmark &\\checkmark &\\checkmark \\\\\nMulti-scale exemplar feature & \\xmark & \\xmark &\\checkmark & \\checkmark \\\\\nTest time adaptation & \\xmark& \\xmark& \\xmark& \\checkmark  \\\\\n\\midrule \nMAE & 31.02 & 30.70 & 27.90 & 26.80 \\\\\nRMSE & 101.6 &  95.93 & 79.78 & 73.83 \\\\\n\\bottomrule\n\\end{tabular}\n\\vskip -0.1in\n\\caption{\\vr{entire table}{\\bf Analyzing the components of FamNet}. Each of the components of FamNet adds to the performance. \\label{tab:ablation}}\n\\end{table}\n\\fi\n\n\\setlength{\\tabcolsep}{3pt}\n\\begin{table}[!thb]\n\\centering\n\\begin{tabular}{lcccc}\n\\toprule\nComponents & \\multicolumn{4}{c}{Combinations}   \\\\\n\\midrule\nMulti-scale image feature & \\xmark & \\checkmark &\\checkmark & \\checkmark \\\\\nMulti-scale exemplar feature  & \\xmark  & \\xmark &\\checkmark &\\checkmark \\\\\nTest time adaptation & \\xmark& \\xmark& \\xmark& \\checkmark  \\\\\n\\midrule \nMAE & 32.70 & 27.80 & 24.32 & 23.75 \\\\\nRMSE & 104.31 &  93.53 & 70.94 & 69.07 \\\\\n\\bottomrule\n\\end{tabular}\n\\vskip -0.1in\n\\caption{{\\bf Analyzing the components of FamNet}. Each of the components of FamNet adds to the performance. \\label{tab:ablation}}\n\\end{table}\n\n\\subsection{Ablation Studies}\nWe perform ablation studies on the validation set of FSC-147 to analyze: (1) how the counting performance changes as the number of exemplars increases, and (2) the benefits of different components of FamNet. \n\nIn \\Tref{tab:NoExemplars}, we analyze the performance of FamNet as the number of exemplars is varied between one to three during the testing of FamNet. We see that FamNet can work even with one exemplar, and it outperforms all the competing methods presented in \\Tref{tab:baseline} with just 2 exemplars. Not surprisingly, the performance of FamNet improves as the number of exemplars is increased. This suggests that an user of our system can obtain a reasonable count even with a single exemplar, and they can obtain a more accurate count by providing more exemplars. \n\nIn \\Tref{tab:ablation}, we analyze the importance of the key components of FamNet: multi-scale image feature map, the multi-scale exemplar features, and test time adaptation. We train models without few/all of these components on the training set of FSC-147, and report the validation performance. We notice that all of the components of FamNet are important, and adding each of the component leads to improved results. \n\n\\subsection{Counting category-specific objects}\n\nFamNet is specifically designed to be general, being able to count generic objects with only a few exemplars. As such, it might not be fair to demand it to work extremely well for a specific category, such as counting cars. Cars are popular objects that appear in many datasets and this category is the explicit or implicit target for tuning for many networks, so it would not be surprising if our method does not perform as well as other customized solutions. Having said that, we still investigate the suitability of using FamNet to count cars from the CARPK dataset~\\cite{hsieh2017drone}, which consists of overhead images of parking lots taken by downward facing drone cameras. The training and test set consists of 989 and 459 images respectively. There are around 90,000 instances of cars in the dataset. \n\nWe experiment with two variants of FamNet: a pre-trained model and a model trained on CARPK dataset. The pre-trained FamNet model is called FamNet--, which is trained on FSC-147, without using the data from CARPK or the car category from FSC-147. The FamNet model trained with training data from CARPK is called FamNet+, and it is trained as follows. \nWe randomly sample a set of 12 exemplars from the training set, and use these as the exemplars for all of the training and test images. We train FamNet$+$ on the CARPK training set. \\Tref{tab:carspk} displays the results of several methods on this CARPK dataset.\nFamNet+ outperforms all methods except GMN~\\cite{lu2018class}. GMN, unlike all the other approaches, uses extra training data from the ILSVRC video dataset which consists of video sequences of cars. Perhaps this may be why GMN works particularly well on CARPK.\n\n\\setlength{\\tabcolsep}{3pt}\n\\begin{table}[!tb]\n\\vskip .1in\n\\centering\n\\begin{tabular}{lrrrr}\n\\toprule\nMethod & MAE & RMSE \\\\\n\\midrule \nYOLO~\\cite{Redmon-et-al-CVPR16,hsieh2017drone} & 48.89 & 57.55 \\\\\nFaster RCNN~\\cite{ren2015faster,hsieh2017drone} & 47.45 & 57.39 \\\\\nOne-look Regression~\\cite{mundhenk2016large,hsieh2017drone} & 59.46 & 66.84 \\\\\nFaster RCNN~\\cite{ren2015faster,hsieh2017drone}(RPN-small) & 24.32 & 37.62 \\\\\nSpatially Regularized RPN~\\cite{hsieh2017drone} & 23.80 &36.79 \\\\\nGMN \\cite{lu2018class} & 7.48 & 9.90  \\\\\nFamNet-- (pre-trained)    & 28.84 &  44.47 \\\\\nFamNet+ (trained with CARPK data) & 18.19 & 33.66 \\\\\n\\bottomrule \n\\end{tabular}\n\\vskip -0.1in\n\\caption{{\\bf Counting car performance on the CARPK dataset}. FamNet-- is a FamNet model, that is trained without any CARPK images nor images from the car category of FSC-147. Other methods use the entire CARPK train set. Pre-trained FamNet-- outperforms three of of the previous approaches. FamNet+, yields even better performance. \n\\label{tab:carspk}}\n\\end{table}\n\n\\newcommand\\qualextwo{0.23\\textwidth}\n\\newcommand\\qualexthree{0.15\\textwidth}\n\\newcommand\\heightqualex{2.2cm}\n\\begin{figure}[h]  \n\\centering\n\\makebox[\\qualextwo]{Image}\n\\makebox[\\qualextwo]{Prediction}\n\n    \\includegraphics[width=\\qualextwo,height=\\heightqualex]{Images/Quali/2807_image.png}\n    \\includegraphics[width=\\qualextwo,height=\\heightqualex]{Images/Quali/2807_pred.png} \\\\    \n\\makebox[\\qualextwo]{GT Count: 263}\n\\makebox[\\qualextwo]{Pred Count: 280} \\\\ \\vspace{2ex}\n\n    \\includegraphics[width=\\qualextwo,height=\\heightqualex]{Images/Quali/197_image.png}\n    \\includegraphics[width=\\qualextwo,height=\\heightqualex]{Images/Quali/197_pred.png} \\\\    \n\\makebox[\\qualextwo]{GT Count: 77}\n\\makebox[\\qualextwo]{Pred Count: 77} \\\\ \\vspace{2ex}\n\n    \\includegraphics[width=\\qualextwo,height=\\heightqualex]{Images/Quali/4571_image.png}\n    \\includegraphics[width=\\qualextwo,height=\\heightqualex]{Images/Quali/4571_pred.png} \\\\    \n\\makebox[\\qualextwo]{GT Count: 47}\n\\makebox[\\qualextwo]{Pred Count: 46} \\\\ \\vspace{2ex}\n\n    \\includegraphics[width=\\qualextwo,height=\\heightqualex]{Images/Quali/762_image.png}\n    \\includegraphics[width=\\qualextwo,height=\\heightqualex]{Images/Quali/762_pred.png} \\\\    \n\\makebox[\\qualextwo]{GT Count: 77}\n\\makebox[\\qualextwo]{Pred Count: 192} \\\\ \\vspace{2ex}\n\\iffalse\n    \\includegraphics[width=\\qualexthree,height=\\heightqualex]{Images/Quali/2243_image.png}\n    \\includegraphics[width=\\qualexthree,height=\\heightqualex]{Images/Quali/2243_pred.png}\n        \\includegraphics[width=\\qualexthree,height=\\heightqualex]{Images/Quali/2243_pred_adapt.png}\\\\    \n\\makebox[\\qualexthree]{GT Count: 544}\n\\makebox[\\qualexthree]{Count: 147} \n\\makebox[\\qualexthree]{Count: 177}\\\\ \\vspace{2ex} \\fi\n  \\vskip -0.1in\n  \\caption{{\\bf Predicted density maps and counts of FamNet}. }\n  \\label{fig:Qualitative}\n\\end{figure}\n\n\\begin{figure}[h]  \n\\centering\n\\makebox[\\qualexthree]{Image}\n\\makebox[\\qualexthree]{Pre Adapt.}\n\\makebox[\\qualexthree]{Post Adapt.}\n    \\includegraphics[width=\\qualexthree,height=1.8cm]{Images/Quali/860_image.png}\n    \\includegraphics[width=\\qualexthree,height=1.8cm]{Images/Quali/860_pred.png}\n        \\includegraphics[width=\\qualexthree,height=1.8cm]{Images/Quali/860_pred_adapt.png}\\\\    \n\\makebox[\\qualexthree]{GT Count: 240}\n\\makebox[\\qualexthree]{Count: 356} \n\\makebox[\\qualexthree]{Count: 286}\\\\ \\vspace{2ex}\n  \\vskip -0.1in\n  \\caption{{\\bf Test time adaptation}. Shown are the initial density map (Pre Adapt) and final density map after adaptation (Post Adapt). In case of over counting, adaptation decreases the density values at dense locations. }\n  \\label{fig:Qualitative2}\n\\end{figure}\n\n\\subsection{Qualitative Results}\n\\Fref{fig:Qualitative} shows few images and FamNet predictions. The first three are success cases,and the last is a failure case. For the fourth image, FamNet confuses portions of the background as being the foreground, because of similarity in appearance between the background and the object of interest. \\Fref{fig:Qualitative2} shows a test case where test time adaptation improves on the initial count by decreasing the density values in the dense regions.\n\n\\section{Conclusions}\nIn this paper, we posed counting as a few-shot regression task. Given the non-existence of a suitable dataset for the few-shot counting task, we collected a visual counting dataset with relatively large number of object categories and instances. We also presented a novel approach for density prediction suitable for the few-shot visual counting task. We compared our approach with several state-of-art detectors and few shot counting approaches, and showed that our approach outperforms all of these approaches.\n\n\\myheading{Acknowledgements:} This project is partially supported by MedPod, the SUNY2020 Infrastructure Transportation Security Center, and the NSF I/UCRC Center for Visual and Decision Informatics at Stony Brook.\n\n{\\small\n\\setlength{\\bibsep}{0pt}\n\n}\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{SPot-the-Difference Self-Supervised Pre-training for Anomaly Detection and Segmentation}\n\n\\begin{document}\n\n\\pagestyle{headings}\n\\mainmatter\n\\def\\ECCVSubNumber{2149}  \n\n\\title{SPot-the-Difference Self-Supervised Pre-training for Anomaly Detection and Segmentation}\n\n\\titlerunning{SPot-the-Difference Representation Learning}\n\\author{Yang Zou\\inst{1}, Jongheon Jeong\\inst{2}\\thanks{~~work done during an Amazon internship}, Latha Pemula\\inst{1} \\\\Dongqing Zhang\\inst{1}, Onkar Dabeer\\inst{1}}\n\\authorrunning{Y. Zou et al.}\n\\institute{$^1$AWS AI Labs ~~~~~ $^2$KAIST \\\\\n\\email{\\{yanzo,lppemula,zdongqin,onkardab\\}@amazon.com, jongheonj@kaist.ac.kr}}\n\\maketitle\n\n\\begin{abstract}\n Visual anomaly detection is commonly used in industrial quality inspection. In this paper, we present a new dataset as well as a new self-supervised learning method for ImageNet pre-training to improve anomaly detection and segmentation in 1-class and 2-class 5/10/high-shot training setups. We release the Visual Anomaly (VisA) Dataset consisting of 10,821 high-resolution color images (9,621 normal and 1,200 anomalous samples) covering 12 objects in 3 domains, making it the largest industrial anomaly detection dataset to date. Both image and pixel-level labels are provided. We also propose a new self-supervised framework - SPot-the-difference (SPD) - which can regularize contrastive self-supervised pre-training, such as SimSiam, MoCo and SimCLR, to be more suitable for anomaly detection tasks. Our experiments on VisA and MVTec-AD dataset show that SPD consistently improves these contrastive pre-training baselines and even the supervised pre-training. For example, SPD improves Area Under the Precision-Recall curve (AU-PR) for anomaly segmentation by 5.9\\% and 6.8\\% over SimSiam and supervised pre-training respectively in the 2-class high-shot regime.\n We open-source the project at \\url{http://github.com/amazon-research/spot-diff}.\n\n\\keywords{Representation learning, pre-training, anomaly detection, anomaly segmentation, industrial anomaly dataset}\n\\end{abstract}\n\n\\section{Introduction}\nVisual surface anomaly detection and segmentation identify and localize defects in industrial manufacturing \\cite{bergmann2021mvtec}. While anomaly detection and segmentation are instances of image classification and semantic segmentation problems, respectively, they have unique challenges. First, defects are rare, and it is hard to obtain a large number of anomalous images. Second, common types of anomalies, such as surface scratches and damages, are often small. Fig. ~\\ref{fig:teaser} (a) gives an example. Third, manufacturing is a performance sensitive domain and usually requires highly accurate models. Fourth, inspection in manufacturing spans a wide range of domains and tasks, from detecting leakages in capsules to finding damaged millimeter-sized components on a complex circuit board. % \\Onkar{the defect does not show low-contrast}.\n\nUpon the aforementioned challenges, previous surface anomaly detection models have been typically trained for a particular object and require re-training for different ones. For each object, there are only slight global differences in lighting and object pose/positions across images while the diversity in the defects on objects is large. \nMoreover, due to the rarity of anomalous data, there has been a predominant focus on 1-class anomaly detection, which only requires normal images for model training \\cite{caron2020unsupervised,cohen2020sub,defard2021padim,li2021cutpaste,Roth_2022_CVPR,yi2020patch}. In mature manufacturing domains, anomalous samples are also available and sometimes sufficient. In such cases, one can improve over 1-class methods with a standard 2-class \nmodel \\cite{cui2019class,feng2021few,gornitz2013toward,lin2017focal} by incorporating the anomalous data in training, which is in fact a well-established practice in % , as is the case in some \ncommercial visual inspection AI services \\cite{LfV,VIAI}.\nFor both setups, existing state-of-the-art methods for surface anomaly detection commonly leverage supervised representations pre-trained on ImageNet \\cite{deng2009imagenet}, \neither as feature extractors \\cite{defard2021padim,Roth_2022_CVPR} or as initialization for fine-tuning on the target dataset \\cite{li2021cutpaste,yi2020patch}.\n\n\\begin{figure}[!t]\n  \\centering\n  \\includegraphics[width=0.95\\linewidth]{figs/teaser.png}\n  \\caption{(a) Normal and anomalous samples of VisA - PCB1 with real defect (molten metal), anomaly highlighted by red ellipse; (b) A pair of images for the spot-the-difference (SPD) puzzle \\cite{jhamtani2018learning}; (c) An anchor image and its variant augmented by SmoothBlend for synthetic spot-the-difference; (d) GradCAM attention visualization for PCB1 - Anomaly image based on self-supervised ImageNet pre-training w/wo proposed SPD. With SPD, attention is more focused on the local defects.}\n  \\label{fig:teaser}\n\\end{figure}\n\nMeanwhile, recent advances in self-supervised learning (SSL) have shown that pre-trained representations learned without categorical labels might be a better choice for transfer learning compared to those from supervised in object detection and segmentation \\cite{chen2020simple,chen2021exploring,he2020momentum}. However, their application to anomaly detection and segmentation is underdeveloped. SSL for surface anomaly detection was explored in CutPaste \\cite{li2021cutpaste} to learn representation from downstream images for each specific object. However, such representations hardly generalize to different objects and can lead to overfitting in a practical setting where only 1-20 normal samples are available. Also, there are previous works focusing on SSL for high-level semantic anomaly detection such as cat among a distribution of dogs \\cite{cook2020consult,davis2006relationship,saito2015precision}. However, as \\cite{ruff2021unifying} pointed out, surface anomaly detection aims to spot the low-level textual anomalies such as scratch and crack which has challenges different from semantic anomaly detection. Until now, the universal self-supervised pre-trained representation with good generalization ability have not yet been attempted for surface anomaly detection and segmentation.\n\nRegarding the evaluation protocol, the community has been experiencing the lack of challenging benchmarks. The popular MVTec Anomaly Detection (AD) benchmark \\cite{bergmann2021mvtec} is  saturating with the Area Under the Receiver Operating Characteristic (AU-ROC) approaching $\\sim$95\\% \\cite{defard2021padim,li2021cutpaste}, and the benchmark is limited to the 1-class setup. But the anomaly detection problems in practice is still far from solved, demanding new datasets and metrics that better represent the real-world. In this paper, we introduce a new challenging Visual Anomaly (VisA) dataset. VisA is collected to present several new characteristics: objects with complex structures such as printed circuit board (PCB), multiple instances with different locations in a single view, 12 different objects spanning 3 domains, and multiple anomaly classes (up to 9) for each object. VisA contains 10,821 high-resolution color images - 9,621 normal and 1,200 anomalous - with both image and pixel-level labels. To our best knowledge, VisA is currently the largest and most challenging public dataset for anomaly classification and segmentation. Moreover, to cover different use cases in practice, we establish benchmarks not only in standard 1-class training setup but also 2-class training setups with 5/10/high-shot. For evaluation, we propose to use Area Under the Precision-Recall curve (AU-PR) in combination with standard AU-ROC. In the imbalanced defect dataset, AU-ROC might present inflated view of performances and AU-PR is more informative to measure anomaly detection performance \\cite{cook2020consult,davis2006relationship,saito2015precision}.\n\nIn addition to an improved dataset, we also explore self-supervision to improve anomaly detection. As we argue below, our hypothesis is that previous contrastive SSL methods \\cite{chen2020simple,chen2021exploring,he2020momentum} are sub-optimal to transfer learning for anomaly detection. Specifically, SimCLR, MoCo and other methods regard globally augmented images of a given image as one class and other images in the same batch as negative classes. Transformations, such as cropping and color jittering, are applied globally to the anchor for positives generation. The InfoNCE or cosine similarity losses \\cite{chen2020simple,chen2021exploring,he2020momentum} encourage invariance to these global deformations, and capturing semantic information instead of local details \\cite{geirhos2020on}. However, anomaly detection relies on local textual details to spot defects. Thus the subtle and local intra-object (or intra-class) differences are important but not well modeled by previous methods. Figure \\ref{fig:teaser} (d) illustrates the sub-optimality in one of the previous SSL methods using the GradCAM attention map \\cite{selvaraju2017grad}. As far as we know, improving representations by self-supervision for better downstream anomaly detection/segmentation has not been studied before and we explore this angle. \n\nInspired by the spot-the-difference puzzle shown in Fig. \\ref{fig:teaser} (b), we propose a contrastive SPot-the-Difference (SPD) training to promote the local sensitivity of previous SSL methods. In the puzzle, players need to be sensitive to the subtle differences between the two globally alike images, which is similar to anomaly detection. In the contrastive SPD training, as shown in Fig. \\ref{fig:teaser} (c), a novel augmentation called SmoothBlend is proposed to produce the local perturbations on SPD negatives for synthetic spot-the-difference. The (locally) augmented images are regarded as negatives, which is different from regarding (globally) augmented images as positives in SimCLR/MoCo. Moreover, weak global augmentations, such as weak cropping and color jittering, are also applied to the SPD negatives as anomaly detection should spot defects under slight global changes in lighting and object pose/position. Additionally, to prevent models from using the slight global changes as shortcuts to differentiate negatives, SPD positives are generated by applying weak global augmentations on the anchor. Lastly, SPD training minimizes the feature similarities between SPD negative pairs while maximizing the similarities between SPD positives, which encourages models to be locally sensitive to anomalous patterns and invariant to slight global variations.\n\nOur main contributions are as follows:\n\\begin{enumerate}\n    \\item We propose a new VisA dataset, 2$\\times$ larger than MVTec-AD, with both image and pixel-level annotations. It spans 12 objects across 3 domains, with challenging scenarios including complex structures in objects, multiple instances and object pose/location variations. Moreover, we establish both 1-class and 5/10/high-shot 2-class benchmarks to cover different use cases.\n    \\item To promote the local sensitivity to anomalous patterns, a SPot-the-Difference (SPD) training is proposed to regularize self-supervised ImageNet pre-training, which benefits their transfer-learning ability for anomaly detection and localization. As far as we know, we are the first one to explore self-supervised pre-training on large-scale datasets for surface defect detection tasks. \n    \\item Compared to strong self-supervised pre-training baselines such as SimSiam, MoCo and SimCLR, extensive experiments show our proposed SPD learning improves them for better anomaly detection and segmentation. We also show the SPD improves over supervised ImageNet pre-training for both tasks. \n\\end{enumerate}\n\\section{Related Works}\n\\noindent\\textbf{Unsupervised Anomaly Detection and Segmentation} use only normal samples to train models, which have drawn extensive attention. Many recent methods are proposed to detect low-level texture anomalies \\cite{ruff2021unifying}, such as scratches and cracks, which are common cases in industrial visual inspection \\cite{Deng_2022_CVPR,Ristea-CVPR-2022,RudWeh2022,yi2020patch}. SPADE \\cite{defard2021padim} and PatchCore \\cite{Roth_2022_CVPR} extract features at patch level and use nearest neighbor methods to classify patches and images as anomalies. PaDiM \\cite{defard2021padim} learns a parametric distribution over patches for anomaly detection. CutPaste \\cite{li2021cutpaste} learns a representation based on images augmented by cut-and-pasted patches. The supervised ImageNet models are used in these methods either as feature extractors or initialization for fine-tuning. However, self-supervised pre-training on large-scale datasets is an unexplored area for quality inspection applications. In addition, several works \\cite{reiss2021mean,sohn2021learning,tack2020csi,pmlr-v80-ruff18a} focus on high-level semantic anomaly detection. As mentioned in \\cite{ruff2021unifying}, semantic anomaly detection approaches can be less effective for texture anomaly detection as their challenges are different.\n\n\\noindent\\textbf{Self-Supervised Learning (SSL)} have gathered momentum in the last 5 years. Several surrogate tasks have been proposed for self-supervision, such as image colorization \\cite{zhang2016colorful}, rotation prediction \\cite{gidaris2018unsupervised}, jigsaw puzzles \\cite{noroozi2016unsupervised}. Recently, multi-view based methods such as MoCo \\cite{he2020momentum}, SimCLR \\cite{chen2020simple}, SimSiam \\cite{chen2021exploring} and BYOL \\cite{grill2020bootstrap} present better or comparable performances than supervised pre-training in transfer learning tasks including image classification, object detection \\cite{Yang_2021_CVPR} and semantic segmentation \\cite{wang2021dense}. Moreover, to promote spatial details of representations for localization tasks, several approaches proposed to encourage the invariance of patch features to global augmentations \\cite{wang2021dense,xie2021detco,liu2020self,chen2021multisiam}, although they may not lead to local sensitivity to tiny defects. As far as we know, none of these works explored their generalization ability to surface defect detection tasks. \n\n\\begin{figure}[!t]\n \\centering\n\\includegraphics[width=.8\\linewidth]{figs/ssl_comp.png}\n\\caption{(a) Contrastive learning in SimCLR, MoCo and SimSiam; (b) Contrastive learning in SPD training. Local deformation in SPD negative is highlighted by circle.}\n\\label{fig:ssl_comp}\n\\end{figure}\n\n\\section{SPot-the-Difference (SPD) Regularization}\nTo promote local sensitivity of standard self-supervised contrastive learning, we propose a contrastive SPot-the-Difference (SPD) regularization. As mentioned earlier, SPD aims to increase model invariance to slight global changes by maximizing the feature similarity between an image and its weak global augmentation, while forcing dissimilarity for local perturbations, as shown in Fig. \\ref{fig:ssl_comp} (b). In the following, we first present background in contrastive learning, and then the augmentations used in SPD followed by the learning with SPD.\n\\subsection{Background on Self-supervised Contrastive Learning}\n\\begin{figure}[!t]\n\\centering\n\\includegraphics[width=\\linewidth]{figs/aug_comp.png}\n\\caption{(a) Samples for synthetic spot-the-difference; (b) Augmentation comparison}\n\\label{fig:aug_compare}\n\\end{figure}\n\nMany self-supervised learning methods, such as SimCLR \\cite{chen2020simple} and MoCo \\cite{he2020momentum}, are based on contrastive learning. As shown in Fig. \\ref{fig:ssl_comp} (a), given an image, these methods maximize the feature similarity between two strongly augmented samples $x_i$ and $\\hat{x}_i$ while minimizing the similarities between the anchor $x_i$ and other images $x_j$'s in the same batch of size $N$. Strong global augmentations, such as grayscaling, large cropping and strong color jittering, are used to get positives. Typically, an encoder extracts features $h_i,\\hat{h}_i$ and $h_j$'s which are inputs to a multilayer perceptron (MLP) head. The MLP head extracts the L2 normalized embeddings $z_i,\\hat{z}_i$ and $z_j$'s to compute the InfoNCE loss defined as follows.\n\\begin{equation}\n    \\mathcal{L}_{\\mathrm{NCE}}(x_i,\\hat{x}_i)= -\\log\\frac{\\exp{(z_i\\cdot \\hat{z}_i/\\tau)}}{\\exp{(z_i\\cdot \\hat{z}_i/\\tau)}+\\sum_{j=1}^{N}\\mathbbm{1}_{j\\neq i}\\exp{(z_i,z_j}/\\tau)}\n    \\label{eqn:loss_nce}\n\\end{equation}\n$\\tau$ is a temperature scaling hyperparameter. In addition, SimSiam \\cite{chen2021exploring} shows that self-supervised models can be trained even without negatives where only similarity modeling is implemented for positives.\n\n\\noindent\\textbf{Remark:} Images augmented by most strong global transformations in SSL, such as grayscaling and large cropping, share semantics with anchor but with different local details (a dog v.s. a dog head). Thus to maximize their similarity, the features are forced to be invariant about local details and capture the global semantics. This is even enforced by minimizing similarities between anchor and different images in a batch as they have different global structures \\cite{chen2020simple,ericsson2021self}. This further motivates us to promote local sensitivity in SSL for anomaly detection.\n\\subsection{Augmentations for SPD}\n\\noindent\\textbf{Local augmentation:} In SPD, the locally deformed images, rather than other images of a batch in standard contrastive training, are used as negatives. SmoothBlend is proposed to produce local deformations. The first column in Fig. \\ref{fig:aug_compare} (b) presents the samples augmented by SmoothBlend. It is implemented by a smoothed alpha blending between an image and a small randomly cut patch of the same image. Specifically, color jittering is applied to a cut patch. Then an all-zero foreground layer $u$ is created with the patch pasted to a random location. An alpha mask $\\alpha$ is created where the pixels corresponding to the pasted patch are set to $1$ otherwise $0$, followed by a Gaussian blur. Finally, the augmented sample is obtained by $\\bar{x} = (1-\\alpha) \\odot x + \\alpha \\odot u$. $\\odot$ is the element-wise product. \n\n\\noindent\\textbf{Global augmentation:} To generate global variations for both SPD positives and negatives, we use weak global augmentation. Adding global variations to SPD is motivated by the potentially small global variations in realistic manufacturing environment, such as lighting, object positions, etc. To simulate such slight changes, we choose weak random cropping, Gaussian blurring, horizontal flipping and color jittering. Such weak global augmentations are different from strong transformations used in SimSiam, SimCLR and MoCo which is illustrated by last two columns in Fig. \\ref{fig:aug_compare} (b). As we can see, there might be just $20\\%$ overlap between the anchor and strongly augmented positive. If the network is designed to maximize the distance between negatives with only subtle changes while minimizing the distance between positives with largely global transformations, it is a confusing task which might harm representation learning for anomaly detection.\n\n\\noindent\\textbf{Remark:} SmoothBlend is a smoothed version of CutPaste augmentation proposed in \\cite{li2021cutpaste}. Both of them can be used to generate structural local deformations, illustrated by the first two columns in Fig. \\ref{fig:aug_compare} (b). Unlike the sharp edges of the CutPaste patches, the local and subtle perturbations with smooth edges from SmoothBlend provides a challenging puzzle for models. \n\\begin{figure}[!t]\n \\centering\n\\includegraphics[width=\\linewidth]{figs/spd_reg_v2.png}\n\\caption{The contrastive spot-the-difference learning}\n\\label{fig:spd_reg}\n\\end{figure}\n\\subsection{Training with SPD}\nBased on the above augmentations, we propose the SPD learning illustrated by Fig. \\ref{fig:spd_reg} with Fig. \\ref{fig:aug_compare} (a) presents more SPD training samples. For an anchor image $x_i$, a negative $\\tilde{x}_i^-$ is generated by applying weak global augmentations followed by SmoothBlend. The positive $\\tilde{x}_i^+$  is produced by weak global transformations only. Then a shared feature extractor $f(\\cdot)$ extracts the representations $h_i, \\tilde{h}_i^-, \\tilde{h}_i^+$ ($h_i$'s are used for downstream anomaly detection tasks). They are further inputted into a shared multilayer perceptron (MLP) $g(\\cdot)$ to get the projections $z_i, \\tilde{z}_i^-,\\tilde{z}_i^+$. The cosine similarity between $z_i, \\tilde{z}_i^-$ is minimized while similarity between $z_i, \\tilde{z}_i^+$ is maximized. In summary, the SPD learning minimizes the following SPD loss. \n\\begin{equation}\n    \\mathcal{L}_{\\mathrm{SPD}}(x_i,\\tilde{x}_i^-,\\tilde{x}_i^+)=\n\\cos(z_i, \\tilde{z}_i^-)-\\cos(z_i, \\tilde{z}_i^+).\n    \\label{eqn:l_spd}\n\\end{equation}\n\n\\noindent\\textbf{Standard contrastive SSL with SPD:} Regularizing SSL with SPD is simple. Taking SimCLR as an example baseline, for a given image, SimCLR generates the anchor $x_i$ and positive $\\hat{x}_i$ via strong global augmentations with other images $x_j$'s in the same batch as negatives. Then SPD positives $\\hat{x}_i^+$ and negatives $\\hat{x}_i^-$ are generated by SmoothBlend and weak global augmentations. The shared encoder and MLP head in SimCLR are used to extract the image feature projections for loss computation. Finally the network is trained by the following combined loss.\n\\begin{equation}\n    \\mathcal{L}(x_i,\\hat{x}_i,\\tilde{x}_i^-,\\tilde{x}_i^+) = \\mathcal{L}_{\\mathrm{NCE}}(x_i,\\hat{x}_i) + \\eta \\cdot \\mathcal{L}_{\\mathrm{SPD}}(x_i,\\tilde{x}_i^-,\\tilde{x}_i^+)\n\\end{equation}\nSimilary, we can apply SPD to MoCo. For SimSiam, $\\mathcal{L}_{\\mathrm{NCE}}(x_i,\\hat{x}_i)$ loss is replaced by a cosine distance loss for positive pairs without considering negatives \\cite{chen2021exploring}.\n\n\\noindent\\textbf{Standard supervised pre-training with SPD:} With the class labels, standard supervised pre-trained features also capture global semantics to distinguish categories with less attention to local details, similar to SSL. Thus SPD could improve its local sensitivity. Specifically, on top of the last feature layer of the standard supervised model (ResNet-50 \\cite{he2016deep}), an auxiliary classifier is added to classify if an augmented SPD image has a local perturbation or not, which is trained by cross-entropy loss. The backbone is shared to extract features.\n\\section{Visual Anomaly (VisA) Dataset}\n\\subsection{Dataset Description}\n\\begin{wraptable}[11]{r}{0.55\\textwidth}\n\\setlength\\abovecaptionskip{-1.8\\baselineskip}\n\\scriptsize\n\\centering\n\\caption{Overview of VisA dataset}\n\\medskip\n\\label{tab:ava_stat}\n\\begin{adjustbox}{width=\\linewidth}\n\\begin{tabular}{c|c|c|c|c}\n\\hline\n                                                                               & Object   & \\# normal & \\# anomaly & \\# anomaly \\\\\n &             & samples & samples & classes \\\\ \\hline\n\\multirow{4}{*}{\\begin{tabular}[c]{@{}c@{}}Complex \\\\ structure\\end{tabular}}  & PCB1     & 1,004     & 100        & 4          \\\\\n & PCB2        & 1,001   & 100     & 4       \\\\\n & PCB3        & 1,006   & 100     & 4       \\\\\n & PCB4        & 1,005   & 100     & 7       \\\\ \\hline\n\\multirow{4}{*}{\\begin{tabular}[c]{@{}c@{}}Multiple \\\\ instances\\end{tabular}} & Capsules & 602       & 100        & 5          \\\\\n & Candle      & 1,000   & 100     & 8       \\\\\n & Macaroni1   & 1,000   & 100     & 7       \\\\\n & Macaroni2   & 1,000   & 100     & 7       \\\\ \\hline\n\\multirow{4}{*}{\\begin{tabular}[c]{@{}c@{}}Single\\\\  instance\\end{tabular}}   & Cashew      & 500     & 100     & 9       \\\\\n & Chewing gum & 503     & 100     & 6       \\\\ \n & Fryum    & 500       & 100        & 8          \\\\\n & Pipe fryum  & 500     & 100     & 6       \\\\ \\hline\n\\end{tabular}\n\\end{adjustbox}\n\\end{wraptable}\nThe VisA dataset contains 12 subsets corresponding to 12 different objects. Fig. \\ref{fig:ava_samples} gives images in VisA. There are 10,821 images with 9,621 normal and 1,200 anomalous samples. Four subsets are different types of printed circuit boards (PCB) with relatively complex structures containing transistors, capacitors, chips, etc. For the case of multiple instances in a view, we collect four subsets: Capsules, Candles, Macaroni1 and Macaroni2. Instances in Capsules and Macaroni2 largely differ in locations and poses. Moreover, we collect four subsets including Cashew, Chewing gum, Fryum and Pipe fryum, where objects are roughly aligned. The anomalous images contain various flaws, including surface defects such as scratches, dents, color spots or crack, and structural defects like misplacement or missing parts. There are 5-20 images per defect type and an image may contain multiple defects. The defects were manually generated to produce realistic anomalies. All images were acquired using a $4,000\\times6,000$ high-resolution RGB sensor. Both image and pixel-level annotations are provided. Table \\ref{tab:ava_stat} gives the statistics of VisA dataset. \n\\begin{figure}[!t]\n \\centering\n\\includegraphics[width=\\linewidth]{figs/ava_dataset_uncropped.png}\n\\caption{Samples of VisA datasets. First row: normal images; Second row: anomalous images; Third row: anomalies viewed by zooming in.}\n\\label{fig:ava_samples}\n\\end{figure}\n\nFig. \\ref{fig:ava_vs_mvtec} illustrates the differences between VisA and MVTec-AD. First, VisA considers more complex structures, comparing the VisA - PCB3 with multiple electronic components to a single one of MVTec - transistor as an example. Second, multiple objects can appear in VisA (Capsules) as opposed to a single object in MVTec-AD. Third, large variation in object locations is covered by VisA (Capsules) while almost all objects in MVTec-AD are roughly aligned. Lastly, MVTec-AD has $5,354$ images and VisA is 2$\\times$ larger with $10,821$ images.\n\\subsection{Evaluation Protocol and Metrics}\nWe establish three evaluation protocols for each of 12 objects in VisA dataset. First, following MVTec-AD 1-class protocol, we establish VisA 1-class protocol by assigning $90\\%$ normal images to train set while $10\\%$ normal images and all anomalous samples are grouped as test set. Second, we establish 2-class high/low-shot evaluation protocols as proxies for realistic 2-class setups in commercial products \\cite{LfV,VIAI}. In high-shot setup, for each object, $60\\%$/$40\\%$ normal and anomalous images are assigned to train/test set respectively. For low-shot benchmark, firstly, $20\\%$/$80\\%$ normal and anomalous images are grouped to train/test set respectively. Then the k-shot (k=5,10) setup randomly samples k images from both classes in train set for training. The averaged performances over 5 random runs will be reported. Note that for both 1-class and 2-class training setups, test sets have samples from both classes. In addition, we report model performances averaged over all subsets of VisA and MVTec-AD in Sec. \\ref{sec:exp}. The model performances for each subset are reported in Sec. \\textcolor{red}{D} of supplementary.\n\n\\begin{figure}[!t]\n \\centering\n\\includegraphics[width=.95\\linewidth]{figs/ava_vs_mvtec2.png}\n\\caption{Comparing VisA and MVTec-AD. VisA is more challenging due to the complex object structures, multiple instances, large variations of objects and scale.}\n\\label{fig:ava_vs_mvtec}\n\\end{figure}\n\nFor metrics, we report Area Under Precision-Recall curve (AU-PR) in combination with the Area Under Receiver Operator Characteristic curve (AU-ROC). AU-ROC is the most widely used metric for anomaly detection tasks \\cite{defard2021padim,Roth_2022_CVPR,yi2020patch}. But as pointed out in \\cite{cook2020consult,davis2006relationship,saito2015precision}, in imbalanced dataset where performance of minor class is more important, AU-ROC might provide an inflated view of performance which may cause challenges in measuring models' true capabilities. This is true for anomaly detection where anomalies are often rare. In \\cite{bergmann2021mvtec}, the best method is Student-Teacher \\cite{bergmann2020uninformed} with $92.2\\%$ AU-ROC which seems to be close to perfection. However, it only gets $59.9\\%$ AU-PR which is far-from satisfactory. The imbalance issue is more extreme in anomaly segmentation where normal pixels (negatives) can be tens/hundreds times more than anomalous pixels (positives). Even for a bad model, the false positive rate can be small due to numerous negatives, leading to a high AU-ROC. Thus we argue AU-PR is a better performance measurement. Our experiments also demonstrate this point.\n\n\\section{Experiments}\\label{sec:exp}\n\n\\noindent\\textbf{Datasets:} For self-supervised as well as supervised pre-training, we use ImageNet 2012 classification dataset \\cite{deng2009imagenet}. ImageNet consists $1,000$ classes with $1.28$ million training images. For downstream tasks, in addition to our VisA dataset, we use MVTec-AD dataset \\cite{bergmann2019mvtec} as a 1-class training benchmark. MVTec-AD contains $15$ sub-datasets with a total of $5,354$ images.\n\n\\noindent\\textbf{Anomaly detection and segmentation algorithms:} To evaluate the transfer learning performances of different pre-training, we adopt the following algorithms for anomaly detection and segmentation. \n\n\\noindent\\emph{1-class anomaly classification/segmentation:} We leverage PaDiM \\cite{defard2021padim} which is one of the top performing 1-class anomaly detection/localization methods. \n\n\\noindent\\emph{2-class anomaly classification/segmentation:} We train a standard binary ResNet \\cite{he2016deep} as the supervised model for classification. A U-Net \\cite{ronneberger2015u} is used as segmentation model. The focal loss \\cite{lin2017focal} is used to overcome the data imbalance. \n\n\\noindent\\textbf{Implementation details:} Unless otherwise noted, we choose ResNet-50 as the major backbone. We adopt exactly the same hyperparameters in SimSiam, MoCo, SimCLR and supervised learning for pre-training. More implementation details are in the supplementary.\n\n\\subsection{SPD in high-shot 1-class/2-class Regimes}\n\\begin{figure}[!t]\n\\centering\n\\includegraphics[width=0.45\\textwidth]{figs/scatter_plot/ava_1class_aupr.png}\n\\includegraphics[width=0.45\\textwidth]{figs/scatter_plot/mvtec_1class_aupr.png}\\\\\n \\caption{Scatter plots for various ImageNet pre-training models in 1-class setup.}\n \\label{fig:scatter}\n\\end{figure}\n\n\\begin{table*}[!t]\n\t\\centering\n    \\caption{1-class performance evaluation of various ImageNet pre-training options on VisA and MVTec-AD with PaDiM. Bold numbers refers to the highest score. In the brackets are the gaps to the ImageNet supervised/self-supervised pre-training counterpart. In green are the gaps of at least \\textcolor{forestgreen}{+0.5} point.} \n    \\label{tab:1cls}\n\t\\setlength{\\tabcolsep}{7pt}\n\t\\resizebox{\\linewidth}{!}{\n\t\\centering\n    \\begin{tabular}{c|c|llll||llll}\n    \\hline\n    \\multicolumn{1}{l|}{\\multirow{2}{*}{}} &\n      \\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}ImageNet\\\\ labels\\end{tabular}} &\n      \\multicolumn{4}{c||}{VisA (1-class)} &\n      \\multicolumn{4}{c}{MVTec-AD (1-class)} \\\\ \\cline{3-10} \n    \\multicolumn{1}{l|}{} &\n       &\n      \\multicolumn{2}{c|}{Classification} &\n      \\multicolumn{2}{c||}{Segmentation} &\n      \\multicolumn{2}{c|}{Classification} &\n      \\multicolumn{2}{c}{Segmentation} \\\\ \\hline\n                         &  & AU-PR & \\multicolumn{1}{l|}{AU-ROC} & AU-PR & AU-ROC & AU-PR & \\multicolumn{1}{l|}{AU-ROC} & AU-PR & AU-ROC \\\\ \\hline\n    Sup. pre-train       & \\cmark & 88.2 & \\multicolumn{1}{l|}{87.8}   & 11.4 & 93.1   & 97.4  & \\multicolumn{1}{l|}{94.5}  & 35.2 & 94.4  \\\\ \\hline\\hline\n    SimSiam              & \\xmark & 80.2 & \\multicolumn{1}{l|}{78.1}   & 9.1  & 93.1  & 92.6 & \\multicolumn{1}{l|}{83.9}  & 29.7 & 92.1  \\\\\n    +SPD                 & \\xmark & 82.8 \\textcolor{forestgreen}{(+2.6)} & \\multicolumn{1}{c|}{81.2 \\textcolor{forestgreen}{(+3.1)}}  & 9.4 (+0.3) & 92.7 (-0.4) & 94.1 \\textcolor{forestgreen}{(+1.5)} & \\multicolumn{1}{l|}{88.0 \\textcolor{forestgreen}{(+4.1)}}  & 32.0 \\textcolor{forestgreen}{(+2.3)} & 92.2 (+0.1) \\\\ \\hline\n    MoCo                 & \\xmark & 83.6 & \\multicolumn{1}{l|}{83.4}  & 10.5 & 93.4  & 95.0 & \\multicolumn{1}{l|}{90.4}  & 33.2 & 93.4  \\\\\n    +SPD                 & \\xmark & 84.1 \\textcolor{forestgreen}{(+0.5)} & \\multicolumn{1}{l|}{83.0 (-0.4)}  & 11.0 \\textcolor{forestgreen}{(+0.5)} & 93.5 (+0.1) & 95.6 \\textcolor{forestgreen}{(+0.6)} & \\multicolumn{1}{l|}{90.5 (+0.1)}  & 33.5 (+0.3) & 93.5 (+0.1) \\\\ \\hline\n    SimCLR               & \\xmark & 82.7 & \\multicolumn{1}{l|}{81.6}  & 8.8  & 89.7   & 94.7 & \\multicolumn{1}{l|}{90.7}  & 29.8 & 92.1  \\\\\n    +SPD                 & \\xmark & 83.9 \\textcolor{forestgreen}{(+0.8)} & \\multicolumn{1}{l|}{82.6 \\textcolor{forestgreen}{(+1.0)}}  & 8.7 (-0.1) & 89.9 (+0.2) & 96.8 \\textcolor{forestgreen}{(+2.1)} & \\multicolumn{1}{l|}{93.8 \\textcolor{forestgreen}{(+3.1)}}  & 31.7 \\textcolor{forestgreen}{(+1.9)} & 92.9 \\textcolor{forestgreen}{(+0.8)} \\\\ \\hline\n    Sup. pre-train+SPD & \\cmark & \\textbf{88.6} (+0.4) & \\multicolumn{1}{l|}{\\textbf{87.8} (+0.0)}  & \\textbf{12.0} \\textcolor{forestgreen}{(+0.6)} & \\textbf{93.8} \\textcolor{forestgreen}{(+0.7)}  & \\textbf{97.5} (+0.1) & \\multicolumn{1}{l|}{\\textbf{94.6} (+0.1)}  & \\textbf{36.3} \\textcolor{forestgreen}{(+1.1)} & \\textbf{94.6} (+0.2) \\\\ \\hline \n    \\end{tabular}\n\t}\n\\end{table*}\n\nFor the 1-class setting, the results of PaDiM with various pre-training options w/wo SPD are shown in Table \\ref{tab:1cls}. The results are also visualized as scatter plots in Fig. \\ref{fig:scatter}. We have several key observations. First, SPD improves performances of both anomaly detection and segmentation across almost all pre-training baselines on both VisA and MVTec-AD. While we report both AU-PR and AU-ROC, the former metric is more relevant to the application and we see that self-supervised methods are improved up to AU-PR of $2.6\\%$. Note both metrics are averaged over the 12 objects in VisA. For different objects, the gains differ and are given in Sec. \\textcolor{red}{D} of the supplementary. Second, the gap between self-supervised pre-training with SimSiam, SimCLR, MoCo, and supervised pre-training is large. SPD reduces this gap, but no combination of SSL and SPD beats supervised pre-training. This is in contrast to the low-shot regime in Section \\ref{sect:few-shot-results}, where self-supervision has advantages in some cases. Third, PaDiM is one of the SOTA methods with $>97\\%$ AU-ROC in MVTec. But it just achieves $<90\\%$ AU-PR and AU-ROC in VisA - classification. For VisA - segmentation, PaDiM only achieves about $10\\%$ AU-PR. This shows the difficulty of the VisA 1-class benchmark. Moreover, the gap between low AU-PR and high AU-ROC for both VisA/MVTec segmentation justifies the inflated performance view of AU-ROC, in favor of AU-PR as a more suitable metric in imbalanced datasets. In addition, even in terms of AU-ROC, the SPD consistently improves almost all baselines. \n\n\\begin{table*}[!t]\n\t\\centering\n    \\caption{2-class fine-tuning with different pre-training on VisA high-shot setup.}\n    \\label{tab:ava_2cls_high}\n\t\\setlength{\\tabcolsep}{7pt}\n\t\\resizebox{0.75\\linewidth}{!}{\n\t\\centering\n    \\begin{tabular}{c|c|llll}\n    \\hline\n    \\multirow{2}{*}{} & \\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}ImageNet\\\\ labels\\end{tabular}} & \\multicolumn{4}{c}{VisA (2-class, high-shot)} \\\\ \\cline{3-6} \n                       &  & \\multicolumn{2}{c|}{Classification} & \\multicolumn{2}{c}{Segmentation} \\\\ \\hline\n                       &  & AU-PR & \\multicolumn{1}{c|}{AU-ROC} & AU-PR          & AU-ROC          \\\\ \\hline\n    Sup. pre-train     & \\cmark & 97.5  & \\multicolumn{1}{l|}{99.5}  & 65.1           & 97.3            \\\\ \\hline \\hline\n    SimSiam            & \\xmark & 88.7  & \\multicolumn{1}{l|}{97.9}   & 53.8           & 97.3            \\\\\n    +SPD               & \\xmark & 93.2 \\textcolor{forestgreen}{(+4.5)} & \\multicolumn{1}{l|}{98.7 \\textcolor{forestgreen}{(+0.8)}}  & 59.7 \\textcolor{forestgreen}{(+5.9)}          & 98.1 \\textcolor{forestgreen}{(+0.8)}           \\\\ \\hline\n    MoCo               & \\xmark & 93.9  & \\multicolumn{1}{l|}{98.8}   & 62.4           & 98.0            \\\\\n    +SPD               & \\xmark & 94.2 (+0.3) & \\multicolumn{1}{l|}{98.8 (+0.0)}   & 64.4 \\textcolor{forestgreen}{(+2.0)}          & 97.9 (-0.1)           \\\\ \\hline\n    SimCLR             & \\xmark & 93.4  & \\multicolumn{1}{l|}{98.5}   & 67.7           & 95.3            \\\\\n    +SPD               & \\xmark & 92.7 (-0.7) & \\multicolumn{1}{l|}{98.6 (+0.1)}   & 68.2 \\textcolor{forestgreen}{(+0.5)}          & 95.7 (+0.4)           \\\\ \\hline\n    Sup. pre-train+SPD & \\cmark & \\textbf{98.3} \\textcolor{forestgreen}{(+0.8)} & \\multicolumn{1}{l|}{\\textbf{99.7} (+0.2)}   & \\textbf{71.9} \\textcolor{forestgreen}{(+6.8)}          & \\textbf{98.5} \\textcolor{forestgreen}{(+1.2)}           \\\\ \\hline\n    \\end{tabular}%\n    }\n\\end{table*}\n\nIn Table \\ref{tab:ava_2cls_high}, we show the results for the 2-class high-shot regime on the VisA and observe similar trends as above. However, the AU-PR gains from SPD on top of SimSiam and supervised pre-training are higher at 5.9\\% and 6.8\\% respectively for segmentation. Another key point to note here is that the AU-ROC metrics are saturating even though AU-PR metrics show room for improvement, particularly for segmentation. This another data point for preferring AU-PR metric. \nComparing Tables \\ref{tab:1cls} and \\ref{tab:ava_2cls_high}, there is a significant gap between 1-class and 2-class performance on VisA. As anomalies are harder to obtain compared to normal images, bridging the gap is an open challenge to the research community.\n\n\\subsection{SPD in Low-shot 2-class Regime}\\label{sect:few-shot-results}\n\n\\begin{table*}[!t]\n\t\\centering\n\t\\caption{Low-shot anomaly detection and segmentation on VisA.}\n    \\label{tab:low_shot}\n\t\\setlength{\\tabcolsep}{7pt}\n\t\\resizebox{\\linewidth}{!}{\n\t\\centering\n    \\begin{tabular}{c|c|llll||llll}\n    \\hline\n    \\multirow{2}{*}{}  & \\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}ImageNet\\\\ labels\\end{tabular}} & \\multicolumn{4}{c||}{Classification (2-class, low-shot)}                                & \\multicolumn{4}{c}{Segmentation (2-class, low-shot)}                                  \\\\ \\cline{3-10} \n                       &                                                                            & \\multicolumn{2}{c|}{5-shot}                    & \\multicolumn{2}{c||}{10-shot} & \\multicolumn{2}{c|}{5-shot}                    & \\multicolumn{2}{c}{10-shot} \\\\ \\hline\n                       &                                                                            & AU-PR       & \\multicolumn{1}{c|}{AU-ROC}      & AU-PR         & AU-ROC       & AU-PR       & \\multicolumn{1}{c|}{AU-ROC}      & AU-PR        & AU-ROC       \\\\ \\hline\n    Sup. pre-train     &   \\cmark                                                                         & 59.2        & \\multicolumn{1}{l|}{85.5}        & 70.4          & 91.7         & 17.8        & \\multicolumn{1}{l|}{74.6}        & 28.3         & 81.8         \\\\ \\hline\\hline\n    SimSiam            &   \\xmark                                                                          & 51.9        & \\multicolumn{1}{l|}{82.3}        & 65.0          & 89.4         & 17.3        & \\multicolumn{1}{l|}{75.2}        & 28.5         & 81.6         \\\\\n    +SPD               &   \\xmark                                                                        & 56.1 \\textcolor{forestgreen}{(+4.2)} & \\multicolumn{1}{c|}{84.0 \\textcolor{forestgreen}{(+1.7)}} & 67.6 \\textcolor{forestgreen}{(+2.6)}   & 90.8 \\textcolor{forestgreen}{(+1.4)}  & 18.2 \\textcolor{forestgreen}{(+0.9)} & \\multicolumn{1}{l|}{\\textbf{76.0} \\textcolor{forestgreen}{(+0.8)}} & 29.7 \\textcolor{forestgreen}{(+1.2)}  & 83.2 \\textcolor{forestgreen}{(+1.6)}  \\\\ \\hline\n    MoCo               &   \\xmark                                                                         & 56.1        & \\multicolumn{1}{l|}{83.8}        & 68.7          & 90.6         & 21.5        & \\multicolumn{1}{l|}{80.5}        & 32.3         & \\textbf{85.7}         \\\\\n    +SPD               &   \\xmark                                                                         & 56.4 (+0.3) & \\multicolumn{1}{l|}{83.9 (+0.1)} & 68.0 (-0.7)   & 90.1 (-0.5)  & \\textbf{22.1} \\textcolor{forestgreen}{(+0.6)} & \\multicolumn{1}{l|}{78.5 (-2.0)} & \\textbf{32.8} \\textcolor{forestgreen}{(+0.5)}  & 84.9 (-0.8)  \\\\ \\hline \n    SimCLR             &  \\xmark                                                                          & 48.4        & \\multicolumn{1}{l|}{79.6}        & 58.2          & 86.0         & 18.4        & \\multicolumn{1}{l|}{71.2}        & 23.0         & 75.1         \\\\\n    +SPD               &  \\xmark                                                                         & 47.4 (-1.0) & \\multicolumn{1}{l|}{79.9 (+0.3)} & 59.0 \\textcolor{forestgreen}{(+0.8)}   & 86.1 (+0.1)  & 18.9 \\textcolor{forestgreen}{(+0.5)} & \\multicolumn{1}{c|}{74.5 \\textcolor{forestgreen}{(+3.3)}} & 25.1 \\textcolor{forestgreen}{(+2.1)}  & 78.2 \\textcolor{forestgreen}{(+3.1)}  \\\\ \\hline\n    Sup. pre-train+SPD &  \\cmark                                                                          &    \\textbf{59.8} \\textcolor{forestgreen}{(+0.6)}         & \\multicolumn{1}{l|}{\\textbf{85.9} (+0.4)}            &         \\textbf{71.2} \\textcolor{forestgreen}{(+0.8)}      &   \\textbf{92.1} (+0.4)          & 18.7 \\textcolor{forestgreen}{(+0.9)} & \\multicolumn{1}{c|}{75.9 \\textcolor{forestgreen}{(+1.3)}} & 30.6 \\textcolor{forestgreen}{(+2.3)}  & 81.8 (+0.0)  \\\\ \\hline\n    \\end{tabular}%\n    }\n\\end{table*}\n\n\\noindent\\textbf{Low-shot anomaly segmentation:} With different ImageNet pre-training as initialization, a 2-class U-Net with ResNet-50 encoder is trained for each 5/10-shot segmentation setup. From Table \\ref{tab:low_shot}, SPD again improves all baselines in both 5-shot and 10-shot evaluation, with AU-PR gain up to $2.3\\%$. One departure from the high-shot regime is that for few-shot anomaly segmentation, MoCo+SPD is the best method, even outperforming supervised pre-training.   \n\n\\noindent\\textbf{Low-shot anomaly detection:} Initialized with different ImageNet pre-training, a 2-class ResNet-50 is trained in 5/10-shot setups for anomaly detection. From Table \\ref{tab:low_shot}, overall the supervised pre-training with SPD outperforms both supervised pre-training only and other SSL's. Moreover, SPD significantly improves SimSiam with $4.2\\%$ AU-PR in 5-shot and $2.6\\%$ AU-PR in 10-shot, although it's still inferior to supervised pre-training.\n\\subsection{Ablation Study}\n\n\\begin{table*}[!t]\n\t\\centering\n\t\\caption{Ablation study}\n\t\\label{tab:ablation}\n\t\\setlength{\\tabcolsep}{7pt}\n\t\\resizebox{\\linewidth}{!}{\n\t\\centering\n\t\\begin{tabular}{c|cccc|cccc}\n    \\hline\n    \\multirow{2}{*}{} & \\multicolumn{4}{c|}{VisA (1-class)}                             & \\multicolumn{4}{c}{MVTec-AD (1-class)}                         \\\\ \\cline{2-9} \n     & \\multicolumn{2}{c|}{Classification} & \\multicolumn{2}{c|}{Segmentation} & \\multicolumn{2}{c|}{Classification} & \\multicolumn{2}{c}{Segmentation} \\\\ \\hline\n                      & AU-PR & \\multicolumn{1}{c|}{AU-ROC} & AU-PR & AU-ROC & AU-PR & \\multicolumn{1}{c|}{AU-ROC} & AU-PR & AU-ROC \\\\ \\hline\n    SimSiam w/ Res50     & 80.2 & \\multicolumn{1}{c|}{78.1}   & 9.1  & 93.1  & 92.6 & \\multicolumn{1}{c|}{83.9}  & 29.7 & 92.1  \\\\ \\hline\\hline\n    +SPD $(\\eta=0.1)$         & 82.8 & \\multicolumn{1}{c|}{81.2}  & 9.4  & 92.7  & 94.1 & \\multicolumn{1}{c|}{88.0}  & 32.0 & 92.2  \\\\ \n    +SPD $(\\eta=0.5)$         & 80.5 & \\multicolumn{1}{c|}{79.3}  & 8.7  & 93.0  & 93.3 & \\multicolumn{1}{c|}{84.9}  & 30.1 & 91.9  \\\\\n    +SPD $(\\eta=1.0)$           & 81.5 & \\multicolumn{1}{c|}{79.8}  & 9.4  & 92.8  & 93.4 & \\multicolumn{1}{c|}{85.8}  & 30.0 & 92.0  \\\\ \\hline\n    +SPD w/ CutPaste    & 78.8 & \\multicolumn{1}{c|}{77.0}  & 9.7  & 93.1  & 93.5 & \\multicolumn{1}{c|}{85.2}  & 28.2 & 91.3  \\\\\n    +SPD w/ Xent        & 71.4 & \\multicolumn{1}{c|}{66.6}  & 2.7  & 84.8  & 86.3 & \\multicolumn{1}{c|}{71.0}  & 15.2  & 82.6  \\\\ \\hline\n    SimSiam w/ WideRes50 & 80.3 & \\multicolumn{1}{c|}{77.7}  & 9.9  & 93.6  & 93.0 & \\multicolumn{1}{c|}{84.7}  & 31.3 & 92.2  \\\\\n    +SPD              & 81.9 & \\multicolumn{1}{c|}{80.4}  & 10.5  & 93.7  & 93.4 & \\multicolumn{1}{c|}{85.4}  & 32.5 & 92.8  \\\\ \\hline\n    \\end{tabular}\n    }\n\n\\end{table*}\n\\begin{table*}[!t]\n\t\\centering\n    \\caption{1-class performance evaluation on VisA and MVTec-AD with PatchCore.}\n    \\label{tab:1cls_patchcore}\n\t\\setlength{\\tabcolsep}{7pt}\n\t\\resizebox{\\linewidth}{!}{\n\t\\centering\n    \\begin{tabular}{c|cccc|cccc}\n    \\hline\n    \\multirow{2}{*}{\\begin{tabular}[c]{@{}c@{}}Backbone:\\\\ Wide ResNet50\\end{tabular}} & \\multicolumn{4}{c|}{VisA (1-class)}                                     & \\multicolumn{4}{c}{MVTec-AD (1-class)}                                 \\\\ \\cline{2-9} \n                                                                                   & \\multicolumn{2}{c|}{Classification} & \\multicolumn{2}{c|}{Segmentation} & \\multicolumn{2}{c|}{Classification} & \\multicolumn{2}{c}{Segmentation} \\\\ \\hline\n                          & AU-PR & \\multicolumn{1}{c|}{AU-ROC} & AU-PR           & AU-ROC          & AU-PR & \\multicolumn{1}{c|}{AU-ROC} & AU-PR          & AU-ROC          \\\\ \\hline\n    Sup. pre-train        & 93.3  & \\multicolumn{1}{c|}{92.4}   & 38.4            & 98.4            & 99.2  & \\multicolumn{1}{c|}{99.8}   & 48.8           & 97.6            \\\\\n    Sup. pre-train+SPD    & 93.8 \\textcolor{forestgreen}{(+0.5)} & \\multicolumn{1}{c|}{92.5 (+0.1)}   & 39.3 \\textcolor{forestgreen}{(+0.9)}           & 98.1 (-0.3)           & 99.0 (-0.2) & \\multicolumn{1}{c|}{99.7 (-0.1)}   & 49.3 \\textcolor{forestgreen}{(+0.5)}          & 97.5 (-0.1)           \\\\ \\hline\n    \\end{tabular}%\n\t}\n\\end{table*}\n\nWe conduct extensive ablation studies based on ImageNet SimSiam pre-training and PaDiM as the anomaly detection and segmentation algorithms trained in the 1-class setups of VisA and MVTec-AD. Results are shown in Table \\ref{tab:ablation}.\n\n\\noindent\\textbf{Sensitivity analysis on SPD loss weight $\\eta$:} From Table \\ref{tab:ablation}, we see consistent improvement for $\\eta=0.1,0.5,1.0$ in at least one task for both datasets. SPD loss with $\\eta=0.1$ gives us the best performances in both datasets, which is chosen as the default SPD loss weight for all pre-training with SPD. So the SimSiam+SPD ($\\eta=0.1$) is regarded as SimSiam+SPD for better clarity.\n\n\\noindent\\textbf{Comparison between SPD and CutPaste \\cite{li2021cutpaste}:} CutPaste and cross-entropy loss used in \\cite{li2021cutpaste} for anomaly detection training can also be used in ImageNet pre-training. An ablation study is done to demonstrate the superiority of the proposed SmoothBlend and SPD loss. With $\\mathcal{L}_{\\mathrm{SPD}}$, SmoothBlend is arguably better than CutPaste by $4.0\\%$ and $3.8\\%$ AU-PR improvement in VisA - classification and MVTec - segmentation (+SPD v.s. +SPD w/ CutPaste). With the SmoothBlend, the SPD loss significantly outperforms cross-entropy loss (+SPD v.s. +SPD w/ Xent). Such results demonstrate the validity of proposed methods.\n\n\\noindent\\textbf{SPD with different backbones:} ResNet-50 is adopted as the backbone for all major experiments in this paper. We demonstrate the SPD can generalize to different network architectures by experiments of SimSiam w/wo SPD on wide ResNet-50 \\cite{zagoruyko2016wide}. As in Table \\ref{tab:ablation}, SPD still improves the baseline.\n\n\\noindent\\textbf{Results with PatchCore: }\nIn addition to PaDiM, we also evaluate supervised pre-trained models based on another state-of-the-art 1-class method PatchCore \\cite{Roth_2022_CVPR}. Wide ResNet-50 is chosen as the backbone network. As in Table \\ref{tab:1cls_patchcore}, on VisA, SPD improves supervised pre-trained model by $0.5\\%$ and $0.9\\%$ AU-PR for both classification and segmentation. On MVTec-AD, SPD improves by $0.5\\%$ AU-PR for segmentation with slightly performance decreased in classification.\n\n\\noindent\\textbf{Extending SPD to other tasks:} Besides improvement on defect detection and segmentation, SPD also improves ImageNet supervised classification accuracy:  $69.8\\%\\rightarrow70.2\\%$ for ResNet-18 and $76.1\\%\\rightarrow76.4\\%$ for ResNet-50.\nPre-trained models with better ImageNet accuracy are expected to benefit downstream tasks more. Thus we speculate that SPD will work well for object recognition and detection, especially on fine-grained classification and small object detection as SPD promotes local sensitivity. In addition, we will leverage the proposed SPD training as a 1-class anomaly detection model to be trained by downstream data.\n\n\\noindent\\textbf{Qualitative results:} To qualitatively demonstrate the effectiveness of SPD regularization, we represent attention maps and anomaly segmentation in Sec. \\textcolor{red}{E} of the supplementary due to page limits.\n\n\\section{Conclusions}\nIn this work, we present a spot-the-difference (SPD) training to regularize pre-trained models' local sensitivity to anomalous patterns. We also present a novel Visual Anomaly (VisA) dataset which is the largest industrial anomaly detection dataset. Extensive experiments demonstrate the benefits of SPD for various contrastive self-supervised and supervised pre-training for anomaly detection and segmentation. Compared to standard supervised pre-training, SimSiam with SPD obtains superior or competitive performances in low-shot regime while supervised learning with SPD presents better performances in various setups. \n\n\\section*{Acknowledgments}\nThe authors would like to thank Fanyi Xiao, Erhan Bas, Aditya Deshpande and Joachim Stahl for idea brainstorming\nand providing insightful comments on the manuscript.\n\n\\clearpage\n\n\\section*{Appendix}\nIn this appendix, we present the additional details and\nresults that are not covered by the main paper.\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2104.06064v3.tex",
        "arXiv-2104.08391v1.tex",
        "arXiv-2207.14315v1.tex"
    ],
    "group_id": "group_88",
    "response": "### Title: Mixed Supervision and Self-Supervised Learning for Surface Defect Detection and Counting\n\n### Introduction\n\nSurface defect detection and counting are critical tasks in industrial quality control, aiming to identify and quantify anomalies in production items. Traditionally, these tasks have been approached using classical machine vision methods, but with the advent of deep learning, more sophisticated algorithms have been developed. However, deep learning models require a substantial amount of annotated data, which is often difficult and costly to obtain in industrial settings due to the rarity and complexity of defect samples. This has led to a need for methods that can reduce the annotation burden while maintaining high performance. The field has seen advancements in unsupervised and weakly supervised learning, which use minimal or no annotations, but these methods often underperform compared to fully supervised approaches. Additionally, there is a growing interest in few-shot learning, where models are trained with a limited number of annotated examples, to address the challenges of data scarcity and annotation cost.\n\nIn recent years, researchers have explored various strategies to mitigate these challenges, including the use of mixed supervision, which combines fully and weakly labeled data, and self-supervised learning, which leverages unannotated data to improve model performance. This summary will focus on three recent papers that address these challenges through innovative approaches. The first paper introduces a deep learning architecture for surface defect detection that can handle mixed supervision, the second paper proposes a few-shot counting method for handling a large number of visual categories, and the third paper presents a new dataset and a self-supervised learning method to enhance anomaly detection and segmentation.\n\n### Paper 1: Mixed Supervision for Surface-Defect Detection\n\nJakob Bo≈æiƒç, Domen Tabernik, and Danijel Skoƒçaj from the University of Ljubljana propose a deep learning model for surface defect detection that can work in a mixed supervision mode, combining fully supervised and weakly supervised learning. The model is designed to handle the scarcity of defect samples and the difficulty of obtaining pixel-level annotations, which are often required for high-precision defect segmentation. They introduce an end-to-end architecture with two sub-networks: one for defect segmentation and another for defect classification. The segmentation sub-network utilizes pixel-level annotations when available, while the classification sub-network works with image-level labels. This dual sub-network approach allows the model to be trained with a combination of fully and weakly labeled data, thereby reducing the annotation cost.\n\nThe authors evaluate their model on several datasets, including DAGM, KolektorSDD, and Severstal Steel Defect, and introduce a new dataset, KolektorSDD2, which contains over 3000 images with various defect types. They demonstrate that their model outperforms existing unsupervised and weakly supervised methods, and achieves performance comparable to fully supervised models with a significantly reduced annotation cost. The model's architecture and training process are designed to handle spatial uncertainty in coarse region-based annotations, further reducing the need for detailed annotations.\n\n### Paper 2: Learning to Count Everything\n\nViresh Ranjan, Udbhav Sharma, Thu Nguyen, and Minh Hoai from Stony Brook University and VinAI Research propose a novel architecture called Few Shot Adaptation and Matching Network (FamNet) for few-shot counting. FamNet is designed to count objects from any category given only a few labeled examples from the same image. This approach is particularly useful for handling the large number of visual categories that are common in real-world applications, where collecting a large amount of annotated data for each category is impractical. FamNet consists of a multi-scale feature extraction module and a density prediction module, which are trained to be agnostic to the visual categories. At test time, the network is adapted to the specific category using a few gradient descent steps based on the provided exemplars.\n\nTo facilitate the development and evaluation of few-shot counting methods, the authors introduce a new dataset called Few-Shot Counting-147 (FSC-147), which contains over 6000 images from 147 different categories. The dataset is annotated with both dot and bounding box annotations, making it suitable for both counting and detection tasks. They compare FamNet with several state-of-the-art methods and show that FamNet outperforms them in terms of Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) on the FSC-147 dataset. The authors also perform an ablation study to analyze the importance of each component in FamNet, demonstrating that all components contribute to the performance improvement.\n\n### Paper 3: SPot-the-Difference Self-Supervised Pre-training for Anomaly Detection and Segmentation\n\nYang Zou, Jongheon Jeong, Latha Pemula, Dongqing Zhang, and Onkar Dabeer from AWS AI Labs and KAIST present a new dataset and a self-supervised learning method called SPot-the-Difference (SPD) to improve anomaly detection and segmentation. The Visual Anomaly (VisA) dataset contains 10,821 high-resolution images from 12 different objects, making it the largest industrial anomaly detection dataset to date. The VisA dataset is designed to be more challenging than existing datasets by including complex structures, multiple instances, and large variations in object locations and poses.\n\nSPD is a regularization method that promotes local sensitivity of self-supervised contrastive learning, which is essential for anomaly detection tasks. The authors compare the performance of different pre-training methods, including supervised and self-supervised pre-training, with and without SPD, on VisA and MVTec-AD datasets. They find that SPD consistently improves the performance of self-supervised models, particularly in the low-shot regime, where the number of annotated examples is limited. The authors also conduct ablation studies to analyze the impact of different components of SPD, demonstrating the effectiveness of the proposed method.\n\n### Commonalities and Innovations\n\nAll three papers address the challenges of data scarcity and annotation cost in industrial quality control. Paper 1 introduces a mixed supervision approach that combines fully and weakly labeled data, reducing the need for detailed annotations. Paper 2 proposes a few-shot counting method that can handle a large number of visual categories with minimal annotated data. Paper 3 presents a new dataset and a self-supervised learning method that improves the local sensitivity of pre-trained models for anomaly detection and segmentation.\n\nThe innovations in these papers include the introduction of a new dataset (KolektorSDD2 and VisA), the development of novel architectures (FamNet and mixed supervision architecture), and the proposal of new training methods (SPD regularization and test-time adaptation). These innovations aim to reduce the annotation burden and improve the performance of deep learning models in practical industrial settings.\n\n### Comparison of Results\n\nThe results of the three papers are evaluated on different datasets and tasks, making a direct comparison challenging. However, they all demonstrate significant improvements over existing methods in their respective domains. Paper 1 shows that the proposed mixed supervision model outperforms unsupervised and weakly supervised methods and achieves performance comparable to fully supervised models with a reduced annotation cost. Paper 2 demonstrates that FamNet outperforms several state-of-the-art few-shot detection methods in terms of MAE and RMSE on the FSC-147 dataset. Paper 3 shows that SPD improves the performance of self-supervised pre-training methods in both 1-class and 2-class anomaly detection and segmentation tasks, particularly in the low-shot regime.\n\n### Conclusion\n\nThe three papers presented in this summary offer innovative solutions to the challenges of data scarcity and annotation cost in industrial quality control. They introduce new datasets, architectures, and training methods that can significantly reduce the annotation burden while maintaining high performance. The mixed supervision approach in Paper 1 demonstrates the effectiveness of combining fully and weakly labeled data, while Paper 2 proposes a few-shot counting method that can handle a large number of visual categories. Paper 3 presents a self-supervised learning method that improves the local sensitivity of pre-trained models for anomaly detection and segmentation.\n\nThese findings suggest that mixed supervision, few-shot learning, and self-supervised learning are promising directions for reducing the annotation cost in industrial quality control. Future research could explore the integration of these methods to further improve the performance and reduce the annotation burden. Additionally, the development of more challenging and diverse datasets, such as VisA, could facilitate the evaluation and improvement of deep learning models in practical industrial settings."
}