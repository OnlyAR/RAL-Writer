{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model}\n\n\\begin{document}\n\n\\title{PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model} \n\n\\author{Zheng Zhang\\inst{1,2}\\thanks{Equal contribution} \\and Yeyao Ma\\inst{1}$^\\star$ \\and Enming Zhang\\inst{1}$^\\star$ \\and Xiang Bai\\inst{1}}\n\n\\authorrunning{Z. Zhang, Y. Ma, E. Zhang, X. Bai}\n\n\\institute{Huazhong University of Science and Technology \\and Microsoft Research Asia}\n\n\\maketitle\n\n\\begin{abstract}\nPSALM is a powerful extension of the Large Multi-modal Model (LMM) to address the segmentation task challenges. To overcome the limitation of the LMM being limited to textual output, PSALM incorporates a mask decoder and a well-designed input schema to handle a variety of segmentation tasks. This schema includes images, task instructions, conditional prompts, and mask tokens, which enable the model to generate and classify segmentation masks effectively. The flexible design of PSALM supports joint training across multiple datasets and tasks, leading to improved performance and task generalization. PSALM achieves superior results on several benchmarks, such as RefCOCO/RefCOCO+/RefCOCOg, COCO Panoptic Segmentation, and COCO-Interactive, and further exhibits zero-shot capabilities on unseen tasks, such as open-vocabulary segmentation, generalized referring expression segmentation and video object segmentation, making a significant step towards a GPT moment in computer vision. Through extensive experiments, PSALM demonstrates its potential to transform the domain of image segmentation, leveraging the robust visual understanding capabilities of LMMs as seen in natural language processing. Code and models are available at \\url{https://github.com/zamling/PSALM}. \n\\keywords{Segmentation \\and Large Multimodal Model \\and Visual Language}\n\\end{abstract}\n\n\\label{sec:intro}\n\\begin{figure}[!t]\n  \\centering\n  \\includegraphics[width=0.9\\textwidth]{images/teaser_v4.pdf}\n  \\caption{PSALM has capability to handle multiple segmentation tasks in only one single model. We visualize some tasks, including Panoptic segmentation in COCO~\\cite{coco}; Open-Vocabulary instance segmentation in ADE20K~\\cite{ade20k}; Interactive segmentation in COCO-Interactive; Referring segmentation in RefCOCO~\\cite{refcoco}; Generalized referring segmentation in gRefCOCO~\\cite{grefcoco}; Ego-exo correspondence in Ego-Exo4d~\\cite{egoexo}; Video object segmentation in DAVIS2017~\\cite{davis}.}\n  \\label{fig:teaser}\n\\end{figure}\n\\section{Introduction}\nLarge multi-modal model (LMM) has ignited the dawn of the vision GPT~\\cite{GPT3} moment by making ground-breaking progress in various advanced visual understanding tasks by compressing image and language information into a single auto-regressive model. \nHowever, a major obstacle on the road to achieving vision GPT is that current LMM can only perform text outputs, making it challenging to address the pixel-level image understanding problem directly, \\textit{i.e.}, image segmentation, which is one of the most critical tasks in computer vision.\n\nBehind the obstacles are many challenges. First, the default output of the LMM is discrete tokens, and there is no apparent way to generate masks directly. Second, the variety of image segmentation tasks requires different forms of inputs and outputs. For example, semantic segmentation needs support inputs with different categories. Instance segmentation demands generate object IDs and the class confidence score for each object. Referring segmentation is supposed to have a language sentence as input and interactive segmentation has more varied inputs than the other tasks, which can be points, scribbles, bounding boxes, or masks. Third, unifying different segmentation tasks with a shared weight model is also challenging because different tasks require varied capabilities.\n\nIn this work, we propose a method named PSALM (\\textbf{P}ixelwise \\textbf{S}egment\\textbf{A}tion with \\textbf{L}arge Multi-Modal \\textbf{M}odel) that aims to address the above challenges and extend the capabilities of LMM from text-output tasks to general segmentation tasks (Fig.~\\ref{fig:teaser} shows representative tasks). Specifically, PSALM externalizes a mask decoder on the top of LMM and designs a flexible input schema to unify different segmentation tasks into a single model.\n\nThe input schema consists of four different parts: images, task instruction prompt, condition prompt, and a set of mask tokens, where the instruction prompt is a text sentence describing the task itself, condition prompt contains the additional necessary information to solve the task, either in terms of category names, sentence or visual features, and mask tokens are a set of learnable embeddings. All these inputs are fed into the LMM, and the resulting output mask tokens are further used as input by the mask generator to present mask proposals. \nApart from producing the mask proposals, it is also necessary to predict the class of each segmentation mask or estimate a confidence score, which can be achieved by using the output embedding of the condition prompt as the classifier weights to classify each mask proposal. In practice, we categorize conditions into category condition, sentence condition, and visual-prior condition, and present the corresponding methods to build the classifier weights according to the properties of each type of condition. \n\nSome other methods, represented by LISA~\\cite{lisa}, also aim to use LMM for segmentation tasks. However, these methods are usually designed for referring segmentation and fail to justify their ability to solve generalized segmentation tasks (see Tab.~\\ref{tab:capability_comparison}).\nIn contrast, thanks to the generality and flexibility of the proposed architecture, PSALM can not only solve a variety of segmentation tasks but also be able to joint train on different tasks, which makes the model task generalizable while allowing the model to take full advantage of the intrinsic connections of different datasets/tasks to achieve better performance. \nSpecifically, with the joint training of COCO Panoptic Segmentation~\\cite{coco}, RefCOCO~\\cite{refcoco}/RefCOCO+/RefCOCOg~\\cite{refcocog}, and COCO Interactive, we observe a significant performance improvement compared to training at different tasks individually, and therefore result in even better performance than other task-specific methods. On referring segmentation tasks, we outperform other LLM-based pixel reasoning methods (\\textit{e.g.}, LISA, PixelLM~\\cite{pixellm} and GSVA~\\cite{xia2023gsva}) on RefCOCO, RefCOCO+, and RefCOCOg, and it worth noting that we only use Phi-1.5 1.3B model~\\cite{phi15} while others adopt Vicuna-7B~\\cite{vicuna} or LLama2-13B model~\\cite{llama2}.\n\nThe flexible design of architecture and input schema, multi-task joint-training, and the strong visual understanding capability of LMM not only make PSALM perform well on trained in-domain tasks but also enable generalizability to out-of-domain tasks in a zero-shot manner, \\textit{i.e.,} directly dealing with unseen tasks without additional training. We test on three tasks: open-vocabulary segmentation, generalized referring expression segmentation, and video object segmentation. PSALM achieves promising zero-shot performance on these tasks. We believe this task-level generalizability is crucial, which is one of the key properties of the large language model for its success in natural language processing.\n\nThrough extensive experiments on a variety of segmentation tasks, we show that presented PSALM has strong potential to address general image segmentation tasks and exhibits a certain degree of task generalization capability as LLM does in NLP. We believe this work facilitates the realization of the GPT moment in computer vision.\n\\begin{table}[t]\n\\centering\n\\footnotesize\n\\caption{Capability for different methods. Our proposed PSALM can handle more segmentation tasks than other LLM-centric methods. LLM-centric methods can also deal with text generation tasks, which is hard for most vision-centric methods.}\n\\scalebox{0.8}{\n\\begin{tabular}{l|lclclclc}\n\\toprule\n                           & Methods    & Generic Seg.              &  & Referring Seg.            &  & Interactive Seg.          &  & OV Seg.                   \\\\ \\midrule\n\\multirow{5}{*}{{Vision centric}}   & Mask2Former~\\cite{mask2former} & \\Checkmark &  &                           &  &                           &  &                           \\\\\n                           & ODISE~\\cite{odise}      & \\Checkmark &  &                           &  &                           &  & \\Checkmark \\\\\n                           & UNINEXT~\\cite{uninext}    & \\Checkmark &  & \\Checkmark &  & \\Checkmark &  &                           \\\\\n                           & SEEM~\\cite{seem}       & \\Checkmark &  & \\Checkmark &  & \\Checkmark &  & \\Checkmark \\\\\n                           & OMG-Seg~\\cite{omgseg}    & \\Checkmark &  &                           &  & \\Checkmark &  & \\Checkmark \\\\ \\midrule\n\\multirow{4}{*}{{LLM centric}} & LISA~\\cite{lisa}       &                           &  & \\Checkmark &  &                           &  &                           \\\\\n                           & GLAMM~\\cite{glamm}      &                           &  & \\Checkmark &  &                           &  &                           \\\\\n                           & PixelLM~\\cite{pixellm}    &                           &  & \\Checkmark &  &                           &  &                           \\\\ \n                           & PSALM (Ours)      & \\Checkmark &  & \\Checkmark &  & \\Checkmark &  & \\Checkmark \\\\ \\bottomrule\n\\end{tabular}\n}\n\\label{tab:capability_comparison}\n\\end{table}\n\\section{Related works}\n\\subsection{Large Multimodal Models}\nWith the release of GPT-V~\\cite{GPT-V} and Gemini~\\cite{Gemini}, more attention and efforts from open-source and research communities are shifting from large language models (LLM) to large multi-modal models (LMM). \nLLaVA~\\cite{llava}, BLIP-2~\\cite{blip2}, and Flamingo~\\cite{flamingo} are three representative works, where the core idea of both LLaVA and BLIP-2 is to map visual features into the input space of LLM to implement multi-modal capabilities, while Flamingo employs deeper feature fusion in the intermediate layers of LLM. \nSome works, such as Kosmos-2~\\cite{kosmos_2}, Shikra~\\cite{chen2023shikra}, and Ferret~\\cite{you2023ferret}, further introduce object localization tasks into the LMM, while others, such as Emu~\\cite{emu}, CogVLM~\\cite{wang2023cogvlm}, and DreamLLM~\\cite{dong2023dreamllm}, focus on how to integrate visual generation into the LMM. \nHowever, these above methods are mainly designed for text output tasks or image generation and cannot directly deal with pixel-level understanding tasks such as image segmentation, which is different from ours and we can base on these models.\n\\subsection{Pixel Reasoning with LMM}\nSimilar to our goal, some existing works attempt to enable LMMs to generate segmentation masks. LISA~\\cite{lisa} is a pioneering work that uses a special seg token to aggregate information of a given sentence and use it as a prompt embedding in a SAM decoder to predict the segmentation mask. u-LLaVA~\\cite{ullava} further supports object grounding tasks on the basis of LISA, and NExT-Chat~\\cite{nextchat} introduces richer inputs, such as bounding boxes. Furthermore, since LISA can only deal with a single object, many subsequent works that attempt to address the multi-object case, such as GLaMM~\\cite{glamm}, PerceptionGPT~\\cite{pi2023perceptiongpt}, PixelLM~\\cite{pixellm}, GSVA~\\cite{xia2023gsva}, and LISA++~\\cite{lisa++}, all of which share the basic idea of introducing a seg token for each sentence describing a different object, and except PixelLM, all other methods are based on SAM decoder~\\cite{sam}.\n\nAlthough all these methods can generate masks, they are primarily designed for reference segmentation. In contrast, our method is designed for generalized segmentation tasks, which have diverse input and output requirements. In addition, the difference in goals also brings technical discrepancies: these methods use language models to directly generate the final segmentation masks, while our approach is closer to Mask2Former~\\cite{mask2former}, which first generates mask proposals and then classifies the masks.\n\\begin{figure}[!t]\n  \\centering\n  \\includegraphics[width=.9\\textwidth]{images/pipeline_v3.pdf}\n  \\caption{PSALM architecture overview.}\n  \\label{fig:overview}\n\\end{figure}\n\\subsection{Unified Segmentation Framework}\nAnother class of relevant explorations~\\cite{uninext,seem,omgseg,glee} studies how to unify different segmentation tasks into a single framework. For instance, Mask2Former~\\cite{mask2former} presents a unified architecture of generic segmentation~\\footnote{Generic segmentation includes semantic segmentation, instance segmentation, and panoptic segmentation.} but with different models.  OneFormer~\\cite{jain2023oneformer} further integrates these three tasks within a single model. UNINEXT~\\cite{uninext} aims to unify instance-perception tasks and introduce text inputs and thus it can handle referring segmentation. X-Decoder~\\cite{xdecode} presents a flexible decoder architecture that can support generic segmentation, referring segmentation, retrieval, and image captioning. SEEM~\\cite{seem} designs a generic encoder-decoder to unify different segmentation tasks, where the encoder is used for projecting image, text, and human inputs into a joint visual-semantic space, and the decoder is used for mask prediction. \nHowever, all these works are not based on LMM, instead, they are mostly vision-centric models, \\textit{i.e.}, usually designed for visual tasks only and thus cannot address language tasks very well.\n\n\\section{Methods}\n\nFig.~\\ref{fig:overview} provides an overview schematic of PSALM, which consists of a large multimodal model (LMM), a mask generator, and a flexible input schema designed for general segmentation tasks. The input schema has four different types of inputs: image, task instruction prompt, condition prompt, and a set of mask tokens. LMM processes the input tokens and the output embedding of mask tokens is further fed into the mask generator to generate masks. In the following, we will introduce our approach in detail.\n\\subsection{Large Multimodal Model and Input Schema}\nPSALM is built on large multimodal models (LMM), and there are many different LMM architectures, such as LLaVA~\\cite{llava}, BLIP~\\cite{blip}, and Flamingo~\\cite{flamingo}. Here, we adopt the design of LLaVA because of its proven performance and simplicity, but the other LMM architectures are also compatible with our approach without any theoretical difficulties.\n\nThe LMM used in our work has a visual encoder and pre-trained large language model (LLM). The two models are connected by a lightweight vision-language alignment model, which is a $3\\times3$ convolution layer followed by a linear layer. \nThe official LLaVA model uses a frozen CLIP model~\\cite{CLIP} as a visual coder, whose features lack the fine-grained information that is required for segmentation tasks~\\cite{SimBaseline}. Therefore, we train a customized LLaVA model by using the Swin Transformer~\\cite{Swin}, and limited by resources, we additionally replace the LLM from the Vicuna 7B model~\\cite{vicuna} to a smaller Phi-1.5 1.3B model~\\cite{phi15}. Here, we applied only the first visual-language alignment stage of LLaVA by following its default settings. In our ablations, we found the alignment stage is crucial for open-vocabulary segmentation and referring segmentation tasks (Tab.~\\ref{tab:vl_align}).\n\nDifferent segmentation tasks need different forms of inputs and outputs, which motivates us to present a flexible input schema to unify various requirements. In addition to the input image used in the visual encoder, our input schema has three other different types of inputs: task instruction prompt, condition prompt, and a set of mask tokens. We will introduce each of them and summarize the prompts used for all different tasks in the Appendix. \n\n\\noindent\\textbf{Task Instruction Prompt.} The task instruction prompt is usually a text sentence describing and specifying the model's task. For example, in panoptic segmentation and open-vocabulary segmentation, the task instruction can be \\textit{“You need to segment all objects. This is all the candidate categories.”} and in referring segmentation, the instruction can be \\textit{“Please segment according to the following instruction.”} \n\n\\noindent\\textbf{Condition Prompt.} Some tasks require additional information to perform, \\textit{e.g.}, panoptic segmentation needs specifying the candidate set of categories to be segmented, and interactive segmentation needs interactive inputs. The condition prompt is designed for these tasks. In addition to providing information, condition prompt also plays an important role in predicting categories or estimating confidence scores for each segmentation mask. In Sec.~\\ref{sec:condition_prompt}, we will discuss the design of condition prompts for different tasks in detail.\n\n\\noindent\\textbf{Mask Token.} The LLM is designed for text output and cannot directly generate segmentation masks. To bypass this challenge, we append a set of mask tokens after other inputs, and then these mask tokens are decoded to segmentation masks by a mask generator (will be introduced in Sec.~\\ref{sec:mask_generator}). This design is inspired by Mask2Former~\\cite{mask2former}, with the difference that the mask tokens in Mask2Former are used directly in the mask generator, whereas the mask tokens in our approach are first updated by the LMM and then used in the mask generator, and we found our approach leads better performance in practice (see Tab.~\\ref{tab:mask_query}).\n\nSome works, such as LISA~\\cite{lisa} and PixelLM~\\cite{pixellm}, take similar seg tokens as input and employ a decoder to generate the masks. However, our objective is fundamentally different: in LISA and PixelLLM, seg tokens are used to generate the final prediction, while we generate mask proposals and further classify them based on condition prompts. Compared to the design of LISA and PixelLLM, our approach is more flexible and adaptable to more tasks, yet decoupling mask generation and classification is more efficient (see Tab.~\\ref{tab:decouple} and Tab.~\\ref{tab:mask_proposals}).\n\n\\subsection{Mask Generator}\n\\label{sec:mask_generator}\nThe mask generator predicts the mask and their category probabilities from three inputs: a multi-level visual features $\\{v_{l}\\}_{l=1}^{L}$, a set of mask tokens $\\{q_i\\}_{i=1}^{N}$, and a set of condition embeddings $\\{c_{k}\\}_{k=1}^{K}$. It can be formally defined as:\n\\begin{equation}\n    \\small\n    \\{(m_{i}, p_{i})\\}_{i=1}^{N} = \\texttt{MaskGenerator}(\\{v_{l}\\}_{l=1}^{L}, \\{q_{i}\\}_{i=1}^{N}, \\{c_{k}\\}_{k=1}^{K}),\n\\end{equation}\nwhere $m_{i}\\in \\mathbb{R}^{H\\times W}$ is the i-th predicted segmentation mask and $p_{i}\\in \\mathbb{R}^{K}$ is the corresponding category probability. \n\nIn practice, the multi-level visual features are the internal output features of the Swin visual encoder used in LMM, and the design of the mask generator follows Mask2Former, which employs multi-scale deformable attention as a pixel decoder and a transformer-based mask decoder to generate segmentation masks. The class of each mask is predicted by the condition embedding $\\{c_{k}\\}$, which is basically obtained from the output of the condition prompt, the obtaining method is slightly different for different types of conditions.\n\n\\begin{figure}[!t]\n  \\centering\n  \\includegraphics[width=0.9\\textwidth]{images/taskdetail_v4.pdf}\n  \\caption{Detailed processing for different condition prompts. (a) shows the processing for category condition. (b) shows the processing for sentence condition. (c) shows the processing for visual-prior condition.}\n  \\label{fig:taskdetail}\n\\end{figure}\n\\subsection{Design of Condition Prompts}\n\\label{sec:condition_prompt}\nIn our approach, the conditional prompt serves two important purposes: First, it provides the necessary information required to solve the task; Second, we use the output embedding of the conditional prompt in the LLM as classifier weights to predict the class of each segmentation mask. The design of the conditional prompt is closely tied to the type of task, and based on the information required for different tasks, we summarize three condition types: category condition, sentence condition, and visual-prior condition.\n\n\\noindent\\textbf{Category Condition} This condition type is used for tasks that need specifying a set of categories to be segmented, such as semantic segmentation, instance segmentation, and panoptic segmentation, and often needs to predict the class probability of each segmentation mask. Specifically, given a set of category names, we join them into a sentence by a comma separator, \\textit{e.g.,}, given three categories: person, bicycle, and car, the joint sentence is \\textit{\"person, bicycle, car\"}. \n\nThe joint sentences are then proceeded by LMM to get the output embeddings, which can be further used to classify the predicted segmentation masks. Specifically, for each category, we select its corresponding output embeddings and apply the \\texttt{avg\\_pooling} over them to obtain a condition embedding $c \\in \\mathbb{R}^{D}$, where $D$ is the embedding dimension, and thus condition embedding for all categories is a set $\\{c_{k}\\}_{k=1}^{K}$, where $K$ is the number of categories (see Fig.~\\ref{fig:taskdetail} (a)). This embedding set can be used by the mask generator to predict the class.\n\n\\noindent\\textbf{Sentence Condition} This condition is usually used for referring segmentation. Unlike the category condition, where category names are usually short, sentences are much longer, and not every word in the sentence is useful, so the \\texttt{avg\\_pooling} is not the optimal choice here. Instead, we introduce a special \\texttt{[REF]} token, which is appended after the condition sentence as an anchor to aggregate useful information, and the output embedding of \\texttt{[REF]} token, \\textit{i.e.,} the output features of LMM on the location of \\texttt{[REF]} token, is used as condition embedding $c$ and used by mask generator, as shown in Fig.~\\ref{fig:taskdetail} (b).\n\n\\noindent\\textbf{Visual-Prior Condition} We formulate most interaction (\\textit{e.g.,} point, mask, box, or scribble) used in interactive segmentation tasks as the visual-prior condition. Taking the scribble as an example, we first generate a binary scribble curve with a width of 5 pixels on a binary map, whose size is the same as the input image of the Swin vision encoder, and then apply the binary map to the output features of the vision-language alignment model, performing average mask pooling by upsampling the output feature map to the size of the binary map. The average pooled feature is used as the visual-prior condition and fed into LMM. \nIf there are multiple interactions, the process is repeated for each, yielding multiple pooled features as inputs, each feature separated using a strategy akin to the category condition. \nFor other types of interaction, we adopt similar approaches. Specifically, for box or mask, we directly apply the pooling operator by treating them as binary masks, and for point, we bold it to a 10-pixel circle and then apply the mask pooling. \nAfter that, we use the output embedding of the visual-prior condition as the classifier weight in the mask generator to estimate the confidence of each mask proposal, as shown in Fig.~\\ref{fig:taskdetail} (c).\n\n\\subsection{Training Objectives}\nThe training process of PSALM can be divided into two stages: In the first stage, we train the visual language alignment model following LLaVA setting, and freeze the vision encoder and LMM; In the second stage, we only freeze the vision encoder and fine-tune all other modules, including the mask generator. Similarly to Mask2Former, we use matching loss in the second stage training, \\textit{i.e.}, we use bipartite matching to find the optimal assignments between the mask proposals and the ground truth masks by minimizing the matching loss and use these assignments to perform training. The loss has two terms: $\\mathcal{L}=\\mathcal{L}_{mask} + \\mathcal{L}_{cls}$, where $\\mathcal{L}_{mask}$ indicates the mask loss which comprises a pixel-level Binary Cross-Entropy (BCE) loss and Dice loss~\\cite{DiceLoss}, while the $\\mathcal{L}_{cls}$ indicates the category classification loss, and we use Cross-Entropy (CE) loss for category condition and BCE loss for other cases. More training details are in the Appendix. \n\n\\section{Experiments}\n\\subsection{Datasets and Tasks}\nIn our experiments, many different datasets and tasks are involved. In training, we used a two-stage training strategy. In the first stage, we use CC3M~\\cite{cc3m} to align the vision features to the text input space. In the second stage, we train the model on four different tasks and corresponding datasets: generic segmentation(COCO Panoptic Segmentation~\\cite{coco}), referring segmentation(RefCOCO/+/g~\\cite{refcoco,refcocog}), interactive segmentation(COCO-Interactive) and the vision-language instruction task (LLaVA1.5 training data~\\cite{llavav1_5}). Note that COCO-Interactive is our in-house dataset, as there is no well-established public dataset that supports all four interaction types (point, scribble, box, and mask); we will release this dataset, and its construction details are given in the Appendix.\n\nThe evaluation tasks are classified into in-domain tasks and out-of-domain tasks, according to if the evaluation task appears in the training. Specifically, we use three out-of-domain tasks in this work: generalized referring expression segmentation(gRefCOCO~\\cite{grefcoco}), open-vocabulary segmentation (ADE20K~\\cite{ade20k}, Cityscapes~\\cite{cordts2016cityscapes}, Pascal Context~\\cite{pc}, and Pascal VOC~\\cite{pascal-voc-2010}) and video object segmentation (DAVIS-2017~\\cite{davis}), more dataset details are given in the Appendix.\n\n\\subsection{Implementation Details}\nSwin-B~\\cite{Swin} is used as a visual encoder with a Phi-1.5 1.3B~\\cite{phi15}, the architecture of the mask generator is the same as Maks2Former, the number of mask tokens is set to 100, and both Swin-B and mask generator are initialized from the pre-trained Maks2Former model weight. By default, we train PSALM for 56k iterations by distributing equal training iterations across each task in system-level comparison and 9k iterations in ablations. All training images are resized to $1024^2$ by padding the shorter side to keep the aspect ratio; the batch size is 64. AdamW optimizer is used and sets the initial learning rate to $4\\times10^{-5}$ with cosine schedule and without weight decay. If not specified, the model is trained with a joint training setting and without additional task-specific fine-tuning. All experiments are run on 16$\\times$V100 GPUs.\n\n\\subsection{Ablations}\nWe first ablate key designs and present behind insights in this section. To better show how different designs affect the performance on a wide range of tasks, we mainly report the results on three in-domain benchmarks: COCO Panoptic Segmentation (COCO-Pan), RefCOCO-val (RefCOCO), and COCO Interactive Segmentation with point inputs (COCO-Point), and one out-of-domain benchmark: open-vocabulary instance segmentation on ADE20K-150 (A150-OV).\n\\begin{table}[h!]\n  \\begin{minipage}{.6\\textwidth}\n  \\centering\n\\footnotesize\n\\caption{Ablation on the design of mask tokens. w.LLM: use mask tokens as inputs of LLM. Prefix: place mask tokens at the front. Suffix: place mask tokens at the end.}\n\\renewcommand{\\arraystretch}{0.9}\n\\scalebox{0.8}{\n\\begin{tabular}{cccccc}\n\\toprule\n\\multicolumn{2}{c}{Mask Tokens}               & COCO-Pan              & RefCOCO               & COCO-Point            & A150-OV              \\\\\n\\cmidrule{1-2}\nw.LLM & Pos.   & PQ                    & cIoU                  & mIoU                  & mAP                   \\\\ \\midrule\n\\Checkmark            & Suffix & 55.1                  & 76.1                  & 53.3                  & 9.3                  \\\\\n\\XSolidBrush          & -      & 54.8($\\downarrow$0.3) & 74.3($\\downarrow$1.8) & 53.1($\\downarrow$0.2) & 8.2($\\downarrow$1.1) \\\\\n\\Checkmark            & Prefix & 55.0($\\downarrow$0.1) & 75.1($\\downarrow$1.0) & 53.0($\\downarrow$0.3) & 7.8($\\downarrow$1.5) \\\\ \\bottomrule\n\\end{tabular}\n}\n\\label{tab:mask_query}\n\n  \\end{minipage}\\hfill\n  \\begin{minipage}{.35\\textwidth}\n  \\centering\n\\captionsetup{font=small}\n\\footnotesize\n\\caption{Effect of decouple design in COCO Semantic Segmentation. }\n\\scalebox{1.0}{\n\\centering\n\\begin{tabular}{ccc}\n\\toprule\nDecouple & mIoU & fwIoU \\\\ \\midrule\n    \\Checkmark         & 66.5          & 72.5          \\\\\n    \\XSolidBrush         & 42.7          & 35.0          \\\\ \\bottomrule\n\\end{tabular}\n\\label{tab:decouple}\n}\n\n  \\end{minipage}\n\\end{table}\n\n\\noindent\\textbf{Design of Mask Tokens.}\nIn our approach, we use a set of mask tokens to predict the mask proposal. In practice, we have found that using mask tokens as inputs to the LLM leads to better performance than applying them directly to the mask generator, which is the default method of Mask2Former. Tab.~\\ref{tab:mask_query} shows the results, where the direct use of mask tokens leads to a noticeable performance degradation in RefCOCO and A150-OV. \nWe believe this is because using mask tokens as input leads to a better awareness of the information needed for the task and thus improves performance, which is essential for these two tasks. For better validation, we placed mask tokens before conditional prompts and task instruction prompts and found a similar performance drop to that of not using mask tokens in LMM, which further supports our hypothesis.\n\nCompared to LISA and other methods that use seg token to generate final segmentation results directly, our mask proposal approach has three advantages: First, our design is more flexible and thus can be applied to a wider range of segmentation tasks, especially tasks that require predicting category or confidence scores; Second, our design decouples the mask prediction and classification, which alleviating the learning difficulties for some tasks. In Tab.~\\ref{tab:decouple}, we study how the decouple design affects the semantic segmentation performance on COCO Semantic Segmentation\\footnote{This benchmark is introduced by Mask2Former, which is composed by merging instances belonging to same class together in COCO Panoptic Segmentation.}, and we found our decouple design is significantly better\\footnote{More experimental details are in Appendix.}. Third, the mask proposals allow multiple masks to be generated for a single instance, which makes the mask accuracy superior to solutions like LISA that only predict a single mask. Tab.~\\ref{tab:mask_proposals} shows that using more mask proposals on RefCOCO gives a clear improvement over using a single mask.\n\n\\begin{table}[h!]\n  \\begin{minipage}{.4\\textwidth}\n\n\\centering\n\n\\captionsetup{font=small}\n\\footnotesize\n\\centering\n\\caption{Effects of number of mask tokens in RefCOCO(cIoU). The results of LISA are listed as a reference.}\n\\scalebox{0.8}{\n\n\\begin{tabular}{cccccc}\n\\toprule\n  \\#Mask Token                  &  &  & val & testA & testB \\\\ \\midrule\n\\gray{LISA} &          &  &   \\gray{74.1}       &        \\gray{76.5}   &   \\gray{71.1}    \\\\\n\\midrule\n1  &          &  &   75.3       &        78.0   &   72.2       \\\\\n100   &           &  &   76.5      &   78.5       &   73.4       \\\\\n\\bottomrule\n\\end{tabular}\n\\label{tab:mask_proposals}\n\n}\n  \\end{minipage}\\hfill\n  \\begin{minipage}{.55\\textwidth}\n\\centering\n\\footnotesize\n\\caption{Ablation on different designs for condition prompts.\n}\n\\scalebox{0.8}{\n\\begin{tabular}{ccccc}\n\\toprule\n\\multicolumn{2}{c}{Condition}   & COCO-Pan  & RefCOCO  & A150-OV \\\\ \\cmidrule{1-2}\nCategory & Sentence & PQ        & cIoU      & mAP        \\\\ \\midrule\n\\texttt{avg\\_pooling}          & \\texttt{avg\\_pooling}          & 55.1     & 75.3    & 9.2        \\\\\n\\texttt{[REF]}          & \\texttt{[REF]}          & 54.9     & 76.1     & 8.4        \\\\\n\\texttt{avg\\_pooling}          & \\texttt{[REF]}          & 55.1      & 76.1    & 9.3        \\\\ \\bottomrule\n\\end{tabular}\n}\n\\label{tab:arch}\n\n  \\end{minipage}\n\\end{table}\n\n\\noindent\\textbf{Design of Condition Prompts.}\nAnother key design in PSALM is the condition prompt, particularly the way we obtain the condition embeddings, which are used as classifier weights in the mask generator to predict the class of mask proposals. As described in Sec.~\\ref{sec:condition_prompt}, for category condition, we use the \\texttt{avg\\_pooling} over output embeddings of each class name as the condition embedding, and for sentence condition, we adopt a \\texttt{[REF]} token to aggregate useful information. \n\nTab.~\\ref{tab:arch} shows the ablation, where we first tried to use the same design for all conditions and found that \\texttt{avg\\_pooling} performed slightly better on COCO-Pan, with a larger improvement on A150-OV, while \\texttt{[REF]} worked better on RefCOCO. We further used different designs and found that the advantages of each design are preserved, and the best overall performance is achieved.\n\n\\begin{table}[h!]\n  \\begin{minipage}{.48\\textwidth}\n\\centering\n\\footnotesize\n\\caption{Ablation on effect of vision-language alingment.}\n\\scalebox{0.65}{\n\\begin{tabular}{ccccc}\n\\toprule\n\\multirow{2}{*}{VL Alignment} & COCO-Pan              & RefCOCO           & COCO-Point             & A150-OV               \\\\\n                              & PQ                    & cIoU                  & mIoU                   & mAP                   \\\\ \\midrule\n\\Checkmark     & 55.1                  & 76.1                  & 53.3                   & 9.3                  \\\\\n\\XSolidBrush   & 54.9($\\downarrow$0.2)  & 71.7($\\downarrow$4.4)  & 53.0 ($\\downarrow$0.3)  & 8.2($\\downarrow$1.1) \\\\ \\bottomrule\n\\end{tabular}\n}\n\\label{tab:vl_align}\n  \\end{minipage}\\hfill\n  \\begin{minipage}{.48\\textwidth}\n\\centering\n\\footnotesize\n\\caption{Ablation on joint training.}\n\\scalebox{0.62}{\n\\begin{tabular}{c|cccc}\n\\toprule\n\\multirow{2}{*}{Ablation} & COCO-Pan  & RefCOCO & COCO-Point & A150-OV \\\\\n                          & PQ           & cIoU        & mIoU   & mAP      \\\\ \\midrule\nTask Specific Train       & 55.6         & 76.5        & 62.2  & 7.0     \\\\\nJoint Train               & 55.9         & 83.6        & 64.1  & 9.0     \\\\\n$\\Delta$                  & +0.3       & + 7.1       & + 1.9   & +2.0    \\\\ \\bottomrule\n\\end{tabular}\n}\n\\label{tab:joint}\n\n  \\end{minipage}\n\\end{table}\n\n\\noindent\\textbf{Importance of VL-alignments.}\nThe visual-language alignment stage (\\textit{i.e.,} our first training stage) is to project the visual features into the text input space, and it is the most important step towards making the LLM understand images. In Tab.~\\ref{tab:vl_align}, we examine the impact of this stage, and we found that without VL alignment, the performance of all four tasks becomes worse, with the performance of A150-OV and RefCOCO being significantly affected, for example, the A150-OV dropped by -1.1 mAP and RefCOCO even dropped by -4.4 cIoU, probably because these two tasks require a strong requirement on understanding the relationship between vision and language. This result also suggests that the VL alignment is essential, and the LMM-based segmentation models have strong potential.\n\n\\noindent\\textbf{Joint Training.}\nOur architecture design and input schema help integrate various segmentation tasks so that they can be trained on one model. Tab.~\\ref{tab:joint} shows the effect of this joint training on different tasks. For the task-specific models, we perform the training on the corresponding task data for 18k iterations. In contrast, the joint training setting (see implementation section for details) has a total of 56k training iterations, which corresponds to 14k iterations per task.\n\nThe results show that joint training of different tasks greatly improves the performance of the model. This suggests that learning between tasks is mutually beneficial, which is also the secret of success in LLM. For example, the generic segmentation task helps refine the mask prediction in the referring segmentation. In addition, referring expressions also enhance the model’s ability to recognize more unseen categories, which in turn improves the performance of open-vocabulary segmentation tasks.\n\n\\begin{table}[h!]\n\\centering\n\\footnotesize\n\\caption{Comparison with the state-of-the-art methods on three referring image segmentation benchmarks with cIoU. (ft) denotes models further finetuned on RefCOCO/+/g after mix training.\nWe abbreviate the datasets: COCO(C)~\\cite{coco}, LVIS(L)~\\cite{lvis}, RefCOCO(RC)~\\cite{refcoco}, Object365(O365)~\\cite{shao2019objects365}, Video segmentation datasets(V), ADE20K(A)~\\cite{ade20k}, COCO-Stuff(CS)~\\cite{coco_stuff}, PACO-LVIS(PL)~\\cite{paco_lvis}, PASCAL-Part(PP)~\\cite{pascal_part}, GranD(G)~\\cite{glamm}, VOC2010(VOC)~\\cite{pascal-voc-2010}, Visual Genome(VG)~\\cite{vg}, Flicker30k(F30K)~\\cite{plummer2015flickr30k}, MUSE(M)~\\cite{pixellm}, gRefCOCO(gRC)~\\cite{grefcoco}, COCO-Interactive(CI).}\n\\renewcommand{\\arraystretch}{0.9}\n\\scalebox{0.7}{\n\\begin{tabular}{lcccccccccccc}\n\\toprule\n\\multicolumn{1}{c}{\\multirow{2}{*}{Method}} & \\multirow{2}{*}{Segmentation Data} & \\multirow{2}{*}{LLM Type} & \\multicolumn{3}{c}{RefCOCO} &  & \\multicolumn{3}{c}{RefCOCO+} &  & \\multicolumn{2}{c}{RefCOCOg} \\\\ \\cmidrule{4-6} \\cmidrule{8-10} \\cmidrule{12-13} \n\\multicolumn{1}{c}{}                        &                             &                           & val     & testA   & testB   &  & val     & testA    & testB   &  & val           & test         \\\\ \\midrule\nSEEM-L~\\cite{seem}                                      & C, L, RC                    & -                         & -       & -       & -       &  & -       & -        & -       &  & 65.6          & -            \\\\\nUNINEXT-L~\\cite{uninext}                                   & O365, C, RC, V                           & -                         & 80.3    & 82.6    & 77.8    &  & 70.0    & 74.9     & 62.6    &  & 73.4          & 73.7         \\\\\nUNINEXT-H~\\cite{uninext}                                   & O365, C, RC, V                            & -                         & 82.2    & 83.4    & 81.3    &  & 72.5    & 76.4     & 66.2    &  & 74.7          & \\textbf{76.4}         \\\\ \\midrule\nLISA~\\cite{lisa}                                        & A, CS, RC, PL, PP                             & Vicuna-7B                 & 74.1    & 76.5    & 71.1    &  & 62.4    & 67.4     & 56.5    &  & 66.4          & 68.5         \\\\\n\\gray{LISA(ft)}~\\cite{lisa}                                    & \\gray{A, CS, RC, PL, PP}                           & \\gray{Vicuna-7B}                 & \\gray{74.9}    & \\gray{79.1}    & \\gray{72.3}    &  & \\gray{65.1}    & \\gray{70.8}     & \\gray{58.1}    &  & \\gray{67.9}          & \\gray{70.6}         \\\\\nGLaMM~\\cite{glamm}                                       & G, RC                            & Vicuna-7B                 & 79.5    & 83.2    & 76.9    &  & 72.6    & 78.7     & 64.6    &  & 74.2          & 74.9         \\\\\nu-LLaVA~\\cite{ullava}                                     & A, CS, RC, PL, VOC                           & Vicuna-7B                 & 80.4    & 82.7    & 77.8    &  & 72.2    & \\textbf{76.6}     & 66.8    &  & \\textbf{74.8}          & 75.6         \\\\\nPerceptionGPT~\\cite{pi2023perceptiongpt}                               & RC, VG, F30k                           & Vicuna-13B                & 75.3    & 79.1    & 72.1    &  & 68.9    & 74.0     & 61.9    &  & 70.7          & 71.9         \\\\\nPixelLM~\\cite{pixellm}                                     & A, CS, RC, PL, M                          & Llama2-13B                & 73.0    & 76.5    & 68.2    &  & 66.3    & 71.7     & 58.3    &  & 69.3          & 70.5         \\\\\nGSVA~\\cite{xia2023gsva}                                        & A, CS, RC, PL, PP, gRC                           & Llama2-13B                & 77.7    & 79.9    & 74.2    &  & 68.0    & 71.5     & 61.5    &  & 73.2          & 73.9         \\\\\n\\gray{GSVA(ft)}~\\cite{xia2023gsva}            & \\gray{A, CS, RC, PL, PP, gRC}                           & \\gray{Llama2-13B}                & \\gray{79.2}    & \\gray{81.7}    & \\gray{77.1}    &  & \\gray{70.3}    & \\gray{73.8}     & \\gray{63.6}    &  & \\gray{75.7}          & \\gray{77.0}         \\\\ \\midrule\nPSALM                                       & C, RC, CI                           & Phi-1.5 (1.3B)            & \\textbf{83.6}    & \\textbf{84.7}    & \\textbf{81.6}    &  & \\textbf{72.9}    & 75.5     & \\textbf{70.1}    &  & 73.8          & 74.4         \\\\ \\bottomrule\n\\end{tabular}\n}\n\\label{tab:ref_coco_system}\n\\end{table}\n\n\\subsection{System-Level Comparison on In-Domain Tasks}\nIn this section, we compared our model on three in-domain tasks with other state-of-the-arts to illustrate the effectiveness of our approach.\n\n\\noindent\\textbf{Referring Segmentation.} \nMost existing works aimed at getting LMMs to perform image segmentation are designed for reference segmentation tasks. We compare PSALM with other works on RefCOCO, RefCOCO+, and RefCOCOg, and Tab.~\\ref{tab:ref_coco_system} shows the results. \nOwing to the generalized and flexible design of PSALM and the advantages of joint training on multiple tasks and datasets, our system was able to achieve state-of-the-art (SOTA) performance on RefCOCO and RefCOCO+, and competitive performance on RefCOCOg with STOA, despite being driven by LLM with only 1.3B parameters. \nIt is worth noting unlike methods such as LISA and GSVA, which may achieve improvements through task-specific fine-tuning (gray-labeled results), PSALM does not perform additional fine-tuning but still achieves better performance on RefCOCO and RefCOCO+ than their fine-tuned models.\n\n\\noindent\\textbf{Generic Segmentation.} \nWe evaluate PSALM with state-of-the-art methods on the COCO Panoptic Segmentation validation set (Tab.~\\ref{tab:sys_pan}). Here, we follow the evaluation protocol used in Mask2Former to report PQ, which is the main metric for panoptic segmentation, mAP on thing classes for instance segmentation, and mIoU by merging instance masks from the same category for semantic segmentation. \nCompared to other methods, PSALM achieves comparable performance at similar visual backbone sizes, demonstrating that PSALM is a powerful architecture, even when compared to approaches designed for specific tasks.\n\n\\begin{table}[h!]\n\\centering\n\\caption{Comparison with the state-of-the-art methods on Panoptic COCO-val. We abbreviate the datasets: COCO-SAM(CM)~\\cite{omgseg}, VIPSeg (VIP)~\\cite{vipseg}, while others following Tab.~\\ref{tab:ref_coco_system}.}\n\\footnotesize\n\\renewcommand{\\arraystretch}{0.9}\n\\scalebox{0.85}{\n\\begin{tabular}{lllclclclc}\n\\toprule\nMethod      & Backbone                                &  & Seg. Data                                &  & PQ   &  & mAP   &  & mIoU \\\\ \\midrule\nMask2Former~\\cite{mask2former} &  Swin-B       &  &  C             &  & 55.1 &  & 45.2 &  & 65.1 \\\\\nMask2Former~\\cite{mask2former} &  Swin-L       &  &  C             &  & 57.8 &  & 48.6 &  & 67.4 \\\\\nX-Decoder~\\cite{xdecode}   &  DaViT-B      &  &  C, L, RC      &  & 56.2 &  & 45.8 &  & 66.0 \\\\\nSEEM~\\cite{seem}        &  DaViT-B      &  &  C, L, RC      &  & 56.1 &  & 46.4 &  & 66.3 \\\\\nOMG-Seg~\\cite{omgseg}     &  ConvNeXt-XXL &  &  C, VIP, CM, V &  & 55.4 &  & -    &  & -    \\\\ \\midrule\nPSALM       &  Swin-B       &  &  C, RC, CI     &  & 55.9 &  & 45.7 &  & 66.6 \\\\ \\bottomrule\n\\end{tabular}\n}\n\\label{tab:sys_pan}\n\\end{table}\n\n\\begin{table}[h!]\n\\centering\n\\caption{Comparison with the state-of-the-art methods on COCO-Interactive. The results of \\gray{SEEM-B$^*$} is the result reported in official paper, which is evaluated on 600 random samples of \\texttt{COCO-val}, while all others are evaluated on all samples of \\texttt{COCO-val}. Abbreviations for each dataset are the same as Tab.~\\ref{tab:sys_pan}.}\n\\footnotesize\n\\renewcommand{\\arraystretch}{0.9}\n\\scalebox{0.8}{\n\\begin{tabular}{llclccccccccccc}\n\\toprule\n\\multirow{2}{*}{Method}            &  & \\multirow{2}{*}{Seg. Data}       &  & \\multicolumn{2}{c}{Point}                                &  & \\multicolumn{2}{c}{Scribble}                             &  & \\multicolumn{2}{c}{Box}                                  &  & \\multicolumn{2}{c}{Mask}                              \\\\ \\cmidrule{5-6} \\cmidrule{8-9} \\cmidrule{11-12} \\cmidrule{14-15} \n                                   &  &                                  &  & mIoU                         & cIoU                      &  & mIoU                         & cIoU                      &  & mIoU                         & cIoU                      &  & mIoU                         & cIoU                      \\\\ \\midrule\nSAM-B~\\cite{sam}                              &  & SA-1B                            &  & 48.7                         & 33.6                      &  & -                            & -                         &  & 73.7                         & 68.7                      &  & -                            & -                         \\\\\nSAM-L~\\cite{sam}                               &  & SA-1B                            &  & 51.8                         & 37.7                      &  & -                            & -                         &  & 76.6                         & 71.6                      &  & -                            & -                         \\\\\nSEEM-B~\\cite{seem}                             &  & C, L, RC                         &  & 47.8                         & 57.8                      &  & 43.0                         & 44.0                      &  & 44.9                         & 42.1                      &  & 48.4                         & 65.0                      \\\\\n\\gray{SEEM-B$^*$}~\\cite{seem} &  & \\gray{C, L, RC} &  & \\gray{81.7} & \\gray{-} &  & \\gray{83.5} & \\gray{-} &  & \\gray{75.7} & \\gray{-} &  & \\gray{76.0} & \\gray{-} \\\\\nOMG-Seg~\\cite{omgseg}                            &  & C, VIP, CM, V                    &  & 59.3                         & -                         &  & -                            & -                         &  & -                            & -                         &  & -                            & -                         \\\\ \\midrule\nPSALM                              &  & C, RC, CI                        &  & 64.3                         & 74.0                      &  & 66.9                         & 80.0                      &  & 67.3                         & 80.9                      &  & 67.6                         & 82.4                      \\\\ \\bottomrule\n\\end{tabular}\n}\n\\label{tab:inter_coco}\n\\end{table}\n\n\\noindent\\textbf{Interactive Segmentation}. \nWe also evaluate PSALM in the interactive segmentation tasks. Since the task does not have a well-developed dataset containing all four instructions, previous works have typically used the in-house dataset, thus we re-evaluate other methods on the COCO interactive validation set. The results are shown in Tab.~\\ref{tab:inter_coco}, and PSALM achieves leading performance on point, scribble, and mask instructions than all other methods, while on box instruction, SAM performs better on mIoU but worse on cIoU, and we hypothesize might be caused by the different distribution of training data, and the fact that SAM is trained on SA-1B~\\cite{sam}, which has a much larger data scale than what we used. \nIn addition, we also report the official results of SEEM, which only evaluated 600 samples from the COCO validation set as a reference. \n\n\\subsection{Generalizability on Out-of-Domain Tasks }\nThanks to the flexible design of architecture and input schema, multi-task joint-training, and the strong visual understanding capability of LMM, PSALM shows excellent performance on in-domain tasks, but more importantly, PSALM also demonstrates great potential to generalize to out-of-domain tasks in the zero-shot setting. In this section, we conduct experiments on three different out-of-domain tasks: open-vocabulary segmentation, generalized referring expression segmentation, and video object segmentation. We also tested the zero-shot result of the correspondence benchmark in Ego-Exo4D\\cite{egoexo} in the Appendix.\n\n\\begin{table}[h!]\n\\centering\n\\footnotesize\n\\caption{Comparison with the state-of-the-art methods on open-vocabulary instance segmentation and semantic segmentation benchmarks. We use mAP for instance segmentation and mIoU for semantic segmentation.We abbreviate the datasets: Pascal Context-459(PC459)~\\cite{pc}, Pascal Context-59(PC59)~\\cite{pc}, Pascal VOC-20(PAS20)~\\cite{pascal-voc-2010}}\n\\scalebox{0.85} {\n\\begin{tabular}{lccccccc}\n\\toprule\n\\multirow{2}{*}{Method} & \\multicolumn{2}{c}{OV Instance Seg.} &  & \\multicolumn{4}{c}{OV Semantic Seg.} \\\\ \\cmidrule{2-3} \\cmidrule{5-8} \n                        & A150          & Cityscapes         &  & PC459   & A150  & PC59  & PAS20  \\\\ \\midrule\nMaskCLIP~\\cite{maskclip}               & 6.0             & -                  &  & 10.0     & 23.7   & 45.9   & -       \\\\\nODISE~\\cite{odise}                   & 14.4            & -                  &  & 14.5     & 29.9   & 57.3   & -       \\\\\nSAN~\\cite{SAN}                     & 10.6            & -                  &  & 17.1     & 33.3   & 60.2   & 95.5    \\\\ \\midrule\nPSALM                   & 9.0             & 20.5               &  & 10.2     & 18.2   & 48.5   & 81.3    \\\\\nPSALM+LVIS              & 13.9            & 19.3               &  & 14.0     & 24.4   & 57.2   & 95.0    \\\\ \\bottomrule\n\\end{tabular}\n}\n\\label{tab:ov_system}\n\\end{table}\n\n\\noindent\\textbf{Open-Vocabulary Segmentation.} \nWe first evaluate our PSALM in open-vocabulary segmentation tasks, which require the model to have the ability to deal with unseen categories in training. Here, we conduct experiments on both open-vocabulary instance segmentation and open-vocabulary semantic segmentation, and Tab.~\\ref{tab:ov_system} shows the results. Without any special design, PSALM achieves reasonably good performance, although it is still worse than the best specific method in this task, such as SAN, but we believe that PSALM has a strong potential to be further improved by adding more diverse training data, which is advantages of our method. We have also made a preliminary attempt by further involving the LVIS dataset, and as we expected, the performance has significantly improvements.\n\nIn addition, existing open-vocabulary segmentation methods are built upon the CLIP model or diffusion model, while our approach is based on LMM models, which is a new path and attempt to bring new inspiration to the community, which we believe is even more important than the performance.\n\n\\begin{table}[h!]\n\\centering\n\\footnotesize\n\n\\caption{Our method's zero-shot performance on gRefCOCO. (ft) denotes models further fine-tuned on gRefCOCO after mix training.}\n\n\\renewcommand{\\arraystretch}{0.9}\n\\scalebox{0.8}{\n\\begin{tabular}{lcccccccccc}\n\\toprule\n\\multirow{2}{*}{Methods} & \\multirow{2}{*}{LLM Type} & \\multirow{2}{*}{Zero-Shot} & \\multicolumn{2}{c}{val} &  & \\multicolumn{2}{c}{testA} &  & \\multicolumn{2}{c}{testB} \\\\ \\cmidrule{4-5} \\cmidrule{7-8} \\cmidrule{10-11} \n                         &                           &                            & cIoU       & gIoU       &  & cIoU        & gIoU        &  & cIoU        & gIoU        \\\\ \\midrule\nMattNet~\\cite{yu2018mattnet}                  & -                         & \\XSolidBrush                          & 47.5       & 48.2       &  & 58.7        & 59.3        &  & 45.3        & 46.1        \\\\\nLTS~\\cite{lts}                      & -                         & \\XSolidBrush                          & 52.3       & 52.7       &  & 61.9        & 62.6        &  & 49.9        & 50.4        \\\\\nReLA~\\cite{grefcoco}                    & -                         & \\XSolidBrush                          & \\textbf{62.4}       & \\textbf{63.6}       &  & \\textbf{69.3}        & 70.0        &  & 59.9        & 61.0        \\\\ \\midrule\nLISA~\\cite{lisa}                     & Vicuna-7B                         & \\XSolidBrush                          & 38.7       & 32.2       &  & 52.6        & 48.5        &  & 44.8        & 39.7        \\\\\n\\gray{LISA(ft)}~\\cite{lisa}                     & \\gray{Vicuna-7B}                         & \\gray{\\XSolidBrush}                          & \\gray{61.7}       & \\gray{63.3}       &  &  \\gray{69.2}       & \\gray{70.1}        &  & \\gray{60.3}        & \\gray{61.3}        \\\\\nGSVA~\\cite{xia2023gsva}                      & Vicuna-7B                         & \\XSolidBrush                          & 61.7       & 63.3       &  & 69.2        & \\textbf{70.1}        &  & \\textbf{60.3}        & \\textbf{61.3}        \\\\ \n\\gray{GSVA(ft)}~\\cite{xia2023gsva}                     & \\gray{Vicuna-7B}                         & \\gray{\\XSolidBrush}                          & \\gray{63.3}       & \\gray{66.5}       &  &  \\gray{69.9}       & \\gray{71.1}        &  & \\gray{60.5}        & \\gray{62.2}        \\\\\\midrule\nPSALM                     & Phi-1.5 (1.3B)                         & \\Checkmark                          & 42.0       & 43.3       &  & 52.4        & 54.5        &  & 50.6        & 52.5        \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\label{tab:grefcoco}\n\\end{table}\n\n\\noindent\\textbf{Generalized Referring Expression Segmentation.} \nThe referring segmentation datasets used in training contain only a single object, however, the design of the mask proposal allows PSALM to directly address multi-target without any further training or fine-tuning. We evaluate the gRefCOCO benchmark which contains multiple segment targets. In practice, given an expression, we compute the similarity with all mask proposals and retain all masks with similarity greater than 0.6 as foreground. Tab.~\\ref{tab:grefcoco} shows the results, PSALM also achieved very promising performance, even outperforming the LISA version that only pre-trained on gRefCOCO but without task-specific fine-tuning.\n\n\\begin{table}[h!]\n\\centering\n\\footnotesize\n\\caption{Our method's zero-shot performance on DAVIS-2017 \\textit{val}. Note the SEEM report results on 345 randomly sampled frames, while others are evaluated on all frames.}\n\\renewcommand{\\arraystretch}{0.9}\n\\scalebox{0.85}{\n\\begin{tabular}{lcccccccc}\n\\toprule\nMethods                        &  & Video Data                                          &  & J\\&F                         &  & J                            &  & F                            \\\\ \\midrule\nXMem~\\cite{cheng2022xmem}                           &  & \\Checkmark                           &  & 87.7                         &  & 84.0                         &  & 91.4                         \\\\\nOMG-Seg~\\cite{omgseg}                        &  & \\Checkmark                           &  & 76.9                         &  & -                            &  & -                            \\\\\nPainter~\\cite{painter}                        &  & \\XSolidBrush                         &  & 34.6                         &  & 28.5                         &  & 40.8                         \\\\\nSegGPT~\\cite{wang2023seggpt}                         &  & \\XSolidBrush                         &  & 75.6                         &  & 72.5                         &  & 78.6                         \\\\\n\\gray{SEEM-B}~\\cite{seem} &  & \\gray{\\XSolidBrush} &  & \\gray{62.8} &  & \\gray{59.5} &  & \\gray{66.2} \\\\ \\midrule\nPSALM                          &  & \\XSolidBrush                         &  & 68.8                         &  & 65.9                         &  & 71.7                         \\\\ \\bottomrule\n\\end{tabular}\n}\n\\label{tab:davis}\n\\end{table}\n\n\\noindent\\textbf{Video Object Segmentation.} \nWe also evaluate PSALM on video object segmentation task, which typically gives an image of the first frame and a segmentation reference, and expect to segment subsequent videos based on the given reference. DAVIS-2017~\\cite{davis} is the most commonly used dataset, and we test PSALM on it. During inference, we extract the region feature based on the last-frame prediction (or first-frame segmentation reference) as the visual-prior condition and use the image of the current frame to predict the mask. Tab.~\\ref{tab:davis} shows the results, without training on any video data, PSALM shows promising zero-shot performance. \n\n\\section{Conclusion}\nThe PSALM proposed in this study extends the capability of LMM from text output tasks to image segmentation, addresses the output limitations of the LMM, and unifies various segmentation tasks with a novel input model. PSALM exhibits excellent performance in multiple in-domain tasks, and its generalization ability in out-of-domain tasks further underscores its potential. The success of PSALM in in-domain and cross-domain tasks highlights the importance of flexibility and adaptability in model design, paving the way for future innovations in visual understanding.\n\n\\newpage\n\\section{Prompts for Different Tasks}\nAs discussed in Sec.~\\ref{sec:condition_prompt}, our input schema has three kinds of inputs: task instruction prompt, condition prompt, and a set of mask tokens. \nFor different tasks, we use different instruction prompts and condition prompts, as listed in Tab.~\\ref{tab:prompts}. \nBasically, tasks that use the same type of condition prompts also adopt the same task instruction prompt. \n\nIn addition, some tasks that use category condition often need to deal with background as well, such as instance segmentation, for which we specifically append a \\textit{`background`} at the end of the joint sentence. \n\\setcounter{table}{13}\n\\begin{table}[h!]\n\\footnotesize\n\\centering\n\\caption{Detail prompts for all tasks.}\n\\scalebox{0.7}{\n\\begin{tabular}{c|c|c|c}\n\\toprule\nTask                       & Dataset                                                 & Instruction prompt                                                                      & Condition prompt                                                                                 \\\\ \\midrule\n\\multirow{2}{*}{Panoptic Seg.}              & \\multirow{2}{*}{COCO}                                   & \\multirow{2}{*}{\\textit{\\makecell[c]{You need to segment all objects.\\\\ This is all the candidate categories:}}} & \\multirow{2}{*}{\\texttt{[class1], [class2] ...}}                                               \\\\\n                           &                                                         &                                                                                         &                                                                                                  \\\\\n& & & \\\\\n                           \n\\multirow{2}{*}{OV Seg.}                    & \\multirow{2}{*}{ADE20K, \\textit{etc.}} & \\multirow{2}{*}{\\textit{\\makecell[c]{You need to segment all objects.\\\\ This is all the candidate categories:}}} & \\multirow{2}{*}{\\texttt{[class1], [class2], ...}}                                                \\\\\n                           &                                                         &                                                                                         &                                                                                                  \\\\\n& & & \\\\\n\\multirow{2}{*}{Referring Seg.}             & \\multirow{2}{*}{RefCOCO/+/g}                            & \\multirow{2}{*}{\\textit{\\makecell[c]{Please segment according to \\\\ the following instruction:}}}                 & \\multirow{2}{*}{\\texttt{object description}}                                              \\\\\n                           &                                                         &                                                                                         &                                                                                                  \\\\\n& & & \\\\\n\\multirow{2}{*}{Generalized Referring Seg.} & \\multirow{2}{*}{gRefCOCO}                               & \\multirow{2}{*}{\\textit{\\makecell[c]{Please segment according to \\\\ the following instruction:}}}                  & \\multirow{2}{*}{\\texttt{object description}}                                      \\\\\n                           &                                                         &                                                                                         &                                                                                                  \\\\\n& & & \\\\\n\\multirow{2}{*}{Interactive Seg.}           & \\multirow{2}{*}{COCO-Interactive}                       & \\multirow{2}{*}{\\textit{\\makecell[c]{Please segment by given regions:}}}                                       & \\multirow{2}{*}{\\texttt{<interaction1>, <interaction2>...}} \\\\\n                           &                                                         &                                                                                         &                                                                                                  \\\\\n& & & \\\\\n\\multirow{2}{*}{Video Object Seg.}          & \\multirow{2}{*}{DAVIS}                                  & \\multirow{2}{*}{\\textit{\\makecell[c]{Please segment by given regions:}}}                                    & \\multirow{2}{*}{\\texttt{<interaction1>, <interaction2>...}} \\\\\n                           &                                                         &                                                                                         &                                                                                                  \\\\\n                    & & & \\\\\n        \\multirow{2}{*}{Ego-exo Correspondence}                   & \\multirow{2}{*}{Ego-Exo4D}                              & \\multirow{2}{*}{\\textit{\\makecell[c]{Please segment by given regions:}}}                                    & \\multirow{2}{*}{\\texttt{<interaction1>, <interaction2>...}} \\\\\n     &                                                         &                                                                                         &                                                                                                  \\\\ \\bottomrule\n\\end{tabular}\n}\n\\label{tab:prompts}\n\\end{table}\n\n\\section{Training Details}\nIn this section, we introduce the training details of our model.\n\nOur model has two training stages, the first stage is the vision-language alignment stage, in which we strictly follow the default settings of LLaVA, with the only change being the adoption of Phi-1.5 as the LLM and the use of Swin as the visual encoder with the hyper-parameters as shown in Tab.~\\ref{tab:first_train}.\n\nIn the second stage, to train the final model, we use a total of 56k training iterations. In each iteration, we randomly sample one task with equal probability from four tasks: generic segmentation (COCO Panoptic), referring segmentation (RefCOCO/+/g), interactive segmentation (COCO-Interactive), and a visual-language instruction task (LLaVA1.5 training data) and all images are resized to $1024^2$ by padding the shorter side to keep the aspect ratio. During training, the visual encoder is frozen while all other model parts are trainable. In addition, we adopt the Hungarian matching to automatically assign the ground-truth of mask proposals during training. In generic segmentation tasks, we use both classification loss and mask loss as cost matrix for matching, while other tasks use only mask loss. Tab.~\\ref{tab:second_train} shows the hyper-parameters. \n\nFor ablation, we use the same training setting as the final model, but with the shorter 9k training iterations to reduce the cost of the experiment. \n\n\\begin{table}[h!]\n\\setlength{\\tabcolsep}{2.5mm}\n\\centering\n\\footnotesize\n\\caption{Hyper parameters of our model in the first stage training.}\n\\begin{tabular}{lcccr}\n\\toprule\nParameters   &&& & Value \\\\ \\midrule\nOptimizer    &&& &  AdamW     \\\\\nLearning Rate &&& &  $2\\times10^{-3}$     \\\\\nBatch Size &&& &  128    \\\\\nNumber of Iteration &&& &  4,650    \\\\\nLearning Rate Schedule &&& &  Cosine Decay     \\\\\nWeigth Decay &&& &  0.0     \\\\\nWarmup Steps &&& &  140     \\\\\n$\\beta_1$ &&& &  0.9     \\\\\n$\\beta_2$ &&& &  0.999     \\\\\n        Training Data &&& &  CC3M     \\\\\n\nImage Size &&& &  $1024\\times 1024$     \\\\\n\\multirow{2}{*}{Image Processing} &&& &  \\multirow{2}{*}{\\makecell[r]{Resize long edge to 1024 \\\\ and padding short edge to 1024.}}     \\\\\n        &&&& \\\\\n        \\bottomrule\n\\end{tabular}\n\\label{tab:first_train}\n\\end{table}\n\\begin{table}[h!]\n\\setlength{\\tabcolsep}{0.6mm}\n\\centering\n\\footnotesize\n\\caption{Hyper parameters of our model in the second stage training.}\n\\begin{tabular}{lcccr}\n\\toprule\nParameters   &&& & Value \\\\ \\midrule\nOptimizer    &&& &  AdamW     \\\\\nLearning Rate &&& &  $4\\times10^{-5}$     \\\\\nBatch Size &&& &  64    \\\\\nNumber of Iteration &&& &  56,000    \\\\\nNumber of Iteration (for Ablation) &&& &  9,000    \\\\\nLearning Rate Schedule &&& &  Cosine Decay     \\\\\nWeigth Decay &&& &  0.0     \\\\\nWarmup Steps &&& &  1680     \\\\\nWarmup Steps (for Ablation) &&& &  270    \\\\\n$\\beta_1$ &&& &  0.9     \\\\\n$\\beta_2$ &&& &  0.999     \\\\\n        \\multirow{2}{*}{Training Data} &&& &  \\multirow{2}{*}{\\makecell[r]{COCO-Panoptic (25\\%); RefCOCO/+/g(25\\%); \\\\ COCO-Interactive(25\\%); LLaVA 1.5(25\\%)}}     \\\\\n\n        &&&& \\\\\nImage Size &&& &  $1024\\times 1024$     \\\\\n\\multirow{2}{*}{Image Processing} &&& &  \\multirow{2}{*}{\\makecell[r]{Resize long edge to 1024 \\\\ and padding short edge to 1024.}}     \\\\\n        &&&& \\\\\n        \\bottomrule\n\\end{tabular}\n\\label{tab:second_train}\n\\end{table}\n\n\\section{Details on building COCO-Interactivate Dataset}\nIn this section, we describe in detail how to build the COCO-Interactive dataset.\nThe COCO-Interactivate is based on image and annotations of COCO2017 instance segmentation, which provide the masks and bounding boxes for each instance, and we use the annotations to automatically generate four types of visual prompts: point, scribble, mask, and box. Fig.~\\ref{fig:interactive} shows the illustration of the visual prompts, and we will introduce each of them in the following:\n\n\\noindent\\textbf{Point}. For each instance, we generate a point visual prompt by randomly sampling a point within a circular region centered on the bounding box with a radius of half the short side of the bounding box.  \n\n\\noindent\\textbf{Scribble}. The generation process of scribble has two steps. First, we randomly jitter the width and height of the ground-truth bounding box from a scale factor range of [0.5, 1.2], and ensure that the IoU between the jittered box with the original box is greater than 0.5. Then, given a jittered box, we randomly select one of its diagonals and generate a sin curve along it. The amplitude of the sin curve is randomly chosen from [10, 20], the frequency is randomly sampled from \\([0.2 \\times 2\\pi, 2\\pi]\\), and the phase shift is randomly sampled from \\([0, 2\\pi]\\). \n\n\\noindent\\textbf{Box}. We randomly jittered ground truth boxes as box prompts, and the length and width of each jittered box were obtained by scaling from a scale sampled in the range [0.9, 1.1].\n\n\\noindent\\textbf{Mask}. The mask visual prompts are obtained by applying a Gaussian filter on the ground truth mask at first, with the standard deviation of the Gaussian kernel set to 2, and then binarizing the blurred mask. \n\n\\setcounter{figure}{3}\n\n\\begin{figure}[!t]\n  \\centering\n  \\includegraphics[width=1.\\textwidth]{images/sup_interactive_v2.pdf}\n  \\caption{Visualization of different types of visual prompts}\n  \\label{fig:interactive}\n\\end{figure}\n\\section{Datasets}\n\\noindent\\textbf{COCO-Panoptic.} The COCO-Panoptic dataset is an extension of the COCO dataset, specifically designed for panoptic segmentation tasks. It consists of over 200,000 images with detailed annotations that cover 80 object categories for instance segmentation and additional categories for semantic segmentation. \n\n\\noindent\\textbf{RefCOCO.} RefCOCO is a dataset designed for the task of referring expression comprehension and segmentation. It consists of images from the COCO dataset that are annotated with referring expressions, where each expression uniquely identifies a particular object within the image. It includes three splits: RefCOCO, RefCOCO+, and RefCOCOg, each with different characteristics and annotation styles.\n\n\\noindent\\textbf{LVIS.} The LVIS dataset is a benchmark for instance segmentation with a large vocabulary of object categories. It features high-quality instance annotations for over 1,000 object categories across a diverse set of images. LVIS is particularly known for its long-tail distribution of categories, which presents a unique challenge for segmentation models.\n\n\\noindent\\textbf{ADE20K.} ADE20K is a widely used dataset as an open-vocabulary segmentation benchmark, it contains both things and stuffs annotations and thus can evaluate panoptic segmentation. It encompasses a diverse collection of images from various indoor and outdoor scenes. It is part of the MIT Scene Parsing Benchmark and provides dense pixel-wise annotations for 150 object categories, facilitating research in scene understanding and segmentation.\n\n\\noindent\\textbf{Cityscapes.} \nCityscape is a dataset focused on urban street scenes. The dataset contains a large number of high-quality video sequences and pixel-accurate annotations from 30 categories in 50 different urban street scenes. With its detailed instance-level annotations, Cityscapes is pivotal to advancing semantic and instance segmentation research, especially in autonomous driving and urban scene perception.\n\n\\noindent\\textbf{Pascal VOC.} Pascal VOC contains 20 classes of semantic segmentation annotation.\n\n\\noindent\\textbf{Pascal Context.} Pascal Context is an extension of the PASCAL VOC dataset, providing comprehensive scene understanding through detailed semantic labels for the entire scene in each image. It comes in two versions: PC-59, which focuses on the most frequent 59 categories, and PC-459 includes a broader set of 459 categories.\n\n\\noindent\\textbf{DAVIS-2017.} DAVIS-2017 is a video segmentation benchmark that provides high-quality, full-resolution video sequences with per-pixel annotations of multiple objects. It is commonly used to evaluate the performance of video object segmentation methods, particularly in semi-supervised settings where the first-frame mask is provided.\n\n\\noindent\\textbf{gRefCOCO.} gRefCOCO is the first large-scale Generalized Referring Expression Segmentation dataset that contains multi-target, no-target, and single-target expressions. \n\n\\noindent\\textbf{Ego-Exo4D.} Ego-Exo4D is a diverse, large-scale multimodal multiview video dataset and benchmark challenge. Ego-Exo4D centers around simultaneously captured and time-synced egocentric and exocentric vides of skilled human activities. More than 800 participants from 13 cities worldwide performed these activities in 131 different natural scene contexts, yielding long-form captures from 1 to 42 minutes each and 1,422 hours of video combined. The Correspondence benchmark needs the model to predict the corresponding mask for the same object in each synchronized frame of the other view if it is visible.\n\n\\noindent\\textbf{CC3M.} CC3M is a large-scale dataset of image-caption pairs designed for training and evaluating visual-language models. It contains around three million images sourced from the web, each accompanied by a descriptive caption. \n\n\\section{Implement Details of Decouple Ablation}\nIn Tab.~3, we compare the decouple design and non-decouple design on COCO Semantic Segmentation. Here, we will introduce the implementation detail for the non-decouple design. Specifically, we omitted the mask token, and instead of using the category condition embeddings as the mask query, and fed into the mask generator to generate masks. In this case, the matching loss mechanism is unnecessary, the condition embedding of a category is used to predict the class and mask at the same time. \n\n\\begin{table}[h!]\n\\centering\n\\footnotesize\n\\caption{Performance on multi-modal benchmarks.}\n\\begin{tabular}{lccccc}\n\\toprule\nMethods         & LLM Type          & VQA$^{V2}$ & SQA & MMB & POPE \\\\ \\midrule\nOpenFlamingo~\\cite{flamingo}    & MPT-7B        & 51.8   & -         & 5.7     & -    \\\\\nKosmos-2~\\cite{kosmos_2}        & -            & 51.1   & -         & -       & -    \\\\\nBLIP-2~\\cite{blip2}          & Vicuna-13B    & 41.0   & 61.0      & -       & 85.3 \\\\\nInstructionBLIP~\\cite{dai2024instructblip} & Vicuna-7B    & -      & 60.5      & -       & 36.0 \\\\\nIDEFICS~\\cite{idefics}           & Llama-7B    & 50.9      & 44.2         & 48.2    & - \\\\\nLLaVA-1.5~\\cite{llavav1_5}       & Vicuna-7B & 78.5   & 66.8      & 64.3    & 85.9\\\\\n\\midrule\nPSALM            & Phi-1.5 (1.3B) & 62.3   & 64.9      & 52.5    & 80.3 \\\\ \\bottomrule\n\\end{tabular}\n\\label{tab:vl_bench}\n\\end{table}\n\n\\begin{table}[h!]\n\\caption{Zero-shot performance of Correspondence benchmark on Ego-Exo4D.}\n  \\begin{minipage}{.48\\textwidth}\n  \\centering\n\\footnotesize\n\\scalebox{0.9}{\n  \\begin{tabular}{llcccc}\n\\toprule\n\\multirow{2}{*}{Query Mask} & \\multirow{2}{*}{Method} & \\multirow{2}{*}{Zero-Shot}  & Test &  & Val \\\\\n                            &                         &                             & IoU  &  & IoU \\\\ \\midrule\nEgo                         & XSegTx                 & \\Checkmark   & 0.6  &  & -   \\\\\nEgo                         & XMem                    & \\Checkmark   & 4.6  &  & -   \\\\\nEgo                         & XSegTx                  & \\XSolidBrush & 13.9 &  & -   \\\\\nEgo                         & XMem                    & \\XSolidBrush & 14.6 &  & -   \\\\\n\\midrule\nEgo                         & PSALM                   & \\Checkmark   & -    &  & 7.9 \\\\  \\bottomrule\n\\end{tabular}\n}\n\\label{tab:mask_query}\n\n  \\end{minipage}\\hfill\n  \\begin{minipage}{.48\\textwidth}\n  \\centering\n  \\footnotesize\n  \\scalebox{0.9}{\n  \\begin{tabular}{llcccc}\n\\toprule\n\\multirow{2}{*}{Query Mask} & \\multirow{2}{*}{Method} & \\multirow{2}{*}{Zero-Shot}  & Test &  & Val \\\\\n                            &                         &                             & IoU  &  & IoU \\\\ \\midrule\nExo                         & XSegTx                  & \\Checkmark   & 1.6  &  & -   \\\\\nExo                         & XMem                    & \\Checkmark   & 21.8 &  & -   \\\\\nExo                         & XSegTx                  & \\XSolidBrush & 43.8 &  & -   \\\\\nExo                         & XMem                    & \\XSolidBrush & 43.4 &  & -   \\\\\n\\midrule\nExo                         & PSALM                   & \\Checkmark   & -    &  & 9.6 \\\\ \\bottomrule\n\\end{tabular}\n}\n\n  \\end{minipage}\n\\label{tab:ego4d}\n\\end{table}\n\\section{Additional Experiment}\n\\subsubsection{Multi-modal Benchmark Evaluation.} \n\nOur PSALM model is based on MLLM and thus able to deal with vision and language tasks, and therefore we evaluate our model on several commonly used multi-modal benchmarks, and results are shown in Tab.~\\ref{tab:vl_bench}. PSALM achieved promising results compared with other MLLM methods, such as BLIP-2~\\cite{blip2} and InstructionBLIP~\\cite{dai2024instructblip}. Although our model still lags behind the official LLaVA1.5 7B model, we believe that increasing the model size can largely close the performance gap.\n\nGiven the absence of training data, our related works such as LISA\\cite{lisa} and PixelLM\\cite{pixellm}, despite their theoretical capability to handle such tasks, yield suboptimal results. Take LISA as an instance, it achieves a mere \\textbf{0.12} on the VQA score in a zero-shot manner.\n\n\\subsubsection{Ego-Exo4D Correspondence Benchmark.} \nEgo-Exo4D~\\cite{egoexo} is a large-scale multi-modal multiview video dataset, and its correspondence benchmark is designed to predict the mask of an object in a novel view based on a given view. We evaluate this benchmark in a zero-shot manner to show the task generality of our model for such tasks. \nWe performed the evaluation on Ego-Exo4D benchmark, since the official test set and model have not been released yet, we cannot directly compare the performance under the same setting, so we only report the quantitative results on the validation set as a reference in Tab.~\\ref{tab:ego4d} and shows more qualitative results in Fig.~\\ref{fig:Ego-Exo4D}. \n\n\\section{More Qualitative Results}\nFig.~\\ref{fig:panoptic}, Fig.~\\ref{fig:refer} and Fig.~\\ref{fig:interactive} show more qualitative examples of in-domain tasks. Fig.~\\ref{fig:ov}, Fig.~\\ref{fig:grefcoco} and Fig.~\\ref{fig:davis} shows more examples of out-of-domain tasks.\n\n\\begin{figure}[!t]\n  \\centering\n  \\includegraphics[width=1.\\textwidth]{images/sup_pan_v2.pdf}\n  \\caption{More examples of panoptic segmentation in COCO~\\cite{coco}.}\n  \\label{fig:panoptic}\n\\end{figure}\n\\begin{figure}[!t]\n  \\centering\n  \\includegraphics[width=1.\\textwidth]{images/sup_ref_v3.pdf}\n  \\caption{More examples of referring segmentation in RefCOCO~\\cite{refcoco}.}\n  \\label{fig:refer}\n\\end{figure}\n\\begin{figure}[!t]\n  \\centering\n  \\includegraphics[width=1.\\textwidth]{images/sup_inter_v2.pdf}\n  \\caption{More examples of interactive segmentation in COCO-Interactive.}\n  \\label{fig:interactive}\n\\end{figure}\n\\begin{figure}[!t]\n  \\centering\n  \\includegraphics[width=1.\\textwidth]{images/sup_ov_v3.pdf}\n  \\caption{More examples of open-vocabulary instance segmentation on ADE20K~\\cite{ade20k}.}\n  \\label{fig:ov}\n\\end{figure}\n\\begin{figure}[!t]\n  \\centering\n  \\includegraphics[width=1.\\textwidth]{images/sup_grefcoco_v2.pdf}\n  \\caption{More examples of generalized referring segmentation in gRefCOCO~\\cite{grefcoco}.}\n  \\label{fig:grefcoco}\n\\end{figure}\n\\begin{figure}[!t]\n  \\centering\n  \\includegraphics[width=1.\\textwidth]{images/sup_davis_v2.pdf}\n  \\label{fig:davis}\n\\end{figure}\n\\begin{figure}[!t]\n  \\centering\n  \\includegraphics[width=1.\\textwidth]{images/sup_davis_v2_2.pdf}\n  \\caption{More examples of video object segmentation in DAVIS \\textit{val}~\\cite{davis}.}\n  \\label{fig:davis}\n\\end{figure}\n\\begin{figure}[!t]\n  \\centering\n  \\includegraphics[width=1.\\textwidth]{images/sup_ego.pdf}\n  \\caption{More examples of Ego-exo correspondence in Ego-Exo4D~\\cite{egoexo}.}\n  \\label{fig:Ego-Exo4D}\n\\end{figure}\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Multi-label Cluster Discrimination \\\\ for Visual Representation Learning}\n\n\\begin{document}\n\n\\title{Multi-label Cluster Discrimination \\\\ for Visual Representation Learning} \n\n\\titlerunning{Multi-label Cluster Discrimination for Visual Representation Learning}\n\n\\author{Xiang An\\inst{1}\\orcidlink{0009-0008-4652-8296}\n\\and\nKaicheng Yang \\inst{1}\\orcidlink{0009-0008-6073-9014}\n\\and\nXiangzi Dai \\inst{1}\\orcidlink{0009-0009-3236-8380}\n\\and \\\\\nZiyong Feng \\inst{1}\\orcidlink{0009-0007-8689-8366}\n\\and\nJiankang Deng\\thanks{Corresponding author.}\\inst{2}\\orcidlink{0000-0002-3709-6216}\n}\n\n\\authorrunning{Xiang An, Kaicheng Yang, Xiangzi Dai, Ziyong Feng and Jiankang Deng}\n\n\\institute{\nDeepGlint \\\\\n\\email{xiangan@deepglint.com} \\and\nHuawei Noah’s Ark Lab \\\\\n\\email{jiankang.deng@gmail.com}\n}\n\n\\maketitle\n\n\\begin{abstract}\n\\label{sec:abstract}\nContrastive Language Image Pre-training (CLIP) has recently demonstrated success across various tasks due to superior feature representation empowered by image-text contrastive learning. However, the instance discrimination method used by CLIP can hardly encode the semantic structure of training data. To handle this limitation, cluster discrimination has been proposed through iterative cluster assignment and classification. Nevertheless, most cluster discrimination approaches only define a single pseudo-label for each image, neglecting multi-label signals in the image.\nIn this paper, we propose a novel Multi-Label Cluster Discrimination method named MLCD to enhance representation learning. In the clustering step, we first cluster the large-scale LAION-400M dataset into one million centers based on off-the-shelf embedding features. Considering that natural images frequently contain multiple visual objects or attributes, we select the multiple closest centers as auxiliary class labels. In the discrimination step, we design a novel multi-label classification loss, which elegantly separates losses from positive classes and negative classes, and alleviates ambiguity on decision boundary. We validate the proposed multi-label cluster discrimination method with experiments on different scales of models and pre-training datasets. Experimental results show that our method achieves state-of-the-art performance on multiple downstream tasks including linear probe, zero-shot classification, and image-text retrieval. Code and models have been released at \\github \n\\url{https://github.com/deepglint/unicom} and \\huggingface \\href{https://huggingface.co/collections/DeepGlint-AI/mlcd-670d18d767cea37ea7436e69}{Hugging Face}.\n\n\\keywords{Visual Representation Learning, Instance Discrimination, Cluster Discrimination, Multi-label Learning}\n\\end{abstract}\n\n\\section{Introduction}\n\n\\label{sec:intro}\n\nLanguage-supervised visual pre-training, \\eg, CLIP~\\cite{radford2021learning} and ALIGN~\\cite{jia2021scaling}, \nhas been established as a simple yet effective methodology for visual representation learning. Empowered by image-text contrastive learning, pre-trained CLIP models exhibit remarkable versatility and transferability across various downstream tasks (\\eg, linear probe, zero-shot classification, and image retrieval). As illustrated in Fig.~\\ref{fig1:instance}, CLIP aligns the visual and textual signals of each instance into a unified semantic space by cross-modal instance discrimination. Nevertheless, the instance discrimination method used by CLIP can hardly encode the semantic structure of training data, because instance-wise contrastive learning always treats two samples as a negative pair if they are from different instances, regardless of their semantic similarity. When a large number of instances are selected into the mini-batch to form the contrastive loss, negative pairs that share similar semantics will be undesirably pushed apart in the embedding space. \n\nTo handle the limitations of instance discrimination, cluster discrimination methods (\\eg, DeepCluster~\\cite{caron2018deep}, SeLa~\\cite{asano2019self}, ODC~\\cite{zhan2020online}, SwAV~\\cite{caron2020unsupervised}, CoKe~\\cite{qian2022unsupervised}, and UNICOM~\\cite{an2023unicom}) have been proposed for deep unsupervised learning through jointly learning image embeddings and cluster assignments. Learning representations with clusters will pull similar instances together, which is beneficial for capturing semantic structures in data. However, most cluster discrimination approaches only define a single pseudo-label for each image as depicted in Fig.~\\ref{fig1:cluster}. By contrast, natural language supervision proposed in CLIP can provide richer forms of labels for a single image, \\eg, objects, scenes, actions, and relations, at multiple levels of granularity. \n\nAs can be seen from Fig.~\\ref{fig:pruityconflict}, \na web image frequently contains multiple classification targets, such as objects~\\cite{yang2016exploit} or attributes~\\cite{pham2021learning}. The existence of multiple objects in the image requires laborious cropping~\\cite{Li_2023_ICCV,Abdelfattah_2023_ICCV} to construct single-label annotations, while some scenario elements and attributes in the image are hard to disentangle to obtain single-label instances~\\cite{pham2021learning,Zhu_2023_ICCV}. These real-world challenges pose so-called multi-label classification where an image is equipped with multiple labels beyond a single label.\n\nIn this paper, we aim to boost the visual representation power of the CLIP model by introducing a novel Multi-Label Cluster Discrimination (MLCD) approach.\nIn the clustering step, we follow UNICOM~\\cite{an2023unicom} to conduct one step of offline clustering by using the features predicted by a pre-trained CLIP model. Due to the limited discrimination power of the CLIP model~\\cite{radford2021learning}, the single pseudo-label may not cover all of the visual signals (\\eg, objects or attributes) in the image. To this end, we further perform a similarity-based sorting against $k$ class centers and select the top $l$ class centers as the positive class centers for that image. \nIn the discrimination step, we follow the Circle loss~\\cite{sun2020circle} to \ndesign a multi-label loss to effectively deal with multiple labels. The vanilla version of \nthe multi-label loss exploits relative similarity comparisons between positive and negative classes. More specifically, the optimization seeks to narrow the gap between the intra-class similarities $\\{s_i\\}$ and \nthe inter-class similarities $\\{s_j\\}$ by reducing all possible $(s_j-s_i)$. However, optimizing $(s_j-s_i)$\nusually leads to a decision boundary allowing ambiguity~\\cite{sun2020circle}. To this end, we introduce another two optimization targets (\\ie, decreasing $s_j$ and increasing $s_i$) into the loss function. Introducing the additional two items enables an elegant separation of positive class loss and negative class loss (Eq.~\\ref{eqn:mlc2}), {which can alleviate the ambiguity on the decision boundary}. To alleviate inter-class conflict and save the computation time on the classifier layer, we also employ PartialFC~\\cite{an2022killing} and randomly sample part of the negative class centers during each iteration.  \n\nThe main contributions of our paper are the following:\n\\begin{enumerate}\n\\item We propose a novel multi-label cluster discrimination method for visual representation learning on large-scale data. In the clustering step, we employ one step of offline k-means to predict multiple labels for each training sample. In the discrimination step, we explore multi-label classification, which considers multiple supervision signals for a single image and learns better semantic structure in data.\n\\item To avoid ambiguity during the optimization of $(s_j-s_i)$, we add additional optimization targets by maximizing the within-class similarity $s_i$, as well as to minimizing the between-class similarity $s_j$. By doing so, the loss from positive class labels and negative class labels can be elegantly separated.\n\\item The proposed multi-label cluster discrimination significantly boosts the representation power compared to the instance discrimination-based model (\\eg, OpenCLIP~\\cite{cherti2023reproducible} and FLIP~\\cite{li2023scaling}) and the cluster discrimination-based model (\\eg, UNICOM~\\cite{an2023unicom}) on the downstream tasks (\\eg, linear probe, zero-shot classification, zero-shot retrieval).\n\\end{enumerate}\n\n\\section{Related Work}\n\n\\noindent{\\bf Visual Representation Learning.}\nVisual representation pre-training methods can be mainly divided into three categories: (1) supervised learning by using manually annotated class labels (\\eg, ImageNet-1K/-21K~\\cite{deng2009imagenet} and JFT-300M/-3B~\\cite{dosovitskiy2021image,zhai2022scaling}), (2) weakly-supervised learning by employing hashtags~\\cite{mahajan2018exploring,singh2022revisiting} or text descriptions~\\cite{radford2021learning,jia2021scaling,li2023scaling}, and (3) unsupervised learning~\\cite{chen2020big,he2020momentum,caron2018deep} by \ndesigning appropriate pretext tasks (\\eg, solving jigsaw puzzles~\\cite{noroozi2016unsupervised}, invariant mapping~\\cite{chen2021exploring}, and masked image inpainting~\\cite{he2022masked}). Even though fully supervised pre-training can learn a strong semantic signal from each training example, manual label annotation is time-consuming and expensive thus supervised learning is less scalable. In this paper, we focus on annotation-free pre-training which can be easily scaled to billions of web images to learn visual representation for downstream tasks. \n\n\\noindent{\\bf Instance and Cluster Discrimination.}\nInstance discrimination~\\cite{chen2020big,he2020momentum,radford2021learning} is usually implemented by the contrastive loss to pull images from the same instance as well as push away images from different instances. Among these instance discrimination methods,  \nlanguage-supervised visual pre-training, \\eg, CLIP~\\cite{radford2021learning,yang2023alip,gu2024rwkv},\nis a simple yet powerful approach to take advantage of rich forms of labels at multiple levels of granularity for a single image. Even though CLIP~\\cite{radford2021learning} has recently demonstrated impressive success, instance-wise contrastive learning always treats different instances as negative pairs thus it can hardly capture the full semantic information from the training data. \n\nTo explore potential semantic structures in the training data, cluster discrimination~\\cite{caron2018deep,asano2019self,zhan2020online,li2020prototypical,caron2020unsupervised,qian2022unsupervised} is proposed with two iterative steps: (1) the clustering step to assign a single class label for each sample, and (2) the classification step to learn a classifier to predict the assigned pseudo label. In cluster discrimination methods, each cluster contains more than one instance, visually similar instances will be pulled closer and thus cluster discrimination can better capture semantic structures from data. However, multiple visual elements can exist in one single image and the single label used by cluster discrimination may not cover all visual signals. \n\n\\noindent{\\bf Multi-label Classification.} \nMulti-label classification~\\cite{tsoumakas2007multi,zhang2013review} assigns a set of multiple labels for each instance. Compared with single-class classification, where each instance is assigned with a single label, multi-label classification~\\cite{yang2016exploit,zhao2021transformer,Xia_2023_ICCV} is more challenging~\\cite{liu2017easy,liu2021emerging}. Considering multiple labels are drawn from $k$ categories, the multi-label classification can be decomposed into $k$ binary classification tasks. However, the binary cross-entropy loss involves issues regarding imbalance~\\cite{ridnik2021asymmetric}. Through analyzing the intrinsic loss functions\nof the classification loss and the metric loss~\\cite{wang2019multi}, Sun \\etal~\\cite{sun2020circle} formulate a unified multi-label loss function to exploit relative comparison between positive and negative classes. Nevertheless, the relative comparison $(s_j-s_i)$ allows ambiguity for convergence. Su \\etal~\\cite{su2022zlpr} introduce a threshold into the multi-label loss and design the Threshold-bounded Log-sum-exp and Pairwise Rank-based (TLPR) loss, hoping that the logits of positive categories will be larger than the threshold and the logits of negative categories will be smaller than the threshold. However, the TLPR loss is only designed for clean multi-label datasets and is not suitable for large-scale multi-label datasets with heavy noises.\nIn this paper, we only employ one step of offline clustering to predict multiple labels for each image and then design a robust multi-label classification disambiguation loss to achieve good feature representation when training on the automatically clustered large-scale data.\n\n\\begin{figure}[t]\n\\centering\n\\begin{subfigure}{0.33\\textwidth}\n\\includegraphics[height=1.3\\textwidth]{figs/method_01.pdf}\n\\captionsetup{justification=centering}\n\\caption{ Instance \\linebreak  Discrimination}\n\\label{fig1:instance}\n\\end{subfigure}\\hspace{-10pt}\n\\begin{subfigure}{0.33\\textwidth}\n\\includegraphics[height=1.3\\textwidth]{figs/method_02.pdf}\n\\captionsetup{justification=centering}\n\\caption{ Cluster \\linebreak Discrimination}\n\\label{fig1:cluster}\n\\end{subfigure}\\hspace{-5pt}\n\\begin{subfigure}{0.33\\textwidth}\n\\includegraphics[height=1.3\\textwidth]{figs/method_03.pdf}\n\\captionsetup{justification=centering}\\caption{ Multi-Label \\linebreak Cluster Discrimination}\n\\label{fig1:multicluster}\n\\end{subfigure}\n\\caption{Comparisons of instance discrimination, cluster discrimination, and the proposed multi-label cluster discrimination. (a) Instance discrimination treats each image-text pair as a unique instance, failing to capture the semantic structure within the training data. (b) Cluster discrimination improves the semantic embedding by grouping similar instances but struggles with multi-label signals in a single image. (c) The proposed multi-label cluster discrimination addresses this challenge by assigning multiple class labels to each sample, \ncapturing different granularities of visual signals (\\eg, objects or attributes) in one image.}\n\\label{fig1:main}\n\\vspace{-4mm}\n\\end{figure}\n\n\\section{Method}\nGiven a training set $X=\\{x_1, x_2,...,x_n\\}$ including $n$ images, visual representation learning aims at learning a function $f$ that maps images $X$ to normalized embeddings $E=\\{e_1, e_2,...,e_n\\}$ with $e_i=f(x_i)$, such that embeddings can describe the semantic similarities between different images. \n\n\\subsection{Preliminaries}\n\n\\noindent\\textbf{Instance Discrimination} achieves semantic embedding by minimizing a contrastive loss function represented as:\n\\vspace{-2mm}\n\\begin{equation}\n\\label{eqn:ID}\n\\mathcal{L}_\\mathrm{ID} = - \\log \\frac{\\exp(e_i'^T e_i   )}{\\sum_{j=1}^k\\exp(e_j'^T e_i   ) },\n\\end{equation}\nwhere $\\exp(\\cdot)$ denotes the exponential function, and $e_i$ and $e_i'$ denote the normalized image and text embeddings for the instance $i$ in CLIP~\\cite{radford2021learning}. Meanwhile, $e_j'$ contains one positive text representation for $i$ and $(k-1)$ negative text representations sourced from different instances. As illustrated in Fig.~\\ref{fig1:instance}, the instance discrimination based CLIP model jointly trains an image encoder and a text encoder to predict the correct image-text pairings from a batch of training examples.\n\n\\noindent{\\bf Cluster Discrimination}\nis composed of two primary stages: the clustering process and the discrimination process.\nDuring the clustering phase, every instance is assigned one pseudo-class label. This label is later employed as a guiding factor for training a classifier in the subsequent discrimination phase. \nFor the normalized embedding feature $e_i = f(x_i)$,\nthe clustering process determines a centroid matrix $W \\in \\mathbb{R}^{d\\times k}$ and assigns the cluster label $y_i$ for each image $x_i$. This is achieved by\n\\vspace{-2mm}\n\\begin{equation}\n\\label{eq:kmeans}\n\\min_{W \\in \\mathbb{R}^{d\\times k}}\n\\frac{1}{n}\n\\sum_{i=1}^n\n\\min_{y_i \\in \\{0,1\\}^{k}}\n\\| e_i  -  W y_i \\|_2^2\n\\quad\n\\text{s.t.}\n\\quad\ny_i^\\top \\bf{1}_k = 1,\n\\end{equation}\nwhere $n$ is the number of training samples, $e_i$ is the normalized feature embedding obtained by using the image encoder $f$, and\nthe centroid $w_i$ belonging to centroid matrix $W\\in {R}^{d\\times k}$ is considered the normalized prototype of $i$-th cluster. $y_i$, falling within the set $\\{0,1\\}^k$, stands as a single label assignment restricted by the condition $y_i^\\top \\bf{1}_k = 1$, where $ \\bf{1}_k $ is 1-vector with a length of $k$. \n\nThen, the training data, denoted as $\\{x_i\\}_{i=1}^n$, is divided into $k$ classes represented by prototypes $W=\\{w_i\\}_{i=1}^k$. \nUtilizing the pseudo labels and centroids derived from the clustering phase, the process of cluster discrimination can be executed by minimizing a conventional softmax classification loss, formulated as:\n\\vspace{-2mm}\n\\begin{align}\n\\label{eqn:cd}\n \\mathcal{L}_\\mathrm{CD} \n& = - \\log \\frac{\\exp( w_{y_i}^T e_i)}{\\sum_{j=1}^{k} \\exp(w_j^Te_i)} \n   = - \\log \\frac{\\exp({s_i})}{\\sum_{j=1}^{k} \\exp({s_j})} \\notag \\\\ \n&  = \\log (1 + \\sum_{j=1, j \\neq i}^{k} \\exp(s_j-s_i)), \n\\end{align}\nwhere $e_i$ is the normalized embedding corresponding to the image $x_i$, and $x_i$ is categorized under the class symbolized by the normalized prototype $w_{y_i}$. For a more straightforward representation, we define the intra-class similarity $w_{y_i}^Te_i$, and the inter-class similarity, $w_j^Te_i$ as $s_i$ and $s_j$, respectively. \nBased on Eq.~\\ref{eqn:cd}, in the discrimination phase that employs classification, $s_j$ and $s_i$ are paired to optimize the reduction of the difference ($s_j-s_i$). As depicted in Fig.~\\ref{fig1:cluster}, the cluster discrimination based UNICOM model~\\cite{an2023unicom} trains an image encoder to predict the one-hot pseudo label for each image from a batch of training examples.\n\n\\definecolor{color1}{RGB}{182,0,76}\n\\definecolor{color2}{RGB}{0,106,91}\n\n\\begin{figure}[!t]\n\\centering\n\\includegraphics[width=0.95\\textwidth]{figs/multilabel.png}\n\\caption{Illustration of the multiple visual elements (\\eg, objects or attributes) in images from the automatically clustered LAION-400M dataset.}\n\\label{fig:pruityconflict}\n\\vspace{-4mm}\n\\end{figure}\n\n\\subsection{Multi-label Cluster Discrimination} \n\n\\noindent\\textbf{Clustering.}\nConsidering the time consumption of iterative clustering and discrimination~\\cite{caron2018deep}, An \\etal~\\cite{an2023unicom} implemented a single step of offline clustering with the aid of the pre-trained CLIP model (\\ie, ViT-L/14) and efficient feature quantization~\\cite{johnson2019billion}. On the large-scale LAION-400M dataset, it only takes around 10 minutes to cluster one million classes. Despite the straightforwardness of the clustering step, the automatically clustered large-scale dataset inevitably confronts intra-class purity and inter-class conflict problems due to the specific definition of class granularity.\n\nIn the realm of clustering algorithms, there often exists a trade-off between maintaining high within-class purity and ensuring low inter-class conflict. In the context of contrastive learning, the issue of inter-class conflict can be significantly alleviated by reducing the number of sampled negative instances within the mini-batch and adopting a suitable semi-hard mining technique. In this paper, we follow UNICOM~\\cite{an2023unicom} to prioritize intra-class purity (\\ie, clustering one million level classes from 400 million images) and employ margin-based PatialFC~\\cite{an2022killing,deng2019arcface} to alleviate inter-class conflict (\\ie, randomly sampling part of the negative class centers during each iteration). \n\n\\begin{figure}[!t]\n\\centering\n  \\begin{subfigure}{0.24\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figs/MLC_MCLD_02.pdf}\n    \\caption{$s_i$ of MLC} \n    \\label{cosine:mlc}\n  \\end{subfigure}\n  \\begin{subfigure}{0.24\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figs/MLC_MCLD_01.pdf}\n    \\caption{$s_i$ of MLCD}\n    \\label{cosine:mlcd}\n  \\end{subfigure}\n  \\begin{subfigure}{0.24\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figs/pos_cosine.png}\n    \\caption{Mean $s_i$}\n    \\label{cosine:si}\n  \\end{subfigure}\n  \\begin{subfigure}{0.24\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figs/neg_cosine.png}\n    \\caption{Mean $s_j$}\n    \\label{cosine:sj}\n  \\end{subfigure}\n  \\caption{Intra-class and inter-class similarity score comparisons between MLC and MLCD. Here, MLC and MLCD are trained on the LAION-400M dataset with the ViT-B/32 as the backbone and a batch size of 32K. (a) and (b) showcase histograms that compare the distributions of positive cosine similarities $\\{s_i\\}$ between MLC and MLCD, with MLCD clearly showing tighter sample alignment to positive class centers. (c) demonstrates that MLCD consistently achieves higher mean positive cosine values than MLC over iterations, indicating enhanced intra-class compactness. (d) demonstrates MLCD’s effectiveness in reducing mean negative cosine values compared to MLC, which indicates a more orthogonal relationship between samples and their negative class centers. This greater orthogonality facilitated by MLCD contributes to enhanced class separability. These figures highlight MLCD’s advanced capability in refining feature spaces for more distinct representation compared to MLC.}\n  \\label{fig:mlc_vs_mlcd}\n  \\vspace{-4mm}\n\\end{figure}\n\n\\noindent{\\bf Multi-label Classification.}\nAs illustrated in Fig.~\\ref{fig:pruityconflict}, a single image can encompass several visual components (\\eg, objects or attributes). This implies that the single-class label may not cover all visual cues present in the image. \nTo consider the different granularities of visual information\nfor each sample, we perform a similarity-based sorting against one million class centers, selecting the top $l$ class centers as the positive class centers for that sample. \nDuring training, this sample will be directed to move closer to these $l$ positive class centers, while simultaneously distancing from the other $k-l$ negative class centers. \nAs shown in Fig.~\\ref{fig1:multicluster}, our method assigns multiple class labels to each training example, capturing different granularities of visual signals in one image.\n\nThe corresponding similarity scores are represented as $\\{s_i\\}\\, (i=1,2,\\cdots,l)$ and $\\{s_j\\}\\,(j=1,2,\\cdots,k-l)$, respectively. \nTo minimize each $s_j$ ($ \\forall j \\in \\{1,2,\\cdots,k-l\\}$) as well as to maximize $s_i$ ($\\forall i\\in \\{1,2,\\cdots,l\\}$), we employ a multi-label classification strategy~\\cite{li2017improving,sun2020circle}. This is achieved by\n\\vspace{-2mm}\n\\begin{align}\n\\label{eqn:mlc}\n\\small\n\\mathcal{L}_\\mathrm{MLC}  =  \\log(1+ \\underbrace{\\sum_{j=1}^{k-l} \\sum_{i=1}^l \\exp(s_j-s_i))}_{contrastive}) \n = \\log(1+\\underbrace{\\sum_{j \\in \\Omega_{n}}\\exp({s_j})\\sum_{i \\in \\Omega_{p}}\\exp({-s_i})}_{contrastive}), \n\\end{align}\nwhere $\\Omega_{n}$ and $\\Omega_{p}$ denote the negative and positive class set to simplify the representation. Eq.~\\ref{eqn:mlc} iterates through every similarity pair to reduce $(s_j-s_i)$. To alleviate inter-class conflict as in~\\cite{an2022killing,an2023unicom}, we also employ negative class sampling into Eq.~\\ref{eqn:mlc}. Therefore, the loss is changed from $\\log(1+\\sum_{j \\in \\Omega_{n}}\\exp({s_j})\\sum_{i \\in \\Omega_{p}}\\exp({-s_i}))$ to $\\log(1+\\sum_{j \\in \\Omega'_{n}}\\exp({s_j})\\sum_{i \\in \\Omega_{p}}\\exp({-s_i}))$, where $\\left |\\Omega'_{n}  \\right |= \\left |\\Omega_{n}  \\right |*r$, and $r \\in\\left [0,1 \\right ]$ is the negative class sampling ratio. $\\Omega'_{n}$ is a subset of $\\Omega_n$ that is randomly sampled during each loss calculation step.\n\n\\noindent{\\bf Multi-label Classification Disambiguation.}\nOptimizing $(s_j-s_i)$ usually leads to a decision boundary of $s_j-s_i=m$ ($m$ is the margin). \nHowever, this decision boundary allows ambiguity as indicated in Circle loss~\\cite{sun2020circle}. \nFor example, $\\{s_j,s_i\\}=\\{0.1,0.4\\}$ and $\\{s'_j,s'_i\\}=\\{0.5,0.8\\}$ both achieve the margin $m=0.3$. However, the gap between $s_i$ and $s'_j$ is only $0.1$, compromising the separability of the feature space. \n\nAs we expect to maximize the within-class similarity $s_i$ and to minimize the between-class similarity $s_j$, we further introduce these two items into the multi-label classification loss:\n\\vspace{-2mm}\n\\begin{align}\n\\label{eqn:mlc2}\n\\mathcal{L}_\\mathrm{MLCD}\n& = \\log(1+\\underbrace{\\sum_{j\\in\\Omega'_{n}}\\exp({s_j})\\sum_{i\\in\\Omega_{p}}\\exp({-s_i})}_{contrastive}+\\underbrace{\\sum_{j\\in\\Omega'_{n}}\\exp({s_j})}_{negative}+\\underbrace{\\sum_{i\\in\\Omega_{p}}\\exp({-s_i})}_{positive}\n) \\notag \\\\ \n& = \\log(1+\\sum_{i \\in \\Omega_{p}}\\exp({-s_i}))+ \\log(1+\\sum_{j \\in \\Omega'_{n}}\\exp({s_j})), \n\\end{align}\nwhere $\\Omega_p$ symbolizes the collection of positive class labels for each sample, $s_i$ encapsulates the score associated with each positive class, $\\Omega'_n$ denotes the collection of negative class labels for each sample, and $s_j$ corresponds to the score for each negative class.\nIn Eq.~\\ref{eqn:mlc2}, loss from positive class labels $\\log(1+\\sum_{i \\in \\Omega_{p}}\\exp({-s_i}))$ and loss from negative class labels $\\log(1+\\sum_{j \\in \\Omega'_{n}}\\exp({s_j}))$ are elegantly separated.\nIn Fig.~\\ref{cosine:mlc} and Fig.~\\ref{cosine:mlcd}, we compare the dynamic distributions of $s_i$ of MLC (Eq.~\\ref{eqn:mlc}) and MLCD (Eq.~\\ref{eqn:mlc2}) during training steps. Besides, Fig.~\\ref{cosine:si} illustrates the average $s_i$ from MLC and MLCD during training. As we can see, the item designed for maximizing the within-class similarity $s_i$ in Eq.~\\ref{eqn:mlc2} can significantly increase the intra-class cosine similarities, enhancing the intra-class compactness. In Fig.~\\ref{cosine:sj}, the item designed for minimizing the between-class similarity $s_j$ can effectively suppress the inter-class cosine similarities, enforcing the inter-class discrepancy.\n\n\\section{Experiments}\n\\subsection{Experimental Setting}\nOur models are pre-trained on the LAION-400M dataset~\\cite{schuhmann2021laion} with the same model configurations as CLIP. The training process consists of 32 epochs, utilizing a batch size of 32K on 80 NVIDIA A100 GPUs. To expedite the training, we employ mixed-precision computation~\\cite{micikevicius2017mixed} and flash attention~\\cite{dao2023flashattention2}, while leveraging the DALI library for efficient data loading and pre-processing. We use the AdamW optimizer with a learning rate of $0.001$ and weight decay of $0.2$. To assess the performance of zero-shot classification and zero-shot image-text retrieval tasks, we employ contrastive learning to train a text encoder from scratch for 32 epochs with a frozen image encoder following Locked-image Tuning (LiT)~\\cite{zhai2022lit}. The structure of the text encoder is also identical to CLIP. In the following experiments, unless otherwise specified, the model used is ViT-L/14, the number of classes ($k$) is one million, the ratio of sampled negative class centers ($r$) is $0.1$, and the number of positive labels ($l$) assigned to each image is $8$.\n\n\\subsection{Linear Probe}\n\nFollowing the same evaluation setting as CLIP, we report the linear probe performance of our method on 26 datasets. As depicted in Tab.~\\ref{tab:linear-probe-big-table}, inherent biases exist in different pre-training data. The WIT dataset is beneficial for action-related datasets (\\eg, Kinetics700, UCF101), while LAION exhibits superior proficiency in object datasets (\\eg, Cars, Birdsnap). Nevertheless, our method still achieves an average improvement of 1.1\\% compared to CLIP. \nTo isolate the confounding effects of pre-training data, we compare our model with OPENCLIP and UNICOM by using the LAION-400M dataset as the training data.\nAs shown in Fig.~\\ref{fig:openclip_linear}, our method outperforms OPENCLIP on 25 datasets, demonstrating an average improvement of 2.3\\%. \nIn Fig.~\\ref{fig:unicom_linear},\nour model surpasses UNICOM on 23 datasets and achieves an average improvement of 1.3\\%, confirming the effectiveness of the proposed multi-label loss.\n\n\\subsection{Zero-shot Classification}\n\nIn Tab.~\\ref{tab:zeroshot}, we present a comparison of our method with state-of-the-art approaches in zero-shot classification on 25 datasets. The prompt templates and class names are consistent with previous works~\\cite{li2023scaling}. As depicted in Fig.~\\ref{fig:openclip_zeroshot}, our method surpasses OpenCLIP on 23 datasets with 3.9\\% average performance improvement. Although FLIP uses masking to save memory footprint to learn more samples per iteration, our method demonstrates better results on 15 out of 25 datasets in Tab.~\\ref{tab:zeroshot} and achieves a significant performance boost of 1.5\\% on average.\n\n\\begin{table}[t]\n    \\centering\n    \\caption{Linear probe performance of various pre-trained models on 26 datasets. $\\dag$: Results reported in CLIP paper. $\\ddag$: Results we reproduced. Entries in green are the best results using LAION-400M. Here, all methods employ the same backbone of ViT-L/14.}\n    \\vspace{-4mm}\n    \\label{tab:linear-probe-big-table}\n    \\resizebox{1.0\\linewidth}{!}{\n        \\tablestyle{0.2mm}{1.1}\n        \\begin{tabular}{*l^l|^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c}\n            CASE & DATA & \\datatag{Food101} & \\datatag{{CIFAR10}} & \\datatag{{CIFAR100}} & \\datatag{{Birdsnap}} & \\datatag{{SUN397}} & \\datatag{{Cars}} & \\datatag{{Aircraft}} & \\datatag{{VOC2007}} & \\datatag{{DTD}} & \\datatag{{Pets}} & \\datatag{{Cal101}} & \\datatag{{Flowers}} & \\datatag{{MNIST}} & \\datatag{{FER2013}} & \\datatag{{STL10}} & \\datatag{{EuroSAT}} & \\datatag{{RESISC45}} & \\datatag{{GTSRB}} & \\datatag{{KITTI}} & \\datatag{{Country211}} & \\datatag{{PCAM}} & \\datatag{{UCF101}} & \\datatag{{K700}} & \\datatag{{CLEVR}} & \\datatag{{HM}} & \\datatag{{SST}} & \\datatag{{AVG}}\\\\\n            \\midrule\n\n            \\rowstyle{\\color{dt}} CLIP$^{\\dag}$&WIT-400M& 95.2 &98.0 &87.5 &77.0 &81.8 &90.9 &69.4 &89.6 &82.1 &95.1 &96.5 &99.2 &99.2 &72.2 &99.8 &98.2 &94.1 &92.5 &64.7 &42.9 &85.8 &91.5 &72.0 &57.8 &76.2 &80.8 &84.2 \\\\\n            \\rowstyle{\\color{dt}} CLIP$^{\\ddag}$&WIT-400M&95.3 &98.1 &87.2 &77.8 &81.5 &90.7 &68.0 &89.7 &80.9 &94.9 &96.0 &99.2 &99.2 &72.3 &99.8 &96.7 &94.5 &92.9 &65.9 &41.9 &85.3 &91.0 &70.6 &59.6 &61.8 &79.8 &83.5 \\\\\n            OPNCLIP$^{\\ddag}$&LAION-400M&93.3 &97.9 &87.9 &78.0 &81.0 &93.6 &64.4 &91.7 &83.0 &93.3 &95.5 &98.8 &99.2 &66.5 &99.2 &97.1 &92.4 &92.5 &77.5 &32.5 &84.3 &88.1 &64.0 &59.8 &57.6 &71.9 &82.3 \\\\\n            UNICOM&LAION-400M&93.4 &98.5 &90.8 &82.4 &80.0 &94.6 &74.5 &91.4 &82.2 &94.2 &95.7 &\\better{99.3} &99.2 &68.7 &98.5 &96.7 &92.6 &\\better{92.7} &77.8 &33.4 &85.4 &87.4 &66.7 &\\better{60.3} &57.4 &72.4 &83.3 \\\\\n            Ours&LAION-400M&\\better{94.3} &\\better{98.9} &\\better{92.0} &\\better{83.4} &\\better{82.1} &\\better{94.8} &\\better{79.6} &\\better{92.5} &\\better{84.6} &\\better{95.3} &\\better{97.2} &\\better{99.3} &\\better{99.3} &\\better{72.4} &\\better{99.3} &\\better{99.1} &\\better{94.7} &92.5 &\\better{78.2} &\\better{34.5} &\\better{86.0} &\\better{90.0} &\\better{68.5} &60.1 &\\better{57.9} &\\better{73.4} &\\better{84.6} \\\\\n            \\bottomrule\n        \\end{tabular}\n    }\n\\end{table}\n\n\\begin{table}[t!]\n    \\caption{Zero-shot classification performance on 25 datasets. $\\dag$: Results reported in CLIP paper. $\\ddag$: Results reported in FLIP paper. Entries in green are the best results using LAION-400M. Here, all methods employ the same backbone of ViT-L/14.}\n    \\label{tab:zeroshot}\n    \\vspace{-4mm} \n    \\centering\n    \\resizebox{1.0\\linewidth}{!}{\n        \\tablestyle{0.2mm}{1.1}\n        \\begin{tabular}{*l^l|^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c}\n            CASE & DATA & \\datatag{Food101} & \\datatag{{CIFAR10}} & \\datatag{{CIFAR100}} & \\datatag{{Birdsnap}} & \\datatag{{SUN397}} & \\datatag{{Cars}} & \\datatag{{Aircraft}} & \\datatag{{VOC2007}} & \\datatag{{DTD}} & \\datatag{{Pets}} & \\datatag{{Cal101}} & \\datatag{{Flowers}} & \\datatag{{MNIST}} & \\datatag{{STL10}} & \\datatag{{EuroSAT}} & \\datatag{{RESISC45}} & \\datatag{{GTSRB}} & \\datatag{{KITTI}} & \\datatag{{Country211}} & \\datatag{{PCAM}} & \\datatag{{UCF101}} & \\datatag{{K700}} & \\datatag{{CLEVR}} & \\datatag{{HM}} & \\datatag{{SST}} & \\datatag{{AVG}} \\\\\n            \\midrule\n\n            \\rowstyle{\\color{dt}}CLIP$^{\\dag}$&WIT-400M&92.9 &96.2 &77.9 &48.3 &67.7 &77.3 &36.1 &84.1 &55.3 &93.5 &92.6 &78.7 &87.2 &99.3 &59.9 &71.6 &50.3 &23.1 &32.7 &58.8 &76.2 &60.3 &24.3 &63.3 &64.0 &66.9 \\\\\n            \\rowstyle{\\color{dt}}CLIP$^{\\ddag}$&WIT-400M&91.0 &95.2 &75.6 &51.2 &66.6 &75.0 &32.3 &83.3 &55.0 &93.6 &92.4 &77.7 &76.0 &99.3 &62.0 &71.6 &51.6 &26.9 &30.9 &51.6 &76.1 &59.5 &22.2 &55.3 &67.3 &65.6 \\\\\n            OpenCLIP$^{\\ddag}$&LAION-400M&87.4&94.1 &77.1 &61.3 &70.7 &86.2 &21.8 &83.5 &54.9 &90.8 &\\better{94.0} &72.1 &71.5 &98.2 &53.3 &67.7 &47.3 &29.3 &21.6 &51.1 &71.3 &50.5 &22.0 &55.3 &57.1 &63.6 \\\\\n            FLIP$^{\\ddag}$&LAION-400M&89.3&\\better{97.2} &\\better{84.1} &\\better{63.0} &\\better{73.1} &\\better{90.7} &29.1 &83.1 &60.4 &92.6 &93.8 &75.0 &\\better{80.3} &98.5&53.5 &\\better{70.8} &41.4 &\\better{34.8} &23.1 &50.3 &74.1 &55.8 &\\better{22.7} &54.0 &\\better{58.5} &66.0 \\\\\n            Ours&LAION-400M&\\better{90.3} &95.3 &83.7 &62.9 &72.1 &90.1 &\\better{39.4} &\\better{84.5} &\\better{62.3} &\\better{93.7} &93.9 &\\better{79.4} &78.5 &\\better{99.1} &\\better{59.7} &69.9 &\\better{50.7} &28.7 &\\better{27.9} &\\better{53.7} &\\better{75.7} &\\better{57.7} &22.2 &\\better{58.4} &57.9 &\\better{67.5} \\\\\n\n            \\bottomrule\n        \\end{tabular}} \n\\end{table}\n\n\\begin{figure}[t!]\n\\centering\n\\begin{subfigure}[b]{0.328\\textwidth}\n  \\centering\n  \\includegraphics[width=\\textwidth]{figs/linear_OpenCLIP_tiny.png}\n  \\captionsetup{font=tiny} \n  \\caption{MLCD vs. OpenCLIP on linear}\n  \\label{fig:openclip_linear}\n\\end{subfigure}\n\\hfill\n\\begin{subfigure}[b]{0.328\\textwidth}\n  \\centering\n  \\includegraphics[width=\\textwidth]{figs/zeroshot_OpenCLIP_tiny.png}\n  \\captionsetup{font=tiny} \n  \\caption{MLCD vs. OpenCLIP on zero-shot}\n  \\label{fig:openclip_zeroshot}\n\\end{subfigure}\n\\hfill\n\\begin{subfigure}[b]{0.328\\textwidth}\n  \\centering\n  \\includegraphics[width=\\textwidth]{figs/linear_Unicom_tiny.png}\n  \\captionsetup{font=tiny} \n  \\caption{MLCD vs. UNICOM on linear}\n  \\label{fig:unicom_linear}\n\\end{subfigure}\n\\caption{Linear probe and zero-shot comparisons on different downstream datasets. The Y-axis shows the performance difference. Green bars indicate our model outperforms the baselines, while the orange bars depict our model is surpassed by the baselines.}\n\\label{fig:fig_histogram}\n\\vspace{-4mm}\n\\end{figure}\n\n\\subsection{Zero-shot Retrieval}\n\nTab.~\\ref{tab:retrieval} reports zero-shot image-text retrieval results on Flickr30k and MSCOCO. In comparison to OpenCLIP, our model achieves 60.8\\%/44.5\\% I2T/T2I retrieval Recall@1 on the MSCOCO dataset, which is 2.8\\%/3.2\\% higher than OpenCLIP. Similarly, our model demonstrates significant improvements of 1.8\\%/3.9\\% on the Flickr30k dataset. Furthermore, compared to FLIP, our model exhibits either competitive or superior retrieval performance.\n\n\\begin{table}[t]\n    \\centering\n    \\caption{Zero-shot image-text retrieval on the test splits of Flickr30k and MSCOCO. $\\ddag$: Results reported in FLIP paper. Entries in green are the best results using LAION-400M. Here, all methods employ the same backbone of ViT-L/14.}\n    \\label{tab:retrieval}\n    \\vspace{-4mm}\n    \\resizebox{0.85\\linewidth}{!}{\n        \\tablestyle{3pt}{1.05}\n        \\begin{tabular}{*l^l^c^c^c^c^c^c^c^c^c^c^c^c}\n            \\toprule\n             &  & \\multicolumn{6}{c}{Text retrieval} & \\multicolumn{6}{c}{Image retrieval} \\\\\n             &  & \\multicolumn{3}{c}{Flickr30k} & \\multicolumn{3}{c}{MSCOCO} & \\multicolumn{3}{c}{Flickr30k} & \\multicolumn{3}{c}{MSCOCO} \\\\\n            CASE & DATA & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\\\ \\midrule\n\n            \\rowstyle{\\color{dt}} CLIP$^{\\ddag}$ & WIT-400M & 87.8 & 99.1 & 99.8 & 56.2 & 79.8 & 86.4 & 69.3 & 90.2 & 94.0 & 35.8 & 60.7 & 70.7 \\\\\n            OpenCLIP$^{\\ddag}$ & LAION-400M & 87.3 & 97.9 & 99.1 & 58.0 & 80.6 & 88.1 & 72.0 & 90.8 & 95.0 & 41.3 & 66.6 & 76.1 \\\\\n            FLIP$^{\\ddag}$     & LAION-400M & \\better{89.1} & \\better{98.5} & \\better{99.6} & 60.2 & 82.6 & 89.9 & 75.4 & 92.5 & 95.9 & 44.2 & 69.2 & 78.4 \\\\\n            Ours               & LAION-400M & \\better{89.1} & 98.4 & 99.5 & \\better{60.8} & \\better{83.2} & \\better{91.3} & \\better{75.9} & \\better{93.1} & \\better{96.8} & \\better{44.5} & \\better{69.6} & \\better{79.9} \\\\\n\n            \\bottomrule\n        \\end{tabular}}\n\n\\end{table}\n\\setlength{\\fboxsep}{1.5pt}\n\n\\begin{table}[t]\n    \\caption{ImageNet results under finetuning, linear probe, zero-shot, and zero-shot robustness evaluation settings. $\\ddag$: Results reported in FLIP paper. Entries in green are the best results using LAION-400M. Here, all methods employ the same backbone of ViT-L/14.}    \n    \\label{tab:imagenet_robust}\n    \\vspace{-4mm}\n    \\centering\n    \\resizebox{1.0\\linewidth}{!}{\n        \\tablestyle{4pt}{1.05}\n        \\begin{tabular}{*l^l|^c^c^c|^c^c^c^c^c }\n        \\toprule\n            CASE & DATA & Finetune  & Linear Probe  &  Zero Shot & IN-V2 & IN-A & IN-R & ObjectNet & IN-Sketch \\\\\n            \\midrule\n            \\rowstyle{\\color{dt}}CLIP$^{\\ddag}$ & WIT-400M & -       & 83.9        & 75.3 & 69.5 & 71.9 & 86.8 & 68.6 & 58.5 \\\\\n            OpenCLIP$^{\\ddag}$ & LAION-400M & 86.2 &  82.1 &   72.8  & 64.0 & 48.3 & 84.3 & 58.8 & 56.9 \\\\\n            FLIP$^{\\ddag}$ & LAION-400M & -      & -       & 74.6 & 66.8 & 51.2 & \\better{86.5} & 59.1 & 59.9 \\\\\n            \\midrule\n            Ours & LAION-400M & \\better{87.1}  & \\better{84.6}     &\\better{75.6} & \\better{68.9} & \\better{56.4} & 85.1 & \\better{62.7} & \\better{60.4} \\\\\n        \\bottomrule\n        \\end{tabular}}\n\\end{table}\n\n\\subsection{ImageNet Classification and Robustness Evaluation}\n\nWe evaluate performance on ImageNet~\\cite{deng2009imagenet} under three distinct settings: finetuning, linear probe, and zero-shot. As shown in Tab.~\\ref{tab:imagenet_robust}, our ViT-L/14 model achieves better performance on all settings, outperforming OpenCLIP by $0.9\\%$ under the finetuning setting, and surpassing FLIP by $1.0\\%$ under the zero-shot setting. These improvements indicate that multi-label cluster discrimination can better encode the semantics of data than instance discrimination. Following FLIP~\\cite{li2023scaling}, we conduct a robustness evaluation as shown in Tab.~\\ref{tab:imagenet_robust}. In comparison to the models pre-trained on LAION, our method demonstrates superior robustness compared to both OpenCLIP and FLIP. It is worth noting that the performance gap between our model pre-trained on LAION and CLIP pre-trained on WIT arises from the statistical differences in pre-training data.\n\n\\subsection{Ablation Study}\n\n\\begin{table}[t]\n    \\caption{\\textbf{Ablation experiments}. The model backbone used here is ViT-B/32. Pre-training is executed on the LAION-400M dataset for a duration of 5 epochs. Performance assessment is undertaken using a linear probe on the ImageNet validation set.}\n    \\vspace{-2mm}\n    \\label{tab:ablationpara}\n    \\tablestyle{3pt}{1.05}\n    \\begin{subtable}{.495\\textwidth}\n        \\resizebox{\\linewidth}{!}{\n            \\begin{tabular}{y{80}|x{25}x{25}x{25}x{25}x{25}x{25}}\n            \\toprule\n                 Num Classes & 100K  & 200K  & 500K  & 1M            & 2M   & 5M   \\\\\n                \\midrule\n                IN1K       & 66.9 & 71.1 & 74.4 & \\better{75.2} & 74.9 & 74.7 \\\\\n            \\bottomrule\n            \\end{tabular}}\n        \\caption{The number of \\textbf{classes} in training set.}\n        \\label{tab:abl1}\n    \\end{subtable}%\n    \\hfill\n    \\begin{subtable}{.495\\textwidth}\n        \\resizebox{\\linewidth}{!}{\n            \\begin{tabular}{y{80}|x{25}x{25}x{25}x{25}x{25}x{25}}\n            \\toprule\n                Sampling Ratio & 0.01 & 0.05 & 0.1           & 0.2  & 0.5  & 1.0  \\\\\n                \\midrule\n                IN1K         & 73.4 & 75.1 & \\better{75.2} & 74.9 & 68.3 & 63.2 \\\\\n            \\bottomrule\n            \\end{tabular}}\n        \\caption{The \\textbf{ratio} of negative class centers.}\n        \\label{tab:abl2}\n    \\end{subtable}%\n    \\vspace{-0.1cm}\n    \\begin{subtable}{.495\\textwidth}\n        \\resizebox{\\linewidth}{!}{\n            \\begin{tabular}{y{80}|x{25}x{25}x{25}x{25}x{25}x{25}}\n            \\toprule\n                Positive Centers & 1    & 2    & 4    & 8             & 16   & 32   \\\\\n                \\midrule\n                IN1K             & 71.4 & 72.9 & 73.2 & \\better{75.2} & 72.1 & 68.7 \\\\\n                \\bottomrule\n            \\end{tabular}\n        }\n        \\caption{The effect of \\textbf{multi labels} per sample.}\n        \\label{tab:abl3}\n    \\end{subtable}%\n    \\hfill\n    \\begin{subtable}{.495\\textwidth}\n        \\resizebox{\\linewidth}{!}{\n            \\begin{tabular}{y{80}|x{25}x{25}x{25}x{25}x{25}x{25}}\n                \\toprule\n                Positive Threshold & 0.95 & 0.93 & 0.91          & 0.89 & 0.87 & 0.85 \\\\\n                \\midrule\n                IN1K               & 72.2 & 72.7 & \\better{73.3} & 72.4 & 68.7 & 63.2 \\\\\n                \\bottomrule\n            \\end{tabular}\n        }\n        \\caption{The effect of \\textbf{positive thresholds}.}\n        \\label{tab:abl4}\n    \\end{subtable}%\n\n\\end{table}\n\n\\noindent{\\bf Number of Classes.}\nThe number of classes ($k$) plays a crucial role in balancing inter-class conflict and intra-class purity. In Tab.~\\ref{tab:abl1}, we observe that as the number of classes increases from 100K to 1M, there is a gradual increase in intra-class purity, leading to an improved performance on ImageNet. However, as the number of classes continues to increase from 1M to 5M, inter-class conflicts gradually escalate, resulting in a deteriorated performance. \n\n\\noindent{\\bf Inter-class sampling Ratio.}\nThe inter-class sampling ratio ($r$) influences the number of negative samples and directly affects the likelihood of encountering inter-class conflicts. A sample ratio of $0.01$ yields a linear probe performance of only $73.4\\%$ due to the limited number of negative samples, which adversely affects the representation learning. Conversely, a sample ratio of $1.0$ substantially increases the probability of encountering inter-class conflicts. Tab.~\\ref{tab:abl2} presents that the superior linear probe performance of $75.2\\%$ is achieved when employing a sample ratio of $0.1$.\n\n\\noindent{\\bf Multi-label Assignment.}\nWe explore two different approaches to obtain multi-labels. Firstly, we artificially assign a predetermined number of labels to each sample. Tab.~\\ref{tab:abl3} presents linear probe results on ImageNet with different numbers of positive centers. Consequently, we observe a gradual improvement in performance as the number of positive centers increases from 1 to 8. However, as the number of positive centers continues to increase, the inclusion of excessive positive centers introduces noise labels, leading to a degradation in performance.\nAdditionally, we have also investigated the use of sample-cluster similarity thresholds to obtain multiple labels. This approach results in varying numbers of positive centers associated with each sample. However, as shown in Tab.~\\ref{tab:abl4}, the performance of applying adaptive positive centers is generally lower compared to that of using fixed assignment of positive centers (Tab.~\\ref{tab:abl3}). This indicates that the global similarity threshold is hard to search while the fixed assignment strategy benefits from the prior that the daily image statistically contains several visual concepts.\n \n\\noindent{\\bf Effectiveness of MLCD Compared to MLC.}\nIn Tab.~\\ref{tab:mlcvsmlcd}, we compare the performance of the vanilla MLC (Eq.~\\ref{eqn:mlc}) and the proposed MLCD (Eq.~\\ref{eqn:mlc2}) on the ImageNet. Both MLC and MLCD employ the negative class center sampling with a ratio of $0.1$. MLCD outperforms MLC in all three settings, confirming the effectiveness of the two additional optimization targets. \n\n\\begin{table}[t!]\n    \\caption{Ablation experiments of the proposed contrastive loss decomposition. Pre-training is executed on the LAION-400M dataset by 32 epochs. The model backbone used here is ViT-B/32. Results are reported on the ImageNet validation dataset.}\n    \\label{tab:mlcvsmlcd}\n    \\vspace{-2mm}\n    \\centering\n    \\resizebox{0.60\\linewidth}{!}{\n        \\tablestyle{4pt}{1.05}\n        \\begin{tabular}{y{24}y{60}|ccc}  \n        \\toprule\n            CASE & DATA &  Finetune & Linear Probe & Zero Shot \\\\\n            \\midrule\n            MLC  &  LAION-400M & 80.9 & 76.9 & 63.9 \\\\\n            MLCD &  LAION-400M & \\better{81.2} & \\better{78.1} & \\better{64.5}  \\\\  \n            \\bottomrule\n        \\end{tabular}}\n\n\\end{table}\n\n\\noindent{\\bf Scalability.}\nIn Fig.~\\ref{scale:0} and Fig.~\\ref{scale:1}, we validate the scalability of our method. Scaling up the ViT model and incorporating more data can significantly enhance our model's performance.\n\n\\begin{figure}[!t] \n\\centering\n  \\begin{subfigure}[b]{0.35\\textwidth} \n    \\centering \n    \\includegraphics[width=\\textwidth]{figs/scale_01.png}\n    \\caption{Performance vs. Epochs}\n    \\label{scale:0}\n  \\end{subfigure}\n  \\hspace{1cm} \n  \\begin{subfigure}[b]{0.35\\textwidth} \n    \\centering \n    \\includegraphics[width=\\textwidth]{figs/scale_02.png}\n    \\caption{Performance vs. Num examples}\n    \\label{scale:1}\n  \\end{subfigure}\n  \\caption{(a) The convergence curves of different ViTs. (b) The scalability curves of different ViTs under varying dataset scales. Larger ViTs and datasets lead to better model performance.}\n  \\label{scale}\n\\end{figure}\n\n\\noindent{\\bf Effectiveness of MLCD on Different Training Data.}\nIn Tab.~\\ref{tab:laioncoyo}, we compare the linear probe performance of the proposed multi-label cluster discrimination approach (\\ie, MLCD) and the single-label cluster discrimination method (\\eg, UNICOM) on LAION-400M and COYO-700M. The hyper-parameter settings on COYO-700M follow the best settings on LAION-400M as explored in Tab.~\\ref{tab:ablationpara}. \nAs we can see from the results, the proposed MLCD consistently outperforms UNICOM by $2.2\\%$ and $1.6\\%$ when using LAION-400M and COYO-700M as the training data. In addition, the COYO-700M supports superior performance on action-related evaluation, achieving $3.3\\%$ improvement on Kinetics700 by using MLCD.\n\n\\begin{table}[t]\n    \\caption{Comparisons of linear probe performance across 26 different datasets for models trained on LAION-400M and COYO-700M datasets. Here, all methods employ the same backbone of ViT-B/32.}\n    \\vspace{-4mm}\n    \\label{tab:laioncoyo}\n    \\centering\n    \\resizebox{1.0\\linewidth}{!}{\n        \\tablestyle{0.2mm}{1.1}\n        \\begin{tabular}{*l^l|^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c}\n            CASE & DATA & \\datatag{Food101} & \\datatag{{CIFAR10}} & \\datatag{{CIFAR100}} & \\datatag{{Birdsnap}} & \\datatag{{SUN397}} & \\datatag{{Cars}} & \\datatag{{Aircraft}} & \\datatag{{VOC2007}} & \\datatag{{DTD}} & \\datatag{{Pets}} & \\datatag{{Cal101}} & \\datatag{{Flowers}} & \\datatag{{MNIST}} & \\datatag{{FER2013}} & \\datatag{{STL10}} & \\datatag{{EuroSAT}} & \\datatag{{RESISC45}} & \\datatag{{GTSRB}} & \\datatag{{KITTI}} & \\datatag{{Country211}} & \\datatag{{PCAM}} & \\datatag{{UCF101}} & \\datatag{{K700}} & \\datatag{{CLEVR}} & \\datatag{{HM}} & \\datatag{{SST}} & \\datatag{{AVG}}\\\\\n            \\midrule\n\n            UNICOM&LAION-400M&85.8 &96.8 &86.6 &70.2 &74.6 &93.3 &70.7 &88.3 &78.0 &93.1 &94.6 &98.5 &98.7 &64.3 &97.8 &96.8 &90.6 &90.0 &76.4 &22.5 &82.9 &84.2 &57.2 &52.6 &52.4 &62.1 &79.2 \\\\\n            MLCD&LAION-400M&\\better{87.8} &\\better{97.5} &\\better{88.2} &\\better{72.4} &\\better{77.6} &\\better{93.8} &\\better{71.4} &\\better{91.9} &\\better{80.4} &\\better{93.2} &\\better{96.9} &\\better{98.8} &\\better{99.3} &\\better{66.4} &\\better{98.6} &\\better{98.6} &\\better{92.1} &\\better{90.5} &\\better{77.7} &\\better{30.9} &\\better{83.4} &\\better{86.3} &\\better{60.9} &\\better{54.1} &\\better{57.9} &\\better{70.4} &\\better{81.4} \\\\\n             \\midrule\n            UNICOM&COYO-700M &88.1 &95.4 &85.8 &71.4 &76.6 &93.1 &72.7 &88.1 &81.7 &93.3 &95.6 &97.5 &\\better{99.3} &70.3 &98.7 &97.8 &91.5 &89.9 &76.7 &30.4 &82.1 &86.3 &61.8 &57.4 &64.3 &69.1 &81.3 \\\\\n            MLCD&COYO-700M &\\better{90.2} &\\better{96.9} &\\better{86.8} &\\better{72.1} &\\better{77.4} &\\better{93.5} &\\better{74.7} &\\better{90.4} &\\better{83.5} &\\better{93.6} &\\better{97.7} &\\better{98.8} &\\better{99.3} &\\better{70.9} &\\better{99.1} &\\better{99.0} &\\better{92.7} &\\better{90.1} &\\better{77.5} &\\better{33.7} &\\better{84.4} &\\better{87.5} &\\better{64.2} &\\better{59.2} &\\better{68.4} &\\better{73.4} &\\better{82.9} \\\\\n\n            \\bottomrule\n        \\end{tabular}\n    }\n\n\\end{table}\n\n\\newcolumntype{S}{@{}>{\\lrbox0}l<{\\endlrbox}}  %\n\\definecolor{lightgreen}{HTML}{D8ECD1}\n\n\\noindent{\\bf Effectiveness of MLCD in Vision Language Model.}\nTab.~\\ref{tab:VLM} compares the performance of replacing the vision tower in LLaVA-1.5~\\cite{liu2024llava_V1_5} from the CLIP model with our MLCD model. We validate the effectiveness of our MLCD under both Qwen2-7B and Qwen2-72B~\\cite{qwen1,qwen2} settings across 14 test datasets. To align the experimental settings as in LLaVA-1.5, our model is fine-tuned for one epoch at a resolution of $336\\times336$\nafter training at a resolution of $224\\times224$.\nIt can be observed that our method, MLCD, outperforms CLIP on most of the test datasets. However, there is a noticeable drop in performance on OCR-related benchmarks, such as TextVQA~\\cite{vlm_text_vqa} and AI2D~\\cite{vlm_ai2d}, under both 7B and 72B settings. \nTo this end, we will incorporate additional OCR models for clustering to enhance our OCR capabilities in the future.\n\n\\setlength{\\fboxsep}{3pt}\n\\begin{table}[t]\n\\caption{Evaluation of different visual towers (\\ie, CLIP and MLCD) used in VLM. The evaluation settings and test datasets align with LLaVA-1.5. The MLCD model (ViT-L/14) used here has employed training data from both LAION-400M and COYO-700M.}\n\\vspace{-2mm}\n\\label{tab:VLM}\n\\centering\n\\resizebox{1.0\\linewidth}{!}{\n\\tablestyle{1.6pt}{1.2}\n\\begin{tabular}{*l^c|^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c}\n\\toprule\n\\multirow{2}{*}{LLM}\n & \\multirow{2}{*}{Vision Tower} & \\multirow{1}{*}{VQAv2} & \\multirow{1}{*}{GQA} & \\multirow{1}{*}{VisWiz} & \\multirow{1}{*}{SQA} & \\multirow{1}{*}{TVQA}  & \\multirow{1}{*}{L-Wild} & \\multirow{1}{*}{AI2D} & \\multirow{1}{*}{MathV} & \\multirow{1}{*}{HBI} & \n\\multirow{1}{*}{MMMU} & \\multirow{1}{*}{cMMMU}  & \\multicolumn{2}{c}{MMBench} & \\multicolumn{3}{c}{SEED-Bench} & \\multicolumn{2}{c}{MME} \\\\\n\\cline{3-20}\n& & Val & Eval & Val & Img & Val & Test & Test & Mini  & ALL & Val & Val & EN & CN & All & Img & Vid & Per & Cog\\\\\n\\midrule\nQwen2-7B&CLIP&77.99 &62.66 &\\better{48.58}&72.24 &\\better{48.98}&58.70 &\\better{64.86}&\\better{33.60}&\\better{39.96}&40.70 &\\better{33.70}&72.03 &70.29 &64.25 &69.40 &44.72 &1512 &335 &\\\\\nQwen2-7B&Ours&\\better{78.32}&\\better{63.56}&46.27 &\\better{74.22}&42.52 &\\better{58.90}&62.82 &\\better{33.60}&39.46 &\\better{42.30}&33.10 &\\better{73.88}&\\better{71.47}&\\better{65.79}&\\better{71.05}&\\better{45.89}&\\better{1558}&\\better{384}&\\\\\nQwen2-72B&CLIP&79.47 &63.81 &67.14 &\\better{76.10}&\\better{62.31}&65.41 &\\better{72.41}&38.30 &45.10 &39.70 &37.45 &76.63 &75.39 &66.54 &72.28 &44.71 &1596 &378 &\\\\\nQwen2-72B&Ours&\\better{79.51}&\\better{66.80}&\\better{67.37}&74.69 &57.32 &\\better{66.00}&71.41 &\\better{46.5}&\\better{45.21}&\\better{44.70}&\\better{41.20}&\\better{78.59}&\\better{77.24}&\\better{68.67}&\\better{76.53}&\\better{45.91}&\\better{1633}&\\better{383}&\\\\\n\n\\bottomrule\n\\end{tabular}\n}\n\\end{table}\n\n\\begin{figure}\n\\centering\n  \\includegraphics[width=1.0\\textwidth]{figs/PCAvisualization.png}\n\\caption{PCA visualization of patch features extracted by our MLCD model. We fine-tuned the ViT-L/14 model on LAION-400M for one epoch at the resolution of $448\\times448$, which allows each image to have $32\\times32$ tokens for visualization. \nFor each image, PCA is conducted on the extracted patch features to three principal components, which are subsequently normalized to the range of [0, 255] and mapped into the RGB space. Patches displaying similar colors indicate semantic similarities, reflecting that they embody analogous elements or attributes.}\n\\label{fig:pca}\n\\end{figure}\n\n\\noindent{\\bf Semantic Visualization.}  In Fig.~\\ref{fig:pca}, we show the results of the Principal Component Analysis (PCA) performed on the patch features extracted by our MLCD model. We fine-tune our ViT-L/14 model on the LAION-400M dataset by one epoch using the resolution of $448\\times448$. As the patch size is $14\\times14$, we can obtain $32\\times32\\times1024$ spatial-wise tokens for each image. Then, we build a PCA projection from $32\\times32\\times1024$ to $32\\times32\\times3$. After we threshold\nthe first component, we only keep patches with a positive value. \nAs we can see from Fig.~\\ref{fig:pca}, the unsupervised foreground/background detector, based on detecting the\nhighest variance direction, can separate the salient objects from the background. Afterward, we map the three PCA projection parameters into three different colors (\\ie, [R, G, B]). \nAs shown in Fig.~\\ref{fig:pca}, objects from the same category exhibit color consistency, and objects from different categories present distinguishable colors, which indicates that the proposed multi-label cluster discrimination method can effectively capture multiple semantic signals from one image.\n\n\\section{Conclusions}\nIn this paper, we propose a novel multi-label cluster discrimination method to cope with multiple visual signals existing in one image. Compared to the vanilla version of the multi-label loss, which seeks to narrow the relative gap between inter-class similarities and intra-class similarities, our method introduces another two\noptimization targets (\\ie, decreasing inter-class similarities and increasing intra-class similarities) into the loss function. Introducing these two items enables the elegant separation of losses from positive and negative classes and alleviates the ambiguity on the decision boundary.\nExtensive experimental results show that the proposed multi-label cluster discrimination loss is effective for providing better transferrable features on multiple downstream tasks than both instance and cluster discrimination methods.\n\n\\newpage\n\\appendix\n\\section{Appendix}\n\n\\subsection{Pre-training Details}\n\n\\subsubsection{Encoders}\nTab.~\\ref{tab:vit} shows the architectures used in this paper. The designs follow CLIP~\\cite{radford2021learning}. Our image encoders involve \\mbox{ViT-B} and \\mbox{ViT-L}, using the same patch size as in CLIP.\n\n\\subsubsection{Hyper-parameters}\nOur default pre-training configuration is shown in Tab.~\\ref{hyperparameter}. During the training process of the text encoder, the hyper-parameters are the same as those of the pre-training for the image encoder. The vision model is frozen, preventing any backpropagation of gradients. When calculating the multi-label contrastive loss, we follow the approaches of ArcFace~\\cite{deng2019arcface} and Unicom~\\cite{an2023unicom}, we apply L2 normalization to both the features and the class centers, and introduce a margin ($m=0.3$) for the positive classes.\n\n\\subsubsection{Downstream Datasets}  \nWe use 27 image classification datasets to prove the effectiveness of our method. These datasets\ninclude Food101~\\cite{bossard2014food}, CIFAR10~\\cite{krizhevsky2009learning}, CIFAR100~\\cite{krizhevsky2009learning}, \nBirdsnap~\\cite{berg2014birdsnap},\nSUN397~\\cite{xiao2010sun},\nStanford Cars~\\cite{KrauseStarkDengFei-Fei_3DRR2013},\nFGVC Aircraft~\\cite{maji2013fine},\nVOC2007~\\cite{everingham2007pascal},\nDTD~\\cite{cimpoi2014describing},\nPets~\\cite{parkhi2012cats}, \nCaltech101~\\cite{fei2004learning},\nFlowers102~\\cite{nilsback2008automated},\nMNIST~\\cite{lecun1998gradient},\nFER2013~\\cite{Goodfellow2013ChallengesIR},\nSLT10~\\cite{coates2011analysis},\nEuroSAT~\\cite{helber2019eurosat},\nRESISC45~\\cite{cheng2017remote},\nGTSRB~\\cite{stallkamp2012man},\nKITTI~\\cite{geiger2012we},\nCountry211~\\cite{radford2021learning},\nPCAM~\\cite{veeling2018rotation},\nUCF101~\\cite{soomro2012ucf101},\nKinetics700~\\cite{carreira2019short},\nCLEVR~\\cite{johnson2017clevr},\nHateful Memes~\\cite{kiela2020hateful},\nSST2~\\cite{radford2021learning}, and\nImageNet~\\cite{deng2009imagenet}. Details on each dataset and the corresponding evaluation metrics are provided in Tab.~\\ref{linearprobedatasets}.\n\n\\begin{table}[h]\n\\vspace{2mm}\n\\caption{ViT hyper-parameters.}\n\\vspace{-4mm}\n\\label{tab:vit}\n\\centering\n\\tablestyle{2mm}{1.05}\n\\resizebox{1.0\\linewidth}{!}{\n\\begin{tabular}{l|ccccccccc} \\toprule\n& Learning & Embedding & Input      & \\multicolumn{3}{c}{Vision Transformer} & \\multicolumn{3}{c}{Text Transformer} \\\\\nModel & rate & dimension & resolution & layers & width & heads  & layers & width & heads \\\\ \\midrule\nViT-B/32 & $5 \\times 10^{-4}$ & 512 & 224 & 12 & 768 & 12 & 12 & 512 & 8 \\\\\nViT-B/16 & $5 \\times 10^{-4}$ & 512 & 224 & 12 & 768 & 12 & 12 & 512 & 8 \\\\\nViT-L/14 & $4 \\times 10^{-4}$ & 768 & 224 & 24 & 1024 & 16 & 12 & 768 & 12 \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{table}\n\n\\begin{table}[t]\n\\caption{Training hyper-parameters.}\n\\label{hyperparameter}\n\\centering\n\\tablestyle{5mm}{1.05}\n\\begin{tabular}{l|y{30}} \\toprule\nHyperparameter  & Value \\\\ \\midrule\nBatch size & 32800 \\\\\nVocabulary size & 49408 \\\\\nTraining epochs & 32 \\\\\nMaximum temperature & 100.0 \\\\\nWeight decay & 0.2 \\\\\nWarm-up iterations & 2000 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\\begin{table}[t!]\n        \\caption{List of linear probe datasets with the data distribution and evaluation metrics.}\n    \\label{linearprobedatasets}\n    \\centering\n    \\resizebox{0.99\\linewidth}{!}{\n        \\tablestyle{5mm}{1.1}\n        \\begin{tabular}{llllr}\n            \\toprule\n            \\multicolumn{1}{l}{Dataset} & \\multicolumn{1}{c}{Num Classes} & \\multicolumn{1}{c}{Train size} & \\multicolumn{1}{c}{Test size} & \\multicolumn{1}{c}{Evaluation metric}\\\\\n            \\midrule\n            LAION400M~\\cite{schuhmann2021laion} & 1000000 & 389737314 & - & -\\\\\n            COYO700M~\\cite{kakaobrain2022coyo-700m}  & 1000000 & 686591232 & - & -\\\\\n            \\midrule\n            Food101~\\cite{bossard2014food} & 102 & 75,750 & 25,250 & accuracy\\\\\n            CIFAR10~\\cite{krizhevsky2009learning} & 10 & 50,000 & 10,000 & accuracy\\\\\n            CIFAR100~\\cite{krizhevsky2009learning} & 100 & 50,000 & 10,000 & accuracy\\\\\n            Birdsnap~\\cite{berg2014birdsnap} & 500 & 42,138 & 2,149 & accuracy\\\\\n            SUN397~\\cite{xiao2010sun} & 397 & 19,850 & 19,850 & accuracy\\\\\n            Cars~\\cite{KrauseStarkDengFei-Fei_3DRR2013} & 196 & 8,144 & 8,041 & accuracy\\\\\n            Aircraft~\\cite{maji2013fine} & 100 & 6,667 & 3,333 & mean per class\\\\\n            VOC2007~\\cite{everingham2007pascal} & 20 & 5011 & 4952 & 11-point mAP\\\\\n            DTD~\\cite{cimpoi2014describing} & 47 & 3,760 & 1,880 & accuracy\\\\\n            Pets~\\cite{parkhi2012cats} & 37 & 3,680 & 3,669 & mean per class\\\\\n            Caltech101~\\cite{fei2004learning} & 101 & 3,000 & 5,677 & mean-per-class\\\\\n            Flowers~\\cite{nilsback2008automated} & 102 & 2,040 & 6,149 & mean per class\\\\\n            MNIST~\\cite{lecun1998gradient} & 10 & 60,000 & 10,000 & accuracy\\\\\n            FER2013~\\cite{KrauseStarkDengFei-Fei_3DRR2013} & 8 & 32,140 & 3,574 & accuracy\\\\\n            STL10~\\cite{coates2011analysis} & 10 & 5,000 & 8,000 & accuracy\\\\\n            EuroSAT~\\cite{helber2019eurosat} & 10 & 10,000 & 5,000 & accuracy\\\\\n            RESISC45~\\cite{cheng2017remote} & 45 & 3,150 & 25,200 & accuracy\\\\\n            GTSRB~\\cite{stallkamp2012man} & 43 & 26,640 & 12,630 & accuracy\\\\\n            KITTI~\\cite{geiger2012we} & 4 & 6770 & 711 & accuracy\\\\\n            Country211~\\cite{radford2021learning} & 211 & 42,200 & 21,100 & accuracy\\\\\n            PCAM~\\cite{veeling2018rotation} & 2 & 294,912 & 32,768 & accuracy\\\\\n            UCF101~\\cite{soomro2012ucf101} & 101 & 9,537 & 1,794 & accuracy\\\\\n            Kinetics700~\\cite{carreira2019short} & 700 & 530,779 & 33,944 & mean(top1,top5)\\\\\n            CLEVR~\\cite{johnson2017clevr} & 8 & 2,000 & 500 & accuracy\\\\\n            Memes~\\cite{kiela2020hateful} & 2 & 8,500 & 500 & ROC AUC\\\\\n            SST2~\\cite{radford2021learning} & 2 & 7,792 & 1,821 & accuracy\\\\\n            ImageNet~\\cite{deng2009imagenet} & 1000 & 1,281,167 & 50,000 & accuracy\\\\\n            \\bottomrule\n        \\end{tabular}}\n\\end{table}\n\n\\begin{table}[t!]\n\\caption{Comparisons of linear probe performance across 27 different downstream datasets. Different models (\\ie, UNICOM and MLCD) are trained on the automatically clustered ImageNet dataset with different class numbers. UNICOM employs a single label, while the proposed MLCD employs eight labels for each training sample. All methods use the same ResNet50~\\cite{he2016deep,radford2021learning} architecture as the backbone.}\n\\label{tab:unicom_mlcd}\n\\centering\n\\resizebox{1.0\\linewidth}{!}{\n\\tablestyle{0.6mm}{1.1}\n\\begin{tabular}{*l^l|^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c}\nCASE & CLASSES  & \\datatag{Food101} & \\datatag{{CIFAR10}} & \\datatag{{CIFAR100}} & \\datatag{{Birdsnap}} & \\datatag{{SUN397}} & \\datatag{{Cars}} & \\datatag{{Aircraft}} & \\datatag{{VOC2007}} & \\datatag{{DTD}} & \\datatag{{Pets}} & \\datatag{{Cal101}} & \\datatag{{Flowers}} & \\datatag{{MNIST}} & \\datatag{{FER2013}} & \\datatag{{STL10}} & \\datatag{{EuroSAT}} & \\datatag{{RESISC45}} & \\datatag{{GTSRB}} & \\datatag{{KITTI}} & \\datatag{{Country211}} & \\datatag{{PCAM}} & \\datatag{{UCF101}} & \\datatag{{K700}} & \\datatag{{CLEVR}} & \\datatag{{HM}} & \\datatag{{SST}} &  \\datatag{{ImageNet}} & \\datatag{{AVG}}\\\\\n\\midrule\nUNICOM&500&45.5 &77.6 &49.8 &27.1 &50.3 &20.1 &21.4 &60.1 &49.9 &80.6 &70.8 &58.0 &94.9 &42.5 &82.6 &91.6 &61.2 &59.4 &60.3 &5.0 &77.4 &45.5 &21.9 &28.8 &44.7 &50.1 &42.1 &52.6\\\\\nUNICOM&1000&51.3 &79.0 &50.7 &28.5 &53.6 &22.1 &22.5 &65.9 &52.0 &83.6 &71.5 &60.6 &95.2 &43.1 &83.7 &92.8 &61.7 &60.6 &63.1 &5.5 &78.6 &46.6 &24.9 &29.3 &45.8 &52.6 &58.4 &54.9\\\\\nUNICOM&2000&52.7 &79.0 &51.3 &29.7 &54.6 &22.8 &24.2 &67.8 &50.7 &84.0 &77.3 &61.0 &95.7 &46.0 &84.4 &93.3 &62.3 &64.2 &65.0 &6.3 &79.4 &46.0 &26.2 &30.4 &47.6 &54.9 &61.5 &56.2\\\\\nUNICOM&4000&54.1 &76.4 &49.7 &35.2 &56.6 &25.3 &28.5 &72.1 &54.1 &82.9 &77.1 &61.6 &95.2 &47.6 &85.3 &94.6 &63.3 &62.4 &66.2 &6.7 &78.7 &48.3 &30.8 &34.6 &50.4 &56.1 &62.8 &57.6\\\\\nUNICOM&8000&54.9 &77.1 &50.5 &35.6 &57.6 &28.7 &28.6 &71.9 &53.3 &82.6 &78.4 &67.8 &95.0 &47.7 &86.3 &94.3 &66.0 &63.5 &66.4 &6.9 &78.1 &52.1 &31.5 &32.8 &50.4 &54.8 &62.4 &58.3\\\\\nUNICOM&20000&56.1 &78.2 &51.1 &38.5 &60.0 &32.6 &32.4 &72.9 &55.1 &84.4 &80.6 &71.2 &95.4 &48.1 &86.6 &95.7 &68.7 &66.3 &65.3 &7.5 &80.8 &55.4 &35.8 &32.8 &49.8 &55.2 &61.5 &59.9\\\\\nUNICOM&40000&57.1 &77.7 &51.8 &38.1 &60.3 &37.3 &35.6 &71.3 &56.9 &83.4 &81.0 &76.8 &95.3 &48.7 &85.4 &96.1 &71.9 &68.9 &68.2 &7.8 &79.3 &56.7 &33.7 &32.8 &50.8 &54.6 &59.5 &60.6\\\\\nUNICOM&80000&57.1 &76.9 &51.5 &38.8 &58.9 &33.9 &35.1 &69.1 &57.4 &81.3 &78.9 &77.4 &95.4 &48.8 &83.1 &95.9 &74.1 &67.3 &67.8 &7.9 &80.3 &55.8 &32.1 &33.8 &49.2 &53.7 &56.4 &59.9\\\\\nUNICOM&160000&56.3 &75.1 &50.1 &37.5 &59.2 &35.1 &35.9 &67.9 &56.5 &79.0 &79.0 &79.5 &96.3 &48.9 &82.3 &96.0 &76.7 &68.4 &70.2 &7.7 &80.2 &57.5 &36.2 &33.0 &51.4 &56.0 &53.6 &60.2\\\\\nUNICOM&320000&53.7 &73.3 &49.8 &34.9 &57.1 &31.7 &37.4 &66.0 &54.8 &74.2 &77.4 &78.4 &96.6 &49.6 &78.3 &95.3 &75.7 &69.4 &69.9 &7.6 &78.7 &56.4 &36.0 &35.6 &50.4 &55.7 &48.5 &59.0\\\\\nUNICOM&500000&48.8 &69.8 &46.2 &28.3 &53.5 &26.9 &36.2 &63.8 &50.3 &67.9 &72.9 &75.0 &97.2 &47.4 &74.3 &94.4 &73.5 &62.3 &73.4 &7.4 &80.0 &52.3 &33.0 &35.6 &49.4 &56.4 &41.6 &56.2\\\\\n\\midrule\nMLCD&500&55.3 &82.1 &54.3 &41.0 &67.1 &28.1 &35.3 &72.7 &62.3 &87.0 &87.9 &75.3 &97.4 &48.1 &93.1 &94.4 &68.7 &70.7 &64.3 &10.9 &80.8 &60.8 &37.6 &31.6 &50.0 &52.1 &63.2 &61.9\\\\\nMLCD&1000&59.1 &83.2 &59.6 &43.2 &68.1 &30.8 &38.9 &75.2 &64.9 &87.7 &88.5 &77.4 &97.1 &48.5 &94.1 &95.4 &71.3 &72.4 &65.8 &10.6 &79.9 &61.2 &40.6 &34.1 &50.5 &53.7 &67.2 &63.7\\\\\nMLCD&2000&62.0 &84.0 &61.8 &45.4 &69.0 &30.7 &39.7 &77.5 &65.6 &88.0 &88.1 &79.0 &97.0 &49.7 &93.9 &96.7 &74.3 &73.4 &68.8 &10.6 &80.9 &64.6 &41.2 &35.2 &50.6 &54.7 &68.2 &64.8\\\\\nMLCD&4000&65.1 &84.7 &63.5 &47.4 &69.7 &37.2 &41.7 &79.2 &67.6 &88.5 &89.7 &82.6 &97.7 &51.4 &94.4 &97.0 &78.0 &75.7 &70.5 &11.0 &80.5 &67.3 &42.1 &39.8 &50.6 &58.2 &69.9 &66.7\\\\\nMLCD&8000&66.2 &85.1 &64.9 &50.0 &70.4 &42.2 &46.0 &80.2 &68.2 &88.6 &90.2 &84.6 &97.8 &51.5 &94.3 &97.1 &78.9 &78.6 &69.1 &10.8 &81.4 &67.9 &43.7 &37.2 &50.8 &56.4 &69.7 &67.5\\\\\nMLCD&20000&65.9 &84.1 &63.0 &46.5 &68.6 &44.8 &44.9 &81.9 &66.5 &89.9 &89.7 &82.8 &97.5 &51.8 &93.2 &97.1 &78.6 &78.7 &69.5 &10.3 &81.3 &67.1 &42.4 &38.0 &50.2 &56.8 &69.0 &67.0\\\\\nMLCD&40000&65.2 &83.4 &62.4 &48.5 &68.4 &48.3 &46.1 &82.2 &66.6 &90.2 &89.4 &83.1 &97.2 &50.3 &91.9 &96.8 &78.7 &75.2 &70.9 &10.5 &80.0 &66.5 &42.2 &36.6 &52.4 &56.8 &68.8 &67.0\\\\\nMLCD&80000&69.9 &85.6 &65.4 &56.0 &70.7 &57.0 &52.4 &83.2 &68.5 &90.3 &91.2 &88.8 &97.3 &53.3 &93.3 &97.7 &81.2 &79.4 &73.1 &11.5 &79.8 &71.6 &45.1 &38.4 &50.0 &55.5 &70.0 &69.5\\\\\nMLCD&160000&71.3 &86.2 &67.4 &59.2 &71.7 &61.0 &56.1 &84.6 &69.8 &91.3 &91.7 &90.8 &98.0 &53.5 &93.1 &97.7 &83.3 &80.9 &72.7 &11.6 &80.9 &71.9 &46.6 &40.0 &51.8 &55.2 &70.0 &70.7\\\\\nMLCD&320000&72.2 &86.1 &67.4 &60.1 &71.7 &64.8 &56.4 &85.9 &68.7 &90.8 &92.1 &91.7 &98.2 &54.3 &93.2 &98.0 &84.6 &81.9 &74.0 &11.6 &81.3 &73.9 &48.0 &45.2 &49.6 &55.2 &69.7 &71.4\\\\\nMLCD&500000&72.5 &86.3 &68.3 &59.8 &71.7 &65.0 &56.8 &83.1 &69.9 &90.6 &91.7 &92.3 &98.3 &54.7 &93.1 &97.8 &85.2 &82.7 &74.0 &11.5 &81.7 &73.8 &47.1 &44.0 &50.4 &54.5 &68.9 &71.3\\\\\n\\bottomrule\n\\end{tabular}\n}\n\n\\end{table}\n\n\\subsubsection{Linear Probe Evaluation}\nIn our linear probing analysis, we adhered to the same configuration as CLIP. We utilized the L-BFGS optimization algorithm as implemented in PyTorch, executing it on a GPU with an upper limit of 1000 iterations. We adopted CLIP's parametric binary search protocol to optimize the hyper-parameter $\\lambda$, with the optimization process conducted on the validation set. In cases where a dataset lacks a predefined validation set, we manually partition the dataset. This streamlined methodology allowed us to efficiently run tests across all 27 datasets within a few hours. For the final results, the validation set is merged back into the training set for an additional round of training.\n\n\\subsubsection{Zero-shot Evaluation}\nFor the experiments in zero-shot, we use the prompts same as FLIP.\nFollowing CLIP ~\\cite{radford2021learning}, we report the mean accuracy per class for FGVC Aircraft, Oxford-IIIT Pets, Caltech-101, and Oxford Flowers 102 datasets. We report the mean of top-1 and top-5 accuracy for Kinetics-700, ROC AUC for Hateful Memes, and 11-point mAP for Pascal VOC 2007 Classification. We report top-1 accuracy for the rest of the datasets.\n\n\\subsubsection{Zero-shot Retrieval}\nWe assess the effectiveness of zero-shot retrieval using two established benchmarks: Flickr30K~\\cite{young2014image} and COCO~\\cite{lin2014microsoft}, each containing 1K and 5K image-text pairs in their test sets, respectively. In adhering to the procedures outlined in CLIP and FLIP, we derive the image and text embeddings from the relevant encoders, and then execute retrieval by calculating cosine similarities across potential image-text pairs, without prompt being utilized.\n\n\\subsubsection{Zero-shot Robustness Evaluation}\nIn our zero-shot robustness assessment on ImageNet-related sets, we employ the 7 prompts provided by CLIP, with dataset preparation and division adhering to the methods used in OpenCLIP. For ObjectNet, we emulate the approach of CLIP by utilizing class names without any prompt.\n\n\\begin{table}\n\\caption{Comparisons of linear probe performance across 27 different downstream datasets.\nThe network structure used here is ResNet50 following the BiT~\\cite{bigtransfer} and CLIP~\\cite{radford2021learning} papers. $\\dag$: Results reported in the CLIP paper referring to the linear probe results of BiT~\\cite{bigtransfer} with ResNet50 trained on the ImageNet1K dataset. $\\ddag$: Results reported in our testing by using the open-sourced BiT~\\cite{bigtransfer} ResNet50 model. Different from the baseline model trained with the ground-truth labels, the proposed MLCD models are trained with the automatically clustered class labels.}\n\\label{tab:imagenet1k}\n    \\centering\n    \\resizebox{1.0\\linewidth}{!}{\n        \\tablestyle{0.7mm}{1.1}\n        \\begin{tabular}{*l^l|^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c^c}\n            CASE & CLASSES  & \\datatag{Food101} & \\datatag{{CIFAR10}} & \\datatag{{CIFAR100}} & \\datatag{{Birdsnap}} & \\datatag{{SUN397}} & \\datatag{{Cars}} & \\datatag{{Aircraft}} & \\datatag{{VOC2007}} & \\datatag{{DTD}} & \\datatag{{Pets}} & \\datatag{{Cal101}} & \\datatag{{Flowers}} & \\datatag{{MNIST}} & \\datatag{{FER2013}} & \\datatag{{STL10}} & \\datatag{{EuroSAT}} & \\datatag{{RESISC45}} & \\datatag{{GTSRB}} & \\datatag{{KITTI}} & \\datatag{{Country211}} & \\datatag{{PCAM}} & \\datatag{{UCF101}} & \\datatag{{K700}} & \\datatag{{CLEVR}} & \\datatag{{HM}} & \\datatag{{SST}} &  \\datatag{{ImageNet}} & \\datatag{{AVG}}\\\\\n            \\midrule\n\n\\rowstyle{\\color{dt}}RN50$^{\\dag}$ &1000&72.5 &91.7 &74.8 &57.7 &61.1 &53.5 &52.5 &83.7 &72.4 &92.3 &91.2 &92.0 &98.4 &56.1 &76.7 &97.4 &85.0 &70.0 &66.0 &12.5 &83.0 &72.3 &47.5 &48.3 &54.1 &55.3 &75.2 &70.1\\\\\n\\rowstyle{\\color{dt}}RN50$^{\\ddag}$ &1000&72.6 &91.5 &74.2 &57.9 &60.1 &51.2 &51.8 &84.1 &70.9 &91.5 &91.5 &91.8 &97.9 &56.1 &77.5 &96.4 &84.7 &73.3 &64.7 &11.4 &83.9 &71.8 &45.4 &44.3 &51.2 &53.4 &75.6 &69.5\\\\\nMLCD&1000&59.1 &83.2 &59.6 &43.2 &68.1 &30.8 &38.9 &75.2 &64.9 &87.7 &88.5 &77.4 &97.1 &48.5 &94.1 &95.4 &71.3 &72.4 &65.8 &10.6 &79.9 &61.2 &40.6 &34.1 &50.5 &53.7 &67.2 &63.7\\\\\nMLCD&160000&71.3 &86.2 &67.4 &59.2 &71.7 &61.0 &56.1 &84.6 &69.8 &91.3 &91.7 &90.8 &98.0 &53.5 &93.1 &97.7 &83.3 &80.9 &72.7 &11.6 &80.9 &71.9 &46.6 &40.0 &51.8 &55.2 &70.0 &70.7\\\\\nMLCD&320000&72.2 &86.1 &67.4 &60.1 &71.7 &64.8 &56.4 &85.9 &68.7 &90.8 &92.1 &91.7 &98.2 &54.3 &93.2 &98.0 &84.6 &81.9 &74.0 &11.6 &81.3 &73.9 &48.0 &45.2 &49.6 &55.2 &69.7 &71.4\\\\\n            \\bottomrule\n        \\end{tabular}\n    }\n\\end{table}\n\n\\begin{figure}[t!]\n\\centering\n\\includegraphics[width=0.7\\textwidth]{figs/complementary.png}\n\\caption{Visualization of top 3 labels given to the training samples from the automatically clustered ImageNet dataset. Multiple positive labels show complementary visual signals.}\n\\label{fig:pruityconflict_in1k}\n\\end{figure}\n\n\\subsubsection{VLM Evaluation}\nFor VLM evaluation, we tested on the VQAv2~\\cite{vlm_vqav2}, GQA~\\cite{vlm_gqa}, VizWiz~\\cite{vlm_vizwiz}, SQA~\\cite{vlm_sqa}, TextVQA~\\cite{vlm_text_vqa}, LLaVA-Wild~\\cite{liu2024llava_V1_5}, AI2D~\\cite{vlm_ai2d}, MathVista~\\cite{vlm_mathvista}, HallusionBench\n~\\cite{vlm_hb}, MMMU~\\cite{vlm_mmmu}, cMMMU~\\cite{vlm_cmmmu}, MMBench~\\cite{vlm_mmbench}, SEED-Bench~\\cite{vlm_seedbench} and MME~\\cite{vlm_mme} test sets. We used the LMMs-Eval~\\cite{lmms_eval2024} tool to evaluate the model. During training, we aligned the hyper-parameters with LLaVA-1.5, using the same pre-training and instruction fine-tuning data as LLaVA-1.5. We also utilized DeepSpeed Zero3\\cite{rajbhandari2020zero} to accelerate the training process.\n\n\\subsection{Multi-label Learning on ImageNet}\n\nIn Tab.~\\ref{tab:unicom_mlcd}, we compare the proposed multi-label cluster discrimination and the single-label cluster discrimination (UNICOM~\\cite{an2023unicom}) on ImageNet with the clustered class number ranging from 0.5K to 0.5M. The clustering step is conducted by using the features predicted by the CLIP model (\\ie, ViT-L/14). In the discrimination step, both UNICOM and MLCD employ the negative class center sampling with a ratio of $0.1$, and the positive number for MLCD is set as $8$. As we can see, the proposed multi-label learning significantly surpasses UNICOM and achieves the best performance of $71.4\\%$ when the class number is 320K. In Fig.~\\ref{fig:pruityconflict_in1k}, we visualize the top three labels for our training samples. \nWhen training with multiple labels, our method can\nlearn complementary visual signals (\\eg, different activities in the snow, different breeds of dogs, different locations of figs, and different objects in the room) to improve visual representation learning. \n\nIn Tab.~\\ref{tab:imagenet1k}, we compare the performance between models trained with the ground-truth class labels and the automatically clustered class labels. As we can see,\n2nd and 3rd rows demonstrate the performance gap between models trained by the ground-truth 1K classes and the automatically clustered 1K classes. Row 4 and 5 indicate that with a significant increase in class numbers (\\eg, high purity within each cluster), the results significantly improve. Although the performance of the proposed MLCD on ImageNet does not surpass the supervised case ($69.7\\%$ vs. $75.6\\%$), the proposed method demonstrates superior feature representation learning capacity across the different datasets ($71.4\\%$ vs. $69.5\\%$).\n\n\\subsection{Acknowledgment}\n\nWe would like to thank Tongliang Liu, Jia Guo, and Jing Yang for their insightful discussions on the experimental design of this paper. We thank Bin Qin, Lan Wu, Haiqiang Jiang, and Yuling Wu for their help with the downloading and organization of all the web datasets. We also thank Yin Xie for help with the VLM experiment.\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{HyperSeg: Towards Universal Visual Segmentation with Large Language Model}\n\n\\begin{document}\n\n\\twocolumn[{%\n\\renewcommand\\twocolumn[1][]{#1}%\n\\maketitle\n\n\\vspace{-5mm}\n\\centering\n\\includegraphics[width=\\textwidth]{fig/intro.pdf}\n\\vspace{-2mm}\n\\captionof{figure}{Illustration of our \\name~which can conduct image and video segmentation tasks with various language and visual instructions. Additionally, \\name~can handle complicated reasoning perception tasks compared with previous universal segmentation methods. To our knowledge, \\name~is the first VLLM-based universal segmentation model with perception and complex reasoning abilities in both image and video domains.}\n\\label{fig:intro}\n\\vspace{0.6cm}\n}]\n\n\\renewcommand{\\thefootnote}{\\fnsymbol{footnote}}\n\\footnotetext{$^{\\dagger}$Corresponding authors.}\n\n\\begin{abstract}\n\\indent This paper aims to address universal segmentation for image and video perception with the strong reasoning ability empowered by Visual Large Language Models (VLLMs). Despite significant progress in current unified segmentation methods, limitations in adaptation to both image and video scenarios, as well as the complex reasoning segmentation, make it difficult for them to handle various challenging instructions and achieve an accurate understanding of fine-grained vision-language correlations.\nWe propose HyperSeg, the first VLLM-based universal segmentation model for pixel-level image and video perception, encompassing generic segmentation tasks and more complex reasoning perception tasks requiring powerful reasoning abilities and world knowledge.\nBesides, to fully leverage the recognition capabilities of VLLMs and the fine-grained visual information, HyperSeg incorporates hybrid entity recognition and fine-grained visual perceiver modules for various segmentation tasks. \nCombined with the temporal adapter, HyperSeg achieves a comprehensive understanding of temporal information. \nExperimental results validate the effectiveness of our insights in resolving universal image and video segmentation tasks, including the more complex reasoning perception tasks. Our code is available \\href{https://github.com/congvvc/HyperSeg}{here}. \n\\vspace{-4mm}\n\n\\end{abstract}\n\n\\section{Introduction}\\label{sec:intro}\nVisual segmentation is one of the most significant tasks in computer vision research, which aims to perform accurate pixel-level semantic understanding.\nMany specialist models~\\cite{he2017mask,cheng2022masked,jain2023oneformer,Lai2023LISARS} have made great progress in specific segmentation tasks while showing limitations in handling diverse and complicated scenarios since new training data, paradigms, and model architectures are required to adapt to new segmentation tasks. \nRecent works~\\cite{lin2023uninext,zhang2023simple,li2024omg} propose a single framework to unify diverse segmentation tasks.\nDespite promising, they show the inability to tackle text instructions and complex reasoning segmentation tasks needing powerful reasoning capabilities and world knowledge.\n\nVisual Large Language Models (VLLMs) have exhibited excellent reasoning and conversation abilities, which play a pivotal role in various vision-language co-understanding tasks~\\cite{cho2021unifying,liu2024visual,li2023blip,zhu2023minigpt,bai2023qwen}. \n\\textbf{However, these methods are based on rudimentary vision-language alignment, which limits their ability to comprehend finer details in visual perception tasks, like pixel-level segmentation.}\nRecent studies~\\cite{Lai2023LISARS,wei2024lasagna,zhang2024psalm,yan2024visa,zhang2024omg} enables VLLMs to perform fine-grained visual understanding, like referring and reasoning segmentation. \n~\\cite{Lai2023LISARS,wei2024lasagna,Ren2023PixelLMPR}\nuse the special token [SEG] generated by VLLMs as the prompt for the mask decoder to generate segmentation masks while ~\\cite{zhang2024psalm,zhang2024omg} focus on incorporating instance-aware mask tokens into VLLMs.\n\\textbf{Though impressive, they show limitations to the universal segmentation framework based on VLLMs for both image and video domains and the capabilities of handling more complex video reasoning segmentation tasks.}\n\nTo this end, we introduce \\name, the first VLLM-based universal segmentation model for pixel-level image and video perception with complex reasoning and conversation capabilities. \\name~can conduct diverse image and video segmentation tasks with various\nelaborate prompts and temporal adapter module. Besides, \\name~shows excellent abilities in complicated vision-language reasoning perception tasks needing rich world knowledge, which is significant for real-world understanding and interactions.\nAs shown in Fig.~\\ref{fig:intro}, the explored tasks contain both image and video domains.\nWe organize the tasks into two unified prompt formats: (1) text prompts (class names, reasoning questions, and referring languages), (2) visual prompts (box, mask, etc.). \nOwing to such flexible and cohesive design, \\name~benefits from concurrent training on diverse segmentation tasks and vision domains, facilitating the intricate correlations between different instructions and visual concepts.\nTo further enhance fine-grained object perception and video understanding, we introduce three distinct designs in the following.\n\nFirstly, we incorporate a hybrid entity recognition strategy to enhance the exploitation of VLLM's recognition capacity.\nGeneration-only works~\\cite{Lai2023LISARS,xia2023gsva, Ren2023PixelLMPR} solely rely on VLLM for object prediction leading to poor performance in complex multi-object segmentation scenarios.\nDecode-only methods~\\cite{zhang2024psalm,zhang2024omg} use the prompt embedding and mask tokens decoded by VLLM to obtain class scores for each mask, which makes the mask tokens interact insufficiently with the semantic condition as they ignore the powerful generative capabilities of VLLM.\nThe proposed hybrid entity recognition leverages the VLLM's powerful generative abilities to enhance the mask tokens' comprehension of category semantics while maintaining the final class scores decoding process.\n\nSecondly, previous VLLMs usually use coarse-level visual features obtained from CLIP~\\cite{radford2021learning} series which primarily encode global visual information while overlooking visual details. \nTo enhance VLLMs' ability of capturing visual details efficiently, we use the Fine-grained Visual Perceiver (FVP) to merge multi-scale visual features into fixed-length fine-grained tokens, allowing retrieval of rich visual details from various scales in the hierarchical vision encoder~\\cite{cheng2022masked}. \n\nThirdly,  recent VLLM-based segmentation methods~\\cite{Lai2023LISARS,zhang2024psalm,zhang2024omg} demonstrate limitations in handling video perception tasks for video temporal understanding. To this end, \nwe propose the temporal adapter for comprehensive video perception which incorporates global prompt aggregation and local space-time information injection for the coalescence of both long-term and short-term vision-language information.\n\nExtensive experiments on various segmentation benchmarks demonstrate the preeminent segmentation ability of \\name~, providing strong evidence of the effectiveness of our insights. Our \\name~also exhibits promising performance on common Multi-modal benchmarks. Additionally, we explore the mutual influence among different tasks involving various visual and task types. \n\nOur contributions are summarized as follows: \n\\begin{itemize}[leftmargin=0.5cm]\n    \\item We present \\name, the first VLLM-based universal segmentation model for pixel-level image and video perception, covering a broad spectrum of common segmentation tasks, complex reasoning, and conversation-based vision-language understanding tasks.\n    \\item We incorporate hybrid entity recognition and fine-grained visual perceiver modules \n    to VLLM, which allow full exploitation of VLLM’s semantic recognition capacity and injection of fine-grained visual information to improve diverse detail-aware segmentation tasks.\n    With the temporal adapter, \\name~can conduct more challenging video perception tasks, achieving universal segmentation.\n    \\item \\name~demonstrates superior capabilities on multiple segmentation tasks, achieving excellent performance on both generic and complex reasoning benchmarks with only one model.\n    \n\\end{itemize}\n\n\\section{Related Work}\n\\textbf{Visual Large Language Model.}~\nThe emergence of Large Language Model (LLM) has significantly contributed to the development of VLMM. In this context, LLMs are enhanced with multimodal comprehension capabilities, allowing the vision-language co-understanding~\\cite{li2023blip, alayrac2022flamingo, zhu2023minigpt, liu2024visual, liu2023improved, bai2023qwen}. \nSeveral notable examples of LLMs with multimodal comprehension include BLIP-2~\\cite{li2023blip}, Flamingo~\\cite{alayrac2022flamingo}, MiniGPT-4~\\cite{zhu2023minigpt}, LLaVA~\\cite{liu2024visual}, InstructBLIP~\\cite{instructblip}, and Qwen-VL~\\cite{bai2023qwen}. While these models have demonstrated impressive performance in vision-language tasks, they solely produce textual outputs that describe the entire image. This restricts their applicability in tasks that require the pixel-level detailed understanding.\n\n\\noindent \\textbf{Perception with VLLM.}~\nSeveral methods have been proposed to enhance VLLMs with a more detailed comprehension capability~\\cite{chen2023shikra,wang2024visionllm,peng2023kosmos,you2023ferret,Lai2023LISARS,Ren2023PixelLMPR,rasheed2023glamm,pi2023perceptiongpt}.\nShikra~\\cite{chen2023shikra}, Ferret~\\cite{you2023ferret}, Kosmos-2~\\cite{peng2023kosmos}, and VisionLLM~\\cite{wang2024visionllm} are examples that provide grounding capabilities through regression of box coordinates. Conversely, LISA~\\cite{Lai2023LISARS}, PixelLM~\\cite{Ren2023PixelLMPR}, GLaMM~\\cite{rasheed2023glamm}, and PerceptionGPT~\\cite{pi2023perceptiongpt} employ a mask decoder to predict object masks from special tokens. \nMost of the existing methods utilize a next-token-prediction approach, which restricts their applicability. PSALM~\\cite{zhang2024psalm} makes an important attempt to bring VLLM into visual perception tasks but fails to fully unleash the potential of VLLM. In contrast, our method propose to use a hybrid strategy to mitigate this problem and keep the capacity in high-level reasoning.\n\n\\noindent \\textbf{Unified segmentation model.}~\nAnother line of studies focuses on the integration of various segmentation tasks into a single model. Mask2former~\\cite{cheng2022masked} proposes a unified architecture that requires separate training on different segmentation tasks. \nOpenSeeD~\\cite{zhang2023simple} introduces a text encoder and extends it to the Open-Set setting. Simultaneously, UNINEXT~\\cite{lin2023uninext} supports referring segmentation with the assistance of text inputs and a text encoder. \nHowever, these works fall short of following complicated instructions and reasoning. In this work, we improve the understanding ability toward language by incorporating LLM, while also maintaining the original ability of vision-centric models. \n\n\\subsection{Overview}\n\\label{subsec:overview}\n\n\\textbf{Overall architecture.} The architecture of \\name~is illustrated in Fig. \\ref{fig:model}, which consists of a fine-grained pyramid visual encoder, a light-weight VLLM, and a segmentation predictor to generate segmentation masks, class scores, and instance embedding for video correspondence according to user’s instruction.\nThe proposed FVP module fuses multi-scale high-resolution visual features $f_{img}$ into a set of fine-grained tokens to ensure the injection of fine-grained visual information (Sec \\ref{subsec:fvp}).\nThe VLLM takes three types of inputs: visual tokens encoded by the CLIP encoder, renewed fine-grained tokens, and prompt tokens for diverse instructions. The output embeddings of semantically enhanced mask tokens (Sec \\ref{subsec:her}) and prompt tokens are further fed into the segmentation predictor for final segmentation results. Besides, we utilize the space-time information propagation and global prompt aggregation for comprehensive video understanding (Sec \\ref{subsec:videoadapt}).\nWe train the LLM with LoRA for efficient parameter tuning.\n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\textwidth]{fig/model.pdf}\n    \\caption{\\textbf{Overview of \\name}. \\name~encodes the visual input in a multi-grained manner and concatenates the prompt for different perception tasks. We feed learnable fine-grained tokens into a Fine-grained Visual Perceiver (FVP) to integrate multi-scale high-resolution image features into LLM for detailed visual learning and to facilitate space-time information propagation for video understanding.\n    Additionally, we use the semantically enhanced mask tokens and prompt embedding to finally generate the segmentation masks and class scores for generic segmentation, and instance embedding for video instance association.}\n    \\label{fig:model}\n    \\vspace{-3mm}\n\\end{figure*}\n\n\\noindent \\textbf{Visual Large Language Model}. \nWe take a light-weight VLLM as our powerful multi-modal feature encoder, which contains a low-resolution vision encoder like CLIP~\\cite{radford2021learning} and an efficient LLM.\n\nSpecifically, the model takes vision-prompt pairs $\\{(\\mathcal{V},\\mathcal{P})\\}$ as inputs, where $\\mathcal{V}$ is resized to low resolution and then encoded by CLIP encoder $F_{CLIP}$ to get image features $f_{v}$. The $f_{v}$ is further projected and concatenated with other task-specific tokens to ensure the comprehensive understanding of multi-modal inputs through the fusion process of LLM $F_{LLM}$, where $G_{c}$ is the projection function and $E_{O}$ denotes the output embeddings of LLM. Formally,\n\\begin{equation}\n     f_{v}=F_{CLIP}(\\mathcal{V}), E_{O}=F_{LLM}(G_{c}(f_{v}), P, \\mathcal{P}),\n\\label{Eq: PSALM}\n\\end{equation}\nwhere $P$ denotes fine-grained tokens.\nFurthermore, we manually extract semantic enhanced\nmask tokens $E_\\mathcal{Q}$ and prompt embedding $E_{\\mathcal{P}}$\nfrom $E_{O}$, which are further fed into the pre-trained segmentation predictor~\\cite{cheng2022masked} to generate masks, class scores, and instance embedding for final segmentation results.\n\n\\noindent \\textbf{Prompt design.} \nIn order to accommodate the different segmentation tasks, we propose a flexible design for prompt $\\mathcal{P}$. \nAs illustrated above, we divide $\\mathcal{P}$ into two formats: text prompts and visual prompts. \nTo be specific, $\\mathcal{P}$ contains the instructions $\\mathcal{P_I}$ and task-specific conditions $\\mathcal{P_C}$, where $\\mathcal{P_I}$ instructs the model to perform different tasks while \n$\\mathcal{P_C}$ indicates diverse conditions which are further used as classifiers to calculate the class scores of predicted masks.\n\nFor class-based segmentation tasks like panoptic segmentation, open-vocabulary segmentation (OVS), and video instance segmentation (VIS),  $\\mathcal{P}$ can be demonstrated as $\\mathcal{P_I}$: \\emph{``Please segment all the positive objects according to the following potential categories.''} $\\mathcal{P_C}$: \\emph{``[category 1, category 2, category 3, ...]''}\n\\vspace{-0.5mm}\n\nFor referring and reasoning segmentation tasks like referring expression segmentation (RES), reasoning segmentation, referring video object segmentation (R-VOS), and ReasonVOS, $\\mathcal{P}$ can be designed as\n $\\mathcal{P_I}$: \\emph{``Can you perform referring or reasoning segmentation according to the language expression?''}\n $\\mathcal{P_C}$: \\emph{``[referring / reasoning text]''}\n\nFor visual-guided segmentation tasks like interactive segmentation and video object segmentation (VOS), $\\mathcal{P}$ can be designed as\n $\\mathcal{P_I}$: \\emph{``Please segment according to the given visual region reference''}\n $\\mathcal{P_C}$: \\emph{``[vision 1, vision 2, vision 3, ...]''}. \nInstead of using an additional region encoder to extract visual reference features~\\cite{lin2023uninext}, we sample the CLIP visual features $f_{v}$ in VLLM according to the region coordinates and perform adaptive average pooling on them to form the final reference features for each visual prompt.\n\n\\noindent \\textbf{Segmentation predictor.} Segmentation predictor $F_{p}$ generates the masks $m$, corresponding class scores $z$, and instance embedding $e$ through the similar process~\\cite{cheng2022masked, gu2024dataseg} of three inputs: task-specific prompt embedding $\\{E_{\\mathcal{P}}^{k}\\}_{k=1}^{K}$, the semantically enhanced mask tokens $\\{E_{\\mathcal{Q}}^{j}\\}_{j=1}^{N}$ and the multi-scale visual features $f_{img}$, where $K$ and $N$ denote $K$ categories and $N$ mask proposals. Formally,\n\\begin{equation}\n    \\{m_{j}, z_{j}, e_{j}\\}_{j=1}^{N} = F_{p}(\\{E_{\\mathcal{P}}^{k}\\}_{k=1}^{K}, \\{E_{\\mathcal{Q}}^{j}\\}_{j=1}^{N}, f_{img}),\n\\end{equation}\nwhere $m_{j}\\in \\mathbb{R}^{H\\times W}$ is the j-th mask proposal, $z_{j}\\in \\mathbb{R}^{K}$ denotes the class scores of $m_{j}$, and $e_{j}\\in \\mathbb{R}^{D}$ denotes the j-th instance embedding obtained from an extra embedding head only for video domain.\nFor video tasks, we adopt a frame-by-frame manner to get frame-level segmentation results for efficient training and inference processes.\n\n\\noindent \\textbf{Training objectives.} The model can be trained jointly on multiple tasks using the unified loss $\\mathcal{L}$. Specifically, we employ an autoregressive cross-entropy loss $\\mathcal{L}_{text}$ for text prediction, a combination of per-pixel binary cross-entropy loss $\\mathcal{L}_{bce}$ and DICE loss $\\mathcal{L}_{dice}$ for mask supervision $\\mathcal{L}_{mask}$, a cross-entropy loss $\\mathcal{L}_{cls}$ for category classification, and a contrastive loss $\\mathcal{L}_{ins}$ for instance association of video sequences following~\\cite{IDOL}. $\\lambda$ indicates their sum weight respectively. Formally,\n\\begin{equation}\n\\label{eq:total_loss}\n\\mathcal{L}= \\mathcal{L}_{text}+\\lambda_{mask}\\mathcal{L}_{mask}+\\lambda_{cls}\\mathcal{L}_{cls}+\\lambda_{ins}\\mathcal{L}_{ins},\n\\end{equation}\n\\vspace{-6mm}\n\\begin{equation}\n\\label{eq:mask_loss}\n\\mathcal{L}_{mask}=\\lambda_{bce}\\mathcal{L}_{bce}+\\lambda_{dice}\\mathcal{L}_{dice},\n\\end{equation}\n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\textwidth]{fig/cmp.pdf}\n     \\vspace{-5mm}\n    \\caption{\\textbf{The comparison of different recognition strategies}. (a) Generation-Only~\\cite{Lai2023LISARS, Ren2023PixelLMPR}: both the semantic recognition (existing objects) and their mask tokens are generated by LLM. \n    (b) Decode-Only~\\cite{zhang2024psalm,zhang2024omg}: prompt embedding and mask tokens are decoded from LLM. The present objects are then determined by their similarity scores. (c) Hybrid (ours): prompt embedding is decoded from LLM while the semantically enhanced mask tokens are generated by LLM. Their similarity scores reflect the objects' presence.\n    }\n    \\label{fig:cmp}\n    \\vspace{-3mm}\n\\end{figure*}\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.48\\textwidth]{fig/fvp.pdf}\n    \\caption{\\textbf{Comparison between previous vision perceiver and our FVP}. (a): previous vision perceiver~\\cite{li2023blip,bai2023qwen} uses the coarse single-scale CLIP visual features which are inadequate for fine-grained perception tasks.\n    (b): FVP encodes the multi-scale visual features into fine-grained tokens.  }\n    \\label{fig:fvp}\n    \\vspace{-3mm}\n\\end{figure}\n\n\\noindent \\textbf{Differences between \\name~and previous methods.}\nPrevious universal segmentation methods~\\cite{lin2023uninext,li2024omg,liu2024universal} lacking of VLLMs show inability in reasoning perception tasks\nwhile our \\name~demonstrates brilliant reasoning segmentation capability in complex scenarios.  \nBesides, we make a significant generalization of the current VLLM-based segmentation methods~\\cite{Lai2023LISARS,zhang2024psalm,yan2024visa,zhang2024omg} for more diverse segmentation tasks in both image and video domains using a single model framework.\nMoreover, \\name~differs from previous methods in the three designs elaborated in the following sections.\n\n\\subsection{Hybrid Entity Recognition}\n\\label{subsec:her}\nAs shown in Fig. \\ref{fig:cmp} (a), predicting presented objects in the way of sequence generation (semantic prediction) tends to miss objects or produce repetitive predictions \\cite{wei2024lasagna}. On the other hand, Fig. \\ref{fig:cmp} (b), only using VLLM \nto embed class names (prompt tokens) as mask classifier at the decode stage disregards VLLM's powerful semantic recognition capability.\nConsequently, we propose a hybrid approach that leverages LLM in both generation and decoding processes.\n\nInstead of integrating mask tokens in input sequences and extracting the corresponding embedding from the one-pass forward output of VLLM, we instruct VLLM to generate the mask tokens preceded by the estimated objects' names. As illustrated in Fig. \\ref{fig:cmp} (c),  VLLM is compelled to generate all the existing objects in the vision input and then the mask tokens. The semantically enhanced mask tokens contain valuable semantic integrated information about the image, which are subsequently used as input for the segmentation predictor to generate segmentation masks.\n\n\\begin{table*}[t]\n  \\centering\n  \\caption{ Comparison with the state-of-the-art models on the closed-set referring segmentation benchmarks (RefCOCO series) and more challenging generalized referring expression segmentation benchmark gRefCOCO. $\\ddagger$ denotes models using pre-trained SAM~\\cite{kirillov2023segment} for mask generation.\n  * means using gRefCOCO for training while other methods are evaluated in zero-shot manners. Our \\name~exhibits excellent performance over other zero-shot models like LaSagnA~\\cite{wei2024lasagna} and PSALM~\\cite{zhang2024psalm}.\n  }\n  \\scalebox{0.84}{\n    \\begin{tabular}{c|l|ccc|ccc|cc|ccc}\n    \\toprule[1.1pt] \n    \\multirow{2}{*}{ Type } & \\multirow{2}{*}{ Method } & \\multicolumn{3}{c|}{ RefCOCO } & \\multicolumn{3}{c|}{ RefCOCO+ } & \\multicolumn{2}{c|}{ RefCOCOg } & \\multicolumn{3}{c}{ gRefCOCO }\\\\\n    \\cline { 3 - 13 } & & val & testA & testB & val & testA & testB & val(U) & test(U) & val & testA & testB\\\\\n    \\midrule[0.7pt]\n    \\multirow{4}{*}{\\shortstack{Segmentation\\\\Specialist}} \n    & VLT~\\cite{ding2021vision} & 67.5 & 70.5 & 65.2 & 56.3 & 61.0 & 50.1 & 55.0 & 57.7 & 52.5* & 62.2* & 50.5* \\\\\n    & CRIS~\\cite{wang2022cris} & 70.5 & 73.2 & 66.1 & 62.3 & 68.1 & 53.7 & 59.9 & 60.4 & 55.3* & 63.8* & 51.0* \\\\\n    & LAVT~\\cite{yang2022lavt} & 72.7 & 75.8 & 68.8 & 62.1 & 68.4 & 55.1 & 61.2 & 62.1 & 57.6* & 65.3* & 55.0* \\\\\n    & PolyFormer-B~\\cite{liu2023polyformer} & 74.8 & 76.6 & 71.1 & 67.6 & 72.9 & 59.3 &67.8 &69.1 & - & - & - \\\\\n    \\midrule[0.5pt] \n    \\multirow{11}{*}{\\shortstack{MLLM-based\\\\Segmentation Network}} \n    & LISA-7B~\\cite{Lai2023LISARS} $\\ddagger$ & 74.9 & 79.1 & 72.3 & 65.1 & 70.8 & 58.1 &  67.9 & 70.6 & 38.7* & 52.6* & 44.8* \\\\\n    & PixelLM-7B~\\cite{Ren2023PixelLMPR}& 73.0 &76.5 & 68.2 & 66.3 & 71.7 & 58.3 &69.3 & 70.5 & - & - & - \\\\\n    & F-LMM-7B~\\cite{wu2024f} $\\ddagger$ & 76.1 & - & - & 66.4 & - & - & 70.1 & - & - & - & - \\\\ \n    & GSVA-7B~\\cite{xia2023gsva} $\\ddagger$ & 76.4 &77.4 &72.8 &64.5 &67.7 & 58.6 & 71.1 & 72.0 & 61.7* & 69.2* & 60.3* \\\\\n    & GroundHog-7B~\\cite{miao2023spectrum} & 78.5 & 79.9 & 75.7 & 70.5 & 75.0 & 64.9 & 74.1 & 74.6 & 66.7* & - & - \\\\\n    & SAM4MLLM-7B~\\cite{chen2025sam4mllm} $\\ddagger$ & 79.6 & 82.8 & 76.1 & 73.5 & 77.8 & 65.8 & 74.5 & 75.6 & 66.3* & 70.1* & 63.2* \\\\\n    & LaSagnA-7B~\\cite{wei2024lasagna} $\\ddagger$ & 76.8 & 78.7 & 73.8 & 66.4 & 70.6 & 60.1 & 70.6 & 71.9 & 38.1 & 50.4 & 42.1 \\\\\n       \n    & OMG-LLaVA ~\\cite{zhang2024omg} & 78.0   & 80.3  & 74.1   & 69.1    & 73.1     & 63.0   & 72.9     & 72.9  & - & - & -  \\\\\n    &GLaMM~\\cite{rasheed2024glamm} $\\ddagger$ & 79.5    & 83.2    & 76.9  & 72.6    & 78.7     & 64.6    &  74.2   & 74.9   & - & - & - \\\\\n    & PSALM ~\\cite{zhang2024psalm} & 83.6   & 84.7  & 81.6   & 72.9    & 75.5     & 70.1   & 73.8     & 74.4 & 42.0 & 52.4 & 50.6  \\\\ \n \n    & \\textbf{\\name} & \\textbf{84.8} & \\textbf{85.7} & \\textbf{83.4} & \\textbf{79.0} & \\textbf{83.5} & \\textbf{75.2} & \\textbf{79.4} & \\textbf{78.9} & \\textbf{47.5} & \\textbf{57.3} & \\textbf{52.5} \\\\\n\n    \\bottomrule[1.1pt]\n    \\end{tabular}\n}\n\n\\label{tab:ref}\n\\end{table*}\n\n\\begin{table*}[t]\n  \\centering\n  \\caption{ Comparison with the state-of-the-art models on more complex and challenging reasoning segmentation benchmarks: ReVOS in video domain and ReasonSeg in image domain. $\\ddagger$ denotes the same meaning as Tab. \\ref{tab:ref}.\n  Our \\name~outperforms all the previous VLLM-based models in both video and image reasoning segmentation tasks.\n  }\n  \\scalebox{0.87}{\n    \\begin{tabular}{l|c|ccc|ccc|ccc|cc}\n    \\toprule[1.1pt] \n    \\multirow{2}{*}{ Method } & \\multirow{2}{*}{ Backbone } & \\multicolumn{3}{c|}{ ReVOS-Reasoning } & \\multicolumn{3}{c|}{ ReVOS-Referring } & \\multicolumn{3}{c|}{ ReVOS-Overall } & \\multicolumn{2}{c}{ ReasonSeg }\\\\\n    \\cline { 3 - 13 } & & $\\mathcal{J}$ & $\\mathcal{F}$ &  $\\mathcal{J\\&F}$ & $\\mathcal{J}$ & $\\mathcal{F}$ &  $\\mathcal{J\\&F}$ & $\\mathcal{J}$ & $\\mathcal{F}$ &  $\\mathcal{J\\&F}$ & gIoU & cIoU \\\\\n    \\midrule[0.7pt]\n\n    LMPM~\\cite{ding2023mevis} & Swin-T & 13.3 & 24.3 & 18.8 & 29.0 & 39.1 & 34.1 &  21.2 & 31.7 & 26.4 & - & - \\\\\n\n    ReferFormer~\\cite{wu2022language} & Video-Swin-B & 21.3 & 25.6 & 23.4 & 31.2 & 34.3 & 32.7 &  26.2 & 29.9 & 28.1 & - & - \\\\\n\n     LISA-7B~\\cite{Lai2023LISARS} $\\ddagger$ & ViT-H & 33.8 & 38.4 & 36.1 & 44.3 & 47.1 & 45.7 &  39.1 & 42.7 & 40.9 & 52.9 & 54.0\\\\\n    LaSagnA-7B~\\cite{wei2024lasagna} $\\ddagger$ & ViT-H & - & - & - & - & - & - & - & - & - & 48.8 & 47.2\\\\\n    SAM4MLLM-7B~\\cite{chen2025sam4mllm} $\\ddagger$ & EfficientViT-SAM-XL1  & - & - & - & - & - & - & - & - & - & 46.7 & 48.1\\\\\n    TrackGPT-13B~\\cite{zhu2023tracking} $\\ddagger$ & ViT-H & 38.1  & 42.9 & 40.5  & 48.3  & 50.6  & 49.5   & 43.2  & 46.8  & 45.0  & - & - \\\\\n    VISA-7B ~\\cite{yan2024visa} $\\ddagger$ & ViT-H & 36.7  & 41.7 & 39.2  & 51.1  & 54.7  & 52.9   & 43.9  & 48.2  & 46.1  & 52.7 & \\textbf{57.8} \\\\\n    VISA-13B ~\\cite{yan2024visa} $\\ddagger$ & ViT-H & 38.3  & 43.5 & 40.9  & 52.3  & 55.8  & 54.1   & 45.3  & 49.7  & 47.5  & - & - \\\\\n\n    \\midrule[0.5pt] \n\n   \n    \\textbf{\\name-3B} & Swin-B\n & \\textbf{50.2} & \\textbf{55.8} & \\textbf{53.0} & \\textbf{56.0} & \\textbf{60.9} & \\textbf{58.5} & \\textbf{53.1} & \\textbf{58.4}  & \\textbf{55.7} & \\textbf{59.2} & 56.7 \\\\\n\n    \\bottomrule[1.1pt]\n    \\end{tabular}\n}\n\n\\label{tab:reason}\n\\end{table*}\n\n\\subsection{Fine-grained Visual Perceiver}\n\\label{subsec:fvp}\n\\textbf{Why twin-tower vision encoder}?\nAs shown in Fig. \\ref{fig:fvp}, previous VLLMs and VLLM-based segmentation methods usually utilize the pre-trained CLIP encoder to obtain single-scale and low-resolution vision features interacted with diverse languages, which is insufficient for fine-grained image and video segmentation tasks.\nTherefore, we adopt an extra pyramid vision encoder~\\cite{cheng2022masked} to inject details-aware visual information. \n\nSpecifically, we fuse multi-scale visual features into fine-grained tokens (stated as $P$ in Sec \\ref{subsec:overview}) which can inject rich fine-grained visual information into the pre-trained VLLMs without excessive computation cost.\nFormally, given the vision input $\\mathcal{V}$, we leverage a pyramid vision encoder~\\cite{cheng2022masked} $F_{seg}$ \nto get details-aware image features $f_{img}$. For the j-th scale and the previous fine-grained tokens $P_{j-1}$, the FVP module enriches each token through conditional weighted cross-attention: \n\n\\begin{equation}\n         \\hat{P}_{j}=\\textrm{MHCA}(P_{j-1}, G_{p}(f_{img}^{(j)})), \n    \\label{eqn:gate1}\n\\end{equation}\n\\vspace{-5mm}\n\\begin{equation}\n         P_{j}=P_{j-1} + \\textrm{tanh}(\\textrm{MLP}(\\hat{P}_{j})) \\cdot \\hat{P}_{j},\n    \\label{eqn:gate2}\n\\end{equation}\nwhere $\\textrm{MHCA}$ denotes the Multi-Head Cross-Attention layer, $G_p$ is the projection function, $\\textrm{tanh}$ is a normalization function and $\\textrm{MLP}$ is a Multilayer Perceptron. The component of $\\textrm{tanh}(\\textrm{MLP}(\\hat{P}_{j}))$ is the \\emph{conditional weight} used to multiply the enriched fine-grained tokens $\\hat{P}_{j}$ before the residual connection to the previous tokens $P_{j-1}$. Additionally, we initialize the weight value to zero to ensure the adaptation to diverse multi-scale image features while retaining the training stability.\n\n \n\n\\subsection{Temporal Adapter}\n\\label{subsec:videoadapt}\nVideo segmentation entails distinct challenges, requiring reasoning across multiple frames and the maintenance of temporal coherence. Existing VLLM-based methods exhibit limitations in addressing video perception tasks and lack specialized designs for comprehending temporal dynamics in video analysis.\nTo this end, we utilize global prompt aggregation and local space-time information injection in the time dimension to adapt to more complicated video perception tasks. \n\n\\noindent \\textbf{Global prompt aggregation}.\nFor the current prompt embedding $E_{\\mathcal{P}}$ in the video object mask retrieval process, we leverage the adaptive average pooling strategy along the time dimension to aggregate global object and temporal information of previous $T$ frames. \n\\begin{equation} \\label{eq1}\nE_{\\mathcal{P}} =  AvgPool([E_{\\mathcal{P}}^0, E_{\\mathcal{P}}^1, ... , E_{\\mathcal{P}}^T]),\n\\end{equation}\n\n\\noindent \\textbf{Local space-time information injection}.\nWe propose a sequential renewal strategy for space-time information propagation based on fine-grained tokens $P$ to inject object information of adjacent frames. Formally,\n\\begin{equation} \\label{eq2}\nP_t =  G_{l}[F_{LLM}(P_{t-1})],\n\\end{equation}\nwhere $P_t$ denotes the time-aware fine-grained tokens of the current $t$-th frame, $G_{l}$ is the projection function to transfer the previous features to the current space and align the feature dimensions.\n\nThe proposed global prompt aggregation and local space-time information injection within our temporal adapter facilitate the coalescence of both long-term and short-term vision-language information, which is essential for comprehensive video perception.\n\n\\begin{table*}[t]\n  \\centering\n    \\caption{ Quantitative results on the closed-set COCO-Panoptic segmentation, open-vocabulary segmentation (-OV) benchmarks. Our model \\name~achieves remarkable performance compared with the previous state-of-the-art methods.}\n\\scalebox{0.81}{\n  \\begin{tabular}{c|l|c|cc|cc|c|c|c}\n    \\toprule[1.1pt]\n    \\multirow{2}{*}{Type} & \\multirow{2}{*}{Method} & \\multirow{2}{*}{Backbone} & \\multicolumn{2}{c|}{COCO-Panoptic} & \\multicolumn{2}{c|}{ADE-OV} & \\multicolumn{1}{c|}{Citys-OV} & \\multicolumn{1}{c|}{PC59-OV} & \\multicolumn{1}{c}{PAS20-OV} \\\\\n    \\cline { 4 - 10 } & & & PQ & mIoU & PQ & mIoU & PQ & mIoU & mIoU  \\\\\n    \\midrule[0.7pt]\n    \\multirow{7}{*}\n    {\\shortstack{Segmentation \\\\ Specialist}}  &  Mask2former~\\cite{cheng2022masked} & Swin-B  & 55.1 &  65.1 & - & - & - & - & - \\\\\n    & OneFormer~\\cite{jain2023oneformer} & Swin-L  & 57.9  & 67.4 & - & - & - & - & - \\\\\n    & SEEM~\\cite{seem}  &  DaViT-B & 56.1  & 66.3 & - & - & - & - & -  \\\\\n    & MaskCLIP~\\cite{maskclip} & ViT-L & 30.9 & 47.6 & 15.1 & \\textbf{23.7} & - & 45.9 & - \\\\\n    & DeOP~\\cite{han2023open} & ResNet-101c & - & - & - & 22.9 & - & 48.8 & 91.7 \\\\\n\n    \n\n    & SimBaseline~\\cite{xu2022simple} & ViT-B & - & - & - & 20.5 & - & 47.7 & 88.4 \\\\\n\n    \n    & DaTaSeg~\\cite{gu2024dataseg}   &  ViTDet-B  & 52.8  & 62.7 & 12.3 & 18.3 & 28.0 & 51.1 & - \\\\\n\n    \n    \\midrule[0.5pt]\n    \\multirow{3}{*}{\\shortstack{MLLM-based\\\\Segmentation Network}}\n    & OMG-LLaVA ~\\cite{zhang2024omg} & ConvNeXt-L & 53.8 &  - & - & - & - & - & - \\\\\n    & PSALM ~\\cite{Lai2023LISARS} & Swin-B & 55.9 & 66.6 & 13.7 & 18.2 & 28.8 & 48.5 & 81.3 \\\\\n    & \\textbf{\\name} & Swin-B & \\textbf{61.2} & \\textbf{77.2} & \\textbf{16.1} & 22.3 & \\textbf{31.1} & \\textbf{64.6} & \\textbf{92.1} \\\\\n    \\bottomrule[1.1pt]\n  \\end{tabular}\n}\n  \\label{tab:cocoseg}\n\\end{table*}\n\n\\begin{table}[t]\n  \\centering\n  \\caption{ Results of common video segmentation benchmarks, including DAVIS17, Ref-YouTube-VOS, Ref-DAVIS17, and YouTube-VIS 2019. $\\ddagger$ denotes the same meaning as Tab. \\ref{tab:ref}.\n  }\n  \\scalebox{0.64}{\n    \\begin{tabular}{l|c|c|c|c|c}\n    \\toprule[1.1pt] \n    \\multirow{2}{*}{ Method } & \n    \\multirow{2}{*}{ Backbone} & \n    \\multicolumn{1}{c|}{  DAVIS17 } &\n    \\multicolumn{1}{c|}{  Ref-YT } & \\multicolumn{1}{c|}{ Ref-DAVIS }  & \\multicolumn{1}{c}{ YT-VIS } \\\\\n    \\cline { 3 - 6 } & & $\\mathcal{J\\&F}$ & $\\mathcal{J\\&F}$ & $\\mathcal{J\\&F}$ & mAP  \\\\\n    \\midrule[0.7pt]\n    SEEM~\\cite{seem} & DaViT-B & 62.8 & - & - & -   \\\\\n    \n    OMG-Seg~\\cite{li2024omg}  & ConvNeXt-L & 74.3 & - & - & 56.4  \\\\\n\n    ReferFormer~\\cite{wu2022language}  & Video-Swin-B & - & 62.9 & 61.1 & -  \\\\\n    OnlineRefer~\\cite{wu2023onlinerefer}  & Swin-L & - & 63.5 & 64.8 & -  \\\\\n    UNINEXT~\\cite{lin2023uninext}  & ConvNeXt-L & 77.2 & 66.2 & 66.7 & \\textbf{64.3}  \\\\\n\n    \\midrule[0.5pt]\n\n    LISA-7B~\\cite{Lai2023LISARS} $\\ddagger$ & ViT-H & - & 53.9 & 64.8 & -  \\\\\n    VISA-13B~\\cite{yan2024visa} $\\ddagger$ & ViT-H & - & 63.0 & 70.4 & -  \\\\\n    VideoLISA-3.8B~\\cite{bai2024one} $\\ddagger$ & ViT-H & - & 63.7 & 68.8 & -  \\\\\n    \n     \\textbf{\\name-3B} & Swin-B & \\textbf{77.6} & \\textbf{68.5} & \\textbf{71.2} & 53.8 \\\\\n    \n    \\bottomrule[1.1pt]\n    \\end{tabular}\n    }\n\n\\vspace{-3mm}\n  \n\\label{tab:exp-video}\n\\end{table}\n\\section{Experiments}\n\\textbf{Datasets}.\nWe use the one-stage training strategy to train \\name~with the multi-dataset and multi-task manners. For image segmentation, we use COCO Panoptic~\\cite{Lin2014MicrosoftCC}, RefCOCO series~\\cite{yu2016modeling,nagaraja2016modeling}, COCO-Interactive, and ReasonSeg~\\cite{Lai2023LISARS}.\nFor video segmentation, we utilize DAVIS-2017 datasets~\\cite{caelles20182018}, Ref-Youtube-VOS~\\cite{seo2020urvos}, YouTube-VIS 2019~\\cite{yang2019video}, and ReVOS~\\cite{yan2024visa}. Besides, we use LLAVA-150k~\\cite{liu2024visual} to maintain the vision-language conversation capability of VLLM (we show the results on Multi-modal benchmarks in the supplementary material).\n\n    \n\n    \n\n\\noindent \\textbf{Implementation details}\nWe load the pre-trained weights of Mipha~\\cite{zhu2024comprehensive} for our VLLM, and Maks2Former~\\cite{cheng2022masked} for our segmentation predictor. We use three layers of FVP for fine-grained information fusion and utilize LoRA~\\cite{hu2021lora} to finetune the LLM efficiently. We train \\name~on all the tasks jointly for 160k iterations using a batch size of 32 on 8 NVIDIA A100 GPUs, which means each task takes approximately 16k iterations. We employ the AdamW optimizer with a learning rate of $4\\times10^{-5}$ and with a cosine schedule. All the hyper-parameters in the loss $\\mathcal{L}$are assigned values 1.0.\n\n\\begin{table*}[t]\n  \\centering\n  \\caption{\n  The mutual influence between different tasks. Task-specific means training task-specific models only on data from corresponding tasks, Refer+Reason denotes the model is trained on referring and reasoning segmentation data, and Video and Image denote different training visual types: training on video data and image data, respectively.\n  }\n  \\scalebox{0.76}{\n    \\begin{tabular}{c|c|c|c|ccc|cc|ccc|c}\n    \\toprule[1.1pt] \n    \\multirow{2}{*}{Task-specific} & \n    \\multirow{2}{*}{ Refer+Reason } &\n    \\multirow{2}{*}{ Video } &\n    \\multirow{2}{*}{ Image } &\n    \\multicolumn{3}{c|}{  RefCOCO } &\n    \\multicolumn{2}{c|}{  COCO } & \\multicolumn{3}{c|}{ ReVOS }  & \\multicolumn{1}{c}{ YT-VIS } \\\\\n    \\cline { 5 - 13 } & & & & val & testA & testB & PQ & mIoU & Reasoning & Referring & Overall & mAP  \\\\\n    \\midrule[0.7pt]\n    \\Checkmark &  &  &  & 83.8 & 85.9 & 82.2 & 60.8 & 75.1 & 51.2 & 56.6 & 53.9 & 50.7  \\\\\n     & \\Checkmark &  &  & 83.3 & 84.9 & 80.9 & - & - & 53.1 & 57.3 & 55.2 & -  \\\\\n     &  &  & \\Checkmark & 85.6 & 86.1 & 82.4 & 60.9 & 76.5 & - & - & - & -  \\\\\n     &  & \\Checkmark &  & - & - & - & - & - & 51.1 & 57.0 & 54.1 & 50.4  \\\\\n    \n     & \\Checkmark & \\Checkmark & \\Checkmark & 84.8 & 85.7 & 83.4 & 61.2 & 77.2 & 53.0 & 58.5 & 55.7 & 53.8 \\\\ \n    \n    \\bottomrule[1.1pt]\n    \\end{tabular}\n    }\n  \n\\label{tab:ab-data}\n\\end{table*}\n\n\\begin{table*}[t]\n  \\centering\n  \\caption{ The comparison of different LLMs and backbone usages. w/o CLIP means without using CLIP vision encoder. \n  }\n  \\scalebox{0.8}{\n    \\begin{tabular}{c|c|cc|ccc|c|c|c}\n    \\toprule[1.1pt]  \n    \\multirow{2}{*}{ Method} & \n    \\multirow{2}{*}{ LLM } & \n    \\multicolumn{2}{c|}{  COCO } & \\multicolumn{3}{c|}{ ReVOS } &\n    \\multicolumn{1}{c|}{  ADE-OV } &\n    \\multicolumn{1}{c|}{  PC59-OV  } &\n    \\multicolumn{1}{c}{  PAS20-OV  } \\\\\n    \\cline { 3 - 10 }  & & PQ & mIoU & Reasoning & Referring & Overall & mIoU & mIoU & mIoU \\\\\n    \\midrule[0.7pt]\n\n    LISA~\\cite{Lai2023LISARS} & Vicuna-7B & - & - & 36.1 & 45.7 & 40.9 & - & - & - \\\\\n     VISA ~\\cite{yan2024visa} & Vicuna-13B & - & - & 40.9 & 54.1 & 47.5 & - & - & - \\\\\n    PSALM(w/o CLIP) ~\\cite{Lai2023LISARS} & Phi-1.5-1.3B & 55.9 & 66.6 & - & - & - & 18.2 & 48.5 & 81.3 \\\\\n\n    \\midrule[0.5pt]\n\n    \\name~(w/o CLIP) & Phi-1.5-1.3B & 61.1 & 76.0 & 44.0 & 49.7 & 46.9 & 18.9 & 60.0 & 90.6 \\\\\n    \\name & Phi-1.5-1.3B & 60.9 & 76.7 & 50.8 & 57.0 & 53.9 & 20.3 &61.5 & 90.8 \\\\\n    \\name & Phi-2-2.7B & \\textbf{61.2} & \\textbf{77.2} & \\textbf{53.0} &  \\textbf{58.5} & \\textbf{55.7} & \\textbf{22.3} & \\textbf{64.6} & \\textbf{92.1} \\\\ \n    \n    \n    \\bottomrule[1.1pt]\n    \\end{tabular}\n    }\n    \n\\vspace{-2mm}\n\n  \n\\label{tab:ab-llms}\n\\end{table*}\n\n\\begin{table}[t]\n  \\centering\n  \\caption{ Ablation on the core components of \\name. \n  FVP and HER denote the proposed Fine-grained Visual Perceiver and Hybrid Entity Recognition modules.\n  }\n  \\scalebox{0.9}{\n    \\begin{tabular}{c|c|c|cc|c}\n    \\toprule[1.1pt] \n    \\multirow{2}{*}{ FVP} & \n    \\multirow{2}{*}{ HER } & \n    \\multicolumn{1}{c|}{  YT-VIS } &\n    \\multicolumn{2}{c|}{  COCO } &  \\multicolumn{1}{c}{ RefCOCO } \\\\\n    \\cline { 3 - 6 }  & & mAP & PQ & mIoU & cIoU  \\\\\n    \\midrule[0.7pt]\n\n    \n    &  & 48.4 & 54.8 & 66.2  & 82.8  \\\\\n     \\Checkmark &  & 50.8 & 55.8 & 66.6  &  84.6 \\\\\n     & \\Checkmark & 52.0 & 59.7 & 74.6 & 84.3  \\\\ \n     \\Checkmark & \\Checkmark & \\textbf{53.8} & \\textbf{61.2} & \\textbf{77.2} & \\textbf{84.8} \\\\ \n    \n    \n    \\bottomrule[1.1pt]\n    \\end{tabular}\n    }\n  \n\\label{tab:ab-component}\n\\end{table}\n\n\\subsection{Comparisons with State-of-the-Arts}\n\n\\noindent\\textbf{Referring expression segmentation results}.\nWe compare \\name~with the state-of-the-art methods on the benchmarks RefCOCO/+/g~\\cite{yu2016modeling,nagaraja2016modeling} and more challenging generalized referring expression segmentation benchmark gRefCOCO~\\cite{liu2023gres}. in Tab.~\\ref{tab:ref}. Based on the versatile and adaptable design of \\name, our model achieves state-of-the-art performance on all the referring datasets. Specifically, \\name~surpasses the current SOTA by a large margin, reaching 79.7 cIoU on RefCOCO+ val (+6.8 over PSALM).\nBesides, Our model shows superiority in challenging G-RES tasks compared with previous \\textbf{zero-shot} methods, demonstrating the robustness and generalization ability of~\\name.\n\n\\noindent\\textbf{Reasoning segmentation results}.\nWe compare \\name~with the state-of-the-art methods on image reasoning segmentation (ReasonSeg~\\cite{Lai2023LISARS}) and reasoning video object segmentation (ReVOS~\\cite{yan2024visa}) in Tab.~\\ref{tab:reason}.\nOur \\name~achieves superior performance on reasoning tasks, significantly surpassing previous state-of-the-art methods (+12.1 on ReVOS-Reasoning), which shows \\name~powerful reasoning capability of tackling complex scenarios.\n\n\\noindent\\textbf{Generic image segmentation results}.\nWe show the performance of \\name~on  COCO-Panoptic~\\cite{Lin2014MicrosoftCC} and open-vocabulary segmentation~\\cite{zhou2019semantic,Cordts2016Cityscapes,mottaghi2014role,everingham2010pascal} tasks in Tab.~\\ref{tab:cocoseg}.\n\\name~achieves excellent performance compared with both specialist models and VLLMs-based methods on both closed-set and open-vocabulary segmentation tasks. Specifically, \\name~surpasses the VLLM-based PSALM by a significant margin (+5.3 on COCO PQ, and +10.6 on mIoU), which demonstrates our powerful capabilities of handling complex semantic perception and segmentation tasks.\nBesides, we show the results of COCO-Interactive in the supplementary material.\n\n\\noindent\\textbf{Common video segmentation results}.\nWe compare \\name~with previous video segmentation methods in Tab.~\\ref{tab:exp-video}, including visual-prompted semi-supervised VOS (DAVIS17 val), text-prompted referring video object segmentation (Ref-YouTube-VOS, Ref-DAVIS17) and video instance segmentation (YouTube-VIS 2019).\n\\name~shows promising results over previous unified segmentation methods~\\cite{li2024omg,lin2023uninext}.\nBesides, \\name~ performs more video perception tasks than previous VLLM-based models~\\cite{yan2024visa,bai2024one}.\n\n\\subsection{Ablations}\n\n\\noindent\\textbf{The mutual influence between different tasks.} \nOur model can be trained and inferred across multiple tasks and datasets simultaneously.\nWe evaluate the mutual impact of different tasks in Tab.~\\ref{tab:ab-data}. The results show that joint training can enhance the model performance compared with the task-specific model.\nBesides, the performance of video segmentation tasks can be improved significantly by adding the image training datasets.\nThis demonstrates the generalization and self-consistency of our \\name~to perform universal segmentation. \n\n \n\n\\noindent\\textbf{Effect of different LLMs and vision backbone.} In Tab.~\\ref{tab:ab-llms}, we evaluate the effect of different sizes of LLMs and vision backbone.\nOur \\name~achieves excellent performance using smaller LLMs and vision encoder compared with the previous SOTA models like VISA\\cite{yan2024visa} and PSALM\\cite{zhang2024psalm}.\nBesides, the performance of \\name~can be further improved by using the more powerful LLM (Phi-2-2.7B~\\cite{javaheripi2023phi}). \n\n\\begin{table}[t]\n  \\centering\n  \\caption{ Ablation on the Fine-grained Visual Perceiver design. CW denotes the Conditional Weight illustrated in Sec.~\\ref{subsec:fvp}, and Scale denotes the total scale in the proposed FVP module.\n  }\n  \\scalebox{0.84}{\n    \\begin{tabular}{c|c|c|cc|c}\n    \\toprule[1.1pt] \n    \\multirow{2}{*}{ CW } & \n    \\multirow{2}{*}{ Scale } & \n    \\multicolumn{1}{c|}{  YT-VIS } &\n    \\multicolumn{2}{c|}{  COCO } & \\multicolumn{1}{c}{ RefCOCO } \\\\\n    \\cline { 3 - 6 } & & mAP & PQ & mIoU & cIoU  \\\\\n    \\midrule[0.7pt]\n\n    & single-layer & 49.7 & 55.8 & 68.0  &  83.7 \\\\\n    & multi-layers & 50.4  & 58.9 & 73.4  & 84.5 \\\\\n\n    \\Checkmark & multi-layers & \\textbf{53.8} & \\textbf{61.2} & \\textbf{77.2} & \\textbf{84.8} \\\\ \n\n    \n    \\bottomrule[1.1pt]\n    \\end{tabular}\n    }\n\\vspace{-3mm}\n  \n\\label{tab:ab-fvp}\n\\end{table}\n\n\\noindent\\textbf{Ablation on the proposed components.} We assess the effectiveness of our proposed FVP module and Hybrid Entity Recognition strategy.\nAs shown in Tab.~\\ref{tab:ab-component}, with our fine-grained visual integration and hybrid entity semantic enhancement, the segmentation accuracy can be enhanced significantly (+5.4 on YT-VIS, +6.4 on COCO panoptic PQ).\n\n\\noindent\\textbf{Design of the Fine-grained Visual Perceiver}. In the FVP module, we combine multi-scale visual features into fixed perception queries using the condition-wise cross-attention layers to extract rich visual details from different scales of the pyramid encoder. \nAs shown in Tab.~\\ref{tab:ab-fvp}, together with the conditional weight and the multi-scale design, our model makes a significant improvement on both image and video segmentation tasks.\n\n\\noindent\\textbf{Effect of temporal adapter.} We evaluate the effectiveness of the proposed temporal adapter including global prompt aggregation (global) and local space-time information injection (local) in Tab.~\\ref{tab:ab-temporal}.\nIncorporating both global and local components, the temporal adapter significantly enhances model performance across multiple video segmentation tasks.\n\n\\begin{table}[t]\n  \\centering\n  \\caption{ Ablation on the temporal adapter for video tasks, including global prompt aggregation (global) and local space-time information injection (local).\n  }\n  \\scalebox{0.9}{\n    \\begin{tabular}{c|c|c|c|c}\n    \\toprule[1.1pt] \n    \\multirow{2}{*}{ Global } & \n    \\multirow{2}{*}{ Local } & \n    \\multicolumn{1}{c|}{  Ref-DAVIS17 } &\n   \\multicolumn{1}{c|}{ ReVOS }  & \\multicolumn{1}{c}{ YT-VIS } \\\\\n    \\cline { 3 - 5 } & & $\\mathcal{J\\&F}$  & $\\mathcal{J\\&F}$ & mAP  \\\\\n    \\midrule[0.7pt]\n      &  & 67.3 & 54.1 & 47.9 \\\\\n    \\Checkmark &  & 68.8 & 54.5 & 48.5  \\\\\n     & \\Checkmark & 69.3 & 54.8 & 50.2  \\\\\n    \\Checkmark & \\Checkmark & \\textbf{71.2} & \\textbf{55.7} & \\textbf{53.8}  \\\\\n    \n    \\bottomrule[1.1pt]\n    \\end{tabular}\n    }\n\n\\vspace{-3mm}\n  \n\\label{tab:ab-temporal}\n\\end{table}\n\n\\section{Conclusion}\nIn this study, we aim to present \\name, the first VLLM-based universal segmentation model designed for pixel-level image and video perception, encompassing a wide range of generic segmentation and complex reasoning tasks.\nWe propose the Hybrid Entity Recognition and Fine-grained Visual Perceiver to leverage the recognition capacity of VLLMs more effectively and enhances the VLLM's ability by capturing diverse levels of visual information without incurring excessive computational costs. With additional Temporal Adapter, \\name~can tackle challenging video tasks by incorporating global and local information. \\name~surpasses existing methods on complex reasoning segmentation and traditional perception tasks. The insights presented in this work expand the possibilities of VLLMs in visual perception and lay a foundation for future research on the integration of vision-language models.\n{\n    \\small\n        }\n\n\\clearpage\n\\setcounter{page}{1}\n\\maketitlesupplementary\n\\appendix\n\n\\section{Additional Implementation Details}\n\\label{sec:implement_details}\n\n\\subsection{Evaluation Metrics}\n\nIn our experiments, we use the widely used metrics to evaluate the performance of our \\name~on various segmentation tasks consistent with previous studies. Specifically, cumulative Intersection-over-Union (cIoU) for referring expression segmentation (RES), interactive segmentation, and generalized referring expression segmentation (G-RES), cIoU and the average of all per-image Intersection-over-Unions (gIoU) for reasoning segmentation task, region similarity $\\mathcal{J}$ and contour accuracy $\\mathcal{F}$ for reasoning video object segmentation (ReasonVOS), video object segmentation (VOS), referring video object segmentation (R-VOS), panoptic quality (PQ), mean intersection-over-Union (mIoU) for image generic segmentation, and mean average precision (mAP) for video instance segmentation (VIS).\n\n\\subsection{Training Details}\nIn our experiments, we use Phi-2~\\cite{javaheripi2023phi} with 2.7B parameters as our Large Language Model,  SigLIP~\\cite{zhai2023sigmoid} as our vanilla encoder, and Swin-B~\\cite{liu2021swin} as our pyramid encoder.\nWe use PyTorch to implement our \\name~and use Deepspeed zero-1 optimization for efficient training. \nFurthermore, the vanilla encoder and pyramid encoder are kept frozen, the LLM is finetuned with LORA (rank=8), the FVP, HER, and segmentation predictor are fully trained. Our codes and model weights will be publicly released.\n\n\\section{Additional Experimental Results}\n\n\\subsection{Multi-modal Question Answering Benchmarks}\nOur \\name~is the first VLLM-based universal segmentation model for pixel-level image and video perception with complex reasoning and conversation capabilities, which is capable of tackling vision-language comprehension tasks. \nTherefore, we evaluate our model on various Multi-modal question answering benchmarks. \nAs shown in Tab.~\\ref{tab:mllm_results}, our \\name~achieves comparable performance compared with previous VLLMs like \nInstructBLIP~\\cite{instructblip}, Qwen-VL~\\cite{bai2023qwen}, and LLaVA-1.5~\\cite{liu2024visual} with fewer model parameters, demonstrating the insights into the model’s powerful conversational and reasoning capabilities.\n\n\\begin{table}[ht]\n    \\centering\n    \\caption{Quantitative results of our \\name~on Multi-modal question answering benchmarks. \\name~achieves promising performance compared with previous VLLMs in several widely used Multi-modal benchmarks.}\n    \\scalebox{0.7}{\n    \\begin{tabular}{l|c|c|c|c|c|c}\n    \\toprule[1.1pt]  \n    Method & \n    LLM & \n    MMB  & VQA\\textsuperscript{v2}  &  GQA & POPE & SQA  \\\\\n    \\midrule[0.5pt]\n\n    BLIP-2~\\cite{li2023blip} & Vicuna-13B & - & 65.0 & 41.0 & 85.3 & 61.0 \\\\\n    InstructBLIP~\\cite{instructblip} & Vicuna-7B & 36.0 & - & 49.2 & - & 60.5  \\\\\n    InstructBLIP~\\cite{instructblip} & Vicuna-13B & - & - & 49.5 & 78.9 & 63.1  \\\\\n    Shikra~\\cite{chen2023shikra} & Vicuna-13B & 58.8 & 77.4 & - & - & -  \\\\\n    Qwen-VL~\\cite{bai2023qwen} & Qwen-7B & 38.2 & 78.8 & 59.3 & - & 67.1  \\\\\n    Qwen-VL-Chat~\\cite{bai2023qwen} & Qwen-7B & 60.6 & 78.2 & 57.5 & - & 68.2  \\\\\n    LLaVA-1.5~\\cite{liu2024visual} & Vicuna-7B & 64.3 & 78.5 & 62.0 & 85.9 & 66.8  \\\\\n\n    \\name & Phi-2-2.7B & 67.9 & 78.2 & 60.9 & 86.6 & 66.2 \\\\ \n    \n    \\bottomrule[1.1pt]\n    \\end{tabular}\n    }\n    \n    \\label{tab:mllm_results}\n\\end{table}\n\n\\subsection{Interactive Segmentation}\n\nWe also evaluate \\name~on the COCO-Interactive validation set for the interactive segmentation task. As shown in Tab.~\\ref{tab:interactive_seg}, our \\name~achieves promising performance on various visual prompt types. \nNotably, our model surpasses previous segmentation specialists such as SAM~\\cite{kirillov2023segment}, which utilizes a larger vision backbone and much more high-quality training data, and SEEM~\\cite{seem}. \nHowever, the VLLM-based model PSALM~\\cite{zhang2024psalm} exhibits superior performance in the interactive segmentation task. \nWe hypothesize that this discrepancy arises from differences in feature scale utilization during the visual prompt sampling process: PSALM~\\cite{zhang2024psalm} employs the visual prompt features derived from a high-resolution Swin-based vision encoder, whereas \\name~utilizes features from a more streamlined CLIP-based visual encoder.\n\n\\begin{table}[ht]\n  \\centering\n    \\caption{ Quantitative results on COCO-Interactive benchmark.}\n\\scalebox{0.81}{\n  \\begin{tabular}{l|c|c|c|c|c}\n    \\toprule[1.1pt]\n    Method & Backbone & Box & Scribble & Mask & Point \\\\\n    \\midrule[0.7pt]\n    \n    SAM~\\cite{kirillov2023segment} & ViT-B & \n    68.7 & - & - & 33.6 \\\\\n    SAM~\\cite{kirillov2023segment} & ViT-L & \n    71.6 & - & - & 37.7 \\\\\n    SEEM~\\cite{seem}  &  DaViT-B & \n    42.1 & 44.0 & 65.0 & 57.8 \\\\\n    \n    PSALM ~\\cite{Lai2023LISARS} & Swin-B & \n    80.9 & 80.0 & 82.4 & 74.0 \\\\\n    \\name & Swin-B & \n    77.3 & 75.2 & 79.5 & 63.4 \\\\\n    \\bottomrule[1.1pt]\n  \\end{tabular}\n}\n  \\label{tab:interactive_seg}\n\\end{table}\n\n\\begin{table*}[ht]\n  \\centering\n  \\caption{ The comparison of different settings between our model and previous segmentation specialists and VLLM-based segmentation methods. Generic Seg denotes common class-based segmentation, such as panoptic segmentation and semantic segmentation. Open-set denotes the open-vocabulary segmentation. \\name~can perform more comprehensive segmentation tasks in one model. \n  }\n  \\scalebox{0.65}{\n    \\begin{tabular}{c|l|c|cc|ccccc}\n    \\toprule[1.1pt] \n    \\multirow{2}{*}{ Type } & \\multirow{2}{*}{ Method } & \\multirow{2}{*}{ Multi-task Training } & \\multicolumn{2}{c|}{ Visual Type } & \\multicolumn{5}{c}{ Task Type } \\\\\n     & & & Image-level & Video-level & Referring Seg & Reasoning Seg  & Generic Seg & Interactive Seg & Open-set\\\\\n    \\midrule[0.7pt]\n    \\multirow{10}{*}{\\shortstack{Segmentation\\\\Specialist}}\n    & Mask2former~\\cite{cheng2022masked} &  & \\Checkmark &  & & & \\Checkmark & &  \\\\\n    & OneFormer~\\cite{jain2023oneformer} &  & \\Checkmark &  & & & \\Checkmark & &  \\\\\n    & VLT~\\cite{ding2021vision} &  & \\Checkmark &  & \\Checkmark &   \\\\\n    & LAVT~\\cite{yang2022lavt} &  & \\Checkmark &  & \\Checkmark &   \\\\\n    & PolyFormer~\\cite{liu2023polyformer} &  & \\Checkmark &  & \\Checkmark &   \\\\\n    & ReferFormer~\\cite{wu2022language} &  &  & \\Checkmark & \\Checkmark &   \\\\\n    & OnlineRefer~\\cite{wu2023onlinerefer} &  &  & \\Checkmark & \\Checkmark &   \\\\\n    & SEEM~\\cite{seem} & \\Checkmark & \\Checkmark  & \\Checkmark & \\Checkmark & & \\Checkmark  & \\Checkmark & \\Checkmark  \\\\\n    & UNINEXT~\\cite{lin2023uninext}  & \\Checkmark & \\Checkmark  & \\Checkmark & \\Checkmark & & \\Checkmark  & \\Checkmark & \\Checkmark  \\\\\n    & OMG-Seg~\\cite{li2024omg}& \\Checkmark & \\Checkmark  & \\Checkmark & & & \\Checkmark  & \\Checkmark & \\Checkmark  \\\\\n    \n    \\midrule[0.5pt] \n    \n    \\multirow{7}{*}{\\shortstack{VLLM-based\\\\Segmentation Network}} \n    & LISA~\\cite{Lai2023LISARS} & \\Checkmark & \\Checkmark &  & \\Checkmark & \\Checkmark \\\\\n    & PixelLM~\\cite{Ren2023PixelLMPR} & \\Checkmark & \\Checkmark &  & \\Checkmark & \\Checkmark \\\\\n    & GSVA~\\cite{xia2023gsva} & \\Checkmark & \\Checkmark &  & \\Checkmark &  \\\\\n    & LaSagnA~\\cite{wei2024lasagna} & \\Checkmark & \\Checkmark &  & \\Checkmark & \\Checkmark & \\Checkmark \\\\\n    & OMG-LLaVA ~\\cite{zhang2024omg} & \\Checkmark & \\Checkmark &  & \\Checkmark & & \\Checkmark \\\\\n    & PSALM ~\\cite{zhang2024psalm} & \\Checkmark & \\Checkmark & \\Checkmark & \\Checkmark & & \\Checkmark &  \\Checkmark &  \\Checkmark  \\\\\n     & VISA ~\\cite{yan2024visa} & \\Checkmark & \\Checkmark & \\Checkmark &  \\Checkmark &  \\Checkmark \\\\\n    \n    & \\name~(Ours) & \\Checkmark & \\Checkmark & \\Checkmark &  \\Checkmark &  \\Checkmark & \\Checkmark &  \\Checkmark &  \\Checkmark \\\\\n \n    \n\n    \\bottomrule[1.1pt]\n    \\end{tabular}\n}\n\n\\label{tab:task-compare}\n\\end{table*}\n\n\\section{Comparison of different settings}\n\\label{sec:comparison_setting}\nWe also make setting comparisons between different models and our \\name.  \nAs shown in Tab.~\\ref{tab:task-compare}, \\name~can handle more comprehensive segmentation tasks than previous segmentation specialists and MLLM-based methods. Firstly, \\name~can tackle both image-level and video-level perception tasks in one model enjoying the benefits of multi-task joint training.\nSecondly, \\name~performs various segmentation tasks, including long-text prompted referring and reasoning segmentation, category prompted generic segmentation, visual prompted interactive segmentation, and open-vocabulary segmentation.\n\n\\section{Qualitative Results}\nIn this section, we present more qualitative results to better demonstrate the segmentation capabilities of our \\name~involving various tasks in image and video domains.\n\n\\subsection{Referring Expression Segmentation (RES)}\nFig. \\ref{fig:visualize_res} shows the visualization of \\name~on referring segmentation benchmarks (RefCOCO/+/g).\nOur model can effectively grasp the true meaning conveyed by the referring text and provide accurate pixel-level segmentation masks.\n\n\\subsection{Interactive Segmentation}\nFig. \\ref{fig:visualize_interact} presents the effectiveness of our \\name~in understanding the visual prompt and outputting the corresponding segmentation masks for the interactive segmentation tasks. \n\n\\subsection{Panoptic Segmentation}\nFig. \\ref{fig:visualize_pano} shows the qualitative results of \\name~in panoptic segmentation tasks, which needs\nboth semantic and instance level dense predictions.\n\n\\subsection{Reasoning Segmentation}\nFig. \\ref{fig:visualize_reason} presents the effectiveness of our \\name~in understanding the complex question and perform segmentation according to the reasoning process. \n\n\\subsection{Reasoning Video Object Segmentation (ReasonVOS)}\nFig. \\ref{fig:visualize_revos} shows the effectiveness of \\name~in comprehending both the reasoning questions and temporal coherence. \\name~is capable of producing segmentation masks that maintain consistency across temporal sequences.\n\n\\subsection{Video Object Segmentation (VOS)}\nThe qualitative results of our method, \\name, are illustrated in Fig. \\ref{fig:visualize_vos}, demonstrating its capability in interpreting the visual prompt, provided by the ground truth object masks of the first frame, and producing accurate segmentation masks that maintain temporal consistency.\n\n\\subsection{Video Instance Segmentation (VIS)}\nFig. \\ref{fig:visualize_vis} illustrates the effectiveness of \\name~in performing instance-level video segmentation with class prompts, and executing accurate segmentation with instance tracking throughout the entire video.\n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\textwidth]{fig/visualize_res.pdf}\n    \\caption{Qualitative results of \\name’s capability in referring expression segmentation. \n    }\n    \\label{fig:visualize_res}\n\\end{figure*}\n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\textwidth]{fig/visualize_interact.pdf}\n    \\caption{Qualitative results of \\name~in interactive segmentation. The green marker indicates the provided visual prompts, such as point and scribble.\n    }\n    \\label{fig:visualize_interact}\n\\end{figure*}\n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\textwidth]{fig/visualize_pano.pdf}\n    \\caption{Qualitative results of \\name~in panoptic segmentation.\n    }\n    \\label{fig:visualize_pano}\n\\end{figure*}\n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\textwidth]{fig/visualize_reason.pdf}\n    \\caption{Qualitative results of \\name~in \n    reasoning segmentation.\n    }\n    \\label{fig:visualize_reason}\n\\end{figure*}\n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\textwidth]{fig/visualize_revos.pdf}\n    \\caption{Qualitative results of \\name~demonstrate its capability in the complex reasoning video object segmentation task, effectively managing challenging video data and producing temporally consistent results following the reasoning process.\n    }\n    \\label{fig:visualize_revos}\n\\end{figure*}\n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\textwidth]{fig/visualize_vos.pdf}\n    \\caption{Qualitative results of \\name~in semi-supervised video object segmentation tasks. With the visual prompts provided by the ground truth object masks of the first frame, \\name~demonstrates its ability to achieve accurate segmentation while maintaining temporal consistency.\n    }\n    \\label{fig:visualize_vos}\n\\end{figure*}\n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\textwidth]{fig/visualize_vis.pdf}\n    \\caption{Qualitative results of \\name~in video instance segmentation tasks. Utilizing the class text prompts and instance tracking strategies, \\name~exhibits its capability to achieve precise segmentation while ensuring temporal consistency.\n    }\n    \\label{fig:visualize_vis}\n\\end{figure*}\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2403.14598v1.tex",
        "arXiv-2407.17331v2.tex",
        "arXiv-2411.17606v2.tex"
    ],
    "group_id": "group_106",
    "response": "### Title: Advances in Visual Segmentation Using Large Language Models (VLLMs)\n\n### Introduction\nVisual segmentation, a core task in computer vision, involves the identification and delineation of objects within images or video frames. Traditionally, segmentation models have been designed to handle specific tasks such as semantic segmentation, instance segmentation, and panoptic segmentation, often requiring specialized architectures and training datasets. However, recent advancements in Large Language Models (LLMs) have opened up new avenues for integrating language understanding into visual segmentation tasks, leading to the emergence of Visual Large Language Models (VLLMs). VLLMs, which combine the capabilities of LLMs and visual encoders, have demonstrated remarkable success in various vision-language tasks, but their application to pixel-level segmentation tasks remains limited. This limitation is primarily due to the discrete token output nature of LLMs, which makes it challenging to directly generate segmentation masks.\n\nThe field of VLLM-based segmentation has seen significant progress with the introduction of methods like PSALM, MLCD, and HyperSeg. These methods aim to extend the capabilities of VLLMs to handle a broader range of segmentation tasks, including image and video segmentation, interactive segmentation, and reasoning tasks. Despite these advancements, challenges remain in effectively capturing fine-grained visual information, handling multi-label signals, and achieving robust performance across diverse tasks without additional fine-tuning.\n\n### Main Content of Each Paper\n\n#### PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model\nPSALM, proposed by Zheng Zhang et al., is a method that extends the capabilities of LMMs (Large Multi-Modal Models) to address pixel-level segmentation tasks. The authors introduce a mask decoder and a flexible input schema to enable the model to generate and classify segmentation masks. The input schema includes images, task instructions, conditional prompts, and mask tokens, which are processed by the LMM and then used by the mask decoder to produce mask proposals. The key innovation of PSALM lies in its ability to handle multiple segmentation tasks within a single model, such as panoptic segmentation, referring segmentation, and interactive segmentation. PSALM achieves superior results on several benchmarks, including RefCOCO, COCO Panoptic Segmentation, and COCO-Interactive, and demonstrates zero-shot capabilities on unseen tasks like open-vocabulary segmentation and video object segmentation.\n\nThe PSALM architecture is built on LMMs, which are typically designed for text output tasks. To overcome the limitation of generating textual outputs, PSALM introduces a mask decoder that takes the output embeddings from the LMM and generates segmentation masks. The authors also explore the design of condition prompts, which provide necessary information for different tasks and are used as classifier weights to predict the class of each mask proposal. PSALM employs a two-stage training strategy: the first stage aligns visual features to the text input space, and the second stage fine-tunes the mask generator on various segmentation tasks. The authors find that joint training across multiple tasks significantly improves performance compared to task-specific training.\n\n#### Multi-label Cluster Discrimination (MLCD) for Visual Representation Learning\nXiang An et al. introduce MLCD, a novel method for enhancing visual representation learning through multi-label cluster discrimination. MLCD addresses the limitation of existing cluster discrimination methods, which only define a single pseudo-label for each image, by predicting multiple labels for each image based on the features from a pre-trained CLIP model. The authors propose a multi-label classification loss that separates losses from positive and negative classes, thereby alleviating ambiguity on the decision boundary. MLCD is validated on different scales of models and pre-training datasets, demonstrating state-of-the-art performance on multiple downstream tasks, including linear probe, zero-shot classification, and image-text retrieval.\n\nThe method involves two primary stages: clustering and discrimination. During the clustering phase, each image is assigned multiple pseudo-class labels based on the similarity to one million class centers. In the discrimination phase, a multi-label classification loss is employed to optimize the intra-class and inter-class similarities. The authors find that increasing the number of positive labels per sample improves performance, but excessive positive labels introduce noise. They also demonstrate that MLCD can be effectively scaled up by increasing the number of classes and the size of the vision model. The results show that MLCD outperforms existing instance and cluster discrimination methods on various datasets, indicating its effectiveness in capturing fine-grained visual signals and improving feature representation.\n\n#### HyperSeg: Towards Universal Visual Segmentation with Large Language Model\nHyperSeg, proposed by the authors of the third paper, is the first VLLM-based universal segmentation model designed for both image and video perception tasks. HyperSeg aims to leverage the reasoning capabilities of VLLMs to handle generic segmentation tasks and more complex reasoning perception tasks. The model incorporates a hybrid entity recognition strategy and a fine-grained visual perceiver (FVP) module to enhance the understanding of fine-grained visual details. Additionally, HyperSeg uses a temporal adapter to incorporate global and local space-time information for video segmentation tasks.\n\nThe hybrid entity recognition strategy ensures that the mask tokens generated by the VLLM are semantically enhanced, allowing the model to better understand and classify the segmentation masks. The FVP module fuses multi-scale visual features into fine-grained tokens, enabling the model to capture rich visual details without incurring excessive computational costs. The temporal adapter includes global prompt aggregation and local space-time information injection, which are crucial for handling video perception tasks. HyperSeg is trained on diverse datasets and tasks, including COCO Panoptic, RefCOCO, COCO-Interactive, and ReasonSeg, and demonstrates superior performance on both image and video segmentation benchmarks compared to existing methods.\n\n### Commonalities and Innovations\nAll three papers focus on extending the capabilities of VLLMs to handle segmentation tasks, which traditionally require specialized architectures and training datasets. They introduce novel methods to address the limitations of VLLMs in generating pixel-level segmentation masks and handling multi-label signals. PSALM introduces a mask decoder and a flexible input schema to unify various segmentation tasks, while MLCD proposes a multi-label cluster discrimination method to enhance visual representation learning. HyperSeg, on the other hand, incorporates a hybrid entity recognition strategy and a fine-grained visual perceiver module to capture fine-grained visual details and uses a temporal adapter for video segmentation tasks.\n\nThe common theme across these papers is the utilization of VLLMs to improve the performance of segmentation tasks. However, each paper introduces unique innovations. PSALM decouples mask generation and classification, leading to better performance on tasks that require predicting category or confidence scores. MLCD introduces a multi-label classification loss to effectively handle multiple visual signals in one image, thereby improving the intra-class compactness and inter-class separability. HyperSeg leverages a hybrid entity recognition strategy and a fine-grained visual perceiver module to fully exploit the recognition capabilities of VLLMs, and uses a temporal adapter to handle video segmentation tasks.\n\n### Comparison of Results\nThe performance of the proposed methods is evaluated on various benchmarks and datasets. PSALM achieves state-of-the-art performance on RefCOCO, RefCOCO+, and RefCOCOg, and competitive performance on COCO Panoptic Segmentation and COCO-Interactive. MLCD outperforms existing instance and cluster discrimination methods on linear probe, zero-shot classification, and image-text retrieval tasks. HyperSeg surpasses existing VLLM-based methods on closed-set and open-vocabulary segmentation tasks, as well as reasoning segmentation and video object segmentation benchmarks.\n\nTable \\ref{tab:ref} compares the performance of PSALM, MLCD, and other methods on closed-set referring segmentation benchmarks, such as RefCOCO and RefCOCOg. PSALM achieves the best performance, with cIoU scores of 84.8, 85.7, and 83.4 on RefCOCO val, testA, and testB, respectively. MLCD also shows significant improvements over existing methods, with cIoU scores of 83.3, 84.6, and 80.9 on RefCOCO val, testA, and testB, respectively. Table \\ref{tab:reason} compares the performance of HyperSeg with other methods on reasoning segmentation benchmarks, such as ReasonSeg and ReVOS. HyperSeg achieves the best performance, with cIoU scores of 53.0, 58.5, and 55.7 on ReasonSeg, ReVOS Reasoning, and ReVOS Overall, respectively. Table \\ref{tab:cocoseg} compares the performance of HyperSeg with other methods on COCO Panoptic Segmentation and open-vocabulary instance segmentation benchmarks. HyperSeg achieves the best performance, with PQ scores of 61.2 and 16.1 on COCO Panoptic and ADE-OV, respectively.\n\n### Conclusion\nThe main findings of these papers highlight the potential of VLLMs in addressing segmentation tasks. PSALM demonstrates the effectiveness of extending LMMs to handle pixel-level segmentation tasks through a mask decoder and flexible input schema. MLCD introduces a multi-label cluster discrimination method to enhance visual representation learning, showing significant improvements over existing instance and cluster discrimination methods. HyperSeg proposes a hybrid entity recognition strategy and a fine-grained visual perceiver module to fully exploit the recognition capabilities of VLLMs, and uses a temporal adapter to handle video segmentation tasks. These methods collectively showcase the versatility and adaptability of VLLMs in handling diverse segmentation tasks, including image and video perception, interactive segmentation, and reasoning tasks.\n\nFuture research directions could focus on further improving the performance of VLLMs on segmentation tasks by incorporating more sophisticated reasoning and perception capabilities. Additionally, exploring the use of more diverse and larger datasets for training could enhance the generalization and robustness of these models. The integration of OCR models for better handling of text-related tasks could also be a promising avenue for future work. Overall, these advancements pave the way for a unified and powerful framework for visual segmentation tasks, leveraging the strengths of VLLMs in understanding complex vision-language correlations.\n\n### Tables\nTo better illustrate the performance comparisons, the following tables are provided:\n\n\\begin{table}[h!]\n    \\centering\n    \\footnotesize\n    \\caption{Performance comparison on closed-set referring segmentation benchmarks.}\n    \\scalebox{0.85}{\n    \\begin{tabular}{l|ccc|ccc|c}\n    \\toprule\n    \\multirow{2}{*}{Method} & \\multicolumn{3}{c|}{RefCOCO} & \\multicolumn{3}{c|}{RefCOCO+} & \\multicolumn{1}{c}{RefCOCOg} \\\\\n    \\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-8}\n    & val & testA & testB & val & testA & testB & val \\\\\n    \\midrule\n    PSALM & 83.6 & 84.7 & 81.6 & 72.9 & 75.5 & 70.1 & 73.8 \\\\\n    MLCD & 84.8 & 85.7 & 83.4 & 79.0 & 83.5 & 75.2 & 79.4 \\\\\n    \\bottomrule\n    \\end{tabular}\n}\n\\label{tab:ref}\n\\end{table}\n\n\\begin{table}[h!]\n    \\centering\n    \\footnotesize\n    \\caption{Performance comparison on reasoning segmentation benchmarks.}\n    \\scalebox{0.85}{\n    \\begin{tabular}{l|ccc|ccc|ccc}\n    \\toprule\n    \\multirow{2}{*}{Method} & \\multicolumn{3}{c|}{ReasonSeg} & \\multicolumn{3}{c|}{ReVOS Reasoning} & \\multicolumn{3}{c}{ReVOS Referring} \\\\\n    \\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10}\n    & cIoU & gIoU & mAP & $\\mathcal{J}$ & $\\mathcal{F}$ & $\\mathcal{J\\&F}$ & $\\mathcal{J}$ & $\\mathcal{F}$ & $\\mathcal{J\\&F}$ \\\\\n    \\midrule\n    MLCD & 53.0 & 58.5 & 55.7 & 50.2 & 55.8 & 53.0 & 56.0 & 60.9 & 58.5 \\\\\n    PSALM & 42.0 & 52.4 & 50.6 & 33.8 & 38.4 & 36.1 & 44.3 & 47.1 & 45.7 \\\\\n    \\bottomrule\n    \\end{tabular}\n}\n\\label{tab:reason}\n\\end{table}\n\n\\begin{table}[h!]\n    \\centering\n    \\footnotesize\n    \\caption{Performance comparison on COCO Panoptic Segmentation and open-vocabulary instance segmentation benchmarks.}\n    \\scalebox{0.85}{\n    \\begin{tabular}{l|ccc|ccc}\n    \\toprule\n    \\multirow{2}{*}{Method} & \\multicolumn{3}{c|}{COCO Panoptic} & \\multicolumn{3}{c}{ADE-OV} \\\\\n    \\cmidrule(lr){2-4} \\cmidrule(lr){5-7}\n    & PQ & mIoU & mAP & PQ & mIoU & mAP \\\\\n    \\midrule\n    PSALM & 55.9 & 66.6 & 45.7 & 13.7 & 18.2 & 28.8 \\\\\n    MLCD & - & - & - & - & - & - \\\\\n    HyperSeg & 61.2 & 77.2 & 66.2 & 16.1 & 22.3 & 31.1 \\\\\n    \\bottomrule\n    \\end{tabular}\n}\n\\label{tab:cocoseg}\n\\end{table}\n\n### Figures\nTo further illustrate the effectiveness of these methods, the following figures are provided:\n\n\\begin{figure}[!t]\n  \\centering\n  \\includegraphics[width=1.\\textwidth]{images/teaser_v4.pdf}\n  \\caption{PSALM's capability to handle multiple segmentation tasks in a single model.}\n  \\label{fig:teaser}\n\\end{figure}\n\n\\begin{figure}[!t]\n  \\centering\n  \\includegraphics[width=0.95\\textwidth]{figs/PCAvisualization.png}\n  \\caption{PCA visualization of patch features extracted by MLCD, showing semantic similarities between patches.}\n  \\label{fig:pca}\n\\end{figure}\n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\textwidth]{fig/model.pdf}\n    \\caption{Overview of HyperSeg, incorporating hybrid entity recognition and fine-grained visual perceiver modules.}\n    \\label{fig:model}\n\\end{figure*}\n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\textwidth]{fig/visualize_res.pdf}\n    \\caption{Qualitative results of HyperSeg in referring expression segmentation.}\n    \\label{fig:visualize_res}\n\\end{figure*}\n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\textwidth]{fig/visualize_interact.pdf}\n    \\caption{Qualitative results of HyperSeg in interactive segmentation.}\n    \\label{fig:visualize_interact}\n\\end{figure*}\n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\textwidth]{fig/visualize_pano.pdf}\n    \\caption{Qualitative results of HyperSeg in panoptic segmentation.}\n    \\label{fig:visualize_pano}\n\\end{figure*}\n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\textwidth]{fig/visualize_reason.pdf}\n    \\caption{Qualitative results of HyperSeg in reasoning segmentation.}\n    \\label{fig:visualize_reason}\n\\end{figure*}\n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\textwidth]{fig/visualize_revos.pdf}\n    \\caption{Qualitative results of HyperSeg in reasoning video object segmentation.}\n    \\label{fig:visualize_revos}\n\\end{figure*}\n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\textwidth]{fig/visualize_vos.pdf}\n    \\caption{Qualitative results of HyperSeg in semi-supervised video object segmentation.}\n    \\label{fig:visualize_vos}\n\\end{figure*}\n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\textwidth]{fig/visualize_vis.pdf}\n    \\caption{Qualitative results of HyperSeg in video instance segmentation.}\n    \\label{fig:visualize_vis}\n\\end{figure*}\n\n### Future Research Directions\nFuture research could explore the integration of more sophisticated reasoning and perception capabilities into VLLMs to further enhance their performance on segmentation tasks. Additionally, the use of larger and more diverse datasets could improve the generalization and robustness of these models. Incorporating OCR models for better handling of text-related tasks could also be a promising direction. The development of more efficient training strategies and the exploration of novel loss functions could also contribute to the advancement of VLLM-based segmentation methods."
}