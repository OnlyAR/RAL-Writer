{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\nWe present Video-LLaMA\\footnote{The video demonstration is available at \\url{https://youtu.be/RDNYs3Rswhc}} a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual \\& audio encoders and the frozen LLMs. Unlike previous works that complement LLMs to process the visual or audio signals only~\\citep{zhu2023minigpt,liu2023visualit,huang2023audiogpt}, Video-LLaMA enables video comprehension by tackling two challenges: (1) capturing the temporal changes in visual scenes, (2) integrating audio-visual signals. To counter the first challenge, we propose a Video Q-former to assemble a pre-trained image encoder into our video encoder and introduce a video-to-text generation task to learn video-language correspondence. For the second challenge, we leverage ImageBind~\\citep{girdhar2023imagebind}, a universal embedding model aligning multiple modalities, as the pre-trained audio encoder and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for the LLM module. To align the output of both visual \\& audio encoders with LLM's embedding space, we first train Video-LLaMA on massive video/image-caption pairs and then tune our model with visual-instruction datasets of moderate amount but higher quality. We found Video-LLaMA shows the ability to perceive and comprehend video content and generate meaningful responses grounded in the visual and auditory information presented in the videos.  %This highlights the potential of Video-LLaMA as a promising prototype for audio-visual AI assistants.\n\\end{abstract}\n\n\\section{Introduction}\n\nLarge Language Models (LLMs)~\\citep{chowdhery2022palm,bai2022constitutional,openai2023gpt4tr} have demonstrated remarkable capability of understanding and following user intentions and instructions\\footnote{https://chat.openai.com/chat}\\footnote{https://www.anthropic.com/product}\\footnote{https://bard.google.com/}. Typically, the user requests and the corresponding responses from LLMs are all in texts, however, text-only human-computer interaction is not sufficient for many application scenarios because real-world information is usually multi-modal. In order to further explore the potential of LLMs, many researchers attempt to endow LLMs with the capability of understanding multi-modal content~\\citep{huang2023audiogpt,zhang2023speechgpt,yin2023survey}. \n\nAmong these efforts, \\citet{alayrac2022flamingoav,wang2022ofa,huang2023languagein,xu2023mplug2,zhang2023speechgpt,sun2023generative} pre-train multi-modal LLMs with massive interleaved image-text data or speech-text data to accommodate multi-modal input. Meanwhile, another group of works adopts a more parameter-efficient way by complementing LLMs with off-the-shelf vision or speech foundation models to achieve multi-modal understanding ~\\citep{li2023blip2bl,zhu2023minigpt,liu2023visualit,ye2023mplugowl,zhang2023vpgtrans,huang2023audiogpt,wu2023onda,pandagpt,li2023otter}. \n\nDespite their effectiveness, these approaches are dedicated to aligning the input from exactly one additional modality with text (i.e., image or audio), which is unsatisfactory for video understanding. Concretely, empowering LLMs to understand video requires comprehensive processing  for different modalities including visual input, auditory input, and textual output, which is more challenging than image-only understanding and audio-only understanding tasks.\nAlthough there are several recent works attempt to unleash the video understanding capability of LLMs~\\citep{li2023videochatcv,maaz2023videochatgpt,luo2023valley}, their primary objective is to comprehend only the visual content of the video, with the auditory content remaining unused.\n\n\\begin{table}[t!]\n\\centering\n\\resizebox{0.47\\textwidth}{!}{\n\\begin{tabular}{|l|ccc|} \\toprule\n\\multirow{2}{*}{Model Name} & \\multicolumn{3}{c|}{Ability}                                                 \\\\\n                            & Static Image & Silent Video & Audio   \\\\ \\midrule\nBLIP2~\\cite{li2023blip2bl}  &   \\Checkmark  &                &                  \\\\\nMiniGPT4~\\cite{zhu2023minigpt}                    &   \\Checkmark  &                &                   \\\\\nLLaVA~\\cite{liu2023visualit}           &   \\Checkmark  &                &               \\\\\nmPLUG-Owl~\\cite{ye2023mplugowl}                  &   \\Checkmark  &   \\Checkmark   &              \\\\\nVideoChat~\\cite{li2023videochatcv}     &   \\Checkmark  &   \\Checkmark   &               \\\\ \nAudioGPT~\\cite{huang2023audiogpt} &    &      &     \\Checkmark  \\\\ \nVideo-ChatGPT~\\cite{maaz2023videochatgpt}     &   \\Checkmark  &   \\Checkmark   &               \\\\ \\midrule\nVideo-LLaMA                  &   \\Checkmark    &  \\Checkmark   &    \\Checkmark   \\\\ \\bottomrule\n\\end{tabular}\n}\n\\caption{Comparison with popular multi-modal large language models. Video-LLaMA has the unique ability to comprehend auditory and visual information simultaneously.}\n\\label{tab.comparison}\n\\end{table}\n\nIn this work, to fill in the blank of audio-visual LLMs, we investigate the possibility of building multi-modal LLMs that support the input of video and allow users to chat with computers around the user-uploaded video, which is usually composed of multiple video frames and audio. Instead of employing external perception models to convert visual/auditory signals to textual signals~\\citep{shen2023hugginggpt,li2023videochatcv}, we choose to build an end-to-end model that can handle the data from multiple modalities within one single framework. Specifically, we adopt the idea of BLIP-2~\\citep{li2023blip2bl} to guarantee the efficiency of cross-modal pre-training. To explicitly capture the change of visual scenes in the video, we use a pre-trained visual encoder to separately compute frame representations. Then, we introduce a frame embedding layer to inject temporal information and a video Q-Former to generate visual query tokens. As for the audio signals from the video, we additionally leverage a pre-trained audio encoder as well as an audio Q-former to learn reasonable auditory query embeddings (see the right part of Figure~\\ref{fig:architecture}). \n\nTo align textual output with video, we devise multi-branch cross-modal pre-training to learn the vision-language correspondence and the audio-language correspondence. For vision-language correspondence, we first pre-train the vision-related components on a large-scale video caption dataset with a video-clips-to-text generation task. To enhance the understanding of static visual concepts, we also add image-caption data into this pre-training stage. Then, we further fine-tune these components on a video-based conversation dataset to execute visual instruction tuning. For the alignment between the audio encoder and language decoder, we further pre-train the audio-related components on an audio caption dataset with an audio-to-text generation task. For the audio-language correspondence, we leverage Imagebind~\\citep{girdhar2023imagebind} as an encoder, which performs exceptionally well in aligning different modalities to a common embedding space. Given the limited availability of audio-text data, we also utilize vision-text data to train the audio-related components. These components learn to align the common embedding space provided by Imagebind with the embedding space of LLMs. Despite not being explicitly trained with audio-text data, Video-LLaMA exhibits a remarkable zero-shot audio understanding capability during inference. \n\nAs shown in Table~\\ref{tab.comparison}, our Video-LLaMA stands out from other existing multi-modal LLMs in terms of its distinctively comprehensive comprehension of audiovisual modal information in videos. In summary, our contributions are as follows:\n\n\\indent $\\bullet$ We propose Video-LLaMA, a multi-modal framework that enables LLM to simultaneously process both the visual and auditory content of a given video and engage in conversation with humans.\n\\\\ \n\\indent $\\bullet$ To empower LLMs with video understanding capability, we propose a multi-branch cross-modal pre-training framework to achieve both vision-language alignment and audio-language alignment. \\\\\n\\indent $\\bullet$ We open-source the entire codebase for pre-training and fine-tuning as well as the model weights of all the variants of Video-LLaMA\\footnote{\\url{https://github.com/DAMO-NLP-SG/Video-LLaMA}}. We also prepared the demos for video-grounded conversation\\footnote{\\url{https://huggingface.co/spaces/DAMO-NLP-SG/Video-LLaMA}}\\footnote{\\url{https://modelscope.cn/studios/damo/video-llama/summary}}.\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[scale=0.44]{figs/video_llama_architecture_v7.pdf}\n    \\caption{Overall architecture of Video-LLaMA.}\n    \\label{fig:architecture}\n\\end{figure*}\n\n\\section{Method}\nVideo-LLaMA aims to empower frozen LLMs with the capability of understanding both visual and auditory content in videos. As shown in Figure~\\ref{fig:architecture}, we design two branches, namely Vision-Language Branch and Audio-Language Branch, to respectively transform the video frames and audio signals into query representations that are compatible with the textual inputs of LLMs. In this section, we first introduce the overall architecture and the building blocks of each branch. Then, we delineate the procedures of the proposed multi-branch cross-modal pre-training and audio-visual instruction tuning.\n\n\\subsection{Architecture}\n\\subsubsection{Vision-Language Branch} \nThe Vision-Language Branch is designed for enabling the LLMs to understand visual inputs. As shown in the left part of Figure~\\ref{fig:architecture}, it is composed of a frozen pre-trained image encoder to extract features from video frames, a position embedding layer to inject temporal information into video frames, a video Q-former to aggregate frame-level representations and a linear layer to project the output video representations into the same dimension as the text embeddings of LLMs. Given one video consists of $N$ frames, the image encoder will first map each frame/image into $K_f$ image embedding vectors, yielding video frame representations $\\mathrm{\\bf V} = [\\mathrm{\\bf v}_1,\\mathrm{\\bf v}_2,...,\\mathrm{\\bf v}_N]$ where $\\mathrm{\\bf v}_i \\in \\mathbb{R}^{ K_f \\times d_f}$ is the set of $d_f$-dimensional image embeddings  corresponding to the $i$-th frame.  \n\nSince the frame representations $\\mathrm{\\bf v}_i$ from the frozen image encoder are computed without considering any temporal information, we further apply position embeddings as the indicator of temporal information to the representations from different frames. Then, we feed the position-encoded frame representations to Video Q-former, which shares the same architecture with Query Transformer (Q-Former) in BLIP-2~\\citep{li2023blip2bl}, to obtain $k_V$ video embedding vectors of dimension $d_v$ as the representation $\\hat{\\mathrm{\\bf v}} \\in \\mathbb{R} ^ {k_V \\times d_v}$ of the video.\n\nTo adapt the video representations to the input of LLMs, we add a linear layer to transform the video embedding vectors into the video query vectors. The video query vectors are of the same dimension as the text embeddings of LLMs. In the forward pass, they will be concatenated to text embeddings as a \\textit{video soft prompt} and guide the frozen LLMs to generate text conditioned on video content.\n\nAs for the implementation of the Vision-Language Branch, we utilize the pre-trained vision component of BLIP-2~\\citep{li2023blip2bl} as the frozen visual encoder, which includes a ViT-G/14 from EVA-CLIP~\\citep{fang2022eva} and a pre-trained Q-former. The remaining components, including the position embedding layer, Video Q-former, and Linear layer are randomly initialized and optimized to well connect the output of the frozen visual encoder to frozen LLMs.\n\n\\subsubsection{Audio-Language Branch}\nTo deal with the auditory content of the given video, we introduce the Audio-Language Branch. Concretely, it consists of a pre-trained audio encoder to compute features given a short segment of origin audio, a position embedding layer to inject temporal information to audio segments, an audio Q-former to fuse the features of different audio segments, and a linear layer to map the audio representation into the embedding space of LLMs.\n\nIn practice, we utilize the pre-trained Imagebind~\\citep{girdhar2023imagebind} as the audio encoder. We first uniformly sample $M$ segments of 2-second short audio clips from the video, then convert each 2-second audio clip into spectrograms using 128 mel-spectrogram bins. After obtaining the spectrogram list of input audio, the audio encoder will map each spectrogram into a dense vector. So the generated audio representation of the given video can be denoted as  $A = [a_1,a_2,...,a_M]$. \n\nSimilar to Video Q-Former, the Audio Q-former injects temporal information by adding learnable positional embeddings to audio segments. It then generates fixed-length audio features by computing the interaction across the position-encoded audio segments.  Audio Q-Former adopts the same architecture as Q-Former. It projects the variable-length audio representation list $A$ into a fixed-length sequence $\\hat{\\mathrm{\\bf A}} \\in \\mathbb{R} ^ {K_a \\times d_a}$, where the $K_a$ is the number of audio embedding vectors and $d_a$ is the dimension of each vector. Finally, we employ a linear layer to map audio features to the embedding space of the LLM.\n\n\\subsection{Multi-branch Cross-Modal Training}\nWe train the vision-language and audio-language branches separately. In the first stage, large-scale vision-caption datasets are used for training, and in the second stage, high-quality instruction-following datasets were used for fine-tuning. The image is treated as a one-frame video.\n\n\\subsubsection{Training of Vision-Language Branch}\nFor pre-training vision-language branch, we utilized Webvid-2M~\\citep{Bain21}, a large-scale dataset of short videos with textual descriptions sourced from stock footage sites. Moreover, we employed the image caption dataset CC595k, which is sourced from CC3M~\\citep{sharma-etal-2018-conceptual} and filtered by \\citet{liu2023visualit}. We adopt a video-to-text generation task during the pre-training stage, i.e., given the representation of a video, prompting the frozen LLM to generate the corresponding text description. We find that a significant portion of textual descriptions are insufficient to reflect the entire content of the videos. Therefore, the visual semantics in the videos are not fully aligned with the textual semantics in the video descriptions. Nevertheless, this stage aimed to utilize a vast amount of data and enable video features to contain as much visual knowledge as possible. We left the abilities of vision-text alignment and instruction-following for the next stage.\n\nAfter the  pre-training stage, the model can generate content about information in the video, but its ability to follow instructions has decreased. Therefore, in the second stage, we fine-tuned the model using high-quality instruction data. We integrated the image-detail-description dataset from MiniGPT-4~\\citep{zhu2023minigpt}, the image-instruction dataset from LLaVA~\\citep{liu2023visualit}, and the video-instruction dataset from Video-Chat~\\citep{li2023videochatcv}. After fine-tuning, Video-LLaMA exhibited remarkable abilities in following instructions and comprehending images and videos. \n\n\\subsubsection{Training of Audio-Language Branch}\nTraining the audio-language branch directly using audio-text data is highly challenging due to the rarity of such data.  The objective of the learnable parameters in the audio-language branch is to align the output embedding of the frozen audio encoder with the embedding space of LLM. Given the scarcity of audio-text data, we employ a workaround strategy to achieve this objective.\nImageBind, which is used as our audio encoder, has a remarkable ability to align different modalities' embeddings to one common space, demonstrating impressive performance on cross-modal retrieval and generation tasks. \nIn light of the scarcity of audio-text data and the abundance of visual-text data, we train the audio-language branch using visual-text data, following the same data and process as the vision branch.  Thanks to the shared embedding space provided by ImageBind, Video-LLaMA exhibits the ability to comprehend audio during inference, even though the audio interface has never been trained on audio data.\n\n\\section{Related Works}\n\\textbf{Large Language Models}: Large language models (LLMs)~\\citep{black2022gpt, scao2022bloom, openai2023gpt4tr, tsimpoukelli2021multimodal} have demonstrated remarkable language understanding and reasoning abilities, enabling the generation of high-quality natural language text across various domains, including articles, conversations, stories, and poetry. LLMs have already sparked a technological revolution and have been widely applied in different applications. Moreover, a series of open source large models, such as LLaMA~\\citep{touvron2023llama}, BLOOM~\\citep{scao2022bloom} and OPT~\\citep{zhang2022opt}, have greatly promoted technological advancement and made outstanding contributions to the NLP community. Building upon these LLMs, researchers have further extended their capabilities and developed excellent models for various NLP tasks. Examples include Vicuna~\\citep{chiang2023vicuna} and Baize~\\citep{xu2023baize}. Our work is based on these LLMs and provides plug-and-play plugins that empower them with the capability of comprehending both visual and auditory content in videos.\n\n\\textbf{Multi-modal Large Language Models}: \nResearchers have been actively exploring the use of LLMs for processing multi-modal inputs~\\citep{gao2023llamaadaptervp,li2023videochatcv}. Existing approaches can be categorized into two main groups. The first category involves employing LLMs as controllers and utilizing existing multi-modal models as tools. In this approach, when receiving the user's text instruction, the LLM recognizes the user's intention and makes decisions about which tools to call. It then generates comprehensive responses by incorporating the results obtained from these off-the-shelf multi-modal models. Examples include ChatGPT~\\citep{wu2023visual}, HuggingGPT~\\citep{shen2023hugginggpt}, and AudioGPT~\\citep{huang2023audiogpt}. \nThe second category focuses on training fundamental large-scale multi-modal models. The key idea of this line of work is to align the pre-trained foundation models for other modalities to textual LLMs. For instance, Flamingo~\\citep{alayrac2022flamingo} utilizes a perceiver resampler and a gated cross-attention layer to connect a frozen image encoder and LLM.\nBLIP2~\\citep{li2023blip2bl} introduces a Q-Former to map learned image queries to the textual embedding space of LLMs. ~\\citep{liu2023visualit}, mPLUG-owl~\\citep{ye2023mplugowl} and MiniGPT4 ~\\citep{zhu2023minigpt} develop instruction-following image-LLMs using image-instruction-following dataset. Video-Chat~\\citep{li2023videochatcv} and Video-ChatGPT~\\citep{maaz2023videochatgpt} extend image encoders to video encoders and connect them with LLMs to understand visual content in videos. PandaGPT~\\citep{pandagpt} utilizes multi-modal encoders from ImageBind, trained exclusively on image-instruction pairs, to enable large models to understand six modalities. Our work falls into the second category, where we train fundamental models to comprehend both the visual and auditory content in videos. \n\n\\section{Examples}\nIn this section, we show some cases to demonstrate Video-LLaMA's  multi-modal instruction-following capability in video/audio/image-grounded conversations. Figure~\\ref{fig.demo_case} shows some of these examples, while additional cases can be found in  Appendix~\\ref{sec:appendix}.\n\n\\begin{figure*}[t]\n    \\centering\n    \\vspace{-0.5cm}\n    \\subfigure[A case that Video-LLaMA answers the questions based on the background sound and visual content of\nthe video]{\n        \\centering\n        \\includegraphics[width =0.45\\linewidth]{figs/demo_case_audio_video_description.pdf}\n        \\label{fig.demo_case_audio_video_description}\n        }\n        \\quad\n     \\subfigure[A case showing Video-LLaMA’s ability to identify actions over time.]{\n        \\centering\n        \\includegraphics[width =0.45\\linewidth]{figs/demo_case_video_qa.pdf}\n        \\label{fig.demo_case_video_qa}\n        }\n        \n        \n    \\subfigure[A case demonstrating Video-LLaMA’s ability to comprehend static images.]{\n        \\centering\n        \\includegraphics[width =0.45\\linewidth]{figs/demo_case_image_understanding.pdf}\n        \\label{fig.demo_case_image_understanding}\n        }\n    \\quad\n    \\subfigure[A case demonstrating Video-LLaMA’s ability to recognize famous landmarks.]{\n        \\centering\n        \\includegraphics[width =0.45\\linewidth]{figs/demo_case_image_concept_knowledge.pdf}\n         \\label{fig.demo_case_image_concept_knowledge}\n        }\n\\caption{Some examples generated by Video-LLaMA. }\n    \\label{fig.demo_case}\n    \\vspace{-0.3cm}\n\\end{figure*}\n\n\\paragraph{(1) Audio-visual integration perception ability.} Figure~\\ref{fig.demo_case_audio_video_description} and Figure~\\ref{fig.demo_case_audio_video_des_2} show Video-LLaMA's unique ability to comprehend auditory and visual information simultaneously. The videos in both cases contain audio. In each conversation, we pose two questions related to visual and auditory content respectively.  If the model could only receive one modal, it would be unable to answer both of these questions. However, we can observe that Video-LLaMA accurately responds to both visual and auditory questions in both cases.\n\n\\paragraph{(2) The ability to capture temporal dynamics in videos.} \nFigure~\\ref{fig.demo_case_video_qa} and Figure~\\ref{fig.demo_case_video_description} illustrate the capability of Video-LLaMA to identify actions over time. \nIt successfully describes the actions of the girl and the moving direction of the boat.\n\n\\paragraph{(3) The ability to perceive and understand static images.} Figure~\\ref{fig.demo_case_image_understanding} and Figure~\\ref{fig.demo_case_image_des}  show Video-LLaMA's ability to perceive and understand pictures. Figure~\\ref{fig.demo_case_image_understanding} demonstrates Video-LLaMA's ability to understand the concept of \"unusual\" and specifically describe the unusual scene.  In Figure~\\ref{fig.demo_case_image_des}, not only does Video-LLaMA accurately describe the main content, but it also associates it with the friendly interaction between a dog and a human.\n\n\\paragraph{(4) The ability of common-knowledge concept recognition.} Figure~\\ref{fig.demo_case_image_concept_knowledge} and Figure~\\ref{fig.demo_case_image_multiturn} demonstrate Video-LLaMA's remarkable capacity for recognizing common-knowledge concepts in visual signals. Video-LLaMA successfully recognizes famous landmarks and characters and can engage in common-sense question-answering.\n\n\\section{Conclusion}\nIn this paper, we present Video-LLaMA, a cutting-edge multi-modal framework that empowers large language models with both audio \\& video understanding capabilities. Our experiments  demonstrated the impressive abilities of Video-LLaMA in audio and video-grounded conversations, highlighting its potential as a promising prototype for audio-visual AI assistants. We have open-sourced the entire training code and various model weights, along with detailed instructions to assist developers in utilizing our code for further development. In addition, we have provided online demo websites and offline demo deployment guides for users to experience Video-LLaMA's capabilities firsthand. We are committed to constantly maintaining and improving Video-LLaMA, and will continue to contribute to the open-source community.\n\n\\section{Limitations}\nAlthough Video-LLaMA has demonstrated impressive abilities in understanding both visual and auditory content in videos, it is still an early-stage prototype and has some limitations, including:\n(1) Limited perception capacities: Video-LLaMA's performance is hindered by the quality and scale of the current training dataset. We are actively constructing a high-quality audio-video-text alignment dataset to enhance the model's perception capabilities. \n(2) Limited ability to handle long videos. Long videos(such as movies, and TV shows) contain a large volume of information and impose higher demands on computational resources. This challenge remains a crucial issue that the research community is actively working to address.\n(3) Hallucination. Video-LLaMA inherits the hallucination problem from the frozen LLMs. We will continue to address these challenges and develop more powerful versions for video understanding.\n\n\\clearpage\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Video-ChatGPT: Towards Detailed Video Understanding via \\\\\nLarge Vision and Language Models}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\nConversation agents fueled by Large Language Models (LLMs) are providing a new way to interact with visual data. While there have been initial attempts for image-based conversation models, this work addresses the under-explored field of \\emph{video-based conversation} by introducing Video-ChatGPT. It is a multimodal model that merges a video-adapted visual encoder with an LLM. The resulting model is capable of understanding and generating detailed conversations about videos. We introduce a new dataset of 100,000 video-instruction pairs used to train Video-ChatGPT acquired via manual and semi-automated pipeline that is easily scalable and robust to label noise. We also develop a quantitative evaluation framework for video-based dialogue models to objectively analyze the strengths and weaknesses of video-based dialogue models. Code: \\url{https://github.com/mbzuai-oryx/Video-ChatGPT}.\n\\footnotetext[1]{Equal contribution.}\n\\end{abstract}\n\n\\section{Introduction}\nThe surge of deep learning applications for video understanding has lead to major advancements in video-related tasks. However, the current video understanding models are still unable to hold an open-ended conversation about the video content in a coherent manner. A video-based dialogue model can revolutionize video search, surveillance operations and help summarize key events and abnormal event detection. Above all, it can provide a unified human-understandable interface to video-related tasks such as action recognition, localization, detection, segmentation, retrieval, and tracking. Further, such a capability is of great interest as it will demonstrate the model's ability to encode temporal and spatial cues, contextual relationships and long-term dependencies. \n\nRecent advancements in multimodal understanding are largely based on the combination of pretrained \\emph{image} models with Large Language Models (LLMs) but generally do not consider video inputs~\\cite{liu2023llava, zhu2023minigpt, blip, blip-2, instructblip}. It is therefore interesting to leverage the vast capabilities of LLMs for video understanding tasks in a way that would not only maintain the temporal and spatial characteristics but also be adept at generating human-like conversations about videos. In this paper, we introduce Video-ChatGPT, a novel multimodal model that merges the representational abilities of a pretrained visual encoder and the generative powers of an LLM, capable of understanding and conversing about videos. \n\nVideo-ChatGPT leverages an adapted LLM~\\cite{liu2023llava} that integrates the visual encoder of CLIP~\\cite{radford2021learning} with Vicuna~\\cite{vicuna2023} as a language decoder, fine-tuned on generated instructional image-text pairs. Our approach further adapts the design for spatiotemporal video modeling and fine-tunes the model on video-instruction data to capture temporal dynamics and frame-to-frame consistency relationships available in video data. In contrast to other concurrent works for video-based conversation~\\cite{2023videochat, damonlpsg2023videollama, su2023pandagpt}, Video-ChatGPT excels at temporal understanding, spatial consistency and contextual comprehension as demonstrated by our extensive evaluations. \n\nA fundamental contribution of this work is the creation of a dataset of 100,000 video-instruction pairs using a combination of human-assisted and semi-automatic annotation methods. Each pair consists of a video and its associated instruction in the form of a question-answer. This provides Video-ChatGPT with a large and diverse dataset to learn from, increasing its video-specific understanding, attention to temporal relationships and conversation capabilities.\n\nMoreover, we introduce the first quantitative video conversation evaluation framework for benchmarking, allowing for a more accurate evaluation of the performance of video conversation models. This framework evaluates models on a variety of capabilities, such as correctness of information, detail orientation, contextual understanding, temporal understanding, and consistency. \n\nThe contributions of this work are as follows,\\vspace{-0.5em}\n\\begin{itemize}\\setlength{\\itemsep}{0mm}\n    \\item We propose Video-ChatGPT, a video conversation model capable of generating meaningful conversations about videos. It combines the capabilities of LLMs with a pretrained visual encoder adapted for spatiotemporal video representations.\n    \\item We introduce 100,000 high-quality video instruction pairs together with a novel annotation framework that is scalable and generates a diverse range of video-specific instruction sets.\n    \\item We develop the first quantitative video conversation evaluation framework for benchmarking video conversation models. We demonstrate Video-ChatGPT to perform well compared to concurrent conversational engines for videos such as Video Chat \\cite{2023videochat}.\n\\end{itemize}\n\n\\section{Related Works}\n\n\\textbf{Vision Language Models:} Significant advancements in the field of computer vision have recently been observed due to the development of many foundational vision-language models. These models represent a significant leap towards creating general-purpose vision models capable of tackling various tasks simultaneously~\\cite{radford2021learning, Alayrac2022Flamingo, gupta2022towards, Maaz2022Multimodal}. A prime example is CLIP~\\cite{radford2021learning}, which is trained on 400M image-text pairs and has demonstrated impressive zero-shot performance on numerous benchmarks. It has been employed in various downstream applications, from image-based object detection and segmentation~\\cite{Hanoona2022Bridging, liang2023open} to 3D applications~\\cite{rozenberszki2022language, ni2022expanding}. Numerous attempts have also been made to adapt CLIP for video applications~\\cite{wang2021actionclip, ni2022expanding}. Similar to our design, ViFi-CLIP~\\cite{hanoonavificlip} suggests employing temporal pooling across video frames to adapt the image-based CLIP model for video-based tasks.\n\n\\noindent \\textbf{Large Language Models:} The field of natural language processing has witnessed a paradigm shift with the advent of pretrained Large Language Models (LLMs) such as GPT~\\cite{brown2020languagegpt}, LLaMA~\\cite{touvron2023llama}, OPT~\\cite{zhang2022opt}, and MOSS~\\cite{OpenLMLab_2023}. These models exhibit extraordinary abilities like language generation and in-context learning, and their knack for understanding intricate tasks given user prompts in a zero-shot manner reflects their impressive adaptability and generalization. The proven capabilities of LLMs have encouraged researchers to fine-tune them to maximize their proficiency.\n\nA key strategy in this pursuit is instruction tuning. This approach focuses on improving the model's alignment with user intentions and optimizing its output quality. For instance, InstructGPT~\\cite{ouyang2022traininginstructgpt} and ChatGPT~\\cite{chatgpt} significantly benefit from this technique, showcasing improvements in diverse conversational interaction capabilities and their aptitude to answer a broad range of complex questions. This effective approach has recently been employed in open-source models like Alpaca~\\cite{alpaca} and Vicuna~\\cite{vicuna2023}, both developed using the LLaMA~\\cite{touvron2023llama} framework, resulting in performance improvements.\n\n\\noindent \\textbf{Pre-trained LLMs in Vision-Language Tasks:} The recent strides in multimodal understanding have primarily been driven by the integration of image-based vision models with LLMs. Seminal contributions such as Flamingo~\\cite{Alayrac2022Flamingo} and BLIP-2~\\cite{blip-2} have demonstrated the power of utilizing web-scale image-text data, as well as pioneering techniques in cross-modal alignment, to exhibit dynamic abilities in conversational and few-shot learning contexts. Building on this foundation, MiniGPT-4~\\cite{zhu2023minigpt} allows image-based conversations by integrating BLIP-2 and Vicuna for zero-shot image comprehension.\n\nEqually significant is the emergence of LLaVA~\\cite{liu2023llava}, a model derived from the LLaMa architecture, leveraging GPT-4's language proficiency to generate multimodal instruction-following data. With instruction tuning applied on the derived data, LLaVA has displayed interesting multimodal chat capability, hinting at the scalability potential of such a methodology. In addition, InstructBLIP~\\cite{instructblip} has demonstrated strong image-based dialogue capabilities via vision-language instruction tuning by innovating with instruction-aware visual feature extraction.\n\nMore closely related to our work, VideoChat~\\cite{2023videochat} employs selective components of video foundational models~\\cite{wang2022internvideo} and image foundation models~\\cite{blip-2}, and integrates them with LLMs ~\\cite{vicuna2023} in conjunction with few learnable layers, tuned using a two-stage lightweight training. Additionally, they construct a video-specific dataset using off-the-shelf vision-language models~\\cite{wu2022grit, blip-2, huang2023tag2text, wang2022internvideo} for generating noisy detailed textual descriptions to enhance the training of video-centric conversational models.\n\nDifferent from VideoChat, we propose a novel human assisted and semi-automatic annotation framework for generating high quality instruction data for videos. Our simple and scalable architecture design utilizes pretrained CLIP~\\cite{radford2021learning} to generate spatiotemporal features which help Video-ChatGPT in generating meaningful video conversation. Further, we are the first to propose quantitative framework for evaluating video conversation tasks (see Section~\"Video Instruction Data Generation\" for more details).\n\n\\begin{figure*}[t]\n  \\centering\n  \\includegraphics[width=0.99\\textwidth]{images/video-chatgpt.png}\n  \\caption{\\textbf{Architecture of Video-ChatGPT.} Video-ChatGPT leverages the CLIP-L/14 visual encoder to extract both spatial and temporal video features. This is accomplished by averaging frame-level features across temporal and spatial dimensions respectively. The computed spatiotemporal features are then fed into a learnable linear layer, which projects them into the LLMs input space. In our approach, we utilize the Vicuna-v1.1 model, comprised of 7B parameters, and initialize it with weights from LLaVA~\\cite{liu2023llava}.}\n  \\label{fig:arch}\n\\end{figure*}\n\n\\section{Video-ChatGPT}\nVideo-ChatGPT is a large vision-language model that aligns video representations with a Large Language Model (LLM), thus enhancing its ability to generate meaningful conversation about videos. Our approach draws from the approach employed in designing vision-language (VL) models for the video domain. Given the limited availability of video-caption pairs and the substantial resources required for training on such data from scratch, these models commonly adapt pretrained image-based VL models for video tasks~\\cite{ni2022expanding, wang2021actionclip, hanoonavificlip}. We adopt a similar approach, starting with the Language-aligned Large Vision Assistant (LLaVA)\\cite{liu2023llava} as our foundation.\n\nLLaVA is a LMM that integrates the visual encoder of CLIP~\\cite{radford2021learning} with the Vicuna language decoder~\\cite{vicuna2023} and is fine-tuned end-to-end on generated instructional vision-language data. We fine-tune this model using our video-instruction data, adapting it for video conversation task. The video-instruction data is obtained as a combination of manual and automated pipelines in our proposed instruction generation setup. This adaptation on video-specific instructions allows for accommodating additional temporal dynamics, frame-to-frame consistency, and long-range relationships present in video data. As a result, our Video-ChatGPT excels in video reasoning, creativity, and understanding of spatial, temporal, and action-oriented components within videos.\n\n\\subsection{Architecture}\nWe use CLIP ViT-L/14, which is pretrained using large-scale visual instruction tuning in LLaVa, as the visual encoder. However, LLaVa visual encoder is meant for images, which we modify to capture spatiotemporal representations in videos. Given a video sample $V_i \\in \\mathbb{R}^{T \\times H \\times W  \\times C}$ with $T$ frames, the visual encoder generates temporal and spatial features.\nThe visual encoder encodes the $T$ frames independently as a batch of images and produces frame-level embeddings $x_i \\in \\mathbb{R}^{T \\times h \\times w \\times D}$, where $h=H/p, w=W/p$. Here p is the patch size (\\ie 14 for ViT-L/14), and we represent the number of tokens as $N$, where $N = h \\times w$. Frame-level embeddings are average-pooled along the spatial dimension to obtain a \\textit{video-level temporal representation} $t_{i} \\in \\mathbb{R}^{T \\times D}$. This operation implicitly incorporates temporal learning through the aggregation of multiple frames. Similarly, the frame-level embeddings are average-pooled along the temporal dimension to yield the \\textit{video-level spatial representation} $z_{i} \\in \\mathbb{R}^{N \\times D}$. The temporal and spatial features are concatenated to obtain the video-level features $v_{i}$, \n\\begin{equation}\n    v_{i} = [t_{i} \\quad z_{i}] \\in \\mathbb{R}^{(T+N) \\times D}.\n\\end{equation}\nA simple trainable linear layer $g$, projects these video-level features into the language decoder's embedding space, transforming them into corresponding language embedding tokens $Q_{v}$,\n\\begin{equation}\n    Q_{v} = g(v_{i}) \\in \\mathbb{R}^{(T+N) \\times K} .\n\\end{equation}\nNote that the function $g$ acts as an adapter and can be implemented with more complicated architectures as well. However, we opt for a simplistic design that gives competitive performance compared to more sophisticated choices in our experiments. \nThe text queries are tokenized to the same dimensions, $Q_{t} \\in \\mathbb{R}^{L\\times K}$. Here $L$ represents the length of text query.\nFinally, $Q_{v}$ is concatenated with $Q_{t}$ and input to the language decoder.\n\n\\begin{figure*}[t]\n  \\centering\n  \\includegraphics[width=0.99\\textwidth]{images/origina_vs_human-min.png}\n  \\caption{\\textbf{Examples of data enrichment via human-assisted annotation}. Human annotators augment video descriptions from video-caption datasets. The captions are enriched by integrating detailed information about spatial and temporal aspects, object relationships, reasoning, scene descriptions, and the chronological sequence of events.}\n  \\label{fig:human_assisted}\n\\end{figure*}\n\n\\subsection{Video Instruction Tuning}\nWe employ instruction-tuning of the LLM on the prediction tokens, utilizing its original auto-regressive training objective. The pretrained model is finetuned with curated, high-quality video-text pairs. During the finetuning phase, we use predefined prompts based on the following template:\n\\begin{center}\n\\noindent\\texttt{USER: <Instruction> <Vid-tokens> Assistant:}\n\\end{center}\n\\noindent Using the notations, we can represent it as,\n\\begin{center}\n\\noindent\\texttt{USER: <$Q_{t}$> <$Q_{v}$> Assistant:}\n\\end{center}\n\nIn this prompt, the \\texttt{<Instruction>} represents a question pertaining to the video, randomly sampled from the training set of video-question-answer pairs. Questions can be general, asking to describe the video, or they may relate to specific temporal, spatial, or creative aspects of the video content. The prediction answer \\texttt{<Answer>} corresponds to the specific question asked. Throughout the training, the weights for both the video encoder and LLM remain frozen, and the model maximizes the likelihood of predicting tokens representing the answer by adapting the linear layer. Consequently, the video features $Q_{v}$ become aligned with the pre-trained LLM word embeddings, equipping Video-ChatGPT with the ability to produce more natural and dependable responses.\n\n\\section{Video Instruction Data Generation}\n\\label{video_instruction_data}\nIn this section, we discuss our data-focused approach, which uses both human-assisted and semi-automatic annotation methods to generate high-quality video instruction data. This data is crucial for training Video-ChatGPT, ensuring accurate and meaningful responses. Our data collection involves two key methods. The \\textit{human-assisted annotation}, involves expert annotators analysing video content and providing detailed descriptions. This process generates data rich in context and detail, which helps our model understand complex aspects of video content.\nOn the other hand, the \\textit{semi-automatic annotation framework} is more cost-effective and scalable. Leveraging state-of-the-art vision-language models, this method generates broad, high-volume annotations, thus increasing the quantity of data without compromising the quality substantially.\nThrough these combined methods, we have successfully accumulated a robust set of 100,000 video-instruction pairs. This extensive dataset is crucial in fine-tuning our model to comprehend video content effectively, integrating both spatial and temporal cues into its understanding.\n\nOur instructional data is both diverse and comprehensive, incorporating a wide range of data types. These include detailed descriptions, summarizations, question-answer pairs, tasks that stimulate creativity or generation of new ideas, and conversational tasks. The data spans a broad spectrum of concepts, ranging from visual appearance and temporal relations to complex reasoning tasks and beyond, providing a diverse training ground for our model to learn from.\n\n\\begin{figure*}[t]\n  \\centering\n  \\includegraphics[width=0.99\\textwidth]{images/semi-auto-example.png}\n  \\caption{\\textbf{Examples of generating instructional data using our proposed semi-automatic annotation pipeline.} We employ off-the-shelf dense prediction and captioning models to augment video descriptions. BLIP-v2~\\cite{blip-2} generates frame-level captions, while GRIT~\\cite{wu2022grit} is utilized for dense frame captions. Tag2Text~\\cite{huang2023tag2text} generates tags for each key-frame, aiding in eliminating noise (e.g. the GRiT descriptions containing \\textit{flower pattern} and \\textit{on phone} would be discarded as there are no corresponding tags detected). Finally, we query GPT-3.5 with in-context examples to generate video-instructional data.}\n  \\label{fig:semi_automated-1}\n\\end{figure*}\n\n\\begin{figure*}[t]\n  \\centering\n  \\includegraphics[width=0.99\\textwidth]{images/original_vs_semi-annotated.png}\n  \\caption{\\textbf{Examples of data enrichment using our proposed semi-automatic annotation.} We employ off-the-shelf dense prediction and captioning models \\cite{blip-2, wu2022grit, huang2023tag2text} to augment video descriptions. All additional context elements are combined with the video captions and undergo a GPT-assisted post-processing stage, generating the final detailed description.}\n  \\label{fig:semi_automated}\n\\end{figure*}\n\n\\subsection{Human-assisted Annotation}\nIn this process, we leverage datasets containing video-caption pairs and utilize the expertise of human annotators to enrich the original ground truth annotations. Specifically, we use a subset of ActivityNet-200~\\cite{caba2015activitynet} which provides concise ground truth descriptions of various activities in distinct video segments.\n\nThe annotators further enrich the captions by adding comprehensive information about physical appearances and spatial and temporal localization, among other critical contextual details. Figure \\ref{fig:human_assisted} shows an example of how a ground truth caption is enriched using human-assisted annotation.  \n\n\\subsection{Semi-automatic Annotation Framework} In addition to the rich human-assisted annotations, we also harness the capabilities of advanced dense image vision-language models, developing a semi-automatic annotation framework. This approach is cost-effective and scalable, thereby increasing the quantity of data without substantially compromising the quality.\n\nSimilar to the human-assisted process, this framework also leverages datasets containing video-caption pairs. We enrich these datasets using contextual information drawn from off-the-shelf dense prediction and captioning image-based vision-language models. These models provide predictions that deliver additional contextual information, thereby enriching the video captions. We developed a comprehensive method that combines these predictions, and utilize specific models for the purpose of eliminating noisy or irrelevant context from the data. This ensures that the data maintains its accuracy and relevance.\n\nBuilding on the use of off-the-shelf models, we apply pretrained models like BLIP-2~\\cite{blip-2} and GRiT~\\cite{wu2022grit} for key-frame analysis in the videos. The BLIP-2 image-captioning model generates frame-level captions, while the GRiT dense captioning model provides detailed captions for scene objects. Additionally, the pretrained Tag2Text~\\cite{huang2023tag2text} model is used to generate tags for each key-frame of the video. Despite their utility, these models can introduce noise into the data.\n\nTo ensure high-quality data and mitigate noise, we implement three key steps. \\emph{First,} we maintain a high prediction threshold for all off-the-shelf models to uphold accuracy. \\emph{Second,} we employ a specialized filtering mechanism that removes any frame-level caption from BLIP-2 or GRiT not matching with the Tag2Text frame-level tags. This process involves extracting words from the frame-level captions that are within the predefined Tag2Text tags vocabulary and eliminates any captions that contain words not in the tags for a given frame. This strategy acts as an additional filtering layer and enriches the captions by integrating predictions from multiple models.\n\nIn the \\emph{third} step, we merge frame-level captions and use the GPT-3.5 model to generate a singular, coherent video-level caption. This step augments the original ground truth caption with context from these models. We also direct GPT-3.5 to discard inconsistent information across frames, ensuring a precise, contextually rich video instruction dataset. Figure~\\ref{fig:semi_automated-1},\\ref{fig:semi_automated} illustrates how a ground truth caption is enriched using this process after all three refinement stages to generate instructional data and detailed descriptive caption. All of our designed prompts for in-context learning along with the curated dataset will be made publicly available.\n\n\\subsection{GPT-Assisted Postprocessing} \nLastly, we implement a GPT-Assisted Post-processing mechanism that refines and optimizes the enriched annotations, in order to generate high-quality video instructional data. We prompt GPT-3.5 model to create question-answer pairs from the enriched and detailed captions that cover a wide variety of aspects using in-context learning. These aspects include detailed descriptions, summarizations, question-answer pairs, tasks that stimulate creativity or the generation of new ideas, and conversational tasks.\n\nEach of these elements plays a crucial role in our data-centric approach. Our ultimate goal is to create a video-based conversation model that is accurate, capable of understanding video content from both spatial and temporal cues, and adept at engaging in conversations.\n\n\\section{Experiments}\n\n\\begin{table*}[t]\n\\centering\n\\renewcommand{\\arraystretch}{1.0} % adds vertical padding to the rows of the table\n\\scalebox{0.85}{\n\\begin{tabular}{lcccc}\n\\toprule\n\\textbf{Evaluation Aspect} & \\textbf{Video Chat} & \\textbf{LLaMA Adapter} & \\textbf{Video-LLaMA} & \\textbf{Video-ChatGPT} \\\\ \n\\midrule\n\\midrule\nCorrectness of Information & 2.23 & 2.03 & 1.96 & \\textbf{2.40}\\\\\nDetail Orientation & 2.50 & 2.32 & 2.18 & \\textbf{2.52}\\\\\nContextual Understanding & 2.53 & 2.30 & 2.16 & \\textbf{2.62}\\\\\nTemporal Understanding & 1.94 & \\textbf{1.98} & 1.82 & \\textbf{1.98}\\\\\nConsistency & 2.24 & 2.15 & 1.79 & \\textbf{2.37}\\\\\n\\bottomrule\n\\end{tabular}}\n\\vspace{0.2em}\n\\caption{\\textbf{Performance benchmarking of text generation models.} An in-depth comparative analysis of Video-ChatGPT and Video Chat~\\cite{2023videochat} across five key evaluation aspects we propose in our benchmark. For a fair comparison, 7B variants are used for all the models. Video-ChatGPT shows competent performance across all key aspects.}\n\\label{table:1}\n\\end{table*}\n\n\\begin{table*}[ht]\n\\centering\n\\setlength{\\tabcolsep}{8pt}\n\\renewcommand{\\arraystretch}{1}\n\\scalebox{0.9}{\n\\begin{tabular}{l c c c c c c c c}\n\\hline\n\\textbf{Model} & \\multicolumn{2}{c}{\\textbf{MSVD-QA}} & \\multicolumn{2}{c}{\\textbf{MSRVTT-QA}} & \\multicolumn{2}{c}{\\textbf{TGIF-QA}} & \\multicolumn{2}{c}{\\textbf{Activity Net-QA}} \\\\\n\\cline{2-9}\n & \\textbf{Accuracy} & \\textbf{Score} & \\textbf{Accuracy} & \\textbf{Score} & \\textbf{Accuracy} & \\textbf{Score} & \\textbf{Accuracy} & \\textbf{Score} \\\\\n\\hline\n\\hline\nFrozenBiLM & 32.2 & -- & 16.8 & -- & 41.0 & -- & 24.7 & -- \\\\\nVideo Chat & 56.3 & 2.8 & 45.0 & 2.5 & 34.4 & 2.3 & 26.5 & 2.2 \\\\\nLLaMA Adapter & 54.9 & 3.1 & 43.8 & 2.7 & - & - & 34.2 & 2.7 \\\\\nVideo LLaMA & 51.6 & 2.5 & 29.6 & 1.8 & - & - & 12.4 & 1.1 \\\\\nVideo-ChatGPT & \\textbf{64.9} & \\textbf{3.3} & \\textbf{49.3} & \\textbf{2.8} & \\textbf{51.4} & \\textbf{3.0} & \\textbf{35.2} & \\textbf{2.8} \\\\\n\\hline\n\\end{tabular}}\n\\caption{\\textbf{Zeroshot question-answering} comparison of Video-ChatGPT with other video generative models. For a fair comparison, 7B variants are used for all the models. Video-ChatGPT performs competitively across all datasets.}\n\\end{table*}\n\n\\subsection{Implementation Details}\nWe use LLaVA~\\cite{liu2023llava} as our baseline model and finetune it on our 100K video instruction pairs. We only update the linear layer projecting the video features to the LLMs' input space, while the rest of the architecture is kept frozen. We finetune the model for 3 epochs using a learning rate of 2$e^{-5}$ and an overall batch size of 32. We use 7B parameter model in all the experiments and its training took around 3 hours on 8 A100 40GB GPUs. During inference, for memory efficiency, we load the models in FP16 mode.\n\nIn our semi-automatic annotation framework, we use Katna~\\cite{Katna} to extract video key-frames. For off-the-shelf Tag2Text~\\cite{huang2023tag2text} model, we use the Swin-B variant with an input size of 384$\\times$384 and a confidence threshold of 0.7. For GRIT~\\cite{wu2022grit}, we use ViT-B version with CenterNet2~\\cite{zhou2021probablistic}.\n\n\\subsection{Quantitative evaluation}\n\\label{quantitative_eval}\nIn this section, we highlight a key contribution of our work: the quantitative evaluation of Video-ChatGPT using advanced metrics and comparative evaluations with existing state-of-the-art models. We conduct two types of quantitative evaluations: i) Video-based Generative Performance Benchmarking and ii) Zero-Shot Question-Answer Evaluation.\n\n\\noindent\\textbf{Video-based Text Generation Performance Benchmarking: }\nWe introduce a benchmark to evaluate the text generation performance of video-based conversation models. To do this, we curate a test set based on the ActivityNet-200 dataset~\\cite{caba2015activitynet}, featuring videos with rich, dense descriptive captions and associated question-answer pairs from human annotations. We also develop an evaluation pipeline using the GPT-3.5 model. This pipeline assesses various capabilities of the model and assigns a relative score to the generated predictions on a scale of 1-5, in the following five aspects:\n\\begin{enumerate}[label=(\\roman*)] %,wide,parsep=0pt, itemsep=0pt, topsep=0pt\n    \\item \\textit{Correctness of Information:} We verify the accuracy of the generated text, ensuring it aligns with the video content and does not misinterpret or misinform.\n    \\item \\textit{Detail Orientation:} We evaluate the depth of the model's responses, looking for both completeness, meaning the model's response covers all major points from the video, and specificity, denoting the inclusion of specific details rather than just generic points in the model's response.\n    \\item \\textit{Contextual Understanding:} We assess the model's understanding of the video's context, checking if its responses align with the overall context of the video content.\n    \\item \\textit{Temporal Understanding:} We examine the model's grasp of the temporal sequence of events in the video when answering questions.\n    \\item \\textit{Consistency:} We evaluate the model's consistency across different but similar questions or different sections of the video.\n\\end{enumerate}\n\nWe present the evaluation results of our proposed model, Video-ChatGPT, using the quantitative benchmarking framework in Table \\ref{table:1}. The results reveal its competent performance across all key aspects compared with the recently introduced contemporary video conversation models, Video Chat~\\cite{2023videochat}, LLaMA Adapter~\\cite{gao2023llamaadapterv2} and Video-LLaMA~\\cite{damonlpsg2023videollama}. Video-ChatGPT shows good performance, largely due to the instruction tuning we perform and its straightforward architecture that leverages LLMs with a pretrained visual encoder fine-tuned for video data. This provides it with the robust ability to generate contextually relevant, detailed, and temporally accurate text from video input.\n\n\\noindent\\textbf{Zero-Shot Question-Answer Evaluation: } \nWe conducted a comprehensive quantitative evaluation using several commonly used open-ended question-answer datasets: MSRVTT-QA~\\cite{xu2017video}, MSVD-QA~\\cite{xu2017video}, TGIF-QA FrameQA~\\cite{jang2017tgif}, and ActivityNet-QA~\\cite{yu2019activitynet}. These evaluations were carried out in a zero-shot manner, employing GPT-assisted evaluation to assess the model's capabilities. This evaluation process measures the accuracy of the model's generated predictions and assigns a relative score on a scale of 1-5.\n\nTo benchmark Video-ChatGPT, we compared its performance with other significant models, such as FrozenBiLM~\\cite{yang2022zero} and the generative video model, Video Chat, LLaMA Adapter and Video-LLaMA. FrozenBiLM is a model that adapts frozen bidirectional language models pretrained on Web-scale text-only data to multi-modal inputs, showing promising results in zero-shot VideoQA settings. Despite the solid foundation established by these models, Video-ChatGPT consistently outperformed them, achieving state-of-the-art (SOTA) performance across all datasets. These results indicate Video-ChatGPT's ability to understand video content and generate accurate, contextually rich answers to questions.\n\n\\subsection{Ablations}\n\n\\noindent\n\\textbf{Impact of Semi-Automatic Annotations: } We train Video-ChatGPT on two subsets: one with human annotations (30\\% of our data) and one with semi-automatic annotations (70\\%). The results in Table.~\\ref{ablation_1} indicate that training solely with human-annotated data or semi-automatically generated data yields good performance.  The overall performance when using only human-generated data is the lowest due to the limited number of labels (30\\% of all data) available in this scenario. However, the optimal results are achieved when utilizing a combined dataset for training.\n\n\\begin{table}[h]\n\\centering\n\\label{table:video_chatgpt_comparison}\n\\resizebox{1.0\\columnwidth}{!}{%\n\\begin{tabular}{lccc}\n\\toprule\n\\textbf{Metric} & \\textbf{Human only} & \\textbf{Automatic only} & \\textbf{Combined} \\\\ \\midrule\nCorrectness & 2.27 & 2.35 & 2.40 \\\\\nDetail Orientation & 2.49 & 2.49 & 2.52 \\\\\nContextual Understanding & 2.50 & 2.56 & 2.62 \\\\\nTemporal Understanding & 1.85 & 1.92 & 1.98 \\\\\nConsistency & 2.21 & 2.38 & 2.37 \\\\\n\\midrule\nAverage & \\textbf{2.28} & \\textbf{2.34} & \\textbf{2.38} \\\\\n\\bottomrule\n\\end{tabular}%\n}\n\\caption{\\small \\textbf{Human Annotated vs Semi-automatically Annotated Data:} Training using both human annotated and semi-automatically annotated data achieves best performance.}\n\\label{ablation_1}\n\\end{table}\n\n\\noindent\n\\textbf{Quantitative Evaluation with GPT-3.5}:\nConsidering the limitations posed by the use of GPT-3.5, which is accessed via API and is not open-source, we perform evaluations using the open-source LLM, Vicuna-1.5 (13B)~\\cite{vicuna2023}. \nThe results in Table.~\\ref{ablation_2} show similar trend in correctness, detail, contextual and temporal understanding and consistency compared with the initial GPT-3.5 evaluation.\nThis ensures our evaluation method remains accessible and replicable.\n\n\\begin{table}[h]\n\\centering\n\\label{table:video_chatgpt_evaluation}\n\\resizebox{1.0\\columnwidth}{!}{%\n\\begin{tabular}{lccc}\n\\toprule\n\\textbf{Metric} & \\textbf{Video Chat} & \\textbf{Video-LLaMA} & \\textbf{Video-ChatGPT} \\\\ \\midrule\nCorrectness & 2.32 & 2.10 & 2.49 \\\\\nDetail Orientation & 2.50 & 2.18 & 2.52 \\\\\nContextual Understanding & 2.76 & 2.41 & 2.85 \\\\\nTemporal Understanding & 2.27 & 2.17 & 2.38 \\\\\nConsistency & 2.95 & 2.67 & 3.09 \\\\ \\bottomrule\n\\end{tabular}%\n}\n\\caption{\\small \\textbf{Evaluation using Vicuna-1.5 (13B) Model:} We observe similar trend when evaluating using open-source Vicuna-1.5 (13B) model versus GPT-3.5-Turbo.}\n\\label{ablation_2}\n\\end{table}\n\n\\noindent\n\\textbf{Ensuring Automatic Annotation Pipeline Consistency:}\nTo ensure consistency between our automatic evaluation pipeline and human assessments, we conducted a blind test comparing QA pairs from human and semi-automatically annotated sources using 50 randomly sampled videos. A 52\\% accuracy rate in distinguishing between the two demonstrated the reliability of our semi-automatic data, confirming that our quality control effectively aligns automatic evaluations with human judgment standards.\n\n\\section{Conclusion}\nIn this work, we presented Video-ChatGPT, a multimodal model that merges a pretrained visual encoder with a large language model (LLM) to enable video understanding and conversations based on videos. Video-ChatGPT leverages an adapter on top of pretrained LLM and vision backbones and is fine-tuned on video-instruction data to capture temporal dynamics and spatial consistency relationships in spatiotemporal sequences. A dataset of 100,000 video-instruction pairs is created to enhance Video-ChatGPT's video-specific understanding and conversation capabilities. The work also introduced a quantitative video conversation evaluation framework for benchmarking, evaluating models on a diverse set of capabilities including conventional video question answering as well as open-ended descriptions.\n\n\\section{Limitations}\nWhile the model performs competitively in several scenarios, we note it finds it challenging to understand subtle temporal relationships in long videos ($>$ 2 min), which can compromise its predictive performance. Additionally, it has difficulty recognizing the details of small objects, often missing additional information embedded in these details.\n\n\\section{Potential Risks}\nVideo-ChatGPT, like any other AI model, must be handled with due caution to prevent misuse and to ensure it upholds the principles of fairness, transparency, and respect for user privacy. \n\nWe made a concerted effort to minimize bias during the dataset creation phase for Video-ChatGPT. Despite these efforts, it is important to recognize the possibility of residual bias persisting. The use of our model should be mindful of these potential biases, which may subtly influence the model's understanding and response to visual content. We encourage all users to consider these limitations in their application of Video-ChatGPT and to strive for ethical and responsible use in all contexts.\n\n\\section{Use of Data and AI Assistant}\nWe curate our dataset based on a subset of the ActivityNet-200 dataset~\\cite{caba2015activitynet}, distributed under MIT LICENSE, available for use in research. \nFurther, the use of GPT models abides by~\\cite{openai}. \nRespecting source license information, we will release all datasets created in this work under MIT LICENSE.\n\n\\section{Human Annotations}\nThe semi-automatic dataset curation involves human annotation. Annotators are provided with concise video caption ground truths. Specific instructions are given to enrich the caption with comprehensive descriptions of the video content, with specific attention to temporal and spatial details. They are given specific instructions to neutralize the tone and biases during the correction process.\n\n\\section{Qualitative Evaluation}\nWe performed an extensive evaluation of our model on a variety of open-ended video question-answering tasks, utilizing diverse videos sourced from ActivityNet and YouTube. The evaluation tasks included video reasoning (\\Cref{fig:video_reasoning}), creative and generative tasks (see~\\Cref{fig:creative_generative}), spatial understanding (\\Cref{fig:spatial_understanding}), action recognition (\\Cref{fig:action_recognition}), video conversation (\\Cref{fig:video_understanding_conversation}), question answering (\\Cref{fig:question_answer}) and temporal understanding (\\Cref{fig:temporal_understanding}). Our model demonstrates proficiency in comprehending the content of the videos and generating accurate responses across multiple video-based tasks. Our model can effectively understand the visual information present in the videos and provide precise answers (see~\\Cref{fig:video_reasoning,fig:creative_generative,fig:spatial_understanding,fig:action_recognition,fig:video_understanding_conversation,fig:question_answer,fig:temporal_understanding}).\n\n\\begin{figure*}[h]\n  \\centering\n  \\includegraphics[width=0.75\\textwidth]{images/video_reasoning.jpg}\n  \\caption{\\textbf{Video Reasoning Task.} This figure illustrates an example from Video-ChatGPT's demonstration showcasing its performance in video reasoning tasks.}\n  \\label{fig:video_reasoning}\n\\end{figure*}\n\n\\begin{figure*}[h]\n  \\centering\n  \\includegraphics[width=0.75\\textwidth]{images/creative_and_generative.jpg}\n  \\caption{\\textbf{Creative and generative tasks}. Illustrative examples from Video-ChatGPT's demonstration highlight its performance in video-based creative and generative tasks, such as crafting a story, poem, or advertisement.}\n  \\label{fig:creative_generative}\n\\end{figure*}\n\n\\begin{figure*}[h]\n  \\centering\n  \\includegraphics[width=0.725\\textwidth]{images/spatial_understanding.jpg}\n  \\caption{\\textbf{Spatial understanding tasks.} The figure depicts examples from Video-ChatGPT's demonstration, emphasizing its capability in video-based spatial understanding tasks, including identifying renowned locations or counting the number of objects in a scene.}\n  \\label{fig:spatial_understanding}\n\\end{figure*}\n\n\\begin{figure*}[hb]\n  \\centering\n  \\includegraphics[width=0.725\\textwidth]{images/action_recognition.jpg}\n  \\caption{\\textbf{Actiong Recognition Task.} This figure illustrates examples from Video-ChatGPT's demonstration showcasing its performance in video action recognition tasks such as playing drums and grooming horse.}\n  \\label{fig:action_recognition}\n\\end{figure*}\n\n\\begin{figure*}[ht]\n  \\centering\n  \\includegraphics[width=0.75\\textwidth]{images/video_understanding_and_conversation.jpg}\n  \\caption{\\textbf{Video Understanding and Conversation Tasks.} This figure illustrates examples from Video-ChatGPT's demonstration showcasing its performance in video understanding and conversation tasks.}\n  \\label{fig:video_understanding_conversation}\n\\end{figure*}\n\n\\begin{figure*}[ht]\n  \\centering\n  \\includegraphics[width=0.75\\textwidth]{images/question_answering.jpg}\n  \\caption{\\textbf{Question-Answering Task.} The figure depicts examples Video-ChatGPT's demonstration showcasing its performance in question-answering tasks.}\n  \\label{fig:question_answer}\n\\end{figure*}\n\n\\begin{figure*}[ht]\n  \\centering\n  \\includegraphics[width=0.72\\textwidth]{images/temporal_understanding.jpg}\n  \\caption{\\textbf{Temporal Understanding Task.} The figure provides examples from Video-ChatGPT's demonstration, highlighting its performance in temporal understanding tasks, particularly in comprehending sequences of events.}\n  \\label{fig:temporal_understanding}\n\\end{figure*}\n\n\\clearpage\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{Video-LLaVA: Learning United Visual Representation by Alignment Before Projection}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\nLarge Vision-Language Model (LVLM) has enhanced the performance of various downstream tasks in visual-language understanding. Most existing approaches encode images and videos into separate feature spaces, which are then fed as inputs to large language models. However, due to the lack of unified tokenization for images and videos, namely misalignment before projection, it becomes challenging for a Large Language Model (LLM) to learn multi-modal interactions from several poor projection layers. \nIn this work, we unify visual representation into the language feature space to advance the foundational LLM towards a unified LVLM. As a result, we establish a simple but robust LVLM baseline, \\textbf{Video-LLaVA}, which learns from a mixed dataset of images and videos, mutually enhancing each other.\nAs a result, Video-LLaVA outperforms Video-ChatGPT by \\textbf{5.8\\%, 9.9\\%, 18.6\\%, and 10.1\\%} on MSRVTT, MSVD, TGIF, and ActivityNet, respectively. Additionally, our Video-LLaVA also achieves superior performances on a broad range of 9 image benchmarks.\nNotably, extensive experiments demonstrate that Video-LLaVA mutually benefits images and videos within a unified visual representation, outperforming models designed specifically for images or videos. We aim for this work to provide modest insights into the multi-modal inputs for the LLM.\n\\end{abstract}\n\n\\section{Introduction}\n\n\\label{sec:intro}\n\\begin{figure}[h]\n\\centering\n    \\includegraphics[width=1.0\\linewidth]{fig/sota.pdf}     \n\\caption{\\textbf{Comparing Different LVLM Paradigms.} Video-LLaVA aligns images and videos before projection, allowing LLM to learn from a unified visual representation and endowing LLM with the ability to comprehend both images and videos simultaneously.}\n\\label{fig:sota}\n\\end{figure}\n\nRecently, LLMs have gained rapid popularity in the AI community, such as GPT-3.5, GPT-4~\\cite{openai2023gpt4}, PaLM~\\cite{bi2020palm,anil2023palm}, and BLOOM~\\cite{scao2022bloom}. They rely on their powerful language comprehension abilities to follow human-provided instructions and provide corresponding responses. Typically, LLMs can only respond within the text input provided by the user, which is insufficient because human interaction with the world involves multiple channels, such as visual and textual. To this end, recent works~\\cite{ye2023mplug,zhu2023minigpt,alayrac2022flamingo} have mapped images into text-like tokens, enabling LLMs to emerge with the ability to comprehend images. Despite their effectiveness, empowering LLMs to understand videos is more challenging than image-only comprehension tasks. Nevertheless, recent work~\\cite{maaz2023video,li2023videochat,zhang2023video} has made initial strides in enabling interactions between video and language.\n\nHowever, most current LVLMs~\\cite{li2023blip,dai2023instructblip,luo2023valley,li2023otter,yin2023survey,fu2023mme} can primarily handle a single visual modality, either image-language or video-language. We compare different LVLM paradigms as shown in Figure~\\ref{fig:sota}, where VideoChat~\\cite{li2023videochat} and Video-LLaMA~\\cite{zhang2023video} utilize a share visual encoder to handle both images and videos. However, due to the inherent differences in the media types of images and videos, it is challenging to learn a unified representation, and the performance falls significantly behind that of the specialized video expert model, Video-ChatGPT. Therefore, X-LLM~\\cite{chen2023x} and Macaw-LLM~\\cite{lyu2023macaw} allocate a modality-specific encoder for each modality, attempting to enable a LLM to comprehend images or videos through several projection layers. But their performances are inferior to dedicated video expert models such as Video-ChatGPT~\\cite{maaz2023video}. We attribute this phenomenon to the lack of \\textit{\\textbf{alignment before projection}}. Because image features and video features reside in their own spaces, this poses a challenge for a LLM to learn their interactions from several poor projection layers. Some similar phenomenon such as \\textit{alignment before fusion} has been discussed by ALBEF~\\cite{li2021align} and ViLT~\\cite{kim2021vilt} in multi-model models. More recently, ImageBind-LLM~\\cite{han2023imagebind} focuses on enabling the LLM to simultaneously process multiple modal inputs by pre-aligning each modality to a common feature space~\\cite{girdhar2023imagebind}. Based on a large image-language model, ImageBind-LLM converts other modalities into the most similar image features by retrieving from a training-free image cached database. However, the indirect alignment approach of ImageBind-LLM may lead to performance degradation, and the LLM has no knowledge of actual video data.\n\nIn this work, we introduce \\textbf{Video-LLaVA}, a simple but powerful baseline for the LVLM simultaneously handling both images and videos. Specifically, As shown in Figure~\\ref{fig:sota}, Video-LLaVA initially aligns the representations of images and videos to a unified visual feature space. Since the visual representations are already aligned prior to projection, we employ a shared projection layer to map the unified visual representation for the LLM. To enhance computational efficiency, Video-LLaVA undergoes joint training of images and videos, achieving remarkable results with 1 training epoch. \n\nAs a result, The proposed Video-LLaVA greatly enhances the ability of the LLM to simultaneously understand both images and videos. For image understanding, Video-LLaVA surpasses advanced LVLMs such as mPLUG-owl-7B and InstructBLIP-7B in 5 image benchmarks. Additionally, utilizing 4 benchmark toolkits for a more comprehensive evaluation, Video-LLaVA-7B even outperforms IDEFICS-80B by 6.4\\% in MMBench. Moreover, similar trends can be observed in video understanding, where Video-LLaVA surpasses Video-ChatGPT by 5.8\\%, 9.9\\%, 18.6\\%, and 10.1\\% respectively on the MSVD, MSRVTT, TGIF, and ActivityNet video question-answering datasets. Extensive ablation experiments demonstrate that alignment before projection yields greater benefits. Additionally, joint training of images and videos can facilitate a unified visual representation in LLM comprehension.\n\nWe summarize our primary contributions as follows:\n\\begin{itemize}\n\\item We introduce \\textbf{Video-LLaVA}, a powerful LVLM baseline. During the training process, Video-LLaVA binds visual signals to the language feature space, unifying visual representations, and proposes a solution to align before projection. We enable an LLM to perform visual reasoning capabilities on both images and videos simultaneously.\n\\item Extensive experiments demonstrate that a unified visual representation benefits LLMs in learning to simultaneously handle both images and videos, validating the complementarity of modalities, showcasing significant superiority when compared to models specifically designed for either images or videos.\n\\end{itemize}\n\n\\section{Related Work}\n\\label{sec:related}\n\n\\subsection{Large Language Models}\n\nWhen the well-known commercial model ChatGPT~\\cite{openai2023gpt4} was introduced, the The AI community released open-source Large Language Models (LLMs) by instruction tuning and increasing model sizes. These include LLaMA~\\cite{touvron2023llama}, Vicuna~\\cite{chiang2023vicuna}, Alpaca~\\cite{taori2023stanford}, and more recently, LLaMA 2~\\cite{touvron2023llama2}. These models are tuned with instruction sets to emulate conversations between humans and AI assistants. Furthermore, InstructGPT~\\cite{ouyang2022training} is trained based on GPT-3~\\cite{brown2020language} with 175 billion parameters through aligning with human preferences. However, LLMs can only interact within text. In this work, we introduce Video-LLaVA, which builds upon the powerful reasoning capabilities of LLM to extend modality interactions to images and videos.\n\n\\begin{table*}\n  \\setlength\\tabcolsep{2.0mm}\n  \\caption{\\textbf{Comparison between different Large Vision-Language Models.} For methods that treat LLMs as scheduler, they do not require pre-alignment and joint training.}\n  \\label{tab:lvlm}\n  \\centering\n  \\begin{tabular}{lcccc}\n    \\toprule\n    \\textbf{Methods} & \\textbf{Image} & \\textbf{Video} & \\textbf{Pre-aligned} & \\textbf{Joint training} \\\\\n    \\midrule\n    \\multicolumn{3}{l}{\\textit{LLMs as scheduler}} \\\\ \n    VisualChatGPT~\\cite{wu2023visual} & \\textcolor{green}{\\ding{52}} & \\textcolor{red}{\\ding{55}} & - & - \\\\\n    HuggingGPT~\\cite{shen2023hugginggpt} & \\textcolor{green}{\\ding{52}} & \\textcolor{red}{\\ding{55}} & - & - \\\\\n    MM-REACT~\\cite{yang2023mm} & \\textcolor{green}{\\ding{52}} & \\textcolor{green}{\\ding{52}} & - & - \\\\\n    ViperGPT~\\cite{suris2023vipergpt} & \\textcolor{green}{\\ding{52}} & \\textcolor{green}{\\ding{52}} & - & - \\\\\n    \\midrule\n    \\multicolumn{3}{l}{\\textit{LLMs as decoder}} \\\\ \n    Mini-GPT4~\\cite{zhu2023minigpt} & \\textcolor{green}{\\ding{52}} & \\textcolor{red}{\\ding{55}} & - & \\textcolor{red}{\\ding{55}} \\\\\n    LLaVA~\\cite{liu2023visual} & \\textcolor{green}{\\ding{52}} & \\textcolor{red}{\\ding{55}} & - & \\textcolor{red}{\\ding{55}} \\\\\n    Video-ChatGPT~\\cite{maaz2023video} & \\textcolor{red}{\\ding{55}} & \\textcolor{green}{\\ding{52}} & - & \\textcolor{red}{\\ding{55}} \\\\\n    VideoChat~\\cite{li2023videochat} & \\textcolor{green}{\\ding{52}} & \\textcolor{green}{\\ding{52}} & \\textcolor{red}{\\ding{55}} & \\textcolor{green}{\\ding{52}} \\\\\n    Video-LLaMA~\\cite{zhang2023video} & \\textcolor{green}{\\ding{52}} & \\textcolor{green}{\\ding{52}} & \\textcolor{red}{\\ding{55}} & \\textcolor{green}{\\ding{52}} \\\\\n    ImageBind-LLM~\\cite{han2023imagebind} & \\textcolor{green}{\\ding{52}} & \\textcolor{green}{\\ding{52}} & \\textcolor{green}{\\ding{52}} & \\textcolor{red}{\\ding{55}} \\\\\n    \\midrule\n    \\rowcolor{blue} \\textbf{Video-LLaVA (Ours)} & \\textcolor{green}{\\ding{52}} & \\textcolor{green}{\\ding{52}} & \\textcolor{green}{\\ding{52}} & \\textcolor{green}{\\ding{52}} \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table*}\n\n\\subsection{Large Vision-Language Models}\nWhen extending LLMs to multi-modal, especially involving images and videos, the main approaches can be categorized into two types in Table~\\ref{tab:lvlm}: \\textit{i)} treating LLM as a scheduler, \\textit{ii)} treating LLM as a decoder.\n\n\\subsubsection{LLMs as scheduler} In the scheduler-based methods, various visual models are treated as plug-and-play modules. LLM schedules them according to the specific visual task requirements, like the assembly of building blocks. Some of these methods focus on images, such as VisualChatGPT~\\cite{wu2023visual} and HuggingGPT~\\cite{shen2023hugginggpt}, while MM-REACT~\\cite{yang2023mm} and ViperGPT~\\cite{suris2023vipergpt} can also handle videos. A key characteristic of these scheduler-based LVLMs is that they do not require end-to-end training, hence eliminating the need for pre-alignment and joint training of each modality. \n\n\\subsubsection{LLMs as decoder} Regarding the approach of treating LLM as a decoder, this is our primary focus. MiniGPT-4~\\cite{zhu2023minigpt} aligns image tokens to the input of the large language model through several linear projection layers. However, this alignment is weak and lacks feedback from human instructions. Subsequently, mPLUG-Owl~\\cite{ye2023mplug} adopts a two-stage training approach. In the first stage, images are aligned with language using an auto-regressive pretraining style, and the second stage involves instruction tuning through using a human instruction dataset. With the increasing scale of large language model backends, approaches such as InstructBLIP~\\cite{dai2023instructblip} and LLaVA series~\\cite{liu2023visual,liu2023improved, lin2024moe} collecte the larger human instruction datasets to train a larger LVLMs (13B parameters). Each answer of instruction datasets strictly follow to the given instructions. Then they undergo end-to-end training using human instruction datasets, enabling the LLM with visual reasoning capabilities. Moreover, Video-ChatGPT~\\cite{maaz2023video} design a 100k video instruction dataset, successfully empowering LLMs to comprehend videos. VideoChat~\\cite{li2023videochat} and Video-LLaMA~\\cite{zhang2023video} achieve this by conducting joint training, allowing LLMs to simultaneously handle images and videos. Expanding LLMs to additional visual modalities typically requires pre-alignment, as seen in LLaMA-Adapter~\\cite{zhang2023llama,gao2023llama} and ImageBind-LLM~\\cite{han2023imagebind}. They bind other modalities to the image space through ImageBind's~\\cite{girdhar2023imagebind} modality encoder. These models have demonstrated that a unified feature space is advantageous for enhancing LLM's multi-modal reasoning capabilities. Distinguished from prior work, Video-LLaVA not only pre-aligns image and video features but also conducts joint training of images and videos, facilitating LLMs in learning multi-modal reasoning capabilities from a unified visual representation.\n\n\\section{Video-LLaVA}\n\\label{sec:videollava}\n\n\\subsection{Model Structure}\n\n\\subsubsection{Framework Overview}\nAs shown in Figure~\\ref{fig:videollava}, Video-LLaVA consists of LanguageBind encoders $f_{\\mathbf{V}}$~\\cite{zhu2023languagebind} to extract features from the raw visual signal (images or videos), a large language model $f_{\\mathbf{L}}$ such as Vicuna, visual projection layers $f_{\\mathbf{P}}$ and a word embedding layer $f_{\\mathbf{T}}$. We initially obtain visual features using LanguageBind encoders. LanguageBind encoders are capable of mapping different modalities into the textual feature space, thereby providing us with a unified visual representation. Subsequently, the unified visual representation is encoded by shared projection layers, which is then combined with tokenized textual queries and fed into a large language model to generate corresponding responses.\n\n\\subsubsection{United Visual Representation}\nOur goal is to map images and videos into a shared feature space to enable the large language model to learn from a unified visual representation. We assume that the same information can be conveyed through multiple media. For example, \\texttt{a running dog} can be expressed through language, a image or a video simultaneously. Therefore, we can compress information from different modalities into a common feature space, allowing the model to extract information from a dense feature space, facilitating modality interactions and complementarity. Hence, we chose the modality encoders from LanguageBind~\\cite{zhu2023languagebind}, which align images and videos with the textual feature space.\n\n\\begin{figure*}[t]\n\\centering\n    \\includegraphics[width=1.0\\linewidth]{fig/Video-LLaVA.pdf}     \n\\caption{\\textbf{Training framework and performance.} Video-LLaVA exhibits remarkable interactive capabilities between images and videos, despite the absence of image-video pairs in the dataset. (a) The Video-LLaVA framework demonstrates a data flow that generates corresponding responses based on input instructions. (b) Video-LLaVA achieves superior performances on a broad range of 15 datasets across image and video.}\n\\label{fig:videollava}\n\\end{figure*}\n\n\\subsubsection{Alignment Before Projection}\nSpecifically, LanguageBind initializes from OpenCLIP~\\cite{ilharco_gabriel_2021_5143773}, naturally aligning images and language in a shared feature space. Subsequently, it aligns video representations to the language space using 3 million video-text pairs from VIDAL-10M~\\cite{zhu2023languagebind}. By sharing a language feature space, the image and video representations ultimately converge into a unified visual feature space, which we refer to as emergent alignment of images and videos. Therefore, our video encoder and image encoder are initialized from the LanguageBind encoders zoo, pre-aligning the inputs for LLM and reducing the gap between representations of different visual signals. The unified visual representation is fed into LLM after passing through a shared projection layer.\n\n\\subsection{Training Pipeline}\nOverall, the process of generating responses by Video-LLaVA is similar to that of a large language model (GPT series). Given a textual input $\\mathbf{X}_{\\text{T}}$ and visual signals $\\mathbf{X}_{\\text{V}}$, the input signals are encoded into a sequence of tokens according to Equation~\\ref{eq:enc}. By maximizing the likelihood probability in Equation~\\ref{eq:gen}, the model ultimately achieves multi-modal understanding capabilities.\n\n\\begin{equation}\n    \\mathbf{Z}_{\\text{T}}=f_{\\mathbf{T}}\\left(\\mathbf{X}_{\\text{T}}\\right), \n    \\mathbf{Z}_{\\text{V}}=f_{\\mathbf{P}}\\left(f_{\\mathbf{V}}\\left(\\mathbf{X}_{\\text{V}}\\right)\\right)\n  \\label{eq:enc}\n\\end{equation}\n\\begin{equation}\n  p\\left(\\mathbf{X}_{\\text{A}} \\mid \\mathbf{X}_{\\text{V}},\\mathbf{X}_{\\text{T}}\\right)=\\prod_{i=1}^L p_\\theta\\left(\\mathbf{X}_{\\text{A}}^{[i]} \\mid \\mathbf{Z}_{\\text{V}}, \\mathbf{Z}_{\\text{T}}^{[1: i-1]}\\right)\n  \\label{eq:gen}\n\\end{equation}\nwhere $L$ is the length of the generated sequence $\\mathbf{X}_{\\text{A}}$, and $\\theta$ is a trainable parameter. We dynamically conduct joint training on images and videos, wherein a single batch contains both image and video samples simultaneously.\n\n\\subsubsection{Understanding Training}\nAt this stage, the model is required to acquire the ability to interpret visual signals within an extensive image/video-text pair dataset. Each visual signal corresponds to a single round of conversation data $(\\mathbf{X}_{\\mathrm{q}}, \\mathbf{X}_{\\mathrm{a}})$, where $\\mathbf{X}_{\\text{T}}=\\mathbf{X}_{\\mathrm{q}}$ and $\\mathbf{X}_{\\mathrm{a}}$ is the ground truth. The training objective of this stage is the original auto-regressive loss, where the model learns the basic ability to view the vision. We freeze the other parameters of the model during this process.\n\n\\subsubsection{Instruction Tuning}\nIn this stage, the model is required to provide responses corresponding to different instructions. These instructions often involve more complex visual comprehension tasks, rather than just describing visual signals. Note that the conversation data $\\left(\\mathbf{X}_{\\mathrm{q}}^1, \\mathbf{X}_{\\mathrm{a}}^1, \\cdots, \\mathbf{X}_{\\mathrm{q}}^N, \\mathbf{X}_{\\mathrm{a}}^N\\right)$ consists of multiple rounds.\n\\begin{equation}\n\\mathbf{X}_{\\text{T}}^r=\\left\\{\\begin{array}{lr}\n\\mathbf{X}_{\\mathrm{q}}^1, & r=1 \\\\\n\\text{Concat}(\\mathbf{X}_{\\mathrm{q}}^{r-1}, \\mathbf{X}_{\\text{A}}^{r-1}, \\mathbf{X}_{\\mathrm{q}}^r), & r>1\n\\end{array}\\right.\n  \\label{eq:tuning}\n\\end{equation}\nwhere $r$ represents the round number. As shown in Equation~\\ref{eq:tuning}, when $r>1$ we concatenate the conversations from all previous rounds with the current instruction as the input for this round. The training objective remains the same as in the previous stage. After this stage, the model learns to generate corresponding responses based on different instructions and requests. The LLM are also involved in training at this stage.\n\n\\section{Experiments}\n\\label{sec:exp}\n\n\\subsection{Experimental Setup}\n\n\\begin{figure}\n\\vspace{-0.4cm} \n\\centering\n    \\includegraphics[width=1.0\\linewidth]{fig/data.pdf}     \n    \\caption{\\textbf{Data composition for training Video-LLaVA.} The dataset for stage 1 consists of single-turn conversation, focusing on concise visual descriptions. In stage 2, the dataset comprises multi-turn conversations, emphasizing complex visual reasoning abilities.}\n    \\label{fig:data}\n\\vspace{-0.4cm} \n\\end{figure}\n\n\\subsubsection{Data Details} In \\ref{fig:data}, for the first stage of understanding pretraining, we use a subset of 558K LAION-CC-SBU image-text pairs with BLIP~\\cite{li2022blip} captions, which is sourced from CC3M~\\cite{sharma2018conceptual} and filtered by LLaVA~\\cite{liu2023visual}. The video-text pairs are derived from a subset provided by Valley~\\cite{luo2023valley}, and we have access to 702k out of a total of 703k pairs, originating from WebVid~\\cite{bain2021frozen}. For the stage of instruction tuning, We gathered instructional datasets from two sources, including a 665k image-text instruction dataset from LLaVA 1.5~\\cite{liu2023improved} and a 100k video-text instruction dataset from Video-ChatGPT~\\cite{maaz2023video}.\n\n\\subsubsection{Model Settings} We employ Vicuna-7B v1.5 as the large language model. The visual encoders are derived from LanguageBind, initialized from OpenCLIP-L/14. The text tokenizer is sourced from LLaMA, with approximately 32,000 classes. The share projection layers consist of 2 fully connected layers with a GeLU~\\cite{hendrycks2016gaussian} activated function.\n\n\\subsubsection{Training Details} In the training process, we resize and crop each image, resulting in a size of 224×224 for each processed image. We uniformly sample 8 frames from each video, and each frame undergoes image pre-processing. The data in each batch is a random combination of images and videos. In the first stage, we train for one epoch with a batch size of 256, using the AdamW optimizer with a cosine learning rate schedule. In the second stage, we reduce the batch size to 128. The initial learning rate for both stages is set to 1e-3, with a warmup ratio of 0.03. Additional hyper-parameter settings can be found in the appendix.\n\n\\subsection{Quantitative Evaluation}\n\n\\begin{table*}[t]\n  \\setlength\\tabcolsep{1.55mm}\n  \\caption{\\textbf{Comparison between different LVLMs on video reasoning benchmarks}. We employ ChatGPT-Assistant to evaluate the performance following Video-ChatGPT~\\cite{maaz2023video}. The version of ChatGPT is ``gpt-3.5-turbo''.}\n  \\label{tab:video_qa}\n  \\centering\n  \\begin{tabular}{lc|cc|cc|cc|cc}\n    \\toprule\n    \\multirow{2}{*}{\\textbf{Methods}} & \\multirow{1}{*}{\\textbf{LLM}} & \\multicolumn{2}{c|}{\\textbf{MSVD-QA}} & \\multicolumn{2}{c|}{\\textbf{MSRVTT-QA}} & \\multicolumn{2}{c|}{\\textbf{TGIF-QA}} & \\multicolumn{2}{c}{\\textbf{ActivityNet-QA}} \\\\\n     & \\multirow{1}{*}{\\textbf{size}} & Accuracy & Score & Accuracy & Score & Accuracy & Score & Accuracy & Score \\\\\n     \\midrule\n    FrozenBiLM & 1B & 32.2 & - & 16.8 & - & 41.0 & - & 24.7 & - \\\\\n    VideoChat & 7B & 56.3 & 2.8 & 45.0 & 2.5 & 34.4 & 2.3 & - & 2.2 \\\\\n    LLaMA-Adapter & 7B & 54.9 & 3.1 & 43.8 & 2.7 & - & - & 34.2 & 2.7 \\\\\n    Video-LLaMA & 7B & 51.6 & 2.5 & 29.6 & 1.8 & - & - & 12.4 & 1.1 \\\\\n    Video-ChatGPT & 7B & 64.9 & 3.3 & 49.3 & 2.8 & 51.4 & 3.0 & 35.2 & 2.7 \\\\\n    Chat-UniVi & 7B & \\underline{65.0} & \\underline{3.6} & \\underline{54.6} & \\underline{3.1} & \\underline{60.3} & \\underline{3.4} & \\textbf{45.8} & \\underline{3.2} \\\\\n    \\rowcolor{blue} \\textbf{Video-LLaVA} & \\textbf{7B} & \\textbf{70.7} & \\textbf{3.9} & \\textbf{59.2} & \\textbf{3.5} & \\textbf{70.0} & \\textbf{4.0} & \\underline{45.3} & \\textbf{3.3} \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table*}\n\n\\begin{table*}[t]\n  \\setlength\\tabcolsep{1.0mm}\n  \\caption{\\textbf{Comparison between different LVLMs on image understanding benchmarks.} ``Res.'', ``L'', ``V'' respectively represent the input image resolution, LLaMA~\\cite{touvron2023llama} and Vicuna~\\cite{chiang2023vicuna}. Benchmark names are abbreviated due to page limitations. VQA-v2~\\cite{goyal2017making}; GQA~\\cite{hudson2019gqa}; VisWiz~\\cite{gurari2018vizwiz}; SQA$^\\text{I}$: ScienceQA-IMG~\\cite{lu2022learn}; VQA$^\\text{T}$: TextVQA~\\cite{singh2019towards}; POPE~\\cite{li2023evaluating}; MMB: MMBench~\\cite{liu2023mmbench}; LLaVA$^\\text{W}$: LLaVA-Bench (In-the-Wild)~\\cite{liu2023visual}; MM-Vet~\\cite{yu2023mm}. $^\\dag$ donates that we reproduce LLaVA-1.5 with LanguageBind-Image encoder to compare fairly. $^*$ donates that there is some overlap in the training data.}\n  \\label{tab:image_res}\n  \\centering\n  \\begin{tabular}{llc|ccccc|cccc}\n    \\toprule\n     \\multirow{2}{*}{\\textbf{Methods}} & \\multirow{2}{*}{\\textbf{LLM}} & \\multirow{2}{*}{\\textbf{Res.}} & \\multicolumn{5}{c|}{\\textbf{Image Question Answering}} & \\multicolumn{4}{c}{\\textbf{Benchmark Toolkit}} \\\\\n      &  &  & VQA$^\\text{v2}$ & GQA & VisWiz & SQA$^\\text{I}$ & VQA$^\\text{T}$ & POPE & MMB & LLaVA$^\\text{W}$ & MM-Vet \\\\\n    \\midrule\n    \\rowcolor{mygray} \\color{gray} LLaVA-1.5 & \\color{gray} V-7B & \\color{gray} 336 & \\color{gray} - & \\color{gray} 62.0$^*$ & \\color{gray} - & \\color{gray} - & \\color{gray} - & \\color{gray} - & \\color{gray} - & \\color{gray} - & \\color{gray} 30.5 \\\\\n    \\rowcolor{mygray} \\color{gray} BLIP-2 & \\color{gray} V-13B & \\color{gray} 224 & \\color{gray} 41.0 & \\color{gray} 41.0 & \\color{gray} 19.6 & \\color{gray} 61.0 & \\color{gray} 42.5 & \\color{gray} 85.3 & \\color{gray} - & \\color{gray} 38.1 & \\color{gray} 22.4 \\\\\n    \\rowcolor{mygray} \\color{gray} InstructBLIP & \\color{gray} V-13B & \\color{gray} 224 & \\color{gray} - & \\color{gray} 49.5 & \\color{gray} 33.4 & \\color{gray} 63.1 & \\color{gray} 50.7 & \\color{gray} 78.9 & \\color{gray} - & \\color{gray} 58.2 & \\color{gray} 25.6 \\\\\n    \\rowcolor{mygray} \\color{gray} IDEFICS-80B & \\color{gray} L-65B & \\color{gray} 224 & \\color{gray} 60.0 & \\color{gray} 45.2 & \\color{gray} 36.0 & \\color{gray} - & \\color{gray} 30.9 & \\color{gray} - & \\color{gray} 54.5 & \\color{gray} - & \\color{gray} - \\\\\n    MiniGPT-4 & L-7B & 224 & - & 30.8 & 47.5 & 25.4 & 19.4 & - & 23.0 & - & 22.1 \\\\\n    IDEFICS-9B & L-7B & 224 & {50.9} & 38.4 & 35.5 & - & 25.9 & - & {48.2} & - & - \\\\\n    mPLUG-Owl & L-7B & 224 & - & 14.0 & 39.0 & 2.8 & 38.8 & - & 46.6 & - & - \\\\\n    Otter & L-7B & 224 & - & 38.1 & \\textbf{50.0} & 27.2 & 21.2 & - & 32.6 & - & 24.6 \\\\\n    InstructBLIP & V-7B & 224 & - & {49.2} & 34.5 & {60.5} & \\underline{50.1} & - & 36.0 & {60.9} & \\underline{26.2} \\\\\n    LLaVA-1.5$^{\\dag}$ & V-7B & 224 & \\underline{72.3}$^*$ & \\underline{56.9}$^*$ & 47.8 & \\textbf{67.9} & 49.2 & \\underline{83.3} & \\underline{59.5} & \\underline{63.3} & 25.7 \\\\\n    \\rowcolor{blue} \\textbf{Video-LLaVA} & \\textbf{V-7B} & \\textbf{224} & \\textbf{74.7}$^*$ & \\textbf{60.3}$^*$ & \\underline{48.1} & \\underline{66.4} & \\textbf{51.8} & \\textbf{84.4} & \\textbf{60.9} & \\textbf{73.1} & \\textbf{32.0} \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table*}\n\n\\subsubsection{Zero-shot Video Understanding}\nAs shown in Table~\\ref{tab:video_qa}, we conduct a quantitative assessment of the video question-answering capabilities of large video-language models on four datasets, including MSVD-QA~\\cite{chen2011collecting}, MSRVTT-QA~\\cite{xu2016msr}, TGIF-QA~\\cite{jang2017tgif} and ActivityNet-QA~\\cite{yu2019activitynet}. The evaluation pipeline for video understanding follows Video-ChatGPT. We report the accuracy and score, which is assessed using GPT-Assistant. Video-LLaVA consistently outperforms Video-ChatGPT in terms of question-answering accuracy, which is an advanced large video-language model. Moreover, Video-LLaVA surpasses the powerful baseline of Video-ChatGPT by 5.8\\%, 9.9\\%, 18.6\\%, and 10.1\\% on MSRVTT, MSVD, TGIF, and ActivityNet, respectively. Additionally, we conduct comparisons with the recent SOTA model, Chat-UniVi~\\cite{Chat-UniVi}. Despite Chat-UniVi utilizing more datasets such as MIMIC-IT~\\cite{li2023otter}, Video-LLaVA still demonstrate competitive results, surpassing Chat-UniVi on MSVD, MSRVTT, and TGIF datasets. In summary, these results validate Video-LLaVA's ability to comprehend videos and provide contextually appropriate responses based on instructions.\n \n\n\\begin{table*}[t]\n  \\setlength\\tabcolsep{0.65mm}\n  \\caption{\\textbf{Zero-shot object hallucination evaluation results} are reported for three POPE evaluation settings. ``Yes'' indicates the proportion of positive responses to the given question. $^\\dag$ donates that we reproduce LLaVA-1.5 with LanguageBind-Image encoder to compare fairly.}\n  \\label{tab:pope}\n  \\centering\n  \\begin{tabular}{ll|ccc|ccc|ccc}\n    \\toprule\n    \\multirow{2}{*}{\\textbf{Methods}} & \\multirow{2}{*}{\\textbf{LLM}}& \\multicolumn{3}{c|}{\\textbf{Adersarial}} & \\multicolumn{3}{c|}{\\textbf{Popular}}  & \\multicolumn{3}{c}{\\textbf{Random}}  \\\\\n     &  & Accuracy & F1-Score & Yes & Accuracy & F1-Score & Yes & Accuracy & F1-Score & Yes \\\\\n    \\midrule\n    \\rowcolor{mygray} \\color{gray} MiniGPT-4 & \\color{gray} V-13B & \\color{gray} 66.6 & \\color{gray} 71.4 & \\color{gray} 66.7 & \\color{gray} 68.3 & \\color{gray} 72.2 & \\color{gray} 64.1 & \\color{gray} 77.8 & \\color{gray} 78.9 & \\color{gray} 54.8 \\\\\n    \\rowcolor{mygray} \\color{gray} InstructBLIP & \\color{gray} V-13B & \\color{gray} {74.4} & \\color{gray} {78.5} & \\color{gray} 69.0 & \\color{gray} {81.4} & \\color{gray} {83.5} & \\color{gray} 62.6 & \\color{gray} {88.7} & \\color{gray} {89.3} & \\color{gray} 55.2 \\\\\n    MM-GPT & L-7B & 50.0 & 66.7 & 100.0 & 50.0 & 66.7 & 100.0 & 50.0 & 66.7 & 100.0 \\\\\n    mPLUG-Owl & L-7B & 50.7 & 66.8 & 98.7 & 50.9 & 66.9 & 98.6 & 54.0 & 66.4 & 95.6 \\\\\n    Chat-UniVi & V-7B & {55.6} & {68.7} & 91.6 & {56.4} & {69.0} & 90.8 & {73.9} & {79.3} & 74.6 \\\\\n    LLaVA-1.5$^\\dag$ & L-7B & \\textbf{84.3} & \\textbf{83.2} & 43.5 & \\underline{79.8} & \\underline{79.4} & 48.0 & \\underline{85.7} & \\underline{84.8} & 43.0 \\\\\n    \\rowcolor{blue} \\textbf{Video-LLaVA} & \\textbf{V-7B} & \\underline{81.6} & \\underline{80.8} & 45.8 & \\textbf{85.3} & \\textbf{84.0} & 42.1 & \\textbf{86.2} & \\textbf{85.2} & 42.0 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table*}\n\n\\subsubsection{Zero-shot Image Question-answering} As shown in Table~\\ref{tab:image_res}, we evaluate our approach for image understanding on five academic image question-answering benchmarks. Compared to the state-of-the-art model InstructBLIP-7B, Video-LLaVA demonstrates powerful image understanding capabilities, outperforming across all five question-answering benchmarks. Additionally, Video-LLaVA exhibits competitive results compared to several more powerful LVLMs, which are tuned based on 13B or 65B LLM, such as surpassing InstructBLIP-13B by 14.7\\% on VisWiz, highlighting its strong understanding ability in natural visual environments. Furthermore, to ensure a fair comparison, we replace the image encoder in LLaVA-1.5 with the LanguageBind-Image encoder, called LLaVA-1.5$^{\\dag}$. This demonstrates that the performance improvement observed in Video-LLaVA is not solely attributed to a stronger image encoder. Additional details can be found in Section~\\ref{sec:jt_img}.\n\n\\vspace{0.1cm}\n\\noindent\\textbf{Evaluation under Image Benchmark Toolkits} Additionally, we evaluate LVLMs using several benchmark toolkits for visual instruction tuning. These benchmark toolkits provide a detailed assessment of the model's capabilities through robust evaluation metrics. Video-LLaVA outperform InstructBLIP-7B by 24.9\\%, 12.2\\%, and 5.8\\% on MMBench, LLaVA-Bench, and MM-Vet, respectively. It is worth noting that Video-LLaVA-7B still demonstrates advanced performance compared to larger LLM models, surpassing InstructBLIP-13B by 6.4\\% on MM-Vet and IDEFICS-80B~\\cite{laurencon2023obelics} by 6.4\\% on MMBench. These results demonstrate that Video-LLaVA exhibits a strong understanding of semantic aspects of scenes, enabling it to answer open-ended and free-form natural language questions about images.\n\n\\subsubsection{Object Hallucination Evaluation} As shown in Table~\\ref{tab:pope}, we report evaluation results for zero-shot object hallucinations, utilizing a evaluation pipeline derived from a polling-based query method~\\cite{li2023evaluating}. Video-LLaVA demonstrates competitive performance across three subsets: random, popular, and adversarial. Specifically, when compared to the 7B foundation model, Video-LLaVA consistently outperforms MM-GPT~\\cite{gong2023multimodal} across all three POPE hallucination evaluation subsets. Furthermore, when benchmarked against the larger 13B LLM, Video-LLaVA even surpasses Mini-GPT4 comprehensively. The successful performance of Video-LLaVA in object hallucination detection validates the consistency between unified visual representations and the generation of textual descriptions.\n\n\\subsection{Ablation Results}\n\n\\subsubsection{Alignment Before Projection} To validate the performance degradation caused by separated visual representation, we conduct experiments to to explore the performance of the LLM learning from different visual representations. We define the use of LanguageBind image encoder as unified visual representation while the MAE encoder~\\cite{he2022masked} use separated visual representation, which is a well-known and effective image feature extractor. Additionally, since MAE do not interact with multi-modal inputs during the training process, we utilize CLIP-L/14, a model of the same size. While CLIP-L/14 exhibits strong multimodal understanding capabilities, it is not pre-aligned with the video encoder. Consequently, this results in a lack of uniformity in the visual features provided to LLM. We only replace the image encoder of the same scale and keep the LanguageBind video encoder. \n\n\\begin{table*}[t]\n  \\setlength\\tabcolsep{0.55mm}\n  \\caption{\\textbf{Effect of alignment before projection on image.} ``\\textcolor[rgb]{0.12,0.47,0.71}{United}'' refers to the unified visual representation, while ``\\textcolor[rgb]{1.0,0.5,0.05}{Separated}'' refers to the separated visual representation. Benchmark names are abbreviated due to page limitations.}\n  \\label{tab:uvr_fig}\n  \\centering\n  \\begin{tabular}{l|ccccc|cccc}\n    \\toprule\n     \\multirow{2}{*}{\\textbf{Methods}} & \\multicolumn{5}{c|}{\\textbf{Image Question Answering}} & \\multicolumn{4}{c}{\\textbf{Benchmark Toolkit}} \\\\\n     & VQA$^\\text{v2}$ & GQA & VisWiz & SQA$^\\text{I}$ & VQA$^\\text{T}$ & POPE & MMB & LLaVA$^\\text{W}$ & MM-Vet \\\\\n    \\midrule\n    \\textcolor[rgb]{1.0,0.5,0.05}{Separated}-MAE & 66.0 & 55.4 & 42.5 & 65.0 & 44.2 & \\underline{80.8} & 45.7 & 35.9 & 20.0 \\\\\n    \\textcolor[rgb]{1.0,0.5,0.05}{Separated}-CLIP & \\underline{74.6} & \\underline{59.9} & \\underline{47.8} & \\textbf{67.3} & \\underline{51.5} & \\textbf{84.4} & \\underline{60.2} & \\underline{68.9} & \\underline{30.6} \\\\\n    \\textcolor[rgb]{0.12,0.47,0.71}{United} & \\textbf{74.7} & \\textbf{60.3} & \\textbf{48.1} & \\underline{66.4} & \\textbf{51.8} & \\textbf{84.4} & \\textbf{60.9} & \\textbf{73.1} & \\textbf{32.0} \\\\\n     $\\Delta$ \\textcolor{sgreen}{$Acc.$} & \\textcolor{sgreen}{{\\textbf{+0.1}}} & \\textcolor{sgreen}{{\\textbf{+0.4}}} &\\textcolor{sgreen}{{\\textbf{+0.3}}} &\\textcolor{gray}{{\\textbf{-0.9}}} &\\textcolor{sgreen}{{\\textbf{+0.3}}} &\\textcolor{sgreen}{{\\textbf{+0.0}}} &\\textcolor{sgreen}{{\\textbf{+0.7}}} &\\textcolor{sgreen}{{\\textbf{+4.2}}} &\\textcolor{sgreen}{{\\textbf{+1.4}}} \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table*}\n\n\\begin{table*}[t]\n  \\setlength\\tabcolsep{0.5mm}\n  \\caption{\\textbf{Effect of joint training on video.} We evaluate on four video question-answering datasets. $^*$ denotes that we utilized only video data in both the first and second stages.}\n  \\label{tab:jl_tab}\n  \\centering\n  \\begin{tabular}{l|cc|cc|cc|cc}\n    \\toprule\n    \\multirow{2}{*}{\\textbf{Methods}} & \\multicolumn{2}{c|}{\\textbf{MSVD-QA}} & \\multicolumn{2}{c|}{\\textbf{MSRVTT-QA}} & \\multicolumn{2}{c|}{\\textbf{TGIF-QA}} & \\multicolumn{2}{c}{\\textbf{ActivityNet-QA}} \\\\\n     & Accuracy & Score & Accuracy & Score & Accuracy & Score & Accuracy & Score \\\\\n     \\midrule\n    Video-LLaVA$^*$ & 64.8 & 3.2 & 58.3 & 3.4 & 67.8 & 3.4 & 40.7 & 2.0 \\\\\n    Joint with Image & \\textbf{70.7} & \\textbf{3.9} & \\textbf{59.2} & \\textbf{3.5} & \\textbf{70.0} & \\textbf{4.0} & \\textbf{45.3} & \\textbf{3.3} \\\\\n    $\\Delta$ \\textcolor{sgreen}{$Acc.$} & \\textcolor{sgreen}{{\\textbf{+5.9}}} & \\textcolor{sgreen}{{\\textbf{+0.7}}} & \\textcolor{sgreen}{{\\textbf{+0.9}}} & \\textcolor{sgreen}{{\\textbf{+0.1}}} & \\textcolor{sgreen}{{\\textbf{+2.2}}} & \\textcolor{sgreen}{{\\textbf{+0.6}}} & \\textcolor{sgreen}{{\\textbf{+4.6}}} & \\textcolor{sgreen}{{\\textbf{+1.3}}} \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table*}\n\n\\begin{figure}[t]\n\\centering\n    \\includegraphics[width=1.0\\linewidth]{fig/uvr_video_fig.pdf}     \n  \\caption{\\textbf{Effect of alignment before projection on video.} We validate and report the accuracy and score on four video question-answering datasets.}\n\\label{fig:uvr_video}\n\\end{figure}\n\n\\begin{figure}[t]\n\\centering\n    \\includegraphics[width=1.0\\linewidth]{fig/jl_fig.pdf}     \n\\caption{\\textbf{Effect of joint training on image.} $^\\dag$  donates that We reproduce the results of LLaVA-1.5 at a resolution of 224×224 \n with LanguageBind-Image encoder for a fair comparison.}\n\\label{fig:jl_fig}\n\\end{figure}\n\n\\subsubsection{For Video Understanding} Due to replacing the image encoder with the MAE encoder, the video features and image features are no longer unified during LLM's initial learning of visual representations. In Figure~\\ref{fig:uvr_video}, compared to separated visual representation, the united visual representation significantly improves performance across 4 video question-answering datasets. Separated visual representations not only exhibit lower accuracy in question-answering, but also demonstrate a similar trend in answer scores. These results demonstrate that the unified visual representation can help the LLM further learn and understand videos.\n\n\\subsubsection{For Image Understanding} The unified visual representation demonstrates strong performance, surpassing the separated visual representation comprehensively across 5 image question-answering datasets and 4 benchmark toolkits in Table~\\ref{tab:uvr_fig}. Additionally, we observe a significant margin of performance improvement in the unified visual representation on the MMBench, LLaVA-Bench, and MM-Vet benchmark toolkits. This highlights that the unified visual representation not only enhances performance in image question-answering but also provides benefits in other aspects of image understanding, such as reducing object hallucination and improving OCR capabilities.\n\n\\subsubsection{Joint Training}\nThis subsection aims to validate the complementarity of images and videos during joint training, which can mutually enhance the LLM's understanding of images and videos based on a unified visual representation. \n\n\\subsubsection{For Video Understanding} For comparing performance on video benchmarks, we remove image data during the training of Video-LLaVA, which is called Video-LLaVA$^*$. We compare with Video-LLaVA$^*$ to assess the performance gains from joint image training on video benchmarks. In Table~\\ref{tab:jl_tab}, we evaluate our model on four video question-answering datasets. Compared to Video-LLaVA$^*$ without image in training, the model trained with joint images and videos achieves comprehensive improvements across all four video datasets. These results demonstrate that joint training of images and videos facilitates LLM's understanding of visual representations.\n\n\\subsubsection{For Image Understanding}\n\\label{sec:jt_img}\nWhen comparing performance on image benchmarks, it is challenging to find a image-based LVLM with the same configuration as Video-LLaVA. To address this, we replace the image encoder in LLaVA-1.5 with the LanguageBind-Image encoder and reproduce the results at a resolution of 224×224 by using the same training configuration, called LLaVA-1.5$^\\dag$. As shown in Figure~\\ref{fig:jl_fig}, Compared to LLaVA-1.5$^\\dag$, which utilizes the same image encoder configuration, we observe performance improvements in 8 out of 9 benchmarks, demonstrating mutual improvement in visual understanding. Video-LLaVA outperform LLaVA-1.5$^\\dag$ in POPE, indicating that joint training with videos alleviates the object hallucination in images. The similar trend is observed on some other benchmark toolkits, such as LLaVA-Bench and MMBench, where video data significantly improves LLM's performance in complex reasoning and image conversation tasks. \n\n\\section{Limitation and Future Directions}\n\n\\subsection{Limitation} While Video-LLaVA exhibits strong competitiveness in both images and videos, we still observed some limitations of Video-LLaVA. To begin with, Video-LLaVA performs moderately in understanding long videos. In Table~\\ref{tab:video_qa}, Chat-UniVi surpasses 0.5 on ActivityNet-QA because Video-LLaVA only utilizes uniformly sampled 8 frames to comprehend the video, which results in the loss of detailed information from long videos. Additionally, training Video-LLaVA is computationally expensive, requiring 3-4 days to complete the training process on 8 A100-80G GPUs. \n\n\\subsection{Future Directions} In the future, We maybe can explore more efficient shared projection mode that can compress tokens while preserving data features. This would support Video-LLaVA in better understanding long videos. Besides, Video-LLaVA can serve as a baseline to extend to additional visual-related modalities, such as depth and infrared images. Additionally, we could explore how to incorporate timestamp embeddings effectively, enabling large visual-language models to answer questions related to temporal relationships.\n\n\\section{Conclusion}\n\\label{sec:conclu}\nIn this work, we introduce Video-LLaVA, a simple but powerful large visual-language baseline model. We propose a novel framework to address the issue of misalignment before projection, utilizing a LanguageBind encoder to pre-bind visual signals into the language feature space. To enable a LLM to comprehend both images and videos simultaneously, we conduct joint training on images and videos, allowing the LLM to learn multi-modal interactions from a unified visual representation. Extensive experiments demonstrate that joint training on images and videos mutually benefits performance. Furthermore, we validate that aligning visual representations before projection aids LLM learning. Remarkably, LLM, after learning from a unified visual representation, exhibits the remarkable ability to simultaneously engage with both images and videos, showcasing a powerful comprehension of unified visual concepts. These results collectively demonstrate the effectiveness of the Video-LLaVA training framework. As a unified visual training framework, the performance of Video-LLaVA even surpasses that of expert models designed specifically for images or videos.\n\n\\section*{Acknowledgments}\n\nThis work was supported in part by the Natural Science Foundation of China (No. 62202014, 62332002, 62425101), Shenzhen Basic Research Program (No.JCYJ20220813151736001).\n\n\\clearpage\n\n\\clearpage\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2306.02858v4.tex",
        "arXiv-2306.05424v2.tex",
        "arXiv-2311.10122v3.tex"
    ],
    "group_id": "group_81",
    "response": "### Title: Advances in Large Vision-Language Models for Video Understanding\n\n### Introduction\nThe field of Large Vision-Language Models (LVLMs) has seen significant advancements in recent years, driven by the integration of powerful Large Language Models (LLMs) with visual encoders. LVLMs are designed to enhance the understanding and generation capabilities of LLMs by incorporating visual information, thereby enabling them to interact more effectively with the world through multiple modalities. The history of LVLMs is rooted in the development of foundational models like CLIP and LLaMA, which have been adapted to handle visual inputs. However, the challenge remains in effectively aligning visual and auditory signals with textual inputs, especially in the context of videos. This summary will focus on three recent papers that address these challenges, each proposing unique solutions to improve the multi-modal understanding of LLMs.\n\nCurrent progress in LVLMs includes the adaptation of LLMs to process visual and auditory content, primarily through the use of pre-trained visual and audio encoders. These models aim to bridge the gap between visual and textual data, allowing LLMs to generate responses grounded in visual and auditory information. Despite these advancements, there are still significant challenges in handling temporal dynamics in videos, integrating auditory content, and maintaining performance across diverse datasets.\n\n### Main Content of Each Paper\n\n#### Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding\nVideo-LLaMA is a multi-modal framework that equips LLMs with the capability to understand both visual and auditory content in videos. It leverages a frozen pre-trained visual encoder and an audio encoder to capture visual and auditory features, respectively. The visual encoder uses a Video Q-former to generate visual query tokens, while the audio encoder uses an Audio Q-former to generate auditory query tokens. Both query tokens are then mapped into the embedding space of the LLM, allowing the model to generate responses grounded in the video content.\n\n**Training Methodology:**\n- **Vision-Language Branch:** Pre-trained on Webvid-2M and CC595k datasets for video-to-text and image-to-text generation tasks. Fine-tuned on visual instruction datasets to enhance comprehension.\n- **Audio-Language Branch:** Trained on audio caption datasets, utilizing ImageBind to align audio features with the textual embedding space of LLMs. Despite not being trained explicitly on audio-text data, Video-LLaMA demonstrates zero-shot audio understanding.\n\n**Innovations:**\n- **Multi-branch Cross-Modal Pre-training:** Allows for the alignment of visual and auditory content with textual inputs.\n- **End-to-end Framework:** Integrates visual and auditory information directly into the LLM's input space, avoiding the need for external perception models.\n\n#### Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models\nVideo-ChatGPT is a multimodal model that merges a video-adapted visual encoder with an LLM to enable detailed video understanding and conversation. It utilizes a dataset of 100,000 video-instruction pairs, which are generated through a combination of human-assisted and semi-automatic annotation methods. The model is fine-tuned using a two-stage process: pre-training on video-caption pairs and fine-tuning on video-instruction pairs.\n\n**Training Methodology:**\n- **Visual Encoder:** Adapted from CLIP ViT-L/14, which generates spatiotemporal features by averaging frame-level features across temporal and spatial dimensions.\n- **Instruction Tuning:** Uses a human instruction dataset to fine-tune the model, focusing on complex visual comprehension tasks.\n\n**Innovations:**\n- **Semi-Automatic Annotation Framework:** Generates a large and diverse dataset of video-instruction pairs, enhancing the model's video-specific understanding.\n- **Quantitative Evaluation Framework:** Provides a benchmark for evaluating video conversation models, assessing capabilities such as correctness, detail orientation, contextual understanding, and temporal understanding.\n\n#### Video-LLaVA: Learning United Visual Representation by Alignment Before Projection\nVideo-LLaVA introduces a novel approach to unify visual representations of images and videos before projection into the LLM's embedding space. This unification is achieved by pre-aligning visual signals using LanguageBind, a universal embedding model. The model is trained on a mixed dataset of images and videos, allowing it to learn from a unified visual representation and generate contextually appropriate responses.\n\n**Training Methodology:**\n- **Unified Visual Representation:** Utilizes LanguageBind to map images and videos into a shared feature space, facilitating joint training on both modalities.\n- **Instruction Tuning:** Employs a dataset of instructional image-text pairs to fine-tune the model, enhancing its ability to generate detailed and contextually rich responses.\n\n**Innovations:**\n- **Alignment Before Projection:** Ensures that visual signals from images and videos are aligned before being projected into the LLM's embedding space, improving the model's multi-modal reasoning capabilities.\n- **Joint Training:** Conducts training on both images and videos simultaneously, demonstrating mutual benefits in visual understanding.\n\n### Commonalities and Innovations\nAll three papers aim to enhance the multi-modal understanding of LLMs by integrating visual and auditory information. Video-LLaMA and Video-LLaVA specifically focus on handling both visual and auditory content, while Video-ChatGPT primarily addresses visual content but with a strong emphasis on temporal dynamics and spatial consistency.\n\n**Commonalities:**\n- **Use of Pre-trained Encoders:** Each model leverages pre-trained visual and audio encoders to extract features from video frames and audio segments.\n- **Instruction Tuning:** All models are fine-tuned using instructional datasets to improve their ability to generate contextually appropriate responses.\n- **Joint Training:** Video-LLaMA and Video-LLaVA both conduct joint training on images and videos, while Video-ChatGPT focuses on video-specific data.\n\n**Innovations:**\n- **Video-LLaMA:** Proposes a multi-branch cross-modal pre-training framework and an end-to-end model that integrates visual and auditory information directly into the LLM's input space.\n- **Video-ChatGPT:** Develops a semi-automatic annotation framework to generate a large and diverse dataset of video-instruction pairs, and introduces a quantitative evaluation framework for video conversation models.\n- **Video-LLaVA:** Introduces a novel approach to unify visual representations of images and videos before projection, ensuring that the LLM can learn from a unified visual representation.\n\n### Comparison of Results\nThe performance of the models is evaluated across various benchmarks, including video question-answering datasets and image question-answering benchmarks. Video-LLaMA, Video-ChatGPT, and Video-LLaVA each demonstrate superior performance compared to existing models in their respective domains.\n\n**Video Question-Answering Benchmarks:**\n- **MSVD-QA:** Video-LLaMA (51.6), Video-ChatGPT (64.9), Video-LLaVA (70.7)\n- **MSRVTT-QA:** Video-LLaMA (29.6), Video-ChatGPT (49.3), Video-LLaVA (59.2)\n- **TGIF-QA:** Video-LLaMA (-), Video-ChatGPT (51.4), Video-LLaVA (70.0)\n- **ActivityNet-QA:** Video-LLaMA (12.4), Video-ChatGPT (35.2), Video-LLaVA (45.3)\n\n**Image Question-Answering Benchmarks:**\n- **VQA-v2:** Video-LLaVA (74.7), InstructBLIP (72.3), MiniGPT-4 (74.6)\n- **GQA:** Video-LLaVA (60.3), InstructBLIP (59.9), MiniGPT-4 (55.4)\n- **VisWiz:** Video-LLaVA (48.1), InstructBLIP (47.8), MiniGPT-4 (42.5)\n- **SQA-I:** Video-LLaVA (66.4), InstructBLIP (67.3), MiniGPT-4 (65.0)\n- **VQA-T:** Video-LLaVA (51.8), InstructBLIP (50.7), MiniGPT-4 (44.2)\n\n**Object Hallucination Evaluation:**\n- **Popular Subset:** Video-LLaVA (85.3), InstructBLIP (81.4), MiniGPT-4 (68.3)\n- **Random Subset:** Video-LLaVA (86.2), InstructBLIP (88.7), MiniGPT-4 (77.8)\n- **Adversarial Subset:** Video-LLaVA (81.6), InstructBLIP (74.4), MiniGPT-4 (66.6)\n\n**Joint Training Evaluation:**\n- **Video Understanding:** Video-LLaVA with joint training (70.7, 59.2, 70.0, 45.3) vs. Video-LLaVA without joint training (64.8, 58.3, 67.8, 40.7)\n- **Image Understanding:** Video-LLaVA with joint training (74.7, 60.3, 48.1, 66.4, 51.8) vs. LLaVA-1.5 with LanguageBind-Image encoder (84.3, 79.8, 85.7, 88.7, 84.8)\n\n### Conclusion\nThe three papers, Video-LLaMA, Video-ChatGPT, and Video-LLaVA, collectively demonstrate significant advancements in the field of LVLMs. Video-LLaMA introduces a multi-branch cross-modal pre-training framework, enabling LLMs to understand both visual and auditory content in videos. Video-ChatGPT focuses on generating a large and diverse dataset of video-instruction pairs and introduces a quantitative evaluation framework for video conversation models. Video-LLaVA proposes a novel approach to unify visual representations of images and videos before projection, enhancing the LLM's multi-modal reasoning capabilities.\n\n**Main Findings:**\n- **Unified Visual Representation:** Video-LLaVA's approach of unifying visual representations before projection significantly improves performance in both image and video understanding tasks.\n- **Temporal Dynamics and Spatial Consistency:** Video-ChatGPT and Video-LLaMA excel in capturing temporal dynamics and spatial consistency in videos, demonstrating superior performance in video question-answering benchmarks.\n- **Zero-shot Audio Understanding:** Video-LLaMA shows remarkable zero-shot audio understanding capabilities, despite not being explicitly trained on audio-text data.\n\n**Future Research Directions:**\n- **Efficient Handling of Long Videos:** Future work should focus on developing more efficient methods to handle long videos, which currently pose a challenge for these models.\n- **Additional Modalities:** Incorporating additional visual-related modalities, such as depth and infrared images, could further enhance the model's understanding and response generation capabilities.\n- **Timestamp Embeddings:** Exploring the effective use of timestamp embeddings could improve the model's ability to answer questions related to temporal relationships in videos.\n- **Reducing Computational Costs:** Reducing the computational costs associated with training these models is crucial for broader adoption and real-world applications.\n\nThese advancements highlight the potential of LVLMs in revolutionizing video understanding and interaction, paving the way for more sophisticated and contextually rich AI assistants."
}