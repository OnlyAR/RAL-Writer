{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nRecently, the remarkable advance of the Large Language Model (LLM) has inspired researchers to transfer its extraordinary reasoning capability to both vision and language data. However, the prevailing approaches primarily regard the visual input as a prompt and focus exclusively on optimizing the text generation process conditioned upon vision content by a frozen LLM. Such an inequitable treatment of vision and language heavily constrains the model's potential. In this paper, we break through this limitation by representing both vision and language in a unified form. Specifically, we introduce a well-designed visual tokenizer to translate the non-linguistic image into a sequence of discrete tokens like a foreign language that LLM can read. The resulting visual tokens encompass high-level semantics worthy of a word and also support dynamic sequence length varying from the image. Coped with this tokenizer, the presented foundation model called \\textbf{LaVIT} can handle both image and text indiscriminately under the same generative learning paradigm. This unification empowers LaVIT to serve as an impressive generalist interface to understand and generate multi-modal content simultaneously. Extensive experiments further showcase that it outperforms the existing models by a large margin on massive vision-language tasks. Our code and models are available at \\url{https://github.com/jy0205/LaVIT}.\n\\end{abstract}\n\n\\section{Introduction}\nThe large language models (LLMs)~\\citep{brown2020language,touvron2023llama} nowadays have demonstrated impressive advances in various linguistic applications. Profiting from the knowledge in the massive text corpus, they possess exceptional understanding capabilities and serve as a general-purpose interface to complete a wide range of real-world tasks. Such success has motivated researchers to investigate the Multi-modal Large Language Models (MLLMs), which aim at extending the powerful pure-text LLMs to process multi-modality inputs. As shown in Figure~\\ref{fig:fig0}-(a), the prevalent approaches mostly leverage an adapter architecture (e.g., the Resampler~\\citep{alayrac2022flamingo}, linear projection~\\citep{liu2023visual}, or Q-Former~\\citep{li2023blip}) to map the visual features encoded by a pre-trained vision backbone~\\citep{radford2021learning} to the semantic space of LLM. \n\nDespite achieving preliminary results in zero-shot multi-modal understanding, they still suffer from inherent design deficiencies. The training objective of prior methodologies~\\citep{li2023blip,huang2023language,zhu2023minigpt} is centered on predicting textual descriptions dependent on visual content, where the visual parts are merely regarded as prompts without any supervision. The inequitable treatment of different modal inputs severely constrains the model's potential, limiting them to only performing comprehension tasks like generating text based on images. Moreover, most of these methods completely delegate the responsibility of vision-language alignment to the newly added adapter with limited trainable parameters, which fails to leverage the remarkable reasoning capabilities of LLM to learn the interaction across different modalities. Although the recent concurrent work Emu~\\citep{sun2023generative} proposes to unlock the text-pretrained LLM by regressing the next visual embedding during pre-training (Figure~\\ref{fig:fig0}-(b)), the inconsistent optimization objectives for image and text are not conducive to unified multi-modal modeling.\n\nIn this work, we introduce \\textbf{LaVIT} (\\textbf{La}nguage-\\textbf{VI}sion \\textbf{T}ransformer), a novel general-purpose multi-modal foundation model that inherits the successful learning paradigm of LLM: predicting the next image/text token in an auto-regressive manner. Our insight is that by employing a unified objective to indiscriminately treat tokens from different modalities, the model can seamlessly achieve ``any-to-any'' multi-modal comprehension and generation. However, the original LLM is specifically crafted to process discrete textual tokens. When dealing with physical signal inputs, such as images, it becomes imperative to embrace a representation seamlessly compatible with text tokens. Therefore, we propose to translate the image into a sequence of tokens like a foreign language that LLM can comprehend, so that both images and texts can be handled simultaneously under the unified generative objective without any specific architectural modification, as shown in Figure~\\ref{fig:fig0}-(c).\n\n\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=0.93\\linewidth]{figure/fig0_v3.pdf}\n\\end{center}\n\\vspace{-0.1in}\n\\caption{\\small The comparisons between different MLLMs. (a) The adapter-style methods rely on an adapter network to project visual features into the semantic space of LLM. During training, visual tokens are merely treated as the prompt to guide text generation. (b) The concurrent work Emu adopts the regression loss for visual features and jointly trains with textual tokens. (c) We craft a visual tokenizer to represent images in the same discrete format as text so as to indiscriminately optimize them under a unified generative objective.}\n\\label{fig:fig0}\n\\vspace{-0.2in}\n\\end{figure}\n\nTo achieve this goal, a crucial element lies in the development of an efficient visual tokenizer for encoding images, which we contend should adhere to the following principles: (i) \\textbf{discrete visual token}: While language models rely on text tokens defined by a dictionary, prior visual tokens, like those derived from ViT, consist of continuous feature vectors encoding a patch. In approaches such as masked image modeling~\\citep{HeCXLDG22} or masked feature prediction~\\citep{00050XWYF22}, regressive objectives on continuous features or raw visual pixels are employed for self-supervised pretraining. Here, we advocate for quantizing the visual tokens into a discrete form, aligning them with the next-token prediction objective in language models. This form is particularly advantageous when the target distribution for the next token is multi-mode. (ii) \\textbf{dynamic token allocation}. Given the varying semantic complexity of different images, employing a fixed length of tokens to encode all images is compute-uneconomical. Moreover, as a key difference from textual tokens, visual patches exhibit a notable interdependence, making it considerably more straightforward to deduce one token from others. This renders the next-token paradigm less effective in learning visual knowledge through self-supervision. Thus we argue for the token-merging to ensure the least redundancy among visual patches, thereby rendering a dynamic token number for different images.\n\nFollowing the aforementioned two crucial fundamentals, LaVIT introduces a novel dynamic visual tokenization mechanism consisting of a selector and a merger to process images. The token selector first decides which visual patches carry informative semantics and are necessary to be selected to encode the whole image. In order to maximally preserve the image details, the token merger further compresses the unselected patches onto the retained ones according to their feature similarity. Such a design enables each retained visual token to contain high-level semantics from multiple similar patches and thus reduce the redundancy among tokens. This selecting and merging strategy will produce a dynamic sequence length varying from the image content itself. The retained visual tokens are further quantized into discrete codes by a learnable codebook~\\citep{esser2021taming}, which will serve as the supervision signals for visual tokens during pre-training. Empowered by this visual tokenizer, our LaVIT can be trained with a simple yet unified objective: predicting the next image/text token in the multi-modal sequence. After pre-training, LaVIT can serve as a multi-modal generalist to perform both multi-modal comprehension and generation without further fine-tuning (See Figure~\\ref{fig:fig1}). The key contributions of this work are summarized as: \n\n\\begin{itemize}[leftmargin=*]\n\\item We introduce LaVIT, a new effective, general-purpose multi-modal foundation model that goes beyond the traditional adapter-based architectures. By transforming images into a sequence of discrete tokens like a foreign language that LLM can comprehend and generate, both modalities can be associated indiscriminately under a unified generative training paradigm. \n\n\\item The developed visual tokenizer can produce discrete visual tokens with dynamic length to reduce the interdependence among visual patches, which enhances the representation compatibility of image and text in LLM and improves computational efficiency. \n\n\\item Our LaVIT showcases the extraordinary multi-modal understanding and generation potential. It can take any modality combinations as input and perform impressive in-context generation of both images and text. As demonstrated by extensive experiments, LaVIT achieves state-of-the-art zero-shot performance on a wide range of vision-language tasks.\n\n\\end{itemize}\n\n\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=0.95\\linewidth]{figure/fig1v3.pdf}\n\\end{center}\n\\vspace{-0.1in}\n\\caption{Given an image-text pair, the image is tokenized into discrete tokens and concatenated with text tokens to form a multi-modal sequence. Then, LaVIT is optimized under a unified generative objective. After training, it can achieve both zero-shot multi-modal comprehension and generation.}\n\\label{fig:fig1}\n\\vspace{-0.15in}\n\\end{figure}\n\n\\vspace{-0.1in}\n\n\\section{Related Work}\n\n\\noindent \\textbf{Vision-Language Pre-training.}\nResearchers have extensively investigated vision-language pretraining (VLP). The pioneer works~\\citep{radford2021learning,jia2021scaling} primarily employ dual-encoder with contrastive objectives to learn the generic cross-modal aligned representations. Recently, the rapid progress of large language models~\\citep{chowdhery2022palm,touvron2023llama} has motivated researchers to delve into the exploration of augmenting LLM towards vision language tasks. The majority of these works adopt an adapter-style network~\\citep{zhang2023llama} that serves as an intermediate bridge connecting the pre-trained vision encoder and frozen language model. For instance, Flamingo~\\citep{alayrac2022flamingo} develops a Perceiver Resampler to generate text-aligned visual representations. Follow-up methods~\\citep{li2023blip,zhu2023minigpt} mainly adopt the Q-Former to project the visual semantics to the LLM's input space. However, visual inputs in these methods~\\citep{huang2023language,alayrac2022flamingo} are only considered as the prompt and not involved in the optimization, which heavily restricts the model potential.\n\n\\noindent \\textbf{Vector Quantization in Computer Vision.}\nVector quantization~\\citep{gray1984vector,nasrabadi1988image} is widely used in image-generative models. The VQ-VAE~\\citep{van2017neural} and DALL-E~\\citep{ramesh2021zero} proposed to convert an image into a set of discrete codes in a learnable discrete latent space by learning to reconstruct the original image pixels. Models like VQGAN~\\citep{esser2021taming} and ViT-VQGAN~\\citep{yu2021vector} leverage adversarial and perceptual objectives to further enhance the image generation quality. The BEIT series of works also adopts the quantized visual codes as the supervision in mask image modeling~\\citep{peng2022beit,wang2023image}. However, most of these methods tokenize the image into a token sequence with a fixed length (e.g., 512 or 1024). Such a long sequence will invariably result in an excessive computational burden. On the contrary, our proposed visual tokenizer reduces the redundancy among image patches and supports dynamic token length, thus enabling efficient multi-modal inference.\n\n\\section{Method}\nThis work proposes to leverage the extraordinary reasoning potential of the large language model to facilitate the modeling of both vision and language modalities. In pursuit of this goal, the key component is to represent these two modalities in a uniform form, so as to exploit LLM's successful learning recipe (i.e., next-token prediction). As shown in Figure~\\ref{fig:fig1}, we develop a visual tokenizer (Section~\\ref{sec:tokenizer}) to convert the non-linguistic image to the input that LLMs can comprehend. It receives the vision features from a pre-trained vision encoder and outputs a sequence of discrete visual tokens possessing word-like high-level semantics. Coped with the crafted tokenizer, the visual input can be integrated with textual tokens to compose a multi-modal sequence, which is subsequently fed into large language models under a unified auto-regressive training objective (Section~\\ref{sec:mllm}). \n\n\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=0.95\\linewidth]{figure/fig2.pdf}\n\\end{center}\n  \\caption{(a) The pipeline of the proposed dynamic visual tokenizer. It employs a token selector to select the most informative patches and a token merger to compress the information of discarded patches onto the retained ones. The whole tokenizer is trained by maximally reconstructing the semantics of the input image. (b) The detailed architecture of token merger.}\n\\label{fig:fig2}\n\\vspace{-0.1in}\n\\end{figure}\n\n\\subsection{Stage-1: Dynamic Visual Tokenizer}\n\\label{sec:tokenizer}\nGiven an image $x \\in \\mathcal{R}^{H\\times W \\times C}$, it is first partitioned into $N=HW/P^2$ non-overlapping patches, where $P$ is the patch size. These patches are fed into a pre-trained ViT encoder~\\citep{fang2023eva} to produce a sequence of the patch features $X=\\{ x_1, ..., x_N \\}$. Then, a straightforward way to encode images is to directly quantize the $N$ patch-level embeddings into discrete tokens as the input to LLMs. This will result in a long visual sequence and bring superfluous computational burden since many visual patches may contain repetitive and trivial background semantics. These redundant patches demonstrate a discernible interdependence, thereby diminishing the efficacy of the next-token paradigm in learning visual knowledge via self-supervision. Consequently, the proposed tokenizer aims to produce visual tokens with a dynamic length according to the complexity of the image content itself. As illustrated in Figure~\\ref{fig:fig2}, it comprises a token selector and a token merger. \n\n\\paragraph{Token Selector} The token selector takes the $N$ patch-level features $X$ as input. It aims to estimate the importance of each image patch and selects the most informative ones that are competent enough to represent the semantics of the whole image. Inspired by~\\citep{rao2021dynamicvit}, it is implemented as a lightweight module consisting of several MLP layers to predict a distribution $\\pi \\in \\mathcal{R}^{N\\times2}$, where $\\pi_{i}=\\text{MLP}(x_i)$. By sampling from the distribution $\\pi$, a binary decision mask $M \\in \\{0, 1\\}^N$ can be generated, which indicates whether to remain the corresponding image patch. To relax the sampling to be differentiable, the Gumbel-Softmax trick~\\citep{maddison2016gumbel} is applied to $\\pi$:\n\\begin{equation}\n    \\hat{\\pi_{i,j}} = \\frac{\\exp ((\\log \\pi_{i,j} + G_{i,j}) / \\tau)}{\\sum_{r=1}^{2} \\exp ((\\log\\pi_{i,r} + G_{i,r}) / \\tau) }.\n\\end{equation}\nwhere $G_{i,j}$ is the noise sampled from a Gumbel distribution, $\\tau$ is the temperature to control smoothness. Then, the binary decision mask $M$ can be sampled from $\\hat{\\pi}$ for end-to-end training. \n\n\\paragraph{Token Merger} According to the generated decision mask, total $N$ image patches can be partitioned into retained and dropped groups, with $T$ and $N-T$ patches respectively, denoted as $X_{r}=\\{x_i\\}_{i=1}^{T}$ and $X_{d}=\\{x_j\\}_{j=1}^{N-T}$. Instead of directly discarding $X_{d}$, we develop a token merger to deal with it to maximally preserve the detailed semantics of the input image. As shown in the right of Figure~\\ref{fig:fig2}, the token merger will progressively compress the information of discarded $X_{d}$ onto the retained $X_{r}$ according to their semantic similarity. Concretely, it consists of $L$ stacked blocks, each of which has a causal self-attention layer, a cross-attention layer, and a feed-forward layer. In the causal self-attention layer, each token in $X_{r}$ attends to its previous tokens with a causal mask. This helps to convert 2D raster-ordered features from the ViT encoder into a sequence with causal dependency, thus ensuring consistency with textual tokens in LLMs. We found this strategy can result in better performance than bi-directional self-attention. The cross-attention layer treats the retained tokens $X_{r}$ as the query and merges tokens in $X_{d}$ based on their similarity in the embedding space. Formally, this layer calculates an update of $X_r$ by:\n\\begin{equation}\n    \\Delta X_r = \\text{softmax}\\left({QK^\\top}/{\\sqrt{D}}\\right)V,\n\\end{equation}\nwhere $D$ denotes the dimension of hidden state, $Q=W_{Q}X_r \\in \\mathcal{R}^{T \\times D}$, $K=W_{K}X_{d} \\in \\mathcal{R}^{(N-T) \\times D}$ and $V=W_{V}X_{d} \\in \\mathcal{R}^{(N-T) \\times D}$. To parallelize the computation, we adopt the predicted decision mask $M$ to control the cross-attention scope between tokens without directly partitioning them into two groups. After $L$ successive token merger blocks, we can obtain the final merged visual tokens $\\hat{X_r}=\\{ \\hat{x_i} \\}_{i=1}^{T}$. Each token implicitly encodes high-level semantics from several image patches possessing similar visual patterns, which we refer to as visual part features $\\hat{X_r}$. \nThe token selector and merger work together to dynamically adjust the visual token sequence length to accommodate images with different content complexity.\n\n\\vspace{-0.1in}\n\n\\paragraph{Vector Quantization and Training} The generated visual part features $\\hat{X_r}$ are then passed into a quantizer. It tokenizes $\\hat{X_r}$ to a sequence of discrete visual codes $V=\\{v_i\\}_{i=1}^T$ by looking up a learnable codebook $\\mathcal{C}=\\{c_k\\}_{k=1}^{K}$, where $K$ is codebook size. To be specific, the $i_{th}$ visual code is calculated by assigning $\\hat{x_i}$ in $\\hat{X_r}$ to its closest neighbourhood code in $\\mathcal{C}$: \n\\begin{equation}\n    v_i = \\arg \\min_{j} \\| l_{2}(\\hat{x_i}) - l_{2}(c_j)\\|_2, \\quad v_i \\in [0, K-1],\n\\end{equation}\nwhere $l_2$ indicates the $L_2$ norm. Based on the indexing visual codes, we can obtain the quantized embeddings $\\{c_{v_i}\\}_{i=1}^T$, which is fed into a decoder to reconstruct the original visual semantic features $X=\\{x_i\\}_{i=1}^{N}$. The insight behind this design is that the reconstruction quality of the image semantics depends on selecting the most informative patches (token selector), along with maximally preserving the visual details only through the remained tokens (token merger). Thus, both token selector and merger can be effectively updated by encouraging a higher semantic reconstruction quality. The final training objective of the visual tokenizer is defined as:\n\\begin{equation}\n    \\mathcal{L}_{\\text{tokenizer}} = \\frac{1}{N} \\sum_{i=1}^{N} \\left( 1 - \\cos (x_i, x_i^{\\text{rec}}) \\right)+ \\lambda (\\rho - \\frac{1}{N} \\sum_{i=1}^{N} M_i)^2,\n\\end{equation}\nwhere $\\cos (x_i, x_i^{\\text{rec}})$ calculates the cosine similarity between the reconstructed and real visual embeddings, $\\rho$ is a pre-defined rate that controls the target mean percentage of the retained visual tokens and $\\lambda$ is set to be $2$. Finally, the tokenized discrete codes $\\{v_i\\}_{i=1}^T$ will serve as the supervision signals for visual tokens in the following pre-training.\n\n\\vspace{-0.1in}\n\n\\paragraph{Decoding to Pixels} The proposed visual tokenizer is capable of reconstructing visual features of input images that contain high-level semantics to represent the image content but lose the pixel-level details. To recover the original pixel space, we employ a conditional de-noising U-Net $\\epsilon_\\theta$~\\citep{rombach2022high} to infill the visual details after training the visual tokenizer. Specifically, it takes the reconstructed $x_{\\text{rec}}$ as the condition to progressively recover image $x$ from a Gaussian noise. Following~\\citet{rombach2022high}, the parameters $\\theta$ of this U-Net are optimized by $\\epsilon$ prediction:\n\\begin{equation}\n    \\mathcal{L}_{\\theta} = \\mathbb{E}_{z, \\epsilon \\sim \\mathcal{N}(0,1), t} \\left[ ||\\epsilon - \\epsilon_{\\theta}(z_t, t, x_{\\text{rec}}) || \\right],\n\\end{equation}\nwhere $z_t$ is the latent state of image $x$ in the diffusion process. We present some pixel decoding results by the trained denoising U-Net in Figure~\\ref{fig:supp_fig_decode} of the appendix. During inference, the generated visual tokens from LaVIT can be decoded into realistic images by this U-Net.\n\n\\subsection{Stage-2: Unified Generative Modeling}\n\\label{sec:mllm}\nGiven an image-text pair, the 2D image can be tokenized into a 1D sequence with causal dependency and then concatenated with text tokens to form a multi-modal sequence $y=(y_1, y_2, .., y_S)$. For differentiating between two modalities, two special tokens [IMG] and [/IMG] are inserted into the beginning and end of the image token sequence respectively, indicating the start and end of image content. To empower LaVIT with the capability to generate both text and images, we employ two different concatenation forms, i.e., $[\\text{image}, \\text{text}]$ and $[\\text{text};\\text{image}]$. When the image is used as a condition (on the left) to generate text, we use the continuous visual features $\\hat{X_r}$ from the token merger instead of quantized visual embeddings as the input to LLMs. Such a design mitigates the loss of detailed information caused by vector quantization, which is crucial for fine-grained multi-modal understanding tasks like visual question answering. Our \\textbf{LaVIT} adopts the general Language Modeling (LM) objective to directly maximize the likelihood of each multi-modal sequence in an auto-regressive manner:\n\\begin{equation}\n    p(y) = \\sum_{y \\in \\mathcal{D}} \\sum_{i=1}^{S} \\log P_\\theta(y_i | y_{< i}).\n\\end{equation}\nSince both image and text are already represented as discrete token IDs, we can use the cross-entropy to supervise the token prediction at each location for both two modalities with a shared prediction head. The complete unification in representation spaces and the training paradigms can help LLMs better learn multi-modal interaction and alignment. When LaVIT is pre-trained, it possesses the capacity to perceive images akin to a foreign language, comprehending and producing them like text. Nevertheless, most of the existing works merely regard images as prompts to guide the generation of text with no supervision, restricting them to solely performing image-to-text tasks.\n\n\\vspace{-0.05in}\n\n\\subsection{Model Pre-training}\nThe LaVIT undergoes a two-stage pre-training procedure on web-scale multi-modal corpora. \n\n\\textbf{Stage-1: Tokenizer Training}. Following the existing MLLMs, the ViT-G/14 of EVA-CLIP~\\citep{fang2023eva} is employed as the visual encoder. The visual codebook size is empirically set to $K=16384$.  We adopt $L=12$ transformer blocks for both token merger and decoder in our tokenizer. During training, this encoder is kept frozen and only the parameters of the selector, merger, and codebook are updated. It is trained for 50K steps on about 100M images from LAION-400M~\\citep{schuhmann2021laion} with the batch size of $2048$ and $\\rho=1/3$. After training the tokenizer, the conditional U-Net for pixel decoding is initialized from the Stable Diffusion v1.5~\\citep{rombach2022high} and finetuned 20k steps on the same dataset. The whole stage-1 training only requires pure image data without corresponding captions.\n\n\\textbf{Stage-2: Unified Vision-Language Pre-training}. \nBased on the trained visual tokenizer, all the images can be tokenized into discrete codes that are amenable to the next token prediction. We utilize the raw 7B version of LLaMA~\\citep{touvron2023llama} as the default LLM. For image-to-text comprehension (i.e., $[\\text{image}, \\text{text}]$), we employ about 93M samples from Conceptual Caption~\\citep{sharma2018conceptual,changpinyo2021conceptual}, SBU~\\citep{ordonez2011im2text}, and BLIP-Capfilt~\\citep{li2022blip}. For the text-to-image synthesis (i.e., $[\\text{text}, \\text{image}]$), an additional 100M image-text pairs from the LAION-Aesthetics (A high-aesthetics image subset of LAION-5B~\\citep{schuhmann2022laion}) are used following Stable Diffusion. Moreover, to reduce catastrophic forgetting of the reasoning capacity in training LLM, we employ the English text corpus from Redpajama~\\citep{together2023redpajama} dataset and mix it with the above image-text pairs to form the multi-modal input sequence. \n\n\\vspace{-0.1in}\n\n\\section{Experiments}\n\\label{sec:exp}\nIn this section, comprehensive experiments are conducted to systematically validate the effectiveness of LaVIT on a wide range of vision-language tasks. Specifically, we mainly evaluate the model's zero-shot multi-modal understanding and generation capacity. \n\n\\begin{table*}[t]\n    \\centering\n    \\resizebox{0.95\\linewidth}{!}{\n    \\begin{tabular}{lcccccc}\n    \\toprule\n    \\multirow{2}{*}{Method}  & \\multicolumn{2}{c}{Image Captioning} & \\multicolumn{4}{c}{Visual Question Answering} \\\\\n    \\cmidrule(lr){2-3} \\cmidrule(lr){4-7} &  Nocaps & Flickr & VQAv2 & OKVQA & GQA & VizWiz \\\\\n    \\midrule\n    Flamingo-3B~\\citep{alayrac2022flamingo}  &  - &   60.6 & 49.2  &  41.2 & - &  28.9  \\\\\n    Flamingo-9B~\\citep{alayrac2022flamingo}   & -  &   61.5 & 51.8  &  44.7 & - &   28.8  \\\\\n    OpenFlamingo-9B~\\citep{awadalla2023openflamingo}   & -  & 59.5  & 52.7  & 37.8 & - & 27.5 \\\\\n    MetaLM~\\citep{hao2022language}   & -  & 43.4  &  41.1 & 11.4 & - & -  \\\\\n    Kosmos-1~\\citep{huang2023language}   & -  & 67.1  &  51.0 & - & - & 29.2 \\\\\n    Kosmos-2~\\citep{peng2023kosmos}   & -  & 80.5  &  51.1 & - & - & - \\\\\n    BLIP-2 (Vicuna-7B)~\\citep{li2023blip} &  107.5 & 74.9  & -  & - & 41.3 & 25.3  \\\\\n    BLIP-2 (Vicuna-13B)~\\citep{li2023blip}  & 103.9  & 71.6  &  - & - & 32.3 &  19.6  \\\\\n    CM3Leon-7B~\\citep{yu2023scaling} &  - & -  &  47.6 & - & - &  37.6 \\\\\n    Emu (LLaMA-13B)~\\citep{sun2023generative}   &  - &  - & 52.0  & 38.2 & - & 34.2 \\\\\n    Ours (LLaMA-7B) &  \\textbf{114.2} &  \\textbf{83.0}  & \\textbf{66.0}  & \\textbf{54.6} & \\textbf{46.8} & \\textbf{38.5}  \\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n    \\caption{Overview of zero-shot evaluation on multi-modal understanding tasks. Compared with previous methods, our LaVIT achieved the best performance on both benchmarks. }\n    \\label{tab:zero_shot}\n    \\vspace{-0.2in}\n\\end{table*}\n\n\\subsection{Zero-Shot Multimodal Understanding}\nWe first quantitatively evaluate the zero-shot multi-modal understanding capacity of LaVIT on Image Captioning (NoCaps~\\citep{agrawal2019nocaps}, Flickr30k~\\citep{plummer2015flickr30k})and Visual Question Answering (VQAv2~\\citep{goyal2017making}, OKVQA~\\citep{marino2019ok}, GQA~\\citep{hudson2019gqa}, VizWiz~\\citep{gurari2018vizwiz}). For visual question answering tasks, we use a simple prompt: ``Question: \\{\\} Answer: \\{\\}''. The widely-used CIDEr score and VQA accuracy are employed as metrics to evaluate captioning and question answering, respectively.\n\\begin{wraptable}{r}{8.5cm}\n\\centering\n\\resizebox{0.95\\linewidth}{!}{\n\\begin{tabular}{lcc}\n    \\toprule  \n    Method & Model Type & FID($\\downarrow$) \\\\\n    \\midrule\n    \\textbf{\\textit{Text2Image Specialist:}}  &  &   \\\\\n    DALL-E~\\citep{ramesh2021zero} & Autoregressive & 28.0\\\\\n    CogView~\\citep{ding2021cogview} & Autoregressive & 27.1 \\\\\n    SD~\\citep{rombach2022high} & Diffusion & 12.6 \\\\\n    GLIDE~\\citep{nichol2021glide} & Diffusion & 12.2 \\\\\n    DALL-E2~\\citep{ramesh2022hierarchical}  & Diffusion & 10.4 \\\\\n    Make-A-Scene~\\citep{gafni2022make} & Autoregressive & 11.8 \\\\\n    MUSE-7.6B~\\citep{chang2023muse}  & Non-Autoregressive & 7.9 \\\\\n    Imagen-3.4B~\\citep{saharia2022photorealistic} & Diffusion & 7.3 \\\\\n    Parti-20B~\\citep{yu2022scaling} & Autoregressive & \\textbf{7.2} \\\\\n    \\midrule\n    \\textbf{\\textit{Multimodal Large Langauge Model:}}  &  &   \\\\\n    GILL (OPT-6.7B)~\\citep{koh2023generating}  & LLM & 12.2 \\\\\n    Emu (LLaMA-13B)~\\citep{sun2023generative} & LLM & 11.7 \\\\\n    CM3Leon-7B~\\citep{yu2023scaling} & LLM & 10.8 \\\\\n    Ours (LLaMA-7B) & LLM & \\textbf{7.4} \\\\ \n    \\bottomrule\n\\end{tabular}\n}\n\\caption{\\small The zero-shot text-to-image generation performance of different models on MS-COCO-30K evaluation benchmark.}\n\\label{wrap-tab:1}\n\\vspace{-0.1in}\n\\end{wraptable}\nThe detailed performance comparisons are shown in Table~\\ref{tab:zero_shot}. As observed, LaVIT surpasses all the existing MLLMs by a large margin on these understanding tasks. For example, it achieves a CIDEr score of 83.0 on the Flickr30k test dataset, compared to 61.5 and 74.9 for the Flamingo-9B and BLIP-2 (Vicuna-7B) under the same scale of model size, respectively. The performance superiority on OKVQA (54.6\\% v.s. 44.7\\% of Flamingo-9B) further showcases the excellent multi-modal understanding capacity of LaVIT, since this benchmark contains questions requiring commonsense knowledge and reasoning about the content of images. It is worth noting that, although the concurrent method Emu~\\citep{sun2023generative} also leverages the LLM to jointly model the vision and language, the direct feature regression objective for visual inputs makes it incompatible with text input. Therefore, despite using more training data (2.6B image-text pairs and 3.8B web-crawled data) and larger LLM (LLaMA 13B), it still achieves inferior performance to ours on all evaluation benchmarks.\n\n\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=0.83\\linewidth]{figure/fig_sample.pdf}\n\\vspace{-0.1in}\n\\end{center}\n  \\caption{The qualitative examples of multi-modal image synthesis.}\n\\label{fig:fig4}\n\\vspace{-0.25in}\n\\end{figure}\n\n\\subsection{Zero-Shot Multimodal Generation}\nSince the proposed visual tokenizer can represent images as discrete tokens, LaVIT possesses the capability to synthesize images by auto-regressively generating visual tokens like text. We first quantitatively evaluate the model's zero-shot text-conditional image synthesis performance on the validation set of the MS-COCO benchmark~\\citep{lin2014microsoft}. The detailed image generation procedure is presented in Appendix~\\ref{sec:supp_imagen}. Following the standard setting like previous text-to-image synthesis works, we randomly sample 30k text prompts and calculate the zero-shot FID metric between real images and generated ones. The detailed comparative results are shown in Table~\\ref{wrap-tab:1}. It can be seen that LaVIT outperforms all the other multi-modal language models. Compared with the concurrent work Emu, it makes a 4.3 FID improvement with a smaller LLM model, demonstrating excellent vision-language alignment capability. In addition, LaVIT achieves comparable performance with state-of-the-art text2image specialists Parti~\\citep{yu2022scaling}, while only using much fewer training data (e.g., 0.2B v.s. 2B training image-text pairs compared to Parti). \n\n\\vspace{-0.1in}\n\n\\paragraph{Generation via Multi-modal Prompt} LaVIT can seamlessly accept several modality combinations (\\emph{e.g.}, text, image+text, image+image) as prompts to generate corresponding images without any fine-tuning. Figure~\\ref{fig:fig4} showcases some examples of the multi-modal image generation results. Our LaVIT can produce high-quality images that precisely reflect the style and semantics of the given multi-modal prompts, which demonstrates the strong multi-modal modeling potential of LaVIT. More interestingly, it can modify the original input image by the input multi-modal prompt (\\emph{e.g.}, in the last example two prompt images with a dog or cat generate a dog's portrait with the cat's whisker). This capability cannot be attained by conventional image generation models like Stable Diffusion in the absence of additional fine-tuned downstream data~\\citep{ruiz2023dreambooth}. \n\n\\subsection{Ablation Study}\nIn this study, we investigate the impact of various component designs in LaVIT on downstream performance. All the ablations were conducted on part of pre-training data by using the clip ViT-L/14~\\citep{jia2021scaling} as the visual encoder due to the costly training resources.\n\n\\textbf{Token Classification or Feature Regression?} \nWhen jointly training vision and language via generative training in text-oriented LLM, it is crucial to select the appropriate optimization objectives for the 2D raster-ordered visual input. When quantizing the continuous visual tokens into the discrete form, it is convenient to use the cross-entropy loss for supervising the next visual token prediction akin to textual tokens. We conjecture that such a uniform objective for both vision and language contributes to aligning them together in the LLM. To validate the superiority of the proposed visual quantization, we change the optimization objective of visual tokens to regressing the next visual embeddings by employing a regression head like Emu~\\citep{sun2023generative}. Table~\\ref{tab:ab1} summarizes the results of different training objectives. As observed, adopting the regression loss for the next visual token prediction will severely degrade the model performance.\n\n\\textbf{Dynamic or Fixed Token Length}. Given the extracted visual features, a straightforward way is to tokenize all the patch embeddings into the visual tokens, which results in a fixed token length (i.e., 256). We compare the impact of fixed and dynamic tokenization strategies in terms of training time, computation overhead, and zero-shot performance on multi-modal understanding. As shown in Table~\\ref{tab:ab2}, the dynamic visual tokenizer achieves superior performance while only requiring 94 tokens on average for the input images, about 36\\% of the fixed one. Given that the attention computation in LLM exhibits a quadratic relationship with respect to the token length, this sparsification can accelerate the training time by 40\\% and reduce the computational cost in inference.\n\n\\begin{figure}[t]\n\\begin{center}\n\\includegraphics[width=0.85\\linewidth]{figure/fig3.pdf}\n\\vspace{-0.1in}\n\\end{center}\n  \\caption{\\small The visualization for the dynamic visual tokenizer (left) and learned codebook (right). Our tokenizer can dynamically select the most informative patches based on the image content and the learned codebook can produce visual codes with high-level semantics.}\n\\label{fig:fig3}\n\\vspace{-0.2in}\n\\end{figure}\n\n\\begin{table}[h]\n    \\begin{subtable}[h]{0.4\\textwidth}\n        \\centering\n        \\scalebox{0.85}{\n        \\begin{tabular}{c|ccc}\n        \\toprule\n         Setting & Flickr & VQAv2 &  OKVQA \\\\\n        \\midrule\n           Regression  & 60.4    & 53.6 &   41.9 \\\\\n           Classification & \\textbf{73.2}   & \\textbf{57.1}  & \\textbf{47.0}  \\\\\n        \\bottomrule\n        \\end{tabular}}\n       \\caption{Ablations of different training objectives.}\n       \\label{tab:ab1}\n    \\end{subtable}\n    \\hfill\n    \\begin{subtable}[h]{0.6\\textwidth}\n        \\centering\n        \\scalebox{0.85}{\n        \\begin{tabular}{c|cc|ccc}\n        \\toprule\n         Setting & Num & Time  & Flickr &   VQAv2 & OKVQA  \\\\\n        \\midrule\n         Fixed  & 256 & 30h &  71.1  & 56.5 & 46.4  \\\\\n         Dynamic  & 94 & 18h &  \\textbf{74.0}  & \\textbf{57.7} & \\textbf{47.6}  \\\\\n        \\bottomrule\n        \\end{tabular}}\n        \\caption{\\small Ablations for the effect of different tokenization strategies.}\n        \\label{tab:ab2}\n     \\end{subtable}\n     \\caption{The ablations of different optimization objectives for visual tokens and tokenization strategies. The num and time in Table~\\ref{tab:ab2} indicate the mean visual token number and pre-training time.}\n     \\label{tab:ablation1}\n\\vspace{-0.1in}\n\\end{table}\n\n\\subsection{Qualitative Analysis}\nWe visualize some examples processed by the proposed dynamic tokenizer. As shown in Figure~\\ref{fig:fig3}, the token selector is capable of dynamically selecting the most informative image patches that are competent enough to represent the semantics of the whole image. Visual patches that contain repetitive or trivial background semantics are filtered during this procedure, thereby reducing redundant information and improving computing efficiency. We also visualize the image patches that belong to the same visual code in Figure~\\ref{fig:fig3}. As observed, the learned discrete codes can convey explicit visual semantics and group the image patches with similar patterns together. For instance, code $4107$ represents a part of a skateboard, and code $9146$ indicates the texture of a giraffe, which strongly demonstrates the interpretability of the learned codebook. \n\n\\section{Conclusion}\nThis paper presents the LaVIT, a new general-purpose foundation model that is capable of simultaneously performing multi-modal understanding and generation. Beyond the previous adapter-based methods, it inherits the successful auto-regressive generative learning paradigm of LLMs by representing both vision and language in a unified discrete token representation via a dynamic visual tokenizer. Through optimization under the unified generative objective, LaVIT can treat images as a foreign language, comprehending and generating them like text. Extensive experimental results further demonstrate the LaVIT's superior capability to serve as a multi-modal generalist. \n\n\\textbf{Acknowledgement}: This research work is supported by National Key R\\&D Program of China (2022ZD0160305).\n\n\\clearpage\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Visual Autoregressive Modeling:\\, Scalable Image Generation via Next-Scale Prediction}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{figure}[ht]\n\\vspace{-16pt}\n\\begin{center}\n\t\\includegraphics[width=0.97\\linewidth]{fig/abs.jpg}\n\\end{center}\n\\vspace{-7pt}\n\\caption{\\small\n\\textbf{Generated samples from Visual AutoRegressive (VAR) transformers trained on ImageNet}.\nWe show 512$\\times$512 samples (top), 256$\\times$256 samples (middle), and zero-shot image editing results (bottom).\n}\n\\vspace{-2pt}\n\\label{fig:abs}\n\\end{figure}\n\n\\begin{abstract}\n\\vspace{-4pt}\n\nWe present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine ``next-scale prediction'' or ``next-resolution prediction'', diverging from the standard raster-scan ``next-token prediction''.\nThis simple, intuitive methodology allows autoregressive (AR) transformers to learn visual distributions fast and can generalize well: VAR, for the \\textit{first time}, makes GPT-style AR models surpass diffusion transformers in image generation.\nOn ImageNet 256$\\times$256 benchmark, VAR significantly improve AR baseline by improving Fréchet inception distance (FID) from 18.65 to 1.73, inception score (IS) from 80.4 to 350.2, with 20$\\times$ faster inference speed.\nIt is also empirically verified that VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions including image quality, inference speed, data efficiency, and scalability.\nScaling up VAR models exhibits clear power-law scaling laws similar to those observed in LLMs, with linear correlation coefficients near $-0.998$ as solid evidence.\nVAR further showcases zero-shot generalization ability in downstream tasks including image in-painting, out-painting, and editing.\nThese results suggest VAR has initially emulated the two important properties of LLMs: \\textbf{Scaling Laws} and \\textbf{zero-shot} generalization.\nWe have released all models and codes to promote the exploration of AR/VAR models for visual generation and unified learning.\n\n\\end{abstract}\n\n\\begin{figure}[ht]\n\\begin{center}\n\\includegraphics[width=\\linewidth]{fig/intro.pdf}\n\\end{center}\n\\vspace{-9pt}\n\\caption{\\small\n\\textbf{Standard autoregressive modeling (AR) \\textit{vs.} our proposed visual autoregressive modeling (VAR).}\n(a) AR applied to language: sequential text token generation from left to right, word by word;\n(b) AR applied to images: sequential visual token generation in a raster-scan order, from left to right, top to bottom;\n(c) VAR for images: multi-scale token maps are autoregressively generated from coarse to fine scales (lower to higher resolutions), with parallel token generation within each scale. VAR requires a multi-scale VQVAE to work.\n\\vspace{-12pt}\n}\n\\label{fig:intro}\n\\end{figure}\n\n\\vspace{-2pt}\n\\section{Introduction} \\label{sec:intro}\n\\vspace{-4pt}\nThe advent of GPT series~\\cite{gpt1,gpt2,gpt3,gpt3.5,gpt4} and more autoregressive (AR) large language models (LLMs)~\\cite{palm,palm2,chinchilla,llama1,llama2,bloom,ernie3,qwen,team2023gemini} has heralded a new epoch in the field of artificial intelligence.\nThese models exhibit promising intelligence in generality and versatility that, despite issues like hallucinations~\\cite{hallucination}, are still considered to take a solid step toward the general artificial intelligence (AGI).\nAt the core of these models is a self-supervised learning strategy -- \\textit{predicting the next token} in a sequence, a simple yet profound approach.\nStudies into \\textbf{the success of these large AR models} have highlighted their \\textbf{scalability and generalizabilty}:\nthe former, as exemplified by \\textit{scaling laws}~\\cite{scalinglaw,scalingar}, allows us to predict large model's performance from smaller ones and thus guides better resource allocation, while the latter, as evidenced by \\textit{zero-shot and few-shot} learning~\\cite{gpt2,gpt3}, underscores the unsupervised-trained models' adaptability to diverse, unseen tasks. These properties reveal AR models' potential in learning from vast unlabeled data, encapsulating the  essence of ``AGI''.\n\nIn parallel, the field of computer vision has been striving to develop large autoregressive or world models~\\cite{unified-io,lu2023unifiedio2,lvm}, aiming to emulate their impressive scalability and generalizability.\nTrailblazing efforts like VQGAN and DALL-E~\\cite{vqgan,dalle1} along with their successors~\\cite{vqvae2,vit-vqgan,rq,movq} have showcased the potential of AR models in image generation. These models utilize a visual tokenizer to discretize continuous images into grids of 2D tokens, which are then flattened to a 1D sequence for AR learning (\\figref{fig:intro}\\,b), mirroring the process of sequential language modeling (\\figref{fig:intro}\\,a).\nHowever, the scaling laws of these models remain underexplored, and more frustratingly, their performance \\textbf{significantly lags} behind diffusion models \\cite{dit,dit-github,rcg}, as shown in \\figref{fig:cmp}.\nIn contrast to the remarkable achievements of LLMs, the power of autoregressive models in computer vision appears to be somewhat \\textbf{locked}.\n\n\\begin{wrapfigure}[18]{r}{0.49\\textwidth}\n\\centering\n\\vspace{-8pt}\n\\includegraphics[width=0.49\\textwidth]{fig/cmp.pdf}\n\\vspace{-12pt}\n\\caption{\\small\n\\textbf{Scaling behavior} of different model families on ImageNet 256$\\times$256 generation benchmark.\nThe FID of the validation set serves as a reference lower bound (1.78).\nVAR with 2B parameters reaches an FID of 1.73, surpassing L-DiT with 3B or 7B parameters.\n}\n\\label{fig:cmp}\n\\end{wrapfigure}\n\nAutoregressive modeling requires defining the order of data.\nOur work reconsiders how to ``order'' an image:\nHumans typically perceive or create images in a hierachical manner, first capturing the global structure and then local details.\nThis \\textbf{multi-scale, coarse-to-fine} nature suggests an ``order'' for images.\nAlso inspired by the widespread multi-scale designs~\\cite{sift,fpn,spark,pggan}, we define autoregressive learning for images as ``next-scale prediction'' in \\figref{fig:intro} (c), diverging from the conventional ``next-token prediction'' in \\figref{fig:intro} (b).\nOur approach begins by encoding an image into multi-scale token maps. The autoregressive process is then started from the 1$\\times$1 token map, and progressively expands in resolution: at each step, the transformer predicts the next higher-resolution token map conditioned on all previous ones.\nWe refer to this methodology as Visual AutoRegressive (VAR) modeling.\n\nVAR directly leverages GPT-2-like transformer architecture~\\cite{gpt2} for visual autoregressive learning.\nOn the ImageNet 256$\\times$256 benchmark, VAR significantly improves its AR baseline, achieving a Fréchet inception distance (FID) of 1.73 and an inception score (IS) of 350.2, with inference speed 20$\\times$ faster (see \\secref{sec:abla} for details).\nNotably, VAR surpasses the Diffusion Transformer (DiT) -- the foundation of leading diffusion systems like Stable Diffusion 3.0 and SORA~\\cite{stable-diffusion3,sora} -- in FID/IS, data efficiency, inference speed, and scalability.\nVAR models also exhibit scaling laws akin to those witnessed in LLMs.\nLastly, we showcase VAR's zero-shot generalization capabilities in tasks like image in-painting, out-painting, and editing.\nIn summary, our contributions to the community include:\n\n\\begin{enumerate}[topsep=3.5pt,itemsep=3pt,leftmargin=20pt]\n\\item A new visual generative framework using a multi-scale autoregressive paradigm with next-scale prediction, offering new insights in autoregressive algorithm design for computer vision.\n\\item An empirical validation of VAR models' Scaling Laws and zero-shot generalization potential, which initially emulates the appealing properties of large language models (LLMs).\n\\item A breakthrough in visual autoregressive model performance, making GPT-style autoregressive methods surpass strong diffusion models in image synthesis \\textit{for the first time}\\footnote{\\scalebox{0.95}{A related work \\cite{magvit2} named ``language model beats diffusion'' belongs to BERT-style masked-prediction model.}}.\n\\item A comprehensive open-source code suite, including both VQ tokenizer and autoregressive model training pipelines, to help propel the advancement of visual autoregressive learning.\n\\end{enumerate}\n\n\\vspace{-2pt}\n\\section{Related Work} \\label{sec:related}\n\\vspace{-2pt}\n\\vspace{-2pt}\n\\subsection{Properties of large autoregressive language models}\n\\vspace{-2pt}\n\n\\firstpara{Scaling laws} are found and studied in \\textit{autoregressive} language models~\\cite{scalinglaw,scalingar}, which describe a power-law relationship between the scale of model (or dataset, computation, \\textit{etc.}) and the cross-entropy loss value on the test set.\nScaling laws allow us to directly predict the performance of a larger model from smaller ones \\cite{gpt4}, thus guiding better resource allocation.\nMore pleasingly, they show that the performance of LLMs can scale well with the growth of model, data, and computation and never saturate, which is considered a key factor in the success of ~\\cite{gpt3, llama1, llama2, opt, bloom, chinchilla}.\nThe success brought by scaling laws has inspired the vision community to explore more similar methods for multimodality understanding and generation~\\cite{llava,alayrac2022flamingo,visionllm,dong2023dreamllm,cm3leon_chameleon,emu_baai,chen2023internvl,dai2023emu_meta,jin2023unified,ge2023seed_llama,ge2024seedx,tian2024mminterleaved,wang2024git}.\n\n\\para{Zero-shot generalization.} Zero-shot generalization~\\cite{multitask_zeroshot} refers to the ability of a model, particularly a Large Language Model, to perform tasks that it has not been explicitly trained on.\nWithin the realm of the computer vision, there is a burgeoning interest in the zero-shot and in-context learning abilities of foundation models, CLIP~\\cite{clip}, SAM~\\cite{sam}, Dinov2~\\cite{dinov2}. Innovations like Painter~\\cite{painter} and LVM~\\cite{lvm} extend visual prompters~\\cite{visualprompttuning1,visualprompttuning2} to achieve in-context learning in vision.\n\n\\vspace{-3pt}\n\\subsection{Visual generation}\n\\vspace{-3pt}\n\\firstpara{Raster-scan autoregressive models} for visual generation necessitate the encoding of 2D images into 1D token sequences.\nEarly endeavors~\\cite{igpt,van2016pixelcnn} have shown the ability to generate RGB (or grouped) pixels in the standard row-by-row, raster-scan manner.\n\\cite{reed2017mspixelcnn} extends \\cite{van2016pixelcnn} by using multiple independent trainable networks to do super-resolution repeatedly.\nVQGAN~\\cite{vqgan} advances \\cite{igpt,van2016pixelcnn} by doing autoregressive learning in the latent space of VQVAE~\\cite{vqvae}.\nIt employs GPT-2 decoder-only transformer to generate tokens in the raster-scan order, like how ViT~\\cite{vit} serializes 2D images into 1D patches.\nVQVAE-2~\\cite{vqvae2} and RQ-Transformer~\\cite{rq} also follow this raster-scan manner but use extra scales or stacked codes.\nParti~\\cite{parti}, based on the architecture of ViT-VQGAN~\\cite{vit-vqgan}, scales the transformer to 20B parameters and works well in text-to-image synthesis.\n\n\\para{Masked-prediction model.} MaskGIT~\\cite{maskgit} employs a VQ autoencoder and a masked prediction transformer similar to BERT~\\cite{bert, beit, mae} to generate VQ tokens through a greedy algorithm.\nMagViT~\\cite{magvit} adapts this approach to videos, and MagViT-2~\\cite{magvit2} enhances~\\cite{maskgit,magvit} by introducing an improved VQVAE for both images and videos.\nMUSE~\\cite{muse} further scales MaskGIT to 3B parameters.\n\n\\para{Diffusion models}' progress has centered around improved learning or sampling~\\cite{scorebased,ddim, dpm-solver,dpmpp,bao2022analytic}, guidance~\\cite{cfg, glide}, latent learning~\\cite{ldm}, and architectures~\\cite{cdm, dit, imagen, raphael}.\nDiT and U-ViT~\\cite{dit,bao2023all} replaces or integrates the U-Net with transformer, and inspires recent image~\\cite{chen2023pixart,chen2024pixart_sigma} or video synthesis systems~\\cite{bar2024lumiere,gupta2023photorealistic} including Stable Diffusion 3.0~\\cite{stable-diffusion3}, SORA~\\cite{sora}, and Vidu~\\cite{bao2024vidu}.\n\n\\section{Method} \\label{sec:method}\n\n\\vspace{-3pt}\n\\subsection{Preliminary: autoregressive modeling via next-token prediction} \\label{sec:ar}\n\\vspace{-2pt}\n\n\\firstpara{Formulation.}\nConsider a sequence of discrete tokens $x = (x_1, x_2, \\dots, x_T)$, where $x_t \\in [V]$ is an integer from a vocabulary of size $V$.\nThe next-token autoregressive posits the probability of observing the current token $x_t$ depends only on its prefix $(x_1, x_2, \\dots, x_{t-1})$.\nThis \\textbf{unidirectional token dependency assumption} allows for the factorization of the sequence $x$'s likelihood:\n\\begin{align}\n    p(x_1, x_2, \\dots, x_T) = \\prod_{t=1}^{T} p(x_t \\mid x_1, x_2, \\dots, x_{t-1}). \\label{eq:ar}\n\\end{align}\nTraining an autoregressive model $p_\\theta$ involves optimizing $p_\\theta(x_t \\mid x_1, x_2, \\dots, x_{t-1})$ over a dataset.\nThis is known as the ``next-token prediction'', and the trained $p_\\theta$ can generate new sequences.\n\n\\para{Tokenization.}\nImages are inherently 2D continuous signals.\nTo apply autoregressive modeling to images via next-token prediction, we must:\\, 1) tokenize an image into several \\textit{discrete} tokens, and\\, 2) define a 1D \\textit{order} of tokens for unidirectional modeling.\\, For 1), a quantized autoencoder such as~\\cite{vqgan} is often used to convert the image feature map $f \\in \\mathbb{R}^{h\\times w\\times C}$ to discrete tokens $q \\in [V]^{h\\times w}$:\n\\begin{align}\n    f = \\mathcal{E}(im), \\quad~~\n    q = \\mathcal{Q}(f),\n\\end{align}\nwhere $im$ denotes the raw image, $\\mathcal{E}(\\cdot)$ a encoder, and $\\mathcal{Q}(\\cdot)$ a quantizer.\nThe quantizer typically includes a learnable codebook $Z \\in \\mathbb{R}^{V\\times C}$ containing $V$ vectors.\nThe quantization process $q = \\mathcal{Q}(f)$ will map each feature vector $f^{(i,j)}$ to the code index $q^{(i,j)}$ of its nearest code in the Euclidean sense:\n\\begin{align}\n    q^{(i,j)} = \\left( \\argmin_{v \\in [V]} \\| \\text{lookup}(Z, v) - f^{(i,j)} \\|_2 \\right) \\in [V],\n\\end{align}\nwhere $\\text{lookup}(Z, v)$ means taking the $v$-th vector in codebook $Z$.\nTo train the quantized autoencoder, $Z$ is looked up by every $q^{(i,j)}$ to get $\\hat{f}$, the approximation of original $f$. Then a new image $\\hat{im}$ is reconstructed using the decoder $\\mathcal{D}(\\cdot)$ given $\\hat{f}$, and a compound loss $\\mathcal{L}$ is minimized:\n\\begin{align}\n    \\hat{f} &= \\text{lookup}(Z, q), % ~~ (\\forall 1 \\le i \\le h, 1 \\le j \\le w), \\\\\n    \\quad\\quad\\quad~~~ \\hat{im} = \\mathcal{D}(\\hat{f}), \\\\\n    \\mathcal{L} &= \\|im - \\hat{im}\\|_2 + \\|f - \\hat{f}\\|_2 + \\lambda_{\\text{P}} \\mathcal{L}_{\\text{P}}(\\hat{im}) + \\lambda_{\\text{G}} \\mathcal{L}_{\\text{G}}(\\hat{im}), \\label{eq:vaeloss}\n\\end{align}\nwhere $\\mathcal{L}_{\\text{P}}(\\cdot)$ is a perceptual loss such as LPIPS~\\cite{lpips}, $\\mathcal{L}_{\\text{G}}(\\cdot)$ a discriminative loss like StyleGAN's discriminator loss~\\cite{stylegan}, and $\\lambda_{\\text{P}}$, $\\lambda_{\\text{G}}$ are loss weights.\nOnce the autoencoder $\\{\\mathcal{E}, \\mathcal{Q}, \\mathcal{D}\\}$ is fully trained, it will be used to tokenize images for subsequent training of a unidirectional autoregressive model. \n\n\\vspace{2pt}\nThe image tokens in $q \\in [V]^{h\\times w}$ are arranged in a 2D grid.\nUnlike natural language sentences with an inherent left-to-right ordering, the order of image tokens must be explicitly defined for unidirectional autoregressive learning.\nPrevious AR methods~\\cite{vqgan,vit-vqgan,rq} flatten the 2D grid of $q$ into a 1D sequence $x = (x_1, \\dots, x_{h\\times w})$ using some strategy such as row-major raster scan, spiral, or z-curve order.\nOnce flattened, they can extract a set of sequences $x$ from the dataset, and then train an autoregressive model to maximize the likelihood in \\eqref{eq:ar} via next-token prediction.\n\n\\para{Discussion on the weakness of vanilla autoregressive models.}\nThe above approach of tokenizing and flattening enable next-token autoregressive learning on images, but introduces several issues:\n\n\\begin{enumerate}[label=\\arabic*),topsep=1pt,itemsep=2pt,leftmargin=20pt]\n\\item \\textbf{Mathematical premise violation.}\\, In quantized autoencoders (VQVAEs), the encoder typically produces an image feature map $f$ with inter-dependent feature vectors $f^{(i,j)}$ for all $i,j$.\nSo after quantization and flattening, the token sequence $(x_1, x_2, \\dots, x_{h\\times w})$ retains \\textit{bidirectional} correlations.\nThis contradicts the \\textit{unidirectional} dependency assumption of autoregressive models, which dictates that each token $x_t$ should only depend on its prefix $(x_1, x_2, \\dots, x_{t-1})$.\n\n\\item \\textbf{Inability to perform some zero-shot generalization.}\\, Similar to issue 1), The unidirectional nature of image autoregressive modeling restricts their generalizability in tasks requiring bidirectional reasoning. E.g., it cannot predict the top part of an image given the bottom part.\n\n\\item \\textbf{Structural degradation.}\\, The flattening disrupts the spatial locality inherent in image feature maps. For example, the token $q^{(i,j)}$ and its 4 immediate neighbors $q^{(i\\pm 1,j)}$, $q^{(i,j\\pm 1)}$ are closely correlated due to their proximity.\nThis spatial relationship is compromised in the linear sequence $x$, where \\textit{uni}directional constraints diminish these \\textit{cor}relations.\n\n\\item \\textbf{Inefficiency.}\\, Generating an image token sequence $x = (x_1, x_2, \\dots, x_{n\\times n})$ with a conventional self-attention transformer incurs $\\mathcal{O}(n^2)$ autoregressive steps and $\\mathcal{O}(n^6)$ computational cost.\n\\end{enumerate}\n\nIssues 2) and 3) are evident (see examples above).\nRegarding issue 1), we present empirical evidence in \\appref{app:dependency}.\nThe proof of issue 3) is detailed in \\appref{app:complexity}.\nThese theoretical and practical limitations call for a rethinking of autoregressive models in the context of image generation.\n\n\\vspace{8pt}\n\\begin{figure}[htb]\n\\begin{center}\n    \\includegraphics[width=\\linewidth]{fig/method_new.pdf}\n\\end{center}\n\\vspace{-4pt}\n\\caption{\\small\n\\textbf{VAR involves two separated training stages.}\n\\textbf{Stage 1:} a multi-scale VQ autoencoder encodes an image into $K$ token maps $R=(r_1, r_2, \\dots, r_K)$ and is trained by a compound loss \\eqref{eq:vaeloss}.\nFor details on ``Multi-scale quantization'' and ``Embedding'', check \\algref{alg:enc} and \\ref{alg:dec}.\n\\,\\textbf{Stage 2:} a VAR transformer is trained via next-scale prediction \\eqref{eq:var}: it takes $(\\texttt{[s]}, r_1, r_2, \\dots, r_{K-1})$ as input to predict $(r_1, r_2, r_3, \\dots, r_K)$. The attention mask is used in training to ensure each $r_k$ can only attend to $r_{\\le k}$. Standard cross-entropy loss is used.\n\\vspace{-4pt}\n}\n\\label{fig:method}\n\\end{figure}\n\n\\subsection{Visual autoregressive modeling via next-scale prediction} \\label{sec:var}\n\\vspace{-2pt}\n\n\\firstpara{Reformulation.}\nWe reconceptualize the autoregressive modeling on images by shifting from ``next-token prediction'' to ``next-scale prediction'' strategy.\nHere, the autoregressive unit is \\textit{an entire token map}, rather than \\textit{a single token}.\nWe start by quantizing a feature map $f \\in \\mathbb{R}^{h\\times w\\times C}$ into $K$ multi-scale token maps $(r_1, r_2, \\dots, r_K)$, each at a increasingly  higher resolution $h_k\\times w_k$, culminating in $r_K$ matches the original feature map's resolution $h\\times w$.\nThe autoregressive likelihood is formulated as:\n\\begin{align}\n    p(r_1, r_2, \\dots, r_K) = \\prod_{k=1}^{K} p(r_k \\mid r_1, r_2, \\dots, r_{k-1}),  \\label{eq:var}\n\\end{align}\nwhere each autoregressive unit $r_k \\in [V]^{h_k \\times w_k}$ is the token map at scale $k$ containing $h_k \\times w_k$ tokens, and the sequence $(r_1, r_2, \\dots, r_{k-1})$ serves as the the ``prefix'' for $r_k$.\nDuring the $k$-th autoregressive step, all distributions over the $h_k \\times w_k$ tokens in $r_k$ will be generated in parallel, conditioned on $r_k$'s prefix and associated $k$-th position embedding map.\nThis ``next-scale prediction'' methodology is what we define as visual autoregressive modeling (VAR), depicted on the right side of \\figref{fig:method}.\nNote that in the training of VAR, a block-wise causal attention mask is used to ensure that each $r_k$ can only attend to its prefix $r_{\\le k}$.\nDuring inference, kv-caching can be used and no mask is needed.\n\n\\para{Discussion.}\nVAR addresses the previously mentioned three issues as follows:\n\n\\begin{enumerate}[label=\\arabic*),topsep=0.7pt,itemsep=2pt,leftmargin=20pt]\n\\item The mathematical premise is satisfied if we constrain each $r_k$ to depend only on its prefix, that is, the process of getting $r_k$ is solely related to $r_{\\le k}$.\nThis constraint is acceptable as it aligns with the natural, coarse-to-fine progression characteristics like human visual perception and artistic drawing (as we discussed in \\secref{sec:intro}).\nFurther details are provided in the \\textit{Tokenization} below.\n\n\\item The spatial locality is preserved as (i) there is no flattening operation in VAR, and (ii) tokens in each $r_k$ are fully correlated. The multi-scale design additionally reinforces the spatial structure.\n\n\\item The complexity for generating an image with $n\\times n$ latent is significantly reduced to $\\mathcal{O}(n^4)$, see Appendix for proof.\nThis efficiency gain arises from the \\textit{parallel} token generation in each $r_k$.\n\\end{enumerate}\n\n\\para{Tokenization.}\nWe develope a new multi-scale quantization autoencoder to encode an image to $K$ multi-scale discrete token maps $R=(r_1, r_2, \\dots, r_K)$ necessary for VAR learning \\eqref{eq:var}.\nWe employ the same architecture as VQGAN~\\cite{vqgan} but with a modified multi-scale quantization layer.\nThe encoding and decoding procedures with residual design on $f$ or $\\hat{f}$ are detailed in algorithms \\ref{alg:enc} and \\ref{alg:dec}.\nWe empirically find this residual-style design, akin to~\\cite{rq}, can perform better than independent interpolation.\n\\Algref{alg:enc} shows that each $r_k$ would depend only on its prefix $(r_1, r_2, \\dots, r_{k-1})$.\nNote that a shared codebook $Z$ is utilized across all scales, ensuring that each $r_k$'s tokens belong to the same vocabulary $[V]$.\nTo address the information loss in upscaling $z_k$ to $h_K\\times w_K$, we use $K$ extra convolution layers $\\{\\phi_k\\}_{k=1}^K$.\nNo convolution is used after downsampling $f$ to $h_k\\times w_k$.\n\n\\begin{center}\n\\begin{minipage}[t]{0.5\\linewidth}\n  \\centering\n  \\scalebox{0.86}\n  {\n  \\begin{algorithm}[H]\n    \\caption{\\small{~Multi-scale VQVAE Encoding}} \\label{alg:enc}\n    \\small{\n    \\textbf{Inputs: } raw image $im$\\;\n    \\textbf{Hyperparameters: } steps $K$, resolutions $(h_k,w_k)_{k=1}^{K}$\\;\n    $f = \\mathcal{E}(im)$, $R=[]$\\;\n    \\For {$k=1,\\cdots,K$}\n    {\n    $r_k = \\mathcal{Q}(\\text{interpolate}(f, h_k, w_k))$\\;\n    $R = \\text{queue\\_push}(R, r_k)$\\;\n    $z_k = \\text{lookup}(Z, r_k)$\\;\n    $z_k = \\text{interpolate}(z_k, h_K, w_K)$\\;\n    $f = f - \\phi_k(z_k)$\\;\n    }\n    \\textbf{Return: } multi-scale tokens $R$\\;\n    }\n  \\end{algorithm}\n  }\n\\end{minipage}%\n\\begin{minipage}[t]{0.5\\linewidth}\n  \\centering\n  \\scalebox{0.81}\n  {\n  \\begin{algorithm}[H]\n    \\caption{\\small{~Multi-scale VQVAE Reconstruction}} \\label{alg:dec}\n    \\small{\n    \\textbf{Inputs: } multi-scale token maps $R$\\;\n    \\textbf{Hyperparameters: } steps $K$, resolutions $(h_k,w_k)_{k=1}^{K}$\\;\n    $\\hat{f} = 0 $\\;\n    \\For {$k=1,\\cdots,K$}\n    {\n    $r_k = \\text{queue\\_pop}(R)$\\;\n    $z_k = \\text{lookup}(Z, r_k)$\\;\n    $z_k = \\text{interpolate}(z_k, h_K, w_K)$\\;\n    $\\hat{f} = \\hat{f} + \\phi_k(z_k)$\\;\n    }\n    $\\hat{im} = \\mathcal{D}(\\hat{f}) $\\;\n    \\textbf{Return: } reconstructed image $\\hat{im}$\\;\n    }\n  \\end{algorithm}\n  }\n\\end{minipage}\n\\end{center}\n\n\\section{Implementation details} \\label{sec:impl}\n\\firstpara{VAR tokenizer.}\nAs aforementioned, we use the vanilla VQVAE architecture~\\cite{vqgan} and a multi-scale quantization scheme with $K$ extra convolutions (0.03M extra parameters).\nWe use a shared codebook for all scales with $V=4096$.\nFollowing the baseline \\cite{vqgan}, our tokenizer is also trained on OpenImages~\\cite{openimages} with the compound loss \\eqref{eq:vaeloss} and a spatial downsample ratio of $16\\times$.\n\n\\para{VAR transformer.}\nOur main focus is on VAR algorithm so we keep a simple model architecture design.\nWe adopt the architecture of standard decoder-only transformers akin to GPT-2 and VQGAN~\\cite{gpt2,vqgan} with adaptive normalization (AdaLN), which has widespread adoption and proven effectiveness in many visual generative models~\\cite{stylegan,stylegan2,stylegan3,stylegan-xl,stylegan-t,gigagan,dit,chen2023pixart}.\nFor class-conditional synthesis, we use the class embedding as the start token \\texttt{[s]} and also the condition of AdaLN.\nWe found normalizing $queries$ and $keys$ to unit vectors before attention can stablize the training.\nWe do not use advanced techniques in large language models, such as rotary position embedding (RoPE), SwiGLU MLP, or RMS Norm~\\cite{llama1,llama2}.\nOur model shape follows a simple rule like \\cite{scalinglaw} that the width $w$, head counts $h$, and drop rate $dr$ are linearly scaled with the depth $d$ as follows:\n\\begin{align}\n    w = 64d,\\quad\\quad h = d,\\quad\\quad dr = 0.1\\cdot d/24.\n\\end{align}\\vspace{-3pt}\nConsequently, the main parameter count $N$ of a VAR transformer with depth $d$ is given by\\footnote{Due to resource limitation, we use a single shared adaptive layernorm (AdaLN) acorss all attention blocks in 512$\\times$512 synthesis. In this case, the parameter count would be reduced to around $12dw^2 + 6w^2 \\approx 49152\\,d^3$.\n}:\n\\vspace{1pt}\n\\begin{align}\n    N(d) = \\underbrace{d\\cdot4w^2}_\\text{self-attention} + \\underbrace{d\\cdot8w^2}_\\text{feed-forward} + \\underbrace{d\\cdot6w^2}_\\text{adaptive layernorm} = 18\\,dw^2 = 73728\\,d^3. \\label{eq:param}\n\\end{align}\\vspace{-4pt}\n\nAll models are trained with the similar settings: a base learning rate of $10^{-4}$ per 256 batch size, an AdamW optimizer with $\\beta_1=0.9$, $\\beta_2=0.95$, $\\text{decay}=0.05$, a batch size from 768 to 1024 and training epochs from 200 to 350 (depends on model size).\nThe evaluations in \\secref{sec:exp} suggest that such a simple model design are capable of scaling and generalizing well.\n\n    \n\n\\section{Empirical Results} \\label{sec:exp}\n\\vspace{-2pt}\n\nThis section first compares VAR with other image generative model families in \\secref{sec:sota}.\nEvaluations on the scalability and generalizability of VAR models are presented in \\secref{sec:law} and \\appref{sec:zero}.\nFor implementation details and ablation study, please see \\appref{sec:impl} and \\appref{sec:abla}.\n\n\\begin{table}[!th]\n\\renewcommand\\arraystretch{1.05}\n\\centering\n\\setlength{\\tabcolsep}{2.5mm}{}\n\\small\n{\n\\caption{\\smallcaption\n\\textbf{Generative model family comparison on class-conditional ImageNet 256$\\times$256}.\n``$\\downarrow$'' or ``$\\uparrow$'' indicate lower or higher values are better.\nMetrics include Fréchet inception distance (FID), inception score (IS), precision (Pre) and recall (rec).\n``\\#Step'': the number of model runs needed to generate an image.\nWall-clock inference time relative to VAR is reported.\nModels with the suffix ``-re'' used rejection sampling.\n$\\dag$: taken from MaskGIT~\\cite{maskgit}.\n}\\label{tab:main}\n\\vspace{-2pt}\n\\scalebox{0.98}\n{\n\\begin{tabular}{c|l|cc|cc|cc|c}\n\\toprule\nType & Model          & FID$\\downarrow$ & IS$\\uparrow$ & Pre$\\uparrow$ & Rec$\\uparrow$ & \\#Para & \\#Step & Time \\\\\n\\midrule\nGAN   & BigGAN~\\cite{biggan}  & 6.95  & 224.5       & \\textbf{0.89} & 0.38 & 112M & 1    & $-$    \\\\\nGAN   & GigaGAN~\\cite{gigagan}     & 3.45  & 225.5       & 0.84 & \\textbf{0.61} & 569M & 1    & $-$ \\\\\nGAN   & StyleGan-XL~\\cite{stylegan-xl}  & 2.30  & 265.1       & 0.78 & 0.53 & 166M & 1    & 0.3~\\cite{stylegan-xl}   \\\\\n\\midrule\nDiff. & ADM~\\cite{adm}         & 10.94 & 101.0        & 0.69 & 0.63 & 554M & 250  & 168~\\cite{stylegan-xl}   \\\\\nDiff. & CDM~\\cite{cdm}         & 4.88  & 158.7       & $-$  & $-$  & $-$  & 8100 & $-$    \\\\\nDiff. & LDM-4-G~\\cite{ldm}     & 3.60  & 247.7       & $-$  & $-$  & 400M & 250  & $-$    \\\\\nDiff. & DiT-L/2~\\cite{dit}     & 5.02  & 167.2       & 0.75 & 0.57 & 458M & 250  & 31     \\\\\nDiff. & DiT-XL/2~\\cite{dit}    & 2.27  & 278.2       & 0.83 & 0.57 & 675M & 250  & 45     \\\\\nDiff. & L-DiT-3B~\\cite{dit-github}    & 2.10  & 304.4       & 0.82 & 0.60 & 3.0B & 250  & $>$45     \\\\\nDiff. & L-DiT-7B~\\cite{dit-github}    & 2.28  & 316.2       & 0.83 & 0.58 & 7.0B & 250  & $>$45     \\\\\n\\midrule\nMask. & MaskGIT~\\cite{maskgit}     & 6.18  & 182.1        & 0.80 & 0.51 & 227M & 8    & 0.5~\\cite{maskgit}  \\\\\nMask. & RCG (cond.)~\\cite{rcg}  & 3.49  & 215.5        & $-$  & $-$  & 502M & 20  & 1.9~\\cite{rcg}     \\\\\n\\midrule\nAR   & VQVAE-2$^\\dag$~\\cite{vqvae2} & 31.11           & $\\sim$45     & 0.36           & 0.57          & 13.5B    & 5120    & $-$  \\\\\nAR    & VQGAN$^\\dag$~\\cite{vqgan} & 18.65 & 80.4         & 0.78 & 0.26 & 227M & 256  & 19~\\cite{maskgit}   \\\\\nAR    & VQGAN~\\cite{vqgan}       & 15.78 & 74.3   & $-$  & $-$  & 1.4B & 256  & 24     \\\\\nAR    & VQGAN-re~\\cite{vqgan}    & 5.20  & 280.3  & $-$  & $-$  & 1.4B & 256  & 24     \\\\\nAR    & ViTVQ~\\cite{vit-vqgan}& 4.17  & 175.1  & $-$  & $-$  & 1.7B & 1024  & $>$24     \\\\\nAR    & ViTVQ-re~\\cite{vit-vqgan}& 3.04  & 227.4  & $-$  & $-$  & 1.7B & 1024  & $>$24     \\\\\nAR    & RQTran.~\\cite{rq}        & 7.55  & 134.0  & $-$  & $-$  & 3.8B & 68  & 21    \\\\\nAR    & RQTran.-re~\\cite{rq}     & 3.80  & 323.7  & $-$  & $-$  & 3.8B & 68  & 21    \\\\\n\\midrule\nVAR   & VAR-$d16$       & 3.30  & 274.4 & 0.84 & 0.51 & 310M & 10   & 0.4      \\\\\nVAR   & VAR-$d20$       & 2.57  & 302.6 & 0.83 & 0.56 & 600M & 10   & 0.5      \\\\\nVAR   & VAR-$d24$       & 2.09  & 312.9 & 0.82 & 0.59 & 1.0B & 10   & 0.6      \\\\\nVAR   & VAR-$d30$       & 1.92  & 323.1 & 0.82 & 0.59 & 2.0B & 10   & 1      \\\\\nVAR   & VAR-$d30$-re    & \\textbf{1.73}  & \\textbf{350.2} & 0.82 & 0.60 & 2.0B & 10   & 1      \\\\\n\t\t& \\graycell{(validation data)}   & \\graycell{1.78}  & \\graycell{236.9} & \\graycell{0.75} & \\graycell{0.67} &      &      &  \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\vspace{-4pt}\n}\n\\end{table}\n\n\\vspace{-0.8pt}\n\\subsection{State-of-the-art image generation} \\label{sec:sota}\n\\vspace{-0.8pt}\n\n\\para{Setup.}\nWe test VAR models with depths 16, 20, 24, and 30 on ImageNet 256$\\times$256 and 512$\\times$512 conditional generation benchmarks and compare them with the state-of-the-art image generation model families.\nAmong all VQVAE-based AR or VAR models, VQGAN~\\cite{vqgan} and ours use the same architecture (CNN) and training data (OpenImages \\cite{openimages}) for VQVAE, while ViT-VQGAN~\\cite{vit-vqgan} uses a ViT autoencoder, and both it and RQTransformer~\\cite{rq} trains the VQVAE directly on ImageNet.\nThe results are summaried in \\tabref{tab:main} and \\tabref{tab:512}.\n\n\\para{Overall comparison.}\nIn comparison with existing generative approaches including generative adversarial networks (GAN), diffusion models (Diff.), BERT-style masked-prediction models (Mask.), and GPT-style autoregressive models (AR), our visual autoregressive (VAR) establishes a new model class.\nAs shown in \\tabref{tab:main}, VAR not only achieves the best FID/IS but also demonstrates remarkable speed in image generation.\nVAR also maintains decent precision and recall, confirming its semantic consistency.\nThese advantages hold true on the 512$\\times$512 synthesis benchmark, as detailed in \\tabref{tab:512}.\nNotably, VAR significantly advances traditional AR capabilities. To our knowledge, this is the \\textit{first time} of autoregressive models outperforming Diffusion transformers, a milestone made possible by VAR's resolution of AR limitations discussed in Section~\\ref{sec:method}.\n\n\\begin{wraptable}[14]{r}{0.49\\textwidth}\n\\renewcommand\\arraystretch{1.06}\n\\centering\n\\small\n{\n\\vspace{-12pt}\n\\caption{\\smallcaption\n\\textbf{ImageNet 512$\\times$512 conditional generation.}\n$\\dag$: quoted from MaskGIT~\\cite{maskgit}. ``-s'': a single shared AdaLN layer is used due to resource limitation.\n}\\label{tab:512}\n\\vspace{5pt}\n{\n\\begin{tabular}{c|l|ccc}%|cc}\n\\toprule\nType & Model          & FID$\\downarrow$ & IS$\\uparrow$   & Time \\\\\n\\midrule\nGAN   & BigGAN~\\cite{biggan}            & 8.43  & 177.9  & $-$ \\\\\n\\midrule\nDiff. & ADM~\\cite{adm}                  & 23.24 & 101.0  & $-$ \\\\\nDiff. & DiT-XL/2~\\cite{dit}             & 3.04  & 240.8  & 81 \\\\\n\\midrule\nMask. & MaskGIT~\\cite{maskgit}          & 7.32  & 156.0  & 0.5$^\\dag$ \\\\\n\\midrule\nAR    & VQGAN~\\cite{vqgan}              & 26.52 & 66.8   & 25$^\\dag$ \\\\\nVAR   & VAR-$d36$-s    & \\textbf{2.63}  & \\textbf{303.2} & 1 \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\vspace{-3pt}\n}\n\\end{wraptable}\n    \n    \n\\para{Efficiency comparison.}\nConventional autoregressive (AR) models~\\cite{vqgan,vqvae2,vit-vqgan,rq} suffer a lot from the high computational cost, as the number of image tokens is quadratic to the image resolution.\nA full autoregressive generation of $n^2$ tokens requires $\\mathcal{O}(n^2)$ decoding iterations and $\\mathcal{O}(n^6)$ total computations.\nIn contrast, VAR only requires $\\mathcal{O}(\\log(n))$ iterations and $\\mathcal{O}(n^4)$ total computations.\nThe wall-clock time reported in \\tabref{tab:main} also provides empirical evidence that VAR is around 20 times faster than VQGAN and ViT-VQGAN even with more model parameters, reaching the speed of efficient GAN models which only require 1 step to generate an image.\n\n\\vspace{1pt}\n\\para{Compared with popular diffusion transformer.}\nThe VAR model surpasses the recently popular diffusion models Diffusion Transformer (DiT), which serves as the precursor to the latest Stable-Diffusion 3~\\cite{stable-diffusion3} and SORA~\\cite{sora}, in multiple dimensions:\n1) In image generation diversity and quality (FID and IS), VAR with 2B parameters consistently performs better than DiT-XL/2~\\cite{dit}, L-DiT-3B, and L-DiT-7B~\\cite{dit-github}. VAR also maintains comparable precision and recall.\n2) For inference speed, the DiT-XL/2 requires 45$\\times$ the wall-clock time compared to VAR, while 3B and 7B models~\\cite{dit-github} would cost much more.\n3) VAR is considered more data-efficient, as it requires only 350 training epochs compared to DiT-XL/2's 1400.\n4) For scalability, \\figref{fig:cmp} and \\tabref{tab:main} show that DiT only obtains marginal or even negative gains beyond 675M parameters.\nIn contrast, the FID and IS of VAR are consistently improved, aligning with the scaling law study in \\secref{sec:law}.\nThese results establish \\textit{VAR as potentially a more efficient and scalable model for image generation than models like DiT}.\n\n\\vspace{4pt}\n\\begin{figure}[th]\n\\begin{center}\n\\includegraphics[width=\\linewidth]{fig/P.pdf}\n\\end{center}\n\\vspace{-2pt}\n\\caption{\\small\n\\textbf{Scaling laws with VAR transformer size $N$}, with power-law fits (dashed) and equations (in legend).\nSmall, near-zero exponents $\\alpha$ suggest a smooth decline in both test loss $L$ and token error rate $Err$ when scaling up VAR transformer.\nAxes are all on a logarithmic scale.\nThe Pearson correlation coefficients near $-0.998$ signify a strong linear relationship between $log(N)$ \\textit{vs.} $log(L)$ or $log(N)$ \\textit{vs.} $log(Err)$.\n\\vspace{-6pt}\n}\n\\label{fig:P}\n\\end{figure}\n\n\\vspace{4pt}\n\\subsection{Power-law scaling laws} \\label{sec:law}\n\n\\firstpara{Background.}\nPrior research~\\cite{scalinglaw,scalingar,chinchilla,gpt4} have established that scaling up autoregressive (AR) large language models (LLMs) leads to a predictable decrease in test loss $L$.\nThis trend correlates with parameter counts $N$, training tokens $T$, and optimal training compute $C_\\text{min}$, following a power-law:\\vspace{1pt}\n\\begin{align}\n    L = (\\beta \\cdot X)^{\\alpha},\n\\end{align}\nwhere $X$ can be any of $N$, $T$, or $C_\\text{min}$.\nThe exponent $\\alpha$ reflects the smoothness of power-law, and $L$ denotes the reducible loss normalized by irreducible loss $L_{\\infty}$~\\cite{scalingar}\\footnote{See~\\cite{scalingar} for some theoretical explanation on scaling laws on negative-loglikelihood losses.}.\nA logarithmic transformation to $L$ and $X$ will reveal a linear relation between $\\log(L)$ and $\\log(X)$:\\vspace{1pt}\n\\begin{align}\n    \\log(L) = \\alpha \\log (X) + \\alpha \\log\\beta.\n\\end{align}\nAn appealing phenomenon is that both \\cite{scalinglaw} and \\cite{scalingar} never observed deviation from these linear relationships at the higher end of $X$, although flattening is inevitable as the loss approaches zero.\n\nThese observed scaling laws~\\cite{scalinglaw,scalingar,chinchilla,gpt4} not only validate the scalability of LLMs but also serve as a predictive tool for AR modeling, which facilitates the estimation of performance for larger AR models based on their smaller counterparts, thereby saving resource usage by large model performance forecasting.\nGiven these appealing properties of scaling laws brought by LLMs, their replication in computer vision is therefore of significant interest.\n\n\\vspace{4pt}\n\\para{Setup of scaling VAR models.}\nFollowing the protocols from~\\cite{scalinglaw,scalingar,chinchilla,gpt4}, we examine whether our VAR model complies with similar scaling laws.\nWe trained models across 12 different sizes, from 18M to 2B parameters, on the ImageNet training set~\\cite{imagenet} containing 1.28M images (or 870B image tokens under our VQVAE) per epoch.\nFor models of different sizes, training spanned 200 to 350 epochs, with a maximum number of tokens reaching 305 billion.\nBelow we focus on the scaling laws with model parameters $N$ and optimal training compute $C_\\text{min}$ given sufficient token count $T$.\n\n\\vspace{4pt}\n\\para{Scaling laws with model parameters $N$.}\nWe first investigate the test loss trend as the VAR model size increases.\nThe number of parameters $N(d)=73728\\,d^3$ for a VAR transformer with depth $d$ is specified in \\eqref{eq:param}.\nWe varied $d$ from $6$ to $30$, yielding 12 models with 18.5M to 2.0B parameters. We assessed the final test cross-entropy loss $L$ and token prediction error rates $Err$ on the ImageNet validation set of 50,000 images~\\cite{imagenet}.\nWe computed $L$ and $Err$ for both the last scale (at the last next-scale autoregressive step), as well as the global average.\nResults are plotted in \\figref{fig:P}, where we\n\n\\vspace{-5pt}\n\\begin{figure}[th]\n\\begin{center}\n\\includegraphics[width=\\linewidth]{fig/C.pdf}\n\\end{center}\n\\vspace{-4pt}\n\\caption{\\small\n\\textbf{Scaling laws with optimal training compute $C_\\text{min}$.}\nLine color denotes different model sizes.\nRed dashed lines are power-law fits with equations in legend.\nAxes are on a logarithmic scale.\nPearson coefficients near $-0.99$ indicate strong linear relationships between $\\log(C_\\text{min})$ \\textit{vs.} $\\log(L)$ or $\\log(C_\\text{min})$ \\textit{vs.} $\\log(Err)$.\n}\n\\vspace{-4pt}\n\\label{fig:C}\n\\end{figure}\n\nobserved a clear power-law scaling trend for $L$ as a function of $N$, as consistent with~\\cite{scalinglaw,scalingar,chinchilla,gpt4}. The power-law scaling laws can be expressed as:\n\\begin{align}\n    &L_\\text{last} = (2.0 \\cdot N)^{-0.23} \\quad \\text{and} \\quad L_\\text{avg} = (2.5 \\cdot N)^{-0.20}.\n\\end{align}\n\nAlthough the scaling laws are mainly studied on the test loss, we also empirically observed similar power-law trends for the token error rate $Err$:\\vspace{4pt}\n\\begin{align}\n    &Err_\\text{last} = (4.9 \\cdot 10^2 N)^{-0.016} \\quad \\text{and} \\quad Err_\\text{avg} = (6.5 \\cdot 10^2 N)^{-0.010}.\n\\end{align}\\vspace{-12pt}\n\nThese results verify the strong scalability of VAR, by which scaling up VAR transformers can continuously improve the model's test performance.\n\n\\para{Scaling laws with optimal training compute $C_\\text{min}$.}\nWe then examine the scaling behavior of VAR transformers when increasing training compute $C$.\nFor each of the 12 models, we traced the test loss $L$ and token error rate $Err$ as a function of $C$ during training quoted in PFlops ($10^{15}$ floating-point operations per second).\nThe results are plotted in \\figref{fig:C}.\nHere, we draw the Pareto frontier of $L$ and $Err$ to highlight the optimal training compute $C_\\text{min}$ required to reach a certain value of loss or error.\n\nThe fitted power-law scaling laws for $L$ and $Err$ as a function of $C_\\text{min}$ are:\n\\begin{align}\n    &L_\\text{last} = (2.2 \\cdot 10^{-5} C_\\text{min})^{-0.13} \\\\\n    &L_\\text{avg} = (1.5 \\cdot 10^{-5} C_\\text{min})^{-0.16}, \\label{equ:claw1} \\\\\n    &Err_\\text{last} = (8.1 \\cdot 10^{-2} C_\\text{min})^{-0.0067} \\\\\n    &Err_\\text{avg} = (4.4 \\cdot 10^{-2} C_\\text{min})^{-0.011}. \\label{equ:claw2}\n\\end{align}\n\nThese relations (\\ref{equ:claw1}, \\ref{equ:claw2}) hold across 6 orders of magnitude in $C_\\text{min}$, and our findings are consistent with those in~\\cite{scalinglaw,scalingar}: when trained with sufficient data, larger VAR transformers are more compute-efficient because they can reach the same level of performance with less computation.\n\n\\subsection{Visualization of scaling effect} \\label{sec:viss}\nTo better understand how VAR models are learning when scaled up, we compare some generated $256\\times256$ samples from VAR models of 4 different sizes (depth 6, 16, 26, 30) and 3 different training stages (20\\%, 60\\%, 100\\% of total training tokens) in \\figref{fig:9grid}.\nTo keep the content consistent, a same random seed and teacher-forced initial tokens are used.\nThe observed improvements in visual fidelity and soundness are consistent with the scaling laws, as larger transformers are thought able to learn more complex and fine-grained image distributions.\n\n\\begin{figure}[!th]\n\\begin{center}\n\\includegraphics[width=\\linewidth]{fig/9grid_text.jpg}\n\\end{center}\n\\vspace{-6pt}\n\\caption{\\small\n\\textbf{Scaling model size $N$ and training compute $C$ improves visual fidelity and soundness.}\nZoom in for a better view.\nSamples are drawn from VAR models of 4 different sizes and 3 different training stages.\n9 class labels (from left to right, top to bottom) are: flamingo \\texttt{130}, arctic wolf \\texttt{270}, macaw \\texttt{88}, Siamese cat \\texttt{284}, oscilloscope \\texttt{688}, husky \\texttt{250}, mollymawk \\texttt{146}, volcano \\texttt{980}, and catamaran \\texttt{484}.\n}\n\\label{fig:9grid}\n\\end{figure}\n\n\\begin{figure}[tbh]\n\\begin{center}\n    \\includegraphics[width=0.95\\linewidth]{fig/zero.pdf}\n\\end{center}\n\\vspace{-8pt}\n\\caption{\\small\n\\textbf{Zero-shot evaluation in downstream tasks} containing in-painting, out-painting, and class-conditional editing.\nThe results show that VAR can generalize to novel downstream tasks without special design and finetuning. Zoom in for a better view.\n}\n\\vspace{-10pt}\n\\label{fig:zeroshot}\n\\end{figure}\n\n\\section{Zero-shot task generalization} \\label{sec:zero}\n\n\\para{Image in-painting and out-painting.}\nVAR-$d$30 is tested.\nFor in- and out-painting, we teacher-force ground truth tokens outside the mask and let the model only generate tokens within the mask.\nNo class label information is injected into the model.\nThe results are visualized in \\figref{fig:zeroshot}.\nWithout modiﬁcations to the network architecture or tuning parameters, VAR has achieved decent results on these downstream tasks, substantiating the generalization ability of VAR.\n\n\\para{Class-conditional image editing.}\nFollowing MaskGIT~\\cite{maskgit} we also tested VAR on the class-conditional image editing task.\nSimilar to the case of in-painting, the model is forced to generate tokens only in the bounding box conditional on some class label.\n\\Figref{fig:zeroshot} shows the model can produce plausible content that fuses well into the surrounding contexts, again verifying the generality of VAR.\n\n\\begin{table}[b]\n\\renewcommand\\arraystretch{1.06}\n\\centering\n\\setlength{\\tabcolsep}{2.5mm}{}\n\\small\n{\n\\caption{\\smallcaption\n\\textbf{Ablation study of VAR.} The first two rows compare GPT-2-style transformers trained under AR or VAR algorithm without any bells and whistles.\nSubsequent lines show the influence of VAR enhancements.\n``AdaLN'': adaptive layernorm.\n``CFG'': classifier-free guidance.\n``Attn. Norm.'': normalizing $q$ and $k$ to unit vectors before attention.\n``Cost'': inference cost relative to the baseline.\n``$\\Delta$'': FID reduction to the baseline.\n}\\label{tab:abla}\n{\\begin{tabular}{ll|ccccc|ccc}\n\\toprule\n    $\\ $  & Description       & Para. & Model & AdaLN & Top-$k$ & CFG  & Cost & FID$\\downarrow$  & $\\Delta$ \\\\\n\\midrule\n\\graycell{1}& AR~\\cite{vqgan}  & 227M  & AR  & \\cha   & \\cha    & \\cha  & 1 & 18.65 & $~~~$0.00     \\\\\n\\ablanum{2} & AR to VAR          & 207M  & VAR-$d$16  & \\cha  & \\cha    & \\cha  & 0.013 & 5.22 & $-$13.43  \\\\\n\\midrule\n\\ablanum{3} & $+$AdaLN           & 310M  & VAR-$d$16  & \\gou  & \\cha  & \\cha  & 0.016 & 4.95 & $-$13.70 \\\\\n\\ablanum{4} & $+$Top-$k$         & 310M  & VAR-$d$16  & \\gou  & 600   & \\cha  & 0.016 & 4.64 & $-$14.01 \\\\\n\\ablanum{5} & $+$CFG             & 310M  & VAR-$d$16  & \\gou  & 600   & 2.0   & 0.022 & 3.60 & $-$15.05 \\\\\n\\ablanum{5} & $+$Attn. Norm.     & 310M  & VAR-$d$16  & \\gou  & 600   & 2.0   & 0.022 & 3.30 & $-$15.35 \\\\\n\\midrule\n\\ablanum{6} & $+$Scale up        & 2.0B  & VAR-$d$30  & \\gou  & 600   & 2.0   & 0.052 & 1.73 & $-$16.85 \\\\\n\\bottomrule\n\\end{tabular}}\n}\n\\vspace{-2pt}\n\\end{table}\n    \n\n\\section{Ablation Study} \\label{sec:abla}\n\nIn this study, we aim to verify the effectiveness and efficiency of our proposed VAR framework.\nResults are reported in \\tabref{tab:abla}.\n\n\\firstpara{Effectiveness and efficiency of VAR.} Starting from the vanilla AR transformer baseline implemented by \\cite{maskgit}, we replace its methodology with our VAR and keep other settings unchanged to get \\ablaref{2}. VAR achieves a way more better FID (18.65 \\textit{vs.} 5.22) with only 0.013$\\times$ inference wall-clock cost than the AR model, which demonstrates a leap in visual AR model's performance and efficiency.\n\n\\para{Component-wise ablation.} We further test some key components in VAR. By replacing the standard Layer Normalization (LN) with Adaptive Layer Normalization (AdaLN), VAR starts yielding better FID than baseline. By using the top-$k$ sampling similar to the baseline, VAR's FID is further improved. By using the classifier-free guidance (CFG) with ratio $2.0$ and normalizing $q$ and $k$ to unit vectors before attention, we reach the FID of 3.30, which is 15.35 lower to the baseline, and its inference speed is still 45 times faster. We finally scale up VAR size to 2.0B and achieve an FID of 1.73. This is 16.85 better than the baseline FID.\n\n    \n\n\\section{Limitations and Future Work} \\label{sec:limit}\n\\vspace{-2pt}\n\nIn this work, we mainly focus on the design of learning paradigm and keep the VQVAE architecture and training unchanged from the baseline \\cite{vqgan} to better justify VAR framework's effectiveness. We expect \\textbf{advancing VQVAE tokenizer} \\cite{movq,fsq,magvit2} as another promising way to enhance autoregressive generative models, which is orthogonal to our work. We believe iterating VAR by advanced tokenizer or sampling techniques in these latest work can further improve VAR's performance or speed.\n\n\\textbf{Text-prompt generation}\\, is an ongoing direction of our research. Given that our model is fundamentally similar to modern LLMs, it can easily be integrated with them to perform text-to-image generation through either an encoder-decoder or in-context manner. This is currently in our high priority for exploration.\n\n\\textbf{Video generation}\\, is not implemented in this work, but it can be naturally extended. By considering multi-scale video features as \\textbf{3D pyramids}, we can formulate a similar ``\\textbf{3D next-scale prediction}'' to generate videos via VAR.\nCompared to diffusion-based generators like SORA \\cite{sora}, our method has inherent advantages in temporal consistency or integration with LLMs, thus can potentially handle longer temporal dependencies.\nThis makes VAR competitive in the video generation field, because traditional AR models can be too inefficient for video generation due to their extremely high computational complexity and slow inference speed: it is becoming prohibitively expensive to generate high-resolution videos with traditional AR models, while VAR is capable to solve this.\nWe therefore foresee a promising future for exploiting VAR models in the realm of video generation.\n\n\\vspace{-4pt}\n\\section{Conclusion} \\label{sec:conc}\n\\vspace{-4pt}\n\nWe introduced a new visual generative framework named Visual AutoRegressive modeling (VAR) that 1) theoretically addresses some issues inherent in standard image autoregressive (AR) models, and 2) makes language-model-based AR models first surpass strong diffusion models in terms of image quality, diversity, data efficiency, and inference speed.\nUpon scaling VAR to 2 billion parameters, we observed a clear power-law relationship between test performance and model parameters or training compute, with Pearson coefficients nearing $-0.998$, indicating a robust framework for performance prediction.\nThese scaling laws and the possibility for zero-shot task generalization, as hallmarks of LLMs, have now been initially verified in our VAR transformer models.\nWe hope our findings and open sources can facilitate a more seamless integration of the substantial successes from the natural language processing domain into computer vision, ultimately contributing to the advancement of powerful multi-modal intelligence.\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{Computational Tradeoffs in Image Synthesis: Diffusion, Masked-Token, and Next-Token Prediction}\n\n\\begin{document}\n\n\\maketitle\n\n\\newcommand{\\aereconstructiontable}{\n\\begin{table}[t]\n\\begin{minipage}{.5\\textwidth}\n  \\centering\n  \\begin{tabular}{lccc}\n    \\toprule\n    Regularizer & Latent space capacity & rFID ($\\downarrow$) \\\\\n    \\midrule\n    KL & 16 channels & 1.060 \\\\\n    KL & 8 channels & 1.560 \\\\\n    KL$_{es}$ & 8 channels & 2.856 \\\\\n    KL & 4 channels & 2.410 \\\\\n    LFQ & 16384 vocabulary & 2.784 \\\\\n    \\bottomrule \\\\\n  \\end{tabular}\n  \\caption{\\textbf{Autoencoders.} Reconstruction metrics for differently regularized and trained autoencoders (downsampling $f=8$). \"es\" is early stopping to match the LFQ autoencoder. \\vspace{-1em}}\n  \\label{tab:aereconstructiontable}\n  \\end{minipage}%%\n  \\hspace{0.2cm}\n  \\begin{minipage}{.5\\textwidth}\n  \\centering\n    \\small\n    \\begin{tabular}{l c c c c c}\n    \\toprule\n    Model size & Layers $N$ & Hidden size $d$ &  Heads \\\\\n    \\midrule \n    S  &   12   &      768    &   12  \\\\\n    M  &    24   &      1024    &   16  \\\\\n    L &    24  &       1536     &   16  \\\\\n    XL & 32 & 2304 & 32 \\\\\n    \\bottomrule \\\\\n    \\end{tabular}\n    \\caption{\\textbf{Transformer configurations.} Base transformer hyperparameters for models we train. Common across all approaches}\n    \\label{tab:tfsizetable}\n  \\end{minipage}\n  \\vspace{-1em}\n\\end{table}\n}\n\n\\newcommand{\\modelflopstable}{\n\\begin{table}\n\\begin{minipage}{.5\\textwidth}\n\\centering\n\\small\n\\scalebox{0.9}{\n\\begin{tabular}{l c c c}\n\\toprule\nModel & n-parameters (\\%) &  Forward TFLOPs \\\\\n\\midrule \nDiT-S  &   131.13 M (97.2\\%)   &   0.2133  \\\\\nDiT-M  &   459.19 M (98.7\\%)   &   0.7234  \\\\\nDiT-L  &   1031.67 M (98.8\\%) &   1.5485  \\\\\nDiT-XL & 3083.69 M (99.2\\%) & 4.4901 \\\\\nNT/MT-S  &   153.73 M (82.9\\%)   &   0.2261  \\\\\nNT/MT-M  &   494.56 M (92.9\\%)   &   0.6631  \\\\\nNT/MT-L  &   1072.14 M (95.1\\%) &   1.3421  \\\\\nNT/MT-XL & 3137.29 M (97.5\\%) & 3.7166 \\\\\n\\bottomrule \\\\\n\\end{tabular}}\n\\caption{\\textbf{Model forward pass costs.} Number of parameters and FLOPs used in each forward pass for all models we trained. DiT - Diffusion Transformer; NT - Next-token; MT - Masked-token}\n\\label{tab:modelflopstable}\n\\end{minipage}%\n\\hspace{0.2cm}\n\\begin{minipage}{.5\\textwidth}\n\\small\n\\scalebox{0.9}{\n\\begin{tabular}{l c c c c c}\n\\toprule\nObjective & Conditioning & FID &  CLIP \\\\\n\\midrule \nNT  &   adaLNzero   &      \\textbf{83.052}    &   \\textbf{0.2213}  \\\\\nNT  &    in context   &      88.176    &   0.2041  \\\\\nNT &    cross attention  &       92.852     &   0.2062  \\\\\n\\midrule\nMT  &   adaLNzero   &      \\textbf{97.021}    &   \\textbf{0.2164}  \\\\\nMT  &    in context   &      100.646    &   0.1925  \\\\\nMT &    cross attention  &       103.221     &   0.1960  \\\\\n\\bottomrule \\\\\n\\end{tabular}}\n\\caption{\\textbf{Conditioning method ablation.} Results for different objectives and conditioning methods. adaLNzero conditioning is used for the remainder of experiments. NT - Next-token; MT - Masked-token.}\n\\label{tab:condablation}\n\\end{minipage}\n\\vspace{-2em}\n\\end{table}\n}\n\n\\newcommand{\\emaablation}{\n\\begin{table}\n\\centering\n\\small\n\\scalebox{0.9}{\n\\begin{tabular}{l c c c c c}\n\\toprule\nObjective & LR schedule & EMA & FID &  CLIP \\\\\n\\midrule \nNext-token & constant & \\xmark &  81.976    &   0.2208  \\\\\nNext-token & constant & \\cmark &   79.571   &   0.2230  \\\\\nNext-token & cosine & \\xmark &    \\textbf{75.715}    &   0.2256  \\\\\nNext-token & cosine & \\cmark &   76.404   &   \\textbf{0.2257}  \\\\\n\\midrule\nDiffusion & constant & \\xmark &  74.087    &   0.2153  \\\\\nDiffusion & constant & \\cmark &   71.789   &   0.2166  \\\\\nDiffusion & cosine & \\xmark &    \\textbf{69.284}    &   \\textbf{0.2195}  \\\\\nDiffusion & cosine & \\cmark &   69.468   &   0.2192  \\\\\n\\bottomrule\n\\\\\n\\end{tabular}}\n\\caption{\\textbf{EMA and learning rate schedules.} EMA on model weights improves results under a constant learning rate schedule but does not exceed the gains from using a cosine decay schedule.}\n\\label{tab:emaablation}\n\\end{table}\n}% Figures\n\n\\newcommand{\\diffae}{\n\\begin{figure*}[t]\n\\centering\n\\begin{subfigure}[t]{0.49\\textwidth}\n    \\centering\n    \\includegraphics[width=0.9\\linewidth]{img/diff_ae_clip.png}\n    \\label{fig:diffae:clip}\n\\end{subfigure}\\hfill%\n\\begin{subfigure}[t]{0.49\\textwidth}\n    \\centering\n    \\includegraphics[width=0.9\\linewidth]{img/diff_ae_fid.png}\n    \\label{fig:diffae:fid}\n\\end{subfigure}\n\\caption{\\textbf{Impact of autoencoder quality on diffusion models.} We train L-size diffusion models on our set of continuous latent space autoencoders. The choice of autoencoder has more impact on FID than CLIP score.  Effectively using a larger latent space requires more compute and model capacity.}\n\\vspace{-1em}\n\\label{fig:diffae}\n\\end{figure*}\n}\n\n\\newcommand{\\mainres}{\n\\begin{figure*}[t]\n\\centering\n\\begin{subfigure}[t]{0.49\\textwidth}\n    \\centering\n    \\includegraphics[width=\\linewidth]{img/main_clip.png}\n    \\label{fig:main:clip}\n\\end{subfigure}\\hfill%\n\\begin{subfigure}[t]{0.49\\textwidth}\n    \\centering\n    \\includegraphics[width=\\linewidth]{img/main_fid.png}\n    \\label{fig:main:fid}\n\\end{subfigure}\n\\vspace{-1em}\n\\caption{\\textbf{Training compute efficiency on perceptual metrics.} Performance on CLIP and FID scores for various models and dataset sizes across different image synthesis approaches. On FID, next-token prediction is initially the most compute-efficient but scaling trends suggest it is eventually matched by diffusion. Token-based methods significantly outperform diffusion in CLIP score. Both axes are in log scale.\n\\vspace{-1em}}\n\\label{fig:main}\n\\end{figure*}\n}\n\n\\newcommand{\\fintl}{\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=0.9\\linewidth]{img/fin_tl.png}\n\\caption{\\textbf{Training compute efficiency on final loss.} All objectives follow predictable scaling trends. Right plot shows the difference in loss scale between diffusion models trained on top of different autoencoders. FLOPs axis is in log scale.\n\\vspace{-1em}}\n\\label{fig:fintl}\n\\vspace{-1em}\n\\end{figure*}\n}\n\n\\newcommand{\\sdres}{\n\\begin{figure*}[t]\n\\centering\n\\begin{subfigure}[t]{0.49\\textwidth}\n    \\centering\n    \\includegraphics[width=\\linewidth]{img/sd3_clip.png}\n    \\label{fig:sdres:clip}\n\\end{subfigure}\\hfill%\n\\begin{subfigure}[t]{0.49\\textwidth}\n    \\centering\n    \\includegraphics[width=\\linewidth]{img/sd3_fid.png}\n    \\label{fig:sdres:fid}\n\\end{subfigure}\n\\caption{\\textbf{Skewing Autoencoding Conditions.} Beyond a specific compute budget, diffusion models, when equipped with a superior autoencoder exceed the FID compute efficiency of next-token prediction. However, in terms of CLIP scores, token-based methods continue to significantly outperform. \n\\vspace{-1em}}\n\\label{fig:sdres}\n\\end{figure*}\n}\n\n\\newcommand{\\emainfluence}{\n\\begin{figure*}[t]\n\\centering\n\\begin{subfigure}[t]{0.5\\textwidth}\n    \\centering\n    \\includegraphics[width=\\linewidth]{img/ema_influence_clip.png}\n    \\label{fig:sd:clip}\n\\end{subfigure}\\hfill%\n\\begin{subfigure}[t]{0.5\\textwidth}\n    \\centering\n    \\includegraphics[width=\\linewidth]{img/ema_influence_fid.png}\n    \\label{fig:sd:fid}\n\\end{subfigure}\n\\caption{\\textbf{Impact of EMA.} EMA significantly improves FID for diffusion models but hurts token based approaches. On CLIP score the effect on diffusion models stays consistent however for token based methods the influence is negligible. \n\\vspace{-1em}}\n\\label{fig:emainfluence}\n\\end{figure*}\n}\n\n\\newcommand{\\infflops}{\n\\begin{figure*}[t]\n\\centering\n\\begin{subfigure}[t]{0.49\\textwidth}\n    \\centering\n    \\includegraphics[width=0.9\\linewidth]{img/inf_clip.png}\n    \\label{fig:sdres:clip}\n\\end{subfigure}\\hfill%\n\\begin{subfigure}[t]{0.49\\textwidth}\n    \\centering\n    \\includegraphics[width=0.9\\linewidth]{img/inf_fid.png}\n    \\label{fig:infflops:fid}\n\\end{subfigure}\n\\caption{\\textbf{Inference compute efficiency on perceptual metrics.} Diffusion and masked token prediction evaluated at 4, 10, 20, 50, and 100 sampling steps. Next token prediction is 1 forward pass factorized over each token individually. Masked token prediction isn't influenced by the number of sampling steps very much. Next token prediction is the most compute efficient. Both axes are in log scale.\n\\vspace{-1em}}\n\\label{fig:infflops}\n\\end{figure*}\n}\n\n\\newcommand{\\scalingsamples}{\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=0.75\\linewidth]{img/scaling_grid_samples.png}\n\\caption{\\textbf{Increasing training compute improves sample quality for all approaches.} For each approach and prompt we sample an image with all combinations of S, M, L model sizes and 50k, 150k, 250k dataset sizes. Going down or right in the 3x3 increases dataset and model size respectively.\n\\vspace{-1em}}\n\\label{fig:scalingsamples}\n\\end{figure*}\n}\n\n\\newcommand{\\bestsamples}{\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=1.0\\linewidth]{img/concatenated_result.png}\n\\caption{\\textbf{Images generated using our best models.} Top row is from a next-token prediction model, bottom row is from a diffusion model. Both models are XL size and trained for 500k steps.\n\\vspace{-1em}}\n\\label{fig:bestsamples}\n\\end{figure*}\n}\n\n\\newcommand{\\quantizers}{\n\\begin{figure*}[h!]\n\\centering\n\n\\begin{subfigure}{0.8\\textwidth}\n    \\centering\n    \\includegraphics[width=\\linewidth]{img/rfid_plot.png}\n\\end{subfigure}\\\\[1ex] % Adjust vertical space between subfigures\n\n\\begin{subfigure}{0.8\\textwidth}\n    \\centering\n    \\includegraphics[width=\\linewidth]{img/psnr_plot.png}\n\\end{subfigure}\\\\[1ex] % Adjust vertical space between subfigures\n\n\\begin{subfigure}{0.8\\textwidth}\n    \\centering\n    \\includegraphics[width=\\linewidth]{img/lpips_plot_w_overlay.png}\n\\end{subfigure}\n\n\\caption{\\textbf{Perceptual reconstruction metrics for various discrete regularization methods.} Classic vector quantization (VQ) struggles without tricks like codebook reinitialization. LFQ and FSQ have different training dynamics, often trading the lead in the beginning phases of training which is highlighted by the red X's.}\n\\vspace{-2ex} % Reduce space below the final caption if necessary\n\\label{fig:quantizers}\n\\end{figure*}\n}\\begin{abstract}\n\\label{sec:abstract}\nNearly every recent image synthesis approach, including diffusion, masked-token prediction, and next-token prediction, uses a Transformer network architecture. Despite this common backbone, there has been no direct, compute controlled comparison of how these approaches affect performance and efficiency. We analyze the scalability of each approach through the lens of compute budget measured in FLOPs. We find that token prediction methods, led by next-token prediction, significantly outperform diffusion on prompt following. On image quality, while next-token prediction initially performs better, scaling trends suggest it is eventually matched by diffusion. We compare the inference compute efficiency of each approach and find that next token prediction is by far the most efficient. Based on our findings we recommend diffusion for applications targeting image quality and low latency; and next-token prediction when prompt following or throughput is more important.\n\\end{abstract}\\section{Introduction}\n\\label{sec:introduction}\n\nFollowing the work of \\cite{peebles2023scalable}, deep image synthesis, including diffusion~\\citep{sohl2015deep,song2019generative,song2020score,ho2020denoising,rombach2022highresolution,esser2024scaling}, masked-token prediction~\\citep{chang2022maskgit, chang2023muse, villegas2022phenaki, yu2024language}, and next-token prediction~\\citep{gafni2022makeascene, yu2022scaling, esser2021taming}, are all build on a common Transformer architecture \\citep{vaswani2023attention}. \nAlthough these approaches are all known to scale well with compute and data, there has been relatively little controlled comparisons of their relative training and inference efficiency. Comparing these latent image synthesis approaches is challenging since the objectives they optimize often have different requirements which limit the set of applicable modules for each approach and influence their optimal configurations. For example, next-token prediction requires discrete input data which makes it unfit for continuous latent space regularization advances. In fact, latent image synthesis will be strongly influenced by the state of autoencoding research, often in an unbalanced way. Examples of this can be found in Section \\ref{sec:relatedworks:latgenmod}.\n\nIn this paper, we measure the computational tradeoffs between popular transformer-based latent image synthesis approaches - diffusion, masked-token prediction, and next-token prediction. We investigate the impact of the autoencoder, which encodes the latent space, on generative results and train a large grid of models with the different approaches, model sizes, and dataset sizes.  Samples from some of our most capable models can be found in Figure \\ref{fig:bestsamples}. Our findings indicate that (i) at smaller compute budgets, next-token prediction yields the best image quality but scaling trends suggest it is eventually matched by diffusion. (ii) Token-based approaches achieve superior controllability. (iii) The quality of the autoencoder impacts the FID more than the CLIP score of diffusion models trained on its latent space. (iv) We find preliminary evidence for improved diffusion training practices. Based on our findings, we recommend diffusion models for applications targeting low latency and high image quality; and next-token prediction for applications where prompt following and throughput are priorities.\n\n\\bestsamples\\section{Related Work}\n\\label{sec:relatedworks}\n\n\\textbf{Scaling transformer-based generative models}.\n\\label{sec:relatedworks:scaling}\nScaling compute budgets for transformer based generative models is a predictable method for improving performance. \\cite{kaplan2020scaling, hoffmann2022training, clark2022unified} showed that for text, final training loss can be accurately predicted as a power law of training compute which depends on model size and dataset size. Following those practices many capable text generation models were trained \\citep{touvron2023llama, brown2020language, rae2022scaling}. Similar results have been found for vision \\citep{zhai2022scaling, alabdulmohsin2024getting, esser2024scaling, dehghani2023scaling} and even mixed modal data \\citep{aghajanyan2023scaling}. We follow these intuitions and analyze image synthesis performance as a function of compute budget.\n\n\\textbf{Latent generative modeling}.\n\\label{sec:relatedworks:latgenmod}\nTraining latent generative vision models has emerged as an efficient alternative to the computationally intensive modeling of high-dimensional pixel space. Studies have demonstrated the advantages of imposing specific structural regularizations within the latent space for enhancing the performance of various generative models. For instance, \\cite{rombach2022highresolution} observed that latent diffusion models operating in VAE-style \\cite{kingma2022autoencoding} latent spaces, when regularized towards a standard Gaussian structure, outperform models trained with alternative regularization techniques. \\cite{yu2024language, mentzer2023finite, yu2022vectorquantized} have shown that simplifying vector quantization methods can mitigate common issues such as poor codebook utilization and enhancing the transfer between autoencoder reconstruction quality and downstream generative model performance for token-based approaches. \\cite{tian2024visual} demonstrated that employing hierarchical next-scale latents enables transformers using next token prediction to leverage their in-context learning capabilities more effectively, significantly improving performance. \\cite{jin2024unified} use image latents dynamically sized based on their information content which allows generative models to allocate more computational resources to complex samples, as these will contain more tokens. We minimize potential bias coming from autoencoding asymmetries by studying the impact of the autoencoder on the generative model trained on top of it.\n\n\\section{Background}\n\\label{sec:methods}\n\\paragraph{Autoencoding}\nTo train latent generative models, we establish an encoder-decoder pair \\((\\mathcal{E}, \\mathcal{D})\\). For an image \\(x \\in \\mathbb{R}^{H \\times W \\times 3}\\), the encoder maps \\(x\\) to a latent representation \\(z = \\mathcal{E}(x)\\), where \\(z \\in \\mathbb{R}^{H/f \\times W/f \\times c}\\) and \\(f = 2^{m \\in \\mathbb{N}}\\) represents the factor of dimensionality reduction. The decoder then reconstructs \\(\\hat{x} = \\mathcal{D}(z)\\), aiming for high perceptual similarity to \\(x\\), effectively making \\(z\\) a perceptually compressed representation of the input. To avoid high-variance latent spaces and ensure structured representations, we employ regularization methods classified into two main types: discrete and continuous. The regularization function \\(\\textbf{q}\\) for a continuous regularizer maps \\(\\mathbb{R}^d \\to \\mathbb{R}^d\\), while a discrete regularizer maps \\(\\textbf{q}: \\mathbb{R}^d \\to \\{0, 1, 2, \\ldots, N\\}\\), making the latent space finite. In the following subsections we use $z \\in \\mathbb{R}^{s \\times d}$ to denote the flattened representation output by the encoder $\\mathcal{E}$. $p(z)$ is the latent data distribution we are interested in estimating using our generative models. We recover images by inputting sampled latents into the corresponding decoder $\\mathcal{D}$.\n\n\\paragraph{Next token prediction}\nIn the context of sequences of discrete tokens represented as \\( z \\in \\{0, 1, 2, \\ldots, N\\}^{s} \\), we employ the chain rule of conditional probability to decompose the target distribution into a product of conditional distributions which are tractable since the range of $z_i$ is finite. To model this distribution, we use a neural network \\( f \\), parameterized by weights \\( \\theta \\). The parameters are optimized by minimizing the negative log-likelihood \\( \\mathcal{L}_{NT} \\).\n\\vspace{-2pt}\n\\begin{align}\np(z) &= \\prod_{i=1}^{n} p(z_i | z_{i-1}, \\ldots, z_1) & \\mathcal{L}_{NT} &= \\mathbb{E}_i [ -\\log{p(z_i | z_{<i}; \\theta}) ]\n\\end{align}\n\\vspace{-2pt}\nSampling from our learned distribution begins with an empty sequence (in practice, a \"start of text\" token is sampled with 1.0 probability). We then sample the first token unconditionally and append it to our sequence. The process continues by iteratively evaluating the conditionals and sampling from them, with each step increasing the sequence length by one.\n\n\\paragraph{Masked token prediction}\nMasked token prediction is a form of iterative denoising and can be viewed as a discrete diffusion process. In this process, tokens progressively transition to an absorbing [MASK] state according to a probability defined by a noise schedule \\(\\gamma(t) \\in (0, 1]\\) where $t \\sim \\mathcal{U}(0, 1)$. This transition can also be mathematically expressed as a product of conditionals, except in a perturbed order $\\sigma$, and implemented as a neural network. Here, \\(\\sigma(i)\\) is a surjective function mapping \\([0, N] \\mapsto [0, N]\\). We follow \\cite{chang2022maskgit, chang2023muse} where $\\sigma(i) = \\sigma(i, t)$ such that \\(p(\\sigma(i, t) < j) = \\gamma(t)\\) meaning the likelihood a token can be attended to is independent of position. In this formulation, we utilize a truncated \\( \\arccos \\) distribution for our noise schedule: $\\gamma(t) = \\frac{2}{\\pi} (1 - t^2)^{-\\frac{1}{2}}$. To apply this method, we generate a mask tensor \\(M \\in \\{0, 1\\}^s\\) by sampling \\(t \\sim \\mathcal{U}(0, 1)\\) and \\(m_i \\sim \\text{Bernoulli}(\\gamma(t))\\). The tensor \\(M\\) is applied elementwise to the latents, replacing \\(z_i\\) with the [MASK] token if \\(m_i = 1\\); otherwise, \\(z_i\\) remains unchanged. Denote the resultant noised sequence as \\(z_M\\). The network is then trained to minimize the masked token loss \\(\\mathcal{L}_{MT}\\).\n\\vspace{-2pt}\n\\begin{align}\np(z) &= \\prod_{i, \\sigma(i) = j}^{n} p(z_j | z_{\\sigma(i) < j}) & \\quad \\mathcal{L}_{MT} &= \\mathbb{E}_{i, m_i=1} [ -\\log{p(z_i | z_{\\overline{M}}; \\theta}) ]\n\\end{align}\n\\vspace{-2pt}\nSampling from the distribution starts with a fully masked sequence and iterates through a discretized noise schedule \\(t_i = i/N\\) over \\(N\\) desired steps. At each step, the model estimates \\(p(z | z_{\\overline{M}})\\) for sampling, followed by re-noising using \\(\\gamma(t_{i+1})\\). This iterative re-noising and sampling process is repeated \\(N\\) times to yield the final sample.\n\n\\paragraph{Diffusion}\nWe adopt the flow matching framework outlined by \\cite{lipman2023flow}, focusing on models that map samples from a noise distribution \\( p_1 \\) to a data distribution \\( p_0 \\) using continuous trajectories governed by an ordinary differential equation (ODE). Furthermore, we enforce straight paths between the terminal distributions (by setting \\( \\alpha_t = 1 - t \\) and \\( \\beta_t = t \\))  since this has been shown to perform well at scale \\citep{esser2024scaling}.\n\\vspace{-2pt}\n\\begin{align}\n\\label{eq:ode}\nd\\phi_t(x) &= v_t(\\phi_t(x)) \\, dt & \\phi_0(x) &= x & z_t &= \\alpha_t x_0 + \\beta_t \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, 1)\n\\end{align}\n\\vspace{-2pt}\nHere, \\( v_t : [0, 1] \\times \\mathbb{R}^d \\mapsto \\mathbb{R}^d \\) represents a time-dependent vector field, which we aim to parameterize using a neural network \\( \\theta \\) and $\\phi_t : [0, 1] \\times \\mathbb{R}^d \\mapsto \\mathbb{R}^d$ is the flow. To optimize our weights, we regress the vector field \\( u_t \\), which generates our paths \\( z_t \\) by employing conditional flow matching which we reformulate as a noise-prediction objective $\\mathcal{L}_{CFM}$. Sampling is performed by using an ODE solver to solve Equation \\ref{eq:ode} in reverse time, utilizing our trained neural network \\( v_{\\Theta}(z, t) \\).\n\n\\begin{equation*}\n  \\mathcal{L}_{CFM}(x_0) = \\mathbb{E}_{t \\sim \\mathcal{U}(t), \\epsilon \\sim \\mathcal{N}(0, I)}\n  \\left[ \\frac{1}{(1-t)^2} \\Vert \\epsilon_\\theta(z_t, t) - \\epsilon \\Vert^2 \\right]\\;\n\\end{equation*}\\section{Experimental Setup}\n\\label{sec:experimentalsetup}\n\n\\paragraph{Data}\nWe train both the autoencoders and the generative models on a large web dataset of image and text pairs at 256x256 resolution. For the conditioning we use the pooled text embedding of the OpenCLIP bigG/14 model from \\cite{cherti2022reproducible}. Once the autoencoders are trained we pre-encode the entire dataset with them for improved training speed.\n\n\\paragraph{Evaluation metrics}\n\\label{sec:experimentalsetup:eval}\nSince we are in the infinite data regime, we look at the final train loss and do not compare across objectives since the losses represent different quantities. We also look at CLIP score \\citep{radford2021learning, hessel2022clipscore} and FID computed on CLIP features \\citep{sauer2021projected} based on the decoded samples $\\hat{x} = \\mathcal{D}(z)$.\n\n\\paragraph{Autoencoding}\n\\label{experimentalsetup:autoencoding}\nWe study well-established autoencoder configurations that have proven effective without special handling for each data type. We adhere to the training and architectural guidelines provided by \\cite{rombach2022highresolution}. Each autoencoder is trained with a downsampling factor \\(f=8\\), reducing \\(256 \\times 256\\) images to a \\(32 \\times 32\\) grid of latents. For continuous variants, \\(\\textbf{q}(z)\\) implements a KL penalty aiming towards the standard normal distribution \\citep{kingma2022autoencoding, rombach2022highresolution}, while for discrete variants, we utilize lookup-free quantization (LFQ) \\citep{yu2024language}. Further details on the selection of discrete regularizers are available in Appendix \\ref{appendix:discrete_regularizers}. To circumvent potential challenges associated with large vocabulary sizes, as highlighted by \\cite{yu2024language}, our LFQ-regularized autoencoder is trained with a vocabulary size of 16384~\\citep{esser2021taming}. Assessing the comparability of autoencoders is difficult since there are many variables of interest such as the (1) information capacity of the latent space; (2) compute used to train the autoencoder; (3) reconstruction quality achieved by the autoencoder. To explore the influence of these factors on the performance of generative models, we train a set of autoencoders similar to those in \\cite{esser2024scaling}, which exhibit a range of information capacities and reconstruction qualities. Additionally, we experiment with targeting specific reconstruction qualities, irrespective of other factors, by training a KL-regularized autoencoder with early stopping to match the reconstruction quality of our discrete autoencoder within a certain threshold \\(\\epsilon\\) \\footnote{Discrete autoencoders typically have worse reconstruction qualities since the information bottleneck is tighter. This can be shown by comparing $\\log(\\text{codebook size})$ to $\\text{num\\_channels} * \\text{sizeof(dtype)}$ for common values of these quantities. In our case we needed to stop at 75k steps vs. 1M for the discrete autoencoder.}. Table \\ref{tab:aereconstructiontable} provides detailed information about the autoencoders.\n\n\\aereconstructiontable\n\n\\textbf{Autoencoder ablation.} We train an L-size diffusion model on top of the latent space of each continuous autoencoder. We then evaluate the models using the metrics described in Section \\ref{sec:experimentalsetup:eval} and plot them against the number of training steps. Results are shown in Figure \\ref{fig:diffae}. We find that the autoencoder's reconstruction quality has a consistently significant impact on the FID score, while its effect on the CLIP score diminishes with larger dataset sizes, where the models tend to yield similar results. This trend likely emerges because improvements in autoencoder quality enhance perceptual reconstruction metrics similar to FID, rather than affecting language or semantic capabilities. Upon examining the number of channels in the autoencoders, our findings concur with those reported by \\cite{esser2024scaling}, indicating that leveraging larger and better latent spaces requires more compute and model capacity. Additionally, the model trained on our early-stopped autoencoder's latent space performed significantly worse than the 4-channel autoencoder, which achieves similar reconstruction quality. This confirms the importance of latent space structure for overall performance.\nBuilding on these insights, we have chosen to use the 4-channel autoencoder for our main diffusion experiments. This model most closely matches the latent space capacity and reconstruction quality of our discrete autoencoder, while also ensuring that the latent structure is adequately developed to support the diffusion model trained on it. Although more advanced autoencoders have been developed—such as those featuring increased channel counts or expanded codebook sizes—our primary focus in this study is to maintain comparability across objectives.\n\n\\diffae\n\n\\subsection{Network Architecture}\n\\label{experimentalsetup:networkarch}\n\\textbf{Backbone.} We opt for the transformer architecture as our primary network backbone, recognizing its capability to scale effectively with computational resources and its status as the state-of-the-art (SOTA) across all evaluated approaches. Configuring a transformer involves many decisions, such as choosing normalization methods, feed-forward layer configurations, positional embedding schemes, conditioning methods, and initialization strategies. Given the prohibitive cost of exploring all possible hyperparameters, we adhere to established practices in recent studies.\n\n\\textbf{Design differences.} For approaches utilizing discrete representations, we primarily follow the configurations used in the LLaMa model \\citep{touvron2023llama}, incorporating SwiGLU feed-forward layers with an expansion ratio of \\( \\frac{2}{3}4 \\) and rotary positional embeddings \\citep{su2023roformer}. An exception is made for masked token prediction, where learned positional embeddings are preferred to address positional ambiguities that degrade performance near the center of the image. For continuous representation approaches, we align with diffusion transformers \\citep{peebles2023scalable}, employing GELU feed-forward layers with an expansion ratio of 4 and learned positional embeddings. All models use QK-normalization \\citep{dehghani2023scaling} for better training stability.\n\n\\textbf{Conditioning ablation.} We choose to ablate the conditioning method, as it significantly impacts the computational cost of model operations. Adaptive layer normalization (AdaLN) \\citep{perez2017film} has shown promise in latent image synthesis for both continuous \\citep{peebles2023scalable} and discrete \\citep{tian2024visual} settings. To validate this choice in the discrete context, we conduct small-scale ablations on S-size models, comparing AdaLNzero \\citep{peebles2023scalable}, with two other common conditioning methods: prepending a projected embedding in the context of the transformer and cross-attention. The outcomes of these ablations are presented in Table \\ref{tab:condablation}, informing our choice of conditioning method for subsequent experiments.\n\n\\textbf{Compute cost.} To assess the computational cost of each model, we first standardize a set of hyperparameters across all transformers, detailed in Table \\ref{tab:tfsizetable}. We then calculate the forward pass FLOPs for a single sample (a sequence of 1024 embeddings) for each approach and model size, and present them in Table \\ref{tab:modelflopstable}. Assuming the backward pass is twice the cost of the forward pass, we compute the training FLOPs for each model as \\( (1 + 2) \\times (\\text{forward FLOPs}) \\times D \\), where \\( D \\) represents the total number of training samples.\n\n\\modelflopstable\n\n\\subsection{Training}\nEach approach also has associated training hyperparameters which past work has found to work well and for the same reasons as stated in \\ref{experimentalsetup:networkarch} we follow them. \n\n\\textbf{Optimization and conditioning.} For diffusion experiments we follow \\cite{esser2024scaling} and use a constant learning rate schedule with a maximum value of $1^{-4}$. For next and masked token prediction we use a cosine decay learning rate with a maximum value of $3^{-3}$ which decays down to $3^{-5}$. All models have a linear learning rate warmup lasting 1000 steps up to the maximum value. We use the AdamW \\citep{loshchilov2019decoupled} optimizer with $\\beta_1 =0.9$, $\\beta_2 = 0.95$, decay=0.01, and epsilon=1e-15 for improved transformer training stability \\citep{wortsman2023smallscale}. All models are trained at bf16-mixed precision \\citep{bfloat16}. We intend to use classifier free guidance (CFG) \\citep{ho2022classifierfree} during sampling so we randomly drop conditioning 10\\% of the time during training. Since its inexpensive and does not influence training, for all models, we store a copy of the model weights which gets updated every 100 training batches with an exponential moving average (EMA) using a decay factor of 0.99 and during evaluation we evaluate both sets of weights. \n\n\\textbf{Training steps.} For each objective and model size we scale to at least 250k training steps with a batch size of 512. For diffusion we decide to go up to 500k steps since constant learning rate schedule allows more flexibility with dataset size\\footnote{With a decaying learning rate, each dataset size we want to study requires a separate run from scratch whereas for constant learning rate schedules you can simply continue from a past checkpoint}. Occasionally we train models for longer to attempt to illustrate convergence or crossing points.\n\n\\subsection{Sampling}\n\\textbf{Classifier free guidance.} \\cite{ho2022classifierfree} introduced it in diffusion models as an elegant way of trading off diversity for fidelity and has been demonstrated to improve results for all approaches we consider in this study \\citep{chang2023muse, gafni2022makeascene, ho2022classifierfree}. We use it here in the form \n\\begin{equation}\nx_g = (1 + w)x_c - wx_u\n\\end{equation}\nwhere $w$ is the guidance scale. For diffusion $x$ will be the position in the denoising trajectory and for token based methods $x$ is the logit distribution at a given timestep.\n\n\\textbf{Hyperparameters.} For our diffusion models we follow \\cite{esser2024scaling} and use 50 sampling steps with a CFG scale of 5. Since the conditioning and input data is slightly different we also perform a small sweep around those parameters to confirm they are still optimal. For the token based models we could not find good resources on reasonable sampling hyperparameters so we perform small sweeps for S-size models to find the best configurations and verify the robustness of those values for larger models. Common between them, we use nucleus sampling \\citep{holtzman2020curious} with a top-p value of $0.9$ and a temperature of $1.0$. For next token prediction and masked token prediction we use CFG scales 8 and 5 respectively. For masked token prediction we perform 10 sampling steps.\\section{Results}\n\\label{sec:results}\n\\subsection{Training tradeoffs}\n\nFor all models, we measure our evaluation metrics every 50k steps of training and plot them in log scale against the log of training compute. Figure \\ref{fig:main} presents this for FID and CLIP score. There we can see that for FID, next token prediction starts out more compute efficient but scaling trends suggest that its eventually matched by diffusion. When looking at CLIP score we see that token prediction is significantly better than diffusion, implying the models generate images that follow the input prompt better. This could be a feature of using more compressed latent spaces which is supported by Figure \\ref{fig:diffae} where the 4 channel continuous autoencoder outperforms both the 8 and 16 channel autoencoder on CLIP score near the end of training.  This is also supported in Figure \\ref{fig:scalingsamples} with interpretable features like human faces emerging sooner in the token based methods. Extending a finding from \\cite{mei2024bigger}, we observe that, for all approaches studied, smaller models trained for longer often surpass larger models. In Figure \\ref{fig:fintl} we show the final training loss of each model against training compute to show that it follow similar scaling trends to what has been shown in past work on scaling deep neural networks, briefly described in Section \\ref{sec:relatedworks:scaling}. Samples from the most capable XL sized next-token prediction and diffusion models can be found in Figure \\ref{fig:bestsamples}.\n\n\\mainres\n\\fintl\n\n\\subsection{Inference tradeoffs}\n\\label{sec:results:inference}\n\\textbf{Inference cost.} We evaluated all models trained for 250k steps to understand the impact of inference FLOPs on perceptual metrics. To adjust the number of inference FLOPs for a single model, we varied the number of sampling steps, applicable only to iterative denoising methods like masked token prediction and diffusion. As shown in Figure \\ref{fig:infflops}, next-token prediction demonstrates far greater inference compute efficiency compared to other objectives. This efficiency arises because when using key-value caching, sampling N tokens autoregressively uses the same amount of FLOPs as forwarding those N tokens in parallel once. However, for iterative denoising methods, this value is multiplied by the number of sampling steps. Interestingly, despite being trained for iterative denoising, the number of steps in masked token prediction appears to have minimal impact on sample quality.\n\n\\infflops\n\n\\textbf{Sampling latency and throughput.} While next-token prediction requires much less compute per sample, the autoregressive dependency of each token causes it to be data bound when few queries are being processed in parallel which results in high latency. Conversely, bidirectional denoising approaches utilize a more parallel sampling process which, despite its high cost, facilitates low latency especially in low-volume settings with models that fit on local devices \\citep{chang2022maskgit}. For high-volume sampling, where throughput becomes more important, such as serving many users via an API, next token prediction could use a batching algorithm to maximize GPU utilization by choosing batch sizes inversely proportional to sequence lengths. The effectiveness of this method is ensured by the fact that, for next-token prediction image synthesis, all responses are the same length so you can easily plan your batches ahead. This way, for high-volume sampling, next-token prediction would enjoy the same benefits over the other approaches as presented in the cost section above but for sample throughput.\n\n\\subsection{EMA ablations}\nAmong the various training practices distinguishing these methods, the use of an exponential moving average (EMA) on the model weights stands out. In the diffusion literature \\citep{karras2024analyzing, karras2022elucidating, peebles2023scalable, esser2024scaling} EMA is an essential component of the training pipeline. In contrast, this practice has not received equivalent attention in other approaches. The differential impact of EMA is evident in Figure \\ref{fig:emainfluence}. For token-based approaches, the influence of EMA is either negligible or, in some cases, harmful, whereas for diffusion models, it is beneficial almost universally. We hypothesize that the impact of EMA may be linked to the learning rate schedule, where decaying schedules similarly minimize weight variation towards the end of training. To test this hypothesis, we conducted an ablation study on an M-sized next-token prediction and diffusion model trained over 250k steps. Our findings verify our hypothesis that EMA enhances performance under a constant learning rate schedule; however, it does not exceed the improvements seen with a cosine decay learning rate schedule. This implies that future diffusion models should consider substituting the EMA for a cosine decay learning rate schedule if they are willing to pay the cost of decreased training length flexibility. Results from this ablation study are presented in Table \\ref{tab:emaablation}. \n\n\\emainfluence\n\\emaablation\n\n\\subsection{Limitations}\n\\label{sec:limitations}\n\nOur analysis has several limitations which result from resource limitations and project scope. We only investigate pretraining whereas most production systems utilize a progression of pretraining, finetuning, and distillation stages. We do not investigate high resolution images. We only measure loss and perceptual metrics and leave out an analysis of utility for potential downstream tasks. There are many others approaches that we leave out such as other discrete diffusion approaches \\citep{austin2023structured, pernias2023wuerstchen}, causally masked token prediction \\citep{aghajanyan2022cm3}, and many more. We choose most hyperparameters by following past work instead of exhaustively sweeping to find the best configurations. And finally, we do not compare approaches using the best possible autoencoders.\n\n\\scalingsamples\\section{Conclusion}\n\\label{sec:conclusion}\n\nWe conduct a compute-controlled analysis comparing transformer-based diffusion, next-token prediction, and masked-token prediction latent image synthesis models. Our findings indicate that token based methods, led by next-token prediction, achieve superior CLIP scores, indicating greater controllability. In terms of FID, and therefore image quality, while next-token prediction is much better at low training compute scales, scaling trends suggest it is eventually matched by diffusion. We find that next token prediction has, by far, the best inference compute efficiency but this comes at the cost of high latency in low data intensity settings. Based on our findings recommend diffusion models when image quality and low latency is important; and next-token prediction for better prompt following and throughput.\n\n\\newpage\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2309.04669v3.tex",
        "arXiv-2404.02905v2.tex",
        "arXiv-2405.13218v2.tex"
    ],
    "group_id": "group_28",
    "response": "### Title: Advances in Large Language Models for Multi-Modal Tasks and Image Synthesis\n\n### Introduction\n\nThe field of Large Language Models (LLMs) has seen remarkable progress in recent years, with models like GPT-3 and LLaMA demonstrating exceptional capabilities in natural language processing tasks. These models are trained on vast amounts of text data to predict the next token in a sequence, enabling them to generate coherent and contextually relevant text. Inspired by the success of LLMs, researchers have begun to explore the potential of these models in handling multi-modal data, particularly in vision-language tasks and image synthesis. However, the traditional approach of using LLMs for vision-language tasks involves treating visual inputs as mere prompts, which limits the model's potential for learning and generating across different modalities. Similarly, in image synthesis, the autoregressive (AR) modeling paradigm has faced challenges in terms of efficiency and scalability, often lagging behind diffusion models. This paper aims to summarize three recent research papers that address these challenges by introducing novel methods to unify the representation of vision and language data, and to enhance the efficiency and scalability of AR models for image synthesis.\n\n#### Vision-Language Pre-training with Large Language Models\n\nVision-language pre-training (VLP) has been a significant area of research, aiming to extend the powerful reasoning capabilities of LLMs to process multi-modal inputs. Early works primarily focused on dual-encoder architectures with contrastive objectives to learn cross-modal aligned representations. Recent advancements have shifted towards adapter-based methods, where a pre-trained vision backbone projects visual features into the semantic space of LLMs, often treating visual inputs as prompts without any supervision. This inequitable treatment of vision and language inputs constrains the model's potential, limiting it to comprehension tasks like generating text based on images. Additionally, these methods fail to leverage the reasoning capabilities of LLMs to learn interactions across different modalities, as the responsibility is delegated to a limited number of trainable parameters in the adapter.\n\n#### Image Synthesis with Autoregressive Models\n\nIn the realm of image synthesis, autoregressive models have been less favored compared to diffusion models due to their high computational cost and inefficiency in generating high-resolution images. Traditional AR models, such as VQGAN and ViT-VQGAN, tokenize images into discrete tokens and then generate these tokens in a raster-scan order, which is similar to the process of generating text tokens. However, this approach introduces several issues, including bidirectional correlations among tokens, loss of spatial locality, and high computational complexity. To address these limitations, researchers have proposed various modifications, such as using multi-scale token maps and next-scale prediction, to improve the efficiency and performance of AR models in image synthesis.\n\n### Main Content of Each Paper\n\n#### Paper 1: Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization\n\nThis paper introduces LaVIT (Language-Vision Transformer), a novel general-purpose multi-modal foundation model that aims to unify the representation of vision and language data. LaVIT leverages the successful auto-regressive generative learning paradigm of LLMs by representing both vision and language in a unified discrete token format. The key innovation lies in the development of a dynamic visual tokenizer that translates images into sequences of discrete tokens, similar to a foreign language that LLMs can comprehend and generate. This tokenizer consists of a token selector and a token merger, which dynamically adjust the visual token sequence length based on the complexity of the image content. The token selector estimates the importance of each image patch and selects the most informative ones, while the token merger compresses the information of discarded patches onto the retained ones to ensure high-level semantics are preserved.\n\nLaVIT undergoes a two-stage pre-training procedure: in the first stage, the visual tokenizer is trained on a large dataset of images, and in the second stage, the model is trained on a combination of image-text pairs and text data to achieve both multi-modal comprehension and generation. The authors demonstrate that LaVIT outperforms existing models on a wide range of vision-language tasks, showcasing its superior capability to serve as a multi-modal generalist.\n\n#### Paper 2: Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction\n\nThis paper presents Visual AutoRegressive (VAR) modeling, a new generation paradigm that redefines autoregressive learning on images as coarse-to-fine \"next-scale prediction\" or \"next-resolution prediction.\" Unlike traditional AR models that generate tokens in a raster-scan order, VAR generates multi-scale token maps from coarse to fine scales, allowing for parallel token generation within each scale. This approach addresses several issues inherent in standard AR models, such as bidirectional correlations among tokens, loss of spatial locality, and high computational complexity.\n\nThe authors develop a multi-scale VQ autoencoder to encode images into $K$ multi-scale token maps, each at a higher resolution. During training, the transformer predicts the next higher-resolution token map conditioned on all previous ones, using a block-wise causal attention mask to ensure unidirectional dependencies. The paper also explores the impact of different components, such as adaptive layer normalization (AdaLN), top-$k$ sampling, and classifier-free guidance (CFG), on the performance and efficiency of VAR models. The results indicate that VAR significantly improves image generation quality and efficiency, surpassing diffusion models like DiT and Stable Diffusion in multiple dimensions, including image quality, inference speed, data efficiency, and scalability.\n\n#### Paper 3: Computational Tradeoffs in Image Synthesis: Diffusion, Masked-Token, and Next-Token Prediction\n\nThis paper analyzes the computational tradeoffs between popular transformer-based latent image synthesis approaches: diffusion, masked-token prediction, and next-token prediction. The authors investigate the impact of the autoencoder on generative results and train a large grid of models with different approaches, model sizes, and dataset sizes. The findings indicate that at smaller compute budgets, next-token prediction yields the best image quality but is eventually matched by diffusion. Token-based approaches achieve superior controllability, while diffusion models are more efficient in terms of inference compute. The paper also explores the impact of different training practices, such as the use of exponential moving average (EMA) on model weights, on the performance of generative models.\n\n### Commonalities and Innovations\n\nAll three papers aim to enhance the efficiency and scalability of LLMs and AR models in handling multi-modal and image synthesis tasks. They introduce novel methods to address the limitations of existing approaches, such as the inequitable treatment of vision and language inputs in VLP and the high computational cost of traditional AR models in image synthesis.\n\n- **LaVIT** introduces a dynamic visual tokenizer to represent images as discrete tokens, allowing LLMs to handle both vision and language inputs indiscriminately under a unified generative learning paradigm. This unification enables LaVIT to serve as a multi-modal generalist, achieving superior performance on vision-language tasks compared to existing models.\n- **VAR** redefines AR learning on images as \"next-scale prediction,\" generating multi-scale token maps from coarse to fine scales to improve the efficiency and performance of AR models. The multi-scale design preserves spatial locality and reduces computational complexity, making VAR competitive with diffusion models in terms of image quality and inference speed.\n- **Paper 3** provides a comprehensive analysis of the tradeoffs between different image synthesis approaches, highlighting the strengths and weaknesses of each method. The paper recommends diffusion models for applications targeting image quality and low latency, while next-token prediction is better suited for prompt following and throughput.\n\n### Comparison of Results\n\nThe three papers present different approaches to enhancing the efficiency and scalability of LLMs and AR models in multi-modal and image synthesis tasks. LaVIT and VAR both demonstrate superior performance on vision-language tasks and image synthesis, respectively, compared to existing models. LaVIT achieves state-of-the-art zero-shot performance on a wide range of vision-language tasks, while VAR surpasses diffusion models in terms of image quality, diversity, and inference speed. Paper 3 provides a detailed analysis of the tradeoffs between different image synthesis approaches, recommending diffusion models for applications targeting image quality and low latency, and next-token prediction for better prompt following and throughput.\n\n### Conclusion\n\nThe main findings of these papers highlight the potential of LLMs and AR models in handling multi-modal and image synthesis tasks. LaVIT introduces a dynamic visual tokenizer to unify the representation of vision and language data, enabling LLMs to serve as multi-modal generalists. VAR redefines AR learning on images as \"next-scale prediction,\" improving the efficiency and performance of AR models in image synthesis. Paper 3 provides a comprehensive analysis of the tradeoffs between different image synthesis approaches, recommending diffusion models for applications targeting image quality and low latency, and next-token prediction for better prompt following and throughput.\n\nThese advancements pave the way for future research directions in multi-modal learning and image synthesis. Future work could focus on further improving the efficiency and scalability of these models, as well as exploring their potential in video generation and other downstream tasks. Additionally, researchers could investigate the impact of advanced tokenizer designs and sampling techniques on the performance and efficiency of these models, potentially leading to even better results."
}