{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{\\model: Masked Generative Image Transformer}\n\n\\begin{document}\n\n\\def\\model{MaskGIT\\xspace}\n\\def\\bestfid{6.18}\n\\def\\bestis{182.1}\n\\def\\bestfidhighres{7.32}\n\n\\title{\\model: Masked Generative Image Transformer}\n\n\\author{Huiwen Chang\\quad Han Zhang\\quad Lu Jiang\\quad Ce Liu$^\\ast$ \\quad William T. Freeman\\\\\n\\vspace{1.3ex}\n{Google Research}\n}\n\n\\renewcommand*{\\thefootnote}{\\fnsymbol{footnote}}\n\\newcommand{\\todo}[1]{\\textcolor{red}{Todo:#1}}\n\\newcommand{\\huiwen}[1]{\\textcolor{red}{Huiwen: #1}}\n\\newcommand{\\han}[1]{\\textcolor{green}{Han: #1}}\n\\newcommand{\\lu}[1]{\\textcolor{blue}{Lu: #1}}\n\n\\interfootnotelinepenalty=1000000\n\n\\twocolumn[{%\n\\vspace{-5mm}\n\\maketitle\n\n\\begin{center}\n    \\newcommand{\\teaserwidth}{\\textwidth}\n    \\vspace{-8mm}\n    \\includegraphics[width=\\teaserwidth]{figures/teaser1.pdf}\n    \\vspace{-6mm}\n    \\captionof{figure}{\\textbf{Example generation by MaskGIT on image synthesis and manipulation tasks}. We show that MaskGIT is a flexible model that can generate high-quality samples on (a) class-conditional synthesis, (b) class-conditional image manipulation, \\eg replacing selected objects in the bounding box with ones from the given classes, and (c) image extrapolation. Examples shown here have resolutions 512$\\times$512, 512$\\times$512, and 512$\\times$2560 in the three columns, respectively. Zoom in to see the details.} \n    \\vspace{1mm}\n\t\\label{fig:teaser}\n\\end{center}%\n}]\n\\footnotetext{$^\\ast$ Currently affiliated with Microsoft Azure AI.}\n\n\\begin{abstract}\n\nGenerative transformers have experienced rapid popularity growth in the computer vision community in synthesizing high-fidelity and high-resolution images. The best generative transformer models so far, however, still treat an image naively as a sequence of tokens, and decode an image sequentially following the raster scan ordering (i.e. line-by-line). We find this strategy neither optimal nor efficient. This paper proposes a novel image synthesis paradigm using a bidirectional transformer decoder, which we term MaskGIT. During training, MaskGIT learns to predict randomly masked tokens by attending to tokens in all directions. At inference time, the model begins with generating all tokens of an image simultaneously, and then refines the image iteratively conditioned on the previous generation. Our experiments demonstrate that MaskGIT significantly outperforms the state-of-the-art transformer model on the ImageNet dataset, and accelerates autoregressive decoding by up to 64x. Besides, we illustrate that MaskGIT can be easily extended to various image editing tasks, such as inpainting, extrapolation, and image manipulation. \n\n\\end{abstract}\n\n\\vspace{-6mm}\n\n\\section{Introduction}\n\\label{sec:intro}\n\n\\begin{figure*}[!ht]\n    \\centering\n    \\vspace{-3mm}\n\t\\includegraphics[width=.9\\textwidth]{figures/iterative_decoding.pdf}\n    \\vspace{-3mm}\n    \\caption{\\textbf{Comparison between sequential decoding and \\model's scheduled parallel decoding.} Rows 1 and 3 are the input latent masks at each iteration, and rows 2 and 4 are samples generated by each model at that iteration. Our decoding starts with all unknown codes (marked in lighter gray), and gradually fills up the latent representation with more and more scattered predictions in parallel (marked in darker gray), where the number of predicted tokens increases sharply over iterations. \\model finishes its decoding in 8 iterations compared to the 256 rounds the sequential method takes.}\n    \\label{fig:decoding}\n\\vspace{-3mm}\n\\end{figure*}\n\nDeep image synthesis as a field has seen a lot of progress in recent years. Currently holding state-of-the-art results are Generative Adversarial Networks (GANs), which are capable of synthesizing high-fidelity images at blazing speeds. They suffer from, however, well known issues include training instability and mode collapse, which lead to a lack of sample diversity. Addressing these issues still remains open research problems.\n\nInspired by the success of Transformer~\\cite{Vaswani17attention} and GPT~\\cite{gpt3} in NLP, generative transformer models have received  growing interests in image synthesis~\\cite{chen2020imagegpt,Esser21vqgan,Razavi19vqvae2}. Generally, these approaches aim at modeling an image like a sequence and leveraging the existing autoregressive models to generate image. Images are generated in two stages; the first stage is to quantize an image to a sequence of discrete tokens (or visual words). In the second stage, an autoregressive model (e.g., transformer) is learned to generate image tokens sequentially based on the previously generated result (\\ie autoregressive decoding). Unlike the subtle min-max optimization used in GANs, these models are learned by maximum likelihood estimation. Because of the design differences, existing works have demonstrated their advantages over GANs in offering stabilized training and improved distribution coverage or diversity.\n\nExisting works on generative transformers mostly focus on the first stage, \\ie how to quantize images such that information loss is minimized, and share the same second stage borrowed from NLP. Consequently, even the state-of-the-art generative transformers~\\cite{Esser21vqgan,Ramesh21dalle} still treat an image naively as a sequence, where an image is flattened into a 1D sequence of tokens following a raster scan ordering, \\ie from left to right line-by-line (\\cf Figure~\\ref{fig:decoding}). We find this representation neither optimal nor efficient for images. Unlike text, images are not sequential. Imagine how an artwork is created. A painter starts with a sketch and then progressively refines it by filling or tweaking the details, which is in clear contrast to the line-by-line printing used in previous work~\\cite{chen2020imagegpt,Esser21vqgan}. Additionally, treating image as a flat sequence means that the autoregressive sequence length grows quadratically, easily forming an extremely long sequence--longer than any natural language sentence. This poses challenges for not only modeling long-term correlation but also renders the decoding intractable.  For example, it takes a considerable 30 seconds to generate a single image on a GPU autoregressively with 32x32 tokens.\n\nThis paper introduces a new bidirectional transformer for image synthesis called Masked Generative Image Transformer (\\model). During training, \\model is trained on a similar proxy task to the mask prediction in BERT~\\cite{Devlin19bert}. At inference time, \\model adopts a novel non-autoregressive decoding method to synthesize an image in constant number of steps. Specifically, at each iteration, the model predicts all tokens simultaneously in parallel but only keeps the most confident ones. The remaining tokens are masked out and will be re-predicted in the next iteration. The mask ratio is decreased until all tokens are generated with a few iterations of refinement. As illustrated in Figure~\\ref{fig:decoding}, \\model's decoding is an order-of-magnitude faster than the autoregresive decoding as it only takes 8 steps, instead of 256 steps, to generate an image and the predictions within each step are parallelizable. Moreover, instead of conditioning only on previous tokens in the order of raster scan, bidirectional self-attention allows the model to generate new tokens from generated tokens in all directions. We find that the mask scheduling (\\ie fraction of the image masked each iteration) significantly affects generation quality. We propose to use the cosine schedule and substantiate its efficacy in the ablation study.\n\nOn the ImageNet benchmark, we empirically demonstrate that \\model is both significantly faster (by up to 64x) and capable of generating higher quality samples than the state-of-the-art autoregressive transformer, \\ie VQGAN, on class-conditional generation with 256$\\times$256 and 512$\\times$512 resolution. Even compared with the leading GAN model, \\ie BigGAN, and diffusion model, \\ie ADM~\\cite{dhariwal2021diffusion}, \\model offers comparable sample quality while yielding more favourable diversity. Notably, our model establishes new state-of-the-arts on classification accuracy score (CAS)~\\cite{Ravuri19CAS} and on FID\\cite{FID} for synthesizing 512$\\times$512 images. To our knowledge, this paper provides the first evidence demonstrating the efficacy of the masked modeling for image generation on the common ImageNet benchmark.  \n\nFurthermore, MaskGIT's multidirectional nature makes it readily extendable to image manipulation tasks that are otherwise difficult for autoregressive models. Fig.~\\ref{fig:teaser} shows a new application of class-conditional image editing in which \\model re-generates content inside the bounding box based on the given class while keeping the context (outside of the box) unchanged. This task, which is either infeasible for autoregressive model or difficult for GAN models, is trivial for our model.\nQuantitatively, we demonstrate this flexibility by applying \\model to image inpainting, and image extrapolation in arbitrary directions.\nEven though our model is not designed for such tasks, it obtains comparable performance to the dedicated models on each task.\n\n\\section{Related Work}\n\\label{sec:related}\n\\subsection{Image Synthesis}\\label{sec:related_synthesis}\nDeep generative models~\\cite{KingmaW14, Vahdat2020nvae, goodfellow2014generative, SAGAN, Song19generative,dhariwal2021diffusion,Oord16pixelcnn, Parmar18imagetransformer} have achieved lots of successes in image synthesis tasks. GAN based methods demonstrate amazing capability in yielding high-fidelity samples~\\cite{goodfellow2014generative, biggan, Karras2019stylegan2, SAGAN, tseng2021regularizing}.\nIn contrast, likelihood-based methods, such as Variational Autoencoders (VAEs) \\cite{KingmaW14, Vahdat2020nvae}, Diffusion Models \\cite{Song19generative,dhariwal2021diffusion,ho2021cascaded} and Autoregressive Models \\cite{Oord16pixelcnn, Parmar18imagetransformer}, offer distribution coverage and hence can generate more diverse samples~\\cite{Song19generative, Vahdat2020nvae, Oord16pixelcnn}. \n\nHowever, maximizing likelihood directly in pixel space can be challenging. So instead,\nVQVAE \\cite{Oord17vqvae, Razavi19vqvae2} proposes to generate images in latent space in two stages. In the first stage, which is known as \\textbf{tokenization}, it tries to compress images into discrete latent space, and primarily consists of three components:\\vspace{-2.5mm}\n\\begin{itemize}\n    \\item an encoder $E$ that learns to tokenize images $x \\in \\mathbb{R}^{H\\times W \\times 3}$ into latent embedding $E(x)$, \\vspace{-2.5mm}\n    \\item a codebook $\\mathbf{e}_k \\in \\mathbb{R}^D, k\\in 1, 2, \\cdots, K$ which serves for a nearest neighbor look up used to quantize the embedding into visual tokens, and  \\vspace{-2.5mm}\n    \\item a decoder $G$ which predicts the reconstructed image $\\hat{x}$ from the visual tokens $\\mathbf{e}$. \n\\end{itemize}\\vspace{-1mm}\n\n\\noindent In the second stage, it first predicts the latent priors of the visual tokens using deep autoregressive models, and then uses the decoder from the first stage to map the token sequences into image pixels. Several approaches have followed this paradigm due to the efficacy of the two-stage approach. DALL-E \\cite{Ramesh21dalle} uses Transformers \\cite{Vaswani17attention} to improve token prediction in the second stage. VQGAN~\\cite{Esser21vqgan} adds adversarial loss and perceptual loss~\\cite{johnson2016perceptual, zhang2018unreasonable} in the first stage to improve the image fidelity. A contemporary work to ours, VIM~\\cite{vim2021}, proposes to use a VIT backbone ~\\cite{dosovitskiy2021vit} to further improve the tokenization stage. Since these approaches still employ an auto-regressive model, the decoding time in the second stage scales with the token sequence length. \n\n\\subsection{Masked Modeling with Bi-directional Transformers}\nThe transformer architecture~\\cite{Vaswani17attention}, was first proposed in NLP, and has recently extended its reach to computer vision~\\cite{dosovitskiy2021vit, caron2021dino}.\nTransformer consists of multiple self-attention layers, allowing interactions between all pairs of elements in the sequence to be captured.\nIn particular, BERT~\\cite{Devlin19bert} introduces the masked language modeling (MLM) task for language representation learning. The bi-directional self-attention used in BERT~\\cite{Devlin19bert} allows the masked tokens in MLM to be predicted utilizing context from both directions.\nIn vision, the masked modeling in BERT~\\cite{Devlin19bert} has been extended to image representation learning~\\cite{he2021mae, Bao2022Beit} with images quantized to discrete tokens. However, few works have successfully applied the same masked modeling to image generation~\\cite{zhang2021ufcbert} because of the difficulty in performing autoregressive decoding using bi-directional attentions.\nTo our knowledge, this paper provides the first evidence demonstrating the efficacy of masked modeling for image generation on the common ImageNet benchmark.\nOur work is inspired by bi-directional machine translation~\\cite{ghazvininejad2019maskpredict, gu2020fully, gu2017non} in NLP, and our novelty lies in the proposed new masking strategy and decoding algorithm which, as substantiated by our experiments, are essential for image generation.\n\n\\begin{figure}[!t]\n    \\centering\n\t\\includegraphics[width=\\linewidth]{figures/pipeline_figure.pdf}\n    \\caption{\\textbf{Pipeline Overview.} \\model follows a two-stage design, with 1) a tokenizer that tokenizes images into visual tokens, and 2) a bidirectional tranformer model that performs MVTM, i.e. learns to predict visual tokens masked at random.}\n    \\vspace{-3mm}\n    \\label{fig:pipeline}\n\\end{figure}\\section{Method}\n\\label{sec:method}\nOur goal is to design a new image synthesis paradigm utilizing parallel decoding and bi-directional generation.\n\nWe follow the two-stage recipe discussed in \\ref{sec:related_synthesis}, as illustrated in Figure~\\ref{fig:pipeline}. Since our goal is to improve the second stage, we employ the same setup for the first stage as in the VQGAN model~\\cite{Esser21vqgan}, and leave potential improvements to the tokenization step to future work.\n\nFor the second stage, we propose to learn a bidirectional transformer by \\textit{Masked Visual Token Modeling} (MVTM). We introduce MVTM training in \\ref{ssec:model} and the sampling procedure in \\ref{ssec:decoding}. We then discuss the key technique of masking design in \\ref{ssec:masking}. \n\n\\subsection{MVTM in Training}\n\\label{ssec:model}\n\\newcommand{\\unknownmask}[0]{{\\mathbf{M}}}\n\\newcommand{\\mathy}[0]{{\\mathbf{Y}}}\n\\newcommand{\\knownmask}[0]{{\\overline{\\mathbf{M}}}}\nLet $\\mathy=[y_i]_{i=1}^N$ denote the latent tokens obtained by inputting the image to the VQ-encoder, where $N$ is the length of the reshaped token matrix, and $\\unknownmask=[m_i]_{i=1}^N$ the corresponding binary mask. During training, we sample a subset of tokens and replace them with a special \\texttt{[MASK]} token. The token $y_i$ is replaced with \\texttt{[MASK]} if $m_i=1$, otherwise, when $m_i=0$, $y_i$ will be left intact. \n\nThe sampling procedure is parameterized by a mask scheduling function $\\gamma (r) \\in (0,1]$, and executes as follows: we first sample a ratio from $0$ to $1$, then uniformly select $\\lceil \\gamma(r) \\cdot N \\rceil$ tokens in $\\mathy$ to place masks, where $N$ is the length. The mask scheduling significantly affects the quality of image generation and will be discussed in \\ref{ssec:masking}.\n\nDenote $Y_{\\knownmask}$ the result after applying mask $\\unknownmask$ to $\\mathy$.\nThe training objective is to minimize the negative log-likelihood of the masked tokens:\n\\begin{equation}\n\\label{eq:loss}\n\\mathcal{L}_{\\text{mask}} = - \\mathop{\\mathbb{E}} \\limits_{\\mathy  \\in \\mathcal{D}} \\Big[ \\sum_{\\forall i \\in [1,N], m_i=1} \\log p(y_i| Y_{\\knownmask}) \\Big],\n\\end{equation}\n\n\\noindent Concretely, we feed the masked $Y_{\\knownmask}$ into a multi-layer bidirectional transformer to predict the probabilities $P(y_i | Y_{\\knownmask} )$ for each masked token, where the negative log-likelihood is computed as the cross-entropy between the ground-truth one-hot token and predicted token.\nNotice the key difference to autoregressive modeling: the conditional dependency in MVTM has two directions, which allows image generation to utilize richer contexts by attending to all tokens in the image.\n\n\\subsection{Iterative Decoding}\n\\label{ssec:decoding}\nIn autoregressive decoding, tokens are generated sequentially based on previously generated output. This process is not parallelizable and thus very slow for image because the image token length, \\eg 256 or 1024, is typically much larger than that of language. We introduce a novel decoding method where all tokens in the image are generated simultaneously in parallel. This is feasible due to the bi-directional self-attention of MTVM. \n\nIn theory, our model is able to infer all tokens and generate the entire image in a single pass. We find this challenging due to inconsistency with the training task. Below, the proposed iterative decoding is introduced.\nTo generate an image at inference time, we start from a blank canvas with all the tokens masked out, \\ie $Y_\\unknownmask^{(0)}$. For iteration $t$, our algorithm runs as follows: \n \n \\vspace{-2mm}\n \\begin{enumerate}\n     \\item \\textbf{Predict.}  Given the masked tokens $Y_\\unknownmask^{(t)}$ at the current iteration, our model predicts the probabilities, denoted as $p^{(t)} \\in \\mathbb{R}^{N \\times K}$, for all the masked locations in parallel.  \\\\ \\vspace{-5mm}\n     \\item \\textbf{Sample.} At each masked location $i$, we sample a token $y_i^{(t)}$ based on its prediction probabilities $p_i^{(t)} \\in \\mathbb{R}^K$ over all possible tokens in the codebook. After a token $y_i^{(t)}$ is sampled, its corresponding prediction score is used as a ``confidence\" score indicating the model's belief of this prediction. \n     For the unmasked position in $Y_\\unknownmask^{(t)}$, we simply set its confidence score to $1.0$. \\\\ \\vspace{-5mm}\n    \\item \\textbf{Mask Schedule.} We compute the number of tokens to mask according to the mask scheduling function $\\gamma$ by $n = \\lceil \\gamma(\\frac{t}{T}) N \\rceil$, where $N$ is the input length and $T$ is the total number of iterations.\n    \\item \\textbf{Mask.} We obtain $Y_\\unknownmask^{(t+1)}$ by masking $n$ tokens in $Y_\\unknownmask^{(t)}$. The mask $\\unknownmask^{(t+1)}$ for iteration $t+1$ is calculated from:\\vspace{-2mm}\n        \\begin{equation*}\n         m_{i}^{(t+1)} = \n        \\begin{cases}\n            1,  & \\text{if $c_i < {\\text{sorted}}_j(c_j)[n]$.}\\\\\n            0,  & \\text{otherwise.}\\vspace{-2mm}\n        \\end{cases},\n        \\end{equation*}\n    where $c_i$ is the confidence score for the $i$-th token.\n\n \\end{enumerate}\n\nThe decoding algorithm synthesizes an image in $T$ steps. At each iteration, the model predicts all tokens simultaneously but only keeps the most confident ones. The remaining tokens are masked out and re-predicted in the next iteration. The mask ratio is made decreasing until all tokens are generated within $T$ iterations. In practice, the masking tokens are randomly sampled with temperature annealing to encourage more diversity, and we will discuss its effect in \\ref{ssec:ablation}. Figure~\\ref{fig:decoding} illustrates an example of our decoding process. It generates an image in $T=8$ iterations, where the unmasked tokens at each iteration are highlighted in the grid, \\eg when $t=1$ we only keep 1 token and mask out the rest.\n\n\\subsection{Masking Design}\n\\label{ssec:masking}\n\nWe find that the quality of image generation is significantly affected by the masking design. We model the masking procedure by a mask scheduling function $\\gamma (\\cdot)$ that computes the mask ratio for the given latent tokens. As discussed, the function $\\gamma$ is used in both training and inference. During inference time, it takes the input of $0/T, 1/T, \\cdots, (T-1)/T$ indicating the progress in decoding. In training, we randomly sample a ratio $r$ in $[0,1)$ to simulate the various decoding scenarios.\n\nBERT uses a fixed mask ratio of 15\\%~\\cite{Devlin19bert}, \\ie, it always masks 15\\% of the tokens, which is unsuitable for our task since our decoder needs to generate images from scratch. New masking scheduling is thus needed. Before discussing specific schemes, we first examine the property of the mask scheduling function. First, $\\gamma(r)$ needs to be a continuous function bounded between $0$ and $1$ for $r \\in [0,1]$. Second, $\\gamma(r)$ should be (monotonically) decreasing with respect to $r$, and it holds that $\\gamma(0)\\rightarrow1$ and $\\gamma(1)\\rightarrow0$. \nThe second property ensures the convergence of our decoding algorithm.\n\nThis paper considers common functions and makes simple transformations so that they satisfy the properties. Figure~\\ref{fig:scheduling} visualizes these functions which are divided into three groups:\n\\begin{itemize}\n    \\vspace{-2mm}\n    \\item \\textbf{Linear function} is a straightforward solution, which masks an equal amount of tokens each time.\n    \\vspace{-2mm}\n    \\item \\textbf{Concave function} captures the intuition that image generation follows a less-to-more information flow. In the beginning, most tokens are masked so the model only needs to make a few correct predictions for which the model feel confident. Towards the end, the mask ratio sharply drops, forcing the model to make a lot more correct predictions. The effective information is increasing in this process. The concave family includes cosine, square, cubic, and exponential.\n    \\vspace{-2mm}\n    \\item \\textbf{Convex function}, conversely, implements a more-to-less process. The model needs to finalize a vast majority of tokens within the first couple of iterations. This family includes square root and logarithmic.\n    \n    \\vspace{-2mm}\n\\end{itemize}\n\nWe empirically compare the above mask scheduling functions in \\ref{ssec:ablation} and find the $\\text{cosine}$ function works the best in all of our experiments.\n\\section{Experiments}\n\\label{sec:experiments}\n\nIn this section, we empirically evaluate \\model on image generation in terms of quality, efficiency and flexibility.\nIn \\ref{ssec:class_conditional_synthesis}, we evaluate \\model on the standard class-conditional image generation tasks on ImageNet~\\cite{deng2009imagenet} 256$\\times$256 and 512$\\times$512.\nIn~\\ref{ssec:applications}, we show \\model's versatility by demonstrating its performance on three image editing tasks, image inpainting, outpainting, and editing. In~\\ref{ssec:ablation}, we verify the necessity of our design of mask scheduling. We will release the code and model for reproducible research.\n\n\\subsection{Experimental Setup}\n\\label{ssec:architecture}\n\nFor each dataset, we only train a single autoencoder, decoder, and codebook with 1024 tokens on cropped 256x256 images for all the experiments. The image is always compressed by a fixed factor of 16, \\ie from $H\\times W$ to a grid of tokens in the size of $h \\times w$, where $h$=$H / 16$ and $w$=$W/16$. We find that this autoencoder, together with the codebook, can be reused to synthesize 512$\\times$512 images.\n\nAll models in this work have the same configuration: 24 layers, 8 attention heads, 768 embedding dimensions and 3072 hidden dimensions. Our models use learnable positional embedding\\cite{Vaswani17attention}, LayerNorm\\cite{ba2016layer}, and truncated normal initialization (stddev=$0.02$). We employ the following training hyperparameters: label smoothing=$0.1$, dropout rate=$0.1$, Adam optimizer~\\cite{kingma2014adam} with $\\beta_{1}$=$0.9$ and $\\beta_{2}$=$0.96$. We use RandomResizeAndCrop for data augmentation. All models are trained on 4x4 TPU devices with a batch size of 256. ImageNet models are trained for 300 epochs while the Places2 model is trained for 200 epochs. \n\n\\subsection{Class-conditional Image Synthesis}\n\\label{ssec:class_conditional_synthesis}\n\nWe evaluate the performance of our model on class-conditional image synthesis on ImageNet 256$\\times$256 and 512$\\times$512. Our main results are summarized in Table~\\ref{tab:maintable}.\n\n\\noindent\\textbf{Quality.}\nOn ImageNet 256$\\times$256, without any special sampling strategies such as beam-search, top-k or nucleus sampling heuristics~\\cite{holtzman2019nucleus} or classifier guidance~\\cite{Razavi19vqvae2}, we significantly outperform VQGAN~\\cite{Esser21vqgan} in both Fr\\'{e}chet Inception Distance (FID)~\\cite{FID} ($\\bestfid$ vs $15.78$) and Inception Score (IS) ($\\bestis$ vs $78.3$). We also report the results with classifier-based rejection sampling in the appendix \\ref{sec:supp_diversity_comparison}. \n\nWe also train a VQGAN baseline with the same tokenizer and hyperparameters as \\model's in order to further highlight the difference between bi-directional and uni-directional transformers, and find that on both resolutions, \\model still outperforms our implemented baseline by a significant margin. \n\nFurthermore, \\model improves BigGAN's FIDs on both resolutions, achieving a new state-of-the-art on 512$\\times$512 with an FID of $7.32$.\n\n\\begin{figure}[!t]\n\t\\centering\n\t\\includegraphics[width=1.\\linewidth]{figures/runtime.pdf}\n\t\\vspace{-5mm}\n\t\\caption{\\textbf{Transformer wall-clock runtime comparison between VQGAN\\cite{Esser21vqgan} and ours.} All results are run on a single GPU. }\n\t\\label{fig:speed}\n \t\\vspace{-4mm}\n\\end{figure}{\n\\newcolumntype{N}{@{}m{0pt}@{}}\n\n\\begin{table*}[h]\n\\small\n    \\centering\n    {\\small\n    \\begin{tabular}{lc ccc ccc ccc cc}\n    \\toprule\n     {Model} & \n    \n     & {FID}  $\\downarrow$ & {IS}  $\\uparrow$ & \n     &  {Prec}  $\\uparrow$ & {Rec}   $\\uparrow$ &\n     \n     &{\\# params} & {\\# steps} &\n     \n     & \\multicolumn{2}{c}{{CAS $\\times 100$} $\\uparrow$}  \n     \\\\\n     \\cline{12-13}\n    &&  && & &&& & && {Top-1 (76.6)} & {Top-5 (93.1)} \\\\[-7pt]\n    \\textbf{ImageNet 256$\\times$256} && &&& &&& &&& &\n    \\\\[-2pt]\n    \\midrule\n    \n    DCTransformer~\\cite{nash2021generating} $^\\square$ &\n    & 36.51 & n/a &\n    & 0.36 & \\textbf{0.67} &\n    & 738M & $>$1024 &   & &\n     \\\\ \n    \n    BigGAN-deep~\\cite{biggan}  & %, 1.0 trunc. &\n    & 6.95 & \\bfseries{198.2} &\n    & \\textbf{0.87} & 0.28 &\n    & 160M & 1 & & 43.99 &67.89\n    \\\\\n    \n    Improved DDPM~\\cite{nichol2021improved}$^\\square$  &\n    & 12.26 & n/a  &\n    & 0.70 & 0.62 &\n     & 280M & 250  &  &  & \\\\\n    \n    ADM~\\cite{dhariwal2021diffusion}$^\\square$   & \n    & 10.94 & 101.0  &\n    & 0.69 & 0.63 &\n    & 554M & 250 &  &  & \\\\\n    \n\n    \n    VQVAE-2~\\cite{Razavi19vqvae2}$^\\square$ &\n    & 31.11 & $\\sim$45 &\n    & 0.36 & 0.57 &\n    & 13.5B$^\\dagger$ & 5120 & & 54.83 & 77.59 \\\\\n    \\midrule\n    VQGAN~\\cite{Esser21vqgan}$^\\square$ &\n    \n    & 15.78 & 78.3 &\n    & n/a & n/a &\n    & 1.4B & 256 &  &  &\n    \\\\\n \n    VQGAN$^\\ast$ &\n    & 18.65 & 80.4 &\n    & 0.78 & 0.26 &\n    & 227M & 256 & \n    & 53.10 & 76.18\n     \\\\   \n    \n   \n    \\bfseries{\\model (Ours)} &\n    \n    & \\bfseries{\\bestfid} & \\bestis &\n    & 0.80 & 0.51 &\n   & 227M  & \\textit{8} &\n   & \\bfseries{63.14} & \\bfseries{84.45}\n    \\\\\n\n   \n    \\bottomrule\n    \\\\[-4pt]\n    \\textbf{ImageNet 512$\\times$512} && &&& &&& &&& &\\\\[-2pt]\n    \\midrule\n    \n     BigGAN-deep~\\cite{biggan} & & 8.43 & \\bfseries{232.5} & & \\textbf{0.88} & 0.29 &\n     &160M&1& \n     &44.02&68.22\\\\\n     \n     ADM~\\cite{dhariwal2021diffusion}$^\\square$ & & 23.24 & 58.06  &&0.73& \\textbf{0.60} & \n     &559M&250&\n     && \\\\\n     \\midrule\n     VQGAN$^\\ast$ & & 26.52 & 66.8 &\n     & 0.73 & 0.31 &\n      & 227M &1024 &\n     &51.29&74.24 \\\\\n     \\bfseries{\\model (Ours)} & & \\bfseries{\\bestfidhighres} & 156.0 & &  0.78 & 0.50 & & 227M &\\textit{12}&\n     &\\textbf{63.43} & \\textbf{84.79}  \\\\\n    \\bottomrule\n    \n    \\end{tabular}\n    }\n    \\vspace{-5pt}\n    \\caption{Quantitative comparison with state-of-the-art generative models on ImageNet 256$\\times$256 and 512$\\times$512. \n    \\footnotesize{``\\# steps” refers to the number of neural network runs needed to generate a sample.  $^\\ast$ denotes the model we train with the same architecture and setup with ours; $^\\square$ denotes values taken from prior publications;  $^\\dagger$ estimated based on the pytorch implementation~\\cite{pytorch2020vqvae2}.} \n    }\n    \\label{tab:maintable}\n\n\\end{table*}\n\n\\begin{figure*}[!ht]\n    \\centering\n    \\begin{tabular}{c c c}\n     \\includegraphics[ width=\\tmpwidth]{figures/diversity/009_biggan_trun1.jpeg} & \\hspace{-3mm}\n    \\includegraphics[ width=\\tmpwidth]{figures/diversity/009_ours.jpeg} & \\hspace{-3mm}\n    \\includegraphics[ width=\\tmpwidth]{figures/diversity/009_gt.jpeg} \\\\ \n    \n \n    \\includegraphics[ width=\\tmpwidth]{figures/diversity/980_biggan_trun1.jpeg} & \\hspace{-3mm}\n    \\includegraphics[ width=\\tmpwidth]{figures/diversity/980_ours1.jpeg} & \\hspace{-3mm}\n    \\includegraphics[ width=\\tmpwidth]{figures/diversity/980_gt.jpeg} \\\\\n    \n    \n    \\includegraphics[ width=\\tmpwidth]{figures/diversity/933_biggan_trun1.jpeg} & \\hspace{-3mm}\n    \\includegraphics[ width=\\tmpwidth]{figures/diversity/933_ours.jpeg} & \\hspace{-3mm}\n    \\includegraphics[ width=\\tmpwidth]{figures/diversity/933_gt.jpeg} \\\\     \n\n    \n    BigGAN-deep (FID=$6.95$) & \\model (FID=\\bestfid) & Training Set \\\\\n    \\end{tabular}\n    \\vspace{-2mm}\n    \\caption{Sample Diversity Comparison between our proposed method \\model and BigGAN-deep~\\cite{biggan} on ImageNet 256$\\times$256. \n    The class ids of the samples from top to bottom  are {\\footnotesize{ \\texttt{009}}, \\footnotesize{\\texttt{980}} and \\footnotesize{\\texttt{993}}} respectively. Please refer to appendix for more comparisons.}\n    \\vspace{-6mm}\n    \\label{fig:diversity}\n\\end{figure*}}\n\n\\newcommand{\\tmpwidth}{0.23\\linewidth}\n\\setlength{\\tabcolsep}{1pt}\n\\begin{figure}[!t]\n    \\centering\n    \\begin{tabular}{c c c c }\n\n\t\\includegraphics[width=\\tmpwidth]{figures/editing/000193_to282_0006_input.jpeg} &\n\t\\includegraphics[width=\\tmpwidth]{figures/editing/000193_to282_0006_output.jpeg}&\n\t\\includegraphics[width=\\tmpwidth]{figures/editing/000689_to282_0000_input.jpeg} &\n\t\\includegraphics[width=\\tmpwidth]{figures/editing/000689_to282_0000_output.jpeg} \n\t\\\\\n\t\\includegraphics[width=\\tmpwidth]{figures/editing/000838_to282_0004_input.jpeg} &\n\t\\includegraphics[width=\\tmpwidth]{figures/editing/000838_to282_0004_output.jpeg} &\n\t\\includegraphics[width=\\tmpwidth]{figures/editing/000018_to282_0002_input.jpeg} &\n\t\\includegraphics[width=\\tmpwidth]{figures/editing/000018_to282_0000_output.jpeg} \n\t\n\t\\\\\n\t\n\t\\includegraphics[width=\\tmpwidth]{figures/editing/000809_to282_0002_input.jpeg} &\n\t\\includegraphics[width=\\tmpwidth]{figures/editing/000809_to282_0002_output.jpeg} &\n\t\\includegraphics[width=\\tmpwidth]{figures/editing/000985_to282_0000_0024_input.jpeg} &\n    \\includegraphics[width=\\tmpwidth]{figures/editing/000985_to282_0000_0012_output.jpeg} \n    \\\\ \n\n    \\end{tabular}\n    \\vspace{-3mm}\n    \\caption{\\textbf{Class-conditional image editing.} Given input images on the left of each pair, and a target class \"tiger cat\", \\model replaces the bounding boxed regions with tiger cats, suggesting the composition ability of our model. % The fact that the synthesized tiger cat blends in with the rest of the image seamlessly highlights the composition ability of our model. \n    }\n    \\vspace{-2mm}\n    \\label{fig:editing}\n\\end{figure}\n\n\\begin{figure}[!t]\n    \\centering\n    \\begin{tabular}{cccc}\n  \n    \\includegraphics[width=\\tmpwidth]{figures/uncrop/harley-davidson-xAHtaYIHlPI_input_0.jpeg}&\n\n    \\includegraphics[width=\\tmpwidth]{figures/uncrop/harley-davidson-xAHtaYIHlPI_output_12.jpeg}&\n    \\includegraphics[width=\\tmpwidth]{figures/uncrop/harley-davidson-xAHtaYIHlPI_output_03.jpeg}&\n    \\includegraphics[width=\\tmpwidth]{figures/uncrop/harley-davidson-xAHtaYIHlPI_output_06__.jpeg}\n\n    \\\\\n    \n    \\includegraphics[width=\\tmpwidth]{figures/uncrop/001704_input.jpeg}&\n    \\includegraphics[width=\\tmpwidth]{figures/uncrop/001704_ours_12.jpeg} &\n    \\includegraphics[width=\\tmpwidth]{figures/uncrop/001704_ours_17.jpeg} &\n    \\includegraphics[width=\\tmpwidth]{figures/uncrop/001704_ours_11.jpeg} \n    \n    \\\\\n    \\includegraphics[width=\\tmpwidth]{figures/uncrop/3996_input.jpeg}&\n    \\includegraphics[width=\\tmpwidth]{figures/uncrop/3996_output8.jpeg} &\n    \\includegraphics[width=\\tmpwidth]{figures/uncrop/3996_output1.jpeg} &\n    \\includegraphics[width=\\tmpwidth]{figures/uncrop/3996_output9.jpeg} \n   \\\\\n    \\vspace{-1mm}\n    Input  & \\multicolumn{3}{c}{------ \\model (Our Samples) ------} \\\\    \n    \\end{tabular}\n    \\vspace{-1mm}\n    \\caption{\\textbf{Inpainting and outpainting.} Given a single input image, \\model synthesizes diverse results for inpainting (first row) and outpainting in different directions (last two rows).}\n    \\label{fig:uncrop_diversity}\n    \\vspace{-3mm}\n\\end{figure}\n\n\\setlength{\\tabcolsep}{4pt}\n\\begin{table}[h]\n\\small\n    \\centering\n    \\resizebox{.92\\linewidth}{!}{\n    \\begin{tabular}{llc ccc}\n    \\toprule\n    {Task} & {Model} &\n    & {FID}  $\\downarrow$ & {IS} $\\uparrow$ & %{L1} $\\downarrow$ \n    \\\\\n    \\midrule\n    \\textit{\\textbf{Outpainting}}\n    & \n    Boundless \\cite{teterwak2019boundless}$^{\\square}$ & \n    & 35.02 & 6.15 & \n    \\\\\n     {\\footnotesize Right 50\\%} & In\\&Out \\cite{cheng2021inout}$^{\\square}$ &\n    & 23.57 & 7.18 &  \\\\\n    & InfinityGAN \\cite{lin2021infinitygan} &\n    & 10.60 & 5.57 &\n    \\\\\n    & Boundless \\cite{teterwak2019boundless} TF $^{\\blackdiamond}$ & \n    & 7.80 & 5.99 &\n    \\\\\n    & \\textbf{\\model (Ours)} $^{512}$& \n    & \\textbf{6.78} & \\textbf{11.69} &  \\\\ % [1pt]\n    \\midrule\n    \\textit{\\textbf{Inpainting}} & \n     DeepFill \\cite{yu2019free} &\n    & 11.51 & 22.55 & \\\\\n    {\\footnotesize Center 50\\%$\\times$50\\%} \n    & ICT\\cite{wan2021ict}$^\\dagger$ & \n    & 13.63 & 17.70 &  \\\\    \n    & HiFill \\cite{yi2020contextual}$^{512}$ & \n    & 16.60 & 19.93 &  \\\\\n    & CoModGAN\\cite{zhao2021comodgan}$^{512}$ &\n    & \\textbf{7.13} & 21.82 & \\\\    \n    & \\textbf{\\model (Ours)$^{512}$} &\n    & 7.92 & \\textbf{22.95} & \\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n\\vspace{-2mm}\n    \\caption{\\textbf{Quantitative Comparisons for Inpainting and Outpainting on Places2.} \\footnotesize{$^{512}$ evaluated on 512$\\times$512 samples while others evaluated on the corresponding 256$\\times$256 ones, consistent with their training; $^\\square$~taken from the prior work; $^\\dagger$~evaluated using the released model trained on a subset of Places2; $^\\blackdiamond$~evaluated using the TFHub model\\cite{boundless_tfhub}.} }\n    \\label{tab:inpaint_uncrop}\n    \\vspace{-3mm}\n\\end{table}\n\n\\noindent\\textbf{Speed.} \nWe evaluate model speed by assessing the number of steps, \\ie forward passes, each model requires to generate a sample. As shown in Table~\\ref{tab:maintable}, \\model requires the fewest steps among all non-GAN-based models on both resolutions.\n\nTo further substantiate the speed difference between \\model and autoregressive models, we perform a runtime comparison between \\model and VQGAN's decoding processes. As illustrated in Figure~\\ref{fig:speed}, \\model significantly accelerates VQGAN by $30$-$64$x, with a speedup that gets more pronounced as the image resolution (and thus the input token length) grows.\n\n\\noindent\\textbf{Diversity.} We consider Classification Accuracy Score (CAS) \\cite{Ravuri19CAS} and Precision/Recall~\\cite{KynkaanniemiKLL19} as two metrics for evaluating sample diversity, in addition to sample quality.\n\nCAS involves first training a ResNet-50 classifier\\cite{ResNet} solely on the samples generated by the candidate model, and then measuring the classifier's classification accuracy on the ImageNet validation set.\nThe last two columns in Table~\\ref{tab:maintable} present the CAS results, where the scores of the classifier trained on real ImageNet training data are included for reference (76.6\\% and 93.1\\% for the top-1 and top-5 accuracy). For image resolution 256$\\times$256,  we follow the common practice of using data augmentation RandAugment\\cite{cubuk2019randaugment}, and report the scores trained without augmentation in the appendix ~\\ref{sec:supp_diversity_comparison}. We find that \\model significantly outperforms prior work VQVAE-2 and VQGAN, establishing a new state-of-the-art of CAS on the ImageNet benchmark on both resolutions.\n\nThe Precision/Recall results in Table~\\ref{tab:maintable} show that \\model achieves better coverage (Recall) compared to BigGAN, and better sample quality (Precision) compared to likelihood-based models such as VQVAE-2 and diffusion models. Compared to our baseline VQGAN, we improve the diversity as measured by recall while slightly boosting its precision. \n\nIn contrast to BigGAN’s samples, \\model's samples are more diverse with more varied lighting, poses, scales and context as shown in Figure~\\ref{fig:diversity}. More comparisons are available in the appendix ~\\ref{sec:supp_diversity_comparison}.\n\n\\subsection{Image Editing Applications}\n\\label{ssec:applications}\nIn this subsection, we present direct applications of \\model on three image editing tasks: class-conditional image editing, image inpainting, and outpainting. All three tasks can be almost trivially translated to ones that \\model can handle if we consider the task as just a constraint on the initial binary mask $\\unknownmask$ \\model uses in its iterative decoding, as discussed in ~\\ref{ssec:decoding}. We show that without modifications to the architecture or any task-specific training, \\model is capable of generating very compelling results on all three applications. Furthermore, \\model obtains comparable performance to dedicated models on both inpainting and outpainiting, even though it is not designed specifically for either task.\n\n\\noindent\\textbf{Class-conditional Image Editing.}\nWe define a new class-conditional image editing task to showcase \\model's flexibility. In this task, the model regenerates content specified inside a bounding box on the given class while preserving the context, \\ie content outside of the box. It is infeasible for autoregressive methods due to the violation to their prediction orders.\n\nFor \\model, however, it is a trivial task if we consider the bounding box region as the input of initial mask to the iterative decoding algorithm. Figure~\\ref{fig:editing} shows a few example results. More can be found in the appendix ~\\ref{sec:supp_image_editing}.\n\nIn these examples, we observe that \\model can reasonably replace the selected object while preserving, or to some extend even completing, the context in the background. Furthermore, we find that \\model seems to be capable of synthesizing unnatural yet plausible combinations unseen in the ImageNet training set, \\eg a flying cat, cat in a soup bowl, and cat in a flower. This suggests that \\model has incidentally learned useful representations for composition, which may be further exploited in related tasks in future works.\n\n\\vspace{2mm}\n\\noindent\\textbf{Image Inpainting.}\n\\label{ssec:inpainting}\nImage inpainting or image completion is a fundamental image editing task to synthesize contents in missing regions so that the completion looks visually realistic. Traditional patch-based methods\\cite{Barnes:2009:patchmatch} work well on texture regions, while deep learning based methods\\cite{yu2019free, yi2020contextual, zhao2021comodgan, saharia2021palette, esser2021imagebart} have been demonstrated to synthesize images requiring better semantic coherence. Both approaches have been are extensively studied in computer vision.\n\nWe extend \\model to this problem by tokenizing the masked image and interpreting the inpainting mask as the initial mask in our iterative decoding. We then composite the output image by linearly blending it with the input based on the masking boundary following \\cite{cheng2021inout}.\nTo match the training of our baselines, we train \\model on the 512$\\times$512 center-cropped images from the Places2\\cite{zhou2017places} dataset. All hyperparameters are kept the same as the \\model model trained on ImageNet.\n\nWe compare \\model against common GAN-based baselines, including DeepFillv2\\cite{yu2019free} and HiFill\\cite{yi2020contextual}, on inpainting with a central 50\\% $\\times$ 50\\% mask, which are evaluated on the Places2 validation set. Table~\\ref{tab:inpaint_uncrop} summarizes the quantitative comparisons. \\model beats both DeepFill and HiFill in FID and IS by a significant margin, while achieving scores close to the state-of-the-art inpainting approach CoModGAN~\\cite{zhao2021comodgan}. We show more qualitative comparisons with CoModGAN in the appendix ~\\ref{sec:supp_inpainting_and_outpainting_comparison_with_gans}.\n\n\\vspace{2mm}\n\\noindent\\textbf{Image Outpainting.}\n\\label{ssec:outpainting}\nOutpainting, or image extrapolation, is an image editing task that has received increased attention recently. It is seen as a more challenging task than inpainting due to the fewer constraints from surrounding pixels and thus more uncertainty in the predicted regions. Our adaptation of the problem and the model used in the following evaluation is the same as in inpainting.\n\nWe compare against common GAN-based baselines, including Boundless~\\cite{teterwak2019boundless}, In\\&Out~\\cite{cheng2021inout}, InfinityGAN\\cite{lin2021infinitygan}, and CoModGAN\\cite{zhao2021comodgan} on extrapolating rightward with a 50\\% ratio. We evaluate on the image set generously provided by the authors of InfinityGAN\\cite{lin2021infinitygan} and In\\&Out\\cite{cheng2021inout}.\n\nTable~\\ref{tab:inpaint_uncrop} summarizes the quantitative comparisons. \\model beats all baselines and achieves state-of-the-art FID and IS. As the examples in Figure~\\ref{fig:uncrop_diversity} illustrate, \\model is also capable of synthesizing diverse results given the same input with different seeds. We observe that \\model completes objects and global structures particularly well, and hypothesize that this is thanks to the model learning useful representations with the global attentions in the transformer.\n\n\\subsection{Ablation Studies}\n\\setlength{\\tabcolsep}{5pt}\n\\begin{table}[!t]\n\\small\n    \\centering\n    {\\small\n    \\resizebox{0.7\\linewidth}{!}{\n        \\begin{tabular}{lc cccc}\n            \\toprule\n            {\\large$\\gamma$} &  %\\bfseries{Sampling method}  &\n              & $T$ & \\bfseries{FID}  $\\downarrow$ & \\bfseries{IS} $\\uparrow$ & \\bfseries{NLL} \\\\\n            \\midrule\n            Exponential & \n            & 8  & 7.89 & 156.3 & 4.83 \\\\\n            Cubic & \n            & 9 & 7.26 & 165.2 & 4.63  \\\\ \n            Square & \n            & 10 & 6.35 & 179.9 & 4.38  \\\\ \n            \\textbf{Cosine} & \n            & 10 & \\textbf{6.06} & \\textbf{181.5} & 4.22 \\\\    \n            Linear & \n            & 16 & 7.51 & 113.2 & 3.75  \\\\\n            Square Root &\n            & 32 & 12.33 & 99.0 & 3.34 \\\\  \n            Logarithmic  &\n            & 60 & 29.17 & 47.9 &  3.08\\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n    }\n    \\vspace{-.1cm}\n    \\caption{\\textbf{Ablation results on the mask scheduling functions.} We report the best FID, IS, and Negative Log-Likelihood loss for each candidate scheduling function. }\n    \\label{tab:ablation}\n\\vspace{-3mm}\n\\end{table}\n\\label{ssec:ablation}\nWe conduct ablation experiments using the default setting on ImageNet 256$\\times$256.\n\n\\noindent{\\textbf{Mask scheduling.}}\nA key design of \\model is the mask scheduling function used in both training and iterative decoding. We compare the functions discussed in ~\\ref{ssec:masking}, visualize them in Figure~\\ref{fig:scheduling}, and summarize the results in Table~\\ref{tab:ablation}.\n\nWe observe that concave functions generally obtain better FID and IS than linear, followed by the convex functions. While cosine and square perform similarly relative to other functions, cosine slightly edges out square in all scores, making cosine the default in our model.\n\nWe hypothesize that concave functions perform favorably because they 1) challenge training with more difficult cases (\\ie encouraging larger mask ratios), and 2) appropriately prioritize the less-to-more prediction throughout the decoding.\nThat said, over-prioritization seems to be costly as well, as shown by the cubic function being worse than square, and exponential being much worse than all other concave functions.\n\n\\begin{figure}[!t]\n    \\centering\n\\includegraphics[width=1.\\linewidth]{figures/scheduling_func.pdf}\n    \\vspace{-6mm}\n    \\caption{\\textbf{Choices of Mask Scheduling Functions} $\\gamma (\\frac{t}{T})$, and \n    \\textbf{number of iterations \\textit{T}}. On the left, we visualize seven functions we consider for $\\gamma$. On the right, we show line graphs of models' FID scores against the number of decoding iterations $T$. Among the candidates, we find that cosine achieves the best FID.}\n    \\label{fig:scheduling}\n    \\vspace{-3mm}\n\\end{figure}\n\\noindent{\\textbf{Iteration number.}}\nWe study the effect of the number of iterations ($T$) on our model by running all candidate masking functions with different $T$s. As shown in Figure~\\ref{fig:scheduling}, under the same setting, more iterations are not necessarily better: as $T$ increases, aside from the logarithmic function which performs poorly throughout, all other functions hit a ``sweet spot\" where the model's performance peaks before it worsens again. The sweet spot also gets ``delayed\" as functions get less concave. As shown, among functions that achieve strong FIDs (\\ie cosine, square, and linear), cosine not only has the strongest overall score, but also the earliest sweet spot at a total of $8$ to $12$ iterations.\nWe hypothesize that such sweet spots exist because too many iterations may discourage the model from keeping less confident predictions, which worsens the token diversity. We think further study on the masking design would be interesting for future work.\n\\section{Conclusion}\n\nIn this paper, we propose \\model, a novel image synthesis paradigm using a bidirectional transformer decoder. Trained on Masked Visual Token Modeling, \\model learns to generate samples using an iterative decoding process within a constant number of iterations. Experimental results show that \\model significantly outperforms the state-of-the-art transformer model on conditional image generation, and our model is readily extendable to various image manipulation tasks. As \\model achieves competitive performance with state-of-the-art GANs, applying our approach to other synthesis tasks is a promising direction for future work. Please see the appendix \\ref{sec:supp_limitations} for the limitations and future work.\n\n\\noindent \\textbf{Acknowledgement} \nThe authors would like to thank Xiang Kong for inspiring related works and anonymous reviewers for helpful comments.\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{simple diffusion: End-to-end diffusion for high resolution images}\n\n\\begin{document}\n\n\\twocolumn[\n\\icmltitle{simple diffusion: End-to-end diffusion for high resolution images}\n\n\\icmlsetsymbol{equal}{*}\n\n\\begin{icmlauthorlist}\n\\icmlauthor{Emiel Hoogeboom}{equal,goog}\n\\icmlauthor{Jonathan Heek}{equal,goog}\n\\icmlauthor{Tim Salimans}{goog}\n\\end{icmlauthorlist}\n\n\\icmlaffiliation{goog}{Google Research, Brain Team, Amsterdam, Netherlands}\n\n\\icmlcorrespondingauthor{Emiel Hoogeboom}{emielh@google.com}\n\n\\icmlkeywords{Machine Learning, ICML}\n\n\\vskip 0.3in\n]\n\n\\printAffiliationsAndNotice{\\icmlEqualContribution} % otherwise use the standard text.\n\n\\begin{abstract}\nCurrently, applying diffusion models in pixel space of high resolution images is difficult. Instead, existing approaches focus on diffusion in lower dimensional spaces (latent diffusion), or have multiple super-resolution levels of generation referred to as cascades. The downside is that these approaches add additional complexity to the diffusion framework.\n\nThis paper aims to improve denoising diffusion for high resolution images while keeping the model as simple as possible. The paper is centered around the research question: How can one train standard diffusion models on high resolution images, and still obtain performance comparable to these alternate approaches? \n\nThe four main findings are: 1) the noise schedule should be adjusted for high resolution images, 2) It is sufficient to scale only a particular part of the architecture, 3) dropout should be added at specific locations in the architecture, and 4) downsampling is an effective strategy to avoid high resolution feature maps. Combining these simple yet effective techniques, we achieve state-of-the-art on image generation among diffusion models without sampling modifiers on ImageNet.\\end{abstract}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=.21\\textwidth]{images/frog_sweater.jpg} \\hspace{.01\\textwidth}\n    \\includegraphics[width=.21\\textwidth]{images/owl_piano.jpg} \\\\\n    \\vspace{.24cm}\n    \\includegraphics[width=.44\\textwidth]{images/robots_chess.jpg}\n    \\caption{\\textit{A dslr photo of a frog wearing a sweater}, \\textit{An owl playing the piano, vivid, fantasy art}, and \\textit{two robots playing chess with New York in the background}. Except for the frozen text encoder, \\textit{simple diffusion} is trained end-to-end and images are generated in full pixel space.}\n    \\label{fig:frog_sweater}\n    \\vspace{-.4cm}\n\\end{figure}\n\n\\section{Introduction}\nScore-based diffusion models have become increasingly popular for data generation. In essence the idea is simple: one pre-defines a diffusion process, which gradually destroys information by adding random noise. Then, the opposite direction defines the denoising process, which is approximated with a neural network.\\vspace{.2cm}\n\nDiffusion models have shown to be extremely effective for image, audio, and video generation. However, for higher resolutions the literature typically operates on lower dimensional latent spaces (latent diffusion) \\citep{rombach2022highresolution} or divides the generative process into multiple sub-problems, for instance via super-resolution (cascaded diffusion) \\citep{ho2022cascaded} or mixtures-of-denoising-experts \\citep{balaji2022ediffi}. The disadvantage is that these approaches introduce additional complexity and usually do not support a single end-to-end training setup.\n\n\\begin{figure*}[]\n    \\centering\n    \\includegraphics[width=.9\\textwidth]{images/overview_samples.jpg}\\vspace{-.2cm}\n    \\caption{Generated images with \\textit{simple diffusion}. Importantly, each image is generated in full image space by a single diffusion model without any cascades (super-resolution) or mixtures of experts. Samples are drawn from the U-Net model with guidance scale 4.}\n    \\label{fig:overview}\n\\end{figure*}\n\nIn this paper, we aim to improve standard denoising diffusion for higher resolutions while keeping the model as simple as possible. Our four main findings are that 1) the noise schedule should be adjusted for larger images,  adding more noise as the resolution increases. 2) It is sufficient to scale the U-Net architecture on the $16 \\times 16$ resolution to improve performance. Taking this one step further is the U-ViT architecture, a U-Net with a transformer backbone. 3) Dropout should be added for improved performance, but not on the highest resolution feature maps. And finally 4) for higher resolutions, one can down-sample without performance degradation. \nMost importantly, these results are obtained using just a single model and an end-to-end training setup. After using existing distillation techniques which now only have to be applied to a single stage, the model can generate an image in 0.4 seconds.\n\n\\section{Background: Diffusion Models}\n\\label{sec:background}\n\nA diffusion model generates data by learning the reverse of a destruction process. Commonly, the diffusion process gradually adds Gaussian noise over time. It is convenient to express the process directly in the marginals $q(\\vz_t | \\vx)$ which is given by:\n\\begin{equation}\\small\n    q(\\vz_t | \\vx) = \\mathcal{N}(\\vz_t | \\alpha_t \\vx, \\sigma_t^2 \\rmI)\n\\end{equation}\nwhere $\\alpha_t, \\sigma_t \\in (0, 1)$ are hyperparameters that determine how much signal is destroyed at a timestep $t$, which can be continuous for instance $t \\in [0, 1]$. Here, $\\alpha_t$ is decreasing and $\\sigma_t$ is increasing, both larger than zero. We consider a variance preserving process, which fixes the relation between $\\alpha_t, \\sigma_t$ to be $\\alpha_t^2 = 1 - \\sigma_t^2$. Assuming the diffusion process is Markov, the transition distributions are given by:\n\\begin{equation}\\small\n    q(\\vz_t | \\vz_s) = \\mathcal{N}(\\vz_t | \\alpha_{ts} \\vz_s, \\sigma_{ts}^2 \\rmI)\n\\end{equation}\nwhere $\\alpha_{ts} = \\alpha_t / \\alpha_s$ and $\\sigma_{ts}^2 = \\sigma_t^2 - \\alpha_{t|s}^2 \\sigma_s^2$ and $t > s$.\n\n\\textbf{Noise schedule}\nAn often used noise schedule is the $\\alpha$-cosine schedule where $\\alpha_t = \\cos(\\pi t / 2)$ which under the variance preserving assumption implies $\\sigma_t = \\sin(\\pi t / 2)$. An important finding from \\citep{kingma2021vdm} is that it is the signal-to-noise ratio $\\alpha_t / \\sigma_t$ that matters, which is then $1 / \\tan(\\pi t / 2)$ or in log space $\\log \\frac{\\alpha_t}{\\sigma_t} = -\\log \\tan(\\pi t / 2)$. \n\n\\textbf{Denoising}\nConditioned on a single datapoint $\\vx$, the denoising process can be written as:\n\\begin{equation}\\small\n    q(\\vz_s | \\vz_t, \\vx) = \\mathcal{N}(\\vz_t | \\vmu_{t \\to s}, \\sigma_{t \\to s}^2 \\rmI).\n\\end{equation}\nwhere $\\vmu_{t \\to s} = \\frac{\\alpha_{ts} \\sigma_s^2}{\\sigma_t^2} \\vz_t + \\frac{\\alpha_s \\sigma_{ts}^2}{\\sigma_t^2} \\vx$ and $ \\sigma_{t \\to s} = \\frac{\\sigma_{ts}^2 \\sigma_{s}^2}{\\sigma_t^2}$. An important and surprising result in literature is that when $\\vx$ is approximated by a neural network $\\hat{\\vx} = f_\\theta(\\vz_t)$, then one can define the learned distribution $p(\\vz_s | \\vz_t) = q(\\vz_s | \\vz_t, \\vx = \\hat{\\vx})$ without loss of generality as $s \\to t$. This works because as $s \\to t$, the true denoising distribution for all datapoints $q(\\vz_s | \\vz_t)$ (which is typically unknown) will become equal to $q(\\vz_s | \\vz_t, \\vx = \\mathbb{E}[\\vx | \\vz_t])$ \\citep{song2021scorebasedsde}.\n\n\\textbf{Parametrization}\nThe network does not need to approximate $\\hat{\\vx}$ directly, and experimentally it has been found that other predictions produce higher visual quality. Studying the re-parametrization of the marginal $q(\\vz_t | \\vx)$ which is $\\vz_t = \\alpha_t \\vx + \\sigma_t \\veps_t$ where $\\veps_t \\sim \\mathcal{N}(0, 1)$, one can for instance choose the \\textit{epsilon} parametrization where the neural net predicts $\\hat{\\veps}_t$. To obtain $\\hat{\\vx}$, one computes $\\hat{\\vx} = \\vz_t / \\alpha_t - \\sigma_t \\hat{\\veps}_t / \\alpha_t$. The problem with the epsilon parametrization is that it gives unstable sampling near $t = 1$. An alternative parametrization without this issue is called \\textit{v prediction} and was proposed in \\citep{salimans2022progressive}, it is defined as $\\hat{\\vv}_t = \\alpha_t \\hat{\\veps}_t - \\sigma_t \\hat{\\vx}$.\n\nNote that given $\\vz_t$ one can obtain $\\hat{\\vx}$ and $\\hat{\\veps}_t$ via the identities $\\sigma_t \\vz_t + \\alpha_t \\hat{\\vv}_t = (\\sigma_t^2 + \\alpha_t^2) \\hat{\\veps}_t = \\hat{\\veps}_t $ and  $\\alpha_t \\vz_t - \\sigma_t \\hat{\\vv}_t = (\\alpha_t^2 + \\sigma_t^2) \\hat{\\vx} = \\hat{\\vx}$. In initial experiments we found \\textit{v prediction} to train more reliably, especially for larger resolutions, and therefore we use this parametrization throughout this paper.\n\n\\begin{figure*}\n    \\centering\n    \\includegraphics[interpolate=false,width=.7\\textwidth]{images/diffusion_shifted.pdf}\n    \\includegraphics[width=.235\\textwidth]{images/plots_shifted.pdf}\n    \\vspace{-.2cm}\n    \\caption{The standard and shifted diffusion noise on an image of $512 \\times 512$, that is visualized by average pooling to a resolution of $64 \\times 64$. The top row shows a conventional cosine schedule, the bottom row shows our proposed shifted schedule.}\n    \\label{fig:diffusion_shifted}\n\\end{figure*}\n\n\\textbf{Optimization}\nTo train the model, we use the standard epsilon loss from \\citep{ho2020denoising}. A way to motivate this choice of loss, is that using variational inference one can derive a lowerbound (in continuous time) on the model log-likelihood as done in \\citep{kingma2021vdm}:\n\\small\\begin{align*}\n\\begin{split}\n   \\log p(\\vx) &= \\log \\mathbb{E}_{q} \\frac{p(\\vx, \\vz_0, \\ldots, \\vz_1)}{q(\\vz_0, \\ldots, \\vz_1 | \\vx)} \\geq \\mathbb{E}_{q} \\log \\frac{p(\\vx, \\vz_0, \\ldots, \\vz_1)}{q(\\vz_0, \\ldots, \\vz_1 | \\vx)} \\\\\n   &= \\mathcal{L}_x + \\mathcal{L}_T - \\mathbb{E}_{t \\sim \\mathcal{U}(0, 1)} \\Big{[} w(t) ||\\veps_t - \\hat{\\veps}_t ||^2 \\Big{]}, \n\\end{split}\n\\end{align*}\\normalsize\nwhere for a well-defined process $\\mathcal{L}_x = -\\log p(\\vx | \\vz_0) \\approx 0$ for discrete $\\vx$, $\\mathcal{L}_T = -\\mathrm{KL}(q(\\vz_T | \\vx) | p(\\vz_T)) \\approx 0$, and where $w(t)$ is a weighting function which for the equation to be true needs to be $w(t) = - \\frac{\\mathrm{d}}{\\mathrm{d}t}\\log \\mathrm{SNR}(t)$ where $\\mathrm{SNR}(t) = \\alpha_t^2 / \\sigma_t^2$. In practice, we generally use the unweighted loss on $\\veps_t$ (meaning that $w(t) = 1$) which in \\citep{ho2020denoising} was found to give superior sample quality. See Appendix~\\ref{sec:addition_info_diffusion} for additional useful background information.\n\n\\section{Method: simple diffusion}\nIn this section, we introduce several modifications that enable denoising diffusion to work well on high resolutions.\n\n\\subsection{Adjusting Noise Schedules}\nOne of the modifications is the noise schedule that is typically used for diffusion models. The most common schedules is the $\\alpha$-cosine schedule, which under the variance preserving assumption amounts to $\\frac{\\sigma_t}{\\alpha_t} = \\tan(\\pi t / 2)$ (ignoring the boundaries around $t=0$ and $t = 1$ for this analysis) \\citep{nichol2021improvedddpm}. This schedule was originally proposed to improve the performance on CIFAR10 which has a resolution of $32 \\times 32$ and ImageNet $64 \\times 64$. \n\nHowever, for high resolutions not enough noise is added. For instance, inspecting the top row of Figure~\\ref{fig:diffusion_shifted} shows that for the standard cosine schedule, the global structure of the image is largely defined already for a wide range in time. This is problematic because the generative denoising process only has a small time window to decide on the global structure of the image. We argue that for higher resolutions, this schedule can be changed in a predictable way to retain good visual sample quality.\n\nTo illustrate this need in more detail, let us study a $128 \\times 128$ problem. Given an input image $\\vx$ the diffusion distribution for pixel $i$ is given by $q(z_t^{(i)} | \\vx) = \\mathcal{N}(z_t^{(i)} | \\alpha_t x_i, \\sigma_t)$. Commonly, diffusion models use network architectures that use \\textit{downsampling} to operate on lower resolution feature maps, in our case with average pooling. Suppose we average pool $\\vz_t$, where we let indices $1, 2, 3, 4$ denote the pixels in a $2 \\times 2$ square that is being pooled. This new pixel is $z^{64 \\times 64}_t = (z_t^{(1)} + z_t^{(2)} + z_t^{(3)} + z_t^{(4)}) / 4$. Recall that for variance of independent random variables is additive meaning that $\\mathrm{Var}[X_1 + X_2] = \\mathrm{Var}[X_1] + \\mathrm{Var}[X_2]$ and that $\\mathrm{Var}[aX] = a^2\\mathrm{Var}[X]$ for a constant $a$. Letting $x^{64 \\times 64}$ denote the first pixel of the average pooled input image, we find that $z^{64 \\times 64}_t \\sim \\mathcal{N}(\\alpha_t x^{64 \\times 64}, \\sigma_t / 2)$. The lower resolution pixel $z^{64 \\times 64}_t$ only has half the amount of noise. We hypothesize that as resolutions increase this is problematic, as much less diffusion time is spent on the lower resolution, a stage at which the global consistency is generated.\n\n\\begin{figure*}\n\\centering\n\\begin{subfigure}[t]{0.33\\textwidth}\n\\centering\n\\includegraphics[width=.99\\textwidth]{images/tti_256/dog_cycling.jpg}\n\\caption{\\textit{A dog riding a bicycle through Amsterdam}}\n\\end{subfigure} \\hfill\n\\begin{subfigure}[t]{0.33\\textwidth}\n\\centering\n\\includegraphics[width=.99\\textwidth]{images/tti_256/car_desert.jpg}\n\\caption{\\textit{A futuristic car driving through the desert}}\n \\end{subfigure} \\hfill\n\\begin{subfigure}[t]{0.33\\textwidth}\n\\centering\n\\includegraphics[width=.99\\textwidth]{images/tti_256/distillation_gold.jpg}\n\\caption{\\textit{A distillation machine on a table creating gold}}\n\\end{subfigure} \\hfill\n\\begin{subfigure}[t]{0.33\\textwidth}\n\\includegraphics[width=.99\\textwidth]{images/tti_256/elephant_abstract.jpg}\n\\caption{\\textit{An abstract painting of an elephant}}\n\\end{subfigure} \\hfill\n\\begin{subfigure}[t]{0.33\\textwidth}\n\\centering\n\\includegraphics[width=.99\\textwidth]{images/tti_256/city_overgrown.jpg}\n\\caption{\\textit{A futuristic city overgrown by nature}}\n\\end{subfigure} \\hfill\n\\begin{subfigure}[t]{0.33\\textwidth}\n\\centering\n\\includegraphics[width=.99\\textwidth]{images/tti_256/lion_vangogh.jpg}\n\\caption{\\textit{A Van Gogh painting of a lion}}\n\\end{subfigure}\n\\begin{subfigure}[t]{0.33\\textwidth}\n\\centering\n\\includegraphics[width=.99\\textwidth]{images/tti_256/sphere_city.jpg}\n\\caption{\\textit{A city inside a glass pearl}}\n\\end{subfigure} \\hfill\n\\begin{subfigure}[t]{0.33\\textwidth}\n\\centering\n\\includegraphics[width=.99\\textwidth]{images/tti_256/horse_hat.jpg}\n\\caption{\\textit{A horse wearing a hat}}\n\\end{subfigure} \\hfill\n\\begin{subfigure}[t]{0.33\\textwidth}\n\\centering\n\\includegraphics[width=.99\\textwidth]{images/tti_256/balloon_brain.jpg}\n\\caption{\\textit{A balloon in the shape of the Google Brain logo}}\n\\end{subfigure}\n\\caption{Text to image samples at resolution $256 \\times 256$, generated by a single stage diffusion model}\n\\end{figure*}\n\nOne can further derive that the $\\alpha_t$ to $\\sigma_t$ ratio at this lower resolution is twice as high, meaning that the signal to noise ratio is $2^2$ as high. And so $\\mathrm{SNR}^{64 \\times 64}(t) = \\mathrm{SNR}^{128\\times 128}(t) \\cdot 2^2$, or in general:\n\\begin{equation}\\small\n    \\mathrm{SNR}^{d / s \\times d / s}(t) = \\mathrm{SNR}^{d \\times d}(t) \\cdot s^2\n\\end{equation}\nIn summary, after averaging over a window of size $s \\times s$, the ratio $\\alpha_t$ to $\\sigma_t$ increases by a factor $s$ (and thus the $\\mathrm{SNR}$ by $s^2$). Hence, we argue that the noise schedule could be defined with respect to some reference resolution, say $32 \\times 32$ or $64 \\times 64$ for which the schedules were initially designed and successfully tested. \nIn our approach one first chooses a reference resolution, for example $64 \\times 64$ (a reasonable choice as we will see empirically). At the reference resolution we define the noise schedule $\\mathrm{SNR}^{64 \\times 64}(t) = 1 / \\tan(\\pi t / 2)^2$ which in turn defines the desired $\\mathrm{SNR}$ at full resolution $d \\times d$:\n\\begin{equation}\n    \\mathrm{SNR}_{\\mathrm{shift}\\, 64}^{d \\times d}(t) = \\mathrm{SNR}^{64 \\times 64}(t) \\cdot (64 / d)^2,\n\\end{equation}\nthe signal to noise ratio is simply multiplied by $(64 / d)^2$, which for our setting $d > 64$ reduces the signal-to-noise ratio at high resolution. In log-space, this implies a simple shift of $2 \\cdot \\log (64 / d)$ (see Figure~\\ref{fig:diffusion_shifted_snr}). For example, the equation of a noise schedule for images of 128 $\\times$ 128 and a reference resolution of 64 the schedule is:\n\\begin{equation*}\n    \\log \\mathrm{SNR}_{\\mathrm{shift} \\, 64}^{128 \\times 128}(t) = - 2 \\log \\tan (\\pi t / 2) + 2 \\log (64 / 128).\n\\end{equation*}\nRecall that under a variance preserving process, the diffusion parameters can be computed as $\\alpha^2_t = \\mathrm{sigmoid}(\\log \\mathrm{SNR}(t))$ and $\\sigma^2_t = \\mathrm{sigmoid}(-\\log \\mathrm{SNR}(t))$.\n\nFinally, it may be worthwhile to study the concurrent and complementary work \\citep{chen2023importancenoise} which also analyzes adjusted noise schedules for higher resolution images and describes several other improvements as well.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=.3\\textwidth]{images/plot_logsnr.pdf}\\vspace{-.1cm}\n    \\caption{Log signal to noise ratio for the original and shifted cosine schedule.}\n    \\label{fig:diffusion_shifted_snr}\n    \\vspace{-.35cm}\n\\end{figure}\n\n\\paragraph{Interpolating schedules}\nA potential downside of shifting the schedule is that high frequency details are now generated much later in the diffusion process due to the increased per-pixel noise. However, we postulate that high-frequency details are weakly correlated when conditioning on the global/low-frequency features that are already generated. It should therefore be possible to generate the high-frequency details in few diffusion steps. Alternatively, one can \\textit{interpolate} different shift schedules, for example for a resolution of $512$ one could include higher frequency details by starting at shift 32 and interpolating in log-space to shift 256. The schedule for $\\log \\mathrm{SNR}_{\\mathrm{interpolate} (32 \\to 256)}(t)$ equals:\n\\begin{equation}\\small\n     t \\log \\mathrm{SNR}_{\\mathrm{shift} \\, 256}^{512 \\times 512}(t) + (1 - t) \\log \\mathrm{SNR}_{\\mathrm{shift} \\, 32}^{512 \\times 512}(t)\n\\end{equation}\nwhich has more equal weighting over low, mid and high frequency details. When sampling guidance is desired (for example in our text to image experiments) we recommend using this interpolated schedule. We found that shifted schedules can only tolerate little guidance, and interpolated schedules get better results with higher guidance weights.\n\n\\subsection{Multiscale training loss}\nIn the last section we argued that the noise schedule of our diffusion model should be adjusted when training on high resolution images so that the signal-to-noise ratio at our base resolution is held constant. However, even when adjusting the noise schedule in this way, the training loss on images of increasingly high resolution is dominated by high frequency details. To correct for this we propose replacing the standard training loss by a multiscale version that evaluates the standard training loss at downsampled resolutions with a weighting factor that increases for the lower resolutions. We find that the multiscale loss enables quicker convergence especially at resolutions greater than $256 \\times 256$. The training loss at the $d \\times d$ resolution can be written as:\n\\[L^{d \\times d}_{\\theta}(\\vx) = \\frac{1}{d^{2}}\\mathbb{E}_{\\veps,t} \\lVert \\text{D}^{d \\times d}[\\veps] - \\text{D}^{d \\times d}[\\hat{\\veps}_{\\theta}(\\alpha_t \\vx+\\sigma_t \\veps, t)] \\rVert_{2}^{2},\\]\nwhere $\\text{D}^{d \\times d}$ denotes downsampling to the $d \\times d$ resolution. If this resolution is identical to the native resolution of our model $\\hat{\\veps}_{\\theta}$ and data $\\vx$, the downsampling does not do anything and can be removed from this equation. Otherwise, $\\text{D}^{d \\times d}[\\hat{\\veps}_{\\theta}]$ can be considered as an adjusted denoising model for data at non-native resolution $d \\times d$. Since downsampling an image is a linear operation, we have that $\\text{D}^{d \\times d}[\\mathbb{E}(\\veps|\\vx)] = \\mathbb{E}(\\text{D}^{d \\times d}[\\veps]|\\vx)$, and this way of constructing the lower-resolution model is thus indeed consistent with our original model.\n\nWe then propose training our high resolution model against the multiscale training loss comprising of multiple resolutions. For instance for the resolutions $32, 64, \\ldots, d$ the loss would be: \n$\\tilde{L}^{d \\times d}_{\\theta}(\\vx) = \\sum_{s \\in \\{32, 64, 128, \\ldots, d\\}} \\frac{1}{s} L^{s \\times s}_{\\theta}(\\vx)$.\n\nThat is, we train against a weighted sum of training losses for resolutions starting at a base resolution (in this case $32 \\times 32$) and always including the final resolution of $d \\times d$. We find that losses for higher resolution are noisier on average, and we therefore decrease the relative weight of the loss as we increase the resolution.\n\n\\subsection{Scaling the Architecture}\nAnother question is how to scale the architecture. Typical model architectures half the channels each time the resolution is doubled such that the flops per operation is the same but the number of features doubles. The computational intensity (flops / features) also halves each time the resolution doubles. Low computational intensity leads to poor utilization of the accelerator and large activations result in out-of-memory issues. As such, we prefer to scale on the lower resolutions feature maps. Our hypothesis is that mainly scaling on a particular resolution, namely the $16 \\times 16$ resolution is sufficient to improve performance within a range of network sizes we consider. Typically, low resolution operations have relatively small feature maps. To illustrate this, consider for example\n\\begin{equation*} \\small\n1024 \\text{ (batch) } \\times 16 \\times 16 \\times 1024 \\text{ (channel)} \\cdot 2 \\text{ bytes} / \\text{dim}\n\\end{equation*}\ncosts $0.5$ GB for a feature map whereas for a $256 \\times 256$ feature map with $128$ channels, a feature map costs $16$ GB, given they are stored in a 16 bit float format. \n\nParameters have a smaller memory footprint: The typical size of a convolutional kernel is $3^2 \\times 128^2  \\text{ dimensions} \\cdot 4 \\text{ bytes} / \\text{dims} \\cdot 5 \\text{ replications } = 2.8$MB and $180$MB for $1024$ channels, with $5$ replications for the gradient, optimizer state and exponential moving average. The point is, at a resolution of $16 \\times 16$ both the size of feature maps are manageable at $16^2$ and the required space for the parameters is manageable. Summarizing this back-of-the-envelope calculation in Table~\\ref{tab:memory_computation} one can see that for the same memory constraint, one can fit $16$GB $/$ $0.7$GB $\\approx 23$ layers at $16 \\times 16$ versus only $1$ at $256 \\times 256$.\n\n\\begin{table}\n    \\centering\n    \\caption{Memory and compute for a convolutional layer at the typical sizes encountered in diffusion architectures. Using more channels is usually much cheaper at lower resolutions in terms of memory, $B=1024$ for this example.}\n    \\label{tab:memory_computation}\n    \\scalebox{.8}{\n    \\begin{tabular}{l l l}\n    \\toprule\n    Size & ($B \\times 256^2 \\times 128)$ & ($B \\times 16^2 \\times 1024)$  \\\\ \\midrule\n    Conv Kernel Memory & 2.8MB & 180MB \\\\\n    Feature Map Memory & 16GB & 0.5GB \\\\ \n    Total Memory & 16GB & 0.7GB \\\\  \\midrule\n    Compute (TFLOPS) & 9 & 2.3 \\\\\n        \\bottomrule\n    \\end{tabular}}\\vspace{-.4cm}\n\\end{table}\n\nOther reasons to choose this resolution is because it is the one at which self-attention starts being used in many existing works in the diffusion literature \\citep{ho2020denoising,nichol2021improvedddpm}. Furthermore, it is the $16 \\times 16$ resolution at which vision transformers for classification can operate successfully \\citep{dosovitskiy2021imageisworth}. Although this may not be the ideal way to scale the architecture, we will show empirically that scaling the $16 \\times 16$ level works well.\n\nAn observant ML practitioner may have realized that when using multiple devices naively, parameters are replicated (typical in JAX and Flax) or stored on the first device (PyTorch). Both cases result in a situation where the memory requirements per device for the feature maps decreases with $1 / \\text{devices}$ as desired, but the parameter requirement is unaffected and requires a lot of memory. We scale mostly at a low resolution where activations are relatively small but parameter matrices are large $O(\\text{features}^2)$. We found that sharding the weights allows us to scale to much larger models without requiring more complicated parallelization approaches like model parallelism.\n\n\\paragraph{Avoiding high resolution feature maps}\nHigh resolution feature maps are memory expensive. If the number of FLOPs is kept constant, memory still scales linearly with the resolution.\n\nIn practise, it is not possible to decrease the channels beyond a certain size without sacrificing accelerator utilization. Modern accelerators have a very high ratio between compute and memory bandwidth. Therefore, a low channel count can make operation memory bound, causing a mostly idling accelerator and worse than expected wall-clock performance.\n\nTo avoid doing computations on the highest resolutions, we down-sample images immediately as a the first step of the neural network, and up-sample as the last step. Surprisingly, even though the neural networks are cheaper computationally and in terms of memory, we find empirically that they also achieve better performance. We have two approaches to choose from.\n\nOne approach is to use the invertible and linear 5/3 wavelet (as used in JPEG2000) to transform the image to lower resolution frequency responses as demonstrated in Figure~\\ref{fig:dwt}. Here, the different feature responses are concatenated spatially for visual purposes. In the network, the responses are concatenated over the channel axis. When more than one level of DWT is applied (here there are two), then the responses differ in resolution. This is resolved by finding the lowest resolution (in the figure $128^2$) and reshaping pixels for the higher resolution feature maps, in the case of $256^2$ they are reshaped $128^2 \\times 4$, as a typical space to depth operations. A guide on the implementation of the DWT can be found here\\footnote{\\url{http://trueharmoniccolours.co.uk/Blog/?p=14}}.\n\nIf the above seems to complicated, there also exists a simpler solution if one is willing to pay a small performance penalty. As a first layer one can use a $d \\times d$ convolutional layer with stride $d$, and an identically shaped \\textit{transposed} convolutional layer as a last layer. This is equivalent to what is called patching in transformer literature. Empirically we show this performs similarly, albeit slightly worse.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.235\\textwidth]{images/tiger_gray.jpg}\\hfill\n    \\includegraphics[width=0.235\\textwidth]{images/dwt.jpg}\n    \\caption{The `5/3' DWT transform transforms an image to low and high frequency responses. Left: original image. Right: The different frequency responses of a two-level DWT, outputs are four $128 \\times 128$ maps and three $256 \\times 256$ maps. Best viewed electronically.} \\vspace{-.6cm}\n    \\label{fig:dwt}\n    \\vspace{-.2cm}\n\\end{figure}\n\n\\subsection{Dropout}\nIn architecture typically used in diffusion, a global dropout hyperparameter is used for the residual blocks, at all resolutions. In CDM \\citep{ho2022cascaded}, dropout is used to generate images at lower resolutions. For the conditional higher resolution images, no dropout is used. However, various other forms of augmentation are performed on the data. This indicates that regularization is important, even for models operating on high resolutions. However, as we will demonstrate empirically, the naive method of adding dropout in all residual blocks does not give desired results. \n\nSince our network design only scales the network size at lower resolutions, we hypothesize that it should be sufficient to only add dropout add the lower resolutions. This avoids regularizing the high resolution layers which are memory-wise expensive, while still using the dropout regularization that has been successful for models trained on lower resolution images.\n\n\\subsection{The U-ViT architecture}\nTaken the above described changes to the architecture one step further, one can replace convolutional layers with MLP blocks if the architecture already uses self-attention at that resolution. This bridges the transformers for diffusion introduced by \\citep{peebles2022scalable} with U-Nets, replacing its backbone with a transformer. Consequently, this relatively small change means that we now are using transformer blocks at these resolutions. The main benefit is that the combination of self-attention and MLP blocks has high accelerator utilization, and thus large models train somewhat faster. See Appendix~\\ref{app:experimental_details} for details regarding this architecture. In essence, this U-Vision Transformer (U-ViT) architecture can be seen as a small convolutional U-Net which through multiple levels down-samples to the $16 \\times 16$ resolution. At this stage a large transformer is applied after which the upsampling is again done via the convolutional U-Net.\n\n\\subsection{Text to image generation}\nAs a proof of concept, we also train a simple diffusion model conditioned on text data. Following \\citep{saharia2022imagen} we use the T5 XXL \\citep{raffel2020exploring} text encoder as conditioning. For further details see Appendix~\\ref{app:experimental_details}. We train three models: One on images of resolution $256 \\times 256$ for a direct comparison to models in literature, one on $512 \\times 512$ and one on $384 \\times 640$. For the last, non-square resolution, images are rotated during prepossessing if their width is smaller than their height, along which a `portrait mode' flag is set to true. As a result, this model can generate natively in a 5:3 aspect ratio for both landscape and portrait orientation.\n\\section{Related Work}\nScore-based diffusion models \\citep{sohldickstein2015diffusion,song2019generativemodellingestimatinggradient,ho2020denoising} are a generative model that pre-defines a stochastic destruction process. The generative process is learned by approximating the reverse process with the help of neural networks. \n\nDiffusion models have been succesfully applied to image generation \\citep{ho2020denoising,ho2022cascaded}, speech generation \\citep{chen2020wavegrad,kong2021diffwave}, video generation \\citep{singer2022makeavideo,saharia2022imagen}. Other types of generative models have also been successfully applied to image generation \\citep{chang2022maskgit,sauer2022styleganxl,anonymous2023discrete}, although modifications such as guidance and low temperature sampling can make it difficult to compare these models fairly.\nDiffusion models for high resolutions (for example $512^2, 256^2, 128^2$) on complicated data (such as ImageNet) are generally not learned directly. Instead, approaches in literature divide the generative process into sub-problems via super-resolution \\citep{ho2022cascaded}, or mixtures-of-denoisers \\citep{feng2022ernievilg,balaji2022ediffi}. Alternatively, other approaches project high resolution data down to a lower dimensional latent space \\citep{rombach2022highresolution}. Although this sub-division makes optimization easier, the engineering complexity increases: Instead of dealing with a single model, one needs to train and keep track of multiple models. In \\citep{gu2022fdm} a different approach to adapt noise to resolution is proposed, although this method seems to generate lower quality samples with a more complicated scheme. We show that it is possible to train a single denoising diffusion model for resolutions up to $512 \\times 512$ with only a small number modifications with respect to the original (modern) formulation in \\citep{ho2020denoising}.\n\n\\section{Experiments}\n\\label{sec:results}\n\n\\subsection{Effects of the proposed modifications}\n\n\\begin{table}\n    \\centering\n    \\caption{Noise Schedule on ImageNet 128 and 256.}\\vspace{-.2cm}\n    \\label{tab:i128_i256_noise_schedule}\n    \\scalebox{.9}{\n    \\begin{tabular}{l l l}\n    \\toprule\n    Noise Schedule & FID train & FID eval \\\\ \\midrule\n    \\textbf{128 $\\times$ 128 resolution} \\\\\n    cosine (original at 128) & 2.96 & 3.38 \\\\\n    cosine (shifted to 64) & 2.41 & 3.03 \\\\\n    cosine (shifted to 32) & \\textbf{2.26} & \\textbf{2.88} \\\\ \\midrule\n    \\textbf{256 $\\times$ 256 resolution} \\\\\n    cosine (original at 256) & 7.65 & 6.87 \\\\\n    cosine (shifted to 128) & 5.05 & 4.74 \\\\\n    cosine (shifted to 64) & 3.94 & 3.89 \\\\\n    cosine (shifted to 32) & \\textbf{3.76} & \\textbf{3.71} \\\\\n        \\bottomrule\n    \\end{tabular}}\\vspace{-.4cm}\n\\end{table}\n\n\\begin{table}\n    \\centering\n    \\caption{Dropout Ablation on ImageNet 128}\\vspace{-.2cm}\n    \\label{tab:dropout}\n    \\scalebox{.9}{\n    \\begin{tabular}{l r r r r}\n    \\toprule\n    Starting from Resolution & FID train & FID eval \\\\ \\midrule\n    128 & 3.19 & 3.85 \\\\\n    64 & \\textbf{2.27} & \\textbf{2.85} \\\\\n    32 & 2.31 & 2.87 \\\\\n    16 & 2.41 & 3.03 \\\\\n    no dropout (at 700K iters) & 3.74 & 3.91 \\\\\n        \\bottomrule\n    \\end{tabular}} \\vspace{-.4cm}\n\\end{table}\n\n\\textbf{Noise schedule}\nIn this experiment it is studied how the noise schedule effects the quality of generated images, evaluated on FID50K score on both train and eval data splits. Recall that our hypothesis was that the cosine schedule does not add sufficient noise, but can be adjusted by `shifting' its log SNR curve using the ratio between the image resolution and the noise resolution. In these experiments, the noise resolution is varied from the original image resolution (corresponding to the conventional cosine schedule) all the way down to $32$ by factors of two.\n\nAs can be seen in Table~\\ref{tab:i128_i256_noise_schedule} for ImageNet at resolution 128 $\\times$ 128 and resolution 256 $\\times$ 256, shifting the noise schedule considerably improves performance. The difference is especially noticeable at the higher resolution, where the difference is 7.65 for the original cosine schedule against 3.76 for the shifted schedule in FID on the train data. Notice that the difference in performance between the shift towards either 64 and 32 is relatively small, albeit slightly better for the 32 shift. Given that the difference is small and that the shift 64 schedule performed slightly better in early iterations, we generally recommend the shift 64 schedule.\n\n\\textbf{Dropout}\nThe ImageNet dataset has roughly 1 million images. As noted by prior work, it is important to regularize the networks to avoid overfitting \\citep{ho2022cascaded,dhariwal2021diffusionbeatgans}. Although dropout has been successfully applied to networks at resolutions of $64 \\times 64$, it is often disabled for models operating on high resolutions. In this experiment we enable dropout only on a subset of the network layers: Only for resolutions below the given `starting resolution' hyperparameter. For example, if the starting resolution is $32$, then dropout is applied to modules operating on resolutions $32 \\times 32$, $16 \\times 16$ and $8 \\times 8$.\n\nRecall our hypothesis that it should be sufficient to regularize the modules of the network that operate on the lower resolution feature maps. As presented in Table~\\ref{tab:dropout}, this hypothesis holds. For this experiment on images of $128 \\times 128$, adding dropout from resolutions $64, 32, 16$ all perform comparatively. Although adding dropout from $16 \\times 16$ performed a little worse, we use this setting throughout the remainder of the experiments because it converged faster in early iterations.\n\nThe experiment also shows two settings that do not work and should be avoided: either adding no dropout, or adding dropout starting from the same resolution as the data. This may explain why dropout for high resolution diffusion has not been widely used thus far: Typically dropout is set as a global parameter for all feature maps at all resolutions, but this experiment shows that such a regularization is too aggressive.\n\n\\textbf{Architecture scaling}\nIn this section we study the effect of increasing the amount of $16 \\times 16$ network modules. In U-Nets, the number of blocks hyperparameter typically refers to the number of blocks on the `down' path. In many implementations, the `up' blocks use one additional block. When the table reads `2 + 3' blocks, that means 2 down blocks and 3 up blocks, which would in literature be referred to as 2 blocks.\n\nGenerally, increasing the number of modules improves the performance as can be seen in Table~\\ref{tab:scaling}. An interesting exception to this is the eval FID going from $8$ to $12$ blocks, which decreases slightly. We believe that this may indicate that the network should be more strongly regularized as it grows. This effect will later be observed to be amplified for the larger U-ViT architectures.\n\n\\begin{table}\n    \\centering\n    \\caption{Scaling the U-Net architecture}\\vspace{-.2cm}\n    \\label{tab:scaling}\n    \\scalebox{.95}{\n    \\begin{tabular}{l l l r}\n    \\toprule\n    \\# blocks at $16 \\times 16$ & FID train & FID eval & steps / sec \\\\ \\midrule\n    2 \\textcolor{gray}{+ 3} & 3.42 & 3.59 & \\textbf{114}\\% \\\\\n    4 \\textcolor{gray}{+ 5} & 2.98 & 3.29 & 100\\% \\\\\n    8 \\textcolor{gray}{+ 9} & 2.46 & \\textbf{3.00} & 76\\% \\\\\n    12 \\textcolor{gray}{+ 13} & \\textbf{2.41} & 3.03 & 62\\% \\\\\n        \\bottomrule\n    \\end{tabular}}\n\\end{table}\n\n\\begin{table}\n    \\centering\n    \\caption{Downsampling strategies on ImageNet 512 $\\times$ 512.}\\vspace{-.2cm}\n    \\label{tab:downsampling}\n    \\scalebox{.9}{\n    \\begin{tabular}{l r r r r}\n    \\toprule\n    Strategy & FID train & FID eval & steps / sec \\\\ \\midrule\n    None & 5.60 & 5.23 & 100\\% \\\\\n    DWT-1 & 5.42 & 4.97 & 139\\% \\\\\n    DWT-2 & \\textbf{4.85} & \\textbf{4.58} & \\textbf{146}\\% \\\\\n    Conv-(2 $\\times$ 2) & 5.99 & 5.33 & 137\\% \\\\\n    Conv-(4 $\\times$ 4) & 5.04 & 4.80 & \\textbf{146}\\% \\\\\n        \\bottomrule\n    \\end{tabular}}\n    \\vspace{-.4cm}\n\\end{table}\n\n\\begin{table}\n    \\centering\n    \\caption{Multiscale loss. Note that the $256$ models use the shift 32 and the $512$ use shift 64. This loss modifications is helpful for the highest resolution, but diminishes performance slightly for lower resolutions.}\n    \\label{tab:multiscale}\\scalebox{.9}{\n    \\begin{tabular}{l l l l}\n    \\toprule\n    Resolution & FID train & FID eval & IS \\\\ \\midrule\n    256 & \\textbf{3.76} & \\textbf{3.71} & \\textbf{171.6} \\\\\n    + multiscale loss (32) & 4.00 & 3.89 & 171.0 \\\\ \\midrule\n    512 & 4.85 & 4.58 & 156.1 \\\\\n     + multiscale loss (32) & \\textbf{4.30} & \\textbf{4.28} & \\textbf{171.0} \\\\ \n        \\bottomrule\n    \\end{tabular}}\n\\end{table}\n\\begin{table}\n    \\centering\n    \\caption{Comparison to generative models in the literature on ImageNet without any guidance or other sampling modifications, except ($^*$) which use temperature scaling.}\\vspace{-.2cm}\n    \\label{tab:literature_comparison}\n    \\scalebox{.8}{\n    \\begin{tabular}{l r r l}\n    \\toprule \n    & \\multicolumn{2}{c}{FID} \\\\\n    Method & train & eval & IS \\\\ \\midrule \n    \\textbf{128 $\\times$ 128 resolution} \\\\\n    ADM \\citep{dhariwal2021diffusionbeatgans} & 5.91 \\\\\n    CDM ($32, 64, 128$) \\citep{ho2022cascaded} & 3.52 & 3.76 &  128.8 {\\small $\\pm$ 2.51} \\\\\n    RIN \\citep{jabri2022scalable} & 2.75 & & 144.1 \\\\\n    simple diffusion (U-Net) (ours) & 2.26 & \\textbf{2.88} & 137.3 {\\small $\\pm$ 2.03} \\\\\n    simple diffusion (U-ViT 2B) (ours) & \\textbf{1.94} & 3.23 & \\textbf{171.9} {\\small $\\pm$ 3.24} \\\\ \\midrule\n    \\textbf{256 $\\times$ 256 resolution} \\\\\n    BigGAN-deep (no truncation) & 6.9\\hspace{.175cm} & & 171.4 {\\small $\\pm$ 2} \\\\\n    MaskGIT \\citep{chang2022maskgit} & 6.18 & & 182.1 \\\\\n    DPC$^\\star$ (full 5) \\citep{anonymous2023discrete} & 4.45 & & \\textbf{244.8} \\\\ \\midrule\n    \\textit{Denoising diffusion models} \\\\ \n    ADM \\citep{dhariwal2021diffusionbeatgans} & 10.94 \\\\\n    CDM ($32, 64, 256$) \\citep{ho2022cascaded} & 4.88 & 4.63 & 158.71 {\\small $\\pm$ 2.26} \\\\\n    LDM-4 \\citep{rombach2022highresolution} & 10.56 & & 103.49 \\\\\n    RIN \\citep{jabri2022scalable} & 4.51 & & 161.0 \\\\\n    DiT-XL/2 \\citep{peebles2022scalable} & 9.62 & & 121.5 \\\\\n    simple diffusion (U-Net) (ours) & 3.76 & \\textbf{3.71} & 171.6 {\\small $\\pm$ 3.07} \\\\\n    simple diffusion (U-ViT 2B) (ours) & \\textbf{2.77} & 3.75 & 211.8 {\\small $\\pm$ 2.93} \\\\ \n    \\midrule\n    \\textbf{512 $\\times$ 512 resolution} \\\\ \n    MaskGIT \\citep{chang2022maskgit} & 7.32 & & 156.0 \\\\\n    DPC (U)$^\\star$ \\citep{anonymous2023discrete} & 3.62 &  & \\textbf{249.4} \\\\ \\midrule\n    \\textit{Denoising diffusion models} \\\\ \n    ADM \\citep{dhariwal2021diffusionbeatgans}& 23.24 \\\\\n    DiT-XL/2 \\citep{peebles2022scalable} & 12.03 & & 105.3 \\\\\n    simple diffusion (U-Net) (ours) & 4.30 & \\textbf{4.28} & 171.0 {\\small $\\pm$ 3.00} \\\\\n    simple diffusion (U-ViT 2B) (ours) & \\textbf{3.54} & 4.53 & 205.3{\\small $\\pm$ 2.65} \\\\\n        \\bottomrule\n    \\end{tabular}}\n    \\vspace{-.2cm}\n\\end{table}\n\n\\begin{table}\n    \\centering\n    \\vspace{-.2cm}\n    \\caption{Text to image result on zero-shot COCO}\\vspace{-.2cm}\n    \\label{tab:text_to_image_fid}\n    \\scalebox{.9}{\n    \\begin{tabular}{l r}\n    \\toprule\n    Method & FID@30K 256 \\\\ \\midrule\n    GLIDE \\citep{nichol2022glide} & 12.24 \\\\\n    Dalle-2 \\citep{ramesh2022hierarchicaltextconditional} & 10.39 \\\\\n    Imagen \\citep{saharia2022imagen} & 7.27 \\\\\n    Muse \\citep{chang2023muse} & 7.88 \\\\\n    Parti \\citep{yu2022scalingautoregressive} & 7.23 \\\\\n    eDiff-I \\citep{balaji2022ediffi} & \\textbf{6.95} \\\\\n    simple diffusion (U-ViT 2B) (ours) & 8.30 \\\\\n        \\bottomrule\n    \\end{tabular}}\n\\end{table}\n\n\\textbf{Avoiding higher resolution feature maps}\nIn this experiment, we want to study the effect of downsampling techniques to avoid high resolution feature maps. For this experiment we first have a standard U-Net for images of resolution 512. Then, when we downsample (either to 256 or to 128) using conventional layers or the DWT. For this study the total number of blocks is kept the same, by distributing the high resolution blocks that are skipped over the lower resolution blocks (see Appendix~\\ref{app:experimental_details} for more details). Recall our hypothesis that downsampling should not cost much in sample quality, while considerably making the model faster. Surprisingly, in addition to being faster, models that use downsampling strategies also obtain better sample quality. It seems that downsampling for such a high resolution enables the network to optimize better for sample quality. Most importantly, it allows training without absurdly large feature maps without performance degradation.\n\n\\textbf{Multiscale Loss}\nFor this final experiment, we test the difference between the standard loss and the multiscale loss, which adds more emphasis on lower frequencies in the image. For the resolutions 256 and 512 we report the sample quality in FID score for a model trained with the multiscale loss enabled or disabled. As can be seen in Figure~\\ref{tab:multiscale}, for 256 the loss does not seem to have much effect and performs slightly worse. However, for the larger 512 resolution the loss has an impact and reduces FID score. \n\n\\subsection{Comparison with literature}\nIn this section, simple diffusion is compared to existing approaches in literature. Although very useful for generating beautiful images, we specifically choose to only compare to methods without guidance (or other sampling modifications such as rejection sampling) to see how well the model is fitted. These sampling modifications may produce inflated scores on visual quality metrics \\citep{ho2022classifierfreeguidance}.\n\nInterestingly, the larger U-ViT models perform very well on train FID and Inception Score (IS), outperforming all existing methods in literature (Table~\\ref{tab:literature_comparison}). However, the U-Net models perform better on eval FID. We believe this to be an extrapolation of the effect we observed before in Table~\\ref{tab:scaling}, where increasing the architecture size did not necessarily result in better eval FID. For samples from the models see Figures~\\ref{fig:overview} \\& \\ref{fig:random_samples_imagenet}.\nIn summary, simple diffusion achieves SOTA FID scores on class-conditional ImageNet generation among all other types of approaches without sampling modifications. We think this is an incredibly promising result: by adjusting the diffusion schedule and modifying the loss, simple diffusion is a single stage model that operates on resolutions as large as 512 $\\times$ 512 with high performance. See Appendix~\\ref{app:additional_exps} for additional results.\n\n\\textbf{Text to image}\nIn this experiment we train a text-to-image model following \\citep{saharia2022imagen}. In addition to the self-attention and mlp block, this network also has cross-attention in the transformer that operates on T5 XXL text embeddings. For these experiments we also replaced convolutional layers with self-attention at the 32 resolution feature maps to improve detail generation. As can be seen in Table~\\ref{tab:text_to_image_fid}, simple diffusion is a little better than some recent text-to-image models such as DALLE-2, although it still lacks behind Imagen. For the resolution $512 \\times 512$, the FID@30K score is 9.57. Importantly, our model is the first model that can generate images of this quality using only a single diffusion model that is trained end-to-end. \n\n\\section{Conclusion}\nIn summary, we have introduced several simple modifications of the original denoising diffusion formulation that work well for high resolution images. Without sampling modifiers, simple diffusion achieves state-of-the-art performance on ImageNet in FID score and can be easily trained in an end-to-end setup. Furthermore, to the best of our knowledge this is the first single-stage text to image model that can generate images with such high visual quality.\n\n\\clearpage\n\n\\clearpage\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{Language Model Beats Diffusion\\\\---  Tokenizer is Key to Visual Generation}\n\n\\begin{document}\n\n\\maketitle\n\\vspace{-3mm}\n\\begin{abstract}\n\\vspace{-3mm}\nWhile Large Language Models (LLMs) are the dominant models for generative tasks in language, they do not perform as well as diffusion models on image and video generation.\nTo effectively use LLMs for visual generation, one crucial component is the visual tokenizer that maps pixel-space inputs to discrete tokens appropriate for LLM learning.\nIn this paper, we introduce \\modelname{}, a video tokenizer designed to generate concise and expressive tokens for both videos and images using a common token vocabulary.\nEquipped with this new tokenizer, we show that LLMs outperform diffusion models on standard image and video generation benchmarks including ImageNet and Kinetics. \nIn addition, we demonstrate that our tokenizer surpasses the previously top-performing video tokenizer on two more tasks: (1) video compression comparable to the next-generation video codec (VVC) according to human evaluations, and (2) learning effective representations for action recognition tasks.\n\\end{abstract}\n\n\\vspace{-4mm}\n\\section{Introduction}\n\\vspace{-2mm}\n\nLarge transformer-based language models, commonly referred to as LMs or LLMs, are the de facto models for natural language generation~\\citep{openai2023gpt4,googlepalm2,touvron2023llama}. Over time, LMs have expanded their capabilities to generate content in various modalities, asserting their dominance in other domains like audio~\\citep{agostinelli2023musiclm}, speech~\\citep{rubenstein2023audiopalm}, code generation~\\citep{li2023starcoder}, medical applications~\\citep{singhal2023towards} and robotics~\\citep{brohan2023rt}.\n\nLMs are capable of generating images and videos. To do so, the image pixels are mapped into a sequence of discrete tokens by a visual tokenizer (\\cf \\cref{sec:background}). These tokens are then fed into the LM transformer, as if they were lexical words, for generative modeling. Despite notable advancements in employing LMs for visual generation~\\citep{esser2021taming,chang2022maskgit}, LMs still do not perform as well as diffusion models~\\citep{rombach2022high}.\nFor instance, when evaluating on the ImageNet dataset, a gold standard benchmark for image generation, the best language model~\\citep{lee2022draft} underperforms \nthe diffusion model~\\citep{gao2023masked} by a substantial 48\\% margin (FID 3.41 \\vs 1.79 when generating images at the 256$\\times$256 resolution).\n\n\\emph{Why do language models lag behind diffusion models in visual generation?} This paper suggests that a primary reason is the lack of a good visual representation, resembling our natural language system, for effectively modeling the visual world.\nTo substantiate this hypothesis, this paper shows that, when utilizing a good visual tokenizer, the masked language model~\\citep{devlin2019bert,chang2022maskgit,yu2022magvit} surpasses the state-of-the-art diffusion models  in terms of both generation fidelity and efficiency across image and video benchmarks, given the same training data, comparable model size, and training budget. To the best of our knowledge, this provides the first evidence that language models beat diffusion models on the hallmark ImageNet benchmark. \n\nIt is worth emphasizing that our intention is not to assert whether the language model is superior to others, but to promote the exploration of visual tokenization methods for LLMs.\nA fundamental difference of LLMs from other models, such as diffusion models, is that LLMs utilize a discrete latent format: tokens obtained from a visual tokenizer.\nWe show that the values of these discrete visual tokens should not be overlooked considering their distinct advantages as follows. \\textbf{(1) Compatibility with LLMs.} The main advantage of a token representation is that it shares the same form as language tokens, making it straightforward to leverage the optimizations our community has developed over many years for LLMs. This includes faster training and inference speeds~\\citep{shazeer2019fast,lester2021power}, advancements in model infrastructure~\\citep{dao2022flashattention,du2022glam}, learning recipes for model scaling~\\citep{brown2020language,chowdhery2022palm}, and GPU/TPU optimization, among other innovations. Unifying vision and language by the same token space could set the stage for a true multimodal LLM that can understand, generate, and reason within our visual environment.\n\\textbf{(2) Compressed representation.} The discrete token may offer a fresh perspective on video compression. The \nvisual tokens can serve as a new video compression format to reduce disk storage and bandwidth during internet transfers. Unlike compressed RGB pixels, these tokens can be fed directly into generative models, bypassing the conventional decompression and latent encoding steps. This allows for faster processing in generative video applications, especially beneficial in edge computing cases. \\textbf{(3) Visual understanding benefits}. \nPrior research has shown that the discrete tokens are valuable as a pre-training target in self-supervised representation learning, as discussed in BEiT~\\citep{bao2021beit} and BEVT~\\citep{wang2022bevt}. \nAdditionally, research finds that using tokens as the model inputs improves the robustness and generalization~\\citep{mao2021discrete}.\n\nIn this paper, we introduce \\modelname{}, a video tokenizer designed to map videos (and images) into compact discrete tokens. Our model is built on the state-of-the-art video tokenizer, MAGVIT~\\citep{yu2022magvit}, within the VQ-VAE framework~\\citep{van2017neural}. We propose two new techniques. First, a novel lookup-free quantization method enables the learning of a large vocabulary that is able to improve generation quality of the language model.\nSecond, through extensive empirical analyses, we have identified modifications to the tokenizer that not only enhance generation quality but also enable the tokenization of both images and videos using a shared vocabulary.\n\nWe empirically demonstrate that our model outperforms the previously top-performing video tokenizer, MAGVIT, in three key areas. First, our model significantly improves the generation quality of MAGVIT, establishing the state of the art on the common image and video benchmarks. Second, user studies indicate that its compression quality exceeds that of MAGVIT and the current video compression standard, HEVC~\\citep{sullivan2012overview}. Moreover, it is on par with the next-generation video codec, VVC~\\citep{vvc}.\nFinally, we show that, compared to MAGVIT, our new tokens are stronger for video understanding tasks across two setups and three datasets.\nThe main contributions of this work are:\n\\vspace{-2mm}\n\\begin{itemize}[nosep, leftmargin=*]\n\\item A new video tokenizer that outperforms the previously best-performing video tokenizer in three areas: visual generation, video compression, and action recognition.\n\\item A novel lookup-free quantization approach that enables improving the visual generation quality of language models by learning a large vocabulary.\n\\item To the best of our knowledge, the first evidence suggesting that a language model can outperform diffusion models on ImageNet when provided with the same training data, an equivalent model size, and a similar training budget.\n\\item A video compressor with better quality than HEVC and VVC, at similar bit rates, according to user studies. To our knowledge, this is the first successful attempt of a visual tokenizer designed for video generation to achieve comparable results to standard codecs.\n\\end{itemize}\n\n\\vspace{-4mm}\n\\section{Background} \\label{sec:background}\n\\vspace{-4mm}\n\\paragraph{Language Model (LM) for visual generation.}\nLMs have been extended to generate images and videos. A visual tokenizer $f$ is used to first map visual inputs into a sequence of discrete tokens. A video  $\\rmV \\in \\sR^{T \\times H \\times W \\times 3}$ (or image when $T=1$) is tokenized into a discrete representation $\\rmX = f(\\rmV) \\in \\{1, 2, \\cdots, K\\}^{T' \\times H' \\times W'}$, where $K$ is the codebook (vocabulary) size of the visual tokenizer. $\\rmX$ is flattened into a 1D token sequence obtained using raster scan ordering and then fed into an LM transformer for generative modeling. \n\nTwo types of LMs are commonly used for visual generation. The \\emph{Autoregressive LM (AR-LM)} includes ImageGPT~\\citep{chen2020generative}, DALL-E~\\citep{ramesh2021zero}, Parti~\\citep{yu2022scaling}, \\etc. \nAn AR-LM predicts the next token given the previous tokens along with additional conditioning information $\\rvc$ using a categorical distribution for $p_\\theta(\\ervx_i \\mid \\rvx_{<i}; \\rvc)$. During inference, AR-LMs use the standard autoregressive decoding over the tokens. Finally, the tokens are converted back to pixels by a decoder associated with the visual tokenizer.\n\nThe \\emph{Masked LM (MLM)} is another type of language model for visual generation, such as: MaskGIT~\\citep{chang2022maskgit}, MAGVIT~\\citep{yu2022magvit}, Phenaki~\\citep{villegas2022phenaki}, and MUSE~\\citep{chang2023muse}, among others. An MLM is trained using a masked token objective \\citep{devlin2019bert}, where some tokens in the sequence are randomly masked and need to be predicted given the observed tokens. Let $\\rvm \\in \\{0,1\\}^n$ be a random binary sequence where $\\rvm^\\top \\vone \\in [0, n-1]$. The MLM learns $p_\\theta(\\ervx_i \\mid \\{\\ervx_j: \\ervm_j=1, \\forall j\\}; \\rvc)$ for all $i$ where $\\rvm_i=0$. To generate a video or image during inference, the MLM uses the non-autoregressive decoding algorithms\nfor images and videos~\\citep{chang2022maskgit,yu2022magvit}. \nThe decoding starts with a fully masked sequence, which is iteratively filled by repeating two steps: (1) sample the whole sequence $\\hat{\\rvx}^{(t)}$ from $p_\\theta$ given the non-masked tokens from the previous step, (2) re-mask the $\\lfloor\\lambda(t)\\cdot n\\rfloor$ tokens in $\\hat{\\rvx}^{(t)}$ with the lowest probability, following a decreasing masking ratio schedule $\\lambda(t)$, according to timestamp $t$.\n\n\\vspace{-4mm}\n\\paragraph{Denoising Diffusion Models (DDM).}\nDDMs \\citep{sohl2015deep,song2019generative} are regarded as the state-of-the-art in visual generation due to their high-quality image~\\citep{dhariwal2021diffusion,ho2022imagen} and video generation~\\citep{ho2022video}. For instance, DDPM~\\citep{ho2020denoising} learns a denoising process parameterized as conditional Gaussian distributions over image pixels. Recently, diffusion models and language models have displayed a significant overlap. Recent DDMs diffuse over latents rather than raw pixels. These latents are obtained using models similar to the visual tokenizer used by LMs. In fact, the very first latent in diffusion, proposed by \\citet{rombach2022high}, is derived from a visual tokenizer. Additionally, the diffusion model's architecture has been shifting from the U-Net to the transformer architecture~\\citep{peebles2022scalable}. Consequently, the boundaries between diffusion and language models in visual generation have become less distinct. \nYet, a fundamental difference between DDMs and LMs lies in the latent format, \\ie, continuous \\vs discrete. We have discussed the benefits of having discrete tokens in \\cref{sec:intro} and \nwill show that the proposed tokenizer improves in these aspects.\n\n\\vspace{-4mm}\n\\paragraph{Visual tokenization.}\nVisual tokenization plays an essential role in mapping pixels into a discrete representation suitable for generative modeling. VQ-VAE \\citep{van2017neural} is a cornerstone work in image tokenization. A VQ-VAE model consists of a convolutional neural network (CNN) encoder, a vector-quantization (VQ) bottleneck, and a CNN decoder.\nGiven a video $\\rmV \\in \\sR^{T \\times H \\times W \\times 3}$, the VQ-VAE's encoder $E$ produces latent embeddings $\\rmZ = E(\\rmV) \\in \\sR^{T'\\times H'\\times W'\\times d}$. \nEach embedding vector $\\rvz \\in \\sR^d$ in $\\rmZ$ is then passed through the vector quantizer $q$, which assigns it to the closest entry $\\rvc \\in \\sR^d$ in the learned codebook embedding $\\rmC \\in \\sR^{K \\times d}$:\n\\begin{equation}\n\\vspace{-2mm}\n    q(\\rvz) = \\rvc_i, \\text{ where } i=\\underset{j \\in \\{1,2,\\cdots,K\\}}{\\argmin}\\Vert \\rvz - \\rvc_j \\Vert_2.\n\\end{equation}\nTo get discrete tokens, we drop the embedding dimension and represent $\\rmZ$ by its indices $\\rmX \\in \\{1, 2, \\cdots, K\\}^{T' \\times H' \\times W'}$. For decoding, embeddings of all image tokens are given as input to the decoder $D$ to reconstruct the input $\\hat{\\rmV} = D(\\rmZ)$. \nFollowing VQ-VAE, VQGAN~\\citep{esser2021taming} introduces an adversarial loss and feature-level perceptual losses to enhance the image quality.\n\nVideo tokenization is more challenging and VQGAN has been adapted to meet this purpose~\\citep{ge2022long,villegas2022phenaki,yu2022magvit}. The state of the art in video tokenization is MAGVIT~\\citep{yu2022magvit}, which introduces a better 3D architecture, an inflation technique for initialization using image pre-training, and robust training losses. With MAGVIT, the LMs achieve leading generation quality across multiple video benchmarks. However, MAGVIT struggles to tokenize images and often results in noticeable flickering in longer videos.\n\n\\vspace{-4mm}\n\\section{Method}\n\\vspace{-2mm}\n\nWe introduce a new \\textbf{video tokenizer} designed to map the spatial-temporal dynamics from a visual scene into compact discrete tokens suitable for language models. \n\\cl{Compared with image generation, video generation still faces substantial challenges in generating consistent and realistic motion. We are interested in exploring the capabilities of language models in tackling this unsolved challenge. Therefore, this paper focuses on a video tokenizer that can effectively represent video for generative modeling.}\nOur approach builds upon the state-of-the-art video tokenizer, MAGVIT, as detailed in \\citet{yu2022magvit}. \nThis section highlights two new designs: a lookup-free quantizer and a collection of enhancements to the tokenizer model.\n\\vspace{-2mm}\n\\subsection{Lookup-Free Quantizer}\n\\vspace{-2mm}\n\nAlthough the community has made great progress in developing VQ-VAEs, the relationship between improvements in the reconstruction quality and subsequent generation quality is still not well understood.\nA common misconception is that improving reconstruction equates to improving the generation of the language model.\nFor example, enlarging the vocabulary can improve reconstruction quality. However, such improvement only extends to generation when the vocabulary size is small, and a very large vocabulary can actually hurt the performance of the language model. \n\nAs illustrated by the dashed curves in \\cref{fig:motivation}, the reconstruction FID, indicated by the right $y$-axis (where a lower value is better), improves as the vocabulary size (the $x$-axis) increases. The orange solid curve in \\cref{fig:motivation} represents the LM's generation quality (the left $y$-axis). The generation FID initially improves but deteriorates for larger vocabulary. This may shed light on why the vocabulary size of most language models for visual generation is around 1-8k~\\citep{esser2021taming,villegas2022phenaki}, which is significantly smaller than the size of natural language vocabulary, \\ie over 200k.\n\n\\begin{wrapfigure}{r}{0.5\\textwidth}\n\\vspace{-4mm}\n    \\centering\n    \\includegraphics[width=\\linewidth,trim={0 0 0 0},clip]{curve.pdf}\n    \\vspace{-6mm}\n    \\caption{\\textbf{Reconstruction and generation quality curves} in FID on ImageNet when scaling the tokenizer's vocabulary size with Vector Quantization (VQ) and Lookup-Free Quantization (LFQ). \n    Comparison is done at 128$\\times$128 resolution using an MLM with 306-372M parameters.\n    }\n    \\label{fig:motivation}\n    \\vspace{-2mm}\n\\end{wrapfigure}\n\nA simple trick for training a larger codebook involves decreasing the code embedding dimension when increasing the vocabulary size~\\citep{yu2021vector}.\nThis trick captures the intuition of limiting the representational capacity of individual tokens, which in turn facilitates learning over the distribution of a large vocabulary.\n\n\\vspace{-4mm}\n\\paragraph{Lookup-Free Quantization (\\quantizername{}).}\n\nMotivated by the above observation, we reduce the VQ-VAE codebook's embedding dimension to zero. Formally, the codebook $\\rmC \\in \\sR^{K \\times d}$ is replaced with an integer set $\\sC$ where $|\\sC| = K$.\nRecall that in VQ-VAE models, the quantizer must look up all $K$ $d$-dimensional embeddings in the codebook, where $d$ is typically $256$, when computing the closest codebook entry to the encoder output. This new design eliminates the need for such embedding lookup entirely hence we call it \\emph{lookup-free quantization (\\quantizername{})}.\nWe found that \\quantizername{} can grow the vocabulary size in a way benefiting the generation quality of language models. As shown by the blue curves in \\cref{fig:motivation}, both reconstruction and generation consistently improves as the vocabulary size increases -- a property not observed in current VQ-VAE methods.\n\nWhile various \\quantizername{} methods are available, this paper discusses a straightforward variant that assumes independent codebook dimensions and binary latents. \nSpecifically, the latent space of \\quantizername{} is decomposed as the Cartesian product of single-dimensional variables, as $\\sC = \\bigtimes_{i=1}^{\\log_2{K}}C_i$.\nGiven a feature vector $\\rvz \\in \\sR^{\\log_2{K}}$, each dimension of the quantized representation $q(\\rvz)$ is obtained from:\n\\begin{equation}\n    q(\\ervz_i) = C_{i, j}, \\text{ where } j = \\argmin_k \\|\\ervz_i - C_{i, k}\\|,\n\\end{equation}\nwhere $C_{i, j}$ is the $j$-th value in $C_i$.\nWith $C_i = \\{-1, 1\\}$, the $\\argmin$ can be computed by the sign function as \n\\begin{equation}\n    q(\\ervz_i) = \\sign(\\ervz_i) = -\\1\\{\\ervz_i \\le 0\\} + \\1\\{\\ervz_i > 0\\}.\n\\end{equation}\nWith \\quantizername{}, the token index for $q(\\rvz)$ is given by:\n\\begin{equation}\n\\vspace{-1mm}\n\\mathit{Index}(\\rvz) = \\sum_{i=1}^{\\log_2{K}} \\argmin_k \\|\\ervz_i - C_{i, k}\\| \\prod_{b=0}^{i-1} |C_b| = \\sum_{i=1}^{\\log_2{K}} 2^{i-1} \\1\\{\\ervz_i > 0\\}, \\label{eq:tokenid}\n\\end{equation}\nwhere $|C_0|=1$ sets the virtual basis.\n\nWe add an entropy penalty during training to encourage codebook utilization:\n\\begin{align}\n    \\gL_\\mathit{entropy} &= \\E[H(q(\\rvz))] - H[\\E(q(\\rvz))].\n    \\label{eq:entropy}\n\\end{align}\nThis penalty is inspired by a similar loss used in image VQGAN model~\\citep{chang2022maskgit}, which is also found in entropy-based clustering~\\citep{jansen2020coincidence}. In \\quantizername{}, given the independence among dimensions, we rewrite $H(q(\\rvz)) = \\sum_{i=1}^{\\log_2{K}} H(q (\\ervz_i))$ .\nThe $H[\\E(q(\\rvz))]$ term can be approximated with sub-groups of dimensions for $K>2^{18}$ where direct estimation is memory bound. \n\nWe note that there are various other variants of \\quantizername{}, \\eg, opting for the multivariant over the binary codebook $C_i$ or employing other quantization techniques such as \\citet{agustsson2019generative}.\nAs the first paper to introduce this concept, we focus on the simplest form with independent binary dimensions, which shows promising improvements. Other \\quantizername{} methods merit further research. \n\n\\cl{In addition to the entropy penalty (\\cref{eq:entropy}), an \\quantizername{}-based tokenizer is trained using the standard combination of \\emph{reconstruction},  \\emph{GAN}, \\emph{perceptual}, and \\emph{commitment} losses~\\citep{esser2021taming}, excluding the inapplicable codebook loss. Following \\cite{yu2022magvit}, we use LeCAM regularization~\\citep{tseng2021regularizing} for improved stability. \n}\n\n\\subsection{Visual Tokenizer Model Improvement}\\label{sec:tokenizer_improvement}\n\\begin{figure}[tp]\n\\vspace{-6mm}\n    \\centering\n    \\begin{subfigure}{1\\textwidth} % no visible\n    \\refstepcounter{subfigure}\\label{fig:arch:a}\n    \\refstepcounter{subfigure}\\label{fig:arch:b}\n    \\refstepcounter{subfigure}\\label{fig:arch:c}\n    \\end{subfigure}%\n    \\includegraphics[width=0.95\\linewidth,trim={0 0 0 0},clip]{causal_arch}\n    \\vspace{-2mm}\n    \\caption{\\textbf{Causal tokenizer architecture comparison}.\n    The decoders, which are omitted from the figure, employ an architecture that is symmetric to the encoder.\n    \\cl{\\scriptsize See detailed architecture diagram in the Appendix.}\n    }\n    \\label{fig:arch}\n    \\vspace{-6mm}\n\\end{figure}\n\n\\vspace{-2mm}\n\\paragraph{Joint image-video tokenization.}\nA desirable feature of visual tokenization is the capability to tokenize images and videos using a shared codebook. However, the MAGVIT tokenizer, which utilizes the 3D CNN, faces challenges in tokenizing images due to the temporal receptive field. %While replicating an image multiple times to form a static video is possible, this approach introduces redundant code and can potentially alter the training data distribution, resulting in reduced motion.\n\nTo build a joint image-video tokenizer, a new design is needed. We begin our discussion by revisiting an existing method C-ViViT~\\citep{villegas2022phenaki}. As depicted in \\cref{fig:arch:a}, C-ViViT employs full spatial transformer blocks combined with causal temporal transformer blocks. This approach performs reasonably well but has two drawbacks. First, unlike CNNs, the positional embeddings makes it difficult to tokenize spatial resolutions that were not seen during training. Second, empirically we found that 3D CNNs perform better than spatial transformer and produce tokens with better spatial causality of the corresponding patch.\n\nTo tackle these drawbacks, we explore two plausible designs. \\cref{fig:arch:b} combines C-ViViT and MAGVIT. Assuming a temporal compression ratio of 4, a 3D CNN processes blocks of 4 frames followed by a causal transformer. In \\cref{fig:arch:c}, we use the temporally causal 3D convolution to replace the regular 3D CNN. Specifically, the temporal padding scheme for a regular 3D convolution layer with kernel size $(k_t, k_h, k_w)$ includes $\\lfloor\\frac{k_t-1}{2}\\rfloor$ frames before and $\\lfloor\\frac{k_t}{2}\\rfloor$ frames after the input frames.\nIn contrast, a causal 3D convolution layer pads with $k_t - 1$ frames before the input and nothing after, so that the output for each frame only depends on the previous frames.\nIn consequence, the first frame is always independent of other frames, allowing the model to tokenize single images.\n\nTemporal convolutional subsampling with stride $s$ is sufficient for $s\\times$ down-sampling by mapping $1 + s \\times t$ frames into $1 + t$.\nAfter a regular $s \\times$ up-sampling, we drop the first $s - 1$ resulting frames, which maps  $1 + t$ frames into $1 + s \\times t$ and allows for the tokenization of a single image. \\cref{tab:ablation_arch} empirically compares the designs in \\cref{fig:arch}, and we find that the causal 3D CNN performs the best.\n\n\\vspace{-5mm}\n\\paragraph{Architecture modifications.}\nIn addition to using causal 3D CNN layers, we made several other architectural modifications to improve upon the MAGVIT model. First, we change the encoder downsamplers from average pooling into strided convolutions to leverage learned kernels, and replace the decoder upsamplers from\nnearest resizing followed by convolution with a depth-to-space operator.\nSecond, we defer the temporal downsampling from the first few encoder blocks to the last ones.\nIn addition, the downsampling layer in the discriminator now utilizes 3D blur pooling~\\citep{zhang2019making} to encourage shift invariance. \nFinally, we add one adaptive group normalization layer before the residual blocks at each resolution in the decoder to pass in the quantized latents as the control signal following StyleGAN~\\citep{karras2019style}.\n\\cref{tab:ablation_imagenet,tab:ablation_ucf101} empirically verify these designs.\n\n\\vspace{-5mm}\n\\paragraph{Token factorization for efficient prediction.}\nThe output tokens can be fed into language models to generate videos. To assist smaller transformers predicting in a large vocabulary, we can factorize the \\quantizername{} token's latent space into equal subspaces. For instance, rather than predicting using a codebook of size $2^{18}$, we can predict in two concatenated codebooks, each of size $2^9$. We embed each subspace token separately and use their embedding summation as the token embedding for the transformer input. \\cl{We find  it beneficial to use weight tying~\\citep{press2017using}, a common technique in language modeling, which involves sharing the weights between the embedding and softmax layers.\nFor the output layer with a factorized vocabulary}, we use the embedding matrix for each subspace to obtain logits with seperate prediction heads. \n\n\\vspace{-4mm}\n\\section{Experiments}\n\\vspace{-2mm}\nThis section empirically verifies the proposed tokenizer across three distinct tasks: video and image generation, video compression, and action recognition.\n\\cref{fig:reconstruction} visually compares the reconstruction quality of our tokenizer with prior works.\nMore qualitative samples are shown at \\webpage{}.\n\n\\vspace{-2mm}\n\\subsection{Experimental Setups}\n\\vspace{-2mm}\n\\paragraph{Datasets.} \nWe use Kinetics-600 (K600)~\\citep{carreira2018short} and UCF-101~\\citep{soomro2012ucf101} for video generation experiments, along with ImageNet~\\citep{deng2009imagenet} for image generaton.\nIn addition, MCL-JCV~\\citep{wang2016mcl} is used as the testbed for video compression, with Kinetics-400 (K400)~\\citep{kay2017kinetics} and SSv2~\\citep{goyal2017something} for video understanding. \n\\vspace{-4mm}\n\\paragraph{Implementation details}\nWe follow the tokenizer training setting and hyperparameters in~\\citep{yu2022magvit}, unless stated otherwise. \\quantizername{} is used, which eliminates the codebook embedding, to increase the default codebook size to $K=2^{18}$. \nThe weight of $\\gL_\\mathit{entropy}$ follows an annealing schedule with a $3\\times$ higher starting point and linearly decays to a fixed value of $0.1$ within 2k steps.\nWe defer details regarding the evaluation setup of each subsection to the Appendix.\n\n\\begin{figure}[tp]\n\\vspace{-6mm}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/reconstruction.pdf}\n    \\vspace{-4mm}\n    \\caption{\\textbf{Image reconstruction samples with different tokenizers}. We compare the VQGAN used in MaskGIT~\\citep{chang2022maskgit} with two of our models trained on ImageNet and web images~\\citep{chen2022pali}. \\mytiny{Original images are by Eric TERRADE and Barth Bailey on Unsplash}.}\n    \\label{fig:reconstruction}\n    \\vspace{-4mm}\n\\end{figure}\n\n\\vspace{-2mm}\n\\subsection{Visual Generation}\\label{sec:gen}\n\\vspace{-2mm}\nThe masked language model (MLM)~\\citep{devlin2019bert} is used in image and video generation. To verify the tokenizer, we employ the same MLM transformers in MAGVIT~\\citep{yu2022magvit}. \\cl{We select the MLM due to its competitive performance on benchmark datasets~\\citep{yu2022magvit,lezama2023discrete}. In the Appendix, we also show that an autoregressive language model (AR-LM) coupled with the proposed tokenizer outperforms the prior work MAGVIT.}\nAs we use a smaller MLM ($\\sim$300M parameters) with a large codebook ($2^{18}\\approx$262K), the token factorization as discussed in \\cref{sec:tokenizer_improvement} is applied using two heads with each predicting from a codebook of size $2^9$.\n\n\\begin{table}[tp]\n\\centering\n\\vspace{-6mm}\n\\caption{\\textbf{Video generation results}: frame prediction on Kinetics-600 and class-conditional generation on UCF-101. We adopt the evaluation protocol of MAGVIT.\n}\n\\label{tab:gen_k600}\n\\vspace{-2mm}\n\\centering\n\\begin{tabular}{@{}l@{\\hspace{5pt}}l@{\\hspace{5pt}}c@{\\hspace{5pt}}c@{\\hspace{5pt}}c@{\\hspace{3pt}}c@{}}\n\\toprule\nType & Method   & K600 FVD$\\downarrow$  & UCF FVD$\\downarrow$  & \\#Params  & \\#Steps  \\\\ \\midrule\nGAN & TrIVD-GAN-FP~\\citep{luc2020transformation}  & 25.7\\mytiny{$\\pm$0.7} &  & & 1                \\\\\nDiffusion & Video Diffusion~\\citep{ho2022video}  & 16.2\\mytiny{$\\pm$0.3} & & 1.1B &   256        \\\\\nDiffusion & RIN~\\citep{jabri2023scalable} & 10.8 & & 411M & 1000 \\\\\n\\hdashline\nAR-LM + VQ & TATS~\\citep{ge2022long}  &  & 332\\mytiny{$\\pm$18} & 321M & 1024 \\\\\nMLM + VQ & Phenaki~\\citep{villegas2022phenaki} & 36.4\\mytiny{$\\pm$0.2} & & 227M   & 48 \\\\\nMLM + VQ & MAGVIT~\\citep{yu2022magvit}        & 9.9\\mytiny{$\\pm$0.3} & 76\\mytiny{$\\pm$2} & 306M   & 12  \\\\ \n\\midrule\n\\cl{MLM + \\quantizername{}} & \\cl{non-causal baseline} & \\cl{11.6\\mytiny{$\\pm$0.6}} & & \\cl{307M} & \\cl{12} \\\\\n\\multirow{2}{*}{MLM + \\quantizername{}} & \\multirow{2}{*}{\\emph{\\modelname{} (this paper)}} & 5.2\\mytiny{$\\pm$0.2} &  & \\multirow{2}{*}{307M} & 12  \\\\\n & & \\textbf{4.3\\mytiny{$\\pm$0.1}}  & \\textbf{58\\mytiny{$\\pm$3}} & & 24  \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\\vspace{-4mm}\n\\paragraph{Video generation.}\nWe consider two standard video benchmarks,\nUCF-101 for class-conditional generation and K600 for frame prediction with 5-frame condition.\nFVD~\\citep{unterthiner2018towards} is used as our primary evaluation metric. \n\\cref{tab:gen_k600} shows that our model surpasses all prior arts in both benchmarks.\nSpecifically, it outperforms the previous best model MAGVIT by a large margin, while using the same MLM transformer backbone. \n\\cl{In addition, it significantly outperforms the non-causal baseline on frame prediction, highlighting the contribution of the causal tokenizer.}\nThese results demonstrate the essential role of a good visual tokenizer in enabling LMs to generate high-quality videos.\n\\cref{fig:k600} shows qualitative samples from the model.\n\n\\begin{table}[tp]\n\\vspace{-2mm}\n\\caption{\\textbf{Image generation results}: class-conditional generation on ImageNet 512$\\times$512.\nGuidance indicates the classifier-free diffusion guidance~\\citep{ho2021classifier}.\n$^*$ indicates usage of extra training data.\nWe adopt the evaluation protocol and implementation of ADM.\n}\n\\label{tab:gen_in512}\n\\vspace{-2mm}\n\\resizebox{\\linewidth}{!}{%\n\\begin{tabular}{@{}l@{\\hspace{5pt}}l@{\\hspace{4pt}}c@{\\hspace{2pt}}c@{\\hspace{3pt}}c@{\\hspace{2pt}}c@{\\hspace{2pt}}c@{\\hspace{2pt}}c@{}}\n\\toprule\n\\multirow{2}{*}{Type}  & \\multirow{2}{*}{Method}  & \\multicolumn{2}{c}{w/o guidance} & \\multicolumn{2}{c}{w/ guidance}  & \\multirow{2}{*}{\\#Params}  & \\multirow{2}{*}{\\#Steps}  \\\\\n& & FID$\\downarrow$ & IS$\\uparrow$ & FID$\\downarrow$ & IS$\\uparrow$ \\\\ \\midrule\nGAN & StyleGAN-XL~\\citep{sauer2022stylegan} & & & 2.41 & 267.8 & 168M & 1 \\\\\nDiff. + VAE$^*$ & DiT-XL/2~\\citep{peebles2022scalable} & 12.03 & 105.3 & 3.04 & 240.8 & 675M  & 250  \\\\\nDiffusion & ADM+Upsample~\\citep{dhariwal2021diffusion}  & 9.96  & 121.8 & 3.85 & 221.7 & 731M & 2000 \\\\\nDiffusion & RIN~\\citep{jabri2023scalable} & 3.95 & 216.0 & & & 320M  & 1000 \\\\\nDiffusion & simple diffusion~\\citep{hoogeboom2023simple} & 3.54 & 205.3 & 3.02 & 248.7 & 2B & 512\\\\\nDiffusion & VDM++~\\citep{kingma2023vdm}  & 2.99 & 232.2 & 2.65 & 278.1  & 2B & 512 \\\\\n\\hdashline\n\n \nMLM + VQ & MaskGIT~\\citep{chang2022maskgit} & 7.32 & 156.0 & & & 227M & 12  \\\\\nMLM + VQ & DPC+Upsample~\\citep{lezama2023discrete}  & 3.62 & 249.4 & & & 619M & 72    \\\\\n\\midrule\n\\multirow{2}{*}{MLM + \\quantizername{}} & \\multirow{2}{*}{\\emph{\\modelname{} (this paper)}} & 4.61 & 192.4 & & & \\multirow{2}{*}{307M} & 12 \\\\\n& & 3.07 & 213.1 & \\textbf{1.91} & \\textbf{324.3} &  & 64 \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\vspace{-6mm}\n\\end{table}\n\\vspace{-4mm}\n\\paragraph{Image generation on ImageNet.}\nWe evaluate \\modelname{} on image generation under the standard ImageNet class-conditional setting.\nWe present results for resolution 512$\\times$512 in \\cref{tab:gen_in512} and refer to the Appendix for 256$\\times$256 results. FID~\\citep{heusel2017gans} and Inception Score (IS)~\\citep{salimans2016improved} are used as evaluation metrics. Our model surpasses the best performing diffusion models both in sampling quality (FID and IS) and inference-time efficiency (sampling steps). \n\nIt is worth noting that all the models compared are trained using the same ImageNet training data, with a comparable model size and training budget. Therefore, the performance primarily evaluates the model's capabilities. The masked language model, equipped with our tokenizer, exhibits a notable improvement in FID over the best diffusion model baseline at 512$\\times$512 (FID=1.91 \\vs 2.65, 28\\%$\\downarrow$). While this margin narrows at 256$\\times$256 resolution, the MLM uses a 50\\% reduced model size and needs much fewer decoding steps (\\eg, 64 \\vs 250) to get the image generation quality.\nQualitative samples in comparison with other models are shown in \\cref{fig:imagenet}.\n\n\\begin{figure}[tp]\n\\vspace{-4mm}\n    \\centering\n    \\includegraphics[width=\\linewidth,trim={0 0 0 0},clip]{figures/k600.pdf}\n    \\vspace{-7mm}\n    \\caption{\\textbf{Frame prediction samples on Kinetics-600}.}\n    \\label{fig:k600}\n    \\vspace{-2mm}\n\\end{figure}\n\\begin{figure}[tp]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/ImageNet512.pdf}\n    \\vspace{-7mm}\n    \\caption{\\textbf{Class-conditional generation samples on ImageNet 512$\\times$512}. We compare with each of the previous works with a random sample from the same image class.}\n    \\label{fig:imagenet}\n    \\vspace{-6mm}\n\\end{figure}\n\n\\setlength\\intextsep{20pt}\n\\begin{wrapfigure}{r}{0.45\\textwidth}\n\\vspace{-2mm}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/elo_2.pdf}\n    \\vspace{-6mm}\n    \\caption{\\textbf{Video compression rater study}.\n    }\n    \\label{fig:compression}\n    \\vspace{-28mm}\n\\end{wrapfigure}\n\\vspace{-8mm}\n\\leavevmode\\subsection{Video Compression}\n\nWe conduct a subjective rater study to assess the compression quality of \\modelname{}. The study is conducted on the 30 videos of the MCL-JCV dataset, resized to a resolution of 640$\\times$360.\nSixteen raters are engaged, each providing responses to an average of roughly 800 pairwise-preference questions.\n\\newpage\n\nWe calculate Elo scores~\\citep{elo1978rating} based on pairwise preferences to quantify the relative visual quality between the models. The study compares our model with MAGVIT as well as the current video compression standard HEVC (H.265) video codec~\\citep{sullivan2012overview} and the next-generation codec VVC (H.266)~\\citep{vvc}. As shown in \\cref{fig:compression}, raters prefer our model to the compared methods at multiple bit rates.\n\n\\begin{wraptable}{r}{0.45\\textwidth}\n\\vspace{-10mm}\n    \\centering\n    \\caption{\\textbf{Video compression metrics}.\n    }\n    \\label{tab:compression}\n    \\vspace{-2mm}\n    \\resizebox{\\linewidth}{!}{%\n    \\begin{tabular}{l@{\\hspace{5pt}}c@{\\hspace{5pt}}c@{\\hspace{5pt}}c@{\\hspace{5pt}}c}\n    \\toprule\n    Method  & LPIPS$\\downarrow$ & PSNR$\\uparrow$ & MS-SSIM$\\uparrow$ \\\\\n    \\midrule\n    HEVC~\\citep{sullivan2012overview} & 0.199& 30.10 & 0.943 \\\\ \n    VVC~\\citep{vvc} & 0.153 & \\textbf{32.65} & \\textbf{0.966} \\\\\n    \\midrule\n    MAGVIT~\\citep{yu2022magvit} & 0.144 & 23.70 & 0.846 \\\\ \n    \\emph{\\modelname{} (this paper)} & \\textbf{0.104} & 26.18 & 0.894 \\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n    \\vspace{-8mm}\n\\end{wraptable}\n\nWe also compare the compression quality using common distortion metrics (LPIPS, PSNR, and MS-SSIM). \n\\cl{\\cref{tab:compression} compares at 0.0384 bpp, the bit rate of MAGVIT, with full curves in the Appendix}.\nThe results show that our model outperforms MAGVIT on all metrics, and it outperforms all methods on LPIPS, a metric which correlates more closely with subjective quality assessments.\n\\cl{\nAt equal bit rates, standard codecs may render local details\n more accurately than neural models but  also introduce block\n artifacts,  detrimental to  perceptual quality yet not captured by PSNR and MS-SSIM~\\citep{agustsson2019generative}.\n}\n\\cl{Despite promising results with TPUs, further research is needed to adapt our model to run efficiently on CPUs like standard codecs.}\n\n\\setlength\\intextsep{20pt}\n\\begin{wraptable}{r}{0.5\\textwidth}\n\\centering\n\\caption{\\textbf{Video action recognition performance}\\\\(classification accuracy$\\uparrow$ $\\times$100).}\n\\label{tab:understanding}\n\\vspace{-2mm}\n\\resizebox{\\linewidth}{!}{%\n\\begin{tabular}{@{}l@{\\hspace{5pt}}c@{\\hspace{10pt}}c@{\\hspace{5pt}}c@{\\hspace{5pt}}c@{}}\n\\toprule\n\\makecell[r]{Token as transformer's:} & Output & \\multicolumn{3}{c}{Input}  \\\\\nTokenizer & SSv2 & SSv2 & K400 & K600 \\\\ \\midrule\n3D VQ-VAE & 64.13 & 41.27 & 44.44 & 45.67 \\\\\nMAGVIT~\\citep{yu2022magvit} & 67.22 & 57.34 & 72.29 & 74.65 \\\\\n\\emph{\\modelname{} (this paper)} & \\textbf{67.38} & \\textbf{62.40} & \\textbf{75.34} & \\textbf{77.93} \\\\ \\midrule\n\\textcolor{gray}{Raw pixel} & \\textcolor{gray}{\\cl{64.83}} & \\textcolor{gray}{63.08} & \\textcolor{gray}{76.13} & \\textcolor{gray}{78.92} \\\\ \n\\textcolor{gray}{\\cl{HoG descriptor~\\citep{wei2022masked}}} & \\textcolor{gray}{\\cl{65.86}} & \\textcolor{gray}{\\cl{n/a}} & \\textcolor{gray}{\\cl{n/a}} & \\textcolor{gray}{\\cl{n/a}} \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\vspace{-14mm}\n\\end{wraptable}\n\\vspace{-8mm}\n\\leavevmode\\subsection{Video Understanding}\n\\vspace{-2mm}\nIn this subsection, we assess the tokenizer's capability to learn a video understanding model for action recognition. Two setups are examined: (1) using tokens as prediction targets for the transformer's output, and (2) using tokens as the input to the transformer. For the former setup, we use a similar architecture following the BEVT~\\citep{wang2022bevt} pre-training. For the tokens as inputs, to work with the ViViT backbone~\\citep{arnab2021vivit}, we detokenize the tokens to pixels before feeding them to \\cl{frozen} ViViT transformers \\cl{trained on raw pixels}.\n\n\\cref{tab:understanding} shows that \\modelname{} outperforms the previous best MAGVIT in these evaluations. Specifically, when using the decoded tokens as input, the performance approaches that of the model trained with ground-truth pixels using the same ViViT backbone. While these numbers are still worse than the state-of-the-art in action recognition, they represent solid improvements credited to the new tokenizer.\n\n\\vspace{-2mm}\n\\subsection{Ablation Study}\n\\vspace{-2mm}\nIn \\cref{fig:motivation}, we have ablated \\quantizername{} \\vs VQ and the vocabulary size.\nIn \\cref{tab:ablation}, we validate the key designs proposed in \\cref{sec:tokenizer_improvement}. Specifically, \\cref{tab:ablation_arch} compares the architecture illustrated in \\cref{fig:arch}; \\cref{tab:ablation_imagenet} and \\cref{tab:ablation_ucf101} verify the \\quantizername{} and other improvements on ImageNet and UCF-101, respectively.\n\n\\begin{table}[tp]\n\\vspace{-4mm}\n\\centering\n\\scriptsize\n\\caption{\\textbf{Ablation study verifying key design choices}.}\n\\label{tab:ablation}\n\\vspace{-2mm}\n\\begin{subtable}{0.36\\linewidth}\n\\centering\n\\caption{\\label{tab:ablation_arch}Causal architectures on UCF-101. FID is calculated on the first frame.}\n\n\\begin{tabular}{@{}l@{\\hspace{2pt}}c@{\\hspace{2pt}}c@{\\hspace{2pt}}c@{}}\n\\toprule\n & \\#Params & FID$\\downarrow$ & FVD$\\downarrow$ \\\\ \\midrule\n\\textcolor{gray}{MAGVIT} & \\textcolor{gray}{39M} & \\textcolor{gray}{n/a} & \\textcolor{gray}{107.15}  \\\\\nC-ViViT & 90M & 28.02 & 437.54 \\\\\nC-ViViT + MAGVIT & 67M & 13.52  & 316.70 \\\\\n\\makecell[l]{\\emph{\\modelname{}}:\\\\\\ \\ Causal 3D CNN} & 58M & \\textbf{7.06} & \\textbf{96.33} \\\\ \\bottomrule\n\\end{tabular}\n\\end{subtable}\n\\hfill\n\\begin{subtable}{0.3\\linewidth}\n\\centering\n\\caption{\\label{tab:ablation_imagenet}Image tokenization on\\\\ImageNet 128$\\times$128.}\n\\begin{tabular}{@{}l@{\\hspace{2pt}}c@{\\hspace{2pt}}c@{}}\n\\toprule\n & FID$\\downarrow$ & LPIPS$\\downarrow$ \\\\ \\midrule\nMAGVIT & 2.65 & 0.1292 \\\\\n+ LFQ & 2.48 & 0.1182 \\\\\n+ large vocabulary & 1.34 & 0.0821 \\\\\n+ up/downsampler & 1.21 & 0.0790 \\\\\n+ deeper model & 1.20 & 0.0686 \\\\\n+ adaptive normalization & \\textbf{1.15} & \\textbf{0.0685} \\\\ \\bottomrule\n\\end{tabular}\n\\end{subtable}\n\\hfill\n\\begin{subtable}{0.33\\linewidth}\n\\centering\n\\caption{\\label{tab:ablation_ucf101}Video tokenization on UCF-101.}\n\\begin{tabular}{@{}l@{\\hspace{2pt}}c@{\\hspace{2pt}}c@{}}\n\\toprule\n & FVD$\\downarrow$ & LPIPS$\\downarrow$ \\\\ \\midrule\nMAGVIT & 24.55 & 0.0988 \\\\\n+ LFQ \\& large vocabulary & 16.12 & 0.0694 \\\\\n+ up/downsampler & 15.37 & 0.0678 \\\\\n+ late temporal downsample & 11.11 & 0.0653 \\\\\n+ deeper model & 8.90 & 0.0542 \\\\\n+ 3D blur pooling & \\textbf{8.62} & \\textbf{0.0537} \\\\ \n\\bottomrule \n\\end{tabular}\n\\end{subtable}\n\\vspace{-4mm}\n\\end{table}\n\n\\vspace{-4mm}\n\\section{Related Work}\n\\vspace{-4mm}\n\\paragraph{Visual tokenization.}\nBeyond the VQ-VAE models discussed in \\cref{sec:background}, additional models have been proposed. ViT-VQGAN \\citep{yu2021vector} introduces transformer blocks as a substitute for CNNs for image tokenization. C-ViViT \\citep{villegas2022phenaki} further extends this idea for video tokenization. %Tokenizing video is more complex since it requires modeling the visual dynamics within the compressed spatial-temporal latent space. \nEarly studies on video tokenization treat frames as independent images with no temporal compression~\\citep{wu2021n,gupta2022maskvit}. Later research \\citep{yan2021videogpt,ge2022long,yu2022magvit} integrates 3D CNNs to tokenize spatial-temporal volumes. Despite these advances in vector quantization (VQ), the codebook learned by previous VQ models is relatively small (\\eg, $8$k) due to the difficulty in improving the generation quality with larger vocabularies. In contrast, our tokenizer can induce a large vocabulary (\\eg, $262$k) that can be effectively modeled by an LM, leading to enhanced image and video generation quality. \n\n\\vspace{-4mm}\n\\paragraph{Text-to-\\{image, video\\}.}\nText-to-image and text-to-video generation has garnered significant rapid advancements using both language models~\\citep{yu2023scaling,chang2023muse} and diffusion models~\\citep{ho2022imagen,blattmann2023align,singer2022make,ge2023preserve,ramesh2022hierarchical}. Although diffusion models, such as Midjourney, are considered the top performers in these tasks, it is unclear whether their advantage stems from the model, data, or some other unidentified factors. Indeed, it is challenging to scientifically compare these text-to-image models as they are trained on varied datasets, with some even being proprietary data, under inconsistent training conditions. To facilitate a fairer comparison, this paper prioritizes using the ImageNet and Kinetics benchmarks.\n\n\\vspace{-4mm}\n\\paragraph{Diffusion models.}\nExhibiting high quality sampling, pixel-space diffusion models \\citep{sohl2015deep,song2019generative,ho2020denoising} raised to the top of the generative modeling space for both image \\citep{ho2020denoising,dhariwal2021diffusion,saharia2022photorealistic} and video \\citep{ho2022video,ho2022imagen,singer2022make} synthesis. The pixel-space denoising diffusion models (DDMs) are later refined by the latent-space DDM \\citep{rombach2022high}, which conducts diffusion over the \\emph{continuous} latent embeddings derived from a pre-trained variational autoencoder (VAE).\nBinary latents for image modeling were used in \\cite{wang2023binary}, where the diffusion process is parameterized with Bernoulli distributions.\nRecent studies have identified advantages in substituting the U-Net~\\citep{ronneberger2015u} denoising backbone with a Transformer \\citep{peebles2022scalable,jabri2023scalable} or a hybrid of both \\citep{hoogeboom2023simple}, making the distinctions between diffusion and language models in visual generation more blurred, with a key distinction being their latent format — continuous for diffusion and discrete for language models.\n\n\\vspace{-4mm}\n\\section{Conclusion and Future Work}\n\\vspace{-3mm}\n\nWe introduce \\modelname{}, a novel video tokenizer that exploits lookup-free quantization along with architectural advancements to tokenize images and video with a shared vocabulary. The experiments show that our tokenizer outperforms the previously leading video tokenizer across three areas: visual generation, video compression, and action recognition in videos. Our results suggest that a good visual tokenizer is key for enabling language models to excel in image and video generation. These results demonstrate the great capabilities of LMs in visual generation, and advocate for further exploration of advanced visual tokenization methods designed for LLMs.\n\n\\subsubsection*{Acknowledgments}\nWe would like to express our gratitude to Yu-Chuan Su and Sergey Ioffe for their valuable comments on our work, to Josh Dillon for discussions, \nand to Eirikur Agustsson for help in compression evaluation.\n\n\\clearpage\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2202.04200v1.tex",
        "arXiv-2301.11093v2.tex",
        "arXiv-2310.05737v3.tex"
    ],
    "group_id": "group_41",
    "response": "### Title: Innovations in Image and Video Generation Using Language Models and Diffusion Models\n\n### Introduction\n\nThe field of generative models, particularly in the context of image and video synthesis, has seen significant advancements in recent years. Initially, Generative Adversarial Networks (GANs) were the leading models in this domain, known for their ability to generate high-fidelity images at blazing speeds. However, they faced challenges such as training instability and mode collapse, which limited the diversity of the generated samples. Likelihood-based models, including Variational Autoencoders (VAEs) and autoregressive models, have offered a more stable training process and improved distribution coverage, but they often struggle with modeling long-term correlations and are slower in generating images. Diffusion models, which have gained popularity due to their high visual quality and flexibility, have also been used for image and video synthesis. These models work by gradually adding noise to data and then learning to reverse this process to generate new data. However, diffusion models typically operate in latent spaces or use multiple stages of super-resolution to handle high-resolution images, adding complexity to the model.\n\nCurrent research is focused on simplifying these models while maintaining high performance in generating high-resolution images and videos. This paper reviews three recent research papers that propose innovative methods to improve image and video generation using language models and diffusion models. The first paper introduces Masked Generative Image Transformer (MaskGIT), a novel bidirectional transformer for image synthesis that significantly outperforms existing state-of-the-art models in terms of both speed and quality. The second paper, \"simple diffusion,\" aims to simplify the diffusion model for high-resolution images, achieving state-of-the-art performance without the need for additional stages or complex architectures. The third paper, \"Language Model Beats Diffusion,\" explores the use of a new visual tokenizer for language models, demonstrating that with the right tokenizer, language models can outperform diffusion models in image and video generation.\n\n### Main Content of Each Paper\n\n#### Paper 1: Masked Generative Image Transformer (MaskGIT)\n\nMaskGIT proposes a new image synthesis paradigm using a bidirectional transformer decoder. The model is trained on a masked visual token modeling (MVTM) task, where it learns to predict randomly masked tokens by attending to tokens in all directions. During inference, it generates all tokens of an image simultaneously and refines the image iteratively, conditioned on the previous generation. This method contrasts with the traditional autoregressive decoding approach, which generates tokens sequentially based on previously generated tokens. MaskGIT's multidirectional nature allows it to handle image manipulation tasks more effectively, such as class-conditional image editing, inpainting, and extrapolation, without requiring task-specific training or modifications.\n\nThe key innovation of MaskGIT lies in its iterative decoding process. It starts with a blank canvas where all tokens are masked out and gradually fills the latent representation with more and more scattered predictions. The mask ratio is decreased over iterations until all tokens are generated. This process is significantly faster than autoregressive decoding, as it takes only 8 iterations to generate an image, compared to 256 iterations for traditional methods. The model's performance is evaluated on the ImageNet dataset, where it achieves state-of-the-art results in terms of Fréchet Inception Distance (FID) and Inception Score (IS) for both 256x256 and 512x512 resolutions. Additionally, MaskGIT establishes new state-of-the-arts in Classification Accuracy Score (CAS) for image generation.\n\n#### Paper 2: Simple Diffusion\n\nThis paper focuses on improving denoising diffusion models for high-resolution images while keeping the model as simple as possible. It addresses the challenge of adding sufficient noise to high-resolution images by proposing a shifted noise schedule, which increases the signal-to-noise ratio (SNR) at higher resolutions. The shifted schedule is derived from a reference resolution, typically 64x64, and is adjusted for higher resolutions by a simple shift in log-space. The paper also introduces a multiscale training loss that evaluates the standard training loss at downsampled resolutions with increasing weighting for lower resolutions, enabling quicker convergence at resolutions greater than 256x256.\n\nAnother key aspect of the paper is the scaling of the architecture. Instead of scaling the entire architecture, the paper suggests scaling only the lower resolution feature maps, specifically the 16x16 resolution, to improve performance while managing memory constraints. This approach is more efficient and allows for better utilization of accelerators. Additionally, the paper proposes using dropout only on lower resolution feature maps to avoid overfitting, which is a common issue in high-resolution image generation. The method is validated on the ImageNet dataset, where it achieves state-of-the-art FID scores without any sampling modifiers, demonstrating the effectiveness of the proposed modifications.\n\n#### Paper 3: Language Model Beats Diffusion\n\nThe third paper introduces a new visual tokenizer, \\modelname{}, designed to map images and videos into discrete tokens suitable for large language models (LLMs). The tokenizer uses a lookup-free quantization (LFQ) method, which eliminates the need for embedding lookups and allows for a larger vocabulary size. This larger vocabulary size improves the generation quality of LLMs, as shown in experiments on ImageNet and video generation benchmarks. The paper also explores architectural modifications to the MAGVIT model, such as using causal 3D CNN layers and adaptive group normalization, to further enhance the tokenizer's performance.\n\nThe main contribution of this paper is demonstrating that with the right tokenizer, LLMs can outperform diffusion models in image and video generation. The experiments show that \\modelname{} surpasses the previous best video tokenizer, MAGVIT, in video generation, compression, and action recognition tasks. For image generation, \\modelname{} achieves state-of-the-art FID scores and Inception Scores (IS) on ImageNet 512x512, outperforming diffusion models like ADM and DiT-XL/2. The paper also includes a subjective rater study on video compression, where \\modelname{} is preferred over standard codecs like HEVC and VVC at multiple bit rates, indicating its potential for practical applications in video compression.\n\n### Commonalities and Innovations\n\nAll three papers address the challenges of generating high-quality images and videos at high resolutions. They explore different aspects of the generative modeling process, including the use of transformers, noise schedules, and tokenization methods. MaskGIT introduces a novel bidirectional transformer for image synthesis, which significantly outperforms existing autoregressive transformer models in terms of both speed and quality. Simple diffusion simplifies the diffusion model for high-resolution images by proposing a shifted noise schedule and a multiscale training loss, achieving state-of-the-art FID scores without additional stages or complex architectures. Language Model Beats Diffusion highlights the importance of a good visual tokenizer for LLMs, demonstrating that with the right tokenizer, LLMs can outperform diffusion models in image and video generation.\n\nThe common theme across these papers is the use of transformers and tokenization methods to improve the efficiency and quality of image and video generation. MaskGIT and Language Model Beats Diffusion both utilize tokenization methods, with MaskGIT focusing on masked modeling and iterative decoding, and Language Model Beats Diffusion introducing a new visual tokenizer with lookup-free quantization. Simple diffusion, on the other hand, focuses on simplifying the diffusion model for high-resolution images by adjusting the noise schedule and training loss, and scaling the architecture in a way that is more memory-efficient.\n\n### Comparison of Results\n\nThe results of the three papers are compared in terms of quality, efficiency, and diversity. MaskGIT achieves state-of-the-art FID scores and Inception Scores on ImageNet for both 256x256 and 512x512 resolutions, outperforming existing models like VQGAN and ADM. It also establishes new state-of-the-arts in Classification Accuracy Score (CAS) for image generation. Simple diffusion achieves state-of-the-art FID scores on ImageNet without any sampling modifiers, demonstrating the effectiveness of the proposed modifications in simplifying the diffusion model for high-resolution images. Language Model Beats Diffusion outperforms the previous best video tokenizer, MAGVIT, in video generation, compression, and action recognition tasks, and achieves state-of-the-art FID scores and Inception Scores on ImageNet 512x512, outperforming diffusion models like ADM and DiT-XL/2.\n\nTable~\\ref{tab:comparison} summarizes the performance of the models across different tasks and resolutions:\n\n\\begin{table}[h]\n\\centering\n\\caption{Comparison of Models Across Different Tasks and Resolutions}\n\\label{tab:comparison}\n\\begin{tabular}{l|c|c|c|c|c|c}\n\\toprule\n\\textbf{Model} & \\textbf{Task} & \\textbf{Resolution} & \\textbf{FID} & \\textbf{IS} & \\textbf{\\#Params} & \\textbf{\\#Steps} \\\\\n\\midrule\nMaskGIT & Image Synthesis & 256x256 & 6.18 & 182.1 & 227M & 8 \\\\\nMaskGIT & Image Synthesis & 512x512 & 7.32 & 156.0 & 227M & 12 \\\\\nSimple Diffusion & Image Synthesis & 256x256 & 3.76 & 205.3 & 2B & 512 \\\\\nSimple Diffusion & Image Synthesis & 512x512 & 3.54 & 248.7 & 2B & 512 \\\\\nLanguage Model Beats Diffusion & Image Synthesis & 512x512 & 1.91 & 324.3 & 307M & 64 \\\\\nLanguage Model Beats Diffusion & Video Generation & Kinetics-600 & 5.2 & 213.1 & 307M & 12 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\nMaskGIT and Simple Diffusion both achieve state-of-the-art FID scores on ImageNet, with MaskGIT outperforming Simple Diffusion on 512x512 resolution but requiring more parameters and steps. Language Model Beats Diffusion, however, achieves the best FID scores on ImageNet 512x512 with the smallest model size and number of steps, indicating its efficiency and effectiveness in generating high-quality images. For video generation, Language Model Beats Diffusion outperforms MAGVIT, demonstrating the importance of a good visual tokenizer for LLMs.\n\n### Conclusion\n\nThe reviewed papers highlight significant advancements in image and video generation using transformers and tokenization methods. MaskGIT introduces a bidirectional transformer for image synthesis, achieving state-of-the-art results in terms of quality and speed. Simple diffusion simplifies the diffusion model for high-resolution images by proposing a shifted noise schedule and multiscale training loss, demonstrating the importance of adjusting the noise schedule for higher resolutions. Language Model Beats Diffusion shows that with the right visual tokenizer, LLMs can outperform diffusion models in image and video generation, establishing the state-of-the-art on ImageNet 512x512 and Kinetics-600 benchmarks.\n\nThese findings suggest that the use of transformers and tokenization methods can significantly improve the efficiency and quality of image and video generation models. Future research directions include further exploration of visual tokenization methods for LLMs, as well as the integration of these methods with diffusion models to create hybrid models that leverage the strengths of both approaches. Additionally, the development of more efficient decoding algorithms for transformers and the exploration of new noise schedules for diffusion models are promising areas for future work."
}