{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Optimal Completion Distillation for Sequence Learning}\n\n\\begin{document}\n\n\\maketitle\n\n\\vspace*{-.2cm}\n\\begin{abstract}\n\\vspace*{-.2cm}\nWe present {\\em Optimal Completion Distillation} (OCD), a training procedure for optimizing\nsequence to sequence models based on edit distance. OCD is efficient, has no hyper-parameters\nof its own, and does not require pretraining or joint optimization with conditional log-likelihood.\nGiven a partial sequence generated by the model, we first identify the set of optimal suffixes\nthat minimize the total edit distance, using an efficient dynamic programming algorithm.\n Then, for each position of the generated sequence, we define a target distribution that puts an equal\n probability on the first token of each optimal suffix. OCD achieves the state-of-the-art\n performance on end-to-end speech recognition, on both Wall Street Journal and Librispeech datasets,\n achieving $9.3\\%$ and \\rebut{$\\libclean\\%$} word error rates, respectively.\n\n\\comment{Then, at each position of the generated sequence, we encourage the model to continue with the set of optimal extensions.}\n\\comment{we develop a dynamic programming algorithm to exactly \nAt each position in the sequence, we teach the model to mimic the optimal suffixes.\n}\n\n\\comment{,\noutperforming all previous techniques, including our own well tuned teacher forcing and scheduled sampling baselines.}\n\n\\comment{Neural sequence models have revolutionized multiple application domains.\nThese models have been predominantly trained via teacher forcing and conditional log-likelihood.}\n\n\\comment{\n\nNeural sequence models have revolutionized multiple application domains.\nThese models have been predominantly trained via teacher forcing and conditional log-likelihood.\nWe present Optimal Completion Distillation (OCD), a training procedure that does not require initialization nor joint optimization with maximum likelihood.\nGiven any arbitrary prefix, (e.g. a partial sequence generated by sampling from the model), we develop an efficient dynamic programming algorithm to compute the optimal actions that minimizes the total edit distance.\nAt each position in the sequence, we distill the optimal completion policy to the model.\nWe achieve the state-of-the-art performance on the Wall Street Journal speech recognition task, achieving 3.1\\% CER and 9.3\\% WER,\noutperforming all other previous work, including our own well tuned teacher forcing and scheduled sampling baselines.\n\nNeural sequence models have revolutionized multiple application domains.\nThese models have been predominantly trained via teacher forcing and conditional log-likelihood.\nWe present Optimal Completion Distillation (OCD), a training procedure that does not require initialization nor joint optimization with conditional likelihood.\nGiven any arbitrary prefix, (e.g. a partial sequence generated by sampling from the model), we develop an efficient dynamic programming algorithm to compute the optimal tokens that minimizes the total edit distance.\nAt each position in the sequence, we distill the optimal completion policy into the model.\nWe achieve the state-of-the-art performance on the Wall Street Journal speech recognition task, achieving 3.1\\% CER and 9.3\\% WER,\noutperforming all other previous work, including our own well tuned teacher forcing and scheduled sampling baselines.\n\n}\\vspace*{-.2cm}\n\\end{abstract}\n\n\\vspace*{-.2cm}\n\\section{Introduction}\n\\vspace*{-.1cm}\n\\label{sec:intro}\nRecent advances in natural language processing and speech recognition\nhinge on the development of expressive neural network architectures for\nsequence to sequence (seq2seq)\nlearning~\\citep{sutskeveretal14,bahdanau2014neural}. Such\nencoder-decoder architectures are adopted in both machine translation~\\citep{bahdanau2014neural,wu2016google,parity2018microsoft} and speech recognition systems~\\citep{chan-icassp-2016,bahdanau-icassp-2016,chiu2017state} achieving impressive performance above traditional multi-stage pipelines~\\citep{koehn2007moses,povey2011kaldi}. %,hinton2012deep\nImproving the building blocks of seq2seq models can fundamentally advance machine translation and speech\nrecognition, and positively impact other domains such as image captioning \\citep{showattendtell2015},\nparsing \\citep{vinyals-nips-2015}, summarization \\citep{rush-emnlp-2015}, and program\nsynthesis \\citep{seq2sql}.\n\nTo improve the key components of seq2seq models, one can either design better architectures,\nor develop better learning algorithms. Recent\narchitectures using convolution~\\citep{gehring-arxiv-2017} and self\nattention~\\citep{transformer2017} have proved to be useful, especially\nto facilitate efficient training. On the other hand, despite many attempts to\nmitigate the limitations of Maximum Likelihood Estimation (MLE)\n\\citep{ranzato-iclr-2016,wiseman-emnlp-2016,norouzi-nips-2016,bahdanau-iclr-2017,searnn2017},\nMLE is still considered the dominant approach for training seq2seq\nmodels\\comment{ \\new{from scratch} \\mohammad{it is also true more generally, no?}}. Current alternative approaches require pre-training or \njoint optimization with conditional log-likelihood. They are difficult to implement and\nrequire careful tuning of new hyper-parameters (\\eg~mixing ratios). In addition, alternative\napproaches typically do not offer a substantial performance improvement over a well tuned MLE\nbaseline, especially when label smoothing~\\citep{pereyra-iclr-2017,edunov2017classical} and\nscheduled sampling~\\citep{bengio-nips-2015} are used.\n\nIn this paper, we borrow ideas from search-based structured prediction~\\citep{daumeetal09,ross-aistats-2011}\nand policy distillation~\\citep{rusu-iclr-2016} and develop an efficient algorithm for optimizing seq2seq models\nbased on edit distance\\footnote{Edit distance between two sequences $\\bu$ and $\\bv$ is the minimum number of insertion,\ndeletion, and substitution edits required to convert $\\bu$ to $\\bv$ and {\\em vice versa}.}. Our key observation is that\ngiven an arbitrary prefix (\\eg~a partial sequence generated by sampling from the model), we can {\\em exactly} and {\\em efficiently}\nidentify all of the suffixes that result in a minimum total edit distance (\\vs~the ground truth target). Our training procedure,\ncalled {\\em Optimal Completion Distillation (OCD)}, is summarized as follows:\n\\begin{enumerate}[noitemsep,parsep=0pt,leftmargin=7mm]\n\\comment{    \\item We always train on output sequences generated by sampling from the seq2seq model.}\n    \\item We always train on prefixes generated by sampling from the model that is being optimized.\n    \\item For each generated prefix, we identify all of the optimal suffixes that\n    result in a minimum total edit distance \\vs~the ground truth target using an efficient dynamic programming algorithm.\n    \\item We teach the model to {\\em optimally extend} each generated prefix\n    by maximizing the average log probability of the first token of each optimal suffix identified in step 2.\n\\end{enumerate}\n\\comment{In sum, during training, we distill the knowledge of the optimal completions into a seq2seq model.\nDuring inference, we resort to the standard beam search process.} The proposed OCD algorithm is efficient,\nstraightforward to implement, and has no tunable hyper-parameters of its own.\n\\comment{Further, OCD provides a significant performance gain over a well tuned MLE baseline on two highly competitive\nspeech recognition benchmarks.}Our key contributions include:\n\\begin{itemize}[noitemsep,parsep=0pt,leftmargin=7mm]\n    \\item\n    We propose \\acronym, a stand-alone algorithm for optimizing seq2seq models based on\n    edit distance. \\acronym is scalable to real-world datasets with long sequences and large vocabularies, and consistently\n    outperforms Maximum Likelihood Estimation (MLE) \\rebut{by a large margin}. \n    \\item Given a target sequence of length $m$ and a generated sequence of length $n$, we present an $O(nm)$ algorithm\n    that identifies all of the optimal extensions for each prefix of the generated sequence.\n    \\item We demonstrate the effectiveness of \\acronym on end-to-end speech\n    recognition using attention-based seq2seq models. On the Wall Street Journal\n    dataset, \\acronym achieves a Character Error Rate (CER) of $3.1\\%$ and a Word Error Rate (WER) of\n    $9.3\\%$ without language model rescoring, outperforming all prior work \\rebut{(\\tabref{tab:results})}. % such as \\citep{bahdanau-icassp-2016,chorowski-interspeech-2017,chan-iclr-2017}.\n    On Librispeech, \\acronym achieves state-of-the-art WER of \\rebut{$\\libclean\\%$ on ``test-clean'' and $\\libother\\%$ on ``test-other'' sets (\\tabref{tab:lib})}.\n\\end{itemize}\n\n\\comment{\n\nThese models are often trained to maximize the log-probability of the\ncorrect output sequence using tractable autoregressive models. Such\nmodels generate the output sequence one token at a time, typically in\na left-to-right fashion.\n\n{\\em Teacher forcing}~\\citep{teacherforcing1989} for the optimization\nof the{\\em~conditional log-likelihood} objective is the standard\ntraining procedure. A teacher provides a student with a{\\em~prefix}\nof the correct output sequence, $(y^*_1, \\ldots, y^*_{i-1})$, and the\nstudent optimizes the log-probability of the next correct token,\n$y^*_{i}$. This resembles a teacher walking a student through a\nsequence of perfect decisions, where the student learns as a passive\nobserver. However, during inference, the student needs to act\nautonomously. To generate a token, $\\hat{y}_i$, the student needs to\ncondition on their own previous outputs,\n$(\\hat{y}_1, \\ldots, \\hat{y}_{i-1})$, since the correct outputs are\nnot available anymore.\n\nWe highlight two problems with teacher forcing and Maximum Likelihood\nEstimation (MLE):\n\\begin{enumerate}\n    \\item There is a mismatch between the prefixes seen by the model\n    during training and inference. If the student's predictions\n    deviate from the correct outputs, \\ie~when the distribution of\n    $(\\hat{y}_1, \\ldots, \\hat{y}_{i-1})$ is different from the\n    distribution of $(y^*_1, \\ldots, y^*_{i-1})$, then the student may\n    find themselves in a novel situation that they have not been\n    trained for. The mismatch between the training and test\n    distributions can result in poor generalization, especially when\n    the training set is small or the model size is large.\n\n    \\item There is a mismatch between the training loss and the task\n    objective. During training, one optimizes the log-probability of\n    the correct output sequence, however this is often different from\n    the task evaluation metric (\\eg~edit distance or BLEU).\n\\end{enumerate}\n\nThere has been a recent surge of interest in understanding and\nmitigating the limitations of teacher forcing and the log-likelihood\nobjective\n(\\eg~\\cite{bengio-nips-2015,ranzato-iclr-2016,wiseman-emnlp-2016,norouzi-nips-2016,bahdanau-iclr-2017}). We\nrefer the reader to \\secref{sec:relatedwork} for a detailed discussion\nof prior work. We present a novel solution to both of the problems\ndiscussed above based on reinforcement learning techniques, while\nmaintaining the efficiency and effectiveness of teacher forcing. Our\nalgorithm is easy to implement, and unlike most alternative\napproaches, does not require initialization nor joint training with\nMLE.\n\nOur key intuition is that a competent teacher should not train a\nstudent only on correct prefixes, but should also teach the student\nabout the best set of next tokens for an incorrect prefix. This is\nespecially helpful when the prefix includes some of the student's\nmistakes. We generate a full sequence from the student, and at each\nposition, we train the student to predict the next set of tokens that\nwould result in an optimal completion. Our key technical contribution\nis the development of a dynamic programming algorithm to find the\noptimal completion for any prefix to minimize the total token error\nrate. Our training procedure, {\\em Optimal Completion Distillation}\n(\\acronym), distills the knowledge of the optimal completions into the\nstudent.\n\nWe demonstrate the effectiveness of \\acronym on end-to-end speech\nrecognition using attention-based sequence-to-sequence models.  We\nadopt the challenging Wall Street Journal dataset and compare \\acronym\nagainst various previous techniques, including our well-tuned\nimplementation of teacher forcing and scheduled sampling. \\acronym\nachieves the state-of-the-art end-to-end speech recognition\nperformance with $3.1\\%$ CER and $9.4\\%$ WER, outperforming all prior\nwork\nincluding \\citep{bahdanau-icassp-2016,chorowski-interspeech-2017,chan-iclr-2017}.\n\n}\n\n\\comment{\nNeural sequence\nmodels \\citep{sutskeveretal14,bahdanau2014neural,gehring-arxiv-2017}\nhave seen remarkable success across many application domains including\nmachine translation \\citep{wu2016google,parity2018microsoft},\nspeech recognition \\citep{deepspeech2016,chan-icassp-2016}, and image\ncaptioning \\citep{showtell2015,showattendtell2015}.\nThese models are often trained to maximize the log-probability of the\ncorrect output sequence using tractable autoregressive models. Such\nmodels generate the output sequence one token at a time, typically in\na left-to-right fashion.\n\n{\\em Teacher forcing}~\\citep{teacherforcing1989} for the optimization\nof the {\\em conditional log-likelihood} objective is the standard\ntraining procedure. A teacher provides a student with a {\\em prefix}\nof the correct output sequence, $(y^*_1, \\ldots, y^*_{i-1})$, and the\nstudent optimizes the log-probability of the next correct token,\n$y^*_{i}$. This resembles a teacher walking a student through a\nsequence of perfect decisions, where the student learns as a passive\nobserver. However, during inference, the student needs to act\nautonomously. To generate a token, $\\hat{y}_i$, the student needs to\ncondition on their own previous outputs,\n$(\\hat{y}_1, \\ldots, \\hat{y}_{i-1})$, since the correct outputs are\nnot available anymore.\n\nWe highlight two problems with teacher forcing and Maximum Likelihood Estimation (MLE):\n\\begin{enumerate}\n    \\item There is a mismatch between the prefixes seen by the model during training and inference. If the student's predictions deviate from the correct outputs, \\ie~when the distribution of $(\\hat{y}_1, \\ldots, \\hat{y}_{i-1})$ is different from the distribution of $(y^*_1, \\ldots, y^*_{i-1})$, then the student may find themselves in a novel situation that they have not been trained for. The mismatch between the training and test distributions can result in poor generalization, especially when the training set is small or the model size is large.\n    \\item There is a mismatch between the training loss and the task objective. During training, one optimizes the log-probability of the correct output sequence, however this is often different from the task evaluation metric (\\eg~edit distance or BLEU).\n\\end{enumerate}\n\nThere has been a recent surge of interest in understanding and mitigating the limitations of teacher forcing and the log-likelihood objective \\citep{bengio-nips-2015,ranzato-iclr-2016,wiseman-emnlp-2016,norouzi-nips-2016,bahdanau-iclr-2017}. We refer the reader to \\secref{sec:relatedwork} for a detailed discussion of prior work. We present a novel solution to both of the problems discussed above based on reinforcement learning techniques, while maintaining the efficiency and effectiveness of teacher forcing. Our algorithm is easy to implement, and unlike most alternative approaches, does not require initialization nor joint training with MLE.\n\nOur key intuition is that a competent teacher should not train a student only on correct prefixes, but should also teach the student about the best set of next tokens for an incorrect prefix. This is especially helpful when the prefix includes some of the student's mistakes. We generate a full sequence from the student, and at each position, we train the student to predict the next set of tokens that would result in an optimal completion. Our key technical contribution is the development of a dynamic programming algorithm to find the optimal completion for any prefix to minimize the total token error rate. Our training procedure, {\\em Optimal Completion Distillation} (\\acronym), distills the knowledge of the optimal completions into the student.\n\nWe demonstrate the effectiveness of \\acronym on end-to-end speech recognition using attention-based sequence-to-sequence models.\nWe adopt the challenging Wall Street Journal dataset and compare \\acronym against various previous techniques, including our well-tuned\nimplementation of teacher forcing and scheduled sampling. \\acronym achieves the state-of-the-art end-to-end speech recognition performance with $3.1\\%$ CER and $9.4\\%$ WER, outperforming all prior work including \\citep{bahdanau-icassp-2016,chorowski-interspeech-2017,chan-iclr-2017}.\n}\n\n\\vspace*{-.2cm}\n\\section{Background: Sequence Learning with MLE}\n\\vspace*{-.1cm}\nGiven a dataset of input output pairs $\\mathcal{D} \\equiv \\{(\\bx, \\bys)_i\\}_{i=1}^N$,\nwe are interested in learning a mapping $\\bx \\to \\by$ from an input $\\bx$ to a target output sequence $\\bys \\in \\mathcal{Y}$.\nLet $\\mathcal{Y}$ denote the set of all sequences of tokens from a finite vocabulary $\\V$ with variable but finite lengths.\n\\comment{\n$= (y^*_1, y^*_2, ...,y^*_T)$ of length $T$.\nWe assume that the tokens in $\\bys$ are from a known finite vocabulary. Let $\\by \\in \\mathcal{Y}$ denote a member of the set of all possible output sequences.}\nOften learning a mapping $\\bx \\to \\by$ is formulated as optimizing the parameters of a conditional distribution $\\pit(\\ba \\mid \\bx)$. Then, the final sequence prediction under the probabilistic model $\\pit$ is performed by exact or approximate\ninference (\\eg~via beam search) as:\n\\begin{equation}\n\\bai ~\\approx~ \\mathrm{argmax\\,}_{\\ba \\in \\mathcal{Y}}\\: \\pit(\\ba \\mid \\bx)~.\n\\label{eq:inference}\n\\end{equation}\nSimilar to the use of log loss for supervised classification, the standard approach to optimize the parameters $\\theta$ of \nthe conditional probabilistic model entails maximizing a conditional log-likelihood objective, $\\mathcal{O}_{\\text{MLE}}(\\theta) = \\E\\nolimits_{(\\bx, \\bas) \\sim p_\\mathcal{D}} \\log \\pit(\\bas \\mid \\bx)$.\nThis approach to learning the parameters is called Maximum Likelihood Estimation (MLE) \\rebut{ and is commonly used in sequence to sequence learning.}\n\n\\cite{sutskeveretal14} propose the use of recurrent neural networks (RNNs)\nfor {\\em autoregressive} seq2seq modeling to tractably optimize $\\mathcal{O}_{\\text{MLE}}(\\theta)$.\nAn autoregressive model estimates the conditional probability of the target sequence given the source\none token at a time, often from left-to-right. A special {\\em end-of-sequence} token is appended at\nthe end of all of target sequences to handle variable length.\nThe conditional probability of $\\bys$ given $\\bx$ is decomposed via the chain rule as,\n\\begin{equation}\n\\pit(\\bas \\mid \\bx) ~\\equiv~ \\prod\\nolimits_{t=1}^{\\lvert \\bas \\rvert} \\rebut{\\pitt}(\\a^*_t \\mid \\bas_{<t},\\bx)~,\n\\label{eq:autoregressive}\n\\end{equation}\n\\comment{\\coloneqq}\nwhere $\\bas_{<t} \\equiv (\\a^*_1, \\ldots, \\a^*_{t-1})$ denotes a prefix of\nthe sequence $\\bas$. To estimate the probability of a token $a$ given a prefix $\\bas_{<t}$ and an input $\\bx$, denoted $\\rebut{\\pitt}(a \\mid \\bas_{<t},\\bx)$, different architectures\nhave been proposed. \nSome papers~(\\eg~\\cite{britz2017massive}) have investigated the use of LSTM~\\citep{hochreiter-nc-1997} and GRU~\\citep{cho2014learning}\ncells, while others proposed new architecturs based on soft attention~\\citep{bahdanau2014neural}, convolution~\\citep{gehring-arxiv-2017}, and self-attention~\\citep{transformer2017}.\n\\rebut{Nonetheless, all of these techniques rely on MLE for learning,\n\\begin{equation}\n\\mathcal{O}_{\\text{MLE}}(\\theta) ~=~ \\E\\nolimits_{(\\bx, \\bas) \\sim p_\\mathcal{D}} \\sum\\nolimits_{t=1}^{\\lvert \\bas \\rvert} \\log \\pitt(\\a^*_t \\mid \\bas_{<t},\\bx)~,\n\\label{eq:cll}\n\\end{equation}\nwhere $p_\\mathcal{D}$ denotes the empirical data\ndistribution, uniform across the dataset $\\mathcal{D}$.\n}\nWe present a new objective function for optimizing autoregressive seq2seq models applicable to any neural architecture.\n\n\\subsection{Limitations of MLE for Autoregressive Models}\n\nIn order to maximize the conditional log-likelihood \\eqref{eq:cll} of an autoregressive seq2seq model \\eqref{eq:autoregressive},\none provides the model with a {\\em prefix} of $t-1$ tokens from the ground truth target sequence, denoted $\\bys_{<t}$, and \nmaximizes the log-probability of $\\ys_t$ as the next token. This resembles a teacher walking a student through a\nsequence of perfect decisions, where the student learns as a passive\nobserver. However, during inference one uses beam search \\eqref{eq:inference}, wherein the student needs to\ngenerate each token $\\bai_t$ by conditioning on its own previous outputs,\n\\ie~$\\bai_{<t}$ instead of $\\bys_{<t}$. %. Since the ground truth target prefixes $\\bys_{<t}$ are not available anymore,\nThis creates a discrepancy between training and test known as {\\em exposure bias} \\citep{ranzato-iclr-2016}. Appendix~\\ref{sec:exposure} expands this further.\n\nConcretely, we highlight two limitations with the use of MLE for autoregressive seq2seq modeling:\n\\begin{enumerate}[noitemsep,parsep=0pt,leftmargin=7mm]\n    \\item There is a mismatch between the prefixes seen by the model during\n    training and inference. When the distribution of\n    $\\bai_{<t}$ is different from the distribution of $\\bys_{<t}$,\n    then the student will find themselves in a novel situation that they have not been\n    trained for. % The mismatch between the training and test distributions\n    This can result in poor generalization, especially when\n    the training set is small or the model size is large.\n    \\item There is a mismatch between the training loss and the task\n    evaluation metric. During training, one optimizes the log-probability of\n    the ground truth output sequence, which is often different from\n    the task evaluation metric (\\eg~edit distance for speech recognition).\n\\end{enumerate}\n\nThere has been a recent surge of interest in understanding and\nmitigating the limitations of MLE for autoregressive seq2seq modeling.\nIn \\secref{sec:relatedwork} we discuss prior work in detail after presenting\nour approach below.\n\n\\comment{We present a novel solution to both of the problems\ndiscussed above based on reinforcement learning techniques, while\nmaintaining the efficiency and effectiveness of teacher forcing. Our\nalgorithm is easy to implement, it is stable and unlike most alternative\napproaches, does not require pretraining nor joint training with\nMLE for variance reduction.}\n\n\\comment{\n\\vspace*{-.2cm}\n\\subsection{Beyond Teacher Forcing and Conditional Log-Likelihood}\n\\vspace*{-.1cm}\nThe task loss is typically captured via a similarity function that compares $\\bai$ and $\\bas$,\ndenoted $R(\\bas, \\bai)$. We call this similarity function a reward function to highlight the connection between sequence prediction and\nreinforcement learning. Common reward functions for sequence prediction include {\\em negative edit distance} and {\\em BLEU} score.\nOnce the model is trained, the quality of the model is measured in terms of its empirical performance on a held out dataset $\\mathcal{D'}$ as,\n\\begin{equation}\n  \\mathcal{O}_{\\text{ER}}(\\theta) = \\sum_{(\\bx, \\bas) \\in \\mathcal{D'}} R(\\bas,\\bai \\approx \\mathrm{argmax}_{\\ba}\\:\\pit(\\ba \\mid \\bx))~.\n\\label{eq:er}\n\\end{equation}\nA key limitation of teacher forcing for sequence learning stems from the discrepancy between the training\nand test objectives. One trains the model using conditional log-likelihood $\\mathcal{O}_{\\text{CLL}}$, but evaluates the quality of the model using\nempirical reward $\\mathcal{O}_{\\text{ER}}$.\n\n\\begin{figure}[t]\n\\vspace*{-.2cm}\n\\begin{center}\n\\begin{tabular}{c@{\\hspace*{2cm}}c}\n      (a) {Teacher Forcing} &\n      (b) {Scheduled Sampling} \\\\[-.1cm]\n      \\includegraphics[width=0.35\\textwidth]{rnn-tf} &\n      \\includegraphics[width=0.35\\textwidth]{rnn-ss} \\\\[-.1cm]\n      \\includegraphics[width=0.35\\textwidth]{rnn-policy} &\n      \\includegraphics[width=0.35\\textwidth]{rnn-ocd} \\\\[-.1cm]\n      (c) {Policy Gradient} &\n      (d) {Optimal Completion Distillation}\\\\\n\\vspace*{-.2cm}\n\\end{tabular}\n\\end{center}\n\\comment{\n                \\end{tabular}\n                \\end{figure}\n                \\begin{figure}[t]\n                  \\centering\n                  \\begin{subfigure}[t]{0.5\\textwidth}\n                      \\centering\n                      \\includegraphics[width=0.75\\textwidth]{rnn-tf}\n                      \\caption{Teacher Forcing}\n                      \\label{subfig:teacher-forcing}\n                  \\end{subfigure}%\n                  \\begin{subfigure}[t]{0.5\\textwidth}\n                      \\centering\n                      \\includegraphics[width=0.75\\textwidth]{rnn-ss}\n                      \\caption{Scheduled Sampling}\n                      \\label{subfig:ss}\n                  \\end{subfigure}\n                  \\begin{subfigure}[t]{0.5\\textwidth}\n                      \\centering\n                      \\includegraphics[width=0.75\\textwidth]{rnn-policy}\n                      \\caption{Policy Gradient}\n                      \\label{subfig:policy}\n                  \\end{subfigure}%\n                  \\begin{subfigure}[t]{0.5\\textwidth}\n                      \\centering\n                      \\includegraphics[width=0.75\\textwidth]{rnn-ocd}\n                      \\caption{Optimal Completion Distillation}\n                      \\label{subfig:ocd}\n                  \\end{subfigure}\n                  \\subref{subfig:teacher-forcing}\n}\n\\caption{Illustration of different training strategies for autoregressive sequence models.\n(a) Teacher Forcing: the model conditions on correct prefixes and is taught to predict the next ground truth token.\n(b) Scheduled Sampling: the model conditions on tokens either from ground truth or drawn from the model and is taught to predict the next ground truth token regardless.\n(c) Policy Gradient: the model conditions on prefixes drawn from the model and is encouraged to reinforce sequences with a large sequence reward $R(\\tilde y)$.\n(d) Optimal Completion Distillation: the model conditions on prefixes drawn from the model and is taught to predict an optimal completion policy $\\pi^*$ specific to the prefix.}\n\\label{fig:trainingstrats}\n\\end{figure}\n\nUnlike teacher forcing and Scheduled Sampling (SS), policy gradient approaches (\\eg~\\cite{ranzato-iclr-2016,bahdanau-iclr-2017})\nand OCD aim to optimize the empirical reward objective \\eqref{eq:er} on the training set. We illustrate four different training strategies\nof MLE, SS, Policy Gradient and OCD in \\figref{fig:trainingstrats}. The drawback of policy gradient techniques is twofold: 1) they cannot\neasily incorporate ground truth sequence information except through the reward function, and 2) they have difficulty reducing the variance of the gradients to\nperform proper credit assignment. Accordingly, most policy gradient approaches~\\cite{ranzato-iclr-2016,bahdanau-iclr-2017,wu2016google} pre-train the model\nusing teacher forcing. By contrast, the OCD method proposed in this paper defines an optimal completion policy $\\pi^*_t$\nfor any {\\em off-policy} prefix by incorporating the ground truth information. Then, OCD optimizes a token level log-loss\nand alleviates the credit assignment problem. Finally, training is much more stable, and we do not require initialization nor joint optimization with MLE.\n\nThere is an intuitive notion of {\\em exposure bias}~\\cite{ranzato-iclr-2016} discussed in the literature as a limitation of teacher forcing.\nWe formalize this notion as follows. One can think of the optimization of the log loss~\\eqref{eq:cll} in an autoregressive models as a classification problem, where the input to the classifier is a tuple $(\\bs, \\bys_{<t})$ and the correct output is $\\a^*_i$, where $\\bas_{<t} \\equiv (y^*_1, \\ldots, y^*_{t-1})$. Then the training dataset comprises different examples and different prefixes of the ground truth sequence. The key challenge is that once the model is trained, one should not expect the model to generalize to a new prefix $\\by_{<t}$ that does not come from the training distribution of $P(\\bys_{<t})$. This problem can\nbecome severe as $\\by_{<t}$ becomes more dissimilar to correct prefixes. During inference, when one conducts beam search with a large beam size then\none is more likely to discover wrong generalization of $\\pit(\\hat{y}_{t} | \\bai_{<t}, \\bx)$, because the sequence is optimized globally.\nA natural strategy to remedy this issue is to train on arbitrary prefixes. Unlike the aforementioned techniques OCD can train on any prefix\ngiven its {\\em off-policy} nature.\n\n\\comment{\nThe input trajectory that the model trains on introduces exposure bias to the model as it constrains the distribution of visited state and action pairs, e.g. generated prefixes. In teacher forcing the effect of exposure bias reveals by the significant performance decreases one observes when increasing the beam size in the beam search algorithm for decoding. Since OCD optimization is off-policy and is conditioned on the input prefix as the temporal dependency, we can train the model on a flexible range of trajectories. Rather than the typical $\\epsilon$-greedy exploration in Q-learning we adopt a parameter-free online sampling regime where we always sample from the model distribution, $p_\\theta(y_t | \\ba_{<t})$, at each step of decoding during training. \n}\n}\n\n\\vspace*{-.2cm}\n\\section{Optimal Completion Distillation}\n\\vspace*{-.1cm}\n\\comment{\n\\begin{figure}[t]\n  \\centering\n    \\includegraphics[width=.5\\textwidth]{ocd2}\n  \\caption{Diagram of different RNN cell input and loss strategies in the baseline seq2seq model vs the baseline with schedule sampling and the DOC approach.}\n\\label{fig:ocd}\n\\vspace{-0.4cm}\n\\end{figure}\n}\n\n\\comment{Inspired by search based structured prediction~\\citep{daumeetal09,ross-aistats-2011}\nand policy distillation~\\citep{rusu-iclr-2016},\nwe develop Optimal Completion Distillation (OCD) for optimizing seq2seq models based on edit distance.}\nTo alleviate the mismatch between inference and training, we {\\em never} train on ground truth target sequences.\nInstead, we always train on sequences generated by sampling from the current model that is being optimized.\nLet $\\byt$ denote a sequence generated by \\rebut{sampling from} the current model, and $\\bys$ denote the ground truth target.\nApplying MLE to autoregressive models casts the problem of sequence learning as optimizing a mapping $(\\bx, \\bys_{<t}) \\to \\ys_t$\nfrom ground truth prefixes to correct next tokens. By contrast, the key question that arises when training on model samples is the choice\nof targets for learning a similar mapping $(\\bx, \\byt_{<t}) \\to \\,??$ from generated prefixes to next tokens. Instead of\nusing a set of pre-specified targets, OCD solves a prefix-specific problem to find optimal extensions that lead to the\nbest completions according to the task evaluation metric. Then, OCD encourages\nthe model to extend each prefix with the set of optimal choices for the next token.\n\nOur notion of optimal completion depends on the task evaluation metric denoted $R(\\cdot, \\cdot)$, which measures\nthe similarity between two complete sequences, \\eg~the ground truth\ntarget \\vs~a generated sequence. Edit distance is a common task metric.\nOur goal in sequence learning is to train a model, which achieves high scores of $R(\\bys,\\byt)$.\nDrawing \\rebut{connection} with the goal of reinforcement learning~\\citep{suttonbarto98}, let us recall the notion of optimal Q-values.\nOptimal Q-values for a state-action pair $(s,a)$, denoted $Q^*(s,a)$, represent the maximum future reward that an agent can accumulate\nafter taking an action $a$ at a state $s$ by following with optimal subsequent actions. Similarly, we define Q-values\nfor a prefix $\\byt_{<t}$ and the extending token $a$, as the maximum score attainable by concatenating $[\\byt_{<t}, a]$\nwith an optimal suffix $\\by$ to create a full sequence $[\\byt_{<t}, a, \\by]$. Formally,\n\\begin{equation}\n   \\forall a \\in \\V,\\qquad Q^*(\\byt_{<t}, a) ~=~ \\mmax{\\by \\in \\Y} \\,R(\\bys, [\\byt_{<t}, a, \\by])~.\n\\label{eq:reward}\n\\end{equation}\nThen, the optimal extension for a prefix $\\byt_{<t}$ can be defined as tokens that attain the maximal Q-values,\n\\ie~$\\mathrm{argmax}_{a} Q^*(\\byt_{<t}, a)$. This formulation allows for a prefix $\\byt_{<t}$ to be sampled\non-policy from the model $p_\\theta$, or drawn off-policy \\rebut{in any way}. \\tabref{tab:sample} includes an example ground truth target from the Wall Street Journal dataset and the corresponding\ngenerated sample from a model. We illustrate that for some prefixes there exist more than a single optimal extension leading\nto the same edit distance.\n\nGiven $Q$-values for our prefix-token pairs, we use an exponential transform followed by normalization % softmax operator\nto convert Q-values to a soft optimal policy over the next token extension, %also called optimal completion policy,\n\\begin{align}\n    \\pi^*(a \\mid \\byt_{<t}) = \\frac{\\exp ({Q^*(\\byt_{<t}, a)}/\\tau)}{\\sum_{a'} \\exp {(Q^*(\\byt_{<t}, a')/\\tau)}}~,\n\\label{eq:pis}\n\\end{align}\nwhere $\\tau \\ge 0$ is a temperature parameter. Note the similarity of $\\tau$ and the label smoothing parameter helpful within MLE.\nIn our experiments, we used the limit of $\\tau \\to 0$ resulting in hard targets and no hyper-parameter tuning.\n\n{Given a training example $(\\bx, \\bys)$, we first draw a full sequence $\\byt \\sim p_\\theta(\\cdot\\mid\\bx)$ {\\em i.i.d.} from the current model, \nand then minimize a per-step KL divergence between the optimal policy and the model distribution over the next token extension at each time step $t$. The OCD objective is\nexpressed as,\n\\comment{\n\\begin{equation}\n \\mathcal{O}_{\\text{OCD}}(\\theta, (\\bx, \\bys), \\byt) ~=~ \\sum\\nolimits_t \\kl{\\pi^*(\\cdot \\mid \\byt_{<t})}{p_\\theta(\\cdot \\mid \\byt_{<t}, \\bx)}~.\n\\end{equation}\n}\n\\begin{eqnarray}\n\\mathcal{O}_{\\text{OCD}}(\\theta) &=& \\E\\nolimits_{(\\bx, \\bas) \\sim p_\\mathcal{D}} \\E_{\\byt \\sim p_\\theta(\\cdot\\mid\\bx)} \\sum\\nolimits_{t=1}^{\\lvert \\byt \\rvert} \\kl{\\pi^*(\\cdot \\mid \\byt_{<t})}{\\pitt(\\cdot \\mid \\byt_{<t}, \\bx)}~.\n\\label{eq:ocdll}\n\\end{eqnarray}\n\\comment{\n\\begin{equation}\n    \\exp ({Q^*(\\byt_{<t}, a)}/\\tau) = \\mmax{a'} ~Q^*(\\byt_{<t}, a') - Q^*(\\byt_{<t}, a) + 1\n\\end{equation}\n\n\\begin{equation}\n\\mathcal{O}_{\\text{OCD}}(\\theta) = \\sum\\nolimits_{(\\bx, \\bas) \\in \\mathcal{D}} C_{\\byt \\sim p_\\theta}(\\bx, \\bys).\n\\label{eq:ocdll}\n\\end{equation}\n\\begin{equation}\nC_{\\byt \\sim p_\\theta}(\\bx, \\bys) = \\sum\\nolimits_t \\kl{\\pi^*(\\ra_t \\mid \\byt_{<t})}{p_\\theta(\\ra_t \\mid \\byt_{<t}, \\bx)}~.\n\\end{equation}\n}%\nFor every prefix $\\byt_{<t}$, we compute the optimal $Q$-values and use \\eqref{eq:pis} to construct the optimal policy distribution $\\pi^*$.\nThen, we distill the knowledge of the optimal policy for each prefix $\\byt_{<t}$ into the parametric model using a KL loss.}\nFor the important class of sequence learning\nproblems where {\\it edit distance} is the evaluation metric,\nwe develop a dynamic programming algorithm to calculate \\rebut{optimal} $Q$-values exactly and efficiently for all prefixes of a sequence $\\byt$, discussed below. \n\\comment{\n\\rebut{\n\\begin{equation}\n\\mathcal{O}_{\\text{AGG}}(\\theta) ~=~ \\E\\nolimits_{(\\bx, \\bas) \\sim p_\\mathcal{D}} \\E_{\\byt \\sim p_\\theta(\\cdot\\mid\\bx)} \\sum\\nolimits_{t=1}^{\\lvert \\byt \\rvert} Q^*(\\byt_{<t}, \\byt_t)~.\n\\label{eq:ocdagg}\n\\end{equation}\n}\n}\n\\begin{table}[t]\n\\vspace*{-.2cm}\n\\begin{center}\n\\begin{tabular}{@{}ll@{}}%|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}\n\\toprule\nTarget sequence $\\bys$ & \\texttt{~a~s~\\_~h~e~\\_~t~a~l~k~s~\\_~h~i~s~\\_~w~i~f~e}\\\\[.1cm]\nGenerated sequence $\\byt$ & \\texttt{~a~s~\\_~e~e~\\_~t~a~l~k~s~\\_~w~h~o~s~e~\\_~w~i~f~e}\\\\[.1cm]\nOptimal extensions for & \\texttt{\\color{blue} a~s~\\_~h~e~\\_~t~a~l~k~s~\\_~h~i~i~s~\\_~\\_~w~i~f~e}\\\\\nedit distance (OCD  & \\texttt{\\color{blue} ~~~~~~~~h~~~~~~~~~~~~~~~~~h~~~i~~~w}\\\\\ntargets)           & \\texttt{\\color{blue} ~~~~~~~~\\_}\\\\\n\\comment{\\hline\n\\hline\nHamming mismatch & a~s~\\_~{\\color{red}e}~e~\\_~t~a~l~k~s~\\_~{\\color{red}wh~o~s~e~\\_~w~i~f~e}\\\\\n\\hline\nEdit distance credit assignment & a~s~\\_~{\\color{red}e}~e~\\_~t~a~l~k~s~\\_~{\\color{red}w}h~{\\color{red}o}~s~{\\color{red}e}~\\_~w~i~f~e\\\\ }\n\\bottomrule\n\\end{tabular}\n\\end{center}\n\\caption{A sample sequence $\\bys$ from the Wall Street Journal dataset, where the model's prediction $\\byt$ is not perfect. The optimal next characters for each prefix of $\\byt$ based on edit distance are shown in blue. For example, for the prefix ``as\\_e'' there are $3$ optimal next characters of ``e'', ``h'', and ``\\_''. All of these $3$ characters when combined with proper suffixes will result in a total edit distance of $1$.\n}\\label{tab:sample}\n\\vspace*{-.2cm}\n\\end{table}\n\n\\comment{helps here and we use an almost zero $\\tau = 0.001$ for hard targets. We keep the formulation more general to be consistent with~\\citep{hinton-nips-2014,rusu-iclr-2016} and indicate how label smoothing can be done in this case. \nsequence of tokens $\\by'$ that, when concatenated produces\na full sequence $[\\byt_{<t}, a, \\byt']$ closest to $\\bys$. }\n\\comment{Unlike reinforcement learning, in supervised sequence prediction one has access to ground truth sequences, and one can afford to do some exhaustive search\nto find the near optimal completions. A natural strategy is to search over the concatenations of the prefix and the ground truth sequence to find\na lower bound on $Q^*$-values as in learning to search literature.}\n\n\\comment{\nOptimal Q-values are uniquely defined for Markov Decision Processes (MDPs) under mild assumptions.\n\nFor each prefix of the generated sequence, we identify the set of all optimal suffixes thatresult in minimum total edit distance, using an efficient dynamic programming algorithm.3.At each position, we minimize a cross-entropy loss to encourage the model to completeeach prefix by selecting one of the optimal next tokens.\n}\n\n\\comment{Inspired by prior work on search based structured prediction~\\citep{daumeetal09,ross-aistats-2011},\nQ-learning~\\citep{watkins1992q}, and policy distillation~\\citep{rusu-iclr-2016},}\n\\comment{\nBased on reinforcement learning~\\citep{suttonbarto98} techniques, i.e. Q-learning, we present a novel solution to both of the problems\ndiscussed above.\nRecall that the optimal Q-value for a state-action pair in a RL environment, denoted $Q^*(s, a)$, represents\nthe maximum future reward that an agent can accumulate after taking an action $a$ at a state $s$ by following up with the optimal actions.\nOptimal Q-values are uniquely defined for Markov Decision Processes (MDPs) under mild assumptions~\\citep{suttonbarto98}.\nIf one has access to optimal Q-values for all states and actions, then one can construct the optimal policy by selecting the action\nwith maximum Q-value at each state. Q-learning improves the estimate of Q-values iteratively by resorting to bootstrapping based on \nthe Bellman optimality equations.\n\n\\comment{\nQ-learning is an optimistic reinforcement learning technique which for any finite Markov decision process determines the optimal stationary policy ($\\pi^*$) of action selection at each state to maximize a total reward $R$ over all next steps \\citep{watkins1992q}. In Q-learning the quality ($Q$) of an action $a$ in a state $s$ is the expectation of maximum attainable reward. The challenge in Q-learning is to estimate the Q-values for an optimal policy, which refers to as $Q^*(s, a)$. Q-learning estimates $Q^*$-values by bootstrapping future values\\comment{ according to the Bellman optimality equations}.\n}\n\nTo build robust autoregressive seq2seq models, we would like the model to be able to predict the best token given\nany prefix of \\new{TODO}\n\n\\acronym's goal is to generate the sequence $\\hat{\\by}$ one token at a time given a generated prefix $\\hat{\\by}_{<t}$ such that the final $\\hat{\\by}$ is similar to $\\bys$ evaluated by a reward function $R(\\bys, \\hat{\\by})$. In this setup, a state is a sequence prefix, and an action is selecting a token from the vocabulary.\nWe show that in this case when $R$ is edit distance\\comment{ (possibly weighted)}, one can efficiently\ncompute $Q^*$-values without any bootstrapping or search.\n\\comment{Inspired by prior work~\\cite{l2s2005,ross-aistats-2011,ranzato-iclr-2016},\nwe make connections between this problem and reinforcement learning,\nand in the notion optimal $Q$-values used in the Q-learning algorithm.  The goal of Q-learning is to estimate the optimal value of each state-action pair,\ndenoted $Q^*(s,a)$, which represents the maximal achievable reward if one takes an action $a$ at a state $s$ and continues with the optimal policy.}\n\n\\comment{\n\\mohammad{this seems more related to SS vs. OCD -- let's move it to the end of the section} As an example of a flaw in teacher forcing the log probability of the correct sequence, consider the target sequence: ``{\\it The cat is here.}'' and a character based model has predicted ``{\\it A ca}'' as the first $4$ characters. In teacher forcing, the model is trained to predict $y^*_4=$`a' regardless of the model predictions so far. On the other hand, considering the characters generated so far the next token that can lead to the minimum {\\it TER} is `t' at time step $4$. \n}\n\\comment{\nOur high level motivation is that ideas from Q-learning and policy distillation can help mitigate the issues faced by current sequence learning algorithms. }\n\n\\comment{More generally, one can combine\nideas from \\secref{sec:oced} with bootstrapping from Q-learning to develop a general variant of OCD, which is applicable to any sequence reward function.\n\n. Furthermore, the exact $Q^*$ can be calculated at each state regardless of the model, therefore  we do not need bootstrappping anymore. Also, calculating $Q^*$ at each step solves the credit assignment problem that cripples the convergence of most reinforcement learning techniques.\n\nIn OCD we map the sequence prediction task to Q-learning by defining each state as the input prefix so far ($\\ba_{<t}$), and the action $a$ is which token to choose as the next character, conditioned on a particular $(\\bx, \\bys)$ pair.\n\nWe define $Q^*(\\ba_{<t}, a)$ as the maximum attainable reward at this state:\n\\begin{align}\n    Q^*(\\ba_{<t}, a) = \\mmax{\\ba' \\in \\mathcal{Y}} \\,R([\\ba_{<t}, a, \\ba'], \\bys),\n\\label{eq:reward}\n\\end{align}\nwhere\n}\n\nGiven the \\comment{optimal }$Q^*$-values for all prefix-token pairs, we define the\noptimal completion policy for a given prefix $\\ba_{<t}$ as,\n\\begin{align}\n    \\pi_\\tau^*(a \\mid \\ba_{<t}) = \\frac{\\exp ({Q^*(\\ba_{<t}, a)}/\\tau)}{\\sum_{a'} \\exp {(Q^*(\\ba_{<t}, a')/\\tau)}}~,\n\\end{align}\nfor a label smoothing term $\\tau$.  We are not\nconvinced that smoothness helps here and we use an almost zero $\\tau = 0.001$ for hard targets. We keep the formulation more general to be consistent with~\\citep{rusu-iclr-2016} and indicate how label smoothing can be done in this case. \n\nWe distill~\\citep{hinton-nips-2014}\nthe optimal completion policy \\comment{from our teacher} into our \\comment{student} model.\nFor training, we sample a sequence from the model denoted $\\byt$, and for each position $t$, we optimize\nthe Kullback-Leibler (KL) divergence between the soft optimal\ncompletion policy $\\pi^*$ and the prediction of the current model, expressed as,\n\\begin{equation}\n \\mathcal{O}_{\\text{OCD}}(\\theta, \\byt) ~=~ \\sum\\nolimits_t \\kl{\\pi_\\tau^*(a \\mid \\byt_{<t})}{p_\\theta(a \\mid \\byt_{<t})}~.\n\\end{equation}\n\n}\n\n\\vspace*{-.2cm}\n\\subsection{Optimal Q-values for Edit Distance}\n\\vspace*{-.1cm}\n\\label{sec:oced}\n\nWe propose a dynamic programming algorithm to calculate optimal\nQ-values exactly and efficiently for the reward metric of negative edit distance,\n\\ie~$R(\\bys,\\byt) = -\\ed(\\bys,\\byt)$. Given two sequences $\\bys$ and $\\byt$, we compute the Q-values for\nevery prefix $\\byt_{<t}$ and any extending token $a \\in \\V$ with an asymptotic complexity of $O(|\\bys|.|\\byt| + |\\V|.|\\byt|)$.\nAssuming that $|\\bys| \\approx |\\byt| \\leq |\\mathcal{V}|$, our algorithm does not increase the time\ncomplexity over MLE, since %$|\\bys| \\approx |\\byt|$ and\ncomputing the cross-entropy losses in MLE also requires\na complexity of $O(|\\bys|.|\\V|)$. \\rebut{When this assumption does not hold, \\eg~genetic applications, OCD is less efficient than MLE.\nHowever, in practice, the wall clock time is dominated by the forward and backward passes of a neural networks,\nand the OCD cost is often negligible. We discuss the efficiency of OCD further in Appendix~\\ref{sec:algorithm}.}\n\n\\renewcommand{\\arraystretch}{1.1}\n\\begin{table}[b]\n\\caption{Each row corresponds to a prefix of ``SATRAPY'' and shows\nedit distances with all prefixes of ``SUNDAY''. We also show OCD targets (optimal extensions) for each prefix,\nand minimum value along each row, denoted $m_i$ \\rebut{(see~\\eqref{eq:min-ed-bound})}.\nWe highlight the trace path for $\\ed(\\text{``Satrapy''}, \\text{``Sunday''})$.}\n\\label{tab:edit}\n\\centering\n\\small\n\\begin{tabular}{@{}|>{\\columncolor[HTML]{C0C0C0}}l|l|l|l|l|l|l|l|l|c|@{}}\n\\hline\n\\multicolumn{8}{|c|}{Edit Distance Table} & \\acronym Targets & $m_i$\\\\ \\hline\n\\rowcolor[HTML]{C0C0C0}\n &  & S & U & N & D & A & Y & & \\\\ \\hline\n & \\cellcolor{Ccolor}{\\bf \\textcolor{Fcolor}0} & 1 & 2 & 3 & 4 & 5 & 6 & S &  {\\bf \\textcolor{Fcolor}0}\\\\ \\hline\nS & 1 & \\cellcolor{Ccolor}{\\bf \\textcolor{Fcolor}0} & 1 & 2 & 3 & 4 & 5 & U &  {\\bf \\textcolor{Fcolor}0}\\\\ \\hline\nA & 2 & {\\bf\\textcolor{Fcolor} 1} & \\cellcolor{Ccolor}{\\bf \\textcolor{Fcolor} 1} & 2 & 3 & 3 & 4 & U, N &  {\\bf \\textcolor{Fcolor}1} \\\\ \\hline\nT & 3 & {\\bf \\textcolor{Fcolor}2} & {\\bf\\textcolor{Fcolor} 2} & \\cellcolor{Ccolor}{\\bf \\textcolor{Fcolor}2} & 3 & 4 & 4 & U, N, D &  {\\bf\\textcolor{Fcolor}2} \\\\ \\hline\nR & 4 & {\\bf \\textcolor{Fcolor}3} & {\\bf \\textcolor{Fcolor}3} & {\\bf \\textcolor{Fcolor}3} & \\cellcolor{Ccolor}{\\bf\\textcolor{Fcolor} 3} & 4 & 5 & U, N, D, A &  {\\bf\\textcolor{Fcolor}3}\\\\ \\hline\nA & 5 & 4 & 4 & 4 & 4 & \\cellcolor{Ccolor}{\\bf \\textcolor{Fcolor}3} & 4 & Y &  {\\bf\\textcolor{Fcolor}3}\\\\ \\hline\nP & 6 & 5 & 5 & 5 & 5 & \\cellcolor{Ccolor}{\\bf \\textcolor{Fcolor}4} & {\\bf \\textcolor{Fcolor}4} & Y, \\textless{}/s\\textgreater~ &  {\\bf\\textcolor{Fcolor}4}\\\\ \\hline\nY & 7 & 6 & 6 & 6 & 6 & 5 & \\cellcolor{Ccolor}{\\bf\\textcolor{Fcolor} 4} & \\textless{}/s\\textgreater~ &  {\\bf\\textcolor{Fcolor}4}\\\\ \\hline\n\\end{tabular}\n\\vspace*{-.2cm}\n\\end{table}\n\\renewcommand{\\arraystretch}{1}\n\nRecall the Levenshtein algorithm \\citep{levenshtein1966binary} for calculating the minimum number of edits (insertion, deletion and substitution) required to convert sequences $\\byt$ and $\\bys$ to each other based on,\n\\begin{equation}\n\\begin{cases}\n    \\ed(\\byt_{<-1}, :) = \\infty\\\\\n    \\ed(:, \\bys_{<-1}) = \\infty\\\\\n    \\ed(\\byt_{<0},\\bys_{<0}) = 0~,~\n\\end{cases}\n    D_{\\text{edit}}(\\byt_{<i},\\bys_{<j}) = \\text{min}\\begin{cases}\n        D_{\\text{edit}}(\\byt_{<i-1},\\bys_{<j}) + 1\\\\\n        D_{\\text{edit}}(\\byt_{<i},\\bys_{<j - 1}) + 1\\\\\n        D_{\\text{edit}}(\\byt_{<i-1},\\bys_{<j-1}) + \\one{\\yt_i \\neq \\ys_j}~.\n    \\end{cases}\n\\label{eq:levenshtein}\n\\end{equation}\n\\tabref{tab:edit} shows an example edit distance table for sequences ``Satrapy'' and ``Sunday''.\nOur goal is to identify the set of all optimal suffixes $\\by \\in \\Y$ that result in a full sequences $[\\byt_{<i}, \\by]$ with\na minimum edit distance \\vs~$\\bys$.\n\\comment{\n\\begin{equation}\n    \\min_{\\by \\in \\Y} \\ed([\\byt_{<i}, \\by], \\bys) \\ge \\min_{0 \\le j \\le |\\bys|} \\,\\ed(\\byt_{<i},\\bys_{<j})~,\n\\label{eq:min-ed-bound}\n\\end{equation}\nbecause for any $\\by$ is, according to \\eqref{eq:levenshtein}, one can trace $\\ed([\\byt_{<i}, \\by], \\bys)$ \nto one of the entries within the row of the edit distance table corresponding to $\\forall j\\, \\ed(\\byt_{<i},\\bys_{<j})$.\nBecause the updates in~\\eqref{eq:levenshtein} are non-decreasing, hence \\eqref{eq:min-ed-bound}. \\mohammad{todo}}\n\\begin{lemma}\n\\label{th:suffix}\nThe edit distance resulting from any potential suffix $\\by \\in \\Y$ is lower bounded by $m_i$, %\\ie\n\\begin{equation}\n   \\forall \\by \\in \\Y,~~ \\ed([\\byt_{<i}, \\by], \\bys) ~\\ge~ \\min_{0 \\le j \\le |\\bys|} \\,\\ed(\\byt_{<i}, \\bys_{<j})\n   ~=~ m_i~.\n\\label{eq:min-ed-bound}\n\\end{equation}\n\\vspace*{-.4cm}\n\\end{lemma}\n\\begin{proof}\nLet's consider the path $P$ that traces $\\ed([\\byt_{<i},\\by], \\bys)$ back to $\\ed(\\byt_{<0}, \\bys_{<0})$ connecting each cell to an adjacent parent cell, which provides the minimum value among the three options in \\eqref{eq:levenshtein}. Such a path for tracing edit distance between ``Satrapy'' and ``Sunday'' is shown in Table~\\ref{tab:edit}.  Suppose the path $P$ crosses row $i$ at a cell $(i, k)$. Since the operations in \\eqref{eq:levenshtein} are non-decreasing, the edit distance along the path cannot decrease, so $\\ed([\\byt_{<i},\\by], \\bys) \\geq \\ed(\\byt_{<i}, \\bys_{<k}) \\ge m_i$.\n\\vspace*{-.2cm}\n\\end{proof}\n\n\\comment{\nSince $D_{\\text{edit}}(\\byt_{<i},\\bys_{<j})$ never decreases as $i$ or $j$ increase according to~\\eqref{eq:levenshtein}. Hence,\n\\begin{equation}\n  \\forall a, \\qquad  Q^*(\\byt_{<i}, a) \\leq -\\mmin{j < |\\bys|} \\,D_{\\text{edit}}(\\byt_{<i},\\bys_{<j}),\n\\end{equation}\nTherefore, the suffix of $\\bys_{<j}$ who has minimum edit distance with $\\byt_{<i}$ are a subset of optimal completions for $\\byt_{<i}$. Furthermore, we prove that\n\\begin{lemma}\n\\label{th:suffix}\nThe set of optimal completions is limited to only suffixes of $\\bys$.\n\\begin{equation}\n\\forall s \\in \\mathcal{Y},~~~    s = \\argmin{\\byt' \\in \\mathcal{Y}} \\, D_{\\text{edit}}([\\byt_{<i},\\byt'], \\bys) \\implies \\exists j,~~~ s = \\bys_{j\\leq} \n\\end{equation}\n\\end{lemma}\n\\begin{proof}\n\nBy contradiction, assume\n\\begin{equation}\n    \\exists s \\in \\mathcal{Y},~~~    s = \\argmin{\\byt' \\in \\mathcal{Y}} \\, D_{\\text{edit}}([\\byt_{<i},\\byt'], \\bys) , ~~~\\forall j: D_{\\text{edit}}([\\byt_{<i},s], \\bys) \\leq D_{\\text{edit}}([\\byt_{<i},\\bys_{j\\leq}], \\bys) \n\\end{equation}\nAmong which, consider the sequence $s$ which has maximum shared suffix with $\\bys$. Reverse both sequences. $D_{\\text{edit}}([\\overline{s},\\overline{\\byt}_{<i}],\\overline{\\bys}) = D_{\\text{edit}}([\\byt_{<i},s], \\bys)$. Assume the first discrepancy between $\\overline{s}$ and $\\overline{\\bys}$ happens at index $h$. Such $h$ exists otherwise $s$ would have been a suffix of $\\bys$. Replacing $\\overline{s}_h$ with $\\overline{\\bys}_h$ does not increase the edit distance. Despite that, it increases the shared suffix of $s$ with $\\bys$ which contradicts with $s$ having the maximum shared suffix.\n\\end{proof}\n}\n\nThen, consider any $k$ such that $\\ed(\\byt_{<i}, \\bys_{<k}) = m_i$. Let $\\bys_{\\geq k} \\equiv (y^*_k, \\ldots, y^*_{\\lvert \\bys \\rvert})$ denote a suffix of $\\bys$. We conclude that $\\ed([\\byt_{<i}, \\bys_{\\geq k}], \\bys) = m_i$, because on the one hand there is a particular edit path that results in $m_i$ edits,\nand on the other hand $m_i$ is a lower bound according to Lemma~\\ref{th:suffix}. Hence any such $\\bys_{\\geq k}$ is an optimal suffix for $\\byt_{<i}$. Further,\nit is straightforward to prove by contradiction that the set of optimal suffixes is limited to suffixes $\\bys_{\\geq k}$ corresponding to\n$\\ed(\\byt_{<i}, \\bys_{<k}) = m_i$.\n\nSince the set of optimal completions for $\\byt_{<i}$ is limited to $\\bys_{\\geq k}$, the only extensions that can lead to maximum reward are the starting token of such suffixes ($\\ys_k$). Since $\\ed(\\byt_{<i}, \\bys_{<k}) = m_i$ as well, we can identify the optimal extensions by calculating the edit distances between all prefixes of $\\byt$ and all prefixes of $\\bys$ which can be efficiently calculated by dynamic programming in $\\mathcal{O}(|\\byt|.|\\bys|)$. For a prefix $\\byt_{<i}$ after we calculate the minimum edit distance $m_i$ among all prefixes of $\\bys$, we set the $Q^*(\\byt_{<i}, \\ys_{k}) = -m_i $ for all $k$ where $\\bys_{<k}$ has edit distance equal to $m_i$. We set the $Q^*$ for any other token to $-m_i-1$.\n We provide the details of our modified Levenshtein algorithm to efficiently compute the $Q^*(\\byt_{<i}, a)$ for all $i$ and $a$ in Appendix~\\ref{sec:algorithm}.\n \n \n \n \n \n \n \n \n \n \n \n \n \\comment{\n\\begin{table}[b]\n\\caption{Each row corresponds to a prefix of ``SATRAPY'' and shows\nedit distances with all prefixes of ``SUNDAY''. We also show OCD targets (optimal next tokens) for each prefix,\nand minimum value along each row, denoted $m_i$.\nWe highlight the race path for $\\ed(\\text{``Satrapy''}, \\text{``Sunday''})$.}\n\\label{tab:edit}\n\\vspace*{-.2cm}\n\\centering\n\\small\n\\begin{tabular}{@{}|>{\\columncolor[HTML]{C0C0C0}}l|l|l|l|l|l|l|l|l|l|@{}}\n\\hline\n\\multicolumn{8}{|c|}{Edit Distance Table} & OCD targets & $m_i$\\\\ \\hline\n\\rowcolor[HTML]{C0C0C0}\n&  & S & U & N & D & A & Y &  \\\\ \\hline\n& \\cellcolor{Ccolor}0 & 1 & 2 & 3 & 4 & 5 & 6 & S %(0)\n\\\\ \\hline\nS & 1 & \\cellcolor{Ccolor}0 & 1 & 2 & 3 & 4 & 5 & U %(0)\n\\\\ \\hline\nA & 2 & \\cellcolor{Ccolor}1 & 1 & 2 & 3 & 3 & 4 & U, N %(-1)\n\\\\ \\hline\nT & 3 & \\cellcolor{Ccolor}2 & 2 & 2 & 3 & 4 & 4 & U, N, D %(-2)\n\\\\ \\hline\nU & 4 & 3 & \\cellcolor{Ccolor}2 & 3 & 3 & 4 & 5 & N %(-2)\n\\\\ \\hline\nR & 5 & 4 & 3 & \\cellcolor{Ccolor}3 & 4 & 4 & 5 & N, D %(-3)\n\\\\ \\hline\nD & 6 & 5 & 4 & 4 & \\cellcolor{Ccolor}3 & 4 & 5 & A %(-3)\n\\\\ \\hline\nA & 7 & 6 & 5 & 5 & 4 & \\cellcolor{Ccolor}3 & 4 & Y %(-3)\n\\\\ \\hline\nY & 8 & 7 & 6 & 6 & 5 & 4 & \\cellcolor{Ccolor}3 & \\textless{}/s\\textgreater%~(-3)\n\\\\ \\hline\n\\end{tabular}\n\\vspace*{-.2cm}\n\\end{table}\n}\n\n\\vspace{-.3cm}\n\\section{Related Work}\n\\vspace{-.2cm}\n\\label{sec:relatedwork}\n\\comment{\nOCD considers the evaluation reward as the optimization goal, similar to Reinforcement Learning  (RL) techniques. Furthermore, it can trace the reward of edit distance back to time steps in the model trajectory, resembling the Imitation Learning techniques. Also, OCD can calculate the step reward for all actions, close to the aim of the Learning to Search techniques. In contrast to prior work, by focusing on Edit Distance, OCD proposes an algorithm to calculate step reward for all actions more precisely and more efficiently. Also, OCD's formulation of the task (combination of optimal reward and KL) has lower variance which is apparent by its ability to train only on model samples unlike other methods. We now discuss OCD's relation to specific prior work in details.\n\n\\textbf{Trajectory Reward Optimization.} Approaches based on reinforcement-learning have been applied to sequence prediction problems,\nincluding REINFORCE \\citep{ranzato-iclr-2016}, Actor-Critic \\citep{bahdanau-iclr-2017} and Self-critical Sequence Training \\citep{rennie-cvpr-2017}.\nThese methods sample sequences from the model's distribution and backpropagate a sequence-level task objective (\\eg~edit distance).\nBeam Search Optimization \\citep{wiseman-emnlp-2016} and Edit-based Minimum Bayes Risk (EMBR)~\\citep{prabhavalkar-icassp-2018} is similar,\nexcept the sampling procedure is replaced with beam search.\nThese training methods suffer from high variances and credit assignment problems.\n\\comment{The use of sequence-level cost makes it difficult for the model to learn the subtleties of the loss, and this is especially true for long sequences.}\nBy contrast, OCD takes advantage of the decomposition of the sequence-level objective into token level optimal completion targets.\nThis reduces the variance of the gradient and stabilizes the model.\nCrucially, unlike most RL-based approaches, we neither need MLE pretraining or joint optimization with log-likelihood.\n\nReward Augmented Maximum Likelihood (RAML) \\citep{norouzi-nips-2016} and related algorithms~\\citep{koyamada2017,ma2017softmax,elbayad2018token,wang2018switchout} are also similar\nto RL-based approaches. Instead of sampling from the model's distribution, RAML samples sequences from the true exponentiated reward distribution.\nHowever, sampling from the true distribution is often difficult and intractable. \\rebut{SPG \\citep{ding2017cold} changes the policy gradient formulation to sample from a reward shaped model distribution. Therefore, its samples are closer than RAML samples to model samples. In order to facilitate sampling from their proposed distribution SPG points out the decomposability of their metrics (ROUGE). Although they have lower variance due to their biased samples, SPG suffers from the same problems as RAML and RL-based methods in credit assignment.}\n\\comment{for long sequences. Additionally, it is unclear whether sampling from the true distribution is the correct thing to do, since during inference, the model samples from the model's posterior.}\n\n\\textbf{Trajectory Reward Optimization per Step.} In order to solve the credit assignment problem, one can try to trace the final reward back into per step rewards. OCD lies in this category of methods and is inspired by Imitation Learning and Learning to Search (L2S)\ntechniques where a student policy is optimized with a policy at each step of the trajectory.\nThere are two questions to be addressed:\n\n\\textbf{Q1)} Given an expert teacher ($\\pi^*$ and/or $Q^*$) how to optimize the student?\n\n\\textbf{A1.1)} DAgger \\citep{ross-aistats-2011} answers this question and in particular is closely\nrelated to OCD. DAgger iterates between augmenting ground truth with new model samples and mimicking expert's policy ($\\pi^*$) on the full aggregated dataset. Similar to DAgger our goal is to mimic $\\pi^*$ but only on model samples. Also, since in practice $\\pi^*$ is only given for expert's trajectory we derive $\\pi^*$ for new states based on reward ($Q^*$) and do not rely on having access to experts policy for new states.\n\\comment{\\citep{sun2018truncated}}\n\nScheduled Sampling (SS)~\\citep{bengio-nips-2015} and Data as Demonstrator (DaD)~\\citep{dad2015} are instantiations of DAgger that aim to alleviate the mismatch between training and inference prefix distributions. SS generates prefixes by substituting some of the ground truth tokens with samples from the model via a tunable mixing schedule. SS uses the ground truth sequence as the training targets uninformed of the synthetic prefixes. Hence their targets are Hamming Distance optimal. By contrast, OCD draws tokens via $100\\%$ sampling from the model, without any schedule. While SS can only handle substitution errors, OCD considers insertions and deletions as well by solving an alignment problem to find Edit Distance optimal targets.\n\n\\textbf{A1.2)} Another answer closely related to our setup is AggreVated \\citep{sun2017deeply} which provides the online and natural derivatives of AggreVaTe \\citep{ross2014reinforcement}, suitable for training with LSTMs. In this line of work, they assume access to $Q^*$ for model policy sampled action-state pairs and they still train on a mixture of ground truth and model trajerctories. Their objective is to minimize the per step cost-to-go $\\mathop{\\mathbb{E}}_{a \\sim \\pi(\\cdot | s_t)}Q_t^*(s_t,a)$ and they define the online gradient as $\\sum_{a}\\nabla_{\\theta}\\pi(a|s_t;\\theta) Q_t^*(s_t, a)$ which is close to optimizing reverse KL without optimizing the entropy. OCD while being similar in having a  $Q^*$ sensitive objective, optimizes the proper KL with the normalized exponentiation of $Q^*$ which is SGD friendly. Therefore, OCD is able\nto have {\\em roll-in} prefixes drawn only from the model. \\cite{cheng2018convergence} showed that\nmixing in ground truth samples is an essential regularizer for value\naggregation convergence in imitation learning. Furthermore, we calculate $Q^*$ directly for our task without assuming it exists or rolling out the expert. \n\n\\textbf{Q2)} How to calculate $\\pi^*$, $Q^*$ or $Q^\\pi$ for rewards other than Hamming Distance in practice?\n\n\\textbf{A2.0)} \\rebut{One may answer this with a structured reward specific algorithm like \\cite{goldberg2012dynamic} where they provide an efficient dynamic oracle for parse tree generation with un-ordered but restricted edges and optimize a margin loss on most probable edge. OCD similarly provides a dynamic oracle but for edit distance on sequence learning and distills all the optimal targets.}\n\n\\textbf{A2.1)} One general answer is the L2S \\citep{l2s2005}\nliterature where works like LoLS \\citep{chang2015learning} and \\cite{goodman2016noise}\nestimate the cost of actions by examining\nmultiple {\\em roll-outs} of a generated\nprefix and they address which action to expand. SeaRNN~\\citep{searnn2017} is an adoptation of L2S for RNNs which approximates the cost of\neach token by computing the task loss for vocabulary size roll-outs\nat each time step. Then in one setup they optimize the KL of $Q^\\pi$ with the RNN. OCD has similar architectural components. Also our cost-sensitive loss is similarly the KL objective but with $Q^*$. Furthermore, the exploration makes SeaRNN's per step time complexity $O(VT)$ and difficult to scale to real world\ndatasets, where either the sequences are long or the\nvocabulary is large. While SeaRNN is a general approach, OCD exploits the special structure in edit distance\nand find $Q^*$ by per step complexity of $O(V+T)$. Hence, unlike L2S and SeaRNN, which require\nground truth prefixes to stabilize training, we are able to train only on model samples.\n\n\\textbf{A2.2)} Another answer more closely related to our work is Policy\nDistillation~\\citep{rusu-iclr-2016}, where a deep Q-Network (DQN)\nagent~\\citep{mnihetal15} is optimized as the teacher first. Then, action sequences are sampled from\nthe teacher and the learned Q-value estimates are distilled~\\citep{hinton-nips-2014} into a\nsmaller student network using a KL loss. OCD adopts a similar loss function,\nbut rather than estimating Q-values using bootstrapping,\nwe estimate exact $Q$-values using dynamic programming. Moreover, we draw\nsamples from the student rather than the teacher.\n\n\\cite{bahdanau-iclr-2016} also noticed some of the nice structure of edit distance, but they optimize the model\nby regressing its outputs to edit distance values leading to suboptimal performance. Rather, we first construct the optimal policy and then use knowledge distillation for training. \\rebut{\\cite{karita-icassp-2018} also noticed the decomposability of the edit distance cost function, however they only used this to reweight the policy gradients (likely-hood of the prefix) at a token level rather than a cost-sensitive classification of actions. Therefore, they still suffered from high variance and had to resort to MLE pretraining.}\n\nGenerally, \\acronym excels at training from scratch, which makes it an ideal substitution for MLE. Hence, OCD is orthogonal to methods which require\nMLE pretraining or joint optimization.\n}\n\nOur work \\rebut{builds upon} Learning to Search \\citep{l2s2005} and Imitation Learning\ntechniques~\\citep{ross-aistats-2011,ross2014reinforcement,sun2018truncated},\nwhere a student policy is optimized to imitate an expert teacher.\nDAgger \\citep{ross-aistats-2011} in particular is closely\nrelated, where a dataset of trajectories \\rebut{from an expert teacher is\naggregated with samples from past student models, \nand a policy is optimized to mimic a given expert policy $\\pi^*$ at various states.\nSimilarly, OCD aims to mimic an optimal policy $\\pi^*$ at all prefixes, but in OCD, the behavior policy is directly obtained from an online student. Further, the oracle policy is not provided during training, and we obtain the optimal policy by finding optimal $Q$-values.\nAggreVaTeD \\citep{sun2017deeply} assumes access to an unbiased estimate of Q-values and relies on variance reduction techniques and conjugate gradients\nto address a policy optimization problem.} OCD calculates\nexact Q-values and uses regular SGD for optimization. Importantly, our {\\em roll-in} prefixes are drawn only from the student model,\nand we do not require mixing in ground truth (\\aka~expert) samples. \\cite{cheng2018convergence} showed that\nmixing in ground truth samples is an essential regularizer for value aggregation convergence in imitation learning.\n\n\\rebut{\nOur work is closely related to Policy\nDistillation~\\citep{rusu-iclr-2016}, where a Deep Q-Network (DQN)\nagent~\\citep{mnihetal15} that is previously optimized is used as the expert teacher.}\nThen, action sequences are sampled from\nthe teacher and the learned Q-value estimates are distilled~\\citep{hinton-nips-2014} into a\nsmaller student network using a KL loss. OCD adopts a similar loss function,\nbut rather than estimating Q-values using bootstrapping,\nwe estimate exact $Q$-values using dynamic programming. Moreover, we draw\nsamples from the student rather than the teacher.\n \n\\rebut{\nSimilar to OCD, the learning to search (L2S)\ntechniques such as LOLS \\citep{chang2015learning} and \\cite{goodman2016noise}\nalso attempt to estimate the Q-values for each state-action pair. Such techniques examine}\nmultiple {\\em roll-outs} of a generated\nprefix \\rebut{and aggregate the return values}. SeaRNN~\\citep{searnn2017} approximates the \\rebut{cost-to-go} for\neach token by computing the task loss for \\rebut{as many roll-outs as the vocabulary size}\nat each time step \\rebut{with a per step complexity of $O(VT)$. It is often difficult to scale approaches based on\nmultiple roll-outs} to real world\ndatasets, where either the sequences are long or the\nvocabulary is large. %\\rebut{OCD optimizes a similar cost sensitive loss but}\nOCD exploits the special structure in edit distance\nand find exact Q-values efficiently \\rebut{in $O(V+T)$ per step}. Unlike L2S and SeaRNN, which require\nground truth prefixes to stabilize training, we solely train on model samples.\n\nApproaches based on Reinforcement Learning (RL) have also been applied to sequence prediction problems,\nincluding REINFORCE \\citep{ranzato-iclr-2016}, Actor-Critic \\citep{bahdanau-iclr-2017} and Self-critical Sequence Training \\citep{rennie-cvpr-2017}.\nThese methods sample sequences from the model's distribution and backpropagate a sequence-level task objective (\\eg~edit distance).\nBeam Search Optimization \\citep{wiseman-emnlp-2016} and Edit-based Minimum Bayes Risk (EMBR)~\\citep{prabhavalkar-icassp-2018} is similar,\nbut the sampling procedure is replaced with beam search.\nThese training methods suffer from high variances and credit assignment problems.\nBy contrast, OCD takes advantage of the decomposition of the sequence-level objective into token level optimal completion targets.\nThis reduces the variance of the gradient and stabilizes the model.\nCrucially, unlike most RL-based approaches, we neither need MLE pretraining or joint optimization with log-likelihood.\n\\cite{bahdanau-iclr-2016} also noticed some of the nice structure of edit distance, but they optimize the model\nby regressing its outputs to edit distance values leading to suboptimal performance. Rather, we first construct the optimal policy and then use knowledge distillation for training. \\rebut{Independently, \\cite{karita-icassp-2018} also decomposed edit distance into the contribution of individual tokens and used this decomposition within the EMBR framework. That said, \\cite{karita-icassp-2018} do not theoretically justify this particular choice of decomposition\nand report high variance in their gradient estimates.}\n\nReward Augmented Maximum Likelihood (RAML) \\citep{norouzi-nips-2016} and its variants~\\citep{ma2017softmax,elbayad2018token,wang2018switchout} are also similiar\nto RL-based approaches. Instead of sampling from the model's distribution, RAML samples sequences from the true exponentiated reward distribution.\nHowever, sampling from the true distribution is often difficult and intractable. RAML suffers from the same problems as RL-based methods in credit assignment. \\rebut{SPG \\citep{ding2017cold} changes the policy gradient formulation to sample from a reward shaped model distribution. Therefore, its samples are closer than RAML to the model's samples. In order to facilitate sampling from their proposed distribution SPG provides a heuristic to decompose ROUGE score. Although SPG has a lower variance due to their biased samples, it suffers from the same problems as RAML and RL-based methods in credit assignment.}\n\nGenerally, \\acronym excels at training from scratch, which makes it an ideal substitution for MLE. Hence, OCD is orthogonal to methods which require\nMLE pretraining or joint optimization.\n\\section{Experiments}\n\\label{sec:experiments}\n\nWe conduct our experiments on speech recogntion on the Wall Street Journal (WSJ)~\\citep{paul1992design} and Librispeech \\citep{librispeech}\nbenchmarks. We only compare end-to-end speech recognition approaches that do not incorporate language model rescoring.\nOn both WSJ and Librispeech, our proposed {\\bf OCD} (Optimal Completion Distillation) algorithm significantly outperforms our own strong baselines\nincluding {\\bf MLE} (Maximum Likelihood Estimation with label smoothing) and {\\bf SS} (scheduled sampling with a well-tuned schedule).\nMoreover, OCD significantly outperforms all prior work, achieving a new state-of-the-art on two competitive benchmarks.\n\\subsection{Wall Street Journal}\n\\begin{figure}[t]\n\\begin{minipage}{.48\\textwidth}\n\\centering\n  \\includegraphics[height=.54\\linewidth]{mismatch.png}\n  \\vspace*{.3cm}\n  \\captionof{figure}{Fraction of \\acronym training prefix tokens on WSJ which does not match ground truth.}\n  \\label{fig:frac}\n\\end{minipage}\n\\hspace{.4cm}\n\\begin{minipage}{.48\\textwidth}\n\\centering\n  \\includegraphics[height=.54\\linewidth]{train_dev3.png}\n  \\vspace*{.3cm}\n  \\captionof{figure}{WSJ validation Character Error Rate (CER) per training CER for MLE and \\acronym.}\n  \\label{fig:gen}\n\\end{minipage}%\n\\end{figure} \n\\begin{table}\n\\centering\n\\caption{WSJ Character Error Rate (CER) and Word Error Rate (WER) of different baselines. Schedule Sampling optimizes for \\textit{Hamming} distance and mixes samples from the model and ground truth with a \\textit{probability} schedule (start-of-training $\\rightarrow$ end-of-training). \\acronym \\textit{always} samples from the model and optimizes for \\textit{all} characters which minimize \\textit{Edit} distance. Optimal Completion Target optimizes for \\textit{one} character which minimizes edit distance and another criteria (shortest or same \\#words). }%\\will{is this caption too long?}}\n\\label{tab:comparison}\n\\begin{tabular}{lcc} \n \\toprule\n \\textbf{Training Strategy} & \\textbf{CER} & \\textbf{WER} \\\\ \n \\midrule\n Schedule Sampling $(1.0 \\rightarrow 1.0)$ & 12.1 & 35.6 \\\\\n Schedule Sampling $(0.0 \\rightarrow 1.0)$ & 3.8 & 11.7 \\\\\n Schedule Sampling $(0.0 \\rightarrow 0.55)$ & 3.6 & 10.2 \\\\\n \\midrule\n Optimal Completion Target (Shortest) & 3.8 & 12.7\\\\\n Optimal Completion Target (Same \\#Words) & 3.3 & 10.2\\\\\n \\midrule\n Optimal Completion Distillation & \\bf{3.1} & \\bf{9.3} \\\\\n \\bottomrule\n\\end{tabular}\n\\end{table}\n\\begin{table}[t]\n\\centering\n\\caption{Character Error Rate (CER) and Word Error Rate (WER) results on the end-to-end speech recognition WSJ task. We report results of our Optimal Completion Distillation (\\acronym) model, and well-tuned implementations of maximum likelihood estimation (MLE) and Scheduled Sampling (SS).}\n\\label{tab:results}\n\\begin{tabular}{lcc} \n \\toprule\n \\textbf{Model} & \\textbf{CER} & \\textbf{WER} \\\\ \n \\midrule\n Prior Work \\\\\n \\qquad CTC \\citeauthoryear{graves-icml-2014} & 9.2 & 30.1 \\\\\n \\qquad CTC + REINFORCE \\citeauthoryear{graves-icml-2014} & 8.4 & 27.3 \\\\\n \\qquad Gram-CTC \\citeauthoryear{liu-icml-2017} & - & 16.7 \\\\\n \\qquad seq2seq \\citeauthoryear{bahdanau-icassp-2016} & 6.4 & 18.6 \\\\\n \\qquad seq2seq + TLE \\citeauthoryear{bahdanau-iclr-2016} & 5.9 & 18.0 \\\\\n \\qquad seq2seq + LS \\citeauthoryear{chorowski-interspeech-2017} & - & 10.6 \\\\\n \\qquad seq2seq + CNN \\citeauthoryear{zhang-icassp-2017} & - & 10.5 \\\\\n \\qquad seq2seq + LSD \\citeauthoryear{chan-iclr-2017} & - & 9.6 \\\\\n \\qquad seq2seq + CTC \\citeauthoryear{kim-icassp-2017} & 7.4 & - \\\\\n \\qquad seq2seq + TwinNet \\citeauthoryear{serdyuk-iclr-2018} & 6.2 & - \\\\\n \\qquad seq2seq + MLE + REINFORCE \\citeauthoryear{tjandra-icassp-2018} & 6.1 & - \\\\\n \\midrule\n Our Implementation \\\\\n \\qquad seq2seq + MLE & 3.6 & 10.6 \\\\\n \\qquad seq2seq + SS & 3.6 & 10.2 \\\\\n \\qquad {\\bf seq2seq + \\acronym} & {\\bf 3.1} & {\\bf 9.3} \\\\\n \\bottomrule\n\\end{tabular}\n\\end{table}\nThe WSJ dataset is readings of three separate years of the Wall Street Journal.\nWe use the standard configuration of si$284$ for training, dev$93$ for validation and report both\ntest Character Error Rate (CER) and Word Error Rate (WER) on eval$92$.\nWe tokenize the dataset to English characters and punctuation. % no `s` after punctuation\nOur model is an attention-based seq2seq network with a deep convolutional frontend as used in~\\cite{zhang-icassp-2017}.\nDuring inference, we use beam search with a beam size of $16$ for all of our models. We describe the architecture and hyperparameter details in Appendix \\ref{sec:architecture}. %\\footnote{We refer the reader to appendix for details of the architecture.}. \nWe first analyze some key characteristics of the OCD model separately, and then compare our results with other baselines and state-of-the-art methods.\n\n{\\bf Training prefixes and generalization}. We emphasize that during training, the generated prefixes sampled from the model do not match the ground truth sequence, even at the end of training.\nWe define OCD prefix {\\em mismatch} as the fraction of OCD training tokens that do not match corresponding ground truth training tokens at each position.\nAssuming that the generated prefix sequence is perfectly matched with the ground truth sequence, then the OCD targets would simply be the following tokens of the ground truth sequence.\nHence, OCD becomes equivalent to MLE. %The \\acronym targets would then be virtually identical to the MLE targets. \n\\figref{fig:frac} shows that OCD prefixes mismatch is more than $25\\%$ for the most of the training.\nThis suggests that OCD and MLE are training on very different input prefix trajectories.\nFurther, \\figref{fig:gen} depicts validation CER as a function of training CER for different model checkpoints during training,\nwhere we use beam search on both training and validation sets to obtain CER values.\nEven at the same training CER, we observe better validation error for OCD, which suggests that OCD improves generalization of MLE, possibly\nbecause OCD alleviates the mismatch between training and inference.\n\n\\comment{\n\\begin{figure}[t]\n\n\\centering\n\\begin{subfigure}{.5\\textwidth}\n  \\centering\n  \\includegraphics[width=\\linewidth]{train_cer_3}\n  \\caption{Training CER}\n\\end{subfigure}%\n\\begin{subfigure}{.5\\textwidth}\n  \\centering\n  \\includegraphics[width=\\linewidth]{val_cer_3}\n  \\caption{Validation CER}\n\\end{subfigure}\n\\caption{Log scale plots of training and validation Character Error Rate (CER). The naive sampling model always samples from the model, while the MLE model always feeds the ground truth as the input to the RNN. Both of these models are trained to match the ground truth at each sequence step. Although the \\acronym module is always sampling from the model, it doesn't suffer like the Naive model and quickly catches the baselines and continues to further improve.\n}\n\\label{fig:cer}\n\\end{figure} }\n{\\bf Impact of edit distance.} We further investigate the role of the optimizer by experimenting with different losses. Table~\\ref{tab:comparison} compares the test CER and WER of the schedule sampling with a fixed probability schedule of $(1.0 \\rightarrow 1.0)$ and \\acronym model. Both of the models are trained only on sampled trajectories. The main difference is their optimizers, where the SS$(1.0\\rightarrow1.0)$ model is optimizing the log likelihood of ground truth (\\aka~Hamming distance). The significant drop in CER of SS$(1.0\\rightarrow1.0)$ emphasizes the necessity of pretraining or joint training with MLE for models such as SS. \\acronym is trained from random initialization and does not require MLE pretraining, nor does it require joint optimization with MLE. We also emphasize that unlike SS, we do not need to tune an exploration schedule, OCD prefixes are simply always sampled from the model from the start of training. We note that even fine tuning a pre-trained SS model which achieves $3.6\\%$ CER with $100\\%$ sampling increases the CER to $3.8$\\%. This emphasizes the importance of making the loss a function of the model input prefixes, as opposed to the ground truth prefixes. Appendix~\\ref{sec:hamm} covers another aspect of optimizing Edit distance rather than Hamming distance.\n\n{\\bf Target distribution.} Another baseline which is closer to MLE framework is selecting only one correct target. Table~\\ref{tab:comparison} compares OCD with several Optimal Completion Target (OCT) models. In OCT, we optimize the log-likelihood of one target, which at each step we pick dynamically based on the minimum edit distance completion similar to OCD. We experiment with several different strategies when there is more than one character that can lead to minimum CER. In the OCT (Shortest), we select the token that would minimize the CER and the final length of the sequence. In the OCT (Same \\#Words), we select the token that in addition to minimum CER, would lead to the closest number of words to the target sequence. We show that \\acronym achieves significantly better CER and WER over the other optimization strategies compared in Table~\\ref{tab:comparison}. This highlights the importance of optimizing for the entire set of optimal completion targets, as opposed to a single target.\n\\comment{\nTable \\ref{tab:results} summarizes prior work and our experimental results. First, we note that our baseline seq2seq model trained with MLE achieves a $3.6$\\% CER and $10.6$\\% WER, considerably better than most published prior work. We attribute this to very effective hyperparameter tuning. We also report a SS model of $3.6$\\% CER and $10.2$\\% WER.\n\n\\new{The SS baseline is trained with a schedule that starts off with MLE training, then transitions to conditioning samples generated by the model. The schedule linearly increases the sampling probability from $0$ to $100$\\% over $1$M training steps. The best validation WER is achieved at $600$k steps where the sampling probability is just under $70$\\%. We find that SS is unable to improve over our MLE baseline in terms of CER.}\nThe scheduled sampling baseline feeds the ground truth with zero probability of sampling from the model at the start of the training. Then linearly increases the probability of sampling from the model to reach $100\\%$ sampling at $1000$K training iterations. However, the best validation WER is achieved at $600$K when the sampling probability hasn't reached $70\\%$ yet. We find that SS does not improve over our MLE baseline in terms of CER.\n}\n\n{\\bf State-of-the-art.} Our model trained with \\acronym optimizes for CER; we achieve 3.1\\% CER and 9.3\\% WER, substantially outperforming our baseline by 14\\% relatively on CER and 12\\% relatively on WER. In terms of CER, our work substantially outperforms prior work as compared in Table~\\ref{tab:results}, with the closest being \\cite{tjandra-icassp-2018} trained with policy gradients on CER. In terms of WER, our work is also outperforming \\cite{chan-iclr-2017}, which uses subword units while our model emits characters. \\comment{We note the much larger gap and improvement in CER over WER, this is due to the fact that in our optimization, we optimized for CER rather than WER. However, it is still encouraging to see that improving CER correlates to improvement in WER.}\n\\comment{\n\\begin{figure}\n\\centering\n\\includegraphics[width=.8\\textwidth]{beamskew.png}\n\\caption{The effect of beam search size on WER on WSJ.}\n\\label{fig:beamsearch}\n\\end{figure}}\n\\subsection{Librispeech} \n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=\\textwidth]{both3.png}\n\\caption{\\rebut{Librispeech training and validation WER per training epoch for OCD and MLE.}}\n\\label{fig:both}\n\\end{figure}\n\\begin{table}[b]\n\\centering \n\\caption{Character Error Rate (CER) and Word Error Rate (WER) on LibriSpeech test sets.}\n\\label{tab:lib}\n\\begin{tabular}{@{}lcccc@{}}\n\\toprule\n\\multirow{2}{*}{\\textbf {Model}} & \\multicolumn{2}{c}{\\textbf{test-clean}} & \\multicolumn{2}{c}{\\textbf{test-other}} \\\\% \\cmidrule(l){2-5} \n & \\textbf{CER} & \\textbf{WER} & \\textbf{CER} & \\textbf{WER} \\\\ \\midrule\n Prior Work \\\\\n \\qquad Wav2letter \\citep{collobert2016wav2letter} & 6.9 & 7.2 & - & - \\\\\n \\qquad Gated ConvNet \\citep{liptchinsky-arxiv-2017} & - &  6.7 & - & 20.8 \\\\\n \\qquad Cold Fusion \\citep{sriram2017cold} & 3.9 & 7.5 & 9.3 & 17.0 \\\\\n \\qquad Invariant Representation Learning \\citep{liang2018learning} & 3.3 & - & 11.0 & - \\\\\n \\qquad Pretraining+seq2seq+CTC  \\citep{zeyer2018improved} & - &  4.9 & - & 15.4\\\\\n \\midrule\n Our Implementation \\\\\n \\qquad seq2seq + MLE & 2.9 & 5.7 & 8.4 & 15.4\\\\\n \\qquad {\\bf seq2seq + OCD} & {\\bf 1.7} & {\\bf \\libclean} & \\bf{6.4} & \\bf{\\libother}\\\\\n \\bottomrule\n\\end{tabular}\n\\end{table}\nFor the Librispeech dataset, we train on the full training set (960h audio data) and validate our results on the dev-other set. We report the results both on the ``clean'' and ``other'' test set. We use Byte Pair Encoding (BPE) \\citep{sennrich-acl-2016} for the output token segmentation. BPE token set is an open vocabulary set since it includes the characters as well as common words and n-grams. We use 10k BPE tokens and report both CER and WER as the evaluation metric. We describe the architecture and hyperparameter details in Appendix \\ref{sec:architecture}.\n\n\\rebut{Fig.~\\ref{fig:both} shows the validation and training WER curves for MLE and OCD. OCD starts outperforming MLE on training decodings after training for 13 epochs and on validation decodings after 9 epochs.} Our MLE baseline achieves $5.7\\%$ WER, while OCD achieves $\\libclean\\%$ WER on test-clean ($21\\%$ improvement) and improves the state-of-the-art results over \\cite{zeyer2018improved}.   test-other is the more challenging test split ranked by the WER of a model trained on WSJ \\citep{librispeech} mainly because readers accents deviate more from US-English accents. On test-other our MLE baseline achieves 15.4\\%, while our OCD model achieves \\libother\\% WER, outperforming the 15.4\\% WER of \\cite{zeyer2018improved}. Table~\\ref{tab:lib} compares our results with other recent works and the MLE baseline on Librispeech. \n\n\\section{Conclusion}\n\\label{sec:conclusion}\nThis paper presents Optimal Completion Distillation (\\acronym), a training procedure for optimizing autoregressive sequence models\nbase on edit distance. OCD is applicable to on-policy or off-policy trajectories, and in this paper,\nwe demonstrate its effectiveness on samples drawn from the model in an online fashion.\nGiven any prefix, \\acronym creates an optimal extension policy by computing the exact optimal Q-values via dynamic programming.\nThe optimal extension policy is distilled by minimizing a KL divergence between the optimal policy and the model.\n\\acronym does not require MLE initialization or joint optimization with conditional log-likelihood.\n\\comment{The ideas presented here can be generalized to other evaluation metrics such as BLEU score, when bootstrapping is used to estimate\nQ-values off-line. }\\acronym achieves $3.1$\\% CER and $9.3$\\% WER on the competitive WSJ speech recognition task, and $\\libclean\\%$ WER on Librispeech\nwithout any language model.\n\\acronym outperforms all published work on end-to-end speech recognition, including our own well-tuned MLE and scheduled sampling baselines\nwithout introducing new hyper-parameters.\n\n\\ifarxiv \n\\section*{Acknowledgements}\nWe thank Geoffrey Hinton for his valuable feedback and reviews. We thank Samy Bengio, Navdeep Jaitly,\nand Jamie Kiros for their help in reviewing the manuscript as well. We thank Zhifeng Chen and\nYonghui Wu for their generous technical help. \\else\n \\ificlrfinal\n\\section*{Acknowledgements}\nWe thank Geoffrey Hinton for his valuable feedback and reviews. We thank Samy Bengio, Navdeep Jaitly,\nand Jamie Kiros for their help in reviewing the manuscript as well. We thank Zhifeng Chen and\nYonghui Wu for their generous technical help.\\fi\n\\fi\n\n\\begin{appendices}\n\n\\renewcommand\\thefigure{\\thesection.\\arabic{figure}}\n\\renewcommand\\thetable{\\thesection.\\arabic{table}}\n\n\\section{OCD Algorithm}\n\\label{sec:algorithm}\n\\begin{algorithm}[h!]\n\\caption[Caption of Alg]{EditDistanceQ op returns {\\bf Q-values} of the tokens at each time step based on the minimum edit distance between a reference sequence $r$ and a hypothesis sequence $h$ of length $t$.\n}\\label{alg:edit}\n\\begin{algorithmic}[1]\n\\For{$j$ in $(0..t)$}\n\\State $d_j \\gets j + 1$\n\\EndFor\n\\For{$i$ in $(1..t)$}\n\\State minDist $\\gets i$\n\\State subCost $\\gets i - 1$\n\\State insCost $\\gets i + 1$\n\\For{$j$ in $(0..t - 1)$}\n\\If {$h_{i - 1}$ = $r_j$}\n\\State repCost $\\gets 0$\n\\Else\n\\State repCost $\\gets 1$\n\\EndIf\n\\State cheapest $\\gets min($subCost $+$ repCost, $d_{j} + 1$, insCost)\n\\State subCost $\\gets d_j$\n\\State insCost $\\gets$ cheapest + 1\n\\State $d_j \\gets $ cheapest\n\\If {$d_j < $ minDist}\n\\State minDist  $\\gets d_j$\n\\EndIf\n\\EndFor\n\\If{minDist $= i$}\n\\State $Q_{i, r_1} \\gets 1$\n\\EndIf\n\\For{$j$ in $(1..t)$}\n\\If{$d_j = $ minDist}\n\\State $Q_{i,r_{j+1}} \\gets 1$\n\\EndIf\n\\EndFor\n\\For{all tokens k}\n\\State $Q_{i,k} \\gets Q_{i,k} - 1 - $ minDist\n\\EndFor\n\\EndFor\n\\Return $Q$\n\\end{algorithmic}\n\\end{algorithm}\n{\\bf Complexity.} The total time complexity for calculating the sequence loss using \\acronym is $O(T^2 + |V|T)$ where $V$ is the vocabulary size and $T$ is the sequence length. MLE loss has a time complexity of $O(|V|T)$ for calculating the softmax loss at each step. Therefore, assuming that $O(T) \\leq O(|V|)$ \\acronym does not change the time complexity compared to the baseline seq2seq+MLE. The memory cost of the OCD algorithm is $O(T + |V|T) = O(|V|T)$, $O(T)$ for the dynamic programming in line 4 - line 13 of Proc.~\\ref{alg:edit} and $O(|V|T)$ for storing the stepwise $Q$ values. MLE also stores the one-hot encoding of targets at each step with a cost of $O(|V|T)$. Therefore, the memory complexity does not change  compared to the MLE baseline either.\n\nAlthough the loss calculation has the same complexity as MLE, online sampling from the model to generate the input of next RNN cell (as in \\acronym and SS) is generally slower than reading the ground truth (as in MLE). Therefore, overall a naive implementation of \\acronym is $\\leq 20\\%$ slower than our baseline MLE in terms of number of step time.  However, since \\acronym is stand alone and can be trained off-policy, we can also train on stale samples and untie the input generation worker from the training workers. In this case it is as fast as the MLE baseline.\n\n\\setcounter{figure}{0} \n\\setcounter{table}{0} \n\n{\\bf Run through.} As an example of how this algorithm works, consider the sequence ``SUNDAY'' as reference and ``SATURDAY'' as hypothesis. Table~\\ref{tab:algorithm} first shows how to extract optimal targets and their respective $Q^*$-values from the table of edit distances between all prefixes of reference and all prefixes of hypothesis. At each row highlighted cells indicate the prefixes which has minimum edit distance in the row. The next character at these indices are the Optimal targets for that row. At each step the $Q^*$-value for the optimal targets is negative of the minimum edit distance and for the non-optimal characters it is one smaller.\n\nTable~\\ref{tab:algorithm} also illustrates how appending the optimal completions for the prefix ``SA'' of the hypothesis can lead to the minimum total edit distance. Concatenating with both reference suffixes, ``UNDAY'' and ``NDAY'' will result in an edit distance of $1$. Therefore, predicting ``U'' or ``N'' at step 2 can lead to the maximum attainable reward of $(-1)$. \n\n\\begin{table}[h]\n\\caption{Top: Each row corresponds to a prefix of ``SATURDAY'' and shows\nedit distances with all prefixes of ``SUNDAY'', along with the optimal targets and their $Q^*$-value at that step. The highlighted cells indicate cells with minimum edit distance at each row. Bottom: An example of appending suffixes of ``SUNDAY'' with minimum edit distance to the prefix ``SA''.}\n\\label{tab:algorithm}\n\\centering\n\\begin{tabular}{@{}|>{\\columncolor[HTML]{C0C0C0}}l|l|l|l|l|l|l|l|l|c|@{}}\n\\hline\n & \\multicolumn{7}{c|}{Edit Distance} & OCD Targets & Q-values\\\\ \\hline\n\\rowcolor[HTML]{C0C0C0}\n &  & S & U & N & D & A & Y & &  \\\\ \\hline\n & \\cellcolor[HTML]{FE996B}0 & 1 & 2 & 3 & 4 & 5 & 6 & S & 0 \\\\ \\hline\nS & 1 & \\cellcolor[HTML]{FE996B}0 & 1 & 2 & 3 & 4 & 5 & U & 0 \\\\ \\hline\nA & 2 & \\cellcolor[HTML]{FE996B}1 & \\cellcolor[HTML]{FE996B}1 & 2 & 3 & 3 & 4 & U, N & -1 \\\\ \\hline\nT & 3 & \\cellcolor[HTML]{FE996B}2 & \\cellcolor[HTML]{FE996B}2 & \\cellcolor[HTML]{FE996B}2 & 3 & 4 & 4 & U, N, D & -2 \\\\ \\hline\nU & 4 & 3 & \\cellcolor[HTML]{FE996B}2 & 3 & 3 & 4 & 5 & N & -2 \\\\ \\hline\nR & 5 & 4 & \\cellcolor[HTML]{FE996B}3 & \\cellcolor[HTML]{FE996B}3 & 4 & 4 & 5 & N, D & -3 \\\\ \\hline\nD & 6 & 5 & 4 & 4 & \\cellcolor[HTML]{FE996B}3 & 4 & 5 & A & -3 \\\\ \\hline\nA & 7 & 6 & 5 & 5 & 4 & \\cellcolor[HTML]{FE996B}3 & 4 & Y & -3 \\\\ \\hline\nY & 8 & 7 & 6 & 6 & 5 & 4 & \\cellcolor[HTML]{FE996B}3 & \\textless{}/s\\textgreater~& -3 \\\\ \\hline\n\\end{tabular}\\\\ \\vspace{.3cm}\n\\begin{tabular}{|\n>{\\columncolor[HTML]{32CB00}}l |l|l|l|l|l|l|l|}\n\\hline\n\\cellcolor[HTML]{C0C0C0} & \\cellcolor[HTML]{C0C0C0} & \\cellcolor[HTML]{C0C0C0}S & \\cellcolor[HTML]{32CB00}U & \\cellcolor[HTML]{32CB00}N & \\cellcolor[HTML]{32CB00}D & \\cellcolor[HTML]{32CB00}A & \\cellcolor[HTML]{32CB00}Y \\\\ \\hline\n\\cellcolor[HTML]{C0C0C0} & \\cellcolor[HTML]{FE996B}0 & 1 & 2 & 3 & 4 & 5 & 6 \\\\ \\hline\n\\cellcolor[HTML]{C0C0C0}S & 1 & \\cellcolor[HTML]{FE996B}0 & 1 & 2 & 3 & 4 & 5 \\\\ \\hline\n\\cellcolor[HTML]{C0C0C0}A & 2 & \\cellcolor[HTML]{32CB00}1 & \\cellcolor[HTML]{FE996B}1 & 2 & 3 & 3 & 4 \\\\ \\hline\nU &  &  & \\cellcolor[HTML]{32CB00}1 &  &  &  &  \\\\ \\hline\nN &  &  &  & \\cellcolor[HTML]{32CB00}1 &  &  &  \\\\ \\hline\nD &  &  &  &  & \\cellcolor[HTML]{32CB00}1 &  &  \\\\ \\hline\nA &  &  &  &  &  & \\cellcolor[HTML]{32CB00}1 &  \\\\ \\hline\nY &  &  &  &  &  &  & \\cellcolor[HTML]{32CB00}1 \\\\ \\hline\n\\end{tabular}~~\n\\begin{tabular}{|l|l|l|l|l|l|l|l|}\n\\hline\n\\rowcolor[HTML]{32CB00} \n\\cellcolor[HTML]{C0C0C0} & \\cellcolor[HTML]{C0C0C0} & \\cellcolor[HTML]{C0C0C0}S &  \\cellcolor[HTML]{C0C0C0} U & N & D & A & Y \\\\ \\hline\n\\cellcolor[HTML]{C0C0C0} & \\cellcolor[HTML]{FE996B}0 & 1 & 2 & 3 & 4 & 5 & 6 \\\\ \\hline\n\\cellcolor[HTML]{C0C0C0}S & 1 & \\cellcolor[HTML]{FE996B}0 & 1 & 2 & 3 & 4 & 5 \\\\ \\hline\n\\cellcolor[HTML]{C0C0C0}A & 2 & \\cellcolor[HTML]{FE996B}1 & \\cellcolor[HTML]{32CB00}1 & 2 & 3 & 3 & 4 \\\\ \\hline\n\\cellcolor[HTML]{32CB00}N &  &  &  & \\cellcolor[HTML]{32CB00}1 &  &  &  \\\\ \\hline\n\\cellcolor[HTML]{32CB00}D &  &  &  &  & \\cellcolor[HTML]{32CB00}1 &  &  \\\\ \\hline\n\\cellcolor[HTML]{32CB00}A &  &  &  &  &  & \\cellcolor[HTML]{32CB00}1 &  \\\\ \\hline\n\\cellcolor[HTML]{32CB00}Y &  &  &  &  &  &  & \\cellcolor[HTML]{32CB00}1 \\\\ \\hline\n\\multicolumn{7}{c}{}\\\\\n\\end{tabular}\n\\end{table}\n\n\\section{Exposure bias}\n\\label{sec:exposure}\nA key limitation of teacher forcing for sequence learning stems from the discrepancy between the training\nand test objectives. One trains the model using conditional log-likelihood $\\mathcal{O}_{\\text{CLL}}$, but evaluates the quality of the model using\nempirical reward $\\mathcal{O}_{\\text{ER}}$.\n\n\\begin{figure}[t]\n\\vspace*{-.2cm}\n\\begin{center}\n\\begin{tabular}{c@{\\hspace*{2cm}}c}\n      (a) {Teacher Forcing (MLE)} &\n      (b) {Scheduled Sampling} \\\\[-.1cm]\n      \\includegraphics[width=0.35\\textwidth]{rnn-tf.png} &\n      \\includegraphics[width=0.35\\textwidth]{rnn-ss.png} \\\\[-.1cm]\n      \\includegraphics[width=0.35\\textwidth]{rnn-policy.png} &\n      \\includegraphics[width=0.35\\textwidth]{rnn-ocd.png} \\\\[-.1cm]\n      (c) {Policy Gradient} &\n      (d) {Optimal Completion Distillation}\\\\\n\\vspace*{-.2cm}\n\\end{tabular}\n\\end{center}\n\\comment{\n                \\end{tabular}\n                \\end{figure}\n                \\begin{figure}[t]\n                  \\centering\n                  \\begin{subfigure}[t]{0.5\\textwidth}\n                      \\centering\n                      \\includegraphics[width=0.75\\textwidth]{rnn-tf}\n                      \\caption{Teacher Forcing}\n                      \\label{subfig:teacher-forcing}\n                  \\end{subfigure}%\n                  \\begin{subfigure}[t]{0.5\\textwidth}\n                      \\centering\n                      \\includegraphics[width=0.75\\textwidth]{rnn-ss}\n                      \\caption{Scheduled Sampling}\n                      \\label{subfig:ss}\n                  \\end{subfigure}\n                  \\begin{subfigure}[t]{0.5\\textwidth}\n                      \\centering\n                      \\includegraphics[width=0.75\\textwidth]{rnn-policy}\n                      \\caption{Policy Gradient}\n                      \\label{subfig:policy}\n                  \\end{subfigure}%\n                  \\begin{subfigure}[t]{0.5\\textwidth}\n                      \\centering\n                      \\includegraphics[width=0.75\\textwidth]{rnn-ocd}\n                      \\caption{Optimal Completion Distillation}\n                      \\label{subfig:ocd}\n                  \\end{subfigure}\n                  \\subref{subfig:teacher-forcing}\n}\n\\caption{Illustration of different training strategies for autoregressive sequence models.\n(a) Teacher Forcing: the model conditions on correct prefixes and is taught to predict the next ground truth token.\n(b) Scheduled Sampling: the model conditions on tokens either from ground truth or drawn from the model and is taught to predict the next ground truth token regardless.\n(c) Policy Gradient: the model conditions on prefixes drawn from the model and is encouraged to reinforce sequences with a large sequence reward $R(\\tilde y)$.\n(d) Optimal Completion Distillation: the model conditions on prefixes drawn from the model and is taught to predict an optimal completion policy $\\pi^*$ specific to the prefix.}\n\\label{fig:trainingstrats}\n\\end{figure}\n\nUnlike teacher forcing and Scheduled Sampling (SS), policy gradient approaches (\\eg~\\cite{ranzato-iclr-2016,bahdanau-iclr-2017})\nand OCD aim to optimize the empirical reward objective \\eqref{eq:reward} on the training set. We illustrate four different training strategies\nof MLE, SS, Policy Gradient and OCD in \\figref{fig:trainingstrats}. The drawback of policy gradient techniques is twofold: 1) they cannot\neasily incorporate ground truth sequence information except through the reward function, and 2) they have difficulty reducing the variance of the gradients to\nperform proper credit assignment. Accordingly, most policy gradient approaches~\\cite{ranzato-iclr-2016,bahdanau-iclr-2017,wu2016google} pre-train the model\nusing teacher forcing. By contrast, the OCD method proposed in this paper defines an optimal completion policy $\\pi^*_t$\nfor any {\\em off-policy} prefix by incorporating the ground truth information. Then, OCD optimizes a token level log-loss\nand alleviates the credit assignment problem. Finally, training is much more stable, and we do not require initialization nor joint optimization with MLE.\n\nThere is an intuitive notion of {\\em exposure bias}~\\cite{ranzato-iclr-2016} discussed in the literature as a limitation of teacher forcing.\nWe formalize this notion as follows. One can think of the optimization of the log loss~\\eqref{eq:cll} in an autoregressive models as a classification problem, where the input to the classifier is a tuple $(\\bs, \\bys_{<t})$ and the correct output is $\\a^*_i$, where $\\bas_{<t} \\equiv (y^*_1, \\ldots, y^*_{t-1})$. Then the training dataset comprises different examples and different prefixes of the ground truth sequence. The key challenge is that once the model is trained, one should not expect the model to generalize to a new prefix $\\by_{<t}$ that does not come from the training distribution of $P(\\bys_{<t})$. This problem can\nbecome severe as $\\by_{<t}$ becomes more dissimilar to correct prefixes. During inference, when one conducts beam search with a large beam size then\none is more likely to discover wrong generalization of $\\pit(\\hat{y}_{t} | \\bai_{<t}, \\bx)$, because the sequence is optimized globally.\nA natural strategy to remedy this issue is to train on arbitrary prefixes. Unlike the aforementioned techniques OCD can train on any prefix\ngiven its {\\em off-policy} nature.\n\nFigure~\\ref{fig:beam} illustrates how increasing the beam size for MLE and SS during inference decreases their performance on WSJ datasets to above $11\\%$ WER. \\acronym suffers a degradation in the performance too but it never gets above $10\\%$ WER.\n\\begin{figure}[H]\n\\centering\n  \\includegraphics[width=.57\\linewidth]{beamdown.png}\n\\caption{Word Error Rate (WER) of WSJ with MLE, SS and OCD for different beam sizes.\n}\n\\label{fig:beam}\n\\end{figure}\n\n\\section{Architecture}\n\\label{sec:architecture}\n{\\bf WSJ.} The input audio signal is converted into $80$-dimensional filterbank features computed every $10$ms with delta and delta-delta acceleration, normalized with per-speaker mean and variance generated by Kaldi \\citep{povey2011kaldi}. Our encoder uses 2-layers of convolutions with $3 \\times 3$ filters, stride $2 \\times 2$ and $32$ channels, followed by a convolutional LSTM with 1D-convolution of filter width $3$, followed by 3 LSTM layers with $256$ cell size. We also apply batch-normalization between each layer in the encoder. The attention-based decoder is a 1-layer LSTM with $256$ cell size with content-based attention. We \\rebut{use Xavier initializer~\\citep{glorot2010understanding} and }train our models for 300 epochs of batch size 8 with 8 async workers. We separately tune the learning rate for our baseline and OCD model, $0.0007$ for OCD vs $0.001$ for baseline. We apply a single $0.01$ drop of learning rate when validation CER plateaus, the same as for our baseline. Both happen around 225 epoch. We implemented our experiments\\footnote{We are in the process of releasing the code for \\acronym.} in TensorFlow \\citep{abadi2016tensorflow}.\n\n{\\bf Librispeech.} Since the dataset is larger than WSJ, we use a larger batch size of 16, smaller learning rate of $0.0005$ for baseline and $0.0003$ for \\acronym. Models are trained for 70 epochs. We remove the convolutional LSTM layers of the encoder, increase the number of LSTM layers in the encoder to 6, and increase the LSTM cell size to 384. All other configs are the same as the WSJ setup.\n\n\\section{Hamming distance VS Edit Distance during training}\n\\label{sec:hamm}\nFigure~\\ref{fig:hamedit} plots the edit distance on training data of \\acronym and MLE for fixed hamming distances during training. The plot shows that for a fixed Hamming distance (which is the metric that MLE correlates with more), \\acronym achieves a lower edit distance compared to MLE. This gives evidence that \\acronym is indeed optimizing for edit distance as intended.\n\n\\begin{figure}[h]\n\\centering\n  \\includegraphics[width=.5\\linewidth]{hamm_edit2.png}\n\\caption{WSJ training Character Error Rate (CER) of MLE and OCD over Character Accuracy at different checkpoints during training. \n}\n\\label{fig:hamedit}\n\\end{figure}\n\n\\comment{% Please add the following required packages to your document preamble:\n\\begin{table}[]\n\\comment{\\begin{tabular}{|l|l|l|l|l|l|l|l|}\n\\hline\n\\rowcolor[HTML]{C0C0C0} \n\\cellcolor[HTML]{FFFFFF}{\\color[HTML]{000000} } &  & S & U & N & D & A & Y \\\\ \\hline\n\\cellcolor[HTML]{C0C0C0} & 0 & 1 & 2 & 3 & 4 & 5 & 6 \\\\ \\hline\n\\cellcolor[HTML]{C0C0C0}S & 1 & 0 & 1 & 2 & 3 & 4 & 5 \\\\ \\hline\n\\cellcolor[HTML]{C0C0C0}A & 2 & 1 & 1 & 2 & 3 & 3 & 4 \\\\ \\hline\n\\cellcolor[HTML]{C0C0C0}T & 3 & \\cellcolor[HTML]{FE996B}2 & \\cellcolor[HTML]{FE996B}2 & \\cellcolor[HTML]{FE996B}2 & 3 & 4 & 4 \\\\ \\hline\n? &  &  &  &  &  &  &  \\\\ \\hline\n? &  &  &  &  &  &  &  \\\\ \\hline\n? &  &  &  &  &  &  &  \\\\ \\hline\n? &  &  &  &  &  &  &  \\\\ \\hline\n? &  &  &  &  &  &  &  \\\\ \\hline\n\\end{tabular}~}\\begin{tabular}{|\n>{\\columncolor[HTML]{32CB00}}l |l|l|l|l|l|l|l|}\n\\hline\n\\cellcolor[HTML]{FFFFFF}{\\color[HTML]{000000} } & \\cellcolor[HTML]{C0C0C0} & \\cellcolor[HTML]{C0C0C0}S & \\cellcolor[HTML]{32CB00}U & \\cellcolor[HTML]{32CB00}N & \\cellcolor[HTML]{32CB00}D & \\cellcolor[HTML]{32CB00}A & \\cellcolor[HTML]{32CB00}Y \\\\ \\hline\n\\cellcolor[HTML]{C0C0C0} & 0 & 1 & 2 & 3 & 4 & 5 & 6 \\\\ \\hline\n\\cellcolor[HTML]{C0C0C0}S & 1 & 0 & 1 & 2 & 3 & 4 & 5 \\\\ \\hline\n\\cellcolor[HTML]{C0C0C0}A & 2 & 1 & 1 & 2 & 3 & 3 & 4 \\\\ \\hline\n\\cellcolor[HTML]{C0C0C0}T & 3 & \\cellcolor[HTML]{32CB00}2 & \\cellcolor[HTML]{FE996B}2 & \\cellcolor[HTML]{FE996B}2 & 3 & 4 & 4 \\\\ \\hline\nU &  &  & \\cellcolor[HTML]{32CB00}2 &  &  &  &  \\\\ \\hline\nN &  &  &  & \\cellcolor[HTML]{32CB00}2 &  &  &  \\\\ \\hline\nD &  &  &  &  & \\cellcolor[HTML]{32CB00}2 &  &  \\\\ \\hline\nA &  &  &  &  &  & \\cellcolor[HTML]{32CB00}2 & \\cellcolor[HTML]{FFFFFF} \\\\ \\hline\nY &  &  &  &  &  &  & \\cellcolor[HTML]{32CB00}2 \\\\ \\hline\n\\end{tabular}~\\begin{tabular}{|l|l|l|l|l|l|l|l|}\n\\hline\n\\rowcolor[HTML]{32CB00} \n\\cellcolor[HTML]{FFFFFF}{\\color[HTML]{000000} } & \\cellcolor[HTML]{C0C0C0} & \\cellcolor[HTML]{C0C0C0}S & \\cellcolor[HTML]{C0C0C0}U & N & D & A & Y \\\\ \\hline\n\\cellcolor[HTML]{C0C0C0} & 0 & 1 & 2 & 3 & 4 & 5 & 6 \\\\ \\hline\n\\cellcolor[HTML]{C0C0C0}S & 1 & 0 & 1 & 2 & 3 & 4 & 5 \\\\ \\hline\n\\cellcolor[HTML]{C0C0C0}A & 2 & 1 & 1 & 2 & 3 & 3 & 4 \\\\ \\hline\n\\cellcolor[HTML]{C0C0C0}T & 3 & \\cellcolor[HTML]{FE996B}2 & \\cellcolor[HTML]{32CB00}2 & \\cellcolor[HTML]{FE996B}2 & 3 & 4 & 4 \\\\ \\hline\n\\cellcolor[HTML]{32CB00}N &  &  &  & \\cellcolor[HTML]{32CB00}2 &  &  &  \\\\ \\hline\n\\cellcolor[HTML]{32CB00}D &  &  &  &  & \\cellcolor[HTML]{32CB00}2 &  &  \\\\ \\hline\n\\cellcolor[HTML]{32CB00}A &  &  &  &  &  & \\cellcolor[HTML]{32CB00}2 & \\cellcolor[HTML]{FFFFFF} \\\\ \\hline\n\\cellcolor[HTML]{32CB00}Y &  &  &  &  &  &  & \\cellcolor[HTML]{32CB00}2 \\\\\\hline\n\\cellcolor[HTML]{C0C0C0} &  &  &  &  &  &  &  \\\\ \\hline\n\\end{tabular}~\\begin{tabular}{|\n>{\\columncolor[HTML]{C0C0C0}}l |l|l|l|l|l|l|l|}\n\\hline\n\\cellcolor[HTML]{FFFFFF}{\\color[HTML]{000000} } & \\cellcolor[HTML]{C0C0C0} & \\cellcolor[HTML]{C0C0C0}S & \\cellcolor[HTML]{C0C0C0}U & \\cellcolor[HTML]{C0C0C0}N & \\cellcolor[HTML]{32CB00}D & \\cellcolor[HTML]{32CB00}A & \\cellcolor[HTML]{32CB00}Y \\\\ \\hline\n & 0 & 1 & 2 & 3 & 4 & 5 & 6 \\\\ \\hline\nS & 1 & 0 & 1 & 2 & 3 & 4 & 5 \\\\ \\hline\nA & 2 & 1 & 1 & 2 & 3 & 3 & 4 \\\\ \\hline\nT & 3 & \\cellcolor[HTML]{FE996B}2 & \\cellcolor[HTML]{FE996B}2 & \\cellcolor[HTML]{32CB00}2 & 3 & 4 & 4 \\\\ \\hline\n\\cellcolor[HTML]{32CB00}D &  &  &  &  & \\cellcolor[HTML]{32CB00}2 &  &  \\\\ \\hline\n\\cellcolor[HTML]{32CB00}A &  &  &  &  &  & \\cellcolor[HTML]{32CB00}2 & \\cellcolor[HTML]{FFFFFF} \\\\ \\hline\n\\cellcolor[HTML]{32CB00}Y &  &  &  &  &  &  & \\cellcolor[HTML]{32CB00}2 \\\\ \\hline\n &  &  &  &  &  &  &  \\\\ \\hline\n &  &  &  &  &  &  &  \\\\ \\hline\n\\end{tabular}\n\\end{table}\n\\begin{table}[]\n\n\\end{table}}\n\\end{appendices}\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{On the Choice of Modeling Unit for Sequence-to-Sequence Speech Recognition}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\nIn conventional speech recognition, phoneme-based models\noutperform grapheme-based models for non-phonetic languages such as English.\nThe performance gap between the two typically reduces as the amount of training data\nis increased.\nIn this work, we examine the impact of the choice of modeling unit for\nattention-based encoder-decoder models.\nWe conduct experiments on the LibriSpeech 100hr, 460hr, and 960hr tasks, using\nvarious target units (phoneme, grapheme, and word-piece);\nacross all tasks, we find that grapheme or word-piece models consistently\noutperform phoneme-based models, even though they are evaluated without a\nlexicon or an external language model.\nWe also investigate model complementarity: we find that we can improve WERs by up to 9\\% relative by\nrescoring N-best lists generated from a strong word-piece based baseline with either the\nphoneme or the grapheme model.\nRescoring an N-best list generated by the phonemic system, however, provides\nlimited improvements.\nFurther analysis shows that the word-piece-based models produce more diverse\nN-best hypotheses, and thus lower oracle WERs, than phonemic models.\n\\end{abstract}\n\\noindent{\\bf Index Terms}:\nEnd-to-end speech recognition, word-pieces, graphemes, phonemes, sequence-to-sequence\\let\\thefootnote\\relax\\footnote{*Work performed during internship at Google.\nWe thank Tara Sainath and Yu Zhang for helpful discussion,\nand Jinxi Guo for sharing his language model setup.\nAn initial version of this paper appears as a pre-print \\cite{irie2019}.}\n\\addtocounter{footnote}{-1}\\let\\thefootnote\\svthefootnote\n\\vspace{-2mm}\n\\section{Introduction}\n\\label{sec:intro}\nSequence-to-sequence learning~\\cite{seq2seq} based on encoder-decoder attention\nmodels~\\cite{bahdanau2014neural} has become popular for both machine\ntranslation~\\cite{wu2016google} and speech recognition~\\cite{RPis2017,\nKimHW17, battenberg2017exploring, WengCWWYSY18, ChiuSWPNCKWRGJL18}.\nSuch models are typically trained to output \\emph{character-based} units:\ngraphemes, byte-pair encodings (BPEs)~\\cite{sennrich16bpe}, or\nword-pieces~\\cite{SchusterN12}, which allow the model to directly map\nthe frame-level input audio features to the\noutput word sequence, without using a hand-crafted pronunciation lexicon.\nThus, when using such character-based output units, end-to-end speech\nrecognition models~\\cite{lasicassp2016} jointly learn the acoustic model,\npronunciation model, and language model within a single neural network.\nIn fact, such models outperform conventional hybrid recognizers~\\cite{Bourlard1993} when trained on\nsufficiently large amounts of data~\\cite{ChiuSWPNCKWRGJL18}.\n\nOne of the main advantages of character-based sequence-to-sequence models lies in their simplicity:\nboth for training, as well as decoding. \nIn fact, the use of characters as units for acoustic modeling has a long history for conventional\nHMM-based automatic speech recognition (ASR) systems (e.g., \\cite{kanthak02, KillerSS03, SungHBS09}, inter alia).\nIn the context of conventional ASR systems, for non-phonetic languages such as English, where the correspondence between orthography and pronunciation is less clear, previous works \\cite{kanthak02, KillerSS03} have found that phoneme-based models outperform grapheme-based models; grapheme-based systems approach the performance of phoneme-based systems only when much larger amounts of training training data are available \\cite{SungHBS09}.\nIt is therefore, natural to ask whether similar observations also apply to recently proposed attention-based encoder-decoder models: specifically, how do attention-based encoder-decoder models perform when using phonemes instead of character-based output units? \nTo the best of our knowledge, this question has only been empirically investigated in the setting where a large amount of labeled training data are available. In previous work \\cite{sainath2017no, zhou2018comparison}, it has been empirically shown that the grapheme-based encoder-decoder models outperform the phoneme-based approach, while \\cite{sainath2017no} find that use of lexica is still useful for recognizing rare words such as named entities. \n\nIn this work, we first investigate whether the previous result \\cite{sainath2017no} which establishes the dominance of lexicon-free graphemic models over the phoneme-based models also hold on tasks with smaller amounts of training data.\nWe carry out evaluations on the three subsets of the LibriSpeech task \\cite{panayotov2015librispeech}: 100hr, 460hr, and 960hr, where we find that grapheme or word-piece models do indeed consistently outperform phoneme-based models, even when training data is limited.\nIn Sec.~\\ref{sec:part2}, we further investigate the benefits offered by phonemic models by studying the complementarity of different units. In experimental evaluations, we find that simple N-best list rescoring results in large improvements in WER. Finally, we conduct a detailed analysis of the differences in the hypotheses produced by the models with various output units, in terms of quality of the top hypotheses, as well as the oracle error rate of the N-best list.\n\\begin{figure}[b]\n\t\\vspace{-5mm}\n\t\\centerline{\\includegraphics[width=0.4\\columnwidth]{las_base.pdf}}\n\t\\caption{\\it LAS model.}\n\t\\label{fig:las_base}\n\\end{figure}\n\\vspace{-2mm}\n\\section{Sequence-to-Sequence Speech Models}\n\\label{sec:model}\nAll our models are Listen, Attend, and Spell (LAS)~\\cite{lasicassp2016} speech models.\nThe LAS model, which is depicted in Figure \\ref{fig:las_base}, has encoder, attention, and decoder modules. % as depicted in Figure~\\ref{fig:las}(a).\nThe \\textit{encoder} transforms the input frame-level audio feature sequence into\na sequence of hidden activations. The \\textit{attention module} summarizes the\nencoder sequence into a single vector for each prediction step, and finally,\nthe \\textit{decoder} models the distribution of the output sequence conditioned\non the history of previously predicted labels.\nBoth the encoder and the decoder are modeled using recurrent neural networks,\nand thus the entire model can be jointly optimized.\nWe refer the interested reader to~\\cite{lasicassp2016, RPis2017, WeissCJWC17} for more details.\nStandard LAS models use \\textit{character-based} output units:\ngrapheme \\cite{lasicassp2016}, word-piece \\cite{ChiuSWPNCKWRGJL18} or BPE \\cite{zeyer2018:asr-attention}.\n\\vspace{-2mm}\n\\section{Phonemic Sequence-to-Sequence Model}\n\\vspace{-1mm}\nThe phonemic LAS model can be obtained by using phonemes as the output unit.\nPhonemes are natural labels for acoustic modeling of non-phonetic languages.\nThe use of a pronunciation lexicon can also ease integration of\ncompletely new words or named entities\\footnote{This might not be as relevant\nfor LibriSpeech evaluation as we use the official LibriSpeech lexicon without\nmodification.}\\cite{bruguier19}.\nHowever, by using a pronunciation lexicon, we give\nup the end-to-end approach, which introduces complications for both training and\ndecoding.\n\nFor training, words with multiple pronunciation variants cause a problem, since\nthere is no unique mapping from such a word to its corresponding phoneme\nsequence.\nWhile we can potentially obtain the correct pronunciation variant by generating\nalignments, we skip this extra effort by choosing a pronunciation simply by\nrandomly choosing one of the pronunciations for each word to define a unique\nmapping.\nIn addition, we include an unknown token \\texttt{UNK} as a part of the phoneme vocabulary and\nuse it to represent words which are not included in the lexicon.\nWe use a dedicated end-of-word token \\texttt{EOW} (as part of the phoneme\ninventory) to model word boundaries, as in~\\cite{sainath2017no}, which we\nfind improves performance.\n\nTo deal with the ambiguity of homophones\\footnote{By choosing phonemes as output units,\nwe are giving up the standalone recognition using the attention-based model. Also, while \nacoustic modeling motivates the use of phonemes, the ability\nof the decoder as a language model can possibly be weaker compared with character-based units,\nsince the phoneme-level language model can be viewed as a subword-level class-based language model \\cite{Brown92}\nwhere the clusters are formed based on the phonemic similarity.}\nduring decoding, we incorporate a\n(word-based) n-gram language model. We use a general weighted finite-state\ntransducer (WFST) decoder to perform a beam search. The lexicon and language\nmodel (LM) are represented as WFST $L$ and $G$ respectively and combined by means of\nFST composition as the search network $L\\circ G$~\\cite{mohri2008}. The search\nprocess then explores partial path hypotheses which are constrained by the\nsearch network and scored by both the LAS model and the n-gram language model.\n\\vspace{-6mm}\n\\section{LibriSpeech Experimental Setup}\n \\subsection{Dataset}\n \\vspace{-1mm}\nThe LibriSpeech task \\cite{panayotov2015librispeech} has three subsets with different amounts of\ntranscribed training data: 100hr, 460hr, and 960hr. A lexicon with pronunciations for 200K words\nis officially distributed. The development and test data are both split into \\textit{clean}\nand \\textit{other} subsets, each of them consisting of about 5 to 6 hours of audio. The number of unique words\nobserved in each subset as well as the out-of-vocabulary (OOV; unseen in training\ndata) rate is summarized in Table~\\ref{tab:oov}.\nFor language modeling, extra text-only data of about 800M words is also\navailable, along with an officially distributed 3-gram word LM; we use the\nunpruned 3-gram LM for decoding the phonemic LAS models.\nIn contrast, the grapheme and word-piece models are evaluated without a lexicon or a language model (unless otherwise indicated).\nWe train word piece models~\\cite{SchusterN12} of size 16K (16,384) on each training subset.\n\\begin{table}[h]\n\t\\centering\n\t\\caption{\\it Out-of-vocabulary (OOV) rates (\\%) with respect to the\n\tvocabulary (unique word list) in different data scenarios, and with respect to the pronunciation lexicon.}\n\t \\label{tab:oov}\n\t \t\\vspace{-2mm}\n\t\\begin{tabular}{ |c|c|c|c|c|c|} \\hline\n\t\tTraining  & Vocab.    & \\multicolumn{2}{|c|}{dev} & \\multicolumn{2}{|c|}{test}    \\\\ \\cline{3-6}\n\t\tdata (h)  & Size      & clean & other & clean & other    \\\\ \\hline\n\t\t100   &  34~K &  2.5  & 2.5 & 2.4 & 2.8  \\\\\n\t\t460   &  66~K &  0.9  & 1.2 & 1.0 & 1.3  \\\\\n\t\t960   &  89~K &  0.6  & 0.8 & 0.6 & 0.8  \\\\  \\hline \\hline\n\t\tLexicon  & 200~K   & 0.3  & 0.6 & 0.4 & 0.5  \\\\ \\hline\n\t\\end{tabular}\n\\vspace{-5mm}\n\\end{table}\n\\vspace{-2mm}\n\\subsection{Models and training}\n\\vspace{-1mm}\n\\label{sec:model_tr}\nWe use 80-dimensional log-mel features with deltas and accelerations as the\nframe-level audio input features.\nReducing input frame rate in the encoder is important for successfully training\nsequence-to-sequence speech models, especially for tasks such as LibriSpeech\nwhich feature long utterances ($\\sim$15s).\nThus, following~\\cite{zhangCJ17}, our encoder layers include two layers of 3$\\times$3 convolution\nwith 32 channels with a stride of 2, which results in a total time reduction factor of 4.\nWe consider three model (\\textit{small}, \\textit{medium}, and \\textit{large})\nwhich differ in terms of the sizes of model components.\nOn top of the convolutional layers, the encoder contains 3 (\\textit{small}) or 4\n(\\textit{medium} and \\textit{large}) layers of bi-directional\nLSTMs~\\cite{schuster1997bidirectional}, with either 256 (\\textit{small}), 512\n(\\textit{medium}), or 1024 (\\textit{large}) LSTM \\cite{hochreiter1997long} cells\nin each layer.\nA projection layer and batch normalization are applied after each LSTM encoder\nlayer~\\cite{zhangCJ17}.\nThe decoder consists of 1 (\\textit{small}) or 2 (\\textit{medium} and\n\\textit{large}) LSTM layers, and uses \\emph{additive attention} as described in~\\cite{WeissCJWC17}.\n\nWe train all models using 16 GPUs by asynchronous stochastic gradient descent\nwith Adam optimizer~\\cite{kingma15} from random initialization without any\nspecial pre-training method\\footnote{We find training to be stable across repeated runs.\nWe avoid plateaus at the beginning of training (which we often observe) by tuning the initial learning rate.\nWe find that our models achieve the best WER on the dev-clean, earlier than on the dev-other set.} for about 80 epochs.\nWe use open-source Tensorflow Lingvo toolkit \\cite{shen2019lingvo} for all experiments. Our\ngrapheme and word-piece based baseline configurations are publicly available online\\footnote{\\url{https://github.com/tensorflow/lingvo}}\nwhere further details about the models can be found.\n\n\\vspace{-2mm}\n\\section{Standalone Performance Results}\n\\label{sec:part1}\n\\subsection{Baseline model performance on 960hr}\n\\vspace{-1mm}\nThe WER performance of grapheme and word-piece based models is summarized in\nTable~\\ref{baseline960}.\nFor both graphemes and word-pieces, we present the performance for \\textit{small}, \\textit{medium} and \\textit{large}\nmodel sizes (as shown by different numbers of parameters) as described in Sec \\ref{sec:model_tr}. The difference of number of parameters between different units only comes from the unit-level vocabulary size.\nAs can be seen in Table \\ref{baseline960}, models benefit from the additional parameters and the best\nWERs are obtained for the large word-piece model.\n\\vspace{-2mm}\n\t\\begin{table}[h]\n\t\t\\centering\n\t\t\\caption{\\it WERs (\\%) for grapheme and word-piece models.}\n\t\t\t \t\\vspace{-2mm}\n\t\t\\label{baseline960}\n\t\t\\begin{tabular}{ |l|c|c|c|c|c|} \\hline\n\t\t\t\\multirow{2}{*}{Unit}      & \\multirow{2}{*}{Param.}  & \\multicolumn{2}{|c|}{dev} & \\multicolumn{2}{|c|}{test}    \\\\ \\cline{3-6}\n\t\t\t&   & clean & other & clean & other    \\\\ \\hline\n\t\t\t\\multirow{3}{*}{Grapheme}   & 7~M    & 7.6 & 20.5 & 7.9 & 21.3  \\\\\n\t\t\t& 35~M   & 5.3 & 15.6 & 5.6 & 15.8  \\\\\n\t\t\t& 130~M  & 5.3 & 15.2 & 5.5 & 15.3  \\\\ \\hline\n\t\t\t\\multirow{3}{*}{Word-Piece} & 20~M   & 5.8 & 16.0 & 6.1 & 16.4 \\\\\n\t\t\t& 60~M   & 4.9 & 14.0 & 5.0 & 14.1 \\\\\n\t\t\t& 180~M  & \\textbf{4.4} & \\textbf{13.2} & \\textbf{4.7} & \\textbf{13.4} \\\\ \\hline\n\t\t\\end{tabular}\n\t\\end{table}\n\\subsection{Phonemic model performance on 960hr}\n\t\t\\vspace{-1mm}\nFor phoneme based models, we first check the phoneme error rates (PER) in order to make\nsure that the models are reasonable\\footnote{By increasing the model size from 7~M to 35~M, then to 130~M, we improve\nthe PERs (\\%) from (3.2, 9.7, 3.2, 9.9), to (2.8, 8.9, 3.0, 9.1), then to  (2.4, 7.9, 2.5, 7.7) on the \ndev-clean/other, test-clean/other sets.}.\nThe WER performance results for decoding with the\nlexicon and the 3-gram word LM (88M n-grams) is shown in Table \\ref{960overview}.\nWe observe that despite the use of an external LM\nwhich is trained on much more data than the transcribed acoustic training data, the phonemic system performs worse\nthan the best graphemic model\\footnote{This is similar to what is reported in \\cite{sainath2017no}. Though, we note that we get about 2\\% absolute degradation in WERs\nwith a model trained without \\texttt{EOW} compared with the model with \\texttt{EOW}.}.\nIt is nevertheless interesting to examine examples where the phonemic model outperforms the best word-piece\nmodel. In Table \\ref{plg_vs_wp}, we present some illustrative examples.\nIn addition, we find that decoding the graphemic model with the 3-gram word LM does not give improvement.\n\\begin{table}[h]\n\t\\centering\n\t\\setlength{\\tabcolsep}{0.5em}\n\t\\vspace{-2mm}\n\t\\caption{\\it WERs (\\%) for the \\textbf{960hr} dataset.}\n\t\\vspace{-4mm}\n\t\\label{960overview}\n\t\\begin{tabular}{ |l|c|c|c|c|c|} \\hline\n\t\t\\multirow{2}{*}{Unit} & \\multirow{2}{*}{LM} & \\multicolumn{2}{|c|}{dev} & \\multicolumn{2}{|c|}{test}    \\\\ \\cline{3-6}\n\t\t& & clean & other & clean & other    \\\\ \\hline\n\t\tPhoneme      & 3-gram & 5.6 & 15.8 & 6.2 & 15.8  \\\\\n\t\tGrapheme     & None & 5.3 & 15.2 & 5.5 & 15.3  \\\\ \n\t\tWord-Piece 16K & None & 4.4 & 13.2 & 4.7 & 13.4 \\\\ \\hline\n\t\tWord-Piece 16K & LSTM & \\textbf{3.3} & \\textbf{10.3} & \\textbf{3.6} & \\textbf{10.3}  \\\\ \\hline \\hline\n\t\t\\multirow{2}{*}{BPE 10K \\cite{zeyer2018:asr-attention}} & None & 4.9 & 14.4 & 4.9 & 15.4 \\\\\n\t\t& LSTM & 3.5 & 11.5 & 3.8 & 12.8 \\\\ \\hline\n\t\t\\multirow{2}{*}{Hybrid system \\cite{han2017capio}}  & N-gram & 3.4 & 8.8 & 3.6 & 8.9 \\\\\n\t\t& LSTM & 3.1 & 8.3 & 3.5 & 8.6 \\\\ \\hline \n\t\\end{tabular}\n\t\\vspace{-4mm}\n\\end{table}\n\\begin{table}[h]\n\t\\centering\n\t\\vspace{-2mm}\n\t\\caption{\\it Examples where the phonemic system's 1-best \\textbf{wins} against the word-piece model's 1-best.}\n\t\\vspace{-4mm}\n\t\\label{plg_vs_wp}\n\t\\begin{tabular}{ |c|c|} \\hline\n\t\tPhoneme  & Word-Piece    \\\\ \\hline\n\t\twhen did you come \\textbf{bartley} & when did you come partly    \\\\\n\t\t\\textbf{kirkland} jumped for the jetty & kerklin jumped for the jetty \\\\ \n\t\tman's eyes \\textbf{remained} fixed & man's eyes were made fixed  \\\\ \\hline\n\t\\end{tabular}\n\t\t\\vspace{-2mm}\n\\end{table}\n\nIn Table  \\ref{960overview}, we also include the WERs from previous work on LibriSpeech 960hr; for fair comparison, systems which employ data augmentation \\cite{park2019specaugment} are excluded.\nOur word-piece model performs better than the previously reported sequence-to-sequence model\nin \\cite{zeyer2018:asr-attention} while the performance is behind the conventional hybrid system with\nan n-gram LM \\cite{han2017capio}. We note that our word-piece models simply trained using the cross-entropy criterion\n(without e.g. minimum word error rate training \\cite{PrabhavalkarSWN18}) is competitive with\nSabour et al.'s model trained with optimal completion distillation \\cite{sabour2018optimal}, which is reported to give 4.5\\% and 13.3\\% on the test-clean and test-other sets.\nFor further comparison, we also report the WERs of our best word-piece model combined with an LSTM language model \\cite{sundermeyer2012lstm} by shallow fusion \\cite{ChorowskiJ17, toshniwal2018comparison}\\footnote{In our experiments,\nwe find it crucial to constrain the emission of end-of-sentence (\\texttt{EOS}) tokens \\cite{ChorowskiJ17} to penalize short sentences\n(rather than applying length normalization) in shallow fusion:\nwe only allow the model to emit \\texttt{EOS} when its score is within 1.0 of the top hypothesis. We check that tuning such an \\texttt{EOS} emission\nconstraint does not improve the baseline systems without language model\nnor beam search with the WFST decoder for phonemic models.}.\nThe LSTM LM consists of one input linear layer of dimension 1024 and 2 LSTM layers with 2048 nodes \\cite{hochreiter1997long}.\nThe LM weight of 0.35 is found to be optimal for dev-clean and dev-other WERs.\nWe obtain similar relative improvements reported in \\cite{zeyer2018:asr-attention} and achieve WERs of 3.6\\%\non the test-clean, and 10.3\\% on the test-other set, which reduces the performance gap from the best hybrid system reported in \\cite{han2017capio}.\n\\subsection{Results on 100hr and 460hr tasks}\n\\vspace{-2mm}\nWe conduct the same experiments in the 100hr and 460hr conditions.\nFor each unit, we obtain the best performance for the \\textit{large} models\nfor the 460hr scenario, whereas for the 100hr case, the \\textit{medium} model perform\nthe best. The results are summarized in Table \\ref{460100overview}. We find that even\nin the small dataset scenarios with higher OOV rates, graphemic and word-piece based\nmodels outperform the phonemic system. We also note that the performance of attention-based\nmodels dramatically degrades when the amount of training data is reduced, unlike conventional\nhybrid approach \\cite{panayotov2015librispeech}.\n\\begin{table}[h]\n\t\\centering\n\t\\vspace{-3mm}\n\t\\caption{\\it WERs for the \\textbf{460hr} and \\textbf{100hr} scenarios.}\n\t\t\\vspace{-1mm}\n\t\t\t\t \t\\vspace{-2mm}\n\t\\label{460100overview}\n\t\\begin{tabular}{ |c|c|c|c|c|c|c|} \\hline\n\t\tTrain   & \\multirow{2}{*}{Unit} & \\multicolumn{2}{|c|}{dev} & \\multicolumn{2}{|c|}{test}    \\\\ \\cline{3-6}\n\t\tdata   &  & clean & other & clean & other    \\\\ \\hline\n\t\t\\multirow{3}{*}{460hr} & Phoneme&   7.6 &  27.3 & 8.5 & 27.8 \\\\\n\t\t& Grapheme&  6.4 & 23.5 & 6.8 &  24.1 \\\\ \n\t\t& Word-Piece  & \\textbf{5.7} & \\textbf{21.8} & \\textbf{6.5} & \\textbf{22.5}  \\\\ \\hline\n\t\t\\multirow{3}{*}{100hr} &  Phoneme &  13.8 & 38.9 & 14.3 & 40.9 \\\\\n\t\t& Grapheme  &  \\textbf{11.6} & 36.1 &  \\textbf{12.0} &  38.0 \\\\ \n\t\t& Word-Piece& 12.7 & \\textbf{33.9} &  12.9 &  \\textbf{35.5}  \\\\ \\hline\n\t\t\n\t\\end{tabular}\n\t\\vspace{-5mm}\n\\end{table}\n\\vspace{-3mm}\n\\section{Rescoring Experiments}\n\\label{sec:part2}\n\\vspace{-2mm}\nWhile Sec.~\\ref{sec:part1} focuses on the comparison of models with different output\nunits, our goal is to ideally get benefits from different model units.\nWe consider two methods for combining LAS models with different output units.\nThe first approach is simple \\textit{N-best list rescoring}.\nWe generate a N-best list from one LAS model, convert the corresponding word\nsequences to the rescorer LAS model's unit, score them, and combine the scores\nby log-linear interpolation to get new scores.\nHowever, rescoring is limited to the hypotheses generated by one LAS model.\nTherefore, we also carry out \\textit{union of N-best list with cross-rescoring}:\nwe independently generate N-best lists from two LAS models, rescore the\nhypotheses generated by one model using the other model and vice versa, to get the\n1-best from the union of the rescored (up to) 2N hypotheses.\n\\vspace{-3mm}\n\\subsection{N-best Rescoring results}\n\\vspace{-2mm}\nWe carry out the N-best rescoring of our best word-piece based model in the 960hr scenario by a graphemic, and\na phonemic model. In all following experiments, the interpolation weights are optimized to obtain the best\ndev-clean WER (which typically also gives the best dev-other WER).\nThe WERs are presented in the upper part of Table \\ref{tab:resc_wp}.\nWe obtain improvements of 9\\% in both cases on the test-clean set;\non the test-other set, we obtain an 8\\% relative with the phonemic model and a 9\\% relative with the graphemic model.\nThus, it can be noted that rescoring is a simple method for using\na phonemic model without an additional language model.\nTo determine if gains by the graphemic and phonemic models are additive, we\ncombine the scores from all models, which obtains only slight improvements of up\nto 0.1 absolute as shown in Table \\ref{tab:resc_wp} (+ Both).\nIn Table \\ref{wp_gr_vs_wp_g_p}, we again show some illustrative examples\nwhere the phonemic model outperforms the combination\nof word-piece and grapheme based models only.\nIt is for example interesting to observe that the correct spelling ``bartley\" is in the N-best hypotheses\nof the word-piece model, and that the phonemic model helps recognize it correctly.\n\nIn the other direction, we also rescore the N-best list generated by the phonemic system by the word-piece model.\nThe results are shown in the lower part of Table \\ref{tab:resc_wp}. We find that the improvements are limited (only\nup to 4\\% relative). In fact, the 30-best list generated by a phonemic system has much higher oracle WERs than the 8-best\nlist of the word-piece model.\n\\begin{table}[h]\n\t\\centering\n\\setlength{\\tabcolsep}{0.4em}\n\t\\caption{\\it WER (\\%) results for N-best list rescoring. Oracle WERs are\n\tshown in parentheses.}\n\t\\vspace{-4mm}\n\t\\label{tab:resc_wp}\n\t\\begin{tabular}{ |l|c|c|c|c|} \\hline\n\t\t\\multirow{2}{*}{ }  & \\multicolumn{2}{|c|}{dev} & \\multicolumn{2}{|c|}{test}    \\\\ \\cline{2-5}\n\t\t& clean & other & clean & other    \\\\ \\hline \\hline\n\t\tWord-Piece        & 4.4 (2.4) & 13.2 (9.2) & 4.7 (2.6) & 13.4 (9.1) \\\\ \n\t\t+ Phoneme         & 4.1  & 12.4 & \\textbf{4.3} &  12.4 \\\\\n\t\t+ Grapheme        & 4.0 &  12.3 &  \\textbf{4.3} &12.3 \\\\ \n\t\t+ Both         & \\textbf{3.9}  & \\textbf{12.2} & \\textbf{4.3} &  \\textbf{12.2} \\\\ \\hline \\hline\n\t\tPhoneme  & 5.6 (4.9) & 15.8 (14.4) & 6.2 (5.5) & 15.8 (14.7) \\\\ \n\t\t+ Word-Piece & 5.4 & 15.5 & 6.0 & 15.5 \\\\ \\hline\n\t\\end{tabular}\n\t\t\t\t\t \t\\vspace{-3mm}\n\\end{table}\n\\begin{table}[h]\n\t\\centering\n\t\\caption{\\it Examples where Word-Piece+Grapheme+Phoneme (WP+G+P) \\textbf{wins} over Word-Piece+Grapheme (WP+G).}\n\t\t\t\t \t\\vspace{-3mm}\n\t\\label{wp_gr_vs_wp_g_p}\n\\resizebox{\\columnwidth}{!}{\n\t\\begin{tabular}{ |c|c|} \\hline\n\t\tWP+G+\\textbf{P}  & WP+G    \\\\ \\hline\n\t\toh \\textbf{bartley} did you write to me & oh bartly did you write to me \\\\\n\t    ... lettuce leaf with \\textbf{mayonnaise} ... & ... lettuce leaf with mayonna is ...    \\\\\n       the manager \\textbf{fell to} his musings & the manager felt of his musings \\\\\n       what \\textbf{a fuss} is made about you & what are fusses made about you \\\\\n\t\t... eyes \\textbf{blazed with} indignation & ... eyes blaze of indignation  \\\\ \\hline\n\t\\end{tabular}\n\t  }\n\t\\vspace{-5mm}\n\\end{table}\n \\vspace{-3mm}\n\\subsection{Union of N-best lists with cross-rescoring results}\n\\vspace{-1mm}\nThe examples in Table \\ref{plg_vs_wp} show some complementarity between the word-piece 1-best hypothesis\nand the phonemic one.\nTo evaluate the potential value of hypotheses generated by the phonemic model, \nwe decode a N-best list from the word-piece based and phoneme based models independently,\nrescore the respective hypotheses (cross-rescoring), and take the 1-best from the 2N hypotheses (union).\nIn Table \\ref{tab:union}, we observe that we only obtain marginal improvements on the test-other set, compared with rescoring the 8-best\nword-piece hypotheses. For a fairer comparison, we also carry out rescoring of 16-best lists generated by\nthe word-piece model by the phonemic model. We find that such an approach is slightly better than the union.\nThis suggests that decoding from the phonemic model has limited benefits for the LibriSpeech task.\n\t\\vspace{-1mm}\n\\begin{table}[h]\n\t\\centering\n\t\\setlength{\\tabcolsep}{0.3em}\n\t\t\\vspace{-2mm}\n\t\\caption{\\it WERs (\\%) results for union of N-best lists with\n\t\tcross-rescoring. Oracle WERs are shown in parentheses.}\n\t\\vspace{-3mm}\n\t\\label{tab:union}\n\t\t\\begin{tabular}{ |l|c|c|c|c|c|} \\hline\n\t\t\t\\multirow{2}{*}{ } & Num.  & \\multicolumn{2}{|c|}{dev} & \\multicolumn{2}{|c|}{test}    \\\\ \\cline{3-6}\n\t\t\t& hyp.  & clean & other & clean & other    \\\\ \\hline\n\t\t\tWord-Piece      & \\multirow{2}{*}{8} & 4.4 (2.4) & 13.2 (9.2) & 4.7 (2.6) & 13.4 (9.1) \\\\\n\t\t\t+ Phoneme      &   & 4.1 & 12.4 & 4.3 & 12.4 \\\\ \\cline{2-6}\n\t\t\tUnion     & 16 & 4.1 & 12.4 & 4.3 & 12.3 \\\\ \\hline\n\t\t\tWord-Piece     & \\multirow{2}{*}{16} & 4.4 (2.0) & 13.2 (8.3) & 4.7 (2.2) & 13.4 (8.1)   \\\\ \n\t\t\t+ Phoneme      &  & \\textbf{4.0} & \\textbf{12.3} & \\textbf{4.3} &  \\textbf{12.2}   \\\\ \\hline\n\t\t\t\n\t\t\t\n\t\t\\end{tabular}\n\\end{table}\n \\vspace{-4mm}\n\\subsection{Why is Oracle WER So High for Phonemic System?}\n      \\vspace{-1mm}\nThe oracle WERs are much worse for the phonemic system than the word-piece model\n(Table \\ref{tab:resc_wp}).\nWe observe that the diversity of hypotheses in the N-best list generated by the phonemic\nsystem is mainly based on homophones, rather than \\textit{difficult} words (i.e. words with unusual\npronunciation).\nFor example, on the reference utterance \\textit{``bozzle had always waited upon\n\thim with a decent coat and a well brushed hat and clean shoes\"}, where\n\\textit{bozzle} is not in the training data, the word-piece based model fills the 8-best beam\nby proposing different spellings for \\textit{bozzle} such as \\{basil, bazil,\nbasle, bosel, bosal, bosell, bossel\\}, which is a reasonable way to model the ambiguity.\nThe phoneme system, instead, only produces \\{bazil, basil\\} as a substitution\nfor \\textit{bozzle} and lists homophones for \\textit{shoes}, \\{shoes, shews,\nshoos, shues, shooes\\} instead.\nHomophone distinction might still be inefficient for a phonemic system as the\nphonemic LAS model gives them all the same score, and a single parameter is used\nto weight the external LM for the entire search. \nAddressing this issue might be crucial to improve the phonemic system.\n\\subsection{Rescoring with an auxiliary decoder}\n      \\vspace{-1mm}\n\\begin{figure}[t]\n  \\centerline{\\includegraphics[width=0.5\\columnwidth]{aux_decoder.pdf}}\n        \\vspace{-1mm}\n\t\\caption{\\it LAS model with an auxiliary decoder: main\n\tdecoder operates on graphemes and the auxiliary decoder predicts\n\tphonemes; dashed lines represent state copying for initialization at\n\teach word boundary.}\n\t\\label{fig:las_aux}\n\t\\vspace{-6mm}\n\\end{figure}\n\\label{sec:twodecmodel}\nFinally, we examine a model with two decoders operating on different units but using a single encoder.\nSuch a model can be convenient for model combination (e.g., rescoring or potentially also decoding from two decoders operating on different units and combining hypotheses in a word synchronous fashion).\nThe design of the model is illustrated in Figure~\\ref{fig:las_aux}.\nThe main decoder (grapheme, in the example) works exactly as in the baseline LAS model (Sec.~\\ref{sec:model}; Figure~\\ref{fig:las_base}).\nThe auxiliary decoder (phoneme, in the example) is designed such that it\npredicts \\emph{only the next word as a sequence of the auxiliary units}.\nWe use separate parameters for the auxiliary attention and initialize all recurrent states\nof the auxiliary component at each word boundary by those of the main decoder (i.e. the prediction\nfrom the auxiliary decoder is conditioned on the word sequence generated thus far from the main decoder).\nThe model is trained in two stages;\nthe main decoder and the encoder are first trained, and their parameters are not modified during the training of\nthe auxiliary components.\nIn experiments, we use word-pieces for the main decoder, and phonemes for the\nauxiliary decoder.\nTable \\ref{tab:aux} shows improvements by rescoring with an auxiliary phoneme decoder of the two decoder-model.\nWe obtain improvements despite small number of additional parameters (30M) corresponding to the phonemic 2-layer LSTM decoder and the attention layer, however rescoring with an independent phoneme model (as in Table \\ref{tab:resc_wp}) gives larger improvements.\n\\vspace{-3mm}\n\\begin{table}[h]\n\t\\centering\n\t\\setlength{\\tabcolsep}{0.3em}\n\t\\caption{\\it WERs (\\%) for rescoring with an auxiliary decoder.}\n\t\\vspace{-3mm}\n\t\\label{tab:aux}\n\t\\begin{tabular}{ |l|c|c|c|c|l|} \\hline\n\t\t\\multirow{2}{*}{ }  & \\multicolumn{2}{|c|}{dev} & \\multicolumn{2}{|c|}{test}  & Total  \\\\ \\cline{2-5}\n\t\t& clean & other & clean & other & Param.   \\\\ \\hline\n\t\tWord-Piece (WP)           & 4.4 & 13.2 & 4.7 & 13.4  &180~M  \\\\\n\t\tWP + Auxiliary phoneme    &  4.3 & 13.0 &  4.6 &  13.1 &  210~M\\\\\n\t\tWP + Phoneme              &  4.1  & 12.4 &  4.3 &  12.4 & 310~M\\\\ \\hline\n\t\t\n\t\\end{tabular}\n\\end{table}\n\\vspace{-6mm}\n\\section{Conclusion}\nOur experiments on different LibriSpeech subsets show that word-piece and grapheme based models\nconsistently outperform phoneme based models.\nTherefore, the dominance of character-based model units in the LAS speech model\nis not due to the amount of training data. This indicates that this behavior is more likely\nrelated to the model itself (e.g., the decoder is conditioned on all predecessor labels).\nFurthermore, we find that the word-piece based attention models can achieve a relatively low\noracle WER with only 8-best hypotheses and rescoring that N-best hypotheses using\ngraphemic or phonemic models gives good improvements.\nFuture work will examine whether streaming end-to-end approaches\n(e.g., RNN-T \\cite{graves2012sequence,RaoSP17}) show similar trends.\n\\label{sec:foot}\n\\clearpage\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{SpecAugment: A Simple Data Augmentation Method\\\\\nfor Automatic Speech Recognition}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\nWe present SpecAugment, a simple data augmentation method for speech recognition. SpecAugment is applied directly to the feature inputs of a neural network (i.e., filter bank coefficients). The augmentation policy consists of warping the features, masking blocks of frequency channels, and masking blocks of time steps. We apply SpecAugment on Listen, Attend and Spell networks for end-to-end speech recognition tasks. We achieve state-of-the-art performance on the LibriSpeech 960h and Swichboard 300h tasks, outperforming all prior work. On LibriSpeech, we achieve 6.8\\% WER on test-other without the use of a language model, and 5.8\\% WER with shallow fusion with a language model. This compares to the previous state-of-the-art hybrid system of 7.5\\% WER. For Switchboard, we achieve 7.2\\%/14.6\\% on the Switchboard/CallHome portion of the Hub5'00 test set without the use of a language model, and 6.8\\%/14.1\\% with shallow fusion, which compares to the previous state-of-the-art hybrid system at 8.3\\%/17.3\\% WER.\n\\end{abstract}\n\\noindent\\textbf{Index Terms}: end-to-end speech recognition, data augmentation\n\n\\section{Introduction}\n\nDeep Learning has been applied successfully to Automatic Speech Recognition (ASR)~\\cite{hinton2012deep}, where the main focus of research has been  designing better  network architectures, for example, DNNs \\cite{dahl-ieeetasl-2012}, CNNs \\cite{sainath-icassp-2013}, RNNs \\cite{graves-icassp-2013} and end-to-end models \\cite{graves-icml-2014,Chan2016ListenAA,bahdanau-icassp-2016}. However, these  models tend to overfit easily and require large amounts of training data \\cite{chiu-icassp-2018}.\n\nData augmentation has been proposed as a method to generate additional training data for ASR. For example, in \\cite{kanda-asru-2013,ragni-interspeech-2014}, artificial data was augmented for low resource speech recognition tasks. Vocal Tract Length Normalization has been adapted for data augmentation in \\cite{jaitly-2013-icml}. Noisy audio has been synthesised via superimposing clean audio with a noisy audio signal in \\cite{hannun-arxiv-2014}. Speed perturbation has been applied on raw audio for LVSCR tasks in \\cite{ko-interspeech-2015}. The use of an acoustic room simulator has been explored in \\cite{kim-interspeech-2017}. Data augmentation for keyword spotting has been studied in \\cite{prabhavalkar-2015-icassp, raju-arxiv-2018}. Feature drop-outs have been employed for training multi-stream ASR systems \\cite{mallidi-icassp-2016}. More generally, learned augmentation techniques have explored different sequences of augmentation transformations that have achieved state-of-the-art performance in the image domain \\cite{Cubuk2018AutoAugmentLA}.\n\nInspired by the recent success of augmentation in the speech and vision domains, we propose SpecAugment, an augmentation method that operates on the log mel spectrogram of the input audio, rather than the raw audio itself. This method is simple and computationally cheap to apply, as it directly acts on the log mel spectrogram as if it were an image, and does not require any additional data. We are thus able to apply SpecAugment online during training. SpecAugment consists of three kinds of deformations of the log mel spectrogram. The first is time warping, a deformation of the time-series in the time direction. The other two augmentations, inspired by ``Cutout\", proposed in computer vision \\cite{devries-arxiv-2017}, are time and frequency masking, where we mask a block of consecutive time steps or mel frequency channels.\n\nThis approach while rudimentary, is remarkably effective and allows us to train end-to-end ASR networks, called Listen Attend and Spell (LAS) \\cite{Chan2016ListenAA}, to surpass more complicated hybrid systems, and achieve state-of-the-art results even without the use of Language Models (LMs). \nOn LibriSpeech \\cite{Panayotov2015LibriSpeechAA}, we achieve 2.8\\% Word Error Rate (WER) on the test-clean set and 6.8\\% WER on the test-other set, without the use of an LM. Upon shallow fusion \\cite{gulcehre-2015-arxiv} with an LM trained on the LibriSpeech LM corpus, we are able to better our performance (2.5\\% WER on test-clean and 5.8\\% WER on test-other), improving the current state of the art on test-other by 22\\% relatively. On Switchboard 300h (LDC97S62) \\cite{switchboard}, we obtain 7.2\\% WER on the Switchboard portion of the Hub5'00 (LDC2002S09, LDC2003T02) test set, and 14.6\\% on the CallHome portion, without using an LM. Upon shallow fusion with an LM trained on the combined transcript of the Switchboard and Fisher (LDC200\\{4,5\\}T19) \\cite{fisher} corpora, we obtain 6.8\\%/14.1\\% WER on the Switchboard/Callhome portion.\n\n\\section{Augmentation Policy}\n\nWe aim to construct an augmentation policy that acts on the log mel spectrogram directly, which helps the network learn useful features. Motivated by the goal that these features should be robust to deformations in the time direction, partial loss of frequency information and partial loss of small segments of speech, we have chosen the following deformations to make up a policy:\n\\begin{enumerate}\n\\item Time warping is applied via the function {\\tt sparse\\_image\\_warp} of {\\tt tensorflow}. Given a log mel spectrogram with $\\tau$ time steps, we view it as an image where the time axis is horizontal and the frequency axis is vertical. A random point along the horizontal line passing through the center of the image within the time steps $(W, \\tau-W)$ is to be warped either to the left or right by a distance $w$ chosen from a uniform distribution from 0 to the time warp parameter $W$ along that line.  We fix six anchor points on the boundary---the four corners and the mid-points of the vertical edges.\n\\item Frequency masking is applied so that $f$ consecutive mel frequency channels $[f_0, f_0 + f)$ are masked, where $f$ is first chosen from a uniform distribution from 0 to the frequency mask parameter $F$, and $f_0$ is chosen from $[0, \\nu -f)$. $\\nu$ is the number of mel frequency channels.\n\\item Time masking is applied so that $t$ consecutive time steps $[t_0, t_0 + t)$ are masked, where $t$ is first chosen from a uniform distribution from 0 to the time mask parameter $T$, and $t_0$ is chosen from $[0, \\tau-t)$.\n \\begin{itemize}\n \\item We introduce an upper bound on the time mask so that a time mask cannot be wider than $p$ times the number of time steps.\n \\end{itemize}\n\\end{enumerate}\nFigure \\ref{fig:augs} shows examples of the individual augmentations applied to a single input. The log mel spectrograms are normalized to have zero mean value, and thus setting the masked value to zero is equivalent to setting it to the mean value.\n\n\\begin{figure}[t]\n  \\centering\n  \\begin{tabular}{c}\n  \\includegraphics[height=1.0cm]{figures/base.png} \\\\\n  \\includegraphics[height=1.0cm]{figures/twarp.png} \\\\\n  \\includegraphics[height=1.0cm]{figures/fblock.png} \\\\\n  \\includegraphics[height=1.0cm]{figures/tblock.png}\n  \\end{tabular}\n  \\vskip -0.1in\n  \\caption{Augmentations applied to the base input, given at the top. From top to bottom, the figures depict the log mel spectrogram of the base input with no augmentation, time warp, frequency masking and time masking applied.}\n  \\label{fig:augs}\n\\end{figure}\n\nWe can consider policies where multiple frequency and time masks are applied. The multiple masks may overlap. In this work, we mainly consider a series of hand-crafted policies, LibriSpeech basic (LB), LibriSpeech double (LD), Switchboard mild (SM) and Switchboard strong (SS) whose parameters are summarized in Table \\ref{t:policies}. In Figure \\ref{fig:policies}, we show an example of a log mel spectrogram augmented with policies LB and LD.\n\n\\begin{table}[h]\n  \\caption{Augmentation parameters for policies. $m_F$ and $m_T$ denote the number of frequency and time masks applied.}\n  \\label{t:policies}\n  \\centering\n  \\footnotesize\n  \\begin{tabular}{ccccccc}\n    \\toprule\n    Policy & $W$ & $F$ & $m_F$ & $T$ & $p$ & $m_T$ \\\\\n    \\midrule\n    None & 0 & 0 & - & 0 & - & - \\\\\n    LB & 80 & 27 & 1 & 100 & 1.0 & 1\\\\\n    LD & 80 & 27 & 2 & 100 & 1.0 & 2 \\\\\n    SM & 40 & 15 & 2 & 70 & 0.2 & 2\\\\\n    SS & 40 & 27 & 2 & 70 & 0.2 & 2\\\\\n    \\bottomrule\n  \\end{tabular}\n  \\vskip -0.1in\n\\end{table}\n\n\\begin{figure}[t]\n  \\centering\n  \\begin{tabular}{c}\n  \\includegraphics[height=1.0cm]{figures/n0.png} \\\\\n  \\includegraphics[height=1.0cm]{figures/n1.png} \\\\\n  \\includegraphics[height=1.0cm]{figures/n2.png}\n  \\end{tabular}\n  \\vskip -0.05in\n  \\caption{Augmentation policies applied to the base input. From top to bottom, the figures depict the log mel spectrogram of the base input with policies None, LB and LD applied.}\n  \\label{fig:policies}\n  \\vskip -0.15in\n\\end{figure}\n\n\\section{Model}\n\nWe use Listen, Attend and Spell (LAS) networks \\cite{Chan2016ListenAA} for our ASR tasks. These models, being end-to-end, are simple to train and have the added benefit of having well-documented benchmarks \\cite{zeyer-interspeech-2018, irie-arxiv-2019} that we are able to build upon to get our results. In this section, we review LAS networks and introduce some notation to parameterize them. We also introduce the learning rate schedules we use to train the networks, as they turn out to be an important factor in determining performance. We end with reviewing shallow fusion \\cite{gulcehre-2015-arxiv}, which we have used to incorporate language models for further gains in performance.\n\n\\subsection{LAS Network Architectures}\n\nWe use Listen, Attend and Spell (LAS) networks \\cite{Chan2016ListenAA} for end-to-end ASR studied in \\cite{irie-arxiv-2019}, for which we use the notation LAS-$d$-$w$. The input log mel spectrogram is passed in to a 2-layer Convolutional Neural Network (CNN) with max-pooling and stride of $2$. The output of the CNN is passes through an encoder consisting of $d$ stacked bi-directional LSTMs with cell size $w$ to yield a series of attention vectors. The attention vectors are fed into a 2-layer RNN decoder of cell dimension $w$, which yields the tokens for the transcript. The text is tokenized using a Word Piece Model (WPM) \\cite{schuster-icassp-2012} of vocabulary size 16k for LibriSpeech and 1k for Switchboard. The WPM for LibriSpeech 960h is constructed using the training set transcripts. For the Switchboard 300h task, transcripts from the training set are combined with those of the Fisher corpus to construct the WPM. The final transcripts are obtained by a beam search with beam size 8. For comparison with \\cite{irie-arxiv-2019}, we note that their ``large model\" in our notation is LAS-4-1024.\n\n\\subsection{Learning Rate Schedules}\n\nThe learning rate schedule turns out to be an important factor in determining the performance of ASR networks, especially so when augmentation is present. Here, we introduce training schedules that serve two purposes. First, we use these schedules to verify that a longer schedule improves the final performance of the network, even more so with augmentation (Table \\ref{t:LibriSpeech}). Second, based on this, we introduce very long schedules that are used to maximize the performance of the networks.\n\nWe use a learning rate schedule in which we ramp-up, hold, then exponentially decay the learning rate until it reaches $\\sfrac{1}{100}$ of its maximum value. The learning rate is kept constant beyond this point. This schedule is parameterized by three time stamps $(s_r, s_i, s_f)$ -- the step $s_r$ where the ramp-up (from zero learning rate) is complete, the step $s_i$ where exponential decay starts, and the step $s_f$ where the exponential decay stops.\n\nThere are two more factors that introduce time scales in our experiment. First, we turn on a variational weight noise \\cite{graves-nips-2011} of standard deviation 0.075 at step $s_\\text{noise}$ and keep it constant throughout training. Weight noise is introduced in the step interval $(s_r, s_i)$, i.e., during the high plateau of the learning rate.\n\nSecond, we introduce uniform label smoothing \\cite{lbsm} with uncertainty 0.1, i.e., the correct class label is assigned the confidence 0.9, while the confidence of the other labels are increased accordingly. As is commented on again later on, label smoothing can destabilize training for smaller learning rates, and we sometimes choose to turn it on only at the beginning of training and off when the learning rate starts to decay.\n\nThe two basic schedules we use, are given as the following:\n\\begin{enumerate}\n\\item B(asic): $(s_r, s_\\text{noise}, s_i, s_f)$ = (0.5k, 10k, 20k, 80k)\n\\item D(ouble): $(s_r, s_\\text{noise}, s_i, s_f)$ = (1k, 20k, 40k, 160k)\n\\end{enumerate}\nAs discussed further in section \\ref{s:discussion}, we can improve the performance of the trained network by using a longer schedule. We thus introduce the following schedule:\n\\begin{enumerate}\n  \\setcounter{enumi}{2}\n  \\item L(ong): $(s_r, s_\\text{noise}, s_i, s_f)$ = (1k, 20k, 140k, 320k)\n\\end{enumerate}\nwhich we use to train the largest model to improve performance. When using schedule L, label smoothing with uncertainty $0.1$ is introduced for time steps $< s_i = \\text{140k}$ for LibriSpeech 960h, and is subsequently turned off. For Switchboard 300h, label smoothing is turned on throughout training.\n\n\\subsection{Shallow Fusion with Language Models}\n\nWhile we are able to get state-of-the-art results with augmentation, we can get further improvements by using a language model. We thus incorporate an RNN language model by shallow fusion for both tasks. In shallow fusion, the ``next token\" $\\mathbf{y}^*$ in the decoding process is determined by\n\\be\n\\mathbf{y}^* = \\underset{\\mathbf{y}}{\\text{argmax}} \\left( \\log P(\\mathbf{y} | \\mathbf{x}) + \\lambda \\log P_{LM} (\\mathbf{y})\\right) \\,,\n\\ee\ni.e., by jointly scoring the token using the base ASR model and the language model. We also use a coverage penalty $c$ \\cite{chorowski-2017-interspeech}.\n\nFor LibriSpeech, we use a two-layer RNN with embedding dimension 1024 used in \\cite{irie-arxiv-2019} for the LM, which is trained on the LibriSpeech LM corpus. We use identical fusion parameters ($\\lambda =0.35$ and $c=0.05$) used in \\cite{irie-arxiv-2019} throughout.\n\nFor Switchboard, we use a two-layer RNN with embedding dimension 256, which is trained on the combined transcripts of the Fisher and Switchboard datasets. We find the fusion parameters via grid search by measuring performance on RT-03 (LDC2007S10). We discuss the fusion parameters used in individual experiments in section \\ref{ss:swbd}.\n\n\\section{Experiments}\n\nIn this section, we describe our experiments on LibriSpeech and Switchboard with SpecAugment. We report state-of-the-art results that out-perform heavily engineered hybrid systems.\n\n\\subsection{LibriSpeech 960h} \\label{ss:librispeech}\n\nFor LibriSpeech, we use the  same setup as \\cite{irie-arxiv-2019}, where we use 80-dimensional filter banks with delta and delta-delta acceleration, and a 16k word piece model \\cite{schuster-icassp-2012}.\n\nThe three networks LAS-4-1024, LAS-6-1024 and LAS-6-1280 are trained on LibriSpeech 960h with a combination of augmentation policies (None, LB, LD) and training schedules (B/D). Label smoothing was not applied in these experiments. The experiments were run with peak learning rate of $0.001$ and batch size of 512, on 32 Google Cloud TPU chips for 7 days. Other than the augmentation policies and learning rate schedules, all other hyperparameters were fixed, and no additional tuning was applied. We report test set numbers validated by the dev-other set in Table \\ref{t:LibriSpeech}. We see that augmentation consistently improves the performance of the trained network, and that the benefit of a larger network and a longer learning rate schedule is more apparent with harsher augmentation.\n\n\\begin{table}[h!]\n  \\caption{LibriSpeech test WER (\\%) evaluated for varying networks, schedules and policies. First row from \\cite{irie-arxiv-2019}.}\n  \\footnotesize\n  \\label{t:LibriSpeech}\n  \\centering\n  \\resizebox{\\columnwidth}{!}{%\n  \\begin{tabular}{cccrrrr}\n    \\toprule\n    \\multirow{2}{*}{\\bfseries Network} & \\multirow{2}{*}{\\bfseries Sch} & \\multirow{2}{*}{\\bfseries Pol}\n    & \\multicolumn{2}{c}{\\bfseries No LM}  & \\multicolumn{2}{c}{\\bfseries With LM} \\\\\n    \\cmidrule(r){4-5} \\cmidrule(r){6-7}\n    & & & {\\bfseries clean} & {\\bfseries other} & {\\bfseries clean} & {\\bfseries other} \\\\\n    \\midrule\n    {LAS-4-1024} \\cite{irie-arxiv-2019}  & B & - & 4.7 & 13.4 & 3.6 & 10.3 \\\\\n    \\midrule\n    \\multirow{5}{*}{LAS-4-1024} \n    & B & LB & 3.7 & 10.0 & 3.4 & 8.3 \\\\\n    & B & LD & 3.6 & 9.2 & 2.8 & 7.5 \\\\\n    \\cmidrule(r){2-7}\n    & D & - &  4.4 &  13.3 & 3.5 & 10.4 \\\\\n    & D & LB & 3.4 & 9.2 & 2.7 & 7.3 \\\\\n    & D & LD & 3.4 & 8.3 & 2.8 & 6.8  \\\\\n    \\midrule\n    \\multirow{3}{*}{LAS-6-1024} & D & - & 4.5 & 13.1 & 3.6 & 10.3 \\\\\n    & D & LB & 3.4 & 8.6 & 2.6 & 6.7  \\\\\n    & D & LD & 3.2 & 8.0 & 2.6 & 6.5 \\\\\n    \\midrule\n    \\multirow{3}{*}{LAS-6-1280} & D & - & 4.3 & 12.9 & 3.5 & 10.5  \\\\\n    & D & LB & 3.4 & 8.7 & 2.8 & 7.1  \\\\\n    & D & LD & 3.2 & 7.7 & 2.7 & 6.5  \\\\\n    \\bottomrule\n  \\end{tabular}\n  }\n\\end{table}\n\nWe take the largest network, LAS-6-1280, and use schedule L (with training time $\\sim$ 24 days) and policy LD to train the network to maximize performance. We turn label smoothing on for time steps $< 140$k as noted before. The test set performance is reported by evaluating the checkpoint with best dev-other performance. State of the art performance is achieved by the LAS-6-1280 model, even without a language model. We can incorporate an LM using shallow fusion to further improve performance. The results are presented in Table \\ref{t:libriSOTA}.\n\n\\begin{table}[h!]\n  \\caption{LibriSpeech 960h WERs (\\%).}\n  \\label{t:libriSOTA}\n  \\centering\n  \\small\n  \\resizebox{\\columnwidth}{!}{%\n  \\begin{tabular}{lcccc}\n    \\toprule\n    \\bfseries Method & \\multicolumn{2}{c}{\\bfseries No LM} & \\multicolumn{2}{c}{\\bfseries With LM} \\\\\n    \\cmidrule(r){2-3} \\cmidrule(r){4-5}\n     & \\bfseries clean & \\bfseries other & \\bfseries clean & \\bfseries other \\\\\n    \\midrule\n    \\bfseries HMM \\\\\n    \\quad Panayotov et al., (2015) \\cite{Panayotov2015LibriSpeechAA} & & & 5.51 & 13.97 \\\\\n    \\quad Povey et al., (2016) \\cite{povey-interspeech-2016} & & & 4.28 \\\\\n    \\quad Han et al., (2017) \\cite{han-arxiv-2017} & & & 3.51 &  8.58 \\\\\n    \\quad Yang et al. (2018) \\cite{yang-arxiv-2018} & & & 2.97 & 7.50 \\\\\n    \\midrule\n    \\bfseries CTC/ASG \\\\\n    \\quad Collobert et al., (2016) \\cite{collobert-arxiv-2016} & 7.2 \\\\\n    \\quad Liptchinsky et al., (2017) \\cite{liptchinsky-arxiv-2017} & 6.7 & 20.8 & 4.8 & 14.5 \\\\\n    \\quad Zhou et al., (2018) \\cite{zhou-icassp-2018} & & & 5.42 & 14.70 \\\\\n    \\quad Zeghidour et al., (2018) \\cite{zeghidour-arxiv-2018} & & & 3.44 & 11.24 \\\\\n    \\quad Li et al., (2019) \\cite{li-arxiv-2019} & 3.86 & 11.95 & 2.95 & 8.79 \\\\\n    \\midrule\n    \\bfseries LAS \\\\\n    \\quad Zeyer et al., (2018) \\cite{zeyer-interspeech-2018} & 4.87 & 15.39 & 3.82 & 12.76 \\\\\n    \\quad Zeyer et al., (2018) \\cite{zeyer-nips-2018} & 4.70 & 15.20 \\\\\n    \\quad Irie et al., (2019) \\cite{irie-arxiv-2019} & 4.7 & 13.4 & 3.6 & 10.3 \\\\\n    \\quad Sabour et al., (2019) \\cite{sabour-iclr-2019} & 4.5 & 13.3 \\\\\n    \\midrule\n    \\bfseries Our Work \\\\\n    \\quad LAS & 4.1 & 12.5 & 3.2 & 9.8 \\\\\n    \\quad LAS + SpecAugment & \\bfseries 2.8 & \\bfseries 6.8 & \\bfseries 2.5 & \\bfseries 5.8 \\\\\n    \\bottomrule\n  \\end{tabular}\n  }\n  \\vskip -0.1in\n\\end{table}\n\n\\subsection{Switchboard 300h} \\label{ss:swbd}\n\nFor Switchboard 300h, we use the Kaldi \\cite{povey-asru-2011} ``s5c'' recipe to process our data, but we adapt the recipe to use 80-dimensional filter banks with delta and delta-delta acceleration. We use a 1k WPM \\cite{schuster-icassp-2012} to tokenize the output, constructed using the combined vocabulary of the Switchboard and Fisher transcripts.\n\nWe train LAS-4-1024 with policies (None, SM, SS) and schedule B. As before, we set the peak learning rate to $0.001$ and total batch size to 512, and train using 32 Google Cloud TPU chips. Here the experiments are run with and without label smoothing. Not having a canonical development set, we choose to evaluate the checkpoint at the end point of the training schedule, which we choose to be 100k steps for schedule B. We note that the training curve relaxes after the decay schedule is completed (step $s_f$), and the performance of the network does not vary much. The performance of various augmentation policies with and without label smoothing for Switchboard 300h is shown in Table \\ref{t:SWBD}. We see that label smoothing and augmentation have an additive effect for this corpus.\n\n\\begin{table}[h!]\n  \\vskip -0.1in\n  \\caption{Switchboard 300h WER (\\%) evaluated for LAS-4-1024 trained with schedule B with varying augmentation and Label Smoothing (LS) policies. No LMs have been used.}\n  \\footnotesize\n  \\label{t:SWBD}\n  \\centering\n  \\begin{tabular}{ccrr}\n    \\toprule\n    {\\bfseries Policy} & {\\bfseries LS} & {\\bfseries SWBD} & {\\bfseries CH} \\\\\n    \\midrule\n    \\multirow{2}{*}{-} & $\\times$ & 12.1 & 22.6 \\\\\n    & $\\circ$ & 11.2 & 21.6 \\\\\n    \\midrule\n    \\multirow{2}{*}{SM}& $\\times$ & 9.5 & 18.8 \\\\\n    & $\\circ$ & 8.5 & 16.1 \\\\\n    \\midrule\n    \\multirow{2}{*}{SS} & $\\times$ & 9.7 & 18.2 \\\\\n    & $\\circ$ & 8.6 & 16.3 \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\nAs with LibriSpeech 960h, we train LAS-6-1280 on the Switchboard 300h training set with schedule L (training time $\\sim$ 24 days) to get state of the art performance. In this case, we find that turning label smoothing on throughout training benefits the final performance. We report the performance at the end of training time at 340k steps. We present our results in the context of other work in Table \\ref{t:swbdSOTA}.  We also apply shallow fusion with an LM trained on Fisher-Switchboard, whose fusion parameters are obtained by evaluating performance on the RT-03 corpus. Unlike the case for LibriSpeech, the fusion parameters do not transfer well between networks trained differently---the three entries in Table \\ref{t:swbdSOTA} were obtained by using fusion parameters $(\\lambda, c) = $ (0.3, 0.05), (0.2, 0.0125) and (0.1, 0.025) respectively.\n\n\\begin{table}[h!]\n  \\caption{Switchboard 300h WERs (\\%).}\n  \\label{t:swbdSOTA}\n  \\centering\n  \\small\n  \\resizebox{\\columnwidth}{!}{%\n  \\begin{tabular}{lcccc}\n    \\toprule\n    \\bfseries Method & \\multicolumn{2}{c}{\\bfseries No LM} & \\multicolumn{2}{c}{\\bfseries With LM} \\\\\n    \\cmidrule(r){2-3} \\cmidrule(r){4-5}\n    & \\bfseries SWBD & \\bfseries CH & \\bfseries SWBD & \\bfseries CH \\\\\n    \\midrule\n    \\bfseries HMM \\\\\n    \\quad Vesel{\\'y} et al., (2013) \\cite{vesely-interspeech-2013} & & & 12.9 & 24.5 \\\\\n    \\quad Povey et al., (2016) \\cite{povey-interspeech-2016} & & & 9.6 & 19.3 \\\\\n    \\quad Hadian et al., (2018) \\cite {hadian-interspeech-2018} & & & 9.3 & 18.9 \\\\\n    \\quad Zeyer et al., (2018) \\cite{zeyer-interspeech-2018} & & & 8.3 & 17.3 \\\\\n    \\midrule\n    \\bfseries CTC \\\\\n    \\quad Zweig et al., (2017) \\cite{zweig-icassp-2017} & 24.7 & 37.1 & 14.0 & 25.3 \\\\\n    \\quad Audhkhasi et al., (2018) \\cite{audhkhasi-interspeech-2019} & 20.8 & 30.4 \\\\\n    \\quad Audhkhasi et al., (2018) \\cite{audhkhasi-icassp-2018} & 14.6 & 23.6 \\\\\n    \\midrule\n    \\bfseries LAS \\\\\n    \\quad Lu et al., (2016) \\cite{lu-icassp-2016} & 26.8 & 48.2 & 25.8 & 46.0 \\\\\n    \\quad Toshniwal et al., (2017) \\cite{toshniwal-interspeech-2017} & 23.1 & 40.8 \\\\\n    \\quad Zeyer et al., (2018) \\cite{zeyer-interspeech-2018} & 13.1 & 26.1 & 11.8 & 25.7 \\\\\n    \\quad Weng et al., (2018) \\cite{weng-interspeech-2018} & 12.2 & 23.3 \\\\\n    \\quad Zeyer et al., (2018) \\cite{zeyer-nips-2018} & 11.9 & 23.7 & 11.0 & 23.1 \\\\\n    \\midrule\n    \\bfseries Our Work \\\\\n    \\quad LAS & 11.2 & 21.6 & 10.9 & 19.4 \\\\\n    \\quad LAS + SpecAugment (SM) & \\textbf{7.2} & 14.6 & \\textbf{6.8} & 14.1 \\\\\n    \\quad LAS + SpecAugment (SS) & 7.3 & \\textbf{14.4} & 7.1 & \\textbf{14.0} \\\\\n    \\bottomrule\n  \\end{tabular}\n  }\n  \\vskip -0.1in\n\\end{table}\n\n\\section{Discussion}\\label{s:discussion}\n\n\\noindent\\textbf{Time warping contributes, but is not a major factor in improving performance.} In Table \\ref{tab:abalation}, we present three training results for which time warping, time masking and frequency masking have been turned off, respectively. We see that the effect time warping, while small, is still existent. Time warping, being the most expensive as well as the least influential of the augmentations discussed in this work, should be the first augmentation to be dropped given any budgetary limitations.\n\\smallskip\n\n\\begin{table}[th]\n  \\vskip -0.1in\n  \\caption{Test set WER (\\%) evaluated without LM for network LAS-4-1024 trained with schedule B.}\n  \\label{tab:abalation}\n  \\centering\n  \\footnotesize\n  \\begin{tabular}{ccccccrr}\n    \\toprule\n    $W$ & $F$ & $m_F$ & $T$ & $p$ & $m_T$ & test-other & test \\\\\n    \\midrule\n    80 & 27 & 1 & 100 & 1.0 & 1 & 10.0 & 3.7 \\\\\n    \\midrule\n    0  & 27 & 1 & 100 & 1.0 & 1 & 10.1 & 3.8 \\\\\n    80 &  0 & - & 100 & 1.0 & 1 & 11.0 & 4.0 \\\\\n    80 & 27 & 1 & 0 & - & - & 10.9 & 4.1 \\\\\n    \\bottomrule\n  \\end{tabular}\n  \\vskip -0.1in\n\\end{table}\n\n\\noindent\\textbf{Label smoothing introduces instability to training.} We have noticed that the proportion of unstable training runs increases for LibriSpeech when label smoothing is applied with augmentation. This becomes more conspicuous while learning rate is being decayed, thus our introduction of a label smoothing schedule for training LibriSpeech, where labels are only smoothed in the initial phases of the learning rate schedule.\n\\smallskip\n\n\\noindent\\textbf{Augmentation converts an over-fitting problem into an under-fitting problem.} As can be observed from the training curves of the networks in Figure \\ref{f:training}, the networks during training not only under-fit the loss and WER on the augmented training set, but also on the training set itself when trained on augmented data. This is in stark contrast to the usual situation where networks tend to over-fit to the training data. This is the major benefit of training with augmentation, as explained below.\n\\smallskip\n\n\\begin{figure}[h]\n  \\centering\n  \\begin{tabular}{cc}\n  \\includegraphics[height=3.3cm]{figures/tcurve_tset_1.png} &\n  \\includegraphics[height=3.3cm]{figures/tcurve_dother_2.png}\n  \\end{tabular}\n  \\vskip - 0.05in\n  \\caption{LAS-6-1280 on LibriSpeech with schedule D.}\n  \\label{f:training}\n  \\vskip -0.1in\n\\end{figure}\n\n\\noindent\\textbf{Common methods of addressing under-fitting yield improvements.} We were able to make significant gains in performance by standard approaches to alleviate under-fitting---making larger networks and training longer. The current reported performance was obtained by the recursive process of applying a harsh augmentation policy, and then making wider, deeper networks and training them with longer schedules to address the under-fitting.\n\\smallskip\n\n\\noindent\\textbf{Remark on related works.} We note that an augmentation similar to frequency masking has been studied in the context of CNN acoustic models in \\cite{kovacs2017increasing}. There, blocks of adjacent frequencies are pre-grouped into bins, which are randomly zeroed-out per minibatch. On the other hand, both the size and position of the frequency masks in SpecAugment are chosen stochastically, and differ for every input in the minibatch. More ideas for structurally omitting frequency data of spectrograms have been discussed in \\cite{toth2018perceptually}.\n\n\\section{Conclusions}\n\nSpecAugment greatly improves the performance of ASR networks. We are able to obtain state-of-the-art results on the LibriSpeech 960h and Switchboard 300h tasks on end-to-end LAS networks by augmenting the training set using simple hand-crafted policies, surpassing the performance of hybrid systems even without the aid of a language model. SpecAugment converts ASR from an over-fitting to an under-fitting problem, and we were able to gain performance by using bigger networks and training longer.\n\\smallskip\n\n\\noindent\n\\textbf{Acknowledgements:}\nWe would like to thank Yuan Cao, Ciprian Chelba, Kazuki Irie, Ye Jia, Anjuli Kannan, Patrick Nguyen, Vijay Peddinti, Rohit Prabhavalkar, Yonghui Wu and Shuyuan Zhang for useful discussions. We also thank Gy\\\"orgy Kov\\'acs for introducing us to the works \\cite{kovacs2017increasing, toth2018perceptually}.\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-1810.01398v2.tex",
        "arXiv-1902.01955v2.tex",
        "arXiv-1904.08779v3.tex"
    ],
    "group_id": "group_70",
    "response": "### Summary of Research Papers on Sequence Learning and Speech Recognition\n\n#### Title: Innovations in Sequence Learning and Speech Recognition: Optimal Completion Distillation, Modeling Units, and Data Augmentation\n\n#### Introduction\nThe field of sequence learning, particularly in the context of speech recognition, has seen significant advancements with the advent of neural network architectures such as Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), and Transformer models. These models have been widely adopted in various domains including machine translation, speech recognition, and image captioning, achieving impressive performance compared to traditional multi-stage pipelines. However, the training of these models is often fraught with challenges such as exposure bias and the mismatch between training and evaluation metrics, which can lead to poor generalization and suboptimal performance. Additionally, the choice of modeling units and the use of data augmentation techniques have been critical in enhancing the robustness and accuracy of these models. This summary explores three recent papers that address these challenges and propose innovative solutions to improve the performance of sequence-to-sequence (seq2seq) models in speech recognition.\n\nThe first paper introduces Optimal Completion Distillation (OCD), a novel training procedure for seq2seq models that optimizes the models based on edit distance. OCD aims to mitigate the exposure bias problem by training on prefixes generated from the model itself rather than ground truth sequences. The second paper examines the impact of different modeling units (phonemes, graphemes, and word-pieces) on the performance of LAS models in speech recognition, finding that word-piece and grapheme models outperform phoneme models across various datasets. The third paper presents SpecAugment, a simple yet effective data augmentation method for speech recognition, which operates directly on the log mel spectrogram of the input audio. SpecAugment consists of time warping, frequency masking, and time masking, and is shown to significantly improve the performance of LAS models on both LibriSpeech and Switchboard datasets.\n\n#### Main Content of Each Paper\n\n**Paper 1: Optimal Completion Distillation for Sequence Learning**\n\nThis paper introduces Optimal Completion Distillation (OCD), a training procedure for seq2seq models that is based on edit distance. OCD addresses two major issues associated with Maximum Likelihood Estimation (MLE) and teacher forcing: the mismatch between training and inference prefixes, and the discrepancy between the training loss and the task evaluation metric. Unlike MLE, which trains the model to maximize the log-probability of the correct output sequence, OCD trains the model to predict optimal extensions for any given prefix, thereby minimizing the total edit distance between the generated sequence and the ground truth. The OCD training procedure involves generating a full sequence from the model and then identifying the set of optimal suffixes that minimize the edit distance. For each prefix, OCD defines a target distribution that encourages the model to continue with the optimal extensions. The key innovation lies in the development of an efficient dynamic programming algorithm to compute these optimal Q-values, which are then used to distill the optimal completion policy into the model. The authors demonstrate the effectiveness of OCD on end-to-end speech recognition tasks using attention-based seq2seq models, achieving state-of-the-art performance on the Wall Street Journal (WSJ) and Librispeech datasets.\n\n**Paper 2: On the Choice of Modeling Unit for Sequence-to-Sequence Speech Recognition**\n\nThis paper investigates the impact of different modeling units (phonemes, graphemes, and word-pieces) on the performance of attention-based encoder-decoder models for speech recognition. The authors conduct experiments on the LibriSpeech 100hr, 460hr, and 960hr tasks, as well as the Switchboard 300h task. They find that word-piece and grapheme models consistently outperform phoneme-based models, even when evaluated without a lexicon or an external language model. Furthermore, they demonstrate that rescoring N-best lists generated from a strong word-piece based baseline with either a phoneme or grapheme model can improve Word Error Rate (WER) by up to 9% relative. The authors also note that word-piece models produce more diverse N-best hypotheses, leading to lower oracle WERs. The paper highlights the complementarity between different modeling units and suggests that combining them can yield better results than using a single unit.\n\n**Paper 3: SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition**\n\nThis paper presents SpecAugment, a data augmentation method that operates directly on the log mel spectrogram of the input audio. SpecAugment consists of three types of deformations: time warping, frequency masking, and time masking. The authors apply SpecAugment on Listen, Attend and Spell (LAS) networks for end-to-end speech recognition tasks and achieve state-of-the-art performance on the LibriSpeech 960h and Switchboard 300h tasks. They find that SpecAugment consistently improves the performance of the trained networks, and that the benefits of larger networks and longer learning rate schedules are more apparent with harsher augmentation policies. The paper also discusses the impact of label smoothing on training stability and the role of augmentation in converting over-fitting into under-fitting, which can be alleviated by training larger networks for longer periods.\n\n#### Commonalities and Innovations\n\nAll three papers focus on improving the performance of seq2seq models for speech recognition tasks. Paper 1 introduces a new training objective based on edit distance, which is more aligned with the evaluation metric used in speech recognition. Paper 2 explores the role of different modeling units in seq2seq models and demonstrates the benefits of using word-piece and grapheme models over phoneme models. Paper 3 presents a data augmentation method that operates directly on the feature inputs of the network, which is shown to improve the robustness and accuracy of LAS networks.\n\nThe key innovation in Paper 1 is the development of an efficient dynamic programming algorithm to compute optimal Q-values for edit distance, which are then used to distill an optimal completion policy into the model. This approach is stand-alone and does not require pre-training or joint optimization with MLE, making it more stable and straightforward to implement. Paper 2 highlights the complementarity between different modeling units and suggests that rescoring N-best lists can be a simple yet effective method to combine the strengths of different models. Paper 3 introduces SpecAugment, a method that is simple and computationally cheap to apply, and is shown to significantly improve the performance of LAS networks even without the use of a language model.\n\n#### Comparison of Results\n\n**Wall Street Journal (WSJ) Dataset**\n- **Paper 1**: OCD achieves a Character Error Rate (CER) of 3.1% and a Word Error Rate (WER) of 9.3% on the WSJ dataset, outperforming all previous work.\n- **Paper 2**: No specific results are provided for the WSJ dataset, but the authors compare their results with other work on LibriSpeech and Switchboard datasets.\n\n**LibriSpeech Dataset**\n- **Paper 1**: OCD achieves state-of-the-art WER of 5.8% on the ``test-clean'' and 13.4% on the ``test-other'' sets.\n- **Paper 2**: The best word-piece model achieves a WER of 4.4% on the ``test-clean'' and 13.2% on the ``test-other'' sets. Rescoring with grapheme and phoneme models improves WER by up to 9% relative.\n- **Paper 3**: LAS networks trained with SpecAugment achieve a WER of 2.8% on the ``test-clean'' and 6.8% on the ``test-other'' sets without a language model, and 2.5% and 5.8% with shallow fusion with a language model.\n\n**Switchboard 300h Dataset**\n- **Paper 2**: The best word-piece model achieves a WER of 4.4% on the Switchboard portion and 13.4% on the CallHome portion.\n- **Paper 3**: LAS networks trained with SpecAugment achieve a WER of 7.2% on the Switchboard portion and 14.6% on the CallHome portion without a language model, and 6.8% and 14.1% with shallow fusion with a language model.\n\n#### Discussion of Differences\nThe results across the three papers show that OCD (Paper 1) and SpecAugment (Paper 3) are particularly effective in improving the performance of seq2seq models for speech recognition. OCD achieves the best results on the WSJ dataset, while SpecAugment achieves the best results on the LibriSpeech and Switchboard datasets. Paper 2 highlights the importance of the choice of modeling units and suggests that word-piece and grapheme models are more effective than phoneme models, even when trained on smaller datasets. The authors also demonstrate that rescoring N-best lists with different models can improve performance, but the gains are more significant when rescoring with a grapheme or word-piece model.\n\n#### Conclusion\nThe three papers presented here contribute significantly to the field of sequence learning and speech recognition. OCD provides a new training objective that is more aligned with the evaluation metric used in speech recognition, leading to improved performance on the WSJ dataset. Paper 2 demonstrates the dominance of word-piece and grapheme models over phoneme models, even when trained on smaller datasets, and suggests that rescoring N-best lists can be a simple method to combine the strengths of different models. SpecAugment (Paper 3) is a data augmentation method that significantly improves the performance of LAS networks on both LibriSpeech and Switchboard datasets, even without the use of a language model. Future research could explore the combination of these methods to further enhance the performance of seq2seq models in speech recognition. Additionally, the impact of these methods on other sequence learning tasks such as machine translation and image captioning could also be investigated.\n\n#### Future Research Directions\nFuture research could focus on combining the OCD training procedure with SpecAugment to further improve the performance of seq2seq models in speech recognition. Additionally, the impact of these methods on other sequence learning tasks such as machine translation and image captioning could be explored. The use of different modeling units could also be investigated in the context of other tasks, and the potential benefits of rescoring N-best lists with different models could be further studied."
}