{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{\\icon GLM-130B: An Open Bilingual Pre-Trained Model}\n\n\\begin{document}\n\n\\doparttoc\n\\faketableofcontents\n\n\\maketitle\n\n\\renewcommand{\\thefootnote}{\\fnsymbol{footnote}}\n    \\footnotetext[1]{The two lead authors AZ and XL contributed equally (\\texttt{\\{zengaohan,shawliu9\\}@gmail.com})}\n    \\footnotetext[2]{Work partially done when AZ, XL, and ZD interned at Zhipu.AI.}\n    \\footnotetext[3]{Team leads: YD and JT. Corresponding author: JT (\\texttt{jietang@tsinghua.edu.cn})}\n\\renewcommand{\\thefootnote}{\\arabic{footnote}}\n\n\\begin{abstract}\n\nWe introduce \\glm, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. \nIt is an attempt to open-source a 100B-scale model at least as good as \nGPT-3 (davinci) and unveil how models of such a scale can be successfully pre-trained. \nOver the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and divergence. \nIn this paper, we introduce the training process of \\glm including its design choices, training strategies for both efficiency and stability, and engineering efforts. \nThe resultant \\glm model offers significant outperformance over GPT-3 175B (davinci) on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. \nIt also consistently and significantly outperforms ERNIE TITAN 3.0 260B---the largest Chinese language model---across related benchmarks. \nFinally, we leverage a unique scaling property of \\glm to reach INT4 quantization without post training, with almost no performance loss, making it the first among 100B-scale models and more importantly, allowing its effective inference on 4$\\times$RTX 3090 (24G) or 8$\\times$RTX 2080 Ti (11G) GPUs, the most affordable GPUs required for using 100B-scale models. \nThe \\glm model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at \\url{https://github.com/THUDM/GLM-130B/}.\n\\blfootnote{For detailed author contributions, please refer to Appendix~\\ref{app:contribution}.}\n\\end{abstract}\n\n\\hide{%%%%%%%%%%%%%%%%%%%%\n\n\\vspace{-8mm}\n\\begin{abstract}\nWe introduce \\glm, an open and inclusive 130-billion parameters bilingual large language model (LLM).\nAs an initial attempt from academia, we commit to promoting ``LLM Inclusivity'', which aims at offering highly accessible, usable, and transparent LLMs to all researchers and \nindividual developers.\nWe share a series of \nreal lessons on architecture design, training strategies choice, and inference algorithm from our 8-month trajectory for developing an LLM.\nAll these insights result in \\glm, which achieves \nbetter \nperformance than other LLMs including GPT-3 175B, OPT 175B, BLOOM 176B, and ERNIE Titan 3.0 on a number of English and Chinese benchmarks.\nIn addition to its quality, GLM's special architecture and the scaling makes \\glm the first LLM to be quantized into INT4 precision, which allows it to perform inference on popularized GPUs such as 4 $\\times$ RTX 3090 (24G) or 8 $\\times$ RTX 2080 Ti (11G).\nTogether with our speeding up efforts, \\glm can be up to 8.4$\\times$ faster than BLOOM 176B's official implementation, making \\glm a real inclusive LLM for everyone.\nThe model weights and code are open-sourced at \\url{https://anonymous.4open.science/r/GLM-130B/}.\n\\end{abstract}\n\n}%%%%%%%%%%%%%%%%%%%%%%%\n\n\\section{Introduction}\nLarge language models (LLMs), particularly those with over 100 billion (100B) parameters~\\citep{brown2020language,thoppilan2022lamda,rae2021scaling,chowdhery2022palm,wang2021ernie}, have presented attractive % but mysterious\nscaling laws~\\citep{wei2022emergent}, where emergent zero-shot and few-shot capabilities suddenly arose.\nAmong them, GPT-3~\\citep{brown2020language} with 175B parameters pioneers the study of 100B-scale LLMs by strikingly generating better performance with 32 labeled examples than the fully-supervised BERT-Large model on a variety of benchmarks. \nHowever, both GPT-3 (and many other closed-sourced 100B-scale ones)---the model itself---and how it can be trained, have been thus far intransparent to the public. \nIt is of critical value to train a high-quality LLM of such scale with both the model and training process shared with everyone.  %certainly with ethical concerns in mind. \n\nWe thus \\textit{aim to pre-train an open and highly-accurate 100B-scale model} with ethical concerns in mind. \nOver the course of our attempt, we have come to realize that pre-training a dense LLM at such a scale raises numerous unexpected technical and engineering challenges compared to training 10B-scale models, in terms of pre-training efficiency, stability, and convergence.  \nSimilar difficulties have also been concurrently observed in training OPT-175B~\\citep{zhang2022opt} and BLOOM-176B~\\citep{scao2022what}, further demonstrating the significance of GPT-3 as a pioneer study. \n\n\\begin{figure}[t]\n    \\centering\n    \\vspace{-4mm}\n    \\includegraphics[width=0.98\\linewidth]{figures/intro.pdf}\n    \\vspace{-4mm}\n    \\caption{A summary of the performance evaluation \n    and ethical studies.}\n    \\label{fig:intro}\n    \\vspace{-4mm}\n\\end{figure}\n\n\\begin{table}[t]\n\\centering\n\\footnotesize\n\\renewcommand\\tabcolsep{2.5pt}\n\\renewcommand\\arraystretch{0.9}\n\n\\caption{A comparison between \\glm and other 100B-scale LLMs and PaLM 540B. \n(LN: layer norm.; \nFPF: floating-point format; \nMIP: multi-task instruction pre-training; \nCN : Chinese)}\n\\vspace{-4mm}\n\\scalebox{0.9}{\n\\begin{tabular}{@{}lcccccccc@{}}\n\\toprule[1.2pt]\n                        &                                                                          & \\multicolumn{3}{c}{Architecture \\& Data}                                                                                                                                                                               & \\multicolumn{2}{c}{Training}                                               & \\multicolumn{2}{c}{Inference}                                                                                                                                \\\\ \\cmidrule(l){3-5} \\cmidrule(l){6-7} \\cmidrule(l){8-9} \n\\multirow{-2}{*}{Model} & \\multirow{-2}{*}{\\begin{tabular}[c]{@{}c@{}}Open-\\\\ source\\end{tabular}} & Objective                                                                                      & LN                                                   & Major Lang.                                                    & FPF  & Stabilization                                                       & Quantization                                        & GPU Needed                                                                                             \\\\ \\midrule\nGPT-3 175B              & $\\times$                                                                 &                                                                                                &                                                      & English                                                        & FP16 & {\\color[HTML]{9B9B9B} \\textit{\\small{undisclosed}}}                 & {\\color[HTML]{9B9B9B} \\textit{\\small{undisclosed}}} & {\\color[HTML]{9B9B9B} \\textit{\\small{undisclosed}}}                                                    \\\\\nOPT-175B                & $\\checkmark$                                                             &                                                                                                &                                                      & English                                                        & FP16 & Manual Adjusting                                                    & INT8                                                & 8 $\\times$ 3090                                                                                        \\\\\nBLOOM-176B              & $\\checkmark$                                                             & \\multirow{-3}{*}{GPT}                                                                          & \\multirow{-3}{*}{Pre-LN}                             & Multi-lingual                                                  & BF16 & Embedding Norm                                                      & INT8                                                & 8 $\\times$ 3090                                                                                        \\\\ \\midrule\nPaLM 540B               & $\\times$                                                                 & GPT                                                                                            & Pre-LN                                               & English                                                        & BF16 & Manual Adjusting                                                    & {\\color[HTML]{9B9B9B} \\textit{\\small{undisclosed}}} & {\\color[HTML]{9B9B9B} \\textit{\\small{undisclosed}}}                                                    \\\\ \\midrule\n\\glm                    & $\\checkmark$                                                             & {\\color[HTML]{CB0000} \\begin{tabular}[c]{@{}c@{}}GLM (Blank \\\\ Infilling \\& MIP)\\end{tabular}} & \\begin{tabular}[c]{@{}c@{}}Deep-\\\\ Norm\\end{tabular} & \\begin{tabular}[c]{@{}c@{}}Bilingual\\\\ (EN \\& CN)\\end{tabular} & FP16 & \\begin{tabular}[c]{@{}c@{}}Embedding\\\\ Gradient Shrink\\end{tabular} & INT4                                                & {\\color[HTML]{CB0000} \\begin{tabular}[c]{@{}c@{}}4 $\\times$ 3090 or\\\\ 8 $\\times$ 1080 Ti\\end{tabular}} \\\\ \\bottomrule[1.2pt]\n\\end{tabular}\n}\n\\vspace{-6mm}\n\\label{tab:intro}\n\\end{table}\n\nIn this work, we introduce the pre-training of a 100B-scale model---\\glm, in terms of engineering efforts, model design choices, training strategies for efficiency and stability, and quantization for affordable inference. \nAs it has been widely realized that it is computationally unaffordable to empirically enumerate all possible designs for training 100B-scale LLMs, we present not only the successful part for training \\glm but also many of the failed options and lessons learned. \nParticularly, the training stability is {the} decisive factor in the success of training models of such a scale. \nDifferent from practices such as manually adjusting learning rates in OPT-175B and using embedding norm in the sacrifice of performance in BLOOM-176B, we experiment with various options and find the strategy of embedding gradient shrink can significantly stabilize the training of \\glm.  \n\nSpecifically, \\glm is a bilingual (English and Chinese) bidirectional dense model with 130 billion parameters, pre-trained over 400 billion tokens \non a cluster of 96 NVIDIA DGX-A100 (8$\\times$40G) GPU nodes \nbetween May 6 and July 3, 2022. \nInstead of using the GPT-style architecture, we adopt the  General Language Model (GLM) algorithm~\\citep{du2022glm} to leverage its bidirectional attention advantage and autoregressive blank infilling objective. \nTable~\\ref{tab:intro} summarizes the comparison between \\glm, GPT-3 and another two open-source efforts---OPT-175B and BLOOM-176B, as well as PaLM 540B~\\citep{chowdhery2022palm}---a 4$\\times$ larger model---as a reference. \n\nAltogether, the conceptual uniqueness and engineering efforts enable \\glm to exhibit performance that surpasses the level of GPT-3 on a wide range of benchmarks (in total 112 tasks) and also outperforms PaLM 540B in many cases, while outperformance over GPT-3 has not been observed in OPT-175B and BLOOM-176B (Cf. Figure ~\\ref{fig:intro} left). \nFor zero-shot performance, \\glm is better than GPT-3 175B (+5.0\\%), OPT-175B (+6.5\\%), and BLOOM-176B (+13.0\\%) on LAMBADA~\\citep{paperno2016lambada}, and achieves 3$\\times$ better performance than GPT-3 on Big-bench-lite~\\citep{srivastava2022beyond}. \nFor the 5-shot MMLU~\\citep{hendrycks2021measuring} tasks, it is  better than GPT-3 175B (+0.9\\%) and BLOOM-176B (+12.7\\%). \nAs a bilingual LLM also in Chinese, it offers significantly better results than ERNIE TITAN 3.0 260B~\\citep{wang2021ernie}---the largest Chinese LLM---on 7 zero-shot CLUE~\\citep{xu2020clue} datasets (+24.26\\%) and 5 zero-shot FewCLUE~\\citep{xu2021fewclue} ones (+12.75\\%). \nImportantly, as summarized in Figure~\\ref{fig:intro} right, \\glm as an open model is associated with \\textit{significantly less bias and generation toxicity than its 100B-scale counterparts}. \n\nFinally, we design \\glm to empower as many people as possible to conduct 100B-scale LLM studies. \nFirst, instead of using 175B+ parameters as OPT and BLOOM, the 130B size is decided because such a size supports inference on a single A100 (8$\\times$40G) server. \nSecond, to further lower the GPU requirements, we quantize \\glm into INT4 precision without post training  while OPT and BLOOM can only reach INT8. \nDue to a unique property of the GLM architecture, \\glm's INT4 quantization introduces negligible performance degradation, e.g., -0.74\\% on LAMBADA and even +0.05\\% on MMLU, making it still better than the uncompressed GPT-3. \nThis enables \\glm's fast inference with performance guarantee on a server of 4$\\times$RTX 3090 (24G) or 8$\\times$RTX 2080 Ti (11G), \\textit{the most affordable GPU required for using 100B-scale LLMs to date. }\n\nWe open-source the model checkpoints, code, training logs, related toolkits, and lessons learned. \n\n\\hide{%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n\\section{Introduction}\nLarge Language Models (LLMs), which refers to transformer-based~\\citep{vaswani2017attention} statistical language models with an enormous quantity of parameters trained on web-scale text contents, have experienced a surge in the past few years.\nParticularly, those who exceed 100 billion parameters, to which we use ``LLMs'' to refer to in a narrow sense~\\citep{brown2020language,thoppilan2022lamda,rae2021scaling,chowdhery2022palm,wang2021ernie},  have presented attractive but mysterious scaling laws, where emergent zero-shot and few-shot capabilities suddenly arise~\\citep{wei2022emergent}.\nDespite much controversy on whether merely scaling LLMs leads to general machine intelligence, now it is generally accepted that we shall look into LLMs.\n\nNevertheless, academic  researchers and individual developers have been suffered from limited access and poor usability of LLMs.\nFor accessibility, most LLMs are unavailable to public and a few of them~\\citep{brown2020language,lieber2021jurassic} provide intransparent limited APIs with fees.\nFor usability, even if these LLMs are open-sourced, most individuals and academic institutions can hardly afford the outrageous cost for inference, let alone further tuning.\n``LLM Monopoly'' does no good to LLM's development, as it requires the whole community's exertion, including big companies, academia, and individuals, to push LLM's boundary and realize its promised welfare to people.\n\nIn this work, we introduce \\glm as an initial step to fulfill our commitment of ``LLM Inclusivity''.\nIt is a GLM~\\citep{du2022glm} language model possessing 130 billion parameters and is pre-trained over both English and Chinese corpora for 400 billion tokens.\n\\glm's effort is in concurrence with recent OPT-175B~\\citep{zhang2022opt} and BLOOM-176B~\\citep{scao2022what} to open-source and popularize the research and use of LLMs.\nHowever, compared to them \\glm is uniquely focusing on offering our community a powerful and high-usability LLM, from the very beginning of its conception and design.\nSpecifically, our elaboration covers the four main aspects concerning an LLM's architecture, objective, training, and inference. For the first time, \n\\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.2em,topsep=0.0em,partopsep=0.0em]\n    \\item \\textbf{Architecture}: we adopt bidirectional GLM instead of unidirectional GPT in an LLM and demonstrate its superiority in downstream evaluation. Other necessary components are also identified.\n    \\item \\textbf{Objective}: we include Multitask Instruction Pre-training (MIP), along with the autoregressive blank infilling objective of GLM, as the LLM's objective.\n    \\item \\textbf{Training Strategy}: we unveil part of hidden reasons behind the extreme instability in LLM's pre-training, and propose effective strategies to stabilize its training without harming performance.\n    \\item \\textbf{Inference}: we quantize \\glm, a 130B LLM, into INT4 precision with little performance loss to allow its fast inference on even 4 $\\times$ RTX 3090 Ti (24G) or 8 $\\times$ RTX 2080 Ti (11G).\n\\end{itemize}\n\nIn the end, it turns out that our design and training enable \\glm, a bilingual language model, to outperform similar-sized LLMs such as GPT-3 175B~\\citep{brown2020language}, BLOOM-176B~\\citep{scao2022what}, and ERNIE Titan 3.0 (260B)~\\citep{wang2021ernie} over a wide range of zero-shot and few-shot benchmarks, including language modeling~\\citep{paperno2016lambada,marcinkiewicz1994building}, multiple-choice question answering~\\citep{hendrycks2021measuring}, and versatile reasoning challenges~\\citep{srivastava2022beyond} and Chinese language tasks~\\citep{xu2020clue,xu2021fewclue}.\nIt even outstrips PaLM-540B~\\citep{chowdhery2022palm}, a 4$\\times$ larger language model, on zero-shot language modeling dataset LAMBADA~\\citep{paperno2016lambada} and zero-shot BIG-bench-lite~\\citep{srivastava2022beyond} to set up new records.\n\nTo sum up, we introduce our following contributions regarding LLMs and \\glm in this work \n\\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.2em,topsep=0.0em,partopsep=0.0em]\n    \\item We demonstrate our long-term commitment towards ``LLM Inclusivity'' technically and economically. It cares about the participation of all community members in LLMs' research and application. We appeal to all individuals and organizations in this field for joint efforts in this commission.\n    \\item We introduce \\glm, an open and inclusive bilingual LLM. It is fully optimized for high quality and popularization on its architecture, training and inference strategies. Our experiments on extensive zero-shot and few-shot benchmarks shows that \\glm can be competent and even better than its counterparts such as GPT-3 175B, OPT-175B, and BLOOM 176B.\n    \\item We open-source model checkpoints, code, training logs, and toolkit related to \\glm. Based on which people can easily reproduce all our evaluation, and do fast offline inference on their own machines with hardwares as popularized as 4 $\\times$ RTX 3090 (24G) or 8 $\\times$ RTX 2080 Ti (11G).\n\\end{itemize}\n\n}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \\clearpage\n\n\\vspace{-3mm}\n\\section{The Design Choices of \\glm}\n\\vspace{-2mm}\nThe architecture of a machine learning model defines its inductive bias. %, which matters to its scaling efficiency~\\citep{tay2022scaling}. \nHowever, it has been realized that it is computationally unaffordable to explore various architectural designs for LLMs.\nWe introduce and explain the unique design choices of \\glm. \n\n\\vspace{-2mm}\n\\subsection{\\glm's Architecture} \\label{sec:glm-arch}\n\\vspace{-2mm}\n\n\\vvpara{GLM as  Backbone.}\nMost recent 100B-scale LLMs, such as GPT-3, PaLM, OPT, and BLOOM, follow the traditional GPT-style~\\citep{radford2019language} architecture of decoder-only autoregressive language modeling. %, likely due to the arduously expensive cost of stably training with a new architecture. \nIn \\glm, we instead make an attempt to explore the potential of a bidirectional GLM---General Language Model~\\citep{du2022glm}---as its backbone.\n\nGLM is a transformer-based language model that leverages autoregressive blank infilling as its training objective.\nBriefly, for a text sequence $\\boldsymbol{x}=[x_1,\\cdots,x_n]$, text spans $\\{\\boldsymbol{s}_1,\\cdots,\\boldsymbol{s}_m\\}$ are sampled from it, each of which $\\boldsymbol{s}_i$ denotes a span of consecutive tokens $[s_{i,1},\\cdots,s_{i,l_i}]$ and is replaced (i.e., corrupted) with a single mask token to form $\\boldsymbol{x}_{\\text{corrupt}}$. \nThe model is asked to recover them autoregressively.\nTo allow interactions between corrupted spans, their visibility to each other is decided by a randomly sampled permutation on their order. \n\nGLM's bidirectional attention over unmasked (i.e., uncorrupted) contexts distinguishes \\glm from GPT-style LLMs in which the unidirectional attention is used. \nTo support both understanding and generation, it mixes two corruption objectives, each indicated by a special mask token:\n\\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.2em,topsep=0.0em,partopsep=0.0em]\n    \\item \\textbf{[MASK]}: short blanks in sentences whose lengths add up to a certain portion of the input.\n    \\item \\textbf{[gMASK]}: random-length long blanks at the end of sentences with prefix contexts provided.\n\\end{itemize}\n\n\\begin{wrapfigure}{r}{6cm}\n    \\small\n    \\vspace{-6mm}\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{figures/architecture.pdf}\n    \\vspace{-6mm}\n    \\caption{\\glm and LLMs of similar scale on zero-shot LAMBADA language modeling. \n    Details on GLM's bidirectional attention are provided in \\citet{du2022glm}.}\n    \\label{fig:lambada}\n    \\vspace{-8mm}\n\\end{wrapfigure}\n\nConceptually, the blank infilling objective with bidirectional attention enables a more effective comprehension of contexts than GPT-style models: \nwhen using [MASK], \\glm behaves as BERT~\\citep{devlin2019bert} and T5~\\citep{raffel2020exploring}; \nwhen using [gMASK], \\glm behaves similarly to PrefixLM~\\citep{liu2018generating,dong2019unified}.\n\nEmpirically, \n\\glm %with bidirectional attention \noffers a record-high accuracy of 80.2\\% on zero-shot LAMBADA  by outperforming both GPT-3 and PaLM 540B in Figure \\ref{fig:lambada}. \nBy setting the attention mask, \\glm's  unidirectional variant is comparable to GPT-3 and OPT-175B. \nOur observations are in line with existing findings~\\citep{liu2018generating,dong2019unified}. \n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=.92\\linewidth]{figures/layernorm.pdf}\n    \\vspace{-2mm}\n    \\caption{Trials on different LayerNorms for \\glm training. It turns out that DeepNorm is the most stable one, as it has small gradient norm and does not spike in the early stage training.}\n    \\vspace{-6mm}\n    \\label{fig:layernorm}\n\\end{figure}\n\n\\vvpara{Layer Normalization (LN,~\\cite{ba2016layer}).}\nTraining instability is one major challenge for training LLMs~\\citep{zhang2022opt,scao2022what,chowdhery2022palm} (Cf. Figure ~\\ref{fig:collapse} in Appendix for  collapses in training several 100B-scale models). \nA proper choice of LNs can help stabilize the training of LLMs. %, such as Pre-LN~\\citep{xiong2020layer} for GPT-3, OPT-175B, BLOOM-176B, and Sandwich-LN in CogView~\\citep{ding2021cogview} and NormFormers~\\citep{shleifer2021normformer}.\nWe experiment with existing practices, e.g., Pre-LN~\\citep{xiong2020layer}, Post-LN~\\citep{ba2016layer}, %\\yuxiao{xxx}\nSandwich-LN~\\citep{ding2021cogview}, which are unfortunately incapable of stabilizing our \\glm test runs (Cf. Figure~\\ref{fig:layernorm} (a) and Appendix~\\ref{app:ln} for details). \n\nOur search is later focused on Post-LN due to its favorable downstream results in preliminary experiments though it does not stabilize \\glm. \nFortunately, one of the attempts on Post-LN initialized with the newly-proposed DeepNorm~\\citep{wang2022deepnet} generates promising training stability. \nSpecifically, given the number of \\glm's layers $N$, \n    we adopt $\\textrm{DeepNorm}(\\boldsymbol{x}) = \\textrm{LayerNorm}(\\alpha\\cdot \\boldsymbol{x} + \\textrm{Network}(\\boldsymbol{x}))$, where $\\alpha=(2N)^\\frac{1}{2}$, \n    and \n   apply the Xavier normal initialization with the scaling factor of $(2N)^{-\\frac{1}{2}}$ to \\texttt{ffn}, \\texttt{v\\_proj} and \\texttt{out\\_proj}. Additionally, all bias terms are initialized to zero.\nFigure~\\ref{fig:layernorm} shows it significantly benefits the training stability of \\glm. \n\n\\vvpara{Positional Encoding and FFNs.}\nWe empirically test different options for positional encoding (PE) and FFN improvements in terms of both training stability and downstream performance (Cf. Appendix \\ref{app:pe-ffn} for details).  \nFor PEs in \\glm, we adopt Rotary Positional Encoding (RoPE, \\cite{su2021roformer}) rather than  ALiBi~\\citep{press2021train}. \nTo improve FFNs in Transformer, we pick GLU with the GeLU~\\citep{hendrycks2016gaussian} activation as the replacement.\n\n\\vspace{-2mm}\n\\subsection{\\glm's Pre-Training Setup} \\label{sec:training_objective}\n\\vspace{-2mm}\n\nInspired by recent works~\\citep{aribandi2022ext5,wei2022finetuned,sanh2022multitask}, the \\glm pre-training objective includes not only the self-supervised GLM autoregressive blank infilling) but also multi-task learning for a small portion of tokens. \nThis is expected to help boost its downstream zero-shot performance. \n\n\\vvpara{Self-Supervised Blank Infilling (95\\% tokens).}\nRecall that \\glm uses both [MASK] and [gMASK] for this task. \nEach training sequence is applied with one of them independently at a time.\nSpecifically, [MASK] is used to mask consecutive spans in 30\\% of training sequences for blank infilling. The lengths of spans follow a Poisson distribution ($\\lambda=3$) and add up to 15\\% of the input. \nFor the other 70\\% sequences, the prefix of each sequence is kept as context and [gMASK] is used to mask the rest of it. \nThe masked length is sampled from the Uniform distribution.\n\nThe pre-training data includes 1.2T Pile (train split)~\\citep{gao2020pile} English, 1.0T Chinese WudaoCorpora~\\citep{yuan2021wudaocorpora}, %\\todo{to exclude or not?},\nand 250G Chinese corpora (including online forums, encyclopedia, and QA) we crawl from the web, which form a balanced composition of English and Chinese contents.\n\n\\vvpara{Multi-Task Instruction Pre-Training (MIP, 5\\% tokens).}\nT5~\\citep{raffel2020exploring} and ExT5~\\citep{aribandi2022ext5} suggest that multi-task learning in pre-training can be more helpful than fine-tuning, we thus propose to include a variety of instruction prompted datasets including language understanding, generation, and information extraction in \\glm's pre-training.\n\nCompared to recent works~\\citep{wei2022finetuned,sanh2022multitask} that leverage multi-task prompted fine-tuning to improve zero-shot task transfer, MIP only accounts for 5\\% tokens and is set in the pre-training stage to prevent spoiling LLMs' other general ability, e.g., unconditional free generation.\nSpecifically, we include 74 prompted datasets from ~\\citep{sanh2022multitask,wang2022deepstruct}, listed in Appendix~\\ref{app:data} and Table~\\ref{tab:mip}. \n\\glm users are suggested to {avoid evaluating its zero-shot and few-shot capabilities on these datasets} according to the criterion illustrated in Section~\\ref{sec:results}.\n\n\\hide{%====start of hide =========================================================================\n\n\\section{Algorithm Designs}\nIn this section, we introduce the algorithm designs concerning \\glm's architectural thoughts and the pre-training objectives.\nCompared to most existing LLMs such as GPT-3~\\citep{brown2020language}, PaLM-540B~\\citep{chowdhery2022palm}, OPT~\\citep{zhang2022opt}, and BLOOM~\\citep{scao2022what}, \\glm is not a GPT architecture language model, but a bidirectional General Language Model (GLM, \\cite{du2022glm}) trained to fill in the blanks. \nThe design introduces significant advantages, as well as emerged challenges.\nIn this section, we explain the reasons to adopt the GLM and the solutions for conquering the challenges in scaling it up to 130 billion parameters.\n\n\\subsection{\\glm's Architecture} \\label{sec:glm-arch}\nArchitectures define the inductive bias for machine learning models, which matters to their scaling efficiency~\\citep{tay2022scaling}.\nHowever, due to the outrageous computing costs of LLMs, it is unaffordable to exploit various architectural designs.\nIn \\glm, based on real experiments we present several key insights on architectural designs for the goal of inclusive LLMs.\n\n\\vvpara{Backbone Architecture.}\nIn the past few years, the community has witnessed the surge of a series of different language model architectures~\\citep{radford2019language,devlin2019bert,yang2019xlnet,raffel2020exploring}.\nNevertheless, existing LLMs mostly follow the traditional GPT-style~\\citep{radford2019language} architecture of decoder-only autoregressive language modeling.\nIt is probably because LLMs can be extremely unstable in training and are thus too expensive to test new ideas once failed.\nHowever, it is questionable that GPT-style architecture would be the best option for LLMs.\n\nIn \\glm, we take an audacious step to adopt a new architecture---GLM~\\citep{du2022glm}---for pre-training a 130-billion LLM.\nIn brief, GLM is transformer-based language model which leverages autoregressive blank infilling as its basic training objective.\nFor a text sequence $\\boldsymbol{x}=[x_1,\\cdots,x_n]$, text spans $\\{\\boldsymbol{s}_1,\\cdots,\\boldsymbol{s}_m\\}$ are sampled from it, each of which denotes a span of consecutive tokens $[s_{i,1},\\cdots,s_{i,l_i}]$.\nThey are replaced (i.e., corrupted) with a single mask token respectively to form $\\boldsymbol{x}_{\\text{corrupt}}$ and the model is asked to recover them autoregressively.\nTo allow interactions between corrupted spans, their visibility to each other are decided by a randomly sampled permutation on their order.\nThen, we can define the pre-training objective for \\glm as\n\\begin{equation}\n    \\mathcal{L}_{\\text \\glm} \n    = \\max_\\theta \\E_{\\boldsymbol{z}\\sim Z_m}\\left[\\sum_{i=1}^m\\log \\prod_{j=1}^{l_i} p(s_{i,j}|\\boldsymbol{x}_{\\text{corrupt}},\\boldsymbol{s}_{\\boldsymbol{z}_{<i}},\\boldsymbol{s}_{i,<j}) \\right]\n    \\label{eqn:objective}\n\\end{equation}\n\\noindent where $Z_m$ denotes the set of the sequence's all permutations and $\\boldsymbol{s}_{\\boldsymbol{z}_{<i}}$ denotes $[\\boldsymbol{s}_{z_1},\\cdots,\\boldsymbol{s}_{z_{i-1}}]$.\n\n\\glm's bidirectional attention over unmasked (i.e., uncorrupted) contexts distinguishes itself from other GPT-style LLMs' unidirectional attention. \nIt mixes two corruption objectives to allow strong performance on both understanding and generation, each indicated with a special mask token:\n\\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.2em,topsep=0.0em,partopsep=0.0em]\n    \\item \\textbf{[MASK]}: short consecutive blanks in sentences whose lengths add up to 15\\% of the input.\n    \\item \\textbf{[gMASK]}: random-length long blanks at the end of sentences with prefix contexts provided.\n\\end{itemize}\n\n\\begin{wrapfigure}{r}{6cm}\n    \\small\n    \\vspace{-6mm}\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{figures/architecture.pdf}\n    \\caption{\\glm and other LLMs on zero-shot LAMBADA language modeling. More explanations on GLM's bidirectional attention are provided in \\citet{du2022glm}.}\n    \\label{fig:lambada}\n    \\vspace{-6mm}\n\\end{wrapfigure}\n\nThe blanking infilling objective with bidirectional attention contributes to a more effective comprehension of the context information than GPT-style models: when using [MASK], \\glm behaves as BERT~\\citep{devlin2019bert} and T5~\\citep{raffel2020exploring}; when using [gMASK], \\glm behaves similarly to PrefixLM~\\citep{liu2018generating,dong2019unified}.\n\nAs an example, we show its results on LAMBADA~\\citep{paperno2016lambada} zero-shot language modeling.\nWe follow the evaluation method used in GPT-2~\\citep{radford2019language} and GPT-3~\\citep{brown2020language}.\nWe observe that \\glm with bidirectional attention over context set up a new record of 80.2\\% accuracy, even outperforming the 4$\\times$ larger PaLM-540B~\\citep{chowdhery2022palm}.\nOn the contrary, \\glm's ablated unidirectional variant (by setting the attention mask) reports 75.3\\% on LAMBADA, comparable to other similar-sized GPT-style models including GPT-3 and OPT-175B~\\cite{zhang2022opt}.\nOur observations also accord with findings in~\\citep{liu2018generating,dong2019unified}.\n\n\\begin{insight}\n\\rm Bidirectional-attention GLM can be stronger than unidirectional GPT at large scale.\n\\end{insight}\n\n\\vvpara{Layer Normalization.}\nLayer normalization (or LN, \\cite{ba2016layer}) stabilizes the transformer training and has a significant impact on models' quality and downstream performance.\nA proper choice of LNs becomes more critical for LLMs as models scale up, where the challenge of training instability significantly exaggerates. %\\todo{do we need figures here?}\nThere have been several successful LN variants including vanilla Post-LN~\\citep{vaswani2017attention}, Pre-LN~\\citep{xiong2020layer}, and Sandwich-LN~\\citep{ding2021cogview}. \nHowever, in our extensive experiments, we verify that none of their performance on \\glm is satisfying (Cf. Appendix~\\ref{app:ln} for our detailed study).\n\nWe commit to finding a Post-LN variant that can stabilize \\glm's training, and finally targeting on DeepNorm~\\citep{wang2022deepnet}, a new initialization approach for Post-LN.\nIt is reported to stabilize the training of a 1000-layer transformer.\nGiven the number of \\glm's layers $N$\n\n\\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.2em,topsep=0.0em,partopsep=0.0em]\n    \\item We adopt $\\textrm{DeepNorm}(\\boldsymbol{x}) = \\textrm{LayerNorm}(\\alpha\\cdot \\boldsymbol{x} + \\textrm{Network}(\\boldsymbol{x}))$, where $\\alpha=(2N)^\\frac{1}{2}$.\n    \\item We apply Xavier normal initialization with the scaling factor of $(2N)^{-\\frac{1}{2}}$ to \\texttt{ffn}, \\texttt{v\\_proj} and \\texttt{out\\_proj}. Additionally, all bias terms are initialized to zero.\n\\end{itemize}\n\n\\begin{wrapfigure}{r}{6cm}\n    \\small\n    \\vspace{-4mm}\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{figures/layernorm.pdf}\n    \\vspace{-5mm}\n    \\caption{Different LNs' gradient norm along \\glm's early training steps.}\n    \\label{fig:layernorm}\n    \\vspace{-6mm}\n\\end{wrapfigure}\n\nThough \\glm only possesses 70 layers, the strategy significantly benefits the training stability.\nIn Figure~\\ref{fig:layernorm}, along the early pre-training steps of \\glm, we plot gradient norm (i.e., the gradients' $l^2$-norm) of Sandwich-LN and Post-LN with DeepNorm initialization, which reflects how drastic a model is updated at a certain step.\nWe observe that \\glm with Sandwich-LN has several magnitude larger gradient norms than that of Post-LN with DeepNorm, and it finally collapses at around 2.5k steps; while Post-LN with DeepNorm helps \\glm to be stably trained.\n\n\\begin{insight}\n\\rm It is a counter-stereotype that the Post-LN initialized with DeepNorm would be the only feasible layer normalization to stabilize \\glm's pre-training. \n\\end{insight}\n\n\\vvpara{Rotary Positional Encoding (RoPE, \\cite{su2021roformer}).}\nVanilla transformer adopts absolute (or sinuous) PE, and is later evolved into relative PE~\\citep{dai2019transformer}.\nTwo recent popular relative PE variants are RoPE adopted by PaLM-540B~\\citep{chowdhery2022palm} and GPT-J~\\cite{gpt-j}, and ALiBi~\\citep{press2021train} adopted by BLOOM-176B~\\citep{scao2022what}.\nIn our testing, we find RoPE to be a better option for \\glm, \nlikely because its bidirectional attention makes ALiBi relatively inefficient for both engineering implementation and training convergence.\n\nAnd in \\glm, different from the two-dimensional positional encoding used in vanilla GLM, we turn back to conventional one-dimensional positional encoding.\nAt the time when we designed \\glm, unfortunately we did not figure out how to implement two-dimensional RoPE\\footnote{It was not until recently that we found a blog by RoPE's author on suggestions for two-dimensional RoPE.}.\nAs a substitute plan, in \\glm we simply remove the second dimension used in original GLM as we find that the unidirectional attention mask sub-matrices for [MASK] generation indicate the token order as well.\nFor long generation with [gMASK] (which is always at the end of the context), generated tokens will just prolong the first-dimensional positional encoding from the last context token.\nMore details on \\glm's positional encoding formulation are presented in Appendix~\\ref{app:pe}.\n\n\\vvpara{Gated Linear Units (GLU, \\cite{dauphin2017language}).}\nSome recent efforts on transformer has been on improving the FFNs.\nWe have noticed the GLU adopted in PaLM-540B and its new variant Gated Attention Unit (GAU, \\cite{hua2022transformer}), and test them in our experiments by pre-training GLM-base (110M) over a random 50G Chinese and English mixed corpus. \nIt turns out that both GLU and GAU can improve upon the vanilla FFN, while GLU can be better and more stable in training.\nConsequently in \\glm we adopt GLU with GeLU~\\citep{hendrycks2016gaussian} activation\n\\begin{equation}\n\\operatorname{FFN}_{\\mathrm{GeGLU}}\\left(\\boldsymbol{x}; \\boldsymbol{W}_1, \\boldsymbol{V}, \\boldsymbol{W}_{2}\\right)=\\left(\\mathrm{GeLU}(\\boldsymbol{x} \\boldsymbol{W}_1) \\otimes \\boldsymbol{x} \\boldsymbol{V}\\right) \\boldsymbol{W}_{2}\n\\end{equation}\n\nIn order to keep the same parameter as the vanilla FFN, the feed-forward size $d_{\\mathrm{ffn}}$ (which is usually $4 d_{\\mathrm{H}}$, where $d_{\\mathrm{H}}$ is the hidden dimension) is reduced to $\\frac 8 3 d_{\\mathrm{H}}$ as the $\\boldsymbol{V}$ is additionally introduced.\n\n\\subsection{\\glm's Pre-training Objectives and Data}\nIn \\glm, instead of mere self-supervised autoregressive blank infilling, we refer to recent works~\\citep{aribandi2022ext5,wei2022finetuned,sanh2022multitask} to include a small portion of multi-task learning in the pre-training to boost its downstream zero-shot learning capability. \n\n\\vvpara{Self-supervised Pre-training (95\\% tokens).}\nIn the \\glm's implementation, for 30\\% training tokens, we use [MASK] to mask consecutive spans whose lengths conform to the Poisson distribution ($\\lambda=3$) and they add up to 15\\% of the input sequences.\nFor the other 70\\% tokens, we provide the prefix of a sequence as context and use [gMASK] to mask the rest tokens for \\glm to predict; the masked length is sampled from the Uniform distribution.\nThe setting endows \\glm with strong capability on both natural language understanding (using [MASK] similar to BERT) and natural language generation (using [gMASK] similar to GPT) with proper prompts.\n\nThe pre-training data include 1.2T Pile~\\citep{gao2020pile} English corpus, 1.0T Chinese WudaoCorpora~\\citep{yuan2021wudaocorpora} \\todo{to exclude or not?}, and around 250G several Chinese corpora we crawl from the web, which finally forms a balanced composition of English and Chinese corpora.\n\n\\vvpara{Multi-task Instruction Pre-training (MIP, 5\\% tokens).}\nRecent work such as FLAN~\\citep{wei2022finetuned} and T0~\\citep{sanh2022multitask} shows that LLMs fine-tuned on multi-task instruction datasets can be better at zero-shot learning.\nInstead of doing it in fine-tuning, following conclusion in T5~\\citep{raffel2020exploring} and ExT5~\\citep{aribandi2022ext5} that multi-task learning in pre-training can be even helpful than fine-tuning, we propose to include a variety of instruction prompted datasets including language understanding, generation, and information extraction in \\glm's pre-training.\n\nWe originally planed to only include training datasets' training splits of T0~\\citep{sanh2022multitask} and DeepStruct~\\citep{wang2022deepstruct}.\nAll prompts for T0 datasets are from PromptSource~\\citep{bach2022promptsource} and prompts for DeepStruct datasets are newly created.\nbut by a mistake at first we include the training and evaluation datasets' training splits and exclude DeepStruct datasets. \nWe fixed the mistake at around 23k steps and shifted back to the correct version. \nAll 74 datasets, corresponding prompts used, and the data preprocess details in MIP are listed in Appendix \\ref{app:data}.\nWe suggest that users should not evaluating \\glm's zero-shot and few-shot capabilities on these datasets.\n\n}%====end of hide =========================================================================\n\n\\subsection{Platform-Aware Parallel Strategies and Model Configurations} \\label{sec:parallel_strategy}\n\n\\glm is trained on a cluster of 96 DGX-A100 GPU (8$\\times$40G) servers with a 60-day access.  \nThe goal is to pass through as many tokens as possible, as a recent study~\\citep{hoffmann2022training} suggests that most existing LLMs are largely under-trained. \n\n\\vvpara{The 3D Parallel Strategy.}\nThe data parallelism~\\citep{valiant1990bridging} and tensor model parallelism~\\citep{shoeybi2019megatron} are the de facto practices for training billion-scale models~\\citep{gpt-j,du2022glm}. \nTo further handle\nthe huge GPU memory requirement and the decrease in overall GPU utilization resulted from applying tensor parallel between nodes---as 40G rather than 80G A100s are used for training \\glm, we combine the pipeline model parallelism with the other two strategies to form a 3D parallel strategy. \n\nThe pipeline parallelism divides the model into sequential stages for each parallel group, and to further minimize bubbles introduced by pipeline, we leverage the PipeDream-Flush~\\citep{narayanan2021memory} implementation from DeepSpeed~\\citep{rasley2020deepspeed} to train \\glm with a relative big global batch size (4,224) to reduce time and GPU memory wasting. \nThrough both numerical and empirical examinations, we adopt 4-way tensor parallelism and 8-way pipeline parallelism (Cf. Appendix~\\ref{app:pipeline} for details). Following the calculation in~\\citep{chowdhery2022palm}, we report hardware FLOPs utilization (HFU) of 43.3\\% and model FLOPs utilization (MFU) of 32.5\\% due to re-materialization.\n\n\\vvpara{\\glm Configurations.}\nWe aim to enable our 100B-scale LLM to run a single DGX-A100 (40G) node in FP16 precision. \nBased on the hidden state dimension of 12,288 we adopt from GPT-3, the resultant model size has to be no more than 130B parameters, thus \\glm.\nTo maximize GPU utilization, we configure the model based on the platform and its corresponding parallel strategy. \nTo avoid insufficient memory utilization in the middle stages\ndue to the additional word embedding at both ends, we balance the pipeline partition by removing one layer from them, making 9$\\times$8-2=70 transformer layers in \\glm.\n\nDuring the 60-day access to the cluster, we manage to train \\glm for 400 billion tokens (roughly 200 billion each for Chinese and English) with a fixed sequence length of 2,048 per sample. \nFor the [gMASK] training objective, we use a context window of 2,048 tokens. \nFor the [MASK] and multi-task objectives, we use a context window of 512 and concatenate four samples together to cater the 2,048-sequence-length. \nWe warm-up the batch size from 192 to 4224 over the first 2.5\\% samples. \nWe use AdamW~\\citep{loshchilov2017decoupled} as our optimizer with $\\beta_1$ and $\\beta_2$ set to 0.9 and 0.95, and a weight decay value of 0.1. \nWe warm up the learning rate from $10^{-7}$ to $8\\times 10^{-5}$ over the first 0.5\\% samples, then decay it by a $10\\times$ cosine schedule. \nWe use a dropout rate of 0.1 and clip gradients using a clipping value of 1.0 (Cf. Table~\\ref{tab:config} for the full configurations).\n\n\\hide{%start of hide %%%%%%%%%%%%%=====================================================================\n\n\\subsection{Parallel Strategies} \\label{sec:parallel_strategy}\nTraining LLMs can be outrageously expensive.\nIt is said that GPT-3~\\citep{brown2020language} was trained over 10,000 NVIDIA V100 GPUs for months at substantial expenses.\nHowever, recent study~\\citep{hoffmann2022training} shows that existing LLMs are largely under-trained; what matters in the first place to LLMs' performance is the number of trained tokens.\nThus for Inclusive LLM, an optimized framework that fully exploits computing potential is never overemphasized.\n\nHere we introduce \\glm's framework-level insights, including the 3D parallel strategy (Cf. Figure~\\ref{fig:3d-parallelism} in Appendix), LLM configuration principles, and computation graph optimization.\nThey in all contribute \\todo{} speeding up to pre-training, making a crucial improvement to \\glm's quality.\n\n\\vvpara{3D Parallel Strategy.}\n\\aohan{I think this section should be downplayed as more details can be found in the appendix}\nParallel strategies, including data parallelism~\\citep{valiant1990bridging} and tensor model parallelism~\\citep{shoeybi2019megatron}, are de facto practices for training billion-scale language models. \nHowever, when models continue to scale up to over 100B, the scaling up of a tensor parallelism group become insufficient.\nOn one hand, as the computation per layer is evenly distributed in a group, a single node's computational granularity would decrease when group sizes grow up, causing a performance drop of matrix multiplication operator \\texttt{GEMM} and a decrease in overall utilization.\nOn the other hand, if group size exceeds certain threshold (e.g., number of GPUs per node), the \\texttt{All-Reduce} operation becomes a bottleneck due to the expensive cross-node communication.\n\nTherefore, in \\glm we combine another parallel strategy---pipeline model parallelism---with two established practices to form the 3D parallel strategy.\nThe pipeline parallelism divides the model into sequential stages for each parallel group, and to minimize bubbles introduced by pipeline, we refer to Gpipe~\\citep{huang2019gpipe} and PipeDream-Flush~\\citep{narayanan2021memory} implementation from DeepSpeed~\\citep{rasley2020deepspeed} to reduce time and GPU memory wasting (Cf. Appendix~\\ref{app:pipeline}).\n\nWe analyze the bubble ratio in \\glm's pre-training. Given the number of pipeline stages $p$, the number of micro-batches in a parallel group $m$, and the time for forward and backward per micro-batch $t_f$ and $t_b$. \nIn ideal case without pipeline, forward and backward take $t_{\\mathrm{ideal}} = m(t_f + t_b)$. \nBut with pipeline, the splitting causes $p - 1$ forward and backward bubbles respectively for a total time of $t_{\\mathrm{bubble}} = (p - 1)( t_f + t_b)$.\nIf there are $n$ GPUs in a parallel group, in practice we assign each GPU only with one tensor parallel split (group size $t$) from one pipeline parallel stage (altogether $p$ stages), which results in $n=t\\times p$.\nThen we have\n\\begin{equation}\n    \\begin{split}\n        \\text{bubble-ratio} = \\frac {t_{\\mathrm{bubble}}} {t_{\\mathrm{ideal}} + t_{\\mathrm{bubble}}} = \\frac {p - 1} {m + p - 1} = \\frac {n/t - 1} {m + n/t - 1} \n    \\end{split}\n\\end{equation}\n\\noindent where increasing tensor parallel splits $t$ and the number of micro-batches $m$ could reduce the bubble ratios.\nBut $t$ should not exceed the number of GPUs per node and a too large $m$ is known to harm language models' performance~\\citep{you2020large}.\nAfter testing on our 96 DGX-A100 nodes, we adopt $t=4$, $m=176$, and $p=8$, which leads to a bubble ratio of only 3.8\\% and a utilization of 135 TFLOP/s per GPU in pre-training.\n\n\\vvpara{Principles for LLM Configuration.}\nCompared to ordinary-sized language models, configurations of LLMs should be based on the computing platforms to exploit training efficiency.\nIn \\glm, these peculiarly platform-dependent configurations include:\n\\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.2em,topsep=0.0em,partopsep=0.0em]\n    \\item \\textbf{Total size}: one of our commission for inclusivity is that \\glm should be able to inference on a single DGX-A100 (40G) node in FP16 precision. Based on 12,288 hidden state dimension we adopt following~\\citep{brown2020language}, it leads to not more than 130B parameters in our LLM and each parallel group contains not more than 9 layers. \n    \\item \\textbf{Number of layers}: as the pipeline parallel stages in both ends in LLMs store additional word embeddings' parameters, to avoid insufficient memory utilization on the middle stages, they should contain one less layer than others. Given an $l$-layer \\glm stage contains $k$ layers, we find the the most balanced partition being $l=kp - 2$, resulting in $9\\times8 - 2=70$ transformer layers in \\glm.\n\\end{itemize}\n\n\\yuxiao{how about move the configuration to sec 2.2?}\n\n\\begin{insight}\n\\rm Configure your LLMs based on the computing cluster and parallel strategy. Squeezing hardware potential is always a top priority for LLM pre-training out of big companies.\n\\end{insight}\n\n\\vvpara{Computation Graph Optimization.}\nGPU operators, including compute-intensive and memory-intensive, manage the computation of modern neural networks in GPUs. \nIn Transformer, there is another time-consuming access-intensive element-wise operators in large quantities. \nWe may consider operator fusion for them in computational graphs to reduce memory access for acceleration. \nSpecifically, we use the JIT method provided by the PyTorch framework to implement operator fusion on the operator combinations of \\texttt{bias+dropout+add}, \\texttt{bias+gelu}, and the rotary position encoding (\\cite{su2021roformer}, Cf. Section~\\ref{sec:glm-arch}) used in \\glm. \nWe also implement a customized CUDA kernel function from Megatron-LM for the \\texttt{scale+mask+softmax} operation. \n\\yuxiao{possible to have a number to show off the optimization effort?, otherwise, we may have to remove this para or move it to appendix}\n\n}%end of hide %%%%%%%%%%%%%=====================================================================\n\n\\hide{\n\\subsection{mixed precision training strategy}\n\nExisting exascale pre-trained language models are often trained in BF16 half-precision format to save memory and speed up computation \\cite{rae2021scaling, chowdhery2022palm}. BF16 format has the same representation range as single-precision, which can reduce the overflow up and down during the training of exascale language models. However, the efficient computation of BF16 format is only supported by a few high-end gas pedals (e.g., Nvidia's A100, 3090, etc.), and the parameters of models trained in BF16 format cannot be converted to FP16 format due to the difference in representation range, and regressing to single-precision for use will lead to a great waste of time and memory. Therefore, pre-trained language models in BF16 format greatly increase the threshold for researchers to use. Our work is devoted to exploring mixed precision training strategies using FP16 format in the training of very large scale language models.\n\nThe \\ref{sec:mixed-precision-training} section of this paper introduces a number of mixed-precision strategies commonly used in training deep neural networks, including techniques such as preserving single-precision parameter copies and dynamic scaling of the loss function, which have successfully accelerated the training process for a range of models. However, in our experiments, we observed that the direct application of these strategies to the training of very large scale language models at the scale of tens and hundreds of billions in FP16 half-precision format still results in training non-convergence. It is not surprising that the pre-training model OPT \\cite{zhang2022opt} with 100 billion parameters using FP16 precision also experienced multiple divergences during training, which implies that we need to propose targeted mixed-precision strategies for the training of very large scale language models.\n\nEach attention head in the multi-headed attention layer of the Transformer model is computed as follows\n\\begin{equation}\n    \\operatorname{Attention}\\left(\\boldsymbol{Q}_i, \\boldsymbol{K}_i, \\boldsymbol{V}_i\\right) = \\operatorname{softmax}\\left(\\frac{\\ boldsymbol{Q}_i\\boldsymbol{K}_i^{\\top}}{\\sqrt{d}}\\right) \\boldsymbol{V}_i\n\\end{equation}\n\nWe observed that, especially in the case of large sizes of individual attention heads in large-scale language models, the elements in the fraction matrix $\\boldsymbol{Q}_i\\boldsymbol{K}_i^{\\top}\\left/\\sqrt{d}\\right.$ may be much larger than the input and overflow. Subsequent researchers have explored this problem, and Ming Ding et al. found that the fraction matrix inside each head in multihead attention has the property of small variance but large bias when training a multimodal model with billions of parameters, and targeted the PB-Relax method \\cite{ding2021cogview}, which uses the Softmax function translation invariant property eliminates the bias in the fractional matrix, thus alleviating the overflow problem. Its core formula is as follows.\n\\vspace{0.5em}\n\\begin{equation}\n   \\operatorname{softmax}\\left(\\frac{\\boldsymbol{Q}_i\\boldsymbol{K}_i^{\\top}}{\\sqrt{d}}\\right) =\n   \\operatorname{softmax}\\left(\\left(\\frac{\\boldsymbol{Q}_i\\boldsymbol{K}_i^{\\top}}{alpha\\sqrt{d}} - \\max\\left(\\frac{\\boldsymbol{Q}_i\\ boldsymbol{K}_i^{\\top}}{\\alpha\\sqrt{d}}\\right)\\right)\\times\\alpha\\right) \n\\end{equation}\n\nOur experimental results show that the PB-Relax method can solve the training non-convergence problem for models with tens of billions of parameters, however, the training scattering phenomenon still occurs when the number of parameters is expanded by tens and hundreds of billions. Numerically, the PB-Relax method still cannot avoid the overflow phenomenon when the variance within the fraction matrix is relatively large, so we propose a hybrid precision-based calculation method of\n\\vspace{0.5em}\n\\begin{equation}\n   \\operatorname{softmax}\\left(\\frac{\\boldsymbol{Q}_i\\boldsymbol{K}_i^{\\top}}{\\sqrt{d}}\\right) =\n   \\operatorname{FP16}\\left(\\operatorname{softmax}\\left(\\operatorname{FP32}\\left(\\frac{\\boldsymbol{Q}_i\\boldsymbol{K}_i^{\\top}}{\\ alpha\\sqrt{d}}\\right)\\times\\alpha\\right)\\right)\n\\end{equation}\n\nIn this method, the more computationally intensive $\\boldsymbol{Q}_i\\boldsymbol{K}_i^{\\top}\\left/\\sqrt{d}\\right.$ is still computed in half-precision to ensure efficiency, and $\\alpha$ is pre-divided to avoid overflow. the Softmax operation is computed in single precision, and the The softmax operation is computed in single precision, and the single-precision fraction matrix is multiplied back to $\\alpha$ before computation to ensure that the result remains unchanged. $\\alpha$ is a data- and model-dependent hyperparameter, and in our experiments, taking $\\alpha$ equal to the number of layers in the network avoids overflow. In terms of efficiency, the only cost of this mixed-precision strategy is the computation of the single-precision Softmax function, however, we find that matrix multiplication is the absolute dominant model computation in the pre-training of very large models. Compared to computing the Softmax function at half precision, the overall computational efficiency is only about 1\\% loss. We were surprised to find that computing Softmax with half-precision and using the PB-Relax method to mitigate the overflow instead reduced the training efficiency by 8\\%, which may be due to the reduction in efficiency caused by too intensive manipulation of the fractional matrix with large shapes in the multi-headed attention.\n}\n\n\\section{The Training Stability of \\glm}\n\nThe training stability is the decisive factor in \\glm's quality, which is also largely impacted by the number of tokens it passes through~\\citep{hoffmann2022training}. \nThus, given the computing usage constraint, there has to be a trade-off between efficiency and stability with regard to floating-point (FP) formats: \nlow-precision FP formats (e.g., 16-bit precision---FP16) improve computing efficiency but are prone to overflow and underflow errors, resulting in training collapses. \n\n \\begin{wrapfigure}{r}{4.3cm}\n    \\centering\n    \\vspace{-3mm}\n    \\includegraphics[width=\\linewidth]{figures/shrink-small.pdf}\n    \\vspace{-7mm}\n    \\caption{EGS reduces gradient scale and variance to stabilize LLMs' pre-training.}\n    \\label{fig:shrink}\n    \\vspace{-8mm}\n\\end{wrapfigure}\n\n\\vvpara{Mixed-Precision.}\nWe follow the common practice of a mixed-precision~\\citep{micikevicius2018mixed} strategy (Apex O2), i.e., FP16 for forwards and backwards and FP32 for optimizer states and master weights, to reduce the GPU memory usage and improve training efficiency. \nSimilar to OPT-175B and BLOOM-176B (C.f. Figure~\\ref{fig:collapse} in Appendix), the training of \\glm faces frequent loss spikes resulted from this choice, which tends to become increasingly frequent as the training goes on. \nThe precision related spikes are often without clear reasons: % or at least not due to bad data as suggested by PaLM: \nsome recover on their own; others come with a portent of suddenly soaring gradient norm and eventually a spike or even NaN in loss. \nOPT-175B attempted to fix by manually skipping data and adjusting hyper-parameters; BLOOM-176B did so via the embedding norm technique~\\citep{dettmers20218}. %\\yuxiao{xxxxx}\nWe spent months to empirically investigate the spikes and realize that a few issues emerge when transformers scale up: \n\nFirst, the transformer main branch's value scale can be extremely large in deeper layers if using Pre-LN. \nThis is addressed in \\glm by using DeepNorm based Post-LN (Cf.  Section ~\\ref{sec:glm-arch}), which makes the value scale always bounded. \n\nSecond, the attention scores grow so large that they exceed FP16's range, as the model scales up. \nThere are a few options to overcome this issue in LLMs. \nIn CogView~\\citep{ding2021cogview}, PB-Relax is proposed to remove bias terms and deduct extremum value in attention computation to avoid the problem, which unfortunately does not help avoid disconvergence in \\glm. \nIn BLOOM-176B, the BF16 format is used instead of FP16, due to its wide range of values on NVIDIA Ampere GPUs (i.e., A100). \nHowever, BF16 consumes $\\sim$15\\% more run-time GPU memory than FP16 in our experiments due to its conversion to FP32 in gradient accumulation, and more importantly it is not supported on other GPU platforms (e.g., NVIDIA Tesla V100), limiting the accessibility of produced LLMs. \nAnother option from BLOOM-176B is to apply embedding norm with BF16, but in sacrifice of a significant penalty on model performance, as they notice that embedding norm can harm model's zero-shot learning (Cf. Section 4.3 in~\\citep{scao2022what}). %\\yuxiao{ the embedding norm based technique, which  xxxx xxx}\n\n\\vvpara{Embedding Layer Gradient Shrink (EGS).}\nOur empirical search identifies that the gradient norm can serve as an informative indicator of training collapses. %, which was also discovered in vision transformers~\\citep{chen2021empirical} as we later realized. \nSpecifically, we find that \na training collapse usually lags behind a ``spike'' in gradient norm by a few training steps. %\\todo{do we need evidence here, or delete it?}\nSuch spikes are usually caused by the embedding layer's abnormal gradients, as we observe that its gradient norm is often several magnitude larger that those of other layers in \\glm's early stage training (Cf. Figure~\\ref{fig:shrink} (a)). \nIn addition, it tends to fluctuate dramatically in the early training. \nThe problem is handled in vision models~\\citep{chen2021empirical} via freezing the patch projection layer. \nUnfortunately, we cannot freeze the training of the embedding layer in language models.\n\nFinally, we find the gradient shrink on embedding layers could overcome loss spikes and thus stabilize \\glm's training. \nIt is first used in the multi-modal transformer CogView~\\citep{ding2021cogview}. \nLet $\\alpha$ be the shrinking factor, the strategy can be easily implemented via\n  $  \\mathsf{word\\_embedding} = \\mathsf{word\\_embedding} * \\alpha + \\mathsf{word\\_embedding.detach()} * (1 - \\alpha)$. \nFigure~\\ref{fig:shrink} (b) suggests that empirically, setting $\\alpha=0.1$ wipes out most spikes we would have met, with negligible latency. \n\nIn fact, the final \\glm training run only experiences three late-stage loss divergence cases, though it fails numerous times due to hardware failures. \nFor the three unexpected spikes, it turns out further shrinking the embedding gradient can still help stabilize the \\glm training. \nSee the training notes and Tensorboard logs in our code repository\nfor details. \n\n\\hide{%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\\section{Training Strategy}\n\nThe root cause of  is that there is a trade-off between training efficiency and stability with regard to floating-point (FP) formats:\n\\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.2em,topsep=0.0em,partopsep=0.0em]\n\\item \\textbf{Efficiency}: low-precision FP formats (e.g., FP16) reduce memory and computation costs\n\\item \\textbf{Stability}: low-precision FP formats are prone to overflow and underflow, resulting in collapses\n\\end{itemize}\n\n\\subsection{Platform-Aware Parallel Strategies and Model Configurations} \\label{sec:parallel_strategy}\n\n\\glm is trained on a cluster of 96 DGX-A100 GPU (8$\\times$40G) servers with a 60-day access.  \nThe goal is to pass through as many tokens as possible, as a recent study~\\citep{hoffmann2022training} suggests that most existing LLMs are largely under-trained. \n\n\\vvpara{The 3D Parallel Strategy.}\nThe data parallelism~\\citep{valiant1990bridging} and tensor model parallelism~\\citep{shoeybi2019megatron} are the de facto practices for training billion-scale models~\\citep{gpt-j,du2022glm}. \nTo further handle\nthe huge GPU memory requirement and the decrease in overall GPU utilization resulted from applying tensor parallel between nodes---as 40G rather than 80G A100s are used for training \\glm, we combine the pipeline model parallelism with the other two strategies to form a 3D parallel strategy. \n\nThe pipeline parallelism divides the model into sequential stages for each parallel group, and to further minimize bubbles introduced by pipeline, we leverage the PipeDream-Flush~\\citep{narayanan2021memory} implementation from DeepSpeed~\\citep{rasley2020deepspeed} to train \\glm with a relative big global batch size (4,224) to reduce time and GPU memory wasting. \nThrough both numerical and empirical examinations, we adopt 4-way tensor parallelism and 8-way pipeline parallelism (Cf. Appendix~\\ref{app:pipeline} for details). Following the calculation in~\\citep{chowdhery2022palm}, we report hardware FLOPs utilization (HFU) of 43.3\\% and model FLOPs utilization (MFU) of 32.5\\% due to re-materialization.\n\n\\vvpara{\\glm Configurations.}\nWe aim to enable our 100B-scale LLM to run a single DGX-A100 (40G) node in FP16 precision. \nBased on the hidden state dimension of 12,288 we adopt from GPT-3, the resultant model size has to be no more than 130B parameters, thus \\glm.\nTo maximize GPU utilization, we configure the model based on the platform and its corresponding parallel strategy. \nTo avoid insufficient memory utilization in the middle stages\ndue to the additional word embedding at both ends, we balance the pipeline partition by removing one layer from them, making 9$\\times$8-2=70 transformer layers in \\glm.\n\nDuring the 60-day access to the cluster, we manage to train \\glm for 400 billion tokens (roughly 200 billion each for Chinese and English) with a fixed sequence length of 2,048 per sample. \nFor the [gMASK] training objective, we use a context window of 2,048 tokens. \nFor the [MASK] and multi-task objectives, we use a context window of 512 and concatenate four samples together to cater the 2,048-sequence-length. \nWe warm-up the batch size from 192 to 4224 over the first 2.5\\% samples. \nWe use AdamW~\\citep{loshchilov2017decoupled} as our optimizer with $\\beta_1$ and $\\beta_2$ set to 0.9 and 0.95, and a weight decay value of 0.1. \nWe warm up the learning rate from $10^{-7}$ to $8\\times 10^{-5}$ over the first 0.5\\% samples, then decay it by a $10\\times$ cosine schedule. \nWe use a dropout rate of 0.1 and clip gradients using a clipping value of 1.0 (Cf. Table~\\ref{tab:config} for the full configurations).\n\n\\hide{%start of hide %%%%%%%%%%%%%=====================================================================\n\n\\subsection{Parallel Strategies} \\label{sec:parallel_strategy}\nTraining LLMs can be outrageously expensive.\nIt is said that GPT-3~\\citep{brown2020language} was trained over 10,000 NVIDIA V100 GPUs for months at substantial expenses.\nHowever, recent study~\\citep{hoffmann2022training} shows that existing LLMs are largely under-trained; what matters in the first place to LLMs' performance is the number of trained tokens.\nThus for Inclusive LLM, an optimized framework that fully exploits computing potential is never overemphasized.\n\nHere we introduce \\glm's framework-level insights, including the 3D parallel strategy (Cf. Figure~\\ref{fig:3d-parallelism} in Appendix), LLM configuration principles, and computation graph optimization.\nThey in all contribute \\todo{} speeding up to pre-training, making a crucial improvement to \\glm's quality.\n\n\\vvpara{3D Parallel Strategy.}\n\\aohan{I think this section should be downplayed as more details can be found in the appendix}\nParallel strategies, including data parallelism~\\citep{valiant1990bridging} and tensor model parallelism~\\citep{shoeybi2019megatron}, are de facto practices for training billion-scale language models. \nHowever, when models continue to scale up to over 100B, the scaling up of a tensor parallelism group become insufficient.\nOn one hand, as the computation per layer is evenly distributed in a group, a single node's computational granularity would decrease when group sizes grow up, causing a performance drop of matrix multiplication operator \\texttt{GEMM} and a decrease in overall utilization.\nOn the other hand, if group size exceeds certain threshold (e.g., number of GPUs per node), the \\texttt{All-Reduce} operation becomes a bottleneck due to the expensive cross-node communication.\n\nTherefore, in \\glm we combine another parallel strategy---pipeline model parallelism---with two established practices to form the 3D parallel strategy.\nThe pipeline parallelism divides the model into sequential stages for each parallel group, and to minimize bubbles introduced by pipeline, we refer to Gpipe~\\citep{huang2019gpipe} and PipeDream-Flush~\\citep{narayanan2021memory} implementation from DeepSpeed~\\citep{rasley2020deepspeed} to reduce time and GPU memory wasting (Cf. Appendix~\\ref{app:pipeline}).\n\nWe analyze the bubble ratio in \\glm's pre-training. Given the number of pipeline stages $p$, the number of micro-batches in a parallel group $m$, and the time for forward and backward per micro-batch $t_f$ and $t_b$. \nIn ideal case without pipeline, forward and backward take $t_{\\mathrm{ideal}} = m(t_f + t_b)$. \nBut with pipeline, the splitting causes $p - 1$ forward and backward bubbles respectively for a total time of $t_{\\mathrm{bubble}} = (p - 1)( t_f + t_b)$.\nIf there are $n$ GPUs in a parallel group, in practice we assign each GPU only with one tensor parallel split (group size $t$) from one pipeline parallel stage (altogether $p$ stages), which results in $n=t\\times p$.\nThen we have\n\\begin{equation}\n    \\begin{split}\n        \\text{bubble-ratio} = \\frac {t_{\\mathrm{bubble}}} {t_{\\mathrm{ideal}} + t_{\\mathrm{bubble}}} = \\frac {p - 1} {m + p - 1} = \\frac {n/t - 1} {m + n/t - 1} \n    \\end{split}\n\\end{equation}\n\\noindent where increasing tensor parallel splits $t$ and the number of micro-batches $m$ could reduce the bubble ratios.\nBut $t$ should not exceed the number of GPUs per node and a too large $m$ is known to harm language models' performance~\\citep{you2020large}.\nAfter testing on our 96 DGX-A100 nodes, we adopt $t=4$, $m=176$, and $p=8$, which leads to a bubble ratio of only 3.8\\% and a utilization of 135 TFLOP/s per GPU in pre-training.\n\n\\vvpara{Principles for LLM Configuration.}\nCompared to ordinary-sized language models, configurations of LLMs should be based on the computing platforms to exploit training efficiency.\nIn \\glm, these peculiarly platform-dependent configurations include:\n\\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.2em,topsep=0.0em,partopsep=0.0em]\n    \\item \\textbf{Total size}: one of our commission for inclusivity is that \\glm should be able to inference on a single DGX-A100 (40G) node in FP16 precision. Based on 12,288 hidden state dimension we adopt following~\\citep{brown2020language}, it leads to not more than 130B parameters in our LLM and each parallel group contains not more than 9 layers. \n    \\item \\textbf{Number of layers}: as the pipeline parallel stages in both ends in LLMs store additional word embeddings' parameters, to avoid insufficient memory utilization on the middle stages, they should contain one less layer than others. Given an $l$-layer \\glm stage contains $k$ layers, we find the the most balanced partition being $l=kp - 2$, resulting in $9\\times8 - 2=70$ transformer layers in \\glm.\n\\end{itemize}\n\n\\yuxiao{how about move the configuration to sec 2.2?}\n\n\\begin{insight}\n\\rm Configure your LLMs based on the computing cluster and parallel strategy. Squeezing hardware potential is always a top priority for LLM pre-training out of big companies.\n\\end{insight}\n\n\\vvpara{Computation Graph Optimization.}\nGPU operators, including compute-intensive and memory-intensive, manage the computation of modern neural networks in GPUs. \nIn Transformer, there is another time-consuming access-intensive element-wise operators in large quantities. \nWe may consider operator fusion for them in computational graphs to reduce memory access for acceleration. \nSpecifically, we use the JIT method provided by the PyTorch framework to implement operator fusion on the operator combinations of \\texttt{bias+dropout+add}, \\texttt{bias+gelu}, and the rotary position encoding (\\cite{su2021roformer}, Cf. Section~\\ref{sec:glm-arch}) used in \\glm. \nWe also implement a customized CUDA kernel function from Megatron-LM for the \\texttt{scale+mask+softmax} operation. \n\\yuxiao{possible to have a number to show off the optimization effort?, otherwise, we may have to remove this para or move it to appendix}\n\n}%end of hide %%%%%%%%%%%%%=====================================================================\n\n\\hide{\n\\subsection{mixed precision training strategy}\n\nExisting exascale pre-trained language models are often trained in BF16 half-precision format to save memory and speed up computation \\cite{rae2021scaling, chowdhery2022palm}. BF16 format has the same representation range as single-precision, which can reduce the overflow up and down during the training of exascale language models. However, the efficient computation of BF16 format is only supported by a few high-end gas pedals (e.g., Nvidia's A100, 3090, etc.), and the parameters of models trained in BF16 format cannot be converted to FP16 format due to the difference in representation range, and regressing to single-precision for use will lead to a great waste of time and memory. Therefore, pre-trained language models in BF16 format greatly increase the threshold for researchers to use. Our work is devoted to exploring mixed precision training strategies using FP16 format in the training of very large scale language models.\n\nThe \\ref{sec:mixed-precision-training} section of this paper introduces a number of mixed-precision strategies commonly used in training deep neural networks, including techniques such as preserving single-precision parameter copies and dynamic scaling of the loss function, which have successfully accelerated the training process for a range of models. However, in our experiments, we observed that the direct application of these strategies to the training of very large scale language models at the scale of tens and hundreds of billions in FP16 half-precision format still results in training non-convergence. It is not surprising that the pre-training model OPT \\cite{zhang2022opt} with 100 billion parameters using FP16 precision also experienced multiple divergences during training, which implies that we need to propose targeted mixed-precision strategies for the training of very large scale language models.\n\nEach attention head in the multi-headed attention layer of the Transformer model is computed as follows\n\\begin{equation}\n    \\operatorname{Attention}\\left(\\boldsymbol{Q}_i, \\boldsymbol{K}_i, \\boldsymbol{V}_i\\right) = \\operatorname{softmax}\\left(\\frac{\\ boldsymbol{Q}_i\\boldsymbol{K}_i^{\\top}}{\\sqrt{d}}\\right) \\boldsymbol{V}_i\n\\end{equation}\n\nWe observed that, especially in the case of large sizes of individual attention heads in large-scale language models, the elements in the fraction matrix $\\boldsymbol{Q}_i\\boldsymbol{K}_i^{\\top}\\left/\\sqrt{d}\\right.$ may be much larger than the input and overflow. Subsequent researchers have explored this problem, and Ming Ding et al. found that the fraction matrix inside each head in multihead attention has the property of small variance but large bias when training a multimodal model with billions of parameters, and targeted the PB-Relax method \\cite{ding2021cogview}, which uses the Softmax function translation invariant property eliminates the bias in the fractional matrix, thus alleviating the overflow problem. Its core formula is as follows.\n\\vspace{0.5em}\n\\begin{equation}\n   \\operatorname{softmax}\\left(\\frac{\\boldsymbol{Q}_i\\boldsymbol{K}_i^{\\top}}{\\sqrt{d}}\\right) =\n   \\operatorname{softmax}\\left(\\left(\\frac{\\boldsymbol{Q}_i\\boldsymbol{K}_i^{\\top}}{alpha\\sqrt{d}} - \\max\\left(\\frac{\\boldsymbol{Q}_i\\ boldsymbol{K}_i^{\\top}}{\\alpha\\sqrt{d}}\\right)\\right)\\times\\alpha\\right) \n\\end{equation}\n\nOur experimental results show that the PB-Relax method can solve the training non-convergence problem for models with tens of billions of parameters, however, the training scattering phenomenon still occurs when the number of parameters is expanded by tens and hundreds of billions. Numerically, the PB-Relax method still cannot avoid the overflow phenomenon when the variance within the fraction matrix is relatively large, so we propose a hybrid precision-based calculation method of\n\\vspace{0.5em}\n\\begin{equation}\n   \\operatorname{softmax}\\left(\\frac{\\boldsymbol{Q}_i\\boldsymbol{K}_i^{\\top}}{\\sqrt{d}}\\right) =\n   \\operatorname{FP16}\\left(\\operatorname{softmax}\\left(\\operatorname{FP32}\\left(\\frac{\\boldsymbol{Q}_i\\boldsymbol{K}_i^{\\top}}{\\ alpha\\sqrt{d}}\\right)\\times\\alpha\\right)\\right)\n\\end{equation}\n\nIn this method, the more computationally intensive $\\boldsymbol{Q}_i\\boldsymbol{K}_i^{\\top}\\left/\\sqrt{d}\\right.$ is still computed in half-precision to ensure efficiency, and $\\alpha$ is pre-divided to avoid overflow. the Softmax operation is computed in single precision, and the The softmax operation is computed in single precision, and the single-precision fraction matrix is multiplied back to $\\alpha$ before computation to ensure that the result remains unchanged. $\\alpha$ is a data- and model-dependent hyperparameter, and in our experiments, taking $\\alpha$ equal to the number of layers in the network avoids overflow. In terms of efficiency, the only cost of this mixed-precision strategy is the computation of the single-precision Softmax function, however, we find that matrix multiplication is the absolute dominant model computation in the pre-training of very large models. Compared to computing the Softmax function at half precision, the overall computational efficiency is only about 1\\% loss. We were surprised to find that computing Softmax with half-precision and using the PB-Relax method to mitigate the overflow instead reduced the training efficiency by 8\\%, which may be due to the reduction in efficiency caused by too intensive manipulation of the fractional matrix with large shapes in the multi-headed attention.\n}\n\\subsection{Stability: Systematic and Numerical Challenges}\n\n Modern computing devices such as NVIDIA A100s and TPUs support 16-bit precision computation with much higher throughput compared to 32-bit precision. \nTherefore, it has become a de facto practice to train language models with mixed-precision~\\citep{micikevicius2018mixed} to reduce GPU memory usage and improve computation efficiency without hurting performance. \nHowever, as far as we have observed, LLMs are far more vulnerable to loss spikes in the mixed-precision setup than smaller ones, which is well-aligned with other LLMs' observations (C.f. Figure~\\ref{fig:collapse} in Appendix) such as OPT-175B~\\citep{zhang2022opt} and BLOOM-176B~\\citep{scao2022what}.\n\nHandling these unexpected spikes has been the most challenging part in \\glm's training. \nWe empirically ascribe them into structural and numerical instability:\n\n\\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.2em,topsep=0.0em,partopsep=0.0em]\n\\item \\textbf{Systematic Instability}:\nwhich tends to appear in the early stage of training. \nIts appearance is often accompanied with a gradual but steady rise in gradient norm. \nSkipping data or adjusting hyper-parameters do not eliminate, but only make it happen at other random neighboring steps. \n\n\\item \\textbf{Numerical Instability}:\nwhich instead tends to occur in the late stage of training and becomes increasingly frequent as the training goes on. \nIts resultant spikes are often without clear reasons: some recover on their own; others come with a portent of suddenly soaring gradient norm and eventually a spike or even NaN in loss, as the model has been optimized by the abnormal gradients. \n\\end{itemize}\n\nWe refer to the first type of instability as \\emph{systematic}, since regardless of its optimization configuration, a model just cannot converge ideally owing to its current setup (i.e., model architectures and mixed-precision training strategies). \nTo eliminate consequent disconvergence, a more stable model architecture and proper mixed-precision strategies are necessary.\n\nThe second type of instability is referred to as \\emph{numerical}, since we believe it is likely to derive from some unknown numerical reasons in 16-bit precision training.\nIts related spikes can be mitigated via re-loading the model from a checkpoint 100 to 200 iterations ahead with data skipping, or changing hyper-parameters such as the learning rate.\nSuch instability may not arise from bad data samples as is previously assumed, because changing the learning rate also allows the model to go through them.\nAuthors of~\\citep{chowdhery2022palm} also report that no bad patterns have been observed from data causing training spikes in their examination.\nThus, numerical spikes are likely to occur only when a certain model goes through certain data, and can be avoided by applying some perturbation to the training process.\n\nDue to the large cost of LLM training, conquering these spikes are crucial: it would be a tragedy if the training diverged on the half road and turned out unrecoverable. \nWe conduct plenitude of experiments spanning few months to find several effective strategies to stabilize \\glm's training.\n\n\\begin{insight}\n\\rm It is both systematic and numerical instabilities that LLMs suffer from in training, which takes us months to identify and conquer from scratch in \\glm.\n\\end{insight}\n\n\\vvpara{Mixed-precision strategy.} \nMixed-precision~\\citep{micikevicius2018mixed} training with FP16 has become a default in mainstream frameworks such as DeepSpeed~\\citep{rasley2020deepspeed}.\nA common mixed-precision training strategy (Apex O2) is as follows: forward and backward are calculated in pure 16-bit precision (FP16), but the optimizer states and master weights are FP32 are used in the optimization.\nHowever, such a strategy is prone to meet severe numerical instability in a conventional transformer architecture when a model scales up, due to its two main bottlenecks:\n\\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.2em,topsep=0.0em,partopsep=0.0em]\n    \\item Under Pre-LN, the transformer main branch's value scale can be extremely large in deeper layers.\n    \\item As a model scales up, attention scores grow so large that they exceed FP16's representation scope.\n\\end{itemize}\n\nFor the first bottleneck, our use of DeepNorm (a variant of Post-LN) servers as LayerNorm in the main branch, so the value scale is always bounded. \nFor the second bottleneck, CogView~\\citep{ding2021cogview} proposes PB-Relax to remove bias terms in attention's linear projection and deduct the largest attention score from each attention score matrix to avoid the problem. \nNevertheless, the technique does not eliminate disconvergence in \\glm, probably due to its gigantic size.\n \nAs a remedy for fp16's narrow representation range, NVIDIA Ampere GPUs provide BF16 floating-point format (adopted by BLOOM 176B) to mitigate the problem. \nNevertheless, BF16's dissupport on other computing platforms (e.g., NVIDIA Tesla V100) has significantly hampered its wider applications.\nIn practice, we also notice that compared to FP16, BF16 requires around 15\\% additional run-time GPU memory due to its necessary conversion to FP32 in gradient accumulation.\n\nTo support as many researchers and developers as possible, \\glm thus chooses FP16 as its training floating-point format. \nOn the other hand, it means our model is faced with more stability challenges, as is also reported in another FP16 LLM OPT-175B. \nFortunately, after great struggle, we successfully get it through via a set new techniques derived from some following insights.\n\n\\begin{insight}\n\\rm FP16 is a challenging but rewarding decision: it suffers from more instability, but is a must for enabling LLMs to train and inference on inclusive ranges of platforms.\n\\end{insight}\n\n\\vvpara{Gradient Shrink on the Embedding Layer.}\nBesides explicit precision issues caused by attention scores, there are implicit problems arouses by random noisy samples appearing along the training.\nFor example, in our observation these samples might consist bunches of semantically plausible repeating lines, which can be hardly cleaned in rule-guided preprocesses.\nWe find LLMs are so vulnerable to these noises, which likely results in unrecoverable unexpected training collapses.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/shrink.pdf}\n    \\vspace{-6mm}\n    \\caption{Gradient shrink on embedding layer stabilizes \\glm's training. (a) Preliminary experiments on GLM-base show that gradient norm for embedding layer can be much larger; (b) 40B-scale testing shows that embedding gradient shrink is necessary for LLM training.}\n    \\label{fig:shrink}\n    \\vspace{-6mm}\n\\end{figure}\n\nFor such instability, we notice that the gradient norm serves as an effective indicator, which accords with observations from vision transformers~\\citep{chen2021empirical,ding2021cogview}: the happening of a training collapse usually lags behind a ``spike'' in gradient norm by few training steps. %\\todo{do we need evidence here, or delete it?}\nSuch spikes are usually caused by embedding layer's abnormal gradient, as we observe a remarkably larger gradient norm from it than other layers---often several magnitudes larger---in \\glm's early stage training (Cf. Figure~\\ref{fig:shrink} (a)).\nIt also tends to fluctuate dramatically in the early stage training of GLM compared to other layers.\nThe issue is handled in~\\citep{chen2021empirical} via freezing the patch projection layer; however, a language model cannot freeze the training of its embedding layer.\n\nAlternatively, we find the strategy of gradient shrink over embedding layer can overcome the problem in LLM's training, which is similarly applied in text-to-image transformer CogView~\\citep{ding2021cogview}.\nLet $\\alpha$ be the shrinking factor, the strategy can be easily implemented via\n\\begin{equation}\n    \\mathsf{word\\_embedding} = \\mathsf{word\\_embedding} * \\alpha + \\mathsf{word\\_embedding.detach()} * (1 - \\alpha)\n\\end{equation}\nSpecifically, we shrink to $\\alpha=0.1$ and discover that it wipes out most spikes we would have met, with negligible speed loss (Cf. Figure~\\ref{fig:shrink} (b)).\nActually, in our late stage training, we observe that further shrinking embedding gradient still works when unexpected spikes take place.\n\n\\begin{insight}\n\\rm Shrinking embedding layer's gradient to its 0.1 can solve most instability problems.\n\\end{insight}\n\nWe train \\glm for 400 billion tokens (roughly 200 billion each for Chinese and English) with a fixed sequence length of 2,048 per sample. \nFor the [gMASK] training objective, we use a context window of 2,048 tokens. \nFor the [MASK] and multi-task training objectives, we use context windows of length 512 and concatenate four samples together to cater the 2,048-sequence-length. \nWe warm-up the batch size from 192 to 4224 over the first 2.5\\% samples. \nWe use AdamW~\\citep{loshchilov2017decoupled} as our optimizer with $\\beta_1$ and $\\beta_2$ set to 0.9 and 0.95, and a weight decay value of 0.1. \nWe warm-up the learning rate from $10^{-7}$ to $8\\times 10^{-5}$ over the first 0.5\\% samples, then decay it by a $10\\times$ cosine schedule. \nWe use a dropout rate of 0.1 and clip gradients using a clipping value of 1.0.\n\n\\glm's pre-training lasts 60 days and it takes up 96 NVIDIA DGX-A100 (40G) nodes with 400G bandwidth IB network, which would cost equivalently 4.9 million dollars based on the GPU pricing on public cloud services in the same period.\nOn top of the basic training setup, during the long training we have experienced several crises and failures.\nWe successfully managed to recover the training via slight adjustments over some of configuration above (mostly about learning rate and shrinking factor), which are reported in our released training notes and Tensorboard logs\\footnote{\\url{https://github.com/THUDM/GLM-130B/tree/main/logs}}.\n\n}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \\clearpage\n\\section{\\glm Inference on RTX 2080 Ti}\n\nOne of the major goals of \\glm is to lower the hardware requirements for accessing 100B-scale LLMs without efficiency and effectiveness disadvantages. \n\nAs mentioned, the model size of 130B is determined for running the full \\glm model on a single A100 (40G$\\times$8) server, rather than the high-end A100 (80G$\\times$8) machine required by OPT-175B and BLOOM-176B. \nTo accelerate \\glm inference, we also leverage FasterTransformer~\\citep{timonin2022accelerated}\nto implement \\glm in C++.  \nCompared to the PyTorch implementation of BLOOM-176B in Huggingface, \\glm's decoding inference is 7-8.4$\\times$ faster on the same single A100 server. (Cf. Appendix~\\ref{app:inference_acceleration} for details).\n\n\\vvpara{INT4 Quantization for RTX 3090s/2080s.}\nTo further support popularized GPUs,\nwe attempt to compress \\glm as much as possible while maintaining performance superiority, particularly via  quantization~\\citep{zafrir2019q8bert,shen2020q,tao2022compression}, which introduces little task-agnostic performance drops for generative language models. \n\n\\begin{wrapfigure}{r}{6cm}\n    \\small\n    \\vspace{-6mm}\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{figures/quantization-scaling-law.png}\n    \\vspace{-4mm}\n    \\caption{(Left) \\texttt{attn-dense} and \\texttt{w2}'s weight distributions; (Right) \\glm's INT4 weight quantization scaling law.}\n    \\label{fig:scaling_law}\n    \\vspace{-5mm}\n\\end{wrapfigure}\n\nTypically, the practice is to quantize both model weights and activations to INT8.\nHowever, our analysis in Appendix~\\ref{app:activation_analysis} suggests that LLMs' activations may contain extreme outliers. \nConcurrently, the emergent outliers in OPT-175B and BLOOM-176B are also discovered~\\citep{dettmers2022llm}, which influence only about 0.1\\% feature dimensions and are thus solved by matrix multiplication decomposition for the outlying dimensions.\nDifferently, there exist about 30\\% outliers in GLM-130B's activations, making the technique above far less efficient. % due to the slow high-precision computation.\nThus, we decide to focus on the quantization of model weights (i.e., mostly linear layers) while keeping the FP16 precision for activations.  \nThe quantized model is dynamically converted to FP16 precision at runtime, introducing a small computational overhead but greatly reducing the GPU memory usage for storing model weights. \n\nExcitingly, we manage to reach the INT4 weight quantization for \\glm while existing successes have thus far only come to the INT8. \nMemory-wise, by comparing to INT8, the INT4 version helps additionally save half of the required GPU memory to 70GB, thus allowing \\glm inference on 4 $\\times$ RTX 3090 Ti (24G) or 8 $\\times$ RTX 2080 Ti (11G). \nPerformance-wise, Table~\\ref{tab:quantization} left indicates that without post-training at all, the INT4-version \\glm experiences almost no performance degradation, thus maintaining the performance advantages over GPT-3 on common benchmarks. \n\n\\vvpara{GLM's INT4 Weight Quantization Scaling Law.}\nWe examine the underlying mechanism of this unique INT4 weight quantization scaling law exhibited in Figure~\\ref{fig:scaling_law} right. %, while it does not apply to GPT-based BLOOM-176B.  \nWe plot the weight value distributions in  Figure~\\ref{fig:scaling_law} left, which turns out to directly impact the quantization quality.\nSpecifically, a wider-distributed linear layer needs to be quantized with larger bins, leading to more precision loss.\nThus the wide-distributed \\texttt{attn-dense} and \\texttt{w2} matrices explain the INT4 quantization failure for GPT-style BLOOM.\nConversely, GLMs tend to have much narrower distributions than those of similar-sized GPTs, and the gap between INT4 and FP16 versions keeps further decreasing as the GLM model size scales up (Cf.  Figure~\\ref{fig:quantization_appendix} in Appendix for details).\n\n\\begin{table*}[t]\n\\footnotesize\n\\centering\n\\caption{Left: Quantized \\glm's performance on several benchmarks; Right: INT4 quantized \\glm's inference speed (encode and decode) with FasterTransformer.}\n\\vspace{-2mm}\n\\begin{subtable}\n    \\centering\n    \\begin{threeparttable}\n    \\centering\n    \\renewcommand\\tabcolsep{3pt}\n    \\renewcommand\\arraystretch{0.75}\n    \\begin{tabular}{@{}llccc@{}}\n    \\toprule[1.2pt]\n    \\multirow{2}{*}{Model Precision} & \\multicolumn{3}{c}{\\glm} & GPT-3 \\\\ \\cmidrule(l){2-4} \\cmidrule(l){5-5} \n                                     & FP16   & INT8   & INT4   & FP16  \\\\ \\midrule\n    MMLU (acc, $\\uparrow$)           & 44.75  & 44.71  & 44.80  & 43.9  \\\\\n    LAMBADA (acc, $\\uparrow$)        & 80.21  & 80.21  & 79.47  & 76.2  \\\\\n    Pile (a part, BPB, $\\downarrow$) & 0.634  & 0.638  & 0.641  & 0.74  \\\\ \\bottomrule[1.2pt] %PTB ($\\downarrow$)\n    \\end{tabular}\n    \\end{threeparttable}\n\\end{subtable}%\n\\hspace{.02\\linewidth}%\n\\begin{subtable}\n    \\centering\n    \\begin{threeparttable}\n    \\centering\n    \\renewcommand\\tabcolsep{2pt}\n    \\renewcommand\\arraystretch{0.95}\n    \\begin{tabular}{@{}lcccc@{}}\n    \\toprule[1.2pt]\n    GPU Type                     & \\multicolumn{2}{c}{128 Enc./Dec.} & \\multicolumn{2}{c}{512 Enc./Dec,} \\\\ \\midrule\n    8 $\\times$ A100 (40G)        & 0.15s           & 4.29s           & 0.18s           & 17.7s           \\\\\n    8 $\\times$ V100 (32G)        & 0.31s           & 6.97s           & 0.67s           & 28.1s           \\\\\n    4 $\\times$ RTX 3090 (24G)    & 0.37s           & 8.16s           & 1.30s           & 32.3s           \\\\\n    8 $\\times$ RTX 2080 Ti (11G) & 0.39s           & 6.77s           & 1.04s           & 27.3s           \\\\ \\bottomrule[1.2pt]\n    \\end{tabular}\n    \\end{threeparttable}\n    \\end{subtable}\n    \\vspace{-4mm}\n\\label{tab:quantization}\n\\end{table*}\n\n\\hide{%%%%%%%% hide start %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n\\section{Inference for Inclusivity}\nIn terms of LLM inclusivity, we think its scope should not be limited to accessibility, but also usability.\nIt is particularly a fact that, even with released LLM checkpoints such as OPT-175B~\\citep{zhang2022opt} and BLOOM-176B~\\citep{scao2022what}, individuals and academic researchers can hardly afford the cost for inference, which have been reported to depend on an 8 $\\times$ A100 (80G) server.\nThus, a considerable amount of sweat we spend in \\glm has been on supporting its high usability and fast inference speed on popularized GPUs, such as NVIDIA RTX 3090 Ti (24G).\n\nInterestingly, such efforts actually deeply associate with our \\textit{algorithm designs}, rather than mere engineering undertakings.\nWe will show that GLM series presents \\textit{a unique scaling law to allow INT4 quantization}, while other existing public LLMs (OPT and BLOOM series) would fail.\n\n\\vvpara{Inference Acceleration.}\nA model's plain PyTorch implementation is easy to read and run; but it can be intolerably slow for LLMs. \nBased on NVIDIA's FasterTransformer\\footnote{\\url{https://github.com/NVIDIA/FasterTransformer}} we spend two months \\aohan{no need to mention?} implementing \\glm into C++ to speed up its inference (Cf. Appendix~\\ref{app:inference_acceleration} for details).\nCompared to a naive implementation (e.g., BLOOM-176B by Huggingface Transformers), \\glm's decoding inference on the same-length sequences can be up to $\\times$8.4 faster.\n\n\\vvpara{GLM's INT4 Weight Quantization Scaling Law.}\nNotwithstanding \\glm's accelerated inference speed, its demand for at least a DGX-A100 (40G) server can still discourage individual users.\nIn light of the situation, we endeavor to compress \\glm to fit into 4 $\\times$ RTX 3090 Ti (24G)'s capacity, particularly via the quantization~\\citep{zafrir2019q8bert,shen2020q,tao2022compression}, which presents little task-agnostic performance drop for generative language models.\n\n\\begin{table*}[t]\n\\footnotesize\n\\centering\n\\caption{Left: Quantized \\glm's performance on several benchmarks; Right: INT4 quantized \\glm's inference speed (encode and decode) with FasterTransformer.}\n\\vspace{-2mm}\n\\begin{subtable}\n    \\centering\n    \\begin{threeparttable}\n    \\centering\n    \\renewcommand\\tabcolsep{3pt}\n    \\renewcommand\\arraystretch{0.75}\n    \\begin{tabular}{@{}llccc@{}}\n    \\toprule[1.2pt]\n    \\multirow{2}{*}{Model Precision} & \\multicolumn{3}{c}{\\glm} & GPT-3 \\\\ \\cmidrule(l){2-4} \\cmidrule(l){5-5} \n                                     & FP16   & INT8   & INT4   & FP16  \\\\ \\midrule\n    MMLU (acc, $\\uparrow$)           & 44.75  & 44.71  & 44.80  & 43.9  \\\\\n    LAMBADA (acc, $\\uparrow$)        & 80.21  & 80.21  & 79.47  & 76.2  \\\\\n    Pile (a part, BPB, $\\downarrow$) & 0.634  & 0.638  & 0.641  & 0.74  \\\\ \\bottomrule[1.2pt] %PTB ($\\downarrow$)\n    \\end{tabular}\n    \\end{threeparttable}\n\\end{subtable}%\n\\hspace{.02\\linewidth}%\n\\begin{subtable}\n    \\centering\n    \\begin{threeparttable}\n    \\centering\n    \\renewcommand\\tabcolsep{2pt}\n    \\renewcommand\\arraystretch{0.95}\n    \\begin{tabular}{@{}lcccc@{}}\n    \\toprule[1.2pt]\n    GPU Type                     & \\multicolumn{2}{c}{128 Enc./Dec.} & \\multicolumn{2}{c}{512 Enc./Dec,} \\\\ \\midrule\n    8 $\\times$ A100 (40G)        & 0.15s           & 4.29s           & 0.18s           & 17.7s           \\\\\n    8 $\\times$ V100 (32G)        & 0.31s           & 6.97s           & 0.67s           & 28.1s           \\\\\n    4 $\\times$ RTX 3090 (24G)    & 0.37s           & 8.16s           & 1.30s           & 32.3s           \\\\\n    8 $\\times$ RTX 2080 Ti (11G) & 0.39s           & 6.77s           & 1.04s           & 27.3s           \\\\ \\bottomrule[1.2pt]\n    \\end{tabular}\n    \\end{threeparttable}\n    \\end{subtable}\n    \\vspace{-4mm}\n\\label{tab:quantization}\n\\end{table*}\n\\begin{wrapfigure}{r}{4.5cm}\n    \\small\n    \\vspace{-5mm}\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{figures/quantization_scaling_law.pdf}\n    \\vspace{-4mm}\n    \\caption{\\glm's INT4 weight quantization scaling law.}\n    \\label{fig:scaling_law}\n    \\vspace{-8mm}\n\\end{wrapfigure}\n\nTypical methods quantize both model weights and activation to INT8.%, enabling the INT8 matrix multiplication kernel for efficiency. \nBut in our analysis, we find LLMs' activations may contain extreme outliers (Cf. Appendix~\\ref{app:activation_analysis}).\nConcurrent to our efforts, \\citep{dettmers2022llm} also discovers the emergent outliers in OPT-175B and BLOOM-176B, which influence only about 0.1\\% feature dimensions and are thus solved by matrix multiplication decomposition for outlying dimensions.\nHowever, we find substantial outliers (30\\%) in GLM-130B's activations, making it far less efficient to apply due to the slow high-precision computation.\n\nAfter a few weeks of trial, we finally decide to keep activations' FP16 precision and only consider the symmetric quantization of model weights (majorly consist of linear layers). \nThe quantized model is dynamically converted to FP16 precision at runtime, introducing a small computational overhead but greatly reducing GPU memory usage for storing model weights.\n\nSurprisingly we notice \\glm's unusual talent for INT4 weight quantization, which saves half of required GPU memory to 70GB compared to INT8 and thus allows \\glm's inference on even 4 $\\times$ RTX 3090 Ti (24G) or 8 $\\times$ RTX 2080 Ti (11G).\nMore surprising, it requires no post-training at all and shows little quality degradation (Cf. Table~\\ref{tab:quantization}).\nHowever, such a nice property does not apply to BLOOM-176B and OPT series models (Cf. Figure~\\ref{fig:scaling_law}).\nTo understand it deeper, we dive into LLMs' weight value distribution (Cf. Figure~\\ref{fig:weight_dist},~\\ref{fig:quantization_appendix}), which directly impact the quantization quality.\nWe find language models' skewed \\texttt{w2} matrix is the major cause for INT4 quantization failure, especially for GPTs.\nOn the contrary, GLMs usually present more symmetrical \\texttt{w2} value distributions than similar-sized GPTs, and it seems to keep improving as they scale up.\nWe summarize the discovery as \\textbf{GLM's Scaling Law for INT4 Weight Quantization}, which demonstrates GLM's unique advantage to be quantized into INT4 precision as the architecture scales up.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/quantization.pdf}\n    \\caption{Weight value distribution of \\texttt{w2} from BLOOM-176B and \\glm's first-8 layers' FFN (or GLU). A more skewed distribution with outliers is likely to suffer from more quantization loss.}\n    \\label{fig:weight_dist}\n    \\vspace{-5mm}\n\\end{figure}\n\n\\section{Inference for Inclusivity}\nIn terms of LLM inclusivity, we think its scope should not be limited to accessibility, but also usability.\nIt is particularly a fact that, even with released LLM checkpoints such as OPT-175B~\\citep{zhang2022opt} and BLOOM-176B~\\citep{scao2022what}, individuals and academic researchers can hardly afford the cost for inference, which have been reported to depend on an 8 $\\times$ A100 (80G) server.\nThus, a considerable amount of sweat we spend in \\glm has been on supporting its high usability and fast inference speed on popularized GPUs, such as NVIDIA RTX 3090 Ti (24G).\n\nInterestingly, such efforts actually deeply associate with our \\textit{algorithm designs}, rather than mere engineering undertakings.\nWe will show in the following part that \\glm is the only LLM so far being able to be quantized into INT4 precision, while other existing public LLMs (OPT-175B, BLOOM-176B, and even those 10B-scale smaller language models) would fail.\n\n\\subsection{Inference Acceleration.}\n\\du{This section can be mostly moved to appendix, leaving only the resutls.}\nA model's plain Pytorch implementation is easy to read and run; but it can be intolerably slow for LLMs. \nBased on NVIDIA's FasterTransformer\\footnote{\\url{https://github.com/NVIDIA/FasterTransformer}} we spend two months implementing \\glm into C++ to speed up inference, including the following main optimizations: \n\\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.2em,topsep=0.0em,partopsep=0.0em]\n    \\item Optimize time-costing operations such as GeGLU, Layer Normalization and SoftMax.\n    \\item Reduce number of GPU kernel calls (e.g., fuse MultiheadAttention into one computation kernel).\n    \\item Specify the algorithm of the best performance when calling cuBLAS.\n    \\item Improve the computing efficiency by transposing the model parameters in advance.\n    \\item Use half2 in FP16 computation to double the half's access bandwidth and computing throughput.\n\\end{itemize}\n\nWe currently pack up the full FasterTransformer implementation for \\glm into a plug-and-play docker image for users' convenience, and we are still working on adapting it to our Pytorch implementation\nby only changing one line of code.\nA comparison between our speeding up \\glm implementation and the so far default available BLOOM-176B implementation in Huggingface Transformers\\footnote{\\url{https://huggingface.co/docs/transformers/model_doc/bloom}} is shown in Table~\\ref{tab:fastertransformer}.\nOur implementation for \\glm can be 7.0 to 8.4 times faster than BLOOM-176B's Pytorch implementation.\nThe exertion to accelerate LLM for tolerable response speed could be extremely crucial to its popularization.\n\n\\begin{table*}[t]\n\\footnotesize\n\\centering\n\\caption{Left: Quantized \\glm's performance on several benchmarks; Right: INT4 quantized \\glm's inference speed (encode and decode) with FasterTransformer.}\n\\vspace{-2mm}\n\\begin{subtable}\n    \\centering\n    \\begin{threeparttable}\n    \\centering\n    \\renewcommand\\tabcolsep{3pt}\n    \\renewcommand\\arraystretch{0.75}\n    \\begin{tabular}{@{}llccc@{}}\n    \\toprule[1.2pt]\n    \\multirow{2}{*}{Model Precision} & \\multicolumn{3}{c}{\\glm} & GPT-3 \\\\ \\cmidrule(l){2-4} \\cmidrule(l){5-5} \n                                     & FP16   & INT8   & INT4   & FP16  \\\\ \\midrule\n    MMLU (acc, $\\uparrow$)           & 44.75  & 44.71  & 44.80  & 43.9  \\\\\n    LAMBADA (acc, $\\uparrow$)        & 80.21  & 80.21  & 79.47  & 76.2  \\\\\n    Pile (a part, BPB, $\\downarrow$) & 0.634  & 0.638  & 0.641  & 0.74  \\\\ \\bottomrule[1.2pt] %PTB ($\\downarrow$)\n    \\end{tabular}\n    \\end{threeparttable}\n\\end{subtable}%\n\\hspace{.02\\linewidth}%\n\\begin{subtable}\n    \\centering\n    \\begin{threeparttable}\n    \\centering\n    \\renewcommand\\tabcolsep{2pt}\n    \\renewcommand\\arraystretch{0.95}\n    \\begin{tabular}{@{}lcccc@{}}\n    \\toprule[1.2pt]\n    GPU Type                     & \\multicolumn{2}{c}{128 Enc./Dec.} & \\multicolumn{2}{c}{512 Enc./Dec,} \\\\ \\midrule\n    8 $\\times$ A100 (40G)        & 0.15s           & 4.29s           & 0.18s           & 17.7s           \\\\\n    8 $\\times$ V100 (32G)        & 0.31s           & 6.97s           & 0.67s           & 28.1s           \\\\\n    4 $\\times$ RTX 3090 (24G)    & 0.37s           & 8.16s           & 1.30s           & 32.3s           \\\\\n    8 $\\times$ RTX 2080 Ti (11G) & 0.39s           & 6.77s           & 1.04s           & 27.3s           \\\\ \\bottomrule[1.2pt]\n    \\end{tabular}\n    \\end{threeparttable}\n    \\end{subtable}\n    \\vspace{-4mm}\n\\label{tab:quantization}\n\\end{table*}\n\\subsection{Compression via Quantization}\nWe have improved \\glm's inference speed substantially, but its demand for at least a DGX-A100 (40G) server can still discourage individual developers and researchers.\nIn light of the situation, we endeavor to compress \\glm to fit into 4 $\\times$ RTX 3090 Ti (24G)'s capacity, particularly via the quantization~\\citep{zafrir2019q8bert,shen2020q,tao2022compression}, which presents little task-agnostic performance drop for generative language models.\n\n\\vvpara{INT4 Weight Quantization.}\nTypical methods quantize both model weights and activation to INT8, enabling the INT8 matrix multiplication kernel for efficiency. \nHowever, we find substantial outliers in GLM-130B's activations, making it hard for precision reduction. %to reduce the precision of activations. \nConcurrent to our efforts and observations in \\glm, researchers~\\citep{dettmers2022llm} also discover the issue of emergent outliers in LLMs. \nIn their in-depth analysis on OPT-175B and BLOOM-176B, they find the outliers make up only about 0.1\\% of all feature dimensions.\nTherefore, they propose to decompose the matrix multiplication for high-precision multiplication in outlying dimensions.\n\nBut our detailed analysis demonstrates that the outliers in GLM-130B can sometimes make up at most 30\\% of the feature dimension, which is different from GPT-based LLMs. %\\todo{, possibly because we used GLU as a variant of FFN.}\nTherefore, a mixed-precision decomposition for \\texttt{matmul} can be far less efficient than a single FP16 \\texttt{matmul}. \nAfter a few weeks of trial, we finally decide to keep the FP16 precision of activation and only consider the quantization of model weights. \nIn this case, the quantized \\glm is dynamically converted to FP16 precision at runtime, introducing a small computational overhead but greatly reducing GPU memory usage for storing model weights.\nWe quantize all linear layers, which take up most of the transformer parameters, via vector-wise symmetric quantization, and leave input/output embedding, layer normalization, and bias terms unchanged. \n\nSurprisingly we note \\glm's unusual talent for INT4 weight quantization.\nAt the quantization precision of INT4, two INT4 weights are compressed into one INT8 weight for saving GPU memory usage.\nThus approximately only 70GB GPU memory is required to serve INT4 \\glm weights, which allow \\glm's inference on even 4 $\\times$ RTX 3090 Ti (24G) or 8 $\\times$ RTX 2080 Ti (11G).\nMore surprising, it requires no post-training at all and shows almost no quality degradation.\nOn the contrary, it does not apply to BLOOM-176B in our experiments.\n\n\\vvpara{Empirical Understanding.}\nSuch nice property for \\glm is fascinating but mysterious.\nWe try our best to understand it and empirically demonstrate that the INT4 weight quantization for \\glm may be a compound result of multiple factors, rather than a single one.\n\nIn terms of whether a model can be properly quantized, the most fundamental reason lies in the skewness of its weight distribution.\nSince existing methods on weight mostly rely on symmetrical quantization, if weight distributions in certain linear layers are too skewed, the quantization would lose much precision.\n\\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.2em,topsep=0.0em,partopsep=0.0em]\n    \\item \\textbf{GLM pre-training objective}: \n    \\item \\textbf{Gated Linear Unit (GLU)}:\n    \\item \\textbf{Scaling}:\n\\end{itemize}\n\n\\begin{insight}\n\\rm Backbone architecture matters in LLMs' post quantization. While GLM scaled up to \\glm can be quantized to as low as INT4, other LLMs only support INT8.\n\\end{insight}\n\n} \n\n\\section{The Results}\n\\label{sec:results}\n\nWe follow the common settings in LLMs such as GPT-3 and PaLM to evaluate \\glm for English~\\footnote{\\small Results in OPT-175B's paper are reported as applications to access it have not been approved for months.}. \nAs a bilingual LLM with Chinese, \\glm is also evaluated on Chinese benchmarks. \n\n\\vvpara{Discussion on the Scope of Zero-Shot Learning in \\glm.} %\\label{sec:zero-shot-criterion}\nSince \\glm has been trained with MIP, here we clarify its scope of zero-shot evaluation.\nIn fact, ``zero-shot'' seems to have controversial interpretations without a consensus in the community. \nWe follow one of the influential related surveys~\\citep{xian2018zero}, which says \n\\textit{``At test time, in zero-shot learning setting, the aim is to assign a test image to an unseen class label''}\nwhere involving unseen class labels is a key. \nTherefore, we derive our criterion to pick \\glm's zero-shot (and few-shot) datasets as:\n\\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.2em,topsep=0.0em,partopsep=0.0em]\n    \\item \\textbf{English}: 1) For tasks with fixed labels (e.g., \\textit{natural language inference}): no datasets in such tasks should be evaluated on; 2) For tasks without fixed labels (e.g., \\textit{(multiple-choice) QA, topic classification}): only datasets with an obvious domain transfer from those in MIP should be considered. % different\n    \\item \\textbf{Chinese}: All datasets can be evaluated as there exists a zero-shot cross-lingual transfer.\n\\end{itemize}\n\n\\vvpara{Filtering Test Datasets.}\nFollowing prior practices~\\citep{brown2020language,rae2021scaling} and our criterion mentioned above, we filter and refrain to report potentially contaminated datasets' evaluation results.\nFor LAMBADA and CLUE, we find minimal overlap under the 13-gram setting.\nPile, MMLU, and BIG-bench are either held-out or released later than the crawling of corpora.\n\n\\subsection{Language Modeling} \\label{sec:language_modeling}\n\n\\vvpara{LAMBADA.}\nLAMBADA~\\citep{paperno2016lambada} is a dataset to test the last word language modeling capability.\nThe results previously shown in Figure~\\ref{fig:lambada} suggest \\glm achieves a zero-shot accuracy of 80.2 with its bidirectional attention, setting up a new record on LAMBADA.\n\n\\begin{wraptable}{r}{5.5cm}\n\t\\centering\n\t\\footnotesize\n\t\\vspace{-6mm}\n    \\renewcommand\\tabcolsep{2pt}\n\\renewcommand\\arraystretch{1}\n\t\\caption{\\glm's average BPB on Pile evaluation (18 sub-datasets).}\n\t\\vspace{-3mm}\n\t\\scalebox{0.97}{\n\t\\begin{tabular}{@{}lccc@{}}\n    \\toprule[1.2pt]\n                       & Jurassic-1     & GPT-3          & \\glm           \\\\ \\midrule\n    Avg. BPB      & 0.650          & 0.742          & \\textbf{0.634} \\\\ \\bottomrule[1.2pt]\n    \\end{tabular}\n    }\n    \\vspace{-4mm}\n\\end{wraptable}\n\n\\vvpara{Pile.} \nThe Pile test-set~\\citep{gao2020pile} includes a series of benchmarks for language modeling.\nOn average, \\glm performs the best on its 18 shared test sets in terms of weighted BPB when compared to GPT-3 and Jurassic-1~\\citep{lieber2021jurassic} whose results are directly adopted from the latter, demonstrating its strong language capability (Cf. Appendix~\\ref{app:pile} for details).\n\n\\subsection{Massive Multitask Language Understanding (MMLU)} \\label{sec:mmlu}\n\nMMLU~\\citep{hendrycks2021measuring} is a diverse benchmark including 57 multi-choice question answering tasks concerning human knowledge ranging from high-school-level to expert-level. \nIt is released after the crawling of Pile and serves as an ideal test-bed for LLMs' few-shot learning.\nThe GPT-3 result is adopted from MMLU and BLOOM-176B is tested by using the same prompts as \\glm's \n(Cf. Appendix~\\ref{app:mmlu} and Table~\\ref{tab:mmlu} for details).\n\n\\glm's few-shot (5-shot) performance on MMLU approaches GPT-3 (43.9) after viewing about 300B tokens in Figure~\\ref{fig:mmlu}. \nIt continues moving up as the training proceeds, achieving an accuracy of 44.8 when the training has to end (i.e., viewing 400B tokens in total).  %and does not seem to saturate when training terminates.\nThis aligns with the observation~\\citep{hoffmann2022training} that most existing LLMs are far from adequately trained. \n\n\\subsection{Beyond the Imitation Game Benchmark (BIG-bench)} \\label{sec:big-bench}\n\n\\begin{figure}[t]\n\\begin{minipage}{0.33\\linewidth}\n    \\small\n    \\vspace{-5mm}\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{figures/mmlu.pdf}\n    \\vspace{-4mm}\n    \\caption{\\glm on MMLU (57 tasks) along training steps.}\n    \\label{fig:mmlu}\n    \\vspace{-6mm}\n\\end{minipage}\n\\hspace{.03\\linewidth}%\n\\begin{minipage}{0.34\\linewidth}\n    \\centering\n\t\\includegraphics[width=0.99\\columnwidth]{figures/big-bench-lite_1e10.pdf}\n\t\\vspace{-5mm}\n\t\\caption{BIG-bench-lite evaluation (24 tasks) across scales.}\n\t\\label{fig:big-bench}\n\\end{minipage}\n\\hspace{.002\\linewidth}%\n\\begin{minipage}{0.27\\linewidth}\n    \\centering\n    \\renewcommand\\tabcolsep{1pt}\n    \\renewcommand\\arraystretch{1.2}\n    \\scalebox{0.9}{\n    \\footnotesize\n    \\begin{tabular}{@{}lccc@{}}\n    \\toprule[1.2pt]\n               & 0-shot         & 1-shot         & 3-shot         \\\\ \\midrule\n    GPT-3 2.6B & 0.60           & 0.71           & 1.83           \\\\\n    GPT-3 6.7B & -0.06          & 2.93           & 5.40           \\\\\n    GPT-3 13B  & 1.77           & 5.43           & 7.95           \\\\\n    GPT-3 175B & 4.35           & 11.34          & 13.18          \\\\ \\midrule\n    PaLM 540B  & 8.05           & \\textbf{37.77} & -              \\\\ \\midrule\n    \\glm       & \\textbf{13.31} & 14.91          & \\textbf{15.12} \\\\ \\bottomrule[1.2pt]\n    \\end{tabular}}\n    \\vspace{1mm}\n    \\captionof{table}{Details on BIG-bench-lite (24 tasks).}\n    \\label{tab:big-bench}\n\\end{minipage}  \n\\vspace{-5mm}\n\\end{figure}\n\nBIG-bench~\\citep{srivastava2022beyond} benchmarks challenging tasks concerning models' ability on reasoning, knowledge, and commonsense. \nGiven evaluating on its 150 tasks is time-consuming for LLMs, we report the BIG-bench-lite---an official 24-task sub-collection---for now.\nObserved from Figure~\\ref{fig:big-bench} and Table~\\ref{tab:big-bench}, \n\\glm outperforms GPT-3 175B and even PaLM 540B (4$\\times$ larger) in zero-shot setting. \nThis is probably owing to \\glm's bidirectional context attention and MIP, which has been proved to improve zero-shot results in unseen tasks~\\citep{wei2022finetuned,sanh2022multitask}. \nAs the number of shots increases, \\glm's performance keeps going up, maintaining its outperformance over GPT-3  \n(Cf. Appendix~\\ref{app:big-bench} and Table~\\ref{tab:big-bench-details} for details on each model and task).\n\n\\vvpara{Limitations and Discussions.}\nIn the experiments above, we observe that \\glm's performance growth (13.31 to 15.12) with the increase of few-shot samples is not as significant as GPT-3's (4.35 to 13.18). \nHere is our intuitive attempt to understand the phenomenon. \n\nFirst, the bidirectional nature of \\glm could lead to strong zero-shot performance (as is indicated in zero-shot language modeling), thus getting closer to the few-shot ``upper-bound'' for models of similar scale (i.e., 100B-scale) than unidirectional LLMs.\nSecond, it may be also attributed to a deficit of existing MIP paradigms~\\citep{wei2022finetuned,sanh2022multitask}, which only involve zero-shot prediction in the training and will be likely to bias \\glm for stronger zero-shot learning but relatively weaker in-context few-shot performance.\nTo correct the bias, a potential solution we came up with would be to employ MIP with varied shots of in-context samples rather than only zero-shot samples.% if we ever got a chance to continue pre-training \\glm. \n\nFinally, despite almost the same GPT architecture as GPT-3, PaLM 540B's relative growth with few-shot in-context learning is substantially more significant than GPT-3's. \nWe conjecture this further acceleration in performance growth is a source of PaLM's high-quality and diverse private-collected training corpora. \nBy combining our experiences with~\\citep{hoffmann2022training}'s insights, we came to realize that better architectures, better data, and more training FLOPS should be further invested. \n\n\\subsection{Chinese Language Understanding Evaluation (CLUE)} \\label{sec:clue}\n\nWe evaluate \\glm's Chinese zero-shot performance on established Chinese NLP benchmarks, CLUE~\\citep{xu2020clue} and FewCLUE~\\citep{xu2021fewclue}.%, which focus on language understanding tasks such as text matching and sentimental analysis. \nNote that we do not include any Chinese downstream tasks in MIP. %, so there is only possibly a zero-shot cross-lingual transfer that contributes to \\glm's zero-shot Chinese performance. \nTo date, we have finished testing on part of the two benchmarks, including 7 CLUE and 5 FewCLUE datasets (Cf. Appendix~\\ref{app:clue} for details). \nWe compare GLM-130B to the largest existing Chinese monolingual language model---the 260B ERNIE Titan 3.0~\\citep{wang2021ernie}. %, which possesses 260B parameters. \nWe follow its setting to report zero-shot results on dev datasets. \n\\glm consistently outperforms ERNIE Titan 3.0 across 12 tasks (Cf. Figure~\\ref{fig:clue}). \nInterestingly, \\glm performs at least 260\\% better than ERNIE on two abstractive MRC datasets (DRCD and CMRC2018), possibly due to \\glm's pre-training objective that naturally resonates to abstractive MRC's form. \n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/clue.pdf}\n    \\vspace{-6mm}\n    \\caption{\\glm and ERNIE Titan 3.0 260B evaluated on zero-shot CLUE and FewCLUE.}\n    \\label{fig:clue}\n    \\vspace{-4mm}\n\\end{figure}\n\n\\hide{%%%%%start of hide %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n\\section{Main Results}\n\\label{sec:results}\nIn this section, we introduce details in our experiments and evaluation settings.\nFollowing other works on LLMs~\\citep{brown2020language,rae2021scaling,chowdhery2022palm}, we report \\glm's zero-shot and few-shot performance over a number of benchmarks, domains, and tasks.\nSince \\glm is a bilingual LLM proficient in both English and Chinese, we also evaluate it over popular Chinese language evaluation benchmarks in a zero-shot setting.\n\n\\subsection{Discussion: Scope of Zero-shot Learning in \\glm} \\label{sec:zero-shot-criterion}\nAs we are leveraging Multi-task Instruction Pre-Training (MIP), it is important to clarify our setting of \"zero-shot\".\nHowever, there seems to be no officially recognized definition for zero-shot learning, and many different interpretations exist in the community. \nTo our best knowledge, we refer to the definition from one of the most influential zero-shot learning surveys~\\citep{xian2018zero}, which says \n\\textit{``... At test time, in zero-shot learning setting, the aim is to assign a test image to an unseen class label, ...\n''}\nwhere whether the evaluated task involves unseen class labels is a key. \nTherefore, for NLP tasks we derive our criterion to select \\glm zero-shot and few-shot evaluation datasets:\n\\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.2em,topsep=0.0em,partopsep=0.0em]\n    \\item \\textbf{English}: 1) For tasks with fixed labels (e.g., \\textit{natural language inference, paraphrase identification}): no datasets in such tasks should be evaluated on; 2) For tasks without fixed labels (e.g., \\textit{question answering, topic classification, multiple-choice QA}): only datasets with an obvious domain transfer and different labels from those in MIP should be considered.\n    \\item \\textbf{Chinese}: All datasets can be evaluated as there exists a zero-shot cross-lingual transfer.\n\\end{itemize}\n\n\\todo{Filtering test set}\n\n\\subsection{Language Modeling}\nLanguage modeling is an intrinsic evaluation criterion for any language models.\nSince memorization would substantially help language modeling, for \\glm, we refrain to only report \\glm's performance on LAMABADA~\\citep{paperno2016lambada} and Pile test-set~\\citep{gao2020pile}.\n\n\\vvpara{LAMBADA.}\nFollowing other models~\\citep{black2022gpt,gpt-j} trained on Pile corpus, we report results on LAMBADA, a dataset to test last word language modeling capability.\nThe results have been shown in Figure~\\ref{fig:lambada}, in which \\glm achieves a zero-shot accuracy of 80.2 with its superior bidirectional attention, setting up a new record on LAMBADA.\n\n\\begin{wraptable}{r}{6.5cm}\n\t\\centering\n\t\\footnotesize\n\t\\vspace{-4mm}\n    \\renewcommand\\tabcolsep{2pt}\n\t\\caption{\\glm and its similar-sized LLMs' average BPB on Pile (18 sub-datasets).}\n\t\\vspace{-2mm}\n\t\\scalebox{0.97}{\n\t\\begin{tabular}{@{}lccc@{}}\n    \\toprule[1.2pt]\n                       & Jurassic-1     & GPT-3          & \\glm           \\\\ \\midrule\n    Weighted Avg. BPB      & 0.650          & 0.742          & \\textbf{0.634} \\\\ \\bottomrule[1.2pt]\n    \\end{tabular}\n    }\n    \\vspace{-2mm}\n\\end{wraptable}\n\n\\vvpara{Pile Evaluation.} \nWe evaluate \\glm's performance on Pile test-set~\\citep{gao2020pile}, which includes a series of benchmarks for language modeling.\nWe compare \\glm's BPB on Pile test-set with GPT-3 and Jurassic-1~\\citep{lieber2021jurassic} based on results reported in~\\citep{lieber2021jurassic}.\nOn average, \\glm performs the best on these 18 shared test sets in these three LLMs, which proves its strong language capability. Detailed analysis and results are shown in Appendix~\\ref{app:pile}.\n\n\\subsection{Massive Multitask Language Understanding (MMLU)} \\label{sec:mmlu}\n\n\\begin{wrapfigure}{r}{5.3cm}\n    \\small\n    \\vspace{-4mm}\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{figures/mmlu.pdf}\n    \\vspace{-4mm}\n    \\caption{\\glm on MMLU (57 tasks) along its training trajectory.}\n    \\label{fig:mmlu}\n    \\vspace{-6mm}\n\\end{wrapfigure}\n\nMMLU~\\citep{hendrycks2021measuring} is a diverse benchmark including 57 multi-choice question answering tasks concerning human knowledge ranging from high-school-level to expert-level. \nIt serves as an ideal test-bed for LLMs' few-shot learning and is released after the crawling of Pile.\n\nWe plot \\glm's few-shot (5-shot) performance along its training trajectory in Figure~\\ref{fig:mmlu}. \nIt approaches GPT-3 comparable performance 43.9 after viewing about 300 billion tokens. \nIts performance continues growing as the training proceeds, achieving 44.8 after viewing 400 billion tokens and does not seem to saturate when training terminates.\nThis aligns with the observation in~\\citep{hoffmann2022training} that existing LLMs are still far from adequately trained.\nGPT-3 results are from~\\citep{hendrycks2021measuring} and BLOOM 176B results are tested by ourselves using the same prompts as \\glm does.\nMore detailed results on each discipline of MMLU can be find in Appendix~\\ref{app:mmlu} and Table~\\ref{tab:mmlu}.\n\n\\subsection{Beyond the Imitation Game Benchmark (BIG-bench)}\n\n\\begin{figure}[t]\n\\begin{minipage}{0.72\\linewidth}\n    \\centering\n\t\\includegraphics[width=0.99\\columnwidth]{figures/BIG-bench.pdf}\n\t\\vspace{-2.2mm}\n\t\\caption{BIG-bench-lite evaluation (24 tasks) across scales.}\n\t\\label{fig:big-bench}\n\\end{minipage}\n\\begin{minipage}{0.27\\linewidth}\n\\centering\n\\renewcommand\\tabcolsep{1pt}\n\\renewcommand\\arraystretch{1.4}\n\\scalebox{0.9}{\n\\footnotesize\n\\begin{tabular}{@{}lccc@{}}\n\\toprule[1.2pt]\n           & 0-shot         & 1-shot         & 3-shot         \\\\ \\midrule\nGPT-3 2.6B & 0.60           & 0.71           & 1.83           \\\\\nGPT-3 6.7B & -0.06          & 2.93           & 5.40           \\\\\nGPT-3 13B  & 1.77           & 5.43           & 7.95           \\\\\nGPT-3 175B & 4.35           & 11.34          & 13.18          \\\\ \\midrule\nPaLM 540B  & 8.05           & \\textbf{37.77} & -              \\\\ \\midrule\n\\glm       & \\textbf{13.31} & 14.91          & \\textbf{15.12} \\\\ \\bottomrule[1.2pt]\n\\end{tabular}\n}\n\\vspace{1mm}\n\\captionof{table}{Details on BIG-bench-lite (24 tasks).}\n\\label{tab:big-bench}\n\\end{minipage}  \n\\vspace{-5mm}\n\\end{figure}\n\nBIG-bench~\\citep{srivastava2022beyond} is a novel benchmark with many challenging tasks concerning models' ability on reasoning, world knowledge, and commonsense.\nSince the full BIG-bench covers 150 tasks, a complete evaluation can be too expensive and time-consuming for LLMs.\nThus the BIG-bench-lite, an officially released 26-task sub-collection, is considered in \\glm at first.\n\nOur results are shown in Figure~\\ref{fig:big-bench} and Table~\\ref{tab:big-bench}.\nWith 130 billion parameters, \\glm outperforms larger-sized GPT-3 175B and even PaLM-540B in zero-shot setting.\nThis is probably owing to \\glm's bidirectional attention over contexts and MIP training (which has been proved to encourage zero-shot learning in unseen tasks~\\citep{wei2022finetuned,sanh2022multitask}).\nAs the number of shots increases, \\glm's performance on BIG-bench-lite also grows and outperforms GPT-3.\nMore detailed results on each model for each task and evaluation can be find in Appendix~\\ref{app:big-bench} and Table~\\ref{tab:big-bench-details}.\n\n\\vvpara{Limitations and Discussion.}\nWe observe that \\glm's growth when increasing few-shot samples is not as significant as GPT-3's. \nThere are some reasons that intuitively explain the phenomenon.\nFirst, \\glm's bidirectional nature could lead to stronger zero-shot performance (as is indicated in zero-shot language modeling) and approach significantly more to its few-shot upper-bound than unidirectional LLMs.\nSecond, it may be also attributed to a deficit of existing MIP paradigms~\\citep{wei2022finetuned,sanh2022multitask}, which only involve zero-shot prediction in the training and will be likely to bias \\glm for stronger zero-shot learning but relatively weaker in-context few-shot performance.\nWe think a potential remedy would be to employ MIP with varied shots of in-context samples rather than only zero-shot samples in our continual pre-training to correct the bias.\n\nFinally, despite almost the same GPT architecture, PaLM-540B's growth with few-shot in-context learning is exceedingly substantial compared to GPT-3, which we suppose is because of its high-quality and diverse private collected corpora. \nCombining observations from~\\citep{hoffmann2022training}, we think it is high time that our community should invest more on better pre-training architecture, data, and more training FLOPS, rather than a mere larger size.\n\n\\subsection{Chinese Language Understanding Evaluation (CLUE)}\n\nAs GLM-130B is a bilingual language model, we also evaluate its zero-shot performance on established Chinese NLP benchmarks, CLUE~\\citep{xu2020clue} and FewCLUE~\\citep{xu2021fewclue}, which majorly address language understanding tasks such as text matching and sentimental analysis.\nNote that we do not include any Chinese downstream tasks in MIP, so there is only possibly a zero-shot cross-lingual transfer that contributes to \\glm's zero-shot Chinese performance. \nWe currently have finished the testing on part of the two benchmarks, including 7 CLUE and 5 FewCLUE datasets.\n\nWe compare GLM-130B to the largest existing Chinese monolingual language model ERNIE Titan 3.0~\\citep{wang2021ernie}, which possesses 260B parameters. \nFollowing its setting, we report zero-shot results on dev datasets. \nAs is shown in the Figure~\\ref{fig:clue}, \\glm generally outperforms ERNIE Titan 3.0.\nInterestingly, \\glm performs extraordinarily well on abstractive MRC datasets DRCD and CMRC2018.\nWe speculate that \\glm's blank infilling objective naturally resonates to abstractive machine reading comprehension.\nMore details please refer to Appendix~\\ref{app:clue}.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/clue.pdf}\n    \\vspace{-6mm}\n    \\caption{\\glm and ERNIE Titan 3.0 (260B) evaluated on zero-shot CLUE and FewCLUE.}\n    \\label{fig:clue}\n    \\vspace{-6mm}\n\\end{figure}\n\n}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\nIn this section, we review related work to \\glm on topics of pre-training, transferring, and inference of pre-trained LLMs~\\citep{qiu2020pre,bommasani2021opportunities}.\n\n\\vvpara{Pre-Training.}\nVanilla language modeling refers to decoder-only autoregressive models (e.g., GPT~\\citep{radford2018improving}), but it also recognizes any forms of self-supervised objectives on texts. %over unlabeled corpora.%, especially with the transformer architecture.\nRecently, transformer-based~\\citep{vaswani2017attention} language models present a fascinating scaling law: new abilities~\\citep{wei2022emergent} arise as models scale up, from 1.5B~\\citep{radford2019language}, 10B-scale language models~\\citep{raffel2020exploring,shoeybi2019megatron,black2022gpt}, to 100B-scale GPT-3~\\citep{brown2020language}.\nLater, despite many 100B-scale LLMs~\\citep{lieber2021jurassic,thoppilan2022lamda,rae2021scaling,smith2022using,chowdhery2022palm,wu2021yuan,zeng2021pangu,wang2021ernie} in both English and Chinese, they are not available to public or only accessible via limited APIs.\nThe closeness of LLMs severely stymies its development.\n\\glm's efforts, along with recent ElutherAI, OPT-175B~\\citep{zhang2022opt}, and BLOOM-176B~\\citep{scao2022what}, aim to offer high-quality open-sourced LLMs to our community.\n\n\\vvpara{Transferring.}\nThough fine-tuning has been a \\textit{de facto} way for transfer learning, the evaluation for LLMs has been focused on prompting and in-context learning due to their tremendous sizes~\\citep{brown2020language,liu2021pre}.\nNevertheless, some recent attempts has been on parameter-efficient learning on language models\n~\\citep{houlsby2019parameter} and prompt tuning (i.e., P-tuning, ~\\cite{li2021prefix,liu2021gpt,lester2021power,liu2022p}). %,hu2022lora,zaken2022bitfit\nFor now we do not focus on them and will leave the comprehensive testing of them on \\glm in future study.\n\n\\vvpara{Inference.}\nMost public-accessible LLMs nowadays are providing their services via limited APIs.%, as their size is not for economical GPUs.\nIn this work, an important part of our endeavor has been on LLMs' efficient and fast inference.\nRelated work may include distillation~\\citep{sanh2019distilbert,jiao2020tinybert,wang2020minilm}, quantization~\\citep{zafrir2019q8bert,shen2020q,tao2022compression}, and pruning~\\citep{michel2019sixteen,fan2019reducing}.\nVery recent work~\\citep{dettmers2022llm} shows that LLMs such as OPT-175B and BLOOM-176B can be quantized to 8 bit due to special distribution of outlier dimensions.\nIn this work, we demonstrate GLM's scaling law for INT4 weight quantization, which allows \\glm to inference on as few as 4$\\times$RTX 3090 (24G) GPUs or 8$\\times$RTX 2080 Ti (11G) GPUs.\n\nWe introduce \\glm, a bilingual pre-trained language model that aims to facilitate open and inclusive LLM research. \n\\glm's technical and engineering undertakings generate insight into LLMs' architectures, pre-training objectives, training stability and efficiency, and affordable inference. Altogether, it contributes to the high quality of \\glm in terms of both language performance on 112 tasks and ethical results on bias and toxicity benchmarks. \nOur experiences of both success and failure are condensed into the lessons for training 100B-scale LLMs, attached in the Appendix \\ref{sec:lessons}.\n\n\\hide{\n\\begin{enumerate}[leftmargin=*,itemsep=0pt,parsep=0.2em,topsep=0.0em,partopsep=0.0em]\n    \\item \n    The bidirectional-attention GLM is a strong objective alternative, in addition to GPTs. % at large scale.\n    \\item Configure LLMs based on the cluster and parallel strategy used to squeeze hardware potential.\n    \\item Counter-stereotypically, DeepNorm, a type of Post-LN, is the LN option to stabilize \\glm.\n    \\item Training instability that LLMs suffer from arises systematically, numerically, and unexpectedly.\n     \\item Though FP16 induces more instability, it enables training and inference on diverse platforms. \n \n    \\item Shrinking embedding layer's gradient by a factor of 10 can solve most numerical instability.\n    \\item GLM has a INT4 weight quantization scaling law unobserved in GPT-style BLOOM.\n    \\item To create powerful LLMs, the main focus can be on 1) more and better data, 2) better architectures and pre-training objectives, and 3) more sufficient training.\n\\end{enumerate}\n}\n\n\\hide{\nWe introduce \\glm, an open bilingual pre-trained model that aims to promote transparency and inclusivity in LLM research.\nOur engineering undertakings and unique insights into LLMs' architectures, objectives, and training process jointly contribute to its average strong language performance over 112 tasks and mitigated bias and toxicity benchmarks we evaluate.\nWe condense our experiences of both success and failure into the following valuable lessons to our community:\n\n\\begin{enumerate}[leftmargin=*,itemsep=0pt,parsep=0.2em,topsep=0.0em,partopsep=0.0em]\n    \\item Bidirectional-attention GLM can be stronger than unidirectional GPTs at large scale.\n    \\item Configure your LLMs based on your cluster and parallel strategy to squeeze hardware potential.\n    \\item Counter-stereotypically, DeepNorm, a type of Post-LN, is the LN option to stabilize \\glm.\n    \\item Instabilities that LLMs suffer from in pre-training arouses systematically and numerically.\n    \\item FP16 is a challenging but rewarding decision for \\glm: it induces more systematical instability, but is a must for enabling LLMs to train and inference on inclusive ranges of platforms.\n    \\item Shrinking embedding layer's gradient to its 0.1 can solve most numerical instability problems.\n    \\item GLM architecture presents a unique scaling law of INT4 weight quantization unseen in BLOOM.\n    \\item To create inclusive LLMs better than those of big companies', community's later focus should be on 1) more and better data, 2) better architectures and objectives, and 3) more sufficient training.\n\\end{enumerate}\n\n}\n\n\\section*{Acknowledgement}\nThis research was supported by Natural Science Foundation of China (NSFC) 61825602, 62276148 and Zhipu.AI.\nWe thank all our collaborators and partners from the Knowledge Engineering Group (KEG), Parallel Architecture \\& Compiler technology of Mobile, Accelerated, and Networked systems Group (PACMAN), Natural Language Processing Group (THUNLP) at Tsinghua University, and Zhipu.AI.\n\n\\section*{Ethics Statement}\nWe hereby acknowledge that all of the co-authors of this work are aware of the provided ICLR Code of Ethics and honor the code of conduct.\nThis work introduces an open-source Large Language Model (LLM), which could be used to generate synthetic text for harmful applications, such as telemarketing fraud, political propaganda, and personal harassment as is discussed in~\\citep{weidinger2021ethical,Sheng2021SocietalBI,Dev2021HarmsOG}. \nWe do not anticipate any hazardous outputs, especially towards vulnerable and historically disadvantaged groups of peoples, after using the model.\n\nAnd to better collaborate with our community to prevent and ultimately eliminate the risks technically, we make the following crucial open efforts in this work:\n\n\\vvpara{Open-Sourced LLMs for Ethical Risk Study.}\nWhile some people think that restricting the access of LLMs can prevent such harmful applications, we argue that promoting LLM inclusivity can lead to better defense against potential harms caused by LLMs. \nCurrently, only governments and large corporations can afford the considerable costs of pre-training LLMs. \nThere is no guarantee that organizations having the the substantial financial resources will not do harm using a LLM. \nWithout access to such LLMs, individuals cannot even realize the role of LLMs in the harm. \n\nConversely, releasing an open LLM can provide access and transparency to all the researchers and promote the research to reduce the potential harm of LLMs, like algorithms to identify the synthetic text~\\cite{gehrmann2019gltr}. \nAlso, it is known that LLMs can suffer from problems in fairness, bias, privacy, and truthfulness~\\cite{abs-2112-12938,lin2022truthfulqa,Liang2021SocialBias,Bender2021Danger}. \nAn open LLM can reveal the model parameters and internal states corresponding to specific inputs instead of providing APIs to black-box models. \nIn conclusion, researchers can conduct analysis of LLMs' flaws in depth and propose improved algorithms to solve the problems. \n\n\\vvpara{Ethical Evaluation and Improvements.}\nWe also evaluate our model over a wide range of English ethical evaluation benchmarks, including bias measurement~\\citep{nadeem2021stereoset,nangia2020crows}, hate speech detection~\\citep{mollas2020ethos}, and toxic generation estimation~\\citep{gehman2020realtoxicityprompts}.\nNotwithstanding their deficiency~\\citep{blodgett2021stereotyping,jacobs2021measurement}, these datasets serve as a meaningful initial step towards an open quantitative evaluation LLMs.\n\nOur evaluation implies that our algorithm designs, especially the bilingual pre-training of a LLM, can significantly mitigate the biases and toxicity an LLM may present while keeping its strong language performance compared to other LLMs~\\citep{brown2020language,zhang2022opt} trained with monolingual English corpora (Cf. Appendix~\\ref{app:ethics} for more details).\n\n\\section*{Reproducibility}\nCompared to mainstream closed-sourced LLMs including GPT-3 175B\\citep{brown2020language}, PaLM 540B~\\citep{chowdhery2022palm}, Gopher~\\citep{rae2021scaling}, Chinchilla~\\citep{hoffmann2022training}, LaMDA~\\citep{thoppilan2022lamda}, FLAN~\\citep{wei2022finetuned}, and many others, \\glm is open-sourced and devotes to promote openness and inclusivity in LLM research from the very beginning.\n\nWe have paid great effort to ensure the reproducibility of our evaluation.\nFor pre-training section, despite the unaffordable costs it needs to reproduce at present, we still make our best efforts to disclose the code, details, and the whole process of \\glm's pre-training. \nOur endeavor to allow \\glm inference on few popularized GPUs such as 3090/2080 Ti also aligns with the reproducibility undertaking, as it allows most academic researchers to reproduce \\glm's results on their offline machines.\nWe also provide free APIs for individual users to test \\glm's ability.\n\n\\vvpara{Pre-Training.}\nWe provide the complete training notes, Tensorboard logs, and code for our pre-training in our repository (Cf. Abstract).\nThe pre-training hyper-parameters and cluster configuration are provided in Section~\\ref{sec:parallel_strategy} and Table~\\ref{tab:config}.\nThe training corpora composition and details for Multi-task Instruction Pre-training are provided in Section~\\ref{sec:training_objective} and Appendix~\\ref{app:mip_description} and~\\ref{app:mip_dataset}. \n\n\\vvpara{Evaluation.}\nWe organize all the evaluation, including language benchmarks (LAMBADA, Pile, MMLU, BIG-bench, CLUE, and FewCLUE) and ethical benchmarks (CrowS-Pairs, StereoSet, ETHOS, RealToxicPrompts), into one-command-to-run bash scripts in our code repository.\nData processing details for language modeling benchmarks are provided in Section~\\ref{sec:language_modeling} and Appendix~\\ref{app:pile}, for MMLU are provided in Section~\\ref{sec:mmlu} and Appendix~\\ref{app:mmlu}, for BIG-bench are provided in Section~\\ref{sec:big-bench} and Appendix~\\ref{app:big-bench}, for CLUE and FewCLUE are provided in ~\\ref{sec:clue}.\nFor all ethical evaluation, please refer to Appendix~\\ref{app:ethics} for details.\n\n\\clearpage\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{\\modelname{}: Empowering Mathematical Reasoning for Large Language Models via \\REIname{}}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\n\nLarge language models (LLMs), such as GPT-4, have shown remarkable performance in natural language processing (NLP) tasks, including challenging mathematical reasoning. However, most existing open-source models are only pre-trained on large-scale internet data and without math-related optimization. In this paper, we present \\modelname{}, which enhances the mathematical CoT reasoning abilities of LLMs without using external python tools, by applying our proposed \\REInameF{} (\\textbf{\\REInameS{}}) method to the domain of math. Through extensive experiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary capabilities of our model. Remarkably, \\modelname{}-Mistral 7B surpasses top-tier open-source LLMs by a substantial margin with higher data efficiency. Furthermore, \\modelname{} 70B even outperforms GPT-3.5-Turbo, Claude 2, Gemini Pro and GPT-4-early-version. Additionally, our preliminary exploration highlights the pivotal role of instruction evolution and process supervision in achieving exceptional math performance.\nFor more details refer to \\url{https://github.com/nlpxucan/WizardLM}.\n\n\\end{abstract}\n\\section{Introduction}\n\nRecently, Large-scale language models (LLMs) have  garnered significant attention and become the go-to approach for numerous natural language processing (NLP) tasks, including open domain conversation~\\citep{ouyang2022training,openai2023gpt4,touvron2023llama}, coding~\\citep{chen2021evaluating-humaneval,codet5,li2023starcoder} and math~\\citep{taylor2022galactica,lewkowycz2022solving, shao2024-deepseekmath, yang2024-qwen2.5-math}. A conspicuous example is ChatGPT\\footnote{\\quad \\url{https://openai.com/}\\label{fn:chatgpt}}\n, developed by OpenAI. This model uses extensive pre-training on large-scale internet data and further fine-tuning with specific instruction data and methods. As a result, it achieves state-of-the-art zero-shot performance on various benchmarks. Subsequently, Anthropic,  Google, and Meta  also launched their competitive products one after another. Notably, Meta's series of  Llama~\\citep{touvron2023llama, touvron2023llama2, dubey2024-llama3} have sparked an open-source revolution and quickly narrowed the gap with those closed-source LLMs. This trend also gradually stimulates the releases of Mistral~\\citep{jiang2023mistral}, Alpaca~\\citep{alpaca}, Vicuna~\\citep{vicuna2023}, and WizardLM~\\citep{xu2023wizardlm}, etc. However, these open models still struggle with the scenarios which require  complex multi-step  quantitative reasoning, such as solving mathematical and science challenges~\\citep{ahn2024-comprehention-LLM, long2024-llms-survey}. \n\n\\begin{figure}[t]\n\\centering\n  \\includegraphics[width=0.82\\textwidth, trim=39 43 190 10,clip]{Figures/Main.pdf}\n\n\\caption{A diagram illustrating the three steps of our \\REInameF{} (\\textbf{\\REInameS{}})}\n\n\\label{fig:reinforcement_evol_instruct_pic}\n\\end{figure}\nChain-of-thought (CoT)~\\citep{wei2022chain} proposes to design better prompts to  generate step-by-step solutions, which can lead to improved performance. Self-Consistency~\\citep{wang2023selfconsistency} also achieves remarkable performance on many reasoning benchmarks, which generates several possible answers from the model and selects the correct one based on majority vote~\\citep{fu2023complexity-cot-based}. Llemma~\\citep{azerbayev2023llemma} and MathPile~\\citep{wang2023generative} continue pretraining LLMs with math corpus to improve domain capacity. MetaMath~\\citep{yu2023metamath} and Xwin-Math~\\citep{li2024-Xwin-math} bootstraps mathematical questions by augmenting the question from multiple perspectives. MAmmoTH ~\\citep{yue2023mammoth} and TORA~\\citep{gou2023tora} presents a unique hybrid of CoT and program-of-thought (PoT) to ensure extensive coverage of diverse fields in math. Recently, Evol-Instruct is an effective method for large-scale data synthesis using LLMs. It has been widely verified and proven to be effective in enhancing the model's instruction following capability. It employs In-depth Evolving and In-breadth Evolving to automate the generation of diverse and complex open-domain instructions using LLMs, instead of relying on human-crafted instruction datasets. In-depth Evolving incrementally enhances instruction complexity by introducing additional constraints, deepening, concretizing, increasing reasoning steps, and complicating input. In-breadth Evolving focuses on improving topic diversity and dataset richness by creating entirely new instructions.  To enhance the correctness of each step in the model's generation process, \\citep{wang2024-mathshepherd,chen2024-autoprm, lightman2023openai-verify-step-by-step} finds that process supervision with reinforcement learning significantly outperforms outcome supervision for solving challenging MATH problems.\n\nInspired by \\EIname{} and Process-supervised Reinforcement Learning, this work aims to enhance the  mathematical reasoning abilities of the LLMs. As shown in the Figure~\\ref{fig:reinforcement_evol_instruct_pic}, we propose a new method named \\REInameF{} (\\textbf{\\REInameS{}}), which could firstly generate diverse math instructions data by brand-new \\MathEIname{}, which includes two downward  evolution and upward evolution progress to produce the grade school math and  challenging high school math respectively. However different from WizardLM~\\citep{xu2023wizardlm} and WizardCoder~\\citep{luo2023wizardcoder}, which mainly focus on the SFT stage and are susceptible to learning hallucinated information from the teacher model, we innovatively introduce PRM to address the False-Positive issue in the problem-solving process. Moreover, to prevent instruction evolution from spiraling out of control, we incorporate an instruction reward model (IRM) as a mitigating strategy.  Thus, we train an instruction  reward model (IRM) and a process-supervised reward model (PRM)~\\citep{lightman2023openai-verify-step-by-step,uesato2022deepmind-orms, wang2024-mathshepherd, chen2024-autoprm}, the former indicates the quality of the evolved instruction and the latter offers feedback for each reasoning step in the  solution. Initially, we finetune LLMs with the evolved math data. Immediately, we leverage GPT-4 to produce the ranking order of instructions, and the correctness of each reasoning step, then optimize the LLMs to obtain the reward models. Finally, we implement the step-by-step PPO to train our \\modelname{}.\n\nWe perform experiments on two widely used mathematical reasoning benchmarks, namely GSM8k~\\citep{cobbe2021training_gsm8k_2} and MATH~\\citep{hendrycks2021measuring} covering math problems from grade to high school levels, the results show that our \\modelname{} outperforms all other open-source LLMs at the same model size, achieving state-of-the-art performance. For instance, \\modelname{}-70B significantly outperforms MetaMath-70B by a\nsignificant margin on GSM8k (92.8 vs. 82.3) and on MATH (58.6 vs. 26.6). Specifically, \\modelname{}-Mistral-7B observed a substantial improvement in pass@1 with an increase of +12.8 (90.7. vs. 77.9) on GSM8k, and +26.8 (55.4 vs. 28.6) on MATH compared to MetaMath-Mistral-7B. Notably, our 70B model even also significantly surpasses those powerful proprietary LLMs, such as GPT-3.5-Turbo,  Claude 2~\\citep{bai2022constitutional-claude}, Mistral Medium~\\citep{jiang2024mixtral}, Gemini-Pro ~\\citep{geminiteam2023gemini}, PaLM-2~\\citep{palm2} and GPT-4-early-version.\n\nThe main contributions of this work are as follows:\n\n\\begin{itemize}\n\\item We introduce \\modelname{} model, which enhances the LLMs' mathematical reasoning abilities across a range of problem difficulties, from grade to high school levels.\n\\item We propose a new fully AI-powered automatic reinforcement learning method,  \\REInameF{} (\\textbf{\\REInameS{}}), alongside \\MathEIname{} and Process Supervision, for improving reasoning performance. \n\\item \\modelname{} surpasses top-tier open-source LLMs by a substantial margin with higher data efficiency and also significantly outperforms various proprietary LLMs on both GSM8k and MATH, demonstrate the effectiveness of our \\textbf{\\REInameS{}}.\n\\end{itemize}\n\n\\section{Related Work}\n\n\\textbf{Large Language Models.} LLMs have significantly advanced Natural Language Processing, with models like OpenAI's GPT Series ~\\citep{GPT3,openai2023gpt4}, Anthropic's Claude~\\citep{bai2022constitutional-claude}, Google's PaLM~\\citep{PaLM,palm2}, Gemini~\\citep{geminiteam2023gemini}, and Gemma~\\citep{team2024gemma} featuring billions of parameters and trained on massive textual datasets. The AI field has also seen a rise in open-source LLMs such as Mistral~\\citep{jiang2023mistral}, Llama Series ~\\citep{touvron2023llama,touvron2023llama2, dubey2024-llama3, taylor2022galactica},  DeepSeek~\\citep{Bi2024DeepSeekLS, shao2024-deepseekmath}, Qwen~\\citep{Bai2023QwenTR, yang2024-qwen2.5-math} etc. Notably, Llama serves as a foundational model for supervised fine-tuning, leading to the development of models like Alpaca, Vicuna~\\citep{alpaca,vicuna2023}.\n\n\\textbf{Large Language Models For Mathematical reasoning.} NLP models face challenges with complex reasoning, including mathematical~\\citep{long2024-llms-survey, zhang2024-geoeval, xia2024-evaluating}, common-sense~\\citep{talmor-etal-2019-commonsenseqa}. Significant research focuses on Mathematical Word Problems (MWP), which demand understanding of mathematical concepts and multi-step reasoning~\\citep{zheng2023php,zhao2023modelselection, yuan2023RFT}. Models are tested on various MWP benchmarks ~\\citep{roy-roth-2015-solving-multiarith, hendrycks2021measuring}. Techniques like Chain-of-Thought Prompting~\\citep{wei2022chain}, Least-to-Most prompting~\\citep{Zhou2022LeasttoMostPE}, and Complex CoT~\\citep{fu2023complexity-cot-based} enhance reasoning by introducing multiple steps and breaking problems into sub-problems. There are some models aimed at improving math CoT reasoning skills such as MetaMath~\\citep{yu2023metamath}, \n MathScale~\\citep{tang2024mathscale}, Xwin-Math~\\citep{li2024-Xwin-math}, DART-Math~\\citep{tong2024-DART} etc. Some models enhance mathematical reasoning by integrating python tools, such as TORA~\\citep{gou2023tora}, \n  MAmmoTH~\\citep{yue2023mammoth}, \n  Openmathinstruct~\\citep{toshniwal2024-openmathinstruct}, NuminaMath~\\citep{li2024-numinamath} etc. In our work, we mainly improve the CoT reasoning ability of mathematics without using external Python tools.\n\n\\textbf{Reinforcement Learning for Large Language Models.} State-of-the-art models often display logical errors and illusions, particularly in domains requiring complex, multi-step reasoning, leading to significant challenges~\\citep{bubeck2023sparks, maynez2020faithfulness}. Strategies such as training reward models help discriminate between desirable and undesirable outputs~\\citep{lightman2023openai-verify-step-by-step,Wu2023FineGrainedHF, chen2024-fine-grain-improving}. Historically, outcome-based approaches focused on algorithmic tasks~\\citep{ Li2016NeuralPL, cai2017making,Yu2023OutcomesupervisedVF}, while recent research demonstrates the efficacy of reward models or validators in enhancing model performance~\\citep{cobbe2021training_gsm8k_2,Wang2023MakingLL,Wang2023LargeLM,Li2022MakingLM}. Reward models have also been incorporated into reinforcement learning pipelines and employed in rejection sampling to align Large Language Models (LLMs) with human preferences~\\citep{ shen2021generate,bai2022constitutional-claude,yuan2023rrhf, dong2023raft,song2023preference, touvron2023llama2, rafailov2024DPO, meng2024simpo}. A contrast is drawn between outcome-supervised and process-supervised reward models, with the latter being more effective at addressing discrepancies arising from incorrect reasoning paths leading to correct outcomes~\\citep{uesato2022deepmind-orms,zelikman2022star, creswell2022selectioninference}. Recent advances have promoted process-based supervision through manual annotation, significantly benefiting LLMs over outcome-based approaches~\\citep{lightman2023openai-verify-step-by-step,wang2024-mathshepherd, sun2024-easy-to-hard, chen2024-autoprm, wang2024-multi-prm, zhang2024-rest-mcts}. In this paper, we leverage AI models like ChatGPT to automatically offer process annotation to improve the efficiency of this research line.\n\n\\section{Method}\n\nIn this section, we elaborate on the details of our \\modelname{}. Following WizardLM and PRMs~\\citep{lightman2023openai-verify-step-by-step}, we propose \\REInameF{} (\\textbf{\\REInameS{}}) method, which integrates the math \\EIname{} and reinforced instruction and process supervision to evolve GSM8k and MATH, and fine-tune the pre-trained language models with the evolved data and reward models. \n\n\\subsection{Math Evol-Instruct}\n\nMotivated by the Evol-Instruct~\\citep{xu2023wizardlm} method proposed by WiazrdLM and its effective application on WizardCoder~\\citep{luo2023wizardcoder}, this work attempts to make math instructions with various complexities and diversity to enhance the pre-trained LLMs. Specifically, we adapt Evol-Instruct to a new paradigm including two evolution lines:\n\n1) Downward evolution: It enhances instructions by making the questions easier. For example i): revising high difficulty questions to lower difficulty, or ii) producing a new and easier question with another different topic.\n\n2) Upward evolution:  Derived from original Evol-Instruct method, it deepens and generates new and harder questions by i) adding more constraints, ii) concretizing,  iii) increasing reasoning.\n\nThe complete prompts of above evolution are shown in Appendix \\ref{appendix:evol_prompts}. For each instruction, we use GPT-4 to evolve 5 rounds (2 downward and 3 upward) of new instructions progressively, each new one is generated by the previous round of evolution.\n    \n\n\\subsection{Reward Models}\\label{section:Reward Models}\n\nConsidering the necessity of quality control for evolved instructions  and inspired by PRMs~\\citep{lightman2023openai-verify-step-by-step}, we train two reward models to predict the quality of the instructions and  the correctness of each step in the answer respectively:\n\\paragraph{Instruction Reward Model (IRM)} This model aims to judge the quality of the evolved instructions on two aspects: i) Difficulty, and ii) Definition. To produce the ranking list training data of IRM, we leverage GPT-4 to rank the quality between those evolved instructions and original instruction. The one with high difficulty and clear definition will deserve a higher ranking. The detailed prompt of above ranking process is shown in the Appendix \\ref{appendix:IRM_prompts}. \n\nSpecifically, given an math instructions $q$, IRM ($Q \\rightarrow \\mathbb{R} $) assigns a score to $q$ to indicate its quality. We optimize ORM via the following pairwise ranking loss: \n\\begin{align}\n\\mathcal{L}_{IRM}= - \\log \\sigma(r^q_j - r^q_k - m)\\\n\\label{eq:pairwise-loss}\n\\end{align}\nwhere $r^q_j$ is the reward of chosen instruction and  $r^q_k$ is the reward of rejected instruction, $m$ is the margin.\n\n\\paragraph{Process-supervised Reward Model (PRM)} As there is no simple way to support highly precise process supervision without professional and expensive human-labelers, we depend on GPT-4 to provide process supervision, and ask it to assess the correctness of each step in the solutions generated by our model to produce PRM training data. The detailed prompt of above step level labeling process is shown in the Appendix \\ref{appendix:PRM_prompts}. \n\nFor exactly, given an math instructions $q$ and its answer $a$,  PRM ($Q \\times A \\rightarrow \\mathbb{R}^{+}$) assigns a score to each step of $a$, we train PRM with the following cross-entropy loss:\n\\begin{align}\n\\mathcal{L}_{PRM}= \\sum_{i=1}^{L} y_i \\log r^a_i + (1 - y_i) \\log(1 - r^a_i)\\\n\\label{eq:prm-loss}\n\\end{align}\nwhere $L$ is the reasoning steps of answer $a$. $y_i$ is the ground-truth label of the $i$-th step of answer $a$, $y_i = 1$ if $a_i$ is correct, otherwise $y_i = 0$. $r^a_i$ is the reward score (assigned by PRM) of the $i$-th step of answer $a$.\n\n\\subsection{Reinforcement Learning with IRM and PRM}\n\nImmediately, we exploit reinforcement learning to optimize LLMs. Following ~\\citep{lightman2023openai-verify-step-by-step}, we employ step by step Proximal Policy Optimization (PPO) to reward both instruction and each reasoning step.\n\nFor each math instruction $q$ and generated answer $a$, we use IRM to assign instruction reward $r^q$, and use the minimum score across all reasoning steps to represent the final reward score $r^a$ of the answer $a$ assigned by PRM. Then we apply a product as the final reward of this instruction-answer pair:\n\\begin{align}\nr = r^q \\cdot r^a \\\n\\label{eq:final-loss}\n\\end{align}\n\n\\subsection{PRM for Verification}\nFollowing ~\\citep{lightman2023openai-verify-step-by-step} and ~\\citep{li-etal-2023-making}, we leverage both majority voting and PRM verifier to  aggregate the predictions of different reasoning paths.\n\\begin{align}\n\\hat{a} = \\mathop{\\arg\\max}_{a} \\sum_{i=1}^{N} \\mathbb{I}_{a_i = a} \\cdot PRM(q, a_i)\\\n\\label{eq:loss}\n\\end{align}\nwhere $PRM(q, a_i)$ is the score of the $i$-th reasoning path assigned by PRM for instruction $q$.  $\\mathbb{I}_{a_i = a}$ is an indicator function that returns 1(or 0) if $a_i = a$. \n\n\\section{Experiment}\n\nThis section provides a comprehensive overview of the advanced models. Subsequently, we mainly elucidate the performance metrics of our models on two prevalent mathematical benchmarks from grade to high school problems: GSM8k~\\citep{cobbe2021training_gsm8k_2} and MATH~\\citep{hendrycks2021measuring}.\n\n\\subsection{Experimental Setup} \\label{sections: exper-setup}\n\n\\textbf{SFT Training Data.} Firstly, use the GSM8k and MATH training sets as the initial seed collection, then employ both upward and downward math Evol-Instruct approach for five rounds. Each round need to evolve the initial instructions 6 times, and the temperature parameter is set to 0.7. Next, we remove duplicate instructions 17k. Hence, a total of 448k unique instructions were obtained. Subsequently, 30k data were excluded by the data filtering method to avoid contamination, ultimately leaving 418k data. Finally, we use GPT-4-0613 to generate the answer with a step-by-step format, and leverage them for supervised fine-tuning.\n\n\\textbf{Reward Models Training Data.} \nTo train the reward models, We conducted additional 5 rounds of evolution on the initial instruction set and obtain 90k instructions. we use GPT-4-0613 to rank each instruction list with the quality  from 1 to 6 as the training data of IRM. To obtain the training data of PRM, We use our Llama-2 70B SFT model to generate 5 answers for each instruction, and GPT-4-0613 is employed to assign correctness judgement for each reasoning step.\n\n\\textbf{Implementation Details.}\nWe employ our method on two open-source foundational models Llama 2~\\citep{touvron2023llama2} and Mistral-7B~\\citep{jiang2023mistral}. Llama 2 encompasses three distinct parameter sizes: 7B, 13B, and 70B. We utilize GPT-4-0613 for instruction evolution and  the training data construction of reward models. For SFT, we train 3 epochs, and the learning rate is 2e-5, 1e-5 and 5e-6 for Llama 2 7B/13B, 70B and Mistral-7B. The batch size is 512, and the sequence length is 2048.\n For the reward model, we train Llama 2 and Mistral-7B with learning rate 4e-6 and 1e-6 for one epoch. For RL, the lr is 4e-7 and 1e-7 for Llama 2 and Mistral-7B and train one epoch.\n\n    \n\n\\clearpage\n\\begin{wraptable}{r}{0.53\\textwidth}\n\\vspace{-1.3cm}\n    \\centering\n    \\caption{The models' CoT pass@1 results  on GSM8k and MATH without using any external python tool.}\n    \\scalebox{0.55}{\n    \\begin{tabular}{lccll}\n    \\toprule\n    \\textbf{Model} & \\textbf{Base} & \\textbf{Params} & \\textbf{GSM8k} \n   &\\textbf{MATH}  \\\\ \n    \\midrule\n    \\multicolumn{5}{c}{Proprietary models}\\\\\n    \\midrule\n    GPT-o1~\\citep{openai2023gpt4} & - & - & - & 94.8 \\\\  \n    GPT-o1-mini & - & - & - & 90.0 \\\\  \n    Gemini-1.5 002 & - & - & - & 86.5 \\\\  \n    Claude 3.5 Sonnet~\\citep{bai2022constitutional-claude} & - & - & 96.4 & 71.1 \\\\  \n    GPT-4o-2024-0513 & - & - & 96.1 & 76.6 \\\\  \n    GPT-4-turbo-0125~\\citep{openai2023gpt4} & - & - & 94.2 & 64.5 \\\\  \n    GPT-4-0314 & - & - & 94.7 & 52.6 \\\\  \n    GPT-4 (original version) & - & - & 92.0 & 42.5 \\\\  \n    Baichuan-3~\\citep{yang2023baichuan-2} & - & - & 88.2 & 49.2 \\\\  \n    GLM-4~\\citep{glm2024chatglm-4} & - & - & 87.6 & 47.9 \\\\  \n    Gemini Pro~\\citep{geminiteam2023gemini} & - & - & 86.5 & 32.6 \\\\  \n    Claude2 & - & - & 85.2 & 32.5 \\\\  \n    GPT-3.5-Turbo & - & - & 81.6 & 43.1 \\\\  \n    PaLM2~\\citep{palm2} & - & - & 80.7 & 34.3 \\\\  \n    Minerva~\\citep{lewkowycz2022solving} & - & 540B & 58.8 & 33.6 \\\\\n    GPT3.5~\\citep{GPT3} & - & - & 57.1 & - \\\\\n    \\midrule\n    \\multicolumn{5}{c}{Open-Source Models (0.1B-3B)}\\\\\n    \\midrule  \n    GPT-2-Small~\\citep{brown2020language_gpt2} & - & 0.1B & 6.9 & 5.4 \\\\   \n    GPT-2-Medium~\\citep{brown2020language_gpt2} & - & 0.3B & 11.2 & 6.2 \\\\  \n    GPT-2-Large~\\citep{brown2020language_gpt2} & - & 0.7B & 13.6 & 6.4 \\\\\n    GPT-2-XL~\\citep{brown2020language_gpt2} & - & 1.5B & 15.4 & 6.9 \\\\\n    \\rowcolor{gray!30}\n    WizardMath-GPT & GPT-2-Small & 0.1B & 26.4 & 12.3 \\\\ \n    \\rowcolor{gray!30}\n    WizardMath-GPT & GPT-2-Medium & 0.3B & 38.7 & 15.6 \\\\  \n    \\rowcolor{gray!30}\n    WizardMath-GPT & GPT-2-Large & 0.7B & 50.1 & 21.2 \\\\  \n    \\rowcolor{gray!30}\n    WizardMath-GPT & GPT-2-XL & 1.5B & 58.9 & 25.4 \\\\  \\\\[-0.7em]  \\hdashline \\\\[-0.7em]\n    \\rowcolor{gray!30}\n    WizardMath-Qwen & Qwen-Math-2.5 & 1.5B & 86.7 & 68.6 \\\\  \n\n    \\\\[-0.7em]  \\hdashline \\\\[-0.7em]\n    \n    Llama-3.2-Instruct~\\citep{dubey2024-llama3} & Llama 3.2 & 1B & 44.4 & 30.6 \\\\  \n    \\rowcolor{gray!30}\n    WizardMath-Llama & Llama 3.2 & 1B & 63.3 & 33.5 \\\\  \n    Llama-3.2-Instruct & Llama 3.2 & 3B & 77.7 & 48.0 \\\\ \n    \\rowcolor{gray!30}\n    WizardMath-Llama & Llama 3.2 & 3B & 85.5 & 49.9 \\\\  \n    \\midrule\n    \\multicolumn{5}{c}{Open-Source Models (7B-8B)}\\\\\n    \\midrule   \n    Llama-2~\\citep{touvron2023llama2} & - & 7B & 14.6 & 2.5 \\\\  \n    MAmmoTH-CoT~\\citep{yue2023mammoth} & Llama-2 & 7B & 50.5 & 10.4 \\\\  \n    MathScale~\\citep{tang2024mathscale} & Llama-2 & 7B & 66.3 & 31.1 \\\\  \n    MetaMath~\\citep{yu2023metamath} & Llama-2 & 7B & 66.5 & 19.8 \\\\  \n    MuggleMath~\\citep{Li2023mugglemath} & Llama-2 & 7B & 68.4 & - \\\\  \n    Skywork-Math~\\citep{zeng2024-skywork-math} & Llama-2 & 7B & 72.9 & 47.7 \\\\  \n    Math-Shepherd~\\citep{wang2024-mathshepherd} & Llama-2 & 7B & 73.2 & 21.6 \\\\  \n    Xwin-Math~\\citep{li2024-Xwin-math} & Llama-2 & 7B & 82.6 & 40.6 \\\\ \n    \\rowcolor{gray!30}\n    WizardMath-Llama & Llama-2 & 7B & 84.1 & 43.5 \\\\ \\\\[-0.7em]  \\hdashline \\\\[-0.7em]\n    Mistral-v0.1~\\citep{jiang2023mistral} & - & 7B & 42.9 & 12.9 \\\\  \n    MathScale~\\citep{tang2024mathscale} & Mistral-v0.1 & 7B & 74.8 & 35.2 \\\\  \n    MMIQC~\\citep{liu2024-MMIQC} & Mistral-v0.1 & 7B & 74.8 & 36.0 \\\\  \n    MetaMath~\\citep{yu2023metamath} & Mistral-v0.1 & 7B & 77.9 & 28.6 \\\\  \n    KPMath-Plus~\\citep{huang2024-KPMath} & Mistral-v0.1 & 7B & 82.1 & 46.8 \\\\  \n    DART-Math~\\citep{tong2024-DART} & Mistral-v0.1 & 7B & 82.6 & 43.5 \\\\  \n    Skywork-Math~\\citep{zeng2024-skywork-math} & Mistral-v0.1 & 7B & 83.9 & 51.2 \\\\  \n    Math-Shepherd~\\citep{wang2024-mathshepherd} & Mistral-v0.1 & 7B & 84.1 & 33.0 \\\\  \n    MAmmoTH2-Plus~\\citep{yue2024mammoth2} & Mistral-v0.1 & 7B & 84.7 & 45.0 \\\\  \n    JiuZhang3.0~\\citep{zhou2024jiuzhang3} & Mistral-v0.1 & 7B & 88.6 & 52.8 \\\\  \n    Xwin-Math~\\citep{li2024-Xwin-math} & Mistral-v0.1 & 7B & 89.2 & 43.7 \\\\  \n    \\rowcolor{gray!30}\n    WizardMath-Mistral & Mistral-v0.1 & 7B & 90.7 & 55.4 \\\\  \n    \\rowcolor{gray!30}\n    WizardMath-Mistral & Mistral-v0.3 & 7B & 90.4 & 55.6 \\\\  \n    \\rowcolor{gray!30}\n    WizardMath-Mathstral & Mathstral-v0.1 & 7B & 93.8 & 70.9 \\\\  \\\\[-0.7em]  \\hdashline \\\\[-0.7em]\n    \\rowcolor{gray!30}\n    WizardMath-Qwen & Qwen2.5-Math & 7B & 93.9 & 77.8 \\\\  \n    \\rowcolor{gray!30}\n    WizardMath-Qwen & Qwen2.5 & 7B & 94.0 & 74.5 \\\\  \\\\[-0.7em]  \\hdashline \\\\[-0.7em]\n    DeepSeekMath-Base~\\citep{shao2024-deepseekmath} & - & 7B & 64.2 & 36.2 \\\\  \n    NuminaMath-CoT~\\citep{li2024-numinamath} & DeepseekMath & 7B & 75.4 & 55.2 \\\\  \n    MMIQC~\\citep{liu2024-MMIQC} & DeepSeekMath & 7B & 79.0 & 45.3 \\\\  \n    KPMath-Plus~\\citep{huang2024-KPMath} & DeepSeekMath & 7B & 83.9 & 48.8 \\\\  \n    DeepSeekMath-RL~\\citep{shao2024-deepseekmath} & DeepSeekMath & 7B & 88.2 & 51.7 \\\\  \n    DART-Math~\\citep{tong2024-DART} & DeepSeekMath & 7B & 88.2 & 52.9 \\\\ \n    \\rowcolor{gray!30}\n    WizardMath-DeepSeek & DeepSeekMath & 7B & 91.0 & 64.6 \\\\  \\\\[-0.7em]  \\hdashline \\\\[-0.7em]\n    MetaMath~\\citep{yu2023metamath} & Llama 3 & 8B & 77.3 & 20.6 \\\\  \n    MMIQC~\\citep{liu2024-MMIQC} & Llama 3 & 8B & 77.6 & 29.5 \\\\  \n    DART-Math~\\citep{tong2024-DART} & Llama 3 & 8B & 82.5 & 45.3 \\\\   \n    MAmmoTH2-Plus~\\citep{yue2024mammoth2} & Llama 3 & 8B & 84.1 & 42.8 \\\\  \n    Llama 3.1-Instruct~\\citep{dubey2024-llama3} & Llama 3 & 8B & 84.5 & 51.9 \\\\ \n    JiuZhang3.0~\\citep{zhou2024jiuzhang3} & Llama 3 & 8B & 88.6 & 51.0 \\\\  \n    \\rowcolor{gray!30}\n    WizardMath-Llama & Llama 3 & 8B & 90.3 & 58.8 \\\\  \n    \\midrule\n    \\multicolumn{5}{c}{Open-Source Models (13B)}\\\\\n    \\midrule  \n    Llama-2~\\citep{touvron2023llama2} & - & 13B & 28.7 & 3.9 \\\\  \n    MAmmoTH-CoT~\\citep{yue2023mammoth} & Llama 2 & 13B & 56.3 & 12.9 \\\\  \n    MathScale~\\citep{tang2024mathscale} & Llama 2 & 13B & 71.3 & 33.8 \\\\  \n    MetaMath~\\citep{yu2023metamath} & Llama 2 & 13B & 72.3 & 22.4 \\\\  \n    MuggleMath~\\citep{Li2023mugglemath} & Llama 2 & 13B & 74.0 & - \\\\  \n    KPMath-Plus~\\citep{huang2024-KPMath} & Llama 2 & 13B & 81.6 & 41.0 \\\\  \n    Xwin-Math~\\citep{li2024-Xwin-math} & Llama 2 & 13B & 88.1 & 44.9 \\\\  \n    \\rowcolor{gray!30}\n    WizardMath-Llama & Llama 2 & 13B & 89.7 & 50.6 \\\\  \n    \\midrule\n    \\multicolumn{5}{c}{Open-Source Models (70B)}\\\\\n    \\midrule \n    Llama-2~\\citep{touvron2023llama2} & - & 70B & 56.8 & 13.5 \\\\  \n    MAmmoTH-CoT~\\citep{yue2023mammoth} & Llama-2 & 70B & 72.4 & 21.1 \\\\  \n    MetaMath~\\citep{yu2023metamath} & Llama-2 & 70B & 82.3 & 26.6 \\\\  \n    KPMath-Plus~\\citep{huang2024-KPMath} & Llama-2 & 70B & 87.4 & 48.6 \\\\  \n    Xwin-Math~\\citep{li2024-Xwin-math} & Llama-2 & 70B & 90.6 & 52.8 \\\\  \n    \\rowcolor{gray!30}\n    WizardMath-Llama & Llama-2 & 70B & 92.8 & 58.6 \\\\ \n    \\bottomrule\n    \\end{tabular}\n    }\n\n    \\label{tab:gsm8k_math_merge}\n    \\vspace{-2cm}\n\\end{wraptable}\n\n\\subsection{Main Results}\n\nTable ~\\ref{tab:gsm8k_math_merge} shows the CoT~\\citep{wei2022chain} pass@1 results of the current state-of-the-art models on GSM8k and MATH. In this study, to ensure equitable and cohesive evaluations, we report the socres of all models within the settings of \\textbf{greedy decoding and CoT without using any external python tool}. \n\n\\paragraph{Comparing with the proprietary Models.} As shown in the Table~\\ref{tab:gsm8k_math_merge}, our \\emph{\\textbf{WizardMath}} demonstrates notable superiority over various proprietary LLMs on the  GSM8k and MATH benchmarks in terms of pass@1:\n\n1) \\emph{\\textbf{WizardMath-Llama 70B}}, the largest model, demonstrated exceptional performance on the GSM8k and MATH , surpassing earlier versions of GPT-4, Claude-2, and Gemini Pro, and performing on par with GPT-4-0314. It significantly outperformed GPT-3.5-Turbo by 11.2\\% on GSM8k and by 15.5\\% on MATH.\n\n2) \\emph{\\textbf{WizardMath-Mistral 7B}}, the smaller-sized model, outperformed Baichuan 3 on GSM8k (90.7 vs. 87.6) and surpassed GPT-4-0314 on MATH (55.4 vs. 52.6), significantly exceeding the performance of GPT-3.5-Turbo and Gemini Pro. Meanwhile, WizardMath-Mathstral, trained on Mathstral-7B-v0.1, demonstrated performance comparable to GPT-4-turbo-0125. Additionally, WizardMath-Qwen, trained on Qwen2.5-Math, surpassed GPT-4-2024-0513 on MATH (77.8 vs. 76.6).\n\n\\paragraph{Comparing with the Open-Source Models.} The results presented in Table~\\ref{tab:gsm8k_math_merge} unequivocally indicate that our \\emph{\\textbf{WizardMath-Llama 70B}} exhibits a significant performance superiority over strong models in both the GSM8k and MATH benchmarks with higher data efficiency across the range from 0.1B to 70B parameters. The detailed results are as follows:\n\n1) With the same model parameter size, our model surpasses the previous best model such as MetaMath, MAmmoTH2-Plus, Xwin-Math. Particularly, \\emph{\\textbf{WizardMath-Llama 70B}} achieves a substantial improvement of 10.5\\% on GSM8K and 32.0\\% on MATH compared to MetaMath-Llama 70B in testing accuracy.  In the Table ~\\ref{tab:math_topics}, we show the detailed results of MATH subtopics with our WizardMath 70B model.  Specifically, \\emph{\\textbf{WizardMath-Mistral 7B}} also surpasses top-tier open source models, outperforming MetaMath-Mistral 7B with a notable margin (90.7 vs 77.9 on GSM8k) and (55.4 vs 28.6 on MATH). It demonstrats the effectiveness of our RLEIF method in enhancing mathematical reasoning capabilities across a range of problem difficulties, from grade to high school levels.\n\n2) By employing diverse pre-trained models (i.e., GPT-2, Llama 2, Mistral, Qwen, DeepSeek) as base models, WizardMath demonstrated notable advancements on the GSM8k and MATH benchmarks. Specifically, WizardMath-Llama2-7B, based on Llama2-7B, improved performance by 69.5\\% on GSM8k and 41.0\\% on MATH. Similarly, WizardMath-GPT2-XL, built on GPT2-XL, achieved a 43.5\\% improvement on GSM8k and 18.5\\% on MATH, performing on par with Llama2-70B and outperforming GPT-3.5 on GSM8k. This demonstrates that our RLEIF method is equally effective for smaller models in enhancing mathematical reasoning capabilities, proving its scalability and robustness across various model backbones.\n\n  \n\n   \n\n\\begin{table}[t]\n    \\centering\n    \\begin{minipage}{.45\\textwidth}\n        \\centering\n        \\caption{Results of pass@1 (\\%) on MATH subtopics (i.e., Intermediate Algebra, Geometry) with WizardMath 70B model. }\n        \\scalebox{0.9}{\n        \\begin{tabular}{lcc}\n            \\toprule\n            \\textbf{MATH subtopics} & \\textbf{WizardMath 70B}\\\\\n            \\midrule\n            Intermediate Algebra & 36.3\\\\\n            Precalculus & 38.9\\\\\n            Geometry & 48.3\\\\\n            Number Theory & 58.5\\\\\n            Counting \\& Probability & 54.8\\\\\n            Prealgebra & 74.6\\\\\n            Algebra & 78.5\\\\\n            \\midrule\n            Overall & \\textbf{58.6}\\\\\n            \\bottomrule\n        \\end{tabular}}\n        \\label{tab:math_topics}\n    \\end{minipage}%\n    \\hfill\n    \\begin{minipage}{.52\\textwidth}\n        \\centering\n        \\caption{Explore the effects of PRM and IRM during PPO training. }\n        \\small\n        \\renewcommand{\\arraystretch}{1.06}\n        \\scalebox{0.8}{\n        \\begin{tabular}{lcc}\n            \\toprule\n            \\textbf{Models}  &\\textbf{GSM8K}  & \\textbf{MATH}  \\\\\n            \\midrule\n            GPT-2-XL-1.5B: WizardMath-SFT & 51.9 & 18.3 \\\\\n            \\midrule\n            \\quad + PRM  & 55.8 & 22.1 \\\\\n            \\quad + PRM + IRM   & \\textbf{58.9} & \\textbf{25.4} \\\\\n            \\midrule\n            Llama2-7B: WizardMath-SFT & 77.4 & 35.6 \\\\\n            \\midrule\n            \\quad + PRM  & 81.7 & 39.9 \\\\\n            \\quad + PRM + IRM   & \\textbf{84.1} & \\textbf{43.5} \\\\\n            \\midrule\n            \\midrule\n            Mistral-7B: WizardMath-SFT & 82.8 & 48.1 \\\\\n            \\midrule\n            \\quad + PRM  & 87.2 & 52.7 \\\\\n            \\quad + PRM + IRM   & \\textbf{90.7} & \\textbf{55.4} \\\\\n            \\bottomrule\n        \\end{tabular}\n        }\n        \\label{tab:math_rl_instruction_rm}\n    \\end{minipage}\n\\end{table}\n\n\\subsection{ANALYSIS}\n\n\\begin{wrapfigure}{r}{0.55\\textwidth} % {r}{0.5\\textwidth} 调整 r 为 l, r, 或 c 以改变位置\n\\vspace{-16pt}\n\n\\centering\n     \\includegraphics[width=1\\linewidth]{Figures/data_scale_figure.pdf}\n     \\vspace{-0.6cm}\n     \\caption{Accuracy of Mistral-7B fine-tuned\nin different sizes of augmentation data on GSM8K and MATH }\n    \\vspace{-0.5cm}\n     \\label{fig:data_scale}\n\\end{wrapfigure}\n\n\\textbf{The impact of training data size}\n\nWe are curious about to how the training data size of different dataset construction methods impact the reasoning capacity of LLMs. Thus we conduct different number of training instances from ours evolved data and MetaMathQA to fine tune Mistral 7B. As shown in the Figure \\ref{fig:data_scale}, Math Evol-Instruct achieves superior data efficiency. Specifically, our model  constantly  outperforms MataMath by more than 3\\% $\\sim$ 6\\% on GSM8k and 15\\% $\\sim$ 20\\% on MATH under the same number of conditions. Our findings indicate that Math Evol-Instruct exhibits a higher potential upper bound compared to MetaMath, thus demonstrating the effectiveness of Evol-Instruct for math reasoning senario.\n\n\\textbf{The impact of PRM and IRM during PPO training}\n\nTo verify the contributions of the instruction reward model and process-supervised reward model, we consider the following variants: (1) SFT + PRM: only use PRM in the PPO training. (2) SFT + PRM + IRM: use both IRM and PRM in the PPO training.  As shown in Table \\ref{tab:math_rl_instruction_rm}, applying PRM alone for PPO training on GSM8k and MATH yields a 3\\%-4\\% improvement. When combined with IRM, an additional 2.5\\%-4\\% gain is observed. Thus, the integration of PRM and IRM results in a substantial overall improvement of 6\\%-8\\%. So, we can conclude that (1) PRM is crucial to WizardMath, since the variant with PRM significantly outperforms the SFT one without any PPO training (2) IRM also plays a key role in the success of reinforcement learning, as there is a remarkable improvement when we combine PRM with IRM, further demonstrating the necessity of taking instruction's quality into account and correcting false positives in the problem-solving process when we optimize the LLMs.\n\n\\begin{table}[h]\n    \\centering\n    \\begin{minipage}[t]{0.48\\textwidth}\n        \\centering\n        \\caption{The effect of different reward models during PPO training}\n        \\small\n        \\scalebox{0.95}{\n        \\begin{tabular}{lcc}\n        \\toprule\n         \\textbf{Models}  &\\textbf{GSM8K}  & \\textbf{MATH}  \\\\\n         \\midrule\n        Llama2-7B: WizardMath-SFT  & 77.4 & 35.6   \\\\\n        \\midrule\n        \\quad + ORM (ours) & 79.1 & 36.8  \\\\\n        \\quad + PRM800k   & 79.7 & 38.7 \\\\\n        \\quad + Math-Shepherd   & 80.3 & 38.2 \\\\\n        \\quad + PRM (ours)  & \\textbf{81.7} & \\textbf{39.9} \\\\\n        \\midrule\n         \\midrule\n        Mistral-7B: WizardMath-SFT  & 82.8 & 48.1   \\\\\n        \\midrule\n        \\quad + ORM (ours) & 84.6 & 49.6  \\\\\n        \\quad + PRM800k   & 85.4 & 50.8 \\\\\n        \\quad + Math-Shepherd   & 86.1 & 50.3 \\\\\n        \\quad + PRM (ours)  & \\textbf{87.2} & \\textbf{52.7} \\\\\n        \\bottomrule\n        \\end{tabular}\n        }\n        \\label{tab:math_rl_greedy_decoding}\n    \\end{minipage}\n    \\hfill\n    \\begin{minipage}[t]{0.48\\textwidth}\n        \\centering\n        \\caption{Results of reinforcement learning combined with validation. The SFT and Reward models are trained based on Mistral-7B. The verifier is based on 256 sample outputs.}\n        \\small\n        \\scalebox{0.95}{\n        \\begin{adjustbox}{max width=\\textwidth}\n        \\begin{tabular}{clcc}\n        \\toprule\n         \\textbf{Generators}  & \\textbf{Verifiers} & \\textbf{GSM8K}  & \\textbf{MATH}  \\\\\n         \\midrule\n        \\multirow{3}{*}{\\rotatebox{0}{ SFT}} &  Self-Consistency   & 90.7 & 57.5\\\\\n         & ORM  & 93.0 & 58.3  \\\\\n        & PRM & 93.9  &  61.7  \\\\\n        \\midrule\n        \\multirow{3}{*}{\\rotatebox{0}{ SFT + ORM}} &  Self-Consistency & 91.2 & 57.7  \\\\\n        & ORM  & 93.4 &  59.4 \\\\\n        & PRM & 94.1 & 63.3 \\\\\n\n        \\midrule\n        \\multirow{3}{*}{\\rotatebox{0}{SFT + PRM}} &  Self-Consistency & 92.3 & 59.3  \\\\\n        & ORM  & 94.1 &  60.8 \\\\\n        & PRM & \\textbf{95.2} & \\textbf{64.7}  \\\\\n        \\bottomrule\n        \\end{tabular}\n        \\end{adjustbox}\n        }\n        \\label{tab:math_rl_verifier}\n    \\end{minipage}\n\\end{table}\n\n\\begin{wraptable}{r}{0.57\\textwidth}\n\\centering\n\\footnotesize % Sets the font size to footnotesize\n\\vspace{-0.6cm}\n\\caption{\\footnotesize Impact of different Downward and Upward Evol-Instruct turns on {Mistral-7B} SFT. \\textit{D-i} refers to the \\textit{i} round of downward evolution, whereas \\textit{U-i} denotes the \\textit{i} round of upward evolution. \\textit{Ori} is the original manually annotated 7.5k data of GSM8k and MATH.}\n\\vspace{0.4cm}\n\\scalebox{0.68}{ % Scales the table; adjust scale as needed\n    \\setlength{\\tabcolsep}{2.5pt} % Adjusts the space between columns\n    \\renewcommand{\\arraystretch}{1.2} % Adjusts the space between rows\n    \n    \\begin{tabular}{c|ccccccc|ccccccc}\n    \\hline\n        \\multirow{2}{*}{Data} & \\multicolumn{7}{c|}{GSM8K} & \\multicolumn{7}{c}{MATH} \\\\\n& \\textbf{Ori} & \\textbf{D-1} & \\textbf{D-2} & \\textbf{U-1} & \\textbf{U-2} & \\textbf{U-3} & \\textbf{pass@1} & \\textbf{Ori} & \\textbf{D-1} & \\textbf{D-2} & \\textbf{U-1} & \\textbf{U-2} & \\textbf{U-3} & \\textbf{pass@1} \\\\\n    \\hline\nOri & \\checkmark & \\xmark & \\xmark & \\xmark & \\xmark & \\xmark & 59.7 & \\checkmark & \\xmark & \\xmark & \\xmark & \\xmark & \\xmark & 15.1 \\\\\n\\midrule\n\\multirow{9}{*}{ \\makecell{Math \\\\Evol}} & \\checkmark & \\checkmark & \\xmark & \\xmark & \\xmark & \\xmark & 71.9 & \\checkmark & \\checkmark & \\xmark & \\xmark & \\xmark & \\xmark & 30.3 \\\\\n & \\checkmark & \\xmark & \\checkmark & \\xmark & \\xmark & \\xmark & 70.5 & \\checkmark & \\xmark & \\checkmark & \\xmark & \\xmark & \\xmark & 28.7 \\\\\n & \\checkmark & \\xmark & \\xmark & \\checkmark & \\xmark & \\xmark & 73.7 & \\checkmark & \\xmark & \\xmark & \\checkmark & \\xmark & \\xmark & 33.4 \\\\\n & \\checkmark & \\xmark & \\xmark & \\xmark & \\checkmark & \\xmark & 71.6 & \\checkmark & \\xmark & \\xmark & \\xmark & \\checkmark & \\xmark &  32.6\\\\\n & \\checkmark & \\xmark & \\xmark & \\xmark & \\xmark & \\checkmark & 70.2 & \\checkmark & \\xmark & \\xmark & \\xmark & \\xmark & \\checkmark & 30.9 \\\\\n & \\checkmark & \\checkmark & \\checkmark & \\xmark & \\xmark & \\xmark & \\textbf{74.5} & \\checkmark & \\checkmark & \\checkmark & \\xmark & \\xmark & \\xmark & \\textbf{34.7} \\\\\n & \\checkmark & x & x & \\checkmark & \\checkmark & x & 77.1 & \\checkmark & x & x & \\checkmark & \\checkmark & x & 38.6 \\\\\n & \\checkmark & x & x & \\checkmark & \\checkmark & \\checkmark & \\textbf{78.6} & \\checkmark & x & x & \\checkmark & \\checkmark & \\checkmark & \\textbf{42.5} \\\\\n & \\checkmark & \\checkmark & \\checkmark & \\checkmark & \\xmark & \\xmark & 76.6 & \\checkmark & \\checkmark & \\checkmark & \\checkmark & \\xmark & \\xmark & 40.3 \\\\\n & \\checkmark & \\checkmark & \\checkmark & \\checkmark & \\checkmark & \\xmark & 79.8 & \\checkmark & \\checkmark & \\checkmark & \\checkmark & \\checkmark & \\xmark & 44.6 \\\\\n & \\checkmark & \\checkmark & \\checkmark & \\checkmark & \\checkmark & \\checkmark & \\textbf{81.2} & \\checkmark & \\checkmark & \\checkmark & \\checkmark & \\checkmark & \\checkmark & \\textbf{46.2} \\\\\n    \\hline\n    \\end{tabular}\n}\n\\label{exp:abl-effect-evol-gsm8k}\n\\end{wraptable}\n\n\\textbf{The impact of Evol-Instruct turns}. Table~\\ref{exp:abl-effect-evol-gsm8k} illustrates the impact of combining downward and upward evolution in SFT training. Two rounds of downward evolution improved GSM8k by 14.8\\% (74.5 vs. 59.7) and MATH by 19.6\\% (34.7 vs. 15.1) over the original. Three rounds of upward evolution yielded a 18.9\\% improvement on GSM8k (78.6 vs. 59.7) and a 27.4\\% improvement on MATH (42.5 vs. 15.1). Furthermore,  combining downward evolution based on upward evolution resulted in an additional 2.6\\% improvement on GSM8k (81.2 vs. 78.6), a total improvement of 21.5\\% over the original. Similarly, a 1.9\\% improvement on MATH (46.5 vs. 42.5), a 31.4\\% total improvement. These results underscore the complementary and significant effectiveness of upward and downward evolution.\n\n\\textbf{ORM v.s. PRM; Human v.s. AI.} The Table \\ref{tab:math_rl_greedy_decoding} presents the performance of different answer reward methods for LLMs in terms of pass@1. As is shown: 1) Our step-by-step PRM significantly enhances the performance of both Llama and Mistral based SFT models.  Specifically, the Mistral-7B powered by our PRM achieves 87.2\\% and 52.7\\% on GSM8k and MATH respectively. 2)  PRM models consistently outperforms ORM on both GSM8k and MATH, indicating the effectiveness of step-by-step supervision. 3) The PRM trained on our fully AI-labeled data  outperforms both the manually annotated PRM800k and Math-Shepherd, which utilizes MCTS tree search for annotation. When training WizardMath-Mistral-SFT with PPO, our PRM  improves upon PRM800k by 1.8\\% and Math-Shepherd by 1.1\\% on GSM8k, while surpassing PRM800k by 1.9\\% and Math-Shepherd by 2.4\\% on MATH. This demonstrates powerful AI can also provide good  process supervision quality, highlighting the effectiveness of utilizing AI to construct PRM training data. \n\n\\textbf{PRM as Verifier.} Table \\ref{tab:math_rl_verifier} presents the performance comparison of various generators with different verifiers on GSM8K and MATH in terms of pass@256. We find that: 1) PRM verifier consistently demonstrates superior performance compared to Self-Consistency and ORM. Specifically, our SFT + PRM generator, enhanced by the PRM verifier, achieves 95.2\\% and 64.7\\% accuracy on GSM8K and MATH respectively. 2) When compared to ORM, PRM exhibits a more significant advantage on the more challenging MATH dataset which aligns with the findings in ~\\citep{uesato2022deepmind-orms} and ~\\citep{lightman2023openai-verify-step-by-step}. This can be attributed to the fact that GSM8K involves fewer and less complex steps in problem-solving than MATH. 3) Particularly, the generator with PRM PPO training  surpasses those SFT and ORM PPO trained generators regardless of employing Self-Consistency, ORM, and the PRM verifiers. This further demonstrates the effectiveness of our PRM.\n\n       \n\n\\begin{figure}[t]\n\\centering\n\\subfigure{\n     \\includegraphics[width=0.48\\linewidth,trim=14 0 33 10,clip]{Figures/GSM8k_BON_figure.pdf}\n}\n\\subfigure{\n     \\includegraphics[width=0.48\\linewidth,trim=14 0 33 10,clip]{Figures/MATH_BON_figure.pdf}\n}\n\n\\caption{Performance of Mistral-7B SFT with different verification strategies.}\n\\label{fig:math_gsm8k_sample_n}\n\\end{figure}\n\nFigure \\ref{fig:math_gsm8k_sample_n} also shows the performance of different Verification strategies across a range of candidate numbers from 1 to 256 on two benchmarks. The main observations are as follows: 1) PRM verifiers consistently achieves superior performance compared to both ORM and majority voting, and this superiority becomes more evident as N increases. 2) For MATH benchmark, our PRM trained on the AI-annotated datasets slightly surpassed the human-annotated PRM800K.\n\n\\begin{wraptable}{r}{0.57\\textwidth}\n\\vspace{-0.7cm}\n\\caption{Performance of WizardMath  on the 7 out-of-domain evaluation results covering K-12, college, and competition level math problems.  The results of  models  in the table refer to {\\sc MwpBench} ~\\citep{tang2024mathscale}. ``AGIE'' stands for AGIEval. We report the models’ CoT pass@1 results on MwpBench without using any external python tool}\n\\centering\n\n\\scalebox{0.66}{\n\\footnotesize\n\\setlength{\\tabcolsep}{3pt}\n\\renewcommand{\\arraystretch}{1.15}\n\\begin{tabular}{lcccccccccccc}\n\\toprule\n\\textbf{Models}  & \\makecell[l]{\\textbf{College}\\\\\\textbf{Math}} & \\textbf{TAL} & \\makecell[l]{\\textbf{Math23k}} & \\makecell[l]{\\textbf{Ape210k}} & \\makecell[l]{\\textbf{Gaokao}\\\\\\textbf{Bench}\\\\\\textbf{Math}} & \\makecell[l]{\\textbf{AGIE}\\\\\\textbf{Gaokao}\\\\\\textbf{Math}} & \\makecell[l]{\\textbf{AGIE}\\\\\\textbf{SAT}\\\\\\textbf{Math}} & \\makecell[l]{\\textbf{AVG}}\\\\\n\\midrule\n\\multicolumn{13}{c}{\\textit{Proprietary models}} \\\\\n\\makecell[l]{GPT-4} &  \\textbf{24.4} & \\textbf{51.8} & \\textbf{76.5} & \\textbf{61.5} & \\textbf{35.4} & \\textbf{28.2} & \\textbf{68.6} & \\textbf{49.5}\\\\\n\\makecell[l]{GPT-3.5-Turbo}  & 21.6 & 42.9 & 62.5 & 44.0 & 23.2 & 15.3 & 55.8 & 37.9\\\\\n\\midrule\n\\multicolumn{13}{c}{\\textit{Models based on LLaMA-2 13B}} \\\\\n\\makecell[l]{LLaMA-2 13B}  & 1.2 & 6.3 & 9.5 & 7.9 & 0.7 & 0.4 & 6.8 & 4.7\\\\\n\\makecell[l]{MAmmoTH-CoT}  & 6.5 & 17.3 & 39.5 & 28.1 & 5.9 & 4.9 & 20.5  & 17.5\\\\\n\\makecell[l]{GAIR-Abel}  & 7.9 & 21.1 & 42.2 & 27.8 & 7.0 & 4.9 & 30.3  & 20.2\\\\\n\\makecell[l]{MetaMath}  & 10.1 & 25.4 & 48.6 & 31.6 & 9.6 & 5.6 & 38.2 & 24.2\\\\\n\\makecell[l]{MathScale 13B}  & 20.4 & 38.1 & 61.1 & 43.7 & 20.0 & 12.3 & 55.8 & 35.9\\\\\n\\rowcolor{gray!30}\nWizardMath  & \\textbf{22.9} & \\textbf{43.3} & \\textbf{70.3} & \\textbf{50.8} & \\textbf{33.1} & \\textbf{25.7} & \\textbf{64.7}  & \\textbf{44.4}\\\\\n\\midrule\n\\multicolumn{13}{c}{\\textit{Models based on LLaMA-2 7B}} \\\\\n\\makecell[l]{LLaMA-2 7B}  & 2.3 & 7.6 & 6.8 & 7.3 & 2.1 & 2.9 & 2.9  & 4.6\\\\\n\\makecell[l]{MAmmoTH-CoT}  & 6.2 & 13.3 & 34.6 & 21.4 & 3.9 & 2.7 & 19.6  & 14.5\\\\\n\\makecell[l]{GAIR-Abel}  & 6.6 & 18.3 & 35.4 & 24.5 & 4.3 & 4.4 & 23.5  & 16.7\\\\\n\\makecell[l]{MetaMath}  & 9.4 & 22.5 & 44.0 & 29.9 & 5.9 & 5.1 & 36.2 & 21.9\\\\\n\\makecell[l]{MathScale 7B}  & 20.9 & 35.2 & 59.0 & 41.8 & 19.6 & 12.6 & 57.8 & 35.3\\\\\n\\rowcolor{gray!30}\nWizardMath  & \\textbf{21.2} & \\textbf{40.2} & \\textbf{67.3} & \\textbf{46.1} & \\textbf{28.9} & \\textbf{18.7} & \\textbf{62.7}  & \\textbf{40.7}\\\\\n\\midrule\n\\multicolumn{13}{c}{\\textit{Models based on Mistral 7B}} \\\\\n\\makecell[l]{Mistral 7B}  & 7.5 & 17.9 & 18.5 & 15.5 & 6.2 & 5.9 & 22.5  & 13.4\\\\\n\\makecell[l]{MetaMath Mistral}  & 15.7 & 31.4 & 55.1 & 38.1 & 15.3 & 10.1 & 50.9  & 30.9\\\\\n\\makecell[l]{MathScale Mistral}  & 21.8 & 39.9 & 64.4 & 46.0 & 21.4 & 14.3 & 57.8  & 37.9\\\\\n\\rowcolor{gray!30}\nWizardMath Mistral  & \\textbf{24.8} & \\textbf{44.8} & \\textbf{71.2} & \\textbf{52.6} & \\textbf{37.2} & \\textbf{24.5} & \\textbf{64.7} & \\textbf{45.7} \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\label{tab:mwpbench}\n\\end{wraptable}\n\n\\textbf{Performance of Out-of-Domain.} Table \\ref{tab:mwpbench}  presents the results of WizardMath on the 7 out-of-domain evaluation results covering K-12, college, and competition level math problems, highlighting the following salient observations:  (1) With math Evol-Instruct and reinforcement learning, WizardMath consistently\nsurpasses prior state-of-the-art open-source models (e.g. MetaMath, MathScale) across all scales, and achieves improvement of 5\\%-10\\% across 7 tasks on average. (2) The accuracy of WizardMath-Mistral is about 5.0\\% higher than WizardMath-Llama on the same size. Especially it exceeds GPT-3.5-Turbo (45.7 vs. 37.9) while being comparable to GPT-4. This also indicates that Mistral-7B has more potential in mathematical reasoning. (3) Especially on difficult benchmarks (i.e., College Math, AGIE Gaokao Math), WizardMath outperforms MetaMath by a significant margin . This demonstrates our model and RLEIF method has stronger robustness and better significant generalization ability for invisible mathematical problems. \n\n\\paragraph{Employ Open-source Model to Math Evol-Instruct.} \nIn Table ~\\ref{tab: open_source_evol}, we investigate the use of open-source models (i.e., Llama-3-70B-Instruct) as a substitute for GPT-4 during the SFT stage for Evol Instruct, employing the same evolution strategy. The results demonstrate that WizardMath-\n\\begin{wraptable}{r}{0.5\\textwidth}\n\\centering\n\\vspace{-0.4cm}\n    \\caption{The impact of using open source models for Math-Evol and use Mistral-7B-v0.1 for SFT . }\n    \\scalebox{0.9}{\\begin{tabular}{lcc}\n    \\toprule\n     Models    & GSM8k & MATH   \\\\\n    \\midrule \n    Mistral-7B-v0.1 & 42.9 & 12.9   \\\\\n    \\midrule\n    WizardMath-SFT-GPT-4-Evol & 82.8 & 48.1 \\\\\n    WizardMath-SFT-Llama3-Evol & 76.7 & 43.5  \\\\ \n    \\bottomrule\n\n\\label{tab: open_source_evol}\n    \\end{tabular}\n}\n\\end{wraptable}\nLlama3-Evol achieved a 33.8\\% improvement on GSM8k and a 30.6\\% improvement on MATH, indicating that the math evol instruct strategy remains effective on open-source models. However, compared to GPT-4 evolution, there is still a 5\\%-6\\% performance gap. Despite this, the strategy shows significant potential in balancing computational cost and accuracy.\n\n\\begin{table*}[!tp]\n\\setlength\\tabcolsep{4pt}\n\\vspace{-0.5cm}\n\\small\n\\centering\n    \\caption{A case study from GSM8k test set. We rate the response using PRM and ORM. Red text denotes the wrong reasoning steps which PRM successfully detected, but ORM failed.\n    }\n    \\begin{tabular} {p{1.2cm}|p{12cm}}\n        \\toprule\n         \\textbf{Question} & \\textit{The farm has 30 cows and the zoo has 20 sheep.  The zoo has twice as many cows as the farm does. The farm has half as many sheep as the zoo does. How many animals do the farm and zoo have combined? \\quad\\quad \\textbf{Ground truth answer: 120}}\n         \\\\\n         \\midrule\n         \\multirow{10}{*}{\\rotatebox{0}{\\textbf{Steps}}} \n& Step 1: we know that the farm has 30 cows.  \\textcolor{blue}{PRM Score: 0.95.}\\\\\n& Step 2: we are told that the zoo has twice as many cows as the farm does. \\textcolor{blue}{PRM Score: 0.97.}\\\\\n& Step 3: So, we can calculate the number of cows in the zoo by multiplying the number of cows in the farm by 2. So, 2 * 30 = 60, So, the zoo has 60 cows. \\textcolor{blue}{PRM Score: 0.97.}\\\\\n& Step 4: we know that the farm has half as many sheep as the zoo does. \\textcolor{blue}{PRM Score: 0.96.}\\\\\n& Step 5: So, we can calculate the number of sheep in the farm by dividing the number of sheep in the zoo by 2. So, 20 / 2 = 10. So, the farm has 10 sheep. \\textcolor{blue}{PRM Score: 0.93.}\\\\\n& Step 6: Now, we can calculate the total number of animals in the farm and the zoo by adding the number of cows and the number of sheep in both places. \\textcolor{blue}{PRM Score: 0.92.}\\\\\n& Step 7: \\textcolor{red}{So, 30 (farm's cows) + 60 (zoo's cows) + 10 (farm's sheep) = 100.} \\textcolor{blue}{PRM Score: 0.11.}\\\\\n& Step 8: \\textcolor{red}{So, the farm and the zoo have a total of 100 animals combined.} \\textcolor{blue}{PRM Score: 0.10.}\\\\\n& Step 9: \\textcolor{red}{Therefore, the final answer is \\boxed{100}. The answer is: 100.} \\textcolor{blue}{PRM Score: 0.06.} \\textcolor{olive}{ORM Score: 0.89.}\n\\\\\\midrule\n    \\end{tabular}\n    \\label{tab:prm_case}\n\\end{table*}\n\\vspace{-0.1cm}\n\n\\subsection{Data Contamination Check}\nApart from the performance analysis, we also investigate whether evolution leads to the data contamination between training data and test set. To address this consideration,   we employ instructions in the  GSM8k and MATH test set as queries to retrieve the top-5 samples from all evolved training data with an   embedding model, gte-large ~\\citep{Li2023TowardsGT}.\nAdditionally, we employ GPT-4  to provide similarity judgement between the test sets and the retrieved samples, and remove the top-2 similar instructions. The prompt and details are shown in Appendix \\ref{subsec:data_contamination} and \\ref{appendix:data_filter_appendix}. Figure \\ref{fig:data_filter} illustrates that the evolution process does not yield higher similarity scores. \n\n\\subsection{Case Study}\n\\vspace{-0.1cm}\n\\textbf{Evol-Instruct.} The Examples 3 and 4 in the Appendix \\ref{appendix:evol_prompts} shows the prompt and corresponding cases of GSM8k and MATH instruction evolution, demonstrating that the evolved instructions exhibit more complexity and diversity than the original training set.\n\n\\textbf{PRM v.s. ORM.} We present a comprehensive case study to  illustrate the effectiveness of our PRM. As delineated in Table \\ref{tab:prm_case}, PRM demonstrates precise performance on a challenge math problem from the GSM8k test set. Remarkably, our PRM effectively distinguished the incorrect solution, in the meanwhile the ORM struggled in this task. Furthermore, PRM demonstrated exceptional insight by accurately detecting the incorrect steps of the solution chosen by ORM, specifically the steps 7, 8, and 9. Subsequently, PRM also assigned lower score logits to these erroneous steps.\n\n\t\t\n \n\nThis paper introduces \\modelname{}, a mathematics model fine-tuned with \\textbf{\\REInameS{}}. The experimental results demonstrate that \\modelname{} achieves SOTA performance surpassing existing open-source LLMs on  GSM8k and MATH from grade to high school problems. Notably,  \\modelname{} 70B exhibits superior performance compared to some of the well-known proprietary  LLMs, including ChatGPT-3.5, Claude Instant, PaLM-2, Gemini Pro. Furthermore, our preliminary exploration highlights the pivotal role of instruction evolution and process supervision in achieving exceptional performance.\n\n\\clearpage\n\n\\newpage\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models}\n\n\\begin{document}\n\n\\doparttoc % Tell to minitoc to generate a toc for the parts\n\\faketableofcontents\n\\maketitle\n\n\\blfootnote{\\textsuperscript{$\\star$}Equal contribution \\quad \\textsuperscript{\\textdagger}Corresponding author}\n\n\\vspace{-16mm}\n\\begin{center}\n          \\fontsize{9pt}{\\baselineskip}\\selectfont\n          {Project page:}~\\tt\\href{https://meta-math.github.io/}{\\textbf{meta-math.github.io}}\n         \\vspace{3mm}\n\\end{center}\n\n\\vspace{-3mm}\n\\begin{abstract}\n\\vspace{-2.5mm}\n\n    Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (\\eg, LLaMA-2) are still far away from satisfactory for solving mathematical problems due to the complex reasoning procedures. To bridge this gap, we propose \\emph{MetaMath}, a finetuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives, which results in a new dataset called {MetaMathQA}. Then we finetune the LLaMA-2 models on MetaMathQA.\n    Experimental results on two popular benchmarks (\\ie, GSM8K and MATH) for mathematical reasoning \n    demonstrate that \n    MetaMath outperforms a suite of open-source LLMs by a significant margin.  Our MetaMath-7B model achieves $66.5\\%$ on GSM8K and $19.8\\%$ on MATH, exceeding the state-of-the-art models of the same size by $11.5\\%$ and $8.7\\%$.\n    Particularly,\n    {MetaMath-70B} achieves an accuracy of $82.3\\%$ on {GSM8K}, slightly better than {GPT-3.5-Turbo}.\n    We release the {MetaMathQA} dataset, the {MetaMath} models with different model sizes and the training code for public use.\n\\end{abstract}\n \n\\begin{figure}[ht]\n    \\centering\n    \\vskip -.2in\n    \\begin{minipage}{1\\linewidth}\n        \\centering\n        \\includegraphics[width=.95\\linewidth]{figs/fig1_1735.pdf}\n        \\label{fig:large}\n    \\end{minipage}\n    \\vspace{-2mm}\n    \\begin{minipage}{0.42\\textwidth}\n        \\centering\n        \\includegraphics[width=\\linewidth]{figs/gsm_over_bar.pdf}\n        \\label{fig:small1}\n    \\end{minipage}\n    \\hspace{0.8cm}\n    \\begin{minipage}{0.42\\textwidth}\n        \\centering\n        \\includegraphics[width=\\linewidth]{figs/math_over_bar.pdf}\n        \\label{fig:small2}\n    \\end{minipage}\n    \\vspace{-3.5mm}\n    \\caption{\\footnotesize Overview of the {MetaMathQA} dataset and the mathematical problem-solving LLM -- {MetaMath}. We note that our MetaMath-70B is finetuned by QLoRA \\cite{dettmers2023qlora} due to the computing resource limitation.}\n    \\label{fig:overview}\n    \\vskip -.15in\n\\end{figure}\n    \n\n\\section{Introduction}\n\\vspace{-5mm}\n    Recent years have witnessed the rapid development of large language models (LLMs) which emerge as the favored approach for various applications and demonstrate multi-dimensional abilities, including instruction following~\\citep{brown2020language,alpaca,ouyang2022training, li2023deepinception}, coding assistance \\citep{chen2021evaluating,nijkamp2022codegen,luo2023wizardcoder,li2023starcoder}, and mathematical problem-solving \\citep{yuan2023scaling,imani2023mathprompter,luo2023wizardmath, collins2023evaluating}. \n    Among various tasks, solving mathematical problems is more challenging as they often require highly complex and symbolic multi-step reasoning capabilities. \n    Although some close-sourced models, \\eg, {GPT-3.5-Turbo}~\\citep{gpt3-5}, {GPT-4}~\\citep{gpt4} and {PaLM-2}~\\citep{touvron2023llama}, have demonstrated promising performance on some mathematical problem-solving benchmarks, it is still a mystery how these models are trained and what data these models use. Therefore, how to equip open-source LLMs (\\eg, {LLaMA} \\citep{touvron2023llama1,touvron2023llama}) with good mathematical problem-solving skills remains an open challenge. \n\n    \\vspace{-0.25mm}\n    \n    To tackle this challenge, two popular lines of research to improve the mathematical problem-solving abilities of LLMs are: \\emph{prompt-based methods} and \\emph{finetuning-based methods}. Prompt-based methods~\\cite{wei2022chain,fu2023complexitybased,wang2023selfconsistency,zhou2023leasttomost,xiong2023dq,xin2023lego} aim to activate the potential capacities of LLMs by choosing suitable prompting inputs without modifying the model parameters. Finetuning-based methods update the open-source LLMs (\\eg, LLaMA) under the guidance of some other powerful closed-source LLMs (\\eg, GPT-3.5~\\cite{gpt3-5}, GPT-4~\\cite{gpt4}). While prompt-based methods are model-dependent and sensitive to many factors, finetuning-based methods, despite being simple and model-agnostic, heavily rely on effective training data on downstream mathematical questions. Our work aims to improve finetuning-based methods with a novel method to bootstrap available mathematical questions in the training set. \n    Specifically, we propose to bootstrap the questions in both forward and backward reasoning directions. For the forward direction, we have the original and LLM-rephrased questions. For the backward direction, we have the self-verification question~\\cite{weng2023large} and FOBAR question~\\cite{jiang2023backward}. To construct backward reasoning questions, we mask a token in a question using an identifier ``x'' and ask the model to predict the masked token if the answer is provided. Different from \\cite{weng2023large,jiang2023backward} that apply backward reasoning for inference verification, we use it as a form of question for language model fine-tuning. For answers, we adopt an answer augmentation method based on rejection sampling~\\cite{yuan2023scaling}, where diverse reasoning paths are generated and only those with correct answers are used. After combining both forward and backward mathematical questions with augmented answers, we construct a new dataset for fine-tuning, called \\emph{MetaMathQA}. By fine-tuning LLaMA-2 on MetaMathQA, we obtain our \\emph{MetaMath} model. Our approach is guided by the insight that a mathematical question represents merely a single view of the underlying meta-knowledge. Therefore, question bootstrapping can be viewed as a form of multi-view augmentation in order to enable the transfer of the meta-knowledge. Leveraging the MetaMathQA dataset, MetaMath demonstrates exceptional performance in mathematical reasoning, positioning it among the top performers on widely recognized evaluation benchmarks.\n\n    \\vspace{-0.25mm}\n    \n    \\begin{wrapfigure}{r}{0.5\\linewidth}\n    \\vspace{-2.3em}\n    \\centering\n    \\includegraphics[width=0.97\\linewidth]{figs/Dual_Y_Axis_fig1_refined.pdf}\n    \\vspace{-.9em}\n\t\\caption{\\footnotesize GSM8K accuracy of LLaMA-2-7B finetuned on different sizes of answer augmentation data. A larger diversity gain indicates the question is more diverse compared to the existing questions. Detailed experimental setup is given in Section \\ref{sec:expt-setup}.}\n\t\\label{fig:Accuracy Saturation}\n    \\vspace{-1em}\n    \\end{wrapfigure}\n    \n    Another motivation behind question bootstrapping is to enlarge the question diversity~\\citep{eldan2023tinystories} such that the question distribution can be rich enough to cover more unseen scenarios. We quantify the question diversity of the original questions and our MetaMathQA dataset in Figure~\\ref{fig:Accuracy Saturation}. The diversity gain~\\citep{bilmes2022submodularity} indicates how diverse the question is compared to the existing dataset, and a larger diversity gain means the new question is more different from the existing dataset. With question bootstrapping, our MetaMathQA dataset is much more diverse than the original dataset. We also observe that the test accuracy without bootstrapped questions rapidly reaches a state of saturation. In contrast, the test accuracy, when using bootstrapped questions, continues to exhibit a steady increase.\n\n    \\vspace{-0.25mm}\n\n    Question bootstrapping also has an intrinsic connection to dataset distillation~\\cite{wang2018dataset,zhao2020dataset} and machine teaching~\\cite{zhu2015machine,liu2017iterative,liu2021iterative,qiu2023iterative}, where the shared target is to construct a training dataset that best facilitates generalization. Unlike both methods that focus on optimizing the training empirical risk, question bootstrapping uses the reasoning diversity of questions as a heuristic proxy and maximizes this diversity by constructing forward, backward and rephrased questions. MetaMath aims to transfer the underlying meta-knowledge to enable strong generalization~\\cite{kilbertus2018generalization}. Our contributions are listed below:\n\n    \\begin{itemize}[leftmargin=*,nosep]\n    \\setlength\\itemsep{0.28em}\n        \\item We propose a novel question bootstrapping method to augment the training dataset, resulting in MetaMathQA. Question bootstrapping rewrites questions with both forward and backward reasoning paths and also leverages LLMs to rephrase the question text.\n        \\item Based on the MetaMathQA dataset, MetaMath is finetuned from state-of-the-art open-source LLMs (\\eg, LLaMA-2), showing excellent elementary mathematical problem-solving capability.\n        \\item We identify an important factor when creating the MetaMathQA dataset -- question diversity. The diversity is particularly important in reasoning directions, and backward reasoning questions are very helpful for LLMs to understand mathematical knowledge without memorization.\n        \\item We conduct experiments on two standard mathematical reasoning benchmarks: GSM8K \\citep{cobbe2021training} and MATH \\citep{hendrycks2021measuring}.  MetaMath outperforms existing open-source LLMs by a large margin. MetaMath-7B has achieved $66.5\\%$ on GSM8K ($+11.5\\%$ compared to the previous best open-source LLM) on GSM8K and $19.8\\%$ on MATH ($+8.7\\%$ compared to the previous best open-source LLM).\n        \\item Our work studies data augmentation for improving the mathematical problem-solving ability of LLMs. Despite being simple, our method significantly outperforms many intricate methods. Our results highlight the importance of data augmentation and also shed light on other reasoning tasks.\n    \\end{itemize}\n\t\t\n    \\vspace{-2mm}\n    \\section{Related Work}\n    \\vspace{-2mm}\n\n    \\textbf{Large Language Models (LLMs)} \\cite{devlin2019bert, liu2019roberta, gpt2, Colin2020t5,brown2020language, touvron2023llama1, sun2023survey} have achieved great success in various natural language processing tasks, \\eg, topic classification \\citep{min2022metaicl, jiang2023effective, jiang2023byom}, sentiment classification \\citep{brown2020language, min2022metaicl}, translation \\citep{brown2020language}, by few-shot prompting (or in-context learning) \\citep{brown2020language, min2022metaicl, chen2022meta}. Recently, \\citet{wei2022chain,wang2023selfconsistency} show that LLMs with more than 100B parameters (\\eg, GPT-3 \\citep{brown2020language} with 175B, PaLM with 540B \\citep{chowdhery2022palm}) can solve complex tasks by generating multiple reasoning steps towards the answer when given a few reasoning examples as demonstration. While both GPT-3.5 \\citep{gpt3-5} and GPT-4 \\citep{gpt4} have shown promising reasoning ability for complex mathematical tasks like MATH \\cite{hendrycks2021measuring}, the performance of open-source models (\\eg, LLaMA-1 \\citep{touvron2023llama1}, LLaMA-2 \\citep{touvron2023llama}) is far from satisfactory.\n\n    \\vspace{-1mm}\n    \\textbf{Learning Mathematical Reasoning} for \n    complex math tasks\n    like GSM8K \\citep{cobbe2021training} and MATH \\citep{hendrycks2021measuring} is one of the most challenging\n    problem in open-source LLMs.\n    \\citet{wei2022chain} enhances the reasoning ability of LLMs by augmenting the output with a sequence of intermediate steps toward the answer. \n    A few methods~\\citep{fu2023complexitybased,wang2023selfconsistency,zhou2023leasttomost} are proposed to \n    improve the quality of reasoning paths.\n    For example,\n    Complexity-based CoT \\citep{fu2023complexitybased}\n    selects examples with more steps as in-context demonstrations and shows that prompting with more reasoning steps leads to better performance.\n    Self-Consistency \\citep{wang2023selfconsistency}\n    samples multiple reasoning paths\n    and selects the final answer by majority voting.   \n    Another category of work is finetuning-based methods, which finetunes open-source models (\\eg, LLaMA) with the knowledge from some advanced closed-source LLMs \\citep{gpt3-5,gpt4}. \n    \\citet{magister2023teaching} investigates the transfer of reasoning capabilities via knowledge distillation. \n    \\citet{yuan2023scaling} proposes to apply rejection sampling finetuning (RFT) to improve mathematical reasoning performance.  \n    WizardMath \\citep{luo2023wizardmath} proposes a reinforced evol-instruct method to enhance reasoning abilities by supervised fine-tuning and PPO training \\citep{schulman2017proximal}. MAmmoTH~\\citep{yue2023mammoth} combines CoT and Program-of-Thought \\citep{chen2022program} rationales for teaching LLMs to use external tools (e.g., Python interpreter) for solving mathematical problems. \\citet{wang2023making} propose a constraint alignment loss to finetune LLMs for calibration.\n    \n    \\vspace{-1mm}\n    \\textbf{Knowledge Distillation} \\citep{hinton2015distilling, gou2021knowledge} transfers knowledge from a larger teacher model to a smaller student model, achieving promising performance in many applications \\citep{shen2018feature, park2019relational, he2019knowledge, mirzadeh2020improved},\n    Recently, \\citep{li2022explanations, huang2022large, ho2023large, magister2023teaching, hsieh2023distilling, fu23d, shridhar2023distilling} propose to \n    transfer reasoning abilities \n    from LLMs (\\eg, GPT-3.5 \\citep{gpt3-5}, PaLM \\citep{chowdhery2022palm}) to small language models (\\eg, T5 \\citep{Colin2020t5}, GPT-2 \\citep{gpt2}). For example,\n    Finetune-CoT \\citep{ho2023large} samples multiple reasoning paths from LLMs and  finetune the student model with correct ones,\n    while Self-Improve \\citep{huang2022large} chooses the one with the highest confidence.\n    \\citet{li2022explanations} further feeds the question and ground-truth label to LLMs\n    for prompting its reasoning path.\n    \\citet{shridhar2023distilling} proposes to generate sub-questions and solution pairs for training. Small models finetuned by knowledge distillation can achieve similar performance to LLMs \\citep{magister2023teaching, ho2023large} on both common sense reasoning (\\eg, CommonSenseQA \\citep{talmor2019commonsenseqa}) and symbol reasoning (\\eg, Coin Flip \\citep{wei2022chain}). However, for solving challenging mathematical problems (\\eg, GSM8K \\citep{cobbe2021training}), there is still a large performance gap  \\citep{ho2023large, fu23d, magister2023teaching}.\n\n    \\vspace{-2mm}\n    \\section{Method}\t\\label{sec:method}\n    \n\\vspace{-2mm}\nThe overview of our method is illustrated in Figure~\\ref{fig:overview}. Given a meta-question (a sample in the original mathematical training set), we can generate a series of variants. Specifically, we perform three types of question bootstrapping. Combined with answer augmentation, we present {MetaMathQA}, a diverse and high-quality mathematical dataset based on GSM8K and MATH. We then present {MetaMath}, a family of LLMs finetuned on MetaMathQA focusing on elementary mathematical problem-solving. \n\n\\subsection{Answer Augmentation (AnsAug)}\n\\vspace{-1mm}\n\nGenerating more reasoning paths \nis a simple but effective way\nto augment \nthe training set.\nFor a question $q_i$,\nwe \nuse few-shot chain-of-thought prompting with temperature sampling \nto generate $K_{\\text{AnsAug}}$ more reasoning paths $\\{(r_i^{(j)}, a_i^{(j)}): j=1,\\dots, K_{\\text{AnsAug}}\\}$:\nthe question is appended to a few in-context reasoning examples,\nthen fed to the LLM\nfor generating its reasoning path $r_i^{(j)}$ and answer $a_i^{(j)}$.\nWe filter out reasoning paths with correct answers as:\n\\begin{equation}\n\\small\n\\hD_{\\text{AnsAug}} = \\{(q_i, r_i^{(j)}, a_i^{(j)}): a_i^{(j)} = a_i^\\star;  i=1,\\dots, N_q; j=1,\\dots, K_{\\text{AnsAug}}\\}.\n\\end{equation}\n\n\\subsection{Question Bootstrapping by LLM Rephrasing}\n\\vspace{-1mm}\n\nGenerating more answers for mathematical questions with LLMs is straightforward, but creating questions is more challenging. Math Questions\nare written by well-educated teachers. Hence, enlarging the question set through manual creation is time-consuming and labor-intensive. To address this issue, we propose rephrasing prompting to generate more questions through the LLM. \n\n\\vspace{2mm}\n\\begin{exmp}{Rephrasing Question}{rephrase-example}\n\\small\n\\textbf{Question:} What is the total amount that James paid when he purchased 5 packs of beef, each weighing 4 pounds, at a price of \\$5.50 per pound? \n\n\\textbf{Answer:} Each pack of beef weighs 4 pounds, so 5 packs weigh 4 * 5 = 20 pounds in total. The price per pound of beef is \\$5.50, so the total cost for 20 pounds is 20 * \\$5.50 = \\$110. ~...~\nThe answer is: 110.\n\\end{exmp}\n\\vspace{2.5mm}\n\nSpecifically, for a question $q_i$, we append it to the prompt, which is then fed to the LLM for generating the rephrased question. Example~\\ref{exmp:rephrase-example} shows a generated rephrased question and the complete prompt is shown in Appendix \\ref{sec:rephrase-prompt}. We adopt temperature sampling to sample $K_{\\text{rephrase}}$ rephrased questions for each meta-question. For the rephrased questions, it is time-consuming to manually check the consistency compared with the original questions. We propose a supervised method to evaluate the correctness between the rephrased questions and the meta-questions. For each rephrased question $\\hat{q}_i^{(j)}$, we use few-shot Chain-of-Thought prompting to generate its reasoning path $\\hat{r}_i^{(j)}$ and answer $\\hat{a}_i^{(j)}$, which is compared with the ground-truth answer $a_i^\\star$. The accuracy of Complexity-based CoT~\\citep{fu2023complexitybased} for answering the rephrased question by GPT-3.5-Turbo is $76.30\\%$, which is comparable to that of answering the original training questions ($80.74\\%$). This suggests that the quality of rephrased questions is preserved high while the question diversity is improved. We collect the rephrased questions with correct answers (\\ie, $\\hat{a}_i^{(j)} = a_i^\\star$) as the augmented data:\n\\begin{equation}\n\\small\n\\hD_{\\text{rephrase}} = \\{(\\hat{q}_i, \\hat{r}_i^{(j)}, \\hat{a}_i^{(j)}): \\hat{a}_i^{(j)} = a_i^\\star;  i=1,\\dots, N_q; j=1,\\dots, K_{\\text{rephrase}}\\}.\n\\end{equation}\n\n \n\t\n\\subsection{Question Bootstrapping by Backward Reasoning}\n\\vspace{-1mm}\n\nBackward reasoning plays an important role in answering many mathematical questions, \\ie, starting with a given condition and thinking backward to determine an unknown variable in the question. One specific example between a question and a backward question is illustrated in Example \\ref{exmp:if}. \nHowever, existing methods (SFT, RFT, WizardMath) have significantly lower accuracy on backward questions, as shown in Figure~\\ref{fig:reverse}, motivating us to bootstrap backward questions to improve the reasoning ability.\n\n\\vspace{2mm}\n\n\\begin{exmp}{Question and Backward Question}{if}\n    \\small\n    \\textbf{Question:}\n    James buys 5 packs of beef that are 4 pounds each. The price of beef is \\$5.50 per pound. How much did he pay? \\textbf{Answer:} He bought 5*4=20 pounds of beef. He paid 20*5.5=\\$110. The answer is: 110 \\textcolor{red}{{\\ding{51}}}\n    \n    \\textbf{Backward Question:}\n    James buys x packs of beef that are 4 pounds each. The price of beef is \\$5.50 per pound. How much did he pay? If we know the answer to the above question is 110, what is the value of unknown variable x? \\textbf{Answer:} The total weight of the beef is 4*x because 4*5.5 = 22. ...  The answer is: 27 \\textcolor{red}{\\ding{55}}\n\\end{exmp}\n\\vspace{2.5mm}\n   \n \n\t\nTo improve the backward reasoning ability of finetuned models,\nwe generate more questions which can be solved in a backward manner: \na number in the question $q_i$ is masked by \n``x'',\nwhile the LLM is asked to predict the \nvalue of ``x'' when its answer $a_i^\\star$\nis provided.\nDifferent from forward reasoning,\nwhich generates explicit intermediate steps towards the\nfinal answer,\nbackward reasoning starts with the answer\nand generates multiple reasoning steps \nto predict the masked number.\nRepresentative backward reasoning methods\ninclude \nSelf-Verification \\citep{weng2023large}\nand FOBAR \\citep{jiang2023backward}.\n\nIn Self-Verification (SV) \\citep{weng2023large}, the question with the answer is first rewritten into a declarative statement, \\eg, ``How much did he pay?'' (with the answer 110) is rewritten into ``He paid \\$10''. \nThen, a question for asking the value of $\\vx$ is appended, \\eg, ``What is the value of unknown variable $\\vx$?''. Example \\ref{exmp:sv-example} gives an augmented example. We collect the new questions and their generated reasoning paths with correct answers as the augmented data:\n\\begin{equation}\n\\small\n\\hD_{\\text{SV}} = \\{(\\tilde{q}_i^{(j)}, \\tilde{r}_i^{(j)}, \\tilde{a}_i^{(j)}): \\tilde{a}_i^{(j)} = a_i^\\star;  i=1,\\dots, N_q; j=1,\\dots, K_{\\text{SV}}\\}.\n\\end{equation}\n\n\\vspace{2.5mm}\n\\begin{exmp}{Self-Verification \\citep{weng2023large} Question}{sv-example}\n\\small\n\\textbf{Question:} James buys {\\color{red3}x} packs of beef that are 4 pounds each.  The price of beef is \\$5.50 per pound. He paid 110.\n{\\color{red3}What is the value of unknown variable x?} \\\\\n\\textbf{Answer:} \nTo solve this problem, we need to determine the value of x, which represents the number of packs of beef that James bought.\nEach pack of beef weighs 4 pounds and \n~...~\nThe value of x is 5.\n\\end{exmp}\n\\vspace{5mm}\n\n\\begin{exmp}{FOBAR~\\citep{jiang2023backward} Question}{fobar}\\small\n\\textbf{Question:} \nJames buys {\\color{red3}x} packs of beef that are 4 pounds each.  The price of beef is \\$5.50 per pound.  How much did he pay?\n{\\color{red3}\\!If we know the answer to the above question is 110, what is the value of unknown variable x?\\!\\!\\!}\n\n\\textbf{Answer:}\nJames buys x packs of beef that are 4 pounds each, so he buys a total of 4x pounds of beef.\nThe price of beef is \\$5.50 per pound, so the total cost of the beef is 5.50 * 4x = 22x.\n~...~\nThe value of x is 5.\n\\end{exmp}\n\n\\vspace{3mm}\n \nSelf-Verification\nneeds to rewrite the question with an answer\ninto a declarative\nstatement,\nwhich is challenging for complex questions.\nTo address this issue,\nFOBAR~\\citep{jiang2023backward} proposes to\ndirectly append the answer to the question,\n\\ie,\n``\\textit{If we know the answer to the above question is \\{$a_i^\\star$\\} , what is the value of unknown variable {\\normalfont x}?}''\nExample~\\ref{exmp:fobar}\nshows an example.\nWe collect the new questions along with their correct answers\nas our augmented data:\n\\begin{equation}\n\\small\n    \\hD_{\\text{FOBAR}} = \\{(\\bar{q}_i^{(j)}, \\bar{r}_i^{(j)}, \\bar{a}_i^{(j)}): \\bar{a}_i^{(j)} = a_i^\\star;  i=1,\\dots, N_q; j=1,\\dots, K_{\\text{FOBAR}}\\}.\n\\end{equation}\n    \n\n\t\n\\subsection{Finetuning Objective Functions}\nWe merge all the augmented data, including answer-augmented data and bootstrapped questions (Rephrasing, Self-Verification, FOBAR) as $ \\small{\t\\hD_{\\text{MetaMathQA}} = \\hD_{\\text{AnsAug}}  \\cup \\hD_{\\text{rephrase}} \\cup \\hD_{\\text{SV}} \\cup \\hD_{\\text{FOBAR}}}$.\nWe finetune a LLM model (parameterized by $\\vtheta$) on $\\hD_{\\text{MetaMathQA}}$ to obtain the MetaMath model\nby maximizing the log likelihood of the reasoning path conditioned on the question, \\ie, $\\small{\n\\mathcal{L}(\\vtheta)= \\sum_{(q, r, a) \\in \\hD_{\\text{MetaMathQA}} } \\log \\bP(r \\mid  q;\\vtheta).\n\\label{eq:mle}}$ Although we only consider LLaMA-2 here, MetaMathQA can also be used to finetune other LLMs.\n    \n\\vspace{-.5mm}\n\\section{Experiments and Results}\n\\vspace{-1mm}\n    \\subsection{Experimental Setup}\\label{sec:expt-setup}\n\\vspace{-.5mm}\n    \\setlength{\\columnsep}{9pt}\n    \\begin{wraptable}{r}[0cm]{0pt}\n    \\centering\n    \\footnotesize\n    \\setlength{\\tabcolsep}{2.6pt}\n\t\\renewcommand{\\arraystretch}{1.25}\n    \\begin{tabular}{c|ccccc}\n    \\specialrule{0em}{0pt}{-22pt}\n     Dataset   & AnsAug & Rephrasing & SV & FOBAR & Overall \\\\\n    \\shline\n    MetaMathQA-GSM8K & 80K & 80K & 40K & 40K & 240K \\\\\n    MetaMathQA-MATH  & 75K & 50K & 15K & 15K & 155K \\\\\n    MetaMathQA  & 155K  & 130K  & 55K  & 55K & 395K \\\\\n    \\specialrule{0em}{0pt}{-7pt}\n    \\end{tabular}\n    \\caption{\\footnotesize Number of samples in the proposed MetaMathQA.}\\label{exp:dataset}\n    \\vspace{-1.5mm}\n    \\end{wraptable}\n    \n    \\textbf{Datasets.}\n    We use two popular\n    mathematical reasoning benchmarks:\n    \\begin{enumerate*}[(i), series = tobecont, itemjoin = ~~]\n    \\item GSM8K \\citep{cobbe2021training} is a dataset consisting of high-quality grade school math \n    problems, containing 7,473 training samples and 1,319 testing samples;\n    and \n    \\item MATH \\citep{hendrycks2021measuring}\n    dataset consists of high school math competition problems that span \n    seven subjects including \n    Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra,\n    and Precalculus.\n    It contains 7,500 and 5,000 samples for training and testing, respectively.\n    \\end{enumerate*}\n    Questions in GSM8K \\citep{cobbe2021training} take between 2 and 8 steps to reach the answer, while \n    MATH\n    is much more challenging.\n    \\vspace{-1.mm}\n    \n\\newpage\n\\setlength{\\columnsep}{14pt}\n    \\begin{wraptable}{r}[0cm]{0pt}\n    \\centering\n    \\hspace{-2.4mm}\n    \\footnotesize\n    \\setlength{\\tabcolsep}{6.5pt}\n    \\renewcommand{\\arraystretch}{1.223}\n    \\begin{tabular}{l|ccc}\n    \\specialrule{0em}{0pt}{-1pt}\n    Model& \\#params & GSM8K & MATH \\\\\\shline\n    \\multicolumn{4}{c}{\\textit{closed-source models}}  \\\\\n    GPT-4 \\citep{gpt4} & - & 92.0 & 42.5 \\\\\n    GPT-3.5-Turbo \\citep{gpt3-5-turbo} & -& 80.8 & 34.1 \\\\\n    PaLM \\citep{chowdhery2022palm} & 8B & 4.1 & 1.5 \\\\ \n    PaLM \\citep{chowdhery2022palm} & 62B & 33.0 & 4.4 \\\\ \n    PaLM \\citep{chowdhery2022palm} & 540B & 56.5 & 8.8 \\\\ \n    PaLM-2 \\citep{anil2023palm} & 540B & 80.7 & 34.3 \\\\\n    Flan-PaLM 2 \\citep{anil2023palm} & 540B & 84.7 & 33.2 \\\\\n    Minerva \\citep{lewkowycz2022solving} & 8B & 16.2 & 14.1 \\\\\n    Minerva \\citep{lewkowycz2022solving} & 62B & 52.4 & 27.6\\\\\n    Minerva \\citep{lewkowycz2022solving} & 540B & 58.8 & 33.6\\\\\\shline\n    \\multicolumn{4}{c}{\\textit{open-source models (1-10B)}}  \\\\\n    LLaMA-2 \\citep{touvron2023llama} & 7B & 14.6 & 2.5 \\\\\n    MPT \\citep{MosaicML2023Introducing} & 7B & 6.8 & 3.0 \\\\\n    Falcon \\citep{penedo2023refinedweb} & 7B & 6.8 & 2.3 \\\\\n    {Code-LLaMA \\cite{ro023code}} & {7B} & {25.2}& {  13.0}\\\\\n    InternLM \\citep{2023internlm} & 7B & 31.2 & - \\\\\n    GPT-J \\citep{gpt-j} & 6B & 34.9 & - \\\\\n    ChatGLM 2 \\citep{zeng2022glm} & 6B & 32.4 & - \\\\\n    Qwen \\citep{qianwen} & 7B & 51.6 & - \\\\\n    Baichuan-2 \\citep{baichuan2} & 7B & 24.5 & 5.6 \\\\\n    SFT \\citep{touvron2023llama} & 7B & 41.6 & - \\\\\n    RFT \\citep{yuan2023scaling} & 7B & 50.3 & - \\\\\n    {MAmooTH-CoT \\cite{yue2023mammoth}} & { 7B} & {50.5}& {10.4}\\\\\n    WizardMath \\citep{luo2023wizardmath} & 7B & 54.9 & 10.7 \\\\ \n    \\rowcolor{Gray}\n    MetaMath  & 7B & \\textbf{66.5}& \\textbf{19.8} \\\\\\shline\n    \\multicolumn{4}{c}{\\textit{open-source models (11-50B)}}  \\\\\n    LLaMA-2 \\citep{touvron2023llama} & 13B & 28.7 & 3.9 \\\\\n    LLaMA-2 \\citep{touvron2023llama} & 34B & 42.2 & 6.2\\\\\n    MPT \\citep{MosaicML2023Introducing} & 30B & 15.2 & 3.1 \\\\\n    Falcon \\citep{penedo2023refinedweb} & 40B & 19.6 & 2.5 \\\\\n    GAL \\citep{taylor2022galactica} & 30B & - & 12.7  \\\\\n    {Platypus \\cite{platypus2023}} & {13B} & {25.7}& {2.5}\\\\\n    {Orca-Platypus \\cite{platypus2023}} & {13B} & {38.4}& {3.0} \\\\\n    Vicuna \\citep{vicuna2023} & 13B & 27.6 & - \\\\\n    {Code-LLaMA \\cite{ro023code}} & {13B} & {36.1}& {16.4}\\\\\n    Baichuan-2 \\citep{baichuan2} & 13B & 52.8 & 10.1\\\\\n    SFT \\citep{touvron2023llama} & 13B & 50.0 & - \\\\\n    RFT \\citep{yuan2023scaling} & 13B & 54.8 & - \\\\\n    {MAmooTH-CoT \\cite{yue2023mammoth}} & {13B} & {56.3}& {12.9}\\\\\n    WizardMath \\citep{luo2023wizardmath} & 13B & 63.9 & 14.0  \\\\ \n    \\rowcolor{Gray}\n    MetaMath  & 13B & \\textbf{72.3} & \\textbf{22.4} \\\\\\shline\n    \\multicolumn{4}{c}{\\textit{open-source models (51-70B)}} \\\\\n    LLaMA-2 \\citep{touvron2023llama} & 70B & 56.8 & 13.5 \\\\\n    RFT \\citep{yuan2023scaling}             & 70B & 64.8 & - \\\\\n    {Platypus \\cite{platypus2023}} & {70B} & {70.6}& { 15.6}\\\\\n    {MAmooTH-CoT \\citep{yue2023mammoth}} & {70B} & {72.4}& {21.1}\\\\\n    WizardMath \\citep{luo2023wizardmath}    & 70B & 81.6 & 22.7  \\\\ \n    \\rowcolor{Gray}\n    MetaMath$^\\ddag$               & 70B & \\textbf{82.3} & \\textbf{26.6} \\\\\n    \\specialrule{0em}{-5pt}{0pt}\n    \\end{tabular}\n    \\caption{\\footnotesize Comparison of testing accuracy to existing LLMs on GSM8K and MATH. $^\\ddag$Due to the computing resource limitation, we finetune MetaMath-70B using QLoRA \\citep{dettmers2023qlora}.}\n    \\label{exp:main-expt}\n    \\vspace{-25mm}\n    \\end{wraptable}\n\n    \\textbf{Models.}\n    We use the current state-of-the-art open-source model\n    LLaMA-2\n    \\citep{touvron2023llama},\n    including three different parameter sizes:\n    7B, \n    13B,\n    and \n    70B,\n    as the base model for fine-tuning.\n    GPT-3.5-Turbo is used for rephrasing questions as well as \n    generating answers in all four augmentations,\n    where the temperature is set to 0.7 as in \\cite{wang2023selfconsistency}.\n    The LLaMA-2-7B and LLaMA-2-13B are trained by fully fine-tuning. LLaMA-2-70B is finetuned by QLoRA~\\citep{dettmers2023qlora} for computational efficiency. More experimental details can be seen in Appendix~\\ref{expdetalis}.\n    \n    \\textbf{Baselines.}\n    The proposed methods are compared with \n    \\begin{enumerate*}[(i), series = tobecont, itemjoin = \\quad]\n    \\item closed-source models such as GPT-3.5-Turbo~\\citep{gpt3-5-turbo}, PaLM~\\citep{chowdhery2022palm};\n    \\item open-source models such as LLaMA-1~\\citep{touvron2023llama1}, LLaMA-2~\\citep{touvron2023llama};\n    \\item Supervised Fine-Tuning (SFT),\n    which uses the training set of the original GSM8K or MATH datasets;\n    \\item Rejection sampling Fine-Tuning (RFT) \\citep{yuan2023scaling}\n    generates and collects correct reasoning paths as \n    augmented data for fine-tuning; \n    \\item WizardMath \\citep{luo2023wizardmath}\n    which generates samples and trains two reward models using ChatGPT \\footnote{\\url{https://openai.com/}} to select samples for fine-tuning. \n    \\end{enumerate*}\n\n    \n\n    \\textbf{Diversity Gain.} We use the diversity gain \\citep{bilmes2022submodularity} to measure to what extent a new dataset added to a basic dataset can improve the overall data diversity. For a base dataset $\\hD_{\\text{base}}=\\{x_i=(q_i, r_i, a_i)\\}_{i=1}^N$ with $N$ samples, and a new dataset $\\hD_{\\text{new}}=\\{x_i=(q_i, r_i, a_i)\\}_{i=1}^M$ with M samples, the diversity gain is defined as: $\\hD_{\\text{new}}$ relative to $\\hD_{\\text{base}}$ as: $d_{\\text{gain}}=\\frac{1}{M}\\sum_{x_{i}\\in\\hD_{\\text{new}}}\\min_{x_{j}\\in\\hD_{\\text{base}}}(\\|f(x_{i}) -f(x_{j})\\|_2^2)$, where $f$ is the feature extractor and we use the OpenAI Embedding API \\textit{text-embedding-ada-002} for feature extraction. For Figure~\\ref{fig:Accuracy Saturation}, we change the data size of base data and select a fixed set of 20K new data points that the model has not encountered to form $\\hD_{\\text{new}}$.\n\n    \\vspace{-1mm}\n    \\subsection{Results on GSM8K and MATH}\n    \\vspace{-1.5mm}\n\n    Table~\\ref{exp:dataset} illustrates the detailed description of our MetaMathQA collection and \n    Table \\ref{exp:main-expt}\n    shows the testing accuracy on GSM8K and MATH.\n    As can be seen,\n    for open-source models with \n    1-10B parameters,\n    MetaMath achieves the state-of-the-art performance.\n    Compared to the previous best LLM, MetaMath achieves a large improvement of 11.6\\% on GSM8K and 9.1\\% on MATH in testing accuracy,\n    showing that finetuning on our MetaMathQA data is effective.\n\n    \n    \n     \n   \n    \n    As for LLMs with 11-50B parameters,\n    the proposed MetaMath performs the best.\n    Particularly,\n    on both GSM8K and MATH,\n    MetaMath achieves higher accuracy than SFT, RFT, and WizardMath\n    by a large margin (+7\\%), demonstrating \n    the effectiveness of the MetaMath data in improving mathematical reasoning ability.\n    Furthermore,\n    for LLMs with 51-70B parameters,\n    again, \n    MetaMath achieves the highest testing accuracy.\n    Particularly,\n    MetaMath is better than GPT-3.5-Turbo on GSM8K, \n    which is used for generating augmented data for finetuning.\n    \n    \n    \\vspace{-2mm}\n\t\\subsection{Effect of Augmentations} \\label{sec:eff-aug}\n    \\vspace{-1.5mm}\n\t\n\tIn this section,\n\twe conduct experiments \n\tto study\n\tthe effect of augmentations in MetaMath.\n\tWe first finetune the {LLaMA-2-7B}\n\tmodel on augmented GSM8K (MetaMath-GSM8K) data, \n\tand test the finetuned model \n\ton GSM8K and MATH.\n\tTable \\ref{exp:abl-effect-aug}\n\tshows the testing accuracy of\n\tdifferent combinations of augmentations{, where we mix all augmented data together for each model}.\n\tAs can be seen, \n\ton GSM8K,\n\tthe models trained on answer augmentation (AnsAug) or rephrasing  augmentation \n\tachieve much higher\n\taccuracy than SFT, which is only trained on \n\tthe training set.\n\tCombing answer augmentation and rephrasing  augmentation data\n\tfor fine-tuning leads to a slightly higher accuracy,\n\twhich is further improved by about 4\\% through merging the FOBAR and SV augmentation data.\n\tAs for MATH,\n\tMetaMath trained only on MetaMahQA-GSM8K data\n\tperforms better than SFT, suggesting its effectiveness in generalizing to\n\tunseen mathematical tasks. \n\t\n\t\n    We also conduct an experiment\n    by fine-tuning\n    {LLaMA-2-7B} on the augmented MATH (MetaMathQA-MATH) data\n    then evaluate the model on GSM8K and MATH.\n    Table \\ref{exp:abl-effect-aug}\n    shows the testing accuracy.\n    Again,\n    MetaMath trained on AnsAug or rephrasing augmentation data \n    performs much better\n    than SFT.\n    Furthermore,\n    merging all augmented data together for\n    fine-tuning is better than merging AnsAug and rephrasing augmentation data, demonstrating \n    the effectiveness of SV and FOBAR augmentation data\n    in improving mathematical reasoning ability.\n    Moreover,\n    for the unseen GSM8K task,\n    MetaMath trained on MetaMathQA-MATH data is significantly better\n    than SFT (+20\\%).\n\t\n\\begin{table}[!t]\n\\footnotesize\n\\centering\n\\setlength{\\tabcolsep}{2.5pt}\n\\renewcommand{\\arraystretch}{1.25}\n\\vspace{-4mm}\n    \\begin{tabular}{c|cccccc|cccccc}\n        \\multirow{2}{*}{Method}& \\multicolumn{6}{c|}{GSM8K}& \\multicolumn{6}{c}{MATH}\\\\\n         & AnsAug & Rep. & SV  & FOBAR  \n        & GSM8K & MATH & AnsAug & Rep. & SV  & FOBAR  \n        & GSM8K & MATH\\\\\\shline\n        SFT \\citep{touvron2023llama} & \\xmark & \\xmark & \\xmark & \\xmark & 41.6 & 3.0 & \\xmark & \\xmark & \\xmark & \\xmark & 13.8 & 4.7\\\\\n        \\hline\n        \\multirow{4}{*}{MetaMath}& \\cmark & \\xmark & \\xmark & \\xmark & 59.6 & 4.4 & \\cmark & \\xmark & \\xmark & \\xmark & 28.4 & 12.9 \\\\\n        & \\xmark & \\cmark & \\xmark & \\xmark & 59.7 & 4.4 & \\xmark & \\cmark & \\xmark & \\xmark & 30.4 & 12.4 \\\\\n        & \\cmark & \\cmark & \\xmark & \\xmark & 60.6 & 4.4& \\cmark & \\cmark & \\xmark & \\xmark & 29.1 & 15.3 \\\\\n        & \\cmark & \\cmark & \\cmark & \\cmark & \\textbf{64.4} & \\textbf{5.7}& \\cmark & \\cmark & \\cmark & \\cmark & \\textbf{34.6} & \\textbf{17.7}\\\\\n    \\end{tabular}\n   \\vspace{-1.75mm}\n\\caption{\\footnotesize Effect of different question augmentation with {LLaMA-2-7B} finetuned on GSM8K or MATH.}\\label{exp:abl-effect-aug}\n\\vspace{-3.5mm}\n\\end{table}\n\t\n    \n  \n\n    \\begin{figure}[!b]\n    \\begin{minipage}{0.44\\linewidth}\n    \\vspace{-1em}\n    \\centering\n    \\includegraphics[width=0.99\\linewidth]{figs/ppl_box.pdf}\n    \\vspace{-1.85em}\n\t\\caption{\\footnotesize  Lower perplexity of MetaMathQA.}\n\t\\label{fig:perplexity}\n    \\vspace{-0.5em}\n    \\end{minipage}\n    \\hfill\n    \\begin{minipage}{0.54\\linewidth}\n    \\vspace{-1em}\n    \\centering\n    \\includegraphics[width=0.99\\linewidth]{figs/table5.pdf}\n    \\vspace{-2.18em}\n\t\\caption{\\footnotesize  Accuracy correlates positively with diversity.}\n\t\\label{fig:DiversityGains}\n    \\vspace{-0.5em}\n    \\end{minipage}\n    \\end{figure}\n\n        \\begin{figure}[b]\n    \\begin{minipage}{0.3\\linewidth}\n    \\centering\n    \\vspace{-1.2em}\n    \\includegraphics[width=0.99\\linewidth]{figs/diff_bar.pdf}\n    \\vspace{-1.8em}\n\t\\caption{\\footnotesize Combing RFT~\\citep{yuan2023scaling} dataset with our MetaMathQA leads to a performance drop.}\n\t\\label{fig:Less is More}\n    \\vspace{-0.7em}\n    \\end{minipage}\n    \\hfill\n    \\begin{minipage}{0.3\\linewidth}\n    \\centering\n    \\vspace{-1.2em}\n    \\includegraphics[width=0.99\\linewidth]{figs/gsm8k_backward.pdf}\n    \\vspace{-1.8em}\n\t\\caption{\\footnotesize The accuracy gap between GSM8K and GSM8K-Backward.}\n\t\\label{fig:reverse}\n    \\vspace{-0.7em}\n    \\end{minipage}\n    \\hfill\n    \\begin{minipage}{0.3\\linewidth}\n    \\centering\n    \\vspace{-1.2em}\n    \\includegraphics[width=0.99\\linewidth]{figs/question_length.pdf}\n    \\vspace{-1.8em}\n\t\\caption{\\footnotesize Testing accuracy on questions with short length, medium length and long length.}\n\t\\label{fig:erroranalysis}\n    \\vspace{-0.7em}\n    \\end{minipage}\n    \\end{figure}\n\n    \\vspace{-2mm}\n    \\subsection{Discussion from a Perplexity Perspective}    \n    \\vspace{-1.5mm}\n\n    According to the Superficial Alignment Hypothesis proposed by ~\\citet{zhou2023lima}, the capability of a model is rooted in pretraining, and data from downstream tasks acts to activate the inherent ability of LLMs that has been learned during pretraining. There are two important questions that arise from such a hypothesis: (i) \\textit{what} kind of data is most effective at activating possible latent knowledge, and (ii) \\textit{why} is one dataset better than another at such activation? Our empirical results suggest that, in the mathematical tasks we consider, our MetaMathQA dataset may serve as a superior activator of mathematical knowledge. Yet, \\textit{why} MetaMath yields superior performance than training on the data of correct answer-only or GSM8K CoT is unclear. We speculate that perhaps it is the simplicity of the data that matters. As shown in Figure~\\ref{fig:perplexity}, we compute the perplexity~\\citep{wang2023making,marion2023less} for the under-finetuned {LLaMA-2-7B} model, in terms of answer-only data, GSM8K CoT, and the subsections of {MetaMathQA} data. The perplexity of {MetaMathQA} is significantly lower than the other two datasets. This highlights its inherently easy-to-learn nature, which may be more conducive to eliciting bolstered problem-solving abilities from an LLM. This is also aligned with the findings with TinyStories~\\citep{eldan2023tinystories}, where short and easy story data can help LLMs generate content fluently.\n\n    \n        \n\n    \n\n    \\vspace{-2.00mm}\n    \\subsection{Discussion from a Diversity perspective}\n    \\vspace{-1.5mm}\n    As shown in Figure~\\ref{fig:Accuracy Saturation}, naively prompting GPT-3.5-Turbo for answer augmentation leads to a clear accuracy saturation. After accuracy saturation, increasing the AnsAug data only yields a limited performance gain. For instance, using 80K answer augmentation data to train a LLaMA-2 7B model leads to a 59.6\\% accuracy, adding new 20K AnsAug data would only take 0.1\\% performance gain. This is due to the homogeneity of the additional samples, contributing to a diversity gain of only 0.05 (shown in Figure~\\ref{fig:DiversityGains}). In comparison, adding the same amount of data generated by question bootstrapping leads to a significant performance boost, which is due to the noticeable diversity gain brought by question bootstrapping. As shown in Figure~\\ref{fig:DiversityGains}, adding 20K data from Rephrasing, FOBAR, or SV takes an increasing diversity gain, thus causing a 0.4\\%, 2.3\\%, and 2.6\\% accuracy gain, respectively. This experiment demonstrates a positive correlation (the Pearson coefficient is 0.972) between the diversity brought by the bootstrapping methods and accuracy. This is also aligned with the success of MetaMath, which is trained with the diverse MetaMathQA dataset including 4 kinds of data reflecting both the forward and backward reasoning paths.\n\n\\vspace{-2.00mm}\n\\subsection{Evaluating the Reversal Mathematical Capability}\n \\vspace{-1.5mm}\nThe Reversal Curse~\\citep{berglund2023reversal}, where LLMs trained from a sentence ``A is B\" are not able to generalize to answer ``B is A\", also aligns with the observation in this paper that LLMs lack backward mathematical reasoning ability. To evaluate the backward mathematical capability, we propose a GSM8K-Backward test set, including 1270 backward questions by using SV and FOBAR to augment the original GSM8K test set (as shown in Example~\\ref{exmp:sv-example} and Example~\\ref{exmp:fobar}). \nFigure~\\ref{fig:reverse} shows the accuracy comparison of different 7B mathematical LLMs between the GSM8K and GSM8K-Backward datasets. As can be seen, existing LLMs struggle to solve mathematical problems in backward rationales and our MetaMath has a significant improvement on both datasets. Specifically, the ways where different LLMs solve the backward mathematical problem are illustrated through examples in Appendix~\\ref{app:reversestudy}.\n\n    \\vspace{-2.00mm}\n    \\subsection{Reasoning Paths with Incorrect Answer Can Also Be Useful}\n    \\vspace{-1.5mm}\n\n    \\setlength{\\columnsep}{9pt}\n\n        \\begin{wraptable}{r}[0cm]{0pt}\n    \\centering\n    \\footnotesize\n    \\setlength{\\tabcolsep}{2.6pt}\n\t\\renewcommand{\\arraystretch}{1.15}\n    \\begin{tabular}{c|c}\n     \\specialrule{0em}{0pt}{-14pt}\n        Data & Accuracy \\\\\\shline\n        GSM8K~\\citep{cobbe2021training}  &  41.6  \\\\\n        Incorrect Answers   & 43.6 \\\\\n        Correct Answers  & \\textbf{52.2}  \\\\\n        \\specialrule{0em}{0pt}{-5pt}\n    \\end{tabular}\n    \\vspace{-0.5cm}\n    \\caption{\\footnotesize Testing accuracy on GSM8K of {LLaMA-2-7B} trained on different data.}\n    \\label{table:incorrect} \n    \\vspace{-0.25cm}\n    \\end{wraptable}\n    \n    We conduct experiments on GSM8K using {LLaMA-2-7B} to study whether the answer augmentation samples with incorrect answers are helpful for finetuning the LLM. We randomly choose 7,473 reasoning paths with incorrect answers from the generated answers, and we ensure that the size is the same as that of the original training set. From Table~\\ref{table:incorrect}, we observe that the model finetuned on the augmented data with incorrect answers is still better than SFT, which is counter-intuitive. We hypothesize that although the final answer is incorrect, some intermediate reasoning steps are correct (see Example \\ref{exmp:incorrect}). These reasoning steps can still be useful supervision signals. Our results are also aligned with~\\citep{lightman2023let}, where they discover the importance of intermediate process supervision for reasoning.\n    \n    \n\n    \n    \\begin{exmp}{A Reasoning Path with Incorrect Answer}{incorrect}\\small\n    \\textbf{Question:} Tonya is in a hamburger eating contest. Each hamburger is 4 ounces. Last year the winner ate 84 ounces. How many hamburgers does she have to eat to beat last year's winner? {\\color{red3}(Ground-truth answer is $22$)}\n    \n    \\textbf{Generated Answer:} If each hamburger is 4 ounces and last year's winner ate 84 ounces, then Tonya needs to eat 84/4 = 21 hamburgers to beat last year's winner. The answer is: 21\n    \\end{exmp}\n\n\\vspace{2mm}  \n\\subsection{More Data is not Always Better}\n\\vspace{-1mm}\n    \nThere are also previous works that augment mathematical reasoning data for fine-tuning~\\citep{luo2023wizardmath,yuan2023scaling}. An interesting question is whether combining existing augmented datasets with our {MetaMathQA} can improve the overall mathematical problem-solving performance. We select the RFT~\\citep{yuan2023scaling} dataset as the external dataset. Figure~\\ref{fig:Less is More} shows that merging the RFT data into {MetaMathQA} actually hurts the performance,\nindicating that the RFT data may not be beneficial to {MetaMath}. Such a phenomenon is consistently observed in the {MetaMathQA} dataset under different sizes (from 20K to 100K), and the added RFT dataset is about 47K. The performance drop implies that more augmented data does not always help the generalization. \n\n\\vspace{2.5mm}\n\\begin{exmp}{Case Study in Test Set of GSM8K}{case-study}\\small\n\\textbf{Question:}\nDarrell and Allen's ages are in the ratio of 7:11, If their total age now is 162, calculate Allen's age 10 years from now. {\\color{red3}(Ground-truth answer is $109$)}\n\n\\textbf{SFT Answer:}\nThe total ratio representing their ages is 7+11=$<<$7+11=18$>>$18. The fraction representing Darrell's age is 7/18, and since their total age now is 162, Darrell's age is 7/18*162 = $<<$7/18*162=60$>>$60. The fraction representing Allen's age is 11/18, and since their total age now is 162, Allen's age is 11/18*162 = $<<$11/18*162=84$>>$84. Ten years from now, Allen's age will be 84 + 10 = $<<$84+10=94$>>$94.\n\n\\textbf{RFT Answer:}\nThe total ratio representing Darrell and Allen's ages is 7+11 = $<<$7+11=18$>>$18. The fraction representing Darrell's age from the total ratio is 7/18, and since their total age is 162, Darrell is 7/18*162 = 100 years old. Allen's age, from the total ratio, is 11/18, and since Darrell is 100, Allen is 11/18*109 = $<<$11/18*100=66$>>$66 years old. Ten years from now, Allen will be 66 + 10 = 76 years old.\n\n\\textbf{WizardMath Answer:}\nWe know Darrell and Allen's ages are in the ratio of 7:11, which means the total ratio representing their ages is 7+11 = 18. If their total age now is 162, we can calculate Darrell's age by using the ratio. Specifically, Darrell's age is 7/18 of 162, which is 7/18*162 = 49 years old.  ...  The answer is: 76.\n\n\\textbf{Our Answer:}\nThe ratio of Darrell's age to Allen's age is 7:11. Let's assume Darrell's age is 7x and Allen's age is 11x. The total age of Darrell and Allen is 7x + 11x = 18x. We are given that the total age is 162, so 18x = 162. Dividing both sides by 18, we get x = 9. Therefore, Allen's age is 11x = 11 * 9 = 99. 10 years from now, Allen's age will be 99 + 10 = 109.\n\\end{exmp}\n\n\\vspace{2mm}\n\\subsection{Error Analysis}\n\\vspace{-1mm}\n\nWe have demonstrated that -- across multiple scales -- our MetaMath models can achieve stellar problem-solving performance. Yet, it is important to consider the characteristics of problems that induce errors in MetaMath and existing open-source mathematical models. In particular, we consider the relationship between question length and model performance. To investigate, we divide the GSM8K test set into three equally-sized subsets based on the different lengths of questions and calculate the accuracy of the models over each subset. We find in Figure~\\ref{fig:erroranalysis} that, MetaMath and related methods struggle under longer questions. However, excitingly, MetaMath always obtains superior performance. We see the study of improving model performance with longer question lengths -- for instance, by further augmenting the MetaMathQA dataset -- as ripe grounds for future work. \n\\vspace{-.5mm}\n\t\n\n\\section{Concluding Remarks}\n\\label{sec:conclusion}\n\n \\vspace{-1mm}\n In this paper, we focus on improving the mathematical problem-solving abilities of open-source LLMs. By bootstrapping mathematical questions on GSM8K and MATH, we present a high-quality and diverse dataset {MetaMathQA}, involving forward reasoning and backward reasoning samples. Our family of LLMs finetuned on {MetaMathQA}, called {MetaMath}, have achieved state-of-the-art on mathematical benchmarks among all open-source LLMs. Remarkably, {MetaMath-7B} reaches $66.5\\%$ on GSM8K and $19.8\\%$ on MATH, surpassing previous open-source LLMs by a significant margin. Our work further emphasizes the importance of the characteristics of the training data on boosting LLM problem-solving capabilities.\n\n\\newpage\n\n\\section*{Acknowledgement}\nThe authors would like to sincerely thank Katherine M. Collins from University of Cambridge for her valuable insights and suggestions.\n\nThis work was supported by NSFC key grant 62136005,\nNSFC general grant 62076118, and Shenzhen fundamental research program JCYJ20210324105000003. This research was supported in part by the Research Grants Council\nof the Hong Kong Special Administrative Region (Grants\n16200021 and 16202523). AW acknowledges support from a Turing AI Fellowship under grant EP/V025279/1, and the Leverhulme Trust via CFI. \n\t\n \n \\clearpage\n \\newpage\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2210.02414v2.tex",
        "arXiv-2308.09583v2.tex",
        "arXiv-2309.12284v4.tex"
    ],
    "group_id": "group_97",
    "response": "### Summary of Large Language Model Research\n\n#### Title: Enhancing Large Language Models with Novel Architectures and Training Techniques\n\nLarge language models (LLMs) have become a cornerstone in natural language processing (NLP) and other computational tasks, significantly advancing the field with their ability to understand and generate text at a scale previously unattainable. These models, which typically encompass billions of parameters, have shown remarkable performance across a wide range of benchmarks, from language modeling to complex reasoning tasks. However, the development of LLMs has been largely monopolized by large corporations, limiting accessibility and usability for researchers and developers worldwide. This summary explores three recent research papers that address these challenges by introducing new architectures, training strategies, and inference techniques for LLMs, aiming to make these powerful models more inclusive and accessible.\n\n#### Introduction\n\nThe field of LLMs has seen rapid advancements in recent years, with models like GPT-3, PaLM, and BLOOM demonstrating unprecedented capabilities in understanding and generating text. These models are trained on vast datasets, often exceeding 100 billion parameters, and have been shown to exhibit emergent zero-shot and few-shot learning abilities. However, the training and inference of such models require substantial computational resources, making them inaccessible to many researchers and developers. Moreover, the training of these models often faces numerous technical and engineering challenges, such as loss spikes and instability, which can significantly affect their performance and usability. This summary focuses on three recent papers that introduce novel approaches to enhance the performance, stability, and accessibility of LLMs, particularly in the context of mathematical reasoning and bilingual capabilities.\n\n#### Main Content of Each Paper\n\n**Paper 1: GLM-130B: An Open Bilingual Pre-Trained Model**\n\nThe first paper introduces GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. GLM-130B is designed to be open-source and accessible, with a focus on both quality and usability. The model is trained using a unique architecture called General Language Model (GLM), which leverages bidirectional attention and autoregressive blank infilling objectives. This architecture is different from the traditional GPT-style models, which use unidirectional attention. The GLM architecture is particularly effective in downstream evaluations, as it allows for better comprehension of context information. GLM-130B is trained on a cluster of 96 NVIDIA DGX-A100 (8x40G) GPU nodes, using a combination of data parallelism, tensor model parallelism, and pipeline model parallelism to handle the massive computational requirements. The training process includes several innovations, such as the use of DeepNorm for layer normalization and the introduction of Embedding Layer Gradient Shrink (EGS) to stabilize training. GLM-130B is also optimized for inference, with the ability to be quantized into INT4 precision without post-training, significantly reducing GPU memory requirements and enabling fast inference on affordable hardware like 4x RTX 3090 (24G) or 8x RTX 2080 Ti (11G) GPUs. The model's performance is evaluated on a variety of benchmarks, including LAMBADA, Pile, MMLU, BIG-bench, CLUE, and FewCLUE, demonstrating superior zero-shot and few-shot capabilities compared to other models like GPT-3, OPT-175B, and BLOOM-176B. GLM-130B also exhibits lower bias and generation toxicity than its counterparts, aligning with the authors' commitment to promoting inclusivity in LLM research.\n\n**Paper 2: WizardMath: Enhancing Mathematical Reasoning for Large Language Models via Reinforcement Learning with Evolved Instructions**\n\nThe second paper presents WizardMath, a method to enhance the mathematical reasoning abilities of LLMs without relying on external Python tools. WizardMath leverages a novel approach called Reinforcement Learning with Evolved Instructions (RLEIF), which includes two main components: Math Evol-Instruct and Process Supervised Reward Models (PRMs). Math Evol-Instruct generates diverse mathematical instructions by evolving them in both downward and upward directions, making the questions easier and more complex, respectively. This evolution process is designed to increase the diversity and complexity of the training data, thereby improving the model's reasoning capabilities. The PRMs are trained to provide feedback on the correctness of each reasoning step, helping to mitigate false positives and improve the overall accuracy of the model. WizardMath is evaluated on two benchmarks, GSM8K and MATH, demonstrating significant improvements over existing open-source LLMs and even surpassing some proprietary models. For instance, WizardMath-Mistral 7B achieves a pass@1 accuracy of 90.7\\% on GSM8K and 55.4\\% on MATH, outperforming GPT-3.5-Turbo by 11.2\\% and 32.0\\%, respectively. The effectiveness of WizardMath is further highlighted by its superior performance on out-of-domain evaluation tasks, indicating strong generalization ability.\n\n**Paper 3: MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models**\n\nThe third paper introduces MetaMath, a method to fine-tune LLMs for better mathematical reasoning by bootstrapping questions through various augmentation techniques. MetaMath generates augmented data by rephrasing questions, applying backward reasoning, and using answer augmentation. The rephrasing and backward reasoning techniques aim to increase the diversity of the training data, while answer augmentation helps in collecting correct reasoning paths. MetaMath is evaluated on GSM8K and MATH benchmarks, showing significant improvements over existing open-source LLMs. For example, MetaMath-7B achieves a pass@1 accuracy of 66.5\\% on GSM8K and 19.8\\% on MATH, surpassing the previous best open-source LLMs by 11.5\\% and 8.7\\%, respectively. MetaMath also demonstrates strong performance on out-of-domain evaluation tasks, indicating its robustness and generalization ability. The authors emphasize the importance of data diversity and the simplicity of the data in improving the model's reasoning capabilities.\n\n#### Commonalities and Innovations\n\nAll three papers aim to enhance the performance and usability of LLMs, particularly in the context of mathematical reasoning and bilingual capabilities. They introduce novel methods and architectures to address the challenges faced during training and inference. GLM-130B uses a bidirectional attention mechanism and DeepNorm for layer normalization, while WizardMath leverages Reinforcement Learning with Evolved Instructions (RLEIF) to improve reasoning paths. MetaMath focuses on generating diverse and high-quality mathematical questions through bootstrapping and augmentation techniques. These innovations contribute to the models' superior performance on various benchmarks and highlight the importance of architectural design and training strategies in achieving high-quality LLMs.\n\n#### Comparison of Results\n\nThe performance of the models introduced in these papers is evaluated on a range of benchmarks, including GSM8K, MATH, CLUE, and FewCLUE. GLM-130B demonstrates strong performance on both English and Chinese benchmarks, outperforming GPT-3 and other similar-sized models. WizardMath-Mistral 7B achieves a pass@1 accuracy of 90.7\\% on GSM8K and 55.4\\% on MATH, outperforming GPT-3.5-Turbo by 11.2\\% and 32.0\\%, respectively. MetaMath-7B reaches a pass@1 accuracy of 66.5\\% on GSM8K and 19.8\\% on MATH, surpassing the previous best open-source LLMs by 11.5\\% and 8.7\\%, respectively. These results indicate the effectiveness of the novel methods and architectures introduced in each paper, particularly in enhancing mathematical reasoning capabilities.\n\n#### Conclusion\n\nThe three papers discussed in this summary highlight the importance of architectural design, training strategies, and data augmentation in improving the performance and usability of LLMs. GLM-130B introduces a bidirectional attention mechanism and unique training strategies to achieve superior performance on a wide range of benchmarks. WizardMath leverages Reinforcement Learning with Evolved Instructions (RLEIF) to enhance reasoning paths, demonstrating strong performance on mathematical reasoning tasks. MetaMath focuses on generating diverse and high-quality mathematical questions through bootstrapping and augmentation techniques, achieving significant improvements over existing open-source LLMs. These innovations not only improve the models' performance but also make them more accessible and usable for researchers and developers. Future research directions could include further exploration of these methods on other reasoning tasks and the development of more efficient and inclusive training strategies for LLMs.\n\n### Summary of Key Contributions\n\n**GLM-130B: An Open Bilingual Pre-Trained Model**\n\n- **Architecture**: GLM-130B uses a bidirectional attention mechanism and autoregressive blank infilling objectives, which allows for better comprehension of context information.\n- **Training Strategies**: The model is trained using a combination of data parallelism, tensor model parallelism, and pipeline model parallelism, along with DeepNorm for layer normalization and Embedding Layer Gradient Shrink (EGS) to stabilize training.\n- **Inference Optimization**: GLM-130B is optimized for inference, with the ability to be quantized into INT4 precision without post-training, significantly reducing GPU memory requirements and enabling fast inference on affordable hardware.\n\n**WizardMath: Enhancing Mathematical Reasoning for Large Language Models via Reinforcement Learning with Evolved Instructions**\n\n- **Method**: WizardMath uses Reinforcement Learning with Evolved Instructions (RLEIF) to improve reasoning paths, which includes generating diverse mathematical instructions and using Process Supervised Reward Models (PRMs) for feedback.\n- **Performance**: WizardMath-Mistral 7B achieves a pass@1 accuracy of 90.7\\% on GSM8K and 55.4\\% on MATH, outperforming GPT-3.5-Turbo by 11.2\\% and 32.0\\%, respectively.\n- **Innovation**: The method is innovative in its use of AI models to automatically offer process annotation, improving the efficiency of training and evaluation.\n\n**MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models**\n\n- **Method**: MetaMath generates augmented data by rephrasing questions, applying backward reasoning, and using answer augmentation, resulting in a diverse and high-quality dataset called MetaMathQA.\n- **Performance**: MetaMath-7B achieves a pass@1 accuracy of 66.5\\% on GSM8K and 19.8\\% on MATH, surpassing the previous best open-source LLMs by 11.5\\% and 8.7\\%, respectively.\n- **Innovation**: The method is innovative in its use of question bootstrapping to increase data diversity and improve reasoning capabilities.\n\n### Future Research Directions\n\nThe research presented in these papers opens up several avenues for future exploration:\n- **Architectural Design**: Further research could explore the effectiveness of other novel architectures in improving LLM performance.\n- **Training Strategies**: More work is needed to develop and refine training strategies that balance efficiency and stability, particularly for models with over 100 billion parameters.\n- **Data Augmentation**: The importance of data augmentation and diversity in training LLMs for specific tasks, such as mathematical reasoning, should be further studied.\n- **Inference Optimization**: Continued efforts should be made to optimize inference for LLMs, making them more accessible and usable on affordable hardware.\n\nIn conclusion, these papers contribute significantly to the field of LLMs by introducing novel methods and architectures that enhance their performance and usability, particularly in the context of mathematical reasoning and bilingual capabilities."
}