{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers}\n\n\\begin{document}\n\n\\runningtitle{Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers}\n\n\\runningauthor{Choromanski, Li, Likhosherstov, Dubey, Luo, He, Yang, Sarlos, Weingarten, Weller}\n\n\\twocolumn[\n\n\\aistatstitle{Learning a Fourier Transform for\\\\Linear Relative Positional Encodings in Transformers}\n\n\\aistatsauthor{Krzysztof Marcin Choromanski$^{*1,2}$ $\\qquad$\nShanda Li$^{*3}$  $\\qquad$\nValerii Likhosherstov$^{4}$ \n}\n\\aistatsauthor{Avinava Dubey$^{1}$  $\\qquad$ Shengjie Luo$^{5}$  $\\qquad$ Di He$^{5}$   $\\qquad$  Yiming Yang$^{3}$ \n}\n\\aistatsauthor{Tamas Sarlos$^{1}$  $\\qquad$\nThomas Weingarten$^{1}$  $\\qquad$\nAdrian Weller$^{6,7}$  }\n\\vspace{0.5mm}\n\\aistatsaddress{$^{1}$Google Research $\\qquad$ $^{2}$Columbia University $\\qquad$\n$^{3}$Carnegie Mellon University}\n\\vspace{-7.5mm}\n\\aistatsaddress{$^{4}$Waymo  $\\qquad$\n$^{5}$Peking University  $\\qquad$\n$^{6}$University of Cambridge $\\qquad$\n$^{7}$Alan Turing Institute }\n\\vspace{-7.5mm}\n\\aistatsaddress{\\texttt{\\{kchoro,avinavadubey\\}@google.com}  $\\qquad$\n\\texttt{shandal@cs.cmu.edu}}\n\\vspace{-6mm}\n\\aistatsaddress{$^*$\\footnotesize\\sffamily These two authors contributed equally. The authorship is in alphabetical order.}\n]\n\n\\begin{abstract}\nWe propose a new class of linear Transformers called $\\mathrm{FourierLearner}$-$\\mathrm{Transformers}$ (\\FLTs), which incorporate a wide range of relative positional encoding mechanisms (RPEs). These include regular RPE techniques applied for sequential data, as well as novel RPEs operating on geometric data embedded in higher-dimensional Euclidean spaces. \\FLTs\\ construct the optimal RPE mechanism implicitly by learning its spectral representation. As opposed to other architectures combining efficient low-rank linear attention with RPEs, \\FLTs\\ remain practical in terms of their memory usage and do not require additional assumptions about the structure of the RPE mask. Besides, \\FLTs\\ allow for applying certain structural inductive bias techniques to specify masking strategies, e.g. they provide a way to learn the so-called \\textit{local RPEs} introduced in this paper and give accuracy gains as compared with several other linear Transformers for language modeling. We also thoroughly test \\FLTs\\ on other data modalities and tasks, such as image classification, 3D molecular modeling, and learnable optimizers. To the best of our knowledge, for 3D molecular data, \\FLTs\\ are the first Transformer architectures providing linear attention and incorporating RPE masking.\n\\end{abstract}\n\n\\vspace{-2mm}\n\\section{INTRODUCTION}\n\\label{sec:intro}\n\nTransformers have revolutionized the landscape of machine learning, introducing a paradigm shift in the way that people approach complex tasks in natural language processing (NLP) \\cite{devlin}, computer vision (CV) \\cite{vits}, molecular modeling \\cite{jumper2021highly}, and beyond. \n\nThe largest computational bottleneck in Transformers is also the source of their success, the attention module. The attention module propagates signals between different tokens in the input sequence and has quadratic time and space complexity with respect to the input length $L$, which limits its scalability to long sequences. Thus, designing efficient attention modules has been an active area of research. Recently, the research on ``efficient'' Transformers has taken on new importance as the size of Transformer models grew from the GPT-1 architecture of ``only'' \\textbf{117M} parameters to GPT-3 with \\textbf{175B} parameters, a \\textbf{1000$\\times$} increase within just two years \\cite{gpt3}. \n\nOne class of efficient Transformers is based on \\textit{sparse attention} \\cite{li2019enhancing, local-attention,zaheer2020big,routing, apoorv, reformer, sparse-hash}. \nThese methods do not aim at approximating the regular attention, but rather propose simpler and more tractable attention mechanisms, sometimes with additional constraints (e.g. identical queries and keys \\cite{reformer}). Another popular class of efficient Transformers is based on the \\textit{kernelized attention} \\cite{choromanski,tsai2019transformer,katharopoulos2020transformers}. The key idea is to find an approximate low-rank decomposition of the attention matrix and leverage it to improve space and time complexity of the attention mechanism via the associativity property of matrix multiplications. Performer \\cite{choromanski} is a successful example of this model class. In contrast to previously discussed methods, Performer's approximate attention matrix (which is never explicitly constructed but rather implicitly used) is an unbiased estimate of the original attention matrix encoding similarities between tokens via the softmax kernel. Performers have been adopted into many Transformer stacks to provide linear space and time complexity \\cite{perf-vit-1, perf-vit-2, tay2021omninet,performer-mpc}.\n\nUnfortunately, the simplicity of Performers comes at a price. It is well known that incorporating structural inductive priors -- which is usually implemented via various additive relative masking mechanisms in regular attention architectures -- is difficult for Performers. We refer to these methods as \\textit{Relative Positional Encodings} (RPEs) \\cite{shaw,raffel,li2021can,luo2022your}.\nRPEs play a critical role in improving the performance of Transformers in long-range modeling for language \\cite{dai-etal-2019-transformer}, speech \\cite{liutkus}, vision \\cite{wupeng}, and genomic data \\cite{Avsec2021EffectiveGE}. However, at first glance, Performers are not compatible with general RPE techniques, since they seem to require explicit materialization of the attention matrix to apply the RPE mask, which is exactly what Performers avoid in order to achieve computational improvements.\nSubstantial efforts are made to reconcile Performers with RPEs (more details in Sec. \\ref{sec:related}), but so far all these attempts fall short of providing two properties at the same time: (a) practical computational gains, and (b) inclusion of general RPE methods, for inputs with nontrivial topological structures.\n\nIn this paper, we propose a new class of linear Transformers called $\\mathrm{FourierLearner}$-$\\mathrm{Transformers}$ (\\FLTs), which incorporate a wide range of relative positional encoding mechanisms (RPEs). These include regular RPE techniques applied for sequential data, and novel RPEs operating on geometric data embedded in higher-dimensional Euclidean spaces (e.g. molecular structures). \\FLTs\\ construct the optimal RPE mechanism implicitly by learning its spectral representation, and enjoy provable uniform convergence guarantees. As opposed to other architectures combining efficient low-rank linear attention with RPEs, \\FLTs\\ remain practical in terms of their memory usage and do not require additional assumptions about the structure of the RPE mask. Besides, \\FLTs\\ allow the application of certain structural inductive bias techniques to specify masking strategies, e.g. they provide a way to learn what we call \\textit{local RPEs}, introduced in this paper and providing accuracy gains compared with several other linear Transformers for language modeling. We also thoroughly test \\FLTs\\ on other data modalities and tasks, such as image classification and molecular modeling. To the best of our knowledge, for 3D molecular data, \\FLTs\\ are the first Transformer architectures providing linear attention and incorporating RPE masks, which broadens the scope of RPE-enhanced linear attention.\n\nTo summarize, our main contributions are as follows:\n\\vspace{-2mm}\n\\begin{itemize}\n    \\item We introduce the proposed RPE-enhanced linear attention,    $\\mathrm{FourierLearner}$-$\\mathrm{Transformers}$ (\\FLTs). \\FLTs~are applicable to not only sequential data (e.g., texts) but also geometric data embedded in higher-dimensional Euclidean spaces (e.g., 3D molecular data), significantly broadening the scope of RPE-enhanced linear attention.\n    \n    \\item We provided detailed theoretical analysis on \\FLTs, including the uniform convergence and sample complexity bound on its approximation (Sec. \\ref{subsec:flt}). We discuss several instantiations, in particular \\FLTs\\ with so-called Gaussian mixture RPEs, shift-invariant kernel RPEs and local RPEs (Sec. \\ref{sec:topology}).\n\n    \\item We extensively evaluate \\FLTs~on language modeling (Sec. \\ref{sec:lm}), image classification (Sec. \\ref{sec:images}), and molecular property predictions (Sec. \\ref{sec:molecular_dynamics}). Our experiments show that \\FLTs~can be easily applied to a wide range of data modalities and demonstrate strong performance and efficiency. \n\\end{itemize}\n\\section{RELATED WORKS}\n\\label{sec:related}\n\\paragraph{Kernelized attention with RPE.}\nOne of the first attempts to address the problem of combining kernelized attention Transformers with RPEs is \\cite{liutkus}, where two variants, namely $\\mathrm{sineSPE}$ and $\\mathrm{convSPE}$, are proposed. Both variants model the RPE mask as a stationary position kernel with a Toeplitz mask structure. While their complexity is linear in the sequence length $L$, extra dependency on the number of sinusoidal components $T$ (for $\\mathrm{sineSPE}$) / the convolution filter lengths $P$ (for $\\mathrm{convSPE}$) is introduced. In practice, $T$ or $P$ has to be sufficiently small due to computational budgets. Besides, they constrain the RPE mask to be a valid kernel matrix, while our \\FLTs~do not require such assumptions. Both $\\mathrm{sineSPE}$ and $\\mathrm{convSPE}$ significantly underperform \\FLTs~on language modeling (Sec. \\ref{sec:lm}). And they cannot be applied for more general RPEs with tokens embedded in the higher-dimensional Euclidean spaces, e.g., RPEs for 3D molecular data.\n\nRecently, \\cite{rpe-performers, topmasking} show that the RPE mechanism can be combined with Performers in $O(L\\log(L))$ time complexity. The method relies on the elegant observation that log-linear time complexity can be achieved as long as the exponentiated RPE mask supports fast matrix-vector multiplication. RPEs for sequential data satisfy this condition since the corresponding masks have a Toeplitz structure. However, this method has large space complexity and high memory consumption in practice (Sec. \\ref{sec:lm}). \nMoreover, it heavily relies on the structure of sequential data and does not apply to 3D molecular data where the RPE masks do not have a Toeplitz structure.\n\n\\paragraph{Random Fourier features (RFFs).} There has been voluminous literature on the field of RFFs \\cite{rahimi, kapralov-rfs, szabo, li-rfs, hybrid_rfs, crts, tr-kernel}. However, the research on learnable RFF variants \\cite{learning-rfs-1} is relatively new. Furthermore, prior works are mostly narrowed to applying RFFs in the context of positive definite (PD) kernels, while our work breaks this limitation since RPEs do not need to be defined by PD kernels. Several papers also explore the development of Transformer-based models whose attention mechanism operates in the spectral domain \\cite{tamkin2020language, moreno2023deep}, but they do not study efficient RPE modeling.\n\n\\paragraph{Long sequence modeling.} Applying deep learning models to long sequences is an active research direction. \nWe study efficient Transformer models for long sequence modeling. \nWhile our focus lies within the Transformer realm, it's worth noting the existence of alternative, non-Transformer architectures \\cite{ham,bello2021lambdanetworks, s4}. \nBeyond efficiency, \\cite{o2021context, liu2024lost} probe context usage of long sequence language models; \\cite{press2022train, ruoss2023randomized, li2024functional} design sequence models with length generalization ability (i.e., training on short sequences and generalize to long sequences); \\cite{yun2020n, yang2024efficient} study the theoretical capability of those models. \nNote that existing works most focus on one data modality, while \\FLT~is evaluated across a wide range modalities.\\section{PRELIMINARIES}\n\\label{sec:preliminaries}\n\\paragraph{General RPE mechanism in Transformers.}\nConsider an input sequence $\\mathbf{X}\\in\\mathbb{R}^{L\\times d_{\\text{in}}}$ where $L$ and $d_{\\text{in}}$ denote the number and embedding size of tokens. The self-attention module in Transformers linearly projects the input into three matrices $\\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\in \\mathbb{R}^{L \\times d}$ called \\textit{queries}, \\textit{keys} and \\textit{values} respectively. We also associate all the tokens with positional features $\\mathbf{r}_{1},...,\\mathbf{r}_{L} \\in \\mathbb{R}^{\\ell}$ that are used to define the relative positional encoding (RPE) mechanism below:\n\n\\begin{definition}[General RPE for attention]\n\\label{gen_graph_attention}\n\\textit{General Relative Positional Encoding} enhanced attention is of the following form, where $\\mathbf{N} = [f(\\mathbf{r}_{i}-\\mathbf{r}_{j})]_{i,j\\in [L]} \\in \\mathbb{R}^{L \\times L}$ is the so-called \\textit{RPE mask}\\footnote{We use $[L]$ to denote $\\{1, \\cdots, L\\}$ in this paper.} and $f:\\mathbb{R}^{\\ell} \\rightarrow \\mathbb{R}$ is a (potentially learnable) functon:\n\\vspace{-1.5mm}\n\\begin{align}\n    &\\mathrm{Att}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V},\\mathbf{N}) = \\mathbf{D}^{-1} \\mathbf{A} \\mathbf{V}, \\nonumber \\\\\n    \\text{where }& \\mathbf{A} = \\exp \\left(\\mathbf{N}  +\\frac{\\mathbf{Q} \\mathbf{K}^\\top} {\\sqrt{d}}\\right), \\mathbf{D} = \\mathrm{diag} ( \\mathbf{A} \\mathbf{1}_L ).  \\label{eq:attnorm1}\n\\end{align}\n\\end{definition}\n\\vspace{-1.5mm}\n\nHere $\\exp (\\cdot)$ is applied element-wise, $\\mathbf{1}_L$ is the all-one vector of length $L$, and $\\mathrm{diag} (\\cdot)$ is a diagonal matrix with the input vector as the diagonal. The time complexity of computing Eq. (\\ref{eq:attnorm1}) is $O(L^2 d)$.\n\n\\underline{\\textit{Discussions.}} Definition \\ref{gen_graph_attention} is highly general because one can flexibly choose the representation the positions $\\mathbf{r}_i$ and the function $f$. For example, for sequential data like texts, positional indices in the sequence serves as the positional features ($\\mathbf{r}_i=i$), and the RPE mask is a learnable Topelitz matrix ($f(i-j)=c_{i-j}$ with parameters $\\{c_{k}\\}_{k=-(L-1)}^{L-1}$) \\cite{raffel}. For geometric data like 3D molecular structures, one can view $\\mathbf{r}_{j}$ as the 3D coordinates $\\mathbf{r}_{j}$ of tokens (e.g., atoms) and use some domain-specific $f$ \\cite{shi2022benchmarking}. We emphasize that the general formulation is novel and important. It motivates the highly general \\FLTs~applicable to a wide range of data and tasks, as opposed to existing approaches that heavily rely on the structure of sequential data and Toeplitz RPE masks (Sec. \\ref{sec:related}). \n\n\\paragraph{Kernelized linear attention.} Kernelized attention techniques, e.g., Performers, leverage a decomposition of the attention matrix $\\mathbf{A}$ to avoid explicit materialization of $\\mathbf{A}$, hence avoid the quadratic complexity in $L$. For the softmax attention, this is achieved by linearizing the softmax kernel $\\exp(\\mathbf{x}^{\\top}\\mathbf{y})$ via random features, i.e., constructing for certain randomized mappings $\\phi: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{m}$ such that $\\exp(\\mathbf{x}^{\\top}\\mathbf{y}) = \\mathbb{E}[\\phi(\\mathbf{x})^{\\top}\\phi(\\mathbf{y})]$.\n\nDefine $\\mathbf{Q}',\\mathbf{K}' \\in \\mathbb{R}^{L \\times m}$ as matrices of rows given as $\\phi(\\mathbf{q}_{i}^{\\top}d^{-\\frac{1}{4}})^{\\top}$ and $\\phi(\\mathbf{k}_{i}^{\\top}d^{-\\frac{1}{4}})^{\\top}$ respectively. Then the above linearization of softmax kernel directly leads to the following approximate algorithm for attention \\textit{without} RPE masks:\n\\begin{align}\n    \\widehat{\\mathrm{Att}_\\mathrm{K}} (\\mathbf{Q}, \\mathbf{K}, \\mathbf{V})&  = \\widehat{\\mathbf{D}}^{-1} (\\mathbf{Q}'(\\mathbf{K}'^{\\top} \\mathbf{V})) \\nonumber \\\\\n    \\text{where }\\widehat{\\mathbf{D}} &= \\mathrm{diag} (\\mathbf{Q}'(\\mathbf{K}'^{\\top} \\mathbf{1}_L) ). \\label{performers_attention}\n\\end{align}    \nHere $\\widehat{\\mathrm{Att}_{\\mathrm{K}}}$ stands for the approximate attention and brackets indicate the order of computations. The time and space complexity of this mechanism are $O(Lmd)$ and $O(Lm+md+Ld)$ respectively, compared to $O(L^{2}d)$ and $O(L^{2}+Ld)$ for regular attention. Thus, for $m \\ll L $, Performers provide substantial computational improvements.\n\n\\section{METHOD}\n\\label{sec:algorithms}\n\\subsection{Efficient RPE-enhanced attention}\n\\label{subsec:flt}\nThe algorithm presented in Eq. (\\ref{performers_attention}) does not incorporate RPE mechanisms. In this subsection, we first present in Theorem \\ref{thm:rpe-decompose} a novel technique to derive the (approximate) low rank decomposition of general RPE mask $\\mathbf{N}$ in Definition \\ref{gen_graph_attention}. Next, we introduce \\textrm{FourierLearner-Transformer} (\\FLT), a Performer-friendly RPE attention mechanism based on the decomposition. \n\n\\begin{algorithm*}\n\\caption{FourierLearner Transformer: linear-complexity RPE-enhanced attention}\n\\label{alg:flt}\n\\begin{algorithmic}[1] % The argument [1] ensures line numbers are printed.\n\\REQUIRE Input queries, keys, values $\\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\in \\mathbb{R}^{L \\times d}$ and positions $\\mathbf{R}\\in\\mathbb{R}^{L\\times \\ell}$; random feature map for attention $\\phi$ (see Sec. \\ref{sec:preliminaries}); Fourier Transform of the RPE function $g_{\\theta}$ (potentially parametrized by $\\theta$).\n\\ENSURE Approximate RPE-enhanced attention (Definition \\ref{gen_graph_attention})\n\\STATE \\texttt{\\# Apply random feature maps for RPE approximation.}\n\\STATE \\texttt{\\# $\\varphi$ and $\\psi$ are defined in Theorem \\ref{thm:rpe-decompose} and applied column-wise; $g_{\\theta}$ is called in $ \\varphi$ and $\\psi$.}\n\\STATE $\\mathbf{N}_{1}\\leftarrow \\varphi(\\mathbf{R})$, $\\mathbf{N}_{2}\\leftarrow \\psi(\\mathbf{R})$ \n\\STATE \\texttt{\\# Concatenate along the second axis.}\n\\STATE $\\widehat{\\mathbf{Q}} \\leftarrow [\\mathbf{N}_{1},\\mathbf{Q}d^{-\\frac{1}{4}}] $,\n$\\widehat{\\mathbf{K}} \\leftarrow [\\mathbf{N}_{2},\\mathbf{K}d^{-\\frac{1}{4}}]$ \n\\STATE \\texttt{\\# Apply random feature map $\\phi$.}\n\\STATE $\\mathbf{Q}' \\leftarrow \\phi(\\widehat{\\mathbf{Q}})$, $\\mathbf{K}' \\leftarrow \\phi(\\widehat{\\mathbf{K}})$\n\\STATE \\texttt{\\# Kernelized linear attention (Sec. \\ref{sec:preliminaries}). Brackets indicate the order of computations.}\n\\STATE $\\mathbf{B}_1\\leftarrow \\mathbf{Q}'(\\mathbf{K}'^{\\top} \\mathbf{V})$, $\\mathbf{B}_2\\leftarrow \\mathbf{Q}'(\\mathbf{K}'^{\\top} \\mathbf{1}_L)$, $\\mathbf{O}\\leftarrow\\mathrm{diag}(\\mathbf{B}_2)^{-1}\\mathbf{B}_1$.\n\\RETURN $\\mathbf{O}$\n\\end{algorithmic}\n\\end{algorithm*}\n\n\\begin{theorem}\n\\label{thm:rpe-decompose}\n    Given $f:\\mathbb{R}^{\\ell} \\rightarrow \\mathbb{R}$ and\n    $\\mathbf{N} = [f(\\mathbf{r}_{i}-\\mathbf{r}_{j})] \\in \\mathbb{R}^{L \\times L}$ as defined in Definition \\ref{gen_graph_attention}, denote by $g$ the Fourier Transform of $f$. Assume $p$ is some probability density function supported over $\\mathbb{R}^{\\ell}$. Sample $\\xi_1, \\cdots, \\xi_r\\overset{\\mathrm{iid}}{\\sim} p$ and define the following random feature maps (where $\\mathbf{i}=\\sqrt{-1}$):\n    \\begin{align*}\n        \\varphi(\\mathbf{z})=&\\tfrac{1}{\\sqrt{r}}\\left[\\mathrm{e}^{2\\pi \\mathbf{i}\\mathbf{z}^{\\top}\\boldsymbol{\\xi}_1}\\sqrt{\\tfrac{g(\\boldsymbol{\\xi}_1)}{p(\\boldsymbol{\\xi}_1)}}, \\cdots, \\mathrm{e}^{2\\pi \\mathbf{i}\\mathbf{z}^{\\top}\\boldsymbol{\\xi}_r}\\sqrt{\\tfrac{g(\\boldsymbol{\\xi}_r)}{p(\\boldsymbol{\\xi}_r)}}\\right]^{\\top};\\\\\n        \\psi(\\mathbf{z})=&\\tfrac{1}{\\sqrt{r}}\\left[\\mathrm{e}^{-2\\pi \\mathbf{i}\\mathbf{z}^{\\top}\\boldsymbol{\\xi}_1}\\sqrt{\\tfrac{g(\\boldsymbol{\\xi}_1)}{p(\\boldsymbol{\\xi}_1)}}, \\cdots, \\mathrm{e}^{-2\\pi \\mathbf{i}\\mathbf{z}^{\\top}\\boldsymbol{\\xi}_r}\\sqrt{\\tfrac{g(\\boldsymbol{\\xi}_r)}{p(\\boldsymbol{\\xi}_r)}}\\right]^{\\top},\n    \\end{align*}\n    Define $\\mathbf{N}_{1}=  \\left[\\varphi(\\mathbf{r}_{1}),\\cdots,\\varphi(\\mathbf{r}_{L})\\right]^{\\top} \\in \\mathbb{R}^{L \\times r}$ and $\\mathbf{N}_{2}=\\left[\\psi(\\mathbf{r}_{1}),\\cdots,\\psi(\\mathbf{r}_{L})\\right]^{\\top} \\in \\mathbb{R}^{L \\times r}$. Then $\\mathbb E [\\mathbf{N}_{1}\\mathbf{N}_{2}] = \\mathbf{N}$.\n\\end{theorem}\n\n\\paragraph{Performer-friendly RPE attention.}Theorem \\ref{thm:rpe-decompose} implies that $\\widehat{\\mathbf{N}}=\\mathbf{N}_{1}\\mathbf{N}_{2}$ is a low-rank unbiased estimator of $\\mathbf{N}$. Consequently, a Performer-friendly RPE attention mechanism with linear complexity can be obtained. Specifically, let $\\widehat{\\mathbf{Q}} = [\\mathbf{N}_{1},\\mathbf{Q}d^{-\\frac{1}{4}}] \\in \\mathbb{R}^{L \\times (m+r)}$,\n$\\widehat{\\mathbf{K}} = [\\mathbf{N}_{2},\\mathbf{K}d^{-\\frac{1}{4}}] \\in \\mathbb{R}^{L \\times (m+r)}$ where the concatenation is conducted along the second axis. Then\n\\begin{equation}\n\\label{eq:Performer-friendly-RPE}\n\\widehat{\\mathbf{A}} \\overset{\\mathrm{def}}{=} \\exp\\left(\\widehat{\\mathbf{N}}+\\frac{\\mathbf{Q}\\mathbf{K}^{\\top}}{\\sqrt{d}}\\right)\n= \\exp\\left(\\widehat{\\mathbf{Q}}\\widehat{\\mathbf{K}}^{\\top}\\right).\n\\end{equation}\nIn Eq. (\\ref{eq:Performer-friendly-RPE}), RPE-masked attention is now translated to regular softmax attention that admits ``Performerization'' as described in Eq. (\\ref{performers_attention}). This observation naturally leads to an efficient RPE-enhanced attention algorithm, with a pseudo-code implementation provided in Algorithm \\ref{alg:flt}. The time and space complexity of the algorithm are $O(L(m+r)d)$ and $O(L(m+r)+(m+r)d+Ld)$, respectively.\n\nIn Algorithm \\ref{alg:flt}, instead of learning $f$ and trying to compute its Fourier Transform $g$ for the low-rank decomposition of $\\mathbf{N}$, we propose to directly learns $g$ and refer to our approach as \\textbf{FourierLearner-Transformer} (\\textbf{FLT}). Note that \\FLT~effectively learns a spectral representation of $f$. \n\nWe point out that our formulations are general enough to cover a wide range of RPE variants used in practice:\n\n\\alertinline{Regular RPE for sequential data.}\nIn this setting the input sequence does not have richer geometric structure and thus vectors $\\mathbf{r}_{j}$ can be identified as the indices of tokens in the sequence, i.e., $\\mathbf{r}_{j}=j$. Thus, \\FLT~learns a function $g:\\mathbb{R} \\rightarrow \\mathbb{C}$ (Sec. \\ref{sec:lm}, \\ref{sec:images}).\n\n\\alertinline{RPE for 3D-data.} For this input type (e.g. 3D molecular data), it is natural to identify $\\mathbf{r}_{j}$ as the 3D coordinates of atoms. Thus, \\FLT~learns a function $g:\\mathbb{R}^{3} \\rightarrow \\mathbb{C}$. Note that existing methods \\cite{liutkus, rpe-performers, topmasking} are \\textit{inapplicable} while \\FLT~works well in this setting (Sec. \\ref{sec:molecular_dynamics}).\n\nFinally, we note that \\FLT~necessitates specifying some distribution $p$ supported over $\\mathbb{R}^{\\ell}$ to satisfy the assumption in Theorem \\ref{thm:rpe-decompose}.  Practical considerations dictate that $p$ needs to be chosen in such a way that we can efficiently sample from it and compute its density function. In our experiments, we use Gaussian distributions zero mean and unit variance/learnable variance for $p$.\n\n\\subsection{Theoretical analysis of FLT}\nWe have theoretically investigated \\FLT 's RPE approximation. In particular, we prove the following theorem, which states that under mild assumptions, the estimated RPE mask $\\widehat{\\mathbf{N}}$ can approximate the true RPE mask $\\mathbf{N}$ up to arbitrary precision with high probability. Besides, the theorem provides sample complexity bound for such accurate approximation.\n\n\\begin{theorem}[Uniform convergence and sample complexity for approximation]\\label{thm:sample-complexity-flt}\n    Given $L$ vectors $\\mathbf{r}_{1},...,\\mathbf{r}_{L} \\in \\mathbb{R}^{\\ell}$, define the RPE attention mask $\\mathbf{N} = [f(\\mathbf{r}_{i}-\\mathbf{r}_{j})]_{i,j\\in [L]}$.   \n    Assume that $c= \\||g(\\mathbf{x})|/p(\\mathbf{x})\\|_{\\infty}$, where $g$ is the Fourier Transform of $f$ and $p$ is some probability density function over $\\mathbb{R}^{\\ell}$.\n    \n    For any $\\varepsilon, \\delta>0$, if the number of random features $r=\\Theta \\left(\\frac{c^2}{\\varepsilon^2}\\log\\frac{L}{\\delta}\\right)$, then \\FLT 's RPE approximator $\\widehat{\\mathbf{N}}$ satisfies \n    $$\\mathbb{P}\\left(\\|\\mathbf{N}-\\widehat{\\mathbf{N}}\\|_{\\max}\\leq \\varepsilon \\right)>1-\\delta,$$\n    where $\\|\\cdot\\|_{\\max}$ denotes the max norm of a matrix.\n\\end{theorem}\n\nWe also prove variance bound of the estimated RPE and present the result in the supplementary material. The proofs and detailed discussions on the theoretical results can be found in the supplementary material as well.\n\n\\subsection{The topology of the Fourier Transform}\n\\label{sec:topology}\n\nNowhere in the analysis in Sec. \\ref{subsec:flt} have we relied on any structural properties of $f$. In particular, the matrix $\\mathbf{N}$ does not need to be a valid positive definite kernel-matrix or even symmetric. However, if needed, desired inductive bias can be incorporated into \\FLT~via certain parameterization schemes used to train $g$, as we discuss in this subsection.\n\n\\paragraph{Gaussian mixture RPEs.} One of the most general parameterizations of $g$ that we have considered is the so-called \\textit{Gaussian mixture} variant:\n\\begin{equation*}\ng(\\boldsymbol{\\xi}) = \\sum_{t=1}^{T} w_{t} \\exp\\left(-\\frac{\\|\\boldsymbol{\\xi}-\\boldsymbol{\\mu}_{t}\\|^{2}}{2 \\sigma_{t}^{2}}\\right).\\label{eq:gaussian-mixture-rpe}\n\\end{equation*}\nTherefore, the FT $g$ is parameterized by $(2+\\ell)T$ numbers: $w_{1},...,w_{T},\\sigma_{1},...,\\sigma_{T} \\in \\mathbb{R}$, $\\boldsymbol{\\mu}_{1},...,\\boldsymbol{\\mu}_{T} \\in \\mathbb{R}^{\\ell}$. In the special case where $T=1$, the FT becomes a renormalized Gaussian kernel and as such, defines $f$ as another Gaussian kernel. \n\n\\paragraph{Shift-invariant kernels for RPE masks.} It is straightforward to apply the \\FLT~mechanism for RPEs to make $\\mathbf{N}$ a kernel-matrix of any \\textit{shift-invariant} kernel \\cite{rahimi}. By Bochner's Theorem, for a shift-invariant kernel: $\\mathrm{K}: \\mathbb{R}^{\\ell} \\times \\mathbb{R}^{\\ell}  \\rightarrow \\mathbb{R}$, there exists a corresponding probabilistic distribution $p_{\\mathrm{K}}$ and some positive constant $C>0$, such that\n\\begin{equation*}\n\\mathrm{K}(\\mathbf{x},\\mathbf{y}) = C\\int_{\\mathbb{R}^{d}} e^{\\mathbf{i}(\\mathbf{x}-\\mathbf{y})^{\\top}\\boldsymbol{\\xi}}p_{\\mathrm{K}}(\\boldsymbol{\\xi})\\mathrm{d}\\boldsymbol{\\xi}\n\\end{equation*}\nThus, to obtain an unbiased approximation of the RPE mask $\\mathbf{N}$ given by the kernel matrix $[\\mathrm{K}(\\mathbf{s}_{i},\\mathbf{s}_{k})]_{i,k=1,...,L}$ for the shift-invariant kernel $\\mathrm{K}$, it suffices to take $\\mathbf{r}_{j}=\\frac{1}{2\\pi}\\mathbf{s}_{j}$,  \n$g(\\boldsymbol{\\xi})=Cp_{\\mathrm{K}}(\\boldsymbol{\\xi})$ for $j=1,....r$. Even if a particular class of\nshift-invariant kernels has been chosen, \\FLT~still provides a way to learn its specific instantiation through learning an appropriately parameterized $g$. \n\n\\paragraph{Local RPEs.} Through the corresponding structured parameterized Fourier Transforms $g$, \\FLT~is also capable of modeling various schemes where the RPE mechanism needs to be applied only locally and regular attention is to be used for tokens far enough from each other. We call such strategies \\textit{local RPEs}. Local RPEs can be derived for both sequential data and high-dimensional geometric data.\n\nThe most basic local RPE takes $\\mathbf{r}_{j}=j$ and, for an attention radius $v>0$ and $C \\in \\mathbb{R}$, defines $f$ as\\footnote{Note that instead of using one indicator function in Eq. (\\ref{eq:sin}), one can also apply a linear combination of many with learnable radii and a list of coefficients.}\n\\begin{equation}\n\\label{eq:sin}\nf_{v, C}(\\Delta r) = C \\cdot \\mathbb{I} [|\\Delta r| \\leq v].\n\\end{equation}\nSuch an RPE mechanism would (de)amplify the regular attention score between tokens close to each other by a certain multiplicative amount and might play a similar role as local attention \\cite{local_attention}. It turns out that the FT for such a $f$ has a particularly simple form:\n$$\ng_{f_{v, C}}(\\xi) = C \\cdot \\frac{\\sin(2\\pi v\\xi)}{\\pi \\xi}.\n$$\n\nInterestingly, RPEs from Eq. (\\ref{eq:sin}) can be easily generalized to a higher-dimensional local RPE. In this case, we consider the positional encoding function $f:\\mathbb{R}^{\\ell}\\to \\mathbb{R}$ of the following form:\n\\begin{equation*}\nf_{\\mathbf{v}, C}(\\Delta \\mathbf{r}) =  \\prod_{j=1}^{\\ell} C \\cdot \\mathbb{I}[|\\Delta r^{(j)}| \\leq v_{j}]\\quad (\\Delta \\mathbf{r}\\in\\mathbb{R}^{\\ell}),\n\\end{equation*}\n\nwhere $\\Delta r^{(j)}$ denotes the $j$-th entry of $\\Delta \\mathbf{r}$. The corresponding Fourier Transform $g$ can be factorized as \n$$\ng_{f_{\\mathbf{v}, C}}(\\boldsymbol{\\xi}) = C \\cdot \\prod_{j=1}^{\\ell} \\frac{\\sin(2\\pi v_{j}\\xi_{j})}{\\pi \\xi_{j}}.\n$$\nThis result can be further generalized. Consider the function $g$ of the following form: \n\\begin{equation*}\ng_{k_{1},...,k_{\\ell}}^{v_{1},...,v_{\\ell}}(\\boldsymbol{\\xi}) = C \\cdot \\prod_{j=1}^{\\ell} \\frac{\\sin^{k_{j}}(2\\pi v_{j}\\xi_{j})}{\\pi \\xi_{j}}\n\\end{equation*}\nThe inverse Fourier Transform of $g$ can be written as\n$$\n    f(\\Delta \\mathbf{r}) = M \\cdot \\prod_{j=1}^{d}f_{j}^{v_{j}}(\\Delta r_j),\n$$\nwhere $M$ is a constant and each $f^{v_{j}}_{j}$ is (a) continuous, (b) symmetric, (c) with compact support of length depending on $v_{j}$, and (d) piece-wise a polynomial of order $k_{j}-1$.\nSuch functions $f$ are natural candidates for continuous local RPE mechanisms for tokens with positions embedded in $\\mathbb{R}^{\\ell}$ and any $\\ell \\geq 1$. \nExamples of local RPE variants for $\\ell=2$, supported via \\FLT, are presented in Fig. \\ref{fig:local} in Appendix \\ref{sec:visual-local}.\n\nThe above theoretical results can be directly obtained via straightforward integration and a realization that the $N$-dim FT of a function: $h(x_{1},\\cdots,x_{N}) \\overset{\\mathrm{def}}{=} h_{1}(x_{1})\\cdot\\cdots\\cdot h_{N}(x_{N})$ can be represented as the product of 1D FTs of the individual components $h_{j}$.\n\n\\paragraph{Remark.} We point out that all the three parametrization schemes above are parameter-efficient. In all our experiments, \\FLT~introduced $<0.03$M additional parameters for relative positional encoding. Note that the number of additional parameters does not increase with the input sequence length.\n\n\\section{EXPERIMENTS}\n\\label{sec:expeeriments}\n\nIn this section, we provide experimental results on diverse tasks to demonstrate the effectiveness of the \\FLT~architecture. We first study language modeling with sequential text data, which is a standard setting for efficient RPE-enhanced attention and enables thorough comparisons with existing baselines. Next, we consider the computer vision domain and test \\FLTs~on several image classification datasets. Finally, to show that \\FLTs~ broaden the scope of RPE-enhanced efficient Transformers, we experiment on molecular property prediction with complicated RPE masks that existing efficient RPE-enhanced attention baselines cannot handle. The complete experimental setup, the hyper-parameters for each of the tasks, and hardware details are provided in the supplementary material.\n\n\\begin{table}\n    \\begin{center}\n    \\caption{\\textbf{Language model perplexity scores} on the WikiText-103 validation set. The lowest perplexity is highlighted in \\textbf{bold}.}\\label{tab:lm}\n    \\scalebox{0.95}{\\begin{tabular}{@{}lc@{}}\n    \\toprule\n    Model &  Perplexity \\\\ \\midrule\n    Linear Trans. \\cite{katharopoulos2020transformers}  &38.4\\\\\n    RFA-Gaussian \\cite{peng2021random}&  33.6 \\\\\n    RFA-arccos \\cite{peng2021random}& 36.0 \\\\\n    RFA-GATE-Gaussian \\cite{peng2021random}& 31.3 \\\\\n    RFA-GATE-arccos \\cite{peng2021random} & 32.8 \\\\\n    Performer \\cite{choromanski}& 31.1\\\\\n    CosFormer \\cite{cosformer}& 30.7\\\\\\midrule\n    Performer-sineSPE \\cite{liutkus} & 38.0 \\\\\n    Performer-convSPE \\cite{liutkus} & 37.8 \\\\\n    Log-linear Performer \\cite{rpe-performers}  &30.6\\\\\\midrule\n    \\FLT~(Gaussian mixture RPE) (ours) & 30.3 \\\\\n    \\FLT~(local RPE) (ours) & \\textbf{30.1}\\\\ \\bottomrule\n    \\end{tabular}\n    }\n    \\end{center}\n\\end{table}\n\n\\subsection{Language modeling}\n\\label{sec:lm}\n\nWe conduct experiments on the WikiText-103 language modeling task to show the effectiveness of our proposed method in NLP applications. Most existing baselines are applicale to sequential text data. Thus, we provide comprehensive empirical comparisons on model quality and efficiency with baselines in this subsection.\n\n\\paragraph{Compared methods.}\nIn this experiment, we study \\FLT~with two RPE vaiants, Gaussian mixture RPE and local RPE. We compare our model with the following strong baselines:\n\n\\begin{itemize} \n\\item \\textit{Linear Transformer} \\cite{katharopoulos2020transformers}, which uses kernelized low-rank attention with $\\mathrm{elu}(\\cdot)+1$ as the feature map. \n\\item  \\textit{Random feature attention} (RFA) {\\small \\cite{peng2021random}}, which has two variants (Gaussian and arc-cosine) and an optional gating mechanism. \n\\item  The regular \\textit{Performer} \\cite{choromanski}, which applies the FAVOR+ mechanism for attention matrix approximation. \n\\item  \\textit{CosFormer} \\cite{cosformer}, which designs a linear operator and a cosine-based distance re-weighting mechanism for attention matrix approximation. \n\\item \\textit{Performer-SPE} \\cite{liutkus}, which incorporates a special class of RPE into low-rank attention and has two variants (sineSPE and convSPE). \n\\item  The \\textit{log-linear Performer} \\cite{rpe-performers} which extends Performers to work with an arbitrary Toeplitz RPE attention mask.\n\\end{itemize}\n\n\\begin{figure*}[!t]\n    \\centering\n    \\includegraphics[width=0.48\\linewidth]{img/fig-time.pdf}\n    \\includegraphics[width=0.48\\linewidth]{img/fig-mem.pdf}\n    \\vspace{-4mm}  \n    \\caption{\\textbf{Model forward speed} (left) \\textbf{and peak memory} (right) \\textbf{comparisons} between \\FLT~and baselines under different input sequence lengths.}\n    \\vspace{-3mm}\n    \\label{fig:efficiency}\n\\end{figure*}\n\n\\paragraph{Implementation details.} All the tested models are efficient Transformers based on kernelized low-rank attention, with 6 decoder layers. More details regarding model configurations and training are in the supplementary material. We use the validation perplexity as the evaluation metric; lower perplexity indicates better performance.\n\n\\paragraph{Results.}\nThe results are shown in Table \\ref{tab:lm}. Both variants of our \\FLT~outperform all the baselines. Compared with efficient Transformers without RPE, \\FLT~achieves much stronger performance. For example, the validation perplexity of our \\FLT~with local RPE is 1.0 point lower than that of the regular Performer, indicating that our method brings substantial performance gains by incorporating RPE into the attention module.\n\nCompared with other efficient Transformer variants \\textit{with} RPE, our \\FLT~is still very competitive. For example, our \\FLT~achieves lower perplexity than the strong log-linear Performer baseline. Note that log-linear Performer relies on more expensive FFT and is less efficient in practice. Specifically, the time and space complexity of the \\FLT~are $O(L(m+r)d)$ and $O(L(m+r)+(m+r)d+Ld)$, respectively, while the time and space complexity of log-linear Performer are $O(Lmd\\log L)$ and $O(Lmd)$.\nThus, our \\FLT~obtains both better quality and efficiency than existing efficient RPE-enhanced Transformer variants on this task. \n\nIn addition, we further investigate the attention matrices of \\FLT\\footnote{\\FLT~does not explicitly construct attention matrices during training so that it avoids the quadratic computational complexity. However, we can still materialize the attention matrices approximated by \\FLT~}. We visualize the attention matrices of different attention heads in an \\FLT~model trained on WikiText-103 language modeling in Fig. \\ref{fig:attn-mat} in Appendix \\ref{sec:attn-mat}. The visualizations show that some attention heads pay more attention to nearby tokens, while others shows global attention patterns. Quantitatively, the average attention probability over the most distant/nearby 10\\% tokens is 0.068/0.279 respectively. Thus, \\FLT~ learns locality bias in language while maintaining the advantage to capture global contexts and leverage information in distant tokens.\n\n\\paragraph{Computational cost comparisons.} \nAs discussed above, \\FLT~enjoys much better time/space complexity compared with the strongest baseline method, the log-linear Performer. To showcase \\FLT 's efficiency in practice, we construct one Transformer layer with 12 attention heads whose hidden dimension is set to 768, and FFN dimension is set to 3072. \nWe feed inputs with varying lengths and a batch size of 8 into the model and measure the efficiency. We report the average forward time and the maximum peak memory consumption across 5 runs under different input sequence lengths in Fig. \\ref{fig:efficiency}. \nWe compare \\FLT~with the strongest baseline, log-linear Performer, and we also include the regular Performer as a reference.\nIt's clear that \\FLT~only introduces negligible memory overhead compared with the regular Performer, and scales much better than the log-linear Performer in practice, in terms of both model forward time and peak memory. Therefore, our experiment show that \\FLT~is both more accurate and more scalable than the baselines on sequential text data.\n\n\\begin{table}[t]\n  \\centering\n  \\caption{\\textbf{Image classification accuracy} comparisons. Log-linear Performer is omitted due to its infeasible memory complexity and out-of-memory issues. The best performances are highlighted in \\textbf{bold}.}\n  \\label{tab:image}\n  \\begin{tabular}{@{}lccc@{}}\n    \\toprule\n    & ImageNet & Places365 & FashionMnist \\\\\n    \\midrule\n    Performer & 75.1\\% & 55.0\\% & 91.1\\% \\\\\n    CosFormer & 76.2\\% & 55.6\\% & 91.6\\% \\\\\n    FLT (ours) & \\textbf{77.4}\\% & \\textbf{56.0}\\% & \\textbf{92.1}\\% \\\\\n    \\bottomrule\n  \\end{tabular}\n\\end{table}\n\n\\begin{figure*}[!ht]\n\\begin{minipage}[t]{0.51\\linewidth}\n    \\centering\n    \\vspace{-43mm}\n    \\captionof{table}{\\textbf{Comparisons of FLT with the regular Performer} on OC20 IS2RE task. The suffix ``-$k$L'' means the model consists of $k$ layers, e.g., \\FLT -10L refers to a 10-layer \\FLT. The evaluation metrics are Mean Absolute Error (MAE, lower is better) of the energies and the percentage of Energies within a Threshold (EwT, higher is better). We highlighted in \\textbf{bold} the best performance.}\\label{tab:oc20}\n    \\vspace{-1.5mm}\n    \\scalebox{0.90}{\n        \\begin{tabular}{lcc} \\toprule\n                        & Energy MAE (eV) & EwT (\\%) \\\\ \\midrule\n        Performer-12L       & 0.5454          & 4.90      \\\\\n        \\FLT -10L (ours)  & 0.5157          & \\textbf{5.44}     \\\\\n        \\FLT -12L (ours)  & \\textbf{0.5046}          & 5.33    \\\\ \\bottomrule\n        \\end{tabular}\n    }\n\\end{minipage}\\hfill\n\\begin{minipage}[t]{0.46\\linewidth}\n    \\centering\n    \\includegraphics[width=0.99\\linewidth]{img/fig-oc20.pdf}\n    \\vspace{-7.5mm}  \n    \\caption{\\textbf{Validation loss} of FLTs and the regular Performer on the IS2RE task of OC20 dataset.}\n    \\label{fig:oc20}\\vspace{-6mm}\n\\end{minipage}\n\\end{figure*}\n\n\\subsection{Image classification}\n\\label{sec:images}\nWe thoroughly benchmarked \\FLT~variants of Vision Transformers (ViTs) \\cite{vits} on several image classification datasets, including ImageNet, Places365, and FashionMnist. Details of these datasets can be found in the supplementary material.\n\n\\paragraph{Compared methods and implementation details.} We compare \\FLT~with the regular Performer as well as the most competitive competitor from Sec. \\ref{sec:lm}, \\textit{CosFormer} and \\textit{log-linear Performer}. All tested ViTs consist of 12 layers with 12 attention heads in each layer. More details regarding model configurations and training are in the supplementary material.\nFor our \\FLT~variants, we apply Gaussian mixture RPEs (Sec. \\ref{sec:topology}) with the number of Gaussian mixture modes $T$ set to 25 and the number of random features for RPE-encoding $r$ set to 64. % In addition, we run more detailed ablation studies on $T$ and $r$ for the $\\mathrm{ImageNet}$ dataset.\n\n\\paragraph{Results.}\nThe results are presented in Table \\ref{tab:image}. The \\textit{log-linear Performer} architecture run out of memory for $m=128$ and does not train when $m$ was reduced (with a fixed batch size of 4096) to fit the assigned memory. Thus, it is omitted in the comparison. Compared with the other two baselines, our \\FLT~obtains strongest performances on all the three datasets. For instance, on ImageNet, \\FLT~provides a $\\mathbf{2.3}$\\% accuracy improvement over the regular Performer; and is even $\\mathbf{1.2}$\\% better than the strong CosFormer baseline. The results demonstrate that \\FLT~also works well on image data.\n\n\\subsection{Molecular property prediction}\n\\label{sec:molecular_dynamics}\n\nAs highlighted in previous discussions, \\FLT~broadens the scope of RPE-enhanced efficient Transformers and can be applied to geometric data embedded in high-dimensional Euclidean spaces. To validate this claim, in this subsection, we further evaluate our \\FLT~model on the molecular property prediction task to show its capability to handle 3D input data and complicated (non-Toeplitz) RPE masks. To the best of our knowledge, in this scenario, \\FLT~is the \\textit{first} Transformer providing RPE-enhanced scalable attention that enjoys \\textit{linear} complexity with respect to the number of input tokens.\n\nWe use a publicly-available large-scale electrocatalysts dataset - the Open Catalyst 2020 (OC20) dataset and focus on the IS2RE task which requires to predict the energy of the relaxed structure given the initial structure of solid catalysts with adsorbate molecules \\cite{ocp_dataset}.\n\n\\paragraph{Compared methods.}\nExisting technicques considered in the previous experiments do not apply to this setting. Thus, we only compare our \\FLT~with the regular Performer without RPE. For the \\FLT~model, we consider to approximate RPE masks based on Gaussian basis functions, which are popularly used in neural networks for molecular modeling \\cite{gasteiger2021gemnet, shi2022benchmarking, luo2023one}. Specifically, the RPE mask is defined as $\\mathbf{N} = [f(\\mathbf{r}_{i}-\\mathbf{r}_{j})]_{i,j\\in[L]} \\in \\mathbb{R}^{L \\times L}$, where $\\mathbf{r}_{i}\\in \\mathbb{R}^{3}$ is the position of the $i$-th input atom, $L$ is the total number of input atom, and\n$$\n    f(\\mathbf{r}) = \\sum_{t=1}^T \\frac{w_t}{(\\sqrt{2\\pi}\\sigma_t)^3} \\exp\\left(-\\frac{\\|\\mathbf{r}\\|^2}{2\\sigma_t^2}\\right).\n$$\nNote that  RPE only calculates the relative distances between atoms, which naturally preserves many invariant and equivariant properties.\nIt easy to see that the Fourier Transform of $f$ is \n$$\n    g (\\boldsymbol{\\xi})=\\sum_{t=1}^T w_t\\exp\\left(-2\\pi^2\\sigma_t^2\\|\\boldsymbol{\\xi}\\|^2\\right),\n$$\nwhich enables us to approximate the RPE mask $\\mathbf{N}$ in \\FLTs~using the technique described in Sec. \\ref{sec:algorithms}.\n\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=0.95\\linewidth]{img/S4-exps-2.pdf}\n    \\caption{\\textbf{Results of learnable optimizer experiments. } \\textbf{Left:} \\textrm{Adam} \\& learnable optimizers using \\FLT~and S4 on the task of training ViT-Base classifier on ImageNet. \\textbf{Right:} Adam \\& various learnable optimizers on the task of optimizing Rastrigin-type functions (from private conversation with the authors of \\cite{jain2023mnemosyne}).}\n    \\label{fig:learn-opt}\n\\end{figure*}\n\n\\paragraph{Implementation details.} We adopt most of the training strategies of 3D-Graphormer \\cite{shi2022benchmarking}. \nSpecifically, we trained a regular Performer with 12 layers and two \\FLT~with 10 and 12 layers respectively. \nMore details regarding model configurations and training are in the supplementary material.\nWe evaluate the model performance on the in-domain validation set. We use Mean Absolute Error (MAE) of the energies and the percentage of Energies within a Threshold (EwT) of the ground truth energy to evaluate the accuracy of the predicted energies. \n\n\\paragraph{Results.}\nThe results are presented in Table \\ref{tab:oc20}. We also present the validation loss curves of the models in Fig. \\ref{fig:oc20} for a more comprehensive comparison. \nClearly, our \\FLT~models obtain better performance in both evaluation metrics and produce more accurate energy predictions. For example, the energy MAE of the 12-layer \\FLT~is more than 0.04eV lower than that of the 12-layer regular Performer, which indicates that the use of RPE effectively increases the predictive power of the model. One may argue that the use of RPE in \\FLT~may add some computational overhead and increase the number of model parameters. However, it should be noted that a shallower 10-layer \\FLT~can also significantly outperform the 12-layer regular Performer, while being faster and using less parameters. \n\n\\subsection{Learnable optimizers}\n\\label{sec:mnemosyne}\n\n\\FLT~has also been compared independently by authors and other researchers on longer contexts with other classes of efficient architecture, including LSTM \\cite{hochreiter1997long} and state-space models \\cite{s4}. \nThe corresponding task is practical and challenging: applying Transformers as memory models in learnable optimizers (with context length up to 2000). \nIn this setting, long-range temporal (to understand the \\textit{history} of the optimization) and spatial (to understand the \\textit{landscape} of the loss function more globally) reasoning is critical \\cite{jain2023mnemosyne,gartner2023transformer}. \n\n\\paragraph{Compared methods and implementation details.}\nIn the first experiment, \\FLT-based learnable optimizer is compared against S4-based learnable optimizer \\cite{s4} and standard non-learnable Adam optimizer on ImageNet classification.\n\nIn the second experiment, \\FLT~is further applied on population(swarm)-based learnable optimizers, in which the masking mechanism implemented by FLT was applied to modulate how the members of the population attend to each other. The baselines include standard non-learnable Adam optimizer as well as learnable optimizers based on LSTM and Performer. The evaluation is conducted on Rastrigin-like functions. \n\n\\paragraph{Results.} \nThe results of the first/second experiment are shown in the left/right panel of Fig. \\ref{fig:learn-opt}. We note that in both experiments, \\FLT-based approach provides drastic improvements over all other variants. The results shows the consistent effectiveness of our model in capturing long range dependencies in learnable optimizers. \n\n\\section{CONCLUSIONS}\n\\label{sec:conclusion}\nWe introduce \\textrm{FourierLearner-Transformers} (\\FLTs) that efficiently adapt the relative positional encoding (RPE) mechanism into Performers - kernelized implicit-attention Transformers with linear space and time complexity. \nIn contrast to other architectures combining Performers with RPEs, \\FLTs~maintain linear complexity of the attention modules with no additional structural assumptions regarding the RPE mask. \nWe provide theoretical analysis and show that \\FLTs~ can accurately approximate RPE. \nWe further conduct extensive experiments to show the efficiency and quality of \\FLTs~across a wide range of tasks and data modalities, including texts, images, molecules, and optimizer memory.\n\\section*{ACKNOWLEDGEMENTS}\n\\label{sec:ac}\n\nWe would like to thank Deepali Jain for a discussion on using \\FLT~for learnable optimizers, as well as proposing and performing experiments with population(swarm)-based methods for the \\FLT~variants provided by the authors. We also thank the reviewers for their helpful comments.\n\nThis work is supported in part by the United States Department of Energy via the Brookhaven National Laboratory under Contract No. 384608.\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Length Generalization of Causal Transformers without Position Encoding}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\n\nGeneralizing to longer sentences\nis important for recent Transformer-based language models.\nBesides algorithms manipulating explicit position features,\nthe success of \nTransformers without position encodings (NoPE)\nprovides a new way to overcome the challenge.\nIn this paper, we study the length generalization property \nof NoPE.\nWe find that although NoPE can extend to longer sequences \nthan the commonly used explicit position encodings, \nit still has a limited context length.\nWe identify a connection between the failure of NoPE’s \ngeneralization and the distraction of attention distributions.\nWe propose a parameter-efficient tuning for \nsearching attention heads' best temperature hyper-parameters, \nwhich substantially expands NoPE's context size.\nExperiments on long sequence language modeling, the synthetic\npasskey retrieval task and real-world long context tasks show \nthat NoPE can achieve competitive performances with state-of-the-art \nlength generalization algorithms.\nThe source code is publicly \naccessible\\footnote{$\\ \\ $\\url{https://github.com/AntNLP/nope_head_scale}}.\n\n\\section{Introduction}\n\\label{sec:intro}\n\nCausal Transformer has been widely applied \nin modern language models. \nTo help models recognize the correct ordering of words, \nit is common to configure Transformers\nwith \\emph{explicit} position encodings\n(e.g., the sinusoidal embeddings in the original\ndevelopment of Transformer \\cite{NIPS2017_3f5ee243}, \nthe relative position encoding in T5 \\cite{JMLR:v21:20-074},\nand the rotary position encoding in GPT series \\cite{DBLP:journals/corr/abs-2104-09864}).\nThe setup of position features provides\nflexibility to include prior knowledge structure on \ndescribing distance,\nbut it also brings the problem of \\emph{length generalization}:\nlanguage models trained with in-domain position features\ncan not handle longer sentences \n(i.e., those with out-of-domain position features)\nin testing time.\nGeneralizing to unseen sentence length\nis crucial in many language model applications\nlike retrieval augmented language models \\cite{DBLP:journals/jmlr/IzacardLLHPSDJRG23}, \npersonalized language models \\cite{wang2023rolellm},\nlanguage-model-based agents \\cite{10.1145/3586183.3606763}.\n\nDeparting from the standard ways of encoding positions,\none may ask (following the principle of parsimony) that \nare the explicit position features necessary?\nThe answer is no.\nBoth empirically \\cite{haviv-etal-2022-transformer} and theoretically \\cite{chi-etal-2023-latent,kazemnejad2023the},\nthe casually masked Transformers are shown to be able to\nsuccessfully model languages\nwithout any prior position encoding (\\textbf{NoPE}).\nThe finding calls for a deeper understanding of\n\\emph{implicit} position information in \nTransformer-based language models,\nand also inspires a new direction for length generalization:\n\\emph{without explicit position features, can NoPE generalize?}\n\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=\\linewidth]{fig/fig_vis_entro_.pdf}\n  \\caption{Length generalization from $2$K to $4$K. \n  For different testing lengths (or, positions of sequences), \n  dashed lines draw the log-perplexity of models\n  (measured on validation set of the pre-training dataset),\n  and solid lines represent the entropy of attention heads\n  (averaged on all heads).}\n  \\label{fig:vis_entro}\n\\end{figure}\n\n\\begin{figure*}[t]\n  \\centering\n  \\includegraphics[width=0.49\\linewidth]{fig/fig_vis_uni_scale_entro_nope.pdf}\n  \\includegraphics[width=0.49\\linewidth]{fig/fig_vis_uni_scale_entro_rope.pdf}\n  \\caption{\\textbf{UniformScale} modifies the temperature hyper-parameter of the $\\mathrm{SoftMax}$ operator in self-attention layers (Left, NoPE; Right, RoPE). NoPE can generalize to longer context by merely scaling the softmax scores. However, this exact technique does not directly apply to RoPE models.}\n  \\label{fig:vis_uni_entro}\n\\end{figure*}\n\nIn this paper, we study the length generalization property of NoPE.\nOur main findings are,\n\n\\begin{itemize}[leftmargin=*]\n    \\item When extending to unseen sentence length, \n    NoPE has less performance loss.\n    However, beyond a certain range, NoPE also fails to extend, \n    with no substantial difference observed when compared to explicit position encodings.\n    For example, NoPE can effectively extend the training length by \n    $20\\%$ (from $2$K to $2.4$K, Figure \\ref{fig:vis_entro}) \n    without a significant increase in perplexity.\n    In contrast, the rotary position encoding (RoPE) is only capable of extending by $10\\%$. \n    \n    \\item \n    We analyze the failure cases of NoPE's generalization\n    and find that they always co-occur with \n    the distraction of attention distributions:\n    the attention heads begin to allocate their weights to tokens evenly when NoPE's extension performance begins to collapse. \n    The connection between NoPE's generalization \n    and concentration of attention heads suggests controlling the behaviors of attention heads during length extension.\n    \\item We show that by simply searching one temperature hyper-parameter, NoPE's length generalization \n    can be significantly improved. For example, by scaling the attention score by a factor of $1.2$, NoPE can immediately generalize to over $4$K tokens (Figure \\ref{fig:vis_entro}).\n    \\item Moreover, we developed an advanced version of this \n    strategy by searching temperature parameters for each head, in the light that different layers and heads exhibit varied behaviors. \n    The procedure resembles a parameter-efficient fine-tuning,\n    with an extremely small number of tunable parameters \n    ($704$ delta parameters over $1$B model parameters).\n    We show that the proposed method can\n    help NoPE to generalize further (Figure \\ref{fig:vis_head_vs_uni}). %(results XXX).\n\\end{itemize}\n\nWe conduct length generalization experiments on \nlong sequence language modeling, \nsynthetic tasks (passkey retrieval), and \nLongBench.\nThe results show that NoPE \nenjoys a competitive\nextension performances to state-of-the-art \nlength generalization methods for explicit\nposition encodings (e.g., \nPI \\cite{chen2023extending}, \nYaRN \\cite{peng2024yarn}).\n\n\\section{Length Generalization of NoPE}\n\\label{sec:method}\n\n\\subsection{Language Modeling with NoPE}\n\\label{sec:lm}\n\nBefore diving into the length generalization problem, \nwe first briefly describe the NoPE models used in this paper.\n\\footnote{For simplicity, \nwe refer NoPE to both the implicit way of encoding positions \nand the language model trained without position encoding.}\nOur default NoPE has $1.1$B parameters. It is trained from \nthe TinyLlama \\cite{zhang2024tinyllama} code base\n\\footnote{\\url{https://github.com/jzhang38/TinyLlama}}, \nwith training sequence length $L=2048$ and $50$K steps \n($\\approx 100$B tokens). More details can be found in \nSection~\\ref{ssec:nope}.\n\nWe also include the original TinyLlama model\nwhich uses rotary position encoding (RoPE) for comparison.\nBy default, both models are trained with identical settings.\n\n\\subsection{Length Generalization}\n\\label{sec:lg}\n\nGiven a language model (LM) with pre-trained maximal sequence length $L$, the goal of length generalization is to expand it to length $L^\\prime > L$.\nLength generalization can be tested in a zero-shot manner \n(``train short, test long'') or with some fine-tuning.\n\nFigure \\ref{fig:vis_entro} depicts language modeling \nperformances of NoPE (and RoPE).\nWe can observe that, \nwithin the pre-training length ($L = 2048$),\nNoPE has a similar performance as RoPE,\nwhich agrees with existing works:\ncasual masking can implicitly encode the positions of a sequence\n\\citep{haviv-etal-2022-transformer,chi-etal-2023-latent}.\n\nWhen the testing sequence length exceeds the training length,\nwe see that \n1) NoPE's length generalization error \n(light blue dashed line, measured with log-perplexity) \nis lower than RoPE (light red dashed line).\n2) vanilla NoPE still has an increased perplexity \nthan in-domain tests.\nTherefore, though it is not a perfect solution, \nremoving explicit position encoding can \neffectively reduce the length generalization error.\nNext, we will try to find the reason \nfor the failure of NoPE's length generalization,\nand also develop algorithms for improving it.\n\n\\subsection{Extension? Attention!}\n\\label{ssec:vis_entro}\n\nTo analyze NoPE's generalization failure,\nwe first see that since\nexplicit position encodings have been dropped, the casual Transformer block is only left with three core modules,\nthe embedding layer, feed-forward layers, and self-attention layers.\nThe outputs of the former two modules are independent of their inputs' position in sequence (i.e., no matter which position, they always have the same output).\nTherefore, multi-head attention layers become our main target.\n\nWe visualize the attention pattern of NoPE at different lengths.\nSpecifically, \ngiven a validation set with a size $n$ and\na target position $i$, we define the average attention entropy \n$\\widebar{\\mathcal{H}}_i$ at position $i$, as\n\\begin{align}\n\\widebar{\\mathcal{H}}_i &= \\frac{1}{n\\!\\times\\!m} \\sum_{x,h} \\mathcal{H}_i^{(h)}(x) \\\\\n\\mathcal{H}_i^{(h)}(x) &= -\\sum_{j=1}^{i} \\alpha_{ij}^{(h)}(x) \\cdot \\log \\alpha_{ij}^{(h)}(x)\n\\label{eq:entropy}\n\\end{align}\nwhere $x$ is a sample, \n$\\alpha_{ij}^{(h)}(x)$ is the attention probability of \ntoken $i$ focusing on token $j$ in the $h$-th attention head\n($h\\in\\{1,2,...,m\\}$),\n$\\mathcal{H}_i^{(h)}(x)$ is the entropy of the attention distribution $\\alpha_{ij}^{(h)}(x)$ evaluated at position $i$.\n\nThe light solid lines in Figure~\\ref{fig:vis_entro} show\nthe average entropy for NoPE (light blue) and RoPE (light red).\nWe can observe that,\n\\textbf{the inflection point of $\\widebar{\\mathcal{H}}_i$ is highly consistent with the inflection point of perplexity}.\nIt implies that failed length generalization of NoPE (and RoPE)\nmight be connected to the distraction of attention:\nattention heads begin to allocate attention to more tokens.\nTo further verify the connection,\nwe also draw a successful extension algorithm for RoPE \n(RoPE-NTK \\cite{bloc97} which interpolates out-of-domain\nencodings to in-domain encodings).\nIts length generalization loss curve is flat,\nwhile its entropy curve also has no steeply increasing point.\n\nUnlike explicit position encodings, NoPE has no clear target objects\nto manipulate, thus it is quite challenging to perform\nlength generalization without fine-tuning on longer sequences.\nHowever, the strong correlation between length extension \nand attention pattern transition suggests such an object,\nthe entropy of attention heads.\n\n\\subsection{Uniform Attention Scale}\n\\label{ssec:uni_scale}\n\nWe write the general scaled dot-product attention as\n\\begin{equation}\n\\alpha_{ij}^{(h)} = \\frac{e^{\\lambda \\bm{q}_i^{(h)} \\cdot \\bm{k}_j^{(h)}}}{\\sum_k e^{\\lambda \\bm{q}_i^{(h)} \\cdot \\bm{k}_k^{(h)}}}\n\\end{equation}\nwhere the scaling factor \n$\\lambda$ is the temperature hyper-parameter of the $\\mathrm{SoftMax}$ operator.\nThe prevalent setting is $\\lambda = \\frac{1}{\\sqrt{d}}$.\n\nBased on observations in Section~\\ref{ssec:vis_entro}, \nwe know that NoPE's failure of length generalization \nmight be correlated with distracted attention, \nhence we can try to gradually increase the scale factor \n$\\lambda$ to reconcentrate attention,\nand see whether the generalization error can be reduced.\nFigure~\\ref{fig:vis_uni_entro} \nvisualizes the average entropy under different scale values and the corresponding perplexity curves.\n\nWe first find that when increasing the scale factor \nduring length generalization evaluation\n(e.g., the pre-training scale $\\lambda\\!=\\!\\frac{1}{\\sqrt{d}}$ \nis increased to $\\lambda\\!=\\!\\frac{1.2}{\\sqrt{d}}$),\nthe inflection points of entropy curves are shifted to\nlonger lengths, at the same time, \nNoPE all generalize to further positions ($L \\text{=2k}\\to L^\\prime\\text{=4k}$). \nThat is, with all NoPE's parameters frozen and \nonly \\emph{uniformly} increasing the softmax's temperature, \nNoPE can successfully generalize to unseen lengths.\n\nThe same conclusion doesn't hold for RoPE\n(Figure~\\ref{fig:vis_uni_entro} Right):\nno matter what value the scale takes (from $\\lambda$=0.8 to $\\lambda$=1.4), \nthe inflection points of entropy curves remain almost unchanged,\nmeaning that it fails to generalize to longer lengths.\nOn the other side, successful RoPE extension algorithms\n(e.g., RoPE-NTK in Figure \\ref{fig:vis_entro})\ncan control the distraction of entropy by\nexplicitly manipulate position encodings.\nTherefore, though attention scaling has been used for RoPE\n\\cite{kexuefm8823,chiang2022overcoming},\nit may contribute marginally to RoPE's generation.\n\nWe also find that \nextending NoPE to more distant positions generally requires a larger scale (i.e., a more concentrated attention distribution).\nAs the position becomes further, the number of tokens involved in the attention calculation increases, the attention is more easily scattered, \nand therefore, a larger scaling factor is needed to concentrate the attention.\nIn particular, for our NoPE model, generalizing to twice the pre-training length requires about 1.2 times the scale, four times the length requires about 1.5 times the scale, and eight times the length requires about 1.8 times the scale.\nAppendix~\\ref{app:fit_func} reports the fitted function of the scaling factor with respect to the generalization length $L'$.\n\nFinally, we note remark that the attention scaling factor in this section takes the \\emph{same} value for all positions, including the pre-training length (\\emph{uniform} scaling). \nWe experimented with a piecewise function\nwhich use the original scale within the pre-training positions, and a more concentrated attention scale for the extrapolated positions. \nWe also try position-dependent functions, \nwhere the scale increases with position. \nHowever, none of these methods could further improve generalization. \nWe speculate that if the attention at earlier positions is not highly concentrated, the learned token representations may hinder the concentration of attention at latter positions. \nWe leave a deeper discussion and analysis of this observation in future work.\n\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=\\linewidth]{fig/fig_vis_head_entro_.pdf}\n  \\caption{The attention entropy across all heads for the original NoPE, head-based scaled NoPE and uniform-scaled NoPE, with each model represented in a separate row. The attention heads exhibit divergent patterns.}\n  \\label{fig:vis_head_entro}\n\\end{figure}\n\n\\section{Head-based Attention Scale}\n\\label{sec:head_scale}\n\nAfter verifying that the attention scaling can \nhelp NoPE generalizing,\nwe delved deeper into the multi-head attention mechanism and posed a new question, ``\\emph{Does each attention head require a unique scaling factor?}''\n\nIn this section, we first visualize the average entropy curves for each head and find that they have different attention patterns. \nHence we propose to replace the uniform scaling with head-based scaling (from one factor to $22\\times 32=704$ factors).\nTo address the issue of an exploding search space, we efficiently determine the values of scaling factors through automated hyperparameter search, considering both parameter efficiency and data efficiency.\nAs a result, head-based scaling generalizes better than uniform scaling.\nMoreover, correlation analysis shows that within each layer, the smaller the converged entropy (i.e., the more concentrated attention), the larger the required scaling factor to maintain that concentration.\n\n\\subsection{Visual Analysis}\n\\label{ssec:vis_head_scale}\n\nThe entropy values span a broad spectrum, with each attention head demonstrating a distinct attention pattern. In Figure~\\ref{fig:vis_head_entro}, certain attention heads show a highly concentrated pattern, with entropy values converging to $\\approx 1$, while others exhibit a highly dispersed pattern, with entropy values converging to $\\approx 10$. \nThe full head visualization of Figure~\\ref{fig:vis_head_entro} is located in Appendix~\\ref{app:vis_all_heads}.\n\nThis phenomenon casts doubt on uniform scaling — how can a single scaling factor cater to diverse attention heads? \nInspired by this, we further propose a head-based scale method.\n\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=\\linewidth]{fig/fig_vis_head_vs_uni_scale_.pdf}\n  \\caption{Comparing uniform and head-based scale (denoted as $\\lambda^{(h)}$). UniformScale fails eventually as the perplexity increases with longer sequences. HeadScale is capable of handling much longer context by assigning different scale factors to each attention head.}\n  \\label{fig:vis_head_vs_uni}\n\\end{figure}\n\\subsection{Head-based Scale}\n\\label{ssec:head_scale}\n\nWe reformulate the uniform attention scale as head-base attention scales\n\\begin{equation}\n\\alpha_{ij}^{(h)} = \\frac{e^{\\lambda^{(h)} \\bm{q}_i^{(h)} \\cdot \\bm{k}_j^{(h)}}}{\\sum_k e^{\\lambda^{(h)} \\bm{q}_i^{(h)} \\cdot \\bm{k}_k^{(h)}}}\n\\label{eq:head_scale}\n\\end{equation}\nwhere $\\lambda^{(h)}$ is a unique attention scaling factor for each head, totaling 704.\nCompared to a uniform attention scale, 704 head-based scales make it difficult to determine the optimal values by grid search.\nSimilar to AutoML \\citep{he2021automl}, we model the scales' optimal search as a parameter-efficient fine-tuning task.\nGiven a NoPE model $\\mathcal{M}$ and a set of head-based scales $\\{\\lambda^{(1)}, \\lambda^{(2)}, \\dots, \\lambda^{(m)}\\}$, we fix the model $\\mathcal{M}$ and define the head-based scales as trainable parameters $\\theta=\\{\\lambda^{(1)}, \\lambda^{(2)}, \\dots, \\lambda^{(m)}\\}$.\nWe aim to find an optimal set of values $\\theta^*=\\{\\lambda^{*(1)}, \\lambda^{*(2)}, \\dots, \\lambda^{*(m)}\\}$, that allows the model $\\mathcal{M}{(\\theta^*)}$ to successfully extend to the target length $L^\\prime$.\nTo this end, we optimize the language modeling loss function $\\mathcal{L}_{\\text{LM}}$ on the pre-training dataset $D$ with length $L^\\prime$ and size $n^\\prime, n^\\prime \\ll n$.\n\\begin{equation}\n\\theta^* = \\underset{x \\in D}{\\text{minimize}} \\quad \\mathcal{L}_{\\text{LM}}\\left(\\mathcal{M}{(\\theta, x)}\\right)\n\\label{eq:head_scale_obj}\n\\end{equation}\nThe search process is highly efficient. (1) The number of tunable parameters is extremely small, only 704 delta parameters over 1B model parameters; 2) The amount of training tokens for fine-tuning is extremely small too, only 0.03\\% of the pre-training data.\n\nIn addition, to ensure that the attention is reconcentrated instead of distracted \nby the scaling factors, we apply a focus constraint\nduring the optimization of Equation~\\ref{eq:head_scale_obj} \n\\begin{equation}\n\\lambda^{(*)} \\ge \\frac{1}{\\sqrt d}\n\\label{eq:concentration}\n\\end{equation}\n\n\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=\\linewidth]{fig/fig_vis_corr_.pdf}\n  \\caption{Correlation analysis for head-based scale when extended to 8K context. The analysis was conducted on the converged entropy values at 8K position, in relation to the scale searched. Each data point represents a unique attention head.}\n  \\label{fig:vis_corr}\n\\end{figure}\n\\paragraph{Initializing HeadScale}\nIn practice, we found that the initial value of head-based scales has a significant impact on the search of $\\theta^*$.\nAn obvious approach is to use the default value $\\lambda^{(*)}\\!=\\!\\frac{1}{\\sqrt{d}}$ from the pre-training phase. \nHowever, its length generalization results are quite unstable, with most being subpar,\nas the optimal scale often deviates significantly from the default value.\nWe propose another approach to utilize the best uniform scale from the grid search as the initial value.\nThe ablation study for the initialization approach is in Section~\\ref{ssec:ablation}.\n\nFigure~\\ref{fig:vis_head_vs_uni} compares the two generalization methods of NoPE, uniform scale versus head-based scales.\nHead-based scale exhibits better generalization than the uniform scale, achieving a lower log-PPL by 0.2 at 4K positions ($2\\!\\times\\!L$) and by 0.8 at 8K positions ($4\\!\\times\\!L$).\nThe average entropy $\\widebar{\\mathcal{H}}_i$ of the head-based scale is higher than that of the uniform scale, suggesting that the uniform scale method over-concentrates attention, particularly for some heads that inherently have more distracted patterns.\n\nFigure~\\ref{fig:vis_corr} shows the correlation between the converged entropy and the searched scale.\nTo save space, we uniformly sampled 7 layers and all their respective heads. \nWe observed that the correlation is layer-dependent, within each layer, heads with more concentrated attention (i.e., lower entropy) searched for larger scales, while heads with more dispersed attention (i.e., higher entropy) searched for smaller scales.\nThe result is as expected, the more concentrated the attention pattern, the larger the scaling factor needed to maintain its focus.\nFurthermore, we observed that attention heads in lower layers are generally more dispersed, whereas heads in higher layers are generally more concentrated (note that this is not strictly observed).\n\n\\section{Experiment}\n\\label{sec:exper}\n\n\\begin{table*}[t]\n    \\centering\n    \\small\n        \\begin{tabular}{lccccccccc}\n            \\toprule\n            \\textbf{Model} & \\textbf{Avg.} & \\textbf{arc\\_challenge} & \\textbf{arc\\_easy} & \\textbf{boolq} & \\textbf{hellaswag} & \\textbf{openbookqa} & \\textbf{piqa} & \\textbf{winogrande}\\\\\n            \\midrule\n            RoPE & 46.1 & \\textbf{24.3} & \\textbf{44.9} & \\textbf{59.7} & \\textbf{43.5} & 29.8 & 67.3 & \\textbf{53.3} \\\\ \n            NoPE & \\textbf{46.2} & 24.0 & \\textbf{44.9} & 58.1 & 43.4 & \\textbf{31.8} & \\textbf{68.4} & 52.9 \\\\\n            \\bottomrule\n        \\end{tabular}\n    \\caption{Commonsense reasoning ability of the pre-trained base models. }\n    \\label{tab:cs}\n\\end{table*}\nWe train a NoPE base model from scratch and investigate its capability in length generalization. We conduct length generalization experiments on long sequence language modeling, synthetic tasks (passkey retrieval), and real-world long context tasks (LongBench). Detailed experiment setup can be found in Appendix~\\ref{app:setup}.\n\n\\begin{table*}[t]\n    \\centering\n    \\small\n    \\begin{tabular}{lrrrrrrrrrr}\n    \\toprule\n        \\multirow{2}{*}{\\textbf{Model}} & \\multicolumn{2}{c}{\\textbf{FT}}   & \\multicolumn{4}{c}{\\textbf{PG19}} & \\multicolumn{4}{c}{\\textbf{Proof-pile}}\\\\\n        \\cmidrule(lr){2-3} \\cmidrule(lr){4-7} \\cmidrule(lr){8-11}\n        & \\bf $L^{\\prime}$ & \\bf Tokens & \\textbf{2K} & \\textbf{4K} & \\textbf{8K} & \\textbf{16K} & \\textbf{2K} & \\textbf{4K} & \\textbf{8K} & \\textbf{16K} \\\\ \n        \\midrule\n        \\rowcolor{gray!10} \\multicolumn{11}{c}{\\textit{\\textbf{Original LMs}}} \\\\\n        RoPE & - & - & 14.5 & 491.4 & 488.5 & 599.5 & 3.5 & 303.0 & 432.1 & 759.5 \\\\ \n        NoPE & - & - & 14.6 & 326.9 & $>10^3$ & $>10^3$ & 3.5 & 117.4 & $>10^3$ & $>10^3$ \\\\\n        BLOOM & - & - & 27.7 & 158.0 & 264.6 & 403.4 & 6.9 & 74.1 & 176.2 & 334.5\\\\\n        MPT & - & - & 10.6 & 103.6 & 361.6 & 345.1 & 2.8 & 70.1 & $>10^3$ & $>10^3$ \\\\\n        \\arrayrulecolor{gray!20}\n        \\rowcolor{gray!10} \\multicolumn{11}{c}{\\textit{\\textbf{Generalization for RoPE}}} \\\\\n        NTK$^{\\text{zero}}$ & - & - & 14.5 & 14.9 & 22.8 & 80.4 & 3.5 & 3.3 & 4.1 & 13.3 \\\\ \n        YaRN$^{\\text{zero}}$ & - & - & 14.5 & 14.5 & 15.0 & 17.1 & 3.5 & 3.3 & 3.2 & 3.6 \\\\ \n        \\hline\n        \\multicolumn{1}{l}{\\multirow{3}{*}{PI$^{\\text{fair}}$}}  & 4K & 6M & 16.0 & 15.9 & 551.9 & $>10^3$ & 3.8 & 3.4 & 307.9 & 633.8 \\\\ \n        \\multicolumn{1}{l}{} & 8K & 13M & 17.4 & 17.1 & 17.1 & 752.8 & 4.0 & 3.6 & 3.4 & 406.3 \\\\ \n        \\multicolumn{1}{l}{} & 16K & 30M & 18.7 & 18.4 & 18.3 & 18.2 & 4.3 & 3.9 & 3.6 & 3.6 \\\\ \n       \n        \\hline\n        \\multicolumn{1}{l}{\\multirow{3}{*}{YaRN$^{\\text{fair}}$}}  & 4K & 6M & 15.5 & 15.4 & 545.2 & $>10^3$ & 3.7 & 3.4 & 351.5 & 698.2 \\\\ \n        \\multicolumn{1}{l}{} & 8K & 13M & 15.7 & 15.4 & 15.5 & 794.6 & 3.8 & 3.4 & 3.2 & 492.8 \\\\ \n        \\multicolumn{1}{l}{} & 16K & 30M & 15.9 & 15.6 & 15.4 & 15.5 & 3.8 & 3.5 & 3.2 & 3.2 \\\\ \n        \\hline\n        \\multicolumn{1}{l}{\\multirow{3}{*}{PI$^{\\text{raw}}$}} & 4K & 33M & 15.2 & 15.0 & 623.8 & 951.7 & 3.6 & 3.3 & 334.4 & 595.5 \\\\ \n        \\multicolumn{1}{l}{} & 8K & 66M & 15.4 & 15.1 & 15.0 & 909.6 & 3.6 & 3.3 & 3.0 & 463.0 \\\\ \n        \\multicolumn{1}{l}{} & 16K & 131M & 15.6 & 15.3 & 15.0 & 14.9 & 3.7 & 3.3 & 3.0 & 3.0 \\\\ \n        \\hline\n        \\multicolumn{1}{l}{\\multirow{3}{*}{YaRN$^{\\text{raw}}$}} & 4K  & 33M & 15.1 & 15.0 & 573.3 & 951.4 & 3.6 & 3.3 & 358.8 & 656.8 \\\\ \n        \\multicolumn{1}{l}{}& 8K  & 66M & 15.1 & 14.8 & 14.8 & 816.0 & 3.6 & 3.3 & 3.1 & 501.5 \\\\ \n        \\multicolumn{1}{l}{}& 16K  & 131M & 15.0 & 14.8 & 14.5 & 14.5 & 3.6 & 3.3 & 3.0 & 3.0 \\\\ \n        \n        \\rowcolor{gray!10} \\multicolumn{11}{c}{\\textit{\\textbf{Generalization for NoPE}}} \\\\\n        $\\lambda\\!=\\!\\frac{1.2}{\\sqrt{d}}$  & - & -  & 15.0 & 16.0 & 513.7 & $>10^3$ & 3.6 & 3.3 & 175.3 & $>10^3$ \\\\ \n        $\\lambda\\!=\\!\\frac{1.5}{\\sqrt{d}}$  & - & -  & 19.0 & 20.2 & 45.3 & 224.1 & 3.9 & 3.7 & 4.9 & 99.2 \\\\ \n        $\\lambda\\!=\\!\\frac{1.8}{\\sqrt{d}}$  & - & -  & 30.4 & 42.4 & 69.1 & 198.8 & 5.1 & 5.6 & 8.5 & 38.2 \\\\ \n        \\hline\n        \\multicolumn{1}{l}{\\multirow{3}{*}{$\\lambda^{(h)}$}} & 4K  & 6M & 14.8 & 15.3 & 404.5 & $>10^3$ & 3.5 & 3.2 & 153.4 & $>10^3$ \\\\ \n        \\multicolumn{1}{l}{} & 8K & 13M & 15.7 & 15.3 & 21.1 & 721.7 & 3.6 & 3.3 & 3.2 & 318.5 \\\\ \n        \\multicolumn{1}{l}{} & 18K & 30M & 18.3 & 19.0 & 18.8 & 30.4 & 4.0 & 3.7 & 3.3 & 4.1 \\\\ \n        \\arrayrulecolor{black}\n        \\bottomrule\n        \\vspace{-0.5cm}\n    \\end{tabular}\n    \\caption{Sliding window perplexity of different context window extension methods tested on PG19 and ProofPile. The ``fair'' and ``raw'' versions of PI and YaRN differ from the training data, as detailed in Appendix~\\ref{app:setup}. The notation $\\lambda=*$ denotes uniform attention scale by the given number, and $\\lambda^{(h)}$ represents head-based scale. }\n    \\label{tab:ppl}\n\\end{table*}\n\\subsection{NoPE pre-trained model}\n\\label{ssec:nope}\n\nFor a fair comparison with RoPE, we train a NoPE model with $1.1$B parameters from the TinyLlama \\cite{zhang2024tinyllama} code base\\footnote{\\url{https://github.com/jzhang38/TinyLlama}}. The NoPE model has $22$ layers of Transformer blocks, $32$ attention heads per layer, $2048$ embedding size. The model is trained on Slimpajama \\cite{cerebras2023slimpajama} joint with Starcoderdata \\cite{li2023starcoder} by $50$K steps ($\\approx 100$B tokens) with sequence length $L=2048$. \n\nAll settings are kept identical to those of TinyLlama, including the model architecture, training data, training procedure, and hyper-parameters, except that the rotary position embedding (RoPE) in TinyLlama is removed, making it a NoPE model, and the learning rate is set to $3.5\\times10^{-4}$.\n\nFollowing TinyLlama, we evaluate the commonsense reasoning ability of the NoPE model and report acc\\_norm in Table~\\ref{tab:cs}. We compare with the TinyLlama checkpoint that is trained on 100B tokens. The purpose of this experiment is to prove the NoPE base model performs on par with RoPE.\n\n\\subsection{Long Sequence Language Modeling}\n\\label{ssec:ppl}\n\nSuccess on long sequence language modeling tasks is essential for length generalization.\nA method that does not perform well in language modeling probably won't handle real-world long-context tasks.\n\n\\paragraph{Settings.}\nTo evaluate the long sequence language modeling performances, we test our NoPE-based methods and RoPE-based baselines on PG19 \\cite{Rae_Potapenko_Jayakumar_Hillier_Lillicrap_2020} and proof-pile \\cite{proofpile} datasets.\nFor each dataset, we sample a subset of the test set and evaluate on $2$M tokens using sliding window evaluation ($S=256$) suggested by \\citet{press2022train}.\nWe report the perplexity (PPL) of the models in Table~\\ref{tab:ppl}.\n\n\\begin{figure*}[t]\n  \\centering\n  \\includegraphics[width=0.32\\linewidth]{fig/fig_pk_rope.pdf}\n  \\includegraphics[width=0.32\\linewidth]{fig/fig_pk_yarn8k.pdf}\n  \\includegraphics[width=0.32\\linewidth]{fig/fig_pk_yarn16k.pdf}\\\\\n  \\includegraphics[width=0.32\\linewidth]{fig/fig_pk_nope.pdf}\n  \\includegraphics[width=0.32\\linewidth]{fig/fig_pk_hs8k.pdf}\n  \\includegraphics[width=0.32\\linewidth]{fig/fig_pk_hs16k.pdf}\n  \\caption{The figures illustrate the passkey retrieval accuracy for both RoPE and NoPE methods. The vertical dashed line represents the context length of the models, which could be either the pre-training length or the fine-tuning length. The title of each sub-figure indicates the average accuracy within the model’s context length. Notably, NoPE demonstrates robust performance even beyond the model’s context window, indicating significant potential for generalization.}\n  \\label{fig:passkey}\n\\end{figure*}\n\\paragraph{Main results.} \nFirstly, by comparing the original language models, NoPE's perplexity (PPL) is comparable to RoPE's for lengths within the training distribution, confirming the findings of \\citet{haviv-etal-2022-transformer,chi-etal-2023-latent}. \nHowever, all LMs, including ALiBi models, fail to generalize out-of-the-distribution, indicating that explicit positional encoding is not the main reason for their failure in generalization. \nCurrent work on length generalization still focuses mainly on manipulating positional encoding.\nTherefore, the length generalization issue within causal Transformer networks warrants a reanalysis and reinterpretation.\n\nSecondly, by comparing the two generalization methods for NoPE proposed in this paper, the uniform scale method has significant limitations. \nAlthough using a larger scale can reduce the PPL at greater positions, it significantly affects the PPL at closer ranges. \nFor instance, with a scale value of 1.8, the PPL on 2K@PG19 rises from 14.6 to 30.4, and on 2K@Proof-pile, it rises from 3.5 to 5.1.\nOn the contrary, the head-based scale method not only successfully extrapolates to 16k but also has minimal impact on the PPL at closer distances (for 18K, increases only +3.7 on 2K@PG19, +0.5 on 2K@Proof-pile), proving that attention heads with different patterns indeed require distinct scale values.\n\nThird, a full comparison with RoPE LM's generalization method. \nComparing the \\emph{zero-shot} generalization methods, the head-based scale has better generalization than NTK, but weaker than YaRN.\nIn a fair comparison with the RoPE generalization methods which require \\emph{fine-tuning}, \nthe head-based scale method is competitive with these RoPE baselines, especially the Proof-pile dataset.\nHowever RoPE baselines (PI, YaRN) still benefit from more training tokens, and the head-based scale on NoPE reaches its upper limit.\n\nIn summary, the head-based scale generalization method for NoPE slightly outperforms RoPE's early generalization method NTK, but still lags behind the recently introduced YaRN, particularly in near-distance PPL performance. \nConsidering the significant challenge of generalizing NoPE compared to RoPE (due to the lack of explicit positional encoding to manipulate), this work, as the first to tackle length generalization for NoPE, has achieved its set goals.\n\nThe observed gap may imply that constraining the NoPE model to focus on fewer tokens could detrimentally affect its efficacy. Future efforts will be directed at enhancing the head-based scaling method to regain the level of performance seen in pretraining.\n\n\\begin{table*}[!t]\n    \\centering\n\n    \\resizebox{\\textwidth}{!}{\n        \\begin{tabular}{lrrrrrrrrrrrrrrrrrr}\n        \\toprule\n        \\multirow{2}{*}{\\textbf{Model}} & \\multirow{2}{*}{\\textbf{Ctx.}} & \\multirow{2}{*}{\\textbf{Avg.}} & \\multicolumn{3}{c}{\\textbf{Singl-Doc QA}} & \\multicolumn{3}{c}{\\textbf{Multi-Doc QA}} & \\multicolumn{3}{c}{\\textbf{Summarization}} & \\multicolumn{3}{c}{\\textbf{Few-shot Learning}} & \\multicolumn{2}{c}{\\textbf{Synthetic}} & \\multicolumn{2}{c}{\\textbf{Code}} \\\\\n        \\cmidrule(lr){4-6} \\cmidrule(lr){7-9} \\cmidrule(lr){10-12} \\cmidrule(lr){13-15} \\cmidrule(lr){16-17} \\cmidrule(lr){18-19} \n        & & & \\textbf{NQA} & \\textbf{Qsp} & \\textbf{MulF} & \\textbf{HpQA} & \\textbf{2WQA} & \\textbf{Musq.} & \\textbf{GRpt} & \\textbf{QSum} & \\textbf{MulN} & \\textbf{TREC} & \\textbf{TrQA} & \\textbf{SSum} & \\textbf{PsgC} & \\textbf{PsgR} & \\textbf{Lcc} & \\textbf{Re-P} \\\\\n        \\midrule\n        \\rowcolor{gray!10} \\multicolumn{19}{c}{\\textit{\\textbf{Original LMs}}} \\\\\n        RoPE & 2K & 16.5 & 3.5 & 4.7 & 17.5 & 3.4 & 8.8 & 2.8 & 26.9 & 8.4 & \\textbf{25.9} & 33.5 & 18.8 & 15.7 & 1.9 & 2.5 & 49.5 & 40.1 \\\\ \n        NoPE & 2K & 18.3 & 6.1 & 7.9 & 22.4 & 6.6 & 10.3 & 3.1 & \\textbf{28.9} & 8.8 & 25.1 & \\textbf{41.5} & 30.0 & 3.5 & 1.0 & 3.0 & 48.4 & 46.6 \\\\ \n        \\arrayrulecolor{gray!20}\n        \\rowcolor{gray!10} \\multicolumn{19}{c}{\\textit{\\textbf{Generalization for RoPE}}} \\\\\n        \\multicolumn{1}{l}{\\multirow{3}{*}{PI$^{\\text{raw}}$}} & 4K & 16.7 & 5.4 & 8.6 & 18.6 & 4.5 & 9.1 & 3.9 & 26.4 & 9.9 & 18.5 & 21.5 & 21.2 & 22.2 & \\textbf{2.7} & 1.5 & 48.5 & 44.6 \\\\ \n        \\multicolumn{1}{l}{} & 8K & 16.7 & 4.7 & 9.6 & 16.3 & 5.4 & 9.3 & 4.0 & 14.6 & 9.4 & 20.7 & 27.0 & 23.1 & 23.5 & 2.1 & 3.4 & 50.0 & 44.7 \\\\ \n        \\multicolumn{1}{l}{} & 16K & 17.2 & 4.8 & 8.1 & 18.6 & 5.4 & 9.4 & 3.8 & 22.9 & 9.9 & 21.3 & 24.0 & 23.9 & \\textbf{25.4} & 1.6 & 1.8 & \\textbf{50.5} & 43.8 \\\\ \n        \\hline\n        \\multicolumn{1}{l}{\\multirow{3}{*}{YaRN$^{\\text{raw}}$}} & 4K & 16.2 & \\textbf{6.4} & 8.7 & 18.2 & 4.0 & 11.0 & 3.0 & 17.5 & 9.0 & 15.6 & 27.5 & 21.5 & 20.3 & 1.6 & 0.5 & 49.8 & 45.2 \\\\ \n        \\multicolumn{1}{l}{\\multirow{3}{*}{}} & 8K & 16.4 & 6.0 & 11.4 & 16.0 & 5.0 & 8.3 & 3.5 & 16.3 & \\textbf{10.3} & 19.6 & 21.0 & 24.9 & 22.1 & 1.3 & 2.0 & 49.6 & 45.3 \\\\ \n        \\multicolumn{1}{l}{\\multirow{3}{*}{}} & 16K & 17.7 & 4.5 & 10.5 & 17.1 & 5.2 & 8.9 & \\textbf{4.7} & 18.9 & 9.2 & 19.5 & 38.0 & 24.4 & 25.2 & 1.7 & 1.8 & 49.8 & 44.6 \\\\ \n        \\rowcolor{gray!10} \\multicolumn{19}{c}{\\textit{\\textbf{Generalization for NoPE}}} \\\\\n        \\multicolumn{1}{l}{\\multirow{3}{*}{$\\lambda^{(h)}$}} & 4K & \\textbf{18.5} & 6.3 & 11.1 & \\textbf{23.1} & 5.7 & 10.1 & 4.2 & 27.7 & 8.9 & 23.4 & 25.5 & \\textbf{35.7} & 13.7 & 0.6 & \\textbf{4.5} & 47.9 & \\textbf{46.9} \\\\ \n        \\multicolumn{1}{l}{\\multirow{3}{*}{}} & 8K & 17.2 & 5.8 & 11.7 & 21.4 & 6.1 & 10.8 & 3.9 & 24.1 & 8.9 & 18.3 & 31.0 & 31.4 & 4.5 & 0.6 & 3.1 & 47.3 & 46.5 \\\\ \n        \\multicolumn{1}{l}{\\multirow{3}{*}{}} & 18K & 17.0 & 6.0 & \\textbf{12.8} & 20.3 & \\textbf{7.0} & \\textbf{12.9} & 4.1 & 17.2 & 8.4 & 16.1 & 41.0 & 32.9 & 5.1 & 0.3 & 2.1 & 44.5 & 41.0 \\\\ \n        \\arrayrulecolor{black}\n        \\bottomrule\n        \\end{tabular}\n    }\n    \\caption{Real-world Long-Context performance of NoPE-extension methods and various RoPE baselines. The ``Ctx.'' column represents testing context length during evaluation, which corresponds to either the pre-training length for base models or the extended length for length generalization methods.}\n    \\label{tab:longbench}\n\\end{table*}\n\\subsection{Synthetic Long Context Tasks}\n\\label{ssec:passkey}\n\nA synthetic task is constructed in Landmark Attention \\cite{mohtashami2023randomaccess} called \"Passkey Retrieval\".\nIt aims to test the effective context window size of the model.\nThe task is to retrieve a randomly placed passkey from a long sequence of tokens, where the passkey is a randomly sampled number of 5 digits and the sequence is built by concatenating irrelevant sentences.\n\n\\paragraph{Settings.} We evaluate the performance of passkey retrieval across various context lengths. For each specified context length, we conduct tests on 10 distinct passkey depths, each associated with 10 randomly selected passkeys. We report the retrieval accuracy in this task.\n\nIt is observed in Figure~\\ref{fig:passkey} that both the NoPE base model and head-based scale perform well even when evaluating on $2\\times$ the pretraining or fine-tuning context window, while RoPE strictly operates within the pre-trained sequence length and immediately fails outside of it. The result indicates that NoPE possesses significant potential for generalization.\n\n\\begin{table}[t]\n    \\centering\n    \\resizebox{\\linewidth}{!}{\n        \\begin{tabular}{lcccc}\n        \\toprule\n        \\multirow{2}{*}{\\textbf{Model}} & \\multicolumn{2}{c}{\\textbf{PPL@16K ($\\downarrow$)}}  & \\multirow{2}{*}{\\textbf{Passkey ($\\uparrow$)}} & \\multirow{2}{*}{\\textbf{LongBench ($\\uparrow$)}} \\\\\n        \\cmidrule(lr){2-3}\n        & \\textbf{PG19} & \\textbf{Proof-pile} & &\\\\\n        \\midrule\n        $\\lambda^{(h)}$ 18K & 30.4 & \\textbf{4.1} & \\textbf{81} & \\textbf{17.0} \\\\ \n        w/o focus constraint & \\textbf{25.9} & 4.2 & 53 & 16.7 \\\\\n        w/o initialization & 31.4 & 4.3 & 26 & 15.8 \\\\\n        \\bottomrule\n        \\end{tabular}\n    }\n    \\caption{Ablation study on the two variants of HeadScale. Passkey results are listed as average accuracy, and LongBench results are averaged score among all sub-tasks.}\n    \\label{tab:ablation}\n\\end{table}\n\\subsection{Real-World Long Context Tasks}\n\\label{ssec:longbench}\n\nLongBench \\cite{bai2023longbench} is a comprehensive assessment of the long context understanding capabilities of large language models. We test all models using beam search decoding with beam size $5$. The evaluation context size is set to the model context window accordingly in order to test the model's capability to utilize a longer context. We only include raw PI and YaRN as the baseline in this task.\n\nWe find that the performance of the NoPE base model is better than its RoPE counterpart. Concluding better information utilization in the original length. Moreover, the head-based scale at a 4k extension length performs the best among all baselines. We attribute it to the capability of the NoPE base model and the successful length generalization of the head-based attention scale method.\nWhile the head-based model still suffers from performance degradation when extending to a longer context, as it is stated in Section~\\ref{ssec:ppl}.\n\n\\subsection{Ablation Study}\n\\label{ssec:ablation}\n\nWe have introduced two key components of HeadScale in Section~\\ref{ssec:head_scale}, a concentration constraint and an initializing technique. The ablation study in Table~\\ref{tab:ablation} depicts that although occasionally perform better in language modeling, the two variants are less preferment in passkey retrieval and LongBench, indicating their inability to utilize long context information. \n\nDetailed results of the passkey retrieval task can be found in Figure~\\ref{fig:ablation_pk} in the Appendix~\\ref{app:abl_hs_pk}. They are completely unable to answer the passkey except when it is at the beginning of the context window.\n\n\\section{Related Work}\n\\label{sec:related-work}\n\n\\paragraph{Transformers without position encoding}\n\\citet{haviv-etal-2022-transformer} was the first to discover that causal Transformer networks could perform language modeling tasks successfully even without explicit PE.\n\\citet{chi-etal-2023-latent} provided a theoretical explanation for NoPE, demonstrating that for an initialized NoPE LM, the variance of the hidden representations in each layer is position-dependent, with variance decreasing for larger positions.\nBoth works demonstrate that the NoPE hidden layer representation implies positional information through the probing task.\n\\citet{kazemnejad2023the} proved through constructive methods that NoPE can learn absolute PE from the first layer and relative PE from the second layer. \nThey also showed that NoPE has an extremely weak length generalization ability (train $\\sim$20, test $\\sim$40), but is slightly better than LM with explicit PE.\nThis paper first proposes length generalization methods for NoPE with uniform scale and head-based scale.\nFor the first time verifies the effectiveness of NoPE generalization in real LLM settings.\n\n\\paragraph{Length generalization}\nDue to high computational and memory requirements, LLM training is usually limited to short inputs. \nDirectly applying LLMs to long inputs faces the challenge of out-of-distribution (OOD) issues. \nResearch to enable LLMs to process long inputs has been extensive \\cite{huang2023advancing,dong2023survey}. \nThe earliest methods involved designing new relative PE mechanisms during pre-training\n\\citep{press2021train,sun-etal-2023-length}. \nSubsequent studies focused primarily on the widely used RoPE \\cite{su2024roformer} and proposed length extension by mitigating RoPE's OOD issues through interpolated positions\n\\citep{chen2023extending,kaiokendev9444,peng2023yarn,emozillareddit,bloc97,bloc972}. \nOther works employed sliding window attention mechanisms to prevent relative positions from exceeding the maximum distance seen in pre-training \n\\citep{mohtashami2023landmark,han2023lminfinite,xiao2023efficient,jin2024llm,zhang2024soaring}. \nHowever, these models ignore information from distant tokens, thus failing to capture long-distance context dependencies.\nAll existing methods rely on specific explicit PEs. \nHowever, the NoPE architecture is more streamlined and more aligned to the form of human language modeling.\nExploring NoPE's length generalization is therefore more intriguing and attractive.\n\n\\section{Discussion}\nWe studied the length generalization of Casual Transformer \nwithout explicit position encoding.\nWe developed a parameter-efficient tuning algorithm \nwhich aims to search for the best temperature hyper-parameters\nfor attention heads.\nThrough empirical evaluation, we saw that \nNoPE can achieve competitive length generalization and might be \na promising alternative for long-context language modeling.\n\nNoPE provides a new perspective to understanding the role of positional information by isolating and eliminating the effects of explicit positional encoding. Our work demonstrates the correlation between length generation failures and distraction of attention in NoPE models, thus the proposed method concentrates the attention by adjusting the scaling factor. While current works on length generalization mainly focus on manipulating positional encoding, our work suggests a new key component to generalization.\n\n\\section*{Limitation}\nThe length generalization algorithms discussed in this paper exhibit \n competitive performances, but the NoPE model itself still\n underperforms with state-of-the-art RoPE models, which \n makes the results over long sequence language modeling tasks \n and LongBench tasks are less competitive.\n NoPE still faces the challenges of considerable memory usage and computational complexity due to the quadratic nature of attention computation when processing extremely long contexts. \nHardware limitations are likely to become a constraining factor for length generalization soon.\n We plan to further \n improve the NoPE's performances for a fairer comparison.\n This paper is also most an empirical one, which requires \n a deeper theoretical understanding of NoPE's length generalization\n in the future.\n\n\\section*{Acknowledgement}\nThe authors wish to thank all reviewers for their helpful comments and suggestions.\nThe corresponding authors are Tao Ji, Yuanbin Wu and Xiaoling Wang.\nThis research was (partially) supported by NSFC(62076097), National Key R\\&D\nProgram of China (2021YFC3340700),  the Open Research Fund of Key Laboratory of Advanced Theory and Application in Statistics and Data Science (East China Normal University), Ministry of Education.\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{Fourier Position Embedding:\\\\Enhancing Attention's Periodic Extension for Length Generalization}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\nExtending the context length of Language Models (LMs) by improving Rotary Position Embedding (RoPE) has become a trend.\nWhile existing works mainly address RoPE's limitations within attention mechanism, this paper provides an analysis across nearly all parts of LMs, uncovering their adverse effects on length generalization for RoPE-based attention.\nUsing \\textit{Discrete Signal Processing} theory, we show that RoPE enables periodic attention by implicitly achieving \\textit{Non-Uniform Discrete Fourier Transform}.\nHowever, this periodicity is undermined by the spectral damage caused by: 1) linear layers and activation functions outside of attention; 2) insufficiently trained frequency components brought by time-domain truncation. \nBuilding on our observations, we propose \\textbf{\\textit{Fourier Position Embedding (FoPE)}}, which enhances attention's frequency-domain properties to improve both its periodic extension and length generalization. \nFoPE constructs \\textit{Fourier Series} and zero-outs the destructive frequency components, increasing model robustness against the spectrum damage.\nExperiments across various model scales show that, within varying context windows, FoPE can maintain a more stable perplexity and a more consistent accuracy in a needle-in-haystack task compared to RoPE and ALiBi.\nSeveral analyses and ablations bring further support to our method and theoretical modeling.\n\n\\end{abstract}\n\n\\section{Introduction}\n\n\\begin{figure}\n    \\subfloat[Accuracy on Passkey Retrieval (higher is better)]{\n        \\includegraphics[width=0.95\\linewidth]{figures/c4_512_downstream.pdf}\n    }\n    \\hfill\n    \\subfloat[Perplexity on C4 (lower is better)]{\n        \\includegraphics[width=0.95\\linewidth]{figures/c4_512_overall.pdf}\n    }\n    \\caption{Training with max\\_seq\\_length=512.}\n    \\label{fig:c4_512}\n\\end{figure}\n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/main.pdf}\n    \\caption{The reasons why RoPE’s periodic extension deteriorates and how FoPE addresses these issues to improve length generalization. (a) As signals pass through linear and nonlinear transformations, this causes spectral leakage and distortion, mixing multiple frequencies into a single dimension. Under RoPE, each dimension is treated as a single-frequency component. By contrast, FoPE models each dimension as a Fourier series of different frequency components, thereby separating information more effectively and mitigating spectral damage. (b) FoPE eliminates inadequately trained frequency components, which are harmful for periodic extension. By preserving only the zero-frequency component, FoPE safeguards periodic extension and delivers more robust length generalization.}\n    \\label{fig:main}\n\\end{figure*}\n\nGeneration based on the information from long contexts is crucial for Language Models (LMs). However, LMs are typically trained on a fixed context window \\citep{vaswani2017attention, touvron2023llama, groeneveld2024olmo} and tends to overfit to the specific context length.\n\nMany studies consider the absolute position embedding \\citep{vaswani2017attention} to be the source of overfitting in length generalization.\nAs mitigation, several relative position embedding methods have been proposed \\citep{press2021train, su2024roformer, peng2023yarn, jin2024llm} to improve LMs' long-distance dependency.\nAmong these, ALiBi \\citep{press2021train} introduced a position-biased attention mask, which linearly declines the attention weights based on distance. ALiBi delivers stable perplexity in pre-training, but it loses the information from long-distance tokens, resulting in poor performance on long-context downstream tasks.\nAnother method, RoPE \\citep{su2024roformer}, uses the phase of complex numbers to store the position information. Combined with continual pre-training and other interpolation-based methods \\citep{peng2023yarn,xiong-etal-2024-effective,chen2024clex,jin2024llm}, RoPE provides better access to long-distance information, making it one of the most widely used position embedding. \nHowever, RoPE-based LMs still struggle with length generalization without supplementary methods. \n\nIn this paper, we take a closer look at RoPE in the frequency-domain with tools from \\textit{Discrete Signal Processing (DSP)} theory.\nOur modeling reveals that RoPE implicitly performs \\textit{Non-Uniform Discrete Fourier Transform (NUDFT)} on the hidden states, enabling periodic attention based on the frequency-domain encoding.\nHowever, we find that the periodicity is hindered by the spectral damage caused by: 1) linear layers and activation functions outside attention; 2) inadequately-trained frequency components within attention (See Fig \\ref{fig:main}). This explains why RoPE fails to achieve length generalization without assistance from other methods. \n\nBuilding on our observations above, we propose \\textbf{\\textit{Fourier Position Embedding (FoPE)}} to further improve the attention's periodic extension for better length generalization. Compared to RoPE, FoPE introduces two main improvements:\n1) While RoPE treats each dimension as a single-frequency function, FoPE models each dimension as a \\textit{Fourier Series}, consisting of a dominate frequency component and several harmonic components. This approach better mirrors the actual spectrum in LMs and helps attention separate information across different wavelengths, mitigating the negative effects of Spectral Damage.\n2) FoPE clips inadequately trained frequency components that is harmful to length generalization. To keep the passing of long wavelength information, we substitute these components with zero, as the zero-frequency component corresponds to the longest wavelength.\n\nWe summarize our contribution as follows:\n\n    1. Based on DSP, we provide frequency-domain analysis to reveal the negative influence from nearly all parts from LMs. We find that the length generalization is hindered by the Spectrum Damage arised from: 1) linear layers and activation functions; 2) undertrained frequency components. \n\n    2. We propose FoPE to improve attention's robustness on the Spectrum Damage. FoPE construct Fourier Series to extract multi-frequency information in each dimension, and clip the frequency of destructive components to zero. Thus, FoPE delivers better periodic extension of attention, thus bringing better length generalizaion.\n\n    3. We conduct experiments across several model scales and datasets. The perplexity in pre-training and the accuracy in needle-in-haystack demonstrate FoPE's superiority over RoPE and ALiBi on length generalization. Ablations on both frequency and time domain bring further support to our method and theoretical modeling.\n    \n    \n    \n\n\\section{Preliminaries}\n\\subsection{Non-Uniform Discrete Fourier Transform}\nGiven a finite sequence of equally-spaced samples $\\{x_n\\}:=x_0, x_1, ..., x_{N-1}$ of a continuous function $x$, \\textit{\\textbf{Discrete Fourier Transform (DFT)}} converts them into equally-spaced frequency-domain components $\\{X_m\\}:=X_0, X_1, ..., X_{M-1}$, the original samples can be recovered by Inverse DFT (IDFT):\n\\begin{equation}\\label{eq:dft}\n    \\resizebox{0.98\\hsize}{!}{%\n    $X_m = \\sum\\limits_{n=0}^{N-1}x_n e^{-i2\\pi\\frac{n}{N}m}, \\ \\ x_n = \\frac{1}{M}\\sum\\limits_{m=0}^{M-1}X_m e^{i2\\pi\\frac{m}{M}n}$\n    }\n\\end{equation}\nAs $e^{i\\omega n}=\\cos\\omega n+i\\sin\\omega n$ is periodic in the original domain, DFT implicitly transforms the original function into a linear combination of periodic waves with frequency $\\omega_m=2\\pi\\frac{m}{M}$. \nThus, DFT is an estimation of the original function, which is lossless only if the original function exactly composed of these specific periodic components. \n\nTo achieve a more precise approximation, the sampled frequencies can follow any arbitrary distribution $\\{\\omega_m\\}:=\\omega_0, \\omega_1, ..., \\omega_{M-1}$, \nwhere the only constraint is $\\omega_m\\in[0, 2\\pi)$. The generalization of DFT is called \\textit{\\textbf{Non-Uniform DFT (NUDFT)}}.\n\n\\subsection{RoPE implicitly achieves Periodic Attention based on NUDFT}\n\nGiven a $M$ dimension Query $Q$ and Key $K$ of token $a$ and $b$, RoPE rotates them to different phase based on the dimension $m$:\n\\begin{equation}\n    \\widetilde{q_m}(n_a) = Q_me^{i\\omega_mn_a}, \\widetilde{k_m}(n_b) = K_me^{i\\omega_mn_b}\n\\end{equation}\nwhere $\\omega_m=1/{\\theta^{^{(2m/M)}}}$ and $\\theta$ is the pre-defined parameters in RoPE. Then, the attention weight $h_m(n)$ in each dimension will be calculated as:\n\\begin{equation}\n    \\widetilde{h_m}(n) = \\widetilde{q_m}(n_a)\\widetilde{k_m}^*(n_b) = H_me^{i\\omega_mn}\n\\end{equation}\nwhere $n=n_a-n_b$ and $H_m=Q_mK_m$. Finally, the overall attention weight between different tokens can be formalized as:\n\\begin{equation}\n    h(n) = \\sum\\limits_{m=0}^{M-1}\\widetilde{h_m}(n) = \\sum\\limits_{m=0}^{M-1}H_me^{i\\omega_mn}\n\\end{equation}\nComparing it with Eq (\\ref{eq:dft}), it can be observed that RoPE implicitly achieves a token-level Inverse NUDFT with frequency components $\\{\\omega_m\\}$. \n\nBased on NUDFT, RoPE models the interactions between different tokens as functions composed of several periodic components, which brings \\textbf{\\textit{periodic extension}} in each dimension $m$:\n\\begin{equation}\\label{eq:periodic}\n    \\widetilde{h_m}(n+N_{\\omega_m}) =  \\widetilde{h_m}(n)\n\\end{equation}\nwhere $N_{\\omega_m}=\\frac{2\\pi}{\\omega_m}$ is this component's period. This property can generalize LMs to longer context.\n\n\\section{Spectrum Damage Confine the Length Generalization}\n\\label{sec:spectrum_damage}\nIdeally, RoPE-based Attention achieves periodic extension in any length scenario. However, this extension is confined as a key ideal property that is not guaranteed in LMs.\n\n\\subsection{Negative Influence of Spectrum Damage}\nThe ideal coefficients and frequencies of NUDFT have one-to-one correspondence. The coefficient of each frequency represents the influence of each token on others propagated at a specific wavelength.\n\nHowever, the periodic extension is hindered, if the coefficient also contains the information from another frequency component $\\omega_o$ with coefficient $H_{\\omega_o}=\\sigma H_\\omega$, called the \\textbf{\\textit{Spectrum Damage}}. \n\nIf we define the damaged function as $h_{m}' = H_{\\omega_m}[(1-\\sigma)e^{i\\omega_m n}+\\sigma e^{i\\omega_o n}]$, we find:\n\\begin{equation}\\label{eq:spectrum_damage}\n    h_{m}'(n+N_{\\omega_m}) \\neq h_{m}'(n)\n\\end{equation}\nas $N_{\\omega_m}$ is not the period of $h_{\\omega_o}$. \nIn other words, the information from each component is transmitted through waves with mismatched wavelengths, leading to inaccurate estimation of the influence propagated within each wavelength. As a result, the periodic extension and length generalization of attention are adversely affected.\n\n\\subsection{Spectrum Damage Outside Attention}\n\\label{subsec:spectrum_damage}\n\nThe LMs' linear layers and activation functions outside attention bring two types of spectrum damage, destroying the one-to-one correspondence between coefficients and frequencies.\n\n\\textbf{Linear Layer} uses weights $W\\in\\mathbb{R}^{M \\times M}$ to map a $M$ dimension hidden state $X\\in\\mathbb{R}^M$ to another hidden state $Y\\in\\mathbb{R}^M$. Thus, each dimension of $Y$ will be a linear combination of different components of $X$:\n\\begin{equation}\n    Y_m = \\sum\\limits_{k=0}^{M-1}W_{km}X_k\n\\end{equation}\nThis results in \\textbf{\\textit{Spectrum Leakage}}, as different frequency components exhibit interplay. \n\n\\textbf{Activation Function} has non-linearity in the time domain, generating harmonic frequencies as described by the following Lemma:\n\n\\begin{lemma}\nGiven a double-frequency sinusoid function $x(n)=\\cos\\omega_1 n + \\cos\\omega_2 n$ and any time-independent non-linear function $g$. The effect of $g$ on $x(n)$ will produce waves whose frequencies are the linear combinations of $\\omega_1$ and $\\omega_2$:\n\\begin{equation}\n    g(x(n)) = \\sum\\limits_{j \\in N}\\sum\\limits_{k \\in N} a_{j,k} \\cos(j \\omega_1 + k \\omega_2)n\n\\end{equation}\nwhich can be generalized to any multi-frequency function $x(n)=\\sum(a_\\omega\\sin\\omega n+b_\\omega\\cos\\omega n)$\\footnote{From \\citep{oppenheim1982signal}}.\n\\label{lemma:harmonic}\n\\end{lemma}\n\nAs the hidden states have been transformed into multi-frequency functions by Linear Layer, passing them across Activation Functions introduces additional harmonic components, leading to serious \\textbf{\\textit{Spectrum Distortion}}.\n\nThese two types of Spectrum Damage undermine the periodic extension of attention (as shown in Eq.(\\ref{eq:periodic})(\\ref{eq:spectrum_damage})), hindering the model's length generalization property.\n\n\\subsection{Spectrum Damage Inside Attention}\n\\label{subsec:undertrained_frequency}\nBesides the spectrum damage outside attention, the undertrained components of attention within extremely low frequencies ($\\omega_m<\\frac{2\\pi}{N}$) also bring spectrum damage.\n\nConsider a single-frequency function $x_m(n)=e^{i\\omega_m n}\\text{rect}(n)$ truncated by a square wave:\n\\begin{equation}\n    \\text{rect}(n)=\n    \\left\\{\n        \\begin{array}{cc}\n            1 & ,n \\leq N \\\\\n            0 & ,n > N\n        \\end{array}\n    \\right.\n\\end{equation}\nBased on the results of DFT, the spectrum estimation of x(n) is\\footnotemark[1]:\n\\begin{equation}\n    X(\\omega) = \\alpha\\delta(\\omega_m)+ \\frac{\\sin[(N-\\alpha N_m)(\\omega-\\omega_m)]}{\\omega-\\omega_m}\n    \\label{eq:undertrained}\n\\end{equation}\nwhere $\\alpha=\\lfloor\\frac{N}{N_m}\\rfloor$ and $N_m=\\frac{2\\pi}{\\omega_m}$. \n\nIn the frequency domain, time-domain truncation introduces noisy components via the latter sub-function. When the period of the primary frequency component exceeds the truncation length, its amplitude is significantly weakened. Consequently, noisy components dominate these dimensions, impairing the periodic extension (as defined by Eq.(\\ref{eq:periodic})(\\ref{eq:spectrum_damage})). \nIn contrast, high-frequency components are minimally affected because their coefficients $\\alpha$ dominate over the noisy components.\n\nIntuitively, when sampling sinusoidal functions based on token positions, these low-frequency components cannot cover a complete cycle. Therefore, for positions exceeding the pre-training sequence length, these dimensions may sample outside the training domain, leading to difficulties in generalization. Although previous works \\citep{peng2023yarn} have identified this issue, we are the first to model it from a Fourier perspective and provide a theoretical explanation.\n\n\\begin{figure*}[ht]\n    \\subfloat[Accuracy on Passkey Retrieval (higher is better)]{\n        \\includegraphics[width=\\textwidth]{figures/c4_512_downstream_extra_1024.pdf}\n    }\n    \\hfill\n    \\subfloat[Perplexity on C4 (lower is better)]{\n        \\includegraphics[width=\\textwidth]{figures/c4_512_ppl_extra_1024.pdf}\n    }\n    \\caption{Effectiveness of FoPE in length extrapolation. Starting point models trained with a maximum sequence length of 512 are extrapolated using YARN and FoPE on a corpus with a maximum sequence length of 1024.}\n    \\label{fig:length-extrapolation}\n\\end{figure*}\n\n\\section{Fourier Position Embedding}\nTo mitigate the negative affect of the non-ideal frequency-domain properties in LMs, we propose \\textit{\\textbf{Fourier Position Embedding (FoPE)}} to modify frequency-domain properties of attention:\n\n\\textbf{Treating Each Dimension as Multi-Frequency.} Although Linear Layers and Activation Functions bring serious Spectrum Leakage and Spectrum Distortion, they are crucial for enhancing expressive capacity. Therefore, we keep these modules unchanged but focus on modifying how attention processes information within each dimension.\n\nTo achieve this, we replace the single frequency in each dimension with Fourier Series:\n\\begin{equation}\n    h_m(n) = H_m(n)(e^{i\\omega_m n}+\\sum\\limits_{\\omega}a_{\\omega} e^{i\\omega n})\n\\end{equation}\nwhere $a_\\omega<1$ because $\\omega_m$ is the dominant frequency. This allows attention modules to capture multi-frequency information in each dimension. \n\nWe initialize vector $\\{\\omega_m\\}$ as same as RoPE, and initialize vector $\\{\\omega\\}$ and matrix $\\{a_\\omega\\}$ based on the analysis in Sec \\ref{subsec:spectrum_damage}: \nFor $\\{\\omega\\}\\in\\mathbb{R}^D$, we make sure $M \\leq D$ so that $\\{\\omega_m\\}\\subseteq\\{\\omega\\}$, and the other frequencies can be sampled within $[0, \\pi]$ in any distribution.\nFor $\\{a_\\omega\\}\\in\\mathbb{R}^{D \\times M}$, we initialize it with $N(0,\\sigma)$ based on the hypothesis that the Spectrum Damage obeys the similar distribution as the Linear Layers. The coefficients for the real and imaginary part of the frequency are sampled separately in our implementation, which can also use the same coefficient.\nThe $D$ and $\\sigma$ are kept as hyper-parameters to be adjusted.\n\n\\textbf{Zero-out Undertrained Frequencies.} As analyzed in Sec \\ref{subsec:undertrained_frequency}, the inadequate training of extremely-low frequencies $\\omega_m<\\frac{2\\pi}{N}$ impairs the frequency-domain properties of attention. Thus, we define the floor frequency as $\\omega_l=\\frac{2\\pi}{N}$, and clip the frequencies under the floor frequency to zero. \n\nWe choose zero as the substitute because the zero-frequency component can represent any period, making it easier to train and ensuring stable periodic extensions. Also, since the zero-frequency component has the longest wavelength and typically carries the most information, this substitution does not compromise the length generalization or hinder model fitting.\n\n\\textbf{Overall function of FoPE} can be formalized as:\n\\begin{equation}\n    h_m(n)=H_m(n)f(\\omega_m)\n\\end{equation}\n\\begin{equation}\n    f(\\omega_m) = \n    \\left\\{\n        \\begin{array}{cc}\n            1 & ,\\omega_m < \\omega_l \\\\\n            e^{i\\omega_m n}+\\sum\\limits_{\\omega}a_{\\omega} e^{i\\omega n} & ,\\omega_m \\geq \\omega_l\n        \\end{array}\n    \\right.\n\\end{equation}\nwhich treats each dimension either as a Fourier Series or as a zero-frequency component.\n\n\\textbf{Implementation of FoPE} can be easily achieved with a weight matrix $W^F\\in\\mathbb{R}^{D \\times (M-M_0)}$, where $M_0$ is the number of zero-frequency components in each head (details in \\ref{appendix:implementation}). This matrix maps the coefficients of all frequencies to a Fourier Series for each dimension. \nSince the zero-frequency sinusoidal function does not affect the original hidden states, the output dimension is less than the dimension of each head. \nTo introduce more diversity and better simulate the randomness of the Spectrum Damage, we assign separate weights for different heads, as well as for the cosine and sine functions. In our implementation, gradients are not required for these matrices, so FoPE adds negligible memory and computation overhead compared to RoPE.\n\n\\section{Experiments}\nTo demonstrate the effectiveness of FoPE as both a position embedding and an extrapolation method, we conduct experiments during pre-training (Sec. \\ref{subsec:pre-training}) and fine-tuning (Sec. \\ref{subsec:fine-tuning}). Additionally, we perform ablation studies to analyze the impact of hyperparameters on FoPE (Sec. \\ref{subsec:ablation}) and analysis to demonstrate the necessity to zero-out undertrained components (Sec. \\ref{subsec:analysis}).\n\n\\subsection{Basic Settings}\nWe mainly consider two metrics: perplexity for pre-training and accuracy on Passkey Retrieval.\n\n\\textbf{Perplexity} quantifies how well a language model predicts a sequence of words or tokens. A lower perplexity indicates the model is more confident and accurate in its predictions.\n\n\\textbf{Accuracy on Passkey Retrieval} \\citep{mohtashami2023landmark} measures the models' ability in retrieving a short passkey (i.e., a five-digit number) from a large context full of meaningless text. \nWe conduct this evaluation based on the implementation from \\citep{peng2023yarn}. During evaluation, the passkey is randomly positioned at uniformly distributed locations within the context. For each context length, we test for 1000 trials to ensure the positions sampled are sufficiently dispersed.\n\nWe conduct experiments with the OLMo \\citep{groeneveld2024olmo} framework and consider different scale models having 60M, 180M, 1.2B parameters. \n\n\\subsection{Length Generalization after Pre-Training}\n\\label{subsec:pre-training}\nWe consider two settings to evaluate both the intra-domain and out-of-domain generalization: \n\n\\textbf{Setting 1:} We train models with a 10B-tokens subset of C4 \\citep{raffel2020exploring} and evaluate them in a validation set from C4. \n\n\\textbf{Setting 2:} We train models with $\\sim$5B tokens from Gutenberg Books \\citep{gutenbergbooks} and evaluate them in the same validation set as Setting 1. In this setting, the language distribution is different between the validation set and the training set, which can further evaluate the generalization ability of different methods.\n\n\\textbf{Results of Perplexity.} (See Fig \\ref{fig:c4_512}.b \\& \\ref{fig:books_512}) In both settings, FoPE shows a significant advantage over RoPE. But FoPE is slightly worse than ALiBi, as there is an issue when ALiBi meets this training corpus, which is also mentioned in other papers \\citep{peng2023yarn, chen2024clex}. On the one hand, the corpus in C4 and Books mainly have short-distance dependency, thus the information from a short context window is enough for the prediction of almost all tokens. On the other hand, AliBi uses linear declined attention to eliminate long-distance information, and only pays attention to short-distance dependency. Based on these two reasons, ALiBi does not have any decline in perplexity as the context length increases. \n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/books_512_overall.pdf}\n    \\caption{Training with max\\_seq\\_length=512 on Gutenberg Books and evaluating on a validation set of C4, FoPE also demonstrates its ability to generalize across different data distributions.}\n    \\label{fig:books_512}\n\\end{figure}\n\n\\begin{figure*}[t]\n    \\subfloat[Ablation for different sub-methods]{\n        \\includegraphics[width=0.32\\textwidth]{figures/books_512_ablation_scale_method.pdf}\n        \\label{subfig:ablation-sub-methods}\n    }\n    \\hfill\n    \\subfloat[Ablation for different $\\sigma$]{\n        \\includegraphics[width=0.32\\textwidth]{figures/books_512_ablation_var.pdf}\n        \\label{subfig:ablation-sigma}\n    }\n    \\hfill\n    \\subfloat[Ablation for different $D$]{\n        \\includegraphics[width=0.32\\textwidth]{figures/books_512_ablation_dim.pdf}\n        \\label{subfig:ablation-D}\n    }\n    \\caption{Ablation Studies. (a)(b) evaluate $\\text{PPL Ratio}=\\text{PPL}_{\\text{c4}}/\\text{PPL}_{\\text{books}}$, (c) evaluate accuracy on Passkey.}\n\\end{figure*}\n\n\\textbf{Results of Passkey.} (See Fig \\ref{fig:c4_512}.a) In this task, FoPE demonstrates a significant advantage over both RoPE and ALiBi. RoPE’s accuracy drops sharply to zero at twice the training length and remains at zero for longer sequences. ALiBi shows a linear decline in accuracy, further illustrating that its linearly declining attention is unable to capture information from long distances. In contrast, FoPE maintains stable retrieval accuracy for passkeys at any position, demonstrating a strong ability to extract subtle information from long sequences.\n\n\\subsection{Length Generalization after Fine-Tuning}\n\\label{subsec:fine-tuning}\nBeyond the use of positional embeddings during the pre-training phase, several post-pre-training extrapolation methods \\citep{peng2023yarn, chen2024clex} have been proven critical for enhancing length generalization. Thus, we investigate two key aspects of FoPE: 1) whether existing extrapolation methods are also effective for FoPE; 2) whether FoPE can enable extrapolation on RoPE-based models, thereby allowing seamless integration with existing open-source models. In this sub-experiment, we select a representative extrapolation method, YARN \\citep{peng2023yarn}, as our baseline. We fine-tune the last checkpoint from pre-training for $\\sim$ 1B tokens in this setting.\n\n\\textbf{Results (See Fig \\ref{fig:length-extrapolation}).} Compared to RoPE+YARN, FoPE+YARN achieves significantly better length generalization performance, as demonstrated by lower perplexity on the C4 dataset and higher accuracy in the Passkey Retrieval task. Moreover, FoPE outperforms YARN in length extrapolation for both RoPE-based and FoPE-based models. These findings underscore the effectiveness and practical utility of FoPE, which holds the potential to enhance all RoPE-based open-source models.\n\n\\subsection{Ablation Studies}\n\\label{subsec:ablation}\nWe also conduct ablation studies on various hyper-parameters to observe their effects on our algorithm.\nConsidering the consistent performance of FoPE across different parameter scales, we only evaluate the 60M models in ablation studies. \n\n\\textbf{Both sub-methods of FoPE are useful} (See Fig \\ref{subfig:ablation-sub-methods}). FoPE is constitutive of two parts, called \\textit{Fourier Series (FS)} and \\textit{Clip Floor to Zero (CF)}. Although these two sub-methods are both useful for length generalization, combining them together brings a more significant improvement. On one hand, FS contributes more to length generalization, which demonstrates that the Spectrum Damage have a significant influence on length generalization. On the other hand, CF contributes more to fitting the current dataset and sequence length, which implies the zero-frequency component is the most informative and indispensable component.\n\n\\textbf{Increasing the dimension of attention heads is more beneficial than increasing the number of attention heads or layers} (See Fig \\ref{subfig:ablation-sub-methods}). More dimensions introduce more frequency components, making attention more robust to Spectral Damage. In contrast, adding more attention heads and layers aggravates Spectrum Damage, which diminishes the benefits of expanding the parameter scale.\n\n\\textbf{Variance $\\sigma$ of $\\{a_\\omega\\}$} (See Fig \\ref{subfig:ablation-sigma}). We keep $D=16$ to only evaluate $\\sigma$'s influence. By grid searching $\\sigma$ from 0 to 0.5, we find that setting $\\sigma=0.3$ for 60M model can obtain the best perplexity, especially for long context length. The best $\\sigma$ implies the estimated strength of Spectrum Damage in 60M models, and the estimation may become larger as the parameter scale increases.\n\n\\textbf{Number $D$ of $\\{\\omega\\}$.} We keep $\\sigma=0.3$ to only evaluate $D$'s influence. By grid searching $\\sigma$ from 16 to 128, we find that $D$ does not significantly influence the perplexity, but it is important for Passkey Retrieval. Setting $D=64$ can obtain the best accuracy for Passkey Retrieval. The best $D$ is the estimated number of strong enough noisy components of each model, and this number may become larger as the parameter scale increases. The harmonic frequencies tend to be weaker than the base frequencies, and this phenomenon is more significant to the higher-order harmonics. Thus, there are limited noisy frequency components that have enough intensity to disturb the passing of the base wave, paying attention to not important components hinders the effectiveness of the model.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/qk.pdf}\n    \\caption{Average activations of $q$,$k$ vectors.}\n    \\label{fig:qk}\n\\end{figure}\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/rope-pos-attn_weight-sample.pdf}\n    \\caption{The statistical average contribution of RoPE to attention scores. We sample 1k Q and K vectors from Gaussian distributions with mean of 0, 0.1, and 1.0. The long-distance decay effect weakens as the mean decreases, disappearing entirely when the mean is 0.}\n    \\label{fig:decay}\n\\end{figure}\n\n\\begin{table}[t]\n    \\centering\n    \\begin{adjustbox}{width=0.48\\textwidth}\n    \\begin{tabular}{l|c|c|c|c|c}\n        \\toprule\n        \\textbf{Sequence Length} & \\textbf{512} & \\textbf{1024} & \\textbf{2048} & \\textbf{4096} & \\textbf{8192} \\\\\n        \\midrule\n        RoPE & 5.50 & 6.01 & 6.58 & 6.99 & 7.16\\\\\n        RoPE + QK\\_Norm & \\underline{5.46} & \\underline{5.56} & \\underline{5.89} & \\underline{6.32} & \\underline{6.66} \\\\\n        \\midrule\n        RoPE-A & 5.72 & \\underline{5.86} & \\underline{6.18} & \\underline{6.46} & \\underline{6.67}\\\\\n        RoPE-A + QK\\_Norm & \\underline{5.69} & 5.89 & 6.27 & 6.59 & 6.81 \\\\\n        \\midrule\n        NoPE & 5.65 & \\underline{6.03} & \\underline{6.60} & \\underline{6.81} & \\underline{6.99} \\\\\n        NoPE + QK\\_Norm & \\underline{5.59} & 6.18 & 6.87 & 7.10 & 7.43 \\\\\n        \\bottomrule\n        \\end{tabular}\n    \\end{adjustbox}\n    \\caption{The loss of 20M toy models trained on a sequence length of 512. We consider three types of position embeddings, among which only RoPE has frequencies that cannot complete full cycles. For RoPE-A, all frequencies are adjusted from RoPE to the nearest values that exactly complete full cycles.}\n    \\label{tab:qk_norm}\n\\end{table}\n\n\\subsection{The Necessity to Zero-Out Undertrained Components}\n\\label{subsec:analysis}\nTo further explain the negative influence of the extremely low-frequency components and demonstrate the necessity to zero-out them, we also investigate their properties in the time domain.\n\nBy visualizing the numerical expectations of the $q$ and $k$ vectors in each dimension (details in Appendix \\ref{appendix:visualize-of-qk}), we observe that the absolute values of the dimensions corresponding to undertrained frequencies are noticeably greater than zero, whereas those of adequately trained dimensions are close to zero.\nOn the other side, the components that do not complete a full cycle during pre-training introduce weights with non-zero means when applied to $q$,$k$ vectors. Thus, these components introduce positional bias (shown in Fig \\ref{fig:decay}) and may adversely affect robustness to out-of-domain rotation matrix values during length generalization.\n\nTo verify this hypothesis, we normalized the $q$,$k$ vectors (enforcing a mean of 0 and variance of 1) before applying the rotation matrix to eliminate the positional bias. Based on the results in Table \\ref{tab:qk_norm}, normalization on naive RoPE showed a positive impact on length generalization. However, if all frequency components are pre-trained to complete full cycles, such normalization does not improve generalization. These experimental results validate our hypothesis.\nAdditionally, the normalization completely disrupts the decay property of RoPE's attention scores with respect to token distances. This suggests that the locality induced by long-distance decay is not critical for length generalization.\n\n\\section{Related Work}\n\\textbf{Frequency-Domain Embedding.}\nDiscrete Fourier Transform (DFT) \\citep{oppenheim1982signal} has been widely used in various areas having periodic signals \\citep{edfors2000analysis, sanchez2010cluster}.\nIn machine learning, \\citep{uteuliyeva2020fourier, lin2024cyclenet, tancik2020fourier, tamkin2020language, lee2022fnet, gillman2024fourier} employed Fourier features into neural networks to enhance performance on NLP or CV tasks.\nS4 \\citep{gu2021efficiently} also leveraged FFT and IFFT to shift its core computation into the frequency-domain, delivering more efficient computation.\n\\citep{wang2019encoding, su2024roformer} improved the attention mechanism by defining position embedding with complex number, while \"phase\" used in these methods is a typical concept in frequency-domain.\n\n\\textbf{Length Generalization.}\nDue to resource constraints, LMs are trained on limited-length corpus chunks and struggle with longer contexts \\citep{voita2023neurons, dong2024exploring, hong2024token}.\nWhile absolute position embeddings \\citep{vaswani2017attention} restrict the general use of positional information, methods as \\citep{shaw2018self, yang2019xlnet} directly adjust the attention mechanism, \nanother intuitive method is to redesign the position embedding \\citep{press2021train, chi2022kerple, kazemnejad2024impact, su2024roformer, wang2024length, choromanski2024learning}. \nAmong these, RoPE \\citep{su2024roformer} encodes positional information using the phase of complex numbers, leveraging their periodicity to enhance access to long-distance dependencies.\nSeveral training-free or fine-tuning-based methods can also improve the LM's length generalization by refining RoPE \\citep{peng2023yarn,chen2024clex,jin2024llm,lin2024mixture}. However, these works mainly address the drawbacks of RoPE in attention mechanism, neglecting the influence of other components in LMs.\n\n\\section{Conclusion}\n\nIn this paper, we analyze RoPE-based attention by modeling it in the frequency domain using \\textit{Discrete Signal Processing (DSP)} theory. \nOur analysis reveals that RoPE achieves periodic attention by implicitly performing \\textit{Non-Uniform Discrete Fourier Transform (NUDFT)}, corrupted by the non-ideal spectrum properties brought by other parts in LMs.\nWe propose \\textbf{\\textit{Fourier Position Embedding (FoPE)}} to enhances attention's periodic extension and length generalization. FoPE models each dimension as Fourier Series and zero-out inadequately-trained frequency components. Experiments demonstrate that FoPE significantly improves length generalization compared to baselines across diverse tasks and datasets. Our ablation studies and visualizations provide further support for our method and theoretical modeling.\n\n\\section{Limitations}\nOur DSP-based modeling in the frequency domain provides a novel perspective for LMs to enhance length generalization and explore broader applications, such as kv-cache compression and multi-model collaboration.\nHowever, our work only pay attention to the undesirable frequency domain properties hindering length generalization. Extending the applicability of this modeling to areas such as other tasks may require additional effort. \n\n\\section{Discussion}\nRecently, we found \\citet{barbero2024round} and \\citet{chen2024hope} also mentioned the need to zero-out the relatively low-frequency components in RoPE. While our work focus on the improvement of attention's robustness in frequency domain, with \"zeroing-out\" being only a sub-method, these works focus entirely on this sub-method driven by different motivations.\n\n\\citet{barbero2024round} visualized attention patterns to show that low-band and high-band frequencies correspond to semantic and positional information, respectively. To bring a more robust semantic channel, they truncate the low-band components to zero-frequency. \nHowever, they did not analyses the effectiveness of their method on length generalization and pre-training phase. \n\n\\citet{chen2024hope} claimed that the relatively low-frequency components bring U-shape attention pattern, which is harmful for length generalization. However, they provided insufficient evidence linking the U-shape pattern to length generalization. Also, they claimed that their method relies on high-band frequencies, which is factually incorrect, as the highest frequency in RoPE remains within the low-band of discrete frequency domain (shown in Appendix \\ref{appendix:lowpass}). \n\nIn contrast, we demonstrate that the \\textit{Spectrum Damage} is a key factor in ineffective length generalization for RoPE-based Transformer, supported by both theoretical and empirical evidence. Thus, we propose FoPE to improve the robustness of attention in the frequency domain, leading to better length generalization.\n\n\\newpage\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2302.01925v2.tex",
        "arXiv-2404.12224v2.tex",
        "arXiv-2412.17739v2.tex"
    ],
    "group_id": "group_42",
    "response": "### Summary of Research Papers on Efficient Transformers with Relative Positional Encodings\n\n#### Title: Efficient Transformers with Relative Positional Encodings: A Comparative Analysis of FourierLearner-Transformers, NoPE, and Fourier Position Embeddings\n\n#### Introduction\nTransformers have become a cornerstone in machine learning, particularly in natural language processing (NLP), computer vision (CV), and molecular modeling. The attention mechanism, which is central to Transformers, allows for the propagation of signals between tokens in the input sequence, enabling the model to capture long-range dependencies. However, the attention mechanism's quadratic time and space complexity with respect to the input length $L$ poses a significant computational bottleneck, limiting the scalability of Transformers to long sequences. This limitation has spurred extensive research into developing efficient attention mechanisms, such as sparse attention and kernelized attention, which aim to reduce the computational complexity to linear or sub-linear levels.\n\nSparse attention techniques simplify the attention mechanism by focusing on a subset of tokens, while kernelized attention, exemplified by the Performer model, approximates the attention matrix using random features to achieve linear space and time complexity. However, these efficient attention mechanisms often struggle to incorporate structural inductive bias, such as relative positional encodings (RPEs), which are crucial for enhancing the model's performance in tasks involving long-range dependencies. RPEs are used to modulate the attention scores based on the relative positions of tokens, thereby improving the model's ability to capture context-specific information.\n\nThe first paper introduces FourierLearner-Transformers (FLT), a new class of linear Transformers that can incorporate a wide range of RPE mechanisms, including both regular RPEs for sequential data and novel RPEs for geometric data embedded in higher-dimensional Euclidean spaces. The second paper explores the length generalization capabilities of Transformers without explicit position encoding (NoPE), providing a new perspective on how attention scaling can improve generalization to longer sequences. The third paper proposes Fourier Position Embedding (FoPE), a method that enhances the periodic extension of attention by modifying the frequency-domain properties of the attention mechanism.\n\n#### Main Content of Each Paper\n\n**Paper 1: Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers**\n\nThe authors of Paper 1 propose FourierLearner-Transformers (FLT), an efficient Transformer architecture that incorporates relative positional encodings (RPEs) to improve performance on tasks involving long-range dependencies. FLT uses a Fourier Transform to learn the spectral representation of the RPE function, enabling the model to approximate the RPE mask without explicitly constructing it. This approach allows FLT to maintain linear space and time complexity while incorporating a wide range of RPE mechanisms, including Gaussian mixture RPEs, shift-invariant kernel RPEs, and local RPEs. The theoretical analysis in the paper provides uniform convergence guarantees and sample complexity bounds for the RPE approximation.\n\nThe authors conduct extensive experiments on diverse tasks, including language modeling, image classification, and molecular property prediction. In language modeling, FLT with local RPE outperforms several efficient Transformer baselines, achieving a lower validation perplexity than the regular Performer. For image classification, FLT achieves the highest accuracy among the tested models. In molecular property prediction, FLT demonstrates superior performance compared to the regular Performer, indicating its capability to handle 3D input data and complex RPE masks. The authors also evaluate FLT on learnable optimizers, showing its effectiveness in capturing long-range dependencies in this setting.\n\n**Paper 2: Length Generalization of Causal Transformers without Position Encoding**\n\nPaper 2 investigates the length generalization capabilities of Transformers without explicit position encoding (NoPE). The authors find that NoPE can extend to longer sequences than models with explicit position encodings but still faces limitations in context size. They identify a connection between the failure of NoPE's generalization and the distraction of attention distributions, leading to a proposal for a parameter-efficient tuning method that searches for the best temperature hyper-parameters for attention heads. This method, called \\textit{UniformScale}, gradually increases the attention scaling factor to reconcentrate attention, thereby improving length generalization. The authors also introduce a more advanced version, \\textit{HeadScale}, which searches for a unique scaling factor for each attention head.\n\nExperiments are conducted on long sequence language modeling, synthetic tasks (such as passkey retrieval), and real-world long context tasks (such as LongBench). The results show that NoPE can achieve competitive performances with state-of-the-art length generalization algorithms, particularly when using the HeadScale method. However, NoPE still underperforms compared to RoPE-based models, especially in terms of memory usage and computational complexity when processing extremely long contexts.\n\n**Paper 3: Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization**\n\nPaper 3 provides a frequency-domain analysis of RoPE-based attention mechanisms, revealing that they are hindered by spectral damage caused by linear layers and activation functions outside attention, as well as undertrained frequency components within attention. To mitigate these issues, the authors propose Fourier Position Embedding (FoPE), which treats each dimension as a Fourier Series and clips undertrained frequencies to zero. This approach improves the robustness of attention in the frequency domain, leading to better periodic extension and length generalization.\n\nExperiments are conducted on various model scales and datasets, demonstrating that FoPE outperforms RoPE and ALiBi in terms of both perplexity and passkey retrieval accuracy. Ablation studies show that the Fourier Series (FS) sub-method of FoPE is more beneficial for length generalization, while the Clip Floor to Zero (CF) sub-method is more effective for fitting the current dataset. The authors also find that increasing the dimension of attention heads is more beneficial than increasing the number of heads or layers for mitigating spectral damage.\n\n#### Commonalities and Innovations\nAll three papers focus on enhancing the efficiency and effectiveness of Transformer architectures for tasks involving long-range dependencies. Paper 1 introduces FourierLearner-Transformers (FLT) that can incorporate a wide range of RPE mechanisms, including Gaussian mixture RPEs, shift-invariant kernels, and local RPEs. Paper 2 explores the length generalization capabilities of Transformers without explicit position encoding (NoPE) and proposes a parameter-efficient tuning method called HeadScale to improve generalization. Paper 3 provides a frequency-domain analysis of RoPE-based attention mechanisms and proposes Fourier Position Embedding (FoPE) to enhance periodic extension and length generalization.\n\nThe main innovation of Paper 1 is the use of Fourier Transform to implicitly learn the RPE mechanism, allowing FLT to handle both sequential and geometric data. Paper 2 introduces a new perspective on length generalization by isolating and eliminating the effects of explicit positional encoding, proposing HeadScale as a method to reconcentrate attention. Paper 3 offers a theoretical explanation for the limitations of RoPE-based attention mechanisms and proposes FoPE to mitigate spectral damage, thereby improving length generalization.\n\n#### Comparison of Results\nThe results of the three papers demonstrate the effectiveness of their proposed methods in various tasks and datasets. FLT achieves the lowest validation perplexity in language modeling and the highest accuracy in image classification and molecular property prediction. NoPE, when using HeadScale, shows competitive length generalization performances, particularly in passkey retrieval and LongBench tasks. FoPE outperforms RoPE and ALiBi in perplexity and passkey retrieval accuracy, especially when extrapolating to longer context lengths.\n\nHowever, there are differences in the results. FLT introduces a negligible memory overhead compared to the regular Performer and scales better in practice. NoPE, despite its competitive performance, still underperforms RoPE-based models in terms of memory usage and computational complexity for extremely long contexts. FoPE, while demonstrating significant improvements in length generalization, faces challenges in fitting the current dataset when zeroing out undertrained frequencies.\n\n#### Conclusion\nThe three papers provide valuable insights into enhancing the efficiency and effectiveness of Transformer architectures for tasks involving long-range dependencies. FLT introduces a novel approach to implicitly learning RPE mechanisms, allowing efficient handling of both sequential and geometric data. NoPE explores the length generalization capabilities of Transformers without explicit position encoding, proposing HeadScale as a parameter-efficient tuning method. FoPE offers a theoretical explanation for the limitations of RoPE-based attention mechanisms and proposes a method to enhance periodic extension and length generalization.\n\nThe main findings of the papers suggest that incorporating RPE mechanisms into efficient Transformers can significantly improve performance on tasks involving long-range dependencies. However, the methods proposed in the papers face different challenges, such as memory overhead and computational complexity for extremely long contexts. Future research directions could include further improving the efficiency of these methods, particularly in terms of memory usage and computational complexity, and extending their applicability to other tasks and datasets.\n\n#### Future Research Directions\nFuture research could focus on further optimizing the computational efficiency of FLT, NoPE, and FoPE, particularly in terms of memory usage and computational complexity for extremely long contexts. Additionally, the applicability of these methods to other tasks and datasets could be explored, such as speech recognition and genomic data modeling. The theoretical understanding of these methods could also be deepened, providing a more comprehensive explanation for their effectiveness in length generalization and long-range dependency tasks."
}