{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{GPTQ:  Accurate Post-Training Quantization for Generative Pre-trained Transformers}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nGenerative Pre-trained Transformer (GPT) models set themselves apart through breakthrough performance across  complex language modelling tasks, but also by their extremely high computational and storage costs. Specifically, due to their massive size, even inference for large, highly-accurate GPT models may require multiple performant GPUs to execute, which limits the usability of such models. While there is emerging work on relieving this pressure via model compression, the applicability and performance of existing compression techniques is limited by the scale and complexity of GPT models. In this paper, we address this challenge, and propose GPTQ, a new one-shot weight quantization method based on approximate second-order information, that is both highly-accurate and highly-efficient. Specifically, GPTQ can quantize GPT models with 175 billion parameters in approximately four GPU hours, reducing the bitwidth down to 3 or 4 bits per weight, with negligible accuracy degradation relative to the uncompressed baseline. Our method more than doubles the compression gains relative to previously-proposed one-shot quantization methods, preserving accuracy, allowing us for the first time to execute an 175 billion-parameter model inside a single GPU. We show experimentally that these improvements can be leveraged for end-to-end inference speedups over FP16, of around 2x when using high-end GPUs (NVIDIA A100) and 4x when using more cost-effective ones (NVIDIA A6000). The implementation is available at \\url{https://github.com/IST-DASLab/gptq}.\n\\end{abstract}\n\n\\section{Introduction}\n\nPre-trained generative models from the Transformer~\\cite{vaswani2017attention} family, commonly known as GPT or OPT~\\cite{radford2019language, brown2020language, zhang2022opt}, have shown breakthrough performance for complex language modelling tasks, leading to massive academic and practical interest. One major obstacle to the usability of such models is their computational and storage cost, which ranks among the highest for known models. For instance, the best-performing model variants, e.g. GPT3-175B, have in the order of 175 billion parameters and require tens-to-hundreds of GPU-years to train~\\cite{zhang2022opt}. Even the simpler task of inferencing over a pre-trained model, which is our focus in this paper, is highly challenging: for example, the parameters of GPT3-175B occupy 326GB of memory (counting in multiples of 1024) when stored in an already-compact float16 format. This exceeds the memory capacity of even the highest-end single GPUs, and thus inference must be performed using more complex and expensive setups, such as multi-GPU deployments. \n\nA standard approach to eliminating these overheads is \\emph{model compression}, e.g.~\\cite{hoefler2021sparsity, gholami2021survey}. Yet, surprisingly little is known about compressing such models for inference. \nOne reason is that more complex, but accurate methods for low-bitwidth quantization or model pruning usually require \\emph{model retraining}, which is extremely expensive for billion-parameter models. \nAlternatively, \\emph{post-training} methods~\\cite{nagel2020up, wang2020towards, hubara2020improving, nahshan2021loss}, which compress the model in one shot, without retraining, would be very appealing in this setting. \nUnfortunately, the more accurate variants of such methods~\\cite{li2021brecq,hubara2021accurate,frantar2022obc} are complex, and do not scale to billions of parameters~\\cite{yao2022zeroquant}. \nTo date, only basic variants of round-to-nearest quantization~\\cite{yao2022zeroquant, dettmers2022llm} have been applied at the scale of GPT-175B; while this works well for low compression targets, e.g., 8-bit weights, they fail to preserve accuracy at higher rates. It therefore remains open whether one-shot \\emph{post-training quantization} to higher compression rates is generally-feasible. \n\n\\begin{figure}[h]\n    \\centering\n    \\begin{subfigure}{0.45\\linewidth}\n        \\centering\n        \\includegraphics[width=\\linewidth, height=5.5cm]{opt4-abs.pdf}\n    \\end{subfigure}~~~~~\n    \\begin{subfigure}{0.45\\linewidth}\n        \\centering\n        \\includegraphics[width=\\linewidth, , height=5.5cm]{bloom3-abs.pdf}\n    \\end{subfigure}\n    \\caption{Quantizing OPT models to 4 and BLOOM models to 3 bit precision, comparing GPTQ with the FP16 baseline and round-to-nearest (RTN)~\\cite{yao2022zeroquant, dettmers2022llm}.\\vspace{-1.5em}}\n    \\label{fig:intro-comparison}\n\\end{figure}\n\\vspace{10pt}\n\n\\paragraph{Contribution.} In this paper, we present a new post-training quantization method, called GPTQ\\footnote{This combines the name of the GPT model family with the abbreviation for post-training quantization (PTQ).}, which is efficient enough to execute on models with hundreds of billions of parameters in at most a few hours, \nand precise enough to compress such models to 3 or 4 bits per parameter without significant loss of accuracy. \nGPTQ is the first method to leverage approximate second-order (Hessian) information at this scale, and can be implemented extremely efficiently on modern GPUs. \nFor illustration, GPTQ can quantize the largest publicly-available models, OPT-175B and BLOOM-176B, in approximately four GPU hours, with minimal increase in perplexity, known to be a very stringent accuracy metric. In addition, we develop an execution harness which allows us to execute the resulting compressed models efficiently for generative tasks. \nSpecifically, we are able to run language generation on the compressed OPT-175B model for the first time on a single NVIDIA A100 GPU, or using only two more cost-effective NVIDIA A6000 GPUs. We also implement bespoke GPU kernels which are able to leverage compression for faster memory transfer, \nresulting in speedups of $\\approx 2 \\times$ when using A100 GPUs, and $4\\times$ when using A6000 GPUs.\n\nAt a high level, our work tackles a standard problem in the low-precision quantization literature:  \nfor each layer, we wish to identify an assignment of standard-precision weights to quantization levels, which minimizes the discrepancy between the output of the original layer and that of the compressed one~\\cite{nagel2020up, li2021brecq, hubara2021accurate, yao2022zeroquant, frantar2022obc}, which we approach at unprecedented scale. Usually, this discrepancy is measured in $\\ell_2$-distance on a small amount of calibration data. \nOnce such accurate layer-wise weight assignments are found, the layers can be ``stitched together'' to reconstruct the quantized model. \nThis layer-wise assignment problem is the basis for most state-of-the-art quantization schemes, and is approached via increasingly-sophisticated solvers, using integer programming~\\cite{hubara2020improving} or second-order approximations~\\cite{li2021brecq, frantar2022obc}. However, existing approaches simply do not scale to large models from the GPT family~\\cite{yao2022zeroquant}.\n\nThe GPTQ algorithm starts from a similar strategy to the recently-proposed Optimal Brain Quantization (OBQ) layer-wise solver~\\cite{frantar2022obc}.  \nThe OBQ approach quantizes weights \\emph{one-at-a-time}, always updating the remaining unquantized weights to minimize the layer's output error. \nBoth the choice of the next weight to quantize and the weight update leverage second-order information, which is reasonable to work with given that the output error is quadratic, and the layer Hessian is thus constant. \nYet, this approach is impractical at GPT scale: we estimate that quantizing GPT-175B using this approach would take at least 6 months of computation, even if one completely ignores memory constraints. \n\nIn this context, the first key idea behind our approach is that there exists a way to quantize weights \\emph{in large blocks} that is orders of magnitude more efficient relative to OBQ, but that can still leverage second-order information so that it stays \\emph{highly accurate}. \nConcretely, central to GPTQ is a new second-order quantization algorithm with per-layer runtime $O( d_\\text{col}^2 \\cdot \\max \\{ d_\\text{row} , d_\\text{col} \\})$, \nwhere $d_\\text{row}$ and $d_\\text{col}$ are the layer matrix's row and column dimensions, respectively. \nAsymptotically, this is faster than OBQ by a factor of $\\Theta(\\text{min} \\, \\{d_\\text{row}, d_\\text{col}\\})$, which is often around $1000\\times$ in practice.\nYet, this algorithmic insight is not sufficient for a fast implementation: we additionally resolve a number of practical barriers, including the low compute-to-memory ratio of the above algorithm, and numerical stability issues when working with matrix inverses at this scale.\n\nWe implemented the resulting GPTQ algorithm in Pytorch~\\cite{paszke2019pytorch}, and used it to quantize publicly-available models from the OPT~\\cite{zhang2022opt} and BLOOM~\\cite{laurencconbigscience} model families, counting from 125M to 176B parameters. \nOur results show that, both for standard perplexity tasks, e.g., WikiText2~\\cite{wikitext103} or C4~\\cite{C4} and for standard zero-shot tasks, e.g. LAMBADA~\\cite{paperno2016lambada}, our method is able to compress models down to 4 or even 3 bits per weight with negligible loss of accuracy. We are even able to compress models accurately down to around 2 bits per weight when quantizing at lower granularity. \nMoreover, GPTQ compresses the largest models in approximately 4 GPU hours, and can execute on a single GPU.  \nAs illustrated in Figure~\\ref{fig:intro-comparison}, relative to prior work, GPTQ is the first method to reliably compress LLMs to 4 bits or less, more than doubling compression at minimal accuracy loss, and allowing for the first time to fit an OPT-175B model for inference inside a single GPU.\n\nTo realize speedups in practice, we observe that our method particularly benefits generative inference, which is one of the most interesting use-cases for GPT models. Specifically, generative inference produces outputs in token-by-token fashion, in an autoregressive manner. Thus, the method cannot benefit from batching, and so most of the execution reduces to matrix-vector products, which have low arithmetic intensity and put particular stress on the memory subsystem, as it must transfer the parameter matrices between main memory and the memory of a computational accelerator (e.g., GPU or IPU). \nBy quantizing the model, we allow weight matrices to fit into the faster accelerator-local memory, leading to speedup and usability improvements. \nWe illustrate this technique by implementing a fast  GPU kernel, \nwhich allows us to maintain weights in quantized format, ``decoding'' them layer-by-layer at execution time. \nWe show that this approach can result in end-to-end speedups of $1.9 - 4\\times$ when executed on modern GPUs. \n\nTo our knowledge, we are the first to show that extremely accurate language models with hundreds of billions of parameters can be quantized to $2.5 - 4$ bits per component on average: prior \\emph{post-training methods} only remain accurate at 8 bits~\\cite{yao2022zeroquant, dettmers2022llm}, while prior \\emph{training-based} techniques have only tackled  models that are smaller by one to two orders of magnitude~\\cite{wu2022extreme, tao2022compression}. This high degree of compression may appear unsurprising, as these networks are overparametrized; yet, as we discuss in our detailed analysis of results, compression induces non-trivial tradeoffs between the accuracy of the language modeling (perplexity), bit-width, and the size of the original model.\n\nWe hope that our work will stimulate further research in this area, and can be a further step towards making these models available to a wider audience. \nIn terms of limitations, our method currently does not provide speedups for the actual multiplications, due to the lack of direct hardware support for mixed-precision operands (e.g. FP16 $\\times$ INT4) on mainstream architectures. Moreover, our current results do not include activation quantization, as they are not a significant bottleneck in our target scenarios; however, this can be supported using complementary techniques~\\cite{dettmers2022llm, yao2022zeroquant}. \n\n\\section{Related Work}\n\nQuantization methods fall broadly into two categories: quantization during training, and post-training methods. The former methods quantize models during typically extensive retraining and/or finetuning, using some approximate differentiation mechanism for the rounding operation \\cite{gholami2021survey, nagel2021white}. By contrast, post-training (``one-shot'') methods quantize a pretrained model using modest resources, typically a few thousand data samples and a few hours of computation. Post-training approaches are particularly interesting for huge models, for which full model training or even finetuning can be extremely resource intensive. \nWe thus focus on this scenario here. \n\n\\paragraph{Post-training Quantization.} \nHistorically, post-training methods have focused on vision models: for instance, quantizing the ResNet50 model~\\cite{he2016deep} is as a standard benchmark. \nUsually, accurate methods operate by quantizing either individual layers, or small blocks of consecutive layers. (See Section \\ref{sec:layerwise-quantization} for more details.) \nThe AdaRound method~\\cite{nagel2020up} computes a data-dependent rounding by annealing a penalty term, which encourages weights to move towards grid points corresponding to quantization levels. BitSplit~\\cite{wang2020towards} constructs quantized values bit-by-bit using a squared error objective on the residual error, while AdaQuant \\cite{hubara2021accurate} performs  optimization of the quantization levels based on straight-through estimation. \nBRECQ \\cite{li2021brecq} introduces Fisher information into the objective, and optimizes layers within a single residual block jointly. \nFinally, Optimal Brain Quantization (OBQ) \\cite{frantar2022obc} generalizes the classic Optimal Brain Surgeon (OBS) second-order weight pruning framework~\\cite{hassibi1993optimal, singh2020woodfisher, frantar2021m} to apply to quantization.\nOBQ quantizes weights one-by-one, in order of quantization error, always adjusting the remaining weights. \nWhile these approaches can produce good results for models up to $\\approx100$ million parameters in a few GPU hours, they will not scale to networks orders of magnitude larger. \nWe compare against all these methods on the classic ResNet18/ResNet50 benchmark (see Table~\\ref{tab:ptq-vision-comp}). Although it is not our goal to compete with these highly-accurate methods, GPTQ is in fact fairly competitive in terms of accuracy, while being at least an order of magnitude faster in practice than these methods, even at this model scale. \n \n\\paragraph{Large-model Quantization.} With the recent open-source releases of models like BLOOM~\\cite{laurencconbigscience} or OPT-175B~\\cite{zhang2022opt}, researchers have started to develop affordable methods for compressing such giant networks for inference. \nTo our knowledge, all existing works---ZeroQuant~\\cite{yao2022zeroquant}, LLM.int8()~\\cite{dettmers2022llm}, and nuQmm~\\cite{park2022nuqmm}---employ relatively simple quantization schemes based on rounding to the nearest (RTN) quantization level. This simple approach has the advantage of maintaining acceptable runtimes for very large models. \nZeroQuant further proposes layer-wise knowledge distillation, similar to AdaQuant, but the largest model it can apply this approach to has only 1.3 billion parameters. At this scale, ZeroQuant already takes $\\approx 3$ hours of compute: by our conservative estimate, ZeroQuant would require at least 500 hours to quantize OPT-175B. \nLLM.int8() observes that \\emph{activation outliers} in a few feature dimensions break the quantization of larger models, and proposes to fix this problem by keeping those dimensions in higher precision. Lastly, nuQmm develops efficient GPU kernels for a binary-coding based quantization scheme, which can, at low bit-width, bring significant speedups over full precision, when executing single layers in isolation. \n\nRelative to this line of work, we show that a significantly more complex and accurate weight quantizer can be implemented efficiently at large model scale. \nSpecifically, GPTQ more than doubles the amount of compression relative to these prior techniques, at negligible accuracy loss, as it is the first method to achieve accurate quantization to between $2.5$ to $4$ bits per weight, which brings significant practical gains.  \n\n\\section{Methods}\n\n\\subsection{Layer-Wise Quantization}\n\\label{sec:layerwise-quantization}\n\nAt a high level, our method follows the structure of state-of-the-art post-training quantization methods \\cite{nagel2020up, wang2020towards, hubara2021accurate, frantar2022obc}, by performing quantization  layer-by-layer, solving a corresponding reconstruction problem for each layer.\nConcretely, let $\\mathbf{W}$ be the weights corresponding to a linear layer $\\ell$ and let $\\mathbf{X}$ denote the layer input corresponding to a small set of $m$ data points running through the network. \nThen, the objective is to find a matrix of quantized weights $\\mathbf{\\widehat{W}}$ which minimizes the squared error, relative to the full precision layer output. Formally, this can be restated as \n\\begin{equation}\n    \\label{eq:layerwise-quantization}\n    \\text{argmin}_{\\mathbf{\\widehat{W}}} \\, ||\\mathbf{W} \\mathbf{X} - \\mathbf{\\widehat{W}} \\mathbf{X}||_2^2.\n\\end{equation}\nFurther, similar to \\cite{nagel2020up, li2021brecq, frantar2022obc}, we assume that the quantization grid for $\\mathbf{\\widehat{W}}$ is fixed before the process, \nand that individual weights can move freely as in \\cite{hubara2021accurate, frantar2022obc}.\n\n\\subsection{Optimal Brain Quantization (OBQ)}\n\nOur approach builds on the recently-proposed Optimal Brain Quanization (OBQ) method~\\cite{frantar2022obc} for solving the layer-wise quantization problem defined above, \nto which we perform a series of major modifications, which allow it to scale to large language models, providing more than \\emph{three orders of magnitude} computational speedup in practice.\nTo aid understanding, we first briefly summarize the original OBQ method.\n\nThe OBQ method starts from the observation that Equation (\\ref{eq:layerwise-quantization}) can be written as the sum of the squared errors over each row of $\\mathbf{W}$. \nThen, OBQ handles each row $\\mathbf{w}$ independently, quantizing one weight at a time while always updating all not-yet-quantized weights, in order to compensate for the error incurred by quantizing a single weight. \nThe corresponding objective is a quadratic with Hessian $\\mathbf{H}_F = 2\\mathbf{X}_F\\mathbf{X}_F^\\top$, where $F$ denotes the set of remaining full-precision weights. Then, the greedy-optimal weight to quantize next, which we denote by $w_q$, and the corresponding optimal update of all weights in $F$, denoted by $\\boldsymbol{\\delta}_F$, are given by the following formulas, where $\\text{quant}(w)$ rounds $w$ to the nearest value on the quantization grid:\n\\begin{equation}\n    \\label{eq:obs-quant}\n    w_q = \\text{argmin}_{w_q} \\, \\frac{(\\text{quant}(w_q) - w_q)^2}{[\\mathbf{H}_F^{-1}]_{qq}}, \\quad \\boldsymbol{\\delta}_F = - \\frac{w_q - \\text{quant}(w_q)}{[\\mathbf{H}_F^{-1}]_{qq}} \\cdot (\\mathbf{H}_F^{-1})_{:, q}.\n\\end{equation}\nOBQ quantizes weights iteratively using these two equations, until all the weights of $\\mathbf{w}$  are quantized.\nThis is done efficiently, avoiding expensive full recomputations of $\\mathbf{H}^{-1}$, by removing the $q$th row and column of $\\mathbf{H}$, which is necessary after quantizing $w_q$, directly in the inverse via one step of Gaussian elimination. Namely, the updated inverse is given by the formula\n\\begin{equation}\n    \\label{eq:inv-update}\n    \\mathbf{H}_{-q}^{-1} = \\Big(\\mathbf{H}^{-1} - \\frac{1}{[\\mathbf{H}^{-1}]_{qq}} \\mathbf{H}^{-1}_{:, q} \\mathbf{H}^{-1}_{q, :} \\Big)_{-q}.\n\\end{equation}\n\nThis method comes with a vectorized implementation, handling multiple rows of $\\mathbf{W}$ in parallel. \nEventually, the algorithm can achieve reasonable runtimes on medium-sized models: for instance, it can fully quantize the  ResNet50 model (25M parameters) in $\\approx 1$ hour on a single GPU, which is roughly in line with other post-training methods achieving state-of-the-art accuracy~\\cite{frantar2022obc}. \nHowever, the fact that OBQ's runtime for a $d_\\text{row} \\times d_\\text{col}$ matrix $\\mathbf{W}$ has \\emph{cubic} input dependency $O(d_\\text{row} \\cdot d_\\text{col}^3)$ means that applying it to models with billions of parameters is impractical.\n\n\\subsection{The GPTQ Algorithm}\n\\label{sec:gptq}\n\n\\paragraph{Step 1: Initial Insight.} As explained in the previous section, OBQ quantizes weights in greedy order, i.e. it always picks the weight which currently incurs the least additional quantization error. Interestingly, we find that, while this natural strategy does indeed seem to perform very well, its improvement over quantizing the weights in arbitrary order is generally small, in particular on large, heavily-parametrized layers. \nWe speculate that this is because the slightly lower number of quantized weights with large individual error is balanced out by those weights being quantized towards the end of the process, when only few other unquantized weights that can be adjusted for error compensation remain. As we will now discuss, the simple insight that \\emph{any fixed order may perform well}, especially on large models, has interesting practical ramifications.\n\n\\begin{wrapfigure}{r}{0.49\\textwidth}\n    \\vspace{-10pt}\n  \\begin{center}\n    \\includegraphics[width=\\linewidth]{imgs/GPTQ.pdf}\n  \\end{center}\n      \\caption{ Illustration of the GPTQ quantization procedure. Blocks of consecutive \\emph{columns} (bolded) are quantized at a given step, using the inverse Hessian information stored in the Cholesky decomposition, and the remaining weights (blue) are updated at the end of the step. The quantization procedure is applied recursively inside each block: the white middle column is currently being quantized.}\n    \\vspace{-10pt}\n\\label{fig:illustration}\n\\end{wrapfigure}\n\nThe original OBQ method quantizes rows of $\\mathbf{W}$ independently, in a specific order defined by the corresponding errors. \nBy contrast, we will aim to quantize the weights of \\emph{all rows in the same order}, and will show that this typically yields results with a final squared error that is similar to the original solutions. As a consequence, the set of unquantized weight indices $F$ and therefore the corresponding Hessian inverse $\\mathbf{H}_F^{-1}$ will be always the same for all rows. \n(See Figure~\\ref{fig:illustration} for an illustration. In more detail, the latter is due to the fact that $\\mathbf{H}_F$ depends only on the layer inputs $\\mathbf{X}_F$, which are the same for all rows, and not on any weights.) We can leverage this to perform the update of $\\mathbf{H}_F^{-1}$ given by Equation~(\\ref{eq:inv-update}) only $d_\\text{col}$ times, once per column, rather than $d_\\text{row} \\cdot d_\\text{col}$ times, once per weight. This reduces the overall runtime from $O(d_\\text{row} \\cdot d_\\text{col}^3)$ to $O(\\text{max} \\, \\{d_\\text{row} \\cdot d_\\text{col}^2, d_\\text{col}^3\\})$, i.e., by a factor of $\\text{min} \\, \\{d_\\text{row}, d_\\text{col}\\}$. \nFor larger models, this difference is of several orders of magnitude. \nHowever, before this algorithm can actually be applied to very large models in practice, two additional major obstacles remain. \n\n\\paragraph{Step 2: Achieving Fast Runtime.}\nFirst, a direct implementation of the scheme described previously will not be fast enough in practice because the algorithm has a relatively low compute-to-memory-access ratio. For example, Equation~(\\ref{eq:inv-update}) needs to update all elements of a potentially huge matrix using just a few FLOPs for each entry. Here, the implementation will be bottlenecked by the significantly lower memory bandwidth of modern GPUs, relative to their computational power.\n\nFortunately, this problem can be resolved by the following observation: The final rounding decisions for column $i$ are only affected by updates performed on this very column, and so updates to later columns are irrelevant at this point in the process. This makes it possible to ``batch'' updates together, thus achieving much better computational utilization. Concretely, we apply the algorithm to $B = 128$ columns at a time, keeping updates contained to those columns and the corresponding $B \\times B$ block of $\\mathbf{H}^{-1}$. (Please see Figure~\\ref{fig:illustration} for an illustration.)  Only once a block has been fully processed, we perform global updates of the entire $\\mathbf{H}^{-1}$ and $\\mathbf{W}$ matrices using the multi-weight versions of Equations~(\\ref{eq:obs-quant}) and (\\ref{eq:inv-update}) given below, with $Q$ denoting a set of indices, and $\\mathbf{H}_{-Q}^{-1}$ denoting the inverse matrix with the corresponding rows and columns removed: \n\\begin{align}\n    \\boldsymbol{\\delta}_F &= -\\Big(\\mathbf{w}_Q - \\text{quant}(\\mathbf{w}_Q)\\Big)([\\mathbf{H}_F^{-1}]_{QQ})^{-1} (\\mathbf{H}_F^{-1})_{:, Q}, \\\\ \n    \\mathbf{H}_{-Q}^{-1} &= \\Big(\\mathbf{H}^{-1} - \\mathbf{H}^{-1}_{:, Q} ([\\mathbf{H}^{-1}]_{QQ})^{-1} \\mathbf{H}^{-1}_{Q, :} \\Big)_{-Q}. \\label{eq:group-inv-update}\n\\end{align}\nAlthough this strategy does not reduce the theoretical amount of compute, it effectively addresses the memory-throughput bottleneck. This provides an order of magnitude speedup for very large models in practice, making it a critical component of our algorithm.\n\n\\paragraph{Step 3: Numerical Stability.}\nThe final technical issue we have to address is given by numerical inaccuracies, which can become a major problem at the scale of existing models, especially when combined with the block updates discussed in the previous step.  \nSpecifically, it can occur that the matrix $\\mathbf{H}_F^{-1}$ becomes indefinite, which we notice can cause the algorithm to aggressively update the remaining weights in incorrect directions, resulting in an arbitrarily-bad quantization of the corresponding layer.\nIn practice, we observed that the probability of this happening increases with model size: concretely, it almost certainly occurs for at least a few layers on models that are larger than a few billion parameters, and so it would prevent us from tackling the very largest models.  \nThe main issue is given by repeated applications of Equation~(\\ref{eq:group-inv-update}), which accumulate various numerical errors, especially through the additional matrix inversion.\n\nFor smaller models, applying dampening, that is adding a small constant $\\lambda$ (we always choose 1\\% of the average diagonal value) to the diagonal elements of $\\mathbf{H}$ appears to be sufficient to avoid numerical issues. However, larger models require a more robust and general approach.\n\nTo address this, we begin by noting that the only information required from $\\mathbf{H}_{F_q}^{-1}$, where $F_q$ denotes the set of unquantized weights when quantizing weight $q$, is in $q$'s row, or, more precisely, the elements in this row starting with the diagonal. The consequence is that we could precompute all of these rows using a more numerically-stable method without any significant increase in memory consumption. \nIndeed, the row removal via (\\ref{eq:inv-update}) for our symmetric $\\mathbf{H}^{-1}$ essentially corresponds to taking a Cholesky decomposition, except for the minor difference that the latter divides row $q$ by $\\sqrt{[\\mathbf{H}^{-1}_{F_q}]_{qq}}$. Hence, we can leverage efficient Cholesky kernels to compute all information we will need from $\\mathbf{H}^{-1}$ upfront. In combination with mild diagonal dampening, the resulting method is robust enough to execute on huge models without issues. As a bonus, using a well-optimized Cholesky kernel also yields further speedup. \n\n\\paragraph{The Full Algorithm.}\nFinally, we present the full pseudocode for our algorithm, including the optimizations discussed above. We provide an efficient Pytorch implementation at \\url{https://github.com/IST-DASLab/gptq}.\n\n\\newlength{\\commentindent}\n\\setlength{\\commentindent}{.5\\textwidth}\n\\makeatletter\n\\renewcommand{\\algorithmiccomment}[1]{\\unskip\\hfill\\makebox[\\commentindent][l]{\\textit{//~#1}}\\par}\n\\LetLtxMacro{\\oldalgorithmic}{\\algorithmic}\n\\renewcommand{\\algorithmic}[1][0]{%\n  \\oldalgorithmic[#1]%\n  \\renewcommand{\\ALC@com}[1]{%\n    \\ifnum\\pdfstrcmp{##1}{default}=0\\else\\algorithmiccomment{##1}\\fi}%\n}\n\\makeatother\n\n\\begin{algorithm}[H]\n    \\centering\n    \\caption{Quantize $\\mathbf{W}$ given inverse Hessian $\\mathbf{H}^{-1} = (2 \\mathbf{X} \\mathbf{X}^\\top + \\lambda \\mathbf{I})^{-1}$ and blocksize $B$.}\n    \\small\n    \\label{alg:trueobs-row}\n    \\begin{algorithmic}\n        \\STATE $\\mathbf{Q} \\gets \\mathbf{0}_{d_\\text{row} \\times d_\\text{col}}$ \\quad \\COMMENT{quantized output}\n        \\STATE $\\mathbf{E} \\gets \\mathbf{0}_{d_\\text{row} \\times B}$ \\quad \\COMMENT{block quantization errors}\n        \\STATE $\\mathbf{H}^{-1} \\gets \\text{Cholesky}\n        (\\mathbf{H}^{-1})$ \\COMMENT{Hessian inverse information}\n        \\FOR {$i = 0, B, 2B, \\dots$}\n            \\FOR {$j = i, \\dots, i + B - 1$}\n                \\STATE $\\mathbf{Q}_{:, j} \\gets \\text{quant}(\\mathbf{W}_{:, j})$ \\quad \\COMMENT{quantize column}\n                \\STATE $\\mathbf{E}_{:, j - i} \\gets (\\mathbf{W}_{:, j} - \\mathbf{Q}_{:, j}) \\, / \\, [\\mathbf{H}^{-1}]_{jj}$ \\COMMENT{quantization error}\n                \\STATE $\\mathbf{W}_{:, j:(i + B)} \\gets \\mathbf{E}_{:, j - i} \\cdot \\mathbf{H}^{-1}_{j, j:(i + B)}$ \\COMMENT{update weights in block}\n            \\ENDFOR\n            \\STATE $\\mathbf{W}_{:, (i + B):} \\gets \\mathbf{E} \\cdot \\mathbf{H}^{-1}_{i:(i + B), (i + B):}$ \\COMMENT{update all remaining weights}\n        \\ENDFOR\n    \\end{algorithmic}\n\\end{algorithm}\n\n\\section{Experimental Validation}\n\\label{sec:experiments}\n\n\\paragraph{Overview.} We begin our experiments by validating the accuracy of GPTQ relative to other accurate-but-expensive quantizers,  on smaller models, for which these methods provide reasonable runtimes.\nNext, we examine GPTQ's runtime scaling for very large models. \nThen, we present 3- and 4-bit quantization results for the entire BLOOM and OPT model families, evaluated via perplexity on challenging language generation tasks. \nIn addition, we show that our method is also stable for 2-bit quantization when the granularity is reduced to small blocks of consecutive weights. \nTo complement this perplexity analysis, we also evaluate the resulting quantized models on a series of standard zero-shot tasks.\nFinally, we focus on the two largest (and interesting) openly-available models, BLOOM-176B and OPT-175B, where we perform a detailed evaluation on several tasks. \nFor these models, we also present practical improvements, namely reducing the number of GPUs required for inference as well as end-to-end speedups for generative tasks.\n\n\\paragraph{Setup.} We implemented GPTQ in PyTorch \\cite{paszke2019pytorch} and worked with the HuggingFace integrations of the BLOOM~\\cite{laurencconbigscience} and OPT~\\cite{zhang2022opt} model families. We quantized all models (including the 175 billion parameter variants) \\emph{using a single NVIDIA A100 GPU} with 80GB of memory. \nOur entire GPTQ calibration data consists of 128 random 2048 token segments from the C4 dataset~\\cite{C4}, i.e. excerpts from randomly crawled websites, which represents generic text data. \nWe emphasize that this means that GPTQ does not see any task-specific data, and our results thus remain truly ``zero-shot''. \nWe perform standard uniform per-row asymmetric quantization on the min-max grid, similar to~\\cite{dettmers2022llm}.\n\nTo ensure that the entire compression procedure can be performed with significantly less GPU memory than what would be required to run the full precision model, some care must be taken. Specifically, we always load one Transformer block, consisting of 6 layers, at a time into GPU memory and then accumulate the layer-Hessians and perform quantization. Finally, the current block inputs are sent through the fully quantized block again to produce the new inputs for the quantization of the next block. Hence, the quantization process operates not on the layer inputs in the full precision model but on the actual layer inputs in the already partially quantized one. We find that this brings noticeable improvements at negligible extra cost.\n\n\\paragraph{Baselines.} Our primary baseline, denoted by RTN, consists of rounding all weights to the nearest quantized value on the same grid that is also used for GPTQ. This is currently the method of choice in all works on quantization of very large language models~\\cite{dettmers2022llm, yao2022zeroquant, park2022nuqmm}: its runtime scales well to networks with many billions of parameters since it simply performs direct weight rounding in a single pass. As we will also discuss in detail, more accurate methods, such as AdaRound~\\cite{nagel2020up} or BRECQ~\\cite{li2021brecq}, are currently far too slow for models with many billions of parameters, the main focus of this work. Nevertheless, GPTQ is competitive with such methods for small models, while scaling to massive ones.\n\n\\paragraph{Quantizing Small Models.} As a first ablation study, we compare GPTQ's performance relative to state-of-the-art post-training quantization (PTQ) methods, on ResNet18 and ResNet50, which are standard PTQ benchmarks, in the same setup as~\\cite{frantar2022obc}. \nAs can be seen in Table~\\ref{tab:ptq-vision-comp}, GPTQ performs on par at 4-bit, and slightly worse than the most accurate methods at 3-bit. At the same time, it significantly outperforms AdaQuant, the fastest amongst prior PTQ methods. \nFurther, we compare against the full greedy OBQ method on two smaller language models: BERT-base \\cite{devlin2018bert} and OPT-125M~\\cite{zhang2022opt}. \nThe results are shown in Appendix Table~\\ref{tab:obq-comparison}. \nAt 4 bits, both methods perform similarly, and for 3 bits, GPTQ surprisingly performs slightly better. \nWe suspect that this is because some of the additional heuristics used by OBQ, such as early outlier rounding, might require careful adjustments for optimal performance on non-vision models. \nOverall, GPTQ appears to be competitive with state-of-the-art post-training methods for smaller models, while taking only $< 1$ minute rather than $\\approx 1$ hour. \nThis enables scaling to much larger models.\n\n\\begin{table}[h]\n\\begin{minipage}[t]{.45\\textwidth}\n    \\centering\n    \\vspace{0pt}\n    \\scalebox{0.9}{\n        \\begin{tabular}{|l|cc|cc|}\n        \\toprule\n        \\multirow{2}{*}{Method} & \\multicolumn{2}{c|}{RN18 -- 69.76\\%} & \\multicolumn{2}{c|}{RN50 -- 76.13\\%} \\\\\n        & 4bit & 3bit & 4bit & 3bit \\\\\n        \\midrule\n        AdaRound & 69.34 & 68.37 & 75.84 & 75.14 \\\\\n        AdaQuant & 68.12 & 59.21 & 74.68 & 64.98 \\\\\n        BRECQ & 69.37 & 68.47 & 75.88 & 75.32 \\\\\n        OBQ  & 69.56 & 68.69 & 75.72 & 75.24 \\\\\n        \\midrule\n        GPTQ & 69.37 & 67.88 & 75.71 & 74.87 \\\\\n        \\bottomrule\n    \\end{tabular}\n    }\n    \\vspace{5pt}\n    \\caption{Comparison with state-of-the-art post-training methods for vision models.}\n    \\label{tab:ptq-vision-comp}\n\\end{minipage}\n\\hfill\n\\begin{minipage}[t]{.45\\textwidth}\n    \\centering\n    \\vspace{-2pt}\n    \\includegraphics[width=0.95\\linewidth]{imgs/runtimes.pdf}    \n    \\captionof{figure}{GPTQ runtime and estimated runtimes for other methods on OPT models.}\n    \\label{fig:runtimes}\n\\end{minipage}\n\\end{table}\n\\vspace{-15pt}\n\n\\paragraph{Runtime.} Figure~\\ref{fig:runtimes} compares GPTQ's runtime for fully quantizing OPT models of varying size on a single NVIDIA A100 GPU, with runtime estimates for other post-training quantization approaches (precise runtime numbers for all models are provided in Appendix~\\ref{app:runtimes}). Concretely, we compare with methods based on the straight-through estimator (STE)~\\cite{yao2022zeroquant, hubara2021accurate}. We estimate their runtime by linearly extrapolating the results of the most efficient implementation, ZeroQuant-LKD~\\cite{yao2022zeroquant}, taking $\\approx 1$ hour per 350 million parameters.\nFurther, we estimate the runtime of rounding-based methods~\\cite{nagel2020up, li2021brecq}, which are significantly more accurate, but typically perform $10\\times$ more SGD steps. \nHence, we optimistically approximate their runtime as $10\\times$ the estimate for STE methods. \nWhile GPTQ can quantize 175 billion parameter models in $\\approx 4$ hours, this would take current STE methods about 3 weeks, and current adaptive rounding methods would need half a year. \nOur estimates are generous, as they do not consider factors such as memory constraints, and assume that the number of SGD steps is constant w.r.t. model size, although some methods, e.g.,~\\cite{yao2022zeroquant}, increase them linearly with model size. Thus, GPTQ is the first highly-accurate post-training method that can scale to extremely large models. \n\n\\paragraph{Language Generation.} \nWe begin our large-scale study by compressing the entire OPT and BLOOM model families to 3 and 4 bits per weight. \nWe then evaluate those models on several language modelling tasks including WikiText2~\\cite{wikitext103} (see Figure \\ref{fig:intro-comparison} and Appendix~\\ref{app:generation}), Penn Treebank (PTB)~\\cite{PTB} (see Tables \\ref{tab:opt-ptb} and \\ref{tab:bloom-ptb}) and C4~\\cite{C4} (see Appendix~\\ref{app:generation}). \nWe focus on these perplexity-based tasks, as they are known to be particularly sensitive to model quantization~\\cite{yao2022zeroquant}. \nOn OPT models, GPTQ clearly outperforms RTN, by significant margins. \nFor example, GPTQ loses 0.14 perplexity at 4-bit on the 175B model, while RTN drops 4.53 points, performing worse than the nearly $50\\times$ smaller full-precision 2.7B model. \nAt 3-bit, RTN collapses completely, while GPTQ can still maintain reasonable perplexity, in particular for larger models. \nBLOOM shows a similar pattern: the gaps between methods are usually smaller, indicating that this model family might be easier to quantize. \nOne interesting trend (see also Figure~\\ref{fig:intro-comparison}) is that larger models generally (with the exception of OPT-66B\\footnote{Upon closer inspection of the OPT-66B model, it appears that this is correlated with the fact that this trained model has a significant fraction of dead units in the early layers, which may make it harder to compress.}) appear easier to quantize. \nThis is good news for practical applications, as these are the cases where compression is also the most necessary.\n\n\\begin{table}[h]\n    \\centering\n    \\scalebox{0.9}{\n    \\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|}\n        \\toprule\n        OPT & Bits & 125M & 350M & 1.3B & 2.7B & 6.7B & 13B & 30B & 66B & 175B \\\\\n        \\midrule\n        Baseline & 16 & 32.55 & 26.08 & 16.96 & 15.11 & 13.09 & 12.34 & 11.84 & 11.36 & 10.33 \\\\\n        \\midrule\n        RTN & 4 & 45.01 & 31.12 & 34.14 & 22.10 & 16.09 & 15.40 & 14.17 & 274 & 15.00 \\\\\n        GPTQ & 4 & \\textbf{36.96} & \\textbf{28.85} & \\textbf{18.16} & \\textbf{15.96} & \\textbf{13.80} & \\textbf{12.58} & \\textbf{11.98} & \\textbf{11.58} & \\textbf{10.47} \\\\\n        \\midrule\n        RTN & 3 & 1.2e3 & 81.07 & 1.1e4 & 9.4e3 & 3.4e3 & 2.5e3 & 1.4e3 & 3.6e3 & 4.8e3 \\\\\n        GPTQ & 3 & \\textbf{65.18} & \\textbf{39.48} & \\textbf{26.08} & \\textbf{20.30} & \\textbf{18.45} & \\textbf{13.89} & \\textbf{12.73} & \\textbf{15.79} & \\textbf{10.92} \\\\\n        \\bottomrule\n    \\end{tabular}\n    }\n    \\vspace{5pt}\n    \\caption{OPT model family perplexity results on Penn Treebank.}\n    \\label{tab:opt-ptb}\n\\end{table}\n\n\\vspace{-1em}\n\n\\begin{table}[h]\n    \\centering\n    \\scalebox{0.9}{\n    \\begin{tabular}{|l|c|c|c|c|c|c|c|}\n        \\toprule\n        BLOOM & Bits & 560M & 1.1B & 1.7B & 3B & 7.1B & 176B \\\\\n        \\midrule\n        Baseline & 16 & 41.24 & 46.98 & 27.93 & 23.12 & 19.40 & 13.63 \\\\\n        \\midrule\n        RTN & 4 & 48.57 & 54.54 & 31.21 & 25.40 & 20.93 & 14.07 \\\\\n        GPTQ & 4 & \\textbf{44.42} & \\textbf{50.98} & \\textbf{29.70} & \\textbf{24.27} & \\textbf{20.16} & \\textbf{13.76} \\\\\n        \\midrule\n        RTN & 3 & 117 & 152 & 115 & 59.96 & 32.05 & 190 \\\\\n        GPTQ & 3 & \\textbf{65.48} & \\textbf{70.68} & \\textbf{42.23} & \\textbf{30.70} & \\textbf{24.16} & \\textbf{14.60} \\\\\n        \\bottomrule\n    \\end{tabular}\n    }\n    \\vspace{5pt}\n    \\caption{BLOOM model family perplexity results for Penn Treebank.}\n    \\label{tab:bloom-ptb}\n\\end{table}\n\\vspace{-10pt}\n\n\\paragraph{175-Billion Parameter Models.} We now focus on BLOOM-176B and OPT-175B, the largest dense openly-available models. \nTable~\\ref{tab:results-biggest-models} summarizes results across Wikitext-2, PTB and C4. \nWe observe that, at 4 bits, GPTQ models reach only $< 0.2$ lower perplexity than the full-precision versions, with a large gap to RTN results on OPT-175B. \nAt 3-bit, RTN collapses, while GPTQ is still able to maintain good performance on most tasks, losing only $0.3 - 0.5$ points for more than $5\\times$ compression. \nWe note that GPTQ's accuracy can be further improved via finer-granularity grouping/bucketing~\\cite{alistarh2016qsgd, park2022nuqmm}: with group-size 1024, GPTQ's OPT-175B 3-bit WikiText2 PPL further improves from 8.68 to 8.45. We examine the impact of group size more carefully below. \n\n\\begin{table}[h]\n    \\centering\n    \\scalebox{0.9}{\n    \\begin{tabular}{|c|c|ccc|c|ccc|c|}\n        \\toprule\n        \\multirow{2}{*}{Method} & \\multirow{2}{*}{Bits} & \\multicolumn{4}{c|}{OPT-175B} & \\multicolumn{4}{c|}{BLOOM-176B} \\\\\n        & & Wiki2 & PTB & C4 & LAMB. $\\uparrow$  & Wiki2 & PTB & C4 & LAMB. $\\uparrow$ \\\\\n        \\midrule\n        Baseline & 16 & 8.34 & 10.33 & 9.56 & 75.59 & 8.11 & 13.63 & 10.98 & 67.40 \\\\\n        \\midrule\n        RTN & 4 & 10.54 & 15.00 & 10.92 & 71.34 & 8.37 & 14.07 & 11.26 & 66.70 \\\\\n        GPTQ & 4 & \\textbf{8.37} & \\textbf{10.47} & \\textbf{9.67} &\n        \\textbf{76.80} &\n        \\textbf{8.21} & \\textbf{13.76} & \\textbf{11.06} &\n        \\textbf{67.71} \\\\\n        \\midrule\n        RTN & 3 & 7.3e4 & 4.8e4 & 4.4e4 & 0 & 571. & 190. & 309 & 0.17 \\\\\n        GPTQ & 3 & \\textbf{8.68} & \\textbf{10.92} & \\textbf{9.98} & \n        \\textbf{76.19} &\n        \\textbf{8.64} & \\textbf{14.60} & \\textbf{11.45} &\n        \\textbf{65.10} \\\\\n        \\midrule\n        GPTQ & 3G & 8.45 & 10.69 & 9.84 & 77.39 & 8.35 & 14.01 & 11.22 & 67.47 \\\\\n        \\bottomrule\n    \\end{tabular}\n    }\n    \\vspace{5pt}\n    \\caption{Results summary for OPT-175B and BLOOM-176B; 3G indicates 3-bit results with group-size 1024.}\n    \\label{tab:results-biggest-models}\n\\end{table}\n\\vspace{-10pt}\n\n\\paragraph{Practical Speedups.} \nFinally, we focus on practical applications. \nAs an interesting use-case, we focus on the OPT-175B model: quantized to 3 bits, this model takes approximately 63GB of memory, including the embeddings and the output layer, which are kept in full FP16 precision. \nAdditionally, storing the complete history of keys and values for all layers, a common optimization for generation tasks, consumes another $\\approx 9$GB for the maximum of 2048 tokens. Hence, we can actually fit the entire quantized model into a single 80GB A100 GPU, which can be executed by dynamically dequantizing layers as they are required during inference. (The model would not fully fit using 4 bits.) \nFor reference, standard FP16 execution requires 5$\\times$80GB GPUs, and the state-of-the-art 8-bit LLM.int8() quantizer~\\cite{dettmers2022llm} requires 3 such GPUs. \n\nWe focus on language generation, one of the most appealing applications of these models, with the goal of latency reduction. \nUnlike LLM.int8(), which reduces memory costs but has the same runtime as the FP16 baseline, we show that our quantized models can achieve significant speedups for this application. \nFor language generation, the model processes and outputs one token at-a-time, which for OPT-175B can easily take a few 100s of milliseconds per token. \nIncreasing the speed at which the user receives generated results is challenging, as compute is dominated by matrix-vector products. \nUnlike the more intensive matrix-matrix products, matrix-vector products are primarily limited by memory bandwidth. \nWe address this problem by developing a quantized-matrix full-precision-vector product kernel which performs a matrix vector product by dynamically dequantizing weights when needed. Most notably, this does \\emph{not} require any activation quantization.\nWhile dequantization consumes extra compute, the kernel has to access a lot less memory, leading to significant speedups, as shown in Table~\\ref{tab:practical-results}. We note that almost all of the speedup is due to our kernels, as communication costs are negligible in the setup of Dettmers et al.~\\cite{dettmers2022llm}, which we also consider here.\n\n\\begin{table}[h]\n    \\centering\n    \\scalebox{0.9}{\n    \\begin{tabular}{|l|c|c|c|c|}\n         \\toprule\n         GPU & FP16 & 3bit & Speedup & GPU reduction \\\\\n         \\midrule\n         A6000 -- 48GB & 589ms & 132ms & $4.46\\times$ & $8 \\rightarrow 2$ \\\\\n         A100 -- 80GB & 230ms & 121ms & $1.90\\times$ & $5 \\rightarrow 1$ \\\\\n         \\bottomrule\n    \\end{tabular}\n    }\n    \\vspace{5pt}\n    \\caption{Average per-token latency (batch size 1) when generating sequences of length 128.}\n    \\label{tab:practical-results}\n\\end{table}\n\nFor example, using our kernels, the 3-bit OPT-175B model obtained via GPTQ running on a single A100 is over $\\mathbf{1.9\\boldsymbol{\\times}}$ faster than the FP16 version (running on 5 GPUs) in terms of average time per token. \nMore accessible GPUs, such as the NVIDIA A6000, have much lower memory bandwidth, so this strategy is even more effective: executing the 3-bit OPT-175B model on $2\\times$ A6000 GPUs reduces latency from 589 milliseconds for FP16 inference (on 8 GPUs) to 132 milliseconds, a $\\mathbf{4.46\\boldsymbol{\\times}}$ reduction. This is only $\\approx10\\%$ slower than 3-bit inference on the more expensive A100 GPU.\n\n\\begin{figure}[h]\n        \\centering\n        \\includegraphics[width=0.9\\linewidth]{imgs/lambada_task_new.pdf}\n    \\vspace{-3pt}\n    \\caption{The accuracy of OPT and BLOOM models post-GPTQ, measured on LAMBADA.}\n    \\label{fig:lambada_task}\n\\end{figure}\n\n\\paragraph{Zero-Shot Tasks.} While our focus is on language generation, we also evaluate the performance of quantized models on some popular zero-shot tasks, namely LAMBADA~\\cite{paperno2016lambada}, ARC (Easy and Challenge)~\\cite{boratko2018systematic} and  PIQA~\\cite{tata2003piqa}. \nFigure~\\ref{fig:lambada_task} visualizes model performance on LAMBADA (and see also the LAMB. results in Table~\\ref{tab:results-biggest-models}). \nWe observe similar behavior as before: the outliers are that 1) quantization appears ``easier'' across the whole spectrum of models at 4-bit, where even RTN performs relatively well, and 2) at 3-bit, RTN breaks down, while GPTQ still provides good accuracy. We provide additional results in Appendix~\\ref{app:zero-shot}.\n\n\\paragraph{Additional Tricks \\& 2-bit Quantization.} While our experiments so far have focused exclusively on vanilla row-wise quantization, we now show that GPTQ is also compatible with more advanced quantization tricks, leading to further improvements. Specifically, we investigate standard \\textit{grouping} \\cite{alistarh2016qsgd, park2022nuqmm}, i.e. applying independent quantization to groups of $G$ consecutive weights. This interacts very well with GPTQ, as the group parameters can actually be determined during the quantization process of each layer, always using the most current updated weights. The last row of Table~\\ref{tab:results-biggest-models} (marked 3G) includes 3-bit results with group-size 1024 for BLOOM and OPT-175B. At the cost of only $< 0.05$ bits extra storage per weight, this reduces the accuracy drops by another $0.2 - 0.3$ points, bringing 3-bit even closer to FP16 performance.\n\n\\begin{table}[h]\n    \\centering\n    \\begin{tabular}{|l|c|cccccc|c|}\n        \\toprule\n        Model & FP16 & 1024 & 512 & 256 & 128 & 64 & 32 & 3-bit \\\\\n        \\midrule\n        OPT-175B & 8.34 & 11.84 & 10.85 & 10.00 & 9.58 & 9.18 & 8.94 & 8.68 \\\\\n        BLOOM & 8.11 & 11.80 & 10.84 & 10.13 & 9.55 & 9.17 & 8.83 & 8.64 \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\vspace{5pt}\n    \\caption{2-bit GPTQ quantization results with varying group-sizes; perplexity on WikiText2.}\n    \\label{tab:2bit}\n    \\vspace{-15pt}\n\\end{table}\n\nWhile we find grouping at 4-bit and further reducing the group-size at 3-bit to bring only rather minor gains, grouping actually makes it possible to achieve reasonable performance for extreme 2-bit quantization. Table~\\ref{tab:2bit} shows results on WikiText2 when quantizing the biggest models to 2-bit with varying group-sizes. Even at group-size 1024, the perplexity drops only by about $3.5$ points, with the accuracy loss quickly decreasing for smaller group-sizes to only $\\approx 1$ at group-size 64, corresponding to 2.5 bit. Interestingly, we find that the performance of $G = 32$ is only $0.2 - 0.3$ points worse than vanilla 3-bit quantization, at the same memory consumption, which might also be interesting for practical kernel implementations. In summary, we think that these results are a very encouraging first step towards pushing highly-accurate \\emph{one-shot} compression of very large language models even lower than 3-bit.\n\n\\section{Discussion}\n\n\\paragraph{Summary and Limitations.} \nWe have presented GPTQ, an approximate second-order method for quantizing truly large language models. \nGPTQ can accurately compress some of the largest publicly-available models down to $2.5 - 4$ bits per component on average, which leads to significant usability improvements, and to end-to-end speedups, at low accuracy loss. \nWe hope that our method will make these models accessible to more researchers and practitioners. \nAt the same time, we emphasize some significant limitations: \nOn the technical side, our implementation obtains speedups from reduced memory movement, and does not lead to computational reductions. \nIn addition, our study focuses on generative tasks, and does not consider activation quantization, nor speedups in batched execution.  \nThese are natural directions for future work, and we believe this can be achieved with carefully-designed GPU kernels and extensions of existing complementary techniques~\\cite{yao2022zeroquant, wu2022extreme}.\n\n\\paragraph{Ethical Concerns.}\nOur work introduces a general method for compressing large language models (LLMs) via quantization, with little-to-no accuracy loss in terms of standard accuracy metrics such as perplexity. \nOur method is task-agnostic, as it only uses a tiny amount of randomly-chosen input data for calibration. \nWe do not foresee any significant ethical implications arising directly from the technical details of our method. \nHowever, one notable consideration is that our study focused on ``leading accuracy'' metrics, such as perplexity, which is essentially standard in the literature~\\cite{dettmers2022llm, yao2022zeroquant}. \nWe believe a thorough study of the impact of compression upon secondary measures, and in particular transferrability~\\cite{iofinova2022well} or bias effects~\\cite{bender2021dangers} is warranted, and may be rendered easier through our work.  \nAt the same time, our work makes inference on extremely large language models more accessible, for better or for worse. \nIn time, these massive models will become much easier to use and deploy, making the need to understand their power and limitations even more stringent. \n\n\\section*{Acknowledgments}\n\nElias Frantar and Dan Alistarh gratefully acknowledge funding from the European Research Council (ERC) under the European Union’s Horizon 2020 programme (grant agreement No. 805223 ScaleML), as well as experimental support from Eldar Kurtic, and from the IST Austria IT department, in particular Stefano Elefante, Andrei Hornoiu, and Alois Schloegl. \nThe work of Saleh Ashkboos and Torsten Hoefler was supported by the PASC DaCeMI project, received EuroHPC-JU funding under grant MAELSTROM, No. 955513. We thank the Swiss National Supercomputing Center (CSCS) for supporting us with compute infrastructure.\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration}\n\n\\begin{document}\n\n\\twocolumn[\n\\mlsystitle{AWQ: \\underline{A}ctivation-aware \\underline{W}eight \\underline{Q}uantization for \\\\On-Device LLM Compression and Acceleration}\n\n\\mlsyssetsymbol{equal}{*}\n\\mlsyssetsymbol{equal-sys}{\\dag}\n\n\\begin{mlsysauthorlist}\n\\mlsysauthor{Ji Lin}{equal,mit}\n\\mlsysauthor{Jiaming Tang}{equal,mit,sjtu}\n\\mlsysauthor{Haotian Tang}{equal-sys,mit}\n\\mlsysauthor{Shang Yang}{equal-sys,mit}\n\\mlsysauthor{Wei-Ming Chen}{nvidia}\n\\mlsysauthor{Wei-Chen Wang}{mit}\n\\mlsysauthor{Guangxuan Xiao}{mit}\n\\mlsysauthor{Xingyu Dang}{mit,tsinghua}\n\\mlsysauthor{Chuang Gan}{ibm,umass}\n\\mlsysauthor{Song Han}{mit,nvidia}\n\\end{mlsysauthorlist}\n\n\\mlsysaffiliation{mit}{MIT}\n\\mlsysaffiliation{sjtu}{Shanghai Jiao Tong University}\n\\mlsysaffiliation{nvidia}{NVIDIA}\n\\mlsysaffiliation{tsinghua}{Tsinghua University}\n\\mlsysaffiliation{ibm}{MIT-IBM Watson AI Lab}\n\\mlsysaffiliation{umass}{UMass Amherst}\n\n\\mlsyscorrespondingauthor{Song Han}{songhan@mit.edu}\n\\centering\n\\url{https://github.com/mit-han-lab/llm-awq}\n\n\\mlsyskeywords{Machine Learning, MLSys}\n\\vskip 0.3in\n\\begin{abstract}\n\nLarge language models (LLMs) have transformed numerous AI applications. \\textit{On-device} LLM is becoming increasingly important: running LLMs locally on edge devices can reduce the cloud computing cost and protect users' privacy. However, the astronomical model size and the limited hardware resource pose significant deployment challenges. We propose \\method (\\methodshort), a hardware-friendly approach for LLM low-bit weight-only quantization. \\methodshort finds that not all weights in an LLM are equally important. Protecting \\textit{only 1\\%} salient weights can greatly reduce quantization error. To identify salient weight channels, we should refer to the activation distribution, not weights. \nTo avoid the hardware-inefficient mix-precision quantization, we mathematically derive that scaling up the salient channels can reduce the quantization error.\n\\methodshort employs an equivalent transformation to scale the salient weight channels to protect them. The scale is determined by collecting the activation statistics offline.\n\\methodshort does not rely on any backpropagation or reconstruction, so it generalizes to different domains and modalities without overfitting the calibration set. \\methodshort outperforms existing work on various language modeling and domain-specific benchmarks (coding and math). Thanks to better generalization, it achieves excellent quantization performance for \\emph{instruction-tuned} LMs and, for the first time, \\emph{multi-modal} LMs. \nAlongside AWQ, we implement \\system, an efficient and flexible inference framework tailored for 4-bit on-device LLM/VLMs. With kernel fusion and platform-aware weight packing, \\system offers more than \\textbf{3}$\\times$ speedup over the Huggingface FP16 implementation on both desktop and mobile GPUs. It also democratizes the deployment of the 70B Llama-2 model on mobile GPUs. \n\n\\end{abstract}\n\n]\n\n \n\\newcommand{\\mlsysEqualContributionOurs}{\\textsuperscript{*}: Algorithm co-lead, \\textsuperscript{\\dag}: system co-lead.}\n\\printAffiliationsAndNotice{\\mlsysEqualContributionOurs} %\n\\section{Introduction}\n\nDeploying large language models (LLMs) directly on edge devices is crucial. On-device usage eliminates delays caused by sending data to a cloud server and enables LLMs to operate offline, which is beneficial for real-time applications like virtual assistants, chatbots, and autonomous vehicles. The operational costs associated with maintaining and scaling centralized cloud infrastructure can also be reduced. On-device LLM also enhances data security by keeping sensitive information local, reducing the chance of data breaches. LLMs, grounded in transformer-based architectures~\\cite{vaswani2017attention}, have gathered significant attention for their impressive performance across diverse benchmarks~\\cite{gpt3, opt, touvron2023llama, scao2022bloom}. However, the large model size leads to the high serving costs. For example, GPT-3 has 175B parameters, which is 350GB in FP16, while the latest B200 GPU only has 192GB memory, let alone edge devices.\n\n\\begin{figure}\n    \\centering\n     \\includegraphics[width=0.5\\textwidth]{figures/teaser.pdf}\n    \\caption{We introduce \\textbf{AWQ}, a versatile weight quantization method for LLM. To implement AWQ, we developed \\textbf{\\system} to deploy 4-bit quantized LLMs into various edge platforms, achieving a \\textbf{3-4$\\times$} performance boost compared to FP16. Notably, we've also manufactured a \\textbf{TinyChat computer}, powered by \\system, which contains an NVIDIA Jetson Orin Nano with only 8GB of memory and 15W power consumption. Demo: \\url{https://youtu.be/z91a8DrfgEw}.} \n    \\label{fig:teaser}\n\\end{figure}\n\nLow-bit weight quantization for LLMs can significantly reduce the memory footprint of on-device LLM inference but is hard.\nQuantization-aware training (QAT) is not efficient due to the high training cost, while post-training quantization (PTQ) suffers from large accuracy degradation under a low-bit setting. The closest work is GPTQ~\\cite{frantar2022gptq}, which uses second-order information to perform error compensation. \nHowever, it may overfit the calibration set during reconstruction, distorting the learned features on out-of-distribution domains (Figure~\\ref{fig:calib_set_ablation}), which is problematic since LLMs are \\emph{generalist} models. \n\nIn this paper, we propose  \\method (\\methodshort), a hardware-friendly low-bit weight-only quantization method for LLMs. Our method is based on the observation that \\emph{weights are not equally important} for LLMs' performance. There is a small fraction (0.1\\%-1\\%) of \\emph{salient} weights; skipping the quantization of these salient weights will significantly reduce the quantization loss (Table~\\ref{tab:fp_ratio}). To find the salient weight channels, the insight is that we should refer to the \\emph{activation} distribution instead of the \\emph{weight} distribution, despite we are doing \\emph{weight-only} quantization: weight channels corresponding to larger activation magnitudes are more salient since they process more important features. \nTo avoid the hardware-inefficient mixed-precision implementation, we analyze the error from weight quantization and derive that \\emph{scaling up the salient channels can reduce their relative quantization error} (Equation~\\ref{eq:scaled_quant}). Following the intuition, we designed a per-channel scaling method to automatically search for the optimal scaling that minimizes the quantization error under full-weight quantization.\n\\methodshort does not rely on any backpropagation or reconstruction, so it can well preserve LLMs’ generalization ability on various domains and modalities without overfitting to the calibration set. \n\nTo implement AWQ, we designed \\system, an efficient inference framework to convert theoretical memory savings from 4-bit LLM to measured speedup. Our framework significantly speeds up linear layers through on-the-fly dequantization. We also take advantage of efficient 4-bit weight packing and kernel fusion to minimize the inference overhead (\\eg, intermediate DRAM access and kernel launch overhead), such that we can better realize the speed up from quantizing the weights to 4-bit, despite the computer is byte-aligned. \n\nExperiments show that \\methodshort outperforms existing work on various tasks\nfor  different model families (\\eg, LLaMA~\\cite{touvron2023llama}, OPT~\\cite{opt}) and model sizes.  Thanks to better generalization, it also achieves good quantization performance for \\emph{instruction-tuned} LMs (\\eg, Vicuna) and, for the first time, \\emph{multi-modal} LMs (OpenFlamingo~\\cite{openflamingo}). \n\\system further translates the $\\sim$4$\\times$ lower memory footprint to measured speedup. On desktop, laptop and mobile GPUs, we consistently observe a \\textbf{3.2-3.3}$\\times$ average speedup compared to the FP16 implementation by Huggingface across a diverse spectrum of LLMs. Furthermore, it facilitates effortless deployment of the Llama-2-70B model on a single NVIDIA Jetson Orin with 64GB of memory. It also democratizes 13 billion parameter LLM at an interactive pace of 30 tokens/second on a laptop RTX 4070 GPU with only 8GB of memory.  \\methodshort has been widely adopted by industry and open-source community: \\href{https://huggingface.co/docs/transformers/main_classes/quantization}{HuggingFace Transformers}, \\href{https://github.com/NVIDIA/TensorRT-LLM/}{NVIDIA TensorRT-LLM}, \\href{https://blogs.windows.com/windowsdeveloper/2024/05/24/quantization-with-directml-helps-you-scale-further-on-windows/}{Microsfot DirectML}, \\href{https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama-2-quantized}{Google Vertex AI}, \\href{https://github.com/intel/neural-compressor}{Intel Neural Compressor}, \\href{https://aws.amazon.com/blogs/machine-learning/boost-inference-performance-for-llms-with-new-amazon-sagemaker-containers/}{Amazon Sagemaker}, \\href{https://community.amd.com/t5/ai/reduce-memory-footprint-and-improve-performance-running-llms-on/ba-p/686157}{AMD}, \\href{https://github.com/lm-sys/FastChat/blob/main/docs/awq.md}{FastChat}, \\href{https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/quantization/awq.py}{vLLM}, \\href{https://github.com/InternLM/lmdeploy}{LMDeploy}, and enables Falcon-180B deployable on a \\href{https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/Falcon180B-H200.md}{single} H200 GPU.\n\n\\begin{figure*}[t]\n    \\centering\n     \\includegraphics[width=\\textwidth]{figures/method.pdf}\n    \\caption{We observe that we can find 1\\% of the salient weights in LLMs based on the \\emph{activation distribution} (middle). Keeping the salient weights in FP16 can significantly improve the quantized performance (PPL from 43.2 (left) to 13.0 (middle)), but the mixed-precision format is not hardware-efficient. We follow the activation-awareness principle and propose \\methodshort (right). \\methodshort performs per-channel scaling to protect the salient weights and reduce quantization error. We measure the perplexity of OPT-6.7B under INT3-g128 quantization. %\n    } \n    \\label{fig:overview}\n\\end{figure*}\n\n\\section{Related Work}\n\\myparagraph{Model quantization methods. }\nQuantization reduces the bit-precision of deep learning models~\\cite{han2016deep, jacob2018quantization, nagel2019data, wang2019haq, nagel2020up,lin2020mcunet}, which helps to reduce the model size and accelerate inference. Quantization techniques generally fall into two categories: quantization-aware training (QAT, which relies on backpropagation to update the quantized weights)~\\cite{bengio2013estimating, gholami2021survey, nagel2021white, choi2018pact} and post-training quantization~\\cite{jacob2018quantization, nagel2019data, nagel2020up} (PTQ, usually training-free). The QAT methods cannot easily scale up to large models like LLMs. Therefore, people usually use PTQ methods to quantize LLMs. \n\n\\myparagraph{Quantization of LLMs.} People study two settings for LLM quantization: (1) W8A8 quantization, where both activation and weights are quantized to INT8~\\cite{dettmers2022llmint8, xiao2022smoothquant, zeroquant, outlier_suppression, wei2023outlier}; (2) Low-bit weight-only quantization (\\eg, W4A16), where only weights are quantized into low-bit integers~\\cite{frantar2022gptq, dettmers2022case, sheng2023high, park2022nuqmm}. We focus on the second setting in this work since it not only reduces the hardware barrier (requiring a smaller memory size) but also speeds up the token generation (remedies memory-bound workload). Apart from the vanilla round-to-nearest baseline (RTN), GPTQ~\\cite{frantar2022gptq} is the closest to our work. However, the reconstruction process of GPTQ leads to an over-fitting issue to the calibration set and may not preserve the generalist abilities of LLMs for other modalities and domains. It also requires a reordering trick to work for some models (\\eg, LLaMA-7B~\\cite{touvron2023llama} and OPT-66B~\\cite{opt}). Apart from quantiztion methods designed for general-purporse hardware, SpAtten~\\cite{spatten} designs a progressive approach to gradually increase the number of bits used in softmax calculation.\n\n\\myparagraph{System support for low-bit quantized LLMs. } Low-bit quantized LLMs have been a popular setting to reduce inference costs. There are some system supports to achieve a practical speed-up. GPTQ~\\cite{frantar2022gptq} provides INT3 kernels for OPT models and \\texttt{GPTQ-for-LLaMA} extends kernel support for INT4 reordered quantization with the help of Triton~\\cite{tillet2019triton}. FlexGen~\\cite{sheng2023high}, \\texttt{llama.cpp}\\footnote{https://github.com/ggerganov/llama.cpp} and \\texttt{exllama}\\footnote{https://github.com/turboderp/exllama} perform group-wise INT4 quantization to reduce I/O costs and offloading. FasterTransformer implements FP16$\\times$INT4 GEMM for weight-only per-tensor quantization but does not support group quantization. LUT-GEMM~\\cite{park2022nuqmm} performs bitwise computation on GPU CUDA cores with the help of lookup tables. Our concurrent work, MLC-LLM~\\cite{mlc-llm} offers strong results on multiple edge CPU and GPU platforms thanks to the powerful TVM~\\cite{chen2018tvm,feng2022tensorir} backend. %\n\n\\section{\\methodshort: \\method}\n\n\\emph{Quantization} maps a floating-point number into lower-bit integers. It is an effective method to reduce the model size and inference costs of LLMs~\\cite{dettmers2022llmint8, frantar2022gptq, zeroquant, xiao2022smoothquant}. In this section, we first propose a weight-only quantization method to improve accuracy \\emph{without training/regression} by protecting more ``important'' weights. And then develop a data-driven method to search for the optimal scaling that reduces quantization errors (Figure~\\ref{fig:overview}). \n\n\\subsection{Improving LLM Quantization by Preserving 1\\% Salient Weights}\n\\renewcommand \\arraystretch{1.05}\n\\begin{table*}[t]\n    \\setlength{\\tabcolsep}{4.5pt}\n    \\small\n    \\centering\n    \\vspace{5pt}\n    \\begin{tabular}{lccccccccccc}\n        \\toprule\n          \\multirow{2}{*}{\\textbf{PPL $\\downarrow$}}   & \\multirow{2}{*}{FP16} & \\multirow{2}{*}{RTN}   & \\multicolumn{3}{c}{FP16\\% (based on act.)}  & \\multicolumn{3}{c}{FP16\\% (based on W)} &  \\multicolumn{3}{c}{FP16\\% (random)} \\\\ \\cmidrule(lr){4-6} \\cmidrule(lr){7-9}  \\cmidrule(lr){10-12}\n         & & (w3-g128) & 0.1\\% & 1\\% & 3\\% &   0.1\\% & 1\\% & 3\\% &  0.1\\% & 1\\% & 3\\%   \\\\\n        \\midrule\n        OPT-1.3B  & 14.62 & 119.00 & 25.03 & \\textcolor{codegreen}{16.91} & \\textcolor{codegreen}{16.68} & 108.71 & 98.55 & 98.08 & 119.76 & 109.38 & 61.49 \\\\\n        OPT-6.7B & 10.86 & 23.54 & \\textcolor{codegreen}{11.58} & \\textcolor{codegreen}{11.39} & \\textcolor{codegreen}{11.36} & 23.41 & 22.37 & 22.45 & 23.54 & 24.23 & 24.22 \\\\\n        OPT-13B &  10.13 & 46.04 &\\textcolor{codegreen}{10.51} & \\textcolor{codegreen}{10.43} & \\textcolor{codegreen}{10.42} & 46.07 & 48.96 & 54.49 & 44.87 & 42.00 & 39.71 \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\caption{Keeping a small fraction of weights (0.1\\%-1\\%) in FP16 significantly improves the performance of the quantized models over round-to-nearest (RTN). It is only effective when we select the important weights in FP16 by looking at \\emph{activation} distribution instead of \\emph{weight} distribution. We highlight results with a decent perplexity in \\textcolor{codegreen}{green}.  \n    We used INT3 quantization with a group size of 128 and measured the WikiText perplexity ($\\downarrow$).  \n    } \n    \\label{tab:fp_ratio}\n\\end{table*}\n\nWe observe that the weights of LLMs are \\emph{not equally important}: there is a small fraction of \\emph{salient} weights that are much more important for LLMs' performance compared to others. Skipping the quantization of these salient weights can help bridge the performance degradation due to the quantization loss \\emph{without} any training or regression (Figure~\\ref{fig:overview}(b)).\nTo verify the idea, we benchmark the performance of quantized LLMs when skipping part of the weight channels in Table~\\ref{tab:fp_ratio}. We measured the performance of INT3 quantized models while keeping some ratios of weight channels in FP16. A widely used method to determine the importance of weights is to look at its magnitude or $L_2$-norm~\\cite{han2015learning, frankle2018lottery}. But we find skipping the weight channels with large norm (\\ie, FP16\\% (based on W)) does not significantly improve the quantized performance, leading to a similar marginal improvement as random selection. \nInterestingly, selecting weights based on \\emph{activation magnitude} can significantly improve the performance despite keeping only 0.1\\%-1\\% of channels in FP16. \nWe hypothesize that the input features with larger magnitudes are generally more important. Keeping the corresponding weights in FP16 can preserve those features, which contributes to better model performance. \n\n\\textbf{Limitations: }\nDespite keeping 0.1\\% of weights in FP16 can improve the quantized performance without a noticeable increase in model size (measured in total bits), such a mixed-precision data type will make the system implementation difficult. We need to come up with a method to protect the important weights without actually keeping them as FP16. \n\n\\subsection{Protecting Salient Weights by Activation-aware Scaling}\nWe propose an alternative method to reduce the quantization error of the salient weight by \\emph{per-channel scaling}, which does not suffer from the hardware inefficiency issue. \n\n\\textbf{Analyzing the quantization error.}\n\n\\begin{table}\n    \\setlength{\\tabcolsep}{2pt}\n    \\small\n    \\centering\n    \\begin{tabular}{lccccc}\n        \\toprule\n     \\textbf{OPT-6.7B}   & $s=1$ & $s=1.25$ & $s=1.5$ & $s=2$ & $s=4$ \\\\ \\midrule\n    proportion of $\\Delta^{'} \\neq \\Delta$ & 0\\% & 2.8\\% & 4.4\\% & 8.2\\% & 21.2\\% \\\\\n    average $\\Delta^{'}/ \\Delta$ & 1 & 1.005 & 1.013 & 1.038 & 1.213 \\\\\n    average $\\frac{\\Delta^{'}}{\\Delta} \\cdot \\frac{1}{s}$ & 1 & 0.804  & 0.676 & 0.519 & \\textbf{0.303} \\\\ \\midrule \n    Wiki-2 PPL & 23.54 & 12.87 & 12.48 & \\textbf{11.92} & 12.36 \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{Statistics when multiplying the 1\\% salient channels by $s>1$. Scaling up the salient channels significantly improves the perplexity (23.54 to 11.92). As $s$ goes larger, the percentage of changed $\\Delta$ increases, and the error reduction rate for salient channels also increases. However, the best perplexity is achieved at $s=2$, since further increasing $s$ will increase the quantization error for \\emph{non-salient} channels. \n    } \n    \\label{tab:scale_study}\n\\end{table}\n\nWe start by analyzing the error from weight-only quantization. \nConsider a group/block of weight $\\mathbf{w}$; the linear operation can be written as $y=\\mathbf{w}\\mathbf{x}$, and the quantized counterpart is $y=Q(\\mathbf{w})\\mathbf{x}$. Specifically, the quantization function is defined as:\n\\begin{equation}\n    Q(\\mathbf{w}) = \\Delta \\cdot \\text{Round}(\\frac{\\mathbf{w}}{\\Delta}), \\quad \\Delta = \\frac{\\max(|\\mathbf{w}|)}{2^{N-1}},\n    \\label{eq:org_quant}\n\\end{equation}\nwhere $N$ is the number of quantization bits, and $\\Delta$ is the quantization scaler determined by the absolute maximum value. Now consider a weight element $w\\in \\mathbf{w}$, if we multiply $w$ with $s> 1$ and the inversely scale $x$, we will have $Q(w\\cdot s)(x/s)$, which is:\n\\begin{equation}\n    Q(w\\cdot s)\\cdot \\frac{x}{s} = \\Delta^{'} \\cdot \\text{Round}(\\frac{ws}{\\Delta^{'}}) \\cdot x \\cdot \\frac{1}{s},\n    \\label{eq:scaled_quant}\n\\end{equation}\nwhere $\\Delta^{'}$ is the new quantization scaler after applying $s$. We empirically find that: (1) The expected error from $\\text{Round}(\\cdot)$ (denoted as $\\texttt{RoundErr}(\\cdot)$) does not change: since the round function maps a floating-point number to an integer, the error is roughly uniformly distributed from [0,0.5], resulting in an average error of $0.25$; i.e., $\\texttt{RoundErr}(\\cdot)\\sim 0.25$. (2) Scaling up a single element $w$ usually does not change the maximum value from the group $\\mathbf{w}$. Therefore we have $\\Delta^{'}\\approx\\Delta$;\n(3) As $\\Delta$ and $x$ are represented in FP16, they have no quantization error. Consequently, the quantization error from equation~\\ref{eq:org_quant} and ~\\ref{eq:scaled_quant} can be expressed as \n\\begin{equation}\n\\begin{aligned}\n\\texttt{Err}(Q(w) x) &= \\Delta\\cdot \\texttt{RoundErr}(\\frac{w}{\\Delta})\\cdot x \\\\\n\\texttt{Err}(Q(w \\cdot s)(\\frac{x}{s})) &= \\Delta^{'}\\cdot \\texttt{RoundErr}(\\frac{ws}{\\Delta^{'}})\\cdot x\\cdot \\frac{1}{s}\n\\end{aligned}\n\\end{equation}\nThe ratio of the new error to the original error is $\\frac{\\Delta^{'}}{\\Delta} \\cdot \\frac{1}{s}$. Given $\\Delta^{'}\\approx\\Delta$ and $s>1$, the relative error is smaller for the salient weight $w$. \n\nTo verify the idea, we multiply the 1\\% salient channels with $s>1$ for the OPT-6.7B model, and measure the change in $\\Delta$ for each group in Table \\ref{tab:scale_study}. We find that scaling up the salient channels is quite effective: the perplexity improves from 23.54 for $s=1$ (simply RTN) to 11.92 for $s=2$. \nAs $s$ goes larger, the percentage of changed $\\Delta$ generally gets larger, but the percentage is still quite small for $s<2$ (less than 5\\%); the relative error for the salient channels continues to go smaller as $s$ increases. Nonetheless, the best PPL actually appears at $s=2$. This is because if we use a very large $s$, it will increase the relative error for the \\emph{non-salient} channels when $\\Delta$ increases (the error of non-salient channels will be amplified by $\\frac{\\Delta^{'}}{\\Delta}$, and the ratio is larger than 1 for 21.2\\% of the channels under $s=4$), which can damage the model's overall accuracy. Therefore, we need to also consider the error from non-salient channels when protecting salient ones. \n\n\\begin{table}\n    \\setlength{\\tabcolsep}{5pt}\n    \\small\n    \\centering\n    \\begin{tabular}{llccccccc}\n      \\toprule\n        \\textbf{OPT (PPL}$\\downarrow$) & 1.3B & 2.7B & 6.7B  & 13B & 30B   \\\\  \\midrule\n      FP16 & 14.62 & 12.47 & 10.86 & 10.13\t& 9.56 \\\\ \\midrule\n      RTN & 119.47 & 298.00 & 23.54 & 46.04\t& 18.80\t \\\\\n       1\\% FP16 & 16.91 & 13.69 & \\textbf{11.39} & \\textbf{10.43} & 9.85 \\\\\n       $s=2$ & 18.63 & 14.94 & 11.92 & 10.80 & 10.32 \\\\\n       \\methodshort  &  \\textbf{16.32} & \\textbf{13.58} &\\textbf{11.39} & 10.56 & \\textbf{9.77}\t\\\\\n      \\bottomrule\n    \\end{tabular}\n    \\caption{\\methodshort protects salient weights and reduces quantization error by using a scaling-based method. It consistently outperforms Round-to-nearest quantization (RTN) and achieves comparable performance as mixed-precision (1\\% FP16) while being more hardware-friendly. We use 3-bit quantization with group size 128.}\n    \\label{tab:opt_ppl}\n\\end{table}\n\n\\begin{figure*}[t]\n    \\centering\n     \\includegraphics[width=\\textwidth]{figures/memory_bound_analysis.pdf}\n    \\caption{Bottleneck analysis for Llama-2-7B on NVIDIA RTX 4090. \\textbf{Left}: In on-device LLM applications, generation stage is much slower than the context stage. \\textbf{Middle}: The generation stage is memory bound and has low arithmetic intensity. W4A16 quantization can effectively improve the arithmetic intensity by 4$\\times$. \\textbf{Right}: The amount of weight access is orders of magnitude larger than the amount of activation access. Thus, weight-only quantization is more effective for on-device LLMs.} \n    \\label{fig:memory_bound_analysis}\n\\end{figure*}\n\n\\textbf{Searching to scale. }\nTo consider both salient and non-salient weights, we choose to automatically search for an optimal (per input channel) scaling factor that minimizes the output difference after quantization for a certain layer. Formally, we want to optimize the following objective:\n\\begin{equation}\n\\begin{aligned}\n    \\mathbf{s}^* &= \\mathop{\\arg\\min}_{\\mathbf{s}} \\mathcal{L}(\\mathbf{s}) \\\\  \n    \\mathcal{L}(\\mathbf{s}) = \\lVert Q(\\mathbf{W}\\cdot \\text{di}&\\text{ag}(\\mathbf{s}))  (\\mathbf{\\text{diag}(s)^{-1}} \\cdot \\mathbf{X}) - \\mathbf{W}\\mathbf{X}  \\rVert\n\\end{aligned}\n\\end{equation}\nHere $Q$ means the weight quantization function (\\eg, INT3/INT4 quantization with group size 128), $\\mathbf{W}$ is the original weights in FP16, and $\\mathbf{X}$ is the input features cached from a small calibration set (we take a small calibration set from he pre-training dataset in order not to overfit to a specific task). $\\mathbf{s}$ is a per-(input) channel scaling factor; for $\\mathbf{s^{-1}} \\cdot \\mathbf{X}$, it can usually be fused into the previous operator~\\cite{wei2022outlier, xiao2022smoothquant}.\nSince the quantization function is not differentiable, we are not able to directly optimize the problem with vanilla backpropagation. There are some techniques relying on approximated gradients~\\cite{bengio2013estimating, esser2019learned}, which we found still suffers from unstable convergence.  \n\nTo make the process more stable, we define a \\emph{search space} for the optimal scale by analyzing the factors that will affect the choice of scaling factor. As shown in the last section, the saliency of weight channels is actually determined by the activation scale (thus ``activation-awareness''). Therefore, we simply use a very simple search space:\n\\begin{equation}\n\\label{eq:scale_search_formula}\n    \\mathbf{s}=\\mathbf{s_X}^{\\alpha}, \\quad \\alpha^*=\\mathop{\\arg\\min}_{\\alpha}\\mathcal{L}(\\mathbf{s_X}^{\\alpha})\n\\end{equation}\n$\\mathbf{s_X}$ is the average magnitude of activation (per-channel), and we use a single hyper-parameter $\\alpha$ to balance between the protection of salient and non-salient channels. We can find the best $\\alpha$ by a fast grid search over the interval of $[0, 1]$ ($0$ means we do not scale; $1$  corresponds to the most aggressive scaling in our search space). We further apply weight clipping to minimize the MSE error of quantization. We provide an ablation study on OPT models under INT3-g128 quantization in Table~\\ref{tab:opt_ppl}; \\methodshort consistently outperforms round-to-nearest quantization (RTN) and achieves comparable performance as mixed-precision (1\\% FP16) while being more hardware-friendly.\n\n\\textbf{Advantages.} \nOur method does not rely on any regression~\\cite{frantar2022gptq} or backpropagation, which is required by many quantization-aware training methods. It has minimal reliance on the calibration set since we only measure the average magnitude per channel, thus preventing over-fitting (Figure~\\ref{fig:calib_set_ablation}). Therefore, our method requires fewer data for the quantization process and can preserve LLMs' knowledge outside of the calibration set's distribution. \nSee Section~\\ref{sec:ablation_study} for more details. %\n\n\\section{\\system: Mapping AWQ onto Edge Platforms}\n\nAWQ can substantially reduce the size of LLMs. However, converting the theoretical memory savings from W4A16 (4-bit weight, 16-bit activation) quantization into measured speedup is non-trivial. Alternative W8A8 quantization methods, such as SmoothQuant~\\cite{xiao2022smoothquant}, maintain \\textit{the same} data precision for both storage and computation. This allows the dequantization procedure to be seamlessly integrated into the computation kernel's epilogue. On the other hand, W4A16 quantization employs \\textit{different} data types for memory access and computation. As a result, its dequantization must be incorporated into the primary computation loop for optimal performance, posing implementation challenges. To tackle this, we introduce \\system: a nimble system for AWQ model inference. It boasts a PyTorch frontend and a backend harnessing device-specific instruction sets (e.g., CUDA/PTX, Neon, AVX).\n\n\\subsection{Why AWQ Helps Accelerate On-Device LLMs}\n\n\\begin{figure*}\n    \\centering\n     \\includegraphics[width=\\textwidth]{figures/weight_packing.pdf}\n    \\caption{SIMD-aware weight packing for ARM NEON with 128-bit SIMD units. Original weights are reordered and packed to align with the bit width so that the weights can be unpacked into bytes at runtime using AND and shift bitwise operations with a 128-bit mask.} %\n    \\label{fig:weight_packing}\n\\end{figure*}\n\nTo understand the acceleration opportunities in quantized LLMs on the edge, we start by profiling the latency breakdown of LLaMA-7B~\\cite{touvron2023llama} model on an RTX 4090 GPU. We adopt an inference batch size of 1, catering for edge use cases, and implement the model in FP16 with NVIDIA FasterTransformer. \n\n\\myparagraph{Context \\textit{vs} generation latency.} As in Figure~\\ref{fig:memory_bound_analysis}(a), it takes 310 ms to generate 20 tokens, while summarizing a prompt with 200 tokens only takes 10 ms. Consequently, the generation phase is substantially slower than the context stage, particularly for on-device interactive applications. \n\n\\myparagraph{Generation stage is memory-bound.} To accelerate the generation phase, we conduct a roofline analysis in Figure~\\ref{fig:memory_bound_analysis}(b). The 4090 GPU has a peak computation throughput of 165 TFLOPS and a memory bandwidth of 1TB/s. Therefore, any workload with arithmetic intensity (the ratio of FLOPs to memory access) less than 165 is  memory bounded on 4090 GPUs. Notably, when executed in FP16, the generation stage for on-device LLMs has arithmetic intensity$\\approx$1. This underscores the memory-bound nature of the workload. Since the FLOPs of a given model is fixed, the only way to improve the peak performance is to reduce the total amount of memory traffic. AWQ reduces the weight memory by four times. \n\n\\myparagraph{Weight access dominates memory traffic.}\n We therefore further break down the memory access for weight and activation in Figure~\\ref{fig:memory_bound_analysis}(c). Clearly, weight access dominates the memory traffic for on-device LLMs. Quantizing the model weights to 4 bit integers will approximately increase the arithmetic intensity to 4 FLOPs/Byte, leading to a 4TFLOPS peak performance in Figure~\\ref{fig:memory_bound_analysis}(b). Since weight-only quantization leads to a lower bit width for weights (and thus higher theoretical performance upper bound), it is natural for AWQ to follow this setting for on-device LLM applications.\n\n\\subsection{Deploy AWQ with \\system}\n\nTo this end, we demonstrated that 4-bit weight quantization could lead to a 4$\\times$ theoretical peak performance. We further design \\system to realize this speedup. On GPUs, we only focus on implementing essential components, including attention, layer normalization, and linear projection kernels. The flexible frontend allows easy customization and fast support for new models. \\system with 4-bit AWQ achieves more than \\textbf{3$\\times$} speedup compared with the Huggingface FP16 implementation across different families of LLMs on GPUs. On CPUs, we lower the entire computation graph to C++ to minimize overhead.\n\n\\myparagraph{On-the-fly weight dequantization.} For quantized layers, as the hardware does not provide multiplication instructions between INT4 and FP16, we need to dequantize the integers to FP16 before performing matrix computation. We avoid writing dequantized weights into DRAM by fusing dequantization kernels with the matrix multplication kernel. Note that such fusion is adopted for both matrix-matrix (MM) and matrix-vector (MV) product kernels. %\n\n\\myparagraph{SIMD-aware weight packing.} On-the-fly weight dequantization reduces intermediate DRAM access, but remains expensive. For instance, dequantizing \\textit{a single 4-bit weight} involves 1 shift, 1 bitwise AND, and 1 FMA scaling operations, while the dequantized weight undergoes only 1 FMA computation. This process is particularly costly on CPUs with SIMD architecture that favor vectorized instructions. To mitigate this, we suggest platform-specific weight packing tailored to the bitwidth of a device's SIMD units. Figure~\\ref{fig:weight_packing} demonstrates our strategy for ARM CPUs with 128-bit SIMD registers offering up to 1.2$\\times$ speedup. Here, each register holds 32 4-bit weights, sequenced as $w_0, w_{16}, w_{1}, w_{17}, ..., w_{15}, w_{31}$. This approach requires just three SIMD instructions to unpack \\textit{all 32 weights}, as opposed to 3 scalar instructions \\textit{per weight} in a conventional packing ($w_0, w_1, ..., w_{31}$). Generally, for $2^n$-bit SIMD registers, adjacent weights will have indices off by $1/8\\times2^n$, since each register can hold $1/8\\times2^n$ 8-bit integers. On GPUs, we found it more efficient to pack each 8 weights into $w_{\\{0,2,4,6,1,3,5,7\\}}$ following~\\cite{kim2022says}. \n\n\\myparagraph{Kernel fusion.} We also extensively apply kernel fusion to optimize on-device LLM inference. For layer normalization, we fuse all operators (\\eg multiplication, division and square root) into a single kernel. For attention layers, we fuse QKV projections into a single kernel, and also perform on-the-fly positional embedding calculation. We also pre-allocate KV caches and perform cache updates within the attention kernel. Kernel fusion is particularly useful for models with inefficient forward pass implementations, such as Falcon~\\cite{penedo2023refinedweb} and StarCoder~\\cite{li2023starcoder}. Notably, the computation time for each FP16 kernel is in the order of 0.01ms on the 4090 GPU, comparable to the GPU kernel launch overhead. Hence, reducing number of kernel calls through kernel fusion leads to direct speedups.\n\n\\section{Experiments}\n\\begin{table*}\n    \\setlength{\\tabcolsep}{12pt}\n    \\small\n    \\centering\n    \\begin{tabular}{llccccccc}\n        \\toprule\n          \\multirow{2}{*}{\\textbf{PPL}$\\downarrow$} & & \\multicolumn{3}{c}{\\textbf{Llama-2}} & \\multicolumn{4}{c}{\\textbf{LLaMA}}  \\\\  \\cmidrule(lr){3-5} \\cmidrule(lr){6-9}\n          & & 7B & 13B & 70B & 7B  & 13B & 30B & 65B\\\\ \\midrule\n        FP16 & - & 5.47 & 4.88 & 3.32 & 5.68 &\t5.09 & 4.10\t& 3.53\\\\ \\midrule\n        \\multirow{4}{*}{\\shortstack{INT3\\\\g128}} & RTN & 6.66 &\t5.52 & 3.98 & 7.01 &\t5.88 & 4.88\t& 4.24  \\\\\n        & GPTQ &  6.43\t& 5.48 & 3.88 & 8.81 & 5.66 & 4.88 & 4.17\\\\\n         & GPTQ-R &  6.42\t& 5.41 & 3.86 & 6.53 & 5.64 & 4.74 & 4.21\\\\\n         & \\methodshort  & \\textbf{6.24} & \\textbf{5.32} & \\textbf{3.74} & \\textbf{6.35} & \\textbf{5.52} & \\textbf{4.61} & \\textbf{3.95} \\\\ \\midrule\n         \\multirow{4}{*}{\\shortstack{INT4\\\\g128}} & RTN & 5.73 & 4.98 & 3.46& 5.96 & 5.25 & 4.23 & 3.67\\\\\n         & GPTQ &  5.69\t& 4.98 & 3.42 & 6.22 & 5.23 & 4.24 & 3.66\\\\\n         & GPTQ-R & 5.63 & 4.99 & 3.43 & 5.83 & 5.20 & 4.22 & 3.66\\\\\n         & \\methodshort & \\textbf{5.60} & \\textbf{4.97} & \\textbf{3.41}  & \\textbf{5.78} & \\textbf{5.19} & \\textbf{4.21} & \\textbf{3.62}\\\\\n        \\bottomrule\n    \\end{tabular}\n    \\caption{\\methodshort improves over round-to-nearest quantization (RTN) for different model sizes and different bit-precisions. It consistently achieves better perplexity than GPTQ (w/ and w/o reordering) on LLaMA \\& Llama-2 models.\n    }\n    \\label{tab:llama_opt_ppl}\n\\end{table*}\n\\begin{table}\n    \\setlength{\\tabcolsep}{5pt}\n    \\small\n    \\centering\n    \\begin{tabular}{lcccccccc}\n      \\toprule\n        \\textbf{Wikitext2 PPL}$\\downarrow$ & Mixtral-8x7B  & Mistral-7B \\\\  \\midrule\n      FP16 & 5.94 & 4.14 \\\\ \\midrule\n      INT4-g128 & 6.05 & 4.30\t \\\\\n       INT3-g128 & 6.52 & 4.83  \\\\\n      \\bottomrule\n    \\end{tabular}\n    \\caption{\\methodshort quantization results on Mistral-7B-Instruct-v0.2\\cite{jiang2023mistral} and Mixtral-8x7B-Instruct-v0.1 model ~\\cite{jiang2024mixtral}. The PPL result on wikitext shows that \\methodshort can achieve superior quantization performance on different model architectures including LLMs with GQA and Mixture-of-Experts (MoE) models.}\n    \\label{tab:opt_ppl}\n\\end{table}\n\n\\subsection{Settings}\n\n\\myparagraph{Quantization. } We focus on \\emph{weight-only grouped} quantization in this work. As shown in previous work~\\cite{dettmers2022case, frantar2022gptq}, grouped quantization is always helpful for improving performance/model size trade-off. We used a group size of 128 throughout the work, except otherwise specified. We focus on INT4/INT3 quantization since they are able to mostly preserve the LLMs' performance~\\cite{dettmers2022case}. For \\methodshort, we used a small calibration set from the Pile~\\cite{gao2020pile} dataset in order not to overfit to a specific downstream domain. We used a grid size of 20 to search for the optimal $\\alpha$ in Equation~\\ref{eq:scale_search_formula}. \n\n\\myparagraph{Models. } We benchmarked our method on LLaMA~\\cite{touvron2023llama} and OPT~\\cite{opt} families. There are other open LLMs like BLOOM~\\cite{scao2022bloom}, but they are generally worse in quality, so we do not include them in our study. We further benchmark an instruction-tuned model Vicuna~\\cite{vicuna2023} and visual language models OpenFlamingo-9B~\\cite{openflamingo} and LLaVA-13B~\\cite{liu2023llava} to demonstrate the generability of our method. \n\n\\myparagraph{Evaluations.} Following previous literature~\\cite{dettmers2022llmint8, xiao2022smoothquant, frantar2022gptq, dettmers2022case, zeroquant}, we mainly profiled the quantized models on language modeling tasks (perplexity evaluation on WikiText-2~\\cite{merity2016pointer}) since perplexity can stably reflect the LLM's performance~\\cite{dettmers2022case}.\n\n\\myparagraph{Baselines.} Our primary baseline is vanilla round-to-nearest quantization (RTN). It is actually quite strong when using a small group size like 128~\\cite{frantar2022gptq, dettmers2022case}. We also compare with a state-of-the-art method GPTQ~\\cite{frantar2022gptq} for LLM weight quantization. For GPTQ, we also compare with an updated version that uses a ``reorder'' trick (denoted as GPTQ-Reorder or GPTQ-R). Other techniques like ZeroQuant~\\cite{zeroquant}, AdaRound~\\cite{nagel2020up}, and BRECQ~\\cite{li2021brecq} rely on backpropagation to update the quantized weights, which may not easily scale up to large model sizes; they also do not outperform GPTQ~\\cite{frantar2022gptq}, thus not included for study. \n\n\\subsection{Evaluation}\n\\definecolor{winblue}{RGB}{114, 147, 182}\n\n\\begin{figure}%\n    \\centering\n     \\includegraphics[width=0.5\\textwidth]{figures/vicuna_gpt4_eval_small.pdf}\n    \\caption{Comparing INT3-g128 quantized Vicuna models with FP16 counterparts under GPT-4 evaluation protocol~\\cite{vicuna2023}. More winning cases (in \n    \\textcolor{winblue}{blue}) indicate better performance. \\methodshort consistently improves the quantized performance compared to RTN and GPTQ~\\cite{frantar2022gptq}, showing generalization to instruction-tuned models.\n    } \n    \\label{fig:vicuna_gpt4_eval}\n\\end{figure}\n    \n\n\\begin{table*}%\n    \\small\n    \\setlength{\\tabcolsep}{11pt}\n    \n    \\centering\n    \\begin{tabular}{llcccccc}\n        \\toprule\n        \\multicolumn{2}{l}{\\textbf{COCO (CIDEr $\\uparrow$)}} & 0-shot & 4-shot & 8-shot & 16-shot & 32-shot & \\emph{$\\Delta$(32-shot)} \\\\  \\midrule\n    FP16 & - & 63.73 & 72.18 & 76.95 & 79.74 & 81.70  & - \\\\ \\midrule\n    \\multirow{3}{*}{\\shortstack{INT4\\\\g128}} & RTN & 60.24 & 68.07 & 72.46 & 74.09 & 77.13 & -4.57 \\\\ \n    & GPTQ & 59.72 & 67.68 & 72.53 & 74.98 & 74.98 & -6.72 \\\\ %\n    & \\methodshort & \\textbf{62.57} & \\textbf{71.02} & \\textbf{74.75} & \\textbf{78.23} & \\textbf{80.53} & \\textbf{-1.17} \\\\ \\midrule\n     \\multirow{3}{*}{\\shortstack{INT3\\\\g128}} & RTN & 46.07 & 55.13 & 60.46 & 63.21 & 64.79 & -16.91\\\\ \n    & GPTQ & 29.84 & 50.77 & 56.55 & 60.54 & 64.77 & -16.93\\\\ %\n    & \\methodshort & \\textbf{56.33} & \\textbf{64.73} & \\textbf{68.79} & \\textbf{72.86} & \\textbf{74.47} & \\textbf{-7.23}\\\\\n        \\bottomrule\n    \\end{tabular}\n    \\caption {Quantization results of a visual language model OpenFlamingo-9B~\\cite{openflamingo} on COCO Captioning datasets. \\method{} outperforms existing methods under zero-shot and various few-shot settings, demonstrating the generability to different modalities and in-context learning workloads. \\method{} reduces the quantization degradation (32-shot) from 4.57 to 1.17 under INT4-g128, providing 4$\\times$ model size reduction with negligible performance loss. \n    }\n    \\label{tab:openflamingo}\n\\end{table*}\n\\begin{table*}\n    \\setlength{\\tabcolsep}{5pt}\n    \\small\n    \\centering\n\\begin{tabular}{@{}lccccccccccc@{}}\n\\toprule\n\\textbf{Model (Accuracy$\\uparrow$)} & VQAv2 & GQA & VizWiz & SQA-I & VQA-T & POPE & MME & MMB & SEED & llava-bench & MM-Vet \\\\\n\\midrule\nVILA-7B           & 80.3 & 63.1 & 59.6   & 68.0  & 62.6  & 86.3  & 1489.4 & 69.8\t   & 61.7 & 75.2        & 35.1   \\\\\nVILA-7B-AWQ       & 80.1 & 63.0 & 57.8   & 68.0  & 61.9  & 85.3  & 1486.3 & 68.8   & 61.3 & 75.8        & 35.9   \\\\\nVILA-13B          & 80.5 & 63.6 & 63.1   & 70.5  & 64.0  & 86.3  & 1553.6 & 73.8   & 62.8 & 78.3        & 42.6   \\\\\nVILA-13B-AWQ      & 80.4 & 63.6 & 63.0   & 71.2  & 63.5  & 87.0  & 1552.9 & 73.6   & 62.2 & 77.6        & 42.0   \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{INT4-g128 results of VILA-7B and VILA-13B~\\cite{lin2023vila} on 11 visual-language benchmarks. \\methodshort consistently shows lossless performance on all benchmarks. Benchmark names are abbreviated due to space limits. VQA-v2~\\cite{goyal2017vqav2}; GQA~\\cite{hudson2019gqa}; VisWiz~\\cite{gurari2018vizwiz}; SQA$^\\text{I}$: ScienceQA-IMG~\\cite{lu2022learn}; VQA$^\\text{T}$: TextVQA~\\cite{singh2019textvqa}; POPE~\\cite{li2023pope}; MME~\\cite{fu2023mme}; MMB: MMBench~\\cite{liu2023mmbench}; MMB$^\\text{CN}$: MMBench-Chinese~\\cite{liu2023mmbench}; SEED: SEED-Bench~\\cite{li2023seed}; LLaVA$^\\text{W}$: LLaVA-Bench (In-the-Wild)~\\cite{liu2023llava}; MM-Vet~\\cite{yu2023mmvet}.}\n\\label{tab:vila_acc}\n\n\\end{table*}\n\n\\myparagraph{Results on LLaMA models.}\nWe focus on LLaMA models (LLaMA~\\cite{touvron2023llama} and Llama-2~\\cite{touvron2023llama2}) due to their superior performance compared to other open-source LLMs~\\cite{opt, scao2022bloom}; it is also the foundation of many popular open-source models~\\cite{alpaca, vicuna2023}. We evaluate the perplexity before and after quantization in Table~\\ref{tab:llama_opt_ppl}. \n\\methodshort consistently outperforms round-to-nearest (RTN) and GPTQ~\\cite{frantar2022gptq} (w/ and w/o reordering) across different model scales (7B-70B) and generations. \n\n\\myparagraph{Results on Mistral / Mixtral models.}\nWe also evaluated \\methodshort on the Mistral and Mixtral models, which are among the most popular open-source LLMs and Mixture-of-Experts (MoE) models, respectively~\\cite{jiang2023mistral, jiang2024mixtral}. The results indicate that \\methodshort achieves superior performance on both the Mistral and Mixtral models. This demonstrates that \\methodshort is effective across various model architectures.\n\n\\begin{figure*}[t]\n    \\centering\n     \\includegraphics[width=\\textwidth]{figures/visual_reasoning.pdf}\n    \\caption{Visual reasoning examples from LLaVA-13B model~\\cite{liu2023llava}. \\methodshort improves over the round-to-nearest (RTN) baseline, providing more reasonable answers. We color the text to show the \\textcolor{codegreen}{correct} or \\textcolor{red}{wrong} responses. \n    } \n    \\label{fig:visual_reasoning}\n\\end{figure*}\n\n\\begin{figure*}[t]\n    \\centering\n     \\includegraphics[width=\\textwidth]{figures/coco_caption_samples_w4.pdf}\n    \\caption{Qualitative results of quantized OpenFlamingo-9B~\\cite{openflamingo} on COCO captioning dataset (4-shot, INT4-g128 quantization). Our method significantly improves the captioning quality compared to the round-to-nearest (RTN) baseline. We color the text to show the \\textcolor{codegreen}{correct} or \\textcolor{red}{wrong} captions. }\n    \\label{fig:coco_sample}\n\\end{figure*}\n\n\\myparagraph{Quantization of instruction-tuned models.}\nInstruction tuning can significantly improve the models' performance and usability \n~\\cite{wei2021finetuned,sanh2021multitask,ouyang2022training,chung2022scaling}. It has become an essential procedure before model deployment. We further benchmark our method's performance on a popular instruction-tuned model Vicuna~\\cite{vicuna2023} in Figure~\\ref{fig:vicuna_gpt4_eval}. We used the GPT-4 score to evaluate the quantized models' performance against the FP16 counterpart on 80 sample questions~\\cite{vicuna2023}.\nWe compare the responses with both orders (quantized-FP16, FP16-quantized) to get rid of the ordering effect (we found GPT-4 tends to increase the rating of the first input), leading to 160 trials. \n\\methodshort consistently improves the INT3-g128 quantized Vicuna models over RTN and GPTQ under both scales (7B and 13B), demonstrating the generability to instruction-tuned models.  \n    \n\n\\begin{table}\n    \\small\n    \\setlength{\\tabcolsep}{2pt}\n    \n    \\centering\n    \\begin{tabular}{llcccccc}\n        \\toprule\n        \\textbf{MBPP (7B)} & pass@1 & pass@10 \\\\  \\midrule\n    FP16 & 38.53 & 49.77 \\\\ \\midrule\n    RTN & 37.51 & 48.49  \\\\ \n    GPTQ & 31.97 & 44.75  \\\\ %\n    \\methodshort & \\textbf{40.64} & \\textbf{49.25}  \\\\ \n        \\bottomrule\n    \\end{tabular}\n    \\begin{tabular}{llcccccc}\n        \\toprule\n        \\textbf{GSM8K} & 7B & 13B & 70B \\\\  \\midrule\n    FP16  & 13.87 & 26.16 & 56.41 \\\\ \\midrule\n    RTN & 11.07 & 21.23 & 53.98 \\\\ \n    GPTQ & 12.13 & 24.26 & 56.03  \\\\ %\n    \\methodshort & \\textbf{13.57} & \\textbf{25.25} & \\textbf{56.40} \\\\ \n        \\bottomrule\n    \\end{tabular}\n    \\caption {INT4-g128 quantization results of CodeLlama-7b-Instruct-hf on MBPP dataset and Llama-2 (7B/13B/70B) on GSM8K dataset. AWQ outperforms existing methods on programming and math datasets, demonstrating the generability to different scenarios and evaluation settings. Notably, AWQ under the INT4-g128 configuration demonstrates comparable performance to the original FP16 model across both datasets.}\n    \\label{tab:code_and_math}\n\\end{table}\n\n\\myparagraph{Quantization of multi-modal language models.} Large multi-modal models (LMMs) or visual language models (VLMs) are LLMs augmented with vision inputs~\\cite{alayrac2022flamingo,li2023blip,koh2023grounding,driess2023palm,zhang2023llama,liu2023llava}. Such models are able to perform text generation conditioned on image/video inputs. Since our method does not have the overfitting issue to the calibration set, it can be directly applied to VLMs to provide accurate and efficient quantization. \nWe perform experiments with the OpenFlamingo-9B model~\\cite{openflamingo} (an open-source reproduction of~\\cite{alayrac2022flamingo}) on COCO captioning~\\cite{chen2015microsoft} dataset (Table~\\ref{tab:openflamingo}). We measured the average performance of 5k samples under different few-shot settings. We only quantize the language part of the model since it dominates the model size. \n\\methodshort outperforms existing methods under zero-shot and various few-shot settings, demonstrating the generability to different modalities and in-context learning workloads. It reduces the quantization degradation (32-shot) from 4.57 to 1.17 under INT4-g128, providing 4$\\times$ model size reduction with negligible performance loss. \nTo further demonstrate the generability of \\methodshort, we also evaluated \\methodshort on one of the SoTA multi-image visual language models: VILA. The result in Table~\\ref{tab:vila_acc} shows that \\methodshort achieves lossless quantization performance on 11 visual-language benchmarks.\nWe further provide some qualitative captioning results in Figure~\\ref{fig:coco_sample} to show our advantage over RTN.  \nOur method provides a push-the-button solution for LMM/VLM quantization. It is the \\emph{first} study of VLM low-bit quantization to the best of our knowledge.\n\n\\begin{figure*}\n    \\centering\n     \\includegraphics[width=\\textwidth]{figures/calib_set_ablation_small.pdf}\n    \\caption{\\textbf{Left:} \\methodshort needs a much smaller calibration set to reach a good quantized performance. It can achieve better perplexity using 10$\\times$ smaller calibration set compared to GPTQ. \\textbf{Right:} Our method is more robust to the calibration set distribution. Overall, using the same calibration and evaluation distribution works the best (PubMed-PubMed, Enron-Enron). But when using a different calibration distribution (PubMed-Enron, Enron-PubMed), \\methodshort only increases the perplexity by 0.5-0.6, while GPTQ has 2.3-4.9 worse perplexity. All experiments are done with the OPT-6.7B model under INT3-g128 quantization.}\n    \\label{fig:calib_set_ablation}\n\\end{figure*}\n\n\\begin{figure*}[t]\n    \\centering\n     \\includegraphics[width=\\textwidth]{figures/speedup.pdf}\n\\caption{\\system provides a turn-key solution to transform the theoretical memory footprint reduction into a quantifiable speedup. As a result, \\system is up to \\textbf{3.9$\\times$} and \\textbf{3.5$\\times$} faster than the FP16 implementation from Huggingface on 4090 (desktop GPU) and Orin (mobile GPU), respectively. AWQ also democratizes Llama-2-13B deployment on laptop GPUs (4070) with merely 8GB memory.} %\n    \\label{fig:kernel_speedup}\n\\end{figure*}\n\n\\begin{figure*}[t]\n    \\centering\n     \\includegraphics[width=\\textwidth]{figures/speedup_comparisons.pdf}\n\\caption{\\system offers \\textbf{1.2-3.0$\\times$} speedup over existing systems when running 4-bit quantized Llama models on NVIDIA Jetson Orin. It also supports a diverse range of general-purpose and coding-specific LLMs with at least \\textbf{2.6$\\times$} speedup over AutoGPTQ, which also supports all these workloads. Moreover, \\system seamlessly operates on Raspberry Pi and enables the deployment of LLMs with up to 7 billion parameters on extremely resource-constrained IoT devices.} %\n    \\label{fig:kernel_speedup_comparisons}\n\\end{figure*}\n\\begin{table}\n\\small\n    \\setlength{\\tabcolsep}{3pt}\n    \n    \\centering\n    \\begin{tabular}{llccccc}\n        \\toprule\n          \\textbf{OPT (Wiki PPL$\\downarrow$)} & 1.3B & 2.7B & 6.7B & 13B & 30B  \\\\  \\midrule\n        FP16& 14.62 & 12.47 & 10.86 & 10.13 & 9.56 \\\\ \\midrule\n        RTN & 10476 & 193210 & 7622 & 17564 & 8170 \\\\\n        GPTQ & 46.67 & 28.15 & 16.65 & 16.74 &  11.75 \\\\ \\cmidrule(lr){2-6}\n        \\methodshort+GPTQ & \\textbf{35.71} & \\textbf{25.70} & \\textbf{15.71} & \\textbf{13.25} & \\textbf{11.38}   \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\caption{Our method is orthogonal to GPTQ: it further closes the performance gap under extreme low-bit quantization (INT2-g64) when combined with GPTQ. Results are WikiText-2 perplexity of OPT models.  }\n    \\label{tab:int2}\n\\end{table}\n\n\\myparagraph{Visual reasoning results.}\nWe further provide some qualitative visual reasoning examples of the LLaVA-13B~\\cite{liu2023llava} model in Figure~\\ref{fig:visual_reasoning}. \\methodshort improves the responses compared to round-to-nearest (RTN) for INT4-g128 quantization, leading to more reasonable answers. In this first example, the \\methodshort model can understand the meme as it resembles the Earth when looking from space, while RTN produces wrong descriptions (marked in \\textcolor{red}{red}). %\n\n\\myparagraph{Results on programming and math tasks}\nTo further evaluate the performance of AWQ on tasks involving complex generations, we also tested AWQ on MBPP~\\cite{austin2021program} and GSM8K~\\cite{cobbe2021training}. MBPP~\\cite{austin2021program} consists of around 1,000 Python programming problems, designed to be solvable by entry level programmers, covering programming fundamentals, standard library functionality, etc. GSM8K~\\cite{cobbe2021training} was created to support the task of question answering on basic mathematical problems that require multi-step reasoning. We quantize CodeLlama-7b-Instruct-hf and Llama-2 to INT4-g128 and perform experiments on programming and math datasets (Table~\\ref{tab:code_and_math}). AWQ outperforms existing methods on both datasets, demonstrating the generability to complex generation. AWQ under the INT4-g128 configuration demonstrates comparable\nperformance to the original FP16 model on both datasets.\n\n\\myparagraph{Extreme low-bit quantization.}\nWe further quantize LLM to INT2 to accommodate limited device memory (Table~\\ref{tab:int2}). RTN completely fails, and \\methodshort brings significant perplexity improvement on top of GPTQ.%\nOur method is orthogonal to GPTQ.\nWe can combine our method with GPTQ to further improve the INT2 quantization performance, making it a more practical setting. \n\n\\subsection{Data Efficiency and Generalization}\n\\label{sec:ablation_study}\n\\myparagraph{Better data-efficiency for the calibration set.} Our method requires a smaller calibration set since we do not rely on regression/backpropagation; we only measure the average activation scale from the calibration set, which is data-efficient.\nTo demonstrate the idea, we compare the perplexity of the OPT-6.7B model with INT3-g128 quantization in Figure~\\ref{fig:calib_set_ablation} (a). \\methodshort needs a much smaller calibration to reach a good quantized performance; it can achieve better perplexity using 10$\\times$ smaller calibration set compared to GPTQ (16 sequences \\emph{v.s.} 192 sequences).\n\n\\myparagraph{Robust to the calibration set distributions. }\nOur method is less sensitive to the calibration set distribution since we only measure the average activation scale from the calibration set, which is more generalizable across different dataset distributions. We further benchmarked the effect of the different calibration set distributions in Figure~\\ref{fig:calib_set_ablation}(b). We took two subsets from the Pile dataset~\\cite{gao2020pile}: PubMed Abstracts and Enron Emails~\\cite{klimt2004enron}. We use each of the subsets as the calibration set and evaluate the quantized model on both sets (the calibration and evaluation sets are split with no overlapping; we used 1k samples for evaluation). Overall, using the same calibration and evaluation distribution works the best (PubMed-PubMed, Enron-Enron). But when using a different calibration distribution (PubMed-Enron, Enron-PubMed), \\methodshort only increases the perplexity by 0.5-0.6, while GPTQ has 2.3-4.9 worse perplexity. This demonstrates the robustness of \\methodshort to the calibration set distribution. \n\n\\subsection{Speedup Evaluation}\n\\begin{table}\n    \\setlength{\\tabcolsep}{5pt}\n    \\small\n    \\centering\n    \\begin{tabular}{lcccc} \\toprule\n\\textbf{Model (Throughput$\\uparrow$)}          & Precision & A100 & 4090 & Orin \\\\ \\midrule\nVILA-7B      & FP16      & 81.6              & 58.5              & 11.5              \\\\\nVILA-7B-AWQ  & W4A16     & 155.3             & 168.1             & 35.6              \\\\ \\midrule\nVILA-13B     & FP16      & 48.5              & OOM               & 6.1               \\\\\nVILA-13B-AWQ & W4A16     & 102.1             & 99.0              & 17.5 \\\\\\bottomrule             \n\\end{tabular}\n    \\caption{\\system also enables seamless deployment of VILA~\\cite{lin2023vila}, a state-of-the-art visual-language model, on multiple GPU platforms. Leveraging our 4-bit AWQ quantization, \\system accelerates VILA-7B by up to \\textbf{3.1$\\times$} and VILA-13B by up to \\textbf{2.9$\\times$}.}\n    \\label{tab:vila_latency}\n\\end{table}\n\n\\myparagraph{Settings.} In Figure~\\ref{fig:kernel_speedup}, we demonstrate the system acceleration results from \\system. \\system optimizes both linear layers and layers that do not have quantized weights. We conduct benchmarking experiments on RTX 4090 and Jetson Orin following the protocol described in exllama~\\footnote{\\url{https://github.com/turboderp/exllama}}. We perform batch size = 1 inference for all LLMs using a fixed prompt length of 4 tokens. We generate 200 tokens for each inference run and calculate the median latency as the final result.\n\n\\myparagraph{Results.} As in Figure~\\ref{fig:kernel_speedup}(a), \\system brings \\textbf{2.7-3.9$\\times$} speedup to three families of LLMs (Llama-2, MPT and Falcon) on 4090 compared with the Huggingface FP16 implementation. For Llama-2-7B, we improve the inference speed from 52 tokens/s to 62 tokens/s through FP16 kernel fusion. On top of the stronger FP16 baseline, we further harvest \\textbf{3.1$\\times$} additional speedup from the fast quantized linear kernels. For Falcon-7B, the official implementation did not support KV cache correctly during the inference time, and thus it is significantly slower than other models. In this case, our FP16 optimizations bring about a larger speedup of \\textbf{1.6$\\times$}. On the laptop 4070 GPU with only 8GB memory, we are still able to run Llama-2-13B models at 33 tokens/s, while the FP16 implementation cannot fit 7B models. We also demonstrate visual-language model~\\cite{lin2023vila} acceleration results in Table \\ref{tab:vila_latency}. \\system brings about \\textbf{3$\\times$} speedup to both VILA-7B and VILA-13B on NVIDIA Jetson Orin. Notably, we implement the forward pass for all AWQ models using native PyTorch APIs, and this code is reused across various GPU architectures. Hence, \\system offers exceptional extensibility. %\n\n\\myparagraph{Comparisons against other systems.}\nWe compare \\system against existing edge LLM inference systems AutoGPTQ, llama.cpp and exllama in Figure~\\ref{fig:kernel_speedup_comparisons}. Our system achieves up to 1.7$\\times$ speedup over llama.cpp on Orin. Furthermore, llama.cpp and exllama exhibit limited adaptability, primarily tailored for LLaMA and Llama-2 models. In contrast, our \\system supports a wide range of applications, including StarCoder~\\cite{li2023starcoder}, StableCode (GPT-NeoX)~\\cite{black2022gpt}, Mistral~\\cite{jiang2023mistral}, and Falcon~\\cite{penedo2023refinedweb} while consistently delivering significant speedup over AutoGPTQ. \\system even democratizes LLM deployment on extremely resource-constrained Raspberry Pi 4B, achieving 0.7 tokens/s for 7B models. \n\n\\section{Conclusion}\nIn this work, we propose \\method (\\methodshort), a simple yet effective method for low-bit weight-only LLM compression.\nBased on the observation that weights are not equally important in LLMs, \\methodshort performs per-channel scaling to reduce the quantization loss of salient weights. \\methodshort does not over-fit the calibration set and preserves the generalist abilities of LLMs in various domains and modalities. It outperforms existing work on language modeling and is applicable to instruction-tuned LMs and multi-modal LMs. Our \\system system further translates the\ntheoretical memory savings achieved by \\methodshort into \\textbf{3.2-3.3$\\times$} measured speedups over the FP16 implementations from Huggingface on desktop and mobile GPUs, democratizing LLM deployment on the edge.\n\n\\section*{Acknowledgements}\nWe thank MIT AI Hardware Program, National Science Foundation, MIT-IBM Watson AI Lab, Amazon and MIT Science Hub, Microsoft Turing Academic Program, and Samsung for supporting this research.\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{QQQ: Quality Quattuor-Bit Quantization for Large Language Models}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\nQuantization is a proven effective method for compressing large language models. Although popular techniques like W8A8 and W4A16 effectively maintain model performance, they often fail to concurrently speed up the prefill and decoding stages of inference. W4A8 is a promising strategy to accelerate both of them while usually leads to a significant performance degradation.\nTo address these issues, we present QQQ, a \\textbf{Q}uality \\textbf{Q}uattuor-bit \\textbf{Q}uantization method with 4-bit weights and 8-bit activations. QQQ employs adaptive smoothing and Hessian-based compensation, significantly enhancing the performance of quantized models without extensive training.\nFurthermore, we meticulously engineer W4A8 GEMM kernels to increase inference speed. \nOur specialized per-channel W4A8 GEMM and per-group W4A8 GEMM achieve impressive speed increases of 3.67$\\times$ and 3.29 $\\times$ over FP16 GEMM.\nOur extensive experiments show that QQQ achieves performance on par with existing state-of-the-art LLM quantization methods while significantly accelerating inference, achieving speed boosts up to 2.24 $\\times$, 2.10$\\times$, and 1.25$\\times$ compared to FP16, W8A8, and W4A16, respectively. Code is available at \\url{https://github.com/HandH1998/QQQ}.\n\\end{abstract}\n\n\\section{Introduction}\nIn recent years, large language models (LLMs) have been widely applied in various tasks and shown excellent performance, such as OPT \\citep{zhang2022opt} and LLaMA series \\cite{touvron2023llama, touvron2023llama2}.\nThe LLMs usually have large number of parameters and long inference time, making it inapplicable to resource-limited devices and real-time scenarios.\nTherefore, it is crucial to reduce LLMs' storage and computation overhead while retaining their performance. \n\nQuantization is a crucial technique for reducing the memory and computational requirements of LLMs. It generally involves two main strategies: weight-activation and weight-only quantization. The W8A8 weight-activation quantization method, as demonstrated by SmoothQuant \\citep{xiao2023smoothquant}, compresses both weights and activations to 8-bit, leveraging specialized hardware like INT8 Tensor Cores to increase computational throughput. This method excels in enhancing the compute-bound prefill phase of LLM inference.\nWeight-only quantization, such as the W4A16 method seen in GPTQ \\citep{frantar2022gptq} and AWQ \\citep{lin2023awq}, reduces the weight precision to 4-bit while maintaining 16-bit activations. This approach effectively cuts memory bandwidth requirements and is particularly beneficial during the memory-bound decoding phase of LLM inference.\nAlthough these methods successfully retain model performance, they struggle to simultaneously accelerate both the prefill and decoding stages of LLM inference. The W4A8 weight-activation quantization approach, which can expedite both inference phases by utilizing efficient hardware and reducing memory bandwidth, often leads to a significant decrease in model performance.\nTherefore, the pursuit of a viable W4A8 quantization method that balances performance with speed remains an active area of research.\n\nIn response to these challenges, we introduce QQQ, a quality W4A8 quantization approach that excels in both model performance and inference speed. QQQ employs an adaptive smoothing mechanism for activation channels with significant outliers, while leaving other channels intact. This targeted smoothing allows for more effective activation quantization. Following the smoothing process, QQQ utilizes a Hessian-based compensation technique to offset the loss from weight quantization. Through these innovative strategies, QQQ significantly improves the performance of quantized models.\nTo achieve fast inference speed, we meticulously design specialized W4A8 GEMM kernels. These kernels are specifically tailored for both per-channel and per-group weight quantization, drastically enhancing throughput across various batch sizes.\nOur contributions compared to previous works are threefold:\n\n\\begin{itemize}\n\\item\nWe present QQQ, a quality W4A8 quantization method, that delivers performance on par with existing state-of-the-art LLM quantization methods.\n\\item \nWe develop innovative W4A8 GEMM kernels tailored for both per-channel weight and per-group weight quantization. These specialized kernels significantly enhance speed, with per-channel W4A8 GEMM and per-group W4A8 GEMM achieving speedups of 3.67$\\times$ and 3.29$\\times$ over FP16 GEMM, respectively.\n\\item \nWe integrate QQQ into vLLM \\citep{kwon2023efficient}, achieving impressive speed boosts of 2.24$\\times$, 2.10$\\times$, and 1.25$\\times$ over FP16, W8A8, and W4A16 Marlin \\citep{frantar2024marlin} implementation, respectively.\n\\end{itemize}\n\n\\section{Related Works}\\label{section:sec2}\n\n\\subsection{Large Language Models}\nIn recent times, large language models (LLMs) such as GPT-3 \\citep{brown2020language}, GLM \\citep{du2021glm}, OPT \\citep{zhang2022opt}, and the LLaMA series \\cite{touvron2023llama, touvron2023llama2} have revolutionized the field of natural language processing by delivering unprecedented performance enhancements across various tasks. Despite their impressive capabilities, LLMs come with a significant computational footprint due to their billions of parameters, which takes long inference time. This makes them less suitable for deployment on devices with limited resources or in applications requiring real-time responses. For example, the GPT-3 \\citep{brown2020language} model, with its colossal count of 175 billion parameters, demands a staggering 350 GB of memory for loading its parameters in FP16 format. This necessitates the use of at least five A100-80G GPUs solely for inference purposes.\n\\subsection{Quantization}\nCurrent strategies for quantizing LLMs fall into two main categories: weight-activation and weight-only quantization.\n\n\\textbf{Weight-activation quantization.} Weight-activation quantization compresses both weights and activations to lower bit representations such as W8A8, W4A8, W6A6, and W4A4. SmoothQuant \\citep{xiao2023smoothquant}, LLM.int8() \\citep{dettmers2208llm}, QUIK \\citep{ashkboos2023towards} and Outlier Suppression+ \\citep{wei2023outlier} all enhance quantized model performance by managing activation outliers. LLM.int8() \\citep{dettmers2208llm} and QUIK \\citep{ashkboos2023towards} employ mixed-precision decomposition, while SmoothQuant \\citep{xiao2023smoothquant} and Outlier Suppression+ \\citep{wei2023outlier} use channel-wise scaling. RPTQ \\citep{yuan2023rptq} mitigates the impact of range differences between channels by rearranging the channels and quantizing them in clusters. OmniQuant \\citep{shao2023omniquant} proposes learnable weight clipping and learnable equivalent transformation. Atom \\citep{zhao2023atom} attains high accuracy by applying a novel mixed-precision and fine-grained quantization process.  QuaRot \\citep{ashkboos2024quarot} removes outliers by rotating the inputs of the model using randomized Hadamard transformations.\n\n\\textbf{Weight-only quantization.} Weight-only quantization compresses model weights to lower bit precision (e.g., 4-bit or 2-bit) while keeping activations at higher precision (16-bit). {GPTQ \\citep{frantar2022gptq} adopts second-order information to minimize precision loss.} SpQR \\citep{dettmers2023spqr} and AWQ \\citep{lin2023awq} prioritize the preservation of important weights to reduce quantization errors, with SpQR applying mixed-precision and AWQ optimizing per-channel scaling. QuIP \\citep{chee2024quip} further quantizes weights to 2-bit based on the insight that model parameters should ideally be incoherent.\n\n\\section{Methodology}\\label{section:sec4}\nIn this section, we detail our quantization method, referred to as QQQ. As depicted in Figure \\ref{fig:fig1}, QQQ is a two-stage weight-activation quantization method.\nIt integrates adaptive smoothing and Hessian-based quantization compensation to enhance the performance of quantized models. QQQ adaptively smooths activation channels with significant outliers while leaving other channels intact, enabling better activation quantization.\nAfter smoothing the activations, QQQ tackles the challenges of weight quantization by employing Hessian-based compensation, which effectively minimizes the loss incurred during the quantization process.\nTo expedite practical inference, QQQ incorporates highly efficient W4A8 GEMMs tailored for both per-channel and per-group quantization. The quantization process will be further elaborated in the following subsections.\n\n\\begin{figure}[ht]\n\\centering\n\\includegraphics[scale=0.8]{images/QQQ.pdf}\n\\caption{Framework of QQQ.}\n\\label{fig:fig1}\n\\vspace{-0.1in}\n\\end{figure}\n\n\\subsection{Adaptive Smoothing}\nWe employ channel-wise smoothing to refine problematic activation channels with outliers, enhancing their suitability for quantization. As indicated by research \\citep{wei2023outlier}, smoothing only the activation channels with significant outliers while leaving other channels intact can enhance the performance of quantized models. Therefore, we further implement an adaptive smoothing strategy that identifies and selectively smooths only those channels that display significant outliers.\n\nSpecifically, we introduce an outlier threshold $\\sigma$, which is determined through a grid search within the range $[0, \\mbox{max}(\\mbox{abs}(\\mathbf{X}))]$. Activation channels that exhibit outliers exceeding this threshold $\\sigma$ are then selectively smoothed:\n\\begin{equation}\n    \\mathcal{T}=\\{\\,t\\,|\\,\\mbox{max}(\\mbox{abs}(\\mathbf{X}_{:, t}))<\\sigma \\},\n    \\label{eq:selct_channel}\n\\end{equation}\nwhere $\\mathcal{T}$ denotes the selected channel index for $\\mathbf{X}$.\n\nTo optimize the smoothing parameters, we aim to reduce the squared error between the product of activations and weights both before and after quantization:\n\\begin{equation}\n\\arg\\min_{\\mathbf{s}}\\|Q(\\mathbf{X}_{:,\\mathcal{T}} \\oslash \\mathbf{s})Q(\\mathbf{W}_{\\mathcal{T}, :} \\odot \\mathbf{s})- \\mathbf{X}_{:,\\mathcal{T}}\\mathbf{W}_{\\mathcal{T}, :}\\|_2^2,\n\\end{equation}\nwhere $Q$ denotes the quantization function;\n$\\mathbf{W}$ denotes the unquantized weight matrix;\n$\\mathbf{X}$ denotes the unquantized activation matrix;\n$\\mathbf{s}$ denotes the smoothing parameter;\n$\\oslash$ and $\\odot$ are element-wise division and multiplication, respectively.\n\n\\subsection{Hessian-based Quantization Compensation}\nAfter transferring the quantization challenge from activations to weights through adaptive smoothing, it becomes crucial to enhance the quantization technique for weights. We adopt a layer-wise quantization framework GPTQ \\citep{frantar2022gptq}, which advocates for a precise and efficient Hessian-based quantization compensation technique. This method involves an application of two key formulas, Eq.(\\ref{eq:eq2}) and Eq.(\\ref{eq:eq3}), which are instrumental in the iterative quantization of weights. Specifically, this algorithm adjusts the subset $F$ of full-precision weights by an increment $\\delta_F$, compensating for the quantization error induced by the quantized weights $Q(\\textbf{W}_i)$.\n\\begin{equation}\n    \\mathbf{W}_i = \\mathop{\\mbox{argmin}}\\limits_{\\mathbf{W}_i}\\frac{(Q(\\textbf{W}_i) - \\textbf{W}_i)^2}{[\\mathbf{H}_{F}^{-1}]_{ii}} \\label{eq:eq2}\n\\end{equation}\n\\begin{equation}\n    \\bm{\\delta}_F = - \\frac{\\textbf{W}_i - Q(\\textbf{W}_i)}{[\\mathbf{H}_{F}^{-1}]_{ii}} \\cdot (\\mathbf{H}_{F}^{-1})_{:,i} \\label{eq:eq3}\n\\end{equation}\nwhere $\\textbf{W}_i$ denotes the $i$-row of weight matrix $\\mathbf{W}$, $\\mathbf{H}_{F} = 2\\textbf{X}_F^\\mathsf{T}\\textbf{X}_F$ denotes the Hessian matrix.\n\n\\subsection{Novel W4A8 GEMM}\nWe carefully develop innovative W4A8 GEMMs tailored to optimize the inference speed. It's worth noting that our design exclusively supports symmetric quantization due to its superior hardware efficiency compared to asymmetric quantization.\n\nThe W4A8 GEMM design introduces two critical considerations beyond conventional GEMM implementations.\nFirstly, as GPUs support GEMM operations only for identical data types, it's necessary to convert INT4 weights into INT8 to utilize the capabilities of the INT8 Tensor Cores.\nSecondly, to avoid potential overflow during INT8 GEMM, the final output is typically stored in INT32 format, which subsequently requires a dequantization setp from INT32 to FP16.\n\nTo further reduce memory access, we integrate both the type conversion and dequantization process within the W4A8 GEMM routine.\nWe have different W4A8 GEMM configurations for per-channel weight quantization and per-group weight quantization, as they have different quantization schemes, which will be detailed in the following subsections.\n\n\\begin{figure}[ht]\n\\centering\n\\includegraphics[scale=0.65]{images/per-channel.pdf}\n\\caption{The W4A8 GEMM for per-channel weight quantization. $\\mathbf{A}$ denotes the activation matrix. $\\mathbf{W}$ denotes the weight matrix. $\\mathbf{s_A}$ denotes the per-token quantization scale of activations. $\\mathbf{s_W}$ denotes the per-channel quantization scale of weights. The superscript represents the data type. The green sections indicate \\green{data}, and the orange sections indicate \\orange{operations}.}\n\\label{fig:fig3}\n\\vspace{-0.1in}\n\\end{figure}\n\n\\subsubsection{Per-channel Weight Quantization}\nThe specialized W4A8 GEMM process for per-channel weight quantization is depicted in Figure \\ref{fig:fig3}. Our method involves an initial conversion of INT4 weights to INT8 before executing the \\texttt{INT8 GEMM}, followed by a dequantization step that transforms the INT32 output of the \\texttt{INT8 GEMM} into a FP16 result. To ensure high efficiency in the type conversion, we utilize the \\texttt{FastINT4toINT8} \\citep{li2023speed} technique. This process involves positioning the INT4 weight into the upper 4 bits of an INT8 by multiplying by 16, essentially performing a left shift by 4 bits. Once the GEMM computation is complete, we scale down the result by a factor of 16 to obtain the accurate output. This scaling operation is seamlessly integrated into the dequantization step, \\texttt{Dequant}, by adjusting the weight quantization scale offline by the same factor. \n\n\\begin{figure}[ht]\n\\centering\n\\includegraphics[scale=0.65]{images/per-group.pdf}\n\\caption{The W4A8 GEMM for per-group weight quantization. $\\mathbf{s_{W_c}}$ represents the per-channel quantization scale converting from dequantized FP16 weights back to INT8 weights. $\\mathbf{s_{W_g}^{*}}$ represents the fused per-group scale converting from INT4 weights to INT8 weights.}\n\\label{fig:fig4}\n\\vspace{-0.1in}\n\\end{figure}\n\n\\begin{figure}[ht]\n\\centering\n\\includegraphics[scale=0.6]{images/fuse_dequant_quant.pdf}\n\\caption{Design of \\texttt{FusedDequantQuant}. $\\mathbf{s_{W_g}}$ represents the per-group quantization scale converting from FP16 weights to INT4 weights.\nThe orange sections indicate \\orange{online operations}, and the blue sections indicate \\cyan{offline operations}.}\n\\label{fig:fig5}\n\\vspace{-0.1in}\n\\end{figure}\n\n\\subsubsection{Per-group Weight Quantization.}\nThe W4A8 GEMM tailored for per-group weight quantization is detailed in Figure \\ref{fig:fig4}. This approach involves a more complex set of operations prior to the \\texttt{INT8 GEMM} compared to the per-channel weight quantization. Although the conversion from INT4 to INT8 weights is still necessary, additional steps are required due to the unique per-group quantization scales that are not directly compatible with standard GEMM procedures.\n\nInitially, INT4 weights are converted to FP16 using the \\texttt{FastINT4toFP16} \\citep{kim2022says} method. Subsequently, each group's weight is dequantized by multiplying it by its respective scale. To re-quantize into INT8, the result is divided by the per-channel quantization scale. These two steps are elegantly merged into a single operation, \\texttt{FusedDequantQuant}, by dividing the per-group scale by the per-channel scale, as illustrated in Figure \\ref{fig:fig5}.\nThe final stage of the process involves converting the FP16 results of \\texttt{FusedDequantQuant} back into INT8 format to proceed with the \\texttt{INT8 GEMM}. Similar to the per-channel method, the INT32 output from the \\texttt{INT8 GEMM} undergoes dequantization to FP16.\n\n\\begin{figure}[ht]\n\\centering\n\\includegraphics[scale=0.6]{images/fast_fp16_to_int8.pdf}\n\\caption{Design of \\texttt{FastFP16toINT8}.}\n\\label{fig:fig6}\n\\vspace{-0.1in}\n\\end{figure}\n\nThe conventional instruction for converting FP16 to INT8 is significantly slow, which hinders the overall GEMM performance. To tackle this issue, we introduce a fast conversion technique, \\texttt{FastFP16toINT8}, as depicted in Figure \\ref{fig:fig6}. The conversion begins by adjusting the FP16 values, adding $128$ to shift them into the range of $[0.0, 255.0]$, which corresponds to the representational span of UINT8. This is followed by an addition of $1024$, effectively aligning the 8 bits of UINT8 within the lower segment of FP16's mantissa. The final step involves extracting these lower 8 bits from FP16 and applying an XOR operation with $0\\textrm{x}80$ to achieve the desired INT8 format. In practice, the fusion of the per-group quantization scale multiplication with the addition of the magic number $128 + 1024$ is executed through a single \\texttt{FMA} (Fused Multiply-Add) instruction. Furthermore, the process leverages two expedient bit manipulation instructions, \\texttt{PRMT} and \\texttt{XOR}, to finalize the conversion.\n\n\\begin{table*}[t]\n\\centering\n\\begin{small}\n\\renewcommand{\\arraystretch}{1.5}\n\\begin{tabular}{llccccccc}\n\\toprule\n\\multirow{2}*{\\#Bits} & \\multirow{2}*{Method} & \\multicolumn{3}{c}{LLaMA-1} & \\multicolumn{3}{c}{LLaMA-2} & LLaMA-3\\\\\n\\cline{3-9}\n&& 7B & 13B & 30B & 7B & 13B & 70B & 8B\\\\\n\\midrule\nFP16 & - & 5.68 & 5.09 & 4.10 & 5.47 & 4.88 & 3.32 & 6.14\\\\\n\\hline\nW8A8 & SmoothQuant & 5.78 & 5.17 & 4.28 & 5.58 & 4.93 & 3.39 & 6.25\\\\\n\\hline\n\\multirow{2}*{W4A16} & GPTQ-g128 & 5.83 & 5.20 & 4.22 & 5.63 & 4.99 & 3.43 & 6.56\\\\\n& AWQ-g128 & 5.78 & 5.19 & 4.21 & 5.60 & 4.97 & 3.41 & 6.54\\\\\n\\hline\nW4A4 & Atom-g128 & 6.25 & 5.52 & 4.61 & 6.12 & 5.31 & 3.73 & 7.76\\\\\n\\hline\n\\multirow{4}*{W4A8} & QoQ & 5.93 & 5.28 & 4.34 & 5.75 & 5.12 & 3.52 &6.89\\\\\n& QoQ-g128 & 5.89 & 5.25 & 4.28 & 5.70 & 5.08 & 3.47 & 6.76\\\\\n& QQQ & 6.19 & 5.43 & 4.61 & 5.95 & 5.21 & 3.68 & 7.41\\\\\n& QQQ-g128 & 5.87 & 5.24 & 4.30 & 5.71 & 5.01 & 3.50 & 6.64\\\\\n\\bottomrule\n\\end{tabular}\n\\end{small}\n\\caption{WikiText2 perplexity with 2048 sequence length. The lower is the better. '-g128' denotes applying per-group quantization on the weights and the group size is 128.\n}\n\\label{tab:tab1}\n\\end{table*}\n\n\\begin{table*}[t]\n\\centering\n\\begin{small}\n\\renewcommand{\\arraystretch}{1.5}\n\\begin{tabular}{lllcccccc}\n\\toprule\nLLaMA-2 & \\#Bits & Method & PIQA & ARC-e & ARC-c & HellaSwag & WinoGrande & Avg.\\\\\n\\midrule\n\\multirow{4}*{7B} & FP16 & - & 79.05 & 74.58 & 46.25 & 76.05 & 68.98 & 68.98\\\\\n\\cline{2-9}\n& W4A4 & Atom-g128 & 75.14 & 52.99 & 38.40 & 69.37 & 62.75 & 59.73\\\\\n\\cline{2-9}\n& \\multirow{4}*{W4A8} & QoQ & 77.64 & 72.81 & 43.60 & 74.00 & 68.03 & 67.22 \\\\\n&& QoQ-g128 & 78.07 & \\textbf{73.32} & \\textbf{44.80} & \\textbf{74.98} & \\textbf{68.59} & \\textbf{67.95}\\\\\n&& QQQ & 77.42 & 69.15 & 42.15 & 73.54 & 65.98 & 65.65\\\\\n&& QQQ-g128 & \\textbf{78.51} & 72.94 & 44.37 & 74.53 & 67.01 & 67.47\\\\\n\\hline\n\\multirow{4}*{13B} & FP16 & - & 80.52 & 77.44 & 49.06 & 79.38 & 72.22 & 71.72\\\\\n\\cline{2-9}\n& W4A4 & Atom-g128 & 76.50 & 57.49 & 42.32 & 73.84 & 67.40 & 63.51\\\\\n\\cline{2-9}\n& \\multirow{4}*{W4A8} & QoQ & 79.71 & 75.97 & 48.38 & 77.80 & 70.96 & 70.56\\\\\n&& QoQ-g128 & 79.43 & \\textbf{77.06} & \\textbf{48.81} & 78.35 & 70.48 & 70.83\\\\\n&& QQQ & 79.43 & 74.75 & 48.12 & 77.27 & 70.32 & 69.98\\\\\n&& QQQ-g128 & \\textbf{79.98} & 76.64 & 48.55 & \\textbf{78.63} & \\textbf{71.82} & \\textbf{71.13}\\\\\n\\bottomrule\n\\end{tabular}\n\\end{small}\n\\caption{Zero-shot accuracy on five common sense tasks with 2048 sequence length. The higher is the better.\n}\n\\label{tab:tab2}\n\\end{table*}\n\n\\section{Experiments}\\label{section:sec5}\n\\subsection{Setups}\nIn this section, we introduce the experimental configurations in models, datasets, algorithm and inference system.\n\n\\textbf{Models and Datasets.} We conduct the experiments using the LLaMA series \\cite{touvron2023llama, touvron2023llama2}. We evaluate the model performance on language modeling and zero-shot tasks. For the language modeling aspect, we utilize the WikiText2 \\citep{merity2016pointer} dataset to measure perplexity. For the zero-shot tasks, we extend the evaluation to a variety of datasets, including PIQA \\citep{bisk2020piqa}, HellaSwag \\citep{zellers2019hellaswag}, WinoGrande \\citep{sakaguchi2021winogrande}, and ARC \\citep{clark2018think}, utilizing the \\texttt{lm\\_eval} \\citep{eval-harness} toolkit for a comprehensive analysis. Additionally, we select a subset of 128 sequences from the PILE \\citep{gao2020pile} dataset to facilitate calibration.\n\n\\textbf{Algorithm.} QQQ is specifically designed to focus on per-token quantization for activations and supports both per-channel and per-group quantization for weights. To optimize inference speed, QQQ exclusively uses symmetric quantization for both activations and weights, thus avoiding the additional computational overhead associated with asymmetric quantization.\n\n\\textbf{Inference System.} The inference system that we have develop to support QQQ is based on vLLM\\footnote{\\url{https://github.com/vllm-project/vllm}} \\citep{kwon2023efficient} v0.4.1. The system's capabilities are further enhanced by the implementation of W4A8 GEMM kernels, which are based on the advanced Marlin\\footnote{\\url{https://github.com/IST-DASLab/marlin}} \\citep{frantar2024marlin} kernels. All experimental procedures are conducted on NVIDIA A100 80G GPUs under PyTorch 2.1.2 with CUDA 11.8.\n\n\\subsection{Model Performance}\nWe perform a comprehensive comparison of QQQ method against leading LLM quantization methods, including SmoothQuant \\citep{xiao2023smoothquant}, GPTQ \\citep{frantar2022gptq}, AWQ \\citep{lin2023awq}, Atom \\citep{zhao2023atom}, and the recently introduced W4A8 method QoQ \\citep{lin2024qserve}. By default, weights employ per-channel quantization. GPTQ, AWQ, and QoQ utilize asymmetric quantization for weights, whereas SmoothQuant, Atom, and QQQ opt for symmetric quantization. In terms of activation quantization, Atom stands out by implementing per-group quantization, while the remaining methods, including QQQ, employ per-token quantization. Across all methods under consideration, activations apply symmetric quantization.\n\nTable \\ref{tab:tab1} presents a detailed perplexity comparison on the WikiText2 dataset. The per-group QQQ demonstrates competitive performance with with W8A8 SmoothQuant, W4A16 GPTQ, and W4A16 AWQ across various models. For example, per-group QQQ only increases perplexity by up to 0.13 on LLaMA-2-13B compared with them. When benchmarked against W4A4 Atom, both the per-channel and per-group QQQ consistently outperform Atom on all tested models. For instance, per-group QQQ achieves a significant reduction in perplexity by 0.41 on LLaMA-2-13B when compared to Atom. In comparison with W4A8 QoQ, per-group QQQ outperforms per-group QoQ on the majority of models, exemplified by a 0.12 decrease in perplexity on LLaMA-3-8B. However, it is observed that per-channel QQQ falls slightly behind per-channel QoQ. This divergence is hypothesized to be due to QoQ's application of asymmetric quantization on weights, as opposed to the symmetric quantization employed by QQQ.\n\nThe zero-shot accuracy evaluation across five common sense tasks is showed in Table \\ref{tab:tab2}. The QQQ method exhibits a significant performance edge over Atom across all tasks and model sizes. For instance, the per-group QQQ achieves a 4.79\\% higher accuracy than Atom on the HellaSwag task when utilizing the LLaMA-2-13B model. In comparison with QoQ, QQQ demonstrates comparable results, especially when per-group quantization is applied. For instance, QQQ shows a marginal improvement of 0.3\\% over QoQ with the LLaMA-2-13B model on average.\n\n\\begin{figure*}[ht]\n\\centering\n\\includegraphics[scale=0.53]{images/speedup.pdf}\n\\caption{Same-batch throughput comparison of quantized LLaMA-2 models under various batch size.}\n\\label{fig:fig7}\n\\vspace{-0.1in}\n\\end{figure*}\n\n\\subsection{Speedup}\nIn the evaluation of inference efficiency, QQQ is benchmarked against a suite of quantization methods including SmoothQuant\\footnote{\\url{https://github.com/vllm-project/vllm/pull/1508}} \\citep{xiao2023smoothquant}, AWQ \\citep{lin2023awq}, and GPTQ \\citep{frantar2022gptq}, all of which are integrated within the vLLM \\citep{kwon2023efficient}. For optimal inference speed, we leverage the Marlin \\citep{frantar2024marlin} implementation of GPTQ. All methods are tested under conditions that enable continuous batching and paged attention. The benchmarks are conducted with an input sequence length of 1024 and an output sequence length of 128.\n\nThe throughput comparison outlined in Figure \\ref{fig:fig7} indicates that QQQ outperforms the alternative methods across a range of batch sizes in most cases. Specifically, QQQ attains a substantial speed increase, reaching up to 2.24$\\times$, 2.10$\\times$, 1.59$\\times$, and 1.25$\\times$ faster than FP16, SmoothQuant, AWQ, and Marlin on the LLaMA-2-13B model, respectively. When contrasted with W4A16 AWQ and W4A16 Marlin, QQQ displays a comparable inference speed at smaller batch sizes but gains a significant advantage as the batch size increases. Against W8A8 SmoothQuant, QQQ stands out with an approximate 2.0$\\times$ speedup for smaller batches and maintains a similar level of acceleration even at a batch size of 64. Moreover, QQQ's performance enhancement is more pronounced on larger models, as evidenced by the greater speedup achieved on LLaMA-2-13B compared to LLaMA-2-7B.\n\n\\begin{figure}[ht]\n\\centering\n\\includegraphics[scale=0.50]{images/gemm.pdf}\n\\caption{Speedup over PyTorch FP16 GEMM (Calling CUTLASS) of all GEMMs under different numbers of input tokens. The weight matrix size is ($N=8192, K=21760$).}\n\\label{fig:fig8}\n\\vspace{-0.1in}\n\\end{figure}\n\n\\subsection{W4A8 GEMM Studies} \nWe conduct a series of experiments to demonstrate the capabilities of our W4A8 GEMMs. By keeping the weight matrix size constant and varying the number of input tokens, we evaluate the speed improvements over the PyTorch FP16 GEMM benchmark. The results are depicted in Figure \\ref{fig:fig8}.\n\nOur W4A8 GEMMs demonstrate impressive speedups, achieving up to 3.67$\\times$, 2.51$\\times$, and 1.71$\\times$ faster performance compared to PyTorch FP16 GEMM, W8A8 GEMM, and W4A16 Marlin GEMM, respectively. Specifically, W4A8 GEMMs show a significant advantage when the token count is below 64 compared to W8A8 GEMM. With an increasing number of tokens, the per-group W4A8 GEMM maintains a similar level of efficiency, whereas the per-channel W4A8 GEMM tends to be slower. This discrepancy is attributed to the higher type conversion overhead in per-group W4A8 GEMM.\nCompared to Marlin GEMM, our GEMMs demonstrate superior performance under the majority of token counts, especially per-group GEMM.\nAs the token count rises, Marlin GEMM's performance gradually declines, falling behind even the FP16 GEMM, while our GEMMs consistently outperform the FP16 benchmark. This clearly indicates the superior efficiency of INT8 Tensor Cores over FP16 Tensor Cores in compute-bound scenarios.\n\n\\subsection{Ablation Studies}\n\n\\begin{table}[t]\n\\centering\n\\begin{small}\n\\renewcommand{\\arraystretch}{1.3}\n\\setlength{\\tabcolsep}{0.8mm}{\n\\begin{tabular}{llccc}\n\\toprule\nGranularity & Model & B & B+AS & B+AS+GPTQ\\\\\n\\midrule\n\\multirow{7}*{Per-channel} & LLaMA-1-7B & 6.93 & 6.77 & \\textbf{6.19} \\\\\n\\cline{2-5}\n& LLaMA-1-13B & 5.84 & 5.84 & \\textbf{5.43}\\\\\n\\cline{2-5}\n& LLaMA-1-30B & 5.04 & 4.89 & \\textbf{4.61}\\\\\n\\cline{2-5}\n& LLaMA-2-7B & 7.36 & 7.29 & \\textbf{5.95}\\\\\n\\cline{2-5}\n& LLaMA-2-13B & 5.47 & 5.43 & \\textbf{5.21}\\\\\n\\cline{2-5}\n& LLaMA-2-70B & 4.07 & 3.88 & \\textbf{3.68}\\\\\n\\cline{2-5}\n& LLaMA-3-8B & 10.88 & 9.46 & \\textbf{7.41}\\\\\n\\hline\n\\multirow{7}*{Per-group} & LLaMA-1-7B & 6.16 & 5.96 & \\textbf{5.87}\\\\\n\\cline{2-5}\n& LLaMA-1-13B & 5.43 & 5.32 & \\textbf{5.24}\\\\\n\\cline{2-5}\n& LLaMA-1-30B & 4.51 & 4.34 & \\textbf{4.30}\\\\\n\\cline{2-5}\n& LLaMA-2-7B & 5.86 & 5.77 & \\textbf{5.71}\\\\\n\\cline{2-5}\n& LLaMA-2-13B & 5.10 & 5.06 & \\textbf{5.01}\\\\\n\\cline{2-5}\n& LLaMA-2-70B & 3.67 & 3.52 & \\textbf{3.50}\\\\\n\\cline{2-5}\n& LLaMA-3-8B & 7.11 & 6.96 & \\textbf{6.64}\\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{small}\n\\caption{WikiText2 perplexity of LLaMA series when applying various quantization techniques. 'B' represents the Baseline, which refers to the vanilla W4A8 method without any additional quantization techniques. 'AS' denotes Adaptive Smoothing.\n}\n\\label{tab:tab3}\n\\end{table}\n\n\\subsubsection{Impact of Quantization Techniques}\nHere we examine the impact of different quantization techniques on model performance. We use the standard W4A8 method, which lacks any specialized quantization techniques, as the baseline for comparison. The perplexity results for the LLaMA series are summarized in Table \\ref{tab:tab3}. Overall, the combination of adaptive smoothing with GPTQ proves to be the most effective method, yielding the lowest perplexity scores. For instance, this approach reduces perplexity by 0.47 in per-group quantization compared with the baseline.\n\n\\begin{figure}[ht]\n\\centering\n\\includegraphics[scale=0.5]{images/fastfp16toint8_line.pdf}\n\\caption{Speedup over PyTorch FP16 GEMM (Calling CUTLASS) of two per-group W4A8 GEMMs under different numbers of input tokens. The weight matrix size is ($N=8192, K=21760$).}\n\\label{fig:fig9}\n\\vspace{-0.15in}\n\\end{figure}\n\\subsubsection{Impact of FastFP16toINT8}\nHere we explore the performance enhancements achieved in per-group W4A8 GEMM through the implementation of the \\texttt{FastFP16toINT8} conversion technique. We benchmark the optimized W4A8 GEMM against the conventional W4A8 GEMM, which utilizes the standard FP16 to INT8 conversion instruction. As illustrated in Figure \\ref{fig:fig9}, the GEMM utilizing \\texttt{FastFP16toINT8} demonstrates a significant increase in speedup compared to its counterpart without this optimization, across a range of input tokens. The \\texttt{FastFP16toINT8} technique can accelerate per-group W4A8 GEMM by up to 2.02$\\times$. This finding confirms that the FP16 to INT8 conversion process is a critical bottleneck, and our \\texttt{FastFP16toINT8} effectively enhances the efficiency of per-group W4A8 GEMM by mitigating this constraint.\n\n\\section{Conclusion}\\label{section:sec6}\nIn this paper, we propose a quality W4A8 quantization method optimized for hardware efficiency. QQQ leverages the power of adaptive smoothing combined with Hessian-based quantization compensation to significantly improve the performance of quantized models. In addition, we meticulously develop W4A8 GEMM kernels to maximize the inference speed. Through extensive experiments, QQQ demonstrates superior performance over existing state-of-the-art LLM quantization methods in terms of both performance and speed. We believe QQQ can serve as a powerful tool that opens up access to the deployment and application of LLMs.\n\n\\section*{Limitations}\nQQQ method has two limitations: 1) The two-stage quantization method requires more time for quantizing models. 2) The mixed-precision GEMMs only support 4-bit weights for now.\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2210.17323v2.tex",
        "arXiv-2306.00978v5.tex",
        "arXiv-2406.09904v3.tex"
    ],
    "group_id": "group_9",
    "response": "### Title: Advances in Post-Training Quantization for Large Language Models\n\n### Introduction\n\nLarge Language Models (LLMs) from the Transformer family, such as Generative Pre-trained Transformers (GPT) and OPT, have revolutionized natural language processing (NLP) by achieving unprecedented performance on complex language tasks. However, their massive size and computational requirements pose significant challenges for practical deployment, particularly on devices with limited resources. The computational and storage costs associated with these models are extremely high, necessitating the development of efficient model compression techniques. Model compression, including quantization and pruning, is a standard approach to mitigate these costs. Quantization, in particular, reduces the bitwidth of model weights, thereby decreasing memory usage and accelerating inference. Post-training quantization (PTQ) methods, which do not require retraining the model, are especially appealing for compressing large models. However, existing PTQ methods struggle to maintain high accuracy while achieving substantial compression, particularly for models with billions of parameters. This summary explores recent advancements in PTQ methods for LLMs, focusing on GPTQ, AWQ, and QQQ, and discusses their contributions, commonalities, and differences.\n\n### Main Content of Each Paper\n\n#### GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers\n\nGPTQ introduces a novel one-shot weight quantization method for LLMs, leveraging approximate second-order information to achieve high accuracy and efficiency. The method quantizes weights layer-by-layer, using a matrix of quantized weights $\\mathbf{\\widehat{W}}$ to minimize the squared error relative to the full-precision layer output. GPTQ's key innovation lies in its ability to handle the quantization of weights in large blocks, significantly reducing the runtime compared to previous methods. This approach is particularly effective for models with billions of parameters, such as OPT-175B and BLOOM-176B, which can be quantized to 3 or 4 bits per weight in approximately four GPU hours. The quantized models maintain minimal accuracy loss, as measured by perplexity, and can be executed on a single GPU, offering speedups of around 2x on high-end GPUs (NVIDIA A100) and 4x on more cost-effective ones (NVIDIA A6000). GPTQ's implementation in Pytorch is available at \\url{https://github.com/IST-DASLab/gptq}.\n\n#### AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration\n\nAWQ proposes an activation-aware weight quantization method for LLMs, designed to be hardware-friendly and applicable to a wide range of edge devices. The method identifies a small fraction of salient weights that are crucial for model performance and scales them up to reduce quantization error. AWQ does not rely on backpropagation or reconstruction, ensuring that it generalizes well across different domains and modalities without overfitting to the calibration set. The authors also introduce \\system, an efficient inference framework that supports 4-bit quantized LLMs, offering speedups of up to 3-4x over FP16 implementations on desktop and mobile GPUs. AWQ's effectiveness is demonstrated through experiments on various language models and multi-modal models, showing that it can maintain high performance even under extreme low-bit quantization (INT2-g64).\n\n#### QQQ: Quality Quattuor-Bit Quantization for Large Language Models\n\nQQQ presents a quality W4A8 (4-bit weights, 8-bit activations) quantization method for LLMs, combining adaptive smoothing and Hessian-based compensation to enhance model performance. The method selectively smooths problematic activation channels with outliers while leaving other channels intact, enabling better activation quantization. Additionally, QQQ employs Hessian-based compensation to minimize quantization errors during weight quantization. The authors develop specialized W4A8 GEMM (General Matrix Multiply) kernels tailored for both per-channel and per-group quantization, achieving speedups of up to 3.67x and 3.29x over FP16 GEMM. QQQ is integrated into the vLLM inference system, demonstrating significant performance improvements over existing methods like SmoothQuant, GPTQ, and AWQ. The method is particularly effective for larger models and higher batch sizes, highlighting its potential for real-time applications.\n\n### Commonalities and Innovations\n\nAll three papers focus on reducing the bitwidth of weights in LLMs to achieve significant memory savings and speedups in inference. GPTQ, AWQ, and QQQ each introduce innovative techniques to address the challenges of quantizing large models while maintaining high accuracy. GPTQ uses second-order information to minimize the discrepancy between the original and quantized layer outputs, while AWQ identifies and scales salient weights to reduce quantization error. QQQ combines adaptive smoothing and Hessian-based compensation to enhance the performance of quantized models, and develops specialized W4A8 GEMM kernels to optimize inference speed.\n\n#### Commonalities\n\n1. **Focus on Low-bit Quantization:** Each method aims to reduce the bitwidth of weights to 3, 4, or even lower bits, such as 2.5 bits, to achieve significant memory savings and speedups.\n2. **Layer-wise Quantization:** All methods perform quantization layer-by-layer, ensuring that the quantization process is both accurate and efficient.\n3. **Post-training Quantization:** The methods are designed to be applied after the model is fully trained, avoiding the need for extensive retraining or finetuning.\n4. **Efficient Implementation:** Each paper provides an efficient implementation of their quantization method, with GPTQ and AWQ offering Pytorch implementations, and QQQ integrating into vLLM with specialized GEMM kernels.\n\n#### Innovations\n\n1. **GPTQ:** Introduces a block-wise quantization approach that leverages second-order information to achieve high accuracy and efficiency, even for models with billions of parameters.\n2. **AWQ:** Proposes an activation-aware scaling method to protect salient weights without using mixed precision, ensuring hardware efficiency and better generalization across different domains.\n3. **QQQ:** Combines adaptive smoothing and Hessian-based compensation to enhance model performance, and develops specialized W4A8 GEMM kernels to optimize inference speed, particularly for larger models and higher batch sizes.\n\n### Comparison of Results\n\n#### Perplexity and Accuracy\n\n- **GPTQ:** Achieves minimal perplexity increase (0.14 points for 4-bit and 0.3-0.5 points for 3-bit) on the largest models (OPT-175B and BLOOM-176B). It outperforms round-to-nearest (RTN) quantization, which fails to maintain reasonable perplexity at 3-bit.\n- **AWQ:** Demonstrates superior perplexity results compared to RTN and GPTQ, especially when scaling salient weights. For example, on OPT-6.7B, AWQ achieves a perplexity of 11.39 for 4-bit quantization, significantly lower than RTN's 23.54.\n- **QQQ:** Maintains competitive perplexity scores with existing methods like SmoothQuant, GPTQ, and AWQ, and outperforms Atom on all tested models. For instance, on LLaMA-2-13B, QQQ achieves a perplexity of 5.21 for 4-bit quantization, compared to Atom's 5.43.\n\n#### Speedup\n\n- **GPTQ:** Provides speedups of approximately 2x on high-end GPUs (NVIDIA A100) and 4x on more cost-effective ones (NVIDIA A6000) for generative tasks. This is due to reduced memory movement, allowing weight matrices to fit into GPU memory.\n- **AWQ:** Offers speedups of up to 3-4x over FP16 implementations on desktop and mobile GPUs, thanks to efficient weight packing and kernel fusion. AWQ also facilitates the deployment of large models on resource-constrained devices, such as a single NVIDIA Jetson Orin for models like LLaMA-2-70B.\n- **QQQ:** Achieves up to 2.24x, 2.10x, 1.59x, and 1.25x speedup over FP16, SmoothQuant, AWQ, and Marlin, respectively, on the LLaMA-2-13B model. The speedup is more pronounced for larger models and higher batch sizes.\n\n### Conclusion\n\nThe three papers present significant advancements in the field of post-training quantization for large language models. GPTQ, AWQ, and QQQ each offer unique solutions to the challenge of compressing large models while maintaining high accuracy. GPTQ's block-wise quantization approach and efficient Hessian-based compensation enable it to quantize models with billions of parameters to 3 or 4 bits with negligible accuracy loss. AWQ's activation-aware scaling method ensures hardware efficiency and robustness across different domains, while QQQ combines adaptive smoothing and Hessian-based compensation to further enhance model performance and develops specialized GEMM kernels for faster inference.\n\nThe main findings from these papers suggest that post-training quantization can achieve substantial compression and speedup without significant loss of accuracy. Future research directions could include extending these methods to support activation quantization, exploring mixed-precision computation, and further optimizing inference frameworks for real-time applications. Additionally, understanding the impact of quantization on secondary measures, such as bias and generalization, remains a critical area for further investigation.\n\n### Acknowledgements\n\nThe authors of these papers acknowledge the support of various organizations, including the European Research Council, NVIDIA, MIT, and Samsung, for their contributions to the development and implementation of these quantization methods. Their work not only advances the technical capabilities of LLMs but also democratizes their deployment on a wider range of devices, making these powerful models more accessible to researchers and practitioners alike."
}