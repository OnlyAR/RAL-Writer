{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nIn this work, we present a novel method to tackle the token generation challenge in Vision Language Models (VLMs) for video and image understanding, called LLaMA-VID. \nCurrent VLMs, while proficient in tasks like image captioning and visual question answering, face computational burdens when processing long videos due to the excessive visual tokens. \nLLaMA-VID addresses this issue by representing each frame with two distinct tokens, namely context token and content token. \nThe context token encodes the overall image context based on user input, whereas the content token encapsulates visual cues in each frame. \nThis dual-token strategy significantly reduces the overload of long videos while preserving critical information. \nGenerally, LLaMA-VID empowers existing frameworks to support hour-long videos and pushes their upper limit with an extra context token. \nIt is proved to surpass previous methods on most of video- or image-based benchmarks. \nCode is available at~\\href{https://github.com/dvlab-research/LLaMA-VID}{https://github.com/dvlab-research/LLaMA-VID}.\n\n\\end{abstract}\n\n\\section{Introduction}\nLarge Language Models (LLMs)~\\cite{ChatGPT,zhang2022opt,llama}, through their capacity to generate contextually accurate responses, have significantly advanced the field of AI.\nDrawing from the strengths of LLMs, Vision Language Models (VLMs)~\\cite{instructblip,llava,GPT4} have been developed to extend these capabilities to visual data, demonstrating their adeptness in tasks like image captioning and visual question answering. \nHowever, a substantial challenge emerges in the context of long video, where an excessive number of tokens are required to represent consecutive frames. \nThe computational demands escalate with the video length, thereby constraining the practical application of VLMs for extensive videos.\n\nRecently, several approaches have been proposed to handle videos, moving beyond image-only VLMs.\nThese methods aim to alleviate the token issue by utilizing representative queries~\\cite{videochat,videollama} or applying temporal compression~\\cite{videochatgpt,luo2023valley}.\nDespite these efforts, the challenge of long videos remains unresolved. \nThe primary obstacle stems from the excessive number of tokens required for each video frame. \nFor instance, models like BLIP~\\cite{blip2,instructblip} and LLaVA~\\cite{llava} require 32 and over 256 tokens respectively for a single image. \nA video containing 10K frames would thus necessitate over 320K tokens, exceeding the capacity of current VLMs. \nFurthermore, simple temporal compression can significantly damage the representation over long-term intervals. \nThis drawback hampers their performance, thereby underscoring the need for a robust solution.\n\n\\begin{figure}[t!]\n\\centering\n\\includegraphics[width=0.89\\linewidth]{Figure/intro.pdf} \n\\caption{\nThe proposed LLaMA-VID achieves leading performance on most of benchmarks with 7B LLMs. The video-based and image-based benchmarks are noted in \\textcolor{blue}{blue} and \\textcolor{purple}{purple} color, respectively.\nPlease refer to Tables~\\ref{tab:main_video},~\\ref{tab:main_video_gen}, and~\\ref{tab:main_img} for more details.\n}\n\\label{fig:intro}\n\\end{figure}\n\nIn this work, we present LLaMA-VID, a novel approach to effectively manage the token generation issue in long videos. \nOur core idea is to represent each video frame with two distinct tokens: {\\em context token} and {\\em content token}. \nThe context token is designed to encode the overall context of the image based on user input, which efficiently condenses the broader picture into {\\em a single token}. \nSimultaneously, the content token captures finer aspects of each frame. \nAccording to computational constraints, the length of content token can be extended to include more details, {\\em e.g.}, 1 token/frame for video input and beyond 256 token/frame for single image. \nIn this way, the overload of long videos can be significantly reduced without sacrificing critical information.\n\nIn particular, our method employs a dual-token generation strategy that is both efficient and effective. \nFor each frame, we first extract image features using a pre-trained vision transformer~\\cite{vit}, akin to other VLMs~\\cite{instructblip,llava}. \nThe key question is how to generate the context-related token according to user instructions.\nWe provide the solution by leveraging the cross-modality design~\\cite{devlin2018bert,blip2} for instruction-guided queries, which carry the interactive intention from users.\nFor {\\em context token}, these queries interact with previously generated image features in the designed attention module, termed as context attention.\nTo generate {\\em content token}, the image features are average pooled to formulate tokens that adapt to different settings.\nFor instance, global pooling is adopted to maintain efficiency for video input while details are preserved with more tokens for single image input. \nThe context and content tokens are subsequently projected to the space of LLMs with simple linear layers for final prediction.\nFurthermore, to better support hour-long videos in VLMs, we construct an instruction-based dataset that contains 9K movie-level conversations for plot reasoning and detail understanding.\n\nGenerally, LLaMA-VID can be distinguished from two aspects.\nOn one hand, with the dual-token paradigm, each frame can be efficiently encoded with only two tokens, which empowers existing LLMs to support long videos.\nOn the other hand, the context token aggregates the most informative feature of each image, which further extends the upper limit of VLMs with an extra token. \n\nThe overall framework, dubbed LLaMA-VID, can be easily instantiated with various decoders and LLMs, as elaborated in Section~\\ref{sec:method}.\nExtensive empirical studies are conducted in Section~\\ref{sec:experiment} to reveal the effectiveness of each component.\nRemarkably, our model can complete training within 2 days on a single machine with 8$\\times$A100 GPUs, and it outperforms previous leading methods on most of video- and image-based benchmarks, as shown in Figure~\\ref{fig:intro}.\n\n\\section{Related Work}~\\label{sec:related}\nIn this section, we first review large language models and delve into recent advances in vision language models.\n\\subsection{Large Language Models}\nThe field of Natural Language Processing (NLP) has witnessed tremendous advancements with the evolution of LLMs.\nTransformer~\\cite{vaswani2017attention} marked a pivotal milestone, with subsequent language models~\\cite{devlin2018bert,liu2019roberta,zhang2022opt} demonstrating remarkable capabilities.\nGPT~\\cite{brown2020language} revolutionized this field by utilizing generative pre-trained transformers for auto-regressive prediction, which is proved to be a potent language modeling paradigm. \nRecent groundbreaking works, such as ChatGPT~\\cite{ChatGPT}, GPT-4~\\cite{GPT4}, and LLaMA~\\cite{llama}, have pushed the boundaries even further. \nTrained on vast amounts of text data, these models exhibit exceptional capabilities in complex linguistic tasks.\nTo leverage the potential of pre-trained LLMs, instruction tuning~\\cite{wei2021finetuned,ouyang2022training} is a crucial component for high-quality output.\nThis strategy is widely adopted in open-source models like Alpaca~\\cite{alpaca} and Vicuna~\\cite{vicuna}, which improve over LLaMA~\\cite{llama} using specially designed instruction pairs. \nThere are also researches~\\cite{visualchatgpt,gpt4tools} that utilize the reasoning ability of LLMs and invoke pre-defined tools for visual applications.\nDifferent from them, we collect multi-modality instruction data that contains text, images, and videos in this work, which is employed to empower LLMs for long video processing.\n\n\\subsection{Vision Language Models}\nThe advancements in computer vision and NLP have led to the emergence of vision-language models (VLMs) that integrate vision models with language models for cross-modality understanding~\\cite{cococap,msrvtt} and reasoning~\\cite{vqav2,scienceqa,lai2023lisa}.\nPioneering large-scale VLMs like CLIP~\\cite{CLIP} and ALIGN~\\cite{ALIGN} have extended language models to vision-language tasks. \nThe recent progress has seen an increasing focus on leveraging the power of LLMs.\nNotably, Flamingo~\\cite{flamingo} and BLIP-2~\\cite{blip2} utilize web-scale image-text pairs for cross-modality alignment, thereby enhancing learning performance.\nTo further exploit the potential of such pre-trained models, InstructBLIP~\\cite{instructblip} and MiniGPT-4~\\cite{minigpt4} construct high-quality instruction pairs based on BLIP-2 and achieve superior results. \nSimultaneously, LLaVA~\\cite{llava} employs a simple linear projector with a few learnable parameters to align the image and text space of LLaMA.\nGiven the tailored instruction data, this straightforward approach demonstrates strong capabilities.\nTo support video understanding in LLMs, Video-LLaMA~\\cite{videollama} and VideoChat~\\cite{videochat} attempt to utilize BLIP-2 for video embedding extraction, while Video-ChatGPT~\\cite{videochatgpt} proposes spatial and temporal pooling for video features.\nHowever, given the substantial number of tokens required for each frame, LLMs encounter significant challenges when processing extensive video sequences. \nIt prevents previous work from representing long video sequences that exceed a duration of one hour in LLMs.\nTo solve the issue, we propose to efficiently encode each frame with only 2 tokens, which supports long video understanding in existing LLMs.\n\n\\begin{figure*}[t!]\n\\centering\n\\includegraphics[width=0.98\\linewidth]{Figure/main.pdf} \n\\caption{\nThe framework of LLaMA-VID.\nWith user directive, LLaMA-VID operates by taking either a {\\em single image} or {\\em video frames} as input, and generates responses from LLM.\nThe process initiates with a visual encoder that transforms input frames into the visual embedding.\nThen, the text decoder produces text queries based on the user input.\nIn context attention, the text query aggregates text-related visual cues from the visual embedding.\nFor efficiency, an option is provided to downsample the visual embedding to various token sizes, or even to a single token.\nThe text-guided {\\em context token} and the visually-enriched {\\em content token} are then formulated using a linear projector to represent each frame at time $t$.\nFinally, the LLM takes the user directive and all visual tokens as input and gives responses.\n}\n\\label{fig:main}\n\\end{figure*}\n\n\\section{LLaMA-VID}~\\label{sec:method}\nThe framework of LLaMA-VID is conceptually simple: encoder and decoder are adopted to produce visual embedding and text-guided features, respectively; \ncontext token and content token are transformed with the tailored token generation strategy; \ninstruction tuning is designed to unleash the potential of LLMs for image and video.\n\n\\subsection{Encoder and Decoder}~\\label{sec:sub_encoder}\nThe proposed LLaMA-VID can be utilized to interact with single image or long videos.\nFor clarity, we assume the input image is captured from a video sequence, as presented in Figure~\\ref{fig:main}.\nGiven a video frame ${\\mathbf V}_t\\in\\mathbb{R}^{H\\times W\\times 3}$ at time $t$, a transformer-based visual encoder is first employed to produce the visual embedding  ${\\mathbf X}_t\\in\\mathbb{R}^{N\\times C}$. Here, $N=H/p\\times W/p$ and $C$ indicate the number of image patches and embedding channels, respectively.\nThe patch size $p$ is typically set to 14 for ViT-based backbones~\\cite{vit,CLIP,evaclip}.\nMeanwhile, we take the user instruction as input and generate the text-guided query ${\\mathbf Q}_t\\in\\mathbb{R}^{M\\times C}$ with the produced ${\\mathbf X}_t$, where $M$ denotes the number of queries.\nAs depicted in Figure~\\ref{fig:main}, this cross-modality interaction predominantly occurs in the text decoder, which can be easily instantiated with BERT~\\cite{devlin2018bert} or QFormer~\\cite{instructblip}, as compared in Table~\\ref{tab:abla_text_vis}.\nIn this way, the text query ${\\mathbf Q}_t$ contains highlighted visual cues that are most related to the user instruction.\n\n\\subsection{Token Generation}~\\label{sec:sub_token}\nWith the text query ${\\mathbf Q}_t$ and visual embedding ${\\mathbf X}_t$, we can easily generate representative tokens for LLMs.\nSpecifically, context attention is designed to aggregate text-related visual features and condense them to a single context token.\nAs shown in Figure~\\ref{fig:main}, it takes ${\\mathbf Q}_t$ and ${\\mathbf X}_t$ as input and formulates the context-related embedding ${\\mathbf E}_t\\in\\mathbb{R}^{1\\times C}$ as\n\n\\begin{algorithm}[t!]\n\\caption{Pseudo Code for Token Generation.}\n\\label{algo:code}\n\\algcomment{\\fontsize{7.2pt}{0em}\\selectfont \\texttt{F}: torch.nn.functional; \\texttt{ctxproj}, \\texttt{visproj}: predefined linear projectors.\n}\n\\definecolor{codeblue}{rgb}{0.25,0.5,0.5}\n\\lstset{\n  backgroundcolor=\\color{white},\n  basicstyle=\\fontsize{7.2pt}{7.2pt}\\ttfamily\\selectfont,\n  columns=fullflexible,\n  breaklines=true,\n  captionpos=b,\n  commentstyle=\\fontsize{7.2pt}{7.2pt}\\color{codeblue},\n  keywordstyle=\\fontsize{7.2pt}{7.2pt},\n}\n\\begin{lstlisting}[language=python]\n# B: batch size; C: channel size; n: content shape\n# M: query length; N: shape of flatten image pacthes; \n# text_q: text query in shape (B, M, C)\n# vis_embed: visual embedding in shape (B, N, C)\n\n# Key part 1: calculate context-related embedding\nctx_embed = text_q @ vis_embed.transpose(-1,-2)\nctx_embed = ctx_embed / (vis_embed.shape[-1]**0.5)\nctx_embed = (ctx_embed.softmax(-1)@vis_embed).mean(1)\nctx_embed = self.ctxproj(ctx_embed[:,None])\n\n# Key part 2: calculate visual embedding\ncur_shape = int(vis_embed.shape[1]**0.5)\nvis_embed = vis_embed.reshape(B, cur_shape, -1, C)\nvis_embed = F.avg_pool2d(vis_embed.permute(0,3,1,2), kernel_size=cur_shape//n, stride=cur_shape//n)\nvis_embed = vis_embed.permute(0,2,3,1).flatten(1,2)\nvis_embed = self.visproj(vis_embed)\n\n# concat token in shape (B, n+1, C), n in [1,N]\nfinal_token = torch.cat([ctx_embed, vis_embed], dim=1)\n\\end{lstlisting}\n\\end{algorithm}\n\n\\begin{equation}\\label{equ:context_att}\n{\\mathbf E}_t = {\\mathrm {Mean}}({\\mathrm {Softmax}}({\\mathbf Q}_t \\times {\\mathbf X}^{\\mathsf{T}}_t) \\times {\\mathbf X}_t),\n\\end{equation}\nwhere the $\\mathrm {Softmax}$ function and $\\mathrm {Mean}$ operation are conducted along the $N$ and $M$ dimensions, respectively.\nUnlike QFormer~\\cite{instructblip} that adopts 32 visual queries as LLMs tokens, we only utilize the text query ${\\mathbf Q}_t$ to aggregate the visual features with high-response scores to input instructions.\nAs a result, the most crucial visual cues related to user input are efficiently preserved in the condensed embedding ${\\mathbf E}_t$. \nThe effectiveness of this context-related token generation is demonstrated in Table~\\ref{tab:abla_token_type} and Figure~\\ref{fig:heatmap}.\nSubsequently, a linear projector is utilized to transform the embedding ${\\mathbf E}_t$ into the context token ${\\mathbf E}^{T}_t \\in\\mathbb{R}^{1\\times C}$, which aligns with the language space of LLMs. \nMeanwhile, we employ an adaptive pooling strategy for the visual embedding according to computational constraints to produce the content token ${\\mathbf E}^{V}_t \\in\\mathbb{R}^{n\\times C}$, where $n\\in[1,N]$.\nFor instance, we maintain the original resolution of visual embedding ${\\mathbf X}_t$ when input single image, while we downsample ${\\mathbf X}_t$ to 1 token for long videos. \nThis approach significantly reduces the overload of LLMs for each frame, thereby supporting hour-long videos effectively.\nFinally, the generated context token ${\\mathbf E}^{T}_t$ and the content token ${\\mathbf E}^{V}_t$ are concatenated to represent the frame at time $t$. Along with frames at other timestamps, the entire video sequence is translated into the language space in token format, which is then used to generate responses from LLMs. \nThe whole process is summarized in Algorithm~\\ref{algo:code}.\n\n\\begin{figure}[t!]\n\\centering\n\\includegraphics[width=\\linewidth]{Figure/data.pdf} \n\\caption{\nMultimodal data distribution and instruction format for model training in each stage.\n$\\texttt{<image>}$ and $\\texttt{<image-i>}$ denote the token for single image and the $i$-th video frame, respectively.\n}\n\\label{fig:data}\n\\end{figure}\n\n\\subsection{Training Strategy}~\\label{sec:sub_train}\nTraining strategy, particularly instruction tuning, has proven to be crucial in LLMs~\\cite{llama,alpaca,vicuna} and VLMs~\\cite{instructblip,llava,llava1.5}.\nConsidering training efficiency, in this work, we divide the training procedure into three stages, {\\em i.e.}, modality alignment, instruction tuning, and long video tuning.\n\n\\begin{figure}[t!]\n\\centering\n\\includegraphics[width=\\linewidth]{Figure/long_movie.pdf} \n\\caption{\nAn example to construct instruction pairs for the movie Titanic.\nGiven the movie synopsis and script, we utilize the developed LLMs like GPT-4~\\cite{GPT4} and Claude-2~\\cite{Claude2} to generate movie summaries, plot-related QA pairs, and general reasoning QA pairs.\n}\n\\label{fig:long_movie}\n\\end{figure}\n\n\\vspace{0.5em}\n\\noindent\n\\textbf{Modality Alignment.} \nAs shown in Figure~\\ref{fig:main}, each video frame is projected into the space of LLMs in each forward pass.\nTherefore, it is crucial to ensure visual features are well aligned with the language space.\nTo this end, we construct a compact dataset that contains 790K high-quality image- and video-caption pairs.\nAs illustrated in Figure~\\ref{fig:data}, it mainly consists of 558K image-caption pairs from the LLaVA-filtered CC3M dataset~\\cite{cc3m} and 232K video-caption pairs sampled from the WebVid 2.5M dataset~\\cite{webvid}.\nThe instruction format for each modality is presented in Figure~\\ref{fig:data}.\nIn this stage, we primarily optimize the context attention and projectors in Figure~\\ref{fig:main}, while freezing the pre-trained modules like the visual encoder and text decoder.\n\n\\vspace{0.5em}\n\\noindent\n\\textbf{Instruction Tuning.} \nTo enhance the multi-modality understanding of LLMs, we build the instruction pairs from~\\cite{llava1.5} and~\\cite{videochatgpt}.\nIn particular, the constructed dataset mainly involves content from three sources, {\\em i.e.}, 40K text conversations from ShareGPT~\\cite{ShareGPT}, 625K single- or multi-turn visual QA pairs from~\\cite{llava,vqav2,gqa,okvqa,ocrvqa,aokvqa,textcaps,referitgame,refcoco,vg}, and 98K video QA pairs from~\\cite{activitynet}.\nFor the instruction, we adopt different formats for text, image, and video input, as illustrated in Figure~\\ref{fig:data}.\nAnd the input prompt $\\texttt{<prompt>}$ and answer $\\texttt{<answer>}$ vary with datasets.\nPlease refer to~\\cite{llava1.5} and~\\cite{videochatgpt} for more details.\nMeanwhile, the image token $\\texttt{<image-i>}$ is randomly inserted at the beginning or end of the user input during our training.\nIn instruction tuning, all the modules are optimized except the frozen visual encoder.\n\n\\begin{table*}[t!]\n\n \\centering\n\\begin{tabular}{llc|ccccccccccc}\n  \\toprule\n  \\multirow{2}{*}{Method} & \\multirow{2}{*}{LLM} & \\multirow{2}{*}{Res.} & \\multicolumn{2}{c}{\\bf MSVD-QA} & & \\multicolumn{2}{c}{\\bf MSRVTT-QA} & & \\multicolumn{2}{c}{\\bf ActivityNet-QA} \\\\ \\cline{4-5} \\cline{7-8} \\cline{10-11}\n  & & & Acc & Score & & Acc & Score & & Acc & Score \\\\\n  \\midrule\n  FrozenBiLM~\\cite{frozenbilm} & DeBERTa-V2 & 224 & 32.2 & -- & & 16.8 & -- & & 24.7 & -- \\\\\n  VideoLLaMA~\\cite{videollama} & Vicuna-7B & 224 & 51.6 & 2.5 & & 29.6 & 1.8 & & 12.4 & 1.1 \\\\\n  LLaMA-Adapter~\\cite{llamaadapter} & LLaMA-7B & 224 & 54.9 & 3.1 & & 43.8 & 2.7 & & 34.2 & 2.7 \\\\\n  VideoChat~\\cite{videochat} & Vicuna-7B & 224 & 56.3 & 2.8 & & 45.0 & 2.5 & & 26.5 & 2.2 \\\\\n  Video-ChatGPT~\\cite{videochatgpt} & Vicuna-7B & 224 & 64.9 & \\underline{3.3} & & 49.3 & 2.8 & & 35.2 & 2.7 \\\\\n  BT-Adapter~\\cite{btadapter} & Vicuna-7B & -- & 67.5 & {\\bf 3.7} & & 57.0 & \\underline{3.2} & & 45.7 & \\underline{3.2} \\\\\n  \\midrule\n  \\rowcolor{mygray}\n  {\\bf LLaMA-VID} & Vicuna-7B & 224 & \\underline{69.7} & {\\bf 3.7} & & \\underline{57.7} & \\underline{3.2} & & \\underline{47.4} & {\\bf 3.3} \\\\\n  \\rowcolor{mygray}\n  {\\bf LLaMA-VID} & Vicuna-13B & 224 & {\\bf 70.0} & {\\bf 3.7} & & {\\bf 58.9} & {\\bf 3.3} & & {\\bf 47.5} & {\\bf 3.3} \\\\\n  \\bottomrule\n\\end{tabular}\n \\caption{Comparison with leading methods on 4 zero-shot video QA datasets. \n We report results with 2 tokens for each frame.\n For fair comparisons, our model is trained with data of stage 1 and stage 2 without long video tuning in Figure~\\ref{fig:data}.\n Res indicates image resolution.\n }\n \\label{tab:main_video}\n\\end{table*}\n\n\\vspace{0.5em}\n\\noindent\n\\textbf{Long Video Tuning.} \nTo further unleash the potential for hour-long videos, we construct 15K long QA pairs, including 9K conversions in movie scenes and 6K data sampled from LongLoRA~\\cite{longlora} for token expanding.\nSpecifically, we utilize more than 400 long movies and corresponding scripts in MovieNet~\\cite{movienet} to build the training set.\nThe key components for instruction generation are visualized in Figure~\\ref{fig:long_movie}.\nGenerally, the generated dataset includes QA pairs from three aspects: video summary, movie plot, and detail reasoning. \nFor video summaries, we collect movie synopses to produce brief and detailed summaries for each movie using developed LLMs like GPT-4~\\cite{GPT4}. \nIt brings about 1K summary-level instruction pairs in total.\nFor plot-level data, we take the entire movie synopsis as input and leverage GPT-4~\\cite{GPT4} to generate plot-related and character-related QA pairs.\nThese include plot understanding, description, analysis, character relationship, personality, and behavior. \nIn particular, we generate 5 plot-related pairs and 5 character-related pairs for each movie, resulting in 4K plot-level QA data.\nAs for detail-level data, we feed the long movie script into Claude-2~\\cite{Claude2} and generate 5 plot-related reasoning pairs and 5 detail-related descriptions for each movie, which brings 4K pairs in total.\nWith long videos and the generated pairs, we perform instruction tuning by concatenating visual tokens and subtitle tokens for each frame, as depicted in Figure~\\ref{fig:data}.\nIn this way, LLaMA-VID can well support 64K tokens with more than 3-hour video as input.\nPlease refer to {\\em supplementary material} for more details.\n\n\\begin{table*}[t!]\n \\centering\n\\begin{tabular}{llc|ccccc}\n  \\toprule\n  Method & LLM & Res. & {\\bf Correctness} & {\\bf Detail} & {\\bf Context} & {\\bf Temporal} & {\\bf Consistency} \\\\\n  \\midrule\n  VideoLLaMA~\\cite{videollama} & Vicuna-7B & 224 & 1.96 & 2.18 & 2.16 & 1.82 & 1.79 \\\\\n  LLaMA-Adapter~\\cite{llamaadapter} & LLaMA-7B & 224 & 2.03 & 2.32 & 2.30 & 1.98 & 2.15 \\\\\n  VideoChat~\\cite{videochat} & Vicuna-7B & 224 & 2.23 & 2.50 & 2.53 & 1.94 & 2.24 \\\\\n  Video-ChatGPT~\\cite{videochatgpt} & Vicuna-7B & 224 & 2.40 & 2.52 & 2.62 & 1.98 & 2.37 \\\\\n  BT-Adapter~\\cite{btadapter} & Vicuna-7B & -- & 2.68 & 2.69 & 3.27 & 2.34 & 2.46 \\\\\n  \\midrule\n  \\rowcolor{mygray}\n  {\\bf LLaMA-VID} & Vicuna-7B & 224 & \\underline{2.96} & \\underline{3.00} & \\underline{3.53} & \\underline{2.46} & \\underline{2.51} \\\\\n  \\rowcolor{mygray}\n  {\\bf LLaMA-VID} & Vicuna-13B & 224 & {\\bf 3.07} & {\\bf 3.05} & {\\bf 3.60} & {\\bf 2.58} & {\\bf 2.63} \\\\\n  \\bottomrule\n\\end{tabular}\n \n \\caption{Comparison with leading methods on the video-based generative performance benchmark~\\cite{videochatgpt}.\nWe report results with 2 tokens for each frame.\nFor fair comparisons, our model is trained with data of stage 1 and stage 2 without long video tuning in Figure~\\ref{fig:data}.\nRes indicates image resolution.\n{\\em Correctness}, {\\em Detail}, {\\em Context}, {\\em Temporal}, and {\\em Consistency} indicate the evaluation metric of Correctness of Information, Detail Orientation, Contextual Understanding, Temporal Understanding, and Consistency, respectively.\n }\n \\label{tab:main_video_gen}\n\\end{table*}\n\n\\begin{table*}[t!]\n\\centering\n\\scalebox{1.0}{\n\\begin{tabular}{l l p{5mm} | p{8mm}P{10mm}P{8mm}P{8mm}P{8mm}P{8mm}P{8mm}p{8mm} }\n\\toprule\nMethod & LLM & Res. & {\\bf GQA} & {\\bf MMB} & {\\bf MME} & {\\bf POPE} & {\\bf SEED} & {\\bf SQA}$^\\text{I}$ & {\\bf VizWiz} & {\\bf VQA}$^\\text{v2}$ \\\\\n\\midrule\nInstructBLIP~\\cite{instructblip} & Vicuna-7B & 224 & 49.2 & 36.0 & -- & -- & 53.4 & 60.5 & 34.5 & -- \\\\\nIDEFICS-9B~\\cite{IDEFICS} & LLaMA-7B & 224 & 38.4 & 48.2 & -- & -- & -- & -- & 35.5 & 50.9 \\\\\nQwen-VL$^\\dagger$~\\cite{bai2023qwen} & Qwen-7B & 448 & 59.3* & 38.2 & -- & -- & 56.3 & 67.1 & 35.2 & 78.8* \\\\\nQwen-VL-Chat$^\\dagger$~\\cite{bai2023qwen} & Qwen-7B & 448 & 57.5* &  60.6 & 1487.5 & -- & 58.2 & 68.2 & 38.9 & 78.2* \\\\\nLLaVA-1.5~\\cite{llava1.5} & Vicuna-7B & 336 & \\underline{62.0}* & \\underline{64.3} & \\underline{1510.7} & \\underline{85.9} &  \\underline{58.6} & \\underline{66.8} & \\underline{50.0} & \\underline{78.5}* \\\\\n\\midrule\n\\rowcolor{mygray}\n{\\bf LLaMA-VID} & Vicuna-7B & 336 & {\\bf 64.3}* & {\\bf 65.1} & {\\bf 1521.4} & {\\bf 86.0} & {\\bf 59.9} & {\\bf 68.3} & {\\bf 54.2} & {\\bf 79.3}* \\\\\n\\midrule\nBLIP-2~\\cite{blip2} & Vicuna-13B & 224 & 41.0 & -- & 1293.8 & 85.3 & 46.4 & 61.0 & 19.6 & 41.0 \\\\\nInstructBLIP~\\cite{instructblip} & Vicuna-13B & 224 & 49.5 & -- & 1212.8 & 78.9 & -- & 63.1 & 33.4 & -- \\\\\nShikra~\\cite{chen2023shikra} & Vicuna-13B & 224 & -- & 58.8 & -- & -- & -- & -- & -- & \\underline{77.4}* \\\\\nIDEFICS-80B~\\cite{IDEFICS} & LLaMA-65B & 224 & 45.2 & 54.5 & -- & -- & -- & -- & 36.0 & 60.0 \\\\\nLLaVA-1.5~\\cite{llava1.5} & Vicuna-13B & 336 & \\underline{63.3}* & {\\bf 67.7} & \\underline{1531.3} & \\underline{85.9} & \\underline{61.6} & {\\bf 71.6} & \\underline{53.6} & {\\bf 80.0}* \\\\\n\\midrule\n\\rowcolor{mygray}\n{\\bf LLaMA-VID} & Vicuna-13B & 336 & {\\bf 65.0}* & \\underline{66.6} & {\\bf 1542.3} & {\\bf 86.0} & {\\bf 62.3} & \\underline{70.0} & {\\bf 54.3} & {\\bf 80.0}* \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\caption{Comparison with leading methods on 8 benchmarks. \nHere, we use the same training and instruction finetuning data as that in LLaVA-1.5.\nWe report results with 1 context token and $n$ content tokens, where $n$ is kept the same with that in LLaVA-1.5, {\\em i.e.}, $n=(336/14)^2=576$.\nFor fair comparisons, our model is trained without video data of stage 1 and stage 2 in Figure~\\ref{fig:data}.\nRes indicates input image resolution.\n$^*$ and $^\\dagger$ denote the {\\em train} subset is included for training and the data is not publicly available, respectively.}\n\\label{tab:main_img}\n\\end{table*}\n\n\\section{Experiments}~\\label{sec:experiment}\nIn this section, we provide the experimental setup and comparisons with leading methods on several benchmarks.\nMore details are attached in {\\em supplementary material}.\n\n\\subsection{Experimental Setup}\n\\noindent\n\\textbf{Implementation Details.}\nIn this work, we instantiate the model with the pre-trained EVA-G~\\cite{evaclip} for visual encoder and QFormer~\\cite{instructblip} for text decoder by default.\nDuring training, we keep the visual encoder fixed in all stages and freeze the text decoder, as well as the LLM, in the modality alignment stage, except for the BERT module in Table~\\ref{tab:abla_text_vis} that is not pre-trained.\nFollowing the strategy in~\\cite{llava1.5}, we optimize trainable parameters with the designed data and instructions in Figure~\\ref{fig:data}, running for 1 epoch in each stage.\nFor video input, we extract frames at a speed of 1 FPS.\nAll models are trained using 8$\\times$NVIDIA A100 GPUs.\nAdditional hyperparameters are provided in the {\\em supplementary material}.\n\n\\vspace{1.0em}\n\\noindent\n\\textbf{Datasets.}\nIn this study, we construct the training set mainly from~\\cite{llava1.5,webvid,videochatgpt,movienet}, as illustrated in Section~\\ref{sec:sub_train}.\nMoreover, we report results on several video- and image-based benchmarks.\nIn particular, for video input, we evaluate the zero-shot performance on the open-ended QA benchmarks like MSVD~\\cite{msvd}, MSRVTT~\\cite{msrvtt}, ActivityNet~\\cite{activitynet}, and the newly-proposed generative performance benchmark~\\cite{videochatgpt}.\nAs for image-based evaluation, we conduct experiments on several widely-adopted benchmarks, including GQA~\\cite{gqa}, MMB (MMBench)~\\cite{mmbench}, MME~\\cite{mme}, POPE~\\cite{pope}, SEED~\\cite{seed}, SQA$^\\text{I}$ (Image-based setting in ScienceQA)~\\cite{scienceqa}, VQA$^\\text{T}$ (TextVQA)~\\cite{textvqa}, VizWiz~\\cite{vizwiz}, and VQA$^\\text{v2}$ (VQA V2)~\\cite{vqav2}.\n\n\\begin{figure*}[t!]\n\\centering\n\\includegraphics[width=\\linewidth]{Figure/demo.pdf} \n\\caption{\nExamples of LLaMA-VID with single image, short video, and hour-long video using Vicuna-7B based models.\n}\n\\label{fig:demo}\n\\end{figure*}\n\n\\subsection{Main Results}\n\\noindent\n\\textbf{Results on Video-based Benchmarks.}\nIn Table~\\ref{tab:main_video}, we provide a comparative evaluation of LLaMA-VID against various state-of-the-art methods across three zero-shot video QA benchmarks: MSVD-QA~\\cite{msvd}, MSRVTT-QA~\\cite{msrvtt}, and ActivityNet-QA~\\cite{activitynet}.\nNotably, the results are reported with only two tokens for each frame.\nIt is evident that LLaMA-VID, employing Vicuna-7B and Vicuna-13B as the LLMs, consistently delivers superior performance across all datasets. \nOn the MSVD-QA and MSRVTT-QA datasets, it achieves the accuracy of 69.7\\% and 57.7\\% with Vicuna-7B, surpassing the previous leading approach~\\cite{btadapter} with absolute gains of 2.2\\% and 0.7\\%, respectively.\nAs for the ActivityNet-QA dataset, LLaMA-VID attains top performance in accuracy and the highest score of 3.3.\nIn Table~\\ref{tab:main_video_gen}, we also carry out experiments on the newly proposed video-based generative performance benchmark~\\cite{videochatgpt}.\nOur LLaMA-VID is validated to achieve the best performance across all the evaluation metrics, surpassing previous approaches by a large margin.\nAnd we find that the performance can be further improved as the LLMs scale up.\nIn general, LLaMA-VID is demonstrated to bring robust performance on all benchmarks, validating its effectiveness and efficiency for video processing.\n\n\\vspace{1.0em}\n\\noindent\n\\textbf{Results on Image-based Benchmarks.}\nAs illustrated in Section~\\ref{sec:sub_token}, LLaMA-VID not only efficiently represents long video, but also expands the upper limit of VLMs with an additional context token.\nIn Table~\\ref{tab:main_img}, we perform comparisons with leading VLMs on 8 widely-adopted benchmarks.\nIt should be noted that we maintain the same training data and image resolution as in LLaVA-1.5~\\cite{llava1.5} for fair comparisons.\nIt is evident that LLaMA-VID outperforms other leading methods across most of benchmarks with different LLMs.\nIn particular, with Vicuna-7B as the LLM, LLaMA-VID attains the best results across all the datasets and surpasses LLaVA-1.5 with significant gains in GQA, MME, and VizWiz, where the improvement reaches up to 2.3\\%, 10.7, and 4.2\\%, respectively.\nWith a larger Vicuna-13B as the LLM, LLaMA-VID also outperforms other methods in 6 benchmarks and achieves top-2 in the other datasets.\nThis demonstrates the generality of the proposed LLaMA-VID, which can be scaled up with a stronger foundation model.\nIn summary, LLaMA-VID is proven to push the upper bound of VLMs, especially in efficient settings.\n\n\\noindent\n\\textbf{Qualitative Results.}\nIn Figure~\\ref{fig:demo}, we apply LLaMA-VID to different types of data, including single images, short videos, and long movies.\nWe represent each image with 577 tokens for single images and 2 tokens for videos.\nLLaMA-VID demonstrates various capabilities with different inputs.\nSpecifically, for single images, it focuses on details and accurately recognizes the character without any text clues.\nMoreover, it can also connect the image content to the plot of the game in multi-turn conversations.\nGiven a short trailer video, LLaMA-VID summarizes the overall plot and infers the movie name, producer, and the type of movie. \nAs for a 3-hour movie, the proposed model adeptly describes the storyline and demonstrates plot-related reasoning and detailed understanding.\n\n\\subsection{Component-wise Analysis}~\\label{sec:exp_abla}\nIn this subsection, we conduct ablation studies with input resolution 224 and 2 tokens for each image by default.\nHere, we mainly perform experiments on image-based settings to investigate the effectiveness of each component.\n\n\\vspace{1.0em}\n\\noindent\n\\textbf{Generated Token Types.}\nAs illustrated in Figure~\\ref{fig:main}, each image is represented with a context token and a content token in LLMs.\nTo validate the effectiveness of each part, we conduct experiments with different types of tokens in Table~\\ref{tab:abla_token_type}. \nWithout the context token, the compressed content token, which encodes each image with 1 token, cannot adjust to input instructions, leading to subpar performance.\nCompared with a single content token, the instruction-guided context token results in significant gains across all datasets with only 1 token.\nWith both tokens for each image, the model achieves the best performance across all benchmarks.\nIt shows that both instruction cues in the context token and the image content itself in the content token are important.\n\n\\begin{table}[t]\n \\centering\n \\resizebox{0.49\\textwidth}{12mm}{\n\\begin{tabular}{cc|cccc}\n  \\toprule\n  {\\em context} & {\\em content} & {\\bf GQA} & {\\bf POPE} & {\\bf SQA$^\\text{I}$} & {\\bf VQA$^\\text{T}$}  \\\\\n  \\midrule\n  \\xmark & \\cmark & 53.3 & 80.9 & 66.1 & 46.5 \\\\\n  \\cmark & \\xmark & 54.3 & 82.4 & 67.7 & 48.3 \\\\\n  \\rowcolor{mygray}\n  \\cmark & \\cmark & {\\bf 55.5} & {\\bf 83.1} & {\\bf 68.8} & {\\bf 49.0} \\\\\n  \\bottomrule\n\\end{tabular}\n}\n \\caption{Comparison with different token types.\nWe report results with 1 {\\em context} token (if exists) and 1 {\\em content} token.\n }\n \\label{tab:abla_token_type}\n\\end{table}\n\n\\vspace{1.0em}\n\\noindent\n\\textbf{Generated Token Numbers.}\nIn Table~\\ref{tab:abla_token_num}, we conduct experiments with different numbers of tokens for further investigation.\nWith an image size 224$\\times$224, we set up experiments with $n$ content tokens, where $n=(224/14)^2=256$ for uncompressed settings in the first two rows.\nThe results clearly show that the context token consistently improves performance across different benchmarks with only 1 extra token.\nWhen we compress the content token to $1/4$ with $n=64$, the performance drops about 1\\% to 2\\% but increases 1\\% in SQA$^\\text{I}$. \nConsidering the extra efficient setting for hour-long videos, we compress the content token to $1/256$ with $n=1$ by default.\nCompared to the original setting without context token, we can reduce the computational cost to $1/128$ with about 2\\%-6\\% performance drop, which is generally acceptable.\nThe linear increase in performance presents significant potential for token compression.\nFor instance, we can dynamically compress the content token to different numbers according to resource budget and content importance.\nInterestingly, the model achieves peak performance in SQA$^\\text{I}$ with only 2 tokens.\nThis could be attributed to the fact that problems in ScienceQA~\\cite{scienceqa} focus more on visual-based reasoning rather than image details.\nAs demonstrated in Tables~\\ref{tab:main_video} and~\\ref{tab:main_video_gen}, with only 2 tokens for each image, LLaMA-VID still outperforms all previous work in different video-based benchmarks.\nThis makes it feasible to enable LLMs for hour-long video processing.\n\n\\begin{table}[t]\n \\centering\n\\begin{tabular}{cc|cccc}\n  \\toprule\n  {\\em context} & {\\em content} & {\\bf GQA}  & {\\bf POPE} & {\\bf SQA$^\\text{I}$} & {\\bf VQA$^\\text{T}$} \\\\\n  \\midrule\n  0 & 256 & 61.9 & 85.5 & 67.5 & 53.0 \\\\\n  \\midrule\n  1 & 256 & {\\bf 63.0} & {\\bf 86.6} & 67.7 & {\\bf 53.8} \\\\\n  1 & 64 & 60.8 & 85.1 & 68.7 & 52.3 \\\\\n  1 & 16 & 58.2  & 83.1 & 67.4 & 50.8 \\\\\n  1 & 4 & 56.2  & 83.5 & 68.7 & 49.1 \\\\\n  \\rowcolor{mygray}\n  1 & 1 & 55.5  & 83.1 & {\\bf 68.8} & 49.0 \\\\\n\n  \\bottomrule\n\\end{tabular}\n \\caption{Comparison with different token numbers.\nWe report results with various numbers of {\\em context} token and {\\em content} token.\n }\n \\label{tab:abla_token_num}\n\\end{table}\n\n\\begin{table}[t]\n \\centering\n\\begin{tabular}{c|cccc}\n  \\toprule\n  {\\em text} & {\\bf GQA}  & {\\bf POPE} & {\\bf SQA$^\\text{I}$} & {\\bf VQA$^\\text{T}$} \\\\\n  \\midrule\n  -- & 53.3  & 80.9 & 66.1 & 46.5 \\\\\n  BERT & 54.1 & 80.8 & 67.9 & 48.1 \\\\\n  \\rowcolor{mygray}\n  QFormer & {\\bf 55.5} & {\\bf 83.1} & {\\bf 68.8} & {\\bf 49.0} \\\\\n  \\bottomrule\n\\end{tabular}\n  \\caption{Comparison with different text decoders.\nWe report results with 1 {\\em context} token (if exists) and 1 {\\em content} token.\n }\n \\label{tab:abla_text_vis}\n\\end{table}\n\n\\vspace{1.0em}\n\\noindent\n\\textbf{Text Decoder.}\nAs depicted in Figure~\\ref{fig:main}, the text decoder plays an essential role in producing instruction-guided context cues.\nHere, we further perform comparisons with different text decoders in Table~\\ref{tab:abla_text_vis}.\nWe mainly instantiate the text decoder with two types of modules, namely BERT~\\cite{devlin2018bert} and QFormer~\\cite{instructblip}.\nFor BERT, we randomly initialize it as a cross-modality decoder and only retain the first two layers.\nAs for QFormer, we utilize the pre-trained modules and fix them for modality alignment.\nEven with a simple 2-layer BERT, as shown in Table~\\ref{tab:abla_text_vis}, the generated context token achieves significant gains in most of benchmarks.\nThis proves the effectiveness of the paradigm for context token generation.\nWith a pre-trained text decoder like QFormer, the model can be further enhanced and attains peak performance in all datasets with 2.2\\% to 2.7\\% significant gain.\n\n\\begin{figure}[t!]\n\\centering\n\\includegraphics[width=\\linewidth]{Figure/heatmap.pdf} \n\\caption{\nHigh response areas with top scores to input question in Equation~\\ref{equ:context_att}.\nWe present the response of first two queries in ${\\mathbf Q}_t$.\nImages are randomly sampled from VQA V2~\\cite{vqav2} {\\em test-dev} set.\n}\n\\label{fig:heatmap}\n\\end{figure}\n\n\\vspace{1.0em}\n\\noindent\n\\textbf{Response in Context Attention.}\nTo more vividly explore the context attention, we visualize the high response areas with the top 20 scores in Figure~\\ref{fig:heatmap}.\nSpecifically, we draw the normalized heatmap for the first two queries in ${\\mathbf Q}_t$ before applying the $\\text{Softmax}$ function, which is used to formulate context token in Equation~\\ref{equ:context_att}.\nAs shown in Figure~\\ref{fig:heatmap}, the text-guided query ${\\mathbf Q}_t$ effectively focuses on important areas relevant to the input questions.\nFor example, in the second row, when inquiring whether the image depicts a fishing village, the query ${\\mathbf Q}_t$ focuses more on buildings along the river and a seagull.\nThese are all typical characteristics to distinguish a fishing village in common sense.\nOther examples also confirm that the designed context attention successfully achieves its goal of formulating the context token under instruction guidance.\nMore visualizations can be found in the attached {\\em supplementary material}.\n\n\\section{Conclusion}\nWe have introduced LLaMA-VID, a simple yet effective token generation approach for VLMs.\nThe central concept behind LLaMA-VID is to represent an image with the context token and the content token.\nIn particular, the context token is generated according to input instructions, and the content token is produced based on the image content.\nDepending on the budget, the content token can be compressed to one token or expressed without compression.\nIt allows us to represent a single image with preserved details and efficiently encode each video frame with only two tokens.\nMoreover, we have constructed an instruction dataset for hour-long video understanding.\nOur experiments on several video- and image-based benchmarks prove the superiority of our method.\nWe hope that LLaMA-VID can serve as a strong benchmark for efficient visual representation.\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Long Context Transfer from Language to Vision}\n\n\\begin{document}\n\n\\maketitle\n\\renewcommand{\\thefootnote}{\\fnsymbol{footnote}}\n\\footnotetext[1]{ Equal contribution. $^\\dagger$Project lead. \\textsuperscript{\\Letter}Corresponding author.}\n\\renewcommand*{\\thefootnote}{\\arabic{footnote}}\n\n\\begin{abstract}\n\nVideo sequences offer valuable temporal information, but existing large multimodal models (LMMs) fall short in understanding extremely long videos. Many works address this by reducing the number of visual tokens using visual resamplers. Alternatively, in this paper, we approach this problem from the perspective of the language model. By simply extrapolating the context length of the language backbone, we enable LMMs to comprehend orders of magnitude more visual tokens without any video training. We call this phenomenon \\textit{long context transfer} and carefully ablate its properties. To effectively measure LMMs' ability to generalize to long contexts in the vision modality, we develop V-NIAH (Visual Needle-In-A-Haystack), a purely synthetic long vision benchmark inspired by the language model'\ns NIAH test. Our proposed Long Video Assistant (LongVA) can process 2000 frames or over 200K visual tokens without additional complexities. With its extended context length, LongVA achieves state-of-the-art performance on Video-MME and MLVU among 7B-scale models by densely sampling more input frames. Our work is open-sourced at \\href{https://github.com/EvolvingLMMs-Lab/LongVA}{\\texttt{https://github.com/EvolvingLMMs-Lab/LongVA}}.\n\n\\end{abstract}\\section{Introduction}\n\nDriven by the progress of Large Language Models (LLMs)~\\citep{gpt3, anil2023palm, touvron2023llama, geminiteam2024gemini, ormazabal2024reka,mixtral2024,commandrplus2024}, multiple studies are conducted to extend their capability to understand images and videos~\\citep{li2023blip2, dai2023instructblip, qwenvl2024, liu2023llava}. With modality alignment and visual instruction tuning, these Large Multimodal Models (LMMs) have shown impressive abilities such as captioning and visual question-answering. While current LMMs have demonstrated promising performance on tasks involving single images and short videos~\\citep{song2024moviechat, lin2023videollava, maaz2023videochatgpt, zhang2023videollama}, effectively processing and understanding extremely long videos remains a significant challenge~\\citep{wang2024lvbench}.\n\nOne primary reason for this challenge is the excessive number of visual tokens generated by the vision encoder. For instance, LLaVA-1.6~\\citep{liu2024llavanext} can produce 576 to 2880 visual tokens for a single image. The number of visual tokens increases significantly with the addition of more frames. To address this problem, numerous methods have been proposed to reduce the number of visual tokens. One popular direction is to modify the visual resampler that connects the vision encoder and LLM, aiming to extract fewer tokens~\\citep{li2023blip2, li2023llamavid, cai2024matryoshka, cheng2024videollama}. Alternative approaches~\\citep{chen2024image,shang2024llavaprumerge, jin2024chatunivi, zhou2024streaming} employ heuristic techniques to prune or merge the visual features. However, despite these efforts, Table~\\ref{table:existing models} demonstrates that the majority of current LMMs are still limited in their ability to process a large number of frames effectively.\n\nAnother issue hindering the development of high-performance long video LMMs is the lack of high-quality long video datasets. In Table \\ref{table:it_dataset_info}, we list the average video length of existing video instruction tuning data. Most datasets consist of video clips within 1 minute. Even if some datasets do contain longer videos, the corresponding text pairs are generated by annotating only several frames within that video, lacking long and dense supervision signals. \n\nGiven the circumstance, in this paper, instead of reducing the visual tokens, we identify the more critical issue limiting the visual context length in existing LMMs: the context length of the language model backbone. Given a language model, we first extend its context length by training on longer text data. We then use this context-extended LM as the backbone to perform modality alignment and visual instruction tuning without any long video text pairs. By training this way, the context length of the language model is directly transferred to that of the LMMs. We further proposed \\textit{UniRes}, a unified encoding scheme that represents videos as extended images, enhancing the capability fusion between images and videos.\nTo facilitate benchmarking and accurately assess the context length in the visual domain, we created V-NIAH, a synthetic visual benchmark based on the Needle-in-a-haystack test~\\citep{niah} used in language models. Our model, Long Video Assistant (LongVA), is capable of accurately retrieving visual information from 2000 frames or more than 200K visual tokens. Experiments show that additional frames during inference lead to improved performance on long video question-answering benchmarks, and LongVA achieves state-of-the-art performance among 7B models on the Video-MME~\\citep{fu2024videomme} and MLVU~\\citep{zhou2024mlvu} dataset. \nIn summary, our paper makes the following contributions:\n\n\\textbf{(1)} \\textbf{Long Context Transfer}: We discovered the \\textit{long context transfer} phenomenon where the context of the language model can be directly transferred to the modality-aligned multi-modal models.\n\n\\textbf{(2)} \\textbf{Visual Needle-In-A-Haystack (V-NIAH)}: We proposed the V-NIAH benchmark to test LMMs ability in locating and retrieving visual information over extremely long contexts.\n\n\\textbf{(3)} \\textbf{Long Video Assistant (LongVA)}: With \\textit{long context transfer} and \\textit{UniRes}, we developed LongVA that can perceive more than 200K visual tokens, achieving SoTA performance on the Video-MME and MLVU dataset. \n\n\\begin{table}[t]\n\\begin{center}\n\\scalebox{0.8}{ % Adjust the value to scale the table\n\\begin{tabular}{lrrrrrr}\n\\toprule\n Model & Tokens/Frames\\textsuperscript{*}  &Training Max Frames\\textsuperscript{*}  & LM Backbone & LM Context Length \\\\\n\\midrule\nMPLUG-Owl-video~\\citep{ye2024mplugowl}  &  256 &  4  &   LLaMA  &  4K\t \\\\\nMovieChat~\\cite{song2024moviechat}  &  32 &  8  &   Vicuna-v0  &  2K\t \\\\\nVideo-LLaVA~\\citep{zhang2023videollama}  &  49 &  8  &   Vicuna-1.5  &  4K\t  \\\\\nVideoChat~\\citep{li2024videochat}  &  32$/$196 &  8  &   Vicuna-v0  &  2K\t  \\\\\nLLaVA-NeXT-Video~\\citep{zhang2024llavanextvideo} &  144 &  16 &  Vicuna-1.5 &  4K \\\\\nST-LLM~\\citep{liu2024stllm}  &  256 &  16  &   Vicuna-1.1  &  2K\t  \\\\\nVideo-LLaMA~\\citep{cheng2024videollama}  &  32 &  32  &   LLaMA-2  &  4K\t  \\\\\nChat-UniVi~\\citep{jin2023chatunivi}  &  112 &  64  &   Vicuna-1.5  &  4K\t \\\\\nTimeChat~\\citep{ren2024timechat}  &  4 &  96  &   LLaMA-2  &  4K\t \\\\\nVideo-ChatGPT~\\cite{maaz2023videochatgpt}  &  256 &  100  &   Vicuna-1.1 &  2K\t \\\\\nLLaMA-VID~\\citep{li2023llamavid}  &  2 &  300  &   Vicuna-1.5  &  4K\t \\\\\n\\midrule\nLongVA (Ours) & 144 & - & Qwen2-Extended & 224K+ \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{center}\n\\caption{To enable longer video inputs, previous works train \n fewer visual tokens to increase the maximum frames during training. Our LongVA, on the other hand, enables long video capability by extending the backbone language model. \\textsuperscript{*}We report it based on the best available information from their paper or released codebase.\n}\n\\label{table:existing models}\n\\end{table}\n\n\\section{Related Work}\n\t\n\\paragraph{Vision Language Connector in Large Multimodal Models}\nExisting studies explore different architectures to extract and inject visual features into LLMs. One line of work~\\citep{flamingo, li2023otter, open_flamingo, lauren√ßon2023obelics}, pioneered by Flamingo~\\cite{flamingo}, adopts a resampler to compress the visual feature and inserts cross-gated attention layers into the LLM. Some other works still use a reampler~\\citep{li2023blip2, zhu2023minigpt4, qwenvl2024} while directly feeding the image feature into the input layer of the language model. The LLaVA series~\\citep{liu2024llavanext, liu2023improvedllava, liu2023llava} use a simple and scalable design to directly project the image features into language model without any pooling or resampling. When the field moves from image-only models to include multi-image and video inputs, more modifications to the visual language connector were proposed. \\cite{zhang2024llavanextvideo} and \\cite{cai2024matryoshka} use a simple average pooling. \\cite{jin2024chatunivi} dynamically drop the visual tokens. \\cite{cheng2024videollama} adopt a spatial-temporal convolution to better capture the dynamics of video data and reduce feature size. Our proposed context transfer from text to image is orthogonal to those works and can further enable LMMs to understand more frames. \n\n\\begin{table}[t]\n\\centering\n\\begin{minipage}{0.45\\textwidth}\n\\centering\n\\caption{Existing Video SFT Datasets}\n\\label{table:it_dataset_info}\n\\scalebox{0.8}{\n\\begin{tabular}{lcc}\n\\toprule\nDataset Name & Video Length (sec.) & Text Length  \\\\\n\\midrule\nVideoChatGPT-100K~\\citep{maaz2023videochatgpt} & 123.4 & {\\color{white}0}68.0 \\\\\nLLaVA-Hound-255K ~\\citep{zhang2024direct} & {\\color{white}0}52.4 & {\\color{white}0}37.6 \\\\\nShareGPT4Video\\citep{chen2024sharegpt4video} & {\\color{white}0}26.6 & 273.3 \\\\\nTimeIT~\\citep{ren2024timechat} & 190.8 & {\\color{white}0}52.5 \\\\\nVideoChat~\\citep{li2024videochat} & {\\color{white}00}9.5 & {\\color{white}0}59.0 \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{minipage}\n\\hfill\n\\begin{minipage}{0.45\\textwidth}\n\\centering\n\\caption{ Video Benchmarks}\n\\label{table:existing_benchmarks}\n\\scalebox{0.8}{\n\\begin{tabular}{lc}\n\\toprule\nBenchmark Name  & Video Length (sec.)\\\\\n\\midrule\nVideoChatGPT~\\citep{maaz2023videochatgpt} &  {\\color{white}0}108.0 \\\\\nNexTQA~\\citep{xiao2021nextqanext} &  {\\color{white}00}42.9 \\\\\nEgoSchema~\\citep{mangalam2023egoschema} &  {\\color{white}0}179.8 \\\\\nVideoMME~\\citep{fu2024videomme} &  1017.0 \\\\\nV-NIAH (Ours) & {\\color{white}000.}$\\infty$ \\\\\n\\bottomrule \n\\end{tabular}\n}\n\\end{minipage}\n\\end{table}\n \n\\paragraph{Context Extrapolation in Transformer}\nTransformer does not directly work on sequences longer than its training length. To alleviate that, various RoPE-based ~\\citep{su2023roformer} extension techniques~\\citep{chen2023extending,NTK-aware, rozi√®re2024code,peng2023yarn,ding2024longrope} have been proposed to allow for training-free context extrapolation. Efforts have also been made on data curation~\\citep{fu2024data,xiong2023effective,bai2024longalign} and system optimization~\\citep{li-etal-2023-sequence,liu2023ring,jacobs2023deepspeed} during long context training. There has been limited exploration of the context extrapolation in the domain of LMMs. \\cite{liu2023world} are closest to our work and train LMM with long context language models, but they do not benchmark the effective visual context length of their model.\n\n\\paragraph{Video Language Benchmarks}\nRecent years have witnessed significant progress in Video Question-Answering\\cite{antol2015vqa}.\nTo accurately measure the progress of the video LMMs' performance, researchers have developed various benchmarks encompassing a broad spectrum of tasks. These range from fundamental visual perception tasks such as activity recognition\\cite{yu2019activitynet}, concept detection~\\cite{xu2017video}, and counting~\\cite{jang2017tgif}, to more complex visual reasoning tasks including compositional~\\cite{grunde2021agqa}, causal~\\cite{xiao2021next,yi2019clevrer,xu2021sutd}, and situated reasoning~\\cite{wu2021star}. \nHowever, most of those benchmarks focus on short videos, lacking data and metrics to test LMMs' capability over a long context. Inspired by the NIAH test~\\citep{niah} in the language model community, we proposed V-NIAH to benchmark LMMs' ability over long visual inputs with the minimum overhead of data collection and human annotation. Several concurrent works also developed multimodal versions of the Needle-in-a-haystack test~\\cite{wang2024needle,zhou2024mlvu,song2024milebenchbenchmarkingmllmslong, wang2024multimodalneedlehaystackbenchmarking}. However, they only measure on several hundreds of frames and lack a strong baseline to properly analyze the properties of visual context length.\n\\section{Long Video Assistant}\n\nAs in Figure \\ref{fig:longva_plot_main}, this paper centers around the hypothesis that \\textit{if the modality of vision and language can be truly aligned, the capability to handle long contexts could also transfer from text to vision}, and this could happen even without explicit long video training. Our methodology is thus very straightforward. Given a language model, we first perform long context training purely on language to extend its text context (Section \\ref{sec:long-lm-training}). We then detailed how we augment this language model with long visual capabilities by training solely on short image data in Section \\ref{sec:vl-alignment}.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{image/longva_plot.png}\n    \\caption{\\textbf{Left:} to develop long vision models,  previous studies proposed better visual resamplers to reduce the number of visual tokens. \\textbf{Right:} LongVA approaches this problem from the angle of the language model. We leverage image data (short visual input) to align long-context LLM with vision. During the test time, LongVA can zero-shot process extremely long videos, thanks to the property of long context transfer.}\n    \\label{fig:longva_plot_main}\n\\end{figure}\n\n\\subsection{Training Long Language Model}\n\\label{sec:long-lm-training}\nWe use Qwen2-7B-Instruct~\\citep{qwen2} as the backbone language model and perform continued pretraining with a context length of 224K\\footnote{224K is the maximum we can fit with 8$\\times$A100-80G for Qwen-2-7B. We find that the embedding size significantly impacts the maximum sequence length in our optimized codebase. Qwen2 has a huge vocabulary of 152K tokens. For LLaMA2 with 32K vocabulary, we can train it with 700K context length.} over a total of 900M tokens. We follow~\\cite{xiong2023effective} to increase RoPE~\\citep{su2023roformer}  base frequency during the continued pertaining and specifically set it to 1B. A constant learning rate of 1e-5 is maintained for a batch size of one million tokens across 1,000 training steps. Following \\cite{fu2024data}, we construct the dataset used for long context training from Slimpajama~\\citep{slimpajama} by upsampling documents longer than 4096 and keeping the domain mixture ratio unchanged. Multiple documents are packed into a single sequence separated by a \\texttt{BOS} token.\n\n We employed several optimization strategies to perform training on such long sequences. These includes FlashAttention-2~\\citep{dao2023flashattention2}, Ring Attention~\\cite{liu2023ring, li-etal-2023-sequence}, activation checkpointing, and parameter offload~\\cite{rajbhandari2020zero}. To balance the load across different GPUs, we shard the sequence in a zigzag way~\\cite{zigzagringattn} in ring attention. The resulting training framework is memory efficient and maintains very high GPU occupancy.  Note that we do not use any parameter-efficient methods such as LoRA~\\citep{hu2021lora} or approximate attention~\\citep{child2019generating}. With those optimizations, the compute used in long context training is minimal compared to that of language model pretraining, making it feasible for academic budgets. The long context training can finish in 2 days with 8 A100 GPUs.\n\nIn Figure \\ref{fig:text_niah_results}, we evaluate the extended Qwen2 with the Needle-in-a-haystack (NIAH) test~\\citep{arizeniah, niah}. It achieves perfect results within the training context length (224K) and generalizes even further. We find the vanilla NIAH to be a relatively trivial benchmark and further test it with 5 distractors randomly inserted into the documents. The detailed configuration can be found in Appendix \\ref{sec:text_niah_details}.\n\n\\subsection{Aligning Long Language Model Using Short Vision Data}\n\\label{sec:vl-alignment}\n\n Inspired by the \\textit{AnyRes} encoding scheme in LLaVA-NeXT~\\citep{liu2024llavanext, li2024llavanext-ablations}, we designed \\textit{UniRes} that provides a unified encoding scheme for both images and videos, as shown in Figure \\ref{fig:unires_plot}. Unlike \\textit{AnyRes} which retains a small base image and flattens ViT patches across the grids, \\textit{UniRes} removes the base image, flattens patches within each grid, and 2x2 pool the visual features by default (Appendix \\ref{appendix:unires}). This approach allows us to maintain consistent representation when extending image data into videos where multiple frames are viewed as multiple grids in a row. \n \n Specifically, \\textit{UniRes} divides an input image of resolution $a \\times b$ into smaller grids, each with a resolution of $336 \\times 336$ pixels. This results in $(a//336) \\times (b//336)$ grids. For very high-resolution images, we limit the maximum number of grids to 49, resizing images larger than this threshold. Each grid is separately encoded using \\texttt{CLIP-ViT-L-336px}~\\citep{radford2021learning} and then projected through a 2-layer MLP to match the LM's input dimension, resulting in 576 features per grid. We then apply 2x2 average pooling, finally converting an $a \\times b$ image into $(a//336) \\times (b//336) \\times 144$ tokens. During inference, this visual encoding scheme allows videos to be represented as very long images (even though we do not train on videos). An $N$-frame video is treated as an image of size $336 \\times (336 \\times N)$, divided into $N$ grids where each grid corresponds to a video frame. Using CLIP encoding, MLP projection, and average pooling, an $N$-frame video is encoded into $144N$ visual tokens. \n\nTo clearly ablate the long context transfer phenomenon from language to vision, we adopt a \\textit{train short, test long} protocol where we only use image-text data during training, but test on long videos. We trained our model using the same data recipe and two-stage training approach as LLaVA-1.6. Our experiments show that compared to \\textit{AnyRes}, \\textit{UniRes} has slightly lower scores on low-resolution image benchmarks (Table \\ref{tab:image-results}) but performs better on V-NIAH (Figure \\ref{fig:vniah_plot}) and Video-MME (Table \\ref{tab:video-mme-results}). We believe the unified encoding scheme for images and videos is crucial, thus choosing this as the encoding scheme of LongVA. The image-text alignment can be finished in 1.5 days. With 2 days for long context training on text, the total training cost of LongVA is 3.5 days on 8$\\times$A100-80G.\n\nIt is worth noting previous work largely inspired the design choice of LongVA. For example,\\cite{xiong2023effective} first demonstrates the effectiveness of long context continued pretraining with increased RoPE base frequency (thus decreasing the rotation angles). We sample the long text data following the guidance of \\citep{fu2024data}. We adopt the same vision encoder and training data as that of LLaVA-1.6~\\cite{liu2024llavanext}. We try to keep our methods as simple as possible to clearly show the phenomenon of long context transfer without other confounders.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{image/uniresv2.png}\n    \\caption{\\textit{UniRes}'s unified encoding scheme of images and videos. During training, images are divided into multiple grids. During inference, videos are treated as extended images with each frame considered as a grid. }\n    \\label{fig:unires_plot}\n\\end{figure}\\section{V-NIAH}\n\\label{sec:v-niah}\n\nTo measure the context length of language models on extremely long input, earlier works calculate perplexity scores over long documents. Recently, many have started using the Needle-in-a-Haystack (NIAH) test to benchmark LLMs' ability to retrieve long context information precisely. We note that there is so far no benchmark to measure the visual context length of LMMs. \nTo evaluate LongVA's capacity to locate and retrieve long-range visual information, we extend the NIAH test from text to video and propose V-NIAH. \n\nAs shown in Table~\\ref{tab:vniah-needles}, we designed 5 video question-answering problems as the needle and inserted each as a single frame into hours-long videos. We sampled the videos at 1 FPS as the visual input.  The image of the needle is sourced from existing VQA benchmarks or AI-generated to avoid any contamination. The AI-generated images and questions are purposely chosen to be \"counterfactual\" or \"counter-commonsense\", ensuring the model cannot answer based on language knowledge alone. Each question includes a \"locating prompt\" so that a capable system or human can locate the needle frame from the video haystack and answer the question.\n\nWhen testing LongVA with visual inputs of up to 3000 frames, one difficulty we encountered was that processing a 200K-token input requires up to 100GB of GPU memory for the KV cache for a 7B LM like LLaMA. Even with advanced LM serving systems like vLLM~\\citep{kwon2023efficient} with tensor parallelism to shard the KV cache across multiple GPUs, the sampling process remains extremely slow due to limited memory and batchsize. To address this, we used \"perplexity-based\" evaluation to measure the correctness of the model output. We first encode all frames and save their corresponding visual embeddings. During the evaluation, we only load the language model from LongVA and concatenate the visual embeddings, question tokens, and answer tokens for a single forward pass with ring attention. This approach makes the workload compute-bound and eliminates the need to cache the KV state. The model's output is considered correct only if the highest output logits index of all tokens in the answer span matches the correct answer.\n\n\\begin{figure}[t]\n\\label{fig:insert_niah}\n    \\centering\n    \\includegraphics[width=\\textwidth]{image/example.pdf}\n    \\caption{V-NIAH consists of a haystack video, a needle image, and a question related to the needle. The needle is inserted at various positions in the haystack video.}\n\\end{figure}\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\textwidth]{image/video_niah.png}\n    \\caption{The V-NIAH results of LongVA and its baselines. The x-axis represents the total number of frames in the video haystack. The y-axis shows the position where the needle image is located. For instance, a frame depth of 0\\% would place the needle image at the very beginning of the video. The black dotted line denotes the training length of the backbone language model, with each frame corresponding to 144 tokens.}\n    \\label{fig:vniah_plot}\n\\end{figure}\n\n\\section{Experiments}\n\\label{sec:experiments}\n\n\\begin{table}[t]\n\\begin{center}\n\\scalebox{0.80}{\n\\begin{tabular}{l|cc|cccc}\n\\toprule\n    \\textbf{Model} & \\textbf{LLM Params} & \\textbf{Frames} & \\textbf{Short} & \\textbf{Medium} & \\textbf{Long} & \\textbf{Overall} \\\\\n   \\midrule\nInternVL-Chat-V1.5~\\citep{chen2023internvl} & 20B & {\\color{white} 0}10 & 60.2  & 46.4  & 45.6 & 50.7 \\\\\nLLaVA-NeXT-Video-34B~\\citep{zhang2024llavanextvideo} & 34B & {\\color{white}0}32 & 61.7  & 50.1  & 44.3 & 52.0 \\\\\nVILA-1.5~\\citep{lin2023vila} & 34B & {\\color{white} 00}8 & 68.1 & 58.1 & 50.8 & 59.0 \\\\  \n\\midrule\nQwen-VL-Chat~\\citep{qwenvl2024} & {\\color{white}0}7B & {\\color{white}00}4 & 46.9  & 38.7  & 37.8 & 41.1 \\\\\nVideo-LLaVA~\\citep{lin2023videollava}& {\\color{white} 0}7B & {\\color{white}00}8 & 45.3  & 38.0  & 36.2 & 39.9 \\\\\nST-LLM~\\citep{liu2024stllm} & {\\color{white} 0}7B & {\\color{white}0}64 & 45.7  & 36.8  & 31.3 & 37.9 \\\\\nVideoChat2-Mistral~\\citep{li2024videochat} & {\\color{white} 0}7B & {\\color{white}0}16 & 48.3  & 37.0  & 33.2 & 39.5 \\\\\nChat-UniVi-V1.5~\\citep{jin2023chatunivi} & {\\color{white} 0}7B & {\\color{white}0}64 & 45.7  & 40.3  & 35.8 & 40.6 \\\\\nVideoLLaMA2~\\citep{cheng2024videollama} & {\\color{white} 0}8B & {\\color{white}0}16 & 56.0 & 45.4 & 42.1 & 47.9 \\\\\nLLaVA-NeXT-Qwen2 & {\\color{white} 0}7B & {\\color{white}0}32 & 58.0\t& 47.0\t& 43.4\t&49.5 \\\\\n\\midrule\n\\multirow{6}{*}{LongVA} & \\multirow{6}{*}{{\\color{white} 0}7B} & {\\color{white}00}8 & 55.1 & 46.3 & 42.1 & 47.9 \\\\\n      &    & {\\color{white}0}16 & 59.0  & 46.6 & 43.6 & 49.7 \\\\\n      &    & {\\color{white}0}32 & 61.1 & 48.8 & 45.4 & 51.8 \\\\\n      &    & {\\color{white}0}64 & \\textbf{61.4} & \\textbf{50.9} & 45.0 & 52.4 \\\\\n      &    & 128 & 61.1&  50.4 & \\textbf{46.2} & \\textbf{52.6} \\\\\n      &    & 384 & 60.3 & 48.9 & 46.1 & 51.8\\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{center}\n\\caption{Performance comparison of various LMMs on Video-MME~\\citep{fu2024videomme} \n\\textit{without subtitles}. LongVA achieves state-of-the-art results among 7B models. Its performance also increases with denser sampling of video frames.}\n\\vspace{-20pt}\n\\label{tab:video-mme-results}\n\\end{table}\n\n\\begin{wraptable}{r}{0.45\\textwidth}\n\\begin{center}\n\\scalebox{0.80}{\n\\begin{tabular}{l|cc}\n\\toprule\n    \\textbf{Model} & \\textbf{Qwen2-224K} & \\textbf{UniRes} \\\\\n\n   \\midrule\nLLaVA-Next-Qwen2 & $\\times$ & $\\times$ \\\\\nLongVA (\\textit{AnyRes}) & $\\checkmark$ & $\\times$ \\\\\nLongVA & $\\checkmark$ & $\\checkmark$ \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{center}\n\\caption{LongVA and its baselines.}\n\\label{tab:model-comparison}\n\\end{wraptable}\n\nWe primarily assess the long visual capability of LongVA on two benchmarks: V-NIAH (Section \\ref{subsec:vniah} and Video-MME~\\citep{fu2024videomme} (Section \\ref{subsec:video_eval}).  V-NIAH provides quick signals about the visual context length of LongVA. However, it only tests the model's ability to retrieve information and does not cover other abilities necessary for a real-world long video assistant. Therefore, we also include LongVA's performance on Video-MME,  a comprehensive evaluation suite for video LMMs that includes diverse data types and qualitative annotations. Video-MME is an ideal benchmark for assessing LMMs' ability to handle long videos in real-world scenarios, given its average video duration of 1017 seconds and the inclusion of short, medium, and long subsets.  We further include the benchmark results on MLVU\\citep{zhou2024mlvu} in Appendix \\ref{sec:mlvu results}.\n\nWe mainly compare LongVA against other image and video LMMs. To validate the phenomenon of \\textit{long context transfer}, we trained LLaVA-Next-Qwen2, a baseline model based on Qwen2-7B-Instruct using the LLaVA-NeXT~\\cite{liu2023improvedllava, li2024llavanext-ablations} training recipe. Additionally, we trained LongVA (\\textit{AnyRes}) to showcase the advantages of our \\textit{UniRes} encoding scheme. The difference between LongVA and our baselines can be found in Table \\ref{tab:model-comparison}.\n\n\\subsection{V-NIAH Results}\n\\label{subsec:vniah}\n\n\\textbf{Long context transfers from language to vision} Figure \\ref{fig:vniah_plot} shows the V-NIAH performance of LongVA and other LMMs. Specifically, Figure \\ref{fig:vniah_plot} (iii) demonstrates that the visual context length of LLaVA-NeXT-Video-32K~\\citep{zhang2024llavanextvideo} is constrained by the 32K context length of its language backbone, Mistral-7B-Instruct-v0.2~\\citep{jiang2023mistral}, equivalent to approximately 200 frames. Beyond this limit, the V-NIAH accuracy drops significantly.  As a stronger baseline, we include the results of LLaVA-NeXT-Video-32K enhanced with a training-free length extrapolation algorithm~\\citep{ntk_rope} by increasing its RoPE base frequency. We empirically determine the optimal extrapolation frequency by choosing from [3M, 10M, 30M, 100M, 300M, 1B]. As indicated in Figure \\ref{fig:vniah_plot} (iv), although this training-free extrapolation allows the model to process information across an extended context, the improvement is marginal.  These findings led us to develop LongVA, a model that unlocks the visual context by extending the language model purely on text.  As shown in Figure \\ref{fig:vniah_plot} (i), LongVA can almost perfectly retrieve information and answer the needle question for input frames fewer than 2000. Although we only trained LongVA's language backbone on a context length of 224K (equivalent to 1555 frames), it generalizes well beyond that, maintaining satisfactory performance within 3000 frames. Those results clearly corroborate of hypothesis of \\textit{long context transfer}.\n\n\\textbf{Unified encoding enables better visual context extrapolation}\nWe also present the V-NIAH heatmap of LongVA trained with \\textit{AnyRes} encoding scheme, keeping all other factors unchanged in Figure \\ref{fig:vniah_plot} (ii). LongVA-\\textit{AnyRes} demonstrates strong retrieval capabilities. However, its performance still lags behind LongVA trained with UniRes. We believe that the unified representation of images and videos in UniRes, where a video is encoded in the same way as a long image, enhances the long context transfer from language to vision. This approach also facilitates effective training with short vision data (images) and enables zero-shot understanding of long videos during inference.\n\n\\subsection{Video Evaluation}\n\\label{subsec:video_eval}\n\nOn Video-MME (Table \\ref{tab:video-mme-results}), LongVA achieves \\textit{state-of-the-art} performance among LMMs under 10B parameters, rivaling much larger ones such as LLaVA-NeXT-Video-34B~\\citep{zhang2024llavanextvideo} and InternVL-Chat-V1.5~\\citep{chen2023internvl}. Notably, LongVA is trained without any video data, so its performance on video can be considered \\textit{zero-shot}. As the number of sampled frames increases, LongVA shows improved performance on the long subset, handling up to 384 frames\\footnote{We limited our analysis to 384 frames due to computational and memory constraints as detailed in Section \\ref{sec:v-niah}.}.  Even though LongVA's score slightly drops when we upsample from 128 to 384 frames, it maintains a competitive performance. To our knowledge, LongVA is the \\textit{only} open-source model that can handle such large input frames on Video-MME.\nThese findings highlight the \\textit{long context transfer} effect, where LongVA, originating from a long context language model, can process significantly more frames than its baseline, despite being trained on the same multimodal data.\n\nWe also tested LongVA on shorter benchmarks with average video durations under 120 seconds.  As indicated in Table \\ref{tab:short-video-results}, although LongVA scores higher with more densely sampled frames on datasets such as NeXTQA~\\citep{xiao2021nextqanext} and ActivityNetQA~\\citep{yu2019activitynetqa}, the gains quickly plateau and are not as significant as those observed in Video-MME, which can be attributed to the shorter duration of these datasets.  On the VideoChatGPT and Video Detailed Description (Video-DD) benchmarks, increasing frames does not lead to better performance, and LongVA generally achieves lower scores compared to LLaVA-NeXT-Video-7B. \nSince both benchmarks use OpenAI's GPT API as a judge, we believe their metrics are closely related to the answering format. To address this, we perform a lightweight Direct Preference Optimization (DPO) on the LLaVA-Hound-DPO~\\citep{zhang2024direct} dataset. We observe significantly improved performance for LongVA-DPO, confirming the findings in \\cite{zhang2024direct}.\n\n\\begin{table}[t]\n\\begin{center}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|c|cc|c|ccccc|c}\n\\toprule\nModel &\n   &\n  \\multicolumn{2}{c}{NeXTQA~\\citep{xiao2021nextqanext}} &\n  \\multicolumn{1}{|c|}{ActivityNetQA~\\citep{yu2019activitynetqa}} &\n  \\multicolumn{5}{c|}{VideoChatGPT~\\citep{maaz2023videochatgpt}} &\n  \\multicolumn{1}{c}{Video-DD~\\citep{maaz2023videochatgpt}} \\\\\n &\n  frames &\n  MC &\n  OE &\n  Score &\n  Consistency &\n  Correctness &\n  Detail &\n  Context &\n  Temporal &\n  Score \\\\\n  \\midrule\nLLaVA-NeXT-Video~\\citep{zhang2024llavanextvideo} & 32 & 57.93 & 26.90 & 3.20 & 3.12 & 3.39 & 3.29 & 3.92 & 2.60 & 3.32 \\\\\nLongVA & {\\color{white}0}8  & 50.78 & 27.71 & 2.73 & 3.73 & 3.09 & 3.14 & 3.72 & 2.39 & 3.19 \\\\\nLongVA & 16 & 61.61 & 27.87 & 2.78 & 3.61 & 3.13 & 3.15 & 3.75 & 2.40 & 3.22 \\\\\nLongVA & 32 & 67.08 & 27.87 & 2.80 & 3.65 & 3.08 & 3.10 & 3.74 & 2.28 & 3.19 \\\\\nLongVA & 64 & 68.27 & 27.81 & 2.84 & 3.64 & 3.05 & 3.09 & 3.77 & 2.44 & 3.14 \\\\\nLongVA-DPO & 32 &  69.26 & 28.02 & 2.80 & 4.07 & 3.55 &  3.32& 4.09 & 2.86 & 3.58\t\\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{center}\n\\caption{Video evaluation results for LongVA on various short video benchmarks with comparison to 7B scale models.}\n\\vspace{-20pt}\n\\label{tab:short-video-results}\n\\end{table}\n\n\\subsection{Image Evaluation}\n\\label{sec: image-eval}\n\\begin{table}[htp]\n    \\begin{center}\n    \\setlength{\\tabcolsep}{8pt}\n    \\resizebox{\\textwidth}{!}{\n        \\begin{tabular}{l|ccccccc}\n        \\toprule\nModel                          & AI2D~\\citep{kembhavi2016diagram}  & ChartQA~\\citep{masry2022chartqa} & DocVQA~\\citep{mathew2020docvqa} & InfoVQA~\\citep{mathew2020docvqa} & RealworldQA~\\citep{grok_1.5v_2024} & MMMU~\\citep{yue2023mmmu} \\\\\n\\midrule\nLLaVA-1.6-Vicuna~\\citep{liu2024llavanext} & 66.6 & 54.8   & 74.4  & 37.1   & 57.8 & 35.1     \\\\\nLLaVA-NeXT-LLaMA3~\\citep{li2024llavanext-strong} & 71.6 & 69.5   & 78.2  & 37.6   & 60.0    & 41.7     \\\\\nLLaVA-NeXT-Qwen2               & 73.5 & 74.0      & 81.3  & 42.0   & 61.6  & 41.9    \\\\\nLongVA (\\textit{AnyRes}) & 73.1 & 74.4 & 81.5 & 43.3 & 62.4 & 42.1 \\\\\nLongVA (\\textit{UniRes})  & 70.7 & 70.4   & 80.8  & 49.4   & 60.0   & 42.6      \\\\\n\\bottomrule\n\\end{tabular}\n    }\n\\end{center}\n\\vspace{1mm}\n\\caption{Image evaluation results for LongVA on multiple benchmarks. Compared to other image multimodal models, our methods maintain high performance and achieve better scores on InfoVQA\\citep{mathew2020docvqa}.}\n\\vspace{-20pt}\n\\label{tab:image-results}\n\\end{table}\n\n\\begin{figure}[htp]\n    \\centering\n    \\makebox[0.31\\textwidth]{}\n    \\makebox[0.31\\textwidth]{}\n    \\makebox[0.31\\textwidth]{}\n    \\\\\n    \\includegraphics[width=0.31\\textwidth]{image/ai2d_image_plot.jpg}\n    \\includegraphics[width=0.31\\textwidth]{image/chartqa_image_plot.jpg}\n    \\includegraphics[width=0.31\\textwidth]{image/infovqa_val_image_plot.jpg}\n\n    \\caption{The 2D-histogram of the image width and height of different image benchmarks. InfoVQA\\citep{mathew2020docvqa} consists of many high-resolution images compared to other benchmarks. }\n    \\label{fig:image-size}\n\\end{figure}\n\nWe further evaluate our model on various image benchmarks to investigate the image performance of LongVA (Table \\ref{tab:image-results}). Compared to the LongVA (\\textit{AnyRes}) baseline, LongVA with \\textit{UniRes} achieves significantly increased performance on InfoVQA \\citep{mathew2020docvqa}, while the scores drop to some extent on AI2D~\\citep{kembhavi2016diagram} and ChartQA \\citep{masry2022chartqa}. To better understand this phenomenon, we recorded and analyzed the image size of those datasets, as shown in Figure \\ref{fig:image-size}. We found that InfoVQA consists of higher-resolution images, while many images in AI2D and ChartQA are smaller than 768$\\times$768.\nCompared to \\textit{Anyres}, \\textit{UniRes} operate 2$\\times$2 average pooling on each image, reducing to $1/4$ visual tokens per image grid. However, the grid upper bound is set to 49 for \\textit{UniRes} while 4 for \\textit{AnyRes}, so \\textit{UniRes} may produce more image grids if the input images are of higher resolution. By using more grids per image, \\textit{UniRes} allocates more visual tokens on datasets such as InfoVQA, achieving superior performance compared to the previous 7B LLaVA model. However, most of the images in ChartQA and AI2D require fewer than 4 grids to represent. This may explain why the image performance decreases on those benchmarks.\n\n\\section{Qualitative Results}\nThe qualitative results of LongVA-DPO are illustrated in Figure \\ref{fig:demo}. The short video example comes from \\citep{xie2023funqa} and the two long videos are sourced from \\href{https://www.bilibili.com/video/BV1Kn4y1d7G7/?buvid=c066506d947f48e761c78e00991fd406&from_spmid=main.space-contribution.0.0&is_story_h5=false&mid=804D54DYmsNyGN1vdgHxnA%3D%3D&p=1&plat_id=116&share_from=ugc&share_medium=iphone&share_plat=ios&share_session_id=35F6AE42-FAD2-436C-B164-107EF91E283D&share_source=WEIXIN&share_tag=s_i&spmid=united.player-video-detail.0.0&timestamp=1719191857&unique_k=AOiQBaI&up_id=23947287&share_source=weixin}{link1} and \\href{https://www.bilibili.com/video/BV1F4411v7rj/?spm_id_from=333.1007.top_right_bar_window_custom_collection.content.click&vd_source=a8b9ec3a8fda7923ed9b151c46917050}{link2}, respectively. In the figure, LongVA accurately describes the short, humorous video involving individuals playfully interacting with condiments. It also identifies specific details in long videos, such as the color of a train and the colors of umbrellas used in a scene, showcasing its proficiency in retrieving and interpreting visual information over extended video contexts. These capabilities highlight LongVA's potential to overcome the challenges associated with processing and understanding extremely long videos.\n\n\\begin{figure}[htp]\n    \\centering\n    \\includegraphics[width=\\textwidth]{image/demo1.pdf}\n    \\includegraphics[width=\\textwidth]{image/demo3.pdf}\n    \\includegraphics[width=\\textwidth]{image/demo4.pdf}\n    \\caption{Qualitative Result of the LongVA-DPO. Our model demonstrates superior capability in understanding both short and long videos. \n    }\n    \\vspace{-10pt}\n    \\label{fig:demo}\n\\end{figure}\n\n\\section{Conclusion}\n\nThis work addresses the challenges of understanding long videos in Large Multimodal Models. By extending the language model on text and then aligning this extended model with visual inputs, we significantly improved the capability of LMMs to handle long videos thanks to the \\textit{long context transfer} phenomenon. Our model, LongVA, shows improved performance with more input frames and achieves state-of-the-art results on Video-MME. Additionally, we introduce a synthetic benchmark, V-NIAH, to effectively measure the visual context length of video LMMs.  We hope this work inspires further research in the field of long video LMMs and multimodal agents.\\section{Acknowledgements}\nThis research/project is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG2-PhD-2022-01-029). Besides, this project is supported by NTU NAP, MOE AcRF Tier 2 (MOE-T2EP20221-0012), and under the RIE2020 Industry Alignment Fund ‚Äì Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as advise from the industry partner(s).\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding}\n\n\\begin{document}\n\n\\maketitle\n\n\\section{Introduction}\n\nLarge Language Models (LLMs)~\\citep{brown2020language,ouyang2022training,chatgpt,achiam2023gpt,vicuna2023,touvron2023llama2,jiang2024mixtral} manifest universal capabilities that are instrumental in our progress towards general intelligence. Through the integration of modality alignment and visual instruction tuning, Multimodal Large Language Models (MLLMs)~\\citep{alayrac2022flamingo,li2023blip2bl,zhu2023minigpt,liu2024visual,ye2023mplugowl,bai2023qwen,chen2023internvl,dong2024internlm} have demonstrated exceptional competencies in tasks such as captioning and visual question-answering. Recent literatures have initiated explorations of  extending MLLMs for the comprehension of video content~\\citep{li2023videochat,zhang2023video,maaz2023video,lin2023video,wang2024internvideo2,liu2024world}. Despite exhibiting potentials across specific benchmarks, effectively processing and understanding of exceedingly lengthy videos remains a significant challenge.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{assets/teaser.pdf}\n    \\caption{Effectiveness of our \\modelname~over commonly-used uniform sampling and dense sampling. Uniform sampling overlooks critical frames due to its sparse nature. Dense sampling may surpass the maximum context length, leading to truncation of tokens from targeted frames. In contrast, our method can adaptively conduct spatiotemporal compression, accommodating long video sequences while preserving more visual details.}\n    \\label{fig:teaser}\n\\end{figure}\n\nOne primary reason is that it is impractical to process all the information for hour-long videos, given that advanced MLLMs represent a single image using hundreds of tokens. For instance, 576 $\\sim$ 2,880 tokens per image are used in LLaVA-1.6~\\citep{liu2024llavanext} and 7,290 tokens are used in LLaVA-OneVision~\\citep{li2024llava}. However, a commonly used and computationally manageable context length for multimodal training is 8k, which limits processing 125 frames (2-minutes video) even at 64 tokens per frame, while an hour-long video could require over 200k tokens. Consequently, in video scenarios with an extra temporal dimension, it is intractable for training due to the demand of excessive GPU memory. Various studies have attempted to establish a balance between the number of tokens and the frequency of frame sampling. Most of these studies~\\citep{li2024llava,cheng2024videollama,zhang2024llavanextvideo,chen2024sharegpt4video} opt for a uniform sampling of a fixed number of video frames as the input. However, these methods naively overlook non-uniform content, e.g., static vs dynamic scenes within the video, as shown in Figure~\\ref{fig:teaser}. Other approaches~\\citep{li2023videochat,li2023llama,jin2023chatunivi} employ intensive resampling modules that significantly decrease the quantity of visual tokens, leading to a considerable loss of essential visual information.\n\nIn this paper, we propose \\modelname~that aims to preserve as much frame information as possible while accommodating lengthy videos without exceeding the context length of commonly used LLMs. Video by its nature contains significant temporal redundancy. MovieChat~\\citep{song2024moviechat} employs a similarity-based frame-level feature selection using visual representation from CLIP~\\citep{radford2021learning}. While we argue that DINOv2~\\citep{oquab2023dinov2}, through self-supervised training with a feature similarity objective on vision-centric tasks, captures subtle frame differences and low-level visual features more effectively than vision-language contrastive methods~\\citep{radford2021learning,zhai2023sigmoid}, as shown in Figure~\\ref{fig:dinosim}. Hence, \\textbf{(1)} we apply a temporal reduction strategy on the frame sequence by leveraging similarity from DINOv2~\\citep{oquab2023dinov2} features to remove redundant video frames. In addition, \\textbf{(2)} we jointly capture the detailed spatial semantic and long-range temporal context by performing selective feature reduction via cross-modal query, where we preserve full tokens for frames that are relevant to the given text query, while applying spatial pooling to reduce the remaining frames to a low-resolution token representation. \\textbf{(3)} A spatial token reduction mechanism based on temporal dependencies is applied for excessively long videos. As a result, our model is capable of processing 1fps sampled video input with high performance, which can adaptively reduce the number of tokens per frame to 2 on average to accommodate an hour-long video for MLLM within 8k context length.\n\nTo evaluate our method, we conduct extensive experiments across various video understanding benchmarks, including EgoSchema~\\citep{mangalam2024egoschema}, MVBench~\\citep{li2024mvbench}, VideoMME~\\citep{fu2024video}, and MLVU~\\citep{zhou2024mlvu}. Our \\modelname~significantly outperformes several recent open-source video LLM models, such as VideoChat2~\\citep{li2024mvbench}, LongVA~\\citep{zhang2024long}, and LLaVA-OneVision~\\citep{li2024llava}, by a large margin. For example, our \\modelname~outperforms a strong open-source baseline, LLaVA-OneVision~\\citep{li2024llava} by approximately $\\sim$5\\% in average accuracy. We also observed that our light-weight \\modelname, basing Llama3.2-3B~\\citep{llama32} as the language backbone, significantly improves over previous state-of-the-art small video-LLMs, e.g., Phi-3.5-vision-instruct-4B~\\citep{abdin2024phi}, by 3.4\\% on VideoMME Long subset. Our \\modelname~established new state-of-the-art results on video understanding benchmarks among video-language models. We believe that our proposed approach marks a meaningful progression towards long video understanding MLLMs. \n\n\\subsection{Vision Language Models} \n\nEarly visual language models (VLMs) such as CLIP~\\citep{radford2021learning}, is trained with a contrastive loss to project both vision and language embeddings to a shared representation space. SigLIP~\\citep{zhai2023sigmoid} takes a sigmoid loss instead, allowing further scaling up training batch size with better performance.\n\nThe development of LLMs has significantly advanced VLMs. Kosmos-1~\\citep{huang2023language,peng2023kosmos} introduces an end-to-end framework that integrates visual inputs with LLM in a cohesive training regime. Flamingo~\\citep{alayrac2022flamingo} and BLIP-2~\\citep{li2023blip} merge visual and linguistic features through cross-attention and a Q-Former module, respectively. MiniGPT-4~\\citep{zhu2023minigpt} and LLaVA~\\citep{liu2024visual} simplify the integration by projecting visual features directly into the LLM embedding space using a MLP.\n\nLater studies~\\citep{chen2023shikra,peng2023kosmos,wang2023cogvlm,chen2023minigpt} have expanded LMM applications to broader multi-modal tasks, enhancing spatial perception through visual grounding. Recent efforts~\\citep{liu2024llavanext,dong2024internlm} aim to create general models that unify diverse tasks, employing sophisticated optimization techniques, high-quality multi-task datasets, and complex training strategies to boost performance across extensive vision-language tasks. Cambrian~\\citep{tong2024cambrian} combines features from multiple vision encoders with Spatial Vision Aggregator (SVA) for a more capable MLLM. By exploring different vision encoders, Cambrian~\\citep{tong2024cambrian} finds that  SigLIP~\\citep{zhai2023sigmoid} is a strong language-supervised model and  DINOv2~\\citep{oquab2023dinov2} performs well on  vision-centric tasks.\n\n\\subsection{Video Large Language Models}\n\nRecent advancements in MLLMs have broadened their application to video understanding tasks. Video LMMs process videos by extracting and encoding frames, then rearranging these as final video features. Several works~\\citep{li2023videochat,li2024mvbench,cheng2024videollama}, use the Q-Former module from BLIP-2 to merge visual and text features, while others~\\citep{lin2023video,luo2023valley,ataallah2024minigpt4} concatenate frame features directly. \n\nWhen processing lengthy videos, the constraint on context length inevitably causes a trade-off between the number of tokens per frame and the number of frames to input. Most existing works~\\citep{li2023videochat,ataallah2024minigpt4,cheng2024videollama,zhang2024llavanextvideo,li2024llava} address this challenge by uniformly sampling frames from the video, which, however, results in a significant loss of visual details within the video. Video-ChatGPT~\\citep{maaz2023videochatgpt} employs pooling modules to reduce data dimensions, enhancing processing efficiency. Other works try to preserve the maximum number of frames in video content. LLaMA-VID~\\citep{li2023llama} employs an additional text decoder to embed the text query for cross-attention between frame features and compress the context token to one token per frame, while MovieChat~\\citep{song2023moviechat} and TimeChat~\\citep{ren2023timechat} develop memory modules and timestamp-aware encoders to capture detailed video content. Golfish~\\citep{ataallah2024goldfish} segments long videos into shorter clips, processes each segment independently, and retrieves the most relevant segment in response to user queries. Our work focuses on maximizing the preservation of frames in video content (1fps) within given context length by proposing spatiotemporal compression of video tokens.\n\n\\subsection{Video Token Compression}\n\nRecent methods has explored dynamic image tokens~\\citep{ma2023image,xu2022groupvit,bolya2022token} or video tokens~\\citep{lee2024multi,ren2023testa,choi2024vid} within the Transformer~\\citep{vaswani2017attention} framework. Chat-UniVi~\\citep{jin2023chatunivi} extends the dynamic tokens for visual features in MLLMs by merging K-nearest neighbor tokens across frame features of the video input. SlowFast-LLaVA~\\citep{xu2024slowfast} uniformly samples 8 frames for high-resolution tokens, while performing spatial pooling to decrease the number of tokens in frames sampled at a higher frame rate. In our work, we propose a spatiotemporal adaptive token reduction strategy that leverages both cross-modal query and inter-frame dependencies. This approach effectively mitigates temporal redundancy in video content, thereby enabling the accommodation of long videos within given context length.\\section{Method}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{assets/main.pdf}\n    \\caption{Architecture of \\modelname. Given a densely sampled video frames, we first utilize DINOv2~\\citep{oquab2023dinov2} prior to remove redundant frames, and fuse the remaining frame features from both SigLIP~\\citep{zhai2023sigmoid} and DINOv2~\\citep{oquab2023dinov2}, described in Section~\\ref{sec:dino}. Then we selectively reduce visual tokens via cross-modal query, detailed in Section~\\ref{sec:query}. Finally, as demonstrated in Section~\\ref{sec:prune}, we conduct spatial token compression based on temporal dependencies to further meet the context length of LLMs.}\n    \\label{fig:main}\n\\end{figure}\n\nWe propose spatiotemporal adaptive compression in three steps to effectively process long video, as shown in Figure~\\ref{fig:main}. Initially, we implement a temporal reduction strategy on the frame sequence by leveraging the prior knowledge from DINOv2~\\citep{oquab2023dinov2} (Section~\\ref{sec:dino}). Then, we selectively preserve full tokens for key frames via cross-modal query, while applying spatial pooling to reduce the remaining frames into low-resolution token representations (Section~\\ref{sec:query}). Furthermore, we implement a spatial token reduction mechanism based on inter-frame temporal dependencies (Section~\\ref{sec:prune}).\n\n\\subsection{Frame Feature Extractor and Temporal Reduction} \n\\label{sec:dino}\n\n DINOv2~\\citep{oquab2023dinov2}, through its self-supervised (SSL) training with a feature similarity objective on vision-centric tasks, can effectively capture subtle frame differences and low-level visual features. In contrast, CLIP-based~\\citep{zhai2023sigmoid,radford2021learning} models are trained with vision-language contrastive loss in the semantic space, excelling at language alignment while sacrificing low-level features as shown in Figure~\\ref{fig:dinosim}. Moreover, Cambrian~\\citep{tong2024cambrian} discovered that combining features from both SigLIP~\\citep{zhai2023sigmoid} and DINOv2~\\citep{oquab2023dinov2} leads to a significant performance boost in vision-centric tasks. Therefore, we pioneer to leverage both SSL-based model DINOv2~\\citep{oquab2023dinov2} with vision-language contrastive-based model SigLIP~\\citep{zhai2023sigmoid} as frame feature extractors for MLLM in video understanding task.\n \n Note that processing the entire long video can be computationally expensive. Given a 1fps-sampled video with $N$ frames, denoted as $I=\\{I^1,...,I^N\\}$, we first use DINOv2~\\citep{oquab2023dinov2} to extract features from each frame, leading to a set of DINO features $\\{V_{\\text{dino}}^1, \\ldots, V_{\\text{dino}}^N\\}$. We then calculate the average similarity $\\text{sim}^i = \\frac{1}{J-1} \\sum_{j=1, j \\neq i}^J \\text{sim}(V_{\\text{dino}}^i, V_{\\text{dino}}^j)$ within each non-overlapping window with $J=8$ frames and reduce frames that exhibit high similarity with other frames. This step significantly reduces video redundancy by temporally compressing the original $N$ frames to $T$ frames, which reduces approximately half of the video frames, as detailed in Section~\\ref{sec:compress}. \n \n We then extract features of the remaining $T$ frames using SigLIP~\\citep{zhai2023sigmoid} vision encoder, resulting in $T$ features $\\{V_{sig}^1,...,V_{sig}^T\\}$. Subsequently, following Cambrian~\\citep{tong2024cambrian}, we combine these two types of visual features via Spatial Vision Aggregator (SVA)~\\citep{tong2024cambrian} that employs learnable queries to spatially aggregate visual features from multiple vision encoders. We denote the fused frames features as $V=\\{V^1,...,V^T\\}$.\n\n\\subsection{Selective Feature Reduction via Cross-modal Query}\n\\label{sec:query}\n\nAfter temporal reduction, we obtain a set of fused frame features from both vision encoders, $V=\\{V^1,...,V^T\\} \\in \\mathbb{R}^{T\\times (H_{h}\\times W_{h}) \\times D_v}$, where $H_{h}\\times W_{h}$ denotes the spatial dimension of the frame features, and $D_v$ indicates the channel dimension of the frame feature after SVA. If the concatenated frame features exceed the given context length, i.e., $T\\times H_h\\times W_h \\ge L_{max}$, we develop a selective compression strategy for certain frames, in order to capture both the detailed spatial semantic and long-range temporal context.\n\nTo achieve this, we propose using text query to help reduce spatial tokens of certain frames from $H_{h}\\times W_{h}$ to $H_{l}\\times W_{l}$. Given the LLM embedding of the text query $Q \\in \\mathbb{R}^{L_q\\times D_q}$, where $L_q$ is the length of text query and $D_q$ is the dimensionality of LLM's embedding space, we strategically choose $N_h$ frames to preserve their original token resolution, while the remaining undergoes a process of spatial pooling to achieve a reduced resolution. The selection mechanism is based on the cross-modal attention scores between each frame feature and the text query. The number of frames to keep original resolution can be formulated as, \n\n\\begin{equation}\\label{eq:query}\n    \\mathbf{Top}_{N_h} \\left (\\frac{1}{H_hW_hL_q} \\sum_{h,w,l}\\mathcal{F}(V)Q^T \\right ), \\quad N_h = \\max\\left (0, \\frac{L_{\\text{max}} - L_q - T H_l W_l}{H_hW_h - H_lW_l}\\right ),\n\\end{equation}\nwhere $L_{max}$ is the given context length, $\\mathcal{F}(\\cdot)$ denotes a multi-layer perceptron (MLP)-based multimodal adapter designed to align visual features with the input space of the LLM. Note that we omit the system prompt in the instruction template for Equation~\\ref{eq:query} simplification. If $N_h=0$, indicating that no frames are selected for retention at their original resolution, we will skip the computation of attention scores and will directly perform spatial pooling across all the frames to the lower resolution.\n\n\\subsection{Spatial Token Compression}\\label{sec:prune}\n\nAs previously discussed, there are cases where the concatenated visual features with low resolution tokens still exceeds the given context length, i.e., $T\\times H_l\\times W_l \\geq L_{max}$. Under these circumstances, further token compression is necessary. We partition the sequence of frame features into non-overlapping segments with a sliding window of size $K<T$, within which we conduct spatial token compression (STC). The first frame in each window retains its full token resolution. We then compute the cosine similarity between the first frame and subsequent frames within the window, conducting an element-wise comparison of spatial tokens between the first frame and its successors. Spatial tokens that exhibit a cosine similarity $\\text{sim}(\\cdot,\\cdot)$ greater than the threshold $\\theta$ with the corresponding tokens of the first frame at the same spatial location will be pruned, which can be formulated as, \n\\begin{equation}\\label{eq:prune} v_i^* \\leftarrow \\begin{cases} v_i(h, w) & \\text{sim}(v_1(h, w), v_i(h, w)) \\leq \\theta \\\\ \\emptyset & \\text{otherwise} \\end{cases}, \\quad \\forall h \\in [1, H_l], w \\in [1, W_l], i \\in [2, K] \\end{equation}\n\nGiven that videos often contain significant pixel-level redundancy, particularly in static background, this method allows spatial tokens reduction via temporal dependencies.\nWe chose the first frame in each sliding window for comparison, assuming DINOv2~\\citep{oquab2023dinov2} has effectively reduced video redundancy across frames, making each frame less similar. We also tested alternative strategies, like using the middle frame or adaptively selecting based on frame changes (Section~\\ref{sec:ablation}), but these provided similar performance and compression rates. Therefore, we chose the first-frame strategy in each sliding window for its simplicity and effectiveness.\n\\section{Experiments}\n\n\\subsection{Datasets}\n\nWe adopt two stages of training in our experiments: image-language pre-training and video-language finetuning. For the image-language pre-training stage, previous methods~\\citep{chen2023shikra,peng2023kosmos,wang2023cogvlm,chen2023minigpt,liu2024llavanext,dong2024internlm} usually use two steps for alignment and finetuning. For simplicity, we combine these two steps in one stage using Single-Image data from LLaVA-OneVision~\\citep{li2024llava}. For video-language finetuning, we utilize a large-scale video-text pairs sourced from several publicly accessible databases. The video training data contains a subset of VideoChat2-IT~\\citep{li2024mvbench}, which includes TextVR~\\citep{wu2025large}, Youcook2~\\citep{zhou2018towards}, Kinetics-710~\\citep{kay2017kinetics}, NExTQA~\\citep{xiao2021next}, CLEVRER~\\citep{yi2019clevrer}, EgoQA~\\citep{fan2019egovqa}, TGIF~\\citep{li2016tgif}, WebVidQA~\\citep{yang2021just}, ShareGPT4Video~\\citep{chen2024sharegpt4video}, and MovieChat~\\citep{song2024moviechat} as the long video complementary. All the training datasets are listed in Table~\\ref{tab:traindata}.\n\n\\subsection{Benchmarks and metrics}\nWe evaluate our model on EgoSchema~\\citep{mangalam2024egoschema}, MVBench~\\citep{li2024mvbench}, VideoMME~\\citep{fu2024video} and MLVU~\\citep{zhou2024mlvu}. VideoMME~\\citep{fu2024video} (1 min $\\sim$ 1 hour) and MLVU~\\citep{zhou2024mlvu} (3 mins $\\sim$ 2 hours) are long video benchmarks for assessing long video understanding ability. For VideoMME~\\citep{fu2024video}, videos are officially split based on duration, which  contains a subset of long videos ranging from 30 minutes to 1 hour. We perform standardized evaluations using greedy decoding (\\textit{num\\_beams}=1) and benchmark our results against other open-source and proprietary models.\n\n\\begin{table}[!htbp]\n    \\centering\n\\begin{adjustbox}{width=\\linewidth,center}\n\\renewcommand{\\arraystretch}{1.2}\n\\setlength{\\tabcolsep}{1.5mm}\n\\begin{tabular}{lcccccccccc}\n\\toprule  \\multirow{2}{*}{\\textbf{Models}} & \\multirow{2}{*}{\\textbf{Size}} & \\multirow{2}{*}{\\textbf{Context Length}} & \\multirow{2}{*}{\\textbf{\\#Frames}} & \\multirow{2}{*}{ \\textbf{EgoSchema} }  & \\multirow{2}{*}{\\textbf{MVBench}} & \\multirow{2}{*}{\\textbf{MLVU}} & \\multicolumn{2}{p{3.2cm}}{\\centering \\textbf{VideoMME} } \\\\ \\cline{8-9}\n&&&&&&& Overall & Long \\\\\n\\rowcolor{gray!10} Duration & & & & 179.8 sec & 16 sec & 3$\\sim$120 min & 1$\\sim$60 min & 30$\\sim$60 min \\\\\n\\midrule\n\\textit{Proprietary Models} \\\\\nGPT4-V~\\citep{openai2023gpt4v} & - & - & 1fps & 55.6 & 43.7  & - & 60.7 & 56.9 \\\\\nGPT4-o~\\citep{openai2024gpt4o} & - & - & 1fps & 72.2 & 64.6  & 66.2 & 77.2 & 72.1 \\\\\n\\midrule\n\\textit{Open-Source Video MLLMs} \\\\\nVideo-LLaVA~\\citep{lin2023video} & 7B & 4k & 8 & 38.4 & 41.0  & 47.3 & 40.4 & 38.1 \\\\\nLLaMA-VID~\\citep{li2023llama} & 7B & 4k & 1fps & 38.5 & 41.9  & 33.2 & - & - \\\\\nChat-UniVi~\\citep{jin2023chatunivi} & 7B & 4k & 64 & - & -  & - & 45.9 & 41.8 \\\\\nShareGPT4Video~\\citep{chen2024sharegpt4video} & 8B & 8k & 16 & - & 51.2  & 46.4 & 43.6 & 37.9 \\\\\nLLaVA-NeXT-Video~\\citep{zhang2024llavanextvideo} & 7B & 8k & 32 & 43.9  & 33.7 & - & 46.5 & - \\\\\nVideoLLaMA2~\\citep{cheng2024videollama} & 7B & 8k & 32 & 51.7 & 54.6  & 48.5 & 46.6 & 43.8 \\\\\nLongVA~\\citep{zhang2024long} & 7B & 224k & 128 & - & -  & 56.3 & 54.3 & 47.6 \\\\\nVideoChat2~\\citep{li2024mvbench} & 7B & 8k & 16 & 54.4 & 60.4  & 47.9 & 54.6 & 39.2 \\\\\nLLaVA-OneVision~\\citep{li2024llava} & 7B & 8k & 32 & 60.1 & 56.7  & 64.7 & 58.2 & 46.7 \\\\\n\\rowcolor{blue!10} \\modelname~(Ours) & 7B & 8k & 1fps & \\textbf{67.6} & \\textbf{66.9}  & \\textbf{65.4} & \\textbf{60.6} & \\textbf{59.5} \\\\\n\\bottomrule\n\\end{tabular}\n\\end{adjustbox}\n\\caption{Results on comprehensive video understanding benchmarks}\n\\label{tab:main}\n\\end{table}\n\n\\subsection{Implementation Details}\n\nWe use SigLIP~\\citep{zhai2023sigmoid} (so400m-patch14-384) and DINOv2~\\citep{oquab2023dinov2} as the vision encoder while choose Qwen2-7B~\\citep{qwen2} and Llama3.2-3B~\\citep{llama32} as our language foundation model. We only compute cross-entropy loss for autoregressive text generation. We use AdamW~\\citep{loshchilov2017decoupled} optimizer with a cosine schedule for all the trainings. In the image-language pre-training stage, we train the model for one epoch with global batch size of 128. The learning rate is set to 1e-5, and the warmup rate is 0.03. The number of tokens per image are set to 576. For the video-language finetuning stage, we train the model for one epoch with global batch size of 64. The learning rate is set to 1e-5, and the warmup rate is 0.03. The maximum number of tokens per frame are set to 144 ($H_h=W_h=12$), while each might be reduced by our proposed adaptive compression approach ($\\leq 64, \\, H_l=W_l=8$). The STC reduction threshold $\\theta=0.8$ and the sliding window size $K = 8$. Our model is trained on 64 NVIDIA H100 GPUs.\n\n\\subsection{Video Understanding}\n\n\\noindent\\textbf{Quantitative Results.} Table~\\ref{tab:main} presents our experimental results on multiple video understanding benchmarks. Our results compares favorably to all the baselines across various video understanding benchmarks. For example, on VideoMME~\\citep{fu2024video}, our LongVU outperforms VideoChat2~\\citep{li2024mvbench}, LLaVA-OneVision~\\citep{li2024llava} by 6.0\\% and 2.4\\% respectively. Notably, on VideoMME Long subset~\\citep{fu2024video}, our model surpasses LLaVA-OneVision~\\citep{li2024llava} by 12.8\\%. These results indicate the strong video understanding capabilities of our model. Note that our model achieves significant improved performance with a much smaller training dataset, comparing to LLaVA-OneVision~\\citep{li2024llava} trained on OneVision-1.6M (multi-image, video) that has not yet been made publicly available\\footnote{LLaVA-OneVision~\\citep{li2024llava} only release single-image set at the time of current submission. \\href{https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-Data/discussions/6}{https://huggingface.co/datasets/lmms-lab/LLaVA-OneVision-Data/discussions/6}}. With the same video training dataset from VideoChat2-IT~\\citep{li2024mvbench}, our LongVU shows much higher performance than VideoChat2~\\citep{li2024mvbench}, $\\sim$10\\% accuracy improvement in average. \nInterestingly, we also find that our model can even beat proprietary model GPT4-o~\\citep{openai2024gpt4o} on MVBench~\\citep{li2024mvbench} with densely sampled video input and reduce the accuracy gap comparing to proprietary models on other video benchmarks. \n\nWe also scale our LongVU with a lightweight LLM, Llama3.2-3B~\\citep{llama32}, to further demonstrate the strong video understanding capabilities. We observe the consistent improvement of our light-weight LongVU over baselines in Table~\\ref{tab:mainsmall}. Our method outperforms Phi-3.5-vision-instruct~\\citep{abdin2024phi} on VideoMME (Long) by margin of 3.4\\% accuracy. This set of experiments validate the effectiveness of our method even scaling to a smaller size. \n\n\\begin{table}[!htbp]\n    \\centering\n\\begin{adjustbox}{width=\\linewidth,center}\n\\renewcommand{\\arraystretch}{1.2}\n\\setlength{\\tabcolsep}{1.5mm}\n\\begin{tabular}{lccccccccc}\n\\toprule  \\multirow{2}{*}{\\textbf{Models}} & \\multirow{2}{*}{ \\textbf{EgoSchema} }  & \\multirow{2}{*}{\\textbf{MVBench}} & \\multicolumn{2}{p{2.5cm}}{\\centering \\textbf{VideoMME} } & \\multirow{2}{*}{\\textbf{MLVU}} \\\\ \\cline{4-5}\n& & & Overall & Long & & \\\\\n\\midrule\nInternVL2 (InternLM2-1.8B)~\\citep{internvl2} & - & 60.2 & 47.3 & 42.6 & -\\\\\nVideoChat2 (Phi-3-mini-4B)~\\citep{li2024mvbench} & 56.7 & 55.1 & - & - & - & \\\\\nPhi-3.5-vision-instruct (Phi-3-mini-4B)~\\citep{abdin2024phi} & - & - & 50.8 & 43.8 & - \\\\ \n\\rowcolor{blue!10} \\modelname~(Ours) (Llama3.2-3B) & \\textbf{59.1} & \\textbf{60.9} & \\textbf{51.5} & \\textbf{47.2} & 55.9 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{adjustbox}\n\\caption{Results of small-size video language models across video understanding benchmarks.}\n\\label{tab:mainsmall}\n\\end{table}\n\n\\noindent\\textbf{Qualitative Results.} We now provide the qualitative results in Figure~\\ref{fig:qual}. Specifically, we demonstrate various video understanding abilities in the examples, such as accurately recognizing the orientation of moving objects in Figure~\\ref{fig:qual}(a), providing detailed video descriptions in Figure~\\ref{fig:qual}(b), identifying inserted needle frames and conducting action counting in Figure~\\ref{fig:qual}(c), and responding precisely to questions about specific frames in an hour-long video in Figure~\\ref{fig:qual}(d). These results demonstrate that our model has competing video-language understanding capabilities. \n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.95\\linewidth]{assets/qual.pdf}\n    \\caption{Examples for various video understanding capabilities of LongVU model. We showcase that our LongVU is able to complete different types of video understanding tasks.}\n    \\label{fig:qual}\n\\end{figure}\n\n\\subsection{Ablation Studies}\n\\label{sec:ablation}\n\n\\noindent\\textbf{Effects of the number of tokens per frame.} We ablate the number of tokens in our uniform-sampling baselines. There is a trade-off between the number of tokens per frame and the sampling frequency of frames. Table~\\ref{tab:ablation} shows the experimental results when using different number of tokens with different sampling. When applying uniforming sampling, 144 tokens per frame shows better performance than 64 tokens in an 8k context length on VideoMME~\\citep{fu2024video} and MLVU~\\cite{zhou2024mlvu} while worse on EgoSchema~\\cite{mangalam2024egoschema}. With 144 tokens per frame, it preserves more visual details, but restricts the total number of frames, i.e., less than 60 frames within 8k context length. This demonstrate that adaptive tokens are needed for better performance across different video benchmarks. \n\n\\noindent\\textbf{DINOv2 vs SigLIP.}  Our results in Table~\\ref{tab:ablation} verify that DINOv2~\\citep{oquab2023dinov2} features are more effective than SigLIP~\\citep{zhai2023sigmoid} features. As expected, we also find that using DINO-based features for temporal frame reduction outperforms uniform sampling. Therefore, DINOv2~\\citep{oquab2023dinov2} is an useful vision-centric feature extractor to help perform  temporal reduction. \n\n\\noindent\\textbf{Query guided selection.} We apply text-guided frame selection after temporal reduction, where relevant frames are maintained at full token capacity (144 tokens), while others are reduced to 64 tokens. This helps preserve essential visual features and accommodates more long-range context within the context length. In Table~\\ref{tab:ablation}, we observe the improvement with query guided frame selection across all benchmarks. Moreover, in Table~\\ref{tab:mlvu}, the results of each subtask in  MLVU~\\citep{zhou2024mlvu} show significant performance improvements when using cross-modal queries, particularly for frame-retrieval tasks such as counting and needle detection. \n\n\\noindent\\textbf{Spatial token compression.} We further apply spatial token compression after query guided selection. We find that spatial token compression (STC) not only enhances performance within 8k context length, but also achieve results comparable or slightly better than 16k context length in Table~\\ref{tab:ablation}. We also note some improvements for most subtasks in MLVU~\\citep{zhou2024mlvu}. \n\n\\begin{table}[!htbp]\n    \\centering\n\\begin{adjustbox}{width=0.9\\linewidth,center}\n\\renewcommand{\\arraystretch}{1.2}\n\\setlength{\\tabcolsep}{1.5mm}\n\\begin{tabular}{lccccc}\n\\toprule \\textbf{Methods} & \\textbf{Context Length} & \\textbf{\\#Tokens} & \\multicolumn{1}{c}{ \\textbf{EgoSchema} } & \\multicolumn{1}{c}{ \\textbf{VideoMME} } & \\multicolumn{1}{c}{\\textbf{MLVU}} \\\\\n\\midrule\n\\rowcolor{gray!10} Uniform & 16k & 144 & 67.12\t& 60.01 & 64.70 \\\\\n\\rowcolor{gray!10} DINO & 16k & 144 & 67.34 & 61.25 & 64.83 \\\\\n\\midrule\nUniform & 8k & 64 & 66.84 & 57.56 & 60.87 \\\\\nUniform & 8k & 144 & 66.28 & 58.84 & 63.28 \\\\\n\\midrule\nSigLIP & 8k & 64 & 66.04 & 58.63\t& 62.17 \\\\\nDINO & 8k & 64 & 66.20 & 59.90 & 62.54 \\\\\nDINO + Query & 8k & 64/144 & 67.30 & 60.08 & 65.05 \\\\\n\\rowcolor{blue!10} DINO + Query + STC (default) & 8k & dynamic & \\textbf{67.62} & \\textbf{60.56} & \\textbf{65.44} \\\\\n\\bottomrule\n\\end{tabular}\n\\end{adjustbox}\n\\caption{Ablation studies of number of tokens per frame, different context lengths, and our spatiotemporal compression components.}\n\\label{tab:ablation}\n\\end{table}\n\n\\begin{table}[!htbp]\n    \\centering\n\\begin{adjustbox}{width=0.95\\linewidth,center}\n\\begin{tabular}{lccccccccc}\n\\toprule \\textbf{Stratgy} & \\multicolumn{1}{c}{ \\textbf{count} } & \\multicolumn{1}{c}{ \\textbf{ego} } &  \\multicolumn{1}{c}{\\textbf{needle}} & \\multicolumn{1}{c}{ \\textbf{order} } & \\multicolumn{1}{c}{\\textbf{plotQA}} & \\multicolumn{1}{c}{\\textbf{anomaly}} & \\multicolumn{1}{c}{\\textbf{reasoning}} & \\multicolumn{1}{c}{\\textbf{Avg}} \\\\\n\\midrule\nDINO & 24.15 & 59.09 & 68.16 & 52.89 & 71.24 & 74.00 & 86.36 & 62.54 \\\\\nDINO+Query & 28.98 & 55.39 & \\textbf{78.87} & 56.37 & \\textbf{72.35} & 75.50 & \\textbf{87.87} & 65.05 \\\\\n\\rowcolor{blue!10} DINO+Query+STC (default) & \\textbf{28.98} & \\textbf{59.37} & 76.33 & \\textbf{58.30}\t& 71.61 & \\textbf{76.00} & 87.50 & \\textbf{65.44} \\\\\n\\bottomrule\n\\end{tabular}\n\\end{adjustbox}\n\\caption{Ablation study on each subtask in MLVU~\\citep{zhou2024mlvu}.}\n\\label{tab:mlvu}\n\\end{table}\n\n\\noindent\\textbf{Different strategies for spatial token compression.} We now ablate different strategies of our spatial token compression mechanism. This analysis explores different strategies for determining anchor frames: the first/middle one in each sliding window, or the frame that exhibits significant changes compared to its adjacent frames. In Table~\\ref{tab:stcab}, our results indicate that taking the first frame in each sliding window gives a slightly better performance with similar reduction rates across all strategies.\n\n\\begin{table}[!ht]\n    \\centering\n    \\renewcommand{\\arraystretch}{1.3}\n    \\begin{adjustbox}{width=0.9\\linewidth,center}\n    \\begin{tabular}{lcccccc}\n    \\toprule\n        \\textbf{Model}  & \\textbf{Short} & \\textbf{Medium} & \\textbf{Long} & \\textbf{Overall} & \\textbf{Reduction rate} \\\\\n        \\midrule\n        \\rowcolor{blue!10} $1^{st}$ frame in sliding window (default) & 64.7\t& 58.2 & 59.5 & 60.9 & 55.47\\% \\\\\n        $(K/2)^{th}$ frame in sliding window & 64.7 & 58.7\t& 58.6 & 60.7 & 54.97\\% \\\\ \n        frame with high changes & 64.7 & 58.2 & 58.3 & 60.4 & 55.62\\% \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\end{adjustbox}\n    \\caption{Different strategies for spatial token compression on VideoMME~\\citep{fu2024video}.}\n    \\label{tab:stcab}\n\\end{table}\n\n\\subsection{Spatiotemporal Compression Analysis}\n\\label{sec:compress}\n\n\\noindent\\textbf{Compression analysis.} We sampled hundreds of videos to demonstrate the distribution of frame/token reduction rate. Figure~\\ref{fig:reduce} (a) presents the number of frames before and after temporal reduction based on the similarity of DINOv2 features across frames. We find that $\\sim$45.9\\% of the frames are maintained after temporal reduction on average. Figure~\\ref{fig:reduce} (b) shows the number of tokens before and after spatial token compression (Section~\\ref{sec:prune}). We observe that $\\sim$40.4\\% tokens are reduced on average. These results demonstrate the effective video token compression with temporal and spatial token reduction. \n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=\\linewidth]{assets/reduce.pdf}\n    \\caption{We randomly sample hundreds of videos to demonstrate the frames/tokens level reduction rate. (a) The number of frames before/after temporal reduction based on DINOv2 features (Section~\\ref{sec:dino}). (b) The number of tokens before/after spatial token compression (Section~\\ref{sec:prune}).}\n    \\label{fig:reduce}\n\\end{figure}\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=\\linewidth]{assets/needle.pdf}\n    \\caption{Needle-in-a-Haystack results. Our adaptive token compression scheme improves the score for locating the needle frame within an hour-long video from 0.80 to 0.88 on average.}\n    \\label{fig:needle}\n\\end{figure}\n\n\\noindent\\textbf{Long context analysis.} Recently, the Needle-in-a-Haystack task~\\citep{hsieh2024ruler,kamradt2023needle} has been used to assess the ability of Large Language Models (LLMs) to retrieve long context information. We follow~\\citep{zhang2024long} to conduct a video needle-in-a-haystack experiment to demonstrate the effectiveness of our compression strategy on identifying the needle frame within an hour-long video.\n\nTo facilitate this evaluation, we randomly select an one-hour-long test video from MLVU~\\citep{zhou2024mlvu}. We then insert each image from a set of VQA problems as a needle frame into this long video for creating a challenging search task. We sample the video at 1 FPS and control the frame length ranging from 200 to 3.6k frames. We also vary the needle frame insertion depth from 0\\% to 100\\% of the total input frames. We conduct experiments with 8k context length and compare our adaptive token compression to the one without applying query-guided selection (w/o Query) and spatial token compression (w/o STC) after temporal reduction. Figure ~\\ref{fig:needle} demonstrates that our adaptive compression mechanism could accurately resolve the needle VQA problem of 1k frames within 8k context length and improve score with more frames. This demonstrates the advantage of our method for long context video understanding. \n\n\\section{Conclusion}\n\nWe introduced LongVU, a MLLM that can address the significant challenge of long video understanding within a commonly used context length. To achieve this, we proposed a spatiotemporal adaptive compression scheme of LongVU for helping reduce video tokens without losing much visual details of long videos by leveraging cross-modal query and inter-frame similarities. Experiments on various video understanding benchmarks consistently validate the advantages of our model. We also demonstrate that our method helps build a quality light-weight video language understanding model based on Llama3.2-3B, which suggests that LongVU has many potential applications in the vision-language community. \n\n\\clearpage\n\\newpage\n\\beginappendix\n\n\\section{Training Datasets}\n\nFor the image-language training stage, previous methods~\\citep{chen2023shikra,peng2023kosmos,wang2023cogvlm,chen2023minigpt,liu2024llavanext,dong2024internlm} usually use two stages of alignment and finetuning. For simplicity, we combine and alignment in one stage using single image version of LLaVA-OneVision~\\citep{li2024llava} data. For video-language training, we utilize a large-scale video-text pairs sourced from several publicly accessible databases. The video training data is a subset of VideoChat2-IT~\\citep{li2024mvbench}, which includes TextVR~\\citep{wu2025large}, Youcook2~\\citep{zhou2018towards}, Kinetics-710~\\citep{kay2017kinetics}, NExTQA~\\citep{xiao2021next}, CLEVRER~\\citep{yi2019clevrer}, EgoQA~\\citep{fan2019egovqa}, TGIF~\\citep{li2016tgif}, WebVidQA~\\citep{yang2021just}, ShareGPT4Video~\\citep{chen2024sharegpt4video}, in addition to above, we use MovieChat~\\citep{song2024moviechat} as long video complementary. All the training data is demonstrated in Table~\\ref{tab:traindata}.\n\n\\begin{table}[ht]\n\\centering                         \n\\renewcommand{\\arraystretch}{1.4}  \n\\setlength{\\tabcolsep}{1.5mm}      \n\\footnotesize                      \n\\begin{tabular}{c|c|cc}\n\\toprule\n\\textbf{Modality} & \\multicolumn{1}{c|}{\\textbf{Task}} & \\textbf{\\# Samples} & \\textbf{Dataset} \\\\ \n\\hline\n Image-Text & \\cellcolor{gray!10} Single-Image     & \\cellcolor{gray!10} 3.2M & \\cellcolor{gray!10} LLaVA-OneVision \\\\\n\\hline\n & Captioning     & 43K  & TextVR, MovieChat, YouCook2 \\\\\n & \\cellcolor{gray!10} Classification & 1K \\cellcolor{gray!10}  & \\cellcolor{gray!10} Kinetics-710 \\\\\n & VQA             & 424K  & \\begin{tabular}[c]{@{}l@{}}NExTQA, CLEVRER, EgoQA, \\\\ TGIF, WebVidQA, DiDeMo \\end{tabular} \\\\\n\\multirow{-4}{*}{Video-Text} & \\cellcolor{gray!10} Instruction & \\cellcolor{gray!10}  85K & \\cellcolor{gray!10}  \\begin{tabular}[c]{@{}l@{}} ShareGPT4Video \\end{tabular} \\\\ \n\\bottomrule\n\\end{tabular}\n\\vspace{3mm}\n\\caption{Training data statistics.}\n\\label{tab:traindata}\n\\end{table}\n\n\\begin{table}[ht]\n    \\centering\n    \\renewcommand{\\arraystretch}{1.3}\n    \\begin{tabular}{ccccccc}\n    \\toprule\n        \\textbf{Model} & \\textbf{Size} & \\textbf{Frames} & \\textbf{Short} & \\textbf{Medium} & \\textbf{Long} & \\textbf{Overall} \\\\\n        \\midrule\n        Video-LLaVA~\\citep{lin2023video} & 7B & 8 & 46.1 & 40.7 & 38.1 & 41.6 \\\\\n        ShareGPT4Video~\\citep{chen2024sharegpt4video} & 8B & 16 &  53.6 & 39.3 & 37.9  & 43.6 \\\\\n        Chat-Univi-v1.5~\\citep{jin2023chatunivi} & 7B & 64 & 51.2 &  44.6  & 41.8  & 45.9 \\\\\n        VideoLLaMA2~\\citep{cheng2024videollama} & 7B & 16 & 59.4 & 47.6  & 43.8 & 50.3 \\\\\n        VideoChat2~\\citep{li2024mvbench} & 7B & 16  & 52.8 & 39.4  & 39.2 & 43.8 \\\\ \n        LongVA~\\citep{zhang2024long} & 7B & 128 & 61.6 & 50.4  & 47.6 & 54.3 \\\\\n        LLaVA-OneVision~\\citep{li2024llava} & 7B & 32 & \\textbf{69.1} & 53.3 & 46.7 & 58.2 \\\\\n        \\rowcolor{blue!10} \\modelname~(Ours) & 7B & 1fps & 64.7\t& \\textbf{58.2} & \\textbf{59.5} & \\textbf{60.9} \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\caption{Comparison with other video LMMs on VideoMME~\\citep{fu2024video} benchmark.}\n    \\label{tab:videomme}\n\\end{table}\n\n\\section{Frame-level Position Encoding}\n\\label{sec:pos} \n\nTo alleviate potential confusion arising from frame-by-frame feature concatenation, we incorporate a frame-level position encoding to enforce the temporal boundaries across frames and capture inter-dependencies within each frame. Given that we temporally reduce several frames, a straightforward concatenation of all frames renders the model unaware of the relative timestep across frames. Furthermore, our dynamic token sampling strategy does not delineate clear boundaries between each frame. To address this, we incorporate frame-level positional embeddings (FPE) that correspond to the absolute timestep of each frame, utilizing a shared sinusoidal position encoding~\\citep{vaswani2017attention} for frames at time $t$, shown in Equation~\\ref{eq:pos}.\n\n\\begin{equation}\\label{eq:pos}\n    PE(t,2i) = sin(t/10000^{2i/d}), \n    PE(t,2i+1) = cos(t/10000^{2i/d}) \n\\end{equation}\n\nThe ablation shows in Table~\\ref{tab:fpe} and Table~\\ref{tab:mlvufpe} that adding the FPE does not affect much to the overall performance across several benchmarks. Therefore, we decide not to include it in our default setting.\n\n\\begin{table}[!htbp]\n    \\centering\n\\begin{adjustbox}{width=0.9\\linewidth,center}\n\\renewcommand{\\arraystretch}{1.2}\n\\setlength{\\tabcolsep}{1.5mm}\n\\begin{tabular}{lccccc}\n\\toprule \\textbf{Methods} & \\textbf{Context Length} & \\textbf{\\#Tokens} & \\multicolumn{1}{c}{ \\textbf{EgoSchema} } & \\multicolumn{1}{c}{ \\textbf{VideoMME} } & \\multicolumn{1}{c}{\\textbf{MLVU}} \\\\\n\\midrule\nDINO + Query & 8k & 64/144 & 67.30 & 60.08 & 65.05 \\\\\n\\rowcolor{blue!10} DINO + Query + STC (default) & 8k & dynamic & 67.62 & 60.56 & 65.44 \\\\\nDINO + Query + STC + FPE & 8k & dynamic & 67.87 & 60.89 & 64.56 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{adjustbox}\n\\caption{Ablation study on with or without FPE.}\n\\label{tab:fpe}\n\\end{table}\n\n\\begin{table}[!htbp]\n    \\centering\n\\begin{adjustbox}{width=0.95\\linewidth,center}\n\\begin{tabular}{lccccccccc}\n\\toprule \\textbf{Stratgy} & \\multicolumn{1}{c}{ \\textbf{count} } & \\multicolumn{1}{c}{ \\textbf{ego} } &  \\multicolumn{1}{c}{\\textbf{needle}} & \\multicolumn{1}{c}{ \\textbf{order} } & \\multicolumn{1}{c}{\\textbf{plotQA}} & \\multicolumn{1}{c}{\\textbf{anomaly}} & \\multicolumn{1}{c}{\\textbf{reasoning}} & \\multicolumn{1}{c}{\\textbf{Avg}} \\\\\n\\midrule\nDINO & 24.15 & 59.09 & 68.16 & 52.89 & 71.24 & 74.0 & 86.36 & 62.54 \\\\\nDINO+Query & 28.98 & 55.39 & 78.87 & 56.37 & 72.35\t & 75.5 & 87.87 & 65.05 \\\\\n\\rowcolor{blue!10} DINO+Query+STC (default) & 28.98 & 59.37 & 76.33 & 58.30\t& 71.61 & 76.0 & 87.50 & 65.44 \\\\\nDINO+Query+STC+ FPE & 29.46 & 60.79 & 74.08 & 52.12\t& 71.79\t& 74.5\t& 86.74 & 64.56 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{adjustbox}\n\\caption{Strategy ablations on each subtask in MLVU~\\citep{zhou2024mlvu}.}\n\\label{tab:mlvufpe}\n\\end{table}\n\n\\section{DINOv2 v.s. SigLIP}\n\nDINOv2~\\citep{oquab2023dinov2}, through self-supervised training with a feature similarity objective on visually-centric tasks, captures subtle frame differences and low-level visual features more effectively than vision-language contrastive methods~\\citep{radford2021learning,zhai2023sigmoid}, as shown in Figure~\\ref{fig:dinosim}.\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=\\linewidth]{assets/dinosim.pdf}\n    \\caption{Similarity comparison between SigLIP~\\citep{zhai2023sigmoid} and DINOv2~\\citep{oquab2023dinov2} features. The similarity is calculated between the first frame and the remainings. DINO concentrating on vision centric task effectively capture subtle frame differences compared with SigLIP~\\citep{zhai2023sigmoid} which is aligned on semantic space.}\n    \\label{fig:dinosim}\n\\end{figure}\n\n\\section{Needle-In-A-Video-Haystack}\n\nWe conducted experiments using an 8k context length to evaluate our default setting, which incorporates our adaptive compression, against configurations without spatial token compression (w/o STC) and without querying guided reduction  (w/o Query), as depicted in Figure~\\ref{fig:needle1}. By integrating a cross-modal query to selectively retain full tokens of frames relevant to the text query, the model significantly enhances its ability to accurately identify key frames when the total number of video frames is fewer than 1.4k. Moreover, our adaptive token compression mechanism further boosts VQA accuracy with increased frames.\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=\\linewidth]{assets/needle1.pdf}\n    \\caption{Needle-In-A-Video-Haystack results. Our spatiotemporal adaptive token compression scheme improves the score for locating the needle frame.}\n    \\label{fig:needle1}\n\\end{figure}\n\n\\begin{table}[!ht]\n    \\centering\n    \\renewcommand{\\arraystretch}{1.3}\n    \\begin{tabular}{cccccc}\n    \\toprule\n        \\textbf{Model} & \\textbf{SQA-IMG} & \\textbf{MMVP} & \\textbf{POPE} &  \\textbf{RealWorldQA} \\\\\n        \\midrule\n        Before video SFT & 95.44 & 51.33 & 86.65 & 61.06 \\\\\n        After video SFT & 83.94 & 32.00 & 81.23 & 47.65 \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\caption{We mainly focus on video understanding task and use video-only data for video SFT stage. We observe a decrease in performance on image understanding after video SFT stage.}\n    \\label{tab:image}\n\\end{table}\n\n\\section{Limitation}\n\nOur research is primarily concentrated on video understanding tasks, for which we employ video-only data during the video supervised fine-tuning (SFT) stage. As evidenced in Table~\\ref{tab:image}, there is a decrease observed in the model's image understanding capabilities after video SFT. A potential remedy could involve integrating a mix of image, multi-image, and video data during training. However, due to constraints in GPU resources, we leave it as a future work with larger datasets for stronger unified image and video models.\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2311.17043v1.tex",
        "arXiv-2406.16852v2.tex",
        "arXiv-2410.17434v1.tex"
    ],
    "group_id": "group_90",
    "response": "### Title: Advances in Efficient Token Generation and Compression for Vision-Language Models in Video Understanding\n\n### Introduction\n\nThe field of Vision-Language Models (VLMs) has seen significant advancements in recent years, driven by the capabilities of Large Language Models (LLMs) like GPT-3, LLaMA, and Vicuna. These models have been extended to include visual data, enabling them to perform tasks such as image captioning and visual question answering (VQA). However, the challenge of processing long videos remains unresolved due to the computational burden associated with the large number of visual tokens required. Each video frame necessitates a substantial number of tokens, leading to an excessive token count for hour-long videos that exceeds the capacity of current VLMs. This issue has prompted the development of several strategies, including the use of representative queries and temporal compression, but these methods often compromise the model's performance over long-term intervals. The papers discussed here propose innovative solutions to address this challenge, focusing on efficient token generation and compression strategies for VLMs.\n\n### Paper 1: LLaMA-VID\n\nLLaMA-VID introduces a dual-token paradigm to represent video frames, aiming to reduce the computational burden associated with long videos while preserving critical information. The method employs a pre-trained vision transformer to extract image features, which are then processed into context tokens and content tokens. The context token is designed to encode the overall context of the image based on user input, condensing the broader picture into a single token. Conversely, the content token captures finer aspects of each frame. The context token is generated through a cross-modality design, where the text query interacts with the visual features in a context attention module. The content token is produced by average pooling the image features, with the option to extend the number of tokens based on computational constraints. For instance, a video frame can be represented with only 2 tokens, whereas a single image can be represented with up to 256 tokens.\n\nTo better support hour-long videos, LLaMA-VID constructs an instruction-based dataset containing 9K movie-level conversations for plot reasoning and detail understanding. The model is trained in three stages: modality alignment, instruction tuning, and long video tuning. During modality alignment, the model is optimized to ensure visual features are well-aligned with the language space. In instruction tuning, the model is fine-tuned on a dataset that includes text, image, and video data. Finally, in long video tuning, the model is trained on a dataset that includes 15K long QA pairs to enhance its understanding of extensive video sequences.\n\nLLaMA-VID demonstrates superior performance on several video and image-based benchmarks, such as MSVD-QA, MSRVTT-QA, ActivityNet-QA, and Video-MME. The model can process hour-long videos with only two tokens per frame, significantly reducing the computational load while maintaining high accuracy. The dual-token strategy allows existing frameworks to support hour-long videos and pushes their upper limit with an extra context token. LLaMA-VID is trained using 8 NVIDIA A100 GPUs and can be instantiated with various decoders and LLMs.\n\n### Paper 2: Long Video Assistant (LongVA)\n\nLongVA addresses the challenge of understanding long videos by extending the context length of the language model backbone rather than reducing the number of visual tokens. The paper proposes the concept of \"long context transfer,\" where the extended context length of the language model is transferred to the multimodal model without any additional video training. LongVA uses a unified encoding scheme called UniRes, which represents videos as extended images, enhancing the fusion between image and video capabilities.\n\nThe training process of LongVA involves two stages: long language model training and vision-language alignment. In the first stage, the language model is trained on long text data to extend its context length. In the second stage, the model is fine-tuned on short image data to align the extended language model with vision. This approach allows LongVA to process extremely long videos without additional complexities, achieving state-of-the-art performance on Video-MME and MLVU among 7B-scale models.\n\nLongVA is trained on 64 NVIDIA H100 GPUs and achieves significant performance improvements on various benchmarks. The model's ability to handle long contexts is validated through the V-NIAH benchmark, a synthetic visual benchmark inspired by the language model's NIAH test. LongVA can process up to 2000 frames or over 200K visual tokens without any additional training on long video data.\n\n### Paper 3: LongVU\n\nLongVU proposes a spatiotemporal adaptive compression scheme to efficiently process long videos within a commonly used context length. The method leverages both DINOv2 and SigLIP vision encoders to extract features from video frames, with DINOv2 being particularly effective in capturing subtle frame differences and low-level visual features. After extracting features, LongVU implements a temporal reduction strategy to remove redundant frames based on their similarity. It then selectively reduces spatial tokens via cross-modal queries, preserving essential visual features while accommodating more long-range context within the context length. Finally, spatial token compression (STC) is applied to further reduce the number of tokens, based on inter-frame similarities.\n\nLongVU is trained on 64 NVIDIA H100 GPUs and achieves state-of-the-art results on multiple video understanding benchmarks, including VideoMME and MLVU. The model's performance is validated through ablation studies, demonstrating the effectiveness of each component in the compression scheme. LongVU also shows improved performance with a lightweight LLM, Llama3.2-3B, indicating its potential for various applications in the vision-language community.\n\n### Commonalities and Innovations\n\nAll three papers address the challenge of processing long videos in VLMs by reducing the number of visual tokens required per frame. However, they employ different strategies to achieve this goal. LLaMA-VID uses a dual-token paradigm to represent each frame with a context token and a content token, significantly reducing the token count while preserving critical information. LongVA focuses on extending the context length of the language model backbone and uses a unified encoding scheme to represent videos as extended images. LongVU introduces a spatiotemporal adaptive compression scheme that leverages both temporal and spatial token reduction to accommodate long videos within a commonly used context length.\n\nThe innovations in these papers lie in their unique approaches to token generation and compression. LLaMA-VID's dual-token strategy enables efficient representation of long videos and single images. LongVA's long context transfer phenomenon allows the model to process long videos without additional video training, leveraging the extended context length of the language model. LongVU's spatiotemporal adaptive compression scheme effectively reduces video tokens while maintaining high performance on various video understanding benchmarks.\n\n### Comparison of Results\n\nLLaMA-VID, LongVA, and LongVU all demonstrate significant improvements over existing models on various video understanding benchmarks. LLaMA-VID achieves the best performance on most video-based benchmarks, such as MSVD-QA, MSRVTT-QA, and ActivityNet-QA, with an average accuracy of 69.7\\% and 70.0\\% using Vicuna-7B and Vicuna-13B, respectively. LongVA achieves state-of-the-art results among 7B models on Video-MME and MLVU, with an overall accuracy of 60.9\\% and 65.44\\%, respectively. LongVU surpasses several recent open-source video LLM models, such as Video-LLaVA and LLaVA-OneVision, on VideoMME and MLVU benchmarks, achieving an overall accuracy of 60.9\\% and 65.44\\%, respectively.\n\nThe performance gains of these models can be attributed to their unique token generation and compression strategies. LLaMA-VID's dual-token paradigm allows it to efficiently encode each frame with only two tokens, thereby supporting long video understanding. LongVA's long context transfer phenomenon enables it to process long videos without additional video training, leveraging the extended context length of the language model. LongVU's spatiotemporal adaptive compression scheme effectively reduces video tokens while maintaining high performance on various video understanding benchmarks.\n\n### Conclusion\n\nThe three papers discussed here introduce novel methods to address the challenge of processing long videos in VLMs. LLaMA-VID employs a dual-token paradigm to efficiently represent each frame, LongVA leverages the extended context length of the language model backbone, and LongVU uses a spatiotemporal adaptive compression scheme to reduce video tokens. These methods significantly improve the performance of VLMs on various video understanding benchmarks, demonstrating the potential of efficient token generation and compression strategies in the field of vision-language models.\n\nFuture research directions could include further optimization of token generation and compression strategies, as well as the integration of larger datasets to enhance the model's understanding of diverse visual content. Additionally, exploring the use of these methods in real-world applications, such as video summarization and visual question answering, could provide valuable insights into their practical utility. The development of more robust benchmarks to measure the performance of VLMs on long video understanding tasks is also crucial for advancing the field."
}