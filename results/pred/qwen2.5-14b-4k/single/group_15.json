{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{MM-DFN: Multimodal Dynamic Fusion Network For Emotion Recognition in Conversations}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\n\n  Emotion Recognition in Conversations (ERC) has considerable prospects for developing empathetic machines. For multimodal ERC, it is vital to understand context and fuse modality information in conversations. Recent graph-based fusion methods generally aggregate multimodal information by exploring unimodal and cross-modal interactions in a graph. However, they accumulate redundant information at each layer, limiting the context understanding between modalities. In this paper, we propose a novel Multimodal Dynamic Fusion Network (MM-DFN) to recognize emotions by fully understanding multimodal conversational context. Specifically, we design a new graph-based dynamic fusion module to fuse multimodal context features in a conversation.  The module reduces redundancy and enhances complementarity between modalities by capturing the dynamics of contextual information in different semantic spaces. Extensive experiments on two public benchmark datasets demonstrate the effectiveness and superiority of the proposed model. \n\\end{abstract}\n\n\\begin{keywords}\nemotion recognition, emotion recognition in conversations, multimodal fusion, dialogue systems\n\\end{keywords}\n\n\\section{Introduction}\n\\label{sec:intro}\nEmotion Recognition in Conversations (ERC) aims to detect emotions in each utterance of the conversation.  %from their textual, visual, and acoustic emotional expressions , expressed by the speakers\nIt has considerable prospects for developing empathetic machines \\cite{DBLP:journals/inffus/MaNXC20}.\nThis paper studies ERC under a multimodal setting,\n\\textit{i.e.}, acoustic, visual, and textual modalities. % utilizing information from \n\nA conversation often contains rich contextual clues \\cite{DBLP:conf/acl/PoriaHMNCM19,DBLP:conf/acl/HuWH20}, which are essential for identifying emotions.\nThe key success factors of multimodal ERC are accurate context understanding and multimodal fusion.\nPrevious context-dependent works \\cite{DBLP:conf/aaai/MajumderPHMGC19,DBLP:conf/emnlp/GhosalMPCG19,DBLP:conf/acl/HuWH20} model conversations as sequence or graph structures to explore contextual clues within a single modality. % mainly，typical，classical\nAlthough these methods can be naturally extended multimodal paradigms by performing early/late fusion such as \\cite{DBLP:conf/acl/PoriaCHMZM17, DBLP:conf/emnlp/HazarikaPMCZ18, DBLP:conf/icmcs/FuOWGSLD21}, it is difficult to capture contextual interactions between modalities, which limits the utilization of multiple modalities.\nBesides, some carefully-designed hybrid fusion methods \\cite{DBLP:conf/emnlp/ZadehCPCM17,DBLP:conf/acl/MorencyLZLSL18,DBLP:conf/aaai/ZadehLMPCM18,DBLP:conf/mm/ChenSOLS21} focus on the alignment and interaction between modalities in isolated or sequential utterances. \nThese methods  \nignore complex interactions between utterances, \nresulting in leveraging context information in conversations insufficiently.\n\nRecent remarkable works \\cite{DBLP:conf/acl/HuLZJ20,DBLP:conf/icassp/LiuCWLFGD21} model unimodal and cross-modal interactions in a graph structure, which provides complementarity between modalities for tracking emotions. % remarkable, advanced, noteworthy\nHowever, these graph-based fusion methods aggregate contextual information in a specific semantic space at each layer, gradually accumulating redundant information. \nIt limits context understanding between modalities. \nThe contextual information continuously aggregated can be regarded as specific views where each view can have its individual representation space and dynamics.\nWe believe that modeling these dynamics of contextual information in different semantic spaces can reduce redundancy and enhance complementarity, accordingly boosting context understanding between modalities.\n\nIn this paper, we propose a novel Multimodal Dynamic Fusion Network (MM-DFN) to recognize utterance-level emotion by sufficiently understanding multimodal conversational context.\nFirstly, we utilize a modality encoder to \ntrack speaker states and context in each modality. \nSecondly, \ninspired by \\cite{DBLP:journals/neco/HochreiterS97,DBLP:conf/icml/ChenWHDL20},\nwe improve the graph convolutional layer  \\cite{DBLP:conf/iclr/KipfW17} with gating mechanisms and design a new Graph-based Dynamic Fusion (GDF) module to fuse multimodal context information. \nThe module utilizes graph convolution operation to aggregation context information of both inter- and intra-modality in a specific semantic space at each layer.\nMeanwhile, the gating mechanism is used to learn the intrinsic sequential patterns of contextual information in adjacent semantic space. The GDF module can control information flow between layers, reducing redundancy and promoting the complementarity between modalities.\nThe stack of GDFs can naturally fuse multimodal context features by embedding them into a dynamic semantic space. % GDF modules\nFinally, \nan emotion classifier is used to predict the emotion label of the utterance.\n \n\n\\begin{figure*}[t]\n  \\centering\n  \\includegraphics[width=0.93\\linewidth]{figures/figure-emotion.pdf}\n  \\caption{The architecture of the proposed MM-DFN. Given input multimodal features, modality encoder first captures features of context and speaker in each modality.\n  Then, in each conversation, we construct the fully connected graph in each modality, and connect nodes corresponding to the same utterance between different modalities.\n  Based on the graph, graph-based dynamic fusion modules are stacked to fuse multimodal context features, dynamically and sequentially.  \n    Finally, based on the concatenation of features, an emotion classifier is applied to identify emotion label of each utterance.\n }\n  \\label{fig:overall}\n\\end{figure*}\n \n\nWe conduct a series of experiments on two public benchmark datasets, {\\it i.e.,} {\\it IEMOCAP} and {\\it MELD}. Results consistently demonstrate that MM-DFN significantly outperforms comparison methods.  % a series of, state-of-the-art \nThe main contributions are summarized as follows:\n1) We propose a novel MM-DFN to facilitate multimodal context understanding for ERC. \n2) We design a new graph-based dynamic fusion module to fuse multimodal conversational context.\nThis module can reduce redundancy and enhance complementarity between modalities.\n3) Extensive experiments on two benchmark datasets demonstrate the effectiveness and superiority of the proposed model\\footnote{The code is available at {https://github.com/zerohd4869/MM-DFN}}.\n\n\\section{Methodology}\n\nFormally, given a conversation $U=[u_1, ..., u_N]$, $u_i = \\{ \\mathbf{u}^a_i,  \\mathbf{u}^v_i, \\mathbf{u}^t_i \\}$, where $N$ is the number of utterances. \n$\\mathbf{u}^a_i, \\mathbf{u}^v_i, \\mathbf{u}^t_i$ denote the raw feature representation of $u_i$ from the acoustic, visual, and textual modality, respectively.\nThere are $M$ speakers $ P =\\{p_1, ..., p_M\\}(M \\geq 2)$.\nEach utterance $u_i$ is spoken by the speaker $p_{\\phi(u_i)}$, where $\\phi$ maps the index of the utterance into the corresponding speaker.\nMoreover, we define $U_{{\\lambda}}$ to represent the set of utterances spoken by the party $p_{\\lambda}$.\n  $U_{{\\lambda}} = \\{ u_i | u_i \\in U \\text{ and }  u_i \\text{ spoken by }   p_{\\lambda}, \\text{ } \\forall i \\in [1, N]   \\}, \\lambda \\in [1,M]$.\nThe goal of multimodal ERC is to predict the emotion label $y_i$ for each utterance $u_i$ from pre-defined emotions $\\mathcal{Y}$.\n\nIn this section, we propose a novel Multimodal Dynamic Fusion Network (MM-DFN) to fully understand the multimodal conversational context for ERC,\nas shown in Fig.~\\ref{fig:overall}.\n\n\\subsection{Modality Encoder}\nTo capture context features for the textual modality, we apply a bi-directional gated recurrent unit (BiGRU); for the acoustic and visual modalities, we apply a  fully connected network. The context embedding can be computed as: \n\\begin{equation}\n \\begin{split}\n  & \\mathbf{c}^{\\varsigma}_i = \\mathbf{W}^{\\varsigma}_c \\mathbf{u}_{i}^{\\varsigma} + \\mathbf{b}^{\\varsigma}_c, \\varsigma \\in \\{ a, v\\}, \\\\\n  & \\mathbf{c}^{t}_i, \\mathbf{h}^{c}_{i}  = {\\overleftrightarrow{GRU}_c}(\\mathbf{u}^t_{i}, \\mathbf{h}^{c}_{i-1}), \\\\\n\\end{split}\n\\end{equation} \nwhere $\\overleftrightarrow{GRU}_c$ is a BiGRU to obtain context embeddings and $\\mathbf{h}^c_{i}$ is the hidden vector.  \n$\\mathbf{W}^a_c, \\mathbf{W}^v_c, \\mathbf{b}^a_c, \\mathbf{b}^v_c$ are trainable parameters.\nConsidering the impact of speakers in a conversation, we also employ a shared-parameter BiGRU to encode different contextual information from multiple speakers:\n\\begin{equation}\n\\resizebox{0.88\\linewidth}{!}{$\n\\mathbf{s}^{\\delta}_i, \\mathbf{h}^{s}_{\\lambda, j}  = {\\overleftrightarrow{GRU}_s}(\\mathbf{u}^{\\delta}_{i}, \\mathbf{h}^{s}_{\\lambda,j-1}), j \\in [1,|U_{\\lambda}|], \\delta \\in \\{ a, v, t \\},\n$}\n\\end{equation}  \nwhere $\\overleftrightarrow{GRU}_s$ indicates a BiGRU to obtain speaker embeddings. \n$\\mathbf{h}^s_{\\lambda, j}$ is the $j$-th hidden state of the party $p_\\lambda$. $\\lambda = \\phi(u_i)$. \n$U_\\lambda$ refers to all utterances of $p_\\lambda$ in a conversation.  %the party \n\n \n\n\\subsection{Graph-based Dynamic Fusion Modules}\n\\subsubsection{Graph Construction}\nFollowing \\cite{DBLP:conf/acl/HuLZJ20}, we build an undirected graph to represent a conversation, denoted as $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$.\n$\\mathcal{V}$ refers to a set of nodes. Each utterance can be represented by three nodes for differentiating acoustic, visual, and textual modalities. Given $N$ utterances, there are $3N$ nodes in the graph.\nWe add both context embedding and speaker embedding to initialize the embedding of nodes in the graph:\n\\begin{equation}\n  \\mathbf{x}^{\\delta}_i = \\mathbf{c}^{\\delta}_i + \\gamma^{\\delta} \\mathbf{s}^{\\delta}_i, \\delta \\in \\{ a, v, t\\},\n\\end{equation} \nwhere $\\gamma^{a}, \\gamma^{v}, \\gamma^{t}$ are trade-off hyper-parameters.\n$\\mathcal{E}$ refers to a set of edges, which are built based on two rules. \nThe first rule is that any two nodes of the same modality in the same conversation are connected.\nThe second rule is that each node is connected with nodes corresponding to the same utterance but from different modalities.\nFollowing \\cite{DBLP:conf/textgraphs/SkianisMV18}, edge weights are computed as:\n$\\mathbf{A}_{ij} = 1 - \\frac{\\text{arccos}(\\text{sim}(\\mathbf{x}_i,\\mathbf{x}_j))}{\\pi},$\nwhere $\\text{sim}(\\cdot)$ is cosine similarity function.\n\n\\subsubsection{Dynamic Fusion Module}\nBased on the graph, we improve \n\\cite{DBLP:conf/iclr/KipfW17}\nwith gating mechanisms to fuse multimodal context features in the conversation. \nWe utilize graph convolution operation to aggregate context information of both inter- and intra-modality in a specific semantic space at each layer.\nMeanwhile, inspired by \\cite{DBLP:journals/neco/HochreiterS97}, we leverage gating mechanisms to learn  intrinsic  sequential patterns of contextual  information in different semantic spaces.\nThe updating process using gating mechanisms is defined  as:\n\\begin{equation}\n  \\resizebox{0.8\\linewidth}{!}{$\n  \\begin{split}\n    \\mathbf{\\Gamma}^{(k)}_{\\varepsilon} &= \\sigma(\\mathbf{W}^g_{{\\varepsilon}} \\cdot [\\mathbf{g}^{(k-1)}, {\\mathbf{H}'}^{(k-1)}] + \\mathbf{b}^g_{\\varepsilon} ),  {\\varepsilon}=\\{ u, f, o \\}, \\\\  \n    \\tilde{\\mathbf{C}}^{(k)} &= \\text{tanh}(\\mathbf{W}^g_C \\cdot [\\mathbf{g}^{(k-1)}, {\\mathbf{H}'}^{(k-1)}] + \\mathbf{b}^g_C ), \\\\\n    \\mathbf{C}^{(k)} &=  \\mathbf{\\Gamma}^{(k)}_f \\odot \\mathbf{C}^{(k-1)} + \\mathbf{\\Gamma}^{(k)}_u  \\odot  \\tilde{\\mathbf{C}}^{(k)}, \\ \\mathbf{g}^{(k)} = \\mathbf{\\Gamma}^{(k)}_o \\odot \\text{tanh}( \\mathbf{C}^{(k)}  ),\n  \\end{split}\n  $}\n\\end{equation}\nwhere $\\mathbf{\\Gamma}^{(k)}_u, \\mathbf{\\Gamma}^{(k)}_f, \\mathbf{\\Gamma}^{(k)}_o$ refer to the update gate, the forget gate, and the output gate in the $k$-th layer, respectively. \n$\\mathbf{g}^{(0)}$ is initialized with zero. \n$\\mathbf{W}^g_{\\Gamma}, \\mathbf{b}^g_{\\Gamma}$ are learnable parameters. \n$\\sigma(\\cdot)$ is a sigmoid function.\n$\\tilde{\\mathbf{C}}^{(k)}$ stores contextual information of previous layers.\nThe update gate $\\mathbf{\\Gamma}^{(k)}_u$ controls what part of the contextual information is written to the memory, while the forget gate $\\mathbf{\\Gamma}^{(k)}_f$  decides what redundant information in ${\\mathbf{C}}^{(k)}$ is deleted.\nThe output gate $\\mathbf{\\Gamma}^{(k)}_o$ reads selectively for passing into a graph convolution operation. \n Following \\cite{DBLP:conf/icml/ChenWHDL20}, the modified convolution operation can be defined as: \n\\begin{equation}  \n\\resizebox{0.99\\linewidth}{!}{$\n\\begin{split}\n    \\mathbf{H}^{(k)} = \\text{ReLU} \\left(\n    (  (1-\\alpha) \\tilde{\\mathbf{P}} {\\mathbf{H}'}^{(k-1)} + \\alpha \\mathbf{H}^{(0)} ) \n    (  (1-\\beta_{k-1}) \\mathbf{I}_n + \\beta_{k-1} \\mathbf{W}^{(k-1)} ) \n     \\right), \\\\ \n\\end{split}\n$}\n\\end{equation}\nwhere $\\tilde{\\mathbf{P}} = \\tilde{\\mathbf{D}}^{-1/2} \\tilde{\\mathbf{A}} \\tilde{\\mathbf{D}}^{-1/2}$ is the graph convolution matrix with the renormalization trick. \n$\\alpha, \\beta_{k}$ are two hyperparameters. $\\beta_{k}=log(\\frac{\\rho}{k} + 1)$. $\\rho$ is also a hyperparameter.  $\\mathbf{W}^{(k)}$ is the weight matrix.\n${\\mathbf{H}}^{(0)}$ is initialized with $ \\mathbf{X}^a, \\mathbf{X}^v, \\mathbf{X}^t$.\n$\\mathbf{I}_n$ is an identity mapping matrix.\nThen, the output of $k$-th layer can be computed as,\n  ${\\mathbf{H}'}^{(k)} = {\\mathbf{H}}^{(k)} + \\mathbf{g}^{(k)}$.\n\n\\subsection{Emotion Classifier} \\label{sec:emo}\nAfter the stack of $K$ layers, representations of three modalities for each utterance $i$ can be refined as $\\mathbf{o}^a_i, \\mathbf{o}^v_i, \\mathbf{o}^t_i$. \nFinally, a classifier is used to predict the emotion of each utterance:\n\\begin{equation}\n    \\hat{\\mathbf{y}}_i = \\text{Softmax}(\\mathbf{W}_z  [\\mathbf{x}^a_i;\\mathbf{x}^v_i;\\mathbf{x}^t_i; \\mathbf{o}^a_i;\\mathbf{o}^v_i;\\mathbf{o}^t_i] + \\mathbf{b}_z),    \n\\end{equation} \nwhere $\\mathbf{W}_z$ and $\\mathbf{b}_z$ are trainable parameters.\nWe apply cross-entropy loss  along with L2-regularization to train the model:\n\\begin{equation}\n\\resizebox{0.8\\linewidth}{!}{$\n    \\mathcal{L} = - \\frac{1}{\\sum_{l=1}^L \\tau(l)} \\sum_{i=1}^{L} \\sum^{\\tau(i)}_{j=1} {\\mathbf{y}}^l_{i,j} log (\\hat{\\mathbf{y}}^l_{i,j}) + \\eta \\| \\Theta \\|_2, \n$}\n\\end{equation}    \nwhere $L$ is the total number of samples in the training set. $\\tau(i)$ is the number of utterances in sample $i$. $\\mathbf{y}^l_{i,j}$ and $\\hat{\\mathbf{y}}^l_{i,j}$ denote the one-hot vector and probability vector for emotion class $j$ of utterance $i$ of sample $l$, respectively.\n$\\Theta$ refers to  all trainable parameters. $\\eta$ is the L2-regularization weight.\n\n\\section{Experiments}\n\n\\begin{table*}[t]\n  \\centering\n      \\resizebox{1.0\\linewidth}{!}{$\n  \\begin{tabular}{l|cccccc|cc||ccccc|cc}\n  \\hline\n  \\multicolumn{1}{c|}{\\multirow{2}{*}{\\textbf{Methods}}} & \\multicolumn{8}{c||}{\\textbf{IEMOCAP}} & \\multicolumn{7}{c}{\\textbf{MELD}} \n  \\\\ \n  \\cline{2-16}\n   & \\textit{Happy} &\\textit{Sad} & \\textit{Neutral} & \\textit{Angry} & \\textit{Excited} & \\textit{Frustrated} & {{Acc}} & {{w-F1}} \n   & \\textit{Neutral} & \\textit{Surprise} & \\textit{Sadness}  \n   &  \\textit{Happy}\n   &\\textit{Anger}  & {Acc} & {{w-F1}}    \\\\\n  \\hline\n    TFN \\cite{DBLP:conf/emnlp/ZadehCPCM17}\n    & 37.26 & 65.21 & 51.03 & 54.64 & 58.75 & 56.98 & 55.02 & 55.13\n    & 77.43 & 47.89 & 18.06 & 51.28 & 44.15 & 60.77 & 57.74 \\\\ \n    LMF \\cite{DBLP:conf/acl/MorencyLZLSL18}\n    & 37.76 & 66.53 & 52.39 & 57.53 & 58.41 & 59.27 & 56.50 & 56.49\n    & 76.97 & 47.06 & 21.15 & 54.20 & 46.64 & 61.15 & 58.30 \\\\\n    MFN \\cite{DBLP:conf/aaai/ZadehLMPCM18}    \n    &  48.19\t&   73.41\t&   56.28 &     63.04 &     64.11 &\t    61.82 &\t    61.24\t&   61.60\t\n    &  77.27 & 48.29 &  23.24 & 52.63 & 41.32 &   60.80\t&   57.80   \\\\\n    bc-LSTM \\cite{DBLP:conf/acl/PoriaCHMZM17} %$^\\dagger$     \n    &  33.82   & \t78.76   & \t56.75 & \t64.35 & \t60.25 & \t60.75 & \t60.51   & \t60.42   \n    &  75.66 & 48.57 & 22.06 & 52.10  &  44.39 &  59.62   & \t57.29   \\\\ \n    ICON \\cite{DBLP:conf/emnlp/HazarikaPMCZ18}  \n    & 32.80    & \t74.40   & \t60.60 & \t68.20 & \t68.40 & \t66.20 & \t64.00   & \t63.50   \n    & -  & -  & -  & - & -  & - & -    \\\\ \n    DialogueRNN \\cite{DBLP:conf/aaai/MajumderPHMGC19} %$^\\dagger$   \n    & 32.20    & \t80.26   & \t57.89 & \t62.82 & \t73.87 & \t59.76 & \t63.52   & \t62.89   \n    & 76.97 & 47.69 & 20.41 & 50.92 & 45.52 & 60.31 & 57.66 \\\\\n    DialogueCRN \\cite{DBLP:conf/acl/HuWH20} %$^\\dagger$   \n    & \\textbf{53.23} & \\textbf{83.37} & 62.96 & 66.09 & 75.40 & 66.07 & 67.16 & 67.21\n    & 77.01 & 50.10 & 26.63 & 52.77 & 45.15 & 61.11 & 58.67  \\\\\n    DialogueGCN \\cite{DBLP:conf/emnlp/GhosalMPCG19} %$^\\dagger$ \t \n    & 51.57\t&   80.48\t&   57.69 & \t53.95 &\t    72.81 &\t    57.33 &\t    63.22\t&   62.89\t\n    & 75.97 & 46.05 & 19.60 & 51.20 & 40.83 & \t58.62   & \t56.36   \\\\\n    MMGCN \\cite{DBLP:conf/acl/HuLZJ20}\n    & 45.14    & \t77.16   & \t64.36 & \t68.82 & \t74.71 & \t61.40 & \t66.36   & \t66.26   \n    & 76.33 & 48.15 & \\textbf{26.74} & 53.02 & 46.09 & 60.42 & 58.31 \\\\\n    \\hline \n    \\textbf{MM-DFN}   \n    & 42.22    & \t78.98   & \t\\textbf{66.42}$^{*}$ & \t\\textbf{69.77}$^{*}$ & \t\\textbf{75.56}$^{*}$ & \t\\textbf{66.33}$^{*}$ & \t\\textbf{68.21}$^{*}$   & \t\\textbf{68.18}$^{*}$   \n    & \\textbf{77.76}$^{*}$   &   \\textbf{50.69}$^{*}$  &  {22.93}  &  \\textbf{54.78}$^{*}$  &  \\textbf{47.82}$^{*}$ & \t\\textbf{62.49}$^{*}$   & \t\\textbf{59.46}$^{*}$   \\\\\n  \\hline\n  \\end{tabular}    \n  $}\n  \\caption{\n  Results under the multimodal setting (A+V+T).\n  We present the overall performance of Acc and w-F1, which mean the overall accuracy score and weighted-average F1 score, respectively.\n  We also report F1 score per class, except two classes (i.e. \\textit{Fear} and \\textit{Disgust}) on MELD, whose results are not statistically significant due to the smaller number of training samples.\n  Best results are highlighted in bold.  $^{*}$ represents statistical significance over state-of-the-art scores under the paired-$t$ test ($p < 0.05$).\n} \\label{tab:result}\n\\end{table*}\n\n\\begin{table}[t]\n  \\centering\n      \\resizebox{1.0\\linewidth}{!}{$\n    \\begin{tabular}{p{6cm}|p{1.9cm}<{\\centering}|p{1.9cm}<{\\centering}}\n      \\hline\n      \\multicolumn{1}{c|}{\\multirow{1}{*}{\\textbf{Methods}}} \n      & \\multicolumn{1}{c|}{\\multirow{1}{*}{\\textbf{IEMOCAP}}}  \n      & \\multicolumn{1}{c}{\\multirow{1}{*}{\\textbf{MELD}} } \\\\ \n      \\hline\n      \\text{MM-DFN}                                     & \t{68.18}       & \t{59.46}                 \\\\ \n      \\  { } - w/o GDF \\ - w Speaker  \\ \\ \\ \\ - w Context          & \t63.80         &\t58.50           \\\\\n      \\  { } - w GDF \\ \\ \\ \\  - w/o Speaker \\ - w Context          & \t66.89         & \t58.45       \\\\\n      \\  { } - w/o GDF \\ - w/o Speaker  \\ - w Context        & \t62.90         &\t58.50                   \\\\\n      \\  { } - w/o GDF \\ - w/o Speaker \\ - w/o Context      & \t54.81         &\t58.08               \\\\      \n      \\hline\n    \\end{tabular}      \n  $}\n  \\caption{\n  Ablation results of MM-DFN. We report w-F1 score for both datasets.\n  }\n\\label{tab:abla}\n\\end{table}\n\n\\begin{table}[t]\n  \\centering\n      \\resizebox{1.0\\linewidth}{!}{$\n  \\begin{tabular}{p{6cm}|p{1.9cm}<{\\centering}|p{1.9cm}<{\\centering}} %p{6cm}\n  \\hline\n  \\multicolumn{1}{c|}{\\multirow{1}{*}{\\textbf{Fusion Modules}}} & \\multicolumn{1}{c|}{\\textbf{IEMOCAP}}  & \\multicolumn{1}{c}{\\textbf{MELD}} \n  \\\\ \n  \\hline\n  {Concat / Gate Fusion}        & \t63.80 / 64.30   & \t58.50 / 57.87   \\\\ \n  {Tensor / Memory Fusion}\t    & \t61.05 / 65.51   & \t58.54 / 58.48   \\\\ \n{Early / Late Fusion + GCN}    & 64.19 / 65.34  & 58.69 / 58.43 \\\\\n  \\hline\n  {{Graph-based Fusion (GF)}}        & 67.02     & 58.54     \\\\ \n    {{ }\\  \\  - w/o {{Inter-Modal} \\   - w {Intra-Modal}}} \t&\t66.91 &    58.53 \\\\\n    {{ }\\  \\  - w {{Inter-Modal}}  \\ \\ \\ \\ - w/o  {{Intra-Modal}}}  &\t66.11 \t&\t58.29  \\\\  \n  \\hline\n  {\\textbf{Graph-based Dynamic Fusion (GDF)}}  & 68.18 &  \t59.46 \\\\  \n    {{ } \\ \\  - w/o {{Inter-Modal} \\  - w {Intra-Modal}}}  \t&\t67.82 &\t59.15 \\\\ \n    {{ } \\ \\  - w {{Inter-Modal}}  \\ \\ \\ \\  - w/o  {{Intra-Modal}}}  \t&\t66.22 \t&\t58.31 \\\\  \n\\hline\n  \\end{tabular}    \n  $}\n  \\caption{\n  Results against different fusion modules. We report w-F1 score for both datasets. \n  } \\label{tab:fusion}\n\\end{table}\n\n\\begin{table}[t]\n\\centering\n\\resizebox{1.0\\linewidth}{!}{$\n\\begin{tabular}{p{1.6cm}|cc|cc} %p{1.6cm}   p{1.9cm}<{\\centering}\n\\hline\n\\multicolumn{1}{c|}{\\multirow{2}{*}{\\textbf{Modality}}} & \\multicolumn{2}{c|}{\\multirow{1}{*}{\\textbf{IEMOCAP}}}  & \\multicolumn{2}{c}{\\multirow{1}{*}{\\textbf{MELD}} }\n\\\\ \n\\cline{2-5}\n  & \\multirow{1}{*}{{GF}} & \\multirow{1}{*}{\\textbf{GDF}} & \\multirow{1}{*}{{GF}} & \\multirow{1}{*}{\\textbf{GDF}}  \\\\ \\hline\n  A / V / T\t & -   & \t47.79 / 27.46 / 61.07  &  - &\t42.72 / 32.34 / 56.95  \\\\   \n  \\hline\n  A + V\t    & 54.73         & \t56.35 & \t42.74 & \t44.67 \\\\  \n  A + T       & 65.03  & \t65.41   & \t57.85 & \t58.34 \\\\  \n  V + T\t    & 62.07         & \t62.63 & \t57.78 & \t58.49 \\\\\n  {A + V + T}   & {67.02}       & {68.18} & {58.54}   &   {59.46} \\\\ \n\\hline\n\\end{tabular} \n$}\n\\caption{Results of graph-based fusion methods under different modality settings. \nFusion modules are not used under unimodal types.\nWe report w-F1 score for both datasets.\n} \\label{tab:modality}\n\\end{table}\n\n\\subsection{Datasets}\n\\textit{\\textbf{IEMOCAP}} \\cite{DBLP:journals/lre/BussoBLKMKCLN08} \ncontains dyadic conversation videos between pairs of ten unique speakers. \nIt includes 7,433 utterances and 151 dialogues. Each utterance is annotated with one of six emotion labels.\nWe follow the previous studies \\cite{DBLP:conf/emnlp/GhosalMPCG19,DBLP:conf/acl/HuLZJ20} that use the first four sessions for training, use the last session for testing, and randomly extract 10\\% of the training dialogues as validation split.\n\\textit{\\textbf{MELD}}\n\\cite{DBLP:conf/acl/PoriaHMNCM19} \ncontains multi-party conversation videos collected from Friends TV series, where two or more speakers are involved in a conversation. \nIt contains 1,433 conversations, 13,708 utterances and 304 different speakers. Each utterance is annotated with one of seven emotion labels.\nFor a fair comparison, we conduct experiments using the pre-defined train/validation/test splits in MELD.\n\n\\subsection{Comparison Methods}\n\n\\textbf{TFN} \\cite{DBLP:conf/emnlp/ZadehCPCM17} and \\textbf{LMF} \\cite{DBLP:conf/acl/MorencyLZLSL18} \nmake non-temporal multimodal fusion by tensor product.\n\\textbf{MFN} \\cite{DBLP:conf/aaai/ZadehLMPCM18} synchronizes multimodal sequences using a multi-view gated memory.\n\\textbf{bc-LSTM} \\cite{DBLP:conf/acl/PoriaCHMZM17} leverages an utterance-level LSTM to capture multimodal features. % from the surrounding utterances. bidirectional,  with a hierarchical fusion framework\n\\textbf{ICON} \\cite{DBLP:conf/emnlp/HazarikaPMCZ18}, an extension of CMN \\cite{DBLP:conf/naacl/HazarikaPZCMZ18}, provides conversational features from modalities by multi-hop memories.\n\\textbf{DialogueRNN} \\cite{DBLP:conf/aaai/MajumderPHMGC19}  \nintroduces a recurrent network to track speaker states and context during the conversation.\n\\textbf{DialogueCRN} \\cite{DBLP:conf/acl/HuWH20} designs multi-turn reasoning modules to understand conversational context. %extract and integrate emotional clues.\n\\textbf{DialogueGCN} \\cite{DBLP:conf/emnlp/GhosalMPCG19} utilizes graph structures to combine contextual dependencies.\n\\textbf{MMGCN} \\cite{DBLP:conf/acl/HuLZJ20} uses a graph-based fusion module to capture intra- and inter- modality contextual features.\nAll baselines are reproduced under the same environment, except \\cite{DBLP:conf/emnlp/HazarikaPMCZ18}, which is only applicable for dyadic conversation and the results are from the original paper. % from their open-souce code\nBecause \\cite{DBLP:conf/aaai/MajumderPHMGC19,DBLP:conf/emnlp/GhosalMPCG19,DBLP:conf/acl/HuWH20} are designed for unimodal ERC, \na early concatenation fusion is introduced to capture multimodal features in their implementations. % \n  \n  \n\n\\textit{\\textbf{Implementation Details.}}\nFollowing \\cite{DBLP:conf/acl/HuLZJ20}, raw utterance-level features of acoustic, visual, and textual modality are extracted by TextCNN \\cite{DBLP:conf/emnlp/Kim14}, \\textit{OpenSmile}  \\cite{DBLP:journals/speech/SchullerBSS11}, and DenseNet \\cite{DBLP:conf/cvpr/HuangLMW17}, respectively. % using\nWe use focal loss \\cite{lin2017focal} for training due to the class imbalance. %on both datasets\nThe number of layers $K$ are 16 and 32 for IEMOCAP and MELD. \n$\\alpha$ is set to 0.2 and $\\rho$ is set to 0.5. \n\n\\subsection{Experimental Results and Analysis}\n\\textit{\\textbf{Overall Results and Ablation Study.}}\nThe overall results are reported in Table~\\ref{tab:result}.\nMM-DFN consistently obtains the best performance over the comparison methods on both datasets, which shows the superiority of our model.\nTable~\\ref{tab:abla} shows ablation studies by removing key components of the proposed model. % experiments\nWhen removing either the graph-based dynamic fusion (GDF) module or speaker embedding (Speaker), the results decline significantly on both datasets. \nWhen further removing the context embedding (Context), the results decrease further. \nIt shows the effectiveness of the three components.\n\n \\textit{\\textbf{Comparison with Different Fusion Modules.}}\nAfter the modality encoder, \nwe replace GDF with the following six fusion modules:\n\\textbf{Concat/Gate Fusion}, % \\textbf{ Fusion}, \n\\textbf{Tensor/Memory Fusion}\\cite{DBLP:conf/acl/MorencyLZLSL18,DBLP:conf/aaai/ZadehLMPCM18},\n\\textbf{Early/Late Fusion + GCN }\\cite{DBLP:conf/acl/HuLZJ20}, and  \n\\textbf{Graph-based Fusion} (GF)   \\cite{DBLP:conf/acl/HuLZJ20}.\nFrom Table~\\ref{tab:fusion}, GF and GDF outperform all fusion modules in the first block since the two graph-based fusion modules sufficiently capture intra- and inter-modality interactions in conversations, which provides complementarity between modalities. \nGDF achieves better performance, reducing  redundancy  and  promoting the  complementarity  between  modalities, which shows the superiority of multimodal fusion. %   of  the  next  layer\nBesides, for GF and GDF, we analyze the impact of inter- and intra-modality edges in the graph for fusion.\n{Intra-}/{Inter-Modal} refers to building edges according to the first/second rule.\nIgnoring any rules can hurt performance in GF and GDF, which shows that modeling contextual interactions of both inter- and intra-modality, can better utilize the complementarity  between modalities.\nCompared with GF, GDF obtains a better performance in all variants. % almost\nIt shows that GDF can reduce both inter- and intra-modality redundancies and fuse multimodal context better.\n\n \\textit{\\textbf{Comparison under Different Modality Settings.}}\nTable~\\ref{tab:modality} shows the results of MM-DFN and the GF-based variant under different modality settings.\nAs expected, bimodal and trimodal models outperform the corresponding unimodal models on both datasets. % \nUnder unimodal types, textual modality performs better than acoustic and visual. \nUnder bimodal types, GDF outperforms GF consistently.\nIt again confirms the superiority of GDF. % in the multimodal fusion.\nMeanwhile, under acoustic and textual modalities (A+T), both GF and GDF achieve the best performance over other bimodal types, which indicates a stronger complementarity between rich textual semantics and affective audio features. %there is \nGDF can reduce redundancy as well as enhance complementarity between modalities and thus obtain better results. \nMoreover, under acoustic and visual modalities (A+V), GDF outperforms GF by a large margin.\nThis phenomenon reflects that the acoustic and visual features have high entanglement and redundancy, limiting the performance of GF.\nOur GDF encourages disentangling and reduces redundancy by controlling information flow between modalities, accordingly obtaining better fusion representations.\n\n\\section{Conclusion}\n\nThis paper proposes a Multimodal Dynamic Fusion Network (MM-DFN) to fully understand conversational context for multimodal ERC task. \nA graph-based dynamic fusion (GDF) module is designed to fuse multimodal features in a conversation. The stack of GDFs learns dynamics of contextual information in different semantic spaces, successfully reducing redundancy and enhancing complementarity between modalities. \nExtensive experiments on two benchmark datasets demonstrate the effectiveness and superiority of MM-DFN. \n\n\\begin{thebibliography}{10}\n\n  \\bibitem{DBLP:journals/inffus/MaNXC20}\n  Yukun Ma, Khanh~Linh Nguyen, Frank~Z. Xing, et~al.,\n  \\newblock ``A survey on empathetic dialogue systems,''\n  \\newblock {\\em Inf. Fusion}, vol. 64, pp. 50--70, 2020.\n  \n  \\bibitem{DBLP:conf/acl/PoriaHMNCM19}\n  Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, et~al.,\n  \\newblock ``{MELD:} {A} multimodal multi-party dataset for emotion recognition\n    in conversations,''\n  \\newblock in {\\em {ACL}}, 2019, pp. 527--536.\n  \n  \\bibitem{DBLP:conf/acl/HuWH20}\n  Dou Hu, Lingwei Wei, and Xiaoyong Huai,\n  \\newblock ``Dialoguecrn: Contextual reasoning networks for emotion recognition\n    in conversations,''\n  \\newblock in {\\em {ACL/IJCNLP}}, 2021, pp. 7042--7052.\n  \n  \\bibitem{DBLP:conf/aaai/MajumderPHMGC19}\n  Navonil Majumder, Soujanya Poria, Devamanyu Hazarika, et~al.,\n  \\newblock ``Dialoguernn: An attentive {RNN} for emotion detection in\n    conversations,''\n  \\newblock in {\\em {AAAI}}, 2019, pp. 6818--6825.\n  \n  \\bibitem{DBLP:conf/emnlp/GhosalMPCG19}\n  Deepanway Ghosal, Navonil Majumder, Soujanya Poria, et~al.,\n  \\newblock ``Dialoguegcn: {A} graph convolutional neural network for emotion\n    recognition in conversation,''\n  \\newblock in {\\em {EMNLP/IJCNLP}}, 2019, pp. 154--164.\n  \n  \\bibitem{DBLP:conf/acl/PoriaCHMZM17}\n  Soujanya Poria, Erik Cambria, Devamanyu Hazarika, et~al.,\n  \\newblock ``Context-dependent sentiment analysis in user-generated videos,''\n  \\newblock in {\\em {ACL}}, 2017, pp. 873--883.\n  \n  \\bibitem{DBLP:conf/emnlp/HazarikaPMCZ18}\n  Devamanyu Hazarika, Soujanya Poria, Rada Mihalcea, et~al.,\n  \\newblock ``{ICON:} interactive conversational memory network for multimodal\n    emotion detection,''\n  \\newblock in {\\em {EMNLP}}, 2018, pp. 2594--2604.\n  \n  \\bibitem{DBLP:conf/icmcs/FuOWGSLD21}\n  Yahui Fu, Shogo Okada, Longbiao Wang, et~al.,\n  \\newblock ``{CONSK-GCN:} conversational semantic- and knowledge-oriented graph\n    convolutional network for multimodal emotion recognition,''\n  \\newblock in {\\em {ICME}}. 2021, pp. 1--6, {IEEE}.\n  \n  \\bibitem{DBLP:conf/emnlp/ZadehCPCM17}\n  Amir Zadeh, Minghai Chen, Soujanya Poria, et~al.,\n  \\newblock ``Tensor fusion network for multimodal sentiment analysis,''\n  \\newblock in {\\em {EMNLP}}, 2017, pp. 1103--1114.\n  \n  \\bibitem{DBLP:conf/acl/MorencyLZLSL18}\n  Zhun Liu, Ying Shen, Varun~Bharadhwaj Lakshminarasimhan, et~al.,\n  \\newblock ``Efficient low-rank multimodal fusion with modality-specific\n    factors,''\n  \\newblock in {\\em {ACL} {(1)}}. 2018, pp. 2247--2256, ACL.\n  \n  \\bibitem{DBLP:conf/aaai/ZadehLMPCM18}\n  Amir Zadeh, Paul~Pu Liang, Navonil Mazumder, et~al.,\n  \\newblock ``Memory fusion network for multi-view sequential learning,''\n  \\newblock in {\\em {AAAI}}, 2018, pp. 5634--5641.\n  \n  \\bibitem{DBLP:conf/mm/ChenSOLS21}\n  Feiyu Chen, Zhengxiao Sun, Deqiang Ouyang, et~al.,\n  \\newblock ``Learning what and when to drop: Adaptive multimodal and contextual\n    dynamics for emotion recognition in conversation,''\n  \\newblock in {\\em {ACM} Multimedia}, 2021, pp. 1064--1073.\n  \n  \\bibitem{DBLP:conf/acl/HuLZJ20}\n  Jingwen Hu, Yuchen Liu, Jinming Zhao, et~al.,\n  \\newblock ``{MMGCN:} multimodal fusion via deep graph convolution network for\n    emotion recognition in conversation,''\n  \\newblock in {\\em {ACL/IJCNLP}}, 2021, pp. 5666--5675.\n  \n  \\bibitem{DBLP:conf/icassp/LiuCWLFGD21}\n  Jiaxing Liu, Sen Chen, Longbiao Wang, et~al.,\n  \\newblock ``Multimodal emotion recognition with capsule graph convolutional\n    based representation fusion,''\n  \\newblock in {\\em {ICASSP}}, 2021, pp. 6339--6343.\n  \n  \\bibitem{DBLP:journals/neco/HochreiterS97}\n  Sepp Hochreiter and J{\\\"{u}}rgen Schmidhuber,\n  \\newblock ``Long short-term memory,''\n  \\newblock {\\em Neural Comput.}, vol. 9, no. 8, pp. 1735--1780, 1997.\n  \n  \\bibitem{DBLP:conf/icml/ChenWHDL20}\n  Ming Chen, Zhewei Wei, Zengfeng Huang, et~al.,\n  \\newblock ``Simple and deep graph convolutional networks,''\n  \\newblock in {\\em {ICML}}, 2020, pp. 1725--1735.\n  \n  \\bibitem{DBLP:conf/iclr/KipfW17}\n  Thomas~N. Kipf and Max Welling,\n  \\newblock ``Semi-supervised classification with graph convolutional networks,''\n  \\newblock in {\\em {ICLR} (Poster)}, 2017.\n  \n  \\bibitem{DBLP:conf/textgraphs/SkianisMV18}\n  Konstantinos Skianis, Fragkiskos~D. Malliaros, and Michalis Vazirgiannis,\n  \\newblock ``Fusing document, collection and label graph-based representations\n    with word embeddings for text classification,''\n  \\newblock in {\\em TextGraphs@NAACL-HLT}, 2018, pp. 49--58.\n  \n  \\bibitem{DBLP:journals/lre/BussoBLKMKCLN08}\n  Carlos Busso, Murtaza Bulut, Chi{-}Chun Lee, et~al.,\n  \\newblock ``{IEMOCAP:} interactive emotional dyadic motion capture database,''\n  \\newblock {\\em Lang. Resour. Evaluation}, vol. 42, no. 4, pp. 335--359, 2008.\n  \n  \\bibitem{DBLP:conf/naacl/HazarikaPZCMZ18}\n  Devamanyu Hazarika, Soujanya Poria, Amir Zadeh, et~al.,\n  \\newblock ``Conversational memory network for emotion recognition in dyadic\n    dialogue videos,''\n  \\newblock in {\\em {NAACL-HLT}}, 2018, pp. 2122--2132.\n  \n  \\bibitem{DBLP:conf/emnlp/Kim14}\n  Yoon Kim,\n  \\newblock ``Convolutional neural networks for sentence classification,''\n  \\newblock in {\\em {EMNLP}}, 2014, pp. 1746--1751.\n  \n  \\bibitem{DBLP:journals/speech/SchullerBSS11}\n  Bj{\\\"{o}}rn~W. Schuller, Anton Batliner, Stefan Steidl, et~al.,\n  \\newblock ``Recognising realistic emotions and affect in speech: State of the\n    art and lessons learnt from the first challenge,''\n  \\newblock {\\em Speech Commun.}, vol. 53, no. 9-10, pp. 1062--1087, 2011.\n  \n  \\bibitem{DBLP:conf/cvpr/HuangLMW17}\n  Gao Huang, Zhuang Liu, Laurens van~der Maaten, et~al.,\n  \\newblock ``Densely connected convolutional networks,''\n  \\newblock in {\\em {CVPR}}, 2017, pp. 2261--2269.\n  \n  \\bibitem{lin2017focal}\n  Tsung-Yi Lin, Priya Goyal, Ross Girshick, et~al.,\n  \\newblock ``Focal loss for dense object detection,''\n  \\newblock in {\\em ICCV}, 2017, pp. 2980--2988.\n  \n  \\end{thebibliography}\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{EmoCaps: Emotion Capsule based Model for\\\\ \n\tConversational Emotion Recognition}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\n\tEmotion recognition in conversation (ERC) aims to analyze the speaker's state and identify their emotion in the conversation. Recent works in ERC focus on context modeling but ignore the representation of contextual emotional tendency. In order to extract multi-modal information and the emotional tendency of the utterance effectively, we propose a new structure named Emoformer to extract multi-modal emotion vectors from different modalities and fuse them with sentence vector to be an emotion capsule. Furthermore, we design an end-to-end ERC model called EmoCaps, which extracts emotion vectors through the Emoformer structure and obtain the emotion classification results from a context analysis model. Through the experiments with two benchmark datasets, our model shows better performance than the existing state-of-the-art models.\n\\end{abstract}\n\\section{Introduction}\nEmotion recognition in conversation (ERC) is a work that recognizes the speaker's emotion and its influencing factors in the process of conversation. Nowadays, social media such as Facebook and Twitter generate a large amount of dialogue data with various modalities of textual, audio, and video all the time. The study of speaker emotional tendency has huge potential value in the fields of public opinion analysis, shopping, and consumption. Therefore, conversation emotion recognition has attracted more and more attention from researchers and companies. \n\nIn ERC, existing research mainly focuses on the way of contextual information modeling (Majumder et al., 2019; Ghosal et al., 2019). However, These models have some shortcomings due to their inability to better extract the grammatical and semantic information of the utterance. Recent studies (Yuzhao Mao et al., 2020; Weizhou Shen et al., 2020) have introduced the transformer structure into the utterance feature extraction to solve the above problems. Li et al. (2021) proposed a new expression vector, \"emotion vector\" for ERC, which is obtained by mapping from sentence vector, but only for textual modality. Meanwhile, existing studies (Song et al., 2004; Dellaert et al., 1996; Amir, 1998) have shown that only textual information is not enough for emotional presentation, the tone and intonation reflect the speaker’s emotions to a certain extent, and the facial expressions also express the inner feelings of the speaker in most cases. As shown in Figure 1, different modalities contain different information, and all are slightly flawed, so multi-modal based information can better identify the speaker's emotion than a single modality in ERC. \n\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=0.9\\columnwidth]{fig0.png} \n\t\\caption{Visualization of the heatmap for an utterance in a conversation, with three modalities.}\n\t\\label{fig0}\n\\end{figure}\n\nIn order to identify the speaker’s emotion in conversation effectively, it is necessary to obtain good utterance features. Also we can't ignore the role of the utterance’s emotional tendency. As shown in Figure 2, the emotional tendency of the utterance itself is like an \"offset vector\", which makes the neutral utterance have an \"emotional direction\". For single-sentence emotion classification, emotional tendency is consistent with the results of emotion recognition, while in ERC, the influence of the context may cause the emotional tendency to be inconsistent with the result of emotion recognition. However, emotional tendency can provide features for the model so that model can \"understand\" the reason for emotional reversal.\n\nSo, we propose a new multi-modal emotional tendency extraction method called Emoformer, which is a Transformer-based model but doesn't include the decoder part. As shown in Figure 3, Emoformer extracts the emotional tendency, i.e., emotion vector, from the modal features through the multi-head self-attention layer and feed-forward layer. More details we will analysis in Section 3.\n\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=0.9\\columnwidth]{fig1.png} \n\t\\caption{A map for emotion classification. The dashed arrow represent the offset vector, which is added to the neutral vector to obtain the vector with an emotional direction.}\n\t\\label{fig1}\n\\end{figure}\n\nBased on the Emoformer, we further propose an end-to-end ERC model to classify the emotion based on multi-modal information, named EmoCaps. Specifically, we employ the Emoformer structure to extract emotion vectors of textual, audio, and visual features. Then, we merge the emotion vectors of the three modalities with the sentence vector to an emotion capsule. Finally, we employ a context analysis model to get the final result of the emotion classification.\n\nIn general, the contributions of this paper are as follows:\n\n\\begin{itemize}\n\t\n\t\\item We innovatively introduce the concept of emotion vectors to multi-modal emotion recognition and propose a new emotion feature extraction structure, Emoformer, which is used to jointly extract emotion vectors of three modalities and merge them with sentence vector to the emotion capsule. %Compared with the existing methods, it can better obtain the emotional tendency information contained in the data of various modalities.\n\t\\item Based on Emoformer, we further propose an end-to-end emotion recognition model named EmoCaps to identify the emotion from multi-modal conversation.  \n\t\\item Our model and the existing state-of-the-art model are tested on MELD and IEMOCAP datasets. The test results show that our model has the best performance both in multi-modality and text-modality.\n\\end{itemize}\n\nThe rest of the paper is organized as follows: Section 2 discusses related works; Section 3 introduces the proposed EmoCaps model in detail; Section 4 and 5 present the experiment setups on two benchmark datasets and the analysis of experiment results; Finally, Section 6 concludes the paper.\n\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=0.8\\columnwidth]{fig2.png} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.\n\t\\caption{Schematic diagram of Emoformer. The mapping network consists of 5 fully connected layers.}\n\t\\label{fig2}\n\\end{figure}\n\\section{Related Work}\n\\subsection{Emotion Recognition in Conversation}\nPoria et al. (2017) use Biredectional LSTM (Hochreiter and Schmidhuber 1997) in ERC, which builds context information without differentiating among the speakers. ICON (Hazarika et al., 2018b) is an extension of CMN (Hazarika et al.,2018), which contains another GRU structure to connect the output in the CMN model to distinguish the speaker relationship. Majumder et al. (2019) use three GRUs to obtain context information and update the speaker status. Ghosal et al. (2019) construct a conversation into a graph, then use a graph convolutional neural network to convert the emotion classification task of the conversation into a node classification problem of the graph. Ghosal et al. (2020) use common sense knowledge to learn the interaction of interlocutors. Shen et al. (2021) design a directed acyclic neural network for encoding the utterances. Hu et al. (2021) propose the DialogueCRN to fully understand the conversational context from a cognitive perspective. \n\\subsection{Multi-modal Emotion Recognition}\nZadeh et al. (2017) propose the TFN model, which is a multi-modal method using the tensor outer product. Liang et al. (2018) propose the model which use a multi-level attention mechanism to extract different modal interaction information. Cai et al. (2019) propose a hierarchical fusion model to model graphic information for irony recognition. But the above models are not applied in ERC. Hazarika et al. (2018b) propose the CMN model in ERC, which uses a GRU structure to store multi-modal data information and considers the role of contextual information in conversation emotion recognition. Jingwen Hu et al. (2021) propose the MMGCN model, which is a graph convolutional neural network model based on a multi-modal hybrid approach. \n\\subsection{Transformer Models}\nInspired by the self-attention mechanism (Bengio et al. 2014), the Transformer is proposed for computing representations and efficiently obtaining long-distance contextual information without using sequence (Vaswani et al. 2017), which has achieved great success in the field of computer vision and audio processing (Tianyang Lin et al. 2021). Devlin et al. (2019) use the Transformer structure to train a large-scale general-purpose text corpus to obtain a language model with syntactic and semantic information. By employing a transformer-based pretraining model, Hazarika et al. (2020) transfer the context-level weight of the generated conversation model to the conversation emotion recognition model. Yuzhao Mao et al. (2020) use Transformer to explore differentiated emotional behaviors from the perspective of within and between models. Weizhou Shen et al. (2020) use the XLNet model for conversation emotion recognition (DialogXL) to obtain longer-term contextual information. The above-mentioned algorithms use a transformer-based structure but they are not applied for multi-modal models.\n\\section{Methodology}\n\\begin{figure*}[t]\n\t\\centering\n\t\\includegraphics[width=0.8\\textwidth]{fig3.png} % Reduce the figure size so that it is slightly narrower than the column.\n\t\\caption{Framework illustration of the EmoCaps based ERC model.}\n\t\\label{fig3}\n\\end{figure*}\n\\subsection{Problem Definition}\n\\begin{table}[]\n\t\\centering\n\t\\begin{tabular}{c|c|c}\n\t\t\\hline\n\t\t\\multirow{2}{*}{\\textbf{Parameter   Setting}} & \\multicolumn{2}{c}{\\textbf{Dataset}} \\\\ \\cline{2-3} \n\t\t& IEMOCAP            & MELD             \\\\ \\hline\n\t\tEpochs                                        & 80                 & 80               \\\\\n\t\tLr                                            & 0.0001             & 0.0001           \\\\\n\t\tDr                                            & 0.1                & 0.1              \\\\\n\t\tBatch size                                    & 30                 & 30               \\\\\n\t\tDim-T                                         & 100                & 600              \\\\\n\t\tDim-V                                         & 256                & 256              \\\\\n\t\tDim-A                                         & 100                & 300              \\\\ \\hline\n\t\\end{tabular}\n\t\\caption{ Parameter settings detail of MELD dataset and IEMOCAP dataset. Epochs represents number of training epochs, Lr represents the learning rate, Dr represents the dropout rate. Dim-T represent the the total dimension of sentence vector and textual emotion vector, Dim-V and Dim-A represent the emotion vector dimensions of visual and audio modalities.}\n\\end{table}\nGiven a dialogue:{$u_1,u_2,u_3,…,u_n$}, where $n$ is the number of utterances. The purpose of conversation emotion recognition is to input a dialogue and identify the correct emotion classification of each sentence in the dialogue from the emotion label set $y$:{$y_1,y_2,y_3,...,y_m$}, where $m$ is the number of emotional label. \n\\subsection{Unimodal Feature Extraction}\n\nWe extract the features of the utterance u, represented as U. In particular, when the input data is multi-modal, features of utterance U can be expressed as:\n\\begin{equation}\n\tU=[U_t,U_a,U_v]\n\\end{equation}\t\nwhere $U_t$ means textual feature, $U_a$ means audio feature, and $U_v$ means visual feature.\n\n\\noindent\\textbf{Textual Feature Extraction}: In order to obtain good utterance representation, we use a pre-trained language model, BERT, to extract text feature vectors. BERT is a large general-purpose pre-trained language model proposed by Devlin et al. (2019), which can effectively represent the grammatical and semantic features of the utterance. Specifically, we first split the dialogue into a series of individual utterances, which are used as the input of the BERT-base model. Unlike other downstream tasks, we use the transformer structure to encode the utterances without classifying or decoding; then we get the sentence vector of every utterance with 512 dimensions. It should be noted that using a larger pre-tranined BERT model did not improve the performance, and a smaller BERT model couldn't get good enough performance.\n\n\\noindent\\textbf{Audio Feature Extraction}: Identical to Hazarika et al. (2018), we use OpenSMILE (Eyben et al. 2010) for acoustic feature extraction. Specifically, in this work, we use the IS13 ComParE config file, which extracts a total of 6373 features for each utterance video, then we use the fully connected layer to reduce the dimensionality to 512 dimensions.\n\n\\noindent\\textbf{Visual Feature Extraction}: We use the 3D-CNN model to extract video features, especially the facial expression features of the speaker. The 3D-CNN model can capture changes in the speaker's expression, which is important information in ERC. Specifically, we use 3D-CNN with three fully connected layers to get a 512-dimensional vector.\n\n\\subsection{Our Method} \nWe assume that the emotion of the utterances in the dialogue depends on three factors:\n\\begin{itemize}\n\t\\item The emotional tendency of the utterance itself.\n\t\\item Emotional information contained in different modal of utterance.\n\t\\item Context information\n\\end{itemize}\nBased on the above three factors, our model EmoCaps is modeled as follows: We obtain three modal features of dialogue data: textual, audio, and visual; and input them into the Emoformer structure, then get the emotion vector of three modals and fuse them with sentence vector; finally, the context analysis model is used to obtain the emotion recognition result. The framework of the model is shown in Figure 4. It is worth noting that our text features, i.e., the sentence vector, are encoded by a transformer-based pre-trained language model, so we no longer use the self-attention mechanism but directly employ a mapping network to extract the emotion vector, then the residual structure concats sentence vector with emotion vector.\n\n\\noindent\\textbf{Emoformer Block} Existing methods mainly use CNN, TextCNN, GRU, etc., to extract text feature vectors, which extract grammatical information weakly. At the same time, they only take the original feature vectors without emotional tendency as input. Based on this, we propose to use the Emoformer structure to extract the emotion vectors of various modalities. As shown in Figure 3, Emoformer has an Encoder structure similar to Transformer, but does not include the Decoder structure. A multi-head attention layer is used to get the emotional tendency feature from the original feature, both are connected through the residual structure, then emotion vector is obtained through a mapping network composed of 5 fully connected layers. The self-attention layer can be used to extract features that contain emotional tendencies or emotional factors effectively, and the residual structure ensures the integrity of the original information; finally the mapping network decouples features and reduces feature dimensions.\n\nIdentical to Vaswani et al. (2017), for a given input feature $U$, we calculate three matrix of query $Q$ $\\in$ $\\mathbb{R}^{{T}_{Q} \\times {d}_{Q}}$, key $K$ $\\in$ $\\mathbb{R}^{{T}_{K} \\times {d}_{K}}$ and value $V$ $\\in$ $\\mathbb{R}^{{T}_{V} \\times {d}_{V}}$ by linear transformation from $U$:\n\\begin{equation}\n\t[Q,K,V]=U [W^Q,W^K,W^V]\n\\end{equation}\nwhere ${T}_{Q}$, ${T}_{K}$, ${T}_{V}$ represent the sequence length of the $Q$, $K$, $V$, and ${d}_{Q}$, ${d}_{K}$, ${d}_{V}$ represent the dimensions of the $Q$, $K$, $V$, and $W^Q$ $\\in$ $\\mathbb{R}^{{d}_{Q} \\times {d}_{m}}$, $W^K$ $\\in$ $\\mathbb{R}^{{d}_{K} \\times {d}_{m}}$, $W^V$ $\\in$ $\\mathbb{R}^{{d}_{V} \\times {d}_{m}}$.\n\nThen we can express the formula of the self-attention layer as:\n\\begin{equation}\n\tA=softmax (\\frac{QK^{T}}{\\sqrt{d_k}})V\n\\end{equation}\nwhere $A$ is the weight of value $V$, $d_k$ is equal to the dimension of $u$. In this way, multiple self-attention layers are concatenated to get Multi-Head Attention layer:\n\\begin{equation}\n\tMultiHead (A)=Concat (A_1,...,A_h) W\n\\end{equation}\nwhere $A_1,...,A_h$ are the output of self-attention layers, $h$ is the number of layers, and $W$ is the weight parameter.\n\nThen a residual connection with normalization layer is used to normalize the output of Multi-Head attention layer, and a Feed Forward layer is employed to get the output of the self-attention parts:\n\\begin{equation}\t\n\tN=Norm (A+MultiHead (A))\n\\end{equation}\n\\begin{equation}\n\tF=max (0,NW_1+b_1)W_2+b_2\\vspace{0.6ex}\n\\end{equation}\n\\begin{equation}\n\tG=Norm(F+MultiHead(F))\n\\end{equation}\nwhere $W_1$, $W_2$ are the weight parameter, $b_1$, $b_2$ are the bias parameter.\n\nFinally, the orignal features $U$ and the output of self-attention parts $G$ are connected through the residual structure, and a mapping network is employed to get the final output $E$:\n\\begin{equation}\n\tH=U \\oplus G\n\\end{equation}\n\\begin{equation}\n\tE=Map (H)\n\\end{equation}\nwhere the Map represents the mapping network, which consists of 5 fully connected layers.\n\nCombine the above Eq. (2) to (9), we can get different modality emotion vectors from different input channels with Emoformer:\n\\begin{equation}\n\t[E_a,E_v,E_t]=Emoformer(U_a,U_v,U_t)\n\\end{equation}\t\nwhere $U_a, U_v, U_t$ represent the original input of audio,visual and textual features, and $E_a, E_v, E_t$ represent the emotion vectors of modalities. \n\n\\noindent\\textbf{Emotion Capsule} For the composition of the emotion capsule, we are based on the following rules: the text feature vector of the utterance contains grammatical and semantic features, emotion vector represents the emotional tendency of the utterance. Both are the main sources of conversation emotion recognition. Textual features most intuitively represent the meaning, emotions, characteristics, etc., of the utterance. However, visual features and audio features contain a few of emotional factors and emotional features, which can provide some emotional clues when the text features do not have sufficient emotional inclination. Therefore, sentence vector concats with three modalities' emotion vector to be an emotion capsule, which just like a \"capsule\", the emotion vector is \"wrapped\" by the sentence vector and \"absorbed\" by the context analysis model to determine the speaker's emotion finally. Our emotion capsule $O$ can be expressed as:\n\\begin{equation}\n\tO=U_t \\oplus E_t \\oplus E_v \\oplus E_a\n\\end{equation}\t\n\\begin{table*}[]\n\t\\centering\n\t\\begin{tabular}{c|c|c|c|c|c|c|c}\n\t\t\\hline\n\t\t& \\multicolumn{7}{c}{IEMOCAP}                                                                                         \\\\ \\cline{2-8} \n\t\t& Happy          & Sad            & Neutral        & Angry          & Excited        & Frustrated     & Average        \\\\ \\hline\n\t\tBC-LSTM        & 43.40          & 69.82          & 55.84          & 61.80          & 59.33          & 60.20          & 59.19          \\\\\n\t\tCMN            & 30.38          & 62.41          & 52.39          & 59.83          & 60.25          & 60.69          & 56.13          \\\\\n\t\tDialogueRNN    & 33.18          & 78.80          & 59.21          & 65.28          & 71.86          & 58.91          & 62.75          \\\\\n\t\tDialogueGCN    & 42.75          & 84.54          & 63.54          & 64.19          & 63.08          & \\textbf{66.99} & 64.18          \\\\\n\t\tDialogXL       & -              & -              & -              & -              & -              & -              & 65.95          \\\\\n\t\tDialogueCRN       & -              & -              & -              & -              & -              & -              & 66.20          \\\\\n\t\tDAG-ERC       & -              & -              & -              & -              & -              & -              & 68.03          \\\\\n\t\tMMGCN          & 42.34          & 78.67          & 61.73          & \\textbf{69.00} & 74.33          & 62.32          & 66.22          \\\\ \\hline\n\t\tEmoCaps & \\textbf{71.91} & \\textbf{85.06} & \\textbf{64.48} & 68.99          & \\textbf{78.41} & 66.76          & \\textbf{71.77} \\\\ \\hline\n\t\\end{tabular}\n\t\\caption{Experimental results (F1 score) on the IEMOCAP dataset. Average means weighted average. Some of the models only provide overall average results without results under each emotion category, so some data cells are lacking. }\n\\end{table*}\n\\noindent\\textbf{Context Modeling} Since the same emotion has different expressions, and the same expression can express different emotions in different contexts, it is very difficult to infer the true emotions from a single word (Barrett, 2017). According to Grice's theory of implicature (1957), the meaning of a sentence can be canceled, so it is necessary to integrate context to infer the true meaning of a sentence. Therefore, contextual information is an indispensable part of conversation emotion recognition. Context information is divided into two parts: the information obtained from the previous moment is named emotional clue traceability, and the information obtained from the next moment is named emotional reasoning.\n\nIn this paper, we employ a Bi-directional LSTM model as the context analysis model to extract contextual information. \nIn a conversation, we form a batch of emotion capsules of all utterances into the Bi-LSTM model in the order of dialogue, and each LSTM cell corresponds to an emotion capsule.  For the time i, in the forward propagation sequence, the contextual information $C_i$ at this moment is composed of the hidden state output of the LSTM cells at all previous moments, that is, the emotional clue traceability; in the backpropagation sequence, the contextual information at this moment is composed of the hidden state output of the LSTM cells at all following moments, which is emotional reasoning. The two fed into an MLP with fully connected layers and get the values of the utterance $u_i$ under each emotion label:\n\n\\begin{equation}\n\tl_i=ReLU(W_l C_i+b_l)\n\\end{equation}\t\n\\begin{equation}\n\tP_i=softmax(W_{smax} l_i+b_{smax})\n\\end{equation}\nwhere $W_l$, $W_{smax}$ are the weight parameter, $b_l$, $b_{smax}$ are the bias parameter.\n\nFinally we choose the max value as the emotion label $y$ for the i-th utterance:\n\\begin{equation}\n\ty_i=\\mathop{\\arg\\max}_{m}(P_i [m])\\vspace{0.5ex}\n\\end{equation}\n\n\\section{Experiment Setting}\n\n\\subsection{Dataset}\n\\noindent\\textbf{IEMOCAP} (Busso et al. 2008): The IEMOCAP dataset includes video data of impromptu performances or scripted scenes of about 10 actors. There are in total 7433 utterances and 151 dialogues in IEMOCAP dataset. At the same time, it contains audio and text transcription to meet the needs of multimodal data. In this data set, multiple commentators set the emotional labels of the utterances into six categories: including happy, sad, neutral, angry, excited and frustrated.\n\n\\noindent\\textbf{MELD} (Poria et al. 2019): The MELD dataset contains 13708 utterances and 1433 conversations, which making up from TV series \"Friends\". It is also a multi-modal dataset containing video, audio, and text formats. In this dataset, multiple commentators set the emotional labels of the words into seven categories: including neutral, surprise, fear, sadness, joy, disgust, and angry.\n\\begin{table*}[]\n\t\\centering\n\t\\begin{tabular}{c|c|c|c|c|c|c|c|c}\n\t\t\\hline\n\t\t& \\multicolumn{8}{c}{MELD}                                                                                                           \\\\ \\cline{2-9} \n\t\t& Neutral        & Surprise       & Fear          & Sadness        & Joy            & Disgust       & Angry          & Average        \\\\ \\hline\n\t\tBC-LSTM        & 73.80          & 47.70          & \\textbf{5.40} & 25.10          & 51.30          & 5.20          & 38.40          & 55.90          \\\\\n\t\n\t\tDialogueRNN    & 73.50          & 49.40          & 1.20          & 23.80          & 50.70          & 1.70          & 41.50          & 57.03          \\\\\n\t\tDialogueGCN    & -              & -              & -             & -              & -              & -             & -              & 58.23          \\\\\n\t\tDialogXL       & -              & -              & -             & -              & -              & -             & -              & 62.41          \\\\\n\t\tDialogueCRN       & -              & -              & -             & -              & -              & -             & -              & 58.39          \\\\\n\t\tDAG-ERC       & -              & -              & -             & -              & -              & -             & -              & 63.65          \\\\\n\t\tMMGCN          & -              & -              & -             & -              & -              & -             & -              & 58.65          \\\\ \\hline\n\t\tEmoCaps & \\textbf{77.12} & \\textbf{63.19} & 3.03          & \\textbf{42.52} & \\textbf{57.50} & \\textbf{7.69} & \\textbf{57.54} & \\textbf{64.00} \\\\ \\hline\n\t\\end{tabular}\n\t\\caption{Experimental results (F1 score) on the MELD dataset. Average means weighted average. The CMN model only for two-party conversation, but MELD is a multi-party conversation dataset. Some of the models only provide overall average results without results under each emotion category, so some data cells are lacking. }\n\\end{table*}\n\\subsection{Baseline Models}\n\\noindent\\textbf{BC-LSTM} (Poria et al. 2017): Bc-LSTM uses Bi-directional LSTM structure to encode contextual semantic information, it does not recognize the speaker relationship.\n\n\\noindent\\textbf{CMN} (Hazarika et al. 2018): It takes a multimodal approach comprising audio, visual and textual features with gated recurrent units to model past utterances of each speaker into memories.\n\n\\noindent\\textbf{DialogueRNN} (Majumder et al. 2019): DialogueRNN uses different GRU units to obtain contextual information and speaker relationships. It is the first conversation emotion analysis model to distinguish between speakers.\n\n\\noindent\\textbf{DialogueGCN} (Ghosal et al. 2019): DialogueGCN constructs a conversation into a graph, transforms the speech emotion classification problem into a node classification problem of the graph, and uses the graph convolutional neural network to classify the results.\n\n\\noindent\\textbf{DialogXL} (Weizhou Shen et al. 2020): DialogXL use XLNet model for conversation emotion recognition to obtain longer-term contextual information.\n\n\\noindent\\textbf{DialogueCRN} (Hu et al. 2021): DialogueCRN introduces the cognitive phase to extract and integrate emotional clues from context retrieved by the perceptive phase for context modeling.\n\n\\noindent\\textbf{DAG-ERC} (Weizhou Shen et al. 2021): DAG-ERC  is a directed acyclic graph neural network for ERC, which provides a intuitive way to model the information flow between long-distance conversation background and nearby context.\n\n\\noindent\\textbf{MMGCN} (Jingwen Hu et al. 2021): MMGCN uses GCN network to obtain contextual information, which can not only make use of multimodal dependencies effectively, but also leverage speaker information.\n\\begin{table}[]\n\t\\centering\n\t\\begin{tabular}{c|c|c}\n\t\t\\hline\n\t\t\\multirow{2}{*}{\\textbf{Modality}} & \\multicolumn{2}{c}{\\textbf{Dataset}} \\\\ \\cline{2-3} \n\t\t& IEMOCAP           & MELD              \\\\ \\hline\n\t\tText                               & 69.49             & 63.51             \\\\\n\t\tAudio                              & 33.00             & 31.26             \\\\\n\t\tVideo                              & 31.64             & 31.26             \\\\\n\t\tT+A                                & 71.39             & 63.73             \\\\\n\t\tT+V                                & 71.30             & 63.58             \\\\\n\t\tT+V+A                              & \\textbf{71.77}    & \\textbf{64.00}    \\\\ \\hline\n\t\\end{tabular}\n\t\\caption{Performance (F1 score) of EmoCaps under different multimodal settings. T represent textual modality, A represent audio modality, and V represent visual modality.}\n\\end{table}\n\\subsection{Implementation}\nFor textual data, we use BERT model to obtain the sentence vector then get the textual emotion vector from a mapping network. For audio and visual data, we use Emoformer obtain the audio and visual emotion vector. \n\nAs for the hyperparameter settings, we follow Li et al. (2021). For both of MELD dataset and IEMOCAP dataset, the epochs is set to 80, the learning rate is set to 0.0001, and the dropout rate is set to 0.1. The detailed parameter setting is shown in Table 1.\n\n\\section{Results and Analysis}\nOur proposed model is compared with other state-of-the-art models on the IEMOCAP dataset and MELD dataset, which are under the same parameter conditions. The experimental results are shown in Table 2 and Table 3, our model has the best performance on both datasets.\n\n\\subsection{Compare with Other Baseline Models}\nOn the one hand, compared with existed methods, our model encodes sentences through a pre-trained language model to obtain a better utterance representation. On the other hand, our emotion capsule contains the emotional tendency of the utterance itself, combined with contextual information, can more effectively identify the speaker's emotion. The experimental results prove the rationality of our assumptions about the emotional factors in ERC.\n\\begin{table}[]\n\t\\centering\n\t\\begin{tabular}{c|c|c}\n\t\t\\hline\n\t\t\\multirow{2}{*}{Model} & \\multicolumn{2}{c}{Dataset}    \\\\ \\cline{2-3} \n\t\t& IEMOCAP        & MELD           \\\\ \\hline\n\t\tDialogueRNN            & 71.08          & \\textbf{65.86} \\\\\n\t\tBi-LSTM                & \\textbf{71.77} & 64.00          \\\\ \\hline\n\t\\end{tabular}\n\t\\caption{Performance(F1 score) under the multimodal setting of different models as context modeling model.}\n\\end{table}\n\\subsection{Various Modality}\nTable 4 shows the performance of our model on the MELD dataset and the IEMOCAP dataset under different modality combinations. It is easy to find that the performance of multi-modal input is better than single-modal input. At the same time, among the three modalities of textual, audio, and visual, the textual modal has better performance than the other two modalities. \n\n\\subsection{Error Analysis}\nAs shown in Table 4, the performance of audio and visual modal is not good enough. For audio features, the frequency and amplitude of the sound features can only reflect the intensity of the speaker's emotions, not the specific emotional tendencies. Therefore, when certain emotions have similar frequencies and amplitudes, it is difficult to correctly distinguish the speaker's emotions only through audio data. For example, for two emotions of excited and fear, the frequency and amplitude characteristics in the audio mode are both at high values. Thus it's hard to distinguish the two emotions. For visual features, It is easy for us to judge the speaker's expression by facial features, but when the speaker hides his own expression, the video feature is not enough to judge the speaker's emotion. In addition, for a single video modality, the emotional changes in the context are unexplainable. \n\nWhen the textual modality is added, the performance is significantly improved. In other words, textual modality play a major role in conversation emotion recognition, while audio and visual modalities can help improve the accuracy of recognition, which is consistent with the previous assumptions.\n\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=0.8\\columnwidth]{fig4.png} \n\t\\caption{Visualization of the heatmap in the \"Friends\" Season 3 Dialogue 2. Speaker C refer to Chandler and Speaker M refer to Monica.}\n\t\\label{fig4}\n\\end{figure}\n\n\\subsection{Impact of Speaker Embedding}\nIn order to analyze the impact of speaker modeling on conversation emotion recognition, we use a variant of DialogueRNN as a context modeling model to test its performance on two benchmark datasets. As shown in Table 5, the performance of the DialogueRNN-based model on the MELD dataset is better than the LSTM-based model. The reason is that most of the MELD dataset belongs to multi-person dialogue situations, so the speaker modeling model (DialogueRNN-based) is more effective in identifying speaker emotions than the model not using speaker modeling (LSTM-based). However, in the IEMOCAP dataset, which is based on two-person dialogue situations, speaker modeling becomes insignificant.\n\nFurthermore, compared with the LSTM-based model, using DialogueRNN-based or other models that include speaker modeling structures consumes more computing resources and time.\n\n\\subsection{Case Study}\nFigure 5 shows the influence of emotion vector when emotion reversal in a conversation. At the beginning of the conversation, both speakers are in a neutral emotional state, while utterance 4 changes the situation that speakers' emotion turn into surprise and sadness. The sentence vector doesn't \"understand\" the reason why emotion change, but the emotion vector contains a negative emotion tendency which easier to get the correct emotion label. Utterance 7 shows that when the context is in the sad emotion, the emotion vector makes the utterance \"biased\" to \"sad\", while the sentence vector is in a neutral emotion. It proves the role of emotion vector in ERC.\n\\section{Conclusion}\nIn this paper, we propose a new multi-modal feature extraction structure, Emoformer, which is based on the transformer structure. Further, we design a new ERC model, namely EmoCaps. First, we use Emoformer structures to extract the emotion vectors of textual, audio, and visual modalities, then fuse the three modalities emotion vectors and sentence vectors to be an emotion capsule; finally, we employ a context analysis model to get the emotion recognition result. We conduct comparative experiments on two benchmark datasets. The experimental results show that our model performs better than the existing state-of-the-art models. The experimental results also verified the rationality of our hypothesis.\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{A Low-rank Matching Attention based Cross-modal Feature Fusion Method for Conversational Emotion Recognition}\n\n\\begin{document}\n\n\\title{A Low-rank Matching Attention based Cross-modal Feature Fusion Method for Conversational Emotion Recognition}\n\n\\author{Yuntao~Shou, Huan Liu$^*$, \n\tXiangyong Cao, Deyu Meng\n\tand~Bo Dong \\\\% <-this % stops a space\n\t\\thanks{$^*$ Corresponding Author: Huan Liu~(huanliu@xjtu.edu.cn)}\n\t\\thanks{This work is supported by the National Nature Science Foundation of\n\tChina (No. 62272374, No. 62192781, No. 62250009, No. 62137002, No. 62202367, No. 62272375), Natural\n\tScience Foundation of Shaanxi Province (No. 2024JC-JCQN-62), Project of\n\tChina Knowledge Center for Engineering Science and Technology, Project of\n\tChinese academy of engineering “The Online and Offline Mixed Educational\n\tService System for ‘The Belt and Road’ Training in MOOC China”, and the\n\tK. C. Wong Education Foundation.}\n\t\\IEEEcompsocitemizethanks{\\IEEEcompsocthanksitem Y. Shou, X. Cao, and B. Dong are with School of Computer Science and Technology, the Ministry of Education Key Laboratory for Intelligent Networks and Network Security, and the Shaanxi Province Key Laboratory of Big Data Knowledge Engineering, Xi’an Jiaotong, Xi’an, Shaanxi 710049, China.\n\t\t(shouyuntao@stu.xjtu.edu.cn,\n\t\tcaoxiangyong@mail.xjtu.edu.cn,\n\t\tdong.bo@mail.xjtu.edu.cn).}\n\t\t\n\t\t\\IEEEcompsocitemizethanks{\\IEEEcompsocthanksitem H. Liu are with School of Computer Science and Technology, Xi’an Jiaotong, Xi’an, Shaanxi 710049, China, and the BigKE Joint Innovation Center, SHMEEA, Shanghai, China. (huanliu@xjtu.edu.cn).}\n\t\t\n\t\t\\IEEEcompsocitemizethanks{\\IEEEcompsocthanksitem D. Meng is with School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China. (dymeng@mail.xjtu.edu.cn).}\n\t\t\n\t\t\n}\n\n\\maketitle\n\n\\begin{abstract}\n\tConversational emotion recognition (CER) is an important research topic in human-computer interactions. {Although recent advancements in transformer-based cross-modal fusion methods \n\thave shown promise in CER tasks, they tend to overlook the crucial intra-modal and inter-modal emotional interaction or suffer from high computational \n\tcomplexity. To address this, we introduce a novel and lightweight cross-modal \n\tfeature fusion method called Low-Rank Matching Attention Method (LMAM). LMAM \n\teffectively captures contextual emotional semantic information in \n\tconversations while mitigating the quadratic complexity issue caused by the \n\tself-attention mechanism. Specifically, by setting a matching weight and calculating inter-modal features attention scores row by row, LMAM requires only one-third of the parameters of self-attention methods. We also employ the low-rank decomposition \n\tmethod on the weights to further reduce the number of parameters in LMAM. As a result, LMAM offers a lightweight model while avoiding overfitting problems caused by a large number of parameters. Moreover, LMAM is able to fully exploit the intra-modal emotional contextual information within each modality and integrates complementary emotional semantic information across modalities by computing and fusing similarities of intra-modal and inter-modal features simultaneously. Experimental results verify the superiority of LMAM compared with other popular cross-modal fusion methods on the premise of being more lightweight. Also, LMAM can be embedded into any existing state-of-the-art CER methods in a plug-and-play manner, and can be applied to other multi-modal recognition tasks, e.g., session recommendation and humour detection, demonstrating its remarkable generalization ability.}\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nAttention, Cross Modal Feature Fusion, Low Rank Decomposition, Multimodal Emotion Recognition\n\\end{IEEEkeywords}\n\n\\section{Introduction}\n\n{With the development of the multi-modal research field, Conversational Emotion Recognition (CER) that utilizes three modal data (i.e., video, audio and text) to identify the speaker’s emotional changes during the conversation has become a hot research topic \\cite{mohammad2022ethics, geetha2024multimodal}.} Nowadays, CER has shown its promising performance in many practical social media scenarios. For example, in the field of intelligent recommendation, a recommendation system with emotional tendencies can recommend products that users are more interested in by identifying changes in consumers’ emotions. Therefore, it is of great importance to accurately identify the speaker’s emotional changes during the conversation.\n\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=0.95\\linewidth]{com_fusion.pdf}\n\t\\caption{(a) Early fusion through multimodal concatenation. (b) Multimodal feature fusion through self-attention. $W^Q$, $W^K$, and $W^V$ {represent} the learnable parameters of the query, key, and value vectors respectively. The parameter number of {self-attention} is 9 times of the parameter number of $W^Q$. (c) Multimodal feature fusion through LMAM. The parameter number of our LMAM fusion method is less than 3 times of the parameter number of $W^Q$. $|W^{Q}_i|$ means the parameter number of the column vectors of $W_Q$ after low rank decomposition. $r(\\cdot)$ indicates the rank of the matrix.}\n\t\\label{figure1}\n\\end{figure}\n\n{Many existing studies \\cite{ren2021lr, shou2022conversational, tu2022exploration} have proven that multi-modal feature fusion plays an important role in CER tasks. For example, as shown in Fig. \\ref{figure1}(a), previous work \\cite{cambria2018benchmarking} achieved multi-modal feature fusion by simply concatenating multi-modal features. Although this method has utilized multimodal data to a certain extent, it often adopts a strategy of treating each modality equally without fully considering the difference in the amount of emotional information between different modalities. The above method has two limitations: 1) the modality with less information may introduce noise during the fusion process, weakening the performance of the overall model; 2) the dominant role of the modality with more information in emotion recognition is not fully utilized. In addition, existing methods cannot capture contextual emotional semantic information within and between modalities, which limits the performance of CER tasks.}\n\n{To tackle the above problems, many Transformer-based methods (e.g., CTNet \\cite{lian2021ctnet}, and EmoCaps \\cite{li2022emocaps}, etc) have been proposed and have become the current mainstream multi-modal feature fusion methods in CER task. Specifically, as shown in Fig. \\ref{figure1}(b), these Transformer-based multi-modal feature fusion methods mainly use the self-attention mechanism to calculate the attention scores of different modal features, and achieve the fusion of intra-modal and inter-modal contextual emotional semantic information based on attention scores. {Although the Transformer-based method can more accurately capture and fuse multimodal features in CER tasks, it also has two limitations. 1) This attention-based method has high computational complexity, especially when processing large-scale data, the demand for computing resources increases significantly. 2) As the model complexity increases, the risk of overfitting also increases accordingly, especially when the amount of training data is insufficient or the data diversity is limited. This may lead to a decrease in the generalization ability of the model and make it difficult to maintain stable performance in practical applications.}\n\n{To solve these problems, existing research proposes low-rank decomposition methods \\cite{kolda2009tensor} to reduce the number of parameters. For example, Zhu et al. \\cite{zhu2020multimodal} proposed a Low-rank Tensor Multi-modal Fusion method to achieve multi-modal data fusion through a low-rank tensor outer product and attention mechanism. Jin et al. \\cite{jin-etal-2020-dual} proposed a Fine-grained Temporal Low-rank Multi-modal Fusion (FT-LMF) method to fuse multi-modal information in time and space through low-rank tensor outer product. Sahay et al. \\cite{sahay2020low} proposed Low-rank fusion based transformers to extract potential interactive information in multi-modal information through low-rank tensor outer product and cross-modal Transformer. {Although the above method achieves the fusion of multimodal information through low-rank tensor outer product, this approach has significant limitations. Specifically, this fusion method lacks fine-grained processing capabilities when capturing the potential relationship information between discourses. Although the low-rank tensor outer product method can reduce the computational complexity to a certain extent, due to its inherent structural constraints, it cannot fully express the complex interactions between modalities, resulting in inaccurate capture of potential relationship information.} \n\n{Different from the aforementioned CER methods, we thoroughly leverage the respective advantages of low-rank decomposition and Transformer techniques. Specifically, as shown in Fig. \\ref{figure1}(c), we directly introduce a low-rank weight $\\boldsymbol{W}$ into the self-attention, and then perform row-by-row attention calculation on multi-modal features to achieve fine-grained cross-modal information fusion. Our method not only retains the accuracy of Transformer in feature capture, but also effectively reduces the computational complexity through low-rank decomposition, avoiding the overfitting problem that may be caused by traditional methods.} \n\n{Overall, we introduce a novel and lightweight cross-modal feature fusion method called Low-Rank Matching Attention Method (LMAM). LMAM effectively captures contextual emotional semantic information in conversations while mitigating the quadratic complexity issue caused by the self-attention mechanism. Different from the existing Transformers with a self-attention mechanism, the number of parameters required by LMAM is less than one-third of the self-attention mechanism. Furthermore, LMAM is much easier to learn and may also reduce the risk of over-fitting. Fig.~\\ref{figure1} illustrates a comparison of different cross-modal fusion methods. }\n\n{Specifically, we first use the text, video and audio features pre-extracted by BERT, 3D-CNN and openSMILE respectively to obtain the corresponding query feature vector through a linear layer. In order to reduce the amount of parameters required by the network, we introduce low-rank weight decomposition to achieve compression of the learnable parameters $W$ of the linear layer. Then we perform row-by-row attention calculations on the text query feature vector, video query feature vector, and audio query feature vector with the other two modal features (i.e., matching feature vectors). Finally, we fuse the obtained multi-modal features to obtain the final high-level discourse representation with emotional contextual semantic information.}\n\nOverall, the contributions of this work are summarized as:\n\n\\begin{itemize}\n\t\\item We introduce a novel and lightweight cross-modal feature fusion method called Low-Rank Matching Attention Method (LMAM). LMAM effectively captures contextual emotional semantic information in conversations while mitigating the quadratic complexity issue caused by the self-attention mechanism.\n\t\n\t\\item By introducing only one learnable matching feature vectors and further utilizing the low-rank decomposition method, LMAM can significantly reduce the model complexity of existing Transformer-based cross-modal fusion methods. Furthermore, LMAM can also reduce the risk of overfitting.\n\t\n\t\\item Extensive experiments also verify that the proposed LMAM method can be embedded into the existing DL-based CER methods to improve their recognition accuracy in a plug-and-play manner. In addition, LMAM is a general cross-modal feature fusion method and has potential application value in other multi-modal feature fusion tasks.\n\\end{itemize}\n\n\\section{Related work}\n\n\\subsection{Conversational Emotion Recognition}\nConversational emotion recognition (CER) involves cross-field knowledge such as cognitive science and brain science, and has received extensive attention from researchers \\cite{yang2022hybrid, liu2022social, zhang2024cross}.  Current CER research mainly includes three directions, i.e., sequential context-based emotion recognition, distinguishing speaker state-based emotion recognition, and speaker information-based emotion recognition \\cite{li2022contrast}.\n\nFor the sequential context-based CER approaches, Poria et al. \\cite{poria2017context} proposed a Bidirectional LSTM (bcLSTM), which utilizes recurrent neural units to extract the speaker’s context information in the video, audio, and text features, and then uses the attention mechanism to fusion the information. Hazarika et al. \\cite{hazarika2018icon} designed an interactive conversational memory network (ICON) to extract the multi-modal features of different speakers following the idea of hierarchical modeling, and then input them into the global attention network for information fusion. Xing et al. \\cite{9128015} introduced an Adapted Dynamic Memory Network (A-DMN) to fine-grainedly model the dependencies between contextual utterances. Shahin et al. \\cite{shahin2022novel} proposed a dual-channel long short-term memory compressed-CapsNet to improve the hierarchical representation of contextual information.\n\nFor the different speaker states-based CER methods, Majumder et al.~\\cite{majumder2019dialoguernn} proposed a DialogueRNN with three gating neural units (i.e., global GRU, party GRU and emotion GRU) to encode and update context and speaker information. Hu et al. \\cite{hu2021dialoguecrn} proposed a Contextual Reasoning Networks (CRN) to distinguish the speaker's emotional changes in the perceptual stage and the cognitive stage.\n\nFor the speaker information-based CER methods, Ghosal et al. \\cite{ghosal2019dialoguegcn}  proposed a DialogueGCN to model the dialogue relationship between speakers by constructing a speaker relationship graph from the concatenated multi-modal feature vectors. Sheng et al. \\cite{sheng2020summarize} designed a summarization and aggregation graph inference network (SumAggGIN) to consider global inferences related to dialogue topics and local inferences with adjacent utterances. Hu et al. \\cite{hu2021dialoguecrn} proposed a dialogue contextual reasoning network (DCRN) to extract contextual information from a cognitive perspective, and designed a multi-round reasoning module to fuse the emotional information. {The DCDM proposed by Su et al. \\cite{su2024dynamic} introduced a causal directed acyclic graph (DAG) structure to establish complex correlations between hidden emotional information and other observed dialogue elements and construct a dynamic time disentanglement model to capture the hidden emotional change information in the dialogue.}\n\n\\subsection{Multimodal Feature Fusion Approaches}\nIn this subsetion, we briefly review the multi-modal feature fusion methods for the CER task. Liu et al. \\cite{Liu2018EfficientLM} designed a low-rank multi-modal fusion method (LFM) to reduce the computational complexity caused by the change of tensor dimensions. Hu et al. \\cite{hu2021mmgcn} proposed a multi-modal fused graph convolutional network (MMGCN) to model dialogue relations between speakers and fuse the cross-modal features. Lian et al. \\cite{lian2021ctnet} proposed a Conversational Transformer Network to fuse complementary semantic information from different modalities. Hu et al. \\cite{hu2022mm} proposed Multimodal Dynamic Fusion Network (MM-DFN), which performs emotion recognition by eliminating contextual redundant information. {Ma et al. \\cite{10109845} proposed a transformer-based self-distillation model to dynamically learn the intra-modal and inter-modal information interaction.} {Li et al. \\cite{li2023graphmft} proposed a Graph network based Multi-modal Fusion Technology (GraphMFT) for CER. GraphMFT achieves the fusion of intra-modal and inter-modal contextual semantic information by building multiple graph attention networks.} {The CFN-ESA proposed by Li et al. \\cite{li2024cfn} effectively integrated the data distribution between modalities and promotes the interaction of multimodal information by constructing a unimodal encoder, a cross-modal encoder and a sentiment conversion module.} Although these multi-modal fusion approaches can obtain discriminative fused feature by exploiting the information of different modalities, they are either computationally expensive or do not fully consider the complementary information of different modals.\n\n\\section{Preliminary Information}\n\n\\subsection{Problem Definition}\nWe assume the participants in the dialogue are $P=\\{p_1,p_2,\\ldots,p_N \\}$, where $N$ represents the number of participants $(N\\geq2)$. We define sequential context $T=\\{t_1,t_2,\\ldots,t_M \\}$, where $M$ represents the total number of sessions and $t_i$ represents the $i$-th utterance. The task of CER is to identify the discrete emotions (e.g., happy, sad, disgust, neutral, excited, etc.) in each utterance.\n\n\\subsection{Multimodal Feature Extraction}\nIn the CER task, three types of modality data are included, namely text, video and audio. The feature extraction method of each modal is different, and the semantic information they contain is also different \\cite{ghosh2022comma}. Next, we will briefly introduce their data preprocessing methods.\n\n\\textbf{Word Embedding:} To obtain the feature vector representation of characters that computers can understand \\cite{schuster2022bert}, we use the large-scale pre-training model BERT to encode text features. First, we use the Tokenizer method to segment the text to get each word and its index. We then feed them into the BERT model for feature encoding, and use the first 100-dimensional features in the BERT model as our text feature vectors.\n\n\\textbf{Visual Feature Extraction:} Following Hazarika et al. \\cite{hazarika2018conversational}, we use 3D-CNN to extract the speaker's facial expression features and gesture change features in video frames, which have an important impact on the model's understanding of the speaker's emotional changes. Specifically, we utilize 3D-CNN and a linear layer with 512 neurons to obtain video feature vectors with rich semantic information.\n\n\\textbf{Audio Feature Extraction:} Following Hazarika et al. \\cite{hazarika2018conversational}, we use openSMILE \\cite{eyben2010opensmile} to extract acoustic features in audio (e.g., loudness, Mel-spectra, MFCC). Specifically, we utilize the $IS12\\_ComParE1$ extractor\\footnote{ http://audeering.com/technology/opensmile}  in openSMILE and a linear layer with 100 neurons to obtain speaker audio features.\n\n{\\subsection{Self-attention Mechanism}}\n{Self-Attention Mechanism is an algorithm used for sequence data processing to calculate the correlation between various positions of the input sequence. Owing to its powerful information capture capabilities, self-attention mechanisms have received considerable research attention in various fields, e.g., natural language understanding and text generation. A typical attention mechanism is to obtain the attention score by calculating the correlation between each element in the input sequence and other elements. For a given input sequence $\\boldsymbol{X}$, query $\\boldsymbol{Q}$, key $\\boldsymbol{K}$ and value $\\boldsymbol{V}$ are calculated as follows:}\n\\begin{equation}\n    \\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V} = \\boldsymbol{W}^Q\\boldsymbol{X},\\boldsymbol{W}^K\\boldsymbol{X}, \\boldsymbol{W}^V\\boldsymbol{X}\n\\end{equation}\n{where $\\boldsymbol{Q}$, $\\boldsymbol{K}$,and $\\boldsymbol{V}$ are the learnable parameters.}\n\n{Given the query $\\boldsymbol{Q}$, key $\\boldsymbol{K}$ and value $\\boldsymbol{V}$, the embeddings representation of the sequence are obtained through the dot product and Softmax function as follows:}\n\\begin{equation}\n\\text{Attention}\n(\\boldsymbol{Q},\\boldsymbol{K},\\boldsymbol{V})=\\text{softmax}\\left(\\frac{\\boldsymbol{QK}^\\top}{\\sqrt{d_k}}\\right)\\boldsymbol{V}\n\\end{equation}\n{In this work, we only use one query $\\boldsymbol{Q}$ as our matching feature vectors for self-attention calculation since the high complexity of the model easily leads to the risk of overfitting.}\n\n\\section{Proposed LMAM Cross-modal Fusion Method}\nIn this section, we propose a novel cross-modal fusion method, namely Low-rank Matching Attention Mechanism (LMAM).\n\n\\subsection{Matching Attention Layer}\nWe concatenate the row vectors of all 1s with the extracted multi-modal features to obtain $\\xi$ as follows:\n\\begin{equation}\n\t\\boldsymbol{\\xi}=\\left\\{\\boldsymbol{\\xi}_u, \\boldsymbol{\\xi}_a, \\boldsymbol{\\xi}_v\\right\\},\n\\end{equation}\nwhere $\\boldsymbol{\\xi}_u$ represents context utterence features, $\\boldsymbol{\\xi}_a$ represents audio features, and $\\boldsymbol{\\xi}_v$ represents video features.\n\nThe existing CER approaches usually use feature splicing or feature summation to fuse the cross-modal feature \\cite{zadeh2017tensor,Liu2018EfficientLM,hu2021mmgcn,Liu2018EfficientLM}. As introduced in the related work, these cross-modal fusion methods are either computationally expensive or do not fully consider the complementary information of different modals. Therefore, our goal is to construct an efficient and effective fusion method that captures the differences among multimodal features by computing the correlation among the three modalities of text, video and audio and realizes the fusion of complementary semantic information across modalities. Specifically, the computation process of our proposed LMAM fusion method is shown as follows.\n\n\\begin{figure*}\n\t\\centering\n\t\\includegraphics[width=0.98\\linewidth]{low-rank_attention}\n\t\\caption{LMAM achieves information fusion of multimodal features through parallel low-rank decomposition of weight (i.e., $\\omega_a, \\omega_v, \\omega_t$) and modal features. We add an extra dimension to each modality feature and pad them with 1 to ensure that the intra-modal semantic information is preserved during inter-modal feature fusion.}\n\t\\label{fig:low-rank-attention}\n\\end{figure*}\n\n\\begin{algorithm}[tb]\n\t\\caption{Matching Attention Mechanism (LMAM)}\n\t\\label{alg:algorithm}\n\t\\textbf{Input}: Text feature vectors $\\xi_t$, video feature vectors $\\xi_v$ and audio feature vectors $\\xi_a$; the number of iterations $\\epsilon$; the size of dataset $\\phi$.\\\\ \n\t\n\t\\textbf{Output}: The enhanced multimodal fusion feature vectors $\\xi_f$.\n\t\n\t\\begin{algorithmic}[1] %[1] enables line numbers\n\t\t\\STATE Initialize the model weights $W$ and bias $b$.\n\t\t\\STATE Initialize the set of multimodal feature fusion $\\xi_{fusion}$.\n\t\t\\FOR{EPOCH $\\leftarrow 1,2,\\ldots,\\epsilon$}\n\t\t\\FOR{$i \\leftarrow 1,2,\\ldots,len(\\phi) / 32$ }\n\t\t\\STATE Sample a batch $\\xi=\\{\\xi_t, \\xi_v, \\xi_a\\}_{i=1}^{32}$.\n\t\t\\FOR{modal in $\\xi$}\n\t\t\\FOR{t in modal}\n\t\t\\STATE att\\_emo, score=MatchingAttention(modal, t)\n\t\t\\STATE att\\_emotions.append(att\\_em.unsqueeze(0))\n\t\t\\STATE scores.append(score[:, 0, :])\n\t\t\\ENDFOR\n\t\t\\STATE att\\_emotions=torch.cat(att\\_emotions, dim=0)\n\t\t\\STATE $\\varPsi$   =att\\_emotions + F.gelu(modal)\n\t\t\\ENDFOR\n\t\t\\ENDFOR\n\t\t\\ENDFOR\n\t\t\\STATE $\\xi_{fusion}=\\{\\varPsi_1,\\varPsi_2,\\ldots,\\varPsi_{len(\\phi)/32}\\}$.\n\t\t\\STATE \\textbf{return} the enhanced multimodal feature vectors $\\xi_{fusion}$.\n\t\\end{algorithmic}\n\\end{algorithm}\n\n{For a given model input $\\boldsymbol{K}_i$, $\\boldsymbol{V}_i$ and $\\boldsymbol{M}_i$, {where $\\boldsymbol{K}_i = \\boldsymbol{\\xi}_i$, $\\boldsymbol{V}_i = \\boldsymbol{\\xi}_i$, and $\\boldsymbol{M}_i = \\boldsymbol{\\xi}_i$.} We first compute the query matrix $\\boldsymbol{Q}_i \\in \\mathbb{R}^{L_{\\boldsymbol{Q}_i} \\times d_{\\boldsymbol{Q}_i}}$ {by linear transformation from $\\boldsymbol{M}_i$} as follows:}\n\\begin{equation}\n\t\\boldsymbol{Q}_i = \\boldsymbol{M}_i\\boldsymbol{W}^{{Q}_i}+\\boldsymbol{b}^{{Q}_i},\n\\end{equation}\n{where $\\boldsymbol{K}_i$ and $\\boldsymbol{M}_i$ represent the $i$-th modal features. {$L_{{Q}_i}$} represents the sequence length of the modal features. $d_{{Q}_i}$ represents the feature dimension after linear layer mapping, and $\\boldsymbol{W}^{{Q}_i} \\in \\mathbb{R}^{d_m \\times d_{{Q}_i}}$, $d_m$ represents the feature embedding dimension of $\\boldsymbol{Q}_i$.}\n \n\nNext, we get the attention score using the following formula:\n\\begin{equation}\t\n\t\\begin{aligned}\n\t\\boldsymbol{\\alpha}_i=softmax\\left(Tanh\\left(\\frac{\\boldsymbol{Q}_i \\boldsymbol{K}_i^\\top}{\\sqrt{d_k}}\\right)\\right) \\\\\n\t\\end{aligned}\n\\end{equation}\n{where $\\top$ represents the matrix transpose operation. $d_k$  represents the sequence length of the modal features, $\\boldsymbol{\\alpha}_i$ represents the attention score, and $\\boldsymbol{\\alpha}_i \\in [0,1]$.}\n\nSubsequently, we perform matrix multiplication by the attention score and the modality feature $\\boldsymbol{I}_i$ to obtain the attention output as follows:\n\\begin{equation}\n\t\\begin{aligned}\n\t\\boldsymbol{A}_i =\\boldsymbol{\\alpha_i\\boldsymbol{V}_i}\n\t\\end{aligned}\n\\end{equation}\n{where {$\\boldsymbol{A}_i$} is the feature vector after attention calculation. As shown in Fig. \\ref{fig:transformer}, we present the fusion process of text features and audio features using a matched attention mechanism. Similarly, the fusion process for text and video, and video and audio follows the same paradigm.}\n\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=0.9\\linewidth]{Transformer}\n\t\\caption{An illustrative example of the proposed LMAM method fusing complementary semantic information between different modal features. We show the fusion process of text features and audio features. The fusion process for text and video, and video and audio follows the same paradigm.}\n\t\\label{fig:transformer}\n\\end{figure}\n\n{To prevent the problem of gradient disappearance and information collapse in the model training, we also build a residual connection layer with normalization operation. Finally, we use a linear layer with ReLU activation function to get the final output of the LMAM. The formulas are as follows:}\n\\begin{equation}\n\\begin{aligned}\n\t&\\boldsymbol{E}_i = Norm(\\boldsymbol{A}_i + \\boldsymbol{I}_i), \\\\\n\t&\\boldsymbol{O}_i = ReLU(Linear(\\boldsymbol{E}_i)).\n \\end{aligned}\n\\end{equation}\n\n\\subsection{Low-rank Weight Decomposition}\n\nLow-rank weight decomposition technology achieves parameter reduction and model compression by decomposing the original weight matrix into the product of two or more low-rank matrices \\cite{kolda2009tensor}. Since it can effectively reduce the complexity of the model, low-rank decomposition technology has received extensive research attention on many tasks, e.g., image processing and multi-modal fusion. {Tucker decomposition is a commonly used technique for tensor decomposition, which can decompose a high-order tensor into a smaller core tensor and a set of factor matrices. Unlike other tensor decomposition methods, Tucker decomposition allows the core tensor to maintain complex relationships between multiple modes, thus being able to capture multimodal dependencies in the data.}\n\n\\begin{figure*}\n\t\\centering\n\t\\includegraphics[width=1\\linewidth]{MAM.pdf}\n\t\\caption{Three embedding ways for cross-modal fusion using LMAM module. (a) Embedding the LMAM module before model's feature extraction. (b) Embedding the LMAM module and using the residual connections before model's feature extraction. (c) Embedding the LMAM module after model's feature extraction. (d) The overall flow of the LMAM module. {$\\boldsymbol{Q}_i$ is represented as the query feature vectors, and $\\boldsymbol{I}_i$ is represented as the matching feature vectors.}}\n\t\\label{fig:attention}\n\\end{figure*}\n\n{As shown in Fig. \\ref{fig:low-rank-attention}, the idea behind low-rank decomposition in LMAM is to decompose the {weight $\\boldsymbol{W}$} into specific factors that match the modal features. For any $N$-order weight $\\boldsymbol{W}_i$, there is always a low-rank decomposition method. The formula is defined as follows:}\n\\begin{equation}\n\t\\begin{aligned}\n\t\\widetilde{\\boldsymbol{W}}_i&=Tucker(\\boldsymbol{W}_i) \\\\\n\t&=\\sum_{j=1}^R \\omega_{n, i}^{(j)}, \\omega_{n, i}^{(j)} \\in \\mathbb{R}_n^d\n\t\\end{aligned}\n\\end{equation}\n{where $r$ represents the rank of the weight $W_i$, {$\\left\\{\\left\\{\\omega_{n, i}^{(j)}\\right\\}_{n=1}^\\eta\\right\\}_{j=1}^r$} is a collection of low-rank decomposition factors, and $\\eta$ is the size of the multimodal feature set $\\xi=\\{\\xi_u,\\xi_a,\\xi_v\\}$, i.e., $\\eta=3$. $\\widetilde{\\boldsymbol{W}}_i$ is computed during training.}\n\n{Therefore, Eq. 4 can be calculated as:}\n\\begin{equation}\n\t\\boldsymbol{Q}_i=\\left(\\sum_{j=1}^r \\omega_{n, i}^{(j)}\\right)\\boldsymbol{M}_i+\\boldsymbol{b}^{\\boldsymbol{Q}_i}, \\omega_{n, i}^{(j)} \\in \\mathbb{R}_n^d\n\\end{equation}\n\nThe whole computational process of the LMAM method is shown in Figure~\\ref{figure1}(c) and the pseudocode of the LMAM method is summarized in Algorithm 1.\n\n\\subsection{Comparison to Self-attention}\nStudies \\cite{dosovitskiyimage} have shown that the performance of the self-attention mechanism is lower than CNN, RNN and other methods when the amount of data is small, while its performance can gradually exceed CNN and RNN when the amount of data is very large. The difference in performance may be attributed to  that the self-attention mechanism needs to learn the query vectors $\\boldsymbol{Q}$, the key vectors $\\boldsymbol{K}$, and the value vectors $\\boldsymbol{V}$ at the same time, which makes the optimization of the model more difficult. Unlike classic self-attention, LMAM only needs a very low rank weight to achieve better performance than self-attention. Specifically, we only set a learnable parameter $\\boldsymbol{W^Q}$ for cross-modal feature fusion and capture of complementary semantic information. Furthermore, we perform a parallel low-rank decomposition of $\\boldsymbol{W^Q}$ with modality-specific factors to further reduce the amount of parameters required for $\\boldsymbol{W^Q}$. LMAM can reduce the difficulty of network optimization while maintaining performance.\n\n\\begin{table}[htbp]\n\t{\\caption{The division of training set, validation set and test set of IEMOCAP and MELD data sets.}}\n\t\\renewcommand\\arraystretch{1.2}\n\t\\setlength{\\tabcolsep}{3mm}{\n\t\t{\n\t\t\t\\begin{tabular}{cccc}\n\t\t\t\t\\toprule\n\t\t\t\tDatasets                 & Partition & Utterance Count & Dialogue Count \\\\ \\midrule\n\t\t\t\t\\multirow{2}{*}{IEMOCAP} & train+val & 5810            & 120            \\\\\n\t\t\t\t& test      & 1623            & 31             \\\\\n\t\t\t\t\\multirow{2}{*}{MELD}    & train+val & 11098           & 1153           \\\\\n\t\t\t\t& test      & 2610            & 280  \\\\\n\t\t\t\t\\multirow{2}{*}{CMU-MOSI}    & train+val & 1513           &    62        \\\\\n\t\t\t\t& test      & 686            & 31 \\\\\n\t\t\t\t\\multirow{2}{*}{POM}    & train+val & 700           &    N/A        \\\\\n\t\t\t\t& test      & 203            & N/A\n\t\t\t\t\\\\ \\bottomrule\n\t\\end{tabular}}}\n\\end{table}\n\n\\begin{table*}[!t]\n\t\n\t\\renewcommand\\arraystretch{1.1}\n\t\\caption{Experimental results on IEMOCAP dataset. Methods with $\\ast$ represent the method equipped with our LMAM module without any further changes. The best result in each column is in bold.}\n\t\\label{table1}\n\t\\renewcommand\\arraystretch{1.2}\n\t\\setlength{\\tabcolsep}{3mm}{\n\t\t\\begin{tabular}{l|ccccccc}\n\t\t\t\\hline\n\t\t\t\\multirow{3}{*}{Methods} & \\multicolumn{7}{c}{IEMOCAP}                                                              \\\\ \\cline{2-8}\n\t\t\t& Happy      & Sad        & Neutral    & Angry      & Excited    & Frustrated & Average(w) \\\\ \\cline{2-8}\n\t\t\t& Acc.  F1   & Acc.  F1   & Acc.  F1   & Acc.  F1   & Acc.  F1   & Acc.  F1   & Acc.  F1   \\\\ \\hline\n\t\t\tTextCNN \\cite{2014Convolutional}                      & 27.73  29.81 & 57.14  53.83 & 34.36  40.13 & \\textbf{61.12}  52.47 & 46.11  50.09 & \\textbf{62.94}  55.78 & 48.89  48.08 \\\\\n\t\t\t\\rowcolor{gray!30}\n\t\t\tTextCNN$^\\ast$ & \\textbf{37.24 37.37} & \\textbf{81.41 72.97} & \\textbf{50.57 53.95} & 57.21 \\textbf{62.96} & \\textbf{70.28 63.87} & 59.01 \\textbf{59.16} & \\textbf{60.35 59.34}\\\\ \n\t\t\t$\\Delta$ &  \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$9.51} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$7.56}  &  \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$24.27} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$19.14}  &\\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$16.21} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$13.82}   &\\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$3.91} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$10.49}   &\\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$24.17} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$13.78}  &\n\t\t\t\\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$3.87} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$3.29}  &\\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$11.46} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$11.26}   \\\\ \\hline\n\t\t\t\n\t\t\t\n\t\t\tbc-LSTM \\cite{poria2017context}                 & 29.16  34.49 & 57.14  60.81 & 54.19  51.80 & 57.03  56.75 & 51.17  57.98 &\\textbf{ 67.12}  58.97 & 55.19  54.96 \\\\\n\t\t\tbc-LSTM+Att \\cite{poria2017context} & 30.56 \\textbf{35.63} & 56.73 62.09 & 57.55 53.00 & {59.41} \\textbf{59.24} & 52.84 58.85 & 65.88 \\textbf{59.41} & 56.31 56.08  \\\\\n\t\t\t\\rowcolor{gray!30}\n\t\t\tbc-LSTM$^\\ast$ & \\textbf{86.67} 31.52 & \\textbf{66.43 73.08} & \\textbf{63.09 55.54} & \\textbf{81.51} 49.79 & \\textbf{68.08 77.01} & 50.39 59.31 & \\textbf{65.55 60.29}\\\\ \n\t\t\t$\\Delta$ &  \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$56.11} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$4.11}  & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$9.7} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$10.99}   & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$5.54} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$2.54}   & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$22.1} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$9.45}   & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$15.24} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$18.16}  & \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$15.49} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$0.1}  & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$10.36} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$5.33} \\\\ \\hline\n\t\t\t\n\t\t\t\n\t\t\tDialogueRNN   \\cite{majumder2019dialoguernn}           & 25.63  33.11 & \\textbf{75.14  78.85} & 58.56  {59.24} & 64.76 \\textbf{65.23} & \\textbf{80.27}  71.85 & \\textbf{61.16}  58.97 & 63.40  62.77 \\\\\n\t\t\t\\rowcolor{gray!30}\n\t\t\tDialogueRNN$^\\ast$ & \\textbf{63.51 66.43} & 72.43 78.64 & \\textbf{62.94 59.53} & \\textbf{78.89} 55.04 & 77.00 \\textbf{77.65} & 55.07 \\textbf{59.07} & \\textbf{66.84 65.79}\\\\ \n\t\t\t$\\Delta$ & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$37.88} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$33.32}   & \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$2.71} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$0.21}   & \\textcolor[rgb]{0.2,0.2,0.2}{$\\uparrow$4.38} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.29}   &  \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$14.13} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$10.19}  & \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$3.27} \\textcolor[rgb]{0.2,0.2,0.2}{$\\uparrow$5.8}  & \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$6.09} \\textcolor[rgb]{0.2,0.2,0.2}{$\\uparrow$0.1}  & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$3.44} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$3.02}  \\\\  \\hline\n\t\t\t\n\t\t\t\n\t\t\tDialogueGCN   \\cite{ghosal2019dialoguegcn}           & 40.63  42.71 & \\textbf{89.14  84.45} & 61.97  \\textbf{63.54} & 67.51  \\textbf{64.14} & 65.46  63.08 & \\textbf{64.13  66.90} & 65.91  65.62 \\\\\n\t\t\t\\rowcolor{gray!30}\n\t\t\tDialogueGCN$^\\ast$ & \\textbf{62.60 59.69} & 76.86 78.98 & \\textbf{62.82} 60.06 & \\textbf{74.40} 63.48 & \\textbf{72.38 77.93} & 56.40 58.30 & \\textbf{66.39 66.12}\\\\ \n\t\t\t$\\Delta$ & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$21.97} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$16.98}   & \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$12.28} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$5.47}   &  \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.85} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$3.48}  &  \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$6.89} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$0.66}  & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$6.92} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$14.85}  & \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$7.73} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$8.6}  & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.48} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.51}  \\\\  \\hline\n\t\t\t\n\t\t\tMM-DFN \\cite{hu2022mm}& {40.17 42.22} & \\textbf{74.27} 78.98 & \\textbf{69.13 66.42} & {70.25} \\textbf{69.97} & 76.99 75.56 & \\textbf{68.58 66.33} & {68.77 68.20}\\\\\n\t\t\t\\rowcolor{gray!30}\n\t\t\tMM-DFN$^\\ast$ & \\textbf{74.58 69.57} & 74.01 \\textbf{81.03} & 64.93 63.71 & \\textbf{73.91} 66.67 & \\textbf{81.40 80.00} & 59.84 61.71 & \\textbf{69.94 69.69}\\\\ \n\t\t\t$\\Delta$ &  \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$34.41} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$27.35}  &  \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$0.26} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$2.05}  &  \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$4.2} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$2.71}  &  \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$3.66} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$3.3}  & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$4.41} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$4.44}  & \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$8.74} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$4.62}  & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$1.16} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$1.48}  \\\\  \\hline\n\t\t\t\n\t\t\tM2FNet \\cite{chudasama2022m2fnet}& {65.92 60.00} & \\textbf{79.18 82.11} & 65.80 \\textbf{65.88} & \\textbf{75.37 68.21} & {74.84 72.60} & \\textbf{59.87 68.31} & {69.11 69.86}\\\\\n\t\t\t\\rowcolor{gray!30}\n\t\t\tM2FNet$^\\ast$ & \\textbf{73.35 69.53} & 74.37 81.42 & \\textbf{67.26} 63.51 & {72.86} 66.23 & \\textbf{81.57 81.29} & 59.64 62.50 & \\textbf{70.31 70.07}\\\\ \n\t\t\t$\\Delta$ & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$7.43} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$9.53}  &  \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$4.81} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$0.69}  & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$1.46} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$2.37}   & \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$2.51} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$1.98}   & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$6.73} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$8.69}  & \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$0.23} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$5.81}  & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$1.20} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.21} \\\\   \\hline\n\t\t\t\n\t\t\tEmoCaps \\cite{li2022emocaps}& {\\textbf{70.34}} {72.86} & 77.39 82.45 & 64.27 65.10 & 71.79 \\textbf{69.14} & 84.50 73.90 &\\textbf{63.94} 63.41 & {71.22 70.06} \\\\\n\t\t\t\\rowcolor{gray!30}\n\t\t\tEmoCaps$^\\ast$ & {69.93} {\\textbf{74.31}} & \\textbf{82.52 85.47} & \\textbf{68.41 67.03} & \\textbf{79.49} 65.26 & \\textbf{84.85 80.14} & 63.33 \\textbf{68.38} & \\textbf{73.67 73.01}\\\\ \n\t\t\t$\\Delta$ &  \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$0.41} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$1.45}  &  \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$5.13} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$3.02}  &  \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$4.14} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$1.93}  & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$7.7} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$3.88}   & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.35} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$6.24}  & \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$0.61} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$4.97}  & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$2.45} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$2.95} \\\\   \\hline\n\t\\end{tabular}}\n\\end{table*}\n\n\\begin{table*}[!t]\n\t\n\t\\caption{Experimental results on MELD dataset. Methods with $\\ast$ represent the method equipped with our LMAM module without any further changes. The best result in each column is in bold.}\n\t\\label{table2}\n\t\\renewcommand\\arraystretch{1.2}\n\t\\setlength{\\tabcolsep}{7.7pt}{\n\t\t\\begin{tabular}{l|cccccccc}\n\t\t\t\\hline\n\t\t\t\\multirow{3}{*}{Methods} & \\multicolumn{8}{c}{MELD}                                                                            \\\\ \\cline{2-9}\n\t\t\t& Neutral     & Surprise    & Fear     & Sadness    & Joy        & Disgust  & Anger      & Average(w) \\\\ \\cline{2-9}\n\t\t\t& Acc.  F1    & Acc.  F1    & Acc.  F1 & Acc.  F1   & Acc.  F1   & Acc.  F1 & Acc.  F1   & Acc. F1   \\\\ \\hline\n\t\t\tTextCNN \\cite{2014Convolutional}                      & \\textbf{71.23}  74.91  & \\textbf{43.35  45.51}  & \\textbf{4.63  3.71} & 18.25  21.17 & 46.14  49.47 & \\textbf{8.91  8.36} & 35.33  34.51 & 52.48 55.09 \\\\\n\t\t\t\\rowcolor{gray!30}\n\t\t\tTextCNN$^\\ast$ & 70.23 {\\textbf{75.79}} & {36.47 44.78} & 0.00 0.00 & \\textbf{24.19 21.19} & \\textbf{50.58 52.46} & 0.00 0.00 & \\textbf{43.43 41.68} & \\textbf{53.17 56.55} \\\\ \n\t\t\t\n\t\t\t$\\Delta$ & \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$1.00} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.88} & \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$6.88} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$0.73} & \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$4.63} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$3.71} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$5.94} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.02} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$4.44} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$2.99} & \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$8.91} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$8.36}  &  \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$8.10} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$7.17} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.69} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$1.47} \\\\ \\hline\n\t\t\t\n\t\t\tbc-LSTM    \\cite{poria2017context}              & \\textbf{71.45}   73.84 & 46.82   \\textbf{47.71} & \\textbf{3.84  5.46} & 22.47  \\textbf{25.19} & 51.61  {51.34} & \\textbf{4.31  5.23} & 36.71  38.44 & 54.18  55.89 \\\\\n\t\t\tbc-LSTM+Att & 70.45 {\\textbf{75.55}} & 46.43 46.35 & 0.00 0.00 &21.77 16.27 & 49.30 50.72 & 0.00 0.00 & 41.77 40.71 & 53.73 55.82 \\\\\n\t\t\t\\rowcolor{gray!30}\n\t\t\tbc-LSTM$^\\ast$ & 70.78 {75.46} & \\textbf{47.18} 46.47 & 0.00 0.00 & \\textbf{26.09} 24.58 & \\textbf{52.33 53.11} & 0.00 0.00 & \\textbf{43.23 40.92} & \\textbf{54.97 56.85}\\\\ \n\t\t\t$\\Delta$ & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.33} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$0.09} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.75} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.12} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.00} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.00} &  \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$4.32} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$8.31} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$3.03} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$2.39} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.00} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.00} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$1.46} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.21} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$1.24} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$1.03}  \\\\ \\hline\n\t\t\t\n\t\t\tDialogueRNN  \\cite{majumder2019dialoguernn}            & \\textbf{72.12}   73.54 & \\textbf{46.42  49.47}  & {1.61  1.23} & 23.97  \\textbf{23.83} & \\textbf{52.01}  50.74 & {1.52  1.73} & 41.01  {41.54} & 55.10  55.97 \\\\\n\t\t\t\\rowcolor{gray!30}\n\t\t\tDialogueRNN$^\\ast$ & 71.74 \\textbf{75.76} & 45.83 48.23 & \\textbf{3.13 2.77} & \\textbf{31.71} 17.93 & 49.25 \\textbf{53.04} & \\textbf{2.01 2.58} & \\textbf{42.40} {\\textbf{42.21}} & \\textbf{55.27 56.93}\\\\ \n\t\t\t$\\Delta$ & \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$0.38} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$2.22} &  \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$0.59} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$1.24} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$1.52} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$1.54}  & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$7.74} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$5.90} & \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$2.76} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$2.30} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.49} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.85} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$1.39} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.67} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.17} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.96}  \\\\ \\hline\n\t\t\t\n\t\t\tDialogueGCN    \\cite{ghosal2019dialoguegcn}            & 75.61   77.45       & 51.32  52.76        & \\textbf{5.14  10.09}     & 30.91  32.56       & {\\textbf{54.31  56.08}}       & \\textbf{11.62}  \\textbf{11.27}     & 42.51  44.65       & 58.74 60.55 \\\\\n\t\t\t\\rowcolor{gray!30}\n\t\t\tDialogueGCN$^\\ast$ & \\textbf{78.19 77.82} & \\textbf{52.27 54.11}  & 2.17 2.31 & \\textbf{35.79 36.43} & {54.15 55.07} & 4.05 {2.12} & \\textbf{48.31 47.22} & \\textbf{60.96 60.98}\\\\ \n\t\t\t$\\Delta$ & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$2.58} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.37} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.95} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$1.35} & \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$2.97} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$7.78} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$4.88} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$3.87} & \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$0.16} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$1.01} & \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$7.57} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$9.15} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$5.80} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$2.57} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$2.22} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.43}  \\\\ \\hline\n\t\t\t\n\t\t\tMM-DFN  \\cite{hu2022mm}              & \\textbf{78.17 77.76}       & 52.15 50.69        & {0.00  0.00}     & 25.77  22.93       & \\textbf{56.19  54.78}       & 0.00 0.00     & \\textbf{48.31  47.82  }     & 60.30  59.44 \\\\\n\t\t\t\\rowcolor{gray!30}\n\t\t\tMM-DFN$^\\ast$ & {77.08 76.56} & \\textbf{53.79 56.84}  & \\textbf{2.07 4.11} & \\textbf{38.10 31.92} & {53.63 50.53} & \\textbf{4.23 7.10} & {47.99 46.08} & \\textbf{60.65 59.62}\\\\ \n\t\t\t$\\Delta$ & \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$1.09} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$1.20} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$1.64} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$6.15}  & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$2.07} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$4.11} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$12.33} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$8.99} & \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$2.56} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$4.25} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$4.23} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$7.10} & \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$0.32} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$1.74}  & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.35} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.18} \\\\ \\hline\n\t\t\t\n\t\t\tM2FNet \\cite{chudasama2022m2fnet} & \\textbf{68.88 67.98} & 72.76 58.66 & 5.57 3.45 & 50.09 \\textbf{47.03} & {68.49 65.50} & \\textbf{17.69 25.24} &  57.33 55.25&63.64 60.87\\\\\n\t\t\t\\rowcolor{gray!30}\n\t\t\tM2FNet$^\\ast$ & {68.40 67.27} & \\textbf{73.15 60.37} & \\textbf{9.13 11.25} & \\textbf{51.77} 46.68 & \\textbf{69.11 65.92} & 15.19 17.62 & \\textbf{60.76 57.31} & \\textbf{64.13 60.97}\\\\ \n\t\t\t$\\Delta$ & \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$4.48} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$0.71} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.39} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$1.71} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$3.56} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$7.80} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$1.68} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$0.35} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.62} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.42} & \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$2.50} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$7.62} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$3.43} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$2.06} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.50} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.10}  \\\\ \\hline\n\t\t\t\n\t\t\tEmoCaps \\cite{li2022emocaps} & 75.24 {\\textbf{75.12}} & {63.57 63.19} & \\textbf{3.45 3.03} & {\\textbf{43.78 42.52}} & 58.34 57.05 & {\\textbf{7.01 7.69}} & 58.79 57.54 & 63.52 62.97 \\\\\n\t\t\t\\rowcolor{gray!30}\n\t\t\tEmoCaps$\\ast$ & \\textbf{76.37}  74.28 & {\\textbf{66.57  64.74}}  & 3.11  2.14 & {40.17  42.35} & \\textbf{63.33  62.52} & {6.21  7.05} & \\textbf{59.45  60.26} & \\textbf{64.93  63.88}\\\\ \n\t\t\t$\\Delta$ & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$1.13} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$0.84} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$3.00} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$1.55} & \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$0.34} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$0.89} & \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$3.61} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$0.17} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$4.99} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$5.47} & \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$0.80} \\textcolor[rgb]{0.2,0.2,0.2}{$\\downarrow$0.84} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.66} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$2.72} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$1.41} \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.97}  \\\\ \\hline\n\t\\end{tabular}}\n\\end{table*}\n\n\\begin{table*}[htbp]\n\t\\centering\n\t\n\t\\caption{Experimental results on CMU-MOSI, and POM dataset. Methods with $\\ast$ represent the method equipped with our LMAM module without any further changes. The best result in each column is in bold.}\n\t\\label{table12}\n\t\\renewcommand\\arraystretch{1.2}\n\t\\setlength{\\tabcolsep}{12pt}{\n\t\t\\begin{tabular}{c|ccccc|ccc}\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{Methods}         & \\multicolumn{5}{c}{CMU-MOSI}       & \\multicolumn{3}{c}{POM} \\\\ \\cline{2-9} \n\t\t\t& Acc-2 ($\\uparrow$) & F1 ($\\uparrow$)  & MAE ($\\downarrow$)  & Corr ($\\uparrow$) & Acc-7 ($\\uparrow$) & MAE ($\\downarrow$)   & Acc ($\\uparrow$)   & Corr ($\\uparrow$) \\\\ \\hline\n\t\t\t\\multicolumn{1}{c|}{TextCNN \\cite{2014Convolutional}}     & 62.2  & 62.0 & 1.42 & 0.41 & 23.3  & 0.88   & 34.0   & 0.13  \\\\\n\t\t\t\\rowcolor{gray!30}\n\t\t\t\\multicolumn{1}{c|}{TextCNN$^\\ast$}     & \\textbf{69.7}  & \\textbf{69.6} & \\textbf{1.31} & \\textbf{0.49} & \\textbf{24.6}  & \\textbf{0.87}   & \\textbf{34.0}   & \\textbf{0.21}  \\\\ \n\t\t\t$\\Delta$ & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$7.5} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$7.6}  &  \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.11} &  \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.08} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$1.3}  & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.01}  & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.0} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.08} \\\\ \\hline\n\t\t\t\n\t\t\t\\multicolumn{1}{c|}{bc-LSTM \\cite{poria2017context}}     & 73.9  & 73.9 & 1.08 & 0.61 & 28.7  & 0.84   & 34.8   & 0.28  \\\\\n\t\t\t\\rowcolor{gray!30}\n\t\t\t\\multicolumn{1}{c|}{bc-LSTM+Att} & 75.2  & 75.2 & 1.05 & 0.62 & 34.6  & 0.82   & 35.3   & 0.30  \\\\\n\t\t\t\n\t\t\t\\multicolumn{1}{c|}{bc-LSTM$^\\ast$}     & \\textbf{77.0}  & \\textbf{77.1} & \\textbf{0.96} & \\textbf{0.64} & \\textbf{34.7}  & \\textbf{0.80}   & \\textbf{39.1}   & \\textbf{0.35}  \\\\ \n\t\t\t$\\Delta$ & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$1.8} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$1.9}  & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.09}  &  \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.02} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.1}  & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.02}  & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$3.8} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.05} \\\\ \\hline\n\t\t\t\n\t\t\t\\multicolumn{1}{c|}{DialogueRNN \\cite{majumder2019dialoguernn}} & 75.7  & 75.8 & 1.02 & 0.62 & 34.6  & 0.81   & 36.2   & 0.32  \\\\\n\t\t\t\\rowcolor{gray!30}\n\t\t\t\\multicolumn{1}{c|}{DialogueRNN$^\\ast$} & \\textbf{77.4}  & \\textbf{77.0} & \\textbf{1.00} & \\textbf{0.67} & \\textbf{34.9}  & \\textbf{0.77}   & \\textbf{39.3}   & \\textbf{0.37}  \\\\ \n\t\t\t$\\Delta$ & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$1.7} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$1.2}  & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.02}  &  \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.05} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.3} &\\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.04}  & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$3.1} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.05}\\\\ \\hline\n\t\t\t\n\t\t\t\\multicolumn{1}{c|}{DialogueGCN \\cite{ghosal2019dialoguegcn}} & 74.3  & 74.0 & 1.06 & 0.61 & 29.6  & 0.83   & 35.0   & 0.29  \\\\\n\t\t\t\\rowcolor{gray!30}\n\t\t\t\\multicolumn{1}{c|}{DialogueGCN$^\\ast$} & \\textbf{74.9}  & \\textbf{74.7} & \\textbf{1.04} & \\textbf{0.62} & \\textbf{31.8}  & \\textbf{0.79}   & \\textbf{36.7}   & \\textbf{0.35}  \\\\ \n\t\t\t$\\Delta$ & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.6}  & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.7}  &  \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.02} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.01}  & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$2.2} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.04}  & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$1.7} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.06} \\\\ \\hline\n\t\t\t\n\t\t\t\\multicolumn{1}{c|}{MM-DFN \\cite{hu2022mm}}      & 76.7  & 76.7 & 0.91 & 0.68 & 33.3  & 0.79   & 42.5   & 0.41  \\\\\n\t\t\t\\rowcolor{gray!30}\n\t\t\t\\multicolumn{1}{c|}{MM-DFN$^\\ast$}      & \\textbf{77.1}  & \\textbf{77.3} & \\textbf{0.89} & \\textbf{0.71} & \\textbf{33.9}  & \\textbf{0.76}  &\\textbf{43.1}   & \\textbf{0.41}  \\\\ \n\t\t\t$\\Delta$ & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.4} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.6}  & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.02}  & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.03}  & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.6} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.03}  & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.6} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.0} \\\\ \\hline\n\t\t\t\n\t\t\t\\multicolumn{1}{c|}{M2FNet \\cite{chudasama2022m2fnet}}      & 78.9  & 78.5 & 0.87 & 0.73 & 34.6  & 0.75   & 43.4   & 0.41  \\\\\n\t\t\t\\rowcolor{gray!30}\n\t\t\t\\multicolumn{1}{c|}{M2FNet$^\\ast$}      & \\textbf{79.6}  & \\textbf{79.9} & \\textbf{0.87} & \\textbf{0.75} & \\textbf{35.5}  & \\textbf{0.73}   & \\textbf{43.6 }  & \\textbf{0.43}  \\\\ \n\t\t\t$\\Delta$ & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.7} &  \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$1.4} &  \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.02}  & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.9} &  \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.02} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.2} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.02} \\\\ \\hline\n\t\t\t\n\t\t\t\\multicolumn{1}{c|}{EmoCaps \\cite{li2022emocaps}}     & 80.2  & 80.3 & 0.86 & 0.78 & 36.7  & 0.71   & 43.7   & 0.43  \\\\\n\t\t\t\\rowcolor{gray!30}\n\t\t\t\\multicolumn{1}{c|}{EmoCaps$^\\ast$}                          & \\textbf{82.7}  & \\textbf{82.9} & \\textbf{0.81} & \\textbf{0.89} & \\textbf{38.2}  & \\textbf{0.63}   & \\textbf{44.9}   & \\textbf{0.47}  \\\\ $\\Delta$ & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$2.5} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$2.6}  & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.05}  & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.11}  & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$1.5} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.08}  & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$1.2} & \\textcolor[rgb]{0.0,0.6,0.0}{$\\uparrow$0.04} \\\\ \\hline\n\t\\end{tabular}}\n\\end{table*}\n\n\\subsection{Network Architecture of LMAM Module}\nIn this section, we design a network to implement the LMAM method. The overall network architecture of the LMAM module is illustrated in Figure~\\ref{fig:attention}(d). From Figure~\\ref{fig:attention}(d), we can observe that the LMAM module first receives three modal data as input, and then {generates} two types of vectors (i.e., query feature vector and matched feature vector) by a linear transformation layer. Subsequently, we compute the {attention} score based on these feature vectors. Finally, we {generate} the final fusion feature by conducting cross-modal feature fusion followed by a fine-tuning step.\n\nAs shown in Figure \\ref{fig:attention}, there are three ways to use the proposed LMAM module, i.e., early fusion, early fusion with residual connections, and late fusion. For early fusion, we concatenate the three modalities and then input them into the LMAM module for feature fusion. For early fusion with residual connections, the concatenated features vectors of our three modalities are added to the feature vectors after feature fusion through the LMAM module. For late fusion, we extract the contextual semantic information from the model (e.g., EmoCaps) and then input it to the LMAM module for feature fusion.\nIt should be noted that the selection of the LMAM fusion ways depends on the baseline model itself. In the following experiment, we mainly adopt the latter two ways of fusion, i.e., early fusion with residual connections framework and late fusion.\n\n\\section{Experiments}\nIn this section, we conduct several experiments to verify the effectiveness of our proposed LMAM cross-modal fusion method. Specifically, the overall experimental setting is shown as follows. Firstly, we choose seven state-of-the-art DL-based approaches, including TextCNN~\\cite{2014Convolutional}, bc-LSTM~\\cite{poria2017context}, DialogueRNN~\\cite{majumder2019dialoguernn}, DialogueGCN ~\\cite{ghosal2019dialoguegcn}, MM-DFN\\cite{hu2022mm}, M2FNet\\cite{chudasama2022m2fnet}, and EmoCaps~\\cite{li2022emocaps}, as backbones and embed the proposed LMAM fusion method into these approaches. {In particular, TextCNN uses CNN, 3D-CNN and openSMILE to extract text, video and audio features respectively and input the obtained multi-modal data into 1D-CNN to complete emotion classification.} Secondly, we compare our proposed LMAM method with four popular cross-modal fusion methods, including classical add operation and concatenate operation, and latest low-rank multi-modal fusion (LFM)~\\cite{Liu2018EfficientLM}, tensor fusion network (TFN)~\\cite{zadeh2017tensor}, {Dual Low-Rank Multimodal Fusion (Dual-LMF) \\cite{jin-etal-2020-dual}, Low-Rank Multimodal Fusion with Self-Attention (Att-LMF) \\cite{zhu2020multimodal}. and Low-Rank Multimodal Fusion with multi-modal Transformer (LMF-MulT) \\cite{sahay2020low}.} Thirdly, we conduct an ablation study to verify the necessity of considering the multi-modal datasets. Finally, we apply the proposed LMAM method to other multi-modal recognition tasks. All the experiments are conducted on a PC with Intel Core i7-8700K CPU, and one GeForce RTX 3090 with 24GB memory.\n\n\\subsection{Datasets and Evaluation Metrics}\n{The IEMOCAP~\\cite{busso2008iemocap}, MELD~\\cite{poria2018meld} CMU-MOSI \\cite{zadeh2016mosi}, and POM \\cite{park2014computational} datasets are widely used for conversational emotion recognition.} Therefore, {this paper selects these four benchmark datasets} to verify the effectiveness of our LMAM fusion method. The IEMOCAP dataset contains three modal data to meet the needs of multimodal research, namely video, text, and audio. The IEMOCAP dataset contains 151 dialogues and 7433 utterances of 5 actors and 5 actresses. The emotional labels of the IEMOCAP dataset were annotated by at least three experts, and they divided the labels into six categories, namely ``happy\", ``neutral\", ``sad\", ``excited\", ``angry\" and ``frustrated\". The MELD also includes video, text, and audio three modal data. The MELD dataset contains 13,708 utterances and 1,433 dialogues by multiple actors for 13.7 hours. The emotional labels of the MELD dataset were annotated by at least five experts, and they divided the labels into seven categories, namely ``fear\", ``neutral\", ``angry\", ``joy\", ``sadness\", ``disgust” and ``surprise\". {The CMU-MOSI data set is a multi-modal sentiment analysis data set, including video, audio, text and other data modalities. The CMU-MOSI dataset is annotated with an emotion label in the range [-3, 3]. The POM dataset contains 903 movie review videos, each of which is annotated with 16 speaker features. The speaker in each video is annotated with an emotion label including confidence, enthusiasm, and other characteristics.}\n\nThe IEMOCAP dataset only contains the training set and the test set, so we divide the test set into a test set and a validation set at a ratio of 8$:$2. The MELD dataset includes a training set, a test set, and a validation set. {Similarly, the CMU-MOSI and POM datasets also include a training set, a test set, and a validation set. Two popular metrics are chosen to evaluate the performance of each method on the IEMOCAP and MELD datasets, i.e., classification accuracy and $F1$ score. Five metrics are chosen to evaluate the performance of each method on the CMU-MOSI dataset, i.e., binary classification accuracy (Acc-2),  seven classification accuracy (Acc-7), mean absolute error (MAE), Pearson’s correlation (Corr), and $F1$ score. Three metrics are chosen to evaluate the performance of each method on the POM dataset, i.e., classification accuracy, MAE, and Corr.}\n\n\\begin{table}[htbp]\n\t\\renewcommand\\arraystretch{1.2}\n\t\\setlength{\\tabcolsep}{3mm}{\n\t\t\\caption{The results of the equal parameters experiment on IEMOCAP and MELD datasets. The parameters of methods with $\\diamond$ are incremented to be the same as methods with $\\ast$. The best result in each column is in bold.}\n\t\t\\label{table3}\n\t\t\\begin{tabular}{l|ccccc}\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{Method} & \\multicolumn{1}{c}{\\multirow{2}{*}{Params}} & \\multicolumn{2}{c}{IEMOCAP}                           & \\multicolumn{2}{c}{MELD}                              \\\\ \\cline{3-6}\n\t\t\t& \\multicolumn{1}{c}{}                       & \\multicolumn{1}{c}{Acc.}  & \\multicolumn{1}{c}{F1}    & \\multicolumn{1}{c}{Acc.}  & \\multicolumn{1}{c}{F1}    \\\\ \\hline\n\t\t\tbc-LSTM  \\cite{poria2017context}                & 0.53M                                      & 55.19                     & 54.96                     & 54.18                    & 55.89                    \\\\\n\t\t\tbc-LSTM$^\\diamond$                 & 1.15M                                      & 54.37                     & 52.19                     & 53.11                     & 50.93                     \\\\\n\t\t\tbc-LSTM$^\\ast$                 & 1.15M                                      & \\textbf{65.55}                     & \\textbf{60.29}                     & \\textbf{54.97}                     & \\textbf{56.85}                     \\\\ \\hline\n\t\t\tMM-DFN \\cite{hu2022mm}                & 2.21M                                      & 68.77                     & 68.20                     & 60.30                     & 59.44                     \\\\\n\t\t\tMM-DFN$^\\diamond$                 & 2.83M                                      & 67.33                     & 67.02                     & 60.01                     & 57.65                     \\\\\n\t\t\tMM-DFN$^\\ast$                 & 2.83M                                      & \\textbf{69.94}                     & \\textbf{69.69}                     & \\textbf{60.65}                     & \\textbf{59.62 }                     \\\\ \\hline\n\t\t\tM2FNet  \\cite{chudasama2022m2fnet}               & 8.47M                                      & 69.11                     & 69.86                     & 63.64                     & 60.87                     \\\\\n\t\t\tM2FNet$^\\diamond$                 & 9.09M                                      & 69.46                     & 68.79                     & 62.49                     & 60.91                     \\\\\n\t\t\tM2FNet$^\\ast$                 & 9.09M                                      & \\textbf{70.31}                     & \\textbf{70.07}                     & \\textbf{64.13}                     & \\textbf{60.97}                     \\\\ \\hline\n\t\t\tEmoCaps  \\cite{li2022emocaps}               & \\multicolumn{1}{c}{12.74M}                 & \\multicolumn{1}{c}{71.22} & \\multicolumn{1}{c}{71.06} & \\multicolumn{1}{c}{63.52} & \\multicolumn{1}{c}{62.97} \\\\\n\t\t\tEmoCaps$^\\diamond$                 & 13.36M                                     & 71.18                     & 70.56                     & 62.17                     & 62.11                     \\\\\n\t\t\tEmoCaps$^\\ast$                 & 13.36M                                     & \\textbf{73.67}                     & \\textbf{73.01}                     & \\textbf{64.93}                     & \\textbf{63.88}                     \\\\ \\hline\n\t\\end{tabular}}\n\\end{table}\n\n\\subsection{Performance Verification Experiment}\nTo verify the {effectiveness} of our designed LMAM module, we first test our method in a plug-and-play way by directly embedding the LMAM module into seven state-of-the-art DL-based CER methods. The experimental results are shown in Table~\\ref{table1}, Table~\\ref{table2}, {Table~\\ref{table12}}. From Table~\\ref{table1}, Table~\\ref{table2}, and {Table~\\ref{table12}}, it can be easily seen that all the seven backbones have a significant performance improvement {on the four datasets} after using our proposed LMAM module. The performance improvement may attribute to the full interaction and fusion of different modal information in our proposed LMAM method, while the seven backbone networks only make a simple fusion of cross-modal information and thus neglect some complementary semantic information {between} different modals. Besides, we also compare the emotion recognition results of bc-LSTM$+$Att and bc-LSTM$^\\ast$ (i.e.,bc-LSTM$+$LMAM), and the performance of bc-LSTM$^\\ast$ is significantly better than that of the bc-LSTM$+$Att, which implies that the proposed LMAM module is better than the self attention module.\n\nSince the above experiment embeds our proposed LMAM module into the backbones, thus it will increase the  parameter number of the backbone network. To verify that the performance improvement doesn't come from the increase of model complexity but the reasonable design of our LMAM module, we increase the parameter number of four backbones (i.e., bc-LSTM, MM-DFN, M2FNet, and EmoCaps) to the same as after embedding the LMAM module. The experimental results are shown in Table~\\ref{table3}. It can be observed from Table~\\ref{table3} that the performance of both bc-LSTM, MM-DFN, M2FNet, and EmoCaps methods embedded with the LMAM module are better than the bc-LSTM, MM-DFN, M2FNet, and EmoCaps models with the same parameter number, which proves that the performance improvement is not due to the increase of parameter number but is brought by our LMAM module.\n\n{\\subsection{Parameter Size and Runtime Analysis}}\n\n{To verify that the proposed LMAM is a lightweight plug-and-play module, we analyze the changes in parameter size and running time after adding the LMAM method to the baselines on different emotion recognition datasets. In particular, we perform all experiments under the same experimental configuration to ensure the fairness of the experiments. As shown in Table \\ref{tab:run}, we observe that after adding our proposed LMAM module to the baselines, the running time and model parameter size only increase slightly, but the performance of emotion recognition can be greatly improved. The experimental results show that through appropriate model optimization or architecture improvement, the computational overhead can be effectively controlled while improving the model performance. The performance improvement is attributed to the fact that the proposed LMAM can more effectively realize feature fusion, so that the model can better capture and utilize the complementary information between different modalities, thereby improving the accuracy of emotion recognition. At the same time, the increase in running time and parameter size is small, indicating that the design of the model successfully avoids excessive growth in resource requirements while retaining complexity and depth.}\n\n\\begin{table*}[htbp]\n\t\n\t\\centering\n\t\\renewcommand\\arraystretch{1.2}\n\t\\setlength{\\tabcolsep}{6.5mm}{\n\t\t\\caption{The results of the parameter size and running time experiment on IEMOCAP, MELD, CMU-MOSI, and POM datasets. The parameters of methods with $\\diamond$ are incremented to be the same as methods with $\\ast$.}\n\t\t\\label{tab:run}\n\t\t\\begin{tabular}{l|ccccc}\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{Methods} & \\multirow{2}{*}{Params} & IEMOCAP      & MELD         & CMU-MOSI     & POM          \\\\ \\cline{3-6} \n\t\t\t&                         & Running Time & Running Time & Running Time & Running Time \\\\ \\hline\n\t\t\tTextCNN \\cite{2014Convolutional}                  & 0.23M                   & 0.68s        & 1.15s        & 0.25s        & 0.11s        \\\\\n\t\t\tTextCNN$^\\ast$                  & 0.85M                   & 1.65s        & 2.34s        & 0.32s        & 0.14s        \\\\ \\hline\n\t\t\tbc-LSTM \\cite{poria2017context}                  & 0.53M                   & 1.21s        & 2.17s        & 0.27s        & 0.19s        \\\\\n\t\t\tbc-LSTM+Att                  & 4.7M                    & 4.13s        & 6.63s        & 0.83s        & 0.44s        \\\\\n\t\t\tbc-LSTM$^\\ast$                  & 1.15M                   & 2.18s        & 2.89s        & 0.46s        & 0.25s        \\\\ \\hline\n\t\t\tDialogueRNN  \\cite{majumder2019dialoguernn}            & 14.47M                  & 47.96s       & 76.92s       & 12.56s       & 7.73s        \\\\\n\t\t\tDialogueRNN$^\\ast$              & 15.09M                  & 48.54s       & 78.43s       & 13.17s       & 8.42s        \\\\ \\hline\n\t\t\tDialogueGCN \\cite{ghosal2019dialoguegcn}             & 12.92M                  & 56.18s       & 82.34s       & 17.48s       & 9.32s        \\\\\n\t\t\tDialogueGCN$^\\ast$              & 13.54M                  & 57.77s       & 88.53s       & 18.23s       & 9.74s        \\\\ \\hline\n\t\t\tMM-DFN  \\cite{hu2022mm}                 & 2.21M                   & 3.15s        & 6.17s        & 1.41s        & 0.71s        \\\\\n\t\t\tMM-DFN$^\\ast$                   & 2.83M                   & 3.89s        & 7.04s        & 1.83s        & 0.79s        \\\\ \\hline\n\t\t\tM2FNet \\cite{chudasama2022m2fnet}                  & 8.47M                   & 12.48s       & 25.16s       & 3.39s        & 1.15s        \\\\\n\t\t\tM2FNet$^\\ast$                   & 9.09M                   & 13.11s       & 28.33s       & 3.77s        & 1.82s        \\\\ \\hline\n\t\t\tEmoCaps \\cite{li2022emocaps}                 & 12.74M                  & 16.73s       & 31.07s       & 4.82s        & 2.56s        \\\\\n\t\t\tEmoCaps$^\\ast$                  & 13.36M                  & 17.54s       & 34.35s       & 5.14s        & 3.06s        \\\\ \\hline\n\t\\end{tabular}}\n\\end{table*}\n\n\\begin{table*}[htbp]\n\t\n\t\\caption{Statistically significant results on IEMOCAP dataset. We report P-values for each emotion category under paired t-tests.}\n\t\\label{tab:S2}\n\t\\renewcommand\\arraystretch{1.2}\n\t\\setlength{\\tabcolsep}{15.4pt}{\n\t\t\\begin{tabular}{c|cccccc}\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{Methods} & \\multicolumn{6}{c}{IEMOCAP}                          \\\\ \\cline{2-7} \n\t\t\t& Happy & Sad & Neutral & Angry & Excited & Frustrated \\\\ \\hline\n\t\t\tTextCNN \\cite{2014Convolutional}                 & 9.8 $\\times 10^{-10}$     & 6.0 $\\times 10^{-10}$  &  $6.1 \\times 10^{-7}$       &  $1.1 \\times 10^{-6}$     &   $4.8 \\times 10^{-7}$      &   $ 4.2 \\times 10^{-6}$         \\\\\n\t\t\tbc-LSTM \\cite{poria2017context}                 &  4.8  $\\times 10^{-3}$   &  $6.7 \\times 10^{-4}$   &    $3.1 \\times 10^{-4}$     &   $5.6 \\times 10^{-5}$    &  $1.7 \\times 10^{-3}$      &  $3.9 \\times  10^{-3}$          \\\\\n\t\t\tbc-LSTM+Att                 &     $4.3 \\times 10^{-5}$    &   $8.9 \\times 10^{-8}$       &   $5.5 \\times 10^{-6}$   &   $1.2\\times 10^{-8}$      & 5.9 $\\times 10^{-7}$   &  $3.2 \\times 10^{-3}$             \\\\\n\t\t\tDialogueRNN \\cite{majumder2019dialoguernn}             & 4.9 $\\times 10^{-8}$     &   $3.4 \\times 10^{-2}$  &      $2.9 \\times 10^{-5}$   &    $2.8 \\times 10^{-3}$   &      $6.4 \\times 10^{-8}$   &     $1.7 \\times 10^{-9}$       \\\\\n\t\t\tDialogueGCN  \\cite{ghosal2019dialoguegcn}            & 3.9 $\\times 10^{-7}$      & 5.2 $\\times 10^{-8}$   &      $5.1 \\times 10^{-7}$   &    $1.9 \\times 10^{-6}$   &    $7.3 \\times 10^{-3}$     & 9.4  $\\times 10^{-8}$         \\\\\n\t\t\tMM-DFN \\cite{hu2022mm}                  &  1.0  $\\times 10^{-8}$   &  $4.5 \\times 10^{-5}$   &    $5.8 \\times 10^{-5}$     &  1.1  $\\times 10^{-5}$   &  6.8   $\\times 10^{-7}$    &  4.5  $\\times 10^{-7}$        \\\\\n\t\t\tM2FNet \\cite{chudasama2022m2fnet}                  & 9.3  $\\times 10^{-7}$     & $3.5\\times 10^{-3}$    &     $1.7 \\times 10^{-4}$    &   $4.1 \\times 10^{-4}$    &  5.8   $\\times 10^{-7}$   & 1.1 $\\times 10^{-8}$           \\\\\n\t\t\tEmocaps \\cite{li2022emocaps}                 & 5.1 $\\times 10^{-4}$      & 4.3 $\\times 10^{-5}$    & 2.1 $\\times 10^{-4}$       & 7.2 $\\times 10^{-6}$     &  3.5 $\\times 10^{-8}$      &  2.6   $\\times 10^{-7}$       \\\\ \\hline\n\t\\end{tabular}}\n\\end{table*}\n\n\\begin{table*}[htbp]\n\t\n\t\\caption{Statistically significant results on MELD dataset. We report P-values for each emotion category under paired t-tests.}\n\t\\label{tab:S1}\n\t\\renewcommand\\arraystretch{1.2}\n\t\\setlength{\\tabcolsep}{10.5pt}{\n\t\t\\begin{tabular}{c|ccccccc}\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{Methods} & \\multicolumn{7}{c}{MELD}                                    \\\\ \\cline{2-8} \n\t\t\t& Neutral & Surprise & Fear & Sadness & Joy & Disgust & Anger \\\\ \\hline\n\t\t\tTextCNN  \\cite{2014Convolutional}                &  $1.1 \\times 10^{-8}$       &   $2.9 \\times 10^{-3}$       &  $1.8\\times 10{-6}$    &  $1.4 \\times 10^{-2}$      & $7.6 \\times 10^{-5}$    &  $2.1 \\times 10^{-9}$       &  $8.4 \\times 10^{-9}$     \\\\\n\t\t\tbc-LSTM \\cite{poria2017context}                 &  $2.3 \\times 10^{-4}$       &  $4.3 \\times 10^{-3}$        & $2.5 \\times 10^{-7}$     &   $4.6 \\times 10^{-10}$      &   $7.8 \\times 10^{-4}$  &  $1.1 \\times 10^{-7}$       &  $5.6 \\times 10^{-4}$     \\\\\n\t\t\tbc-LSTM+Att                 &   $2.4 \\times 10^{-4}$      &     $5.1 \\times 10^{-3}$     &   $2.5 \\times 10^{-7}$   &   $1.9\\times 10^{-3}$      &  $6.6 \\times 10^{-4}$   &    $2.7 \\times 10^{-5}$     &   $8.9 \\times 10^{-4}$    \\\\\n\t\t\tDialogueRNN \\cite{majumder2019dialoguernn}             &     $3.7 \\times 10^{-4}$    &   $4.9 \\times 10^{-3}$       &    $2.2 \\times 10^{-3}$  &   $8.7 \\times 10^{-8}$      &   $5.4 \\times 10^{-5}$  &   $2.8 \\times 10^{-3}$      &  $1.5 \\times 10^{-3}$      \\\\\n\t\t\tDialogueGCN  \\cite{ghosal2019dialoguegcn}             &   $1.3 \\times 10^{-2}$      &   $3.4 \\times 10^{-4}$       &   $1.7 \\times 10^{-3}$   &   $6.7 \\times 10^{-6}$      &  $3.7 \\times 10^{-3}$   &     $1.2 \\times 10^{-10}$    &   $5.2 \\times 10^{-5}$    \\\\\n\t\t\tMM-DFN  \\cite{hu2022mm}                 &  $3.7 \\times 10^{-3}$       &   $7.2 \\times 10^{-8}$       &  2.5 $\\times 10^{-6}$   &  8.9   $\\times 10^{-10}$    & 6.6 $\\times 10^{-6}$   & 9.1    $\\times 10^{-9}$    &  $4.7 \\times 10^{-3}$     \\\\\n\t\t\tM2FNet \\cite{chudasama2022m2fnet}                   &   $1.4 \\times 10^{-2}$      &  6.9   $\\times 10^{-4}$     &  7.6  $\\times 10^{-10}$  & $3.0 \\times 10^{-3}$        & 2.9  $\\times 10^{-2}$  &    $9.2 \\times 10^{-10}$     &  $6.1 \\times 10^{-4}$     \\\\\n\t\t\tEmocaps\\cite{li2022emocaps}                  &    $6.9 \\times 10^{-5}$     &  1.7   $\\times 10^{-3}$     & 4.4 $\\times 10^{-3}$     & 4.7 $\\times 10^{-2}$       & 8.4  $\\times 10^{-7}$   &   $3.9 \\times 10^{-2}$      &  $5.7 \\times 10^{-5}$     \\\\ \\hline\n\t\\end{tabular}}\n\\end{table*}\n\n{\\subsection{Statistical Significance}}\n{In this section, to further verify the significance level between the LMAM method proposed in this paper and the baseline method, we conducted a paired t-test to ensure the reliability of our experimental conclusions. The significance level results are shown in Tables \\ref{tab:S1} and \\ref{tab:S2}. We run the baseline model and the baseline model with the LMAM module 5 times and obtained the corresponding F1 value as an evaluation metric of significance statistics. All emotion categories on IEMOCAP and MELD datasets are statistically significant in F1 values under paired t-test ($p<0.05$).} \n\n\\subsection{Comparison with Other Cross-modal Fusion Methods}\n{In this section, to further verify the superiority of our proposed LMAM module, we also conduct an experiment to compare our LMAM module with other four typical cross-modal fusion approaches, i.e., classical add operation and concatenate operation, and latest low-rank multi-modal fusion (LFM)~\\cite{Liu2018EfficientLM}, tensor fusion network (TFN)~\\cite{zadeh2017tensor}, {Dual Low-Rank Multimodal Fusion (Dual-LMF) \\cite{jin-etal-2020-dual}, Low-Rank Multimodal Fusion with Self-Attention (Att-LMF) \\cite{zhu2020multimodal}, Low-Rank Multimodal Fusion with multi-modal Transformer (LMF-MulT) \\cite{sahay2020low}, AuxFormer \\cite{goncalves2022auxformer}, Cross-modality Context fusion and Semantic Refinement Network (CMCF-SRNet) \\cite{zhang2023cross}, and Low-Rank Adaption (LoRA) \\cite{hulora}.} The selected backbone network is EmoCaps~\\cite{li2022emocaps} and the used datasets are also IEMOCAP and MELD.}\n\n\\begin{table}[H]\n\t\\centering\n\t\\renewcommand\\arraystretch{1.2}\n\t\\setlength{\\tabcolsep}{4.8mm}{\n\t\t\\caption{{The comparison results of our LMAM fusion method with other cross-modal methods. The best result is highlighted in bold. $^\\diamond$ represents randomly initialized pre-training weights, and $^\\ast$ using the pre-trained weights of EmoCaps.}}\n\t\t\\label{table4}\n\t\t\\begin{tabular}{c|cccc}\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{Methods} & \\multicolumn{2}{c}{IEMOCAP} & \\multicolumn{2}{c}{MELD} \\\\ \\cline{2-5}\n\t\t\t& Acc.       & F1              & Acc.         & F1         \\\\ \\hline\n\t\t\tAdd {\\cite{lian2021ctnet}}                      & 71.3        & 71.0       & 64.3          & 64.0           \\\\\n\t\t\tConcatenate  \\cite{cambria2018benchmarking}              & 68.9        & 68.1                & 62.6          & 61.1           \\\\\n\t\t\tTFN \\cite{zadeh2017tensor} & 70.8        &  70.1               & 63.0          & 62.8           \\\\\n\t\t\tLFM \\cite{Liu2018EfficientLM} & 72.3 & 71.7 & 64.1 & 62.3\\\\\n\t\t\t{Dual-LMF \\cite{jin-etal-2020-dual}} & {72.6} & {72.0} & \n\t\t\t{64.8} & \n\t\t\t{63.2}\\\\\n\t\t\t\n\t\t\t{Att-LFM \\cite{zhu2020multimodal}} & \n\t\t\t{72.5} & \n\t\t\t{71.3} & \n\t\t\t{64.3} & \n\t\t\t{62.9}\\\\\n\t\t\t\n\t\t\t{LMF-MulT \\cite{sahay2020low}} & \n\t\t\t{72.5} & {71.9} & {64.6} & {62.5}\\\\\n\t\t\t\n\t\t\t{AuxFormer \\cite{goncalves2022auxformer}}  &   {71.6}   &   {72.1}   &  {63.9}   & {63.0}   \\\\\n\t\t\t\n\t\t\t{CMCF-SRNet \\cite{zhang2023cross}}   & {72.3}    &  {71.4}   & {62.3}    & {63.3}  \\\\\n\t\t\t\n\t\t\t{LoRA$^\\diamond$} \\cite{hulora}  & {60.7}        & {58.3}                & {55.6}          & {57.5}          \\\\\n\t\t\t\n\t\t\t{LoRA$^\\ast$} \\cite{hulora}  & {73.3}        & {73.0}                & {64.4}          & {63.5}          \\\\\n\t\t\t\n\t\t\t{LMAM (Ours)} & \\textbf{73.7}        & {\\textbf{73.0}}                & {\\textbf{64.9} }         & {\\textbf{63.9}   }        \\\\ \\hline\n\t\\end{tabular}}\n\\end{table}\n\n{The experimental results are recorded in Table~\\ref{table4}. As shown in the Table~\\ref{table4}, the LMAM method achieves the best experimental results on the IEMOCAP and MELD datasets, with Acc of 73.0\\% and 65.4\\%, respectively, and F1 values of 73.0\\% and 64.9\\%, respectively. Specifically, compared with the Add method, the Acc and F1 values of the LMAM method on the IEMOCAP dataset are increased by 1.7\\% and 2.0\\%, respectively, and the Acc and F1 values on the MELD dataset are increased by 1.1\\% and 0.9\\%, respectively. Compared with the Concatenate method, the Acc and F1 values of the LMAM method on the IEMOCAP dataset are increased by 4.1\\% and 4.9\\%, respectively, and the Acc and F1 values on the MELD dataset are increased by 2.8\\% and 3.8\\%, respectively. We think this is because the Add method and the Concatenate method do not model complementary semantic information within and between modalities. Additionally, compared with the TFN, LFM, Dual-LMF, Att-LMF, LMF-MulT, AuxFormer and CMCF-SRNet methods, the LMAM method has also achieved better performance in the accuracy and F1 value of emotion recognition, which further illustrates the superiority of our designed LMAM fusion method. Moreover, to further illustrate the effectiveness of our proposed LMAM method, we also compare it with LoRA. Specifically, we divide LoRA into two versions, one with randomly initialized pre-trained weights and the other with pre-trained weights using EmoCaps. Experimental results show that LoRA using EmoCaps' pre-trained weights performs similarly to our proposed LMAM method, but the performance of LoRA is very poor when the pre-trained weights are randomly initialized. We believe this is because the pre-trained weights of LoRA are frozen, so it is very dependent on the prior information of the pre-trained weights. Although the LMAM method we proposed randomly initializes the weights, since our method is end-to-end and the weights is learnable, it can learn weights with better generalization performance.}\n\n\\begin{table}[htbp]\n\t\\renewcommand\\arraystretch{1.2}\n\t\\setlength{\\tabcolsep}{5.5mm}{\n\t\t\\caption{Experimental results of using single-modal data and multi-modal data on IEMOCAP and MELD datasets. T, A, and V represent text, audio, and video, respectively.}\n\t\t\\label{table5}\n\t\t\\begin{tabular}{c|cccc}\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{Modality} & \\multicolumn{2}{c}{IEMOCAP} & \\multicolumn{2}{c}{MELD} \\\\ \\cline{2-5}\n\t\t\t& Acc.       & F1              & Acc.         & F1         \\\\ \\hline\n\t\t\tT                      & 70.8        & 69.2       & 63.1          & 61.8           \\\\\n\t\t\tA              & 66.0        & 65.3               & 60.4          & 59.2           \\\\\n\t\t\tV & 58.8 & 56.8 & 53.9 & 50.7\\\\\n\t\t\t{T+A}  & {72.3}  &  {71.5}  & {64.4}   & {62.7}  \\\\\n\t\t\t{T+V}  & {70.8}  & {69.6}   &  {63.5}  & {62.1}  \\\\\n\t\t\t{V+A}  & {66.7}  & {65.8}   & {61.0}   & {59.9}   \\\\\n\t\t\tT+A+V & \\textbf{73.7}        & \\textbf{73.0}                & \\textbf{64.9}          & \\textbf{63.9}           \\\\ \\hline\n\t\\end{tabular}}\n\\end{table}\n\n\\subsection{Ablation study}\n\\subsubsection{Necessity of multi-modal data}\nTo illustrate the necessity of multi-modal research, we used the EmoCaps method equipped with LMAM module as the backbone to conduct a comparative experiment of {uni-modal, bi-modal and multi-modal} on the IEMOCAP and MELD datasets. The experimental results are shown in Table \\ref{table5}. We conducted a {uni-modal} experiment to utilize only one of the three modalities (i.e., text, video, and audio), {a bi-modal experiment to use any two modalities} and a multi-modal experiment to use all the three modalities. For the {uni-modal} experiments, we found that the features of the text modality performed best for emotion recognition on both datasets, followed by the features of the audio modality, and the worst performance of the features of the video modality. {In the bi-modal experiment, the emotion recognition effect of text + audio is the best, followed by the emotion recognition of text + video, and the emotion recognition of video + audio is the worst.} In the multimodal experiment, we can find that the emotion recognition effect of the combination of the three modalities is the best. Experimental results demonstrate that it is necessary to consider the multi-modal study. Furthermore, designing multi-modal feature fusion methods to improve the effect of emotion recognition is also necessary.\n\n\\subsubsection{Comparison of different embedding ways}\nTo compare the performance of the early fusion and early fusion with residual connections embedding ways introduced in Section 4.2, we conduct comparative experiments using the bc-LSTM, MM-DFN, M2FFNet, and EmoCaps algorithms on the IEMOCAP and MELD datasets. The experimental results are shown in Table {\\ref{tab:fusion}}. As can be seen, the LMAM module with residual connections can obtain better performance compared with the early fusion without {residual} connection.\n\n\\begin{table}[htbp]\n\t\\caption{Methods with $\\ast$ represent the method equipped with our LMAM module without any further changes. Methods with $\\ast$(R) represent the method equipped with our LMAM module with residual connections. The best result is highlighted in bold.}\n\t\\label{tab:fusion}\n\t\\renewcommand\\arraystretch{1.2}\n\t\\setlength{\\tabcolsep}{4.3mm}{\n\t\t\\begin{tabular}{l|cccc}\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{Methods} & \\multicolumn{2}{c}{IEMOCAP} & \\multicolumn{2}{c}{MELD} \\\\ \\cline{2-5}\n\t\t\t& Acc.          & F1          & Acc.         & F1        \\\\ \\hline\n\t\t\tbc-LSTM$\\ast$              &    59.49           &   59.16          &    57.69          &     55.47      \\\\\n\t\t\tbc-LSTM$\\ast$(R)   & \\textbf{61.77}                     & \\textbf{60.49}                     & \\textbf{59.56}                     & \\textbf{57.49}                     \\\\ \\hline\n\t\t\tMM-DFN$\\ast$              &    67.93           &   67.15         &     {63.28}        &  {61.12 }        \\\\\n\t\t\tMM-DFN$\\ast$(R)   & \\textbf{69.82}                     & \\textbf{69.68}                     &   {\\textbf{ 68.02}}                   &         {\\textbf{65.24} }            \\\\ \\hline\n\t\t\tM2FNet$\\ast$              &    68.35           &   57.96          &    67.47          &     66.59      \\\\\n\t\t\tM2FNet$\\ast$(R)   & \\textbf{70.27}                     & \\textbf{70.07}                     & \\textbf{68.34}                     & \\textbf{67.25}                     \\\\ \\hline\n\t\t\tEmoCaps$\\ast$              &       71.49        &    71.01        &        65.03       &    64.22       \\\\\n\t\t\tEmoCaps$\\ast$(R)              & \\textbf{73.67}                     & \\textbf{73.01}                     & \\textbf{64.93}                     & \\textbf{63.88}                     \\\\ \\hline\n\t\\end{tabular}}\n\\end{table}\n\n\\subsection{Complexity Analysis}\nWe assume that the query vectors $W^Q$, key vector  $W^K$, and value vector  $W^V$ in the self-attention mechanism have the same dimensions as the input multimodal feature $d_n$. Theoretically, the computational complexity of the LMAM method proposed in this paper is $O\\left(\\sum_{n=1}^M \\sum_{i=1}^{r(d_n)} ({d_n^{(i)}})^3\\right)$ compared to $O\\left(\\sum_{n=1}^M (d_n)^3\\right)$ of self-attention model. {Furthermore}, we evaluate the computational complexity and computation time of LMAM and self-attention mechanism. As shown in the Table \\ref{compare}, the training time and parameter of the LMAM method are much smaller than the self-attention mechanism. Here, we set the rank size to 45.\n\n\\subsection{Rank Settings}\nWe verified the impact of different rank parameter settings on the accuracy of emotion recognition on the IEMOCAP dataset. The experimental results are shown in the Fig \\ref{fig:rank}. We observe that when $rank=45$, the training effect of the model is the best, and the training effect of the model is stable when the rank is between 30 and 55. When $rank > 45$, the training result of the model becomes unstable and the effect is poor. Therefore, better experimental results can be obtained by using lower ranks. Therefore, better experimental results can be obtained by using lower ranks.\n\n\\begin{table}\n\t\\centering\n\t\\caption{Comparison the training speed and parameters of self-attention mechanism and LMAM method in an epoch. Emocaps is selected as our architecture.}\n\t\\label{compare}\n\t\\renewcommand\\arraystretch{1.2}\n\t\\setlength{\\tabcolsep}{4mm}{\n\t\t\\begin{tabular}{lcc}\n\t\t\t\\toprule\n\t\t\tMethods & Training time (s)  &  Parameters (M) \\\\ \\toprule\n\t\t\tLMAM    &       17.8         &   0.62  \\\\\n\t\t\tSelf-attention \\cite{vaswani2017attention}  &   58.7     &  3.42  \\\\ \\bottomrule\n\t\\end{tabular}}\n\\end{table}\n\n\\begin{figure}\n\t\\centering\n\t\\includegraphics[width=0.96\\linewidth]{rank}\n\t\\caption{The impact of different rank parameter settings on the experimental accuracy. When the rank exceeds 55, the training results of the model start to be unstable.}\n\t\\label{fig:rank}\n\\end{figure}\n\n\\subsection{Potential Applications}\nIn order to verify that our LMAM method has a potential application in other multi-modal recognition tasks, we further apply the proposed LMAM module to session recommendation and humor detection tasks. Specifically, we embed our LMAM method into dual channel hypergraph convolutional network (DHCN)~\\cite{xia2021self} for session recommendation task and Contextual Memory Fusion Network (C-MFN)~\\cite{zadeh2018memory} for humor detection task, respectively. The session recommendation task is conducted on the Digietica dataset\\footnote{http://cikm2016.cs.iupui.edu/cikm-cup/}, and the humor detection task is carried out on the UR-FUNNY~\\cite{hasan2019ur} dataset. The experimental results are illustrated in Table~\\ref{table6} and Table~\\ref{table7}. As shown in Table~\\ref{table6} and Table~\\ref{table7}, we can observe that our proposed LMAM module can improve the performance of the backbone networks in other multi-modal recognition tasks.\n\n\\begin{table}[htbp]\n\t\\renewcommand\\arraystretch{1.2}\n\t\\setlength{\\tabcolsep}{1.5mm}{\n\t\t\\caption{Experimental results of DHCN method on the Digietica dataset for the session recommendation task. We use P@K (Precision) and MRR@K (Mean Reciprocal Rank) to evaluate the recommendation results. $\\ast$ means the method equipped with the LMAM module. The best result is highlighted in bold.}\n\t\t\\label{table6}\n\t\t\\begin{tabular}{c|cccccc}\n\t\t\t\\hline\n\t\t\t\\multirow{2}{*}{Method} & \\multicolumn{6}{c}{Digietica}                   \\\\ \\cline{2-7}\n\t\t\t& P@5   & P@10  & P@20  & MRR@5 & MRR@10 & MRR@20 \\\\ \\hline\n\t\t\tDHCN \\cite{xia2021self}                   & 27.24 & 39.87 & 53.18 & 15.28 & 17.53  & 18.44  \\\\\n\t\t\tDHCN$^\\ast$                    & \\textbf{28.19} & \\textbf{40.38} & \\textbf{53.70} & \\textbf{16.43} & \\textbf{17.90}  & \\textbf{18.69}  \\\\ \\hline\n\t\\end{tabular}}\n\\end{table}\n\n\\begin{table}[htbp]\n\t\\renewcommand\\arraystretch{1.2}\n\t\\setlength{\\tabcolsep}{2.9mm}{\n\t\t\\caption{Experimental results of C-MFN method on the UR-FUNNY dataset for the humor detection task. C-MFN (C) means using only contextual information without punchlines. C-MFN (P) means using only punchlines with no contextual information. $\\ast$ means the method equipped with the LMAM module. The best result is highlighted in bold.}\n\t\t\\label{table7}\n\t\t\\begin{tabular}{lccccc}\n\t\t\t\\hline\n\t\t\t\\multicolumn{6}{c}{UR-FUNNY}                                                 \\\\ \\hline\n\t\t\t\\multicolumn{1}{l|}{Modality}        & T     & A+V   & T+A   & T+V   & T+A+V \\\\ \\hline\n\t\t\t\\multicolumn{1}{l|}{C-MFN(P) \\cite{hasan2019ur} }       & 62.85 & 53.30 & 63.28 & 63.22 & 64.47 \\\\\n\t\t\t\\multicolumn{1}{l|}{C-MFN(C) \\cite{hasan2019ur}}        & 57.96 & 50.23 & 57.78 & 57.99 & 58.45 \\\\\n\t\t\t\\multicolumn{1}{l|}{C-MFN \\cite{hasan2019ur}}          & 64.44 & 57.99 & 64.47 & 64.22 & 65.23 \\\\\n\t\t\t\\multicolumn{1}{l|}{C-MFN(P)$^\\ast$} & \\textbf{65.43} & \\textbf{59.36} & \\textbf{66.04} & \\textbf{66.59} & \\textbf{66.87} \\\\\n\t\t\t\\multicolumn{1}{l|}{C-MFN(C)$^\\ast$} & \\textbf{59.46} & \\textbf{53.69} & \\textbf{58.39} & \\textbf{58.68} & \\textbf{59.23} \\\\\n\t\t\t\\multicolumn{1}{l|}{C-MFN$^\\ast$}    & \\textbf{65.66} & \\textbf{59.34} & \\textbf{65.68} & \\textbf{64.97} & \\textbf{67.00} \\\\ \\hline\n\t\\end{tabular}}\n\\end{table}\n\n\\section{Conclusion}\nIn this paper, we propose a novel cross-modal feature fusion method to enable better cross-modal feature fusion. {To capture the complementary emotional contextual semantic information in different modalities,} we utilize a low-rank matching attention mechanism (LMAM) to realize the interaction between multimodal features and use low-rank weights to improve efficiency. LMAM is better than the existing fusion methods while has a lower complexity. Extensive experimental results verify that LMAM can be embedded into any existing DL-based CER methods to improve their performance in a plug-and-play manner. We also mathematically prove the effectiveness of our method. Further, LMAM is a general cross-modal feature fusion method and has potential application value in other multi-modal {recognition} tasks, e.g., session recommendation and humor detection.\n\n{Although the LMAM method can achieve better performance improvement and increase the running speed of the model, the introduction of low-rank decomposition technology may cause the model to lose contextual semantic information. Therefore, in future research work, we will study how to achieve efficient fusion of multi-modal features without losing contextual semantic information.}\n\n\\begin{IEEEbiography}[{\\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Yuntao-Shou.jpg}}]{Yuntao Shou} is currently pursuing the M.S. degree in the School of Computer Science and Technology, Xi'an Jiaotong University, Xi'an. His research interests include graph learning and emotion recognition. (Email: shouyuntao@stu.xjtu.edu.cn)\n\\end{IEEEbiography}\n\n\\begin{IEEEbiography}[{\\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{LiuHuan.jpg}}]{Huan Liu}  received the B.Sc. and Ph.D. degrees from Xi’an\n\tJiaotong University, Xi’an, China, in 2013 and 2020, respec\n\ttively. He is currently an Associate Professor with the School\n\tof Computer Science and Technology, Xi’an Jiaotong Uni\n\tversity. His research interests include affective computing,\n\tmachine learning, and deep learning. (Email: huanliu@xjtu.edu.cn)\n\\end{IEEEbiography}\n\n\\begin{IEEEbiography}[{\\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{cxy.jpg}}]{Xiangyong Cao}  received the B.Sc. and Ph.D. degrees from\n\tXi’an Jiaotong University, Xi’an, China, in 2012 and 2018,\n\trespectively. From 2016 to 2017, he was a Visiting Scholar\n\twith Columbia University, New York, NY, USA. He is currently an Associate Professor with the School of Computer\n\tScience and Technology, Xi’an Jiaotong University. His\n\tresearch interests include statistical modeling and image\n\tprocessing. (Email: caoxiangyong@mail.xjtu.edu.cn)\n\\end{IEEEbiography}\n\n\\begin{IEEEbiography}[{\\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mdy.png}}]{Deyu Meng} received the B.Sc., M.Sc., and Ph.D. degrees\n\tfrom Xi’an Jiaotong University, Xi’an, China, in 2001, 2004,\n\tand 2008, respectively. From 2012 to 2014, he took his\n\ttwo-year sabbatical leave with Carnegie Mellon University,\n\tPittsburgh, PA, USA. He is currently a Professor with\n\tthe School of Mathematics and Statistics, Xi’an Jiaotong\n\tUniversity, and an Adjunct Professor with the Faculty of\n\tInformation Technology, The Macau University of Science\n\tand Technology, Macau, China. His research interests in\n\tclude model-based deep learning, variational networks, and\n\tmeta-learning (Email: dymeng@mail.xjtu.edu.cn)\n\\end{IEEEbiography}\n\n\\begin{IEEEbiography}[{\\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{db.png}}]{Bo Dong}  received the Ph.D. de\n\tgree in computer science and technology from Xi’an\n\tJiaotong University, Xi’an, China, in 2014. From\n\t2014 to 2017, he was Postdoc Researcher of control\n\tscience and engineering, Xi’an Jiaotong University,\n\twhere he is currently a Professor with the School of\n\tDistance Education. His main research interests are\n\tdata mining, and pattern recognition. (Email: dong.bo@mail.xjtu.edu.cn)\n\\end{IEEEbiography}\n\n\\vfill\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2203.02385v1.tex",
        "arXiv-2203.13504v1.tex",
        "arXiv-2306.17799v2.tex"
    ],
    "group_id": "group_15",
    "response": "### Title: Multimodal Emotion Recognition in Conversations: A Comparative Analysis of Recent Advances\n\n### Introduction\nEmotion Recognition in Conversations (ERC) is a burgeoning field within human-computer interaction and affective computing, aiming to detect emotional states from various modalities such as text, audio, and video during a dialogue. The ability to recognize emotions in conversations is crucial for developing empathetic machines that can understand and respond to human emotions more effectively. This capability has significant applications in areas such as public opinion analysis, customer service, and social media interaction. The history of ERC research spans several decades, initially focusing on unimodal approaches where a single modality, such as text or audio, was used to infer emotions. However, with the advent of multimodal datasets and advancements in deep learning, the field has shifted towards more sophisticated models that can integrate multiple modalities to enhance the accuracy of emotion recognition.\n\nRecent progress in ERC has seen the introduction of graph-based models and transformer-based architectures, which have shown promise in capturing complex interactions within and between modalities. However, these models often suffer from limitations such as high computational complexity, overfitting, and the inability to fully exploit the complementarity of different modalities. The challenge lies in designing models that can efficiently capture and fuse multimodal features while maintaining a balance between computational efficiency and performance.\n\nThis summary delves into three recent research papers that address these challenges by proposing novel multimodal fusion methods for ERC. Each paper introduces a unique approach to multimodal feature fusion, aiming to improve the accuracy and efficiency of emotion recognition. The first paper, \"MM-DFN: Multimodal Dynamic Fusion Network For Emotion Recognition in Conversations,\" proposes a dynamic fusion network that reduces redundancy and enhances complementarity between modalities. The second paper, \"EmoCaps: Emotion Capsule based Model for Conversational Emotion Recognition,\" introduces an end-to-end model that utilizes emotion vectors extracted through a transformer-based structure. The third paper, \"A Low-rank Matching Attention based Cross-modal Feature Fusion Method for Conversational Emotion Recognition,\" proposes a lightweight cross-modal fusion method that leverages low-rank decomposition to achieve efficient and effective feature fusion. \n\n### Main Content of Each Paper\n\n#### MM-DFN: Multimodal Dynamic Fusion Network For Emotion Recognition in Conversations\nThe MM-DFN paper addresses the issue of redundancy in graph-based fusion methods, which accumulate redundant information at each layer, limiting the context understanding between modalities. To tackle this, the authors propose a novel Multimodal Dynamic Fusion Network (MM-DFN) that employs a Graph-based Dynamic Fusion (GDF) module to fuse multimodal context features dynamically and sequentially. The GDF module captures the dynamics of contextual information in different semantic spaces, thereby reducing redundancy and promoting complementarity between modalities. This is achieved through the use of gating mechanisms in the graph convolutional layers, which control the flow of information between layers. The MM-DFN model is evaluated on two public benchmark datasets, IEMOCAP and MELD, demonstrating superior performance compared to existing methods.\n\nThe MM-DFN model consists of three main components: a modality encoder, the GDF module, and an emotion classifier. The modality encoder captures context and speaker states in each modality using BiGRU for textual features and fully connected networks for acoustic and visual features. The GDF module, which is the core of the model, uses graph convolution operations to aggregate context information both within and across modalities, while gating mechanisms ensure that the model learns the intrinsic sequential patterns of the context. Finally, an emotion classifier is used to predict the emotion label of each utterance based on the fused multimodal features.\n\n#### EmoCaps: Emotion Capsule based Model for Conversational Emotion Recognition\nThe EmoCaps paper introduces a new structure called Emoformer to extract emotion vectors from different modalities and fuse them with sentence vectors to form an emotion capsule. This model is designed to capture the emotional tendency of the utterance, which is crucial for understanding the speaker's emotional state in the context of a conversation. The EmoCaps model is an end-to-end ERC model that employs the Emoformer structure to extract emotion vectors from textual, audio, and visual features. These vectors are then merged with the sentence vector to form an emotion capsule, which is fed into a context analysis model to obtain the final emotion classification results.\n\nThe EmoCaps model innovates by introducing the concept of emotion vectors to multi-modal emotion recognition, which are obtained through a transformer-based structure. The Emoformer structure is similar to a Transformer but does not include the decoder part, focusing on extracting emotion vectors from the input features. The emotion capsule combines these emotion vectors with the sentence vector, providing a richer representation of the utterance that includes both grammatical and semantic information, as well as emotional tendencies. The model is evaluated on the MELD and IEMOCAP datasets, showing better performance than existing state-of-the-art models.\n\n#### A Low-rank Matching Attention based Cross-modal Feature Fusion Method for Conversational Emotion Recognition\nThe third paper, \"A Low-rank Matching Attention based Cross-modal Feature Fusion Method for Conversational Emotion Recognition,\" proposes a novel and lightweight cross-modal feature fusion method called Low-Rank Matching Attention Method (LMAM). LMAM aims to mitigate the quadratic complexity issue caused by the self-attention mechanism while effectively capturing contextual emotional semantic information. The method introduces low-rank weight decomposition to reduce the number of parameters required for the fusion process, thereby improving efficiency and reducing the risk of overfitting. LMAM is embedded into various existing ERC models to demonstrate its effectiveness in improving recognition accuracy.\n\nThe LMAM method operates by first obtaining query feature vectors through linear transformations from the input modal features. It then computes attention scores row by row for each modality, using a low-rank weight matrix to achieve efficient and effective cross-modal feature fusion. This process is repeated for each modality pair, allowing the model to capture both intra-modal and inter-modal interactions. The LMAM module is designed to be a plug-and-play component that can be integrated into any existing ERC model, enhancing its performance without significantly increasing computational complexity.\n\n### Commonalities and Innovations\nAll three papers focus on improving multimodal feature fusion for ERC, aiming to enhance the accuracy and efficiency of emotion recognition. They share the common goal of capturing the complex interactions between different modalities and leveraging these interactions to improve the overall performance of ERC models. However, each paper introduces unique innovations to address specific challenges in multimodal fusion.\n\n- **MM-DFN** introduces a dynamic fusion module (GDF) that captures the dynamics of contextual information in different semantic spaces, reducing redundancy and enhancing complementarity between modalities. This is achieved through gating mechanisms in graph convolutional layers, which control the flow of information between layers.\n- **EmoCaps** innovates by introducing the concept of emotion vectors and an emotion capsule that combines these vectors with the sentence vector. This approach allows the model to capture both grammatical and semantic information, as well as emotional tendencies, providing a richer representation of the utterance.\n- **LMAM** proposes a lightweight cross-modal fusion method that uses low-rank weight decomposition to reduce the number of parameters required for the fusion process. This method is designed to be a plug-and-play component that can be integrated into any existing ERC model, improving its performance without significantly increasing computational complexity.\n\n### Comparison of Results and Discussion\nThe experimental results of the three papers are evaluated on the IEMOCAP and MELD datasets, which are widely used for ERC research. The results demonstrate the effectiveness and superiority of the proposed models in various settings.\n\n- **MM-DFN** achieves the best performance on both datasets, outperforming existing state-of-the-art models in terms of weighted-average F1 score. The model's performance is significantly improved when using the GDF module and speaker embeddings, indicating the importance of these components in capturing contextual interactions and speaker states.\n- **EmoCaps** also shows superior performance on both datasets, with the best results in terms of weighted-average F1 score. The model's performance is enhanced when using the Emoformer structure to extract emotion vectors, demonstrating the effectiveness of this approach in capturing emotional tendencies.\n- **LMAM** achieves the best performance among the compared fusion methods on both datasets, with significant improvements over existing methods in terms of accuracy and F1 score. The LMAM module is shown to be effective in improving the performance of various ERC models, while maintaining a lower computational complexity compared to traditional self-attention mechanisms.\n\nThe results highlight the importance of multimodal feature fusion in ERC and the effectiveness of the proposed methods in addressing the challenges of redundancy and computational complexity. The MM-DFN and EmoCaps models demonstrate the benefits of capturing contextual interactions and emotional tendencies, respectively, while the LMAM method shows the potential of lightweight fusion methods in improving performance without increasing computational overhead.\n\n### Conclusion\nThe three papers discussed in this summary propose innovative multimodal fusion methods for ERC, each addressing specific challenges in the field. The MM-DFN model introduces a dynamic fusion module to reduce redundancy and enhance complementarity between modalities, achieving superior performance on benchmark datasets. The EmoCaps model utilizes emotion vectors and an emotion capsule to capture both grammatical and emotional information, providing a richer representation of the utterance and improving recognition accuracy. The LMAM method proposes a lightweight cross-modal fusion approach that uses low-rank weight decomposition to reduce computational complexity while maintaining high performance.\n\nThese findings suggest that multimodal feature fusion is a critical component in improving the accuracy of ERC models. Future research could explore further improvements in capturing intra-modal and inter-modal interactions, as well as developing more efficient fusion methods that can be applied to other multi-modal recognition tasks. Additionally, the integration of contextual information and emotional tendencies could be enhanced to provide more nuanced understanding of speaker emotions in conversations."
}