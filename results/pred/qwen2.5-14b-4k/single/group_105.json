{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Exploring Cross-sentence Contexts for\\\\Named Entity Recognition with BERT}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\nNamed entity recognition (NER) is frequently addressed as a sequence classification task with each input consisting of one sentence of text. It is nevertheless clear that useful information for NER is often found also elsewhere in text. Recent self-attention models like BERT can both capture long-distance relationships in input and represent inputs consisting of several sentences. This creates opportunities for adding cross-sentence information in natural language processing tasks. This paper presents a systematic study exploring the use of cross-sentence information for NER using BERT models in five languages. We find that adding context as additional sentences to BERT input systematically increases NER performance. Multiple sentences in input samples allows us to study the predictions of the sentences in different contexts. We propose a straightforward method, Contextual Majority Voting (CMV), to combine these different predictions and demonstrate this to further increase NER performance. Evaluation on established datasets, including the CoNLL'02 and CoNLL'03 NER benchmarks, demonstrates that our proposed approach can improve on the state-of-the-art NER results on English, Dutch, and Finnish, achieves the best reported BERT-based results on German, and is on par with other BERT-based approaches in Spanish. We release all methods implemented in this work under open licenses.\n\n\\end{abstract}\n\n\\section{Introduction}\n\\label{intro}\n\nNamed entity recognition (NER) approaches have evolved through various methodological phases, broadly including rule/knowledge-based, unsupervised, feature engineering and supervised learning, and feature inferring approaches \\cite{yadav-bethard-2018-survey,Li_2020}. The use of cross-sentence information in some form has been a normal part of many NER methods in the former categories, but its role has diminished with the current feature inferring deep learning based approaches. Rule/knowledge-based approaches such as that of \\newcite{mikheev-etal-1998-description} typically match strings to lexicons and similar domain knowledge sources, possibly going through text multiple times with refinement based on entities found on earlier passes. Later, manually engineered features were used to incorporate information from the surrounding text, whole documents, data sets and also from external sources. The number of different features and classifiers grew during the years and it was normal that the features also contained cross-sentence information in some form as for example in \\cite{krishnan-manning-2006-effective}. Dense representations of text such as word, character, string and subword embeddings first started to appear in NER methods as additional features given to classifiers \\cite{collobert2011}. Step by step, feature engineering has been demoted to a lesser role, as the most recent deep learning approaches learn to create meaningful and context-sensitive representations of text by pre-training with vast amounts of unlabelled data. These contextual representations are often used directly as features for existing NER architectures or fine-tuned with labelled data to match a certain task. \n\n\\blfootnote{\n    \n    \\hspace{-0.65cm}  % space normally used by the marker\n    This work is licensed under a Creative Commons \n    Attribution 4.0 International Licence.\n    Licence details:\n    \\url{http://creativecommons.org/licenses/by/4.0/}.\n     \n}\n\nIn recent years, the development of Natural Language Processing (NLP) in general and NER in particular have been greatly influenced by deep transfer learning methods capable of creating contextual representations of text, to the extent that many of the state-of-the-art NER systems mainly differ from one another on the basis of how these contextual representations are created \\cite{Peters_2018,devlin2018bert,akbik2018coling,Baevski_2019}. Using such models, sequence tagging tasks are often approached one sentence at a time, essentially discarding any information available in the broader surrounding context, and there is only little recent study on the use of cross-sentence context -- sentences around the sentence of interest -- to improve sequence tagging performance. In this paper, we present a comprehensive exploration of the use of cross-sentence context for named entity recognition, focusing on the recent BERT deep transfer learning models \\cite{devlin2018bert} based on self-attention and the transformer architecture \\cite{vaswani2017attention}. BERT uses a fixed-size window that limits the amount of text that can be input to the model at one time. The model maximum window size, or \\emph{maximum sequence length}, is fixed during pre-training, with 512 wordpieces a common choice. This window fits dozens of typical sentences of input at a time, allowing us to include extensive sentence context. Here, we first study the effect of predicting tags for individual sentences when they are moved around the window, surrounded by their original document context from the source data. Second, we utilize different predictions for the same sentences to potentially further improve performance, combining predictions using majority voting, adapting an approach that has been used already in early NER implementations \\cite{tjong-kim-sang-etal-2000-applying,van-halteren-etal-2001-improving,florian-etal-2003-named}.\nWe evaluate these approaches on five languages, contrasting NER results using BERT without cross-sentence information, sentences in context, and aggregation using Contextual Majority Voting (CMV) on well-established benchmark datasets.\nWe show that using sentences in context consistently improves NER results on all of the tested languages and CMV further improves the results in most cases. Comparing performance to the current state-of-the-art NER results in the 5 languages, we find that our approach establishes new state-of-the-art results for English, Dutch, and Finnish, the best BERT-based results on German, and effectively matches the performance of a BERT-based method in Spanish. \n\n\\section{Related work}\n\nThe state-of-the-art in NER has recently moved from approaches using word/character representations and manually engineered features \\cite{passos-etal-2014-lexicon,Chiu_2016} toward approaches directly utilizing deep learning-based contextual representations \\cite{akbik2018coling,Peters_2018,devlin2018bert,Baevski_2019} while adding few manually engineered features, if any. While successful in terms of NER performance, these approaches have tended to predict tags for one sentence at a time, discarding information from surrounding sentences.\n\nOne recent method taking sentence context into account is that of \\newcite{akbik-etal-2019-pooled}, which addresses a weakness of an earlier contextual string embedding method \\cite{akbik2018coling}, specifically the issue of rare word representations occurring in underspecified contexts. \\newcite{akbik-etal-2019-pooled} make the intuitive assumption that such occurrences happen when a named entity is expected to be known to the reader, i.e.\\ the name is either introduced earlier in text or is of general in-domain knowledge. Their approach is to maintain a memory of contextual representations of each unique word/string in text and pool together contextual embeddings of a string occurring in text with the contextual embeddings of the same string earlier in text. This pooled contextual embedding is then concatenated with the current contextual embedding to get the final embedding to use in classification.\n\nAnother recent approach taking broader context into account for NER was proposed by \\newcite{luo2020hierarchical}, where in addition to token representations, also sentence and document level representations are calculated and used for classification using a CRF model. A sliding window is used by \\newcite{Wu_2019} so that part of the input is preserved as context when the window is moved forward in text. \n\\newcite{Baevski_2019} state that they use longer paragraphs in pre-training their model, but it is not mentioned in the paper if such longer paragraphs are used also in fine-tuning the model or predicting tags for NER.\nSome other approaches such as that of \\newcite{liu-etal-2019-towards} include explicit global information in the form of e.g.\\ gazetteers. Also, some approaches formulate NER as a span finding task instead of sequence labelling \\cite{banerjee2019knowledge,li-etal-2020-unified}. These approaches would likely allow the use of longer sequences, but the incorporation of cross-sentence information is not explicitly proposed by the authors. In the paper introducing BERT, \\newcite{devlin2018bert} write in the description of their NER evaluation ``we include the maximal document context provided by the data.'' However, no detailed description of how this inclusion was implemented is provided, and some NER implementations using BERT have struggled to reproduce the results of the paper.\\footnote{\\url{https://github.com/google-research/bert/issues/581}}\\textsuperscript{,}\\footnote{\\url{ https://github.com/google-research/bert/issues/569}} \nThe addition of document context to NER using BERT is discussed also by \\newcite{virtanen2019multilingual}, who fill each input sample with the following sentences and use the first sentence in each sample for predictions, and thus only introduce context appearing \\emph{after} the sentence of interest in the source text.\n\nOf the related work discussed above, our approach most closely resembles that of \\newcite{virtanen2019multilingual}, which in turn aims to directly follow \\newcite{devlin2018bert}.\nBy contrast to other studies discussed above, we do not introduce extra features or embeddings representing cross-sentence information or incorporate extra information in addition to that captured by the BERT model. Instead, we directly utilize the BERT architecture and rely on self-attention and voting to combine predictions for sentences in different contexts. \n\n\\section{Data}\n\nThe data used in this study consists of pre-trained BERT models and NER datasets for five different languages. We aimed to use monolingual BERT models as numerous recent studies have suggested that well-constructed language-specific models outperform multilingual ones \\cite{virtanen2019multilingual,vries_bertje_2019,le2020flaubert}.\nWe selected the following language-specific pre-trained BERT models for our study, focusing on languages that also have established benchmark data for NER:\n\\begin{itemize}\n\\item BERTje base, Cased for Dutch \\cite{vries_bertje_2019}\\footnote{\\url{https://github.com/wietsedv/bertje}}\n\\item BERT-Large, Cased (Whole Word Masking) for English \\footnote{\\url{https://github.com/google-research/bert}}\n\\item FinBERT base, Cased for Finnish \\cite{virtanen2019multilingual}\\footnote{\\url{https://github.com/TurkuNLP/FinBERT}}\n\\item German BERT, Cased for German \\footnote{\\url{https://deepset.ai/german-bert}}\n\\item BETO, Cased for Spanish \\cite{CaneteCFP2020}\\footnote{\\url{https://github.com/dccuchile/beto}} .\n\\end{itemize}\nFor comparison purposes we also tested multilingual BERT\\footnote{\\url{https://github.com/google-research/bert}} with the Spanish language. From the models introduced above all except German and multilingual BERT have used the Whole Word Masking variation of the Masked Language Model objective in pre-training instead of the method introduced in the original paper \\cite{devlin2018bert}. Whole Word Masking was introduced by the developers of BERT after the original paper was published. In this pre-training objective, all the tokens corresponding to one word in text are masked instead of completely random tokens, which often leaves some of the tokens in multi-token words unmasked.       \nWe aimed to apply sufficiently large, widely-used benchmark datasets for evaluating NER results, assessing our methods primarily on the CoNLL'02 and CoNLL'03 Shared task Named entity recognition datasets \\cite{Tjong_Kim_Sang_2002,Tjong_Kim_Sang_2003}, which cover four of our five target languages. For the fifth language, Finnish, we use two recently published named entity recognition corpora \\cite{ruokolainen2019finnish,luoma-EtAl:2020:LREC}\\footnote{\\url{https://github.com/mpsilfve/finer-data}}\\textsuperscript{,}\\footnote{ \\url{https://github.com/TurkuNLP/turku-ner-corpus}}. These two Finnish datasets are annotated in a compatible way, and for this study they are combined into a single corpus by simple concatenation, following \\newcite{luoma-EtAl:2020:LREC}.\n\n\\begin{table}[!t]\n\\centering\n\\begin{tabular}{l|lllll}\n\\textbf{Tokens}   & English &  German & Spanish & Dutch   & Finnish \\\\ \\hline\nTrain             & 203,621 & 206,931 & 264,715 & 202,644 & 342,924 \\\\ %\\hline % 180,178 + 163,344 \\\\ \\hline\nDevelopment       &  51,362 &  51,444 &  52,923 &  37,687 &  31,872 \\\\ %\\hline % 13,564 + 18,384 \\\\ \\hline\nTest              &  46,435 &  51,943 &  51,533 &  68,875 &  67,425 \\\\ %\\hline % 46,363 + 21,140 \\\\ \\hline\n\\multicolumn{6}{c}{} \\\\\n\\textbf{Entities} & English &  German & Spanish & Dutch   & Finnish \\\\ \\hline\nTrain             &  23,499 &  11,851 &  18,798 &  13,344 &  27,026 \\\\ %\\hline % 17,645 + 9,380 \\\\ \\hline\nDevelopment       &   5,942 &   4,833 &   4,352 &   2,616 &   2,286 \\\\ %\\hline % 1,223 + 1,063 \\\\ \\hline\nTest              &   5,648 &   3,673 &   3,559 &   3,941 &   5,129 \\\\ %\\hline % 4,123 + 1,005 \\\\ \\hline\n\\end{tabular}\n\\caption{Key statistics of the NER data sets}\n\\label{ner-data}\n\\end{table}\n\nAll of the NER datasets define separate training, development and test sets, and we follow the given subdivision for each. The training sets for each language are used for fine-tuning the corresponding BERT model for NER, development sets are used for evaluation in hyperparameter selection, and the test sets are only used in final experiments for evaluating models trained with the selected hyperparameters. As previous studies vary in whether to combine development data to training data for training a final model, we report also results where models are trained with a combined training and development set for final test experiments.\nThe datasets for the CoNLL shared task languages contain four different classes of named entities: Person (PER), Organization (ORG), Location (LOC) and Miscellaneous (MISC). The Finnish NER datasets also use the PER, ORG, and LOC types along with three others, Product (PROD), Event (EVENT), and Date (DATE). For implementation purposes we converted all the datasets to the same format prior to experiments: The character encoding of each file was converted to UTF-8, and the NER labelling scheme was converted to IOB2 \\cite{ratnaparkhi1998maximum} also for corpora that were originally in the IOB scheme \\cite{ramshaw-marcus-1995-text}. By contrast to the older IOB scheme, in the IOB2 scheme the label for the first token of a named entity is always marked with a B-prefix (e.g.\\ B-PER), even if the previous token is not part of a named entity.\nThe key statistics for the NER datasets are presented in Table~\\ref{ner-data}. Finally, we note that all the datasets except CoNLL'02 Spanish provide information on document boundaries using special \\texttt{-DOCSTART-} tokens at the start of each new document.\n\n\\section{Methods}\n\\label{sec:methods}\n\nAs the starting point for exploring the cross-sentence information for NER using BERT, we use a NER pipeline implementation introduced by \\newcite{virtanen2019multilingual} that closely follows the straightforward approach presented by \\newcite{devlin2018bert}. Here, the last layer of the pre-trained BERT model is followed by a single time-distributed dense layer which is fine-tuned together with the pre-trained BERT model weights to produce the softmax probabilities of NER tags for input tokens. No modelling of tag transition probabilities or any additional processing to validate tag sequences is used.\n\nIn our implementation, exactly one example is constructed for each sentence of the corpus unless the sentence is so long that it does not fit to the maximum sequence length\\footnote{In this special case the long sentence is split to produce multiple input sequences that are considered as sentences for the rest of the implementation.}. The sentence is placed at the beginning of the BERT window and following sentences from the corpus are used to fill the window (up to the maximum sequence length), with special separator (\\texttt{[SEP]}) tokens separating the sentences. Partial sentences are used to fill up the BERT examples. As a special case, the sentences used for filling the window for the last sentences in input data are picked by wrapping back to the beginning of the corpus. This approach creates situations where some input samples contain sentences from different original documents, if the documents were next to one another in the corpus. For this reason, we also implemented documentwise wrapping of sentences if the input data had document boundaries marked with \\texttt{-DOCSTART-} tokens. We used this information to build input samples by filling the sentences at the end of one document with the sentences from beginning of that same document instead of the next sentences in the original data. In this case only full sentences are added to each input sample, and padding (\\texttt{[PAD]}) tokens are used to fill empty space if the next sentence in the input data does not fit into the window as demonstrated in (Figure~\\ref{fig:context}b).\n\n\\begin{figure}[!t]\n\\includegraphics[width=0.98\\textwidth]{images/context.png}\n\\caption{Illustration of various input representations for sequence labelling tasks. a) One sentence per example (\\emph{Single}), b) including following sentences (\\emph{First, CMV}), c) including preceding and following sentences (\\emph{Sentence in context}). CMV combines predictions for the same sentence (e.g. $\\textrm{S}_2$ in b) in various positions and contexts. The empty square ({\\small $\\square$}) stands for special separator symbols (e.g.\\ \\texttt{[CLS]}, \\texttt{[SEP]} and \\texttt{[PAD]} for BERT); a light background color is used to represent special symbols and incomplete sentences in c).\n}\n\\label{fig:context}\n\\end{figure}\n\nConstructing inputs in this way implies that the same sentences from the original data occur in different positions and with varying (sizes of) left and right contexts in different samples. We wanted to examine the predictions in different contexts more closely to see if there are consistent effects on tag prediction quality depending on the starting position of a sentence inside a context. One challenge here was how to consistently measure performance with different contexts: sentences are of different lengths, and as they are added to input samples, the beginning of the window was only place where the starting locations of sentences would align. Also, the number of sentences that fit into the window vary substantially. For this reason, it is not possible e.g.\\ to always pick the N\\emph{th} sentence to study as there are no guarantees one will exist in all examples. To address this issue and build input samples for testing predictions at different locations, we placed the sentence of interest to start at a specified location inside the window, and filled the window in both directions with sentences before / after the sentence of interest in the original data. We tested the starting positions of the sentence of interest from 1 (0 being the \\texttt{[CLS]} token) up to the maximum sequence length (512 wordpieces) with intervals of 32 wordpieces. If the sentence of interest was longer than the space between a starting position and the maximum sequence length, the starting position for that particular sentence was moved backwards to fit the sentence in the window. \n\nEnsembles of classifiers are commonly used to improve classification performance at various tasks, and it seems reasonable to assume that predictions for the same input sentences in different positions and contexts create an ensemble-like construct. This is not an ensemble in the conventional sense, as the number of predictions we get for each sentence varies.\nWe evaluate two different variations combining the results from multiple predictions in different contexts. The first approach is to assign labels to sentences in each location first, and then take a majority vote of the assigned labels. The second approach is to add together the softmax probabilities of predictions in different contexts, and then take the argmax of the sum. For simplicity, we here term both Contextual Majority Voting (CMV) as they are variations of the same underlying idea. The implementation uses only predictions of tokens in whole sentences, not ones in partial sentences that may appear in input examples. \n\nFor fine-tuning the pre-trained BERT models, we largely follow the process introduced in \\cite{devlin2018bert}.\nWe use the maximum sequence length of 512 in all experiments to include maximal cross-sentence context, the Adam optimizer \\cite{kingma2014adam} ($\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\epsilon = 1e-6$) with warmup of 10\\% of samples, linear learning rate decay, a weight decay rate of 0.01, and norm clipping on 1.0. Sample weights are used for inputs so that the special tokens \\texttt{[CLS]} and \\texttt{[PAD]} are given zero weight and everything else 1 when calculating the loss (sparse categorical cross-entropy).\n \nWe select hyperparameters with an exhaustive search of the grid proposed by Devlin et~al., modified to skip batch size 32 and add batch sizes 2 and 4 instead as our initial experiments indicated better performance with smaller batch sizes. That is, the grid search is done over the following parameter ranges: \n\\begin{itemize}\n    \\setlength\\itemsep{-0.5em}\n    \\item Learning rate: 2e-5, 3e-5, 5e-5 \n    \\item Batch size: 2, 4, 8, 16\n    \\item Epochs: 1, 2, 3, 4\n\\end{itemize}\nWe repeated each experiment 5 times with every hyperparameter combination. The best hyperparameters were selected based on the mean of exact mention-level F1 scores, as evaluated against the development set using a Python implementation of the standard conlleval evaluation script.\n\nAs a reference we use a BERT model which is fine-tuned using only single sentences from the input data. For this baseline, predictions are also made on the basis of single sentences (see Figure~\\ref{fig:context}a).\n\n\\begin{figure}[!t]\n\\begin{subfigure}{0.33\\textwidth}\n\\includegraphics[width=\\linewidth,height=4cm]{images/en-large-comparison.eps} \n\\caption{English}\n\\label{fig:subim1}\n\\end{subfigure}\n\\begin{subfigure}{0.33\\textwidth}\n\\includegraphics[width=\\linewidth, height=4cm]{images/du-comparison.eps}\n\\caption{Dutch}\n\\label{fig:subim2}\n\\end{subfigure}\n\\begin{subfigure}{0.33\\textwidth}\n\\includegraphics[width=\\linewidth, height=4cm]{images/de-comparison.eps} \n\\caption{German}\n\\label{fig:subim3}\n\\end{subfigure}\n\\begin{subfigure}{0.33\\textwidth}\n\\includegraphics[width=\\linewidth, height=4cm]{images/esp-comparison.eps}\n\\caption{Spanish}\n\\label{fig:subim4}\n\\end{subfigure}\n\\begin{subfigure}{0.33\\textwidth}\n\\includegraphics[width=\\linewidth, height=4cm]{images/fi-comparison.eps}\n\\caption{Finnish}\n\\label{fig:subim5}\n\\end{subfigure}\n\\caption{NER performance on development set measured with CMV and in different sentence starting locations. The lower curves show mean performance over whole hyperparameter range, and the upper curves the results with the best hyperparameters (mean of 5 repetitions) for each location. The flat dashed lines show the best CMV results.}\n\\label{fig:image2}\n\\end{figure}\n\n\\section{Results}\n\nBased on initial development set results, we decided to focus only on CMV using examples constructed document-wise of the variations of this method (see Section~\\ref{sec:methods}). The exception here is the Spanish CoNLL dataset, for which document boundary information was not available. Further, as the differences between CMV variations were found not to be large, we decided to only consider the variant that first assigns labels and then votes between the labels. \n\nThe effect of the sentence of interest starting location and the effect of CMV method on development data is illustrated in Figure~\\ref{fig:image2}. Our initial expectation was that placing the sentence of interest near the middle of the sequence would generally yield the best performance. However, while this effect can be observed e.g.\\ for English (Figure~\\ref{fig:image2}a), the pattern does not hold in all cases, although in most cases performance does improve when moving the starting position away from either end of the context window. The problem was that the performance in the middle of the context did not appear to be stable enough to pick a reliable starting position to look at prediction time. This can be seen in the figure \\ref{fig:image2} where the results for different starting locations tend to vary without a clear central optimum. The results for Dutch (Figure \\ref{fig:subim2}) deviated the most from our expectations, and a possible reason for this was later found from the source data: the sentence order of the documents inside the original Dutch language data set has been randomized for copyright reasons. To test if randomizing the sentence order of documents has an effect on results, we tested this with other languages. However, in our initial experiments randomizing sentences inside each document did not result in significant performance drop on any of the tested languages. \n\n\\begin{table*}[t!]\n\\centering\n\\begin{tabular}{ll@{\\hskip 5pt}cl@{\\hskip 5pt}cl@{\\hskip 5pt}cl@{\\hskip 5pt}c}\n                            & \\multicolumn{2}{c}{Precision} & \\multicolumn{2}{c}{Recall} & \\multicolumn{2}{c}{F1} & \\multicolumn{2}{c}{F1 train+dev} \\\\ \\hline\nEnglish, CMV             & 93.06 & (0.25) & \\textbf{93.78} & (0.08) & 93.42 & (0.12) & 93.57 & (0.33) \\\\  %fixed\nEnglish, First          & \\textbf{93.15} & (0.15) & 93.73 & (0.04) & \\textbf{93.44} & (0.06) & \\textbf{93.74} & (0.25)\\\\ %BERT Large WMM\nEnglish, Single         & 91.12 & (0.25) & 92.28 & (0.23) & 91.70 & (0.24) & 91.94 & (0.15)\\\\ \\hline %BERT Large VMM\nDutch, CMV               & \\textbf{93.12} & (0.26) & 93.26 & (0.18) & 93.19 & (0.21) &  \\textbf{93.49} &  (0.23) \\\\ %valmis\nDutch, First            & 93.03 & (0.65) & \\textbf{93.38} & (0.38) & \\textbf{93.21} & (0.51) & 93.39 & (0.26)\\\\ %valmis\nDutch, Single           & 91.57 & (0.35) & 91.49 & (0.41) & 91.53 & (0.37) & 91.92 & (0.30)\\\\ \\hline %valmis\nFinnish, CMV             & \\textbf{92.91} & (0.18) & \\textbf{94.42} & (0.13) & \\textbf{93.66} & (0.13) & \\textbf{93.78} & (0.26)\\\\ %valmis\nFinnish, First          & 92.56 & (0.14) & 94.24 & (0.08) & 93.39 & (0.10) & 93.65 & (0.26) \\\\ %valmis\nFinnish, Single         & 90.74 & (0.10) & 92.11 & (0.24) & 91.42 & (0.16) & 91.97 & (0.21) \\\\ \\hline %valmis\nGerman, CMV              & \\textbf{86.91} & (0.31) & \\textbf{84.38} & (0.32) & \\textbf{85.63} & (0.30) & \\textbf{87.31} & (0.27)\\\\ %fixed\nGerman, First           & 86.37 & (0.39) & 84.07 & (0.10) & 85.21 & (0.22) & 86.91 & (0.11)\\\\ %valmis\nGerman, Single          & 85.55 & (0.20) & 81.81 & (0.31) & 83.64 & (0.21) & 85.67 & (0.25)\\\\ \\hline %fixed\nSpanish, CMV              & \\textbf{87.80} & (0.25) & \\textbf{87.98} & (0.18) & \\textbf{87.89} & (0.21) & \\textbf{87.97} & (0.21)\\\\ %valmis\nSpanish, First           & 86.71 & (0.31) & 87.41 & (0.28) & 87.06 & (0.28) & 87.27 & (0.25)\\\\ %valmis\nSpanish, Single         & 87.43 & (0.53) & 87.90 & (0.34) & 87.66 & (0.43) & 87.52 & (0.41)\\\\ \\hline %valmis\nS-mBERT, CMV              & \\textbf{87.25} & (0.50) & \\textbf{88.67} & (0.46) & \\textbf{87.95} & (0.47) & \\textbf{88.32} & (0.26)\\\\ %vslmis\nS-mBERT, First          & 86.92 & (0.40) & 87.88 & (0.44) & 87.40  & (0.42) & 87.54 & (0.25) \\\\ %valmis\nS-mBERT, Single         & 87.19 & (0.28) & 87.81 & (0.26) & 87.50 & (0.26) & 87.57 & (0.29) \\\\ \\hline %valmis \n\\end{tabular}\n\\caption{NER results for different methods and languages (standard deviation in parentheses).}\n\\label{ner-results-language}\n\\end{table*}\n\nThe final test set results for models trained with the best hyperparameter combinations found using the development sets are summarized in Table~\\ref{ner-results-language}. We report precision, recall and F1-score for models trained only on the training dataset, and additionally F1-scores for models trained with combined training and development sets using the same hyperparameters. For each language/BERT model pair, we report performance for the baseline using only a single sentence per window (Single), the approach where sentences from the following context are included but only predictions for the first sentence in each window are used (First), and, finally, performance with CMV (see also Figure~\\ref{fig:context}).\n\nThese results show that BERT NER predictions systematically benefit from access to cross-sentence context. For all tested languages except Spanish, models that are fine-tuned and tested with samples containing context outperform models which do not use any context and are relying only on single sentences. What is not directly seen from Table~\\ref{ner-results-language} is that generally the results with the method First outperform the results with the method Single, and similarly the method CMV generally outperforms the method First. Both English and Dutch seem to perform well with the method First and for Spanish the method Single also performs well. One thing to note is that English and Dutch results with CMV outperform the method First with the hyperparameters that produced the best result for the method First.\nHowever, the final results for CMV just were not as good with the hyperparameters that produced the best performance for CMV on the development data. \n\nTo further evaluate the performance of CMV method, we checked the results of each fine-tuned model on the development set during hyperparameter search. There were 48 hyperparameter combinations to evaluate for each model. For English, German, Spanish and Finnish, the CMV method outperformed the method First for every hyperparameter combination when calculating the results as the mean of mention-level F1 scores from 5 repetitions. For Spanish this includes both the experiments with the Spanish monolingual model as well as the experiments with the multilingual model. The only exception to this were the results on Dutch, for which CMV outperformed the method First in 41 cases out of 48. The fact that sentences in Dutch data are in randomized order may contribute to this. In total, the CMV method improved the results over method First in 281 cases out of 288.\nIn the same fashion, we evaluated the difference in performance between the method Single and the method First evaluated against the development set. The method First outperformed the method Single for every hyperparameter combination for every tested language. \n\n\\begin{table}[t!]\n\\small\n\\centering\n\\begin{tabular}{l|llll}\nModel         & Our F1 &  Our F1 (t+d) & Current BERT & Current SOTA    \\\\ \\hline\nEnglish   & 93.44 & \\textbf{93.74} & 93.47 \\cite{Liu_2019} & 93.5 \\cite{Baevski_2019}\\\\ \\hline\nDutch   & 93.21 & \\textbf{93.49} & 90.94 \\cite{Wu_2019} & 92.69 \\cite{strakova-etal-2019-neural}\\\\ \\hline\nFinnish        &  93.66 &  \\textbf{93.78} & 93.11 \\cite{luoma-EtAl:2020:LREC} & 93.11 \\cite{luoma-EtAl:2020:LREC} \\\\ \\hline\nGerman  &  85.63 & 87.31 & 82.82 \\cite{Wu_2019} & \\textbf{88.32} \\cite{akbik2018coling} \\\\ \\hline\nSpanish   & 87.89 & 87.97 & 88.43 \\cite{CaneteCFP2020} & \\textbf{89.72} \\cite{conneau-etal-2020-unsupervised} \\\\ \\hline\nSpanish, mBERT   & 87.95 & 88.32 & 88.43 \\cite{CaneteCFP2020} & \\textbf{89.72} \\cite{conneau-etal-2020-unsupervised} \\\\ \\hline\n\\end{tabular}\n\\caption{NER result comparison to the state of the art.}\n\\label{ner-sota2}\n\\end{table}\n\nIn Table \\ref{ner-sota2} we compare the results using cross-sentence context with current the state-of-the-art in NER for the languages studied here. We are able to establish a new state-of-the-art result for three languages, English, Dutch and Finnish, as well as improve the best BERT-based score on German. These results benefit from using the combined training and development set in final model training. The previous state-of-the-art is also surpassed on Dutch and Finnish when only the training set is used for the final model. On Spanish our results fall slightly below the reported state-of-the-art. Perhaps somewhat surprising was that multilingual BERT outperformed the dedicated Spanish language BERT model, failing to replicate the results of \\newcite{CaneteCFP2020}, who reported that the Spanish model outperformed that of \\newcite{Wu_2019}, who had previously reached the best Spanish BERT performance using multilingual BERT.\nDespite this minor discrepancy, we find that both the simple approach of including following sentences as context as well as CMV are very effective, allowing a straightforward BERT NER model to achieve state-of-the-art performance with only a few modifications of the representation.\n\n\\section{Discussion}\n\nThe results presented here are, as far as we know, the first systematic study on how cross-sentence information can be utilized with BERT for NER, and the methods presented here form a good starting point for discussion and further research into the subject. Contextual Majority Voting is straightforward to implement in existing BERT-based systems as the actual model and associated infrastructure is not modified. It is quite probable that similar ways of including cross-sentence information or majority voting structures may be beneficial with other attention-based models as well. The computational overhead for the required pre- and postprocessing of the samples is very modest, but increasing the maximum sequence length in fine-tuning e.g. from 128 to 512 to fit more sentences in one sample does come with a tradeoff of increased computational cost.\n\nOne aspect deserving more study is how prediction performance is affected if sentences are not repeated, or repeated fewer times, in examples during prediction. Reducing or entirely avoiding repetition would allow for more efficient use of the model while still providing context for sentences, which might be a reasonable compromise between performance and computational efficiency for large-scale practical applications.\nA further possibility for future research would be to explore weighted majority voting. Our results lend some support to the idea that predictions made for tokens around in the center of the window are generally more reliable than predictions for tokens near its edges, where context is limited on one side of the token. Providing higher weight to predictions in the middle of the sequence could potentially help further improve the performance of the aggregation approach. Another aspect for future work would be to study the effect of the context and sentence order. Our preliminary tests with randomized sentence order from same documents showed minimal effect on performance. Is it enough to have context from the same document? Would the situation change drastically if random sentences from the whole input data were used instead? Finally, the incorporation of transition probabilities or other processing to check tag sequences for illegal transitions would likely improve performance further.\n\n\\section{Conclusions}\n\nWe have presented a comprehensive evaluation of the effect of including cross-sentence context for named entity recognition with BERT and introduced a simple and easy-to-implement approach for the task using majority voting. The proposed method established new state-of-the-art results in named entity recognition for three languages and is near the state-of-the-art for two other languages, demonstrating how simple ideas may boost the performance of even very strong models.\n\nWe release all methods implemented in this work under open licenses from \\url{https://github.com/jouniluoma/bert-ner-cmv} .\n\n\\section*{Acknowledgements}\n\nWe wish to thank the CSC -- IT Center for Science, Finland, for generous computational resources. This work was funded in part by the Academy of Finland.\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{FLERT: Document-Level Features for Named Entity Recognition}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\n\nCurrent state-of-the-art approaches for named entity recognition (NER) typically consider text at the sentence-level and thus do not model information that crosses sentence boundaries. However, the use of transformer-based models for NER offers natural options for capturing document-level features. In this paper, we perform a comparative evaluation of document-level features in the two standard NER architectures commonly considered in the literature, namely \"fine-tuning\" and \"feature-based LSTM-CRF\". We evaluate different hyperparameters for document-level features such as context window size and enforcing document-locality. We present experiments from which we derive recommendations for how to model document context and present new state-of-the-art scores on several CoNLL-03 benchmark datasets. Our approach is integrated into the \\textsc{Flair} framework to facilitate reproduction of our experiments. \n\n\\end{abstract}\n\n\\section{Introduction}\n\\vspace{-1mm}\n\nNamed entity recognition (NER) is the well-studied NLP task of predicting shallow semantic labels for sequences of words, used for instance for identifying the names of persons, locations and organizations in text. Current approaches for NER often leverage pre-trained transformer architectures such as BERT \\citep{devlin-etal-2019-bert} or XLM \\citep{lample2019cross}.\n\n\\noindent \n\\textbf{Document-level features.} While NER is traditionally modeled at the sentence-level, transformer-based models offer a natural option for capturing document-level features by passing a sentence with its surrounding context. As Figure~\\ref{overview-bert-fine-tuning-document-level} shows, this context can then influence the word representations of a sentence: The example sentence \"I love Paris\" is passed through the transformer together with the next sentence that begins with \"The city is\", potentially helping to resolve the ambiguity of the word \"Paris\".  \nA number of prior works have employed such document-level features~\\cite{devlin-etal-2019-bert, virtanen2019multilingual,yu-etal-2020-named} but only in combination with other contributions and thus have not evaluated the impact of using document-level features in isolation. \n\n\\noindent\n\\textbf{Contributions.}\nWith this paper, we close this experimental gap and present an evaluation of document-level features for NER. As there are two conceptually very different approaches for transformer-based NER that are currently used across the literature, we evaluate document-level features in both: \n\n\\begin{enumerate}\n\\vspace{-1mm}\n    \\item In the first, we \\textit{fine-tune} the transformer itself on the NER task and only add a linear layer for word-level predictions~\\cite{devlin-etal-2019-bert}.\n\\vspace{-1mm}\n    \\item In the second, we use the transformer only to provide \\textit{features} to a standard LSTM-CRF sequence labeling architecture~\\cite{2015arXiv150801991H} and thus perform no fine-tuning.\n\\end{enumerate}\n\\vspace{-1mm}\nWe discuss the differences between both approaches and explore best hyperparameters for each. In their best determined setup, we then perform a comparative evaluation.\nWe find that (1) document-level features significantly improve NER quality and that (2) fine-tuning generally outperforms feature-based approaches. We also determine best settings for document-level context and report several new state-of-the-art scores on the classic CoNLL benchmark datasets. Our approach is integrated as the \"FLERT\"-extension into the \\textsc{Flair} framework~\\cite{akbik-etal-2019-flair} to facilitate further experimentation.\n\n\\begin{figure*}\n\\vspace{-3mm}\n \\centering\n \\includegraphics[width=\\linewidth]{figures/bert_document_level_3.pdf}\n\\vspace{-7mm}\n \\caption{To obtain document-level features for a sentence that we wish to tag (\"I love Paris\", shaded green), we add 64 tokens of left and right tokens each (shaded blue). As self-attention is calculated over all input tokens, the representations for the sentence's tokens are influenced by the left and right context.}\n\\vspace{-3mm}\n \\label{overview-bert-fine-tuning-document-level}\n\\end{figure*}\n\n   \n\n\\section{Document-Level Features}\n\\vspace{-2mm}\n\nIn a transformer-based architecture, document-level features can easily be realized by passing a sentence with its surrounding context to obtain word embeddings, as illustrated in Figure~\\ref{overview-bert-fine-tuning-document-level}. \n\n\\noindent\n\\textbf{Prior approaches.}\nThis approach was first employed by~\\citet{devlin-etal-2019-bert} with what they described as a \"maximal document context\", though technical details were not listed. Subsequent work used variants of this approach. For instance, \\citet{virtanen2019multilingual} experiment with adding the following (but not preceding) sentence as context to each sentence. \\citet{yu-etal-2020-named} instead use a 64 surrounding token window for each token in a sentence, thus calculating a large context on a per-token basis. By contrast, \\citet{luoma2020exploring} adopt a multi-sentence view in which they combine predictions from different windows and sentence positions.  \n\n\\noindent\n\\textbf{Our approach.}\nIn this paper, we instead use a conceptually simple variant in which we create context on a per-sentence basis: For each sentence we wish to classify, we add 64 subtokens of left and right context, as shown in Figure~\\ref{overview-bert-fine-tuning-document-level}. This has computational and implementation advantages in that each sentence and its context need only be passed through the transformer once and that added context is limited to a relatively small window. Furthermore, we can still follow standard procedure in shuffling  sentences at each epoch during training, since context is encoded on a per-sentence level. We use this approach throughout this paper. \n\n\\section{Baseline Parameter Experiments}\n\\label{sec:baseline_experiments}\n\\vspace{-2mm}\n\nAs mentioned in the introduction, there are two common architectures for transformer-based NER, namely fine-tuning and feature-based approaches. In this section, we briefly introduce the differences between both approaches and conduct a study to identify best hyperparameters for each. The best respective setups are then used in the final comparative evaluation in Section~\\ref{sec:evaluation}. \n\n\\begin{table*}\n\\vspace{-2mm}\n\\centering\n\\resizebox{440px}{!}{\n\\begin{tabular}{lccccc}\n\\hline\n Fine-tuning Approach &  \\textsc{En} & \\textsc{De}      & $\\textsc{De}_{06}$ & \\textsc{Nl} & \\textsc{Es} \\\\ \\hline\nTransformer-Linear       &      96.64 $\\pm$ 0.14        & 89.06 $\\pm$ 0.18 & 91.86 $\\pm$ 0.41   &    93.41 $\\pm$ 0.19         & 88.95 $\\pm$ 0.19 \\\\\n + \\textit{Document features}            &   96.82 $\\pm$ 0.07           & \\textbf{89.79} $\\pm$ 0.13 & \\textbf{93.09} $\\pm$ 0.06   &     94.19 $\\pm$ 0.14        & 90.34 $\\pm$ 0.27 \\\\ \n  + WE                 &     96.82 $\\pm$ 0.13         & 88.96 $\\pm$ 0.10 & 92.12 $\\pm$ 0.10   &    93.51 $\\pm$ 0.09         & 89.09 $\\pm$ 0.36 \\\\\n+ WE  + \\textit{Document features}       &    \\textbf{97.02 $\\pm$ 0.09}          & 89.74 $\\pm$ 0.46 & 92.83 $\\pm$ 0.12   &   94.01 $\\pm$ 0.27          & 90.17 $\\pm$ 0.25 \\\\ \\hline\n Transformer-CRF   &    96.79 $\\pm$ 0.11          & 88.52 $\\pm$ 0.10 & 92.21 $\\pm$ 0.07   &    93.61 $\\pm$ 0.15         & 88.77 $\\pm$ 0.20 \\\\ \n + \\textit{Document features}            &    96.90 $\\pm$ 0.06          & 89.67 $\\pm$ 0.24 & 92.87 $\\pm$ 0.21   &     94.16 $\\pm$ 0.07        & \\textbf{90.56} $\\pm$ 0.09 \\\\ \n  + WE                 &   96.79 $\\pm$ 0.15           & 88.84 $\\pm$ 0.15 & 91.97 $\\pm$ 0.09   &    93.36 $\\pm$ 0.04         & 88.63 $\\pm$ 0.47 \\\\\n\n+ WE + \\textit{Document features}        &   96.87 $\\pm$ 0.00           & 89.69 $\\pm$ 0.22 & 92.88 $\\pm$ 0.26   &     \\textbf{94.34 $\\pm$ 0.13}        & 90.37 $\\pm$ 0.14 \\\\\\hline\n\\end{tabular}%\n}\n\\vspace{-1mm}\n\\caption{Evaluation of different variants using the fine-tuning approach. The evaluation is performed against the \\textbf{development set} of all 4 languages of the CoNLL-03 shared task for NER.}\n\\label{tab:finetuning_development_dataset}\n\\vspace{-3mm}\n\\end{table*}\n\\subsection{Setup}\n\n\\noindent\n\\textbf{Data set.} We use the development datasets of the CoNLL shared tasks \\citep{tjong-kim-sang-de-meulder-2003-introduction,tjong-kim-sang-2002-introduction} for NER on four languages (English, German, Dutch and Spanish). Following \\citet{yu-etal-2020-named} we report results for both the original and revised dataset for German (denoted as $\\textsc{De}_{06}$).\n\n\\noindent\n\\textbf{Transformer model.}\nIn all experiments in this section, we employ the multilingual XLM-RoBERTa (XLM-R) transformer model proposed by \\citet{conneau2019unsupervised}. We use the \\texttt{xlm-roberta-large} model in our experiments, trained on 2.5TB of data from a cleaned Common Crawl corpus \\cite{wenzek-etal-2020-ccnet} for 100 different languages\n\n\\noindent\n\\textbf{Embeddings (+WE).}\nFor each setup we experiment with concatenating classic word embeddings to the word-level representations obtained from the transformer model. Following~\\citet{akbik-etal-2018-contextual}, we use \\textsc{GloVe} embeddings \\citep{pennington-etal-2014-glove} for English and \\textsc{FastText} embeddings \\citep{bojanowski-etal-2017-enriching} for other languages.\n\n\\subsection{First Approach: Fine-Tuning}\n\\label{sec:fine_tuning}\n\\vspace{-1mm}\n\nFine-tuning approaches typically only add a single linear layer to a transformer and fine-tune the entire architecture on the NER task. To bridge the difference between subtoken modeling and token-level predictions, they apply \\textit{subword pooling} to create token-level representations which are then passed to the final linear layer. Conceptually, this approach has the advantage that everything is modeled in a single architecture that is fine-tuned as a whole. More details on parameters and architecture are provided in the Appendix. \n\n\\noindent \n\\textbf{Evaluated variants.} \nWe compare two variants: \n\n\\begin{description}\n\\vspace{-1mm}\n\\item[Transformer-Linear] In the first, we use the standard approach of adding a simple linear classifier on top of the transformer to directly predict tags. \n\\vspace{-1mm}\n\\item[Transformer-CRF] In the second, we evaluate if it is helpful to add a conditional random fields (CRF) decoder between the transformer and the linear classifier~\\cite{souza2019portuguese}. \n\\end{description}\n\\vspace{-1mm}\n\\noindent\nResults are listed in Table~\\ref{tab:finetuning_development_dataset}.\n\n\\subsection{Second Approach: Feature-Based}\n\\label{sec:feature_based}\n\\vspace{-1mm}\n\nFeature-based approaches instead use the transformer only to generate embeddings for each word in a sentence and use these as input into a standard sequence labeling architecture, most commonly a LSTM-CRF~\\cite{2015arXiv150801991H}. The transformer weights are frozen so that training is limited to the LSTM-CRF. Conceptually, this approach benefits from a well-understood model training procedure that includes a real stopping criterion. See Appendix B for more details on training parameters. \n\n\\noindent \n\\textbf{Evaluated variants.} \nWe compare two variants:\n\\begin{description}\n\\vspace{-1mm}\n\\item[All-layer-mean] In the first, we obtain embeddings for each token using mean pooling across all transformer layers, including the word embedding layer. This representation has the same length as the hidden size for each transformer layer. This approach is inspired by the ELMO-style~\\cite{peters-etal-2018-deep} \"scalar mix\".\n\\vspace{-1mm}\n\\item[Last-four-layers] In the second, we follow~\\citet{devlin-etal-2019-bert} to only use the last four transformer layers for each token and concatenate their representations into a final representation for each token. It thus has four times the length of the transformer layer hidden size.\n\\end{description}\nThe results for English\\footnote{Other languages show similar results (omitted for space).} are shown in Table~\\ref{tab:feature_based_development_dataset}.\n\\begin{table}\n\\centering\n\\resizebox{190px}{!}{\n\\begin{tabular}{lc}\n\\hline\n Feature-based Approach             & \\textsc{En}                \\\\ \\hline %& \\textsc{De}   & \\textsc{Es}   & \\textsc{Nl}               \\\\ \\hline\n \\textsc{LSTM-CRF} (last-four-layers)  & 91.17 $\\pm$ 0.29           \\\\        %&               &               & 83.12 $\\pm$ 0.63          \\\\\n + \\textit{Document features}                          & 94.23 $\\pm$ 0.19           \\\\        %&               &               & 85.67 $\\pm$ 0.88          \\\\\n  + WE                               & 92.19 $\\pm$ 0.46           \\\\        %&               &               & 84.39 $\\pm$ 1.53          \\\\\n\n  + WE   + \\textit{Document features}                  & 94.61 $\\pm$ 0.10           \\\\ \\hline %& 85.59 / - / - & 87.13 / - / - & 87.22 $\\pm$ 0.17          \\\\\\hline\n \\textsc{LSTM-CRF} (all-layer-mean)    & 94.37 $\\pm$ 0.06           \\\\        %&               &               & 89.47 $\\pm$ 0.21          \\\\\n + \\textit{Document features}                          & 96.09 $\\pm$ 0.07           \\\\        %&               &               & 91.63 $\\pm$ 0.36          \\\\\n  + WE                               & 95.63 $\\pm$ 0.04           \\\\        %&               &               & 90.91 $\\pm$ 0.28          \\\\\n\n + WE + \\textit{Document features}                      & \\textbf{96.53} $\\pm$ 0.10  \\\\ \\hline %&               &               & \\textbf{92.52 $\\pm$ 0.05} \\\\\\hline\n\\end{tabular}%\n}\n\\vspace{-2mm}\n\\caption{Evaluation of feature-based approach on CoNLL-03 \\textbf{development set}.}\n\\label{tab:feature_based_development_dataset}\n\\vspace{-4mm}\n\\end{table}\n\n\\begin{table*}\n\\vspace{-2mm}\n\\centering\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{lllllll}\n\\hline\n Approach &  \\textit{Doc. features?} & \\textsc{En} & \\textsc{De} & $\\textsc{De}_{06}$ & \\textsc{Nl} & \\textsc{Es} \\\\ \\hline\n \\textit{Feature-based}  & & & & & & \\\\ \n \n   LSTM-CRF (all layer mean) & no & 91.83 $\\pm$ 0.06 & 82.88 $\\pm$ 0.28 & 87.35 $\\pm$ 0.17 & 89.87 $\\pm$ 0.45 & 88.78 $\\pm$ 0.08\\\\ \n    LSTM-CRF (all layer mean) & yes & 93.12 $\\pm$ 0.14 & 84.86 $\\pm$ 0.11 & 89.88 $\\pm$ 0.26 & 91.73 $\\pm$ 0.21 & 88.98 $\\pm$ 0.11\\\\  \\hline\n \\textit{Fine-tuning}  & & & & & & \\\\ \n   Transformer-Linear  & no & 92.79 $\\pm$ 0.10 & 86.60 $\\pm$ 0.43 & 90.04 $\\pm$ 0.37 & 93.50 $\\pm$ 0.15 & 89.94 $\\pm$ 0.24 \\\\ \n  Transformer-Linear & yes &  93.64 $\\pm$ 0.05 & 86.99 $\\pm$ 0.24 & 91.55 $\\pm$ 0.07 & 94.87 $\\pm$ 0.20 & 90.14 $\\pm$ 0.14 \\\\ \\hline\n  \\textit{Fine-tuning (Ablations)}  & & & & & & \\\\ \n  \n   Transformer-Linear &  yes (\\textit{+enforce})  & 93.75 $\\pm$ 0.16 & 87.35 $\\pm$ 0.15 & 91.33 $\\pm$ 0.18 & \\textbf{95.21 $\\pm$ 0.08} & -- \\\\ \n  \n  \n   Transformer-Linear (\\textsc{+dev}) &   yes (\\textit{+enforce})     & 94.09 $\\pm$ 0.07 & \\textbf{88.34 $\\pm$ 0.36} & \\textbf{92.23 $\\pm$ 0.21} & 95.19 $\\pm$ 0.32  & -- \\\\  \\hline\n\n \n \\textit{Best published} & & & & & & \\\\ \n \\citet{akbik2019naacl} & pooling & 93.18 $\\pm$ 0.09 & -- & 88.27 $\\pm$ 0.30 & 90.44 $\\pm$ 0.20 & -- \\\\\n \\citet{yu-etal-2020-named} & yes & 93.5 & 86.4 & 90.3 & 93.7 & \\textbf{90.3} \\\\\n \\citet{strakova-etal-2019-neural} & yes & 93.38 & 85.10 & -- & 92.69 & 88.81 \\\\ \n \\citet{yamada-etal-2020-luke} & yes & \\textbf{94.3} & -- & -- & -- & -- \\\\\\hline\n \n\\end{tabular}%\n}\n\\vspace{-2mm}\n\\caption{Comparative evaluation of best configurations of fine-tuning and feature-based approaches on test data.\n}\n\\label{tab:stats_best_configurations}\n\\vspace{-2mm}\n\\end{table*}\n\n\\subsection{Results: Best Configurations}\n\\vspace{-1mm}\n\nWe evaluate both approaches in each variant in all possible combinations of adding standard word embeddings \"(+WE)\" and document-level features \"(+\\textit{Document features})\". Each setup is run three times to report average F1 and standard deviation.\n\n\\noindent\n\\textbf{Results.} For fine-tuning, we find that additional word embeddings and using a CRF decoder improves results only for some languages, and often only minimally so (see Table~\\ref{tab:finetuning_development_dataset}). We thus choose a minimal Transformer-Linear architecture. For the feature-based approach, we find that an all-layer-mean strategy and adding word embeddings very clearly yields the best results (see Table~\\ref{tab:feature_based_development_dataset}). \n\n\\begin{table}\n\\centering\n\\resizebox{220pt}{!}{\n\\begin{tabular}{ccccccc}\n\\hline\n CW &  \\textsc{En}   & \\textsc{De}    & $\\textsc{De}_{06}$ & \\textsc{Nl}    & \\textsc{Es}    & Avg. \\\\ \\hline\n 48             & 96.86\t\t\t & 89.47\t\t  & 92.63\t\t\t   & 94.09\t\t    & 90.31\t         & 92.67 \\\\\n 64             & 96.82          & 89.64          & \\textbf{92.87}     & 94.19          & \\textbf{90.34} & \\textbf{92.77} \\\\\n 96             & \\textbf{96.90} & \\textbf{89.67} & 92.58              & 94.03          & 90.31          & 92.70 \\\\\n 128            & \\textbf{96.90} & 88.97          & 92.56              & \\textbf{94.22} & 90.15          & 92.56 \\\\\n \\hline\n\\end{tabular}%\n}\n\\vspace{-2mm}\n\\caption{Comparative evaluation of context window sizes of fine-tuning approach on development set.}\n\\label{tab:comparison_different_context_window_sizes}\n\\end{table}\n\n\\section{Comparative Evaluation}\n\\label{sec:evaluation}\n\\vspace{-1mm}\n\nWith the best configurations identified in Section~\\ref{sec:baseline_experiments} on the development data,    \nwe conduct a final comparative evaluation on the test splits of the CoNLL-03 datasets, with and without document features.\n\n\\subsection{Main Results}\n\n\\vspace{-1mm}\nThe evaluation results are listed in Table~\\ref{tab:stats_best_configurations}. We make the following observations:\n \n\n\\begin{table}\n\\centering\n\\resizebox{220pt}{!}{\n\\begin{tabular}{lccccc}\n\\hline\n Entity &  \\textsc{En} & \\textsc{De}       & $\\textsc{De}_{06}$ & \\textsc{Nl}       & \\textsc{Es} \\\\ \\hline\n {\\tt LOC}    & +0.44        & +0.23             & +1.97              & \\underline{-0.74} & +0.17 \\\\\n {\\tt MISC}   & +0.22        & \\underline{-0.90} & +1.66              & +1.16             & +0.72 \\\\\n {\\tt ORG}    & +1.21        & +0.56             & +0.74              & +1.66             & +0.11 \\\\\n {\\tt PER}    & +1.19        & +1.15             & +1.50              & \\underline{-0.34} & +0.14 \\\\\n \\hline\n\\end{tabular}%\n}\n\\vspace{-2mm}\n\\caption{Relative change in F1 for different entity types and languages when adding document-level features.}\n\\label{tab:comparison_context_vs_no_context}\n\\vspace{-2mm}\n\\end{table}\n\n\\noindent\n\\textbf{Fine-tuning document-level features best.} As Table~\\ref{tab:stats_best_configurations} shows, we find that fine-tuning outperforms the feature-based approach across all experiments ($\\approx \\uparrow$2 pp on average). \nSimilarly, we find that document-level features clearly outperform sentence-level features ($\\uparrow$1.15 pp on average). We thus find fine-tuning with document-level features to work best across all languages. \n\n\\noindent\n\\textbf{Enforcing document boundaries.} For fine-tuning, we also test an ablation in which we truncate document-features at document boundaries, meaning that context can only come from the same document. As the columns \"yes (\\textit{+enforce})\" in Table~\\ref{tab:stats_best_configurations} show, this increases F1-score across nearly all experiments. Our initial expectation that transformers would learn automatically to respect document boundaries (marked up in all datasets except Spanish) did not materialize, thus we recommend enforcing document boundaries if possible.\n\n\\noindent \n\\textbf{New state-of-the-art results.} \nCombining fine-tuning, and strict document-level features yields new state-of-the-art scores for several datasets. Especially when including dev data in training (indicated as \\textsc{+dev} in Table~\\ref{tab:stats_best_configurations}) as is possible for fine-tuning as no stopping criterion is used. For German, we outperform ~\\cite{yu-etal-2020-named} by $\\uparrow$1.81 pp and $\\approx \\uparrow$2 pp on the original and revised German datasets respectively. For Dutch, we see an increase of by $\\uparrow$1.5 pp over the next best approach. While we do not set new state-of-the-art scores for English and Spanish, our results are very competitive. \n\n\\subsection{Analysis}\n\\vspace{-1mm}\n\\noindent\n\\textbf{Impact of context window size (Table~\\ref{tab:comparison_different_context_window_sizes}).}\nWe evaluate the impact of the number of surrounding tokens used in document-level contexts on\nperformance using the best configuration for fine-tuning approach. The context window is searched in $[48, 64, 96, 128]$. As Table~\\ref{tab:comparison_different_context_window_sizes} shows, impact is marginal, with 64 the best across languages. \n\n\\noindent\n\\textbf{Entity type analysis (Table \\ref{tab:comparison_context_vs_no_context}).}\nWe perform a per-type analysis to compare average results across entity types with and without document-level features. We find that while the difference in F1-score depend on the type and the language, in particular the \n{\\tt ORG} (organization) and {\\tt PER} (person) entity types improve the most when including document-level features, indicating that cross-sentence contexts are most important here.\n\n\\vspace{-1mm}\n\\section{Conclusion}\n\\vspace{-1mm}\n\nWe evaluated document-level features in two commonly used NER architectures, for which we determined best setups. Our experiments show that document-level features significantly improve overall F1-score and that fine-tuning outperforms the feature-based LSTM-CRF. We also surprisingly find that enforcing document boundaries improves results, potentially adding to recent evidence that transformers have difficulties in learning positional signals~\\cite{huang-etal-2020-improve}. We integrate our approach as the \"FLERT\"-extension\\footnote{To be released with \\textsc{Flair} version 0.8.} into the \\textsc{Flair} framework, to enable the research community to leverage our best determined setups for training and applying state-of-the-art NER models. \n\n\\newpage\n\\appendix\n\\clearpage\n\\newpage\n\\section{Appendix}\n\n\\subsection{Training: Fine-tuning Approach}\n\nFine-tuning only adds a single linear layer to a transformer and fine-tunes the entire architecture on the NER task. To bridge the difference between subtoken modeling and token-level predictions, they apply \\textit{subword pooling} to create token-level representations which are then passed to the final linear layer. A common subword pooling strategy is \"first\"~\\cite{devlin-etal-2019-bert} which uses the representation of the first subtoken for the entire token. See Figure~\\ref{fig:subword_pooling} for an illustration. \n\n\\begin{table}[h]\n\\begin{center}\n\\begin{tabular}{ l l }\n\\toprule\nParameter & Value \\\\\n\\midrule\nTransformer layers & last \\\\\nLearning rate & 5e-6 \\\\\nMini batch size & 4 \\\\\nMax epochs & 20 \\\\\nOptimizer & AdamW \\\\ % FIXME: table width ~\\cite{loshchilov2018decoupled} \\\\\nScheduler & One-cycle LR \\\\\nSubword pooling & first \\\\\n\\bottomrule\n\\end{tabular}\n\\end{center}\n\\vspace{-2mm}\n\\caption{\\label{tab:finetuning_params} Parameters used for fine-tuning.}\n\\end{table}\n\n\\noindent \n\\textbf{Training procedure.} \nTo train this architecture, prior works typically use the AdamW~\\cite{loshchilov2018decoupled} optimizer, a very small learning rate and a small, fixed number of epochs as a hard-coded stopping criterion~\\cite{conneau2019unsupervised}. We adopt a one-cycle training strategy~\\cite{2018arXiv180309820S}, inspired from the HuggingFace transformers~\\cite{wolf2019huggingface} implementation, in which the learning rate linearly decreases until it reaches $0$ by the end of the training. Table~\\ref{tab:finetuning_params} lists the architecture parameters we use across all our experiments. \n\n\\begin{figure}[t!]\n \\centering\n \\includegraphics[width=\\linewidth]{figures/bert_first_subword_pooling.pdf}\n \\captionof{figure}{\n Illustration of first subword pooling. The input \"The Eiffel Tower\" is subword-tokenized, splitting \"Eiffel\" into three subwords (shaded green). Only the first (\"E\") is used as representation for \"Eiffel\".\n}\n \\label{fig:subword_pooling}\n\\end{figure}\n\n\\subsection{Training: Feature-based Approach}\n\nFigure~\\ref{overview-bert-feature-based-document-level} gives an overview of the feature-based approach: Word representations are extracted from the transformer by either averaging over all layers (all-layer-mean) or by concatenating the representations of the last four layers (last-four-layers). These are then input into a standard LSTM-CRF architecture~\\cite{2015arXiv150801991H} as features. We again use the subword pooling strategy illustrated in Figure~\\ref{fig:subword_pooling}. \n\n\\noindent \n\\textbf{Training procedure.} We adopt the standard training procedure used in earlier works. We with SGD with a larger learning rate that is annealed against the development data. Training terminates when the learning rate becomes too small. \nThe parameters used for training a feature-based model are shown in Table~\\ref{ner-training-parameters-feature-based}. \n\n\\begin{table}[h!]\n\\begin{center}\n\\begin{tabular}{ l l }\n\\toprule\nParameter & Value \\\\\n\\midrule\nLSTM hidden size & 256 \\\\\nLearning rate & 0.1 \\\\\nMini batch size & 16 \\\\\nMax epochs & 500 \\\\\nOptimizer & SGD \\\\\nSubword pooling & first \\\\\n\\bottomrule\n\\end{tabular}\n\\end{center}\n\\caption{\\label{ner-training-parameters-feature-based} Parameters for feature-based approach.}\n\\end{table}\n\n\\subsection{Reproducibility Checklist}\n\n\\noindent \n\\textbf{Dataset statistics.} Table \\ref{tab:dataset_stats} shows the number of sentences for for each dataset.\n\n\\begin{table}[h!]\n\\centering\n\\begin{tabular}{lllll}\n\\hline\n Split       &  \\textsc{En} & \\textsc{De} / $\\textsc{De}_{06}$ & \\textsc{Nl} & \\textsc{Es} \\\\ \\hline\n Train       & 14,987       & 12,705                           & 16,093      & 8,323 \\\\\n Dev         & 3,466        & 3,068                            & 2,969       & 1,915 \\\\\n Test        & 3,684        & 3,160                            & 5,314       & 1,517 \\\\\n \\hline\n\\end{tabular}%\n\\caption{Number of sentences for each CoNLL dataset.}\n\\label{tab:dataset_stats}\n\\end{table}\n\n\\noindent\n\\textbf{Average training runtime.} We conduct experiments on a NVIDIA V-100 (16GB) for fine-tuning and a NVIDIA RTX 3090 TI (24GB) for the feature-based approach. We report average training times for our best configurations in Table \\ref{tab:training_runtimes}.\n\n\\begin{table}[h!]\n\\centering\n\\begin{tabular}{ccccc}\n\\hline\n Approach       &  \\textsc{En} & \\textsc{De} / $\\textsc{De}_{06}$ & \\textsc{Nl} & \\textsc{Es} \\\\ \\hline\n Fine-Tuning    &  10h            & 10h                        & 10h             &  5h \\\\\n Feature-based  &  7h          &  5.5h                 &  5.75h      & 5.5h \\\\\n \\hline\n\\end{tabular}%\n\\caption{Average training runtimes for our approaches.}\n\\label{tab:training_runtimes}\n\\end{table}\n\n\\noindent\n\\textbf{Number of model parameters.} The reported number of model parameters from \\citet{conneau2019unsupervised} for XLM-R is 550M. Our fine-tuned model has 560M parameters ($\\uparrow$1.8\\%), whereas the feature-based model comes with 564M parameters ($\\uparrow$2.5\\%).\n\n\\noindent\n\\textbf{Evaluation metrics.} We evaluate our models using the CoNLL-2003 evaluation script\\footnote{\\url{https://www.clips.uantwerpen.be/conll2003/ner/bin/conlleval}} and report averaged F1-score over three runs.\n\n\\begin{figure*}[t]\n \\centering\n \\includegraphics[width=\\linewidth]{figures/bert_feature_based.pdf}\n \\captionof{figure}{\n Overview of feature-based approach. Self-attention is calculated over all input tokens (incl. left and right context). The final representation for each token in the sentence (\"I love Paris\", shaded green) can be calculated as a) mean over all layers of transformer-based model or b) concatenating the last four layers. \n}\n \\label{overview-bert-feature-based-document-level}\n\\end{figure*}\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{Improving Named Entity Recognition by \\\\ External Context Retrieving and Cooperative Learning}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\n\nRecent advances in Named Entity Recognition (NER) show that document-level contexts can significantly improve model performance. In many application scenarios, however, such contexts are not available. In this paper, we propose to find external contexts of a sentence by retrieving and selecting a set of semantically relevant texts through a search engine, with the original sentence as the query. We find empirically that the contextual representations computed on the retrieval-based input view, constructed through the concatenation of a sentence and its external contexts, can achieve significantly improved performance compared to the original input view based only on the sentence. Furthermore, we can improve the model performance of both input views by Cooperative Learning, a training method that encourages the two input views to produce similar contextual representations or output label distributions. Experiments show that our approach can achieve new state-of-the-art performance on 8 NER data sets across 5 domains.\\footnote{Our newest code is publicly available at \\url{https://github.com/modelscope/AdaSeq/tree/master/examples/RaNER}. The older version: \\url{https://github.com/Alibaba-NLP/CLNER}.}\n\n\\end{abstract}\n\n\\section{Introduction}\nPretrained contextual embeddings such as ELMo \\citep{peters-etal-2018-deep}, Flair \\citep{akbik-etal-2018-contextual} and BERT \\citep{devlin-etal-2019-bert} have significantly improved the accuracy of Named Entity Recognition (NER) models. Recent work \\citep{devlin-etal-2019-bert,yu-etal-2020-named,yamada-etal-2020-luke} found that including document-level contexts of the target sentence in the input of contextual embeddings methods can further boost the accuracy of NER models. \nHowever, there are a lot of application scenarios in which document-level contexts are unavailable in practice. For example, there are sometimes no available contexts in users' search queries, tweets and short comments in various domains such as social media and E-commerce domains. When professional annotators annotate ambiguous named entities in such cases, they usually rely on domain knowledge for disambiguation. This kind of knowledge can often be found through a search engine. Moreover, when the annotators are not sure about a certain entity, they are usually encouraged to find related knowledge through a search engine \\citep{wang-etal-2019-crossweigh}. Therefore, we believe that NER models can benefit from such a process as well. \n\n\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[scale=0.215]{motivating.pdf}\n\t\\caption{A motivating example from WNUT-17 dataset. The retrieved texts help the model to correctly predict the named entities of ``democrats'' and ``republican''.}\n\t\\label{fig:motivate}\n\\end{figure}\n\nIn this paper, we propose to improve NER models by retrieving texts related to the input sentence by an off-the-shelf search engine. We re-rank the retrieved texts according to their semantic relevance to the input sentence and select several top-ranking texts as the external contexts. Consequently, we concatenate the input sentence and external contexts together as a new retrieval-based input view and feed it to the pretrained contextual embedding module, so that the resulting semantic representations of the input tokens can be improved. The token representations are then fed into a CRF layer for named entity prediction. A motivating example is shown in Figure \\ref{fig:motivate}.\n\nMoreover, we consider utilizing the new input view to improve model performance with the original input view that does not have external contexts. This can be useful in application scenarios when external contexts are unavailable or undesirable (e.g., in time-critical scenarios). To this end, we propose Cooperative Learning (CL) that encourages the two input views to produce similar predictions. We propose two approaches to CL which minimize either the $L_2$ distances between the token representations of the two input views or the KullbackLeibler (KL) divergence between the prediction distributions of the two input views during training. \n\nOur experiments show that including the retrieved external contexts can significantly improve the accuracy of NER models on 8 NER datasets from 5 domains. With CL, the accuracy of the NER models with both input views can be further improved. Our approaches outperform previous state-of-the-art approaches in each domain. \n\nThe contributions of this paper are:\n\\begin{enumerate}[leftmargin=*]\n    \\item We propose a simple and straight-forward way to improve the contextual representation of an input sentence through retrieving related texts using a search engine. We take the retrieved texts together with the input sentence as a new retrieval-based view.\n    \\item \n    We propose Cooperative Learning to jointly improve the accuracy of both input views in a unified model. We propose two approaches in CL based on the $L_2$ norm and KL divergence respectively. CL can utilize unlabeled data for further improvement.\n    \\item We show the effectiveness of our approaches in several NER datasets across 5 domains and our approaches achieve state-of-the-art accuracy. By leveraging a large amount of unlabeled data, the performance can be further improved. % It will be more clearer when all results are got\n\\end{enumerate}\n\\begin{figure*}[ht]\n\t\\centering\n\t\\includegraphics[scale=0.45]{framework.pdf}\n\t\\caption{The architecture of our framework. An input sentence $\\xvec$ is fed into a search engine to get $k$ related texts. The related texts are then fed into the re-ranking module. The framework selects $l$ highest ranking related texts output from the re-ranking module and feeds the texts to a transformer-based model together with the input sentence. Finally, we calculate the negative likelihood loss $\\mcL_{\\text{NLL}}$ and $\\mcL_{\\text{NLL-EXT}}$ together with the CL loss (either $\\mcL_{\\text{CL-}L_2}$ or $\\mcL_{\\text{CL-KL}}$).}\n\t\\label{fig:architecture}\n\\end{figure*}\n\n\\section{Framework}\nGiven a sentence of $n$ tokens $\\vx = \\{x_1, \\cdots, x_n\\}$, the input sentence is fed into a search engine as a query. The search engine returns the top $k$ relevant texts $\\{\\hat{\\vx}_1, \\cdots, \\hat{\\vx}_k\\}$. Our framework feeds these texts into a re-ranking model. We concatenate $l$ top-ranking texts output from the re-ranking model as the external contexts. The NER model is fed with either an input view with the input sentence (original input view) or a concatenation of the input sentence and external contexts (retrieval-based input view) as input. The model outputs the predictions of labels $\\vy = \\{y_1, \\cdots, y_n\\}$ at each position based on the CRF layer. To further improve the model, we use Cooperative Learning to train a unified model that is strong in both input views. With CL, the model is additionally constrained to be consistent in the internal representations or the output distributions of both input views. The architecture of our framework is shown in Figure \\ref{fig:architecture}.\n\n\\subsection{Re-ranking}\nGiven an input sentence as a search query, the search engine returns ranked relevant texts. %%However, some of the results may not be semantically similar to the query because the off-the-shelf search engines are usually highly optimized for a fast speed over a large set of documents. The results may contain some noise and the relevance of each result may not be correctly ranked. \nHowever, the off-the-shelf search engine is highly optimized for a fast speed over a large set of documents, so it may sometimes produce semantically irrelevant results or rank the results using inaccurate relevance scores.\nSince the NER task targets at semantically recognizing named entities, it is more helpful if the relevant texts are semantically similar to the input sentence. Therefore, we need to re-rank the retrieved texts so that the most semantically relevant texts are chosen. We propose to apply BERTScore \\citep{Zhang*2020BERTScore:} to score the relatedness of each retrieved text to the input sentence. BERTScore is a language generation metric that calculates a sum of cosine similarity between token representations of two sentences. Therefore, it is more likely that the search query and the retrieved texts have strong semantic relations when BERTScore is large. The token representations are generated from pretrained contextual embeddings such as BERT. Given the corresponding pre-normalized token representations $\\{\\rvec_1, \\cdots, \\rvec_n\\}$ of the input sentence $\\vx$ and the pre-normalized token representations $\\{\\hat{\\rvec}_1, \\cdots, \\hat{\\rvec}_m\\}$ of a certain retrieved text $\\hat{\\vx}$ with $m$ words, the Precision (P), Recall (R) of BERTScore measure the semantic similarities from one to another:\n\\begin{align*} % For space issue, we may write R and P in a single line\n    &\\text{R} =\\frac{1}{n} \\sum_{x_i \\in \\vx}   \\max_{\\hat{x}_j \\in \\hat{\\vx}} \\rvec_i^\\top \\hat{\\rvec}_{j};\\;\\;\n    \\text{P} = \\frac{1}{m}  \\sum_{\\hat{x}_j\\in \\hat{\\vx}}   \\max_{x_i\\in \\vx}  \\rvec_{i}^\\top \\hat{\\rvec}_{j}\n\\end{align*}\n We re-rank the retrieved texts by the F1 scores $\\text{F1}{=} 2\\frac{\\text{P} \\cdot \\text{R} }{\\text{P} + \\text{R} }$ and concatenate $l$ top-ranking texts $\\{\\hat{\\vx}_1, \\cdots, \\hat{\\vx}_l\\}$ with F1 scores together as the external contexts:\n\\begin{displaymath}\n\\tilde{\\vx} = [sep\\_token; \\hat{\\vx}_1; \\cdots; \\hat{\\vx}_l]\n\\end{displaymath} \nwhere $sep\\_token$ is a special token representing a separate of sentences in the transformer-based pretrained contextual embeddings (for example, ``[SEP]'' in BERT).\n\n\\subsection{NER Model}\nWe solve the NER task as a sequence labeling problem. We apply a neural model with a CRF layer, which is one of the most popular state-of-the-art approaches to the task \\citep{lample-etal-2016-neural,ma-hovy-2016-end,akbik-etal-2019-pooled}. In the sequence labeling model, the input sentence $\\vx$ is fed into a transformer-based pretrained contextual embeddings model to get the token representations $\\{\\vv_1, \\cdots, \\vv_n\\}$ by $\\vv_i {=} \\text{embed}_i (\\vx)$.\nThe token representations are fed into a CRF layer to get the conditional probability $p_\\theta(\\vy|\\vx)$:\n\\begin{align}\n    \\psi(y', y, \\vv_i) &= \\exp(\\Wvec_{y}^{T} \\vv_i + \\bvec_{y',y}) \\label{eq:psi}\\\\\n    p_\\theta(\\vy|\\vx) &= \\frac{\\prod\\limits_{i=1}^{n} \\psi(y_{i-1}, y_i, \\vv_i)}{\\sum\\limits_{\\vy' \\in \\mathcal{Y}(\\vx)} \\prod\\limits_{i=1}^{n} \\psi(y'_{i-1}, y'_i, \\vv_i)}\\nonumber\n\\end{align}\nwhere $\\psi$ is the potential function and $\\theta$ represents the model parameters. $\\mathcal{Y}(\\vx)$ denotes the set of all possible label sequences given $\\vx$. $y_0$ is defined to be a special start symbol. $\\Wvec^{T}\\in \\sR^{t\\times d}$ and $\\bvec \\in \\sR^{t \\times t}$ are parameters computing emission and transition scores respectively. $d$ is the hidden size of $\\vv$ and $t$ is the size of the label set. During training, the negative log-likelihood loss for the input sequence with gold labels $\\vy^*$ is defined by:\n\\begin{align}\n\\mcL_{\\text{NLL}}(\\theta) = - \\log p_\\theta(\\vy^*|\\vx) \\label{eq:nll_loss}\n\\end{align}\n\nIn our approach, we concatenate the external contexts $\\tilde{\\vx}$ at the end of the input sentence $\\vx$ to form the retrieval-based input view. %Therefore, lower ranked texts are far away from the input sentence and have less effect to the input sentence. \nThe token representations are now given by:\n\\begin{displaymath}\n\\{\\vv_1^{\\prime},\\cdots,\\vv^{\\prime}_n,\\cdots\\} = \\text{embed} ([\\vx;\\tilde{\\vx}])\n\\end{displaymath}\nThe architecture of our NER model is shown in Figure \\ref{fig:ner}. Now the conditional probability $p_\\theta(\\vy|\\vx)$ becomes $p_\\theta(\\vy|\\vx, \\tilde{\\vx})$. \nThe loss function in Eq. \\ref{eq:nll_loss} becomes:\n\\begin{align}\n\\mcL_{\\text{NLL-EXT}}(\\theta) = - \\log p_\\theta(\\vy^*|\\vx,\\tilde{\\vx}) \\label{eq:nll_loss_x}\n\\end{align}\n\n\\subsection{Cooperative Learning}\nIn practice, there are two application scenarios for the NER model: 1) offline prediction, which requires high accuracy of the prediction but the prediction speed is less emphasized; 2) online serving, which requires a faster prediction speed. The retrieval-based input view meets the requirement of the first scenario for its strong token representations. However, it does not meet the requirement of the second scenario. The external contexts are usually significantly longer than the input sentence and a search engine may not meet the latency requirements. These two issues significantly slow down the prediction speed of the model. Therefore, it is essential to improve the accuracy of the original input views in a unified model to meet these two scenarios.\n\n\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[scale=0.28]{transformer_structure.pdf}\n\t\\caption{An illustration of our NER model architecture. ``[CLS]'' and ``[SEP]'' are an example of cls token and sep token in the embedding. }\n\t\\label{fig:ner}\n\\end{figure}\n\nCooperative Learning targets at using the retrieval-based input view to help improve the accuracy of the model when there are no external contexts available. CL adds constraints between the internal representations or the output distributions between two input views to enforce that the predictions of both views should be near. The objective function of CL is calculated by:\n\\begin{align}\n\\mcL_{\\text{CL}}(\\theta)&=D(h([\\vx;\\tilde{\\vx}]), h([\\vx])) \\label{eq:cl_loss}\n\\end{align}\nwhere $D$ is a distance function between a function $h$ with different inputs. Because the representations or the distributions with retrieval-based input view are usually informative, we do not backpropagate the gradient through $h([\\vx;\\tilde{\\vx}])$. We propose two approaches for CL.\n\n\\paragraph{Token Representations:} Stronger token representations usually lead to better accuracy on the task. Therefore, CL constrains the token representations of two input views to be similar. This helps the model learn to predict the token representations with external contexts even if the contexts are not available. In this approach, $D$ is the $L_2$ norm to represent the distances of the token representations:\n\\begin{align}\n\\mcL_{\\text{CL-$L_2$}}(\\theta)&=\\sum_{i=1}^n||\\vv_{i}^{\\prime}-\\vv_{i}||_2^2 \\label{eq:L2_loss}\n\\end{align}\n\n\\paragraph{Label Distributions:} Since CL enforces the label predictions of both input views to be similar, a straight-forward approach is constraining the label distributions predicted by the model to be similar with the two input views. In this approach, we use the KL divergence as the function $D$. Then objective function in Eq. \\ref{eq:cl_loss} becomes the KL divergence between $p_\\theta(\\vy|\\vx, \\tilde{\\vx})$ and $p_\\theta(\\vy|\\vx)$:\n\\begin{align}\n\\mcL_{\\text{CL-KL}}(\\theta)&{=}\\sum_{\\mathclap{\\vy \\in \\mcY(\\vx)}} \\text{KL}(p_\\theta(\\vy|\\vx, \\tilde{\\vx})||p_\\theta(\\vy|\\vx)) \\label{eq:cl_kl_loss}\n\\end{align}\nWith the CRF layer, the loss function is difficult to calculate because the output space of $p_\\theta(\\vy|\\bullet)$ is exponential in size. To alleviate this issue, we calculate the KL divergence between the marginal distributions $q_\\theta(y_i|\\vx,\\tilde{\\vx})$ and $q_\\theta(y_i|\\vx)$ at each position of the sentence to approximate Eq. \\ref{eq:cl_kl_loss}. The marginal distributions can be obtained using the forward-backward algorithm:\n\\begin{align}\n\\alpha(y_k)&=\\sum\\limits_{\\{y_0,\\dots,y_{k-1}\\}} \\prod\\limits_{i=1}^{k} \\psi(y_{i-1}, y_i, \\vv_i)\\nonumber\\\\\n\\beta(y_k) &= \\sum\\limits_{\\{y_{k+1},\\dots,y_n\\}} \\prod\\limits_{i=k+1}^{n} \\psi(y_{i-1}, y_i, \\vv_i)\\nonumber\\\\\nq_\\theta(y_k|\\vx)\n&\\propto \\alpha(y_k) \\times \\beta(y_k) \\label{eq:posterior}  %\\label{eq:forward-backward}\\\\\n\\end{align}\nAs mentioned earlier, we do not back-propagate the gradient through $p_\\theta(\\vy|\\vx, \\tilde{\\vx})$. Therefore calculating the KL divergence is equivalent to calculating the cross-entropy loss between $q(\\vy|\\vx, \\tilde{\\vx})$ and $q(\\vy|\\vx)$:\n\\begin{align}\n\\mcL_{\\text{CL-KL}}{(}\\theta{)}{=}{-}{\\sum_{\\mathclap{i=1}}^{n}}{\\sum_{\\mathclap{y_i=1}}^{t}} q_\\theta(y_i|\\vx,\\tilde{\\vx}){\\log} q_\\theta(y_i|\\vx) \\label{eq:cl_posterior_loss}\n\\end{align}\nTogether with the negative log-likelihood losses in Eq. \\ref{eq:nll_loss}, \\ref{eq:nll_loss_x}, the total loss in training is a summation of label losses and a CL loss:\n\\begin{align}\n\\mcL(\\theta) = \\mcL_{\\text{NLL}}(\\theta) + \\mcL_{\\text{NLL-EXT}}(\\theta) + \\mcL_{\\text{CL}}(\\theta) \\label{eq:final_loss}\n\\end{align}\nwhere $\\mcL_{\\text{CL}}(\\theta)$ can be one of the CL loss in Eq. \\ref{eq:L2_loss}, \\ref{eq:cl_posterior_loss} or a summation of both of them.\n\n\\section{Experiments}\n\n\\begin{table*}[t]\n\\centering\n\\small\n\\begin{tabular}{l|ccccccc}\n\\hlineB{4}\n& \\textbf{\\# Train} & \\textbf{\\# Dev} & \\textbf{\\# Test} & \\textbf{\\# Entity Labels} & \\textbf{Avg. Length} & \\textbf{Avg. Length w/ Context}\\\\\n\\hline\n\\textbf{\\textsc{WNUT-16}} & 2,394 & 1,000 & 3,849 & 10 & 19.41 & 138.58 \\\\ \n\\textbf{\\textsc{WNUT-17}} & 3,394 & 1,009 & 1,287 & 6 & 18.48 & 139.49 \\\\\n\\textbf{\\textsc{CoNLL-03}} & 14,987 & 3,466 & 3,684 & 4 & 13.64 & 116.23\\\\ \n\\textbf{\\textsc{CoNLL++}} & 14,987 & 3,466 & 3,466 & 4 & 13.64 & 116.23\\\\ \n\\textbf{\\textsc{BC5CDR}} & 4,560 & 4,581 & 4,797 & 2 & 25.91 & 144.13\\\\ \n\\textbf{\\textsc{NCBI}} & 5,424 & 923 & 940 & 1 & 25.01 & 135.76 \\\\ \n\\textbf{\\textsc{E-Commerce}} & 38,959 & 5,000 & 5,000 & 26 & 2.54 & 124.61\\\\ \n\\hlineB{4}\n\\end{tabular}\n\\caption{Statistics of the dateset split, number of entity types and the average lengths with and without external contexts.}\n\\label{tab:stat}\n\\end{table*}\n\n\\subsection{Settings}\n\\paragraph{Datasets} To show the effectiveness of our approach, we experiment on 8 NER datasets across 5 domains:\n\\begin{itemize}[leftmargin=*]\n    \\item {\\bf Social Media}: We use WNUT-16 \\citep{strauss-etal-2016-results} and WNUT-17 \\citep{derczynski-etal-2017-results} datasets collected from social media. We use the standard split for these datasets.\n    \\item {\\bf News}: We use CoNLL-03 English \\cite{tjong-kim-sang-de-meulder-2003-introduction} dataset and CoNLL++ \\citep{wang-etal-2019-crossweigh} dataset. The CoNLL-03 dataset is the most popular dataset for NER. CoNLL++ is a revision of the CoNLL-03 datasets. \\citet{wang-etal-2019-crossweigh} fixed annotation errors on the test set by professional annotators and improved the quality of the training data through their CrossWeigh approach. We use the standard dataset split for these datasets.\n    \\item {\\bf Biomedical}: We use BC5CDR \\citep{li2016biocreative} and NCBI-disease \\citep{dougan2014ncbi} datasets, which are two popular biomedical NER datasets. We merge the training and development data as training set following \\citet{nooralahzadeh-etal-2019-reinforcement}.\n    \\item {\\bf Science and Technology}: We use CBS SciTech News dataset collected by \\citet{jia-etal-2019-cross}. The dataset only contains the test set with the same label set as the CoNLL-03 dataset. We use the dataset to evaluate the effectiveness of cross-domain transferability from the news domain.\n    \\item {\\bf E-commerce}: %We collect and annotate a dataset from an E-commerce website. \n    We collect and annotate an internal dataset from one anonymous E-commerce website. The dataset contains 25 named entity labels for goods in short texts. We also collect 300,000 unlabeled sentences for semi-supervised training.\n\\end{itemize}\nWe show the statistics of the datasets in Table \\ref{tab:stat}.\n\n\\paragraph{Annotations of the E-commerce dataset}\nWe manually labeled the user queries through crowdsourcing from \\url{www.aliexpress.com}, which is a real-world E-commerce website. For each query, we asked one annotator to label the entities and ask another annotator to check the quality. After that, we randomly select 10\\% of the dataset and ask the third annotator to check the accuracy. As a result, the overall averaged query-level accuracy\\footnote{the accuracy of a query counts 1.0 if all the entities in the query are correctly recognized and 0.0 otherwise.} is 95\\%. The dataset will not be released due to user privacy.\n\n\\paragraph{Retrieving and Ranking} We use an internal E-commerce search engine for the E-commerce dataset. For the other datasets, we use Google Search as the search engine. Google Search is an off-the-shelf search engine and can simulate the offline search over various domains.\nWe use summarized descriptions from the search results as the retrieved texts\\footnote{If the descriptions are not available, we use the titles of the results instead.}. As Google Search limits the maximal length of searching queries to 32 words, we chunk a sentence into multiple sub-sentences based on punctuation if the sentence is longer than 30, feed each sub-sentence to the search engine, and retrieve up to 20 results. We filter the retrieved texts that contain any part of the datasets. Our re-ranking module selects top $6$ relevant texts\\footnote{We determined that $6$ is a reasonable number based on preliminary experiments.} as the external contexts of the input sentence and chunk the external contexts if the total sub-token lengths of the input sentence and external contexts exceeds 510.\n\n\\paragraph{Model Configurations}\nFor the re-ranking module, we use Roberta-Large \\citep{liu2019roberta} for token representations which is the default configuration in the code\\footnote{\\url{https://github.com/Tiiiger/bert_score}} of BERTScore \\citep{Zhang*2020BERTScore:}. For token representations in the NER model, we use pretrained Bio-BERT \\citep{lee2020biobert} for datasets from the biomedical domain and use XLM-RoBERTa \\citep{conneau-etal-2020-unsupervised} for datasets from other domains. %We use the first subtoken from the output of the last layer of the transformers as the token representations.\n\n\\paragraph{Training} \nDuring training, we fine-tune the pretrained contextual embeddings by AdamW \\citep{loshchilov2018decoupled} optimizer with a batch size of $4$. We use a learning rate of $5\\times 10^{-6}$ to update the parameters in the pretrained contextual embeddings. For the CRF layer parameters, we use a learning rate of $0.05$. We train the NER models for $10$ epochs for the datasets in Social Media and Biomedical domains while we train the NER models for $5$ epochs for other datasets for efficiency as these datasets have more training sentences.\n\n\\begin{table*}[ht!]\n\\centering\n\\setlength\\tabcolsep{5pt}\n\\small\n\\begin{tabular}{l||cc|cc|cc|c}\n\\hlineB{4}\n& \\multicolumn{2}{c|}{\\textbf{Social Media}} & \\multicolumn{2}{c|}{\\textbf{News}} & \\multicolumn{2}{c|}{\\textbf{Biomedical}} & \\multirow{2}{*}{\\textbf{E-commerce}}\\\\\n& WNUT-16 & WNUT-17 & CoNLL-03 & CoNLL++ & BC5CDR & NCBI & \\\\\n\\hline\\hline\n\\citet{zhou-etal-2019-dual} & 55.43 & 42.83 & - & - & - & - & - \\\\\n\\citet{nguyen-etal-2020-bertweet} & 52.10 & 56.50 & - & - & - & - & - \\\\\n\\citet{nie-etal-2020-named} & 55.01 & 50.36  & - & - & - & - & -  \\\\\n\\hline\n\\citet{baevski-etal-2019-cloze} & - & - & 93.50 & - & - & - & -  \\\\\n\\citet{wang-etal-2019-crossweigh} & - & - & 93.43 & 94.28 & - & - & -  \\\\\n\\citet{li-etal-2020-dice} & - & - & 93.33 & - & - & - & -  \\\\\n\\hline\n\\citet{nooralahzadeh-etal-2019-reinforcement} & - & - & - & - & 89.93 & - & - \\\\\nBio-Flair \\shortcite{sharma2019bioflair} & - & - & - & - & 89.42 & 88.85 & - \\\\\nBio-BERT \\shortcite{lee2020biobert} & - & - & - & - & - & 87.70 & - \\\\\n\\hline\\hline\n  \\multicolumn{8}{c}{Evaluation: {\\sc \\textbf{w/o Context}}}\\\\\n\\hline\n{\\bf\\textsc{LUKE}} \\shortcite{yamada-etal-2020-luke} & 54.04 & 55.22 & 92.42 & 93.99 & 89.18 & 87.62 & 77.64 \\\\\n{\\bf\\textsc{w/o Context}} & 56.04 & 57.86 & 93.03 & 94.20 & 90.52 & 88.65 & 81.47\\\\\n{\\bf\\textsc{CL-$L_2$}} & 57.35\\rlap{$^{\\dagger}$} & 58.68\\rlap{$^{\\dagger}$} & 93.08  & 94.38\\rlap{$^{\\dagger}$} & 90.70\\rlap{$^{\\dagger}$} & 89.20\\rlap{$^{\\dagger}$} & 82.43\\rlap{$^{\\dagger}$}\\\\\n{\\bf\\textsc{CL-KL}} & 58.14\\rlap{$^{\\dagger}$} & 59.33\\rlap{$^{\\dagger}$} & 93.21\\rlap{$^{\\dagger}$}  & 94.55\\rlap{$^{\\dagger}$} & 90.73\\rlap{$^{\\dagger}$} & \\textbf{89.24}\\rlap{$^{\\dagger}$} & 82.31\\rlap{$^{\\dagger}$}\\\\\n\\hline\n  \\multicolumn{8}{c}{Evaluation: {\\sc \\textbf{w/ Context}}}\\\\\n\\hline\n{\\bf\\textsc{w/ Context}} & 57.43\\rlap{$^{\\dagger}$} & 60.20\\rlap{$^{\\dagger}$} & 93.27\\rlap{$^{\\dagger}$} & 94.56\\rlap{$^{\\dagger}$} & 90.76\\rlap{$^{\\dagger}$} & 89.01\\rlap{$^{\\dagger}$} & 83.15\\rlap{$^{\\dagger}$}\\\\\n{\\bf\\textsc{CL-$L_2$}} & 58.61\\rlap{$^{\\dagger}$} & 60.26\\rlap{$^{\\dagger}$} & 93.47\\rlap{$^{\\dagger}$} & 94.62\\rlap{$^{\\dagger}$} & \\textbf{90.99}\\rlap{$^{\\dagger}$} & 89.22\\rlap{$^{\\dagger}$} & 83.87\\rlap{$^{\\dagger}$}\\\\\n{\\bf\\textsc{CL-KL}} & \\textbf{58.98}\\rlap{$^{\\dagger}$} & \\textbf{60.45}\\rlap{$^{\\dagger}$} & \\textbf{93.56}\\rlap{$^{\\dagger}$} & \\textbf{94.81}\\rlap{$^{\\dagger}$} & 90.93\\rlap{$^{\\dagger}$} & 88.96\\rlap{$^{\\dagger}$} & \\textbf{83.99}\\rlap{$^{\\dagger}$}\\\\\n\\hlineB{4}\n\\end{tabular}\n\\caption{A comparison among recent state-of-the-art models, the baseline and our approaches. ${\\dagger}$ represents the model is significantly stronger than the baseline model ({\\bf\\textsc{w/o Context}}) with $p<0.05$ on Student's T test.}\n\\label{tab:main}\n\\end{table*}\n\n\\subsection{Results}\nWe experiment on the following approaches:\n\\begin{itemize}[leftmargin=*]\n    \\item {\\sc\\textbf{LUKE}} is a very recent state-of-the art model on CoNLL-03 NER dataset proposed by \\citet{yamada-etal-2020-luke}. We use the same parameter setting as \\citet{yamada-etal-2020-luke} and use a single sentence as the input instead of taking document-level contexts in the dataset as in \\citet{yamada-etal-2020-luke} for fair comparison.\n    \\item {\\sc\\textbf{w/o Context}} represents training the NER model without external contexts (Eq. \\ref{eq:nll_loss}), which is the baseline of our approaches. \n    \\item {\\sc\\textbf{w/ Context}} represents training the NER model with external contexts (Eq. \\ref{eq:nll_loss_x}). \n    \\item {\\sc\\textbf{CL-$L_2$}} represents minimizing the $L_2$ distance between token representations (Eq. \\ref{eq:L2_loss}).\n    \\item {\\sc\\textbf{CL-KL}} represents minimizing the KL divergence (Eq. \\ref{eq:cl_posterior_loss}) between CRF output distributions.\n\\end{itemize}\nBesides, we also compare our approaches with previous state-of-the-art approaches over entity-level F1 scores\\footnote{We do not compare the results from previous work such as \\citet{yu-etal-2020-named,luoma-pyysalo-2020-exploring,yamada-etal-2020-luke} that utilizes the document-level contexts in CoNLL-03 NER here. We conduct a comparison with these approaches in Appendix \\ref{app:versus}. }.\nDuring the evaluation, our approaches are evaluated using inputs without external contexts ({\\sc\\textbf{w/o Context}}) and inputs with them ({\\sc\\textbf{w/ Context}}). We report the results averaged over 5 runs in our experiments. The results are listed in Table \\ref{tab:main}\\footnote{For the result of Bio-BERT \\citep{lee2020biobert} on NCBI-disease dataset, we report the results reported in official code (\\url{https://github.com/dmis-lab/biobert}). The results (89.71 in NCBI-disease) reported in the paper used token-level F1 score instead of entity-level F1 score.}. With the external contexts, our models with CL outperform previous state-of-the-art approaches on most of the datasets. Our approaches significantly outperform the baseline that is trained without external contexts. \nComparing with LUKE, our approaches and our baseline outperform LUKE in all the cases. The possible reason is that LUKE is pretrained only using long word sequences, which makes the model prone to fail to capture the information of entities based on short sentences\\footnote{We have confirmed with the authors of LUKE \\citep{yamada-etal-2020-luke} that the accuracy on the CoNLL-03 dataset is consistent with their experimental results.}. For our approaches, with CL, the accuracy can be improved on both input views comparing with {\\sc \\textbf{w/o Context}} and {\\sc \\textbf{w/ Context}}, which shows adding constraints between the two views during training helps the model better utilize the original text information. For the two constraints in CL, we find that \\textbf{CL-KL} is relatively stronger than \\textbf{CL-$L_2$} in a majority of the cases. \n\n\\begin{table}[t!]\n\\centering\n\\setlength\\tabcolsep{5pt}\n\\small\n\\begin{tabular}{l|cc}\n\\hlineB{4}\n & \\multicolumn{2}{c}{Evaluation} \\\\\n & \\multicolumn{2}{c}{\\textbf{Science and Technology}} \\\\\n \\hline\nApproach & {\\sc \\textbf{w/o Context}} & {\\sc \\textbf{w/ Context}} \\\\\n\\hline\\hline\n\\citet{jia-etal-2019-cross} & 73.59 & - \\\\\n\\hline\n{\\sc\\textbf{w/o Context}} & 75.87 & 75.74  \\\\\n{\\sc\\textbf{w/ Context }} & 75.72 & 75.94  \\\\\n{\\sc\\textbf{CL-$L_2$ }} & 76.16 & 76.10  \\\\\n{\\sc\\textbf{CL-KL }} & \\textbf{76.37} & \\textbf{76.38}  \\\\\n\\hlineB{4}\n\\end{tabular}\n\\caption{A comparison of different approaches in transfer learning. The models are trained on the CoNLL-03 dataset.}\n\\label{tab:transfer}\n\\end{table}\n\n\\subsection{Cross-Domain Transfer}\nFor cross-domain transfer, we train the models on the CoNLL-03 datasets, evaluate the accuracy on the CBS SciTech News dataset, and compare the results with those in \\citet{jia-etal-2019-cross}. We evaluate our approaches with each input view and the results are shown in Table \\ref{tab:transfer}. Our approaches can improve the accuracy in cross-domain evaluation. The external contexts during evaluation can help to improve the accuracy of {\\sc\\textbf{w/ Context}}. However, the gap between the two input views for the CL approaches is diminished. The observation shows that CL is able to improve the accuracy in cross-domain transfer for both views and eliminate the gap between the two views. \n\n\\begin{table}[t!]\n\\centering\n\\setlength\\tabcolsep{5pt}\n\\small\n\\begin{tabular}{l|cc}\n\\hlineB{4}\n & \\multicolumn{2}{c}{Evaluation} \\\\\n \\hline\nApproach & {\\sc \\textbf{w/o Context}} & {\\sc \\textbf{w/ Context}} \\\\\n\\hline\\hline\n{\\sc\\textbf{CL-$L_2$}} & 82.43 & 83.87  \\\\\n{\\sc\\textbf{CL-KL}} & 82.31 & 83.99  \\\\\n\\hline\n{\\sc\\textbf{CL--$L_2$+Semi}} & \\textbf{82.88}\\rlap{$^{\\dagger}$} & 83.92  \\\\\n{\\sc\\textbf{CL-KL+Semi}} & 82.58\\rlap{$^{\\dagger}$} & \\textbf{84.10}  \\\\\n\\hlineB{4}\n\\end{tabular}\n\\caption{A comparison between of CL approaches with and without semi-supervised learning. {\\sc \\textbf{Semi}} represents the approaches with semi-supervised learning. ${\\dagger}$ represents the approach is significantly ($p<0.05$) stronger than the approach without semi-supervised learning with the same input view.}\n\\label{tab:semi}\n\\end{table}\n\n\\subsection{Semi-supervised Cooperative Learning}\nCooperative learning can take advantage of large amounts of unlabeled text for further improvement. We jointly train on the labeled data and unlabeled data in training to form a semi-supervised training manner. During training, we alternate between minimizing the loss (Eq. \\ref{eq:final_loss}) for labeled data and the CL loss for unlabeled data (Eq. \\ref{eq:cl_loss}). We conduct the experiment on the E-commerce dataset as an example. Results in Table \\ref{tab:semi} show that the accuracy of both input views can be improved especially for the input without external contexts, which shows the effectiveness of CL in semi-supervised learning.\n\n\\section{Analysis}\nWe use the WNUT-17 dataset in the analysis.\n\\subsection{Comparison of Re-ranking Approaches}\nVarious re-ranking approaches may affect the token representations of the model. We compare our approach with three other re-ranking approaches. The first is the ranking from the search engine without any re-ranking approaches. The second is re-ranking through a fuzzy match score. The approach has been widely applied in a lot of previous work \\citep{gu2018search,zhang-etal-2018-guiding,hayati-etal-2018-retrieval,xu-etal-2020-boosting}. The third is BERTScore with tf-idf importance weighting which makes rare words more indicative than common words in scoring. We train our models ({\\sc \\textbf{w/ Context}}) with external contexts from these re-ranking approaches and report the averaged and best results on WNUT-17 in Table \\ref{tab:ranking}. Our results show that re-ranking with BERTScore performs the best, which shows the semantic relevance is helpful for the performance. However, for BERTScore with the tf-idf weighting, the accuracy of the model drops significantly (with $p<0.05$). The possible reason might be that the tf-idf weighting gives high weights to irrelevant texts with rare words during re-ranking.\n\n\\begin{table}[t]\n\\centering\n\\small\n\\begin{tabular}{l|cccc}\n\\hlineB{4}\n& \\textbf{SE} & \\textbf{FM} & \\textbf{BS} & \\textbf{BS+tf-idf}\\\\\n\\hline\n\\textbf{\\textsc{Avg.}} & 59.95 & 59.54 & \\textbf{60.20} & 59.71 \\\\ \n\\hline\n\\textbf{\\textsc{Best}} & 61.79 & 60.89 & \\textbf{62.29} & 60.96 \\\\ \n\\hlineB{4}\n\\end{tabular}\n\\caption{A comparison of different re-ranking approaches by the F1 scores on WNUT-17. \\textbf{SE}: Search engine. \\textbf{FM}: Fuzzy match score. \\textbf{BS}: BERTScore.}\n\\label{tab:ranking}\n\\end{table}\n\n\\begin{table}[t]\n\\centering\n\\setlength\\tabcolsep{5pt}\n\\small\n\\begin{tabular}{l|c}\n\\hlineB{4}\n& WNUT-17 \\\\\n\\hline\n\\textbf{w/ Context (Ours)} & \\textbf{60.20} \\\\\n\\hline\n\\textbf{w/o Context } & 57.86 \\\\\n\\textbf{w/ Context (Dataset)} & 57.21\\\\\n\\textbf{w/ Context (Generated)} & 57.71\\\\\n\\textbf{w/ Context (Random Retrieved)} & 57.53 \\\\\n\\textbf{w/ Context (Random Data)} & 47.69\\\\\n\\hlineB{4}\n\\end{tabular}\n\\caption{A comparison among different contexts types.}\n\\label{tab:random}\n\\end{table}\n\n\\subsection{How the Context Quality Affects Accuracy}\nWe analyze how the NER model will perform when the quality of external contexts varies. We train and evaluate the NER model in four conditions with various contexts. The first one takes each dataset split as a document and encodes each sentence with document-level contexts. In this case, we encode the document-level contexts following the approach of \\citet{yamada-etal-2020-luke}. The second one uses GPT-2 \\citep{radford2019language} to generate 6 relevant sentences as external contexts. The other two conditions randomly select from the retrieved texts or the dataset as external contexts. Results in Table \\ref{tab:random} show that all these conditions result in inferior accuracy comparing with the model without any external context. However, our external contexts are more semantically relevant to the input sentence and helpful for prediction.\n\n\\begin{table}[t]\n\\centering\n\\setlength\\tabcolsep{5pt}\n\\small\n\\begin{tabular}{l|cc}\n\\hlineB{4}\n & \\multicolumn{2}{c}{Evaluation} \\\\\n \\hline\nApproach & {\\sc \\textbf{w/o Context}} & {\\sc \\textbf{w/ Context}} \\\\\n\\hline\\hline\n{\\sc\\textbf{w/o Context }} & 57.86 & 59.40  \\\\\n{\\sc\\textbf{w/ Context }} & 57.46 & 60.20  \\\\\n{\\sc\\textbf{w/o CL}} & 58.14 & 59.64 \\\\\n{\\sc\\textbf{CL-$L_2$ + CL-KL}} & 58.69 & 60.16 \\\\\n\\hline\n{\\sc\\textbf{CL-$L_2$ }} & 58.68 & 60.26  \\\\\n{\\sc\\textbf{CL-KL }} & \\textbf{59.33} & \\textbf{60.45}  \\\\\n\\hlineB{4}\n\\end{tabular}\n\\caption{An ablation study of the training and prediction of models.}\n\\label{tab:ablation}\n\\end{table}\n\n\\subsection{Ablation Study}\nTo show the effectiveness of CL, we conduct three ablation studies for our approach. The first one is training the NER model based on one view and predict on the other. The second is jointly training both views without the CL loss term (removing $\\mcL_{\\text{CL}}(\\theta)$ in Eq. \\ref{eq:final_loss}). The final one is using both CL losses to train the model ($\\mcL_{\\text{CL}}(\\theta)=\\mcL_{\\text{CL-$L_2$}}(\\theta)+\\mcL_{\\text{CL-KL}}(\\theta)$ in Eq. \\ref{eq:final_loss}). Results in Table \\ref{tab:ablation} show that the external context can help to improve the accuracy even when the NER model is trained without the contexts. However, when the model is trained with the external contexts, the accuracy of the model drops when predicting the inputs without external contexts. In joint training without CL, the accuracy of the model over inputs without contexts can be slightly improved but the accuracy over inputs with contexts drops, which shows the benefit of adding CL. For the model trained with both CL losses, we find no improvement over the models trained with a single CL loss. %The possible reason is that using both constraints may limits the learning of the model. %As a result, we do not train the model with both CL losses on other datasets.\n\n\\section{Related Work}\n\\paragraph{Named Entity Recognition}\nNamed Entity Recognition \\citep{Sundheim1995NamedET} has been studied for decades. Most of the work takes NER as a sequence labeling problem and applies the linear-chain CRF \\citep{10.5555/645530.655813} to achieve state-of-the-art accuracy \\citep{ma-hovy-2016-end,lample-etal-2016-neural,akbik-etal-2018-contextual,akbik-etal-2019-pooled,wang-etal-2020-more}. \nRecently, the improvement of accuracy mainly benefits from stronger token representations such as pretrained contextual embeddings such as BERT \\citep{devlin-etal-2019-bert}, Flair \\citep{akbik-etal-2018-contextual} and LUKE \\citep{yamada-etal-2020-luke}. Very recent work \\citep{yu-etal-2020-named,yamada-etal-2020-luke} utilizes the strength of pretrained contextual embeddings over long-range dependency and encodes the document-level contexts for token representations to achieve state-of-the-art accuracy on CoNLL 2002/2003 NER datasets \\citep{tjong-kim-sang-2002-introduction,tjong-kim-sang-de-meulder-2003-introduction}. \n\n\\paragraph{Improving Models through Retrieval}\nRetrieving related texts from a certain database (such as the training set) has been widely applied in tasks such as neural machine translation \\citep{gu2018search,zhang-etal-2018-guiding,xu-etal-2020-boosting}, text generation \\citep{weston-etal-2018-retrieve,kim-etal-2020-retrieval}, semantic parsing \\citep{hashimoto2018retrieve,guo-etal-2019-coupling}. Most of the work uses the retrieved texts to guide the generation or refine the retrieved texts through the neural model, while we take the retrieved texts as the contexts of the input sentence to improve the semantic representations of the input tokens.\nFor the re-ranking models, fuzzy match score \\citep{gu2018search,zhang-etal-2018-guiding,hayati-etal-2018-retrieval,xu-etal-2020-boosting}, attention mechanisms \\citep{cao-etal-2018-retrieve,cai-etal-2019-retrieval}, and dot products between sentence representations \\citep{lewis2020retrieval,xu-etal-2020-boosting} are usual scoring functions to re-rank the retrieved texts. Instead, we use BERTScore to re-rank the retrieved texts instead as BERTScore evaluates semantic correlations between the texts based on pretrained contextual embeddings.\n\n\\paragraph{Multi-View Learning}\nMulti-View Learning is a technique applied to inputs that can be split into multiple subsets. Co-training \\citep{blum1998combining} and co-regularization \\citep{sindhwani2005co} train a separate model for each view. These approaches are semi-supervised learning techniques that require two independent views of the data. The model with higher confidence is applied to construct additional labeled data by predicting on unlabeled data. \\citet{sun2013survey} and \\citet{xu2013survey} have extensively studied various multi-view learning approaches. \\citet{hu2021multi} shows the effectiveness of multi-view learning on cross-lingual structured prediction tasks. Recently, \\citet{clark-etal-2018-semi} proposed Cross-View Training (CVT), which trains a unified model instead of multiple models and targets at minimizing the KL divergence between the probability distributions of the model and auxiliary prediction modules. Comparing with CVT, CL targets at improving the accuracy of two kinds of inputs rather than only one of them. We also propose to minimize the distance of token representations between different views in addition to KL-divergence. Besides, CL utilizes the external contexts and therefore we do not need to construct auxiliary prediction modules in the model. Moreover, CVT cannot be directly applied to our transformer-based embeddings. Finally, our decoding layer in the model uses the CRF layer instead of the simple Softmax layer as in CVT. The CRF layer is stronger but more difficult for KL-divergence computation.\n\n\\paragraph{Knowledge Distillation}\nKnowledge distillation \\citep{Bucilua:2006:MC:1150402.1150464,44873} transfers the knowledge of ``teacher'' models to smaller ``student'' models through minimizing the KL divergence of prediction probability distribution between the models. In speech recognition \\citep{Huang2018} and natural language processing \\citep{wang-etal-2020-structure,wang2020structural}, the marginal probability distribution of the linear-chain CRF layer has been applied to distill the knowledge between teacher models and student models. Comparing with these approaches, our approaches train a single unified model instead of transferring the knowledge between two models. We also show that the accuracy of both views can be improved with our approaches, unlike in knowledge distillation only the student model is updated and improved.\n\n\\section{Conclusion}\nIn this paper, we propose to improve the NER model's accuracy by retrieving related contexts from a search engine as external contexts of the inputs. To improve the robustness of the models when no external contexts are available, we propose Cooperative Learning. Cooperative Learning adds constraints between two input views over either the token representations or label distributions of both input views to be consistent. Empirical results show that our approach significantly outperforms the baseline models and previous state-of-the-art approaches on the datasets over 5 domains. We also show the effectiveness of Cooperative Learning in a semi-supervised training manner. %In the analysis, we analyze the effectiveness of our re-ranking approach .\n\n\\section*{Acknowledgments}\nThis work was supported by the National Natural Science Foundation of China (61976139) and by Alibaba Group through Alibaba Innovative Research Program. We thank Kaibo Zhang for his help in crawling related texts from Google Search and thank Jiong Cai and Zhuo Chen for their comments and suggestions on writing. \n\n\\newpage\n\n\\appendix\n\n\\begin{table}[t]\n\\centering\n\\small\n\\begin{tabular}{l|c}\n\\hlineB{4}\n Approach & CoNLL-03 \\\\\n \\hline\n\\citet{yu-etal-2020-named}\\rlap{$^{\\dagger}$} & 93.50\\\\\n\\citet{yamada-etal-2020-luke} & 94.30\\\\\n\\citet{luoma-pyysalo-2020-exploring}\\rlap{$^{\\dagger}$} & 93.74\\\\\n\\citet{wang2020automated} & \\textbf{94.60}\\\\\n{\\sc\\textbf{w/ Doc Context}} & 94.12 \\\\\n\\hline\n{\\sc\\textbf{w/o Context }} & 93.30 \\\\\n{\\sc\\textbf{w/ Context }} & 93.55 \\\\\n{\\sc\\textbf{CL-$L_2$ }} & 93.68  \\\\\n{\\sc\\textbf{CL-KL }} & 93.85  \\\\\n\\hlineB{4}\n\\end{tabular}\n\\caption{A comparison of retrieved contexts and document-level contexts. ${\\dagger}$: These approaches are trained on training and development sets.}\n\\label{tab:document}\n\\end{table}\n\n\\section{Retrieved Contexts Versus Document-level contexts on CoNLL-03}\n\\label{app:versus}\nWe conduct a comparison between our retrieved contexts and the document-level contexts on CoNLL-03 datasets. In Table \\ref{tab:document}, we report the best model on development set following \\citet{yamada-etal-2020-luke}. Comparing with previous state-of-the-art approaches with encoding document-level contexts, our approaches are competitive and even stronger than some of the previous approaches utilizing maximal document-level contexts. Comparing with our model trained on document-level contexts ({\\sc\\textbf{w/ Doc Context}}), we find that there is still a gap between the document-level contexts and retrieved contexts but our CL approaches can reduce the gap between these two contexts.\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2006.01563v2.tex",
        "arXiv-2011.06993v2.tex",
        "arXiv-2105.03654v3.tex"
    ],
    "group_id": "group_105",
    "response": "### Title: Enhancing Named Entity Recognition through Cross-sentence Context and External Retrieval\n\n### Introduction\n\nNamed Entity Recognition (NER) is a fundamental task in Natural Language Processing (NLP) that involves identifying and categorizing named entities within a text into predefined categories such as person (PER), organization (ORG), location (LOC), and miscellaneous (MISC). Historically, NER has seen a progression from rule-based and unsupervised methods to feature engineering and deep learning approaches. The introduction of contextual embeddings and self-attention mechanisms, particularly with models like BERT, has significantly advanced the state-of-the-art in NER. However, current NER models often operate at the sentence level, which limits their ability to capture broader context, especially cross-sentence information. This summary explores three recent research papers that address this limitation by incorporating cross-sentence context and external retrieval to improve NER performance.\n\nThe field of NER has witnessed a shift towards deep learning models, particularly those based on contextual embeddings, which have shown remarkable improvements over traditional methods. Despite these advancements, there is still a gap in performance when models are restricted to sentence-level contexts. The challenge lies in effectively utilizing cross-sentence information to enhance the model's understanding of named entities within a broader context. This summary aims to highlight the methodologies and results of three papers that tackle this challenge, providing insights into how cross-sentence context and external retrieval can be integrated into NER models.\n\n### Main Content of Each Paper\n\n#### Paper 1: Exploring Cross-sentence Contexts for Named Entity Recognition with BERT\n\nThis paper investigates the impact of incorporating cross-sentence context on the performance of BERT-based NER models across five languages: English, Dutch, Finnish, German, and Spanish. The authors introduce a method called Contextual Majority Voting (CMV) to combine predictions from multiple sentences in different contexts, thereby creating a more robust model. They construct input samples by placing the sentence of interest at various positions within a window of up to 512 wordpieces, including both preceding and following sentences from the corpus. \n\nThe CMV method involves predicting tags for the same sentence in different positions and contexts, then taking a majority vote of these predictions. This approach is straightforward and does not require additional features or embeddings. The paper evaluates the effectiveness of CMV on established NER datasets, including CoNLL'02 and CoNLL'03 benchmarks. The results show that CMV consistently improves NER performance across all languages, establishing new state-of-the-art (SOTA) results for English, Dutch, and Finnish, and matching SOTA results for Spanish.\n\n#### Paper 2: FLERT: Document-Level Features for Named Entity Recognition\n\nThe second paper introduces FLERT (Feature-based LSTM-CRF with External Retrieval), a method that leverages document-level features to enhance NER performance. FLERT evaluates two standard NER architectures: fine-tuning and feature-based LSTM-CRF, in the context of document-level features. The authors use a multilingual XLM-RoBERTa (XLM-R) model and experiment with different hyperparameters, such as context window size and enforcing document-locality.\n\nFLERT's approach involves adding 64 tokens of left and right context to each sentence, allowing the transformer to capture broader context. The authors find that document-level features significantly improve NER quality, and fine-tuning generally outperforms feature-based approaches. They present new SOTA scores on several CoNLL-03 benchmark datasets and integrate their approach into the Flair framework for reproducibility.\n\n#### Paper 3: Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning\n\nThe third paper proposes a novel method that improves NER models by retrieving external contexts through a search engine and applying Cooperative Learning (CL) to enhance the model's performance. The authors use an internal E-commerce search engine for the E-commerce dataset and Google Search for other datasets. They experiment with different re-ranking approaches to ensure the retrieved contexts are semantically relevant to the input sentence.\n\nThe CL approach involves training the model with two input views: one with external contexts and one without. The model is encouraged to produce similar predictions for both views, which helps improve the robustness of the model when external contexts are not available. The authors find that CL significantly outperforms the baseline models and previous SOTA approaches on datasets across five domains, including social media, news, biomedical, science and technology, and E-commerce.\n\n### Commonalities and Innovations\n\nAll three papers focus on enhancing NER models by incorporating broader context information. Paper 1 uses Contextual Majority Voting (CMV) to combine predictions from multiple sentences in different contexts, while Paper 2 introduces FLERT, which leverages document-level features to improve NER performance. Paper 3 proposes a method that retrieves external contexts through a search engine and applies Cooperative Learning (CL) to improve the model's performance across different input views.\n\nThe common theme is the use of contextual embeddings to capture broader context information, which is crucial for NER tasks. Paper 1's CMV method is innovative in its simplicity and effectiveness, relying solely on BERT's architecture without introducing additional features. Paper 2's FLERT approach is unique in its systematic evaluation of document-level features in two standard NER architectures, providing clear recommendations for hyperparameters. Paper 3's external context retrieval and CL approach is novel in its use of a search engine to retrieve relevant contexts and its dual-input training methodology, which enhances the model's performance in both offline and online prediction scenarios.\n\n### Comparison of Results\n\nThe results from the three papers are summarized in Table \\ref{tab:comparison_results}.\n\n\\begin{table*}[ht!]\n\\centering\n\\small\n\\begin{tabular}{l|cccccc}\n\\hlineB{4}\n& \\textbf{English} & \\textbf{Dutch} & \\textbf{Finnish} & \\textbf{German} & \\textbf{Spanish} & \\textbf{E-commerce} \\\\\n\\hline\n\\textbf{Paper 1} & 93.44 & 93.21 & 93.66 & 85.63 & 87.89 & - \\\\\n\\textbf{Paper 2} & 97.02 & 94.34 & 94.61 & 89.69 & 90.56 & - \\\\\n\\textbf{Paper 3} & 59.33 & - & - & 88.96 & 60.45 & 84.10 \\\\\n\\hlineB{4}\n\\end{tabular}\n\\caption{Comparison of NER results across different languages and datasets.}\n\\label{tab:comparison_results}\n\\end{table*}\n\nPaper 1 achieves state-of-the-art results for English, Dutch, and Finnish, and near-state-of-the-art results for German and Spanish. Paper 2, using a multilingual XLM-R model, achieves higher F1 scores for English and Dutch, while Paper 3 focuses on cross-domain transferability and semi-supervised learning, achieving significant improvements on the E-commerce dataset.\n\nThe differences in results can be attributed to the specific methodologies and datasets used. Paper 1's CMV method is effective across multiple languages but does not achieve the highest scores for Spanish. Paper 2's FLERT approach, which fine-tunes the model with document-level features, outperforms CMV in terms of F1 scores, especially for English and Dutch. Paper 3's external context retrieval and CL approach is particularly effective in scenarios where external contexts are available, achieving new SOTA results on the E-commerce dataset.\n\n### Conclusion\n\nThe three papers highlight the importance of incorporating broader context information in NER models. Paper 1 demonstrates the effectiveness of Contextual Majority Voting (CMV) in combining predictions from multiple sentences in different contexts, establishing new SOTA results for English, Dutch, and Finnish. Paper 2 introduces FLERT, which uses document-level features to enhance NER performance, achieving higher F1 scores for English and Dutch. Paper 3 proposes a method that retrieves external contexts through a search engine and applies Cooperative Learning (CL) to improve the model's performance across different input views, achieving new SOTA results on the E-commerce dataset.\n\nThe main findings are that cross-sentence context and external retrieval can significantly improve NER performance. CMV and FLERT are effective in leveraging document-level contexts, while CL enhances the model's robustness by encouraging similar predictions for input views with and without external contexts. Future research directions include exploring more sophisticated re-ranking approaches for external contexts, further optimizing CL for different domains, and investigating the use of CL in other NLP tasks that benefit from broader context information.\n\n### Future Research Directions\n\nFuture research could explore the following areas:\n- **Advanced Re-ranking Approaches**: Develop more sophisticated re-ranking methods to ensure the retrieved external contexts are highly relevant and semantically aligned with the input sentence.\n- **Optimizing CL for Different Domains**: Investigate how CL can be optimized for specific domains, such as social media or biomedical texts, to further enhance model performance.\n- **Cross-Domain Applications**: Apply CL and external context retrieval to other NLP tasks that require broader context understanding, such as relation extraction or coreference resolution.\n- **Efficiency Improvements**: Develop more efficient methods for incorporating cross-sentence context and external retrieval to reduce computational overhead and latency, making these approaches more practical for real-world applications.\n\nThese advancements could lead to more accurate and robust NER models, capable of handling a wider range of contexts and domains, thereby pushing the boundaries of current state-of-the-art performance."
}