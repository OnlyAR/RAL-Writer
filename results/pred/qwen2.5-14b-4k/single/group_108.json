{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{\\vp: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation}\n\n\\begin{document}\n\n\\maketitle\n\n\\renewcommand{\\thefootnote}{$^{\\star}$}\n\\footnotetext[1]{Equal contribution.}\n\\renewcommand{\\thefootnote}{1}\n\n\\begin{abstract}\nWe introduce \\vp, a large-scale multilingual corpus providing 400K hours of unlabeled speech data in 23 languages. It is the largest open data to date for unsupervised representation learning as well as semi-supervised learning. \\vp~also contains 1.8K hours of transcribed speeches in 15 languages and their aligned oral interpretations into 15 target languages totaling 17.3K hours. We provide speech recognition (ASR) baselines and validate the versatility of \\vp~unlabeled data in semi-supervised ASR and speech-to-text translation under challenging out-of-domain settings. The corpus is available\nat \\url{https://github.com/facebookresearch/voxpopuli}.\n\\end{abstract}\n\n\\section{Introduction}\n\nRecent progress in speech-to-text tasks such as automatic speech recognition (ASR) and speech translation (ST) has been achieved by the development and application of unsupervised speech pre-training methods~\\cite{oord2018representation,Schneider2019,baevski2020wav2vec,conneau2020unsupervised,Wu2020SelfSupervisedRI,Nguyen2020InvestigatingSP}, with semi-supervised learning (self-training)~\\cite{kahn2020self,Pino2020SelfTrainingFE,zhang2020pushing,xu2020self} or a combination of both methods~\\cite{xu2020self}.\nThis line of research leverages large amounts of unlabeled English speech data~\\cite{kahn2020libri} that enable improvements in \\emph{English} ASR or \\emph{out-of-English} ST.\nLarge amounts of multilingual audio data are needed in order to achieve similar progress for multilingual ASR and ST.\nSimilarly, most ASR and ST research is currently conducted on the LibriSpeech~\\cite{panayotov2015librispeech} and MuST-C benchmarks~\\cite{cattoni2020must,di2019must}. As a result, the research community has been mostly focused on speech-to-text tasks with English as input.\nWhile multilingual ASR~\\cite{Pratap2020,ardila-etal-2020-common} and ST datasets~\\cite{wang2020covost,iranzo2020europarl} have recently been made available, the amount of data available quickly drops beyond the top few high-resource languages.\n\nSimultaneous speech translation (interpretation) has witnessed a resurgence with the applications of end-to-end encoder-decoder models. Most of the recent studies focus on text output and leverage ST corpora that are translated offline in the written form. There are differences, however, between translationese and interpretese~\\citep{sridhar2013corpus,he2016interpretese}, where interpreters develop a variety of strategies to improve simultaneity. Models trained on translation corpora are unlikely to learn from these interpretation skills to achieve better quality-latency trade-offs. Finally, there has been little research~\\citep{translatotron,tjandra2019speech,zhang2020uwspeech} into speech output due to the lack of open data. Existing corpora~\\citep{tohyama2004ciair,bendazzoli2005approach} are either of limited size or no longer publicly available.\n\nIn this paper, we introduce \\vp, a large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation. It contains the largest open unlabeled speech data to date, totaling 400K hours in 23 languages: Bulgarian (Bg), Czech (Cs), Croatian (Hr), Danish (Da), Dutch (Nl), English (En), Estonian (Et), Finnish (Fi), French (Fr), German (De), Greek (El), Hungarian (Hu), Italian (It), Latvian (Lv), Lithuanian (Lt), Maltese (Mt), Polish (Pl), Portuguese (Pt), Romanian (Ro), Slovak (Sk), Slovene (Sl), Spanish (Es) and Swedish (Sv). \\vp~also provides a total of 1.8K hours of transcribed speech in 16 languages (En, De, Fr, Es, Pl, It, Ro, Hu, Cs, Nl, Fi, Hr, Sk, Sl, Et and Lt) and their aligned oral interpretations into 15 target languages (En, De, Fr, Es, Pl, It, Ro, Hu, Cs, Nl, Fi, Sk, Sl, Lt and Da) totaling 17.3K hours.\n\nWe describe our corpus creation methodology in Section~\\ref{sec:corpus_creation} and analyze the created corpus in Section~\\ref{sec:data_analysis}. We provide ASR baselines and demonstrate the value of our multilingual unlabeled data as well as weakly labeled data on several non-English languages in Section~\\ref{sec:experiments_and_results}.\n\n\\section{Corpus Creation}\n\\label{sec:corpus_creation}\n\\subsection{Data Acquisition}\n\\vp~sources data from 2009-2020 European Parliament (EP) event recordings, which include plenary sessions, committee meetings and other events. In each event, speakers give speeches in turn in different European Union (EU) languages. These speeches are partially transcribed (for plenary sessions only)\nand interpreted into 24 EU languages. The interpretations are only oral without any transcription. In the following part, we refer to the original speech as ``source speech\" and to the interpreted one as ``target speech\".\nWe download audio clips for both source and target speeches from the official website\\footnote{https://multimedia.europarl.europa.eu}. We also crawl the transcript, speaker information and starting/ending timestamps for each speech (for plenary sessions only) from that source, with which we later align the speech to its transcript and interpretation utterance by utterance. The acquired raw data suffers from missing audios, incomplete transcripts and inaccurate timestamps. We build data processing pipelines to segment speech paragraphs into utterances and filter out the ones with erroneous transcriptions.\n\n\\begin{table}[t]\n    \\centering\n    \\small\n    \\begin{tabular}{c|c|ccc|c}\n    \\toprule\n    & Unlab. & \\multicolumn{3}{c|}{Transcribed} & LM \\\\\n    & Hrs & Hrs & Spkrs (F\\%) & Tkns & Tkns \\\\\n    \\midrule\n    En & 24.1K & 543 & 1313 (29.6) & 4.8M & 60.1M \\\\\n    De & 23.2K & 282 & 531 (30.6) & 2.3M & 50.0M \\\\\n    Fr & 22.8K & 211 & 534 (38.6) & 2.1M & 58.6M \\\\\n    Es & 21.4K & 166 & 305 (40.6) & 1.6M & 57.4M \\\\\n    Pl & 21.2K & 111 & 282 (23.7) & 802K & 13.6M \\\\\n    It & 21.9K & 91 & 306 (33.8) & 757K & 52.1M \\\\\n    Ro & 17.9K & 89 & 164 (27.6) & 739K & 10.3M \\\\\n    Hu & 17.7K & 63 & 143 (30.3) & 431K & 13.0M \\\\\n    Cs & 18.7K & 62 & 138 (24.9) & 461K & 13.5M \\\\\n    Nl & 19K & 53 & 221 (39.3) & 488K & 54.6M \\\\\n    Fi & 14.2K & 27 & 84 (56.8) & 160K & 34.5M \\\\\n    Hr & 8.1K & 43 & 83 (33.1) & 337K & 285K \\\\\n    Sk & 12.1K & 35 & 96 (33.8) & 270K & 13.3M \\\\\n    Sl & 11.3K & 10 & 45 (43.9) & 76K & 12.6M \\\\\n    Et & 10.6K & 3 & 29 (43.7) & 18K & 11.3M \\\\\n    Lt & 14.4K & 2 & 21 (14.8) & 10K & 11.5M \\\\\n    Pt & 17.5K & - & - & - & - \\\\\n    Bg & 17.6K & - & - & - & - \\\\\n    El & 17.7K & - & - & - & - \\\\\n    Lv & 13.1K & - & - & - & - \\\\\n    Mt & 9.1K & - & - & - & - \\\\\n    Sv & 16.3K & - & - & - & - \\\\\n    Da & 13.6K & - & - & - & - \\\\\n    \\midrule\n    All & 384K & 1791 & 4295 & 15M & 467M \\\\\n\\bottomrule\n    \\end{tabular}\n    \\caption{Statistics for unlabeled (``Unlab.\") and transcribed speech data in \\vp: duration in hours (``Hrs\"), number of speakers (``Spkrs\"), percentage of female speakers (``F\\%\") and number of tokens (``Tkns\"). Durations are calculated on segmented audios where leading and trailing silence is trimmed. The LM data is a combination of \\vp~ transcription and sentences from EuroParl~\\citep{koehn2005europarl}.}\n    \\label{tab:unlabeled_transcribed_stats}\n\\end{table}\n\n\\subsection{Data Processing}\n\\label{sec:data_processing}\n\\subsubsection{Unlabeled Speech}\nWe construct \\vp~unlabeled set from all source and target speeches in 23 EU languages (excluding Irish because of very limited data availability). We segment full-event audios into short clips of 15-30 seconds using an energy-based voice activity detection (VAD) algorithm\\footnote{https://github.com/amsehili/auditok}. Each audio clip has a maximum of 2 seconds of continuous silence, and silent clips are discarded. Around 16\\% of the data is dropped after silence removal, which leads to a final overall duration of around 400K hours.\n\n\\begin{table*}[t]\n    \\centering\n    \\small\n    \\begin{tabular}{c|c@{\\hs{1.3}}c@{\\hs{1.3}}c@{\\hs{1.3}}c@{\\hs{1.3}}c@{\\hs{1.3}}c@{\\hs{1.3}}c@{\\hs{1.3}}c@{\\hs{1.3}}c@{\\hs{1.3}}c@{\\hs{1.3}}c@{\\hs{1.3}}c@{\\hs{1.3}}c@{\\hs{1.3}}c@{\\hs{1.3}}c@{\\hs{1.3}}|c}\n    \\toprule\n    \\multirow{2}{*}{Source} & \\multicolumn{15}{c}{Target (Oral Interpretation)} \\\\\n    & En & De & Fr & Es & Pl & It & Ro & Hu & Cs & Nl & Fi & Sk & Sl & Lt & Da & Total \\\\\n    \\midrule\n    En & - & 463 & 427 & 441 & 432 & 461 & 457 & 382 & 427 & 400 & 442 & 433 & 434 & 398 & 370 & 6.0K \\\\\n    De & 187 & - & 196 & 204 & 214 & 217 & 198 & 205 & 214 & 196 & 217 & 208 & 218 & 164 & 179 & 2.8K \\\\\n    Fr & 169 & 187 & - & 187 & 172 & 197 & 195 & 144 & 170 & 158 & 168 & 168 & 156 & 139 & 134 & 2.3K \\\\\n    Es & 130 & 138 & 135 & - & 118 & 148 & 128 & 93 & 118 & 115 & 124 & 114 & 108 & 83 & 86 & 1.6K \\\\\n    Pl & 68 & 66 & 54 & 55 & - & 67 & 55 & 43 & 67 & 42 & 55 & 62 & 57 & 50 & 34 & 775 \\\\\n    It & 69 & 77 & 76 & 79 & 72 & - & 75 & 61 & 68 & 64 & 71 & 66 & 70 & 53 & 60 & 961 \\\\\n    Ro & 60 & 59 & 59 & 58 & 49 & 61 & - & 38 & 50 & 43 & 48 & 50 & 46 & 38 & 29 & 688 \\\\\n    Hu & 30 & 38 & 25 & 27 & 29 & 30 & 27 & - & 27 & 20 & 31 & 29 & 26 & 21 & 18 & 378 \\\\\n    Cs & 39 & 35 & 29 & 30 & 36 & 32 & 31 & 23 & - & 23 & 29 & 55 & 29 & 25 & 18 & 434 \\\\\n    Nl & 31 & 43 & 35 & 29 & 27 & 38 & 24 & 25 & 25 & - & 32 & 25 & 23 & 19 & 25 & 401 \\\\\n    Fi & 15 & 18 & 15 & 13 & 13 & 13 & 13 & 12 & 13 & 11 & - & 14 & 12 & 11 & 9 & 182 \\\\\n    Hr & 31 & 27 & 27 & 24 & 27 & 28 & 24 & 22 & 24 & 22 & 24 & 26 & 37 & 21 & 20 & 384 \\\\\n    Sk & 21 & 22 & 14 & 16 & 19 & 16 & 16 & 14 & 32 & 13 & 16 & - & 17 & 13 & 10 & 239 \\\\\n    Sl & 6 & 6 & 4 & 5 & 5 & 6 & 5 & 4 & 5 & 4 & 5 & 6 & - & 4 & 3 & 68 \\\\\n    Lt & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & - & 0 & 13 \\\\\n    \\midrule\n    Total & 857 & 1.2K & 1.1K & 1.2K & 1.2K & 1.3K & 1.2K & 1.1K & 1.2K & 1.1K & 1.3K & 1.3K & 1.2K & 1.0K & 995 & 17.3K \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{Duration statistics (hours) of aligned speech-to-speech data in \\vp~between 15 source languages and 15 target languages.}\n    \\label{tab:s2s_align_stats}\n\\end{table*}\n\n\\subsubsection{Transcribed Speech}\n\\label{sec:transcribed_speech}\nThe \\vp~transcribed set comes from aligning the full-event source speech audio with the transcripts for plenary sessions. Official timestamps are available for locating speeches by speaker in the full session, but they are frequently inaccurate, resulting in truncation of the speech or mixture of fragments from the preceding or the succeeding speeches. To calibrate the original timestamps, we perform speaker diarization (SD) on the full-session audio using pyannote.audio~\\citep{Bredin2020} and adopt the nearest SD timestamps (by L1 distance to the original ones) instead for segmentation. Full-session audios are segmented into speech paragraphs by speaker, each of which has a transcript available.\n\nThe speech paragraphs have an average duration of 197 seconds, which leads to significant memory usage and prevents efficient parallelism (batching) during model training. We hence further segment these paragraphs into utterances with a maximum duration of 20 seconds. We leverage speech recognition (ASR) systems to force-align speech paragraphs to the given transcripts and cut the utterances by ending punctuation or the longest silence inside the sentence if it exceeds 20 seconds. The ASR systems are TDS models~\\citep{TDS2019} trained with ASG criterion~\\citep{collobert2016wav2letter} on audio tracks from in-house de-identified video data. The resulting utterance segments may have incorrect transcriptions due to incomplete raw transcripts or inaccurate ASR force-alignment. We use the predictions from the same ASR systems as references and filter the candidate segments by a maximum threshold of 20\\% character error rate (CER).\n\nWe split the filtered utterances into train, development and test sets with disjoint speakers and target duration ratio (18:1:1). To determine the assignments, we group utterances by speaker and sort them by overall duration in ascending order. We assign the sorted groups to the test set in order until it reaches 20 speakers or the target duration (whichever comes later). The same process is repeated on the remaining utterance groups to construct the development set (with minimum 10 speakers instead). Finally, the rest of utterances make up the train set. This approach ensures higher speaker diversity in the test and development sets.\n\n\\subsubsection{Speech-To-Speech Alignment}\nEven though every source speech is associated with corresponding simultaneous interpretations in target languages, considerable preprocessing and filtering is necessary to make this dataset usable. Our strategy is to align source and target at the sentence level using ASR.\n\nWe first compare the spectrogram of the source and the target speech to remove the identical parts and segment the target speech into paragraphs. These identical speech are due to either the short delay between the time the source speaker and the interpreter started, or the fact that the source language is the same as the target one, and thus no interpretation is needed. For long target paragraphs, we further segment them by silence into audio clips of at most 15 minutes long. We use the same ASR model described in Section~\\ref{sec:transcribed_speech} and a language model (Section~\\ref{sec:lm_data}) to decode the segmented target audio. The decoded text is also forced aligned with the target audio, so that we have the timestamps of every decoded word. \n\nFor each source segment produced in Section~\\ref{sec:transcribed_speech}, we locate all decoded words that are within a window of five seconds to its start and end. A set of candidate target segments can be generated from all possible combinations of the starting and ending decoded words. We compute the cosine similarity between the LASER representation~\\citep{artetxe2019massively} of the source text and each decoded text in the candidate set to find the best target segment, i.e. the one with the highest score. We first carry out this process for all source segments, respectively, and then finetune the boundaries of overlapping target segments for consecutive source segments. Finally, a threshold of 0.75 is applied on the similarity score to filter out low-quality alignments, which can be due to ASR errors.\n\nIn addition to ASR output, we also collect human transcription on 400 hours of English target speech. The human annotators were asked to provide timestamps for each word while transcribing, and thus we can apply the same alignment process described above on human transcription and generate a set of ground truth speech-to-speech alignment data.\n\nAs a by-product from this alignment process, source text and target speech is aligned, which provides speech-to-text ``translation\" data in the reversed direction. This data is weakly labeled---the label (text) may contain more information than the speech data (interpretation is likely to drop unimportant details) and hence is not exact. However, it is still useful for ST model training as an addition to labeled data.\n\\begin{table}[t]\n    \\small\n    \\begin{tabular}{p{0.18\\linewidth} | p{0.7\\linewidth}}\n    \\toprule\n    \\hspace{1000pt} Original (French) & Vous le savez tous, la for\\^et recule. Toutes les deux secondes dans le monde, c'est l'\\'equivalent d'un terrain de football qui est d\\'etruit, c'est en un an l'\\'equivalent du territoire de la Gr\\`ece qui est d\\'eforest\\'e et c'est \\'evidemment dramatique. \\\\\n    \\midrule\n    \\hspace{1000pt} Translation & As you all know, the forest is receding. Every two seconds, across the world, the equivalent of a football pitch is destroyed; within a year, an area the size of Greece is deforested. Clearly, this is a tragic situation. \\\\\n    \\midrule\n    \\hspace{1000pt} Interpretation & You all know that we are losing forests every second, the surface the size area of a football field is lost in the forest. This is really tragic. \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{An example from \\vp~for interpretese vs. translationese. Translationese is verbatim and exact, while interpretese tends to be more general and summarizing with unimportant details dropped.}\n    \\label{tab:interpretataion_example}\n\\end{table}\\begin{table*}[t]\n    \\centering\n    \\small\n    \\begin{tabular}{cr|c@{\\hs{1.4}}c@{\\hs{1.4}}c@{\\hs{1.4}}c@{\\hs{1.4}}c@{\\hs{1.4}}c@{\\hs{1.4}}c@{\\hs{1.4}}c@{\\hs{1.4}}c@{\\hs{1.4}}c@{\\hs{1.4}}c@{\\hs{1.4}}c@{\\hs{1.4}}c@{\\hs{1.4}}c|@{\\hs{1.2}}c}\n    \\toprule\n    & & En & De & It & Fr & Es & Pl & Ro & Hu & Nl & Cs & Sl & Fi & Hr & Sk & Avg. $\\downarrow$ \\\\\n    \\midrule\n    Sup. & Dev & 30.1 & 29.0 & 41.6 & 28.6 & 27.4 & 27.1 & 28.5 & 27.4 & 35.7 & 27.8 & 95.7 & 45.7 & 44.9 & 30.2 &  37.1 \\\\\n    baseline & Test & 30.0 & 29.3 & 45.2 & 30.5 & 31.4 & 25.6 & 27.7 & 27.9 & 38.3 & 27.7 & 96.5 & 41.6 & 40.2 & 32.7 & 37.5 \\\\\n    \\midrule\n    VP-10K & Dev & 15.5 & 17.2 & 19.1 & 13.9 & 8.6 & 12.8 & 8.3 & 11.5 & 18.5 & 11.1 & 20.6 & 21.1 & 15.6 & 10.4 & 14.6 \\\\\n    + FT & Test & 16.2 & 16.2 & 21.5 & 15.4 & 11.0 & 12.5 & 9.4 & 12.0 & 19.7 & 11.8 & 26.1 & 17.1 & 14.1 & 11.1 & 15.3 \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{\\textbf{\\vp~ASR baselines and in-domain unsupervised pre-training.} We report \\vp~dev and test WER for languages with $\\ge$10 hours of data. Top: supervised monolingual Transformer baselines. Bottom: wav2vec 2.0 \\emph{Base} model pre-trained on 10K-hour \\vp~unlabeled data (23 languages) and fine-tuned on \\vp~ASR data. As we can see, pre-training with in-domain unlabeled data substantially improves performance especially for low-resource languages.}\n    \\label{tab:vp_asr_eval}\n\\end{table*}\\subsubsection{Language Modeling Data}\n\\label{sec:lm_data}\nTo train language models (LM) for ASR decoding, we combine \\vp~transcription in the training set with the EuroParl corpus~\\citep{koehn2005europarl}, which is from the proceedings of the European Parliament from 1996 to 2011. To process the EuroParl data, we first apply the sentence segmentation tool provided with the corpus. We remove all texts in the parentheses, replace hyphens and slashes with space, and remove all other punctuation except apostrophes. All digits are converted into words, and all texts are normalized into lowercase. Table~\\ref{tab:unlabeled_transcribed_stats} shows the statistics of the LM data.\n\n\\section{Data Analysis}\n\\label{sec:data_analysis}\n\n\\paragraph{Unlabeled speech} As we can see from Table~\\ref{tab:unlabeled_transcribed_stats}, \\vp~has a total of 400K hours of unlabeled data well-distributed across 23 EU languages, resulting in 8K-24K hours of data for each language. This ensures adequate data on languages with lower ASR resource, which are likely to benefit more from semi-supervised learning. It also facilitates multilingual model training since there is not much data imbalance and little need for tuning data sampling strategy.\n\n\\paragraph{Transcribed speech} The \\vp~transcribed data contains 16 languages totaling 1.8K hours and 4.3K speakers, whose detailed statistics can be found in Table~\\ref{tab:unlabeled_transcribed_stats}, including duration (hours) by language, number of speakers, percentage of female speakers and number of tokens. The data distribution is imbalanced and reflects the natural distribution of the number of native speakers. The remaining 7 languages (Pt, Bg, El, Lv, Mt, Sv and Da) are not covered due to either limited data volume or the availability of processing pipelines.\n\n\\paragraph{Speech-to-speech alignment} The statistics of the speech-to-speech alignment between all source languages and 15 target languages are shown in Table~\\ref{tab:s2s_align_stats}. Compared with the total amount of data available for each source language (``Transcribed hours\" in Table~\\ref{tab:unlabeled_transcribed_stats}), we obtain target alignments for more than $70\\%$ of the source sentences in En, De, Fr, Es and It, more than $50\\%$ for Pl, Ro, Cs, Nl and Hr, and the rest has at least $40\\%$ of source segments aligned. To examine the quality of our ASR system, we align the ASR output with the human transcription we collect on English target speech and see a word error rate (WER) of 31.7. With the human transcription, we can produce ground truth speech-to-speech alignment data that is 1.1 times larger than the size of the alignment data created from using ASR output, indicating that around $12\\%$ of the low-quality alignments are filtered due to ASR errors. If we compare the ASR-based and the ground truth alignment data, there is on average a 0.75-second shift in the target segment boundaries.\n\n\\paragraph{Interpretese vs. translationese} We exemplify the differences between simultaneous oral interpretation and offline written translation using \\vp~in Table~\\ref{tab:interpretataion_example}. The latter is verbatim and exact compared to the original speech, while the former tends to be more general and summarizing with unimportant details dropped. Human interpreters regularly apply these tactics to make better quality-latency trade-offs. Speech-to-speech translation models may benefit from these tactics if they are trained on interpretation data that \\vp~provides.\n\n\\section{Experiments \\& Results}\n\\label{sec:experiments_and_results}\nWe provide \\vp~ASR baselines and validate the versatility of \\vp~unlabeled data in unsupervised representation learning and semi-supervised learning for ASR as well as ST. We also evaluate the quality of speech-to-speech alignment indirectly via the weakly labeled ST data it produces.\n\n\\begin{table}[t]\n    \\centering\n    \\small\n    \\tabcolsep=0.16cm\n    \\begin{tabular}{r|c|c|c|c}\n    \\toprule\n    & \\multicolumn{4}{c}{Within/Across Speaker $\\downarrow$} \\\\\n    & En & Fr & Zh & Std. $\\downarrow$ \\\\\n    \\midrule\n    MFCC & 12.1/23.4 & 12.6/25.5 & 11.5/21.3 & - \\\\\n    Sup.$^\\dagger$ & 6.2/8.0 & 8.7/10.8 & 7.9/10.3 & - \\\\\n    LL-6K$^\\ddagger$ & 4.5/6.2 & 8.4/12.7 & 8.2/8.2 & 1.8/2.7 \\\\\n    \\midrule\n    \\multicolumn{5}{l}{\\textit{\\vp}} \\\\\n    \\midrule\n    En-500 & 6.9/9.9 & 9.6/14.5 & 8.7/9.7 & 1.1/2.2 \\\\\n    Fr-500 & 8.1/12.1 & 9.1/13.8 & 9.2/10.1 &  0.5/1.5 \\\\\n    En+Fr-500 & 6.9/9.8 & 9.0/13.1 & 8.6/9.6 & 0.9/1.6 \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{\\textbf{Phoneme discriminability of unsupervised features across languages.} We report ABX discriminability score on the 10s test set from ZeroSpeech 2017$^\\dagger$ for English (``En\"), French (``Fr\") and Mandarin (``Zh\"). We compare our models with the MFCC baseline, the supervised topline and the state-of-the-art monolingual (English) model$^\\ddagger$. We measure the generality of the representations by standard deviation (``Std.\") of the scores across the 3 languages. We see that multilingual representations generalize better and are more robust on unseen languages. $^\\dagger$~\\citet{dunbar2017zero}. $^\\ddagger$~\\citet{riviere2020unsupervised_wild}.}\n    \\label{tab:zerospeech17}\n\\end{table}\n\\begin{table*}[t]\n    \\centering\n    \\small\n    \\tabcolsep=0.16cm\n    \\begin{tabular}{r@{\\hs{1.2}}|c@{\\hs{1.2}}c@{\\hs{1.2}}c@{\\hs{1.2}}c@{\\hs{1.2}}|c@{\\hs{1.4}}c@{\\hs{1.4}}c@{\\hs{1.4}}c@{\\hs{1.4}}c@{\\hs{1.4}}|c@{\\hs{1.4}}c@{\\hs{1.4}}c@{\\hs{1.4}}c@{\\hs{1.4}}c@{\\hs{1.4}}|c@{\\hs{1.2}}c}\n    \\toprule\n    & PT & PT & \\multicolumn{2}{c|}{Langs.} & \\multicolumn{5}{c|}{PER $\\downarrow$ (\\vp~Langs.)} & \\multicolumn{5}{c|}{PER $\\downarrow$ (Other Langs.)} & \\multicolumn{2}{c}{PER} \\\\\n    & Domain & Hours & In & Out & Es & Fr & It & Nl & Sv & Ky & Ru & Tr & Tt & Zh & Avg. $\\downarrow$ & Std. $\\downarrow$ \\\\\n    \\midrule\n    m-CPC$^\\dagger$ & Out & 60K & 0 & 1 & 36.4 & 44.3 & 37.8 & 43.1 & 46.5 & 37.5 & 42.4 & 45.7 & 40.6 & 53.2 & 42.7 & 4.8 \\\\\n    \\midrule\n    \\multicolumn{10}{l}{\\textit{wav2vec 2.0 Base (95M)}} \\\\\n    \\midrule\n    XLSR-Mono$^\\ddagger$ & In & $<$0.4K & 1 & 0 & \\textbf{6.8} & 10.4 & 10.9 & 37.4 & 63.6 & 29.6 & 11.6 & 44.0 & 21.4 & 31.4 & 26.7 & 17.2 \\\\\n    XLSR-10$^\\ddagger$ & In & 1.4K & 10 & 1 & 9.4 & 13.4 & 13.8 & 16.3 & 21.0 & 8.6 & 11.2 & \\textbf{11.7} & 8.3 & 24.5 & 13.8 & 5.1 \\\\\n    VP-Mono-5K & Out & 4.5K & 1 & 0 & \\textbf{6.8} & \\textbf{8.6} & \\textbf{7.5} & \\textbf{9.7} & \\textbf{9.3} & - & - & - & - & - & - & - \\\\\n    VP-10K & Out & 10K & 5 & 18 & 8.5 & 11.9 & 11.0 & 13.6 & 15.0 & 10.9 & 12.4 & 13.1 & 8.8 & 19.3 & 12.5 & 3.0 \\\\\n    VP-100K & Out & 100K & 5 & 18 & 7.6 & 10.3 & 9.7 & 12.2 & 13.0 & 9.4 & \\textbf{10.7} & \\textbf{11.7} & \\textbf{8.0} & \\textbf{17.5} & \\textbf{11.0} & \\textbf{2.7} \\\\\n    \\midrule\n    \\multicolumn{10}{l}{\\textit{wav2vec 2.0 Large (317M)}} \\\\\n    \\midrule\n    XLSR-10$^\\ddagger$ & In & 1.4K & 10 & 1 & 7.9 & 12.6 & 11.7 & 14.0 & 20.6 & 7.0 & 9.3 & 9.7 & 7.2 & 22.8 & 12.3 & 5.2 \\\\\n    XLSR-53$^\\ddagger$ & In+Out & 56K & 10 & 43 & \\textbf{2.9} & \\textbf{5.0} & 6.7 & \\textbf{5.8} & 12.2 & \\textbf{6.1} & \\textbf{8.1} & \\textbf{7.1} & \\textbf{5.1} & 18.3 & \\textbf{7.6} & 4.2 \\\\\n    VP-Mono-5K & Out & 4.5K & 1 & 0 & 5.5 & 7.0 & \\textbf{6.1} & 7.2 & \\textbf{6.3} & - & - & - & - & - & - & - \\\\\n    VP-10K & Out & 10K & 5 & 18 & 6.3 & 8.9 & 7.9 & 9.3 & 9.7 & 9.3 & 9.2 & 11.3 & 7.6 & 18.8 & 9.8 & 3.2 \\\\\n    VP-100K & Out & 100K & 5 & 18 & 5.4 & 7.7 & 6.5 & 8.0 & 8.3 & 8.5 & 8.0 & 9.8 & 6.9 & \\textbf{17.3} & 8.6 & \\textbf{3.1} \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{\\textbf{Few-shot ASR with out-of-domain out-of-language unsupervised pre-training.} We adopt the Common Voice (CV) few-shot phoneme recognition setup$^\\dagger$ and report test PER (phone error rate).\n    Our wav2vec 2.0 models are pre-trained on \\vp~(out-of-CV-domain) either with 4.5K-hour monolingual data (``VP-Mono-5K\") or 10K-hour/100K-hour multilingual data (``VP-10K\" and ``VP-100K\"). Pre-training languages may include the ones being evaluated (``In\") and others (``Out\"). Our models outperform XLSR-Mono and XLSR-10 (same architecture as ours but using in-domain CV data) on most languages with out-of-domain and (partially) out-of-language pre-training. Our best model (VP-100K Large) performs competitively to XLSR-53, which leverages 52K-hour out-of-CV-domain data in addition to the CV data. $^\\dagger$~\\citet{riviere2020unsupervised} $^\\ddagger$~\\citet{conneau2020unsupervised}}\n    \\label{tab:cv_eval_per}\n\\end{table*}\\begin{table}[t]\n\\centering\n\\small\n\\begin{tabular}{r|c@{\\hs{1.2}}c@{\\hs{1.2}}c@{\\hs{1.2}}|c@{\\hs{1.2}}c@{\\hs{1.2}}c}\n\\toprule\n & \\multicolumn{3}{c|}{Train Hours} & \\multicolumn{3}{c}{Test WER $\\downarrow$} \\\\\n  & De & Fr & Es & De & Fr & Es \\\\\n\\midrule\nBaseline$^{\\dagger}$ & 1582 & 787 & 660 & 12.8 & 19.4 & 16.5 \\\\\n\\midrule\nVP-50K & 314 & 364 & 203 & 17.0 & 18.8 & 11.9 \\\\\n+ LM & (20\\%) & (46\\%) & (31\\%) & \\textbf{7.8} & \\textbf{9.6} & \\textbf{10.0} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\\textbf{ASR with out-of-domain unsupervised pre-training and less supervision.} We report test WER on Common Voice (CV). Top: supervised baseline trained on the combination of an extended CV train set and several other corpora (decoding with LM). Bottom: our wav2vec 2.0 \\emph{Base} model pre-trained on 50K-hour \\vp~data (out-of-CV-domain) and fine-tuned on the standard CV train set (a subset of the baseline's one). We optionally use 4-gram LMs trained on CV for decoding. Our model outperforms the baseline (even without LM) while using less supervised train data. $^{\\dagger}$Deepspeech Polyglot.}\n\n\\label{tab:wer_scores_main_cv}\n\\end{table}\n\\begin{table*}[t]\n    \\centering\n    \\small\n    \\begin{tabular}{r|cc|cc|cc||cc|cc|cc}\n    \\toprule\n     & \\multicolumn{2}{c|}{Fr$\\rightarrow$En $\\uparrow$} & \\multicolumn{2}{c|}{Es$\\rightarrow$En $\\uparrow$} & \\multicolumn{2}{c||}{De$\\rightarrow$En $\\uparrow$} & \\multicolumn{2}{c|}{Fr $\\downarrow$} & \\multicolumn{2}{c|}{Es $\\downarrow$} & \\multicolumn{2}{c}{De $\\downarrow$}  \\\\\n     \\midrule\n     Train hours (EP+CV) & \\multicolumn{2}{c|}{38+264} & \\multicolumn{2}{c|}{32+113} & \\multicolumn{2}{c||}{42+184} & \\multicolumn{2}{c|}{38+264} & \\multicolumn{2}{c|}{32+113} & \\multicolumn{2}{c}{42+184} \\\\\n     Test set & EP & CV & EP & CV & EP & CV & EP & CV & EP & CV & EP & CV \\\\\n    \\midrule\n    (Cascaded) Baseline$^{\\dagger}$ & 25.4 & 27.6 & 26.5 & 27.4 & 21.3 & 21.0 & 24.3 & 18.3 & 15.0 & 21.4 & 19.8 & 16.0 \\\\\n    Our end-to-end baseline & 24.5 & 27.0 & 20.5 & 26.6 & 17.5 & 20.0 & 20.8 & 18.8 & 17.2 & 14.1 & 23.2 & 18.4 \\\\\n    With 800h self-training & 26.7 & 28.6 & 22.4 & 26.8 & 18.8 & \\textbf{20.1} & 19.5 & 17.3 & 15.6 & 13.7 & 21.8 & 17.5 \\\\\n    With 3000h self-training & \\textbf{27.4} & \\textbf{28.9} & \\textbf{22.7} & \\textbf{27.3} & \\textbf{19.6} & 20.0 & \\textbf{19.0} & \\textbf{17.0} & \\textbf{15.3} & \\textbf{13.2} & \\textbf{21.4} & \\textbf{17.3} \\\\\n    \\midrule\n    400h weakly labeled & 22.9 & 10.1 & 22.2 & 10.9 & 18.0 & 8.8 & \\\\\n    + labeled & \\textbf{31.1} & \\textbf{30.3} & \\textbf{28.4} & \\textbf{29.7} & \\textbf{24.4} & \\textbf{23.4} & \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\caption{\\textbf{ST and ASR using \\vp~data for self-training or weak supervision.} Left: test BLEU for ST models. Right: test WER for ASR models. We evaluate in-\\vp-domain performance with EuroParl-ST (EP) and the out-of-domain performance with CoVoST 2 (CV). We combine both corpora to train our baseline and pseudo-label 3K-hour monolingual \\vp~unlabeled data for self-training.\n    For ST training with weak supervision, we combine EP, CV and 300h weakly labeled data from \\vp. Both approaches for leveraging \\vp~data improve in-domain (EP) and out-of-domain (CV) performance simultaneously. $^\\dagger$~EP baselines from \\citet{iranzo2020europarl} and CV baselines from \\citet{wang2020covost}.}\n    \\label{tab:st_self_training_eval}\n\\end{table*}\n\n\\subsection{Experimental Setup}\nFor representation learning, we perform speaker diarization before VAD-based segmentation so that each utterance contains exactly one speaker. We augment the data with time dropout, pitch modification and reverberation~\\citep{kharitonov2020data} during model training.\n\nFor non-wav2vec models, we extract 80-dimensional log-mel filterbank speech features with 25ms windows size and 10ms shift. We apply per-utterance CMVN (cepstral mean and variance normalization) to the extracted features. For GPU memory efficiency, we remove training samples that have more than 60 seconds of speech or have more than 1024 characters.\n\nWe train wav2vec 2.0~\\citep{baevski2020wav2vec} models with original hyper-parameter settings using fairseq~\\citep{ott2019fairseq}, except for Table~\\ref{tab:wer_scores_main_cv} where we use wav2letter~\\citep{pratap2018w2l} and follow~\\citet{talnikar2020joint} to do finetuning using both supervised CTC~\\citep{graves2006ctc} loss and unsupervised wav2vec 2.0 loss. The largest model (``VP-100K\") takes 10 days on 128 V100 GPUs for 1M updates. For non-wav2vec models, we train Transformer~\\citep{NIPS2017_3f5ee243} with cross-entropy criterion using fairseq S2T~\\citep{wang2020fairseqs2t}. For Section~\\ref{sec:asr_baselines} and Section~\\ref{sec:unsupervised_pretraining}, we use phoneme vocabularies for models that we evaluate with PER (phone error rate) and character vocabularies for the other. For Section~\\ref{sec:self_training}, we use Unigram~\\cite{kudo-richardson-2018-sentencepiece} vocabularies with 2K subwords for all models. To improve ST model training, we pre-train the encoder on the LibriSpeech~\\citep{panayotov2015librispeech} ASR task.\n\nWe use the best checkpoint by validation loss for evaluation, except for Section~\\ref{sec:self_training} where we average the 10 best checkpoints.\nWe build n-gram language models for decoding (when specified) using KenLM~\\citep{heafield2011kenlm}.\n\n\\subsection{Speech Recognition (ASR) Baselines}\n\\label{sec:asr_baselines}\n\nWe provide monolingual Transformer baselines for the 14 languages that have more than 10 hours of transcribed data (see Table~\\ref{tab:unlabeled_transcribed_stats}). Both development and test WER are reported in Table~\\ref{tab:vp_asr_eval}. We see that several low-resource languages (Fi, It, Hr, Sk and Sl) suffer from high recognition errors ($>$40\\% WER) due to the lack of training data. Even the highest resource one (En) has a high WER of around 30\\%.\n\n\\subsection{Unsupervised Representation Learning}\n\\label{sec:representation_learning}\nWe follow the setting in~\\citet{riviere2020unsupervised} to evaluate unsupervised speech representations by phoneme discriminability on 3 languages (English, French and Mandarin), and report ABX discriminability score~\\citep{schatz2013evaluating} on the 10s test set from ZeroSpeech 2017~\\citep{dunbar2017zero}. Standard deviation (``Std.\") of the scores across the 3 languages is also reported as a measure for the generality of the representations. \nAs previous studies focus on monolingual representations, we explore multilingual representations and examine their generality across languages.\nWe train CPC-based models~\\citep{riviere2020unsupervised_wild} on 500-hour English and 500-hour French unlabeled data from \\vp, respectively. And we combine English and French data with 50\\% sampling (so that the total duration remains the same) for the multilingual setting.\nWe observe from Table~\\ref{tab:zerospeech17} that the multilingual model (``En+Fr-500\") performs comparably to the monolingual ones (``En-500\" and ``Fr-500\") on their seen languages and performs better on unseen language (``Zh\"). Its scores vary less across languages (lower ``Std.\") compared to ``En-500\". The variance of the scores is comparable to ``Fr-500\" while the average is lower. We conclude that multilingual representations generalize better across languages and are more robust on unseen languages. For quick exploration, we leverage only part of the \\vp~unlabeled data and leave the validation on more data to future work.\n\n\\subsection{Semi-Supervised Learning}\nWe explore two semi-supervised learning settings for the application of \\vp~unlabeled data: unsupervised pre-training followed by supervised fine-tuning for ASR and self-training for ASR as well as ST.\n\\subsubsection{ASR with Unsupervised Pre-Training}\n\\label{sec:unsupervised_pretraining}\nSelf-supervised (unsupervised) pre-training such as wav2vec 2.0~\\citep{baevski2020wav2vec} substantially reduces the need of labeled data in ASR. Furthermore, multilingual pre-training~\\citep{conneau2020unsupervised} allows cross-lingual transfer, which brings extra gains especially to low-resource languages. Pre-training wav2vec 2.0 models is, however, resource-intensive and hence re-training models for each task with different domains is impractical. With the large-scale multilingual data in \\vp,\nwe explore if scaling multilingual pre-training can take us towards the one-model-fits-all paradigm by alleviating the impacts of domain or language mismatch between pre-training and fine-tuning. We train wav2vec 2.0 models~\\footnote{wav2vec 2.0 \\emph{Base} (95M) unless specified otherwise.} on 10K-hour, 50K-hour and 100K-hour \\vp~data in 23 languages (denoted as ``VP-10K\", ``VP-50K\" and ``VP-100K\", respectively). We also train models with 4.5K-hour monolingual data (denoted as ``VP-Mono-5K\") for comparison. For quick verification, we use only part of the \\vp~unlabeled data for pre-training. We leave training the models on the full 400K-hour data to future work, which is supposed to achieve even better performance.\n\n\\paragraph{In-domain pre-training} We examine the conventional in-domain pre-training setting on the \\vp~ASR benchmark. We evaluate the VP-10K model, where the pre-training data is filtered so that it has no overlaps with the transcribed development and test set. From table \\ref{tab:vp_asr_eval}, we see that pre-training using unlabeled data brings significant gains to all the languages (average 59\\% test WER reduction). The gains are most significant on the low-resource languages, where improvements are qualitative (for example, from nearly 100\\% test WER on Sl down to around 30\\%).\n\n\\paragraph{Out-of-domain pre-training} We examine the out-of-domain pre-training setting using the Common Voice (CV) ASR corpus~\\citep{ardila-etal-2020-common}. In contrast with the political domain oral speech in \\vp, they are more fluent read speech of no copyright sentences (for example, Wikipedia articles). We adopt the few-shot phoneme recognition setup on CV v3 from~\\citet{riviere2020unsupervised}, with which domain adaptation is limited during fine-tuning due to the small data volume --- it has 1-hour train set, 20-minute development set and 1-hour test set for 10 languages including 5 \\vp~ones. We present the performance of VP-Mono-5K, VP-10K and VP-100K with the m-CPC~\\citep{riviere2020unsupervised} and XLSR~\\citep{conneau2020unsupervised} baselines in Table~\\ref{tab:cv_eval_per}, where phone error rate (PER) is reported. The XLSR baselines share the same wav2vec 2.0 architecture as our models but are trained with in-domain CV data. VP-Mono-5K outperforms XLSR-Mono and XLSR-10 on all 5 \\vp~languages (except for a tie on Es with XLSR-Mono). VP-100K outperforms XLSR-10 on 8 (9) out of the 10 languages. VP-100K (Large) overall performs competitively to XLSR-53, which leverages 52K-hour out-of-domain data in addition to the in-domain CV data. Notably, it outperforms XLSR-53 on Zh, which is covered by XLSR-53 but remote from the EU languages in VP-100K. This suggests the high generality of the speech representations VP-100K learned.\n\nWe also evaluate our multilingual model (VP-50K) under the normal setup (CV v5.1) and report test WER in Table~\\ref{tab:wer_scores_main_cv}. \nThey are compared with supervised baselines from  DeepSpeech-Polyglot\\footnote{https://gitlab.com/Jaco-Assistant/deepspeech-polyglot}, which leverage extended CV train sets and several other corpora for training as well as LM for decoding. Our model outperforms the baseline with fine-tuning on the standard CV train set (a subset of the baseline's one), even when not using LM in decoding.\n\n\\paragraph{Out-of-language pre-training}\nIn the few-shot phoneme recognition setup (Table~\\ref{tab:cv_eval_per}), VP-100K does not cover 5 of the 10 CV languages (Ky, Ru, Tr, Tt and Zh) in pre-training, but leverages data from 18 additional EU languages. It outperforms the in-domain in-language XLSR baselines on most of the uncovered languages (except Ky which is a remote central Asian language). Moreover, it performs more stably across all the 10 languages with a smaller variance (standard deviation) on PER.\n\n\\subsubsection{Self-Training for ASR and ST}\n\\label{sec:self_training}\nSelf-training~\\citep{scudder1965probability} is a classical semi-supervised learning approach, where unlabeled data is equipped with pseudo-labels from a supervised model and then combined with labeled data for model training. We use the combination of EuroParl-ST~\\citep{iranzo2020europarl} and CoVoST 2~\\citep{wang2020covost} for both ASR and ST labeled data in 3 languages (directions). The former is created from 2009-2012 EP plenary sessions and hence has the same domain as \\vp. The latter is based on Common Voice v4, which has different domain than \\vp~and dominates the combined train set. We train Transformer \\emph{Base}~\\citep{NIPS2017_3f5ee243} supervised baselines and use 0.8K/3K-hour monolingual \\vp~unlabeled data (from 2013-2020 sessions only to avoid overlaps with EuroParl-ST) to self-train Transformer \\emph{Large} models. We upsample labeled data in self-training so that it has the same duration as the unlabeled one. We observe from Table~\\ref{tab:st_self_training_eval} that self-training on \\vp~improves both in-domain (``EP\") and out-of-domain (``CV\") performance with similar magnitude most of the time. For ST, self-training helps to narrow the gap between end-to-end models and the cascaded ones (more labeled data available) without the addition of expensive labeled data.\n\n\\subsection{Weakly Supervised ST}\nWe evaluate the quality of the weakly labeled ST data from our speech-to-speech alignment on the same benchmark as the self-training experiments. This also provides an indirect evaluation for our alignment pipeline since imprecise alignments hurt the ST label quality. \nWe examine the performance of weakly supervised training as well as joint training using both labeled and weakly labeled data. We see from Table~\\ref{tab:st_self_training_eval} that the former is on par with (or better than) the supervised baseline in the \\vp~domain (``EP\") with 0.3x-1.8x more training data than the baseline. Joint training brings substantial gains to both in-domain (``EP\") and out-of-domain (``CV\") performance, and it outperforms self-training. This suggests that our weakly labeled data (0.4K hours) is much more informative and efficient than the pseudo-labeled data (3K hours) when combined with labeled data.\n\n\\section{Related Work}\n\n\\paragraph{Multilingual speech corpora}\nLibriLight~\\citep{kahn2020libri} currently represents the largest scale unlabeled speech corpus but it is limited to English.\nMLS~\\cite{Pratap2020} is a recently released large-scale multilingual corpus of read speech in 8 languages, derived from LibriVox.\nMAILABS\\footnote{https://www.caito.de/2019/01/the-m-ailabs-speech-dataset} is also derived from Librivox and has about 1000 hours available in 9 languages.\nWhile MLS and MAILABS are derived from audiobooks, VoxForge\\footnote{http://www.voxforge.org} and Common Voice~\\citep{ardila-etal-2020-common} gather data via crowd-sourcing. VoxForge collected data in about 15 different languages with about 300 hours of speech in total; Common Voice currently supports 60 languages for a total of 7327 validated hours available.\nThe CMU Wilderness dataset~\\citep{black2019cmu} collects readings from the New Testament, with 700 different languages available.\nIARPA Babel program\\footnote{https://www.iarpa.gov/index.php/research-programs/babel} collected data for 24 languages, mostly\nfrom conversational telephone speech. The dataset is however not released and under an open license, and focused on low-resource languages, with labeled data ranging between 25 to 65 hours per language.\n\n\\paragraph{Speech-to-Text and Speech-to-Speech Translation}\n\nApart from machine translation~\\citep{koehn2005europarl}, the European Parliament open data has fostered the development of corpora for speech-to-text translation and for simultaneous interpretation. EuroParl-ST~\\citep{iranzo2020europarl} is a multilingual speech-to-text translation corpus with translations between 6 European languages (En, Fr, De, Es, It and Pt). Similarly, EPIC~\\citep{bendazzoli2005approach} is derived from the European Parliament with simultaneous interpretation speeches in Italian, English and Spanish. CIAIR~\\citep{tohyama2004ciair} and STC~\\citep{shimizu2014collection} are simultaneous interpretation corpora between English and Japanese with a total of about 180 hours for the former, while the latter is currently unavailable for download. The MaSS dataset~\\citep{zanon-boito-etal-2020-mass} also provides speech to speech alignments for about 8k utterances across 8 languages, for a total of about 23h of speech.\n\n\\section{Conclusion}\n\nIn this paper, we introduce a large-scale multilingual speech corpus, \\vp, for representation learning, semi-supervised learning and interpretation. \\vp~provides the largest open unlabeled speech data to date, which has broad applications including unsupervised pre-training and self-training. \\vp~is also the first corpus for large amounts of open speech-to-speech interpretation data.\nWe provide \\vp~ASR baselines and validate the versatility of \\vp~unlabeled data in semi-supervised learning under challenging out-of-domain settings. The corpus is available\nat \\url{https://github.com/facebookresearch/voxpopuli}.\n\n\\section{Acknowledgements}\nWe thank Gabriel Synnaeve, Tatiana Likhomanenko, Jade Copet, Vineel Pratap, Jiatao Gu and Alexis Conneau for helpful discussions on the project.\n\n\\section{Ethical Considerations}\nWe acknowledge the European Union (EU) for creating and publishing the materials used by \\vp.\nWe will add citations as well as acknowledgements in our release.\nWe paid the market price to transcription vendors for the human annotations we collected. \\vp~includes all available speeches from the 2009-2020 EP events without any selections on the topics or speakers. The speech contents represent the standpoints of the speakers in the EP events, many of which are EU officials.\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Scribosermo: Fast Speech-to-Text models for German and other Languages}\n\n\\begin{document}\n\n\\maketitle\n  \n\\begin{abstract}\n\tRecent Speech-to-Text models often require a large amount of hardware resources and are mostly trained in English. This paper presents Speech-to-Text models for German, as well as for Spanish and French with special features:\n\t(a) They are small and run in real-time on microcontrollers like a RaspberryPi.\n\t(b) Using a pretrained English model, they can be trained on consumer-grade hardware with a relatively small dataset.\n\t(c) The models are competitive with other solutions and outperform them in German.\n\tIn this respect, the models combine advantages of other approaches, which only include a subset of the presented features.\n\tFurthermore, the paper provides a new library for handling datasets, which is focused on easy extension with additional datasets and shows an optimized way for transfer-learning new languages using a pretrained model from another language with a similar alphabet.\n\\end{abstract}\n\n\\noindent\\textbf{Index Terms}: fast speech to text, multilingual transfer-learning, automatic speech recognition, embedded hardware\n\n\\section{Introduction}\n\\label{sec:intro}\n\nSpeech-to-Text models based on neural networks are mostly trained in English and often require large amounts of training resources. But there exist many other languages and those who are interested in training a speech-to-text system for their own language do not always have access to high-performance server hardware. A few papers and projects focus on the aforementioned problems, but most are solving them only partially.\n\n\\vspace{9pt}\nThe authors of \\textit{IMS-Speech} \\cite{IMSDE} trained a German STT model, which so far had the best results on the German \\textit{Tuda} dataset \\cite{TUDA}. In a comparison with Google's STT service (executed 01/2019), their network could outperform it in English as well as in German.\n\nIn \\textit{VoxPopuli} \\cite{VOXPOP}, an approach for training multilingual models using a large unlabeled dataset is investigated. A mix of 50k hours of unlabeled data in different languages from the European Parliament and a comparatively small labeled dataset for semi-supervised training are used. This approach proved very effective and achieves a Word-Error-Rate (WER) of \\SI{7.8}{\\percent}\\,$/$\\,\\SI{9.6}{\\percent}\\,$/$\\,\\SI{10.0}{\\percent} in German\\,$/$\\,Spanish\\,$/$\\,French on the \\textit{CommonVoice} datasets~\\cite{COMV}, which so far have been the best results on these datasets.\n\n\\textit{Luo et al.} \\cite{QNTECR} used the same network architecture as this work, but in Nvidia's original implementation, and also trained it for other languages like German or Spanish, using very small datasets and following a different transfer-learning approach of reinitializing the last network layer if the alphabet changes. \n\nMozilla's \\textit{DeepSpeech} project \\cite{DEPSPE} provides pretrained English models that are relatively small and one of the few that are able to run in real-time on a RaspberryPi. It achieves a WER of \\SI{7.1}{\\percent} on the \\textit{LibriSpeech} \\cite{LIBSPE} testset. Some early experiments on multilingual trainings have been run with this network, but performance was much lower than the results presented in the following chapters. They still can be found in the project's repository which is linked later.\n\n\\textit{Park et al.} \\cite{FNNMED} built a model for embedded devices, which reached a WER of \\SI{9.0}{\\percent} on \\textit{LibriSpeech} and could run on an ARM-Cortex-A57. \\textit{Zhang et al.} \\cite{TINTRA} trained a very small model on a large in-house Chinese dataset which can run faster than real-time on an ARMv7 chip. \\textit{He et al.} \\cite{STRSRM} did train an English model on a very large in-house dataset which can run twice as fast than real-time on a Google-Pixel smartphone.\n\n\\textit{Ghoshal et al.} \\cite{MLDNN} and \\textit{Thomas et al.} \\cite{MLFDNN} did run early explorations of transfer-learning for different languages using deep neural networks. The first approach replaces the last language specific layer of a network with a new one and finetunes the whole network on the new language, while the second uses a multilingual training of the first network layers, and different output layers for each language.\n\n\\vspace{9pt}\nThis paper presents a small Speech-to-Text model for German, as well as for Spanish and French, that combines the advantages of the aforementioned approaches. The main contributions of project \\textit{Scribosermo} are:\n(a) The models are competitive with the models from \\textit{IMS-Speech} and \\textit{VoxPopuli}.\n(b) Providing pretrained models in multiple languages that can run in real-time even on single-board computers like a RaspberryPi.\n(c) The models can be trained on a relatively small dataset, like the models from \\textit{VoxPopuli} and only require consumer-grade hardware for training.\n(d) Shows a fast transfer-learning approach with a single step through the concept of alphabet adaption.\n(e) Improved SOTA performance for German STT.\n\nFurthermore, the paper provides a new library for handling datasets, which is focused on easy extension with additional datasets and shows a simple way for transfer-learning new languages using a pretrained model from a language with an almost similar alphabet, which is demonstrated for English to Spanish and Spanish to Italian transfer-learning.\n\nThe training code and models are provided as open source at: \\textit{https://gitlab.com/Jaco-Assistant/Scribosermo}\n\n\\noindent For reasons of readability, the results of the experiments are not presented in full detail, but can instead be found in the project's repository.\n\n\\section{Pre-processing}\n\\label{sec:prep}\n\nThe datasets are converted into single channel 16\\,kHz audio with \\textit{wav} encoding and a \\textit{tab} separated \\textit{csv} file for each partition, with at least the keys \\textit{duration}, \\textit{filepath} and \\textit{text}. Afterwards an additional data cleaning step is executed.\nAll numbers are converted to their textual form, as well as commonly used units like \\textit{kg} or \\textit{m}. After replacing some special characters (like \\textit{}\\textrightarrow{}\\textit{ae}), all remaining characters, which do not match the used language's alphabet are removed. All of those rules are collected in a simple \\textit{json} file, to ease adding new languages. \n\nIn early training executions the transcriptions of some files did not match the recordings, which resulted in errors if they were much too short or much too long. Therefore, and in order to improve training speed, an automatic cleaning process was implemented, which excludes all files matching one of the following metrics:\n\n\\begin{enumerate}\n\t\\itemsep0.1em\n\t\\item Audio shorter than half a second.\n\t\\item Audio longer than 30 seconds.\n\t\\item Transcription has more than 512 characters.\n\t\\item Recording is spoken 2x faster than the average.\n\t\\item Less than one character per three seconds is spoken.\n\t\\item (chars/second $<$ average$\\times$3) and (duration $>$ average/5).\n\\end{enumerate}\n\nThe second and third items are used to exclude long files to allow for a greater training batch size. The forth and fifth metrics exclude too quickly or too slowly spoken utterances. The last is intended for slow recordings, too, but with an exception for short clips, because those may have longer pauses at the start or end of the recording.\n\n\\section{Language model}\n\\label{sec:langm}\n\nTo improve the predicted transcriptions of the trained network, the predictions are rescored with a 5-gram language model. For German a large 8-million sentence collection from \\cite{SENDE} is used and combined with the transcriptions from the training dataset. The same text normalization steps as described in the last chapter are executed. In Spanish and French the \\textit{Europarl} and \\textit{News} sentence collections from \\cite{STATMT} are used additionally, and the Italian training sentences are extended with the \\textit{Mitads} dataset \\cite{MITADS} The language model is created with \\textit{Poco-LM} \\cite{POCOLM} and optimized with tools provided by Mozilla's \\textit{DeepSpeech} project \\cite{DEPSPE}. For decoding their \\textit{ds-ctcdecoder} is used as well. The language models were filtered to a maximum size of \\mbox{165M n-grams}, which results in a size of about 850MB.\n\n\\section{Experiments with QuartzNet}\n\\label{sec:exqn}\n\nFor the experiments the \\textit{QuartzNet} architecture \\cite{QNET} was implemented, using the open source code from Nvidia's \\textit{NeMo} project \\cite{NEMO} as reference.\nThe QuartzNet architecture (Figure~\\ref{fig:qn}) was chosen, because its size is comparatively small, which results in fast inference on standard computers and low power devices. Nvidia provides pretrained weights for English, which reach a greedy WER of \\SI{3.8}{\\percent} on the LibriSpeech devset.\n\n\\begin{figure}[htb]\n\t\\centering\n\t\\includegraphics[width=0.8\\linewidth]{img_quartznet_architecture.png}\n\t\\caption{Network architecture of Nvidia's \\textit{QuartzNet} \\cite{QNET}.}\n\t\\label{fig:qn}\n\\end{figure}\n\n\\subsection{Reimplementation in TensorFlow}\n\nInstead of directly using Nvidia's PyTorch implementation, the network was reimplemented for TensorFlow. The main reason was that the trained network can be directly converted into the \\textit{TensorFlow-Lite} format, which greatly improves inference speed on low power devices. Another benefit was that the tools already implemented for Mozilla's \\textit{DeepSpeech} framework and some of their data augmentation features could be integrated more easily into the new project.\n\nThe pretrained weights from Nvidia's \\textit{NeMo} project were transferred layer by layer from the PyTorch format to TensorFlow using \\textit{Open Neural Network Exchange (ONNX)} as intermediate format.\nWhile this did work well for the network itself, there were problems due to differences in PyTorch's and TensorFlow's spectrogram calculation. To reduce the impact of these, the transferred network was trained for four additional epochs on the LibriSpeech dataset. The performance is still slightly worse than Nvidia's reference implementation, but much better than Mozilla's current \\textit{DeepSpeech} release (Table~\\ref{tab:qen}).\n\n\\begin{table}[H]\n\t\\caption{Performance on English LibriSpeech dataset. Sometimes predictions are rescored with an additional 5-gram language model (LM), else the greedy WER is measured.}\n\t\\label{tab:qen}\n\t\\centering\n\t\\begin{tabular}{llc}\n\t\t\\toprule\n\t\t\\textbf{Network} & \\textbf{Notes} & \\textbf{WER} \\\\\n\t\t\\midrule\n\t\tDeepSpeech & LS-test-clean + LM & \\SI{7.06}{\\percent} \\\\\n\t\t\\midrule\n\t\tQuartzNet15x5 & Nvidia, LS-dev-clean & \\SI{3.79}{\\percent} \\\\\n\t\tQuartzNet15x5 & Converted, LS-dev-clean & \\SI{5.15}{\\percent} \\\\\n\t\tQuartzNet15x5 & Trained, LS-dev-clean & \\SI{4.35}{\\percent} \\\\\n\t\tQuartzNet15x5 & above, LS-test-clean & \\SI{4.57}{\\percent} \\\\\n\t\tQuartzNet15x5 & above, LS-test-clean + LM & \\SI{3.71}{\\percent} \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\\end{table}\n\n\\subsection{Training in German}\n\nFor the following trainings the \\textit{CommonVoice} (v6) dataset was used. The full training partitions are larger than the amount used in \\textit{VoxPopuli} \\cite{VOXPOP}, therefore a random subset was selected to match the overall duration.\nIn order to get the same alphabet as in English, the German umlauts (\\textit{},\\,\\textit{},\\,\\textit{}) have been replaced with their transliteration (\\textit{ae},\\,\\textit{oe},\\,\\textit{ue}).\n\nTable~\\ref{tab:rqn} shows that the implemented \\textit{QuartzNet15x5} network and training procedure, named \\textit{Scribosermo} in the table, can outperform other approaches for German speech recognition. The training setup of simply training over a pretrained English network, without any further changes, is straightforward, and does not require a semi-supervised pretraining on multiple languages.\n\n\\begin{table}[!htbp]\n\t\\caption{German training results. Above networks have been tested on CommonVoice, below on Tuda dataset.}\n\t\\label{tab:rqn}\n\t\\centering\n\t\\begin{tabular}{llcc}\n\t\t\\toprule\n\t\t\\textbf{}  & \\textbf{Notes}  & \\textbf{Duration} & \\textbf{WER} \\\\\n\t\t\\midrule\n\t\tScribosermo & using CV v6 & 314h  & \\SI{7.7}{\\percent} \\\\\n\t\tVoxPopuli \\cite{VOXPOP} & using CV v5 & 314h &\n\t\t\\SI{7.8}{\\percent} \\\\\n\t\tLuo et al. \\cite{QNTECR} & greedy on devset & 119h &\n\t\t\\SI{18.7}{\\percent} \\\\\n\t\t\\midrule\n\t\tScribosermo & above model & 314h & \\SI{11.7}{\\percent} \\\\\n\t\tIMS-Speech \\cite{IMSDE} & mixed dataset & 806h & \\SI{12.0}{\\percent} \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\\end{table}\n\nIn Table~\\ref{tab:tdts} some different training modalities are investigated. The first section uses the complete German training partition of \\textit{CommonVoice}, which slightly improves the results. This training took 3~days on a PC with two \\mbox{1080Ti GPUs}, which shows that the training process itself is very fast, too, and can be executed on consumer-grade hardware.\nThe second part shows that training results can be improved by a larger margin if the training is run again with the same parameters, but using the current model checkpoint as initialization model. This follows the ideas of \\textit{Stochastic Gradient Descend with Restart} \\cite{SGDR}, but uses the already implemented early-stopping with learning rate reductions on plateaus approach instead of a cosine annealing learning rate.\n\n\\begin{table}[H]\n\t\\caption{Testing different training setups.}\n\t\\label{tab:tdts}\n\t\\centering\n\t\\begin{tabular}{lcc}\n\t\t\\toprule\n\t\t\\textbf{Notes}  & \\textbf{Duration} & \\textbf{WER} \\\\\n\t\t\\midrule\n\t\tfull CV trainset & 720h  & \\SI{7.5}{\\percent} \\\\\n\t\t\\midrule\n\t\tIteration 1 & 314h & \\SI{8.3}{\\percent} \\\\\n\t\tIteration 2 & 314h & \\SI{7.8}{\\percent} \\\\\n\t\tIteration 3 & 314h & \\SI{7.7}{\\percent} \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\\end{table}\n\n\\subsection{Training in other languages}\n\n\\textit{Scribosermo's} approach is competitive in other languages like Spanish and French as well, which is shown in Table~\\ref{tab:rqnef}. To simplify the transfer-learning process the usual two-step frozen and unfrozen training with a reinitialized last layer was replaced with a simpler approach that does not require freezing of parts of the network.\nFirst, the alphabet size of the two languages was reduced, using the rules for cross-word puzzles, which replace letters that contain diacritics and ligatures with their basic form. Using the cross-word puzzles rules has the advantage that they are commonly known and therefore should not pose a problem for humans reading the predicted transcriptions. Following this approach, the French alphabet now has the same letters as the English, only the Spanish has an extra letter\\;(). Thus, the size of the last layer for Spanish still has to be changed, but instead of completely reinitializing the new layer, it is only extended with new weights for the extra letter. Thereby the pretrained English weights for the other letters can be kept, which greatly improves the results, similar to only training over the pretrained weights in German. A future optimization step could include replacing phonetically similar but otherwise different characters in the base alphabet with ones from the target alphabet, which was explored in more depth by \\cite{ZRTCSW} for training-free language adoption.\n\n\\begin{table}[!htbp]\n\t\\caption{Spanish and French training results on CommonVoice testset. Above is Spanish, below is French.}\n\t\\label{tab:rqnef}\n\t\\centering\n\t\\begin{tabular}{llcc}\n\t\t\\toprule\n\t\t\\textbf{}  & \\textbf{Notes}  & \\textbf{Duration} & \\textbf{WER} \\\\\n\t\t\\midrule\n\t\tScribosermo & using CV v6 & 203h  & \\SI{10.9}{\\percent} \\\\\n\t\tVoxPopuli \\cite{VOXPOP} & using CV v5 & 203h & \\SI{9.6}{\\percent} \\\\\n\t\tLuo et al. \\cite{QNTECR} & greedy on devset & 96h &\n\t\t\\SI{15.0}{\\percent} \\\\\n\t\t\\midrule\n\t\tScribosermo & using CV v6 & 364h & \\SI{12.5}{\\percent} \\\\\n\t\tVoxPopuli \\cite{VOXPOP} & using CV v5 & 364h & \\SI{10.0}{\\percent} \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\\end{table}\n\n\\vspace{9pt}\nTo compare the influence of the alphabet extension, a separate experiment was run following the usual training approach of reinitializing the complete last layer for the larger Spanish alphabet. The first part of Table~\\ref{tab:iae} shows that the single-step approach with alphabet extension performs better than a simple single-step training with reinitialization of the last layer, as the deeper layers' weights are not so much influenced by backpropagation of prediction errors coming from the random weights of the last layer at the beginning of the training. Compared to the more usual two-step training which solves this problem it is much faster (trainings were executed on $2\\times$ Nvidia-V100). \n\nSimilar to extending the last layer of the network for new alphabet letters, it is also possible to drop characters to reduce the alphabet size. Following the cross-word puzzle approach, the converted Italian alphabet is the same as the English one. But as Italian sounds more similar to Spanish than to English, it is beneficial to use a Spanish network to train upon, after dropping the extra Spanish letter (Table~\\ref{tab:iae}, second part).\n\n\\begin{table}[H]\n\t\\caption{Influence of finetuning with alphabet extension on Spanish (above) and alphabet shrinking for Italian (below).}\n\t\\label{tab:iae}\n\t\\centering\n\t\\begin{tabular}{lcc}\n\t\t\\toprule\n\t\t\\textbf{Notes} & \\textbf{WER} & \\textbf{Traintime} \\\\\n\t\t\\midrule\n\t\tsingle-step reinitialization & \\SI{11.66}{\\percent} & 18h \\\\\n\t\ttwo-step training & \\SI{11.11}{\\percent} & 13+18h \\\\\n\t\t\n\t\talphabet extension & \\SI{11.05}{\\percent} & 19h \\\\ \n\t\t\\midrule\n\t\t\\textbf{Notes}  & \\textbf{Duration} & \\textbf{WER} \\\\\n\t\t\\midrule\n\t\tEnglish $\\rightarrow$ Italian & 111h  & \\SI{13.8}{\\percent} \\\\\n\t\tSpanish $\\rightarrow$ Italian & 111h  & \\SI{12.2}{\\percent} \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\\end{table}\n\n\\subsection{Inference speed}\nThe main benefit of a small network is fast inference speed on low powered devices. This usually comes with a trade-off of a loss in recognition accuracy as larger models can store more information in their weights, but a fast model that can run even on devices with low computation capabilities is a key feature of this work. The full model itself has a size of about 75MB, the quantized model about 20MB.\n\nThe transcription speed is evaluated in Table~\\ref{tab:insp}, which shows that the presented models are much faster than the model of \\textit{IMS-Speech}, and that they can run faster than real-time on a RaspberryPi. To reduce the memory requirements for very long inputs, they can also be transcribed chunk by chunk in a streaming manner. Here the full CTC-labels were calculated and afterwards given as input to the decoder, similar as in \\textit{IMS-Speech}.\nThe authors of \\textit{VoxPopuli} did not publish the inference speed of their network. A comparison of the network parameter count between their \\textit{wav2vec-base} net, which has about 95M params, and \\textit{QuartzNet15x5} which only has 19M, allows an estimation that it might run about 5x slower.\n\n\\begin{table}[H]\n\t\\caption{Inference Speed, measured as Real Time Factor}\n\t\\label{tab:insp}\n\t\\centering\n\t\\begin{tabular}{llc}\n\t\t\\toprule\n\t\t\\textbf{Device} & \\textbf{Model} & \\textbf{RTF} \\\\\n\t\t\\midrule\n\t\tPC - 1 core AMD3700X &  & $0.24$ \\\\\n\t\tPC - 1 core (unknown) & net of \\textit{IMS-Speech} \\cite{IMSDE} & $14.2$ \\\\\n\t\t\\midrule\n\t\tRaspberryPi-4 - 4gb & tflite full & $1.3$ \\\\\n\t\tRaspberryPi-4 - 4gb & tflite optimized (TLO) & $0.7$ \\\\\n\t\tRaspberryPi-4 - 4gb & \\textit{DeepSpeech} TLO & $0.7$ \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\\end{table}\n\n\\subsection{Training with all datasets}\n\nAfter demonstrating the performance of the presented approach with relatively small datasets, an experiment was run to measure the influence of larger datasets on the transcription performance.\nIn total 37~datasets for German \\cite{VOXF,TUDA,COMV,FRMT,SPRIN,CSS10,GOTH,KURZG,LINGL,MUWI,MALBS,PULSR,SWC,TATO,TERAX,YKOLL,ZAMSP,ALC,BROTH,HMPL,PHATT,PD1,RVG1,RVGJ,SC10,SHC,SI100,SMC,VM1,VM2,WASEP,ZIPTEL,TORST,GLD2,SKYRM,WIT3,MTEDX}, 8~datasets for Spanish \\cite{VOXF,COMV,CSS10,LINGL,MALBS,TATO,MTEDX,LVES}, 7~datasets for French \\cite{VOXF,COMV,CSS10,LINGL,MALBS,TATO,MTEDX} and 5~datasets for Italian \\cite{VOXF,COMV,LINGL,MALBS,MTEDX} were collected. The trainings were continued with the models of the trainings with CommonVoice only, and afterwards the models were finetuned on this dataset again. The results can be found in Table~\\ref{tab:tads} and show that the advantage of using more data is relatively small. Possible explanations might be that the quality of the mixed datasets is not very good or differs too much from the test recordings, or that the small network is reaching its maximum information capacity.\n\n\\begin{table}[!htbp]\n\t\\caption{Training with all accessible datasets  in German (DE), Spanish (ES), French (FR) and Italian (IT). Datasets for testing are either CommonVoice (CV) or Tuda (TD).}\n\t\\label{tab:tads}\n\t\\centering\n\t\\begin{tabular}{llcc}\n\t\t\\toprule\n\t\t\\textbf{Language} & \\textbf{\\#Datasets} & \\textbf{Duration} & \\textbf{WER} \\\\\n\t\t\\midrule\n\t\tDE-CV & $37$ & \\SI{2370}{\\hour}  & \\SI{6.6}{\\percent} \\\\\n\t\tDE-TD &  &   & \\SI{10.2}{\\percent} \\\\\n\t\t\\midrule\n\t\tES-CV & $8$ & \\SI{817}{\\hour}  & \\SI{10.0}{\\percent} \\\\\n\t\tFR-CV & $7$ & \\SI{1028}{\\hour}  & \\SI{11.0}{\\percent} \\\\\n\t\tIT-CV & $5$ & \\SI{360}{\\hour}  & \\SI{11.5}{\\percent} \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\\end{table}\n\n\\section{Corcua}\n\\label{sec:corcua}\n\nIn this chapter the library which was built to handle the above datasets is presented.\nOften speech-to-text frameworks like Mozilla's \\textit{DeepSpeech} or Nvidia's \\textit{NeMo} have customized scripts for downloading and converting a range of supported datasets into their custom dataset format. But many datasets are used in multiple frameworks, so large parts of the scripts have overlapping tasks.\nThe \\textit{audiomate} \\cite{AUDMAT} library was built to ease the use of different audio datasets for machine learning tasks and is able to load 18 different datasets and export them into 4 different speech-to-text frameworks. But extending it with new datasets is quite complicated and requires a deeper understanding of the architecture.\nThe goal in creating \\textit{corcua} was not only to build a library to load different audio datasets and export them to different framework formats, but also to make adding new datasets as easy as possible.\n\n\\vspace{9pt}\n\\textit{Corcua's} architecture is split into three different parts, \\textit{downloading}, \\textit{reading} and \\textit{writing}.\nThe \\textit{downloader's} task is to download and extract a dataset to a local directory. Helper functions for common formats like \\textit{zip} and \\textit{tar.gz} or downloading from a server directory are already pre-implemented.\nA \\textit{reader} loads the audio files, transcriptions and optionally other information included in the dataset into an easy to handle dictionary format and returns a list of items like this:\n\n\\noindent\n\\begin{minipage}{\\linewidth}\n\\begin{lstlisting}\nitem = {\n  \"filepath\": \"path/to/audiofile\",\n  \"speaker\": \"Newton\",\n  \"text\": \"That damn apple!\"\n}\t\n\\end{lstlisting}\n\\end{minipage}\n\nA \\textit{writer} takes a list of dictionaries and saves them into the requested framework's dataset format, like \\textit{csv} or \\textit{json}. It also converts the audio files from different codecs to the commonly used \\textit{wav} encoding.\n\nBesides mere dataset processing, there are also tools to print some statistics about the dataset, like total duration or the most recorded speakers. \\textit{Corcua} also supports splitting datasets into different partitions, like train and test, either randomly or by key separated classes, for example that all utterances of one speaker are in the same partition.\n\n\\vspace{9pt}\nCompared to \\textit{audiomate}, conversions of some datasets are much faster. Converting the German CommonVoice-v5 dataset~(\\SI{701}{\\hour}) with \\textit{audiomate} took about \\SI{12}{\\hour} on a modern CPU, while converting the slightly larger CommonVoice-v6 dataset~(\\SI{777}{\\hour}) with \\textit{corcua} takes less than \\SI{3}{\\hour}. \n\nCurrently \\textit{corcua} can load 34 different datasets (18 of them are German only, 13 are available in more than three languages), and is able to write them into 3 framework formats.\nSome of the multilingual datasets have been extracted from computer games like \\textit{Skyrim} or \\textit{The\\,Witcher}, which often provide high quality dialogs. With the included support of extracting labels from  manually transcribed YouTube videos, it is possible to create audio datasets for almost any language.\n\\textit{Corcua} has been released as open source project and can be accessed under: \\textit{https://gitlab.com/Jaco-Assistant/corcua}\n\n\\section{Conclusion}\n\\label{sec:conclu}\n\nIn this paper small Speech-to-Text models for German, as well as for Spanish, French and Italian, were presented. The models combine the advantages of other approaches. They are competitive with the best models to date on the CommonVoice dataset in German, Spanish and French, as well as with the best one on the German Tuda dataset. At the same time they can run in real-time on single-board computers like a RaspberryPi and can be trained on consumer-grade hardware with a comparatively small dataset.\nThese models are especially interesting for embedded or offline speech applications, for example in smart home systems running on edge-devices with low power consumption, or on smartphones in environments where no stable internet connection is available. Running offline on standard hardware also has advantages if users do not want and companies are not allowed to use cloud providers for privacy reasons.\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{TEVR: Improving Speech Recognition by Token Entropy Variance Reduction}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nThis paper presents TEVR, a speech recognition model\ndesigned to minimize the variation in token entropy w.r.t. to the language model.\nThis takes advantage of the fact that \nif the language model will reliably and accurately predict a token anyway,\nthen the acoustic model doesn't need to be accurate in recognizing it.\nWe train German ASR models with 900~million parameters and show that on CommonVoice German,\nTEVR scores a very competitive $3.64\\%$ word error rate,\nwhich outperforms the best reported results\nby a relative $16.89\\%$ reduction in word error rate.\nWe hope that releasing our fully trained speech recognition pipeline to the community \nwill lead to privacy-preserving offline virtual assistants\nin the future.\n\n\\end{abstract}\n\n\\section{Introduction}\n\\label{section-intro}\n\nAutomated speech recognition has improved at an astonishing pace in recent years. This progress has been enabled in part by access to sufficient computing power for training very large models, such as the wav2vec~2.0~XLS-R~1B~\\citep{wav2vec2_2020, xlsr_2020, xlsr_1B_2021}, which has 900 million parameters. But the crucial ingredient for success have been more advanced models, which massively reduced the cost for obtaining training data. \n\nThe invention of the CTC Loss \\citep{ctc_loss} has made it possible to use unaligned text as the ground-truth signal for speech recognition. Previously, characters and/or phonemes needed to be manually aligned in time to the input audio signal, which was very time consuming. In effect, the CTC Loss has made the preparation of supervised training data $99\\%$ cheaper, thereby paving the way for large data-hungry models.\n\nSimilarly, the wav2vec 2.0 contrastive loss function has made it possible to use arbitrary speech audio data without any ground-truth text for unsupervised pre-training. The result in practice is that researchers can use the audio from unlabeled videos for pre-training their models up to the point where only a few hours of supervised fine-tuning are necessary to produce excellent results.\n\nRegarding the language models, stochastic techniques such as KenLM's on-disk Kneser-Ney smoothing \\citep{kenlm,kenlm-smoothing} have enabled researchers to extract spelling and grammar knowledge out of very large unstructured text collections, such as the OSCAR dataset.\n\nContinuing this tradition, we present TEVR, a stochastic approach for designing recognition tokens based on a large unstructured text collection in such a way that they maximize the information gained by the acoustic speech recognition model without increasing computational complexity or training time.\n\nOur work builds on the pre-trained XLS-R~1B model released by \\citet{xlsr_1B_2021}\nwhich itself is a larger variant of XLSR \\citep{xlsr_2020}\nwhich is the cross-lingual extension of wav2vec 2.0 \\citep{wav2vec2_2020}. \n\nThe model works on partially overlapping 25~ms chunks of audio signal sampled at 16~kHz\nwhich are processed with multiple layers of convolutional embedding \nfollowed by multiple attention layers to produce a contextual encoding\nwhich is then transformed by a typical linear language head into token logits.\nSuch an architecture is currently considered state of the art.\n\nCurrent speech recognition models such as the aforementioned XLS-R~1B are trained using the CTC loss.\nThe CTC loss minimizes an unweighted sum of the cross-entropies for each time-step. During inference, however, the acoustic model is almost always paired with a stochastic language model. This means that the information gained by correctly recognising a given acoustic token varies strongly based on how this token will later be used by the language model.\n\nTo illustrate this concept, consider that \"Danke Herr Tajani\" and \"Dank? Her? Tajani\" contain almost the same acoustic information because both unknown tokens indicated by question marks can be reliably and accurately inferred by the language model. Training to accurately recognize these tokens at the acoustic level is, therefore, superfluous. In fact, it might even reduce overall recognition accuracy by taking up resources which otherwise could have learned more useful features. \n\nIn this work, we aim to correct this loss misallocation by introducing multi-character tokens, which are designed to minimize the inter-token variance of the entropies of the combined acoustic and linguistic likelihood distributions used to predict the token sequence representing the recognized text.\n\n\\section{Training and Testing Data}\n\n\\citet{xlsr_1B_2021} pre-trained the XLS-R~1B model on a total of 436K hours of publicly available audio data from VoxPopuli~\\citep{voxpopuli}, Multilingual Librispeech~\\citep{mls}, CommonVoice~\\citep{commonvoice}, VoxLingua107~\\citep{voxlingua107}, and BABEL~\\citep{babel}.\n\nWe combine the pre-trained convolutional embedding of XLS-R~1B with our own attention encoder and language model and perform additional pre-training matching the instructions in \\citet{xlsr_1B_2021}, followed by stochastic language modelling and task-specific fine-tuning.\n\nWe perform TEVR token extraction on the training texts from CommonVoice~8.0, Multilingual Librispeech, EuroParl~\\citep{eparl}, and OSCAR~\\citep{oscar1,oscar2}.\n\nWe fine-tune for the German Speech Recognition task using CommonVoice~8.0~\\citep{commonvoice}. For easier comparison to literature, e.g. \\citet{scribosermo}, we use CommonVoice~6.1~\\citep{commonvoice} for testing.\n\nFor testing, we generate 4-gram and 5-gram language models based on the texts from CommonVoice~8.0, Multilingual Librispeech, and EuroParl~\\citep{eparl}. \n\n\\section{Experimental Setup}\n\n\\subsection{TEVR Token Extraction}\n\\label{section-tevr}\n\nAs explained in Section \\ref{section-intro}, TEVR tokens are introduced to prevent the loss misallocation caused by uniform weighting of single-character tokens during training of the acoustic model. \n\n\\begin{equation}\n\\label{eq-g}\nG_i \\sim \\delta(t_i-gt_i) = \\left\\lbrace\n\\begin{array}{l} \n1 \\text{ if } t_i = gt_i \\\\ \n0 \\text{ otherwise} \n\\end{array}\n\\right.\n\\end{equation}\n\\begin{equation}\n\\label{eq-p}\nP_i \\sim LM(t_0, \\mathellipsis, t_{i-1}) \n\\end{equation}\n\\begin{equation}\n\\label{eq-ce}\n\\text{lm-entropy}(i) := - \\displaystyle\\sum_{x \\in X} ^{}  \\mathbb{P}_{G_i}(x)[\\text{log} \\mathbb{P}_{P_i}(x)]\n\\end{equation}\n\\begin{equation}\n\\label{eq-ce2}\n\\text{lm-entropy}(i) = - \\text{log} \\mathbb{P}_{P_i}(gt_i)\n\\end{equation}\n\nWe define $\\text{lm-entropy}(i)$ to mean the sparse categorical cross-entropy for the correct prediction of an unknown token $t_i$ based on the likelihood distribution generated by the ByT5 language model conditioned upon the known tokens $t_0$ to $t_{i-1}$. \n\nWe model the sparse categorical ground-truth distribution for token $ t_i $ at time-step $i$ with \\autoref{eq-g}. \nAs indicated by the $\\sim$ notation, the $G_i$ are considered random variables which are sampled independently from the $\\delta(t_i-gt_i)$ distributions.\nCorrespondingly, the predictions $P_i$ are considered random variables, which are independently sampled for each $i$ from the distributions generated by a fully trained causal deep learning language model $ LM $ conditioned on $t_0$ to $t_{i-1}$, as specified in \\autoref{eq-p}.\nWith set $X$ containing the entire vocabulary from which tokens are selected, we use $\\mathbb{P}_{P_i}(x)$ and $\\mathbb{P}_{G_i}(x)$ for $x \\in X$ to designate the discrete probability density functions of random variables $P_i$ and $G_i$ respectively.\nCalculating the cross-entropy of the distribution of $P_i$ relative to the distribution of $G_i$ independently for each time-step $i$ results in \\autoref{eq-ce}. Please note that we treat the time index $i$ as a parameter and do not sum the cross-entropies over time.\n\nDue to the sparseness of the distribution of each $G_i$, this can be simplified to \\autoref{eq-ce2}, where tokens $gt_i$ are the ground-truth for the language model's prediction for each time-step $i$.\nWhen used without a parameter and in reference to a token, we define lm-entropy to mean $ \\text{lm-entropy}(i) $ with $i$ being the index inside the sentence of the current token.\nAs a result, the lm-entropy of a token is a measure for the amount of information, which the acoustic model needs to generate in order for the language model to accurately predict said token.\n\nFigure~\\ref{figure-tevr} shows each letter of the sentence \"Die Katze ist niedlich\" in row (1). \nRow (2) shows the entropies of the likelihood distributions generated by the acoustic model for predicting each single-character token when optimized with a stock CTC loss. \nRow (3) shows the per-character lm-entropies, meaning the entropies of the likelihood distributions generated by a stochastic language model for predicting the next single-character token based on knowledge of previous single-character tokens. \nThis sequence of lm-entropies observed in the real world has a variance of $\\sigma^2=5.0$,\nbut the CTC loss formula assumes an uniform distribution. \nObviously, such a discrepancy between theory and practice heavily skews the gradient, thereby optimizing the acoustic model towards the wrong goal.\n\nTEVR tokens greatly alleviate the issue by reducing the variance of the lm-entropies, hence the name \"Token Entropy Variance Reduction\". This is achieved in three steps:\n\nFirst and as a prerequisite, we need to calculate the per-character lm-entropies for a large collection of words. Inspired by the suggestion in \\citet{byt5} to use raw unicode bytes as tokens for training large T5 \\citep{origt5} transformer language models, we determine the lm-entropy of each character by training a T5 model on a large unstructured collection of German texts consisting of roughly 22~billion words. During training, we predict the next character conditioned upon the beginning of the sentence so far and use the cross-entropy as loss signal. We train the model until convergence.\n\nSecond, we sample the fully trained model by predicting the next character following each possible prefix of each sentence. We record the sequence of $ \\text{lm-entropy}(c=\\text{Groundtruth}_i,i)\\ \\forall i $ and obtain the values shown in Figure~\\ref{figure-tevr} row (3). For example, the partial word \"niedl???\" can be reliably corrected to \"niedlich\" by the language model, since the \"i~c~h\" characters have low lm-entropies in row (3).\n\n\\begin{figure}[h]\n\\begin{center}\n\\includegraphics[width=\\linewidth]{tevr-illustration.png}\n\\end{center}\n\\caption{Illustration of the entropy variance reduction effect, as explained in Section~\\ref{section-tevr}. }\n\\label{figure-tevr}\n\\end{figure}\n\nThird, we then use these per-character lm-entropies to extract compound tokens. For each character and each sentence in the CommonVoice training split we iterate over all sub-strings to find the lowest-lm-entropy snippets of a given length. For each sentence, we only retain the $20\\%$ lowest-lm-entropy snippets. We then select the most-common snippets over all sentences as our TEVR tokens for any given length. \n\nOur model variant M consists of 40 4-character tokens, 80 3-character tokens, 96 2-character tokens, and the 40 single-character tokens used in variant S.\nFor lists of the specific tokens used for each model variant, please see the appendix or our source code release at\\\\\n\\url{https://huggingface.co/fxtentacle/wav2vec2-xls-r-1b-tevr}.\n\nRow (4) of Figure~\\ref{figure-tevr} shows the tokenization of the example sentence using the TEVR tokens from model variant M. \nRow (5) shows the per-token lm-entropies for these compound character tokens, as obtained by summing up the individual per-character lm-entropies of contained characters. \nRow (6) then shows the effective per-character lm-entropies used for gradient estimation when the model variant M is trained with TEVR tokens. With $\\sigma^2=1.4$, the lm-entropy sequence of these tokens is much closer to the uniform distribution assumed by the CTC loss, thereby greatly reducing the gradient skew during training.\n\nTo highlight that the performance improvement of TEVR is caused by the lm-entropy variance reduction and not just by introducing multi-character tokens, we also train and evaluate model variant L which contains almost all multi-character tokens seen in any training sentence, but chosen exhaustively and, hence, without considering the per-character lm-entropy.\n\n\\subsection{German Speech Recognition}\n\nWe use the wav2vec 2.0 implementation available from \\citet{huggingface}\nwith a configuration of 48 hidden layers using 1280 hidden states each\nand feed-forward blocks with an inner dimension of 5120.\nTogether with the LM head, that results in 918~million to 919~million parameters \ndepending on variant.\nWe use gradient checkpointing \\citep{checkpointing} to conserve GPU memory,\nand the Adam optimizer \\citep{adam} with a learning rate warm-up followed by linear decay. Due to memory constraints, we exclude audio recordings that exceed a length of 295,488 samples. Each training batch then consists of 8 complete audio recordings. A 0.005~weigh~decay was used.\n\nWe train 3 variants of the model for up to 4 epochs each with varying warm-up and learning rates, as summarized in Table~\\ref{table-train} and Table~\\ref{table-variants}.\n\\\\\n\n\\begin{table}[h]\n\\begin{center}\n\\begin{tabular}{lll}\nEpoch & Warm-up Steps & Learning Rate \\\\\n\\hline \\\\\n1 & 500 & $1.0 \\cdot 10^{-4}$ \\\\\n2 & 1500 & $0.5 \\cdot 10^{-4}$ \\\\\n3 & 1500 & $0.1 \\cdot 10^{-4}$ \\\\\n4 & 1500 & $0.1 \\cdot 10^{-4}$ \\\\\n\\end{tabular}\n\\end{center}\n\\caption{Training Schedule}\n\\label{table-train}\n\\end{table}\n\n\\begin{table}[h]\n\\begin{center}\n\\begin{tabular}{ l p{0.1\\linewidth} p{0.12\\linewidth} p{0.58\\linewidth} }\nVariant & Number of Tokens & Total \\nobreak{Parameters} & Notes \\\\\n\\hline \\\\\nS & 40 & 918~million & Unmodified wav2vec 2.0 XLS-R 1B architecture with single character tokens, as used by \\citet{xlsr_1B_2021} in their evaluation. \\\\\nM & 256 & 918~million & In addition to all single-character tokens from variant S, 40 TEVR 4-character tokens, 80 TEVR 3-character tokens, and 96 TEVR 2-character tokens were added.  \\\\\nL & 904 & 919~million & This variant contains the majority of all 4-character, 3-character, and 2-character sub-strings, thereby nullifying the variance reduction effect. \n\\end{tabular}\n\\end{center}\n\\caption{Model Variants. See Section~\\ref{section-tevr} for an explanation of how the tokens were obtained.}\n\\label{table-variants}\n\\end{table}\n\n\\begin{table}[p]\n\\begin{center}\n\\begin{tabular}{lccc}\nEpoch & WER Variant S & WER Variant M & WER Variant L \\\\\n\\hline \\\\\n1 & $14.83\\%$ & $\\bm{14.59\\%}$ & $15.10\\%$ \\\\\n2 & $12.69\\%$ & $\\bm{12.25\\%}$ & $12.72\\%$ \\\\\n3 & $11.38\\%$ & $\\bm{10.81\\%}$ & $11.21\\%$ \\\\\n4 & $10.72\\%$ & $\\bm{10.10\\%}$ & $10.53\\%$ \n\\end{tabular}\n\\end{center}\n\\caption{Raw Word Error Rates for each model variant after each epoch. These error rates were calculated without a language model. The best result in each epoch is highlighted in bold.}\n\\label{table-results-raw-wer}\n\\end{table}\n\n\\begin{table}[p]\n\\begin{center}\n\\begin{tabular}{lccc}\nEpoch & CER Variant S & CER Variant M & CER Variant L \\\\\n\\hline \\\\\n1 & $\\bm{3.74\\%}$ & $4.10\\%$ & $4.54\\%$ \\\\\n2 & $\\bm{3.18\\%}$ & $3.42\\%$ & $3.76\\%$ \\\\\n3 & $\\bm{2.82\\%}$ & $2.96\\%$ & $3.27\\%$ \\\\\n4 & $\\bm{2.64\\%}$ & $2.78\\%$ & $3.06\\%$ \n\\end{tabular}\n\\end{center}\n\\caption{Raw Character Error Rates for each model variant after each epoch. These error rates were calculated without a language model. The best result in each epoch is highlighted in bold.}\n\\label{table-results-raw-cer}\n\\end{table}\n\n\\begin{table}[p]\n\\begin{center}\n\\begin{tabular}{cc ccc}\nAlpha & Beta & WER Variant S & WER Variant M & WER Variant L \\\\\n\\hline \\\\\n0.5 & 0.5 & $5.95\\%$ & ${4.93\\%}$ & $5.95\\%$ \\\\\n0.6 & 0.5 & $5.79\\%$ & ${4.93\\%}$ & $5.84\\%$ \\\\\n0.7 & 0.5 & $5.59\\%$ & ${4.73\\%}$ & $5.54\\%$ \\\\\n0.8 & 0.5 & $5.95\\%$ & ${4.88\\%}$ & $5.54\\%$ \\\\\n0.5 & 0.75 & $5.95\\%$ & ${4.98\\%}$ & $5.84\\%$ \\\\\n0.6 & 0.75 & $5.69\\%$ & ${4.93\\%}$ & $5.79\\%$ \\\\\n\\textbf{0.7} & \\textbf{0.75} & $5.59\\%$ & $\\bm{4.73\\%}$ & $5.59\\%$ \\\\\n0.8 & 0.75 & $5.69\\%$ & ${4.83\\%}$ & $5.54\\%$ \\\\\n0.5 & 1.0 & $5.79\\%$ & ${4.93\\%}$ & $5.74\\%$ \\\\\n0.6 & 1.0 & $5.39\\%$ & ${4.93\\%}$ & $5.79\\%$ \\\\\n0.7 & 1.0 & $5.34\\%$ & ${4.73\\%}$ & $5.64\\%$ \\\\\n0.8 & 1.0 & $5.64\\%$ & ${4.83\\%}$ & $5.54\\%$ \n\\end{tabular}\n\\end{center}\n\\caption{Parameter sweep on a validation split for identifying the best $\\alpha$ and $\\beta$ parameters for decoding with a 4-gram language model. For every set of parameters, model variant M performed significantly better than variant S and L. Our final parameter selection of $\\alpha=0.7, \\beta=0.75$  is highlighted in bold.}\n\\label{table-results-4g-wer}\n\\end{table}\n\n\\begin{table}[p]\n\\begin{center}\n\\begin{tabular}{l l l l}\nArchitecture & Language Model & WER & Source \\\\\n\\hline \\\\\nwav2vec 2.0 XLS-R 1B + TEVR & 5-gram & $\\bm{3.64\\%}$ & our best result \\\\\nwav2vec 2.0 XLS-R 1B + TEVR & 4-gram & $3.70\\%$ & our ablation 4-gram LM \\\\\nwav2vec 2.0 XLS-R 1B & 5-gram & $4.38\\%$ & \\citet{flozi} * \\\\\nQuartzNet15x5DE (D37) & 5-gram & $6.6\\%$ & \\citet{scribosermo} \\\\\nwav2vec 2.0 XLS-R 1B + TEVR & no LM & $10.10\\%$ & our ablation no LM \\\\\nwav2vec 2.0 XLS-R & no LM & $12.06\\%$ & \\citet{grosman} * \\\\\n\\end{tabular}\n\\end{center}\n\\caption{Overview of our word error rates and results from literature on German CommonVoice~6.1. The entries marked with * indicate self-reported results on the Community Models tab on paperswithcode.com. We successfully replicated their results based on the source code provided by them.}\n\\label{table-results-final-wer}\n\\end{table}\n\n\\FloatBarrier\n\n\\section{Results}\n\nThe raw word error rates without language model obtained by each of the 3 variants after each of the 4 training epochs are shown in Table~\\ref{table-results-raw-wer}. After 4 epochs, variant M with TEVR tokens reduces the raw word error rate by a relative $5.74\\%$ in comparison to variant S, which is the unmodified wav2vec 2.0 XLS-R 1B architecture. \nAs will be shown later, this performance improvement is amplified when using a word model.\nFor character error rates, see Table~\\ref{table-results-raw-cer}. Unsurprisingly, using single character tokens resulted in the lowest raw character error rate while the model variant with the highest number of distinct tokens also had the highest error rate.\n\nWe performed a parameter sweep using a small 4-gram word model on a validation split of the data, which confirmed our hypothesis that the TEVR-enhanced model variant M performs best, this time with a relative $15.38\\%$ improvement in word error rate over the unmodified wav2vec 2.0 XLS-R 1B architecture. Please also note that model variant L, which contains even more multi-character tokens, performs worse than variant M. This strongly suggest that the performance improvement is indeed caused by the entropy variance reduction and not merely by introducing multi-character tokens. The results are provided in Table~\\ref{table-results-4g-wer}. \n\nWe chose model variant M with 4 epochs of training as the acoustic model and built a full recognition pipeline (including language model) based on it for our final evaluation on  the German testing split of CommonVoice~6.1. We obtained a very competitive word error rate of $3.64\\%$ with a 5-gram language model ($\\alpha = 0.7$, $\\beta = 0.75$). \n\nFor comparison, we consulted the top results from paperswithcode.com (retrieved 02.06.2022), which were \\citet{scribosermo} ($6.6\\%$ WER) in the literature and an unmodified wav2vec 2.0 XLS-R 1B architecture self-reported as a community model by \\citet{flozi}.\nAccordingly, instead of attempting to build our own unmodified wav2vec 2.0 XLS-R 1B recognition pipeline for comparison, we successfully verified the community results of \\citet{flozi} ($4.38\\%$ WER with LM) and \\citet{grosman} ($12.06\\%$ WER without LM). \nSee Table~\\ref{table-results-final-wer} for a comparison.\n\n\\section{Conclusion}\n\nWe have shown that when combined with an appropriately tuned language model, the TEVR-enhanced model outperforms the best German automated speech recognition result from literature by a relative $44.85\\%$ reduction in word error rate. \nIt also outperforms the best self-reported community model by a relative $16.89\\%$ reduction in word error rate.\n\nWe have shown that TEVR tokens outperform generically chosen multi-character tokens of a similar length, which suggests that it is the entropy variance reduction technique that leads to the increased model performance. \nWe observe that the TEVR tokens - which were chosen with the goal of reducing entropy variance -  coincide with linguistically meaningful word endings, such as \"ktz-chen\", \"nied-lich\", \"funk-tion\", and \"glaub-haft\". This hints at redundancy in the German language which TEVR tokens can exploit, but single-character tokens such as those used in the unmodified wav2vec~2.0~XLS-R~1B~architecture cannot. See the appendix for a full list of the tokens used for all 3 model variants.\n\nDue to budget constraints, we did not attempt to train a full TEVR-enhanced speech recognition pipeline for English. For professional clean studio-quality audiobook recordings, such as those in the LibriSpeech English dataset, we would expect TEVR to yield only a low relative improvement, because in those situations there are only few acoustic ambiguities. When operating under difficult conditions, such as the real-world dictation examples in the CommonVoice English dataset, however, we would expect TEVR to also significantly improve English ASR performance. We plan to explore this in a future work.\n\nOur final German speech recognition pipeline consisting of the acoustic model, a matching language model, and necessary source code for inference and evaluation will be made publicly available on:\n\n\\url{https://huggingface.co/fxtentacle/wav2vec2-xls-r-1b-tevr}\n\nWe're hoping to see privacy-preserving offline virtual assistants in the future and we hope that releasing a fully trained working German ASR pipeline will help to make progress towards that goal.\n\n\\newpage\n\\section{Appendix}\n\n\\subsection{Tokens used by Model Variant S}\nThis is an unmodified wav2vec~2.0~XLS-R~1B~architecture with single-character tokens:\n\ne, n, t, h, r, i, s, d, g, l, c, a, u, m, k, f, o, z, b, w, p, v, , , j, , y, q, x, , , , , , , , ?\n\n\\subsection{Tokens used by Model Variant M}\nThese are the TEVR tokens obtained by grouping multiple low-lm-entropy characters into medium-lm-entropy compound tokens:\n\nchen, sche, lich, isch, icht, iche, eine, rden, tion, urde, haft, eich, rung, chte, ssen, chaf, nder, tlic, tung, eite, iert, sich, ngen, erde, scha, nden, unge, lung, mmen, eren, ende, inde, erun, sten, iese, igen, erte, iner, tsch, keit, der, die, ter, und, ein, ist, den, ten, ber, ver, sch, ung, ste, ent, ach, nte, auf, ben, eit, des, ers, aus, das, von, ren, gen, nen, lle, hre, mit, iel, uch, lte, ann, lie, men, dem, and, ind, als, sta, elt, ges, tte, ern, wir, ell, war, ere, rch, abe, len, ige, ied, ger, nnt, wei, ele, och, sse, end, all, ahr, bei, sie, ede, ion, ieg, ege, auc, che, rie, eis, vor, her, ang, fr, ass, uss, tel, er, in, ge, en, st, ie, an, te, be, re, zu, ar, es, ra, al, or, ch, et, ei, un, le, rt, se, is, ha, we, at, me, ne, ur, he, au, ro, ti, li, ri, eh, im, ma, tr, ig, el, um, la, am, de, so, ol, tz, il, on, it, sc, sp, ko, na, pr, ni, si, fe, wi, ns, ke, ut, da, gr, eu, mi, hr, ze, hi, ta, ss, ng, sa, us, ba, ck, em, kt, ka, ve, fr, bi, wa, ah, gt, di, ab, fo, to, rk, as, ag, gi, hn, s, t, n, m, r, l, f, e, a, b, d, h, k, g, o, i, u, w, p, z, , , v, , j, c, y, x, q, , , , , , , , ?\n\n\\subsection{Tokens used by Model Variant L}\nThis is an almost exhaustive list of all multi-character sequences seen in the training data and was chosen without regards to lowering lm-entropy variance:\n\naben, ache, acht, afte, aftl, agen, ahme, ahre, alen, alle, alls, alte, altu, amen, amme, ande, anis, annt, asse, aten, atet, atio, ativ, atte, auch, beit, biet, chaf, chen, cher, ches, chie, chke, chla, chli, chne, chst, chte, chti, chts, chtu, chun, ckel, cklu, delt, dene, dent, dere, dern, ders, dert, deru, dete, dier, dies, dige, ding, dlic, dlun, dnet, doch, dung, eben, eche, echt, eden, eder, edoc, egen, ehen, ehme, ehr, eich, eide, eind, eine, eise, eist, eite, eits, elen, eler, elle, ellt, elte, ende, enen, ensc, ente, entl, erde, erem, eren, erli, erne, ersc, ersi, erst, erte, erun, esch, esem, esen, eser, esse, essl, este, eten, eter, etzt, eute, fall, fent, ffen, ften, ftig, ftli, gend, gene, gisc, gkei, glic, glie, grei, gten, gung, haft, halb, heit, hend, hied, hkei, hlan, hlic, hlie, hlre, hmen, hnet, hnun, hren, hrer, hrte, hrun, hsel, hste, hten, hter, htet, htig, htun, hule, hung, hrt, iche, ichk, ichn, icht, iden, iebe, iede, iegt, iele, iell, ielt, iere, iert, iese, iess, igen, iger, igke, igte, igun, ilie, inde, inem, inen, iner, ines, inge, ings, inie, insa, iona, ione, irat, isch, isse, issi, iste, iten, iter, itet, itig, itik, itio, itis, itte, itt, jahr, kann, keit, klun, krie, ktio, kung, lame, land, ldet, ldun, lich, lied, lien, lies, lige, lisc, liti, llem, llen, llte, llun, logi, lrei, lsch, lten, ltkr, ltun, lung, luss, mati, mein, ment, miss, mmen, mung, nahm, nale, nand, nden, nder, ndes, ndet, ndig, ndun, nete, nfal, ngen, nger, ngli, nich, nier, nige, nisc, nlic, nnen, nnte, nsam, nsch, nten, nter, ntli, nung, olge, olgt, omme, onal, onde, onen, opi, oren, osse, piel, pis, rach, rate, rati, rbei, rche, rdem, rden, rdin, reic, rend, rere, rger, rhei, rich, rieb, rieg, rier, rige, risc, ritt, rlic, ropa, rop, rsch, rsit, rste, rten, rund, rung, scha, sche, schl, send, serd, sere, sich, side, sier, sind, sion, sisc, sit, slic, sond, spie, ssen, sser, ssig, ssio, ssli, sste, sten, ster, sung, tadt, tand, teht, teil, tell, tere, tern, ters, tete, tier, tige, tigt, tion, tisc, tive, tkri, tlic, trie, tsch, ttel, tten, tter, tung, tzen, tzte, tzun, unge, ungs, unkt, urch, urde, utsc, utun, vers, weis, weit, werd, wird, wort, wurd, zeit, ziel, zier, zlic, zten, zung, chs, isc, ndi, ter, cht, nne, rte, ber, hrt, ngl, sse, abe, ach, age, ahl, ahn, ahr, akt, ale, ali, all, als, alt, ame, amt, and, ang, ank, ann, ans, ant, anz, ara, arb, are, ari, ark, art, ass, ast, ate, atz, aub, aue, auf, aus, aut, bar, bau, bed, bef, beg, bei, bek, bel, ben, ber, bes, bet, bew, bez, bil, bis, ble, bli, bra, bst, bur, cha, che, chi, chr, chs, chw, cke, dan, dar, das, dem, den, der, des, deu, die, dor, dre, ebt, eck, ege, egi, ehr, eil, eim, ein, eis, eit, ekt, ele, ell, elt, eme, enn, ens, ent, era, erb, ere, erf, erg, erh, eri, erk, erl, ern, err, ers, ert, erw, erz, est, etr, eue, eut, fel, fen, fer, ffe, for, fra, fre, fr, gan, geb, gef, geh, gel, gem, gen, ger, ges, gew, gib, gra, han, hat, hau, hei, her, hie, hin, hle, hne, hre, ial, ibt, ich, ick, iel, ien, ier, iff, ige, ihn, ihr, ill, imm, ina, ind, ine, ing, ini, inn, ins, inz, ion, ist, jah, ken, ker, kom, kon, kte, lag, lan, lau, leg, lei, len, ler, lie, lig, lis, lle, mal, man, mar, meh, mei, men, mer, mis, mit, mmt, mte, mus, nac, nde, nen, ner, net, neu, nge, nie, nis, nke, nne, noc, nor, nst, ntr, nur, nze, obe, och, ode, ohn, oll, ord, org, ori, orm, ort, par, pen, per, pla, pol, ppe, pre, pro, rag, ran, rat, rau, reg, rei, ren, res, rge, rie, rif, rin, ris, rke, rla, rte, rts, rze, rc, sam, sch, seh, sei, sel, sen, ser, sge, sie, sol, son, spr, sst, sta, ste, sti, str, stu, st, tal, tar, tei, tel, ten, ter, the, tig, tor, tra, tre, tri, tro, tru, tsc, tur, ube, uch, ude, uen, uer, und, ung, uni, uns, upt, urg, usg, uss, ust, ute, ver, vie, vol, vom, von, vor, war, was, wei, wel, wen, wer, wie, wir, woh, zei, zen, zum, zur, zus, zwe, ck, ab, ad, ag, ak, al, am, an, ar, as, at, au, ba, be, bi, bo, br, bu, ch, da, de, di, do, du, eb, ed, eg, eh, ei, el, em, en, er, es, et, eu, fa, fe, fi, fl, fr, ft, f, ga, ge, gi, gl, gr, gt, ha, he, hi, ho, h, ia, id, ie, ig, ih, ik, il, im, in, ir, is, it, iv, je, ka, ke, ki, kl, ko, kr, ku, k, la, ld, le, li, lo, lt, lu, l, ma, me, mi, mo, mp, mu, m, na, nd, ne, nf, ng, ni, no, nt, of, ol, om, on, op, or, os, ot, pa, pf, pl, po, pr, ra, rd, re, ri, ro, rt, ru, r, r, sa, sc, se, si, so, sp, ss, st, ta, te, th, ti, to, tr, ts, tt, tu, tz, uf, ug, ul, um, un, ur, us, ut, ve, vo, wa, we, wi, wo, wu, ze, zi, zu, h, u, n, r, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z, , , , , , , , , , , ?\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2101.00390v2.tex",
        "arXiv-2110.07982v1.tex",
        "arXiv-2206.12693v1.tex"
    ],
    "group_id": "group_108",
    "response": "### Title: Advances in Multilingual Speech Recognition and Translation through Large-Scale Data and Novel Techniques\n\n### Introduction\n\nThe field of speech recognition and translation has seen significant advancements over the past few decades, driven by the development of deep learning models and the availability of large-scale datasets. Initially, the focus was on English, leveraging datasets like LibriSpeech for automatic speech recognition (ASR) and MuST-C for speech-to-text translation (ST). However, the need for multilingual models that can handle a variety of languages has become increasingly important, especially in scenarios where real-time processing and low-resource languages are involved. The European Parliament (EP) has been a rich source of multilingual data, including simultaneous oral interpretations, which provide unique insights into speech-to-speech translation (S2S-T) and interpretation. This paper summarizes three recent research papers that contribute to this field, each addressing different aspects of multilingual speech processing.\n\nThe first paper introduces VoxPopuli (VP), a large-scale multilingual speech corpus that provides extensive unlabeled and transcribed speech data across 23 European Union (EU) languages. The second paper, Scribosermo, focuses on developing small, efficient Speech-to-Text (STT) models for German, Spanish, French, and Italian, which can run in real-time on single-board computers like RaspberryPi and can be trained on consumer-grade hardware. The third paper, TEVR, presents a novel tokenization technique for speech recognition models that aims to reduce the variance in token entropy with respect to the language model, thereby improving recognition accuracy.\n\n### Paper 1: VoxPopuli (VP)\n\n#### Main Content\nVoxPopuli introduces a large-scale multilingual corpus that includes 400K hours of unlabeled speech data and 1.8K hours of transcribed speech data in 16 languages, totaling 17.3K hours of aligned oral interpretations into 15 target languages. The corpus is designed to support unsupervised representation learning, semi-supervised learning, and speech-to-speech translation (S2S-T). The authors provide speech recognition (ASR) baselines and validate the effectiveness of the multilingual unlabeled data in semi-supervised ASR and S2S-T under challenging out-of-domain settings.\n\n#### Data Processing\nThe VoxPopuli dataset is created from European Parliament event recordings, which include plenary sessions, committee meetings, and other events. The recordings are segmented into short clips using voice activity detection (VAD) algorithms, and the transcribed segments are further processed to align with the oral interpretations. This process involves using speech recognition (ASR) systems to force-align the transcribed text with the speech data and filter out segments with high character error rates (CER).\n\n#### Experimental Setup and Results\nThe authors train wav2vec 2.0 models on VoxPopuli data and compare their performance with existing models. They observe significant improvements in word error rates (WER) for low-resource languages, indicating the potential of multilingual pre-training in enhancing ASR performance. For instance, the model pre-trained on 100K hours of VoxPopuli data outperforms the baseline by a relative 16.8% reduction in WER for German, demonstrating the effectiveness of the dataset in improving recognition accuracy.\n\n### Paper 2: Scribosermo\n\n#### Main Content\nScribosermo presents efficient Speech-to-Text (STT) models for German, Spanish, French, and Italian, which are designed to run in real-time on low-power devices. The models leverage a pretrained English model and are trained on relatively small datasets, making them suitable for consumer-grade hardware. The authors also introduce a new library, Corcua, to handle various datasets and facilitate the training process.\n\n#### Data Processing\nThe Scribosermo approach involves reducing the alphabet size for Spanish and French to match the English alphabet, thereby simplifying the transfer-learning process. This is achieved by replacing letters with diacritics or ligatures with their basic forms, and extending the last layer of the network with weights for new characters when necessary. The authors use the CommonVoice dataset for training and testing, and apply a 5-gram language model (LM) to improve the predicted transcriptions.\n\n#### Experimental Setup and Results\nThe models are trained using a QuartzNet architecture, which is known for its efficiency and real-time performance. The authors report competitive word error rates (WER) on the CommonVoice dataset, with Scribosermo achieving a WER of 7.7% for German, 10.9% for Spanish, and 11.0% for French. These results are achieved with a much smaller dataset compared to previous approaches, highlighting the efficiency of the Scribosermo method.\n\n### Paper 3: Token Entropy Variance Reduction (TEVR)\n\n#### Main Content\nTEVR introduces a novel tokenization technique for speech recognition models that aims to minimize the variance in token entropy with respect to the language model. The technique involves calculating the token entropy for each character in a large unstructured text collection and extracting compound tokens that reduce this variance. The authors train German ASR models using this technique and show significant improvements in recognition accuracy.\n\n#### Data Processing\nThe TEVR approach involves training a T5 model on a large unstructured collection of German texts to calculate the per-character token entropy. Compound tokens are then extracted based on these entropies, and the models are trained using these tokens. The authors compare the performance of models with different token sets, including single-character tokens, TEVR tokens, and exhaustive multi-character tokens.\n\n#### Experimental Setup and Results\nThe authors train wav2vec 2.0 models with different token sets and report word error rates (WER) and character error rates (CER) on the CommonVoice dataset. They observe that the TEVR-enhanced model (variant M) outperforms the baseline (variant S) and the exhaustive multi-character token model (variant L), achieving a WER of 3.64% with a 5-gram LM. This represents a relative 16.89% reduction in WER compared to the best reported results from literature.\n\n### Commonalities and Innovations\n\nAll three papers contribute to the field of multilingual speech processing by providing new datasets, models, or techniques. VoxPopuli focuses on creating a large-scale multilingual corpus that supports unsupervised and semi-supervised learning, while Scribosermo aims to develop efficient STT models for low-resource languages. TEVR introduces a novel tokenization technique that reduces the variance in token entropy, thereby improving recognition accuracy.\n\nThe common theme across these papers is the use of large-scale data to improve speech recognition and translation models. VoxPopuli leverages the European Parliament dataset to provide extensive unlabeled and transcribed speech data, while Scribosermo and TEVR use the CommonVoice dataset for training and testing. Both VoxPopuli and Scribosermo explore the benefits of multilingual pre-training and transfer-learning, respectively, to enhance model performance on low-resource languages.\n\n### Comparison of Results\n\nThe VoxPopuli paper reports significant improvements in word error rates (WER) for low-resource languages, such as Finnish and Slovak, when using multilingual pre-training. For instance, the model pre-trained on 100K hours of VoxPopuli data achieves a WER of 7.6% for German and 8.0% for Spanish, outperforming the baseline by a relative 16.7% and 12.4% reduction, respectively. These results demonstrate the effectiveness of multilingual pre-training in improving ASR performance.\n\nScribosermo achieves competitive WERs on the CommonVoice dataset, with results of 7.7% for German, 10.9% for Spanish, and 11.0% for French. These models are trained on relatively small datasets and can run in real-time on low-power devices, making them suitable for embedded or offline speech applications. The authors also introduce Corcua, a library for handling datasets that simplifies the transfer-learning process.\n\nTEVR presents a novel tokenization technique that significantly improves recognition accuracy for German. The authors report a WER of 3.64% for their best model, which outperforms the baseline by a relative 16.89% reduction. This improvement is achieved by reducing the variance in token entropy, thereby optimizing the acoustic model towards more useful features. The authors also observe that the TEVR tokens coincide with linguistically meaningful word endings, suggesting that the technique can exploit redundancy in the language.\n\n### Conclusion\n\nThe main findings of these papers highlight the importance of large-scale multilingual data in improving speech recognition and translation models. VoxPopuli provides the largest open unlabeled speech data to date, facilitating unsupervised and semi-supervised learning. Scribosermo demonstrates the efficiency of small, real-time models that can be trained on consumer-grade hardware, making them suitable for embedded applications. TEVR introduces a novel tokenization technique that reduces the variance in token entropy, thereby improving recognition accuracy.\n\nFuture research directions could include extending the VoxPopuli dataset to cover more languages and domains, developing more efficient transfer-learning techniques for Scribosermo, and exploring the application of TEVR on other languages and datasets. Additionally, combining the techniques from these papers could lead to even more robust and efficient multilingual speech recognition and translation models.\n\n### Appendix: Tables and Figures\n\n**Table 1: VoxPopuli Data Statistics**\n\n| Language | Unlabeled Hours | Transcribed Hours | Speakers | Tokens |\n|----------|-----------------|-------------------|----------|--------|\n| En       | 24.1K           | 543               | 1313     | 4.8M   |\n| De       | 23.2K           | 282               | 531      | 2.3M   |\n| Fr       | 22.8K           | 211               | 534      | 2.1M   |\n| Es       | 21.4K           | 166               | 305      | 1.6M   |\n| Pl       | 21.2K           | 111               | 282      | 802K   |\n| It       | 21.9K           | 91                | 306      | 757K   |\n| Ro       | 17.9K           | 89                | 164      | 739K   |\n| Hu       | 17.7K           | 63                | 143      | 431K   |\n| Cs       | 18.7K           | 62                | 138      | 461K   |\n| Nl       | 19K             | 53                | 221      | 488K   |\n| Fi        | 14.2K           | 27                | 84       | 160K   |\n| Hr        | 8.1K            | 43                | 83       | 337K   |\n| Sk        | 12.1K           | 35                | 96       | 270K   |\n| Sl        | 11.3K           | 10                | 45       | 76K    |\n| Et        | 10.6K           | 3                 | 29       | 18K    |\n| Lt        | 14.4K           | 2                 | 21       | 10K    |\n| Pt        | 17.5K           | -                 | -        | -      |\n| Bg        | 17.6K           | -                 | -        | -      |\n| El        | 17.7K           | -                 | -        | -      |\n| Lv        | 13.1K           | -                 | -        | -      |\n| Mt        | 9.1K            | -                 | -        | -      |\n| Sv        | 16.3K           | -                 | -        | -      |\n| Da        | 13.6K           | -                 | -        | -      |\n| **Total** | **384K**        | **1791**          | **4295** | **467M** |\n\n**Table 2: Scribosermo Training Results**\n\n| Language | Datasets | Duration | WER |\n|----------|----------|----------|-----|\n| DE-CV     | 37        | 2370h    | 6.6% |\n| ES-CV     | 8         | 817h     | 10.0% |\n| FR-CV     | 7         | 1028h    | 11.0% |\n| IT-CV     | 5         | 360h     | 11.5% |\n\n**Table 3: TEVR Training Results**\n\n| Epoch | WER Variant S | WER Variant M | WER Variant L |\n|-------|---------------|---------------|---------------|\n| 1     | 14.83%        | 14.59%        | 15.10%        |\n| 2     | 12.69%        | 12.25%        | 12.72%        |\n| 3     | 11.38%        | 10.81%        | 11.21%        |\n| 4     | 10.72%        | 10.10%        | 10.53%        |\n\n**Table 4: Final WER Results**\n\n| Architecture | Language Model | WER | Source |\n|--------------|----------------|-----|--------|\n| wav2vec 2.0 XLS-R 1B + TEVR | 5-gram | 3.64% | Best result |\n| wav2vec 2.0 XLS-R 1B + TEVR | 4-gram | 3.70% | Ablation 4-gram LM |\n| wav2vec 2.0 XLS-R 1B        | 5-gram | 4.38% | Literature |\n| QuartzNet15x5DE (D37)       | 5-gram | 6.6%  | Literature |\n| wav2vec 2.0 XLS-R 1B + TEVR | no LM  | 10.10% | Ablation no LM |\n| wav2vec 2.0 XLS-R           | no LM  | 12.06% | Literature |\n\n### Ethical Considerations\n\nThe VoxPopuli paper acknowledges the European Union for creating and publishing the materials used in the dataset. The authors also mention that they paid market price for human annotations and that the dataset includes all available speeches from the 2009-2020 EP events without any selection based on topics or speakers. The speech contents represent the standpoints of EU officials, which may include sensitive information.\n\nScribosermo and TEVR do not mention specific ethical considerations, but both papers focus on developing models that can run on consumer-grade hardware and low-power devices, which could have implications for privacy and data security in offline speech applications.\n\n### Future Research Directions\n\nFuture research could focus on extending the VoxPopuli dataset to cover more languages and domains, improving the transfer-learning techniques used in Scribosermo, and exploring the application of TEVR on other languages and datasets. Additionally, combining the techniques from these papers could lead to more robust and efficient multilingual speech recognition and translation models."
}