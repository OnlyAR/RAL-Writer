{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\nThe collection and curation of high-quality training data is crucial for developing text classification models with superior performance, but it is often associated with significant costs and time investment. \nResearchers have recently explored using large language models (LLMs) to generate synthetic datasets as an alternative approach. However, the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks. \nTo better understand factors that moderate the effectiveness of the LLM-generated synthetic data, in this study, we look into how the performance of models trained on these synthetic data may vary with the {\\em subjectivity} of classification. \nOur results indicate that subjectivity, at both the task level and instance level, is negatively associated with the performance of the model trained on synthetic data.  \nWe conclude by discussing the implications of our work on the potential and limitations of leveraging LLM for synthetic data generation\\footnote{The collected human annotations are available at \\\\huggingface.co/datasets/xfleezy/human\\_annotation\\_emnlp23.}. \n\\end{abstract}\n\n\\section{Introduction}\nToday, machine-learning-powered text classification models have been widely applied in diverse applications such as detecting biased or toxic language on online platforms~\\cite{wiegand2019detection} and filtering spam emails~\\cite{jindal2007review}. However, the performance of these models largely depends on the quality of the training data. This poses a substantial challenge in practice, especially when models need to be built for a novel task domain or to incorporate new classification categories, as the training data collection and curation process is often costly, time-consuming, and complex. \n\nMeanwhile, with the recent advancements in large language models (LLMs), researchers have started to explore the potential of utilizing LLMs for generating synthetic data tailored to specific tasks and augmenting the training data in low-resourced data settings~\\cite{kumar2020data,yoo2021gpt3mix,hartvigsen2022toxigen,sahu2022data}.\nMost recently, a few studies also investigate into the feasibility of generating a synthetic dataset from scratch using LLMs to support zero-shot learning~\\cite{ye2022zerogen,wang2021towards,tang2023does,gao2023self}. \nWhile LLM-based data augmentation is often found to outperform other data augmentation methods in boosting the model performance, mixed results are reported regarding whether the LLM-generated synthetic data can effectively support model training to enable a level of model performance that is comparable to models trained on the data collected in the real world and carefully annotated. This leaves uncertainty for researchers and practitioners in deciding whether to rely on LLMs for synthetic data generation or to proceed with the traditional data collection and curation pipeline when they need to construct a text classification model for a new task. Naturally, one may wonder {\\em what factors might moderate the effectiveness of LLM-generated synthetic data in facilitating successful model training}. \n\nWe conjecture that one such factor could be the {\\em subjectivity} of classification tasks. Indeed, language is inherently subjective and interpretive~\\cite{benveniste1971subjectivity,wiebe2004learning}. Previous research has showed that people often perceive the same text in different ways because of their personal biases and perspectives~\\cite{sap2021annotators,li2022towards,gordon2022jury}. Thus, achieving high model performance for classification tasks with high subjectivity seems to impose a greater demand on the training data in reflecting the richness and nuances present in human language, and the extent to which LLM-generated synthetic data can acompolish this objective is unclear.   \n\nThus, in this paper, we formally evaluate the effectiveness of LLM (i.e., the cutting-edge GPT-3.5-Turbo model) in generating synthetic data to support model training for different text classification tasks. We adopt two approaches for synthetic data generation---a {\\em zero-shot} setting in which the LLM is directly prompted to generate text instances with different labels of interests, and a {\\em few-shot} setting in which a few real-world data instances are provided as examples to guide the LLM in generating the synthetic data. We conduct two evaluation studies, each corresponding to one dimension of subjectivity---the first study examines the effectiveness of the synthetic data on 10 types of classification tasks and explores how it varies with the {\\em task-level subjectivity} (i.e., whether this type of classification task is subjective); the second study concerns that given a specific classification task, how the performance of a model trained on synthetic data changes with the {\\em instance-level subjectivity} (i.e., whether people tend to disagree with each other on the label of this task instance).  Our findings suggest that across the 10 types of classification tasks that we have considered in this study, models trained on the LLM-generated synthetic data generally perform worse than those trained on the real-world data, yet guiding LLM's synthetic data generation process with a small amount of real-world data (i.e., as done in the few-shot data generation setting) can improve the effectiveness of the data generated. \nMoreover, we find that the performance of models trained on the LLM-generated synthetic data is very close to those trained on the real-world data for tasks with low subjectivity (e.g., news topic classification, spam email detection), while the performance decrease is much bigger on tasks with high subjectivity (e.g., humor or sarcasm detection). Finally, even within the same type of classification task, models trained on the LLM-generated synthetic data tend to exhibit a higher level of performance on those task instances with lower subjectivity, for which human annotators exhibit a higher level of agreement in their annotation. \n\nTogether, our study provides important experimental evidence regarding the potential and limitations of using LLMs to generate synthetic data for text classification tasks. We conclude by discussing the implications, limitations, and future work of our study. \n\n\\section{Related Work}\n\\noindent \\textbf{Generative AI in synthetic data generation.} Recent advancements in generative AI have motivated numerous studies to explore the potential of leveraging generative models to create synthetic data for training machine learning models, especially for computer vision (CV) and natural language processing (NLP) tasks. In the realm of CV, several works have utilized GAN-based models~\\cite{karras2019style} or diffusion models~\\cite{nichol2021glide} to generate synthetic data for image recognition~\\cite{besnier2020dataset,he2022synthetic} or object segmentation~\\cite{zhang2021datasetgan}. Similarly, in the NLP field, researchers have also probed into the capacity of language models in generating synthetic data for various text classification tasks~\\cite{kumar2020data,chung2023increasing, sahu2022data, yoo2021gpt3mix, ye2022zerogen,wang2021towards,hartvigsen2022toxigen,meng2022generating,gao2022self,aggarwal2022entity, chen2022weakly}, with mixed results reported regarding the effectiveness of the synthetic data generated.  \nIn this study, we aim to obtain a better understanding of {\\em when} the synthetic data generated by language models can lead to effective model training, and we focus on exploring the role of task subjectivity in moderating the effectiveness of the synthetic data.  \n\n\\noindent \\textbf{Large language models.}\nBased on the Transformer architecture~\\cite{vaswani2017attention}, large language models (LLMs) have facilitated remarkable progress in the field of natural language processing. The utilization of bidirectional contexts in the BERT model~\\cite{devlin2018bert} has resulted in superior performance across a wide range of tasks. Building on this, OpenAI's GPT series, comprising of models like GPT-2~\\cite{radford2019language}, the colossal GPT-3~\\cite{brown2020language} with an impressive 175 billion parameters and the most recent GPT-4~\\cite{openai2023gpt4}, pushed the boundaries of possibilities of LLMs. These models exhibit remarkable proficiency in generating high-quality human-like text~\\cite{clark2021all,dou2021gpt,10.1145/3544548.3581318}, showcasing capabilities in rudimentary reasoning~\\cite{wei2021finetuned}, translation~\\cite{brown2020language}, scientific synthetic data generation~\\cite{10.1145/3544548.3580688}, and code generation~\\cite{10.1145/3544548.3580940}.  \nIn this study, we focus on leveraging the cutting-edge GPT-3.5-Turbo model\\footnote{We used GPT-3.5-Turbo as the foundational model to generate synthetic data because at the time of this study, an official API for the more advanced GPT-4 model was not yet available from OpenAI.} to explore its capabilities and limitations in synthesizing data for text classification tasks with different subjectivity levels.\n\n\\section{Methodolgy}\nIn this section, we outline the procedure we have followed when leveraging the large language model  to generate the synthetic training data for text classification. We consider two data generation settings in this study, i.e., the \\textit{zero-shot} setting and the \\textit{few-shot} setting.\n\n\\subsection{Zero-shot Synthetic Data Generation}\n\\label{zero-shot}\nUnder the {\\em zero-shot} synthetic data generation setting, given a text classification task, we assume that the real-world data in the form of ``text-label pairs'' do not exist. \nThus, in order to obtain synthetic training data for the text classification task, two sequential prompts are constructed and supplied to the pretrained large language model (i.e., the GPT-3.5-Turbo model). First, a customized ``context prompt'' relevant to the targeted domain of interest is used to set the context. For example, in the case of the IMDB movie review classification task~\\cite{maas-EtAl:2011:ACL-HLT2011}, the customized context prompt used is ``Imagine you are a movie reviewer on the IMDB platform''. This prompt aims to encourage the LLM to generate synthetic data that resemble the real texts produced in the targeted domain. \nAfter the context is set, a second prompt, i.e., the ``data generation prompt'', is provided to the LLM, instructing the model to generate texts with a specific style, label (with respect to the classification task of interest), and word limit. \nFor example, for the IMDB movie review classification task, the style of the text is a movie review, and the label is a targeted sentiment conveyed by the review (i.e., ``positive'' or ``negative''). \nTo further enhance the diversity of the generated data, after the generation of every $n$ data points (i.e., texts of targeted styles, labels, and word limits)\\footnote{To increase data diversity while maintaining a reasonable data generation speed, $n$ is set to 10 for generating short texts (i.e., texts with a maximum length of 30 words), and 1 for generating longer paragraphs. \n},  %\\hx{For tasks with data content as short sentences, we will request diversity every tenth time. For data with longer paragraphs, we ask for diversity every time. (Since the GPT API needs to manually return previous conversation records to the model as prompts, ten short sentences/one long paragraph is more appropriate as an interval. Too long a chat history can significantly slow down the generation speed, lead to extremely high Rate Limit Error probability, and result in higher costs)} iterations \\my{What does ``several'' mean? Do you have a specific frequency?}, \nwe provide a ``diversity prompt'' to the LLM---``Can you provide something more diverse compared to the previously generated data?''---aiming to increase the diversity of the synthetic data generated.\n\n\\subsection{Few-shot Synthetic Data Generation}\n\\label{few-shot}\nUnder the {\\em few-shot} synthetic data generation setting, we assume that a small amount of %real data with golden labels \nreal-world data \nare available for the text classification task. These data points can then serve as the examples for the large language model in the data generation process, which can potentially provide LLM with insights of the patterns exhibited in the real-world data.  We again start the data generation process by using a context prompt to set the context. However, different from that in the zero-shot setting, here, each time before we instruct the LLM to generate a piece of text, we first provide the model with a few randomly sampled real-world data instances (including both the text and the label) as the examples. \nTo keep the LLM from merely rephrasing the provided examples, an additional prompt is used to impose a constraint on the LLM in generating the synthetic data (i.e., ``You should imitate the example I have provided, but you cannot simply modify or rewrite the example I have given.''). \n\nFor more details about prompts used for generating data for each type of text classification task, please refer to the App.~\\ref{prompts}.\n\\section{Evaluation \\RNum{1}: Comparison Across Different Types of Tasks}\nIn our first evaluation study, we investigate into how well the synthetic data generated by LLM under both zero-shot and few-shot settings can support effective model training for different types of text classification tasks. \nWe are especially interested in comparing the model performance between those trained on the real-world data and on the LLM-generated synthetic data, and in understanding how the performance of those models trained on the LLM-generated synthetic data varies with the subjectivity of the text classification task.\n\\subsection{Datasets and Tasks}\nWe experiment  with 10 representative datasets covering a variety of text classification tasks: AG's news \\citep{Zhang2015CharacterlevelCN}, IMDB reviews \\citep{maas-EtAl:2011:ACL-HLT2011}, \nSMS  spam~\\citep{Almeida2011SpamFiltering}, Financial phrase bank~\\citep{Malo2014GoodDO},\nReddit emotion~\\citep{demszky2020goemotions}, %\\my{Missing citation} \\hx{Added},\nRelation classification~\\citep{gao-etal-2019-fewrel}, Tweet irony speech~\\citep{van2018semeval}, \nTweet emotions~\\citep{mohammad2018semeval}, \nSarcasm news (\\citealp{misra2023Sarcasm}, \\citealp{misra2021sculpting}), and \nHumor speech~\\citep{annamoradnejad2020colbert}. \nSee App.~\\ref{dataset} for detailed descriptions of datasets and the corresponding text classification tasks.\nThese datasets are selected with the goal of spanning a wide range of task subjectivity in mind. For example, we conjecture that classifying the news topic category (e.g., as that in the AG's news dataset) is relatively objective, while determining whether texts are humorous (e.g., as that in the Humor speech dataset) is quite subjective~\\cite{veatch1998theory}. \n\n\\subsection{Task-level Subjectivity Determination}\n\\label{subrank}\n\nTo formally determine the subjectivity levels of different text classification tasks, we first conduct a crowdsourced study to collect subjectivity judgements from the crowd.\n\n\\noindent \\textbf{Study procedure.}\nWe adopt a comparative approach to collect crowdsourced subjectivity judgements in this study. \nSpecifically, \nwe recruited crowd workers from Amazon Mechanical Turk (MTurk), and each worker was asked to complete a sequence of 10 subjectivity judgement tasks. \nIn each task, we randomly sampled a pair of text classification tasks from the 10 tasks that we considered in this evaluation, and we presented to the worker the task description, label description, and task examples for each task in the pair. \nThen, the worker was asked to determine which text classification task in the pair was more objective, with ``objectivity'' of a task defined as ``the classification of a piece of text is based on clear, identifiable features in the text (e.g., keywords or phrases), and can be done without being affected by any personal interpretation of the text resulted from personal biases, emotions or beliefs.''  The study was restricted to U.S. workers. Each worker was allowed to participate only once and received a \\$1.2 payment. An attention check question was included in the study to validate the worker's engagement, and only the data from workers who successfully passed the attention check were considered valid.\n\n\\noindent \\textbf{Ranking task subjectivity.}\nAfter excluding responses from inattentive workers, a total of 540 pairwise subjectivity comparisons for the 10 tasks were obtained from 54 workers. For each pair of tasks, we aggregated relative subjectivity judgments made on this pair to determine which task was perceived as more subjective (i.e., less objective). To produce a ranking of the  subjectivity of the 10 tasks, we constructed a directed graph based on the pairwise subjectivity comparisons---each task was a node in this graph, and directed edges were added between each pair of tasks, pointing from the one that was deemed as more subjective (on the aggregate level) to the one deemed as less subjective. \nThe topological sort algorithm~\\cite{cormen2022introduction} was then applied to this directed graph to obtain a linear ordering of the nodes. If a cycle was detected within the graph, the corresponding tasks were considered to have the same level of subjectivity\nand were merged into a single meta-node before re-runing the algorithm. \nOur final task subjectivity ranking results are shown in  Table~\\ref{tab:results}.\n\n\\subsection{Model Training}\nGiven a text classification task, following the procedures outlined in Sections~\\ref{zero-shot} and~\\ref{few-shot}, 3,000 synthetic data points were generated for each candidate label under both zero-shot and few-shot settings. \nWe then trained classification models using the real-world training data provided by the original dataset, the synthetic data generated under the zero-shot settings, and the synthetic data generated under the few-shot settings\\footnote{Under the few-shot setting, we randomly sampled $10\\%$ of the data points from the real-world training data provided in the original dataset as the example pool to guide the LLM's synthetic data generation process, but only the sythetic data generated were used to train the models.}, respectively. \nSpecifically, we utilized the pre-trained BERT~\\cite{devlin2018bert} and RoBERTa~\\cite{liu2019roberta} models from Huggingface's transformers library~\\cite{wolf-etal-2020-transformers} as the encoders, and used the representation embeddings from the last layer of these models as the input to our classification models. The classification model itself comprised a hidden layer of 768 units and an output layer, and it was fine-tuned with a learning rate of $5e-5$ and a batch size of 64. For datasets that provided official partitions for training and test sets, we directly evaluated the classification model's performance on the test sets. Otherwise, we randomly divided the dataset into training (70\\%), validation (5\\%), and test (25\\%) sets\\footnote{To ensure a fair comparison, we maintained an equal size for both the real-world and synthetic training data by downsampling the dataset with a larger size. \n\n}.  \nModels' performance was evaluated via Macro-F1 and Accuracy scores, and they were computed by comparing the model's predictions with the gold labels provided in the test sets. \nTo ensure the robustness of our results, all experiments were repeated three times, and the average performance across these repetitions was reported. \n\n\\begin{table*}[t]\n  \\centering\n  \\resizebox{\\textwidth}{!}{\n  \\begin{tblr}{\n    row{2} = {c},\n    row{3} = {c},\n    column{2} = {l},\n    cell{1}{1} = {r=3}{},\n    cell{1}{2} = {r=3}{},\n    cell{1}{3} = {c=6}{c},\n    cell{1}{9} = {c=6}{c},\n    cell{2}{3} = {c=2}{},\n    cell{2}{5} = {c=2}{},\n    cell{2}{7} = {c=2}{},\n    cell{2}{9} = {c=2}{},\n    cell{2}{11} = {c=2}{},\n    cell{2}{13} = {c=2}{},\n    cell{4}{3} = {c},\n    cell{4}{4} = {c},\n    cell{4}{5} = {c},\n    cell{4}{6} = {c},\n    cell{4}{7} = {c},\n    cell{4}{8} = {c},\n    cell{4}{9} = {c},\n    cell{4}{10} = {c},\n    cell{4}{11} = {c},\n    cell{4}{12} = {c},\n    cell{4}{13} = {c},\n    cell{4}{14} = {c},\n    cell{5}{3} = {c},\n    cell{5}{4} = {c},\n    cell{5}{5} = {c},\n    cell{5}{6} = {c},\n    cell{5}{7} = {c},\n    cell{5}{8} = {c},\n    cell{5}{9} = {c},\n    cell{5}{10} = {c},\n    cell{5}{11} = {c},\n    cell{5}{12} = {c},\n    cell{5}{13} = {c},\n    cell{5}{14} = {c},\n    cell{6}{3} = {c},\n    cell{6}{4} = {c},\n    cell{6}{5} = {c},\n    cell{6}{6} = {c},\n    cell{6}{7} = {c},\n    cell{6}{8} = {c},\n    cell{6}{9} = {c},\n    cell{6}{10} = {c},\n    cell{6}{11} = {c},\n    cell{6}{12} = {c},\n    cell{6}{13} = {c},\n    cell{6}{14} = {c},\n    cell{7}{3} = {c},\n    cell{7}{4} = {c},\n    cell{7}{5} = {c},\n    cell{7}{6} = {c},\n    cell{7}{7} = {c},\n    cell{7}{8} = {c},\n    cell{7}{9} = {c},\n    cell{7}{10} = {c},\n    cell{7}{11} = {c},\n    cell{7}{12} = {c},\n    cell{7}{13} = {c},\n    cell{7}{14} = {c},\n    cell{8}{3} = {c},\n    cell{8}{4} = {c},\n    cell{8}{5} = {c},\n    cell{8}{6} = {c},\n    cell{8}{7} = {c},\n    cell{8}{8} = {c},\n    cell{8}{9} = {c},\n    cell{8}{10} = {c},\n    cell{8}{11} = {c},\n    cell{8}{12} = {c},\n    cell{8}{13} = {c},\n    cell{8}{14} = {c},\n    cell{9}{3} = {c},\n    cell{9}{4} = {c},\n    cell{9}{5} = {c},\n    cell{9}{6} = {c},\n    cell{9}{7} = {c},\n    cell{9}{8} = {c},\n    cell{9}{9} = {c},\n    cell{9}{10} = {c},\n    cell{9}{11} = {c},\n    cell{9}{12} = {c},\n    cell{9}{13} = {c},\n    cell{9}{14} = {c},\n    cell{10}{3} = {c},\n    cell{10}{4} = {c},\n    cell{10}{5} = {c},\n    cell{10}{6} = {c},\n    cell{10}{7} = {c},\n    cell{10}{8} = {c},\n    cell{10}{9} = {c},\n    cell{10}{10} = {c},\n    cell{10}{11} = {c},\n    cell{10}{12} = {c},\n    cell{10}{13} = {c},\n    cell{10}{14} = {c},\n    cell{11}{3} = {c},\n    cell{11}{4} = {c},\n    cell{11}{5} = {c},\n    cell{11}{6} = {c},\n    cell{11}{7} = {c},\n    cell{11}{8} = {c},\n    cell{11}{9} = {c},\n    cell{11}{10} = {c},\n    cell{11}{11} = {c},\n    cell{11}{12} = {c},\n    cell{11}{13} = {c},\n    cell{11}{14} = {c},\n    cell{12}{3} = {c},\n    cell{12}{4} = {c},\n    cell{12}{5} = {c},\n    cell{12}{6} = {c},\n    cell{12}{7} = {c},\n    cell{12}{8} = {c},\n    cell{12}{9} = {c},\n    cell{12}{10} = {c},\n    cell{12}{11} = {c},\n    cell{12}{12} = {c},\n    cell{12}{13} = {c},\n    cell{12}{14} = {c},\n    cell{13}{3} = {c},\n    cell{13}{4} = {c},\n    cell{13}{5} = {c},\n    cell{13}{6} = {c},\n    cell{13}{7} = {c},\n    cell{13}{8} = {c},\n    cell{13}{9} = {c},\n    cell{13}{10} = {c},\n    cell{13}{11} = {c},\n    cell{13}{12} = {c},\n    cell{13}{13} = {c},\n    cell{13}{14} = {c},\n    vline{2-4,9} = {1-3}{},\n    vline{4,6,10,12} = {2}{dashed},\n    vline{8} = {2}{},\n    vline{4,6,8,10,12,14} = {3-13}{dotted},\n    vline{5,7,11,13} = {2-13}{dashed},\n    vline{9} = {3}{},\n    vline{2-3,9} = {4-13}{},\n    hline{1,4,14} = {-}{},\n    hline{2-3} = {3-14}{},\n  }\n  \\textbf{Dataset}                  & \\textbf{Subjectivity} & \\textbf{BERT}           &                         &                                       &                                       &                                       &                                       & \\textbf{RoBERTa}        &                         &                                       &                                       &                                       &                                       \\\\\n                                    &                             & \\textbf{Real-world data}       &                         & \\textbf{Zero-shot setting}                    &                                       & \\textbf{Few-shot setting}       &                                       & \\textbf{Real-world data}       &                         & \\textbf{Zero-shot setting}                    &                                       & \\textbf{Few-shot setting}       &                                       \\\\\n                                    &                             & \\textbf{Macro-F1 } & \\textbf{Accuracy Score} & \\textbf{Macro-F1 }               & \\textbf{Accuracy Score}               & \\textbf{Macro-F1 }               & \\textbf{Accuracy Score}               & \\textbf{Macro-F1} & \\textbf{Accuracy Score} & \\textbf{Macro-F1}               & \\textbf{Accuracy Score}               & \\textbf{Macro-F1}               & \\textbf{Accuracy Score}               \\\\\n  AG                         & \\OB                         & 95.3\\%                  & 95.3\\%                  & 89.3\\% \\textcolor{pf1color}{(-6.0\\%)}  & 89.3\\% \\textcolor{pf1color}{(-6.0\\%)}  & 91.5\\% \\textcolor{pf1color}{(-3.8\\%)}  & 91.6\\% \\textcolor{pf1color}{(-3.7\\%)}  & 94.6\\%                  & 94.6\\%                  & 88.6\\% \\textcolor{pf1color}{(-6.0\\%)}  & 88.6\\% \\textcolor{pf1color}{(-6.0\\%)}  & 92.9\\% \\textcolor{pf1color}{(-1.7\\%)}  & 92.9\\% \\textcolor{pf1color}{(-1.7\\%)}  \\\\\n  Relation                    & \\OB\\OB                      & 98.6\\%                  & 98.6\\%                  & 92.4\\% \\textcolor{pf1color}{(-6.2\\%)}  & 92.7\\% \\textcolor{pf1color}{(-5.9\\%)}  & 96.4\\% \\textcolor{pf1color}{(-2.2\\%)}  & 96.4\\% \\textcolor{pf1color}{(-2.2\\%)}  & 97.0\\%                  & 96.9\\%                  & 91.4\\% \\textcolor{pf1color}{(-5.6\\%)}  & 91.6\\% \\textcolor{pf1color}{(-5.3\\%)}  & 94.1\\% \\textcolor{pf1color}{(-2.9\\%)}  & 94.1\\% \\textcolor{pf1color}{(-2.8\\%)}  \\\\\n  IMDB                   & \\OB\\OB\\OB                   & 87.6\\%                  & 87.6\\%                  & 81.2\\% \\textcolor{pf1color}{(-6.4\\%)}  & 81.5\\% \\textcolor{pf1color}{(-6.1\\%)}  & 81.1\\% \\textcolor{pf1color}{(-6.5\\%)}  & 81.2\\% \\textcolor{pf1color}{(-6.4\\%)}  & 89.0\\%                  & 89.0\\%                  & 81.2\\% \\textcolor{pf1color}{(-7.8\\%)}  & 81.3\\% \\textcolor{pf1color}{(-7.7\\%)}  & 82.4\\% \\textcolor{pf1color}{(-1.6\\%)}  & 82.4\\% \\textcolor{pf1color}{(-1.6\\%)}  \\\\\n  SMS spam                          & \\OB\\OB\\OB\\OB                & 97.2\\%                  & 98.8\\%                  & 93.8\\% \\textcolor{pf1color}{(-3.4\\%)}  & 95.1\\% \\textcolor{pf1color}{(-3.7\\%)}  & 94.3\\% \\textcolor{pf1color}{(-2.9\\%)} & 94.8\\% \\textcolor{pf1color}{(-4.0\\%)} & 97.3\\%                  & 98.8\\%                  & 93.5\\% \\textcolor{pf1color}{(-3.8\\%)}  & 95.9\\% \\textcolor{pf1color}{(-2.9\\%)}  & 94.0\\% \\textcolor{pf1color}{(-3.3\\%)}  & 95.7\\% \\textcolor{pf1color}{(-3.1\\%)}  \\\\\n\n  Reddit emotion                      & \\OB\\OB\\OB\\OB\\OB             & 93.7\\%                  & 94.6\\%                  & 72.7\\% \\textcolor{pf1color}{(-21.0\\%)} & 74.4\\% \\textcolor{pf1color}{(-20.2\\%)} & 81.9\\% \\textcolor{pf1color}{(-11.8\\%)} & 82.0\\% \\textcolor{pf1color}{(-12.6\\%)} & 91.3\\%                  & 92.1\\%                  & 77.9\\% \\textcolor{pf1color}{(-13.4\\%)} & 78.1\\% \\textcolor{pf1color}{(-14.0\\%)} & 87.5\\% \\textcolor{pf1color}{(-3.8\\%)}  & 87.7\\% \\textcolor{pf1color}{(-4.4\\%)}  \\\\\n  Tweet irony   & \\OB\\OB\\OB\\OB\\OB             & 72.2\\%                  & 73.9\\%                  & 63.4\\% \\textcolor{pf1color}{(-8.8\\%)}  & 63.6\\% \\textcolor{pf1color}{(-10.3\\%)} & 81.5\\% \\textcolor{f1color}{(+9.3\\%)} & 81.9\\% \\textcolor{f1color}{(+8.0\\%)} & 74.0\\%                  & 75.5\\%                  & 57.8\\% \\textcolor{pf1color}{(-16.2\\%)} & 59.1\\% \\textcolor{pf1color}{(-16.4\\%)} & 83.3\\% \\textcolor{f1color}{(+9.3\\%)} & 83.7\\% \\textcolor{f1color}{(+8.2\\%)} \\\\\n  Tweet emotions & \\OB\\OB\\OB\\OB\\OB             & 77.7\\%                  & 81.1\\%                  & 58.1\\% \\textcolor{pf1color}{(-19.6\\%)} & 64.5\\% \\textcolor{pf1color}{(-16.6\\%)} & 64.6\\% \\textcolor{pf1color}{(-13.1\\%)} & 69.1\\% \\textcolor{pf1color}{(-12.0\\%)} & 75.8\\%                  & 78.9\\%                  & 64.6\\% \\textcolor{pf1color}{(-11.2\\%)} & 71.5\\% \\textcolor{pf1color}{(-7.4\\%)}  & 66.3\\% \\textcolor{pf1color}{(-9.5\\%)}  & 72.7\\% \\textcolor{pf1color}{(-6.2\\%)}  \\\\\n  Sarcasm              & \\OB\\OB\\OB\\OB\\OB             & 89.9\\%                  & 90.3\\%                  & 51.1\\% \\textcolor{pf1color}{(-38.8\\%)} & 51.2\\% \\textcolor{pf1color}{(-39.1\\%)} & 63.6\\% \\textcolor{pf1color}{(-26.3\\%)} & 64.8\\% \\textcolor{pf1color}{(-25.5\\%)} & 91.8\\%                  & 92.0\\%                  & 54.3\\% \\textcolor{pf1color}{(-37.5\\%)} & 54.3\\% \\textcolor{pf1color}{(-37.7\\%)} & 61.5\\% \\textcolor{pf1color}{(-30.3\\%)} & 63.6\\% \\textcolor{pf1color}{(-28.4\\%)} \\\\\n   Financial              & \\OB\\OB\\OB\\OB\\OB             & 83.2\\%                  & 84.6\\%                  & 48.2\\% \\textcolor{pf1color}{(-35.0\\%)} & 60.7\\% \\textcolor{pf1color}{(-23.9\\%)} & 70.6\\% \\textcolor{pf1color}{(-12.6\\%)} & 74.2\\% \\textcolor{pf1color}{(-10.4\\%)} & 85.0\\%                  & 86.6\\%                  & 58.5\\% \\textcolor{pf1color}{(-26.5\\%)} & 70.3\\% \\textcolor{pf1color}{(-16.3\\%)} & 75.0\\% \\textcolor{pf1color}{(-10.0\\%)} & 78.9\\% \\textcolor{pf1color}{(-7.7\\%)}  \\\\\n  Humor speech          & \\OB\\OB\\OB\\OB\\OB             & 97.0\\%                  & 97.0\\%                  & 56.0\\% \\textcolor{pf1color}{(-41.0\\%)} & 61.7\\% \\textcolor{pf1color}{(-35.3\\%)} & 86.9\\% \\textcolor{pf1color}{(-10.1\\%)} & 87.0\\% \\textcolor{pf1color}{(-10.0\\%)} & 96.7\\%                  & 96.7\\%                  & 54.9\\% \\textcolor{pf1color}{(-41.8\\%)} & 60.9\\% \\textcolor{pf1color}{(-35.8\\%)} & 84.0\\% \\textcolor{pf1color}{(-12.7\\%)} & 84.0\\% \\textcolor{pf1color}{(-12.7\\%)} \n  \\end{tblr}\n  }\n  \\caption{Comparing the performance of classification models trained on the LLM-generated synthetic data under the zero-shot or few-shot settings, with those trained with the original real-world data, in terms of Macro-F1 (\\%) and Accuracy Score (\\%). In the ``Subjectivity'' column, more \"$\\star$\" symbols indicate a higher level of task subjectivity.  }\n  \\label{tab:results}\n\\end{table*}\n\n\\subsection{Evaluation Results}\nTable~\\ref{tab:results} summarizes the comparative performance  of classification models trained with different data. Below, we highlight a few key observations we get from this comparison. \n\n\\noindent \\textbf{Models trained on the real-world data consistently outperform those trained on the synthetic data.} Our results indicate that models trained on the original real-world data consistently outperform their counterparts trained on the synthetic data generated under either zero-shot or few-shot settings, almost for every task. In particular, with the RoBERTa model, we observe that the average improvements of the model trained on the real-world data \nover \nthe models trained on zero-shot synthetic data and few-shot synthetic data\nare $16.9\\%$ and $6.7\\%$ in terms of Macro-F1, and $14.9\\%$ and  $6.1\\%$ in terms of accuracy. \nSimilar trends are observed with the BERT model as well.  \n\n\\noindent \\textbf{Guiding LLM with real-world data examples can boost the effectiveness of the synthetic data.}\nWe also observe that models trained on those synthetic data generated under the few-shot settings almost always outperform those trained on the synthetic data generated under the zero-shot settings. For instance, for the BERT model, we see an average increase of $10.6\\%$ and $8.8\\%$ in Macro-F1 and accuracy scores, respectively, across the 10 tasks in the few-shot setting, as compared to the zero-shot setting. Similarly, with the RoBERTa model, there is an average increase of $10.3\\%$ in Macro-F1 and $8.9\\%$ in accuracy scores across the 10 tasks when the real-world data are used as examples for LLM to mimic in the synthetic data generation process. For more analysis of the few-shot synthetic data, please see App.~\\ref{e1a1} and \\ref{e1a3}.\n\n\\noindent \\textbf{Synthetic data support more effective model training for tasks that are less subjective.} Finally, we notice that for classification tasks with relatively low levels of subjectivity (e.g., those in the AG's news, Relation classification, IMDB reviews, and SMS spam datasets), the performance difference between models trained on the synthetic data and those trained on the real-world data is remarkably small. \nHowever, for tasks with high subjectivity, the performance decrease resulted from the usage of the synthetic data is more significant---for instance, across the cluster of 6 tasks with the highest level of subjectivity in our evaluation, there is an average decrease of 27.4\\% and 24.2\\% in Macro-F1 and accuracy, respectively, comparing the BERT models trained on the zero-shot synthetic data with those trained on the real-world data. In other words, for text classification tasks that are highly objective, there is great potential in training high-performing models simply based on synthetic data generated by LLMs, but the same method falls short in generating synthetic data that can effectively support model training for highly subjective classifications. \n\n\\subsection{Exploratory Analysis: Data Diversity}\n\\label{sec:exploratory}\n\\begin{figure}[t]\n  \\centering\n  \\subfloat[Remote Clique]{\\includegraphics[width=0.24\\textwidth]{figures/remote_clique.png}\\label{fig:remote_clique}}\n  \\hfill\n  \\subfloat[Chamfer Distance]{\\includegraphics[width=0.24\\textwidth]{figures/chamer_distance.png}\\label{fig:chamfer}}\n  \\caption{Comparing the diversity of the real-world data and the synthetic data.}\n  \\label{fig: diversity}\n\\end{figure}\n\nTo explore the potential reasons underlying the model performance difference, we conducted an exploratory analysis on the diversity of the training data. Following \\citeauthor{10.1145/3411764.3445782}~\\shortcite{10.1145/3411764.3445782}, we used the {\\em Remote Clique Score} (i.e., the average mean distance of a data instance to other instances) and the {\\em Chamfer Distance Score} (i.e., the average minimum distance of a data instance to other instances) to quantify the diversity of a set of data. \nFor both metrics, higher values indicate greater data diversity. \nAs shown in Figure~\\ref{fig: diversity}, we find that in general, the real-world data appear to be more diverse than the synthetic data generated under the few-shot settings, which in turn seem to be more diverse than the zero-shot synthetic data. This might partially explain why models trained on the real-world data and the few-shot synthetic data tend to outperform those trained on the zero-shot synthetic data. \n\nIn addition, we also notice that compared to that on the low subjectivity tasks (i.e., AG, Relation, IMDB, Spam), the differences in data diversity between the real-world data and the synthetic data seem to be more salient on the high subjectivity tasks (i.e., the other 6 tasks), especially in terms of the Chamfer Distance Score. In fact, a t-test shows that the decrease of the Chamfer Distance Score in the zero-shot synthetic data compared to the real data is significantly larger for the high subjectivity tasks than for the low subjectivity tasks ($p<0.01$). \nThis suggests that for tasks with high subjectivity, such as interpreting humor or sarcasm in language, LLMs may not be able to \ngenerate data instances that can cover the full spectrum of real-life scenarios, which may limit the performance of models trained on the synthetic data. \n\n\\section{Evaluation \\RNum{2}: Comparison Across Different Task Instances}\n\n\\begin{table*}\n\\centering\n\\resizebox{\\linewidth}{!}{\n\\begin{tblr}{\n  cells = {c},\n  hline{1-2,5} = {-}{},\n}\n\\textbf{Dataset} & \\textbf{AG} & \\textbf{Relation} & \\textbf{IMDB}  & \\textbf{SMS Spam}  & \\textbf{Reddit Emotion}  &\\textbf{Humor Speech} &\\textbf{Tweet Irony} & \\textbf{Sarcasm }   & \\textbf{Tweet Emotions}  & \\textbf{Finanical} \\\\\nAverage Agreement $\\overline{a}$     & 0.80 (4.2)  & 0.78 (4.5) & 0.76 (7.3)  & 0.73 (8.5) & 0.69 (6.6)  & 0.68 (7.1) & 0.68 (6.7)  & 0.64 (7.7) &  0.64 (4.6) & 0.57 (7.6) \\\\\nKrippendorff's $\\alpha$   & 0.51  & 0.43 & 0.19  & 0.27 & 0.30  & 0.06 & 0.03  & 0.01 &  0.17 & -0.03 \\\\\nSubjectivity Level &\\OB  &\\OB\\OB  & \\OB\\OB\\OB &  \\OB\\OB\\OB\\OB & \\OB\\OB\\OB\\OB\\OB  & \\OB\\OB\\OB\\OB\\OB  &\\OB\\OB\\OB\\OB\\OB  &\\OB\\OB\\OB\\OB\\OB  & \\OB\\OB\\OB\\OB\\OB &\\OB\\OB\\OB\\OB\\OB  \n\\end{tblr}\n}\n\\caption{The average instance-level annotation agreement for different types of tasks, \nalongside the corresponding task-level subjectivity. Numbers in parentheses in the first row\nrepresent the average number of annotations received per task instance. Higher values for both the average agreement $\\overline{a}$ and Krippendorff's $\\alpha$ indicate a higher degree inter-annotator agreement.\n}\n\\label{tab:hit2}\n\\end{table*}\n\nIn the previous section, we have discovered that the subjectivity of a task can adversely affect the performance of classification models trained on the LLM-generated synthetic data. However, even for the same type of task, the classification for each individual task instance may exhibits different levels of subjectivity as well. Naturally, one may wonder whether models trained on the LLM-generated synthetic data may show different performance on task instances of different subjectivity. We aim to explore the answers to this question in this section.\n\n\\subsection{Instance-level Subjectivity Determination}\nGiven a text classification task and a specific text instance, we consider the degree of {\\em agreement among annotators} on the label of this text as a proxy for the subjectivity of this instance---a lower level of agreement means that annotators hold more divergent views, hence the task may have a higher level of subjectivity. Thus, to formally quantify the subjectivity of different instances for different tasks, we again conduct a crowdsourced study to collect instance-level annotations.\n\n\\noindent \\textbf{Study procedure.} We again considered the 10 types of text classification tasks as that in the first evaluation study. For each type of task, we randomly sampled 50 text instances per category from the test set to compose our ``evaluation dataset'' for that task.\nWe then recruited U.S. workers from MTurk to complete annotation tasks for those instances in our evaluation dataset. Specifically,  each worker was randomly assigned to one type of text classification tasks. After going through a brief \ninstruction of the assigned task, \nthe worker was asked to complete 20 classification tasks of the assigned type to get a payment of \\$1.2, where the texts presented in these 20 tasks were randomly sampled from the evaluation dataset for the assigned type of task.  \nAgain, we included two attention check questions in our study to filter out inattentive workers.  We ensured that each task instance received at least three annotations from unique MTurk workers. \n\n\\noindent \\textbf{Computing instance subjectivity.}\nBased on annotations we obtained from attentive workers, \nwe quantify the subjectivity level of each task instance using the fraction of annotators who agree with the majority label for the task instance, that is: \n\\begin{equation}\n    a_i = \\frac{{\\max}_{y \\in \\mathcal{Y}} \\sum_{k=1}^{K_i} \\mathds{1}{(r_{i}^{k} = y)}}{K_i}\n\\end{equation}\nwhere $\\mathcal{Y}=\\{1,\\cdot\\cdot\\cdot, Y\\}$ is the set of all possible labels, $K_i$ is the total number of annotators who labeled instance $i$, and $r_{i}^{k}$ is the $k$-th annotator's annotation on instance $i$. Intuitively, a lower value of $a_i$ suggests that consensus is less likely to be reached among annotators on  instance $i$, thus instance $i$ may have a higher level of subjectivity.  In Table~\\ref{tab:hit2}, we report the average values of\n$a_i$\n(i.e., $\\overline{a}$) for instances in the evaluation datasets of different types of tasks, along with the average inter-annotator agreement on each task instance (as measured by the Krippendorff's $\\alpha$) as well as the task-level subjectivity level for different types of tasks. We can see that \n$\\overline{a}$ closely aligns with the Krippendorff's $\\alpha$, and tasks with higher levels of subjectivity also exhibit a higher value of $\\overline{a}$ in general, indicating that $a_i$ can potentially serve as a reasonable proxy for the subjectivity of each task instance.     \n\n\\begin{figure*}[htbp]\n  \\centering\n  \\subfloat[AG]{\\includegraphics[width=0.19\\textwidth]{figures/ag_line.png}\\label{fig:ag_line}}\n  \\hfill\n  \\subfloat[Relation]{\\includegraphics[width=0.19\\textwidth]{figures/relations_line.png}\\label{fig:rel_line}}\n  \\hfill\n  \\subfloat[IMDB Reviews]{\\includegraphics[width=0.19\\textwidth]{figures/imdb_line.png}\\label{fig:imdb_line}}\n  \\hfill\n  \\subfloat[SMS Spam]{\\includegraphics[width=0.19\\textwidth]{figures/spam_line.png}\\label{fig:spam_line}}\n  \\hfill\n  \\subfloat[Reddit Emotion]{\\includegraphics[width=0.19\\textwidth]{figures/go_emotions_line.png}\\label{fig:goemo_line}}\n  \\\\\n  \\hfill\n  \\subfloat[Sarcasm News]{\\includegraphics[width=0.19\\textwidth]{figures/sarcasm_line.png}\\label{fig:sarcasm_line}}\n  \\hfill\n  \\subfloat[Humor Detection]{\\includegraphics[width=0.19\\textwidth]{figures/humor_line.png}\\label{fig:humor_line}}\n  \\hfill\n  \\subfloat[Tweet Emotions]{\\includegraphics[width=0.19\\textwidth]{figures/tweet_emotion_line.png}\\label{fig:temo_line}}\n  \\hfill\n  \\subfloat[Tweet Irony Speech]{\\includegraphics[width=0.19\\textwidth]{figures/tweet_irony_line.png}\\label{fig:tirony_line}}\n  \\hfill\n   \\subfloat[Financial Phrasebank]{\\includegraphics[width=0.19\\textwidth]{figures/financial_line.png}\\label{fig:fin_line}}\n  \n  \\caption{Changes in the accuracy of the BERT model trained on zero-shot synthetic data as the instance-level annotation agreement threshold varies.  \n  The solid blue line in each plot is the linear regression fitted on the data,  and the $R$-squared score quantifies the goodness of fit. The Spearman's $\\rho$ assesses the strength of rank correlation between the instance-level agreement threshold and the model accuracy for each task. Higher values for both $R$-squared and Spearman's $\\rho$, ideally close to $1$, indicate a stronger monotonic relationship between the instance-level subjectivity and the model accuracy. }\n  \\label{fig:instance_corr}\n  \\vspace{-10pt}\n\\end{figure*}\n\n\\subsection{Evaluation Results}\n\\label{sec:eval2}\n\nWe now look into whether models trained on the LLM-generated synthetic data exhibit different performance on instances with different levels of subjectivity, and we focus on the models trained on zero-shot synthetic data in this evaluation. Specifically, given a classification task, we trained a BERT model using the zero-shot synthetic data and computed its accuracy on the subset of task instances in the evaluation dataset whose instance-level annotation agreement (i.e., $a_i$) exceeds a threshold $\\gamma$, and we repeated this computation for many times as we varied the value of $\\gamma$.  \n\nFigure~\\ref{fig:instance_corr} illustrates how the model accuracy varies with the instance-level annotation agreement threshold $\\gamma$ for different types of tasks. For most tasks (except for the tasks in the Scarcasm News and Finanical Phrasebank datasets), we observe a strong {\\em monotonically increasing} relationship between $\\gamma$ and the model accuracy, with correlations between them (i.e., $\\beta$) being positive and  values of the Spearman's rank correlation coefficient $\\rho$ often exceeding 0.85. Since increasing the instance-level annotation agreement threshold $\\gamma$ effectively filters out task instances with high subjectivity, this observation suggests that models trained on synthetic data indeed tend to have varying performance on different instances---even within the same type of tasks, these models still perform better on those task instances with low subjectivity.  \n\nAs a comparison, we also investigate into whether models trained on the real-world data exhibit similar behaviors. The detailed results are reported in App.~\\ref{e2a}. On the high level, while we also observe the trend that these models' performance appears to increase as the instance-level task subjectivity decreases, such relationship is usually weaker than that illustrated in the models trained on the synthetic data (e.g., $\\beta$ and $\\rho$ are smaller). \n\n\\vspace{-8pt}\n\n\\section{Conclusions and Discussions}\n\\vspace{-4pt}\nIn this paper, we present an initial exploration into factors that moderate the effectiveness of LLM-generated synthetic data for facilitating the training of text classification models.  \nOur results show that the performance of the models trained on synthetic data decreases both for classification tasks with higher levels of subjectivity and on task instances with higher subjectivity. In this section, we provide some potential explanations for the observations of our study, and discuss the implications, limitations, and future directions of our work.\n\\vspace{-2pt}\n\\subsection{Why subjectivity adversely impacts the effectiveness of the synthetic data?}\nWe provide a few explanations for why task subjectivity is found to be negatively associated with the performance of models trained on the LLM-generated synthetic data.\nFirst, highly subjective tasks often require a deep understanding of nuanced human emotions and contextual subtleties, as well as the ability to discern and accurately interpret different perspectives. As such, LLMs may encounter limitations in generating data that can capture the extensive range and complexity of real-life use of language. Indeed, as shown in our exploratory analysis in Section~\\ref{sec:exploratory}, the diversity of the LLM-generated synthetic data appears to be particularly limited on tasks with high subjectivity, when compared to the real-world data. This implies that one potential way to improve the effectiveness of synthetic data on high subjectivity tasks is to increase the data diversity and ensure the synthetic data can better reflect real-world data distributions. \n\nSecond, specific to the relationship between the instance-level subjectivity and model performance, we note that the ``gold label'' of a task instance is usually decided by a majority vote within a group of annotators. This means that the gold label may not represent the perspective of each individual~\\cite{goyal2022your}, and they are sometimes ``biased'' themselves depending on the annotator decomposition~\\cite{li2022towards}. Thus, it may be challenging for LLMs to generate synthetic data to recover such potentially biased ``majority view,'' especially if the LLMs are trained to maintain neutrality. Alternatively, one may ask for subjective task instances that humans can hardly reach any consensus on, whether the ``gold label'' is really the only ``correct'' label? If not, a rethinking of how to develop and evaluate models for these task instances is urgently needed.\n\n\\subsection{Explaining a few exceptions}\n\nIn Table~\\ref{tab:results}, we surprisingly find that on the Tweet irony detection tasks, models trained on the few-shot synthetic data even outperform models trained on the real-world data. One plausible explanation is that the nature of generating irony texts for social media involves a creative writing task with few language formality constraints, and recent research suggests that LLMs have\nthe potential to exhibit comparable creativity with human writers in such task~\\cite{franceschelli2023creativity}. Another exception we find is in Section~\\ref{sec:eval2}---for the Financial Phrasebank and Scarcasm datasets, unlike other tasks, the effectiveness of the models trained on the synthetic data do not vary much with the instance-level task subjectivity. \nWe conjecture that this can be caused by some \ntask-specific properties.\nOn the Financial Phasebank dataset, accurate sentiment analysis \nrequires the understanding of specialized terminology related to finance. Similarly, the Sarcasm detection task aims at identifying sarcasm in news headlines from selected sources and requires the comprehension on political topics. Thus, on these tasks, LLMs might not be fully equipped with the necessary domain knowledge to create effective synthetic data under the zero-shot setting. In fact, as shown in Figure~\\ref{fig:instance_corr}, models trained on the zero-shot synthetic data have very low performance on these two datasets, regardless of the subjectivity levels of task instances. \n\n\\vspace{-4pt}\n\\subsection{Limitations and future work}\n\nWe acknowledge that task subjectivity may not be the only factor that moderates the effectiveness of the LLM-generated synthetic data. Future studies can look into the potential moderating role of other factors, such as language formality and the requirement for domain-specific knowledge.  Our reliance on crowd workers in determining task subjectivity may introduce some variability due to their lack of linguistic expertise. Our evaluation is also based on the GPT-3.5-Turbo model only. It is important to note that the conclusions we get here may not generalize to other LLMs (e.g., the more advanced GPT-4), considering the continuous improvements of LLMs in generating human-like texts.\n\nOur findings suggest that incorporating real-world data examples into the synthetic data generation process can increase the data diversity and boost the performance of the resulting models. Thus, future work can explore strategies that leverage human intelligence, such as feedback or direct intervention in the generation process, to further enrich the diversity of synthetic data~\\cite{chung2023increasing} and to identify the most ``informative'' type of data instance to generate.  Finally, the significant correlation between the subjectivity of tasks or instances and the performance of models trained on synthetic data also suggests the potential to utilize the performance of such models as a proxy for approximating task or instance subjectivity, or to estimate the reliability of gold labels.\n\n\\clearpage\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\nClinical natural language processing faces challenges like complex medical terminology and clinical contexts.\nRecently, large language models (LLMs) have shown promise in this domain. Yet, their direct deployment can lead to privacy issues and are constrained by resources. \nTo address this challenge, we delve into synthetic clinical text generation with LLMs for clinical NLP tasks. \nWe propose an innovative, resource-efficient approach, {\\ours}, which infuses knowledge into the process. Our model involves clinical knowledge extraction and context-informed LLM prompting. \nBoth clinical topics and writing styles are drawn from external domain-specific knowledge graphs and LLMs to guide data generation. \nOur extensive empirical study across 8 clinical NLP tasks and 18 datasets reveals that {\\ours} consistently enhances performance across various tasks by 7.7\\%-8.7\\% on average, effectively aligning the distribution of real datasets and  enriching the diversity of generated training instances. \nOur code is available at \\url{https://github.com/ritaranx/ClinGen}.\n\\end{abstract}\n\n\\vspace{-1ex}\n\\section{Introduction}\n\\label{sec:intro}\nClinical Natural Language Processing (NLP) emerges as a distinct subfield including the extraction, analysis, and interpretation of unstructured clinical text~\\citep{wornow2023shaky}.  \nDespite its significance, unique challenges exist for methodology development in clinical NLP. For example, clinical texts are often dense with abbreviations and specialized medical terminologies can be perplexing to standard NLP models~\\citep{lee2023ai}. \nFortunately, recent advances in Large Language Models (LLMs)~\\citep{brown2020language,chung2022scaling,ouyang2022training,chatgpt,gpt4} provide a promising way to resolve these issues, as they contain billions of parameters and have been pretrained on massive corpora, thus inherently capture a significant amount of clinical knowledge~\\citep{agrawal2022large,singhal2022large}.  \nThese progresses inspire the need for designing specialized approaches for adapting LLMs to clinical settings, which both address the terminology complexities and improve models through clinical data finetuning~\\citep{tu2023towards,liu2023utility}.\n\nDespite the strong capacity of general LLMs, \ndirectly applying them to infer over clinical text data is often undesired in practice. \nFirstly, these LLMs often have billions of parameters that translate to significant computational resources even for inference, leading to \\emph{increased infrastructure costs} and \\emph{long inference time}. \nFurthermore, the sensitive patient information in the clinical text naturally raises \\emph{privacy and regulatory compliance concerns}~\\citep{mesko2023imperative}. \nTo combat these challenges, generating synthetic training data using LLMs serves as a promising solution, as it leverages the capability of LLMs in a resource-efficient and privacy-centric way. When trained with synthetic data mimicking real-world clinical data, models can achieve high performance while obeying data protection regulations. %This paves the way for the broader and safer integration of advanced NLP techniques into the healthcare domain.\n\nSynthetic data generation with LLMs is a popular research area in NLP~\\citep{meng2022generating,ye2022zerogen,ye2022progen,wang2023lets}, with a focus on gemeral-domain data. \nHowever, adapting LLMs trained on general texts for generating high-quality clinical data poses distinct challenges. \nTo assess the quality of data generated by existing methods, we carry out an  evaluation centered on distribution and diversity, detailed in Section~\\ref{sec:preliminary}, which\nindicate a noteworthy data distribution shift. We further examine the clinically-related entity quantities and frequencies in synthetic data, where a notable decline is observed when contrasting synthetic data with ground truth data.  \nWhile some research has delved into clinical data generation with language models, many of these efforts are tailored to specific tasks. \nExamples include medical dialogues~\\citep{chintagunta2021medically}, clinical notes~\\citep{giorgi2023clinical}, \nand electronic health records~\\citep{ive2020generation}. \nThese studies often directly adopt language models for text generation, and sometimes on excessive training data.\nTill now, a unified principle to better adapt LLMs for generating synthetic text for facilitating clinical downstream applications is still missing.\n\nMotivated by the above analysis, we propose \\ours, a \\emph{clinical knowledge-infused}  framework for high-quality clinical text generation in few-shot scenarios. \nOur ultimate goal is to bridge the gap between synthetic and real data while enhancing topic diversity. \nTowards this end, we propose to utilize clinical knowledge extraction to contextualize the prompts. \\blue{This includes generating clinical topics on entity and relation information from both KGs and LLMs and deriving writing style suggestions from LLMs.}\nBy doing this, {\\ours} integrates both \\emph{non-parametric insights} from external clinical knowledge graphs with the \\emph{intrinsic parametric knowledge} encoded in LLMs and \\emph{enjoys higher diversity} via dynamically composing different topics and writing styles together during the data generation process.\nIt is worth noting that, \\ours only relies on minimal additional human efforts, and can be readily applied to a wide array of core tasks in clinical NLP.\n\nOur contributions can be summarized as follows:\n\n$\\bullet$ We propose {\\ours}, a generic clinical knowledge-infused framework for clinical text data generation in few-shot settings. It can be readily applied to a wide range of tasks in clinical NLP.\n\n$\\bullet$ We present an analysis of the pitfall of existing data generation approaches for clinical text data, and propose a \nsimple yet effective strategy to extract clinical knowledge and customize the prompts toward target clinical NLP tasks. This includes generating clinical topics from both KGs and LLMs and deriving writing style suggestions from LLMs. \n\n$\\bullet$ We conduct an exhaustive evaluation of synthetic clinical data generation \\textbf{across 8 clinical NLP tasks and 18 datasets}. Empirical findings demonstrate that {\\ours} not only aligns more closely with the distribution of the original data but also amplifies the diversity of the generated training samples. The empirical performance gains are consistent across various tasks with different LLMs and classifiers (8.7\\% for PubMedBERT$_{\\texttt{Base}}$ and 7.7\\% for PubMedBERT$_{\\texttt{Large}}$).\n\n\\section{Related Work}\nGenerating additional training data enables a more precise analysis of medical text, and has gained more attention in the past years. \nEarlier research has employed data augmentation techniques to generate similar samples to existing instances with word substitution~\\citep{kang2021umls}, back translation~\\citep{uda}, pretrained transformers~\\citep{xu2023weakly,melm}. But they often yield rigid transformations and the quality of the augmented text cannot be always guaranteed. \n\nThe emergence of LLMs has presented new possibilities for synthetic data generation~\\citep{meng2022generating,meng2023tuning,ye2022zerogen,li-etal-2023-synthetic}. However, these methods often use generic and simple prompts that may not fully capture domain-specific knowledge, thus potentially limiting the quality of the generated data. \n\\citet{liu2022wanli,chung-etal-2023-increasing,yu2023large} employ interactive learning to generate instances, at the cost of additional human efforts.\nSeveral recent studies explore LLM-based synthetic data generation for clinical NLP.  \n\\citet{tang2023does} rely on a \\emph{much larger training set} to generate candidate entities, which {disregards the practical low-resource setting}~\\citep{perez2021true}. \nMoreover, these studies often concentrate on specific target tasks, thus lacking generality for diverse clinical NLP scenarios.\n\nOn the other hand, several works aimed at optimizing prompts using LLMs~\\citep{zhou2023large,wang2023promptagent} or knowledge graphs~\\citep{cui2023a,liu-etal-2022-generated,chen2022knowprompt}, yet they mainly focus on refining prompts to obtain the answer for the given input, and the prompt template often remains unchanged. \nInstead, we focus on the different task of generating training instances. By composing different topics and styles together, we can generate diverse templates for prompting LLMs to improve the quality of the synthetic data.\n \\begin{figure*}[!t]\n\t\\centering\n\t\\vspace{-2.5ex}\n\t\\subfigure[CMD]{\n\t\t\\includegraphics[width=0.34\\linewidth]{figures/cmd-baseline.pdf}\n\t\t\\label{fig:cmd-baseline}\n\t} %\\hfill\n         \\hspace{-1.5ex}\n     \\subfigure[Entity Coverage]{\n\t\t\\includegraphics[width=0.34\\linewidth]{figures/avg-entity-baseline.pdf}\n\t\t\\label{fig:avg-entity-baseline}\n\t}\n \\hspace{-1.5ex}\n      \\subfigure[Entity Frequency]{\n\t\t\\includegraphics[width=0.27\\linewidth]{figures/bc5cdr_disease_freq_bsl.pdf}\n\t\t\\label{fig:bc5cdr_disease_freq_bsl}\n\t}\n\t\\caption{Preliminary Studies. (c) is from BC5CDR-Disease and is in log scale. \\vspace{-1ex}}\n\n\\label{fig:prelim2}\n\\end{figure*}\n\n\\section{Preliminary Study}\n\\vspace{-0.5ex}\n\n\\label{sec:preliminary}\nThis section first presents the foundational setup of synthetic data generation. \nThen, we provide an in-depth investigation into the pitfalls of existing synthetic data generation methods. \n\n\\vspace{-1ex}\n\\subsection{Problem Setup}\nIn this paper, we study synthetic data generation under the few-shot setting.\nThe input consists of a training set $\\cD=\\{(x_i,y_i)\\}_{i=1}^K$, where $(x_i, y_i)$ represents an input text and its corresponding label $y_i \\in \\cY$ for the $i$-th example. $K$ denotes the total number of training samples, which is  kept at a very small value (5-shot per label). The primary objective is to harness the LLM $\\cM$ to generate a synthetic dataset, denoted as $\\tilde{\\cD}=\\{(\\tilde{x_i},\\tilde{y_i})\\}_{i=1}^N$, where $N$ is the number of generated samples ($N \\gg K$). \nWe use $\\rho(\\cdot)$ to denote the generation process from the LLM.\nFor each downstream task, we fine-tune a classifier $\\cC_{\\theta}$ \\blue{(a moderate-size pre-trained language model)} parameterized by $\\theta$ on the synthetic dataset $\\tilde{\\cD}$ for evaluating its quality.\\footnote{While In-context Learning~\\citep{brown2020language} can also be utilized, it is often hard to fit all generated instances into the context window, especially for datasets with high cardinality.}\n\n\\vspace{-0.5ex}\n\\subsection{Limitations of Existing  Methods}\n\\label{sec:limitations}\nDenote the task-specific prompts for class label name $j$ as $p_j$, we take a closer look at the synthetic text data generated by two representative approaches: ZeroGen~\\citep{ye2022zerogen}, which directly instructs LLMs for data generation as  $\\tilde{\\cD}_{\\text{Zero}} \\sim \\rho_{j\\sim\\cY}(\\cdot; p_j)$, and DemoGen~\\citep{gpt3mix,meng2023tuning}, which augments the prompt with few-shot demonstrations $\\cD$ as $\\tilde{\\cD}_{\\text{Demo}} \\sim \\rho_{j\\sim\\cY}\\left(\\cdot; [p_j, \\cD]\\right)$.\nThe prompt format of ZeroGen and DemoGen are in Appendix~\\ref{sec:prompt_format_bsl}.\nWe observe that these methods often introduce \\textit{distribution shifts} and exhibit \\textit{limited diversity}, which can lead to suboptimal downstream performance. \n\n\\noindent \\textbf{Distribution Shift.} An inherent issue when adapting LLMs to specific domains for text generation is the \\emph{distribution shift}, given that LLMs are primarily trained on vast amounts of web text in general domains. \nTo quantify the data distribution shift, we employ Central Moment Discrepancy (CMD)~\\citep{zellinger2017central} to measure the gap between synthetic and real data across six clinical NLP datasets --- a high CMD value indicates a large gap between two distributions\\footnote{Details of calculating CMD is in Appendix \\ref{sec:cmd}.}. Figure \\ref{fig:cmd-baseline} illustrates that both ZeroGen and DemoGen exhibit elevated CMD scores. \nDespite the inclusion of few-shot demonstrations in DemoGen, this limitation remains evident, indicating a notable disparity between the ground-truth and synthetic data.\n\n\\noindent \\textbf{Limited Diversity.}\nClinical datasets in real-world scenarios often include rich domain knowledge that can be challenging to replicate in synthetic data. We evaluate synthetic dataset diversity by using both entity quantity and their normalized frequencies. The results are illustrated in Figures~\\ref{fig:avg-entity-baseline} and \\ref{fig:bc5cdr_disease_freq_bsl}. Our analysis reveals that datasets generated by ZeroGen and DemoGen exhibit a limited number of clinical entities, having a substantial discrepancy with the ground truth. \nFurthermore, it is highlighted that only a minority of potential entities and relations are frequently referenced across instances, while the majority are generated infrequently.\n\nTo explicitly illustrate the limitations, we present a case study in Figure~\\ref{fig:prelim1},  Appendix~\\ref{sec:add_prelim}. \nThe comparison reveals that samples generated by ZeroGen and DemoGen lack  \\textit{sufficient details} present in the ground truth data. \nBesides, the generated samples adhere to a more uniform style, while the ground truth encompasses various situations and writing styles, including urgent and informal inquiries.\n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=0.97\\linewidth]{figures/clingen-framework.pdf}\n    \\caption{The overview of \\ours. \\vspace{-2ex}\n    }\n    \\label{fig:overall}\n\\end{figure*}\n\\section{Knowledge Infused Data Generation}\nSection \\ref{sec:preliminary} highlights the necessity of domain-tailored knowledge for clinical synthetic data generation. In pursuit of this, we present {\\ours}, a knowledge-informed framework for clinical data generation. The overview of {\\ours} is shown in Figure~\\ref{fig:overall}. \nThis two-step methodology harnesses the emergent capabilities of LLMs and external knowledge from KGs to facilitate the synthesis of clinical data, even with few-shot examples only.  \n\n\\subsection{Clinical knowledge extraction}\nContrary to previous studies~\\citep{ye2022zerogen,ye2022progen,meng2023tuning} which employ generic queries $p_j$ to prompt LLMs for text generation, {\\ours} emphasizes refining clinically informed prompts. This approach aims to extract rich clinically relevant knowledge from parametric (\\eg LLMs) or non-parametric sources (\\eg knowledge graphs) and tailor it to clinical NLP tasks.\nTo realize this, our modeling contains two dimensions including \\emph{clinical topics} $\\cT$ and \\emph{writing styles} $\\cW$, which are integrated into the original prompts to infuse domain-specific knowledge. \nThe \\emph{Clinical topic} refers to a \\emph{clinical entity} (e.g., disease) or \\emph{relation} (e.g., the relationship between diseases and medications), which is usually a phrase, while the \\emph{writing style} is a phrase that depicts the tone, and overall presentation of the text.\nBy composing different topics and writing styles together, {\\ours} provide a diverse suite of prompts, resulting in a wider spectrum of text produced from the LLM $\\cM$.\nFor details of prompt formats across various tasks, please see \\textbf{Appendix~\\ref{sec:prompt_format}}.\n\n\\subsubsection{Clinical Topics Generation}\nWe provide two choices to generate clinical topics $\\cT$-- one is to sample related entities or relations from external KG, and the other is to query relevant knowledge from LLM.\n\n\\noindent \\textbf{Topics $\\cT_{\\operatorname{KG}}$ sampled from Non-Parametric KGs.} \nHealthcare KGs offer \na rich collection of medical concepts and their complex relationships, \nwhich organizes medical knowledge in a structured way~\\citep{li2022graph}. \nIn our study, we employ the integrative biomedical knowledge hub (iBKH) as the KG~\\citep{su2023biomedical} $\\cG$ to generate topics $\\cT_{\\operatorname{KG}} \\sim \\operatorname{query}(\\cG)$\ndue to its broad coverage over clinical entities. \nTo illustrate, for the Disease Recognition task (NCBI,~\\citet{ncbi-disease}), we extract all disease nodes $e$ from the iBKH to bolster the medical information as \n$\\cT_{\\operatorname{KG}}^{\\operatorname{NCBI}}\\sim \\operatorname{query}(\\cG_{\\operatorname{disease}})$, \n$\\cG_{\\operatorname{disease}}=\\{e\\in \\cG|\\operatorname{type}(e)=\\operatorname{disease}\\}$.\nAs another example, we retrieve links between chemicals  $c$ and diseases $d$  for the chemical and disease relation extraction (CDR,~\\citet{cdr_dataset}) as\n$\\cT_{\\operatorname{KG}}^{\\operatorname{CDR}}\\sim \\operatorname{query}(\\cG_{\\operatorname{relation\\_{cd}}})$, \n$\\cG_{\\operatorname{relation\\_{cd}}}=\\{\\langle c,r,d \\rangle\\in \\cG|\\operatorname{type}(r)=\\operatorname{has\\_{relation}}\\}$. \nBy injecting information from the KG into the data generation step, we ensure the generated samples are more contextually accurate and semantically rich.\n\n\\noindent \\textbf{Topics $\\cT_{\\operatorname{LLM}}$ queried from Parametric LLMs.} \nPre-trained on extensive text corpora such as medical literature, LLMs provide an alternative method for acquiring domain knowledge.\nSpecifically, we aim to harness the rich clinical domain knowledge encoded in ChatGPT (\\texttt{gpt-3.5-turbo-0301}) to augment the prompt. \nThe incorporated prior knowledge from LLMs focus on entity types that hold relevance within clinical text datasets, including \\emph{diseases}, \\emph{drugs}, \\emph{symptoms}, and \\emph{side effects}.\nFor each of entity types $e_i$, we prompt the LLMs by formulating inquiries $q(e_i)$, \\eg, ``\\emph{Suppose you are a clinician and want to collect a set of <Entity Type>. Could you list 300 entities about <Entity Type>?}''. These crafted conversational cues serve as effective prompts to retrieve clinically significant entities from the rich domain knowledge within LLMs as \n$\\cT_{\\operatorname{LLM}} \\sim \\rho\\left(\\cdot;q(e_i)\\right)$. \nFor each entity type, we generate 300 entities for synthetic data generation.\n\n\\subsubsection{Clinical Writing Styles Suggestion}\n\\textbf{Styles suggested by LLMs.} To address the limitations mentioned in Sec~\\ref{sec:limitations} and introduce a diverse range of writing styles $\\cW$ for synthetic samples, we leverage the powerful LLM to suggest candidate writing styles for each task. Specifically, for the task $i$, we incorporate task names $n_i$ into our prompts $p^{\\operatorname{style}}_{i}$ (e.g., \\emph{disease entity recognition}, \\emph{recognizing text entailment}) and integrate few-shot demonstrations $d^{\\operatorname{style}}_{i}$. We then engage LLM in suggesting several potential sources, speakers, or authors of the sentences as $\\cW \\sim \\rho\\left(\\cdot; [p^{\\operatorname{style}}_{i}, d^{\\operatorname{style}}_{i}]\\right)$. \nResponses such as ``\\emph{medical literature}\" or ``\\emph{patient-doctor dialogues}\" are augmented into the prompts to imitate the writing styles found in real datasets. \n\n\\subsection{Knowledge-infused Data Generation}\nWith the generated topics and styles, the key challenge becomes how to leverage them to extract rich clinical information from the LLM for improving synthetic data quality.\nDirectly putting all the elements to enrich the prompt is often infeasible due to the massive size of entities.\nTo balance informativeness as well as diversity, we propose a knowledge-infused strategy, where for each class label name $j\\in\\cY$, the collected clinical topics and writing styles serve as the base unit. \nIn each step, we randomly sample a topic $t \\in \\cT$ and a writing style $w \\in \\cW$ from the candidate set to augment the prompt for class $j \\in \\cY$  as \n$p^{\\operatorname{Clin}}_j(t, w) = [p_j, t, w]$. \nThen, we use the augmented prompt $p^{\\operatorname{Clin}}_j(t, w)$ together with the few-shot demonstrations $\\cD$ to generate the synthetic dataset $\\tilde{\\cD}_{\\operatorname{Clin}}$ as \n\\begin{equation}\n\\setlength{\\abovedisplayskip}{6pt}\n\\setlength{\\belowdisplayskip}{6pt}\n\\tilde{\\cD}_{\\operatorname{Clin}} \\sim \\rho_{j\\sim\\cY, t\\sim \\cT, w\\sim\\cW}\\left(\\cdot; \\left[p_j, t, w\\right], \\cD\\right).\n\\nonumber\n\\end{equation}\nDespite its simplicity, this strategy enjoys several merits: \n(1) \\emph{Clinical infusion}: the clinical context is incorporated into the prompts to directly guide data generation; \n(2) \\emph{Diversity}: it encourages data  diversity via dynamically composing different entities and writing styles into prompts; \n(3) \\emph{Flexibility}: it is compatible with different sources of $\\cT$ and $\\cW$ without reliance on specific knowledge formats. \nConsequently, the quality and clinical relevance of the generated synthetic data are enhanced.  \nWhile some works focus on prompt optimization for data generation or other NLP tasks, they typically utilize a fixed prompt and optimize this prompt format, which is orthogonal to \\ours{}.\n\n\\subsection{Language Model Fine-tuning}\nAfter generating synthetic data $\\tilde{\\cD}$, we fine-tune a pre-trained classifier $\\cC_{\\theta}$ for each downstream task. Following \\citet{meng2023tuning}, we first fine-tune $\\cC_{\\theta}$ on $\\cD$ with standard supervised training objectives on few-shot examples (denoted as $\\ell(\\cdot)$) in Stage 1, then on synthetic data $\\tilde{\\cD}$ in Stage 2 as  \n\\begin{align}\n\\setlength{\\abovedisplayskip}{6pt}\n\\setlength{\\belowdisplayskip}{6pt}\n    \\label{eq:stage1}\n    \\theta^{(1)} &= \\min_{\\theta}~\\mathbb{E}_{(x, y) \\sim \\cD} \\ell\\left( f(x; \\theta), y \\right), \\nonumber \\\\\n    \\theta^{(2)} &=  \\min_{\\theta}~\\mathbb{E}_{(\\tilde{x}, \\tilde{y}) \\sim \\tilde{\\cD}} \\ell\\left( f(\\tilde{x}; \\theta), \\tilde{y} \\right),  \\theta_{\\text{init}} = \\theta^{(1)}.\n    \\nonumber\n\\end{align} \nIt's important to highlight that we strictly follow a standard fine-tuning process and avoid using any extra techniques: (1) for standard classification tasks, $\\ell(\\cdot)$ is the cross-entropy loss; (2) for multi-label classification tasks, $\\ell(\\cdot)$ is the binary cross-entropy loss; \n(3) for token-level classification tasks, we stack an additional linear layer as the classification head and  $\\ell(\\cdot)$ is the token-level cross-entropy loss. \nThe design of \\emph{advanced learning objectives} as well as \\emph{data mixing strategies}, while important, are orthogonal to the scope of this paper. \n\n\\section{Empirical Evaluation}\nGiven our focus on data generation, our major interest lies in faithfully evaluating different synthetic text generation approaches under few-shot scenarios, rather than competing in a ``\\emph{state-of-the-art}\" race with general few-shot NLP methods. \nThe following questions particularly intrigue us:\n\\textbf{RQ1}: How does {\\ours} perform when compared with baselines on different downstream tasks?  \n\\textbf{RQ2}: What impact do factors like LLM generators and synthetic data size have on the performance of {\\ours}?\n\\textbf{RQ3}: How is the quality of the synthetic data generated by {\\ours} and baselines?\n\n\\begin{table*}[t]\n  \\caption{Experimental results aggregated by tasks. \\textbf{Bold} and \\underline{underline} denote the best and second-best results. $\\dagger$: Models exclusive to NER tasks. $*$: Since the two $\\dagger$ models only report results on two NER datasets, we report the average performance on those two datasets for a fair comparison. \n  \"Supervised-Full\" and \"Supervised-Few\" denote the results using the original dataset and using only the few-shot examples as training data, respectively.\n  \\vspace{-1ex}}\n  \\renewcommand\\arraystretch{0.95}\n  \\resizebox{0.98\\linewidth}{!}{\n  \\begin{tabular}{lcc|ccccc|ccccc}\n  \\toprule\n  \\multirow{3.5}{*}{\\bf Task}  & \\multicolumn{2}{c|}{\\textit {Single-Sentence Tasks}} &  \\multicolumn{5}{c|}{\\textit{Sentence-Pair Tasks}} & \\multicolumn{5}{c}{\\textit{Token Classification Tasks}} \\\\\n  \\cmidrule(lr){2-3} \\cmidrule(lr){4-8} \\cmidrule(lr){9-13} \n  & \\bfseries Text Class (2) & \\bfseries RE (3) & \\bfseries NLI (3) & \\multicolumn{2}{c}{\\textbf{Fact Verification (2)}} & \\bfseries STS (1)& \\bfseries QA (2)& \\multicolumn{2}{c}{\\textbf{NER (4)}} & \\multicolumn{3}{c}{\\textbf{MedAttr (1)}} \\\\\n  \\cmidrule(lr){2-2} \\cmidrule(lr){3-3} \\cmidrule(lr){4-4} \\cmidrule(lr){5-6} \\cmidrule(lr){7-7} \\cmidrule(lr){8-8} \\cmidrule(lr){9-10} \\cmidrule(lr){11-13}\n  & F1 & F1 & Acc & Acc & F1 & Acc & Acc & F1 & F1-subset$^*$ & P & R & F1\\\\\n  \\midrule\n  \\multicolumn{12}{l}{\\textbf{PubMedBERT$_{\\texttt{Base}}$}} \\\\\n  \\midrule\n  Supervised-Full & 77.01 & 77.34 & 79.20 & 67.58 & 65.49 & 75.70 & 74.70 & 89.67 & 87.27 & --- & --- & ---\\\\\n  Supervised-Few & 18.61 & 43.89 & 44.64 & 29.43 & 27.10 & 55.70& 54.74 & 39.41 & 34.12 & 38.11 & 43.82 & 40.77 \\\\\n  \\midrule\n  DA-Word Sub~\\shortcite{checklist} & 40.74 & 38.14 & 55.08 & 28.86 & 25.83 & 54.40 & 53.58& 44.30 & 40.41 & 40.25 & 47.65 & 43.64\\\\\n  DA-Back Trans~\\shortcite{uda} & 47.24 & --- & 54.30 & 32.15 & 28.04 & 55.80 & 53.28 & --- & --- & --- & --- & ---\\\\\n  DA-Mixup~\\shortcite{chen2020mixtext,seqmix} & 45.09 & 43.37 & 53.52 & 32.78 & 29.12 & 58.20 & 51.91 & 42.20 & 37.65 & 42.37 & 48.96 & 45.43\\\\\n  DA-Transformer~\\shortcite{melm,kumar2020data}  & 41.02 & 47.56 & 55.71 & 35.32 & 31.77 & 58.80& 56.36 & 44.75 & 39.66 & 37.82 & 44.28 & 40.80\\\\\n  LightNER$^\\dagger$~\\shortcite{lightner} & --- & --- & --- & --- & --- & --- & --- &  ---- & 39.49 & --- & --- & ---\\\\\n  KGPC$^\\dagger$~\\shortcite{chen2023few} & --- & --- & --- & --- & --- & --- & --- &--- &  51.60 & --- & --- & ---\\\\\n  \\midrule\n  ZeroGen~\\shortcite{ye2022zerogen,meng2022generating} & 59.02 & 63.84 & 55.96 & 35.30 & 32.50 & 68.35 &  61.89 & 56.97 & 48.26 & 52.80 & 49.53 & 51.11\\\\\n  DemoGen~\\shortcite{meng2023tuning,gpt3mix} & 64.09 & 67.46 & 59.80 & 40.30 & 35.95 & 70.85 & 62.01 & 60.16 & 53.91 & 58.15 & 56.84 & 57.49\\\\\n  ProGen~\\shortcite{ye2022progen} & 65.16 & 67.23 & 59.57 & 37.71 & 34.54 & 69.30 & 60.74 & 60.49 & 55.11 & 57.76 & 58.57 & 58.16\\\\  \nS3~\\shortcite{wang2023lets} & 65.12 & 67.60 & 61.36 & 40.17 & 36.44 & 70.20 &  63.58 & 60.36 & 54.25 & 56.21 &\t63.60 &\t59.68 \\\\\n  \\midrule\n  \\rowcolor{teal!10} {\\ours} w/ KG & \\underline{67.15} & \\underline{69.01} & \\underline{64.89} & \\underline{43.83} & \\underline{39.43} & \\underline{72.20} & \\bf 71.49 & \\textbf{64.26} & \\textbf{60.11} & \\textbf{71.75} & \\underline{65.20} & \\textbf{68.32}\\\\\n  \\rowcolor{teal!10} {\\ours} w/ LLM & \\textbf{67.82} & \\textbf{70.06} & \\textbf{67.24} & \\textbf{46.50} & \\textbf{41.46} & \\textbf{73.30} & \\underline{69.60} & \\underline{63.17} & \\underline{58.49} & \\underline{68.19} & \\textbf{66.79} & \\underline{67.48}\\\\\n  \\rowcolor{gray!15} Performance Gain & 4.08\\% & 3.63\\% & 9.58\\% & 15.38\\% & 13.77\\% & 3.47\\% & 12.44\\% & 6.23\\% & --- & --- & --- & 14.48\\% \\\\\n  \\midrule\n  \\multicolumn{12}{l}{\\textbf{PubMedBERT$_{\\texttt{Large}}$}} \\\\\n  \\midrule\n  Supervised-Full & 80.06 & 79.64 & 82.65 & 72.97 & 69.23 & 78.80 &80.37 &  90.15 & 87.68 & --- & --- & ---\\\\\n  Supervised-Few & 17.86 & 52.68 & 50.00& 40.90 & 30.50 & 59.73 & 59.50& 42.84 & 37.57 & 41.30 & 45.02 & 43.08 \\\\\n  \\midrule\n  DA-Word Sub~\\shortcite{checklist} & 43.99 & 44.35 & 57.66 & 35.51 & 31.95 & 55.30 & 58.57& 46.67 & 43.70 & 46.77 & 43.52 & 45.09\\\\\n  DA-Back Trans~\\shortcite{uda} & 50.98 & --- & 58.39 & 34.12 & 31.36 & 56.40 &57.19 & --- & --- & --- & --- & ---\\\\\n  DA-Mixup~\\shortcite{chen2020mixtext,seqmix} & 46.74 & 50.97 & 57.35 & 34.01 & 31.10 & 58.50 & 56.68 & 46.69 & 43.01 & 41.25 & 52.09 & 46.04\\\\\n  DA-Transformer~\\shortcite{melm,kumar2020data} & 44.41 & 46.12 & 58.94 & 35.09 & 30.95 & 58.10 & 59.30 & 46.94 & 43.50 & 43.36 & 45.78 & 44.54\\\\\n  \\midrule\n  ZeroGen~\\shortcite{ye2022zerogen,meng2022generating} & 61.51 & 65.18 & 63.47 & 41.12 & 36.10 & 72.69 &66.02 & 57.79 & 49.10 & 54.04 & 51.40 & 52.69\\\\\n  DemoGen~\\shortcite{meng2023tuning,gpt3mix} & 64.97 & 68.65 & 64.58 & 42.61 & 38.69 & 74.37 & 65.04 & 61.43 & 55.61 & 62.67 & 61.02 & 61.83\\\\\n  ProGen~\\shortcite{ye2022progen} & 65.01 & 69.23 & 63.32 & 42.79 & 38.63 & 74.90 & 63.27 & 62.47 & 57.31 & 57.21 & 63.70 & 60.28\\\\\n S3~\\shortcite{wang2023lets} & 64.33 & 69.65 & 65.07 & 41.76 & 37.72 & 73.20 & 66.33 & 61.97 & 56.29 & 63.07\t&62.72&\t62.89 \\\\\n\n  \\midrule\n  \\rowcolor{teal!10} {\\ours} w/ KG & \\underline{66.76} & \\underline{71.47} & \\textbf{70.90} & \\underline{48.62} & \\underline{42.45} & \\underline{75.40} & \\bf 73.94 & \\textbf{65.48} & \\textbf{62.23} & \\underline{70.96} & \\textbf{69.66} & \\textbf{70.30}\\\\\n  \\rowcolor{teal!10} {\\ours} w/ LLM & \\textbf{67.61} & \\textbf{72.81} & \\underline{70.50} & \\textbf{49.51} & \\textbf{43.72} & \\textbf{76.21} &  \\underline{73.40} &\\underline{65.36} & \\underline{61.89} & \\textbf{71.61} & \\underline{66.86} & \\underline{69.15}\\\\\n  \\rowcolor{gray!15} {Performance Gain} & 4.00\\% & 4.54\\% & 8.96\\% & 15.70\\% & 13.00\\% & 3.47\\% & 11.47\\% & 1.76\\% & --- & --- & --- & 11.78\\% \\\\\n  \\bottomrule\n  \\end{tabular}\n   }\n  \\label{tab:main-table}\n\\end{table*}\n\\subsection{Experiment Setup}\nWe conduct experiments in the few-shot settings with 5 examples for each class. We employ ChatGPT~\\citep{chatgpt} (\\texttt{gpt-3.5-turbo-0301}) as the LLM generator $\\cM$\\footnote{Studies on using Medical LLMs are in Appendix \\ref{sec:med_llm_gen}.} and \\textbf{maintain the same amount of synthetic training data for both {\\ours} and baselines for a fair comparison.} The pre-trained PubMedBERT~\\citep{gu2021domain} is then applied to fine-tune on the synthetic data for both {\\ours} and baselines, where we consider both the \\texttt{Base} and \\texttt{Large} model. \n\n\\noindent \\textbf{Datasets and Tasks.}\nWe undertake a comprehensive evaluation of \\textbf{18 datasets} across a diverse array of tasks in clinical NLP benchmarks~\\citep{blue,fries2022bigbio}: 2 text classification, 3 relation extraction (RE), 3 natural language inference (NLI), 2 fact verification, 2 question answering (QA), 1 sentence similarity (STS), 4 Named Entity Recognition (NER), and 1 attribute extraction datasets. \nPlease see Appendix~\\ref{sec:dataset_description} for descriptions and the statistics of each dataset.\n\n\\noindent  \\textbf{Baselines.}\nWe compare {\\ours} with \\textbf{10 baselines} in total, including\n6 data augmentation and 4 LLM-based data generation techniques. \nSee Appendix~\\ref{sec:baseline_details} for their descriptions. \n\n\\noindent \\textbf{Implementation Details.}\n\\label{sec:implementation_details}\nFor implementation, we use PyTorch~\\citep{paszke2019pytorch} and HuggingFace~\\citep{wolf2019huggingface}. For each dataset, we randomly sample 5 examples from each class to provide few-shot demonstrations and keep a validation set of the same size. \nDuring the data generation process when we call the ChatGPT APIs~\\citep{chatgpt}, we set the parameter $\\operatorname{top\\_p}=1.0$ and temperature $t=1.0$ to balance between the quality of the generated text as well as diversity~\\citep{chung-etal-2023-increasing,yu2023large}\\footnote{We do not further increase $t$, as previous analysis  \\citep{chung-etal-2023-increasing,yu2023large} has shown that increasing $t$ to larger value does not help with additional performance gain.}. \nIn the experiments, We generate 5000 synthetic training data for both {\\ours} and the baselines and report the average performance over 3 random seeds for all the results. \nWith the generated synthetic dataset, we follow the common few-shot learning setting~\\citep{perez2021true} to train all the models for 6 epochs and use the model with the best performance on the validation set for evaluation. \nDuring the PubMedBERT fine-tuning, we adopt AdamW~\\citep{loshchilov2017decoupled} for optimization with a linear warmup of the first 5\\% steps and linear learning rate decay. The learning rate is set to 2e-5 for \\texttt{Base} and 1e-5 for \\texttt{Large}, and the maximum number of tokens per sequence is 256. \n\n\\begin{figure*}[t]\n\\vspace{-2.5ex}\n    \\centering\n    \\begin{minipage}{0.48\\textwidth}\n        \\centering\n        \\subfigure[HOC]{\n            \\includegraphics[width=0.48\\textwidth]{figures/generator-HOC.pdf}\n            \\label{fig:generator-HOC}\n        } \\hspace{-3mm}\n        \\subfigure[MEDIQA-RQE]{\n            \\includegraphics[width=0.48\\textwidth]{figures/generator-MEDIQA-RQE.pdf}\n            \\label{fig:generator-MEDIQA-RQE}\n        }\n        \\vspace{-2ex}\n        \\RawCaption{\\caption{Different generators at \\texttt{Base}.}\\label{fig:generator}}\n    \\end{minipage}%\n    \\begin{minipage}{0.48\\textwidth}\n        \\centering\n        \\subfigure[HOC]{\n            \\includegraphics[width=0.5\\textwidth]{figures/size-HOC.pdf}\n            \\label{fig:size-HOC}\n        } \\hspace{-6mm}\n        \\subfigure[MEDIQA-RQE]{\n            \\includegraphics[width=0.5\\textwidth]{figures/size-Mediqa-rqe.pdf}\n            \\label{fig:size-Mediqa-rqe}\n        }\n        \\vspace{-2ex}\n        \\RawCaption{\\caption{Different proportion of data at \\texttt{Base}.}\\label{fig:size-synthetic}}\n    \\end{minipage}%\n    \\vspace{-0.5ex}\n\\end{figure*}\n\n\\begin{table*}[t]\n  \\caption{Comparison between prompting LLM for inference and {\\ours} at \\texttt{Large} scale.\\vspace{-1ex}}\n    \\renewcommand\\arraystretch{0.95}\n  \\resizebox{0.99\\linewidth}{!}{\n  \\begin{tabular}{lcccccccccccccc}\n  \\toprule\n  & \\bfseries HOC & \\multicolumn{3}{c}{\\textbf{GAD}} & \\bfseries ChemProt & \\bfseries MEDIQA-RQE & \\multicolumn{2}{c}{\\textbf{PUBHEALTH}} & \\multicolumn{3}{c}{\\textbf{NCBI-Disease}} & \\multicolumn{3}{c}{\\textbf{CASI}}\\\\\n  \\cmidrule(lr){2-2} \\cmidrule(lr){3-5} \\cmidrule(lr){6-6} \\cmidrule(lr){7-7} \\cmidrule(lr){8-9} \\cmidrule(lr){10-12} \\cmidrule(lr){13-15}\n  & F1 & P & R & F1 & F1 & ACC & ACC & F1 & P & R & F1 & P & R & F1\\\\\n  \\midrule\n  \n  ChatGPT Inference~\\scriptsize{(\\citeauthor{chatgpt})} & 68.76 & 84.21 & \\textbf{97.46} & 90.35 & 49.42 & 74.31 & \\textbf{69.50} & \\textbf{52.47} & 46.62 & 52.31 & 49.30 & 48.82 & \\textbf{74.75} & 59.07\\\\\n  \\blue{PMC-LLaMa-13B Inference}~\\scriptsize{(\\citeauthor{wu2023pmcllama})} & 50.07 & 89.61 & 81.18 & 85.19  & 33.35 & 52.17 & 48.01 & 32.84 & 27.11 & 23.97 & 25.44 & 56.38 & 36.87 & 41.58\n  \\\\\n \\blue{MedAlpaca-13B Inference}~\\scriptsize{(\\citeauthor{han2023medalpaca})} & \\blue{40.44} & \\blue{71.95} & \\blue{72.48} & \\blue{72.21} & \\blue{31.29} & \\blue{58.12} & \\blue{55.40} & \\blue{34.63} & \\blue{44.69} & \\blue{31.16} & \\blue{27.85} & \\blue{52.51} & \\blue{49.16} & \\blue{51.64}\n \\\\\n  \\midrule\n  \\rowcolor{teal!10} {\\ours} w/ KG & 77.71 & 94.30 & 89.09 & \\textbf{91.62} & 60.12 & \\textbf{79.92} & 50.20 & 41.26 & \\textbf{62.46} & \\textbf{64.08} & \\textbf{63.26} & 70.96 & 69.66 & \\textbf{70.30} \\\\\n  \\rowcolor{teal!10} {\\ours} w/ LLM & \\textbf{78.14} & \\textbf{95.08} & 86.14 & 90.39 & \\textbf{63.05} & 77.36 & 52.96 & 43.31 & 61.12 & 60.16 & 60.64 & \\textbf{71.61} & 66.86 & 69.15 \\\\\n  \\bottomrule\n  \\end{tabular}\n  }\n  \\label{tab:gpt_inference}\n  \\vspace{-1ex}\n\\end{table*}\n\\subsection{Model Performance with Synthetic Data}\n\\label{sec:model_perf}\nTable~\\ref{tab:main-table} summarizes the experimental results. \nDue to space limits, we report the average performance over all datasets for each task, but provide the detailed results for each dataset in Tables~\\ref{tab:single-sent}, \\ref{tab:sent-pair}, \\ref{tab:token-class} in Appendix~\\ref{sec:more_experimental_results}. \nBased on the experimental results, we have the following findings:\n\n\\noindent $\\diamond$ Our approach, {\\ours}, consistently outperforms the baselines across all tasks. The average performance gain over all \\textit{main} metrics is 8.7\\% at \\texttt{Base} scale and 7.7\\% at \\texttt{Large} scale. \nLLM-based methods outperform traditional DA techniques, showcasing their ability to capture task-specific information from a few examples. \nDemoGen and ProGen's gains over ZeroGen highlight the positive impact of few-shot examples.\nDespite being one of the most powerful data generation approaches, S3's gains are marginal in the few-shot setting due to its reliance on large validation sets.\n\n\\noindent $\\diamond$ In \\textit{token classification tasks}, {\\ours} performs better with KG compared to LLM due to the better alignment between the task's target and the generated domain knowledge, where the extracted topics serve as direct labels. \nConversely, single-sentence and sentence-pair tasks favor LLM-based knowledge extraction. \nThis could be because (1) These tasks prioritize sentence comprehension over specific terminologies, and some specialized terms might even impede LLM comprehension. (2) KGs \\emph{may not} always contain the required information, e.g., certain relations in chemical/protein relation extraction tasks, limiting performance gains.\n\n\\noindent $\\diamond$ Some DA methods are task-specific, limiting their generalizability. For example, LightNER and KGPC are designed for NER. It is also non-trivial to apply Back Translation to NER or RE, as it requires locating related entities in the generated sentence accurately.\nIn contrast, {\\ours} is flexible and can be readily applied to various tasks.\n\n\\subsection{Ablation and Parameter Studies}\n\\label{sec:ablation}\n\\noindent \\textbf{Effect of Different LLM Generators.}\nTo investigate the impact of various LLMs on {\\ours},  \nwe utilize InstructGPT (\\texttt{text-curie-001})~\\citep{ouyang2022training} and GPT-4~\\citep{gpt4}. Note that we only generate 500 samples in the GPT-4 setting due to budget constraints, but we provide the results of GPT-3.5 with same amount of synthetic samples for a fair comparison. \nFrom Figure~\\ref{fig:generator} we observe that {\\ours} generally outperforms the best baseline in all settings.\nAdditionally, we observe generally improved performance with larger models, as they often have better capabilities to follow our designed instructions for the given prompts. See Appendix~\\ref{sec:add_ablation_para} for more results.\n\n\\noindent \\textbf{Effect of Size of Synthetic Data.}\nIn Figure~\\ref{fig:size-synthetic} (and more in Appendix~\\ref{sec:add_ablation_para}), we study the effect of the size of synthetic data. The result shows that {\\ours} consistently outperforms the best baseline, using only around 10\\% of the synthetic examples. This illustrates that incorporating domain knowledge and increasing the diversity of the prompts could be an effective way to improve the sample efficiency and narrow the gap between the performance of synthetic and ground-truth datasets.\n\n\\begin{table}[tp]\n  \\caption{Ablation studies on topic extraction and style suggestion at \\texttt{Base} scale. \\vspace{-1ex}}\n  \\resizebox{\\linewidth}{!}{\n  \\begin{tabular}{lcc|cc|cc|cc}\n  \\toprule\n  & \\multicolumn{2}{c}{\\textbf{HOC}} & \\multicolumn{2}{c}{\\textbf{CDR}} & \\multicolumn{2}{c}{\\textbf{MEDIQA-RQE}} & \\multicolumn{2}{c}{\\textbf{NCBI-Disease}}\\\\\n  \\cmidrule(lr){2-3} \\cmidrule(lr){4-5} \\cmidrule(lr){6-7} \\cmidrule(lr){8-9}\n  & w/ KG & w/ LLM & w/ KG & w/ LLM & w/ KG & w/ LLM & w/ KG & w/ LLM \\\\\n  \\midrule\n  \\rowcolor{teal!10} {\\ours} & \\textbf{76.28} & \\textbf{76.42} & \\textbf{61.74} & \\textbf{63.34} & \\textbf{74.85} & \\textbf{72.40} & \\textbf{59.46} & \\textbf{55.95} \\\\\n  w/o Styles & 73.25 & 74.40 & 59.10 & 60.15 & 67.21 & 66.50 & 57.97 & 54.70 \\\\\n  w/o Topics & \\multicolumn{2}{c|}{70.86} & \\multicolumn{2}{c|}{58.51} & \\multicolumn{2}{c|}{69.86} & \\multicolumn{2}{c}{55.09} \\\\\n  \\bottomrule\n  \\end{tabular}\n  }\n  \\label{tab:ablation}\n  \\vspace{-1ex}\n\\end{table}\n\n\\begin{figure*}[tp]\n\t\\centering\n\t\\vspace{-1ex}\n\t\\subfigure[t-SNE plot]{\n\t\t\\includegraphics[width=0.24\\linewidth]{figures/bc5cdr_disease_sentencebert_ours.pdf}\n\t\t\\label{fig:bc5cdr_disease_sentencebert_ours}\n\t} %\\hfill\n         \\hspace{-0.2ex}\n     \\subfigure[Case study of generated examples]{\n\t\t\\includegraphics[width=0.65\\linewidth]{figures/case_study_llm.pdf}\n\t\t\\label{fig:case_study_llm}\n\t}\n\t\\caption{Data distribution and diversity measures on {\\ours}. (a) is from BC5CDR-Disease and (b) is from MEDIQA-RQE using {\\ours} with LLM. \\vspace{-2ex}}\n\\label{fig:quality_ana1}\n\\end{figure*}\n\n \\begin{figure*}[tp]\n\t\\centering\n\t\\vspace{-1ex}\n\t\\subfigure[CMD]{\n\t\t\\includegraphics[width=0.362\\linewidth]{figures/cmd-all.pdf}\n\t\t\\label{fig:cmd-all}\n\t} %\\hfill\n         \\hspace{-1.5ex}\n     \\subfigure[Entity Coverage]{\n\t\t\\includegraphics[width=0.362\\linewidth]{figures/avg-entity-all.pdf}\n\t\t\\label{fig:avg-entity-all}\n\t}\n \\hspace{-1.5ex}\n      \\subfigure[Entity Frequency]{\n\t\t\\includegraphics[width=0.248\\linewidth]{figures/bc5cdr_disease_freq.pdf}\n\t\t\\label{fig:bc5cdr_disease_freq}\n\t}\n\t\\caption{Data distribution and diversity measures on {\\ours}. (c) is from BC5CDR-Disease.\\vspace{-2ex}}\n\t\\vspace{-1.5ex}\n\\label{fig:quality_ana2}\n\\end{figure*}\n\n\\noindent  \\textbf{Comparison with few-shot inference via prompting LLM.}\nWe also evaluate the performance of 5-shot in-context learning with ChatGPT \\blue{and 3 medical LLMs, namely PMC-LLaMa-13b~\\citep{wu2023pmcllama}, MedAlpaca-13b~\\citep{han2023medalpaca}}. Due to budget limits, we  run experiments on datasets with few testing samples for each task. \nAs presented in Table~\\ref{tab:gpt_inference}, {\\ours} at PubMedBERT$_{\\texttt{Large}}$ scale achieves better results on 5 out of 6 datasets than ChatGPT few-shot learning, which uses $\\sim 530 \\times$ more parameters. \nOne exception is for PUBHEALTH, as it requires complex reasoning abilities that PubMedBERT$_{\\texttt{Large}}$ may not fully possess. \nThree medical LLMs, on the other hand, perform less effectively than both {\\ours} and GPT-3.5 due to fewer parameters, limited reasoning capabilities, and training on a general medical corpus unsuited for the tasks.\nOverall, {\\ours} offers cost-effective and time-efficient advantages. \nWhile it entails a one-time investment in both money and time for synthetic training data generation, subsequent prediction relying on a moderate-sized model is much more efficient. \nBesides, the continued use of ChatGPT for inference on new testing data \nincurs ongoing time and financial costs, while our model requires zero additional costs for new data. \n\n\\noindent \\textbf{Effect of Topic Extraction and Style Suggestion.}\nWe inspect different components of {\\ours} in Table~\\ref{tab:ablation}. It is observed that both Topics Extraction and Style Suggestion contribute to model performance as they enhance the relevance of generated samples to domain knowledge and introduce greater diversity. Different from the other datasets, MEDIQA-RQE shows more performance gain incorporating writing style than topics. It is because NLI tasks focus on capturing the relationships between two sentences while incorporating additional knowledge entities does not directly help the model improve the reasoning ability.\n\n\\section{Quality Analysis of the Synthetic Data}\n\\label{sec:quality_analysis}\n\\textbf{Data Distribution Measures.}\nFigure~\\ref{fig:bc5cdr_disease_sentencebert_ours} shows the t-SNE plot of data generated by {\\ours} and baselines compared with the ground truth. This visualization demonstrates that {\\ours} exhibits a greater overlap with the ground truth, indicating a similar distribution as the original dataset.\nIn addition, as depicted in Figure \\ref{fig:cmd-all}, the embedding of \\textit{\\ours} aligns more closely with the ground truth distribution than other baselines across all six datasets, further justifying the efficacy of {\\ours} for mitigating the distribution shift issue.\n\n\\begin{table}[t]\n  \\caption{Average Pairwise Similarity. \\vspace{-3ex}}\n  \\resizebox{0.92\\linewidth}{!}{\n  \\begin{tabular}{lcccc}\n  \\toprule\n  & \\bfseries HOC & \\bfseries CDR & \\bfseries MEDIQA-RQE & \\bfseries NCBI-Disease \\\\\n  \\midrule\n  ZeroGen & 0.512 & 0.469  &   0.277 &  0.528  \\\\\n  DemoGen & 0.463 &    0.377   & 0.289  &  0.281  \\\\\n  ProGen  & 0.481 &   0.321   & 0.290  &  0.357  \\\\\n  \\rowcolor{teal!10} {\\ours} w/ KG  & 0.440 & \\textbf{0.291}   & \\textbf{0.243} &  0.180    \\\\\n  \\rowcolor{teal!10} {\\ours} w/ LLM & \\textbf{0.432}  & 0.338  &  0.255 &  \\textbf{0.155} \\\\\n  Ground truth   & 0.265  & 0.268  &  0.164 &  0.262  \\\\\n  \\bottomrule\n  \\end{tabular}\n  }\n  \\label{table:aps}\n  \\vspace{-1ex}\n\\end{table}\n\n\\noindent \\textbf{Diversity Measures.}\n\\label{sec:diversity_measures}\nTable~\\ref{table:aps} calculates the average cosine similarity for sample pairs using SentenceBERT embeddings.\nCompared to baselines, the dataset generated with {\\ours} exhibits lower cosine similarity and the average similarity is close to that of the ground truth training data, which shows {\\ours} could render more diverse data.  \n\nMoreover, Figure \\ref{fig:avg-entity-all} highlights {\\ours} covers a broader range of entities than baselines, with {\\ours} w/ KG capturing more entities due to KGs' extensive knowledge. Figure \\ref{fig:bc5cdr_disease_freq} reflects {\\ours} has a more balanced entity frequency distribution aligned with ground truth, ensuring diverse topic coverage.\n\n\\begin{table*}[t]\n  \\caption{The average cost (in US dollars) of running {\\ours} on various datasets per 1000 samples, compared with prompting GPT-3.5 for inference and DemoGen.}\n  \\resizebox{0.8\\linewidth}{!}{\n  \\begin{tabular}{lccccccc}\n  \\toprule\n  & \\bfseries HOC & \\bfseries GAD & \\bfseries ChemProt & \\bfseries MEDIQA-RQE & \\bfseries PUBHEALTH & \\bfseries NCBI-Disease & \\bfseries CASI\\\\\n  \\midrule\n  GPT-3.5 Inference & 1.09 & 1.05 & 5.75 & 2.15 & 2.80 & 0.90 & 1.30 \\\\ \n  DemoGen & 0.59 & 0.66 & 1.35 & 0.81 & 0.92 & 1.12 & 1.28 \\\\\n  \\rowcolor{teal!10} {\\ours} w/ KG & 0.65 & 0.73 & 1.47 & 0.86 & 1.01 & 1.41 & 1.55 \\\\\n  \\rowcolor{teal!10} {\\ours} w/ LLM & 0.72 & 0.84 & 1.51 & 0.90 & 1.34 & 1.49 & 1.62 \\\\\n  \\bottomrule\n  \\end{tabular}\n  }\n  \\label{tab:money_cost}\n  \\vspace{-1ex}\n\\end{table*}\n\n\\noindent \\textbf{Case Study.}\nIn Figure~\\ref{fig:case_study_llm}, we present a case study of examples generated by {\\ours} with LLM on MEDIQA-RQE dataset, which consists of consumer health queries. The examples reveal that the sentences generated by {\\ours} include more extensive contextual information compared with the baseline. These sentences closely resemble the queries people might pose in real-life scenarios.\n\n\\noindent \\textbf{Study on Factual Consistency.} \nA human evaluation was carried out to assess the factual accuracy of the generated outputs across six representative tasks: LitCovid, CDR, Mediqa-RQE, MQP, PubHealth, and BC5CDR. For each task, a sample of 100 examples per class was randomly selected. Medical students then examine the generated text and evaluate its factuality. The findings from this rigorous human study revealed no instances of misinformation or hallucinated content in the randomly sampled examples, verifying the system's reliability in generating factually sound outputs.\n\n\\noindent \\textbf{Monetary Cost}\n\\label{sec:apd_cost}\nWe display the monetary cost of {\\ours} for calling the OpenAI APIs, with a comparison with prompting GPT-3.5 for direct inference and DemoGen. From the values shown in Table~\\ref{tab:money_cost}, we observe that inference via GPT-3.5 generally has a higher cost, as it needs to input all the testing samples for prompting. In contrast, DemoGen has a relatively lower cost, because it does not include the topics and writing styles to the prompts as {\\ours} does.\n\n\\section{Conclusion}\n\nIn this work, we study clinical text data generation using LLMs. We thoroughly assess existing methods for clinical data generation and identify issues including distribution shifts and limited diversity. \nTo tackle these challenges, we introduce {\\ours}, a  framework that leverages clinical knowledge from non-parametric KGs and parametric LLMs. \nThis empowers data generation by utilizing clinical topic knowledge and real-world writing styles in domain-specific prompts. \nOur extensive empirical evaluations across 8 clinical NLP tasks and 18 datasets, compared to 10 baseline methods, consistently show that {\\ours} improves task performance, aligns closely with real data, and enhances data diversity. \nWe expect {\\ours} can be seamlessly incorporated into a broad suite of clinical text tasks to advance clinical NLP research.% \\clearpage\n\\section*{Acknowledgement}\nWe thank the anonymous reviewers and area chairs for valuable feedbacks. \nThis research was partially supported by the Emory Global Diabetes Center of the Woodruff Sciences Center, Emory University. Research reported in this publication was supported by the National Institute Of Diabetes And Digestive And Kidney Diseases of the National Institutes of Health under Award Number K25DK135913. \nThe research also receives partial support by the National Science Foundation under Award Number IIS-2145411. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.\nWe also thank Microsoft for providing research credits under the Accelerating Foundation Models Research Program.\n\n\\section*{Limitation}\nIn this work, we propose {\\ours} to better harness the LLM for synthetic text data generation. \nDespite its strong performance, we mainly verify their efficacy from their empirical performance, sample diversity, and distribution gaps. There are still some limitations to this work:\n\n\\noindent \\textbf{Factuality of LLM-generated Text}. One issue with LLM-based synthetic data generation is the phenomenon of \\emph{hallucination}, wherein the model generates information that does not ground in reality~\\citep{zhang2023siren}. This can lead to the propagation of misinformation, which may have negative impacts on the clinical domain. However, we have conducted a human study to justify that \\emph{our generated synthetic data does not suffer from the issue of misinformation}.\n\n\\noindent \\textbf{Application to other type of clinical data}.\nApart from text, there are other types of clinical data: \nFor example, EHR data falls within a distinct modality (i.e. tabular data) from textual data, which may require different methodologies and approaches~\\citep{wornow2023shaky}. \n\n\\section*{Ethics Consideration}\nOn specific issue is about patient privacy. To eliminate this concern, we carefully select the five few-shot demonstrations to ensure they are fully free from any Protected Health Information (PHI) related to patients.  We also make a deliberate effort to \\emph{avoid any instructions} that can potentially extract sensitive patient information within the prompts. \nIn addition, we have opted out of human review for the data by completing the Azure OpenAI Additional Use Case Form\\footnote{\\url{https://aka.ms/oai/additionalusecase}}. This allows us to use the Azure OpenAI service while ensuring Microsoft does not have access to patient data.\n\n\\clearpage\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{LAB: Large-Scale Alignment for ChatBots}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nThis work introduces LAB (Large-scale Alignment for chatBots), a novel methodology designed to overcome the scalability challenges in the instruction-tuning phase of large language model (LLM) training. Leveraging a taxonomy-guided synthetic data generation process and a multi-phase tuning framework, LAB significantly reduces reliance on expensive human annotations and proprietary models like GPT-4. We demonstrate that LAB-trained models can achieve competitive performance across several benchmarks compared to models trained with traditional human-annotated or GPT-4 generated synthetic data. Thus offering a scalable, cost-effective solution for enhancing LLM capabilities and instruction-following behaviors without the drawbacks of catastrophic forgetting, marking a step forward in the efficient training of LLMs for a wide range of applications.\n\\end{abstract}\n\n\\section{Introduction}\n\nLarge language models (LLMs) have achieved remarkable levels of success in various natural language processing (NLP) applications, including question-answering \\cite{}, entity extraction \\cite{}, and summarization \\cite{}. This has been made possible, in large part, by the introduction of the transformer architecture \\cite{}, which can leverage large amounts of unlabeled, unstructured data, enabling the scaling of LLMs to billions, or even trillions of parameters. LLMs are typically trained in phases: a self-supervised pre-training phase, followed by supervised alignment tuning phases. \n\nThe majority of the cost of training an LLM comes from the pre-training phase. During this phase, a model is trained in an auto-regressive manner to predict the next token in the target language using trillions of tokens worth of unlabeled data, requiring thousands of GPUs training for months at a time. Alignment tuning, typically happens in two stages: instruction tuning, followed by preference tuning. Instruction tuning is more akin to the traditional model training approach in machine learning, where the model is trained directly on tasks of interest. In this stage, the model is given a task description in the form of an natural language instuction (e.g. \\textit{Summarize the following news article in 2 lines: \\{News article\\}}) and the model is trained to maximize the likelihood of the provided ground truth summary. Preference tuning, on the other hand, is done using techniques such as RLHF \\citep{stiennon2022learning,ouyang2022training} and DPO \\citep{rafailov2023direct}, where the response from an instruction-tuned model is rated as preferred or unpreferred using human feedback. \n\nIn comparison to pre-training, the instruction tuning and preference tuning stages comprise a small fraction of the overall training procedure, both in terms of the data used as well as the compute infrastructure required to train models \\cite{touvron2023llama}. For example, Meta's LLaMA 2 models were trained with just tens of thousands of high quality human-generated instruction/response data pairs, followed by multiple rounds of RLHF with a comparatively limited number of examples as compared to pretraining data volumes \\cite{touvron2023llama}. From a traditional machine learning training perspective, this imbalance in the scale across the phases is unconventional---typically one would expect a model to perform best when it has been trained directly on the desired tasks, using as much data as possible. The deviation from the tradtional LLM approach relies on the idea that pretraining captures enough of the distribution of language and knowledge, such that a small amount of supervised training can ``unlock'' or shape latent abilities related to the ultimate desired instruction-following behavior of the model. However, unlike the unstructured data that is abundantly available in the public domain, high-quality, human-generated task-specific instruction data is costly to procure, even via crowd-sourcing, and human-generated instruction data is typically closely guarded by model builders, even for ostensibly ``open'' model-building efforts.\nIn this work, we address the challenges associated with scaling of the alignment-tuning phase and propose a new method called LAB: Large-scale Alignment for chatBots. % DDC: I don't think it's actually necessary to use the \"labrador\", as opposed to LAB, name here (here affectionately referred to as ``Labrador'').\nThe LAB method consists of two components: (i) a taxonomy-guided synthetic data generation method and quality assurance process that yields a highly diverse and high-quality instruction dataset, without resorting to the use of proprietary LLMs like GPT-4 or substantial human curation, and (ii) a novel multi-phase training framework and unconventional tuning regime that allows for adding new knowledge and instruction-following abilities into pre-trained LLMs without suffering from catastrophic forgetting. Our findings show that LAB-trained models can perform competitively with proprietary and open-source models that use human annotations and/or synthetic data generated using GPT-4 on a number of benchmarks. \n\n\\section{Related Work}\n\nExisting methods for instruction tuning typically either rely on humans for generating high-quality datasets, or use synthetic data generation using a large teacher model.  \nOpenAI \\citep{ouyang2022training} arguably set the standard for model alignment from human data, employing human annotators to gather data for supervised fine tuning (SFT) and reinforcement learning with human feedback (RLHF) training. Collecting human-generated data for these steps is complex undertaking; the selection of annotators requires a rigorous multi-stage screening process aimed at achieving high inter-annotator agreement, and collecting even modest amounts data (by LLM standards) requires the coordination of large groups of annotators.\nThe creators of the LLaMA 2 model series \\citep{touvron2023llama}  followed a similar recipe, collecting tens of thousands of human-generated instruction samples, and approximately 1 million human-annotated binary comparisons for reward modeling. \nNot only are such approaches expensive and time consuming, but they can also potentially limit agility in exploring the space of instructions and capabilities the model is trained to perform.\nAlternatives to this approach, such as transforming existing human datasets into instructions via templating \\citep{weifinetuned} can be more cost effective, but face limitations in the naturalness and length of the responses used for training.\n\nMore recently, training with synthetic data generated from LLMs has emerged as an alternative to purely human-data-based approaches.\n\\cite{wang2023selfinstructaligning} introduced Self-Instruct, which leverages a small number of handwritten human seed instructions as input to bootstrapping process to generate a large number of samples using an LLM's own generation abilities.\n\\cite{alpaca} built upon Self-Instruct, using a larger teacher model to generate synthetic data to train a smaller student model, and incorporating principles in the generation prompt to promote diversity in the generated instruction data.\n\\cite{xu2023wizardlmempowering} introduces Evol-Instruct, another variant of Self-Instruct, that synthesizes iteratively more complex instruction to overcome shortcomings of previous methods. \n\\cite{mukherjee2023orca}, \\cite{mitra2023orcateaching} present a synthetic data generation approach to enhance task diversity and scalability, alongside a progressive training framework aimed at improving the model's reasoning ability and response style to match teacher models. This is achieved by generating rich reasoning signals in the generated answer and progressively training on datasets of varying difficulty in incremental phases. \n\nSimilar to LAB, concurrent work, GLAN \\citep{li2024synthetic}, employs a semi-automatic approach to synthetic data generation that uses a human-curated taxonomy to generate instruction tuning data from a teacher model. However, as explained in section \\ref{sec:know-sdg}, unlike LAB, GLAN cannot be used to generate synthetic data from domains that are not captured in the teacher model's support. As such, while LAB uses the open-source Mixtral model as the teacher, like many other synthetic data generation approaches, GLAN has to rely on a large proprietary model (GPT-4). This poses complicated questions about the usability of generated data (especially for commercial purposes) since the terms of use of proprietary models typically forbid using the model to improve other models.\n\n\\section{Methodology}\n\nLAB consists of two components: (i) a taxonomy to enable data curation (\\secref{sec:tax}) as well as, guide the synthetic data generator (\\secref{sec:sdg}) and (ii) a multi-phased instruction-tuning method with replay buffers to enable large-scale alignment-tuning. (\\secref{sec:train}).\n(i) serves the purpose of ensuring high diversity and quality in the synthetically generated instruction-tuning dataset while (ii) ensures training stability and prevents catastrophic forgetting. \n\\Figref{fig:overview} provides an overview of the end-to-end pipeline of applying the LAB method to align a pre-trained LLM.\n\\begin{figure}\n\\hspace*{-5.5em}\n    \\centering\n\\begin{tikzpicture}[level 1/.style={sibling distance=12em},\n                    level 2/.style={sibling distance=5em},\n                    level distance=3em]\n\\tikzstyle{ex} = [rectangle, draw, rounded corners, fill=blue!20]\n\\tikzstyle{data} = [cylinder, shape border rotate=90, aspect=0.05, draw, fill=gray!20]\n\\tikzstyle{doc} = [rectangle, draw, rounded corners, fill=gray!20]\n\n\\node {taxonomy root}\n    child {node {knowledge}\n      child {node {$\\dots$}}\n      child {node {textbook}\n        child [level distance=3em] {node (finance) {finance}\n          child {node [ex] (knowledge-ex-1) {example 1}}\n          child {node [ex] (knowledge-ex-2) {example 2}}\n          child {node [ex] (knowledge-ex-3) {example 3}}\n        }\n      }\n      child {node {$\\dots$}}\n    }\n    child {node [align=center] {foundational\\\\skills}\n      child {node {$\\dots$}}\n      child {node {mathematics}\n        child [level distance=2em] {node {arithmetic}\n          child {node {addition}\n            child [level distance=2.5em] {node [data] (cot-data) {math instruct data}}\n          }\n        }\n      }\n      child {node {$\\dots$}}\n    }\n    child {node [align=center] (c-skills) {compositional\\\\skills}\n      child {node {$\\dots$}}\n      child {node {writing}\n        child [level distance=2em] {node {email}\n          child {node {earnings report}\n            child {node [ex] (c-skills-ex-1) {example 1}}\n            child {node [ex] (c-skills-ex-2) {example 2}}\n            child {node [ex] (c-skills-ex-3) {example 3}}\n          }\n        }\n      }\n      child {node {$\\dots$}}\n    };\n\n\\node (document) [doc, left of=knowledge-ex-1, xshift=-2.5em] {document};\n\\node (anchor) [below of=cot-data, yshift=-1em] {};\n\\node (sdg-1) [shape=rectangle, draw, below of=knowledge-ex-2, yshift=-1em, text width=14em, text centered] {Synthetic Data Generator 1};\n\\node (sdg-2) [shape=rectangle, draw, below of=c-skills-ex-2, yshift=-1em, text width=14em, text centered] {Synthetic Data Generator 2};\n\n\\node [data, below of=sdg-1, yshift=-0.5em] (synth-art-history-data) {synthetic ``finance'' data};\n\\node [right of=synth-art-history-data, xshift=4.5em] {0.1--2k};\n\\node [data, below of=sdg-2, yshift=-0.5em] (synth-outline-data) {synthetic ``email'' data};\n\\node [right of=synth-outline-data, xshift=3.9em] {0.1--2k};\n\n\\node [below of=anchor, shape=rectangle, draw, text width=32em, text centered, yshift=-4em, minimum height=2em] (phased-training) {Phased training};\n\\node [left of=phased-training, xshift=-20em] (llm) {pre-trained LLM};\n\\draw[-] (llm) -- (phased-training);\n\n\\draw[-] (finance) -| (document);\n\\draw[->] (document) |- (sdg-1);\n\\draw[->] (knowledge-ex-1) -- (sdg-1);\n\\draw[->] (knowledge-ex-2) -- (sdg-1);\n\\draw[->] (knowledge-ex-3) -- (sdg-1);\n\\draw[->] (sdg-1) -- (synth-art-history-data);\n\\draw[->] (synth-art-history-data) -- (phased-training);\n\n\\draw[->] (cot-data) -- (phased-training);\n\n\\draw[->] (c-skills-ex-1) -- (sdg-2);\n\\draw[->] (c-skills-ex-2) -- (sdg-2);\n\\draw[->] (c-skills-ex-3) -- (sdg-2);\n\\draw[->] (sdg-2) -- (synth-outline-data);\n\\draw[->] (synth-outline-data) -- (phased-training);\n\n\\node (anchor-sec-1) [above right of=c-skills, xshift=5em, yshift=2em] {};\n\\node (anchor-sec-2) [below right of=c-skills-ex-2, xshift=5em] {};\n\\node (anchor-sec-3) [below of=anchor-sec-2, yshift=-4em] {};\n\\node (anchor-sec-4) [below of=anchor-sec-3, yshift=-1em] {};\n\n\\draw [decorate, decoration={brace, amplitude=5pt, mirror, raise=4ex}] (anchor-sec-2) -- (anchor-sec-1) node [midway,xshift=5em]{\\secref{sec:tax}};\n\n\\draw [decorate, decoration={brace, amplitude=5pt, mirror, raise=4ex}] (anchor-sec-3) -- (anchor-sec-2) node [midway,xshift=5em]{\\secref{sec:sdg}};\n\n\\draw [decorate, decoration={brace, amplitude=5pt, mirror, raise=4ex}] (anchor-sec-4) -- (anchor-sec-3) node [midway,xshift=5em]{\\secref{sec:train}};\n\n\\end{tikzpicture}\n    \\caption{Overview of the LAB alignment method. Starting from the taxonomy root, data are curated in each top-level groups and examples in the leaf nodes are used by the synthetic data generators to generate orders of magnitude data for the phased-training step for instruct-tuning.}\\label{fig:overview}\n\\end{figure}\n\n\\subsection{Taxonomy}\\label{sec:tax}\n\nTo enable the data curator or the model designer to organize the instruction-tuning training data, we define a taxonomy that hierarchically classifies the data samples into smaller task groups. At a high level, the taxonomy has three main branches: knowledge, foundational skills, and compositional skills. Each of these branches is further split into more granular levels where the tasks are defined in the leaf nodes and exemplified by providing manually written instruction-response pairs. This allows for easily identifying missing tasks in the target LLM and other tasks of interest and adding them to the training data pool. New tasks are added to the taxonomy by creating a leaf node under the appropriate branch and attaching 1--3 examples.\n\n\\paragraph{Knowledge}\nThe knowledge branch in the taxonomy is first divided based on document types like textbooks, technical manuals, etc., which are further divided into various domains like finance, statistics, etc.; see the sub-tree for knowledge in \\Figref{fig:overview} as an example. \nEach domain has a collection of documents and a sample set of domain-specific questions and answers.\nThis organization allows for better control over the licensing of text documents. As described in the next section, only the documents with permissible licenses are selected for synthetic data generation, excluding knowledge sources that lack proper licensing, reinforcing the integrity of our knowledge-generation processes. \n\n\\paragraph{Foundational skills}\nWe identify mathematics, coding, linguistic ability and reasoning as foundational skills that the model requires to prime itself for better knowledge acquisition and build further complex and compositional skills.\nTo teach the model foundational skills, we employ publicly available datasets \\citep{flan,mathins,conala,musique}; see the sub-tree for foundational skills in \\Figref{fig:overview} for an example.\n\n\\paragraph{Compositional skills}\nCompositional skills refer to the tasks that require a combination of knowledge and foundational skills, synergistically, to answer complex queries from users. For instance, the model's ability to write a company-wide email sharing insights about the company's performance last quarter and guidance for the upcoming year would require the model to understand the financial aspects of revenue, profit and loss, the skills of doing basic arithmetic and also have the skills to compose a formal email.\n\n\\subsection{Taxonomy-driven Synthetic Data Generator}\\label{sec:sdg}\n\nThe small number of manually curated data samples, embedded in the leaf nodes of the taxonomy, can be directly used for instruction tuning of the chatbot, however, the model may still perform poorly. Prior work \\citep{humpback} has shown that typically, a large amount of high-quality instruction data is required for improving instruction following performance of LLMs. It is possible to leverage existing SDGs like \\cite{wang2023selfinstructaligning, alpaca} to use the embedded examples and generate a lot more instruction data synthetically using teacher LLMs. But, such distillation-based SDGs tend to over-sample from the dominant modes of the teacher model and thus lack in diversity and quality of the generated data \\cite{gudibande2023false}. \nWe argue that this limitation is attributed to the random selection of examples from the pool of seed samples: with random selection, the examples used to prompt the teacher model at each time are an ``average'' of the seed pool i.e. they do not \\emph{focus} on any specific task. This lack of focus tends to encourage the teacher model to generate more synthetic data from its dominant modes and ignore the long tail of interesting tasks.\n\nTo address this issue, we replace the random sampling in existing SDGs with a taxonomy-driven approach to guide the sampling of synthetic data, enabling targeted coverage of the support of the teacher model distribution around the individual leaf nodes of the taxonomy.\n\\Figref{fig:intuition} illustrate the high-level idea behind this change.\n\\begin{figure}[h]\n\\begin{center}\n\\begin{subfigure}[b]{0.48\\textwidth}\n    \\centering\n     \\includegraphics[width=\\textwidth]{figs/intuition-input}\n     \\caption{Input distributions}\\label{fig:intuition-input}\n\\end{subfigure}\n\\begin{subfigure}[b]{0.48\\textwidth}\n    \\centering\n     \\includegraphics[width=\\textwidth]{figs/intuition-output}\n     \\caption{Output distributions}\\label{fig:intuition-output}\n\\end{subfigure}\n\\end{center}\n\\caption{Intuition of how taxonomy-driven sampling produces diverse set of synthetic data and hence improve the data used to train student model across the task domain. \\Figref{fig:intuition-input} shows how taxonomy-driven sampling leads to an input distribution with wide support and distinct modes while self-instruct gives an smooth input distribution. \\Figref{fig:intuition-output} shows the consequence using inputs in generating synthetic data: teacher model will focus its own dominant modes if the input is smooth but focus on each task better if the inputs are also concentrated on each task.}\\label{fig:intuition}\n\\end{figure}\n\\Figref{fig:intuition-input} shows the issue of randomly sampling in the \\emph{input} space of the teacher model (i.e.~prompts).\nGiven a set of seed examples (red), randomly sampling with more than one example gives an approximation to the average of the seed pool, leading to a smoothed distribution, e.g.~ self-instruct distribution (blue).\nWith the taxonomy-driven sampling, since only the examples within each of the leaf nodes are used when sampling for the corresponding tasks, \neach of the tasks are guaranteed to be well represented in the prompts (purple).\nSecond, when it comes to the \\emph{output} space, for a given teacher model (red), prompting it with random examples (i.e.~smoothened input distribution) tends to make it sampling from its own dominant mode (blue) while prompting it with focused examples in each leaf node (i.e.~input distribution with distinct modes), \nthe teacher model is guaranteed to generate synthetic data for each of the tasks (purple).\n\nWith the above insight, we now introduce two new synthetic data generation (SDG) methods in LAB that leverage the taxonomy to guide the data generation process. The first one is targeted for skills generation and uses the handful of task examples in the leaf nodes to generate a lot more using the open-source Mixtral-7x8B model.  The second one is targeted at knowledge generation. While it still uses the Mixtral-7x8B model, unlike prior works, it does not rely on the knowledge stored in the teacher model. \n\n\\begin{figure}[]\n    \\centering\n\\begin{tcolorbox}\n\\small\n\nYou are asked to come up with a set of \\{num\\_samples\\} diverse questions on \\{task\\}.\\\\\n\nPlease follow these guiding principles when generating responses:\\\\\n\n* Use proper grammar and punctuation.\\\\\n* Always generate safe and respectful content. Do not generate content that is harmful, abusive, or offensive.\\\\\n* Always generate content that is factually accurate and relevant to the prompt.\\\\\n* The questions should be clear and human-like.\\\\\n* The questions should be diverse and cover a wide range of topics.\\\\\n* The questions should not be template-based or generic, it should be very diverse.\\\\\n* Simply return the questions, do not return any answers or explanations.\\\\\n* Strictly adhere to the prompt and generate responses in the same style and format as the example.\\\\\n\nTo better assist you with this task, here is an example:\\\\\n\\#\\#\\# Question:\\\\\n1. \\{icl\\_question\\}\\\\\n\nNow generate \\{num\\_samples\\} such questions, remember to follow the principles mentioned above and use the same format as the examples. Remember to use the same style and format as the example above. Return your responses in the format of [\\#\\#\\# Question [question number]: [question]]\n\\end{tcolorbox}\n\\caption{Instruction Generator prompt template}\\label{fig:question-template}\n\\end{figure}\n\\subsubsection{Skill Generation}\nSkills-SDG uses four prompt templates, one for each of the four, below-mentioned, stages of data generation. Each template has its own set of principles and instructions that control the role of the teacher model (generator vs evaluator) and guide the generation/evaluation process.\n\n\\begin{enumerate}\n    \\item \\textbf{Instruction generation:} In the first stage, the teacher model acts as a question generator, using a specialized prompt (see \\Figref{fig:question-template} for an example) to leverage its knowledge and create diverse questions. By iterating through each leaf node of a taxonomy, the teacher generates queries that adhere to specific principles and thoroughly explore the targeted domain, enhancing the comprehensiveness of the generated content. \n    \\item \\textbf{Evaluating synthetic instruction:} In this stage, the teacher model assumes the role of an instruction evaluator, the teacher model uses targeted prompts to filter out questions that don't meet predefined principles, including relevance to the domain, potential harm, or questions beyond a language model's answering capabilities. This ensures that only high-quality, contextually appropriate questions move forward in the process.\n    \\item \\textbf{Generating responses:} The teacher model, functioning as a response generator in this stage, adopts dual personas for precision and creativity, guided by distinct prompts. This tailored approach helps to generate both, creative responses for domains like writing and role-play, and precise answers for STEM and data extraction, aligning the response style to human expectations through principles and seed examples in the leaf nodes.\n    \\item \\textbf{Evaluating the synthetic instruction-response pair:} The final stage involves a rigorous process to filter and select high-quality instruction and response pairs. Using a 3-point rating system (see \\Figref{fig:qaevaluation-template} for an example), the teacher model evaluates each sample, filtering out those that are incorrect, irrelevant, or deviate from the provided principles, ensuring the training dataset's quality and relevance are enhanced for the student model.\n\n\\end{enumerate}\n\n\\begin{figure}[]\n    \\centering\n\\begin{tcolorbox}\n\\small\n\nPlease act as an impartial judge and evaluate the quality of the answer provided by an AI assistant to the questions displayed below. Evaluate whether or not the answer is a good example of how AI Assistant should respond to the user's instruction. Please assign a score using the following 3-point scale:\\\\\n\n1: It means the answer is incorrect, irrelevant, unsafe or provides incomplete and garbage information. For instance, the answer may be factually wrong, off-topic, or filled with irrelevant content that doesn't address the user's question or it could be incomplete and hanging. It may also include any harmful, unethical, racist, sexist, explicit, offensive, toxic, dangerous, or illegal content.  \\\\\n\n2: It means the answer provides the correct answer, but it is brief and to the point without explanations. While it directly answers the user's question, it lacks additional context or in-depth explanations. \\\\\n\n3: It means the answer is a perfect answer from an AI Assistant. It intentionally addresses the user's question with a comprehensive and detailed explanation. It demonstrates expert knowledge in the area, is very well written, logical, easy to follow, engaging, and insightful. And the answer is safe and does not include any harmful content. \\\\\n\nBegin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must rate the answer on a scale of 1 to 3 as mentioned above. Please use the following examples as a reference for your evaluation.\n\n\\end{tcolorbox}\n\\caption{Instruction-response Evaluation template}\\label{fig:qaevaluation-template}\n\\end{figure}\n\n\\subsubsection{Knowledge-generation}\n\\label{sec:know-sdg}\nSynthetic data generators are inherently limited by the knowledge and capabilities of the teacher model. This is one of the main reasons why most successful SDG methods \\citep{xu2023wizardlmempowering, mukherjee2023orca, mitra2023orcateaching} depend on GPT-4 model, which presumably has the highest coverage of knowledge and skills. However, there are many domains that no open/proprietary model is trained on and hence cannot work as a teacher model using existing SDG methods. To address this limitation, in LAB we devised a new SDG pipeline for generating instruction data on domains that the teacher model has not been trained on. We call it knowledge-SDG.\n\nSimilar to the process of skills generation, knowledge-SDG uses the curator-provided examples embedded in the leaf nodes of the knowledge branch of the taxonomy. But additionally, the teacher model is provided a knowledge source in the form of documents, manuals, and books on the target subject to ground the generated instruction data into a reliable source thus avoiding dependence on the internal knowledge base of a teacher model, which may struggle with specialized domains and could lead to inaccuracies or hallucinations especially on highly specialized, technical domains. \n\nTo ensure that the generated answers remain faithful to the content of the source material, similar to the skills-SDG, teacher model is repurposed as an evaluator that validates the generated responses are grounded and faithful to the source documents.\n\n\\subsection{Multi-Phase Training}\\label{sec:train}\n\nLAB training happens in two phases, knowledge tuning, followed by skills tuning. \n\nIn the knowledge-tuning phase, the model is trained on samples from the knowledge and foundational skills branches of the taxonomy. This phase in-turn, is carried out in two steps. We split the data under the knowledge and foundational skills branches into two buckets based on the response length. Then we first train the model on the samples with short responses before moving on to training on samples with long responses. Similar to prior work \\citep{mitra2023orcateaching}, our empirical results also suggest that this two-step approach to knowledge-tuning improves model performance.\n\nPost-knowledge tuning, we start the skills-tuning phase where the best model checkpoint from the knowledge-tuning phase is trained on the compositional skills branch of the taxonomy. To address the challenge of catastrophic forgetting when training in two distinct phases, a replay buffer of the data from the knowledge-tuning phase in employed. Our empirical findings indicate that starting with knowledge and foundational skills training, before progressing to compositional skills leads to significantly better benchmark performance. \n\nFor selecting the best model checkpoint during intermediate phases, we rely on the MMLU benchmark \\citep{hendrycks2020measuring} during the knowledge-tuning phase and the MT-bench \\citep{zheng2024judging} during the skills-tuning phase. Please refer to \\tabref{tab:phased-training} for an overview of our training phases.\n\n\\paragraph{Training Details}\nIn our training process, we consciously avoid overtraining. Despite the possibility of achieving higher scores on intermediate benchmarks, we have found that selecting checkpoints from earlier stages of training results in more reliable and generalizable model performance. We employ small learning rates with an extended warm-up period, specifically $2 \\times 10^{-5}$ for Llama-based models and $1 \\times 10^{-6}$ for Mistral-based models, each beginning with a linear warm-up. This strategy is hypothesized to aid the model in transitioning from broad dataset-wide learning to more focused, task-specific adjustments. Additionally, we utilize a large effective batch size of 3840, achieved through gradient accumulation, to enhance stability across the diverse range of tasks being learned concurrently. Our findings suggest that using cosine decay on learning rates during intermediate phases can destabilize subsequent training stages, likely due to the learning rate's reduction to near zero, narrowing the loss landscape and complicating the integration of new phase gradients. Refer to \\tabref{tab:hps} for an overview of our training hyper-parameters.\n\n\\begin{table}[]\n\\centering\n\\begin{tabular}{lllll}\n\\toprule\nPhase            & Step & Training data                                                                  & Replay buffer &  \\\\\n\\hline\n\\multirow{2}{*}{Knowledge Tuning} & 1    & Knowledge (short)                                                               &             N/A  &  \\\\\\cline{2-5}\n & 2    & \\makecell[l]{Knowledge (long)\\\\Foundational skills} &       KT/1 data       &  \\\\\\hline\nSkill Tuning     &  N/A    & Compositional skills   & KT/1 \\& KT/2 data              & \\\\ \\bottomrule\n\\end{tabular}\n\\caption{Data and reply buffers used in phase-training.}\\label{tab:phased-training}\n\\end{table}\n\n\\section{Results}\n\nIn this study, we implemented the LAB method on two distinct open models, \\textsc{Llama-2-13b} \\citep{touvron2023llama}and \\textsc{Mistral-7B} \\citep{jiang2023mistral7b}, utilizing \\textsc{Mixtral-8x7B-Instruct-v0.1} \\citep{jiang2024mixtralexperts} as the teacher model. This approach yielded two LAB-aligned models: \\textsc{\\labl} and \\textsc{\\labm}.\n\nDuring the synthetic data generation phase, we employed a taxonomy consisting of numerous leaf nodes to produce a dataset comprising 1.2 million samples, divided almost evenly between knowledge-based (617k) and skill-based (588k) samples. The specific training hyper-parameters employed during this study are summarized in \\tabref{tab:hps}.\n\n\\begin{table}[t]\n    \\setlength{\\tabcolsep}{2pt}\n    \\centering\n    \\begin{small}\\begin{sc}\n    \\begin{tabular}{ll|ccccccc}\n    \\toprule\n    Model   & Phase/step          & \\makecell{Learning\\\\rate} & \\makecell{Batch\\\\size} & \\makecell{Context\\\\length} & \\#samples & \\#warm-up   &  \\#epochs    \\\\ \\hline\n    \\multirow{3}{*}{\\labl}  & KT/1 & \\multirow{3}{*}{2e-5} & \\multirow{3}{*}{3840} & \\multirow{2}{*}{2048} & 630k & \\multirow{3}{*}{385}&  5\\\\ \n                            & KT/2 &                       &                       &                       & 230k &                   & 7\\\\ \n                            & ST   &                       &                       & 4096                  & 550k &                   & 7\\\\ \\hline\n    \\multirow{3}{*}{\\labm} & KT/1  & \\multirow{3}{*}{1e-6} & \\multirow{3}{*}{3840} & \\multirow{2}{*}{2048} & 630k & \\multirow{3}{*}{800}& 4\\\\ \n                           & KT/2  &                       &                       &                       & 230k &                   & 4\\\\ \n                           & ST    &                       &                       & 4096                  & 550k &                   & 7\\\\ \n    \\bottomrule\n    \\end{tabular}\n    \\end{sc}\\end{small}\n    \\caption{Hyper-parameters used in training for \\textsc{\\labl} and \\textsc{\\labm}.}\\label{tab:hps}\n    \\setlength{\\tabcolsep}{6pt}\n\\end{table}\n\nWe compare the performance of \\textsc{\\labl} and \\textsc{\\labm} against other models that use the same base models for alignment, which include\n\\paragraph{\\textsc{Llama-2-13b}}\n\\begin{itemize}\n    \\item \\textsc{Llama-2-13b-chat} \\citep{touvron2023llama}: RLHF with human annotators by the same team that develops \\textsc{Llama-2-13b}\n    \\item \\textsc{Orca-2} \\citep{mitra2023orcateaching}: \n    \\item \\textsc{WizardLM-13B-V1.2} \\citep{xu2023wizardlmempowering}: model with the highest MT-Bench amongs those use \\textsc{Llama-2-13b} as the base model on LMSYS Chatbot Arena Leaderboard \\citep{mtbench}.\n\\end{itemize}\n\\paragraph{\\textsc{Mistral-7B}}\n\\begin{itemize}\n    \\item \\textsc{Mistral-7B-Instruct-v0.2} \\citep{jiang2023mistral7b}: instruction-tuning using supervised fine-tuning (SFT) on publicly available conversation datasets by the same team that develops \\textsc{Mistral-7B}\n    \\item \\textsc{Zephyr-7b-beta} \\citep{tunstall2023zephyrdirect}: model with the highest MT-Bench amongs those use \\textsc{Mistral-7B} as the base model on LMSYS Chatbot Arena Leaderboard \\citep{mtbench}. \n\\end{itemize}\nTo compare the aligned LLMs, we consider the following evaluation metrics with the settings consistent with those used by LMSYS Chatbot Arena Leaderboard \\citep{mtbench}\n\\begin{itemize}\n    \\item MT-Bench \\citep{mtbench}: 1-turn and 2-turn average\n    \\item MMLU \\citep{mmlu}: 5-shot\n    \\item ARC \\citep{clark2018thinkyou}: 25-shot\n    \\item HellaSwag \\citep{zellers2019hellaswagcan}: 10-shot\n    \\item Winogrande \\citep{sakaguchi2019winograndeadversarial}: 5-shot\n    \\item GSM8k \\citep{cobbe2021trainingverifiers}: 5-shot strict \n\\end{itemize}\nAll results are reported in \\tabref{tab:res}.\n\\begin{table}[t]\n\\hspace*{-5em}\n    \\setlength{\\tabcolsep}{2pt}\n    \\centering\n    \\begin{small}\\begin{sc}\n    \\begin{tabular}{lll|cccccccc}\n    \\toprule\n    Model             & Alignment & Teacher           & \\makecell{MT-Bench} & \\makecell{MMLU}   & \\makecell{ARC}   & \\makecell{HellaSwag} & \\makecell{Winogrande} & \\makecell{GSM8K} \\\\ \\hline\n    Llama-2-13b-chat  & SFT + RLHF & \\makecell[l]{Human\\\\annotators}                & 6.65$^\\dagger$                & 54.58 & 59.81 & 82.52     & 75.93      & 34.80 \\\\ \n    Orca-2            & \\makecell[l]{Progressive\\\\Training} & GPT-4 & 6.15$^\\dagger$          &        60.37 & 59.73 & 79.86     & 78.22      & \\textbf{48.22} \\\\ \n    WizardLM-13B & \\makecell[l]{Evol-\\\\Instruct}  & GPT-4        & 7.20$^\\dagger$           &        54.83  & 60.24 & 82.62     & 76.40       & 43.75  \\\\ \\hline\n    \\labl       & LAB & \\makecell[l]{Mixtral-8x7B-\\\\Instruct}                  & 7.23$^\\ddagger$ &        58.89 & 61.69 & 83.15 & \\textbf{79.56} & 40.11 \\\\ \n    \\hline\n    \\hline\n    Mistral-7B-Instruct      &  SFT  & \\makecell[l]{Public\\\\Datasets} & 6.84$^\\dagger$ & 60.37 & 63.65 & \\textbf{84.76} & 76.80 & 41.85 \\\\ \n    Zephyr-7b-$\\beta$      & SFT + DPO    & GPT-4 & 7.34$^\\dagger$ &    61.07     & 63.74 & 84.19 &  78.06 & 34.04 \\\\ \n    \\hline\n    \\labm      & LAB   & \\makecell[l]{Mixtral-8x7B-\\\\Instruct} & \\textbf{{7.66}}$^\\ddagger$ & \\textbf{64.88}  & \\textbf{63.99}     & 84.37 & 78.24 & 44.58 \\\\ \n    \\bottomrule\n    \\end{tabular}\n    \\end{sc}\\end{small}\n    $^\\dagger$ taken from the LMSYS Chatbot Arena Leaderboard.\\\\\n    $\\ddagger$ average of 3 runs. \\\\\n    \\caption{Evaluation of LLMs with different alignment methods over a comprehensive set of benchmark metrics. Settings of each metric can be found in the main text.}\\label{tab:res}\n    \\setlength{\\tabcolsep}{6pt}\n\\end{table}\n\nNotably, in terms of MT-Bench, \\textsc{\\labl} performs better than the current best model fine-tuned on \\textsc{Llama-2-13b} and \\textsc{\\labm} performs better than the current best model fine-tuned on \\textsc{Mistral-7B}, achieving state-of-the-art performance in term of chatbot capability.\nImportantly, out training method ensures that the model is not only good at multi-turn conversation but also maintains its knowledge or reasoning capability, as shown by the overall superior performance in the rest of the metrics.\nBesides, unlike those top models that use GPT-4 as the teacher model, we achieve this performance using the open-weights \\textsc{\nMixtral-8x7B-Instruct-v0.1}, which is relatively weaker teacher model at orders of magnitude less cost.\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2310.07849v2.tex",
        "arXiv-2311.00287v2.tex",
        "arXiv-2403.01081v3.tex"
    ],
    "group_id": "group_68",
    "response": "### Summary of Research Papers on Synthetic Data Generation with Large Language Models\n\n#### Title: Leveraging Large Language Models for Synthetic Data Generation in Text Classification and Clinical NLP Tasks\n\n#### Introduction\nThe field of natural language processing (NLP) has seen significant advancements with the advent of large language models (LLMs), which have facilitated the generation of synthetic data for training purposes. This synthetic data generation process aims to address the challenges of collecting and curating high-quality training datasets, which are often costly and time-consuming. The use of LLMs to generate synthetic data has been explored in various contexts, including text classification and clinical NLP tasks. However, the effectiveness of LLM-generated synthetic data varies widely across different tasks and instances, raising questions about the factors that influence its utility. This summary delves into three research papers that investigate the potential and limitations of LLMs in generating synthetic data for text classification and clinical NLP tasks, providing insights into the subjectivity of tasks and the integration of domain-specific knowledge.\n\nHistorically, the development of NLP models has relied heavily on the availability of large, annotated datasets. With the rise of LLMs, researchers have started to explore the use of these models to generate synthetic data, which can then be used to train downstream models. This approach has shown promise in augmenting training data, especially in low-resource settings, but the effectiveness of the synthetic data remains a subject of debate. The first paper, \"Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations,\" examines the impact of task subjectivity on the performance of models trained on synthetic data. The second paper, \"Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models,\" introduces a method to enhance the quality of synthetic clinical data by integrating clinical knowledge from knowledge graphs (KGs) and LLMs. The third paper, \"LAB: Large-Scale Alignment for ChatBots,\" proposes a taxonomy-driven approach to generate synthetic data for instruction tuning, aiming to overcome the scalability challenges in the alignment phase of LLM training.\n\n#### Main Content of Each Paper\n\n**Paper 1: Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations**\n\nThis paper investigates the effectiveness of LLM-generated synthetic data in training text classification models across different levels of task subjectivity. The authors use two approaches for synthetic data generation: zero-shot and few-shot settings. In the zero-shot setting, the LLM is prompted to generate data without any real-world examples, while in the few-shot setting, a small number of real-world examples are provided to guide the LLM's generation process. The study evaluates the performance of models trained on synthetic data generated under both settings, comparing them with models trained on real-world data. The authors find that models trained on synthetic data generally perform worse than those trained on real-world data, particularly for tasks with high subjectivity. However, the few-shot setting shows promise in improving the performance of models trained on synthetic data. The paper also explores the diversity of synthetic data, noting that LLMs may struggle to generate data that fully reflects the nuances of human language, especially in tasks requiring a deep understanding of contextual subtleties.\n\n**Paper 2: Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models**\n\nThe authors of this paper propose a framework called \\ours, which integrates clinical knowledge from KGs and LLMs to generate high-quality synthetic data for clinical NLP tasks. The framework aims to address the limitations of existing synthetic data generation methods, which often suffer from distribution shifts and limited diversity. \\ours uses a two-step process: clinical knowledge extraction and context-informed LLM prompting. The knowledge extraction step involves generating clinical topics and writing styles from KGs and LLMs, while the context-informed prompting step utilizes these topics and styles to guide the LLM in generating diverse and contextually accurate synthetic data. The paper evaluates the performance of models trained on synthetic data generated by \\ours across 18 clinical datasets and finds that \\ours consistently outperforms existing methods, with an average performance gain of 7.7\\%-8.7\\%. The authors also conduct a human evaluation to ensure the factual accuracy of the generated data, confirming that \\ours does not suffer from the issue of misinformation.\n\n**Paper 3: LAB: Large-Scale Alignment for ChatBots**\n\nThis paper introduces LAB (Large-scale Alignment for chatBots), a method designed to enhance the instruction-following capabilities of LLMs through a taxonomy-guided synthetic data generation process. LAB leverages a hierarchical taxonomy to organize instruction-tuning data into smaller task groups, ensuring high diversity and quality in the generated synthetic data. The method consists of two phases: knowledge tuning and skills tuning. During the knowledge tuning phase, the model is trained on samples from the knowledge and foundational skills branches of the taxonomy, while during the skills tuning phase, it is trained on samples from the compositional skills branch. The authors demonstrate that LAB-trained models can achieve competitive performance on several benchmarks, including MT-Bench and MMLU, using the open-source Mixtral model as the teacher. LAB is designed to prevent catastrophic forgetting by employing a replay buffer of data from previous phases, ensuring that the model retains its knowledge and skills as it progresses through the training process.\n\n#### Commonalities and Innovations\n\nAll three papers explore the use of LLMs to generate synthetic data for training purposes. However, they differ in their approaches and target applications. Paper 1 focuses on text classification tasks and evaluates the impact of task subjectivity on the performance of models trained on synthetic data. Paper 2 targets clinical NLP tasks and introduces \\ours, a framework that integrates clinical knowledge from KGs and LLMs to generate high-quality synthetic data. Paper 3 aims to enhance the instruction-following capabilities of LLMs through a taxonomy-guided synthetic data generation process, demonstrating the scalability and cost-effectiveness of LAB.\n\nThe common theme across these papers is the recognition of the limitations of existing synthetic data generation methods and the introduction of new strategies to improve the quality and diversity of the generated data. Paper 1 highlights the challenges of generating synthetic data for highly subjective tasks, while Paper 2 addresses the issue of distribution shifts and limited diversity in clinical NLP tasks. Paper 3 introduces a multi-phase training framework and a taxonomy-driven synthetic data generation process to enhance the instruction-following capabilities of LLMs.\n\n#### Comparison of Results and Discussion of Differences\n\nThe results from Paper 1 indicate that models trained on synthetic data generated under the zero-shot setting generally perform worse than those trained on real-world data, especially for tasks with high subjectivity. However, the few-shot setting shows promise in improving the performance of models trained on synthetic data. The authors observe a strong monotonically increasing relationship between the instance-level annotation agreement and the model accuracy, suggesting that models trained on synthetic data perform better on instances with lower subjectivity.\n\nPaper 2 reports consistent performance gains across various clinical NLP tasks when using \\ours to generate synthetic data. The authors find that \\ours not only aligns more closely with the distribution of real datasets but also enhances the diversity of generated training instances. The empirical performance gains are consistent across different LLMs and classifiers, with an average improvement of 7.7\\%-8.7\\%. The authors also conduct a human evaluation to ensure the factual accuracy of the generated data, confirming that \\ours does not suffer from the issue of misinformation.\n\nPaper 3 demonstrates that LAB-trained models can achieve competitive performance on several benchmarks, including MT-Bench and MMLU, using the open-source Mixtral model as the teacher. The authors find that LAB-trained models perform better than models fine-tuned using proprietary teacher models like GPT-4, achieving state-of-the-art performance in terms of chatbot capability. LAB is designed to prevent catastrophic forgetting by employing a replay buffer of data from previous phases, ensuring that the model retains its knowledge and skills as it progresses through the training process.\n\n#### Conclusion: Main Findings and Future Research Directions\n\nThe main findings from these papers suggest that LLMs can generate synthetic data that supports model training, but the effectiveness of this data varies significantly across different tasks and instances. Paper 1 highlights the challenges of generating synthetic data for highly subjective tasks and suggests that incorporating real-world data examples can improve the performance of models trained on synthetic data. Paper 2 introduces \\ours, a framework that integrates clinical knowledge from KGs and LLMs to generate high-quality synthetic data for clinical NLP tasks, demonstrating consistent performance gains across various tasks. Paper 3 proposes LAB, a taxonomy-driven synthetic data generation process that enhances the instruction-following capabilities of LLMs, achieving competitive performance on several benchmarks using the open-source Mixtral model as the teacher.\n\nFuture research directions could include exploring the impact of other factors, such as language formality and the requirement for domain-specific knowledge, on the effectiveness of LLM-generated synthetic data. Additionally, further studies could investigate the potential of using the performance of models trained on synthetic data as a proxy for approximating task or instance subjectivity and estimating the reliability of gold labels. The integration of human intelligence, such as feedback or direct intervention in the generation process, could also be explored to enrich the diversity of synthetic data and identify the most \"informative\" type of data instance to generate. Finally, the scalability and cost-effectiveness of LAB could be further evaluated in different NLP applications, and the potential of using LAB in other domains, such as EHR data generation, could be explored.\n\n#### Tables for Comparison\n\n**Table 1: Comparative Performance of Models Trained on Synthetic Data vs Real-World Data (Paper 1)**\n| Dataset | Subjectivity | BERT (Real-world) | BERT (Zero-shot) | BERT (Few-shot) | RoBERTa (Real-world) | RoBERTa (Zero-shot) | RoBERTa (Few-shot) |\n|---------|--------------|-------------------|------------------|-----------------|---------------------|--------------------|-------------------|\n| AG       | Low          | 95.3%             | 89.3%            | 91.5%           | 94.6%              | 88.6%              | 92.9%            |\n| Relation | Low          | 98.6%             | 92.4%            | 96.4%           | 97.0%              | 91.4%              | 94.1%            |\n| IMDB     | Medium       | 87.6%             | 81.2%            | 81.1%           | 89.0%              | 81.2%              | 82.4%            |\n| SMS Spam | Medium       | 97.2%             | 93.8%            | 94.3%           | 97.3%              | 93.5%              | 94.0%            |\n| Reddit Emotion | High | 93.7%             | 72.7%            | 81.9%           | 91.3%              | 77.9%              | 87.5%            |\n| Tweet Irony | High | 72.2%             | 63.4%            | 81.5%           | 74.0%              | 57.8%              | 83.3%            |\n| Tweet Emotions | High | 77.7%             | 58.1%            | 64.6%           | 75.8%              | 64.6%              | 72.7%            |\n| Sarcasm | High | 89.9%             | 51.1%            | 63.6%           | 91.8%              | 54.3%              | 64.8%            |\n| Financial Phrasebank | High | 83.2%             | 48.2%            | 70.6%           | 85.0%              | 58.5%              | 75.0%            |\n| Humor Speech | High | 97.0%             | 56.0%            | 86.9%           | 96.7%              | 54.9%              | 84.0%            |\n\n**Table 2: Performance of Models Trained on Synthetic Data vs Baselines (Paper 2)**\n| Task | PubMedBERT$_{\\texttt{Base}}$ | PubMedBERT$_{\\texttt{Large}}$ |\n|------|-----------------------------|------------------------------|\n| Text Class | 59.02 (ZeroGen) | 61.51 (ZeroGen) |\n| RE     | 63.84 (ZeroGen) | 65.18 (ZeroGen) |\n| NLI    | 55.96 (ZeroGen) | 63.47 (ZeroGen) |\n| Fact Verification | 35.30 (ZeroGen) | 41.12 (ZeroGen) |\n| STS    | 72.69 (ZeroGen) | 72.69 (ZeroGen) |\n| QA     | 66.02 (ZeroGen) | 66.33 (ZeroGen) |\n| NER    | 57.79 (ZeroGen) | 57.31 (ZeroGen) |\n| MedAttr | 54.04 (ZeroGen) | 56.29 (ZeroGen) |\n| \\ours w/ KG | 67.15 (PubMedBERT$_{\\texttt{Base}}$) | 66.76 (PubMedBERT$_{\\texttt{Base}}$) |\n| \\ours w/ LLM | 70.06 (PubMedBERT$_{\\texttt{Base}}$) | 72.81 (PubMedBERT$_{\\texttt{Base}}$) |\n| Performance Gain | 4.08\\% (PubMedBERT$_{\\texttt{Base}}$) | 4.00\\% (PubMedBERT$_{\\texttt{Large}}$) |\n\n**Table 3: Performance of LAB-Trained Models vs Other Models (Paper 3)**\n| Model | Alignment | Teacher | MT-Bench | MMLU | ARC | HellaSwag | Winogrande | GSM8K |\n|-------|-----------|---------|----------|------|------|-----------|-----------|-------|\n| Llama-2-13b-chat | SFT + RLHF | Human annotators | 6.65 | 54.58 | 59.81 | 82.52 | 75.93 | 34.80 |\n| Orca-2 | Progressive Training | GPT-4 | 6.15 | 60.37 | 59.73 | 79.86 | 78.22 | 48.22 |\n| WizardLM-13B | Evol-Instruct | GPT-4 | 7.20 | 54.83 | 60.24 | 82.62 | 76.40 | 43.75 |\n| \\labl | LAB | Mixtral-8x7B-Instruct | 7.23 | 58.89 | 61.69 | 83.15 | 79.56 | 40.11 |\n| Mistral-7B-Instruct | SFT | Public Datasets | 6.84 | 60.37 | 63.65 | 84.76 | 76.80 | 41.85 |\n| Zephyr-7b-$\\beta$ | SFT + DPO | GPT-4 | 7.34 | 61.07 | 63.74 | 84.19 | 78.06 | 34.04 |\n| \\labm | LAB | Mixtral-8x7B-Instruct | 7.66 | 64.88 | 63.99 | 84.37 | 78.24 | 44.58 |\n\n#### Acknowledgements\nThe authors of these papers acknowledge the contributions of various research groups and organizations, such as Amazon Mechanical Turk, Huggingface, and Microsoft Azure OpenAI. They also thank the National Institutes of Health, the National Science Foundation, and Microsoft for providing research credits and support.\n\n#### Limitations\nThe limitations of these studies include the reliance on specific LLMs and the potential for hallucinations or misinformation in the generated data. Additionally, the scalability and cost-effectiveness of the proposed methods are evaluated primarily in the context of text classification and clinical NLP tasks, and their applicability to other domains remains to be explored."
}