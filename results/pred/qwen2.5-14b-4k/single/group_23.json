{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{\\name: LLM-powered Task Automation in Android}\n\n\\begin{document}\n\n\\title{\\name: LLM-powered Task Automation in Android}\n\n\\begin{abstract}\nMobile task automation is an attractive technique that aims to enable voice-based hands-free user interaction with smartphones.\nHowever, existing approaches suffer from poor scalability due to the limited language understanding ability and the non-trivial manual efforts required from developers or end-users.\nThe recent advance of large language models (LLMs) in language understanding and reasoning inspires us to rethink the problem from a model-centric perspective, where task preparation, comprehension, and execution are handled by a unified language model. \nIn this work, we introduce \\name, a mobile task automation system \\shpd{capable of handling} arbitrary tasks on any Android application without manual efforts. \nThe key insight is to combine the commonsense knowledge of LLMs and domain-specific knowledge of apps through automated dynamic analysis.\nThe main components include a functionality-aware UI representation method that bridges the UI with the LLM, exploration-based memory injection techniques that augment the app-specific domain knowledge of LLM, and a multi-granularity query optimization module that reduces the cost of model inference. \nWe integrate \\name with off-the-shelf LLMs including online GPT-4/GPT-3.5 and on-device Vicuna, and evaluate its performance on a new benchmark for memory-augmented Android task automation with 158 common tasks. The results demonstrated that \\name is able to precisely generate actions with an accuracy of 90.9\\%, and complete tasks with a success rate of 71.3\\%, outperforming the GPT-4-powered baselines by 36.4\\% and 39.7\\%. %The demo, benchmark suites, and source code of \\name will be released at \\url{https://autodroid-sys.github.io/}.\n\\end{abstract}\n\n\\begin{CCSXML}\n<ccs2012>\n   <concept>\n       <concept_id>10003120.10003138</concept_id>\n       <concept_desc>Human-centered computing~Ubiquitous and mobile computing</concept_desc>\n       <concept_significance>500</concept_significance>\n       </concept>\n   <concept>\n       <concept_id>10010147.10010178</concept_id>\n       <concept_desc>Computing methodologies~Artificial intelligence</concept_desc>\n       <concept_significance>300</concept_significance>\n       </concept>\n </ccs2012>\n\\end{CCSXML}\n\n\\ccsdesc[500]{Human-centered computing~Ubiquitous and mobile computing}\n\\ccsdesc[300]{Computing methodologies~Artificial intelligence}\n\n\\keywords{Task Automation, Large Language Models, App Analysis}\n\n\\maketitle\n\n\\section{Introduction}\n\nSmartphone is one of the most sophisticated devices for individuals. With millions of mobile applications (apps for short) that have access to various embedded sensors and rich personal data, smartphones \\shpd{can be used} for a lot of daily tasks such as ordering food, managing social networks, sensing and tracking health conditions, etc. \nTherefore, how to intelligently automate tasks on smartphones has become an attractive topic for mobile developers and researchers, due to its potential to significantly improve user experience and enable helpful virtual personal assistants.\n\nThe major approaches to mobile task automation can be classified as developer-based, demonstration-based, and learning-based techniques.\nMost existing commercial products (\\eg Siri, Google Assistant, Cortana, etc.) take a developer-based approach, which requires significant development efforts to \\shpd{support} a new task.\nFor example, to enable an automated task with Google Assistant,\napp developers need to identify the functionality which they want to trigger, configure and implement the corresponding intent, and register the intent \\shpd{with} the assistant. When executing a task, the assistant uses natural language understanding (NLU) modules to map the user command to the intent, extract the intent parameters, and invoke the corresponding developer-defined function.\nResearchers have explored various methods to ease the development efforts. However, these methods still suffer from poor scalability, since they either require ad-hoc and/or large-scale human demonstrations of tasks (\\eg programming-by-demonstration approaches \\cite{kite, ulink,li2017programming_iot} and supervised learning approaches \\cite{seq2act, metagui, motif}) or require defining a clear reward for task completion (\\eg reinforcement learning approaches \\cite{ToyamaEtAl2021AndroidEnv, rl_use_computer, glider}).\nDue to the lack of scalability, there are few automated tasks supported today, even in the most popular apps.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.47\\textwidth]{figure/intro.pdf}\n    \\caption{An illustration of LLM-powered mobile task automation. \n    The agent interacts with the smartphone GUI to complete an arbitrary task, with the guidance of LLM and app domain knowledge.}% \\yuanchun{where is the action for Aug 17?}}\n    \\label{fig:intro}\n    \\vspace{-0.5cm}\n\\end{figure}\n\nRecently, the emergence of large language models (LLMs) like ChatGPT \\cite{chatgpt} and Claude \\cite{claude} shows the promise \\shpd{in solving} the scalability issue of task automation.\nCompared to traditional models, LLMs demonstrate unique abilities such as instruction following \\cite{alpaca}, step-by-step reasoning \\cite{cot}, and zero-shot generalization \\cite{zero-shot-cot}.\nSuch abilities are enabled by self-supervised learning on a huge corpus (more than 1.4 trillion tokens \\cite{touvron2023llama}) followed by tuning with human feedback.\nWith these capabilities, researchers have managed to let LLMs invoke tools automatically, such as search engines \\cite{webgpt}, code interpreters \\cite{copilot}, and third-party APIs \\cite{gorilla, chamelon}. \nSimilarly, using LLMs can potentially avoid the cumbersome manual efforts in mobile task automation. Meanwhile, connecting LLMs to smartphones can further unleash the power of LLMs in personal domains.\n\nThe goal of LLM-powered mobile task automation is to build an autonomous agent that can complete user-specified tasks by interacting with the smartphone. \\wh{Although existing research \\cite{talking_with_ui} attempts to \\shpd{enable} LLMs to understand mobile UIs, it simply relies on prompt engineering and \\shpd{does} not utilize app-specific domain knowledge. \\name combines the capabilities of LLM and the app-specific knowledge through dynamic app analysis, which enables it to handle arbitrary unseen tasks without manual efforts (illustrated in Figure~\\ref{fig:intro}). } %By combining the public-domain knowledge from the LLM and domain-specific knowledge from the apps, \nWe identify three key problems to achieve this goal.\n\\begin{enumerate}\n    \\item \\textbf{GUI Representation.} The input and output of task automators are graphical user interface (GUI) states and actions, unlike the natural language sentences LLMs can handle.\n    To help LLMs better understand the GUI information and make precise interaction decisions, the GUI states and actions must be converted to text format while incorporating rich structured information.\n    \\item \\textbf{Knowledge Integration.} Solving tasks with LLMs requires domain-specific knowledge about the applications.\n    Unlike other tools studied in prior work (\\eg APIs) that LLMs can be easily configured to use, a smartphone app is usually a more complicated automata. LLMs need to navigate between different states to figure out how to complete the tasks.\n    \\item \\textbf{Cost Optimization.} Querying LLMs is costly and compute-intensive, while completing a task with LLMs may involve many lengthy queries due to complexity of tasks and smartphone apps. Thus, it is desirable to optimize the efficiency of LLM queries to facilitate responsive task automation experience.\n\\end{enumerate}\n\nWe introduce a mobile task automation framework, \\name, to \\shpd{address} the above problems.\nOverall, \\name executes tasks by prompting the LLMs with an HTML-style text representation of GUI and querying for action guidance. To augment the LLMs with app knowledge, \\name randomly explores the target apps and \\shpd{extracts} UI transition graphs from them. By analyzing the UI states and transitions with LLMs, \\name can \\shpd{convert} the raw information to task completion knowledge, which is then integrated into the task automator by injecting foreseen functionalities into the prompts, matching relevant UI traces, or tuning the LLM parameters.\nThe cost of querying LLMs is reduced by reducing and simplifying the queries based on app knowledge.\n\nTo systematically study the performance and challenges of LLM-powered task automation on Android, we build a benchmark with 158 manually labeled tasks from 13 open-source common mobile apps (Calendar, Messenger, Contacts, etc.).\nThe source code and executable environments of the apps are provided for obtaining auxiliary information and reproducing task executions.\nThe tasks include frequently asked how-to questions from the PixelHelp \\cite{seq2act} dataset and common functionalities in the apps.\nFor each task, we manually labeled the steps to complete the tasks, where each step is associated with both the GUI state and the GUI action.\nOur benchmark evaluates the performance of LLM-powered task automation in terms of accuracy and cost.\n\nWe evaluate the effectiveness of our \\name approach on the benchmark with different types of LLMs, including state-of-the-art online LLM services (GPT-3.5 and GPT-4) and open-source on-device LLMs (Vicuna). \nThe results have demonstrated that \\name can complete unseen tasks with a success rate of 71.3\\% with GPT-4, in which each action is selected with an accuracy of 90.9\\%.\nAs compared with the baselines powered by off-the-shelf LLMs, the task completion rates are improved by 36.4\\% to 39.7\\%, and the average cost of querying LLMs is reduced by 51.7\\%. %to \\xx\\%.\n\nOur work makes the following technical contributions:\n\\begin{enumerate}\n    \\item To the best of our knowledge, this is the first work on enhancing mobile task automation by combining LLMs and app-specific knowledge. We build a benchmark for this problem.\n    \\item We introduce a novel UI representation method that connects smartphones with LLMs, a task synthesis method \\shpd{for augmenting} LLMs with app knowledge, and various LLM query optimization techniques to reduce the cost of task automation.\n    \\item Through a comprehensive evaluation, we demonstrate the effectiveness of our \\shpd{approach} and the potential to advance the field of mobile task automation.\n\\end{enumerate}\n\n\\section{Background and Motivation}\n\\label{section:background}\n\n\\subsection{Mobile Task Automation}\n\nThe goal of mobile task automation is to automatically complete different kinds of tasks given by users. Its input is an arbitrary task described with natural language and a mobile app to execute the task. The output is a sequence of UI actions that can be executed on a smartphone. \n\nA \\textbf{task} is a multi-step functionality request from the user intended for completion on a smartphone, often lacking explicit instructions. \nA \\textbf{UI state}, visible to users on their mobile device, is an arrangement of controls depicted through images and text, typically organized as a GUI tree. %refers to the screen state that users see when using a mobile device. It consists of a series of controls composed of images and text and is typically represented as a \\textit{GUI tree}. \nA \\textbf{UI action}, performable by the user or an agent on the device's screen, is defined by a tuple \\textit{(target element, action type, value)}. \\textit{Target element} refers to a control in the UI state, such as a button, text box, input field, or slider. \\textit{Action type} represents how the target element is manipulated. We consider three main types of smartphone interactions, including ``click'', ``input'', and ``swipe''. The \\textit{value} field is the text content of the ``input'' action, which is empty for other action types.\n\n\\wh{\nIn contrast to existing methods that utilize LLMs to summarize or \\shpd{respond} to queries about individual mobile UIs \\cite{talking_with_ui, venkatesh2022ugif}, automating mobile tasks demands the capability to plan task solutions and an in-depth understanding of which UIs are essential for task completion. \\name aims to achieve multi-step task automation by leveraging app-specific knowledge. \nFurthermore, unlike most existing approaches that require significant developer/user efforts \\cite{ulink} to enable automated tasks, we aim to achieve unsupervised task automation, \\ie support the automation of arbitrary tasks on black-box apps (whose internal mechanisms are unknown) without human effort. }\nHowever, we assume that the apps are available for automated analyses, \\eg exploring the states, crawling the content, and analyzing the code. Such an assumption is reasonable because the app packages are all available for download and static/dynamic app analysis techniques have been extensively studied before \\cite{droidbot, Caiipa, li2019humanoid, VDfarms}.\n\\vspace{-0.13cm}\n\\subsection{Large Language Models}\nLarge language models (LLMs for short) mainly refer to the Transformer-based \\cite{transformer} language models that contain billions of parameters and are trained on massive amounts of text data, such as ChatGPT \\cite{chatgpt}, GPT-4 \\cite{openai2023gpt4}, PaLM \\cite{palm}, LLaMA \\cite{touvron2023llama}, etc. \nThese models exhibit capabilities that are not present in smaller models, \nincluding mathematical reasoning \\cite{solve_math}, program synthesis \\cite{copilot}, and multi-step reasoning \\cite{cot}. Specifically, LLM can perform the tasks better than the benchmark models trained on dedicated datasets. \nThe input of an LLM is a prompt, which is an instruction to guide its generation of responses. The prompt is tokenized into tokens (words or subwords) before being fed into the LLM.\n\nResearchers are actively exploring methods to enhance the problem-solving capabilities of LLMs by incorporating reasoning skills \\cite{cot} and tool utilization \\cite{chamelon, gorilla, responsible_task_automation}. These efforts aim to enable LLMs to use tools by teaching them to call APIs or to synthesize codes. However, task automation in smartphone apps is more complex since it is often related to the environment without documented interfaces. \n\n\\subsection{LLM meets Mobile Task Automation}\n\nWe believe that incorporating LLMs into mobile task automation brings unique advantages and strengths to both fields.\n\nFirst, \\textbf{LLMs have the potential to significantly advance the applications of mobile task automation.}\nThe voice-controlled intelligent personal assistants (IPA) are typical applications of mobile task automation, \\shpd{aiming} to provide intelligent, efficient, hands-free user experience on mobile devices.\nSuch applications are not only useful in smartphones, but also in many other scenarios, including automotive in-vehicle infotainment (IVI) systems \\cite{IVIsys}, wearable fitness trackers \\cite{uiwear, iself}, and VR/AR devices \\cite{AMash}.\nTo support IPA services, developers usually have to manually configure the task workflows, which is a cumbersome process even for experienced developers.\nResearchers have also attempted to build agents that can directly manipulate GUI elements like human users \\cite{seq2act, metagui, kite, talking_with_ui}. \nHowever, they usually require a lot of human demonstrations, step-by-step instructions, or clearly-designed task-specific reward functions for task completion \\cite{rl_use_computer, glider}. %\\yuanchun{refs}. \nLLM-based agents can be better at GUI task automation with their strong language comprehension and reasoning abilities.\n\nSecond, \\textbf{equipping LLMs with smartphones can significantly augment their abilities.}\nLLMs are trained with large-scale public data that contains rich commonsense and world knowledge, while they have limited knowledge about individual users and limited abilities to provide personalized services.\nSmartphones have been an important part of daily life by helping people connect with others, stay organized with calendars, navigate and get directions, control smart-home devices, and so on.\nIf LLMs learn to use smartphone apps and access data siloed in them, they could become much better personal assistants with access to the rich sensors and personal data in mobile apps.\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.37\\textwidth]{figure/motivation.pdf}\n    \\caption{An example task of \\textit{``Remove all the events in the Calendar''}. The agent needs to tap on \\textit{\"more options\"} and \\textit{\"settings\"} on the first and second GUI, which do not have a direct semantic association with the ultimate goal. This association can be grasped more \\shpd{easily} with app analysis.}\n    \\label{fig:motivation}\n    \\vspace{-0.2cm}\n\\end{figure}\n\nYet, applying LLMs to mobile task automation involves several challenges, including GUI representation, knowledge integration, and cost optimization. \nFirst, LLMs are only capable of processing plain text data and cannot directly handle GUI or interact with it. Although the GUI state in Android can be represented as text using the UI Hierarchy Viewer or Accessibility Services, it is usually lengthy (about 40k tokens on average for each UI state) and difficult for LLMs to interpret.\nSecond, LLMs lack knowledge and experience about certain applications, which may lead to incorrect execution of instructions. Figure \\ref{fig:motivation} shows an example where a deep understanding of the app is needed to complete the task. It is difficult to determine solely based on semantics and prior knowledge that clicking on \\textit{`more options'} and then \\textit{'settings'} on the first two screens will lead to the screen containing the option to \\textit{`delete all events'}.\n\\wh{Therefore, relying \\shpd{solely} on prompt engineering for LLMs to produce common-sense solutions can result in mistakes. A better approach might be to let LLMs investigate and learn from mobile apps, gaining practical experience prior to undertaking tasks for users. }%A possible solution is to explore and analyze the mobile apps and acquire experience before trying to complete tasks for users.}\nThird, using LLMs for task completion may be costly. The price of querying ChatGPT API \\cite{chatgpt} is \\$1.5 / 1000K tokens. Even if we can deploy a private LLM service, the computational cost is still high. For example, inferring a single token with LLaMA-7B \\cite{touvron2023llama} takes 6.7 billion FLOPs,\nand the whole process of task completion may use over 2000 tokens.\n\n\\section{Our Approach: \\name}\n\\label{sec:approach}\n\nWe introduce \\name, an LLM-powered end-to-end mobile task automation system to solve the aforementioned challenges. In the offline stage, \\name obtains app-specific knowledge by exploring UI relations and synthesizing simulated tasks. In the online stage, \\name continuously queries the memory-augmented LLMs to obtain guidance on the next action. The task is completed by following the LLM-suggested actions. \\name adopts several techniques to improve the task completion rate and optimize the query cost. Figure \\ref{fig:overview} illustrates the workflow. \n\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{figure/overview.pdf}\n    \\vspace{-0.3cm}\n    \\caption{The workflow of \\name.}\n    \\label{fig:overview}\n    \\vspace{-0.2cm}\n\\end{figure*}\n\nWe explain the functioning of \\name using the example of automating tasks in a calendar app:\nDuring the offline stage, \\name explores the app by randomly clicking buttons on the screen and records the result in a UI Transition Graph (UTG) memory (\\textit{Step 1}). Next, it traverses all the UI elements in the UTG and summarizes the tasks they can accomplish (\\textit{Step 2}).\nDuring online operation, when the user issues a command such as ``delete all the events in the calendar'', the \\textit{Prompt Generator} generates a prompt based on the task, the UI state description, and relevant information stored in the \\textit{App Memory}. This information includes instructions on how to navigate to the GUI page that contains the ``delete events'' option.\nSubsequently, the \\textit{Privacy Filter} replaces any sensitive information in the prompt to safeguard privacy. The filtered prompt is then sent to the LLM.\nOnce the LLM provides an answer, the \\textit{Task Executor} parses the action that can be executed on the smartphone and verifies its security before performing it. If the executor deems the action to be potentially risky, such as ``delete all the events'' in this particular task, it will seek confirmation from the user before proceeding. We will explain how \\name does all of these in the rest of this section.\n\n\\subsection{Task-oriented UI Prompting}\n\\label{section: prompting}\n\nUI prompting refers to the process of representing underlying UI information in text and injecting it into the prompt to query the LLM. \nThe goal of UI prompting is to clearly present the UI textual and structural content to the LLM and restrict the output of the LLM to predict only valid UI interactions. %, and (iii) encourage the LLM to reason about how to complete the task with the underlying app. \nFigure \\ref{fig:prompt} showcases an example of \\name converting a GUI interface into a prompt \\shpd{while} completing the task.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.45\\textwidth]{figure/prompt.pdf}\n    \\vspace{-0.3cm}\n    \\caption{An illustration of the prompt used by \\name. The content in red, blue, and green boxes are the overall guidance, the task representation, and the output requirements respectively. The `current UI State' in the prompt refers to the UI displayed within the black box.}\n    \\label{fig:prompt}\n    \\vspace{-0.2cm}\n\\end{figure}\n\n\\subsubsection{Converting GUI to Simplified HTML Representation} % DOM tree}\n\\label{sec:html_desc}\nWe develop a GUI parsing module to convert GUI to a simplified HTML representation that can be processed by LLMs. Researchers have found that LLMs are better at understanding HTML than natural-language-described UIs due to the large amount of HTML code in the training data of LLMs \\cite{talking_with_ui}. %The original GUI tree parsed by UI automators \\cite{droidbot} often contains hundreds of nodes including Layouts, Textviews, EditText, etc. Directly tokenizing this information will consume tens of thousands of tokens, which will exceed the length limit of LLMs. \nTherefore, we represent the GUI in HTML style, which can preserve the attribute information of UI elements. We use five types of HTML tags, namely <button>, <checkbox>, <scroller>, <input>, and <p>, which represent elements that can be clicked, checked, swiped, edited, and any other views respectively. The properties included for each element are: ID (the order in which the element appears in the GUI tree), label (the content description that describes the function of the element), onclick (hints about the UI states that will be accessed upon clicking this button or checking/unchecking this checkbox, which will be introduced in \\S\\ref{sec:prompt_augment}), text (the text on the element), direction (scrolling direction, including up/down/left/right), checked (whether the checkbox is checked or not), value (the text that has been input to the text box). The classes and properties of GUI elements are shown in Table \\ref{tab:element_properties}. %(The ``onclick'' \nWe further simplify the DOM tree by pruning invisible elements and merging functionally equivalent elements, which will be introduced in \\S \\ref{section: query_optimization}. The texts of two merged UI elements are separated by ``<br>'', which represents a line break in HTML.\n\n\\begin{table}\n\t\\caption{The classes and properties of GUI elements}\n\t\\vspace{-0.3cm}\n\t\\centering\n        \\resizebox{.47\\textwidth}{!}\n        {\n\t\\begin{tabular}{lll}\n\t\t\\toprule\n\t\t Class                     & Properties                                        & Available action\\\\\n\t\t\\midrule\n\t\t<button>                & ID, label, onclick (\\S\\ref{sec:prompt_augment}), text                      & click \\\\\n\t\t<checkbox>              & ID, checked, label, onclick (\\S\\ref{sec:prompt_augment}), text              & check/uncheck\\\\\n\t\t<scroller>              & ID, scroll direction, label, text     & scroll <direction>\\\\\n\t\t<input>                 & ID, label, text, value                & input <text>\\\\\n\t\t<p>                     & ID, label, text                       & N/A\\\\  \n\t\t\\bottomrule\n\t\\end{tabular}\n }\n\t\\label{tab:element_properties}\n     \\vspace{-0.5cm}\n\\end{table}     \n\nIn our experiments, we observe that the agent generally does not proactively scroll on interfaces that can be scrolled vertically (shown in \\S \\ref{eval:action_selection}). However, having information about the scrolled interface is crucial for decision-making, especially when the target button is located on a scrolled portion of the interface that is not yet visible. Therefore, to provide the agent with comprehensive information, we need to include the components from the scrolled portion of the interface in the current UI state. \nTo achieve this, for a given interface, \\name first automatically scrolls through all scrollable components and records the information of the visible UI elements, and then provides this information to the LLM for decision-making. This approach offers two advantages. Firstly, it prevents LLMs from making blind selections when they cannot see all the information on the interface. Secondly, it eliminates the need for LLMs to provide explicit instructions for scrolling, reducing the frequency of calling the LLM and lowering the associated computational overhead.\n\n\\subsubsection{Restricting the Action Space with Selections}\nA key characteristic of UI task automation is that all agent actions need to be confined to the constraints of the underlying app \\textit{i.e.}, the agent can only perform actions of a supported action type on one of the existing UI elements. Thus, a challenge is to adapt LLMs, which are generative in nature, to such a discrete choice task. \nHence, we impose the necessity for LLMs to produce results in a predetermined structure by completing the following requirement: ``- id=<id number> - action=<tap/input> input text=<text or N/A> (in the event of task completion, id=-1)''. LLMs must refrain from generating id or input in an arbitrary format.\n\n\\subsection{Exploration-based Memory Injection}\n\nExploration-based memory injection aims to provide app-related information to LLMs, enabling them to gain insights into apps, understand app utilization methods, and make effective decisions. However, there are challenges in utilizing automated app-related knowledge to assist LLMs in task automation, including: (i) The UI Transition Graph (UTG) obtained through random exploration cannot be directly processed by the LLM. (ii) Memory acquired solely through UI automation tools contains only UI and action data, without the essential information needed to directly enable task automation. This includes details about the specific UI elements and actions necessary to accomplish a particular task. \n(iii) An app may have numerous UI screens and UI elements (buttons, text boxes, etc.), exceeding the token length limit of LLMs if all of them are included in a prompt.\nTo overcome these challenges, \\name synthesizes simulated tasks based on the randomly explored UI graph. %which can be used to enhance LLMs.\nThese simulated tasks serve as a guide for LLMs on how to accomplish a user task.\n\n\\subsubsection{Simulated Task Generation}\n\n\\name generates simulated tasks by analyzing the UI Transition Graph (UTG) as depicted in Figure \\ref{fig:generating_functions}. \nThe UTG generated by the UI automator contains crucial information about the application, such as the connections between UIs and the presence of different UI elements on each screen. By summarizing the functionalities of all UI elements, we can gain a thorough understanding of the tasks that can be performed within the app and determine the corresponding UI elements required to execute them. As a result, \\name parses all UI states and UI elements present in the UTG and extracts their functions by querying LLMs.\n\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=0.9\\textwidth]{figure/appmem.pdf}\n    \\caption{Workflow of offline simulated task synthesis. Given the UI Transition Graph (UTG), Memory Generator synthesizes a simulated task for each UI element with LLMs, \\shpd{then} records the task-states-elements in the App Memory.}\n    \\label{fig:generating_functions}\n    \\vspace{-0.4cm}\n\\end{figure*}\n\nSpecifically, UTG can be regarded as a directed graph, where the nodes and edges are all UI states and actions recorded by the random Explorer, denoted as $\\mathbf{U}$ and $\\mathbf{A}$ respectively. \nFor each UI state $\\mathbf{U}_i$, the memory generator queries LLM to summarize the functionalities of all the UI elements $\\{e_i^{j}\\}_{j=1}^{|U_i|}$, where $|U_i|$ denotes the number of elements in $U_i$. \nNote that \\name only extracts the functionality of an element on the UI state that is closest to the initial UI if it appears on multiple UI states.\nAfter traversing all UI elements in the UTG, we obtain the \\textit{simulated task table} in the app memory containing $n$ entries, where $n$ represents the total number of UI elements on the UTG. Each entry in the table corresponds to a UI element $e_i^{j}$ and is divided into three parts: \\textit{<Simulated task, UI states, UI elements>}.  ``Simulated task'' represents the functionality of $e_i^{j}$ that has been summarized by LLM, which can be perceived as a simulated task that can be completed by clicking this element. ``UI elements'' includes all the elements that were clicked, starting from the initial UI of the app and leading up to the attainment of $U_i$. ``UI states'' represents the sequence of UI states that were traversed from the initial UI state to $U_i$. \nThis table provides the agent with information about the required operations to achieve each functionality, aiding the agent in planning how to complete a given task efficiently.\nApart from the \\textit{simulated task table}, there is an additional table called the \\textit{UI function table} in the app memory. It provides a summary of the functionality associated with each UI state in the UTG. This information is obtained by querying the LLM to summarize the function of each UI state.\n\n\\subsubsection{Augmenting Prompts with App Memory}\n\\label{sec:prompt_augment}\n\nThe most straightforward approach to leveraging app-specific knowledge is to incorporate the app memory directly into the prompt, which can provide guidance to the LLM.\nHowever, this may exceed the maximum token limit of the LLM such as 4097 tokens for GPT-3.5 \\cite{chatgpt}. %Additionally, it wastes a significant amount of computational resources by including redundant information. \nIn many cases, only a few UI elements are necessary to complete a user's task.\nHence, we selectively incorporate the most relevant UI information into the prompt.\n\n\\name determines the importance of a UI element in the app memory based on the similarity between its simulated task and the current user task.\nWe use an embedding model (Instructor-XL \\cite{instructor-xl}) that maps natural language sentences to a fixed-dimension embedding, where the embeddings of sentences with similar meanings are closer. The cosine similarity between the embeddings of the simulated task $S$ and the current task $T$ is denoted as $sim(E(S), E(T))$.\nThen, we can find k most similar simulated tasks in the app memory, denoted as $\\{S_1, S_2,..., S_k\\}$.\nFor each $S_i$, we can retrieve the corresponding \\textit{UI states} and the \\textit{UI elements} from the ``simulated task table'' in the app memory. \nIn the online stage, if the current UI matches one of the \\textit{UI states} associated with $S_i$, we give hints about the UI elements that the random explorer interacted with in this UI state. This helps LLMs understand the outcome of interacting with the elements. \nSpecifically, the prompt generator of \\name will add a new property ``onclick'' to the HTML UI statement in the prompt (shown in Table\\ref{tab:element_properties}). In HTML, ``onclick'' is used to describe the event that will occur when users click a button, link, or image. \nIn our prompt, the content of \\shpd{the} ``onclick'' property refers to the functionality of the \\textit{UI states} that will be accessed after clicking this element, which \\shpd{is} most relevant to completing the $S_i$. Algorithm \\ref{algo:prompt-augmenting} shows how to augment prompt with $\\{S_1, S_2,...,  S_k\\}$ and app memory $M$.\n\n\\begin{algorithm}\n\\footnotesize\n\t\\caption{Prompt Augmentation}\n\t\\label{algo:prompt-augmenting}\n\t\\begin{algorithmic}[1]\n\t\t\\Require Current user task $T$, k most similar simulated tasks in the app memory $\\mathcal{S}=\\{S_1, S_2... S_k\\}$, App Memory $M$ %Target edge scenario $e$, task description $task^e$, latency requirement ${LAT}^{e}$, memory requirement ${MEM}^{e}$, edge-specific performance evaluator $Evaluator^{e}$.\n\t\t\\Ensure Final GUI state after completing the task $T$\n\t\t\n\t\t\\Function{Online-Main}{}:\n\t\t\t\\State $Guide \\gets GenerateGuide(M, \\mathcal{S})$\n\t\t\t\n\t\t\t\\While{$T$ not completed}\n\t\t\t\t\\State $UI \\gets$ Current GUI of SmartPhone\n\t\t\t\t\\If{$UI \\in Guide.UIs$}\n\t\t\t\t\t\\State $UI.elements.hint \\gets Guide.UI.elements.Function$ %\\Comment{provide the element possibly associated with the current task a property ``hint''}\n\t\t\t\t\\EndIf\n\t\t\t\t\\State $Prompt \\gets PromptGenerator(T, UI, History)$\n\t\t\t\t\\State $Action \\gets LLM(Prompt)$\n                    \\State $TaskExecutor.execute(Action)$\n\t\t\t\t\\State $History.insert(Action)$\n\t\t\t\\EndWhile \\\\\n\t\t\\Return $Current \\ GUI$\n\t\t\\EndFunction\\\\\n\t\t\n\t\t\\Function{GenerateGuide}{$M, \\mathcal{S}$}: \n\t\t\t\\For{each $S_i$ of $\\mathcal{S}$}\n\t\t\t\t\\For{each $UI_i^{j}$ of $M.Simulated\\_Task\\_Table\\{S_i\\}$}\n                        \\State $Guide.UI_i^{j}.Element_i^{j}.hint \\gets M.Function\\{UI_{i}^{j+1,j+2...}\\}$\n\t\t\t\t\\EndFor\n\t\t\t\\EndFor \\\\\n\t\t\\Return $Guide$ \n\t\t\\EndFunction\n\t\\end{algorithmic}\n\\end{algorithm}\n\nTake the task shown in Figure \\ref{fig:motivation} as an example. Given the task \\textit{``Remove all the events in the calendar''}, \\name can retrieve in the app memory and find the simulated task of the \\textit{``delete all events''} button in GUI 3 to be a relevant task. Additionally, \\name can find that clicking \\textit{``more options''} and \\textit{``settings''} in GUI 1 and GUI 2 can lead to the target button. \nTherefore, if the current UI screen is GUI 1, %\\name will augment the prompt by adding the hint that clicking \\textit{``more options''} can navigate to a button that may be helpful for completing the task. Specifically, \nthe HTML description of \\textit{``more options''} in GUI 1 will change from\n``<button label=`More options'></button>'', to\n``<button label=`More options' onclick=\\textit{`navigate to GUIs that can: 1.add contact holidays and anniversaries, import and export events, manage settings, 2.Delete all events in the app, manage event reminders, etc.}'></button>''.\n\n\\subsubsection{Tuning Local LLM with App-specific Data}\n\\label{sec:finetuning}\n\\name can also utilize smaller local LLMs (\\eg Vicuna-7B \\cite{vicuna2023}) to make decisions, as a cost-effective alternative to larger on-cloud LLMs (\\textit{e.g.} GPT-3.5 \\cite{chatgpt}).\nHowever, the reasoning ability of these smaller LLMs is weaker than on-cloud LLMs, leading to a noticeable decrease in accuracy. It is observed that local LLMs still exhibit suboptimal performance even with the prompt augmentation methods introduced in \\S \\ref{sec:prompt_augment}. Researchers have found that fine-tuning using domain-specific data is an effective way to improve small LLM's abilities \\cite{vicuna2023, alpaca}. Therefore, we can augment smaller LLMs by fine-tuning using app-specific data.\n\nA key challenge in our scenario is how to generate high-quality (question, answer) pairs to fine-tune the LLM.\nA naive way is to directly synthesize these data pairs from the simulated task table of the app memory. For a simulated task $S$, the memory generator records a sequence of UI states $\\{U_1, U_2, ..., U_k\\}$ and the UI elements $\\{e_1, e_2, ..., e_k\\}$ to complete it. \nWe can directly generate k data pairs $(q_i, a_i)_{i=1}^k$ based on this record.\nSpecifically, $q_i$ is a prompt  generated based on the task $S$, previous UI actions $\\{A_1, A_2, ..., A_k\\}$ (where the target elements are $\\{e_1, e_2, ..., e_k\\}$ and the action type is \\textit{click}), and the current UI state $U_i$. Then, the description of the action $A_i$ can be the answer $a_i$.\nThe rationale behind this approach is that: Based on the generation process of the app memory, we already know that when transitioning from interface $U_i$ to complete task $S$, action $A_i$ needs to be performed. Therefore, the correct answer of which action to choose given the state $U_i$ should be $A_i$.\n\nHowever, the answers generated in this way only include \\textit{<target element, action type, value>}, lacking detailed information or context. Thus, it is difficult for the local LLM to learn how to choose the correct action based on the prompt. \nIf we include the reasons for choosing the target action in the answers, it will enhance the local LLM's understanding and enable it to learn how to reason based on the current task and UI \\cite{LLMdistill}. \nThus, we can ask larger LLMs (such as GPT-4 \\cite{openai2023gpt4}) to answer the reason why $A_i$ is chosen to complete task $S$, and prompt it to reason in a step-by-step manner like a Zero-shot Chain-of-Thought (0-shot CoT) \\cite{zero-shot-cot}. The prompt sent to the larger LLM is mainly the same as Figure \\ref{fig:prompt}. Additionally, we provide the correct action to choose $A_i$, and prompt the LLM to reason about the correct action by changing the ``output requirements'' part to the following format:\n\n\\textit{Your answer should always use the following format:\n1. Completing this task on a smartphone usually involves these steps: <?>.\n2. Analyses of the relations between the task and the previous UI actions and current UI state: \n3. Based on the previous actions, is the task already finished? <Y/N>. The next step should be <?/None>.\n4. Can the task be proceeded with the current UI state? <Y/N>. Fill in the blanks about the next one interaction: - id=<id number> - action=<tap/input> - input text=<text or N/A>.\n}\n\nThe answer to the above questions can be used as the answer in the (question, answer) pair for fine-tuning the local LLM.\nThe thinking and reasoning data generated by these larger LLMs contains rich information and knowledge. Using it as answers to fine-tune smaller LLMs can enable it to mimic the emergent reasoning abilities of the large model.\nBesides leveraging the knowledge from larger LLMs, fine-tuning LLMs with app-specific data also has the bellow two advantages: \n(i) Learning from the UTG and incorporating the insights gained from it. %The generated training data includes GUI transition relationships, which help the large model understand the usage patterns of an app. Therefore, the finetuned local LLM can perform more effectively on challenging tasks. \n(ii) Let smaller LLMs generate answers that adhere to the desired format instead of unrestricted formatting in the answers.%.  Smaller LLMs often struggle to generate answers that adhere to the prescribed format, which hampers our ability to extract actions from their responses. By fine-tuning with the properly generated answers from our training data, we can typically ensure that the fine-tuned LLMs produces answers that conform to the desired format. Consequently, this enhances the regularity of the model's responses.\n\n\\subsection{Multi-granularity Query Optimization}\n\\label{section: query_optimization}\nWe observe that the primary source of overhead in \\name arises from querying LLMs. \nConsequently, reducing the frequency of LM queries for each task will result in a reduction of \\name's overhead.\nAdditionally, as a more granular approach, pruning unnecessary tokens in the prompt, we can effectively decrease the computational cost of LLM.\n\n\\subsubsection{Pruning Tokens by Merging Functionally Equivalent Elements.}\nThe HTML statement of UI described in \\S \\ref{section: prompting} contains a lot of redundant information, which will increase the number of tokens and cause the LLM to overlook the most useful information. Therefore, We adopt two techniques to reduce the length of the text:\nFirst, we prune the elements without any visual or textual information (such as background or container items). %It is observed that most of the nodes are containers that can not be viewed by humans on a screen. Therefore, pruning these invisible views from the GUI representation can largely reduce the number of tokens. We heuristically prune the views that do not include any text or image information and can not be edited or swiped. \nSecond, we merge functionally equivalent UI elements into one element and separate the originally different elements with a ``<br>'' delimiter, which means a line-break-like spacing in HTML. \nWe merge UI elements based on two rules: (i) Based on UTG: If operating on these two UI elements leads to the same interface, we combine them into a single component. Specifically, if the starting and ending points of two edges representing actions in the UTG are the same, we merge the components they operate on. \n(ii) Based on UI tree analysis: We merge the non-interactive (plain text or image) UI leaf nodes sharing the same interactive ancestor (button, checkbox, text field, etc.) in the UI tree. \nFor example, in the GUI screenshot shown in Figure \\ref{fig:prompt}, ``Alarms'' and ``0 items'' are two single plain-text nodes in the GUI tree that have a common clickable ancestor. \nThus, we can merge them into an HTML statement: ``<button id=5>Alarms<br>0 items</button>'' instead of two single statements ``<button id=5>Alarms</button>'' and ``<button id=6>0 items</button>''.\n\n\\subsubsection{Reducing Query Times by Shortcuts and GUI Merging}\n\nGUI merging is to include several GUI states into one prompt if LLMs need them all to make decisions. \nThe automatic scrolling introduced in \\S \\ref{sec:html_desc} can accomplish this by skipping the intermediate steps like ``scroll down''. \nWithout automatic scrolling, \\name has to query LLMs at least twice to touch an element within the GUI after swiping, involving both scrolling and clicking. After merging the scrolled UIs into one prompt, we only need to call LLMs once and get the action \\textit{``Scroll down to Button A and touch it''}. \n\nThe shortcut is to execute simple actions directly with the help of the app memory. Although some steps are crucial and require a large model to make decisions, others are straightforward and do not require it. So if we can identify steps that are simple enough so that a local embedding model \\cite{instructor-xl} can make decisions, we can reduce the number of queries. Specifically, let $T$, $E$, and $\\{S_1, S_2, ...\\}$ denote the user task, the embedding model, and the simulated tasks respectively. If we find $sim({E}(S_k), {E}(T))>\\gamma$ where \n$S_k=\\operatorname*{arg\\, max}_{S_i \\in \\mathcal{S}} sim({E}(S_i), {E}(T))$, \nthen $S_k$ is very similar to $T$, and accomplishing $S_k$ is straightforward because we have a series of actions $\\{A_k^1, A_k^2, ...\\}$ in the app memory that navigate from the initial UI state to $S_k$. Thus we can perform $S_k$ by the task executor without calling LLM. \n$\\gamma$ is a hyper-parameter, the larger the value of $\\gamma$, the stricter our criteria for selecting similar simulated tasks become.\nWe observe that even if the shortcut navigates to UI states unrelated to the task, LLM is still able to identify issues and quickly navigate to the correct UI states.\n\\section{Implementation}\n\\label{sec:implementation}\n\nWe implement \\name using Python and Java.  \nThe local LLM Vicuna \\cite{vicuna2023} is fine-tuned using PyTorch.\n\n\\textbf{Identifying Risky Actions.}\nSome actions may potentially alter local or server data, or cannot be undone once performed. These actions are considered risky and require user confirmation before being executed by the agent.\nFor example, before calling a contact, \\name needs to first prompt the user to verify the correctness of the action. If the user notices any errors in the number about to be dialed, they can manually make the necessary modifications.\n\\name accomplishes this by prompting the LLM to identify risky actions, \\textit{i.e.} appending the sentence \\textit{``If this action potentially leads to a change of user data or server state that requires user confirmation, please answer requires\\_confirmation=Yes)''} to the prompt.\nIn addition, \\name also utilizes key phrases on the UI, such as ``warning'', to further identify potentially risky actions.\n\n\\textbf{Eliding Private Information.}\nWe add a privacy filter that can mask the private information in the query. During online processing, it runs a Personal Identifiable Information (PII) scanner \\cite{pii_detector} that can detect sensitive information in the prompt, including name, phone number, email address, etc. This personal information is replaced with non-private words (\\eg ``<name>''$\\rightarrow$``Alice'') before sending the prompt to the cloud. After receiving the response from LLMs, \\name maps the special words back to the original ones before parsing actions.\n\\label{sec:benchmark}\n\nWe introduce \\datasetname, an Android Task Automation benchmark suite designed to evaluate the performance of end-to-end mobile task automation systems.\n\\datasetname consists of 158 high-level tasks extracted from 13 popular apps. What sets our benchmark apart is that it not only provides tasks and corresponding GUI action traces but also offers the exploration memory and environment for the underlying apps. %a real Android environment that include all the exact apps utilized in our dataset.\nAgents can actively interact with the environment during the offline stage, gathering information about the apps and recording UTGs.\nAll 13 apps used to collect the tasks are installed, granted necessary permissions, and can reproduce the GUI action traces in our environment.\nWe will release the environment in the form of an Android Virtual Machine Snapshot, allowing researchers to restore the exact environment in which we collected our data.\nWhile previous benchmarks \\cite{seq2act, motif, metagui} also provide tasks and corresponding actions, they lack a reproducible environment. However, with the emergence of LLM-powered task automation methods \\cite{gorilla, react}, which often require dynamic information about the environment for decision-making, our benchmark offers greater convenience for evaluating the performance of autonomous agents on mobile phones.\n\nWe develop a system for collecting datasets that can interact with Android smartphones. The selected apps primarily consist of common mobile tools (such as contacts, dialer, camera, calendar, etc.) from F-Droid, a free and open-source app platform.\nFor each app, we ask annotators to provide a list of 5-15 tasks described in natural language. To complete each task, annotators interact with the smartphone through a desktop computer in an iterative manner.\nDuring each iteration, the system displays the smartphone's user interface (UI) in its current state to the annotator, along with a list of available actions. Annotators can also directly observe the actual state of the smartphone. They can choose an action from the following options: 1. Touch <Button ID>, 2. Input <input text> to <EditBox ID>, 3. Swipe <Scroller ID> <direction> in the terminal.\nThe distribution of tasks is shown in Figure \\ref{fig:task_destribution}.\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.48\\textwidth]{figure/task_analysis.pdf}\n    \\vspace{-0.8cm}\n    \\caption{The distribution of tasks in \\datasetname across different numbers of steps (a) and different apps (b).}\n    \\label{fig:task_destribution}\n    \\vspace{-0.4cm}\n\\end{figure}\n\n\\section{Evaluation}\n\\label{sec:experiment}\n\nWe conduct experiments to examine the accuracy and cost of \\name in mobile task automation.\n\n\\subsection{Experimental Setup}\n\\label{eval:setup}\n\\textbf{Dataset.}\nWe mainly evaluate \\name on \\datasetname (mentioned in \\S\\ref{sec:benchmark}). We also utilize MoTiF \\cite{motif} dataset to train the baseline methods and fine-tune the LLMs. MoTiF \\cite{motif} is a large-scale mobile app task dataset with more than 4.7k tasks (excluding tasks without valid demonstrations). It also provides the screenshot and the tree-based representation of the GUI screens that annotators interacted with when completing these tasks, but lacks the exploration environment of the apps.\n\n\\textbf{Hardware.}\nWe evaluate the end-to-end performance of \\name on a OnePlus ACE 2 Pro with 8 3.2 GHz ARM-based cores (Snapdragon 8 Gen2 CPU) and Adreno™ 740 GPU. \\wh{The local LLM Vicuna-7B \\cite{vicuna2023} is deployed on the smartphone based on Machine Learning Compilation for LLM (MLC LLM) \\cite{mlc-llm}. %We also deploy it on an edge server with 1 NVIDIA 3090 24G GPU to evaluate the latency of a small LLM on an edge server. \nAdditionally, it is deployed on an edge server equipped with 1 NVIDIA 3090 24G GPU to assess inference latency in an edge computing context. }The Vicuna-7B model is fine-tuned on an 8$\\times$ A100 80GB server for about 4 GPU hours.\n\n\\textbf{Baselines.}\nWe choose META-GUI \\cite{metagui} and an existing LLM-based design for UI task automation \\cite{talking_with_ui} (referred to as LLM-framework) as our main baselines.\nMETA-GUI \\cite{metagui} is a training-based conversational agent on mobile GUI that can accomplish various tasks. We train it on the MoTiF \\cite{motif} dataset.\nLLM-framework \\cite{talking_with_ui} is an LLM-based framework that enables diverse language-based interactions with mobile UIs.\nWe also implement two relatively simple baselines, random performer (randomly selecting one UI element within each UI screen) and similarity-based (selecting the UI element that is semantically closest to the task using a SOTA embedding model \\cite{instructor-xl}) performer. %to show the complexity of the tasks.\n\n\\textbf{Metrics.}\nGiven a sequence of UIs $\\{U_1, U_2, ..., U_n\\}$ in which human annotators performed actions $\\mathcal{A}=\\{A_1, A_2, ..., A_n\\}$ to complete a task $T$, if one agent can make a sequence of decisions $\\hat{\\mathcal{A}}=\\{\\hat{A}_1, \\hat{A}_2, ..., \\hat{A}_n\\}$ on $\\{U_1, U_2, ..., U_n\\}$, we use below two metrics to measure its performance:\n\n(i) \\textbf{Action Accuracy}: The ratio of the action $\\hat{A}_i$ matching the ground-truth actions $A_i$, namely $P(\\hat{A}_i=A_i)$. One action is right only if the target UI element and input text (``null'' if there is no need to input) are both right. This metric reflects the ability of the agent to make correct decisions based on the available information.\n\n(ii) \\textbf{Completion Rate}: The probability of completing all the actions in one sequence correctly, namely $P(\\hat{\\mathcal{A}}=\\mathcal{A})$. This metric reflects the probability of the agent being able to consistently and successfully complete a task. \n\n\\subsection{Action Accuracy}\n\\label{eval:action_selection}\n\\begin{table*}[htbp]\n    \\centering\n    \\caption{Action accuracy of \\name and baselines on \\datasetname. Rand: Randomly selecting actions, Sim: Similarity-based action prediction, LLM-F: LLM-framework \\cite{talking_with_ui}, Complete: Determining completion.}\n    \n    \\vspace{-0.3cm}\n    \\resizebox{.8\\textwidth}{!}\n    {\n        \\begin{tabular}{cccccccccc}\n        \\toprule\n        & & & & \\multicolumn{2}{c}{\\textbf{Vicuna-7B}} & \\multicolumn{2}{c}{\\textbf{GPT-3.5}} & \\multicolumn{2}{c}{\\textbf{GPT-4}}\\\\\n        \\cmidrule(r){5-6} \\cmidrule{7-8} \\cmidrule(l){9-10} \n        \\textbf{Action} & \\textbf{Rand} & \\textbf{Sim} & \\textbf{MG}   & LLM-F & \\name & LLM-F & \\name & LLM-F & \\name \\\\\n        \\midrule \n        \n        \n          \n          \n          Click          & 2.3\\% & 35.1\\% & 25.3\\%  & 15.2\\%      & 74.5\\%   & 58.1\\%       & 72.1\\%   & 65.4\\%    & 91.2\\% \\\\\n          Input          & 0     & 0      & 0       & 0           & 40.0\\%   & 5.0\\%        & 62.5\\%   & 27.5\\%     & 82.5\\% \\\\\n          Scroll         & 2.5\\% & 0      & 0       & 8.2\\%       & N/A      & 0            & N/A      & 0.6\\%    & N/A \\\\\n          Complete       & 2.5\\% & 0      & N/A     & 4.4\\%       & 5.7\\%    & 0            & 41.8\\%   & 0        & 93.7\\% \\\\\n        \\midrule\n         Overall         & 2.3\\% & 20.8\\%  &22.4\\%  & 11.3\\%     & \\textbf{57.7\\%}  & 34.7\\%       & \\textbf{65.1\\%}   & 54.5\\%    & \\textbf{90.9\\%} \\\\\n        \\bottomrule\n        \\end{tabular}\n    }\n    \\label{tab:app_action_acc}\n\\end{table*}\n\nWe first evaluate the action accuracy of \\name. The open-sourced LLM Vicuna-7B \\cite{vicuna2023} \\shpd{is fine-tuned} using the generated app-specific data, as mentioned in \\S \\ref{sec:finetuning}. For the \\shpd{closed-source} LLM such as GPT-3.5 \\cite{chatgpt} and GPT-4 \\cite{openai2023gpt4}, which can not be fine-tuned directly, we augment them with automatically generated app memory, as mentioned in \\S \\ref{sec:prompt_augment}. \n\\wh{The temperature of the LLMs is set to a lower value of 0.25 to encourage creativity while preventing it from being overly random. }\nThe action accuracy of \\name and baselines is listed in Table \\ref{tab:app_action_acc}. \n\\name outperforms baselines on every action type, resulting in an overall accuracy improvement of 37.6\\%. \nAmong all the actions, clicking is the simplest, only requiring the decision of the element ID. On the other hand, scrolling and inputting necessitate specifying the direction or value of the UI element, and determining completion entails considering all previous actions.%, while scrolling, inputting require the direction or value of the UI element, and determining completion requires considering all previous actions.% Therefore, clicking is the easiest action. Determining completion is relatively hard for smaller models like Vicuna-7B \\cite{vicuna2023} and GPT-3.5 \\cite{chatgpt}, but larger LLM (GPT-4 \\cite{openai2023gpt4}) achieve a higher accuracy of 93.67\\% in it due to their superior reasoning capabilities. \nIt is also observed that with the LLM going larger, LLM-based methods outperform the model trained from scratch \\cite{metagui}. This is because the model has only been exposed to apps and tasks from specific datasets \\cite{motif}. \nThus, it will not perform well on new apps and tasks in the \\datasetname. However, by accumulating sufficient prior knowledge and incorporating our memory integration, LLMs can engage in rational reasoning on how to solve problems on new apps.\nFor scrollable UIs, \\name will first browse and traverse all the components on the screen, eliminating the need for the ``scroll'' action. From the scroll accuracy of the baseline, it is observed that the probability of the agent actively selecting this action is very low. Thus, browsing and traversing first can improve the overall accuracy of the agent. \n\nThe reason \\name outperforms baselines is: (i) \\name prunes and merges UI elements, which reduces the action space (from 36.4 to 13.2 choices per GUI state on average). (ii) The exploration-based memory can enhance the LLM with domain-specific knowledge about the apps, which will be detailed in \\S \\ref{eval:ablation}. (iii) The output format of the fine-tuned model is aligned more closely with the format requirements specified in the output requirements. If the output is not standardized, the task executor would be unable to extract or recognize the element ID and action.\n\nWe further analyze why and how \\name fails on some steps. We randomly sample 20 failure cases by \\name (\\shpd{using} GPT-4 \\cite{openai2023gpt4} as the LLM), and categorize 3 typical failure causes, as explained below:\n1. Multiple Correct Choices. In certain cases, there can be multiple valid ways to complete a task.\nAnnotators may not be able to exhaustively list all the possible ways to complete a task, and if the agent attempts a different approach than what the annotators specified, it may be deemed incorrect.\n2. Unable to accurately determine if the task has been completed. Sometimes \\name mistakenly considers a task completed when it detects the presence of a specific UI element. %, instead of actually interacting with it and continuing the execution. %And \\name may overlook buttons like `OK' or `Save' on the screen, resulting in the omission of the final step of an operation. Furthermore, \\name may not always accurately determine when a task has already finished and may repeat clicking on the same UI element that was clicked before.\n3. \\shpd{Lack of} understanding of the GUI. \\name occasionally overlooks important information or hints in the UI and makes decisions based on its prior experience. For example, in the task \\textit{``open the camera and record a short video, name it `test.mp3' ''}, the agent only needs to input `test' into the ``name'' box. This is because the GUI indicates that the file extension `.mp3' is already displayed in the ``file type'' box. However, \\name still selects `test.mp3' as the input to the ``name'' box.\n\n\\subsection{Task Completion Rate}\n\\label{eval:task_completion}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.48\\textwidth]{figure/completion_rate.pdf}\n    \\vspace{-0.8cm}\n    \\caption{Task completion rate of \\name with different LLMs and with \\shpd{varying numbers} of steps. Vi: Vicuna-7B, 3.5: GPT-3.5, 4: GPT-4.}%\\yuanchun{need somewhere to show the distribution of \\#steps of tasks.}}\n    \\label{fig:completion_rate}\n    \\vspace{-0.2cm}\n\\end{figure}\n\nThe Task Completion Rate of \\name and LLM-framework \\cite{talking_with_ui} is shown in Figure \\ref{fig:completion_rate} (a). Note that we do not include the completion determination step for clear comparison. \\name outperforms baseline by 40.5\\%, 26.4\\%, and 39.7\\% for Vicuna-7B, GPT-3.5, and GPT-4 respectively. We also show the completion rate of \\name with and without memory augmentation in Figure \\ref{fig:completion_rate} (b). As the number of steps increases, the overall completion rate decreases. This is because (i) the probability of each step being executed correctly decreases. (ii) Tasks that involve multiple steps often have multiple approaches to completion (e.g., creating a new contact by entering either the name or the phone number first). However, human annotators typically only annotate one approach, which can lead to the model's solution being mistakenly judged as incorrect. The actual completion rate in the real-system can be higher than the reported results, but we do not include real-system results since determining task completion can be ambiguous.\n\n\\subsection{Ablation Study}\n\\label{eval:ablation}\n\\subsubsection{Memory Injection}\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.45\\textwidth]{figure/memory_completion_rate.pdf}\n    \\vspace{-0.4cm}\n    \\caption{Action accuracy and task completion rate of \\name, with and without memory augmentation.}\n    \\label{fig:ablation_memory}\n    \\vspace{-0.2cm}\n\\end{figure}\nThe action accuracy and task completion Rate of \\name with and without memory is shown in Figure \\ref{fig:ablation_memory}. We can observe that the improvement in overall completion rate is much higher than the improvement in single-step accuracy. This is because the introduction of memory allows LLMs to make crucial single-step decisions (such as the example in Figure \\ref{fig:motivation}). Although these critical steps account for a small proportion of all the action steps, they are essential for successfully completing certain tasks.\nMoreover, it can be observed that smaller models benefit more from the inclusion of memory in terms of task completion rate. This is because smaller models possess less prior knowledge, thus requiring more guidance from application-specific knowledge. However, even for larger models, the incorporation of memory remains meaningful. Limited \\shpd{model capacity} cannot store the vast and ever-growing knowledge present in the world, making it difficult to stay updated on the evolving usage patterns of new applications. Therefore, automatic exploration and recording of their usage patterns play a crucial role in enabling LLMs to effectively use applications.\n\n\\subsubsection{Zero-shot Chain of Thought Fine-tuning}\n\n\\begin{table}\n \\caption{Action accuracy and completion rate of \\name based on Vicuna-7B with different fine-tuning techniques. Original: Vicuna-7B without fine-tuning. CoT: Fine-tuning with zero-shot chain-of-thought. Mo: Incorporating a small portion of MoTiF \\cite{motif} dataset for fine-tuning.} \n    \\vspace{-0.3cm}\n    \\centering\n    \\resizebox{.48\\textwidth}{!}\n    {\n        \\begin{tabular}{cccccc}\n\t\t\\toprule\n            Metric      &  Original     &  \\name      & No Mo  &  No CoT   & No Mo\\&CoT \\\\\n\t\t\\hline\n            Action        &   11.3\\%  &   57.7\\%   &   51.9\\%    &  20.6\\%  &  51.9\\%   \\\\\n\t\t  Completion    &   0.6\\%   &   41.1\\%   &   29.8\\%    &   0.6\\%  &  31.6\\%   \\\\\n            \n\t\t\\bottomrule\n\t\\end{tabular}\n    }\n \\label{tab:cot}\n\\end{table}\n\nThe action accuracy and task completion rate of \\name based on Vicuna-7B \\cite{vicuna2023} fine-tuned with and without Zero-shot Chain-of-Thought (0-shot CoT) \\cite{zero-shot-cot} is shown in Table \\ref{tab:cot}. Since the app memory automatically generated by \\name only contains clicking and checking actions, the LLM fine-tuned merely on the app memory is poor on inputting and adjusting whether the task has been completed.\nHence, we incorporate a small portion of manually annotated data for fine-tuning. Specifically, we add only the input and completion judgment data from MoTiF dataset \\cite{motif} into the app memory dataset. Note that the app and task in the MoTiF dataset \\cite{motif} are unrelated to our dataset. Thus, adding this portion of data will not result in any test data leakage. It simply enables the model to learn to input and to determine the task's completion.\n\nVicuna-7B \\cite{vicuna2023} fine-tuned with Zero-shot Chain-of-Thought data generated by app memory mixed with a small portion of MoTiF \\cite{motif} (\\textit{\\name}) can achieve 57.7\\% action accuracy and 41.1\\% completion rate on \\datasetname, with an input accuracy of 40.0\\%.\nWithout MoTiF \\cite{motif} data (\\textit{``No Mo''}), the fine-tuned model can achieve 51.9\\% action accuracy, and the inputting accuracy is 0\\%. %However, after incorporating a small portion of MoTiF \\cite{motif} data, the input accuracy increases to 40.0\\%. This indicates that by including a portion of data containing input actions, the model can learn to input.\nWe observe that when there are no CoT and no MoTiF data (\\textit{``No Mo\\&CoT''}), the fine-tuned LLM can achieve a high accuracy rate with simple click actions, and it can generally handle tasks that involve only clicking. However, once the MoTiF dataset is introduced (\\textit{``No CoT''}) to teach the LLM additional types of actions (such as input and task completion judgments), the LLM is heavily misled by the completion of judgment tasks. As a result, it outputs a significant number of ``task completed'' instead of selecting actions correctly. Consequently, the action accuracy drops from 51.9\\% to 20.6\\%.\n\\vspace{-0.3cm}\n\\subsection{Cost Analysis}\n\\label{eval:overhead}\n\n\\textbf{Runtime Cost.}\n\\name reduces runtime overhead by addressing two aspects: reducing the number of tokens per query and minimizing the frequency of query. In Figure \\ref{fig:prompt_length} (a), we show the count for each prompt length. Our baseline \\cite{talking_with_ui} \\shpd{includes} only visible leaf nodes in the UI tree, and contains 625.3 tokens within each prompt on average. \\name merges functionally equivalent nodes in the UI tree and further simplifies the expression of properties, reducing the token count by nearly half (339.0 on average). %There are two main benefits: firstly, reducing token length can significantly decrease model inference latency; secondly, if there is a need to call a large model API, it can reduce costs. This will reduce the inference latency of LLM and save costs for each API call (from \\$9.38 to \\$5.09 for every 10000 queries).\nThere are two main benefits: (i) Reducing token length can significantly decrease the model's inference latency. (ii) For calling on-cloud LLM API, it can reduce costs. For example, for GPT-3.5 and GPT-4, the cost can be reduced from \\$0.938 and \\$18.76 to \\$0.509 and \\$10.17 every 1000 queries respectively on average.\n\nIn Table \\ref{tab:inference_lat}, \\shpd{we} randomly select five baseline prompts and find the corresponding prompts optimized by \\name. \\wh{We measure their real latency on the Vicuna-7B \\cite{vicuna2023} deployed on the smartphone as well as on the edge server. Our optimized prompt reduces inference latency by 21.3\\% on average.} Note that the inference latency of LLMs on the smartphone and the edge server primarily depends on the number of output tokens. Therefore, when deploying the LLM on a mobile device, we do not require the LLM to output the Chain-of-Thought but rather output in the original manner shown in Figure \\ref{fig:prompt}. In the case of \\textit{$P_5$}, due to the excessive length of the baseline, it was truncated after outputting only one word, resulting in minimal inference latency.\n\nFigure \\ref{fig:prompt_length} (b) shows the component of per-step latency of \\name. \\wh{The Vicuna-7B model is deployed on the smartphone and on the edge server. }On-cloud GPT-3.5 and GPT-4 models are accessed by making API calls. The embedding model \\cite{instructor-xl} is deployed on an edge 1080 Ti GPU with 11 GB memory.\n\\wh{Note that the latency in calling GPT-3.5 and GPT-4 is significantly influenced by network conditions, server load, and so on. Therefore, we \\shpd{make 10 measurements to calculate the average latency}, but there still remains a considerable degree of instability. }\n\\wh{Calling LLMs (\\textit{``LLM''}) accounts for the majority of the latency, with 42.1\\%, 51.9\\%, 77.6\\%, and 87.1\\% of the latency based on GPT-3.5, Vicuna-7B (on-server), GPT-4, and Vicuna-7B (on-device) respectively. }Therefore, reducing LLM calls can largely reduce the end-to-end overhead. Besides, Embedding the task and searching the most similar UI element  (\\textit{``Embed''}) account for only 1.7\\% of the overhead, and only needs to be executed once for every task. Therefore, the overheads of finding the shortcuts and memory injection are acceptable.\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.48\\textwidth]{figure/overhead.pdf}\n    \\vspace{-0.7cm}\n    \\caption{Overhead of \\name and LLM-framework \\cite{talking_with_ui}. Left: The number of prompts with different token counts. Right: The component of per-step latency of \\name based on 3 LLMs respectively. \\wh{Vicuna-E: Vicuna-7B deployed on the edge server. Vicuna-S: Vicuna-7B deployed on the smartphone.} }% Vi: Vicuna-7B (deployed on the OnePlus ACE 2 Pro smartphone based on MLC-LLM \\cite{mlc-llm}). GPT-3.5 and GPT-4 are accessed by making API calls directly}}\n    \\label{fig:prompt_length}\n    \\vspace{-0.3cm}\n\\end{figure}\n\n\\begin{table}\n \\caption{Per-step inference cost of \\name with Vicuna-7B deployed on the OnePlus ACE 2 Pro smartphone. LLM-F: LLM-framework \\cite{talking_with_ui}. $P_{1\\sim5}$: Five random prompts from LLM-framework.}\n    \\vspace{-0.1cm}\n    \\centering\n    \\resizebox{.45\\textwidth}{!}\n    {\n        \\begin{tabular}{crrrrr}\n\t\t\\toprule\n\t\t\\cmidrule(r){1-2}\n            Prompt length / Inference latency     & $P_1$  & $P_2$  & $P_3$  & $P_4$  & $P_5$    \\\\\n\t\t\\midrule\n            LLM-F input length (token)    &252  &401 &460  &559   &  719 \\\\\n\t\t  \\name input length (token)    &299  &280 &233  &177   &  233 \\\\\n            \\midrule\n\t\tOn device LLM-F latency (s)             &40.6  &50.2 &63.9  &64.9 &  36.0  \\\\\n            On device \\name latency (s)   &39.7  &30.8 &33.9  &39.7 &  22.7 \\\\\n            \\wh{On cloud LLM-F latency (s)}&\\wh{4.4}  &\\wh{5.5} &\\wh{6.4}  &\\wh{16.1} &  \\wh{6.5} \\\\\n            \\wh{On cloud \\name latency (s)} &\\wh{4.2}  &\\wh{8.8} &\\wh{4.9}  &\\wh{5.3} &  \\wh{5.6} \\\\\n            \n\t\t\\bottomrule\n\t\\end{tabular}\n\t}\n \\label{tab:inference_lat}\n \\vspace{-0.2cm}\n\\end{table}\n\nWe also conducted experiments on saving the number of calls based on merging GUI and shortcuts. \nOn average, \\name reduces LLM calls by 1.2 per task resulting in an overall decrease of 13.7\\% in the total number of calls using the GUI merging technique.\nOur shortcuts correctly guide LLMs in 75\\% of cases. Considering only the correct shortcuts, we save 38.02\\% of the number of steps, with an average savings of 1.73 steps per task. \n\n\\textbf{Offline Preparation Cost.}\nFor every app, it takes about 0.5-1 hour to generate the UI Transition Graph (UTG), which is then analyzed to synthesize simulated tasks based on LLMs, taking about 5-10 minutes. Finally, the simulated tasks are mapped into high-dimensional vectors by an embedding model \\cite{instructor-xl} for runtime lookup, which typically takes about 10 seconds on a desktop computer. The offline preparation is a one-time process and does not need to be performed again at runtime.\n\n\\subsection{Influence of Security/Privacy Filtering}\n\\label{eval:security_privacy}\n\nWe ask the annotators of \\datasetname to also determine whether each action could potentially change the state of the user or the app. If so, we consider the action to be risky and prompt the user to confirm whether to proceed with \\shpd{the action}. We evaluate \\name's accuracy in detecting risky actions in five apps that may contain risky actions (contacts, dialer, SMS messenger, clock, and calendar). We consider risky actions as positive examples and \\name achieved a precision of 75.0\\% and a recall of 80.5\\%. We further show the influence of adding privacy information replacing and security confirmation into the prompt in Table \\ref{tab:dangerous_action}. When privacy replacement and security confirmation are added, a decrease in accuracy and completion rate can be observed, which is acceptable.\n\n    \n\n\\begin{table}\n \\caption{Action accuracy and completion rate of \\name based on GPT-4 with privacy information \\shpd{replacement} and security confirmation on 5 apps in \\datasetname. Priv: Privacy information \\shpd{replacement}, Sec: Security confirmation.} %\\yuanchun{would be good to show completion rate as well}}\n    \\vspace{-0.1cm}\n    \\centering\n    \\resizebox{.35\\textwidth}{!}\n    {\n        \\begin{tabular}{ccccc}\n\t\t\\toprule\n\t\t\\cmidrule(r){1-2}\n  \n            Metric            & Original  & +Priv  & +Sec  & +Priv\\&Sec    \\\\\n\t\t\\midrule\n\t\t  Acc               &  92.9\\%   &  89.9\\%  &  89.9\\%   &  89.9\\%  \\\\\n\t\tCompletion        &  75.4\\%   &  69.9\\%  &  68.5\\%   &  69.9\\%  \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\t}\n \\vspace{-0.4cm} \n\n \\label{tab:dangerous_action}\n\\end{table}\n\n\\section{Related Work}\\label{sec:related_work}\n\n\\wh{\\textbf{UI Understanding and Automation. }\\shpd{There has been growing interest in using machine learning techniques to comprehend and summarize user interfaces, enabling use cases such as accessibility and task-oriented bots.} Key areas of research include: 1) Semantic analysis of GUIs to summarize functions \\cite{vut, spotlight}, interpret UI elements' purposes \\cite{li-etal-2020-widget, screen_recognition}, and address user questions related to the GUI \\cite{screen2words, talking_with_ui}. It is crucial for various interaction tasks such as UI automation and accessibility.\n2) Mapping user instructions to UI elements \\cite{seq2act, metagui, kite}. These methods aim to to select the most relevant GUI elements for given tasks. \n3) Mobile UI task automation \\cite{responsible_task_automation,wen2023droidbot-gpt}. These methods build agents to complete tasks for users by performing actions on the GUI. \n\\name, on the other hand, leverages the UI transition memory to complete complex, multi-step tasks on smartphones. The memory can help agents to understand richly informative UIs and the usage of apps, and augment the LLMs in reasoning and planning.\nAfter the first release of \\name, there were various LLM-based UI agents proposed, which had been comprehensively summarized in a recent survey \\cite{li2024personal}.}\n\n\\textbf{Augmented LLM.}\nAlthough LLMs excel in tasks like question answering and text generation, they are still constrained by the information they can store in their fixed set of weights and context length. Therefore, researchers are augmenting LLMs with different tools, such as web browser \\cite{webgpt, mind2web}, APIs \\cite{chamelon, gorilla}, and other DNN models \\cite{hugginggpt}. % These works often rely on cleverly designed prompts to improve the reasoning ability of LLMs or fine-tune LLMs on domain-specific dataset. \nUnlike existing approaches that often depend on public APIs, our method does not require custom APIs, which are uncommon in mobile applications.\n\n\\vspace{-0.25cm}\n\\section{Discussion}\\label{sec:discussion}\n\\textbf{Randomness of LLMs.} We can set the `temperature' hyperparameter to 0 for consistent responses. But setting temperature too small will \\shpd{inhibit} innovative answers,  \\shpd{thereby potentially reducing} the performance of our system. In our experiments, we set the temperature to 0.25. And we observe a 2.1\\% accuracy \\shpd{reduction} when we set the `temperature' of GPT-3.5 to 0. \\shpd{Conversely}, increasing the temperature to 0.7 boosted action accuracy by 3.8\\%.\n\n\\textbf{Increased latency} limits the practical use of \\name. Our work could be extended by a collaborative approach between LLMs and smaller models. %Since invoking LLMs is resource-intensive, \nWe could call LLMs only once for each task to create a guideline based on the filtered domain-specific knowledge about the app. Subsequently, smaller models could be employed to associate these guidelines with UI elements \\cite{seq2act, metagui}. %Additionally, an instruction cache could be introduced to remember and replicate frequent user commands. This would allow for the execution of tasks similar to frequent commands without the need to invoke LLMs again, potentially reducing latency.\n Introducing an instruction cache could further reduce latency by storing and reusing common commands, minimizing the need for repeated LLM invocations.\n\\vspace{-0.25cm}\n\\section{Conclusion}\nWe present an LLM-powered mobile task automation system that can support arbitrary tasks without manual efforts.\nExperiment results have shown that our method can achieve effective task automation, outperforming existing training-based and LLM-based baselines.\nWe believe that the synergy between the commonsense knowledge of LLMs and domain-specific knowledge in mobile apps can potentially bring truly intelligent and helpful personal assistants into reality.\n\\section*{Acknowledgement}\n\\label{sec:acknowledgement}\nThis work is supported by the National Key R\\&D Program of China (No.2022YFF0604501), NSFC (No.62272261), and Tsinghua University (AIR)--AsiaInfo Technologies (China) Inc. Joint Research Center.\n\n\\balance\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{VisionTasker: Mobile Task Automation Using Vision Based UI Understanding and LLM Task Planning}\n\n\\begin{document}\n\n\\title{VisionTasker: Mobile Task Automation Using Vision Based UI Understanding and LLM Task Planning}\n\n\\author{Yunpeng Song}\n\\orcid{0000-0002-4186-0408}\n\\affiliation{%\n  \\institution{MOE KLINNS Lab}\n  \\country{Xi'an Jiaotong University}\n}\n\\email{yunpengs@xjtu.edu.cn}\n\n\\author{Yiheng Bian}\n\\orcid{0009-0004-8628-1751}\n\\affiliation{%\n  \\institution{MOE KLINNS Lab}\n  \\country{Xi'an Jiaotong University}\n}\n\\email{yhbian@stu.xjtu.edu.cn}\n\n\\author{Yongtao Tang}\n\\orcid{0009-0007-9941-8456}\n\\affiliation{%\n  \\institution{MOE KLINNS Lab}\n  \\country{Xi'an Jiaotong University}\n}\n\\email{tyt18050701377@stu.xjtu.edu.cn}\n\n\\author{Guiyu Ma}\n\\orcid{0009-0000-9910-5301}\n\\affiliation{%\n  \\institution{MOE KLINNS Lab}\n  \\country{Xi'an Jiaotong University}\n}\n\\email{guiyu.ma@stu.xjtu.edu.cn}\n\n\\author{Zhongmin Cai}\n\\authornote{Corresponding author.}\n\\orcid{0000-0002-4903-3992}\n\\affiliation{%\n  \\institution{MOE KLINNS Lab}\n  \\country{Xi'an Jiaotong University}\n}\n\\email{zmcai@sei.xjtu.edu.cn}\n\n\\renewcommand{\\shortauthors}{Song et al.}\n\n\\begin{abstract}\nMobile task automation is an emerging field that leverages AI to streamline and optimize the execution of routine tasks on mobile devices, thereby enhancing efficiency and productivity. Traditional methods, such as Programming By Demonstration (PBD), are limited due to their dependence on predefined tasks and susceptibility to app updates. Recent advancements have utilized the view hierarchy to collect UI information and employed Large Language Models (LLM) to enhance task automation. However, view hierarchies have accessibility issues and face potential problems like missing object descriptions or misaligned structures. This paper introduces VisionTasker, a two-stage framework combining vision-based UI understanding and LLM task planning, for mobile task automation in a step-by-step manner. VisionTasker firstly converts a UI screenshot into natural language interpretations using a vision-based UI understanding approach, eliminating the need for view hierarchies. Secondly, it adopts a step-by-step task planning method, presenting one interface at a time to the LLM. The LLM then identifies relevant elements within the interface and determines the next action, enhancing accuracy and practicality. Extensive experiments show that VisionTasker outperforms previous methods, providing effective UI representations across four datasets. Additionally, in automating 147 real-world tasks on an Android smartphone, \\textcolor{black}{VisionTasker demonstrates advantages over humans in tasks where humans show unfamiliarity} and shows significant improvements when integrated with the PBD mechanism. \\textcolor{black}{VisionTasker is open-source and available at \\url{https://github.com/AkimotoAyako/VisionTasker}.}\n\\end{abstract}\n\n\\begin{CCSXML}\n<ccs2012>\n   <concept>\n       <concept_id>10003120.10003121.10003124.10010865</concept_id>\n       <concept_desc>Human-centered computing~Graphical user interfaces</concept_desc>\n       <concept_significance>500</concept_significance>\n       </concept>\n </ccs2012>\n\\end{CCSXML}\n\n\\ccsdesc[500]{Human-centered computing~Graphical user interfaces}\n\n\\keywords{Mobile Task Automation, Large Language Models}\n\n\\maketitle\n\n\\section{Introduction}\n\nSmartphones have become indispensable in our daily routines, handling tasks like managing emails, paying bills, and ordering food. However, for individuals with sensory or motor limitations, such as restricted hand mobility, or in scenarios demanding divided attention like driving, smartphones can become less accessible despite their intuitive interfaces. Challenges also arise in repetitive tasks, such as setting up numerous calendar entries or sending similar messages to multiple contacts, which are quite common in both daily usage and mobile app testing scenarios. Additionally, interacting with different Apps can be confusing and prone to errors for elderly users, particularly when navigating complex menu systems. Consequently, investigating methods to augment smartphone intelligence for task automation that aligns with user intent is a valuable pursuit, promising significant enhancements in usability and productivity.\n\nPrevious research has explored mobile task automation through Programming By Demonstration(PBD) \\cite{li2017sugilite,li2018kite,sereshkeh2020vasta,riva2021etna,vu2023voicify}. This technique records user actions to create scripts for later use. However, PBD is restricted to tasks users have previously demonstrated and necessitates manually defining task intents and parameters for modification. Updates to apps, which alter workflows or interfaces, may largely affect the effectiveness of PBD. Researchers have also delved into training a multi-modal model to decipher the relationship between user commands and actions based on large-scale datasets in an end-to-end manner \\cite{li2020mapping,sun2022meta,li2023spotlight,zhan2023you}. Yet, constructing large, high-quality datasets is challenging, and current datasets frequently contain human trials in task execution, which are random and largely irrelevant to the executed task, leading to many incorrect samples. Furthermore, the task planning skills that models acquire from these datasets are usually confined to known instructions and interfaces, showing a lack of adaptability to new situations. More recent approaches have leveraged the reasoning abilities of Large Language Models to enhance task automation~\\cite{wang2023enabling,wen2023empowering}. Prior works primarily employ view hierarchies to gather UI information. However, view hierarchies are not always accessible and are prone to issues like missing object descriptions or misaligned structures.\n\nConsidering the variety of tasks and the flexibility of app interfaces and workflows, let's step back and examine how humans typically complete tasks using mobile apps. Effective app design hinges on intuitive interaction, encapsulating principles such as \"recognition rather than recall\"~\\cite{nielsen1994enhancing}. This principle suggests that users should be able to easily understand and use an app, even a new one, based primarily on UI cues.\nAdditionally, humans often tackle complex problems by dividing them into smaller sequential subtasks. Take, for example, the process of paying an electricity bill using a mobile app. A user, perhaps unfamiliar with the app, would start by exploring its UI. She then may find a section labeled ``City Services'', which seems relevant to bill payment. Delving into it, they may next navigate to ``Utilities'' among various presented services, and finally locate and select the option for electricity bill payments. This process reflects step-by-step reasoning, where the user focuses on identifying relevant elements from the current UI and utilizing them to advance the task. Inspired by this approach, we pose the question: Is it possible to automate mobile tasks through a human-like strategy of sequential visual recognition and planning when interacting with interfaces?\n\nTo address this, we introduce VisionTasker, a two-stage framework designed for task automation that emphasizes vision-based UI understanding and task planning.\nThe first stage involves analyzing a UI screenshot and extracting semantic information. This includes building models to detect UI elements, recognize text, group them into semantically meaningful blocks based on visual layout, and output the UI interpretation using the natural language. In the second stage, we employ LLMs for task planning. Since directly generating action sequences to complete the task can lead to impractical instructions due to ``hallucination''~\\cite{maynez-etal-2020-faithfulness}. Utilizing the capability of vision based UI understanding, we introduce a step-by-step approach, presenting a description of current interface image at each step of the task execution to the LLM and asking it to identify relevant elements within the interface and determine the next move. This strategy not only provides context but also ensures decisions are based on actual UI elements, leading to more accurate instructions. Additionally, we incorporate a Programming By Demonstration (PBD) mechanism to assist the LLM in complex task planning situations.\n\nWe implement our approach using public GUI datasets for training the UI understanding models, and the off-the-shelf ERNIE Bot as LLM navigator. We conduct extensive experiments to evaluate the performance and challenges of our method, and the results demonstrate that, VisionTasker, our vision-based UI understanding scheme surpasses previous methods, offering effective UI representations across four public datasets. Moreover, our method exhibits impressive performance, comparable to that of human abilities, in automating 147 real-world tasks on an Android smartphone. \\textcolor{black}{It shows advantages over humans in handling unfamiliar tasks and significant improvement when combined with the PBD mechanism.} The main contributions of this paper are as follows:\n\\begin{itemize}\n    \\item We propose VisionTasker, a two-stage framework combining vision-based UI understanding and LLM task planning, for mobile task automation in a step-by-step manner. Our approach eliminates the need for accessing view hierarchies to represent the UI and a vast dataset to train large models.\n    \\item We tailor a scheme of vision based UI understanding and devise a systematic procedure to transform the UI screenshot into expressive UI layout semantics in the form of natural language. The scheme can be further enhanced by incorporating the Programming by Demonstration (PBD) mechanism. \n    \\item Through comprehensive experiments, we demonstrate the effectiveness and challenges of our approach in automating tasks across screen sizes, task types, and levels of complexity. \n\\end{itemize}\n\n\\section{Related Work}\n\n\\textcolor{black}{Past approaches to automating tasks on mobile devices fall into three main categories: first, watching how users interact with their devices to create scripts that can repeat similar actions; second, using large datasets to train custom models that understand screen content and link instructions to UI elements; and third, using LLMs and multimodal LLMs to plan tasks.}\n\n\\subsection{Programming by Demonstration}\n\nProgramming by Demonstration (PBD) offers an intuitive way to automate tasks on smartphones. This method typically captures users' actions to create scripts that can be activated later. For example, SUGILITE~\\cite{li2017sugilite} performs a pioneering study utilizing the Android accessibility API to grasp the app's UI structure and using a conversational UI for script creation and task execution. Appinite~\\cite{li2018appinite} extends this by incorporating Natural Language Understanding (NLU) to better recognize UI elements in response to user commands. Further advancements are seen in systems like Etan~\\cite{riva2021etna} and KITE~\\cite{li2018kite}, which introduce task-oriented bots assisting in template creation for app functions. AutoVCI~\\cite{pan2022automatically} improves user guidance by asking questions to pinpoint target actions and parameters. VASTA~\\cite{sereshkeh2020vasta} enhances script robustness by processing user queries in natural language, allowing for flexible task descriptions and triggering appropriate scripts. ParamMacros~\\cite{krosnick2022parammacros} enables users to generalize queries and identify parameters through text annotation, thus broadening systems' applicability. However, these methods typically rely on precise, user-defined scripts, making them vulnerable to changes in the GUI and workflow from the demonstration to the execution phase. This reliance means only the exact procedures converted into scripts can be replayed, limiting flexibility and adaptability.\n\n\\subsection{Understanding the Interfaces}\n\nPrevious research has focused on understanding the entire UI screen by converting it into a vector, which is then utilized for tasks like retrieving screens and classifying apps~\\cite{li2021screen2vec,Fu2021UnderstandingMG,ang2022learning}. Studies have also taken a more detailed approach, such as extracting information like object attributes and structural details of the UI~\\cite{zhang2021screen,wu2021screen,li-etal-2020-widget,xie2022psychologically}, \\textcolor{black}{recognizing non-text icons~\\cite{chen2020object,chen2022towards,he2021actionbert}, and presenting the entire UI using textual representations~\\cite{wang2021screen2words,leiva2022describing,baechler2024screenai}. }\n\n\\textcolor{black}{The creation of large-scale datasets linking UIs with text descriptions/instructions has led researchers to focus on training vision-language models for mobile interface understanding~\\cite{li2020mapping,sun2022meta,burns2022dataset,venkatesh2022ugif,rawles2023androidinthewild}. These models primarily aim to connect UI elements with specific text instructions.} Li et al.~\\cite{li2020mapping} use a dual-transformer approach to link text and images based on a dataset of 295k synthetic single-step UI-instruction pairs. Similarly, ILuvUI~\\cite{jiang2023iluvui} uses a large dataset of 353k text-image samples, along with visual and text encoders and a LLM as the decoder, to generate task planning responses. Li et al.~\\cite{li2023spotlight} collected a dataset of 2.5 million screens with view hierarchies to train a vision-language model that can determine if a target region matches a one-step natural language command. Rawles et al.~\\cite{rawles2023androidinthewild} and Zhan et al.~\\cite{zhan2023you} use the AITW dataset, which includes 30k instructions and corresponding UI sequences, to train multimodal transformers that predict the next action needed to complete a task. Yet, these models are heavily shaped by training data, and current datasets frequently contain human trials in task execution, which are largely irrelevant to the target task, leading to many incorrect samples. Besides, the planning capability that models acquire from these datasets are confined to known instructions and UIs, showing a lack of adaptability to adjustments in the UI and processes, e.g., introduced through app updates.\n\n\\subsection{Automation with Large Language Models}\n\nResearchers also explore leveraging the generic knowledge and reasoning ability of LLM for automation. This approach typically involves a two-step process: identifying UI elements and using an LLM to infer the elements that require interaction. \nWang et al.~\\cite{wang2023enabling} conduct the pioneering research to transform the view hierarchy into an HTML-like structure for using LLMs to enable conversational interaction on mobile UIs. AutoDroid~\\cite{wen2023empowering} combines LLMs and app-specific knowledge in a novel way by generating a UI Transition Graph through offline exploration. \nResponsibleTA~\\cite{zhang2023responsible} leverages LLMs to plan the entire task action sequences, supplementing this with an auxiliary model to ensure the viability, completeness, and security of the plans. Meanwhile, MM-Navigator~\\cite{yan2023gpt} employs the SegmentAnything Model (SAM)~\\cite{kirillov2023segment} to dissect UI into identifiable segments, each marked with an ID, and combines this with user tasks in GPT-4V to generate relevant instructions. \n\\textcolor{black}{Recent work has also applied LLMs to automated GUI testing~\\cite{liu2023chatting}, accessibility testing~\\cite{taeb2024axnav}, and bug replay~\\cite{feng2024prompting}.} These works rely on view hierarchies or general models like SAM to identify UI elements, which can be impractical or yield low data quality. \n\n\\textcolor{black}{Recent advances have also led to multimodal LLMs capable of handling various UI tasks, including answering questions and automating processes. These models, such as Ferret UI~\\cite{you2024ferret}, CogAgent~\\cite{hong2024cogagent}, and Fuyu~\\cite{ADEPT}, take a different approach by combining UI understanding and task planning in a single stage. While these models show promise, their performance still needs improvement, especially in accurately grounding UI elements.}\n\n\\section{Overview of Our Method}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=1\\linewidth]{figures/framework.pdf}\n    \\caption{The workflow of VisionTasker.}\n    \\label{fig:framework}\n    \\vspace{-6mm}\n\\end{figure}\n\nOur goal is to develop an intelligent agent capable of performing human-like actions to automate mobile tasks in a step-by-step manner, as shown in Fig~\\ref{fig:framework}. Users will simply express their desired tasks in natural language. In response, our agent takes screenshots, comprehends the UI's current semantic context, and devises a strategic action plan. It then executes low-level interactions—such as taps and swipes—to advance the task autonomously. After executing each action, the agent reviews the new screen and previous actions to guide its subsequent move. This iterative process persists until the task is complete or some action limit is reached. Meanwhile, users are liberated from hands-on interactions, yet they can still monitor the task's progression through visible cues, like text inputs or option selections. Importantly, users retain the power to halt the process at any moment, ensuring their control over the agent. \n\nA large portion of previous works on mobile task automation rely on view hierarchies, which are a kind of Android UI metadata containing a tree-based representation of an application's UI, as a source for the description of UI. However, view hierarchies are not always accessible and can suffer from issues like incomplete object descriptions or misaligned structural information~\\cite{ross2018examining,li2022learning,XDA}, which may hinder the model's effectiveness and generalization. Figure~\\ref{fig:vh_errors} shows some issues that can arise when using view hierarchies in common apps, including missing elements or descriptions, empty view hierarchies, wrong descriptions, and the inclusion of non-existing elements.\n\nOur approach's viability hinges on the principle that a well-crafted UI, being expressive and informative, allows even those unfamiliar with it to grasp its semantics and intuitively navigate the application. This principle is based on the alignment of UI design's semantics and logic with users' mental models~\\cite{nielsen1994enhancing}. In accordance with this, we simulate human comprehension of the UI by integrating the visual layout and the content of UI elements, as identified or extracted by our vision models, to output the UI's semantics. Furthermore, the recent advancements in LLMs, propelled by their extensive training on vast datasets of human-generated text and their significant size, have equipped LLMs with the ability to mimic human knowledge acquisition and reasoning processes. Leveraging this, our intelligent agent uses an LLM as a navigator for human-like task planning, applying the UI semantics decoded by vision models to translate high-level human instructions into step-by-step machine commands effectively.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=1\\linewidth]{figures/vh_errors.pdf}\n    \\caption{Cases where using view hierarchies for UI description is problematic. Screenshots from Facebook, Uber, Spotify, Temu, Shopee, and Shein, each with 100M+ installs.}\n    \\label{fig:vh_errors}\n\\end{figure}\n\nThe intelligent agent comprises three key components: the UI understanding module, the task planning module, and the execution module. In the UI understanding module, components like widget detectors and text recognizers analyze screenshots to identify widgets, text, and their locations. These elements are then grouped based on the visual UI layout and output in natural language, forming distinct blocks to convey varied semantics for task planning. The task planning module relies on a LLM to break down and plan tasks into steps. Taking input in the form of a prompt containing task description, action history, and, UI semantics, it generates specific low-level actions required in the current UI to advance the task, expressed in plain language, such as ``tapping the Settings button on the current UI''. The execution module interprets these text-based actions into corresponding operation commands by associating the objects with the UI elements provided by the UI understanding module and determining the coordinates of the operations, like ``sending a touch-down event at coordinates (X, Y)''.\n\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=1\\linewidth]{figures/eng_UI_understanding.pdf}\n    \\caption{The process of vision based UI understanding.}\n    \\label{fig:ui_understanding}\n\\end{figure*}\n\n\\section{UI Understanding}\nUnlike previous methods that depend on view hierarchy or accessibility services to access UI elements, our module directly examines UI screenshots. This approach enables us to tap into the extensive semantic data within the UI's visual structure comprehensively. It detects text, buttons, and various widgets, taking into account the dynamic and interactive features of the UI, and organizes them into intelligible, meaningful clusters. The output of this module is a detailed description of the UI semantics in natural language. The whole process is shown in Fig~\\ref{fig:ui_understanding}.\n\n\\subsection{Widget Detection}\n\\label{sec:widget}\nUnderstanding the UI first requires accurately identifying and recognizing its various graphic widgets. This is also essential for grasping the action space of the interface and performing actions such as tapping certain buttons or typing in text boxes. To accurately recognize the varied widgets within the UI, we carefully retrain the state-of-the-art model, YOLOv8~\\cite{Jocher_YOLO_by_Ultralytics_2023}, specifically for UI object detection, leveraging open-source frameworks to optimize its performance for UI understanding. YOLOv8 employs a Convolutional Neural Network architecture and has undergone training on a substantial dataset of photos from everyday scenarios, allowing it to adeptly detect and recognize objects like apples, bikes, and buses. However, as UI significantly differ from everyday scene photos, and UI elements don't align directly with the class labels in these models, we craft a training set tailored for UI using publicly available datasets RICO~\\cite{deka2017rico} and VINS~\\cite{bunian2021vins}, and retrain the models. The output of the models consists of all the detected elements with their class labels and bounding box coordinates. Given the long-tail distribution of widgets in UI, at this stage, we focus on the 12 most common classes, including status bar, navigation bar, button, edit text, image, page indicator, seek bar, rating bar, check box, radio group, spinner, and switch.\n\nOwing to the uneven distribution of samples across the 12 classes in the dataset (for example, a large number of buttons but a scarcity of text boxes), we employ a synthetic UI approach for data augmentation. In the first step, we create a canvas with a \\textcolor{black}{random} color, add a status bar and a navigation bar by stretching them horizontally to fit the canvas size, and position them at the top and bottom to serve as the UI background. Then, elements are sequentially added to the canvas. For example, to address the Page Indicator, we catalogue every occurrence, assess the distribution of their placements within the UI landscape of our dataset, and proceed to randomly augment characteristics such as brightness, contrast, color balance, and transparency of a chosen Page Indicator. This modified widget is then positioned according to its observed distribution. \n\\textcolor{black}{In total, we synthesize 4856 UI images using element patches from various real UI images. The augmented elements include five types that are less presented in the dataset: ratio group, page indicator, spinner, seek bar, and rating bar.} The resultant synthetic UIs are then merged into our training dataset. By injecting controlled variability into the UI generation process, this method not only compensates for the scarcity of certain elements in the original dataset but also significantly enriches the dataset's diversity, thereby broadening the training spectrum for underrepresented elements.\n\n\\subsection{Text Recognition}\nIn addition to graphic widget elements, user interfaces typically include rich textual information. To extract text and its location from screenshots, we employ an implementation of the open-source PaddleOCR framework\\footnote{https://github.com/PaddlePaddle/PaddleOCR}, which consists of two main components: text detection and text recognition. The text detection component identifies text regions within input images using the PP-LCNetV3 model. This model is extensively trained with text images, allowing it to accurately locate text, even when it varies in terms of position, orientation, or size within the image. Meanwhile, the text recognition module is responsible for extracting specific text content from the regions identified by the detection module. It performs OCR processing on these detected text regions, utilizing the SVTR model~\\cite{ijcai2022p124} to handle sequential data. Through the connectionist temporal classification loss~\\cite{graves2006connectionist} and GTC optimization method~\\cite{hu2020gtc}, it accomplishes character-level recognition of text regions and assembles them into complete text content. By combining these two components, we can obtain both the text and its corresponding location within the screenshot. It's important to note that due to the presence of various graphical patterns or icons in UI, which can potentially interfere with text recognition, we have set a high threshold for recognized texts. Only content recognized with a confidence value exceeding 0.95 is recorded as text.\n\n\\subsection{Semantic Grouping}\nWidget detection and text recognition enable identifying the types of interactive widgets, extracting the content of the text, and determining the positions of each element. This section will explore how to integrate these elements into a cohesive and semantic whole based on the UI layout.\n\n\\subsubsection{Text-Widget Matching}\nThe appearances and semantics of low level graphic widgets are closely related to the high level design of the app's UI. Consequently, even widgets with the same function may exhibit different styles in varied applications, and conversely, widgets with similar styles may serve different purposes. To alleviate user confusion stemming from these variations, important widgets such as buttons in the UI are typically accompanied by explanatory text nearby, providing explanations and cues for their functions. Therefore, it is crucial to correctly align the widgets with the associated text in the interface to retrieve true semantics of various widgets.\n\nOur approach to text-widget matching leverage an observation that when humans encounter a collection of UI elements, they tend to treat elements in close proximity as a cohesive whole. This observation forms a key aspect of UI design known as the proximity principle~\\cite{wertheimer1938laws}, where items placed close together are perceived as more closely related than those spaced farther apart.\nBuilding on this principle, we use a proximity-based method to match texts with widgets inspired by~\\cite{xie2022psychologically}. For various widgets and texts identified, we utilize their bounding box coordinates to serve as representations for each element. We determine widget-text matches by calculating distances between a widget and its text neighbors, considering those with a distance below the maximum threshold. And a widget can match with one or more texts if they are aligned to maintain continuity. In certain instances, such as with app icons on the home screen, where only the widget portion is clickable, we merge widget-text matches by retaining only the widget. Despite its simplicity, this approach proves to be effective, facilitating the establishment of associations between widgets and textual elements. This, in turn, permits the leveraging of text data for precise widget interpretation, significantly improving the descriptiveness and quality of our annotations.\n\n\\subsubsection{Unmatched Button Interpretation}\n\nUnmatched buttons refer to buttons with no matched text. They are usually graphical elements that perform specific actions or lead to certain functionalities within the app or website but lack explicit text explaining their purpose. Human users are often expected to understand these buttons' functions based on their design, context within the UI, or common conventions (e.g., expressive icons such as a magnifying glass for search, a gear for settings). For unmatched buttons, an underlying design principle is the appearances or related icons are sufficiently familiar to users, negating the need for textual explanations. To effectively interpret unmatched buttons, we utilize the capabilities of CLIP, a powerful pre-trained multi-modal model. By fine-tuning CLIP~\\cite{radford2021learning}, we enhance its proficiency in inferring the functions of these buttons based on their visual design. This is particularly useful in our context as CLIP excels in understanding both images and text within a joint embedding space, enabling it to interpret various meanings of non-text-buttons in real-world UIs.\n\nWe employ CLIP to match a non-text-button with one text in a set of descriptions for common button functions such as ``Setting'', ``Send'', ``Submit'', etc. Let \\(V=\\{v_1,v_2,...,v_n\\}\\) denote the images of the buttons, and \\(T=\\{t_1,t_2,...t_n\\}\\) denote their corresponding text descriptions. Our goal is to fine-tune a CLIP model, aligning both the images and text within the same latent space. To achieve this, we utilize the IconSeer dataset~\\cite{li2023you}, a publicly available resource comprising over 180k buttons with text descriptions spanning more than 170 common categories for the fine-tuning process. We employ a contrast loss function:\n\\[\n\\mathcal{L} = -\\frac{1}{2N} \\sum_{j=1}^{N} \\left[ \\log \\sigma(v_j, t_{+,j}) + \\log \\sigma(v_{+,j}, t_j) \\right]\n\\]\nwhere \\( v_j \\) is the image embedding for the \\( j \\)-th pair, \\( t_{+,j} \\) is the matching text embedding for the \\( j \\)-th text, \\( \\sigma(a, b) \\) represents the softmax term \\(\\frac{\\exp(a \\cdot b / \\tau)}{\\sum_{i=1}^{N} \\exp(a \\cdot k_i / \\tau)}\\). Here, \\( N \\) is the batch size, and \\( \\tau \\) is a temperature scaling parameter. For rapid inference, we opt for a small model of ViT-B/32\\footnote{https://huggingface.co/sentence-transformers/clip-ViT-B-32}. \n\n\\subsubsection{Semantic Block Division}\nIn UI design, it's common to group similar functions and information in distinct sections, or ``blocks'', to make navigation easier and reduce the effort it takes for users to find what they need. These blocks are visually set apart from their surroundings, often using distinct colors to create clear border lines. By identifying such border lines in UIs, we can effectively pinpoint these semantic blocks. However, the varying orientations, lengths, and spacings of these lines in different designs present a challenge.\n\nTo address this, we employ a probability-based approach with a focus on gradient information for border detection, as shown in Alg.~\\ref{alg:border} in Appendix. First, we quantize the image by reducing its colors to the most predominant ones, based on the distribution of colors in the UI. This step helps to reduce visual noise and makes borders between groups more obvious. Next, we apply edge detection to identify the significant edges. We only retain line segment edges by regarding them as connected groups of pixels and use the direction and strength of their gradients to find them. Then, using a probabilistic approach, we group these segments by how close and similarly oriented they are to identify the borders of groups. Finally, we identify blocks or group areas typically as rectangles formed by intersecting or nearly intersecting horizontal and vertical lines, as shown in Fig.~\\ref{fig:ui_understanding}.\n\n\\subsubsection{Block Captioning and Active Tab Detection}\n\nFor a semantic block, there is usually a caption serving to clarify its logic, offering descriptions that improve accessibility, aid navigation, and highlight structure and interactivity, thereby providing important cues for UI understanding. To annotate captions, we examine the top area within the region to determine if there is left- or top-justified pure text content without an associated image. If such text exists, we consider it as the caption, providing additional context for the block's elements. This is especially useful in scenarios where captions fill in critical information gaps. For example, in the ticket booking UI, dates are usually shown using only the number of the day, while the corresponding name of the month is separately positioned at the top of the date list for each month. This design choice, while visually clean, can lead to misunderstandings for LLM navigators. They may recognize the month for the initial date described behind the month but fail to correctly associate subsequent dates with their respective months. \n\nFurthermore, our method can be further utilized to identify the active tab in current UI layout. At first, we identifies the application's tab bar as a unique semantic block, attributable to its distinctive visual characteristics such as color and border lines. To pinpoint the active page within this tab bar, we introduce a recognition method based on color differences, focusing on the H (Hue) component of the HSV color space. This choice stems from the challenge of recognizing color intensity differences solely through specific RGB components without prior knowledge of the exact color. In contrast, the H component is effective in representing color variations as perceived by humans. By analyzing the intensity distribution of the H component in the buttons, we pinpoint the one with the most significant difference in distribution compared to others, attributing to it the additional semantic of the ``currently selected element''.\n\n\\section{LLM for Task Planning}\nAfter extracting semantic information from the UI, this section discusses how to effectively utilize UI semantics to assist LLMs in task planning and introduces additional support mechanisms to address situations beyond the capabilities of LLMs.\n\n\\subsection{Planning Step by Step}\n\\label{sec:prompt}\n\nDrawing from expansive datasets, LLMs possess a wealth of knowledge and the capacity for complex reasoning~\\cite{brown2020language,chowdhery2023palm,wei2022chain}. Their proficiency in navigating multi-step tasks—like solving math problems—positions them as valuable navigators for automating processes. However, LLMs are not without shortcomings. At times, they may produce implausible content, a result of hallucination issues~\\cite{maynez-etal-2020-faithfulness} where they invent features not present in the current app to fulfill a task. To mitigate these issues, we've taken cues from the idea of a chain of thought~\\cite{wei2022chain} and adapted it into a chain of screens. This technique entails a sequential feedback loop, where the LLM uses the output of the previous action and the current UI to plan the subsequent step. This approach serves two crucial functions: it deepens the LLM’s grasp of the current interface, reducing the risk of confusion when confronted with similar interfaces, and it directs the model to select actions only from available UI elements.\n\nThe design of our prompt is streamlined into five essential parts for effective interaction with a LLM. Firstly, the \\textbf{role} part establishes the LLM's role as an intelligent agent to enhance task automation by identifying and interacting with UI elements appropriately. The LLM is required to pick the element on the current UI to advance the task. \n\nSecondly, the \\textbf{task description} allows users to articulate their objectives in natural language, such as ``Play 'Baby Blue' by Badfinger and add it to my favorites''. This is crucial for maintaining the focus of the LLM on the user's initial goal throughout the process.\n\nThirdly, the \\textbf{action history} part records the sequence of actions executed by the LLM at each UI step, such as ``Tap Settings button -> Tap WLAN button ->...''. Keeping a record of action sequences serves two purposes: it helps in situations where the same interface requires multiple actions, like changing the UI language, which involves selecting the target language and tapping the save button on the same page. Without recording the action sequence, the LLM might keep selecting the target language and overlook the save button. It also allows for documenting any explored but failed paths during the automation process, preventing the LLM from getting stuck in a useless loop. \n\nThe fourth part is \\textbf{UI semantics}. Contrasting with previous approaches that transform view hierarchies into HTML formats, our method emulates human cognitive processes, utilizing natural language descriptions of the UI’s semantic blocks to enhance decision-making. This description is structured from top to bottom and left to right, detailing each semantic block's title (optional) and UI element within, along with the element's category, functions, and IDs. \n\nFinally, the \\textbf{example} part is an optional and offers high-level task-solution pairs from the PBD mechanism or help documentations to improve the planning ability for related tasks.\n\nWhen the LLM generates responses, it specifies both the element and the required action, such as ``Tap SAVE button''. If the LLM suggests an action on an element not present in the current UI, it's prompted to reassess and select from the elements that are actually available. This approach reduces errors and keeps the LLM focused on realistic tasks. After receiving a valid response, the conversation with the LLM is terminated. A new dialogue is initiated with the above parts by the agent when planning the next step. During this process, the action history records past actions rather than re-documenting the semantic description of each interface. This strategy is designed to prevent an excessive increase in the number of tokens due to too many turns in the conversation.\n\n\\subsection{Programming by Demonstration}\n\\label{sec:pbd}\nIn addition to relying on the inherent knowledge and reasoning abilities of LLMs for task planning, we have implemented a mechanism of programming by demonstration to address situations where LLMs may not be able to plan the correct path. This involves our UI understanding module, which bridges the gap between a task's semantic elements and their physical locations on a screen. This module interprets simple touchscreen actions, like a tap on the screen, as meaningful steps in a task. For example, a tap at a certain spot on the screen could be understood as hitting the ``save'' button, turning human actions into a sequence of semantic steps for the LLM to follow.\n\nWhen the agent encounters difficulty in completing a specific user task within certain steps, it holds the task description and prompts the user for manual execution. The UI understanding module tracks each action on the screen and deciphers the higher-level meaning of those actions. For example, it might record a series of actions like ``tapping the WeChat icon on the home screen -> tapping the Plus button (indicating actions like More options, New, or Create) -> tapping the QR code button''. By integrating these user-shown steps into its prompts, the LLM can make more informed decisions. Unlike methods that just record and play back what the user does, our approach offers more adaptability. It comprehends actions at a more abstract level, allowing it to cope with changes in app layouts or updates that alter the interface. This adaptability ensures that the LLM remains effective even when facing app updates or screen resolution changes.\n\n\\section{Executing Module}\n\n\\textbf{Screenshots Capturing:} The initial step for the execution module is to capture screenshots for subsequent analysis. In fact, many app pages contain content that extends beyond a single screen view. To address this, the execution module first attempts to scroll to the bottom of the page or until a specified number of scrolls is reached (for pages that continuously load content as you scroll). Each scroll covers half the screen's length, and a screenshot is taken before each scroll. These screenshots are then stitched together to form a complete view of the current interface. During the stitching process, overlapping portions in the screenshots are identified and removed to reduce redundancy. This long screenshot is then sent into the UI understanding module to ensure we capture the entire content of a page, even if it spans multiple screen lengths.\n\n\\textbf{Command Parsing:} \nNext, the module receives instructions from the LLM in the form of a natural language description. These instructions are expected to consist of an action-object pair to guide the task forward, due to the specific instructions in the prompt. The actions are confined to tapping (including long press), entering specific text, swiping, and stopping, while the objects are usually corresponding UI elements or locations, such as ``Tap the Settings button'' or ``Enter pizza hut in the search bar''. The execution module examines whether the UI elements required by the LLM's output exist in the current interface, utilizing the list of UI elements from the UI understanding module. Should these necessary elements be absent, the execution module prompts the LLM for a reconsideration, based on the elements available in the current interface. In contrast, if the required elements are present, the execution module retrieves the bounding box coordinates of these elements for the subsequent step of execution. \n\n\\textbf{Action Execution:} In executing commands such as ``Tap the Settings button'', the execution module initially identifies the touchpoint on the screen using the retrieved coordinates of the Settings button's bounding box. The center of these coordinates is calculated to pinpoint the exact location for tapping. In instances where long screenshots result in coordinates extending beyond the screen size, it becomes necessary to compute the distance to swipe down and recalculate the coordinates for the post-swipe interface. Hence, the actual operation involves a sequence of actions, such as (Swipe, [$Y_{start}$, $Y_{end}$]) followed by (Tap, [$X$, $Y$]). These steps are converted into Android Debug Bridge\\footnote{https://developer.android.com/tools/adb} (ADB) commands, which are then transmitted to the smartphone to be carried out. Once executed, the command is recorded in the action history, facilitating the planning of subsequent steps. \n\n\\begin{table*}\n    \\centering\n    \\footnotesize\n    \\renewcommand{\\arraystretch}{1.2}\n    \\caption{Comparative analysis of three methods for UI Understanding.}\n    \\label{tab:7_1}\n    \\begin{tabular}{ccccccccccc}\n        \\toprule\n        \\multirow{2}{*}{Method} & \\multicolumn{2}{c}{A1. Element detection} & \\multirow{1}{*}{A2. Icon interpretation} &\\multicolumn{6}{c}{A3. UI  questions answering} & \\multirow{2}{*}{Token}\\\\\n        \\cmidrule(lr){2-3} \\cmidrule(lr){4-4} \\cmidrule(lr){5-10}\n        & Precision& Recall &   Accuracy   & Status & Content & Count & Functionality & Summary & Overall &     \\\\\n        \\midrule               \n        GPT-4V~\\cite{OpenAIGPT4V2023}&    -\\textsuperscript{$a$} &    -\\textsuperscript{$a$} & 0.83 & \\textbf{0.67} & 0.73 & 0.54 & 0.71 & 0.98 & 0.75 & 1105\\textsuperscript{$b$}\\\\\n        VH~\\cite{wang2023enabling}    & 0.37 & 0.68 & 0.63 & 0.20 & 0.73 & 0.74 & 0.56 & 0.92 & 0.63 & 1267 \\\\\n        \\rowcolor{lightgray!40}\n        Ours  & \\textbf{0.94} & \\textbf{0.94} & \\textbf{0.85} & 0.57 & \\textbf{0.91} & \\textbf{0.88} & \\textbf{0.90} & \\textbf{0.98} & \\textbf{0.87} & \\textbf{265}  \\\\\n        \\bottomrule\n        \\multicolumn{10}{l}{\\textsuperscript{$a$}GPT-4V cannot accurately output the specific coordinates of UI~\\cite{yang2023set}, hence it was not adopted in the A1 evaluation.}\\\\\n        \\multicolumn{10}{l}{\\textsuperscript{$b$}An image with dimensions of 1920x1080 consumes 1105 tokens in the GPT-4-1106-vision-preview model.}\\\\\n    \\end{tabular}\n\\end{table*}\n\n\\section{Evaluation}\nIn this section, we conducted extensive experiments to evaluate our approach in four key aspects: its accuracy in recognizing diverse UI elements and interpreting unmatched buttons, the effectiveness of our vision-based UI understanding module in enhancing accurate task planning, the approach's performance in real-world tasks, and the impact of the PBD mechanism in challenging scenarios faced by the LLM.\n\\subsection{Performance of UI Understanding}\n\\label{sec:7_1}\n\n\\aptLtoX[graphic=no,type=html]{\\begin{figure}\n        \\centering\n        \\includegraphics[width=1\\columnwidth]{figures/image_read.pdf}\n        \\caption{Our method finds more details and non-exist elements.} \n        \\label{fig:image_read}\n    \\end{figure}\n    \\begin{figure}\n        \\centering\n        \\includegraphics[width=1\\columnwidth]{figures/cases.pdf}\n        \\caption{Cases of correctly and incorrectly identified status.}\n        \\label{fig:cases}\n   \\end{figure}}{\\begin{figure*}\n    \\begin{minipage}[t]{0.52\\linewidth}\n        \\centering\n        \\includegraphics[width=1\\columnwidth]{figures/image_read.pdf}\n        \\caption{Our method finds more details and non-exist elements.} \n        \\label{fig:image_read}\n    \\end{minipage}\n    \\begin{minipage}[t]{0.47\\linewidth}\n        \\centering\n        \\includegraphics[width=1\\columnwidth]{figures/cases.pdf}\n        \\caption{Cases of correctly and incorrectly identified status.}\n        \\label{fig:cases}\n    \\end{minipage}\n\\end{figure*}}\n\nOur UI understanding method can analyze screenshots to identify the positions and functions of UI elements and provide a natural language description of the entire screenshot. We assess the performance of our method through three key dimensions: A1. accuracy in detecting UI elements, A2. the ability to interpret the meanings of unmatched icon-only buttons, and A3. the effectiveness of generated screen semantic descriptions for LLM to answer UI related questions. \n\n\\subsubsection{Dataset}\nOur evaluation employs a dataset of 136 screens from 31 widely-used applications, including 16 built-in system apps and 15 top apps from the app store, each with over a billion installs. Our selection aimed to capture a diverse array of layout patterns to enhance the diversity of interfaces, as depicted in Fig.~\\ref{fig:layout} in Appendix. To ensure our model's broad applicability and to prevent any overlap between training and testing datasets, we trained our vision-based detection and interpretation models on UI screenshots from English apps collected years ago from the Google App Store. In contrast, for testing, we used the most recent Chinese apps in the Huawei App Store. \n\nThree authors carefully annotated the selected screens, including identifying all textual and graphical elements by marking their bounding box coordinates (A1), annotating the function description of icon-only buttons (A2).\n\nFor UI question answering by LLM (A3), we design a series of question-and-answer pairs for each UI screen. Example questions are shown in Table~\\ref{tab:question_exp} in Appendix. The questions are crafted examining UIs from four perspectives: \n\\begin{itemize}\n    \\item \\textit{Status inquiry}: questions about the state of elements, e.g., the on or off status of the VPN.\n    \\item \\textit{Content questions}: inquiries regarding the text information displayed in text or images, e.g., the customer service numbers shown in the UI.\n    \\item \\textit{Item count}: requests for counting specific items displayed, e.g., the number of flights shown on a screen.\n    \\item \\textit{Functionality query}: questions concerning the execution of specific function within the UI, e.g., how to adjust the number of rooms in a booking.\n    \\item \\textit{Purpose summary}: questions aimed at summarizing the main function or purpose of the page based on its content.\n\\end{itemize}\n\nThrough manual annotation, we marked 4464 bounding boxes pinpointing element locations, 161 descriptive annotations for icon-only buttons, and 586 question-answer pairs. These pairs include 89 about status, 188 about content, 105 regarding functionality, 68 on item counts, and 136 on summaries.\n\n\\subsubsection{Metrics}\nTo evaluate the performance of our element detection model (A1), we utilized the common metrics of precision and recall at an IoU of 0.5 from the object detection domain~\\cite{bunian2021vins}. Precision reflects the proportion of predicted bounding boxes that accurately match manually annotated boxes, while recall measures the extent to which true bounding boxes are successfully predicted by the model. For A2 and A3, accuracy is used as the metric. Three authors independently reviewed the model generated answers and reached a consensus to resolve any discrepancies. \n\n\\subsubsection{Baselines}\nWe set two baselines for comparison, incorporating both a vision-based approach (GPT-4V~\\cite{OpenAIGPT4V2023}) and a view hierarchy (VH) based method ~\\cite{wang2023enabling}. GPT-4V is the state-of-the-art image-to-text model and exhibits remarkable visual comprehension and inference capabilities. Unlike two-stage processes that separate UI understanding from task planning, GPT-4V processes screenshots and instructions together as input, allowing it to respond to queries directly. However, its limitation in generating precise locations for interface elements precluded its use in assessing A1. The VH based approach detailed by~\\cite{wang2023enabling} presents a novel technique of extracting useful information from the view hierarchy, translating it into HTML format as UI description. This method directly accesses element positions within the view hierarchy, and is shown effective in providing a comprehensive UI description for LLM understanding. For the question-answering task (A3), we utilize an Ernie Bot to answer four types of questions based on generated UI semantic descriptions  (VH in HTML format and ours in natural language). The results are shown in Table~\\ref{tab:7_1}.\n\n\\subsubsection{Results and Analysis}\n\nOur method for detecting UI elements (A1) significantly surpasses the accuracy of VH-based approaches. The inherent limitation of VH is its frequent failure to accurately represent UI elements, often omitting existing elements or including non-existent ones. In contrast, \\textcolor{black}{our visual-based method effectively identifies UI components even when applied to apps from different cultures, such as training on English apps and testing on Chinese apps. Examples are shown in Fig.~\\ref{fig:image_read}(a). However, it is important to note that there is a minor possibility of false positive detections, where the method may occasionally identify non-existent icons, as shown in Fig.~\\ref{fig:image_read}(b).}\n\nThe challenge extends to recognizing icon-only buttons (A2), where VH-based methods only manage to accurately describe 63\\% of such elements. A contributing factor to this shortfall is that programmers often fail to provide clear descriptions for common icons or resort to using app-wide custom encodings (such as private Unicode characters), which VH struggles to interpret correctly. On the other hand, our method, along with GPT-4V, demonstrates superior performance in recognizing these commonly used icons, achieving an accuracy rate exceeding 80\\%.\n\nFor LLM question answering using generated UI semantic descriptions (A3), our method outperforms the other two approaches overall. It's noteworthy that, for queries about element status, GPT-4V leads in accuracy due to its advanced image understanding capabilities. Our analysis reveals a significant gap in VH files regarding the annotation of element states (such as on/off, selected/unselected), a detail often overlooked by programmers in XML files or code. Our method adeptly recognizes the status of elements through additional icons such as toggle buttons and checkboxes. When it comes to identifying selected items in tabs or navigation bars, our method relies on comparing the main color differences between elements. While this approach is effective for simpler designs, it struggles with more complex layouts in tabs or navigation bars. Fig~\\ref{fig:cases} shows the correct and incorrect cases of our method in recognizing the status.\n\nFor item count related queries (A3), such as listing all the orders or flights displayed, our method also achieve better accuracy than the baselines. This is attributed to our approach of grouping UI elements based on their layout and visual cues, enabling to more accurately aggregate related items in natural language descriptions. However, GPT-4V falls short in this area due to two identified issues: the generation of hallucinated items that don't exist and an incomplete understanding of the UI, often only recognizing items arranged in the first row under the target description and disregarding subsequent lines as unrelated.\n\nFor content related questions (A3), our method has advantages of dealing with images that lack text descriptions by utilizing OCR to read key information in images, such as product descriptions in showcase images or tracking numbers in photos of shipping labels. Similarly, for functionality queries, many functions are executed through image buttons, which might not be accurately described in the view hierarchy. Thus, our approach surpasses VH-based methods by providing a richer understanding of the UI information and functionalities. \n\n\\subsection{Performance on One-step Prediction across Public Datasets}\n\\label{sec:7_2}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=1\\columnwidth]{figures/one_step_new.pdf}\n    \\caption{\\textcolor{black}{Accuracy of one-step prediction across four datasets.}} \n    \\label{fig:one_step_new} \n\\end{figure}\n\n\\begin{table}\n    \\footnotesize\n    \\caption{Error analysis of the one-step prediction experiment.}\n    \\label{tab:7_2}\n    \\begin{tabular}{cccccc}\n        \\toprule\n        \\multirow{2}{*}{Method} & \\multicolumn{3}{c}{UI error} & \\multirow{2}{*}{\\makecell{Planning\\\\error}} & \\multirow{2}{*}{\\makecell{Total\\\\error}}\\\\\n        \\cmidrule(lr){2-4} \n        & Missed & Overdetected & Misread   &  &   \\\\\n        \\midrule               \n        GPT-4V~\\cite{yan2023gpt}& 12 & 2 & -\\textsuperscript{$c$} & 232 & 246\\\\\n        VH~\\cite{wang2023enabling} & 27 & 8 & 37 & 154 & 226 \\\\\n        \\rowcolor{lightgray!40}\n        Ours  & 12 & 5 & 12 & 126 & 155 \\\\\n        \\bottomrule\n        \\multicolumn{6}{l}{\\textsuperscript{$c$}GPT-4V inputs the screenshot with tags to concurrently analyze the elements} \\\\ \n        \\multicolumn{6}{l}{and plan the next step, so the misread and planning errors are combined. }\\\\\n    \\end{tabular} \n    \\label{tab:error}\n\\end{table}\n\nThis section investigates whether our generated natural language descriptions of UIs support LLMs to plan tasks effectively.\n\n\\subsubsection{Datasets}\nTo explore this, we carried out a comparative analysis, setting our method against two recent methods across four expansive UI control benchmark datasets: MoTIF~\\cite{burns2021mobile}, META~\\cite{sun2022meta}, UGIF~\\cite{venkatesh2022ugif}, and AITW~\\cite{rawles2023androidinthewild}. These datasets document human-device interactions across different tasks, including a series of screenshots, view hierarchies, user actions for each task. Due to the lack of real-time feedback following action execution in the datasets, in this section, we focus on just predicting the immediate next step based on the current task and interface. Following the experiment settings in~\\cite{rawles2023androidinthewild}, \\textcolor{black}{we randomly selected 470 distinct tasks from these datasets. From the action sequences to complete each task, we sampled a snapshot to represent the current state for predicting the subsequent action to accomplish the task. The testing samples included 98 tasks from MoTIF, 95 from META, 93 from UGIF, and 184 from AITW.}\n\n\\subsubsection{Baselines}\nSimilarly, we selected the VH based approach (VH + LLM)~\\cite{wang2023enabling} and a GPT-4V~\\cite{yan2023gpt} based methods as the baselines. To highlight the performance differences resulting from UI semantics, we employed ERNIE Bot as the LLM coordinator for both VH and our method. We unified the prompt across the two methods to maintain consistency. The prompt includes instructions for the LLM (``Supposing you are an intelligent agent to help users complete mobile tasks. Given the screen, predict the element in the current UI to complete the task''), a task description, and UI semantics (HTML representations for VH or natural language descriptions for ours). We intentionally omitted action history from the prompt to compel the LLM to base its predictions solely on the current UI semantics.\n\nGPT-4V's capability to directly process images and instructions allows it to bypass the need for a LLM coordinator. However, its limitation in outputting precise coordinates prevents direct comparison with the dataset's ground truth to evaluate the accuracy of its responses. To address this, we adopted the approach in~\\cite{yang2023set}, which involves detecting UI elements and their bounding boxes first using our approach. We then identified these elements by placing tags at the center of their bounding boxes. GPT-4V was tasked with predicting the tags of the elements to interact with, allowing to parse the predicted interaction locations into accurate coordinates to compare with the ground truth. For these experiments, we used the latest gpt-4-vision-preview\\footnote{https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo} API and the prompt is similar to the other two methods, except it did not include the UI semantics descriptions in text.\n\n\\subsubsection{Results and Analysis}\nThe results presented in Fig.~\\ref{fig:one_step_new} demonstrate the general effectiveness of various methods in predicting the next step based on the current UI, demonstrating the efficiency of these UI representations. In over half of the cases, these methods were able to accurately forecast the subsequent step. Notably, our approach outperformed the others, \\textcolor{black}{achieving an accuracy rate of 67\\% across four datasets—a substantial improvement of over 15\\% compared to baseline methods.} Upon closer examination of the errors (see Table~\\ref{tab:error}), we discovered they primarily fall into two categories: errors in understanding the UI, where the model fails to correctly identify the UI elements essential for completing the task, and errors in task planning, where, despite recognizing necessary elements, the model does not correctly plan the next move.\n\nDelving deeper into the UI understanding errors, our data categorizes them into three specific types: missed detections, where the ground-truth elements that should have been identified were overlooked; overdetections, where non-existent elements were incorrectly recognized as part of the next step; and misread, where elements were correctly detected but their functions or statuses were misunderstood, leading to erroneous interactions. Our method significantly reduced these types of errors compared to the VH method, suggesting that visual-based approaches have the potential to overcome the limitations of view hierarchies, including information omission, false positives, and inaccuracies.\n\nBeyond UI misunderstandings, more errors arose during the task planning phase. The semantic descriptions of interfaces may fail to support LLMs in making accurate plans, leading to incorrect predictions of the element to interact with—even when both the element and the ground truth were correctly understood within the UI descriptions. Our approach resulted in 21 fewer errors compared to HTML-based UI descriptions used in the VH method. This improvement suggests that our natural language description method, which utilizes semantic blocks to enrich element semantics, provides more effective support for LLMs in task planning. This advancement not only highlights our method's superiority in understanding UIs but also in facilitating more accurate task execution by AI models.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=1\\linewidth]{figures/step_and_category.pdf}\n    \\caption{\\textcolor{black}{Steps and categories with task completion rates.}}\n    \\label{fig:category}\n\\end{figure}\n\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=1\\linewidth]{figures/auto_task_new.pdf}\n    \\caption{\\textcolor{black}{Results of the real-world tasks automation experiments. ``Ours-qwen'' refers to the implementation of our framework using the open-source Qwen while ``Ours'' indicates the use of the Ernie Bot as the LLM.\"}}\n    \\label{fig:task}\n\\end{figure*}\n\n\\subsection{Real-world Task Automation Experiment}\n\\label{sec:7_3}\nIn addition to conducting simulated experiments on public datasets, in this section, we will explore the performance of our method in completing real-world multi-step tasks.\n\n\\subsubsection{Dataset and device}\nWe designed 147 real-world multi-step tasks to test our approach on a HUAWEI P20 smartphone. The device runs on the Android operating system with a 5.8-inch screen and a resolution of 2244$\\times$1080. It was connected to a desktop running a smart agent through ADB, enabling the transmission of interface screenshots to the agent and the execution of commands sent by the agent. The tasks spanned 12 categories (see Fig~\\ref{fig:category}(b)) across 42 commonly used apps in China and required between 2 to 13 steps (with an average of 4.7, the distribution of these task steps is illustrated in Fig~\\ref{fig:category}(a)) to complete. All the tasks are listed in Appendix~\\ref{sec:tasks}. A task was considered complete when the intended goal was achieved and the agent recognized the appropriate point to cease actions. We used the task completion rate as a key metric for evaluating performance. \n\n\\subsubsection{Baselines}\n\\textcolor{black}{For comparison, we recruited 12 human evaluators to manually perform these 147 tasks with the same device as the human baseline. These evaluators were from local institutions, aged between 22 and 35}, with at least a bachelor's degree and over two hours of daily smartphone usage, thus proficient in various everyday mobile tasks. They were shown the descriptions of all the tasks and instructed to complete the tasks in as few steps as possible, with the option to skip any step they didn't know how to complete. One author supervised the user study for assistance. All apps related to the tasks were pre-logged in, and no personal information was needed for the study. The study lasted approximately 70 minutes, and evaluators were free to stop at any time. They were compensated based on the local average hourly wage. \\textcolor{black}{We also include two additional baselines: the VH-based approach~\\cite{wang2023enabling} and the GPT-4V method~\\cite{yan2023gpt}. To further evaluate the adaptability of our approach, we substitute the Ernie Bot in our framework with the open-source Qwen1.5-110b model\\footnote{https://github.com/QwenLM/Qwen2} (4-bit quantization version). This substitution serves as an additional method for comparison.}\n\n\\subsubsection{Results and Analysis}\n\\textcolor{black}{We grouped tasks into three categories based on how many evaluators completed them: unfamiliar (12\\%, 17 tasks) with 5 or fewer evaluators completing; familiar (14\\%, 21 tasks) with 6-10 evaluators completing; and intuitive (74\\%, 109 tasks) with more than 10 evaluators completing. Fig~\\ref{fig:task}(a) shows the completion rates for both humans and our approach. Our method completed 82\\% of intuitive tasks and 67\\% of familiar tasks, slightly below human rates but far above the other three methods. For unfamiliar tasks, where humans only completed 26\\%, our method achieved a 47\\% completion rate, demonstrating an advantage over human evaluators. Overall, our approach achieved a 76\\% completion rate across all 147 tasks, close to the human evaluators' 87\\%. When we added programming by demonstration prompts, our method's completion rate improved to 94\\%, surpassing human evaluators. Notably, our method achieved good results using the open-source Qwen model as the LLM, significantly outperforming VH and GPT-4V. This shows that open-source models can adapt well to our method, opening up possibilities for more cost-effective, reliable deployment using open-source models.}\n\nOur method's completion rate, as depicted in Fig.~\\ref{fig:category}, varies with the number of steps and the category of tasks. Tasks with more than eight steps are grouped together due to the small sample size. There is a general decline in task completion rate as the number of steps increases. However, tasks requiring more than six steps still maintain a completion rate of around 50\\%. Fig.~\\ref{fig:category}(b) demonstrates that our method performs well across different task categories, with completion rates exceeding 60\\%. The lowest performance was observed in media-related tasks, where two tasks failed due to the complexity of UI backgrounds (e.g., recommended media or music promotion covers), which interfered with the recognition of transparent function buttons on the UI. The other seven failed tasks involved four Hard tasks, including two tasks that all human annotators failed to complete. These tasks, featuring functions that are difficult to discover, also posed significant challenges to LLM task planning.\n\nSimilar to Sec.~\\ref{sec:7_2}, we classified the errors from the methods across the tasks that were not completed into two types: UI understanding errors and task planning errors. The distribution of these error types across different methods is depicted in Fig.~\\ref{fig:task}(b). The results indicate that GPT-4V encountered the highest number of UI issues, primarily due to significant hallucination problems during the experiments. GPT-4V tended to invent non-existent buttons, often required for later stages of a task but absent from the current interface. For instance, in a task involving checking the balance in a wallet, GPT-4V would instruct to click on a ``My Wallet'' button—a button necessary in the penultimate step of the task but not present on the current screen. These hallucinations likely stem from GPT-4V's extensive general knowledge, which includes similar information, yet it fails to accurately comprehend all elements on the interface, leading to such errors.\n\nIn contrast, the VH method exhibited fewer UI errors, even though one app's view hierarchy lacked all UI elements information in this study. However, this method faced the most task planning issues. This suggests that directly converting VH to HTML for UI description, as proposed in \\cite{wang2023enabling}, might not perform well against the complexity of real-world interfaces. The HTML descriptions, by stacking elements sequentially, lack explicit representations of the relationships between elements. Consequently, even if the elements are correctly identified, forming a coherent semantic description to support accurate LLM decision-making is challenging. On the other hand, our method, even when employing the same LLM, can leverage the associations between text and icons, as well as the layout of the UI, to create a more accurate representation of the UI. This enhanced semantic understanding significantly supports LLM decision-making, leading to fewer task planning errors.\n\n\\textcolor{black}{\nWhen examining how task familiarity affects our method's performance, we found that for intuitive tasks, the LLM's built-in knowledge and reasoning skills usually led to accurate task planning. Errors mainly occurred due to the agent's incomplete understanding of the UI. As tasks became less common, planning errors increased, similar to how humans struggle with planning unfamiliar tasks. In Fig~\\ref{fig:task}(d), we compared the steps taken on tasks completed by both our method and at least one evaluator. The LLM often found more direct ways to complete tasks, while human users relied more on trial and error, especially for unfamiliar tasks where they made significantly more attempts. These findings suggest that the LLM can potentially be more efficient than humans, highlighting the importance of adding external knowledge to prompts to further improve the LLM's performance.\n}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=1\\linewidth]{figures/pbd.pdf}\n    \\caption{The effectiveness of PBD under varied conditions.}\n    \\label{fig:pbd}\n\\end{figure}\n\n\\subsection{Impact of Programming By Demonstration}\nFocusing on the 36 real-world tasks that our method failed to complete, we incorporated the PBD mechanism into our approach to see if it improved performance. Specifically, an author manually executed these tasks on the experimental smartphone as the demonstration, while our agent captured a series of screens and translated them into natural language action sequences. These sequences, along with the descriptions of the tasks, were used to create 36 task-solution pairs as the PBD solutions. For each task, we enriched the prompts with the corresponding PBD solutions to examine if augmented prompts could effectively assist the LLM in accomplishing tasks that were previously unsolvable.\n\nTo rigorously test the efficacy of our PBD mechanism under varied conditions, we conducted task completion in three distinct environments: (1) on the same smartphone used for the initial demonstrations, maintaining consistency in UI and workflow; (2) on a large screen pad, specifically a 12-inch device with a 2000$\\times$1200 resolution, to challenge the method with different UI layouts, screen sizes, and resolutions; and (3) on the same phone but with applications set to the English version, which might be a bit different from the original version in both layout and workflow, introducing version variation to assess adaptability to different linguistic contexts.\n\nThe results, depicted in Fig.~\\ref{fig:pbd}(a), indicate that incorporating PBD significantly improves task completion, successfully automating about 70\\% of the previously failed tasks. Notably, the generated PBD sequences were robust against variations in screen size, resolution, and language. As indicated in Fig.~\\ref{fig:pbd}(b), our results uncovered that the primary challenge in tasks where PBD failed shifted from task planning to UI understanding. This shift highlights PBD's effectiveness in assisting with the planning of accurate task paths that LLMs find difficult. It's important to note, though, that even with PBD guidance, LLMs may still inaccurately plan tasks.\n\n\\section{Discussion}\n\n\\subsection{Latency and Token Cost}\n\nDuring the task automation with VisionTasker, delays predominantly arise from two main sources: the time taken by the UI understanding module to analyze screenshots and the communication latency with the LLM. During our real-device experiments, utilizing an Intel i9 10850k CPU and an RTX 2080 Ti GPU, the UI understanding process averaged 5.9 seconds, while communication with the LLM averaged 4.5 seconds, resulting in an average execution time of over 12 seconds per action. Significantly reducing the UI understanding delays could be achieved by employing lighter models or using knowledge distillation techniques in place of the Yolov8 and CLIP models.\n\nAnother important factor is the number of tokens, which directly influences the quality of the LLM's response. In a task in Sec.~\\ref{sec:7_3}, the HTML generated using the VH method consumed over 8K tokens, leading to the LLM denying service. A comparison in Table~\\ref{tab:7_1} reveals that our method requires approximately a quarter of the tokens needed by the other two methods to describe the same UI. Furthermore, our prompts maintain task chains by recording completed actions as action history rather than saving previous UI descriptions, making the additional number of tokens needed due to increased task steps negligible in comparison to those needed for UI semantic descriptions. This efficiency enables the handling of tasks with longer sequences, as demonstrated by the completion of a task involving 13 steps in Sec.~\\ref{sec:7_3}.\n\n\\subsection{Reading from Images}\n\nOur method employs a visual approach to extract information from user interfaces, which might miss some elements. However, for a user-centered design, essential functionalities are generally designed to be visually prominent, often represented by standardized icons and noticeable sizes, making them more readily detectable by models. In our experiments, we observed that search boxes are the elements most likely to be overlooked by our model. This is primarily because search boxes are usually located at the top of the UI. In some applications, such as the official train ticket booking app 12306, which boasts over 1.5 billion installs, advertisements are placed in this area. These ads can make the search box semi-transparent to enhance advertising effects, thereby hindering its detection. Nonetheless, other crucial functionalities within the UI are largely detectable.\n\nCompared to the view hierarchies that requires programmer involvement, our visual approach can potentially uncover more information but also risks detecting non-existent elements. The impact of these two types of errors on the completion of the final task differs, as the result of UI understanding serves the task planning. For instance, elements ignored by the view hierarchy, such as coupons, functionalities, and information within image buttons, which may be important to the task, can be detected by our method (Fig.~\\ref{fig:image_read}(a)). The non-existent elements our method detects are mainly identified as generic images or some specifically meaningful icons. Since the wrongly identified icons' meanings are generally random and do not align with the overall functionality of the UI (Fig.~\\ref{fig:image_read}(b)), these additional pieces of information, even if included in the semantic description of the UI, often do not affect subsequent tasks. Hence, using vision-based method is a viable approach for task automation in practice.\n\n\\subsection{Adapting to Desktop Tasks}\nOur visual-based UI understanding method can be effectively adapted to desktop environments. Desktop UI has two distinct characteristics compared to mobile UI. First, icons in desktop apps are generally smaller; for example, a large paste button in Microsoft Word measures approximately 40x40 pixels, whereas a typical button size in mobile UI is around 150x150 pixels. This size difference means the detection model trained on mobile interfaces might not perform well in detecting desktop buttons. Second, desktop applications use less text and more icon-only buttons, relying more on the graphical representation of a button to convey its functionality, which demands higher accuracy from button interpretation models. However, these challenges can be addressed by training models with desktop interface and element datasets, without needing to alter the method's process and framework, making the adaptation to desktop environments relatively straightforward. The discussion above applies to general apps commonly used in everyday life. For highly specialized desktop apps, such as Photoshop, which feature buttons with professional meanings and where a LLM might not possess the necessary knowledge of professional workflows, our method may not yet be effectively adapted to such highly specialized apps.\n\n\\subsection{Utilizing Other LLMs}\nIn the task planning phase, the core competency comes from the contextual reasoning abilities LLMs acquire during their pre-training from extensive corpuses. Thus, beyond utilizing Ernie Bot, employing other LLMs like ChatGPT, Claude, and Doubao could achieve comparable or even superior reasoning outcomes. This paper primarily opts for Ernie Bot due to its ease of access, low latency, and cost-effectiveness. With the ever-improving inferencing capabilities of these models, using LLMs for task planning is becoming increasingly accurate. Our approach can also be implemented using locally deployed open-source large models \\textcolor{black}{(see Sec.~\\ref{sec:7_3})}, such as Qwen, LLaMA~\\cite{touvron2023llama}, and ChatGLM~\\cite{du2022glm}. However, achieving optimal outcomes with these models may require fine-tuning with specific datasets, such as AiTW~\\cite{rawles2023androidinthewild}. Notably, our PBD mechanism can effectively apply to localized models. With sufficient demonstrations obtained through crowdsourcing or crawling from help documentations, relying on our visual UI understanding approach combined with extensive PBDs can compensate for the deficiencies of open-source LLMs in many scenarios.\n\n\\subsection{Limitations and Future Work}\nVisionTasker has several limitations. For instance, some interfaces, like login screens and payment QR code scans, could not be captured via screenshots. Additionally, \\textcolor{black}{accurately understanding UI remains challenging. In some cases, even when our method correctly identifies and segments all elements, it fails to fully convey the information. For example, in a local ticketing app we tested, non-bookable dates were displayed in lighter colors—a subtle distinction our method couldn't detect. Our UI understanding part doesn't incorporate advanced screen parsing methods such as screen parser~\\cite{wu2021screen} and fully utilize context and graphic cues for better comprehension. VisionTasker's current approach of capturing screenshots at fixed intervals after touch events isn't optimal for games or slow-loading applications. It also isn't designed for interactive use. When a task lacks specific parameters, the LLM uses default values to complete it. For instance, in a food delivery task, orders are sent to a default address. For critical parameters, an interactive approach that prompts the user would be more appropriate. To improve our method, one could investigate advanced planning strategies such as self-reflection~\\cite{shinn2024reflexion} and staged planning~\\cite{li2023zero}. Testing VisionTasker on a wider range of tasks and conducting more detailed ablation studies would also help better understand and refine our method.}\n\n\\section{Conclusion}\n\nThis paper introduced VisionTasker and examined its efficacy for mobile task automation, addressing limitations found in traditional methods such as Programming By Demonstration and the challenges posed by view hierarchies. VisionTasker is empowered by a vision-based UI understanding technique, which translates UI screenshots into natural language, eliminating the need for view hierarchies required in other approaches. It integrates Large Language Models (LLM) for task planning in a step-by-step manner to enhance accuracy and practicality. Extensive experiments show that VisionTasker outperforms previous methods, providing effective UI representations across four datasets. Additionally, in automating 147 real-world tasks on an Android smartphone, VisionTasker surpasses human performance in complex tasks and showing significant improvements when integrated with the PBD mechanism, suggesting a promising direction in task automation.\n\n\\begin{acks}\nWe are grateful to Professor Xiaohong Guan for his kind support of this work and anonymous reviewers for their insightful comments. This work is supported by the National Natural Science Foundation of China (No. 62102308), Initiative Postdocs Supporting Program (No. BX2021243), and China Postdoctoral Science Foundation (No. 2021M702627).\n\\end{acks}\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{CoCo-Agent: A Comprehensive Cognitive MLLM Agent \\\\ for Smartphone GUI Automation}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\nMultimodal large language models (MLLMs) have shown remarkable potential as human-like autonomous language agents to interact with real-world environments, especially for graphical user interface (GUI) automation.\nHowever, those GUI agents require comprehensive cognition ability including exhaustive perception and reliable action response.\nWe propose a \\underline{Co}mprehensive \\underline{Co}gnitive LLM \\underline{Agent}, CoCo-Agent, with two novel approaches, comprehensive environment perception (CEP) and conditional action prediction (CAP), to systematically improve the GUI automation performance. \nFirst, CEP facilitates the GUI perception through different aspects and granularity, including screenshots and complementary detailed layouts for the visual channel and historical actions for the textual channel.\nSecond, CAP decomposes the action prediction into sub-problems: action type prediction and action target conditioned on the action type.\nWith our technical design, our agent achieves new state-of-the-art performance on AITW and META-GUI benchmarks, showing promising abilities in realistic scenarios. Code is available at \\url{https://github.com/xbmxb/CoCo-Agent}.\n\n\\end{abstract}\n\n\\section{Introduction}\n\nGraphical user interface (GUI) automation aims to enable human-like operations on operating systems with artificial intelligence instead of human efforts.\nLarge language models (LLMs) have demonstrated commendable performance as human-like agents, showing emergent abilities of perceiving \\cite{yao2023react}, reasoning \\cite{li2023camel, Park2023GenerativeAgents}, and acting \\cite{wang2023voyager, autogpt}.\nWith the multimodal enhancement, MLLM agents become promising autonomous GUI assistants to deal with complex tasks on behalf of human operators.\nTo interact with the GUI environment, those agents require comprehensive cognition, including exhaustive perception and reliable action response.\n\nCurrent vital challenges for autonomous agents lie in two aspects. One is \\textbf{(i) the dependence on strong (M)LLMs}, and the other is \\textbf{(ii) the insufficient GUI environment modeling}.\n\nAlthough \\textit{strong (M)LLMs} like GPT-4V \\cite{openai2023gpt4} and ChatGPT \\cite{ouyang2022training} ignite the development of autonomous agents, they exhibit shortcomings in realistic use.\nFirst, the alignment requires a trustworthy domain transfer as there is a large disparity between GUI commands and natural languages. \nGUI agents are expected to generate accurate and well-formulated responses as executable GUI commands, which is non-trivial for zero-shot prompting.\nFor example, given the GUI that accepts commands as ``\\texttt{\\{action: click, touch\\_point:[y$_0$, x$_0$], touch\\_point:[y$_1$, x$_1$], typed\\_text: `'\\}}'', semantically equivalent generations like ``\\texttt{Open the address book on your phone}'' is plausible but unavailable.\nSecond, the black-box APIs are likely to cause unexpected safety issues.\nRisks to privacy and integrity may arise when granting personal device authority to a black-box API.\nThis significantly reduces realistic usability.\nThird, the performance mainly relies on the prompt design.\nThe issues mentioned above leave heavy burdens on the design of prompt lines for those agents.\nBesides necessary environment descriptions, the prompts (and post-processing) need to be sophisticated to enhance domain alignment, instruction following, and security risk mitigation in different circumstances.\n\nSecond, GUI agents necessitate a comprehensive multimodal perception for the modeling of the informative environment.\nExisting methods for visual language models are mainly endowed with favorable abilities in semantic alignment between the vision and language modalities \\cite{instructblip, ye2023mplugowl, zhao2023mmicl}. \nHowever, GUI contains fine-grained details and intricate semantic connections, presenting a challenge for agents to comprehend \\cite{rawles2023android, li2023otterhd}. \nConsider a screenshot that includes a magnifier icon, where the conventionally accepted meaning of ``\\textit{search}'' is conveyed. It implies a potential action through implicit semantics despite its small pixel size.\nThus, only leveraging general image semantics like captioning is insufficient for GUI environment modeling.\nIn addition, the perception of environmental information is limited by the finite input window\n, where a balance between the visual and textual feature length needs to be struck. \n\nThis work proposes CoCo-Agent, a \\underline{Co}mprehensive \\underline{Co}gnitive MLLM \\underline{Agent}, to address the challenges above for smartphone GUI automation.\nCoCo-Agent adopts a multimodal backbone of LLaVA \\cite{liu2023llava} and further enhances comprehensive cognition, respectively for exhaustive perception and reliable action response.\nThe two proposed approaches are \ncomprehensive environment perception (CEP) and conditional action prediction (CAP).\nSpecifically, CEP integrates GUI perception elements of textual goal, historical action, and high-level and detailed description of the vision channel.\nCAP decomposes the complex and redundant GUI action commands into sub-problems following a \\textit{top-down} order.\nOur experiments cover diverse tasks in two GUI benchmarks, AITW \\cite{rawles2023android} and META-GUI \\cite{sun-etal-2022-meta}, including application manipulation, web operation, and dialogues. CoCo-Agent achieves SOTA performance with a limited parameter size.\nSubsequently, we present deep analyses including element ablation, visual module selection, and future action prediction.\nWe show the significant effect of each perception element and the favorable choice of visual module.\nWe also analyze the limitations of existing datasets and illustrate the additional potential of CoCo-Agent for realistic scenarios.\n\nOur contributions are summarized as follows:\n\n$\\circ$ We propose CoCo-Agent, an autonomous agent with comprehensive cognition for GUI, with novel approaches to enhance the perception and action response, namely comprehensive environment perception (CEP) and conditional action prediction (CAP).\n\n$\\circ$ CoCo-Agent achieves state-of-the-art performance on representative GUI benchmarks, demonstrating superior performance.\n\n$\\circ$ Extensive analyses for a systematic study of GUI automation demonstrate our significant effectiveness and realistic potential.\n\n\\section{Related Work}\nThis section introduces studies on autonomous language agents and multimodal perception of LLMs.\n\n\\subsection{Autonomous Language Agents}\nRecent studies \\cite{li2023camel, autogpt} use the term \\textit{language agent} to refer to language models that interact with an environment or other agents and solve a problem.\nThis paper investigates the autonomous language agents that perceive the environment and then act on the environment. \n\nOne research line relies on the strong fundamental competencies of (M)LLMs. Based on ChatGPT or GPT-4, autonomous agents can be built by only well-written prompts. Existing works have proved the reasoning, planning, and generalizing abilities of GPT-based agents, e.g., AutoGPT \\cite{autogpt}, BabyAGI \\cite{babyagi}, AgentGPT \\cite{agentgpt}, HuggingGPT \\cite{shen2023hugginggpt}, and MM-Navigator \\cite{yan2023gpt}.\n\nHowever, when we expect practicality and reliability, we pursue the trainable language agent that can be customized and privatized to align with given environments \\cite{shao-etal-2023-character}.\nThus, another research line turns to trainable methods for open-source language models. \nm-BASH \\cite{sun2022meta} adopted ROI pooling to present GUI icons in a BERT-based multi-task system.\nThe Auto-UI \\cite{zhang2023you} was trained on a multimodal T5 \\cite{2020t5}, formulating the GUI interaction to a first-principal VQA form.\nCogAgent \\cite{hong2023cogagent} integrated an extra attention-based high-resolution visual module with alignment pre-training.\nThis paper follows the second research line to discuss the trainable, open-source language agent.\n\n\\begin{figure*}[h]\n    \\centering\n    \\includegraphics[width=0.98\\textwidth]{figs/agent_v2_0215night-crop.pdf}\n    \\caption{Overview of CoCo-Agent, illustrating the perception and action response on a time step. \\textit{CEP} integrates the shown fine-grained elements. The predicted actions are formulated following \\textit{CAP}. }\n    \\label{overview}\n\\end{figure*}\n\n\\subsection{Multimodal Perception}\nBeyond language modeling, recent works have studied the fusion with channels of other modalities.\nBecause of the development of LLMs, mainstream methods usually follow a language-centric framework, i.e., encoding information of other modalities into the language embedding space. These models consist of a pre-trained encoder of other modalities, a language model, and an adapter (or a projector) as the bridge.\nFor example, LLaVA \\cite{liu2023llava} uses a linear layer to map the vision encoding from CLIP, while BLIP-2 \\cite{li2023blip} adopts a Q-former to learn a query vector to represent the image.\nThis endeavor has given rise to the emergence of various multimodal LLMs, such as Flamingo \\cite{alayrac2022flamingo}, mPLUG \\cite{ye2023mplugowl}, MiniGPT-4\\&v2 \\cite{zhu2023minigpt, chen2023minigptv2}, Video-LLaMA \\cite{damonlpsg2023videollama}, and SpeechGPT \\cite{zhang2023speechgpt}.\n\nHowever, the multimodal perception is even more challenging for GUI agents.\nBecause GUI contains extensive detailed information with intricate semantic connections, such as very small icons conveying customary meanings (shown in Figure \\ref{overview}). \nA gap remains between existing visual modules and the perception necessitated for GUI agents.\n\n\\section{Methodology}\nIn this section, we will first formulate the GUI automation task and then propose our CoCo-Agent. Concretely, we will describe our technical designs of cognition, namely, comprehensive environment perception (CEP) and conditional action prediction (CAP), to improve the GUI automation performance systematically. Figure \\ref{overview} shows an overall illustration.\n \n\\subsection{Task Formalization}\nThe task of GUI automation is defined as an interactive sequence generation problem.\nFirst, the user instructs the agent with a goal $g$ that can be achieved in the GUI environment in several steps.\nAt each step, the agent first perceives the present GUI \\textit{state}, $s_t$, and predicts the next \\textit{action} $a_t$, leading to the next GUI state $s_{t+1}$.\nThe sequential $(s, a)$ that accomplishes a goal forms an \\textit{episode}. An interaction record is formulated as\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{5pt}\n\\setlength{\\belowdisplayskip}{5pt}\n\\textsc{Record} = (g, [(s_t, a_t)]_{t=1}^n).\n\\label{formalization}\n\\end{equation}\n\nThe action space is a finite operation command set with limited parameters. \nExamples are illustrated in Table \\ref{redefine}.\nThe state space includes any possible display from the smartphone.\nAs the output recipient of agents is not human but a GUI, accurate actions are expected instead of flexible expressions like natural language.\n\n\\begin{table*}[tbh]\n\t\\centering\\small\n \\resizebox{\\linewidth}{!}{\n\t{\\begin{tabular}{p{4cm}p{2cm}p{1.9cm}p{1.5cm}|p{7cm}}\n\t\t\\toprule\n\t\t\\textbf{Action Type} & \\textbf{Touch\\_point}& \\textbf{Lift\\_point} &\\textbf{Typed\\_text} & \\textbf{Redefined Actions in CAP} \\\\\n            \\midrule\n            \\midrule\n             PRESS\\_HOME & \"[-1.0, -1.0]\" & \"[-1.0, -1.0]\" & \"\" & I need to <PRESS\\_HOME> \\\\\n             PRESS\\_BACK &\"[-1.0, -1.0]\"&\"[-1.0, -1.0]\"&\"\" & I need to <PRESS\\_BACK> \\\\\n             PRESS\\_ENTER &\"[-1.0, -1.0]\"&\"[-1.0, -1.0]\"&\"\" & I need to <PRESS\\_ENTER> \\\\\n             STATUS\\_TASK\\_COMPLETE &\"[-1.0, -1.0]\"&\"[-1.0, -1.0]\"&\"\" & For this goal, no more action is needed, so <STATUS\\_TASK\\_COMPLETE> \\\\\n             TYPE &\"[-1.0, -1.0]\"&\"[-1.0, -1.0]\"&\"\\{string\\}\" & I need to <TYPE> a string here, \"typed\\_text\": \"\\{string\\}\" \\\\\n             DUAL\\_POINT &\"\\{coordinate\\}\"&\"\\{coordinate\\}\"&\"\\{string\\}\" & I need to <SCROLL> \\{direction\\} \\\\\n             DUAL\\_POINT &\"\\{coordinate\\}\"&\"\\{coordinate\\}\"&\"\\{string\\}\" & I need to <CLICK> \\{item name\\}, the location of \\{item name\\} is  \"tap\\_point\": \"\\{coordinate\\}\" \\\\\n             DUAL\\_POINT &\"\\{coordinate\\}\"&\"\\{coordinate\\}\"&\"\\{string\\}\" & I need to <TAP> on the screen, the location is \"tap\\_point\": \"\\{coordinate\\}\" \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\t}}\n        \\caption{Illustration of JSON-formatted GUI commands in AITW (left) and our definition in CAP style (right). \\textit{\"[-1.0, -1.0]\"} follows the default value in AITW. \\textit{String}, \\textit{item name}, \\textit{coordinate}, and \\textit{direction} are required parameters.}\n\t\\label{redefine}\n \\vspace{-1.2em}\n\\end{table*}\n\n\\subsection{Backbone}\nOur backbone follows LLaVA \\cite{liu2023llava}, which uncovers the generalization of LLM to vision modality. LLaVA consists of a Llama-2-chat-7B \\cite{touvron2023llama2}, a vision encoder ($\\textsc{Encoder}_{image}$), CLIP \\cite{Radford2021LearningTV}, and a one-layer linear projector ($\\textsc{Prj}$) to bridge the image features to the space of language embedding ($\\textsc{Embed}_{text}$). \nThe input is denoted as $X$, including text $X_{text}$ and image $X_{image}$, The output is denoted as $Y$.\nThe backbone can be formulated as \n\\begin{equation}\n\\begin{split}\n& {H}_{text} = \\textsc{Embed}_{text} \\textup{( } {X}_{text} \\circ \\hat{Y}^{0:t-1} \\textup{ )}, \\\\\n& {Z}_{image} = \\textsc{Encoder}_{image} \\textup{( } {X}_{image} \\textup{ )}, \\\\\n& {H}_{image} = \\textsc{Prj} \\textup{( } {Z}_{image} \\textup{ )}, \\\\\n& {H}_{t}^{Decoder} = \\textsc{Decoder} \\textup{( } {H}_{image} \\circ {H}_{text}^{t} ),\\\\\n& {P}_{t} = \\textsc{LM}_{head} \\textup{( } {H}_{t}^{Decoder} \\textup{ )},\\\\\n& \\mathcal{L} = \\sum_{t} \\textsc{CE} \\textup{( } {P}_{t} , {Y}_{t} \\textup{ )},\n\\end{split}\n\\label{llama}\n\\end{equation}\nwhere $\\circ$ denotes the concatenation operation. The training objective $\\mathcal{L}$ is cross entropy ($\\textsc{CE}$).\n\n\\subsection{Comprehensive Environment Perception}\nEnvironment perception is a crucial prerequisite for action responses.\nThe environment can be simplified to only a GUI screenshot \\cite{zhang2023you}, which is highly subject to the upper-bound ability of the vision encoder.\nHowever, there is a bottleneck for the vision channel. \nFirst, the size of the encoder is restricted to a relatively low resolution, e.g., 224 $\\times$ 224. \nSecond, the existing pre-training objectives on vision encoders mainly focus on image captioning \\cite{Radford2021LearningTV, li2023blip}, which is general, high-level semantic modeling. \nThus, fine-grained information on the screen needs to be enhanced as a complement to the high-level perception.\n\nOur proposed comprehensive environment perception fully leverages tools like optical character recognition (OCR), which gives fine-grained layouts with readable textual hints, e.g., ``\\textit{ICON\\_SETTINGS: [0.1783, 0.8701]}''.\nBesides the global goal, $g$, the environment state is perceived from three aspects, the present screenshot, $X_{image}$, the layouts from OCR, $L$,  and the previous actions in the present episode, ${a_{t-h:t-1}}$. The total input can be noted as\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{5pt}\n\\setlength{\\belowdisplayskip}{5pt}\n\\begin{split}\n& {X}_{text} = \\textsc{Prompt} \\textup{( }g, L, {a_{t-h:t-1}} \\textup{ )}, X_{image},\n\\end{split}\n\\label{input}\n\\end{equation}\nwhere $\\textsc{Prompt}$ denotes a prompt template (Appendix \\ref{a1}). $h$ denotes the number of action histories involved. The layouts $L$ are listed \\textit{(item name, item coordinate)}, where \\textit{items} denotes OCR results.\n\n\\subsection{Conditional Action Prediction}\nRegarding action response, we propose to refactor GUI actions following the thinking order.\nAs is shown in the left part of Table \\ref{redefine}, existing GUI actions involve redundant parameters of each command, \nincluding the action type, the beginning coordinates, the ending coordinates, and the possible input text.\nHowever, these parameters are not independent of each other but show significant relations. \nFor example, the coordinates are based on the action type. If the action is to click on an icon, then the touch and lift coordinates are accordingly determined.\nPredicting such JSON-formatted parameters would be a waste of effort.\n\nThus, we propose conditional action prediction.\nThe GUI actions are refactored for relation decomposition as illustrated in Table \\ref{redefine}. \nThe actions are decomposed into two sub-problems to address, (i) action type prediction and (ii) optional action target prediction conditioned on the action type prediction.\nAlso, we use natural language-like expressions without compromising the accuracy.\nAs illustrated in Table \\ref{redefine}, we change the action to a prompt line \\textit{step-by-step}, explicitly decomposing and clarifying those actions.\nNotably, the \\textit{dual\\_point} action is refined into three types: (i) \\textit{scroll} action, if the beginning and ending points are farther apart than the threshold \\cite{rawles2023android}; (ii) \\textit{click} action involving \\textit{item name}, if the tap point falls in a bounding box; (iii) \\textit{tap} action, if it is not a \\textit{scroll} action but matches no bounding box.\n\nIn this way, the action prediction follows a \\textit{top-down} order.\nFirst, the agent decides on action types, conditioned on which the agent further decides on the target item and coordinates.  \n\n\\noindent\\textbf{Normalization.}\nBased on CEP and CAP, the actions are normalized to alleviate noise, which is inevitable in real-world data.\nSpecifically, the target coordinates of \\textit{click} actions are normalized to the centroid of the bounding box from OCR. The \\textit{scroll} actions are normalized into four-direction swipes \\cite{zhang2023you}. \n\n\\section{Experiments}\nThis section will introduce the experimental settings including the dataset, implementation details, and baselines, followed by our empirical results.\n\n\\subsection{Dataset}\nThe following benchmarks of GUI automation are considered in the empirical evaluation. The dataset statistics are presented in Table \\ref{datastat}.\n\n\\textbf{AITW} \\cite{rawles2023android} is a benchmark for smartphone GUI, containing 715K operation episodes under 30K reality intentions. Each entry includes a goal in natural language, screenshots, and actions as Eq. \\ref{formalization}. Humans collect the data on various devices and operation systems in various screen resolutions.\nAccording to the applications domain, AITW consists of five subsets: General, Install, GoogleApps, Single, and WebShopping. \n\n\\textbf{META-GUI} \\cite{sun2022meta} smartphone dataset is originally released for multimodal dialogue generation.\nDifferently, the agent is enabled to communicate with the user to verify the present state or further operation, e.g., ``\\textit{Is this good?}''. \nEq. \\ref{formalization} is expanded with optional utterances,\n\\begin{equation}\n\\setlength{\\abovedisplayskip}{5pt}\n\\setlength{\\belowdisplayskip}{5pt}\n(s_t, a_t) \\rightarrow (s_t, a_t, u^{agent}_t, u^{user}_t),\n\\label{meta}\n\\end{equation}\nwhere utterances from the agent and the user are denoted as $u^{agent}$ and $u^{user}$.\nThese utterances cut an episode into several dialogue turns. META-GUI consists of 1k episodes with 18k steps. The data diversity lies in 11 applications of 6 domains including weather, calendar, search, etc. \n\n\\begin{table}[htb]\n\t\\centering\\small\n\t{\\begin{tabular}{p{1.9cm}p{1.3cm}p{1.5cm}p{1.0cm}}\n\t\t\\toprule\n\t\t\\textbf{AITW} & \\textbf{Episode} &\\textbf{Screen} & \\textbf{Goal} \\\\\n            \\midrule\n            General &9,476 &85,413 &545 \\\\\n            Install &25,760 &250,058 &688\\\\\n            Google Apps  &625,542 &4,903,601 &306\\\\\n            Single &26,303 &85,668 &15,366\\\\\n            Web Shopping  &28,061 &365,253 &13,473\\\\\n            \\midrule\n\t\t\\textbf{META-GUI} & \\textbf{Episode} &\\textbf{Dial. turn} & \\textbf{Screen} \\\\\n            \\midrule\n            Train  &897 & 3692 & 14,539\\\\\n            Dev &112 & 509 & 1,875\\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\t}\n        \\caption{Dataset statistics.}\n\t\\label{datastat}\n\\end{table}\n\n\\subsection{Implementation}\nOur implementation adopts LLaVA \\cite{liu2023llava} with a LLaMA-2-chat-7B and a vision encoder, CLIP.\\footnote{openai/clip-vit-large-patch14.\\label{clip}} The maximum input length is 2048 following the vision instruct tuning. \nFor the subsets of AITW, our experiments include two setups, i.e., separate training on each subset and unified training on the whole set. Details are shown in \\ref{a1}.\n\n\\noindent \\textbf{Metrics.} \nAccuracy is computed at each time step of all parameters as our metric.\nThe refactored action is parsed to JSON format and each parameter is compared to the action label following existing work \\cite{rawles2023android}.\nThe predicted coordinate is considered correct if it falls in the same element bounding box as the labeled one or falls within a 14\\% screen distance from the labeled one.\nA scroll action is correct if its main direction is correct.\nThe accuracy of other parameters are exact match except for \\textit{typed\\_text} or dialogue responses.\nThe typed text of AITW is correct if the label is in the predicted text.\nFor META-GUI, F1 is computed for input text, and BLEU is computed for response generation.\nOne action is regarded as correct if all the JSON fields are correctly predicted.\n\n\\begin{table*}[htb]\n\t\\centering\\small\n\t\\begin{tabular}{p{2.8cm}p{1.1cm}p{1.2cm}p{0.7cm}p{0.8cm}p{0.8cm}p{0.8cm}p{1.3cm}p{0.8cm}p{1.1cm}}\n\t\t\\toprule\n\t\t\\textbf{\\emph{AITW}} & \\textbf{API} & \\textbf{Modality} &\\textbf{Unified} &\\textbf{Overall} &\\textbf{General} & \\textbf{Install} & \\textbf{GoogleApps} &\\textbf{Single} &\\textbf{WebShop.} \\\\\n            \\midrule\n            PaLM-2  &PaLM-2 &\\textit{Text} &\\makebox[0.7cm][c]{\\ding{51}} &39.6 & -- & -- & -- & -- & --  \\\\\n            ChatGPT & ChatGPT & \\textit{Text} &\\makebox[0.7cm][c]{\\ding{51}} &7.72& 5.93 &4.38 &10.47& 9.39& 8.42\\\\\n            MM-Navigator & GPT-4V & \\textit{Text+Vision} & \\makebox[0.7cm][c]{\\ding{51}}  &50.54  &41.66  &42.64  &49.82  &72.83  &45.73 \\\\\n            MM-Navigator$_{\\textup{w/ text}}$ & GPT-4V & \\textit{Text+Vision} &\\makebox[0.7cm][c]{\\ding{51}} &51.92 &42.44  &49.18  &48.26  &76.34  &43.35 \\\\\n            MM-Navigator$_{\\textup{w/ history}}$ & GPT-4V & \\textit{Text+Vision} &\\makebox[0.7cm][c]{\\ding{51}}  &52.96  &43.01  &46.14  &49.18  &78.29  &48.18\\\\\n            \\hdashline\n            BC & N/A & \\textit{Text+Vision} & \\makebox[0.7cm][c]{\\ding{55}} &68.7 & -- & -- & -- & -- & -- \\\\\n            BC $_{\\textup{w/ history}}$ & N/A & \\textit{Text+Vision} & \\makebox[0.7cm][c]{\\ding{55}} &73.1 &63.7 &77.5 &75.7 &80.3 &68.5\\\\\n            LLaMA-2 & N/A & \\textit{Text} & \\makebox[0.7cm][c]{\\ding{55}} &28.40 &28.56 &35.18 &30.99 &27.35 &19.92 \\\\\n            Auto-UI$_{\\textup{separate}}$ & N/A & \\textit{Text+Vision} & \\makebox[0.7cm][c]{\\ding{55}} &74.22 &65.94 &77.62 &76.45 &81.39 &69.72 \\\\\n            Auto-UI$_{\\textup{unified}}$  & N/A & \\textit{Text+Vision} & \\makebox[0.7cm][c]{\\ding{51}} &74.27 &68.24 &76.89 &71.37 &84.58 &70.26 \\\\\n            CogAgent & N/A & \\textit{Text+Vision} & \\makebox[0.7cm][c]{\\ding{55}} &76.88 & 65.38 & 78.86 & 74.95 & \\textbf{93.49} & 71.73 \\\\\n            \\hdashline\n            LLaVA$_{\\textup{unified}}$ & N/A & \\textit{Text+Vision} & \\makebox[0.7cm][c]{\\ding{51}} & 70.37  &58.93  & 72.41 & 70.81 & 83.73 & 65.98 \\\\ \n            CoCo-Agent$_{\\textup{separate}}$ & N/A & \\textit{Text+Vision} & \\makebox[0.7cm][c]{\\ding{55}} &77.82 &69.92 &80.60  &75.76 & 88.81 & 74.02\\\\\n            CoCo-Agent$_{\\textup{unified}}$ & N/A & \\textit{Text+Vision} & \\makebox[0.7cm][c]{\\ding{51}} &\\textbf{79.05} & \\textbf{70.96} &\\textbf{81.46} & \\textbf{76.45} & 91.41 & \\textbf{75.00} \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n        \\begin{tabular}{p{1.8cm}p{2.4cm}p{1.3cm}p{1.3cm}p{1.3cm}p{1.3cm}p{1.3cm}p{1.5cm}}\n\t\t\\toprule\n\t\t\\textbf{\\textit{AITW}} & \\textbf{Model} & \\textbf{Action} & \\textbf{Act. type} & \\textbf{CoT. type} &\\textbf{Item} & \\textbf{Direction} & \\textbf{Input(F1)} \\\\\n            \\midrule\n            \\multirow{2}{*}{General} & LLaVA$_{\\textup{unified}}$ &58.93& 80.08 & N/A & 56.76 & 63.31 & 93.29  \\\\\n            & CoCo-Agent$_{\\textup{unified}}$  & 70.96 & 87.49& 76.72 &68.91 & 75.80& 97.10 \\\\\n            \\hdashline\n            \\multirow{2}{*}{Install} & LLaVA$_{\\textup{unified}}$  & 72.41 & 85.11 & N/A &72.52 & 70.20  &  94.31 \\\\\n            & CoCo-Agent$_{\\textup{unified}}$ & 81.46 & 90.82& 85.12 &81.52  &80.49  & 97.36 \\\\\n            \\hdashline\n            \\multirow{2}{*}{GoogleApps} & LLaVA$_{\\textup{unified}}$ & 70.81 & 88.49 & N/A & 65.55 & 74.95 & 98.75 \\\\\n            & CoCo-Agent$_{\\textup{unified}}$ &75.30 & 92.10 & 79.80 & 70.03 & 82.03 &  99.03  \\\\\n            \\hdashline\n            \\multirow{2}{*}{Single} & LLaVA$_{\\textup{unified}}$  & 83.73 &88.19 & N/A & 85.63 & 83.95 &93.83 \\\\\n            & CoCo-Agent$_{\\textup{unified}}$ & 91.41& 95.34 &92.49 & 91.84 & 92.74 & 98.15 \\\\\n            \\hdashline\n            \\multirow{2}{*}{WebShopping} & LLaVA$_{\\textup{unified}}$  & 65.98& 85.43 & N/A & 64.81 & 68.61 & 92.60 \\\\\n            & CoCo-Agent$_{\\textup{unified}}$ & 76.10 &  89.80 &80.20 &73.88  & 78.48 & 96.96  \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n        \\caption{Results on AITW. Part 1: Action accuracy, where primary setups are labeled: the reliance on API backends (``API''), the perceptual modalities (``Modality''), and the general ability across subsets (``Unified''). Part 2: Detailed parameter accuracies comparing our unified CoCo-Agent and LLaVA baseline.}\n\t\\label{mainaitw}\n\\end{table*}\n\n\\begin{table*}[htb]\n\t\\centering\\small\n\t\\begin{tabular}{p{2cm}p{0.8cm}p{1.5cm}p{1cm}p{1.3cm}p{1cm}p{1cm}p{1.4cm}p{1.9cm}}\n\t\t\\toprule\n\t\t\\textbf{\\emph{META-GUI}} & \\textbf{API} & \\textbf{Modality} & \\textbf{Action} & \\textbf{Act. type} &\\textbf{Item} & \\textbf{Direction} & \\textbf{Input (F1)} & \\textbf{Utter. (BLEU)} \\\\\n            \\midrule\n            LayoutLM &N/A & \\textit{Text} &67.76 & 82.22 & 71.98 & 94.87 & 90.56 & 50.43\\\\\n            LayoutLMv2  &N/A & \\textit{Text+Vision} &64.48 & 85.60 & 64.38 & 92.95 & 70.76 & 58.20\\\\\n            BERT &N/A & \\textit{Text} &78.42 &87.52 & 82.84 & 93.59 & 97.24 & 62.19\\\\\n            m-BASH  &N/A & \\textit{Text+Vision} &82.74 & 90.80 & 85.90 & 96.42 & 94.23 &63.11\\\\\n            \\hdashline\n            LLaVA  &N/A & \\textit{Text+Vision} & 76.27 &  87.47 & 77.49 & 98.18 &96.06  & 67.24 \\\\\n            LLaVA $_{\\textup{w/ history}}$  &N/A & \\textit{Text+Vision} &81.08&  91.68& 81.23 &  97.62&  96.93 & 66.57 \\\\\n            CoCo-Agent &N/A & \\textit{Text+Vision} & \\textbf{88.27} & \\textbf{92.59}  & \\textbf{91.72} & \\textbf{98.39} &96.15  & 65.90 \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n        \\caption{Results on META-GUI. }\n\t\\label{mainmetagui}\n\\end{table*}\n\n\\subsection{Baselines}\nFor AITW, our proposed approach is compared with the following baselines.\n\n$\\bullet$ \\textbf{Uni-modal API-based methods.}  \n\\citet{rawles2023android} and \\citet{zhang2023you} have evaluated 5-shot performance on \n\\textbf{PaLM-2} \\cite{anil2023palm} and \\textbf{ChatGPT} \\cite{ouyang2022training}. The images are represented by pseudo HTML codes. The action target prediction is the item name or index, without verifying the coordinate numbers.\n\n$\\bullet$ \\textbf{Multimodal API-based methods.}  \\textbf{MM-Navigator} \\cite{yan2023gpt} is a GPT-4V-based multimodal agent, achieving few-shot SOTA.\n\n$\\bullet$ \\textbf{Training-based methods.} \n(i) \\textbf{Behavioural Cloning} \\cite{rawles2023android} is a Transformer-based specialized agent that models goals, screens, and historical information using a BERT \\cite{devlin-etal-2019-bert}. \n(ii) \\textbf{LLaMA-2} is shown as a representative trainable uni-modal LLM with pseudo HTML code inputs instead of images. The results are from \\citet{zhang2023you}.\n(iii) \\textbf{Auto-UI} \\cite{zhang2023you} bases on a multimodal encoder-decoder language model with T5 and BLIP. \n(iv) \\textbf{CogAgent} \\cite{hong2023cogagent} is a 9B-parameter visual LLM pre-trained for specializing in GUI understanding with a novel high-resolution cross module, which tops on AITW.\n\nFor META-GUI, we present baselines following \\citet{sun2022meta} including \\textbf{LayoutLMs} \\cite{xu2020layoutlm, xu-etal-2021-layoutlmv2}, \\textbf{BERT}, and \\textbf{m-BASH} \\cite{sun2022meta}. All of those need training.\nm-BASH achieves SOTA which is a multi-task Transformer with Faster R-CNN \\cite{ren2015faster} and ROI pooling for vision modeling.\n\n\\subsection{Main Results}\nTables \\ref{mainaitw} and \\ref{mainmetagui} present the main experimental results. \n\\textbf{Our method surpasses the baselines significantly and achieves overall state-of-the-art performance (except for Single subset).} The unified model shows consistent advances compared to the separate training, indicating that the model learns generality across various situations.\n\nThe lower part of Table \\ref{mainaitw} shows the detailed performance on AITW.\n\\textbf{CoCo-Agent is enabled to mimic the behavior patterns on GUI, while the limitations lie in predicting target items and scroll directions.}\n(i) The action type scores achieve around 90\\%. This high level indicates that the agent can learn the action patterns to operate GUI. The lower CoT type scores indicate that it is harder for the agent to differentiate \\textit{dual point} actions. This is reasonable as these types of action can be much more flexible than others whose effects are more definite, like \\textit{press back} and \\textit{press home}.\n(ii) The prediction of items and directions is more difficult for agents. Especially, the item accuracy is close to the action accuracy. \n(iii) Input prediction is relatively easy, and F1 scores are up to 97\\%.\n\nOn META-GUI, we can also observe a significant improvement in action accuracy (12\\%).\nThe accuracy of the item increases by a very large margin, while the action type and swipe direction accuracy is close to the perfect score. The input and utterance predictions are relatively consistent.\n\n\\section{Analysis}\nThis section presents further analysis and discussions. Section \\ref{51} shows the effects of comprehensive cognition elements by ablation and replacement. Section \\ref{52} discusses the capability of visual modules, followed by future action prediction in Section \\ref{53}. Dataset features are analyzed in Section \\ref{54}, including action type categories and alignment with realistic scenarios.\n\n\\subsection{Effects of Environment Elements}\n\\label{51}\n\\subsubsection{Ablation}\nOur method combines CEP and CAP to characterize the GUI environment.\nWe ablate each CEP element along with the refactoring method of CAP to observe their significance for the CoCo-Agent.\nResults are shown in Table \\ref{ablation}. The improvement of each element is significant, especially for layouts (+5.82\\%) and action history (+5.63\\%). \n\nBesides the coordinates, the layouts provide the item names like icon names and texts shown on the screen. When combined with CAP, they bridge the prediction through rationales, making predictions easier than direct coordinate grounding.\nNotably, although no historical screens are provided, historical actions with complete parameters lead to better scores than historical action types.\n\n\\begin{table}[thb]\n\t\\centering\\small\n        \\setlength{\\abovecaptionskip}{0.1cm}\n        \\setlength{\\belowcaptionskip}{-0.4cm}\n\t{\\begin{tabular}{p{0.3cm}p{0.3cm}p{0.3cm}p{0.6cm}p{1.6cm}p{0.8cm}p{0.8cm}}\n\t\t\\toprule\n\t\tGoal & Img. &CAP & Layout &History  & \\textbf{General} &\\textbf{META} \\\\\n            \\midrule\n            \\ding{51} &\\ding{51} &\\ding{55}& \\ding{55} & 0   & 57.81 & 73.60\\\\\n            \\ding{51} &\\ding{51} & \\ding{51}& \\ding{55} & 0   & 58.47 &76.90\\\\\n            \\ding{51} &\\ding{51} & \\ding{51}& \\ding{51} & 0   & 64.29 &85.55\\\\\n            \\ding{51} &\\ding{51} & \\ding{51}& \\ding{51} & 8 act. types   & 67.80 & 86.99\\\\\n            \\ding{51} &\\ding{51} & \\ding{51}& \\ding{51} & 8 full actions  & 69.92 & 88.27\\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\t}\n        \\caption{Action accuracy for ablation studies on General of AITW and META-GUI (``META''). ``Act. types'' denotes only to provide history action types, while ``full actions'' denotes to provide actions with complete parameters.}\n\t\\label{ablation}\n\\end{table}\n\n\\subsubsection{Replacement}\nActions on GUI can be flexible and the dependency on the environment is complex. \nThus, we design a related task to probe the environment modeling further. \nWe choose one element from the goal, the screen, the layouts, and the action history and replace it with another from a random data point. \nWith such corrupted input, the agent is trained to select the replaced element and also predict the next action.\n\n\\begin{table}[htb]\n\t\\centering\\small\n        \\setlength{\\belowcaptionskip}{-0.2cm}\n\t{\\begin{tabular}{p{1.2cm}p{0.5cm}p{0.5cm}p{0.5cm}p{0.7cm}p{0.5cm}p{0.5cm}}\n            \\toprule\n            Replace & None & Goal & Img & Layout & Hist. & Avg. \\\\\n                \\midrule\n                Detection  &N/A &94.60 & 93.94 & 91.02 &92.69 &93.06\\\\\n                Action  &70.96 &63.21 & 59.69 &57.55 &58.45 &59.73\\\\\n            \\bottomrule\n        \\end{tabular}\n        }\n        \\caption{Results after replacement on General subset. \\textit{Detection} denotes the accuracy of the replaced element selection. \\textit{Action} denotes action accuracy.}\n\t\\label{corruption}\n\\end{table}\n\nThe results are consistent with intuitions. \n(i) The wrong image and goal are more obvious replacements and get higher detection accuracy, while the layouts and action history are more complex to distinguish. \n(ii) Regarding action prediction, the accuracy decreases more with wrong layouts or wrong action history.\nThose are hard to memorize and require complex modeling, therefore rely more on correct inputs.\n(iii) The damage caused by a wrong image is limited due to the complement from layouts.\nThis suggests again that layouts are important fine-grained complements for the screen image, while the image gives an overall impression and the layouts describe detailed information. \n\\begin{table}[thb]\n\t\\centering\\small\n        \\setlength{\\abovecaptionskip}{0.2cm}\n        \\setlength{\\belowcaptionskip}{-0.3cm}\n\t{\\begin{tabular}{p{1.1cm}p{2.2cm}p{2.2cm}p{0.4cm}}\n\t\t\\toprule\n\t\t  Model & Vision Encoder & LM & Acc   \\\\\n            \\midrule\n            Auto-UI & BILP-2 Encoder &FLAN-Alpaca & 65.9  \\\\\n            MMICL\\textsuperscript{\\ref{pre1k}} & Q-Former & Flan-T5-xxl & 56.4 \\\\\n            mPLUG\\textsuperscript{\\ref{pre1k}} & Abstractor & Vicuna-7B & 53.0\\\\\n            CogAgent & Low \\& High Res. & Vicuna-7B & 65.4 \\\\\n            LLaVA & CLIP  & Llama-2-chat-7B & 58.9\\\\\n            Ours & CLIP \\& Layout & Llama-2-chat-7B & 71.0\\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\t}\n        \\caption{GUI agents with different vision encoders.}\n\t\\label{vision}\n\\end{table}\n\n\\begin{table}[thb]\n\t\\centering\\small\n        \\setlength{\\abovecaptionskip}{0.2cm}\n        \\setlength{\\belowcaptionskip}{-0.3cm}\n\t{\\begin{tabular}{p{1.2cm}p{1.0cm}p{1.0cm}p{1.0cm}p{1.0cm}}\n\t\t\\toprule\n\t\t  Train & 1-next & 3-next & 3-next & 3-next   \\\\\n            \\midrule\n            Test & 1-next & 1-next & 2-next & 3-next \\\\\n            \\midrule\n            Accuracy  & 70.96 & 58.90 & 43.40 & 37.74 \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\t}\n        \\caption{Future action prediction accuracy.}\n\t\\label{future}\n\\end{table}\n\n\\begin{table*}[htb]\n\t\\centering\\small\n        \\setlength{\\abovecaptionskip}{0.2cm}\n        \\setlength{\\belowcaptionskip}{-0.2cm}\n        \\begin{tabular}{p{1.4cm}p{1.7cm}p{0.5cm}p{1.3cm}p{0.4cm}p{0.4cm}p{0.4cm}p{0.4cm}p{0.3cm}p{0.4cm}p{0.3cm}p{0.4cm}p{0.4cm}p{0.5cm}}\n\t\t\\toprule\n\t\t\\textbf{Dataset} & \\textbf{Model} &\\multicolumn{2}{c}{\\textbf{Dual Point}} & \\multicolumn{2}{c}{\\textbf{Text}} & \\multicolumn{2}{c}{\\textbf{Press Back}} & \\multicolumn{2}{c}{\\textbf{Press Home}} & \\multicolumn{2}{c}{\\textbf{Press Enter}} & \\multicolumn{2}{c}{\\textbf{Complete}} \\\\\n            \\midrule\n            &  & Prop. &Acc& Prop. &Acc & Prop. &Acc & Prop. &Acc & Prop. &Acc & Prop. &Acc \\\\\n            \\midrule\n            \\multirow{2}{*}{General} & LLaVA  & \\multirow{2}{*}{86.09}&54.16|84.71 & \\multirow{2}{*}{10.90} &86.09 & \\multirow{2}{*}{1.17}&8.16 & \\multirow{2}{*}{5.36}&77.73 &  \\multirow{2}{*}{2.61} &41.55 & \\multirow{2}{*}{10.67} &62.53 \\\\\n            & CoCo-Agent  &  &67.73|91.59 & & 84.34 & &15.31 & &89.76 &&57.99 & &78.19 \\\\\n            \\hdashline\n            \\multirow{2}{*}{Install} & LLaVA  & \\multirow{2}{*}{69.26}& 72.53|90.77 &\\multirow{2}{*}{11.77}  & 92.19 & \\multirow{2}{*}{1.96} &14.98 & \\multirow{2}{*}{5.79} & 67.38 &\\multirow{2}{*}{0.81} & 12.69  &  \\multirow{2}{*}{10.38}& 67.66  \\\\\n            & CoCo-Agent  & & 80.46|93.84 & & 93.20 & & 41.35 & & 82.19& &  68.53 & & 83.15 \\\\\n            \\hdashline\n            \\multirow{2}{*}{GoogleApps} & LLaVA & \\multirow{2}{*}{78.42}& 71.41|93.95 & \\multirow{2}{*}{1.54} & 75.73 & \\multirow{2}{*}{1.34} & 11.13 &\\multirow{2}{*}{6.00} & 74.14 & \\multirow{2}{*}{0.07} & 18.18  & \\multirow{2}{*}{12.63} & 71.52 \\\\\n            & CoCo-Agent & &75.41|95.04 & &80.11  & & 23.93 & & 86.60 & & 39.39 & & 83.40  \\\\\n            \\hdashline\n            \\multirow{2}{*}{Single} & LLaVA & \\multirow{2}{*}{49.28} & 79.04|88.09 &\\multirow{2}{*}{14.06} &89.74 & \\multirow{2}{*}{0.17} & 57.14 & \\multirow{2}{*}{0.23} & 89.47 & \\multirow{2}{*}{4.52} & 72.43 & \\multirow{2}{*}{31.74} & 90.06  \\\\\n            & CoCo-Agent & & 88.94|96.92 & & 93.22 & &64.29 & & 100.00 & & 75.95  & &96.73\\\\\n            \\hdashline\n            \\multirow{2}{*}{WebShop.} & LLaVA  & \\multirow{2}{*}{72.64} & 64.78|91.55 &\\multirow{2}{*}{11.96} & 86.51 & \\multirow{2}{*}{0.56}&14.00 & \\multirow{2}{*}{3.82}& 75.67 & \\multirow{2}{*}{3.29} & 36.32 & \\multirow{2}{*}{7.73}& 57.08 \\\\\n            & CoCo-Agent & &72.52|94.09 & & 88.72 & & 28.00 & & 89.15 & & 73.90 & & 74.04 \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n        \\caption{Accuracy of different types of actions. The agents are in the unified training setting. \\textit{Prop.} is type proportion in datasets. \\textit{Acc} is action accuracy|type accuracy for \\textit{Dual Point} and action accuracy for others. }\n\t\\label{difftype}\n\\end{table*}\n\\subsection{Visual Capability}\n\\label{52}\nThe vision encoder and projector largely influence the visual capability of GUI agents.\nWe compare a range of visual LMs with various vision encoders.\n\nThe results are shown in Table \\ref{vision}. Our integrated CLIP with projector encodes an image to a 256-length vector with a 4096 hidden size.\\textsuperscript{\\ref{clip}}\nWith fine-grained layouts, CoCo-Agent receives exhaustive visual information.\nAuto-UI uses a BLIP-2 with pooling leading to a 1-length image vector, which is integrated into language embedding by an attention-based fusion module.\nDifferently, MMICL adopts Q-Former to learn a 32-length query vector, and mPLUG adopts Abstractor to learn a 64-length vector. \\footnote{The first 1000 samples in \\textit{General}, the most difficult subset of AITW.\\label{pre1k}} \nCogAgent uses an EVA2-CLIP-E \\cite{sun2023eva} as a \\textit{low-resolution encoder} and an EVA2-CLIP-L \\cite{sun2023eva} with cross-attention layers as a \\textit{high-resolution encoder}. In cross-attention layers, the high-resolution image features interact with each layer of the language model.\n\nThe models with only an image encoder outperform those with learnable queries.\nLearning queries from \\textit{image-text attention} can be unsuitable for GUI tasks, as input texts are complex and different from generic captions. \nBesides the overall semantic impression of images, straightforward textual layouts can work as an even better high-resolution module for image detail enhancement.\n\n\\subsection{Future Actions}\n\\label{53}\nThis section considers a more challenging setting of $n-$next actions prediction. \nThe task is much less trivial as the agent only receives the environment state $s_t$, without the perception of future states $s_{t+1:t+n-1}$.\nThus, predicting future actions $a_{t:t+n-1}$ involves harder reasoning, planning, and environment simulation.\nTable \\ref{future} shows the results with $n=3$. Although the next action can be predicted with 70.96\\% accuracy, predicting the following actions without environmental feedback remains to be improved.\n\n\\subsection{Dataset Features} \n\\label{54}\n\\subsubsection{Action Type Categories}\nEach action type has very different proportions in AITW, which is decided by the unbalanced distribution in natural operations. For example, the \\textit{click} action is the most frequent but the \\textit{complete} action appears at most once in each episode.\nThus, we divide datasets into categories according to the ground truth action type. Table \\ref{difftype} shows the proportion and action accuracy.\n(i) \\textit{Dual point} action (including click, tap, and scroll) accounts for 69.26\\% - 86.09\\% in long-episode tasks and accounts for around half even in the Single subset. Other types, especially \\textit{press back} and \\textit{press enter} consistently account for low proportions.\nSuch an unbalance can limit the performance of less frequent actions.\n(ii) For \\textit{dual point} type, the data is sufficient and type accuracy scores are up to above 90\\%. The action accuracy scores are limited by the difficulties in predicting target and direction scores as shown in Table \\ref{mainaitw}. \n(iii) Single subset shows better performance on all action types and more significant on less frequent actions.\nThis is because the samples in Single are segmented sub-goals that give clear instructions and require fewer steps.\n\n\\subsubsection{Potential for Realistic Scenarios}\n\\label{542}\nThere is a disparity between the evaluation and realistic scenarios.\nThe benchmarks show randomness because the actions can be stochastically chosen with different paths for the same goal.\nHowever, when the predicted action indicates an alternative path, it is reasonable but fails to match the label.\nThis leads to an underestimation. Thus, the agent has extra potential in practice. \n\nWe study the first 500 samples of General dataset to see this phenomenon. In the first 500 actions, there are 118 (23.6\\%) actions whose predictions are different from the labels. Our human evaluation considers two criteria. We check the predicted action with the goal, the last action, the present screen, and the next screen to see if (i) the predicted action can result in a situation that is similar to the next screen, or (ii) the predicted action is consistent with the goal.\nWe observed that only 54 (10.8\\%) samples strictly contradict the episode, while other 64 predictions do not betray the goal. Among the 64 samples, there are 25 (5\\%) predicted actions that can lead to a similar next state. \nFor example, after typing a query in the search bar of a search engine, many actions can lead to the search results including \\textit{press enter}, \\textit{click the magnifier icon}, or \\textit{click a proper query suggestion} (Figure \\ref{same} and \\ref{nobetray}).\n\n\\section{Conclusion}\nThis paper proposes CoCo-Agent, an MLLM-based autonomous agent for smartphone GUI.\nOur method facilitates comprehensive cognition with exhaustive perception and reliable action response. \nCoCo-Agent is enhanced with two approaches. Comprehensive environment perception (CEP) integrates GUI perception, and conditional action prediction (CAP) enables a decomposition of complex commands.\nCoCo-Agent achieves SOTA performance in experiments on two GUI datasets.\nFurther analysis shows that the agent learns the behavior patterns in smartphone GUI.\nThe significance of the proposed enhancements is also verified.\nWe discuss the unbalanced category distributions and the underestimation of agent performance.\nOur work reveals the promising capabilities of MLLMs as autonomous agents, especially for complex environment perception and action response.\n\n\\section*{Limitations}\nWe acknowledge the limitations of this work.\n(i) Resource consumption. The data is of a large amount. The training process costs computational resources compared to the zero-shot methods. Whereas, the training process only needs to be conducted once. And our unified model achieves the SOTA performance. \nEfficient LLM training or inference methods can improve the balance between resource cost and performance.\n(ii) More complex settings. As is shown in Section \\ref{53}, the predictions of future action remain to be improved. The ultimate goal is to empower the agent to operate the full episodes in a simulated GUI environment or a realistic device.\n\n\\section*{Future Work}\nThe following directions for future work are proposed based on our study.\nFirst, GUI agents require generalization ability to support new instructions, new applications, or even new operating systems. The effectiveness of CoCo-Agent in task adaptation across domains of AITW has been preliminarily validated.\nSecond, GUI agents require improved multimodal training strategies. Multimodal LLMs can be strengthened by integrating GUI perception into vision-language pre-training or instruction tuning.\nThird, GUI agents require comprehensive measurements. The measurements can be improved to reflect different paths for the same goal in practical scenarios.\nLast but not least, there is still room for performance improvement (79.05\\% for AITW and 88.27\\% for META-GUI) and future action reasoning and planning for full-episode prediction.\n\n\\section*{Ethics Statement}\nThis section presents ethics statements in the following aspects.\n(i) Data Privacy. The research dataset AITW \\cite{rawles2023android} has declared control over Personal Identifiable Information. The instructions contain no toxic, biased, or misleading content. The META-GUI is based on a task-oriented dialogue dataset, SMCalFlow \\cite{andreas-etal-2020-task}, crowdsourced on Amazon Mechanical Turk with qualification requirements.  CoCo-Agent does not rely on LLM APIs, preserving privacy data from leakage to LLM companies.\n(ii) System security. Compared to the command-line interface (CLI), the GUI is more interpretable and controllable. Since GUI actions follow human behavior, security considerations can better align with human-oriented mechanisms, which are already implemented in existing GUIs for operating systems. However, the potential risks of GUI agents have yet to be well explored \\cite{yuan2024rjudge, Yang2024WatchOF}.\n(iii) Potential social impacts. On the one hand, GUI automation can facilitate efficiency and save human resources for more high-level work. On the other hand, malicious actors could abuse GUI agents to achieve undesirable purposes. For practical applications of GUI agents, platforms may need to update detection, authorization, and governance mechanisms to control potential risks for social impacts.\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2308.15272v4.tex",
        "arXiv-2312.11190v2.tex",
        "arXiv-2402.11941v3.tex"
    ],
    "group_id": "group_23",
    "response": "### Summary: Large Language Models (LLMs) for Mobile Task Automation\n\n#### Introduction\nMobile task automation is a rapidly evolving field that seeks to enhance user interaction with smartphones by enabling voice-based, hands-free operations. This technology has the potential to significantly improve user experience and productivity, especially for individuals with sensory or motor limitations. Traditional methods of task automation, such as Programming By Demonstration (PBD), have limitations in scalability and adaptability, often requiring significant manual effort from developers or users to configure and implement task workflows. Recent advancements in Large Language Models (LLMs) have opened new avenues for task automation, leveraging their ability to understand and reason about complex tasks and environments. However, integrating LLMs into mobile task automation presents unique challenges, including the need for a robust graphical user interface (GUI) representation, the incorporation of domain-specific knowledge, and the optimization of computational costs. This summary explores three recent papers that address these challenges, each proposing innovative solutions to enhance the effectiveness and practicality of mobile task automation using LLMs.\n\n#### Paper 1: \\name - LLM-powered Task Automation in Android\nThe first paper introduces \\name, a mobile task automation system that leverages LLMs to automate tasks on Android applications without requiring manual effort. \\name addresses the key challenges of GUI representation, knowledge integration, and cost optimization through a combination of techniques. The system converts GUI states into simplified HTML representations, which are then used to query LLMs for action guidance. This HTML representation includes essential attributes of UI elements, such as their ID, label, and onclick properties, to provide context for the LLM. To augment the LLM with app-specific knowledge, \\name employs an exploration-based memory injection technique, which involves randomly exploring the app to generate a UI Transition Graph (UTG) and using this graph to synthesize simulated tasks. These simulated tasks are then used to fine-tune smaller LLMs, enhancing their reasoning and planning capabilities. Additionally, \\name optimizes the efficiency of LLM queries by reducing and simplifying the prompts based on app knowledge, thereby minimizing computational overhead.\n\nThe authors also present a benchmark suite, \\datasetname, which consists of 158 common tasks across 13 popular apps. This benchmark evaluates the performance of \\name and other LLM-based methods in terms of action accuracy and task completion rate. The results demonstrate that \\name achieves an overall action accuracy of 90.9% and a task completion rate of 71.3%, significantly outperforming existing baselines. The system is evaluated using both state-of-the-art online LLM services (GPT-3.5 and GPT-4) and open-source on-device LLMs (Vicuna), showcasing its versatility and effectiveness.\n\n#### Paper 2: VisionTasker - Mobile Task Automation Using Vision-Based UI Understanding and LLM Task Planning\nThe second paper presents VisionTasker, a two-stage framework designed to automate mobile tasks by combining vision-based UI understanding and LLM task planning. VisionTasker eliminates the need for view hierarchies and large datasets by directly analyzing UI screenshots to extract semantic information. The framework consists of three main components: a UI understanding module, a task planning module, and an execution module. The UI understanding module uses advanced vision models, such as YOLOv8 for widget detection and PaddleOCR for text recognition, to identify and interpret UI elements. These elements are then grouped into semantic blocks based on visual layout and proximity principles, providing a natural language description of the UI.\n\nThe task planning module relies on an LLM, such as Ernie Bot, to break down and plan tasks into sequential steps. The LLM uses a prompt that includes the task description, action history, and UI semantics to generate specific low-level actions. The execution module translates these natural language actions into corresponding operation commands, such as touch events and text inputs, and executes them on the smartphone. VisionTasker also incorporates a Programming By Demonstration (PBD) mechanism, which allows the agent to prompt users for manual execution of specific steps and then integrate these steps into its prompts to improve task planning.\n\nVisionTasker is evaluated on four public datasets and 147 real-world tasks, demonstrating superior performance in recognizing UI elements and interpreting unmatched buttons. The system achieves an overall action accuracy of 87% and a task completion rate of 76%, surpassing human performance in unfamiliar tasks and showing significant improvements when integrated with the PBD mechanism. The authors also discuss the limitations of their approach, such as the potential for false positives in vision-based detection and the need for more advanced planning strategies.\n\n#### Paper 3: CoCo-Agent - A Comprehensive Cognitive MLLM Agent for Smartphone GUI Automation\nThe third paper introduces CoCo-Agent, a comprehensive cognitive MLLM agent designed for smartphone GUI automation. CoCo-Agent employs a multimodal backbone, specifically LLaVA, and enhances its perception and action response capabilities through two novel approaches: comprehensive environment perception (CEP) and conditional action prediction (CAP). CEP integrates fine-grained layouts and historical actions to provide a more exhaustive understanding of the GUI environment, while CAP decomposes complex action commands into sub-problems, making the task planning process more systematic and reliable.\n\nCoCo-Agent is evaluated on two datasets, AITW and META-GUI, and achieves state-of-the-art performance in action accuracy and task completion rates. The system's performance is significantly improved by the inclusion of layouts and historical actions, which help the agent to better understand the context and predict the correct actions. The authors also conduct ablation studies and replacement experiments to validate the significance of each component in the framework. They find that layouts and historical actions are crucial for enhancing the agent's performance, especially in predicting target items and scroll directions.\n\n#### Commonalities and Innovations\nAll three papers share the common goal of leveraging LLMs to automate mobile tasks, but they each propose unique solutions to overcome the challenges of GUI representation, knowledge integration, and cost optimization. \\name focuses on the integration of app-specific knowledge through dynamic analysis and fine-tuning of smaller LLMs, while VisionTasker emphasizes the use of vision-based UI understanding to provide natural language descriptions of the interface. CoCo-Agent, on the other hand, combines comprehensive environment perception with conditional action prediction to enhance the agent's reasoning and planning abilities.\n\nThe key innovation in \\name is the use of app memory and fine-tuning techniques to improve the performance of smaller LLMs, making the system more cost-effective. VisionTasker introduces a vision-based UI understanding module that can detect and interpret UI elements without relying on view hierarchies, offering a more flexible and robust approach to GUI automation. CoCo-Agent's main contribution is the systematic decomposition of action commands into sub-problems, which allows the agent to better understand and execute complex tasks.\n\n#### Comparison of Results\nThe three systems are evaluated using different datasets and metrics, but they all demonstrate significant improvements over existing baselines. \\name achieves an overall action accuracy of 90.9% and a task completion rate of 71.3% on \\datasetname, outperforming GPT-4-powered baselines by 36.4% and 39.7%. VisionTasker achieves an overall action accuracy of 87% and a task completion rate of 76% on 147 real-world tasks, surpassing human performance in unfamiliar tasks and showing significant improvements with the PBD mechanism. CoCo-Agent achieves state-of-the-art performance on both AITW and META-GUI, with action accuracies of 79.05% and 88.27%, respectively.\n\nThe differences in performance can be attributed to the specific design choices and techniques employed by each system. \\name's use of app memory and fine-tuning enhances the reasoning abilities of smaller LLMs, leading to higher action accuracy. VisionTasker's vision-based UI understanding module provides a more comprehensive and accurate representation of the GUI, improving task completion rates. CoCo-Agent's decomposition of action commands into sub-problems allows for more systematic and reliable task planning, which is reflected in its high action accuracy scores.\n\n#### Conclusion\nThe three papers highlight the potential of LLMs in mobile task automation and propose innovative solutions to address the challenges of GUI representation, knowledge integration, and cost optimization. \\name demonstrates the effectiveness of combining app-specific knowledge with LLMs, VisionTasker emphasizes the use of vision-based UI understanding to enhance task planning, and CoCo-Agent introduces a systematic approach to action prediction and execution. These systems achieve state-of-the-art performance on various benchmarks and real-world tasks, showcasing the potential of LLMs in automating complex mobile tasks.\n\nFuture research directions include improving the generalization ability of these agents to support new instructions and applications, developing more efficient multimodal training strategies, and refining the measurement criteria to better reflect the practical performance of GUI agents. Additionally, addressing the limitations of these systems, such as resource consumption and potential risks, will be crucial for their widespread adoption and practical usability. The field of mobile task automation is poised for significant advancements as researchers continue to explore the integration of LLMs and advanced perception techniques, ultimately bringing truly intelligent and helpful personal assistants closer to reality."
}