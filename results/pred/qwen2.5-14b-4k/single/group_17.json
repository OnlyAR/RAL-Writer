{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Pareto Multi-Task Learning}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nMulti-task learning is a powerful method for solving multiple correlated tasks simultaneously. However, it is often impossible to find one single solution to optimize all the tasks, since different tasks might conflict with each other. Recently, a novel method is proposed to find one single Pareto optimal solution with good trade-off among different tasks by casting multi-task learning as multiobjective optimization. In this paper, we generalize this idea and propose a novel Pareto multi-task learning algorithm (Pareto MTL) to find a set of well-distributed Pareto solutions which can represent different trade-offs among different tasks. The proposed algorithm first formulates a multi-task learning problem as a multiobjective optimization problem, and then decomposes the multiobjective optimization problem into a set of constrained subproblems with different trade-off preferences. By solving these subproblems in parallel, Pareto MTL can find a set of well-representative Pareto optimal solutions with different trade-off among all tasks. Practitioners can easily select their preferred solution from these Pareto solutions, or use different trade-off solutions for different situations.  Experimental results confirm that the proposed algorithm can generate well-representative solutions and outperform some state-of-the-art algorithms on many multi-task learning applications.\n\\end{abstract}\n\n\\section{Introduction}\n\n\\begin{wrapfigure}{R}{0.41\\linewidth}\n\t\\begin{minipage}{\\linewidth}\n    \\begin{figure}[H]\n\t    \\centering\n\t    \\includegraphics[width=1.0 \\linewidth]{Figures/PF_MTL.png}\n\t    \\caption{\\textbf{Pareto MTL} can find a set of widely distributed Pareto solutions with different trade-offs for a given MTL. Then the practitioners can easily select their preferred solution(s).}\n        \\label{MOPMTL}\n    \\end{figure}\n\t\\end{minipage}\n\\end{wrapfigure}\n\nMulti-task learning (MTL)~\\cite{caruana1997multitask}, which aims at learning multiple correlated tasks at the same time, is a popular research topic in the machine learning community. By solving multiple related tasks together, MTL can further improve the performance of each task and reduce the inference time for conducting all the tasks in many real-world applications. Many MTL approaches have been proposed in the past, and they achieve great performances in many areas such as computer vision~\\cite{kokkinos2017ubernet}, natural language processing~\\cite{subramanian2018learning} and speech recognition~\\cite{huang2015rapid}.\n\nMost MTL approaches are proposed for finding one single solution to improve the overall performance of all tasks~\\cite{ruder2017overview,zhang2017survey}. However, it is observed in many applications that some tasks could conflict with each other, and no single optimal solution can optimize the performance of all tasks at the same time~\\cite{kendall2017multi}. In real-world applications, MTL practitioners have to make a trade-off among different tasks, such as in self-driving car~\\cite{wang2018dels}, AI assistance~\\cite{kim2017towards} and network architecture search~\\cite{dong2018dpp,cai2018proxylessnas}.\n\n\\clearpage\n\nHow to combine different tasks together and make a proper trade-off among them is a difficult problem. In many MTL applications, especially those using deep multi-task neural networks, all tasks are first combined into a single surrogate task via linear weighted scalarization. A set of fixed weights, which reflects the practitioners' preference, is assigned to these tasks. Then the single surrogate task is optimized. Setting proper weights for different tasks is not easy and usually requires exhaustive weights search. In fact, no single solution can achieve the best performance on all tasks at the same time if some tasks conflict with each other.\n\nRecently, Sener and Koltun~\\cite{sener2018multi} formulate a multi-task learning problem as a multi-objective optimization problem in a novel way. They propose an efficient algorithm to find one Pareto optimal solution among different tasks for a MTL problem. However, the MTL problem can have many (even an infinite number of ) optimal trade-offs among its tasks, and the single solution obtained by this method might not always satisfy the MTL practitioners' needs.\n\nIn this paper, we generalize the multi-objective optimization idea~\\cite{sener2018multi} and propose a novel Pareto Multi-Task Learning (Pareto MTL) algorithm to generate a set of well-representative Pareto solutions for a given MTL problem. As shown in Fig.~\\ref{MOPMTL}, MTL practitioners can easily select their preferred solution(s) among the set of obtained Pareto optimal solutions with different trade-offs, rather than exhaustively searching for a set of proper weights for all tasks.\n\nThe main contributions of this paper are: \\footnote{The code is available at: \\url{https://github.com/Xi-L/ParetoMTL}}\n\n\\begin{itemize}\n\\item We propose a novel method to decompose a MTL problem into multiple subproblems with different preferences. By solving these subproblems in parallel, we can obtain a set of well-distributed Pareto optimal solutions with different trade-offs for the original MTL.\n\\item We show that the proposed Pareto MTL can be reformulated as a linear scalarization approach to solve MTL with dynamically adaptive weights. We also propose a scalable optimization algorithm to solve all constrained subproblems with different preferences.\n\\item Experimental results confirm that the proposed Pareto MTL algorithm can successfully find a set of well representative solutions for different MTL applications.\n\\end{itemize}\n\n\\section{Related Work}\n\nMulti-task learning (MTL) algorithms aim at improving the performance of multiple related tasks by learning them at the same time. These algorithms often construct shared parameter representation to combine multiple tasks. They have been applied in many machine learning areas. However, most MTL algorithms mainly focus on constructing shared representation rather than making trade-offs among multiple tasks~\\cite{ruder2017overview,zhang2017survey}.\n\nLinear tasks scalarization, together with grid search or random search of the weight vectors, is the current default practice when a MTL practitioner wants to obtain a set of different trade-off solutions. This approach is straightforward but could be extremely inefficient. Some recent works~\\cite{kendall2017multi,chen2018grad} show that a single run of an algorithm with well-designed weight adaption can outperform the random search approach with more than one hundred runs. These adaptive weight methods focus on balancing all tasks during the optimization process and are not suitable for finding different trade-off solutions.\n\nMulti-objective optimization~\\cite{miettinen2012nonlinear} aims at finding a set of Pareto solutions with different trade-offs rather than one single solution. It has been used in many machine learning applications such as reinforcement learning~\\cite{van2014multi}, Bayesian optimization~\\cite{zuluaga2013active,hernandez2016predictive,shah2016pareto} and neural architecture search~\\cite{dong2018dpp,elsken2018efficient}. In these applications, the gradient information is usually not available. Population-based and gradient-free multi-objective evolutionary algorithms~\\cite{zitzler1999evolutionary, Deb2001} are popular methods to find a set of well-distributed Pareto solutions in a single run. However, it can not be used for solving large scale and gradient-based MTL problems.\n\nMulti-objective gradient descent~\\cite{desideri2012mutiple,fliege2016method,fliege2000steepest} is an efficient approach for multi-objective optimization when gradient information is available.  Sener and Koltun~\\cite{sener2018multi} proposed a novel method for solving MTL by treating it as multi-objective optimization. However, similar to the adaptive weight methods, this method tries to balance different tasks during the optimization process and does not have a systematic way to incorporate trade-off preference. In this paper, we generalize it for finding a set of well-representative Pareto solutions with different trade-offs among tasks for MTL problems.\n\n\\section{Multi-Task Learning as Multi-Objective Optimization}\n\n\\begin{figure*}[t]\n\\centering\n\\subfloat[Random Linear Scalarization]{\\includegraphics[width = 0.33\\textwidth]{Figures/toy_random_search.eps}}\n\\subfloat[MOO MTL]{\\includegraphics[width = 0.33\\textwidth]{Figures/toy_moo_mtl.eps}}\n\\subfloat[Pareto MTL (Ours)]{\\includegraphics[width = 0.33\\textwidth]{Figures/toy_pareto_mtl.eps}}\n\\caption{The convergence behaviors of different algorithms on a synthetic example. (a) The obtained solutions of random linear scalarization after 100 runs. (b) The obtained solutions of the MOO-MTL~\\cite{sener2018multi} method after 10 runs. (c) The obtained solutions of the Pareto MTL method proposed by this paper after 10 runs. The proposed Pareto MTL successfully generates a set of widely distributed Pareto solutions with different trade-offs. Details of the synthetic example can be found in section 5.}\n\\label{PF_toy_example}\n\\end{figure*}\n\n\\subsection{MTL as Multi-Objective Optimization}\n\nA MTL problem involves a set of $m$ correlated tasks with a loss vector:\n\\begin{align}\\label{eq:mop}\n\\mathrm{min}_{\\theta} \\ \\mathcal{L}(\\theta) = (\\mathcal{L}_1(\\theta),\\mathcal{L}_2(\\theta),\\cdots,\n\\mathcal{L}_m(\\theta))^{\\mathrm{T}},\n\\end{align}\nwhere $\\mathcal{L}_i(\\theta)$ is the loss of the $i$-th task. A MTL algorithm is to optimize all tasks simultaneously by exploiting the shared structure and information among them.\n\nProblem~(\\ref{eq:mop}) is a multi-objective optimization problem. No single solution can optimize all objectives at the same time. What we can obtain instead is a set of so-called Pareto optimal solutions,  which provides different optimal trade-offs among all objectives. We have the following definitions~\\cite{zitzler1999multiobjective}:\n\n\\textbf{Pareto dominance.} Let $\\theta^a,\\theta^b$ be two points in $\\Omega$, $\\theta^a$ is said to dominate $\\theta^b$ ($\\theta^a \\prec \\theta^b$) if and only if $\\mathcal{L}_i(\\theta^a) \\leq \\mathcal{L}_i(\\theta^b), \\forall i \\in \\{1,...,m\\}$ and $\\mathcal{L}_j(\\theta^a) < \\mathcal{L}_j(\\theta^b), \\exists j \\in \\{1,...,m\\}$.\n\n\\textbf{Pareto optimality.} $\\theta^{\\ast}$ is a Pareto optimal point and $\\mathcal{L}(\\theta^{\\ast})$ is a Pareto optimal objective vector if it does not exist $\\hat \\theta \\in  \\Omega$ such that $\\hat \\theta \\prec \\theta^{\\ast}$.\nThe set of all Pareto optimal points is called the Pareto set. The image of the Pareto set in the loss space is called the Pareto front.\n\nIn this paper, we focus on finding a set of well-representative Pareto solutions that can approximate the Pareto front. This idea and the comparison results of our proposed method with two others are presented in Fig.~\\ref{PF_toy_example}.\n\n\\subsection{Linear Scalarization}\n\nLinear scalarization is the most commonly-used approach for solving multi-task learning problems. This approach uses a linear weighted sum method to combine the losses of all tasks into a single surrogate loss:\t\n\\begin{eqnarray}\\label{eq:sum}\n\\mathrm{min}_{\\theta} \\ \\mathcal{L}(\\theta) = \\sum_{i=1}^{m}\\vw_i \\mathcal{L}_i(\\theta),\n\\end{eqnarray}\nwhere $\\vw_i$ is the weight for the $i$-th task. This approach is simple and straightforward, but it has some drawbacks from both multi-task learning and multi-objective optimization perspectives.\n\nIn a typical multi-task learning application, the weights $\\vw_i$ are needed to be assigned manually before optimization, and the overall performance is highly dependent on the assigned weights. Choosing a proper weight vector could be very difficult even for an experienced MTL practitioner who has expertise on the given problem.\n\nSolving a set of linear scalarization problems with different weight assignments is also not a good idea for multi-objective optimization. As pointed out in~\\cite[Chapter~4.7]{boyd2004convex}, this method can only provide solutions on the convex part of the Pareto front. The linear scalarization method with different weight assignments is unable to handle a concave Pareto front as shown in Fig.~\\ref{PF_toy_example}.\n\n\\subsection{Gradient-based method for multi-objective optimization}\n\nMany gradient-based methods have been proposed for solving multi-objective optimization problems~\\cite{desideri2012mutiple,fliege2016method}. Fliege and Svaiter~\\cite{fliege2000steepest} have proposed a simple gradient-based method, which is a generalization of a single objective steepest descent algorithm. The update rule of the algorithm is $\\theta_{t + 1} = \\theta_t + \\eta d_t$ where $\\eta$ is the step size and the search direction $d_t$ is obtained as follows:\n\\begin{eqnarray}\n    \\label{submop1}\n         (d_t,\\alpha_t) = \\text{arg} \\min_{d\\in R^n,\\alpha \\in R} \\alpha + \\frac{1}{2} \\norm{d}^2, s.t.~~~ \\nabla \\mathcal{L}_i(\\theta_t)^Td \\leq \\alpha,  i = 1,...,m.\n\\end{eqnarray}\nThe solutions of the above problem will satisfy:\n\n\\textbf{Lemma 1~\\cite{fliege2000steepest}:} Let $(d^k,\\alpha^k)$ be the solution of problem~(\\ref{submop1}).\n\\begin{enumerate}\n  \\item If $\\theta_t$ is Pareto critical, then $d_t = 0 \\in \\mathbb{R}^n$ and $\\alpha_t = 0$.\n  \\item If $\\theta_t$ is not Pareto critical, then\n    \\begin{eqnarray}\\label{eq:bound}\n    \\begin{aligned}\n    & \\alpha_t \\leq -(1/2) \\norm{d_t}^2 < 0, \\\\\n    & \\nabla \\mathcal{L}_i(\\theta_t)^Td_t \\leq \\alpha_t, i = 1,...,m,\n    \\end{aligned}\n    \\end{eqnarray}\n\\end{enumerate}\nwhere $\\theta$ is called Pareto critical if no other solution in its neighborhood can have better values in all objective functions. In other words, if $d_t = \\boldsymbol{0}$, no direction can improve the performance for all tasks at the same time. If we want to improve the performance for a specific task, another task's performance will be deteriorated (e.g., $\\exists i, \\mathcal{L}_i(\\theta_t)^Td_t > 0$). Therefore, the current solution is a Pareto critical point. When $d_t \\neq \\boldsymbol{0}$, we have $\\nabla \\mathcal{L}_i(\\theta_t)^Td_t < 0, i = 1,...,m$, which means $d_t$ is a valid descent direction for all tasks. The current solution should be updated along the obtained direction $\\theta_{t + 1} = \\theta_t + \\eta d_t$.\n\nRecently, Sener and Koltun~\\cite{sener2018multi} used the multiple gradient descent algorithm (MGDA)~\\cite{desideri2012mutiple} for solving MTL problems and achieve promising results. However, this method does not have a systemic way to incorporate different trade-off preference information. As shown in Fig.~\\ref{PF_toy_example}, running the algorithm multiple times can only generate some solutions in the middle of the Pareto front on the synthetic example. In this paper, we generalize this method and propose a novel Pareto MTL algorithm to find a set of well-distributed Pareto solutions with different trade-offs among all tasks.\n\n\\section{Pareto Multi-Task Learning}\n\n\\subsection{MTL Decomposition}\n\n\\begin{wrapfigure}{R}{0.45\\linewidth}\n\t\\begin{minipage}{\\linewidth}\n    \\begin{figure}[H]\n    \t\\centering\n    \t\\includegraphics[width=1 \\linewidth]{Figures/MTL_M2M.png}\n    \t\\caption{\\textbf{Pareto MTL} decomposes a given MTL problem into several subproblems with a set of preference vectors. Each MTL subproblem aims at finding one Pareto solution in its restricted preference region.}\n        \\label{MOPM2M}\n    \\end{figure}\n\t\\end{minipage}\n\\end{wrapfigure}\n\nWe propose the Pareto Multi-Task Learning (Pareto MTL) algorithm in this section. The main idea of Pareto MTL is to decompose a MTL problem into several constrained multi-objective subproblems with different trade-off preferences among the tasks in the original MTL. By solving these subproblems in parallel, a MTL practitioner can obtain a set of well-representative solutions with different trade-offs.\n\nDecomposition-based multi-objective evolutionary algorithm~\\cite{zhang2007moea,trivedi2016survey}, which decomposes a multi-objective optimization problem (MOP) into several subproblems and solves them simultaneously, is one of the most popular gradient-free multi-objective optimization methods. Our proposed Pareto MTL algorithm generalizes the decomposition idea for solving large-scale and gradient-based MTL.\n\nWe adopt the idea from~\\cite{liu2014decomposition} and decompose the MTL into $K$ subproblems with a set of well-distributed unit preference vectors $\\{\\vu_1,\\vu_2,...,\\vu_K\\}$ in $R^m_{+}$. Suppose all objectives in the MOP are non-negative, the multi-objective subproblem corresponding to the preference vector $\\vu_k$ is:\n\n\\begin{eqnarray}\n    \\label{submop}\n       \\min_{\\theta}  \\mathcal{L}(\\theta) = (\\mathcal{L}_1(\\theta),\\mathcal{L}_2(\\theta),\\cdots,\n\\mathcal{L}_m(\\theta))^{\\mathrm{T}},   s.t.~~~ \\mathcal{L}(\\theta) \\in \\Omega_k,\n\\end{eqnarray}\nwhere $\\Omega_k (k = 1,...,K)$ is a subregion in the objective space:\n\\begin{eqnarray}\n    \\label{subregion}\n\t\\Omega_k = \\{\\vv \\in R^m_{+}| \\vu_j^T\\vv \\leq \\vu_k^T\\vv, \\forall j = 1,...,K  \\}\n\\end{eqnarray}\nand $\\vu_j^T\\vv$ is the inner product between the preference vector $\\vu_j$ and a given vector $\\vv$. That is to say, $\\vv \\in \\Omega_k$  if and only if $\\vv$ has the smallest acute angle to $\\vu_k$ and hence the largest inner product $\\vu_k^T\\vv$ among all $K$ preference vectors.\n\nThe subproblem (\\ref{submop}) can be further reformulated as:\n\\begin{eqnarray}\n    \\label{submop_v1}\n    \\begin{aligned}\n        & \\min_{\\theta} \\mathcal{L}(\\theta) = (\\mathcal{L}_1(\\theta),\\mathcal{L}_2(\\theta),\\cdots,\n\\mathcal{L}_m(\\theta))^{\\mathrm{T}}\\\\\n        &s.t.~~~ \\mathcal{G}_j(\\theta_t) = (  \\vu_j -\\vu_k)^T \\mathcal{L}(\\theta_t) \\leq 0, \\forall j = 1,...,K, \\\\\n    \\end{aligned}\n\\end{eqnarray}\n\nAs shown in Fig.~\\ref{MOPM2M}, the preference vectors divide the objective space into different sub-regions. The solution for each subproblem would be attracted by the corresponding preference vector and hence be guided to its representative sub-region. The set of solutions for all subproblems would be in different sub-regions and represent different trade-offs among the tasks.\n\n\\subsection{Gradient-based Method for Solving Subproblems}\n\n\\subsubsection{Finding the Initial Solution}\nTo solve the constrained multi-objective subproblem (\\ref{submop}) with a gradient-based method, we need to find an initial solution which is feasible or at least satisfies most constraints. For a randomly generated solution $\\theta_r$, one straightforward method is to find a feasible initial solution $\\theta_0$ which satisfies:\n\\begin{eqnarray}\n    \\label{init_sol}\n    \\begin{aligned}\n        & \\min_{\\theta_0} \\norm{\\theta_0 - \\theta_r}^2\n        &s.t.~~~ \\mathcal{L}(\\theta_0) \\in \\Omega_k.\n    \\end{aligned}\n\\end{eqnarray}\nHowever, this projection approach is an $n$ dimensional constrained optimization problem~\\cite{gebken2017descent}. It is inefficient to solve this problem directly, especially for a deep neural network with millions of parameters. In the proposed Pareto MTL algorithm, we reformulate this problem as unconstrained optimization, and use a sequential gradient-based method to find the initial solution $\\theta_0$.\n\nFor a solution $\\theta_{r}$, we define the index set of all activated constraints as $I(\\theta_{r}) = \\{j |\\mathcal{G}_j(\\theta_{r}) \\geq 0, j = 1,...,K \\}$. We can find a valid descent direction $d_{r}$ to reduce the value of all activated constraints $\\{\\mathcal{G}_j(\\theta_{r})|j \\in I(\\theta_{r})\\}$ by solving:\n\\begin{eqnarray}\n    \\label{submop1_feasible}\n         (d_{r},\\alpha_{r}) = \\text{arg} \\min_{d\\in R^n,\\alpha \\in R} \\alpha + \\frac{1}{2} \\norm{d}^2, s.t.\\nabla \\mathcal{G}_j(\\theta_{r})^Td \\leq \\alpha, j \\in I(\\theta_{r}).\n\\end{eqnarray}\nThis approach is similar to the unconstrained gradient-based method (\\ref{submop1}), but it reduces the value of all activated constraints. The gradient-based update rule is $\\theta_{r_{t+1}} = \\theta_{r_t} + \\eta_r d_{r_t}$ and will be stopped once a feasible solution is found  or a predefined number of iterations is met.\n\n\\subsubsection{Solving the Subproblem}\nOnce we have an initial solution, we can use a gradient-based method to solve the constrained subproblem. For a constrained multiobjective optimization problem, the Pareto optimality restricted on the feasible region $\\Omega_k$ can be defined as~\\cite{fliege2000steepest}:\n\n\\textbf{Restricted Pareto Optimality.} $\\theta^{\\ast}$ is a Pareto optimal point for $\\mathcal{L}(\\theta)$ restricted on $\\Omega_k$ if $\\theta^{\\ast} \\in \\Omega_k$ and  it does not exist $\\hat \\theta \\in  \\Omega_k$ such that $\\hat \\theta \\prec \\theta^{\\ast}$.\n\nAccording to~\\cite{fliege2000steepest,gebken2017descent}, we can find a descent direction for this constrained MOP by solving a subproblem similar to the subproblem ({\\ref{submop1}}) for the unconstrained case:\n\\begin{eqnarray}\n    \\label{submop2}\n    \\begin{aligned}\n         (d_t,\\alpha_t) = &\\text{arg} \\min_{d\\in R^n,\\alpha \\in R} \\alpha + \\frac{1}{2} \\norm{d}^2 \\\\\n        & s.t.~~~ \\nabla \\mathcal{L}_i(\\theta_t)^Td \\leq \\alpha, i = 1,...,m. \\\\\n         & ~~~~~~~~ \\nabla \\mathcal{G}_j(\\theta_t)^Td \\leq \\alpha, j \\in I_{\\epsilon}(\\theta_t),\n    \\end{aligned}\n\\end{eqnarray}\nwhere $I_{\\epsilon}(\\theta)$ is the index set of all activated constraints:\n\\begin{eqnarray}\n    \\label{act_set}\n    I_{\\epsilon}(\\theta) = \\{j \\in I|\\mathcal{G}_j(\\theta) \\geq -\\epsilon \\}.\n\\end{eqnarray}\n\nWe add a small threshold $\\epsilon$ to deal with the solutions near the constraint boundary. Similar to the unconstrained case, for a feasible solution $\\theta_t$, by solving problem~(\\ref{submop2}), we either obtain $d_t = \\boldsymbol{0}$ and confirm that $\\theta_t$ is a Pareto critical point restricted on $\\Omega_k$, or obtain $d_t \\neq \\boldsymbol{0}$ as a descent direction for the constrained multi-objective problem~(\\ref{submop_v1}). In the latter case, if all constraints are inactivated (e.g., $I_{\\epsilon}(\\theta) = \\emptyset$), $d_t$ is a valid descent direction for all tasks. Otherwise, $d_t$ is a valid direction to reduce the values for all tasks and all activated constraints.\n\n\\textbf{Lemma 2~\\cite{gebken2017descent}:} Let $(d^k,\\alpha^k)$ be the solution of problem~(\\ref{submop2}).\n\\begin{enumerate}\n  \\item If $\\theta_t$ is Pareto critical restricted on $\\Omega_k$, then $d_t = \\boldsymbol{0} \\in \\mathbb{R}^n$ and $\\alpha_t = 0$.\n  \\item If $\\theta_t$ is not Pareto critical restricted on $\\Omega_k$, then\n    \\begin{eqnarray}\\label{eq:bound_v2}\n    \\begin{aligned}\n    & \\alpha_t \\leq -(1/2) \\norm{d_t}^2 < 0, \\\\\n    & \\nabla \\mathcal{L}_i(\\theta_t)^Td_t \\leq \\alpha_t, i = 1,...,m \\\\\n    & \\nabla \\mathcal{G}_j(\\theta_t)^Td_t \\leq \\alpha_t, j \\in I_{\\epsilon}(\\theta_t).\n    \\end{aligned}\n    \\end{eqnarray}\n\\end{enumerate}\n\nTherefore, we can obtain a restricted Pareto critical solution for each subproblem with simple iterative gradient-based update rule $\\theta_{t+1} = \\theta_{t} + \\eta_r d_t$. By solving all subproblems, we can obtain a set of diverse Pareto critical solutions restricted on different sub-regions, which can represent different trade-offs among all tasks for the original MTL problem.\n\n\\subsubsection{Scalable Optimization Method}\nBy solving the constrained optimization problem~(\\ref{submop2}), we can obtain a valid descent direction for each multi-objective constrained subproblem. However, the optimization problem itself is not scalable well for high dimensional decision space. For example, when training a deep neural network, we often have more than millions of parameters to be optimized, and solving the constrained optimization problem~(\\ref{submop2}) in this scale would be extremely slow. In this subsection, we propose a scalable optimization method to solve the constrained optimization problem.\n\nInspired by~\\cite{fliege2000steepest}, we first rewrite the optimization problem~(\\ref{submop2}) in its dual form. Based on the KKT conditions, we have\n\\begin{eqnarray}\n    \\label{submop2_KKT}\n         d_t = - (\\sum_{i=1}^{m} \\lambda_i \\nabla \\mathcal{L}_i(\\theta_t) +  \\sum_{j \\in I_{\\epsilon(\\vx)}} \\beta_i \\nabla \\mathcal{G}_j(\\theta_t)), ~~\\sum_{i=1}^{m} \\lambda_i + \\sum_{j \\in I_{\\epsilon(\\theta)}} \\beta_j = 1,\n\\end{eqnarray}\nwhere $\\lambda_i \\geq 0$ and $\\beta_i \\geq 0$ are the Lagrange multipliers for the linear inequality constraints. Therefore, the dual problem is:\n\\begin{eqnarray}\n    \\label{submop2_dual}\n    \\begin{aligned}\n        &\\max_{\\lambda_i,\\beta_j}~ -\\frac{1}{2}\\norm{\\sum_{i=1}^{m} \\lambda_i \\nabla \\mathcal{L}_i(\\theta_t) +  \\sum_{j \\in I_{\\epsilon(\\vx)}} \\beta_i \\nabla \\mathcal{G}_j(\\theta_t)}^2 \\\\\n        & s.t.~~~ \\sum_{i=1}^{m} \\lambda_i + \\sum_{j \\in I_{\\epsilon(\\theta)}} \\beta_j = 1,  \\lambda_i \\geq 0, \\beta_j \\geq 0, \\forall i = 1,...,m, \\forall j \\in I_{\\epsilon}(\\theta).\n    \\end{aligned}\n\\end{eqnarray}\nFor the above problem, the decision space is no longer the parameter space, and it becomes the objective and constraint space. For a multiobjective optimization problem with $2$ objective function and $5$ activated constraints, the dimension of problem~(\\ref{submop2_dual}) is $7$, which is significantly smaller than the dimension of problem~(\\ref{submop2}) which could be more than a million.\n\nThe algorithm framework of Pareto MTL is shown in \\textbf{Algorithm}~\\ref{alg:ParetoMTL}. All subproblems can be solved in parallel since there is no communication between them during the optimization process. The only preference information for each subproblem is the set of preference vectors. Without any prior knowledge for the MTL problem, a set of evenly distributed unit preference vectors would be a reasonable default choice, such as $K+1$ preference vectors $\\{(cos(\\frac{k\\pi}{2K}),sin(\\frac{k\\pi}{2K}))| k = 0,1,...,K\\}$ for 2 tasks. We provide more discussion on preference vector setting and sensitivity analysis of the preference vectors in the supplementary material.\n\n\\clearpage\n\n\\begin{algorithm}[H]\n\t\\caption{Pareto MTL Algorithm}\n\t\\label{alg:ParetoMTL}\n\t\\begin{algorithmic}[1]\n\t\t\n\t\t\\STATE \\textbf{Input:} A set of evenly distributed vectors $\\{\\vu_1,\\vu_2,...,\\vu_K\\}$\n\t\t\\STATE \\textbf{Update Rule:}\n\t\t\\STATE (can be solved in parallel)\n\t\t\\FOR{$k = 1$ to $K$}\n\t\t    \\STATE randomly generate parameters $\\theta_r^{(k)}$\n\t\t    \\STATE find the initial parameters $\\theta_0^{(k)}$ from $\\theta_r^{(k)}$ using gradient-based method\n    \t\t\\FOR{$t = 1$ to $T$}\n    \t\n        \t\t\n        \t\t\\STATE obtain $\\lambda_{ti}^{(k)} \\geq 0, \\beta_{ti}^{(k)} \\geq 0, \\forall i = 1,...,m, \\forall j \\in I_{\\epsilon}(\\theta)$ by solving subproblem~(\\ref{submop2_dual})\n        \t\t\\STATE calculate the direction $d_t^{(k)} = - (\\sum_{i=1}^{m} \\lambda_{ti}^{(k)} \\nabla \\mathcal{L}_i(\\theta^{(k)}_{t}) +  \\sum_{j \\in I_{\\epsilon(\\vx)}} \\beta_{ti}^{(k)} \\nabla \\mathcal{G}_j(\\theta^{(k)}_{t})$\n        \t\t\\STATE update the parameters $\\theta^{(k)}_{t + 1} = \\theta^{(k)}_{t} + \\eta d_t^{(k)}$\n    \t\t\\ENDFOR\n\t\t\\ENDFOR\t\n\t\t\\STATE \\textbf{Output:} The set of solutions for all subproblems with different trade-offs  $\\{\\theta^{(k)}_{T}| k = 1,\\cdot,K\\} $\n\t\t\n\t\\end{algorithmic}\n\\end{algorithm}\n\n\\subsection{Pareto MTL as an Adaptive Linear Scalarization Approach}\n\nWe have proposed the Pareto MTL algorithm from the multi-objective optimization perspective. In this subsection, we show that the Pareto MTL algorithm can be reformulated as a linear scalarization of tasks with adaptive weight assignment. In this way, we can have a deeper understanding of the differences between Pareto MTL and other MTL algorithms.\n\nWe first tackle the unconstrained case. Suppose we do not decompose the multi-objective problem and hence remove all constraints from the problem (\\ref{submop2_dual}), it will immediately reduce to the update rule proposed by MGDA~\\cite{desideri2012mutiple} which is used in~\\cite{sener2018multi}. It is straightforward to rewrite the corresponding MTL into a linear scalarization form:\n\\begin{eqnarray}\n    \\label{update_mtl_nc}\n    \\begin{aligned}\n        & \\mathcal{L}(\\theta_t) = \\sum_{i=1}^{m} \\lambda_i  \\mathcal{L}_i(\\theta_t),\n    \\end{aligned}\n\\end{eqnarray}\nwhere we adaptively assign the weights $\\lambda_i$ by solving the following problem in each iteration:\n\\begin{eqnarray}\n    \\label{submop2_dual_nc}\n        \\max_{\\lambda_i} -\\frac{1}{2}\\norm{\\sum_{i=1}^{m} \\lambda_i \\nabla \\mathcal{L}_i(\\theta_t)}^2, ~~~\n         s.t.~ \\sum_{i=1}^{m} \\lambda_i = 1, ~~~ \\lambda_i \\geq 0, \\forall i = 1,...,m.\n\\end{eqnarray}\n\nIn the constrained case, we have extra constraint terms $ \\mathcal{G}_j(\\theta_t)$.  If $\\mathcal{G}_j(\\theta_t)$ is inactivated, we can ignore it. For an activated $\\mathcal{G}_j(\\theta_t)$, assuming the corresponding reference vector is $\\vu_k$, we have:\n\\begin{eqnarray}\n    \\label{g_act}\n    \\nabla \\mathcal{G}_j(\\theta_t)  = (  \\vu_j -\\vu_k)^T \\nabla \\mathcal{L}(\\theta_t) = \\sum_{i=1}^{m} (\\vu_{ji} - \\vu_{ki}) \\nabla \\mathcal{L}_i(\\theta_t).\n\\end{eqnarray}\nSince the gradient direction $d_t$ can be written as a linear combination of all $\\nabla \\mathcal{L}_i(\\theta_t)$ and $\\nabla \\mathcal{G}_j(\\theta_t)$ as in~(\\ref{submop2_KKT}), the general Pareto MTL algorithm can be rewritten as:\n\\begin{eqnarray}\n    \\label{update_mtl_c}\n    \\mathcal{L}(\\theta_t) = \\sum_{i=1}^{m} \\alpha_i  \\mathcal{L}_i(\\theta_t), ~\\text{where }  \\alpha_i = \\lambda_i + \\sum_{j \\in I_{\\epsilon(\\theta)}} \\beta_j (\\vu_{ji} - \\vu_{ki}),\n\\end{eqnarray}\nwhere $\\lambda_i$ and $\\beta_j$ are obtained by solving problem (\\ref{submop2_dual}) with assigned reference vector $\\vu_{k}$.\n\nTherefore, although MOO-MTL~\\cite{sener2018multi} and Pareto MTL are both derived from multi-objective optimization, they can also be treated as linear MTL scalarization with adaptive weight assignments. Both methods are orthogonal to many existing MTL approaches. We provide further discussion on the adaptive weight vectors in the supplementary material.\n\n\\section{A Synthetic Example}\n\nTo better analyze the convergence behavior of the proposed Pareto MTL, we first compare it with two commonly used methods, namely the linear scalarization method and the multiple gradient descent algorithm used in MOO-MTL~\\cite{sener2018multi}, on a simple synthetic multi-objective optimization problem:\n\\begin{eqnarray}\n    \\label{toy_example}\n    \\begin{aligned}\n        &\\min_{\\vx} f_1(\\vx) = 1 - \\exp{(- \\sum_{i=1}^{d} (\\vx_d - \\frac{1}{\\sqrt{d}})^2)} \\\\\n        &\\min_{\\vx} f_2(\\vx) = 1 - \\exp{(- \\sum_{i=1}^{d} (\\vx_d + \\frac{1}{\\sqrt{d}})^2)}\n    \\end{aligned}\n\\end{eqnarray}\nwhere $f_1(\\vx)$ and $f_2(\\vx)$ are two objective functions to be minimized at the same time and $\\vx = (\\vx_1,\\vx_2,...,\\vx_d)$ is the $d$ dimensional decision variable. This problem has a concave Pareto front on the objective space.\n\nThe results obtained by different algorithms are shown in Fig.~\\ref{PF_toy_example}. In this case, the proposed Pareto MTL can successfully find a set of well-distributed Pareto solutions with different trade-offs. Since MOO-MTL tries to balance different tasks during the optimization process, it gets a set of solutions with similar trade-offs in the middle of the Pareto front in multiple runs. It is also interesting to observe that the linear scalarization method can only generate extreme solutions for the concave Pareto front evenly with 100 runs. This observation is consistent with the theoretical analysis in~\\cite{boyd2004convex} that the linear scalarization method will miss all concave parts of a Pareto front. It is evident that fixed linear scalarization is not always a good idea for solving the MTL problem from the multi-objective optimization perspective.\n\n\\section{Experiments}\n\nIn this section, we compare our proposed Pareto MTL algorithm on different MTL problems with the following algorithms: 1) \\textbf{Single Task}: the single task baseline; 2) \\textbf{Grid Search}: linear scalarization with fixed weights; 3) \\textbf{GradNorm}~\\cite{chen2018grad}:  gradient normalization; 4) \\textbf{Uncertainty}~\\cite{kendall2017multi}: adaptive weight assignments with uncertainty balance; 5) \\textbf{MOO-MTL}~\\cite{sener2018multi}: finding one Pareto optimal solution for multi-objective optimization problem. More experimental results and discussion can be found in the supplementary material.\n\n\\subsection{Multi-Fashion-MNIST}\n\n\\begin{figure}[H]\n\\centering\n\\subfloat[MultiMNIST]{\\includegraphics[width = 0.33\\textwidth]{Figures/new_Multi_MNIST.pdf}}\n\\subfloat[MultiFashionMNIST]{\\includegraphics[width = 0.33\\textwidth]{Figures/new_Multi_Fashion_Two.pdf}}\n\\subfloat[Multi-(Fashion+MNIST)]{\\includegraphics[width = 0.33\\textwidth]{Figures/new_Multi_Fashion_MNIST.pdf}} \\\\\n\\caption{\\textbf{The results for three experiments with Task1\\&2 accuracy:} our proposed Pareto MTL can successfully find a set of well-distributed solutions with different trade-offs for all experiments, and it significantly outperforms Grid Search, Uncertainty and GradNorm. MOO-MTL algorithm can also find promising solutions, but their diversity is worse than the solutions generated by Pareto MTL. }\n\n\\label{mnist_results}\n\\end{figure}\n\nIn order to evaluate the performance of Pareto MTL on multi-task learning problems with different tasks relations, we first conduct experiments on MultiMNIST~\\cite{sabour2017dynamic} and two MultiMNIST-like datasets. To construct the MultiMNIST dataset, we randomly pick two images with different digits from the original MNIST dataset~\\cite{lecun1998gradient}, and then combine these two images into a new one by putting one digit on the top-left corner and the other one on the bottom-right corner. Each digit can be moved up to $4$ pixels on each direction. With the same approach, we can construct a MultiFashionMINST dataset with overlap FashionMNIST items~\\cite{xiao2017fashion}, and a Multi-(Fashion + MNIST) with overlap MNIST and FashionMNIST items.  For each dataset, we have a two objective MTL problem to classify the item on the top-left (task 1) and to classify the item on the bottom-right (task 2). We build a LeNet~\\cite{lecun1998gradient} based MTL neural network similar to the one used in~\\cite{sener2018multi}. The obtained results are shown in Fig.~\\ref{mnist_results}.\n\nIn all experiments, since the tasks conflict with each other, solving each task separately results in a hard-to-beat single-task baseline. Our proposed Pareto MTL algorithm can generate multiple well-distributed Pareto solutions for all experiments, which are compatible with the strong single-task baseline but with different trade-offs among the tasks. Pareto MTL algorithm achieves the overall best performance among all MTL algorithms. These results confirm that our proposed Pareto MTL can successfully provide a set of well-representative Pareto solutions for a MTL problem.\n\nIt is not surprising to observe that the Pareto MTL's solution for subproblems with extreme preference vectors (e.g., $(0,1)$ and $(1,0)$) always have the best performance in the corresponding task. Especially in the Multi-(Fashion-MNIST) experiment, where the two tasks are less correlated with each other. In this problem, almost all MTL's solutions are dominated by the strong single task's baseline. However, Pareto MTL can still generate solutions with the best performance for each task separately. It behaves as auxiliary learning, where the task with the assigned preference vector is the main task, and the others are auxiliary tasks.\n\nPareto MTL uses neural networks with simple hard parameter sharing architectures as the base model for MTL problems. It will be very interesting to generalize Pareto MTL to other advanced soft parameter sharing architectures~\\cite{ruder2017overview}. Some recently proposed works on task relation learning~\\cite{zamir2018taskonomy,Ma2018Modeling,zhang2018learning} could also be useful for Pareto MTL to make better trade-offs for less relevant tasks.\n\n\\subsection{Self-Driving Car: Localization}\n\n\\begin{figure}[H]\n    \\centering\n    \\subfloat{\n        \\adjustbox{valign=b}{\n\n        \\begin{tabular}{cccc}\n            \\hline\n            Method      & Reference             & Translation    & Rotation                                 \\\\\n                        & Vector                & (m)            & ($^\\circ$) \\\\ \\hline\n            Single Task & \\textbf{-}            & 8.392          & 2.461                                    \\\\ \\hline\n                        & (0.25,0.75)           & 9.927          & 2.177                                    \\\\\n            Grid Search & (0.5,0.5)             & 7.840          & 2.306                                    \\\\\n                        & (0.75,0.25)           & 7.585          & 2.621                                    \\\\ \\hline\n            GradNorm    & -                     & 7.751          & 2.287                                    \\\\\n            Uncertainty & -                     & 7.624          & 2.263                                    \\\\\n            MOO-MTL     & -                     & 7.909          & 2.090                                    \\\\ \\hline\n                        & (0,1)                 & \\textbf{7.285} & 2.335                                    \\\\\n            Pareto MTL  & ($\\frac{\\sqrt{2}}{2},\\frac{\\sqrt{2}}{2}$) & 7.724          & 2.156                                    \\\\\n                        & (1,0)                 & 8.411          & \\textbf{1.771}                           \\\\ \\hline\n        \\end{tabular}\n       }\n    }\n    \\subfloat{\n\n        \\includegraphics[width=0.4\\linewidth,valign=b]{Figures/apollpscape.pdf}\n\n        }\n    \\caption{\\textbf{The results of self-location MTL experiment:}  Our proposed Pareto MTL outperforms other algorithms and provides solutions with different trade-offs.}\n\\label{PF_self_localization}\n\\end{figure}\n\nWe further evaluate Pareto MTL on an autonomous driving self-localization problem~\\cite{wang2018dels}. In this experiment, we simultaneously estimate the location and orientation of a camera put on a driving car based on the images it takes. We use data from the apolloscape autonomous driving dataset~\\cite{apolloscape_arXiv_2018,wang2019the}, and focus on the Zpark sample subset. We build a PoseNet with a ResNet18~\\cite{kendall2015posenet} encoder as the MTL model. The experiment results are shown in Fig.~\\ref{PF_self_localization}. It is obvious that our proposed Pareto MTL can generate solutions with different trade-offs and outperform other MTL approaches.\n\nWe provide more experiment results and analysis on finding the initial solution, Pareto MTL with many tasks, and other relative discussions in the supplementary material.\n\n\\section{Conclusion}\nIn this paper, we proposed a novel Pareto Multi-Task Learning (Pareto MTL) algorithm to generate a set of well-distributed Pareto solutions with different trade-offs among tasks for a given multi-task learning (MTL) problem. MTL practitioners can then easily select their preferred solutions among these Pareto solutions. Experimental results confirm that our proposed algorithm outperforms some state-of-the-art MTL algorithms and can successfully find a set of well-representative solutions for different MTL applications.\n\n\\clearpage\n\n\\subsubsection*{Acknowledgments}\nThis work was supported by the Natural Science Foundation of China under Grant 61876163 and Grant 61672443, ANR/RGC Joint Research Scheme sponsored by the Research Grants Council of the Hong Kong Special Administrative Region, China and France National Research Agency (Project No. A-CityU101/16), and Hong Kong RGC General Research Funds under Grant 9042489 (CityU 11206317) and Grant 9042322 (CityU 11200116).\n\n\\medskip\n\n\\clearpage\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Multi-Objective Learning to Predict Pareto Fronts Using Hypervolume Maximization}\n\n\\begin{document}\n\n\\title{Multi-Objective Learning to Predict Pareto Fronts Using Hypervolume Maximization}\n\n\\author{\\name Timo M. Deist\\textsuperscript{1}\\thanks{equal contribution}\n        \\email timo.deist@cwi.nl \\\\\n       \\AND\n       \\name Monika Grewal\\textsuperscript{1}\\textsuperscript{*}\n       \\email monika.grewal@cwi.nl \\\\\n       \\AND\n       \\name Frank J.W.M. Dankers\\textsuperscript{2}\n       \\email f.j.w.m.dankers@lumc.nl \\\\\n       \\AND\n       \\name Tanja Alderliesten\\textsuperscript{2}\n       \\email t.alderliesten@lumc.nl \\\\\n       \\AND\n       \\name Peter A.N. Bosman\\textsuperscript{1, 3}\n       \\email peter.bosman@cwi.nl \\\\\n       \n       \\addr \\textsuperscript{1}Life Sciences \\& Health Research Group,\\\\\n       Centrum Wiskunde \\& Informatica, Amsterdam, The Netherlands\\\\\n       \n       \\addr \\textsuperscript{2}Department of Radiation Oncology,\\\\\n       Leiden University Medical Center, Leiden, The Netherlands\\\\\n       \n       \\addr \\textsuperscript{3}Faculty of Electrical Engineering, Mathematics and Computer Science,\\\\\n       Delft University of Technology, Delft, The Netherlands\n        }\n\n\\maketitle\n\n\\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file\nReal-world problems are often multi-objective with decision-makers unable to specify a priori which trade-off between the conflicting objectives is preferable. Intuitively, building machine learning solutions in such cases would entail providing multiple predictions that span and uniformly cover the Pareto front of all optimal trade-off solutions. We propose a novel approach for multi-objective training of neural networks to approximate the Pareto front during inference. In our approach, the neural networks are trained multi-objectively using a dynamic loss function, wherein each network's losses (corresponding to multiple objectives) are weighted by their hypervolume maximizing gradients. We discuss and illustrate why training processes to approximate Pareto fronts need to optimize on fronts of individual training samples instead of on only the front of average losses. Experiments on three multi-objective problems show that our approach returns outputs that are well-spread across different trade-offs on the approximated Pareto front without requiring the trade-off vectors to be specified a priori. Further, results of comparisons with the state-of-the-art approaches highlight the added value of our proposed approach, especially in asymmetric Pareto fronts.\n\\end{abstract}\n\n\\begin{keywords}\n  multi-objective optimization, neural networks, Pareto front, hypervolume, multi-objective learning\n\\end{keywords}\n\n\\section{Introduction}\n\\label{sec:introduction}\nMulti-objective (MO) optimization refers to finding Pareto optimal solutions according to multiple, often conflicting, objectives. In MO optimization, a solution is called Pareto optimal if none of the objectives can be improved without a simultaneous detriment in performance on at least one of the other objectives \\citep{van2000multiobjective}. MO optimization is used for MO decision-making in many real-world applications \\citep{stewart2008real} e.g., e-commerce recommendation \\citep{lin2019pareto}, treatment plan optimization \\citep{maree2019evaluation, Mller2017MulticriteriaPO}, and aerospace engineering \\citep{oyama2002multiobjective}. In this paper, we focus on learning-based MO decision-making i.e., MO training of machine learning (ML) models such that MO decision-making is possible during inference. Furthermore, we specifically focus on generating Pareto fronts\\footnote{Note that only \\emph{near} Pareto front solutions can be generated during inference due to the generalization gap between training and inference.} (the Pareto front is the set of losses corresponding to all Pareto optimal solutions) for each sample separately during inference because decisions are made on a per-sample basis.\n\nThe most straightforward approach for MO optimization is linear scalarization, i.e., employing single-objective formulations of the problem as linear combinations of different objectives according to scalarization weights. The scalarization weights are based on the desired trade-off between multiple objectives which is often referred to as `user-preference'. A major issue with linear scalarization is that the user-preferences cannot always be straightforwardly translated to linear scalarization weights. Recently proposed approaches have tackled this issue and find solutions on the Pareto front (of average losses) for conflicting objectives according to a pre-specified user-preference vector \\citep{lin2019paretomtl, mahapatra2020multi}. However, in many real world problems, the user-preference vector cannot be known a priori and decision-making is only possible \\emph{a posteriori}, i.e., after multiple solutions are generated that are (near) Pareto optimal for a specific sample. For example, in neural style transfer \\citep{gatys2016image} where photos are manipulated to imitate an art style from a selected painting, the user-preference between the amount of semantic information (the photo's content) and artistic style can only be decided by looking at multiple different resultant images on the Pareto front (Figure~\\ref{fig:style_transfer_2d_example_corrected}). Therefore, to enable a posteriori decision-making per sample, an approximation set consisting of multiple solutions on the Pareto front needs to be generated, each representing a different trade-off between multiple objectives. For more information on a posteriori decision-making, please refer to \\cite{hwang2012multiple}. \n\nMoreover, defining multiple trade-offs, typically by defining multiple scalarizations, to evenly cover the Pareto front is far from trivial, for example, if the Pareto front is asymmetric. Here, we define asymmetry in Pareto fronts as asymmetry in the distribution of Pareto optimal solutions in the objective space on either side of the 45$^{\\circ}$-line, the line which represents the trade-off of equal marginal benefit along all objectives (Figure~\\ref{fig:pareto_fronts}, notation will be explained in Section~\\ref{sec:approach}). We demonstrate and discuss this further in Section~\\ref{sec:experiments}. We will also demonstrate through experiments (in Section~\\ref{sec:batch_vs_persample}) that MO training of an ML model for a trade-off between losses, which are averaged over multiple training samples, may not yield the same trade-off for each sample's losses in certain scenarios. Therefore, existing methods that train ML models to generate solutions which approximate the Pareto front \\emph{for average losses of the training samples} are not sufficient when the goal is to generate models that approximate the Pareto front \\emph{for losses of individual samples}.\n\nTherefore, in learning-based a posteriori decision-making scenarios, it is crucial to have an MO approach for training ML models that can provide multiple diversely distributed outputs on the Pareto front per sample without requiring the user-preference vectors up front. Despite many developments in the direction of MO training of neural networks with pre-specified user-preferences, research in the direction of MO learning allowing for a posteriori decision-making is still scarce. In this paper, we present a novel method to multi-objectively train a set of neural networks to this end, leveraging the concept of hypervolume. Although we present our approach for training neural networks, the proposed formulation can be used for a wide range of ML models. \n\n\\begin{figure}[h!]\n    \\centering\n    \\includegraphics[width=0.45\\textwidth]{figures/approach/pareto_figure_v4.pdf}\n    \\caption{Pareto fronts in loss space for two samples and Pareto optimal predictions by three networks $\\theta_{1}$, $\\theta_{2}$, $\\theta_{3}$. Both fronts are asymmetric in $L_{1}=L_{2}$.}\n    \\label{fig:pareto_fronts}\n\\end{figure}\n\nThe hypervolume (HV) -- the objective space dominated by a given set of solutions \\citep{zitzler1999multiobjective} -- is a popular metric to compare the quality of different sets of solutions approximating the Pareto front. Theoretically, if the HV is maximal for a set of solutions, these solutions are on the Pareto front \\citep{fleischer2003measure}. Additionally, HV not only encodes the proximity of a set of solutions to the Pareto front but also their diversity, which means that HV maximization provides a straightforward way for finding diverse solutions on the Pareto front. Therefore, we use hypervolume maximization for MO training of neural networks. We train the set of neural networks with a dynamically weighted combination of loss functions corresponding to multiple objectives, wherein the weight of each loss is based on the HV-maximizing gradients. In summary, our paper has the following main contributions:\n\\begin{itemize}\n    \\item An MO approach for training neural networks \n    \\begin{itemize}\n        \\item using gradient-based HV maximization\n        \\item predicting Pareto optimal and diverse solutions on the Pareto front per sample without requiring specification of user-preferences\n        \\item enabling learning-based a posteriori decision-making.% instead of optimization.\n    \\end{itemize}\n    \\item An analysis highlighting the advantage of learning per sample over average-loss Pareto front approximations for differently shaped fronts.\n    \\item Experiments in real-world scenarios to demonstrate the added value of the proposed approach, specifically in asymmetric Pareto fronts.\n\\end{itemize}\n\n\\section{Related Work}\n\\textbf{MO optimization has been used in machine learning} for hyperparameter tuning of machine learning models \\citep{koch2015efficient, avent2020automatic}, multi-objective classification of imbalanced data \\citep{tari2020automatic}, and discovering the complete Pareto set starting from a single Pareto optimal solution \\citep{ma2020efficient}. \\cite{iqbal2020flexibo} used MO optimization for finding configurations of deep neural networks for conflicting objectives. \\cite{gong2015multiobjective} proposed optimizing the weights of an autoencoder multi-objectively for finding the Pareto front of sparsity and reconstruction error. \\cite{mao2020tchebycheff} used the Tchebycheff procedure for multi-objective optimization of a single neural network with multiple heads for multi-task text classification. Although we do not focus in these directions, our proposed approach can be used in similar applications.\n\n\\textbf{MO training of neural networks} has been researched widely, especially, for multi-task learning (MTL) \\citep{sener2018multi, lin2019paretomtl, mahapatra2020multi, ma2020efficient, lin2020controllable, navon2020learning}. MO training of a set of neural networks such that their predictions approximate the Pareto front of multiple objectives is closely related to the work presented in this paper. Similar to our work, \\cite{lin2019paretomtl,mahapatra2020multi} describe approaches with dynamic loss formulations to train multiple networks such that the predictions from these multiple networks together approximate the Pareto front. However, in these approaches, the trade-offs between conflicting objectives are required to be known in advance whereas our proposed approach does not require knowing the set of trade-offs beforehand. Other approaches \\citep{navon2020learning,lin2020controllable} for MO training of neural networks involve training a ``hypernetwork'' to predict weights of another neural network based on a user-specified trade-off. A recent work has proposed to condition a neural network for an input user-preference vector to allow for predicting multiple points at the Pareto front during inference \\citep{DBLP:journals/corr/abs-2103-13392}. While these approaches can approximate the Pareto front by iteratively predicting neural network weights or outputs based on multiple user-preference vectors, the process of sampling the user-preference vectors may still be intensive for an unknown Pareto front shape. Another key distinction of our approach from the abovementioned approaches is that these approaches learn to approximate the Pareto front of average losses for a set of training samples, while our approach learns to approximate the Pareto front for each individual sample in the training set. \n\n\\textbf{Gradient-based HV maximization} is a key component of our proposed approach. \\cite{miranda2016single} have described gradient-based HV maximization for single networks and formulated a dynamic loss function treating each sample's error as a separate loss. \\cite{albuquerque2019multi} applied this concept for training in generative adversarial networks. HV maximization is also applied in reinforcement learning \\citep{van2014multi,xu2020prediction}. While these approaches use HV maximizing gradients to optimize the weights of a single neural network, our proposed approach formulates a dynamic loss based on HV maximizing gradients for a \\emph{set} of neural networks. Different from our approach, other concurrent approaches for HV maximization are based on transformation to $(m-1)D$ (where m is the number of objectives) integral by use of polar coordinates \\cite{deng2019approximating}, random scalarization \\citep{golovin2020random}, and q-Expected hypervolume improvement function \\citep{daulton2020differentiable}.\\section{Approach}\n\\label{sec:approach}\nMO learning of a network parameterized by a vector $\\theta$ can be formulated as minimizing a vector of $n$ losses $\\mathcal{L}(\\theta,s_{k})=\\left[L_{1}(\\theta,s_{k}),\\dots,L_{n}(\\theta,s_{k})\\right]$ for a given set of samples $S = \\{s_{1},\\dots, s_{k},\\dots, s_{|S|}\\}$. These loss functions form the loss space in which the subspace attainable per sample $s_{k}$ is bounded by its Pareto front, i.e., the combination of loss values $L_{j}(\\theta,s_{k})$ of which none can be decreased without simultaneously increasing another loss (shown in Figure~\\ref{fig:pareto_fronts} for two losses and two samples).\nTo learn multiple networks with loss vectors on each sample's Pareto front, we replace $\\theta$ by a set of parameters $\\Theta = \\{\\theta_{1},\\dots,\\theta_{p} \\}$, where each parameter vector $\\theta_{i}$ represents a network. The corresponding set of loss vectors is $\\left\\{\\mathcal{L}(\\theta_{1},s_{k}),\\dots,\\mathcal{L}(\\theta_{p},s_{k}) \\right\\}$ and is represented by a stacked loss vector $\\mathfrak{L}(\\Theta,s_{k})=\\left[\\mathcal{L}(\\theta_{1},s_{k}),\\dots,\\mathcal{L}(\\theta_{p},s_{k}) \\right]$.\n\\textbf{Our goal is to learn a set of $p$ networks such that loss vectors in $\\mathfrak{L}(\\Theta,s_{k})$ corresponding to the networks' predictions for sample $s_{k}$ lie on and span the Pareto front of loss functions for sample $s_{k}$}. That is, each network's loss vector is Pareto optimal and lies in a distinct subsection of the Pareto front. To achieve this goal, we train networks so that the loss subspace Pareto dominated by the networks' predictions is maximal.\n\n\\begin{figure*}\n\\begin{subfigure}{0.32\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/approach/hv_fig_v8.pdf}\n    \\caption{Dominated subspaces}\n    \\label{fig:hv_intuition}\n\\end{subfigure}\n\\begin{subfigure}{0.32\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/approach/hv_grad_1front_v6.pdf}\n    \\caption{HV gradients}\n    \\label{fig:hv_grad_1front}\n\\end{subfigure}\n\\begin{subfigure}{0.32\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/approach/hv_multifront_v7.pdf}\n    \\caption{Domination-ranked fronts}\n    \\label{fig:hv_grad_intuition}\n\\end{subfigure}\n    \\centering\n    \\caption{\\textbf{(a)} Three Pareto optimal loss vectors $\\mathcal{L}(\\theta_{i},s)$ on the Pareto front (green) with dominated subspaces $D_{r}(\\mathcal{L}(\\theta_{i},s_{k}))$ with respect to reference point $r$. The union of dominated subspaces is the dominated hypervolume (HV) of $\\mathfrak{L}(\\Theta,s_{k})$.\n    \\textbf{(b)} Gray markings illustrate the computation of the HV gradients $\\dd{\\mathrm{HV(\\mathfrak{L}(\\Theta,s))}}{\\mathcal{L}(\\theta_{i},s)}$ (gray arrows) in the three non-dominated solutions.\n    \\textbf{(c)} The same five solutions grouped into two domination-ranked fronts $\\Theta_{0}$ and $\\Theta_{1}$ with corresponding HV, equal to their dominated subspaces $D_{r}(\\mathcal{L}(\\theta_{i},s_{k}))$, and HV gradients.}\n    \\label{fig:hv}\n\\end{figure*}\n\n\\subsection{MO Learning by HV Maximization}\nThe HV of a loss vector  $\\mathcal{L}(\\theta_{i},s_{k})$ for a sample $s_{k}$ is the volume of the subspace $D_{r}(\\mathcal{L}(\\theta_{i},s_{k}))$ in loss space dominated by $\\mathcal{L}(\\theta_{i},s_{k})$. This is illustrated in Figure \\ref{fig:hv_intuition}. To keep this volume finite, the HV is computed with respect to a reference point $r$ which bounds the space to the region of interest\\footnote{The reference point is generally set to large coordinates in loss space to ensure that it is always dominated by all loss vectors.}. Subsequently, the HV of multiple loss vectors $\\mathfrak{L}(\\Theta,s_{k})$ is the HV of the union of dominated subspaces $D_{r}(\\mathcal{L}(\\theta_{i},s_{k})), \\forall i \\in \\{1, 2, ..., p\\}$. The MO learning problem to maximize the mean HV over all $|S|$ samples is as follows:\n\\begin{align}\n    &\\text{maximize} \\frac{1}{|S|}\\sum_{k=1}^{|S|}\\mathrm{HV}\\left(\\mathfrak{L}(\\Theta,s_{k})\\right)\\label{eq:max_mean_hv}\n\\end{align}\nConcordantly, the update direction of gradient ascent for parameter vector $\\theta_{i}$ of network $i$ is:\n\\begin{equation}\n\\dd{\\frac{1}{|S|}\\sum_{k=1}^{|S|}\\mathrm{HV}(\\mathfrak{L}(\\Theta,s_{k}))}{\\theta_{i}} \\label{eq:training_set_gradients1}\n\\end{equation}\nBy exploiting the chain rule decomposition of HV gradients as described in \\cite{emmerich2014time}, the update direction in Equation~\\eqref{eq:training_set_gradients1} for parameter vector $\\theta_{i}$ of network $i$ can be written as follows:\n\\begin{equation}\n\\frac{1}{|S|} \\sum_{k=1}^{|S|}\\dd{\\mathrm{HV}\\left( \\mathfrak{L}(\\Theta,s_{k}) \\right)}{\\mathcal{L}(\\theta_{i},s_{k})}\\cdot\\dd{\\mathcal{L}(\\theta_{i},s_{k})}{\\theta_{i}} \\quad\\forall i\\in\\{1,\\dots,p\\} \\label{eq:training_set_gradients_dot_product}\n\\end{equation}\n\nThe dot product of $\\dd{\\mathrm{HV}\\left( \\mathfrak{L}(\\Theta,s_{k}) \\right)}{\\mathcal{L}(\\theta_{i},s_{k})}$ (the HV gradients with respect to loss vector $\\mathcal{L}(\\theta_{i},s_{k})$) in loss space,  \nand $\\dd{\\mathcal{L}(\\theta_{i},s_{k})}{\\theta_{i}}$ (the matrix of loss vector gradients in the network $i$'s parameters $\\theta_{i}$) in parameter space, can be decomposed to\n\\begin{equation}\n \\frac{1}{|S|} \\sum_{k=1}^{|S|}\\sum_{j=1}^{n}\\dd{\\mathrm{HV}\\left( \\mathfrak{L}(\\Theta,s_{k}) \\right)}{L_{j}(\\theta_{i},s_{k})}\\dd{L_{j}(\\theta_{i},s_{k})}{\\theta_{i}} \\quad\\forall i\\in\\{1,\\dots,p\\} \\label{eq:training_set_gradients}\n\\end{equation}\nwhere $\\dd{\\mathrm{HV}(\\mathfrak{L}(\\Theta,s_{k}))}{L_{j}(\\theta_{i},s_{k})}$ is the scalar HV gradient in the single loss function $L_{j}(\\theta_{i},s_{k})$, and $\\dd{L_{j}(\\theta_{i},s_{k})}{\\theta_{i}}$ are the gradients used in gradient descent for single-objective training of network $i$ for loss $L_{j}(\\theta_{i}, s_{k})$. Based on Equation~\\eqref{eq:training_set_gradients}, one can observe that mean HV maximization of loss vectors from a set of $p$ networks for $|S|$ samples can be achieved by weighting their gradient descent directions for loss functions $L_{j}(\\theta_{i}, s_{k})$ with their corresponding HV gradients $\\dd{\\mathrm{HV}(\\mathfrak{L}(\\Theta,s_{k}))}{L_{j}(\\theta_{i},s_{k})}$ for all $i$, $j$. In other terms, the MO learning of a set of $p$ networks can be achieved by minimizing\\footnote{Minimizing (instead of maximizing) the dynamic loss function maximizes the HV because the reference point $r$ is in the positive quadrant (``to the right and above 0'').} the following dynamic loss function for each network $i$:\n\\begin{equation}\n    \\frac{1}{|S|}\n    \\sum_{k=1}^{|S|}\\sum_{j=1}^{n}\\dd{\\mathrm{HV}\\left( \\mathfrak{L}(\\Theta,s_{k}) \\right)}{L_{j}(\\theta_{i},s_{k})}L_{j}(\\theta_{i},s_{k}) \\quad\\forall i\\in\\{1,\\dots,p\\} \\label{eq:joint_loss_max_mean_hv_multi_sample}\n\\end{equation}\nThe computation of the HV gradients $\\dd{\\mathrm{HV}(\\mathfrak{L}(\\Theta,s_{k}))}{L_{j}(\\theta_{i},s_{k})}$ is illustrated in Figure~\\ref{fig:hv_grad_1front}. These HV gradients are equal to the marginal decrease in the subspace dominated only by $\\mathcal{L}(\\theta_{i},s_{k})$ when increasing $L_{j}(\\theta_{i},s_{k})$.\n\n\\subsection{HV Maximization of Domination-Ranked Fronts}\n\\label{sec:domination_ranking}\nA relevant caveat of gradient-based HV maximization is that HV gradients $\\dd{\\mathrm{HV}\\left( \\mathfrak{L}(\\Theta,s_{k}) \\right)}{L_{j}(\\theta_{i},s_{k})}$ in strongly dominated solutions, i.e., solutions in the interior of the dominated HV, are zero \\citep{emmerich2014time} because no movement in any direction will affect the HV (Figure~\\ref{fig:hv_grad_1front}). Further, gradients in weakly dominated solutions are undefined \\citep{emmerich2014time}. As a consequence, HV gradients cannot be used for optimizing (weakly or strongly) dominated solutions. To resolve this issue, we follow \\cite{wang2017hypervolume}'s approach to gradient-based HV optimization. Other strategies to handle dominated solutions exist \\citep{wang2017steering,deist2020multi}, but \\cite{wang2017hypervolume} was selected as it only requires HV computation and non-dominated sorting and a comparison had shown that it performs similar to a competing approach \\citep{deist2020multi}. The selected approach avoids the problem of dominated solutions by sorting all loss vectors into separate fronts $\\Theta_{l}$ of mutually non-dominated loss vectors and optimizing each front separately (Figure \\ref{fig:hv_grad_intuition}).\n$l$ is the domination rank and $q(i)$ is the mapping of network $i$ to domination rank $l$. By maximizing the HV of each front, trailing fronts with domination rank $>0$ eventually merge with the non-dominated front $\\Theta_{0}$ and a single front is maximized by determining optimal locations for each loss vector on the Pareto front.\n\nFurthermore, we normalize the HV gradients $\\ddtfrac{\\mathrm{HV}\\left(\\mathfrak{L}(\\Theta_{q(i)},s_{k}) \\right)}{\\mathcal{L}(\\theta_{i},s_{k})}$ as in \\cite{deist2020multi} such that their length in loss space is 1. The dynamic loss function including domination-ranking of fronts and HV gradient normalization is:\n\\begin{equation}\n\\frac{1}{|S|}\n \\sum_{k=1}^{|S|}\n \\sum_{j=1}^{n}\n \\frac{1}{w_{i}}\\dd{\\mathrm{HV}\\left(\\mathfrak{L}(\\Theta_{q(i)},s_{k}) \\right)}{L_{j}(\\theta_{i},s_{k})}\n L_{j}(\\theta_{i},s_{k})\n \\label{eq:final_joint_loss}\n \\quad \\forall i\\in\\{1,\\dots,p\\}\n\\end{equation}\nwhere $w_{i} = \\left\\Vert \\ddtfrac{\\mathrm{HV}\\left(\\mathfrak{L}(\\Theta_{q(i)},s_{k}) \\right)}{\\mathcal{L}(\\theta_{i},s_{k})} \\right\\Vert$. The algorithm is summarized in Algorithm~\\ref{alg:pseudocode}.\n\n\\begin{algorithm}[htbp]\n\\caption{Training networks $\\Theta$ for Pareto front prediction by HV maximization of domination-ranked fronts}\n\\begin{algorithmic}\n\\State Initialize $p$ networks $\\Theta=\\{\\theta_{1},\\dots,\\theta_{p}\\}$\n\\For{each batch $\\tilde{S}$}\n    \\For{each network $\\theta_{i}$}\n        \\For{each sample $s_{k}\\in \\tilde{S}$}\n            \\State Compute loss vector $\\mathcal{L}(\\theta_{i},s_{k})$% per sample $s_{k}$\n        \\EndFor\n    \\EndFor\n    \\For{each sample $s_{k}\\in \\tilde{S}$}\n        \\State Stack loss vectors $\\mathcal{L}(\\theta_{i},s_{k})$ into $\\mathfrak{L}(\\Theta,s_{k})$\n        \\State Sort $\\mathfrak{L}(\\Theta,s_{k})$ into multiple fronts $\\mathfrak{L}(\\Theta_{l},s_{k})$ by domination ranking (Section~\\ref{sec:domination_ranking})\n        \\For{each front $l$}\n            \\State Compute loss weights $\\dd{\\mathrm{HV}\\left(\\mathfrak{L}(\\Theta_{q(i)},s_{k}) \\right)}{L_{j}(\\theta_{i},s_{k})}\\forall i,j$ using algorithm by\n            \\State \\cite{emmerich2014time} % new line for better alignment\n        \\EndFor\n    \\EndFor\n    \\For{each network $\\theta_{i}$}\n        \\State Backpropagate on joint loss from Equation~\\eqref{eq:final_joint_loss}\n    \\EndFor\n    \\State Update $\\Theta$ by stepping into gradient direction\n\\EndFor\n\\end{algorithmic}\n\\label{alg:pseudocode}\n\\end{algorithm}\n\n\\subsection{Average vs per-sample dynamic loss formulations}\n\\label{sec:batch_vs_persample}\nThe dynamic loss in \\eqref{eq:final_joint_loss} maximizes the HV for each sample's loss vectors by weighting the loss vectors with the corresponding HV maximizing gradients -- which is the ideal objective for MO training of the set of neural networks. However, this means that one network $\\theta_{i}$ is not necessarily trained exclusively for a specific trade-off. Instead, across different samples $s_{k}$, one network could generate outputs corresponding to different trade-offs. This may have three practical implications: 1) HV maximizing gradients need to be calculated for each sample, which could be expensive, 2) the predictions on the different unseen samples may not follow a similar ordering along the Pareto front and a surrogate method might be required to ascertain the trade-offs of the different outputs comprising the approximated Pareto front during inference, 3) since the networks may learn a different trade-off corresponding to different samples, the optimization problem underlying the joint training of the $p$ learners becomes more complex.\n\nA simple workaround to avoid the abovementioned implications is to reformulate the dynamic loss in \\eqref{eq:final_joint_loss} such that the HV maximizing gradients are calculated for average losses of multiple samples. Existing approaches \\citep{sener2018multi, lin2019paretomtl, mahapatra2020multi} for MO training of neural networks also optimize for average losses (but not their HV). With the use of average losses, the dynamic~loss~\\eqref{eq:final_joint_loss} would be simplified to:\n\\begin{equation}\n \\sum_{j=1}^{n}\\frac{1}{w_{i}}\\dd{\\mathrm{HV}\\left(\\overline{\\mathfrak{L}(\\Theta_{q(i)},S)} \\right)}{\\overline{L_{j}(\\theta_{i},S)}}\\overline{L_{j}(\\theta_{i},S)} \\label{eq:joint_loss_max_hv_of_mean_losses}\\quad\\forall i\\in\\{1,\\dots,p\\}\n\\end{equation}\nwhere the different loss vectors are replaced by their corresponding averages over the training samples (or a batch of samples when training batchwise). Changing the formulation from \\eqref{eq:final_joint_loss} to \\eqref{eq:joint_loss_max_hv_of_mean_losses} has two practical benefits: 1) the number of HV gradient computations $\\ddtfrac{\\mathrm{HV}\\left(\\cdot \\right)}{{L_{j}(\\cdot)}}$ reduces from $n|S|$ to $n$, which gives a considerable speed-up especially in the cases of large batchsize and shallow neural network training, 2) since the weights of a neural network are updated according to one trade-off (dynamically estimated as HV gradients) for all samples, the prediction ordering remains same across samples during inference making the presentation of approximated Pareto fronts to the decision-maker easier. However, this simplification in \\eqref{eq:final_joint_loss} may lead to imperfect Pareto front approximations on individual samples in certain cases, examples of which are outlined below.\n\\begin{figure}\n    \\centering\n    \\begin{tabular}{ccc}\n        & \\textbf{Training per sample} & \\textbf{Training on average losses} \\\\\n    & $\\max \\tfrac{1}{|S|}\\sum_{k=1}^{|S|} \\mathrm{HV}\\left(\\mathfrak{L}(\\Theta,s_{k})\\right)$ & $\\max \\mathrm{HV}\\left(\\overline{\\mathfrak{L}(\\Theta,S)}\\right)$  \\\\\n    & (Dynamic~loss~\\eqref{eq:final_joint_loss}) & (Dynamic~loss~\\eqref{eq:joint_loss_max_hv_of_mean_losses})  \\\\\n    \\multirow[t]{1}{*}{\n    \\rotatebox[origin=c]{90}{\\textbf{Strictly convex}}}\n    &\n\\begin{subfigure}{0.30\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/optimization_example/convex_per_sample_higamo.png}\n    \\caption{}\n    \\label{fig:opt_ex_loss_per_sample_convex_higamo}\n\\end{subfigure}\n    &\n\\begin{subfigure}{0.30\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/optimization_example/convex_mean_loss_higamo.png}\n    \\caption{}\n    \\label{fig:opt_ex_mean_losses_convex_higamo}\n\\end{subfigure}\n    \\\\    \n    \\multirow[t]{1}{*}{\n    \\rotatebox[origin=c]{90}{\\textbf{Linear}}}\n    &\n\\begin{subfigure}{0.30\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/optimization_example/linear_per_sample_higamo.png}\n    \\caption{}\n    \\label{fig:opt_ex_loss_per_sample_line_higamo}\n\\end{subfigure}\n    &\n\\begin{subfigure}{0.30\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/optimization_example/linear_mean_loss_higamo.png}\n    \\caption{}\n    \\label{fig:opt_ex_mean_losses_line_higamo}\n\\end{subfigure}\n\\\\\n    \\multirow[t]{1}{*}{\n    \\rotatebox[origin=c]{90}{\\textbf{Non-convex}}}\n    &\n\\begin{subfigure}{0.30\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/optimization_example/nonconvex_per_sample_higamo.png}\n    \\caption{}\n    \\label{fig:opt_ex_loss_per_sample_concave_higamo}\n\\end{subfigure}\n&\n\\begin{subfigure}{0.30\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/optimization_example/nonconvex_mean_loss_higamo.png}\n    \\caption{}\n    \\label{fig:opt_ex_mean_losses_concave_higamo}\n\\end{subfigure}\n\n\\\\\n    \\end{tabular}    \n    \\caption{HV maximization of each sample's losses (left) and of average losses (right) for a two sample problem with different Pareto front shapes (rows).\n    }\n    \\label{fig:optimization_example_higamo_hv}\n\\end{figure}\n\nConsider the following three cases of MO training of five neural networks to predict strictly convex, linear, or non-convex fronts for two training samples ($k \\in \\{1, 2\\} $) each.\n\\begin{enumerate}\n\n\\item \\textbf{Strictly convex:} given the centres $x_{k}^{1}$ and $x_{k}^{2}$ of two circles as inputs, each neural network $\\theta_{i}$ in the set outputs the coordinates $z_{k}$ % = f(x_{k}^{1}, x_{k}^{2} | \\theta_{i})$\nwith minimal squared Euclidean distance from both the circle centres such that the set of the outputs approximate the Pareto front. This results in a strictly convex Pareto front. The corresponding losses to minimize are:\n\\begin{equation*}\n    L_{1} = \\left\\lVert z_{k} - x_{k}^{1} \\right\\rVert_{2}^{2}  \\quad L_{2} = \\left\\lVert z_{k} - x_{k}^{2}\\right\\rVert_{2}^{2}\n\\end{equation*}\n\\item \\textbf{Linear (simultaneously convex \\& concave):} For the problem specified above, if the Euclidean distance from the two centres is minimized, the shape of the corresponding Pareto front will become linear. The losses to minimize are:\n\\begin{equation*}\n    L_{1} = \\left\\lVert z_{k} - x_{k}^{1} \\right\\rVert_{2}  \\quad L_{2} = \\left\\lVert z_{k} - x_{k}^{2}\\right\\rVert_{2}\n\\end{equation*}\n\\item \\textbf{Non-convex:} To generate a non-convex front\\footnote{It is named \\emph{non-convex} instead of \\emph{concave} or \\emph{strictly concave} because it is concave for the most part but, when $L_{1}$ or $L_{2}$ are close to 0, the front is locally convex.}, we adapted the optimization example used in  \\cite{lin2019paretomtl} and \\cite{mahapatra2020multi} to a learning problem with two samples. The problem is formulated as: given a scalar input $x_{k}$, each network outputs a scalar $z_{k}$ % = f(x_{k} | \\theta_{i})$) \nsuch that they minimize the following losses simultaneously. \n\\begin{equation*}\n    L_{1} = x_{k} \\left(1- \\exp{\\left(-\\left\\lVert z_{k}-1\\right\\rVert_{2}^{2}\\right)} \\right)  \\, L_{2} = x_{k} \\left(1- \\exp{\\left(-\\left\\lVert z_{k}+1\\right\\rVert_{2}^{2}\\right)} \\right)\n\\end{equation*}\n\n\\end{enumerate}\n\nFigure~\\ref{fig:optimization_example_higamo_hv} (left column) shows results for maximizing the average of each sample's HV (Dynamic~loss~\\eqref{eq:final_joint_loss}) and Figure~\\ref{fig:optimization_example_higamo_hv} (right column) shows results for maximizing the HV of average losses (Dynamic~loss~\\eqref{eq:joint_loss_max_hv_of_mean_losses}). The approximated Pareto fronts per sample (small rectangles with different colors per network $\\theta_{i}$) and each sample's Pareto fronts (dashed/dotted green lines) are displayed. Additionally on the right, because the networks are trained on the average losses, the averaged predictions (single large rectangles with black borders) and the corresponding Pareto front (solid green line) are shown. \nFigures~\\ref{fig:optimization_example_higamo_hv} and \\ref{fig:optimization_example_existing_methods} show each method's best observed performance, either maximal average HV across samples or maximal HV on average losses, over a tuning grid (Table~\\ref{tab:tuning_info} in the appendix).\n  \nFor the strictly convex and linear cases (Figure~\\ref{fig:optimization_example_higamo_hv}, top and middle rows), minimizing either of the Dynamic losses~\\eqref{eq:final_joint_loss} or \\eqref{eq:joint_loss_max_hv_of_mean_losses} yields well-spread outputs across each sample's Pareto front. In case of a linear front (Figure~\\ref{fig:opt_ex_mean_losses_line_higamo}), however, a decent spread cannot always be guaranteed because the same HV of average losses with irregular spread can be achieved, for example, by shifting the orange outputs to the left along Sample~1's front and to the right along Sample~2's front.\nFor the non-convex Pareto fronts, minimizing Dynamic loss~\\eqref{eq:joint_loss_max_hv_of_mean_losses} does not yield maximal HV per sample (small rectangles) as is clear by the uneven spread of the predictions across each sample's Pareto front (Figure~\\ref{fig:opt_ex_mean_losses_concave_higamo}). Minimizing Dynamic loss~\\eqref{eq:final_joint_loss} does lead to well-distributed predictions per sample across the Pareto front irrespective whether the front is strictly convex, linear or non-convex (Figure~\\ref{fig:optimization_example_higamo_hv}, left column).\n\n\\subsubsection{The differences between average and per-sample formulations are not specific to HV maximization}\n\\label{sec:differences not specific to HV}\nWe investigated the abovementioned cases with three other existing approaches: a baseline linear scalarization approach, i.e., linear combination of losses with fixed weights, and two state-of-the-art approaches PMTL and EPO using dynamic loss formulations to achieve a priori specified user-preferences on the Pareto front. All three methods attempt to learn an approximation of the Pareto front of average losses. Corresponding results are shown in Figure~\\ref{fig:optimization_example_existing_methods}. It can be seen that, for linear and non-convex Pareto fronts, Pareto MTL and Linear scalarization are not guaranteed to achieve the desired user-preferences on each sample's front when training models for the chosen user-preferences on the Pareto front of average losses. EPO returns outputs per sample following the chosen user-preferences in the strictly convex and linear cases, but also fails in the non-convex setting.\nThese findings thus corroborate that training for predictions with specific trade-offs/user-preferences on average losses does not translate to predictions for individual samples for all Pareto front curvatures.\n\n\\subsubsection{Training for trade-offs on the average loss Pareto front need not translate to predictions on each sample's front with the same trade-off}\nThe above results show that, in the strictly convex case, trade-offs on the average Pareto front either given by HV gradients in our approach, weights from dynamic loss formulations (EPO, Pareto MTL), or fixed weights (Linear scalarization) correspond to predictions with the same trade-offs on the individual samples' Pareto fronts. To understand why this happens for the chosen strictly convex case but not for the linear and non-convex case, one should consider the relationship between trade-offs on average and the individual samples' fronts.\nIn this strictly convex case, almost all given trade-offs on the strictly convex average loss front can be realized by averaging the \\emph{same} trade-offs on each sample's strictly convex front. The blue solution is an exception as it is slightly above the average loss front. \nFor the linear case (Figure~\\ref{fig:opt_ex_mean_losses_line_higamo}) however, \\emph{many} pairs of trade-offs on the two samples' fronts can be averaged to yield the given trade-off on the average loss front. \nSimilarly, in the non-convex case, predictions with even \\emph{opposing} trade-offs on concave sections of the two samples' Pareto fronts can be averaged to yield the desired trade-off on the Pareto front of average losses. Consider, for example, the two small light blue markers at the extremes of each sample's front averaging to a almost 1:1 trade-off on the average loss front in Figure~\\ref{fig:opt_ex_mean_losses_concave_higamo}. At this example it can be observed that optimizing for a specific trade-off on the \\emph{average loss} front need not result in that specific trade-off on \\emph{each sample's} front. \n\n\\subsubsection{Strict convexity of individual samples' Pareto fronts does not guarantee the average loss formulation to yield optimal per-sample Pareto fronts}\n\\label{sec:counter example strict convexity}\nFor HV maximization in the above strictly convex case (Figure~\\ref{fig:opt_ex_mean_losses_convex_higamo}), the trade-offs chosen by HV maximization on the average front also appear optimal for HV maximization per sample. This observation is not true in general as illustrated in the following example. Figure~\\ref{fig:optimization_example_strictly_convex_counter_example_higamo_hv} shows the result of training five neural networks for the strictly convex case described above but using the following loss functions:\n\\begin{equation*}\n    L_{1} = \\left\\lVert z_{k} - x_{k}^{1} \\right\\rVert_{2}^{2}  \\quad L_{2} = \\left\\lVert z_{k} - x_{k}^{2}\\right\\rVert_{2}^{1.01}\n\\end{equation*}\nwith $x_{1}^{1} = [0,0]$, $x_{1}^{2} = [1,1]$, $x_{2}^{1} = [0.05,0.4]$, $x_{2}^{2} = [0.5,0.5]$. Both samples' fronts are strictly convex but asymmetric in the 45$^{\\circ}$-line $L_{1}=L_{2}$ and with differing curvature.\nIt becomes apparent that the Pareto front predictions of one sample are not well-distributed when using Dynamic loss \\eqref{eq:joint_loss_max_hv_of_mean_losses} (Figure~\\ref{fig:opt_ex_mean_losses_convex_counter_example_higamo_hv}). Dynamic loss \\eqref{eq:final_joint_loss}, however, still generates well-distributed predictions per front (Figure~\\ref{fig:opt_ex_loss_per_sample_convex_counter_example_higamo_hv}). This observation indicates that strict convexity of the samples' Pareto fronts alone does not guarantee that HV maximizing trade-offs on the front of average losses are also optimal for each sample. %This also fits the findings by \\cite{auger2009theory}.\n\\begin{figure}\n    \\centering\n    \\begin{tabular}{ccc}\n        & \\textbf{Training per sample} & \\textbf{Training on average losses} \\\\\n    & $\\max \\tfrac{1}{|S|}\\sum_{k=1}^{|S|} \\mathrm{HV}\\left(\\mathfrak{L}(\\Theta,s_{k})\\right)$ & $\\max \\mathrm{HV}\\left(\\overline{\\mathfrak{L}(\\Theta,S)}\\right)$  \\\\\n    & (Dynamic~loss~\\eqref{eq:final_joint_loss}) & (Dynamic~loss~\\eqref{eq:joint_loss_max_hv_of_mean_losses})  \\\\\n    \\multirow[t]{1}{*}{\n    \\rotatebox[origin=c]{90}{\\textbf{Strictly convex}}}\n    &\n\\begin{subfigure}{0.30\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/optimization_example/convexcounterexample_per_sample_higamo.png}\n    \\caption{}\n    \\label{fig:opt_ex_loss_per_sample_convex_counter_example_higamo_hv}\n\\end{subfigure}\n    &\n\\begin{subfigure}{0.30\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/optimization_example/convexcounterexample_mean_loss_higamo.png}\n    \\caption{}\n    \\label{fig:opt_ex_mean_losses_convex_counter_example_higamo_hv}\n\\end{subfigure}\n    \\end{tabular}\n    \\caption{An example of a learning problem with strictly convex Pareto fronts in which HV maximization of average losses does not result in well-distributed outputs on both samples' fronts. HV maximization of each sample's losses (left) and of average losses (right).}\n    \\label{fig:optimization_example_strictly_convex_counter_example_higamo_hv}\n\\end{figure}\n\n\\subsubsection{Per-sample HV maximization does not guarantee the same output ordering across samples}\nApart from the increased computational burden, another potential disadvantage of using Dynamic loss~\\eqref{eq:final_joint_loss} is that a given network does not guarantee the same ordering of trade-offs for different samples. For example, in the strictly convex case of Figure~\\ref{fig:opt_ex_loss_per_sample_convex_higamo}, the colored squares on Sample 1's front are beige, light blue, green, blue, and orange from left to right, while Sample 2's front reads beige, orange, blue, green, and light blue. The ordering can only be known during inference if losses can be computed (so not in, e.g., regressions (Section~\\ref{sec:sin_cos}), segmentations (Section~\\ref{sec:mo_segmentation}), or classification tasks where labels are not known during inference). This condition would pose a limitation in scenarios where a decision-maker requires the ordering to make a decision. A prediction's position on the approximated Pareto front would remain unknown and a choice between predictions would have to be made without knowing what trade-off each prediction represents. Using Dynamic loss~\\eqref{eq:joint_loss_max_hv_of_mean_losses} for training would permit to estimate the ordering and trade-offs from data where ground truth labels are available, e.g., training data. In some real-life scenarios, however, losses can be computed without labels (e.g., style transfer, Section~\\ref{sec:style_transfer}) or decision-makers rely more on the predictions themselves, e.g., organ segmentations (Section~\\ref{sec:mo_segmentation}) or generated images (Section~\\ref{sec:style_transfer}), rather than their estimated ordering or trade-offs. Therefore, the practical disadvantage of unordered outputs for per-sample HV maximization is minor.\n\nIn conclusion, the above illustrative examples and arguments highlight why optimizing the trade-offs for each sample is preferable to average loss formulations and crucial to ensure approximation of a well-distributed Pareto front for each sample.\n\n\\begin{figure}\n    \\centering\n    \\begin{tabular}{cccc}\n    & \\multicolumn{3}{c}{\\textbf{Training on average losses}}\\\\\n    \\cmidrule{2-4}\n    & \\textbf{Linear scalarization} & \\textbf{Pareto MTL}  & \\textbf{EPO}  \\\\\n    \\multirow[t]{1}{*}{\n    \\rotatebox[origin=c]{90}{\\textbf{Strictly convex}}\n    }\n    &\n\\begin{subfigure}{0.30\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/optimization_example/convex_mean_loss_linear_scalarization.png}\n    \\caption{}\n    \\label{fig:opt_ex_mean_losses_convex_linscal}\n\\end{subfigure}  \n    &\n\\begin{subfigure}{0.30\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/optimization_example/convex_mean_loss_pareto_mtl.png}\n    \\caption{}\n    \\label{fig:opt_ex_mean_losses_convex_paretomtl}\n\\end{subfigure}\n    &\n\\begin{subfigure}{0.30\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/optimization_example/convex_mean_loss_epo.png}\n    \\caption{}\n    \\label{fig:opt_ex_mean_losses_convex_epo}\n\\end{subfigure}\n    \\\\   \n    \\multirow[t]{1}{*}{\n    \\rotatebox[origin=c]{90}{\\textbf{Linear} }\n    }\n    &\n\\begin{subfigure}{0.30\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/optimization_example/linear_mean_loss_linear_scalarization.png}\n    \\caption{}\n    \\label{fig:opt_ex_mean_losses_line_linscal}\n\\end{subfigure}\n    &\n\\begin{subfigure}{0.30\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/optimization_example/linear_mean_loss_pareto_mtl.png}\n    \\caption{}\n    \\label{fig:opt_ex_mean_losses_line_paretomtl}\n\\end{subfigure}\n    &\n\\begin{subfigure}{0.30\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/optimization_example/linear_mean_loss_epo.png}\n    \\caption{}\n    \\label{fig:opt_ex_mean_losses_line_epo}\n\\end{subfigure}\n\\\\\n    \\multirow[t]{1}{*}{\n    \\rotatebox[origin=c]{90}{\\textbf{Non-convex}} \n    }\n    &\n\\begin{subfigure}{0.30\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/optimization_example/nonconvex_mean_loss_linear_scalarization.png}\n    \\caption{}\n    \\label{fig:opt_ex_mean_losses_concave_linscal}\n\\end{subfigure}  \n    &\n\\begin{subfigure}{0.30\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/optimization_example/nonconvex_mean_loss_pareto_mtl.png}\n    \\caption{}\n    \\label{fig:opt_ex_mean_losses_concave_paretomtl}\n\\end{subfigure}\n    &\n\\begin{subfigure}{0.30\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/optimization_example/nonconvex_mean_loss_epo.png}\n    \\caption{}\n    \\label{fig:opt_ex_mean_losses_concave_epo}\n\\end{subfigure}\n \\\\\n    \\end{tabular}    \n    \\centering\n    \\caption{Optimization in average loss space for a two sample problem with\n    Pareto front shapes (rows) using Linear scalarization (left), Pareto MTL (center), and EPO (right).\n    }\n    \\label{fig:optimization_example_existing_methods}\n\\end{figure}\n\n\\subsection{Implementation}\nWe implemented the HV maximization of losses from multiple networks, as defined in Equation~\\eqref{eq:final_joint_loss}, in Python\\footnote{Code is available at \\url{https://github.com/timodeist/multi_objective_learning}}. The neural networks were implemented using the PyTorch framework \\citep{pytorch}. We use \\cite{fonseca2006improved}'s HV computation reimplemented by Simon Wessing, available from \\cite{higamo_code}. The HV gradients\n$\\ddtfrac{\\mathrm{HV}\\left(\\mathfrak{L}(\\Theta_{q(i)},s_{k}) \\right)}{L_{j}(\\theta_{i},s_{k})}$\n are computed following the algorithm by \\cite{emmerich2014time}. Networks with identical losses are assigned the same HV gradients. For non-dominated networks with one or more identical losses (which can occur in training with three or more losses), the left- and right-sided limits of the HV function derivatives are not the same \\citep{emmerich2014time} and they are set to zero. Non-dominated sorting is implemented based on \\cite{deb2002fast}.\nWe experimentally tested our approach for two and three objectives, but the algorithms for HV and HV gradient computations also extend to more objectives. \n\n\\subsection{Time Complexity}\nThe published time complexities of different steps in calculating HV maximizing gradients for $n$ losses and $p$ solutions are as follows: $\\mathcal{O}(np^{2})$ for non-dominated sorting \\citep{deb2002fast}, $\\mathcal{O}(p^{(n-2)}\\log{p}))$ for HV computation of $p$ non-dominated solutions if $n>2$, $\\mathcal{O}(p)$ for HV calculation for $n=2$ after sorting in one loss \\citep{fonseca2006improved}, $\\mathcal{O}(p\\log(p))$ for calculating HV gradients \n$\\dd{\\mathrm{HV}\\left(\\mathfrak{L}(\\Theta_{q(i)},s_{k}) \\right)}{L_{j}(\\theta_{i},s_{k})}$\n for two and three losses, and $\\mathcal{O}(p^{2})$ for HV gradient calculation of four losses \\citep{emmerich2014time}. Note that the latter two complexities assume specialized non-dominated sorting and HV computation subroutines that we did not implement. Overall, for moderate $p$ values and $n\\leq4$, this means only little additional computational load compared to computing loss gradients for neural network training, which gives an HV maximization-based approach an edge over other competitive approaches in this direction.\\section{Experiments}\n\\label{sec:experiments}\nWe now present the application of our method (Dynamic~loss~\\eqref{eq:final_joint_loss}) in experiments with different MO problems: a simple MO regression example, a multi-observer medical image segmentation, and a neural style transfer optimization problem.  \n\nWe compared the performance of our approach with \\textbf{linear scalarization} of average losses and two state-of-the-art approaches called \\textbf{Pareto MTL} \\citep{lin2019paretomtl}, and \\textbf{EPO} \\citep{mahapatra2020multi}. Both Pareto MTL and EPO try to find Pareto optimal solutions on the Pareto front of average losses for a given trade-off vector using dynamic loss functions. For a consistent comparison, we used the trade-offs used in the original experiments of EPO for Pareto MTL, EPO, and as fixed weights in linear scalarization.\n\nExperiments were run on systems using Intel(R) Xeon(R) Silver 4110 CPU @ 2.10GHz with NVIDIA GeForce RTX 2080Ti, or Intel(R) Core(R) i5-3570K @ 3.40Ghz with NVIDIA GeForce GTX 1060 6GB. The training was done using the Adam optimizer \\citep{kingma2014adam}. The learning rate and $\\beta_{1}$ of Adam were tuned for each approach based on the maximal HV of validation loss vectors in the last iteration. Details of the hyperparameter tuning experiments are provided in Appendix~\\ref{suppl:tuning}.\n\n\\subsection{MO Regression}\n\\label{sec:sin_cos}\nTo illustrate our proposed approach, we begin with an artificial MO learning example. Consider two conflicting objectives: given a sample $x_k$ from input variable $X\\in [0,2\\pi]$, predict the corresponding output $z_{k}$ that matches $y_{k}^{(j)}$ from target variable $Y_{j}$, where $X$ and $Y_{j}$ are related as follows:\n\\begin{equation*}\n  Y_{1}=\\cos(X),\\quad Y_{2}=\\sin(X)\n\\end{equation*}\nThe corresponding loss functions are $L_{j}=\\mathrm{MSE}_{j}=\\tfrac{1}{|S|}\\sum_{k=1}^{|S|}(y_{k}^{(j)}-z_{k})^{2}$. We generated 200 samples of input and target variables for training and validation each. We trained five neural networks for 20000 iterations each with two fully connected linear layers of 100 neurons followed by ReLU nonlinearities. The reference point was set to $(20,20)$.\n\nFigure~\\ref{fig:sincos_2d_losses} shows the HV over training iterations for the set of networks which stabilizes visibly. Figure~\\ref{fig:sincos_2d_predictions} shows predictions (y-axis) for validation samples evenly sampled from $[0,2\\pi]$ (x-axis). These predictions by five neural networks constitute Pareto front approximations for each sampled $x_k$, and correspond to precise predictions for $\\cos(X)$ and $\\sin(X)$, and trade-offs between both target functions. As discussed in Section~\\ref{sec:batch_vs_persample}, a network may generate predictions with changing trade-offs for different samples. This is clearly demonstrated by Networks~2-5 in Figure~\\ref{fig:sincos_2d_predictions} for $x\\in [\\tfrac{3/2}\\pi,2\\pi]$ which change their trade-offs.\n\nFigure~\\ref{fig:sincos_2d_os} shows these Pareto front predictions in loss space (only a selection of outputs is shown to simplify visualization). It becomes clear from Figures~\\ref{fig:sincos_2d_predictions}~\\&~\\ref{fig:sincos_2d_os} that each $x_k$ has a differently sized Pareto front which the networks are able to predict. The Pareto fronts for samples corresponding to $x =\\tfrac{1}{4}\\pi$ (and $x=\\tfrac{5}{4}\\pi$) reduce to a single point because $\\cos(X)$ and $\\sin(X)$ are equal. An illustration of MO learning on three losses is provided in Appendix~\\ref{suppl:sin_cos}.\n\n\\begin{figure*}[ht!]\n\\begin{subfigure}{0.32\\textwidth}\n    \\centering\n    \\includegraphics[width=0.80\\textwidth]{figures/experiments/sincos/sin_cos_2d_hv_convergence.png}\n    \\caption{}\n    \\label{fig:sincos_2d_losses}\n\\end{subfigure}\n\\begin{subfigure}{0.32\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/sincos/sin_cos_2d_predictions.png}\n    \\caption{}\n    \\label{fig:sincos_2d_predictions}\n\\end{subfigure}\n\\begin{subfigure}{0.32\\textwidth}\n    \\centering\n    \\includegraphics[width=0.80\\textwidth]{figures/experiments/sincos/sin_cos_2d_pareto_fronts_in_os.png}\n    \\caption{}\n    \\label{fig:sincos_2d_os}\n\\end{subfigure}\n    \\centering\n    \\caption{MO regression on two losses. (a) HV values for a set of networks over training iterations. (b) Network outputs for $X\\in[0,2\\pi]$. (c) Generated Pareto front predictions for a selection of six samples from $[\\tfrac{1}{4}\\pi,\\tfrac{3}{4}\\pi]$ in loss space.}\n    \\label{fig:sincos_2d}\n\\end{figure*}\n\n\\begin{figure}[h]\n\\begin{tabular}{ccccc}\n    & \\textbf{\\footnotesize Linear scalarization} & \\textbf{\\footnotesize Pareto MTL} & \\textbf{\\footnotesize EPO} & \\textbf{\\footnotesize HV maximization}\\\\\n    \\midrule\n    \\rotatebox[origin=c]{90}{\\textbf{\\footnotesize MSE \\& MSE}} &\n    \\begin{subfigure}{0.2\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/sincos/pareto_fronts_per_sample_FOR_sin_cos_linear_scalarization.png}\n    \\caption{}\n    \\label{fig:comparison_ls_sin_cos}\n    \\end{subfigure}\n    & \n    \\begin{subfigure}{0.2\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/sincos/pareto_fronts_per_sample_FOR_sin_cos_pareto_mtl.png}\n    \\caption{}\n    \\label{fig:comparison_pmtl_sin_cos}\n    \\end{subfigure}\n    &\n    \\begin{subfigure}{0.2\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/sincos/pareto_fronts_per_sample_FOR_sin_cos_epo.png}\n    \\caption{}\n    \\label{fig:comparison_epo_sin_cos}\n    \\end{subfigure}\n    &\n    \\begin{subfigure}{0.2\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/sincos/pareto_fronts_per_sample_FOR_sin_cos_higamo_hv.png}\n    \\caption{}\n    \\label{fig:comparison_higamo_hv_sin_cos}\n    \\end{subfigure}\\\\\n    \n    \\rotatebox[origin=c]{90}{\\textbf{\\footnotesize MSE \\& L1-Norm}}\n    & \n    \\begin{subfigure}{0.2\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/sincos/pareto_fronts_per_sample_FOR_sin_cos_l1_linear_scalarization.png}\n    \\caption{}\n    \\label{fig:comparison_ls_sin_cos_l1}\n    \\end{subfigure}\n    &\n    \\begin{subfigure}{0.2\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/sincos/pareto_fronts_per_sample_FOR_sin_cos_l1_pareto_mtl.png}\n    \\caption{}\n    \\label{fig:comparison_pmtl_sin_cos_l1}\n    \\end{subfigure}\n    &\n    \\begin{subfigure}{0.2\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/sincos/pareto_fronts_per_sample_FOR_sin_cos_l1_epo.png}\n    \\caption{}\n    \\label{fig:comparison_epo_sin_cos_l1}\n    \\end{subfigure}\n    &\n    \\begin{subfigure}{0.2\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/sincos/pareto_fronts_per_sample_FOR_sin_cos_l1_higamo_hv.png}\n    \\caption{}\n    \\label{fig:comparison_higamo_hv_sin_cos_l1}\n    \\end{subfigure}\\\\\n    \n    \\rotatebox[origin=c]{90}{\\textbf{\\footnotesize MSE \\& scaled MSE}}\n    &\n    \\begin{subfigure}{0.2\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/sincos/pareto_fronts_per_sample_FOR_sin_cos_scaledmse_linear_scalarization.png}\n    \\caption{}\n    \\label{fig:comparison_ls_sin_cos_scaledmse}\n    \\end{subfigure}\n     &\n    \\begin{subfigure}{0.2\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/sincos/pareto_fronts_per_sample_FOR_sin_cos_scaledmse_pareto_mtl.png}\n    \\caption{}\n    \\label{fig:comparison_pmtl_sin_cos_scaledmse}\n    \\end{subfigure}\n    & \n    \\begin{subfigure}{0.2\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/sincos/pareto_fronts_per_sample_FOR_sin_cos_scaledmse_epo.png}\n    \\caption{}\n    \\label{fig:comparison_epo_sin_cos_scaledmse}\n    \\end{subfigure}\n    & \n    \\begin{subfigure}{0.2\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/sincos/pareto_fronts_per_sample_FOR_sin_cos_scaledmse_higamo_hv.png}\n    \\caption{}\n    \\label{fig:comparison_higamo_hv_sin_cos_scaledmse}\n    \\end{subfigure}\n\\end{tabular}\n    \\centering\n    \\caption{Pareto front approximations by sets of five neural networks on a random subset of validation samples trained using four approaches. Three different pairs of loss functions are used: (a)-(d) MSE and MSE, (e)-(h) MSE and L1-Norm, and (i)-(l) MSE and scaled MSE.}\n    \\label{fig:comparison_sin_cos_all}\n\\end{figure}\n\n\\begin{table}[]\n\\renewcommand{\\arraystretch}{1.5}\n\\caption{\\small The mean HV of the approximated Pareto fronts for 200 validation samples for three pairs of loss functions (columnwise): MSE \\& MSE, MSE \\& L1-Norm, and MSE \\& scaled MSE. Median (inter-quartile range) values of the mean HV for 25 runs are reported. The maximal median HV in each column is \\textbf{highlighted}. Statistical significance of one-sided Wilcoxon signed rank tests after correction for multiple comparison is indicated for comparisons: LS vs HV maximization (\\textsuperscript{$\\ast$}), PMTL vs HV maximization (\\textsuperscript{$\\dagger$}), and EPO vs HV maximization (\\textsuperscript{$\\ddagger$}). The relevance of small increases in HV close to the maximum (400) is described in Section~\\ref{sec:sin_cos_asymmetric}.\n}\n\\label{tab:HV_sin_cos}\n\\begin{tabular}{m{2cm}m{3.8cm}m{3.8cm}m{3.8cm}}\n\\toprule\n\\textbf{} & \\textbf{MSE \\& MSE} & \\textbf{MSE \\& L1-Norm} & \\textbf{MSE \\& scaled MSE}\\\\\n\\midrule\n\\small Linear scalarization (LS) & \\footnotesize \\textbf{399.5929\\textsuperscript{$\\ast$} \\newline(399.5776 -- 399.6018)}  & \\footnotesize 399.2909 \\newline(399.2738 -- 399.3045)  & \\footnotesize 399.9859 \\newline(399.9857 -- 399.9864)\\\\ \n\n\\small Pareto MTL (PMTL) & \\footnotesize 397.1356 \\newline(396.3212 -- 397.6288)  & \\footnotesize 392.2956 \\newline(392.0377 -- 393.4942)  & \\footnotesize 398.3159 \\newline(397.4799 -- 398.6699)\\\\ \n\n\\small EPO & \\footnotesize 399.5135 \\newline(399.5051 -- 399.5348) & \\footnotesize 399.0884 \\newline(398.998 -- 399.1743)  & \\footnotesize 399.9885 \\newline(399.9883 -- 399.9889)\\\\ \n\n\\small HV maximization & \\footnotesize 399.5823\\textsuperscript{$\\dagger \\ddagger$} \\newline(399.5619 -- 399.6005) & \\footnotesize \\textbf{399.3795\\textsuperscript{$\\ast \\dagger \\ddagger$} \\newline(399.3481 -- 399.4039)}  & \\footnotesize \\textbf{399.9954\\textsuperscript{$\\ast \\dagger \\ddagger$} \\newline(399.9927 -- 399.9957)}\\\\ \n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\\subsubsection{Asymmetric Pareto Fronts}\n\\label{sec:sin_cos_asymmetric}\nConflicting loss functions may behave or scale differently, giving rise to asymmetric Pareto fronts (asymmetric in the line where losses are equal, i.e., $L_{1}=L_{2}$). We investigated the ability of different MO learning approaches to predict different types of Pareto fronts using combinations of basic loss functions: the symmetric case with two MSE losses as in Figure \\ref{fig:sincos_2d}, and two asymmetric cases each with MSE as one loss and L1-norm or MSE scaled by $\\tfrac{1}{100}$ as the second loss. Figure~\\ref{fig:comparison_sin_cos_all} shows Pareto front approximations for all three cases. The mean HV over 200 validation samples is computed for all approaches and Table~\\ref{tab:HV_sin_cos} displays the median and inter-quartile ranges (IQR) over 25 runs. %Median and IQR statistics were chosen because the distribution of mean HVs is not known. \nThe magnitude of the HV is largely determined by the position of the reference point. For $r=(20,20)$ the maximal HV equals 400 minus the area bounded by the utopian point $(0,0)$ and a sample's Pareto front. Even poor approximations of a sample's Pareto front can yield a HV $\\geq390$. For these reasons, HVs in Table~\\ref{tab:HV_sin_cos} appear large and minuscule differences between HVs are relevant.\n\nFigures~\\ref{fig:comparison_ls_sin_cos}~\\&~\\ref{fig:comparison_epo_sin_cos} show that fixed linear scalarizations and EPO produce networks generating well-distributed outputs with low losses that predict a sample's symmetric Pareto front for two conflicting MSE losses. As these a priori selected trade-offs appear to span this Pareto front shape well, linear scalarization's training based on fixed loss weights is more efficient than training on a dynamic loss as used by HV maximization (Figure~\\ref{fig:comparison_higamo_hv_sin_cos}) which is affected by stochastic effects. This increased efficiency of training using fixed weights that are suitable for symmetric MSE losses presumably results in a slightly higher HV (Table~\\ref{tab:HV_sin_cos}) for linear scalarization. \nThe positions on the front approximated by linear scalarization seem to be far from the pre-specified trade-offs (gray lines). This is expected because, by definition of linear scalarization, the solutions should lie on the approximated Pareto front where the tangent is perpendicular to the search direction specified by the trade-offs. For Pareto MTL, networks are clustered closer towards the center of the approximated Pareto front. Figure~\\ref{fig:comparison_sin_cos_all} indicates that methods relying on pre-specified trade-offs can be sufficient when Pareto fronts are symmetric, which is also supported by an MO segmentation experiment on medical imaging data (Section~\\ref{sec:mo_segmentation}).\n\nOptimizing MSE and L1-Norm (Figures~\\ref{fig:comparison_ls_sin_cos_l1}-\\ref{fig:comparison_higamo_hv_sin_cos_l1}) results in an asymmetric Pareto front approximation. The predictions by our HV maximization-based approach remain well distributed across the fronts. EPO also still provides a decent spread albeit less uniform across samples whereas linear scalarization and Pareto MTL tend to both or mostly the lower extrema, respectively.\n\nThe difficulty of manually pre-specifying the trade-offs without knowledge of the Pareto front becomes more evident when optimizing losses with highly different scales (Figures~\\ref{fig:comparison_ls_sin_cos_scaledmse}-\\ref{fig:comparison_higamo_hv_sin_cos_scaledmse}). The pre-specified trade-offs do not evenly cover the Pareto fronts and consequently the networks trained by EPO do not cover the Pareto front evenly despite following the pre-specified trade-offs. Further, the networks optimized by Pareto MTL cover only the upper part of the fronts. Networks trained with fixed linear scalarizations tend towards both extrema. Our approach, on the other hand, trains networks that follow well-distributed trade-offs on the Pareto front.One might suggest that losses that exhibit different scales as in Figures~\\ref{fig:comparison_ls_sin_cos_scaledmse}-\\ref{fig:comparison_higamo_hv_sin_cos_scaledmse} can be `fixed' by manual rescaling. Results presented in Appendix~\\ref{suppl:loss_rescale} show that rescaling losses based on scale differences observed at initialization does not sufficiently improve methods based on pre-specified trade-offs (Pareto MTL, EPO) or fixed linear scalarizations.\n\nThese experiments clearly demonstrate the advantage of MO learning by HV maximization over existing strategies when the Pareto front is asymmetric. \n\n\\begin{figure}[h!]\n    \\centering\n    \\includegraphics[width=0.7\\textwidth]{figures/experiments/mo_seg/mo_seg_predictions.png}\n    \\caption{An example of one approximated Pareto set for Multi-observer segmentation consisting of predicted segmentation contours from five neural networks trained with HV maximization.}\n    \\label{fig:mo seg predictions}\n\\end{figure}\n\n\\subsection{Multi-Observer Medical Image Segmentation}\n\\label{sec:mo_segmentation}\nMulti-observer medical image segmentation pertains to learning automatic segmentation based on delineations provided by multiple expert observers, which may be conflicting due to inter-observer variability \\cite{villeirs2005interobserver, white2009inter}. The key motivation behind learning automatic medical image segmentation multi-objectively rather than learning from average of the two conflicting expert delineations, is to be able to present multiple automatically segmented contours covering the entire range of conflict between expert delineations to the clinicians. It is expected that this will increase the chance of one of the automatic contours being accepted without modification, and therefore would increase the clinical adaptability of this automatic segmentation approach. \n\nWe applied our MO learning approach to the multi-observer medical image segmentation scenario mentioned in \\cite{ArkadiySPIE2020}. The dataset \\citep{simpson2019large} contains Magnetic Resonance Imaging (MRI) scans of prostate regions of 32 patients. The original single observer delineations are systematically perturbed to simulate different styles of delineation. We generate a bi-observer learning scenario from this dataset (Figure~\\ref{fig:mo seg predictions}), where the two observer delineations disagree in the extent of the prostate region. % architecture\nWe trained five neural networks for 10000 iterations to minimize soft Dice losses with the delineations provided by the two observers. The famous UNet \\citep{UnetMiccai} architecture was used for the neural networks. The reference point was set to $(20, 20)$.\n\\begin{figure*}[h!]\n\\begin{subfigure}{0.4\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/mo_seg/pareto_fronts_per_sample_FOR_mo_segmentation_linear_scalarization.png}\n    \\caption{Linear scalarization}\n    \\label{fig:mo_seg_lin_scal}\n\\end{subfigure}\n\\begin{subfigure}{0.4\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/mo_seg/pareto_fronts_per_sample_FOR_mo_segmentation_pareto_mtl.png}\n    \\caption{Pareto MTL}\n    \\label{fig:mo_seg_pareto_mtl}\n\\end{subfigure}\n\\begin{subfigure}{0.4\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/mo_seg/pareto_fronts_per_sample_FOR_mo_segmentation_epo.png}\n    \\caption{EPO}\n    \\label{fig:mo_seg_epo}\n\\end{subfigure}\n\\begin{subfigure}{0.4\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/mo_seg/pareto_fronts_per_sample_FOR_mo_segmentation_higamo_hv.png}\n    \\caption{HV maximization}\n    \\label{fig:mo_seg_higamo_hv}\n\\end{subfigure}\n    \\centering\n    \\caption{Pareto front approximations for 10 randomly selected validation samples for multi-observer medical image segmentation by different approaches.}\n    \\label{fig:mo seg results}\n\\end{figure*}\n\nFigure~\\ref{fig:mo seg predictions} shows one example of multi-objective decision-making in our multi-observer medical image segmentation scenario. The delineations from Observer 2 consistently have a prostate region that is under-segmented by 10 pixels as compared to Observer 1. The predictions from two out of five neural networks follow one delineation style each, the rest of the predictions partially match both of the delineation styles, thus allowing the decision-maker to choose one segmentation from five possibilities between two extreme styles of delineation.\n\nThe Pareto front approximations (represented by soft Dice loss) from different approaches for ten randomly selected validation samples are shown in Figure~\\ref{fig:mo seg results}. It can be seen for all the approaches that, while the Pareto front approximations seem to be optimal for some validation samples, it is not true for all the samples. This highlights the unavoidable generalization gap on the validation samples and the shortcoming of the existing as well as our proposed approach in meeting the goal of MO decision-making per sample during inference.\n\nThe hypervolume values of the Pareto front approximation on validation samples from 50 Monte Carlo cross-validation runs with a 80:20 split are reported in Table \\ref{tab:HV_mo_segmentation}. A t-test was chosen because the distribution of mean HVs was approximately normal. The network initializations were different in each run. The hypervolume was observed to be maximal for our proposed HV maximization-based approach indicating that the Pareto fronts approximations on validation samples were closer to the Pareto front when using the HV maximization approach. The differences in hypervolume values were statistically significant between  Pareto MTL and HV maximization ($t(49)=-8.350, p=5.59e^{-11}$), and EPO and HV maximization ($t(49)=-3.066, p=0.004$), but not between linear scalarization and HV maximization ($t(49)=-0.915, p=0.365$). It may be attributed to the fact that the Pareto front is strictly convex and symmetrical for all validation samples. On the one hand, symmetric Pareto fronts are well-suited for linear scalarization (in line with results shown in Section~\\ref{sec:sin_cos_asymmetric}), and on the other hand, the noise in HV maximizing gradients due to batchwise training might have limited the performance of HV maximization approach. \n\n\\begin{table}[]\n\\renewcommand{\\arraystretch}{1.5}\n\\centering\n\\caption{\\small{Mean $\\pm$ standard deviation hypervolume of the approximated Pareto fronts by different approaches for validation samples from 50 Monte Carlo cross-validation runs. The maximal mean hypervolume is \\textbf{highlighted}. Statistical significance using one-sided paired t-tests is indicated for post-hoc comparisons: LS vs HV maximization (\\textsuperscript{$\\ast$}), PMTL vs HV maximization (\\textsuperscript{$\\dagger$}), and EPO vs HV maximization (\\textsuperscript{$\\ddagger$}). The relevance of small increases in HV close to the maximum (400) is explained in Section~\\ref{sec:sin_cos_asymmetric}.\n}}\n\\label{tab:HV_mo_segmentation}\n\\begin{tabular}{rl}\n\\toprule\n\\textbf{} & \\textbf{Hypervolume}\\\\\n\\midrule\n\\small Linear scalarization (LS) & \\footnotesize 396.6268 $\\pm$ 0.6108\\\\\n\\small Pareto MTL (PMTL) & \\footnotesize 396.2791 $\\pm$ 0.6128\\\\\n\\small EPO & \\footnotesize 396.4999 $\\pm$ 0.6700\\\\\n\\small HV maximization & \\footnotesize \\textbf{396.6778 $\\pm$ 0.5777\\textsuperscript{$\\dagger \\ddagger$}}\\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\\subsection{Neural Style Transfer}\n\\label{sec:style_transfer}\nWe further apply our approach to the problem of style transfer, i.e., the transfer of the artistic style of an image onto a target image while preserving its semantic content. Users likely cannot provide their preferred trade-off between style and content without seeing the resulting images. Providing an approximation of Pareto front is thus a useful tool in aiding decision-making.\n\nContrary to MO regression (Section~\\ref{sec:sin_cos}) and MO segmentation (Section~\\ref{sec:mo_segmentation}), this is not a MO learning but a MO optimization problem using neural networks with differently shaped Pareto fronts per sample. We selected the problem definition by \\cite{gatys2016image}, where pixels of an image are optimized to minimize a weighted combination of content loss (semantic similarity with the target image) and style loss (artistic similarity with the style image). The content loss and the style loss are computed from features of a pretrained VGG network \\citep{Simonyan15}. We reused and adjusted Pytorch's neural style transfer implementation \\citep{pytorch_style_tutorial}. The reference point was chosen as (100, 10000) based on preliminary runs. %This also approximately corresponds to the different scale of losses at initialization.\nThe results presented below are based on 25 image pairs, obtained by combining 5 content and 10 style images. All images or their references are available in Appendix~\\ref{suppl:image_details}, most were collected from WikiArt \\citep{wikiart} and are in the public domain or available under fair use.\n\n\\begin{figure*}[h!]\n\\begin{subfigure}{0.45\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/style_transfer/2d_style_transfer_fronts_lin_scal.png}\n    \\caption{}\n    \\label{fig:2d_style_transfer_comparison_multi_fig_lin_scal}\n\\end{subfigure}\n\\begin{subfigure}{0.45\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/style_transfer/2d_style_transfer_fronts_pareto_mtl.png}\n    \\caption{}\n    \\label{fig:2d_style_transfer_comparison_multi_fig_pareto_mtl}\n\\end{subfigure}\n\\begin{subfigure}{0.45\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/style_transfer/2d_style_transfer_fronts_epo.png}\n    \\caption{}\n    \\label{fig:2d_style_transfer_comparison_multi_fig_epo}\n\\end{subfigure}\n\\begin{subfigure}{0.45\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/experiments/style_transfer/2d_style_transfer_fronts_higamo.png}\n    \\caption{}\n    \\label{fig:2d_style_transfer_comparison_multi_fig_higamo_hv}\n\\end{subfigure}\n    \\centering\n    \\caption{Pareto front estimates in loss space by different approaches for neural style transfer using four approaches: (a) Linear scalarization (b) Pareto MTL, (c) EPO, and (d) HV maximization. Sections within the black frames are magnified.}\n    \\label{fig:2d_style_transfer_comparison_multi_fig}\n\\end{figure*}\nFigure~\\ref{fig:2d_style_transfer_comparison_multi_fig} shows the obtained Pareto front estimates for 25 image sets by each approach. Linear scalarization (a) and EPO (c) determine solutions close to or on the chosen user-preferences which, however, do not diversely cover the range of possible trade-offs. Pareto MTL (b) achieves sets of clustered and partly dominated solutions which do not cover trade-offs with low content loss. On the other hand, HV maximization (d) returns Pareto front estimates that broadly cover trade-offs across image sets without having to specify user preferences, which is also reflected in the significantly larger median HVs reported in Table~\\ref{tab:HV_style_transfer}. As noted in Section~\\ref{sec:sin_cos_asymmetric}, the magnitude of the reported HVs is due to the choice of reference point and already small increases in the HV can indicate a relevant improvement in Pareto front approximation quality.\nThese results show that estimates based on pre-specified trade-offs may not span the Pareto front well. HV maximization appears to find better estimates when the shape of the Pareto front is unknown, which enables a posteriori decision-making.  \n\n\\begin{figure*}[h!]\n\\centering\n\\includegraphics[width=1.00\\textwidth]{figures/experiments/style_transfer/style_transfer_2d_images.png}\n\\caption{Images generated by all four approaches for image set B19.} %(B19, see Table~\\ref{tab:style_transfer_image_info})\n\\label{fig:style_transfer_2d_example_corrected}\n\\end{figure*}\n\nFigure~\\ref{fig:style_transfer_2d_example_corrected} shows the images generated by each approach for one of the sets used in Figure~\\ref{fig:2d_style_transfer_comparison_multi_fig}. This case (B19, see Appendix~\\ref{suppl:image_details} for image set definitions) was manually selected for its aesthetic appeal.\\footnote{Generated images for all image sets B1-B25 are available at \\url{https://github.com/timodeist/multi_objective_learning}}%(generated images for all image sets B1-B25 are shown in Appendix~\\ref{suppl:style_transfer_image_grids}). \nThe images seen here match observations from Figure~\\ref{fig:2d_style_transfer_comparison_multi_fig}, e.g., Pareto MTL's images show little diversity in style and content, many images by linear scalarization of EPO have too little style match (`uninteresting' images), and images by HV maximization show most interesting diversity.\n\n\\begin{table}[]\n\\renewcommand{\\arraystretch}{1.5}\n\\centering\n\\caption{\\small{Median (inter-quartile range) hypervolume of the approximated Pareto fronts by different approaches for 25 image sets of neural style transfer, each time using a different content and style image. The maximal median hypervolume is \\textbf{highlighted}. Statistical significance in one-sided Wilcoxon signed rank test after multiple comparisons correction is indicated for comparisons: LS vs HV maximization (\\textsuperscript{$\\ast$}), PMTL vs HV maximization (\\textsuperscript{$\\dagger$}), and EPO vs HV maximization (\\textsuperscript{$\\ddagger$}).\nThe relevance of small increases in HV close to the maximum ($10^{6}$) is described in Section~\\ref{sec:sin_cos_asymmetric}.\n}}\n\\label{tab:HV_style_transfer}\n\\begin{tabular}{rl}\n\\toprule\n\\textbf{} & \\textbf{Hypervolume}\\\\\n\\midrule\n\\small Linear scalarization (LS) & \\footnotesize 999990.7699 (999988.6580 -- 999992.5850)\\\\\n\\small Pareto MTL (PMTL) & \\footnotesize 997723.8748 (997583.5152 -- 998155.6837)\\\\\n\\small EPO & \\footnotesize 999988.4297 (999984.4808 -- 999989.8338)\\\\\n\\small HV maximization & \\footnotesize \\textbf{999999.7069 (999999.4543 -- 999999.8266)\\textsuperscript{$\\ast \\dagger \\ddagger$}}\\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\\begin{figure}[h!]\n\\centering\n\\includegraphics[width=0.8\\textwidth]{figures/experiments/style_transfer/styletransfer_t-shape_paper_shifted_style_images.png} %width=0.48\n\\caption{Neural multi-style transfer as an example for HV optimization in three (style) dimensions. The T-shape approximately reflects their style loss ordering. For example, the Frenchman's Bay loss of Image~4 is lower than of Image~5 and Image~2.}%: images in the top left have lowest style loss with Cole's \\emph{View Across Frenchman's Bay}, top right corresponds to Picasso's \\emph{Fanny Tellier}, bottom corresponds to Hokusai's \\emph{Kajikazawa in Kai Province}.}\n\\label{fig:style_transfer_tshape}\n\\end{figure}\n\n\\subsubsection{Neural multi-style transfer}\nTrading-off content against one or more style losses results in many `uninteresting' images with visually irrelevant changes in content loss (Figure~\\ref{fig:style_transfer_2d_example_corrected}). To improve neural style transfer by removing these undesirable images and thereby demonstrating the uses of HV maximization over three objectives, we removed the content loss, defined style losses for three different style images, initialized the optimization on the content image, and ensured that sufficient original content is retained by tuning (limiting) the number of optimization iterations.  Figure~\\ref{fig:style_transfer_tshape} shows the Pareto front approximation with six images after HV maximization. This example was selected for its aesthetic appeal. Three solutions are close to the distinct artistic styles, and the others are mixes of different styles with trade-offs between the style losses. Viewing the images in loss space (Figure \\ref{fig:style_transfer_objective_space}) demonstrates that the images are diverse and clearly dispersed from each other.\\section{Discussion}\n\\label{sec:discussion}\n\\begin{figure}[!]\n\\centering\n\\includegraphics[width=0.48\\textwidth]{figures/experiments/style_transfer/styletransfer_objectivespace.png}\n\\caption{Neural multi-style transfer Pareto front approximation in loss space. Each filled circle corresponds to the image with a matching colored number in Figure~\\ref{fig:style_transfer_tshape}.}\n\\label{fig:style_transfer_objective_space}\n\\end{figure}\n\nWe adapted the gradient-based HV maximization approach from MO optimization for the goal of training a set of neural networks so that they jointly predict Pareto front approximations for each sample during inference, without prior need for user-specified trade-offs. We further show that training for Pareto front approximations of average losses is, in general, not sufficient to attain well-spread approximations on convex or concave Pareto fronts of individual samples. \n\nThe performance of our approach is demonstrated on two MO learning cases and one neural style transfer optimization case. The experimental results show that, while existing approaches perform similar to HV maximization for problems that exhibit a symmetric Pareto front, a priori specifying the trade-offs fails when the Pareto front is asymmetric and consequently causes worse performance. In contrast, our HV maximization approach still finds well-spread solutions on these asymmetric Pareto fronts. Furthermore, we have shown that the issue of a priori specified trade-offs not being able to cover the entire Pareto front in case of asymmetric Pareto front shapes cannot be solved trivially by rescaling the losses based on their initial magnitude.\nIt should also be noted that multi-objective machine learning problems in real-world are likely not symmetric. Our HV-based approach therefore is preferable over methods relying on trade-offs specified a priori.\n\nHV maximization does not require specifying $p$  trade-offs a priori (based on the number of predictions, $p$, required on the Pareto front), which essentially are $p(n-1)$ hyperparameters of the learning process for $n$ losses. Choosing these trade-offs well requires knowledge of the Pareto front shapes which is often not known a priori. HV maximization, however, introduces the $n$-dimensional reference point $r$ and thus $n$ additional hyperparameters. Luckily, choosing a reference point such that the entire Pareto front is approximated by well-spread predictions is not complex. It often suffices to use losses of randomly initialized networks rescaled by a factor $\\geq$1 as the reference point. For the special case that only a specific section of the Pareto front is relevant and this information is known a priori, the reference point can be chosen so that the Pareto front approximation only spans the chosen section.\n\nThe HV-based training for set of neural networks can, in theory, be applied to any number of networks, $p$, and loss functions, $n$. In practice, the time complexity of exact HV (exponential in $n$, \\cite{fonseca2006improved}) and HV gradient (quadratic in $p$ with $n\\leq 4$, \\cite{emmerich2014time}) computations is limiting but may be overcome by algorithmic improvements using, e.g., HV approximations. Further, in our current implementation, a separate network is trained corresponding to each prediction. This increases computational load linearly if more predictions on the Pareto front are desired. We chose for this setup for the sake of simplicity in experimentation and clarity when demonstrating our proof-of-concept. It is expected that the HV maximization formulation would work similarly if the parameters of some of the neural network layers are shared, which would decrease computational load.\n\nWhen training networks on multiple losses using the preferred per-sample Dynamic loss~\\eqref{eq:final_joint_loss}, a network is not restricted to generate predictions in a specific order of trade-offs for all samples. Instead, predictions for different samples can follow any order of networks on the samples' Pareto fronts as observed in Figure~\\ref{fig:sincos_2d_predictions}. Limiting this flexibility might simplify the learning problem and speed up convergence without significantly decreasing prediction quality.\n\nIn conclusion, the present work describes a method for and advantages of learning-based a posteriori MO decision-making based on MO training of neural networks using HV maximization. We provided a detailed analysis of MO learning for different Pareto front shapes using an artificial MO regression problem. Additionally, the MO segmentation (Section~\\ref{sec:mo_segmentation}) and MO style transfer problem (Section~\\ref{sec:style_transfer}) yielded encouraging results that emphasize the real-world relevance of the proposed method. Future work should further investigate the usability of HV maximization in more complex learning problems on real-world data. \n\n\\acks{ We would like to thank dr. Marco Virgolin from Chalmers University of Technology for his valuable contributions and discussions on concept and code.\n The research is part of the research programme, Open Technology Programme with project number 15586, which is financed by the Dutch Research Council (NWO), Elekta, and Xomnia. Further, the work is co-funded by the public-private partnership allowance for top consortia for knowledge and innovation (TKIs) from the Ministry of Economic Affairs.}\n\n\\newpage\n\n\\vskip 0.2in\n\n\\newpage\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{LibMOON: A Gradient-based MultiObjective OptimizatioN Library in PyTorch}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\n    Multiobjective optimization problems (MOPs) are prevalent in machine learning, with applications in multi-task learning, learning under fairness or robustness constraints, etc. Instead of reducing multiple objective functions into a scalar objective, MOPs aim to optimize for the so-called Pareto optimality or Pareto set learning, which involves optimizing more than one objective function simultaneously, over models with thousands / millions of parameters. Existing benchmark libraries for MOPs mainly focus on evolutionary algorithms, most of which are zeroth-order / meta-heuristic methods that do not effectively utilize higher-order information from objectives and cannot scale to large-scale models with thousands / millions of parameters. In light of the above gap, this paper introduces \\algoname, the first multiobjective optimization library that supports state-of-the-art gradient-based methods, provides a fair benchmark, and is open-sourced for the community.\n\\end{abstract}\n\n\\keywords{Mathematical tools \\and Multiobjective optimization \\and Pareto set learning \\and Bayesian optimization \\and Pareto Machine Learning}\n\n\\section{Introduction}\nMultiObjective Optimization problems (MOPs) are ubiquitous in machine learning. For instance, trustworthy machine learning includes algorithmic fairness problems balancing the fairness level and the accuracy level~\\cite{zhao2022inherent,xian2023fair}; \nin robotics, it is necessary to balance several objectives (e.g., forward speed and energy consumption) ~\\cite{brandao2019multi, xu2020prediction}; similarly, recommendation systems face potentially conflicting objectives, such as novelty, accuracy, and diversity~\\cite{jannach2022multi,keat9791369,zaizi2023102233}.\nFor all the applications above, the underlying optimization problem involves an MOP with \\( m \\) objectives and can be (informally) defined as:\n\\begin{equation}\n    \\min_\\vtheta \\vL(\\vtheta) = ( L_1(\\vtheta), \\ldots, L_m(\\vtheta) ),\n    \\label{eqn:mop}\n\\end{equation}\nwhere \\( L_1(\\vtheta), \\ldots, L_m(\\vtheta) \\) denote \\( m \\) (potentially) conflicting objectives and we denote the size of the model parameter as \\(N \\coloneqq |\\vtheta| \\). Note that as informally defined above,~\\Cref{eqn:mop} is a vector optimization problem that does not necessarily admit a total ordering.\nFor a non-trivial MOP, no single solution can attain the minimum of all objectives simultaneously. To compare two solutions for an MOP, we introduce the concepts of \\emph{dominance} and \\emph{Pareto optimality}~\\cite{ehrgott2005multicriteria}. \nDominance occurs when solution \\( \\vtheta^{(a)} \\) satisfies \\( L_i(\\vtheta^{(a)}) \\leq L_i(\\vtheta^{(b)}) \\) for all \\( 1 \\leq i \\leq m \\), with at least one strict inequality.\nA solution is Pareto optimal if no other solution in the feasible region dominates it. The set of all Pareto optimal solutions is called the Pareto set (PS), and its image set is called the Pareto front (PF).\n\nOver the last few decades, multiobjective evolutionary algorithms (MOEAs) emerged as a widely used methodology for addressing MOPs due to their population nature to approximate the PS. Several popular MOEA libraries have emerged, including PlatEMO (Matlab) ~\\cite{tian2017platemo}, Pagmo (C++)~\\cite{Biscani2020}, and Pymoo (Python)~\\cite{blank2020generating}. Compared to MOEAs, gradient-based multiobjective optimization (MOO) methods are particularly suitable designed for large-scale machine learning tasks involving thousands to millions of neural network parameters. While, gradient based MOO methods can only find Pareto \\emph{stationary} solutions—solutions that cannot be \\emph{locally} improved, in practice Pareto stationary solutions approximate global Pareto solutions well.\n\nWith the growing needs of gradient-based MOO methods for neural networks~\\cite{sener2018multi, lin2019pareto,xu2020prediction,peitz2018gradient}, there is a pressing need for the development of a standard library to benchmark related algorithms and problems. For this reason, we introduce \\algoname, the first modern gradient-based MOO library supporting over twenty state-of-the-art (SOTA) methods. We summarize our contributions as:\n\\begin{enumerate}[itemsep=-0.2em, topsep=0.0em, leftmargin=1.0em]\n    \\item We propose the \\textit{first} modern gradient-based MOO library, called \\algoname. \\algoname~is implemented in PyTorch\\cite{imambi2021pytorch} and carefully designed to support GPU acceleration. \\algoname~supports synthetic problems, real-world problems, and MO machine learning problems such as fair classification, MO classification, MO regression, MO distribution matching and etc. \n    \\item \\algoname~supports over twenty SOTA gradient-based MOO methods for constructing PS/PF, including those using \\emph{finite} solutions to approximate the whole PS/PF~\\cite{mahapatra2020multi,liu2021profiling}, Pareto set learning (PSL)~\\cite{navon2020learning,lin2020controllable} aimed at approximating the \\emph{entire} PS/PF with a single neural model, and MOBO methods that designed to avoid frequent function evaluations for  \n    \\item \\algoname~has already been open-sourced at \\href{https://github.com/xzhang2523/libmoon}{Github}~\\footnote{\\url{https://github.com/xzhang2523/libmoon}} with document at \\href{https://libmoondocs.readthedocs.io/en/latest/}{LibMOON Docs} ~\\footnote{\\url{https://libmoondocs.readthedocs.io/en/latest/}}. Beyond examining its source code, \\algoname~can be installed via \\texttt{pip install libmoon} as an off-the-shelf gradient-based multiobjective tool for industrial usage. \n\\end{enumerate}\n\n\\paragraph{Notation.} In this paper, bold letters represent vector (e.g., $\\vlam$ denotes a preference vector), while non-bold letters represent scalars. $\\vx_k$ denotes vector $\\vx$ at $k$-th iteration and $x_k$ denotes the $k$-th entry of $\\vx$. The preference vector $\\vlam$ lies in the $m$-dim simplex ($\\mDelta_m$), satisfying $\\sum_{i=1}^m \\lambda_i=1$ and $\\lambda_i \\geq 0$. The decision network parameter $\\vtheta$ has a size of $n$.\nFor two $m$-D vectors $\\vx^{(a)}$ and $\\vx^{(b)}$, $\\vx^{(a)} \\preceq \\vx^{(b)}$ means $\\vx_i^{(a)} \\leq \\vx_i^{(b)}$ for all $i \\in [m]$; $\\vx^{(b)}$, $\\vx^{(a)} \\preceq_\\mathrm{strict} \\vx^{(b)}$ means that $\\vx_i^{(a)} \\leq \\vx_i^{(b)}$ for all $i \\in [m]$ and for at least one index $j$, $\\vx_j^{(a)} < \\vx_j^{(b)}$. $\\vx^{(a)} \\prec \\vx^{(b)}$ means that $\\vx_i^{(a)} < \\vx_i^{(b)}$ for all $i \\in [m]$. \nRefer to~\\Cref{tab:notation} for the full notation meanings.\n\n\\section{Related works}\n\\subsection{Gradient-based multiobjective optimization} \\label{sec:gbmoo}\nGradient-based MOO has a long-standing research history. For example, the well-known convex optimization \\cite{boyd2004convex}[Chap 4] book outlines how linear scalarization can transform an MOO problem into a single-objective optimization (SOO) problem. However, for much of the past few decades, gradient-based methods have not been the primary approach for MOO, with MOEAs gaining more prominence due to their population-based approach, which is well-suited for approximating the PS and avoiding local optima. In recent years, however, gradient-based MOO has experienced a resurgence, particularly in (deep) machine learning, where these methods scale better with the number of decision variables. A pivotal contribution in this area is the MGDA-UB~\\cite{sener2018multi}, which introduced MOO techniques into deep learning by casting multi-task learning (MTL) as an MOO problem. Since then, many approaches have followed, including EPO~\\cite{mahapatra2020multi}, Pareto Multi-Task Learning (PMTL)~\\cite{lin2019pareto}, MOO with Stein Variational Gradient Descent (MOO-SVGD)~\\cite{liu2021profiling}, and methods for learning the entire PS~\\cite{lin2020controllable,navon2020learning,zhong2024panacea,lin2024smooth,lin2022pslmobo}. To make a fair comparison and for the ease of developing new methods, this paper implements them following a standardized manner.\n\n\\subsection{Multiobjective optimization libraries} \\label{sec:moolib}\nA number of multiobjective libraries exist before our work. LibMTL~\\cite{lin2023libmtl} is a library for multitask learning; the difference is that \\algoname~is designed to study the distributions of Pareto solutions, while LibMTL is mainly designed to study how to find a single solution that benefits all objectives. The major difference between \\algoname~and other previous libraries is \\algoname~is a gradient-based library, while others are evolutionary computation-based libraries. Comparisons are provided in~\\Cref{tab:library_comparison}.\n\n\\textbf{LibMTL}~\\cite{lin2023libmtl} is a Python-based library for multitask learning. LibMTL aims to find a single network to benefit all tasks, such as calculating a benign updating direction or optimizing a network architecture. In contrast, \\algoname~addresses inherent trade-offs in machine learning problems, where improving one objective inevitably worsens others, and explores the distribution of Pareto solutions.\n\n\\textbf{jMetal}~\\cite{jMetal}, \\textbf{Pymoo}~\\cite{pymoo} and \\textbf{PlatEMO}~\\cite{tian2017platemo} are Java, python and Matlab frameworks for MOEAs, supporting popular methods such as NSGA-III~\\cite{nsga31,nsga32}, MOEA/D~\\cite{zhang2007moea}, and SMS-EMOA \\cite{beume2007sms}. Pymoo allows flexible algorithm customization with user-defined operators and data visualization. PlatEMO is a MATLAB-based multiobjective optimization tool supporting over 160 MOEAs and a comprehensive test problems, including sparse, high-cost, large-scale, and multimodal. PlatEMO also contains a number of metrics and supporting visualization during the optimization process. \n\n\\textbf{Pagmo}~\\cite{Biscani2020} is a C++ library for parallel multiobjective global optimization, utilizing evolutionary algorithms and gradient-based methods like simplex, SQP, and interior-point techniques. It supports constrained, unconstrained, single- and multi-objective, continuous, integer, stochastic, and deterministic optimization problems.\n\n\\begin{wraptable}{r}{0.6\\textwidth} % Added width for the wraptable\n\\centering\n\\vspace{-15pt}\n\\setlength\\tabcolsep{2.5 pt}\n\\caption{Previous MOO libraries and \\algoname.}\n\\label{tab:library_comparison}\n\\tiny\n    \\begin{tabular}{lcclp{4cm}} % Ensure the layout is correct\n    \\toprule\n    \\textbf{Name} & \\textbf{Language} & \\textbf{Year} & \\textbf{Key Features} \\\\ \n    \\midrule\n    \\textbf{Pymoo}     & Python  & 2020 & \\begin{tabular}[l]{@{}l@{}}(1) Evolutionary computation (EC) \\\\ (2) Zero-order methods \\\\ (3) Diverse problem types\\end{tabular} \\\\\n    \\midrule\n    \\textbf{jMetal}    & Java    & 2011 & \\begin{tabular}[l]{@{}l@{}}(1) Single-/multi-objective optimization \\\\ (2) Parallel algorithms \\\\ (3) Diverse problem types\\end{tabular} \\\\\n    \\midrule \n    \\textbf{PlatEMO}   & Matlab  & 2017 & \\begin{tabular}[l]{@{}l@{}}(1) Over 160 MOEAs \\\\ (2) Various figure demonstrations \\\\ (3) Powerful and friendly GUI\\end{tabular} \\\\ \n    \\midrule \n    \\textbf{Pagmo}     & C++     & 2020 & \\begin{tabular}[l]{@{}l@{}}(1) Global optimization \\\\ (2) Parallel optimization\\end{tabular} \\\\\n    \\midrule\n    \\textbf{LibMTL}    & Python  & 2023 & \\begin{tabular}[l]{@{}l@{}}(1) Unified codebase \\\\ (2) Comprehensive SOTA MTL methods \\\\ (3) Flexible extension for new methods\\end{tabular} \\\\\n    \\midrule\n    \\textbf{EvoTorch}  & Python  & N/A & \\begin{tabular}[l]{@{}l@{}}(1) Distribution-based search algorithms \\\\ (2) Population-based search algorithms \\\\ (3) Multiple CPUs, GPUs, computers\\end{tabular} \\\\\n    \\midrule\n    \\textbf{EvoX} & Python  & 2024 & \\begin{tabular}[l]{@{}l@{}}(1) GPU acceleration optimization \\\\ (2) Single-/multi-objective optimization \\\\ (3) Neuroevolution/RL tasks\\end{tabular}  \\\\\n    \\midrule\n    \\textbf{LibMOON}   & Python  & 2024  & \\begin{tabular}[l]{@{}l@{}}(1) GPU-accelerated gradient solvers \\\\ (2) Pareto set learners \\\\ (3) Large-scale (millions \\# params.) ML tasks\\end{tabular} \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\vspace{-55pt}\n\\end{wraptable}\n\n\\textbf{EvoTorch}~\\cite{toklu2023evotorch} and \\textbf{EvoX}~\\cite{huang2024evox}.\nEvoTorch accelerates evolutionary algorithms in PyTorch, while EvoX scales them to large, complex problems with GPU-accelerated parallel execution for single and multiobjective tasks, including synthetic problems and reinforcement learning.\n\n\\section{\\algoname: A gradient-based MOO library in PyTorch} \\label{sec:method}\nThis section introduces \\algoname. We introduce its framework in \\Cref{sec:framework}, and briefly introducing its supporting \\textbf{problems} and \\textbf{metrics}. Then we introduce supported \\textbf{solvers} in~\\Cref{sec:moo_solvers,sec:psl,sec:mobod}.\n\n\\begin{wrapfigure}{r}{0.6\\textwidth}\n    \\centering\n    \\vspace{-15pt}\n    \\includegraphics[width=0.6\\textwidth]{Figure/framework.pdf}\n    \\caption{\n    \\textbf{Supported solvers and problems in \\algoname}: \\algoname~addresses synthetic, real-world and MTL problems with three categories of solvers: MOO, PSL and MOBO solvers. \n    }\n    \\vspace{-20pt}\n    \\label{fig:framework}\n\\end{wrapfigure}\n\n\\subsection{Framework} \\label{sec:framework}\n\\Cref{fig:framework} demonstrates \\algoname~components, including three categories of solvers: MOO solvers aiming to find a finite set of Pareto solutions satisfying certain requirements, Pareto set learning (PSL) solvers aiming to learn whole PS with a single model, and MOBO solvers aiming to solve expensive MO problems. Each solver category is designed highly modulized and new solvers are easy to plugin \\algoname~by rewriting only a small portion of code, e.g., the specific algorithm of gradient manipulations~\\footnote{An example of adding a new solver is provided in the \\href{https://libmoondocs.readthedocs.io/en/latest/develop/add_method.html}{\\algoname~Doc}.}. MOO and PSL solvers support all synthetic, MTL, and realworld (RE) problems, while MOBO solvers support synthetic and RE problems.\n\n\\paragraph{Supported problems.}\n\\algoname~currently supports three categories of methods, synthetic problems, MTL problems, and RE problems. \n\n\\begin{wraptable}{r}{0.6\\textwidth}\n    \\centering\n    \\vspace{-10pt}\n    \\caption{Supported MO machine learning problems.} \\label{tab:supp:problems}\n    \\footnotesize\n    \\begin{threeparttable}\n    \\begin{tabular}{llllll}\n        \\toprule\n        Method & $L_1$ & $L_2$ \\\\\n        \\midrule\n        Fairness classification~\\cite{ruchte2021scalable} & BCE & DEO \\\\\n        MO classification~\\cite{lin2019pareto} & CE - BR & CE - UL \\\\\n        MO regression~\\cite{hu2023revisiting} & MSE & MSE \\\\\n        MO distribution matching & $D(\\cdot \\| \\cdot)$ & $D(\\cdot \\| \\cdot)$ \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\begin{tablenotes}\n        \\item \\noindent \\tiny BCE: Binary Cross Entropy; DEO~\\cite{padh2021addressing}: Difference of Equality of Opportunity; CE: Cross Entropy; BR: Bottom Right; UL: Upper Left; MSE: Mean Square Error. \n    \\end{tablenotes}\n    \\end{threeparttable}\n\\end{wraptable}\n\n\\paragraph{Supported metrics.}\n\\algoname~supports a number of metrics, (1) hypervolume (HV), (2) inverted general distance (IGD), (3) fill distance (FD), (4) minimal distance ($\\mathrm{l}_{\\min}$), (5) smooth minimal distance ($\\mathrm{sl}_{\\min}$), (5) Spacing, (6) Span, (7) penalty-based intersection (PBI), (8) inner product (IP), (9) cross angle ($\\vartheta$). Full descriptions of these indicators are provided in~\\Cref{sec:metrics}.\n\n\\subsection{MOO solvers} \\label{sec:moo_solvers}\n\nIn this paper, we specially mean MOO solvers are solvers to find a set of Pareto solutions. The most simple and commonly used method is convert the MOO problem to a single objective optimization problem through some aggregation functions. \n\\paragraph{Aggregation-based methods.}\nA straightforward way is to use some aggregation functions $g_\\vlam(): \\mathbb{R}^m \\mapsto \\mathbb{R}$ to convert a MOP to a single objective optimization problem. The reason that optimizing this converted single objective optimization problem will yield Pareto optimal solutions is due to the following two theorems.\n\n\\begin{theorem}[Adapted from Theorem 2.6.2~\\cite{miettinen1999nonlinear}] \\label{thm:po}\n    If \\( g_\\vlam(\\cdot) \\) is \\textbf{strictly decreasing} w.r.t vector \\( \\vL(\\vtheta) \\), i.e., \\( g_\\vlam(\\vL(\\vtheta^{(a)})) < g_\\vlam(\\vL(\\vtheta^{(b)})) \\) when \\( \\vL_i(\\vtheta^{(a)}) \\preceq_\\mathrm{strict} \\vL_i(\\vtheta^{(b)}) \\), then the optimal solution $\\theta^*$ of \\( g_\\vlam(\\vL(\\vtheta)) \\) serves as a \\textbf{Pareto optimal} solution for the original MOP.\n\\end{theorem}\n\\textit{Proof.} See Mitten's book~\\cite{miettinen1999nonlinear}, Page 22. Similarly, for only decreasing aggregation functions, we have the following theorem.\n\\begin{theorem} \\label{thm:wpo}\n    If \\( g_\\vlam() \\) is \\textbf{decreasing} w.r.t. vector $\\vL(\\vtheta)$ (i.e., $g_\\vlam(\\vL(\\vtheta^{(a)})) \\leq g_\\vlam(\\vL(\\vtheta^{(b)}))$ when $\\vL(\\vtheta^{(a)}) \\preceq_\\mathrm{strict} \\vL(\\vtheta^{(b)})$, then the optimal solution $\\theta^*$ of \\( g_\\vlam(\\vL(\\vtheta)) \\) serves as a \\textbf{weakly Pareto optimal} solution for the original MOP.\n\\end{theorem}\n\\textit{Proof.} Similar to the proof of Mitten's book~\\cite{miettinen1999nonlinear}, Page 22. \n\nIn \\Cref{thm:wpo}, \\emph{weakly Pareto optimality} means that for a solution $\\vtheta^{(a)}$, no other solutions $\\vtheta'$ can strictly dominate it, i.e., $\\vL_i(\\vtheta') < \\vL_i(\\vtheta^{(a)})$ for all $i \\in [m]$.\nSome common aggregation functions include the linear scalarization (LS) function, where \\( g_\\vlam^\\mathrm{LS}(\\vL(\\vtheta)) = \\sum_{i=1}^m \\lambda_i L_i(\\vtheta) \\), the Tchebycheff function, where \\( g_\\vlam^\\mathrm{Tche}(\\vL(\\vtheta)) = \\max_{i \\in [m]} \\lambda_i \\cdot (L_i(\\vtheta) - z_i)  \\) ($\\vz$ is a reference point), Penalty-Based Intersection (PBI) function~\\cite{zhang2007moea}, and COSMOS function~\\cite{ruchte2021scalable}. For expressions and other aggregation functions, please refer to \\Cref{sec:agg}. \n\nFor a preference vector $\\vlam \\succ \\bm 0$, $g_\\vlam^\\mathrm{LS}(\\cdot)$ is a \\emph{strict decreasing function} meaning that directly optimizing $g_\\vlam^\\mathrm{LS}(\\cdot)$ yields \\emph{Pareto optimal solutions} (by \\Cref{thm:po}). However, for any $\\vlam \\in \\mDelta_m$, optimizing $g_\\vlam^\\mathrm{Tche}(\\cdot)$ only yields weakly \\emph{Pareto optimal solutions} (by \\Cref{thm:wpo}). For an improper setting of the weight factor $\\mu$ (see \\Cref{sec:agg} item 1 and 6), the optimal solution of $g_\\vlam^\\mathrm{PBI}(\\cdot)$ or $g_\\vlam^\\mathrm{COSMOS}(\\cdot)$ can be non-(weakly) Pareto optimal solutions of the original MOP. \n\nAn aggregation function is optimized by gradient descent in \\algoname~via backpropagation, i.e, \n$\n     \\vtheta_{k+1} = \\vtheta_k - \\eta \\vd_k = \\vtheta - \\eta \\frac{\\partial g_\\vlam(\\vL(\\vtheta))}{\\partial \\vtheta} |_{\\vtheta_k},   \n$\nwhere $\\vd_k$ is called the updating direction at the $k$th iteration.\n\n\\paragraph{Gradient manipulation-based methods.}\nBesides, directly optimizing the aggregation function, a number of so-called \\emph{``gradient manipulation methods''} solve an updating direction $\\vd_k$ using gradient information for each iteration for some specific purpose. For example, as listed in~\\Cref{tab:finite_solver}, EPO~\\cite{mahapatra2020multi} aims to find ``exact Pareto solutions'' (intersection points of Pareto front and preference vectors), HVGrad~\\cite{deist2021multi} aim to maximize the hypervolume of a set of solutions, MOO-SVGD~\\cite{liu2021profiling} aim to find diverse solutions, and PMTL~\\cite{lin2019pareto} aim to find sector-constrained Pareto solutions.\n\\begin{wraptable}{r}{0.6\\textwidth}\n\\setlength\\tabcolsep{1.5 pt}\n\\centering\n\\small\n\\vspace{-10pt}\n\\begin{threeparttable}\n\\caption{\\footnotesize MOO solvers, properties, and complexities.}\n\\label{tab:finite_solver}\n    \\tiny\n    \\begin{tabular}{llll}\n    \\toprule\n    \\textbf{Method} & \\textbf{Solution Property} & \\textbf{Complexity} & \\textbf{Pref.} \\\\\n    \\midrule\n    \\href{https://proceedings.mlr.press/v119/mahapatra20a.html}{EPO}~\\cite{mahapatra2020multi} & Exact solutions &  $O(m^2nK)$ & \\checkmark \\\\\n    \\href{https://arxiv.org/abs/2102.04523}{HVGrad}~\\cite{deist2021multi} & Solutions with maximal HV & $O(m^2nK^2)$ & \\texttimes \\\\\n    \\href{https://arxiv.org/abs/1810.04650}{MGDA-UB}~\\cite{sener2018multi} & Random solutions & $O(m^2nK)$ & \\texttimes  \\\\\n    \\href{https://papers.nips.cc/paper_files/paper/2021/hash/7bb16972da003e87724f048d76b7e0e1-Abstract.html}{MOO-SVGD}~\\cite{liu2021profiling} & Diversity by particles repulsion & $O(m^2nK^2)$ & \\texttimes \\\\\n    \n    \\href{https://arxiv.org/abs/2402.09492}{PMGDA}~\\cite{zhang2024pmgda} & Solutions under specific demands & $O(m^2nK)$ & \\checkmark \\\\\n\n    \\href{https://arxiv.org/abs/1912.12854}{PMTL}~\\cite{lin2019pareto} & Solutions in sectors & $O(m^2nK^2)$ & \\texttimes \\\\\n    \n    \\href{https://arxiv.org/abs/2111.10603}{Random}~\\cite{lin2021reasonable} & Random solutions & $O(m^2nK)$ & \\texttimes \\\\\n        \\href{https://link.springer.com/book/10.1007/978-1-4615-5563-6}{Agg-LS}~\\cite{miettinen1999nonlinear} & Convex part of a PF & $O(mnK)$ & \\checkmark \\\\\n    \n    \\href{https://ieeexplore.ieee.org/document/4358754}{Agg-Tche}~\\cite{zhang2007moea} & Exact solutions &  $O(mnK)$ & \\checkmark \\\\\n    \n    \\href{https://ieeexplore.ieee.org/document/7927726}{Agg-mTche}~\\cite{ma2017tchebycheff} & Exact solutions & $O(mnK)$ & \\checkmark \\\\\n    \n    \\href{https://ieeexplore.ieee.org/document/4358754}{Agg-PBI}~\\cite{zhang2007moea} & Approximate exact solutions & $O(mnK)$ & \\checkmark \\\\\n    \n    \\href{https://arxiv.org/abs/2103.13392}{Agg-COSMOS}~\\cite{ruchte2021scalable}  & Approximate exact solutions & $O(mnK)$ & \\checkmark \\\\\n    \n    \\href{https://arxiv.org/abs/2402.19078}{Agg-SmoothTche}~\\cite{lin2024smooth} & Approximate exact solutions & $O(mnK)$ & \\checkmark \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\begin{tablenotes}\n    \\tiny \n       \\item \\tiny $m$: number of objectives. $n$: number of decision variables. $K$: number of subproblems. $m$ is usually small (e.g., 2-4), $K$ is relatively large (e.g., 20-40), and $n$ is particularly large (e.g., 10,000). Therefore, $m^2$ is not a big concern, while $K^2$ and $n^2$ are big concerns. Complexity is for time complexity, and Pref. denotes whether this method is preference-based or not. \n    \\end{tablenotes}\n\\end{threeparttable}\n\\vspace{-10pt}\n\\end{wraptable}\nInterestingly, until now, all these gradient manipulation methods can be implemented in two steps: \\textcircled{1} calculating a dynamic weight vector \\( \\tilde{\\bm \\alpha}\\) and then \\textcircled{2} performing backpropagation on a generalized aggregation function \\( \\tilde{g}_\\vlam(\\vL(\\vtheta_k)) \\), where \\( \\tilde{g}_\\vlam(\\vL(\\vtheta_k)) = \\sum_{i=1}^m \\tilde{\\alpha}_i L_i(\\vtheta)\\). At each iteration, gradient manipulation methods can be equivalently expressed as updating the gradient of its induced generalization aggregation function $\\tilde{g}_\\vlam(\\vL(\\vtheta_k))$. The weight vector $\\tilde{\\bm \\alpha}$ are achieved by solving a linear programming (LP) problem (e.g., \\cite{mahapatra2020multi}[Eq. 24]) in EPO, a quadratic programming (QP) problem (e.g., \\cite{lin2019pareto}[Eq. 14]) in PMTL, or other more complex algorithms as used to calculate the hypervolume gradient~\\cite{emmerich2014time}.\n\nSome MOO solvers accept preference vectors $\\vlam$ as input, termed \\emph{preference-based}, affecting Pareto solution positions. The others, called \\emph{preference-free}, do not accept preferences, such as those maximizing dominated hypervolume. A summary of these solvers is in~\\Cref{tab:finite_solver}.\n\n\\subparagraph{Zero-order optimization.}\nFor the previous discussions, we assume that all gradients of objective functions $\\nabla L_i(\\vtheta)$ can be easily achieved via backward propagation. However, for some black-box optimization problems, $\\nabla L_i(\\vtheta)$'s may not easily be achieved. Therefore, \\algoname~not only supports first-order optimization, but also supports zero-order optimization methods with estimated gradients $\\hat{\\nabla} L_i(\\vtheta)$ such as evolutionary strategy (ES)~\\cite{beyer2002evolution}.\n\n\\subsection{Pareto set learning solvers} \\label{sec:psl}\n\n\\begin{table}[h!]\n\n\\caption{Comparison of different PSL methods.} \n\\label{tab:psl_comparison}\n\\centering\n\\small\n\\begin{threeparttable}\n    \\begin{tabular}{llll}\n    \\toprule\n    \\textbf{Method} & \\textbf{Property} & Vector $\\tilde{\\bm \\alpha}$ & Matrix $\\mB$ \\\\\n    \\midrule\n    \\href{https://openreview.net/pdf/9c01e8c47f7e80e87af0175ac2a5e9a356f518bd.pdf}{EPO-based PSL}~\\cite{navon2020learning} & Exact solutions & MOO solvers & BP\\\\\n    \n    \\href{https://arxiv.org/abs/2402.09492}{PMGDA-based PSL}~\\cite{zhang2024pmgda} & Solutions under specific demands & MOO solvers & BP \\\\\\href{https://openreview.net/pdf/9c01e8c47f7e80e87af0175ac2a5e9a356f518bd.pdf}{Aggregation-based PSL}~\\cite{navon2020learning} & Solutions with optimal aggregation values & BP & BP\\\\\n    \n    \\href{https://arxiv.org/pdf/2310.20426}{Evolutionary PSL}~\\cite{lin2023evolutionary} & Mitigating local minima by ES & BP & ES \\\\\n    \n    \\href{https://openreview.net/pdf?id=a2uFstsHPb}{LoRA PSL}~\\cite{chen2024efficient,zhong2024panacea,dimitriadis2024pareto} & A lighter Pareto model structure & BP & BP \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\begin{tablenotes}\n        \\footnotesize\n        \\item \n        BP: backward propagation, \n        ES: evolutionary strategy.\n    \\end{tablenotes}\n\\end{threeparttable}\n\\end{table}\n\n\\algoname~supports Pareto Set Learning (PSL), which trains a model with parameter $\\vphi$ to approximate the \\emph{entire} PS/PF. A Pareto model is denoted as $\\vtheta_\\vphi(\\cdot) : \\mDelta_m \\mapsto \\mathbb{R}^n$ with input as a preference vector and output as a Pareto solution. \n\n\\begin{wrapfigure}{r}{0.6\\textwidth}\n    \\centering\n    \\vspace{-15pt}\n    \\subfloat[Synthetic problem]{\\includegraphics[width=0.3\\textwidth]{Figure/illus/psl_synthetic.pdf}\\label{<figure1>}}\n    \\subfloat[MultiTask Learning]{\\includegraphics[width=0.3\\textwidth]{Figure/illus/psl_mtl.pdf}\\label{<figure2>}}\n    \\caption{Architecture of Pareto models.}\n    \\vspace{-15pt}\n    \\label{fig:psl:arch}\n\\end{wrapfigure}\n\n\\textbf{PSL Architecture.} Pareto models vary in structure. For synthetic problems, the simplest model is a fully-connected neural network that takes a preference as input and outputs the corresponding Pareto solution. In multitask learning, the input is a pair $(\\vx, \\vy)$ from dataset $\\mathcal{D}$, and the decision variable $\\vtheta$ represents the target network's parameter, with $\\phi$ as the hypernetwork's parameter~\\cite{ha2016hypernetworks}. The loss vector is calculated as $\\vL(\\vtheta) = \\E_{(\\vx,\\vy)\\sim \\mathcal{D}}(\\ell(\\bm t_\\vtheta(\\vx), \\vy))$, where $\\ell$ is a basic loss function like cross-entropy or mean square error. Structures of these two models are illustrated by \\Cref{fig:psl:arch}. Besides these two models, \\algoname~also supports LoRA (low rank adaptation)-based PSL~\\cite{chen2024efficient,zhong2024panacea,dimitriadis2024pareto}, which admits a low rank adaptation structure and other structures. PSL structures are decoupled from the training loss and used as plug-ins.\n\n\\textbf{PSL Training.} Goal of PSL is to find a model with parameter $\\vphi$ optimizing the PSL loss $\\ell_\\mathrm{psl}$\n$\n    \\min_\\vphi \\ell_\\mathrm{psl} = \\E_{\\vlam \\sim \\mathrm{Dir}(\\bm p)} \\tilde{g}_\\vlam(\\vL( \\vtheta_\\vphi(\\vlam) )),\n$\nwhere $\\mathrm{Dir}(\\bm p)$ is a Dirichlet distribution with hyperparameter $\\bm p$. $\\tilde{g}_\\vlam(\\cdot)$ can either be a generalized aggregation function as introduced in the previous section or a normal aggregation function. The gradient of $\\ell_\\mathrm{psl}$ can be estimated by the chain rule:\n\n\\begin{equation}\n    \\underbrace{\\frac{\\partial \\ell_\\mathrm{psl}}{\\partial \\vphi}}_{1 \\times D}\n    = \n    \\E_{\\vlam \\sim \\mathrm{Dir}(\\bm p)} \\underbrace{\\grad{\\tilde{g}_\\vlam}{\\vL}}_{\\tilde{\\bm \\alpha}: (1 \\times m)} \\cdot \\underbrace{\\grad{\\vL}{\\vtheta}}_{\\mB: (m \\times n)} \\cdot \\underbrace{\\grad{\\vtheta}{\\vphi}}_{\\mC: (n \\times D)}.\n    \\label{eqn:grad:psl}\n\\end{equation}\n\nEmpirically, the expectation involved in \\Cref{eqn:grad:psl} is estimated using a mini-batch of \\(K\\) preferences. The gradient vector \\(\\tilde{\\bm \\alpha} =(\\tilde{\\alpha}_1, \\ldots, \\tilde{\\alpha}_m)\\) is computed by MOO solvers as introduced in the previous section. The gradient matrix \\(\\mB\\) can be calculated either via backpropagation (when gradients are easily obtained) or using a zero-order method such as ES. \\(\\mC\\) can always be estimated through backward propagation, since \\(\\vtheta\\) is a continuous vector function of \\(\\vphi\\). Gradient calculations in existing PSL methods is summarized in \\Cref{tab:psl_comparison}. Parameter \\(\\vphi\\) is iteratively updated by gradient descent: \\(\\vphi \\xleftarrow{} \\vphi - \\eta \\frac{\\partial \\ell_\\mathrm{psl}}{\\partial \\vphi}\\).\n\n\\subsection{Multiobjective Bayesian optimization solvers}\n\\label{sec:mobod}\nWhen the evaluation of objective functions is costly, multiobjective Bayesian optimization (MOBO) is often the preferred methodology for tackling such challenges. While there are several existing libraries, such as BoTorch~\\cite{balandat2020botorch} and HEBO~\\cite{Cowen-Rivers2022-HEBO}, that facilities MOBO, they largely overlook algorithms that leverage decomposition techniques like PSL. To bridge this gap, \\algoname~also includes three decomposition-based MOBO algorithms, including PSL-MOBO~\\cite{lin2022pslmobo}, PSL-DirHVEI~\\cite{lin2022pslmobo,zhao2024hypervolume}, and DirHV-EGO~\\cite{zhao2024hypervolume}. In each iteration, these methods build Gaussian process (GP) models for each objectives and generate a batch of query points for true function evaluations.\n\nPSL-MOBO is an extension of PSL method for expensive MOPs. It optimizes the preference-conditional acquisition function (AF) $\\alpha(\\vtheta|\\vlam)$ over an infinite number of preference vectors to generate a set of promising candidates:\n$\n   \\min_\\vphi \\ell_\\mathrm{psl} = \\E_{\\vlam \\sim \\mathrm{Dir}(\\bm p)} \\mbr{\\alpha(\\vtheta_\\vphi(\\vlam)|\\vlam)}, \\vtheta_\\vphi(\\vlam) : \\mDelta_m \\mapsto\\R^N.\n$\n \\begin{wraptable}{r}{0.6\\textwidth}\n     \\centering\n     \\setlength\\tabcolsep{1.5 pt}\n     \\caption{\\textbf{Supported preference-conditional AFs}.} \\label{tab:mobod:acquisition}\n     \\small\n     \\begin{tabular}{c|c}\n     \\toprule\n      & preference-conditional AFs  \\\\ \\midrule\n      TLCB & $\\alpha_\\mathrm{{TLCB}}(\\vtheta|\\vlam)=\\underset{i \\in [m]}{\\max}\\{\\lambda_i(\\hat{\\mu}_i(\\vtheta)-\\beta\\hat{\\sigma}_i(\\vtheta)-z_i^*)\\}$  \\\\\n         DirHV-EI &  $\\alpha_\\mathrm{{DirHVEI}}(\\vtheta | \\vlam) =\\mathbb{E}_{\\vy\\sim p(\\vy | \\vtheta,\\mathcal{D})} \\mbr{ \\prod_{i=1}^{m} [\\xi_i-y_i]_{+} }$ \\\\\n     \\bottomrule\n\\end{tabular}\n\\vspace{-9pt}\n\\end{wraptable}\n\nCurrently, our library supports two representative preference-conditional AFs: the Tchebycheff scalarization of lower confidence bound (TLCB)~\\cite{lin2017cec,lin2022pslmobo,paria2020flexible} and the expected direction-based hypervolume improvement (DirHV-EI)~\\cite{zhao2024hypervolume}. We note that DirHV-EI can be regarded as an unbiased estimation of a weighted expected hypervolume improvement. Our library also supports DirHV-EGO, which employs a finite set of predetermined reference vectors as in~\\cite{zhang2007moea}.\n\n\\section{Empirical studies} \\label{sec:results}\nIn this section, we present the empirical results of \\algoname. The experiments were conducted on a personal computer with an i7-10700 CPU and a 10GB RTX3080 GPU. GPU acceleration analysis was performed using RTX3080, 4060, and 4090. The results cover six areas: MOO solvers for synthetic problems, Pareto set learning solvers for synthetic problems, MOO solvers for MTL problems, Pareto set learning solvers for MTL problems, MOBO for synthetic problems, and GPU acceleration (details in \\Cref{sec:gpu}).\n\n\\subsection{MOO solvers for synthetic problems}\nWe report the performance of various finite Pareto solution solvers using the VLMOP2 problem, where $f_1(\\vtheta) = 1 - \\exp (-\\sum_{i=1}^{n} (\\theta_i - \\frac{1}{\\sqrt{n}} )^2 )$, $f_2(\\vtheta) = 1 - \\exp (-\\sum_{i=1}^{n} (\\theta_i + \\frac{1}{\\sqrt{n}})^2)$. VLMOP2 has been widely studied in the literature~\\cite{mahapatra2020multi,lin2019pareto} since its PF is non-convex~\\footnote{In MOO, a PF is convex or non-convex based on whether the objective space is convex or non-convex. The PF represents the non-dominated boundary of the objective space.}. Some methods (e.g., Agg-LS) fail immediately on this problem since its PF is non-convex. Visualization results are shown by \\Cref{fig:finit_synthetic} and the numerical results are shown by \\Cref{tab:finite_synthetic}. We present the following key findings:\n\n\\begin{figure} %\n    \\centering%52 44\n    \\subfloat[Agg-COSMOS]{\\includegraphics[width= \\pentawidth \\textwidth]{FigureRebuttal/discrete/VLMOP2/agg_cosmos/res.pdf}}\n    \\subfloat[Agg-LS]{\\includegraphics[width= \\pentawidth \\textwidth]{FigureRebuttal/discrete/VLMOP2/agg_ls/res.pdf}} \n    \\subfloat[Agg-PBI]{\\includegraphics[width= \\pentawidth \\textwidth]{FigureRebuttal/discrete/VLMOP2/agg_pbi/res.pdf}}\n    \\subfloat[Agg-SmoothTche]{\\includegraphics[width= \\pentawidth \\textwidth]{FigureRebuttal/discrete/VLMOP2/agg_softtche/res.pdf}}\n    \\subfloat[Agg-Tche]{\\includegraphics[width= \\pentawidth \\textwidth]{FigureRebuttal/discrete/VLMOP2/agg_tche/res.pdf}} \\\\\n    \\vspace{-10pt}\n    \\subfloat[EPO]{\\includegraphics[width= \\pentawidth \\textwidth]{FigureRebuttal/discrete/VLMOP2/epo/res.pdf}} \n    \\subfloat[HVGrad]{\\includegraphics[width= \\pentawidth \\textwidth]{FigureRebuttal/discrete/VLMOP2/hvgrad/res.pdf}}\n    \\subfloat[MGDA-UB]{\\includegraphics[width= \\pentawidth \\textwidth]{FigureRebuttal/discrete/VLMOP2/mgdaub/res.pdf}}\n    \\subfloat[MOO-SVGD]{\\includegraphics[width= \\pentawidth \\textwidth]{FigureRebuttal/discrete/VLMOP2/moosvgd/res.pdf}}\n    \\subfloat[PMTL]{\\includegraphics[width= \\pentawidth \\textwidth]{FigureRebuttal/discrete/VLMOP2/pmtl/res.pdf}}\n    \\\\\n    \\caption{Finite Pareto solutions by ten MOO solvers on VLMOP2 problem.} \\label{fig:finit_synthetic}\n\\end{figure}\n\n\\begin{table}[ht]\n\\setlength\\tabcolsep{2.0 pt}\n\\centering\n\\tiny\n\\begin{threeparttable}\n\\caption{Numerical results of finite Pareto solutions for the VLMOP2 problem.}\n\\label{tab:finite_synthetic}\n    \\begin{tabular}{l|l|l|l|l|l|l|l|l}\n    \\toprule\n    Method & $l_\\mathrm{min}$ & $sl_\\mathrm{min}$ & Spacing & Sparsity & HV & IP & Cross Angle & PBI \\\\\n    \\midrule\n    EPO & 0.162 (0.000) & 0.061 (0.000) & 0.029 (0.001) & 0.043 (0.000) & 0.283 (0.000) & 0.776 (0.000) & \\textbf{0.046 (0.041)} & \\textbf{0.930 (0.003)} \\\\\n    MGDA-UB & 0.012 (0.013) & -0.098 (0.011) & 0.036 (0.011) & \\textbf{0.006 (0.001)} & 0.228 (0.008) & 0.606 (0.010) & 31.278 (1.533) & 2.986 (0.088) \\\\\n    PMGDA & 0.150 (0.001) & 0.055 (0.000) & 0.034 (0.001) & 0.042 (0.000) & 0.283 (0.000) & 0.775 (0.000) & 0.318 (0.037) & 0.952 (0.003) \\\\\n    Random & 0.000 (0.000) & -0.161 (0.002) & \\textbf{0.000 (0.000)} & 0.272 (0.000) & 0.044 (0.000) & 0.410 (0.127) & 52.290 (12.938) & 3.894 (0.590) \\\\\n    MOO-SVGD & 0.060 (0.002) & -0.077 (0.004) & 0.033 (0.018) & 0.009 (0.003) & 0.212 (0.003) & 0.633 (0.024) & 29.647 (3.305) & 2.963 (0.197) \\\\\n    PMTL & 0.014 (0.010) & -0.068 (0.009) & 0.061 (0.020) & 0.018 (0.007) & 0.260 (0.012) & 0.706 (0.004) & 15.036 (1.270) & 1.993 (0.093) \\\\\n    HVGrad & \\textbf{0.182 (0.000)} & \\textbf{0.067 (0.000)} & 0.016 (0.000) & 0.041 (0.000) & \\textbf{0.286 (0.000)} & 0.578 (0.069) & 34.090 (8.607) & 3.062 (0.465) \\\\\n    Agg-LS & 0.000 (0.000) & -0.159 (0.001) & 0.002 (0.001) & 0.272 (0.001) & 0.043 (0.002) & \\textbf{0.227 (0.008)} & 71.168 (0.958) & 4.764 (0.047) \\\\\n    Agg-Tche & 0.158 (0.001) & 0.061 (0.000) & 0.031 (0.001) & 0.043 (0.000) & 0.283 (0.000) & 0.348 (0.000) & 55.174 (0.049) & 3.889 (0.004) \\\\\n    Agg-PBI & 0.113 (0.074) & 0.032 (0.046) & 0.045 (0.030) & 0.042 (0.002) & 0.281 (0.002) & 0.657 (0.097) & 11.374 (9.125) & 1.434 (0.402) \\\\\n    Agg-COSMOS & 0.141 (0.000) & 0.045 (0.000) & 0.035 (0.000) & 0.039 (0.000) & 0.285 (0.000) & 0.771 (0.000) & 1.085 (0.000) & 1.011 (0.000) \\\\\n    Agg-SmoothTche & 0.004 (0.000) & -0.074 (0.000) & 0.154 (0.001) & 0.074 (0.000) & 0.244 (0.000) & 0.276 (0.000) & 63.106 (0.018) & 4.253 (0.001) \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\begin{tablenotes}\n    \\item All methods were run five times with random seeds; results are presented in the format of (mean)(std).\n    \\end{tablenotes}\n\\end{threeparttable}\n\\end{table}\n\n\\textcircled{1}. Agg-COSMOS produces rough `exact' Pareto solutions due to a cosine similarity term encouraging Pareto objectives to be close to preference vectors. However, the position of Pareto objectives can not be determined. Agg-LS can only find two endpoints of the PF. \n\\textcircled{2}. Agg-PBI generates ``exact'' Pareto solutions when the coefficient of \\( d_2 \\) exceeds a specific value~\\cite{wang2021parameter}. However, this parameter is challenging to tune, which is influenced by the curvature of a PF. Additionally, PBI may transform a convex multi-objective optimization problem into a non-convex one, making Agg-PBI less recommended.\n\\textcircled{3}. Agg-Tche generates diverse solutions and produces exact Pareto solutions corresponding to the element-wise inverse of the preference vector. Both Agg-Tche and Agg-SmoothTche retain convexity - their aggregation functions keep convex when all objectives are convex.\n\\textcircled{4}. HVGrad updates the decision variable using the hypervolume gradient, resulting in the largest hypervolume.\nPMTL is a two-stage method. In the first stage, solutions are updated to specific regions and in the second stage, solutions are updated to Pareto solutions constrained in these specific regions. MOO-SVGD's update direction has two conflicting goals: promoting diversity and ensuring convergence. This conflict makes MOO-SVGD less stable and take more iterations to converge. \n\\textcircled{5}. Among these methods, MGDA-UB, Random, Agg-PBI, and MOO-SVGD exhibit relatively large deviations. MGDA-UB and Random generate arbitrary Pareto solutions due to their computation nature. Agg-PBI results in non-convex aggregation functions, leading to variable solutions based on different initializations.\n\n\\textbf{Conclusion.} For synthetic problems, the most recommended method is \\emph{Agg-Tche} since (1) it keeps convexity, (2) it finds `exact' Pareto solutions under quite mild conditions (3) it does not need to calculate the Jacobian matrix for each iteration.\n\n\\subsection{Pareto set learning on synthetic problems}\nIn this section, we present PSL results (\\Cref{fig:psl_synthetic,tab:psl_synthetic}) also using VLMOP2. We have:\n\n\\begin{table}[]\n\\setlength\\tabcolsep{2.0 pt}\n\\centering\n\\scriptsize\n\\caption{Pareto set learning results on VLMOP2 problem.} \\label{tab:psl_synthetic}\n\\tiny \n    \\begin{tabular}{l|l|l|l|l|l|l|l|l|l}\n    \\toprule\n    Method & $l_\\mathrm{min}$ & $sl_\\mathrm{min}$ & Spacing           & Sparsity          & HV                & IP                & Cross Angle       & PBI               & Span              \\\\\n    \\midrule\n    COSMOS & 0.045 (0.000)     & -0.127 (0.000)     & 1.560 (0.004)     & \\textbf{0.525 (0.000)} & 0.318 (0.000)     & 0.752 (0.000)     & 0.950 (0.001)     & 0.995 (0.000)     & 0.907 (0.000)     \\\\\n    Agg-LS & 0.000 (0.000)     & -0.258 (0.000)     & \\textbf{0.115 (0.094)} & 13.245 (1.871)    & 0.045 (0.003)     & \\textbf{0.239 (0.001)} & 70.541 (0.156)    & 4.811 (0.007)     & \\textbf{0.982 (0.000)} \\\\\n    Agg-Tche & 0.047 (0.004)     & -0.121 (0.000)     & 1.476 (0.146)     & 0.579 (0.008)     & 0.319 (0.000)     & 0.383 (0.003)     & 51.558 (0.280)    & 3.775 (0.011)     & 0.955 (0.005)     \\\\\n    SmoothTche & 0.000 (0.000)     & -0.187 (0.000)     & 6.711 (0.003)     & 1.060 (0.000)     & 0.302 (0.000)     & 0.300 (0.000)     & 60.386 (0.001)    & 4.169 (0.000)     & 0.982 (0.000)     \\\\\n    EPO & \\textbf{0.050 (0.001)} & \\textbf{-0.120 (0.000)} & 1.332 (0.078)     & 0.583 (0.005)     & 0.319 (0.000)     & 0.756 (0.000)     & 0.388 (0.098)     & 0.952 (0.008)     & 0.961 (0.003)     \\\\\n    PMGDA & 0.047 (0.000)     & -0.121 (0.000)     & 1.446 (0.051)     & 0.580 (0.002)     & \\textbf{0.319 (0.000)} & 0.756 (0.000)     & \\textbf{0.215 (0.062)} & \\textbf{0.939 (0.005)} & 0.958 (0.001) \\\\\n    \\bottomrule\n    \\end{tabular}\n\\end{table}\n\n\\textcircled{1}. PSL with the COSMOS aggregation function fails to find all marginal Pareto solutions because COSMOS does not guarantee the discovery of the entire PS/PF. PSL with linear scalarization function could not fit the two endpoints of the PF. Those PSL results inherit from their base MOO solvers.\n\\textcircled{2}. PSL with the smooth Tchebycheff function finds diverse but non-exact Pareto solutions. In contrast, PSL with Agg-Tche, EPO, and PMGDA as base solvers discovers the entire PS/PF, as all three methods find exact Pareto solutions. By traversing the preference simplex, the model accurately fits the entire PS.\n\n\\textbf{Conclusion.} The most recommended method is still \\emph{Agg-Tche-based PSL} since its basic MOO solver Agg-Tche has attractive properties as mentioned in the previous section. \n\n\\begin{figure}%\n    \\centering%52 44\n    \\subfloat[COSMOS]{\\includegraphics[width= \\sixwidth \\textwidth]{FigureRebuttal/psl/VLMOP2/agg_cosmos/res.pdf}}\n    \\subfloat[Agg-LS]{\\includegraphics[width= \\sixwidth \\textwidth]{FigureRebuttal/psl/VLMOP2/agg_ls/res.pdf}} \n    \\subfloat[SmoothTche]{\\includegraphics[width= \\sixwidth \\textwidth]{FigureRebuttal/psl/VLMOP2/agg_softtche/res.pdf}}\n    \\subfloat[Tche]{\\includegraphics[width= \\sixwidth \\textwidth]{FigureRebuttal/psl/VLMOP2/agg_tche/res.pdf}}\n    \\subfloat[EPO]{\\includegraphics[width= \\sixwidth \\textwidth]{FigureRebuttal/psl/VLMOP2/epo/res.pdf}} \n    \\subfloat[PMGDA]{\\includegraphics[width= \\sixwidth \\textwidth]{FigureRebuttal/psl/VLMOP2/pmgda/res.pdf}}\n    \\\\\n    \\caption{Predicted Pareto solutions by different PSL solvers on VLMOP2 problem.} \\label{fig:psl_synthetic}\n\\end{figure}\n\n\\subsection{MOO solvers for MTL problems}\nWe evaluate the performance of finite Pareto solvers on the Adult dataset, a multitask fairness classification problem. The decision variable $\\vtheta$ represents the parameters of a fully-connected neural network with $|\\vtheta|=28033$. The first objective is cross-entropy loss, and the second is the DEO loss~\\cite{ruchte2021scalable}[Eq. 6]. Key findings from the \\Cref{fig:finit_mtl,tab:finite_mtl} are as follows:\n\n\\textcircled{1}. Agg-LS has two drawbacks: (1) it cannot identify the non-convex part of a PF (as previous section mentioned), and (2) the relationship between preference vectors and Pareto objectives is unknown; different preference vectors may yield duplicate solutions. Agg-PBI and Agg-COSMOS only find a small portion of the PF. \n\\textcircled{2}. Agg-Smooth mTche and Agg-mTche perform well on this task, as they can find (approximate) `exact' Pareto solutions. Once the Pareto front range is known, diverse solutions can be easily found using uniform preference vectors. The Random and MGDA-UB methods only find a single Pareto solution, since the position of this solution cannot be controlled by these methods. \n\\textcircled{3}. Among the three methods that directly find a set of Pareto solutions (MOO-SVGD, PMTL, and HV-Grad), HV-Grad produces the most diverse solutions with the largest hypervolume. PMTL, being a two-stage method, may fail when solutions fall outside the sector due to stochastic factors. MOO-SVGD optimizes both convergence and diversity but is generally unstable based on our tests.\n\n\\textbf{Conclusion.} For convex Pareto fronts in MTL problems, \\emph{Agg-LS} is recommended for computational efficiency. However, \\emph{PMGDA} or \\emph{EPO} may offer better convergence and preference-solution correspondence. \n\n\\begin{figure} %\n    \\centering %52 44\n    \\subfloat[Agg-COSMOS]{\\includegraphics[width=\\pentawidth \\textwidth]{FigureRebuttal/discrete/adult/agg_cosmos/res.pdf}}\n    \\subfloat[Agg-LS]{\\includegraphics[width= \\pentawidth \\textwidth]{FigureRebuttal/discrete/adult/agg_ls/res.pdf}} \n    \\subfloat[Agg-PBI]{\\includegraphics[width= \\pentawidth \\textwidth]{FigureRebuttal/discrete/adult/agg_pbi/res.pdf}}\n    \\subfloat[SmoothmTche]{\\includegraphics[width= \\pentawidth  \\textwidth]{FigureRebuttal/discrete/adult/agg_softmtche/res.pdf}}\n    \\subfloat[Agg-mTche]{\\includegraphics[width= \\pentawidth  \\textwidth]{FigureRebuttal/discrete/adult/agg_mtche/res.pdf}}\n    \\\\\n    \\vspace{-10pt}\n    \\subfloat[EPO]{\\includegraphics[width= \\pentawidth \\textwidth]{FigureRebuttal/discrete/adult/epo/res.pdf}} \n    \\subfloat[HVGrad]{\\includegraphics[width= \\pentawidth \\textwidth]{FigureRebuttal/discrete/adult/hvgrad/res.pdf}}\n    \\subfloat[MGDA-UB]{\\includegraphics[width= \\pentawidth \\textwidth]{FigureRebuttal/discrete/adult/mgdaub/res.pdf}}\n    \\subfloat[MOO-SVGD]{\\includegraphics[width= \\pentawidth \\textwidth]{FigureRebuttal/discrete/adult/moosvgd/res.pdf}}\n    \\subfloat[PMTL]{\\includegraphics[width= \\pentawidth \\textwidth]{FigureRebuttal/discrete/adult/pmtl/res.pdf}}\n    \\\\\n    \\caption{Finite Pareto solutions by different solvers on Adult problem.} \\label{fig:finit_mtl}\n\\end{figure}\n\n\\begin{table}[]\n\\centering\n\\setlength\\tabcolsep{2.0 pt}\n\\begin{threeparttable}\n\\caption{Numerical results of finite Pareto solutions for the Adult problem.}\n\\label{tab:finite_mtl}\n\\centering\n\\tiny\n\\begin{tabular}{l|l|l|l|l|l|l|l|l|l}\n\\toprule\nMethod                         & $l_\\mathrm{min}$ & $sl_\\mathrm{min}$          & Spacing           & Sparsity          & HV                & IP                & Cross Angle       & PBI               & Span              \\\\ \n\\midrule\nAgg-COSMOS                    & 0.004 (0.000)     & -0.194 (0.000)     & 0.463 (0.004)     & 0.014 (0.001)     & 0.657 (0.000)     & 0.295 (0.000)     & 2.787 (0.013)     & 0.426 (0.000)     & 0.052 (0.000)     \\\\ \nAgg-LS                        & 0.000 (0.000)     & -0.223 (0.000)     & \\textbf{0.016 (0.005)} & 0.000 (0.000)     & 0.636 (0.001)     & \\textbf{0.272 (0.001)} & 6.595 (0.028)     & 0.500 (0.001)     & 0.002 (0.000)     \\\\ \nAgg-PBI                       & 0.001 (0.000)     & -0.216 (0.000)     & 0.107 (0.008)     & 0.001 (0.000)     & 0.642 (0.000)     & 0.277 (0.001)     & 4.995 (0.007)     & 0.462 (0.001)     & 0.010 (0.000)     \\\\ \nAgg-SmoothmTche                 & 0.005 (0.001)     & -0.163 (0.000)     & 4.237 (0.029)     & 0.329 (0.003)     & 0.675 (0.001)     & 0.347 (0.001)     & 3.385 (0.053)     & 0.500 (0.002)     & 0.072 (0.000)     \\\\ \nAgg-mTche                     & 0.002 (0.000)     & -0.177 (0.000)     & 2.034 (0.072)     & 0.091 (0.004)     & 0.674 (0.000)     & 0.316 (0.000)     & \\textbf{1.962 (0.045)} & \\textbf{0.422 (0.001)} & 0.067 (0.001)     \\\\ \nEPO                          & 0.002 (0.000)     & -0.175 (0.000)     & 2.136 (0.054)     & 0.101 (0.004)     & 0.674 (0.001)     & 0.320 (0.000)     & 2.002 (0.025)     & 0.426 (0.001)     & 0.066 (0.000)     \\\\ \nMGDA-UB                      & 0.000 (0.000)     & -0.222 (0.001)     & 0.050 (0.039)     & 0.000 (0.000)     & 0.510 (0.003)     & 0.410 (0.004)     & 9.586 (0.072)     & 0.878 (0.011)     & 0.001 (0.000)     \\\\ \nRandom                       & 0.000 (0.000)     & -0.224 (0.000)     & 0.040 (0.016)     & \\textbf{0.000 (0.000)} & 0.633 (0.001)     & 0.279 (0.000)     & 5.863 (0.010)     & 0.491 (0.000)     & 0.002 (0.000)     \\\\ \nPMTL                         & 0.002 (0.001)     & -0.176 (0.002)     & 2.236 (0.045)     & 0.156 (0.028)     & 0.617 (0.002)     & 0.372 (0.004)     & 7.039 (0.242)     & 0.675 (0.016)     & 0.035 (0.001)     \\\\ \nMOO-SVGD                     & \\textbf{0.049 (0.007)} & \\textbf{-0.079 (0.003)} & 5.382 (4.539)     & 2.660 (1.857)     & 0.548 (0.005)     & 0.657 (0.013)     & 8.354 (0.211)     & 1.274 (0.029)     & 0.065 (0.010)     \\\\ \nHVGrad                       & 0.014 (0.002)     & -0.153 (0.002)     & 1.347 (0.041)     & 0.110 (0.003)     & \\textbf{0.678 (0.001)} & 0.347 (0.004)     & 7.043 (0.260)     & 0.663 (0.017)     & \\textbf{0.075 (0.001)} \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tablenotes}\n    \\small\n    \\item All methods were run five times with random seeds; results are presented in the form of (mean)(std).\n\\end{tablenotes}\n\\end{threeparttable}\n\\end{table}\n\n\\subsection{Pareto set learning on MTL}\nThis section presents the Pareto set learning results on the MO-MNIST problem using a hypernet architecture. The Pareto set model was trained for 20 epochs, optimizing approximately 3.24M parameters, with the first and second objectives being the cross-entropy losses for the top-right and bottom-left images. EPO-based or PMGDA-based PSL is not very suitable for this task since manipulating gradient of 3.2M parameters is not efficient. \nEmpirically, for regression tasks, the PF shape is nearly convex. Therefore, Agg-LS is adequate to recover the whole PF. From \\Cref{fig:psl_mtl,tab:psl_mtl}, we have that Agg-LS significantly outperforms other methods, evidenced by the HV of Agg-LS is much larger than other methods. Furthermore, the training losses across all methods are almost stable after 40 epochs. From the figure, to further reduce the training loss, it needs much more computational resources. \\\\\n\\textbf{Conclusion.} For MTL problems with a convex PF, it is highly recommended to use \\emph{Agg-LS-based} PSL for the sake of computational efficiency.\n\n\\begin{figure}%\n    \\centering%52 44\n    \\subfloat[Agg-COSMOS]{\\includegraphics[width= \\pentawidth \\textwidth]{FigureRebuttal/psl/mnist/agg_cosmos/res.pdf}}\n    \\subfloat[Agg-LS]{\\includegraphics[width= \\pentawidth \\textwidth]{FigureRebuttal/psl/mnist/agg_ls/res.pdf}} \n    \\subfloat[Agg-SmoothTche]{\\includegraphics[width= \\pentawidth \\textwidth]{FigureRebuttal/psl/mnist/agg_softtche/res.pdf}}\n    \\subfloat[Agg-Tche]{\\includegraphics[width= \\pentawidth \\textwidth] {FigureRebuttal/psl/mnist/agg_tche/res.pdf}}     \\\\\n    \\vspace{-10pt}\n    \\subfloat[Agg-COSMOS]{\\includegraphics[width= \\pentawidth \\textwidth]{FigureRebuttal/psl/mnist/agg_cosmos/train_loss.pdf}}\n    \\subfloat[Agg-LS]{\\includegraphics[width= \\pentawidth \\textwidth]{FigureRebuttal/psl/mnist/agg_ls/train_loss.pdf}} \n    \\subfloat[Agg-SmoothTche]{\\includegraphics[width= \\pentawidth \\textwidth]{FigureRebuttal/psl/mnist/agg_softtche/train_loss.pdf}}\n    \\subfloat[Agg-Tche]{\\includegraphics[width= \\pentawidth \\textwidth] {FigureRebuttal/psl/mnist/agg_tche/train_loss.pdf}} \n    \\caption{Training process for generating predicted Pareto solutions using different PSL solvers on MO-MNIST problem.} \\label{fig:psl_mtl}\n\\end{figure}\n\n\\begin{table}[]\n\\tiny\n\\caption{Pareto set learning results on MO-MNITS problem.}\n\\label{tab:psl_mtl}\n\n\\setlength\\tabcolsep{2.0 pt}\n\\centering\n\\begin{tabular}{l|l|l|l|l|l|l|l|l|l}\n\\toprule\nMethod & $l_\\mathrm{min}$ & $sl_\\mathrm{min}$ & Spacing & Sparsity & HV                & IP & Cross Angle & PBI & Span \\\\\n\\midrule\nCOSMOS                     & \\textbf{0.033 (0.002)}     & \\textbf{-0.152 (0.007)}     & 0.967 (0.202)              & 0.278 (0.038)              & 0.512 (0.015)              & 0.535 (0.028)              & \\textbf{9.208 (0.541)}     & 1.221 (0.050)              & \\textbf{0.497 (0.043)}     \\\\ \nAgg-LS                         & 0.001 (0.000)              & -0.286 (0.002)              & 0.068 (0.029)              & 0.000 (0.000)              & \\textbf{0.557 (0.004)}     & \\textbf{0.257 (0.004)}     & 27.536 (0.116)             & \\textbf{1.149 (0.021)}     & 0.016 (0.003)              \\\\ \nAgg-PBI                        & 0.000 (0.000)              & -0.295 (0.000)              & \\textbf{0.019 (0.006)}     & \\textbf{0.000 (0.000)}     & 0.536 (0.008)              & 0.269 (0.005)              & 26.270 (0.117)             & 1.150 (0.019)              & 0.002 (0.001)              \\\\ \nSmoothTche                   & 0.001 (0.000)              & -0.248 (0.002)              & 0.440 (0.040)              & 0.012 (0.001)              & 0.538 (0.008)              & 0.281 (0.005)              & 32.244 (0.417)             & 1.471 (0.007)              & 0.087 (0.012)              \\\\ \nAgg-Tche                       & 0.000 (0.000)              & -0.235 (0.004)              & 0.988 (0.110)              & 0.045 (0.015)              & 0.533 (0.010)              & 0.292 (0.008)              & 35.292 (0.742)             & 1.693 (0.075)              & 0.131 (0.014) \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n\\subsection{MOBO for synthetic and real-world problems}\nIn this section, we test three MOBO algorithms in \\algoname~on three benchmark problems, including ZDT1, RE21, VLMOP1 and VLMOP2. To ensure a fair comparison, we generate $11n-1$ initial samples using Latin Hypercube Sampling for each method. The maximum number of function evaluations is set as 200.\nOur experimental results, illustrated in Figure~\\ref{fig:psl_synthetic}, clearly demonstrate the rapid convergence capabilities of all three methods. DirHV-EGO, PSL-DirHV-EI, and PSL-MOBO not only efficiently navigate the solution space but also quickly reach optimal solutions. This highlights the robustness and effectiveness of our implemented algorithms in handling different types of MOPs.\n\n\\begin{figure}%\n    \\centering%52 44\n    \\subfloat[ZDT1]{\\includegraphics[width= \\quadwid \\textwidth]{Figure/mobo/ZDT1.pdf}}\n    \\subfloat[RE21]{\\includegraphics[width= \\quadwid \\textwidth]{Figure/mobo/RE21.pdf}} \n    \\subfloat[VLMOP1]{\\includegraphics[width= \\quadwid \\textwidth]{Figure/mobo/VLMOP1.pdf}}\n    \\subfloat[VLMOP2]{\\includegraphics[width= \\quadwid \\textwidth]{Figure/mobo/VLMOP2.pdf}}\n    \\\\\n    \\caption{HV curves on MOBO problem. Results are averaged on five random seeds. Reference point to calculate HV : [1.2, 1.2].} \\label{fig:mobo}\n\\end{figure}\n\n\\section{Conclusion, limitations, and further works} \\label{sec:conclusion}\n\\paragraph{Conclusion.}\nWe introduced the \\textit{first} modern gradient-based MOO framework called \\algoname~in PyTorch for the research community's convenience. \n\\algoname~supports more than 20 mainstream gradient-based MOO methods;\nthe modular design of \\algoname~further allows the library to address various MOPs via various methods in a plug-and-play manner.\n\\algoname~can thus be leveraged to quickly yet robustly test new MOO ideas.\n\n\\paragraph{Limitations} include: (1) rapid developments of gradient-based MOO methods makes it hard to incorporate all methods, so some effective methods may be missing; (2) gradient-based solvers may fail for problems with a number of local optimas. \n\n\\paragraph{Future Work} includes (1) maintaining a user and development community to address issues and (2) adding newly published methods as quickly as possible.\n\n\\clearpage\n\\section*{Acknowledge}\n\\algoname~was made possible with the valuable feedback and comments from a lot of researchers. We appreciate the help from Hongzong Li (for some advice on skipping local optima), Xuehai Pan (for some software design advice), Weiyu Chen (for his code on LoRA PSL), Baijiong Lin (for his advice on automatic code testing), Prof. Hao Wang (for early email communication of HVGrad), Prof. Jingda Deng (for his advice on high-D HV calculation), Prof. Ke Shang (HV computation and RE test suit), Prof. Genghui Li, Prof. Zhichao Lu and Prof. Zhenkun Wang.\n\nThe work described in this paper was supported by the Research Grants Council of the Hong Kong Special Administrative Region, China [GRF Project No. CityU 11215622].\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-1912.12854v1.tex",
        "arXiv-2102.04523v2.tex",
        "arXiv-2409.02969v3.tex"
    ],
    "group_id": "group_17",
    "response": "### Title: Pareto Multi-Task Learning and Gradient-Based Multi-Objective Optimization: A Comparative Analysis\n\n### Introduction\nMulti-Task Learning (MTL) and Multi-Objective Optimization (MOO) are two prominent areas in machine learning that have seen significant advancements in recent years. MTL aims to improve the performance of multiple related tasks simultaneously by exploiting shared structures and information among them, while MOO focuses on optimizing multiple conflicting objectives to find a set of Pareto optimal solutions that represent different trade-offs among these objectives. Historically, MTL has often been approached using linear weighted scalarization, where a single surrogate task is created by combining all tasks into a weighted sum. However, this method is limited as it requires manual assignment of weights, and the optimal weights are not always straightforward to determine, especially when tasks are conflicting. MOO, on the other hand, has traditionally been addressed using evolutionary algorithms, which are population-based and do not scale well with the number of decision variables, particularly in the context of deep learning and large-scale problems.\n\nIn recent times, there has been a resurgence of interest in gradient-based MOO methods, which leverage the gradient information available in deep learning tasks to find Pareto optimal solutions more efficiently. These methods have shown promise in various applications, including multi-task learning, where they can generate solutions that span the Pareto front without requiring a priori knowledge of the trade-offs. However, the challenge remains in generating a diverse set of Pareto solutions that can represent different trade-offs among tasks, especially in scenarios where the Pareto front is asymmetric or non-convex.\n\nThis summary focuses on three research papers that explore different aspects of Pareto Multi-Task Learning (Pareto MTL) and gradient-based MOO methods. The first paper introduces a novel Pareto MTL algorithm to find a set of well-distributed Pareto solutions for a given MTL problem, enabling practitioners to select their preferred solutions. The second paper proposes a method for multi-objective training of neural networks to predict Pareto fronts using hypervolume maximization, highlighting the benefits of this approach over existing methods, particularly in scenarios with asymmetric Pareto fronts. The third paper presents a gradient-based MOO library called LibMOON, which supports over twenty state-of-the-art methods and is designed to facilitate benchmarking and industrial usage of gradient-based MOO techniques.\n\n### Main Content of Each Paper\n\n#### Paper 1: Pareto Multi-Task Learning\nThe first paper introduces a novel Pareto Multi-Task Learning (Pareto MTL) algorithm to address the challenge of finding a set of well-distributed Pareto solutions for a multi-task learning problem. The authors propose a method to decompose the MTL problem into multiple subproblems with different trade-off preferences among the tasks. Each subproblem is solved in parallel, generating a set of Pareto optimal solutions that can represent different trade-offs among all tasks. The key innovation lies in the decomposition of the MTL problem into subproblems with preference vectors, allowing for the generation of diverse solutions without the need for exhaustive weight searches.\n\nThe paper formulates the MTL problem as a multi-objective optimization problem and decomposes it into subproblems with different preference vectors. The subproblems are defined as:\n\\begin{align*}\n    \\min_{\\theta} \\ \\mathcal{L}(\\theta) = (\\mathcal{L}_1(\\theta),\\mathcal{L}_2(\\theta),\\cdots,\\mathcal{L}_m(\\theta))^{\\mathrm{T}}, \\quad \\text{s.t.} \\quad \\mathcal{L}(\\theta) \\in \\Omega_k,\n\\end{align*}\nwhere $\\Omega_k$ is a subregion in the objective space defined by the preference vector $\\vu_k$. The authors also show that the Pareto MTL algorithm can be reformulated as a linear scalarization approach with dynamically adaptive weights, providing a deeper understanding of its relationship with other MTL algorithms.\n\n#### Paper 2: Multi-Objective Learning to Predict Pareto Fronts Using Hypervolume Maximization\nThe second paper proposes a novel approach for multi-objective training of neural networks to approximate the Pareto front during inference. The authors use a dynamic loss function wherein each network's losses are weighted by their hypervolume maximizing gradients. This method aims to generate a set of well-distributed Pareto solutions for each sample without requiring a priori specification of trade-off vectors. The paper highlights the importance of optimizing trade-offs for each sample rather than for average losses, especially in scenarios with asymmetric Pareto fronts.\n\nThe approach is based on the hypervolume (HV) metric, which is popular for comparing the quality of different sets of solutions approximating the Pareto front. The HV maximization approach is formulated as:\n\\begin{align*}\n    \\max_{\\theta} \\frac{1}{|S|}\\sum_{k=1}^{|S|}\\mathrm{HV}\\left(\\mathfrak{L}(\\Theta,s_{k})\\right),\n\\end{align*}\nwhere $\\mathfrak{L}(\\Theta,s_{k})$ is the stacked loss vector for a set of networks $\\Theta$ on a sample $s_{k}$. The authors demonstrate the effectiveness of their approach through experiments on various MO problems, including a simple MO regression example, multi-observer medical image segmentation, and neural style transfer.\n\n#### Paper 3: LibMOON: A Gradient-Based MultiObjective OptimizatioN Library in PyTorch\nThe third paper introduces LibMOON, a gradient-based MultiObjective Optimization (MOO) library in PyTorch, designed to support over twenty state-of-the-art (SOTA) methods for Pareto set learning (PSL) and MultiObjective Bayesian Optimization (MOBO). The library is modular and allows for easy integration of new methods, making it a valuable tool for researchers and practitioners in the field of MOO. LibMOON supports synthetic problems, real-world problems, and MO machine learning problems such as fair classification and multitask learning.\n\nLibMOON supports a variety of MOO solvers, including aggregation-based methods like Linear Scalarization (LS), Tchebycheff (Tche), and COSMOS, as well as gradient manipulation-based methods like EPO, HVGrad, and MGDA-UB. The library also includes Pareto set learning solvers that train a single model to approximate the entire Pareto set, and MOBO solvers that are designed for expensive MOPs. The authors present empirical results on various problems, including synthetic and real-world MTL problems, demonstrating the effectiveness and efficiency of different methods.\n\n### Commonalities and Innovations\nAll three papers address the challenge of finding Pareto optimal solutions for multi-objective problems, but they approach the problem from different angles. Paper 1 and Paper 2 both focus on generating a set of well-distributed Pareto solutions for multi-task learning problems, while Paper 3 introduces a library to support various gradient-based MOO methods.\n\n**Commonalities:**\n- All papers utilize gradient information to optimize multiple objectives simultaneously.\n- They all aim to generate a set of Pareto optimal solutions that can represent different trade-offs among tasks.\n- The use of hypervolume (HV) as a metric for evaluating the quality of Pareto front approximations is common in both Paper 2 and Paper 3.\n\n**Innovations:**\n- Paper 1 introduces a novel decomposition-based approach to generate a set of well-distributed Pareto solutions for MTL problems.\n- Paper 2 proposes a method for training neural networks to predict Pareto fronts using hypervolume maximization, without requiring a priori specification of trade-off vectors.\n- Paper 3 presents a comprehensive library (LibMOON) that supports a wide range of gradient-based MOO methods, facilitating benchmarking and industrial usage.\n\n### Comparison of Results and Discussion\nThe experimental results from the three papers highlight the effectiveness of their respective approaches in generating well-distributed Pareto solutions for different types of multi-objective problems.\n\n**Paper 1:**\n- The authors demonstrate the effectiveness of their Pareto MTL algorithm on Multi-Fashion-MNIST and self-driving car localization tasks. The algorithm outperforms existing methods, especially in scenarios where tasks are less correlated.\n- In the Multi-Fashion-MNIST experiment, Pareto MTL generates solutions with different trade-offs, outperforming other methods.\n- For the self-driving car localization task, Pareto MTL provides solutions with different trade-offs, outperforming other MTL approaches.\n\n**Paper 2:**\n- The authors show that their hypervolume maximization approach is particularly effective in scenarios with asymmetric Pareto fronts, where existing methods relying on pre-specified trade-offs fail to cover the entire Pareto front.\n- In the MO regression experiment, the hypervolume maximization approach yields better results than fixed linear scalarizations and Pareto MTL, especially for asymmetric Pareto fronts.\n- For the neural style transfer problem, the hypervolume maximization approach generates diverse and interesting images, outperforming other methods in terms of hypervolume.\n\n**Paper 3:**\n- The authors present a detailed comparison of different MOO solvers on synthetic and real-world problems, highlighting the strengths and weaknesses of each method.\n- For synthetic problems, Agg-Tche is recommended due to its ability to find exact Pareto solutions and maintain computational efficiency.\n- For MTL problems, Agg-LS is recommended for convex Pareto fronts, while PMGDA or EPO may offer better convergence and preference-solution correspondence for non-convex fronts.\n- The library supports various metrics, including hypervolume, inverted general distance, and cross angle, providing a comprehensive evaluation framework for MOO methods.\n\n### Conclusion\nThe main findings from the three papers are:\n- Paper 1 introduces a novel Pareto MTL algorithm that can generate a set of well-distributed Pareto solutions for MTL problems, enabling practitioners to select their preferred solutions.\n- Paper 2 demonstrates the effectiveness of hypervolume maximization in generating well-distributed Pareto solutions for neural networks, particularly in scenarios with asymmetric Pareto fronts.\n- Paper 3 presents LibMOON, a comprehensive gradient-based MOO library that supports various methods and facilitates benchmarking and industrial usage.\n\nThese papers collectively advance the field of Pareto Multi-Task Learning and gradient-based Multi-Objective Optimization, providing new tools and methods for researchers and practitioners. Future research directions could include:\n- Investigating the application of Pareto MTL and hypervolume maximization in more complex and real-world scenarios, such as reinforcement learning and robotics.\n- Developing more efficient algorithms for hypervolume computation and gradient manipulation to handle large-scale problems with millions of parameters.\n- Enhancing LibMOON to support more advanced and recently proposed gradient-based MOO methods, and to improve its usability for non-expert users.\n\n### Acknowledgments\nThe research presented in these papers was supported by various grants and organizations, including the Natural Science Foundation of China, the Dutch Research Council (NWO), and the Research Grants Council of Hong Kong. The authors acknowledge the contributions of several researchers and developers in the creation and testing of their methods and libraries."
}