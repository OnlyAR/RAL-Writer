{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{\\uskg: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\n\nStructured knowledge grounding (SKG) leverages structured knowledge to complete user requests, such as semantic parsing over databases and question answering over knowledge bases. \nSince the inputs and outputs of SKG tasks are heterogeneous, they have been studied separately by different communities, which limits systematic and compatible research on \\skg. \nIn this paper, we overcome this limitation by proposing the \\uskg framework, which unifies \\ntasks SKG tasks into a text-to-text format, aiming to promote systematic SKG research, instead of being exclusive to a single task, \\mbox{domain}, or dataset.\nWe use \\uskg to benchmark T5 with different sizes and show that T5, with simple modifications when necessary, achieves state-of-the-art performance on almost all of the \\ntasks tasks. \nWe further demonstrate that multi-task prefix-tuning improves the performance on most tasks, largely improving the overall performance. \n\\uskg also facilitates the investigation of zero-shot and few-shot learning, and we show that T0, GPT-3, and Codex struggle in zero-shot and few-shot learning for SKG. \nWe also use \\uskg to conduct a series of controlled experiments on structured knowledge encoding variants across \\skg tasks. \n\\uskg is easily extensible to more tasks, and it is open-sourced at \\url{https://github.com/hkunlp/unifiedskg}.\\footnote{Latest collections at \\url{https://unifiedskg.com}.}\n\n\\end{abstract}\n\n\\section{Introduction}\n\nStructured knowledge (e.g., web tables, knowledge graphs, and databases) stores large amounts of data in organized structures, forming a basis for a wide range of applications, e.g., medical diagnosis, personal assistants, and customer relations management.\nAccessing and searching data in structured knowledge typically requires mastering query languages through professional training.\nTo promote the efficiency of data access, structured knowledge grounding (\\skg) systems ground user requests in structured knowledge and produce various outputs, including computer programs (e.g., SQL and SPARQL), table cell values, and natural language responses (Figure \\ref{fig:skg}). \nFor example, semantic parsing~\\cite{ZelleM96,Zettlemoyer05} converts natural language questions into formal programs;\nknowledge-base question answering \\cite{BerantCFL13} derives answers from tables or knowledge graphs.\n\n\\skg has attracted significant interest and has been studied through different tasks defined by different communities.\nRecent developments in tasks, models, and datasets for \\skg have led to task-specific modeling advances, making each task's progress seemingly unique and incompatible.\nA main reason is that \\skg tasks are \\textit{heterogeneous}.\nDifferent types of structured knowledge, such as databases or knowledge graphs, lead to highly specialized encoders~\\citep{kagnet-emnlp19,Herzig2020tapas,Wang2020RATSQLRS,yasunaga-etal-2021-qa}.\nSome SKG tasks, e.g., semantic parsing, use customized decoders to generate programs~\\cite{YinN18,ren2021lego}.\nTherefore, instead of solving common challenges in \\skg research, improvements in \\skg have been prone to be exclusive to a single task, domain, or dataset.\n\n\\begin{figure*}[!t]\n    \\centering\n    \\includegraphics[width=0.75\\linewidth]{./figures/unifiedskg.pdf}\n    \\caption{Structured knowledge grounding (SKG) leverages structured knowledge to complete user requests.\n    By casting inputs and outputs into the text-to-text format, \\uskg standardizes datasets, models, code, experiments, and metrics for \\ntasks \\skg tasks.\n    }\n\\label{fig:skg}\n\\end{figure*}\n\nIn this paper, we propose the \\uskg framework to advocate for a unifying view of \\ntasks \\skg tasks across six task families and multiple data domains (Table \\ref{tab:tasks-included}).\n\\uskg standardizes datasets, models, code, experiments, and evaluation metrics into a single framework.\nBy casting user requests, structured knowledge, and outputs into the text-to-text format \\cite{2020t5}, it promotes model advances where new tasks can be framed with our standardized abstraction, and new models can be easily applied to diverse \\skg tasks.\nWhile previous works also cast \\skg tasks into the text-to-text format \\cite{hosseini2020simple,Shaw2021CompositionalGA,liu2021tapex}, their independent choices of pretrained language models (PLMs), input-output formats, and frameworks make our unification non-trivial. \n\\uskg is easily extensible to more \\skg tasks, and it is open-sourced to promote community-wide progress.\n\nUsing \\uskg as a benchmark, we show that finetuning T5 (with constrained decoding or reranking when necessary) on individual tasks achieves state-of-the-art (sota) results on almost all of the \\ntasks tasks, establishing a powerful and reproducible starting point for \\skg research. T5 performance also increases with size on most tasks. \n\n\\uskg facilitates multi-task learning on \\skg, enabling knowledge sharing and cross-task generalization. \nAlthough simple multi-task learning has mixed results, we show that multi-task learning with prefix-tuning \\citep{li2021prefixtuning} benefits most tasks and largely improves the overall performance, on both T5-base and T5-large.\n\n\\uskg is a challenging testbed for few-shot \\cite{brown2020language,Ye2021CrossFitAF} and zero-shot learning \\cite{Zhong2021AdaptingLM,Jason2021,sanh2021multitask} with PLMs.\nOur experiments show that models like T0 \\cite{sanh2021multitask} struggle in zero-shot learning on \\skg tasks, and GPT-3 \\cite{brown2020language} and Codex \\cite{chen2021evaluating} struggle in few-shot learning on \\skg tasks.\n\n\\uskg enables a series of controlled experiments on structured knowledge encoding.\nWe find that T5 is sensitive to encoding variations, and the sensitivity varies across tasks.\n\\uskg aims to facilitate more general and robust structured knowledge encoding methods. \nFinally, we conduct a comprehensive error analysis across \\skg tasks.\nAlthough the errors made by PLMs decrease with the model size, T5-3B may still generate invalid outputs. \n\nIn summary, we 1) unify and benchmark 21 \\skg tasks under the \\uskg framework to evaluate diverse grounding goals and structured knowledge sources, 2) demonstrate (near) sota performance of T5 on all the unified \\skg tasks, using a single, general-purpose approach, 3) show the benefit of knowledge sharing across \\skg tasks via multi-task prefix-tuning, and 4) analyze recent modeling contributions (zero-shot, few-shot, and structured knowledge encoding) on these tasks. \nWe hope \\uskg enables the design of new models and learning algorithms that generalize to diverse \\skg tasks and to identify their challenges.\n\n\\section{Related Work}\n\\label{sec:related-work}\n\n\\begin{table*}[ht]\n    \\small\n\t\\centering\n\t\\begin{adjustbox}{width=0.96\\linewidth}\n\t\t\\begin{tabular}{@{}ccccc@{}}\n\t\t\t\\toprule\n\t\t\tTask Family & Task & Knowledge Input & User Input & Output \\\\\n\t\t\t\\midrule\n\t        \\multirow{4}*{\\textit{Semantic Parsing}} & Spider \\cite{Yu18c} & Database & Question & SQL \\\\\n            & GrailQA \\cite{gu2021beyond} & Knowledge Graph & Question & s-Expression \\\\\n            & WebQSP \\cite{yih-etal-2016-value} & Knowledge Graph & Question & s-Expression \\\\\n            & MTOP \\cite{li-etal-2021-mtop} & API Calls & Question &  TOP Representation\\\\\n\t\t\t\\midrule\n\t\t\t\\multirow{6}*{\\textit{Question Answering}} & WikiSQL \\cite{zhongSeq2SQL2017} & Table & Question & Answer \\\\\n            & WikiTQ \\cite{pasupat-liang-2015-compositional} & Table & Question & Answer \\\\\n            & CompWebQ \\cite{talmor18compwebq} & Knowledge Graph & Question & Answer \\\\\n            & HybridQA \\cite{chen2020hybridqa} & Table + Text Passage & Question & Answer \\\\\n            & MultiModalQA \\cite{talmor2021multimodalqa} & Table + Text + Image & Question & Answer \\\\\n            & FeTaQA \\cite{nan2021feta} & Table & Question & Free-Form Answer  \\\\\n\t\t\t\\midrule\n\t\t\t\\multirow{2}*{\\textit{Data-to-Text}} & DART \\cite{nan2021dart} & Triple & None & Text \\\\\n\t\t\t& ToTTo \\cite{parikh2020totto} & Highlighted Table & None & Text \\\\\n\t\t\t\\midrule \n\t\t\t& MultiWoZ \\cite{budzianowski2018large} & Ontology & Dialog & Dialog State \\\\\n\t\t\t& KVRET \\cite{Eric2017KeyValueRN} & Table & Dialog & Response \\\\\n\t\t\t\\textit{Conversational} & SParC \\cite{Yu19} & Database & Multi turn & SQL \\\\\n\t\t\t& CoSQL \\cite{yu-etal-2019-cosql} & Database & Dialog & SQL \\\\\n\t\t\t& SQA \\cite{iyyer-etal-2017-search} & Table & Multi turn & Answer \\\\\n\t\t\t\\midrule\n\t\t\t\\multirow{2}*{\\textit{Fact Verification}} \n\t\t\t& TabFact \\cite{2019TabFactA} & Table & Statement & Boolean \\\\\n\t\t\t& FEVEROUS \\cite{aly2021fact} & Table + Text & Statement & Boolean \\\\\n\t\t\t\\midrule\n\t\t\t\\multirow{2}*{\\textit{Formal-Language-to-Text}} & SQL2Text \\cite{shu-etal-2021-logic} & Optional Database & SQL & Text \\\\\n\t\t\t& Logic2Text \\cite{chen-etal-2020-logic2text} & Table Schema & Python-like program & Text \\\\\n\t\t\t\\bottomrule\n\t\t\\end{tabular}\n\t\\end{adjustbox}\n\t\\caption{We unify \\ntasks SKG tasks with different knowledge input, user input, and output, covering six task families. \n\t}\n\t\\label{tab:tasks-included}\n\t\\vspace{-3mm}\n\\end{table*}\n\\noindent\\textbf{SKG with PLMs \\ \\ }\nPLMs have been applied to several SKG tasks. \nTo encode structured knowledge, prior work linearized the structured knowledge and concatenated it with the text \\cite{Hwang2019ACE,weijie2019kbert,hosseini2020simple,liu2021tapex}, which has been augmented by positional encoding (e.g., row/column embedding) \\cite{Herzig2020tapas,yin20tabert} and template-based linearization \\cite{chen2020logical,2019TabFactA,oguz2021unik}, and planning \\cite{Su2021PlanthenGenerateCD}.\nRecently, cell-column alignment is modeled by manipulating the attention matrix of transformers \\cite{zhang2020table,eisenschlos2021mate}.\nHierarchical encoding is another way to represent the structure, e.g., \n\\citet{wang2021tuta} used tree-based transformers to represent the structure of the tables;\n\\citet{iida2021tabbie} used transformers to encode row and column representations; \\citet{chen2020hitter} used hierarchical transformers to encode KG triples. \n\\skg's outputs include, but are not limited to, structured meaning representations (e.g., logic forms, SQL), dialogue states, natural language, answer sets, and Boolean values. Among them, structured meaning representation is challenging for PLMs because they are originally trained on natural language. \nTo bridge this gap, \\citet{shin2021constrained} adopted the insights from \\citet{berant2014semantic} and \\citet{marzoev2020unnatural} and proposed to convert formal language into an English-like representation, decode with GPT-3, and map back to formal language automatically.\nWe do not focus on these techniques in this work; instead, we unify all tasks and systematically compare them.\n\n\\noindent\\textbf{Task format unification \\ \\ }\nRecent years witnessed the trend of unifying related but different tasks into a shared format.\n\\citet{abs-1806-08730} unified various tasks as question answering. \\citet{YinRRSX20} and \\citet{abs-2104-14690} unified few-shot learning as textual entailment. PLUR \\cite{chen2021plur} unified program learning, understanding, and repair tasks into a graph-to-sequence format. In this paper, we focus on the text-to-text format \\cite{2020t5} due to its flexibility. \nDifferent from unifying tasks that only take text as input, a core challenge in unifying \\skg tasks into the text-to-text format is to linearize structured knowledge.\nNotably, UnifiedQA \\cite{2020unifiedqa} unified QA tasks, while \\uskg covers a broader scope of six task families for systematic exploration. \n\n\\noindent\\textbf{Cross-task generalization with PLMs \\ \\ } \nMulti-task learning and transfer learning go beyond task boundaries, view different tasks as related, and have been shown to outperform single-task learning \\cite{muppet,vu2021spot}.\nLarge PLMs show potential for zero-shot and few-shot learning, e.g., GPT-2 \\cite{radford2019language} and GPT-3 \\cite{brown2020language}, which can be improved by multi-task learning \\cite{Zhong2021AdaptingLM}, e.g., FLAN \\cite{Jason2021}, T0 \\cite{sanh2021multitask}, and CrossFit \\cite{Ye2021CrossFitAF}. ExT5 \\cite{aribandi2021ext5} shows that scaling up multi-task learning helps improve pretraining efficiency and downstream performances.\n\\uskg facilitates the investigation of multi-task, zero-shot, and few-shot learning on \\skg tasks. \n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=0.9\\linewidth]{figures/modeling.pdf}\n    \\caption{We unify SKG tasks with heterogeneous inputs and outputs into the text-to-text format. }\n\\label{fig:input-output-format}\n\\end{figure*}\n\n\\section{The \\uskg Framework}\n\\label{sec:method}\n\n\\subsection{Task Unification}\n\\label{subsec:task_unification}\nThe guiding principle of \\uskg{}'s task selection is diversity. We unify \\ntasks SKG tasks across six task families and multiple domains (Table \\ref{tab:tasks-included}). Our task families include:\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{Semantic parsing} converts questions to logical forms \\cite{ZelleM96,Zettlemoyer05}.\n    \\item \\textbf{Question answering} derives answers to natural language questions based on structured data \\cite{BerantCFL13}.\n    \\item \\textbf{Data-to-text generation} describes structured data in natural language \\cite{NovikovaDR17}.\n    \\item \\textbf{Fact verification} checks if a statement is true based on the structured data \\cite{2019TabFactA}.\n    \\item \\textbf{Conversational tasks} require understanding of not only the user's last request but also the full interaction history between users and machines \\cite{budzianowski2018large, eric2019multiwoz, yu-etal-2019-cosql}.\n    \\item \\textbf{Formal language to text translation} describes formal language in natural language \\cite{chen-etal-2020-logic2text}. \n\\end{itemize}\n\nAll these tasks take as input $x$ a user request, a structured knowledge input, and an optional (dialogue) context to predict an output $y$.\nFigure \\ref{fig:input-output-format} illustrates how we convert the input $x$ to an input sequence $\\tilde{x}$ and the output $y$ to an output sequence $\\tilde{y}$ by means of ``linearization'' \\cite{liu2021tapex}, enabling the unification of diverse forms of structured knowledge. We provide more details, examples, and input length analysis in the Appendices \\ref{app:task-unification} and \\ref{app:examples}.\nOur code implementation uses Hugging Face's Transformers \\cite{wolf-etal-2020-transformers} and Datasets \\cite{lhoest2021datasets} toolkits. \n\n\\subsection{Modeling}\nThe simplest usage of \\uskg is to train on individual tasks. In this case, we minimize the negative log-likelihood loss averaged over tokens in each batch. \nFor decoding, we use beam search by default.\n\\uskg also facilitates exploration of multi-task learning, few-shot, and zero-shot learning with PLMs, and details are presented in the corresponding parts in Section \\ref{sec:results}. \n\n\\begin{table*}[ht]\n\t\\begin{minipage}{\\textwidth}\n\t    \\small\n        \\centering\n\t    \\begin{adjustbox}{width=0.95\\linewidth}\n    \t\t\\begin{tabular}{@{}l@{}ccccl|l@{}}\n    \t\t\t\\toprule\n    \t\t\t& Metric & T5-base\n    \t\t\t& T5-large\n    \t\t\t& T5-3B & Previous sota (w/o extra) & Previous sota (w/ extra)\n    \t\t\t\\\\ \n    \t\t\t\\midrule\n    \t        \\multirow{1}*{Spider (dev.)} \n                & Match & 58.12 & 66.63 & 71.76 & \\textbf{75.5}$^{+}$ \\cite{Scholak2021:PICARD} & 74.7  \\cite{rubin-berant-2021-smbop} \\\\\n                \\multirow{1}*{GrailQA}\n                & Match & 62.39\t& 67.30 & 70.11 & \n                \\textbf{83.8}$^{+}$ \\cite{ye2021rng} %use gold entities\n                & \\na \\\\\n                \\multirow{1}*{WebQSP}\n                & F1 & 78.83 & 79.45 & 80.70 & \n                \\textbf{83.6}$^{+}$ \\cite{ye2021rng} %use gold entities\n                & \\na \\\\\n                MTOP & Match & 85.49 & 86.17 & \\textbf{86.78} & 86.36 \\cite{pasupat-etal-2021-controllable} & \\na \\\\\n    \t\t\t\\midrule\n                \\multirow{1}*{WikiTQ}\n                & Acc & 35.76 & 43.22 & \\bf 49.29 & 44.5 \\cite{wang-etal-2019-learning} & 57.5  \\cite{liu2021tapex}\\\\\n                \\multirow{1}*{WikiSQL}\n                & Acc & 82.63 & 84.80 & \\textbf{85.96} & 85.8 \\cite{liu2021tapex} & 89.5 \\cite{liu2021tapex}\\\\\n                \\multirow{1}*{CompWebQ}\n                & Acc & 68.43 & 71.38 & 73.26 & 70.4$^{\\ddag}$ \\cite{das2021case} & \\na \\\\\n                \\multirow{1}*{HybridQA (dev.)}\n                & Acc & 54.07 & 56.95 & 59.41 & 60.8$^\\ddag$ \\cite{eisenschlos2021mate} & 63.4$^\\ddag$  \\cite{eisenschlos2021mate} \\\\\n                \\multirow{1}*{MultiModalQA (dev.)}\n                & F1 & 75.51\t& 81.84\t& \\bf 85.28 &    82.7 \\cite{yoran2021turning} &  83.8 \\cite{yoran2021turning} \\\\\n                \\multirow{1}*{FeTaQA}\n                & BLEU & 29.91 & 32.45 & \\bf 33.44 & 30.54 \\cite{nan2021feta} & \\na \\\\\n    \t\t\t\\midrule\n    \t\t\t\\multirow{1}*{DART}\n    \t\t\t& BLEU & 46.22 & \\textbf{46.89} & 46.66 & 46.89 \\cite{nan2021dart} & 47.2  \\cite{aghajanyan2021htlm} \\\\\n    \t\t\t\\multirow{1}*{ToTTo (dev.)}\n    \t\t\t& BLEU & 48.29 & \\bf 48.95 & \\bf 48.95 & % \\textbf{49.5} % They also used T5-3b while our result is weaker.\n    \t\t\t48.95 \\cite{KaleR20a} & \\na \\\\\n                \\midrule\n    \t\t\t\\multirow{1}*{MultiWoZ2.1}\n    \t\t\t& Joint Acc & 54.64 & 54.45 & 55.42 & \\textbf{60.61}$^*$\\cite{dai-etal-2021-preview} & 60.48 \\cite{yu2021SCoRE}\\\\\n    \t\t\t\\multirow{1}*{KVRET}\n    \t\t\t& Micro F1 & 66.45 & 65.85 & \\bf 67.88 & 63.6 \\cite{gou2021contextualize} & \\na \\\\\n    \t\t\t\\multirow{1}*{SParC (dev.)}\n    \t\t\t& Match & 50.54 & 56.69 & \\bf 61.51 & 54.1 \\cite{hui2021dynamic} & 62.2 \\cite{yu2021SCoRE}\\\\\n    \t\t\t\\multirow{1}*{CoSQL (dev.)}\n    \t\t\t& Match & 42.30 & 48.26 & 54.08 & \\textbf{56.9}$^{+}$  \\cite{Scholak2021:PICARD} & 52.1 \\cite{yu2021SCoRE}\\\\\n    \t\t\t\\multirow{1}*{SQA}\n    \t\t\t& Overall Acc  & 52.91 & 61.28 & \\bf 62.37 & 58.6 \\cite{liu2021tapex} & 74.5 \\cite{liu2021tapex}\\\\\n    \t\t\t\\midrule\n    \t\t\t\\multirow{1}*{TabFact}\n    \t\t\t& Acc & 76.13 & 80.85 & \\bf 83.68 & 74.4 \\cite{yang-etal-2020-program} & 84.2 \\cite{liu2021tapex}\\\\\n    \t\t\t\\multirow{1}*{FEVEROUS (dev.)}\n    \t\t\t& Acc & 75.05 & 79.81 & \\bf 82.40 & 82.38 \\cite{aly2021fact} & \\na \\\\\n    \t\t\t\\midrule\n    \t\t\t\\multirow{1}*{SQL2Text}\n    \t\t\t& BLEC & 93.52 & 93.68 & \\textbf{94.78} & 93.7 \\cite{shu-etal-2021-logic} & \\na \\\\\n    \t\t\t\\multirow{1}*{Logic2Text}\n    \t\t\t& BLEC & 90.66\t& 90.57\t& \\bf 91.39 & 88.6 \\cite{shu-etal-2021-logic} & \\na \\\\\n    \t\t\t\\bottomrule\n    \t\t\\end{tabular}\n    \t\\end{adjustbox}\n\t    \\caption{Test or development (dev.) set performance of models trained on individual tasks.\n\t    Vanilla T5 or T5 with simple modifications (e.g., $^{+}$constrained decoding or reranking) achieve sota on nearly all tasks.\n\t    The best result without extra pretraining is shown in \\textbf{bold}.\n\t    More detailed results and result variances can be found in Tables \\ref{tab:baseline-full-dev} and \\ref{tab:baseline-full-test} in Appendix.\n\t    Human evaluation for generation tasks is in Section \\ref{subsec:human-eval}.\n\t    \\textit{w/ (w/o) extra} means with (without) extra pretraining on unsupervised structured data (e.g., web tables).\\footnotemark\n\t    }\n\t\\label{tab:baseline-simple-version}\n\t\\end{minipage}\n\\end{table*}\\begin{table}[t]\n\\small\n\t\\centering\n\t\\begin{adjustbox}{width=0.98\\linewidth}\n\t\t\\begin{tabular}{@{}l@{}c@{}c@{}c@{}c@{}c@{}c@{}}\n\t\t\t\\toprule\n\t\t\t& Spider \\ & \\ WikiTQ \\ & \\ \\ DART \\ \\ &  \\ MWoZ \\ \\ &  \\ TabFact \\ \\ & SQL2Text \\\\ \n\t\t\t\\midrule \n\t\t    T5-3B \\ \\ \\ & \\bf 71.76 & \\bf 50.65 & \\bf 50.38 & 58.46 & 83.97 & 92.71 \\\\\n            T0-3B & 68.09 & 50.62 & 50.16 & \\bf 60.20 & \\bf 85.51 & \\bf 92.93 \\\\\n\t\t\t\\bottomrule\n\t\t\\end{tabular}\n\t\\end{adjustbox}\n\t\\caption{Comparison between T5-3B and T0-3B. T0-3B is initialized from LM-adapted T5 and further pretrained on a large number of non-\\skg tasks. We finetune both models on individual tasks.\n\tT0-3B under-performs T5-3B on semantic parsing (Spider) and outperforms T5-3B on dialogue state tracking (MWoZ) and fact verification (TabFact). We report results on the dev. set. }\n\t\\vspace{-3mm}\n\t\\label{tab:t0}\n\\end{table}\n\n\\section{Experiments and Analysis}\n\\label{sec:results}\n\n\\subsection{Results on Individual Tasks} \n\\label{subsec:baseline-performance}\nWe apply T5 models \\cite{2020t5} on each individual task in \\uskg.\nFor model training, we set the maximum number of epochs as 50--200, depending on the dataset size. We use early stopping and model selection on the development set. \nMore details are shown in Appendix \\ref{subapp:implementation-details}. \nFor each task, we report one commonly used metric in Table~\\ref{tab:baseline-simple-version}. See Appendix \\ref{app:all-results} for all metrics. \n\n\\noindent\\textbf{Comparison with previous sota \\ \\ }\nTable~\\ref{tab:baseline-simple-version} shows that vanilla T5-3B outperforms most previous sota models not trained on extra unsupervised in-domain data.  \nSome semantic parsing sota models, denoted as $^{+}$ in Table \\ref{tab:baseline-simple-version}, are also T5 with constrained decoding \\cite{Scholak2021:PICARD} or reranking \\cite{ye2021rng}. \nThis shows that a generalist architecture like T5, when scaled up to a certain size, can be as good as task-specific architectures for \\skg, suggesting the potential of larger PLMs. \n\n\\noindent\\textbf{Model scalability \\ \\ } \nIn general, T5 performance increases with the model size, but this trend varies across task families.\nSemantic parsing, QA, and fact verification tasks get large benefits from increased sizes, while text generation does not. \nSee Section \\ref{subsec:human-eval} for a human evaluation for text generation tasks.\nAlso, the gap between T5-base (220M) and T5-large (770M) is larger than the gap between T5-large (770M) and T5-3B (3B). \n\n\\noindent\\textbf{Effect of pretraining on structured knowledge} \\ \\\nSome smaller models pretrained on structured knowledge \\cite{liu2021tapex} show competitive performance as T5-3B, suggesting that pretraining with structured data is beneficial for \\skg. This result calls for structured knowledge pretraining that generalizes to different \\skg tasks across domains, which can be systematically explored using \\uskg. \n\n\\noindent\\textbf{Effect of pretraining on non-\\skg tasks \\ \\ }\nT0-3B \\cite{sanh2021multitask} is initialized from T5-3B and pretrained on multiple tasks that (in most cases) do not use structured knowledge as input (non-\\skg tasks). \nExploring the performance of T0-3B on \\skg tasks helps us understand the relationship between \\skg tasks and non-\\skg tasks. \nTable \\ref{tab:t0} shows that T0-3B under-performs T5-3B on semantic parsing and outperforms T5-3B on dialogue state tracking and fact verification. \nWe note that T0-3B is pretrained on dialogue QA, dialogue summarization, and NLI tasks; therefore, pretraining on non-\\skg tasks might not be useful for \\skg unless we add similar \\skg tasks to pretraining. \n\n\\footnotetext{%\nFor GrailQA and WebQSP, we run T5 and rerun the previous sota model~\\cite{ye2021rng} using the gold entities. \nFor MultiModalQA and FEVEROUS, we report performance of T5 and the previous sota models on the dev. samples with at least one table (samples with image input are further excluded for MultiModalQA);\nThe gold table and text candidates are used for both T5 and previous sota~(for MultiModelQA, numbers are from~\\cite{yoran2021turning}, and for FEVEROUS, we rerun the available model~\\cite{aly2021fact} on gold candidates to obtain the number).\nWe use sacreBLEU to report all BLEU results.\n$^\\ddag$We use gold entity linking, but the previous sota does not, which makes the results not directly comparable; therefore, we do not bold any numbers for CompWebQ and HybridQA.\n$^*$T5-base with the independent output scheme \\cite{lee-etal-2021-dialogue} achieves 56.66 on MWoZ2.1, higher than our sequence output scheme. \nFor WebQSP, as the original dataset does not have a dev.~set, we split the original train set into in-house train/dev.~sets (90\\%/10\\%), following prior practice (e.g. \\citet{ren2021lego}). Similarly, for CompWebQ, as the test set is not publicly available, we split the original dev.~set into in-house dev./test sets (20\\%/80\\%). For GrailQA, we split the original dev.~set into in-house dev./test sets (5\\%/95\\%).\n}\n\n\\subsection{Multi-Task Learning}\n\\uskg facilitates the exploration of multi-task learning. In this part, we systematically study multi-task learning on all \\ntasks unified tasks. We find that \\skg benefits from multi-task prefix-tuning on both T5-base and T5-large, showing that the benefits from multi-task learning is scalable in terms of the model size. The baselines we use include:\n\n\\noindent\\textbf{Single-task finetuning (ST-F)}, which is finetuning on individual tasks, same as Section \\ref{subsec:baseline-performance}.\n\n\\noindent\\textbf{Single-task prefix-tuning (ST-P;} \\citealp{li2021prefixtuning}\\textbf{)}, which learns lightweight task-specific parameters while keeping the PLM fixed. We set the prefix length as 10. \\citet{Clive2021ControlPF} also used prefix-tuning on T5 for data-to-text generation. \n\n\\noindent\\textbf{Multi-task finetuning (MT-F)}, which combines the training data of all tasks with temperature mixing (\\citealp{2020t5}; after hyperparameter tuning with a few steps, we set the temperature as 2). We select model weights based on the average metric on all tasks' development set.\n\nTable~\\ref{tab:multitask-dev-simple} shows that ST-P is comparable to ST-F on nearly all tasks. \nHowever, we find that it takes about 5--10 times as many training steps (See Appendix \\ref{app:step}), which is similarly observed for prompt-tuning \\cite{LesterAC21}. \nWe also observe that MT-F leads to mixed results. \nFor many tasks, MT-F is even worse than ST-F.\n\n\\noindent\\textbf{Multi-task prefix-tuning (MT-P) \\ \\ } \nOur explanation for the mixed results of MT-F is that the inputs of SKG tasks contain different structured knowledge from diverse domains, making it difficult to learn shared parameters effectively. \nTo address this challenge, we first pretrain a prefix on all tasks, freezing T5 and using the same temperature mixing as MT-F. In the second step, we initialize each task's prefix with this pretrained prefix and optimize the prefix while freezing T5. \nThis initialization step is similar to the prompt transfer explored in \\citet{vu2021spot}.\nFollowing ST-P, we set the prefix length as 10. \n\nTable~\\ref{tab:multitask-dev-simple} shows that multi-task prefix-tuning outperforms single-task finetuning and single-task prefix-tuning on most tasks, and it largely outperforms the naive multi-task learning baseline.\nIt demonstrates that SKG tasks can be studied together to share data and knowledge.\n\n\\begin{table}[t!]\n\t\\centering\n\t\\begin{adjustbox}{width=0.95\\linewidth}\n\t\t\\begin{tabular}{@{}lcccc|cc@{}}\n\t\t\t\\toprule\n\t\t\t\\multicolumn{1}{c}{}  & \\multicolumn{4}{c|}{T5-base} & \\multicolumn{2}{c}{T5-large} \\\\\n\t\t    & ST-F & ST-P & MT-F & MT-P & ST-F & MT-P\n\t\t\t\\\\ \n\t\t\t\\midrule\n\t        \\multirow{1}*{Spider} \n            & 58.12 & 58.61 & 58.90 & \\bf 59.86 & 66.63 & \\bf 67.60 \\\\\n            \\multirow{1}*{GrailQA}\n            & 60.00 & 61.33 & 56.00 & \\bf 62.67 & \\bf 67.00 & 65.33 \\\\\n            \\multirow{1}*{WebQSP}\n            & 72.50 & 73.81 & 67.25 & \\bf 74.77 & 73.96 & \\bf 74.92 \\\\\n            \\multirow{1}*{MTOP}\n            & \\bf 83.89 & 82.93 & 78.79 & 82.77 & \\bf 84.70 & 84.34 \\\\\n\t\t\t\\midrule\n            \\multirow{1}*{WikiTQ}\n            & 36.94 & 36.42 & \\bf 41.15 & 39.74 & 43.30 & \\bf 50.90 \\\\\n            \\multirow{1}*{WikiSQL}\n            & \\bf 84.50 & 83.09 & 81.85 & 84.44 & 86.27\t& \\bf 87.45 \\\\\n            \\multirow{1}*{CompWQ}\n            & 66.71 & 67.85 & 68.28 & \\bf 69.70 & 68.85\t& \\bf 71.27 \\\\\n            \\multirow{1}*{HybridQA}\n            & 54.07 & \\bf 54.93 & 53.52 & 54.88 & 56.95\t& \\bf 57.33 \\\\\n            \\multirow{1}*{MMQA}\n            & 75.51 & 75.50 & \\bf 76.63 & 76.40 & 81.84\t& \\bf 84.59 \\\\\n            \\multirow{1}*{FeTaQA}\n            & 29.00 & 28.03 & \\bf 31.85 & 29.33 & 30.94\t& \\bf 32.48 \\\\\n\t\t\t\\midrule \n\t\t\t\\multirow{1}*{DART} \n\t\t\t& 50.62 & 50.33 & 49.74 & \\bf 50.68 & \\bf 51.72\t& 50.82 \\\\\n\t\t\t\\multirow{1}*{ToTTo}\n\t\t\t& \\bf 48.29 & 45.70 & 45.29 & 45.21 & \\bf 48.95\t& 47.90 \\\\\n            \\midrule\n\t\t\t\\multirow{1}*{MWoZ2.1}\n\t\t\t& \\bf 57.52 & 56.67 & 53.19 & 57.06 & 58.23\t& \\bf 59.24 \\\\\n\t\t\t\\multirow{1}*{KVRET} \n\t\t\t& 20.04 & 19.68 & 18.53 & \\bf 21.32 & 18.84\t& \\bf 20.76 \\\\\n\t\t\t\\multirow{1}*{SParC}\n\t\t\t& 50.54 & 51.04 & \\bf 51.70 & 51.29 & 56.69\t& \\bf 59.02 \\\\\n\t\t\t\\multirow{1}*{CoSQL}\n\t\t\t& 42.30 & 44.39 & 43.59 & \\bf 45.68 & 48.26\t& \\bf 51.64 \\\\\n\t\t\t\\multirow{1}*{SQA}\n\t\t    & 49.49 & 44.81 & \\bf 51.48 & 48.43 & \\bf 59.12\t& 58.15 \\\\\n\t\t\t\\midrule\n\t\t\t\\multirow{1}*{TabFact}\n\t\t\t& 76.34 & 75.74 & 71.19 & \\bf 77.86 & 81.40 & \\bf 83.62 \\\\\n            \\multirow{1}*{FEVER.}\n            & 75.05 & 75.33 & 76.85 & \\bf 78.02 & 79.81\t& \\bf 82.05 \\\\\n\t\t\t\\midrule\n\t\t\t\\multirow{1}*{SQL2Text}\n\t\t\t& 93.69 & \\bf 94.50 & 93.57 & 93.79 & 93.35\t& \\bf 93.93 \\\\\n\t\t\t\\multirow{1}*{Logic2Text}\n\t\t\t& 92.15 & \\bf 95.25 & 92.24 & 94.70 & 92.88\t& \\bf 93.61\\\\\n\t\t\t\\midrule\n\t\t\tTotal para. & $21T$ & $T + 21P$ & $T$ & $T + 21P$ & $21T$ & $T + 21P$ \\\\\n\t\t\tAvg. score & 60.82 & 60.76 & 60.08 & \\bf 61.84 & 64.27 & \\bf 65.57 \\\\\n\t\t\t\\bottomrule\n\t\t\\end{tabular}\n\t\t\\end{adjustbox}\n\t\\caption{Multi-task learning results. \n\tST and MT stand for single-task and multi-task. F and P stand for finetuning and prefix-tuning. \n\tFor total parameters, $T$ and $P$ are the numbers of T5 and prefix parameters ($P \\ll T$). \n    Multi-task learning with prefix improves the performance on most tasks, largely improving the overall performance. We report results on the dev. set. }\n\t\\label{tab:multitask-dev-simple}\n\t\\vspace{-3mm}\n\\end{table}\n\n\\begin{table}[!t]\n\t\\centering\n\t\\begin{adjustbox}{width=0.85\\linewidth}\n\t\t\\begin{tabular}{@{}ccccc@{}}\n\t\t\t\\toprule\n\t\t\tTask A & Task B & Type & B only & A to B \\\\\n\t\t\t\\midrule\n\t\t\t\\multirow{1}*{WikiSQL}\n\t\t\t& \\multirow{1}*{TabFact}\n\t\t\t& same source & 81.43 & 82.76 \\\\\n\t\t\t\\multirow{1}*{TabFact}\n\t\t\t& \\multirow{1}*{WikiTQ}\n\t\t\t& same source & 43.30 & 45.88 \\\\\n\t\t\t\\multirow{1}*{WikiSQL}\n\t\t\t& \\multirow{1}*{FeTaQA}\n\t        & same source & 30.94 & 31.19 \\\\\n\t\t\t\\multirow{1}*{Spider}\n\t\t\t& \\multirow{1}*{GrailQA}\n\t        & parallel tasks & 67.00 & 67.00 \\\\\n\t\t\t\\multirow{1}*{Spider}\n\t\t\t& \\multirow{1}*{WikiTQ}\n            & subtask & 43.30 & 41.68 \\\\\n\t\t\t\\multirow{1}*{Spider}\n\t\t\t& \\multirow{1}*{TabFact}\n\t\t\t& weakly related & 81.43 & 80.39 \\\\\n\t        \\bottomrule\n\t\t\\end{tabular}\n\t\t\\end{adjustbox}\n\t\\caption{Task knowledge transfer. We use T5-large here. \\textit{B only} means training the model on task B; \\textit{A to B} means to train the model on task A and then to finetune the model on task B. In both settings, we report task B's development set performance. We find that tasks benefit from other tasks with the same data source. }\n\t\\label{tab:task-relationship-main}\n\\end{table}\n\n\\iffalse\n\n\\begin{table}[t]\n\t\\centering\n\t\\begin{adjustbox}{width=\\linewidth}\n\t\t\\begin{tabular}{@{}ccccc@{}}\n\t\t\t\\toprule\n\t\t\tGroup & Task & Single-Finetune & Single-Prefix & Shared-Prefix \\\\\n\t\t\t\\midrule \n\t        \\multirow{3}*{Text-to-SQL}\n\t        & Spider    & 58.12 & 58.61 & 60.64$\\uparrow$ \\\\\n\t        & CoSQL     & 42.30 & 42.30 & 46.87$\\uparrow$ \\\\\n\t        & SParC     & 50.54 & 51.04 & 52.04$\\uparrow$ \\\\\n\t        \\midrule\n\t\t\t\\multirow{8}*{QA}\n\t\t\t& WikiTQ    & 36.94 & 36.42 & 39.60$\\uparrow$ \\\\\n\t\t\t& WikiSQL   & 84.50 & 83.09 & 80.38 \\\\ \n\t\t\t& HybridQA  & 54.07 & 54.93 & 52.34 \\\\ \n\t\t\t& MMQA      & 75.51 & 75.50 & 73.76 \\\\ \n\t\t\t& FeTaQA    & 29.12 & 28.03 & 28.77 \\\\ \n\t\t\t& SQA       & 49.49 & 44.81 & 43.13 \\\\ \n\t\t\t& CompWebQ  & 66.71 & 67.85 & 67.14$\\uparrow$ \\\\ \n\t\t\t\\midrule\n\t\t\t\\multirow{3}*{KG input}\n\t\t\t& GrailQA   & 62.39 & 63.33 & 58.67 \\\\\n\t\t\t& WebQSP    & 72.50 & 73.81 & 72.70 \\\\ \n\t\t\t& CompWebQ  & 68.43 & 67.85 & 68.14 \\\\ \n\t        \\midrule\n\t\t\t\\multirow{2}*{Data-to-text}\n\t\t\t& DART      & 50.62 & 50.33 & 49.13 \\\\\n\t\t\t& ToTTo     & 48.29 & 45.70 & 43.98 \\\\ \n\t\t\t\\midrule\n\t\t\t\\multirow{2}*{Fact veri.}\n\t\t\t& TabFact      &  76.34 & 75.74 & 76.25 \\\\\n\t\t\t& FEVEROUS     &  75.05 & 75.33 & 79.04$\\uparrow$\\\\ \n\t\t\t\\midrule\n\t\t\t\\multirow{2}*{HF NLG}\n\t\t\t& Logic2Text    & 92.15 & 95.25 & 94.79 \\\\\n\t\t\t& SQL2Text      & 93.43 & 94.50 & 94.86 \\\\ \n\t        \\bottomrule\n\t\t\\end{tabular}\n\t\t\\end{adjustbox}\n\t\\caption{Task similarity induced by prefix-tuning. $\\uparrow$ means this task is benefit from other tasks in the group }\n\t\\label{tab:task-relationship-main}\n\\end{table}\n\n\\fi\n\n\\iffalse\n\\begin{table}[t]\n\t\\centering\n\t\\begin{adjustbox}{width=\\linewidth}\n\t\t\\begin{tabular}{@{}ccccc@{}}\n\t\t\t\\toprule\n\t\t\tGroup & Task & Single-task & PAMT \\\\\n\t\t\t\\midrule \n\t        \\multirow{3}*{Text-to-SQL}\n\t        & Spider    & 58.12 & 60.64 \\\\\n\t        & CoSQL     & 42.30 & 46.87 \\\\\n\t        & SParC     & 50.54 & 52.04 \\\\\n\t        \\midrule\n\t\t\t\\multirow{8}*{QA}\n\t\t\t& WikiTQ    & 36.94 & 39.60 \\\\\n\t\t\t& WikiSQL   & 84.50 & 80.38 \\\\ \n\t\t\t& HybridQA  & 54.07 & 52.34 \\\\ \n\t\t\t& MMQA      & 67.29 & 65.42 \\\\ \n\t\t\t& FeTaQA    & 29.12 & 28.77 \\\\ \n\t\t\t& SQA       & 49.49 & 43.13 \\\\ \n\t\t\t& CompWebQ  & 66.71 & 67.14 \\\\ \n\t\t\t\\midrule\n\t\t\t\\multirow{3}*{KG input}\n\t\t\t& GrailQA   & 62.39 & 58.67 \\\\\n\t\t\t& WebQSP    & 72.50 & 72.70 \\\\ \n\t\t\t& CompWebQ  & 68.43 & 68.14 \\\\ \n\t        \\midrule\n\t\t\t\\multirow{2}*{Data-to-text}\n\t\t\t& DART      & 50.62 & 49.13 \\\\\n\t\t\t& ToTTo     & 48.29 & 43.98 \\\\ \n\t\t\t\\midrule\n\t\t\t\\multirow{2}*{Fact veri.}\n\t\t\t& TabFact      &  76.34 & \\running \\\\\n\t\t\t& FEVEROUS     &  75.05 & 79.04 \\\\ \n\t\t\t\\midrule\n\t\t\t\\multirow{2}*{HF NLG}\n\t\t\t& Logic2Text    & 92.15 & 94.79 \\\\\n\t\t\t& SQL2Text      & 93.43 & 94.86 \\\\ \n\t        \\bottomrule\n\t\t\\end{tabular}\n\t\t\\end{adjustbox}\n\t\\caption{Task relationship induced by PAMT. }\n\t\\label{tab:task-relationship-main}\n\\end{table}\n\\fi\\noindent\\textbf{Exploring task knowledge transfer \\ \\ } \n\\uskg facilitates studying knowledge transfer between \\skg tasks. Given two tasks, \\textit{task A} and \\textit{task B}, we first train the model on task A and then continue training on task B. Table \\ref{tab:task-relationship-main} shows that tasks benefit from other tasks with the same data source (e.g., tasks that all use Wikipedia tables as structured knowledge). We do not observe positive transfer between \\textit{parallel tasks} (e.g., semantic parsing tasks with different structured knowledge and different output) and \\textit{subtask} (e.g., question answering can be viewed as the execution semantic parses) when data sources are different.  \nCompared to the positive results in Table \\ref{tab:multitask-dev-simple}, results in this part indicate that manually selecting source and target tasks may not be efficient for multi-task learning. \n\n\\subsection{Zero-Shot and Few-Shot Learning}\n\\label{subsec:pretrained-models-analysis}\nThe text-to-text unification of \\uskg enables us to investigate zero/few-shot learning on SKG with large PLMs. \n\n\\noindent\\textbf{Zero-shot learning setting \\ \\ } \nZero-shot learning enables models to solve tasks with natural language descriptions without training samples. We follow T0 \\cite{sanh2021multitask} to create similar natural language instructions for the unseen tasks. Our instructions are provided in Appendix \\ref{subapp:t0_zero}. \n\n\\iffalse\n\\begin{table}[t]\n    \\small\n\t\\centering\n\t\\begin{adjustbox}{width=\\columnwidth}\n\t\t\\begin{tabular}{lcccc}\n\t\t\t\\toprule\n\t\t\t& T5-large & BART-large & GPT-3 & Codex \\\\ \n\t\t\t\\midrule\n\t\t\t\\multirow{1}*{Spider} \n            & 64.73 & 43.13 & \\running & 41.49  \\\\\n            \\multirow{1}*{WikiTQ}\n            & 43.59 & 33.73 & \\running & 26.21 \\\\\n            \\multirow{1}*{DART} \n\t\t\t& 51.72 & 43.23 & \\running & 42.13  \\\\\n\t\t\t\\multirow{1}*{MultiWoZ2.1}\n\t\t\t& 56.62 & 54.60 & \\running & 23.11 \\\\\n\t\t\t\\multirow{1}*{TabFact}\n\t\t\t& 81.40 & 77.49 & \\running & 50.97 \\\\\n\t\t\t\\multirow{1}*{SQL2Text}\n\t\t\t& 93.07 & 88.86 & \\running & 90.64 \\\\\n\t\t\t\\bottomrule\n\t\t\\end{tabular}\n\t\\end{adjustbox}\n\t\\caption{Comparison of pretrained models. \\tao{add T0/FLAN/GPT3/Codex/MetaICL zero-shot results here too to add a few-shot/zero-shot section in the paper. use their prompt style for T0/FLAN. use a figure to show the big performance gap of T0 on their unseen tasks and our skg tasks}}\n\t\\label{tab:effect-of-models}\n\\end{table}\n\n\\fi \n\n\\begin{table}[t]\n    \\small\n\t\\centering\n\t\\begin{adjustbox}{width=\\columnwidth}\n\t\t\\begin{tabular}{@{}l@{}cccccc@{}}\n\t\t\t\\toprule\n\t\t\t& T5-3B &\n\t\t\t\\multicolumn{1}{c}{T0 3B} &\n\t\t\t\\multicolumn{2}{c}{GPT-3 175B} & \\multicolumn{2}{c}{Codex 175B} \\\\ \n\t\t\t& \\textit{finetune} & \\textit{zero-shot} & \\textit{select} & \\textit{random} & \\textit{select} & \\textit{random} \\\\\n\t\t\t\\midrule\n\t\t\t\\multirow{1}*{Spider} \n            & 71.76 & \\ \\ 0.00 & 20.00 & 18.33$_{3.78}$ & 40.72 & 43.23$_{4.16}$ \\\\\n            \\multirow{1}*{WikiTQ}\n            & 50.65 & 12.68 & 32.00 & 29.33$_{9.04}$ & 26.21 & 20.46$_{4.21}$ \\\\\n            \\multirow{1}*{DART} \n\t\t\t& 50.38 & 23.42 & 40.23 & 34.21$_{4.50}$ & 42.13 & 36.54$_{1.67}$ \\\\\n\t\t\t\\multirow{1}*{MWoZ}\n\t\t\t& 58.46 & \\ \\ 0.00 & 18.00 & \\ \\ 0.02$_{0.02}$ & 23.47 & \\ \\ 0.06$_{0.03}$ \\\\\n\t\t\t\\multirow{1}*{TabFact}\n\t\t\t& 83.97 & 52.45 & 51.00 & 49.67$_{3.79}$ & 50.97 & 51.58$_{1.59}$ \\\\\n\t\t\t\\multirow{1}*{SQL2Text}\n\t\t\t& 92.71 & 39.64 & 94.00 & 85.00$_{2.65}$ & 90.64 & 88.31$_{1.61}$ \\\\\n\t\t\t\\bottomrule\n\t\t\\end{tabular}\n\t\\end{adjustbox}\n\t\\caption{Zero-shot and few-shot learning for SKG. Subscripts show the standard deviation with three runs. \n\t\\textit{select} means to select the most similar training samples as few-shot examples, while \\textit{random} means to randomly select training samples as few-shot examples. \n\tT0 performs poorly on all the tasks in the zero-shot setting.\n\tCodex outperforms GPT-3 on tasks that generate structured programs (Spider and MultiWoZ).\n\t}\n\t\\label{tab:effect-of-models}\n\\end{table}\n\\noindent\\textbf{Few-shot learning settings \\ \\ } \\citet{brown2020language} showed that large PLMs could be few-shot learners by encoding a few training samples as ``context'' to learn without gradient updates. \nWe use GPT-3 \\cite{brown2020language} and Codex \\cite{chen2021evaluating} to explore such few-shot learning for SKG. To stay within our budget, for GPT-3, we report the performance on 100 random dev. set samples. We explore two settings for few-shot learning. \n\nIn the first setting, we randomly sample few-shot examples from the training set; these examples are shared by all dev. set samples, denoted as \\textit{random} in Table \\ref{tab:effect-of-models}. \nFor sequences that are too long for Codex (4096) and GPT-3 (2048), we use as many examples as possible and make sure that there is at least one example (truncated if needed). \n\nIn the second setting, we follow \\citet{gao2021making} to select few-shot examples from the training set. \nWe call this setting \\textit{few-shot with example selection}, denoted as \\textit{select} in Table \\ref{tab:effect-of-models}. \nWe use the pretrained SBERT \\cite{reimers-2020-multilingual-sentence-bert} for sentence embeddings of the user request input (for tasks that only have structured input, we embed the linearized structured input) and sample five most similar examples measured by cosine similarity. Further details (e.g., prompts and task instructions) are provided in Appendix \\ref{subapp:gpt3_codex}.\n\n\\noindent\\textbf{SKG is challenging for zero/few-shot learning. }\nTable~\\ref{tab:effect-of-models} shows that zero-shot performance is very poor on most tasks (Spider and MultiWoZ are even 0). \nIt also shows a large gap between few-shot learning and finetuning for Spider, WikiTQ, MWoZ, and TabFact, while the gap is smaller for generation tasks. \nFor few-shot learning, example selection based on similarity outperforms random selection, but the gap is usually smaller than 10 points out of 100. \nIt is also interesting to compare the results between \\textit{synthesis} tasks (Spider), which requires predicting programs, and \\textit{induction} tasks (WikiTQ and TabFact), where a model directly outputs answers \\citep{Devlin2017RobustFillNP}. \nWe find that PLMs generally struggle more when adapting to induction tasks (e.g., close to random-guess on the binary classification task TabFact), reminiscent of recent attempts in program synthesis and induction using PLMs~\\citep{Austin2021ProgramSW}.\nFor GPT-3 and Codex, \nbetter zero-shot performances can be expected by better prompt design. \n\n\\subsection{Structured Knowledge Encoding}\n\\label{subsec:structured-encoding-analysis}\nStructured knowledge encoding has been widely explored (\\citealp{Bogin2019GlobalRO,kagnet-emnlp19,agarwal2020knowledge,saxena2020improving,yasunaga2020graph,yasunaga2022dragon}; and others detailed in Section \\ref{sec:related-work}). We hope that \\uskg can promote systematic study of \\textit{general} structured knowledge encoding. \nTo this end, this part focuses on the linearization of structured knowledge. \n\n\\begin{table}[t]\n\t\\centering\n\t\\small\n\t\\begin{adjustbox}{width=0.95\\linewidth}\n\t\t\\begin{tabular}{@{}lcccc@{}}\n\t\t    \\toprule\n\t\t    & Spider & WikiTQ & MultiWoZ2.1 & TabFact\\\\\n\t\t\t\\midrule\n\t\t\t \\textit{rs(c)} & \\quad \\ \\ 66.63$_{2.31}$ & \\quad \\ \\ 43.30$_{0.25}$ & \\quad \\ \\ 58.23$_{0.39}$ & \\quad \\ \\ 81.43$_{0.16}$ \\\\\n\t\t\t \\textit{sr} & 64.12 & 38.78 & \\na & 80.98 \\\\ \\textit{rcs} & \\na & \\na & 58.89 & \\na \\\\\n\t\t\t\\bottomrule\n\t\t\\end{tabular}\n\t\\end{adjustbox}\n\t\\caption{Ordering of inputs. Subscripts show the standard deviation with three runs. \\textit{s}, \\textit{r}, and \\textit{c} stand for the structured knowledge, request input, and context. Placing \\textit{r} before \\textit{s} is always better, and placing \\textit{c} between \\textit{r} and \\textit{s} is better for dialogue state tracking (MultiWoZ2.1).\n\t}\n\t\\label{tab:sk_txt_ordering}\n\\end{table}\n\n\\begin{table}[t]\n    \\small\n\t\\centering\n\t\\begin{adjustbox}{width=1\\linewidth}\n\t\t\\begin{tabular}{@{}l@{}cccc@{}}\n\t\t    \\toprule\n\t\t\t& Spider & WikiTQ & DART & MultiWoZ2.1 \\\\\n\t\t\t\\midrule\n\t\t\tSame Order & \\quad \\ \\ 66.63$_{2.31}$ & \\quad \\ \\ 43.30$_{0.25}$ & \\quad \\ \\ 51.72$_{0.15}$ & \\quad \\ \\ 58.23$_{0.39}$ \\\\\n\t\t\tReversed Order & 64.80 & 37.80 & 48.47 & 13.59 \\\\\n\t\t\t\\bottomrule\n\t\t\\end{tabular}\n\t\\end{adjustbox}\n\t\\caption{Order-sensitivity of structured knowledge. Subscripts show the standard deviation with three runs. \\textit{Same Order} is the default benchmark setting. \\textit{Reversed Order} means to reverse the structured knowledge ordering on the development set (but not the training set). \n\tTasks with cross-domain tables (in WikiTQ), databases (in Spider), and triples (in DART) are less order-sensitive, while pre-defined ontology (in MultiWoZ2.1) is highly order-sensitive. }\n\t\\label{tab:sk_ordering}\n\\end{table}\n\n\\iffalse\n\\begin{table}[t]\n    \\small\n\t\\centering\n\t\\begin{adjustbox}{width=0.95\\linewidth}\n\t\t\\begin{tabular}{@{}lccc@{}}\n\t\t    \\toprule\n\t\t\t& WikiTQ & WikiSQL & TabFact \\\\\n\t\t\t\\midrule\n\t\t\tT5-3B & 49.29  & 85.96 & 83.68 \\\\\n\t\t\tTAPAS~\\cite{Herzig2020tapas} & 48.8 & 83.6 & 81.0$^{\\dagger}$ \\\\\n\t\t\tTaBERT~\\cite{yin20tabert} & 52.3 & -- & -- \\\\\n\t\t\t\\bottomrule\n\t\t\\end{tabular}\n\t\\end{adjustbox}\n\t\\caption{Test set performance of T5-3B and specific structured knowledge encoding models. $^\\dagger$Further intermediate pretraining based on TAPAS. T5 with simple linearization is comparable to specific encoding models. }\n\t\\label{tab:specific_encoding_models}\n\\end{table}\n\\fi\\noindent\\textbf{Does the order of user input, structured knowledge, and context matter?} \nTo explore the effect of the order of user input, structured knowledge, and context, we rerun the single-task experiments while switching the order of these components in both the training and development set. \nTable \\ref{tab:sk_txt_ordering} shows that placing the text before structured knowledge (\\textit{rs}) is better than the opposite (\\textit{sr}), which is consistent across \\skg tasks.\nOur explanation is that the position of the text is relatively fixed in \\textit{rs}, helping the decoder to learn stable attention over the text. \nAlso, placing the context in between the text and structured knowledge yields better results.\n\n\\noindent\\textbf{Is T5 sensitive to structured knowledge ordering?}\nOrder-insensitivity is common for most structured knowledge, e.g., permutation of columns in a table preserves the meaning. \nTo study this insensitivity, we evaluate T5-large on a manipulated development set where the order of schema (for database), column (for table), or slots and values (for ontology) is reversed. \nTable \\ref{tab:sk_ordering} shows that tasks with cross-domain tables and databases are less order-sensitive, while models are very sensitive to the order of ontology. \nOther types of robustness (e.g., robustness to cell values irrelevant to the answer) remain an open question in \\uskg. \n\n\\begin{table}[t]\n\t\\centering\n\t\\small\n\t\\begin{adjustbox}{width=0.85\\linewidth}\n\t\t\\begin{tabular}{@{}lccc@{}}\n\t\t    \\toprule\n\t\t\t& Spider & WikiSQL & TabFact \\\\\n\t\t\t\\midrule\n\t\t\tLinearization  & 40.23 & 59.21 & 58.77  \\\\\n\t\t\tNatural Language & 38.59 & 63.16 & 58.56  \\\\\n\t\t\t\\bottomrule\n\t\t\\end{tabular}\n\t\\end{adjustbox}\n\t\\caption{Converting structured knowledge into natural language for low-resource learning. \n\tA large improvement is observed on question answering (WikiSQL), but not on text2SQL semantic parsing (Spider) and fact verification (TabFact). }\n\t\\label{tab:convert-to-nl}\n\\end{table}\\noindent\\textbf{Is it beneficial to represent structured knowledge as natural language?}\nSKG data is not typically used to pretrain PLMs. Given ample training data, PLMs adapt well to SKG tasks, as shown in Table \\ref{tab:baseline-simple-version}. However, under the low-resource setting, converting structured data to natural language might be helpful. \nFor Spider, we use a shared template to convert structured data to natural language. \nFor TabFact and WikiSQL, we randomly selected 236 tables shared by both datasets and manually labeled templates to convert each row into a sentence. Examples of the templates are shown in Appendix \\ref{app:examples_template}. These templates produce about 1000 samples for each task, divided into training and test sets. We find that, in WikiSQL, the conversion to natural language stabilizes and accelerates the training process. % while in TabFact and Spider, it doesn't help much by a large margin.\nTable \\ref{tab:convert-to-nl} shows that conversion to natural language improves the performance on WikiSQL, has no significant influence on TabFact, and slightly degrades the performance on Spider.\n\n\\begin{table}[t]\n\t\\centering\n\t\\small\n\t\\begin{adjustbox}{width=0.85\\linewidth}\n\t\\begin{tabular}{@{}lllll@{}}\n\t\t\t\\toprule\n\t\t\t& Metric & T5-base\n\t\t\t& T5-large\n\t\t\t& T5-3B\n\t\t\t\\\\ \n\t\t\t\\midrule\n            \\multirow{2}*{FeTaQA}\n            & BLEU & 29.00 & 30.94 & 31.73 \\\\\n            & Human$^{*\\dag}$ & 36.0\\% & 51.3\\% & 57.3\\% \\\\\n\t\t\t\\midrule \n\t\t\t\\multirow{2}*{DART} \n\t\t\t& BLEU & 50.62 & 51.72 & 50.38 \\\\\n\t\t\t& Human & 90.7\\% & 91.7\\% & 87.7\\% \\\\\n            \\midrule\n\t\t\t\\multirow{2}*{ToTTo}\n\t\t\t& BLEU & 48.29 & 48.95 & 48.95 \\\\\n\t\t\t& Human & 78.7\\% & 80.0\\% & 81.3\\% \\\\\n            \\midrule\n\t\t\t\\multirow{2}*{KVRET}\n\t\t\t& BLEU & 20.04 & 18.84 & 17.75 \\\\\n\t\t\t& Human$^{\\dag}$ & 72.3\\% & 66.3\\% & 75.0\\% \\\\\n            \\midrule\n\t\t\t\\multirow{2}*{SQL2Text}\n\t\t\t& BLEC & 93.69 & 93.35 & 92.71 \\\\\n\t\t\t& Human$^{*}$ & 83.7\\% & 90.3\\% & 84.7\\% \\\\\n            \\midrule\n\t\t\t\\multirow{2}*{Logic2Text}\n\t\t\t& BLEC & 92.15 & 92.88 & 91.69 \\\\\n\t\t\t& Human$^{\\dag}$ & 77.2\\% & 81.5\\% & 84.2\\% \\\\\n\t\t\t\\bottomrule\n\t\\end{tabular}\n\t\\end{adjustbox}\n\t\\caption{Automatic metrics and human evaluation on the development set of generation tasks. $^{*}p<0.05$ for ``the rank-1 model is better than the rank-2 model''. $^{\\dag}p<0.05$ for ``the rank-2 model is better than the rank-3 model''. Automatic metrics do not always reflect human evaluation. Larger models are not always better. }\n\t\\label{tab:human-eval}\n\\end{table}\n\n\\iffalse\n\\begin{figure}[t]\n\\centering\n    \\subfigure[FeTaQA]{\n        \\includegraphics[width=0.46\\linewidth]{figures/human_and_error/FeTaQA-human.pdf}\n    }\n    \\subfigure[DART]{\n        \\includegraphics[width=0.46\\linewidth]{figures/human_and_error/DART-human.pdf}\n    }\n    \\subfigure[ToTTo]{\n        \\includegraphics[width=0.46\\linewidth]{figures/human_and_error/ToTTo-human.pdf}\n    }\n    \\subfigure[KVRET]{\n        \\includegraphics[width=0.46\\linewidth]{figures/human_and_error/KVRET-human.pdf}\n    }\n    \\subfigure[SQL2Text]{\n        \\includegraphics[width=0.46\\linewidth]{figures/human_and_error/SQL2Text-human.pdf}\n    }\n    \\subfigure[Logic2Text]{\n        \\includegraphics[width=0.46\\linewidth]{figures/human_and_error/Logic2Text-human.pdf}\n    }\n\\caption{\\label{fig:human-eval} Development set performance measured by automatic metrics and human for generation tasks. Generation metrics do not always reflect human evaluation. Larger models are not always better. Human evaluation statistical test: 1) on FeTaQA and SQL2Text, we have $p<0.05$ for ``the rank-1 model is better than the rank-2 model''; 2) on FeTaQA, KVRET, and Logic2Text, we have $p<0.05$ for ``the rank-2 model is better than the rank-3 model''.\n\\ziyu{larger font size in figures?}\n\\tao{take too much space to show a non-key discussion}\n}\n\\end{figure}\n\\fi\\iffalse\n\\begin{table*}[t]\n\t\\centering\n\t\\small\n\t\\begin{adjustbox}{width=\\linewidth}\n\t\\begin{tabular}{@{}l@{}c@{}c@{}c@{}c@{}c@{}ccccccc@{}}\n\t\t\t\\toprule\n\t\t\t& \\multicolumn{6}{c}{Invalid / valid-but-wrong} & \\multicolumn{6}{c}{Missing-information / contradiction / hallucination / ungrammatical (\\%)} \\\\ \n\t\t    \\cmidrule(lr){2-7} \\cmidrule(l){8-13}\n\t\t\t& \\ \\ \\  Spider \\ \\ \\ & \\ \\ SParC \\ \\ & \\ \\ \\ CoSQL \\ \\ & GrailQA \\ & \\ WebQSP \\ & \\ MTOP & FeTaQA & DART & ToTTo & KVRET & SQL2Text & Logic2Text \\\\ \n\t\t\t\\midrule \n\t\t\tT5-base       & 268/140 & 298/257 & 268/248 & 14/29 & 28/62 & 2/358 & 35/34/6/5 & 4/2/2/3 & 16/4/10/2 & 14/16/0/4 & 5/9/2/3 & 7/13/2/7 \\\\\n\t\t\tT5-large      & 167/144 & 181/293 & 228/219 & 13/20 & 33/53 & 1/341 & 33/23/4/4 & 4/2/1/4 & 12/3/10/3 & 22/17/1/3 & 4/5/2/2 & 5/13/1/4 \\\\\n\t\t\tT5-3B         & 121/144 & 150/243 & 131/175 & 14/17 & 29/51 & 2/336 & 33/17/4/2 & 6/3/2/4 & 10/3/13/2 & 13/18/2/2 & 7/6/1/3 & 7/9/2/3 \\\\ \n\t\t\t\\bottomrule\n\t\\end{tabular}\n\t\\end{adjustbox}\n\t\\caption{Error analysis. For semantic parsing, we show the number of invalid/valid-but-wrong outputs. For generation tasks, we show the proportion of missing-information/contradiction/hallucination/ungrammatical outputs among all outputs. \\todo{Noah: better way to present} \\tao{plot instead!} }\n\t\\label{tab:error_distribution}\n\\end{table*}\n\\fi \n\n\\begin{figure}[t]\n\\centering\n    \\subfigure[Spider]{\n        \\includegraphics[width=0.45\\linewidth]{figures/human_and_error/Spider-error.pdf}\n    }\n    \\subfigure[GrailQA]{\n        \\includegraphics[width=0.45\\linewidth]{figures/human_and_error/GrailQA-error.pdf}\n    }\n    \\subfigure[FeTaQA]{\n        \\includegraphics[width=0.45\\linewidth]{figures/human_and_error/FeTaQA-error.pdf}\n    }\n    \\subfigure[ToTTo]{\n        \\includegraphics[width=0.45\\linewidth]{figures/human_and_error/ToTTo-error.pdf}\n    }\n\\caption{\\label{fig:error-analysis} Error analysis. For semantic parsing, we plot the number of invalid/valid-but-wrong predictions. For generation, we plot the proportion of missing-information/contradiction/hallucination/ungrammatical errors among all predictions (one prediction may have multiple errors). Full visualization is in Appendix \\ref{app:all-results}. } \n\\end{figure}\n\n\\subsection{Human Evaluation for Generation Tasks}\n\\label{subsec:human-eval}\nFor each generation task, we randomly sample 100 development set samples and ask human annotators to judge the correctness of each output, using a 0-1 score. Details are provided in Appendix \\ref{subapp:human-evaluation}. \nTable \\ref{tab:human-eval} shows that automatic metrics do not always reflect human evaluation, calling for better automatic metrics to truly reflect the model's ability on generation tasks. \nLarger models are not always better, and detailed error analysis is provided below. \n\n\\subsection{Error Analysis}\n\\label{subsec:error}\n\n\\noindent\\textbf{Error analysis based on output validity \\ \\ }  \nUnconstrained decoding from PLMs may generate \\textit{invalid outputs}. \nFor semantic parsing, we divide wrong outputs into \\textit{invalid outputs} (i.e., not executable when the output is SQL, and not parse-able when the output is s-expression or TOP-representation) and \\textit{valid but wrong answers}. \nFigure \\ref{fig:error-analysis} shows that, for SQL semantic parsing, a large number of errors are caused by invalid outputs, and the number of invalid outputs gradually decreases with the increase of model size. This phenomenon is also observed by \\citet{Scholak2021:PICARD}, who used constrained decoding to improve the validity, largely improving the parsing performance.\nFor s-expression semantic parsing, invalid outputs take up 30--50\\% of all wrong outputs, and increasing the model size does not reduce invalidity significantly. \nFor fact verification tasks, valid outputs are ``entailed'' and ``refuted''. \nWe observe that T5 always generates valid outputs. \nFor question answering, we do not include the validity analysis since the validity check for an answer is non-trivial and could be imprecise. \n\n\\noindent\\textbf{Error analysis for text generation tasks \\ \\ } \nFor generation tasks, we consider four types of errors: \\textit{missing information} (required information is not shown in the output), \\textit{contradiction} (the output is contradictory to the input), 3) \\textit{hallucination} (the output contains information that cannot be verified by the input), and 4) \\textit{ungrammatical}. \nFigure \\ref{fig:error-analysis} shows that the proportion of ungrammatical outputs is generally less than 5\\%. \nMissing information and contradiction are common errors made by T5, and performance gains generally come from reducing contradiction. \nHallucination is not a common error made by T5 except for the highlighted-table-to-text task (ToTTo), where T5 tends to output information of non-highlighted cell values.\n\n\\noindent\\textbf{Case study \\ \\ }\nWe summarize some interesting observations about the model output (more in Appendix~\\ref{app:case-study}).\nCompared with T5-base and T5-large, T5-3B's outputs for text generation tasks tend to be more diverse and creative as shown in Appendix \\ref{app:fetaqa_case} and \\ref{app:totto_case}.\nAlso, T5-3B sometimes leverages domain knowledge to summarize facts in some tasks such as DART (e.g., describing \\textit{rating 5 out of 5} as \\textit{low}), while the other two copy the original expressions in the input, as shown in Appendix \\ref{app:dart_case} and \\ref{app:logic2text_case}.\nHowever, this ability puts T5-3B in the risk of manipulating information and meaning of user request as shown in Appendix \\ref{app:kvret_case2} and \\ref{app:sql2text_case}. \n\n\\section{Conclusions}\nIn this paper, we propose the \\uskg framework to promote systematic research on structured knowledge grounding by unifying \\ntasks \\skg tasks. Using \\uskg as a benchmark, we demonstrate that finetuning T5 on individual tasks achieves state-of-the-art results on almost all \\ntasks tasks. \nWe show that multi-task prefix-tuning benefits most \\skg tasks, largely improving the overall performance. \nFor structured knowledge encoding, we find that the effectiveness of encoding variations varies across tasks. \nMoreover, \\uskg is a challenging testbed for zero-shot and few-shot learning, shown by the poor results of large PLMs. \n\n\\section{Limitations}\n\n\\uskg establishes a powerful and reproducible starting point for \\skg research. New models can be easily applied to diverse \\skg tasks, and new tasks can be easily framed based on our standardized abstraction.\n\\uskg promotes a systematic study on more general and robust advances in structured knowledge encoding, multi-task learning, zero-shot learning, and few-shot learning for \\skg tasks.\nIt also would be interesting to explore general pretraining methods within \\uskg, which potentially benefit all the unified tasks. \nWhen the structured knowledge is too large for GPU memory, we truncate them based on heuristic rules,\ncalling for future study on 1) incorporating retrieval component in \\skg, \n2) designing sparse attention in T5 for structured knowledge or other means to improve model efficiency. \n\n\\uskg currently provides the correct type of structured knowledge for each task. \nHowever, how a system searches for the correct structured knowledge resources, takes appropriate action, and integrates information and results from multiple structured sources given a user request is still under-explored, which are a prerequisite for building a unified multi-purpose \\skg system. \n\nSince we select popular tasks from each task family, we risk disproportionality in terms of the data language,  domain and population, and we actively welcome diverse, multi-lingual tasks to be added into \\uskg. \nAlso, the error analysis of \\skg can more fine-grained, and we hope our findings can promote future work on systematically studying and decomposing the behavior of PLMs on \\skg tasks. \nFurthermore, training and evaluation data should reflect the intents and linguistic phenomena in the real world \\cite{Vries2020TowardsEV}, suggesting more realistic tasks to be added into \\uskg. \n\\clearpage\n\n\\clearpage\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning}\n\n\\begin{document}\n\n\\maketitle\n\n\\renewcommand{\\thefootnote}{\\fnsymbol{footnote}}\n\\footnotetext{\\includegraphics[scale=0.02]{first.png} Equal contribution.}\n\\footnotetext[3]{Work done during an intern at Alibaba DAMO Academy.}\n\\footnotetext[2]{Corresponding authors.}\n\n\\begin{abstract}\nTable-based reasoning has shown remarkable progress in a wide range of table-based tasks.\nIt is a challenging task, which requires reasoning over both free-form natural language (NL) questions and (semi-)structured tabular data. However, previous table-based reasoning solutions usually suffer from significant performance degradation on ``huge'' evidence (tables). In addition, most existing methods struggle to reason over complex questions since the essential information is scattered in different places. To alleviate the above challenges, we exploit large language models (LLMs) as decomposers for effective table-based reasoning, which (i) decompose huge evidence (a huge table) into sub-evidence (a small table) to mitigate the interference of useless information for table reasoning, and (ii) decompose a complex question into simpler sub-questions for text reasoning. First, we use a powerful LLM to decompose the evidence involved in the current question into the sub-evidence that retains the relevant information and excludes the remaining irrelevant information from the ``huge'' evidence. \nSecond, we propose a novel ``parsing-execution-filling'' strategy to decompose a complex question into simper step-by-step sub-questions by generating intermediate SQL queries as a bridge to produce numerical and logical sub-questions with a powerful LLM.\nFinally, we leverage the decomposed sub-evidence and sub-questions to get the final answer with a few in-context prompting examples. \nExtensive experiments on three benchmark datasets (TabFact, WikiTableQuestion, and FetaQA) demonstrate that our method achieves significantly better results than competitive baselines for table-based reasoning. \nNotably, \\textbf{our method outperforms human performance for the first time on the TabFact dataset}. In addition to impressive overall performance, our method also has the advantage of interpretability, where the returned results are to some extent tractable with the generated sub-evidence and sub-questions.\n\\end{abstract}\n\n\\section{Introduction}\n\nTabular data can be an important complement to textual data, which is informative and ubiquitous in our daily lives. Reasoning about tabular and textual information is a fundamental problem in natural language understanding (NLU) and information retrieval (IR) \\citep{wang2021retrieving}. \nIt can benefit various downstream applications such as table-based fact verification (FV) \\citep{tabfact,feverous,infotabs} and table-based question answering (QA) \\citep{wtq,wikisql,fetaqa,cho2019sigirtable}. As shown in Figure~\\ref{intro}, table-based reasoning is challenging since it involves sophisticated textual, numerical, and logical reasoning across both unstructured text and (semi-)structured tables. \nTo address the above challenge, table-based reasoning is conventionally accomplished by synthesizing executable languages (e.g., SQL and SPARQL) to interact with tables \\citep{sqlguide,sparql,Hui2021DynamicHR,Hui2022S2SQLIS}. However, these methods ignore the semantics of text chunks inside the table, struggling to effectively model web tables with unnormalized free-form text in table cells.\n\n\\begin{wrapfigure}{l}{0.5\\textwidth}\n    \\centering\n    \\setlength{\\abovecaptionskip}{5pt}\n    \\includegraphics[width=0.5\\textwidth]{intro.pdf}\n    \\caption{An example of table-based reasoning.}\n    \\label{intro}\n\\end{wrapfigure}\n\nRecently, table-based pre-trained models\\citep{tapas,tapex,omnitab,pasta,Cai2022STARSG} have proved to be powerful in enhancing reasoning skills on both textual and tabular data, which benefit from the rich knowledge learned from large-scale crawled or synthesized tabular and textual data. For example, TaPas~\\citep{tapas} enhanced the understanding of tubular data by recovering masked cell information in  tables. \nNevertheless, these models generally need to be fine-tuned on a significant amount of task-specific downstream datasets, struggling to achieve excellent performances when dealing with new datasets with unseen reasoning types. In addition, fine-tuning a pre-trained model on specific tasks generally destroys its in-context ability \\citep{wang2022preserving}. Instead of fine-tuning the pre-trained models, in-context learning in large language models (LLMs) \\citep{gpt3,cot,zerocot,min2022rethinking} has recently gained noticeable attention to exploring the reasoning ability of LLMs, where several input–output exemplars are provided for prompting. For example, the \\textit{chain of thought} prompting \\citep{cot} has been discovered to empower LLMs to perform complex reasoning by generating intermediate reasoning steps.\n\nThe prior studies \\citep{cot,zerocot} revealed that LLMs could achieve impressive performance on many text reasoning tasks without task-specific model designs and training data. However, the capability of LLMs on table-based reasoning tasks is still under-explored.  \nThere are several technical challenges in leveraging LLMs for table-based reasoning with huge tables and complex questions. \\textbf{First}, since a table could potentially contain a large number of rows and columns, directly encoding all table content via pre-trained models could be computationally intractable and interfere with huge irrelevant information. As pointed out in \\citep{llmtab}, the LLMs are unable to generalize to ``huge'' tables with 30+ rows due to the token limitation. Although previous studies have leveraged some approaches \\citep{yin2020tabert,tabfact} such as text matching to retrieve sub-evidence, these approaches usually require a large amount of domain-specific data for training, as the sub-evidence retrieval process requires deep understanding and reasoning of questions and tables. \n\\textbf{Second}, decomposing a complex question into simpler sub-questions can effectively facilitate multi-step reasoning of a large model \\citep{huang2022language,dua2022successive,pot}. However, directly decomposing a complex question by leveraging chain-of-thought prompting \\citep{cot} could easily fall into a hallucination dilemma \\citep{ji2022survey}, where the models may generate misleading sub-questions containing information that is inconsistent with the evidence. \nChen \\textit{et al.} \\citep{llmtab} showed that LLMs could sometimes make simple mistakes when performing symbolic operations.\nThis will affect the process of subsequent reasoning, thus we need a more reliable table-based method for complex question decomposition\n\nTo mitigate the aforementioned challenges, in this paper, we explore in-context learning in LLMs to \\underline{D}ecompose evidence \\underline{A}nd questions for effective \\underline{T}able-bas\\underline{E}d \\underline{R}easoning (\\textbf{\\textsc{Dater}}). \nFirst, we exploit a powerful LLM to decompose the (semi-)structured evidence (a huge table) involved in the current question into relevant sub-evidence (a small table). We implement sub-evidence extraction by predicting the indexes of the rows and columns with the help of a powerful LLM and a few prompts.\nIn this way, we can retain the relevant evidence and exclude the remaining irrelevant evidence from interfering with the decision. In addition, our method has the advantage of interpretability, where the returned table-based results are tractable.\nSecond, we propose a ``\\textit{parsing-execution-filling}'' strategy which explores the programming language SQL to decompose the complex unstructured natural language (NL) question into the logical and numerical computation. Concretely, we generate an abstract logic sub-question by masking the span of numerical values and then converting the abstract logic into SQL query language that is executed on the evidence for obtaining a reliable sub-question. Finally, we leverage the decomposed sub-evidence and sub-questions to get the final answer with the help of a few in-context prompting examples.  \n\nThe main contributions of this paper are listed as follows:\n\\begin{itemize}\n    \\item We reduce ``huge'' evidence (a huge table) into ``small'' sub-evidence (a small table) by predicting the related indexes of rows and columns of the evidence with the help of a powerful LLM and a few prompting examples. Our evidence decomposition method makes the reasoners focus on the essential sub-evidence related to the given question.\n    \\item We propose a novel ``parsing-execution-filling'' strategy to decompose a complex question into simper step-by-step sub-questions by generating an intermediate SQL as a bridge to produce numerical and logical sub-questions with the help of a powerful LLM and a few prompting examples. Our question decomposition method has proven to be effective in table-based reasoning without requiring a large amount of annotated training data. \n    \\item We conduct extensive experiments on three benchmark datasets belonging to table-based fact verification and table-based question answering tasks. Experimental results demonstrate that our Dater method achieves significantly better results than competitive baselines for table-based reasoning. Particularly, Dater outperforms human performance for the first time on the TabFact dataset. \n    \\item In addition to impressive overall performance, our Dater also has the advantage of interpretability, where the returned results are to some extent tractable with the generated sub-evidence and sub-questions.\n\\end{itemize}\n\n\\section{Related Work}\n\\subsection{Table-based Reasoning}\nTable-based reasoning requires reasoning over both free-form natural language (NL) questions and (semi-)structured tables. Traditional methods produce executable languages (e.g., SQL and SPARQL) to access the tabular data \\citep{sqlguide,sparql}. However, these methods cannot capture the semantics of text chunks inside a table and fail to model web tables with free-form text in table cells. \nRecently, several table-based reasoning benchmarks, such as TabFact \\citep{tabfact}, WikiTableQuestion \\citep{wtq}, and FetaQA \\citep{fetaqa},  have been proposed to help learn different types of table-based tasks.  The availability of large-scale training data has significantly enhanced the performance of table-based reasoning with the help of deep learning techniques \\citep{neeraja2021incorporating,feverous}.  \n\nIn parallel, table pre-training has been proposed to encode both tables and texts, which further improves the performance of table-based reasoning. \nInspired by the success of masked language modeling (MLM), TaPas~\\citep{tapas} enhanced the understanding of tubular data by recovering masked cell information in the table. TAPEX~\\citep{tapex} utilized the BART model to imitate the SQL executor in the pre-training stage so that TAPEX can obtain better table reasoning capability. ReasTAP~\\citep{reastap} designed pre-training tasks based on the reasoning skills of table-based tasks, injecting reasoning skills via pre-training. \n\\textsc{TaBERT} \\citep{yin2020tabert} proposed content snapshots to encode a subset of table content that was most relevant to the input utterance. \nPASTA~\\citep{pasta} introduced a table-operations aware\nfact verification approach, which pre-trained LMs to be aware of common table-based operations with sentence-table cloze questions synthesized from WikiTables.\nSubsequently, \\citep{llmtab,binder} explored the ability of LLMs for table-based tasks.\n\n\\subsection{Large Language Models on Reasoning}\nLarge language models (LLMs) have been shown to confer a range of reasoning abilities, such as arithmetic \\citep{lewkowycz2022solving}, commonsense \\citep{liu2022rainier} and symbolic reasoning \\citep{zhou2022least}, as the model parameters are scaled up \\citep{gpt3}.\nNotable, chain-of-thought (CoT) \\citep{cot} leverages a series of intermediate reasoning steps, achieving better reasoning performance on complex tasks. \nBased on CoT, a number of advanced improvements have been proposed, including ensemble process \\citep{cotsc}, iterative optimization \\citep{zelikman2022star}, and example selection \\citep{creswell2022selection}.\nNotably, ZeroCoT \\citep{zerocot} improved reasoning performance by simply adding ``Let's think step by step'' before each answer.\nFu \\textit{et al.} \\citep{fu2022complexity} proposed complexity-based prompting can generate more reasoning steps for the chain, and achieve significantly better performance.\nZhang \\textit{et al.} \\citep{zhang2022automatic} selected examples of in-context automatically by clustering, without the need for manual writing.\nDespite the remarkable performance of LLMs in textual reasoning, their reasoning capabilities on tabular tasks are still limited.\nThe two most relevant works to this paper are  \\citep{binder} and \\citep{llmtab}, but none of them focus on the ability of a powerful LLM to decompose the evidence (tables), and the reliability of the reasoning steps.\n\n\\begin{figure*}\n    \\centering\n    \\setlength{\\abovecaptionskip}{5pt} \n    \\includegraphics[width=0.9\\textwidth]{dater.pdf}\n    \\caption{The overview of our Dater framework for table-based reasoning. We first use a powerful LLM (Codex) to probe sub-evidence and sub-questions by performing in-context learning. To obtain a reliable sub-question, we propose a novel ``parsing-execution-filling'' strategy to alleviate hallucination issues. Ultimately, the reasoner browses through the sub-evidence and sub-questions to get the final answer.\n    }\n    \\label{pipeline}\n\\end{figure*}\n\n\\subsection{Question Decomposition}\nQuestion decomposition is essential to understand complex questions. Early work \\citep{kalyanpur2012fact} utilized a suite of decomposition rules for question decomposition based on lexicon-syntactic features. The drawback of rule-based methods is that they need experts to manually design rules, making it difficult to extend the rules to new tasks or domains. In recent years, neural models \\citep{talmor2018web,zhang2019complex} have been proposed to perform question decomposition in an end-to-end manner.\nZhang \\textit{et al.} \\citep{zhang2019complex} proposed a hierarchical semantic parsing method based on a sequence-to-sequence model, which combined a question decomposer and an information extractor. \nHowever, these supervised methods rely on a large amount of annotated training data which is labor-intensive to obtain. In parallel, unsupervised decomposition was proposed to produce sub-questions without strong supervision. For example, Perez \\textit{et al.} \\citep{perez2020unsupervised} automatically produced a noisy ``pseudo-decomposition'' for\neach complex question, which trained a\ndecomposition model on the crawled data with unsupervised sequence-to-sequence learning. \nDifferent from previous decomposition methods, we explore LLMs as versatile decomposers, which decompose both huge evidence and complex questions for table-based reasoning with the help of a powerful LLM and a few prompting examples. \n\n\\section{Problem Formulation and Notations}\nEach instance in table-based reasoning consists of a table $T$, a natural language question $Q$, and an answer $A$. In particular, each table $T=\\{v_{i,j}|i<= {Row}_T,j<={Col}_T\\}$ contains ${Row}_T$ rows and ${Col}_T$ columns, with $v_{i,j}$ representing the content in the $(i, j)$-th cell. A question $Q = <q_1,{\\cdots} ,q_n >$ consists of $n$ tokens. In this paper, we focus on two table-based reasoning tasks, including table-based fact verification (FV) and table-based question answering (QA). For table-based FV, the final answer $A \\in \\{0, 1\\}$ is a boolean value that determines the truth or falsity of the input statement. For table-based QA, the answer is a natural language sequence $A = <a_1,{\\cdots}, a_n> $ that answers the question described by the input statement.\n\n\\section{Method}\n\\subsection{In-context Learning}\nGiven a few examples with instructions about detailed specific tasks, Large Language Models (LLMs) can perform in-context learning without training, which learn from analogies.\nThe powerful ability has been widely verified on nature language tasks, including text classification, semantic parsing, mathematical reasoning, etc.\nInspired by the advances of LLMs, we aim to explore whether LLMs could tackle reasoning tasks on structured evidence (i.e., tabular data).\nFormally, the final answer $A_{\\text{test}}$ can be obtained by predicting $p(A_{\\text{test}} \\mid T_{\\text{test}}, Q_{\\text{test}}, C)$ with evidence table $T_{\\text{test}}$ and question $Q_{\\text{test}}$. \nHere, $C=\\left\\{C_1, \\ldots, C_{|C|}\\right\\}$ is a small set of in-context prompts from manual writing, where each example $C_i = (T_{\\text{prompt}}^i, Q_{\\text{prompt}}^i, A_{\\text{prompt}}^i)$. \nWe provide the detailed in-context prompt as prompt 4.1.\n\n\\begin{figure}[htbp]\n\\centering\n\\begin{minipage}[t]{0.48\\textwidth}\n\\centering\n\\includegraphics[width=6cm]{p41.pdf}\n\\end{minipage}\n\\begin{minipage}[t]{0.48\\textwidth}\n\\centering\n\\includegraphics[width=6cm]{p42.pdf}\n\\end{minipage}\n\\end{figure}\n\nBased on empirical observations, performing in-context learning directly by the above prompt 4.1 cannot achieve optimal performance. Table-based reasoning is a sophisticated and complex task that requires human-like thinking and a fine-grained understanding process. Recently, chain-of-thought \\citep{cot} proposed a question decomposition method, which induces LLMs to perform step-by-step thinking, resulting in better reasoning. \nHowever, they did not break down the evidence, resulting in unsatisfactory performance. Specifically, dealing with the ponderous tables directly is usually inefficient and easily interfered with by huge irrelevant information. %redundant. \nTo this end, we introduce evidence and question decomposition to help large models perform fine-grained reasoning over both evidence (tables) and questions.\n\n\\subsection{Evidence Decomposer}\nHumans process table-based reasoning tasks by observing sub-evidence related to the question at hand to complete their judgment. In this paper, we hope to use evidence decomposition to imitate this process.\nAlthough previous studies have leveraged some approaches \\citep{yin2020tabert,tabfact} such as text matching to retrieve sub-evidence, empirical results suggest that these approaches are often imperfect and require a large amount of domain-specific data for training, as the sub-evidence retrieval process relies on strong commonsense as well as domain knowledge and requires a joint understanding and reasoning of questions and tables.\nTo this end, we use a powerful LLM to break down the evidence (tables) involved in the current question, retaining the relevant evidence and excluding the remaining irrelevant evidence from interfering with the decision. \nConcretely, we implement sub-evidence extraction by predicting the indexes of the rows and columns with the help of a powerful LLM and a few prompts.\nFormally, in the in-context learning stage, the row index $\\text{Row}_{\\text{test}} = \\{\\text{Row}_1, \\text{Row}_2, ... \\text{Row}_{|\\text{Row}|}\\}$ and column index $\\text{Col}_{\\text{test}} = \\{\\text{Col}_1, \\text{Col}_2, ... \\text{Col}_{|\\text{Col}|}\\}$ of sub-evidence table $\\hat{T}_{\\text{test}}$ can be obtained by predicting $p(\\text{Row}_{\\text{test}}, \\text{Col}_{\\text{test}} \\mid T_{\\text{test}}, Q_{\\text{test}}, C^\\text{ED})$ with complete evidence $T_{\\text{test}}$ and corresponding question $Q_{\\text{test}}$. $C^{\\text{ED}}=\\left\\{C^\\text{ED}_1, \\ldots, C^\\text{ED}_{|\\text{C}|}\\right\\}$ is a small set of in-context examples, where each $C^\\text{ED}_i$ is the example instance $(\\text{Row}_\\text{prompt}^i, \\text{Col}_\\text{prompt}^i, T_{\\text{prompt}}^i, Q_{\\text{prompt}}^i)$. Some detailed prompts are illustrated as prompt 4.2.\n\n\\subsection{Questions Decomposer}\nDecomposing complex questions into step-by-step sub-questions can effectively facilitate the reasoning process of a large model, which has been proven to be effective in numerical and commonsense reasoning \\citep{huang2022language,dua2022successive,pot}. However, we observed that \nstraightforwardly decomposing a complex question by leveraging a chain-of-thought process could easily fall into a hallucination dilemma, i.e., the LLM might not faithfully generate content consistent with the given evidence (tables), especially when numerical values were involved. This will affect the process of subsequent reasoning, thus we need a reliable method for the sub-question generation.\n\n\\subsubsection{Reliable Sub-questions}\nTo effectively decompose a complex question into step-by-step sub-questions, we propose a ``\\textbf{parsing-execution-filling}'' strategy to extend the vanilla chain-of-thought method by exploring the programming language SQL to divide logical steps and numerical computation. \nSpecifically, we first generate an abstract logic sub-question, where we mask the span of numerical values by using cloze style and then convert the abstract logic into SQL queries, similar to text-to-SQL parsing \\citep{Qin2022ASO,Wang2022ProtonPS}. \nAfterward, we execute the SQL language on the evidence to get the final result for back-filling, yielding a reliable sub-question.\nFor example, as shown in the bottom of Figure \\ref{pipeline}, given a question \n``\\textit{during the 2007 - 08 minnesota wild season, minnesota played at home more times than they played away.}''\n, we first mask the spans involving numerical values in sub-questions in prompting examples, and the remaining part can be regarded as the logical question. Here, the logical sub-questions are \n``\\textit{q1: \\{...\\} times minnesota played at home.}'' and ``\\textit{q2: \\{...\\} times minnesota played away}.'' Formally, a sub-question $\\hat{Q}_{\\text{test}}$ can be obtained by predicting $p(\\hat{Q}_\\text{test} \\mid T_{\\text{test}}, Q_{\\text{test}}, C^{\\text{QD}})$ with the complete evidence $T_{\\text{test}}$ and complete question $Q_{\\text{test}}$. \n$C^{\\text{QD}}=\\left\\{C^{\\text{QD}}_1, \\ldots, C^{\\text{QD}}_{|\\text{C}|}\\right\\}$ is a small set of in-context prompting examples, where each $C^{\\text{QD}}_i$ is an example instance $(T_{\\text{prompt}}^i, Q_{\\text{prompt}}^i, \\hat{Q}_{\\text{prompt}}^i)$.\nThe detailed prompt is as prompt 4.3.\n\\begin{figure}[htbp]\n\\centering\n\\begin{minipage}[t]{0.48\\textwidth}\n\\centering\n\\includegraphics[width=6cm]{p43.pdf}\n\\end{minipage}\n\\begin{minipage}[t]{0.48\\textwidth}\n\\centering\n\\includegraphics[width=6cm]{p44.pdf}\n\\end{minipage}\n\\end{figure}\n\nThe logical statement is then parsed into the SQL queries ``\\texttt{SELECT COUNT(*) FROM w WHERE home = 'minnesota'}'' and \n``\\texttt{SELECT COUNT(*) FROM w WHERE visitor = 'minnesota'}'',\nwhich are further executed on the evidence, yielding reliable spans ``\\textit{6}'' and ``\\textit{8}'' to be backfilled into the placeholder $\\{...\\}$ of sub-questions. In this way, we can obtain the reliable sub-questions \n``\\textit{q1: \\{6\\} times minnesota played at home.}''\nand ``\\textit{q2: \\{8\\} times minnesota played away.}''. \n\nFormally, the SQL query $\\hat{S}_{\\text{test}}$ can be obtained by predicting $p(\\hat{S}_\\text{test} \\mid T_{\\text{test}}, \\hat{Q}_{\\text{test}}, C^{\\text{PR}})$ with the help of complete evidence $T_{\\text{test}}$ and sub-question $Q_{\\text{test}}$. Here, \n$C^{\\text{PR}}=\\left\\{C^{\\text{PR}}_1, \\ldots, C^{\\text{PR}}_{|\\text{C}|}\\right\\}$ is a small set of in-context examples, where each $C^{\\text{PR}}_i$ is an example instance $(T_{\\text{prompt}}^i, \\hat{Q}_{\\text{prompt}}^i, \\hat{S}_{\\text{prompt}}^i)$.\nThe detailed prompt is as prompt 4.4.\n\n\\subsection{Jointly Reasoning}\nAfter performing the above evidence and question decomposition, the reasoner leverages both sub-evidence $\\hat{T}_{\\text{test}}$ and reliable sub-questions $\\hat{Q}_{\\text{test}}$ to get a final answer $A_{\\text{test}}$ by predicting $p(A_{\\text{test}} \\mid \\hat{T}_{\\text{test}}, \\hat{Q}_{\\text{test}}, C^{\\text{JR}})$, where $C^{\\text{JR}}=\\left\\{C^{\\text{JR}}_1, \\ldots, C^{\\text{JR}}_{|\\text{C}|}\\right\\}$ is a small set of new in-context prompting examples. Here, each prompting example is denoted as $C^{\\text{JR}}_i = (\\hat{T}_{\\text{prompt}}^i, \\hat{Q}_{\\text{prompt}}^i, A_{\\text{prompt}}^i)$.\nThe detailed prompt as prompt 4.5.\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.5\\textwidth]{p45.pdf}\n    \\label{p45}\n\\end{figure}\n\n\\section{Experimental Setup}\n\n\\subsection{Datasets}\nWe evaluate our proposed method Dater on three table-based reasoning datasets, including TabFact \\citep{tabfact}, WikiTableQuestion \\citep{wtq}, and FetaQA \\citep{fetaqa}. \nGiven the request and cost constraints of the LLMs, we only evaluate our Dater on the test sets of these corpora, without fine-tuning it on the training sets. \nIt is noteworthy that LLMs were trained primarily on the web-crawled and code data. \nSince the pre-training data of LLMs does not contain tabular data, there is no risk of test data leakage. \nThe details of these three datasets are provided as follows.\n\n\\begin{itemize}\n    \\item \\textbf{TabFact} is a table-based fact verification benchmark, which contains statements written by crowd workers based on Wikipedia tables. For example, a \\textit{statement: ``industrial and commercial panel has four more members than the cultural and educational panel.''} is need to be judged whether it is \\textit{``True''} or \\textit{``False''} according to the given table.\n    We report the accuracy on the test-small set containing 2,024 statements with 298 tables.\n    \\item \\textbf{WikiTableQuestion} contains complex questions annotated by crowd workers based on Wikipedia tables. The crowd workers are asked to write questions that involve several complex operations such as comparison, aggregation and arithmetic operations, which require compositional reasoning over a series of entries in the given table. \n    We use the standard validation set and test set with 2,381 and 4,344 samples, respectively.\n    \\item \\textbf{FetaQA} contains free-form table questions that require deep reasoning and understanding. Most questions in FetaQA are based on information from discontinuous chunks in the table. \n    We evaluate Dater on the test set containing 2,003 samples. \n\\end{itemize}\n\n\\subsection{Evaluation Metrics}\nTabFact is used to evaluate table-based fact verification which aims to check if a statement is true based on tables. We adopt binary classification accuracy as the evaluation metric for the TabFact dataset. For WikiTableQuestion, we use denotation accuracy as our evaluation metric, which verifies whether the predicted answers equal the gold ones. Different from TabFact and WikiTableQuestion datasets that produce short phrase answers, the goal of FetaQA is to generate a complete long-form answer. Therefore, for FetaQA, we utilize BLEU \\citep{bleu}, ROUGE-1, ROUGE-2, ROUGE-L \\citep{rouge} as evaluation metrics.\n\n\\subsection{Implementation Details}\nIn the experiments, we employ GPT-3 Codex (code-davinci-002) as our large language model.  \nFor the final reasoning in-context learning step, we annotate 4, 2, and 6 prompting examples for TabFact, WikiTableQuestion, and FetaQA, respectively. \nTo obtain consistent results, we use a self-consistence decoding strategy \\citep{cotsc}. \n\n\\begin{table}\n\\centering\n\\begin{minipage}[t]{0.45\\linewidth}\n  \\centering\n  \\begin{tabular}{ll}\n    \\toprule\n    \\textsc{\\textbf{Model}} & \\textsc{\\textbf{Test}} \\\\\n    \\midrule\n    \\multicolumn{2}{c}{\\textit{ $\\heartsuit$ Fine-tuning based Methods}} \\\\\n    Table-BERT & 68.1 \\\\\n    LogicFactChecker & 74.3\\\\\n    SAT     &   75.5 \\\\\n    TaPas & 83.9\\\\\n    TAPEX & 85.9 \\\\\n    SaMoE & 86.7\\\\\n    PASTA & 90.8 \\\\\n    \\rowcolor[RGB]{237,237,237} \\quad w/ \\textbf{\\textsc{Dater}} & \\textbf{93.0} ($\\uparrow$ 2.2) \\\\\n    \\midrule\n    \\textit{\\texttt{Human}}& {92.1}\\\\\n    \\midrule\n    \\multicolumn{2}{c}{\\textit{ $\\spadesuit$ LLM based Methods}} \\\\\n    Binder & 85.1 \\\\\n    Codex & 72.6 \\\\\n    \\rowcolor[RGB]{237,237,237} \\quad w/ \\textbf{\\textsc{Dater}} & \\textbf{85.6} ($\\uparrow$ 13.0)  \\\\\n    \\bottomrule\n  \\end{tabular}\n  \\vspace{0.2cm}\n  \\caption{Experimental results on the official small test set of TabFact. Here, ``\\textit{Human}'' indicates the human performance.}\n  \\label{tab:tabfact_small}\n\\end{minipage}\n\\hspace{0.3cm}\n\\begin{minipage}[t]{0.5\\linewidth}\n  \\centering\n  \\begin{tabular}{lcc}\n    \\toprule\n    \\textsc{\\textbf{Model}} & \\textsc{\\textbf{Dev}} & \\textsc{\\textbf{Test}} \\\\\n    \\midrule\n    \\multicolumn{3}{c}{\\textit{$\\heartsuit$ Fine-tuning based Models}} \\\\\n    MAPO & 42.7 & 43.8 \\\\\n    MeRL & 43.2 & 44.1 \\\\\n    LatentAlignment & 43.7 & 44.5\\\\\n    IterativeSearch & 43.1 & 44.7\\\\\n    T5-3B  & - & 49.3 \\\\\n    TaPas  & 49.9 & 50.4 \\\\\n    TableFormer & 51.3 & 52.6 \\\\\n    TAPEX  & 58.0 & 57.2 \\\\\n    ReasTAP & 58.3 & 58.6\\\\\n    TaCube  & 60.9 & 60.8 \\\\\n    OmniTab &  61.3 & 61.2 \\\\\n    \\rowcolor[RGB]{237,237,237} \\quad w/ \\textbf{\\textsc{Dater}} & \\textbf{62.5} ($\\uparrow$ 1.2)\\ & \\textbf{62.5} ($\\uparrow$ 1.3)\\\\\n    \\midrule\n        \\multicolumn{3}{c}{\\textit{ $\\spadesuit$ LLM based Methods}} \\\\\n    Binder & 62.6 & 61.9 \\\\\n    Codex  & 49.3 & 47.6 \\\\\n    \\rowcolor[RGB]{237,237,237} \\quad w/ \\textbf{\\textsc{Dater}} & \\textbf{64.8}($\\uparrow$ 15.5)\\  & \\textbf{65.9} ($\\uparrow$ 18.3) \\\\\n    \\bottomrule\n  \\end{tabular}\n  \\vspace{0.2cm}\n  \\caption{\n  Experimental results on WikiTableQuestion with the official evaluator. \n  }\n  \\label{tab:wikitq}\n\\end{minipage}\n\\end{table}\n\n\\subsection{Baselines}\nWe compare the proposed Dater with a range of strong baseline methods that can be divided into two categories: fine-tuning based methods that require training on task-specific data and LLM-based in-context learning methods that do not require fine-tuning.\n\n\\paragraph{$\\heartsuit$ \\textbf{Fine-tuning based Methods}} \nFor the fine-tuning based methods, we compare the following methods:\n\\textbf{Table-BERT} \\citep{tabfact} adopts rule-based method to linearize a table into a natural language (NL) sentence. The linearized table and statement are then directly encoded using a BERT model.\n\\textbf{LogicFactChecker} \\citep{logicalfactchecker} employs a sequence-to-action semantic parser to generate a program, represented as a tree with multiple operations. A graph neural network is then utilized to encode statements, tables, and the generated programs.\n\\textbf{TaPas} \\citep{tapas} enhances the BERT architecture by incorporating the ability to encode tables as input. It is pre-trained on a joint dataset of text segments and tables collected from Wikipedia, allowing the model to better process tabular data and improving the overall performances on various downstream tasks.\n\\textbf{SAT} \\citep{sat} improves the model's ability to attend to the relevant information within a table by introducing a structure-aware mask matrix. In particular, it focuses on capturing low-level lexical information in the low layers and semantic information in the upper layers.\n\\textbf{TAPEX} \\citep{tapex} is designed to make the BART model imitate the SQL executor, which allows the model to acquire better table comprehension ability. The corpus of pre-trained SQL query templates are extracted from the SQUALL dataset. \n\\textbf{SaMoE} \\citep{samoe} introduces mixture-of-experts (MoE) \\citep{moesurvey} into the field of table-based fact verification, aiming to make different experts focus on different types of reasoning tasks. \n\\textbf{ReasTAP} \\citep{reastap} defines seven reasoning skills over semi-structured data, including in conjunction, temporal comparison, date difference, and so on. It infused tabular reasoning capability by pre-training the model on synthetic dataset.\n\\textbf{PASTA} \\citep{pasta} designs six types of sentence–table cloze tasks that are pre-trained on a synthesized corpus of 1.2 million items from WikiTables. The ``\\textit{fine-tuning with select-then-rank}'' strategy is used to adapt the input length to the model, which allows the model to handle long input sequences effectively. \n\\textbf{TableFormer} \\citep{tableformer} proposes a method that is robust to perturbations in table rows and columns, thereby improving the model's understanding of tables through the use of positional encoding.\n\\textbf{TaCube} \\citep{tacube} is a pre-computation-based approach aiming to improve the ability of PLMs in numerical reasoning. This method pre-computes aggregation/arithmetic results for the table in advance, making them readily available for PLMs when tackling question answering.\n\\textbf{OmniTab} \\citep{omnitab} proposes an omnivorous pre-training approach that consumes both natural and synthetic data to enhance models with abilities for both types of data.\n\n\\paragraph{$\\spadesuit$ \\textbf{LLM based Methods}} For the LLM based methods with in-context learning, we compare the following methods:\n\\textbf{Codex} \\citep{codex} directly generates final answer by performing in-context learning as shown in Prompt 4.1.  \n\\textbf{Binder} \\citep{binder} generates programming language programs and extends the capability of the programming language to solve commonsense problems. \n\n\\begin{table}\n  \\centering\n  \\begin{tabular}{lcccc}\n    \\toprule\n    \\textsc{\\textbf{Model}} & \\textsc{\\textbf{BLEU}} &  \\textsc{\\textbf{ROUGE-1}} & \\textsc{\\textbf{ROUGE-2}} & \\textsc{\\textbf{ROUGE-L}} \\\\\n    \\midrule\n    \\multicolumn{5}{c}{\\textit{$\\heartsuit$ Fine-tuning based Methods}} \\\\\n    T5-small    &   21.60   &   0.55    &   0.33    &   0.47    \\\\\n    T5-base     &   28.14   &   0.61    &   0.39    &   0.51    \\\\\n    T5-large    &   30.54   &   0.63    &   0.41    &   0.53    \\\\\n    \\midrule\n    \\multicolumn{5}{c}{\\textit{ $\\spadesuit$ LLM based Methods}} \\\\\n    Codex & 27.96  &   0.62   &   0.40   &0.52      \\\\\n    \\rowcolor[RGB]{237,237,237} \\quad w/ \\textbf{\\textsc{Dater}}   &   \\textbf{30.92}    &   \\textbf{0.66}   &   \\textbf{0.45}    &    \\textbf{0.56}    \\\\\n    \\bottomrule\n  \\end{tabular}\n  \\vspace{0.2cm}\n  \\caption{Evaluation on FeTaQA test set.}\n  \\label{tab:fetaqa}\n\\end{table}\n\n\\subsection{Main Results}\nWe conduct extensive experiments on TabFact, WikiTableQuesion and FetaQA. The experimental results on TabFact are summarized in Table~\\ref{tab:tabfact_small}. From the results, we can observe that our Dater model achieves a substantial improvement over the compared methods. \nIf we use LLM (Codex) as reasoner, our Dater achieves an accuracy of 85.6\\%, which is 13.0\\% higher than the result predicted by the Codex without evidence and questions decomposition stages. \nOn the other hand, when we use the fine-tuned model as reasoner in Dater framework, i.e., injecting intermediate decomposition generated by Dater into PASTA, the accuracy increases by 2.2\\% (93.0\\% vs. 90.8\\%) over PASTA.\nIt is noteworthy that \\textbf{Dater outperforms human performance for the first time on the TabFact dataset}.  \n\nTable~\\ref{tab:wikitq} provides the experimental results on the WikiTableQuestion dataset. \nDater achieves a new state-of-the-art accuracy of 65.9\\% on the test set, surpassing the best baseline method (i.e., Binder) by 4.0\\%.\nWhen injecting intermediate decomposition generated by Dater into OmniTab, we can still achieve a gain of 1.3\\% on the test set. \nIn addition, Dater has an absolute gain of 18.3\\% compared to the original Codex for table-based QA. The impressive improvement may be due to the fact that WikiTableQuestion has relatively large tables and complex questions, which can be effectively addressed by our table and question decomposition methods.\nTable~\\ref{tab:fetaqa} illustrates the experimental results on the FetaQA dataset.\nWe can observe that our Dater method achieves better results than the compared methods T5 and Codex, further validating the effectiveness of Dater.\n\n\\begin{table*}\n    \\centering\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{l|ccc|ccc|c}\n        \\toprule \n        {\\multirow{2}*{\\textbf{\\textsc{Model}}}} & \\multicolumn{3}{c|}{\\textbf{\\textsc{TabFact}}} & \\multicolumn{3}{c|}{\\textbf{\\textsc{WikiTableQuestion}}}& \\multicolumn{1}{c}{\\textbf{\\textsc{FeTaQA}}} \\\\\n        & All & Simple & Complex & All & Simple & Complex & BLEU  \\\\\n        \\midrule\n            \\textbf{\\textsc{Dater}} & \\textbf{85.6} & \\textbf{91.2} & \\textbf{80.0} & \\textbf{65.9} & \\textbf{68.2}& \\textbf{63.5}& \\textbf{30.92} \\\\\n            \\quad w/o \\textit{Evidence Decomposition} & 81.8 ($\\downarrow$ 3.8) & 86.9 ($\\downarrow$ 4.3) & 76.8 ($\\downarrow$ 3.2) & 63.9 ($\\downarrow$ 2.0) & 65.5 ($\\downarrow$ 2.7)& 62.2 ($\\downarrow$ 1.3) & 28.46 ($\\downarrow$ 2.46)  \\\\\n            \\quad w/o \\textit{Question Decomposition} & 81.9 ($\\downarrow$ 3.7) & 90.0 ($\\downarrow$ 1.2) & 74.1 ($\\downarrow$ 5.9) & 61.4 ($\\downarrow$ 4.5)& 63.6 ($\\downarrow$ 4.6) & 59.1 ($\\downarrow$ 4.4)& 30.73 ($\\downarrow$ 0.19) \\\\\n        \\bottomrule\n    \\end{tabular}\n    }\n    \\caption{Ablation results on the test sets of the three datasets.}\n    \\label{tab:main-ab}\n\\end{table*}\n\n\\subsection{Ablation Study}\nTo analyze the impact of two kinds of decomposition in Dater, we conduct an ablation study by discarding the evidence decomposition module (denoted as w/o \\textit{Evidence Decomposition}) and the question decomposition module (denoted as w/o \\textit{Question Decomposition}) on TabFact, WikiTableQuestion and FetaQA. The questions in the TabFact and WikiTableQuestion datasets can be further divided into two levels based on the difficulty of the questions:  \\textit{Simple} and \\textit{Complex}, which can be used to better evaluate the model performance on different questions. \nSpecifically, TabFact is divided based on officially provided question difficulty labels, while WikiTableQuestion is categorized by the length of the question.  \nWhile FetaQA questions are more similar in length and difficult to distinguish, so we do not divide them.\nThe ablation test results are reported in Table~\\ref{tab:main-ab}. \nIt is no surprise that combining both evidence and question composition achieves the best performance in all the experimental settings.\nFor FabFact, both evidence decomposition and question decomposition have large impacts on the performance of Dater. The accuracy of Dater without evidence/question decomposition decreases by 3.8\\%/3.7\\% on the \\textit{All} test set of FabFact. For WikiTableQuestion, question decomposition has a much larger impact than evidence decomposition on the performance of Dater. This is because WikiTableQuestion contains more complex questions that involve comparison, aggregation and arithmetic operations. This verifies the effectiveness of our question decomposition method in dealing with complex questions. \nFor FetaQA, the questions are simpler and involve less numerical reasoning, thus the performance bottleneck is mainly in table content understanding. The performance of Dater drops significantly when discarding evidence decomposition. \nAs FetaQA primarily focuses on the data-to-text generation, it is within our expectation that there is no significant gain from question decomposition. \n\n\\begin{table*}\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{lll}\n\\toprule\n\\rowcolor[RGB]{240,248,255} \\textsc{\\textbf{Semantic Understanding}} &&\\\\\n\\midrule\nQuestion & on august 25 , remlinger (6 - 6) took the win against the rockies.                                \\\\\nRule-based method & date, loss    & \\sqlwrong{}               \\\\\nDater & date, loss, score, opponent & \\sqlcorrect{}      \\\\\nGround Truth: & date, loss, score, opponent  &      \\\\\n\\midrule\n\\rowcolor[RGB]{240,248,255} \\textsc{\\textbf{Commonsense Knowledge}} &&\\\\\n\\midrule\nQuestion &  the top scorer (matias suarez) has 5 fewer goals than  bart goor , who has 11 goals.&\\\\\nRule-based method &  player, league &\\sqlwrong{}          \\\\\nDater &  player, total   & \\sqlcorrect{}           \\\\\nGround Truth: &  player, total    &      \\\\\n\\midrule\nQuestion &  denver did not lose more than one game in a row during november. & \\\\\nRule-based method &  date    & \\sqlwrong{}            \\\\\nDater &  team, date, score & \\sqlcorrect{}  \\\\\nGround Truth & team, date, score  &  \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\caption{Case study on evidence decomposition results predicted by a rule-based method and Dater. We categorize the questions according to the knowledge required for decomposition, including semantic understanding and commonsense knowledge.}\n  \\label{tab:col_select}\n\\end{table*}\n\n\\begin{table*}[h]\n  \\resizebox{\\textwidth}{!}{\n  \\begin{tabular}{ll}\n    \\toprule\n    \\rowcolor[RGB]{240,248,255} \\textsc{\\textbf{WikiTableQuestion}}  & \\\\\n    \\midrule\n    Question 1    & what is the number of listings from barrington, farmington, and rochester combined?  \\\\\n    Codex CoT   &   there are totally 1+1+1=3 listings from barrington, farmington, and rochester combined.\\\\\n    Codex Prediction &  ``3'' \\sqlwrong{}  \\\\\n    Dater Sub-question  & \\makecell[l]{the number of listings from barrington is \\{...\\}. the number of listings from farmington is \\{...\\}. \\\\ the number of listings from rochester is \\{...\\}.} \\\\\n    Dater Parsing & \\makecell[l]{SELECT COUNT(*) FROM w WHERE city or town = 'barrington' \\\\ SELECT COUNT(*) FROM w WHERE city or town = 'farmington' \\\\ SELECT COUNT(*) FROM w WHERE city or town = 'rochester'}\\\\\n    Dater CoT  & \\makecell[l]{the number of listings from barrington is \\{1\\}. the number of listings from farmington is \\{1\\}. \\\\ the number of listings from rochester is \\{3\\}.} \\\\\n    Dater Prediction   &   ``5'' \\sqlcorrect{} \\\\\n    Gold Answer   &   ``5''  \\\\\n    \\midrule\n    Question 2   &  does the team have more 5th or 7th place finishes during the regular season? \\\\\n    Codex CoT   &  the team have 1 5th place finishes and 2 7th place finish during the regular season. \\\\\n    Codex Prediction   &  ``7th'' \\sqlwrong{} \\\\\n    Dater Sub-question  &  the team have \\{...\\} 5th place finishes during the regular season. the team have \\{...\\} 7th place finishes during the regular season. \\\\\n    Dater Parsing & \\makecell[l]{ SELECT COUNT(*) FROM w WHERE \n    reg.season LIKE '\\%5th\\%'  \\\\ SELECT COUNT(*) FROM w WHERE reg.season LIKE '\\%7th\\%' }\\\\\n    \n    Dater CoT  &  the team have \\{3\\} 5th place finishes during the regular season. the team have \\{2\\} 7th place finishes during the regular season. \\\\\n    Dater Prediction  &   ``5th'' \\sqlcorrect{} \\\\\n    Gold  Answer  &   ``5th''\\\\\n    \\midrule\n    \\rowcolor[RGB]{240,248,255} \\textsc{\\textbf{TabFact}}  &  \\\\\n    \\midrule\n    Question 1   & pádraig harrington and graeme mcdowell are both fron northern ireland.   \\\\\n    Codex CoT  &  pádraig harrington is fron northern ireland.graeme mcdowell is fron northern ireland. \\\\\n    Codex Prediction &  True \\sqlwrong{} \\\\\n    Dater Sub-question  &  pádraig harrington is \\{...\\} fron northern ireland. graeme mcdowell is \\{...\\} fron northern ireland. \\\\\n    Dater Parsing & \\makecell[l]{\n    SELECT COUNT(*) FROM w WHERE player = 'pádraig harrington' AND country = 'northern ireland'\\\\\n    SELECT COUNT(*) FROM w WHERE player = 'graeme mcdowell' AND country = 'northern ireland'}\\\\\n    Dater CoT  &  pádraig harrington is \\{0\\} fron northern ireland. graeme mcdowell is \\{1\\} fron northern ireland. \\\\\n    Dater Prediction &  False \\sqlcorrect{} \\\\\n\n    Gold Answer   &  False \\\\\n    \\midrule\n    Question 2 & atlético ciudad played 28 matches with an average of less than 0.61 \n    \\\\\n\n    Codex CoT  & atlético ciudad played 28 matches. the average of atlético ciudad is 0.61.  \\\\\n    Codex Predict   &   True \\sqlwrong{}    \\\\\n    Dater Sub-question  &   atlético ciudad played \\{...\\} matches. atlético ciudad played \\{...\\} matches with an average of less than 0.61.\\\\\n    Dater Parsing &  \\makecell[l]{SELECT SUM(matches) FROM w WHERE team = 'atlético ciudad'\\\\SELECT COUNT(*) FROM w WHERE team = 'atlético ciudad' AND average < 0.61 }     \\\\\n    Dater CoT  &   atlético ciudad played \\{28\\} matches. atlético ciudad played \\{0\\} matches with an average of less than 0.61.\\\\\n    Dater Prediction & False  \\sqlcorrect{} \\\\\n    Gold Answer   & False \\\\\n    \\bottomrule\n  \\end{tabular}}\n  \\caption{\n  Case study on question decomposition results predicted by Codex and Dater. We also show the intermediate parsing results, which have the potential to provide the advantage of interpretability.\n  }\n  \\label{tab:cs_cot}\n\\end{table*}\n\n\\subsection{Case Study}\n\\paragraph{Case Study for Evidence Decomposition}\nTo better understand how evidence decomposition helps our model capture the relevant sub-evidence, we demonstrate \nthree exemplary cases about sports selected from TabFact. Due to space limitations, we solely provide the table column selection results. From Table~\\ref{tab:col_select}, we can observe that the rule-based method fails to precisely align the question and evidence since it cannot capture the deep semantic information of both the question and evidence. Taking the first case as an example, the rule-based method cannot link the phrase ``\\textit{against the rockies}'' and the column name ``\\textit{opponent}''.  In addition, the rule-based method often suffers from a lack of commonsense knowledge and cannot comprehend complex questions effectively. For example, as shown by the third case, the rule-based method fails to recognize ``\\textit{denver}'' as a team name and does not discover that the ``\\textit{score}'' column can be used to determine the results of the games. \nOn the contrary, our Dater method can address the limitations of previous methods in evidence decomposition and accurately select the relevant columns from a table by eliciting rich semantic and commonsense knowledge from a powerful LLM. \n\n\\begin{wrapfigure}{l}{0.5\\textwidth}\n\\begin{tikzpicture}\n\\begin{axis} [\nybar,\nheight=1.2in, \nbar width=0.4cm,\nwidth=0.95\\linewidth,\nscale only axis,\nymin = 0, \nyticklabels=\\empty,\naxis x line*=bottom,\nhide y axis,\nxticklabel style = {font=\\small,yshift=0.5ex},\nsymbolic x coords={\nTabFact,\nWikiTableQuestion,\nFetaQA,\n},\nlegend style={\n    at={(0,1.0)},\n    anchor=north west,\n    legend columns=-1\n},\nenlarge x limits=0.2,\nxtick=data,\nyticklabels=\\empty,\nnodes near coords,\nnodes near coords align={vertical},\nevery node near coord/.append style={font=\\small},\nlegend style={at={(1,1)},anchor=north east}\n]\n\\addplot coordinates {\n(TabFact, 94) (WikiTableQuestion, 164) (FetaQA, 90)\n};\n\\addplot coordinates {\n(TabFact, 30) (WikiTableQuestion, 56) (FetaQA, 32)\n};\n\\legend{Before, After}\n\\end{axis}\n\\end{tikzpicture}\n\\caption{Comparison of the average number of table cells before and after evidence decomposition on three dataset.}\n\\label{fig:table_cell}\n\\end{wrapfigure}\n\n\\paragraph{Case Study for Question Decomposition}\nWe use four exemplar cases selected from WikiTableQuestion and TabFact datasets to verify the effectiveness of our question decomposition qualitatively.\nTable~\\ref{tab:cs_cot} shows the ``chain of thought'' generated by Codex and our Dater method, where the first two cases are from WikiTableQuestion and the last two cases are from TabFact. From the cases, we can observe that Dater can decompose the complex question into simpler sub-questions that can be easily solved by leveraging a ``parsing-execution-filling'' strategy. In particular, Dater can generate high-quality SQL queries to retrieve correct information from the evidence. The retrieved information is essential to predict the final answer.  In contrast, without question composition, Codex fails to understand and comprehend complex questions, therefore it just gets information from the given question. For example, Codex tends to generate a ``\\textit{chain of thought}'' (a series of intermediate reasoning steps) that is inconsistent with the evidence. \n\n\\subsection{Analysis of Sub-evidence}\nAs revealed by \\citep{llmtab}, the table-based reasoning models are unable to generalize to ``huge'' tables with 30+ rows, which is the major error source. To address this problem, evidence decomposition is proposed to obtain a sub-evidence by filtering the irrelevant information from the ``huge'' evidence. To illustrate the effectiveness of evidence decomposition in narrowing the scope of irrelevant evidence, in Figure \\ref{fig:table_cell}, we report the average number of table cells before and after evidence decomposition on the three datasets. From the results, we can observe that the sub-evidence is about 3x smaller than the original evidence, while achieving better performance. In particular, for WikiTableQuestion, we can reduce the average number of table cells from 164 to 56, significantly relieving the burden of LLMs in dealing with ``huge'' tables. Similar trends can also be observed on TabFact and FetaQA datasets. \n\n\\section{Conclusion}\nIn this paper, we explored in-context learning in LLMs to decompose structured evidence and unstructured NL questions for effective table-based reasoning. First, we utilized a powerful LLM to decompose the evidence involved in the current question into a relevant sub-evidence. The sub-evidence extraction could identify the relevant sub-evidence and exclude the remaining irrelevant evidence by predicting the indexes of the rows and columns with the help of an LLM and a few prompts.\nSecond, we introduced a ``\\textit{parsing-execution-filling}'' strategy, which explored the programming language SQL to decompose the complex question into logical and numerical computation. \nFinally, we leveraged the decomposed sub-evidence and sub-questions to obtain the final answer effectively with the help of a few in-context prompting examples.   \nExperimental results on three benchmark datasets demonstrated that our Dater achieved significantly better results than previous competitive baselines (fine-tuning-based methods and LLM-based in-context learning methods) for table-based reasoning. Particularly, Dater outperforms human performance for the first time on the TabFact dataset. In addition to impressive overall performance, Dater also has the advantage of interpretability, where the returned results are to some extent tractable with the generated sub-evidence and sub-questions. \n\nOur evidence decomposition method extracts indexes of the rows and columns as a whole via a powerful LLM without considering the ``chain-of-thought'' characteristic of the given question. In the future, we plan to learn the fine-grained alignment between tables and questions by performing step-by-step reasoning over tables.\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{CABINET: Content Relevance based Noise\\ \\ \\ \\ Reduction for Table Question Answering}\n\n\\begin{document}\n\n\\def \\approach{ToReQA }\n\n\\maketitle\n\n\\def\\thefootnote{*}\\footnotetext{These authors contributed equally to this work}\\def\\thefootnote{\\arabic{footnote}}\n\n\\begin{abstract}\n\nTable understanding capability of Large Language Models (LLMs) has been extensively studied through the task of question-answering (QA) over tables. Typically, only a small part of the whole table is relevant to derive the answer for a given question. The irrelevant parts act as noise and are \\textit{distracting information}, resulting in sub-optimal performance due to the vulnerability of LLMs to noise. To mitigate this, we propose \\textbf{\\approachname}\\ (\\approachtext) -- a framework to enable LLMs to focus on relevant tabular data by suppressing extraneous information. \\approachname\\ comprises an Unsupervised Relevance Scorer (URS), trained differentially with the QA LLM, that weighs the table content based on its relevance to the input question before feeding it to the question-answering LLM (QA LLM). To further aid the relevance scorer, \\approachname\\ employs a weakly supervised module that generates a parsing statement describing the criteria of rows and columns relevant to the question and highlights the content of corresponding table cells. \\approachname\\ significantly outperforms various tabular LLM baselines, as well as GPT3-based in-context learning methods, is more robust to noise, maintains outperformance on tables of varying sizes, and establishes new SoTA performance on WikiTQ, FeTaQA, and WikiSQL datasets. We release our code and datasets \\href{https://github.com/Sohanpatnaik106/CABINET_QA}{here}.\n\\vspace{-2mm}\n\n\\vspace{-2mm}\n\\section{Introduction}\n\\vspace{-1mm}\n\nUnderstanding tabular data through ML models has been extensively explored through various tasks such as question-answering (QA)~\\citep{chen-etal-2022-convfinqa, cheng-etal-2022-hitab, nan-etal-2022-fetaqa}, fact-verification~\\citep{Chen2020TabFact, wang-etal-2021-semeval, 68d30a95}, table-to-description generation~\\citep{chen-etal-2020-logical, parikh-etal-2020-totto, chen-etal-2020-logic2text, suadaa-etal-2021-towards, nan-etal-2021-dart} and table grounded dialogue~\\citep{nakamura-etal-2022-hybridialogue}. Table QA has been studied with a specific focus as it allows to conveniently query the table in natural language to extract desired information. Large Language Models (LLMs), which have shown remarkable generalization on various Natural Language Processing (NLP) tasks, have also been used to reason over tables achieving impressive performance~\\citep{yu2021grappa, neeraja-etal-2021-incorporating, gu-etal-2022-pasta, chen-2023-large}. \n\nTables contain information organized in rows and columns, and typical transformer-based LLMs such as BERT~\\citep{devlin-etal-2019-bert}, T5~\\citep{JMLR:v21:20-074}, and GPT~\\citep{NEURIPS2020_1457c0d6} trained over unstructured natural language text using standard language modeling objectives do not account for the table structure and underlying compositionality of data~\\citep{yu2021grappa}. Many works on table understanding therefore, adapt LLMs for tables through joint learning over tabular and text content~\\citep{yin-etal-2020-tabert}, pre-training on table semantic parsing~\\citep{liu2022tapex, jiang-etal-2022-omnitab} and synthesizing template-based questions to improve reasoning skills over tables~\\citep{gu-etal-2022-pasta}. Typically, only a small number of cells contain the information required to derive the answer for a question. The irrelevant tabular data acts as \\textit{distracting information} or noise, resulting in sub-optimal performance since LLMs are susceptible to noise in the input~\\citep{kumar-etal-2023-multi, chen2023benchmarking}. Performance degradation is further amplified in large tables due to presence of even more data as illustrated in Figure~\\ref{fig:perf_vs_size} in Section~\\ref{subsec:size_abl}.\n\nConsequently, significant efforts have been made to mitigate the issue of noise by pruning of tabular data, albeit at the cost of accuracy~\\citep{krichene-etal-2021-dot}, and by retrieving content from table and passages for QA~\\citep{wang-etal-2022-muger2, lei-etal-2023-s3hqa, kumar-etal-2023-multi}. More recently, DATER~\\citep{10.1145/3539618.3591708}, one of the state-of-the-art methods for table QA, proposed decomposing a table into simpler sub-tables containing information needed to answer the question by providing in-context examples to GPT-3 based Codex~\\citep{DBLP:journals/corr/abs-2107-03374}. \n\n  \n\n\\begin{figure}[t]\n     \\centering\n     \\includegraphics[scale=0.42]{figs/cabinet_introsample.pdf} \n    \\caption{Comparison between \\approachname and DATER (a GPT-3 based in-context learning method). For the given example, DATER extracts a wrong sub-table through hard decomposition (resulting in loss of useful information) that causes QA reasoner to answer incorrectly. \\approachname\\ weighs relevant table parts higher without removing content explicitly allowing QA LLM to answer correctly.}\n    \\vspace{-15pt}\n    \\label{fig:comparision_intro}\n\\end{figure}\n\nSuch a question-conditioned hard decomposition of table is sub-optimal as the subsequent QA model cannot correct the error made during decomposition if relevant information is not selected (as shown in Figure~\\ref{fig:comparision_intro}). To mitigate this, we propose \\approachname\\ (\\approachtext) --  a framework for table QA that weighs different table parts based on their relevance to the question without explicitly removing any content. \\approachname\\ comprises a relevance scorer (\\S~\\ref{sec:meth_uns_relevance}), which takes question and table as input to provide a relevance score to table content. The score is used to weigh corresponding content passed to the QA LLM, allowing it to focus more on the relevant content. The relevance scorer is unsupervised and trained with QA LLM differentiably due to lack of annotations denoting relevant table information. Although answer generation loss enables learning of relevance scorer, it acts as an indirect training signal. \n\nHence, to aid relevance scorer, inspired by how humans process tables, \\approachname\\ employs a parsing statement generator (\\S~\\ref{sec:highlight}) that describes which rows and columns are relevant to the question. For instance, consider the example in Figure~\\ref{fig:comparision_intro}, \\approachname\\ generates ``consider rows with result as 'loss', and note the higher value in the 'score' column\". The parsing statement is then used to identify corresponding cells, and their content is given more weight during relevance scoring. \\approachname\\ establishes new SoTA on three challenging table QA datasets (WikiTQ, FeTaQA and WikiSQL) significantly outperforming various strong baselines (\\S~\\ref{sec:main_res}). We show that \\approachname\\ is more robust to noise in tables and structural biases i.e. row and column ordering (\\S~\\ref{sec:robustness}). Further, the performance gains achieved by \\approachname\\ are even more pronounced for larger tables (\\S~\\ref{subsec:size_abl}), indicating that it successfully mitigates noisy table information irrelevant to a given question.\n\n\\section{Related Work}\n\\vspace{-2mm}\n\\textbf{Table Specific Architecture:}\nTables contain information in a structured format, organized in rows and columns. Hence, many works have focused on developing table-specific models to utilize the semantics of table structure through its description. TabBERT \\citep{yin-etal-2020-tabert} pre-trains BERT~\\citep{devlin-etal-2019-bert} on paired table-text samples through masked language modeling (MLM). \\citet{deng2020turl} modified the bidirectional attention in BERT to incorporate table structure while performing MLM. TAPAS~\\citep{herzig-etal-2020-tapas} utilizes positional embeddings for rows and columns to explicitly capture cell location. \\citet{yang-etal-2022-tableformer} noted that methods using positional embeddings are vulnerable to column and row permutations. To address this, they introduce TableFormer, a table-text encoding architecture that incorporates tabular structure through learnable attention biases. We show that LLMs become less susceptible to such permutations by learning to focus on relevant table parts through \\approachname\\ (\\S~\\ref{sec:robustness}). We discuss use of graphs to capture table structure and similarity between table-question samples across the dataset~\\citep{iyer-etal-2023-question} in Appendix~\\ref{app:add_related_work}\n\n\\textbf{Table QA Specific Pre-training}: \\citet{eisenschlos-etal-2020-understanding} argued that the MLM objective to just fill in the blanks of table cells and descriptions is insufficient to capture relations between cells and associated text needed to perform table QA. They introduced additional pre-training tasks that require explicit question-table reasoning and complex table operations (such as aggregation). Other improvements include handling of numeric tokens~\\citep{han2022luna}, temporal relations~\\citep{zhao-etal-2022-reastap}, and selectively masking tokens that require table based reasoning~\\citep{gu-etal-2022-pasta}. Another direction focuses on applying semantic parsing over the input text (question) and table to generate a logical form (such as SQL) which when executed fetches relevant information~\\citep{yu2021grappa}. Such methods like TAPEX~\\citep{liu2022tapex}, OmniTab~\\citep{jiang-etal-2022-omnitab} etc. typically involve joint training over natural language-SQL pairs so that the underlying model learns to map the information implied in the question to the required table operations. However, as discussed in experiments (\\S~\\ref{sec:robustness} and \\ref{subsec:size_abl}), these methods suffer significant performance drop when dealing with large and noisy tables owing to their limited capability at identifying information relevant to question.\n\n\\textbf{Few/Zero-Shot Learning with Large Language Models:} Given the remarkable performance of LLMs on various tasks without any task-specific training, their use for table understanding has also been explored extensively. \\citet{chen-2023-large} have shown that LLMs perform strongly on various table QA tasks using Chain of Thought (CoT)~\\citep{wei2022chain, wang2023selfconsistency} prompting. Since typical LLMs are trained over unstructured text data, models specifically designed to handle structured data, such as StructGPT~\\citep{jiang2023structgpt} have also been used for table QA. LEVER~\\citep{ni2023lever} and BINDER~\\citep{cheng2023binding} utilized code-optimized GPT-Codex~\\citep{DBLP:journals/corr/abs-2107-03374} to generate SQL statements that can be executed to answer questions over tabular data. DATER~\\citep{10.1145/3539618.3591708} uses Codex to break table into sub-tables conditioned on a given question through in-context learning. Such methods have no way to recover relevant table part to generate the correct answer in case it is omitted while generating sub-tables (as discussed in Figure~\\ref{fig:comparision_intro}).\n\n\\vspace{-2mm}\n\\section{Methodology}\n\\vspace{-2mm}\n\nWe summarize the architecture of \\approachname\\ in Figure~\\ref{fig:overview}. It comprises two components: \\textbf{1) Unsupervised Relevance Scorer}, an unsupervised module comprising a transformer encoder that takes question and table as input and tokenizes them (steps 1 and 2 in Fig.~\\ref{fig:overview}) followed by assigning a relevance score to each table token (step 3 in Fig.~\\ref{fig:overview}). The relevance score is then multiplied with the corresponding token embedding at the time of giving it as input to QA LLM encoder (step 7 in Fig.~\\ref{fig:overview}). This ensures that noisy content with lower relevance score get suppressed and the QA LLM can focus on relevant tokens. The unsupervised relevance scorer is connected to QA LLM in a differentiable manner enabling it to be trained through answer generation loss (step 8 in Fig.~\\ref{fig:overview}). \n\nEven though answer generation loss enables learning of unsupervised relevance scorer, it acts as an indirect training signal. To aid relevance scoring, we propose a weakly supervised module: \\textbf{2) Relevant Cell Predictor through Table Parsing} that parses table conditioned on question to highlight cells containing relevant information (steps 4 and 5 in Fig.~\\ref{fig:overview}). It comprises two sub-steps where we first train a \\textit{Parsing Statement Generator} that describes criteria in natural language about which rows and columns should be used to derive the answer (step 4 in Fig.~\\ref{fig:overview}). Table cells corresponding to the criteria described in the parsing statement (step 5 in Fig.~\\ref{fig:overview}) are highlighted such that score for content tokens in highlighted cells is boosted by combining it with unsupervised relevance score through a linear combination (step 6 in Fig.~\\ref{fig:overview}). We conduct extensive ablations to establish efficacy for different modules (\\S~\\ref{sec:abl_exp_design}). We now discuss the details of each component.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\textwidth]{figs/cabinet_final_arch_diag.pdf}\n    \\caption{Overview of architecture of \\approachname. The table is linearized (step 1) and embedded along with question through embedding layer of the underlying QA LLM (step 2). The embedded sequence is passed to the unsupervised relevance scorer that assigns a relevance score to each table token (step3). In parallel, the parsing statement generator describes the criteria for rows and columns relevant to deriving the answer (step 4) that is used to identify corresponding cells and assign a cell-based relevance score (step 5). The unsupervised and cell-based relevance is combined (step 6) and used to weigh the table content (step 7) to the QA LLM which generates the answer (step 8).}\n    \\vspace{-4mm}\n    \\label{fig:overview}\n\\end{figure}\n\n\\vspace{-2mm}\n\\subsection{Unsupervised Relevance Scorer}\n\\label{sec:meth_uns_relevance}\n\\vspace{-2mm}\nThe unsupervised relevance scorer is used to assign a score to table content tokens. Since annotating cells of a table relevant to a given question is tedious, the relevance scorer is unsupervised and gets trained along with QA LLM through answer generation loss. Formally, consider a pair of table $\\mathcal{T}$ and a question $\\mathcal{Q}$ about $\\mathcal{T}$. $\\mathcal{Q}_{tokens}=\\{q_1, q_2, ..., q_{|Q|}\\}$ represents the question tokens, $\\mathcal{T} = \\{c_{ij} | 1 \\leq i \\leq \\text{N}_{row}, 1 \\leq j \\leq \\text{N}_{col} \\}$, where $\\text{N}_{row}$ and $\\text{N}_{col}$ indicate number of rows and columns in $\\mathcal{T}$ respectively, and $c_{ij}$ represents string in cell in the $i^{th}$ row and $j^{th}$ column. To make $\\mathcal{T}$ suitable to be fed as input to a transformer-based LLM, we follow the commonly used linearising scheme~\\citep{liu2022tapex} where table is flattened as (step 1 in Fig.~\\ref{fig:overview}):\n\n\\begin{equation}\n\\label{eq:table_flattening}\n\\small\n    \\mathcal{T}_{flattened} = [HEAD]:\\ c_{11}\\ |\\ c_{12}\\ | \\cdots |\\ c_{1N_{col}}\\ |\\ [ROW] 1: c_{21}\\ | \\cdots |\\ c_{2N_{col}}\\ |\\ [ROW] 2: \\cdots \n\\end{equation}\n\n$[HEAD]$ and $[ROW]k$ indicate start of column header row and $k^{th}$ data row respectively. We separate special tokens and cell content using pipe symbol `$|$'. The string in Equation~\\ref{eq:table_flattening} is tokenized using the tokenizer of underlying QA LLM to obtain table tokens $\\mathcal{T}_{tokens}=\\{t_1, t_2, ..., t_{|\\mathcal{T}_{tokens}|}\\}$. $\\mathcal{T}_{tokens}$ is concatenated to ${Q}_{tokens}$ to obtain $\\mathcal{I}_{tokens} = (\\mathcal{Q}_{tokens};\\mathcal{T}_{tokens})$ which is given as input (steps 2 and 3 in Fig.~\\ref{fig:overview}) to Unsupervised Relevance Scorer (URS) comprising a transformer encoder $TE_{URS}$. The contextualized representation $h_p \\in \\mathcal{R}^{d}$ of the $p^{th}$ token is estimated as:\n\\vspace{-1mm}\n\\begin{align}\n    \\vspace{-7pt}\n    e_1^{URS}, e_2^{URS}, \\cdots, &e_{|\\mathcal{I}_{tokens}|}^{URS} =\\ Embedding_{URS}(\\mathcal{I}_{tokens}) \\\\\n    h_1, \\cdots, h_p, \\cdots, h_{|\\mathcal{I}_{tokens}|} &= TE_{URS}(e_1^{URS}, e_2^{URS}, \\cdots, e_{|\\mathcal{I}_{tokens}|}^{URS})\n\\end{align}\n\n\\vspace{-2mm}\nWe aim to predict relevance score for each table token, however, since annotations for relevant table parts are unavailable, token relevance is not explicitly observable and we consider it as a latent variable. Further, we hypothesize that the representation space of table tokens can be structured better for modeling relevance by clustering their encodings into two categories - relevant and non-relevant. Variational Inference (VI) has been commonly used to estimate latent variable probability and group data points on the basis of latent topics~\\citep{srivastava2017autoencoding}. Hence, we estimate relevance $\\eta_{p}^{uns}$ of table token $t_p$ ($|\\mathcal{Q}_{tokens}| + 1 \\leq p \\leq |\\mathcal{Q}_{tokens}| + |\\mathcal{T}_{tokens}|$) as (step 3 in Fig.~\\ref{fig:overview}):\n\\vspace{-1mm}\n\\begin{align}\n    \\vspace{-10pt}\n        \\mu_p &= \\phi_\\mu(h_p);\\ \\  \\sigma_p = \\phi_\\sigma (h_p) \\\\\n        \\eta_{p}^{uns}  &= sigmoid(z_p);\\ \\ z_p = \\mu_p +  s*\\sigma_p\n\\end{align}\n\n\\vspace{-3mm}\n$s$ is sampled from standard normal distribution, $\\phi_\\mu$ and $\\phi_\\sigma$ are FC layers with weights $W_\\mu \\in \\mathcal{R}^{d \\times 1}$ and $W_\\sigma \\in \\mathcal{R}^{d \\times 1}$, $sigmoid$ is applied to normalize the relevance score in the range 0 to 1. To enable the relevance scorer to assign appropriate scores, we structure the latent space of $TE_{URS}$ by clustering table tokens into relevant and non-relevant. We use the method of \\citet{JMLR:v9:vandermaaten08a} (details in appendix \\ref{sec:cluster}) which performs clustering in a trainable manner using clustering loss $\\mathcal{L}_{clu}$ . We apply $\\mathcal{L}_{clu}$ over latent representation $h_p$ of tokens which enables us to tune $TE_{URS}$ for clustering. During experiments, we observed that unit vectors for cluster centroids $\\mu_{relevant}^{clu}$ and $\\mu_{irrelevant}^{clu}$ are not well separated. To mitigate this, we enforce a separation loss $\\mathcal{L}_{sep}$ that increases the distance between unit vectors representing cluster centroids: \n\\begin{equation}\n    \\vspace{-10pt}\n    \\mathcal{L}_{sep} = 2 - \\Big|\\Big| \\mu_{relevant}^{clu} - \\mu_{irrelevant}^{clu} \\Big|\\Big|^2\n    \\label{eqn:sep_loss}\n\\end{equation}\n\nFurther, it is desirable that relevance scores for tokens in one cluster (corresponding to irrelevant tokens) are low. To achieve this, we apply a sparsification loss $\\mathcal{L}_{sparse}$ where the score logit $z_p$ is exponentiated with a negative coefficient to push logit values for relevant and irrelevant clusters to $\\infty$ and $-\\infty$ respectively that enables final score (after applying $sigmoid$) to be close to 1 and 0:\n\\vspace{-3mm}\n\n\\begin{equation}\n    \\vspace{-10pt}\n    \\mathcal{L}_{sparse} = \\frac{1}{|\\mathcal{T}_{tokens}|} \\sum_p \\mathrm{e}^{-z_{p}^2} ;\\ |\\mathcal{Q}_{tokens}|+1 \\leq p \\leq |\\mathcal{Q}_{tokens}|+|\\mathcal{T}_{tokens}| \n\\end{equation}\n\nAt the time of providing question and table as input to transformer encoder $TE_{QA}$ of the QA LLM, embedding ($e_p^{'}$) corresponding to question tokens is used as is while embedding of each table token is multiplied by its corresponding relevance score (steps 7 and 8 in Fig.~\\ref{fig:overview}):\n\\begin{align}\n    \\vspace{-10pt}\n    e_1, e_2, \\cdots, e_{|\\mathcal{I}_{tokens}|} =\\ &Embedding_{QA}(\\mathcal{I}_{tokens}) \\\\\n    \\label{eq:final_rel_score}\n    e_p^{'} =\\ \\eta_{p} \\odot e_p;\\ \\ |\\mathcal{Q}_{tokens}|+&1\\leq p \\leq |\\mathcal{Q}_{tokens}| + |\\mathcal{T}_{tokens}| \\\\\n    h_1^{'}, \\cdots, h_{|\\mathcal{I}_{tokens}|}^{'} &= TE_{QA}(e_1^{'}, e_2^{'}, \\cdots, e_{|\\mathcal{I}_{tokens}|}^{'}) \\\\ \n    a_1, a_2, \\cdots, a_N &= TD_{QA}(h_1^{'}, \\cdots, h_{|\\mathcal{I}_{tokens}|}^{'}) \n\\end{align}\n\n\\vspace{-3mm}\n`$\\odot$' indicates scalar multiplication with vector operation, $TD_{QA}$ represents the transformer decoder of the QA LLM that generates the answer tokens $a_n$ sequentially. $TE_{URS}$, $TE_{QA}$ and $TD_{QA}$ are trained in an end-to-end manner through cross-entropy loss $\\mathcal{L}_{CE}$ between the generated and ground-truth answer tokens. Thus, the total loss $\\mathcal{L}$ becomes:\n\\begin{equation}\n    \\mathcal{L} = \\mathcal{L}_{CE} + \\lambda_{clu}*\\mathcal{L}_{clu} + \\lambda_{sep}*\\mathcal{L}_{sep} + \\lambda_{sparse}*\\mathcal{L}_{sparse}\n\\end{equation}\n\n\\vspace{-2mm}\nThe answer generation loss acts as an indirect training signal for relevance scorer. To aid unsupervised scorer, we propose a weakly-supervised module (trained separately from URS and QA LLM) that highlights relevant cells (discussed in next subsection). Table tokens for highlighted cells are assigned cell-based score $\\eta_p^{cell}$ that is combined with unsupervised relevance score $\\eta_{p}^{uns}$ through linear combination (step 6 in Fig.~\\ref{fig:overview}). Thus, final relevance score $\\eta_p$ used in Eq.~\\ref{eq:final_rel_score} is:\n\\vspace{-1mm}\n\\begin{equation}\n\\label{eq:lin_comb_eta}\n    \\vspace{-7pt}\n    \\eta_p = \\lambda_{uns}*\\eta_p^{uns} + \\lambda_{cell}*\\eta_p^{cell}\n\\end{equation}\n\n\\subsection{Relevant Cell Predictor through Table Parsing}\n\\label{sec:highlight}\n\nAs discussed above, we train a separate module to highlight table cells relevant to a given question in a weakly-supervised manner. Since there is no Table QA dataset that contains annotations for table cells useful to answer a given question, we adopt a two-stage approach where we first train a \\textit{Parsing Statement Generator} (step 4 in Fig.~\\ref{fig:overview}) to generate a natural language text describing criteria for rows and columns relevant to the given question. Subsequently, we train another model that takes the parsing statement and table as input to identify the cells matching the criteria (step 5 in Fig.~\\ref{fig:overview}).\n\n\\textbf{Parsing Statement Generator (PSG)} comprises a pre-trained LLM - Flan T5-xl~\\citep{chung2022scaling} that is fine-tuned to take the question and table as input ($\\mathcal{I}_{tokens}$) to generate a parsing statement $text_{parse}$ (step 4 in Fig.~\\ref{fig:overview}). The statement describes criteria stating which rows and columns are useful to derive the answer. To bootstrap training of PSG, we manually annotate very few ($\\sim$300) question-table pairs with parsing statement. For instance, for table and question shown in Figure~\\ref{fig:overview}, we annotate parsing statement as `\\textit{To derive answer, note the values of higher score in rows with result as loss}'. To circumvent annotating samples for each table QA dataset, we choose WikiTableQuestions (WikiTQ) dataset~\\citep{DBLP:journals/corr/PasupatL15} to select the samples for annotation since it is the most complex QA dataset containing a variety of samples. We sample diverse set of questions for annotation (please refer appendix \\ref{sec:ques_cluster} for details and examples). The sampled question along with its table are manually annotated with parsing statement which is used to fine-tune PSG. The trained PSG model is then used to generate parsing statement for any question-table pair from datasets studied for experiments. We release the dataset of manually annotated parsing statements.\n\n\\textbf{Cell Highlighting based on Parsing Statement:} To identify table cells for the criteria described in the parsing statement $text_{parse}$, we need a way to map the statement to corresponding cells. To this end, we use ToTTo dataset~\\citep{parikh-etal-2020-totto} that contains samples of (table, list of highlighted cell coordinates) pairs. Each pair is accompanied by a text description summarising the content of the corresponding list of cells. We fine-tune a cell highlighting model $Cell\\_Highlighter_{LLM}$ comprising of Flan T5-xl on ToTTo dataset where the table and summarising text are given as input to generate the content of corresponding highlighted cells. Once $Cell\\_Highlighter_{LLM}$ is trained, we provide the table and $text_{parse}$ as input to identify and generate content of corresponding cells. For instance, consider example in Figure~\\ref{fig:overview}, given the parsing statement shown in figure as input, the cell predictor generates $'38-12\\ \\|\\ loss\\ \\|\\ 45-0\\ \\|\\ loss\\ \\| ...'$ (step 5). More formally,\n\\begin{align}\n    \\vspace{-10pt}\n  c_{1}^{highlighted} \\ ||\\ \\cdots\\ ||\\ c_{M}^{highlighted} = Cell\\_Highlighter_{LLM}(\\mathcal{T}, text_{parse})\n      \\vspace{-10pt}\n\\end{align}\n$c_{r}^{highlighted}$ represents the string of $r^{th}$ highlighted cell predicted based on parsing statement. $M$ is a variable number, `$||$' is a delimiter to separate cell content. For $1 \\leq r \\leq M$, if $c_{r}^{highlighted}$ exactly matches with the content of some cell in $\\mathcal{T}$, then the tokens $t_p$ of matching cell is assigned a cell relevance score ($\\eta_p^{cell}$) of 1. $\\eta_p^{cell}$ is set to 0 for table tokens belonging to cells in $\\mathcal{T}$ whose content does not match with any $c_{r}^{highlighted}$. $\\eta_p^{cell}$ is then combined with unsupervised relevance score $\\eta_p^{uns}$ as in Eq.~\\ref{eq:lin_comb_eta}. We now discuss experiments performed to validate the efficacy of our approach.\n\n\\section{Experiments and Evaluation}\n\\label{sec:exp_and_eval}\n\\vspace{-2mm}\n\n\\noindent\n\\textbf{Implementation Details:} For the encoder ($TE_{QA}$) and decoder ($TD_{QA}$) of the QA LLM, we employ the OmniTab~\\citep{jiang-etal-2022-omnitab} backbone (pre-trained for table understanding) comprising of BART-Large~\\citep{lewis-etal-2020-bart}. The embeddings of unsupervised relevance scorer (URS), $Embedding_{URS}$, and QA model, $Embedding_{QA}$, are shared. URS encoder ($TE_{URS}$) is initialized with the architecture and weights of QA LLM encoder ($TE_{QA}$), though they do not share weights during QA training. Consequently, the hidden dimension $d$ of $TE_{URS}$ is 1024. We train \\approachname\\ and baselines (wherever needed) for 30 epochs on an effective batch size (BS) of 128 using 8 80GB A100 GPUs (BS of 8/GPU with gradient accumulation 2) using a learning rate of $1e^{-5}$ with cosine annealing~\\citep{loshchilov2017sgdr} through AdamW optimizer~\\citep{loshchilov2018decoupled}.\n\n\\noindent\n\\textbf{Datasets and Evaluation Metrics:} We evaluate \\approachname\\ on three commonly used datasets --\n\\textit{(i)} \\textbf{WikiTableQuestion (WikiTQ)}~\\citep{DBLP:journals/corr/PasupatL15} which is one of the most commonly used and highly complex datasets consisting of about $2100$ HTML tables from Wikipedia and about $22,033$ questions that require complex operations such as comparison, aggregation, and arithmetic operations to arrive at the answer;  \n\\textit{(ii)} \\textbf{FeTaQA}~\\citep{nan-etal-2022-fetaqa}, a challenging dataset consisting of about $10,000$ questions that have a long-form natural language answer ($18$ words on average) such that it requires fetching multiple entities from the table, aggregating and reasoning over these entities, and structuring the inferred information to produce a coherent answer; \nand \\textit{(iii)} \\textbf{WikiSQL}~\\citep{zhongSeq2SQL2017} that comprises roughly $80,654$ questions over $24,241$ Wikipedia tables. It also provides the equivalent SQL query for each question to obtain the correct answer. While we do not generate SQL (or other implicit logical forms) and only use the natural language questions and answers from this dataset, it serves as a useful benchmark to compare \\approachname\\ with table understanding methods that generate explicit logical forms to extract relevant answers from table. \nThe ground-truth answers in both WikiTQ and WikiSQL datasets are short (1-2 words). Hence, we use exact-match accuracy (Acc.) to compare various methods. For FetaQA dataset, ground-truth answers being long-form ($\\approx 18$ words on average), we employ commonly used overlap-based metric Sacre-BLEU (S-BLEU)~\\citep{post-2018-call}. We report performance on test split for all datasets.\n\n\\subsection{Performance of \\approachname\\ on Table QA}\n\\label{sec:main_res}\n\n\\begin{wraptable}{R}{0.45\\linewidth}\n    \\centering\n    \\small\n    \\vspace{-10pt}\n    \\caption{Comparison of \\approachname\\ with different baselines on WikiTQ. \\approachname\\ achieves significantly better accuracy.}\n    \\resizebox{\\linewidth}{!}{%\n    \\begin{tabular}{@{}lcc@{}}\n    \\toprule\n        \\textbf{Method} & \\textbf{Acc.} & \\textbf{\\# params}\\\\\n        \\midrule\n        \\textbf{Fine-tuning Table-specific LLMs} & & \\\\\n        TAPAS~\\citep{herzig-etal-2020-tapas} & 48.8 & 345 M\\\\\n        TaBERT~\\citep{yin-etal-2020-tabert} & 52.3 & 345 M\\\\\n        MATE~\\citep{eisenschlos-etal-2021-mate} & 51.5 & 340 M\\\\\n        GraPPa~\\citep{yu2021grappa} & 52.7 & 355 M\\\\\n        DoT~\\citep{krichene-etal-2021-dot} & 54.0 & 299 M\\\\\n        TableFormer~\\citep{yang-etal-2022-tableformer} & 52.6 & 345 M\\\\\n        TAPEX~\\citep{liu2022tapex} & 55.5 & 405 M\\\\\n        ReasTAP~\\citep{zhao-etal-2022-reastap} & 58.6 & 406 M\\\\\n        TaCube~\\citep{zhou-etal-2022-tacube} & 60.8 & 406 M\\\\\n        OmniTab~\\citep{jiang-etal-2022-omnitab} & 62.7 & 406 M\\\\\n        & \\\\\n        \\textbf{Fine-tuning text-based LLMs} & & \\\\\n        T5-3b~\\citep{xie-etal-2022-unifiedskg}) & 49.3 & 2.9 B\\\\\n        FlanT5-xl~\\citep{chung2022scaling} & 64.4 & 2.9 B\\\\\n        & & \\\\\n        \\textbf{Few/zero shot Prompting of LLMs} & & \\\\\n        Codex~\\citep{10.1145/3539618.3591708} & 47.6 & 175 B\\\\\n        Codex-COT~\\citep{chen-2023-large} & 48.8 & 175 B \\\\\n        Binder~\\citep{cheng2023binding} & 64.6 & 175 B\\\\\n        LEVER~\\citep{ni2023lever} & 65.8 & 175 B\\\\\n        DATER~\\citep{10.1145/3539618.3591708} & 65.9 & 175 B\\\\\n        ChatGPT~\\citep{jiang2023structgpt} & 43.3 & 175 B\\\\\n        StructGPT~\\citep{jiang2023structgpt} & 48.4 & 175 B\\\\\n        \\\\\n        \\midrule\n        \\textbf{\\approachname\\ (Ours)} & \\textbf{69.1} & 560 M\\\\\n        \\bottomrule\n    \\end{tabular}}\n    \\vspace{-10pt}\n    \\label{tab:da_wikitq}\n\\end{wraptable}\n\nWe present a detailed comparative analysis of results achieved by \\approachname\\ with a variety of baselines. We consider three different categories of methods -- \\textit{(i)} LLMs specifically pre-trained for table understanding and fine-tuned for QA, such as TAPEX~\\citep{liu2022tapex}, ReasTAP~\\citep{zhao-etal-2022-reastap} and OmniTab~\\citep{jiang-etal-2022-omnitab};  \\textit{(ii)} fine-tuning LLMs (pre-trained on text only) such as T5-3b~\\citep{JMLR:v21:20-074} and Flan T5-xl~\\citep{flan-t5}; and \\textit{(iii)} {few or zero shot prompting} of LLMs like StructGPT~\\citep{jiang2023structgpt} and approaches that employ such LLMs for in-context learning like LEVER~\\citep{ni2023lever}, BINDER~\\citep{cheng2023binding} and DATER~\\citep{10.1145/3539618.3591708}.% since we use the Flan model to generate the parsing statement and corresponding cells in our approach.\n\nTable~\\ref{tab:da_wikitq} presents the performance of various methods on the WikiTQ dataset, and we can observe \\approachname\\ with an accuracy of $\\mathbf{69.1\\%}$ outperforms the best-performing baselines in each of the three categories and establishes new state-of-the-art. Specifically, \\approachname\\ outperforms OmniTab, DATER, and fine-tuned Flan T5-xl by $6.4\\%$, $3.2\\%$ and $4.7\\%$, in absolute terms, respectively. Also, note that simple prompting of ChatGPT does not work well for Table QA. We want to highlight that \\approachname\\ performs much better than GPT-3 and Codex-based SoTA in-context learning methods despite containing orders of magnitude fewer parameters.\n\nSimilar observations hold for FeTaQA (Table~\\ref{tab:sb_fetaqa}) and WikiSQL (Table~\\ref{tab:da_wikisql}) datasets where \\approachname\\ achieves new SoTA performance. For generating long descriptive answers for questions in FeTaQA, \\approachname\\ achieves SoTA S-BLEU of $\\mathbf{40.5}$ outperforming OmniTab, fine-tuned Flan T5-xl and DATER by a margin of $5.6$, $4.3$ and $9.6$ absolute percentage points, respectively. We report performance only for baselines that have explored the dataset in their work (except for T5 and Flan T5). We use their code/API for evaluation if available or else specify performance as reported in their paper. Similarly, for WikiSQL dataset, \\approachname\\  pushes the SoTA by $0.7\\%$ on already high performance of ReasTAP (current SoTA). Since best performance on WikiSQL is already high, the absolute performance gains of $0.7\\%$ should be interpreted as a proportion of scope of further improvement possible, i.e., $0.7/(100-88.8)$, which is $\\approx6\\%$.\n\n\\begin{table}[t]\n    \\begin{minipage}{.49\\textwidth}\n    \\centering\n    \\small\n     \\caption{Comparison with different categories \\\\ of baselines on FeTaQA. \\approachname\\ achieves \\\\ significantly better Sacre-BLEU (S-BLEU).}\n    \\label{tab:sb_fetaqa}\n    \\resizebox{\\textwidth}{!}{%\n        \\begin{tabular}{@{}lcc@{}}\n    \\toprule\n        \\textbf{Method} & \\textbf{S-BLEU} & \\textbf{\\# params}\\\\\n        \\midrule\n        \\textbf{Fine-tuning Table-specific LLMs} & & \\\\\n        PeaQA~\\citep{pal-etal-2022-parameter} & 33.5 & 406 M \\\\\n        TAPEX~\\citep{liu2022tapex} & 34.7 & 406 M\\\\\n        OmniTab~\\citep{jiang-etal-2022-omnitab} & 34.9 & 406 M\\\\\n        & & \\\\\n        \n        \\textbf{Fine-tuning text-based LLMs} && \\\\\n        T5-small~\\citep{nan-etal-2022-fetaqa} & 21.6 & 60 M\\\\\n        T5-base~\\citep{nan-etal-2022-fetaqa} & 28.1 & 222 M\\\\\n        T5-large~\\citep{nan-etal-2022-fetaqa} & 30.5 & 738 M\\\\\n        T5-3b~\\citep{xie-etal-2022-unifiedskg} & 33.4 & 2.9 B\\\\\n        FlanT5-xl & 36.2 & 2.9 B\\\\\n        & & \\\\\n        \n        \\textbf{Few/zero shot Prompting of LLMs} & & \\\\\n        Codex-COT ~\\citep{chen-2023-large} & 27.0 & 175 B \\\\\n        Codex~\\citep{10.1145/3539618.3591708} & 27.9 & 175 B\\\\\n        DATER~\\citep{10.1145/3539618.3591708} & 30.9 & 175 B\\\\\n        \\midrule\n        \\textbf{\\approachname\\ (Ours)} & \\textbf{40.5} & 560 M\\\\\n        \\bottomrule\n    \\end{tabular}}\n    \\end{minipage} \n    \\hfill\n    \\begin{minipage}{0.49\\textwidth}\n    \\centering\n    \\small\n    \\caption{Comparison with different categories of baselines on WikiSQL. \\approachname\\ achieves better Accuracy (Acc.).}\n        \\label{tab:da_wikisql}\n      \\resizebox{0.93\\textwidth}{!}{%\n        \\begin{tabular}{@{}lcc@{}}\n        \\toprule\n        \\textbf{Method} & \\textbf{Acc.} & \\textbf{\\# params}\\\\\n        \\midrule\n        \\textbf{Fine-tuning Table-specific LLMs} & & \\\\\n        TAPAS~\\citep{herzig-etal-2020-tapas} & 86.4 & 345 M\\\\\n        GraPPa~\\citep{yu2021grappa} & 84.7 & 355 M\\\\\n        DoT~\\citep{krichene-etal-2021-dot} & 85.5 & 299 M\\\\\n        TAPEX~\\citep{liu2022tapex} & 86.4 & 406 M\\\\\n        OmniTab~\\citep{jiang-etal-2022-omnitab} & 87.9 & 406 M\\\\\n        UTP~\\citep{chen2023bridge} & 88.1 & 345 M \\\\\n        ReasTAP~\\citep{zhao-etal-2022-reastap} & 88.8 & 406 M\\\\\n        & & \\\\\n\n        \\textbf{Fine-tuning text-based LLMs} & & \\\\\n        T5-3b~\\citep{xie-etal-2022-unifiedskg} & 85.9 & 2.9 B\\\\\n        FlanT5-xl & 87.8 & 2.9 B\\\\\n        & & \\\\\n        \n        \\textbf{Few/zero shot Prompting of LLMs} & & \\\\\n        ChatGPT~\\citep{jiang2023structgpt} & 51.6 & 175 B\\\\\n        StructGPT~\\citep{jiang2023structgpt} & 54.4 & 175 B\\\\\n        \\midrule\n        \\textbf{\\approachname\\ (Ours)} & \\textbf{89.5} & 560 M\\\\\n        \\bottomrule\n    \\end{tabular}}\n    \\end{minipage}\n\\end{table}\n\n\\subsection{How Robust is \\approachname\\ to Noise and Irrelevant Information?}\n\\label{sec:robustness}\nDespite the remarkable success of transformer-based models on table understanding, they are sensitive to noise and perturbations to the tabular data~\\citep{pi-etal-2022-towards,yang-etal-2022-tableformer,robut}. We examine the robustness and sensitivity of  \\approachname\\  towards noise while performing Table QA. We introduce noise by perturbing tables in test split and report the relative percentage drop in performance. We perform four types of perturbations: \\textbf{1) Row Addition (RA):} insert noise into a table by adding rows from another table that contains same number of columns; \\textbf{2) Row Permutation (RP):} randomly permute ordering of rows~\\citep{pi-etal-2022-towards}; \\textbf{3) Column Permutation (CP):} randomly permute column ordering; and \\textbf{(4) Cell Replacement (CR):} replace content of certain cells with content from some other table. We perform each perturbation separately to obtain four perturbed test splits for each dataset. Please see appendix \\ref{sec:table_perturb_details} for further details about the procedure.\n\nFigure~\\ref{fig:perturbation} summarizes the relative drop in performance of \\approachname\\ and the dataset-specific best baseline for the three datasets. Note that for all the perturbation categories, \\approachname\\ leads to significantly less drop in performance when compared with the corresponding baseline, highlighting the robustness and ability of \\approachname\\ to identify the relevant portions of the underlying table. Specifically, \\approachname\\ is significantly less sensitive to row and column permutations (RP and CP), indicating that relevance scoring of tokens helps the QA LLM to focus more on relevant information and reduces the potential ordering biases commonly observed in models pre-trained on tabular data~\\citep{yang-etal-2022-tableformer}. For the cell replacement (CR) and row addition (RA) perturbations, where extraneous information is explicitly added to the table, the drop in performance suffered by \\approachname\\ is significantly less compared to the baselines owing to the superior ability of \\approachname\\ to identify relevant information. For instance, in the case of WikiTQ, the relative drop in performance for RA is $\\approx19\\%$ for OmniTab,  almost $40\\%$ higher than \\approachname\\ ($\\approx11.5\\%$). This consistent trend holds for FeTaQA and WikiSQL datasets as well.\n\n        \n        \n\n        \n\n\\begin{figure}[t]\n     \\centering\n     \\begin{subfigure}[b]{0.32\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{figs/perturb_wikitq.pdf}%{figs/wikitq_perturb_no_deletion.pdf}\n         \\caption{WikiTQ}\n         \\label{fig:wikitq_perturb}\n     \\end{subfigure}\n     \\hfill\n     \\begin{subfigure}[b]{0.32\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{figs/perturb_fetaqa.pdf}%{figs/fetaqa_perturb_no_deletion.pdf}\n          \\caption{FeTaQA}\n         \\label{fig:fetaqa_perturb}\n     \\end{subfigure}\n     \\hfill\n     \\begin{subfigure}[b]{0.32\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{figs/perturb_wikisql.pdf}%{figs/wikisql_perturb_reastap.pdf}\n          \\caption{WikiSQL}\n         \\label{fig:wikisql_perturb}\n     \\end{subfigure}\n    \\caption{Relative performance drop (\\%) with perturbations (RA - Row Addition, RP - Row Permutation, CP - Column Permutation, CR - Cell Replacement). We compare \\approachname\\ (green) with OmniTab (red) on WikiTQ and FeTaQA ; and against ReasTAP (red) on WikiSQL. \\approachname\\ is more robust to addition of noise to table and shuffling of row and column ordering.}\n    \\label{fig:perturbation}\n\\end{figure}\n\n\\begin{figure}[t]\n     \\centering\n     \\begin{subfigure}{0.32\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{figs/size_wikitq.pdf}%{figs/wikitq_size_perf.png}\n          \\caption{WikiTQ}\n         \\label{fig:wikitq_size_perf}\n     \\end{subfigure}\n     \\hfill\n     \\begin{subfigure}[b]{0.32\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{figs/size_fetaqa.pdf}%{figs/feta_size_perf.png}\n         \\caption{FeTaQA}\n         \\label{fig:feta_size_perf}\n     \\end{subfigure}\n     \\hfill\n     \\begin{subfigure}[b]{0.32\\textwidth}\n         \\centering\n         \\includegraphics[width=\\textwidth]{figs/size_wikisql.pdf}%{figs/wikisql_size_perf.png}\n         \\caption{WikiSQL}\n         \\label{fig:wikisql_size_perf}\n     \\end{subfigure}\n    \\caption{Variation in performance with table size (\\# cells). We compare \\approachname\\ (green) with OmniTab (red) on WikiTQ (left) and FeTaQA (middle), and against ReasTAP (red) for WikiSQL (right). It can be seen that \\approachname\\ performs much better than the baselines on larger tables.}\n    \\vspace{-14pt}\n    \\label{fig:perf_vs_size}\n\\end{figure}\n\n\\subsection{Impact of Table Size on Performance}\n\\label{subsec:size_abl}\n\\vspace{-1mm}\nWe now study how \\approachname\\ performs with tables of different sizes. Tables typically comprise a large amount of data, so the entire information is usually not required to answer a given question and acts as \\textit{distracting information}~\\citep{neeraja-etal-2021-incorporating}. This noise or irrelevant data poses a severe challenge for table understanding models and leads to poor generalization for larger tables~\\citep{kumar-etal-2023-multi, chen-2023-large}. We consider the number of cells in the table as a proxy for its size and bin all the questions in the three datasets into six categories based on the number of cells (Figure~\\ref{fig:perf_vs_size}) and compare the performance of \\approachname\\ with dataset-specific best-performing baseline. We note that for all the datasets, while model performance drops with increasing table size, \\approachname\\ consistently and significantly outperforms the baseline methods across all table size categories. Moreover, the differences become starker for larger tables. For instance, for the largest tables in FeTaQA, \\approachname\\ achieves double the S-BLEU scores compared to OmniTab ($36$ vs. $18$). Similarly, for the other two datasets, \\approachname\\ achieves significantly high performance for the largest tables ($>500$ cells) compared to the baselines -- accuracy of $38$ vs. OmniTab's $28$ for WikiTQ  and $79$ vs. ReasTAP's $70$ for WikiSQL. These empirical observations provide further evidence for \\approachname's ability to identify relevant content, making the QA LLM relatively robust to table size.\n\n\\vspace{-2mm}\n\\subsection{Discussion on the impact of different Design Choices for \\approachname}\n\\label{sec:abl_exp_design}\n\\vspace{-1mm}\n\\begin{table}[t]\n    \\begin{minipage}{.47\\textwidth}\n    \\centering\n    \\small\n        \\caption{Effect of applying clustering ($\\mathcal{L}_{clu}$), centroid separation ($\\mathcal{L}_{sep}$) and relevance score sparsification loss ($\\mathcal{L}_{sparse}$). Clustering table tokens by enforcing sparsity in relevance scores and distance between cluster centroids improves performance.}\n    \\label{tab:loss_contri}\n    \\resizebox{\\linewidth}{!}{%\n       \\resizebox{\\linewidth}{!}{\\begin{tabular}{@{}ccc|ccc@{}}\n        \\toprule\n        $\\mathcal{L}_{clu}$  & $\\mathcal{L}_{sep}$ & $\\mathcal{L}_{sparse}$\n        & \\textbf{WikiTQ} & \\textbf{FeTaQA}  & \\textbf{WikiSQL} \\\\\n        \\midrule\n         \\xmark  & \\xmark & \\xmark & 60.8  & 35.1 & 86.2 \\\\\n         \\xmark  & \\xmark & \\cmark & 60.9  & 35.1 & 86.3 \\\\\n         \\cmark  & \\xmark & \\xmark & 62.7  & 35.0 & 88.9 \\\\ \n         \\cmark  & \\xmark & \\cmark & 61.0 & 35.0  & \\textbf{89.5} \\\\ \n         \\cmark  & \\cmark & \\xmark & 61.0  & 35.1 & 89.1 \\\\ \n         \\cmark  & \\cmark & \\cmark & \\textbf{65.6} & \\textbf{35.8} & 89.3 \\\\ \n        \\bottomrule\n    \\end{tabular}}\n    }\n    \\vspace{-10pt}\n    \\end{minipage} \n    \\hfill\n    \\begin{minipage}{0.49\\textwidth}\n    \\centering\n    \\small\n            \\caption{Impact of combining unsupervised relevance score (weight $\\lambda_{uns}$) and weakly-supervised cell-based relevance score (weight $\\lambda_{cell}$). Fusing the relevance from both components gives optimal performance.}\n    \\label{tab:rel_weight}\n      \\resizebox{0.95\\linewidth}{!}{%\n        \\begin{tabular}{@{}cc|ccc@{}}\n        \\toprule\n         \\textbf{$\\lambda_{uns}$} &  \\textbf{$\\lambda_{cell}$} & \\textbf{WikiTQ}  & \\textbf{FeTaQA}, & \\textbf{WikiSQL}\\\\\n         \\midrule\n         1 & 0 & 65.6 & 35.8 & \\textbf{89.2} \\\\\n         0.7 & 0.3 & \\textbf{69.1} & \\textbf{40.5} & \\textbf{89.2} \\\\\n         0.5 & 0.5 & 68.6 & \\textbf{40.5} & 88.9 \\\\\n         0.3 & 0.7 & 67.0 & 38.9 & 88.8 \\\\\n         0 & 1 & 37.6 & 24.2 & 34.1  \\\\\n         \\bottomrule\n    \\end{tabular}}\n    \\end{minipage}\n\\end{table}\n\n\\textbf{Effect of Clustering Table Tokens:} We study the impact of clustering the table tokens using their latent representations (discussed in Section~\\ref{sec:meth_uns_relevance}). To do so, we toggle the clustering loss ($\\mathcal{L}_{clu}$), cluster centroids separation loss ($\\mathcal{L}_{sep}$), and score sparsification loss ($\\mathcal{L}_{sparse}$) by setting their weight ($\\lambda_{clu}$, $\\lambda_{sep}$, $\\lambda_{sparse}$) to 0 or 1. For this study, we only use unsupervised relevance scorer by turning off weakly supervised cell predictor to eliminate other influencing factors. Results are summarized in Table~\\ref{tab:loss_contri} where we can observe that applying all three losses yields the best performance (row 6). Specifically, for WikiSQL, clustering improves performance when score sparsification loss is applied (row 4 vs. row 3) which is due to the fact that sparsification enables categorizing scores into low and high. For WikiTQ and FeTaQA, adding the cluster centroids separation loss further increases the efficacy of clustering and sparsification yielding the best results.\n\n\\textbf{Combining Unsupervised Relevance Scorer with Cell Predictor:} We vary the relative importance given to relevance score predicted by unsupervised relevance scorer and weakly-supervised cell predictor by varying $\\lambda_{uns}$ and $\\lambda_{cell}$ in Eq.~\\ref{eq:lin_comb_eta}. Table~\\ref{tab:rel_weight} shows that combining the two modules yields much better accuracy for WikiTQ and FeTaQA compared to just using unsupervised relevance scorer (row 1 vs. row 2). This highlights that the weakly-supervised cell predictor complements unsupervised scorer by identifying further relevant table content (Figure~\\ref{fig:cluster_vis} depicts qualitative visualisation for the same). For WikiSQL, same performance is observed with and without the cell predictor. Further it is observed that using only the cell predictor (last row) achieves significantly low performance due to the fact that the number of cells highlighted by the cell predictor is much lesser resulting in assigning a score of zero to most table content in cases where it misses to identify important cells.\n\nWe show that \\approachname\\ can be used with TAPEX backbone (instead of OmniTab) to improve TAPEX performance showing the generality of our framework (Appendix~\\ref{sec:tapex_cabinet}). We show that giving parsing statement as input to QA LLM, replacing URS with off-the-shelf BERT based similarity metric for relevance scoring, and using question directly instead of parsing statement to generate highlighted cells gives sub-optimal performance compared to \\approachname, hence, justifying our design choice (Appendix~\\ref{app:reason_as_input}). Further, we show a case study to justify the rationale behind how clustering losses interact to yield improvements in Appendix~\\ref{app:case_study}.\n\n \n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width = \\textwidth]{figs/clustering_vis.pdf}\n    \\caption{Visualisation depicting that Unsupervised Relevance Scorer (URS) assigns higher score to table parts relevant to the question (rows where ``two and a half men\" either won or got nominated for an award). Further, the weakly-supervised parsing statement based relevant cell predictor identifies the cells for the row missed by URS (year 2006, golden icon award best actor - comedy series)}\n    \\vspace{-10pt}\n    \\label{fig:cluster_vis}\n\\end{figure}\n\n\\section{Conclusions}\n\\vspace{-2mm}\nWe studied the problem of question-answering over tables and focused on identifying the relevant portions of the table to derive the answer. Generally, only a small subset of the tabular data is required to answer the question, and owing to the vulnerability of LLMs to noise, the extraneous information leads to sub-optimal performance. This problem is further exacerbated in the case of large tables. Our proposed framework, \\approachname\\, addresses this issue by weighing the table content based on its relevance to the question, identifying the relevant rows and columns, and highlighting the content of the relevant cells. \\approachname\\ establishes new SoTA on three commonly used challenging benchmarks, outperforming table-specific models, as well as methods that employ in-context learning with much larger GPT-3 scale models. We show empirically that \\approachname\\ is more robust to noise and generalizes well for larger tables, indicating its efficacy in mitigating noise and overcoming table structural biases typically learned during training.\n\n \n\n\\section{Ethics and Reproducibility Statement}\nWe use publicly available datasets and LLMs (which are commonly used) to conduct the study in our work. The only data that we annotate is $\\sim$ 300 samples of table-question pairs with parsing statement describing rows and columns relevant to question. The parsing statement were written keeping in mind the safety and ethics guidelines without any potential concerns. To encourage reproducibility, we release our code and datasets (including manually written parsing statements) at this anonymous \\href{https://anonymous.4open.science/r/CABINET/}{link}. We describe the details of the datasets in \\S~\\ref{sec:exp_and_eval} (under `Datasets and Evaluation Metrics') and the LLMs used in \\S~\\ref{sec:exp_and_eval} (under `Implementation Details') and \\S~\\ref{sec:highlight}. Further, we provide the implementation details of our method in \\S~\\ref{sec:exp_and_eval} (under `Implementation Details') and discuss baselines used for comparison in \\S~\\ref{sec:main_res}. Finally, we elaborate further details of our method in Appendix - Trainable clustering over latent rerpesentation of table tokens (\\ref{sec:cluster}), Details of parsing statement annotation procedure (\\ref{sec:ques_cluster}) and Further details on table perturbation procedure (\\ref{sec:table_perturb_details}).\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2201.05966v3.tex",
        "arXiv-2301.13808v3.tex",
        "arXiv-2402.01155v3.tex"
    ],
    "group_id": "group_94",
    "response": "### Title: Advances in Large Language Models for Structured Knowledge Grounding and Table Question Answering\n\n### Introduction\nStructured knowledge grounding (SKG) and table question answering (TQA) are two critical areas within the broader field of natural language processing (NLP) and machine learning (ML). SKG involves the conversion of natural language requests into formal representations that can be executed against structured knowledge bases, such as databases or knowledge graphs, to retrieve or generate structured outputs. TQA, on the other hand, focuses on answering questions based on the information contained within tables, requiring sophisticated reasoning over both structured and unstructured data. The history of these fields is marked by the development of specialized models and techniques designed to handle specific types of structured knowledge and tasks. However, recent advancements in large language models (LLMs) have introduced a paradigm shift, enabling more general and versatile approaches to these problems.\n\nLLMs, like T5, GPT-3, and Codex, have shown remarkable capabilities in various NLP tasks, including text-to-text translation, semantic parsing, and question answering. These models are trained on vast amounts of text data, often without explicit task-specific training, and have demonstrated the potential for zero-shot and few-shot learning. Despite their success, LLMs face challenges when applied to SKG and TQA tasks, particularly in handling structured knowledge and complex questions. The main challenges include the sensitivity of LLMs to the format and structure of the input data, the difficulty of generating valid structured outputs, and the need for robustness against noise and irrelevant information in large tables.\n\nCurrent research aims to leverage the strengths of LLMs while addressing their limitations in specific task domains. This involves adapting LLMs to better understand and process structured data, as well as developing strategies to decompose complex questions and tables into more manageable components. The three papers discussed in this summary explore different aspects of these challenges, proposing new frameworks and methodologies to enhance the performance of LLMs on SKG and TQA tasks.\n\n### Main Content of Each Paper\n\n#### Paper 1: \\uskg: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models\nThe first paper introduces the \\uskg framework, which aims to unify and benchmark a diverse set of SKG tasks within a text-to-text format. The \\uskg framework standardizes datasets, models, code, experiments, and evaluation metrics, facilitating a unified approach to SKG research. The authors apply T5 models, a text-to-text transfer transformer, to individual tasks and demonstrate that T5 achieves state-of-the-art (sota) performance on almost all tasks, with performance increasing as the model size grows. The paper also explores multi-task learning with prefix-tuning, showing that this approach benefits most tasks and improves overall performance. Additionally, the authors investigate the effectiveness of zero-shot and few-shot learning with large PLMs (T0, GPT-3, and Codex) and find that these models struggle with SKG tasks, highlighting the need for task-specific training data. The study also examines the sensitivity of T5 to structured knowledge encoding variations, revealing that the order and format of input data significantly impact performance.\n\n#### Paper 2: Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning\nThe second paper proposes the DATER framework, which uses large language models (LLMs) to decompose both structured evidence (tables) and unstructured natural language questions into simpler components for effective reasoning. DATER employs a powerful LLM to extract relevant sub-evidence from large tables and a novel \"parsing-execution-filling\" strategy to decompose complex questions into logical and numerical sub-questions. The framework leverages in-context learning with a few prompting examples to guide the decomposition process. The authors evaluate DATER on three benchmark datasets (TabFact, WikiTableQuestion, and FeTaQA) and demonstrate that it achieves significantly better performance than competitive baselines, including human performance on the TabFact dataset. DATER is noted for its interpretability, as the generated sub-evidence and sub-questions provide insights into the reasoning process.\n\n#### Paper 3: CABINET: Content Relevance based Noise Reduction for Table Question Answering\nThe third paper presents CABINET, a framework designed to improve the robustness of LLMs to noise and irrelevant information in table question answering (TQA) tasks. CABINET uses an unsupervised relevance scorer to weigh table content based on its relevance to the question and a weakly supervised cell predictor to highlight relevant cells. The relevance scorer is trained differentially with the QA LLM through answer generation loss, while the cell predictor is trained separately using a parsing statement generator and a cell highlighting model. The authors evaluate CABINET on three challenging TQA datasets (WikiTableQuestion, FeTaQA, and WikiSQL) and show that it outperforms various strong baselines, including table-specific models and in-context learning methods with GPT-3 scale models. CABINET is also found to be more robust to noise and perturbations in table structure, and its performance improves with larger tables, indicating its effectiveness in mitigating noise and overcoming structural biases.\n\n### Commonalities and Innovations\nAll three papers focus on enhancing the performance of LLMs on SKG and TQA tasks, but they approach the problem from different angles. \\uskg unifies a wide range of SKG tasks under a single framework, enabling systematic benchmarking and multi-task learning. DATER introduces a decomposition strategy to break down complex questions and large tables into simpler components, facilitating fine-grained reasoning. CABINET, on the other hand, emphasizes the importance of identifying and focusing on relevant table content, reducing the impact of noise and irrelevant information.\n\nThe common theme across these papers is the use of LLMs to address the challenges of SKG and TQA tasks. However, each paper innovates in specific ways. \\uskg standardizes the input and output formats for SKG tasks, facilitating a unified evaluation and comparison of different models and techniques. DATER employs a novel \"parsing-execution-filling\" strategy to generate reliable sub-questions, which is crucial for complex reasoning tasks. CABINET uses a combination of unsupervised and weakly supervised modules to weigh and highlight relevant table content, improving the robustness and generalization of LLMs to larger and more complex tables.\n\n### Comparison of Results\nThe three papers present different sets of results, each highlighting the strengths of their proposed frameworks. \\uskg demonstrates that vanilla T5 models achieve sota performance on nearly all SKG tasks, with performance increasing as the model size grows. Multi-task prefix-tuning is shown to significantly improve performance on most tasks, indicating the potential for knowledge sharing and cross-task generalization. However, \\uskg reveals that LLMs struggle with zero-shot and few-shot learning, suggesting the need for task-specific training data.\n\nDATER achieves impressive results on the TabFact dataset, outperforming human performance for the first time. On the WikiTableQuestion dataset, DATER surpasses the best baseline by 18.3\\% in absolute terms, demonstrating its effectiveness in handling complex questions and large tables. For FeTaQA, DATER significantly outperforms Codex and other in-context learning methods, highlighting the importance of reliable sub-question generation.\n\nCABINET establishes new sota performance on all three TQA datasets (WikiTableQuestion, FeTaQA, and WikiSQL) by weighing and highlighting relevant table content. It outperforms table-specific models and in-context learning methods with GPT-3 scale models, indicating its robustness to noise and structural biases in tables. CABINET's performance gains are particularly pronounced for larger tables, suggesting its effectiveness in mitigating the impact of irrelevant information.\n\n### Conclusion\nThe three papers collectively advance the field of SKG and TQA by proposing frameworks and methodologies that enhance the performance of LLMs on these tasks. \\uskg provides a unified benchmark for SKG tasks, enabling systematic evaluation and comparison of different models and techniques. DATER introduces a decomposition strategy that facilitates fine-grained reasoning over complex questions and large tables, while CABINET focuses on reducing noise and irrelevant information in tables to improve the robustness and generalization of LLMs.\n\nThe main findings include the effectiveness of T5 models in achieving sota performance on SKG tasks, the benefits of multi-task prefix-tuning, and the importance of decomposition and relevance scoring in TQA tasks. Future research directions could explore the integration of these methodologies to develop more robust and versatile LLMs capable of handling a wide range of SKG and TQA tasks. Additionally, there is a need for more realistic and diverse datasets to better reflect the challenges of real-world applications and to further improve the interpretability and robustness of LLMs in these domains."
}