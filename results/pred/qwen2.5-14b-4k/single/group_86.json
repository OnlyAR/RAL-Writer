{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{ScoreGrad: Multivariate Probabilistic Time Series Forecasting with Continuous Energy-based Generative Models}\n\n\\begin{document}\n\n\\title{ScoreGrad: Multivariate Probabilistic Time Series Forecasting with Continuous Energy-based Generative Models}\n\n\\author{Tijin Yan,\n        Hongwei Zhang,\n        Tong Zhou,\n        Yufeng Zhan\n        and~Yuanqing Xia,~\\IEEEmembership{Senior Member,~IEEE}% <-this % stops a space\n\\thanks{Y. Xia, T. Yan, H. Zhang, T. Zhou and Y. Zhan are with the School of Automation, Beijing\n\tInstitute of Technology, Beijing 100081, China.}\n\\thanks{Corresponding author: Yuanqing Xia, Email: xia\\_yuanqing@bit.edu.cn.}\n}\n\n\\maketitle\n\n\\begin{abstract}\nMultivariate time series prediction has attracted a lot of attention because of its wide applications such as intelligence transportation, AIOps. Generative models have achieved impressive results in time series modeling because they can model data distribution and take noise into consideration. However, many existing works can not be widely used because of the constraints of functional form of generative models or the sensitivity to hyperparameters. In this paper, we propose ScoreGrad, a multivariate probabilistic time series forecasting framework based on continuous energy-based generative models. ScoreGrad is composed of time series feature extraction module and conditional stochastic differential equation based score matching module. The prediction can be achieved by  iteratively solving reverse-time SDE. To the best of our knowledge, ScoreGrad is the first continuous energy based generative model used for time series forecasting. Furthermore, ScoreGrad achieves state-of-the-art results on six real-world datasets. The impact of hyperparameters and sampler types on the performance are also explored. Code is available at \\url{https://github.com/yantijin/ScoreGradPred}. % TODO:写的太烂，需要重新改\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nTime series forecasting, generative models, stochastic differential equations, energy-based models% TODO:这里应该写什么呢？\n\\end{IEEEkeywords}\n\n\\IEEEpeerreviewmaketitle\n\n\\section{Introduction}\n\\IEEEPARstart{S}{ystems} in various domains become more and more complex in modern societies.  Fortunately, a wide range of sensors can be applied to record the status of systems. As there may be correlations between the states of a system, the recorded data from different sensors can form multivariate time series data. In order to model the evolution of states of complex systems, great efforts have previously been made for multivariate time series prediction. It has attracted much attention because of its wide applications in various fields such as intelligence transportation systems \\cite{lana2018road},  IT operations \\cite{alexandrov2019gluonts}. With the development of deep learning in recent years, much progress has been made for multivariate time series forecasting.\n\nDespite the advances in time series forecasting tasks, existing methods have some limitations. For example, some methods \\cite{su2019robust,shih2019temporal} can not model stochastic information in time series. Furthermore,  some works \\cite{salinas2020deepar,wang2019deep} can not model long range time dependencies. Recently, generative models are proved effective for sequential modeling. \\cite{oord2016wavenet} proposes TCN with dilation to model long range dependencies and constructs a generative model called WaveNet. \\cite{rasul2020multi} combines transformer and masked autoregressive flow together and achieve impressive results. However, the functional form of the models based on VAE and flow based models are constrained. TimeGrad \\cite{rasul2021autoregressive} uses an energy-based generative model (EBM) which transforms data distribution to target distribution by slowly injecting noise and achieves state of the art results on many datasets.\n\nAlthough the energy-based generative model used in TimeGrad is less restrictive on functional forms compared with VAE and flow based models, it still has some limitations. Firstly, denoising diffusion probabilistic models (DDPM) used in TimeGrad is sensitive to the noise scales injected to the original distribution. Secondly, the number of steps used for noise injection needs to be carefully designed. Thirdly, the sampling methods for generation process in DDPM can be further extended.% TODO: 第二点不太合适\n\nIn order to address these issues,  we propose ScoreGrad, a general framework for multivariate time series forecasting based on continuous energy-based generative models. DDPM can be regarded as discrete form of a stochastic differential equation (SDE) as noted by \\cite{song2020score}. Based on this assumption, the number of steps can be replaced by the interval of integration. Furthermore, the noise scales can be easily tuned by diffusion term in SDE. In the end, a conditional continuous energy-based generative model combined with sequential modeling methods is established for forecasting tasks. The prediction can be achieved by iteratively sampling from the reverse continuous-time SDE. Any numerical solvers for SDEs can be used for sampling.\n\nAll in all, the contributions of this paper can be summarized as\n\n\\begin{enumerate}\n\\item To the best of our knowledge, ScoreGrad is the first continuous energy-based generative model used for multivariate time series forecasting. %TODO: 有点别扭，是the first paper that use会好一些吗？\n\n\\item A general framework based on continuous energy-based generative models for time series forecasting is established. The training process at each step is composed of a time series feature extraction module and a conditional SDE based score matching module. The prediction can be achieved by solving reverse time SDE.\n\n\\item ScoreGrad achieves state-of-the-art results for multivariate time series forecasting on six real-world datasets with thousands of correlated dimensions.\n\n\\end{enumerate}\n\n\\section{Related work}\nIn this section, we firstly review some methods for sequential modeling and multivariate time series forecasting. Then energy-based generative models, especially score matching models are briefly introduced.\n\\subsection{Multivariate time series forecasting}\nTime series forecasting has been studied for a long time and the approaches can be split into a few categories. The statistical modeling methods such as exponential average, moving average, ARIMA \\cite{box1968some} and Gaussian process regression have been well established and applied to industrial production. More recently, deep learning based methods have achieved promising results by using the output of statistical engines as features. DeepAR \\cite{salinas2020deepar} combines traditional AR models with RNNs by modeling a probabilistic distribution in an autoencoder fashion. MQRNN \\cite{wen2017multi} uses MLPs as a decoder to solve the error accumulation issue in DeepAR.\n Furthermore, a few works explore the combination of RNNs with attention, residual connection and dilation \\cite{chang2017dilated,kim2017residual,qin2017dual}. Recently, probabilistic models which explicitly model the data distribution based on generative models like normalizing flows \\cite{de2020normalizing}, GANs \\cite{yoon2019time} achieve better performance than those of deterministic modeling methods. However, the function forms of these methods are constrained and some methods are sensitive to hyperparameters. In this paper, an auto-regressive forecasting framework based on probabilistic modeling framework is established in ScoreGrad.\n\n\\subsection{Energy-based generative models}\nEnergy-based models are un-normalized probabilistic models which specify probability density or mass function up to an unknown normalizing constant \\cite{song2021train}. Compared with VAE \\cite{kingma2013auto} and some flow-based generative models \\cite{dinh2014nice,rezende2015variational}, EBMs directly estimate the unnormalized negative log-probability and do not place a restriction on the tractability of the normalizing constants. Therefore, EBMs are much less restrictive in functional form and have wide applications in various fields such as natural language processing \\cite{mikolov2013distributed, deng2020residual}, density estimation \\cite{wenliang2019learning,song2020sliced}.\n\nAlthough EBMs can provide significant modeling advantages, the unknown normalizing constant of EBMs will make training particularly difficult. There are currently several methods for training EBMs: \n\\begin{enumerate}\n\t\\item Maximum likelihood estimation based on MCMC. \\cite{younes1999convergence} proposes to estimate the gradient of the log-likelihood with MCMC sampling methods such as Hamiltonian Monte Carlo \\cite{duane1987hybrid} instead of directly computing the likelihood.\n\t\\item Score matching based methods.  \\cite{hyvarinen2005estimation} proposes to minimize a discrepancy between the gradient of the log-likelihood of data distribution and estimated distribution with Fisher divergence. However, the optimization is computationally expensive due to the difficulty of computing the Hessian of log-density functions.\n\t\\item Noise contrastive estimation. \\cite{gutmann2010noise} is based on the idea that an EBM can be learned by contrasting it with known density. In practice, proper noise distribution is critical to the success of noise contrastive estimation.\n\\end{enumerate}\n\nIn this paper, we focus on score matching based EBMs, which have achieved state-of-the-art results \\cite{ho2020denoising,song2020score} in image generation tasks without using adversarial training methods. The closest related work to ScoreGrad is \\cite{rasul2021autoregressive}, which uses denoising diffusion probabilistic models in probabilistic time series forecasting. However, it's sensitive to the noise scales and the number of steps of noise injection. Inspired by \\cite{song2020score}, a general framework based on continuous SDE based energy-based generative models is proposed for multivariate time series forecasting. % TODO: 讲一下scoregrad和timegrad的区别， sde based energy based 表述是否合理？\n\n\\section{Score based generative models}\nIn this section, the basic theories of score matching generative models will be firstly introduced. Then two recent works that perturb data with a sequence noise scales to estimate score network are presented. The noise involvement can be seen as a discrete form of continuous-time stochastic differential equations. And then some methods from view of continuous-time SDEs for score network estimation will be introduced.\n\\subsection{Score matching models}\nAssume $\\textbf{x} \\sim p_{\\mathcal{X}}(\\textbf{x})$ denotes the distribution of a D-dimensional dataset. Score matching \\cite{hyvarinen2005estimation} is an energy-based generative model which is proposed for learning non-normalized statistical models. Instead of using maximum likelihood estimation, it aims to minimize the distance of the derivatives of the log-density function between data and model distributions. Although the density function of data distribution is unknown, the objective can be simplified as Eq. \\ref{sm_obj} based on a simple trick of partial integration. \n\\begin{equation}\\label{sm_obj}\n\t\\begin{aligned}\n\t\tL(\\theta) &= \\frac{1}{2}\\mathbb{E}_{p_{\\mathcal{X}}(\\textbf{x})}\\Vert\\nabla_\\textbf{x}\\log p_{\\theta}(\\textbf{x}) - \\nabla_\\textbf{x}\\log p_{\\mathcal{X}}(\\textbf{x})\\Vert_2^2\\\\\n\t\t=& \\mathbb{E}_{p_{\\mathcal{X}}(\\textbf{x})}[\\text{tr}(\\nabla_\\textbf{x}^2\\log p_{\\theta}(\\textbf{x})) + \\frac{1}{2}||\\nabla_\\textbf{x} \\log p_{\\theta}(\\textbf{x})||_2^2] + \\text{const}\n\t\\end{aligned}\n\\end{equation}\nwhere $p_{\\theta}(\\textbf{x})$ represents the distribution of model estimated by neural network and $\\theta$ are learnable parameters of the model. $\\nabla_{\\textbf{x}}\\log p_{\\theta}(\\textbf{x})$ is called \\textit{score function}. It's obvious that the optimal solution to Eq. \\ref{sm_obj} equals to $\\nabla_\\textbf{x}\\log p_{\\mathcal{X}}(\\textbf{x})$ for all $\\textbf{x}$ and $t$.\n\n\\subsection{Discrete score matching models}\nRecently, two classes of energy-based generative models that use various levels of noise to estimate score network have achieved good performance on image generation tasks. We will introduce them separately in the following part.\n\n\\subsubsection{Score matching with Langevin dynamics}\nSong \\cite{song2019generative} proposes score matching with Langevin dynamics (SMLD) to improve score-based generative modeling by perturbing data with various levels of noise and trains a  Noise Conditioned Score Network (NCSN) $s_{\\theta}(\\textbf{x}, \\sigma)$ to estimate scores corresponding to all noise levels. The perturbation kernel is defined as \n\\begin{equation}\\label{SMLD_D}\n\tp_{\\sigma}(\\tilde{\\textbf{x}}|\\textbf{x}):=\\mathcal{N}(\\tilde{\\textbf{x}}; \\textbf{x}, \\sigma^2\\textbf{I})\n\\end{equation}\nwhere $\\sigma$ represents noise scales. Consider a noise sequence with ascending order $\\{\\sigma_1, \\sigma_2, \\cdots, \\sigma_N\\}$, where $\\sigma_1$ is small enough such that $p_{\\sigma_1}(\\textbf{x})\\approx p_{\\mathcal{X}}(\\textbf{x})$, $\\sigma_N$ is large enough that $p_{\\sigma_N}(\\textbf{x})\\approx \\mathcal{N}(\\textbf{0}, \\sigma^2_N\\textbf{I})$. The training process is to optimize a weighted sum of denoising score matching \\cite{vincent2011connection} objective.\n\\begin{equation}\\label{smld_loss}\n\tL_{\\theta}= \\text{argmin}_{\\theta}\\sum^N_{i=1}\\mathbb{E}_{p_{\\mathcal{X}}}\\mathbb{E}_{p_{\\sigma_i}}(\\tilde{\\textbf{x}}|\\textbf{x})[\\Vert s_{\\theta}(\\tilde{\\textbf{x}}, \\sigma_i)-\\nabla_{\\tilde{\\textbf{x}}}\\log p_{\\sigma_i}(\\tilde{\\textbf{x}}|\\textbf{x})\\Vert_2^2]\n\\end{equation}\n\nAs for generation, Langevin MCMC is used for iterative sampling. Assume the number of iteration steps is M, then the sampling process for $p_{\\sigma_i}(\\textbf{x})$ can be formulated as\n\\begin{equation}\\label{smld_sample}\n\t\\textbf{x}_{i}^m = \\textbf{x}_{i}^{m-1} + \\epsilon_i s_{\\theta}(\\textbf{x}_i^{m-1}, \\sigma_i) + \\sqrt{2\\epsilon_i}\\textbf{z}_i^m, m=1,2,\\cdots,M\n\\end{equation}\nwhere $\\textbf{z}_i^m\\sim \\mathcal{N}(\\textbf{0}, \\textbf{I})$, $\\epsilon_i$ is the step size. The above iteration sampling process is repeated for N steps with $\\textbf{x}_N^0\\sim \\mathcal{N}(\\textbf{x}|\\textbf{0}, \\sigma_N^2\\textbf{I})$ and $\\textbf{x}_i^0=\\textbf{x}_{i+1}^M$ when $i<N$. According to law of large numbers, $\\textbf{x}_1^M$ will become an exact sample from $p_{\\mathcal{X}}(x)$ if $\\epsilon_i\\rightarrow 0$ and $M\\rightarrow \\infty$\n\n\\subsubsection{Denoising diffusion probabilistic models}\n\\cite{sohl2015deep,ho2020denoising} construct a sequence of noise scales $0<\\beta_i<1$, $i=1,2,\\cdots,N$. Then a discrete Markov chain is constructed as\n \\begin{equation}\\label{DDPM_D}\n p(\\textbf{x}_i|\\textbf{x}_{i-1})\\sim \\mathcal{N}(\\textbf{x}_i; \\sqrt{1-\\beta_i}\\textbf{x}_{i-1}, \\beta_i\\textbf{I})\n \\end{equation}\n \n Then the forward process can be obtained as $p(\\textbf{x}_i|\\textbf{x}_0)\\sim\\mathcal{N}(\\textbf{x}_i;\\sqrt{\\alpha_i}\\textbf{x}_0, (1-\\alpha_i)\\textbf{I})$, where $\\alpha_i=\\prod_{k=1}^i(1-\\beta_k)$. As for reverse process, a variational Markov chain is constructed as $q(\\textbf{x}_{i-1}|\\textbf{x}_i)\\sim \\mathcal{N}(\\textbf{x}_{i-1}; \\frac{1}{\\sqrt{1-\\beta_i}}(\\textbf{x}_i + \\beta_is_{\\theta}(\\textbf{x}_i, i)), \\beta_i\\textbf{I})$. The objective can be considered as a variant of the evidence lower bound (ELBO).\n\\begin{equation}\\label{ddpm_loss}\n\t\\begin{aligned}\n\t\t\tL(\\theta)&=\\text{argmin}_{\\theta}\\sum^N_{i=1}(1-\\alpha_i)  \\mathbb{E}_{p_{\\mathcal{X}}(\\textbf{x})}\\\\\n\t\t&\\mathbb{E}_{p_{\\alpha_i}(\\tilde{\\textbf{x}}|\\textbf{x})}[\\Vert s_{\\theta}(\\tilde{\\textbf{x}}, i)-\\nabla_{\\tilde{\\textbf{x}}}\\log p_{\\alpha_i}(\\tilde{\\textbf{x}}|\\textbf{x})\\Vert^2_2]\n\t\\end{aligned}\n\\end{equation}\n\nThe generation process is based on the inverse Markov chain, which depends on the estimated score network. This sampling method is called \\textit{ancestral sampling} \\cite{song2020score}.\n\\begin{equation}\\label{ddpm_sample}\n\t\\textbf{x}_{i-1}=\\frac{1}{\\sqrt{1-\\beta_i}}(\\textbf{x}_i + \\beta_i s_{\\theta}(\\textbf{x}_i, i)) + \\sqrt{\\beta_i}\\textbf{z}_i\n\\end{equation}\nwhere $i=N, N-1,\\cdots,1$\n\n\\subsection{Score matching with SDEs}\nSong \\cite{song2020score} points out that the noise involvement process of above two methods can be modeled as numerical form of stochastic differential equations. Without loss of generality, consider a SDE as \n\\begin{equation}\\label{sde}\n\td\\textbf{x}=f(\\textbf{x},t_s)dt_s + g(t_s)d\\textbf{w}\n\\end{equation}\nwhere $\\textbf{w}$ represents a standard Wiener process. $f(\\textbf{x},t_s)$ is called \\textit{drift} coefficient and $g(t_s)$ is a scalar function called \\textit{diffusion} coefficient. \\cite{anderson1982reverse} indicates that the reverse process of Eq. \\ref{sde} also satisfies a SDE  as shown in Eq. \\ref{rSDE}. Therefore, the SDE can be reversed if $\\nabla_{\\textbf{x}}\\log p_t(\\textbf{x})$ at each intermediate time step is known.\n\\begin{equation}\\label{rSDE}\n\td\\textbf{x} = [f(\\textbf{x},t_s)-g(t_s)^2\\nabla_x \\log p_{t_s}(\\textbf{x})]dt_s + g(t_s)d\\textbf{w}\n\\end{equation}\n\nIn addition, the forward process of the above two models can be treated as discrete form of continuous-time SDEs.  Specifically, Eq. \\ref{SMLD_D} can be seen as discrete form a process $\\{\\textbf{x}(t_s)\\}^1_{t_s=0}$ such that \n\\begin{equation}\\label{vesde}\nd\\textbf{x} = \\sqrt{\\frac{d[\\sigma^2(t_s)]}{dt_s}}d\\textbf{w}\n\\end{equation}\nNote that Eq. \\ref{vesde} gives a process with exploding variance when $t\\rightarrow \\infty$ and is called Variance Exploding (VE) SDE.\n\nSimilarly, when $N\\rightarrow \\infty$, Eq. \\ref{DDPM_D} converges to Eq. \\ref{vpsde}, which is called Variance Preserving (VP) SDE  because the variance $\\boldsymbol{\\Sigma}(t)$ is always bounded given $\\boldsymbol{\\Sigma}(0)$.\n\\begin{equation}\\label{vpsde}\n\td\\textbf{x} = -\\frac{1}{2}\\beta(t_s)\\textbf{x}dt_s + \\sqrt{\\beta(t_s)}d\\textbf{w}\n\\end{equation}\nThen a new type of SDE called sub-VP SDE is proposed as Eq. \\ref{subvpsde}, it can be proved that the variance is always upper bounded by the corresponding VP SDE\n\\begin{equation}\\label{subvpsde}\n\td\\textbf{x} = -\\frac{1}{2}\\beta(t_s)\\textbf{x}dt_s + \\sqrt{\\beta(t_s)(1-e^{-2\\int^{t_s}_0\\beta(s)ds})}d\\textbf{w}\n\\end{equation}\n\nThe trained score network can be used to construct reverse-time SDE and use numerical methods to sample from $p_0$. \\cite{song2020score} also proposes predictor-corrector (PC) methods for sampling, detail information can be found in Appendix \\ref{prd_cor}. % TODO: 在附录中加入相关采样器的推导\n\nAll in all, we list the relationship between these three score matching based generative models in Fig. \\ref{Gen}. The forward and reverse process are listed in Table \\ref{table_1}. %In addition, the target distribution of VE SDE is set as $\\mathcal{N}(\\textbf{0}, \\sigma^2\\textbf{I})$. As for VP SDE and sub-VP SDE, the target distribution is set as standard Gaussian $\\mathbb{N}(\\textbf{0}, \\textbf{I})$.\n\n\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[height=0.13\\textheight]{./fig1.eps} \n\t\\caption{Forward and reverse process of score based generative models.}\n\t\\label{Gen}\n\\end{figure}\n\\begin{table}[!t]\n\\caption{Comparison of DDPM, SMLD and SDE based score matching models.}\n\\label{table_1}\n\\centering\n\\begin{tabular}{|c|c|c|}\n\\hline\n\\diagbox{Methods}{Direction} & Forward Process & Reverse Process\\\\\n\\hline\nSMLD & Eq. \\ref{SMLD_D} &Eq. \\ref{smld_sample} \\\\\n\\hline\nDDPM & Eq. \\ref{DDPM_D} & Eq. \\ref{ddpm_sample}\\\\\n\\hline\nSDE-Based& Eq. \\ref{sde}  & Eq. \\ref{rSDE} \\\\\n\\hline\n\\end{tabular}\n\\end{table}\n\n\\section{Method}\nIn this section, the symbols and problem definition will be firstly presented. Then, the model architecture of ScoreGrad and the network architecture of conditional score network is established. In the end, training and sampling procedure will be introduced.\n\\subsection{Symbol and problem formulation}\nConsider a D-dimensional multivariate time series defined as $\\mathcal{X}=\\{\\textbf{x}_1^0, \\textbf{x}_2^0, \\cdots, \\textbf{x}_T^0\\}$, where $T$ is length of $\\mathcal{X}$. The probabilistic prediction tasks for multivariate time series can be converted to estimate $q_{\\mathcal{X}}(\\textbf{x}_{t_0:T}^0|\\textbf{x}_{1:t_0-1}^0,\\textbf{c}_{1:T})$, where $\\textbf{c}_{1:T}$ represents covariates which are known for all time points. In this paper, an iterative forecasting strategy is adopted for given prediction time steps.\n\\begin{equation}\\label{iter_pred}\n\tq_{\\mathcal{X}}(\\textbf{x}_{t_0:T}^0|\\textbf{x}_{1:t_0-1}^0,\\textbf{c}_{1:T}) = \\prod_{t=t_0}^Tq_{\\mathcal{X}}(\\textbf{x}_t^0|\\textbf{x}_{t-1}^0, \\textbf{c}_{1:T})\n\\end{equation}\n\n\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=0.48\\textwidth]{./fig2.eps} \n\t\\caption{Model architecture at time step t.}\n\t\\label{Arch}\n\\end{figure}\n\n\\subsection{Model architecture}\nThe general framework of ScoreGrad can be found in Fig. \\ref{Arch}. Actually, the framework can be divided into two modules at each time step:  Time series feature extraction module and conditional SDE based score matching module. Detail introduction of these two modules are presented in the following. \n\n\\subsubsection{Time series feature extraction module} The module aims to get a feature $\\textbf{F}_t$ of historical time series data until time $t-1$ and update $\\textbf{F}_t$ for different time steps. The update function $R$ for $\\textbf{F}_t$ can be defined as \n\\begin{equation}\\label{F_update}\n\t\\textbf{F}_t = R(\\textbf{F}_{t-1}, \\textbf{x}_{t-1}, \\textbf{c}_{t-1})\n\\end{equation}\n\n It's a general framework and many sequential modeling methods can be used here. For example, $F_t$ corresponds to hidden state $\\textbf{h}_{t-1}$ if recurrent neural networks like RNN, GRU are used. For other sequential modeling methods like temporal convolutional networks (TCN) \\cite{oord2016wavenet}, attention based networks \\cite{vaswani2017attention}, $\\textbf{F}_t$ is a vector that represents the features learned by historical data and covariates. What's more, the iterative forecasting strategy in Eq. \\ref{iter_pred} can be converted to a conditional prediction problem in Eq. \\ref{cond_pred}.\n\\begin{equation}\\label{cond_pred}\n\t\\prod_{t=t_0}^Tq_{\\mathcal{X}}(\\textbf{x}_t^0|\\textbf{x}_{t-1}^0, \\textbf{c}_{1:T}) = \t\\prod_{t=t_0}^Tp_{\\theta}(\\textbf{x}_t^0|\\textbf{F}_{t})\n\\end{equation}\nwhere $\\theta$ represents learnable parameters of the time series feature extraction module. In this paper, recurrent neural networks are used in ScoreGrad by default.\n\n\\subsubsection{Conditional SDE based score matching module.} As shown in Fig. \\ref{ScoreNet}, $\\textbf{F}_t$ is used as a conditioner for SDE based score matching models at each time step. We use the states corresponding to time t to introduce the structure of this module. The initial state distribution is $p(\\textbf{x}_t^0|\\textbf{F}_t)$, the forward evolve process follows Eq. \\ref{sde}. Then the conditioned reverse-time SDE can be modified as\n\\begin{equation}\\label{crsde}\n\td\\textbf{x}_t = [f(\\textbf{x}_t,t_s)-g(t_s)^2\\nabla_x \\log p_{t_s}(\\textbf{x}_t|\\textbf{F}_{t})]dt_s + g(t_s)d\\textbf{w}\n\\end{equation}\nwhere $t_s\\in [0, T_s]$ represents the integral time. If the conditional score function $\\nabla_x \\log p_{t_s}(\\textbf{x}_t|\\textbf{F}_{t})$ is known at each intermediate time step, the reverse continuous-time SDE can be solved and we can generate conditional samples with numerical SDE solvers. In this paper, the conditional score function is implemented with a neural network. The detail structure of the conditional score function will be illustrated in the next part.\n\n\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=0.48\\textwidth]{./fig3.eps} \n\t\\caption{Architecture of score network}\n\t\\label{ScoreNet}\n\\end{figure}\n\n\\subsection{Conditional score network} \nInspired by WaveNet \\cite{oord2016wavenet} and DiffWave \\cite{kong2020diffwave}, the conditioned score network is designed to have 8 residual blocks and the structure of a single block is shown in Fig. \\ref{ScoreNet}. The input for conditioned score network includes feature $\\textbf{F}_t$, input state $\\textbf{x}_{t}^{t_s}$ and corresponding time $t_s$. $\\textbf{x}_{t}^{t_s}$ is transformed with 1D CNNs with filter size of 1 and 3. The embedding module for $t_s$ is changed from positional embeddings \\cite{vaswani2017attention} to random Fourier feature embeddings \\cite{tancik2020fourier} compared with discrete form in \\cite{ho2020denoising}. $\\textbf{F}_t$ is served as an conditioner for score network. The sum of hidden representation of inputs and time embeddings is sent to a bidirectional dilated convolution block. Then the sum of hidden representations of conditioner and the output of dilated convolution block is sent to a gated activation unit \\cite{oord2016wavenet}. Then one part of output serves as output of the block and the other one is summed up with skip-connection input and then used as the input of the next block. In the end, the output of all blocks are summed up and transformed with 1D CNN for final output.\n\n\\subsection{Training} \nIn the training process, a multivariate time series data is sent to the network. At time step t, the conditioned SDE based score matching module and time series feature extraction module can be trained via a continuous form of the loss function in Eq. \\ref{ddpm_loss} and Eq. \\ref{smld_loss}.\n\\begin{equation}\\label{L_t}\n\t\\begin{aligned}\n\t\tL_t(\\theta) &= \\text{argmin}_{\\theta}\\mathbb{E}_{t_s} \\big(\\lambda(t_s)\\mathbb{E}_{\\textbf{x}_{t}^0} \\mathbb{E}_{\\textbf{x}_t^{t_s}|\\textbf{x}_t^0} \\\\\n\t\t&\\big[\\Vert s_{\\theta}(\\textbf{x}_t^{t_s}, \\textbf{F}_t, t_s) -\\nabla_{\\textbf{x}_t^{t_s}}\\log p_{0t_s}(\\textbf{x}_t^{t_s}|\\textbf{x}_t^0)\\Vert_2^2 \\big]\\big)\\\\\n\t\\end{aligned}\n\\end{equation}\nwhere $\\lambda: [0, T_s]\\rightarrow \\mathbb{R}_{>0}$ is a weighting function, $t_s$ is randomly sampled from closed interval $[0, T_s]$. As shown by \\cite{song2020score}, we can choose $\\lambda \\propto\\mathbb{E}\\big[\\Vert\\nabla_{\\textbf{x}_t^{t_s}}\\log p_{0t_s}(\\textbf{x}_t^{t_s}|\\textbf{x}_t^0\\Vert_2^2\\big]$. Then the total loss for a sequential multivariate time series $\\{\\textbf{x}_t^{0}, t=1,2,\\cdots, T\\}$ is \n\\begin{equation}\n\tL(\\theta) = \\frac{1}{T}\\sum^T_{t=1}L_t(\\theta)\n\\end{equation}\n\nAll in all, the training procedure can be summarized as Algorithm \\ref{train}. The loss value has a strong correlation with the type of SDE and the target distribution. In other words, the prediction performance cannot be judged according to loss value. More discussion can be found in appendix \\ref{app_loss}.\n\\begin{algorithm}\n\t\\caption{Training procedure of ScoreGrad}\n\t\\label{train}\n\t\\begin{algorithmic}\n\t\t\\Require data $\\textbf{x}_t^0, t=1,2,\\cdots T$, integration time $T_s$\n\t\t\\State Get $\\textbf{F}_{1:T}$ according to Eq \\ref{F_update}.\n\t\t\\Repeat\n\t\t\\State loss = 0\n\t\t\\For{$t\\leftarrow$ 1 to $T$}\n\t\t\\State Initialize $t_s\\sim \\text{Uniform}(0, T_s)$, $\\textbf{z}\\sim\\mathcal{N}(\\textbf{0},\\textbf{I})$\n\t\t\\State Calculate mean $\\textbf{m}_t^{t_s}$ and standard deviation $\\textbf{v}_t^{t_s}$ of marginal probability $p(\\textbf{x}_t^{t_s}|\\textbf{x}_t^0)$ for given SDE.\n\t\t\\State Get samples at $t_s$: $\\textbf{x}_t^{t_s} = \\textbf{m}_t^{t_s} + \\textbf{z}*\\textbf{v}_t^{t_s}$ \n\t\t\\State Calculate $L_t(\\theta)$ according to Eq. \\ref{L_t}.\n\t\t\\State loss += $L_t(\\theta)$\n\t\t\\EndFor\n\t\t\\State\tloss /= T\n\t\t\\State Take gradient step on $\\nabla_{\\theta}\\text{loss}$\n\t\t\\Until{converged}\n\t\\end{algorithmic}\n\\end{algorithm}\n\n\\subsection{Prediction}\nThe prediction process can be converted into iterative sampling from the reverse continuous-time SDE. Detail sampling procedure at time step t is shown in Fig \\ref{Sampling}. Firstly, get samples from target distribution and denote the samples as $\\textbf{x}_t^{T_s}$. Then the feature state $\\textbf{F}_t$, samples $\\textbf{x}_t^{T_s}$ and corresponding time $T_s$ is sent to the sampler, which is a numerical solver of Eq \\ref{rSDE}. After solving the reverse SDE, the prediction value $\\textbf{x}_{t}^0$ for time step t can be obtained. Then the prediction value $\\textbf{x}_{t}^0$, covariate $\\textbf{c}_t$ and feature state $\\textbf{F}_t$ is sent to \\textit{time series feature extraction module} to get feature state at the next moment, which can be used for predicting values of next moment. Repeat the steps as above and we can get prediction values for given prediction time steps in the future.\n\nAs for sampler, it can be designed based on \\textit{predictor-corrector} (PC) samplers as shown in \\cite{song2020score}. The PC samplers can be regarded as a generalization of the original sampling methods. Specifically, a numerical SDE solver is used to get an initial estimation of the sample in the \\textit{predictor} module. For example, Eq. \\ref{smld_sample} and Eq. \\ref{ddpm_sample} can be used as a predictor for continuous SMLD or DDPM models when $s_{\\theta}$ is replaced by conditioned score network. Then an score-based MCMC approach is used to correct the marginal distribution of the sample in \\textit{corrector} module. For example, annealed Langevin  MCMC can be used for corrector. In this framework, the sampler of SMLD can be regarded as a special PC sampler where predictor is an identity function. Similarly, the sampler of DDPM can be regarded as a PC sampler where corrector is an identity function. Detail derivation of predictor and corrector can be found in Appendix \\ref{prd_cor}. All in all, the design of sampler can be summarized as Algorithm \\ref{alg_sampler}. \n\n\\begin{algorithm}\n\t\\caption{Detail procedure of sampler at time step t.}\n\t\\label{alg_sampler}\n\t\\begin{algorithmic}\n\t\t\\Require feature state $\\textbf{F}_t$, integration time $T_s$,\n\t\t number of time steps for reverse SDE $N$ and corrector $M$\n\t\t\\State Initialize $\\textbf{x}_t^{T_s}\\sim p_{\\text{target}}(\\textbf{x})$.\n\t\t\\State Divide interval $[0,T_s]$ into $N$ sections, that is $\\{t_s^i\\, i=1,2,\\cdots N\\}$.\n\t\t\\For{$k\\leftarrow\\text{N-1}$ to 1}\n\t\t\\State $\\textbf{x}_t^k\\leftarrow$ Predictor$(\\textbf{x}_{t}^{k+1}, \\textbf{F}_t, t_s^{k+1})$.\n\t\t\\For{$j\\leftarrow$ 1 to $M$}\n\t\t\\State $\\textbf{x}_t^k\\leftarrow$ Corrector$(\\textbf{x}_{t}^{k}, \\textbf{F}_t, t_s^{k+1})$.\t\n\t\t\\EndFor\n\t\t\\EndFor\n\t\\end{algorithmic}\n\\end{algorithm}\n\n\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=0.48\\textwidth]{./fig4.eps} \n\t\\caption{Sampling procedure at time step t.}\n\t\\label{Sampling}\n\\end{figure}\n\n\\begin{table}[!t]\n\t\\caption{Properties of the datasets, including dimensions, domain, frequency, time steps and prediction length.}\n\t\\label{table_2}\n\t\\centering\n\t\\begin{tabular}{cccccc}\n\t\t\\hline\n\t\tDATA SET&DIM.&DOM.&FREQ.&\\makecell[c]{TIME\\\\STEPS}&\\makecell{PRED.\\\\ LEN.}\\\\\n\t\t\\hline\n\t\tEXCHANGE&8&$\\mathbb{R}^+$&DAY&6071&30\\\\\n\t\tSOLAR&137&$\\mathbb{R}^+$&HOUR&7009&24\\\\\n\t\tELECTRICITY&370&$\\mathbb{R}^+$&HOUR&5833&24\\\\\n\t\tTRAFFIC&963&$(0,1)$&HOUR&4001&24\\\\\n\t\tTAXI&1214&$\\mathbb{N}$&30-MIN&1488&24\\\\\n\t\tWIKIPEDIA&2000&$\\mathbb{N}$&DAY&792&30\\\\\n\t\t\\hline\n\t\\end{tabular}\n\\end{table}\n\n\\section{Experiments}\nIn this section, ScoreGrad are evaluated on six real-world datasets. Basic properties of datasets, hyperparameter settings of ScoreGrad and evaluation metrics will be firstly introduced. Several recent works for multivariate time series are used for comparison. In the end, some ablation studies are carried out for analyzing the influence of various modules on the performance of ScoreGrad.\n\n\\subsection{Datasets description and evaluation metrics}\nIn the experiments, \\textit{Exchange} \\cite{lai2018modeling}, \\textit{Solar} \\cite{lai2018modeling}, \\textit{Electricity}, \\textit{Traffic}, \\textit{Taxi} and \\textit{Wikipedia} are used for evaluation. Detail properties of the datasets can be found in Table \\ref{table_2}. Embeddings for categorical features which describe the relationships within a category is assumed can be captured when training time series models. The preprocessing process is exactly the same as in \n\\cite{salinas2019high}. In order to eliminate the impact of data value on the performance of ScoreGrad, we divide each time series by their context window mean before feeding to the model. And the samples are rescaled to the original scale in the sampling process. As for covariates $\\textbf{c}_t$, time dependent embeddings (e.g. hour of day, day of week) and time independent embeddings such as lag features depending on the time frequency of the dataset are used. Furthermore, it should be noted that all covariates are known for all the periods we want to predict.\n\nAs for evaluation metrics, Continuous Ranked Probability Score (CRPS) \\cite{matheson1976scoring} on each time series dimension and the sum of all time series dimensions (denoted as $\\text{CRPS}_{\\text{sum}}$) are employed, which are the same with those in \\cite{rasul2020multi,rasul2021autoregressive}. CRPS is used to measure the compatibility of a cumulative distribution function (CDF) $F$ with an observation $x$ as\n\\begin{equation}\\label{crps}\n\t\\text{CRPS}(F,x)=\\int_{\\mathbb{R}}(F(z)-\\mathbb{I}\\{x\\leq z\\})^2dz\n\\end{equation}\nwhere\n$$\\mathbb{I}\\{x\\leq z\\}=\\left\\{\n\\begin{array}{rcl}\n\t1& x\\leq z\\\\\n\t0 & \\text{otherwise}\n\\end{array}\n\\right.$$\n\nIt is obvious that Eq. \\ref{crps} attains minimum value when the predictive distribution $F$ is equal to the data distribution. Therefore, it's a proper scoring function for evaluation on time series forecasting tasks. Although exact CDF is intractable, it can be empirically estimated as $\\hat F=\\frac{1}{S}\\sum^S_{s=1}\\mathbb{I}\\{x_s\\leq z\\}$, where $x_s$ represents samples from $F$. Then the CRPS value at each time step can be attained from Eq. \\ref{iter_pred} according to \\cite{jordan2017evaluating}. In addition, $\\text{CRPS}_{\\text{sum}}$ can be obtained by summing across different dimensions and then averaged over the prediction horizon, that is\n\\begin{equation}\n\t\\text{CRPS}_{\\text{sum}} = \\mathbb{E}_t\\big[\\text{CRPS}(\\hat F_{sum}(t), \\sum_ix_{i,t}^0)\\big]\n\\end{equation}\nwhere integer $i\\in [0, D)$ represents the $i^{th}$ dimension of time series.\n\nInstead of using likelihood based metrics, $\\text{CRPS}_{\\text{sum}}$ is selected as a proper scoring function for time series forecasting tasks, which is the same with that in TimeGrad \\cite{rasul2021autoregressive}.\n\\begin{table*}[!t]\n\t\\caption{Comparion of $\\text{CRPS}_{\\text{sum}}$ of the methods on six real-world datasets (lower is better).}\n\t\\label{table_3}\n\t\\centering\n\t\\scalebox{1.25}{\n\t\t\\begin{tabular}{|c|cccccc|}\n\t\t\t\\hline\n\t\t\t\\diagbox{Method}{Dataset}&Exchange&Solar&Electricity&Traffic&Taxi&Wikipedia\\\\\n\t\t\t\\hline\n\t\t\tVAR& \\textbf{0.005}$\\pm$\\textbf{0.000}& 0.840$\\pm$0.007& 0.038$\\pm$0.003& 0.291$\\pm$0.005&- &- \\\\\n\t\t\t\\hline\n\t\t\tLasso-VAR& 0.011$\\pm$0.001&0.521$\\pm$0.006 &0.026$\\pm$0.000 & 0.153$\\pm$0.002& -&3.142$\\pm$0.007 \\\\\n\t\t\t\\hline\n\t\t\tVES&0.006$\\pm$0.000 &0.896$\\pm$0.004 &0.883$\\pm$0.004 &0.362$\\pm$0.003 & - &- \\\\\n\t\t\t\\hline\n\t\t\tGARCH& 0.023$\\pm$0.000&0.884$\\pm$0.002 &0.193$\\pm$ 0.001&0.376$\\pm$0.002 &- &- \\\\\n\t\t\t\\hline\n\t\t\tKVAE&0.014$\\pm$003 &0.339$\\pm$0.022 &0.0502$\\pm$0.019 &0.108$\\pm$0.006 &- &0.095$\\pm$0.012 \\\\\n\t\t\t\\hline\n\t\t\t\\makecell[c]{Vec-LSTM\\\\lowrank-Copula}&0.007$\\pm$0.000 &0.319$\\pm$0.011 &0.064$\\pm$0.008 &0.103$\\pm$0.006 &0.326$\\pm$0.007 &0.241$\\pm$0.033 \\\\\n\t\t\t\\hline\n\t\t\t\\makecell[c]{Vec-LSTM\\\\ind-scaling}&0.008$\\pm$0.001 &0.391$\\pm$0.017 &0.025$\\pm$0.001 &0.087$\\pm$0.041 &0.506$\\pm$0.005 &0.133$\\pm$0.002 \\\\\n\t\t\t\\hline\n\t\t\tGP scaling&0.009$\\pm$0.000 &0.368$\\pm$0.011 &0.022$\\pm$0.000 &0.079$\\pm$0.000 &0.183$\\pm$0.039 &1.483$\\pm$1.034 \\\\\n\t\t\t\\hline\n\t\t\tGP Copula&0.007$\\pm$0.000 &0.337$\\pm$0.024 &0.025$\\pm$0.002 &0.078$\\pm$0.002 &0.208$\\pm$0.018 &0.086$\\pm$0.004 \\\\\n\t\t\t\\hline\n\t\t\tTransformer-MAF&\\textbf{0.005}$\\pm$\\textbf{0.003}&0.301$\\pm$0.014 &0.0207$\\pm$0.000 &0.056$\\pm$0.001 &0.179$\\pm$0.002 &0.063$\\pm$0.003 \\\\\n\t\t\t\\hline\n\t\t\tTimeGrad&0.006$\\pm$0.001 &0.287$\\pm$0.020 &0.0206$\\pm$0.001 &0.049$\\pm$0.006 &0.114$\\pm$0.02 &0.050$\\pm$0.002 \\\\\n\t\t\t\\hline\n\t\t\t\\makecell[c]{\\textbf{ScoreGrad}\\\\ \\textbf{VP SDE}}&0.006$\\pm$0.001 &\\textbf{0.268}$\\pm$\\textbf{0.021} & \\textbf{0.0192}$\\pm$\\textbf{0.001}&0.043$\\pm$0.004 &\\textbf{0.102}$\\pm$\\textbf{0.006} &\\textbf{0.041}$\\pm$\\textbf{0.003} \\\\\n\t\t\t\\hline\n\t\t\t\\makecell[c]{\\textbf{ScoreGrad}\\\\ \\textbf{VE SDE}}&0.007$\\pm$0.001 &0.277$\\pm$0.011 &0.0199$\\pm$0.001 &\\textbf{0.037}$\\pm$\\textbf{0.003} & 0.104$\\pm$0.009& 0.046$\\pm$0.002\\\\\n\t\t\t\\hline\n\t\t\t\\makecell[c]{\\textbf{ScoreGrad}\\\\ \\textbf{sub-VP SDE}}&0.006$\\pm$0.001 &\\textbf{0.256}$\\pm$\\textbf{0.015} &\\textbf{0.0194}$\\pm$\\textbf{0.001} &\\textbf{0.041}$\\pm$\\textbf{0.004} &\\textbf{0.101}$\\pm$\\textbf{0.004} & \\textbf{0.043}$\\pm$\\textbf{0.002}\\\\\n\t\t\t\\hline\n\t\\end{tabular}}\n\\end{table*}\n\n\\subsection{Competitive methods}\nSeveral statistical methods and deep learning based methods for multivariate time series forecasting are used as baselines for comparison study. Here we give brief introductions to these methods.\n\\begin{enumerate}\n\t\\item \\textbf{VAR} \\cite{lutkepohl2005new} is a linear vector auto-regressive model with lag sequences based on the period of time series data. \\textbf{Lasso-VAR} is similar with VAR except an extra Lasso regularization term added into the model.\n\t\\item \\textbf{VES} \\cite{hyndman2008forecasting} is based on a state space model, which is also a classical auto-regressive model for sequential modeling.\n\t\\item \\textbf{GARCH} \\cite{van2002go} is a multivariate conditional heteroskedastic model, which exploits unconditional information first in order to avoid convergence difficulties.\n\t\\item \\textbf{KVAE} \\cite{fraccaro2017disentangled} combines Kalman filter and variational auto-encoder together and disentangles object's representation and the dynamics.\n\t\\item \\textbf{Vec-LSTM} \\cite{salinas2019high} combines an RNN-based time series model with Gaussian copula process with a low-rank covariance structure, which is called \\textbf{Vec-LSTM-lowrank-Copula}. In addition, we also choose another variant called \\textbf{Vec-LSTM-ind-Scaling} for comparison, which outputs the parameters of an independent Gaussian distribution based on mean scaling method.\n\t\\item \\textbf{GP-Scaling} \\cite{salinas2019high} unrolls an LSTM with scaling on each dimensions separately and output the distribution based on a low-rank Gaussian distribution. Different from \\textbf{GP-Scaling}, \\textbf{GP-Copula} obtains the joint emission distribution based on a low-rank plus diagonal covariance Gaussian copula.\n\t\\item \\textbf{Transformer-MAF} \\cite{rasul2020multi} proposes to combine Transformer with Masked Autoregressive Flow \\cite{papamakarios2017masked} and achieves promising results on probabilistic multivariate time-series forecasting tasks.\n\t\\item \\textbf{TimeGrad} \\cite{rasul2021autoregressive} proposes to combine RNN and discrete energy-based generative models for sequential modeling and achieves state of the art results on the six datasets.\n\\end{enumerate}\n\n\\subsection{Hyperparameter settings}\nThe length of time window in the training process is set to twice the prediction length. Two layers of GRU are used for time series feature extraction module and the dimension of hidden states is set to 40 for \\textit{Exchange}, \\textit{Solar}, \\textit{Electricity} and 128 for the other three big datasets. The integral interval of SDEs $t_s$ is set to $[0, 1]$. For VP SDE and sub-VP SDE, $\\beta(t_s)$ is set as a linear transformation of $t_s$. As for VE SDE, $\\sigma(t_s)$ is set as an exponential transformation of $t_s$. The range of $\\beta(t_s)$ and $\\sigma(t_s)$ are based on the type of SDE and dataset according to \\cite{song2020improved}. $\\sigma(0)$ is chosen to be as large as the maximum Euclidean distance between all pairs of training data points. In addition, we find that the sampling process is sometimes unstable in the experiments, exponential moving average (EMA) is applied to parameters to solve this issue according to Technique 5 in \\cite{song2020improved}. \n\nAs for sampling, we use predictor-corrector samplers. The predictor module can be any numerical solvers for SDE and we list three different predictors in Appendix \\ref{pred_md}. The corrector module is implemented with MCMC approaches. Here we use Langevin dynamics and annealed Langevin dynamics to construct corrector. The comparison between these samplers is given in the following parts. What's more, it's worth mentioning that the number of diffusion steps in sampling is very important. In order to explore the effect of hyperparameters on the performance of model, some ablation experiments are carried out for ScoreGrad. More details about ScoreGrad can be found at \\url{https://github.com/yantijin/ScoreGradPred}.\n\n\\subsection{Results}\n\nThree different SDEs are evaluated in the framework of ScoreGrad. Table \\ref{table_3} illustrates the performance of ScoreGrad and the other eleven models on the datasets. The mean and standard deviation of $\\text{CRPS}_{\\text{sum}}$ values are obtained by 10 runs. The top two results on each dataset are bolded in the table.\n\nIt can be known from Table \\ref{table_3} that deep learning based methods generally perform better than statistical methods, especially on big datasets. Therefore, it's important to model the nonlinear dependencies of multivariate time series data. KVAE represents the methods that use autoencoder with variational inference for sequential modeling. The dimension of latent variable has a significant impact on performance. Vec-LSTM and its variants use RNN and Gaussian copula process with a low-rank covariance structure to handle non-Gaussian distribution. GP Copula performs better than KVAE on all datasets. Transformer-MAF adopts transformer to further improve the power of modeling time series and uses normalizing flows to model the distribution of time series. It performs better than GP copula on all datasets. However, the function form of Transformer-MAF is constrained. TimeGrad replaces normalizing flows with DDPM and obtains impressive results on all but the smallest dataset \\textit{exchange}. However, TimeGrad is sensitive to the noise scales and number of steps for noise injection. % （1）基于深度学习的方法大体趋势上都比统计方法要好，因为能建模非线性的依赖 （2）Copula方法的Vec-LSTM以及GP在大多数数据集上的性能比mean scaling或者设想独立的方法要好 （3）将transformer和MAF结合起来，既能保存自回归模型的能力，也能获得建模高维数据的灵活性。 但连续流模型对函数形式的限制很多 TimeGrad采用离散的EBM方法在大体上取得了不错的效果，但是离散的形式对于很多参数比较敏感，而ScoreGrad则建立了一个框架，将每一步分为时序特征提取和将所有离散的EBM进行连续化，在多个数据集上取得了sota结果。 三种SDE的比较是如何的？ 在数据集上的比较是什么？\n\nDDPM is treated as a special discrete form of SDE in ScoreGrad and we propose continuous energy-based generative models for time series forecasting. It's obvious that all the three methods obtain state-of-the-art results on these datasets except the smallest dataset \\textit{Exchange}. Specifically, ScoreGrad based on VP SDE performs better than ScoreGrad based on VE SDE in most cases. However, ScoreGrad based on VE SDE obtains best results on Traffic. Furthermore, the results of ScoreGrad based on sub-VP SDE are basically the same with those of ScoreGrad based on VP SDE on \\textit{Electricity}, \\textit{Taxi} and \\textit{Wikipedia}. As for \\textit{Solar}, ScoreGrad based on sub-VP SDE outperforms all the other methods. Besides, it can be known that the variance of ScoreGrad based on sub-VP SDE is slightly smaller than that of ScoreGrad based on VP SDE from table \\ref{table_3}, which is consistent with the derivation.\n\nAs shown in Fig. \\ref{pred_expample}, the first six channels of the test set of Traffic are used as an example of multivariate time series prediction using ScoreGrad based on VP SDE. The blue and green lines represent the observation values and median prediction. The green areas cover 50\\% and 90\\% distribution intervals based on VP SDE. It's obvious that most observation values are within 50\\% distribution intervals. In addition, the observation values are very small and the range of values of different channels may vary greatly. Therefore, scaling all the values by dividing the context window mean is necessary for model training.\n\n\\begin{figure*}[t]\n\t\\centering\n\t\\includegraphics[width=0.9\\textwidth]{./pred_example_traffic.pdf} \n\t\\caption{Prediction intervals of ScoreGrad based on VP SDE and actual observations of Traffic.}\n\t\\label{pred_expample}\n\\end{figure*}\n\n\\subsection{Ablation study}\n\\subsubsection{Comparison of samplers}\nIn order to explore the impact of the type of samplers on the performance of prediction, we firstly load a trained model and then change the type of samplers while keeping the other parameters unchanged. In this paper, there are three different predictors and two different correctors. Therefore, there are six different samplers. Besides, we also compare the performance of PC samplers with or without corrector. All in all, there are nine different samplers for a certain trained model. The performance of these samplers based on three trained models on \\textit{Solar} is shown in Fig. \\ref{comp_sampler}. \n\nIt's worth mentioning that we can not directly compare the performance between three models in Fig. \\ref{comp_sampler} because they only represent the result of a certain training, not the average of multiple training.  For ScoreGrad based on VP SDE, PC samplers with corrector performs better than those without corrector. However, PC samplers using annealed Langevin dynamics as corrector performs slightly worse than those without corrector for ScoreGrad based on VE SDE for \\textit{Solar}. Furthermore, the combination of reverse diffusion sampler and Langevin dynamics performs best on VP SDE and VE SDE. In addition, it can be known that reverse diffusion sampler always outperforms ancestral sampling on this dataset. Besides, the variance of sub-VP SDE is smaller than that of VP SDE, which is consistent to the derivation. Ancestral sampling is not feasible for sub-VP SDE. As for the remaining six samplers, the combination of reverse diffusion sampler and annealed Langevin dynamics performs best. The comparison of samplers on the other five datasets can be found in Appendix \\ref{cp_s}.\n\n\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=0.5\\textwidth]{./comp_sampler.pdf} \n\t\\caption{Comparison of $\\text{CRPS}_{\\text{sum}}$ on Solar when using different samplers (lower is better).}\n\t\\label{comp_sampler}\n\\end{figure}\n\n\\subsubsection{Effect of diffusion steps}\nThe number of diffusion steps $N$ in ScoreGrad corresponds to the number of noise injection steps in DDPM. TimeGrad is sensitive to the number of steps of noise injection. In contrast, the forward process of ScoreGrad doesn't depend on the diffusion steps because it employs a continuous energy-based generative model and can get the state at any moment. Therefore, it avoids the problem in TimeGrad. Actually, the number of diffusion steps is only used in the sampling process in ScoreGrad. In order to explore the effect of the diffusion steps on the performance of the model, we compare $\\text{CRPS}_{\\text{sum}}$ of \\textit{Electricity} by varying the diffusion or noise injection steps of ScoreGrad and TimeGrad. Specifically, we set $N=20,40,\\cdots, 260$ and obtain the mean and standard error metrics by retraining and evaluating 5 times for each $N$. The results are shown in Fig. \\ref{scale_comp}.\n\nThe performance of ScoreGrad is basically the same with that of TimeGrad when $N=20$. In addition, it's worth mentioning that the performance of ScoreGrad and TimeGrad are already better than that of KVAE and Vec-LSTM with low rank Copula process when $N=20$, which indicates that energy-based models are suitable for modeling time series. With the increase of $N$ when $N\\leq 100$, the performance of TimeGrad becomes better. The optimal value of noise injection steps $N$ is approximately 100 for TimeGrad. Although larger $N$ allows the reverse process to be approximately an Gaussian for TimeGrad according the derivation above, larger $N$ will hurt the performance. ScoreGrad based on VP SDE and VE SDE obtain best result when $N=100$ while $N=180$ for sub-VP SDE. Furthermore, all the three methods of ScoreGrad perform better than TimeGrad when $N\\geq 100$. $\\text{CRPS}_{\\text{sum}}$ values of these three methods fluctuate in a small range with the increase of $N$. We also find similar behavior with the other datasets. Therefore, the performance of ScoreGrad based on VP SDE, VE SDE and sub-VP SDE are more robust to the changes of $N$ compare to TimeGrad. Continuous energy-based generative models can avoid setting the number of injection steps in the training process, which is important in numerical form of SDEs as shown in TimeGrad.\n\n\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=0.49\\textwidth]{./scale.pdf} \n\t\\caption{Comparison of $\\text{CRPS}_{\\text{sum}}$ of ScoreGrad and TimeGrad on the test set of \\textit{Electricity} data by varying the number of steps of noise injection (lower is better).} % 这里是of还是for?\n\t\\label{scale_comp}\n\\end{figure}\n\n\\subsection{Other applications}\nWhen we plot prediction values based on ScoreGrad, we find the values of some channels do not follow history patterns. For example, Fig. \\ref{abnormal} illustrates the prediction intervals of a simple channel of \\textit{Electricity} based on ScoreGrad. It's obvious that the data exhibits the periodic property. However, the values decrease suddenly on September 1st. There are many probabilities for this phenomenon, such as the difference of patterns between holidays and weekdays, power failure in local area. In any case, there should be real-time monitoring to ensuring the reliability of service. \n\nScoreGrad can be also used for time series anomaly detection. For example, if observation values are not in the area of 90\\% prediction intervals for some time (e.g., one hour), the alarm information should be sent to concerned people for more attention. It's obvious that the values on September 1st will get more attention under such a rule.\n\n\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=0.49\\textwidth]{./abnormal.pdf} \n\t\\caption{The prediction of a single channel of \\textit{Electricity} based on ScoreGrad. } % 这里是of还是for?\n\t\\label{abnormal}\n\\end{figure}\n\n\\section{Conclusion and future work}\nIn this paper, we propose ScoreGrad, a multivariate probabilistic time series forecasting framework that adopts continuous energy-based generative models to learn the distribution of data. The framework can be divided into time series feature extraction module and conditional SDE based score matching module. Compared with VAE or flow based models, EBMs have few constrains on function forms. In addition, the connection between EBMs and SDEs enable EBMs to be built within a broader framework. ScoreGrad achieves state-of-the-art results on six real world datasets. What's more, the ablation studies and analysis of samplers indicate that ScoreGrad can better model multivariate time series compared with existing works.\n\nThere is still much work to be done based on the proposed framework. The first direction is about improvement of the network architecture. Powerful sequential modeling methods like Transformer can be used for time series feature extraction module. More SDEs can be designed for conditional SDE based score matching module. The second direction is about reducing computation cost of sampling. \\cite{song2020denoising} proposes to use non-Markov process which allowed for fast sampling. \\cite{shen2019randomized} proposes a fast sampling method for log-concave sampling based on underdamped Langevin diffusion. The third direction is to establish the connection between loss and metrics. The loss function  used in this paper is not directly related to the prediction performance, which is not friendly for the design and evaluation of new models based on the framework. %The fourth direction is about target distribution. Gaussian distribution is used in this paper for target distribution. Actually, various distributions or stochastic processes can be used as target distributions. \nWe leave these three directions as future work.\n\n\\appendices\n\\section{Train loss of ScoreGrad}\\label{app_loss}\nThe prediction performance can not be judged by the loss value. For example, Fig. \\ref{loss} illustrates the training loss of ScoreGrad and TimeGrad on Electricity. Each curve is obtained by training 5 times. It can be known that although the loss value of TimeGrad is smaller than that of ScoreGrad based on sub-VP SDE and VE SDE, the performance in table \\ref{table_3} is not consistent with loss value. Therefore, it's not convenient to design new SDEs for ScoreGrad. We leave it as future work to establish the connection between metrics and training loss as future work.\n\n\\begin{figure}[t]\n\t\\centering\n\t\\includegraphics[width=0.49\\textwidth]{./loss.pdf} \n\t\\caption{Train loss of ScoreGrad and TimeGrad on Electricity.} % 这里是of还是for?\n\t\\label{loss}\n\\end{figure}\n\n\\section{Predictor-Corrector Sampler}\\label{prd_cor}\nAs noted in \\cite{song2020score}, the predictor can be any numerical solver for the reverse-time SDE with a fixed discretization strategy. The corrector can be implemented with any score-based MCMC approach. \n\n\\subsection{Predictor methods}\\label{pred_md}\nGeneral numerical solver for SDEs such as Euler-Maruyama and stochastic Runge-Kutta methods can be used as predictor. Without loss of generality, given an reverse-time SDE as Eq. \\ref{crsde}, the Euler-Maruyama methods can be formulated as\n\\begin{equation}\\label{euler}\n\t\\begin{aligned}\n\t\t\\textbf{x}_{t}^{i} &= \\textbf{x}_{t}^{i+1} - g(t_{i+1})\\sqrt{\\delta} \\textbf{z}_{i+1}\\\\\n\t\t &- (f(\\textbf{x}_t^{i+1}, t_{i+1}) - g(t_{i+1})^2 s_{\\theta}^*(\\textbf{x}_t^{i+1}, t_{i+1}, \\textbf{F}_t)) \\delta\n\t\\end{aligned}\n\\end{equation}\nwhere $\\delta$ represents step size, $i=1,2,\\cdots, N$, $\\textbf{z}_{i+1}$ follows standard Gaussian distribution.\t In addition, ancestral sampling method in Eq. \\ref{ddpm_sample} has been proved to be a special discretization to the same reverse-time SDE \\cite{song2020score}.\n\\begin{equation}\\label{ances_vp}\n\t\\begin{aligned}\n\t\\textbf{x}_t^{i-1}&=\\frac{1}{\\sqrt{1-\\beta_i}}(\\textbf{x}_t^i + \\beta_i s_{\\theta}(\\textbf{x}_t^i, i, \\textbf{F}_t)) + \\sqrt{\\beta_i}\\textbf{z}_i\\\\\n\t&=(2-\\sqrt{1-\\beta_{i}})\\textbf{x}_t^{i} + \\beta_{i}s_{\\theta}(\\textbf{x}_t^i, i, \\textbf{F}_t) + \\sqrt{\\beta_{i}}\\textbf{z}_i\n\t\\end{aligned}\n\\end{equation}\n\nThe ancestral sampling method for SMLD can be similarly derived as \n\\begin{equation}\\label{ances_ve}\n\t\\textbf{x}_t^{i-1} = \\textbf{x}_t^{i} + (\\sigma_i^2 - \\sigma_{i-1}^2) s_{\\theta}^*(\\textbf{x}_t^{i}, i, \\textbf{F}_t) + \\sqrt{\\frac{\\sigma_{i-1}^2(\\sigma_i^2-\\sigma_{i-1}^2)}{\\sigma_i^2}} \\textbf{z}_{i}\n\\end{equation}\n\nDeriving the ancestral samplers for new SDEs can be non-trivial. For example, ancestral sampling is not feasible for sub-VP SDE. Song \\cite{song2020score} also proposes a new sampler called \\textit{reverse diffusion sampler}, which can be used here with minor modification\n\\begin{equation}\\label{reverse_diff}\n\t\\begin{aligned}\n\t\t\\textbf{x}_{t}^{i} =&~~ \\textbf{x}_{t}^{i+1}  + g(t_{i+1})^2 s_{\\theta}^*(\\textbf{x}_t^{i+1}, t_{i+1}, \\textbf{F}_t)\\\\\n\t\t& + g(t_{i+1})\\textbf{z}_{i+1} - f(\\textbf{x}_t^{i+1}, t_{i+1})\n\t\\end{aligned}\n\\end{equation}\nwhere $\\textbf{z}_{i+1}$ follows standard Gaussian distribution.\n\nAll in all, the predictor in Algorithm \\ref{alg_sampler} can be set as Eq. \\ref{euler}, Eq. \\ref{reverse_diff} or ancestral sampling methods in Eq. \\ref{ances_ve} and Eq. \\ref{ances_vp} based on the type of SDEs.\n\n\\subsection{Corrector methods}\nMCMC methods such as Hamiltonian Monte Carlo can be used as corrector. In this paper, we use Langevin dynamics for sampling. In addition, annealed Langevin dynamics with minor modifications can also be used for corrector module and detail procedure for VE SDE and VP SDE can be found in Algorithm \\ref{corr_VE} and Algorithm \\ref{corr_VP}. \n\nAs for sub-VP SDE, there are no similar discrete form as VP SDE. Therefore, $\\alpha_i$ is set as 1 for computing step size when Algorithm \\ref{corr_VP} is used for sub-VP SDE. $r$ is called \"signal-to-noise\" ratio and is set as 0.16 in the experiments.\n\n\\begin{algorithm}\n\t\\caption{Corrector algorithm at each time step for VE SDE.}\n\t\\label{corr_VE}\n\t\\begin{algorithmic}\n\t\t\\Require $\\{\\sigma_i\\}_{i=1}^N, r,N, M$\n\t\t\\State $\\textbf{x}_N^{0}\\sim \\mathcal{N}(\\textbf{0}, \\sigma_{\\text{max}}^2\\textbf{I})$\n\t\t\\For{$i\\leftarrow\\text{N}$ to 1}\n\t\t\\For{$j\\leftarrow$ 1 to $M$}\n\t\t\\State $\\textbf{z}\\sim \\mathcal{N}(\\textbf{0}, \\textbf{I})$\n\t\t\\State $\\textbf{g}\\leftarrow \\textbf{s}_{\\theta}(\\textbf{x}_i^{j-1}, \\sigma_i, \\textbf{F}_t)$\n\t\t\\State $\\epsilon\\leftarrow 2(r\\Vert\\textbf{z}\\Vert_2/\\Vert \\textbf{g}\\Vert_2)^2$\n\t\t\\State $\\textbf{x}_{i}^j\\leftarrow \\textbf{x}_i^{j-1} + \\epsilon \\textbf{g} + \\sqrt{2\\epsilon}\\textbf{z}$\n\t\t\\EndFor\n\t\t\\State $\\textbf{x}_{i-1}^0\\leftarrow\\textbf{x}_i^M$\n\t\t\\EndFor\\\\\n\t\t\\Return $\\textbf{x}_0^0$\n\t\\end{algorithmic}\n\\end{algorithm}\n\n\\begin{algorithm}\n\t\\caption{Corrector algorithm at each time step for VP SDE.}\n\t\\label{corr_VP}\n\t\\begin{algorithmic}\n\t\t\\Require $\\{\\beta_i\\}_{i=1}^N, \\{\\alpha_i\\}_{i=1}^N r,N, M$\n\t\t\\State $\\textbf{x}_N^{0}\\sim \\mathcal{N}(\\textbf{0}, \\textbf{I})$\n\t\t\\For{$i\\leftarrow\\text{N}$ to 1}\n\t\t\\For{$j\\leftarrow$ 1 to $M$}\n\t\t\\State $\\textbf{z}\\sim \\mathcal{N}(\\textbf{0}, \\textbf{I})$\n\t\t\\State $\\textbf{g}\\leftarrow \\textbf{s}_{\\theta}(\\textbf{x}_i^{j-1}, i, \\textbf{F}_t)$\n\t\t\\State $\\epsilon\\leftarrow 2\\alpha_i(r\\Vert\\textbf{z}\\Vert_2/\\Vert \\textbf{g}\\Vert_2)^2$\n\t\t\\State $\\textbf{x}_{i}^j\\leftarrow \\textbf{x}_i^{j-1} + \\epsilon \\textbf{g} + \\sqrt{2\\epsilon}\\textbf{z}$\n\t\t\\EndFor\n\t\t\\State $\\textbf{x}_{i-1}^0\\leftarrow\\textbf{x}_i^M$\n\t\t\\EndFor\\\\\n\t\t\\Return $\\textbf{x}_0^0$\n\t\\end{algorithmic}\n\\end{algorithm}\n\\section{Comparison of samplers}\\label{cp_s}\nThe performance of the model on the other five datasets when using different samplers for evaluation are shown in Fig. \\ref{5_plot}.\n\\begin{figure*}[t]\n\t\\centering\n\t\\includegraphics[width=0.88\\textwidth]{./five_plot.pdf} \n\t\\caption{Comparison of samplers on the other five datasets. } % 这里是of还是for?\n\t\\label{5_plot}\n\\end{figure*}\n\n\\ifCLASSOPTIONcaptionsoff\n  \\newpage\n\\fi\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Generative Time Series Forecasting with Diffusion, Denoise, and Disentanglement}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\n\nTime series forecasting has been a widely explored task of great importance in many applications.\nHowever, it is common that real-world time series data are recorded in a short time period, which results in a big gap between the deep model and the limited and noisy time series. \nIn this work, we propose to address the time series forecasting problem with generative modeling and propose a bidirectional variational auto-encoder (BVAE) equipped with diffusion, denoise, and disentanglement, namely \\ourmodel.\nSpecifically, a coupled diffusion probabilistic model is proposed to augment the time series data without increasing the aleatoric uncertainty and implement a more tractable inference process with BVAE. \nTo ensure the generated series move toward the true target, we further propose to adapt and integrate the multiscale denoising score matching into the diffusion process for time series forecasting. \nIn addition, to enhance the interpretability and stability of the prediction, we treat the latent variable in a multivariate manner and disentangle them on top of minimizing total correlation. \nExtensive experiments on synthetic and real-world data show that \\ourmodel~outperforms competitive algorithms with remarkable margins. \nOur implementation is available at \\url{https://github.com/PaddlePaddle/PaddleSpatial/tree/main/research/D3VAE}. \n\n\\end{abstract}\n\n\\setcounter{footnote}{0}\n\n\\section{Introduction}\n\nTime series forecasting is of great importance for risk-averse and decision-making.~%\nTraditional RNN-based methods capture temporal dependencies of the time series to predict the future. \nLong short-term memories (LSTMs) and gated recurrent units (GRUs)~\\cite{yu2019review,greff2016lstm,gers2002learning,sherstinsky2020fundamentals} introduce the gate functions into the cell structure to handle long-term dependencies effectively.~%\nThe models based on convolutional neural networks (CNNs)\ncapture complex inner patterns of the time series through convolutional operations~\\cite{lea2016temporal,borovykh2017conditional,binkowski2018autoregressive}.~%\nRecently, the Transformer-based models have shown great performance in time series forecasting~\\cite{xu2021autoformer,zhou2021informer,kitaev2020reformer,li2019enhancing} with the help of multi-head self-attention. \nHowever, one big issue of neural networks in time series forecasting is the   uncertainty~\\cite{gawlikowski2021survey,abdar2021review} resulting from the properties of the deep structure.~%\nThe models based on vector autoregression (VAR)~\\cite{cao2003support,fokianos2009poisson,kim2003financial} try to model the distribution of time series from hidden states, which could provide more reliability to the prediction, while the performance is not satisfactory~\\cite{lai2018modeling}. \n\nInterpretable representation learning is another merit of time series forecasting.~%\nVariational auto-encoders (VAEs) have shown not only the superiority in modeling latent distributions of the data and reducing the gradient noise~\\cite{roeder2017sticking,kingma2016improved, li2016renyi, vahdat2020nvae} but also the interpretability of time series forecasting~\\cite{fortuin2020gp,fortuin2019som}. \nHowever, the interpretability of VAEs might be inferior due to the entangled latent variables. \nThere have been  efforts to learn representation disentangling~\\cite{kim2018disentangling,bengio2013representation,higgins2016beta}, which show that the well-disentangled representation can  improve the performance and robustness of the algorithm.\n\nMoreover, real-world time series are often noisy and recorded in a short time period,  which may result in overfitting and generalization issues~\\cite{gamboa2017deep,wang2018gaussian,zou2019complex,arima_2020}{\\footnote{The detailed literature review can be found in \\cref{sec:related}.}}.~%\nTo this end, we address the time series forecasting problem with generative modeling.\nSpecifically, we propose a bidirectional variational auto-encoder (BVAE) equipped with diffusion, denoise, and disentanglement, namely \\ourmodel. \nMore specifically, \nwe first propose a coupled diffusion probabilistic model to remedy the limitation of time series data by augmenting the input time series, as well as the output time series, inspired by the forward process of the diffusion model~\\cite{sohl2015deep,ho2020denoising,nichol2021improved,rasul2021autoregressive}.~%\nBesides, \nwe adapt the Nouveau VAE~\\cite{vahdat2020nvae} to the time series forecasting task and develop a BVAE as a substitute for the reverse process of the diffusion model.~%\nIn this way, the expressiveness of the diffusion model plus the tractability of the VAE can be leveraged together for generative time series forecasting.~%\nThough the merit of generalizability is helpful, the diffused samples might be corrupted, which results in a generative model moving toward the noisy target.\nTherefore, we further develop a scaled denoising score-matching network for cleaning diffused target time series.~%\nIn addition, we disentangle the latent variables of the time series by assuming that different disentangled dimensions of the latent variables correspond to different temporal patterns (such as trend, seasonality, etc.). \nOur contributions can be summarized as follows: \n\n\\begin{itemize}\n\\item\nWe propose a coupled diffusion probabilistic model aiming to reduce the aleatoric uncertainty of the time series and improve the generalization capability of the generative model. \n\n\\item\nWe integrate the multiscale denoising score matching into the coupled diffusion process to improve the accuracy of generated results. \n\n\\item \nWe disentangle the latent variables of the generative model to improve the interpretability for time series forecasting. \n\n\\item \nExtensive experiments on synthetic and real-world datasets demonstrate that \\ourmodel ~outperforms  competitive  baselines with satisfactory  margins.\n\n\\end{itemize}\n\n\\section{Methodology} \n\\label{sec:method}\n\n\\subsection{Generative Time Series Forecasting}\n\n{\\bf Problem Formulation.~}\nGiven an input multivariate time series $X = \\{x_1, x_2, \\cdots, x_n \\,|\\, x_i \\in \\mathbb{R}^{d} \\} $ and the corresponding target time series $ Y = \\{y_{n+1}, y_{n+2}, \\cdots , y_{n+m} \\,|\\, y_j \\in \\mathbb{R}^{d'} \\}$ ($ d^{'} \\leq d $).~%\nWe assume that $Y$ can be generated from latent variables $Z \\in \\Omega_Z$ that can be drawn from the Gaussian distribution $Z \\sim p(Z|X)$. \nThe latent distribution can be further formulated as $ p_\\phi(Z|X) = g_\\phi(X) $ where  $g_\\phi $ denotes a nonlinear function.\nThen, the data density of the target series is given by:\n\\begin{equation}\np_\\theta(Y) = \\int_{\\Omega_Z} p_\\phi(Z|X)( Y - f_\\theta(Z) ) dZ \\, ,\n\\end{equation}\nwhere $f_\\theta$ denotes a parameterized function.~%\nThe target time series can be obtained directly  by sampling from  $p_\\theta(Y)$. \n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=.9\\textwidth]{fig/figure5/framework_crv.png}\n    \\caption{\n    The framework overview of \\ourmodel. \n    First, the input and output series are augmented simultaneously with the {\\em coupled diffusion process}.~%\n    Then the diffused input series are fed into a proposed BVAE model for inference, which can be deemed a {\\em reverse process}.~%\n    A denoising score-matching mechanism is applied to make the estimated target move toward the true target series. \n    Meanwhile, the latent states in BVAE are leveraged for disentangling such that the model interpretability and reliability can be improved.\n    }\t\n    \\label{fig:method:framework}\n\\end{figure}\n\nIn our problem setting, \ntime series forecasting is to learn the representation $Z$ that captures useful signals of $X$, \nand map the low dimensional $X$ to the latent space with high expressiveness. \nThe framework overview of \\ourmodel ~is demonstrated in \\cref{fig:method:framework}. \nBefore diving into the detailed techniques, we first introduce a preliminary proposition. \n\n\\begin{prop} \\label{prop1}\nGiven a time series $X$ and its inherent noise  $\\epsilon_X$,  we have the  decomposition: $X = \\langle X_{r}, \\, \\epsilon_X \\rangle $, where $X_r$ is the ideal time series data without noise. \n$ X_{r}$ and $\\epsilon_X $ are independent of each other. \nLet $p_{\\phi} (Z|X) = p_{\\phi} (Z| X_{r} , \\epsilon_X) $,\nthe estimated target series $\\widehat{Y}$  can be generated with the distribution $p_{\\theta} (\\widehat{Y}|Z) = p_{\\theta} (\\widehat{Y}_r|Z) \\cdot p_{\\theta} (\\epsilon_{\\widehat{Y}}|Z)$ where $ \\widehat{Y}_r $ is the ideal part of $ \\widehat{Y} $ and $ \\epsilon_{\\widehat{Y}} $ is the estimation noise. \nWithout loss of generality, $ \\widehat{Y}_r $ can be fully captured by the model.\nThat is,  $ \\| Y_r - \\widehat{Y}_r \\| \\longrightarrow 0 $ where $Y_r$ is the ideal part of ground truth target series $Y$.\nIn addition, $Y$ can be decomposed as $Y = \\langle \\widehat{Y}_r , \\epsilon_Y \\rangle$ ($\\epsilon_Y$ denotes the noise of $Y$). \nTherefore, the error between ground truth and prediction, i.e., $ \\| Y - \\widehat{Y} \\| = \\| \\epsilon_Y - \\epsilon_{\\widehat{Y}} \\| > 0$, \ncan be deemed as the combination of aleatoric uncertainty and epistemic uncertainty.\n\\end{prop}\n\n\\subsection{Coupled Diffusion Probabilistic Model} \\label{sec2.2}\n\nThe diffusion probabilistic model (diffusion model for brevity) is a family of latent variable models aiming to generate high-quality samples. \nTo equip the generative time series forecasting model with high expressiveness, a coupled {\\em forward process} is developed to augment the input series and target series synchronously. \nBesides, in the forecasting task, more tractable and accurate prediction is expected. \nTo achieve this, we propose a bidirectional variational auto-encoder (BVAE) to take the place of the {\\em reverse process} in the diffusion model. \nWe present the technical details in the following two parts, respectively. \n\n\\subsubsection{Coupled Diffusion Process}\n\nThe forward diffusion process is fixed to a Markov chain that gradually adds Gaussian noise to the data \\cite{sohl2015deep,ho2020denoising}.~%\nTo diffuse the input and output series, we propose a coupled diffusion process, which is demonstrated in \\cref{fig:diffusion_process}.~%\nSpecifically, given the input  $X = X^{(0)} \\sim q(X^{(0)})$, the approximate posterior $q(X^{(1:T)}|X^{(0)})$  can be obtained as \n\\begin{equation}\n    q(X^{(1:T)}|X^{(0)}) = \\prod_{t=1}^{T} q(X^{(t)}|X^{(t-1)})\\, , \\quad \n    q(X^{(t)}|X^{(t-1)}) = \\mathcal{N} (X^{(t)}; \\sqrt{1 - \\beta_t} X^{(t)}, \\beta_t I) \\, ,\n\\end{equation}\nwhere a uniformly increasing \nvariance schedule $ \\bm{\\beta} = \\{\\beta_1, \\cdots, \\beta_{T} \\, | \\, \\beta_t \\in [0, 1) \\} $  is employed to control the level of noise to be added. \nThen, let $ \\alpha_t = 1 - \\beta_t$ and $\\bar{\\alpha}_t = \\prod_{s=1}^{t}\\alpha_s$, we have\n\\begin{equation}    \\label{diff}\n    q(X^{(t)}|X^{(0)}) = \\mathcal{N} ( X^{(t)}; \\sqrt{\\bar{\\alpha}_t} X^{(0)}, (1 - \\bar{\\alpha}_t) I) \\, .\n\\end{equation}\n\nFurthermore, according to \\cref{prop1} we decompose $X^{(0)}$ as $X^{(0)} = \\langle X_r, \\epsilon_X \\rangle$.~%\nThen, with \\cref{diff}, the diffused $X^{(t)}$ can be decomposed as follows: \n\\begin{equation}    \\label{x_t}\n    X^{(t)} = \\sqrt{\\bar{\\alpha}_t}X^{(0)} + (1-\\bar{\\alpha}_t)\\delta_X \n    \\vcentcolon = \n    \\langle \n    \\underbrace{\\sqrt{\\bar{\\alpha}_t}X_r}_{\\text{ideal}\\,\\, \\text{part}}, \\,\n    \\underbrace{\\sqrt{\\bar{\\alpha}_t}\\epsilon_X + (1-\\bar{\\alpha}_t)\\delta_{X}}_{\\text{noisy} \\,\\, \\text{part}}\n    \\rangle \n    \\, ,\n\\end{equation}\nwhere $\\delta_X$ denotes the standard Gaussian noise of $X$. \nAs $\\bm{\\alpha}$ can be determined when the variance schedule $\\bm{\\beta}$ is known, the ideal part is also determined in the diffusion process. \nLet $\\widetilde{X}_r^{(t)} = \\sqrt{\\bar{\\alpha}_t}X_r$ and ${\\delta}_{\\widetilde{X}}^{(t)} = \\sqrt{\\bar{\\alpha}_t} \\epsilon_X + (1-\\bar{\\alpha}_t) \\delta_X $,  \nthen, according to \\cref{prop1} and \\cref{x_t}, we have   \n\\begin{equation}    \\label{eq:target}\n    p_\\phi (Z^{(t)} | X^{(t)}) = p_\\phi (Z^{(t)}|\\widetilde{X}_r^{(t)}, {\\delta}_{\\widetilde{X}}^{(t)}) \n\\, , \\quad \n    p_\\theta (\\widehat{Y}^{(t)}|Z^{(t)}) = p_\\theta (\\widehat{Y}_r^{(t)} | Z^{(t)})  p_\\theta ({\\delta}_{\\widehat{Y}}^{(t)} | Z^{(t)})\n    \\, ,\n\\end{equation}\nwhere $ \\delta_{\\widehat{Y}}^{(t)} $ denotes the generated noise of $\\widehat{Y}^{(t)}$.~%\nTo relieve the effect of aleatoric uncertainty resulting from time series data, we further apply the  diffusion process to the target series $Y = Y^{(0)} \\sim q(Y^{(0)})$. \nIn particular, \na scale parameter $\\omega \\in (0, 1)$ is adopted, such that \n$ {\\beta}_t^{\\prime} = \\omega \\beta_t, \\alpha_t^{\\prime} = 1 - {\\beta}_t^{\\prime}$ \nand \n$ \\bar{\\alpha}^{\\prime}_{t} =  \\prod_{s=1}^{t} \\alpha_s^{\\prime}$. \nThen, according to \\cref{prop1}, we can obtain the following decomposition (similar to \\cref{x_t}):\n\\begin{equation}    \\label{eq:true}\n    Y^{(t)} = \\sqrt{\\bar{\\alpha}^{\\prime}_t} Y^{(0)} + (1-\\bar{\\alpha}^{\\prime}_t) \\delta_Y \n    \\vcentcolon = \n    \\langle \n    \\underbrace{\\sqrt{\\bar{\\alpha}^{\\prime}_t}Y_r}_{\\text{ideal} \\, \\text{part}},\n    \\underbrace{\\sqrt{\\bar{\\alpha}^{\\prime}_t}\\epsilon_{Y} + (1-\\bar{\\alpha}^{\\prime}_t)\\delta_Y}_{\\text{noisy} \\, \\text{part}} \n    \\rangle \n    = \\langle \\widetilde{Y}_r^{(t)}, {\\delta}_{\\widetilde{Y}}^{(t)} \\rangle \\, . \n\\end{equation}\nConsequently, we have \n$  q(Y^{(t)})  =  q(\\widetilde{Y}_r^{(t)})  q({\\delta}_{\\widetilde{Y}}^{(t)}) $.\nAfterward, we can draw the following conclusions with \\cref{prop1} and \\cref{eq:target,eq:true}. \nThe proofs can be found in \\cref{appendix:lemma:derivation}. \n\n\\begin{figure}[t]\n    \\centering\n    \\begin{minipage}[t]{1.0\\textwidth}\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{fig/figure1/diffusion/diff_p.png}\n    \\caption{\n    An illustration of the coupled diffusion process.\n    The input $X^{(0)}$ and the corresponding target $Y^{(0)}$ are diffused simultaneously with different variance schedules.\n    $\\bm{\\beta} = \\{ \\beta_1, \\cdots, \\beta_T \\}$ is the variance schedule for the input and $\\bm{\\beta}^{\\prime} = \\{ \\beta_1^{\\prime},  \\cdots, \\beta_T^{\\prime} \\}$ is for the target.\n    }   \\label{fig:diffusion_process}\n    \\end{minipage}\n\\end{figure}\n\n\\begin{lemma}  \\label{theo1}\n$ \\forall \\varepsilon > 0 $, \nthere exists a probabilistic model $ \\, f_{\\phi, \\theta} \\vcentcolon = (p_{\\phi}, p_{\\theta})$ \nto guarantee that $ \\mathcal{D}_{\\mathrm{KL}} (q(\\widetilde{Y}_r^{(t)}) || p_{\\theta}(\\widehat{Y}_r^{(t)})) < \\varepsilon $, \nwhere $ \\widehat{Y}_r^{(t)} = f_{\\phi,\\theta} (X^{(t)})  $. \n\\end{lemma}\n\n\\begin{lemma} \\label{theo2}\nWith the coupled diffusion process, \nthe difference between  diffusion noise and  generation noise will be reduced, \ni.e., \n$ \\lim_{t \\rightarrow \\infty} \\mathcal{D}_{\\mathrm{KL}} (q ({\\delta_{\\widetilde{Y}}^{(t)}}) || p_{\\theta} (\\delta_{\\widehat{Y}}^{(t)} | Z^{(t)}) ) < \n\\mathcal{D}_{\\mathrm{KL}} (q(\\epsilon_Y) || p_{\\theta}(\\epsilon_{\\widehat{Y}}))$ . \n\\end{lemma}\n\nTherefore, the uncertainty raised by the generative model and the inherent data noise can be reduced through the coupled diffusion process. \nIn addition, the diffusion process simultaneously augments the input series and the target series, which can improve the generalization capability for (esp. short) time series forecasting. \n\n\\subsubsection{Bidirectional Variational Auto-Encoder}\n\nTraditionally, in the diffusion model, a reverse process is adopted to generate high-quality samples~\\cite{sohl2015deep,ho2020denoising}. \nHowever, for the generative time series forecasting problem, \nnot only the expressiveness but also the supervision of the ground truths should be considered. \nIn this work, \nwe employ a more efficient generative model, i.e.,  bidirectional variational auto-encoder (BVAE)~\\cite{vahdat2020nvae}, to take the place of the reverse process of the diffusion model.~%\nThe architecture of BVAE is described in~\\cref{fig:method:framework} where $Z$ is treated in a multivariate fashion $Z = \\{z_1, \\cdots, z_n\\}$ ($ z_i \\in \\mathbb{R}^{m}, z_i = [z_{i,1}, \\cdots, z_{i,m}] $) and $z_{i+1} \\sim p(z_{i+1}|z_{i}, X)$.\nThen, $n$ is determined in accordance with the number of residual blocks in the encoder, as well as the decoder. \nAnother merit of BVAE is that it opens an interface to integrate the disentanglement for improving model interpretability (refer to \\cref{sec:disentangle}).\n\n\\subsection{Scaled Denoising Score Matching for Diffused Time Series Cleaning} \\label{sec_denoise}\n\nAlthough the time series data can be augmented with the aforementioned coupled diffusion probabilistic model, the generative distribution $ p_{\\theta}(\\widehat{Y}^{(t)}) $ tends to move toward the diffused target series $ Y^{(t)} $ which has been corrupted~\\cite{li2019learning, song2019generative}. \nTo further ``clean'' the generated target series, we employ the Denoising Score Matching (DSM) to accelerate the de-uncertainty process without sacrificing the model flexibility. \nDSM~\\cite{vincent2011connection,li2019learning} was proposed to link Denoising Auto-Encoder (DAE)~\\cite{vincent2010stacked} to Score Matching  (SM)~\\cite{hyvarinen2009estimation}.\nLet $ \\widehat{Y} $ denote the generated target series, then we have the objective \n\\begin{equation}\n    L_{\\text{DSM}}(\\zeta) = \\mathbb{E}_{ p_{\\sigma_0} (\\widehat{Y}, Y)} \n    \\| \\nabla_{\\widehat{Y}} \\log(q_{\\sigma_0} (\\widehat{Y}|Y)) + \\nabla_{\\widehat{Y}} E(\\widehat{Y}; \\zeta) \\|^{2} \\, ,\n\\end{equation}\nwhere $p_{\\sigma_0} (\\widehat{Y}, Y)$ is the joint density of pairs of corrupted and clean samples $(\\widehat{Y}, Y)$, \n$ \\nabla_{\\widehat{Y}} \\log(q_{\\sigma_0} (\\widehat{Y}|Y)) $ is derivative of the log density of a single noise kernel, which is dedicated to replacing the Parzen density estimator: $ p_{\\sigma_0} (\\widehat{Y}) = \\int q_{\\sigma_0} (\\widehat{Y}|Y) p(Y) dY$ in score matching, \nand $E(\\widehat{Y}; \\zeta)$ is the energy function. \nIn the particular case of Gaussian noise, \n$\\log(q_{\\sigma_0} (\\widehat{Y}|Y)) = - (\\widehat{Y} - Y)^2/2 {\\sigma_0}^2 + C$. \nThus, we have \n\\begin{equation}\n    L_{\\text{DSM}}(\\zeta) = \\mathbb{E}_{p_{\\sigma_0}(\\widehat{Y}, Y)} \\| Y - \\widehat{Y} + \\sigma_0^2 \\nabla_{\\widehat{Y}} E(\\widehat{Y}; \\zeta) \\|^{2} \\, .\n\\end{equation}\nThen, for the diffused target series at step $t$, we can obtain \n\\begin{equation}\n    L_{\\text{DSM}}(\\zeta, t) = \\mathbb{E}_{p_{\\sigma_0} (\\widehat{Y}^{(t)}, Y)} \\| Y - \\widehat{Y}^{(t)} + \\sigma_0^2 \\nabla_{\\widehat{Y}^{(t)}} E(\\widehat{Y}^{(t)}; \\zeta) \\|^2 \\, .\n\\end{equation}\nTo scale the noise of different levels~\\cite{li2019learning}, a monotonically decreasing series of fixed $\\sigma$ values $ \\{ \\sigma_1, \\cdots, \\sigma_T \\,|\\, \\sigma_t = 1-\\bar{\\alpha}_t \\}$ (refer to the aforementioned variance schedule $\\bm{\\beta}$ in \\cref{sec2.2}) is adopted.\nTherefore, the objective of the multi-scaled DSM is \n\\begin{equation}    \\label{eq:dsm}\n    L(\\zeta,t) = \\mathbb{E}_{q_{\\sigma}(\\widehat{Y}^{(t)}|Y)p(Y)} \n    l(\\sigma_t) \n    \\| Y - \\widehat{Y}^{(t)} + \\sigma_0^2 \\nabla_{\\widehat{Y}^{(t)}} \n    E(\\widehat{Y}^{(t)}; \\zeta) \\|^2\n    \\, ,\n\\end{equation}\nwhere $ \\sigma \\in \\{ \\sigma_1, \\cdots, \\sigma_T \\} $ and $l(\\sigma_t) = \\sigma_t$. \nWith~\\cref{eq:dsm}, we can ensure that the gradient has the right magnitude by setting $\\sigma_0$.\n\nIn the generative time series forecasting setting, the generated samples will be tested without applying the diffusion process. \nTo further denoise the generated target series $\\widehat{Y}$, we apply a single-step gradient denoising jump~\\cite{saremi2019neural}:\n\\begin{equation}\t    \\label{eq:clean}\n    \\widehat{Y}_{\\text{clean}} = \\widehat{Y} - \\sigma_0^2 \\nabla_{\\widehat{Y}} E (\\widehat{Y}; \\zeta) \\, .\n\\end{equation}\nThe generated results tend to possess a larger distribution space than the true target,  and the noisy term in~\\cref{eq:clean} approximates the noise between the generated target series and the ``cleaned'' target series.\nTherefore,  $\\sigma_0^2 \\nabla_{\\widehat{Y}} E (\\widehat{Y}; \\zeta)$  can be treated as the estimated uncertainty of the prediction.\n\n\\subsection{Disentangling Latent Variables for Interpretation}     \\label{sec:disentangle}\n\nThe interpretability of the time series forecasting model is of great importance for many downstream tasks~\\cite{tonekaboni2020went, hardt2020explaining, ismail2020benchmarking}.\nThrough disentangling the latent variables of the generative model, not only the interpretability but also the reliability of the prediction can be further enhanced~\\cite{li2021learning}.\n\nTo disentangle the latent variables $ Z = \\{ z_1, \\cdots, z_n \\} $, we attempt to minimize the Total Correlation (TC)~\\cite{watanabe1960information, kim2018disentangling}, which is a popular metric to measure  dependencies among multiple random variables, % \n\\begin{equation}\n    \\text{TC}(z_i) = \\mathcal{D}_{\\mathrm{KL}} (p_\\phi (z_i) || \\bar{p}_{\\phi} (z_i)), \n    \\qquad \n    \\bar{p}_\\phi (z_i) = \\prod_{j=1}^{m} p_\\phi (z_{i,j})\n\\end{equation}\nwhere $m$ denotes the number of factors of $z_i$ that need to be disentangled. \nLower TC generally means better disentanglement if the latent variables preserve useful information.\nHowever, a very low TC can still be obtained when the latent variables carry no meaningful signals. \nThrough the bidirectional structure of BVAE, such issues can be tackled without too much effort.\nAs shown in~\\cref{fig:method:framework}, the signals are disseminated in both the encoder and decoder, such that rich semantics are aggregated into the latent variables. \nFurthermore, to alleviate the effect of potential irregular values, we average the total correlations of $z_{1:n}$, \nthen the loss w.r.t. the TC  score of BVAE can be obtained:  \n\\begin{equation}\t    \\label{tc}\n    L_{\\text{TC}} = \\frac{1}{n}\\sum_{i=1}^{n}\\text{TC}(z_i) \\, .\n\\end{equation}\n\n\\par\\smallskip\\noindent\n\\centerline{\n\\begin{minipage}[t]{.8\\textwidth}\n\\begin{algorithm}[H]\n\\caption{Training Procedure.} \\label{alg_train}\n\\begin{algorithmic}[1]\n  \\small{\n  \\REPEAT \n  \\STATE\n  $ X^{(0)} \\sim q(X^{(0)}), \\quad Y^{(0)} \\sim q(Y^{(0)}), \\quad \\delta_X \\sim N(0, I_d), \\quad \\delta_Y \\sim N(0, I_d)$ \n  \\STATE \n  Randomly choose $t \\in \\{ 1, \\cdots, T\\}$ and with \\cref{x_t,eq:true}, \n  \\STATE \n  $\\quad X^{(t)} = \\sqrt{\\bar{\\alpha}_t}X^{(0)} + (1-\\bar{\\alpha}_t)\\delta_X, \n   \\quad  Y^{(t)} = \\sqrt{\\bar{\\alpha}^{\\prime}_{t} }Y^{(0)} + (1-\\bar{\\alpha}^{\\prime}_{t})\\delta_Y \n  $\n  \\STATE\n  Generate the latent variable $Z$ with BVAE, $Z \\sim p_\\phi(Z|X^{(t)})$\n  \\STATE \n  Sample $\\widehat{Y}^{(t)} \\sim p_{\\theta}(\\widehat{Y}^{(t)}|Z)$ and calculate $\\mathcal{D}_{\\mathrm{KL}} (q(Y^{(t)})||p_\\theta(\\widehat{Y}^{(t)}))$ \n  \\STATE \n  Calculate DSM loss with \\cref{eq:dsm} \n  \\STATE \n  Calculate total correlation of $Z$ with \\cref{tc}\n  \\STATE \n  Construct the total loss $ \\mathcal{L} $ with \\cref{loss}\n  \\STATE \n  $\\theta, \\phi \\leftarrow \\mathrm{argmin} (\\mathcal{L})$\n  \\UNTIL \n  Convergence\n  }\n\\end{algorithmic} %\\label{alg_train}\n\\end{algorithm}\n\\end{minipage}\n}\n\n\\vspace{-3ex}\n\n\\par\\smallskip\\noindent\n\\centerline{\n\\begin{minipage}[t]{.65\\textwidth}\n\\begin{algorithm}[H]\n\\caption{Forecasting Procedure.} \\label{alg_pred}\n\\begin{algorithmic}[1]\n\\small{\n  \\STATE \n  \\textbf{Input:} $X \\sim q(X)$\n  \\STATE \n  Sample $Z \\sim p_{\\phi}(Z|X)$\n  \\STATE \n  Generate $\\widehat{Y} \\sim p_{\\theta}(\\widehat{Y}|Z)$\n  \\STATE \n  \\textbf{Output:} $\\widehat{Y}_{\\text{clean}}$ and the estimated uncertainty with \\cref{eq:clean}\n}\n\\end{algorithmic} %\\label{alg_pred}\n\\end{algorithm}\n\\end{minipage}\n}\n\n\\iffalse \n\\par\\smallskip\\noindent\n\\centerline{\n}\n\\fi \n\n\\subsection{Training and Forecasting}\n\n\\textbf{Training Objective.} \nTo reduce the effect of uncertainty, the coupled diffusion equipped with the denoising network is proposed without sacrificing generalizability. \nThen we disentangle the latent variables of the generative model by minimizing the TC of the latent variables. \nFinally, we reconstruct the loss with several trade-off parameters, \nand with \\cref{eq:dsm,eq:clean,tc} we have\n\\begin{equation}    \\label{loss}\n\\mathcal{L} \n= \n\\psi \\cdot  \\mathcal{D}_{\\mathrm{KL}} (q(Y^{(t)})||p_\\theta(\\widehat{Y}^{(t)})) \n+ \n\\lambda \\cdot {L} (\\zeta, t) \n+ \n\\gamma \\cdot L_{\\text{TC}} \n+ \nL_{\\text{mse}} (\\widehat{Y}^{(t)}, Y^{(t)}) \n\\, ,\n\\end{equation}\nwhere $L_{\\text{mse}}$ calculates the mean square error (MSE) between $\\widehat{Y}^{(t)}$ and $Y^{(t)}$.\nWe minimize the above objective to learn the generative model accordingly. \n\n{\\bf Algorithms.}\n\\cref{alg_train} displays the complete training procedure of \\ourmodel~with the loss function in~\\cref{loss}. \nFor inference, as described in \\cref{alg_pred}, given the input series $X$, the target series can be generated directly from the distribution $p_{\\theta}$ which is conditioned on the latent states drawn from the distribution $p_{\\phi}$. \n\n\\section{Experiments} \\label{experiment}\n\n\\subsection{Experiment Settings}  \\label{setting}\n\n\\textbf{Datasets.~}%\\quad\nWe generate two synthetic datasets suggested by~\\cite{farnoosh2020deep}, \n\\begin{equation*}\n\\begin{gathered}\n     w_t = a \\cdot  w_{t-1} + \\text{tanh}(b \\cdot  w_{t-2}) + \\text{sin}(w_{t-3}) + \\mathcal{N}(0, 0.5I)\\\\\n     X = [w_1, w_2, ..., w_N] \\cdot  F + \\mathcal{N}(0, 0.5I) \\, ,\n\\end{gathered}\n\\end{equation*}\nwhere $w_t \\in \\mathbb{R}^2$ and $ 0 \\leq w_{t, 1}, w_{t, 2} \\leq 1 $ ($t = 1,2,3$), \n$F \\in \\mathbb{R}^{2 \\times k} \\sim \\mathcal{U}[-1, 1]$, $k$ denotes the dimensionality and $N$ is the number of time points, $a, b$ are two constants.~%\nWe set $a=0.9, b=0.2, k=20$ to generate D$_1$, and $a=0.5, b=0.5, k=40$ for D$_2$, and $N = 800$ for both D$_1$ and D$_2$.\n\n\\begin{table}[t]\n    \\caption{\n    Performance comparisons on synthetic data in terms of MSE and CRPS. \n    The best results are boldfaced.\n    }\n    \\centering\n    \\small\n    \\setlength\\tabcolsep{2.5pt}\n    \\renewcommand{\\arraystretch}{1.2}\n    \\begin{tabular}{c|c|ccccccccc}\n    \\toprule \n    \\multicolumn{2}{c}{Model}& \\ourmodel &NVAE &$\\beta$-TCVAE &f-VAE &DeepAR &TimeGrad &GP-copula & VAE \\\\\n   \\midrule\n    \\multirow{4}{*}{D$_1$} & \\multirow{2}{*}{8}&$\\textbf{0.512}_{\\pm.033}$ &$1.201_{\\pm.027}$&$0.631_{\\pm.003}$&$0.854_{\\pm.099}$&$1.153_{\\pm.125}$&$0.966_{\\pm.102}$&$1.202_{\\pm.108}$&$0.912_{\\pm.132}$\\\\\n    ~&~&$\\textbf{0.585}_{\\pm.021}$&$0.905_{\\pm.011}$&$0.658_{\\pm.002}$&$0.745_{\\pm.036}$&$0.758_{\\pm.038}$&$0.698_{\\pm.024}$&$0.773_{\\pm.033}$&$0.786_{\\pm.053}$\\\\\n    \\cline{2-10}\n   \n    ~& \\multirow{2}{*}{16}& $\\textbf{0.571}_{\\pm.025}$&$1.184_{\\pm.025}$&$0.758_{\\pm.047}$&$1.046_{\\pm.270}$&$0.911_{\\pm.046}$&$0.945_{\\pm.315}$&$0.915_{\\pm.059}$&$0.908_{\\pm.177}$\\\\\n    ~&~&$\\textbf{0.625}_{\\pm.013}$&$0.897_{\\pm.012}$&$0.747_{\\pm.027}$&$0.835_{\\pm.108}$&$0.699_{\\pm.014}$&$0.709_{\\pm.100}$&$0.704_{\\pm.020}$&$0.765_{\\pm.067}$\\\\\n    \n    \\midrule\n    \\multirow{4}{*}{D$_2$} &  \\multirow{2}{*}{8} &$\\textbf{0.599}_{\\pm.049}$&$1.966_{\\pm.047}$&$3.096_{\\pm.197}$&$3.353_{\\pm.430}$&$0.977_{\\pm.137}$&$0.963_{\\pm.385}$&$1.037_{\\pm.082}$&$3.079_{\\pm.345}$\\\\\n    ~&~&$\\textbf{0.628}_{\\pm.027}$& $1.255_{\\pm.021}$&$1.680_{\\pm.062}$&$1.640_{\\pm.154}$&$0.727_{\\pm.058}$&$0.706_{\\pm.123}$&$0.753_{\\pm.026}$&$1.504_{\\pm.098}$\\\\\n    \\cline{2-10}\n    ~& \\multirow{2}{*}{16}& $\\textbf{0.786}_{\\pm.041}$&$1.955_{\\pm.051}$&$3.067_{\\pm.443}$&$3.109_{\\pm.428}$&$0.972_{\\pm.144}$&$0.850_{\\pm.061}$&$1.082_{\\pm.071}$&$3.132_{\\pm.160}$\\\\\n    \n    ~&~&$0.728_{\\pm.026}$ &$1.251_{\\pm.020}$&$1.643_{\\pm.183}$&$1.558_{\\pm.157}$&$0.720_{\\pm.050}$&$\\textbf{0.649}_{\\pm.017}$&$0.762_{\\pm.008}$&$1.560_{\\pm.060}$\\\\\n    \\bottomrule\n    \\end{tabular}\n    \\label{toy1}\n\\vspace{-2ex}\n\\end{table}\n\n\\begin{table}[t]\n  \\caption{\n  The performance comparisons on real-world datasets in terms of MSE and CRPS, and the best results are in boldface.\n  }\n  \\label{real}\n    \\centering\n    \\small\n    \\setlength\\tabcolsep{2.0pt}\n    \\renewcommand{\\arraystretch}{1.2}\n    \\begin{threeparttable}\n    \\begin{tabular}{c|c|ccccccccc}\n    \\toprule  \n    \\multicolumn{2}{c}{Model}& \\ourmodel &NVAE &$\\beta$-TCVAE &f-VAE & DeepAR & TimeGrad & GP-copula & VAE \\\\\n   \\midrule\n    \\multirow{4}{*}{\\rotatebox{90}{Traffic}} & \\multirow{2}{*}{8}&$\\textbf{0.081}_{\\pm.003}$&$1.300_{\\pm.024}$&$1.003_{\\pm.006}$&$0.982_{\\pm.059}$&$3.895_{\\pm.306}$&$3.695_{\\pm.246}$&$4.299_{\\pm.372}$&$0.794_{\\pm.130}$\\\\\n    ~&~&$\\textbf{0.207}_{\\pm.003}$&$0.593_{\\pm.004}$&$0.894_{\\pm.003}$&$0.666_{\\pm.032}$&$1.391_{\\pm.071}$&$1.410_{\\pm.027}$&$1.408_{\\pm.046}$&$0.759_{\\pm.07}$\\\\\n    \\cline{2-10}\n    ~& \\multirow{2}{*}{16}&$\\textbf{0.081}_{\\pm.009}$&$1.271_{\\pm.019}$&$0.997_{\\pm.004}$&$0.998_{\\pm.042}$&$4.141_{\\pm.320}$&$3.495_{\\pm.362}$&$4.575_{\\pm.141}$&$0.632_{\\pm.057}$\\\\\n    ~&~&$\\textbf{0.200}_{\\pm.014}$&$0.589_{\\pm.001}$&$0.893_{\\pm.002}$&$0.692_{\\pm.026}$&$1.338_{\\pm.043}$&$1.329_{\\pm.057}$&$1.506_{\\pm.025}$&$0.671_{\\pm.038}$\\\\\n     \\midrule\n    \\multirow{4}{*}{\\rotatebox{90}{Electricity}} &  \\multirow{2}{*}{8}&$\\textbf{0.251}_{\\pm.015}$&$1.134_{\\pm.029}$&$0.901_{\\pm.052}$&$0.893_{\\pm.069}$&$2.934_{\\pm.173}$&$2.703_{\\pm.087}$&$2.924_{\\pm.218}$&$0.853_{\\pm.040}$\\\\\n    ~&~&$\\textbf{0.398}_{\\pm.011}$&$0.542_{\\pm.003}$&$0.831_{\\pm.004}$&$0.809_{\\pm.024}$&$1.244_{\\pm.037}$&$1.208_{\\pm.024}$&$1.249_{\\pm.048}$&$0.795_{\\pm.016}$\\\\\n    \\cline{2-10}\n    ~& \\multirow{2}{*}{16}&$\\textbf{0.308}_{\\pm.030}$&$1.150_{\\pm.032}$&$0.850_{\\pm.003}$&$0.807_{\\pm.034}$&$2.803_{\\pm.199}$&$2.770_{\\pm.237}$&$3.065_{\\pm.186}$&$0.846_{\\pm.062}$\\\\\n    ~&~&$\\textbf{0.437}_{\\pm.020}$&$0.531_{\\pm.003}$&$0.814_{\\pm.002}$&$0.782_{\\pm.024}$&$1.220_{\\pm.048}$&$1.240_{\\pm.048}$&$1.307_{\\pm.042}$&$0.793_{\\pm.029}$\\\\\n     \\midrule\n    \\multirow{4}{*}{\\rotatebox{90}{Weather}} & \\multirow{2}{*}{8} & $\\textbf{0.169}_{\\pm.022}$&$0.801_{\\pm.024}$&$0.234_{\\pm.042}$&$0.591_{\\pm.198}$&$2.317_{\\pm.357}$&$2.715_{\\pm.189}$&$2.412_{\\pm.761}$&$0.560_{\\pm.192}$\\\\\n    ~&~&$\\textbf{0.357}_{\\pm.024}$&$0.757_{\\pm.013}$&$0.404_{\\pm.040}$&$0.565_{\\pm.080}$&$0.858_{\\pm.078}$&$0.920_{\\pm.013}$&$0.897_{\\pm.115}$&$0.572_{\\pm.077}$\\\\\n     \\cline{2-10}\n    ~& \\multirow{2}{*}{16}&$\\textbf{0.187}_{\\pm.047}$&$0.811_{\\pm.016}$&$0.212_{\\pm.012}$&$0.530_{\\pm.167}$&$1.269_{\\pm.187}$&$1.110_{\\pm.083}$&$1.357_{\\pm.145}$&$0.424_{\\pm.141}$\\\\\n    ~&~&$\\textbf{0.361}_{\\pm.046}$&$0.759_{\\pm.009}$&$0.388_{\\pm.014}$&$0.547_{\\pm.067}$&$0.783_{\\pm.059}$&$0.733_{\\pm.016}$&$0.811_{\\pm.032}$&$0.503_{\\pm.068}$\\\\\n     \\midrule\n    \\iffalse\n    \\multirow{4}{*}{\\rotatebox{90}{Exchange}} &  \\multirow{2}{*}{8} & $0.607_{\\pm.120}$&$2.112_{\\pm.054}$&$1.003_{\\pm.006}$&$3.747_{\\pm.673}$&$0.419_{\\pm.058}$&$0.314_{\\pm.035}$&$0.509_{\\pm.031}$&$3.486_{\\pm.747}$\\\\\n    ~&~&$0.753_{\\pm.077}$&$1.338_{\\pm.027}$&$0.894_{\\pm.003}$&$1.791_{\\pm.173}$&$0.493_{\\pm.018}$&$0.444_{\\pm.015}$&$0.528_{\\pm.021}$&$1.741_{\\pm.167}$\\\\\n    \\cline{2-10}\n    ~& \\multirow{2}{*}{16}&$1.406_{\\pm.315}$&$2.107_{\\pm.030}$&$2.761_{\\pm.405}$&$3.701_{\\pm.482}$&$0.393_{\\pm.045}$&$0.283_{\\pm.041}$&$0.376_{\\pm.023}$&$3.758_{\\pm.484}$\\\\\n    ~&~&$1.144_{\\pm.128}$&$1.330_{\\pm.054}$&$1.629_{\\pm.120}$&$1.804_{\\pm.102}$&$0.467_{\\pm.034}$&$0.411_{\\pm.020}$&$0.451_{\\pm.029}$&$1.815_{\\pm.083}$\\\\\n     \\midrule\n    \\fi\n    \\multirow{4}{*}{\\rotatebox{90}{ETTm1}} &  \\multirow{2}{*}{8} & $\\textbf{0.527}_{\\pm.073}$&$0.921_{\\pm.026}$&$1.538_{\\pm.254}$&$2.326_{\\pm.445}$&$2.204_{\\pm.420}$&$1.877_{\\pm.245}$&$2.024_{\\pm.143}$&$2.375_{\\pm.405}$\\\\\n    ~&~&$\\textbf{0.557}_{0.048}$&$0.760_{\\pm.026}$&$1.015_{\\pm.112}$&$1.260_{\\pm.167}$&$0.984_{\\pm.074}$&$0.908_{\\pm.038}$&$0.961_{\\pm.027}$&$1.258_{\\pm.104}$\\\\\n    \\cline{2-10}\n    ~& \\multirow{2}{*}{16}&$\\textbf{0.968}_{\\pm.104}$&$1.100_{\\pm.032}$&$1.744_{\\pm.100}$&$2.339_{\\pm.270}$&$2.350_{\\pm.170}$&$2.032_{\\pm.234}$&$2.486_{\\pm.207}$&$2.321_{\\pm.469}$\\\\\n    ~&~&$\\textbf{0.821}_{\\pm.072}$&$0.822_{\\pm.026}$&$1.104_{\\pm.041}$&$1.249_{\\pm.088}$&$0.974_{\\pm.016}$&$0.919_{\\pm.031}$&$0.984_{\\pm.016}$&$1.259_{\\pm.132}$\\\\\n     \\midrule\n    \\multirow{4}{*}{\\rotatebox{90}{ETTh1}} & \\multirow{2}{*}{8} &$\\textbf{0.292}_{\\pm.036}$&$0.483_{\\pm.017}$&$0.703_{\\pm.054}$&$0.870_{\\pm.134}$&$3.451_{\\pm.335}$&$4.259_{\\pm1.13}$&$4.278_{\\pm1.12}$&$1.006_{\\pm.281}$\\\\\n    ~&~&$\\textbf{0.424}_{\\pm.033}$&$0.461_{\\pm.011}$&$0.644_{\\pm.038}$&$0.730_{\\pm.060}$&$1.194_{\\pm.034}$&$1.092_{\\pm.028}$&$1.169_{\\pm.055}$&$0.762_{\\pm.115}$\\\\\n    \\cline{2-10}\n    ~& \\multirow{2}{*}{16}&$\\textbf{0.374}_{\\pm.061}$&$0.488_{\\pm.010}$&$0.681_{\\pm.018}$&$0.983_{\\pm.139}$&$1.929_{\\pm.105}$&$1.332_{\\pm.125}$&$1.701_{\\pm.088}$&$0.681_{\\pm.104}$\\\\\n    ~&~&$0.488_{\\pm.039}$&$ \\textbf{0.463}_{\\pm.018}$&$0.640_{\\pm.008}$&$0.760_{\\pm.062}$&$1.029_{\\pm.030}$&$0.879_{\\pm.037}$&$0.999_{\\pm.023}$&$0.641_{\\pm.055}$\\\\\n    \\midrule\n    \\multirow{4}{*}{\\rotatebox{90}{Wind}} & \\multirow{2}{*}{8} & $\\textbf{0.681}_{\\pm.075}$&$1.854_{\\pm.032}$&$1.321_{\\pm.379}$&$1.942_{\\pm.101}$&$12.53_{\\pm2.25}$&$12.67_{\\pm1.75}$&$11.35_{\\pm6.61}$&$2.006_{\\pm.145}$\\\\\n    ~&~&$\\textbf{0.596}_{\\pm.052}$&$1.223_{\\pm.014}$&$0.863_{\\pm.143}$&$1.067_{\\pm.086}$&$1.370_{\\pm.107}$&$1.440_{\\pm.059}$&$1.305_{\\pm.369}$&$1.103_{\\pm.100}$\\\\\n     \\cline{2-10}\n    ~& \\multirow{2}{*}{16}&$1.033_{\\pm.062}$&$1.955_{\\pm.015}$&$\\textbf{0.894}_{\\pm.038}$&$1.262_{\\pm.178}$&$13.96_{\\pm.1.53}$&$12.86_{\\pm2.60}$&$13.79_{\\pm5.37}$&$1.138_{\\pm.205}$\\\\\n    ~&~&$\\textbf{0.757}_{\\pm.053}$&$1.247_{\\pm.011}$&$0.785_{\\pm.037}$&$0.843_{\\pm.066}$&$1.347_{\\pm.060}$&$1.240_{\\pm.070}$&$1.261_{\\pm.171}$&$0.862_{\\pm.092}$\\\\\n    \\bottomrule\n    \\end{tabular}\n    \\end{threeparttable}\n\\vspace{-2ex}\n\\end{table}\n\nSix real-world datasets with diverse spatiotemporal dynamics are selected, \nincluding Traffic~\\cite{lai2018modeling}, \nElectricity\\footnote{\\url{https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014}}, \nWeather\\footnote{\\url{https://www.bgc-jena.mpg.de/wetter/}}, \nWind (Wind Power) \\footnote{This dataset is published at \n\\url{https://github.com/PaddlePaddle/PaddleSpatial/tree/main/paddlespatial/datasets/WindPower}.\n},\nand ETTs~\\cite{zhou2021informer} (ETTm1 and ETTh1). \nTo highlight the uncertainty in short time series scenarios, for each dataset, we slice a subset from the starting point to make sure that each sliced dataset contains at most 1000 time points.~%\nSubsequently, we obtained \n$5\\%$-Traffic, $3\\%$-Electricity, $2\\%$-Weather, $2\\%$-Wind, $1\\%$-ETTm1, and $5\\%$-ETTh1.~%\nThe statistical descriptions of the real-world datasets can be found in \\cref{exp_repro}.~%\nAll datasets are split chronologically and adopt the same train/validation/test ratios, i.e., 7:1:2. \n\n\\textbf{Baselines.~}\nWe compare \\ourmodel ~with one GP (Gaussian Process) based method \n(GP-copula~\\cite{salinas2019high}), two auto-regressive methods (DeepAR~\\cite{salinas2020deepar} and TimeGrad~\\cite{rasul2021autoregressive}), \nand four VAE-based methods, i.e., vanilla VAE, NVAE~\\cite{vahdat2020nvae}, factor-VAE (f-VAE for short)~\\cite{kim2018disentangling} and $\\beta$-TCVAE~\\cite{chen2018isolating}.\n\n\\textbf{Implementation Details.~}%\\quad\nAn input-$l_x$-predict-$l_y$ window is applied to roll the train, validation, and test sets with stride one time-step, respectively, and this setting is adopted for all datasets. \nHereinafter, the last dimension of the multivariate time series is selected as the target variable by default.\n\nWe use the Adam optimizer with an initial learning rate of $5e-4$.\nThe batch size is $16$, and the training is set to $20$ epochs at most equipped with early stopping. \nThe number of disentanglement factors is chosen from $\\{4, 8\\}$, \nand $\\beta_t \\in \\bm{\\beta} $ is set to range from $0.01$ to $0.1$ with different diffusion steps $T \\in [100, 1000]$, \nthen $\\omega$ is set to $0.1$. \nThe trade-off hyperparameters are set as $\\psi = 0.05, \\lambda=0.1, \\gamma=0.001$ for ETTs, and $\\psi = 0.5, \\lambda=1.0, \\gamma=0.01$ for others. \nAll the experiments were carried out on a Linux machine with a single NVIDIA P40 GPU.\nThe experiments are repeated five times, and the average and variance of the predictions are reported. \nWe use the Continuous Ranked Probability Score~(CRPS)~\\cite{matheson1976scoring} and Mean Squared Error~(MSE) as the evaluation metrics.\nFor both metrics, the lower, the better. \nIn particular, CRPS is used to evaluate the similarity of two distributions and is equivalent to Mean Absolute Error (MAE) when two distributions are discrete.\n\n\\subsection{Main Results} \\label{e1}\n\nTwo different prediction lengths, i.e., $l_y \\in \\{8, 16\\}$ ($l_x = l_y$), are evaluated.\nThe results of longer prediction lengths are available in~\\cref{extra}. \n\n\\textbf{Toy Datasets.~~}\nIn~\\cref{toy1}, we can observe that \n\\ourmodel ~achieves SOTA performance most of the time, and achieves competitive CRPS in D$_2$ for prediction length 16. \nBesides, VAEs outperform VARs and GP on D$_1$, \nbut VARs achieve better performance on D$_2$, which demonstrates the advantage of VARs in learning complex temporal dependencies. \n\n\\textbf{Real-World Datasets.~~} \nAs for the experiments on real-world data, \\ourmodel ~achieves consistent  SOTA performance except for the prediction length 16 on the Wind dataset (\\cref{real}). \nParticularly, under the input-8-predict-8 setting, \\ourmodel~can provide remarkable improvements in Traffic, Electricity, Wind, ETTm1, ETTh1 and Weather w.r.t. MSE reduction (90\\%, 71\\%, 48\\%, 43\\%, 40\\% and 28\\%). \nRegarding the CRPS reduction, \\ourmodel~achieves a 73\\% reduction in Traffic, 31\\% in Wind, and 27\\% in Electricity under the input-8-predict-8 setting, \nand a 70\\% reduction in Traffic, 18\\% in Electricity, and 7\\% in Weather under the input-16-predict-16 setting.~%\nOverall, \\ourmodel~gains the averaged 43\\% MSE reduction and 23\\% CRPS reduction among the above settings.~%\nMore results under longer prediction-length settings and on full datasets can be found in  \\cref{appendix:supp-main-results}. \n\n\\textbf{Uncertainty Estimation.~~} \nThe  uncertainty can be assessed by estimating the noise of the outcome series when doing the prediction~(see~\\cref{sec_denoise}).  \nThrough scale parameter $\\omega$, the generated distribution space can be adjusted accordingly (results on the effect of $\\omega$ can be found  in Appendix D.3). %~\\cref{App_omega}). \nThe showcases in \\cref{noise} demonstrate the uncertainty estimation of the yielded series in the Traffic dataset, where the last six dimensions are treated as target variables. \nWe can find that noise estimation can quantify the uncertainty effectively. \nFor example, the estimated uncertainty grows rapidly when extreme values are encountered. \n\n\\begin{figure}[htbp]\n    \\centering\n    \\begin{subfigure}[t]{0.31\\textwidth}\n      \\includegraphics[width=\\textwidth]{fig/figure9/dim1.png}\n    \\end{subfigure}\n    \\begin{subfigure}[t]{0.31\\textwidth}\n      \\includegraphics[width=\\textwidth]{fig/figure9/dim2.png}\n    \\end{subfigure}\n    \\begin{subfigure}[t]{0.31\\textwidth}\n      \\includegraphics[width=\\textwidth]{fig/figure9/dim3.png}\n    \\end{subfigure}\n    \n    \\begin{subfigure}[t]{0.31\\textwidth}\n      \\includegraphics[width=\\textwidth]{fig/figure9/dim4.png}\n    \\end{subfigure}\n    \\begin{subfigure}[t]{0.31\\textwidth}\n      \\includegraphics[width=\\textwidth]{fig/figure9/dim5.png}\n    \\end{subfigure}\n    \\begin{subfigure}[t]{0.31\\textwidth}\n      \\includegraphics[width=\\textwidth]{fig/figure9/dim6.png}\n    \\end{subfigure}\n    \\caption{\n    Uncertainty estimation of the prediction of the last six dimensions in the Traffic dataset and the colored envelope denotes the estimated uncertainty.\n    }\n    \\label{noise}\n\\end{figure}\n\n\\textbf{Disentanglement Evaluation.~} %\\quad\nFor time series forecasting,\nit is difficult to label disentangled factors by hand, \nthus we take different dimensions of $Z$ as the factors to be disentangled: $ z_i = [ z_{i, 1}, \\cdots, z_{i, m}] $ ($z_i \\in Z $). \nWe build a classifier to discriminate whether an instance $z_{i,j}$ belongs to class $j$ such that the disentanglement quality can be assessed by evaluating the classification performance.  \nBesides, \nwe adopt the Mutual Information Gap (MIG) \\cite{chen2018isolating} as a metric to evaluate the disentanglement more straightforwardly. \nDue to the space limit, the evaluation of disentanglement with different factors can be found in~\\cref{disentangle}.\n\n\\begin{table}[t]\n   \\begin{minipage}[b]{0.63\\textwidth}\n    \\centering\n    \\small\n    \\caption{\n    Ablation study of the coupled diffusion probabilistic model w.r.t. MSE and CSPR. \n    } \\label{tab:abla:diffusion}\n    \\renewcommand{\\arraystretch}{1.0}\n    \\setlength\\tabcolsep{0.8pt}\n    \\begin{threeparttable}\n    \\begin{tabular}{c|cc|cc}\n    \\toprule\n     \\multirow{2}{*}{Dataset}& \\multicolumn{2}{c|}{Traffic} & \\multicolumn{2}{c}{Electricity}\\\\\n     \\cline{2-5}\n     ~&16&32&16&32\\\\\n    \\midrule\n    \\multirow{2}{*}{\\ourmodel $_{-\\widetilde{Y}}$} & \n    $0.122_{\\pm.006}$&$0.126_{\\pm.013}$&\n    $0.350_{\\pm.043}$&$0.422_{\\pm.012}$\\\\\n    ~& \n    $0.250_{\\pm.008}$&$0.261_{\\pm.017}$&\n    $0.480_{\\pm.032}$&$0.551_{\\pm.012}$\\\\\n    \\midrule\n    \\multirow{2}{*}{\\ourmodel $_{-\\widetilde{Y}-\\mathrm{DSM}}$} & \n    $0.096_{\\pm.006}$& $0.092_{\\pm.008}$ & $ 0.331_{\\pm.023}$&$0.502_{\\pm.079} $\\\\\n    ~& $0.217_{\\pm.010}$ & $0.220_{\\pm.013}$& $ 0.450_{\\pm.021}$&$0.584_{\\pm.053}$\\\\\n    \\midrule\n    \\multirow{2}{*}{\\ourmodel $_{-\\widetilde{X}}$}&\n    $0.123_{\\pm.003}$&$0.117_{\\pm.007}$&\n    $0.351_{\\pm.047}$&$0.420_{\\pm.056}$\\\\\n    ~&\n    $0.256_{\\pm.006}$&$0.253_{\\pm.013}$&\n    $0.481_{\\pm.036}$&$0.540_{\\pm.046}$\\\\\n    \\midrule\n    \\multirow{2}{*}{\\ourmodel $_{-\\mathrm{CDM}}$}&\n    $0.123_{\\pm.004}$&$0.118_{\\pm.008}$&\n    $0.365_{\\pm.025}$&$0.439_{\\pm.014}$\\\\\n    ~&\n    $0.255_{\\pm.007}$&$0.252_{\\pm.015}$&\n    $0.498_{\\pm.018}$&$0.561_{\\pm.016}$\\\\\n    \\midrule\n    \\multirow{2}{*}{\\ourmodel $_{-\\mathrm{CDM}-\\mathrm{DSM}}$}&\n    $0.123_{\\pm.003}$&$0.119_{\\pm.003}$&\n    $0.338_{\\pm.041}$&$0.448_{\\pm.062}$\\\\\n    ~&\n    $0.255_{\\pm.003}$&$0.253_{\\pm.005}$&\n    $0.467_{\\pm.029}$&$0.555_{\\pm.041}$\\\\\n    \\midrule\n    \\multirow{2}{*}{\\ourmodel}&$\\textbf{0.081}_{\\pm.009}$&$\\textbf{0.091}_{\\pm.007}$&$\\textbf{0.308}_{\\pm.030}$&$\\textbf{0.410}_{\\pm.075}$\\\\\n    ~&$\\textbf{0.200}_{\\pm.014}$&$\\textbf{0.216}_{\\pm.012}$&$\\textbf{0.437}_{\\pm.020}$&$\\textbf{0.534}_{\\pm.058}$\\\\\n    \\bottomrule\n    \\end{tabular}\n    \\end{threeparttable}\n    \\label{tab:my_label}\n    \\end{minipage}\n\\hfill\n\\begin{minipage}[p]{0.35\\textwidth}\n    \\centering\n    \\begin{subfigure}[t]{\\textwidth}\n     \\includegraphics[width=0.9\\textwidth,height=0.54\\textwidth]{fig/figure9/end6.png}\n    \\end{subfigure}\n    \\vspace{1ex}\n     \\begin{subfigure}[t]{\\textwidth}\n     \\includegraphics[width=0.9\\textwidth,height=0.54\\textwidth]{fig/figure9/step5.png}\n    \\end{subfigure}\n    \\captionof{figure}{\n      Comparisons of predictions with different $\\beta_{T}$ and varying $T$ on the Electricity dataset.\n    }\n    \\label{fig:beta}\n\\end{minipage}\n\\vspace{-3ex}\n\\end{table}    \n \n \n\n\\subsection{Model Analysis} \\label{ablation}\n\n\\textbf{Ablation Study of the Coupled Diffusion and Denoising Network.} \nTo evaluate the effectiveness of the coupled diffusion model (CDM), we compare the full versioned \\ourmodel ~with its three variants: \ni) \\ourmodel $_{-\\widetilde{Y}}$, i.e. \\ourmodel ~without diffused $Y$, \nii) \\ourmodel $_{-\\widetilde{X}}$, i.e. \\ourmodel ~without diffused $X$, \nand \niii) \\ourmodel $_{-\\mathrm{CDM}}$, i.e. \\ourmodel ~without any diffusion. \nBesides, the performance of \\ourmodel~without denoising score matching (DSM) is also reported when the target series is not diffused, which are denoted as \\ourmodel $_{-\\widetilde{Y}-\\mathrm{DSM}}$ and \\ourmodel $_{-\\mathrm{CDM}-\\mathrm{DSM}}$. \nThe ablation study is carried out on Traffic and Electricity datasets under input-16-predict-16 and input-32-predict-32. \nIn \\cref{tab:abla:diffusion},  we can find that the diffusion process can effectively augment the input or the target. \nMoreover, when the target is not diffused, the denoising network would be deficient since the noise level of the target cannot be estimated by then. \n\n\\textbf{Variance Schedule $\\bm{\\beta}$ and The Number of Diffusion Steps $T$.} %\\quad\nTo reduce the effect of the uncertainty while preserving the informative temporal patterns, the extent of the diffusion should be configured properly.~%\nToo small a variance schedule or inadequate diffusion steps will lead to a meaningless diffusion process. Otherwise, the diffusion could be out of control {\\footnote{\nAn illustrative showcase can be found in Appendix F. % \\cref{B1}.\n}}.~%\nHere we analyze the effect of the variance schedule $\\bm{\\beta}$ and the number of diffusion steps $T$.  \nWe set $\\beta_1 = 0$ and change the value of $\\beta_t$ in the range of $[0.01, 0.1]$, and $T$ ranges from $100$ to $4000$.\nAs shown in~\\cref{fig:beta}, we can  see  that the prediction performance can be improved if proper $\\bm{\\beta}$ and $T$ are employed.\n\n\\section{Discussion} \\label{discuss}\n\n\\textbf{Sampling for Generative Time Series Forecasting.~}\\\\\nThe Langevin dynamics has been widely applied to the sampling of energy-based models~(EBMs)~\\cite{xie2016theory,du2019implicit,xie2019learning},\n\\begin{equation}\n    Y_k = Y_{k-1} - \\frac{\\rho}{2}\\nabla_{Y}E_{\\phi}(Y_{k-1})  + \\rho^{\\frac{1}{2}} \\mathcal{N} (0, I_d) \\, ,\n\\end{equation}\nwhere $k \\in \\{ 0, \\cdots, K \\} $, \n$K$ denotes the number of sampling steps, \nand $\\rho$ is a constant. \nWith $K$ and $\\rho$ being properly configured, high-quality samples can be generated.  \nThe Langevin dynamics has been successfully applied to applications in computer vision~\\cite{kumar2019maximum,xie2021generative}, and natural language processing~\\cite{deng2020residual}.\n\nWe employ a single-step gradient denoising jump in this work to generate the target series. \nThe experiments that were carried out demonstrate the effectiveness of such single-step sampling. \nWe conduct an extra empirical study to investigate whether it is worth taking more sampling steps for further performance improvement of time series forecasting. \nWe showcase the prediction results under different sampling strategies in \\cref{fig:sample}. \nBy omitting the additive noise in Langevin dynamics, we employ the multi-step denoising for \\ourmodel~to generate the target series and plot the generated results in \\cref{steps}. \nThen, with the standard Langevin dynamics, we can implement a generative procedure instead of denoising and compare the generated target series with different $\\rho$ (see~\\cref{rho=0.003,rho=0.005,rho=0.007}). \nWe can observe that more sampling steps might not be helpful in improving prediction performance for generative time series forecasting~(\\cref{steps}). \nBesides, larger sampling steps would lead to high computational complexity. \nOn the other hand, different configurations of Langevin dynamics (with varying $\\rho$) cannot bring indispensable benefits for time series forecasting (\\cref{rho=0.003,rho=0.005,rho=0.007}). \n\n\\begin{figure}[ht]\n    \\centering\n      \\begin{subfigure}[t]{0.24\\textwidth}\n      \\centering\n       \\includegraphics[width=0.96\\textwidth]{fig/figure9/tmp/samplesteps1.png}\n       \\caption{\n       Multi-step denoising.\n       }\n       \\label{steps}\n      \\end{subfigure}\n      \\begin{subfigure}[t]{0.24\\textwidth}\n      \\centering\n       \\includegraphics[width=0.96\\textwidth]{fig/figure9/tmp/rho-003.png}\n       \\caption{$\\rho = 0.003$.}\n       \\label{rho=0.003}\n      \\end{subfigure}\n     \\begin{subfigure}[t]{0.24\\textwidth}\n        \\centering\n       \\includegraphics[width=0.96\\textwidth]{fig/figure9/tmp/rho-005.png}\n       \\caption{$\\rho = 0.005$.}\n       \\label{rho=0.005}\n      \\end{subfigure}\n      \\begin{subfigure}[t]{0.24\\textwidth}\n      \\centering\n       \\includegraphics[width=0.96\\textwidth]{fig/figure9/tmp/rho-007.png}\n       \\caption{$\\rho = 0.007$.}\n       \\label{rho=0.007}\n      \\end{subfigure}\n      \\caption{\n      The prediction showcases in the Electricity dataset with different sampling strategies.\n      }\n     \\label{fig:sample}\n\\end{figure}\n \n\n\\textbf{Limitations.~} %\\quad \n\\\\\nWith the coupled diffusion probabilistic model, although the aleatoric uncertainty of the time series can be reduced, a new bias is brought into the series to mimic the distribution of the input and target.\nHowever, as a common issue in VAEs that any introduced bias in the input will result in bias in the generated output~\\cite{von2021self}, the diffusion steps and variance schedule need to be chosen cautiously, such that this model can be applied to different time series tasks smoothly. \nThe proposed model is devised for general time series forecasting, it should be used properly to avoid the potential negative societal impacts, such as illegal applications. \n\nIn time series predictive analysis, disentanglement of the latent variables has been very important for interpreting the prediction to provide more reliance.~%\nDue to the lack of prior knowledge of the entangled factors in generative time series forecasting, only unsupervised disentanglement learning can be done, which has been proven theoretically feasible for time series~\\cite{li2021learning}. \nDespite this, for boarder applications of disentanglement and better performance, it is still worth exploring how to label the factors of time series in the future.  \nMoreover, because of the uniqueness of time series data, it is also a promising direction to explore more generative and sampling methods for the time series generation task. \n\n\\section{Conclusion} \\label{conclusion}\n\nIn this work, we propose a generative model with the bidirectional VAE as the backbone. \nTo further improve the generalizability, we devise a coupled diffusion probabilistic model for time series forecasting. \nThen a scaled denoising network is developed to guarantee the prediction accuracy. \nAfterward, the latent variables are further disentangled for better model interpretability. \nExtensive experiments on synthetic data and real-world data validate that our proposed generative model achieves SOTA performance compared to existing competitive generative models.\n\n\\section*{Acknowledgement}\n\nWe thank Longyuan Power Group Corp. Ltd. for supporting this work.  \n\n\\newpage\n\n\\newpage\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{[]{DiffSTG: Probabilistic Spatio-Temporal Graph Forecasting  \\\\ with Denoising Diffusion Models}\n\n\\begin{document}\n\n\\title[]{DiffSTG: Probabilistic Spatio-Temporal Graph Forecasting  \\\\ with Denoising Diffusion Models}\n\t\n\n    \\author{Haomin Wen$^{1}$, Youfang Lin$^{1}$, Yutong Xia$^{2}$, Huaiyu Wan$^{1}$,  Qingsong Wen$^{3}$, \\\\ Roger Zimmermann$^{2}$, Yuxuan Liang$^{4, *}$}\n    \\affiliation{%\n\t\t\\textsuperscript{\\rm 1}School of Computer and Information Technology, Beijing Jiaotong University, China\\\\\n            \\textsuperscript{\\rm 2}National University of Singapore, \\textsuperscript{\\rm 3}DAMO Academy, Alibaba Group\\\\\n            \\textsuperscript{\\rm 4}Hong Kong University of Science and Technology (Guangzhou) *corresponding author\\\\\n            \\{wenhaomin, yflin, hywan\\}@bjtu.edu.cn;  yutong.xia@u.nus.edu; \\\\  qingsongedu@gmail.com; rogerz@comp.nus.edu.sg; yuxliang@outlook.com\n            \\vspace{2em}\n         \\country{}\n\t}\n   \n \n\t\n\t\n \\renewcommand{\\shortauthors}{Haomin Wen et al.}\n\t\n\n\\begin{abstract}\n    \\par Spatio-temporal graph neural networks (STGNN) have emerged as the dominant model for spatio-temporal graph (STG) forecasting. Despite their success, they fail to model intrinsic \\emph{uncertainties} within STG data, which cripples their practicality in downstream tasks for decision-making. To this end, this paper focuses on \\emph{probabilistic} STG forecasting, which is challenging due to the difficulty in modeling uncertainties and complex \\emph{ST dependencies}. In this study, we present the first attempt to generalize the popular denoising diffusion probabilistic models to STGs, leading to a novel non-autoregressive framework called DiffSTG, along with the first denoising network UGnet for STG in the framework. Our approach combines the spatio-temporal learning capabilities of STGNNs with the uncertainty measurements of diffusion models.   Extensive experiments validate that DiffSTG reduces the Continuous Ranked Probability Score (CRPS) by 4\\%-14\\%, and Root Mean Squared Error (RMSE) by 2\\%-7\\% over existing methods on three real-world datasets. The code is in https://github.com/wenhaomin/DiffSTG.\n\\end{abstract}\n\t\n\t\\begin{CCSXML}\n\t\t<ccs2012>\n\t\t<concept>\n\t\t<concept_id>10002951.10003227.10003351</concept_id>\n\t\t<concept_desc>Information systems~Data mining</concept_desc>\n\t\t<concept_significance>500</concept_significance>\n\t\t</concept>\n\t\t<concept>\n\t\t<concept_id>10010405.10010481.10010487</concept_id>\n\t\t<concept_desc>Applied computing~Forecasting</concept_desc>\n\t\t<concept_significance>500</concept_significance>\n\t\t</concept>\n\t\t</ccs2012>\n\t\\end{CCSXML}\n\t\n\t\\ccsdesc[500]{Information systems~Data mining}\n\t\\ccsdesc[500]{Applied computing~Forecasting}\n\t\n\t\n\t\\keywords{Diffusion Model; Probabilistic Forecasting; Spatio-Temporal Graph Forecasting}\n\t\n\t\\settopmatter{printfolios=true}\n\t\\maketitle\n\n \n\t\n\n\\section{Introduction} \\label{introduction}\n\\par Humans enter a world that is inherently structured, in which a myriad of elements interact with each other both spatially and temporally, resulting in a spatio-temporal composition. Spatio-Temporal Graph (STG) is the de facto most popular tool for injecting such structural information into the formulation of practical problems, especially in smart cities. In this paper, we focus on the problem of \\emph{STG forecasting}, i.e.,  predicting the future signals generated on a graph given its historical observations and the graph structure, such as traffic prediction \\cite{li2018diffusion}, weather forecasting \\cite{simeunovic2021spatio}, and taxi demand estimation \\cite{yao2018deep}. To facilitate understanding, a sample illustration is given in Figure \\ref{fig:probalistic_prediction}(a).\n\n\\begin{figure}[!t]\n    \\centering\n    \\includegraphics[width=0.98 \\linewidth]{img/probalistic_prediction.pdf}\n    \\caption{Illustration of probabilistic STG forecasting. (a): Spatio-temporal graph forecasting;  (b): Motivation of probabilistic prediction.}\n    \\label{fig:probalistic_prediction}\n    \\vspace{-1em}\n\\end{figure}\n\n\\par Recent techniques for STG forecasting are mostly deterministic, calculating future graph signals exactly without the involvement of randomness. Spatio-Temporal Graph Neural Networks (STGNN) have emerged as the dominant model in this research line. They resort to GNNs for modeling spatial correlations among nodes, and Temporal Convolutional Networks (TCN) or Recurrent Neural Networks (RNN) for capturing temporal dependencies. Though promising, these deterministic approaches still fall short of handling \\emph{uncertainties} within STGs, which considerably trims down their practicality in downstream tasks for decision-making. For example, Figure \\ref{fig:probalistic_prediction}(b) depicts the prediction results of passenger flows in a metro station. In the black box, the deterministic method cannot provide the reliability of its predictions. Conversely, the probabilistic method renders higher uncertainties (see the green shadow), which indicates a potential outbreaking of passenger flows in that region. By knowing a range of possible outcomes we may experience and the likelihood of each, the traffic system is able to take operations in advance for public safety management.\n\n\\par While prior endeavors on stochastic STG forecasting were conventionally scarce, we are witnessing a blossom of probabilistic models for time series forecasting \\cite{rubanova2019latent,salinas2020deepar,rasul2021autoregressive}. Denoising Diffusion Probabilistic Models (DDPM) \\cite{ho2020denoising} are one of the most prevalent methods in this stream, whose key insight is to produce the future samples by \\emph{gradually transforming noise into a plausible prediction through a denoising process}. Unlike vanilla unconditional DDPMs that were originally designed for image generation, such transformation function between consecutive steps is conditioned on the historical time series readings. For example, TimeGrad \\cite{rasul2021autoregressive} sets the LSTM-encoded representation of the current time series as the condition, and estimates the future regressively. CSDI \\cite{tashiro2021csdi} directly utilizes observed values as the condition to model the data distribution.\n\n\\par However, the above probabilistic time series models are still insufficient for modeling STGs. Firstly, they only model the temporal dependencies within a single node, without capturing the spatial correlations between different nodes. In reality, objects are correlated with each other spatially, for example, nearby sensors in a road network tend to witness similar traffic trends.  Failing to encode such spatial dependencies will drastically deteriorate the predictive accuracy \\cite{STGCN-2018,GraphWaveNet-2019}. Secondly, the training and inference of existing probabilistic time series models, e.g., Latent ODE \\cite{rubanova2019latent} and TimeGrad, suffer notorious inefficiency due to their sequential nature, thereby posing a hurdle to long-term forecasting. \n\n\\par To address these issues, we generalize the popular DDPMs to spatio-temporal graphs for the first time, leading to a novel framework called \\textbf{DiffSTG}, which couples the spatio-temporal learning capabilities of STGNNs with the uncertainty measurements of DDPMs. Targeting the first challenge, we devise an effective module (UGnet) as the denoising network of DiffSTG. As its name suggests, UGnet leverages a \\underline{U}net-based architecture \\cite{ronneberger2015u} to capture multi-scale temporal dependencies and \\underline{G}NN to model spatial correlations. Compared to existing denoising networks in standard DDPMs, our UGnet performs more accurate denoising in the reverse process by virtue of capturing ST dependencies. To overcome the second issue, our DiffSTG produces future samples in a non-autoregressive fashion. In other words, our framework efficiently generates multi-horizon predictions all at once, rather than producing them step by step as what TimeGrad did. In summary, our contributions lie in three aspects:\n\\begin{itemize}[leftmargin=*]\n    \\item We hit the problem of probabilistic STG forecasting from a score-based diffusion perspective with the first shot. Our DiffSTG can effectively model the complex ST dependencies and intrinsic uncertainties within STG data.\n\n    \\item We develop a novel denoising network called UGNet dedicated to STGs for the first time. It contributes as a new and powerful member of DDPMs' denoising network family for modeling ST-dependencies in STG data.\n    \n\n    \\item We empirically show that DiffSTG reduces the Continuous Ranked Probability Score (CRPS) by 4\\%-14\\%, and Root Mean Squared Error (RMSE) by 2\\%-7\\% over existing probabilistic methods on three real-world datasets.\n\n\\end{itemize}\n\n\\par The rest of this paper is organized as follows. We delineate the concepts of DDPM in Section~\\ref{sec:ddpm}. The formulation and implementation of the proposed DiffSTG are detailed in Section~\\ref{sec:DiffSTG} and \\ref{sec:model_implement}, respectively. We then examine our framework and present the empirical findings in Section~\\ref{sec:experiment}. Lastly, we introduce related arts in Section~\\ref{sec:related} and conclude in Section~\\ref{sec:conclusion}.\n\n\\section{Denoising Diffusion Probabilistic Models} \\label{sec:ddpm}\n\n\\par Given samples from a data distribution $q({\\x_0})$, Denoising Diffusion Probabilistic Models (DDPM) \\cite{ho2020denoising} are unconditional generative models aiming to learn a model distribution $p_\\theta(\\x_0)$ that approximates $q({\\x_0})$ and is easy to sample from. Let $\\x_n$ for $n=1,\\cdots, N$ be a sequence of latent variables from the same sample space of $\\x_0$ (denoted as $\\mathcal X$). DDPM are latent variable models of the form $p_\\theta(\\x_0)=\\int p_\\theta(\\x_{0:N})d{\\x_{1:N}}$. It contains two processes, namely the forward process and the reverse process. \n\\par \\textbf{Forward Process.} The forward process is defined by a Markov chain which progressively adds Gaussian noise to the observation $\\x_0$:\n\\begin{equation}\n    q(\\x_{1:N}  | \\x_0 ) = {\\prod_{n=1}^{N}} q(\\x_n | \\x_{n-1}),\n    \\label{eq:forward_process}\n\\end{equation}\nwhere $q(\\x_n | \\x_{n-1})$ is a Gaussian distribution as\n\\begin{equation}\n    q(\\x_n | \\x_{n-1}) = {\\mathcal{N}(\\x_n;\\sqrt{1-\\beta_n} \\x_{n-1}, \\beta_n \\mathbf{I} )},\n    \\label{eq:forward_Gaussian}\n\\end{equation}\nand $\\{ \\beta_1, \\cdots, \\beta_N \\}$ is an increasing variance schedule  with $\\beta_n \\in (0,1)$ that represents the noise level at forward step $n$. Unlike typical latent variable models such as the variational autoencoder \\cite{rezende2014stochastic}, the approximate posterior $q(\\x_{1:N}  | \\x_0 )$ in diffusion probabilistic models is not trainable but fixed to a Markow chain depicted by the above Gaussian transition process. \n\\par Let ${\\hat \\alpha}_n = 1 - \\beta_n$ and $\\alpha_n= \\prod_{i=1}^{n} {\\hat \\alpha}_i$ be the cumulative product of ${\\hat \\alpha}_n$, a special property of the forward process is that the distribution of $\\x_n$ given $\\x_0$ has a close form:\n\\begin{equation}\n    q(\\x_n|\\x_0) = \\mathcal{N}(\\x_n; \\sqrt{\\alpha_n}\\x_0, (1-\\alpha_n){\\mathbf{I}}),\n    \\label{eq:forward_property}\n\\end{equation}\nwhich can also be expressed as $\\x_n = \\sqrt{\\alpha_n} \\x_0 + \\sqrt{1 - \\alpha_n} {\\be}$ by the reparameteriztioin trick \\cite{kingma2013auto}, with $\\be \\in {\\mathcal N}(\\mathbf{0}; \\mathbf{I})$ as a sampled noise. The above property allows us to directly sample $\\x_n$ at any arbitrary noise level $n$, instead of computing the forward process step by step.\n\n\\par \\textbf{Reverse Process.} The reverse process denoises $\\x_N$ to recover $\\x_0$ recurrently. It also follows a Markov chain but with learnable Gaussian transitions starting with $p(\\x_N)={\\mathcal{N}(\\x_N; \\mathbf{0}, \\mathbf{I}) }$, which is defined as \n\\begin{equation}\n    p_{\\theta}(\\x_{0:N}) = p(\\x_N) \\prod_{n=N}^{1}p_\\theta(\\x_{n-1}|\\x_n). \n    \\label{eq:reverse_process}\n\\end{equation}\nThen, the transition between two nearby latent variables is denoted by \n\\begin{equation}\n    p_\\theta(\\x_{n-1}|\\x_n)  = \\mathcal{N}(\\x_{n-1}; {\\mu_\\theta(\\x_n, n), \\sigma_\\theta(\\x_n, n)}), \n    \\label{eq:p_sample}\n\\end{equation}\nwith shared parameters $\\theta$.  Here we choose the same parameterization of $p_\\theta(\\x_{n-1}|\\x_n)$  as in \\cite{ho2020denoising} in light of its promising performance on image generation:\n\\begin{equation}\n    {\\mu}_\\theta (\\x_n, n)=\\frac{1}{\\alpha_n}\\left(\\x_n-\\frac{\\beta_n}{\\sqrt{1-\\alpha_n}} {\\bm \\epsilon}_\\theta\\left(\\x_n, n\\right)\\right),\n\\end{equation}\n\\begin{equation}\n    \\sigma_\\theta(\\x_n, n)= \\frac{1 - \\alpha_{n-1}}{1 - \\alpha_n} \\beta_n,\n\\end{equation}\nwhere ${\\be}_\\theta({\\mathcal X} \\times {\\mathbb R}) \\rightarrow {\\mathcal X}$ is a trainable denoising function that decides how much noise should be removed at the current denoising step. The parameters $\\theta$ are learned by solving the following optimization problem:\n\\begin{equation}\n\\mathop{\\min} _\\theta \\mathcal{L}(\\theta)=\n\\min _\\theta \\mathbb{E}_{\\x_0 \\sim q\\left(\\x_0\\right), \\be \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}), n}\\left\\|\\be-\\be_\\theta\\left(\\x_n, n\\right)\\right\\|_2^2.\n\\label{eq:loss}\n\\nonumber\n\\end{equation}\nSince we already know $\\x_{0}$ in the training stage, and recall that  $\\x_n = \\sqrt{\\alpha_n} \\x_0 + \\sqrt{1 - \\alpha_n} {\\be}$ by the property as mentioned in the forward process, the above training objective of unconditional generation can be specified as \n\\begin{equation} \n\\small\n\\mathop{\\min} _\\theta \\mathcal{L}(\\theta)=\n\\min _\\theta \\mathbb{E}\\left\\|\\be-\\be_\\theta\\left({\\sqrt{\\alpha_n} \\x_0 + \\sqrt{1 - \\alpha_n} {\\be}}, n\\right)\\right\\|_2^2.\n\\label{eq:loss_x0}\n\\end{equation}\nThis training objective can be viewed as a simplified version of loss similar to the one in Noise Conditional Score Networks \\cite{song2019generative, song2020improved}. Once trained, we can sample $\\x_0$ from Eq.~(\\ref{eq:reverse_process}) and Eq.~(\\ref{eq:p_sample}) starting from the Guassian noise $\\x_N$. This reverse process resembles Langevin dynamics, where we first sample from the most noise-perturbed distribution and then reduce the noise scale step by step until we reach the smallest one. We provide details of DDPM in Appendix~\\ref{appendix:DDPM_detail}.\n\n\\section{DiffSTG Formulation} \\label{sec:DiffSTG}\n\n\\par Let $\\mathcal{G}=\\{{\\mathcal V}, {\\mathcal E}, {\\mathbf A} \\}$ represent a graph with $V$ nodes, where $\\mathcal V$,  $\\mathcal E$ are the node set and edge set, respectively. ${\\mathbf A} \\in {\\mathbb R}^{V \\times V}$ is a weighted adjacency matrix to describe the graph topology. \nFor ${\\mathcal V}=\\{ v_1, \\dots, v_{V} \\}$, let ${\\mathbf x}_t \\in {\\mathbb R}^{ F \\times V}$ denote $F$-dimentional signals generated by the $V$ nodes at time $t$. Given historical graph signals $\\xh = [{\\mathbf x_1}, \\cdots, {\\mathbf x}_{T_h}]$ of $T_h$  time steps and the graph $\\G$ as inputs, STG forcasting aims at learning a function $\\mathcal F$ to predict future graph signals $\\xp{}$, which is formulated as:\n\\begin{equation}\n    {\\mathcal F}:(\\xh; \\G) \\rightarrow [{\\mathbf x_{T_h + 1}}, \\cdots, {\\mathbf x_{T_h + T_p}}]:=\\xp{},\n\\end{equation}\nwhere $T_p$ is the forecasting horizon. In this study, we focus on the task of probabilistic STG forecasting, which aims to estimate the distribution of future graph signals. \n\n\\par As introduced in Section~\\ref{introduction}, on the one hand, current deterministic STGNNs can capture the spatial-temporal correlation in STG data, while failing to model the uncertainty of the prediction. On the other hand, diffusion-based probabilistic time series forecasting models  \\cite{rasul2021autoregressive, tashiro2021csdi} have powerful abilities in learning high-dimensional sequential data distributions, while incapable of capturing spatial dependencies and facing efficiency problems when applied to STG data. \n\n\\par To this end, we generalize the popular DDPM to spatio-temporal graphs and present a novel framework called DiffSTG for probabilistic STG forecasting in this section. DiffSTG couples the spatio-temporal learning capabilities of STGNNs with the uncertainty measurements of diffusion models.\n \n\n\\subsection{Conditional Diffusion Model}\n\n\\par The original DDPM is designed to generate an image from a white noise without condition, which is not aligned with our task where the future signals are generated conditioned on their histories.  Therefore, for STG forecasting, we first extend the DDPM to a conditional one by making a few modifications to the reverse process. In the unconditional DDPM, the reverse process $p_{\\theta}(\\x_{0:N})$  in Eq.~(\\ref{eq:reverse_process}) is used to calculate the final data distribution $q(\\x_0)$. To get a conditional diffusion model for our task, a natural approach is to add the history $\\xh$ and the graph structure $\\G$ as the condition in the reverse process in Eq.~(\\ref{eq:reverse_process}). In this way, the conditioned reverse diffusion process can be expressed as\n\\begin{equation}\n    p_{\\theta}(\\xp{0:N} | \\xh, \\G) = p(\\xp{N}) \\prod_{n=N}^{1}p_\\theta(\\xp{n-1}|\\xp{n}, \\xh, \\G). \n    \\label{eq:condition_reverse_process}\n\\end{equation}\nThe transition probability of two latent variables in Eq.~(\\ref{eq:p_sample}) can be extended as\n\\begin{equation}\n    \\begin{array}{l}\n        p_\\theta(\\xp{n-1}|\\xp{n}, \\xh, \\G) \\vspace{5pt}  \\\\ \n        = \\mathcal{N}(\\xp{n-1}; {\\mu_\\theta(\\xp{n}, n | \\xh, \\G), \\sigma_\\theta(\\xp{n}, n | \\xh, \\G)}).\n    \\end{array}\n    \\label{eq:condition_p_sample}\n\\end{equation}\n\\par Furthermore, the training objective in Eq.~(\\ref{eq:loss_x0}) can be rewritten as a conditional one:\n\\begin{equation}\n\\mathop{\\min} _\\theta \\mathcal{L}(\\theta)=\n\\min _\\theta \\mathbb{E}_{\\xp{0}, \\be} \\left\\|\\be-\\be_\\theta\\left(\\xp{n}, n | \\xh, \\G\\right)\\right\\|_2^2.\n\\label{eq:condition_loss}\n\\end{equation}\n\n\\subsection{Generalized Conditional Diffusion Model}\n\n\\par In Eq.~(\\ref{eq:condition_reverse_process})-(\\ref{eq:condition_loss}), the condition $\\xh$ and denoising target $\\xp{}$ are separated into two sample space $\\xh \\in {\\mathcal{X}^{\\rm h}}$ and $\\xp{} \\in {\\mathcal{X}^{\\rm p}}$. However, they are indeed extracted from two consecutive periods. Here we propose to consider the history $\\xh$ and future $\\xp{}$ as a whole, i.e., $\\xall = [ \\xh, \\xp{} ] \\in \\mathbb{R}^{F \\times V \\times  T}$, where $T = T_h + T_p$. The history can be represented by masking all future time steps in $\\xall$, denoted by  $\\xall_{\\rm msk}$. So that the condition $\\xall_{\\rm msk}$ and denoise target $\\xall$ share the same sample space ${\\mathcal{X}^{\\rm all}}$. Thus, the masked version of Eq.~(\\ref{eq:condition_reverse_process}) can be rewritten as \n\\begin{equation}\n    \\small\n    p_{\\theta}(\\xall_{0:N} | \\xall_{\\rm msk}, \\G) = p(\\xall_{N}) \\prod_{n=N}^{1}p_\\theta(\\xall_{n-1}|\\xall_{n}, \\xall_{\\rm msk}, \\G). \n    \\label{eq:masked_condition_reverse_process}\n\\end{equation}\nThe masked version of Eq.~(\\ref{eq:condition_loss}) can be rewritten as \n\\begin{equation}\n\\small\n\\mathop{\\min} _\\theta \\mathcal{L}(\\theta)=\n\\min _\\theta \\mathbb{E}_{\\xall_{0}, \\be} \\left\\|\\be-\\be_\\theta\\left(\\xall_{n}, n | \\xall_{\\rm msk}, \\G\\right)\\right\\|_2^2.\n\\label{eq:masked_condition_loss}\n\\end{equation}\nCompared with the formulation in Eq.~(\\ref{eq:condition_reverse_process})-(\\ref{eq:condition_loss}), this new formulation is a more generalized one which has the following merits. Firstly, the loss in Eq.~(\\ref{eq:masked_condition_loss}) unifies the reconstruction of the history and estimation of the future, so that the historical data can be fully utilized to model the data distribution. Secondly, the new formulation unifies various STG tasks in the same framework, including STG prediction, generation, and interpolation \\cite{li2004interpolation}.\n\n\\par \\textbf{Training.} In the training process, we first construct the masked signals $\\xall_{\\rm msk}$ according to observed values. Then, given the conditional masked information $\\xall_{\\rm msk}$, graph $\\G$ and the target $\\xall_{0}$, we sample noise targets $\\xall_n = \\sqrt{\\alpha_n} \\xall_0 + \\sqrt{1 - \\alpha_n} {\\be}$, and then train $\\epsilon_{\\theta}$ by the loss function in Eq.~(\\ref{eq:masked_condition_loss}). The training procedure of DiffSTG is presented in Algorithm~\\ref{alg:training}.\n\n\\floatname{algorithm}{Algorithm}  \n\\renewcommand{\\algorithmicrequire}{\\textbf{Input:}}  \n\\renewcommand{\\algorithmicensure}{\\textbf{Output:}} \n\\begin{algorithm}[htbp]\n   \\caption{Training of DiffSTG}\n   \\small\n\\begin{algorithmic}[1]\n   \\Require distribution of training data $q(\\xall_0)$, number of diffusion step $N$, variance schedule $\\{\\beta_1, \\cdots, \\beta_N \\}$,  graph  $\\G$.\n   \\Ensure Trained denoising function $\\be_{\\theta}$ \n\n   \\Repeat\n   \\State $n \\sim {\\rm Uniform}(\\{ 1, \\cdots, N\\}), \\xall_{0} \\sim q(\\xall_0)$\n   \n    \\State Constructing the masked signals $\\xall_{\\rm msk}$ according to observed values\n   \\State Sample $\\be \\sim {\\mathcal{N}}(\\mathbf{0}, \\mathbf{I})$ where $\\be$'s dimension corresponds to $\\xall_{0}$\n   \\State Calculate noisy targets $\\xall_{n}=\\sqrt{\\alpha_n}\\xall_{0} + \\sqrt{1 - \\alpha_n }\\be$\n\n   \\State Take gradient step  $\\nabla_\\theta  \\| \\be- \\be_\\theta(\\xall_{n}, n | \\xall_{\\rm msk}, \\G) )\\|_2^2$ according to Eq.~(\\ref{eq:masked_condition_loss})\n   \\Until{converged}\n\\end{algorithmic}\n\\label{alg:training}\n\\end{algorithm}\n\n\\par \\textbf{Inference.} As outlined in Algorithm~\\ref{alg:samping}, the inference process  utilizes the trained denoising function $\\be_{\\theta}$ to sample $\\xall_{n-1}$ step by step according to Eq.~(\\ref{eq:masked_condition_reverse_process}), under the guidance of $\\xall_{\\rm msk}$ and $\\G$. Note that unlike the previous diffusion-based model, i.e., TimeGrad, which requires executing $T_p$ times of reverse diffusion process for predicting the future $T_p$ steps,  DiffSTG only takes one reverse diffusion process to achieve the prediction, thus significantly accelerating the inference speed.\n\n\\begin{algorithm}[htbp]\n    \\caption{Sampling of DiffSTG}\n    \\small\n    \\begin{algorithmic}[1]\n    \\Require Historical graph signal $\\xh$,  graph $\\G$, trained denoising function $\\be_{\\theta}$\n    \\Ensure Future forecasting $\\xp{}$\n\n    \\State Construct $\\xall_{\\rm msk}$ according to $\\xh$\n\n    \\State Sample $\\be \\sim {\\mathcal{N}}(\\mathbf{0}, \\mathbf{I})$ where $\\be$'s dimension corresponds to $\\xall_{\\rm msk}$\n    \\For{$n=N$ {\\bfseries to} $1$}\n        \\State Sample $\\xall_{n-1}$ using Eq.~(\\ref{eq:masked_condition_reverse_process}) by taking $\\xall_{\\rm msk}$ and $\\G$ as condition\n    \\EndFor\n   \\State Take out the forecast target in $\\xall_{0}$, i.e., $\\xp{}$\n    \\State Return $\\xp{}$\n    \\end{algorithmic}\n    \\label{alg:samping}\n  \n\\end{algorithm}\n\n\\section{DiffSTG Implementation} \\label{sec:model_implement}\n\n\\begin{figure*}[htbp]\n    \\centering\n    \\includegraphics[width=1 \\linewidth]{img/model_unet.pdf} \n    \\vspace{-10pt}\n    \\caption{Illustration of proposed DiffSTG and denoising network UGnet. The inference process of DiffSTG  utilizes the trained denoising function $\\be_{\\theta}$ (i.e., UGnet) to sample $\\xall_{n-1}$ step by step, under the guidance of $\\xall_{\\rm msk}$ and $\\G$. UGnet leverages an Unet-based architecture to capture multi-scale temporal dependencies and the Graph Neural Network (GNN) to model spatial correlations.}\n    \\vspace{-5pt}\n    \\label{fig:model}\n\\end{figure*}\n\n\\par After introducing the DiffSTG's formulation, we implement it via elaborately-designed model architecture, which is illustrated in Figure~\\ref{fig:model}. At the heart of the model is the proposed denoising network (UGnet) $\\be_{\\theta}$ in the reverse diffusion process, which performs accurate denoising with the ability to effectively model ST dependencies in the data.\n\n\\subsection{Denoising Network: UGnet}\n\\par  Denoising network $\\be_{\\theta}$ in previous works can be mainly classified into two classes, Unet-based architecture \\cite{ronneberger2015u} for image-related tasks \\cite{rombach2022high,voleti2022mcvd, ho2020denoising},\nand WaveNet-based architecture \\cite{vanwavenet} for sequence-related tasks \\cite{kong2020diffwave, liu2022diffsinger, kim2020glow}. These networks consider the input as either grids or segments, lacking the ability to capture spatio-temporal correlations in STG data. To bridge this gap, we propose a new denoising network $\\be_\\theta({\\mathcal X}^{\\rm all} \\times {\\mathbb R} | {\\mathcal X}_{\\rm msk}^{\\rm all}, \\G) \\rightarrow {\\mathcal X}^{\\rm all}$, named UGnet. It adopts an \\underline{U}net-like architecture in the temporal dimension to capture temporal dependencies at different granularities (e.g., 15 minutes or 30 minutes), and utilizes \\underline{G}NN to model the spatial correlations.\n\n\\par Specifically, as shown in Figure ~\\ref{fig:model}, UGnet takes $\\xall_{\\rm msk}$, $\\xall_{n}$, $n$, $\\G$ as inputs, and outputs the denoised noise $\\be$. It first concatenates $\\xall_{n} \\in \\mathbb{R}^{F \\times V \\times T}$ and $\\xall_{\\rm msk} \\in \\mathbb{R}^{F \\times V \\times T}$ in the temporal dimension to form a new tensor $\\xnewall_n \\in \\mathbb{R}^{F \\times V \\times 2T}$, which is projected to a high-dimensional representation ${\\mathbf H} \\in {\\mathbb R}^{C \\times V \\times 2T}$ by a linear layer, where $C$ is the projected dimension. Then ${\\mathbf H}$ is fed into several Spatio-temporal Residual Blocks (ST-Residual Blocks for short), each capturing temporal and spatial dependencies, respectively. Let ${\\mathbf H}_i \\in {\\mathbb R}^{C \\times V \\times T_{i}}$ (where ${\\mathbf H}_0={\\mathbf{H}}$) denote the input of the $i$-th ST-Residual Block, where $T_{i}$ is the length of time dimension.\n\n\\par \\textbf{Temporal Dependency Modeling.} As shown in Figure ~\\ref{fig:epsilon_theta}, at each ST-Residual Block, ${\\mathbf H}_i$ is fed into a Temporal Convolution Network (TCN) \\cite{bai2018tcn} for modeling temporal dependence, which is a 1-D gated causal convolution of $K$ kernel size with padding to get the same shape with input.  The convolution kernel $\\Gamma_{\\mathcal T} \\in \\mathbb{R} ^ {K \\times C_{\\rm in}^{\\rm t} \\times C_{\\rm out}^{\\rm t}}$ maps the input $\\mathbf{H}_i$ to outputs ${\\mathbf P}_i$, ${\\mathbf Q}_i \\in \\mathbb{R}^{C_{\\rm out}^{\\rm t} \\times V \\times T_{i}}$ with the same shape. Formally, the temporal gated convolution can be defined as\n\\begin{equation}\n    \\Gamma_{\\mathcal T} ({\\mathbf H}_{i})  =  {\\mathbf P}_i \\odot \\sigma({\\mathbf Q}_i) \\in \\mathbb{R} ^ {C_{\\rm out}^{\\rm t} \\times V \\times T_{i}},\n\\end{equation}\nwhere $\\odot$ is the element-wise Hadamard product, and $\\sigma$ is the sigmoid activation function. The item $\\sigma({\\mathbf Q}_i) $ can be considered a gate that filters the useful information of ${\\mathbf P}_i$ into the next layer. We denote the output of TCN as $\\overline{\\bmH}_i$.\n\n\\par \\textbf{Spatial Dependency Modeling.} Graph Convolution Networks (GCNs) are generally employed to extract highly meaningful features in the space domain \\cite{survey-GNN-2020}. The graph convolution can be formulated as\n\\begin{equation}\n    \\Gamma_{\\mathcal G}( \\overline{\\bmH}_i )  =\\sigma \\left(\\Phi \\left(\\mathbf{A}_{\\rm gcn},\\overline{\\bmH}_i\\right) \\mathbf{W}_i\\right),\n\\end{equation}\nwhere $\\mathbf{W}_i \\in \\mathbb{R}^{C_{\\rm in}^{\\rm g} \\times C_{\\rm in}^{\\rm g}}$ denotes a trainable parameter and $\\sigma$ is an activation function.\n$\\Phi(\\cdot)$ is an aggregation function that decides the rule of how neighbors' features are aggregated into the target node.  In this work, we do not focus on developing the function $\\Phi(\\cdot)$. Instead, we use the form in the most popular vanilla GCN~\\cite{GCN-kipf2017} that defines a symmetric normalized summation function as $\\Phi_{\\rm gcn}\\left(\\mathbf{A}_{\\rm gcn},\\overline{\\bmH}_i\\right) = \\mathbf{A}_{\\rm gcn} \\overline{\\bmH}_i,$ where $\\mathbf{A}_{\\rm gcn}=\\mathbf{D}^{-\\frac{1}{2}}(\\mathbf{A}+\\mathbf{I})\\mathbf{D}^{-\\frac{1}{2}} \\in \\mathbb{R}^{V \\times V}$ is a normalized adjacent matrix of graph $\\G$. $\\mathbf{I}$ is the identity matrix and $\\mathbf{D}$ is the diagonal degree matrix with $\\mathbf{D}_{ii}=\\sum_{j}(\\mathbf{A}+\\mathbf{I})_{ij}$. Note that we reshape the output of the TCN layer to $\\overline{\\bmH}_i \\in \\mathbb{R}^{V \\times  C_{\\rm in}^{\\rm g}}$, where $C_{\\rm in}^{\\rm g} = T_{i} \\times C_{\\rm out}^{\\rm t}$, and fed this node feature $\\overline{\\bmH}_i$  to GCN. \n\n\\par \\textbf{Noise Level Embedding.} As shown in the right part of Figure~\\ref{fig:model}, like previous diffusion-based models \\cite{rasul2021autoregressive}, we use positional encodings of the noise level $n \\in [1,N]$ and process it using a transformer positional embedding \\cite{vaswani2017attention}:\n\\begin{equation}\n\\small\n    \\mathbf{e}(n)=\\left[\\ldots, \\cos \\left(n / r^{\\frac{-2 d}{D}}\\right), \\sin \\left(n / r^{\\frac{-2 d}{D}}\\right), \\ldots\\right]^{\\mathrm{T}},\n\\end{equation}\nwhere $d=1,\\cdots, D/2$ is the dimension number of the embedding (set to 32), and $r$ is a large constant 10000. For more details about UGnet, please refer to Appendix~\\ref{appendix:UGnet}.\n\n\\par We highly the unique characteristics and our special design of UGnet by making comparisons with the popular Unet used in diffusion models for the computer version.  1) U-Net is designed for extracting semantic features from Euclidean data, such as images, while UGNet is designed for non-Euclidean STG data to model the spatial-temporal correlations. 2) The U-structure in U-Net serves to aggregate features in the spatial dimension to extract high-level semantic features in the image, while the U-structure in UGNet aggregates signals in the time dimension to capture time dependencies at different granularities. 3) U-Net utilizes 2D-CNNs to capture the spatial correlation, whereas UGNet utilizes graph convolution to model the spatial correlation, which is more suitable for graph-based data.  In summary, the proposed UGnet goes beyond a simple replacement of  U-Net by changing the input from image to graph $\\G$,  and involves careful design considerations for STG data.\n\n\\subsection{Sampling Acceleration}\n\\par From a variational perspective, a large $N$ (e.g.,  $N=1000$ in \\cite{ho2020denoising}) allows results of the forward process to be close to a Gaussian distribution so that the reverse denoise process started with Gaussian distribution becomes a good approximation. However, large $N$ makes the sampling low-efficiency since all $N$ iterations have to be performed sequentially. To accelerate the sampling process, we adopt the sampling strategy in \\cite{song2020denoising}, which only samples a subset $\\{\\tau_1, \\cdots, \\tau_{M} \\}$ of $M$ diffusion steps. Formally, the accelerated sampling process can be denoted as\n\\begin{equation}\n\\begin{array}{l}\n    \\small\n     \\x_{\\tau_{m-1}}=\\sqrt{\\alpha_{\\tau_{m-1}}} {\\left(\\frac{\\x_{\\tau_m}-\\sqrt{1-\\alpha_{\\tau_m}} \\be_\\theta^{(\\tau_m)}}{\\sqrt{\\alpha_{\\tau_m}}}\\right)}+ \\vspace{5pt} \\\\ \n     {\\sqrt{1-\\alpha_{\\tau_{m-1}}-\\sigma_{\\tau_m}^2} \\cdot \\be_\\theta^{(\\tau_m)}} + {\\sigma_{\\tau_m} \\be_{\\tau_m}}, \n\\end{array}\n\\end{equation}\nwhere $\\be_{\\tau_m} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ is standard Gaussian noise independent of $\\x_n$. And $\\sigma_{\\tau_m}$ controls how stochastic the denoising process is. We set $\\sigma_n=\\sqrt{\\left(1-\\alpha_{n-1}\\right) /\\left(1-\\alpha_n\\right)} \\sqrt{1-\\alpha_n / \\alpha_{n-1}}$ for all diffusion steps, to make the generative process become a DDPM. When the length of the sampling trajectory is much smaller than $N$, we can achieve significant increases in computational efficiency. Moreover, note that the data in the last $k$ few reverse steps $\\xall_{i}$ ($i \\in \\{1,\\dots, k\\}$) can be considered a good approximation of the target. Thus we can also add them as samples, reducing the number of the reverse diffusion process from $S$ to $S/k$, where $S$ is the required sample number to form the data distribution.\n\n\\subsection{Comparsion among Different Approaches}\n\n\\par We give the overview of related models in Figure~\\ref{fig:overview}: i) Deterministic STGNNs calculate future graph signals exactly without the involvement of randomness. While the vanilla DDPM is a latent variable generative model without condition; ii) To estimate the data distribution from a trained model, i.e., getting $S$ samples, TimeGrad runs $S \\times T_p \\times N $ diffusion steps for the prediction of all future time steps, where  $N$, $T_p$ is the diffusion step, prediction length, respectively; iii) Compared with current diffusion-based models for time series, DiffSTG 1) incorporates the graph as the condition so that the spatial correlations can be captured, and 2) is a non-autoregressive approach with $\\widetilde{S}~\\times \\widetilde{N} $ diffusion steps to get the estimated data distribution, where $\\widetilde{S}=S/k <S$ and $\\widetilde{N}=M < N$.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=1 \\linewidth]{img/high_level_compare.pdf} \n    \\vspace{-1.5em}\n    \\caption{Overview of different models.}\n    \\label{fig:overview}\n    \\vspace{-15pt}\n\\end{figure}\n\n\\section{Experiments} \\label{sec:experiment}\n\n\\par We conduct extensive experiments to evaluate the effectiveness of our proposed DiffSTG on three real-world datasets and compare it with other probabilistic baselines. \n\n\\subsection{Experimental Setup}\n\\textbf{Datasets.} In the experiments, we choose three real-world datasets from two domains, including a traffic flow dataset PEMS08 \\cite{STSGCN-2020}, and two air quality datasets AIR-BJ and AIR-GZ \\cite{yi2018deep}.  PEMS08 is a traffic flow dataset collected by the Caltrans Performance Measurement System (PeMS). It records the traffic flow recorded by sensors (nodes) deployed on the road network. In this work, we use the dataset extracted by STSGCN \\cite{STSGCN-2020}. The traffic networks (adjacency matrix) for these datasets are constructed according to the actual road network. If the two sensors are on the same road, the two points are considered connected in the spatial network. \n\n\\begin{table}[htbp]\n    \\caption{Details of all datasets.}\n    \\centering\n    \\vspace{-1em}\n    \\setlength \\tabcolsep{3pt}\n    \\resizebox{1 \\linewidth}{!}{\n\t\\begin{tabular}{cccccc}\n\t\t\\toprule\n\t\tDataset & Nodes & $F$  &   Data Type    &     Time interval     &      \\#Samples     \\\\ \n\t\t\\midrule\n\t\tPEMS08  &  170  &   1    & Traffic flow   &       5 minutes       & 17,856 \\\\\n            AIR-BJ  &  34  &   1    & PM$_{2.5}$   &       1 hour       &  8,760 \\\\\n            AIR-GZ  &  41  &   1    & PM$_{2.5}$   &       1 hour       &  8,760 \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n        }\n\t\\label{tab:dataset}\n\\end{table}\n\n\\par The air quality datasets AIR-BJ and AIR-GZ consist of one-year PM$_{2.5}$ readings collected by air quality monitoring stations in two metropolises (i.e., Beijing and Guangzhou) in China \\cite{yi2018deep}, respectively. AIR-BJ records data from 34 stations in Beijing from 2019/01/01 to 2019/12/31. And AIR-GZ records data from 41 stations in Guangzhou from 2017/01/01 to 2017/12/31. We build the spatial correlation matrix $\\mathbf A$ using the distance between two stations. Statistics of the datasets are shown in Table~\\ref{tab:dataset}. \n\n\\begin{table*}[!t]\n\\centering\n\\caption{Experiment results of probabilistic methods. Smaller MAE, RMSE, and CRPS indicate better performance. The best result is in bold, and the second-best result is underlined.}\n\\setlength\\tabcolsep{12.2 pt}\n\\begin{tabular}{l|ccc|ccc|ccc}\n\\hline %\\hline\n\\multirow{2}{*}{Method}\n& \\multicolumn{3}{c|}{ AIR-BJ }& \\multicolumn{3}{c|}{  AIR-GZ }& \\multicolumn{3}{c}{ PEMS08 }\\\\\n\\cline{2-10}\n & MAE & RMSE & CRPS & MAE & RMSE & CRPS & MAE & RMSE & CRPS\\\\\n\\hline \\hline\nLatent ODE \\cite{rubanova2019latent} & 20.61 & 32.27 & 0.47 & 12.92 & 18.76 & 0.30 & 26.05 & 39.50 & 0.11 \\\\ \\hline\nDeepAR \\cite{salinas2020deepar} & 20.15 & 32.09 & 0.37 & 11.77 & 17.45 & \\underline{0.23} & 21.56 & 33.37 & \\underline{0.07} \\\\ \\hline\nCSDI \\cite{tashiro2021csdi} & 26.52 & 40.33 & 0.50 & 13.75 & 19.40 & 0.28 & 32.11 & 47.40 & 0.11 \\\\ \\hline\nTimeGrad \\cite{rasul2021autoregressive} & \\underline{18.64} & \\underline{31.86} & \\underline{0.36} & 12.36 & 18.15 & 0.25 & 24.46 & 38.06 & 0.09 \\\\ \\hline\nMC Dropout \\cite{wu2021quantifying} & 20.80 & 40.54 & 0.45 & \\underline{11.12} & \\underline{17.07} & 0.25 & \\underline{19.01} & \\underline{29.35} & 0.07 \\\\ \\hline\nDiffSTG (ours) & \\textbf{17.88} & \\textbf{29.60} & \\textbf{0.34} & \\textbf{10.95} & \\textbf{16.66} & \\textbf{0.22} & \\textbf{17.68} & \\textbf{27.13} & \\textbf{0.06} \\\\ \nError reduction & -4.1\\% & -7.1\\% & -5.6\\% & -1.5\\% & -2.4\\% & -4.3\\% & -7.0\\% & -7.6\\% & -14.3\\% \\\\ \\hline\n\\end{tabular}\n\n\\label{tab:results}\n\\end{table*}\n\n\\par \\noindent \\textbf{Baselines.} Both start-of-the-art probabilistic and deterministic models are included for the performance comparison.\n\\par \\textit{Probabilistic baselines.} The following methods are implemented as baselines for probabilistic STG forecasting:\n\n\\begin{itemize}[leftmargin=*]\n    \\item Latent ODE \\cite{rubanova2019latent}. It defines a probabilistic generative process over time series from a latent initial state, which can be trained with variational inference.\n    \\item DeepAR \\cite{salinas2020deepar}, which utilizes a Gaussian distribution to model the data distribution; \n    \\item TimeGrad \\cite{rasul2021autoregressive}, which is an auto-regressive model that combines the diffusion model with an RNN-based encoder; \n    \\item CSDI \\cite{tashiro2021csdi}, which is a diffusion-based non-autoregressive model first proposed for multivariate time series imputation. We mask all the future signals to adapt CSDI to our task.\n    \\item MC Dropout \\cite{wu2021quantifying}, which is developed based on MC Dropout \\cite{gal2017concrete} for probabilistic spatio-temporal forecasting.\n\\end{itemize}\n\n\\par \\textit{Deterministic baselines.} We choose some popular and state-of-the-art methods for comparison:\n\\begin{itemize}[leftmargin=*]\n\n    \\item DCRNN \\cite{li2018diffusion}: Diffusion Convolutional Recurrent Neural Network integrates diffusion convolution with sequence-to-sequence architecture to learn the representations of spatial dependencies and temporal relations.\n    \n    \\item STGCN \\cite{STGCN-2018}: Spatial-Temporal Graph Convolution Network combines spectral graph convolution with 1D convolution to capture spatial and temporal correlations. \n\n    \\item STGNCDE \\cite{choi2022graph}. Spatio-Temporal Graph Neural Controlled Differential Equation introduces two neural control differential equations (NCDE) for processing\nspatial and sequential data, respectively, which can be considered as an NCDE-based interpretation\nof graph convolutional networks.\n\n    \\item GMSDR \\cite{liu2022msdr}: Graph-based Multi-Step Dependency Relation improves RNN by explicitly taking the hidden states of multiple historical time steps as the input of each time unit.\n\n\\end{itemize}\n\n\\par \\noindent \\textbf{Metrics.} We choose the Continuous Ranked Probability Score (CRPS) \\cite{matheson1976scoring} as the metric to evaluate the performance of probabilistic prediction, which is commonly used to measure the compatibility of an estimated probability distribution $F$ with an observation $x$:\n\\begin{equation}\n    {\\rm CRPS}(F, x)=\\int_{\\mathbb{R}}(F(z)-\\mathbb{I}\\{x \\leq z\\})^2 \\mathrm{~d} z,\n\\end{equation}\nwhere $\\mathbb{I}\\{x \\leq z\\}$ is an indicator function which equals one if $x \\leq z$, and zero otherwise. Smaller CRPS means better performance.\n\nIn addition, we leverage Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) to evaluate the performance of deterministic prediction. Let $Y$ be the label, and $\\hat{Y}$ denote the predictive result. \n\\begin{equation}\n    {\\rm MAE}(Y, \\hat{Y})=\\frac{1}{\\left| Y \\right|}\\sum^{\\left| Y \\right|}_{i=1}\\left| Y_{i} - \\hat{Y}_{i}\\right|,\n\\end{equation}\n\\begin{equation}\n   {\\rm RMSE}(Y, \\hat{Y})=\\sqrt{\\frac{1}{\\left| Y \\right|}\\sum^{\\left| Y \\right|}_{i=1}\\left( Y_{i} - \\hat{Y}_{i}\\right)^{2}}.\n\\end{equation}\nwhere a smaller metric means better performance. Among those metrics, CRPS is the primary metric of our focus to evaluate the performance of different probabilistic methods. \n\n\\par \\noindent \\textbf{Implementation Details.} As for hyperparameters, we set the batch size as 8 and use the Adam optimizer with a learning rate of 0.002, which is halved every 5 epochs. For CSDI and DiffSTG, we adopt the following quadratic schedule for variance schedule: $ \\beta_n=\\left(\\frac{N-n}{N-1} \\sqrt{\\beta_1}+\\frac{n-1}{N-1} \\sqrt{\\beta_N}\\right)^2$. We set the minimum noise level $\\beta_1 =  0.0001$ and hidden size $C= 32$ and search the number of the diffusion step $N$ and the maximum noise level $\\beta_N$ from a given parameter space ($N \\in [50, 100, 200]$, and $\\beta_N \\in [0,1, 0.2, 0.3, 0.4]$), and each model's best performance is reported in the experiment. For TimeGrad, as TimeGrad is an extremely time-consuming method, we followed the recommendations from the original paper and performed a search in a smaller diffusion setting space for the number of diffusion steps ($N \\in [50, 100]$) and the maximum noise level ($\\beta_N \\in [0,1, 0.2]$). This approach still allowed us to optimize TimeGrad's performance while considering the computational constraints. For other baselines, we utilize their codes and parameters in the original paper. For all datasets, the history length $T_h$, and prediction length $T_p$ are both set to 12. All datasets are split into the training, validation, and test sets in chronological order with a ratio of 6:2:2. The models are trained on the training set and validated on the validation set by the early stopping strategy. %The source code will be released after the review process.\n\n\\subsection{Performance Comparison} \n\n\\par The efforts of stochastic models for probabilistic STG forecasting were traditionally scarce. Hence, we compare our model with state-of-the-art baselines in the field of probabilistic time series forecasting, including Latent ODE \\cite{rubanova2019latent}, DeepAR \\cite{salinas2020deepar}, TimeGrad \\cite{rasul2021autoregressive}, CSDI \\cite{tashiro2021csdi}, and a recent STG probabilistic forecasting method MC Dropout \\cite{wu2021quantifying}. We choose the Continuous Ranked Probability Score (CRPS) \\cite{matheson1976scoring} as an evaluation metric. We also report MAE and RMSE of the deterministic forecasting results by averaging $S$ (set to $8$ in our paper) generated samples. Note that CRPS is the primary metric to evaluate the performance of those probabilistic methods. \n\n\\par In Table \\ref{tab:results}, DiffSTG outperforms all the probabilistic baselines: it reduces the CRPS by 5.6\\%, 4.3\\%, and 14.3\\% on the three datasets compared to the most competitive baseline in each dataset, respectively. Distributions in DeepAR and Latent ODE can be viewed as some types of low-rank approximations of the target, which naturally restricts their capability to model the true data distribution. TimeGrad outperforms LatentODE due to its DDPM-based architecture with tractable likelihoods that models the distribution in a general fashion. CSDI is a diffusion-based model originally proposed for time series imputation, thus performing worse in our forecasting tasks. MC Dropout achieves the second best performance on MAE and RMSE in most datasets, due to its strong ability in modeling the ST correlations. Our DiffSTG yields the best performance in both deterministic and probabilistic prediction, revealing that it can preserve the spatio-temporal learning capabilities of STGNNs as well as the uncertainty measurements of the diffusion models.\n\n\\textbf{Inference Time.} Generally, diffusion-based forecasting models are much slower than other methods (due to their recurrent denoising nature), but they deliver promising performance, as introduced in TimeGrad and DDPM. To this end, here we mainly compare the inference speed of the diffusion-based forecasting method, including DiffSTG, TimeGrad, and CSDI. Table~\\ref{tab:time_cost} reports the average time cost per prediction of the three diffusion-based forecasting models. We observe that TimeGrad is extremely time-consuming due to its recurrent architecture. DiffSTG (with $M$=100 and $k$=1) achieves 40$\\times$ speed-up compared to TimeGrad, which stems from its non-autoregressive architecture. The accelerated sampling strategy achieves 3$\\sim$4$\\times$ speed-up beyond DiffSTG ($M$=100, $k$=1). We also find that when $S$ is large, one can increase $k$ for efficiency without loss of performance. See Sectioin~\\ref{sec:hyperparameter_study} for more details. \n\n\\begin{table}[h]\n    \\caption{Time cost (by seconds) of diffusion-based models (TimeGrad, CSDI, and DiffSTG) in AIR-GZ ($T_h=12, T_p=12, N = 100$). $S$ is the number of samples.}\n    \\centering\n    \\setlength\\tabcolsep{10pt}\n\t\\begin{tabular}{l|ccc}\n\t\t\\toprule\n\t\tMethod & $S$ = 8 &  $S$ = 16  &   $S$ = 32     \\\\ \n\t\t\\midrule\n\t\tTimeGrad \\cite{rasul2021autoregressive} &  9.58   &   128.40    & 672.12      \\\\\n        DiffSTG ($M$=100, $k$=1)  &  0.24  &   0.48    & 0.95    \\\\\n        DiffSTG ($M$=40, $k$=1)  &  0.12  &   0.20    & 0.71    \\\\\n        DiffSTG ($M$=40, $k$=2)  &  0.07  &   0.12    & 0.21   \\\\\n        CSDI   &  0.51  &   0.88    & 1.82   \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\t\\label{tab:time_cost}\n\n\\end{table}\n\n\\textbf{Visualization.} We plot the predicted distribution of different methods to investigate their performance intuitively. \n We choose the AIR-GZ and PEMS08 for demonstration, and more examples on other datasets can be found in Appendix~\\ref{appendix:case_sdudy}. We have the following observations: 1) Figure~\\ref{fig:case_studys}(a) shows that DiffSTG can capture the data distribution more precisely than DeepAR; 2) in Figure~\\ref{fig:case_studys}(b), where the predictions of both DeepAR and DiffSTG cover the observations, DiffSTG provides a more compact prediction interval, indicating the ability to provide more reliable estimations. 3) Note that the model also needs to learn to reconstruct the history in the loss of Eq.~(\\ref{eq:masked_condition_loss}), we also illustrate the model's capability in history reconstruction in Figure~\\ref{fig:case_studys}(c); 4) Figure~\\ref{fig:case_studys}(d) draws the prediction result by a deterministic method STGCN \\cite{STGCN-2018} and DiffSTG. In the red box of Figure~\\ref{fig:case_studys}(d), the deterministic method fails to give an accurate prediction. In contrast, our DiffSTG model renders a bigger area (which covers the ground truth) in that region, indicating that the data therein is coupled with higher uncertainties. Such ability to accurately provide uncertainty can be of great help for practical decision-making; 5) Moreover, as shown in Figure~\\ref{fig:case_studys}(e), we illustrate the estimated distribution of DiffSTG on three stations, to illustrate its spatial dependency learning ability. Compared with station 29, the estimated distribution of station 4 is more similar to station 1, which is reasonable because the air quality of a station has stronger connections with its nearby neighbors. Equipped with the proposed denoising network UGnet, the model is able to capture the ST correlations, leading to more reliable and accurate estimation.  \n \n\n\\subsection{Ablation Study}\n\nWe conduct an ablation study on the AIR-GZ dataset to verify the effect of each component. We design three variants of DiffSTG and compare them with the full version of DiffSTG. The differences between these four models are described as follows:\n\n\\begin{itemize}\n    \\item  \\textbf{(w/o Spatial)}: removing the GNN component in UGnet, i.e., without modeling the spatial correlation.\n    \\item \\textbf{(w/o Temporal)}: removing the TCN component in UGnet, i.e., without modeling the temporal correlation.\n    \\item \\textbf{w/o U-structure}: removing the Unet-based structure in UGnet and only using one TCN for feature extraction.\n    \\item \\textbf{DiffSTG}: the final version of the proposed model.\n\\end{itemize}\n\n\\begin{figure}[!b]\n    \\centering\n    \\includegraphics[width=1\\linewidth]{img/ablation.pdf} \n    \\caption{Ablation study.}\n    \\label{fig:ablation}\n\\end{figure}\n\n \\begin{figure*}[!t]\n    \\centering\n    \\includegraphics[width=1 \\linewidth]{img/case_study/case_all.pdf} \n    \\caption{Example of probabilistic spatio-temporal graph forecasting for air quality and traffic dataset.}\n    \\label{fig:case_studys}\n\\end{figure*}\n\n\\par Figure \\ref{fig:ablation} illustrates the results. We have the following observations: Firstly, removing the spatial dependency learning in UGnet (w/o Spatial) brings considerable degeneration, which validates the importance of modeling spatial correlations between nodes. Secondly, when turning off the temporal dependency learning in UGnet (w/o Temporal), the performance drops significantly on all evaluation metrics. Thirdly, when detaching the Unet-based structure in UGnet, the performance degrades dramatically, which demonstrates the merits of an Unet-based structure in capturing ST-dependencies at different granularities.\n\n\\subsection{Hyperparameter Study} \\label{sec:hyperparameter_study}\n\\par In this section, we examine the impact of several crucial hyperparameters on DiffSTG. Specifically, we report i) the performance on AIR-GZ under different variance schedules (i.e., the combination of $\\beta_N$ and diffusion step $N$) and hidden size $C$. ii) influence of kernel size in TCN; iii)  the number of generated samples $S$ and the number of utilized samples $k$ in the proposed  sampling strategy.\n\n\\par \\textbf{Effect of the Variance Schedule and Hidden Size.} For different variance schedules $\\{\\beta_1, \\dots, \\beta_N\\}$, we set $\\beta_1 = 0.0001$ and let $\\beta_N$ and $N$ be from two search spaces, where $N \\in [50, 100, 200]$ and $\\beta_N \\in [0.1, 0.2, 0.3, 0.4]$. A variance schedule can be specified by a combination of $\\beta_N$ and $N$. The results are shown in Figure~\\ref{fig:hp_sen}. We note that the performance deteriorates rapidly when $N=50$ and $\\beta_N=0.1$. In this case, the result of the forward process is far away from a Gaussian distribution. Consequently, the reverse process starting with Gaussian distribution becomes an inaccurate approximation, which heavily injures the model performance. When $N$ gets larger, there is a higher chance of getting a promising result.\n\n\\begin{figure}[htbp]\n    \\centering\n    \\includegraphics[width=\\linewidth]{img/hp_sen.pdf} \n    \\vspace{-2em}\n    \\caption{Influence of hyperparameters.}\n    \\label{fig:hp_sen}\n\\end{figure}\n\n\\par Figure~\\ref{fig:hp_sen} shows the results of DiffSTG with $N=100$, and $\\beta_N=0.4$ vs. different hidden size $C$, from which we observe that the performance first slightly increases and then drops with the increase in hidden size. Compared with the variance schedule, the model's performance is much less sensitive to the hidden size. \n\n\\par \\textbf{Influence of Kernel Size in TCN.} We report the influence of kernel size ($K =2,3,4,5$) in TCN as in Table~\\ref{tab:tcn_influence}. Overall, we can see that the kernel size in TCN has a small impact on the performance of the model. In particular, a kernel size of 3 appears to be a good choice when the input and prediction horizon is 12.\n\n\\begin{table}[h]\n    \\caption{Influence of the kernel size $K$ in TCN on AIR-GZ ($T_h=12, T_p=12, N = 100, \\beta_N = 0.1$).}\\vspace{-.5em}\n    \\centering\n    \\scriptsize\n    \\setlength\\tabcolsep{4pt}\n    \\resizebox{0.75 \\linewidth}{!}{\n\t\\begin{tabular}{ccccc}\n\t\t\\toprule\n\t\tMetric & $K$=2 &  $K$=3  &  $K$=4 & $K$= 5    \\\\ \n\t\t\\midrule\n\t\tMAE &  13.85   &   13.38   & 14.02 & 14.86      \\\\\n        RMAE &  19.73   &   19.25    & 19.95 & 20.99      \\\\\n        CRPS &  0.27   &   0.26    & 0.28 & 0.29      \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n        }\n\t\\label{tab:tcn_influence}\n  \\vspace{-.5em}\n\\end{table}\n\n\\textbf{Effect of the Number of Generated Samples $S$.} We investigate the relationship between the number of samples $S$ and the performance in Figure~\\ref{fig:n_sample_and_k}. It shows the effect of probabilistic forecasting, as well as the effect on deterministic forecasting. From the figure, we can see that five or ten samples are enough to estimate good distributions. While increasing the number of samples further improves the performance, the improvement becomes marginal over 32 samples.\n\n\\textbf{Effect of $k$.} Recall that $k$ is the number of utilized samples in the last few diffusion steps when sampling $S$ samples. We provide the results of $k=1$ and $k=2$ in Figure~\\ref{fig:n_sample_and_k}, in which we have several interesting observations: i) When $S$ is large enough (i.e., $S>32$), the performance of $k=2$ is almost the same as $k=1$, and the sample speed of $k=2$ is 1.5 times faster than $k=1$; ii) When the number of reverse diffusion processes (i.e., $S/k$) is settled, a large $k$ can increase sample diversity thus leading to better performance, especially when $S$ is small.\n\n\\par In light of the above results, we give the following recommendations for the combination of $S$ and $k$: 1) when $S$ is small, a small $k$ is recommended to increase the sample diversity for better performance; 2) when $S$ is large, one can increase $k$ for efficiency without much lose of the performance.\n\n\\begin{figure}[!t]\n    \\centering\n    \\includegraphics[width=1 \\linewidth]{img/n_sample.pdf} \n    \\vspace{-1em}\n    \\caption{The effect of the number of generated samples and the influence of $k$.}\n    \\label{fig:n_sample_and_k}\n\\end{figure}\n\n\\subsection{Limitations}\n\\par Though promising in probabilistic prediction, DiffSTG still has a performance gap compared with current state-of-the-art STGNNs in terms of deterministic forecasting. Table \\ref{tab:results_determinstic} shows the deterministic prediction performance of DiffSTG (by averaging 8 in generate samples) and four deterministic methods, including DCRNN \\cite{li2018diffusion}, STGCN \\cite{STGCN-2018},  STGNCDE \\cite{choi2022graph}, and GMSDR \\cite{liu2022msdr}. While DiffSTG is competitive with most probabilistic methods, it is still inferior to the state-of-the-art deterministic methods. Different from deterministic methods, the optimization goal of DiffSTG is derived from a variational inference perspective (see details in Appendix.~\\ref{appendix:DDPM_detail}), where the learned posterior distribution might be inaccurate when the data samples are insufficient. We have similar observations in other DDPM-based models, such as TimeGrad and CSDI, as shown Table~\\ref{tab:results}. \n\\par Though, we would like to emphasize a key advantage of probabilistic such as DiffSTG, is its ability to provide probabilistic forecasting. This is the starting point of our focus and an important aspect that even the SOTA GNN-based methods may not be able to offer. We leave improving DiffSTG to surpass those deterministic methods in future work.\n\n\\begin{table}[htbp]\n\n\\centering\n\\small\n\\caption{Comparison with deterministic methods. Lower MAE and RMSE indicate better performance.}\n\\setlength\\tabcolsep{6 pt}\n\n\\begin{tabular}{c|cc|cc|cc}\n\\hline %\\hline\n\\multirow{2}{*}{Method}\n& \\multicolumn{2}{|c|}{ AIR-BJ}& \\multicolumn{2}{|c|}{ AIR-GZ }& \\multicolumn{2}{|c}{ PEMS08 }\\\\\n\\cline{2-7}\n & MAE & RMSE & MAE & RMSE & MAE & RMSE\\\\\n\\hline \\hline\nDCRNN & \\underline{16.99} & \\textbf{28.00} & \\underline{10.23} & \\underline{15.21} & 18.56 & 28.73 \\\\ \\hline\nSTGCN & 19.54 & 30.51 & 11.05 & 16.54 & 20.15 & 30.14 \\\\ \\hline\nSTGNCDE & 19.17 & 29.56 & 10.51 & 16.11 & \\textbf{15.83} & \\underline{25.05} \\\\ \\hline\nGMSDR & \\textbf{16.60} & \\underline{28.50} & \\textbf{9.72} & \\textbf{14.55} & \\underline{16.01} & \\textbf{24.84} \\\\ \\hline\nDiffSTG & 17.88 & 29.60 & 11.04 & 16.75 & 17.68 & 27.13 \\\\ \\hline\n\\end{tabular}\n\\label{tab:results_determinstic}\n\\end{table}\n\n\\section{Related Work}\\label{sec:related}\n\n\\par  \\textbf{Spatio-temporal Graph Forcasting.} Recently, a large body of research has been studied on spatio-temporal forecasting in different scenarios, such as traffic forecasting \\cite{STGCN-2018, GraphWaveNet-2019,  guo2021learning, peng2020spatial, ji2022stden} and air quality forecasting \\cite{liang2022airformer}. STGNNs have become dominant models in this field, which combine GNN and temporal components (e.g., TCN and RNN) to capture the spatial correlations and temporal features, respectively. However, most existing works focus on point estimation while ignoring quantifying the uncertainty of predictions. To fill this gap, this paper develops a conditional diffusion-based method that couples the spatio-temporal learning capabilities of STGNNs with the uncertainty measurements of diffusion models.\n\n\\par \\textbf{Score-based Generative Models.} The diffusion model that we adopt belongs to score-based generative models (please refer to Section~\\ref{sec:ddpm} for more details), which learn the gradient of the log-density with respect to the inputs, called Stein Score function \\cite{hyvarinen2005estimation, vincent2011connection}. At inference time, they use the gradient estimate to sample the data via Langevin dynamics \\cite{song2019generative}. By perturbing the data through different noise levels, these models can capture both coarse and fine-grained features in the original data. Which, leads to their impressive performance in many domains, such as image  \\cite{ho2020denoising}, audio \\cite{kong2020diffwave, chen2020wavegrad}, graph \\cite{niu2020permutation} and time series \\cite{rasul2021autoregressive, tashiro2021csdi}. \n\n\\par \\textbf{Time Series Forecasting.}  Methods in time series forecasting can be classified into two streams. The first one is deterministic methods, including transformer-based approaches \\cite{wen2022tstransformers, zhou2021informer, zhou2022fedformer} and RNN-based models \\cite{che2018recurrent}. The second one is probabilistic methods \\cite{pal2021rnn, salinas2020deepar}, which aims to provide stochastic predictions for future time series. A rising trend in this stream is leveraging the diffusion-based models to improve the performance of probabilistic predictions \\cite{rasul2021autoregressive, wang2023diffload, li2023generative, chang2023tdstf}. Though those methods have achieved promising performance, they still lack the ability to model the spatial correlations of different nodes if applied to STG forecasting. The above limitation also  motivates us to develop a more powerful diffusion-based model for STG that can effectively capture spatial correlations. \n\n\\section{Conclusion and Future Work}\\label{sec:conclusion}\n\n\\par In this paper, we first propose a novel probabilistic framework called DiffSTG for spatio-temporal graph forecasting. To the best of our knowledge, this is the first work that generalizes the DDPM to spatio-temporal graphs. DiffSTG combines the spatio-temporal learning capabilities of STGNNs with the uncertainty measurements of diffusion models. Unlike previous diffusion-based models designed for the image or sequential data, we devise the first denoising network UGnet for capturing the spatial and temporal correlations in STG data. By leveraging an Unet-based architecture to capture multi-scale temporal dependencies and a Graph Neural Network (GNN) to model spatial correlations, UGnet offers a powerful new member to the DDPM denoising network family, tailored for STG data.  Extensive experiments demonstrate the effectiveness and efficiency of our proposed method. \n\\par Furthermore, we accelerate the training and inference speed by elaborately designed  non-autoregressive prediction architecture and a sample acceleration strategy. This improvement in efficiency greatly benefits the potential of diffusion-based methods for STG forecasting tasks, making them more viable and appealing for practical applications. \n\n\\par Some interesting directions are worth exploring in the future. Since we only use the vanilla GCN in the denoising network for the model's simplicity, it remains open to incorporating more powerful graph neural networks to better capture the ST dependencies in the data. Another direction is to apply DiffSTG to other spatio-temporal learning tasks, such as spatio-temporal graph imputation.\n\n\\newpage\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2106.10121v1.tex",
        "arXiv-2301.03028v1.tex",
        "arXiv-2301.13629v4.tex"
    ],
    "group_id": "group_86",
    "response": "### Summary of Research Papers on Probabilistic Time Series and Spatio-Temporal Graph Forecasting\n\n#### Title: Advances in Probabilistic Forecasting with Generative Models for Time Series and Spatio-Temporal Graphs\n\n#### Introduction\nThe field of time series and spatio-temporal graph forecasting has seen significant advancements in recent years, driven by the increasing complexity of systems and the need for predictive analytics in various domains such as transportation, environmental monitoring, and financial markets. Traditional forecasting methods, including vector autoregression (VAR) models and Gaussian processes (GP), have been well-established but often struggle with capturing the nonlinear dependencies and uncertainties inherent in real-world data. With the advent of deep learning, particularly generative models like variational autoencoders (VAEs) and normalizing flows (NFs), researchers have been able to model more complex distributions and provide probabilistic forecasts. However, these models often face limitations in terms of functional form constraints and sensitivity to hyperparameters, which can hinder their practical application.\n\nRecent works have explored the use of energy-based models (EBMs) and denoising diffusion probabilistic models (DDPMs) to address these issues. EBMs are unnormalized probabilistic models that estimate the unnormalized negative log-probability, offering a more flexible approach to modeling data distributions. DDPMs, on the other hand, gradually add noise to the data and then denoise it to generate samples, which can be adapted for time series forecasting. The challenge remains in effectively integrating these models with sequential and spatio-temporal dependencies to achieve robust and accurate probabilistic predictions.\n\nIn this summary, we examine three recent papers that propose novel frameworks for probabilistic forecasting using generative models. The first paper introduces ScoreGrad, a continuous energy-based generative model for multivariate time series forecasting. The second paper presents D3VAE, a bidirectional variational autoencoder (BVAE) equipped with coupled diffusion probabilistic models (CDMs) and denoising score matching (DSM) for time series forecasting. The third paper, DiffSTG, generalizes DDPMs to spatio-temporal graphs (STGs) for probabilistic forecasting, incorporating a denoising network (UGnet) to capture both spatial and temporal dependencies.\n\n#### Main Content of Each Paper\n\n**ScoreGrad: Multivariate Probabilistic Time Series Forecasting with Continuous Energy-based Generative Models**\n\nScoreGrad proposes a framework for multivariate time series forecasting using continuous energy-based generative models (EBMs). The framework consists of two main modules: a time series feature extraction module and a conditional stochastic differential equation (SDE) based score matching module. The time series feature extraction module uses recurrent neural networks (RNNs) to capture temporal dependencies, while the score matching module models the data distribution through continuous SDEs. ScoreGrad's key innovation is the use of continuous SDEs, which allows for a more flexible and robust training process compared to discrete models. The training objective is to minimize the discrepancy between the gradient of the log-density of the data distribution and the estimated distribution, as shown in Equation \\ref{sm_obj}. The sampling process involves predictor-corrector (PC) methods, which can be adapted to different numerical solvers for SDEs. ScoreGrad is evaluated on six real-world datasets, achieving state-of-the-art results in terms of Continuous Ranked Probability Score (CRPS) and demonstrating robustness to hyperparameters.\n\n**D3VAE: Generative Time Series Forecasting with Diffusion, Denoise, and Disentanglement**\n\nD3VAE introduces a bidirectional variational autoencoder (BVAE) equipped with coupled diffusion probabilistic models (CDMs) and denoising score matching (DSM) for time series forecasting. The framework aims to reduce the aleatoric uncertainty of the time series by augmenting both the input and output series with noise. The BVAE is used to infer the latent variables, and the DSM network is employed to denoise the generated target series. Additionally, the latent variables are disentangled to enhance interpretability and reliability. D3VAE is evaluated on synthetic and real-world datasets, showing significant improvements over existing methods in terms of CRPS and Mean Squared Error (MSE). The model's performance is robust to the number of diffusion steps, and it provides effective uncertainty estimation through the noise level parameter $\\omega$. The framework's unique design allows it to capture complex temporal dependencies and reduce overfitting issues, making it particularly suitable for short time series scenarios.\n\n**DiffSTG: Probabilistic Spatio-Temporal Graph Forecasting with Denoising Diffusion Models**\n\nDiffSTG generalizes DDPMs to spatio-temporal graphs (STGs) for probabilistic forecasting, introducing a novel denoising network called UGnet. UGnet combines a U-net architecture with Graph Neural Networks (GNNs) to capture both multi-scale temporal dependencies and spatial correlations. The framework is non-autoregressive, allowing for efficient generation of multi-horizon predictions. DiffSTG is evaluated on three real-world datasets, demonstrating superior performance in terms of CRPS and RMSE compared to existing probabilistic methods. The model's performance is sensitive to the variance schedule and hidden size, but it offers a significant speed-up in inference time compared to other diffusion-based models. DiffSTG's ability to provide probabilistic forecasts and capture ST dependencies makes it a promising approach for practical applications in spatio-temporal forecasting.\n\n#### Commonalities and Innovations\nAll three papers leverage generative models to address the challenges of probabilistic forecasting. They aim to capture both temporal and spatial dependencies, which are crucial for accurate predictions in complex systems. ScoreGrad and D3VAE focus on time series forecasting, while DiffSTG extends the approach to spatio-temporal graphs. The common theme is the use of diffusion processes to augment data and improve model robustness. ScoreGrad innovates by using continuous SDEs, which offer more flexibility and robustness in the training process. D3VAE introduces a bidirectional architecture and disentangles latent variables to enhance interpretability and reliability. DiffSTG proposes a novel denoising network (UGnet) that combines U-net and GNN architectures to effectively model ST dependencies, making it the first DDPM-based model for STG forecasting.\n\n#### Results Comparison\nThe performance of the proposed models is evaluated using CRPS, MSE, and RMSE metrics. ScoreGrad achieves state-of-the-art results on six real-world datasets, demonstrating its effectiveness in modeling multivariate time series. D3VAE outperforms existing probabilistic methods on synthetic and real-world datasets, showing significant improvements in CRPS and MSE. DiffSTG reduces CRPS by 4\\%-14\\% and RMSE by 2\\%-7\\% on three real-world datasets, indicating its superior performance in probabilistic forecasting. However, DiffSTG's deterministic forecasting performance is still inferior to some state-of-the-art deterministic methods, highlighting the trade-off between probabilistic accuracy and deterministic precision.\n\n#### Conclusion\nThe three papers present innovative approaches to probabilistic forecasting using generative models. ScoreGrad introduces continuous SDEs for more flexible and robust training, achieving state-of-the-art results on multivariate time series datasets. D3VAE combines diffusion processes and disentanglement to enhance interpretability and reliability, demonstrating significant improvements on synthetic and real-world datasets. DiffSTG generalizes DDPMs to spatio-temporal graphs, introducing a novel denoising network to capture both temporal and spatial dependencies, and achieving superior probabilistic forecasting performance. Future research directions include incorporating more powerful graph neural networks into DiffSTG and applying the framework to other spatio-temporal learning tasks such as graph imputation. Additionally, further exploration of the impact of hyperparameters and the development of more efficient sampling strategies could enhance the practicality of these models.\n\n#### Tables for Comparison\nBelow are tables summarizing the performance of the models on different datasets:\n\n**ScoreGrad Performance on Real-World Datasets**\n\\begin{table}[!t]\n\\caption{Performance of ScoreGrad on six real-world datasets (lower is better).}\n\\label{table_3}\n\\centering\n\\begin{tabular}{|c|cccccc|}\n\\hline\n\\diagbox{Method}{Dataset}&Exchange&Solar&Electricity&Traffic&Taxi&Wikipedia\\\\\n\\hline\nScoreGrad (VP SDE)&0.006$\\pm$0.001&0.256$\\pm$0.015&0.0194$\\pm$0.001&0.041$\\pm$0.004&0.101$\\pm$0.004&0.043$\\pm$0.002\\\\\nScoreGrad (VE SDE)&0.007$\\pm$0.001&0.277$\\pm$0.011&0.0199$\\pm$0.001&0.037$\\pm$0.003&0.104$\\pm$0.009&0.046$\\pm$0.002\\\\\nScoreGrad (sub-VP SDE)&0.006$\\pm$0.001&0.268$\\pm$0.021&0.0192$\\pm$0.001&0.043$\\pm$0.004&0.102$\\pm$0.006&0.041$\\pm$0.003\\\\\n\\hline\n\\end{tabular}\n\\end{table}\n\n**D3VAE Performance on Real-World Datasets**\n\\begin{table}[t]\n   \\begin{minipage}[b]{0.63\\textwidth}\n    \\centering\n    \\small\n    \\caption{\n    Ablation study of the coupled diffusion probabilistic model w.r.t. MSE and CSPR. \n    } \\label{tab:abla:diffusion}\n    \\renewcommand{\\arraystretch}{1.0}\n    \\setlength\\tabcolsep{0.8pt}\n    \\begin{threeparttable}\n    \\begin{tabular}{c|cc|cc}\n    \\toprule\n     \\multirow{2}{*}{Dataset}& \\multicolumn{2}{c|}{Traffic} & \\multicolumn{2}{c}{Electricity}\\\\\n     \\cline{2-5}\n     ~&16&32&16&32\\\\\n    \\midrule\n    D3VAE (Full Model) & \\textbf{0.081}_{\\pm.009} & \\textbf{0.091}_{\\pm.007} & \\textbf{0.308}_{\\pm.030} & \\textbf{0.410}_{\\pm.075} \\\\\n    \\end{tabular}\n    \\end{threeparttable}\n    \\label{tab:my_label}\n    \\end{minipage}\n\\hfill\n\\begin{minipage}[p]{0.35\\textwidth}\n    \\centering\n    \\begin{subfigure}[t]{\\textwidth}\n     \\includegraphics[width=0.9\\textwidth,height=0.54\\textwidth]{fig/figure9/end6.png}\n    \\end{subfigure}\n    \\vspace{1ex}\n     \\begin{subfigure}[t]{\\textwidth}\n     \\includegraphics[width=0.9\\textwidth,height=0.54\\textwidth]{fig/figure9/step5.png}\n    \\end{subfigure}\n    \\captionof{figure}{\n      Comparisons of predictions with different $\\beta_{T}$ and varying $T$ on the Electricity dataset.\n    }\n    \\label{fig:beta}\n\\end{minipage}\n\\vspace{-3ex}\n\n**DiffSTG Performance on Real-World Datasets**\n\\begin{table}[htbp]\n\n\\centering\n\\small\n\\caption{Comparison with deterministic methods. Lower MAE and RMSE indicate better performance.}\n\\setlength\\tabcolsep{6 pt}\n\n\\begin{tabular}{c|cc|cc|cc}\n\\hline\n\\multirow{2}{*}{Method}\n& \\multicolumn{2}{|c|}{ AIR-BJ}& \\multicolumn{2}{|c|}{ AIR-GZ }& \\multicolumn{2}{|c}{ PEMS08 }\\\\\n\\cline{2-7}\n & MAE & RMSE & MAE & RMSE & MAE & RMSE\\\\\n\\hline \\hline\nDCRNN & \\underline{16.99} & \\textbf{28.00} & \\underline{10.23} & \\underline{15.21} & 18.56 & 28.73 \\\\ \\hline\nSTGCN & 19.54 & 30.51 & 11.05 & 16.54 & 20.15 & 30.14 \\\\ \\hline\nSTGNCDE & 19.17 & 29.56 & 10.51 & 16.11 & \\textbf{15.83} & \\underline{25.05} \\\\ \\hline\nGMSDR & \\textbf{16.60} & \\underline{28.50} & \\textbf{9.72} & \\textbf{14.55} & \\underline{16.01} & \\textbf{24.84} \\\\ \\hline\nDiffSTG & 17.88 & 29.60 & 11.04 & 16.75 & 17.68 & 27.13 \\\\ \\hline\n\\end{tabular}\n\\label{tab:results_determinstic}\n\\end{table}\n\n#### Discussion of Results\nScoreGrad's performance is consistently superior on six real-world datasets, demonstrating its robustness and flexibility in handling multivariate time series. D3VAE achieves remarkable improvements in CRPS and MSE on synthetic and real-world datasets, highlighting its effectiveness in reducing aleatoric uncertainty and enhancing interpretability. DiffSTG's performance is notable on three real-world datasets, showing significant reductions in CRPS and RMSE compared to existing probabilistic methods. However, DiffSTG's deterministic performance is still inferior to some state-of-the-art deterministic methods, indicating a potential area for improvement.\n\n#### Conclusion\nThe three papers collectively advance the field of probabilistic forecasting by introducing novel generative models that effectively capture temporal and spatial dependencies. ScoreGrad's use of continuous SDEs offers a more flexible training process, while D3VAE's bidirectional architecture and disentanglement enhance interpretability. DiffSTG's extension to spatio-temporal graphs and its novel denoising network (UGnet) provide a powerful tool for capturing complex ST dependencies. Future research should focus on improving the deterministic performance of DiffSTG and exploring more efficient sampling strategies for all models. Additionally, incorporating more advanced graph neural networks into DiffSTG could further enhance its performance and applicability.\n\n#### Future Research Directions\n1. **Improving Deterministic Performance**: Further research should aim to enhance the deterministic forecasting capabilities of DiffSTG to match or surpass those of state-of-the-art deterministic methods.\n2. **Efficient Sampling Strategies**: Developing more efficient sampling methods for ScoreGrad and D3VAE could reduce computational costs and improve practicality.\n3. **Advanced Graph Neural Networks**: Integrating more sophisticated graph neural networks into DiffSTG could better capture the intricate dependencies in STG data.\n4. **Application to Other Tasks**: Extending the framework to other spatio-temporal learning tasks, such as graph imputation, could broaden the applicability of these models.\n5. **Hyperparameter Optimization**: Investigating the impact of hyperparameters on model performance could lead to more robust and adaptable generative models.\n\nThese advancements and future directions promise to further refine the capabilities of generative models in probabilistic forecasting, making them more versatile and reliable tools for real-world applications."
}