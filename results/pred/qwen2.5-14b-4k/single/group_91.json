{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{A Diagnostic Benchmark for Very Long-form Video Language Understanding}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\nWe introduce \\name{}{}, a very long-form video question-answering dataset, and benchmark to evaluate long video understanding capabilities of modern vision and language systems. \nDerived from Ego4D, \\name{}{} consists of over $5000$ human curated multiple choice question answer pairs, spanning over 250 hours of real video data, covering a very broad range of natural human activity and behavior. \nFor each question, \\name{}{} requires the correct answer to be selected between five given options based on a three-minute-long video clip. \nWhile some prior works have proposed video datasets with long clip lengths, we posit that merely the length of the video clip does not truly capture the temporal difficulty of the video task that is being considered.\nTo remedy this, we introduce temporal certificate sets, a general notion for capturing the intrinsic temporal understanding length associated with a broad range of video understanding tasks \\& datasets. \nBased on this metric, we find \\name{}{} to have intrinsic temporal lengths over $5.7\\times$ longer than the second closest dataset and $10\\times$ to $100\\times$ longer than any other video understanding dataset. \nFurther, our evaluation of several current state-of-the-art video and language models shows them to be severely lacking in long-term video understanding capabilities. Even models with several billions of parameters achieve QA accuracy less than 33\\% (random is 20\\%) on the \\name{}{} multi-choice question answering task, while humans achieve about 76\\% accuracy.   \nWe posit that \\name{}{}, with its long intrinsic temporal structures and diverse complexity, would serve as a valuable evaluation probe for developing effective long-term video understanding systems in the future.  \nData and Zero-shot model evaluation code are open-sourced under the Ego4D license at \\href{http://egoschema.github.io}{\\texttt{egoschema.github.io}}.  \n\\end{abstract}\n\n\\begin{figure}[h!]\n    \\centering\n    \\includegraphics[width = \\textwidth]{figures/teaser_final.pdf}\n    \\caption{\\textbf{The \\name{}{} dataset} contains over 5000 very long-form video language understanding questions spanning over 250 hours of real, diverse, and high-quality egocentric video data. Each question requires choosing the correct answer out of five choices based on a \\textit{three minute} long video clip. The questions are manually curated to require very long \\textit{temporal certificates} (\\sref{sec:certificate_definition}). \\name{}{} median certificate length is about $100$ seconds, which is $5\\times$ longer than the closest second dataset and $10\\times$ to $100\\times$ longer (\\figref{fig:certificateplot}) than any other video understanding dataset. State-of-the-Art video-language models consisting of billion of parameters achieve very low accuracy (< 33\\%) in Zero-shot evaluation (random is 20\\%) while humans achieve about 76\\%. \\texttt{`C'} refers to the camera wearer. Visualized clips are available at \\href{http://egoschema.github.io/explorer.html}{\\texttt{egoschema.github.io/explorer}}.}\n    \\label{fig:bigteaser}\n\\end{figure}\n\\section{Introduction}\n\nWe introduce\\name{}{}, a diagnostic benchmark for assessing very long-form video-language understanding capabilities of modern multimodal systems. \nUnderstanding long natural videos requires a host of interconnected abilities such as action and scene understanding, perceiving and tracking object states, long-term visual memory, abstract reasoning, hierarchical information aggregation, and more. Shown in \\figref{fig:bigteaser} is an exemplar of the curated\\name{}{} dataset. Consider the visual cognitive faculties involved in answering the question: \\texttt{`What is the overarching behavior of C and the man in the video?'}. First, is the spatial recognition capabilities for disambiguating the referred character \\texttt{`C'} (camera wearer) and \\texttt{`the man'} as well as the present objects such as \\texttt{`cards'}, \\texttt{`notebook'}, \\texttt{deck} as so on. Next is short-term temporal recognition capabilities of understanding the atomic actions and movement of the characters such as \\texttt{`playing'}, \\texttt{`taking notes'}, \\texttt{`shuffling'} etc. Built upon these are the capabilities for visually understanding the mental states such \\texttt{`distracted'}, \\texttt{`attention'} and social dynamics such as \\texttt{`teaching'}, \\texttt{`showing'}. Next are medium-term actions such as \\texttt{`organizing the deck'} or \\texttt{`keeping track'}. Finally, long-term reasoning capabilities need to be employed for abstracting the \\texttt{`overarching behavior'} of the video from all the low-level signals to be able to rule out all the other wrong options and conclude option \\texttt{3} to be correct. Note that even for humans, it is impossible to answer the illustrated questions with only the shown 9 uniformly sampled frames from the three-minute video (\\figref{fig:bigteaser}).             \n\nWhile there have been some prior attempts to formulate long-form video tasks~\\cite{lvu, mad}, they broadly tend to fall into two failure modes. The first failure mode stems from the difficulty of capturing the explosive diversity of human behavior in narrow pre-defined label spaces that leading unduly narrow and oddly specific tasks, such as like ratio or relationship prediction~\\cite{lvu}. Hence, we propose to probe video systems capturing the rich complexity of long-form video with something just as rich and complex -- natural language. However, natural language outputs are notoriously difficult to evaluate with popular metrics such as BLEU~\\cite{bleu} and ROUGE~\\cite{rouge} having well-known shortcomings~\\cite{bleu_role}. Hence, we propose to evaluate language understanding as a multiple-choice question-answering task, thereby using the well-defined benchmark metric of overall question-answering accuracy.\n\nThe second failure mode for a long-term video task is that the proposed task happens to actually be a short-term one - only disguised as a long-term task. To measure the intrinsic \"long-term\" nature of a video understanding task, we propose the notion of temporal \\textit{certificate length}~\\cite{complexitytheory}. Intuitively, certificate length (\\sref{sec:certificate_definition}) is the length of the video a human verifier needs to observe to be convinced of the veracity of the marked annotation. The idea of temporal certificates is not limited only to question-answering or vision-language tasks but is applicable to several video understanding tasks, including pure vision tasks such as action classification, detection, or even temporal action localization. \n\nBased on the length of the temporal \\textit{certificate}, we propose the following  temporal understanding taxonomy for video tasks: Datasets with certificate length in the order of $1$ second are termed short video tasks. Next, we name datasets with certificate length in the order of $10$ seconds as, long-form video tasks. Finally, datasets with certificate length in the order of $100$ seconds are termed as, very long-form video tasks. \\figref{fig:certificateplot} presents estimates of the certificate lengths for a variety of datasets plotted against the temporal length of the video clip. We observe that the temporal certificate length is quite weakly correlated with the length of the video clip. This is due to the intentional design choice in defining the certificate set, which decouples the task of searching or retrieving the relevant sub-clip from a bigger clip from the task of visually understanding the retrieved sub-clip. And in this manner, using temporal certificate length as a metric for measuring the intrinsic temporal hardness of a dataset, avoids the failure mode of formulating an implicitly short-term task disguised as a long-term one. Section \\ref{sec:certificate_definition} details precise operationalizations for estimating the temporal certificate sets.  \n\n\\begin{figure}[t!]\n\\centering\n\\begin{minipage}[t]{0.45\\linewidth}\n  \\includegraphics[width=\\linewidth]{figures/certificate_distribution.pdf}\n  \\caption{We introduce the notion of a temporal certificate set (top, \\sref{sec:certificate_definition}), a tool to measure the intrinsic temporal length of a benchmark and show the \\name{}{} certificate length distribution (bottom, \\sref{sec:eval_certificate}) for randomly chosen $100$ clips.}\n  \\label{fig:certificate_distribution}\n\\end{minipage}\n\\hspace{0.3cm}\n\\begin{minipage}[t]{0.45\\linewidth}\n  \\includegraphics[width=\\linewidth]{figures/certificates_cropped_2.pdf}\n  \\caption{\\textbf{Certificate Length across video datasets} for a broad spectrum of tasks such as action classification, detection, relationship classification, concept classification, video classification, and multiple choice question-answering. \\sref{sec:eval_certificate} details the precise operationalizations.}\n  \\label{fig:certificateplot}\n\\end{minipage}\n\\end{figure}\n\nIn summary, our contributions are three-fold. \\textit{First}, we propose the notion of temporal certificates, a broadly applicable notion that measures the intrinsic temporal hardness of clips in a video understanding dataset. We estimate temporal certificate lengths for a broad variety of existing datasets and show that\\name{}{} has a median temporal certificate of about $100$ seconds, which is $5\\times$ longer than the dataset with the second longest certificate length~\\cite{lvu}, and $25\\times$ to $100\\times$ longer than all other existing video understanding datasets (with or without language). \\textit{Second}, building upon the notion of temporal certificates, we introduce\\name{}{}, a diagnostic benchmark for assessing the very long-form video understanding capability of multimodal video-language systems. \\textit{Third}, we benchmark both state-of-the-art video-language systems and humans in Zero-shot settings on\\name{}{} to find that even the most advanced current video-language understanding systems consisting of billion of parameters achieve very low accuracy in long-from multiple-choice question-answering (< 33\\%) while humans achieve about $76\\%$ accuracy in the unconstrained setting. \n\\section{Related Works}\n\n\\begin{figure}[t!]\n\\center\n\\includegraphics[width = \\textwidth]{figures/pipeline_cropped.pdf}\n\\caption{\\name{}{} data pipeline. Stage I filters the suitable Ego4D RGB videos and narrations for question-answer generation (\\sref{sec:stage1}). Stage II uses narrations in a chained \\colorbox{llm}{LLM prompting} (\\sref{sec:stage2}) procedure to generate multiple $\\mathcal{QAW}$ triplets per three-minute video clip (\\sref{sec:stage2}). Stage III performs pre-filtering with \\colorbox{rule}{rule-based} and \\colorbox{llm}{LLM-based} logic (\\sref{sec:stage3}). Finally, Stage IV involves two rounds of \\colorbox{human}{human} curation on filtered $\\mathcal{QAW}$ for selecting very long-form video-language understanding data (\\sref{sec:stage4}). The stage width ratios are indicative of the filter selection ratios.}\n\\label{fig:pipeline}\n\\end{figure}\n\n\\noindent \\textbf{Video Question-Answering Datasets.}\nVisual Question-Answering~\\cite{vqa} is a popular video-language task with several large internet-scale datasets for video-language pre-training such as Ego4D~\\cite{ego4d}, HowTo100M~\\cite{howto100m} and HowToVQA69M~\\cite{ivqa}. However, as the scope and size of pre-training datasets and models soar, it becomes critical to construct evaluations for assessing the model capabilities on various axes. Hence, many smaller datasets have been proposed for evaluating different aspects of video-language understanding such as compositional reasoning~\\cite{agqa, agqa2}, causal and common scene comprehension~\\cite{nextqa}, instruction understanding~\\cite{ivqa, how2qa}, video description ability~\\cite{msrvtt}, dynamic environments understanding~\\cite{envqa}, complex web video understanding~\\cite{activitynetqa}, situated reasoning~\\cite{star}, spatiotemporal reasoning~\\cite{tgif}, social intelligence~\\cite{socialiq}, dynamic neuro-symbolic reasoning~\\cite{clevrer}, external knowledge-based reasoning~\\cite{knowit} and many more~\\cite{marioqa, youtube2textqa, asrl, fiber, wildqa, v2c, svqa, sutd, tvqa, tvqa+, lifeqa, pano, dramaqa, musicavqa, avqa, pstuts, knowitx, newskvqa}. How2VQA69M~\\cite{ivqa} and iVQA~\\cite{ivqa} have leveraged HowTo100M~\\cite{howto100m} ASR text for generating questions. However, unlike Ego4D narrations that are used in\\name{}{}, ASR text does not necessarily describe the visual elements in the scene. Hence, questions can suffer from biases where a key required information is visually absent. Additionally, generated question-answers also have quite short certificate lengths (iVQA in \\figref{fig:certificate_distribution}) due to the local nature of the ASR text.  \n\n\\noindent \\textbf{Long-form Video Understanding Datasets} have been very sparsely explored in prior works. \\cite{lvu} posits a long-form video understanding benchmark but the proposed tasks are unduly narrow and specific, such as the `like' ratio and view count prediction. Also, \\cite{lvu} average certificate length is about $5.7\\times$ smaller than \\name{}{}.\n\n\\cite{longstream} proposes a dataset for benchmarking efficient video inference consisting of frame-wise object mask annotations from Mask-RCNN~\\cite{maskrcnn} but without any long-term annotations. \\cite{KineticsGEBD} introduces a dataset of about 111 hours of video sourced from Kinetics-400~\\cite{kinetics400} for generic event boundary detection. While the task itself requires comprehensive understanding, the video clip length is only 10 seconds long, with temporal \\textit{certificates} (\\sref{sec:certificate_definition}) being much shorter. \\cite{movieqa} proposes a question-answering dataset based on long movie clips but due to the open-ended nature of questions, successful approaches tend to neglect the visual data and are biased purely with approaches using additional text such as story lines. \\cite{mad} proposes MAD, a language grounding dataset with an average clip of $110$ minutes. However, the length of the retrieved clip is quite short (average $4.1$ seconds) thereby resulting in a temporal \\textit{certificate} (\\sref{sec:certificate_definition}) only a few seconds long. Further, MAD~\\cite{mad} and several other movie-based datasets~\\cite{movienet, moviegraphs, msa} do not release any video data because of copyright issues. In contrast,\\name{}{} has an average certificate length of about $100$ seconds. Further,\\name{}{} will be publicly released under the Ego4D license, which allows direct public use of the video and text data for both research and commercial purposes.    \n\n\\section{Collecting \\name{}{}} \n\\label{sec:pipeline}\n\\begin{figure}[t!]\n\\center\n\\includegraphics[width = \\textwidth]{figures/prompts.pdf}\n\\caption{An abridged example of the generation and filtering prompts used in the \\name{}{} data generation pipeline (\\sref{sec:pipeline}). Full versions are provided in the \\textit{supplementary}.}\n\\label{fig:prompt}\n\\end{figure}\n\nCollecting video and language datasets, even without a focus on very long-form video is quite challenging. Manually collecting, observing, and annotating videos with free-form language, in contrast to using images and pre-defined label categories, is both labor-intensive and time-consuming and thereby quite expensive. In addition to burgeoning cost, ensuring visual data diversity and minimizing visual and linguistic bias while ensuring high quality of marked annotations also contribute to the overall difficulty. All these factors get severely more challenging for long-form videos. \n\nIn this work, we propose a staged data collection pipeline (\\figref{fig:pipeline}) utilizing existing large-scale but short-term video datasets, rule-based filtering procedures, and exciting new capabilities afforded by LLMs to significantly lighten the burden on human annotators. We use the proposed pipeline for curating \\name{}{}, a high-quality and diverse very long-form video question-answering dataset. Associated datasheets~\\cite{datasheets} and data cards~\\cite{pushkarna2022data} for \\name{}{} are provided in the \\textit{supplementary}. \n\\subsection{\\name{}{} Pipeline}\n\\subsubsection{Stage I: Raw Data Filtering}\n\\label{sec:stage1}\nEgo4D~\\cite{ego4d} has over 3670 hours of RGB video spread consisting of over 3.85 million narration instances covering over 1,772 unique verbs (activities) and 4,336 unique nouns (objects)~\\cite{ego4d}. The narrators are instructed to continuously pause and describe everything that the camera wearer (\\texttt{`C'}) does. This creates dense and precise narrations that accurately describe the visuals. \n\nNaturally, the collected video has non-uniform length and narration density. Since we would like to standardize the clip length for evaluation and have sufficiently rich narrations to allow interesting question-answer pairs to form in later stages, we filter the data based on the length and narration density. We choose to filter for non-overlapping three-minute clips each with at least 30 human annotated narrations (each narration is a timestamped sentence) to build \\name{}{}. Detailed statistic of the number of viable clips for different possible length and narration density choices is discussed in \\textit{supplementary}. \n\n\\subsubsection{Stage II: Question Answer Generation}\n\\label{sec:stage2}\nThe filtered narrations are processed with a capable LLM to generate $N$ Question-Answer triplets ($\\mathcal{QAW}$), each consisting of the question $\\mathcal{Q}$, the correct answer $\\mathcal{A}$, and $M$ wrong answers $\\mathcal{W}$, per clip. To achieve this, we experimented with several LLM inference call chaining procedures with trade-offs between quality and cost of generation that are briefly described next. \n\n\\noindent \\textbf{One-shot} is the simplest prompting procedure to prompt for all $N$ instances of $\\mathcal{QAW}$ in one inference call. This is the most cost-efficient option but we found the generations to be of significantly low quality. The generated $\\mathcal{Q}$ often are very similar to each other and the generated $\\mathcal{AW}$ have a very high false positive rate for the correct answers as well as a false negative rate for the wrong answers.\n\n\\noindent \\textbf{N-shot} is the next natural prompting procedure where we generate one $\\mathcal{QAW}$ per LLM inference call. This significantly improves the false positive and false negative rates but since the generated $\\mathcal{Q}$ are independent and generated with the same prompt, they still tend to be very similar (comparable to one-shot), even at higher sampling temperatures. Further, the cost of generation also scales with $N$. \n\n\\noindent \\textbf{QAW-shot} generates each of the $N$ questions $\\mathcal{Q}$ in one inference call, followed by another inference call for generating $N$ correct answer $\\mathcal{A} | \\mathcal{Q}$ and finally, $N \\times M$ wrong answers, $\\mathcal{W} | \\mathcal{Q, A}$. Since each of the $N$ $\\mathcal{Q}$ is generated jointly, they can be forced to be distinct with appropriate prompting. Similarly, the generated $\\mathcal{A}$ and $\\mathcal{W}$ can also be made distinct. However, this requires 3 \\textit{chained} LLM inference calls, and generation failures in earlier calls cascade steeply. \n\n\\noindent \\textbf{Q(AW)-shot} generates each of the $N$ questions $\\mathcal{Q}$ in one inference call, followed by a final inference call for generating all the $N$ correct and $N \\times M$ incorrect answers in one go $\\mathcal{A, W} | \\mathcal{Q}$. It enjoys the same uniqueness properties as QAW-shot while having just two chained calls, making it both ~$30\\%$ cheaper and less prone to generation failure cascading. Further, between Q(AW)-shot and QAW-shot, we observe Q(AW)-shot to have a higher generated $\\mathcal{A}$ quality, perhaps since LLM can jointly model $\\mathcal{W}$ while generating $\\mathcal{A}$. We choose this to be our main method of choice for generating $\\mathcal{QAW}$. \n\n\\noindent \\textbf{Prompt} for imputing narrations into the LLM has a tremendous effect on the quality of generated $\\mathcal{QAW}$. We experiment with several seed prompts for each of which we inspect the quality of the $N$ generated $\\mathcal{QAW}$ for $10$ clips. Based on this we iteratively improve the seed prompts manually in a zeroth order optimization fashion. In total, we experiment with a total of about $85$ prompts in this fashion to arrive at our final \\name{}{} prompts -- $\\mathcal{P}_\\mathcal{Q}$ for generating $N \\times \\mathcal{Q}$ questions and $\\mathcal{P}_\\mathcal{AW}$ for generating all remaining options $\\mathcal{(AW)} | \\mathcal{Q}$. While we fix the $\\mathcal{P}_\\mathcal{Q}$ prompt, we use multiple $\\mathcal{P}_\\mathcal{AW}$ prompts so as to avoid any unintended bias in the options. \\figref{fig:prompt} shows an abridged example of $\\mathcal{P}_\\mathcal{Q}$ and $\\mathcal{P}_\\mathcal{AW}$, full versions available in \\textit{supplementary} material.  \n\n\\noindent \\textbf{Choice of LLM} is extremely crucial for obtaining interesting long-form $\\mathcal{Q}$ and generating hard negatives for $\\mathcal{W}$. With weaker LLMs, the $\\mathcal{Q}$ diversity across video clips remains narrow, and $\\mathcal{W}$ tends to be either obviously wrong or, too similar to $\\mathcal{A}$ and thus a false negative. While we experimented with both GPT-3~\\cite{gpt3} and ChatGPT~\\cite{chatgpt} but only found good quality generated $\\mathcal{QAW}$ at a high enough rate with GPT-4~\\cite{gpt4}, Bard~\\cite{bard}, and Claude~\\cite{claude}. For details please see \\textit{supplementary}. \n\nWe generate $N = 3$ questions per three-minute clip as well as $M = 4$ wrong answers to every question in addition to the correct answer. We observe that larger $N$ or $M$ tends to generate similar questions and wrong answers putting unnecessary pressure on Stages III and IV for filtering.\n\n\\subsubsection{Stage III: Generated Question Answer Filtering}\n \\label{sec:stage3}\n \nWhile Stage II produces several high-quality $\\mathcal{QAW}$, even the best LLM generations are prone to output format aberrations, hallucinations, and sometimes plain false outputs. Further, despite specific pinpointed prompts (\\figref{fig:prompt}), LLMs can fail to comply. Since, we want to ensure\\name{}{} to be extremely high-quality and accurate, we set up several filtering rounds to ensure the correctness and high difficulty of questions. \n\n\\noindent \\textbf{Rule-based filtering.} Keywords from the prompts such as \\texttt{`long-term'}, \\texttt{`narrations'}, \\texttt{`timestamp'} etc. can sometimes bleed into the generated $\\mathcal{QAW}$ which are then discarded. The output generations can also fail to parse according to a specified format and are also then discarded and the concerned $\\mathcal{QAW}$ is regenerated. \n\n\\noindent \\textbf{LLM-based filtering.}  While rule-based filtering weeds out logic errors, we would like to further enrich $\\mathcal{QAW}$ before employing human labor. For example, we aim to ensure\\name{}{} requires grounded visual reasoning to solve, and hence questions should not be answerable \\textit{ungrounded}, without carefully observing the video. Hence, we develop a \"blind\" baseline.\n\n\\noindent \\textbf{Blind filtering baseline} employs LLM to guess the correct answer based on the question, without having access to the video narrations conditioned on the shown filtering prompt (\\figref{fig:prompt}). All such ungrounded questions that can be answered blindly are filtered out. This also ensures that generated $\\mathcal{W}$ are indeed relevant and plausible answers to $\\mathcal{Q}$, since otherwise, the LLM would be able to guess $\\mathcal{A}$ based only on the setting of $\\mathcal{Q}$. Note that this is overly restrictive since it is possible that a question is guessed correctly through chance and is not necessarily ungrounded. However, we choose to optimize precision over recall since the amount of filtered $\\mathcal{QAW}$ is still large enough. \n\n\\noindent \\textbf{No-$\\mathcal{Q}$ baseline.} We also experimented with a No-$\\mathcal{Q}$ baseline, where the LLM is prompted to guess the correct answer using the narrations but without the question $\\mathcal{Q}$. This ensures that the wrong answers are relevant and plausible to the video clip. However, we found this baseline to have near random accuracy ($\\sim20\\%$), highlighting the efficacy of Stage II. Hence, we decided to not use this filter in the final pipeline.  Additional details including the full prompt are in \\textit{supplementary}. \n\n\\subsubsection{Stage IV: Manual $\\mathcal{QAW}$ Curation}\n\\label{sec:stage4}\nWhile LLM filtering ensures that the generated $\\mathcal{QA}$ relates to the video content, it's also necessary to ensure the veracity and a long temporal certificate length for every generated $\\mathcal{QAW}$. This is achieved through a two-step manual curation process. \n\nIn the first round of curation, annotators are tasked with three primary responsibilities: \\textbf{(A)} First, they verify that $\\mathcal{Q}$ is well-formed and $\\mathcal{A}$ is indeed the correct answer to $\\mathcal{Q}$. \\textbf{(B)} Next, they confirm that all the $M$ distractors, $\\mathcal{W}$, are indeed wrong answers to $\\mathcal{Q}$. \\textbf{(C) }Finally, they ensure that the temporal certificate length for answering $\\mathcal{Q}$ is at least 30 seconds. \n\nA $\\mathcal{QAW}$ is discarded if any of these three conditions are not met. This reduces the number of admissible questions by a factor of about $4\\times$ to $5\\times$ within the first round itself. Next is a second round of re-curation, to reinforce the conditions and guarantee data of the highest quality. We find that more than $97\\%$ of the questions that pass the first round also pass the second round, speaking to the efficacy of the curation process. A crucial aspect of ensuring that the question assesses very long-form video-language understanding capabilities is the notion of temporal certificate length (condition (C) above), which we describe next. The detailed procedures for onboarding and training the human annotators, as well as the instructions for the curation process are provided in the \\textit{supplementary}.\n\n\\subsection{Temporal Certificates}\n\\label{sec:certificate_definition}\n\nWe define the temporal \\textit{certificate} of a given video in a video understanding task to be the minimum set of \\textit{subclips} of the video that are both \\textit{necessary} and \\textit{sufficient} to convince a human verifier that the marked annotation for that data (such as timestamps in temporal activity localization, class label in activity recognition or, the correct option in multiple-choice question-answering) is indeed correct, without having to watch the rest of the clip outside of the certificate set (\\figref{fig:certificate_distribution}). Naturally, we define certificate length to be the sum of the temporal lengths of the sub-clips present in the certificate set. \n\n\\noindent \\textbf{Meta-rules.} Datasets often have implicit rules that apply uniformly across the entire dataset. We call these conventions meta-rules and allow the human verifier to be well aware of them. For example, in temporal action localization datasets~\\cite{thumos14}, an implicit assumption is that the action to be localized in a contiguous sub-clip and hence can be uniquely determined by the start and end timestamps. Since this rule is valid for all data, we consider it to be a meta-rule. \n\nA comprehensive understanding of \\textit{meta}-rules of a dataset is necessary for accurate estimation of the certificate set, and hence the certificate length. Otherwise, a spuriously long certificate might be necessary to ensure the veracity of the marked annotations. For example, consider the task of action classification on Kinetics-400. A valid meta-rule to be made available to the human verifier in this case is the mutual exclusivity of action classes i.e., each data point can belong only to one of the 400 classes present in Kinetics-400. Without this understanding, given, say a 10-second clip of a human skiing, the certificate set needs to necessarily encompass the entire 10 seconds since otherwise the human verifier might not be convinced that all of the other 399 actions are not occurring in the clip. However, with the knowledge of the label exclusivity meta-rule, the certificate length will be drastically reduced to just a fraction of a second since just observing the action of skiing in a few frames is sufficient for the human verifier to out-rule all other action classes. \n\n\\noindent \\textbf{Certificate Conventions}. For small certificate lengths, it is difficult for humans to estimate the exact sub-clip timestamps to be included in the certificate set. Hence, we choose to have a minimum length of $0.1$ second for a certificate. Further, in the case of two non-contiguous certificates, we collapse them into one if their closest ends are $<5$ seconds apart. In cases where a fact needs to be verified at several places throughout the video, we let the annotator make a reasonable judgment for the length of the certificate to be included as long as it follows the above conditions. \n\n\\section{Benchmarking\\name{}{}}\n\\subsection{Evaluating Certificate Lengths}\n\\label{sec:eval_certificate}\n\n\\figref{fig:certificateplot} presents certificate lengths for a spectrum of tasks spread across $15$ different datasets such as, action classification (Kinetics~\\cite{kinetics400}, Something-Something~\\cite{something_something}, UCF101~\\cite{ucf101}, HVU-Action~\\cite{hvu}), detection (AVA~\\cite{ava}), relationship classification (LVU~\\cite{lvu}), concept classification (HVU-Concept~\\cite{hvu}), video classification (Youtube-8M~\\cite{youtube8m}), Question-Answering (NextQA~\\cite{nextqa}, AGQA~\\cite{agqa2}, NextQA~\\cite{nextqa}, IVQA~\\cite{ivqa}, MSRVTT~\\cite{msrvtt}, ActivityNet-QA~\\cite{activitynetqa}, \\name{}{}). For\\name{}{} we benchmark the certificate length for 5 hours of video data ($100 \\mathcal{QAW}$) chosen randomly. For each other dataset, we ensure that (A) each annotated label class (if applicable) has at least 1 data sample evaluated and, (B) at least two hours of human effort is applied.  \\figref{fig:certificate_distribution} shows the histogram of estimated \\name{}{} temporal certificate lengths for the 100 clips. \n\n\\figref{fig:certificateplot} plots the certificate length against the actual clip length. We observe that \\name{}{} has temporal certificate length $5.7\\times$ longer than the second longest certificate length dataset, and $10\\times$ to $100\\times$ longer than all other video understanding datasets.\n\n\\subsection{Evaluating Multiple-choice Question Answering on\\name{}{}}\n\\label{sec:eval_mcq}\n\nIn \\tblref{tab:mcq_acc}, We benchmark several state-of-the-art video-language models, with the intention of adding more models in the future, in a Zero-shot question-answering setting on \\name{}{}. We evaluate each model in at least two settings. First is the conventional inference setting, where the model is assessed based on the same number of frames it was trained with. And second is a less challenging setting, where the model is tested on the maximum number of frames possible to execute inference with, using an 80G A100, without exceeding the GPU memory capacity. In both settings, frames are sampled uniformly from the input video clip.\n\n\\noindent\\textbf{FrozenBiLM}~\\cite{bilm} adapts frozen multi-modal encoders trained on web-scale data for the task of question answering and achieves state-of-the-art zero-shot QA accuracy across $8$ video question-answering datasets. We choose the How2QA FrozenBilM model under both $10$ and $90$ frames.\n\n\\setlength{\\columnsep}{5pt}\n\\setlength{\\intextsep}{5pt}\n\\begin{wrapfigure}{r}{0.75\\textwidth}\n\\centering\n\\caption{\\textbf{Benchmarking Zero-shot QA on\\name{}{}}}\n\\begin{tabular}{l|ccc|c}\n\\multirow{2}{*}{Model}  & \\multirow{2}{*}{Release} & Inference & Evaluation & QA \\\\\n& & Params & Setting & Acc \\\\\n\\toprule\n\\multicolumn{4}{l|}{Choosing the correct $\\mathcal{A}$ uniformly at random} & 20.0\\% \n\\\\ \\midrule\n\\multirow{2}{*}{FrozenBiLM~\\cite{bilm}}   & \\multirow{2}{*}{Oct 2022} & \\multirow{2}{*}{1.2B} & 10 frames  & 26.4\\% \\\\\n                     &   &  & 90 frames  &  26.9\\%\\\\\n\\midrule\n\\multirow{2}{*}{VIOLET~\\cite{violet}} & \\multirow{2}{*}{Sept 2022} & \\multirow{2}{*}{198M} & 5 frames   & 19.9\\%  \\\\\n                        & & &   75 frames  & 19.6\\% \\\\\n\\midrule\n\\multirow{5}{*}{mPLUG-Owl~\\cite{mplugowl}} & \\multirow{5}{*}{May 2023} & \\multirow{5}{*}{7.2B}  & 1 frame   & 27.0\\%\\\\\n& & & 5 frames   &   31.1\\%\\\\\n& & &   10 frames  & 29.6\\% \\\\\n& & &   15 frames  & 28.7\\% \\\\\n                        & & &   30 frames  & 20.0\\% \\\\\n\\midrule\n\\multirow{3}{*}{InternVideo~\\cite{internvideo}} & \\multirow{3}{*}{Dec 2022} & \\multirow{3}{*}{478M} \n                        &  10 frames  & 31.4\\% \\\\\n                        & & &  30 frames  & 31.8\\% \\\\\n\\rowcolor{baselinecolor}\n                        & & &  90 frames  & \\textbf{32.1}\\% \\\\\n\\bottomrule\n\\end{tabular}% }\n\\label{tab:mcq_acc}\n\\end{wrapfigure}\n\n\\noindent\\textbf{VIOLET}~\\cite{violet} a masked token modeling-based video language transformer that performs competitively on a variety of video-language tasks. We evaluate four of the best VIOLET models that are finetuned on different tasks for both $5$ and $75$ frames and choose the model with the best overall accuracy. More details are in \\textit{supplementary}. \n\n\\noindent\\textbf{mPLUG-Owl}~\\cite{mplugowl} proposes a training strategy to add image \\& video modality to pretrained large language models. We adapt mPLUG to facilitate the multiple choice QA by prompting the model with each of the options individually in the format: `Given question <\\texttt{question text}>, is answer <\\texttt{answer text}> correct?' along with the video frames. Then, we choose the option with the highest softmax score of the token `Yes' in the output text. We observe accuracy to be non-monotonic in frame length, and report results in $1$ to $30$ frames in Table \\ref{tab:mcq_acc}.\n\n\\noindent\\textbf{InternVideo}~\\cite{internvideo} proposes training video-language models jointly with masked video modeling and contrastive learning objectives. By default, InternVideo does not directly support multiple-choice video QA. We adapt the MSRVTT finetuned InternVideo model, which performs zero-shot multiple-choice tasks, by incorporating the question with each answer choice in the format: 'Question: <\\texttt{question text}>? Is it <\\texttt{answer text}>'. Then, we choose the option with the highest output score as the prediction. We report results spanning 10 to 90 input frames in Table \\ref{tab:mcq_acc}. We observe that performance is monotonic with the number of frames but the gain saturates around just $30$ frames.  \n\n\\noindent\\textbf{Human.} We also benchmark human performance on multiple-choice question answering task on\\name{}{} in Table~\\ref{tab:mcq_acc_human}. \\textit{First}, are time pressure settings where the annotators are asked to choose the correct answer under one (\\texttt{`In <1 min'}) and three (\\texttt{`In <3 min'}) minutes. Humans can already achieve an impressive 67.0\\% accuracy, in under 1 minute! Interestingly, this only slightly increases (+1.0\\%) when allowed three minutes. We believe that this can inform about performance on\\name{}{} in limited model inference capacities. We believe this could inform about the frame rate needed for long-form video understanding in future models. \\textit{Second}, we also benchmark human performance using only 1 fps video (\\texttt{`180 frames'}). Surprisingly, we observe that just with 1 fps humans can achieve an impressive 67.2\\%. \\setlength{\\columnsep}{5pt}\n\\setlength{\\intextsep}{5pt}\n\\begin{wrapfigure}{r}{0.5\\textwidth}\n\\centering\n\\caption{\\textbf{Human Accuracy on \\name{}{}} }\n\\begin{tabular}{l|c}\nEvaluation Setting & QA Accuracy \\\\\n\\toprule\n180 frames          &  67.2\\%\\\\\n\\midrule\nIn <1 min          &  67.0\\%\\\\\nIn <3 min     &  68.0\\%\\\\\n\\midrule\nNo constraint       &  75.0\\%\\\\\n\\rowcolor{baselinecolor}\nVideo $\\rightarrow$ Text         &  \\textbf{76.2}\\%\\\\\n\\bottomrule\n\\end{tabular}% }\n\\label{tab:mcq_acc_human}\n\\end{wrapfigure}\n\\textit{Third}, we evaluate human performance in a restrictive setting where the annotator is forced to first watch the video without reading the text, and then answer the question without re-watching the video (\\texttt{`Video $\\longrightarrow$ Text'}). Curiously, this achieves better accuracy than the \\texttt{`No constraint'} setting where the annotators are asked to simply answer without any constraints (76.2\\% vs. 75.0\\%). A possible hypothesis is that watching the video without text allows the annotator to focus more closely on the video, thereby benefiting performance than the setting where the attention is somewhat divided between the text and video. We believe this will help us understand the performance trade-offs in the early vs. late fusion of video and text modalities for long-form video-language models. Accuracy for \\texttt{`No constraint'} setting is estimated over 9 hours of video. All other accuracies are estimated over 5 hours of video.\n\n\\section{Conclusion}\nWe present\\name{}{}, a novel diagnostic benchmark designed for assessing very long-form video-language understanding capabilities of modern multimodal models. We also introduce the notion of a temporal \\textit{certificate} set, a probe that can be applied to a wide array of video tasks and benchmarks for understanding their intrinsic temporal lengths. We estimate temporal certificates of 15 varied datasets and demonstrate\\name{}{} to exhibit temporal certificate length approximately $5.7\\times$ longer than the next longest dataset and $25\\times$ to $100\\times$ longer than all other video understanding datasets. We also benchmark several state-of-the-art models on\\name{}{} and find their Zero-shot question-answering accuracy to be less than $33\\%$ while humans achieve 76\\%. We believe that\\name{}{} will play a key role in the development and evaluation of future very long-form video-language models.   \n\n\\noindent \\textbf{Limitations.}\\name{}{} RGB clips are sourced from Ego4D~\\cite{ego4d} and inherit Ego4D egocentric video biases. Further, the text is carefully curated for veracity, there are inevitable text data distribution biases that can occur in LLM-generated outputs due to biases present in web-scale LLM training data. Finally, human curation itself is far from perfect and while we perform two rounds of curation to minimize false positives, the collected \\name{}{} is most likely to inevitably contain some small mislabelled or ill-formed question-answer sets. We plan to host a crowd-sourced errata board to minimize human curation error over time with the support of the open-source research community. \n\n\\newpage\n\n\\section*{EgoSchema Datasheet}\n\\label{sec:datasheet}\n\\dssectionheader{Motivation}\n\n\\dsquestionex{For what purpose was the dataset created?}{Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description.}\n\n\\dsanswer{\\name{}{} is a diagnostic benchmark for assessing very long-form video-language understanding capabilities of modern multimodal systems. While some prior works have proposed video datasets with long clip lengths, we posit that merely the length of the video clip does not truly capture the temporal difficulty of the video task that is being considered. To remedy this, we introduce temporal certificate sets, a general notion for capturing the intrinsic temporal understanding length associated with a broad range of video understanding tasks \\& datasets. Please see Section~\\ref{sec:certificate_definition} in the main paper for more details. \n}\n\n\\dsquestion{Who created this dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?}\n\nThe authors created the dataset within the Malik Group at Berkeley AI Research, UC Berkeley. The authors created it for the public at large without reference to any particular organization or institution.\n\n\\bigskip\n\\dssectionheader{Composition}\n\n\\dsquestionex{What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?}{ Are there multiple types of instances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description.}\n\n\\dsanswer{Each instance in the dataset represents a 3-minute video and text that contains a question and five answer options. \n}\n\n\\dsquestion{How many instances are there in total (of each type, if appropriate)?}\n\n\\dsanswer{\\name{}{} has a total of 5063 instances each containing one video, one question, and five answer options. You can see further statistics on the whole data on our website \\href{https://egoschema.github.io}{egoschema.github.io}.\n}\n\n\\dsquestionex{Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?}{ If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable).}\n\n\\dsanswer{The video component of our dataset derives from the broader Ego4D dataset. For our research, we selectively extracted non-overlapping three-minute segments from the Ego4D video data, each segment consisting of a minimum of 30 human-annotated narrations (where each narration refers to a timestamped sentence). Detailed statistic of the number of viable clips for different possible length and narration density choices is discussed in Supplementary Section \\ref{sec:viable_clips}. The selected subset is very diverse in human behavior as can be seen by the activity statistics presented on \\href{https://egoschema.github.io}{egoschema.github.io}.\n}\n\n\\dsquestionex{What data does each instance consist of? “Raw” data (e.g., unprocessed text or images) or features?}{In either case, please provide a description.}\n\n\\dsanswer{Each instance in our dataset comprises raw mp4 video data, captured at a rate of 30 frames per second and with a high resolution. Accompanying this video data, there are six text elements - one question and five corresponding answer options one of which is marked as the correct answer to the question.\n}\n\n\\dsquestionex{Is there a label or target associated with each instance?}{If so, please provide a description.}\n\n\\dsanswer{Each instance is associated with a label ranging from 1 to 5 that indicates which of the five answer options is correct.\n}\n\n\\dsquestionex{Is any information missing from individual instances?}{If so, please provide a description, explaining why this information is missing (e.g. because it was unavailable). This does not include intentionally removed information but might include, e.g., redacted text.}\n\n\\dsanswer{All instances are complete.\n}\n\n\\dsquestionex{Are relationships between individual instances made explicit (e.g., users’ movie ratings, social network links)?}{If so, please describe how these relationships are made explicit.}\n\n\\dsanswer{Some instances may have the same video but different questions and answers. It will be indicated by a clip unique identifier in the final dataset.\n}\n\n\\dsquestionex{Are there recommended data splits (e.g., training, development/validation, testing)?}{If so, please provide a description of these splits, explaining the rationale behind them.}\n\n\\dsanswer{\\name{}{} is designed specifically for zero-shot testing. Its primary purpose is to be able to asses the out of the box long-term video-language understanding capabilities of modern multimodal models.\n}\n\n\\dsquestionex{Are there any errors, sources of noise, or redundancies in the dataset?}{If so, please provide a description.}\n\n\\dsanswer{The dataset was very carefully manually curated to mitigate any incidence of errors within the questions and answers. Although different questions may be posed for the same clip, it is ensured that there is no overlap between any two distinct clips. Further related details are also discussed in the limitations section in the main paper. \n}\n\n\\dsquestionex{Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)?}{If it links to or relies on external resources, a) are there guarantees that they will exist, and remain constant, over time; b) are there official archival versions of the complete dataset (i.e., including the external resources as they existed at the time the dataset was created); c) are there any restrictions (e.g., licenses, fees) associated with any of the external resources that might apply to a future user? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate.}\n\nEntirety of the dataset will be made publicly available at our project website \\href{https://egoschema.github.io}{egoschema.github.io}.\nWe will also provide a download tool for preprocessing all the videos such as cutting clips, associating the question/answer text etc. Text will be released in a JSON format, hosted on our \\href{https://github.com/egoschema/EgoSchema}{github repository}. EgoSchema will be publicly released under the Ego4D license, which allows public use of the video and text data for both research and commercial purposes.\n\n\\dsquestionex{Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals non-public communications)?}{If so, please provide a description.}\n\n\\dsanswer{No\n}\n\n\\dsquestionex{Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?}{If so, please describe why.}\n\n\\dsanswer{No\n}\n\n\\dsquestionex{Does the dataset relate to people?}{If not, you may skip the remaining questions in this section.}\n\n\\dsanswer{Some videos do contain people. However, the Ego4D authors employed an array of de-identification procedures primarily centered on ensuring a controlled environment with informed consent from all participants, and, where applicable, in public spaces with faces and other personally identifiable information suitably obscured. We strictly import all RGB information from Ego4D without any addition of our own. \n}\n\n\\dsquestionex{Does the dataset identify any subpopulations (e.g., by age, gender)?}{If so, please describe how these subpopulations are identified and provide a description of their respective distributions within the dataset.}\n\n\\dsanswer{No\n}\n\n\\dsquestionex{Is it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset?}{If so, please describe how.}\n\n\\dsanswer{No, Ego4D has employed an array of deidentification procedures in order to obscure any personally identifiable information such as people's faces.\n}\n\n\\dsquestionex{Does the dataset contain data that might be considered sensitive in any way (e.g., data that reveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)?}{If so, please provide a description.}\n\n\\dsanswer{No\n}\n\n\\bigskip\n\\dssectionheader{Collection Process}\n\n\\dsquestionex{How was the data associated with each instance acquired?}{Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how.}\n\n\\dsanswer{The video data, which is directly observable, was procured from the publicly accessible Ego4D dataset. In contrast, the text data was generated through the use of Large Language Models (LLMs) including GPT4, BARD, and Claude. These LLMs employed visual narrations from each video within the Ego4D dataset to generate the corresponding text.\n}\n\n\\dsquestionex{What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)?}{How were these mechanisms or procedures validated?}\n\n\\dsanswer{The video and narration data were downloaded in accordance with the official Ego4D guidelines for data access: \\href{https://ego4d-data.org/docs/start-here/}{https://ego4d-data.org/docs/start-here}. For the generation of the text data within our dataset, we utilized API access for GPT4 via OpenAI, for BARD via Google, and for Claude via Anthropic. This allowed us to generate three distinct questions for each video clip sampled from the Ego4D dataset. Upon the generation of these questions for each sampled video clip, we implemented a series of filtering procedures including Rule-based filtering, Blind filtering, and Manual curation. See Section 3.1.2 in the main paper for a more detailed explanation.\n}\n\n\\dsquestion{If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?}\n\n\\dsanswer{\nThe video component of our dataset derives from the broader Ego4D dataset. For our research, we selectively extracted non-overlapping three-minute segments from the Ego4D video data, each segment consisting of a minimum of 30 human-annotated narrations (where each narration refers to a timestamped sentence). Detailed statistic of the number of viable clips for different possible length and narration density choices is discussed in Supplementary Section \\ref{sec:viable_clips}.\n}\n\n\\dsquestion{Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?}\n\n\\dsanswer{Our research utilized the services of Quantigo, a specialized data labelling company. The teams of Quantigo employees that were based in Bangladesh were compensated at a rate of 5 dollars per hour, at a wage significantly higher than the market hourly rate in Bangladesh. This was done to ensure fair compensation for the complex tasks performed while also contributing to the highest quality of the work delivered. It's important to note that our collaboration with Quantigo followed ethical guidelines, with the fair treatment of all employees involved and the appropriate respect for their expertise and labor. For exact instructions for human curation, see Supplementary Section \\ref{sec:quantigo}.\n}\n\n\\dsquestionex{Over what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)?}{If not, please describe the timeframe in which the data associated with the instances was created.}\n\n\\dsanswer{The original videos within the Ego4D dataset were collected across various occasions spanning from 2019 to 2021. As for the EgoSchema, the textual information was collected over several sprints during the first half of 2023 based on the Ego4D narrations.   \n}\n\n\\dsquestionex{Were any ethical review processes conducted (e.g., by an institutional review board)?}{If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation.}\n\n\\dsanswer{No\n}\n\n\\dsquestionex{Does the dataset relate to people?}{If not, you may skip the remaining questions in this section.}\n\n\\dsanswer{Yes\n}\n\n\\dsquestion{Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)?}\n\n\\dsanswer{The video and narration data were acquired in accordance with the official Ego4D guidelines for data access: \\href{https://ego4d-data.org/docs/start-here/}{https://ego4d-data.org/docs/start-here/}. The Ego4D authors had in turn ensured consent of the people involved. \n}\n\n\\dsquestionex{Were the individuals in question notified about the data collection?}{If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself.}\n\n\\dsanswer{Ego4d paper followed several procedures to ensure the preservation of privacy and the upholding of ethical standards. Notably, these procedures included obtaining informed consent from those wearing the cameras and adhering to de-identification requirements for personally identifiable information (PII). Given that the video collection was conducted by Ego4D, we are not in a position to provide specific instructions that were given to the camera wearers. The Ego4D privacy statement is available at \\href{https://ego4d-data.org/pdfs/Ego4D-Privacy-and-ethics-consortium-statement.pdf}{https://ego4d-data.org/pdfs/Ego4D-Privacy-and-ethics-consortium-statement.pdf}\n}\n\n\\dsquestionex{Did the individuals in question consent to the collection and use of their data?}{If so, please describe (or show with screenshots or other information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented.}\n\n\\dsanswer{Ego4d paper privacy procedures have included obtaining informed consent from those wearing the cameras. Given that the video collection was conducted by Ego4D, we are not in a position to provide specific instructions that were given to the camera wearers. See \\href{https://ego4d-data.org/pdfs/Ego4D-Privacy-and-ethics-consortium-statement.pdf}{Ego4D privacy statement}.\n}\n\n\\dsquestionex{If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses?}{If so, please provide a description, as well as a link or other access point to the mechanism (if appropriate).}\n\n\\dsanswer{Ego4d paper privacy procedures have included allowing camera users to ask questions and withdraw at any time. Additionally, they were free to review and redact their own video. Given that the video collection was conducted by Ego4D, we are not in a position to provide specific instructions that were given to the camera wearers. You can find the Ego4D privacy statement at \\href{https://ego4d-data.org/pdfs/Ego4D-Privacy-and-ethics-consortium-statement.pdf}{https://ego4d-data.org/pdfs/Ego4D-Privacy-and-ethics-consortium-statement.pdf}.\n}\n\n\\dsquestionex{Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted?}{If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation.}\n\n\\dsanswer{While we recognize the importance of this topic, we would, once more, refer to the Ego4D paper for an in-depth discussion. Ego4D acknowledges the potential privacy risks associated with the use of wearable devices in data collection and has taken several steps such as depersonalizing any sensitive information, blurring out faces and bodies, etc. towards maintaining privacy. The same carries over to the video data in EgoSchema as well. Broadly, very long-form video understanding is a core capability for agents that are to perceive the natural visual world. Hence, developing datasets such as EgoSchema will be critical to unlocking this key AI capability. Additionally, according to  \\href{https://ego4d-data.org/pdfs/Ego4D-Privacy-and-ethics-consortium-statement.pdf}{Ego4D privacy statement}, all videos from Ego4D were reviewed by an approved member of one of the participant's universities or institutes to identify and assess potential privacy concerns. \n}\n\n\\bigskip\n\\dssectionheader{Preprocessing/cleaning/labeling}\n\n\\dsquestionex{Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)?}{If so, please provide a description. If not, you may skip the remainder of the questions in this section.}\n\n\\dsanswer{The set of generated questions and answers from output was filtered by those LLMs and finally curated by humans. A detailed description can be found in Section 3. There was no preprocessing done on the video clips sampled from Ego4D.\n}\n\n\\dsquestionex{Was the “raw” data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)?}{If so, please provide a link or other access point to the “raw” data.}\n\n\\dsanswer{Human curation was employed to rectify errors in the question-answer sets, particularly cases where the identified correct answer was wrong or a wrong answer was actually correct. Given the crucial role of this step in ensuring the accuracy of our dataset, we do not find it necessary to release a version of the dataset prior to human curation. However, all the discarded \"raw\" data is indeed also saved. \n}\n\n\\dsquestionex{Is the software used to preprocess/clean/label the instances available?}{If so, please provide a link or other access point.}\n\n\\dsanswer{The APIs for the Large Language Models (LLMs) are publicly accessible. The prompts for filtering and instructions for human curation are provided in Supplementary Section \\ref{sec:full_prompts} and Supplementary Section \\ref{sec:quantigo} respectively. Additionally all necessary code for generation, filtering etc. is provided in the supplementary materials. \n}\n\n\\bigskip\n\\dssectionheader{Distribution}\n\n\\dsquestionex{Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created?}{If so, please provide a description.}\n\n\\dsanswer{The dataset will be made publicly available and can be used for both research and commercial purposes under the Ego4D license. \n}\n\n\\dsquestionex{How will the dataset be distributed (e.g., tarball on website, API, GitHub)}{Does the dataset have a digital object identifier (DOI)?}\n\n\\dsanswer{\nThe dataset will be distributed as a JSON file describing the unique identifier for each clip, the associated question, the five answer options, the label, and additional clip information that facilitates the tracing of the clip back to the original Ego4D data, such as the Ego4D video identification of the clip's source video, among other details. In addition, download tools to acquire and pre-process the video RGB data will also be provided on our website. \n}\n\n\\dsquestion{When will the dataset be distributed?}\n\n\\dsanswer{The full dataset will be made available upon the acceptance of the paper before the camera-ready deadline. \n}\n\n\\dsquestionex{Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)?}{If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.}\n\n\\dsanswer{EgoSchema will be publicly released under the Ego4D license, which allows direct public use of the video and text data for both research and commercial purposes.\n}\n\n\\dsquestionex{Have any third parties imposed IP-based or other restrictions on the data associated with the instances?}{If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions.}\n\n\\dsanswer{No\n}\n\n\\dsquestionex{Do any export controls or other regulatory restrictions apply to the dataset or to individual instances?}{If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation.}\n\n\\dsanswer{No\n}\n\n\\bigskip\n\\dssectionheader{Maintenance}\n\n\\dsquestion{Who will be supporting/hosting/maintaining the dataset?}\n\n\\dsanswer{The authors of the paper will be maintaining the dataset, pointers to which will be hosted on github repo \\href{https://github.com/egoschema/EgoSchema}{https://github.com/egoschema/EgoSchema} along with the code for download and preprocessing tool, with the actual data hosted either on Amazon AWS as an S3 bucket or as a google drive folder. \n}\n\n\\dsquestion{How can the owner/curator/manager of the dataset be contacted (e.g., email address)?}\n\n\\dsanswer{We will post the contact information on our website. We will be available through github issues as well as through email. \n}\n\n\\dsquestionex{Is there an erratum?}{If so, please provide a link or other access point.}\n\n\\dsanswer{We will host an erratum on the Github repo in the future, to host any approved errata suggested by the authors or the video research community.  \n}\n\n\\dsquestionex{Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)?}{If so, please describe how often, by whom, and how updates will be communicated to users (e.g., mailing list, GitHub)?}\n\n\\dsanswer{Yes, we plan to host an erratum publicly. There are no specific plans for a v2 version, but there does seem plenty oppurtunities for exciting future dataset work based on \\name{}{}.\n}\n\n\\dsquestionex{If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were individuals in question told that their data would be retained for a fixed period of time and then deleted)?}{If so, please describe these limits and explain how they will be enforced.}\n\n\\dsanswer{No.\n}\n\n\\dsquestionex{Will older versions of the dataset continue to be supported/hosted/maintained?}{If so, please describe how. If not, please describe how its obsolescence will be communicated to users.}\n\n\\dsanswer{N/A There are no older versions at the current moment. All updates regarding the current version will be communicated via our website.\n}\n\n\\dsquestionex{If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?}{If so, please provide a description. Will these contributions be validated/verified? If so, please describe how. If not, why not? Is there a process for communicating/distributing these contributions to other users? If so, please provide a description.}\n\n\\dsanswer{Contributions will be made possible using standard open-source tools, submitted as pull requests to the relevant GitHub repository. Moreover, we will provide information on how to trace sampled clips back to their original source within the Ego4D dataset. This will enable users to access additional Ego4D data, such as narrations, summaries, and object detections, as applicable.\n}\n\\clearpage\n\\section*{Full Prompts}\n\\label{sec:full_prompts}\n\nHere are some of the prompts we developed for generating \\name{}{}.\n\n\\subsection{Set A}\n\\subsubsection{Question prompt}\n\\begin{lstlisting}\nInput: \nI want you to act as a teacher in the class called \"Long-term video understanding\". I will provide video action narrations and their timestamps and you will generate three highly difficult and diverse questions for your students about the high-level details in the video. You want to test students' following abilities:\n\nAbility 1: Students' ability to summarize and compare long parts of the video\nAbility 2: Students' ability to compress information from the video rather than just listing the actions that happened in the video.\nAbility 3: Students' ability to identify the most important parts of the video. \n\nYour questions should not mention any particular timestamps or narrations. Remember to make sure the correct answers to your questions do not list information from the narrations but compress them in a concise conclusion.\n\nExamples of good and difficult questions:\n\"What is the main action of the video?\"\n\"Why did C do action ...?\"\n\nAVOID the following types of questions:\n\"When ...?\"\n\"How many ...?\"\n\"How much ...?\"\n\nWhen announcing the question please label each question as \"Question 1,2,3: [full question]\"\n\nTimestamps and narrations:\n0 - C stares at the lamp\n1 - C looks around the apartment\n3 - C talks to a man\n6 - C walks around the apartment\n10 - C talks to a man\n11 - C looks around the apartment\n14 - a man plays the guitar\n20 - a man walks around the apartment\n21 - a man plays the guitar\n21 - C walks around the apartment\n22 - C stares at the window\n24 - C looks around the apartment\n27 - C talks to a man\n29 - C picks chips from the table\n32 - C looks around the apartment\n36 - C stares at a man\n36 - a man plays the guitar\n37 - C walks around the apartment\n41 - C sits on the sofa\n43 - C stares at a man\n44 - a man plays the guitar\n49 - a man climbs up the sofa\n50 - a man plays the guitar\n62 - a man climbs down the sofa\n63 - a man walks around the apartment\n64 - a man plays the guitar\n93 - C stares at the table\n96 - a man plays a guitar\n111 - C stares at the table\n112 - a man walks around the apartment\n114 - C stares at a man\n115 - a man lifts the guitar\n118 - a man walks around the apartment\n120 - a man places guitar on the sofa\n122 - a man walks around the apartment\n125 - C stands up\n126 - C walks around the apartment\n131 - a man picks a coffee maker jug\n132 - a man pours coffee in a cup\n134 - C stares at a man\n140 - C stares at the window\n142 - C stares at a man\n144 - a man drinks coffee\n145 - C looks around the apartment\n150 - C walks around the apartment\n152 - C talks to a man\n156 - C stares at a bench\n158 - C looks around the apartment\n164 - C stares at a man\n164 - a man talks to C\n167 - C looks around the apartment\n171 - C follows a man\n174 - a man points at the window\n175 - C looks around the bedroom\n================\nOutput:\nQuestion 1: What can you deduce about the relationship between C and the man, based on their interactions and behaviors throughout the video?\n\nQuestion 2: Identify the primary activity in the apartment and discuss its importance/significance. How does it influence the actions and atmosphere of the video?\n\nQuestion 3: Observe the dynamics of the video in terms of changes in characters' actions, interactions, and spatial movement. How do these shifts contribute to the overall narrative?\n\\end{lstlisting}\\subsubsection{Answer prompt}\n\\begin{lstlisting}\nInput:\nI want you to act as a teacher in the class called \"Long-term video understanding.\" I will provide video action narrations and their timestamps and three highly difficult and diverse questions for your students about the high-level details in the video. I want you to test students' following abilities:\n\nAbility 1: Students' ability to summarize and compare long parts of the video\nAbility 2: Students' ability to compress information from the video rather than just listing the actions that happened in the video.\nAbility 3: Students' ability to identify the most important parts of the video. \n\nI want you to create a difficult multiple-choice exam that tests above student abilities based on the three questions I just provided. Each question should have five similar open-ended but short answers, but only one should be correct. Make it very difficult for students to find the correct answer among all the wrong answers. All answers should be closely related to what happens in the video. Make wrong answers significantly longer than correct answers. Ensure all of the correct answers compress information from narrations them into a concise conclusion. Your answers should not mention any particular timestamps or narrations.\n\nDo not use letters for the answer choices\nPrint each correct answer exactly as \"Correct answer: [full answer]\"\nPlease print each wrong answer on a new line and print each wrong answer as \"Wrong answer 1,2,3,4: [full answer]\"\n\nTimestamps and narrations:\n0 - C stares at the lamp\n1 - C looks around the apartment\n3 - C talks to a man\n6 - C walks around the apartment\n10 - C talks to a man\n11 - C looks around the apartment\n14 - a man plays the guitar\n20 - a man walks around the apartment\n21 - a man plays the guitar\n21 - C walks around the apartment\n22 - C stares at the window\n24 - C looks around the apartment\n27 - C talks to a man\n29 - C picks chips from the table\n32 - C looks around the apartment\n36 - C stares at a man\n36 - a man plays the guitar\n37 - C walks around the apartment\n41 - C sits on the sofa\n43 - C stares at a man\n44 - a man plays the guitar\n49 - a man climbs up the sofa\n50 - a man plays the guitar\n62 - a man climbs down the sofa\n63 - a man walks around the apartment\n64 - a man plays the guitar\n93 - C stares at the table\n96 - a man plays a guitar\n111 - C stares at the table\n112 - a man walks around the apartment\n114 - C stares at a man\n115 - a man lifts the guitar\n118 - a man walks around the apartment\n120 - a man places guitar on the sofa\n122 - a man walks around the apartment\n125 - C stands up\n126 - C walks around the apartment\n131 - a man picks a coffee maker jug\n132 - a man pours coffee in a cup\n134 - C stares at a man\n140 - C stares at the window\n142 - C stares at a man\n144 - a man drinks coffee\n145 - C looks around the apartment\n150 - C walks around the apartment\n152 - C talks to a man\n156 - C stares at a bench\n158 - C looks around the apartment\n164 - C stares at a man\n164 - a man talks to C\n167 - C looks around the apartment\n171 - C follows a man\n174 - a man points at the window\n175 - C looks around the bedroom\n\nQuestions:\n\nQuestion: what can you deduce about the relationship between c and the man, based on their interactions and behaviors throughout the video?\n\nQuestion: identify the primary activity in the apartment and discuss its importance/significance. how does it influence the actions and atmosphere of the video?\n\nQuestion: Observe the dynamics of the video in terms of changes in characters' actions, interactions, and spatial movement. how do these shifts contribute to the overall narrative?\n================\nOutput:\nQuestion 1: What can you deduce about the relationship between C and the man, based on their interactions and behaviors throughout the video?\n\nCorrect answer: They have a casual, friendly relationship.\nWrong answer 1: C and the man are complete strangers and never interacted before.\nWrong answer 2: C is deeply in love with the man, and the man reciprocates those feelings.\nWrong answer 3: They are business associates discussing a financial transaction.\nWrong answer 4: C is an intruder in the man's apartment, and the man is trying to get C to leave.\n\nQuestion 2: Identify the primary activity in the apartment and discuss its importance/significance. How does it influence the actions and atmosphere of the video?\n\nCorrect answer: The primary activity is the man playing the guitar, which sets a relaxed environment.\nWrong answer 1: The main activity is C staring at different objects, indicating a deep curiosity.\nWrong answer 2: The primary activity is a heated argument between C and the man that escalates over time.\nWrong answer 3: The main activity is the man anxiously pacing around the apartment, creating a tense atmosphere.\nWrong answer 4: The primary activity is C and the man planning to commit a crime together.\n\nQuestion 3: Observe the dynamics of the video in terms of changes in characters' actions, interactions, and spatial movement. How do these shifts contribute to the overall narrative?\nCorrect Answer: Actions and interactions are casual and relaxed, reflecting a comfortable environment.\nWrong Answer A : The video displays a profound sense of conflict and tension arising between the characters.\nWrong Answer B : The man is showing C the issues that need fixing in the apartment in a professional manner.\nWrong Answer C : Both the characters display an increasingly urgent need to solve an issue in the apartment.\nWrong Answer D : C and the man admire and interact with several objects in the apartment that look beautiful.\n\\end{lstlisting}\n\\subsection{Set B}\n\\subsubsection{Question and answer prompt}\n\\begin{lstlisting}\nInput:\nI want you to act as a teacher in the class called \"Long-term video understanding\". I will provide video action narrations and their timestamps and you will generate three highly difficult and diverse questions for your students about the high-level details in the video. You want to test students' following abilities:\n\nAbility 1: Students' ability to summarize and compare long parts of the video\nAbility 2: Students' ability to compress information from the video rather than just listing the actions that happened in the video.\nAbility 3: Students' ability to identify the most important parts of the video. \n\nYour questions should not mention any particular timestamps or narrations. Remember to make sure the correct answers to your questions do not list information from the narrations but compress them in a concise conclusion.\n\nExamples of good and difficult questions:\n\"What is the main action of the video?\"\n\"Why did C do action ...?\"\n\nAVOID the following types of questions:\n\"When ...?\"\n\"How many ...?\"\n\"How much ...?\"\n\nWhen announcing the question please label each question as \"Question 1,2,3: [full question]\"\n\nTimestamps and narrations:\n3 - C holds the cloth in his right hand.\n5 - the woman picks a carton from the grocery bag on the floor with her right hand.\n6 - the woman drops the carton in a cabinet with her left hand.\n7 - the woman dips both hands into the grocery bag.\n9 - the woman drops a green carton on the floor with her right hand.\n12 - C drops the green carton in the cabinet with his right hand.\n13 - the woman holds a pack bag in her right hand.\n16 - C opens a kitchen cabinet with his left hand.\n18 - C removes a cereal pack from the kitchen cabinet with his left hand.\n19 - C puts the green carton into the kitchen cabinet with his right hand.\n21 - C closes the kitchen cabinet with his left hand.\n24 - the woman removes a plastic from the grocery bag with her right hand.\n25 - the woman drops the plastic on the floor with her right hand.\n33 - C closes a wardrobe with his left hand.\n38 - the woman puts a pack into the cabinet with her right hand.\n43 - a dog lies down on a bed.\n54 - C picks a cloth from the floor with his right hand.\n58 - C adjusts the cloth with both hands.\n66 - C hangs the cloth on the wall with both hands.\n74 - the woman holds a grocery bag in her right hand.\n82 - the woman touches her hair with her right hand.\n92 - the woman talks with C.\n99 - C holds two piece of cloths in both hands.\n100 - the woman picks piece of clothes from a bag with both hands.\n100 - C adjusts a camera on his head with his right hand.\n103 - C drops the two piece of cloths on a couch with his left hand.\n109 - C opens a door with his right hand.\n110 - C walks into a toilet.\n114 - C holds a red towel in his right hand.\n116 - the woman talks with C.\n118 - the woman scratches her back with her right hand.\n118 - the dog walks into the toilet.\n120 - C hangs the red towel on the wall with his right hand.\n121 - C holds a towel in his left hand.\n129 - the woman holds the towel in her left hand.\n129 - the dog walks out of the toilet.\n132 - C holds the door with his left hand.\n133 - the woman touches her face with her right hand.\n148 - C adjusts the camera on his head with his left hand.\n149 - C touches his face with his right hand.\n150 - the woman holds a bin with both hands.\n160 - C opens the door with his right hand.\n161 - C walks into the toilet.\n163 - the woman walks into the toilet.\n166 - the woman drops the bin on the floor with both hands.\n168 - the woman adjusts the bin on the floor with both hands.\n================\nOutput:\nQuestion 1: Can you provide a concise summary of the primary activities taking place in the video, highlighting similarities between the actions of C and the woman?\n\nQuestion 2: Based on the video, what task seems to be the central focus of both C and the woman's actions, and how do they contribute to the accomplishment of this task?\n\nQuestion 3: Identify the three most important segments of the video and explain the significance of each in the overall context of the actions taking place.\n================\nInput:\nStudents are done with the questions. It is time to reveal the correct answers. Correctly provide short one-sentence answers to the questions you created, and additionally generate 4 very similar but wrong answers. Make sure that it will be hard to find the correct answer when combined with the wrong ones. Also, ensure that all the answers are concise and very close to an actual answer in length. Mainly make sure your correct answers do not list information from the narrations but compress them in a concise conclusion. \n\nDo not use letters for the answer choices\nPrint each correct answer exactly as \"Correct answer: [full answer]\"\nPlease print each wrong answer on a new line and print each wrong answer as \"Wrong answer 1,2,3,4: [full answer]\"\n================\nOutput:\nQuestion 1:\nCorrect answer: C and the woman both interact with various objects to organize the space, including placing grocery items and hanging cloths.\nWrong answer 1: [Output disregarded]\nWrong answer 2: [Output disregarded]\nWrong answer 3: [Output disregarded]\nWrong answer 4: [Output disregarded]\n\nQuestion 2:\nCorrect answer: The central focus is organizing and tidying up the space, with C and the woman putting items in their respective places.\nWrong answer 1: [Output disregarded]\nWrong answer 2: [Output disregarded]\nWrong answer 3: [Output disregarded]\nWrong answer 4: [Output disregarded]\n\nQuestion 3:\nCorrect answer: Key segments include organizing groceries, adjusting household items (e.g., cloths, bins), and interacting with a dog.\nWrong answer 1: [Output disregarded]\nWrong answer 2: [Output disregarded]\nWrong answer 3: [Output disregarded]\nWrong answer 4: [Output disregarded]\n\\end{lstlisting}\\subsubsection{Wrong answer prompt}\n\\begin{lstlisting}\nInput:\nI want you to act as a teacher in the class called \"Long-term video understanding.\" I will provide video action narrations and their timestamps and three highly difficult and diverse questions for your students about the high-level details in the video. I will also show the correct answers to the questions. I want you to test students' following abilities:\n\nAbility 1: Students' ability to summarize and compare long parts of the video\nAbility 2: Students' ability to compress information from the video rather than just listing the actions that happened in the video.\nAbility 3: Students' ability to identify the most important parts of the video. \n\nI want you to create a difficult multiple-choice exam that tests above student abilities based on the three questions and their correct answers I just provided. Each question should have five similar open-ended but short answers, but only one should be correct. Make it very difficult for students to find the correct answer among all the wrong answers. All answers should be closely related to what happens in the video. Make wrong answers significantly longer than correct answers. Ensure all of the correct answers compress information from narrations them into a concise conclusion. Your answers should not mention any particular timestamps or narrations.\n\nDo not use letters for the answer choices\nPlease print each wrong answer on a new line and print each wrong answer as \"Wrong answer 1,2,3,4: [full answer]\"\n\nTimestamps and narrations:\n3 - C holds the cloth in his right hand.\n5 - the woman picks a carton from the grocery bag on the floor with her right hand.\n6 - the woman drops the carton in a cabinet with her left hand.\n7 - the woman dips both hands into the grocery bag.\n9 - the woman drops a green carton on the floor with her right hand.\n12 - C drops the green carton in the cabinet with his right hand.\n13 - the woman holds a pack bag in her right hand.\n16 - C opens a kitchen cabinet with his left hand.\n18 - C removes a cereal pack from the kitchen cabinet with his left hand.\n19 - C puts the green carton into the kitchen cabinet with his right hand.\n21 - C closes the kitchen cabinet with his left hand.\n24 - the woman removes a plastic from the grocery bag with her right hand.\n25 - the woman drops the plastic on the floor with her right hand.\n33 - C closes a wardrobe with his left hand.\n38 - the woman puts a pack into the cabinet with her right hand.\n43 - a dog lies down on a bed.\n54 - C picks a cloth from the floor with his right hand.\n58 - C adjusts the cloth with both hands.\n66 - C hangs the cloth on the wall with both hands.\n74 - the woman holds a grocery bag in her right hand.\n82 - the woman touches her hair with her right hand.\n92 - the woman talks with C.\n99 - C holds two piece of cloths in both hands.\n100 - the woman picks piece of clothes from a bag with both hands.\n100 - C adjusts a camera on his head with his right hand.\n103 - C drops the two piece of cloths on a couch with his left hand.\n109 - C opens a door with his right hand.\n110 - C walks into a toilet.\n114 - C holds a red towel in his right hand.\n116 - the woman talks with C.\n118 - the woman scratches her back with her right hand.\n118 - the dog walks into the toilet.\n120 - C hangs the red towel on the wall with his right hand.\n121 - C holds a towel in his left hand.\n129 - the woman holds the towel in her left hand.\n129 - the dog walks out of the toilet.\n132 - C holds the door with his left hand.\n133 - the woman touches her face with her right hand.\n148 - C adjusts the camera on his head with his left hand.\n149 - C touches his face with his right hand.\n150 - the woman holds a bin with both hands.\n160 - C opens the door with his right hand.\n161 - C walks into the toilet.\n163 - the woman walks into the toilet.\n166 - the woman drops the bin on the floor with both hands.\n168 - the woman adjusts the bin on the floor with both hands.\n\nQuestions:\n\nQuestion: Can you provide a concise summary of the primary activities taking place in the video, highlighting similarities between the actions of c and the woman?\nCorrect answer: c and the woman both interact with various objects to organize the space, including placing grocery items and hanging cloths.\n\nQuestion: Based on the video, what task seems to be the central focus of both c and the woman's actions, and how do they contribute to the accomplishment of this task?\nCorrect answer: the central focus is organizing and tidying up the space, with c and the woman putting items in their respective places.\n\nQuestion: Identify the three most important segments of the video and explain the significance of each in the overall context of the actions taking place?\nCorrect answer: key segments include organizing groceries, adjusting household items (e.g., cloths, bins), and interacting with a dog.\n================\nOutput:\nQuestion 1: Can you provide a concise summary of the primary activities taking place in the video, highlighting similarities between the actions of c and the woman?\n\nCorrect answer: C and the woman both interact with various objects to organize the space, including placing grocery items and hanging cloths.\nWrong answer 1: C and the woman engage in lengthy conversations about their day while occasionally placing groceries in cabinets and adjusting cloths on walls.\nWrong answer 2: C and the woman focus on preparing an elaborate dinner, flipping through recipe books and chopping vegetables.\nWrong answer 3: Both C and the woman predominantly pay attention to the dog's well-being, providing it with food and water and playing with it.\nWrong answer 4: The video revolves around C and the woman working on a DIY project, with tools scattered around the room.\n\nQuestion 2: Based on the video, what task seems to be the central focus of both c and the woman's actions, and how do they contribute to the accomplishment of this task?\n\nCorrect answer: The central focus is organizing and tidying up the space, with C and the woman putting items in their respective places.\nWrong answer 1: C and the woman are preoccupied with an ongoing debate, their actions aimlessly directed towards unrelated tasks.\nWrong answer 2: They are both primarily absorbed in their individual tasks, occasionally interacting with each other but with no clear common goal.\nWrong answer 3: The central focus of their actions is providing care and attention to the dog, and their actions revolve around its needs.\nWrong answer 4: The video primarily depicts a disagreement between C and the woman, with actions centered around this conflict.\n\nQuestion 3: Identify the three most important segments of the video and explain the significance of each in the overall context of the actions taking place?\n\nCorrect answer: Key segments include organizing groceries, adjusting household items (e.g., cloths, bins), and interacting with a dog.\nWrong answer 1: The main segments involve discussing a movie, checking on an overheating stove, and attempting to catch a rodent in the room.\nWrong answer 2: The most important segments consist of C and the woman eating a meal, participating in a board game, and reading books.\nWrong answer 3: Three primary segments include C and the woman debating household chores, searching for lost items, and tending to a crying baby.\nWrong answer 4: Key segments include discussing an upcoming event, taking turns answering a phone call, and checking updates from an ongoing sports game.\n\\end{lstlisting}\\clearpage\n\n\\section{Our clip length and narration density choice}\n\\label{sec:viable_clips}\n\\begin{figure}[h!]\n    \\centering\n    \\includegraphics[scale=0.5]{figures/3min_validation_viridis.pdf}\n    \\caption{\\small \\textbf{Heatmap of number of viable clips over a range of clip length and narration density.} There are only a few viable options that offer some degree of balance between the number of clips and the number of narration in the clip. One potential selection is to utilize 3-minute clips with a density of 5 narrations per minute, although this choice bears the significant disadvantage of potentially including clips with an insufficient volume of narration data to generate high-quality results. Another possible choice is to use 1-minute clips with a density of 20 narrations per minute, yet this option carries the drawback of the clips being too brief for the dataset to be very long-term. Hence, we choose 3-minute clips with a narration density of 10 narrations per minute as it offers a satisfactory balance between the number of narrations and clip length for generating \\name{}{}.}\n\\end{figure}\n\n\\section{Human curation}\n\\label{sec:quantigo}\nOur research utilized the services of a third part company (not MTurk), for specifically training annotators to ensure quality. The process involved two distinct annotation procedures: data curation and human accuracy testing.\n\n\\subsection{Curation}\nGenerated data curation was performed by Quantigo employees. These curators were responsible for ensuring that the released \\name{}{} dataset is tge highest high quality possible. Here is the exact instructions that was provided to annotators:\n\\begin{lstlisting}\nThe annotation we need is to say that the Question-correct answer-wrong answer set (the whole set) is good if all these three conditions pass:\n\n(Condition A) Question is Answerable: The question can be answered from the video and requires more than just a second of video to answer (so, if the answer is not present in the video or, if the answer can be formed with just a few frames (less than say, a second) then it fails this condition).\n\n(Condition B) The Marked Correct Answer is correct: The \"\"correct answer\"\" is more the correct answer to the question\n\n(Condition C) The Marked Wrong Answers are wrong: All 4 \"\"wrong answers\"\" are less correct than the \"\"correct answer\"\" (So for example, if a wrong answer is not completely false, but simply does not contain all the information that the \"\"correct answer\"\" does, then it is still a fine \"\"wrong answer\"\") IF even one of the marked answer is correct, the set should be labeled as bad. \n\n(Condition D) The question is actually long-term: This is a very very important condition. We want the certificate for the question to be at least 30 seconds minimum. If the certificate is non-contiguous (ie. 5 seconds at one place, 20 seconds at another, and 15 more seconds at a third place) the sum of lengths of all the sub-certificates together should be more than 30 seconds. Another example is, if a question can be answered simply from a few frames of the video, the certificate is small (and less than 30 seconds) and hence would fail this condition. Additional detials on how to handle certificate edge cases are provided in the annotator training through examples.    \n\n(Condition E) Avoid Boring Questions: Questions that ask about the frequency of something (\"How many times..\") fail this condition.\n\nIf any of these five conditions fail we want the whole set (Question / Correct Answer / Wrong Answer) marked bad. \n\nOptional: \nSince GOOD sets are so rare, in cases where it seems that a set is good but a small part of the above five conditions is not being met or, if one/two words were different this can be a good set, please label as MAYBE and we will fix it in the second round. We expect, Good/Bad to be about 97% of data and Maybe to be not more than 3%.\n\nExtended notes:\n1. In our experience, the wrong answers are made such that they differ from the correct answer in small but crucially meaningful ways. There are many cases where a careless reading of the wrong answer might make it seem that it is correct but upon careful inspection, it will become clear that something about the wrong answer indeed makes it wrong.  While this is common, there are indeed cases where the wrong answer is also just as correct as the correct answer. In such cases, the wrong answer fails condition C, and the set is bad. \n\n2. Roughly speaking, we expect about 20-25% of the questions that we have provided to be found as good. However, this is not necessary and the %age can be smaller or larger depending on each three-minute clip. \n\nEdge Cases: \n1. If the asked question has multiple answers and at least one of them aligns with the correct answer while none of them align with any of the other wrong answers, then provided that the top 5 conditions are met, we can mark the set as good.  \n2. If two questions are very similar (even within different clips) and both are GOOD, only choose one as GOOD and reject the other one with a comment mentioning this. We do not expect this to happen more than 1 or 2 times in a 100. \n3. There might be more such edge cases, please feel free to contact me in such cases and we can expland. \n\\end{lstlisting}\\section{Benchmarking details}\n\\label{sec:benchmarking_details}\n\\subsection{\\textbf{Violet}}\nViolet is a video language model comprised of a visual encoder, text encoder, and multimodal transformer pretrained on a variety of masked visual modeling tasks ranging from simple ones such as RBG pixel values up to more high levels ones such as spatially focussed image features. It performs competitively on a variety of video-language tasks such as Video-QA and Video-Text Retrieval. We evaluate one pre-trained model and 3 models finetuned on lsmdc-mc, msrvtt-qa, and msrvtt-retrieval. We evaluate using both 5 frames and 75 frames and choose the model with the best overall accuracy.\n\\subsection{\\textbf{mPLUG-Owl}}\nBy default, mPLUG-Owl does not possess inherent capabilities for direct video question answering. As such, we undertook several experiments to adapt it to our required format. One approach involved inputting all answer choices in the form of a shuffled test. However, this resulted in a bias towards selecting the first option in most cases. For another approach, FrozenBiLM offered a methodology for frozen zero-shot models to operate in the context of multiple-choice video question answering, which inspired us to adapt this methodology for mPLUG-Owl. As mPLUG-Owl utilizes word-level tokenization, we could extract the confidence score for each generated token, particularly the 'Yes' token. We recorded the 'Yes' token confidence score for each answer option. In instances where the 'Yes' token was absent, we assigned the confidence score as zero, though empirically, in most cases, the model output was positive and contained the 'Yes' token. Ultimately, we selected the answer option with the highest 'Yes' confidence score as the model output given the question. In scenarios where multiple options scored the same highest confidence for the 'Yes' token, we randomly selected the answer from these top-scoring options. It should be noted that mPlug-Owl was originally trained to process a single image, and its capacity to handle additional frames is an emergent ability that has not been thoroughly tested to date.\"\n\\subsection{\\textbf{InternVideo}}\nThe two most closely aligned formats supported by InternVideo are open-ended Video Question Answering and Zero-shot Multiple Choice tasks. In the case of open-ended Video Question Answering, the task is to predict the answer to a question posed within a video. However, due to the restricted vocabulary of open-ended answers in open-ended Video Questions Answering, we decided to formulate EgoSchema within the context of a Zero-shot Multiple Choice task. This task aims to identify the correct answer from a set of given options, without the inclusion of a question. InternVideo has provided finetuned weights for two datasets: MSRVTT and LSMDC. We selected the model finetuned on MSRVTT because it shares greater contextual similarity with EgoSchema.\n\\subsection{\\textbf{Human}}\nTo conduct human benchmarking, we engaged a distinct team of ten employees within the same data annotation company to carry out human benchmarking on our dataset. The answers were randomized and presented in the form of a test. The following are the precise instructions provided to the annotators:\n\\begin{lstlisting}\n- Setting 1: Unlimited setting -- The goal is to get answers as accurately as possible without worrying about time. \n\n- Setting 2: 1 minute timed setting -- In this case, the test taker (annotator) has only 1 minute to spend per question (including watching video/reading text/everything). If they do not have the answer, just guess the best based on their intuition and move on.\n\n- Setting 3: 3 minutes timed setting-- Same as above but with 3 min instead of just one.\n\n- Setting 4: Video -> Text setting -- In this case, the taker is not allowed to read the text before looking at the video and at the video after reading the text. In other words, the test taker can spend as much time as they want to look at the video first and then must move on to answering the question. They cannot go back to the video once they start reading the text. This is an untimed setting -- they can take as much time as they want per question. \n\n- Setting 5: 180 frames setting -- This is the same as the untimed setting except the annotator has access to only 1 frame per second (ie the video feels like a GIF with one frame per second instead of the usual 30 frames per second) -- each video is still 3 minutes long, but it feels more jittery. All instructions remain the same as in an untimed setting. \n\\end{lstlisting}\n\\clearpage\n\\begin{figure}[h!]\n\\centering\n\\includegraphics{figures/ego_stat_1.pdf}\n\n\\includegraphics[width = 0.5\\textwidth]{figures/ego_stat_5.pdf}\n\\includegraphics[width = \\textwidth]{figures/ego_stat_3.pdf}\n\\includegraphics[width = \\textwidth]{figures/ego_stat_4.pdf}\n\\caption{Interactive Version of these statistics visualizations can be found at the \\href{https://public.tableau.com/views/EgoSchema/EGOSchema?:showVizHome=no}{statistics} page on our \\href{https://egoschema.github.io/}{website}.}\n\n\\end{figure}\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{MVBench: A Comprehensive Multi-modal Video Understanding Benchmark}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\n\nWith the rapid development of Multi-modal Large Language Models (MLLMs), \na number of diagnostic benchmarks have recently emerged to evaluate the comprehension capabilities of these models.\nHowever,\nmost benchmarks predominantly assess spatial understanding in the static image tasks,\nwhile \noverlooking temporal understanding in the dynamic video tasks.\nTo alleviate this issue,\nwe introduce a comprehensive \\textbf{M}ulti-modal \\textbf{V}ideo understanding \\textbf{Bench}mark,\nnamely \\textbf{\\BenchName},\nwhich covers \\textbf{20} challenging video tasks that cannot be effectively solved with a single frame.\nSpecifically,\nwe first introduce a novel static-to-dynamic method to define these temporal-related tasks.\nBy transforming various static tasks into dynamic ones, \nwe enable the systematic generation of video tasks that require a broad spectrum of temporal skills, ranging from perception to cognition.\nThen,\nguided by the task definition,\nwe automatically convert public video annotations into multiple-choice QA to evaluate each task.\nOn one hand,\nsuch a distinct paradigm allows us to build \\BenchName\\ efficiently,\nwithout much manual intervention.\nOn the other hand,\nit guarantees evaluation fairness with ground-truth video annotations,\navoiding the biased scoring of LLMs.\nMoreover,\nwe further develop a robust video MLLM baseline,\n\\textit{i.e.}, \n\\textbf{\\ModelName},\nby progressive multi-modal training with diverse instruction-tuning data.\nThe extensive results on our \\BenchName\\ reveal that,\nthe existing MLLMs are far from satisfactory in temporal understanding,\nwhile our \\ModelName\\ largely surpasses these leading models by over \\textbf{15\\%} on \\BenchName.\nAll models and data are available at \\url{https://github.com/OpenGVLab/Ask-Anything}.\n\n\\end{abstract}\\vspace{-0.3cm} \n\\section{Introduction}\n\\label{sec:intro}\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=1.0\\linewidth\n    ]{figs/intro.pdf}\n    \\vspace{-0.7cm}\n    \\caption{\n    \\textbf{Tasks of MVBench.}\n    We define temporal tasks by adapting static image tasks with dynamic evolution.\n    This leads to 20 challenging tasks of video understanding, \n    which cannot be effectively solved within a single frame.\n    For example, \n    ``\\textit{position}'' in an image can be converted into ``\\textit{moving direction}'' through a video.\n    }\n    \\label{fig:intro}\n    \\vspace{-0.5cm}\n\\end{figure}\n\nIn the past few years,\n\\blfootnote{$\\spadesuit$ Interns at Shanghai AI Laboratory. $\\heartsuit$ Corresponding authors.}\nMulti-modal Large Language Models (MLLMs)~\\cite{gpt4v,flamingo,blip2,palme,minigpt4,llava,kosmos,videochat} \nhave gradually driven the advance in vision-language learning, \nby plugging visual encoders within various pretrained LLMs~\\cite{devlin2018bert,palm,chatgpt,llama1,vicuna}. \nWith such a fast development,\nthere is a natural question: \n\\textit{How can we evaluate the comprehension capabilities of these MLLMs?}\nSuch assessment is vital to confirm their design effectiveness and further improve them for a broader understanding of open-world multi-modalities.\n\nIn response to this need,\na number of benchmarks have been launched~\\cite{pope,lvlm_ehub,mme,mmbench,mmvet},\nby evaluating MLLMs with Question Answering (QA) formulation of various perception tasks.\nHowever,\nmost of these benchmarks primarily concentrate on image-based understanding,\nwhere\nall the questions are designed for spatial perception in the static images,\n\\textit{e.g.},\n``\\textit{Is the man on the stage?}'',\nas shown in Fig. \\ref{fig:intro}.\nHence,\nthey suffer from difficulty in assessing temporal evolution in dynamic videos,\nwhich is critical to understanding the procedural activities in our realistic world.\nRecently,\nseveral attempts have tried to evaluate MLLMs on temporal perception in videos~\\cite{seedbench,funqa,perception_test,videochatgpt}. \nBut they either work on the very basic video tasks (\\textit{e.g.}, action recognition and prediction in SEED-Bench~\\cite{seedbench}), \nor focus on the particular domains (\\textit{e.g.}, surprising comprehension in FunQA~\\cite{funqa}) and restricted scenes (\\textit{e.g.}, indoor scenes in Perception Test~\\cite{perception_test}).\nAs a result,\nit is limited to leverage these benchmarks to make a comprehensive evaluation on the temporal understanding skills of MLLMs.\nBesides,\nthey are collected with labor-intensive annotations,\nleading to expensive manual intervention.\nTo tackle these problems, \nwe propose a \\textbf{M}ulti-modal \\textbf{V}ideo understanding \\textbf{Bench}mark (\\textbf{\\BenchName}),\nwhich aims at comprehensively evaluating the temporal perception capabilities of MLLMs in the open world.\nCompared to these existing benchmarks above,\nthere are two distinct designs in our \\BenchName.\n\nFirst,\nwe introduce a novel static-to-dynamic method to systematically define temporal-related tasks,\nby adapting static image tasks with dynamic evolution.\nThis leads to \\textbf{20} challenging tasks of video understanding in the \\BenchName,\nwhich covers a wide range of temporal understanding skills from perception to cognition. \nSpecifically,\nwe use static image tasks in the previous multi-modal benchmarks~\\cite{mme,mmbench} as definition reference.\nThen,\nwe augment the question of these static tasks with temporal context in the video,\n\\textit{e.g.},\nthe \\textit{position} task in the image can be flexibly converted into the \\textit{moving-direction} task in the video \n(``\\textit{Is the man on the stage?}'' $\\rightarrow$ ``\\textit{What direction is the man moving?}'') in Fig. \\ref{fig:intro}. %%%%%%%%%%%%% 前面一段有一个问题的例子。你最好是对应上。这样衔接一致紧密。 \nIn this case,\nwe can effectively convert all these static tasks into the corresponding dynamic tasks,\nwhich cannot be solved without reasoning on the whole video.\n\nSecond,\nguided by the task definition,\nwe design an automatic annotation paradigm to generate multiple-choice QAs for each task,\nby converting \\textbf{11} public video benchmarks with LLMs.\nOn one hand,\nit can largely reduce the cost of expensive human annotations.\nOn the other hand,\nthese 11 benchmarks cover various complex domains and diverse scenes,\nranging from first-person to third-person perspectives, \nand from indoor to outdoor environments.\nHence,\nour \\BenchName\\ is a preferable choice to evaluate the general capability of MLLMs for open-world temporal understanding.\nMore importantly,\nthese benchmarks provide the ground truth for \\BenchName\\,\nwhich guarantees evaluation fairness and accuracy,\navoiding biased scoring of LLMs~\\cite{funqa,videochatgpt}.\n\nFinally,\nwe make a thorough evaluation of various well-known MLLMs on our \\BenchName.  \nSurprisingly,\nthese state-of-the-art image and video MLLMs are far from satisfactory,\nin terms of temporal perception and cognition.\nThis further motivates us to develop a strong video MLLM baseline,\nnamely \\textbf{\\ModelName},\nby bridging LLM with a powerful vision foundation model~\\cite{umt}.\nSubsequently,\nwe introduce a progressive training paradigm with a wide spectrum of multi-modal instructions,\nallowing effective alignment between video and language.\nThe evaluations show that,\nour \\ModelName\\ significantly surpasses the top-performing VideoChat~\\cite{videochat} by over \\textbf{15\\%} accuracy on \\BenchName,\nand also achieves the new state-of-the-art results on video conversation~\\cite{videochatgpt} and zero-shot QA benchmarks~\\cite{msrvtt_qa,activitynet_qa}. \nAll the models and data are publicly available,\nin order to pave the path to general video understanding.\n\\section{Related Works}\n\n\\noindent\\textbf{MLLM.}\nBuilding upon the significant achievements of Large Language Models (LLMs)~\\cite{devlin2018bert,t5,gpt3,Wei2021FinetunedLM,palm}, scholarly interest has increasingly shifted towards the exploration and development of Multi-modal Large Language Models (MLLMs). \nThis shift aims to augment multi-modal understanding and generation capabilities. \nGroundbreaking MLLMs such as Flamingo~\\cite{flamingo} and PaLM-E~\\cite{palme} have seamlessly fused text and vision, setting precedence with their outstanding performances across a range of multi-modal tasks~\\cite{vqa,fickr,msrvtt,okvqa}.\nThe recent open-sourcing of LLMs~\\cite{llama1,llama2,vicuna,glm,internlm} further accelerates the emergence of public MLLMs~\\cite{minigpt4,llava,mmgpt}. \nNotable examples such as LLaVA~\\cite{llava}, MiniGPT-4~\\cite{minigpt4}, and InstructBLIP~\\cite{instructblip} have contributed by proposing a series of visual instruction-tuning data.\nVenturing beyond text and static images, several studies have begun harnessing video modality~\\cite{videochat,videochatgpt,videollama,valley}, tapping into the vast potential of LLMs for video comprehension tasks~\\cite{msrvtt_qa,activitynet_qa,msvd}. \nInnovations like VideoChat~\\cite{videochat}, VideoChatGPT~\\cite{videochatgpt}, and Valley~\\cite{valley} utilize ChatGPT to generate video instruction-tuning data, aiming to enhance instruction-following capabilities.\nIn the \\ModelName, we aim to critically examine the fundamental temporal understanding capabilities of MLLMs, providing valuable design insights for more robust video MLLMs.\n\n\\begin{table*}[tp]\n    \\centering\n    \\setlength\\tabcolsep{4pt}\n    \\resizebox{1.0\\textwidth}{!}{\n        \\begin{tabular}{c|c|c|l}\n        \\Xhline{1.0pt}\n        \\textbf{Spatial} & \\textbf{Temporal} & \\textbf{Source} & \\textbf{Example} \\\\\n        \\Xhline{1.0pt}\n        \\multirow{11}{*}{\\textbf{Action}} & Action & \\multirow{2}{*}{STAR} & \\cellcolor{gray!5}{\\textit{\\darkblue{What happened after the person took the food?}}} \\\\\n        ~ & Sequence & ~ & \\cellcolor{gray!5}{(A) Ate the medicine. (B) Tidied up the blanket. (C) Put down the cup/glass/bottle. (D) Took the box.} \\\\\n        \\hhline{~|-|-|-}\n        ~ & Action & \\multirow{2}{*}{STAR} & \\cellcolor{gray!5}{\\textit{\\darkblue{What will the person do next?}}} \\\\\n        ~ & Prediction & ~ & \\cellcolor{gray!5}{(A) Put down the pillow. (B) Open the door. (C) Take the book. (D) Open the closet/cabinet.} \\\\\n        \\hhline{~|-|-|-}\n        ~ & Action & \\multirow{2}{*}{PAXION\\red{$\\ddag$}} & \\cellcolor{gray!5}{\\textit{\\darkblue{Which one of these descriptions correctly matches the actions in the video?}}}  \\\\\n        ~ & Antonym & ~ & \\cellcolor{gray!5}{(A) not sure (B) scattering something down (C) piling something up} \\\\\n        \\hhline{~|-|-|-}\n        ~ & Fine-grained & \\multirow{2}{*}{MiT V1\\red{$\\ddag$}} & \\cellcolor{gray!5}{\\textit{\\darkblue{What is the action performed by the person in the video?}}}  \\\\\n        ~ & Action & ~ & \\cellcolor{gray!5}{(A) watering (B) leaking (C) pouring (D) planting} \\\\\n        \\hhline{~|-|-|-}\n        ~ & \\multirow{3}{*}{\\makecell[c]{Unexpected\\\\Action}} & \\multirow{3}{*}{FunQA\\red{$\\ddag$}} & \\cellcolor{gray!5}{\\textit{\\darkblue{What unexpected event contributes to the humor in the video?}}}  \\\\\n        ~ & ~ & ~ & \\cellcolor{gray!5}{(A) The man left without dancing. (B) Two women hugged each other at the end.} \\\\\n        ~ & ~ & ~ & \\cellcolor{gray!5}{(C) The man finally danced with the woman. (D) Two men hugged each other unexpectedly.} \\\\\n        \\hline\n        \\multirow{5}{*}{\\textbf{Object}} & Object Existence & CLEVRER & \\cellcolor{gray!5}{\\textit{\\darkblue{Are there any moving green objects when the video ends?}} (A) not sure (B) yes (C) no} \\\\\n        \\hhline{~|-|-|-}\n        ~ & Object Interaction & STAR & \\cellcolor{gray!5}{\\textit{\\darkblue{Which object was tidied up by the person?}} (A) broom (B) cabinet (C) blanket (D) table} \\\\\n        \\hhline{~|-|-|-}\n        ~ & \\multirow{3}{*}{\\makecell[c]{Object\\\\Shuffle}} & \\multirow{3}{*}{\\makecell[c]{Perception\\\\Test}} & \\cellcolor{gray!5}{\\textit{\\darkblue{Where is the hidden object at the end of the game from the person's point of view?}}} \\\\\n        ~ & ~ & ~ & \\cellcolor{gray!5}{(A) Under the first object from the left. (B) Under the third object from the left.}  \\\\\n        ~ & ~  & ~ & \\cellcolor{gray!5}{(C) Under the second object from the left.} \\\\\n        \\hline\n        \\multirow{5}{*}{\\textbf{Position}} & Moving & \\multirow{2}{*}{CLEVRER\\red{$\\ddag$}} & \\cellcolor{gray!5}{\\textit{\\darkblue{What direction is the cyan sphere moving within the video?}}} \\\\\n        ~ &  Direction & ~ & \\cellcolor{gray!5}{(A) The object is stationary. (B) Up and to the right. (C) Down and to the left. (D) Down and to the right.} \\\\\n        \\hhline{~|-|-|-}\n        ~ & \\multirow{3}{*}{\\makecell[c]{Action\\\\Localization}} & \\multirow{3}{*}{\\makecell[c]{Charades-\\\\STA}\\red{$\\ddag$}} & \\cellcolor{gray!5}{\\textit{\\darkblue{During which part of the video does the action `person sitting on a couch' occur?}}} \\\\\n        ~ & ~ & ~ & \\cellcolor{gray!5}{(A) In the middle of the video. (B) At the end of the video.} \\\\\n        ~ & ~ & ~ & \\cellcolor{gray!5}{(C) Throughout the entire video. (D) At the beginning of the video.} \\\\\n        \\hline\n        \\multirow{3}{*}{\\textbf{Scene}} & \\multirow{3}{*}{\\makecell[c]{Scene\\\\Transition}} & \\multirow{3}{*}{MoVQA\\red{$\\ddag$}} & \\cellcolor{gray!5}{\\textit{\\darkblue{What's the right option for how the scenes in the video change?}}} \\\\\n        ~ & ~ & ~ & \\cellcolor{gray!5}{(A) From the reception desk to the conference room. (B) From the kitchen to the dining area.} \\\\\n        ~ & ~ & ~ & \\cellcolor{gray!5}{(C) From the server room to the control center. (D) From the classroom to the library.} \\\\\n        \\hline\n        \\multirow{2}{*}{\\textbf{Count}} & Action Count & \\small{Perception Test} & \\cellcolor{gray!5}{\\textit{\\darkblue{How many times did the person launch objects on the table?}} (A) 3 (B) 2 (C) 4} \\\\\n        \\hhline{~|-|-|-}\n        ~ & Moving Count & CLEVRER & \\cellcolor{gray!5}{\\textit{\\darkblue{How many metal objects exit the scene?}} (A) 2 (B) 3 (C) 1 (D) 0} \\\\\n        \\hline\n        \\multirow{2}{*}{\\textbf{Attribute}} & Moving Attribute & CLEVRER & \\cellcolor{gray!5}{\\textit{\\darkblue{What shape is the moving object when the video begins?}} (A) cylinder (B) sphere (C) cube} \\\\\n        \\hhline{~|-|-|-}\n        ~ & State Change & \\small{Perception Test} & \\cellcolor{gray!5}{\\textit{\\darkblue{Is the lighting device on at any point?}} (A) yes (B) I don't know (C) no} \\\\\n        \\hline\n        \\textbf{Pose} & Fine-grained Pose & \\small{NTU RGB+D\\red{$\\ddag$}} & \\cellcolor{gray!5}{\\textit{\\darkblue{What is the pose performed by the person in the video?}} (A) pick up (B) sit down (C) drop (D) stand up} \\\\\n        \\hline\n        \\textbf{Character} & Character Order & \\small{Perception Test} & \\cellcolor{gray!5}{\\textit{\\darkblue{What letter did the person write first on the paper?}} (A) l (B) v (C) e} \\\\\n        \\hline\n        \\multirow{9}{*}{\\textbf{Cognition}} & \\multirow{2}{*}{\\makecell[c]{Egocentric\\\\Navigation}} & \\multirow{2}{*}{VLN-CE\\red{$\\ddag$}} & \\cellcolor{gray!5}{\\textit{\\darkblue{For an agent following instruction: ``Go left through the door.'' What is the next action it should take?}}}  \\\\ \n        ~ & ~ & ~ & \\cellcolor{gray!5}{(A) Turn left and move forward (B) Move forward (C) Stop (D) Turn right and move forward.}  \\\\ \n        \\hhline{~|-|-|-}\n        ~ & \\multirow{3}{*}{\\makecell[c]{Episodic\\\\Reasoning}} & \\multirow{3}{*}{TVQA} & \\cellcolor{gray!5}{\\textit{\\darkblue{Why did Castle dress like a fairy when he was speaking to Emily?}}} \\\\\n        ~ & ~ & ~ & \\cellcolor{gray!5}{(A) To get her to trust him. (B) He secretly loved fairies. (C) He lost a bet with Emily.} \\\\\n        ~ & ~ & ~ & \\cellcolor{gray!5}{(D) It was dressed like a fairy day at school. (E) Mrs Ruiz made him dress up.} \\\\\n        \\hhline{~|-|-|-}\n        ~ & \\multirow{3}{*}{\\makecell[c]{Counterfactual\\\\Inference}} & \\multirow{3}{*}{CLEVRER} & \\cellcolor{gray!5}{\\textit{\\darkblue{Which of the following will happen if the cylinder is removed?}}} \\\\\n        ~ & ~ & ~ & \\cellcolor{gray!5}{(A) The cyan rubber object and the blue cube collide. (B) The brown cube collides with the metal cube.}  \\\\\n        ~ & ~ & ~ & \\cellcolor{gray!5}{(C) The cyan rubber object and the metal cube collide. (D) The cyan rubber cube collides with the sphere.} \\\\\n        \\Xhline{1.0pt}\n        \\end{tabular}\n    }\n    \\vspace{-0.3cm}\n    \\caption{\n    \\textbf{Task examples of \\BenchName.}\n    The videos are collected from the public datasets, including STAR~\\cite{star}, PAXION~\\cite{paxion}, Moments in Time V1~\\cite{mit}, FunQA~\\cite{funqa}, CLEVRER~\\cite{clevrer}, Perception Test~\\cite{perception_test}, Charades-STA~\\cite{charades_sta}, MoVQA~\\cite{movqa}, NTU RGB+D\\cite{ntu_rgbd}, VLN-CE~\\cite{vln_ce} and TVQA ~\\cite{tvqa}.\n    Tasks requiring QA generation are marked with ``\\red{$\\ddag$}''.\n    More details can be found in Section \\ref{sec:dimension}.\n    }\n    \\label{tab:task_dimension}\n    \\vspace{-0.3cm}\n\\end{table*}\n\\noindent\\textbf{Benchmark.}\nTraditional Vision-Language (VL) benchmarks~\\cite{k400,sth,msrvtt,nextqa,msrvtt_qa} have primarily honed in on specific capabilities like multi-modal retrieval and vision QA. \nThe rise of MLLMs has catalyzed benchmarks designed for assessing integrated VL tasks. \nFor example, LVLM-eHub~\\cite{lvlm_ehub} provides an interactive model comparison platform through image-related queries. \nOther benchmarks such as OwlEval~\\cite{mplug-owl}, MME~\\cite{mme}, SEED-Bench~\\cite{seedbench}, MM-Vet~\\cite{mmvet}, and MMBench~\\cite{mmbench} underscore comprehensive VL skills, introducing evaluation metrics that transcend mere model hierarchies.\nMeanwhile, the video realm showcased benchmarks like Perception Test~\\cite{perception_test}, examining multi-modal video perception and reasoning, and VideoChatGPT~\\cite{videochatgpt} quantifies the capability of dialogue generation from video inputs.\nFunQA~\\cite{funqa} pushes video reasoning limits via counter-intuitive and humorous content.\nIn contrast to the existing benchmarks,\nMVBench sets itself apart by covering a wide range of temporal tasks, \nemphasizing temporally-sensitive videos and efficient use of public annotations, \nand conducting comprehensive evaluations of MLLMs' temporal understanding.\n\\section{\\BenchName}\n\\label{sec:mvpbench}\n\n\\begin{figure*}[thp]\n    \\centering\n    \\includegraphics[width=0.95\\textwidth]{figs/generation_pipeline.pdf}\n    \\vspace{-0.3cm}\n    \\caption{\n    \\textbf{Generation pipeline of \\BenchName.} \n    Within public annotations, \n    data is carefully filtered and relevant multiple-choice QAs are auto-generated.\n    The effective system prompt and efficient answer prompt are employed to guide MLLMs toward precise outputs.\n    }\n    \\label{fig:pipeline}\n    \\vspace{-0.3cm}\n\\end{figure*}\n\nIn this section, \nwe present our \\BenchName\\ in detail. \nWe first design the temporal tasks in Tab. \\ref{tab:task_dimension},\nand then automatically generate multiple-choice QAs for evaluation in Fig. \\ref{fig:pipeline}.\n\n\\subsection{Temporal Task Definition}\n\\label{sec:dimension}\n\nTo design the temporal tasks of \\BenchName, \nwe introduce a concise static-to-dynamic method by adapting static tasks with dynamic goals.\nAs discussed in the introduction,\nmost existing MLLM benchmarks~\\cite{mme,mmbench} focus on spatial understanding with systematical definitions of static image tasks.\nMotivated by this, \nwe propose using these task definitions as references to systematically design temporal tasks, ranging from perception to cognition.\nAs shown in Fig. \\ref{fig:intro}, \nwe begin by summarizing 9 main tasks of spatial understanding from previous benchmarks. \nThen we enrich these image tasks with video context, \ncreating temporal tasks that can not be effectively solved with a single image and require comprehensive video understanding. \nFinally, \nwe define 20 temporal tasks as follows. \nExamples are listed in Tab. \\ref{tab:task_dimension}.\n\n\\textbf{Action.}\n(1) \\textit{Action Sequence:} Retrieve the events occurring before or after a specific action.\n(2) \\textit{Action Prediction:} Infer the subsequent events based on the current actions.\n(3) \\textit{Action Antonym:} Distinguish the correct action from two inversely ordered actions.\n(4) \\textit{Fine-grained Action:} Identify the accurate action from a range of similar options.\n(5) \\textit{Unexpected Action:} Detect surprising actions in videos characterized by humor, creativity, or magic.\n\\textbf{Object.} \n(6) \\textit{Object Existence:} Determine the existence of a specific object during a particular event.\n(7) \\textit{Object Interaction:} Identify the object that participates in a particular event.\n(8) \\textit{Object Shuffle:} Locate the final position of an object in an occlusion game.\n\\textbf{Position.}\n(9) \\textit{Moving Direction:} Ascertain the trajectory of a specific object's movement.\n(10) \\textit{Action Localization:} Determine the time period when a certain action occurs.\n\\textbf{Scene.} \n(11) \\textit{Scene transition:} Determine how the scene transitions in the video.\n\\textbf{Count.}\n(12) \\textit{Action Count:} Calculate how many times a specific action has been performed.\n(13) \\textit{Moving Count:} Calculate how many objects have performed a certain action.\n\\textbf{Attribute.}\n(14) \\textit{Moving Attribute:} Determine the appearance of a specific moving object at a given moment.\n(15) \\textit{State Change:} Determine whether the state of a certain object changes throughout the video.\n\\textbf{Pose.} \n(16) \\textit{Fine-grained Pose:} Identify the accurate pose category from a range of similar options.\n\\textbf{Character.}\n(17) \\textit{Character Order:} Determine the order in which the letters appear.\n\\textbf{Cognition.}\n(18) \\textit{Egocentric Navigation:} Forecast the subsequent action, based on an agent's current navigation instructions.\n(19) \\textit{Episodic Reasoning:} Perform reasoning on the characters, events, and objects within an episode of a TV series.\n(20) \\textit{Counterfactual Inference:} Consider what might happen if a certain event occurs.\n\n\\subsection{Automatic QA Generation}\n\\label{sec:generation}\n\nWith the guidance of temporal task definitions,\nwe next collect and annotate videos for each task.\nSpecifically,\nwe design an automatic QA generation paradigm in Fig. \\ref{fig:pipeline},\nwhich efficiently converts open-sourced video annotations into multiple-choice QAs for evaluating MLLMs.\n\n\\begin{figure*}[thp]\n    \\centering\n    \\includegraphics[width=0.95\\textwidth]{figs/instruction_data.pdf}\n    \\vspace{-0.35cm}\n    \\caption{\n    \\textbf{Instruction-tuning data for \\ModelName.}\n    Co-training of \\ModelName\\  employs both image and video data, with instructions generated by ChatGPT~\\cite{chatgpt}. \n    The resultant dataset comprises 2M samples drawn from 34 diverse datasets across 6 categories. \n    }\n    \\label{fig:instruction}\n    \\vspace{-0.3cm}\n\\end{figure*}\n\n\\textbf{Data Filtration.} \nTo reduce the labor-intensive collection,\nwe propose to select videos from existing benchmarks.\n\\textbf{(1) Video Diversity:}\nTo boost video diversity,\nwe carefully select 11 video datasets (see Tab. \\ref{tab:task_dimension}) that cover a broad spectrum of domains and scenes,\nranging from first-person to third-person perspectives, \nand from indoor to outdoor environments.\n\\textbf{(2) Temporal Sensitivity:}\nTo guarantee that each task is temporal sensitive,\nwe eliminate short clips which generally contain negligible motions, %display negligible activity\nand also delete extremely long videos which often present overly complicated contexts that are hard for evaluation. \nHence,\nwe select videos with intermediate duration, primarily ranging from 5s to 35s.\n\\textbf{(3) Question Difficulty:}\nOverly simple or complex questions may lead to indistinguishable evaluations, due to similar responses.\nTo balance the question difficulty,\nwe design the selection criteria for STAR~\\cite{star} and CLEVRER~\\cite{clevr}.\nFor STAR, \nwe enhance the challenge by randomly shifting the start or end points of the video clips, \nincreasing the complexity of localizing specific events. \nFor CLEVRER, \nwe exclude questions that necessitate more than 10 conditions (\\textit{e.g.,} material, and shape) for describing specific events,\nthus decreasing QA difficulty.\n\n\\textbf{QA Generation.}\nConsidering that not all the annotations of selected datasets follow the multiple-choice QA format,\nwe automatically convert the video annotations into this format via LLMs.\nSpecifically,\nwe first use ChatGPT~\\cite{chatgpt} to generate a question for each video,\nbased on the task definition.\nThen,\nwe create the corresponding answer options as follows.\n\\textbf{(1) Template-Based Construction:} \nFor most questions, \nwe construct the option candidates directly from the ground truth annotations. \nFor example,\nthe candidates for the \\textit{Action Antonym} task contain\nthe \\textit{correct} action, \nits \\textit{opposite} action, \nand a \\textit{not-sure} choice. \nIn the case of the \\textit{Moving Direction} task,\nthe option candidates consist of four directions (\\textit{i.e.}, \\textit{up}, \\textit{down}, \\textit{left}, \\textit{right}) and the \\textit{stationary} state.\n\\textbf{(2) LLM-Based Generation:}\nFor the \\textit{Unexpected Action} task in particular, \nwe leverage ChatGPT for converting open-ended QAs into multiple-choice QA with answer options.\nNote that,\nwe use the multiple-choice format instead of the open-ended one,\nfor evaluation correction and fairness.\nThis is mainly because the open-ended answer has to be scored by LLMs or user studies,\nwhich may either introduce evaluation bias or manual intervention.\nUltimately, \nwe produce 200 multiple-choice QA pairs for each temporal understanding task.\nMore details of QA generation for all the tasks can be found in the appendix.\n \n\n\\textbf{Answer Option Processing.}\nFor all questions,\nwe randomly sample 3 to 5 answer options from the available candidates,\nand shuffle the option order,\nto strengthen the evaluation's robustness.\nAdditionally,\nto prevent the common issue of answer leakage where longer options tend to be correct,\nwe further use LLM to guarantee that all the answer options of a question are of similar and reasonable lengths.\n\n\\subsection{Prompt Design for Evaluation}\n\\label{sec:evaluation}\n\nTo emphasize the temporal sensitivity of MLLMs,\nwe craft a detailed \\textbf{system prompt} for evaluation (see the bottom right of Fig. \\ref{fig:pipeline}). \nThis prompt encourages MLLMs to carefully scrutinize video content to answer questions,\nby paying attention to factors such as \nthe actions and poses of persons, \nand the details and movements of object movements.\n\nMoreover,\nanother significant challenge lies in extracting options from MLLMs' responses. \nMMBench~\\cite{mmbench} attempts to match predictions with multiple option formats. \nIf failed, \nit resorts to ChatGPT~\\cite{chatgpt} to extract options through an intricate design. \nHowever, \nthis way is relatively inefficient, \nyielding an alignment rate of only 87\\% with humans.\nIn contrast,\nour \\BenchName\\ employs a simple approach that guarantees 100\\% rate in option extraction.\nWe enclose the options within parentheses in the questions,\nand use the \\textbf{answer prompt} ``\\textit{Best Option}: ('' to guide MLLMs for option generation. \nResults in Tab. \\ref{tab:ablation_answer_prompt} demonstrate our prompt's effectiveness on various MLLMs, \nallowing us to use accuracy as a reliable metric for evaluation.\n\\section{\\ModelName }\n\\label{sec:mvpchat}\n\nAfter building our \\BenchName,\nwe evaluate a number of popular image and video MLLMs in Tab. \\ref{tab:mvpbench}.\nSurprisingly,\nthe existing MLLMs are far from satisfactory in temporal understanding.\nTo fill the gap,\nwe develop a robust video MLLM baseline,\nwhich is dubbed as \\textbf{\\ModelName}.\n\n\\begin{figure*}[thp]\n    \\centering\n    \\includegraphics[width=0.95\\textwidth]{figs/model.pdf}\n    \\vspace{-0.3cm}\n    \\caption{\n    \\textbf{Progressive multi-modal training of \\ModelName.}\n    Stage1 aligns UMT-L~\\cite{umt}, the visual encoder, with QFormer~\\cite{blip2} to efficiently compress extensive visual inputs. \n    Stage2 extends this connection to incorporate LLM, \n    while Stage3 focuses on effective instruction tuning to enhance model performance.\n    The terms `\\textit{instruction}', `\\textit{question}' and `\\textit{answer}' means `\\texttt{i}', `\\texttt{q}' and `\\texttt{a}' of `\\texttt{QA}' in Fig. \\ref{fig:instruction}.\n    }\n    \\label{fig:model}\n    \\vspace{-0.3cm}\n\\end{figure*}\n\n\\subsection{Instruction-Tuning Data}\n\\label{sec:instruction_data}\n\nPrimarily, \nthe suboptimal performance of MLLMs can be attributed to the limited diversity in instruction-tuning data. \nTo address this issue, \nwe introduce the enriched data as shown in Fig. \\ref{fig:instruction}, \nwhich comprises 2M samples from 34 distinct sources.\nFollowing \\cite{videochat,videollama},\nwe include both image and video data in the instruction set to improve training.\n\nMotivated by M$^3$IT~\\cite{m3it},\nwe reorganize all data samples in a uniform format, as shown on the bottom right of Fig. \\ref{fig:instruction}. \nThere are two keys involved: \\{`\\texttt{image}' or `\\texttt{video}'\\}, and \\{`\\texttt{QA}'\\}.\nThe first key indicates the path to the vision data.\nThe second key represents a list that contains task instruction (`\\texttt{i}') and question-answer(`\\texttt{q}'-`\\texttt{a}').\nMoreover,\ndifferent from M$^3$IT, \nwhich requires researchers to write 10 instructions per dataset, \nwe use ChatGPT to create them, \naccording to \\{dataset description\\}, \\{task description\\}, and \\{instruction example\\} at the top right of Fig. \\ref{fig:instruction}.\nConsequently,\nour whole instruction-tuning data set can be roughly divided into 6 categories as follows:\n\n\\textbf{(1) Conversation} \naims at enhancing multi-turn conversational capabilities. \nWe collect conversation data from LLaVA~\\cite{llava} and VideoChat~\\cite{videochat}.\nTo expand our data, \nwe integrate the caption data from VideoChatGPT~\\cite{videochatgpt} into conversation format based on the video IDs.\n\\textbf{(2) Simple Caption}\naims to improve basic visual description capabilities.\nWe choose the widely used COCO Caption~\\cite{coco} and WebVid~\\cite{bain2021frozen},\ntogether with first-order video captions from YouCook2~\\cite{youcook2}.\n\\textbf{(3) Detailed Caption}\naims at enriching the comprehensive capabilities for understanding visual details.\nWe leverage the detailed caption data from \nMiniGPT-4~\\cite{minigpt4}, \nLLaVA~\\cite{llava} and VideoChat~\\cite{videochat}.\nWe also integrate Paragraph Captioning~\\cite{paragraphs},\nTextCaps~\\cite{textcaps},\nand TextVR~\\cite{textvr},\nwhich require uniquely comprehending text within images and videos.\n\\textbf{(4) VQA} aims to improve visual question-answering capabilities.\nWe include the basic VQA (VQAv2~\\cite{vqa}, GQA~\\cite{gqa}, TGIF-QA~\\cite{tgif_qa} and WebVidQA~\\cite{just_ask}), \nknowledge-based VQA (OK-VQA~\\cite{okvqa}, AOK-VQA~\\cite{aokvqa} and ViQuAE~\\cite{viquae}), \nOCR-based VQA (OCR-VQA~\\cite{ocr_vqa}, TextVQA~\\cite{textvqa}, ST-VQA~\\cite{st_vqa} and DocVQA~\\cite{docvqa}), \nand egocentric VQA from Ego4D~\\cite{ego4d}.\n\\textbf{(5) Reasoning} focuses on enhancing diverse reasoning capacities.\nWe use \nLLaVA-reasoning~\\cite{llava} and CLEVR~\\cite{clevr} for spatial reasoning, \nVisualMRC~\\cite{visualmrc} for reading comprehension, \nNExT-QA~\\cite{nextqa} for temporal reasoning, \nand CLEVRER~\\cite{clevrer} for spatiotemporal reasoning.\n\\textbf{(6) Classification} aims at boosting robustness to object and action recognition.\nWe sample data from ImageNet~\\cite{imagenet}, COCO-ITM~\\cite{coco}, Kinetics-710~\\cite{uniformerv2} and SthSthV2~\\cite{sth}. \n\n\\subsection{Progressive Multi-Modal Training}\n\nAnother critical factor in boosting MLLMs is how to effectively bridge the semantic gap between visual and linguistic representation.\nTo tackle this problem,\nwe adopt a progressive multi-modal training paradigm as shown in Fig. \\ref{fig:model}.\n\n\\textbf{Stage1: Vision-Language Alignment.}\nIn the first stage,\nwe aim at aligning vision and text.\nTo balance efficiency and effectiveness,\nwe freeze the visual encoder and train a flexible QFormer~\\cite{blip2},\nwhich compresses redundant visual tokens into fewer query tokens,\nand align these queries with text tokens by multi-modal losses,\n\\textit{i.e.},\nVision-Text Contrastive learning (VTC), \nVision-Text Matching (VTM), \nand Vision-grounded Text Generation (VTG). \nBut different from~\\cite{blip2},\nwe choose the pretrained UMT-L~\\cite{umt} as our visual encoder,\ndue to its powerful capability of spatial-temporal representation learning.\nMoreover,\nwe train QFormer with only 15M image captions from CC3M~\\cite{cc3m} and CC12M~\\cite{cc12m} but 10M video captions from WebVid-10M~\\cite{bain2021frozen},\nin order to enhance video-language modeling.\n\n\\begin{table*}[tp]\n    \\centering\n    \\setlength\\tabcolsep{1pt}\n    \\resizebox{1.0\\textwidth}{!}{\n        \\begin{tabular}{l|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}\n        \\Xhline{1.0pt}\n        \\textbf{Model} & \\textbf{LLM} & \\textbf{Avg} & \\textbf{AS} & \\textbf{AP} & \\textbf{AA} & \\textbf{FA} & \\textbf{UA} & \\textbf{OE} & \\textbf{OI} & \\textbf{OS} & \\textbf{MD} & \\textbf{AL} & \\textbf{ST} & \\textbf{AC} & \\textbf{MC} & \\textbf{MA} & \\textbf{SC} & \\textbf{FP} & \\textbf{CO} & \\textbf{EN} & \\textbf{ER} & \\textbf{CI} \\\\\n        \\Xhline{1.0pt}\n        Random & - & \\cellcolor{gray!20}{27.3} & 25.0 & 25.0 & 33.3 & 25.0 & 25.0 & 33.3 & 25.0 & 33.3 & 25.0 & 25.0 & 25.0 & 33.3 & 25.0 & 33.3 & 33.3 & 25.0 & 33.3 & 25.0 & 20.0 & 30.9 \\\\\n        \\Xhline{0.8pt}\n        \\multicolumn{23}{l}{\\gray{\\textit{\\textbf{Image MLLMs:} Following~\\cite{instructblip}, all models take \\textbf{4} frames as input, with the output embeddings concatenated before feeding into the LLM.}}} \\\\\n        mPLUG-Owl-I~\\cite{mplug-owl} & LLaMA-7B & \\cellcolor{gray!20}{29.4} & 25.0 & 20.0 & 44.5 & 27.0 & 23.5 & 36.0 & 24.0 & 34.0 & 23.0 & 24.0 & 34.5 & 34.5 & 22.0 & 31.5 & 40.0 & 24.0 & 37.0 & 25.5 & 21.0 & 37.0 \\\\\n        LLaMA-Adapter~\\cite{llamaadapter} & LLaMA-7B & \\cellcolor{gray!20}{31.7} & 23.0 & 28.0 & 51.0 & 30.0 & 33.0 & 53.5 & 32.5 & 33.5 & 25.5 & 21.5 & 30.5 & 29.0 & 22.5 & 41.5 & 39.5 & 25.0 & 31.5 & 22.5 & 28.0 & 32.0 \\\\\n        BLIP2~\\cite{blip2} & FlanT5-XL & \\cellcolor{gray!20}{31.4} & 24.5 & 29.0 & 33.5 & 17.0 & 42.0 & 51.5 & 26.0 & 31.0 & 25.5 & 26.0 & 32.5 & 25.5 & 30.0 & 40.0 & 42.0 & 27.0 & 30.0 & 26.0 & 37.0 & 31.0 \\\\\n        Otter-I~\\cite{otter} & MPT-7B & \\cellcolor{gray!20}{33.5} & 34.5 & 32.0 & 39.5 & 30.5 & 38.5 & 48.5 & 44.0 & 29.5 & 19.0 & 25.5 & 55.0 & 20.0 & 32.5 & 28.5 & 39.0 & 28.0 & 27.0 & 32.0 & 29.0 & 36.5 \\\\\n        MiniGPT-4~\\cite{minigpt4} & Vicuna-7B & \\cellcolor{gray!20}{18.8} & 16.0 & 18.0 & 26.0 & 21.5 & 16.0 & 29.5 & 25.5 & 13.0 & 11.5 & 12.0 & 9.5 & 32.5 & 15.5 & 8.0 & 34.0 & 26.0 & 29.5 & 19.0 & 9.9 & 3.0 \\\\\n        InstructBLIP~\\cite{instructblip} & Vicuna-7B & \\cellcolor{gray!20}{32.5} & 20.0 & 16.5 & 46.0 & 24.5 & 46.0 & 51.0 & 26.0 & 37.5 & 22.0 & 23.0 & 46.5 & \\textbf{42.5} & 26.5 & 40.5 & 32.0 & 25.5 & 30.0 & 25.5 & 30.5 & 38.0 \\\\\n        LLaVA~\\cite{llava} & Vicuna-7B & \\cellcolor{gray!20}{36.0} & 28.0 & 39.5 & 63.0 & 30.5 & 39.0 & 53.0 & 41.0 & 41.5 & 23.0 & 20.5 & 45.0 & 34.0 & 20.5 & 38.5 & 47.0 & 25.0 & 36.0 & 27.0 & 26.5 & 42.0 \\\\\n        \\Xhline{0.8pt}\n        \\multicolumn{23}{l}{\\gray{\\textit{\\textbf{Video MLLMs:} All models take \\textbf{16} frames as input, with the exception of VideoChatGPT, which uses \\textbf{100} frames.}}} \\\\\n        Otter-V~\\cite{otter} & LLaMA-7B & \\cellcolor{gray!20}{26.8} & 23.0 & 23.0 & 27.5 & 27.0 & 29.5 & 53.0 & 28.0 & 33.0 & 24.5 & 23.5 & 27.5 & 26.0 & 28.5 & 18.0 & 38.5 & 22.0 & 22.0 & 23.5 & 19.0 & 19.5 \\\\\n        mPLUG-Owl-V~\\cite{mplug-owl} & LLaMA-7B & \\cellcolor{gray!20}{29.7} & 22.0 & 28.0 & 34.0 & 29.0 & 29.0 & 40.5 & 27.0 & 31.5 & \\textbf{27.0} & 23.0 & 29.0 & 31.5 & 27.0 & 40.0 & 44.0 & 24.0 & 31.0 & 26.0 & 20.5 & 29.5 \\\\\n        VideoChatGPT~\\cite{videochatgpt} & Vicuna-7B & \\cellcolor{gray!20}{32.7} & 23.5 & 26.0 & 62.0 & 22.5 & 26.5 & 54.0 & 28.0 & 40.0 & 23.0 & 20.0 & 31.0 & 30.5 & 25.5 & 39.5 & \\textbf{48.5} & 29.0 & 33.0 & 29.5 & 26.0 & 35.5 \\\\\n        VideoLLaMA~\\cite{videollama} & Vicuna-7B & \\cellcolor{gray!20}{34.1} & 27.5 & 25.5 & 51.0 & 29.0 & 39.0 & 48.0 & 40.5 & 38.0 & 22.5 & 22.5 & 43.0 & 34.0 & 22.5 & 32.5 & 45.5 & 32.5 & 40.0 & 30.0 & 21.0 & 37.0 \\\\\n        VideoChat~\\cite{videochat} & Vicuna-7B & \\cellcolor{gray!20}{35.5} & 33.5 & 26.5 & 56.0 & 33.5 & 40.5 & 53.0 & 40.5 & 30.0 & 25.5 & 27.0 & 48.5 & 35.0 & 20.5 & 42.5 & 46.0 & 26.5 & 41.0 & 23.5 & 23.5 & 36.0 \\\\\n        \\hline\n        \\textbf{\\ModelName$_\\mathbf{text}$} & Vicuna-7B & \\cellcolor{gray!20}{34.7} & 24.5 & 27.0 & 49.5 & 27.0 & 38.0 & 53.0 & 28.0 & 40.0 & 25.5 & 27.0 & 38.5 & 41.5 & 27.5 & 32.5 & 46.5 & 26.5 & 36.0 & 33.0 & 32.0 & 40.0 \\\\\n        \\textbf{\\ModelName} & Vicuna-7B & \\cellcolor{gray!20}{\\textbf{51.1}} & \\textbf{66.0} & 47.5 & \\textbf{83.5} & \\textbf{49.5} & 60.0 & \\textbf{58.0} & \\textbf{71.5} & \\textbf{42.5} & 23.0 & 23.0 & \\textbf{88.5} & 39.0 & \\textbf{42.0} & \\textbf{58.5} & 44.0 & \\textbf{49.0} & 36.5 & \\textbf{35.0} & 40.5 & \\textbf{65.5} \\\\\n        \\Xhline{1.0pt}\n        \\multicolumn{23}{l}{\\gray{\\textit{GPT-4V take \\textbf{16} frames as input, and the resolution is \\textbf{512$\\times$512}, while others use small resolution of \\textbf{224$\\times$224}.}}} \\\\\n        GPT-4V~\\cite{gpt4v} & GPT-4 & \\cellcolor{gray!20}{43.5} & 55.5 & \\textbf{63.5} & 72.0 & 46.5 & \\textbf{73.5} & 18.5 & 59.0 & 29.5 & 12.0 & 40.5 & \\textbf{83.5} & \\textbf{39.0} & 12.0 & 22.5 & 45.0 & 47.5 & 52.0 & 31.0 & \\textbf{59.0} & 11.0 \\\\\n        \\textbf{\\ModelName} & Mistral-7B & \\cellcolor{gray!20}{\\textbf{60.4}} & \\textbf{75.5} & 58.0 & \\textbf{83.5} & \\textbf{50.5} & 60.5 & \\textbf{87.5} & \\textbf{74.5} & \\textbf{45.0} & \\textbf{47.5} & \\textbf{44.0} & 82.5 & 37.0 & \\textbf{64.5} & \\textbf{87.5} & \\textbf{51.0} & \\textbf{66.5} & \\textbf{47.0} & \\textbf{35.0} & 37.0 & \\textbf{72.5} \\\\\n        \\Xhline{1.0pt}\n        \\end{tabular}\n    }\n    \\vspace{-0.3cm}\n    \\caption{\n    \\textbf{Evaluations results on \\BenchName.}\n    Excluding BLIP2 and Otter, \n    all models are built upon \\textbf{LLaMA 1}~\\cite{llama1}\n    for fair comparisons by default. \n    ``\\textbf{Random}'' refers to results from random guesses.\n    ``\\textbf{\\ModelName$_\\mathbf{text}$}'' denotes the model receiving blank videos and excludes LoRA tuning, relying solely on the LLM's capacity for responses.\n    Full results on MVBench can be found at \\url{https://huggingface.co/spaces/OpenGVLab/MVBench_Leaderboard}.\n    Notably, \\textbf{our \\ModelName\\  exceeds the leading models by over 15\\%.\n    Built upon Mistral~\\cite{mistral},\n    our \\ModelName\\  significantly outperforms GPT-4V~\\cite{gpt4v} by 16.9\\%.}}\n    \\label{tab:mvpbench}\n    \\vspace{-0.3cm}\n\\end{table*}\n\\begin{table}[tp]\n    \\centering\n    \\vspace{-0.2cm}\n    \\setlength\\tabcolsep{2.0pt}\n    \\resizebox{1.0\\linewidth}{!}{\n        \\begin{tabular}{l|c|c|c}\n        \\Xhline{1.0pt}\n        \\textbf{Evaluation Aspect} &\n        \\small{VideoChat\\tiny{\\cite{videochat}}} & \\small{VideoChatGPT\\tiny{\\cite{videochatgpt}}} & \\textbf{\\small{\\ModelName}} \\\\\n        \\Xhline{0.8pt}\n        \\small{Correctness of Information} & 2.23 & 2.40 & \\textbf{3.02} \\\\ \n        \\small{Detail Orientation} & 2.50 & 2.52 & \\textbf{2.88} \\\\\n        \\small{Contextual Understanding} & 2.53 & 2.62 & \\textbf{3.51} \\\\\n        \\small{Temporal Understanding} & 1.94 & 1.98 & \\textbf{2.66} \\\\ \n        \\small{Consistency} & 2.24 & 2.37 & \\textbf{2.81} \\\\\n        \\hline\n        \\textbf{Avg} & 2.29 & 2.38 & \\textbf{2.98} \\\\\n        \\Xhline{1.0pt}\n        \\end{tabular}\n    }\n    \\vspace{-0.3cm}\n    \\caption{\\textbf{Results of video conversation benchmark~\\cite{videochatgpt}.}}\n    \\label{tab:videochatgpt_benchmark}\n    \\vspace{-0.4cm}\n\\end{table}\\begin{table}[tp]\n    \\centering\n    \\setlength\\tabcolsep{3.2pt}\n    \\resizebox{1.0\\linewidth}{!}{\n        \\begin{tabular}{l|cc|cc|cc}\n        \\Xhline{1.0pt}\n        \\multirow{2}{*}{\\textbf{Model}}  & \\multicolumn{2}{c|}{\\textbf{MSVD-QA}} & \\multicolumn{2}{c|}{\\textbf{MSRVTT-QA}} & \\multicolumn{2}{c}{\\textbf{ANet-QA}} \\\\\n        ~ & \\textbf{Acc} & \\textbf{Score} & \\textbf{Acc} & \\textbf{Score} & \\textbf{Acc} & \\textbf{Score} \\\\ \n        \\hline\n        VideoLLaMA~\\cite{videollama}  & 51.6 & 2.5 & 29.6 & 1.8 & 12.4 & 1.1 \\\\ \n        VideoChat~\\cite{videochat}  & 56.3 & 2.8 & 45.0 & 2.5 & 26.5 & 2.2 \\\\\n        VideoChatGPT~\\cite{videochatgpt}  & 64.9 & 3.3 & 49.3 & 2.8 & 35.2 & 2.7 \\\\ \n        \\hline\n        \\textbf{\\ModelName} & \\textbf{70.0} & \\textbf{3.9} & \\textbf{54.1} & \\textbf{3.3} & \\textbf{49.1} & \\textbf{3.3} \\\\\n        \\Xhline{1.0pt}\n        \\end{tabular}\n    }\n    \\vspace{-0.3cm}\n    \\caption{\\textbf{Zero-shot video QA results on~\\cite{msrvtt_qa,anet_qa}.}}\n    \\label{tab:zs_vqa}\n    \\vspace{-0.3cm}\n\\end{table}\n\n\\textbf{Stage2: Vision-Language Connection.}\nAfter initial alignment,\nwe then connect the visual encoder with the pretrained LLMs,\nfor building vision-language understanding capabilities.\nFollowing~\\cite{blip2},\nwe apply a linear projection to further transform the query tokens,\nand concatenate the projected tokens with the text tokens into LLM for vision-based caption generation (\\textit{i.e.}, VTG).\nBut different from~\\cite{blip2},\nwe unfreeze the visual encoder for better alignment with LLM.\nIn addition to the aforementioned training data in Stage1, we further introduce 2M image captions (COCO~\\cite{coco}, Visual Genome~\\cite{vg}, and SBU~\\cite{sbu}) and 10M video captions (InternVid~\\cite{internvid}),\nto enrich the caption diversity.\n\n\\textbf{Stage3: Instruction Tuning.}\nIn the final stage, \nwe employ the proposed data in Section \\ref{sec:instruction_data} for instruction tuning. \nTo better align responses with instructions,\nwe use low-rank adaptation~\\cite{lora} on the frozen LLM,\nand tune it along with the visual encoder and QFormer by VTG loss.\nMoreover,\ninspired by~\\cite{instructblip},\nwe integrate instructions \n(\\textit{i.e.}, `\\texttt{i}' of `\\texttt{QA}') into QFormer,\nin order to extract instruction-relevant visual tokens as input to LLM.\nHowever,\ndifferent from~\\cite{instructblip},\nwe do not incorporate questions (\\textit{i.e.}, `\\texttt{q}' of `\\texttt{QA}') into QFormer due to the subpar performances (see appendix.).\n\n\\section{Experiments}\n\\label{sec:exp}\n\n\\textbf{Implementation Details}.\nFor visual encoder and LLM,\nwe apply UMT-L~\\cite{umt} and Vicuna-7B v0~\\cite{vicuna} by default.\nFollowing BLIP2~\\cite{blip2},\nwe deploy QFormer using the pretrained BERT$_{base}$~\\cite{devlin2018bert}.\n32 queries are used in Stage1,\nand \nextra 64 queries are introduced in Stage2 and Stage3 when the visual encoder is unfrozen.\nFor efficient training, \n4-frame videos are processed through 10 epochs in Stage1 and 1 epoch in Stage2. \nTransitioning to Stage3, \nwe shift to 8-frame videos for 3 epochs.\nFor evaluation,\nwe input 16-frame videos with elaborate prompts for better results.\n\n\\subsection{Results on \\BenchName}\nTab.~\\ref{tab:mvpbench} presents the evaluation results on \\BenchName, \nrevealing that current image and video MLLMs are underperforming.\nFor instance, \nVideoChat~\\cite{videochat}, a top-performing video MLLM, only marginally surpasses \\textbf{\\ModelName$_\\mathbf{text}$} by 0.8\\% in average accuracy (35.5\\% \\textit{vs.} 34.7\\%), \nwith the latter generating responses from text alone. \nIn contrast, our \\textbf{\\ModelName}\\  markedly exceeds the leading model by over \\textbf{15\\%}, \nparticularly excelling in categories like action, object, scene, attribute, and pose recognition.\nHowever, \nit struggles in position, count, and character tasks,\nperforming less effectively than \\ModelName$_\\mathbf{text}$,\nwhich could be attributed to the lack of exposure to these tasks during instruction tuning.\n\nSurprisingly,\nbuilt upon Mistral~\\cite{mistral} with SMiT~\\cite{smit} instructions,\nour \\ModelName\\ significantly improves the results,\ndelivering strong performances across various tasks. \n\nFurthermore, we evaluated the powerful GPT-4V~\\cite{gpt4v}. \nThe results show that while GPT-4V achieves satisfactory performance, \ndemonstrating its capacity for temporal understanding, \nour \\ModelName\\ surpasses it, \nincreasing accuracy by 16.9\\%. \nThis further underscores the superiority of our model in handling a broader range of tasks.\n\nMore results on NExT-QA~\\cite{nextqa}, STAR~\\cite{star}, TVQA~\\cite{tvqa}, EgoSchema~\\cite{egoschema} and IntentQA~\\cite{intentqa} can be found in the appendix.\nOur \\ModelName\\ demonstrates robust performance on these complex reasoning tasks.\n\n\\begin{figure}[t]\n    \\centering\n    \\vspace{-0.2cm}\n    \\includegraphics[width=0.95\\linewidth\n    ]{figs/example1.pdf}\n    \\vspace{-0.35cm}\n    \\caption{\n    \\textbf{Qualitative comparison.}\n    \\darkGreen{Green} signifies accurate descriptions, while \\red{red} denotes incorrect or hallucinatory responses.\n    }\n    \\label{fig:example1}\n    \\vspace{-0.3cm}\n\\end{figure}\n\n\\subsection{More Comparisons}\nFollowing~\\cite{videochatgpt},\nwe use ChatGPT~\\cite{chatgpt} to conduct quantitative comparisons among video MLLMs.\n\\textbf{(1) Video Conversation:}\nTab. \\ref{tab:videochatgpt_benchmark} shows the results on the benchmark of \\cite{videochatgpt}.\nCompared with VideoChatGPT~\\cite{videochatgpt},\nour \\ModelName\\  exhibits superior performances across all metrics,\nwith distinct advancements in terms of information correctness as well as context and temporal understanding.\nThis indicates that our \\ModelName\\ is more adept at comprehending both spatial and temporal details\nand providing consistent and reliable responses.\n\\textbf{(2) Zero-Shot Video QA:}\nTab. \\ref{tab:zs_vqa} lists results of typical video QA datasets~\\cite{msrvtt_qa,activitynet_qa}.\nIt is evident that our \\ModelName\\  surpasses all other methods,\nparticularly excelling in understanding long videos in ActivityNet~\\cite{activitynet_qa}.\n\nWe further present a qualitative comparison in Fig. \\ref{fig:example1},\nwhere \\ModelName\\ delivers a precise and thorough response. \nFor more qualitative analyses,\nsee the appendix.\n\n\\begin{table}[tp]\n    \\centering\n    \\setlength\\tabcolsep{4.0pt}\n    \\resizebox{1.0\\linewidth}{!}{\n        \\begin{tabular}{l|l|l|l|l}\n        \\Xhline{1.0pt}\n        \\textbf{Data Source} & \\multicolumn{1}{c|}{\\textbf{Type}} & \\multicolumn{1}{c|}{\\textbf{Task}} & \\multicolumn{1}{c|}{\\textbf{\\#Num}} & \\multicolumn{1}{c}{\\textbf{Avg}} \\\\\n        \\Xhline{0.8pt}\n        VideoChat~\\cite{videochat}\n        & I$+$V & DC$+$R$+$C & 17K & 36.4 \\\\\n        \\hline\n        VideoChatGPT~\\cite{videochatgpt} & V & DC & 100K & 34.3 \\red{$\\downarrow$2.1}  \\\\\n        \\hline\n        \\multirow{4}{*}{\\textbf{Ours}} & I & ALL & 1.1M & 42.1 \\darkGreen{$\\uparrow$5.7} \\\\\n        ~ & V & ALL & 0.9M & 50.5 \\darkGreen{$\\uparrow$14.1}\\\\\n        ~ & I$+$V\\blue{$\\dag$} & ALL & 1.2M  & 50.7 \\darkGreen{$\\uparrow$14.3}  \\\\\n        ~ & \\cellcolor{gray!20}{I$+$V} & \\cellcolor{gray!20}{ALL}  & \\cellcolor{gray!20}{2.0M} & \\cellcolor{gray!20}{\\textbf{51.1} \\darkGreen{$\\uparrow$\\textbf{14.7}}} \\\\\n        \\Xhline{1.0pt}\n        \\end{tabular}\n    }\n    \\vspace{-0.3cm}\n    \\caption{\n    \\textbf{Instruction Data.}\n    ``I'' and ``V'' denote ``Image'' and ``Video'',\n    while ``DC'', ``R'', ``C'' represent ``Detailed Caption'', ``Reasoning'' and ``Conversation''.\n    ``\\blue{$\\dag$}'' symbolizes the version with fewer captions: 100K from COCO~\\cite{coco}, 80K from WebVid~\\cite{bain2021frozen}.\n    }\n    \\label{tab:ablation_data}\n    \\vspace{-0.4cm}\n\\end{table}\n\\begin{table}[tp]\n    \\centering\n    \\setlength\\tabcolsep{6.0pt}\n    \\resizebox{1.0\\linewidth}{!}{\n        \\begin{tabular}{l|l|c|l}\n        \\Xhline{1.0pt}\n        \\textbf{Visual Encoder} &\n        \\textbf{LLM} & \\textbf{LoRA} & \\multicolumn{1}{c}{\\textbf{Avg}} \\\\\n        \\Xhline{0.8pt}\n        \\multirow{2}{*}{EVA-CLIP-g~\\cite{eva_clip}} & \\multirow{2}{*}{Vicuna-7B \\textit{v0}} & \\xmark & 42.4 \\\\\n        ~ & ~ & \\cmark & 45.3 \\darkGreen{$\\uparrow$2.9} \\ \\\\\n        \\hline\n        \\multirow{6}{*}{\\textbf{UMT-L~\\cite{umt}}} & \\multirow{2}{*}{Vicuna-7B \\textit{v0}} & \\xmark & 48.6 \\\\\n        ~ & ~ & \\cmark & 51.1 \\darkGreen{$\\uparrow$2.5}\\ \\\\\n        \\cline{2-4}\n        ~ & Vicuna-13B \\textit{v0} & \\cmark & 51.4 \\\\\n        \\cline{2-4}\n        ~ & \\multirow{2}{*}{Vicuna-7B \\textit{v1.5}} & \\xmark & 48.1 \\\\\n        ~ & ~ & \\cmark & 51.2 \\darkGreen{$\\uparrow$3.1} \\\\\n        \\cline{2-4}\n        ~ & Vicuna-13B \\textit{v1.5} & \\cmark & 51.6 \\\\\n        \\Xhline{1.0pt}\n        \\end{tabular}\n    }\n    \\vspace{-0.3cm}\n    \\caption{\\textbf{Visual Encoder \\& LLM.}\n    Vicuna~\\cite{vicuna} \\textit{v0} and \\textit{v1.5} models are tuned from LLaMA 1~\\cite{llama1} and LLaMA 2~\\cite{llama2} respectively.\n    }\n    \\label{tab:ablation_llm}\n    \\vspace{-0.4cm}\n\\end{table}\n\\begin{table}[tp]\n    \\centering\n    \\setlength\\tabcolsep{2.5pt}\n    \\resizebox{1.0\\linewidth}{!}{\n        \\begin{tabular}{c|c|c|c|l}\n        \\Xhline{1.0pt}\n        \\multicolumn{2}{c|}{\\textbf{Stage2}} & \\multicolumn{2}{c|}{\\textbf{Stage3}} & \\multicolumn{1}{c}{\\multirow{2}{*}{\\textbf{Avg}}} \\\\\n        \\textbf{\\small{Visual Encoder}} & \\textbf{\\small{QFomer}} & \\textbf{\\small{Visual Encoder}} & \\textbf{\\small{QFomer}} & ~ \\\\\n        \\Xhline{0.8pt}\n        \\ice & \\ice & \\ice & \\ice & 38.5 \\\\\n        \\hline\n        \\ice & \\fire & \\ice & \\fire & 47.0 \\darkGreen{$\\uparrow$8.5}  \\\\\n        \\hline\n        \\fire & \\fire & \\ice & \\fire & 47.5 \\darkGreen{$\\uparrow$9.0}  \\\\\n        \\hline\n        \\rowcolor{gray!20} \n        \\fire & \\fire & \\fire & \\fire & \\textbf{51.1} \\darkGreen{$\\uparrow$\\textbf{12.6}}  \\\\\n        \\Xhline{1.0pt}\n        \\end{tabular}\n    }\n    \\vspace{-0.3cm}\n    \\caption{\\textbf{Training Method.} \\ice and\\fire refer to freezing and tuning.\n    We efficiently freeze the visual encoder in Stage1 and LLM in all stages,\n    while tuning the visual encoder and QFormer in Stage2\\&3.\n    }\n    \\label{tab:ablation_training}\n    \\vspace{-0.3cm}\n\\end{table}\n\n\\subsection{Ablations of \\ModelName}\n\\label{sec:ablation}\nIn this section, we conduct comprehensive analyses of the instruction data, model architecture, and prompt designs.\n\n\\textbf{Instruction Data.}\nTab. \\ref{tab:ablation_data} demonstrates that the limited instruction data proposed in VideoChat~\\cite{videochat} (17K) and VideoChatGPT~\\cite{videochatgpt} (100K) is insufficient for temporal understanding. \nAs we increase the data diversity and quantity, \nthe performances are significantly improved, \nwherein video data contributes more than image data (50.5\\% \\textit{vs.} 42.1\\%). \nConsidering the potential redundancy in the simple caption data of COCO~\\cite{coco} and WebVid~\\cite{bain2021frozen}, \nwe randomly compress them. \nThis results in only a minimal impact on performance (50.7\\% \\textit{vs.} 51.1\\%), \nwhile accelerating the tuning by 1.7$\\times$.\n\n\\textbf{Architecture.}\n\\textbf{(1) Visual Encoder:}\nIn Tab. \\ref{tab:ablation_llm}, we first apply EVA-CLIP-g~\\cite{eva_clip} akin to VideoChat, \nwhich achieves 6.9\\% higher accuracy with our instruction data (42.4\\% \\textit{vs.} 35.5\\% for original one in Tab. \\ref{tab:mvpbench}).\nFurther substitutions with UMT-L improve the performance by an additional 6.2\\%,\nwhich demonstrates the effectiveness of our visual encoder.\n\\textbf{(2) LLM:}\nHowever,\nincorporating larger and newer LLMs offers a marginal improvement in the results, \nindicating that \\BenchName\\  relies predominantly on the visual encoder. \nNotably,\nLoRA~\\cite{lora} consistently uplifts the results, \npotentially due to its enhanced capacity for instruction following.\n\n\\begin{table}[tp]\n    \\centering\n    \\setlength\\tabcolsep{2.0pt}\n    \\resizebox{1.0\\linewidth}{!}{\n        \\begin{tabular}{l|c}\n        \\Xhline{1.0pt}\n        \\textbf{System Prompt} & \\textbf{Avg} \\\\\n        \\Xhline{0.8pt}\n        \\textit{\\makecell[l]{Carefully observe the video and choose the best option\\\\ for the question.}} & 49.9 \\\\\n        \\hline\n        \\textit{\\makecell[l]{Carefully watch the video and \\darkGreen{pay attention to the cause,} \\\\ \\darkGreen{sequence of events, and object details and movements.} \\\\Based on your observations, select the best option that\\\\ accurately addresses the question.}} & \\makecell[c]{50.5\\\\\\darkGreen{$\\uparrow$0.6}} \\\\\n        \\hline\n        \\rowcolor{gray!20} \n        \\textit{\\makecell[l]{Carefully watch the video and \\darkGreen{pay attention to the cause} \\\\\\darkGreen{and sequence of events, the detail and movement of} \\\\\\darkGreen{objects and the action and pose of persons.} \\\\Based on your observations, select the best option that \\\\accurately addresses the question.}} & \\textbf{\\makecell[c]{51.1\\\\\\darkGreen{$\\uparrow$1.2}}}\\\\\n        \\Xhline{1.0pt}\n        \\end{tabular}\n    }\n    \\vspace{-0.3cm}\n    \\caption{\\textbf{System Prompt.} It should consider temporal evolution.}\n    \\label{tab:ablation_system_prompt}\n    \\vspace{-0.4cm}\n\\end{table}\n\\begin{table}[tp]\n    \\centering\n    \\setlength\\tabcolsep{3.0pt}\n    \\resizebox{1.0\\linewidth}{!}{\n        \\begin{tabular}{l|l|c|l}\n        \\Xhline{1.0pt}\n        \\textbf{Model} &\n        \\textbf{Answer Prompt} & \\textbf{Hit Ratio} & \\multicolumn{1}{c}{\\textbf{Avg}} \\\\\n        \\Xhline{0.8pt}\n        \\multirow{2}{*}{VideoChat~\\cite{videochat}} & $\\varnothing$ & 78.2\\% & 22.8 \\\\\n        ~ & \\textit{Best option: (} & 100\\% & 35.5 \\darkGreen{$\\uparrow$12.7} \\\\\n        \\hline\n        \\multirow{2}{*}{VideoChatGPT~\\cite{videochatgpt}} & $\\varnothing$ & 64.6\\% & 22.0 \\\\\n        ~ & \\textit{Best option: (} & 100\\% & 32.8 \\darkGreen{$\\uparrow$10.8} \\\\\n        \\hline\n        \\multirow{2}{*}{\\textbf{\\ModelName}} & $\\varnothing$ & 96.4\\% & 50.1 \\\\\n        ~ & \\cellcolor{gray!20}{\\textit{Best option: (}} & \\cellcolor{gray!20}{100\\%} & \\cellcolor{gray!20}{51.1 \\darkGreen{$\\uparrow$1.0}} \\\\\n        \\Xhline{1.0pt}\n        \\end{tabular}\n    }\n    \\vspace{-0.3cm}\n    \\caption{\\textbf{Answer Prompt.} \n    `$\\varnothing$' indicates directly matching the option within responses, similar to~\\cite{mmbench}.\n    Our simple yet effective prompt enhances response precision across various MLLMs.\n    }\n    \\label{tab:ablation_answer_prompt}\n    \\vspace{-0.3cm}\n\\end{table}\n\\textbf{Training Method.}\nInitially, \nwe tune only the linear projection while freezing the visual encoder and QFormer as in MiniGPT-4~\\cite{minigpt4},\nbut it yielded subpar results in Tab. \\ref{tab:ablation_training}. \nBy unfreezing QFormer as ~\\cite{instructblip}, \nwe achieve an 8.5\\% performance boost. \nFurther, when we unfreeze the visual encoder, \nresults consistently improved, \nemphasizing the value of more learnable parameters for visual adaptation.\n\n\\textbf{Prompt Design.}\nTab. \\ref{tab:ablation_system_prompt} reveals that a comprehensive \\textit{system prompt}, which underscores the task requirement, enhances task completion effectiveness.\nDifferent from the unstable ChatGPT-extracting methods~\\cite{mmbench} and more time-consuming log-likelihood comparisons~\\cite{seedbench}, \nwe apply a simple yet effective \\textit{answer prompt} to extra the options.\nResults in Tab. \\ref{tab:ablation_answer_prompt} demonstrate that it accurately targets the option and enhances response precision across various MLLMs.\nMore importantly,\n\\ModelName\\ follows the instructions better to return options even without the prompt.\n\n\\section{Conclusion}\n\\label{sec:conclusion}\n\nThis paper introduces \\BenchName, \na comprehensive benchmark for evaluating the temporal understanding capabilities of MLLMs. \nMoreover,\nwe propose a robust video MLLM baseline,\n\\ModelName,\noutperforming the leading models by over 15\\% on \\BenchName.\nOur extensive analyses further direct the designs of MLLMs for temporal understanding.\\subsection*{Acknowledgement}\nThis work was supported in part by the National Key R\\&D Program of China (No. 2022ZD0160505), and the National Natural Science Foundation of China under Grant (62272450, 62076119).\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{MLVU: Benchmarking Multi-task Long Video Understanding}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\n   The evaluation of Long Video Understanding (LVU) performance poses an important but challenging research problem. Despite previous efforts, the existing video understanding benchmarks are severely constrained by several issues, especially the insufficient lengths of videos, a lack of diversity in video types and evaluation tasks, and the inappropriateness for evaluating LVU performances. To address the above problems, we propose a new benchmark called MLVU (Multi-task Long Video Understanding Benchmark) for the comprehensive and in-depth evaluation of LVU. MLVU presents the following critical values: \\textit{1)} The substantial and flexible extension of video lengths, which enables the benchmark to evaluate LVU performance across a wide range of durations. \\textit{2)} The inclusion of various video genres, e.g., movies, surveillance footage, egocentric videos, cartoons, game videos, etc., which reflects the models' LVU performances in different scenarios. \\textit{3)} The development of diversified evaluation tasks, which enables a comprehensive examination of MLLMs' key abilities in long-video understanding. The empirical study with 23 latest MLLMs reveals significant room for improvement in today's technique, as all existing methods struggle with most of the evaluation tasks and exhibit severe performance degradation when handling longer videos. Additionally, it suggests that factors such as context length, image-understanding ability, and the choice of LLM backbone can play critical roles in future advancements. We anticipate that MLVU will advance the research of long video understanding by providing a comprehensive and in-depth analysis of MLLMs. \n\n\\end{abstract}\n\\vspace{-0.3cm}\\vspace{-0.3cm}\n\\section{Introduction}\n\\label{intro}\n\nLarge language models (LLMs) are growing into a general solution for numerous AI tasks \\cite{brown2020language,touvron2023llama}. In recent years, it becomes increasingly emphasized to extend LLMs with multi-modal capabilities and thus bring the Multi-modal LLM, namely, MLLM. Remarkably, it has been made possible for today's MLLMs to perceive information in texts, images, videos, etc., and solve complicated problems in physical environments \\cite{achiam2023gpt,team2023gemini}. Along with the development of MLLMs, new benchmarks are continuously created to facilitate comprehensive and in-depth analysis of MLLMs \\cite{yue2023mmmu,liu2023mmbench,fu2023mme,mvbench2023}. \n\nHowever, it remains a great challenge to evaluate the MLLMs' long-video understanding (LVU) performances given the following limitations. Firstly, the majority of existing video understanding benchmarks are made up of short videos \\cite{msrvtt2016,mvbench2023,seedbench2023,videobench2023,cvrr-2024}, whose lengths can be merely a few seconds. As a result, they are insufficient to reflect the MLLMs' long-video understanding capabilities. Secondly, there is a notable lack of diversity in both video genres and evaluation tasks. Existing benchmarks often concentrate on a single video type, such as egocentric videos \\cite{ego4d2022, egoschema2023}, or focus on one specific task, like captioning \\cite{msrvtt2016}. These limitations hinder comprehensive evaluation of LVU capabilities. Last but not least, many previous evaluation tasks are not properly designed for LVU, as they can be solved without using the complex information from long videos. For example, many questions are simply about one single frame in the long videos \\cite{moviechat2023, zhang2024longva}. Besides, numerous others are about popular movies and celebrities \\cite{llama-vid2023, videomme2024}, which can be answered directly by MLLMs based on the textual prompts. \n\nConceptually, MLLMs are expected to handle any type of long video and accomplish any related tasks. Therefore, the evaluation of LVU should emphasize two important properties: \\textit{length} and \\textit{diversity}. Furthermore, it is crucial that the evaluation tasks are specifically designed to leverage the complex information inherent in long videos, addressing the shortcomings of previous benchmarks. Based on these principles, we propose a novel benchmark called \\textbf{MLVU} (\\uline{M}ult-task \\uline{L}ong \\uline{V}ideo \\uline{U}nderstanding Benchmark), which presents the following critical advantages.\n\n\\begin{table*}[t!]\n\\centering\n\\vspace{5pt}\n\\renewcommand{\\arraystretch}{1.0} % Slightly reduced row height\n\\begin{adjustbox}{max width=\\textwidth}\n\\begin{tabular}{l>{\\centering\\arraybackslash}m{1.2cm}>{\\centering\\arraybackslash}m{1.2cm}>{\\centering\\arraybackslash}m{1.2cm}>{\\centering\\arraybackslash}m{1.2cm}>{\\centering\\arraybackslash}m{1.2cm}>{\\centering\\arraybackslash}m{1.5cm}>{\\centering\\arraybackslash}m{1.5cm}>{\\centering\\arraybackslash}m{1.5cm}>{\\centering\\arraybackslash}m{1.8cm}}\n\\toprule\n\\textbf{Benchmarks} & \\textbf{\\#Videos} & \\textbf{\\#QA Pairs} & \\textbf{Len. (s)} & \\textbf{Close-Ended} & \\textbf{Open-Ended} & \\textbf{Various Genres} & \\textbf{Multi-Level} & \\textbf{Multi-Dimension} & \\textbf{Referring QA} \\\\\n\\midrule\nNExT-QA~\\cite{nextqa2021} & 1,000 & 8,564 & 39.5 & \\checkmark & \\checkmark & \\checkmark & \\crossmark & \\crossmark & \\crossmark \\\\\nTVQA~\\cite{tvqa2018} & 15,253 & 15,253 & 11.2 & \\checkmark & \\crossmark & \\crossmark & \\crossmark & \\crossmark & \\crossmark\\\\\nMSRVTT-QA~\\cite{msrvtt2016} & 2,900 & 72,821 & 15.2 & \\checkmark & \\crossmark & \\crossmark & \\crossmark & \\crossmark  &\\crossmark \\\\\nMVBench~\\cite{mvbench2023} & 3,641 & 4,000 & 16.0 & \\checkmark & \\crossmark & \\checkmark & \\crossmark & \\crossmark & \\crossmark \\\\\n\\midrule\nMovie101~\\cite{movie101-2023} & 101 & - & 6144 & \\crossmark & \\checkmark & \\crossmark & \\crossmark & \\crossmark & \\crossmark \\\\\nEgoSchema~\\cite{egoschema2023} & 5,063 & 5,063 & 180 & \\checkmark & \\crossmark & \\crossmark & \\crossmark & \\crossmark & \\crossmark \\\\\nMovieChat-1K~\\cite{moviechat2023} & 130 & 1,950 & 500 & \\checkmark & \\checkmark & \\crossmark & \\crossmark & \\checkmark & \\crossmark \\\\\nVideo-MME$^*$~\\cite{videomme2024} & 900 & 2,700 & 1024 & \\checkmark & \\crossmark & \\checkmark & \\checkmark & \\crossmark & \\crossmark \\\\\nLongVideoBench$^*$~\\cite{wu2024longvideobench} & 3,763 & 6,678 & 473 & \\checkmark & \\crossmark & \\checkmark & \\checkmark & \\crossmark & \\checkmark \\\\\n\\midrule\n\\textbf{MLVU}  & 1,730 & 3,102 & 930 & \\checkmark & \\checkmark & \\checkmark & \\checkmark & \\checkmark & \\checkmark \\\\\n\\bottomrule\n\\end{tabular}\n\\end{adjustbox}\n\\vspace{-0.2cm}\n\\caption{Comparison of MLVU with existing benchmarks, including the number of videos (\\textbf{\\#Videos}), number of QA pairs (\\textbf{\\#QA pairs}), average video length (\\textbf{Len.}), presence of \\textbf{Close-Ended} tasks, presence of \\textbf{Open-Ended} tasks, inclusion of various video genres (\\textbf{Various Genres}), coverage of multiple duration levels (\\textbf{Multi-Level}), inclusion of multiple dimensions of LVU tasks (\\textbf{Multi-Dimension}), and questions involving local information with clear referring context rather than direct timestamps~\\cite{moviechat2023} or well-known narrative elements~\\cite{movienet2020,llama-vid2023} (\\textbf{Referring QA}). The first block represents short video understanding benchmarks, and the second block represents long video understanding benchmarks. $^*$ denotes work concurrent with MLVU.} \n\\vspace{-0.5cm}\n\\label{tab:comparison}\n\\end{table*}\n\n\\begin{itemize}%[leftmargin=5.0mm]\n     \\item \\textbf{It makes a substantial extension for the video length.} MLVU is created based on long videos of diversified lengths, \\textit{ranging from 3 minutes to 2 hours}. The average video length is about 15 minutes, which makes it much longer than most of the existing benchmarks. Additionally, each video is further segmented so that evaluation tasks can be created w.r.t. different video clips (e.g., summarization for the first 3 minutes, the first 6 minutes, and the entire duration of the video). Therefore, it is able to flexibly evaluate the MLLMs' performance across different video lengths. \n\n     \\item \\textbf{It encompasses a wide variety of video genres.} MLVU includes diverse real-world videos, such as movies, life records, and egocentric videos. Additionally, it features typical simulated videos like games and cartoons. This diversity allows for a comprehensive assessment of MLLMs' performance across various application scenarios.\n\n     \\item \\textbf{It introduces diversified evaluation tasks tailored for LVU.} MLVU comprises 9 distinct tasks that collectively assess a wide range of MLLMs' LVU capabilities. On one hand, it includes both \\textit{multiple-choice and open-ended generation} tasks, reflecting the models' performance in handling different task formats. On the other hand, some tasks are designed to leverage \\textit{global information from entire videos}, while others require the use of \\textit{specific local information from certain clips}. Moreover, all questions involving local information are annotated with unambiguous context, requiring MLLMs to accurately locate or infer the appropriate clips within long videos.\n\\end{itemize}\n\nTable~\\ref{tab:comparison} shows that MLVU provides a more comprehensive evaluation of LVU compared to existing and concurrent benchmarks. We extensively investigate 23 popular MLLMs with MLVU, which brings in several critical insights. Firstly, \\textit{long-video understanding remains a technically challenging problem for the existing MLLMs}. While GPT-4o\\footnote{https://openai.com/index/hello-gpt-4o/} achieves the leading performance in the experiment, it only attains an average score of 54.5\\% in multi-choice tasks. All methods struggle with tasks requiring fine-grained information from entire videos, such as action counting, ordering, and summarization. \nSecondly, \\textit{recent open-source long video MLLMs have made significant strides in LVU}~\\cite{zhang2024longva,fei2024videoccam,shu2024videoxl}. These advancements have improved the models' capability to process extended visual sequences, thereby closing the gap with leading proprietary models in recent months.\nFinally, \\textit{the empirical results underscore influential factors in LVU}, such as the extension of context length, the improvement of image understanding ability, and the utilization of strong LLM-backbones. In addition to the benchmark's overall conclusion, individual tasks enable fine-grained analysis of MLLMs' performances in each specialized aspects. Therefore, we anticipate the benchmark to assist in improving MLLMs' long-video understanding capabilities by providing insights into their current strengths and weaknesses. \n\n\\vspace{-0.15cm}\n\\section{Related Work}\n\\vspace{-0.15cm}\n\n\\begin{figure*}[h!]\n    \\centering\n    \\vspace{-0.3cm}\n    \\includegraphics[width=1\\textwidth]{figures/statistic.pdf}\n    \\vspace{-0.5cm}\n    \\caption{Statistical Overview of our MLVU benchmark. \\textbf{Left:} Video genres included in MLVU; \\textbf{Top Right:} Distribution of video duration; \\textbf{Bottom Right:} Task types and their counts in MLVU.}    \\label{fig:statistic}\n    \\vspace{-0.5cm}\n\\end{figure*}\n\n\\paragraph{Multimodal Large Language Models.} Multimodal large language models (MLLMs) have attracted significant interest from both academia and industry. Recent advancements in this field have been achieved by integrating LLM backbones with visual encoders and adapters, and fine-tuning the entire architecture through visual instruction tuning~\\cite{llava2023, minigpt4-2023, internvl-1.5-2024}. Based on the same philosophy, MLLMs have been further developed for video processing using video instruction datasets and specialized video adapters~\\cite{videollama, videochatgpt2023, videochat2023, mplug-owl-2023, videollava2023, mvbench2023}. However, most existing models are optimized for short videos, typically under one minute, due to the difficulty in establishing sufficient context for longer videos. \nTo address this challenge, researchers have explored compact video representations or extended the context length of MLLMs. For instance, LLaMa-Vid~\\cite{llama-vid2023} compresses each video frame into two tokens, enabling the model to handle videos several hours long. Methods like MovieChat~\\cite{moviechat2023} and MA-LMM~\\cite{malmm2024} introduce specialized memory components for recursive video processing. Furthermore, approaches such as LWM~\\cite{liu2024world}, LongVA~\\cite{zhang2024longva}, and Video-XL~\\cite{shu2024videoxl} are designed to extend the context length of MLLMs, facilitating the processing of longer video inputs. Additionally, it is also explored to make selective usage of frames or clips from long videos based on retrievers or agents~\\cite{R-VLM-2023, R2A2023, videoagent2024}. Despite these progresses, it remains an open problem for MLLMs to effectively handle long videos. \n\n \n\n\\vspace{-10pt}\n\n\\paragraph{Video Understanding Benchmarks.}\n \nWith the unprecedented interest in MLLMs, the creation of benchmarks for these models has become increasingly emphasized (as advanced by MMMU \\cite{yue2023mmmu}, MME \\cite{fu2023mme}, and many other pioneering works). In video understanding, the research community has made significant efforts as well, particularly for short videos. There are specialized benchmarks for temporal perception~\\cite{activitynetqa2019, star2021}, action understanding~\\cite{star2021, paxion2023}, video classification~\\cite{kinetics-2017}, video reasoning~\\cite{funqa2023, nextqa2021}, and video captioning~\\cite{msrvtt2016, howto100m2019}. Recently, MVBench~\\cite{mvbench2023} provides a comprehensive short-video benchmark to evaluate general capabilities via question-answering. \nFor long video understanding, people seek to leverage long-form videos, like movies, to create benchmarks. For example, LLaMA-Vid~\\cite{llama-vid2023} developed a movie question-answering dataset based on MovieNet~\\cite{movienet2020}. Despite using long videos, many questions focus on well-known narrative elements, allowing them to be answered without analyzing the video's content. In contrast, MovieChat~\\cite{moviechat2023} avoids specific character names or plot details in its questions. However, since each question provides a specific timestamp, the tasks can be reduced to short-video or image understanding problems. Beyond movies, there are task-specific benchmarks like EgoSchema~\\cite{egoschema2023}, which presents video reasoning tasks using first-person footage from Ego4D~\\cite{ego4d2022}. These specialized benchmarks, however, focus on a single aspect of MLLMs rather than offering a comprehensive analysis of long video understanding. Therefore, it is essential to develop a comprehensive benchmark with carefully designed tasks to effectively evaluate MLLMs' capabilities in understanding long videos. \n\\section{MLVU: Multi-task Long Video Understanding Benchmark}\n\\label{sec:dataset}\n\nIn this section, we start with an overview of MLVU, which highlights its constitution and explains its values over the previous works. Then, we discuss how each evaluation task is constructed in MLVU. \n\n\\subsection{Overview}\n\\label{sec:benchoverview}\n\nMLVU is a multi-task benchmark consisting of 3,102 questions across 9 categories, specifically designed for long video understanding. It is divided into a dev set and a test set, containing 2,593 and 509 questions, respectively. The benchmark is distinguished by the following features.   \n\n\\textbf{Diversified Video Categories.} MLVU offers a comprehensive collection of videos across various categories (Figure \\ref{fig:statistic} Left). These include typical real-world videos such as movies, documentaries, TV series, egocentric videos, life records, sports, tutorials, and surveillance footage. Additionally, it features significant simulated videos from animated series and game videos.\n\n\\textbf{Substantial Extension of Video Length.} MLVU is made up of videos of diversified lengths, spanning from 3 min to more than 2 hours (Figure \\ref{fig:statistic} Top Right). Besides, each video is further partitioned as incremental segments, e.g., the first 3 min, the first 6 min, and the entire video, where tasks are created for each individual segment. Thus, the MLLMs can be flexibly evaluated across different video lengths. \n\n\\textbf{Diversified Evaluation Tasks.} MLVU also provides a diverse array of evaluation tasks, which are closely aligned with the common visual capabilities of MLLMs, such as reasoning, captioning, recognition, perception, and summarization (Figure \\ref{fig:statistic} Bottom Right). All the tasks are tailored for LVU. That is to say, the tasks need to be solved based on the in-depth understanding of video. Some of tasks are to examine whether the global information from the entire video can be effectively utilized (holistic LVU); while others focus on whether the MLLMs can make precise usage of proper local information within the long video (detail LVU). Additionally, both multi-choice and free-form generation tasks are included in MLVU, which help to examine MLLMs' capabilities in handling different task formats. \n\n\\subsection{Construction of MLVU}\n\\label{sec:MLVU-construction}\n\nThe evaluation tasks of MLVU can be categorized into three types: 1) \\textit{holistic LVU}, which needs to be solved by making use of the global information from the entire video; 2) \\textit{single-detail LVU}, which relies on leveraging one critical plot within the long video; and 3) \\textit{multi-detail LVU}, which necessitates the joint utilization of multiple plots within the long video. The construction process of MLVU is discussed w.r.t the above three categories. To facilitate the discussion, we define \\textit{ULVC} (Universal Long Video Collection) as the universal collection of long videos from various sources (more details about ULVC are presented in \\Cref{appendix:ulvc}).\n\n\\subsubsection{Holistic LVU}\n\\textbf{Topic Reasoning (TR).} The topic reasoning task requires MLLMs to respond to questions about the principal subject of a long video, as shown with Figure \\ref{fig:pipeline} (a). This includes elements such as the video's genre, pivotal events, or primary settings. %Videos for TR tasks are sourced from movies, documentaries, animated series, egocentric videos, and game videos from ULVC. \nAll questions and answers undergo manual annotation\\footnote{Detailed information and annotation guidelines for annotators are presented in \\Cref{appendix:anno-detail}.}, resulting in a total of 355 questions. TR tasks are formatted as multiple-choice questions, with the model's performance assessed based on accuracy.  \n\n\\noindent\\textbf{Anomaly Recognition (AR).} The anomaly recognition task involves identifying the anomalous behavior within a surveillance footage (Figure \\ref{fig:pipeline} b). We leverage the surveillance video clips from UCF Crime dataset \\cite{ucfcrime-2018} for this task. The selected video clips are longer than three minutes. We create 239 questions based on the original annotations provided by the dataset. The AR task is also conducted in the multiple-choice format, whose performance is measured by accuracy. \n\n\\noindent\\textbf{Video Summarization (VS).} This task requires MLLMs to summarize the key events in a long video (Figure \\ref{fig:pipeline} c). We select the narrative-rich videos from ULVC for this task, including movies, TV series, documentaries, life records, and animated series. There are 257 selected videos in total, whose summaries are manually annotated. During evaluation, the MLLMs are prompted with \"Please summarize the main content of this video\". We employ GPT-4 to assess the generated summaries by comparing with the annotation results. Details about annotation and evaluation are presented in Appendix \\ref{sub-appendix:anno-detail-VS} and \\ref{sub-appendix:evaluation_metric}. \n\n\\begin{figure*}[thp]\n    \\centering\n    \\includegraphics[width=0.86\\textwidth]{figures/task_example_efficient.png}\n    \\caption{Examples of MLVU. There are nine tasks designed to evaluate the \\textit{holistic}, \\textit{single-detail}, and \\textit{multi-detail LVU} capabilities of MLLMs. The MLLMs are asked to solve the problem (with the ground-truth answers marked in blue) based on the long video input and textual prompt. For multiple-choice questions, we set 4 candidates in the dev set and 6 candidates in the test set.} \n    \\label{fig:pipeline}\n\\end{figure*}\n\n\\subsubsection{Single-Detail LVU}\n\n\\textbf{Needle Question-Answering (NQA).} Needle-In-the-Haystack-Search (NIHS) is a popular evaluation task for long-context LLM \\cite{liu2024lost}. Taking the inspiration from NIHS, we create Needle Question-Answering (NQA), shown as Figure \\ref{fig:pipeline} (d). In this task, the MLLM is required to answer a question related to a specific segment (referred as \\textit{needle}) within a long video (referred as \\textit{background video}). The needles are short video clips sampled from WebVid~\\cite{webvid-2021} and Clevrer~\\cite{clevrer-2019}, while the background videos are sampled from our ULVC. The needle is randomly inserted into the background video, where a question-answer pair is annotated. By incorporating necessary details, the question can always correspond to the needle without ambiguity. During evaluation, the MLLM needs to infer the location of the needle based on the details provided in the question, and solve the problem on top of the needle's information. The NQA task is structured as multiple-choice, whose performance is measured by accuracy. \n\n\\noindent\\textbf{Ego Reasoning (ER)}. Ego-centric videos capture a series of consecutive actions from a first-person perspective. The MLLM needs to reason for a question about a specific behavior in the video, e.g., predicting for the event which is correlated or satisfies a certain causal relationship with the behavior (\\Cref{fig:pipeline} e). Both videos and QA annotations are collected from the NLQ task of Ego4D \\cite{ego4d2022}. The ER task is structured as multiple-choice, with a total of 405 questions created for this task. \n\n\\noindent\\textbf{Plot Question-Answering (PQA)}. In this task, the MLLM needs to reason for questions about a plot in a narrative video, shown as \\Cref{fig:pipeline} (f). The video is sampled from the movies, TV series, and animated series in our ULVC. There are 589  question-answer pairs created by manual annotation. During annotation, the human annotators are asked to only provide necessary details about the plot but not to suggest any objective hints, e.g., the two characters in the example video are referred as cat and mouse, rather than Tom and Jerry. Therefore, it can prevent the question from being short-cut by the MLLM's common-sense knowledge (more details about PQA can be found in the Appendix~\\ref{sub-appendix:anno-detail-PQA}). \n\n\\noindent\\textbf{Sub-Scene Captioning (SSC).} In this task, the MLLM needs to generate the caption for a sub-scene in a long video. The long videos in SSC are sampled from the Movie101 dataset \\cite{movie101-2023}, while the questions and answers are manually annotated. During annotation, the human annotator is asked to provide a detailed description for the sub-scene as the ground-truth answer. Besides, they need to offer necessary clues in their questions such that the referred sub-scenes can be identified without ambiguity. During evaluation, we employ GPT-4~\\cite{achiam2023gpt} to measure the quality of caption in comparison with the ground-truth. Details about annotation and evaluation are presented in Appendix~\\ref{sub-appendix:anno-detail-SSC} and \\ref{sub-appendix:evaluation_metric}. \n\n\\subsubsection{Multi-Detail LVU}\n\n\\textbf{Action Order (AO).} In this task, the MLLM needs to predict the right order for a sequence of actions (\\Cref{fig:pipeline} h). The actions are presented by short video clips, called \\textit{probes}. The probes are formulated in two different ways. One is made up of clips from the Kinetics dataset \\cite{kinetics-2017}, where each clip represents a distinct action. The other one is from the consecutive clips of an action in the ActivityNet-Caption dataset~\\cite{activitynet-caption2017}. The probes are inserted into a long \\textit{background} video, which is sampled from ULVC. There are 329 AO questions in total. The task is structured as a multiple-choice prblem, where the right order is selected from the misleading options provided by the annotator. \n\n\\noindent\\textbf{Action Count (AC).} This task requires the MLLM to count the occurrences of an action within a long video (\\Cref{fig:pipeline} i). Each action corresponds to multiple short \\textit{probe} clips sampled from the Kinetics dataset \\cite{kinetics-2017}. The probes of an action are inserted into a long \\textit{background} video sampled from ULVC. We also perform manual examination to ensure that the inserted action does not exist in the original background video. A total of 266 evaluation instances have been created. The AC task is structured as a multiple-choice problem, with performance measured by accuracy. \n\n\\renewcommand{\\arraystretch}{1.2}\n\\begin{table*}[t]\n\\small\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{lccccccccccccc}\n\\toprule\n\\specialrule{0em}{0.3pt}{0.3pt}\n\\multirow{2}{*}{\\textbf{Methods}} & \\multirow{2}{*}{\\textbf{Date}} & \\multirow{2}{*}{\\textbf{Input}}  & \\multicolumn{3}{c}{\\textbf{Holistic}} & \\multicolumn{4}{c}{\\textbf{Single Detail}}    & \\multicolumn{2}{c}{\\textbf{Multi Detail}} & \\multirow{2}{*}{\\textbf{M-Avg}} & \\multirow{2}{*}{\\textbf{G-Avg}} \\\\ \n\\specialrule{0em}{0.3pt}{0.3pt}\n\\cmidrule(r){4-6} \\cmidrule(r){7-10} \\cmidrule(r){11-12}\n\\specialrule{0em}{0.1pt}{0.1pt}\n&~&~& TR   &AR & VS$^{*}$  & NQA &ER &PQA\n& SSC$^{*}$   & AO &AC \\\\     \n\\specialrule{0em}{0.3pt}{0.3pt}\n\\hline\n\\rowcolor[HTML]{eff0f1}Full mark &-- &-- &100 &100  &10  &100  &100  &100  &10  &100  &100  &100  &10    \\\\\n\\rowcolor[HTML]{eff0f1}Random &-- &-- &16.7 &16.7  &--  &16.7  &16.7  &16.7  &--  &16.7  &16.7  &16.7  &--    \\\\\n\n\\hline\n \\rowcolor[HTML]{E3F8F8}\\multicolumn{14}{l}{\\gray{\\textit{\\textbf{Image MLLMs}}}}\\\\\n \\rowcolor[HTML]{E3F8F8}  Otter-I~\\cite{otter2023} &2023-05 &16 frm  & 17.6 & 17.9 &2.03   & 16.7  & 17.0  & 18.0 &3.90 & 15.7 & 16.7 & 17.1 & 2.97 \\\\\n          \\rowcolor[HTML]{E3F8F8}LLaVA-1.6~\\cite{llava2023} &2024-01 &16 frm  & 63.7 & 17.9  & 2.00  & 13.3  & 26.4  & 30.0 &4.20   & 21.4 & 16.7 & 27.1 & 3.10 \\\\\n         \\rowcolor[HTML]{E3F8F8} InternVL-2~\\cite{internvl-1.5-2024} &2024-07 &16 frm & 85.7 & 51.3 & 2.55  & 48.3  & 47.2  & 52.0  & 5.25  & 32.9 & 15.0 & 47.5 & 3.90   \\\\\n         \\rowcolor[HTML]{E3F8F8} Claude-3-Opus$^\\dag$~\\cite{Claude3} &2024-03 &16 frm  & 53.8 & 30.8 & 2.83  & 14.0  & 17.0  & 20.0  & 3.67  & 10.0 & 6.7  & 21.8 & 3.25  \\\\\n        \\rowcolor[HTML]{E3F8F8} Qwen-VL-Max$^\\dag$~\\cite{qwenvl-2023} &2024-01 &16 frm  & 75.8 & 53.8 & 3.00  & 15.0  & 26.4  & 4.84  & 20.0  & 20.7 & 11.7 & 32.2 & 3.92  \\\\\n        \\hline\n         \\rowcolor[HTML]{FFF5F5}\\multicolumn{14}{l}{\\gray{\\textit{\\textbf{Short Video MLLMs}}}} \\\\\n        \\rowcolor[HTML]{FFF5F5}Otter-V~\\cite{otter2023} &2023-05  &16 frm  & 16.5 & 12.8 & 2.18  & 16.7  & 22.6  & 22.0  & 4.20   & 12.9 & 13.3 & 16.7 & 3.19  \\\\\n        \\rowcolor[HTML]{FFF5F5}mPLUG-Owl-V~\\cite{mplug-owl-2023} &2023-04  &16 frm  & 25.3 & 15.4 & 2.20  & 6.7   & 13.2  & 22.0  & 5.01  & 14.3 & 20.0 & 16.7 & 3.61 \\\\\n         \\rowcolor[HTML]{FFF5F5}VideoChat~\\cite{videochat2023} &2023-05 &16 frm   & 26.4 & 12.8 & 2.15  & 18.3  & 17.0  & 22.0  & 4.90  & 15.7 & 11.7 & 17.7 & 3.53  \\\\\n        \\rowcolor[HTML]{FFF5F5}Video-LLaMA-2~\\cite{videollama} &2024-08  &16 frm  & 52.7 & 12.8 & 2.23  & 13.3  & 17.0  & 12.0  & 4.87  & 15.7 & 8.3  & 18.8 & 3.55  \\\\\n         \\rowcolor[HTML]{FFF5F5}VideoChat2-HD~\\cite{mvbench2023} &2024-06 &16 frm  & 74.7 & 43.6 & 2.83  & 35.0  & 34.0  & 30.0  & 5.14  & 21.4 & 23.3 & 37.4 & 3.99  \\\\\n          \\rowcolor[HTML]{FFF5F5}Video-LLaVA~\\cite{videollava2023}  &2023-11 &8 frm  & 70.3 & 38.5 & 20.9  & 2.30  & 26.4  & 26.0  & 5.06  & 20.0 & 21.7 & 29.3 & 3.68  \\\\\n            \\rowcolor[HTML]{FFF5F5}ShareGPT4Video~\\cite{chen2024sharegpt4video} &2024-05 &16 frm  & 73.6 & 25.6 & 2.53  & 31.7  & 45.3  & 38.0  & 4.72  & 17.1 & 8.3  & 34.2 & 3.63  \\\\\n           \\rowcolor[HTML]{FFF5F5}VideoLLaMA2~\\cite{cheng2024videollama2} &2024-06 &16 frm   & 80.2 & 53.8 & 2.80  & 36.7  & 54.7  & 54.0  & 5.09  & 42.9 & 16.7 & 48.4 & 3.95  \\\\\n        \\hline\n        \\rowcolor[HTML]{F1F6EC}\\multicolumn{14}{l}{\\gray{\\textit{\\textbf{Long Video MLLMs}}}}\\\\\n         \\rowcolor[HTML]{F1F6EC}MovieChat~\\cite{moviechat2023} &2023-07 &2048 frm   & 18.7 & 10.3 & 2.30  & 23.3  & 15.1  & 16.0  & 3.24  & 17.1 & 15.0 & 16.5 & 2.77    \\\\\n         \\rowcolor[HTML]{F1F6EC}Movie-LLM~\\cite{moviellm2024} &2024-03 &1 fps  & 27.5 & 25.6 & 2.10  & 10.0  & 11.3  & 16.0  & 4.93  & 20.0 & 21.7 & 18.9 & 3.52   \\\\\n        \\rowcolor[HTML]{F1F6EC}LLaMA-VID~\\cite{llama-vid2023} &2023-11 &1 fps  & 20.9 & 23.1 & 2.70  & 21.7  & 11.3  & 16.0  & 4.15  & 18.6 & 15.0 & 18.1 & 3.43  \\\\\n        \\rowcolor[HTML]{F1F6EC}MA-LMM~\\cite{malmm2024} &2024-04 &1000 frm & 44.0 & 23.1 & 3.04  & 13.3  & 30.2  & 14.0  & 4.61  & 18.6 & 13.3 & 22.4 & 3.83   \\\\\n          \\rowcolor[HTML]{F1F6EC}\n        MiniGPT4-Video~\\cite{minigpt4video-2024} &2024-04 &90 frm  & 64.9 & 46.2 & 2.50  & 20.0  & 30.2  & 30.0  & 4.27  & 15.7 & 15.0 & 31.7 & 3.39   \\\\\n          \\rowcolor[HTML]{F1F6EC}\n        LongVA~\\cite{zhang2024longva} &2024-06 &256 frm  & 81.3 & 41.0 & 2.90  & 46.7  & 39.6  & 46.0  & 4.92  & 17.1 & 23.3 & 42.1 & 3.91   \\\\\n          \\rowcolor[HTML]{F1F6EC}\n        Video-CCAM~\\cite{fei2024videoccam} &2024-08 &96 frm  & 79.1 & 38.5 & 2.65  & 45.0  & 52.8  & 56.0  & 4.49  & 24.3 & 26.7 & 46.1& 3.57   \\\\\n          \\rowcolor[HTML]{F1F6EC}\n        Video-XL~\\cite{shu2024videoxl} &2024-09 &256 frm  & 78.0 & 28.2 & 3.40  & 50.0  & 41.5  & 46.0  & 5.02  & 48.6 & 31.7 & 46.3 & 4.21   \\\\\n          \\rowcolor[HTML]{F1F6EC}\n        LLaVA-Onevision~\\cite{li2024llavaonevision} &2024-08 &32 frm  & 83.5 & 56.4 & 3.75  & 46.7  & 58.4  & 58.0  & 5.09  & 35.7 & 23.3 & 51.7 & 4.42   \\\\\n        \\rowcolor[HTML]{F1F6EC}GPT-4o$^\\dag$~\\cite{gpt4o}  &2024-05 &0.5 fps  & 83.7 & 68.8 & 4.94  & 42.9  & 47.8  & 57.1  & 6.80  & 46.2 & 35.0 & 54.5 & 5.87  \\\\\n\\bottomrule\n \\end{tabular}\n}\n\\vspace{-0.2cm}\n\\caption{The overall performances on MLVU test set, including the holistic LVU tasks, the single-detail LVU tasks, and multi-detail LVU tasks. Date: the release date of the MLLM. M-Avg: the average performance of multiple-choice tasks; G-Avg: the average performance of generation tasks (marked by $*$). Two input strategies are used by the MLLMs in evaluation: Uniform Sampling (\\textbf{N frm}), which evenly samples N frames from the video; Frame Rate Sampling (\\textbf{N fps}), which samples N frames per second. $\\dag$ denotes proprietary models.} \n\\vspace{-0.5cm}\n\\label{tab:overall:test}\n\\end{table*}\n\n\\section{Experiments and Analysis}\n\\label{sec:experiments}\n\n\\subsection{Settings}\n\nWe conduct a comprehensive investigation of 23 MLLMs using our MLVU benchmark, encompassing both open-source and proprietary models. The experimental MLLMs are divided into three categories: \\textbf{\\textit{1) Image MLLMs}}, primarily fine-tuned using image-related instructions; \\textbf{\\textit{2) Short Video MLLMs}}, fine-tuned with short-video related instructions; and \\textbf{\\textit{3) Long Video MLLMs}}, optimized for long-video understanding capability. For Image MLLMs, we leverage their multi-image inference capabilities to process segmented frames from original videos. For Video MLLMs, we employ either a uniform sampling strategy or a frame rate sampling strategy for video processing. All models are evaluated based on their official implementations or available APIs, with evaluations conducted in a zero-shot manner. More details about the evaluation are provided in Appendix \\ref{appendix:evaluation}. \n\n\\subsection{Main Results}\n\nThe overall evaluation results for all investigated MLLMs in the MLVU test set are shown in \\Cref{tab:overall:test} (with dev set results in Appendix \\ref{appendix:dev-result}). Individual performances are reported for each task, while average performances are provided for multiple-choice (M-Avg) and generation tasks (G-Avg). From the results, we derive three primary conclusions:\n\n\\textbf{1) The proprietary model GPT-4o~\\cite{gpt4o} achieves optimal performance in our benchmark.} It leads in multiple-choice tasks with an M-Avg of 54.5\\%(within 0-100\\%) and excels in generation tasks with a G-Avg of 5.87 (within 0.0-10.0), outperforming all other methods.\n\n\\textbf{{2) Recent advances in LVU have achieved significant progress, and the gap between open-source long video MLLMs and GPT-4o on close-ended tasks is narrowing.}} Before June 2024, the best open-source long video MLLMs, MiniGPT4-Video~\\cite{minigpt4video-2024}, lagged significantly behind GPT-4o. However, recent models~\\cite{zhang2024longva, fei2024videoccam, shu2024videoxl, li2024llavaonevision} have made substantial progress. For instance, LLaVA-Onevision trails GPT-4o by only 2.8\\% in M-Avg. These models have improved their ability to handle long visual sequences, achieving significant advancements in single-detail (e.g., NQA) and multi-detail (e.g., AC) tasks compared to previous open-source models.\n\n\\textbf{{3) Existing methods still struggle to handle most tasks in our benchmark.}} For instance, GPT-4o only achieves 42.9\\% in the needle question-answering (NQA) task. In contrast, analogous tasks in the text domain, such as NIHS (Needle-In-the-Haystack-Search) and Passkey Retrieval, are effectively handled by many existing long LLMs \\cite{fu2024data,inftybench-2024}. Additionally, GPT-4o shows even less reliability in tasks like ego-reasoning (ER), action ordering (AO), and action count (AC), with most baseline methods performing even worse. % in these scenarios. \nThese observations indicate that long-video understanding remains a significant challenge for today's MLLMs.\n\nIn addition to the primary conclusions from the overall performances, we can also make the following interesting observations about the individual tasks.\n\n\\textbf{{4) The close-ended holistic tasks present much higher differentiation than other tasks.}} These tasks, i.e., topic reasoning (TR) and anomaly recognition (AR), show significant variance in performance across different models. Proprietary MLLMs, like GPT-4o, and superior open-source models, such as InternVL-2~\\cite{internvl-1.5-2024}, VideoLLaMA2~\\cite{cheng2024videollama2}, and LLaVA-OneVision~\\cite{li2024llavaonevision}, can accurately solve these problems. Meanwhile, many other popular MLLMs still fail to generate meaningful performances. Since these tasks only require an overall understanding of long videos, they can serve as a preliminary indicator of MLLMs' long video understanding (LVU) ability.\n\n\\textbf{{5) It's challenging to deal with tasks that require nuanced understanding of multiple details.}} Although several MLLMs can handle single-detail LVU tasks to some extent, their performances suffer from catastrophic degradation when addressing multi-detail LVU tasks. Most methods, except for GPT-4o and Video-XL~\\cite{shu2024videoxl}, fail entirely in action order (AO) and action count (AC) tasks. Additionally, most approaches struggle with summarization tasks, which require recalling multiple nuanced details from long videos.\n\nAs a brief conclusion, although today's MLLMs can deal with some preliminary LVU tasks, it remains a tough challenge to achieve an in-depth understanding of nuanced information within long videos. \n\n\\subsection{Further Analysis}\n\\label{sec-exp-detail-analysis}\n\n\\renewcommand{\\arraystretch}{1.2}\n\n\\begin{table*}[tp]\n    \\centering\n    \\setlength\\tabcolsep{2.5pt}\n    \\resizebox{0.85\\linewidth}{!}{\n        \\begin{tabular}{l c l||l c l||l c l}\n        \\Xhline{1.0pt}\n        \\multicolumn{3}{c||}{\\textbf{Impact of Context Length}} & \\multicolumn{3}{c||}{\\textbf{Impact of IU}} &   \\multicolumn{3}{c}{\\textbf{Impact of LLM}} \\\\\n        \\Xhline{0.8pt}\n        Model & Context Len. &M-Avg & Model & MMMU (Val) &M-Avg & Model & LLM & M-Avg\\\\\n        \\Xhline{0.8pt}\n        \\multirow{2}{*}{MGV} & 16 & 24.2 & Otter-I & 32.2 &17.1 & \\multirow{2}{*}{VLM2} &Vicuna-7B &13.3 \\\\\n        ~ & 90 & 31.7\\darkGreen{$\\uparrow$7.5} & LLaVA-1.6 & 35.8 &27.1\\darkGreen{$\\uparrow$10.0}  &~ & Vicuna-13B & 18.8\\darkGreen{$\\uparrow$5.5}    \\\\\n        \\hline\n        \\multirow{2}{*}{GPT-4o} & 16 & 45.8 & GPT-4V& 58.1& 43.3 & \\multirow{2}{*}{MGV} & LLaMA-7B & 20.6 \\\\\n        ~ & 256 & 54.5\\darkGreen{$\\uparrow$8.7}  & GPT-4o & 63.8 &45.8\\darkGreen{$\\uparrow$2.5}  & ~ & Mistral-7B &31.7 \\darkGreen{$\\uparrow$11.1}  \\\\\n        \\Xhline{1.0pt}\n        \\end{tabular}\n    }\n    \\vspace{-0.25cm}\n    \\caption{Detailed discussions about the impact from context length, image understanding (IU) ability, and LLM Backbone. For the IU impact experiment, we used 16-frame uniform sampling for both GPT-4V and GPT-4o. MGV: MiniGPT4-Video, VLM2: Video-LLaMA-2.} \n    \\label{tab:ablations}\n    \\vspace{-0.5cm}\n\\end{table*}\n\n\\textbf{{6) Longer videos are more challenging for MLLMs. }}We evaluate MLLMs' performances across various video lengths. For this purpose, we introduce a derivative dataset alongside MLVU, called \\textit{MLVU Time-ladder}. In this dataset, the same kinds of evaluation tasks are created for videos of variant lengths, including 180s, 360s, and 600s (more details presented in \\Cref{appendix:Time-Ladder}). As shown in Figure \\ref{fig:length}, the performances of all models tend to decline as the video length grows, which indicates that the existing MLLMs' LVU abilities are severely constrained by the video length. Moreover, the short video model Video-LLaMA-2~\\cite{videollama} maintains a certain level of LVU ability at 3 minutes, but its performance approaches random results at 10 minutes.\n\n\\textbf{7) The performance of recent advanced long video MLLMs remains robust regardless of the position of the referring clip within the long video.} In single-detail tasks, the referring clip denotes the specific segment of the long video that is referenced or inferred to answer a question. As shown in Figure \\ref{fig:needle_position}, we categorize clip positions into four intervals and assess model performance on two single-detail tasks: ego reasoning (ER) and plot question-answering (PQA). Recent long video MLLMs, such as LongVA~\\cite{zhang2024longva} and Video-XL~\\cite{shu2024videoxl}, maintain consistent performance regardless of the referring clip's position within the video. Conversely, short video MLLMs are more sensitive to clip location. This indicates that recent advancements in long video MLLMs enhance both reliable clue retrieval and effective reasoning from extended visual sequences.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.45\\textwidth]{figures/results_in_ladder.pdf}\n    \\vspace{-0.2cm}\n    \\caption{Experimental performance on varying video lengths. The evaluated metric is the average accuracy across five multiple-choice tasks involving local information: NQA, ER, PQA, AC, and AO.}  \n    \\label{fig:length}\n    \\vspace{-0.3cm}\n\\end{figure}\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.45\\textwidth]{figures/needle_position.pdf}\n    \\vspace{-0.3cm}\n    \\caption{Model performance across different referring clip positions, spanning from the beginning to the end of the entire video.} \n    \\vspace{-0.6cm}\n    \\label{fig:needle_position}\n\\end{figure}\n\n\\textbf{8) The challenge of multi-detail tasks increases with the number of details.} We analyzed model performance on the action count (AC) task by grouping questions based on the number of probes (which correspond to details) and evaluating the average performance within these groups. As shown in Figure~\\ref{fig:multi_detail_num}, performance significantly declines across all models as the number of probes increases. This indicates that current MLLMs face substantial difficulties comprehending and processing multiple details simultaneously, highlighting a critical area for future improvement in long video understanding capabilities.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.45\\textwidth]{figures/multi-details_2.pdf}\n    \\vspace{-0.2cm}\n    \\caption{Model performance on the action count (AC) task in relation to the number of probes.}  \n    \\vspace{-0.5cm}\n    \\label{fig:multi_detail_num}\n\\end{figure}\n\n\\textbf{{9) Context Length, Image-Understanding ability, and the choice of LLM Backbones are key factors in LVU performance.}} As shown in Table \\ref{tab:ablations}, we conducted ablation experiments on several factors affecting MLLMs, using M-Avg as the evaluation metric. First, we examined the models' handling of different context lengths. Specifically, we increased MiniGPT4-Video's input from 16 to 90 frames and GPT-4o's input from 16 to 256 frames (as shown on the left side of Table \\ref{tab:ablations}). Both models showed consistent performance improvements with longer input lengths. To assess the impact of image understanding (IU) capabilities, we referred to the results from MMMU \\cite{yue2023mmmu} (presented in the middle of Table \\ref{tab:ablations}). It is evident that MLLMs' LVU performance generally aligns with their IU performance in MMMU. Finally, we compared MLLMs using different backbones (depicted on the right side of Table \\ref{tab:ablations}). The findings indicate that LVU performance improves with larger (Vicuna-13B vs. Vicuna-7B) and more advanced backbones (Mistral-7B vs. Llama-2-7B). These observations indicate that LVU is the result of multiple complex factors, with the ability to perceive longer videos and effectively utilize the perceived information being crucial for the improvement of LVU. \n\\vspace{-0.2cm}\n\\section{Conclusion}\n\\vspace{-0.2cm}\n\\label{conclusion}\nThis paper presents MLVU, a novel benchmark for the assessment of long video understanding. With several critical innovations: the substantial extension of video lengths, the inclusion of various video genres, and the development of diversified LVU-oriented evaluation tasks, the new benchmark is able provide a comprehensive and in-depth analysis for MLLMs' long-video understanding performance. The empirical study on MLVU reveals LVU remains a technically challenging problem for today's state-of-the-art MLLMs. Future advancements may call for the joint optimization of complex factors, such as context length, image understanding ability, and even LLM backbones. We anticipate this benchmark will facilitate future research in long-video understanding of MLLMs. \n{\n    \\small\n        }\n\n\\clearpage\n\\setcounter{page}{1}\n\\maketitlesupplementary\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2308.09126v1.tex",
        "arXiv-2311.17005v4.tex",
        "arXiv-2406.04264v3.tex"
    ],
    "group_id": "group_91",
    "response": "### Title: Evaluating Long Video Understanding Capabilities in Modern Multimodal Systems\n\n### Introduction\n\nThe field of multimodal large language models (MLLMs) has seen significant advancements in recent years, with models increasingly capable of integrating textual, visual, and audio information to understand and generate content in complex environments. This integration has enabled MLLMs to perform a variety of tasks, ranging from image captioning to video question answering, with varying degrees of success. However, the evaluation of these models' long video understanding (LVU) capabilities remains a challenging problem. Traditional benchmarks often focus on short video clips or specific tasks, which do not fully capture the intricacies of understanding extended visual sequences. This paper aims to provide a comprehensive overview of three recent research papers that introduce novel benchmarks and methodologies to assess the LVU capabilities of MLLMs, highlighting their contributions and the current state of the field.\n\nHistorically, the evaluation of MLLMs has been centered around specific tasks and datasets, such as image captioning and video question answering. These benchmarks have primarily focused on short video clips, which do not adequately reflect the models' ability to understand and reason about longer, more complex videos. The limitations of these benchmarks have led to a need for more comprehensive and nuanced evaluation frameworks that can assess the full range of LVU capabilities, including temporal perception, action recognition, and summarization. The recent surge in MLLM development has brought to light the necessity for benchmarks that can accurately measure the models' performance across a wide range of video lengths and genres, as well as diverse evaluation tasks.\n\nCurrent progress in MLLMs has seen the emergence of models that can handle longer video sequences, but these models still struggle with tasks that require a deep understanding of the video content over extended periods. The challenges include capturing the temporal dynamics of videos, understanding complex scenes and behaviors, and generating coherent summaries or answers based on the entire video. These challenges necessitate the development of new benchmarks that can provide a more thorough evaluation of MLLMs' LVU capabilities.\n\n### Main Content of Each Paper\n\n#### Paper 1: A Diagnostic Benchmark for Very Long-form Video Language Understanding\n\nThis paper introduces EgoSchema, a diagnostic benchmark designed to evaluate the long video understanding (LVU) capabilities of modern multimodal systems. EgoSchema is derived from the Ego4D dataset and consists of over 5000 human-curated multiple choice question-answer pairs, covering a broad range of natural human activity and behavior. Each question is based on a three-minute video clip, requiring the correct answer to be selected from five options. The authors argue that merely the length of the video clip does not capture the intrinsic temporal difficulty of the task, and thus introduce the concept of temporal certificate sets, which measure the minimum video length necessary to verify the correctness of an answer. EgoSchema's median certificate length is about 100 seconds, making it significantly more challenging than other video understanding datasets.\n\nThe authors also detail a staged data collection pipeline that utilizes existing large-scale but short-term video datasets, rule-based filtering, and large language model (LLM) prompting to generate and curate the dataset. They benchmark several state-of-the-art video-language models and find that even models with billions of parameters achieve very low accuracy (<33%) in zero-shot settings, while humans achieve about 76% accuracy. The paper concludes that EgoSchema will serve as a valuable evaluation tool for developing effective LVU systems.\n\n#### Paper 2: MVBench: A Comprehensive Multi-modal Video Understanding Benchmark\n\nMVBench is a comprehensive benchmark designed to evaluate the temporal understanding capabilities of MLLMs. Unlike other benchmarks that focus on spatial understanding, MVBench emphasizes temporal tasks and includes a wide range of video genres and evaluation tasks. The authors propose a static-to-dynamic method to systematically define temporal tasks, such as action sequence retrieval, action prediction, object existence, and more. They also introduce a progressive multi-modal training paradigm to develop a robust video MLLM baseline, dubbed \\ModelName, which outperforms existing models by over 15% on MVBench.\n\nMVBench includes 200 multiple-choice QA pairs for each temporal understanding task, and the authors use a simple yet effective answer prompt to extract options from MLLMs' responses. The evaluation results reveal that existing MLLMs are far from satisfactory in temporal understanding, with \\ModelName achieving the best performance. The authors suggest that future advancements in MLLMs should focus on improving their ability to handle temporal information and complex video sequences.\n\n#### Paper 3: MLVU: Benchmarking Multi-task Long Video Understanding\n\nMLVU is a benchmark designed to evaluate the LVU capabilities of MLLMs across various video lengths and genres. It includes 3,102 questions across nine categories, such as topic reasoning, anomaly recognition, video summarization, and action order prediction. The authors introduce a variety of tasks that require the MLLM to utilize both global and local information from long videos. They also conduct extensive experiments to evaluate 23 MLLMs, including both open-source and proprietary models.\n\nThe results show that while some models, like GPT-4o, achieve optimal performance, others still struggle with most tasks. The authors observe that longer videos are more challenging for MLLMs, and the performance of recent advanced long video MLLMs remains robust regardless of the position of the referring clip within the video. They also find that context length, image understanding ability, and the choice of LLM backbone are key factors in LVU performance. MLVU aims to provide a comprehensive and in-depth analysis of MLLMs' LVU capabilities, highlighting areas for future improvement.\n\n### Commonalities and Innovations\n\nAll three papers address the challenge of evaluating MLLMs' LVU capabilities, but each introduces unique methodologies and benchmarks. EgoSchema focuses on very long-form video clips and introduces the concept of temporal certificate sets to measure the intrinsic temporal difficulty of video tasks. MVBench emphasizes temporal tasks and proposes a progressive multi-modal training paradigm to develop a robust video MLLM baseline. MLVU includes a wide range of video genres and tasks, and evaluates MLLMs across different video lengths and genres.\n\nThe common theme among these papers is the recognition that existing benchmarks are insufficient for evaluating LVU capabilities, and the need for more comprehensive and nuanced evaluation frameworks. Each paper contributes to this goal by introducing new benchmarks and methodologies, and by providing empirical evidence of the current limitations of MLLMs in handling long video sequences.\n\n### Comparison of Results\n\nThe results from the three papers reveal significant room for improvement in MLLMs' LVU capabilities. EgoSchema finds that even state-of-the-art video-language models achieve very low accuracy (<33%) in zero-shot settings, while humans achieve about 76% accuracy. MVBench shows that existing MLLMs struggle with temporal understanding tasks, with \\ModelName achieving the best performance. MLVU reveals that while some models, like GPT-4o, achieve optimal performance, others still struggle with most tasks, especially those requiring nuanced understanding of multiple details.\n\nThe differences in results can be attributed to the varying lengths and genres of videos used in each benchmark, as well as the specific tasks and evaluation metrics. EgoSchema uses three-minute video clips from egocentric videos, while MVBench includes shorter videos from a variety of sources. MLVU, on the other hand, includes videos ranging from 3 minutes to 2 hours, and evaluates MLLMs across different video lengths and genres.\n\n### Conclusion\n\nThe three papers presented in this summary introduce novel benchmarks and methodologies to evaluate the LVU capabilities of MLLMs. They highlight the current limitations of these models in handling long video sequences and provide valuable insights into the factors that influence LVU performance, such as context length, image understanding ability, and the choice of LLM backbone. Future research should focus on developing more robust and comprehensive benchmarks for LVU, and on optimizing MLLMs to better handle the challenges of long video understanding. The authors anticipate that these benchmarks will facilitate future advancements in the field of MLLMs and contribute to the development of more effective LVU systems.\n\n### Future Research Directions\n\nFuture research should aim to further extend the video lengths and genres used in benchmarks, and to develop more diverse and challenging evaluation tasks. Additionally, the impact of different training strategies and architectures on LVU performance should be explored, with a focus on optimizing MLLMs for handling long video sequences. The development of new evaluation metrics that can more accurately reflect the LVU capabilities of MLLMs is also a promising direction for future research."
}