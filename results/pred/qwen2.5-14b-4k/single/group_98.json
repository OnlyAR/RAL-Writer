{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{MusicLM: Generating Music From Text}\n\n\\begin{document}\n\n\\twocolumn[\n\\icmltitle{MusicLM: Generating Music From Text}\n\n\\icmlsetsymbol{equal}{*}\n\n\\begin{icmlauthorlist}\n\\icmlauthor{Andrea Agostinelli}{equal,google}\n\\icmlauthor{Timo I. Denk}{equal,google}\\\\\n\\icmlauthor{Zalán Borsos}{google}\n\\icmlauthor{Jesse Engel}{google}\n\\icmlauthor{Mauro Verzetti}{google}\n\\icmlauthor{Antoine Caillon}{ircam}\n\\icmlauthor{Qingqing Huang}{google}\n\\icmlauthor{Aren Jansen}{google}\n\\icmlauthor{Adam Roberts}{google}\n\\icmlauthor{Marco Tagliasacchi}{google}\n\\icmlauthor{Matt Sharifi}{google}\n\\icmlauthor{Neil Zeghidour}{google}\n\\icmlauthor{Christian Frank}{google}\n\\end{icmlauthorlist}\n\n\\icmlaffiliation{google}{Google Research}\n\\icmlaffiliation{ircam}{IRCAM - Sorbonne Université (work done while interning at Google)}\n\n\\icmlcorrespondingauthor{Christian Frank}{chfrank@google.com}\n\n\\icmlkeywords{music, generative modeling, machine learning, text-conditioning, autoregressive}\n\n\\vskip 0.3in\n]\n\n\\printAffiliationsAndNotice{\\icmlEqualContribution} %\n\n\\providetoggle{showcomments}\n\\settoggle{showcomments}{true} \t\t\t\t\t\t\t\t%\n\n\\iftoggle{showcomments}{%\n    \\newcommand{\\changed}[1]{\\textcolor{blue}{#1}}\n    \\newcommand{\\todo}[1]{\\textcolor{blue}{\\textbf{TODO:} #1}}\n    \\newcommand{\\resolved}[3][]{\\ifstrequal{#1}{resolved}{\\textcolor{blue}{RESOLVED:}~\\textbf{{\\MakeUppercase #2:}}~{#3}}{\\textbf{\\MakeUppercase #2:}~#3}}\n    \\newcommand{\\andrea}[2][]{\\textcolor{brown}{\\resolved[#1]{andrea}{#2}}}\n    \\newcommand{\\timo}[2][]{\\textcolor{ForestGreen}{\\resolved[#1]{timo}{#2}}}\n    \\newcommand{\\christian}[2][]{\\textcolor{violet}{\\resolved[#1]{christian}{#2}}}\n    \\newcommand{\\jesse}[2][]{\\textcolor{DarkYellow}{\\resolved[#1]{jesse}{#2}}}\n    \\newcommand{\\assign}[2][]{\\textcolor{red}{\\resolved[#1]{Assigned to: }{#2}}}\n}{%\n    \\newcommand{\\changed}[1]{#1}\n    \\newcommand{\\todo}[1]{}\n    \\newcommand{\\andrea}[2][]{}\n    \\newcommand{\\timo}[2][]{}\n    \\newcommand{\\christian}[2][]{}\n}\n\n\\definecolor{ForestGreen}{RGB}{34,139,34}\n\\definecolor{DarkYellow}{RGB}{204,204,0}\n\n\\newcommand{\\model}{MusicLM} %\n\\newcommand{\\modelmv}{MusicLM-mv}\n\\newcommand{\\dataset}{MusicCaps}\n\\newcommand{\\datasetshort}{MusicCaps}\n\\newcommand{\\mulan}{MuLan}\n\\newcommand{\\audiolm}{AudioLM}\n\\newcommand{\\soundstream}{SoundStream}\n\\newcommand{\\dalletwo}{DALL$\\cdot$E~2}\n\\newcommand{\\mulanText}{M_T}\n\\newcommand{\\mulanAudio}{M_A}\n\\newcommand{\\sounds}{A}\n\\newcommand{\\mlm}{S}\n\n\\newcommand{\\mlmcodebook}{K}\n\\newcommand{\\matchingthreshold}{\\tau}\n\\newcommand{\\memorizationpromptlen}{T}\n\n\\newcommand{\\website}{https://goto.google.com/text2music-website}\n\n\\linepenalty=1000\n\\begin{abstract}\n\nWe introduce {\\model}, a model for generating high-fidelity music from text descriptions such as \\textit{``a~calming violin melody backed by a distorted guitar~riff''}. {\\model} casts the process of conditional music generation as a hierarchical sequence-to-sequence modeling task,\nand it generates music at 24~kHz that remains consistent over several minutes.\nOur experiments show that {\\model} outperforms previous systems both in audio quality and adherence to the text descriptions. Moreover, we demonstrate that {\\model} can be conditioned on both text and a melody in that it can transform whistled and hummed melodies according to the style described in a text caption. To support future research, we publicly release {\\dataset}, a dataset composed of 5.5k~music-text pairs, with rich text descriptions provided by human experts.\n\n\\small{\\href{https://google-research.github.io/seanet/musiclm/examples/}{google-research.github.io/seanet/musiclm/examples}}\n\n\\end{abstract}\n\\section{Introduction}\n\\label{intro}\n\n\\looseness=-1\nConditional neural audio generation covers a wide range of applications, ranging from text-to-speech~\\citep{ze2013statistical, oord2016wavenet} to lyrics-conditioned music generation~\\cite{jukebox} and audio synthesis from MIDI sequences~\\citep{hawthorne22_diffusion}. Such tasks are facilitated by a certain level of temporal alignment between the conditioning signal and the corresponding audio output. In contrast, and inspired by progress in text-to-image generation~\\cite{dalle, dalle-2, imagen, parti}, recent work has explored generating audio from sequence-wide, high-level captions~\\citep{yang2022diffsound, audiogen} such as \\textit{``whistling with wind blowing''}. While generating audio from such coarse captions represents a breakthrough, these models remain limited to simple acoustic scenes, consisting of few acoustic events over a period of seconds. Hence, turning a single text caption into a rich audio sequence with long-term structure and many stems, such as a music clip, remains an open challenge.\n\n\\looseness=-1\n\\audiolm~\\cite{audiolm} has recently been proposed as a framework for audio generation. Casting audio synthesis as a language modeling task in a discrete representation space, and leveraging a hierarchy of coarse-to-fine audio discrete units (or \\textit{tokens}), \\audiolm~ achieves both high-fidelity and long-term coherence over dozens of seconds. Moreover, by making no assumptions about the content of the audio signal, \\audiolm~ learns to generate realistic audio from audio-only corpora, be it speech or piano music, without any  annotation. The ability to model diverse signals suggests that such a system could generate richer outputs if trained on the appropriate data.\n\n\\looseness=-1\n Besides the inherent difficulty of synthesizing high-quality and coherent audio, another impeding factor is the scarcity of paired audio-text data. This is in stark contrast with the image domain, where the availability of massive datasets contributed significantly to the remarkable image generation quality that has recently been achieved~\\citep{dalle, dalle-2, imagen, parti}. Moreover, creating text descriptions of general audio is considerably harder than describing images. First, it is not straightforward to unambiguously capture with just a few words the salient characteristics of either acoustic scenes (e.g., the sounds heard in a train station or in a forest) or music (e.g., the melody, the rhythm, the timbre of vocals and the many instruments used in accompaniment). Second, audio is structured along a temporal dimension which makes sequence-wide captions a much weaker level of annotation than an image caption.\n\n\\looseness=-1\nIn this work, we introduce {\\model}, a model for generating high-fidelity music from text descriptions. {\\model} leverages {\\audiolm}'s multi-stage autoregressive modeling as the generative component, while extending it to incorporate text conditioning. To address the main challenge of paired data scarcity, we rely on {\\mulan} \\citep{mulan}, a joint music-text model that is trained to project music and its corresponding text description to representations close to each other in an embedding space. This shared embedding space eliminates the need for captions at training time altogether, and allows training on massive audio-only corpora. That is, we use the {\\mulan} embeddings computed from the audio as conditioning during training, while we use {\\mulan} embeddings computed from the text input during inference.\n\nWhen trained on a large dataset of unlabeled music, {\\model} learns to generate long and coherent music at 24 kHz, for text descriptions of significant complexity, such as \\textit{``enchanting jazz song with a memorable saxophone solo and a solo singer''} or \\textit{``Berlin 90s techno with a low bass and strong kick''}. To address the lack of evaluation data for this task, we introduce {\\dataset}, a new high-quality music caption dataset with 5.5k~examples prepared by expert musicians, which we publicly release to support future research.\n\nOur experiments show through quantitative metrics and human evaluations that {\\model} outperforms previous systems such as Mubert~\\citep{Mubert} and Riffusion~\\citep{riffusion}, both in terms of quality and adherence to the caption. \nFurthermore, since describing some aspects of music with words can be difficult or even impossible, we show how our method supports conditioning signals beyond text. Concretely, we extend {\\model} to accept an additional melody in the form of audio (e.g., whistling, humming) as conditioning to generate a music clip that follows the desired melody, rendered in the style described by the text prompt.\n\n\\looseness=-1\nWe acknowledge the risks associated with music generation, in particular, the potential misappropriation of creative content. In accordance with responsible model development practices, we conduct a thorough study of memorization by adapting and extending the methodology of~\\citet{carlini-memorization} used for text-based large language models. Our findings show that when feeding {\\mulan} embeddings to {\\model}, the sequences of generated tokens significantly differ from the corresponding sequences in the training set.\n\nThe key contributions of this work are the following:\n\\begin{enumerate}%\n    \\item We introduce {\\model}, a generative model that produces high-quality music at 24~kHz which is consistent over several minutes while being faithful to a text conditioning signal.\n    \\item We extend our method to other conditioning signals, such as a melody that is then synthesized according to the text prompt. Furthermore, we demonstrate long and coherent music generation of up to 5-minute long clips.\n    \\item We release the first evaluation dataset collected specifically for the task of text-to-music generation: {\\dataset} is a hand-curated, high-quality dataset of 5.5k~music-text pairs prepared by musicians.\n\\end{enumerate}\n\n\\label{contributions}\\newpage\n\\section{Background and Related Work}\n\\label{sec:background-related}\n\nThe state-of-the-art in generative modeling for various domains is largely dominated either by Transformer-based autoregressive models~\\citep{vaswani2017attention} or U-Net-based diffusion models~\\citep{ho2020denoising}. In this section, we review the related work with an emphasis on autoregressive generative models operating on discrete tokens, which share similarities with {\\model}.\n\n\\subsection{Quantization} \n\n\\looseness=-1\nModeling sequences of discrete tokens autoregressively has proven to be a powerful approach in natural language processing \\citep{gpt3, lamda} and image or video generation \\citep{esser2021taming,dalle,parti,phenaki}. Quantization is a key component to the success of autoregressive models for continuous signals, including images, videos, and audio. The goal of quantization is to provide a compact, discrete representation, which at the same time allows for high-fidelity reconstruction. VQ-VAEs~\\citep{vqvae} demonstrated impressive reconstruction quality at low bitrates in various domains and serve as the underlying quantizer for many approaches.\n\n\\soundstream{}~\\citep{soundstream} is a universal neural audio codec capable of compressing general audio at low bitrates, while maintaining a high reconstruction quality. To achieve this, \\soundstream{} uses residual vector quantization (RVQ), allowing scalability to higher bitrate and quality, without a significant computational cost. More specifically, RVQ is a hierarchical quantization scheme composing a series of vector quantizers, where the target signal is reconstructed as the sum of quantizer outputs. Due to the composition of quantizers, RVQ avoids the exponential blowup in the codebook size as the target bitrate increases. Moreover, the fact that each quantizer is fitted to the residual of coarser quantizers introduces a hierarchical structure to the quantizers, where coarser levels are more important for high-fidelity reconstruction. This property is desirable for generation, since the past context can be defined by only attending to the coarse tokens. Recently, \\soundstream{} was extended by EnCodec~\\citep{defossez2022highfi} to higher bitrates and stereophonic audio. In this work, we rely on \\soundstream{} as our audio tokenizer, since it can reconstruct 24 kHz music at 6 kbps with high fidelity. \n\n\\subsection{Generative Models for Audio}\n\nDespite the challenge of generating high-quality audio with long-term consistency, a series of approaches have recently tackled the problem with some success. Jukebox~\\citep{jukebox}, for example, proposes a hierarchy of VQ-VAEs at various time resolutions to achieve high temporal coherence, but the generated music displays noticeable artifacts. PerceiverAR~\\citep{perceiverAR}, on the other hand, proposes to model a sequence of \\soundstream{} tokens autoregressively, achieving high-quality audio, but compromising the long-term temporal coherence. \n\nInspired by these approaches, \\audiolm{}~\\cite{audiolm} addresses the trade-off between coherence and high-quality synthesis by relying on a hierarchical tokenization and generation scheme. Concretely, the approach distinguishes between two token types: (1)~\\emph{semantic} tokens that allow the modeling of long-term structure, extracted from models pretrained on audio data with the objective of masked language modeling; (2)~\\emph{acoustic} tokens, provided by a neural audio codec, for capturing fine acoustic details.\n This allows \\audiolm{} to generate coherent and high-quality speech as well as piano music continuations without relying on transcripts or symbolic music representations.\n\n{\\model} builds on top of \\audiolm{} with three important additional contributions: (1)~we condition the generation process on a descriptive text, (2)~we show that the conditioning can be extended to other signals such as melody, and (3)~we model a large variety of long music sequences beyond piano music (from \\textit{drum'n'bass} over \\textit{jazz} to \\textit{classical music}).\n\n\\subsection{Conditioned Audio Generation}\n\\looseness=-1\nGenerating audio from a text description (such as \\textit{``whistling with laughter in the background''}) has recently been tackled by several works. DiffSound~\\citep{yang2022diffsound} uses CLIP~\\citep{clip} as the text encoder and applies a diffusion model to predict the quantized mel spectrogram features of the target audio based on the text embeddings. AudioGen~\\citep{audiogen} uses a T5~\\citep{t5} encoder for embedding the text, and an autoregressive Transformer decoder for predicting target audio codes produced by EnCodec~\\citep{defossez2022highfi}. Both approaches rely on a modest amount of paired training data such as AudioSet~\\citep{audioset} and AudioCaps~\\citep{audiocaps} (totalling less than 5k~hours after filtering).\n\n Closer to {\\model}, there are also works focusing on music generation conditioned on text. In Mubert~\\cite{Mubert}, the text prompt is embedded by a Transformer, music tags which are close to the encoded prompt are selected and used to query the song generation API. Based on the selected tags, Mubert generates a combination of sounds, which in turn were generated by musicians and sound designers. This is in contrast to Riffusion~\\cite{riffusion}, which fine-tunes a Stable Diffusion model~\\cite{Rombach_2022_CVPR} on mel spectrograms of music pieces from a paired music-text dataset. We use both Mubert and Riffusion as baselines for our work, showing that we improve the audio generation quality and adherence to the text description.\n\n\\looseness=-1\nSymbolic representations of music (e.g., MIDI) can also be used to drive the generative process as a form of strong conditioning, as demonstrated by~\\citet{huang2018music, hawthorne2018wave2midi2wave, engel2020ddsp}. {\\model} enables a more natural and intuitive way of providing a conditioning signal, for example through a hummed melody, which can also be combined with a text description. \n\n\\subsection{Text-Conditioned Image Generation}\n\nPrecursor to text-conditioned audio synthesis are the text-conditioned image generation models, which made significant progress in quality due to architectural improvements and the availability of massive, high-quality paired training data. Prominent Transformer-based autoregressive approaches include \\citet{dalle, parti}, while \\citet{nichol2021glide, rombach2022high, imagen} present diffusion-based models. The text-to-image approaches have been extended to generating videos from a text prompt~\\citep{wu2022nuwa, hong2022cogvideo, phenaki,ho2022video}.\n\nThe closest to our approach among these works is {\\dalletwo}~\\citep{dalle-2}. In particular, similarly to the way {\\dalletwo} relies on CLIP~\\citep{clip} for text encoding, we also use a joint music-text embedding model for the same purpose. In contrast to  {\\dalletwo}, which uses a diffusion model as a decoder, our decoder is based on {\\audiolm}. Furthermore, we also omit the prior model mapping text embeddings to music embeddings, such that the {\\audiolm}-based decoder can be trained on an audio-only dataset and the music embedding is simply replaced during inference by the text embedding.\n\n\\subsection{Joint Embedding Models for Music and Text} \\label{subsec:music-text-joint-models}\n\\mulan~\\cite{mulan} is a music-text joint embedding model consisting of two embedding towers, one for each modality. The towers map the two modalities to a shared embedding space of 128~dimensions using contrastive learning, with a setup similar to~\\citep{clip,wav2clip}.\nThe text embedding network is a BERT \\cite{bert} pre-trained on a large corpus of text-only data, while we use the ResNet-50 variant of the audio tower.\n\n{\\mulan} is trained on pairs of music clips and their corresponding text annotations. Importantly, {\\mulan} imposes only weak requirements on its training data quality, learning  cross-modal correspondences even when the music-text pairs are only weakly associated.\nThe ability to link music to unconstrained natural language descriptions makes it applicable for retrieval or zero-shot music tagging. In this work, we rely on the pretrained and frozen model of \\citet{mulan}.\n\\section{Method}\n\\label{sec:method}\n\nIn this section, we describe {\\model} and its components. \\cref{sec:audio-representation} describes the models that provide the audio representations. Then, we show in \\cref{method-training-inference} how we use these representations for text-conditioned music generation.\n\n\\begin{figure}[t]\n\\begin{center}\n\\centerline{\\includegraphics[trim=5cm 4.1cm 5.5cm 3.4cm, clip, width=0.5\\textwidth]{figures/tokenizer-models.pdf}}\n\\caption{Independent pretraining of the models providing the audio and text representations for {\\model}: SoundStream \\cite{soundstream}, w2v-BERT \\cite{wav2vec-bert}, and MuLan \\cite{mulan}.}\n\\label{fig:models-pretraining}\n\\end{center}\n\\end{figure}\n\\vspace{-4mm}\n\n\\subsection{Representation and Tokenization of Audio and Text} \\label{sec:audio-representation}\nWe use three models for extracting audio representations that will serve for conditional autoregressive music generation, which are illustrated in Figure~\\ref{fig:models-pretraining}. In particular, by following the approach of {\\audiolm}, we use the self-supervised audio representations of {\\soundstream}~\\citep{soundstream}, as acoustic tokens to enable high-fidelity synthesis, and w2v-BERT~\\citep{wav2vec-bert}, as semantic tokens to facilitate long-term coherent generation. For representing the conditioning, we rely on the {\\mulan} music embedding during training and the {\\mulan} text embedding at inference time. All three of these models are pretrained independently and then frozen, such that they provide the discrete audio and text representations for the sequence-to-sequence modeling.\n\n\\paragraph{{\\soundstream}.}\n\\looseness=-1\nWe use a {\\soundstream} model for 24~kHz monophonic audio with a striding factor of 480, resulting in 50~Hz embeddings. The quantization of these embeddings is learned during training by an RVQ with 12 quantizers, each with a vocabulary size of 1024. This results in a bitrate of 6~kbps, where one second of audio is represented by 600~tokens. We refer to these as \\emph{acoustic tokens}, denoted by~$\\sounds$.\n\n\\paragraph{w2v-BERT.}\nSimilarly to \\audiolm, we use an intermediate layer of the masked-language-modeling (MLM) module of a w2v-BERT model with 600M parameters. After pretraining and freezing the model, we extract embeddings from the 7th~layer and quantize them using the centroids of a learned k-means over the embeddings. We use 1024~clusters and a sampling rate of 25~Hz, resulting in 25~\\emph{semantic tokens} for every second of audio, denoted by~$\\mlm$.\n\n\\paragraph{MuLan.}\n\\looseness=-1\nTo train {\\model}, we extract the representation of the target audio sequence from the audio-embedding network of {\\mulan}. Note that this representation is continuous and could be directly used as a conditioning signal in Transformer-based autoregressive models. However, we opt for quantizing the {\\mulan} embeddings in such a way that both the audio and the conditioning signal have a homogeneous representation based on discrete tokens,\naiding further research into autoregressively modeling the conditioning signal as well.\n\n\\looseness=-1\nSince {\\mulan} operates on 10-second audio inputs and we need to process longer audio sequences, we calculate the audio embeddings on 10-second windows with 1-second stride and average the resulting embeddings. We then discretize the resulting embedding by applying an RVQ with 12~vector quantizers, each with a vocabulary size of 1024. This process yields 12~{\\mulan} audio tokens~$\\mulanAudio$ for an audio sequence.\nDuring inference, we use as conditioning the {\\mulan} text embedding extracted from the text prompt, and quantize it with the same RVQ as the one used for the audio embeddings, to obtain 12 tokens $\\mulanText$.\n\n\\looseness=-1\nConditioning on $\\mulanAudio$ during training has two main advantages. First, it allows us to easily scale our training data, since we are not limited by the need of text captions. Second, by exploiting a model like {\\mulan}, trained using a contrastive loss, we increase the robustness to noisy text descriptions.\n\n\\subsection{Hierarchical Modeling of Audio Representations}\n\\label{method-training-inference}\n\n\\begin{figure*}[t!]\n\n    \\centering\n    \\begin{subfigure}[b]{0.59\\textwidth}\n        \\centering\n        \\includegraphics[trim=4cm 2.8cm 4cm 2.8cm, clip, width=\\textwidth]{figures/training_short.pdf}\n        \\label{fig:training}\n    \\end{subfigure}\n    \\hfill\n    \\begin{subfigure}[b]{0.40\\textwidth}\n        \\centering\n        \\includegraphics[trim=6.2cm 2.2cm 7.2cm 3.4cm, clip, width=\\textwidth]{figures/inference_short.pdf}\n        \\label{fig:inference}\n    \\end{subfigure}\n\\vspace{-6mm}\n\\caption{Left: During training we extract the {\\mulan} audio tokens, semantic tokens, and acoustic tokens from the \\emph{audio-only} training set. In the semantic modeling stage, we predict semantic tokens using {\\mulan} audio tokens as conditioning. In the subsequent acoustic modeling stage, we predict acoustic tokens, given both {\\mulan} audio tokens and semantic tokens. Each stage is modeled as a sequence-to-sequence task using decoder-only Transformers.\nRight: During inference, we use {\\mulan} text tokens computed from the text prompt as conditioning signal and convert the generated audio tokens to waveforms using the SoundStream decoder.}\n\\label{fig:transformer-pipeline}\n\\vspace{-1mm}\n\\end{figure*}\n\nWe combine the discrete audio representations presented above with {\\audiolm} to achieve text-conditioned music generation. For this, we propose a hierarchical sequence-to-sequence modeling task, where each stage is modeled autoregressively by a separate decoder-only Transformer. The proposed approach is illustrated in \\cref{fig:transformer-pipeline}.\n\nThe first stage is the \\emph{semantic modeling} stage, which learns the mapping from the {\\mulan} audio tokens to the semantic tokens $\\mlm$, by modeling the distribution $p(\\mlm_t\\vert\\mlm_{<t},\\mulanAudio)$, where $t$ is the position in the sequence corresponding to a the time step. \nThe second stage is the \\emph{acoustic modeling} stage, where the acoustic tokens $\\sounds_q$ are predicted conditioned on both the {\\mulan} audio tokens and the semantic tokens, modeling the distribution $p(\\sounds_t\\vert\\sounds_{<t},\\mlm, \\mulanAudio)$.\n\n\\looseness=-1\nNotably, to avoid long token sequences, {\\audiolm} proposed to further split the acoustic modeling stage into a coarse and fine modeling stage. We rely on the same approach, where the coarse stage models the first four levels from the output of the SoundStream RVQ, and the fine stage models the remaining eight --- we refer to~\\citet{audiolm} for details.\n\\section{Experimental Setup}\n\\label{sec:exp-setup}\n\n\\subsection{Models}\nWe use decoder-only Transformers for modeling the semantic stage and the acoustic stages of {\\audiolm}.\nThe models share the same architecture, composed of 24~layers, 16~attention heads, an embedding dimension of 1024, feed-forward layers of dimensionality~4096, dropout of~0.1, and relative positional embeddings~\\cite{t5}, resulting in 430M~parameters per stage. \n\n\\subsection{Training and Inference}\n\nBy relying on pretrained and frozen {\\mulan}, we need audio-only data for training the other components of {\\model}. We train {\\soundstream} and w2v-BERT on the Free Music Archive (FMA) dataset~\\cite{fma}, whereas the tokenizers and the autoregressive models for the semantic and acoustic modeling stages are trained on a dataset containing five million audio clips, amounting to 280k~hours of music at 24~kHz. Each of the stages is trained with multiple passes over the training data. We use 30~and 10-second random crops of the target audio for the semantic stage and the acoustic stage, respectively. The {\\audiolm} fine acoustic modeling stage is trained on 3-second crops.\n\nDuring inference, we make use of the joint embedding space between audio and text learned by {\\mulan}, that is, we substitute $\\mulanAudio$ with $\\mulanText$. We then follow the stages described above and obtain $\\sounds$ given $\\mulanText$.\nWe use temperature sampling for the autoregressive sampling in all stages, with temperature of 1.0 for the semantic modeling stage, 0.95 and 0.4 for the coarse and fine acoustic modeling stages respectively. These temperature values were chosen based on subjective inspection to provide a good trade-off between diversity and temporal consistency of the generated music.\n\n\\subsection{Evaluation Dataset}\nTo evaluate {\\model}, we prepare \\dataset, a high-quality music caption dataset, which we make publicly available.\\footnote{\\href{https://www.kaggle.com/datasets/googleai/musiccaps}{kaggle.com/datasets/googleai/musiccaps}}\nThis dataset includes 5.5k~music clips from AudioSet~\\cite{audioset}, each paired with corresponding text descriptions in English, written by ten professional musicians. For each 10-second music clip, {\\datasetshort} provides: (1)~a free-text \\emph{caption} consisting of four sentences on average, describing the music and  \n(2)~a list of music \\emph{aspects}, describing genre, mood, tempo, singer voices, instrumentation, dissonances, rhythm, etc. On average, the dataset includes eleven aspects per clip. See Appendix~\\ref{sec:appendix-eval-data} for a few caption and aspect list examples.\n    \n{\\datasetshort} complements AudioCaps~\\cite{audiocaps}, as they both contain audio clips from AudioSet with corresponding textual descriptions. However, while AudioCaps contains non-music content, {\\datasetshort} focuses exclusively on music and includes highly detailed expert-provided annotations. The examples are extracted from both the train and eval split of AudioSet, covering a diverse distribution of genres, as detailed in Appendix~\\ref{sec:appendix-eval-data}. {\\datasetshort} also provides a \\emph{genre-balanced} split of the data with 1k~examples.\n\n\\subsection{Metrics}\n\nWe compute different metrics to evaluate {\\model}, capturing two important aspects of music generation: the audio quality and the adherence to the text description.\n\n\\paragraph{Fréchet Audio Distance (FAD).}\nThe Fréchet Audio Distance~\\cite{fad} is a reference-free audio quality metric, which correlates well with human perception.\nModels producing samples with a low FAD score are expected to generate plausible audio. However, the generated samples might not necessarily adhere to the text description provided as conditioning.\n\nWe report the FAD based on two audio embedding models, both of which are publicly available: (1)~Trill\\footnote{\\href{https://tfhub.dev/google/nonsemantic-speech-benchmark/trill/3}{tfhub.dev/google/nonsemantic-speech-benchmark/trill/3}} \\cite{trill}, which is trained on speech data, and (2)~VGGish\\footnote{\\href{https://tfhub.dev/google/vggish/1}{tfhub.dev/google/vggish/1}}, \\cite{cnn-for-audio-cls} which is trained on the YouTube-8M audio event dataset \\cite{yt8m-data}.\nBecause of the difference in training data, we expect the models to measure different aspects of the audio quality (speech and non-speech, respectively).\n\n\\paragraph{KL Divergence (KLD).}\n\\looseness=-1\nThere is a many-to-many relationship between text descriptions and music clips compatible with them. It is therefore not possible to directly compare the generated music with the reference at the level of the audio waveform. To assess the adherence to the input text description, we adopt a proxy method similar to the one proposed in~\\citet{yang2022diffsound, audiogen}. Specifically, we use a LEAF~\\citep{zeghidour_leaf} classifier trained for multi-label classification on AudioSet, to compute class predictions for both the generated and the reference music and measure the KL~divergence between probability distributions of class predictions. When the KL-divergence is low, the generated music is expected to have similar acoustic characteristics as the reference music, according to the classifier.%\n\n\\paragraph{MuLan Cycle Consistency (MCC).}\nAs a joint music-text embedding model, {\\mulan} can be used to quantify the similarity between music-text pairs. We compute the {\\mulan} embeddings from the text descriptions in {\\dataset} as well as the generated music based on them, and define the MCC metric as the average cosine similarity between these embeddings.\n\n\\paragraph{Qualitative evaluation.}\n\\looseness=-1\nUltimately, we rely on subjective tests to evaluate the adherence of generated samples to the text description. We set up an A-vs-B human rating task, in which raters are presented with the text description and two samples of music generated by two different models, or one model and the reference music.\nThere are five possible answers: strong or weak preference for A or B, and no preference. The raters are instructed not to take the music quality into account when making their decision, because this aspect of the evaluation is already covered by the FAD metric. \n\nWe consider the output of $n$ different models, in addition to the reference music, thus a total of $n+1$~conditions and $n(n+1)/2$~pairs. To aggregate the results of the pairwise tests and rank conditions, we count the number of ``wins'', that is, how often a condition is  strongly or weakly preferred. The samples are selected from the genre-balanced 1k~subset of our evaluation data.\n\n\\paragraph{Training data memorization.}\n\\looseness=-1\nLarge language models have the capacity to memorize patterns seen in the training data \\cite{trm-extract-train-data}. We adapt the methodology used in~\\citet{carlini-memorization} to study the extent to which {\\model} might memorize music segments. We focus on the first stage, responsible for semantic modeling. We select $N$~examples at random from the training set. For each example, we feed to the model a prompt which includes the MuLan audio tokens $\\mulanAudio$ followed by a sequence of the first $\\memorizationpromptlen$ semantic tokens $\\mlm$, with $\\memorizationpromptlen \\in \\{0,\\ldots,250\\}$, corresponding to up to 10~seconds. We use greedy decoding to generate a continuation of 125~semantic tokens (5~seconds) and we compare the generated tokens to the target tokens in the dataset. \nWe measure exact matches as the fraction of examples for which generated and target tokens are identical over the whole sampled segment. \n\n\\looseness=-1\nIn addition, we propose a methodology to detect approximate matches, based on the observation that sequences of seemingly different tokens might lead to acoustically similar audio segments. Namely, we compute the histogram of semantic token counts over the corresponding vocabulary $\\{0,\\ldots,1023\\}$ from both the generated and target tokens, and define a matching cost measure between histograms as follows. First, we compute the distance matrix between pairs of semantic tokens, which is populated by the Euclidean distances between the corresponding k-means centroids used to quantize w2v-BERT to semantic tokens (see Section~\\ref{sec:audio-representation}). Then, we solve an optimal transport problem to find the matching cost between a pair of histograms using the  Sinkhorn algorithm~\\cite{sinkhorn}, considering only the sub-matrix corresponding to non-zero token counts in the two histograms. To calibrate the threshold used to determine whether two sequences might be approximate matches, we construct negative pairs by permuting the examples with target tokens and measure the empirical distribution of matching costs for such negative pairs. We set the match threshold $\\matchingthreshold$ to $0.85$, which leads to less than 0.01\\% false positive approximate matches. \n\\section{Results}\n\\label{results}\n\n\\looseness=-1\nWe evaluate {\\model} by comparing it with two recent baselines for music generation from descriptive text, namely Mubert~\\cite{Mubert} and Riffusion~\\cite{riffusion}. \nIn particular, we generate audio by querying the Mubert API,\\footnote{\\href{https://github.com/MubertAI}{github.com/MubertAI} (accessed in Dec 2022 and Jan 2023)} and by running inference on the Riffusion model.\\footnote{\\href{https://github.com/riffusion/riffusion-app}{github.com/riffusion/riffusion-app} (accessed on Dec 27, 2022)}\nWe perform our evaluations on {\\dataset}, the evaluation dataset we publicly release together with this paper.\n\n\\paragraph{Comparison to baselines.}\n\\label{results/comparison}\n\n\\cref{table:quant-eval} reports the main quantitative and qualitative results of this paper. In terms of audio quality, as captured by the FAD metrics, on FAD$_{\\text{VGG}}$ {\\model} achieves better scores than Mubert and Riffusion. On FAD$_{\\text{Trill}}$, {\\model} scores similarly to Mubert (0.44 vs. 0.45) and better than Riffusion (0.76).\nWe note that, according to these metrics, {\\model} is capable of generating high-quality music comparable to Mubert, which relies on pre-recorded sounds prepared by musicians and sound designers.\nIn terms of faithfulness to the input text description, as captured by KLD and MCC, {\\model} achieves the best scores, suggesting that it is able to capture more information from the text descriptions compared to the baselines.\n\nWe further supplement our evaluation of text faithfulness with a human listening test. \nParticipants are presented with two 10-second clips and a text caption, and asked which clip is best described by the text of the caption on a 5-point Likert scale. \nWe collect 1200 ratings, with each source involved in 600 pair-wise comparisons. Table~\\ref{table:quant-eval} reports the total number of ``wins'', that is, counting how often the human raters preferred a model in a side-by-side comparison. \n{\\model} is clearly preferred over both baselines, while there is still a measurable gap to the ground truth reference music. Full details of the listening study can be found in Appendix~\\ref{sec:qualitative_appendix}.\n\nListening to examples in which the ground truth was preferred over \\model{} reveals the following patterns:\n(1)~captions are extremely detailed, referring to more than five instruments or describing non musical aspects such as \\textit{``wind, people talking``}; (2)~captions describe temporal ordering of the audio being played; (3)~negations are used, which are not well captured by \\mulan.\n\nOverall, we conclude that: (1)~our approach is able to capture fine-grained information from the rich free-text captions of \\dataset{}; (2)~the KLD and MCC metrics provide a quantitative measure of the faithfulness to the text description, which is in accordance with the human rating study.\n\n\\begin{table}\n\\setlength{\\tabcolsep}{4pt}\n\\caption{Evaluation of generated samples using captions from the {\\dataset} dataset. Models are compared in terms of audio quality, by means of Fréchet Audio Distance (FAD), and faithfulness to the text description, using Kullback–Leibler Divergence (KLD) and  MuLan Cycle Consistency (MCC), and counts of wins in pairwise human listening tests (Wins).}\n\\label{table:quant-eval}\n\\vskip 0.15in\n\\begin{center}\n\\scriptsize\n\\begin{sc}\n\\begin{tabular}{lccccc}\n\\toprule\nModel & FAD$_{\\text{Trill}}$ $\\downarrow$ & FAD$_{\\text{VGG}}$ $\\downarrow$ & KLD $\\downarrow$ & MCC $\\uparrow$ & Wins $\\uparrow$ \\\\\n\\midrule\n\nRiffusion & 0.76 & 13.4 & 1.19 & 0.34 & 158 \\\\\nMubert & 0.45 & 9.6 & 1.58 & 0.32 & 97 \\\\\n\\model{} & 0.44 & 4.0 & 1.01 & 0.51 & 312 \\\\\n\\midrule\n\\datasetshort{} & - & - & - & - & 472 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{sc}\n\\end{center}\n\\vskip -0.1in\n\\end{table}\n\n\\paragraph{Importance of semantic tokens.}\n\\label{results/semantic-tokens}\nTo understand the usefulness of decoupling semantic modeling from acoustic modeling, we train a Transformer model which directly predicts coarse acoustic tokens from MuLan tokens, by modeling $p(\\sounds_t\\vert{\\sounds_{<t}},\\mulanAudio)$.\nWe observe that while the FAD metrics are comparable (0.42~FAD$_{\\text{Trill}}$ and 4.0~FAD$_{\\text{VGG}}$), KLD and MCC scores worsen when removing the semantic modeling stage. In particular the KLD score increases from 1.01 to 1.05, and the MCC score decreases from 0.51 to 0.49, indicating that semantic tokens facilitate the adherence to the text description.\nWe also confirm this qualitatively by listening to the samples. In addition, we observe degradation in long term structure.\n\n\\paragraph{Information represented by audio tokens.}\n\\label{results/semantic-vs-acoustic}\nWe conduct additional experiments to study the information captured by the semantic and the acoustic tokens. In the first study, we fix the {\\mulan} text tokens as well as the semantic tokens, running the acoustic modeling stage multiple times to generate several samples. In this case, by listening to the generated music, it is possible to observe that the samples are diverse, yet they tend to share the same genre, rhythmical properties (e.g., drums), and part of the main melody. They differ in terms of specific acoustic properties (e.g., level of reverb, distortion) and, in some cases, different instruments with a similar pitch range can be synthesized in different examples. In the second study, we fix only the {\\mulan} text tokens and generate both the semantic and acoustic tokens. In this case, we observe a much higher level of diversity in terms of melodies and rhythmic properties, still coherent with the text description. We provide samples from this study in the accompanying material. \n\n\\paragraph{Memorization analysis.}\n\\label{results/memorization}\n\\looseness=-1\nFigure~\\ref{fig:memorization} reports both exact and approximate matches when the length of the semantic token prompt is varied between 0~and 10~seconds. We observe that the fraction of exact matches always remains very small ($< 0.2\\%$), even when using a 10 second prompt to generate a continuation of 5 seconds. Figure~\\ref{fig:memorization} also includes results for approximate matches, using $\\matchingthreshold = 0.85$. We can see a higher number of matches detected with this methodology, also when using only MuLan tokens as input (prompt length $\\memorizationpromptlen=0$) and the fraction of matching examples increases as the length of the prompt increases. We inspect these matches more closely and observe that those with the lowest matching score correspond to sequences characterized by a low level of token diversity. Namely, the average empirical entropy of a sample of 125 semantic tokens is 4.6 bits, while it drops to 1.0 bits when considering sequences detected as approximate matches with matching score less than~0.5. We include a sample of approximate matches obtained with $\\memorizationpromptlen=0$ in the accompanying material. Note that acoustic modeling carried out by the second stage introduces further diversity in the generated samples, also when the semantic tokens match exactly. \n\n\\begin{figure}[t]\n\\begin{center}\n    \\centerline{\\includegraphics[clip, width=0.42\\textwidth]{figures/memorization.pdf}}\n    \\vspace{-2mm}\n    \\caption{Memorization results for the semantic modeling stage. We compare the semantic tokens generated for 5~seconds of audio to corresponding tokens in the training set, considering exact and approximate matches.}\n    \\label{fig:memorization}\n    \\vspace{-6mm}\n\\end{center}\n\\end{figure}\n\n\\section{Extensions}\n\\paragraph{Melody conditioning.}\n\\label{melody-conditioning}\nWe extend {\\model} in such a way that it can generate music based on both a text description and a melody, which is provided in the form of humming, singing, whistling, or playing an instrument. \nThis requires extending the conditioning signal in a way that captures the target melody. To this end, we create a synthetic dataset composed of audio pairs with matching melodies but different acoustics. To create such pairs, we use different versions of the same music clip, such as covers, instrumentals, or vocals. Additionally, we acquire data pairs of people humming and singing. %\nWe then train a joint embedding model such that when two audio clips contain the same melody, the corresponding embeddings are close to each other. For implementation details we refer to Appendix~\\ref{appendix:melody-conditioning}. \n\nTo extract the melody conditioning for {\\model}, we quantize the melody embeddings with RVQ, and concatenate the resulting token sequences with the {\\mulan} audio tokens $\\mulanAudio$.\nDuring inference, we compute melody tokens from the input audio clip and concatenate them with the {\\mulan} text tokens $\\mulanText$.\nBased on this conditioning, {\\model} can successfully generate music which follows the melody contained in the input audio clip, while adhering to the text description.\n\n\\paragraph{Long generation and story mode.}\n\\label{story-mode}\n\\looseness=-1\nIn {\\model}, generation is autoregressive in the temporal dimension which makes it possible to generate sequences longer than those used during training. In practice, the semantic modeling stage is trained on sequences of 30 seconds. To generate longer sequences, we advance with a stride of 15~seconds, using 15~seconds as prefix to generate an additional 15~seconds, always conditioning on the same text description. With this approach we can generate long audio sequences which are coherent over several minutes.\n\nWith a small modification, we can generate long audio sequences while changing the text description over time. Borrowing from ~\\citet{phenaki} in the context of video generation, we refer to this approach as \\emph{story mode}. Concretely, we compute $\\mulanText$ from multiple text descriptions and change the conditioning signal every 15 seconds. The model generates smooth transitions which are tempo consistent and semantically plausible, while changing music context according to the text description.\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nWe introduce {\\model}, a text-conditioned generative model that produces high-quality music at 24~kHz, consistent over several minutes, while being faithful to the text conditioning signal. We demonstrate that our method outperforms baselines on {\\dataset}, a hand-curated, high-quality dataset of 5.5k~music-text pairs prepared by musicians.\n\nSome limitations of our method are inherited from {\\mulan}, in that our model misunderstands negations and does not adhere to precise temporal ordering described in the text. Moreover, further improvements of our quantitative evaluations are needed. Specifically, since MCC also relies on {\\mulan}, the MCC scores are favorable to our method.\n\nFuture work may focus on lyrics generation, along with improvement of text conditioning and vocal quality. Another aspect is the modeling of high-level song structure like introduction, verse, and chorus. Modeling the music at a higher sample rate is an additional goal.\n\\section{Broader Impact}\n\\label{sec:broader-impact}\n\n{\\model} generates high-quality music based on a text description, and thus it further extends the set of tools that assist humans with creative music tasks. However, there are several risks associated with our model and the use-case it tackles. The generated samples will reflect the biases present in the training data, raising the question about appropriateness for music generation for cultures underrepresented in the training data, while at the same time also raising concerns about cultural appropriation. \n\n\\looseness=-1\nWe acknowledge the risk of potential misappropriation of creative content associated to the use-case. In accordance with responsible model development practices, we conducted a thorough study of memorization, adapting and extending a methodology used in the context of text-based LLMs, focusing on the semantic modeling stage. We found that only a tiny fraction of examples was memorized exactly, while for $1\\%$ of the examples we could identify an approximate match. We strongly emphasize the need for more future work in tackling these risks associated to music generation --- we have no plans to release models at this point.\n\n\\newpage\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Noise2Music: Text-conditioned Music Generation with Diffusion Models}\n\n\\begin{document}\n\n\\twocolumn[\n\\icmltitle{Noise2Music: Text-conditioned Music Generation with Diffusion Models}\n\n\\icmlsetsymbol{equal}{*}\n\\icmlsetsymbol{core}{$\\dagger$}\n\n\\begin{icmlauthorlist}\n\\icmlauthor{Qingqing Huang}{equal,g}\n\\icmlauthor{Daniel S. Park}{equal,g}\n\\icmlauthor{Tao Wang}{core,g}\n\\icmlauthor{Timo I. Denk}{core,g}\n\\icmlauthor{Andy Ly}{core,g}\n\\icmlauthor{Nanxin Chen}{core,g}\n\\icmlauthor{Zhengdong Zhang}{g}\n\\icmlauthor{Zhishuai Zhang}{g}\n\\icmlauthor{Jiahui Yu}{g}\n\\icmlauthor{Christian Frank}{g}\n\\icmlauthor{Jesse Engel}{g}\n\\icmlauthor{Quoc V. Le}{g}\n\\icmlauthor{William Chan}{w}\n\\icmlauthor{Zhifeng Chen}{w}\n\\icmlauthor{Wei Han}{core,g}\n\\end{icmlauthorlist}\n\n\\icmlaffiliation{g}{Google Research}\n\\icmlaffiliation{w}{Work done while at Google}\n\n\\icmlcorrespondingauthor{Qingqing Huang}{qqhuang@google.com}\n\n\\icmlkeywords{Generative models, Diffusion, Music generation}\n\n\\vskip 0.3in\n]\n\n\\printAffiliationsAndNotice{\\icmlEqualContribution\\icmlCoreContributors} % otherwise use the standard text.\n\n\\begin{abstract}\nWe introduce Noise2Music, where a series of diffusion models is trained to generate high-quality 30-second music clips from text prompts. Two types of diffusion models, a generator model, which generates an intermediate representation conditioned on text, and a cascader model, which generates high-fidelity audio conditioned on the intermediate representation and possibly the text, are trained and utilized in succession to generate high-fidelity music. We explore two options for the intermediate representation, one using a spectrogram and the other using audio with lower fidelity. We find that the generated audio is not only able to faithfully reflect key elements of the text prompt such as genre, tempo, instruments, mood, and era, but goes beyond to ground fine-grained semantics of the prompt. Pretrained large language models play a key role in this story---they are used to generate paired text for the audio of the training set and to extract embeddings of the text prompts ingested by the diffusion models.\n\nGenerated examples: \n\n\\href{\\website}{\\website}\n\n\\end{abstract}\n\n\\section{Introduction}\n\nDeep neural networks have been shown to have remarkable generative ability. In this work, we explore the generative capability of deep models for audio.\n\nWe introduce Noise2Music, a diffusion-based \\cite{sohl2015deep,song2019generative,ho2020denoising} method of generating music from text prompts and demonstrate its capability by generating 30-second long 24kHz music clips.\n\n\\textbf{Modeling:} We train a series of cascading diffusion models \\cite{ho2022cascaded}, where the first model learns the generative task of producing a compressed representation of a 30-second waveform from a text prompt, and the second model learns to generate a 16kHz~waveform conditioned on the compressed representation and optionally the text prompt. We have investigated two options for the intermediate representation: a log-mel spectrogram, or a 3.2kHz~waveform. 1D~U-Nets are used for learning the noise vectors for the diffusion model. The diffusion models are conditioned on user prompts in the format of free-form text, which are encoded by a pre-trained language model (LM) and ingested by the 1D~U-Net layers via cross attention. A final super-resolution cascader is used to generate the 24kHz~audio from the 16kHz waveform.\n\n\\textbf{Data mining:} A large amount of training data is crucial for producing high-quality samples from a deep generative model. We employ a data mining pipeline to construct a large-scale training dataset of diverse music audio clips, each paired with multiple descriptive text labels. The text labels for the audio are generated by employing a pair of pre-trained deep models: first, we use a large language model to generate a large set of generic music descriptive sentences as caption candidates; we then use a pre-trained music-text joint embedding model to score each unlabeled music clip against all the caption candidates and select the captions with the highest similarity score as pseudo labels for the audio clip. We are able to annotate O(150K) hours of {audio sources} this way\nto constitute our training data.\n\n\\textbf{\\MusicCap:} As a by-product of this work, we introduce MuLan-LaMDA Music Caption dataset (\\MusicCap), consisting of O(400K) music-text pairs obtained by annotating the music content from AudioSet by the process described above. Compared to the original AudioSet ontology, where 141 out of 632 label classes are music related, the captions in \\MusicCap ~come from a large vocabulary consisting of 4 million music descriptive sentences and phrases, which have a much higher degree of diversity and granularity. We expect this dataset to be utilized for applications beyond sound classification, e.g., music captioning, retrieval or generation.\n\n\\textbf{Evaluation:} We measure the quality of our text conditioned music generation model with two metrics: the Fr\\'echet Audio Distance (FAD)~\\cite{kilgour2018fr} which measures how the quality of generated audio clips compare to that of two benchmark datasets, e.g. the music split of AudioSet~\\cite{gemmeke2017audio} and MagnaTagATune~\\cite{law2009evaluation}; and the MuLan similarity score~\\cite{mulan2022} which measures the semantic alignment between text prompts and the corresponding generated audio clips. \n\n\\textbf{Generative ability:} Our models demonstrate that they can go beyond simple music attribute conditioning, e.g., genre, instrument, era, and are able to handle complex and fine-grained semantics which can reflect soft attributes such as atmosphere, feeling or activity. This is achieved by constructing a training dataset that not only relies on the metadata tags, but that also leverages the pre-trained music-text joint embedding model to ground the semantics to audio features. Cherry-picked examples of music generated from text prompts can be found in  \\href{\\website#table-1}{\\websiteDisplay\\#table-1} and \\href{\\website#table-4}{\\websiteDisplay\\#table-4}.\n\n\\section{Related Work}\n\\noindent\\textbf{Generative models:}\nDeep generative models have a long and successful history in a wide range of domains. More recently, a significant amount of effort has been focused toward scaling up the dataset size for training models that can produce extremely high quality samples. Here we compile an incomplete list of such recent developments in text~\\cite{brown2020language,thoppilan2022lamda}, speech~\\cite{wang2018style, chen2021wavegrad,audiolm}, images~\\cite{ramesh2022hierarchical,saharia2022photorealistic,yu2022scaling}, and audio~\\cite{briot2021artificial,dhariwal2020jukebox,mubert,kreuk2022audiogen}. \n \n\n\\noindent\\textbf{Diffusion models:}\nDiffusion models, introduced in \\cite{sohl2015deep,song2019generative,ho2020denoising} have shown the capability to generate high quality images \\cite{ho2020denoising,ho2022cascaded}, audio \\cite{yang2022diffsound, popov2021grad} and video \\cite{ho2022video,ho2022imagen}. Cascaded diffusion models \\cite{ho2022cascaded,saharia2022photorealistic}, which uses a series of diffusion models to generate a low-fidelity image and refine the image in succession to produce a high-fidelity image, has been adapted to audio in this work.\n\n\\noindent\\textbf{Audio generation:}\nVarious methods have been employed to generate audio conditioned on external input. Some relevant examples are provided in the context of the text-to-audio task, in which text-conditioned spectrogram generation and spectrogram-conditioned audio has been intensively studied \\cite{popov2021grad, chen2021wavegrad, kong2021diffwave, wu2021otts, chen2022infergrad}. Restricting our attention to audio generation based on descriptive text, text conditioned general sound event generation has been approached with auto-regressive methods by AudioGen~\\cite{kreuk2022audiogen} as well as diffusion-based methods that operate on discrete audio codes by DiffSound~\\cite{yang2022diffsound}. If we narrow our scope to music generation, Jukebox~\\cite{dhariwal2020jukebox}, Mubert~\\cite{mubert}, and \\MusicLM~have taken an auto-regressive approach, while Riffusion~\\cite{Forsgren_Martiros_2022} employed diffusion for spectrogram generation.\n\n\\noindent\\textbf{Conditional signals in audio generation:}\nBroadly speaking, two approaches have been taken on how the conditional signal, which steers the model to generate a specific style of music, is parameterized and communicated to an audio generation model. One approach is to project the signal to a pre-defined, interpretable embedding space---Jukebox~\\cite{dhariwal2020jukebox} relies on a fixed vocabulary of artists and genres mined from the training data to condition the decoder, while Mubert~\\cite{mubert} matches the user prompt to a set of tags in a predefined vocabulary. The other, taken by works such as AudioGen~\\cite{kreuk2022audiogen} and \\MusicLM~is to use a pre-trained text encoder to encode arbitrary user prompts.\n\n\\noindent\\textbf{Authors' Note:}\nDuring the completion of this work, concurrent research which has overlap with this work has appeared \\cite{schneider2023}.\n\n\\section{Methods}\n\n\\subsection{Diffusion models}\n\\label{ss:diffusion-models}\n\nDiffusion models \\cite{sohl2015deep,song2019generative,ho2020denoising} are powerful generative models that generate a sample by iteratively denoising random noise. Here we review the minimal amount of information on diffusion models required for understanding our work. More details can be found in the supplementary material.\n\nThe input to a diffusion model, which we consider to be a generative model of some sample space, is the conditioning signal $\\mathbf{c}$, a randomly sampled time step $t$ and a sample $\\mathbf{x}_t$ obtained by corrupting the original sample $\\mathbf{x}$ via a Gaussian diffusion process with a noise schedule parameterized by the standard deviation $\\sigma_t$ of the noise at time $t$. The range of time $t$ is set to be $[0, 1]$, from which it is uniformly sampled during training, and the diffusion is viewed to progress in the direction of increasing time. The dynamics of Gaussian diffusion are well understood---the distribution of $\\mathbf{x}_t$ is completely parameterized by a single noise vector $\\bm{\\epsilon}$ that belongs to a standard normal distribution, as $\\mathbf{x}_t$ maybe written as a function of the original sample, the deterministic noise schedule, and the noise vector $\\bm{\\epsilon}$, i.e., $\\mathbf{x}_t(\\mathbf{x}, \\sigma, \\bm{\\epsilon})$, where it should be understood that $\\sigma$ is used to denote the entire noise schedule. The model $\\bm{\\epsilon}_\\theta$ is trained to identify the noise vector given this input. The diffusion loss can be written as\n\\begin{equation}\n\\mathbb{E}_{\\mathbf{x}, \\mathbf{c}, \\bm{\\epsilon}, t}\n\\left[ w_t \\Vert  \\bm{\\epsilon}_\\theta (\\mathbf{x}_t, \\mathbf{c}, t)- \\bm{\\epsilon} \\Vert^2 \\right] \\,,\n\\end{equation}\nwhere $w_t$ is a fixed weight function of choice.\n\nInference is carried out by taking random noise at time $t=1$ and denoising it by utilizing the noise predictions given by the model. We use ancestral (or DDPM) sampling \\cite{ho2020denoising}, which provides a flexible framework for inference allowing multiple parameters that can affect the quality of the generated sample. First, the level of stochasticity of the denoising process can be controlled by varying the stochasticity parameter $\\gamma$ of the sampler. Also, an arbitrary denoising schedule can be used, where one may choose an arbitrary partition of the interval $0 = t_0 < \\cdots < t_n =1$ to discretize the denoising steps.\n\nThus a variety of choices present themselves when one wishes to train a diffusion model. We utilize multiple options with respect to the following elements, further details of which can be found in the supplementary material:\n\\begin{itemize}[itemsep=1pt]\n\\item Loss weight ($w_t$): simplified weight $w_t =1$ \\cite{ho2020denoising} and sigma weight $w_t = \\sigma_t^2$\n\\item Variance schedule: linear \\cite{ho2020denoising} and cosine \\cite{nichol2021improved} schedules\n\\item Stochasticity parameter: $\\gamma= 0$ or $1$\n\\item Denoising step schedule\n\\end{itemize}\n\n\\textbf{Classifier-free guidance (CFG):} CFG \\cite{ho2022classifier} is a method for improving the alignment between generated samples and conditional inputs. The conditional input of a portion of the training samples in each training batch are hidden from the network during training, enabling the network to learn how to predict the noise vector unconditionally and conditionally. At inference, the noise vector with and without the conditional input are computed, and the final noise vector applied is set to $w \\bm{\\epsilon}_\\theta(\\mathbf{x}_t, \\mathbf{c}) + (1 - w) \\bm{\\epsilon}_\\theta(\\mathbf{x}_t, \\cdot) $ with $w > 1$. Dynamic clipping \\cite{saharia2022photorealistic} is applied to avoid over-saturation due to CFG.\n\n\\subsection{Architecture}\n\nWe deploy the 1D Efficient U-Net, a one-dimension version of the Efficient U-Net introduced in \\cite{saharia2022photorealistic}, for the diffusion model. The U-Net model, depicted in \\cref{U-Net}, consists of a series of down-sampling and up-sampling blocks which are connected by residual connections. A down/up-sampling block consists of a down/up-sampling layer followed by a series of blocks obtained by composing 1D convolutional layers, self/cross-attention layers and combine layers. The combine layer enables a single vector to interact with a sequence of vectors, where the single vector is used to produce a channel-wise scaling and bias. These blocks closely follow the structure of the blocks of the efficient U-Nets constructed in \\cite{saharia2022photorealistic}, with the two-dimensional convolutions replaced by their one-dimensional counterparts. The exact structure of the blocks are further elaborated in the supplementary material.\n\nThere are four possible routes of entry to the model. The stacked input and output both consist of sequences of some length $T$, while the diffusion time $t$ is encoded into a single time embedding vector and interacts with the model through the aforementioned combine layers within the down and up-sampling blocks. Given that we would like to produce a sequence of length $T$, the noisy sample $\\mathbf{x}_t$ is always part of the stacked input on the left-most side of the figure, while the output is interpreted as the noise prediction $\\bm\\epsilon$. For the cascading models, the low-fidelity audio on which the model is conditioned on can be up-sampled and stacked. Meanwhile, a sequence of vectors with an arbitrary length may interact with the blocks through cross-attention. This is the route through which the text prompts are fed into the model. There is also room for the model to be conditioned on an aligned, but compressed representation of the sequence by addition at the bottom of the ``U\" of the U-Net.\n\n\\begin{figure*}[ht]\n\\vskip 0.05in\n\\begin{center}\n\\centerline{\\includegraphics[width=1.7\\columnwidth]{figures/unet.png}}\n\\vskip -0.05in\n\\caption{U-Net architecture used for the diffusion models. A series of down-sampling, then up-sampling blocks is applied to the main input sequence of length $T$ to produce an output sequence of length $T$. The outputs of the down-sampling blocks are added to the outputs of up-sampling blocks via residual connections. There are four modes of inputs to the model. The first is the (possibly stacked) main input of sequence length $T$, entering on the left-hand side of the diagram. $T$ is the target sequence length. Second, there is a time embedding vector. Third, there can be a text embedding sequence that can be attended to by the down/up-sampling blocks via cross attention. Lastly, there can be input of length $T/C$ that is aligned with the sequence of length $T$ with compression rate $C$.}\n\\label{U-Net}\n\\end{center}\n\\vskip -0.25in\n\\end{figure*}\n\n\\subsection{Cascaded diffusion}\n\nWe train two kinds of diffusion models in this work to produce high-quality 30-second audio from text prompts. Following \\cite{ho2022cascaded}, we train generator models that generate some intermediate representation of the final audio conditioned on a text prompt, and cascader models that produce the final audio based on the intermediate representation. For the intermediate representation, we consider both low-fidelity audio and spectrograms.\n\n\\subsubsection{Waveform Model}\n\n\\textbf{Generator Model:} The generator model generates 3.2kHz audio that is conditioned on the text input. A sequence of vectors derived from the text input is produced and fed into the network as a cross-attention sequence.\n\n\\textbf{Cascader Model:} The cascader model generates 16kHz audio that is conditioned on both the text prompt and the low-fidelity audio generated by the generator model based on the text prompt. The text conditioning takes place via cross attention. Meanwhile, the low-fidelity audio is up-sampled and stacked with $\\mathbf{x}_t$ and fed into the model. The up-sampling is done by applying fast Fourier transform (FFT) to the low-fi audio sequence and then applying inverse FFT to obtain the high-fi audio from the low-fi Fourier coefficients.\n\n\\subsubsection{Spectrogram Model}\n\n\\textbf{Generator Model:} This model generates a log-mel spectrogram conditioned on the text input. The spectrgram has 80 channels and a frequency of 100 features per second. The input and output sequences now have a channel dimension in addition to the sequence dimension. The pixel values of the log-mel spectrogram are normalized to lie within $[-1, 1]$. Text conditioning is achieved through cross attention.\n\n\\textbf{Vocoder Model:} The vocoder model generates 16kHz audio that is conditioned only on the spectrogram, which is treated as aligned input. The down and up-sampling rates of the U-Net model are tuned to achieve the compression rate of the spectrogram against the audio.\n\n\\subsubsection{Super-resolution cascader}\n\nA final light-weight cascader is used to generate 24kHz audio from the 16kHz waveform produced by either model. The 16kHz audio is up-sampled and stacked with $\\mathbf{x}_t$ as input to the model. Text conditioning is not used for this model.\n\n\\subsection{Text understanding}\n\nIt has been shown in the context of text-to-image diffusion models \\cite{saharia2022photorealistic,rombach2021highresolution} that powerful text encoders are able to capture the complexity and compositionality of music descriptive text prompts. We adopt the T5 encoder \\cite{raffel2020exploring} and use the non-pooled token embedding sequence to condition the diffusion models. A thorough comparison with alternative contextual signals such as embeddings from different large language models, or a single vector embedding derived from CLIP-like \\cite{radford2021learning} text encoders trained on music-text pairs \\cite{mulan2022,ilaria-contrastive-audio} is beyond the scope of this work.\n\n\\subsection{Pseudo labeling for music audio}\nHaving large scale training data is a necessary component for ensuring the quality of generative deep neural networks. For example, Imagen \\cite{saharia2022photorealistic} was trained on O(1B) image-text pairs.\nDespite the fact that music content is widely available, high quality paired music-text data is scarce, especially in the case of free-form text that describes the music attributes beyond high-level metadata such as title, artist name, album name, and release year.\n\nTo generate such music-text pairs, we take a pseudo-labeling approach via leveraging MuLan~\\cite{mulan2022}, a pre-trained text and music audio joint embedding model, together with LaMDA~\\cite{thoppilan2022lamda}, a pre-trained large language model, to assign pseudo labels with fine-grained semantic to unlabeled music audio clips.\n\nWe first curate several music caption vocabulary sets, each consisting of a large list of music descriptive texts. As demonstrated below, these texts vastly differ from the captions from the label classes in standard music classification benchmarks, e.g., MagnaTagATune, FMA, and AudioSet, in their scale and the fine-grained semantic granularity. We consider the following three caption vocabularies:\n\n\\textbf{LaMDA-LF}: We prime the large language model LaMDA to describe a list of 150k~popular songs provided the song title and artist names. The precise prompt template is provided in the supplementary material. We then process the LaMDA responses into 4~million clean long-form sentences that are likely to be describing music.\nWe use LaMDA as our LM of choice because it is trained for dialogue applications, and expect the generated text to be closer to user prompts for generating music. \n\n\\textbf{Rater-LF}: We obtain 10,028 rater written captions from \\EvalSet, and split each caption into individual sentences. This produces 35,333 music-describing long-form sentences.\n\n\\textbf{Rater-SF}: From the same evaluation set above, we collect all the short-form music aspect tags written by the raters, which amounts to a vocabulary of size 23,906.\n\nExamples of the caption vocabulary are presented in \\cref{table:caption-vocab}.\n\n\\begin{table}\n\\caption{Caption vocabulary examples.}\n\\vskip 0.05in\n  \\label{table:caption-vocab}\n  \\centering\n  \\resizebox{0.95\\columnwidth}{!}{\n  \\begin{tabular}{ll}\n    \\toprule\n    {\\bf Vocabulary} & {\\bf Examples}  \\\\\n    \\midrule\n    \\begin{tabular}{@{}l@{}}\n    LaMDA-LF \\\\\n    (4M)\n    \\end{tabular}\n    &\n    \\begin{tabular}{@{}l@{}}\n    ``A light, atmospheric drum groove provides a tropical feel.\", \\\\\n    ``A light EDM drumbeat carries a bass guitar, strings, \\\\\n    ~a simple piano, and percussion in the background.\" \\\\\n    \\end{tabular} \\\\\n    \\midrule\n    \\begin{tabular}{@{}l@{}}\n    Rater-LF \\\\\n    (35k)\n    \\end{tabular} &\n    \\begin{tabular}{@{}l@{}}\n    ``A Scottish tenor drum plays a marching beat.\"\\\\\n    ``A bass guitar with a punchy sound contrasts the guitar.\" \\\\\n    \\end{tabular} \\\\\n    \\midrule\n    \\begin{tabular}{@{}l@{}}\n    Rater-SF \\\\\n    (24k)\n    \\end{tabular} &\n    \\begin{tabular}{@{}l@{}}\n    ``50's pop\", ``wide passionate male vocal\", ``vintage vibes\",\\\\\n     ``patriotic mood\", ``vivacious cello\", ``exercise music\" \\\\\n    \\end{tabular} \\\\\n    \\bottomrule\n  \\end{tabular}\n  }\n\\vskip-0.1in\n\\end{table}\n\nWe use the MuLan model as a zero-shot music classifier to assign captions from the vocabulary to unlabeled audio clips.\nMuLan consists of a text encoder and an audio encoder, which are trained on a large amount of highly noisy text-music pairs with a contrastive learning scheme. Similar to how CLIP \\cite{radford2021learning} co-embeds image and text, a 10-second long music audio clip and a sentence that describes the music are placed closely in the same semantic embedding space learned by MuLan.\nFor each audio clip, we compute its audio embedding by first segmenting the clip into non-overlapping 10-second windows, and computing the average of the MuLan audio embeddings of each window. The text embeddings of all the candidate captions in the vocabulary are also computed. The top $K$ captions that are closest to the audio in the embedding space are selected for each clip.  \nWe compute the frequency counts of the captions among all clips, and for each clip further sample $K'$ out of the $K$ captions with probabilities inverse to the frequency counts.\nThis last sampling step serves to balance the label distribution and increase the diversity of the captions. We use $K=10$ and $K'=3$.\n\nAs a warm up for pseudo-labeling our large training set, we produce \\MusicCap, a music captioning dataset derived from the AudioSet \\cite{gemmeke2017audio}, by applying this pseudo-labeling method to 388,262 / 4,497 examples from the AudioSet train / test sets which have labels in the music subtree of AudioSet ontology. Each 10-second audio with music content is associated with 3~captions from the LaMDA-LF vocabulary, 3~captions from Rater-LF, and 6~short form captions from Rater-SF. \n\n\\subsection{Training data mining}\nTo assemble a large-scale collection of audio-text pairs, we collect approximately 6.8M {music audio source files.}\nFrom each {soundtrack}, we extract six non-overlapping 30-second clips. This amounts to nearly 340k~hours of music. Audio is sampled at 24kHz for training the super-resoluton model and 16kHz for training all other models.\n\nFor each {soundtrack}, we consider three types of noisy text labels---the song title, named entity tags associated with the {soundtrack} (e.g., genre, artist name, instrument), and the pseudo labels. We use three pseudo labels from the LaMDA-LF vocabulary, and six pseudo labels from the Rater-SF vocabulary.\nThe pseudo labels from LaMDA-LF and Rater-SF provide complementary information to the named entity tags. Compared to the objective and high-level tags, the pseudo labels include subjective descriptions related to activity (``music for highway driving\") and mood (``a laid back feel\"), and also include compositional elements with fine-grained semantics. \nSince we evaluate our model on \\EvalSet~from which the sentences of Rater-LF were derived, we exclude any pseudo labels from the Rater-LF vocabulary from our training data.\n\nWe include a small amount of high-quality audio to the large pseudo-labeled training set. The audio is taken from a subset of music tracks, which does not require attribution, from an {internally}  {maintained} music library. The music tracks are segmented to non-overlapping 30-second clips, while the metadata of the tracks are concatenated to form the text prompt of the audio. This contributes O(300) hours of annotated audio to our training data.\n\n\\section{Experiments and Results}\n\n\\subsection{Model training details}\nWe train four 1D~U-Net models, the waveform generator and cascader, and the spectrogram generator and vocoder for this work. We have summarized some basic information about the models in \\cref{t:model-specs}, while we relegate further details about the models to the supplementary material. We note that we found the sigma-weighted loss, which weighs the loss more heavily on the ``back end\" of the denoising schedule, crucial for convergence of the spectrogram generator.\n\n\\begin{table}\n\\caption{Models trained in this work. The token length refers to the token length of the text prompts at training time.}\n\\vskip 0.05in\n  \\label{t:model-specs}\n  \\centering\n  \\resizebox{0.95\\columnwidth}{!}{\n  \\begin{tabular}{lccccc}\n    \\toprule\n    {\\bf Model}\n    & {\\bf \\# Params}\n    & {\\bf \\# Training}\n    & {\\bf Token}\n    & {\\bf Loss}\n    & {\\bf Noise} \\\\[-1pt]\n    & \n    & {\\bf steps}\n    & {\\bf length}\n    & {\\bf weight} \n    & {\\bf schedule} \\\\\n    \\midrule\n    Waveform generator & 724M & 1.6M & 64 & Simplified & Cosine \\\\\n    Waveform cascader & 487M & 460k & 64 & Simplified & Linear \\\\\n    \\midrule\n    Spectrogram generator & 745M  & 1.8M & 96 & Sigma & Linear \\\\\n    Spectrogram vocoder & 25.7M & 840k & - & Simplified & Linear \\\\\n    \\midrule\n    Super-resolution cascader & 81M & 270k & - & Simplified & Linear \\\\\n    \\bottomrule\n  \\end{tabular}\n  }\n\\vskip-0.2in\n\\end{table}\n\nAll the models, with the exception of the vocoder, are trained on audio-text pairs, while the vocoder is only trained on audio. For each audio sample, a text batch is formed. The three long prompts constitute three independent elements of the text batch, while the shorter prompts are concatenated, then segmented into a set token length reported in \\cref{t:model-specs} and added to the text batch. For each audio clip, a random element of the corresponding text batch is selected at training time and fed to the model as the paired text to the audio.\n\nThe models are trained with Adam optimization with $\\beta_1=0.9$ and $\\beta_2=0.999$. A cosine learning rate schedule with the end point set to 2.5 M steps is used with peak learning rate 1e-4 and 10k warm-up steps. An exponential moving average (EMA) of the model parameters are taken with decay rate 0.9999 and used at inference time. The super-resolution cascader is trained with batch size 4096, while all other models use batch size 2048. To apply CFG at inference time, we occlude the text prompts for 10\\% of the samples in each training batch. For these samples, the output of the cross attention layers are set to zero.\n\nWhile the generator models use self-attention, the cascaders and vocoder do not. Thus while we need to train the generator models on the entire 30-second representation of the audio, the cascader and vocoder models are trained on 3 to 4-second randomly sampled snippets.\n\nFollowing \\cite{ho2022cascaded}, two augmentations are applied at training time for the cascader/vocoder models. One is to randomly corrupt the conditioning low-fidelity audio or the spectrogram input by applying diffusion noise. To do so, a random diffusion time is chosen within $[0, t_\\text{max}]$ and applied to the intermediate representation of the audio, i.e., the up-sampled low-fi audio or the spectrogram. For the cascader $t_\\text{max}$ is set to 0.5 while for the vocoder and super-resolution cascader it is set to 1.0. The other is blur augmentation. For the cascader model, a 1D blur kernel of size $10$ is used with a Gaussian blur kernel whose standard deviation ranges from $0.1$ to $5.0$. For the vocoder model, a 2D 5x5 blur kernel is applied with the standard deviation ranging from 0.2 to 1.0.\n\n\\subsection{Model inference and serving}\n\n\\subsubsection{Model inference}\n\n\\begin{table}\n\\caption{Inference parameters for the models used in this work.}\n\\vskip 0.05in\n  \\label{t:model-inference-params}\n  \\centering\n  \\resizebox{0.9\\columnwidth}{!}{\n  \\begin{tabular}{lccc}\n    \\toprule\n    {\\bf Model}\n    & {\\bf Denoising}\n    & {\\bf Stochasticity}\n    & {\\bf CFG scale}\\\\[-1pt]\n    & {\\bf step schedule}\n    & {\\bf parameter}\n    &\\\\\n    \\midrule\n    Waveform generator & Front-heavy & 0 & 10.0\\\\\n    Waveform cascader & Front-heavy & 1 & 5.0 \\\\\n    \\midrule\n    Spectrogram generator & Back-heavy & 0 & 5.0 \\\\\n    Spectrogram vocoder & Front-heavy & 0 & N/A \\\\\n    \\midrule\n    Super-resolution cascader & Front-heavy & 0 & N/A \\\\\n    \\bottomrule\n  \\end{tabular}\n  }\n\\vskip-0.2in\n\\end{table}\n\nWe adjust three inference hyperparameters, the denoising schedule, the stochasticity parameter, and the CFG scale. The parameters used for each model are listed in \\cref{t:model-inference-params}.\n\nWe parameterize the denoising step schedule by the time step sizes $[\\delta_1, \\cdots, \\delta_N]$ that translate into denoising steps introduced in section \\ref{ss:diffusion-models} via accumulation: $t_n = \\sum_{i=1}^n \\delta_n$. The inference cost is proportional to the number of time-steps. Thus optimizing the time step schedule with a fixed inference cost amounts to distributing a fixed number of time steps that add up to the total time, 1. The parameter space for the denoising step schedule is extremely large. Nevertheless, we experiment with three different kinds of schedules we denote ``front-heavy,\" ``uniform,\" and ``back-heavy.\" The front-heavy schedule allots many steps to the ``front\" of the schedule near $t=0$ whereas the ``back-heavy\" schedule expends more steps near $t=1$. The uniform schedule uses evenly-spaced time steps. The exact schedules used are produced in the supplementary material.\n\n\\subsubsection{Model serving}\nWe serve the models on Google Cloud TPU~V4, where each service request generates four 30-second music clips. We apply GSPMD \\cite{gspmd} to partition the model on four TPU~V4 devices, reducing the serving time by more than 50\\%. \\cref{table:decode-latency} shows the inference time cost when the model is served on four TPU V4 to produce four samples.\n\n\\begin{table}\n\\caption{Inference time cost on four TPU~V4 for four samples.}\n\\vskip 0.05in\n  \\label{table:decode-latency}\n  \\centering\n  \\resizebox{0.8\\columnwidth}{!}{\n  \\begin{tabular}{lrrr}\n    \\toprule\n    {\\bf Model} & {\\bf time/step (ms)} & {\\bf steps} & {\\bf time (s)}  \\\\\n    \\midrule\n    Waveform generator & 25.0 & 1000 & 25.0 \\\\\n    Waveform cascader & 75.0 & 800 & 60.0 \\\\\n    \\midrule\n    Spectrogram generator & 8.3 & 1000 & 8.3 \\\\\n    Spectrogram vocoder & 29.9 & 100 & 0.3 \\\\\n    \\midrule\n    Super-resolution cascader & 71.7 & 800 & 57.3 \\\\\n    \\bottomrule\n  \\end{tabular}\n  }\n\\vskip-0.2in\n\\end{table}\n\n\\subsection{Evaluation}\n\n\\subsubsection{Parameter selection for the models}\n\\label{ss:model-parameter-selection}\n\nModel parameters, including the architecture, training hyperparameters, checkpoints and inference parameters are selected in a heuristic fashion. A small set of dev prompts, independent of the prompts in any of the evaluation sets presented, are devised by the authors, which are used to generate audio from the trained models. Model parameters are selected based on the quality of the generation results, evaluated according the judgement of the authors, as well as practical limitations such as the availability of computational resources and time. Evaluations are conducted on 16kHz waveforms---the super-resolution cascader is not utilized to generate waveforms for producing evalution metrics.\n\n\\subsubsection{Evaluation metrics} \nWe measure the quality of our text conditioned music generation model with two kinds of metrics: the Fr\\'echet Audio Distance (FAD)~\\cite{kilgour2018fr} and the MuLan similarity score~\\cite{mulan2022}. \n\nFAD measures how the audio quality of the generated audio examples compare to that of a set of reference audio clips.\nIn particular, an audio encoder is used to compute the audio embeddings of both the set of generated audio examples and the background audio clips in the evaluation dataset. Assuming that the distribution of the embeddings from each set are Gaussian, and the Fre\\'echet distance between the two distributions are computed from the mean embedding vectors and the correlation matrix of the two sets.\n\nThree audio encoders are utilized for computing the FAD metric:\na VGG\\footnote{\\href{https://tfhub.dev/google/vggish/1}{tfhub.dev/google/vggish/1}} audio event embedding model \\cite{cnn-for-audio-cls} trained on YouTube-8M \\cite{yt8m-data};\nthe Trill \\cite{trill} model\\footnote{\\href{https://tfhub.dev/google/nonsemantic-speech-benchmark/trill/3}{tfhub.dev/google/nonsemantic-speech-benchmark/trill/3}}, a convolutional speech representation learning model trained on speech containing clips from AudioSet; and the MuLan audio encoder. VGG and Trill produce frame-wise embeddings while MuLan's embeddings are clip-wise. Since the audio encoders are trained on different datasets and tasks, FAD computed with those audio representations focus on different aspects of the audio. We hypothesize that FAD\\textsubscript{VGG} evaluates the general audio quality, FAD\\textsubscript{Trill} is more indicative of the vocal quality, and FAD\\textsubscript{Mulan} captures global musical semantics. \n\nThe contrastive model MuLan provides us a way to quantify the similarity between audio-text pairs as well as audio-audio pairs. %\nFor a given text-audio or audio-audio pair, we define the MuLan similarity as the cosine similarity between the MuLan embeddings of the two entities. For a given evaluation set of music-text pairs, we compute the average similarity between the audio generated from the text prompts of the dataset and either the text or the ground truth audio associated to the text.\nAs a reference, we also compute the average MuLan similarity of the evaluation set against the ground truth audio, as well as a ``random\" audio pairing obtained by shuffling the ground truth audio.\n\n\\subsubsection{Evaluation datasets}\n\nWe report the FAD and MuLan similarity with respect to the following three datasets consisting of text-music pairs.\n\nFirst, we re-purpose the audio tagging benchmark MagnaTagATune (MTAT)~\\cite{law2009evaluation} to evaluate the 29-second long music clips generated by our models.\nMTAT contains 25,863 music clips, 21,638 of which are associated with multiple tags from a vocabulary of 188 music tags. We only utilize these 21,638 examples for evaluation, for each of which we concatenate the music tags with into a single string that we use as the associated text prompt. During evaluation, we generate a single 29-second long audio clip for the prompt associated with each of the 21,638 examples.\n\nSecond, we use AudioSet-Music-Eval, the music portion of AudioSet~\\cite{gemmeke2017audio}. There are 1,482 music related examples in the evaluation split of AudioSet, where each 10-second clip is associated with labels from the non-trivial part of the music subtree of AudioSet ontology. For each example in this set, we use the concatenated labels as the text prompt to generate a 30-second long clip, the middle 10-second portion of which is used for evaluation. \n\nLastly, we evaluate on \\EvalSet~which consists of 5.5K 10-second clips from AudioSet paired with rater written captions. We use the rater captions as text prompts, and report the metrics with the middle 10-second excerpts of the generated audio samples.\n\n\\subsection{Evaluation results}\nIn \\cref{table:eval-FAD}, we report the FAD of our models on the three evaluation datasets, and compare them with baseline models from Riffusion\\footnote{We query Mubert API at \\href{https://github.com/MubertAI}{github.com/MubertAI} as of Dec~24, 2022 to generate 10-second audio clips given the text prompts in the evaluation datasets.} and Mubert\\footnote{We ran inference with riffusion-model-v1 provided by \\href{https://github.com/riffusion/riffusion-app}{github.com/riffusion/riffusion-app} as of Dec~24, 2022 to generate 10-second audio clips.}. \nIn \\cref{table:eval-Mulan}, we report the average audio-text and audio-audio MuLan similarity scores between the generated audio and the evaluation datasets. We also include the metrics computed for the ground truth audio, as well as the shuffled ground truth.\n\nThe evaluation metrics should be interpreted with care, since our result has potential advantages over the baselines presented. First, there is a possibility that our training data distribution is closer to the evaluation datasets compared to the baselines. Also, one may suspect that the MuLan-based metrics might be biased towards our models, since the MuLan model has been used to pseudo-label our data. The reader should thus be cautious to draw conclusions about the effectiveness of the methods used in this paper compared to those of the baselines based on these metrics. The metrics, however, are indeed representative of the performance of the trained models themselves in the AudioSet domain and provides a quantitative measure of final model performance. This ideally should hold true for the MuLan-based metrics as well, if we assume that the MuLan model, much like CLIP \\cite{radford2021learning}, has learned an un-biased, faithful representation of text and audio data.\n\n\\begin{table}[t!]\n\\vskip -0.05in\n\\caption{The FAD between the reference dataset audio and the generated audio with prompts from the reference dataset. Three audio encoders, VGG, Trill and MuLan have been used to measure FAD. A lower value indicates better proximity of quality.}\n  \\label{table:eval-FAD}\n  \\vskip 0.05in\n  \\centering\n  \\resizebox{0.95\\columnwidth}{!}{\\begin{tabular}{lrrr}\n    \\toprule\n    {\\bf Dataset/Model} & {\\bf FAD\\textsubscript{VGG}}  & {\\bf FAD\\textsubscript{Trill}} &  {\\bf FAD\\textsubscript{MuLan}}  \n    \\\\\n    \\midrule\n    \\multicolumn{4}{l}{\\bf \\EvalSet} \\\\[4pt]\n    ~~~~Riffusion~\\cite{Forsgren_Martiros_2022} &\n    13.371 & 0.763 & 0.487 \\\\\n    ~~~~Mubert~\\cite{mubert} &\n    9.620 & 0.449 & 0.366 \\\\\n    ~~~~\\MusicLM &\n    4.0~~~~ & 0.44~~ & -  \\\\[4pt]\n    ~~~~{Noise2Music Waveform} &\n    \\bf 2.134 & \\bf 0.405 & \\bf 0.110 \\\\\n    ~~~~{Noise2Music Spectrogram}    & 3.840  & 0.474  & 0.180 \\\\\n    \\midrule\n    \\multicolumn{4}{l}{\\bf AudioSet-Music-Eval} \\\\[4pt]\n    ~~~~{Noise2Music Waveform}\n    & 2.240  & 0.252  & 0.193 \\\\\n    ~~~~{Noise2Music Spectrogram}    & 3.498  & 0.323  & 0.276  \\\\\n    \\midrule\n    \\multicolumn{4}{l}{\\bf MagnaTagATune} \\\\[4pt]\n    ~~~~{Noise2Music Waveform}\n    & 3.554  & 0.352 & 0.235 \\\\\n    ~~~~{Noise2Music Spectrogram}    & 5.553  & 0.419  & 0.346  \\\\\n    \\bottomrule\n  \\end{tabular}}\n\\vskip-0.15in\n\\end{table}\n\n\\begin{table}[t!]\n\\vskip -0.05in\n\\caption{The average MuLan similarity between the generated audio and either\nthe text prompt or the ground truth audio for each evaluation set. A higher value\nindicates better semantic alignment.}\n  \\label{table:eval-Mulan}\n  \\vskip 0.05in\n  \\centering\n  \\resizebox{0.95\\columnwidth}{!}{\\begin{tabular}{lrr}\n    \\toprule\n    {\\bf Dataset/Model} &\n    \\bf audio $\\leftrightarrow$ gt-text  &\n    \\bf audio $\\leftrightarrow$ gt-audio  \\\\\n    \\midrule\n    \\multicolumn{3}{l}{\\bf \\EvalSet} \\\\[4pt]\n    ~~~~Ground Truth Audio & 0.452 & (1.000) \\\\\n    ~~~~Randomly Shuffled Audio & 0.248 &  0.278 \\\\[4pt]\n    ~~~~Riffusion~\\cite{Forsgren_Martiros_2022} &\n    0.342 & 0.312 \\\\\n    ~~~~Mubert~\\cite{mubert} &\n    0.323 & 0.280 \\\\\n    ~~~~\\MusicLM & \\bf 0.51~~ & -  \\\\[4pt]\n    ~~~~{Noise2Music Waveform}\n    & 0.478 & \\bf 0.489  \\\\\n    ~~~~{Noise2Music Spectrogram}    & 0.434  & 0.464  \\\\\n    \\midrule\n    \\multicolumn{3}{l}{\\bf AudioSet-Music-Eval} \\\\[4pt]\n    ~~~~Ground Truth Audio & 0.470 & (1.000) \\\\\n    ~~~~Randomly Shuffled Audio & 0.274 & 0.265 \\\\[4pt]\n    ~~~~{Noise2Music Waveform}\n    & \\bf 0.563  & \\bf 0.429  \\\\\n    ~~~~{Noise2Music Spectrogram}    & 0.490  & 0.389  \\\\\n    \\midrule\n    \\multicolumn{3}{l}{\\bf MagnaTagATune} \\\\[4pt]\n    ~~~~Ground Truth Audio & 0.498 & (1.000) \\\\\n    ~~~~Randomly Shuffled Audio & 0.277 & 0.315 \\\\[4pt]\n    ~~~~{Noise2Music Waveform}\n    & \\bf 0.518 & \\bf 0.479 \\\\\n    ~~~~{Noise2Music Spectrogram}    & 0.459  & 0.444  \\\\\n    \\bottomrule\n  \\end{tabular}}\n\\vskip-0.1in\n\\end{table}\n\nWe also conduct human listening tests to measure the semantic alignment, whose setup is identical to that used in \\cite{musiclm2023} with now five sources, listed in Table~\\ref{table:human-study}. The participants of the test are presented with a text caption from the MusicCaps evaluation set and 10-second clips from two different sources, then asked which clip is better described by the text of the caption on a 5-point Likert scale. We collect 3k ratings, with each source involved in 1.2k pair-wise comparisons. The the head-to-head comparisons between each pair of sources is evenly distributed.\nIn Table~\\ref{table:human-study}, we report the total number of ``wins'' each model achieved among the 1.2k comparisons it has been subject to. Our waveform model shows comparable performance to MusicLM, while being behind the ground truth audio.\n\\begin{table}[t!]\n\\caption{The number of wins in pair-wise comparisons of the human listening study. Higher indicates better semantic alignment.}\n  \\label{table:human-study}\n  \\vskip 0.05in\n  \\centering\n  \\resizebox{0.95\\columnwidth}{!}{\\begin{tabular}{ccccc}\n    \\toprule\n    \\textbf{MusicCaps} & \\textbf{Noise2Music Waveform} & \\textbf{MusicLM} & \\textbf{Mubert} & \\textbf{Riffusion}\\\\\n    \\midrule\n     959 & 718 & 692 & 254 & 308\\\\\n    \\bottomrule\n  \\end{tabular}}\n\\vskip-0.15in\n\\end{table}\n\n\\subsection{Inference parameter ablations}\n\nWe vary inference parameters of the models and observe its effects. We note that we have conducted ablations with model checkpoints that are slightly less-trained compared to the checkpoints used to produce the evaluation numbers in the previous subsection. The ablations are conducted with respect to the base parameters as listed in \\cref{t:model-inference-params}.\n\n\\begin{figure}[ht]\n\\vskip 0.05in\n\\begin{center}\n\\centerline{\\includegraphics[width=0.7\\columnwidth]{figures/ablations.png}}\n\\vskip -0.05in\n\\caption{We plot how $\\text{FAD}_\\text{VGG}$ and the MuLan similarity score vary as inference parameters are adjusted. The CFG parameters take values from [1, 2, 5, 10, 15], while ``B\"ack-heavy, ``U\"niform and ``F\"ront-heavy denoising step schedules have been applied.}\n\\label{f:ablations}\n\\end{center}\n\\vskip -0.25in\n\\end{figure}\n\nIn \\cref{f:ablations}, we depict how FAD measured with VGG and the MuLan similarity score change as the denoising step schedule and the CFG scale are varied during inference. Only one parameter is varied at a time, and all other parameters stay fixed at the baseline values.\n\nWe find an overall correlation between the FAD metric and the similarity score, except in the case of the cascader, where FAD can get worse while the similarity score improves. We also find that there is an optimal CFG scale, and too big of a CFG scale hurts the generation quality. It can also be seen that the generator CFG scale is a bigger factor than the denoising schedule of the generator, while the impact of cascader denoising schedule is extremely large.\n\n\\begin{figure}[ht]\n\\begin{center}\n\\centerline{\\includegraphics[width=0.7\\columnwidth]{figures/performance.png}}\n\\caption{Quality metrics of the generated examples plotted against computational cost parameterized by inference time.}\n\\label{f:cost}\n\\end{center}\n\\vskip -0.25in\n\\end{figure}\n\n\\subsection{Inference cost and performance}\n\nIn \\cref{f:cost}, we plot the quality metrics against the inference cost measured by the inference time. We reduce/increase the number of inference steps of the generator or the cascader/vocoder and inverse-proportionally scale the step sizes in the inference schedule. We find that the effect of increasing the inference cost of the generator is mixed while the generative quality generally improves with more cascader/vocoder inference steps.\n\n\\section{Qualitative analysis}\n\n\\textbf{Content representation:}  We present generation examples at \\href{\\website#table-2}{\\websiteDisplay\\#table-2}, to illustrate that the model is able to ground the music aspects represented in the text prompt. In particular, we find that the genre, instrument, mood, vocal traits, and era of music implied in the text is manifested in the generated music.\n\n\\textbf{Creative prompts:} While our models often struggle to produce high quality audio from out-of-distribution prompts, they are nevertheless able to generate some interesting examples. In \\href{\\website#table-3}{\\websiteDisplay\\#table-3}, we have collected examples of creative prompts for which the model was able to generate quality music.\n\n\\section{Discussion}\n\n\\textbf{Spectrogram vs. waveform approach:} The spectrogram and waveform approaches have their comparative advantages. The spectrogram models employed in this work are much cheaper to train and serve compared to the waveform models, and are more scalable in time length. This is because the sequence length of the spectrogram is much shorter than that of a low-fi waveform. In addition, the spectrogram contains high-frequency information which is missing in the low-fidelity audio. Meanwhile, the waveform model produces interpretable representations at every step of the generation process, making the model easy to debug and tune. This is partially responsible for our ability to train the waveform models with more ease.\n\n\\textbf{Future directions:} While we have demonstrated the potential of text prompt based music generation, there is much room for improvement beyond our work. Increasing model interpretability, further improving text-audio alignment, reducing training and inference cost, and scaling up the generation length of the audio are just a few directions in which our work needs to be improved. Another interesting direction is to fine-tune the models trained in this work for diverse audio tasks including music completion and modification, as was done for images by \\citet{saharia2022palette}.\n\n\\section{Broader Impact}\n\nWe believe our work has the potential to grow into a useful tool for artists and content creators that can further enrich their creative pursuits. To live up to this promise, more work is needed with musicians and other stakeholders to develop models into a meaningful co-creation tool. \n\nWe acknowledge the limitations of the proposed model. In particular, large generative models learn to imitate patterns and biases inherent in the training sets, and in our case, the model can propagate the potential biases built in the text and music corpora used to train our models. Such biases can be hard to detect as they manifest in often subtle, unpredictable ways, which are not fully captured by our current evaluation benchmarks. Demeaning or other harmful language may be generated in model outputs, due to learned associations or by chance.\n\nBeyond this, we recognize that musical genres are complex and key musical attributes are contextual and change over time. Training data reflect a limited corpus of musical samples and genres, given uneven recording and digitization of samples from global musical cultures. How music is categorized and labeled can essentialize genres; and these labels may be constructed and applied without the participation of communities. \nWhen readers examine the released generation examples in the accompanied website, we caution readers not to presume each sample can generalize to an entire musical genre or one label can capture the diversity of musical genres produced within a region (i.e. ``Latin music\" contains a broad range of cultures and styles). Moreover, musical samples may sound ``authentic\" to those outside these communities, as nuances in musical traditions need trained ears/cultural knowledge to recognize. In generating vocals, there may be possible caricatures, 11mock accents,\" parodies, or other demeaning linguistic harms (e.g., ``mock Black singing\" in a request for ``soulful vocals\" or ``mock Spanish\" in a Latin music request) that arise in text prompts requesting cultural or religious musical genres, or genres that emerged as part of the political struggles of certain communities (e.g., Black American music, Nueva canci\\'on, Chicano folk, Brazilian Tropicalismo, Sufi Qaw).\n\nAs is with any other technology, the result of our research can be misused or abused. We acknowledge the risk of potential misappropriation when the created content exactly matches examples in training data. In accordance with responsible model development practices, duplication checks are a built-in part of our current pipeline of producing and releasing examples, and will continue to be for any future work.\n\nEfforts for identifying potential safety issues and addressing them are important components for improving these generative models. Until there is a more clear understanding of the limitations and risks, we do not intend to release the model.\n\n\\section*{Acknowledgements}\nWe are grateful to Aren Jansen for building MuLan, which is an indispensable component of this project. We give thanks to Austin Tarango, Fernando Diaz, Kathy Meier-Hellstern, Molly FitzMorris, and Renee Shelby for helping us incorporate important responsible AI practices around this project. We acknowledge support from Blake Cunningham, Cara Adams, for giving us advice along the project and assisting us with the publication process. We appreciate valuable feedback and support from Alex Ku, Andrea Agostinelli, Ankur Bapna, Chen Liang, Ed Chi, Ekin Dogus Cubuk, Erica Moreira, Esteban Real, Heiga Zen, Jaehoon Lee, James Qin, Nathan Park, Stephen Kelly, Thang Luoung, Weizhe Hua, Ye Jia, Yifeng Lu, Yonghui Wu, Yu Zhang, Yuma Koizumi. Special thanks to authors of MusicLM for helpful discussions and cooperation, and especially for sharing their evaluation set and manuscript before publication.\n\n\\newpage\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{MusicFlow: Cascaded Flow Matching for Text Guided Music Generation}\n\n\\begin{document}\n\n\\twocolumn[\n\\icmltitle{MusicFlow: Cascaded Flow Matching for Text Guided Music Generation}\n\n\\icmlsetsymbol{equal}{*}\n\n\\begin{icmlauthorlist}\n\\icmlauthor{K R Prajwal}{equal,ox}\n\\icmlauthor{Bowen Shi}{equal,comp}\n\\icmlauthor{Matthew Lee}{comp}\n\\icmlauthor{Apoorv Vyas}{comp}\n\\icmlauthor{Andros Tjandra}{comp}\n\\icmlauthor{Mahi Luthra}{comp}\n\\icmlauthor{Baishan Guo}{comp}\n\\icmlauthor{Huiyu Wang}{comp}\n\\icmlauthor{Triantafyllos Afouras}{comp}\n\\icmlauthor{David Kant}{comp}\n\\icmlauthor{Wei-Ning Hsu}{comp}\n\\end{icmlauthorlist}\n\n\\icmlaffiliation{ox}{VGG, University of Oxford, UK. Work done while at Meta.}\n\\icmlaffiliation{comp}{Meta, USA}\n\n\\icmlcorrespondingauthor{K R Prajwal}{prajwal@robots.ox.ac.uk}\n\\icmlcorrespondingauthor{Bowen Shi}{bshi@meta.com}\n\n\\icmlkeywords{Machine Learning, ICML}\n\n\\vskip 0.3in\n]\n\n\\printAffiliationsAndNotice{\\icmlEqualContribution} % otherwise use the standard text.\n\n\\begin{abstract}\n\nWe introduce MusicFlow, a cascaded text-to-music generation model based on flow matching. Based on self-supervised representations to bridge between text descriptions and music audios, we construct two flow matching networks to model the conditional distribution of semantic and acoustic features. Additionally, we leverage masked prediction as the training objective, enabling the model to generalize to other tasks such as music infilling and continuation in a zero-shot manner. Experiments on MusicCaps reveal that the music generated by MusicFlow exhibits superior quality and text coherence despite being over $2\\sim5$ times smaller and requiring $5$ times fewer iterative steps. Simultaneously, the model can perform other music generation tasks and achieves competitive performance in music infilling and continuation. \nOur code and model will be publicly available. \n\\end{abstract}\n\n\\section{Introduction}\n\\label{sec:intro}\n\nAudio generation has recently received a lot of attention from the research community as well as the general public. Making sound automatically has a lot of practical applications, including voice acting, podcast making, creating foley sound effects~\\cite{Luo2023DiffFoleySV}, making background music for movies~\\cite{liu2023wavjourney}, and can greatly reduce the barrier for audio content creation.  In terms of research, audio generation poses a few challenges due to its long-term structure and complex interaction between channels (e.g., multiple events may appear at the same time), thus being a suitable testbed for generative models.\n\nModeling approaches for audio generation has rapidly progressed over the past few years due to the development of sophisticated generative methods such as autoregressive language models~\\cite{Kreuk2022AudioGenTG,Wang2023NeuralCL} and non-autoregressive approaches~\\cite{Le2023VoiceboxTM,Vyas2023AudioboxUA,liu2023audioldm}.\nA significant portion of the generative models are focused on speech and general sound, where state-of-the-art (SOTA) models~\\cite{Vyas2023AudioboxUA} are able to generate speech in diverse styles or general sound events in highly realistic manner. Compared to these two common modalities, music generation is a particularly challenging problem as it requires modeling long-term temporal structures~\\cite{musiclm} and full frequency spectrum~\\cite{muller2015fundamentals}. Compared to typical sound events (e.g., dog barking), it contains harmonies and melodies from different instruments. Music pieces often consist of multiple tracks, which can be intricately woven together and may involve significant interference.\n\nWith the improvement of audio tokenizers~\\cite{zeghidour2021soundstream,defossez2022highfi} and generative models, the quality of generated music has been greatly improved in recent works~\\cite{musiclm,musicgen}. However, many prior works are built upon language models~\\cite{musiclm,musicgen,yang2023uniaudio}, which requires a computationally expensive auto-regressive inference procedure with number of forward passes proportional to the sequence length. This is worsened because many such models are based on a hierarchical set of units (e.g., Encodec tokens~\\cite{musicgen}), which brings another factor up to the computation. Despite the usage of non-autoregressive models such as diffusion models~\\cite{audio-ldm2,noise2music,forsgren2022riffusion,schneider2023mousai}, these approaches require hundreds of denoising steps during inference to achieve high performance. On the other hand, most of the existing models perform generation in a single stage, which models the audio waveform~\\cite{noise2music} or its low-level representation such as VAE features~\\cite{audio-ldm2} conditioned on text description directly. As music audios contains rich structural information and its text description can be very detailed (e.g., \\emph{This is a live recording of a keyboardist playing a twelve bar blues progression on an electric keyboard. The player\nadds embellishments between chord changes and the piece sounds groovy, bluesy and soulful.}), such approaches commonly fail to capture the intriguing dependency between text description and music pieces. Finally, most existing work focuses on text-to-music (TTM) generation, while lacking the ability to perform other practically useful generative tasks such as music infilling.\n\nIn this paper, we present MusicFlow, a cascaded text-to-music generation model based on flow matching. Our model is composed of two flow-matching networks, which transform text description into a sequence of semantic features and semantics into decodable acoustic features in non-autoregressive fashion. The flow matching objective equips the model with high efficiency in both training and inference, outperforming prior works with smaller model size and faster inference speed. Furthermore, by training with a masked prediction objective, MusicFlow is able to perform multiple music generation tasks, including TTM, music continuation and music infilling in a unified fashion.\n\n\\section{Related Work}\n\\label{sec:related}\n\nEarly works on music generation are mostly on constrained scenarios, such as generating audios for a specific style (e.g., Jazz~\\cite{Hung2019ImprovingAJ}) or a specific instrument (e.g., piano~\\cite{Hawthorne2018EnablingFP}). More recent works shift the focus to generating music from free-form natural language descriptions. Typically, the language description is encoded by a pre-trained text encoder, which is then used for conditioning the model. One big class of the generation backbone falls into the category of language models~\\cite{musiclm,musicgen}. In this type of model, an audio is quantized into discrete units through an auto-encoder (e.g., SoundStorm~\\cite{zeghidour2021soundstream}, Encodec~\\cite{defossez2022highfi}). The language model is built to model the distribution of these units. During inference, the units sampled from the language model is decoded back into raw waveforms with the decoder directly without an explicit vocoder. The units are sampled either autoregressively~\\cite{musicgen,musiclm,yang2023uniaudio} or in conjunction with non-autoregressive unit decoding~\\cite{ziv2024masked}.  Diffusion-based music generation is typically built on top of the audio spectrogram. AudioLDM2~\\cite{audio-ldm2} employs a variational auto-encoder to compress the spectrogram, where a DDIM~\\cite{song2020denoising} model is trained with the compressed features. During inference, the generation is first decoded with the VAE decoder and transformed to waveform with a vocoder.  Similar approaches include Riffusion~\\cite{forsgren2022riffusion}, which directly fine-tunes a stable diffusion model with spectrograms; MeLoDy~\\cite{melody} which proposes a LM-guided Diffusion with a focus on fast sampling speed; and Noise2Music~\\cite{noise2music}, which also builds a diffusion-based vocoder; and StableAudio~\\cite{evans2024fast} which takes a latent diffusion approach, again with a focus on fast inference. \n\nMost of the existing methods directly learns the music distribution conditioned on text , which models the low-level audio features directly. In this work, our cascaded model is bridged by semantic features, which are learned separately with a self-supervised model. A similar approach to ours is MusicLM~\\cite{musiclm}, which learns two language models generating semantic and acoustic units respectively. \nHowever, our model relies on flow matching, which offers improved efficiency. Its non-autoregressive nature also enables the model to better leverage context and generalize to other tasks.\n\n\\section{Method}\n\\label{sec:method}\n\n\\subsection{Background: Flow matching}\nIntroduced in~\\cite{Lipman2022FlowMF}, flow matching is a method addressing continuous transformation of probability densities. Specifically, it studies flow, a time-dependent diffeomorphic mapping $\\phi_t: [0,1]\\times\\mathbb{R}^d\\rightarrow\\mathbb{R}^d$, defined via the ordinary differential equation (ODE):\n\\begin{equation}\\label{eq:ode}\n    \\frac{d}{dt}\\phi_t(x)=v_t(\\phi_t(x))\n\\end{equation}\n$v_t:[0,1]\\times\\mathbb{R}^d\\rightarrow\\mathbb{R}^d$, namely a vector field, is parameterized by a neural network $\\theta$ and learned by minimizing the flow matching objective: $L_{FM}=\\mathbb{E}_{t,p_t(x)}||v_t(x;\\theta)-u_t(x)||^2$, where $p_t(x)$ is a probability density path and $u_t(x)$ is the corresponding vector field. As both $p_t(x)$ and $u_t(x)$ are generally unknown, \\citet{Lipman2022FlowMF} proposes minimizing the following conditional flow matching objective, which is equivalent to minimizing $L_{\\text{FM}}$:\n\\begin{equation}\\label{eq:cfm_loss}\nL_{\\text{CFM}}=\\mathbb{E}_{t,p(x|x_1),q(x_1)}||v_t(x;\\theta)-u_t(x|x_1)||^2\n\\end{equation}\n Considering Gaussian distributions for $p_t(x|x_1)=\\mathcal{N}(x|\\mu_t(x_1),\\sigma_t(x_1)^2I)$, the target vector field for Equation~\\ref{eq:cfm_loss} can be solved in closed form: $u_t(x|x_1)=\\frac{\\sigma^\\prime_t(x_1)}{\\sigma_t(x_1)}(x-\\mu_t(x_1))+\\mu^\\prime_t(x_1)$. Several diffusion models~\\cite{Dickstein2015deep,Ho2020Denoising,song2021scorebased} can be described under the same framework with specific conditional probability paths of $\\sigma_t(x_1)$ and $\\mu_t(x_1)$. Specifically, \\citet{Lipman2022FlowMF} considers a conditional probability path with Gaussian mean and standard deviation changing linearly in time with $\\mu_t(x)=tx$ and $\\sigma_t(x)=1-(1-\\sigma_{min})t$, which produces an optimal transport displacement mapping between conditional distributions. Due to its efficiency in both training and inference~\\cite{Lipman2022FlowMF,Le2023VoiceboxTM}, we always stick to this conditional probability path as the default setting throughout the paper. \n\n\\subsection{Problem Formulation}\\label{sec:problem_formulation}\nWe now describe the music generation task and the general methodology based on flow matching that we employ.\nGiven a dataset consisting of audio-text pairs $(x, w)$, where $x\\in\\mathbb{R}^{T\\times C}$ ($T$: number of timesteps, $C$: number of channels) is the music audio and $w=\\{w_1,w_2,..,w_n\\}$ ($w$: words) \nis the corresponding textual description represented as a sequence of words, the goal is to build a text-conditioned music generation model $p(x|w)$. In addition to generating music from scratch, we further consider two practical tasks: music continuation \n$p(x_{t_1:T}|x_{1:t_1}, w)$\nand music infilling $p(x_{t_1:t_2}|x_{1:t_1}, w, x_{t_2:T})$, with $t_1$, $t_2\\in[0,T]$.\nIn order to allow the model to perform all the text-guided music generation, we formulate our approach as an in-context learning task following~\\cite{Le2023VoiceboxTM}. Specifically, given a binary temporal mask $m$ for a music track $x$, we train a conditional flow matching model predicting the vector field in the masked regions of the music track $x_{m} = x \\odot m$ while conditioning on the\n unmasked regions of the music track $x_{ctx} = x \\odot (1 - m)$ and the text caption $w$ about the music piece. Formally, we train with the following flow matching loss: $L_{\\text{CFM}}=\\mathbb{E}_{t,m,p(x|x_1),q(x_1,w)}||m\\odot(v_t(x,x_{ctx},w;\\theta)-u_t(x|x_{ctx},w))||^2$.\n \n In addition to increasing the model capacity, such masked prediction objective also benefits generative modeling in general, as is shown in~\\cite{jen1,Le2023VoiceboxTM}. Within this framework, the three tasks of TTM, music continuation and music infilling can be conceptualized as setting specific mask values for $p(x_{m}|(1-m)\\odot x,w)$, where $m$ is set to be $\\mathbf{1}_{1:T}$, $[\\mathbf{0}_{1:t_1},\\mathbf{1}_{t_1:T}]$ and $[\\mathbf{0}_{1:t_1},\\mathbf{1}_{t_1:t_2},\\mathbf{0}_{t_2:T}]$ respectively.\n\n\\subsection{A Cascaded Flow-matching Approach}\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=0.75\\linewidth]{figure/model_fig.png}\n    \\caption{MusicFlow Diagram. Note the acoustic encoder, acoustic decoder and semantic encoder are pre-trained and frozen during generative model training. For text-to-music generation (i.e., 100\\% masking), both acoustic and semantic encoder are discarded in inference.\n    }\n    \\label{fig:main_fig}\n\\end{figure*}\nTraining flow matching to directly generate music conditioned on text captions is difficult~\\cite{audio-ldm2} given the vast number of potential music tracks corresponding to a single text caption. As the text caption lacks the fine-grained information to adequately describe a music track, we propose to condition on a latent music representation that describes the music at the frame level.\n\nMusicFlow is thus divided into two stages: semantic modeling and acoustic modeling. The first stage outputs latent representations $h = (h_1, h_2,....h_M) \\in \\mathcal{R}^{M\\times D}$ conditioned on a text caption $w$. In the second stage, we condition on the latent representations from the first-stage model, and a text caption $w$ to output low-level acoustic features of $N$ frames, $x = (x_1, x_2, ...., x_N) \\in \\mathcal{R}^{N\\times C}$. Note $h$ and $x$ are monotonically aligned. Both stages are inherently stochastic, meaning there are multiple potential $(h,x)$ pairs for a given caption $w$. Therefore, we model these two stages separately with flow matching. In both stages, we predict masked vector fields as discussed before and provide detailed descriptions on the two stages below. \n\n\\subsection{Stage 1: Music Semantic Flow Matching from Text}\n\nOur first-stage model consists of generating the semantics of the music piece conditioned on the text description. Here the semantics refer to high-level musical information instead of fine-grained details such as the general audio quality, which are inferred from the text description. For music, the semantics can refer to the melody and rhythm or harmony in a piano piece, analogous to the linguistic content in speech. \n\n\\textbf{Semantic latent representation}\nOne natural way of representing music is through music transcription.\nTranscripts in music typically refer to some notation system (e.g., music scores) that indicates the pitches, rhythms, or chords of a musical piece.\nA notable advantage of music transcript is its interpretability as it is human-readable and thus poses easy alignment with humans. However, for large-scale audio datasets, the associated music transcripts are usually not readily available, while manual annotation involves a non-trivial amount of labeling efforts. Automatic music transcription is a challenging task~\\cite{Benetos2019Automatic} and the existing approaches~\\cite{Bittner2022ALI,Hawthorne2021SequencetoSequencePT,hsu2021vocano,Su2019tent,Hawthorne2018EnablingFP} are heavily restricted to a single-instrument setting (e.g.\npiano, solo vocals, etc.). \n\nTo  address the challenge of acquiring music transcriptions, we adopt HuBERT~\\cite{Hsu2021HuBERTSS}, a popular self-supervised speech representation learning framework, to obtain frame-level semantic features, which can be regarded as a form of pseudo transcription. In essence, a HuBERT model consists of masked prediction of hidden units from the raw audio, which are inferred initially from MFCC and iteratively refined with layerwise features. For speech, HuBERT units have shown to correlate well with phonemes~\\cite{Hsu2021HuBERTSS} and its intermediate features entails rich semantic information~\\cite{Pasad2023comparative}. In music understanding tasks, HuBERT has been successfully applied in source separation~\\cite{Pasini2023SelfSupervisedMS}, shedding light on its potential for capturing musical characteristics. As the original HuBERT model is pre-trained with speech only, we re-train HuBERT using music data following the original recipe. Training details are given in Section~\\ref{sec:exp}.\n\n\\textbf{Semantic flow matching} Given a HuBERT model $\\mathcal{H}$, one can extract the semantic features from its $l$th layer $l$: $h = \\mathcal{H}(x) \\in \\mathcal{R}^{M\\times C_h}$, where $C_h$ is the HuBERT feature dimension. The layer index $l$ is tuned in practice. A text-conditioned semantic flow-matching model $p(h|w)$ can be trained given text-feature pairs $(h, w)$. As described in Section~\\ref{sec:problem_formulation}, we adopt the masked prediction objective by conditioning on the context $h_{ctx}=m\\odot h$, where $m$ is a span mask of length $M$. More formally, we adopt the following training objective for the semantic modeling stage: $L_{\\text{H-CFM}}=\\mathbb{E}_{t,m,p(h|h_1),q(h_1,w)}||m\\odot(v_t(h,h_{ctx},w;\\theta)-u_t(h|h_{ctx},w))||^2$.\nCross-attention layers are integrated into the backbone model, enabling it to attend to the text description $w$, akin to~\\cite{rombach2021highresolution}. \n\nAs an alternative to modeling the distribution of dense features, one can quantize the layerwise features $h$ into units $u$ and model the unit distribution $p(u|w)$ instead. For this case, a straightforward method is to build an autoregressive language model, which factorizes $p(u|w)=\\displaystyle\\prod_{n=1}^{M}p(u_{n}|u_{1:{n-1}},w)$. Using a semantic LM has been explored in~\\cite{musiclm} in hierarchical LMs for music generation. We also noticed its effectiveness when combined with flow matching, as will be shown in Section~\\ref{sec:exp}. However, this hybrid model is unsuitable for music infilling task due to its left-to-right nature.\n\n\\subsection{Stage 2: Music Acoustic Flow Matching from text and semantics}\n\\textbf{Acoustic latent representation} The second-stage model aims to infer the low-level acoustic information (e.g., volume, recording quality) implied by the semantic tokens. Directly predicting raw waveforms ensures the completeness of information while imposes the challenge of modeling the long sequences. To balance between quality and sequence length, we use Encodec~\\cite{defossez2022highfi} to map raw waveforms into dense feature sequences. In a nutshell, Encodec~\\cite{defossez2022highfi}, an auto-encoder based on residual vector quantization, comprises of an encoder $\\mathcal{E}$ and decoder $\\mathcal{D}$. During training, we map raw waveforms into acoustic features with the encoder $\\mathcal{E}$: $e=\\mathcal{E}(x)\\in\\mathbb{R}^{N\\times C_e}$, where $C_e$ is the feature dimension of encodec.\n\n\\textbf{Acoustic flow matching} The second-stage flow matching aims to model the following conditional distribution: $p(e|h, w)$. Similar to semantic flow matching, we apply masked prediction and the corresponding training objective is formulated as: $L_{\\text{E-CFM}}=\\mathbb{E}_{t,m,p(e|h_1, e_1),q(e_1, h_1,w)}||m\\odot(v_t(e,e_{ctx},h_1,w;\\theta)-u_t(e|e_1,h_1,e_{ctx},w))||^2$. \nAs the semantic and acoustic features are aligned ($N/M\\approx{sr}_\\mathcal{E}/{sr}_\\mathcal{H}$, $sr$: sample rate), we simply linearly interpolate the HuBERT feature sequence $h$ to length $N$ before feeding it into encoding. \n\nNote though Encodec includes multiple different codebooks to quantize the latent features, we directly model the dense feature sequence from the encoder $\\mathcal{E}$ without any quantization. This avoids the length increase brought by using multiple codebooks, where the total number of discrete tokens is $K-1$ times more than the dense feature length. Thus, it eliminates the necessity of carefully designing interleaving pattern of discrete tokens to account for dependencies between multiple codebooks~\\cite{musicgen,Wang2023NeuralCL}.\n\n\\subsection{Classifier-free guidance}\n\nDuring inference, we sequentially sample the HuBERT features $\\hat{h}$ and encodec features $\\hat{e}$ using the estimated vector field $v_t(h,h_{ctx},w;\\theta_h)$ and $v_t(e,e_{ctx},\\hat{h},w;\\theta_e)$ following the ODE equation~\\ref{eq:ode}. \nThe acoustic features are decoded into waveforms via the decoder $\\mathcal{D}$ of the Encodec.\n\nAs is common in diffusion models, classifier-free guidance is a widely used technique to balance sample diversity and text coherence. Thus we also adopt it in our cascaded generation framework. For flow matching, using classifier-free guidance~\\cite{Zheng2023GuidedFF} consists of computing a linear combination between conditional and unconditional vector field: $\\tilde{v}^{H}_t(h, w, h^{ctx};\\theta_{h}) = (1 + \\alpha_h)v^{H}_t(h, w, h^{ctx};\\theta_h) - \\alpha_h v^{H,uncond}_t(h;\\theta_h)$ and $\\tilde{v}^{E}_t(e,e^{ctx},\\hat{h},w;\\theta_{e}) = (1 + \\alpha_e)v^{e}_t(e,e^{ctx},\\hat{h},w;\\theta_{e}) - \\alpha_e v^{e,uncond}_t(e;\\theta_e)$. \n\nIn order to model the unconditional vector field $v_t^{H,uncond}$ and $v_t^{E,uncond}$ with $v_t^H$ and $v_t^E$ , we randomly drop the conditions (e.g., text, the contextual features) in both flow models with probability $p^{H}$ and $p^{E}$ in training, whose values are also tuned.\n\n\\section{Experiments}\n\\label{sec:exp}\n\n\\begin{table*}\n\\caption{Comparisons between MusicFlow with previous works in text-to-music generation on the MusicCaps dataset. }\n\\label{tab:main-result}\n\\begin{center}\n\\begin{sc}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{lcccccc}\n\\toprule\nModel & \\# params & FAD($\\downarrow$) & FD($\\downarrow$) & KL-div($\\downarrow$) & ISc.($\\uparrow$) &  CLAP-text($\\uparrow$)\\\\\n\\midrule\nMusicLM~\\cite{musiclm} & 860M & 4.00 & - & - & -  & - \\\\\nMusicGen~\\cite{musicgen} & 1.5B & 3.40 & 24.1 & 1.23 & 2.29  & 0.37\\\\\nUniAudio~\\cite{yang2023uniaudio} & 1B & 3.65 & - & 1.90 & - & -\\\\\nAudioLDM-2~\\cite{audio-ldm2} & 746M & 3.13 & 18.8 & \\textbf{1.20} & 2.77 & 0.43\\\\\nNoise2Music~\\cite{noise2music} & 1.3B & 2.10 & - & - & - & -\\\\\nJEN-1~\\cite{jen1} & 746M     & \\textbf{2.00} & - & 1.29 & -  & -       \\\\\n\\midrule\nMusicFlow (unidirectional LM + FM)  & 546M & 2.69 & \\textbf{13.2} & 1.23 & 2.69 & 0.52 \\\\\nMusicFlow (bidirectional  FM + FM) & 330M & 2.82 & 14.2 & 1.23 & \\textbf{2.78} & \\textbf{0.56} \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{sc}\n\\end{center}\n\\end{table*}\n\n\\subsection{Experimental Setup}\n\\textbf{Data}\nWe use 20K hours of proprietary music data ($\\sim$400K tracks) to train our model. We follow the original recipe in~\\cite{Hsu2021HuBERTSS} to train the music HuBERT model with music data. For data preprocessing, we filter out all the vocal tracks and resample all the data to 32kHz and perform channel-wise averaging to downmix all multi-channel music into mono. Only text descriptions are retained for training, while the other metadata such as genre, BPM and music tags are discarded.\nWe evaluate our model on MusicCaps~\\cite{musiclm}, which incorporates 5.5K\n10s-long audio samples annotated by expert musicians in total. For subjective evaluation, we use the 1K genre-balanced subset following~\\cite{musiclm}.\n\n\\textbf{Implementation details} We follow~\\cite{Le2023VoiceboxTM} for backbone architectures in both stages, which are Transformers~\\cite{Vaswani2017AttentionIA} with convolutional position\nembeddings~\\cite{Baevski2020wav2vec2A}, symmetric bi-directional ALiBi self-attention bias~\\cite{Press2021TrainST} and UNet-style skip connections. Specifically, the transformers of the first and second stage include 8 and 24 layers of 12 attention heads with 768/3072 embedding/feed-forward network (FFN) dimension, leading to 84M and 246M\nparameters (see Section~\\ref{sec:ablation-model-size} for ablation on model size). \nThe models are trained with an effective batch size of 480K frames, for 300K/600K updates in two stages respectively. For efficiency, audios\nare randomly chunked to 10s during training. For masking, we adopt the span masking strategy and the masking ratio is randomly chosen between $70-100\\%$. Condition dropping probabilities (i.e., $p^H$ and $p^E$) are 0.3 for both stages. \nWe use the Adam optimizer~\\cite{kingma2014adam} with learning rate 2e-4, linearly warmed up for 4k steps\nand decayed over the rest of training. \n\n\\textbf{Objective evaluation} We evaluate the model using the standard Frechet Audio Distance (FAD)~\\citep{Kilgour2019FrchetAD}, Frechet Distance (FD) and KL divergence (KLD) based on the pre-trained audio event tagger PANN~\\citep{Kong2019PANNsLP}, and Inception score (ISc)~\\citep{Salimans2016ImprovedTF}, which are adapted from sound generation and has been widely used in prior works for text-to-music generation~\\cite{musiclm,noise2music,musicgen,jen1}.\nSpecifically, FAD and FD measure distribution-level similarity between reference samples and generated samples. KLD is an instance level metric computing the divergence of the acoustic event posterior between the reference and the generated sample for a given description. \nThe metrics are calculated using the \\texttt{audioldm\\_eval} toolkit.\\footnote{\\url{https://github.com/haoheliu/audioldm_eval}}. \nTo measure how well the generated music matches the text description, we use CLAP~\\footnote{We use the \\url{music_speech_epoch_15_esc_89.25.pt} checkpoint, trained on both speech and music data.} similarity, defined as the cosine similarity between audio and text embeddings.\n\n\\bsc{This paragraph is mostly copied from AudioBox, and needs some rewording.}\n\n\\textbf{Subjective evaluation} In addition to the objective metrics mentioned above, we further conduct subjective evaluation with human annotators. The study consists of multiple pairwise studies following the evaluation protocol of~\\citet{musiclm}. Specifically, each human annotator is presented with pairs of audio clips generated by two different systems and is required to give their preference based on how well the generated music captures the elements in the text description. \n\n\\subsection{Main Results}\\label{sec:main-result}\n\nTable~\\ref{tab:main-result} compares our model to prior works in text-to-music generation on MusicCaps in terms of objective metrics. \nGiven the variation in models used for evaluation in prior works, we primarily rely on FAD, which is computed using the vggish feature~\\cite{Kilgour2019FrchetAD} and serves as a unified benchmark across different studies. Specifically, when evaluating MusicGen, we opt for its medium version due to its overall superior performance compared to other variants~\\cite{musicgen}. For MusicGen and AudioLDM2, we use the public model checkpoints in order to get FD, ISc and CLAP similarity since these metrics were not reported in the paper. \n\nIn MusicFlow, we additionally present the results of a model with the first stage functioning as a language model, predicting HuBERT units as detailed in Section~\\ref{sec:method}. The language model includes 24 transformer layers, 16 attention heads, and a hidden dimension of 1024, leading to $\\sim300M$ parameters in total.\n\n\\begin{table*}\n\\label{tab:infil-cont}\n\\caption{Performance of MusicFlow on various music generation tasks on MusicCaps dataset. We compare with AudioLDM-2~\\cite{audio-ldm2} for text-to-music and AudioLDM for music infilling and continuation.}\n\\vspace{-0.2in}\n\\begin{center}\n\\begin{sc}\n\\resizebox{\\textwidth}{!}{%\n\\begin{tabular}{lccccccc}\n\\toprule\nTask // Model  & FAD($\\downarrow$) & FD($\\downarrow$) & KL-div($\\downarrow$) & ISc.($\\uparrow$) & CLAP-sim($\\uparrow$) & CLAP-audio($\\uparrow$) & CLAP-text($\\uparrow$) \\\\\n\\midrule\n\n\\rowcolor{verylightgray}\n\\multicolumn{8}{c}{\\textbf{Text-to-music (100\\%)}} \\\\\nAudioLDM-2~\\cite{audio-ldm2} & 3.13 & 18.8 & \\textbf{1.20} & 2.77 & - & 0.44 & 0.43 \\\\\nMusicFlow & \\textbf{2.82} & \\textbf{14.2} & 1.23 & \\textbf{2.78} & - & \\textbf{0.48} & \\textbf{0.56} \\\\\n\\midrule\n\\rowcolor{verylightgray}\n\\multicolumn{8}{c}{\\textbf{Continuation (last 70\\%)}} \\\\\nAudioLDM~\\cite{liu2023audioldm} & 2.08 & 25.08 & 0.66 & 2.80 & 0.61 & 0.61 & 0.53 \\\\\nMusicFlow & \\textbf{1.63} & \\textbf{6.50} & \\textbf{0.49} & \\textbf{3.37} & \\textbf{0.88} & \\textbf{0.77} & \\textbf{0.56} \\\\\n\\midrule\n\\rowcolor{verylightgray}\n\\multicolumn{8}{c}{\\textbf{Infilling (middle 70\\%)}} \\\\\nAudioLDM~\\cite{liu2023audioldm} & 2.09 & 45.93 & 0.76 & 2.39 & 0.59 & 0.61 & 0.54 \\\\\nMusicFlow & \\textbf{1.71} & \\textbf{6.5} & \\textbf{0.38} & \\textbf{3.18 }& \\textbf{0.89} & \\textbf{0.79} & \\textbf{0.57} \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{sc}\n\\end{center}\n\\end{table*}\n\nIn comparison to all prior works, our model exhibits a significant reduction in size, with parameter reduction ranging from 50\\% to 80\\%, while remaining competitive in terms of generation quality. Compared with a standard diffusion model - AudioLDM-2, MusicFlow achieves a 10\\% lower FAD ($3.13\\rightarrow2.82$) with approximately 50\\% fewer parameters. Similarly, compared to the language-model-based MusicGen, our approach shows a 20\\% improvement in FAD ($3.40\\rightarrow2.82$) while using only 20\\% of the parameters. These results highlight the efficiency of our approach.\n\nIt's noteworthy that MusicLM~\\cite{musiclm} shares similarities with ours, incorporating semantic and acoustic modeling stages based on language models. However, we surpass this approach by roughly 30\\% in FAD with less than 65\\% of the parameters. Additionally, in contrast to the current state-of-the-art model on MusicCaps, Jen-1~\\cite{jen1}, our results shows a mixture of results. While falling behind in FAD, we outperform it in KL divergence with only half of the parameters.\n\n\\textbf{LM vs. FM for first stage} In addition to our main approach, we investigate the integration of a language model for first-stage modeling. Both approaches share the second-stage model. According to the last two rows of table~\\ref{tab:main-result}, using a first-stage LM yields marginally superior results compared to using a flow matching model. This implies that semantic features in music audios possess discrete structures, which can be well captured by an auto-regressive language model. Nonetheless, for the sake of model efficiency and task generalization, we adhere to using the flow matching cascade moving forward.   \n\n\\textbf{Subjective evaluation} Figure~\\ref{fig:human-eval} shows the pairwise comparison between our model and prior works. In particular, we compare MusicFlow to AudioLDM2 and MusicGen, which are the only two publicly available models in Table~\\ref{tab:main-result}.\nFor our model, we use the \\texttt{bidirectional FM+FM} configuration in Table~\\ref{tab:main-result}.\nOur model surpasses both AudioLDM2 and MusicGen.\nThis observation aligns with the objective metrics presented in Table~\\ref{tab:main-result}. However, it's worth noting that there is still a gap between our model and the ground-truth.\n\n\\begin{figure*}\n    \\centering\n    \\label{fig:human-eval}\n    \\includegraphics[width=0.9\\linewidth]{figure/human_eval.jpg}\n    \\vspace{-0.1in}\n    \\caption{Pairwise comparison between MusicFlow, AudioLDM2, MusicGen and ground-truth}\n\\end{figure*}\n\n\\textbf{Inference Efficiency} In Table~\\ref{tab:main-result}, we only lists the model size, which is one aspect of model efficiency.  In Figure~\\ref{fig:fad-nfe}, we plot how FAD changes when we vary the number of function evaluations (NFE) during inference. For flow matching and AudioLDM2, this is achieved by adjusting the number of iterative steps in the ODE solver\\footnote{We use the \\texttt{midpoint} solver for this analysis} and DDIM steps, respectively. Since MusicFlow involves two flow matching models, we simply aggregate the NFE of the two modules as the final NFE we plot. For comparison, we further show the MusicGen, which runs a fixed number of auto-regressive steps. As shown in Figure~\\ref{fig:fad-nfe}, MusicFlow outperforms MusicGen (FAD: 3.13 vs. 3.40) by using $20\\%$ of inference steps. Running with longer steps further improves the performance. The final model takes only $50\\%$ the network forward passes of MusicGen. \nAudioLDM2 exhibits a similar trend to ours, although its generation quality consistently lags behind with the same number of inference steps.\n\n\\subsection{Infilling and Continuation}\nOne advantage of MusicFlow is its ability to handle multiple audio-conditioned generative tasks, such as infilling and continuation, with a single model. These tasks have also been explored in~\\cite{jen1}, albeit without reported quantitative metrics.  Due to lack of baselines, we compare the model performance to the our own text-to-music model, as detailed in Table~\\ref{tab:main-result}. For the infilling task, we infill the middle 70\\% of the audio segment. For the continuation task, given the beginning 30\\% of the audio clip, the model generates the remaining 70\\%.  \n\nAs is shown in Table~\\ref{tab:infil-cont}, our model effectively uses the context to enhance audio generation. In both settings, using a 3s audio context enables a nearly 50\\% reduction in FAD. The text-to-audio similarity is slightly increased in infilling ($0.44\\rightarrow0.45$). \nWe hypothesize this may be because the CLAP model struggles to discern fine-grained details in the text description. Hence, we conduct a subjective study to measure text faithfulness. \nThe MOS scores of text-to-music, conditnuation and infilling are respectively $3.34\\pm 0.18$, $3.47\\pm 0.18$, $3.42\\pm 0.19$ with 95\\% confidence interval. This confirms an improvement in text faithfulness through context utilization.\n\nAdditionally, among other metrics, we compute the CLAP-Audio score, defined as the cosine similarity between the embeddings of the generated and ground-truth audios. Compared to text-only generation, the generated audio achieved higher scores, suggesting better acoustic matching through context conditioning.  \nFinally, we measure the CLAP similarity between the generated segment and the original context (CLAP-SIM). Both settings achieve scores close to 1, implying coherence between the generation and context.\n\n\\subsection{Ablation Study}\nBelow we analyze the impact of different design choices in MusicFlow, particularly focusing on the necessity of a two-stage cascade and how model scales differently in each stage.\n\n\\begin{figure}[htp]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figure/fad_nfe.jpg}\n    \\vspace{-0.3in}\n    \\caption{Comparison between MusicFlow and prior works in FAD-NFE in terms of inference efficiency.}\n    \\label{fig:fad-nfe}\n\\end{figure}\n\n\\subsubsection{Single-stage vs. Two-stage Model} \nWe compare MusicFlow to a simple flow matching baseline of directly generating music based on text descriptions without the intermediate HuBERT features in Table~\\ref{tab:2vs1}. Including a stage of HuBERT prediction consistently improves the performance across various metrics regardless of model size. HuBERT-based flow matching brings a $\\sim 30\\%$ relative improvement in terms of FAD. \nNote while we increased the size of the single-stage model to 431M, it did not yield additional gains, despite having more parameters.\n\n\\begin{table}[h]\n\\caption{Comparison between single-stage and multi-stage flow matching in different model sizes}\n\\label{tab:2vs1}\n\\begin{center}\n\\begin{small}\n\\begin{sc}\n\\begin{tabular}{lcccc}\n\\toprule\nModel & FAD & FD & KL-div & ISc.  \\\\\n\\midrule\nSingle-stage (123M) & 4.58 & 25.5 & 1.62  & 2.52 \\\\\nSingle-stage (246M) & 4.52 & 22.9 & 1.57  & 2.66 \\\\\nSingle-stage (431M) & 5.11 & 27.5 &  1.64 & 2.68 \\\\\nTwo-stage (84M+123M) & 3.37 & 20.6 & 1.50  & 2.59 \\\\\nTwo-stage (84M+246M) & \\textbf{2.82} & \\textbf{14.2} & \\textbf{1.23}& \\textbf{2.78}\\\\\n\\bottomrule\n\\end{tabular}\n\\end{sc}\n\\end{small}\n\\end{center}\n\\end{table}\n\n\\subsubsection{Effect of model size}\n\\label{sec:ablation-model-size}\nEmpirically, we observed the performance of our models is heavily influenced by the model size. In this analysis, we delve into the impact of model size in each stage.\n\n\\noindent\\textbf{Second-stage: Text + HuBERT to Music.} We first examine how the size of the second-stage model, specifically the Text + HuBERT features $\\rightarrow$ music, affects the overall performance. We keep the best first-stage model and scale the second-stage model by altering the number of transformer layers and the hidden dimension of each layer (see Table~\\ref{tab:second-scaling}). The performance improves as we increase the number of layers until reaching 24 layers. \nBeyond this point, increasing the number of layers or feature dimensions results in degradation, suggesting a potential overfitting issue of the model.\n\n\\noindent\\textbf{First-stage: Text to HuBERT.} We fix the configuration for our second-stage model based on the above findings and vary only the first-stage configuration (see Table~\\ref{tab:first-scaling}).\nUnlike the second-stage, where the best model is with 24 transformer layers, our best first-stage model for Text $\\rightarrow$ HuBERT feature prediction is notably smaller with an optimal configuration of only 8 layers. According to Table~\\ref{tab:first-scaling}, smaller models typically perform equally well or even better than their larger counterparts in the first-stage model. We hypothesize that predicting HuBERT features is simpler than predicting the low-level Encodec features, particularly for shorter music pieces with standard music structures, as the former consists of learning only the coarse-grained semantics. \nConsequently, a larger variant is more susceptible to overfitting compared to the second-stage scenario.\n\n\\begin{table}[htp]\n\\caption{Effect of the \\emph{second-stage} model size  on performance. In each row we specify the number of layers, the hidden dimension of the transformer and the total number of trainable parameters.}\n\\label{tab:second-scaling}\n\\begin{center}\n\\begin{small}\n\\begin{sc}\n\\begin{tabular}{lcccc}\n\\toprule\nModel configuration & FAD & FD & KL-div & ISc. \\\\\n\\midrule\n12L, 768d (123M) & 3.37 & 20.6 & 1.50  & 2.59 \\\\\n18L, 768d (123M) & 3.22 & 18.2  & 1.42  & 2.62 \\\\\n24L, 768d (246M) & \\textbf{2.82} & \\textbf{14.2} & \\textbf{1.23}& \\textbf{2.78} \\\\\n32L, 768d, (323M) & 3.12 & 17.9 & 1.42 & 2.64  \\\\\n\\midrule\n12L, 1024d, (217M) & 3.56 & 18.7 & 1.43 & 2.67\\\\\n18L, 1024d, (324M) & 3.26 & 18.4 & 1.42  & 2.67  \\\\\n24L, 1024d, (441M) & 3.40 & 17.8 & 1.43 & 2.71 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{sc}\n\\end{small}\n\\end{center}\n\\end{table}\n\n\\begin{table}[htp]\n\\caption{Effect of the \\emph{first-stage} model size  on performance. In each row we specify the number of layers, the hidden dimension of the transformer and the total number of trainable parameters.}\n\\label{tab:first-scaling}\n\\begin{center}\n\\begin{small}\n\\begin{sc}\n\\begin{tabular}{lcccc}\n\\toprule\nModel configuration & FAD & FD & KL-div & ISc. \\\\\n\\midrule\n12L, 1024d (217M) & 3.18 & 18.2 & 1.44  & 2.74 \\\\\n12L, 768d (123M) & 3.09 & 17.1 & 1.42 & 2.73 \\\\\n8L, 768d (84M) & \\textbf{2.82} & \\textbf{14.2} & \\textbf{1.23}& \\textbf{2.78} \\\\\n6L, 768d, (64M) & 3.30 & 18.1 & 1.47  & 2.69  \\\\\n8L, 512d, (38M) & 3.20 & 17.6 & 1.41 & 2.76 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{sc}\n\\end{small}\n\\end{center}\n\\end{table}\n\n\\subsubsection{Number of training iterations} \nWe notice the performance of both stages in MusicFlow is sensitive to the number of training iterations. Generally, longer training boosts performance, as can be seen from Table~\\ref{tab:iter-tuning}. While varying the number of training iterations, we maintains the sizes of best models from Table~\\ref{tab:first-scaling} and~\\ref{tab:second-scaling}. Comparing the two stages, longer training consistently enhances performance in the second stage, while there is a degradation in performance with further increases in training iterations in the first stage. \nThis aligns with our observations in model scaling, which highlight the different tendencies of model overfitting in both stages.\n\n\\begin{table}[h]\n\\caption{Impact of training steps on the model performance}\n\\label{tab:iter-tuning}\n\\begin{center}\n\\begin{small}\n\\begin{sc}\n\\begin{tabular}{cccccc}\n\\toprule\nStage 1 & Stage 2 & FAD & FD & KL-div & ISc. \\\\\n\\midrule\n100K & \\multirow{4}{*}{600K} & 3.60 & 18.1 & 1.42  & 2.54 \\\\\n200K & & 3.00 & 17.7  & 1.45 & 2.79 \\\\\n300K & & \\textbf{2.82} & \\textbf{14.2} & \\textbf{1.23}& \\textbf{2.78} \\\\\n400K & & 2.90 & {16.3} & {1.39} & 2.85  \\\\\n\\midrule\n\\multirow{3}{*}{300K} & 200K & 3.19 & 19.3 & 1.42 & 2.51 \\\\\n & 400K & 2.84 & 16.6 & \\textit{1.39} & 2.71 \\\\\n  & 600K & \\textbf{2.82} & \\textbf{14.2} & \\textbf{1.23}& \\textbf{2.78} \\\\\n\\bottomrule\n\\end{tabular}\n\\end{sc}\n\\end{small}\n\\end{center}\n\\vskip -0.1in\n\\end{table}\n\n\\subsubsection{Choice of Semantic Latent Representation} \nThe first stage model predicts semantic latent representations conditioned on text tokens. The choice of the semantic latents has an impact on  the final performance. In addition to HuBERT units, we also experiment with MERT units~\\cite{mert} using the officially released pre-trained music model. In Table~\\ref{tab:semantic_latents}, we can see that it is clearly worse compared to using HuBERT units.\n\n\\begin{table}[htp]\n\\caption{Impact of using a different semantic latent representation instead of HuBERT. We compare with MERT~\\cite{mert} units below.}\n\\label{tab:semantic_latents}\n\\begin{center}\n\\begin{small}\n\\begin{sc}\n\\begin{tabular}{lcccc}\n\\toprule\nSemantic Latent & FAD & FD & KL-div & ISc. \\\\\n\\midrule\nMERT & 3.43 & 18.3 & 1.47  & 2.54 \\\\\nHuBERT & \\textbf{2.82} & \\textbf{14.2} & \\textbf{1.23}& \\textbf{2.78} \\\\\n\\bottomrule\n\\end{tabular}\n\\end{sc}\n\\end{small}\n\\end{center}\n\\vskip -0.1in\n\\end{table}\n\n\\subsubsection{Choice of Acoustic Latent Representation} \nThe second stage model predicts acoustic latent representations from the semantic latent features. The choice of the acoustic latents also affects the final performance. In addition to Encodec, we also experiment with the recently proposed UniAudio tokenizer~\\cite{uniaudio} and DAC~\\cite{dac}. We could not achieve convergence with DAC, and found UniAudio to perform slightly worse compared to Encodec in terms of all quantitative metrics. We report the results in Table~\\ref{tab:acoustic_latents}.\n\n\\begin{table}[htp]\n\\caption{Impact of using a different acoustic latent representation instead of Encodec. We compare with UniAudio~\\cite{uniaudio} below. We could not achieve convergence with DAC~\\cite{dac}}\n\\label{tab:acoustic_latents}\n\\begin{center}\n\\begin{small}\n\\begin{sc}\n\\begin{tabular}{lcccc}\n\\toprule\nAcoustic Latent & FAD & FD & KL-div & ISc. \\\\\n\\midrule\nDAC & - & - & - & - \\\\\nUniAudio & 3.18 & 18.2 & 1.44  & 2.74 \\\\\nEncodec & \\textbf{2.82} & \\textbf{14.2} & \\textbf{1.23}& \\textbf{2.78} \\\\\n\\bottomrule\n\\end{tabular}\n\\end{sc}\n\\end{small}\n\\end{center}\n\\end{table}\n\n\\section{Conclusion}\nWe present MusicFlow, a cascaded flow-matching network for text-guided music generation. Our model leverages a self-supervised model to capture semantic information within music audio. \nComprising two flow matching networks that predict semantic and acoustic features in a cascaded manner, MusicFlow consistently outperforms all public text-to-music models in both subjective and objective metrics, with only a fraction of model parameters and inference steps.\nOverall, MusicFlow achieves performance on par with the state-of-the-art models while being significantly smaller. \nAdditionally, our model allows text-guided music continuation and infilling through in-context learning, eliminating the need for task-specific training. Our future work includes further improving model efficiency by using sophisticated ODE solvers such as~\\cite{shaul2023bespoke}. \n\n\\section*{Impact Statement}\nWhile music generation technologies make music creation more accessible to amateur creators, they also pose potential societal challenges. Given that modern music generation models often require substantial data, preventing copyright infringement deserves careful attention. In this work, we ensure the use of music data for model training adheres to legal terms. For future data scaling, it's essential to inform artists of data usage and provide opt-out options, as commonly practiced in concurrent music generation works. Furthermore, we acknowledge the lack of diversity in our model generations, potentially stemming from the predominantly stock music training data with limited world music. Our future objective is to ensure high-quality music generation across diverse genres.\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2301.11325v1.tex",
        "arXiv-2302.03917v2.tex",
        "arXiv-2410.20478v1.tex"
    ],
    "group_id": "group_98",
    "response": "### Summary of Recent Advances in Text-Conditioned Music Generation\n\n#### Title\nRecent Advances in Text-Conditioned Music Generation: A Comparative Analysis of MusicLM, Noise2Music, and MusicFlow\n\n#### Introduction\nThe field of generative modeling has seen significant advancements in recent years, particularly in the domain of text-to-audio generation. This includes the generation of music from text descriptions, which has been a challenging task due to the complexity of music and the scarcity of paired audio-text datasets. Early approaches to music generation were limited to specific styles or instruments, but recent work has shifted towards generating music from free-form natural language descriptions. This transition has been facilitated by the development of sophisticated generative methods, such as autoregressive language models and diffusion models, which can capture the intricate structures and semantics of music. However, these models often struggle with long-term coherence and require extensive computational resources for training and inference.\n\nThe main challenge in text-conditioned music generation lies in the alignment of high-level text descriptions with low-level audio features. This necessitates the use of intermediate representations that can bridge the gap between text and audio. Additionally, the quality of generated music is highly dependent on the richness and diversity of the training data. Recent works have addressed these challenges by leveraging large-scale datasets and advanced modeling techniques, but there remains a need for more efficient and scalable solutions.\n\n#### Main Content of Each Paper\n\n**MusicLM: Generating Music From Text**\nMusicLM, introduced by Andrea Agostinelli and colleagues from Google Research and IRCAM, is a hierarchical sequence-to-sequence model that generates music from text descriptions. The model is built on top of AudioLM, which casts audio synthesis as a language modeling task using a discrete representation space. MusicLM uses a multi-stage autoregressive approach, where the first stage models semantic tokens and the second stage models acoustic tokens. The semantic tokens are derived from a masked-language-modeling (MLM) module of w2v-BERT, while the acoustic tokens are obtained from a residual vector quantization (RVQ) scheme of SoundStream. The conditioning signal during training is provided by MuLan audio embeddings, and during inference, by MuLan text embeddings. This approach allows MusicLM to generate high-fidelity music at 24 kHz, which remains consistent over several minutes. The authors also introduce MusicCaps, a new dataset of 5.5k music-text pairs, to support future research in text-to-music generation.\n\n**Noise2Music: Text-conditioned Music Generation with Diffusion Models**\nNoise2Music, developed by Qingqing Huang and colleagues from Google Research, is a diffusion-based model for generating music from text prompts. The model consists of two cascaded diffusion models: a generator model that produces an intermediate representation conditioned on text, and a cascader model that generates high-fidelity audio conditioned on the intermediate representation and possibly the text. The intermediate representation can be either a log-mel spectrogram or a low-fidelity waveform. The authors use a data mining pipeline to construct a large-scale training dataset of diverse music audio clips, each paired with multiple descriptive text labels. These labels are generated by a large language model (LM) and scored by a pre-trained music-text joint embedding model (MuLan) to select the most relevant captions. The model is trained to generate 30-second music clips at 24 kHz, with the generator model producing 3.2 kHz audio and the cascader model generating 16 kHz audio. The final super-resolution cascader generates 24 kHz audio from the 16 kHz waveform. The authors evaluate the model using Fréchet Audio Distance (FAD) and MuLan similarity scores, demonstrating superior performance compared to baselines such as Mubert and Riffusion.\n\n**MusicFlow: Cascaded Flow Matching for Text Guided Music Generation**\nMusicFlow, presented by K R Prajwal and colleagues from Meta and the University of Oxford, is a cascaded flow-matching network for text-guided music generation. The model consists of two stages: semantic modeling and acoustic modeling. The first stage generates semantic features conditioned on text, while the second stage generates low-level acoustic features conditioned on both text and the semantic features. The semantic features are obtained from a re-trained HuBERT model, which is a self-supervised speech representation learning framework. The acoustic features are derived from Encodec, a residual vector quantization (RVQ) scheme that maps raw waveforms into dense feature sequences. MusicFlow uses classifier-free guidance (CFG) to balance sample diversity and text coherence. The authors evaluate the model using Fréchet Audio Distance (FAD), Frechet Distance (FD), KL divergence (KLD), Inception score (ISc), and CLAP similarity metrics. MusicFlow demonstrates significant improvements in efficiency and performance compared to existing models, achieving competitive results with fewer parameters and inference steps.\n\n#### Commonalities and Innovations\n\nAll three papers address the challenge of generating high-fidelity music from text descriptions. They share a common goal of improving the alignment between text and music, but each paper introduces unique methodologies and datasets to achieve this goal.\n\n- **MusicLM** uses a hierarchical sequence-to-sequence approach, where semantic tokens are derived from a masked-language-modeling (MLM) module of w2v-BERT and acoustic tokens from a residual vector quantization (RVQ) scheme of SoundStream. The conditioning signal during training is provided by MuLan audio embeddings, while during inference, it uses MuLan text embeddings. This approach allows the model to generate coherent and high-quality music over several minutes.\n- **Noise2Music** employs a cascaded diffusion model, where the first stage generates an intermediate representation (either a log-mel spectrogram or a low-fidelity waveform) conditioned on text, and the second stage generates high-fidelity audio conditioned on the intermediate representation and text. The authors use a data mining pipeline to construct a large-scale training dataset of diverse music audio clips paired with descriptive text labels. This approach enables the model to generate music that adheres closely to the text descriptions.\n- **MusicFlow** leverages flow matching to generate music from text descriptions. The model consists of two stages: semantic modeling and acoustic modeling. The first stage generates semantic features conditioned on text, while the second stage generates low-level acoustic features conditioned on both text and the semantic features. MusicFlow uses HuBERT units for semantic features and Encodec features for acoustic features, and it demonstrates superior performance in terms of efficiency and quality metrics.\n\n#### Comparison of Results\n\nThe three models are evaluated using a variety of metrics, including Fréchet Audio Distance (FAD), Frechet Distance (FD), KL divergence (KLD), Inception score (ISc), and CLAP similarity. The results are summarized in Table 1 below:\n\n| Model            | \\# Params (M) | FAD (↓) | FD (↓) | KLD (↓) | ISc (↑) | CLAP-Sim (↑) | CLAP-Audio (↑) | CLAP-Text (↑) |\n|------------------|----------------|---------|---------|---------|-----------|---------------|---------------|---------------|\n| MusicLM          | 860            | 4.00    | -       | -       | -         | -             | -             | -             |\n| MusicGen         | 1500           | 3.40    | 24.1    | 1.23    | 2.29      | -             | 0.44          | 0.37          |\n| UniAudio         | 1000           | 3.65    | -       | 1.90    | -         | -             | -             | -             |\n| AudioLDM-2       | 746            | 3.13    | 18.8    | 1.20    | 2.77      | -             | 0.44          | 0.43          |\n| Noise2Music      | 1300           | 2.10    | -       | -       | -         | -             | -             | -             |\n| JEN-1            | 746            | 2.00    | -       | 1.29    | -         | -             | -             | -             |\n| MusicFlow (LM+FM)| 546            | 2.69    | 13.2    | 1.23    | 2.69      | -             | 0.48          | 0.56          |\n| MusicFlow (FM+FM)| 330            | 2.82    | 14.2    | 1.23    | 2.78      | -             | 0.48          | 0.56          |\n\nFrom the table, it is evident that MusicFlow (FM+FM) achieves the best performance in terms of FAD, FD, and ISc, while also being the most efficient in terms of model size and inference steps. MusicLM and Noise2Music also perform well, with MusicLM achieving the lowest FAD score among the models. However, MusicFlow surpasses MusicLM in terms of CLAP similarity, indicating better alignment between the generated music and the text descriptions.\n\n#### Conclusion\nThe three papers present significant advancements in text-conditioned music generation, each with its unique approach and dataset. MusicLM leverages hierarchical sequence-to-sequence modeling to generate coherent and high-quality music, while Noise2Music uses cascaded diffusion models to achieve similar results. MusicFlow introduces a cascaded flow-matching approach, which demonstrates superior performance and efficiency. The common theme across these models is the use of intermediate representations to bridge the gap between text and audio, enabling the generation of music that adheres closely to the provided text descriptions.\n\nFuture research directions in this field include improving the interpretability of generated music, reducing training and inference costs, and enhancing the diversity of generated music across different genres. Additionally, there is a need to address the potential risks associated with music generation, such as the misappropriation of creative content and the propagation of biases present in the training data. Efforts should be made to ensure that the generated music does not perpetuate harmful stereotypes or cultural appropriation, and that the models are trained on diverse and representative datasets."
}