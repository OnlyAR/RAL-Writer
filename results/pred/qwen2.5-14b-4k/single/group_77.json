{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Document-Level Event Argument Extraction by Conditional Generation}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nEvent extraction has long been treated as a sentence-level task in the IE community. We argue that this setting does not match human information seeking behavior and leads to incomplete and uninformative extraction results.\nWe propose a document-level neural event argument extraction model by formulating the task as conditional generation following event templates. We also compile a new document-level event extraction benchmark dataset \\textsc{WikiEvents} which includes complete event and coreference annotation. On the task of argument extraction, we achieve an absolute gain of 7.6\\% F1 and 5.7\\% F1 over the next best model on the \\textsc{RAMS} and \\textsc{WikiEvents} datasets respectively. On the more challenging task of informative argument extraction, which requires implicit coreference reasoning, we achieve a 9.3\\% F1 gain over the best baseline.  \nTo demonstrate the portability of our model, we also create the first end-to-end zero-shot event extraction framework and achieve 97\\% of \nfully supervised model's trigger extraction performance and 82\\% of the argument extraction  performance given only access to 10 out of the 33 types on \\textsc{ACE}.\n\\footnote{The programs, data and resources are publicly available for research purpose at \\url{https://github.com/raspberryice/gen-arg}.}\n\n\\end{abstract}\n\n\\section{Introduction}\n\nBy converting a large amount of unstructured text into trigger-argument structures, event extraction models provide unique value in assisting us process volumes of documents to form insights. While real-world events are often described throughout a news document (or even span multiple documents), the scope of operation for existing event extraction models have long been limited to the sentence level.\n\nEarly work on event extraction originally posed the task as document level role filling~\\cite{Grishman1996MUC6} \non a set of narrow scenarios %\nand evaluated on small datasets. \nThe release of ACE\\footnote{\\url{https://www.ldc.upenn.edu/collaborations/past-projects/ace}}, a large scale dataset with complete event annotation, opened the possibility of applying powerful machine learning models \nwhich led to substantial improvement in event extraction. The success of such models and the widespread adoption of ACE as the training dataset established sentence-level event extraction as the mainstream task defintion. \n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/cross-sent-example.pdf}\n    \\caption{Two examples of cross-sentence inference for argument extraction from our \\textsc{WikiEvents} dataset. The PaymentBarter argument of the Transaction.ExchangeBuySell event triggered by ``reserved'' in the first sentence can only be found in the next sentence. \n    The Attack.ExplodeDetonate event triggered by ``detonated'' in the third sentence has an uninformative argument ``he'', which needs to be resolved to the name mention ``McVeigh'' in the previous sentences. \n    }\n    \\label{fig:intro-ex}\n     \\vspace{-0.4cm}\n\\end{figure}\nThis formulation signifies a misalignment between the information seeking behavior in real life and the exhaustive annotation process in creating the datasets.\nAn information seeking session~\\cite{mai2016looking} can be divided into 6 stages: task initiation, topic selection, pre-focus exploration, focus information, information collection and search closure~\\cite{kuhlthau1991searchprocess}. Given a target  event ontology, we can safely assume that topic selection is complete and users start from skimming the documents before they discover events of interest, focus on such events and then aggregate all relevant information for the events. In both the ``pre-focus exploration'' and ``information collection'' stages, users naturally cross sentence boundaries. \n\nEmpirically, using sentence boundaries as event scopes conveniently simplifies the problem, but also introduces fundamental flaws: the resulting extractions are \\textit{incomplete} and \\textit{uninformative}.\nWe show two examples of this phenomenon in Figure \\ref{fig:intro-ex}. \nThe first example exemplifies the case of implicit arguments across sentences. \nThe sentence that contains the PaymentBarter argument ``\\$280.32\" is not the sentence that contains the trigger ``reserve\" for the ExchangeBuySell event.\nWithout a document-level model, such arguments would be missed and result in \\textit{incomplete} extraction.\nIn the second example, the arguments are present in the same sentence, but written as pronouns. Such extraction would be \\textit{uninformative} to the reader without cross-sentence coreference resolution.\n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/arg_extraction_model.pdf}\n    \\caption{Our argument extraction model using conditional generation. On the left we show an example document, template and the desired output for the instance.  Each example document may contain multiple event triggers and we use special $\\langle tgr \\rangle$ tokens to markup the target event trigger for argument extraction (the highlighted word ``reserved\"). The input to the model is the concatenation of the template and the document. The decoded tokens are either from the template or the document. The color of the generated tokens indicate its copy source. After the filled template is generated, we extract the spans to produce the final output. \n   }\n    \\label{fig:model}\n  \\vspace{-0.3cm}\n\\end{figure*}\n\nWe propose a new end-to-end document-level event argument extraction model by framing the problem as \\textit{conditional generation} given a template. Conditioned on the unfilled template %\nand a given context, the model is asked to generate a filled-in template with arguments as shown in Figure \\ref{fig:model}. Our model does not require entity recognition nor coreference resolution as a preprocessing step and can work with long contexts beyond single sentences.\nSince templates are usually provided as part of the event ontology definition, this requires no additional human effort. \nCompared to recent efforts~\\cite{Du2020EventQA, Feng2020ProbingAF, Chen2019ReadingTheManual} that retarget question answering (QA)  models for event extraction, our generation-based model can easily handle the case of missing arguments and multiple arguments in the same role without the need of tuning thresholds and can extract all arguments in a single pass. \n\nIn order to evaluate the performance of document-level event extraction, we collect and annotate a new benchmark dataset \\textsc{WikiEvents}.\nThis document-level evaluation also allows us to move beyond the nearest mention of the argument and instead seek the most informative mention\\footnote{We prefer name mentions over nominal mentions and only use pronoun mentions when no other mentions exist.} in the entire document context. \nIn particular, only 34.5\\% of the arguments detected in the same sentence as the trigger can be considered informative.  We present this new task of \\textit{document-level informative argument extraction} and show that while this task requires much more cross-sentence inference, our model can still perform reliably well. \n\nSince we provide the ontology information (which roles are needed for the event) through the template as an external condition, our model has excellent portability to unseen event types.  By pairing up our argument extraction model with a keyword-based zero-shot trigger extraction model, we enable zero-shot transfer for new event types.\n\nThe major contributions of this paper can be summarized as follows:\n\n\\begin{enumerate}\n    \\item We address the  document-level argument extraction task with an end-to-end neural event argument extraction model by conditional text generation. Our model does not rely on entity extraction nor entity/event coreference resolution. Compared to QA-based approaches, it can easily handle missing arguments and multiple arguments in the same role.\n    \\item We present the first \\textit{document-level event extraction} benchmark dataset with \\textit{complete event and coreference annotation}. We also introduce the new \\textit{document-level informative argument extraction} task, which evaluates the ability of models to learn entity-event relations over long ranges.\n    \\item We release the first end-to-end zero-shot event extraction framework by combining our argument extraction model with a zero-shot event trigger classification model. %\n\\end{enumerate}\n\n\\section{Method}\n\\vspace{-0.2cm}\nThe event extraction task consists of two subtasks: trigger extraction and argument extraction. \nThe set of possible event types and roles for each event type are given by the event ontology as part of the dataset. One  template for each event type is usually pre-defined in the ontology. \\footnote{ACE does not come with templates, but since the event types are subsets of the RAMS AIDA ontology and the KAIROS ontology, we reused templates from the these ontologies.}\n\nWe first introduce our document-level argument extraction model in Section 2.1 and then introduce our zero-shot keyword-based trigger extraction model in Section 2.2.\n\n\\subsection{Argument Extraction Model}\n\nWe use a conditional generation model for argument extraction, where the condition is an unfilled template and a context. The template is a sentence that describes the event with $\\langle arg \\rangle$ placeholders. The generated output is a filled template where placeholders are replaced by concrete arguments. An example of the unfilled template from the ontology and the filled template for the event type Transaction.ExchangeBuySell \\footnote{This type is used for a transaction transferring or obtaining money, ownership, possession, or control of something, applicable to any type, nature, or method of acquisition including barter.} can be seen in Figure \\ref{fig:model}. Notably, one template per event type is given in the ontology, and does not require further human curation as opposed to the question designing process in question answering (QA) models~\\cite{Du2020EventQA, Feng2020ProbingAF}.\n\nOur base model is an encoder-decoder language model (BART~\\cite{Lewis2020BARTDS}, T5~\\cite{Raffel2020T5}. The generation process models the conditional probability of selecting a new token given the previous tokens and the input to the encoder. \n\\begin{equation}\n\\small \np(x \\mid c)=\\prod_{i=1}^{|x|} p\\left(x_{i} \\mid x_{<i}, c\\right)\n\\vspace{-0.4em}\n\\end{equation}\nIn the encoder, bidirectional attention layers are used to enable interaction between every pair of tokens and produce the encoding for the context $c$.\nEach layer of the decoder performs cross-attention over\nthe output of the encoder in addition to the attention over the previous decoded tokens.\n\nTo utilize the encoder-decoder LM for argument extraction, we construct an input sequence of \n$\\langle$s$\\rangle$ template $\\langle s\\rangle \\langle / s \\rangle$document $ \\langle / s \\rangle$.\nAll argument names (arg1, arg2, etc.) in the template are replaced by a special placeholder token $\\langle$arg$\\rangle$. \nThe ground truth sequence is the filled template where the placeholder token is replaced by the argument span whenever possible.\nIn the case where there are multiple arguments for the same slot, we connect the arguments with the word ``and\".\n\nThe generation probability is computed by taking the dot product between the decoder output and the embeddings of tokens from the input.\n\n\\begin{equation}\n\\resizebox{\\hsize}{!}{\n$p(x_i=w | x_{< i}, c,t ) =  \n\\begin{cases}\n\\text{Softmax}\\left( h_i^T \\text{Emb}(w) \\right)  & w \\in V_c  \\\\\n 0  & w \\notin V_c \n\\end{cases}$\n}\n\\end{equation}\n\nTo prevent the model from hallucinating arguments, we restrict the vocabulary of words to $V_c$: the set of tokens in the input.\n\nThe model is trained by minimizing the negative loglikelihood over all (content, template, output) instances in the dataset $D$:\n\\begin{equation}\n\\small \n\\mathcal{L}(D)=-\\sum_{i=1}^{|D|} \\log p_{\\theta}\\left(x^i \\mid c^{i} \\right)\n\\end{equation}\n\n\\begin{table*}[t]\n    \\centering\n    \\small\n    \\begin{tabular}{m{23em}|m{12em}| m{10em} }\n    \\toprule \n    Context & Original  & After \\\\\n    \\midrule \n    When outlining her tax reform policy , Clinton has made clear that she wants to tax the wealthy and make sure they ``pay their fair share .\" \\textbf{She} has \\underline{proposed} (PublicStatement) a \\textbf{tax plan} that would require millionaires and billionaires to pay more taxes than middle-class and lower -income individuals.     &  \\textbf{She} communicated with \\textbf{tax plan}  about  $\\langle$ arg $\\rangle$ at  $\\langle$ arg $\\rangle$  place. {\\color{red}\\textbf{She } is a person/organization/country. \\textbf{tax plan} is a person/organization/country.} &  \\textbf{She} communicated with $\\langle$ arg $\\rangle$ about \\textbf{tax plan} at $\\langle$ arg $\\rangle$ place. {\\color{red}\\textbf{She } is a person/organization/country. } \\\\\n    \\bottomrule \n    \\end{tabular}\n    \\caption{Example of adding type constraints through clarification. The Participant argument of PublicStatement event can only be a person, organization or geo-political entity. The Topic argument can be any type of entity.}\n    \\label{tab:type-example}\n\\end{table*}\nThe event ontology often imposes entity type constraints on the arguments. When using the template only, the model has no access to such constraints and can generate seemingly fluent and sensible responses with the wrong arguments. Inspired by \\cite{shwartz2020unsupervised}, we use clarification statements to add back constraints without breaking the end-to-end property of the model.\n\nIn the example presented in Table \\ref{tab:type-example}, we can see that the greedy decoding selects ``tax plan\" as the second Participant argument for the PublicStatement event.\nApart from the preposition ``with\", there is nothing in the template indicating that this slot should be filled in with a person instead of a topic. \nTo remedy this mistake, we append ``clarifications\" for its argument fillers in the form of type statements: $\\langle$ arg $\\rangle$ is a $\\langle$ type $\\rangle$. \nWe then rerank the candidate outputs by the language modeling probability of the filled template and clarifications. When there are multiple valid types, we take the maximum probability of the valid type statements.\n\\begin{equation}\n\\small \n    \\log p(x| c) = \\sum_i \\log p(x_i | x_{<i}, c) + \\max_{e \\in E_r} \\log p(z_e | x, c ) \n\\end{equation}\n$E_r$ is the set of valid entity types for the role $r$ according to the ontology and $z_e$ is the type statement. \nSince ``tax plan is a person.\" goes against commonsense, the probability of generating this sentence will be low.  \nIn this way, we can prune responses with conflicting entity types. \n\n\\subsection{Keyword-Based Trigger Extraction Model}\nOur argument extraction model relies on detected event triggers (type and offset) as input. Any trigger extraction model could be used in practice, but here we describe a trigger extraction model designed to work with only keyword-level supervision. For example for the ``StartPosition\" event, we use 3 keywords ``hire, employ and appoint\" as initial supervision with no mention level annotation. \\footnote{In a fully supervised setting, it might be desirable to use a supervised trigger extraction model for optimal performance.} This module allows quick transfer to new event types of interest. \n\nWe treat the trigger extraction task as sequence labeling and \nour model is an adaptation of TapNet~\\cite{Yoon2019TapNetNN, Hou2020FewshotST}, which was designed for few-shot classification and later extended to Conditional Random Field (CRF) models. Compared with \\cite{Hou2020FewshotST}, we do not collapse the entries of the transition matrix, making it possible for our model to learn different probabilities for each event type.\nSince our model takes class keywords as input, we refer to this model as \\textsc{TapKey}. %\n\nFor each event type, we first obtain a class representation vector $c_k$ based on given keywords using the masked category prediction method in \\cite{Meng2020TextCU}. \nThis class representation vector is an average over the BERT vector representations of the keywords, with some filtering applied to remove ambiguous occurrences. \nDetails of the filtering process are included in Appendix A.\n\nFollowing the linear-chain CRF model, the probability of a tagged sequence is:\n\\begin{equation}\n\\small \n   \\log  p(y | h ; \\theta) \\propto  \\sum_i  \\varphi(y_i|h_i)  + \\sum_i \\psi (y_i | y_{i-1}, h_i)\n\\end{equation}\n$h_i$ is the output of the embedding network (in our case, BERT-large) corresponding to $x_i$.\n\nThe label space for $y_i$ is the set of IO tags. We choose to use this simplified tagging scheme because it has fewer parameters and the fact that consecutive triggers of the same event are very rare.\n\nThe feature function $\\varphi(\\cdot)$ is defined as \n\\begin{equation}\n    \\varphi (y_i=k | h_i) = \\text{Softmax}  \\left(M(h_i)^T M(\\phi_k)\\right)\n\\end{equation}\n$\\phi_k$ is a normalized  reference vector for class $k$ and $M$ is a projection matrix, both of which are parameters of the model.\n$M$ is not a learned parameter, but solved by taking the QR decomposition of a modified reference vector matrix. Specifically, $M$ satisfies the following equation: \n\\begin{equation}\n    M^T (c_k - \\lambda \\hat \\phi_k) = 0\n\\end{equation}\n\nWe refer to the TapNet~\\cite{Yoon2019TapNetNN} paper for details and also provide a simplified derivation in Appendix A.\n\nThe transition score $\\psi(\\cdot)$ between tags is parameterized using two diagonal matrices $W$ and $W_o$:\n\\begin{equation}\n\\resizebox{0.9\\hsize}{!}{\n\t$\n   \\psi (y_i=k| y_{i-1}=l, h_i) = \\left\\{\n\\begin{array}{l c}\n    M(\\phi_k) W M(h_i)  &   k = l\\neq 0 \\\\\n    M(\\phi_k) W_o M(h_i)&  k \\text{ or } l =0\\\\\n    0 & k\\neq l\n\\end{array}\n\\right.$\n}\n\\end{equation}\n\nIn the training stage, the model parameters $\\{\\phi, W, \\theta_f \\}$ are learned by minimizing the negative log probability of the sequences.\n\\begin{equation}\n\\small \n    \\mathcal{L} =  -  \\frac{1}{N}\\sum \\log p(y| h; \\theta) + \\alpha \\Vert \\Phi^T \\Phi - I \\Vert^2   \n\\end{equation}\nThe matrix $\\Phi$ of all reference vectors is initialized as a diagonal matrix and the second term regularizes the vectors to be close to orthonormal during training. $\\alpha$ is a hyperparameter. \n\nIn the zero-shot setting, we first train on pseudo labeled data before we apply the model. In the pseudo labeling stage, we directly use the cosine similarity between class vectors and the embeddings of the tokens from the language model to assign labels to text. We only use labels with high confident for both event I tags and O tags. The remainder of the tokens will be tagged as X for unknown.\nThen we train the model on the token classification task.  Since none of the parameters in the model are class-specific, the model can be used in a zero-shot transfer setting.\n\n\\section{Benchmark Dataset \\textsc{WikiEvents}}\n\\begin{table}[t]\n    \\centering\n    \\small \n    \\begin{tabular}{l|c c c }\n    \\toprule \n         & Train & Dev & Test  \\\\\n        \\midrule \n         \\# Event types &  49 & 35 & 34 \\\\\n         \\# Arg types & 57 & 32 & 44 \\\\\n         \\midrule \n        \\# Docs  & 206 & 20 & 20 \\\\\n        \\# Sentences &  5262 & 378 & 492 \\\\\n        \\# Events & 3241 & 345 & 365 \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\caption{Statistics for the \\textsc{WikiEvents} dataset.}\n    \\vspace{-0.2cm}\n    \\label{tab:kairos-stats}\n\\end{table}\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/kairos_dist.pdf}\n    \\caption{Distribution of event types and argument types in the \\textsc{WikiEvents} dataset. }\n    \\label{fig:kairos_dist}\n\\end{figure}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.9\\linewidth]{figures/arg_dist.pdf}\n    \\caption{Distribution of distance between event trigger and arguments. Distance is measured in number of words.}\n     \\vspace{-0.2cm}\n    \\label{fig:kairos_arg_dist}\n\\end{figure}\n\n\\subsection{Evaluation Tasks}\nOur dataset evaluates two tasks: \\textit{argument extraction} and \\textit{informative argument extraction}. \n\nFor \\textit{argument extraction}, we use head word F1 (Head F1) and coreferential mention F1 (Coref F1) as metrics. \nWe consider an argument span to be correctly identified if the offsets match the reference. If the argument role also matches, we consider the argument is correctly classified. Since annotators are asked to annotate the head word when possible,\nwe refer to this metric as Head F1.\nFor Coref F1, the model is given full credit if the extracted argument is coreferential with the gold-standard argument as used in \\cite{Ji2008RefiningEE}.\n\nFor downstream applications such as knowledge base construction and question answering, argument fillers that are pronouns will not be useful to the user. Running an additional coreference resolution model to resolve them will inevitably introduce propagation errors. \nHence, we propose a new task: \\textit{document-level informative argument extraction}. We define name mentions to be more informative than nominal mentions, and pronouns to be the least informative. When the mention type is the same, we select the longest mention as the most informative one.\nUnder this task, the model will only be given credit if the extracted argument is the most informative mention in the entire document. %\n\n\\subsection{Dataset Creation}\nWe collect English Wikipedia articles that describe real world events and then follow the reference links to crawl related news articles.\nWe first manually identify category pages such as \\url{https://en.wikipedia.org/wiki/Category:Improvised_explosive_device_bombings_in_the_United_States} and then for each event page (i.e. \\url{https://en.wikipedia.org/wiki/Boston_Marathon_bombing}), we record all the links in its ``Reference\" section and use an article scraping tool\\footnote{\\url{https://github.com/codelucas/newspaper}} to extract the full text of the webpage. \n\nWe follow the recently established ontology from the KAIROS project\\footnote{https://www.ldc.upenn.edu/collaborations/current-projects} for event annotation.\nThis ontology defines 67 event types in a three level hierarchy. In comparison, the commonly used ACE ontology has 33 event types defined in two levels.\n\nWe hired graduate students as annotators and provided example sentences for uncommon event types. \nA total of 26 annotators were involved in the process. We used the BRAT\\footnote{\\url{https://brat.nlplab.org/}} interface for online annotation.\n\nThe annotation process is divided into 2 stages: event mention (trigger and argument) annotation and event coreference annotation. In addition to coreferential mention clusters, we also provide the most informative mention for each cluster.\nDetails about the data collection and annotation process can be found in Appendix B.\n\n\\subsection{Dataset Analysis}\nOverall statistics of the dataset are listed in Table \\ref{tab:kairos-stats}.\nCompared to \\textsc{ACE}, our \\textsc{WikiEvents} dataset has a much richer event ontology, especially for argument roles. \nThe observed distributions of event types and argument roles are shown in Figure \\ref{fig:kairos_dist}.\n\nWe further examine the distance between the event trigger and arguments in Figure \\ref{fig:kairos_arg_dist}. When considering the nearest argument mention, the distribution of the arguments is very concentrated towards 0, showing that this annotation standard favors local extractions.\nIn the case of extracting informative mentions, we have a relatively flat long tail distribution with the average distance being 68.82 words (compared to 4.75 words for the nearest mention). \nIn particular, only 34.5\\% of the arguments detected in the same sentence as the trigger can be considered informative.\nThis confirms the need for document level inference in the search of informative argument fillers. \n\n\\section{Experiments}\nOur experiments fall under three settings: (1) document-level event argument extraction; (2) document-level informative argument extraction and (3) zero-shot event extraction. \n\nFor document-level event argument extraction we follow the conventional approach of regarding the argument mention with closest proximity to the trigger as the ground truth. In the second setting we consider the most informative mention of the argument as the ground truth.\n\nThe zero-shot setting examines the portability of the model to new event types. Under this setting we consider a portion of the event types to be known and only annotation for these event types will be seen. We used two settings for selecting known types: 10 most frequent events types and 8 event types, one from each parent type of the event ontology. The evaluation is done on the complete set of event types.\nWe refer the reader to Appendix C for implementation details and hyperparameter settings.\n\n\\subsection{Datasets}\nIn addition to our dataset \\textsc{WikiEvents}, we also report the performance on the Automatic Content Extraction (\\textsc{ACE}) 2005 dataset\\footnote{\\url{https://www.ldc.upenn.edu/collaborations/past-projects/ace}} and the Roles Across Multiple Sentences (\\textsc{RAMS}) dataset\\footnote{\\url{http://nlp.jhu.edu/rams}}. \n\nWe follow preprocessing from \\cite{Lin2020OneIE, Wadden2019EntityRA} for the \\textsc{ACE} dataset.\n\\footnote{Note that our preprocessing procedure is slightly different from \\cite{Du2020EventQA} as we kept pronouns as valid event triggers and arguments.}\nStatistics of the \\textsc{ACE} data splits can be found in Table \\ref{tab:ace-stats}.\n\\begin{table}[]\n    \\centering\n    \\small \n    \\begin{tabular}{l| c c c  }\n    \\toprule \n        Split & Event Types & \\# Sents & \\# Events \\\\\n        \\midrule \n        \\textbf{Full Training} & 33  & 17172 & 4202  \\\\\n        \\textbf{Freq} & 10 & 17172 & 3398  \\\\\n        \\textbf{Ontology} & 8 & 17172 & 1311  \\\\\n        \\textbf{Dev} & - & 923 &   450  \\\\\n        \\textbf{Test} & - & 832 &   403  \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\caption{Dataset statistics for \\textsc{ACE} under multiple settings. In the \\textbf{Freq} split, we keep the 10 most frequent event types. In the \\textbf{Ontology} split, we keep 1 event subtype per general type in LIFE, MOVEMENT, TRANSACTION, BUSINESS, CONFLICT, CONTACT, PERSONNEL and JUSTICE. \\footnote{The selected types are 'Movement:Transport', 'Personnel:Elect',\n'Business:Start-Org', 'Life:Injure', 'Transaction:Transfer-Money',\n'Justice:Arrest-Jail', \n'Contact:Phone-Write', 'Conflict:Demonstrate'.} }\n    \\label{tab:ace-stats}\n\\end{table}\n\\textsc{RAMS}~\\cite{Ebner2020RAMS} is a recently released dataset with cross-sentence argument annotation. A 5-sentence window is provided for each event trigger and the closest argument span is annotated for each role. \nWe follow the official data splits from Version 1.0.\n\n    \n\\subsection{Document-Level Event Argument Extraction}\nTable \\ref{tab:RAMS-AE} shows the performance for argument extraction on \\textsc{RAMS}. \nOn the \\textsc{RAMS} dataset, we mainly compare with Two-step~\\cite{Zhang2020TwoStep}, which is the current SOTA on this dataset.  To handle long contexts, it breaks down the argument extraction into two steps: head detection and expansion.  \n\nIn Table \\ref{tab:KAIROS-AE} we show the results for the \\textsc{WikiEvents} dataset. We compare with a popularly used BERT-CRF baseline~\\cite{shi2019simpleBERTCRF} that performs trigger extraction on sentence-level and BERT-QA~\\cite{Du2020EventQA} ran on sentence-level and document-level.\n\\begin{table}[]\n    \\centering\n    \\small\n    \\begin{tabular}{c|c c  }\n    \\toprule \n        Model  &  Span F1 & Head F1\\\\\n        \\midrule \n       Seq   &  40.5 & 48.0   \\\\\n       Two-step & 41.8 & 49.7  \\\\\n       \n       \\ours & \\textbf{48.64} & \\textbf{57.32} \\\\\n       \\bottomrule \n       \n    \\end{tabular}\n    \\caption{Supervised argument extraction results (\\%) on RAMS test set. Both the \\textbf{Seq} and \\textbf{Two-step} model use type-constrained decoding to improve performance. \n    The official scorer was used to compute results.\n    }\n    \\label{tab:RAMS-AE}\n\\end{table}\n\n\\begin{table}[t]\n    \\centering\n    \\small\n    \\resizebox{\\hsize}{!}{\n    \\begin{tabular}{c|c  c c c }\n    \\toprule \n    Model & \\multicolumn{2}{c}{Arg Identification} & \\multicolumn{2}{c}{Arg Classification}\\\\\n    \\midrule\n          &  Head F1 & Coref F1 &Head F1 & Coref F1 \\\\\n        \\midrule \n        BERT-CRF & 69.83 & 72.24  & 54.48 & 56.72 \\\\\n        BERT-QA  &61.05 &  64.59 & 56.16 &  59.36  \\\\\n        BERT-QA-Doc & 39.15 & 51.25 & 34.77 & 45.96 \\\\\n      \\ours & \\textbf{71.75}  & \\textbf{72.29} & \\textbf{64.57} & \\textbf{65.11} \\\\\n      \\bottomrule \n    \\end{tabular}}\n    \\caption{Argument extraction results (\\%) on \\textsc{WikiEvents} test set. }\n    \\label{tab:KAIROS-AE}\n\\end{table}\n\n\\subsection{Document-Level Informative Argument Extraction}\nWe test on \\textsc{WikiEvents} using the informative argument as the training data and also compare with the BERT-CRF and BERT-QA baselines. \nResults are shown in Table \\ref{tab:KAIROS-info-arg}.\n\n\\begin{table}[t]\n    \\centering\n    \\small\n    \\resizebox{\\hsize}{!}{\n    \\begin{tabular}{c|  c    c  c c }\n    \\toprule \n    Model & \\multicolumn{2}{c }{Arg-I} & \\multicolumn{2}{c}{Arg-C} \\\\\n      & Head F1 & Coref F1  & Head F1 & Coref F1 \\\\\n        \\midrule \n        BERT-CRF & 52.71 & 58.12 & 43.29 &  47.70 \\\\\n        BERT-QA & 46.88 & 49.89 & 43.44 & 46.45 \\\\\n        BERT-QA-Doc & 26.29 & 31.54 & 24.46 & 29.26 \\\\\n      \\ours & \\textbf{56.10} & \\textbf{62.48} & \\textbf{51.03} & \\textbf{57.04} \\\\ \n      \\bottomrule \n    \\end{tabular}}\n    \\caption{Informative argument extraction results on \\textsc{WikiEvents} test set. }\n    \\label{tab:KAIROS-info-arg}\n\\end{table}\n\nComparing the results in Tables \\ref{tab:KAIROS-AE} and \\ref{tab:KAIROS-info-arg}, we have the following findings:\n\\begin{enumerate}\n    \\item Informative argument extraction is a much more difficult task compared to nearest argument extraction. This is exemplified by the large performance gap for all models.\n    \\item While CRF models are good at identifying spans, the  performance is hindered by classification. The arguments follow a long tail distribution and since CRF models learn each argument tag separately, it cannot leverage the similarity between argument roles to improve the performance on rarely seen roles. \n    \\item QA models, on the other hand, suffer from poor argument identification. When the QA model produces multiple answers for the same role, these answer spans are often close to each other or overlap. We show a concrete example in the qualitative analysis. \n    \\item Directly applying the BERT-QA model to document level does not work. The QA model gets easily distracted by the additional context and does not know which event to focus on. We think that this is not a fundamental limitation of the QA approach, but a sign that repurposing QA models for document-level event extraction needs more investigation.\n\\end{enumerate}\n\n\\begin{table*}[]\n    \\centering\n    \\small \n    \\resizebox{\\hsize}{!}{\n    \\begin{tabular}{l |l  c c c }\n    \\toprule \n    Context     &  Role & Ours & BERT-CRF & BERT-QA \\\\\n    \\midrule \n     \\multirow{8}{23em}{\n     \\textbf{I} have been \\underline{in touch} (E1:Contact.Contact.Correspondence) with the NDS \\textbf{official} in the province and they told me that over 100 \\textbf{members} of the NDS were \\underline{killed}(E2:Life.Die)  in the big explosion , \" the former provincial official said .  \n     Sharif Hotak , a member of the provincial council in Maidan Wardak said \\textbf{he} \\underline{saw} \n     (E3:Cognitive.IdentifyCategorize) \\textbf{bodies}  of 35 Afghan forces in the \\textbf{hospital}.\"} \n     & E1: Participant &  I & I  & NDS \\\\\n     & &  NDS official & official & I  \\\\\n     & & & & official \\\\\n     & & & & NDS official \\\\\n     & E2: Victim & members & members & members\\\\\n     & E3: Identifier &  he & N/A& N/A \\\\\n     & E3: IdentifiedObject &  bodies & N/A& N/A \\\\\n     & E3: Place & hospital & N/A& N/A \\\\\n     \\bottomrule \n    \\end{tabular}}\n    \\caption{An example of document-level argument extraction task on \\textsc{WikiEvents}. This excerpt contains 3 events: Contact, Die and IdentifyCategorize. For the Contact event E1, BERT-QA over-generates answers for the participant span. For the Die event, all three models can correctly extract the Victim argument. For the IdentifyCategorize event which is relatively rare, only our model can successfully extract all arguments.\n    }\n    \\label{tab:arg-ex}\n\\end{table*}\n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=.9\\linewidth]{figures/AIDA-example.pdf}\n    \\caption{An example of our model's argument extraction output on the SM-KBP dataset. Arguments highlighted in orange are newly found by our model compared to the baseline system OneIE~\\cite{Lin2020OneIE}. }\n    \\label{fig:aida}\n\\end{figure*}\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/informative-mention-ex.pdf}\n    \\caption{An example of our model's prediction of informative arguments. Only arguments for the boldfaced event triggers are shown. Notably, our model can correctly identify ``Bilal Mohammed and Mieralli Yusufu\" as the as the Defendant for all the ChargeIndict events and ``Erawan Shrine\" as the place of attack.}\n    \\label{fig:info-ex}\n\\end{figure}\n\n\\subsection{Zero-Shot Event Extraction}\nWe show results for the zero-shot transfer setting in Table \\ref{tab:zero-shot}.\nSince the baseline BERT-CRF model~\\cite{shi2019simpleBERTCRF} cannot handle new labels directly, we exclude it from comparison.\nIn addition to BERT-QA, we also replace our \\textsc{TapKey} trigger extraction model with a Prototype Network\\cite{Snell2017PrototypicalNF}\\footnote{When 0 event types are seen, we set the transformation function in the Prototype Network to be an identity function; in other cases, we use a two layer MLP.}. %\nWe replace the prototypes with the class vectors to enable zero-shot learning.  Complete results for trigger extraction are included in Appendix D.\n\nThe performance of BERT-QA is greatly limited by the trigger identification step. Both the Prototype network and our model \\textsc{TapKey} can leverage the keyword information to assist transfer. \nRemarkably, \\textsc{TapKey} has only 3 points drop in F1 using only 30\\% of the training data compared to the full set. \nThe argument extraction component is more sensitive to the reduction in training data, but still performs relatively well. \nWe notice that when a template is completely new, the model might alter the template structure during generation. \n\n\\begin{table*}[t]\n    \\centering\n    \\small \n    \\begin{tabular}{c|c| c c c c   }\n          \\toprule \n      \\# Seen Event Types &  Model  & TI F1 &  TC F1 & AI Head F1 &  AC Head F1  \\\\\n      \\midrule \n       \\multirow{2}{*}{0} \n       & Prototype &  3.19 & 2.22  & - & -  \\\\\n      & \\textsc{TapKey} &  55.19 &  52.10 &- & -\\\\\n      \\midrule \n       \\multirow{3}{*}{10 most frequent} \n      & BERT-QA &  57.06 &  53.83 & 39.37  & 38.27  \\\\\n         & Prototype + BART-Gen  & 69.48  & 66.06 & 46.08 & 41.93  \\\\\n       & \\textsc{TapKey} + BART-Gen &  72.31  & 69.23  & 48.18 &  44.19 \\\\\n      \\midrule \n       \\multirow{3}{*}{1 per general type} \n      & BERT-QA & 27.56 & 25.32 &24.87  & 24.29 \\\\\n      & Prototype + BART-Gen & 68.29 & 65.85 & 39.51 &  34.63  \\\\\n      & \\textsc{TapKey} + BART-Gen  & 72.23 & 68.55 & 39.80  & 35.11 \\\\\n      \\midrule \n      \\multirow{1}{*}{Full} \n      &  \\textsc{TapKey} + BART-Gen & 74.36 & 71.13 & 55.22 & 53.71 \\\\ \n         \\bottomrule \n    \\end{tabular}\n    \\caption{Zero-shot event extraction results (\\%) on \\textsc{ACE}. ``10 most frequent event types\" corresponds to the \\textbf{Freq} data split and ``1 per general type\" corresponds to the \\textbf{Ontology} data split. Fully supervised results are provided for reference. }\n    \\label{tab:zero-shot}\n    \\vspace{-0.2cm}\n\\end{table*}\n\n\\subsection{Qualitative Analysis}\nWe show a comparison of our model's extractions with baselines in Table \\ref{tab:arg-ex} for the argument extraction task on \\textsc{WikiEvents}.\nOur model is able to effectively capture all arguments, while the CRF model struggles with rare event types and the QA model is hindered by over-generation.\n\nAn example of informative argument extraction from our model is displayed in Figure \\ref{fig:info-ex}. \nOur model is able to choose the informative mentions of the Defendant of indiction and Place of attack even when the trigger is a few sentences away.\n\nWe also applied our model as part of a pipeline multimedia multilingual knowledge extraction system~\\cite{GAIASystem2020} for the NIST streaming multimedia knowledge base population task (SM-KBP2020)\\footnote{\\url{https://tac.nist.gov/2020/KBP/SM-KBP/}}. Our model was able to discover 53\\% new arguments compared to the original system, especially for those that were further away from the event trigger. The overall system achieved top 1 performance. We show some examples in Figure \\ref{fig:aida}. \n\n\\subsection{Remaining Challenges}\n\\noindent \\textbf{Ontological Constraints} \nSome of the roles are mutually exclusive, such as the Origin/Destination in the Transport event and the Recipient/Yielder in the Surrender event.\nIn the following example, ``Japan\" was extracted as both part of the Recipient and the Yielder of the Surrender event: \n\\textit{``If South Korea drifts into the orbit of the \\textbf{US and Japan}, China's influence on the Korean peninsula could be badly compromised.\" At a military parade in Beijing to mark the 70th anniversary of the \\underline{surrender} of \\textbf{Japan} last September, ...\"}. \nSuch constraints might be incorporated into the decoding process of the model.\n\n\\noindent \\textbf{Commonsense Knowledge}\nIn the following instance with implicit arguments: \\textit{``Whether the \\textbf{U.S. } extradites \\textbf{Gulen}  or not this will be a political decision, ”Bozdag said.“ If he is not \\underline{extradited}, \\textbf{Turkey} will have been sacrificed for a terrorist.” A recent opinion poll showed two thirds of Turks agree with their president that Gulen was behind the \\textbf{coup plot.}\"}, our model mistakenly labels ``U.S.\" as the Destination of the extradition and ``Turkey\" as the Source even though the Extraditer is correctly identified as ``U.S.\". Commonsense knowledge such as ``The extraditer, if being a country, is usually is same as the source of extradition\" would be helpful to fix this error. \n\n \n \n \n \\section{Related Work}\n\n\\subsection{Document-Level Event Extraction}\nDocument-level event extraction can be traced back role filling tasks from the MUC conferences~\\cite{Grishman1996MUC6} that required retrieving participating entities and attribute values for specific scenarios. The KBP slot filling challenge\\footnote{\\url{https://tac.nist.gov/2017/KBP/index.html}} is akin to this task, but centered upon entities. \n\nIn general, document-level argument extraction is an under-explored topic, mainly due to the lack of datasets.\nThere have been a few datasets published specifically for implicit semantic role labeling, such as the SemEval 2010 Task 10~\\cite{Ruppenhofer2010SemEval2010T1}, the Beyond NomBank dataset~\\cite{Gerber2010BeyondNA} and ON5V~\\cite{Moor2013PredicatespecificAF}. However, these datasets were small in size and only covered a small set of carefully selected predicates. \nRecently, \\cite{Ebner2020RAMS} published the RAMS dataset, which contains annotation for cross-sentence implicit arguments covering a wide range of event types.\nAlbeit, this dataset only annotates one event per document, motivating us to create a new benchmark with complete event and coreference annotation.\n\nThe GRIT model~\\cite{Du2020GRITGR} is a generative model designed for the MUC task which can be seen as filling in predefined tables.  \nIn comparison, we treat the template (for example \"<arg1> attacked <arg2> using <arg3> at <arg4> place\") as part of the model input along with the document context. This allows us to share model parameters across all event types and enables zero-shot transfer to new event types. \n\n\\subsection{Zero-shot Event Extraction}\nEarly attempts at zero-shot or few-shot event extraction rely on preprocessing such as semantic role labeling(SRL)~\\cite{Peng2016EventDA} or abstract meaning representation (AMR)~\\cite{Huang2018ZeroShotTL} to detect trigger mentions and argument mentions before performing classification on the detected spans.\n\nAnother line of work only examines the subtask of trigger detection, essentially reducing the task to few-shot classification. Both \\cite{Lai2020ExtensivelyMF} and \\cite{Deng2020MetaLearningWD} extend upon the prototype network model~\\cite{Snell2017PrototypicalNF} for classification.\n\nRecent work on zero-shot event extraction has posed the problem as question answering~\\cite{Chen2019ReadingTheManual, Du2020EventQA, Feng2020ProbingAF} with different ways of designing the questions.\n\n\\section{Conclusion \\& Future Work}\nIn this paper, we advocate document-level event extraction and propose the first document-level neural event argument extraction model. \nWe also release the first document-level event extraction benchmark dataset~\\textsc{WikiEvents} with complete event and coreference annotation.\nOn both the conventional argument extraction task and the new informative argument extraction task, our proposed model surpasses CRF-based and QA-based baselines by a wide margin.\nAdditionally, we demonstrate the portability of our model by applying it to the zero-shot setting. \nGoing forward, we would like to incorporate more ontological knowledge to produce more accurate extractions.\n\n\\section*{Acknowledgement}\nThe authors would like to thank Tuan Lai and Zhenhailong Wang for their help in the data processing and annotation organization effort.\nWe also thank all the annotators who have contributed to the annotations of our training data (in alphabetical order): Daniel Campos, Anthony Cuff, Yi R. Fung, Xiaodan Hu, Emma Bonnette Hamel, Samual Kriman, Meha Goyal Kumar, Manling Li, Tuan M. Lai, Ying Lin, Sarah Moeller, Ashley Nobi, Xiaoman Pan, Nikolaus Parulian, Adams Pollins, Rachel Rosset, Haoyu Wang, Qingyun Wang, Zhenhailong Wang, Spencer Whitehead, Lucia Yao, Pengfei Yu, Qi Zeng, Haoran Zhang, Hongming Zhang, Zixuan Zhang.\n\nThis research is based upon work supported by U.S. DARPA KAIROS Program No. FA8750-19-2-1004, U.S. DARPA AIDA Program No. FA8750-18-2-0014, National Science Foundation IIS-19-56151, IIS-17-41317, and IIS 17-04532, and Air Force No. FA8650-17-C-7715. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Prompt for Extraction? PAIE: Prompting Argument Interaction for \\\\ Event Argument Extraction}\n\n\\begin{document}\n\n\\maketitle\n\n\\renewcommand{\\thefootnote}{\\fnsymbol{footnote}}\n\\footnotetext[1]{Equal Contribution.}\n\\footnotetext[2]{Work was done when Yubo, Zehao, Mukai and Meiqi were intern researchers at SenseTime Research.}\n\\footnotetext[3]{Corresponding Author.}\n\\renewcommand{\\thefootnote}{\\arabic{footnote}}\n\n\\begin{abstract}\nIn this paper, we propose an effective yet efficient model PAIE for both sentence-level and document-level Event Argument Extraction (EAE), which also generalizes well when there is a lack of training data. On the one hand, PAIE utilizes prompt tuning for extractive objectives to take the best advantages of Pre-trained Language Models (PLMs). It introduces two span selectors based on the prompt to select start/end tokens among input texts for each role. On the other hand, it captures argument interactions via multi-role prompts and conducts joint optimization with optimal span assignments via a bipartite matching loss. Also, with a flexible prompt design, PAIE can extract multiple arguments with the same role instead of conventional heuristic threshold tuning. We have conducted extensive experiments on three benchmarks, including both sentence- and document-level EAE. The results present promising improvements from PAIE ($3.5\\%$ and $2.3\\%$ F1 gains in average on three benchmarks, for PAIE-base and PAIE-large respectively). Further analysis demonstrates the efficiency, generalization to few-shot settings, and effectiveness of different extractive prompt tuning strategies. Our code is available at \\url{https://github.com/mayubo2333/PAIE}.\n\\end{abstract}\n\n\\section{Introduction}\n\nUnderstanding text by identifying the event and arguments has been a long-standing goal in Natural Language Processing (NLP)~\\cite{sundheim-1992-overview}. As shown in Fig.~\\ref{fig: D-EAE task example}, we can quickly understand that the document is talking about a \\textit{Sell} event, with four involved arguments, i.e., \\textit{Vivendi} (Seller), \\textit{Universal Studios} (Artifact), \\textit{parks} (Artifact), and \\textit{company} (Artifact), where the argument roles are in brackets. Since event detection has achieved great success in recent years~\\cite{wang-etal-2021-cleve}, the main challenge lies in Event Argument Extraction (EAE).\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=1.0\\linewidth]{example.pdf}\n\\caption{Examples of (top) sentence-level and (bottom) document-level event argument extraction. Trigger words are included in special tokens <t> and </t>. Underlined words denote arguments and arcs denote roles.}\n\\label{fig: D-EAE task example}\n\\centering\n\\end{figure}\n\nTypical efforts in EAE can be roughly classified into two groups. The first group of methods formulates it as a semantic role labeling problem~\\cite{wei-etal-2021-trigger}. There are generally two steps --- first identifying candidate spans and then classifying their roles. Although joint models are proposed to optimize them together, high dependence on candidates may still suffer from error propagation~\\cite{li-etal-2013-joint}. In the second group, recent studies tend to follow the success of Pre-trained Language Models (PLMs) and solve EAE by Question Answering (QA)~\\cite{liu-etal-2021-machine, wei-etal-2021-trigger, du-cardie-2020-event, liu-etal-2020-event, li-etal-2020-event} and Text Generation~\\cite{lu-etal-2021-text2event, li-etal-2021-document}. QA-based models can effectively recognize the boundaries of arguments with role-specific questions, while the prediction has to be one by one. \nGeneration-based methods are efficient for generating all arguments, but sequential predictions degrade the performance on long-distance and more arguments. Besides, the state-of-the-art performance is still unsatisfactory (around 68\\% F1 on the widely used dataset ACE05~\\cite{doddington-etal-2004-automatic}). Here raises an interesting question, is there any way to combine the merits of the above methods, as well as to boost the performance?\n\nThis paper targets real scenarios, which require the EAE model to be effective yet efficient at both sentence and document levels, and even under the few-shot setting without sufficient training data. To do this, we highlight the following questions:\n\n\\begin{itemize}[leftmargin=*]\n    \\setlength{\\parskip}{0pt}\n   \\setlength{\\itemsep}{0pt plus 1pt}\n    \\item How can we extract all arguments simultaneously for efficiency?\n    \\item How to effectively capture argument interactions for long text, without knowing them in advance?\n    \\item How can we elicit more knowledge from PLMs to lower the needs of annotation?\n\\end{itemize}\n\nIn this paper, we investigate prompt tuning under an extractive setting and propose a novel method \\textbf{PAIE} that \\textbf{P}rompting \\textbf{A}rgument \\textbf{I}nteractions for \\textbf{E}AE. It extends QA-based models to handle multiple argument extraction and meanwhile takes the best advantage of PLMs. The basic idea is to design suitable templates to prompt all argument roles for PLMs, and obtain role-specific queries to jointly select optimal spans from the text. Thus, instead of unavailable arguments, each role in the template serves as a slot for interactions, and during learning, PLMs tend to fill these slots with exact arguments via a matching loss. By predicting arguments together, PAIE enjoys an efficient and effective learning procedure. Besides, the inter-event knowledge transfer between similar role prompts alleviates the heavy burden of annotation cost.\n\nSpecifically, for prompting extraction, we design two span selectors based on role prompts, which select start/end tokens among input texts. We explore three types of prompts: manual template, concatenation template, and soft prompt. They perform well at both sentence-level EAE (S-EAE) and document-level EAE (D-EAE) and ease the requirements of the exhaustive prompt design. For joint span selection, we design a bipartite matching loss that makes the least-cost match between predictions and ground truth so that each argument will find the optimal role prompt. It can also deal with multiple arguments with the same role via flexible role prompts instead of heuristic threshold tuning.\nWe summarize our contributions as follow:\n\\begin{itemize}[leftmargin=*]\n    \\setlength{\\parskip}{0pt}\n   \\setlength{\\itemsep}{0pt plus 1pt}\n    \\item We propose a novel model, PAIE, that is effective and efficient for S-EAE and D-EAE, and robust to the few-shot setting.\n    \\item We formulate and investigate prompt tuning under extractive settings, with a joint selection scheme for optimal span assignments.\n    \\item We have conducted extensive experiments on three benchmarks. The results show a promising improvement with PAIE ($3.5\\%$ and $2.3\\%$ F1 gains on average absolutely in base and large model). Further ablation study demonstrates the efficiency and generalization to few-shot settings of our proposed model, as well as the effectiveness of prompt tuning for extraction. \n\\end{itemize}\\section{Related Works}\n\\label{sec:related}\n\n\\paragraph{Event Argument Extraction:}\nEvent Argument Extraction is a challenging sub-task of event extraction (EE). \nThere have been great numbers of studies on EAE tasks since an early stage~\\cite{chen-etal-2015-event, nguyen-etal-2016-joint-event, huang-etal-2018-zero, yang-etal-2018-dcfee, Sha2018JointlyEE, zheng-etal-2019-doc2edag}. \n\\citet{huang-peng-2021-DVN} propose to leverage Deep Value Networks\n(DVN) that captures cross-event dependencies for EE. \\citet{huang-jia-2021-SCDEE} convert documents to unweighted graph and use GAT to alleviate\nthe role overlapping issue. A common idea is to first identify argument candidates and then fill each with a specific role via multi-label classification~\\cite{,lin-etal-2020-joint}.  To deal with implicit arguments and multiple events, \\citet{xu-etal-2021-document} construct a heterogeneous graph of arguments, while DEFNN \\cite{yang-etal-2021-document} predicts arguments via Parallel Prediction Networks.\n\nA recent trend formulates EAE as an extractive question answering (QA) problem~\\cite{du-cardie-2020-event, liu-etal-2020-event}. This paradigm naturally induces the language knowledge from pre-trained language models by converting EAE tasks to fully-explored reading comprehension tasks via a question template. \\citet{wei-etal-2021-trigger} considers the implicit interaction among roles by adding constraints with each other in template, while~\\citet{liu-etal-2021-machine} leverages data augmentation to improve the performance. However, they can only predict roles one by one, which is inefficient and usually leads to sub-optimal performance.\n\nWith the help of the pre-trained Encoder-Decoder Transformer architecture, such as BART~\\cite{lewis-etal-2020-bart} and T5~\\cite{2020t5}, there are also some recent works converting extraction tasks to generation tasks. \\citet{paolini2021structured} propose TANL to handle a variety of structured prediction tasks, including EAE, by a unified text-to-text approach and extract all arguments in a single pass. \\citet{lu-etal-2021-text2event} follow TANL and also take EAE as a sequential generation problem. \\citet{li-etal-2021-document} target generation model by designing specific templates for each event type. In comparison, we prompt argument interactions to guide PLMs and optimize the multiple argument detection by designing a bipartite matching loss. This not only improves the understanding of long-distance argument dependencies but also enjoys an efficient procedure via prompt-based learning.\n\n\\paragraph{Prompt-based Learning:}\nPrompt-based learning is a new paradigm emerging in the field of pre-trained language models \\cite {liu2021pretrain}. Unlike the pre-training and fine-tuning paradigm, prompt-based methods convert the downstream tasks to the form more consistent with the model's pre-training tasks. \\citet{schick-schutze-2021-exploiting} convert a variety of classification problems to cloze tasks by constructing related prompts with blanks and finding a mapping from particular filled words to predicted categories. \\citet{li-liang-2021-prefix} focus on generation tasks and propose lightweight prefix tuning by freezing model parameters and only adjusting a sequence of continuous task-specific vectors. Different from the above prompt tuning methods designed for classification or generation tasks, our proposed method returns to \\textbf{linear head} setting for fitting extraction task better. It is somewhat similar as a concurrent work P-tuning v2~\\cite{DBLP:journals/corr/abs-2110-07602}. \\begin{figure*}[t]\n    \\centerline{\\includegraphics[width=0.9\\linewidth]{model_architecture.pdf}}\n    \\caption{The overall architecture of PAIE. Given a context (about an event), PAIE first creates joint prompts based on its event type. Then the context and prompt are fed into the BART-Encoder and BART-Decoder to generate context representation and role-specific span selectors. Multiple span selectors extract argument spans from the context simultaneously. A bipartite matching loss finally optimizes the global span assignment.}\n\n    \\label{fig: model framework}\n\\end{figure*}\n\n\\section{Methodology}\n\\label{sec:method}\n\nPAIE considers multiple arguments and their interactions to prompt PLMs for joint extraction. Our model, as illustrated in Fig.~\\ref{fig: model framework}, contains three core components: \\textit{prompt creation}, \n\\textit{span selector decoding}, \nand \\textit{span prediction}. In the following sections, we will first formulate prompt for extraction, and describe each component in turn.\n\n\\subsection{Formulating Prompt for Extraction}\n\\label{subsec: formulation}\nExisting prompt-based methods mainly focus on classification and generation tasks. Conventional extraction objectives are converted into a generation task. This brings an inefficiency issue that the model has to enumerate all of extraction candidates. For example, \\citet{cui-etal-2021-template} design the prompt for named entity recognition: \\textit{[candidate span] is [entity type/not a] entity}. The models need to fill the first slot with candidate entities, and check the outputs of LM for the second slot for extraction. Can prompt-based methods directly be applied on extraction? since the basic idea is similar with classification/generalization --- comparing the slot embeddings with label vocabulary/input tokens. Here, we give a formulation about the general extractive prompting method and then apply it on EAE for a case study.\n\n\\noindent{\\textbf{(1)}} \\textit{Prompt Creation}. Given context $X$ and a series of queries $Q = \\{q_1, q_2, ..., q_K\\}$, we create a joint prompt containing all these queries, where $f_{prompt}$ is the prompt creator.\n$$\nPt = f_{prompt}(Q)\n$$\n\n\\noindent{\\textbf{(2)}} \\textit{Prompted Selector Decoding}. \nGiven a PLM $\\Lagr$, context $X$, and prompt $Pt$, we decode a query-specific (answering) span selector as follows:\n$$\n\\theta_{q_k} = h_{\\Lagr}(q_k; Pt, X) \n$$\nwhere $q_k$ is the $k$-th query in the prompt and $h_{\\Lagr}$ is the outputs of PLMs.\n\n\\noindent{\\textbf{(3)}} \\textit{Prompted Span Selection}. To find the optimal span, we design two selectors for the start and end tokens from context: \n$$\n(s, e)_{q_k} = \\text{Span-search} [g_{\\Lagr}(X; \\theta_q)]\n$$\nwhere $(s, e)_{q_k}$ is the span about $k$-th query and $g_{\\Lagr}$ is the span selector. Clearly, such formulation is better than generative extraction by mainly considering the adjacent constraints of span.\n\n\\paragraph{Task Definition}\nWe formulate EAE task as a prompt-based span extraction problem on dataset $D$. Given an instance $(X, t, e, R^{(e)}) \\in D$, where $X$ denotes the context, $t \\subseteq X$ denotes the trigger word, $e$ denotes the event type and $R^{(e)}$ denotes the set of event-specific role types, we aim to extract a set of span $A$. Each $a^{(r)} \\in A$ is a segmentation of $X$ and represents an argument about $r \\in R^{(e)}$.\n\n\\subsection{Prompt Creation for EAE}\n\\label{subsec: joint prompt creation}\nWe create a set of prompts for each event type $e$ in dataset $D$. Each prompt contains all roles $r \\in R^{(e)}$. For example in Fig.\\ref{fig: model framework}, given event type $e$ as \\textit{negotiate} and $R^{(e)}$ as $\\{\\textit{Participant}, \\textit{Topic}, \\textit{Place}\\}$, the prompt $Pt^{(e)}$ may be defined as follows:\n\n\\textit{\\underline{Participant} communicated with \\underline{Participant} about \\underline{Topic} at \\underline{Place} .}\n\nWe call the mentions of roles in the prompt as \\textbf{slot}, and there are four slots underlined in this example (and colored in Fig.~\\ref{fig: model framework}). Such design allows our model to capture the implicit interactions among different roles.\n\nTo avoid threshold tuning for multiple arguments with the same role, \nthe prompt is flexible to use multiple slots for the same role, such as role \\textit{Participant} in the above example. The number of slots for the role is heuristically determined according to the maximum number of arguments of each role in the training dataset. We design three different prompt creators $f_{prompt}$, the mapping from a set of roles to a prompt as follows:\n\\begin{enumerate}[leftmargin=*]\n    \\setlength{\\parskip}{0pt}\n   \\setlength{\\itemsep}{0pt plus 1pt}\n    \\item Manual Template: All roles are connected manually with natural language. We follow the template from~\\citet{li-etal-2021-document} for fair comparison.\n    \\item Soft Prompt: Following~\\citet{qin-eisner-2021-learning} and \\citet{liu2021gpt}, we connect different roles with learnable, role-specific pseudo tokens.\n    \\item Concatenation Template: To concatenate all role names belonging to one event type.\n\\end{enumerate}\n\n\\noindent{We give one example of these three types of prompt in Table~\\ref{tab: diff prompt} and list more examples in Appendix~\\ref{subsec: prompt example}. Further analysis can be found in Section~\\ref{subsec: prompt variants}.}\n\n\\begin{table*}[t]\n\\small\n    \\centering\n    \\begin{threeparttable}\n    \\begin{tabular}{c|c}\n    \\toprule\n    \\textbf{Prompt Type} & \\textbf{Prompt Example} \\\\\n    \\midrule\n    MA Template  & \\makecell{\\underline{Victor} ( and \\underline{Victor} ) defeated in \\underline{ConflictOrElection} at \\underline{Place}}( and \\underline{Place} ) \\\\\n    \\midrule\n    SF Prompt & \\makecell{<Vic\\_left0> \\underline{Victor} <Vic\\_right0> ( <Vic\\_left0> \\underline{Victor} <Vic\\_right0> ) \\\\ <Conf\\_left0> \\underline{ConflictOrElection} <Conf\\_right0> \\\\ <Place\\_left0> \\underline{Place} <Place\\_right0> ( <Place\\_left0> \\underline{Place} <Place\\_right0> )} \\\\\n    \\midrule\n    CA Template  & \\underline{Victor} ( \\underline{Victor} ) \\underline{ConflictOrElection} \\underline{Place} ( \\underline{Place} )\\\\\n    \\bottomrule\n    \\end{tabular}\n    \\end{threeparttable}\n    \\caption{Variants of prompt introduced in section~\\ref{subsec: joint prompt creation}.  \\textbf{MA}:Manual Template. \\textbf{SF}:Soft Prompt. \\textbf{CA}:Concatenation Template. Words with angle brackets in Soft Prompt denote role-specific pseudo tokens of continuous prompts. For multi-argument cases, we simply add slots within square brackets.}\n    \\label{tab: diff prompt}\n\\end{table*}\n\n\\subsection{Role-specific Selector Generation}\nGiven context $X$ and prompt $Pt$, this module generates the role-specific span selector $\\theta_k$, for each slot $k$ of the prompt. Here we choose $\\Lagr$ as BART~\\cite{lewis-etal-2020-bart}, a standard Transformer-based pre-trained language model consisting both an \\textbf{Encoder} and a \\textbf{Decoder}: $\\Lagr=[\\Lagr_{enc}, \\Lagr_{dec}]$.\n\nWe first define text markers $\\textbf{\\textlangle t\\textrangle}$ and $\\textbf{\\textlangle /t\\textrangle}$ as special tokens then insert them into context $X$ before and after the trigger word respectively.\n$$\\tilde{X} = [x_1, x_2, ...,  \\textbf{\\textlangle t\\textrangle}, x_{trig}, \\textbf{\\textlangle /t\\textrangle}, ... , x_n]$$\n\nInstead of concatenating the processed context $\\tilde{X}$ and prompt $Pt$ directly, we feed the context into BART-Encoder and the prompt into BART-Decoder separately, as illustrated in Fig.~\\ref{fig: model framework}. The prompt and context would interact with each other at the cross-attention layers in the decoder module.\n\n\\begin{equation}\n\\begin{aligned}[c]\n    &H_X^{(enc)} = \\Lagr_{enc}(\\tilde{X}) \\\\\n    &H_X = \\Lagr_{dec}(H_X^{(enc)}; H_X^{(enc)}) \\\\\n    &H_{pt} = \\Lagr_{dec}(Pt; H_X^{(enc)}) \\\\\n\\end{aligned}\n\\end{equation}\n\n\\noindent{where $H_X$ denotes the event-oriented context representation and $H_{pt}$ denotes context-oriented prompt representation. For $k$-th slot in the joint prompt we mean-pool its corresponding representations from $h_{pt}$ and obtain role feature $\\psi_{k} \\in R^h$, where $h$ denotes the dimension of hidden layer in BART. Note that a role may have multiple slots and, correspondingly, multiple role features and span selectors.}\n\nWe adopt a simple but effective modification on previous QA-based methods by deriving \\textbf{role-specific span selector} $\\theta_{k}$ from every role feature in the prompt. Given role feature $\\psi_{k}$, we have:\n\\begin{align*} \n\\psi_{k}^{(start)}& = \\psi_{k} \\circ w^{(start)} \\in R^h \\\\ \n\\psi_{k}^{(end)} &= \\psi_{k} \\circ w^{(end)} \\in R^h\n\\numberthis\n\\end{align*}\n\n\\noindent{where $\\theta = [w^{(start)}; w^{(end)}] \\in R^{h \\times 2}$ is learnable parameters shared among all roles, and $\\circ$ represents element-wise multiplication. $\\theta_{k} = [\\psi_{k}^{(start)}; \\psi_{k}^{(end)}]$ is exactly the span selector for $k$-th slot in the prompt. With only one meta-head $\\theta$ and simple operations, our method enables to generate arbitrary number of role-specific span selectors to extract related arguments from context. Recall the generation process of role feature $\\psi_{k}$ from prompt $h_{pt}$, it is obvious that both the interaction among different roles and the information aggregation between context and roles are considered under this paradigm.}\n\n\\subsection{Learning with Prompted Span Selector}\nGiven context representation $H_X$ and a set of span selectors $\\{\\theta_k\\}$, each $\\theta_k$ aims to extract at most one corresponding argument span $(s_k, e_k)$ from $H_X$. For $\\theta_k$ relating to one argument $a_k = \\tilde{X}_{i:j}$, where $i$ and $j$ are the start and end word indices in context, the selector is expected to output $(\\hat{s}_k, \\hat{e}_k) = (i, j)$ as prediction. And for $\\theta_k$ relating to no argument (when context has no argument about this role, or the slot number of this role exceeds the argument number), it is expected to output $(\\hat{s}_k, \\hat{e}_k) = (0, 0)$ representing an empty argument $\\epsilon$.\n\nWe first follow the extractive prompt formulation in Section~\\ref{subsec: formulation} to calculate the distribution of each token being selected as the start/end of the argument for each role feature.\n\\begin{align*} \n\\text{logit}_{k}^{(start)} &= \\psi_{k}^{(start)} H_X \\in R^L \\\\ \n\\text{logit}_{k}^{(end)} &= \\psi_{k}^{(end)} H_X \\in R^L \\numberthis\n\\label{eq: logit}\n\\end{align*}\nwhere $\\text{logit}_{k}^{(start)}$ and $\\text{logit}_{k}^{(end)}$ represent start and end position distributions over the context tokens for each slot $k$, and $L$ denotes the context length.\n\nThen we calculate probabilities where the start/end positions locate:\n\\begin{align*}\n    p_k^{(start)} &= \\text{Softmax}(\\text{logit}_{k}^{(start)}) \\in R^L \\\\\n    p_k^{(end)} &= \\text{Softmax}(\\text{logit}_{k}^{(end)}) \\in R^L \n\\numberthis\n\\label{eq: softmax without bipartite}\n\\end{align*}\nand define the loss function as:\n\\begin{align*}\n \\Lagr_{k}(X) & = - (\\log p_{k}^{(start)}(s_{k}) + \\log p_{k}^{(end)}(e_{k})) \\\\\n \\Lagr & = \\sum_{X \\in D}\\sum_{k} \\Lagr_{k}(X)\n\\label{eq: loss}\n\\numberthis\n\\end{align*}\nwhere $D$ ranges over all context in dataset and $k$ ranges over all slots in prompt for $X$.\n\n\\noindent\\textbf{Bipartite Matching}\nWe optionally introduce bipartite matching to deal with multiple arguments of the same role for finding the global-optimal assignments with the least-cost match. Since we insert multiple slots about this role and each slot generates one prediction, it is a canonical bipartite matching problem that matches local-optimal predictions (of each slot) and ground truth as much as possible. Following~\\citet{Carion-etal-2020-detr, yang-etal-2021-document}, we use Hungarian algorithm~\\cite{kuhn1955hungarian} and leave the detail about it in Appendix~\\ref{ap: bipartite matching}.\n\n\\subsection{Inference}\nFor inference, we define the set of candidate spans for event arguments as $\\mathcal{C}=\\{(i, j)|(i,j) \\in L^2, 0<j-i \\leq l\\} \\cup \\{(0, 0)\\}$. It contains all spans shorter than the threshold $l$ \\textbf{and} special span $(0, 0)$ indicating no arguments. Our model extracts the argument of each span selector $\\theta_k$ by enumerating and scoring all candidate spans as:\n\\begin{equation}\n \\text{score}_k(i, j) = \\text{logit}_{k}^{(start)}(i) + \\text{logit}_{k}^{(end)}(j)\n\\end{equation}\nand the predicted span of slot $k$ is given by:\n\\begin{equation}\n    (\\hat{s_k}, \\hat{e_k}) = \\argmaxA_{(i, j) \\in \\mathcal{C}} \\text{score}_k(i, j)\n\\end{equation}\nSince at most one span is predicted by each slot in the prompt, this strategy avoids the exhaustive threshold tuning.\\section{Experiments}\n\nIn this section, we explore the following questions:\n\n\\begin{itemize}[leftmargin=*]\n    \\setlength{\\parskip}{0pt}\n   \\setlength{\\itemsep}{0pt plus 1pt}\n    \\item Can PAIE better utilize PLMs for joint extraction to boost the performance of S-EAE and D-EAE?\n    \\item How do different prompt training strategies affect the results?\n    \\item How does PAIE perform in various practical settings, including efficiency and generalization to few-shot, long-distance, and multiple arguments?\n\\end{itemize}\n\n\\begin{table*}\n\\small\n    \\centering\n    \\begin{threeparttable}\n    \\begin{tabular}{A A | B B B B B B B}\n    \\toprule\n    \\multirow{2}{*}{\\textbf{Model}} & \\multirow{2}{*}{\\textbf{PLM}}  & \\multicolumn{2}{c}{\\textbf{ACE05}} &  \\multicolumn{2}{c}{\\textbf{RAMS}} & \\multicolumn{3}{c}{\\textbf{WIKIEVENTS}} \\\\\n    & & \\textbf{Arg-I} & \\textbf{Arg-C} & \\textbf{Arg-I} & \\textbf{Arg-C}  & \\textbf{Arg-I} & \\textbf{Arg-C} & \\textbf{Head-C}\\\\\n    \\midrule\n    FEAE \\small{~\\cite{wei-etal-2021-trigger}} & BERT-b & - & - &  53.5\\rlap{*} & 47.4\\rlap{*} & - & - & -\\\\\n    DocMRC \\small{~\\cite{liu-etal-2021-machine}} & BERT-b & - & - &  - & 45.7\\rlap{*} &  - & 43.3\\rlap{*} & -\\\\\n    \\multirow{2}{*}{OneIE\\small{~\\cite{lin-etal-2020-joint}}} & BERT-b & 65.9 & 59.2 &  - & - &  - & - & -\\\\\n     & BERT-l & 73.2 & 69.3 &  - & - &  - & - & -\\\\\n    \\multirow{2}{*}{EEQA \\small{~\\cite{du-cardie-2020-event}}} & BERT-b & 68.2\\rlap{*} & 65.4\\rlap{*} & 46.4  &44.0 &54.3   &53.2  &56.9 \\\\\n    & BERT-l &70.5  &68.9  & 48.7  &46.7  &56.9  &54.5 & 59.3 \\\\\n    \\multirow{2}{*}{BART-Gen\\small{~\\cite{li-etal-2021-document}}} & BART-b & 59.6 & 55.0 &  50.9 & 44.9 & 47.5 & 41.7  & 44.2\\\\\n    & BART-l & 69.9\\rlap{*} & 66.7\\rlap{*} &  51.2 & 47.1 & 66.8 &62.4  & 65.4\\\\\n    \\multirow{2}{*}{EEQA-BART \\small{(Our implementation)}} & BART-b & 69.6 & 67.7 &49.4 & 46.3 & 60.3 & 57.1 & 61.4\\\\\n    & BART-l & 73.1 & \\underline{72.2} &  51.7 & 48.7 & 61.6  & 57.4 & 61.3\\\\\n    \\midrule\n    \\multirow{2}{*}{PAIE \\small{(Ours)}} & BART-b & \\underline{73.6} & 69.8 & \\underline{54.7} & \\underline{49.5} & \\underline{68.9} & \\underline{63.4} & \\underline{66.5} \\\\\n    & BART-l & \\textbf{75.7} & \\textbf{72.7} &  \\textbf{56.8} & \\textbf{52.2} &  \\textbf{70.5} & \\textbf{65.3} & \\textbf{68.4} \\\\\n    \\bottomrule\n  \\end{tabular}\n  \\caption{Overall performance. We highlight the best result and underline the second best. * means the value from the original paper. \\textbf{b} in column \\textbf{PLM} denotes base model and \\textbf{l} denotes large model.}\n  \\label{tab: main results}\n  \\end{threeparttable}\n\\end{table*}\n\n\\subsection{Experimental Setup}\n\\paragraph{Datasets}\nWe conduct experiments on three common datasets in Event Argument Extraction task: RAMS~\\cite{ebner-etal-2020-multi}, WIKIEVENTS~\\cite{li-etal-2021-document} and ACE05~\\cite{doddington-etal-2004-automatic}. RAMS and WIKIEVENTS are latest document-level EAE benchmarks, while ACE05 is a classical dataset commonly used for sentence-level EAE task. We leave the dataset details in Appendix~\\ref{ap: dataset}.\n\n\\paragraph{Evaluation Metric}\nWe adopt two evaluation metrics. \n(1) Argument Identification F1 score (Arg-I): an event argument is correctly identified if its offsets and event type match those of any of the argument mentions. \n(2) Argument Classification F1 score (Arg-C): an event argument is correctly classified if its role type is also correct. For WIKIEVENTS dataset, we follow \\cite{li-etal-2021-document} and additionally evaluate Argument Head F1 score (Head-C), which only concerns the matching of the headword of an argument.\n\n\\paragraph{Implementation Details} \nPlease refer to Appendix~\\ref{ap: implementation} for implementation details of PAIE.\n\n\\paragraph{Baselines}\nWe compare PAIE with several state-of-the-art models in three categories:\n(1) Multi-label classification model: \\textbf{ONEIE}~\\cite{lin-etal-2020-joint} \n(2) Generation model: \\textbf{BART-Gen}~\\cite{li-etal-2021-document}\n(3) QA-based model: \\textbf{EEQA}~\\cite{du-cardie-2020-event},  \\textbf{DocMRC}~\\cite{liu-etal-2021-machine} and \\textbf{FEAE}~\\cite{wei-etal-2021-trigger}.\nFor a fair comparison, we replace the PLMs used in the strongest baseline EEQA with BART, the same with PAIE, namely \\textbf{EEQA-BART}. \nMore details of baselines are listed in Appendix~\\ref{ap: baselines}.\n\n\\begin{table*}\n \\centering\n  \\small\n    \\begin{threeparttable}\n        \\begin{tabular}{A|BBBB|BBB} \n        \\toprule\n        \\textbf{Model} & \\multirow{2}{*}{\\textbf{\\shortstack{Bipartite\\\\Matching}}} & \\multirow{2}{*}{\\textbf{\\shortstack{Multi-arg\\\\Prompt}}} & \\multirow{2}{*}{\\textbf{\\shortstack{Role-specific\\\\Selector}}} & \\multirow{2}{*}{\\textbf{PLM}} & \\multicolumn{3}{c}{\\textbf{Arg-C}}  \\\\\n        & & & & & \\textbf{ACE05} & \\textbf{RAMS} & \\textbf{WIKI} \\\\\n        \\midrule\n        \\textbf{PAIE} & \\cmark & \\cmark & \\cmark & BART-b &  $69.8${\\tiny$\\pm0.98$} &  $49.5${\\tiny$\\pm0.65$} &  $63.4${\\tiny$\\pm1.17$} \\\\\n        \\midrule\n        \\textbf{PAIE\\_\\tiny w/o bipartite} & \\xmark & \\cmark & \\cmark & BART-b & $68.9${\\tiny $\\pm{1.03}$} & $49.4${\\tiny$\\pm0.98$} & $62.4${\\tiny $\\pm1.09$} \\\\\n         \\textbf{PAIE\\_\\tiny w/o multi-prompt} & \\xmark & \\xmark & \\cmark & BART-b &  $66.9${\\tiny $\\pm0.61$} &  $47.6${\\tiny $\\pm1.20$}&  $59.9${\\tiny $\\pm1.26$} \\\\\n         \\textbf{EEQA-BART} & \\xmark & \\xmark & \\xmark & BART-b &  $67.7${\\tiny $\\pm0.64$} &  $46.3${\\tiny $\\pm0.77$}&  $57.1${\\tiny $\\pm0.82$} \\\\\n        \\midrule\n         \\textbf{EEQA} & \\xmark & \\xmark & \\xmark & BERT-b  & $65.4$ & $44.0$ &  $53.2$\\\\\n        \\bottomrule\n        \\end{tabular}\n    \\end{threeparttable}\n    \\caption{Ablation study on three benchmarks. WIKIEVENTS is abbreviated as WIKI (the same below). \n    }\n    \\label{Ablation study}\n\\end{table*}\n\n\\subsection{Overall Performance}\nTable \\ref{tab: main results} compares our approach with all baselines. We observe that PAIE performs best on all datasets. For S-EAE, our base model achieves an absolute Arg-C improvement of $2.1\\%$ on ACE05. For D-EAE, our base model obtains $2.1\\%$ and $6.3\\%$ Arg-C gains on RAMS and WIKIEVENTS, respectively. Similarly, our large-version model achieves $3.5\\%$ and $2.9\\%$ gains. This demonstrates a good generalization ability of our proposed method on dealing with varying lengths of context.\n\nWe also find that QA-based model sometimes performs well even in document-level EAE tasks. The EEQA-BART model shows almost the same Arg-C with BART-Gen~\\cite{li-etal-2021-document} on RAMS dataset. Other QA-based models (especially those considering interactions among arguments, like FEAE~\\cite{wei-etal-2021-trigger}) also have competitive performance. As for WIKIEVENTS, however, QA-based models are inferior to sequential-generation models significantly. We speculate that the performance of previous QA-based models is not robust to handle longer text.\nBoth BART-Gen~\\cite{li-etal-2021-document} and our model PAIE have a relatively stable performance on various document-level EAE datasets, but our model performs better, especially with smaller PLMs. \n\nNext, we conduct further analysis with the strongest baseline EEQA-BART and our PAIE. We use the base-version BART for a fair comparison.\n\n\\subsection{Ablation Study}\n\\label{subsec: ablation study}\n\nIn this section, we investigate the effectiveness of our main components by removing each module in turn. (1) \\textbf{bipartite matching}. We drop out of the bipartite matching loss and ignore the global optimal span assignment. (2) \\textbf{multi-arg prompt}. We additionally replace the prompt containing multiple roles with several single templates in which include only one role. (3) \\textbf{role-specific selector}. The selector is not role-specific anymore but is shared among all roles. This variant degrades to EEQA-BART.\n\nWe summarize the results of ablation studies in Table~\\ref{Ablation study}. \\textbf{(1)} EEQA-BART outperforms EEQA significantly, which demonstrates that even conventional QA-based methods have substantial space for improvement with a better PLM and span selection strategy. \\textbf{(2)} The role-specific selector further improves Arg-C scores in RAMS and WIKIEVENTS, while taking a slightly negative effect on ACE05. Since the former two datasets are document-level and have more role types (65 in RAMS, 59 in WIKIEVENTS, and 36 in ACE05), we speculate that role-specific selector plays a critical role when identifying and disambiguating roles with complicated ontology structures in long documents.  \\textbf{(3)} Joint multi-argument prompt achieves consistent improvement on all three datasets. It indicates that the joint prompt has the potential to capture implicit interaction among arguments. \\textbf{(4)} Bipartite matching loss has an average improvement of $0.7\\%$ on three benchmarks. We conjectured it is due to the permutation-invariance property of bipartite matching and discuss further in Appendix \\ref{sec: bipartite discuss}.\\section{Evaluation of Extractive Prompting}\n\n\\begin{table}[t]\n \\centering\n \\small\n    \\begin{threeparttable}\n    \\begin{tabular}{B B | B B B}\n        \\toprule\n        \n        \\textbf{Variant} & \\textbf{PLM} & \\textbf{ACE05} & \\textbf{RAMS} & \\textbf{WIKI}  \\\\\n        \\midrule\n        \\multirow{3}{*}{\\textbf{PAIEE}} & BE-b & 65.9 & 46.3 & 62.9 \\\\\n         & BA-b & 70.2 & 49.3 & 62.8 \\\\\n         & BA-l & \\underline{72.3} & \\underline{51.7} & \\underline{65.1} \\\\\n        \\midrule\n        \\multirow{2}{*}{\\textbf{PAIE}} & BA-b & 69.8 & 49.5 & 63.4 \\\\\n         & BA-l &  \\textbf{72.7} & \\textbf{52.2} & \\textbf{65.3} \\\\\n        \\bottomrule\n    \\end{tabular}\n    \\end{threeparttable}\n    \\caption{Arg-C F1 of different PLMs. BE and BA denote BERT and BART. Note that we also try PLM with only encoder such as BERT under PAIEE setting, which does not require a decoder.}\n    \\label{tab: model variation}\n\\end{table}\n\n\\subsection{Architecture Variants}\n\\label{sec: model arch}\nPAIE feeds the context into BART-Encoder and the prompt into BART-Decoder respectively. A plausible and straightforward variant called \\textbf{PAIEE} (PAIE-Encoder) concatenates context and prompt, then feed them into encoder directly. We investigate the performance of PAIEE compared with PAIE in this section, as shown in Table~\\ref{tab: model variation}.\n\nWe can see that concatenating context and prompt slightly impairs the model performance. It seemingly indicates that the over-interaction between context and prompt is not of benefit. Furthermore, the prompt squeezes the limited input length of the encoder kept for a document if it concatenates with the document. The experiments support our strategy feeding context and prompt separately without concatenation to PAIE. \n\n\\subsection{Prompt Variants}\n\\label{subsec: prompt variants}\n\n\\begin{figure*}[htbp]\n\\centering\n\\subfigure[ACE05]{\n\\begin{minipage}[t]{0.33\\linewidth}\n\\centering\n\\includegraphics[width=\\linewidth]{ace_prompt.pdf}\n\\end{minipage}%\n}%\n\\subfigure[RAMS]{\n\\begin{minipage}[t]{0.33\\linewidth}\n\\centering\n\\includegraphics[width=\\linewidth]{rams_prompt.pdf}\n\\end{minipage}%\n}%\n\\subfigure[WIKIEVENTS]{\n\\begin{minipage}[t]{0.33\\linewidth}\n\\centering\n\\includegraphics[width=\\linewidth]{wikievents_prompt.pdf}\n\\end{minipage}\n}%\n\\caption{Arg-C F1 using three different types of joint prompts in Table~\\ref{tab: diff prompt} plus the single template on three benchmarks. \\textbf{MA}: Manual Template. \\textbf{SF}: Soft Prompt. \\textbf{CA}: Concatenate Template. \\textbf{single}: Single Template.}\n\\label{fig:prompt}\n\\end{figure*}\n\nWe investigate how different types of prompts affect the performance in this section, as shown in Fig.~\\ref{fig:prompt}. We compare four different prompts: three joint prompts introduced in Section \\ref{subsec: joint prompt creation} and one single template containing only one role slot, i.e. the question template used in QA-based method.\n\nWe find that \\textbf{(1)} All three joint prompts outperform the single template, which validates the effectiveness of the joint prompt. \\textbf{(2)} Manual template has the most stable performance and usually the better result than others. \\textbf{(3)} Soft prompt achieves comparable result with a manual template. We claim this observation inspiring because the creation of the manual template is laborious and soft prompts almost avoid such a handcrafted process. It also accords with current trends of creating distinct continuous prompts, which usually perform better than manual ones. \\textbf{(4)} Concatenation template performs worst among joint prompts. We conjecture it is due to such prompt neither contains prior knowledge about role interaction (manual template) nor learns such interaction during training (soft prompt). \\section{Analysis on Real Scenario}\n\\subsection{Long-range Dependencies}\n\n\\begin{table}\n\\centering\n\\small\n    \\begin{threeparttable}\n        \\begin{tabular}{D|DDDDD}\n        \\toprule\n        \\multirow{2}{*}{\\textbf{Model}} & \\multicolumn{5}{c}{\\textbf{Trigger-Argument Distance $d$}} \\\\\n        & $\\mathbf{-2_{[79]}}$ & $\\mathbf{-1_{[164]}}$ & $\\mathbf{0_{[1811]}}$ & $\\mathbf{1_{[87]}}$ & $\\mathbf{2_{[47]}}$ \\\\\n        \\midrule\n        \\makecell{ \\textbf{BART-Gen}} & 17.7 & 16.8 & 44.8 & 16.6 & 9.0 \\\\\n        \\makecell{ \\textbf{DocMRC}} & 21.0 & 20.3 & 46.6 & 17.2 & 12.2 \\\\\n        \\makecell{\\textbf{FEAE}} & \\textbf{23.7} & 19.3 & 49.2 & 25.0 & 5.4 \\\\\n        \\makecell{\\textbf{EEQA-BART}} & 15.6 & \\underline{24.0} & 51.7 & 23.5 & 8.0 \\\\\n        \\midrule\n        {\\textbf{PAIE\\_\\tiny w/o multi-prompt}}  & 21.2 & 21.4 & \\underline{52.3} & \\underline{27.9} & \\underline{24.6} \\\\\n        {\\textbf{PAIE}}  & \\underline{21.7} & \\textbf{27.3} & \\textbf{54.7} & \\textbf{29.4} & \\textbf{25.4} \\\\\n        \\bottomrule\n        \\end{tabular}\n    \\end{threeparttable}\n\\caption{Performance (Arg-C F1 score) breakdown by argument-trigger distance $d$ on RAMS development set. The argument number of each case is given in the bracket. \n}\n\\label{tab: dist}\n\\end{table}\n\nIn D-EAE task, arguments could span multiple sentences. Therefore, the model is required to capture long-range dependencies. For better evaluating PAIE and comparing with others, we list their performance breakdown on different sentence distances between arguments and the given trigger word in Table~\\ref{tab: dist}. \nWe can see that \\textbf{(1)} PAIE significantly improves the ability to extract arguments with long distances, especially for those behind the trigger words (see columns with positive $d$ values). \\textbf{(2)} The last two rows of the table indicate that joint prompts in PAIE leverage the implicit interaction among roles, and roles conditioning on each other lower the difficulty to extract long-distance arguments effectively.\n\n\\subsection{Same-role Argument Assignment}\n\\label{subsec:same-role arg}\nMultiple arguments may share the same role in the same event. We show that PAIE outperforms QA-based models dealing with it in both efficiency and effectiveness in this section.\n\n\\noindent{\\textbf{Efficiency}} To solve this problem, QA-based methods usually adopt the thresholding strategy, which compares the score of each text span with a manually tuned threshold. We claim that it consumes lots of time and computational resources for finding a good threshold and usually ends with sub-optimal results. We support such claim by a coarse grid search tuning span threshold on WIKIEVENTS dataset using EEQA and EEQA-BART models, as shown in Fig.~\\ref{th_tuning}. The choice of threshold highly affects the performance of the model. In addition, models with the same architecture but different PLMs have totally different optimal thresholds even on the same dataset, not to mention on distinct datasets. PAIE requires no threshold tuning since each slot in the prompt only predicts at most one argument span and usually achieves much higher inference speed in practice.   \n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=\\linewidth]{wikievents_th.pdf}\n\\caption{Arg-C F1 w.r.t different thresholds for WIKIEVENTS. We draw the performance of PAIE in red dashed line for comparison (no threshold tuning).\n}\n\\label{th_tuning}\n\\end{figure}\n\n\\begin{figure*}[htbp]\n\\centering\n\\subfigure[ACE05]{\n\\begin{minipage}[t]{0.33\\linewidth}\n\\centering\n\\includegraphics[width=\\linewidth]{ace_few_shot.pdf}\n\\end{minipage}%\n}%\n\\subfigure[RAMS]{\n\\begin{minipage}[t]{0.33\\linewidth}\n\\centering\n\\includegraphics[width=\\linewidth]{rams_few_shot.pdf}\n\\end{minipage}%\n}%\n\\subfigure[WIKIEVENTS]{\n\\begin{minipage}[t]{0.33\\linewidth}\n\\centering\n\\includegraphics[width=\\linewidth]{wikievents_few_shot.pdf}\n\\end{minipage}\n}%\n\\caption{Arg-C F1 scores w.r.t different training data ratio on three benchmarks.}\n\\label{fig:fewshot}\n\\end{figure*}\n\n\\noindent{\\textbf{Effectiveness}} We also compare the capability of PAIE and EEQA-BART in predicting multiple arguments with the same role on WIKIEVENTS, a dataset containing diverse multi-argument cases. Table~\\ref{tab: argnum} shows that PAIE outperforms significantly better than EEQA-BART dealing with such cases. For roles with three and four or more arguments, PAIE gains a definite Arg-C F1 improvement of $9.5\\%$ and $26.4\\%$, respectively.\n\n\\begin{table}\n\\centering\n\\small\n    \\begin{threeparttable}\n        \\begin{tabular}{B| B B B B}\n        \\toprule\n        \\multirow{2}{*}{\\textbf{Model}} & \\multicolumn{4}{c}{\\textbf{WIKIEVENTS Argument Number $n$}} \\\\\n        & $\\mathbf{1_{[468]}}$ & $\\mathbf{2_{[66]}}$ & $\\mathbf{3_{[15]}}$ & $\\mathbf{\\geq4_{[17]}}$ \\\\\n        \\midrule\n        \\textbf{EEQA-BART} &  $58.0_{(-16)}$& $59.7_{(-3)}$ & $28.6_{(-10)}$ & $10.0_{(-26)}$  \\\\\n        \\textbf{PAIE} & \\textbf{74.1} & \\textbf{62.6} & \\textbf{38.1} & \\textbf{36.4} \\\\\n        \\bottomrule\n        \\end{tabular}\n    \\end{threeparttable}\n\\caption{Arg-C F1 on WIKIEVENTS breakdown by argument number $n$ of one role. The case number is given in the square bracket. }\n\\label{tab: argnum}\n\\end{table}\n\n\\subsection{Few-shot Setting}\nWe analyze how PAIE performs under a scenario without sufficient annotations. Fig.~\\ref{fig:fewshot} shows the performance of PAIE and two other QA-based baselines with partial training samples on three benchmarks. It demonstrates that \\textbf{(1)} PAIE is superior to EEQA-BART and EEQA in almost all settings with different datasets and training data ratios. \\textbf{(2)} PAIE especially outperforms QA-based methods in document-level tasks (RAMS and WIKIEVENTS). It achieves comparable F1 scores with EEQA-BART using only about $20\\%$ training samples and EEQA using about $10\\%$ samples. \\textbf{(3)} Along with the decreasing number of training data, the gains become larger than baselines. All observations above indicate that PAIE can better utilize PLMs for few-shot settings.\n\n\\subsection{Inference Speed}\n\\label{sec: inference speed}\nMost of the previous sections emphasize the superiority of PAIE from the perspective of accuracy performance. Actually, PAIE also has much better extraction efficiency compared with other approaches.\n\n\\begin{table}[t]\n    \\centering\n    \\small\n    \\begin{threeparttable}\n    \\begin{tabular}{B|BBBBBB}\n        \\toprule\n         \\multirow{2}{*}{\\textbf{Model}} & \\multicolumn{2}{c}{\\textbf{ACE05}} & \\multicolumn{2}{c}{\\textbf{RAMS}} & \\multicolumn{2}{c}{\\textbf{WIKI}} \\\\\n         & B & L & B & L & B & L \\\\\n         \\midrule\n         \\textbf{BART-Gen} & 5.8 & 12.4 & 33.2 & 54.8 & 19.1 & 29.0 \\\\\n         \\textbf{EEQA-BART} & 11.8 & 36.0 & 66.0 & 187.4 & 30.9 & 83.8 \\\\\n         \\midrule\n         \\textbf{PAIE} & \\textbf{2.9} & \\textbf{8.4} & \\textbf{19.0} & \\textbf{38.6} & \\textbf{8.4} & \\textbf{18.3} \\\\\n         \\bottomrule\n    \\end{tabular}\n    \\end{threeparttable}\n    \\caption{Inference time (second) for different models on test set of ACE05, RAMS, WIKIEVENTS. Experiments are run on one same NVIDIA-1080Ti GPU.\n    }\n    \\label{tab: speed}\n\\end{table}\n\nIn Table~\\ref{tab: speed}, we report the overall inference time for different models. PAIE usually runs 3-4 times faster than EEQA, since it predicts multiple roles simultaneously, while EEQA predicts roles one by one. Other QA-based models are likely to have similar speeds with EEQA due to their sequential prediction structure and training process. Also, as discussed in Section~\\ref{subsec:same-role arg}, PAIE is even more advantageous under practical application scenarios since it avoids the heavy threshold tuning.\n\n\\section{Conclusion}\nWe propose a novel model PAIE that effectively and efficiently extracts arguments at both sentence and document levels. We define a new prompt tuning paradigm for extraction tasks, which prompts multiple role knowledge from PLMs via role-specific selectors and joint prompts. Extensive experiments on three standard benchmarks demonstrate our proposed model's effectiveness and the generalization ability in both sentence and document level EAE. We have also conducted ablation studies on the main components, the extractive prompting strategy, and several real scenarios. In the future, we are interested in investigating co-reference as an auxiliary task of EAE and introducing entity information to better determine argument boundaries.\n\\section*{Acknowledgments}\nThis study is supported under the RIE2020 Industry Alignment Fund – Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s). We also thank the KULeuven C1 project Macchina for support.\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{Event Extraction as Question Generation and Answering}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\n\nRecent work on Event Extraction has reframed the task as  Question Answering (QA), with promising results.\\joel{Di - is it safe to say that most EE methods use QA and these are the most performant?}\\di{it might not be safe to claim that. some other generation-based methods and some graph-based method have comparable performance.}\nThe advantage of this approach is that it addresses the error propagation issue found in traditional token-based classification approaches by directly predicting event arguments without extracting candidates first. However, the questions are typically based on fixed templates and they rarely leverage contextual information such as relevant arguments. In addition, prior QA-based approaches have difficulty handling cases where there are multiple arguments for the same role. In this paper, we propose QGA-EE, which enables a Question Generation (QG) model to generate questions that incorporate rich contextual information instead of using fixed templates. We also propose dynamic templates to assist the training of QG model.\nExperiments show that QGA-EE outperforms all prior single-task-based models on the ACE05 English dataset.\\footnote{Our code is available at \\url{https://github.com/dataminr-ai/Event-Extraction-as-Question-Generation-and-Answering} for research purposes.}\n\n\\end{abstract}\n\nEvent Extraction (EE) aims to extract core information elements (e.g. who, what, where, when) from text, and is a very important task in Natural Language Processing (NLP). It provides inputs to downstream applications such as Summarization~\\cite{filatova-hatzivassiloglou-2004-event}, Knowledge Base Population~\\cite{ji2011knowledge}, and Recommendation~\\cite{lu2016cross}. \n\n\\begin{figure}[ht]\n    \\centering\n    \\includegraphics[width=0.9\\linewidth]{fig/qga-ee.pdf}\n    \\caption{An event mention example from ACE. An ACE \\texttt{Conflict.Attack} event with \\textit{pummeled} as trigger word and three event arguments: \\textit{coalition} (\\texttt{Attacker}),  \\textit{jets} (\\texttt{Instrument}) and \\textit{hills} (\\texttt{Place}).}\n    \\label{fig:ace_intro}\n\\end{figure}\n\nPrevious work~\\cite{li2013joint,nguyen2016joint,sha2018jointly} is typically based on a pipeline approach, which first identifies the event trigger word/phrase and argument candidates, and then applies a classifier to the pair-wise features to classify the roles of the candidates.  Unfortunately, errors tend to propagate down the pipeline.\nRecently, some approaches have formulated EE as a Question Answering (QA) problem~\\cite{du2020event,li2020event,lyu2021zero}\\joel{you mention some approaches but then just cite one paper.  good idea to list more}\\di{added more EE-QA paper} to mitigate the issue, in which questions for each argument role are manually defined by templates. \nFor example, extracting the \\texttt{Attack} argument from the \\texttt{Conflict.Attack} event in the sentence \\textit{`That's because coalition fighter jets pummeled this Iraqi position on the hills  above Chamchamal and Iraqi troops made a hasty retreat.'} is reframed as answering the question \\textit{`Who was the attacking agent?'}\\joel{what does it mean to convert to a task?  it sounds weird}\\di{rewrote it} \nThese approaches have shown promising results, but template-based questions are limiting: since the templates are built manually, they are fixed and rarely include contextual information (i.e., specific to the inputs), except for trigger words in some work~\\cite{du2020event}. Formulating good questions, however, has been shown to improve performance for standard QA tasks~\\cite{rajpurkar2018know}. \nFor QA-based EE, a question that incorporates richer contextual information such as other event arguments could yield better results (e.g.  \\textit{`Who used \\underline{jets} in the attack in \\underline{hills}?'} in Figure~\\ref{fig:ace_intro}). \n\\joel{do you mean to say \"could produce better\"?} \n\nIn this paper, we propose QGA-EE, which consists of 1) a QG model for generating a context-aware question conditioned on a target argument role and 2) a QA model for answering the context-aware question to extract the event argument. We also design dynamic templates to generate the gold context-aware questions for QG model training. \n\nTo the best of our knowledge, this is the first QA-based EE work that utilizes dynamic templates and focuses on generating context-aware questions.\n\\citet{li2020event} also propose a model to generate questions that incorporate contextual information for both event trigger and arguments. However, our work has two main advantages. First, in \\citet{li2020event} the question only incorporates the contextual information at the ontology level (e.g. argument role, event type). In our work, the generated questions incorporate contextual information at an event mention-level. For example, the question generated by our model includes the real event argument rather than just the argument role (e.g. ‘hills’ vs ‘Place’). Second, the questions in their work are generated by filling in the templates, but our templates are dynamic and used to train the QG model which can automatically generate the optimal question given a specific event mention and the concerned argument role.\n\nExperimental results show that QGA-EE outperforms all of the single-task-based models on the Automatic Content Extraction (ACE) 2005 English dataset~\\cite{doddington2004automatic} and even achieves competitive performance with state-of-the-art joint IE models.\n\n     \n\n     \n    \n    \n\n\\section{Model}\nFigure~\\ref{fig:ace_intro} shows the overall framework of QGA-EE.\nIt focuses on Event Argument Extraction (EAE) only, but can be paired with any event trigger tagger to perform end-to-end EE. In Section~\\ref{sec:results}, we pair it with a standard sequence labeling trigger tagger to evaluate its end-to-end EE performance. \n\n\\subsection{Question Generation Model}\n\nPrevious QA-based EE work~\\cite{du2020event} fills in pre-designed templates with trigger information to generate the input questions to the QA model. However missing contextual information in the questions is a bottleneck for the performance of the QA model.\n\nQGA-EE uses a QG model to generate context-aware questions conditioned on the input sentence and target role, which is based on a sequence-to-sequence architecture (e.g. BART\\cite{lewis2019bart}, T5\\cite{roberts2019exploring}). In order to train the QG model, we design \\textbf{Dynamic Templates} for each role in the ACE ontology.\\footnote{\\url{https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/english-events-guidelines-v5.4.3.pdf}} We design multiple templates for each role, and each of them includes different combinations of other argument roles.\n\n\\begin{table}[th]\n    \\centering\n    \\small\n    \\begin{tabular}{l}\n    \\hline\n        Who was the attacking agent? \\\\\n        Who attacked [\\texttt{Target}]?\\\\\n        Who used [\\texttt{Instrument}] in the attack?\\\\\n        Who made the attack in [\\texttt{Place}]?\\\\\n        Who attacked [\\texttt{Target}] using [\\texttt{Instrument}]?\\\\\n        Who attacked [\\texttt{Target}] in [\\texttt{Place}]?\\\\\n        Who used [\\texttt{Instrument}] in the attack in [\\texttt{Place}]?\\\\\n        Who attacked [\\texttt{Target}] using [\\texttt{Instrument}] in [\\texttt{Place}]? \\\\ \n        \\hline\n    \\end{tabular}\n    \\caption{Dynamic templates for \\texttt{Attacker} role in \\texttt{Conflict.Attack} event with different combinations of known argument roles based on ACE ontology.}\n    \\label{tab:templates}\n\\end{table}\n\nFor example, the \\texttt{Conflict.Attack} event in ACE has four predefined argument roles: \\texttt{Attacker}, \\texttt{Target}, \\texttt{Instrument} and \\texttt{Place}.\\footnote{We follow the experimental setting of prior work, which excludes all the \\texttt{Value} and \\texttt{Timex}. Thus the argument roles such as \\texttt{Time} are not included.}\nFor the \\texttt{Attacker} role, we exhaustively design eight templates using all of the possible combinations of the other roles included in the question (Table~\\ref{tab:templates}). \nWhen the model fills in the templates given a specific event mention, it is common that some of the predefined argument roles do not exist in the event mention. Thus the model only keeps the templates that contain the slots for argument roles appearing in the event mention.\nFor the example in Figure~\\ref{fig:ace_intro}, the \\texttt{Target} role is not mentioned. So we ignore all of the templates that contain the \\texttt{[Target]} slot, and we obtain four candidate questions for the \\texttt{Attacker} role with corresponding arguments filled in: (1)\\textit{Who was the attacking agent?} (2) \\textit{Who used jets in the attack?} (3) \\textit{Who made the attack in hills?} (4) \\textit{Who used jets in the attack in hills?}\n\nTo train a QG model to generate the questions that cover as many contextual information as possible, we use the question that contains the most contextual arguments as the ground truth. For the example in Figure~\\ref{fig:ace_intro}, we choose the question \\textit{`Who used jets in the attack in hills?'}, because it contains two arguments: \\textit{`jets'} and \\textit{`hills'}, the other three candidate questions listed above contain one or zero arguments. If more than one candidate question contains the most contextual arguments, we then pick the first one. The input and output examples for the QG model are as follows: \n\n\\noindent\\fbox{\n \\parbox{\\linewidth}{\n \\textbf{Input:} role: attacker context: That's because coalition fighter jets * pummeled * this Iraqi position on the hills above Chamchamal and Iraqi troops made a hasty retreat.\n \n \\textbf{Output:} Who used jets in the attack in hills?\n}\n }\n\n \n\n \n \n\n \n \n\n \n\\label{sec:qa_model}\nDifferent from prior QA-based EE work that adapt an encoder-only architecture and predict the offsets of the event arguments~\\cite{chen2019reading, du2020event,li2020event,liu2020event,feng2020probing,lyu2021zero,zhou2021role}, our QA model is based on a sequence-to-sequence architecture (e.g. BART, T5), and generates the answer string directly.\nThis enables prediction of multiple event arguments that are associated with the same role. \\citet{li2021document} also adapts a generation model, but the input template is fixed. The examples of input and output are as follows:\n\n\\noindent\\fbox{\n \\parbox{\\linewidth}{\n \\textbf{Input:} \\textit{question: Who was harmed in * injured * event? context: \\underline{Injured} Russian diplomats and a convoy of America's Kurdish comrades in arms were among unintended victims caught in crossfire and friendly fire Sunday.}\n \n \\textbf{Output:} \\textit{diplomats; convoy; victims $<$ /s $>$}\n}\n }\n \n\n \n \n \n \n\n \n\n\\noindent\\textbf{Post-processing}\nWe split the output into a list of candidates (by \\texttt{`;'}), and retrieve the arguments with offsets by exactly matching against the original sentence.\nWe dynamically change the start position for searching to preserve the order of the retrieved event arguments. \nIf an argument candidate cannot be matched with the original sentence, we discard it.\nUnlike the QG model, we use all of the possible questions as inputs during training for data augmentation purposes, and the size of the training data increases from 15,426 to 20,681.\\joel{what do those numbers refer to?}\\di{rewrote}\nBut in the testing phase, we use the single question generated by the QG model for each argument role.\n\\subsection{Dataset and Evaluation Metrics}\n\n        \n\n        \n   \n \n\nWe conduct the experiments on the ACE 2005 English corpora,\nwhich has 33 event types and 22 argument roles. It contains 599 documents collected from newswire, weblogs, broadcast conversations, and broadcast news.\nMore specifically, we follow the pre-processing steps in \\citet{wadden2019entity},\\footnote{\\url{https://github.com/dwadden/dygiepp}} and evaluate our models on the resulting ACE05-E dataset. \n\n For evaluation, we use the same criteria as prior work~\\cite{li2013joint}: An \\textbf{event trigger} is correctly identified if its offsets exactly match a reference. It is correctly classified if both its offsets and event type match a reference. An \\textbf{event argument} is correctly identified (Arg-I) if its offsets and event type match a reference in the ground truth. It is correctly classified (Arg-C) if all of its offsets, event type, and argument role match a reference.\n \n\\subsection{Compared Baselines}\n\\noindent\\textbf{Model Variants.} To evaluate the generalizability of our approach, we evaluate two QGA-EE variants: \\textbf{QGA-EE$_{BART}$} and \\textbf{QGA-EE$_{T5}$}, which use BART and T5 as backbones respectively.\n\nWe compare the proposed models against SOTA EE models. \\textbf{BERT QA}~\\cite{du2020event} use BERT as the encoder and predict the positions of the argument directly with role-driven questions.\n\\textbf{TANL}~\\cite{paolini2021structured} transfers input sentences into augmented natural language sentences for structured prediction. \\textbf{TEXT2EVENT}~\\cite{lu2021text2event} is a sequence-to-structure network for event extraction.\\footnote{DEGREE~\\cite{hsudegree} is not included because it is not evaluated on all of the argument roles.} \\citet{ma2020resource} utilizes dependency parses as additional features. \\textbf{BART-Gen}~\\cite{li2021document} is a BART-based generation model proposed for document-level event extraction.\n\nWe also compare with joint IE models trained on all of the ACE annotations which include entities, relations, and events. They benefit from additional information from other tasks and usually achieve better performance than the models trained on a single task. \nIt is not fair to directly compare our model with the joint models since they incorporate more information beyond the standard EE training sets, but we still list their scores as a reference. \\textbf{DYGIE++}~\\cite{wadden2019entity} is a BERT-based model that models span representations with within-sentence and cross-sentence context. \\textbf{ONEIE}~\\cite{lin2020joint} leverages global features. \\textbf{FourIE}~\\cite{van2021cross} and \\textbf{GraphIE}~\\cite{van2022joint} are Graph Convolutional Networks-based models and \\textbf{AMR-IE}~\\cite{zhang2021abstract} utilizes AMR~\\cite{banarescu2013abstract} parser.\n\n\\subsection{Implementation Details}\nWe conduct all of the experiments on a single V100 GPU. For finetuning, we use the Adafactor~\\cite{shazeer2018adafactor} optimizer with a learning rate of $1*10^{-4}$, weight decay of $1 * 10^{-5}$,  and clip threshold of $1.0$. We train the model for 20 epochs. Further details such as hyperparameters and data statics for model training and evaluation are in Appendix~\\ref{app:implement}.\n\n        \n\n        \n    \n\n         \n\n        \n\n        \n   \n \n\n \n\n\\label{sec:results}\n\\subsection{Event Argument Extraction Performance}\n\\begin{table}[!th]\n    \\centering\n    \\small\n    \\begin{tabular}{l|c|c}\n    \\hline\n    {} & {Arg-I} & {Arg-C}\\\\\n    \\hline\n\n         {BERT\\_QA~\\cite{du2020event}}&{68.2}&{65.4}  \\\\\n         {TANL$^{+}$~\\cite{paolini2021structured}}& {65.9} & {61.0}\\\\\n        \n         {\\citet{ma2020resource}}&{-} & {62.1}\\\\\n         {BART-Gen~\\cite{li2021document}}& {69.9} & {66.7}\\\\\n    \\hline\n    \n        {DYGIE++$^{*+}$~\\cite{wadden2019entity}}&{66.2}&{60.7}  \\\\\n         {ONEIE$^{*+}$~\\cite{lin2020joint}} & {73.2} & {69.3}\\\\\n         \\hline\n        {QGA-EE$_{BART}$ (ours)} &  {72.4} & {70.3} \\\\\n        {QGA-EE$_{T5}$ (ours)} & {\\textbf{75.0}} & {\\textbf{72.8}}  \\\\\n    \\hline\n    \\end{tabular}\n    \\caption{Event Extraction Results on ACE05-E test data (F1, \\%) with gold triggers. $^{*}$ models are trained with additional entity and relation data. $^{+}$ numbers are reported from~\\citet{hsudegree}, and others are from the original papers.}\n    \\label{tab:arg_overall_gold}\n\\end{table}\nTable~\\ref{tab:arg_overall_gold} shows the performance of QGA-EE models on ACE05-E test set with gold triggers.\\footnote{Performance of FourIE, AMR-IE and GraphIE in gold triggers are not available in their original papers.}\nBoth QGA-EE variants outperform all other approaches, and using T5 as backbone provides an improvement of 2.5\\% over BART.\nThe improvement over the prior QA-based models BERT\\_QA shows that generation-based QA models are more effective than position-based QA models for EE.\nQGA-EE$_{BART}$ outperforms the BART-based baseline BART-Gen and QGA-EE$_{T5}$ outperforms the T5-based baseline TANL, which demonstrates the effectiveness of our models with different backbones.\nOur models even outperform the joint IE models DYGIE++ and ONEIE, which leverage additional information from entities and relations.\n\n\\subsection{Event Extraction Performance}\n\n\\begin{table}[!th]\n    \\centering\n    \\small\n    \\begin{tabular}{l|c|c}\n    \\hline\n    {} & {Arg-I} & {Arg-C}\\\\\n    \\hline\n         {BERT\\_QA~\\cite{du2020event}}&{54.1}&{53.1}  \\\\\n         {TANL~\\cite{paolini2021structured}}& {50.1} & {47.6}\\\\\n         {TEXT2EVENT~\\cite{lu2021text2event}}& {-} & {53.8}\\\\\n         {\\citet{ma2020resource}}& {56.7} & {54.3}\\\\\n         {BART-Gen~\\cite{li2021document}}& {-} & {53.7}\\\\\n    \\hline\n         {DYGIE++$^{*}$~\\cite{wadden2019entity}}&{54.1}&{51.4}  \\\\\n         {ONEIE$^{*}$~\\cite{lin2020joint}} & {59.2} & {56.8}\\\\\n         {FourIE$^{*}$~\\cite{van2021cross}} & {{60.7}} & {{58.0}}\\\\\n         {AMR-IE$^{*}$~\\cite{zhang2021abstract}} & {\\textbf{60.9}} & {{58.6}}\\\\\n         {GraphIE$^{*}$~\\cite{van2022joint}} & {\\textbf{-}} & {\\textbf{59.4}}\\\\\n         \n         \\hline\n        {QGA-EE$_{BART}$ (ours)} & {57.1}& {55.6} \\\\\n        {QGA-EE$_{T5}$ (ours)} & {59.8} & {57.9} \\\\\n        \n   \n \n    \\hline\n    \\end{tabular}\n    \\caption{Event Extraction Results on ACE05-E test data (F1, \\%) with predicted triggers. $^{*}$ models are trained with additional entity and relation data. All numbers of baselines are reported from the original papers.}\n    \\label{tab:arg_overall}\n\\end{table}\nWe also evaluate our models on ACE05-E  in a more ``real world'' fashion with \\textit{predicted} triggers extracted by an ALBERT-based~\\cite{lan2019albert} sequence-labeling model (Table~\\ref{tab:arg_overall}).\\footnote{The model is trained on ACE05-E and the F1 score on test set is 72.96\\%. More details in Appendix.}\nSimilar to the performance on gold triggers, QGA-EE benefits more from the T5 backbone on predicted triggers. Both QGA-EE variants outperform all the EE-task-centered baselines by more than 1\\% on Arg-C. \n\n\\joel{also Tables 2 and 3 should delineate more clearly which ones are single task, and which are joint, otherwise we're going to run into the same issue as in AAAI}\\di{added notes in captions}\nWe also include the scores from SOTA joint IE models, DYGIE++, ONEIE, FourIE, AMR-IE and GraphIE, as reference. But, as stated earlier, it is not fair to compare our models directly with them, as they benefit from being trained with all of the annotations from entities, relations, and events. \nAlso it should be noted that their trigger labeling models have more complicated architectures and thus perform  significantly better than the sequence-labeling based tagger we use (F1 75.4\\% from FourIE and F1 74.7\\% from OneIE). This further boosts the end-to-end EE performance. \n\n\\subsection{Ablation Study}\n\nTable~\\ref{tab:abalation_gold} shows the ablation study of the QGA-EE$_{T5}$ model on the ACE05 test set with gold triggers. By replacing the QG model with simple context-unaware templates, the F1 score decreases by $1.65\\%$. It demonstrates that the context-aware questions generated by our QG component enhance the end-to-end event argument extraction performance. Additionally, the generation-based QA model deals with multi-argument situations better and provides an improvement of  $4.24\\%$.\n\n        \n  \n\n        \n\n\\begin{table}[ht]\n    \\centering\n    \\small\n    \\begin{tabular}{l|cc}\n    \\hline\n    {} & {Arg-I} & {Arg-C}\\\\\n    \\hline\n        {QGA-EE$_{T5}$} & {75.04} & {72.78}  \\\\\n    \\hline\n        \n        {- w/o pretrained QG} & {73.57} & {71.13} \\\\\n    \\hline\n        {- w/o pretrained QG \\& mutli-arg support} & {69.61} & {66.89} \\\\\n    \\hline\n        \n    \\end{tabular}\n    \\caption{Ablation study with gold triggers on ACE05-E test set (F1, \\%).}\n    \\label{tab:abalation_gold}\n\\end{table}\n\n        \n\n\\subsection{Impact of Data Augmentation}\nAs we mentioned in Section~\\ref{sec:qa_model}, the size of the training data increases from 15,426 to 20,681 as a benefit of our proposed dynamic templates. To evaluate the contribution of the data augmentation, we evaluate the performance of QGA-EE on ACE05 test data with partial training data (with gold triggers). With 40\\% of the training examples after data augmentation (8,272), QGA-EE achieves a F1 score of 71.42\\% on ACE05-E test set with gold triggers. It outperforms all of the baselines in Table~\\ref{tab:arg_overall_gold}, which demonstrates the effectiveness of our proposed model.\n\n\\begin{table}[ht]\n    \\centering\n    \\small\n    \\begin{tabular}{l|cc}\n    \\hline\n    {} & {Arg-I} & {Arg-C}\\\\\n    \\hline\n        {QGA-EE$_{T5}$ with 100\\% training data} & {75.04} & {72.78}  \\\\\n    \\hline\n        \n        {QGA-EE$_{T5}$ with 80\\% training data} & {73.86 } & {71.64} \\\\\n    \\hline\n        {QGA-EE$_{T5}$ with 60\\% training data} & { 73.15 } & {71.63 } \\\\\n    \\hline\n        {QGA-EE$_{T5}$ with 40\\% training data} & {73.47} & {71.42} \\\\\n    \\hline\n        {QGA-EE$_{T5}$ with 20\\% training data} & {71.15} & {69.13} \\\\\n    \\hline\n        \n    \\end{tabular}\n    \\caption{Performance of QGA-EE on ACE05 test data (F1, \\%) with gold triggers with partial training data. Training data is randomly sampled.}\n    \\label{tab:data_aug}\n\\end{table}\\subsection{Analysis and Discussion}\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=0.9\\linewidth]{fig/error_analysis_ee.png}\n    \\caption{The portion of each category of error based on 50 error examples in test set.}\n    \\label{fig:error_analysis}\n\\end{figure}\n\nThe average length of the questions generated by QGA-EE$_{T5}$ is 10.5 tokens, compared with 6.7 in~\\citet{du2020event}. They contain more context. \nFor example, QGA-EE generates \\textit{`Who was attacked by mob in state?'} for the \\texttt{Target} role in \\textit{`At least three members of a family in Indias northeastern state of Tripura were \\textbf{[hacked$_{Conflict.Attack}$]} to death by a tribal mob for allegedly practicing witchcraft, police said Thursday.'} It incorporates  \\texttt{Attacker} (`mob') and \\texttt{Place} (`state') information.\n\nWe categorize the errors into four groups:\n\\begin{enumerate}\n    \\item Bad question generated by the QG model. \n\n    For example, QGA-EE generates \\textit{`What did state buy in * sell * event?'} for the \\texttt{Artifact} role in \\textit{`... that the Stalinist state had developed nuclear weapons and hinted it may sell or use them, depending on US actions.'}. It should have been \\textit{`What did state sell in * sell * event?'} and this introduces an error to the QA model.\n    \n     \\item Errors resulting from a mismatch of the QA output result. QGA-EE may retrieve wrong offsets if a target candidate matches with multiple text strings in the original sentence. For example, QGA-EE matches the candidate \\textit{`Welch'} with the first mention in \\textit{`He also wants to subpoena all documents maintained in Jane Beasley \\underline{Welch}'s personnel file by Shearman; Sterling, a prestigious corporate law firm where she worked before she \\textbf{[married$_{Life.Marry}$]} \\underline{Welch}.'}, where the correct one is the second mention.\n\n     \\item Errors resulting from missing entity conference. For example, QGA-EE identifies \\textit{`Jacques Chirac'} as the \\texttt{Entity} of the \\texttt{Contact.Phone-Write} event in \\textit{`French President Jacques Chirac received only a reserved response when he tried to mend fences by placing a telephone call Tuesday to Bush.'}. But \\textit{`he'} is the ground truth and refers to \\textit{`Jacques Chirac'}.\n\n     \\item Predictions not explicitly mentioned. For example, in \\textit{`Kelly, the US assistant secretary for East Asia and Pacific Affairs, arrived in Seoul from Beijing Friday to brief Yoon, the foreign minister.'}, QGA-EE infers \\textit{`Seoul'} as the \\texttt{Place} of the \\texttt{Contact.Meet} event, but it is not explicitly mentioned in the context, thus not covered by the gold annotations.\n\\end{enumerate}\n\nWe manually analyzed a subset of the errors from the test set (50 examples), and show the portion of each category of error in Figure~\\ref{fig:error_analysis}.\n\nIn this paper, we present QGA-EE, a novel sequence-to-sequence based framework for EE, which utilizes a QG model to generate context-aware questions as inputs to a QA model for EAE. Our model naturally supports the cases in which multiple event arguments play the same role within a specific event mention. We conduct experiments on the ACE05-E dataset and the proposed model outperforms all of the single-task-based models and achieves competitive results with state-of-the-art joint IE models.\nIn the future, we plan to utilize the extensibility of the QA framework to incorporate knowledge from semi-structured event-relevant data such as Wikipedia Infoboxes. We also plan to extend our approach to multilingual EE and joint IE.\\section*{Limitations}\nThe design of the dynamic templates requires knowledge of the event ontology and is time-consuming. The authors of the paper spent  30 hours designing the exclusive templates that cover all of the possible argument combinations for each argument role in ACE ontology. With a more complicated ontology, a much larger amount of time is required.\n\nAnother limitation of our approach is the offset retrieval method. If one sentence contains multiple mentions of the same entities, or even multiple text strings that have the same spellings but refer to different entities, the QGA-EE model always retrieves the position where the mention appears for the first time in the sentence as the offset of the extracted target. It may be improved by asking the model to generate contextual text as a position reference.\n\n\\section*{Acknowledgements}\nWe thank our colleague Aoife Cahill and the anonymous reviewers for their constructive comments and suggestions.\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2104.05919v1.tex",
        "arXiv-2202.12109v2.tex",
        "arXiv-2307.05567v1.tex"
    ],
    "group_id": "group_77",
    "response": "### Title: Document-Level Event Argument Extraction: A Comparative Analysis of Conditional Generation, Prompt Tuning, and Question Generation Approaches\n\n### Introduction\n\nEvent Argument Extraction (EAE) is a critical task in Natural Language Processing (NLP) that involves identifying and categorizing key elements (arguments) associated with specific events mentioned in a text. This task has been traditionally approached at the sentence level, where models are trained to extract arguments from individual sentences, often leading to incomplete and uninformative results. The field has seen significant advancements with the advent of large-scale datasets and the application of powerful machine learning models, particularly those based on Pre-trained Language Models (PLMs). However, these advancements have been primarily focused on sentence-level extraction, which does not align well with real-world information-seeking behavior that often spans multiple sentences or even entire documents. The challenge lies in developing models that can effectively handle document-level extraction, which requires more sophisticated reasoning and inference capabilities to capture the full context of events and their arguments.\n\nRecent research has begun to address these limitations by proposing document-level EAE models. These models aim to improve the completeness and informativeness of extracted arguments by considering the entire document as the context. This shift necessitates new methodologies that can handle the complexity of document-level extraction, including implicit arguments and coreference resolution. The current progress in document-level EAE is marked by the introduction of novel models and datasets, but there are still significant challenges in achieving robust performance across diverse event types and argument roles, especially in scenarios with limited training data.\n\nThis summary will explore three recent research papers that tackle the document-level EAE task using different approaches: conditional generation, prompt tuning, and question generation. Each paper presents a unique model designed to address the limitations of traditional sentence-level extraction, aiming to improve the accuracy and efficiency of argument extraction. The common goal of these models is to enhance the understanding of events by leveraging the context of the entire document, while their innovations lie in the specific methodologies they employ to achieve this.\n\n### Main Content of Each Paper\n\n#### Paper 1: Document-Level Event Argument Extraction by Conditional Generation\n\nThis paper introduces a document-level event argument extraction model based on conditional generation. The model is designed to work with templates provided as part of the event ontology, which are used to condition the generation process. The authors argue that traditional sentence-level extraction models often fail to capture arguments that span multiple sentences, leading to incomplete and uninformative results. To address this, they propose an end-to-end model that does not require entity recognition or coreference resolution as preprocessing steps, allowing it to handle long contexts effectively.\n\nThe model uses an encoder-decoder architecture, specifically BART or T5, to generate filled templates from the input document and predefined event templates. The generation process is formulated as:\n\\begin{equation}\n\\small \np(x \\mid c)=\\prod_{i=1}^{|x|} p\\left(x_{i} \\mid x_{<i}, c\\right)\n\\end{equation}\nwhere $p(x \\mid c)$ is the probability of generating the argument spans $x$ given the context $c$. The authors introduce a method to add type constraints through clarification statements to ensure that generated arguments align with the ontology-defined types. This is achieved by appending type statements to the templates and reranking the candidate outputs based on the language modeling probability.\n\nThe authors also compile a new benchmark dataset, \\textsc{WikiEvents}, which includes complete event and coreference annotations. This dataset is used to evaluate the proposed model's performance on both conventional argument extraction and a more challenging task of informative argument extraction, which requires resolving coreferences to find the most informative mentions of arguments. The model achieves significant improvements over existing baselines, such as BERT-CRF and BERT-QA, on both tasks. For instance, on the \\textsc{WikiEvents} dataset, the model achieves an absolute gain of 7.6\\% F1 for argument extraction and 9.3\\% F1 for informative argument extraction.\n\nAdditionally, the paper demonstrates the model's portability by creating a zero-shot event extraction framework. This framework pairs the argument extraction model with a keyword-based trigger extraction model, enabling zero-shot transfer to new event types. The zero-shot performance is evaluated on the \\textsc{ACE} dataset, where the model achieves 97\\% of the fully supervised model's trigger extraction performance and 82\\% of the argument extraction performance using only 10 out of the 33 types.\n\n#### Paper 2: Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction\n\nThe second paper proposes PAIE, a novel model that uses prompt tuning for both sentence-level and document-level EAE. PAIE leverages the power of PLMs by formulating the EAE task as an extractive question answering (QA) problem, where the model is prompted to generate context-aware questions for each argument role. Unlike previous QA-based models, PAIE introduces a bipartite matching loss to optimize the span assignments for multiple arguments with the same role, avoiding the need for heuristic threshold tuning.\n\nPAIE consists of three core components: prompt creation, span selector decoding, and span prediction. The prompt creation involves designing joint prompts that include all argument roles for a given event type. These prompts are fed into the BART-Encoder and BART-Decoder separately, allowing the model to capture interactions among different roles. The span selector decoding generates role-specific span selectors, and the span prediction uses these selectors to extract argument spans from the context. The bipartite matching loss is used to find the optimal span assignments for each role, ensuring that each argument is correctly identified and classified.\n\nThe authors evaluate PAIE on three standard benchmarks: ACE05, RAMS, and \\textsc{WikiEvents}. They find that PAIE outperforms existing baselines, such as BERT-CRF and BERT-QA, on both sentence-level and document-level tasks. For instance, on the \\textsc{WikiEvents} dataset, PAIE achieves an absolute gain of 3.5\\% F1 for argument extraction and 2.3\\% F1 for argument classification. PAIE also demonstrates superior performance in few-shot settings, where it achieves comparable results with significantly fewer training samples.\n\n#### Paper 3: Event Extraction as Question Generation and Answering\n\nThe third paper introduces QGA-EE, a framework that combines question generation (QG) and question answering (QA) to perform EAE. QGA-EE uses a QG model to generate context-aware questions for each argument role, and a QA model to answer these questions and extract the arguments. The QG model is trained using dynamic templates that incorporate contextual information from other event arguments. This approach aims to address the limitations of fixed template-based QA models, which often fail to leverage rich contextual information effectively.\n\nQGA-EE is evaluated on the ACE05 dataset, which includes 33 event types and 22 argument roles. The model outperforms existing single-task baselines, such as BERT-QA and BART-Gen, and even achieves competitive performance with state-of-the-art joint Information Extraction (IE) models. For example, on the ACE05 test set with gold triggers, QGA-EE achieves an absolute gain of 2.5\\% F1 over the BART-Gen baseline. The model also demonstrates robust performance in few-shot settings, where it achieves high F1 scores with only 40\\% of the training data.\n\n### Commonalities and Innovations\n\nAll three papers aim to improve the performance of EAE by considering the entire document as the context. They share the common goal of addressing the limitations of sentence-level extraction, which often fails to capture arguments that span multiple sentences or require coreference resolution. However, each paper introduces a unique approach to achieve this goal.\n\n- **Paper 1** uses conditional generation to fill predefined templates with arguments extracted from the document. This approach does not require entity recognition or coreference resolution as preprocessing steps, simplifying the model architecture. The authors introduce a new benchmark dataset, \\textsc{WikiEvents}, which includes complete event and coreference annotations, allowing for a more comprehensive evaluation of document-level EAE.\n- **Paper 2** proposes PAIE, which uses prompt tuning to generate context-aware questions for each argument role. PAIE captures argument interactions via multi-role prompts and optimizes span assignments using a bipartite matching loss. This approach is particularly effective in handling long-range dependencies and multiple arguments with the same role.\n- **Paper 3** introduces QGA-EE, which combines QG and QA to generate context-aware questions and extract arguments. The QG model is trained using dynamic templates that incorporate contextual information, and the QA model generates the answer string directly. This approach is more flexible and can handle complex scenarios where arguments are not explicitly mentioned in the text.\n\n### Comparison of Results and Discussion\n\nThe performance of the models proposed in each paper is evaluated using standard metrics such as Argument Identification F1 (Arg-I) and Argument Classification F1 (Arg-C). Table \\ref{tab:performance_comparison} summarizes the results on the \\textsc{WikiEvents} and ACE05 datasets.\n\n\\begin{table}[h]\n\\centering\n\\small\n\\begin{tabular}{l|c|c|c|c}\n\\toprule\n\\textbf{Model} & \\textbf{Arg-I (\\%)} & \\textbf{Arg-C (\\%)} & \\textbf{Zero-shot TI F1 (\\%)} & \\textbf{Zero-shot TC F1 (\\%)} \\\\\n\\midrule\nBERT-CRF & 52.71 & 58.12 & - & - \\\\\nBERT-QA & 46.88 & 49.89 & - & - \\\\\nBERT-QA-Doc & 26.29 & 31.54 & - & - \\\\\n\\ours & \\textbf{71.75} & \\textbf{72.29} & 55.19 & 52.10 \\\\\n\\midrule\nQGA-EE$_{BART}$ & 72.4 & 70.3 & - & - \\\\\nQGA-EE$_{T5}$ & \\textbf{75.0} & \\textbf{72.8} & - & - \\\\\n\\midrule\nPAIE (base) & 69.8 & 63.4 & 55.19 & 52.10 \\\\\nPAIE (large) & \\textbf{72.7} & \\textbf{65.3} & \\textbf{72.31} & \\textbf{69.23} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Performance comparison on \\textsc{WikiEvents} and ACE05 datasets.}\n\\label{tab:performance_comparison}\n\\end{table}\n\nFrom Table \\ref{tab:performance_comparison}, we can observe the following:\n\n- **Document-Level Performance**: \\ours achieves the highest performance on both \\textsc{WikiEvents} and ACE05 datasets, with an absolute gain of 19.04\\% F1 for Arg-I and 14.17\\% F1 for Arg-C over the next best model. PAIE also performs well, with a large model achieving comparable results to \\ours. QGA-EE shows competitive performance, especially with the T5 backbone, achieving an absolute gain of 2.5\\% F1 over BART-Gen.\n- **Zero-Shot Performance**: \\ours demonstrates strong zero-shot transfer capabilities, achieving 55.19\\% F1 for trigger identification and 52.10\\% F1 for argument classification using only 30\\% of the training data. PAIE also shows robust performance in few-shot settings, with the base model achieving 55.19\\% F1 for trigger identification and 52.10\\% F1 for argument classification using only 30\\% of the training data. QGA-EE, however, does not report zero-shot results, focusing instead on the effectiveness of its QG and QA components.\n\nThe models exhibit different strengths and weaknesses. \\ours and PAIE are particularly effective in handling long-range dependencies and multiple arguments with the same role, while QGA-EE excels in generating context-aware questions that incorporate rich contextual information. The zero-shot performance of \\ours and PAIE highlights their portability to new event types, a critical feature for real-world applications where event ontologies may be incomplete or rapidly evolving.\n\n### Conclusion\n\nThe three papers discussed in this summary collectively advance the state-of-the-art in document-level EAE by introducing innovative methodologies that leverage the context of the entire document. \\ours and PAIE demonstrate strong performance on both conventional and informative argument extraction tasks, while QGA-EE shows competitive results by generating context-aware questions. The zero-shot transfer capabilities of \\ours and PAIE indicate their potential for real-world applications where event ontologies are incomplete or rapidly changing.\n\nFuture research directions could include incorporating more ontological knowledge and commonsense reasoning to improve the accuracy of extracted arguments. Additionally, extending these models to handle multilingual and multimedia data could further enhance their applicability in diverse domains. The development of more efficient and effective coreference resolution methods could also improve the informativeness of extracted arguments, addressing the limitations of current approaches that rely on explicit mentions.\n\n### Acknowledgements\n\nThe authors would like to thank the anonymous reviewers for their constructive comments and suggestions. Special thanks also go to colleagues and collaborators who have contributed to the development and evaluation of these models."
}