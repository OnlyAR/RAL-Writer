{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units}\n\n\\begin{document}\n\n\\title{HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units}\n\\author{Wei-Ning~Hsu,\n        Benjamin~Bolte,\n        Yao-Hung~Hubert~Tsai,\n        Kushal Lakhotia,\n        \\\\Ruslan~Salakhutdinov,\n        Abdelrahman~Mohamed}\n\n\\maketitle\n\n\\begin{abstract}\nSelf-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19\\% and 13\\% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.\\footnote{The code, pre-trained and fine-tuned models are available at \\url{https://github.com/pytorch/fairseq/tree/master/examples/hubert}.}\n\\end{abstract}\n\n\\begin{IEEEkeywords}\nSelf-supervised learning, BERT.\n\\end{IEEEkeywords}\n\n\\IEEEpeerreviewmaketitle\n\n\\section{Introduction}\n\\label{sec:intro}\nThe north star for many research programs has been learning speech and audio representations through listening and interaction, similar to how babies learn their first language. High fidelity speech representation includes disentangled aspects of the spoken content along with non-lexical information of how it is delivered, e.g., speaker identity, emotion, hesitation, interruptions. Furthermore, reaching a complete situational understanding requires modeling structured noise interleaving and overlapping with the speech signal, e.g., laughter, coughing, lip-smacking, background vehicle engine, birds chirping, or food sizzling sounds.\n\nThe need for such high-fidelity representations drove research in self-supervised learning for speech and audio where the targets driving the learning process of a designed pretext task are drawn from the input signal itself. Examples of pretext tasks for self-supervised speech representation learning include distinguishing near-by features from temporally distant ones \\cite{oord2018representation,schneider2019wav2vec,kharitonov2020data}, next-step prediction of audio features \\cite{chung2019unsupervised}, masked prediction of audio features given unmasked context \\cite{baevski2019vq,baevski2020wav2vec}. Besides, self-supervised learning methods do not rely on any linguistic resources during training, allowing them to learn universal representations since labels, annotations, and text-only material ignores rich information in the input signal.\n\nLearning speech representations without reliance on large volumes of labeled data is crucial for industrial applications and products with ever-increasing coverage of new languages and domains. The time needed to collect large labeled datasets covering each of these scenarios is the real bottleneck in the current fast-moving AI industry, with time-to-market playing a critical role for product success. Building more inclusive applications covering spoken-only dialects and languages is another significant benefit of reducing dependence on linguistic resources. Given their non-standard orthographic rules, many of these languages and dialects have very little or no resources at all.\n\nPseudo-labeling (PL), also known as self-training and belongs to the family of semi-supervised learning techniques, has been the dominant approach for utilizing unlabeled speech and audio with successful applications dating back to the mid-1990s \\cite{Zavaliagkos_98, ma_bbn_06, kahn2020self, hsu2020semi}. PL starts with some supervised data to train a \"teacher\" model in one specific downstream task. Pseudo-labels are then generated for the unlabeled data using the teacher model. Next, a student model is trained using the combined supervised and teacher-labeled data either using the standard cross-entropy \\cite{kahn2020self} loss or using a contrastive loss \\cite{xiao2021contrastive} to account for noise in teacher-generated labels. The pseudo-labeling process may be repeated multiple times to improve teacher label quality \\cite{xu2020iterative} iteratively.\n\nWithout discounting the immense success of pseudo-labeling techniques, self-supervised representations offer two unique advantages: (1) Pseudo-label methods force student models to merely mimic a teacher model, which is limited by its supervised data size and the provided annotation quality. On the other hand, self-supervised pretext tasks force the model to represent the entire input signal by compressing much more bits of information into the learned latent representation. (2) In pseudo-labeling, the supervised data of the teacher model forces the whole learning to be geared towards a single downstream task. On the contrary, self-supervised features show better generalization to a multitude of downstream applications.\n\nThere have been impressive successes for self-supervised learning in Computer Vision (CV) \\cite{caron2020Swav, Chen2020SimSiam, grill2020byol} and Natural Language Processing (NLP) \\cite{brown2020gpt3, liu2019roberta, lewis2019bart} applications. Learning representations of discrete input sequences, such as in Natural Language Processing (NLP) applications, uses either masked prediction \\cite{devlin2018bert, clark2020electra} or auto-regressive generation \\cite{peters2018deep, lewis2019bart} of input sequences with partial obfuscation. For continuous inputs, such as in Computer Vision (CV) applications, representations are often learned through instance classification, in which each image and its augmentations are treated as a single output class to be pulled together \\cite{Chen2020SimSiam, grill2020byol} or contrasted against other negative samples \\cite{he2020momentum}.\n\nSpeech signals differ from text and images in that they are \\textit{continuous-valued} \\textit{sequences}. Self-supervised learning for the speech recognition domain faces unique challenges from those in CV and NLP. Firstly, the presence of multiple sounds in each input utterance breaks the instance classification assumption used in many CV pre-training approaches. Secondly, during pre-training, there is no prior lexicon of discrete sound units available, as in NLP applications in which words or word pieces are used, hindering the use of predictive losses. Lastly, the boundaries between sound units are not known, which complicates masked prediction pre-training.\n\nIn this paper, we introduce \\textbf{H}idden \\textbf{u}nit \\textbf{BERT} (HuBERT) that benefits from an offline clustering step to generate noisy labels for a BERT-like per-training. Concretely, a BERT model consumes masked continuous speech features to predict pre-determined cluster assignments. The predictive loss is only applied over the masked regions, forcing the model to learn good high-level representations of unmasked inputs to infer the targets of masked ones correctly. Intuitively, the HuBERT model is forced to learn both acoustic and language models from continuous inputs. First, the model needs to model unmasked inputs into meaningful continuous latent representations, which maps to the classical acoustic modeling problem. Second, to reduce the prediction error, the model needs to capture the long-range temporal relations between learned representations. One crucial insight motivating this work is the importance of consistency of the targets, not just their correctness, which enables the model to focus on modeling the sequential structure of input data. Our approach draws inspiration from the DeepCluster method for self-supervised visual learning \\cite{caron2018deep}; however, HuBERT benefits from the masked prediction loss over speech sequences to represent their sequential structure.\n\nWhen the HuBERT model is pre-trained on either the standard Librispeech 960h \\cite{panayotov2015librispeech} or the Libri-Light 60k hours \\cite{kahn2020libri}, it either matches or improves upon the state-of-the-art wav2vec 2.0 \\cite{baevski2020wav2vec} performance on all fine-tuning subsets of 10mins, 1h, 10h, 100h, and 960h. We present systematic results on three model sizes pre-trained with HuBERT: \\textsc{Base} (90M parameters), \\textsc{Large} (300M), and \\textsc{X-Large} (1B). The \\textsc{X-Large} model shows up to 19\\% and 13\\% relative WER improvement from \\textsc{Large} models on dev-other and test-other evaluation subsets when pre-trained on the Libri-Light 60k hours.\\section{Method}\n\\subsection{Learning the Hidden Units for HuBERT}\nAn acoustic model trained on text and speech pairs provides pseudo-phonetic labels for each frame via forced alignment in semi-supervised learning. On the contrary, the self-supervised representation learning setup has access to speech-only data. Nevertheless, simple discrete latent variable models such as k-means and Gaussian mixture models (GMMs) infer hidden units that exhibit non-trivial correlation with the underlying acoustic units~\\cite{lee2012nonparametric} (see also Table~\\ref{tab:loss}). More advanced systems can achieve better acoustic unit discovery performance using better graphical models \\cite{ondel2016variational, ebbers2017hidden} or parameterizes the distributions with more powerful neural network models~\\cite{hsu2017learning, hsu2017unsupervised, chorowski2019unsupervised, khurana2019factorial, khurana2020convolutional}.\n\n\\begin{figure}[th]\n  \\centering\n  \\includegraphics[width=\\linewidth]{figures/hubert_arch}\\caption{The HuBERT approach predicts hidden cluster assignments of the masked frames ($y_2, y_3, y_4$ in the figure) generated by one or more iterations of k-means clustering.}\n  \\label{fig:arch}\n\\end{figure}\nInspired by this, we propose to use acoustic unit discovery models to provide frame-level targets. \nLet $X$ denote a speech utterance $X = [x_1, \\cdots, x_T]$ of $T$ frames.\nDiscovered hidden units are denoted with $h(X) = Z = [z_1, \\cdots, z_T]$, where $z_t \\in [C]$ is a $C$-class categorical variable and $h$ is a clustering model, e.g. k-means. \n\n\\subsection{Representation Learning via Masked Prediction}\\label{sec:maskpred}\nLet $M \\subset [T]$ denote the set of indices to be masked for a length-$T$ sequence $X$, and $\\tilde{X} = r(X, M)$ denote a corrupted version of $X$ where $x_t$ is replaced with a mask embedding $\\tilde{x}$ if $t \\in M$. A masked prediction model $f$ takes as input $\\tilde{X}$ and predicts a distribution over the target indeces at each timestep $p_f(\\cdot \\mid \\tilde{X}, t)$. There are two decisions to be made for masked prediction: \\textit{how to mask} and \\textit{where to apply the prediction loss}. \n\nRegarding the first decision, we adopt the same strategies used in SpanBERT~\\cite{joshi2020spanbert} and wav2vec 2.0~\\cite{baevski2020wav2vec} for mask generation, where $p$\\% of the timesteps are randomly selected as start indices, and spans of $l$ steps are masked. To address the second decision, we denote the cross-entropy loss computed over masked and unmasked timesteps as $L_m$ and $L_u$, respectively. $L_m$ is defined as:\n\\begin{equation}\n    L_m(f; X, M, Z) = \\sum_{t \\in M} \\log p_f(z_t \\mid \\tilde{X}, t),\n\\end{equation}\nand $L_u$ is of the same form except that it sums over $t \\not\\in M$.\nThe final loss is computed as a weighted sum of the two terms: $L = \\alpha L_m + (1-\\alpha)L_u$. In the extreme case when $\\alpha = 0$, the loss is computed over the unmasked timesteps, which is similar to acoustic modeling in hybrid speech recognition systems \\cite{young1996large, abdel2012applying, povey2005discriminative, bourlard2012connectionist}. In our setup, this limits the learning process to mimicking the clustering model. \n\nIn the other extreme with $\\alpha=1$, the loss is only computed over the masked timesteps where the model has to predict the targets corresponding to the unseen frames from context, analogous to language modeling. It forces the model to learn both the acoustic representation of unmasked segments and the long-range temporal structure of the speech data. We hypothesize that the setup with $\\alpha=1$ is more resilient to the quality of cluster targets, which is demonstrated in our experiments (see Table~\\ref{tab:loss}).\n\n\\subsection{Learning with Cluster Ensembles}\nA simple idea to improve target quality is to utilize multiple clustering models. While an individual clustering model may perform terribly, cluster ensembles can provide complementary information to facilitate representation learning. For example, an ensemble of k-means models with different codebook sizes can create targets of different granularity, from manner classes (vowel/consonant) to sub-phone states (senones). \nTo extend the proposed framework, let $Z^{(k)}$ be the target sequences generated by the $k$-th clustering model. We can now re-write $L_m$ as:\n\\begin{equation}\n    L_m(f; X, \\{ Z^{(k)} \\}_k, M) = \n    \\sum_{t \\in M} \\sum_{k} \\log p_f^{(k)}(z_t^{(k)} \\mid \\tilde{X}, t), \\label{eq:our_obj}\n\\end{equation}\nand similarly for the unmasked loss $L_u$. This is analogous to multi-task learning, but with tasks created by unsupervised clustering.\n\nAdditionally, ensembling is intriguing because it can be used alongside product quantization (PQ)~\\cite{gray1998quantization}, where a feature space is partitioned into multiple subspaces, and each subspace is quantized separately. PQ allows effective Euclidean distance-based quantization such as k-means for high-dimensional features and heterogeneous features whose scale differs significantly between subspaces. In this case, the theoretical size of the target space is the product of all codebooks' sizes.\n\n\\subsection{Iterative Refinement of Cluster Assignments}\nIn addition to using cluster ensembles, another direction for improved representation is \\textit{refining} the cluster assignments throughout the learning process. Since we expect a pre-trained model to provide better representations than the raw acoustic feature such as MFCCs, we can create a new generation of clusters by training a discrete latent model over the learned latent representations. The learning process then proceeds with the newly discovered units.\n\n\\subsection{Implementation}\\label{sec:impl}\nOur pre-trained models follows the wav2vec 2.0 architecture~\\cite{baevski2020wav2vec}, with a convolutional waveform encoder, a BERT encoder~\\cite{devlin2018bert}, a projection layer and a code embedding layer. We consider HuBERT in three different configurations: \\textsc{Base}, \\textsc{Large}, and \\textsc{X-Large}. The fisrt two follow the architectures of wav2vec 2.0 \\textsc{Base} and \\textsc{Large} closely. The \\textsc{X-Large} architecture expands the model size to about 1 billion parameters, similar to the size of the Conformer XXL model in ~\\cite{zhang2020pushing}. \nThe waveform encoder is identical for all the three configurations, which is composed of seven 512-channel layers with strides [5,2,2,2,2,2,2] and kernel widths [10,3,3,3,3,2,2]. The BERT encoder consists of many identical transformer blocks, whose parameters along with the parameter of the subsequent projection layer are specified in Table~\\ref{tab:arch}.\n\n\\begin{table}[ht]\n    \\centering\n    \\begin{tabular}{cc|ccc}\n        \\toprule\n         & & \\textsc{Base} & \\textsc{Large} & \\textsc{X-Large} \\\\\n        \\midrule\n        \\multirow{3}{*}{CNN Encoder}\n        & strides      & \\multicolumn{3}{c}{5, 2, 2, 2, 2, 2, 2} \\\\\n        & kernel width & \\multicolumn{3}{c}{10, 3, 3, 3, 3, 2, 2} \\\\\n        & channel      & \\multicolumn{3}{c}{512} \\\\\n        \\midrule\n        \\multirow{5}{*}{Transformer} \n        & layer        & 12 & 24 & 48 \\\\\n        & embedding dim. & 768 & 1024 & 1280 \\\\\n        & inner FFN dim. & 3072 & 4096 & 5120 \\\\\n        & layerdrop prob  & 0.05 & 0 & 0 \\\\\n        & attention heads & 8 & 16 & 16 \\\\\n        \\midrule\n        \\multirow{1}{*}{Projection} & dim. & 256 & 768 & 1024 \\\\\n        \\midrule\n        \\multicolumn{2}{c|}{Num. of Params} & 95M & 317M & 964M \\\\\n        \\bottomrule\n        \n    \\end{tabular}\n    \\caption{Model architecture summary for \\textsc{Base}, \\textsc{Large}, and \\textsc{X-Large} HuBERT models}\n    \\label{tab:arch}\n\\end{table}\n\nThe convolutional waveform encoder generates a feature sequence at a 20ms framerate for audio sampled at 16kHz (CNN encoder down-sampling factor is 320x). The audio encoded features are then randomly masked as described in Section \\ref{sec:maskpred}. The BERT encoder takes as input the masked sequence and outputs a feature sequence $[o_1, \\cdots, o_T]$. The distribution over codewords is parameterized with\n\\begin{equation}\n    p_f^{(k)}(c \\mid \\tilde{X}, t) = \\frac{\\exp(\\text{sim}(A^{(k)} o_t, e_c) / \\tau)} {\\sum_{c'=1}^C \\exp(\\text{sim}(A^{(k)} o_t, e_{c'}) / \\tau)},\n\\end{equation}\nwhere $A$ is the projection matrix, $e_c$ is the embedding for codeword $c$, $\\text{sim}(\\cdot, \\cdot)$ computes the cosine similarity between two vectors, and $\\tau$ scales the logit, which is set to 0.1. When cluster ensembles are used, one projection matrix $A^{(k)}$ is applied for each clustering model $k$.\n\nAfter HuBERT pre-training, We use the connectionist temporal classification (CTC)~\\cite{graves2006connectionist} loss for ASR fine-tuning of the whole model weights except the convolutional audio encoder, which remains frozen. The projection layer(s) is removed and replaced with a randomly initialized softmax layer. The CTC target vocabulary includes 26 English characters, a space token, an apostrophe, and a special CTC blank symbol.\n\\section{Related Work}\nWe discuss recent studies on self-supervised speech representation learning by grouping them by training objective. The earliest line of work learns representations by postulating a generative model for speech with latent variables, which are assumed to capture the relevant phonetic information. Training of these models amounts to likelihood maximization. Different latent structures have been applied to encode the prior assumption, such as continuous~\\cite{hsu2017learning}, discrete~\\cite{chorowski2019unsupervised,van2017neural}, or sequential~\\cite{hsu2017unsupervised,ebbers2017hidden,glarner2018full,khurana2019factorial,khurana2020convolutional}. \n\nPrediction-based self-supervised learning has gathered increasing interests recently, where a model is tasked to predict the content of the unseen regions \\cite{chung2019unsupervised, chung2020generative, chung2020improved, ling2020deep, wang2020unsupervised, liu2020mockingjay, chi2020audio, ling2020decoar} or to contrast the target unseen frame with randomly sampled ones \\cite{oord2018representation, kharitonov2020data, schneider2019wav2vec, baevski2020wav2vec}. Some models combine both the predictive and the contrastive losses \\cite{baevski2019vq, baevski2019effectiveness}. These objectives can usually be interpreted as mutual information maximization~\\cite{tsai2020ssl_multi}. Other objectives do not belong to these categories, for example, \\cite{pascual2019learning}.\n\nThis work is most related to DiscreteBERT~\\cite{baevski2019effectiveness}: both HuBERT and DiscreteBERT predict discrete targets of masked regions. However, there are several crucial differences. First, instead of taking quantized units as input, HuBERT takes raw waveforms as input to pass as much information as possible to the transformer layers, which was shown to be important in \\cite{baevski2020wav2vec}. Furthermore, in the experiment section, we show that our model, with simple k-means targets, can achieve better performance than DiscreteBERT that uses vq-wav2vec \\cite{baevski2019vq} learned units. Second, we also present many techniques to improve teacher quality instead of using a single fixed teacher as done in DiscreteBERT.\n\nHuBERT is also related to wav2vec 2.0~\\cite{baevski2020wav2vec}. However, the latter employs a contrastive loss that requires careful design of where to sample negative frames from, an auxiliary diversity loss to encourage the discrete unit usage, and demands a proper Gumbel-softmax temperature annealing schedule. In addition, it only explores quantizing the waveform encoder output, which may not be the best feature for quantization due to the limited capacity of the convolutional encoder, as suggested by our ablation studies in Figure~\\ref{fig:qual_layer}. Concretely, our proposed method adopts a more direct predictive loss by separating the acoustic unit discovery step from the masked prediction representation learning phase and achieves the state-of-the-art results that match or outperform wav2vec 2.0 on different fine-tuning scales.\n\nFinally, the idea of iterative refinement target labels is similar to iterative pseudo labeling for semi-supervised ASR~\\cite{xu2020iterative, likhomanenko2020slimipl}, which leverages an improving student model to generate better pseudo-labels for the next iteration of training. The HuBERT approach can be seen as extending this method to the self-supervised setup with a masked prediction loss.\\section{Experimental Details}\n\n\\subsection{Data}\\label{sec:data}\nFor unsupervised pre-training, we use the full 960 hours of LibriSpeech audio~\\cite{panayotov2015librispeech} or 60,000 hours of Libri-light~\\cite{kahn2020libri} audio, both of which are derived from the LibriVox project that contains English recordings of copyright-free audiobooks by volunteers from the Internet.\nFor supervised fine-tuning, five different partitions are considered: Libri-light 10-minute, 1-hour, 10-hour splits and LibriSpeech 100-hour (\\texttt{train-clean-100}) and 960-hour (\\texttt{train-clean-100}, \\texttt{train-clean-360}, \\texttt{train-other-500} combined) splits. The three Libri-light splits are subsets of the the LibriSpeech training split, and each of them contain half of the audio from \\texttt{train-clean-*} and the other from \\texttt{train-other-500}.\n\n\\subsection{Unsupervised Unit Discovery}\nTo demonstrate the effectiveness of the proposed method on utilizing low-quality cluster assignments, we consider the k-means algorithm~\\cite{lloyd1982least} for acoustic unit discovery by default. It is one of the most naive unit discovery models that can be treated as modeling an isotropic Gaussian with the same scalar variance for each acoustic unit.\nTo generate labels for the first iteration HuBERT training over the 960 hour LibriSpeech training set, we run k-means clustering with 100 clusters on 39-dimensional MFCC features, which are 13 coefficients with the first and the second-order derivatives.\n\nTo generate better targets for the subsequent iterations, we run k-means clustering with 500 clusters on the latent features extracted from the HuBERT model pre-trained in the previous iteration (not fine-tuned) at some intermediate transformer layer.\nSince the feature dimension at the transformer output is much higher than the MFCC features (768-D for HuBERT \\textsc{Base}), we cannot afford to load the entire 960 hour training split to the memory. So instead, we randomly sample 10\\% of the data for fitting the k-means model.\n\nThe \\texttt{MiniBatchKMeans} algorithm implemented in the \\texttt{scikit-learn} \\cite{pedregosa2011scikit} package is used for clustering, which fits a mini-batch of samples at a time.\\footnote{It still requires loading the entire dataset to the memory first.} We set the mini-batch size to be 10,000 frames. k-means++~\\cite{arthur2006k} with 20 random starts is used for better initialization.\n\n\\subsection{Pre-Training}\\label{sec:pretrain}\nWe train the \\textsc{Base} model for two iterations on the 960 hours of LibriSpeech audio on 32 GPUs, with a batch size of at most 87.5 seconds of audio per GPU. The first iteration is trained for 250k steps, while the second iteration is trained for 400k steps using labels generated by clustering the 6-th transformer layer output of the first iteration model. Training for 100k steps takes about 9.5 hours.\n\nNext we train HuBERT \\textsc{Large} and \\textsc{X-Large} for one iteration on 60,000 hours of Libri-light audio on 128 and 256 GPUs, respectively, for 400k steps. The batch sizes are reduced to 56.25 and 22.5 seconds of audio per GPU due to memory constraints.\nInstead of restarting the iterative process from clustering MFCC features, we extract features from the 9-th transformer layer of the second iteration \\textsc{Base} HuBERT for clustering and use those labels for training these two models. Hence, these two models can also be seen as the third iteration models.\n\nFor all HuBERT configurations, mask span is set to $l=10$, and $p=8\\%$ of the waveform encoder output frames are randomly selected as mask start if not otherwise mentioned. Adam~\\cite{kingma2014adam} optimizer is used with $\\beta = (0.9, 0.98)$, and the learning rate ramps up linearly from 0 to the peak learning rate for the first 8\\% of the training steps, and then decays linearly back to zero. The peak learning rates are 5e-4/1.5e-3/3e-3 for \\textsc{Base}/\\textsc{Large}/\\textsc{X-Large} models.\n\n\\subsection{Supervised Fine-Tuning and Decoding}\nWe fine-tune each model on 8 GPUs on the labeled splits described in Section~\\ref{sec:data}. The batch sizes per GPU are at most 200/80/40 seconds of audio for \\textsc{Base}/\\textsc{Large}/\\textsc{X-Large} models. During fine-tuning, the convolutional waveform audio encoder parameters are fixed. Like wav2vec 2.0, we introduce a \\textit{freeze-step} hyperparameter to control how many fine-tuning steps the transformer parameters are fixed, and only the new softmax matrix is trained. \nWe sweep over peak learning rate ([1e-5, 1e-4]), learning rate schedule (percentage of steps for linear ramp-up and decay), number of fine-tuning steps, freeze step, and waveform encoder output masking probability for each model size and fine-tuning split combination using the word error rate (WER) on the \\texttt{dev-other} subset as a criterion for model selection.\n\nWe use the wav2letter++~\\cite{pratap2018wav2letter++} beam search decoder wrapped in Fairseq~\\cite{ott2019fairseq} for language model-fused decoding, which optimizes:\n\\begin{equation}\n    \\log p_{CTC}(Y \\mid X) + w_1 \\log P_{LM}(Y) + w_2 |Y|,\n\\end{equation}\nwhere $Y$ is the predicted text, $|Y|$ is the length of the text, and $w_1$ and $w_2$ denote the language model weight and word score. The decoding hyperparameters are searched with Ax, a Bayesian optimization toolkit,\\footnote{\\url{https://github.com/facebook/Ax}}. In this work, we consider both $n$-gram and transformer language models trained on the official Librispeech language modeling data.\n\n\\subsection{Metrics of Target Quality}\nFor analysis, we derive frame-level forced-aligned phonetic transcripts using a hybrid ASR system to measure the correlation between the k-means cluster assignments and the actual phonetic units. \nGiven aligned frame-level phonetic labels $[y_1, \\cdots, y_T]$ and k-means labels $[z_1, \\cdots, z_T]$, the joint distribution between the two variables $p_{yz}(i, j)$ can be estimated by counting the occurrences: \n\\begin{equation}\n    p_{yz}(i, j) = \\dfrac{\\sum_{t=1}^T [y_t = i \\wedge z_t = j] }{T},\n\\end{equation}\nwhere $i$ denotes the $i$-th phoneme class and $j$ denotes the $j$-th k-means label class.\nThe marginal probabilities are computed as $p_z(j) = \\sum_i p_{yz}(i, j)$ and $p_y(j) = \\sum_j p_{yz}(i, j)$.\n\nFor each phone class $i$, we further compute the most likely target label as:\n\\begin{equation}\n    z^*(i) = \\arg\\max_j p_{yz}(i, j).\n\\end{equation}\nLikewise, for each k-means class $j$, we compute the most likely phone label as:\n\\begin{equation}\n    y^*(j) = \\arg\\max_i p_{yz}(i, j).\n\\end{equation}\nThree metrics are considered:\n\\begin{enumerate}\n    \\item \\textbf{phone purity} (Phn Pur.): \n    \\begin{equation}\n        \\mathbb{E}_{p_z(j)} [ p_{y \\mid z}(y^*(j) \\mid j) ],\n    \\end{equation}\n    where $p_{y \\mid z}(i \\mid j) = p_{yz}(i, j) / p_z(j)$ denotes the conditional probability of phone given a k-means label. This metric measures the average phone purity within one class, which can be interpreted as the frame-level phone accuracy if we transcribe each k-means class with its most likely phone label. When comparing different sets of target labels with the same number of units, higher purity indicates better quality. However, this metric is less meaningful when comparing two sets with different numbers of units: in the extreme case where each frame is assigned a unique target label, the phone purity would be 100\\%.\n    \\item \\textbf{cluster purity} (Cls Pur.):\n    \\begin{equation}\n        \\mathbb{E}_{p_y(i)} [ p_{z \\mid y}(z^*(i) \\mid i) ],\n    \\end{equation}\n    where $p_{z \\mid y}(j \\mid i) = p_{yz}(i, j) / p_y(i)$ denotes the conditional probability of a k-means label given phone label. Cluster purity is the counterpart of phone purity, whose value would typically decrease when the number of units increases. When comparing target labels with the same number of units, higher cluster purity also indicates a better quality, as frames of the same phone are more likely labeled as the same k-means label class.\n    \\item \\textbf{phone-normalized mutual information} (PNMI):\n    \\begin{align}\n        \\dfrac{I(y; z)}{H(y)} &= \\dfrac{ \n            \\sum_i \\sum_j p_{yz}(i, j) \\log \\dfrac{p_{yz}(i, j)}{p_{y}(i)p_{z}(j)}\n        }{\n            \\sum_i p_{y}(i) \\log p_{y}(i)\n        } \\\\\n        &= \\dfrac{H(y) - H(y \\mid z)} {H(y)} \\\\\n        &= 1 - \\dfrac{H(y \\mid z)} {H(y)}.\n    \\end{align}\n    PNMI is an information-theoretic metric that measures the percentage of uncertainty about the phone label $y$ eliminated after observing the k-means label $z$. Higher PNMI also indicates better k-means clustering quality.\n\\end{enumerate}\n\n\\begin{table*}[t]\n    \\centering\n    \\begin{tabular}{lcccccc}\n        \\toprule\n        Model & Unlabeled Data & LM & dev-clean & dev-other & test-clean & test-other \\\\\n        \\midrule\\midrule\n        \\multicolumn{7}{c}{\\textit{\\textbf{10-min labeled}}} \\\\\n        DiscreteBERT~\\cite{baevski2019effectiveness} & LS-960 & 4-gram & 15.7 & 24.1 & 16.3 & 25.2 \\\\\n        wav2vec 2.0 \\textsc{Base}~\\cite{baevski2020wav2vec}   & LS-960 & 4-gram & 8.9 & 15.7 & 9.1 & 15.6 \\\\\n        wav2vec 2.0 \\textsc{Large}~\\cite{baevski2020wav2vec}  & LL-60k & 4-gram & 6.3 & 9.8 & 6.6 & 10.3  \\\\\n        wav2vec 2.0 \\textsc{Large}~\\cite{baevski2020wav2vec}  & LL-60k & Transformer & 4.6 & 7.9 & 4.8 & 8.2 \\\\\n        \\midrule\n        HUBERT \\textsc{Base}    & LS-960 & 4-gram & 9.1 & 15.0 & 9.7 & 15.3  \\\\\n        HUBERT \\textsc{Large}   & LL-60k & 4-gram & 6.1 & 9.4 & 6.6 & 10.1 \\\\\n        HUBERT \\textsc{Large}   & LL-60k & Transformer & 4.3 & 7.0 & 4.7 & 7.6 \\\\\n        HUBERT \\textsc{X-Large} & LL-60k & Transformer & 4.4 & 6.1 & 4.6 & 6.8 \\\\\n        \n        \\midrule\\midrule\n        \\multicolumn{7}{c}{\\textit{\\textbf{1-hour labeled}}} \\\\\n        DeCoAR 2.0~\\cite{ling2020decoar} & LS-960 & 4-gram & - & - & 13.8 & 29.1 \\\\\n        DiscreteBERT~\\cite{baevski2019effectiveness} & LS-960 & 4-gram & 8.5 & 16.4 & 9.0 & 17.6 \\\\\n        wav2vec 2.0 \\textsc{Base}~\\cite{baevski2020wav2vec}   & LS-960 & 4-gram & 5.0 & 10.8 & 5.5 & 11.3 \\\\\n        wav2vec 2.0 \\textsc{Large}~\\cite{baevski2020wav2vec}  & LL-60k & Transformer & 2.9 & 5.4 & 2.9 & 5.8 \\\\\n        \\midrule\n        HUBERT \\textsc{Base}    & LS-960 & 4-gram & 5.6 & 10.9 & 6.1 & 11.3  \\\\\n        HUBERT \\textsc{Large}   & LL-60k & Transformer & 2.6 & 4.9 & 2.9 & 5.4 \\\\\n        HUBERT \\textsc{X-Large} & LL-60k & Transformer & 2.6 & 4.2 & 2.8 & 4.8 \\\\\n        \n        \\midrule\\midrule\n        \\multicolumn{7}{c}{\\textit{\\textbf{10-hour labeled}}} \\\\\n        SlimIPL~\\cite{likhomanenko2020slimipl} & LS-960 & 4-gram + Transformer & 5.3 & 7.9 & 5.5 & 9.0 \\\\\n        DeCoAR 2.0~\\cite{ling2020decoar} & LS-960 & 4-gram & - & - & 5.4 & 13.3 \\\\\n        DiscreteBERT~\\cite{baevski2019effectiveness} & LS-960 & 4-gram & 5.3 & 13.2 & 5.9 & 14.1 \\\\\n        wav2vec 2.0 \\textsc{Base}~\\cite{baevski2020wav2vec}   & LS-960 & 4-gram & 3.8 & 9.1 & 4.3 & 9.5 \\\\\n        wav2vec 2.0 \\textsc{Large}~\\cite{baevski2020wav2vec}  & LL-60k & Transformer & 2.4 & 4.8 & 2.6 & 4.9 \\\\\n        \\midrule\n        HUBERT \\textsc{Base}    & LS-960 & 4-gram & 3.9 & 9.0 & 4.3 & 9.4 \\\\\n        HUBERT \\textsc{Large}   & LL-60k & Transformer & 2.2 & 4.3 & 2.4 & 4.6 \\\\\n        HUBERT \\textsc{X-Large} & LL-60k & Transformer & 2.1 & 3.6 & 2.3 & 4.0 \\\\\n        \n        \\midrule\\midrule\n        \\multicolumn{7}{c}{\\textit{\\textbf{100-hour labeled}}} \\\\\n        IPL~\\cite{xu2020iterative} & LL-60k & 4-gram + Transformer & 3.19 & 6.14 & 3.72 & 7.11 \\\\\n        SlimIPL~\\cite{likhomanenko2020slimipl} & LS-860 & 4-gram + Transformer & 2.2 & 4.6 & 2.7 & 5.2 \\\\\n        Noisy Student\\cite{park2020improved} & LS-860 & LSTM & 3.9 & 8.8 & 4.2 & 8.6 \\\\\n        DeCoAR 2.0~\\cite{ling2020decoar} & LS-960 & 4-gram & - & - & 5.0 & 12.1 \\\\\n        DiscreteBERT~\\cite{baevski2019effectiveness} & LS-960 & 4-gram & 4.0 & 10.9 & 4.5 & 12.1 \\\\\n        wav2vec 2.0 \\textsc{Base}~\\cite{baevski2020wav2vec}   & LS-960 & 4-gram & 2.7 & 7.9 & 3.4 & 8.0 \\\\\n        wav2vec 2.0 \\textsc{Large}~\\cite{baevski2020wav2vec}  & LL-60k & Transformer & 1.9 & 4.0 & 2.0 & 4.0 \\\\\n        \n        \\midrule\n        HUBERT \\textsc{Base}    & LS-960 & 4-gram & 2.7 & 7.8 & 3.4 & 8.1  \\\\\n        HUBERT \\textsc{Large}   & LL-60k & Transformer & 1.8 & 3.7 & 2.1 & 3.9 \\\\\n        HUBERT \\textsc{X-Large} & LL-60k & Transformer & 1.7 & 3.0 & 1.9 & 3.5 \\\\\n        \n        \\bottomrule\n    \\end{tabular}\n    \\caption{Results and comparison with the literature on low resource setups (10-min, 1-hour, 10-hour, and 100-hour of labeled data).}\n    \\label{tab:main_lo}\n\\end{table*}\n\n\\begin{table*}[t]\n    \\centering\n    \\begin{tabular}{lcccccc}\n        \\toprule\n        Model & Unlabeled Data & LM & dev-clean & dev-other & test-clean & test-other \\\\\n        \\midrule\\midrule\n        \\multicolumn{7}{c}{\\textit{\\textbf{Superivsed}}} \\\\\n        Conformer L~\\cite{gulati2020conformer} & - & LSTM & - & - & 1.9 & 3.9 \\\\\n        \n        \\midrule\\midrule\n        \\multicolumn{7}{c}{\\textit{\\textbf{Self-Training}}} \\\\\n        IPL~\\cite{xu2020iterative} & LL-60k & 4-gram + Transformer & 1.85 & 3.26 & 2.10 & 4.01 \\\\\n        Noisy Student~\\cite{park2020improved} & LV-60k & LSTM & 1.6 & 3.4 & 1.7 & 3.4 \\\\\n        \n        \\midrule\\midrule\n        \\multicolumn{7}{c}{\\textit{\\textbf{Pre-Training}}} \\\\\n        wav2vec 2.0 \\textsc{Large}~\\cite{baevski2020wav2vec} & LL-60k & Transformer & 1.6 & 3.0 & 1.8 & 3.3 \\\\\n        pre-trained Conformer XXL~\\cite{zhang2020pushing} & LL-60k & LSTM & 1.5 & 3.0 & 1.5 & 3.1 \\\\\n        \n        \\midrule\\midrule\n        \\multicolumn{7}{c}{\\textit{\\textbf{Pre-Training + Self-Training}}} \\\\\n        wav2vec 2.0 + self-training~\\cite{xu2020self} & LL-60k & Transformer & 1.1 & 2.7 & 1.5 & 3.1\\\\\n        pre-trained Conformer XXL + Noisy Student~\\cite{zhang2020pushing} & LL-60k & LSTM & 1.3 & 2.6 & 1.4 & 2.6 \\\\\n        \n        \\midrule\\midrule\n        \\multicolumn{7}{c}{\\textit{\\textbf{This work (Pre-Training)}}} \\\\\n        HUBERT \\textsc{Large}   & LL-60k & Transformer & 1.5 & 3.0 & 1.9 & 3.3 \\\\\n        HUBERT \\textsc{X-Large} & LL-60k & Transformer & 1.5 & 2.5 & 1.8 & 2.9 \\\\\n        \n        \\bottomrule\n    \\end{tabular}\n    \\caption{Comparison with the literature on high resource setups using all 960 hours of labeled LibriSpeech data.}\n    \\label{tab:main_hi}\n\\end{table*}\n\n\\section{Results}\n\\subsection{Main Results: Low- and High-Resource Setups}\n\nTable~\\ref{tab:main_lo} presents results for the low-resource setup, where pre-trained models are fine-tuned on 10 minutes, 1 hour, 10 hours, or 100 hours of labeled data. We include comparison with semi-supervised (iterative pseudo labeling (IPL)~\\cite{xu2020iterative}, slimIPL~\\cite{likhomanenko2020slimipl}, noisy student~\\cite{park2020improved}) and self-supervised approaches (DeCoAR 2.0~\\cite{ling2020decoar}, DiscreteBERT~\\cite{baevski2019effectiveness}, wav2vec 2.0~\\cite{baevski2020wav2vec}) in the literature.\nIncreasing the amount of unlabeled data and increasing the model size improve performance, demonstrating the scalability of the proposed HuBERT self-supervised pre-training method.\nIn the ultra-low resource setup with just 10 minutes of labeled data, the HuBERT \\textsc{Large} model can achieve a WER of 4.7\\% on the test-clean set and 7.6\\% on the test-other set, which is 0.1\\% and 0.6\\% WER lower, respectively than the state-of-the-art wav2vec 2.0 \\textsc{Large} model. By further scaling up the model size to 1B parameters, the HuBERT \\textsc{X-Large} model can further reduce the WER to 4.6\\% and 6.8\\% on test-clean and test-other. The superiority of HuBERT persists across setups with different amounts of labeled data, with the only exceptions being fine-tuning on 100 hours of labeled data, where HuBERT \\textsc{Large} is 0.1\\% WER higher than wav2vec 2.0 \\textsc{Large} on test-clean, and HuBERT \\textsc{Base} is 0.1\\% WER higher than wav2vec 2.0 \\textsc{Base} on test-other.\nIn addition, HuBERT also outperforms DiscreteBERT by a large margin in all setups, while both are trained with a virtually identical objective - masked prediction of discovered units. The considerable performance gap suggests two things. First, using waveform as the input to the model is crucial for avoiding loss of information during quantization. Second, while vq-wav2vec~\\cite{baevski2019vq}, the units that DiscreteBERT uses for training, may discover better units than k-means clustering of MFCC features, the proposed iterative refinement benefits from the improving HuBERT model and learn better units eventually. We will verify these statements in the ablation study sections.\n\nWe report results of fine-tuning HuBERT models on the full 960 hours of Librispeech data and compare with the literature in Table~\\ref{tab:main_hi}. Prior studies using additional unpaired speech are classified into:\n\\begin{enumerate}\n    \\item self-training: first train an ASR on labeled data to annotate unlabeled speech, and then combine both golden and ASR-annotated text-speech pairs for supervised training.\n    \\item pre-training: first use unlabeled speech for pre-training a model, and then fine-tune the model on labeled data with a supervised training objective.\n    \\item pre-training + self-training: first pre-train and fine-tune a model, and then use it to annotate unlabeled speech for self-training combined with supervised data.\n\\end{enumerate}\nHuBERT outperforms the state-of-the-art supervised and self-training methods and is on par with the two best pre-training results in the literature; both are based on wav2vec 2.0 contrastive learning.\nIn contrast, it lags behind methods combining pre-training with self-training. However, as observed in \\cite{xu2020self} and \\cite{zhang2020pushing}, we expect that HuBERT can achieve comparable or better performance after combining with self-training, since the pre-trained HuBERT model is on par or better than the pre-trained model those two methods use for pseudo labeling.\n\n\\subsection{Analysis: K-Means Stability}\nTo better understand why masked prediction of discovered units is effective, we conduct a series of analyses and ablation studies. We start with probing the stability of the k-means clustering algorithm concerning different numbers of clusters and different sizes of its training data.\nTwo features are considered: 39-dimensional MFCC features and 768-dimensional output from the 6-th transformer layer of the first iteration HuBERT-\\textsc{Base} model. These two features are used to produce cluster assignments for the first and the second iteration HUBERT training, respectively.\n\nFor k-means clustering, we consider $K=\\{100,500\\}$ clusters fitted on \\{1, 10, 100\\} hours of speech sampled from the LibriSpeech training split. Each combination of the hyperparameters and the features are trained for 10 trials, and the mean and standard deviation of the supervised PNMI metric on the development set (combining dev-clean and dev-other from LibriSpeech) is reported in Table~\\ref{tab:stability}.\nThe results show that the k-means clustering is reasonably stable given the small standard deviations across different hyperparameters and features. Furthermore, increasing the amount of data used for fitting k-means models improves PNMI in general, but the gain is only as much as 0.012, suggesting the feasibility of using k-means for unit discovery even with limited CPU memory relative to the feature matrix size. Lastly, the PNMI score is much higher when clustering on HuBERT features than clustering on MFCC features, and the gap is even larger with 500 clusters, indicating that iterative refinement significantly improves the clustering quality.\n\n\\begin{table}[ht]\n    \\centering\n    \\resizebox{\\linewidth}{!}{\n    \\begin{tabular}{cc|ccc}\n        \\toprule\n        \\multirow{2}{*}{feature} & \\multirow{2}{*}{C} \n        & \\multicolumn{3}{c}{PNMI (mean $\\pm$ std) with K-means Training Size = } \\\\\n        & & 1h & 10h & 100h \\\\\n        \\midrule\\midrule\n        \\multirow{2}{*}{MFCC}\n        & 100 & 0.251 $\\pm$ 0.001 & 0.253 $\\pm$ 0.001 & 0.253 $\\pm$ 0.001 \\\\\n        & 500 & 0.283 $\\pm$ 0.001 & 0.285 $\\pm$ 0.000 & 0.287 $\\pm$ 0.001 \\\\\n        \\midrule\n        \\multirow{2}{*}{\\textsc{Base}-it1-L6}\n        & 100 & 0.563 $\\pm$ 0.012 & 0.561 $\\pm$ 0.012 & 0.575 $\\pm$ 0.008 \\\\\n        & 500 & 0.680 $\\pm$ 0.005 & 0.684 $\\pm$ 0.003 & 0.686 $\\pm$ 0.004 \\\\\n        \\bottomrule\n    \\end{tabular}\n    }\n    \\caption{Stability of K-means as an unsupervised unit discovery algorithm with respect to different features, numbers of clusters, and training data sizes. PNMI stands for phone-normalized mutual information.}\n    \\label{tab:stability}\n\\end{table}\n\\begin{table*}[t]\n    \\begin{minipage}[c]{0.68\\textwidth}\n        \\centering\n\\begin{tabular}{cc|c|ccc}\n    \\toprule\n    \\multirow{2}{*}{teacher} & \\multirow{2}{*}{C} & \n    \\multirow{2}{*}{PNMI} &\n    \\multicolumn{3}{c}{dev-other WER (\\%)} \\\\\n    & & & $\\alpha = 1.0$ & $\\alpha = 0.5$ & $\\alpha = 0.0$ \\\\\n    \\midrule\\midrule\n    Chenone (supervised top-line)  & 8976 & 0.809 & 10.38 & 9.16 & 9.79 \\\\\n    \\midrule\n    \\multirow{3}{*}{K-means on MFCC} \n    & 50  & 0.227 & 18.68 & 31.07 & 94.60 \\\\\n    & 100 & 0.243 & 17.86 & 29.57 & 96.37 \\\\\n    & 500 & 0.276 & 18.40 & 33.42 & 97.66 \\\\\n    \\midrule\n    K-means on \\textsc{Base}-it1-layer6 & 500 & 0.637 & 11.91 & 13.47 & 23.29 \\\\\n    K-means on \\textsc{Base}-it2-layer9 & 500 & 0.704 & 10.75 & 11.59 & 13.79 \\\\\n    \\bottomrule\n\\end{tabular}        \\caption{The effect of the training objective and clustering quality on performance. $C$ refers to the number of units, and $\\alpha$ is the weight for masked frames.}\n        \\label{tab:loss}\n    \\end{minipage}\n    \\hspace{.5cm}\n    \\begin{minipage}[c]{0.28\\textwidth}\n        \\centering\n\\begin{tabular}{lc}\n    \\toprule\n    teacher & WER \\\\\n    \\midrule\\midrule\n    K-means \\{50,100\\}       & 17.81 \\\\\n    K-means \\{50,100,500\\}   & 17.56 \\\\\n    \\midrule\n    Product K-means-0-100 & 19.26 \\\\\n    Product K-means-1-100 & 17.64 \\\\\n    Product K-means-2-100 & 18.46 \\\\\n    Product K-means-\\{0,1,2\\}-100 & 16.73 \\\\\n    \\bottomrule\n\\end{tabular}%         \\caption{Cluster ensembles with k-means and product k-means.}\n        \\label{tab:ens}\n    \\end{minipage}\n\\end{table*}\n\n\\begin{figure}[h]\n  \\centering\n  \\begin{subfigure}[b]{\\linewidth}\n    \\centering\n\\begin{tikzpicture}\n\\begin{axis}[\n  title={},\n  legend style={font=\\tiny},\n  ylabel={\\small Cluster Purity (\\%)},\n  ymin=0.0,  ymax=0.4,\n  xtick=data,\n  legend pos=north west,\n  legend columns=3,\n  height=5cm,\n  width=8cm,\n  ymajorgrids=true,\n  grid style=dashed,\n  ylabel near ticks,\n  xlabel near ticks,\n]\n\\addplot[\n    color=blue, mark=square, \n] table [y=hyp-pur, x=layer]{figures/qual_it1_km100.tex};\n\\addlegendentry{C=100, \\textsc{Base}-it1}\n\n\\addplot[\n    color=blue, mark=o, \n] table [y=hyp-pur, x=layer]{figures/qual_it1_km500.tex};\n\\addlegendentry{C=500, \\textsc{Base}-it1}\n\n\\addplot[\n    color=blue, mark=diamond, \n] table [y=hyp-pur, x=layer]{figures/qual_it1_km1000.tex};\n\\addlegendentry{C=1000, \\textsc{Base}-it1}\n\n\\addplot[\n    color=red, mark=square, \n] table [y=hyp-pur, x=layer]{figures/qual_it2_km100.tex};\n\\addlegendentry{C=100, \\textsc{Base}-it2}\n\n\\addplot[\n    color=red, mark=o, \n] table [y=hyp-pur, x=layer]{figures/qual_it2_km500.tex};\n\\addlegendentry{C=500, \\textsc{Base}-it2}\n\n\\addplot[\n    color=red, mark=diamond, \n] table [y=hyp-pur, x=layer]{figures/qual_it2_km1000.tex};\n\\addlegendentry{C=1000, \\textsc{Base}-it2}\n\n\\end{axis}\n\\end{tikzpicture}\n  \\end{subfigure}\n  \\begin{subfigure}[b]{\\linewidth}\n    \\centering\n\\begin{tikzpicture}\n\\begin{axis}[\n  title={},\n  ylabel={\\small Phone Purity (\\%)},\n  ymin=0.35,  ymax=0.75,\n  xtick=data,\n  legend style={font=\\tiny},\n  legend pos=south east,\n  legend columns=2,\n  height=4cm,\n  width=8cm,\n  ymajorgrids=true,\n  grid style=dashed,\n  ylabel near ticks,\n  xlabel near ticks,\n]\n\\addplot[\n    color=blue, mark=square, \n] table [y=ref-pur, x=layer]{figures/qual_it1_km100.tex};\n\\addlegendentry{C=100, HUBERT-it1}\n\n\\addplot[\n    color=red, mark=square, \n] table [y=ref-pur, x=layer]{figures/qual_it2_km100.tex};\n\\addlegendentry{C=100, HUBERT-it2}\n\n\\addplot[\n    color=blue, mark=o, \n] table [y=ref-pur, x=layer]{figures/qual_it1_km500.tex};\n\\addlegendentry{C=500, HUBERT-it1}\n\n\\addplot[\n    color=red, mark=o, \n] table [y=ref-pur, x=layer]{figures/qual_it2_km500.tex};\n\\addlegendentry{C=500, HUBERT-it2}\n\n\\addplot[\n    color=blue, mark=diamond, \n] table [y=ref-pur, x=layer]{figures/qual_it1_km1000.tex};\n\\addlegendentry{C=1000, HUBERT-it1}\n\n\\addplot[\n    color=red, mark=diamond, \n] table [y=ref-pur, x=layer]{figures/qual_it2_km1000.tex};\n\\addlegendentry{C=1000, HUBERT-it2}\n\n\\legend{}\n\\end{axis}\n\\end{tikzpicture}\n  \\end{subfigure}\n  \\begin{subfigure}[b]{\\linewidth}\n    \\centering\n\\begin{tikzpicture}\n\\begin{axis}[\n  title={},\n  legend style={font=\\tiny},\n  xlabel={\\small Layer},\n  ylabel={\\small PNMI (\\%)},\n  ymin=0.35,  ymax=0.75,\n  xtick=data,\n  legend pos=south east,\n  legend columns=2,\n  height=4cm,\n  width=8cm,\n  ymajorgrids=true,\n  grid style=dashed,\n  ylabel near ticks,\n  xlabel near ticks,\n]\n\\addplot[\n    color=blue, mark=square, \n] table [y=MI/H(ref), x=layer]{figures/qual_it1_km100.tex};\n\\addlegendentry{C=100, HUBERT-it1}\n\n\\addplot[\n    color=red, mark=square, \n] table [y=MI/H(ref), x=layer]{figures/qual_it2_km100.tex};\n\\addlegendentry{C=100, HUBERT-it2}\n\n\\addplot[\n    color=blue, mark=o, \n] table [y=MI/H(ref), x=layer]{figures/qual_it1_km500.tex};\n\\addlegendentry{C=500, HUBERT-it1}\n\n\\addplot[\n    color=red, mark=o, \n] table [y=MI/H(ref), x=layer]{figures/qual_it2_km500.tex};\n\\addlegendentry{C=500, HUBERT-it2}\n\n\\addplot[\n    color=blue, mark=diamond, \n] table [y=MI/H(ref), x=layer]{figures/qual_it1_km1000.tex};\n\\addlegendentry{C=1000, HUBERT-it1}\n\n\\addplot[\n    color=red, mark=diamond, \n] table [y=MI/H(ref), x=layer]{figures/qual_it2_km1000.tex};\n\\addlegendentry{C=1000, HUBERT-it2}\n\n\\legend{}\n\\end{axis}\n\\end{tikzpicture}\n  \\end{subfigure}\n  \\caption{Quality of the cluster assignments obtained by running k-means clustering on features extracted from each transformer layer of the first and the second iteration \\textsc{Base} HuBERT models.}\n  \\label{fig:qual_layer}\n\\end{figure}\n\n\\subsection{Analysis: Clustering Quality Across Layers and Iterations}\nWe next study how each layer of the HuBERT model from each iteration performs when used for clustering to generate training targets.\nThe two \\textsc{Base} HuBERT models from the first two iterations as described in Section~\\ref{sec:pretrain} are considered, which are referred to as \\textsc{Base}-it1 and \\textsc{Base}-it2, respectively. There are 26 features representing 12 transformer layers plus the input to the first transformer layer (denoted as ``Layer 0'') from the two HuBERT models.\nFor each feature, we fit three k-means models ($K=\\{100, 500, 1000\\}$ clusters) on a 100 hour subset randomly sampled from the LibriSpeech training data. The teacher quality measured in cluster purity, phone purity, and phone normalized mutual information (PNMI) is shown in Figure~\\ref{fig:qual_layer}.\nAs a baseline, MFCC achieves (cluster purity, phone purity, PNMI) = (0.099, 0.335, 0.255) for $K=100$ and (0.031, 0.356, 0.287) for $K=500$.\n\nBoth \\textsc{Base}-it1 and \\textsc{Base}-it2 features result in significantly better clustering quality on all three metrics than MFCC with the same number of clusters. On the other hand, the best \\textsc{Base}-it2 feature is better than the best \\textsc{Base}-it1 on phone purity and PNMI, but slightly worse on cluster purity.\nFinally, we observe different trends across layers from \\textsc{Base}-it1 and \\textsc{Base}-it2: while \\textsc{Base}-it2 model features generally improve over layers, \\textsc{Base}-it1 has the best features in the middle layers around the 6th layer. Interestingly, the quality of the last few layers degrades dramatically for \\textsc{Base}-it1, potentially because it is trained on target assignments of worse quality, and therefore the last few layers learn to mimic their bad label behavior.\n\n\\subsection{Ablation: The Importance of Predicting Masked Frames}\nWe present a series of ablation studies in the following sections to learn how pre-training objective, cluster quality, and hyperparameters affect the performance. \nThe models for ablation studies are pre-trained for 100k steps and fine-tuned on the 10-hour libri-light split using fixed hyperaprameters. MFCC-based k-means units with C=100 are used if not otherwise mentioned. We report WERs on the dev-other set decoded with the $n$-gram language model using fixed decoding hyperparameters.\n\nTo understand the importance of our proposal to predict the masked frames only, we compare three conditions: 1) predicting masked frames, 2) predicting all frames, and 3) predicting unmasked frames, which can be simulated by setting $\\alpha$ to 1.0, 0.5, and 0.0, respectively. \nWe are comparing three k-means models learned from clustering MFCC teachers with 50, 100, 500 clusters, one learned from clustering HuBERT-\\textsc{Base}-it1 6th transformer layer features, and supervised labels obtained from the forced-alignment of character-based HMM models (chenone)~\\cite{le2019senones}.\n\nResults shown in Table~\\ref{tab:loss} indicate that when learning from bad cluster assignments, computing loss only from the masked regions achieves the best performance, while the inclusion of unmasked loss results in significantly higher WERs. \nHowever, as the clustering quality improves, the model would suffer less when computing losses on the unmasked frames (\\textsc{Base}-it1-layer6) or even achieve better performance as the case of chenone.\n\n\\subsection{Ablation: The Effect of Cluster Ensembles}\nTo understand the effect of combining multiple k-means models for generating targets, we consider two setups. The first one has k-means models of different numbers of clusters presented in Table~\\ref{tab:loss}, denoted with KM-\\{50,100,500\\}. The second one has k-means models trained on spliced MFCC features with a window of three; hence, each input feature is represented as a 117-dimensional vector. In this second case, we apply product quantization on the spliced features, where dimensions are split into the coefficients of the zeroth, first, and second-order derivatives, with each 39-dimensional subspace quantized to a codebook of 100 entries. We denote these codebooks with Product k-means-\\{0,1,2\\}-100, respectively.\nBy comparing the results from Table~\\ref{tab:loss} and Table~\\ref{tab:ens}, it is clear that using an ensemble leads to better performance than what a single k-means clustering can achieve.\n\n\\subsection{Ablation: Impact of Hyperparameters}\nFigure~\\ref{fig:prob_bs} and Table~\\ref{tab:step} studies how hyperparameters affect HuBERT pre-training.\nIt is shown that\n\\begin{enumerate*}[label=(\\arabic*)]\n    \\item the portion of frames selected as mask start is optimal at $p=$8\\%;\n    \\item increasing the batch size can significantly improve the performance; % (20\\% WER reduction when increasing the batch size by 8 times);\n    \\item training for longer consistently helps for both k-means models with C=\\{50, 100\\}, and the best model achieves a WER of 11.68\\%.\n\\end{enumerate*}\nThese findings are also consistent with those from BERT-like models~\\cite{clark2020electra}. In addition, we include a comparable result from DiscreteBERT~\\cite{baevski2019effectiveness} in Table~\\ref{tab:step} which applies k-means to quantize the same MFCC features into 13.5k units, used as both the output and the \\textit{input} to the BERT model. Besides using continuous speech input rather than discrete units, We hypothesize that HuBERT achieves significantly better performance because its fewer k-means clusters of 100 or 500 help capture broad phonetic concepts without delving into inter/intra-speaker variation. \n\n\\begin{table}[ht]\n    \\centering\n\\begin{tabular}{cc|cccc}\n    \\toprule\n    \\multirow{2}{*}{teacher} & \\multirow{2}{*}{C} &  \n    \\multicolumn{4}{c}{dev-other WER (\\%)} \\\\\n    & & steps=100k & 250k & 400k & 800k \\\\\n    \\midrule\n    \\multirow{2}{*}{K-means}  \n    & 50  & 18.68 & 13.65 & 12.40 & 11.82 \\\\\n    & 100 & 17.86 & 12.97 & 12.32 & 11.68 \\\\\n    \\midrule\n    \\cite{baevski2019effectiveness} & 13.5k & \\multicolumn{4}{c}{26.6} \\\\\n    \\bottomrule\n\\end{tabular}    \\caption{Varying the number of HuBERT pre-training steps. $p$ is set to 6.5\\%.}\n    \\label{tab:step}\n\\end{table}\n\n\\begin{figure}[ht]\n  \\centering\n  \\begin{subfigure}[b]{0.49\\linewidth}\n    \\centering\n\\begin{tikzpicture}\n\\begin{axis}[\n  xlabel={\\small $p$},\n  ylabel={\\small WER (\\%)},\n  ymin=16,\n  ymax=24,\n  xtick=data,\n  width=\\linewidth,\n  height=2.5cm,\n  ymajorgrids=true,\n  grid style=dashed,\n  ylabel near ticks,\n  xlabel near ticks,\n  ticklabel style={font=\\scriptsize},\n]\n\\addplot table [y=km100_100k, x=maskp]{figures/maskp.tex};\n\\end{axis}\n\\end{tikzpicture}  \\end{subfigure}\n  \\begin{subfigure}[b]{0.49\\linewidth}\n    \\centering\n\\begin{tikzpicture}\n\\begin{axis}[\n  xlabel={\\small \\#GPUs},\n  ylabel={\\small WER (\\%)},\n  ymin=15,\n  ymax=40,\n  xtick=data,\n  width=\\linewidth,\n  height=2.5cm,\n  ymajorgrids=true,\n  grid style=dashed,\n  ylabel near ticks,\n  xlabel near ticks,\n  ticklabel style={font=\\scriptsize},\n]\n\\addplot table [y=km100_100k, x=ngpu]{figures/batchsize.tex};\n\\end{axis}\n\\end{tikzpicture}  \\end{subfigure}\n  \\caption{Varying masking probability $p$ (left) and effective batch size through the number of GPUs (right).}\n  \\label{fig:prob_bs}\n\\end{figure}\n\\section{Conclusion}\nThis paper presents HuBERT, a speech representation learning approach that relies on predicting K-means cluster assignments of masked segments of continuous input. On both the Librispeech 960 hours and the 60,000 hours Libri-light pre-training setups, HuBERT matches or outperforms the state-of-the-art systems over all fine-tuning subsets of 10mins, 1h, 10h, 100h, and 960h. Furthermore, the learned representation quality improves dramatically with iteratively refining K-means cluster assignments using learned latent representations for a previous iteration. Finally, HuBERT scales well to a 1B transformer model showing a relative reduction in WER of up to 13\\% on the test-other subset. For future work, we plan to improve the HuBERT training procedure to consist of a single phase. Furthermore, given the high quality of its representations, we will consider using HuBERT pre-trained representations for multiple downstream recognition and generation tasks beyond ASR.\n\\ifCLASSOPTIONcaptionsoff\n  \\newpage\n\\fi\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{SPIRAL: Self-supervised Perturbation-\\\\Invariant Representation Learning for\\\\ Speech Pre-Training}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nWe introduce a new approach for speech pre-training named SPIRAL which works by learning denoising representation of perturbed data in a \\teacher-\\student\\ framework.\nSpecifically, given a speech utterance, we first feed the utterance to a \\textit{\\teacher} network to obtain corresponding representation. Then the same utterance is perturbed and fed to a \\textit{\\student} network. The \\student\\ network is trained to output representation resembling that of the \\teacher. At the same time, the \\teacher\\ network is updated as moving average of \\student's weights over training steps. In order to prevent representation collapse, we apply an in-utterance contrastive loss as pre-training objective and impose position randomization on the input to the \\teacher. SPIRAL achieves competitive or better results compared to state-of-the-art speech pre-training method wav2vec 2.0, with significant reduction of training cost (80\\% for \\tscbase{} model, 65\\% for \\tscbig{} model).\nFurthermore, we address the problem of noise-robustness that is critical to real-world speech applications. We propose multi-condition pre-training by perturbing the student's input with various types of additive noise.\nWe demonstrate that multi-condition pre-trained SPIRAL models are more robust to noisy speech (9.0\\% - 13.3\\% relative word error rate reduction on real noisy test data), compared to applying multi-condition training solely in the fine-tuning stage.\nSource code is available \\footnote{https://github.com/huawei-noah/Speech-Backbones/tree/main/SPIRAL}.\n\\end{abstract}\n\n\\section{INTRODUCTION}\n\nIndustrial-scale automatic speech recognition (ASR) systems are usually trained with ten-thousands of hours of hand-transcribed speech data \\citep{galvez2021the}.\nHowever, labeling speech data is expensive and time-consuming, especially for languages with small speaker populations, or for specific domains (e.g., legal, financial, scientific).\n\nRecently, methods of utilizing unlabeled speech data to improve speech recognition system have achieved remarkable progress.\nAmongst them, \\textit{self-training} \\citep{manohar15_interspeech,Kahn2020,synnaeve2020end,chen20m_interspeech,xu20b_interspeech,park2020improved, Xiao2021contrative}, also known as \\textit{pseudo-labeling}, starts by training an ASR model with labeled speech data, which is referred to as teacher model. Then the teacher model, usually combined with a language model (LM), is used to produce pseudo-labels for unlabeled speech data. Finally, the labeled data and the pseudo-labeled data are combined to train a new model, which is referred to as student model. The process is repeated by taking the student model as the teacher in next iteration. Another line of work is speech \\textit{pre-training} \\citep{oord2019representation, Chung2020, Wang2020, wav2vec2, Liu2020}. Pre-training learns speech representation from unlabeled data in a self-supervised way. The pre-trained model is then fine-tuned on the labeled data. Self-training and pre-training are complementary as shown in recent work \\citep{Xu2021,zhang2020pushing}.\n\nIn this paper, we introduce a new speech pre-training method which works by learning denoising representation of perturbed data with the \\teacher-\\student\\ framework, named as Self-supervised Perturbation-Invariant Representation Learning (SPIRAL).\nCompared to state-of-the-art speech pre-training methods such as wav2vec 2.0~\\citep{wav2vec2} and HuBERT~\\citep{hubert}, our method allows end-to-end training with a single contrastive loss, and without relying on discrete unit discovery techniques such as vector quantization~\\citep{Jegou2011,baevski2020vqwav2vec,wav2vec2} or iterative clustering process~\\citep{hubert}.\nWe apply multi-condition training with SPIRAL~\\citep{Seltzer2013,ko15_interspeech} to improve noise-robustness for the downstream speech tasks. \n\nSPIRAL is motivated by the observation that human tolerates speech perturbations or distortions fairly well. For example, people can communicate effectively in a noisy environment, or over a distorted telephone channel.\nTherefore, we hypothesize that by learning representation invariant to perturbation, the model will learn high-level representation which can enhance speech applications.\n\nTo learn perturbation-invariant representation in a self-supervised way, we employ a teacher-student framework similar to \\citet{Tarvainen2017}.\nDuring pre-training, given a speech utterance, we guide the \\student{} network which consumes the perturbed utterance to learn from the \\teacher{} network which consumes the clean utterance.\nThe \\student{} is trained to produce denoised representation of the perturbed utterance similar to \\teacher{}'s representation of the clean utterance.\nMeanwhile, the \\teacher{}, which shares the same model architecture with \\student{}, is updated as moving average of the \\student{}'s weights over past training steps.\n\nWe apply the \\textit{in-utterance} contrastive loss to avoid model collapse to trivial constant representation~\\citep{Chopra2005}. As speech utterance are sequential data, there is another possible trivial solution which we call \\textit{positional collapse}. Positional collapse occurs when the \\student{} ``cheats'' by exploiting position correlation in \\teacher{}'s representation to minimize the loss, while ignoring the content of the input utterance.\nTo prevent positional collapse, we propose \\textit{position randomization} by adding random number of paddings on both sides of input utterance to the \\teacher{}.\n\nLarge-scale speech pre-training is computationally demanding. To reduce computation cost, we adopt a gradual down-sampling strategy in SPIRAL model, which has been verified effective in speech recognition literatures with negligible performance degradation~\\citep{Peddinti2018, han20_interspeech, huang2020conv}. We also speculate that aggressive down-sampling helps to remove redundancy in speech.\n\nTo evaluate the effectiveness of SPIRAL, we conduct experiments on LibriSpeech and Libri-Light datasets.\nBy training a small convolutional classifier on the representation of a frozen SPIRAL model, we can achieve WER of 3.5\\% and 6.4\\% on \\libri{} test-clean and test-other respectively.\nSPIRAL achieves competitive or better results compared to state-of-the-art speech pre-training methods, while being much more training-efficient.\nWe also demonstrate that multi-condition pre-trained SPIRAL are more robust to noisy speech with 9.0\\% - 13.3\\% relative word error rate (WER) reduction on real noisy test data from ChiME-3 \\citep{Barker2015}, compared to the model applying multi-condition training solely in fine-tuning stage.\n\n\\section{Related work}\n\n\\textit{Mean Teacher} (MT)~\\citep{Tarvainen2017} proposes using a student network to learn from a teacher network which is the moving average version of the student in the semi-supervised learning setting. The authors apply a supervised loss for labeled data and a consistency loss between teacher and student predictions for unlabeled data.\nHowever, direct application of MT to self-supervised learning leads to representation collapse~\\citep{Grill2020}.\n\n\\textit{Noisy student training} (NST)~\\citep{xie2019self, park2020improved} is a self-training method. NST demonstrates the importance of the aggressive injection of noise into the student. Although not emphasized, no noise is injected into pseudo-labeling process of the teacher. \nWe consider our work as an extension of self-training approach to the self-supervised learning regime. Instead of using the teacher to provide pseudo-labels, we utilize the teacher for pseudo-reference representation.\n\n\\textit{Denoising autoencoders}~\\citep{dae2008} learn to recover a clean input from a corrupted version. \nHowever, speech data contain redundancy which is irrelevant to some speech applications such as speech recognition. \nPrevious work~\\citep{baevski2019effectiveness} shows that speech pre-training by recovering masked input speech features is not effective.\nIn SPIRAL, we instead enforce latent representation of a corrupted input to resemble that of the corresponding clean input.\n\n\\textit{Bootstrap Your Own Latent} (BYOL)~\\citep{Grill2020} is a self-supervised image representation learning method. The method is based on a teacher-student framework similar to MT. The authors refer to student network as online network and teacher network as target network. They observe that naive application of MT to self-supervised learning leads to trivial constant representation. They prevent the representation collapse by appending a predictor to the student network. The theory behind is under investigation~\\citep{chen2021exploring, tian21a}.\nOur method draws inspirations from BYOL and shares the similar architecture, but there are crucial differences.\nInstead of learning a single global representation for an image as in BYOL, SPIRAL learns a sequence of representation for an utterance. We aim for sequence applications such as speech recognition.\nIn our preliminary experiments, we observe that appending a predictor to student network is not sufficient to prevent trivial constant representation for sequential representation learning.\nWe use in-utterance contrastive loss~\\citep{wav2vec2} combined with input position randomization to successfully avoid representation collapse.\nWe still keep the predictor in SPIRAL, but only for the sake of performance improvement from our observation.\nAnother difference is that BYOL does not perform representation denoising. BYOL applies perturbation, which they call augmentation, to both the inputs of the teacher and the student. \nWe demonstrate that representation denoising is crucial for speech pre-training. When perturbation is applied to the teacher's input, the effectiveness of speech pre-training degrades drastically.\n\n\\textit{Wav2vec 2.0}~\\citep{wav2vec2} is a self-supervised speech representation learning method which belongs to the masked prediction family. Masked prediction methods are effective for text pre-training~\\citep{DevlinCLT19}, but not for speech pre-training when naively applied~\\citep{baevski2019effectiveness}. The reason is that speech data contains redundancy such as speaker information, pronunciation variations, which are irrelevant to the semantic meaning of the utterance.\nTo overcome this problem, wav2vec 2.0 perform masking in intermediate latent space and performs target discretization with a differentiable quantization scheme. \nHowever, quantization leads to a more complex model by introducing additional hyper-parameters and an additional diversity loss.\nSPIRAL does not utilize quantization, and still achieves competitive performance compared to wav2vec 2.0.\nWe hypothesize that aggressive down-sampling and learning by matching output representation may help to remove redundancy from the learned representation.\nWe leave the investigation of whether target discretization could further improve SPIRAL for future work.\n\n\\citet{liang2018learning} demonstrates that under the supervised learning setting, enforcing noise-invariant representation by penalizing difference between clean and noisy data improves ASR model accuracy. \n\n\\section{METHOD}\n\\label{method}\n\n\\subsection{Self-supervised Perturbation-Invariant Representation Learning (SPIRAL)}\n\n\\begin{figure}[th]\n   \\centering\n   \\includegraphics[trim=6.6cm 4.1cm 8.1cm 3.6cm,clip, width=0.55\\linewidth]{spiral_architecture.pdf}\n   \\caption{Illustration of SPIRAL architecture for speech pre-training.}\n   \\label{fig:spiral_architecture}\n\\end{figure}\n\nFigure~\\ref{fig:spiral_architecture} shows the diagram of SPIRAL in the pre-training stage, \nwhere we use two neural networks, a \\textit{\\student} $F_\\theta$ and a \\textit{\\teacher} $F_{\\theta'}$.\nThe weights of the \\teacher\\ $\\theta'$ is the moving average of the weights of the \\student\\ $\\theta$. At step $t$, the weights of the \\teacher\\ $\\theta'_t$ are updated as\n\\begin{equation}\n\\theta'_t \\leftarrow \\alpha_t\\theta'_{t-1} +  (1-\\alpha_t)\\theta_t,\n\\label{eqn:ema_update}\n\\end{equation}\nwhere $\\alpha_t$ determines the rate of weight updates.\nGiven a speech utterance $\\mX=(\n\\vx_1, \\ldots, \\vx_T)$ of length $T$, the \\student\\ takes a perturbed version $\\tilde{\\mX} = s(\\mX) = (\\tilde{\\vx}_1, \\ldots, \\tilde{\\vx}_T)$ as input where $s(\\cdot)$ is a perturbation function. \nThe output of the student is a representation sequence $\\mZ = F(\\tilde{\\mX};\\theta) = (\\vz_1, \\ldots, \\vz_T)$.\nThe \\teacher\\ takes the same utterance without perturbation as input and output another representation sequence $\\mZ' = F(\\mX;\\theta') = (\\vz'_1, \\ldots, \\vz'_T)$.\nFor each representation $\\vz_i \\in \\mZ$, the \\student\\ is trained to match the \\teacher{}'s\\ representation $\\vz'_i$ at the same position amongst $k$ distracting samples.\nThe distracting samples are randomly drawn from other positions of the same utterance in $\\mZ'$, which is found to be more effective than samples drawn from an entire batch of utterances~\\citep{wav2vec2}. \nThe in-utterance contrastive loss is defined following \\citet{sohn2016improved,wu2018unsupervised} as,\n\\begin{equation}\n   \\mathcal{L} = -\\sum_{i=1}^T {\\log \\frac{\\exp(\\phi(\\vz_i, \\vz'_i)/\\kappa)}{\\sum_{j \\in D_i} \\exp(\\phi(\\vz_i, \\vz'_j)/\\kappa)}},\n\\label{eqn:contrastive_loss}\n\\end{equation}\nwhere $\\phi(\\mathbf{a},\\mathbf{b}) = \\mathbf{a}^{T} \\mathbf{b} / \\|\\mathbf{a}\\| \\|\\mathbf{b}\\|$ is cosine similarity, $D_i$ is the set of indices of distractors for the $i$-th position, and $\\kappa$ is the temperature parameter.\n\nHowever, applying in-utterance contrastive loss could cause a kind of representation collapse which we refer to as positional collapse.\nContrastive candidates are sampled based on their positions in utterances. \nWhen a \\teacher{}'s representation $\\vz'_i$ is correlated with its position $i$ (e.g., correlation introduced by positional encoding in Transformer), the \\student{} could exploit this correlation to generate its representation $\\vz_i$ solely based on the position index $i$, while ignoring content of the input.\nIn this case, \nthe model does not learn meaningful representation of the input content.\nTherefore, we prevent positional collapse by randomizing positions of teacher's representation. In particular, we add random number of padding data at both ends of the input to the teacher to randomly shift the position information for each output representation $\\vz'_i$. The \\student{} thereby is unable to exploit the spurious position information to minimize the contrastive loss. Note that when calculating the contrastive loss, we exclude the corresponding representation of the padded data.\n\n\\subsection{Model architecture}\n\n\\begin{figure}[th]\n   \\centering\n   \\includegraphics[trim=0.5cm 7.5cm 0.5cm 7.2cm,clip, width=1\\linewidth]{spiral_model.pdf}\n   \\caption{The architecture of the student model in SPIRAL. The frame rate of input is denoted as `10/40/80 ms'. The dashed line indicates the optional predictor which can be removed with small performance degradation. The structure of the teacher model is the same but without the predictor.}\n   \\label{fig:spiral_model}\n\\end{figure}\n\nAs illustrated in Figure \\ref{fig:spiral_model}, \\student\\ $F_\\theta$ is composed of an encoder $f(\\cdot)$, a projection head $g(\\cdot)$~\\citep{chen2020simple} and an optional predictor $q(\\cdot)$~\\citep{Grill2020}, i.e., $F_\\theta = (f\\circ g\\circ q)(\\cdot;\\theta)$. The \\teacher\\ $F_{\\theta'}$ has the same structure expect that it has no predictor, $F_{\\theta'} = (f\\circ g)(\\cdot;\\theta') $.\nThe encoder consists of two blocks. In each block, we first apply temporal convolutions to perform down-sampling, followed by Transformer~\\citep{NIPS2017_3f5ee243} with convolutional relative position encoding~\\citep{wav2vec2}. Each convolution is followed by layer normalization (LN)~\\citep{ba2016layer} and ReLU.\nFor the projection head, we apply a simple linear layer.\nThe predictor consists of two layers of temporal convolution and a linear layer. The convolutions are followed by batch normalization (BN)~\\citep{batchnorm} and ReLU. \n\nDuring pre-training, we add computation noise to both the \\student{} and the \\teacher{} by applying dropout~\\citep{srivastava2014dropout} and LayerDrop~\\citep{fan2019reducing} in Transformer. We use the same dropout and LayerDrop rates for the \\student{} and the \\teacher{}.\n\n\\subsection{Adaptive SpecAugment}\n\nWe apply adaptive SpecAugment similar to \\citet{large_scale_spec_aug} as the primary perturbation method.\nAlong either time or frequency dimension, we sample uniformly a certain proportion $p$ of all time-steps to be start indices and mask the subsequent consecutive $L$ time-steps.\nThe masked time-steps is filled with zeros along frequency dimension. Along time dimension, we use Gaussian noise as masking values to avoid numerical problems for LN~\\citep{large_scale_spec_aug}.\n\n\\subsection{Multi-condition Pre-training}\nFor noise-robust pre-training with SPIRAL, we perturb input of the \\student{} with various types of additive noise.\nWe consider this technique as an implementation of multi-condition training (MCT)~\\citep{Seltzer2013} in self-supervised setting.\nSpecifically, for each input utterance to the \\student{}, we sample noise clips from a noise dataset, and mix the noise clips with the whole utterance by addition in time-domain. We first uniformly sample a signal-to-noise ratio (SNR) from a pre-defined range for each utterance. Then we scale the noise volume according to the required SNR. In our preliminary experiments, we found that applying additive noise alone as perturbation degrades performance. Therefore, we apply additive noise perturbation together with adaptive SpecAugment.\n\n\\subsection{Model Fine-tuning}\n\nAfter pre-training, we take the encoder from the \\student{} model in SPIRAL and add a randomly initialized convolutional classifier on top of it.\nThe convolutional classifier is composed of two layers of convolution, followed by LN and ReLU, and a linear output projection.  The convolution filters consist of 512 channels with kernel width of 5.\n\nWe fine-tune the model with connectionist temporal classification (CTC)~\\citep{Graves2006} objective for speech recognition.\nWe use 1024 subwords as output units. The sub-words are generated from training transcripts of LibriSpeech with SentencePiece~\\citep{kudo2018sentencepiece}.\n\nWe further investigate SPIRAL's ability to learn high-level representation of speech during pre-training. In addition to whole-model fine-tuning, we apply frozen fine-tuning. We freeze the pre-trained parameters and only fine-tune the convolutional classifier which can only perform local classification due to limited receptive field.\n\n\\section{EXPERIMENTAL SETUP}\n\\label{experimental_setup}\n\n\\subsection{Data}\n\nFor pre-training, we use the 960-hour training data (ignoring the labels) from LibriSpeech~\\citep{panayotov2015librispeech}(LS-960), or 60k-hour unlabeled audio data from Libri-Light~\\citep{kahn2020libri} (LL-60K).\nFor Libri-Light, we segment the data using official tools with a threshold of 16s, resulting in 46.9k hours of data.\nThe two datasets are both derived from English audiobooks from LibriVox project\\footnote{\\url{https://librivox.org/}}.\nFor ASR fine-tuning, we apply 100-hour subset (train-clean-100) as low-resource labeled data and %960-hour data (\\texttt{train-960}) \nentire LS-960 with labels\nas high-resource labeled data, both from LibriSpeech.\n\nFor multi-condition training, we use the noise dataset from \\citet{ms_dns}. The dataset consists of 181 hours of noise data with about 150 noise types and 70,000 clips.\nWe shuffle and split the noise data with a ratio of 8:1:1, which are used for training, synthesizing noisy dev-sets and synthetic noisy test-sets (results in Appendix \\ref{app:synnoise}) respectively. \nSNRs of speech mixtures are set from 0 to 30 dB. \nWe evaluate on real noisy data test set from CHiME-3 \\citep{Barker2015}, which is comprised of speech data recorded in real noisy environments (bus, cafe, pedestrian area, and street junction). The data are recorded with a microphone array composed of multiple microphone channels located at different positions of a tablet, and a close-talking microphone. %Signal-to-noise ratios (SNR) of the data recorded from different channels are therefore different.\n\n\\subsection{Training setups}\n\nWe apply 128-dimensional log-mel filterbank extracted with 20~ms window and 10~ms stride as the input acoustic feature.\nWe experiment with \\tscbase{} model and \\tscbig{} model configurations as shown in Table \\ref{tab:model_config}. The numbers of parameters are comparable to wav2vec 2.0 \\tscbase{} and \\tscbig{} models correspondingly. \nFor SpecAugment, we set $p=0.025$ and $L=20$ for time-dimension mask, and $p=0.02$ and $L=20$ for frequency-dimension mask.\n\n  \n\\begin{table}[t]\n   \\caption{Detailed configurations of the SPIRAL \\tscbase{} and \\tscbig{} models.}\n   \\label{tab:model_config}\n   \\centering\n   \\scalebox{0.9}{\n   \\begin{tabular}{c|ccccccc}\n   \\toprule\n    Modules & Conv.1 & Transf.1 & Conv.2 & Transf.2 & Proj. H. & Predictor & \\#Params \\\\\n  \\midrule\n   \\multirow{5}{*}{\\shortstack{Hyper\\\\ -params}}\n   &           & layer     &  & layer        & & &  \\\\\n   & kernel size & emb. dim. & kernel size & emb. dim. &      & kernel size & \\\\\n   & channel      & ffn dim. & channel       & ffn dim.  & dim. & channel & \\\\\n   & stride       & layerdrop & stride       &  layerdrop &&&\\\\\n   &  & attn. heads & & attn. heads &&&\\\\\n   \\midrule\n   \\multirow{5}{*}{\\shortstack{\\tscbase{}\\\\model}}\n   &             & 2    &          & 10   &     &             &       \\\\\n   & 5,5,1       & 512  & 5,1      & 768  &     & 5,5,1       &       \\\\\n   & 384,512,512 & 2048 & 1536,768 & 3072 & 256 & 256,256,256 & 91.5M \\\\\n   & 2,2,1       & 0    & 2,1      & 0.05 &     &             &       \\\\\n   &             & 8    &          & 12   &     &             &       \\\\\n   \\midrule\n   \\multirow{5}{*}{\\shortstack{\\tscbig{}\\\\model}}\n   &             & 4    &          & 20   &     &             &       \\\\\n   & 5,5,1       & 512  & 5,1      & 1024 &     & 5,5,1       &       \\\\\n   & 384,512,512 & 2048 & 2048,1024& 4096 & 512 & 512,512,512 & 287M \\\\\n   & 2,2,1       & 0.05 & 2,1      & 0.05 &     &             &       \\\\\n   &             & 8    &          & 16   &     &             &       \\\\\n   \\bottomrule\n   \\end{tabular}\n   }\n  \\end{table}\n\n \\begin{table}[t]\n   \\caption{Comparison of pre-training cost between  wav2vec 2.0 and SPIRAL.}\n   \\label{tab:training_cost}\n   \\centering\n   \\scalebox{0.99}{\\small\n   \\begin{tabular}{lcccc}\n   \\toprule\n   Model & Unlabeled data & Training steps & GPU days & Mixed precision \\\\\n   \\midrule\n   Wav2vec 2.0 \\tscbase{}~\\scriptsize{\\citep{wav2vec2}}  & \\librisz{} & 500k & 102.4 & \\checkmark  \\\\\n   SPIRAL \\tscbase{}  & \\librisz{} & 200k &  20.8  & - \\\\\n   \\midrule\n   Wav2vec 2.0 \\tscbig{}~\\scriptsize{\\citep{wav2vec2}}  & \\libriltsz{} & 1000k & 665.6 & \\checkmark  \\\\\n   SPIRAL \\tscbig{}  & \\libriltsz{} & 500k &  232.0 & - \\\\\n   \\bottomrule\n   \\end{tabular}\n   }\n  \\end{table}\n  \nIn pre-training, we optimize with Adam~\\citep{kingma2017adam} optimizer, warming up the learning rate for the first 8\\% of updates to a peak of 3e-3. Then the learning rate decays to 0 with a cosine schedule. The moving average update rate $\\alpha_t$ of teacher's weight also follows a cosine schedule~\\citep{Grill2020}. %For \\tscbase{} model, we increase the value of $\\alpha_t$ from $0.995$ to $1.0$. For \\tscbig{} model, the value of $\\alpha_t$ from $0.990$ to $0.999$.\nWe increase $\\alpha_t$ from $0.995$ to $1.0$ and from $0.990$ to $0.999$ for \\tscbase{}  and \\tscbig{} models respectively.\nWe train the \\tscbase{} model with batch size of 24 per GPU for 200k steps on 16 V100 GPUs, which takes about 1.3 days.\nFor the \\tscbig{} model, we train with batch size of 20 per GPU for 500k steps on 32 V100 GPUs, which takes about 7.25 days.\nAs shown in Table~\\ref{tab:training_cost}, there is a significant reduction of training cost (GPU days) compared to wav2vec 2.0~\\citep{wav2vec2}. SPIRAL requires 80\\% and 65\\% less training cost for \\tscbase{} and \\tscbig{} respectively.\nNote that mix-precision training is not applied for SPIRAL yet. %We expect further speed-up when mix-precision training is implemented.\n\nFor fine-tuning, we optimize with Adam and a tri-state rate schedule where the learning rate is warmed up for the first\n10\\% of updates to 3e-5, held constant for the next 40\\% and then linearly decayed to zero following \\citet{wav2vec2}. We fine-tune \\tscbase{} and \\tscbig{} with batch size of 14 and 18 per GPU respectively on 8 GPUs for 80k steps on train-clean-100. We fine-tune \\tscbig{} with batch size of 10 per GPU on 16 GPUs for 320k steps on LS-960. We apply SpecAugment for whole-model fine-tuning but not for frozen fine-tuning.\nFor multi-condition pre-training and fine-tuning, \nwe randomly perturb each utterance with additive noise with 50\\% probability before applying SpecAugment. SNR is uniformly sampled from 0-30 dB. \n\n\\subsection{Language Model and Decoding}\n\nWe use a word-level Transformer LM~\\citep{baevski2018adaptive} trained on \\libri{} LM corpus which is identical to \\citet{synnaeve2020endtoend}. For low-resource ASR setting, we also evaluate SPIRAL \\tscbase{} with the official LibriSpeech 4-gram LM.\nWe observe that models fine-tuned with subword units performs worse than models fine-tuned with character units when decoding with word-level LM.\nTherefore, we apply character-based models for LM decoding, which is the same setting as wav2vec 2.0. \nThe results of LM decoding with subword-based models are available in Appendix~\\ref{app:lm_decoding_units_comparison}.\n\nAs output frame rate of pre-trained SPIRAL encoder is low (80ms), the output sequence may be too short for character units. To reuse the pre-trained encoder, we devise an upsampling strategy for the SPIRAL encoder output in fine-tuning stage.\nWe apply a 1-D convolution layer to project the original encoder output of dimension $d$ into a vector of dimension $4d$. At each time-step, we reshape the projected output vector from $(1, 4d)$ to $(4, d)$. The frame rate now becomes 20ms. \nThen we feed the upsampled outputs to convolutional classifier. \n\nWe perform random search for decoding parameters and choose the best parameters according to performance on dev-other with beam 50.\nThe final test performance is measured with beam 500.\nWe use the beam search decoder of \\citet{pratap2019w2l}.\n\n\\section{RESULTS}\n\\label{results}\n\n\\subsection{Evaluation under Low-Resource and High-Resource Labeled Data Settings}\n\n\\begin{table}[t]\n \\caption{ASR results fine-tuned from low-resource train-clean-100. Language models used in decoding are listed in LM. \n We compare SPIRAL \\tscbase{} pre-trained on \\librisz{} and SPIRAL \\tscbig{} pre-trained on \\libriltsz{} with previous methods. \n We report WER (\\%) on \\libri{} dev/test sets.}\n \\label{tab:low_resource}\n \\centering\n \\scalebox{0.98}{\\small\n \\begin{tabular}{lccrrrrr}\n \\toprule\n \\multirow{2}{*}{Model} & Unlabeled & \\multirow{2}{*}{LM} & \\multicolumn{2}{c}{dev} && \\multicolumn{2}{c}{test} \\\\\n \\cline{4-5}\\cline{7-8}\n {} & data & {} & clean & other && clean & other \\\\\n \\midrule\n \\midrule\n \\textbf{Supervised/Semi-Supervised} \\\\\n Hybrid DNN/HMM \\scriptsize{\\citep{L_scher_2019}} & - & 4-gram & 5.0 & 19.5 && 5.8 & 18.6 \\\\\n Iter. pseudo-labeling~\\scriptsize{\\citep{xu20b_interspeech}} \n & \\libriltsz{} & 4-gram+Transf. & 3.19 & 6.14 && 3.72 & 7.11 \\\\\n Noisy student~\\scriptsize{\\citep{park2020improved}} & \\libriunsz{} & LSTM & 3.9 & 8.8 && 4.2 & 8.6 \\\\\n\\midrule\\midrule\n   \\textbf{Self-supervised} \\\\\n wav2vec 2.0 \\tscbase{}~\\scriptsize{\\citep{wav2vec2}} & \\librisz{} & - & 6.1 & 13.5 && 6.1 & 13.3 \\\\\n SPIRAL \\tscbase{} frozen (ours)  & \\librisz{} & - & 7.9 & 12.7 && 7.6 & 13.0 \\\\\n SPIRAL \\tscbase{} (ours)  & \\librisz{} & - & 5.5 & 11.1 && 5.4 & 11.2 \\\\\n \\midrule\n wav2vec 2.0 \\tscbase{}~\\scriptsize{\\citep{wav2vec2}} & \\librisz{} & 4-gram &  2.7 &  7.9 &&  3.4 & 8.0 \\\\\n SPIRAL \\tscbase{} (ours)  & \\librisz{} & 4-gram & 2.7 & 7.0 && 3.3 & 7.5 \\\\\n \\midrule\n wav2vec 2.0 \\tscbase{}~\\scriptsize{\\citep{wav2vec2}} & \\librisz{} & Transf. & 2.2 & 6.3 && 2.6 & 6.3 \\\\\n SPIRAL \\tscbase{} (ours)  & \\librisz{} & Transf. & 2.3 & 5.8 && 2.7 & 6.1 \\\\\n  \\midrule\n wav2vec 2.0 \\tscbig{}~\\scriptsize{\\citep{wav2vec2}} & \\libriltsz{} & - & 3.3 & 6.5 &&  3.1 & 6.3 \\\\\n SPIRAL \\tscbig{} frozen (ours)  & \\libriltsz{} & - & 7.1 &  9.2 && 6.6 & 9.7 \\\\\n SPIRAL \\tscbig{} (ours)  & \\libriltsz{} & - &  3.3 & 5.9 && 3.3 &  6.3 \\\\\n    \\midrule\n wav2vec 2.0 \\tscbig{}~\\scriptsize{\\citep{wav2vec2}} & \\libriltsz{} & Transf. & 1.9 & 4.0 && 2.0 & 4.0 \\\\\n SPIRAL \\tscbig{} (ours) & \\libriltsz{} & Transf. & 1.9 & 3.9 && 2.2 & 4.3 \\\\\n \\bottomrule\n \\end{tabular}\n }\n\\end{table}\n\nWe first evaluate our method under a low-resource ASR setting in which we fine-tune the models with 100-hour LibriSpeech data (train-clean-100). \nThe results are shown in Table~\\ref{tab:low_resource}. \nWe evaluate a \\tscbase{} model pre-trained with 960-hour LibriSpeech (LS-960) and a \\tscbig{} model pre-trained with Libri-Light (LL-60K).\nThe frozen \\tscbase{} model performs well, achieving a WER of 13.0\\% on test-other, which is on par with wav2vec 2.0 \\tscbase{}. This suggests that SPIRAL indeed learns meaningful high-level representations in a self-supervised way.\nWhen we fine-tune the whole \\tscbase{} model, the model achieves WER of 5.4\\% and 11.2\\% on test-clean and test-other respectively, outperforming wav2vec 2.0 \\tscbase{} with 11.5\\% and 15.8\\% relative WER reduction.\nWhen decoding with Transformer LM, \nthe \\tscbase{} model achieves WER of 2.7\\% and 6.1\\% on test-clean and test-other respectively. The results are on par with wav2vec 2.0 \\tscbase{}.\n\nThe SPIRAL \\tscbig{} model consists of more parameters and is pre-trained with more data. \nThe model achieves WER of 2.2\\% and 4.3\\% on test-clean and test-other respectively. The significant improvement of \\tscbig{} over \\tscbase{} demonstrates the scalability of SPIRAL. \nThe results of SPIRAL \\tscbig{} are competitive to wav2vec 2.0 \\tscbig{}.\nThis is encouraging, as SPIRAL \\tscbig{} only takes 35\\% of training cost of wav2vec 2.0 \\tscbig{}.\n\nWe further evaluate SPIRAL \\tscbig{} pre-trained with Libri-Light (LL-60K) under a high-resource ASR setting with 960-hour LS-960 as fine-tuning data.\nAs shown in Table~\\ref{tab:high_resource}, the \\tscbig{} model achieves WER of 1.8\\% and 3.5\\% on test-clean and test-other respectively, which are on par with the wav2vec 2.0 \\tscbig{} model.\nWe note that the supervised models and the noisy student model \\citep{park2020improved} in Table~\\ref{tab:high_resource} are autoregressive models.\nOur models are fine-tuned with CTC objective which is non-autoregressive and generally inferior to autoregressive models.\nWe use CTC objective for its simplicity and comparability to previous speech pre-training methods. \n\nWe consider SPIRAL as a preferred alternative to wav2vec 2.0 given that SPIRAL only requires 20\\%$-$35\\% computation cost of wav2vec 2.0. We expect further efficiency improvement when we implement mix-precision training for SPIRAL.\n\n\\begin{table}[t]\n   \\caption{%\n   ASR results fine-tuned from high-resource LS-960. Language models used in decoding are listed in LM. \n   We compare SPIRAL \\tscbig{} pre-trained on \\librilt{}  (\\libriltsz{}) with previous methods. We report WER (\\%) on \\libri{} dev/test sets.}\n   \\label{tab:high_resource}\n   \\centering\n   \\scalebox{0.98}{\\small\n   \\begin{tabular}{lccrrrrr}\n   \\toprule\n   \\multirow{2}{*}{Model} & Unlabeled & \\multirow{2}{*}{LM} & \\multicolumn{2}{c}{dev} && \\multicolumn{2}{c}{test} \\\\\n   \\cline{4-5}\\cline{7-8}\n   {} & data & {} & clean & other && clean & other \\\\\n   \\midrule\\midrule\n   \\textbf{Supervised} \\\\\n   ContextNet~\\scriptsize{\\citep{han2020contextnet}} & - & LSTM & 1.9 & 3.9 && 1.9 & 4.1 \\\\\n   Conformer~\\scriptsize{\\citep{gulati2020conformer}} & - & LSTM & 2.1 & 4.3 && 1.9 & 3.9 \\\\\n   \\midrule\\midrule\n   \\textbf{Semi-supervised} \\\\\n   CTC Transf. + PL~\\scriptsize{\\citep{synnaeve2020end}} & \\libriltsz{} & CLM+Transf. &  2.10 & 4.79 && 2.33 & 4.54 \\\\\n   S2S Transf. + PL~\\scriptsize{\\citep{synnaeve2020end}} & \\libriltsz{} & CLM+Transf. &  2.00 & 3.65 && 2.09 & 4.11 \\\\\n   Iter. pseudo-labeling~\\scriptsize{\\cite{xu20b_interspeech}} & \\libriltsz{} & 4-gram+Transf. & 1.85 & 3.26 && 2.10 & 4.01 \\\\\n   Noisy student~\\scriptsize{\\citep{park2020improved}} & \\libriltsz{} & LSTM & 1.6 & 3.4 && 1.7 & 3.4 \\\\\n   \\midrule\\midrule\n   \\textbf{Self-supervised} \\\\\n   wav2vec 2.0 \\tscbig{}~\\scriptsize{\\citep{wav2vec2}} & \\libriltsz{} & - & 2.1 & 4.5 && 2.2 & 4.5 \\\\\n   SPIRAL \\tscbig{} frozen (ours) & \\libriltsz{} & - & 4.0 & 6.2 &&  3.5 &  6.4 \\\\\n   SPIRAL \\tscbig{} (ours) & \\libriltsz{} & - & 2.1 & 4.3 && 2.2 &  4.6 \\\\\n   \\midrule\n    wav2vec 2.0 \\tscbig{}~\\scriptsize{\\citep{wav2vec2}} & \\libriltsz{} & Transf. & 1.6 & 3.0 && 1.8 & 3.3 \\\\\n    SPIRAL \\tscbig{} (ours) & \\libriltsz{} & Transf. & 1.5 & 3.1 && 1.8 &  3.5 \\\\\n   \\bottomrule\n   \\end{tabular}\n   }\n   \\end{table}\n\n\\subsection{Noise-robust pre-training}\n\n\\begin{table}[t]\n   \\caption{Evaluation on noise-robustness of the models. We use wav2vec 2.0 \\tscbase{} released by the authors as the baseline. The SPIRAL \\tscbase{} models are pre-trained with \\librisz{} and fine-tuned with train-clean-100. We report WER (\\%) on \\libri{} and CHiME-3 real data test sets.}\n   \\label{tab:noisy_test}\n   \\centering\n\n  \\scalebox{0.98}{\\small\n   \\begin{tabular}{lccrrrrrrrrrrr}\n   \\toprule\n   \\multirow{2}{*}{\\tscbase{} model} & Pre-train & Fine-tune & \\multicolumn{2}{c}{Librispeech} && \\multicolumn{4}{c}{CHiME-3} \\\\\n   \\cline{4-5}\\cline{7-10}\n   {} & w/ MCT & w/ MCT & clean & other && ch0 & ch5 & ch1 & ch2 \\\\\n   \\midrule\n   wav2vec 2.0 & -         & -      & 6.1 & 13.3 && 23.2 & 56.1 & 68.3 & 98.1 \\\\\n   SPIRAL & -         & -          & 5.4 & 11.2 &&  24.1 & 52.1 & 58.9 & 92.6 \\\\\n   SPIRAL & -         & \\checkmark & 5.7 & 11.4 && 20.8 & 35.5 & 41.1  & 76.4\\\\\n   SPIRAL & \\checkmark & -         & 5.7 & 11.5 && 20.8 & 33.6 & 38.5 & 74.0 \\\\\n   SPIRAL & \\checkmark & \\checkmark &  5.9 & 11.4 && 20.0 & 31.1 &  35.6 & 69.5 \\\\\n   \\bottomrule\n   \\end{tabular}\n\n  }\n\\end{table}\n\nTo evaluate noise-robustness of the pre-trained models, we compare the effects of applying multi-condition training (MCT) in pre-training or fine-tuning stages of SPIRAL.\nThe results are shown in Table~\\ref{tab:noisy_test}.\nThe vanilla SPIRAL \\tscbase{} model and wav2vec 2.0 \\tscbase{} model deteriorate with significantly higher WER on noisy test data. \n\nOn real noisy test speech data in CHiME-3 for different microphone channels (ch), SPIRAL with multi-condition pre-training significantly improves speech recognition performance.  \nCompared to the model applying MCT solely in fine-tuning, applying MCT both in pre-training and fine-tuning achieves 12.4\\%, 13.3\\% and 9.0\\% relative WER reduction for ch 1, 5 and 2 respectively. There is smaller performance improvement of 3.8\\% relative WER reduction for ch 0, which is a close-talking microphone with the highest SNR. We note that ch 2 faces backwards to the speaker. SNR of the recordings from ch 2 is the lowest, leading to high WER. \nWe note that other pre-training methods including wav2vec 2.0 may benefit from multi-condition training, which are worth for further investigation.\n\n\\subsection{Ablations}\n\\label{sec:Ablations}\n\n\\subsubsection{Input Perturbation and computation noise of teacher}\n\nSPIRAL learns denoised representation of perturbed data. \nBy default, we only apply perturbation to the input of the \\student{}. \nAn alternative method is to perturb both inputs of the \\teacher{} and the \\student{}, and optimize consistency between their representations~\\citep{Grill2020, chen2021exploring}.\nWe conduct experiments to evaluate the effects of perturbing the input and adding computation noise (dropout and LayerDrop) to the \\teacher{}.\nThe results are shown in Table~\\ref{tab:perturb_test} in Appendix \\ref{app:ablation}. The results suggest that applying SpecAugment to \\teacher{}'s input degrades performance significantly.\nPerformance degradation decreases but is still significant with lower ratio and width of the masks.\nThis supports the necessity of representation denoising, and our view of SPIRAL as an extension of self-training in which \\teacher{} network are fed with clean input. \nThe results also support applying computation noise to teacher during pre-training. There is a 15.9\\% relative WER reduction with computation noise. This may be linked to \\citet{dropbayes}.\n\n\\subsubsection{Effects of predictor and projection head}\n\nWe do ablation studies to understand the role of predictor and projection head in SPIRAL.\nThe results are shown in Table~\\ref{tab:predcitor_ablations} in Appendix \\ref{app:ablation}.\nWhen removing the predictor from the \\student{}, we observe performance degradation, but representation collapse does not happen.\nIn the architectures relying on predictor to prevent collapse~\\citep{Grill2020, chen2021exploring},\napplying batch normalization (BN) in the predictor is essential. While in SPIRAL, we observe that BN in the predictor can be replaced by layer normalization (LN) with a small performance degradation.\nWhen the predictor is removed, we observe performance improvement by applying a convolutional projection head.\nThe convolutional projection head is composed of a temporal convolution layer with LN and ReLU, and a linear layer. \nBut applying convolutional projection head to the model with a predictor, there is no further performance improvement. This suggests that convolutional projection head and predictor play a similar role in SPIRAL, and they are not complementary.\n\n\\section{CONCLUSION}\n\\label{conclusion}\nWe presented SPIRAL, a new approach to speech pre-training by learning denoising representation of perturbed data with a teacher-student framework.\nSPIRAL can learn high-level speech representation in self-supervised way. Training a small convolutional classifier on frozen representation of SPIRAL achieves WER of 3.5\\% and 6.4\\% on \\libri{} test-clean and test-other respectively.\nWe show that SPIRAL achieves competitive or better results compared to state-of-the-art speech pre-training methods, with significant reduction of training cost. We investigate multi-condition pre-training and demonstrates that multi-condition pre-training is more effective than solely applying multi-condition training in the fine-tuning stage. We presume SPIRAL as a general pre-training method, which can \napply to other modalities such as images and text. We leave it for future work.\n\n\\newpage\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{Robust Speech Recognition via Large-Scale Weak Supervision}\n\n\\begin{document}\n\n\\twocolumn[\n\\icmltitle{Robust Speech Recognition via Large-Scale Weak Supervision}\n\n\\icmlsetsymbol{equal}{*}\n\n\\begin{icmlauthorlist}\n\\icmlauthor{Alec Radford}{equal,openai}\n\\icmlauthor{Jong Wook Kim}{equal,openai}\n\\icmlauthor{Tao Xu}{openai}\n\\icmlauthor{Greg Brockman}{openai}\n\\icmlauthor{Christine McLeavey}{openai}\n\\icmlauthor{Ilya Sutskever}{openai}\n\\end{icmlauthorlist}\n\n\\icmlaffiliation{openai}{OpenAI, San Francisco, CA 94110, USA}\n\n\\icmlcorrespondingauthor{Alec Radford}{alec@openai.com}\n\\icmlcorrespondingauthor{Jong Wook Kim}{jongwook@openai.com}\n\n\\icmlkeywords{Machine Learning, ICML}\n\n\\vskip 0.3in\n]\n\n\\printAffiliationsAndNotice{\\icmlEqualContribution} % otherwise use the standard text.\n\n\\begin{abstract}\nWe study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.\n\\end{abstract}\n\n\\section{Introduction}\\label{sec:introduction}\n\nProgress in speech recognition has been energized by the development of unsupervised pre-training techniques exemplified by Wav2Vec 2.0 \\cite{baevski2020wav2vec2}. Since these methods learn directly from raw audio without the need for human labels, they can productively use large datasets of unlabeled speech and have been quickly scaled up to 1,000,000 hours of training data \\cite{zhang2021bigssl}, far more than the 1,000 or so hours typical of an academic supervised dataset. When fine-tuned on standard benchmarks, this approach has improved the state of the art, especially in a low-data setting.\n\nThese pre-trained audio encoders learn high-quality representations of speech, but because they are purely unsupervised they lack an equivalently performant decoder mapping those representations to usable outputs, necessitating a fine-tuning stage in order to actually perform a task such as speech recognition\\footnote{\\citet{baevski2021unsupervised} is an exciting exception - having developed a fully unsupervised speech recognition system}. This unfortunately limits their usefulness and impact as fine-tuning can still be a complex process requiring a skilled practitioner. There is an additional risk with requiring fine-tuning. Machine learning methods are exceedingly adept at finding patterns within a training dataset which boost performance on held-out data from the same dataset. However, some of these patterns are brittle and spurious and don't generalize to other datasets and distributions. In a particularly disturbing example, \\citet{radford2021clip} documented a 9.2\\% increase in object classification accuracy when fine-tuning a computer vision model on the ImageNet dataset \\cite{russakovsky2015imagenet} without observing any improvement in average accuracy when classifying the same objects on seven other natural image datasets. A model that achieves ``superhuman'' performance when trained on a dataset can still make many basic errors when evaluated on another, possibly precisely because it is exploiting those dataset-specific quirks that humans are oblivious to \\cite{geirhos2020shortcut}.\n\nThis suggests that while unsupervised pre-training has improved the quality of audio encoders dramatically, the lack of an equivalently high-quality pre-trained decoder, combined with a recommended protocol of dataset-specific fine-tuning, is a crucial weakness which limits their usefulness and robustness. The goal of a speech recognition system should be to work reliably ``out of the box'' in a broad range of environments without requiring supervised fine-tuning of a decoder for every deployment distribution.\n\nAs demonstrated by \\citet{narayanan2018toward}, \\citet{likhomanenko2020rethinking}, and \\citet{chan2021speechstew} speech recognition systems that are pre-trained in a \\textit{supervised} fashion across many datasets/domains exhibit higher robustness and generalize much more effectively to held-out datasets than models trained on a single source. These works achieve this by combining as many existing high-quality speech recognition datasets as possible. However, there is still only a moderate amount of this data easily available. SpeechStew \\cite{chan2021speechstew} mixes together 7 pre-existing datasets totalling 5,140 hours of supervision. While not insignificant, this is still tiny compared to the previously mentioned 1,000,000 hours of unlabeled speech data utilized in \\citet{zhang2021bigssl}.\n\nRecognizing the limiting size of existing high-quality supervised datasets, recent efforts have created larger datasets for speech recognition. By relaxing the requirement of gold-standard human-validated transcripts, \\citet{chen2021gigaspeech} and \\citet{galvez2021people} make use of sophisticated automated pipelines to scale weakly supervised speech recognition to 10,000 and 30,000 hours of noisier training data. This trade-off between quality and quantity is often the right call. Although understudied so far for speech recognition, recent work in computer vision has demonstrated that moving beyond gold-standard crowdsourced datasets such as ImageNet \\cite{russakovsky2015imagenet} to much larger but weakly supervised datasets significantly improves the robustness and generalization of models \\cite{mahajan2018exploring,kolesnikov2020big}.\n\nYet these new datasets are only a few times larger than the sum of existing high-quality datasets and still much smaller than prior unsupervised work. In this work we close that gap, scaling weakly supervised speech recognition the next order of magnitude to 680,000 hours of labeled audio data. We call our approach Whisper\\footnote{If an acronym or basis for the name is desired, WSPSR standing for \\textbf{W}eb-scale \\textbf{S}upervised \\textbf{P}retraining for \\textbf{S}peech \\textbf{R}ecognition can be used.}. We demonstrate models trained at this scale transfer well to existing datasets zero-shot, removing the need for any dataset-specific fine-tuning to achieve high-quality results.\n\nIn addition to scale, our work also focuses on broadening the scope of weakly supervised pre-training beyond English-only speech recognition to be both multilingual and multitask. Of those 680,000 hours of audio, 117,000 hours cover 96 other languages. The dataset also includes 125,000 hours of \\texttt{X$\\rightarrow$en} translation data. We find that for sufficiently large models there is no drawback and even benefits to joint multilingual and multitask training.\n\nOur work suggests that simple scaling of weakly supervised pre-training has been underappreciated so far for speech recognition. We achieve these results without the need for the self-supervision or self-training techniques that have been a mainstay of recent large-scale speech recognition work. To serve as a foundation for further research on robust speech recognition, we release inference code and models at the following URL: \\url{https://github.com/openai/whisper}.\n\n\\section{Approach}\\label{sec:approach}\n\n\\subsection{Data Processing}\\label{subsec:data}\n\nFollowing the trend of recent work leveraging web-scale text from the internet for training machine learning systems, we take a minimalist approach to data pre-processing. In contrast to a lot of work on speech recognition, we train Whisper models to predict the raw text of transcripts without any significant standardization, relying on the expressiveness of sequence-to-sequence models to learn to map between utterances and their transcribed form. This simplifies the speech recognition pipeline since it removes the need for a separate inverse text normalization step in order to produce naturalistic transcriptions. \n\nWe construct the dataset from audio that is paired with transcripts on the Internet. This results in a very diverse dataset covering a broad distribution of audio from many different environments, recording setups, speakers, and languages. While diversity in audio quality can help train a model to be robust, diversity in transcript quality is not similarly beneficial. Initial inspection showed a large amount of subpar transcripts in the raw dataset. To address this, we developed several automated filtering methods to improve transcript quality. \n\nMany transcripts on the internet are not actually human-generated but the output of existing ASR systems. Recent research has shown that training on datasets of mixed human and machine-generated data can significantly impair the performance of translation systems \\cite{ghorbani2021scaling}. In order to avoid learning ``transcript-ese'', we developed many heuristics to detect and remove machine-generated transcripts from the training dataset. Many existing ASR systems output only a limited subset of written language which removes or normalizes away aspects that are difficult to predict from only audio signals such as complex punctuation (exclamation points, commas, and question marks), formatting whitespace such as paragraphs, or stylistic aspects such as capitalization. An all-uppercase or all-lowercase transcript is very unlikely to be human generated. While many ASR systems include some level of inverse text normalization, it is often simple or rule-based and still detectable from other unhandled aspects such as never including commas.\n\nWe also use an audio language detector, which was created by fine-tuning a prototype model trained on a prototype version of the dataset on VoxLingua107 \\cite{valk2021voxlingua107} to ensure that the spoken language matches the language of the transcript according to CLD2. If the two do not match, we don't include the (audio, transcript) pair as a speech recognition training example in the dataset. We make an exception if the transcript language is English and add these pairs to the dataset as \\texttt{X$\\rightarrow$en} speech translation training examples instead. We use fuzzy de-duping of transcript texts to reduce the amount of duplication and automatically generated content in the training dataset.\n\nWe break audio files into 30-second segments paired with the subset of the transcript that occurs within that time segment. We train on all audio, including segments where there is no speech (though with sub-sampled probability) and use these segments as training data for voice activity detection.\n\nFor an additional filtering pass, after training an initial model we aggregated information about its error rate on training data sources and performed manual inspection of these data sources sorting by a combination of both high error rate and data source size in order to identify and remove low-quality ones efficiently. This inspection showed a large amount of only partially transcribed or poorly aligned/misaligned transcripts as well as remaining low-quality machine-generated captions that filtering heuristics did not detect.\n\nTo avoid contamination, we perform de-duplication at a transcript level between the training dataset and the evaluation datasets we thought were at higher risk of overlap, namely TED-LIUM 3 \\cite{Hernandez2018TEDLIUM3T}.\n\n\\begin{figure*}[t]\n\\begin{center}\n\\centerline{\\includegraphics[width=1.0\\textwidth]{figures/approach.pdf}}\n\\caption{\\textbf{Overview of our approach.} A sequence-to-sequence Transformer model is trained on many different speech processing tasks, including multilingual speech recognition, speech translation, spoken language identification, and voice activity detection. All of these tasks are jointly represented as a sequence of tokens to be predicted by the decoder, allowing for a single model to replace many different stages of a traditional speech processing pipeline. The multitask training format uses a set of special tokens that serve as task specifiers or classification targets, as further explained in Section \\ref{sec:multitask}.}\n\\label{fig:approach}\n\\end{center}\n\\vspace{-1em}\n\\end{figure*}\n\n\\subsection{Model}\\label{subsec:model}\n\nSince the focus of our work is on studying the capabilities of large-scale supervised pre-training for speech recognition, we use an off-the-shelf architecture to avoid confounding our findings with model improvements. We chose an encoder-decoder Transformer \\cite{vaswani2017transformer} as this architecture has been well validated to scale reliably. All audio is re-sampled to 16,000 Hz, and an 80-channel log-magnitude Mel spectrogram representation is computed on 25-millisecond windows with a stride of 10 milliseconds. For feature normalization, we globally scale the input to be between -1 and 1 with approximately zero mean across the pre-training dataset. The encoder processes this input representation with a small stem consisting of two convolution layers with a filter width of 3 and the GELU activation function \\cite{hendrycks2016gaussian} where the second convolution layer has a stride of two. Sinusoidal position embeddings are then added to the output of the stem after which the encoder Transformer blocks are applied. The transformer uses pre-activation residual blocks \\cite{child2019generating}, and a final layer normalization is applied to the encoder output. The decoder uses learned position embeddings and tied input-output token representations \\cite{press-wolf-2017-using}. The encoder and decoder have the same width and number of transformer blocks.\nFigure \\ref{fig:approach} summarizes the model architecture.\n\nWe use the same byte-level BPE text tokenizer used in GPT-2 \\cite{sennrich2015neural,radford2019gpt2} for the English-only models and refit the vocabulary (but keep the same size) for the multilingual models to avoid excessive fragmentation on other languages since the GPT-2 BPE vocabulary is English only.\n\n\\subsection{Multitask Format}\\label{sec:multitask}\n\nAlthough predicting which words were spoken in a given audio snippet is a core part of the full speech recognition problem and extensively studied in research, it is not the only part. A fully featured speech recognition system can involve many additional components such as voice activity detection, speaker diarization, and inverse text normalization. These components are often handled separately, resulting in a relatively complex system around the core speech recognition model. To reduce this complexity, we would like to have a single model perform the entire speech processing pipeline, not just the core recognition part. An important consideration here is the interface for the model. There are many different tasks that can be performed on the same input audio signal: transcription, translation, voice activity detection, alignment, and language identification are some examples. \n\nFor this kind of one-to-many mapping to work with a single model, some form of task specification is necessary. We use a simple format to specify all tasks and conditioning information as a sequence of input tokens to the decoder. Since our decoder is an audio-conditional language model, we also train it to condition on the history of text of the transcript in the hope that it will learn to use longer-range text context to resolve ambiguous audio. Specifically, with some probability we add the transcript text preceding the current audio segment to the decoder's context. We indicate the beginning of prediction with a \\texttt{<|startoftranscript|>} token. First, we predict the language being spoken which is represented by a unique token for each language in our training set (99 total). These language targets are sourced from the aforementioned VoxLingua107 model. In the case where there is no speech in an audio segment, the model is trained to predict a \\texttt{<|nospeech|>} token indicating this. The next token specifies the task (either transcription or translation) with an \\texttt{<|transcribe|>} or \\texttt{<|translate|>} token. After this, we specify whether to predict timestamps or not by including a \\texttt{<|notimestamps|>} token for that case. At this point, the task and desired format is fully specified, and the output begins. For timestamp prediction, we predict time relative to the current audio segment, quantizing all times to the nearest 20 milliseconds which matches the native time resolution of Whisper models, and add additional tokens to our vocabulary for each of these. We interleave their prediction with the caption tokens: the start time token is predicted before each caption's text, and the end time token is predicted after. When a final transcript segment is only partially included in the current 30-second audio chunk, we predict only its start time token for the segment when in timestamp mode, to indicate that the subsequent decoding should be performed on an audio window aligned with that time, otherwise we truncate the audio to not include the segment. Lastly, we add a \\texttt{<|endoftranscript|>} token. We only mask out the training loss over the previous context text, and train the model to predict all other tokens. Please see Figure \\ref{fig:approach} for an overview of our format and training setup.\n\n\\subsection{Training Details}\\label{subsec:training-details}\n\nWe train a suite of models of various sizes in order to study the scaling properties of Whisper. Please see Table \\ref{tab:models} for an overview. We train with data parallelism across accelerators using FP16 with dynamic loss scaling and activation checkpointing \\cite{griewank2000algorithm, chen2016training}. Models were trained with AdamW \\cite{loshchilov2017decoupled} and gradient norm clipping \\cite{pascanu2013difficulty} with a linear learning rate decay to zero after a warmup over the first 2048 updates. A batch size of 256 segments was used, and the models are trained for 2\\textsuperscript{20} updates which is between two and three passes over the dataset. Due to only training for a few epochs, over-fitting is not a large concern, and we do not use any data augmentation or regularization and instead rely on the diversity contained within such a large dataset to encourage generalization and robustness. Please see Appendix \\ref{sec:hyperparameters} for full training hyperparameters.\\footnote{After the original release of Whisper, we trained an additional Large model (denoted V2) for 2.5X more epochs while adding SpecAugment \\cite{park2019specaugment}, Stochastic Depth \\cite{huang2016deep}, and BPE Dropout \\cite{provilkov2019bpe} for regularization. Reported results have been updated to this improved model unless otherwise specified.}\n\nDuring early development and evaluation we observed that Whisper models had a tendency to transcribe plausible but almost always incorrect guesses for the names of speakers. This happens because many transcripts in the pre-training dataset include the name of the person who is speaking, encouraging the model to try to predict them, but this information is only rarely inferable from only the most recent 30 seconds of audio context. To avoid this, we fine-tune Whisper models briefly on the subset of transcripts that do not include speaker annotations which removes this behavior.\n\n\\begin{table}[t]\n\\centering\n\\small\n\\begin{tabular}{l|cccc} \\toprule\n    Model & Layers & Width & Heads & Parameters \\\\\n    \\midrule\n    Tiny & 4 & 384 & 6 & 39M \\\\\n    Base & 6 & 512 & 8 & 74M \\\\\n    Small & 12 & 768 & 12 & 244M \\\\\n    Medium & 24 & 1024 & 16 & 769M \\\\\n    Large & 32 & 1280 & 20 & 1550M \\\\\n    \\bottomrule\n\\end{tabular}\n\\caption{Architecture details of the Whisper model family.}\\label{tab:models}\n\\end{table}\n\n\\section{Experiments}\\label{sec:experiments}\n\n\\subsection{Zero-shot Evaluation}\\label{subsec:zeroshot}\n\nThe goal of Whisper is to develop a single robust speech processing system that works reliably without the need for dataset specific fine-tuning to achieve high-quality results on specific distributions. To study this capability, we re-use a wide set of existing speech processing datasets to check whether Whisper is able to generalize well across domains, tasks, and languages. Instead of using the standard evaluation protocol for these datasets, which include both a train and test split, we evaluate Whisper in a zero-shot setting without using any of the training data for each of these datasets so that we are measuring broad generalization.\n\n\\subsection{Evaluation Metrics}\\label{subsec:eval-metric}\n\nSpeech recognition research typically evaluates and compares systems based on the word error rate (WER) metric. However, WER, which is based on string edit distance, penalizes all differences between the model's output and the reference transcript including innocuous differences in transcript style. As a result, systems that output transcripts that would be judged as correct by humans can still have a large WER due to minor formatting differences. While this poses a problem for all transcribers, it is particularly acute for zero-shot models like Whisper, which do not observe any examples of specific datasets transcript formats.\n\nThis is not a novel observation; the development of evaluation metrics that better correlate with human judgement is an active area of research, and while there are some promising methods, none have seen widespread adoption for speech recognition yet. We opt to address this problem with extensive standardization of text before the WER calculation to minimize penalization of non-semantic differences. Our text normalizer was developed through iterative manual inspection to identify common patterns where naive WER penalized Whisper models for an innocuous difference. Appendix \\ref{sec:standardization} includes full details. For several datasets, we observe WER drops of up to 50 percent usually due to a quirk such as a dataset's reference transcripts seperating contractions from words with whitespace. We caution this development procedure comes at a risk of overfitting to the transcription style of Whisper models which we investigate in Section \\ref{subsec:text-normalization-analysis}. We are releasing the code for our text normalizer to allow for easy comparison and to help others study the performance of speech recognition systems in out-of-distribution settings.\n\n\\subsection{English Speech Recognition}\\label{subsec:zero-shot}\n\nIn 2015, Deep Speech 2 \\cite{amodei2015deepspeech2} reported a speech recognition system matched human-level performance when transcribing the LibriSpeech test-clean split. As part of their analysis they concluded: \\textit{``Given this result, we suspect that there is little room for a generic speech system to further improve on clean read speech without further domain adaptation.''} Yet seven years later the SOTA WER on LibriSpeech test-clean has dropped another 73\\% from their 5.3\\% to 1.4\\% \\cite{zhang2021bigssl}, far below their reported human-level error rate of 5.8\\%. Despite this massive and unanticipated further improvement in performance on held-out but in-distribution data, speech recognition models trained on LibriSpeech remain far above human error rates when used in other settings.\nWhat explains this gap between reportedly superhuman performance in-distribution and subhuman performance out-of-distribution?\n\nWe suspect a large part of this gap between human and machine behavior is due to conflating different capabilities being measured by human and machine performance on a test set. This claim may seem confusing at first; if both humans and machines are taking the same test, how can it be that different skills are being tested? The difference arises not in the testing but in how they trained for it. Humans are often asked to perform a task given little to no supervision on the specific data distribution being studied. Thus human performance is a measure of out-of-distribution generalization. But machine learning models are usually evaluated after training on a large amount of supervision from the evaluation distribution, meaning that machine performance is instead a measure of in-distribution generalization. While both humans and machines are being evaluated on the same \\textit{test} data, two quite different abilities are being measured due to a difference in \\textit{train} data.\n\nWhisper models, which are trained on a broad and diverse distribution of audio and evaluated in a zero-shot setting, could potentially match human behavior much better than existing systems. To study whether this is the case (or whether the difference between machine and human performance is due to yet-to-be-understood factors) we can compare Whisper models with both human performance and standard fine-tuned machine learning models and check which they more closely match.\n\n\\definecolor{Highlight}{HTML}{39b54a}  % green\n\n\\begin{figure}[t]\n\\begin{center}\n\\centerline{\\includegraphics[width=1.0\\columnwidth]{figures/human_comparison_v14.pdf}}\n\\caption{\\textbf{Zero-shot Whisper models close the gap to human robustness.} Despite matching or outperforming a human on LibriSpeech dev-clean, supervised LibriSpeech models make roughly twice as many errors as a human on other datasets demonstrating their brittleness and lack of robustness. The estimated robustness frontier of zero-shot Whisper models, however, includes the 95\\% confidence interval for this particular human.}\n\\label{robustness_figure}\n\\end{center}\n\\vspace{-1em}\n\\end{figure}\n\n\\begin{table}[t]\n\\vskip 0.15in\n\\small\n\\begin{center}\n\\begin{tabular}{l|cc|c}\n\\toprule\n& wav2vec 2.0 & Whisper & RER \\\\\nDataset & Large (no LM) & Large V2 & (\\%) \\\\\n\\midrule\nLibriSpeech Clean & \\textbf{2.7} & \\textbf{2.7} & 0.0 \\\\\n\\midrule\nArtie & 24.5 & \\textbf{6.2} & \\color{Highlight}74.7 \\\\\nCommon Voice & 29.9 & \\textbf{9.0} & \\color{Highlight}69.9 \\\\\nFleurs En & 14.6 & \\textbf{4.4} & \\color{Highlight}69.9 \\\\\nTedlium & 10.5 & \\textbf{4.0} & \\color{Highlight}61.9 \\\\\nCHiME6 & 65.8 & \\textbf{25.5} & \\color{Highlight}61.2 \\\\\nVoxPopuli En & 17.9 & \\textbf{7.3} & \\color{Highlight}59.2 \\\\\nCORAAL & 35.6 & \\textbf{16.2} & \\color{Highlight}54.5 \\\\\nAMI IHM & 37.0 & \\textbf{16.9} & \\color{Highlight}54.3 \\\\\nSwitchboard & 28.3 & \\textbf{13.8} & \\color{Highlight}51.2 \\\\\nCallHome & 34.8 & \\textbf{17.6} & \\color{Highlight}49.4 \\\\\nWSJ & 7.7 & \\textbf{3.9} & \\color{Highlight}49.4 \\\\\nAMI SDM1 & 67.6 & \\textbf{36.4} & \\color{Highlight}46.2 \\\\\nLibriSpeech Other & 6.2 & \\textbf{5.2} & \\color{Highlight}16.1 \\\\\n\\midrule\nAverage & 29.3 & \\textbf{12.8} & \\color{Highlight}55.2 \\\\\n\n\\bottomrule\n\\end{tabular}\n\\caption{\\textbf{Detailed comparison of effective robustness across various datasets.} Although both models perform within 0.1\\% of each other on LibriSpeech, a zero-shot Whisper model performs much better on other datasets than expected for its LibriSpeech performance and makes 55.2\\% less errors on average. Results reported in word error rate (WER) for both models after applying our text normalizer.}\n\\label{robustness_table}\n\\end{center}\n\\vspace{-1em}\n\\end{table}\n\nTo quantify this difference, we examine both \\textit{overall} robustness, that is average performance across many distributions/datasets, and \\textit{effective} robustness, introduced by \\citet{taori2020robustness}, which measures the difference in expected performance between a reference dataset, which is usually in-distribution, and one or more out-of-distribution datasets. A model with high effective robustness does better than expected on out-of-distribution datasets as a function of its performance on the reference dataset and approaches the ideal of equal performance on all datasets. For our analysis, we use LibriSpeech as the reference dataset due to its central role in modern speech recognition research and the availability of many released models trained on it, which allows for characterizing robustness behaviors. We use a suite of 12 other academic speech recognition datasets to study out-of-distribution behaviors. Full details about these datasets can be found in Appendix \\ref{sec:datasets}.\n\nOur main findings are summarized in Figure \\ref{robustness_figure} and Table \\ref{robustness_table}. Although the best zero-shot Whisper model has a relatively unremarkable LibriSpeech clean-test WER of 2.5, which is roughly the performance of modern supervised baseline or the mid-2019 state of the art, zero-shot Whisper models have very different robustness properties than supervised LibriSpeech models and out-perform all benchmarked LibriSpeech models by large amounts on other datasets. Even the smallest zero-shot Whisper model, which has only 39 million parameters and a 6.7 WER on LibriSpeech test-clean is roughly competitive with the best supervised LibriSpeech model when evaluated on other datasets. When compared to a human in Figure \\ref{robustness_figure}, the best zero-shot Whisper models roughly match their accuracy and robustness. For a detailed breakdown of this large improvement in robustness, Table \\ref{robustness_table} compares the performance of the best zero-shot Whisper model with a supervised LibriSpeech model that has the closest performance to it on LibriSpeech test-clean. Despite their very close performance on the reference distribution, the zero-shot Whisper model achieves an average relative error reduction of 55.2\\% when evaluated on other speech recognition datasets. \n\nThis finding suggests emphasizing zero-shot and out-of-distribution evaluations of models, particularly when attempting to compare to human performance, to avoid over-stating the capabilities of machine learning systems due to misleading comparisons.\n\n\\subsection{Multi-lingual Speech Recognition}\\label{subsec:multi-lingual}\n\n\\begin{figure}[t]\n\\begin{center}\n\\centerline{\\includegraphics[width=1.0\\columnwidth]{figures/fleurs_asr_v14.pdf}}\n\\caption{\\textbf{Correlation of pre-training supervision amount with downstream speech recognition performance.} The amount of pre-training speech recognition data for a given language is very predictive of zero-shot performance on that language in Fleurs.}\n\\label{fleurs_asr_transfer}\n\\end{center}\n\\vspace{-1em}\n\\end{figure}\n\n\\begin{table}[t]\n\\vskip 0.15in\n\\begin{center}\n\\begin{tabular}{l|cc}\n\\toprule\nModel & MLS & VoxPopuli \\\\\n\\midrule\nVP-10K + FT & - & 15.3 \\\\\nXLS-R (1B) & 10.9 & 10.6 \\\\\nmSLAM-CTC (2B) & 9.7 & 9.1 \\\\\nMaestro & - & \\textbf{8.1} \\\\\n\\midrule\nZero-Shot Whisper & \\textbf{7.3} & 13.6 \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\\textbf{Multilingual speech recognition performance.} Zero-shot Whisper improves performance on Multilingual LibriSpeech (MLS) but is still significantly behind both Maestro, XLS-R, and mSLAM on VoxPopuli.}\n\\label{multilingual_table}\n\\end{center}\n\\vspace{-1em}\n\\end{table}\n\nIn order to compare to prior work on multilingual speech recognition, we report results on two low-data benchmarks: Multilingual LibriSpeech (MLS) \\cite{pratap2020mls} and VoxPopuli \\cite{wang2021voxpopuli} in Table \\ref{multilingual_table}.\n\nWhisper performs well on Multilingual LibriSpeech, outperforming XLS-R \\cite{babu2021xlsr}, mSLAM \\cite{bapna2022mslam}, and Maestro \\cite{chen2022maestro} in a zero-shot setting. We caution that we do use a simple text standardizer for this result which prevents direct comparison or claims of SOTA performance. On VoxPopuli, however, Whisper significantly underperforms prior work and only beats the VP-10K+FT baseline from the original paper. We suspect the underperformance of Whisper models on VoxPopuli could be due to other models including this distribution as a major source for their unsupervised pre-training data and the dataset having significantly more supervised data, which benefits fine-tuning. While MLS has 10 hours of training data per language, the average amount of training data per language is roughly 10$\\times$ higher for VoxPopuli.\n\nThese two benchmarks are somewhat narrow since they only include 15 unique languages, almost all of which are in the Indo-European language family and many of which are high-resource languages. These benchmarks only provide limited coverage and room to study Whisper models multilingual capabilities which include training data for speech recognition in 75 languages. To study the performance of Whisper more broadly we also report performance on the Fleurs dataset \\cite{conneau2022fleurs}. In particular, we were interested in studying the relationship between the amount of training data we have for a given language and the resulting downstream zero-shot performance for that language. We visualize this relation in Figure \\ref{fleurs_asr_transfer}. We find a strong squared correlation coefficient of 0.83 between the log of the word error rate and the log of the amount of training data per language. Checking the regression coefficient for a linear fit to these log-log values results in an estimate that WER halves for every 16$\\times$ increase in training data. We also observed that many of the largest outliers in terms of worse than expected performance according to this trend are languages that have unique scripts and are more distantly related to the Indo-European languages making up the majority of the training dataset such as Hebrew ({\\footnotesize\\textsf{HE}}), Telugu ({\\footnotesize\\textsf{TE}}), Chinese ({\\footnotesize\\textsf{ZH}}), and Korean ({\\footnotesize\\textsf{KO}}). These differences could be due to a lack of transfer due to linguistic distance, our byte level BPE tokenizer being a poor match for these languages, or variations in data quality.\n\n\\subsection{Translation}\\label{subsec:cross-lingual}\n\n\\begin{figure}[t]\n\\begin{center}\n\\centerline{\\includegraphics[width=1.0\\columnwidth]{figures/fleurs_translation_v14.pdf}}\n\\caption{\\textbf{Correlation of pre-training supervision amount with downstream translation performance.} The amount of pre-training translation data for a given language is only moderately predictive of Whisper's zero-shot performance on that language in Fleurs.}\n\\label{translation_transfer}\n\\end{center}\n\\vspace{-1em}\n\\end{figure}\n\n\\begin{table}[t]\n\\vskip 0.15in\n\\begin{center}\n\\begin{tabular}{l|ccc|c}\n\\toprule\n X → English & High & Mid & Low & All \\\\\n\\midrule\nXMEF-X & 34.2 & 20.2 & 5.9 & 14.7 \\\\\nXLS-R (2B) & 36.1 & 27.7 & 15.1 & 22.1 \\\\\nmSLAM-CTC (2B) & 37.8 & 29.6 & 18.5 & 24.8 \\\\\nMaestro & \\textbf{38.2} & 31.3 & 18.4 & 25.2 \\\\\n\\midrule\nZero-Shot Whisper & 36.2 & \\textbf{32.6} & \\textbf{25.2} & \\textbf{29.1} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\\textbf{\\texttt{X$\\rightarrow$en} Speech translation performance.} Zero-shot Whisper outperforms existing models on CoVoST2 in the overall, medium, and low resource settings but still moderately underperforms on high-resource languages compared to prior directly supervised work.}\n\\label{translate_table}\n\\end{center}\n\\vspace{-1em}\n\\end{table}\n\n\\begin{table}[t]\n\\vskip 0.15in\n\\begin{center}\n\\begin{tabular}{l|cc}\n\\toprule\n Language ID & Fleurs \\\\\n\\midrule\nw2v-bert-51 (0.6B) & 71.4 \\\\\nmSLAM-CTC (2B) & \\textbf{77.7} \\\\\n\\midrule\nZero-shot Whisper & 64.5 \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\\textbf{Language identification performance.} Zero-shot Whisper's accuracy at language identification is not competitive with prior supervised results on Fleurs. This is partially due to Whisper being heavily penalized for having no training data for 20 of Fleurs languages.}\n\\label{lang_id_table}\n\\end{center}\n\\vspace{-1em}\n\\end{table}\n\nWe study the translation capabilities of Whisper models by measuring their performance on the \\texttt{X$\\rightarrow$en} subset of CoVoST2 \\cite{wang2020covost}. We compare with Maestro, mSLAM, and XLS-R, the highest-performing prior work. We achieve a new state of the art of 29.1 BLEU zero-shot without using any of the CoVoST2 training data. We attribute this to the 68,000 hours of \\texttt{X$\\rightarrow$en} translation data for these languages in our pre-training dataset which, although noisy, is vastly larger than the 861 hours of training data for \\texttt{X$\\rightarrow$en} translation in CoVoST2. Since Whisper evaluation is zero-shot, it does particularly well on the lowest resource grouping of CoVoST2, improving over mSLAM by 6.7 BLEU. Conversely, the best Whisper model does not actually improve over Maestro and mSLAM on average for the highest resource languages.\n\nFor an additional analysis on an even wider set of languages, we also re-purpose Fleurs, which is a speech recognition dataset, as a translation dataset. Since the same sentences are transcribed for every language we use the English transcripts as reference translations. In Figure \\ref{translation_transfer} we visualize the correlation between the amount of translation training data per language and the resulting zero-shot BLEU score on Fleurs. While there is a clear trend of improvement with increasing training data, the squared correlation coefficient is much lower than the 0.83 observed for speech recognition and only 0.24. We suspect this is partly caused by the noisier training data due to errors in audio language identification. As an example, Welsh ({\\footnotesize\\textsf{CY}}) is an outlier with much worse than expected performance at only 13 BLEU despite supposedly having 9,000 hours of translation data. This large amount of Welsh translation data is surprising, ranking 4th overall for translation data and ahead of some of the most spoken languages in the world like French, Spanish, and Russian. Inspection shows the majority of supposedly Welsh translation data is actually English audio with English captions where the English audio was mis-classified as Welsh by the language identification system, resulting in it being included as translation training data rather transcription data according to our dataset creation rules.\n\n\\subsection{Language Identification}\\label{subsec:language-id}\n\n\\begin{figure}[t]\n\\begin{center}\n\\centerline{\\includegraphics[width=1.0\\columnwidth]{figures/noise-robustness-small.pdf}}\n\\definecolor{C0}{RGB}{31, 119, 180}\n\\definecolor{C2}{RGB}{44, 160, 44}\n\\definecolor{C4}{RGB}{214, 39, 40}\n\\caption{\\textbf{WER on LibriSpeech test-clean as a function of SNR under additive white noise (left) and pub noise (right).} The accuracy of LibriSpeech-trained models degrade faster than the best Whisper model ({\\color{red}$\\bigstar$}). NVIDIA STT models ({\\color{C2}$\\bullet$}) perform best under low noise but are outperformed by Whisper under high noise (SNR $<$ 10 dB). The second-best model under low noise ({\\color{C0}$\\blacktriangledown$}) is fine-tuned on LibriSpeech only and degrades even more quickly.}\n\\label{fig:noise-robustness}\n\\end{center}\n\\vspace{-1em}\n\\end{figure}\n\n\\begin{figure*}[b]\n\\begin{center}\n\\centerline{\\includegraphics[width=1.0\\textwidth]{figures/long-form.pdf}}\n\\caption{\\textbf{Whisper is competitive with state-of-the-art commercial and open-source ASR systems in long-form transcription.} The distribution of word error rates from six ASR systems on seven long-form datasets are compared, where the input lengths range from a few minutes to a few hours. The boxes show the quartiles of per-example WERs, and the per-dataset aggregate WERs are annotated on each box. Our model outperforms the best open source model (NVIDIA STT) on all datasets, and in most cases, commercial ASR systems as well.}\n\\label{fig:long-form}\n\\end{center}\n\\vspace{-1em}\n\\end{figure*}\n\nTo evaluate language identification, we use the Fleurs dataset \\cite{conneau2022fleurs}. The zero-shot performance of Whisper is not competitive with prior supervised work here and underperforms the supervised SOTA by 13.6\\%. However, Whisper is heavily disadvantaged for language identification on Fleurs, since the Whisper dataset contains no training data for 20 of the 102 languages in Fleurs, upper-bounding accuracy at 80.4\\%. On the 82 overlapping languages the best Whisper model achieves 80.3\\% accuracy.\n\n\\subsection{Robustness to Additive Noise}\\label{subsec:noise-robustness}\n\nWe tested the noise robustness of Whisper models and 14 LibriSpeech-trained models by measuring the WER when either white noise or pub noise from the Audio Degradation Toolbox \\cite{mauch2013adt} was added to the audio. The pub noise represents a more natural noisy environment with ambient noise and indistinct chatter typical in a crowded restaurant or a pub. Among the 14 models, twelve are pre-trained and/or fine-tuned on LibriSpeech, and the other two are NVIDIA STT models trained on a mixture dataset similar to prior work like SpeechStew that includes LibriSpeech.\nThe level of additive noise corresponding to a given signal-to-noise ratio (SNR) is calculated based on the signal power of individual examples.\nFigure \\ref{fig:noise-robustness} shows how the ASR performance degrades as the additive noise becomes more intensive. There are many models that outperform our zero-shot performance under low noise (40 dB SNR), which is unsurprising given those models are trained primarily on LibriSpeech, but all models quickly degrade as the noise becomes more intensive, performing worse than the Whisper model under additive pub noise of SNR below 10 dB. This showcases Whisper's robustness to noise, especially under more natural distribution shifts like the pub noise.\n\n\\subsection{Long-form Transcription}\\label{subsec:long-form}\n\nWhisper models are trained on 30-second audio chunks and cannot consume longer audio inputs at once. This is not a problem with most academic datasets comprised of short utterances but presents challenges in real-world applications which often require transcribing minutes- or hours-long audio. We developed a strategy to perform buffered transcription of long audio by consecutively transcribing 30-second segments of audio and shifting the window according to the timestamps predicted by the model. We observed that it is crucial to have beam search and temperature scheduling based on the repetitiveness and the log probability of the model predictions in order to reliably transcribe long audio. The full procedure is described in Section \\ref{subsec:long-form-strategy}.\n\nWe evaluate the long-form transcription performance on seven datasets consisting of speech recordings of various lengths and recording conditions, to cover as diverse a data distribution as possible. These include a long-form adaptation of TED-LIUM3 \\cite{Hernandez2018TEDLIUM3T} concatenated so that each example is a full-length TED talk, a collection of jargon-laden segments taken from The Late Show with Stephen Colbert (Meanwhile), sets of videos/podcasts that has been used as ASR benchmarks in online blogs (Rev16 and Kincaid46), recordings of earnings calls \\cite{del2021earnings}, and the full-length interviews from the Corpus of Regional African American Language (CORAAL) \\cite{gunter2021contextualizing}. Full details about the long-form datasets can be found in Appendix \\ref{sec:datasets}.\n\nWe compare the performance with open-source models as well as 4 commercial ASR services. The results are summarized in Figure \\ref{fig:long-form}, showing the distribution of word error rates from Whisper and the 4 commercial ASR services, as well as the NVIDIA STT Conformer-CTC Large model from the NeMo toolkit \\cite{kuchaiev2019nemo} which performed the best among the open-source models. All commercial ASR services are queried using their default English transcription settings as of September 1st, 2022, and for the NVIDIA STT model we used their buffered inference implementation in the \\texttt{FrameBatchASR} class to enable long-form transcription. The results show that Whisper performs better than the compared models on most datasets, especially on the Meanwhile dataset which is heavy with uncommon words. Additionally, we note the possibility that some of the commercial ASR systems have been trained on some of these publicly available datasets, and therefore these results may not be accurately reflecting the relative robustness of the systems.\n\n\\subsection{Comparison with Human Performance}\\label{subsec:human-comparison}\n\nBecause of ambiguous or indistinct speech as well as labeling errors, there are different levels of irreducible error in each dataset, and with WER metrics from ASR systems alone it is difficult to make sense of how much room for improvement exists in each dataset. To quantify how close Whisper's performance is to the human performance, we selected 25 recordings from the Kincaid46 dataset and used 5 services to obtain transcripts produced by professional transcribers, among which one provides computer-assisted transcription and the other four are entirely human-transcribed. The audio selection covers various recording conditions such as scripted and unscripted broadcast, telephone and VoIP calls, and meetings. Figure \\ref{fig:human-evals} shows the distribution of per-example WERs and aggregate WER across the 25 recordings, where the computer-assisted service has the lowest aggregate WER that is 1.15\\% point better than Whisper's, and the pure-human performance is only a fraction of a percentage point better than Whisper's. These results indicate that Whisper's English ASR performance is not perfect but very close to human-level accuracy.\n\n\\begin{figure}[t]\n\\begin{center}\n\\centerline{\\includegraphics[width=\\columnwidth]{figures/human-evals.pdf}}\n\\caption{\\textbf{Whisper's performance is close to that of professional human transcribers.} This plot shows the WER distributions of 25 recordings from the Kincaid46 dataset transcribed by Whisper, the same 4 commercial ASR systems from Figure \\ref{fig:long-form} (A-D), one computer-assisted human transcription service (E) and 4 human transcription services (F-I). The box plot is superimposed with dots indicating the WERs on individual recordings, and the aggregate WER over the 25 recordings are annotated on each box.}\n\\label{fig:human-evals}\n\\end{center}\n\\vspace{-1em}\n\\end{figure}\n\n\\section{Analysis and Ablations}\\label{sec:ablation}\n\n\\subsection{Model Scaling}\n\n\\begin{figure*}[t]\n\\begin{center}\n\\centerline{\\includegraphics[width=\\textwidth]{figures/scaling_v14.pdf}}\n\\caption{\\textbf{Zero-shot Whisper performance scales reliably across tasks and languages with increasing model size.} Lightly shaded lines represent individual datasets or languages, showing that performance is more varied than the smooth trends in aggregate performance. Large V2 distinguished with a dashed orange line since it includes several changes that are not present for the smaller models in this analysis.}\n\\label{scaling}\n\\end{center}\n\\vspace{-1em}\n\\end{figure*}\n\nA large amount of the promise in weakly supervised training approaches is their potential to use datasets much larger than those in traditional supervised learning. However, this comes with the cost of using data that is possibly much noisier and lower quality than gold-standard supervision. A concern with this approach is that although it may look promising to begin with, the performance of models trained on this kind of data may saturate at the inherent quality level of the dataset, which could be far below human level. A related concern is that as capacity and compute spent training on the dataset increases, models may learn to exploit the idiosyncrasies of the dataset, and their ability to generalize robustly to out-of-distribution data could even degrade. \n\nTo check whether this is the case, we study the zero-shot generalization of Whisper models as a function of the model size. Our analysis is summarized in Figure \\ref{scaling}. With the exception of English speech recognition, performance continues to increase with model size across multilingual speech recognition, speech translation, and language identification. The diminishing returns for English speech recognition could be due to saturation effects from approaching human-level performance as analysis in Section \\ref{subsec:human-comparison} suggests.\n\n\\subsection{Dataset Scaling}\n\nAt 680,000 hours of labeled audio, the Whisper dataset is one of the largest ever created in supervised speech recognition. Exactly how important is the raw dataset size to Whisper's performance? To study this, we trained a series of medium-sized models on subsampled versions of the dataset which are 0.5\\%, 1\\%, 2\\%, 4\\%, and 8\\% of the full dataset size and compared their performance with the same medium-sized model trained on the whole dataset. Early stopping based on the validation loss was used to select model checkpoints for each dataset size. Evaluation was performed on an exponential moving average estimate of the parameters \\cite{polyak1992acceleration} using a smoothing rate of 0.9999 to help reduce the effect of the learning rate not fully decaying to zero for the models trained on the subsampled datasets due to early stopping. Performance on English and multilingual speech recognition and \\texttt{X$\\rightarrow$en} translation is reported in Table \\ref{dataset_scaling_table}.\n\nAll increases in the dataset size result in improved performance on all tasks, although we see significant variability in improvement rates across tasks and sizes. Performance improves rapidly on English speech recognition from 3,000 to 13,000 hours and then slows down noticeably between 13,000 and 54,000 hours. Using the full dataset, which corresponds to another 12.5$\\times$ increase in size results in only a further 1 point drop in WER. This mirrors the diminishing returns observed with model size scaling for English speech recognition and could similarly be explained by saturation effects when approaching human-level performance.\n\n\\begin{table}[t]\n\\vskip 0.15in\n\\begin{center}\n\\begin{tabular}{l|ccc}\n\\toprule\n Dataset & English & Multilingual & X$\\rightarrow$En \\\\\n size & WER ($\\downarrow$) & WER ($\\downarrow$) & BLEU ($\\uparrow$) \\\\\n\\midrule\n3405 & 30.5 & 92.4 & 0.2 \\\\\n6811 & 19.6 & 72.7 & 1.7 \\\\\n13621 & 14.4 & 56.6 & 7.9 \\\\\n27243 & 12.3 & 45.0 & 13.9 \\\\\n54486 & 10.9 & 36.4 & 19.2 \\\\\n681070 & \\textbf{9.9} & \\textbf{29.2} & \\textbf{24.8} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{\\textbf{Performance improves with increasing dataset size. } English speech recognition performance refers to an average over 12 datasets while the Multilingual speech recognition reports performance on the overlapping subset of languages in Fleurs and \\texttt{X$\\rightarrow$en} translation reports average BLEU on CoVoST2. Dataset size reported in hours.}\n\\label{dataset_scaling_table}\n\\end{center}\n\\vspace{-1em}\n\\end{table}\n\nImprovements in WER follow a power-law trend for multilingual speech recognition till 54,000 hours and then deviate from this trend, improving only a further 7 points when increasing to the full dataset size. For \\texttt{X$\\rightarrow$en} translation, performance is practically zero when training on 7,000 hours of audio or less, and then follows a roughly log-linear improvement trend till 54,000 hours before also showing diminishing returns when further scaling to the full dataset size.\n\nThe general trend across tasks of diminishing returns when moving from 54,000 hours to our full dataset size of 680,000 hours could suggest that the current best Whisper models are under-trained relative to dataset size and performance could be further improved by a combination of longer training and larger models. It could also suggest that we are nearing the end of performance improvements from dataset size scaling for speech recognition. Further analysis is needed to characterize ``scaling laws'' for speech recognition in order to decided between these explanations.\n\n\\subsection{Multitask and Multilingual Transfer}\n\nA potential concern with jointly training a single model on many tasks and languages is the possibility of negative transfer where interference between the learning of several tasks results in performance worse than would be achieved by training on only a single task or language. To investigate whether this is occurring, we compared the performance of models trained on just English speech recognition with our standard multitask and multilingual training setup and measured their average performance across our suite of zero-shot English speech recognition benchmarks. We adjust for the amount of FLOPs spent training on the task of English speech recognition as only 65\\% of compute is spent on this task in a joint training setup; analysis would otherwise be confounded by under-training on the task when compared to a same-sized English-only model.\n\nOur results visualized in Figure \\ref{multilingual_multitask_transfer} show that for small models trained with moderate amounts of compute, there is indeed negative transfer between tasks and languages: joint models underperform English-only models trained for the same amount of compute. However, multitask and multilingual models scale better and for our largest experiments outperform their English-only counterparts demonstrating positive transfer from other tasks. For our largest experiments, joint models also slightly outperform English-only models even when not adjusting for compute spent per task.\n\n\\begin{figure}[t]\n\\begin{center}\n\\centerline{\\includegraphics[width=1.0\\columnwidth]{figures/multitask.pdf}}\n\\caption{\\textbf{Multitask and multilingual transfer improves with scale.} For small models, performance on English speech recognition degrades when trained jointly in a multitask and multilingual setup. However, multilingual and multitask models benefit more from scale and eventually outperform models trained on English data only. 95\\% bootstrap estimate confidence intervals are shown.}\n\\label{multilingual_multitask_transfer}\n\\end{center}\n\\vspace{-1em}\n\\end{figure}\n\n\\subsection{Text Normalization}\\label{subsec:text-normalization-analysis}\n\n\\begin{figure}[t]\n\\begin{center}\n\\centerline{\\includegraphics[width=1.0\\columnwidth]{figures/ours-vs-stanford-standardizer.pdf}}\n\\caption{\\textbf{On most datasets, our text normalizer has similar effect on reducing WERs between Whisper models and other open-source models, compared to FairSpeech's normalizer.} For each dataset, the boxplot shows the distribution of relative WER reduction across different models in our eval suite, showing that using our text normalizer generally results in lower WERs than FairSpeech's. On a few datasets our normalizer reduces WER significantly and more so for Whisper models, such as CallHome and Switchboard which have many contractions in the ground truth and WSJ which contains many numerical expressions.}\n\\label{fig:text_normalizer}\n\\end{center}\n\\vspace{-2em}\n\\end{figure}\n\nSince we developed our text normalization jointly with Whisper to discount innocuous word errors, there is a risk that our normalizer is overfitted to fixing Whisper's peculiarities rather than addressing general variation in transcription. To check this, we compared the performance of Whisper using our normalizer versus an independently developed one from the FairSpeech project \\cite{koenecke2020racial}. In Figure \\ref{fig:text_normalizer}, we visualize the differences. On most datasets the two normalizers perform similarly, without significant differences in WER reduction between Whisper and compared open-source models, while on some datasets, namely WSJ, CallHome, and Switchboard, our normalizer reduces the WER of Whisper models' significantly more. The differences in reduction can be traced down to different formats used by the ground truth and how the two normalizers are penalizing them. For example, in CallHome and Switchboard, our standardizer did not penalize differences in common English contractions such as ``you're\" versus ``you are\", and in WSJ, our normalizer standardized the written and spoken forms of numerical and monetary expressions, such as ``sixty-eight million dollars\" versus ``\\$68 million\".\n\n\\begin{table}[t]\n\\scriptsize\n\\setlength{\\tabcolsep}{2pt}\n\\renewcommand{\\arraystretch}{1.2}\n\\vskip 0.15in\n\\begin{center}\n\n\\begin{tabular}{lr|lcccccccr|lc}\n\\toprule\n&&&\n\n\\rotatebox[origin=lc]{90}{TED-LIUM3~} &\n\\rotatebox[origin=lc]{90}{Meanwhile} &\n\\rotatebox[origin=lc]{90}{Kincaid46} &\n\\rotatebox[origin=lc]{90}{Rev16} &\n\\rotatebox[origin=lc]{90}{Earnings-21} &\n\\rotatebox[origin=lc]{90}{Earnings-22} &\n\\rotatebox[origin=lc]{90}{CORAAL} &\n\n&&\n\\rotatebox[origin=lc]{90}{Average}\\\\\n\\midrule\n\nGreedy decoding only &&& 3.95 & 5.16 & 9.69 & 11.7 & 10.7 & 14.0 & 22.0 &&& 11.0 \\\\\n+ Beam search &&& 4.16 & 5.71 & 9.42 & 11.5 & 10.2 & 13.4 & 20.0 &&& 10.6 \\\\\n+ Temperature fallback &&& 4.16 & 5.71 & 9.42 & 11.5 & 10.2 & 13.4 & 20.0 &&& 10.6 \\\\\n+ Voice activity detection &&& 3.56 & \\textbf{4.61} & 9.45 & 11.4 & 10.1 & 13.2 & 19.4 &&& 10.2 \\\\\n+ Previous text conditioning &&& \\textbf{3.42} & 6.16 & 8.72 & \\textbf{11.0} & \\textbf{9.63} & 13.3 & \\textbf{18.1} &&& 10.0 \\\\\n+ Initial timestamp constraint &&& 3.51 & 5.26 & \\textbf{8.41} & 11.5 & 9.73 & \\textbf{12.6} & 19.1 &&& 10.0 \\\\\n\n\\bottomrule\n\\end{tabular}\n\n\\caption{Long-form transcription performance improves incrementally as additional decoding heuristics are employed. Details on each intervention are described in Section \\ref{subsec:long-form-strategy}.}\n\\label{tab:long-form-ablation}\n\\end{center}\n\\vspace{-1em}\n\\end{table}\n\\subsection{Strategies for Reliable Long-form Transcription}\\label{subsec:long-form-strategy}\n\nTranscribing long-form audio using Whisper relies on accurate prediction of the timestamp tokens to determine the amount to shift the model's 30-second audio context window by, and inaccurate transcription in one window may negatively impact transcription in the subsequent windows.\nWe have developed a set of heuristics that help avoid failure cases of long-form transcription, which is applied in the results reported in sections \\ref{subsec:long-form} and \\ref{subsec:human-comparison}.\nFirst, we use beam search with 5 beams using the log probability as the score function, to reduce repetition looping which happens more frequently in greedy decoding. We start with temperature 0, i.e. always selecting the tokens with the highest probability, and increase the temperature by 0.2 up to 1.0 when either the average log probability over the generated tokens is lower than $-1$ or the generated text has a gzip compression rate higher than 2.4. Providing the transcribed text from the preceding window as previous-text conditioning when the applied temperature is below 0.5 further improves the performance.\nWe found that the probability of the \\texttt{<|nospeech|>} token alone is not sufficient to distinguish a segment with no speech, but combining the no-speech probability threshold of 0.6 and the average log-probability threshold of $-1$ makes the voice activity detection of Whisper more reliable.\nFinally, to avoid a failure mode where the model ignores the first few words in the input, we constrained the initial timestamp token to be between 0.0 and 1.0 second.\nTable \\ref{tab:long-form-ablation} shows that adding each of the interventions above incrementally reduces the WER overall, but not evenly across the dataset.\nThese heuristics serve as a workaround for the noisy predictions of the model, and more research would be needed to further improve the reliability of long-form decoding.\n\n\\section{Related Work}\\label{sec:related}\n\n\\paragraph{Scaling Speech Recognition} A consistent theme across speech recognition research has been documenting the benefits of scaling compute, models, and datasets. Early work applying deep learning to speech recognition found improved performance with model depth and size and leveraged GPU acceleration to make training these larger models tractable \\cite{mohamed2009deep}. Further research demonstrated that the benefit of deep learning approaches to speech recognition increased with dataset size, improving from being only competitive with prior GMM-HMM systems when using just 3 hours of TIMIT training data for phone recognition to achieving a 30\\% word error rate reduction when trained on the 2,000 hour Switchboard dataset \\cite{seide2011feature}. \\citet{liao2013large} is an early example of leveraging weakly supervised learning to increase the size of a deep learning based speech recognition dataset by over 1,000 hours. These trends continued with Deep Speech 2 \\cite{amodei2015deepspeech2} being a notable system developing high-throughput distributed training across 16 GPUs and scaling to 12,000 hours of training data while demonstrating continuing improvements at that scale. By leveraging semi-supervised pre-training, \\citet{narayanan2018toward} were able to grow dataset size much further and study training on 162,000 hours of labeled audio. More recent work has explored billion-parameter models \\cite{zhang2020pushing} and using up to 1,000,000 hours of training data \\cite{zhang2021bigssl}.\n\n\\paragraph{Multitask Learning} Multitask learning \\cite{caruana1997multitask} has been studied for a long time. In speech recognition, multi-lingual models have been explored for well over a decade \\cite{schultz2006multilingual}. An inspirational and foundational work in NLP exploring multi-task learning with a single model is \\citet{collobert2011natural}. Multitask learning in the sequence-to-sequence framework \\cite{sutskever2014sequence} using multiple encoders and decoders was investigated in \\citet{luong2015multi}. The use of language codes with a shared encoder/decoder architecture was first demonstrated for machine translation by \\citet{johnson2017google}, removing the need for separate encoders and decoders. This approach was simplified further into the ``text-to-text'' framework of \\citet{mccann2018natural} and popularized by its success with large transformer language models in the work of \\citet{radford2019gpt2} and \\citet{raffel2020exploring}. \\citet{Toshniwal2018MultilingualSR} demonstrated jointly training a modern deep learning speech recognition system on several languages with a single model, and \\citet{Pratap2020MassivelyMA} scaled this line of work significantly to 50 languages with a billion-parameter model. MUTE \\cite{wang2020multitask} and mSLAM \\cite{bapna2022mslam} studied joint training over both text and speech language tasks, demonstrating transfer between them.\n\n\\paragraph{Robustness} The question of how effectively models transfer and how robust they are to distribution shift and other types of perturbations has long been studied and is actively being researched across many fields of machine learning. \\citet{torralba2011databias} highlighted the lack of generalization of machine learning models between datasets over a decade ago. Many other works have shown and continually reiterated how despite high performance on IID test sets, machine learning models can still make many mistakes when evaluated in even slightly different settings \\cite{lake2017building,jia2017adversarial,alcorn2019strike,barbu2019objectnet,recht2019generalize}. More recently, \\citet{taori2020robustness} studied the robustness of image classification models, and \\citet{miller2020nlprobustness} investigated this for question-answering models. A key finding has been that multi-domain training increases robustness and generalization as discussed in the Introduction. This finding has been replicated across many fields in addition to speech recognition including NLP \\cite{hendrycks2020pretrained} and computer vision \\cite{radford2021clip}.\n\n\\section{Limitations and Future Work}\\label{sec:future}\n\nFrom our experimental results, analyses, and ablations, we have noted several limitations and areas for future work.\n\n\\paragraph{Improved decoding strategies.} As we have scaled Whisper, we have observed that larger models have made steady and reliable progress on reducing perception-related errors such as confusing similar-sounding words. Many remaining errors, particularly in long-form transcription seem more stubborn in nature and decidedly non-human/perceptual. They are a combination of failure modes of seq2seq models, language models, and text-audio alignment and include problems such as getting stuck in repeat loops, not transcribing the first or last few words of an audio segment, or complete hallucination where the model will output a transcript entirely unrelated to the actual audio. Although the decoding details discussed in Section \\ref{subsec:long-form-strategy} help significantly, we suspect fine-tuning Whisper models on a high-quality supervised dataset and/or using reinforcement learning to more directly optimize for decoding performance could help further reduce these errors.\n\n\\paragraph{Increase Training Data For Lower-Resource Languages} As Figure \\ref{fleurs_asr_transfer} shows, Whisper's speech recognition performance is still quite poor on many languages. The same analysis suggests a clear route for improvement since performance on a language is very well predicted by the amount of training data for the language. Since our pre-training dataset is currently very English-heavy due to biases of our data collection pipeline, which sourced primarily from English-centric parts of the internet, most languages have less than 1000 hours of training data. A targeted effort at increasing the amount of data for these rarer languages could result in a large improvement to average speech recognition performance even with only a small increase in our overall training dataset size.\n\n\\paragraph{Studying fine-tuning} In this work, we have focused on the robustness properties of speech processing systems and as a result only studied the zero-shot transfer performance of Whisper. While this is a crucial setting to study due to it being representative of general reliability, for many domains where high-quality supervised speech data does exist, it is likely that results can be improved further by fine-tuning. An additional benefit of studying fine-tuning is that it allows for direct comparisons with prior work since it is a much more common evaluation setting.\n\n\\paragraph{Studying the impact of Language Models on Robustness} As argued in the introduction, we suspect that Whisper's robustness is partially due to its strong decoder, which is an audio conditional language model. It's currently unclear to what degree the benefits of Whisper stem from training its encoder, decoder, or both. This could be studied by either ablating various design components of Whisper, such as training a decoder-less CTC model, or by studying how the performance of existing speech recognition encoders such as wav2vec 2.0 change when used together with a language model.\n\n\\paragraph{Adding Auxiliary Training Objectives} Whisper departs noticeably from most recent state-of-the-art speech recognition systems due to the lack of unsupervised pre-training or self-teaching methods. While we have not found them necessary to achieve good performance, it is possible that the results could be further improved by incorporating this.\n\n\\section{Conclusion}\\label{sec:conclusion}\n\nWhisper suggests that scaling weakly supervised pre-training has been underappreciated so far in speech recognition research. We achieve our results without the need for the self-supervision and self-training techniques that have been a mainstay of recent large-scale speech recognition work and demonstrate how simply training on a large and diverse supervised dataset and focusing on zero-shot transfer can significantly improve the robustness of a speech recognition system.\n\n\\subsubsection*{Acknowledgments}\nWe'd like to thank the millions of people who were involved in creating the data used by Whisper. We'd also like to thank Nick Ryder, Will Zhuk, and Andrew Carr for the conversation on the waterfall hike that inspired this project. We are also grateful to the Acceleration and Supercomputing teams at OpenAI for their critical work on software and hardware infrastructure this project used. We'd also like to thank Pamela Mishkin for advising the project from a policy perspective. Finally, we are grateful to the developers of the many software packages used throughout this project including, but not limited, to Numpy \\citep{2020NumPy-Array}, SciPy \\citep{2020SciPy-NMeth}, ftfy \\citep{speer-2019-ftfy}, PyTorch \\citep{NEURIPS2019_9015}, pandas \\citep{reback2020pandas}, and scikit-learn \\citep{scikit-learn}.\n\n\\newpage\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2106.07447v1.tex",
        "arXiv-2201.10207v3.tex",
        "arXiv-2212.04356v1.tex"
    ],
    "group_id": "group_47",
    "response": "### Title: Advances in Self-Supervised Speech Representation Learning: HuBERT, SPIRAL, and Whisper\n\n### Introduction\nThe field of automatic speech recognition (ASR) has seen significant progress in recent years, driven by the development of self-supervised learning techniques and the availability of large-scale datasets. Self-supervised learning (SSL) for speech representation aims to learn meaningful representations directly from raw audio data without explicit labeling, which is particularly advantageous for languages and dialects with limited labeled data. This approach contrasts with traditional supervised learning, which requires extensive human-labeled datasets to achieve high performance. The history of SSL in speech recognition includes early generative models that aimed to capture the underlying structure of speech signals, followed by contrastive learning methods that focus on discriminating between similar and dissimilar audio segments. Recent advancements have introduced masked prediction and clustering techniques to improve the quality of learned representations, leading to state-of-the-art performance on various benchmarks.\n\nDespite these advancements, current SSL methods face unique challenges. Unlike text and images, speech signals are continuous-valued sequences, making it difficult to apply discrete input sequence prediction methods directly. Additionally, the boundaries between sound units are not known, complicating the masked prediction process. These challenges have led researchers to develop innovative solutions, such as offline clustering and iterative refinement of cluster assignments, to generate meaningful targets for the prediction loss. The introduction of these methods has significantly improved the performance of speech recognition models, particularly in low-resource settings.\n\nThis summary focuses on three recent papers that have made significant contributions to the field of SSL for speech recognition: \"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units,\" \"SPIRAL: Self-supervised Perturbation-Invariant Representation Learning for Speech Pre-Training,\" and \"Robust Speech Recognition via Large-Scale Weak Supervision.\" Each paper introduces a novel approach to SSL, with HuBERT utilizing masked prediction of hidden units, SPIRAL employing a teacher-student framework for denoising representations, and Whisper scaling up weakly supervised pre-training to a massive dataset. These papers collectively demonstrate the potential of SSL in enhancing the robustness and generalization of speech recognition models, and they highlight the importance of continuous input processing and noise-robustness in SSL.\n\n### Main Content of Each Paper\n\n#### HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\nHuBERT, introduced by Hsu et al., is a self-supervised speech representation learning method that leverages masked prediction of hidden units. The approach involves an offline clustering step to generate aligned target labels for a BERT-like prediction loss. The key innovation of HuBERT is the focus on masked regions only, forcing the model to learn high-level representations of unmasked inputs to predict the targets of masked ones correctly. This method is inspired by the DeepCluster method for self-supervised visual learning but applies a masked prediction loss specifically to speech sequences.\n\nThe authors pre-trained HuBERT models on the LibriSpeech (960 hours) and Libri-Light (60,000 hours) datasets, both of which are derived from the LibriVox project. The LibriSpeech dataset is widely used for benchmarking ASR systems and consists of English recordings of copyright-free audiobooks. The Libri-Light dataset, on the other hand, is much larger and includes 60,000 hours of English speech data, providing a more diverse training environment. HuBERT models are fine-tuned on various subsets of labeled data, ranging from 10 minutes to 960 hours, and evaluated on the LibriSpeech dev-clean and dev-other subsets.\n\nHuBERT's architecture follows the wav2vec 2.0 model, which includes a convolutional waveform encoder, a BERT encoder, a projection layer, and a code embedding layer. The convolutional waveform encoder generates a feature sequence at a 20ms framerate for audio sampled at 16kHz. The BERT encoder consists of many identical transformer blocks, and the projection layer is used to map the learned latent representations to a specific output dimension. The authors also introduce iterative refinement of cluster assignments, where a new generation of clusters is created by training a discrete latent model over the learned latent representations from a previous iteration.\n\nThe authors conducted extensive experiments to evaluate the effectiveness of HuBERT, including ablation studies on different hyperparameters and cluster ensemble techniques. They found that HuBERT models match or outperform wav2vec 2.0 on all fine-tuning subsets, with the largest model (X-Large) showing up to 19% and 13% relative word error rate (WER) reduction on the dev-other and test-other subsets, respectively. The authors also demonstrated that the quality of cluster assignments improves iteratively, leading to better representation learning. The iterative refinement process, combined with the use of raw waveforms as input, allows HuBERT to achieve superior performance compared to other SSL methods.\n\n#### SPIRAL: Self-supervised Perturbation-Invariant Representation Learning for Speech Pre-Training\nSPIRAL, introduced by the authors of the second paper, is a novel approach to speech pre-training that focuses on learning denoising representations of perturbed data. SPIRAL employs a teacher-student framework, where the teacher network processes clean speech data, and the student network processes perturbed data. The student network is trained to output representations that resemble those of the teacher network, while the teacher network is updated as a moving average of the student's weights. To prevent representation collapse, SPIRAL uses an in-utterance contrastive loss and position randomization on the input to the teacher network.\n\nThe authors pre-trained SPIRAL models on the LibriSpeech (960 hours) and Libri-Light (60,000 hours) datasets, using a suite of 12 other academic speech recognition datasets to evaluate out-of-distribution performance. They found that SPIRAL models achieve competitive or better results compared to state-of-the-art (SOTA) speech pre-training methods, such as wav2vec 2.0 and HuBERT, while being significantly more training-efficient. The authors also demonstrated that multi-condition pre-training, where the student's input is perturbed with various types of additive noise, improves noise-robustness for downstream speech tasks.\n\nSPIRAL's architecture consists of a convolutional waveform encoder, a BERT-like encoder, and a projection layer. The convolutional waveform encoder generates a feature sequence at a 20ms framerate for audio sampled at 16kHz. The BERT-like encoder processes this feature sequence using transformer blocks with convolutional relative position encoding. The projection layer maps the learned latent representations to a specific output dimension. The authors applied adaptive SpecAugment, a technique that adds random noise to the input, to both the teacher and student networks during pre-training.\n\nThe authors fine-tuned SPIRAL models on the LibriSpeech dataset using connectionist temporal classification (CTC) loss, which is a non-autoregressive loss function. They also investigated the ability of SPIRAL to learn high-level representations of speech during pre-training by applying frozen fine-tuning, where only the convolutional classifier on top of the pre-trained encoder is fine-tuned. The authors found that SPIRAL models achieve competitive or better results compared to wav2vec 2.0, with significant improvements in noise-robustness.\n\n#### Robust Speech Recognition via Large-Scale Weak Supervision\nWhisper, introduced by Radford et al., is a large-scale weakly supervised pre-training method for speech recognition that leverages a massive dataset of 680,000 hours of labeled audio data. The dataset is derived from transcripts paired with audio recordings on the internet, and it includes data from 75 languages. Whisper uses an encoder-decoder Transformer architecture, where the encoder processes the input acoustic features, and the decoder predicts the corresponding text. The authors fine-tuned Whisper models on various datasets, including LibriSpeech, Multilingual LibriSpeech (MLS), and VoxPopuli, and evaluated their performance in a zero-shot setting.\n\nWhisper's architecture consists of an encoder with two blocks of temporal convolutions followed by transformer blocks, and a decoder with learned position embeddings and tied input-output token representations. The authors used byte-level BPE (Byte Pair Encoding) text tokenization for the English-only models and refit the vocabulary for multilingual models to avoid excessive fragmentation. They also applied a simple text standardizer to reduce penalization of non-semantic differences during evaluation.\n\nThe authors conducted extensive experiments to evaluate the robustness and generalization capabilities of Whisper models. They found that Whisper models achieve competitive or better performance compared to SOTA supervised and semi-supervised models on various datasets, including LibriSpeech, MLS, and VoxPopuli. Whisper models also demonstrated robustness to additive noise, especially under more natural distribution shifts like pub noise. The authors further investigated the scaling properties of Whisper models and found that performance continues to increase with model size and dataset size, although the improvements become less significant as the models approach human-level performance.\n\n### Commonalities and Innovations\n\n#### Commonalities\nAll three papers focus on improving the robustness and generalization capabilities of speech recognition models through self-supervised learning (SSL). They all leverage large-scale datasets to pre-train their models, with HuBERT and SPIRAL using LibriSpeech and Libri-Light, and Whisper using a dataset of 680,000 hours of labeled audio data. The authors of each paper also emphasize the importance of zero-shot transfer and out-of-distribution generalization, which are critical for real-world applications where fine-tuning on specific datasets is not always feasible.\n\n#### Innovations\n- **HuBERT** introduces a novel approach to SSL by predicting hidden unit assignments of masked segments of continuous input. This method avoids the need for discrete unit discovery techniques, such as vector quantization, and achieves competitive or better results compared to wav2vec 2.0 and other SSL methods. HuBERT also demonstrates the effectiveness of iterative refinement of cluster assignments, where the model learns better representations by refining the cluster assignments over multiple iterations.\n- **SPIRAL** employs a teacher-student framework to learn denoising representations of perturbed data. This method allows for end-to-end training with a single contrastive loss and does not require discrete unit discovery techniques. SPIRAL also introduces multi-condition pre-training to improve noise-robustness for downstream speech tasks. The authors found that SPIRAL models achieve competitive or better results compared to wav2vec 2.0, while being significantly more training-efficient.\n- **Whisper** scales up weakly supervised pre-training to a massive dataset of 680,000 hours of labeled audio data. Whisper models are trained on a diverse set of languages and tasks, including multilingual speech recognition, speech translation, and language identification. The authors found that Whisper models achieve competitive or better performance compared to SOTA supervised and semi-supervised models on various datasets, and they demonstrate the robustness of Whisper models to noise and distribution shifts.\n\n### Comparison of Results\nBoth HuBERT and SPIRAL achieve competitive or better results compared to wav2vec 2.0 on various datasets, including LibriSpeech, MLS, and VoxPopuli. HuBERT models match or outperform wav2vec 2.0 on all fine-tuning subsets, with the largest model (X-Large) showing up to 19% and 13% relative WER reduction on the dev-other and test-other subsets, respectively. SPIRAL models achieve competitive or better results compared to wav2vec 2.0, while being significantly more training-efficient. Whisper models also achieve competitive or better performance compared to SOTA supervised and semi-supervised models on various datasets, with the best model achieving a WER of 1.8% and 3.5% on test-clean and test-other subsets, respectively.\n\nThe authors of each paper also conducted extensive ablation studies to evaluate the impact of different hyperparameters and training techniques on model performance. HuBERT authors found that increasing the model size and pre-training data improves performance, and that iterative refinement of cluster assignments significantly improves the quality of learned representations. SPIRAL authors found that applying SpecAugment to the student's input and adding computation noise to the teacher network during pre-training improves performance. Whisper authors found that performance continues to increase with model size and dataset size, although the improvements become less significant as the models approach human-level performance.\n\n### Conclusion\nThe three papers collectively demonstrate the potential of SSL in enhancing the robustness and generalization capabilities of speech recognition models. HuBERT and SPIRAL achieve competitive or better results compared to wav2vec 2.0 on various datasets, while Whisper scales up weakly supervised pre-training to a massive dataset and achieves competitive or better performance compared to SOTA supervised and semi-supervised models. These findings suggest that SSL methods can be effectively used to pre-train speech recognition models, and that iterative refinement of cluster assignments and multi-condition pre-training can improve the quality of learned representations.\n\nFuture research directions include further investigating the scaling properties of SSL methods, studying the impact of language models on robustness, and exploring the use of auxiliary training objectives to further improve performance. Additionally, there is a need for developing more robust decoding strategies and increasing the amount of training data for lower-resource languages to improve the performance of speech recognition models in a broader range of settings."
}