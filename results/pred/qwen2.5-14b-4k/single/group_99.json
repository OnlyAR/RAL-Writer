{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{\\textsc{NaturalProofs}: Mathematical Theorem Proving in Natural Language}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nUnderstanding and creating mathematics using natural mathematical language -- the mixture of symbolic and natural language used by humans -- is a challenging and important problem for driving progress in machine learning.\nAs a step in this direction, we develop \\textsc{NaturalProofs}, a multi-domain corpus of mathematical statements and their proofs, written in natural mathematical language.\n\\textsc{NaturalProofs} unifies broad coverage, deep coverage, and low-resource mathematical sources, allowing for evaluating both in-distribution and zero-shot generalization.\nUsing \\textsc{NaturalProofs}, we benchmark strong neural methods on mathematical reference retrieval and generation tasks which test a system's ability to determine key results that appear in a proof.\nLarge-scale sequence models show promise \ncompared to classical information retrieval methods, yet their performance and out-of-domain generalization leave substantial room for improvement.\n\\textsc{NaturalProofs} opens many avenues for research on challenging mathematical tasks.\\footnote{Dataset and code available at \\url{https://github.com/wellecks/naturalproofs}.}\n\\end{abstract}\n\n\\section{Introduction}\n\\label{sec:introduction}\n\nSolving the problem of understanding and creating mathematics using \\textit{natural mathematical language} -- the mixture of symbolic and natural language used by humans -- is a path towards developing agents capable of reasoning.\nThe mixture of symbolic and natural text, \nalong with the existence of a formal counterpart, offers a unique setting for studying reasoning that complements research involving natural language alone or purely within a formal system.\nConstructing a mathematical proof involves symbolic manipulation, logical and analogical reasoning, as well as knowledge retrieval.\nCommon sense and natural language abilities are needed to articulate the proof in a concise, comprehensible form.\nMoreover, systems that operate on mathematical text have applications in education and scientific discovery, while bridging informal and formal mathematics can be a key driver of progress in automated reasoning \\citep{carter2013lurch,kang_document-level_2020,szegedy2020promising}.\n\nRecently, techniques from natural language processing have driven advances in \\textit{formalized mathematics} (e.g. \\citet{polu2020generative,rabe2021mathematical,wu2021lime}), in which mathematics is written in a verifiable formal language that resembles source code, such as  Mizar \\citep{urban2006mptp},  Lean \\citep{demoura2015lean}, or Metamath \\citep{megill2019metamath}.\nHowever, this setting does not directly address the \\textit{informal} aspect of human mathematics, which is conveyed with a mixture of symbolic and natural language \\citep{gowers2008princetoncompanion}.\nThis aspect is crucial, since advancing \\textit{human understanding} is a goal of mathematics \\citep{thurston1994proof}, and a significant fraction of mathematical knowledge is in natural language text \\citep{szegedy2020promising}.\n\n\\begin{table}[t]\n\\footnotesize\n\\setlength{\\tabcolsep}{2pt}\n\\begin{center}\n\\begin{tabular}{l p{12cm}}\n\\toprule\n\\textbf{Source} & \\textbf{ProofWiki} \\\\\n\\hline\n\\textbf{Theorem} & \\textbf{Category of Monoids is Category} \\\\\n& Let $\\mathrm{Mon}$ be the category of monoids. \\\\\n& Then $\\mathrm{Mon}$ is a metacategory. \\\\\n\\hline\n\\textbf{Proof} & Let us verify the axioms $(C1)$ up to $(C3)$ for a {\\uline{metacategory}}. We have \\\\\n& {\\uline{Composite of Homomorphisms on Algebraic Structure is Homomorphism}}, verifying $(C1)$. \\\\\n& We have {\\uline{monoid}} $\\left({S, \\circ}\\right)$. Now, $(C2)$ follows from \\\\\n& {\\uline{Identity Mapping is Left Identity and Identity Mapping is Right Identity}}. \\\\\n& Finally, $(C3)$ follows from {\\uline{Composition of Mappings is Associative}}. \\\\\n& Hence $\\mathrm{Mon}$ is a {\\uline{metacategory}}. \\\\\n\\bottomrule\n\\addlinespace[0.2em]\n\\toprule\n\\textbf{Source} & \\textbf{Textbook: Real Analysis} \\\\\n\\hline\n\\textbf{Theorem} & Suppose that  $f$ is continuous on the closed interval $[a,b]$ and differentiable on the \\\\\n& open interval $(a,b),$ and $f(a)=f(b).$\\\\ \n& Then $f'(c)=0$ for some $c$ in the open interval $(a,b).$\\\\\n\\hline\n\\textbf{Proof} & Since $f$ is continuous on $[a,b]$, $f$ attains a maximum and a minimum value on $[a,b]$ (\\uline{Theorem 2.2.9}). If these two extreme values are the same, then $f$ is constant on $(a,b)$, so  $f'(x)=0$ for all $x$ in $(a,b)$. If the extreme values differ, then at least one must be attained at some point $c$ in the open interval  $(a,b)$, and $f'(c)=0$, by \\uline{Theorem 2.3.7}.\\\\\n\\bottomrule\n\\addlinespace[0.2em] \n\\end{tabular}\n\\end{center}\n\\caption{\n    Example theorems and their proofs from $\\textsc{NaturalProofs}$. \n    Given a theorem, the mathematical retrieval task consists of retrieving the \\uline{references} (underlined) that occur in its proof.\n    $\\textsc{NaturalProofs}$ contains data from ProofWiki, Stacks, and two textbooks; we show two sources here and two other sources in \\autoref{tbl:dataset-example-more}.\n    See \\autoref{json-example} and \\autoref{fig:schema} for data format details.\n}\n\\label{tbl:dataset-example}\n\\end{table}\nIn this paper, we describe $\\dsname$, a multi-domain corpus of mathematical statements and their proofs, written in natural mathematical language.\n$\\dsname$ contains \\textit{broad-coverage} data from ProofWiki,\\footnote{\\url{https://proofwiki.org/}} \\textit{deep-coverage} data from the Stacks project,\\footnote{\\url{https://stacks.math.columbia.edu/}} and \\textit{low-resource, real-world} data from mathematics textbooks.\n$\\dsname$ unifies these sources in a common schema and is made publicly available as a resource to drive progress on tasks involving informal mathematics, complementing existing work in this direction (e.g. \\cite{ferreira2020natural,ferreira2020premise,wang2020exploration}).\n\nUsing $\\dsname$, we consider \\textit{mathematical reference retrieval}, an analogue of premise selection~\\citep{alemi2016deepmath,ferreira2020premise}: given a mathematical claim, retrieve the set of references (theorems, lemmas, definitions) that occur in its proof.\nThis task represents a crucial facet of mathematical reasoning, in which a mathematician determines the key results that appear in a proof.\nAs a bridge towards generative tasks using $\\dsname$, we consider \\textit{mathematical reference generation}, which requires additionally recovering the order and number of references in each proof.\n\nIn addition to standard \\textit{in-distribution} evaluation, the multi-domain nature of $\\dsname$ allows for evaluating \\textit{out-of-distribution}, zero-shot generalization. \nWe design an evaluation protocol that tests a system's ability to retrieve references for \\textit{novel} theorems in each setting, and benchmark methods based on large-scale neural sequence models \\citep{devlin2019bert,karpukhin2020dense}, including a strong \\textit{joint retrieval} method that better refines the top of the ranked list, \nas well as an \\textit{autoregressive} variant for reference generation.\nThe neural methods are effective for in-domain retrieval compared to classical techniques,\nyet out-of-distribution generalization, \nleveraging symbolic mathematical content, and fully recovering a proof's references remain as fundamental challenges.\n\\textsc{NaturalProofs} opens many possibilities for developing and evaluating machine learning methods on challenging mathematical tasks.\n\n\\section{Related Work}\n\\label{sec:related-work}\n\n\\myparagraph{Machine learning for mathematical theorem proving}\nA large portion of work integrating machine learning with mathematical reasoning has focused on formalized mathematics.\nEarly work by \\citet{urban2006mptp} used machine learning for selecting relevant premises in the Mizar mathematical library that are passed to an automated theorem prover,\nwhich was later explored with deep neural networks \\citep{alemi2016deepmath}.\n\\citet{bansal2019holist} developed the HOList benchmark based on the HOL Light theorem prover, while other benchmark tasks use the Coq \\citep{huang2019gamepad,yang2019learning}, Metamath \\citep{whalen2016holophrasm,wang2020learning,polu2020generative}, or Isabelle \\citep{li2021isarstep} environments.\nThese formalized settings differ from \\textsc{NaturalProofs}, which uses mathematical language as humans write it.\n\\citet{szegedy2020promising} argues for leveraging both informal and formal mathematics through autoformalization.\n\\citet{wang2020exploration} explore translating between informal and formal mathematics, including via a dataset based on ProofWiki, though their dataset is not made available.\n\\citet{ferreira2020natural,ferreira2020premise} propose a classification-based natural language premise selection task and a dataset based on ProofWiki, while $\\dsname$ covers multiple domains and provides evaluation and benchmarks for full retrieval and generative tasks.\n\n\\myparagraph{Mathematics and language benchmarks}\nSeveral datasets evaluate a model's ability to solve multiple-choice algebraic word problems \\citep{roy2015solving,ling2017program,amini2019mathqa} or arithmetic problems \\citep{saxton2018analysing} with varying degrees of natural language. \n\\citet{lample2020deep} evaluate neural sequence models on symbolic integration problems, while \\citet{hendrycks2021measuring} propose a benchmark based on math competition problems.\n\\textsc{NaturalProofs} focuses on theorem proving rather than calculation, which we hypothesize evaluates different skills, and may prove useful in bridging formal and informal settings.\n\n\\myparagraph{Large-scale neural language models}\nLarge-scale unsupervised pretraining of language models has led to significant advances in many natural language processing domains (e.g. \\cite{devlin2019bert,radford2019language,raffel2020t5,brown2020gpt3}).\nRecent work suggests that these models store knowledge in their parameters \\citep{petroni2020language}, are capable of reasoning in mathematical \\citep{rabe2021mathematical,wu2021lime} and language \\citep{clark2020transformers,tafjord2020proofwriter} domains, and are effective for information retrieval tasks \\citep{nogueira2020passage,nogueira2020beyond}.\nThese advances motivate our work, which explores mathematical reasoning in natural language with large-scale language models through a retrieval task.\n\n\\section{The \\textsc{NaturalProofs} Dataset}\n\\label{sec:dataset}\n\n\\begin{table}[t]\n\\begin{minipage}{.48\\linewidth}\n\\centering\n\\includegraphics[width=\\columnwidth]{images/ref_graph.png}\n\\vspace{5pt}\n\\caption{\n    The reference graph. \n    Nodes are \\textit{statements} and edges are \\textit{reference} links.\n    An edge pointing from A to B means that the proof for \\textit{theorem} B refers to \\textit{statement} A.\n    Edges can start from any type of \\textit{statement}, but they always end at a \\textit{theorem}.\n    In our tasks, the dataset is split so that all theorems in the evaluation sets are \\textit{leaf} nodes in the reference graph. \n}\n\\label{fig:ref-graph}\n\\end{minipage}\n\\hfill\n\\begin{minipage}{.48\\linewidth}\n\\setlength{\\tabcolsep}{3pt}\n\\begin{center}\n\\footnotesize\n\\begin{tabular}{rr|r|rrrr}\n\\toprule\n & \\textbf{Source} & \\textbf{All} & \\textbf{PWiki} & \\textbf{Stacks} & \\textbf{RA} & \\textbf{NT} \\\\\n\\toprule\n\\multirow{4}{*}{\\rotatebox[origin=c]{90}{\\textbf{Theorem}}}\n                     & N       & \\textit{32,579} & \\textit{19,734} & \\textit{12,479} & \\textit{298} & \\textit{68} \\\\\n                     & Tokens  & 46.7 & 38.2 & 60.6 & 33.6 & 23.7 \\\\\n                     & Lines   & 5.9 & 3.6 & 9.7 & 8.4 & 4.5 \\\\\n                     & Refs    & 1.8 & 2.8 & 0.2 & 0.0 & 0.0 \\\\\n                     \\midrule\n\\multirow{4}{*}{\\rotatebox[origin=c]{90}{\\textbf{Proof}}}\n                     & N       & \\textit{32,012} & \\textit{19,234} & \\textit{12,479} & \\textit{235} & \\textit{64} \\\\\n                     & Tokens  & 181.5 & 199.3 & 155.5 & 128.9 & 97.2 \\\\\n                     & Lines   & 24.9 & 25.8 & 23.4 & 36.1 & 16.1 \\\\\n                     & Refs    & 5.6 & 7.4 & 3.0 & 1.6 & 0.9 \\\\\n                     \\midrule\n\\multirow{4}{*}{\\rotatebox[origin=c]{90}{\\textbf{Definition}}}\n                     & N       & \\textit{14,230} & \\textit{12,420} & \\textit{1,687} & \\textit{86} & \\textit{37} \\\\\n                     & Tokens  & 48.4 & 45.0 & 73.2 & 58.6 & 32.6 \\\\\n                     & Lines   & 5.0 & 4.2 & 10.7 & 13.3 & 5.1 \\\\\n                     & Refs    & 2.9 & 3.3 & 0.4 & 0.0 & 0.0 \\\\\n                     \\midrule\n\\multirow{4}{*}{\\rotatebox[origin=c]{90}{\\textbf{Other}}}\n                     & N       & \\textit{1,974} & \\textit{1,006} & \\textit{968} & -- & -- \\\\\n                     & Tokens  & 212.1 & 286.1 & 135.2 & -- & -- \\\\\n                     & Lines   & 34.4 & 46.7 & 21.7 & -- & -- \\\\\n                     & Refs    & 5.7 & 9.2 & 2.0 & -- & -- \\\\\n\\bottomrule\n\\end{tabular}\n\\end{center}\n\\caption{\n    \\textsc{NaturalProofs} dataset statistics. Numbers represent mean value, except for \"N\" rows which represent count.\n    \\textbf{RA} is the Real Analysis textbook; \\textbf{NT} is the Number Theory textbook. \n    See \\autoref{tbl:dataset-stats-detail} for detailed statistics.\n}\n\\label{tbl:dataset-stats}\n\\end{minipage}\n\\end{table}\nThe \\textsc{NaturalProofs} Dataset is a large-scale, multi-domain dataset for studying mathematical reasoning in natural language.\n\\textsc{NaturalProofs} consists of 32k theorem statements and proofs, 14k definitions, and 2k other types of pages (e.g. axioms, corollaries)\nderived from three domains: \\textit{broad-coverage} data from ProofWiki, an online compendium of mathematical proofs written by a community of contributors; \\textit{deep-coverage} data from the Stacks project, a collaborative web-based textbook of algebraic geometry; and \\textit{low-resource, real-world} data from mathematics textbooks.\n\\autoref{tbl:dataset-example} shows example theorems and proofs from \\textsc{NaturalProofs}, and \\autoref{tbl:dataset-stats} shows statistics.\n\n\\myparagraph{Multi-domain} \\textsc{NaturalProofs} provides a common schema for mathematical statements, proofs, and the references that appear in each.\nIts multiple domains provide a challenging evaluation setting for models and opens opportunities for investigating domain transfer, out-of-distribution generalization, and methods for low-resource settings.\nThis differs from existing resources that focus only on ProofWiki \\citep{ferreira2020natural,ferreira2020premise}, and reflects shifts in natural language processing towards multi-domain settings \\citep{williams2018mnli,hu2020xtreme}, out-of-distribution generalization \\citep{lebras2020adversarial,hendrycks2020pretrained,thakur2021beir}, \nand few- or zero-shot generalization in resource-constrained settings \\citep{brown2020gpt3,ebrahimi2021americasnli}.\n\n\\textbf{Structure.}\nEach \\textit{statement} in \\textsc{NaturalProofs} \nis either a theorem or a definition.\n\\textsc{NaturalProofs} provides the statement's title, contents, and references.\nThe \\textit{contents} is a list of sequences, where each sequence contains one line of mixed text and \\LaTeX{}, with reference links displayed in their natural language forms.\nA \\textit{theorem} is associated with one or more proofs when available.\nA \\textit{proof} contains a title, contents, and references in the same format as a statement.\nFinally, we collect \\textit{other} pages (e.g. axioms, corollaries).\nA \\textit{reference} is a theorem, definition, or other page that is linked to within the contents of a statement or proof.\n\\autoref{fig:schema} shows the data format for theorems, definitions, and proofs in $\\textsc{NaturalProofs}$.\nAll statements and the reference links connecting them form a \\textit{reference graph}, shown in \\autoref{fig:ref-graph}.\nThe reference graph can contain cycles, e.g. \\texttt{Pythagoras's Theorem} and \\texttt{Sum of Squares of Sine and Cosine} refer to each other in their proofs.\n\n\\textbf{Data sources and preprocessing.}\nWe describe how we retrieve data from each source and give an overview of preprocessing; for full details see Appendix \\ref{apx:ssec:preprocess} and the Jupyter notebooks we release.\n\n\\begin{itemize}[itemsep=0cm,leftmargin=0.3cm,topsep=0cm]\n\\item \\textbf{ProofWiki.}\nWe download the public ProofWiki XML dump,\\footnote{\\url{https://proofwiki.org/xmldump/latest.xml}. We use the November 12, 2020 version. ProofWiki is licensed under CC BY-SA 3.0.} which contains a snapshot of all pages on ProofWiki.\nWe filter pages according to manually designed rules (e.g. redirects, files, categories), and determine page type, title, contents, and references using each page's WikiMedia data structure.\n\\item \\textbf{Stacks.}\nWe pull the Stacks GitHub repo,\\footnote{\\url{https://github.com/stacks/stacks-project}. We use the April 15, 2021 version (commit 4df67b8). Stacks is licensed under GNU Free Documentation License.} which contains multiple \\LaTeX{} files for various sub-topics in algebraic geometry.\nWe extract statements and proofs by \\LaTeX{} environment names.\nFor example, the content enclosed by \\texttt{\\textbackslash{}begin\\{theorem\\}} and \\texttt{\\textbackslash{}end\\{theorem\\}} would be considered a theorem.\n\\item \\textbf{Textbooks.}\nWe searched for open-source math textbooks with rich theorem-proof structures and reference links.\nOf those, we picked \\textit{Introduction to Real Analysis}\\footnote{\\url{https://digitalcommons.trinity.edu/mono/7/}. Retrieved on April 15, 2021. We did not use the supplementary materials. This textbook is licensed under CC BY-NC-SA 3.0.} (\\textbf{RA} in short) by William F. Trench and \\textit{Elementary Number Theory: Primes, Congruences, and Secrets}\\footnote{\\url{https://github.com/williamstein/ent}. Retrieved on April 15, 2021. We provide a script to download and format the publicly available latex source.} (\\textbf{NT} in short) by William Stein.\nWe downloaded the \\LaTeX{} source of each textbook, and similarly extracted statements and proofs by environment names.\nIn both textbooks, every statement is either a theorem or a definition -- there are no statements that fall under \"others\".\n\\end{itemize}\n\n\\section{\\textsc{NaturalProofs} Reference Retrieval and Generation Tasks}\n\\label{sec:task}\n\n\\begin{table}[t]\n\\footnotesize\n\\centering\n\\begin{tabular}{rr|rrr|rr}\n\\toprule\n& \\textbf{Split} & \\textbf{P+S} & \\textbf{ProofWiki} & \\textbf{Stacks} & \\textbf{RA} & \\textbf{NT} \\\\\n\\toprule\n\\textbf{Examples} $|\\mathcal{E}|$ & \\textbf{total} & \\textbf{25,271} & \\textbf{14,698} & \\textbf{10,573} & \\textbf{167} & \\textbf{40} \\\\\n                                  & train & 21,446 & 12,424 & 9,022 & -- & -- \\\\\n                                  & valid & 1,914 & 1,139 & 775 & -- & -- \\\\\n                                  & test  & 1,911 & 1,135 & 776 & 167 & 40 \\\\\n                                  \\midrule\n\\textbf{Refs} $|\\mathcal{R}|$     & train & 42,056 & 28,473 & 13,583 & -- & -- \\\\\n                                  & valid & 45,805 & 30,671 & 15,134 & -- & -- \\\\\n                                  & test  & 45,805 & 30,671 & 15,134 & 384 & 105 \\\\\n                                  \\midrule\n\\textbf{Refs/Ex} $|\\yb|$          & train & 5.9 & 7.5 & 3.6 & -- & -- \\\\\n                                  & valid & 5.6 & 7.5 & 2.9 & -- & -- \\\\\n                                  & test  & 5.6 & 7.4 & 2.9 & 2.2 & 1.5 \\\\\n\\bottomrule\n\\addlinespace[0.4em]\n\\end{tabular}\n\\caption{\n    \\textsc{NaturalProofs} retrieval dataset statistics.\n    \\textbf{P+S} refers to the combined dataset from the ProofWiki and Stacks sources.\n    \\textbf{RA} (Real Analysis) and \\textbf{NT} (Number Theory) are data from mathematical textbook sources that we use for zero-shot evaluation.\n}\n\\label{tbl:dataset-retrieval-stats}\n\\end{table}\n\\textsc{NaturalProofs} opens many possible machine learning tasks that involve natural mathematical language.\nWe consider \\textbf{mathematical reference retrieval}: given a theorem $\\xb$, retrieve the set of references $\\yb$ that occur in its proof.\nAn example is shown in \\autoref{tbl:dataset-example}, where the task is to retrieve the underlined references given the title and contents of the theorem \\texttt{Category of Monoids is Category}.\nAs a proof is ultimately written as an ordered collection of statements with references often occurring more than once, we also consider \\textbf{mathematical reference generation}: generate the \\textit{sequence} of references that occur in a given theorem's proof. \nThese tasks represent a crucial aspect of theorem proving, in which a mathematician determines the key results that appear in a proof.\n\n\\myparagraph{Reference retrieval and generation}\nEach theorem $\\xb$ has a proof containing a sequence of references $\\yb=(\\rb_1,\\ldots,\\rb_{|\\yb|})$, where each reference $\\rb_m\\in \\mathcal{R}$ is either a theorem, definition, or other statement (see \\S\\ref{sec:dataset}).\nWe consider two tasks: \\textit{retrieval} and \\textit{generation}.\n\nIn the \\textit{retrieval} task, given an input theorem $\\xb$, a model assigns a score to each reference in $\\mathcal{R}$, inducing a ranked list $\\hat{\\rb}^{(1)},\\ldots,\\hat{\\rb}^{(|\\mathcal{R}|)}$.\nThese ranked references are evaluated against the ground-truth reference set using standard retrieval metrics such as mean average precision ($\\textsc{mAP}$), recall (\\textsc{Rec}@$k$), and full recovery ($\\textsc{Full}@k$), which checks whether all references in the proof are in the top-$k$ predicted rankings. \nThis reflects the goal of fully proving a theorem using a fixed number of results.\n\nIn the \\textit{generation} task, a model produces a variable-length sequence of references $(\\hat{\\rb}_1,\\ldots,\\hat{\\rb}_{|\\hat{\\yb}|})$ given an input $\\xb$, with the goal of exactly matching the ground-truth reference sequence $(\\rb_1,\\ldots,\\rb_{|\\yb|})$.\nUnlike retrieval, generation requires the model to correctly predict the total number of references, the number of occurrences of each unique reference, and their orders in the proof.\n\n\\myparagraph{Input-output examples}\nUsing \\textsc{NaturalProofs}, we derive examples of the  form $(\\xb,\\yb)$, where $\\xb=(x_1,\\ldots,x_T)$ is a theorem, and $\\yb=(\\rb_1,\\ldots,\\rb_{|\\yb|})$ is the sequence of references that occur in the proof of $\\xb$.\nFor retrieval, we transform each sequence into a set $\\yb=\\{\\rb_1,\\ldots,\\rb_{|\\yb|}\\}$.\nThe set of all references, $\\mathcal{R}$, consists of theorems, definitions, and other statements (see \\S\\ref{sec:dataset}).\nWe use theorems with at least one proof that has at least one reference, \nresulting in a dataset with roughly 25k examples and a reference set $\\mathcal{R}$ with 46k unique references.\nWe partition the dataset into ProofWiki-only, Stacks-only, and textbook-only datasets.\n\\autoref{tbl:dataset-retrieval-stats} summarizes the size, total references, and average references per example in each dataset.\n\n\\myparagraph{Training and evaluation splits}\nWe design training and evaluation splits that reflect the real-world scenario of proving \\textit{newly seen} theorems at evaluation time.\nThis requires careful attention, since \nnaively sampling evaluation examples would yield evaluation theorems that appear as references in the training set.\nTo ensure that the theorems in the evaluation set have no overlap with the references in the training set,\nwe form an evaluation set using a randomly sampled subset of \\textit{reference graph leaf nodes}, and use the remaining nodes as the training set (\\autoref{fig:ref-graph}).\nWe use roughly half of the evaluation set for validation and the other half for testing.\nSince evaluation theorems are not referred to in training examples, the reference set for training is smaller than that for evaluation (\\autoref{tbl:dataset-retrieval-stats}).\n\n\\section{Methods}\n\\label{sec:methods}\nAs benchmark methods for our tasks, we introduce two \\textit{parallel retrieval} methods, and a \\textit{sequential retrieval} method trained for sequence generation. \nSee Appendix~\\ref{apx:experiments} for further implementation details.\n\n\\myparagraph{Parallel retrieval}\nGiven a theorem $\\xb$, a retrieval model should assign high scores to references in the proof of $\\xb$ and low scores to all other references, which corresponds to minimizing,\n\\begin{align}\n\\label{eqn:loss-exact}\n    \\mathcal{L}(\\xb,\\yb) &= \\mathrm{KL}\\left(p_*(\\mathcal{R}|\\xb)\\|p_{\\theta}(\\mathcal{R}|\\xb)\\right)\\\\\n    &\\propto -\\sum_{\\rb\\in \\yb}\\log \\frac{\\exp\\left(s_\\theta(\\xb,\\rb)\\right)}{\\sum_{\\rb'\\in\\mathcal{R}}\\exp\\left(s_{\\theta}(\\xb,\\rb')\\right)} + \\text{const},\n\\end{align}\nwhere each distribution is over reference indices (i.e. in $\\Delta^{(|\\mathcal{R}|)}$),  and $p_*(\\rb|\\xb)\\propto \\mathbb{I}[\\rb\\in \\yb]$. \nThe denominator requires scores $s_{\\theta}(\\xb,\\rb)$ for all $|\\mathcal{R}|$ references, making backpropagation too expensive when a large-scale neural model is used to compute reference representations.\nAs a result we consider two variants: a \\textit{pairwise} model that approximates \\autoref{eqn:loss-exact}, and a \\textit{joint} model that computes \\autoref{eqn:loss-exact} but with implicit vector representations of each reference.\n\n\\myparagraph{Pairwise parameterization} \nThis model contrasts each positive reference with a set of negatives,\n\\begin{align}\n    \\mathcal{L}(\\xb, \\rb, \\yb_-) &= -\\log \\frac{\\exp(s_\\theta(\\xb, \\rb))}{\\exp(s_\\theta(\\xb, \\rb))+\\sum_{\\rb_-\\in \\yb_-}\\exp(s_\\theta(\\xb, \\rb_-))},\n\\end{align}\nwhere $\\rb$ is a reference that occurs in the proof of $\\xb$, and $\\yb_-$ is a (small) set of negative references.\n\nWe call this a pairwise parameterization since the score of each reference against the theorem x is computed independently of the other references,\n$s_{\\theta}(\\xb,\\rb)=f_{\\theta_1}^{\\text{thm}}(\\xb)^\\top g_{\\theta_2}^{\\text{ref}}(\\rb)$.\nThis model represents retrieval methods such as the dense passage retriever \\citep{karpukhin2020dense} and similar methods \\citep{nogueira2020passage}, and allows for evaluating large-scale sequence models, in our case BERT~\\citep{devlin2019bert}, on mathematical reference retrieval.\n\n\\myparagraph{Joint parameterization}\nThe second model scores all references in a single pass,\n\\begin{align}\n    p_{\\theta}(\\mathcal{R}\\mid \\xb) &= \\text{softmax}\\left(\\mathbf{R}f_{\\theta}(\\mathbf{x})\\right),\n\\end{align}\nwhere $\\mathbf{R}\\in \\mathbb{R}^{|\\mathcal{R}|\\times d}$ is a reference embedding matrix and $f_{\\theta}(\\mathbf{x})\\in \\mathbb{R}^d$ is a neural theorem encoder.\nThis model allows for computing \\autoref{eqn:loss-exact} exactly in our setting, \nbut it must learn implicit representations of each reference, i.e. without observing reference contents.\nTo give the model access to representations that were learned using reference contents, we populate its embedding matrix as,\n\\begin{align}\n\\label{eqn:rmatrix}\n    \\mathbf{R}=\\begin{bmatrix}\n         \\horzbar & g^{\\text{ref}}(\\rb_1) & \\horzbar\\\\\n         &\\ldots &\\\\\n         \\horzbar & g^{\\text{ref}}(\\rb_{|\\mathcal{R}|}) & \\horzbar\\\\\n\\end{bmatrix},\n\\end{align} \nwhere $g^{\\text{ref}}(\\xb)$ is obtained by pretraining an independent model. \n\n\\myparagraph{Sequential generation and retrieval}\nFinally, we consider an autoregressive model,\n\\begin{align}\n\\label{eqn:autoreg}\n    p_{\\theta}(\\rb_1,\\ldots,\\rb_{|\\yb|}\\mid\\xb) &= \\prod_{t=1}^{|\\yb|+1}p_{\\theta}(\\rb_t|\\rb_{<t}, \\xb),\n\\end{align}\nwhere $\\rb_{|\\yb|+1}$ is a special $\\eos$ token denoting the end of the reference sequence.\nThe autoregressive model is trained to maximize the log-likelihood of ground-truth reference sequences.\nUnlike the parallel retrieval models, this model predicts the order and total number of references and can predict multiple occurrences of each reference. It also adjusts its predictions based on preceding predictions.\n\nFor generation, a standard decoding algorithm (e.g. beam search) is used to generate a reference sequence $\\hat{\\yb}=(\\hat{\\rb}_1,\\ldots,\\hat{\\rb}_{|\\hat{\\yb}|}\\eos$).\nFor retrieval, we populate a ranked list using generations $\\{\\hat{\\rb}_1,\\ldots,\\hat{\\rb}_{|\\hat{\\yb}|}\\}$ followed by references ordered according to the first step's probabilities, $p_{\\theta}(\\rb_{1}|\\xb)$.\n\n\\section{Experiments}\n\\label{sec:experiments}\nFirst, we benchmark the neural retrieval methods (\\S\\ref{sec:methods}) on mathematical reference retrieval in terms of their \\textit{in-domain} performance (\\autoref{tbl:retrieval-main}) and their \\textit{out-of-domain} performance on an evaluation set formed from the textbooks in $\\dsname$ (\\autoref{tbl:retrieval-ood}).\nWe perform several analyses to better understand each methodâ€™s strengths, weaknesses, and the factors that contribute to their performance.\n\n\\myparagraph{In-domain performance} The BERT-based retrieval models show strong in-domain performance compared to the classical TF-IDF and naive baselines in terms of average precision, recall, and the ability to fully recover all true references within the top-$k$ results, as seen in \\autoref{tbl:retrieval-main}.\nOn both ProofWiki and Stacks, the pairwise models outperform TF-IDF, with improvements that are consistent across reference types (Appendix~\\autoref{tbl:retrieval-type}).\n\nJoint parameterization substantially improves over the pairwise models that are the starting point of joint training.\nOn ProofWiki, the joint model ranks roughly 4 out of every 10 true references within its top 10 rankings (R@10 42.45) compared to 1 out of 10 for TF-IDF, and an impressive 75\\% within its top 100.\nFor roughly half of the theorems, the joint model's top 100 references contain \\textit{all} of the references \nneeded to prove the theorem (Full@100 50.22).\nOn Stacks the recall@10 is similar at roughly 40\\%, with a higher full recovery rate of 66\\% for the top 100 results.\n\nThe gains from the joint parameterization are most prominent on ProofWiki, e.g. increasing mAP from 16.82 to 36.75.\nJoint parameterization particularly excels at refining the top of the ranked list compared to pairwise parameterization; the percentage improvement in the @10 metrics are larger than those for @100 metrics.\nOn Stacks, the improvements are more modest: though mAP improves by 40\\%, the other metrics are relatively close, suggesting that advances beyond the joint model are needed.\nThis demonstrates the importance of evaluating on multiple domains: each domain presents novel challenges for driving advances in modeling.\nFinally, the BERT models trained on both ProofWiki and Stacks (\\textbf{BERT (P+S)}) show the possibility of training a single multi-domain model, albeit with lower per-domain performance than the models trained individually on each domain.\n\n\\begin{table*}[t]\n\\setlength{\\tabcolsep}{4pt}\n\\begin{center}\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{rr | rrrrr | rrrrr}\n\\toprule\n& & \\multicolumn{5}{c}{\\textbf{ProofWiki}} & \\multicolumn{5}{c}{\\textbf{Stacks}} \\\\\n\\toprule\n& & \\textbf{mAP} & \\textbf{R@10} & \\textbf{R@100} & \\textbf{Full@10} & \\textbf{Full@100} & \\textbf{mAP} &  \\textbf{R@10} & \\textbf{R@100} & \\textbf{Full@10} & \\textbf{Full@100} \\\\\n\\toprule\n\\multicolumn{2}{r|}{\\textbf{Random}}     & 0.04 & 0.00 & 0.19 & 0.00 & 0.00 & 0.07 & 0.05 & 0.60 & 0.00 & 0.13 \\\\\n\\multicolumn{2}{r|}{\\textbf{Frequency}}  & 3.38 & 5.90 & 24.30 & 0.44 & 2.29 & 0.91 & 1.76 & 11.27 & 0.13 & 2.45 \\\\\n\\multicolumn{2}{r|}{\\textbf{TF-IDF}}     & 6.19 & 10.27 & 23.09 & 4.14 & 9.43 & 13.64 & 25.46 & 47.36 & 18.94 & 37.76 \\\\\n\\midrule\n\\multirow{2}{*}{\\textbf{BERT (P+S)}} & +\\textbf{pair} & 13.54 & 20.10 & 58.75 & 6.17 & 31.28 & 18.58 & 34.42 & 71.80 & 28.48 & 65.21 \\\\\n& +\\textbf{joint} & 32.71 & 37.59 & 73.72 & 17.71 & 48.90 & 26.88 & 35.71 & 72.68 & 28.99 & 66.11 \\\\\n\\multirow{2}{*}{\\textbf{BERT (P/S)}} & +\\textbf{pair} & 16.82 & 23.73 & 63.75 & 7.31 & 38.50 & 20.93 & 37.43 & \\textbf{74.21} & 30.03 & \\textbf{66.37} \\\\\n& +\\textbf{joint} & \\textbf{36.75} & \\textbf{42.45} & \\textbf{75.90} & \\textbf{20.35} & \\textbf{50.22} & \\textbf{28.32} & \\textbf{39.10} & 73.61 & \\textbf{31.96} & 65.59 \\\\\n\\bottomrule\n\\end{tabular}}\n\\caption{\n    \\textit{In-domain} performance on the mathematical reference retrieval task (test set).\n    \\textbf{BERT (P/S)} is finetuned on the part of dataset with the same source as the evaluation set, whereas \\textbf{BERT (P+S)} is finetuned on the combined dataset from ProofWiki and Stacks sources.\n    Recall is micro-averaged.\n}\n\\label{tbl:retrieval-main}\n\\end{center}\n\\end{table*}\n\\begin{table*}[t]\n\\setlength{\\tabcolsep}{6pt}\n\\begin{center}\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{llllll}\n\\toprule\n\\textbf{Source}&\\multicolumn{2}{l}{\\textbf{ProofWiki}} \\\\\n\\hline\n\\textbf{Theorem}&\\multicolumn{2}{l}{\\textbf{Category of Monoids is Category}} \\\\\n&\\multicolumn{2}{l}{Let $\\mathrm{Mon}$ be the category of monoids.} \\\\\n&\\multicolumn{2}{l}{Then $\\mathrm{Mon}$ is a metacategory.} \\\\\n\\hline\n&\\uline{\\textbf{Ground-Truth Reference}}  & \\uline{\\textbf{Rank (Pairwise)}} & \\uline{\\textbf{Rank (Joint)}}  \\\\\n&Metacategory  & 1 &1\\\\\n&Identity Mapping is Left Identity & 4 & 5\\\\\n&Identity Mapping is Right Identity  & 5 & 4\\\\\n&Monoid  & 11 & 2 \\\\\n&Composition of Mappings is Associative  & 21& 8 \\\\\n&Identity Mapping is Automorphism & 117 & 64 \\\\\n&Composite of Homomorphisms is Homomorphism& 261 & 54 \\\\\n\\hline\n\\textbf{\\uline{Rank}}&\\textbf{\\uline{Reference (Pairwise)}}  &\\multicolumn{2}{l}{\\textbf{\\uline{Reference (Joint)}}} \\\\ \n1&\\textit{Metacategory }                            & \\multicolumn{2}{l}{\\textit{Metacategory}} \\\\\n2&Monoid Category is Category                       & \\multicolumn{2}{l}{\\textit{Monoid}}  \\\\\n3&Monoid Category                                   & \\multicolumn{2}{l}{Identity Morphism} \\\\\n4&\\textit{Identity Mapping is Left Identity }       & \\multicolumn{2}{l}{\\textit{Identity Mapping is Right Identity}}   \\\\\n5&\\textit{Identity Mapping is Right Identity}       & \\multicolumn{2}{l}{\\textit{Identity Mapping is Left Identity}}  \\\\\n6&Category                                          & \\multicolumn{2}{l}{Associative}   \\\\\n7&Composition of Morphisms                          & \\multicolumn{2}{l}{Identity (Abstract Algebra)/Two-Sided Identity}   \\\\\n8&Dual Category is Category                         & \\multicolumn{2}{l}{\\textit{Composition of Mappings is Associative}}   \\\\\n9&Identity Morphism                                 & \\multicolumn{2}{l}{Composition of Morphisms}   \\\\\n10&Morphism Category                                & \\multicolumn{2}{l}{Semigroup} \\\\\n\\bottomrule\n\\end{tabular}}\n\\caption{\n    Retrieval for a representative theorem. Top: predicted ranks for ground-truth references using the pairwise (left) and its joint (right) BERT models. Bottom: top 10 retrievals from the pairwise (left) and joint (right) models.\n    A retrieved reference is italicized when it is a ground-truth reference.\n}\n\\label{tbl:prediction-example-representative}\n\\end{center}\n\\end{table*}\\myparagraph{Qualitative evaluation}\n\\autoref{tbl:prediction-example-representative} shows model predictions for a representative theorem, \\texttt{Category of Monoids is Category}.\nThe pairwise model retrieves three out of seven true references within its top 50 results, while the joint model retrieves five out of seven.\nThe top 10 results for both models are comprised of references that are related to category theory, which is the subject of the theorem.\nThis illustrates the model's ability to retrieve \\textit{relevant} references, while highlighting its inability to always perform the fine-grained distinction between a relevant reference and one that occurs in the ground-truth proof(s).\nArguably, such a system is still useful for providing hints to a user, so long as the user is confident that all of the true references are in a reasonably small set of results.\n\n\\begin{table}[t!]\n\\setlength{\\tabcolsep}{4pt}\n\\begin{center}\n\\resizebox{0.65\\linewidth}{!}{\n\\begin{tabular}{r | rrr | rrr}\n\\toprule\n & \\multicolumn{3}{c}{\\textbf{Real Analysis}} & \\multicolumn{3}{c}{\\textbf{Number Theory}} \\\\\n\\toprule\n& \\textbf{mAP} & \\textbf{R@10} & \\textbf{Full@10} & \\textbf{mAP} & \\textbf{R@10} & \\textbf{Full@10}  \\\\\n\\midrule\n\\textbf{TF-IDF}     &  \\textbf{15.79} & \\textbf{34.65} & \\textbf{27.54} & \\textbf{16.42} & 39.62 & 30.00\\\\\n\\textbf{BERT-pair (P)}  &  13.24 & 24.01 & 19.16 & 15.12 & \\textbf{41.51} & \\textbf{35.00}\\\\\n\\textbf{+joint} & 11.24 & 20.97 & 16.77 &15.85 & 41.51 & 35.00\\\\\n\\textbf{BERT-pair (S)}&11.56 & 21.28 & 14.97 & 12.58 & 26.42 & 20.00 \\\\\n\\textbf{+joint} & 7.04 & 11.55& 9.58& 14.88 & 26.42 & 20.00\\\\\n\n\\bottomrule\n\\end{tabular}}\n\\end{center}\n\\caption{\\textit{Zero-shot} retrieval performance on out-of-domain textbooks.\n}\n\\label{tbl:retrieval-ood}\n\\end{table}\n\\myparagraph{Out-of-domain performance}\nWhile strong in-domain performance drives applications in scenarios where training data is available, an ambitious goal is building a system with mathematical retrieval skills that automatically generalize to new resources.\nTo evaluate the retrieval methods in this zero-shot, out-of-domain setting, we use each textbook from $\\dsname$ as an evaluation set.\nThis tests situations where the same theorem is expressed using different language (e.g. \\autoref{tbl:dataset-example-same}), generalization across data formats, and whether retrieval ability from in-domain training transfers.\n\n\\autoref{tbl:retrieval-ood} shows the results. \nThe pairwise BERT model trained on ProofWiki underperforms TF-IDF on the Real Analysis textbook, and has comparable performance on the Number Theory textbook.\nJoint training did not improve out of domain performance, despite its favorable in-domain impact.\nTraining BERT on ProofWiki outperforms training on Stacks, showing that the training domain impacts out-of-domain generalization. \nProofWiki's broad coverage of mathematics may help the model generalize better than the deep, single-topic coverage in Stacks.\n\nThe BERT models show some evidence of generalizing to out-of-domain mathematical sources, yet they do not show an advantage over traditional retrieval methods despite strong in-domain performance.\nThis aligns with recent findings about neural retrieval models in various zero-shot settings \\citep{thakur2021beir}.\nAn exciting research direction is using $\\dsname$ to develop and evaluate methods which improve not only in-domain performance, but out-of-domain generalization.\n\n\\begin{table}[t]\n\\setlength{\\tabcolsep}{4pt}\n\\begin{center}\n\\small\n\\begin{tabular}{rrrrrrrrrrrr}\n\\toprule\n& & \\multicolumn{5}{c}{\\textbf{Sequence}}& \\multicolumn{2}{c}{\\textbf{Multiset}} & \\multicolumn{3}{c}{\\textbf{Set}}\\\\\n\\cmidrule(lr){3-7}\\cmidrule(lr){8-9}\\cmidrule(lr){10-12}\n& \\textbf{Model} & \\textbf{EM} & \\textbf{Edit}($\\downarrow$) & $\\textbf{BLEU}_4$ & $\\textbf{BLEU}_2$ & \\textbf{Len} & \\textbf{EM} & \\textbf{F1} & \\textbf{EM} & \\textbf{F1} & $\\textbf{BLEU}_1$  \\\\\n\\toprule\n\\multirow{5}{*}{\\rotatebox[origin=c]{90}{\\textbf{Stacks}}} & \\textit{*-set} & 51.74 & 35.70 & 9.75 & 47.73 & 0.97 & 89.03 & 97.04 & 100.0 & 100.0 & 94.09\\\\\n&\\textit{*-multiset} & 49.42 & 38.13 & 9.71 & 47.71 & 1.00 & 100.0 & 100.0 & 100.0 & 100.0 & 100.0\\\\\n&\\textit{*-halfseq} & 0.00 & 70.49 & 6.13 & 12.08 & 0.30 & 0.00 & 56.86 & 0.65 & 58.01 & 16.87\\\\\n\\cmidrule(lr){2-12}\n&Joint& 0.00 & 98.81 & 0.00 & \\textbf{3.42} & 2.82 & 0.00 & \\textbf{19.24} & 0.00 & \\textbf{19.65} & \\textbf{15.15}\\\\\n& Autoregressive & \\textbf{3.87} & \\textbf{90.65} & 0.00 & 2.59 & \\textbf{0.97} & \\textbf{4.00} & 13.14 & \\textbf{4.90} & 15.04 & 10.06\\\\\n\\hline\n\\multirow{5}{*}{\\rotatebox[origin=c]{90}{\\textbf{ProofWiki}}} &\\textit{*-set} & 18.09 & 58.51 & 7.18 & 29.50 & 0.83 & 49.96 & 82.57 & 100.0 & 100.0 & 65.57\\\\\n&\\textit{*-multiset} & 19.23 & 58.09 & 16.68 & 52.89 & 1.00 & 100.0 & 100.0 & 100.0 & 100.0 & 100.0\\\\\n&\\textit{*-halfseq} & 0.00 & 58.84 & 25.88 & 29.17 & 0.41 & 0.00 & 63.33 & 4.21 & 70.26 & 30.55\\\\\n\\cmidrule(lr){2-12}\n&Joint& 0.00 & 93.03 & 0.00 & 6.88 & 1.42 & 0.09 & 25.30 & 0.18 & \\textbf{30.76} & 19.27\\\\\n&Autoregressive& \\textbf{3.69} & \\textbf{84.30} & \\textbf{5.48} & \\textbf{11.90} & \\textbf{1.18} & \\textbf{3.78} & \\textbf{25.61} & \\textbf{4.65} & 28.97 & \\textbf{20.81}\\\\\n\\bottomrule\n\\end{tabular}\n\\end{center}\n\\caption{\n    In-domain \\textit{generation} results.\n    We show the autoregressive model, a retrieval-only baseline using the top-5 predictions from the joint retrieval model,\n    and oracle benchmarks for correctly predicting the first half of the sequence (\\textit{*-halfseq}), the full multiset with randomized order (\\textit{*-multiset}), and the full set with randomized order (\\textit{*-set)}.\n    The best model-based method is  in bold.\n}\n\\label{tbl:generation}\n\\end{table}\n\n\\subsection{Reference Generation}\n\\label{ssec:refgen}\nNext, we establish a benchmark for recovering the \\textit{sequence} of references occurring in the proof of each theorem via the reference generation task (\\S\\ref{sec:task}).\n\n\\myparagraph{Metrics} We evaluate predicted reference sequences against ground-truth sequences using order-aware \\textbf{sequence} metrics, as well as unordered \\textbf{multiset} and \\textbf{set}-based metrics. \nSequence metrics include exact match (\\textbf{EM}), edit-distance (\\textbf{Edit}), standard $\\textbf{BLEU}_4$ score which uniformly weights 1-4 gram precision, $\\textbf{BLEU}_2$ with only 1-2 gram precision,\nand average length ratio $\\frac{\\text{predicted}}{\\text{true}}$ (\\textbf{Len}).\nUnordered metrics include exact match, \\textbf{F1}-score (corpus level), and 1-gram precision $\\textbf{BLEU}_1$.\n\n\\myparagraph{Methods} We use the autoregressive model to generate a reference sequence for each theorem using beam search.\nAs a retrieval-only baseline, we form a sequence using the joint retrieval model's top-5 predictions, ordered by retrieval score.\nTo judge performance and provide a benchmark for future work, we provide three oracle baselines: correctly predicting the first half of the sequence (\\textit{*-halfseq}), the full multiset of references with random order (\\textit{*-multiset}), and the set with random order (\\textit{*-set}).\n\n\\myparagraph{Results} \\autoref{tbl:generation} shows the in-domain generation results.\nThe task is challenging, with the autoregressive model exactly matching the ground-truth sequence roughly 3\\% of the time.\nThe autoregressive model improves over the retrieval-only baseline on order-aware metrics, aside from $\\textbf{BLEU}_2$ on Stacks.\nIt does length-prediction reasonably well, with length-ratios of 0.97 and 1.18, yet the multiset and set metrics indicate that the autoregressive model struggles to correctly predict the correct references, even after discarding order.\nThe oracle baselines indicate substantial room for future improvement-- for instance, predicting only half of each sequence correctly would move ProofWiki $\\textbf{BLEU}_4$ from 5.48 to 25.88.\nDeveloping models along the full spectrum from set-based retrieval, to reference generation, to full proof generation is an exciting use-case for $\\dsname$.\n\n\\subsection{Ablation Studies}\n\n\\myparagraph{Initialization and autoregressive retrieval}\nAs shown in \\autoref{tbl:autoregressive-ablation}, the autoregressive model trained for sequence generation substantially improves over the pairwise retrieval model, yet underperforms the joint model, which is trained specifically for retrieval.\nInitializing the joint and autoregressive models using the pairwise model was necessary for achieving high performance; in particular, the reference information conveyed through the embedding matrix (\\autoref{eqn:rmatrix}) was crucial.\n\n\\begin{table}[t]\n\\begin{minipage}[t]{.33\\linewidth}\n\\setlength{\\tabcolsep}{3pt}\n\\begin{center}\n\\footnotesize\n\\begin{tabular}[t]{ lcccc }\n  \\toprule\n  \\textbf{Init} & \\textbf{Model} & \\textbf{mAP} \\\\\\toprule\n  -- & Pairwise & 16.99\\\\\n  \\hline\n  -- & Autoregressive & 17.77 \\\\ \n  $f^{\\text{thm}}$ & Autoregressive & 25.07 \\\\ \n  $f^{\\text{thm}},\\mathbf{R}$ & Autoregressive & \\textbf{35.37} \\\\\n  \\hline\n  -- & Joint & 18.71 \\\\\n  $f^{\\text{thm}}$ & Joint & 28.95 \\\\\n  $f^{\\text{thm}},\\mathbf{R}$ & Joint & \\textbf{37.51} \\\\\n  \\bottomrule\n\\end{tabular}\n\\end{center}\n\\caption{Initializing with pairwise components,\nand autoregressive retrieval (ProofWiki).\n}\n\\label{tbl:autoregressive-ablation}\n\\end{minipage}\n\\hfill\n\\begin{minipage}[t]{.32\\linewidth}\n\\setlength{\\tabcolsep}{3pt}\n\\begin{center}\n\\footnotesize\n\\begin{tabular}[t]{cc|rr}\n\\toprule\n\\multicolumn{2}{c}{\\textbf{Train}} & \\multicolumn{2}{c}{\\textbf{Eval}}\\\\\n\\textbf{Lang.} & \\textbf{NatProof} & \\textbf{PW} & \\textbf{Stacks} \\\\\n\\toprule\n\\cmark & \\xmark & 0.14 & 0.30 \\\\\n\\xmark & \\cmark & 0.04 & 0.86 \\\\\n\\cmark & \\cmark & \\textbf{16.99} & \\textbf{21.21} \\\\\n\\bottomrule\n\\end{tabular}\n\\end{center}\n\\caption{Language pretraining and $\\dsname$ finetuning (pairwise retrieval, mAP).}\n\\label{tbl:retrieval-ablation-training}\n\\end{minipage}\n\\hfill\n\\begin{minipage}[t]{.33\\linewidth}\n\\setlength{\\tabcolsep}{3pt}\n\\begin{center}\n\\footnotesize\n\\begin{tabular}[t]{rccrr}\n\\toprule\n& \\textbf{Title} & \\textbf{Content} & \\textbf{PW} & \\textbf{Stacks} \\\\\n\\toprule\n\\multirow{3}{*}{\\rotatebox[origin=c]{90}{\\textbf{TF-IDF}}} & \\xmark & \\cmark & 4.97 & 12.34 \\\\\n                & \\cmark & \\xmark & \\textbf{8.10} & 12.69 \\\\\n                & \\cmark & \\cmark & 6.33 & \\textbf{13.45} \\\\\n\\midrule\n\\multirow{3}{*}{\\rotatebox[origin=c]{90}{\\textbf{BERT}}}   & \\xmark & \\cmark & 16.19 & 19.12 \\\\\n                & \\cmark & \\xmark & \\textbf{24.48} & 19.15 \\\\\n                & \\cmark & \\cmark & 16.99 & \\textbf{21.21} \\\\\n\\bottomrule\n\\end{tabular}\n\\end{center}\n\\caption{\n    Excluding (\\xmark) the title or content of theorems and references (pairwise retrieval, mAP).\n}\n\\label{tbl:retrieval-ablation-titlecontent}\n\\end{minipage}\n\\vspace{-2em}\n\\end{table}\n\\myparagraph{Language pretraining and \\textsc{NaturalProofs} training}\nThe BERT model has two learning phases: pretraining on language data, and finetuning on \\dsname.\nAs seen in \\autoref{tbl:retrieval-ablation-training}, relying on language-pretraining alone without fine-tuning on $\\dsname$ (top row) led to poor performance.\nConversely, training from scratch on $\\dsname$ (middle row) was unsuccessful, suggesting that language pretraining served as an effective initialization for mathematical retrieval.\n\n\\myparagraph{Title and content ablation} \nEach theorem statement and reference consists of a title, as well as contents that is a mixture of symbolic mathematics and natural language. \nAs seen in \\autoref{tbl:retrieval-ablation-titlecontent}, ProofWiki's titles contain a large amount of useful information for retrieval-- TF-IDF and the pairwise BERT model performed better with only access to titles.\nIn principal, the title+content model could learn to ignore the contents if needed, so its lower performance shows a deficiency in the pairwise model.\nOn Stacks, the model performs best with both sources of information, though the degree of improvement suggests that leveraging the mathematical content remains as a fundamental challenge.\n\n\\section{Conclusion}\nBuilding agents that understand and create mathematics using \\textit{natural mathematical language} is a challenging research direction, providing a means for evaluating and developing machine learning methods capable of symbolic reasoning and natural language understanding.\nAs a step in this direction, we develop $\\dsname$, a multi-domain dataset for studying mathematical reasoning in natural language.\n$\\dsname$ allows for evaluating \\textit{in-domain} performance, and \\textit{out-of-domain} generalization in broad and deep coverage mathematics, as well as real-world, low-resource settings.\nWe establish benchmarks for retrieval and generation tasks that represent key steps in real-world theorem proving,\nand are tractable, yet challenging, for current large-scale neural sequence models.\n$\\dsname$ opens many promising avenues for future research.\n\n\\section*{Checklist}\n\n\\begin{enumerate}\n\n\\item For all authors...\n\\begin{enumerate}\n  \\item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?\n    \\answerYes{}\n  \\item Did you describe the limitations of your work?\n    \\answerYes{We discussed limitations throughout our experimental analysis.}\n  \\item Did you discuss any potential negative societal impacts of your work?\n    \\answerNA{Our work pertains to use of natural language in mathematical theorem proving, and more generally reasoning in artificial intelligence. \n    Although a general reasoning agent may present negative societal impacts, we do not foresee any immediate negative societal impact from the domain, dataset, tasks, and study that we present here.\n    Instead, we foresee positive societal impacts through education and scientific discovery from building systems that understand and create natural mathematical content.}\n  \\item Have you read the ethics review guidelines and ensured that your paper conforms to them?\n    \\answerYes{}\n\\end{enumerate}\n\n\\item If you are including theoretical results...\n\\begin{enumerate}\n  \\item Did you state the full set of assumptions of all theoretical results?\n    \\answerNA{We did not include theoretical results.}\n\t\\item Did you include complete proofs of all theoretical results?\n    \\answerNA{}\n\\end{enumerate}\n\n\\item If you ran experiments (e.g. for benchmarks)...\n\\begin{enumerate}\n  \\item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?\n    \\answerYes{We released our code as a GitHub repo and our dataset on Zenodo.}\n  \\item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?\n    \\answerYes{We specified data splits in \\autoref{sec:task}, and hyperparameters in \\autoref{apx:experiments}.}\n\t\\item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?\n    \\answerNo{We report results from a single run of each experiment due to computational constraints.}\n\t\\item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?\n    \\answerYes{We specified the computing resources in \\autoref{apx:experiments}.}\n\\end{enumerate}\n\n\\item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\n\\begin{enumerate}\n  \\item If your work uses existing assets, did you cite the creators?\n    \\answerYes{In \\autoref{sec:dataset}, we cited the authors of mathematical textbooks we used as data sources. ProofWiki and Stacks are collaboratively created on the web.}\n  \\item Did you mention the license of the assets?\n    \\answerYes{We noted the license of each data source in \\autoref{sec:dataset}, and verified that all permit redistribution with modification for non-commercial purposes.}\n  \\item Did you include any new assets either in the supplemental material or as a URL?\n    \\answerYes{We released the \\textsc{NaturalProofs} dataset on Zenodo, and provide additional resources in a public Github repository.}\n  \\item Did you discuss whether and how consent was obtained from people whose data you're using/curating?\n    \\answerNA{The licenses of the data indicate that our usage is permitted.}\n  \\item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?\n    \\answerNA{The data we are using/curating contains no PII or offensive content.}\n\\end{enumerate}\n\n\\item If you used crowdsourcing or conducted research with human subjects...\n\\begin{enumerate}\n  \\item Did you include the full text of instructions given to participants and screenshots, if applicable?\n    \\answerNA{We did not use crowdsourcing or conduct research with human subjects.}\n  \\item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?\n    \\answerNA{}\n  \\item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?\n    \\answerNA{}\n\\end{enumerate}\n\n\\end{enumerate}\n\\clearpage\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{OpenWebMath: An Open Dataset of \\\\High-Quality Mathematical Web Text}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\n\nThere is growing evidence that pretraining on high quality, carefully thought-out tokens such as code or mathematics plays an important role in improving the reasoning abilities of large language models. For example, Minerva, a PaLM model finetuned on billions of tokens of mathematical documents from arXiv and the web, reported dramatically improved performance on problems that require quantitative reasoning. However, because all known publicly released web datasets employ preprocessing that does not faithfully preserve mathematical notation, the benefits of large scale training on quantitive web documents are unavailable to the research community. We introduce OpenWebMath, an open dataset inspired by these works containing 14.7B tokens of mathematical webpages from Common Crawl. We describe in detail our method for extracting text and \\LaTeX{} content and removing boilerplate from HTML documents, as well as our methods for quality filtering and deduplication. Additionally, we run small-scale experiments by training 1.4B parameter language models on OpenWebMath, showing that models trained on 14.7B tokens of our dataset surpass the performance of models trained on over 20x the amount of general language data. We hope that our dataset, \\href{https://huggingface.co/datasets/open-web-math/open-web-math}{openly released on the Hugging Face Hub}, will help spur advances in the reasoning abilities of large language models.\n\\end{abstract}\n\n\\section{Introduction}\nAdvances in large language models have opened up new opportunities in numerous fields, providing a transformative shift in our approach to a wide range of complex problems \\citep{gpt3, raffel2020exploring}. Among these problems, mathematical reasoning has drawn the attention of several researchers in recent years, becoming both a common benchmark to judge the performance of large language models and inspiring new approaches to improve their reasoning capabilities in the hope that they will one day be able to solve complex mathematical problems. One of the biggest advancements in mathematical reasoning in recent years has been the Minerva model \\citep{lewkowycz2022solving}, which achieved state-of-the-art results on quantitative reasoning benchmarks such as MATH \\citep{mathdataset}. Minerva was trained by finetuning PaLM \\citep{palm} on a curated dataset consisting of billions of tokens of high quality technical content sourced from both scientific papers and the web.\n\nMinerva and the datasets used for its training were not released publicly and the current capabilities of open-source models (e.g., \\citet{touvron2023llama, llama2, codellama, openlm2023openllama, pythia}) in quantitative reasoning lags behind. We believe that there are important research directions that can only be enabled through open-access to such models and datasets, such as work on memorization and generalization, reinforcement learning, the development of new reasoning benchmarks, and advancement in the reasoning capabilities of language models.\n\nIn our work, we produce an open alternative to the Math Web Pages dataset used to train Minerva \\citep{lewkowycz2022solving}. We extract documents from Common Crawl\\footnote{\\url{https://commoncrawl.org/}}, applying our pipeline to extract text while preserving mathematical content in the form of \\LaTeX{} equations. We then filter the documents, ensuring that only high-quality English mathematical documents are kept. Finally, we deduplicate the dataset, resulting in 14.7B tokens of high-quality mathematical content suitable for both pretraining and finetuning large language models. The key contributions of this work are as follows:\n\n\\begin{itemize}\n    \\item We publically release OpenWebMath, a dataset of 14.7B tokens of high-quality mathematical web text. Our dataset can be found at \\href{https://huggingface.co/datasets/open-web-math/open-web-math}{https://huggingface.co/datasets/open-web-math/open-web-math} on the Hugging Face Hub.\n    \\item We extensively document our pipeline, sharing our findings with the NLP community. We open-source the code needed to reproduce our results.\n    \\item We analyze the quality of OpenWebMath. First, we analyze the contents of our dataset, providing statistics on the types of webpages, subjects, and top domains. Then, we train several language models on our dataset to show that per-token, it is more effective than existing mathematical pretraining datasets, and is most effective when combined with other datasets.\n\\end{itemize}\\begin{figure}[t!]\n\\begin{center}\n\\includegraphics[width=\\textwidth]{figures/Pipeline.pdf}\n\\end{center}\n\\caption{The pipeline for constructing OpenWebMath involves aggressive filtering so that the final dataset only contains high quality, English, and mathematical content.}\n\\label{fig:pipeline}\n\\end{figure}\n\n\\section{Related Work}\n\\subsection{Mathematics datasets and benchmarks} \n\\paragraph{Mathematics datasets} Over the past couple of years, several datasets of mathematics have been introduced. AMPS, a dataset of informal mathematics, was introduced alongside the MATH dataset \\citep{mathdataset}. AMPS includes more than 100,000 Khan Academy problems with step-by-step solutions in LaTeX and over 5 million problems generated using Mathematica scripts. In total, AMPS contains 23GB of problems and solutions. Another notable example is NaturalProofs \\citep{welleck2021naturalproofs}, which encompasses 32,000 theorem statements and proofs, 14,000 definitions, and 2,000 other types of pages (e.g. axioms, corollaries) derived from ProofWiki, the Stacks project and data from mathematics textbooks. Proof-Pile \\citep{azerbayev2023proofnet} is a dataset of mathematical text that contains more than 14.5GB of informal mathematics texts obtained from arXiv, Stack Exchange, ProofWiki, Wikipedia, openly licensed books, and the MATH dataset. There are also many proprietary datasets for mathematics. WebMath is a large-scale dataset mentioned by OpenAI researchers \\citep{polu2020generative} that contains a 35B token mix of content from Github, arXiv, and Math StackExchange, adding up to 35GB of informal mathematics. MathMix is another OpenAI dataset used to finetune GPT-4 \\citep{verify-step-by-step} that contains 1B high quality mathematical tokens containing both natural and synthetic data. The proprietary web dataset used to train Minerva, called Math Web Pages \\citep{lewkowycz2022solving}, was compiled by collecting 17.5B tokens from web pages that contain \\LaTeX{} code.\n\n\\paragraph{Mathematics benchmarks} Several popular benchmarks have been used by researchers to assess the capabilities of language models on both formal and informal mathematics. The MATH dataset \\citep{mathdataset} is comprised of 12,500 challenging competition problems in informal language. Each problem is also accompanied by a step-by-step informal proof. Answers are delimited by the \\texttt{\\textbackslash boxed} environment, allowing for easier answer verification. GSM8k \\citep{cobbe2021training} is another popular multi-step informal mathematics reasoning benchmark. It contains 8,500 grade school math problems that are intended to be solvable by a bright middle school student. \\citet{lewkowycz2022solving} also introduce a benchmark based on OpenCourseWare. OCWCourses includes a set of 272 automatically-verifiable solutions at the undergraduate level, covering chemistry, information theory, differential equations, special relativity, and more. \\citet{lewkowycz2022solving} also evaluate on a subset of MMLU \\citep{hendrycks2020measuring} called MMLU-STEM, which focuses on science, technology, engineering, and mathematics.\n\n\\begin{figure}[t!]\n    \\begin{center}\n    \\vspace{-3em}\n    \\includegraphics[width=\\textwidth]{figures/pie_charts_combined.pdf}\n    \\end{center}\n    \\caption{\\textbf{Left}: The documents in OpenWebMath are sourced from forum posts, educational content, reference pages, scientific papers, blogs, and more. Most content comes from Q\\&A forums where users discuss how to solve problems. \\textbf{Right}: The majority of the content in OpenWebMath is related to mathematics, but a large part is related to other technical subjects like Physics, Computer Science, Statistics, and more.}\n    \\label{fig:doublefig}\n\\end{figure}\n\n\\subsection{Web Data Processing Pipelines}\nThe pretraining of large language models requires large, diverse datasets. Data scraped from the web is one of the primary sources for such data. However, sources such as Common Crawl, which contains over 200 billion web pages, are known to have significant amounts of low-quality and duplicate content, requiring extensive filtering and deduplication to be suitable for training. Prior works such as C4 \\citep{raffel2020exploring}, RefinedWeb \\citep{refined-web}, CCNet \\citep{wenzek2019ccnet}, The Pile \\citep{gao2020pile}, and GPT-3 \\citep{gpt3} introduce various pipelines for extracting quality data from Common Crawl for the purposes of language model training. These pipelines typically consist of three primary steps: text extraction, filtering, and deduplication.\n\n\\paragraph{Text extraction}\nExtracting plain text from HTML files is a critical step in the creation of Common Crawl-based datasets. The easiest way to extract text from Common Crawl documents is to use the WET corresponding to each webpage, which contains pre-extracted plain text of the webpage. CCNet and C4 both use Common Crawl's WET files. However, the text extracted in WET files may contain too much boilerplate or miss out on important content such as \\LaTeX{} equations. It is also possible to extract text directly from the raw HTML found in Common Crawl WARC files. The Pile uses an open source library called jusText \\citep{justext} to extract text from HTML while RefinedWeb uses a library called Trafilatura \\citep{barbaresi-2021-trafilatura}. These text extraction approaches differ in terms of extraction speed, customization, and their precision and recall for removing boilerplate content.\n\n\\paragraph{Filtering} \nThe first layer of filtering often involves language identification \\citep{wenzek2019ccnet}. Language filtering is used because certain other parts of the pipeline only work for specific languages, and is often done with simple linear classifiers such as from fastText \\citep{joulin2016fasttext}. Quality filtering can be done with a combination of perplexity, classifier, and rule-based methods. CCNet uses a 5-gram Kneser-Ney language model implemented in the KenLM library \\citep{heafield2011kenlm} trained on the target domain. The documents in the dataset are then sorted and filtered by their perplexity under this model. Other datasets such as the one used to train GPT-3 \\citep{gpt3} use a classifier-based approach. This involves training a classifier on known-high-quality documents, such as those from Wikipedia, as positive examples and unfiltered documents from Common Crawl as negative examples. The classifier scores are used to filter low-quality documents from the dataset. Finally, rule-based approaches such as those used in C4 \\citep{raffel2020exploring} and MassiveWeb \\citep{gopher} involve removing pages with certain characters, too many or too few characters, too high a proportion of symbols, or those with an abnormal average word length. OpenMathWeb uses a mixture of these three approaches.\n\n\\paragraph{Deduplication} Given the periodic nature of Common Crawl snapshots and a general redundancy in web-sourced text, deduplication is an important processing step. Document-level near-deduplication (e.g., in \\citep{gpt3, refined-web}) often employs MinHashLSH, an efficient algorithm for estimating the Jaccard similarity of documents. CCNet \\citep{wenzek2019ccnet} uses paragraph-level deduplication, which can help to remove common boilerplate content found in WET text-extractions.\\section{Building OpenWebMath}\n\n\\subsection{Objectives}\n\nOur aim with OpenWebMath is to build a dataset of as many mathematical documents sourced from the web as possible while preserving the formatting of mathematical content such as \\LaTeX{} equations as in \\citet{lewkowycz2022solving}. For the purposes of this work, we define a mathematical document as a document containing either core mathematical contents such as theorems, definitions, proofs, questions and answers, formal mathematics, or interdisciplinary documents featuring mathematical formulas within fields like physics, chemistry, biology, economics, and finance. We source our documents from Common Crawl, which is a large open-access crawl of the web containing petabytes of raw HTML files. Due to the high variance in the quality of documents from Common Crawl, we additionally use several methods for filtering and boilerplate reduction. Throughout the creation of OpenWebMath, we iteratively refined these methods to ensure that we do not remove too many relevant documents, optimizing for high recall whenever possible. Since we expect that OpenWebMath will be used primarily as an additional source of pretraining data for large language models, we prefer having a small percentage of non-mathematical but high quality documents in the dataset rather than removing them and potentially losing relevant mathematical content. Finally, due to the limited number of mathematical data available on the web, we use significantly more manual inspection and tuning of our processing pipeline than other web-based datasets. We document our processing choices and pipeline in the section that follows.\n\n\\subsection{High-level overview of the pipeline}\n\nAs shown in \\autoref{fig:pipeline}, the processing pipeline for OpenWebMath falls into five stages. First, we apply a prefilter to all HTML documents in Common Crawl to quickly judge whether they have mathematical content, skipping those that do not before doing the extensive processing needed to extract text and equations and remove boilerplate. Second, we extract the text, including mathematical content, from the HTML documents. Third, we apply language identification filters, perplexity-based quality filtering, and a mathematical content classifier filter. Fourth, we deduplicate the dataset using SimHash \\citep{manku2007near}. Finally, we manually inspect the documents gathered in the previous steps and view documents from the most popular domains by document-count and character-count, removing domains that are not high quality. We describe each of these steps in detail in the following sections.\n\n\\subsection{Prefiltering}\n\nSince there are over 200B HTML documents in Common Crawl, applying our processing over each document would require a significant amount of compute. To improve the efficiency of the pipeline, we first apply a stack of pre-filters optimized for high recall to reduce the number of documents that need to be processed. Our first filters check for common mathematical strings as in \\citet{lewkowycz2022solving}, such as the presence of \\texttt{tex} classes, \\texttt{<math>} tags, and the word ``mathjax''. See \\autoref{table:math-keywords} for a full list of terms. If none of these terms are present, we search for the presence of the top 100 most-popular \\LaTeX{} symbols in the text. This is done by first filtering for documents containing a backslash command using a simple regular expression and then searching specifically for these \\LaTeX{} symbols in the plain text from the HTML document. If none of these symbols are found, we run the plain text through our \\textit{MathScore} classifier (see \\autoref{sec:math_score}) and keep documents that exceed a confidence threshold of 0.8. By tuning these filters and using hierarchical layers of progressively more accurate but more expensive filters, we were able to reduce the compute needed to process the dataset by several times while retaining a high recall of relevant documents.\n\n\\subsection{Text extraction}\n\nIn contrast with prior works that extract text from Common Crawl such as C4 \\citep{collins2023evaluating}, The Pile \\citep{gao2020pile}, and RefinedWeb \\citep{refined-web}, we chose to make a mostly custom pipeline for extracting the main content from HTML documents. This is because we found that while other tools get decent performance on average over many documents on the internet, they do not work optimally on many of the most common sources of mathematical content on the web. We instead opted to build on top of Resiliparse \\citep{bevendorff:2018, bevendorff:2021c}, a fast and efficient library built in Cython that includes performant tools for parsing HTML pages, processing their DOMs, and extracting the main content. As shown in \\autoref{table:extraction-methods-comparison} in the appendix, Resiliparse is significantly more efficient than alternative libraries such as jusText. Another notable part of our text extraction pipeline is that we randomize the parameters of the extraction to add diversity to the dataset. This includes randomizing whether we use a plain text or Markdown format for the documents and randomizing the amount of boilerplate terms required to trigger a line being removed.\n\n\\begin{table}[t]\n\\centering\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{lcccccccc}\n\\toprule\n\\textbf{Training Dataset} & \\multicolumn{1}{c}{\\textbf{GSM8k}} & \\multicolumn{7}{c}{\\textbf{MATH}} \\\\\n                & & Prealgebra & Algebra & \\makecell{Intermediate\\\\Algebra} & \\makecell{Counting \\&\\\\Probability} & \\makecell{Number\\\\Theory} & \\makecell{Precalculus} & Geometry\\\\\n\\midrule\n\\textbf{The Pile} (14.7B tokens) & 2.2032 & 1.9127 & 1.9751 & 1.8420 & 1.8193 & 1.9227 & 1.6847 & 1.9499 \\\\\n\\textbf{ProofPile} (14.7B tokens) & 2.2350 & 1.7370 & 1.7214 & 1.5739 & 1.6462 & 1.7291 & 1.4838 & 1.7229 \\\\\n\\textbf{OpenWebMath} (14.7B tokens) & 1.9075 & 1.6285 & 1.6503 & 1.5949 & 1.6002 & 1.6894 & 1.4542 & 1.5748 \\\\\n\\textbf{Mixture} (14.7B tokens) & \\textbf{1.8968} & \\textbf{1.6055} & \\textbf{1.6190} & \\textbf{1.5301} & \\textbf{1.5719} & \\textbf{1.6607} & \\textbf{1.4119} & \\textbf{1.5599} \\\\\n\\midrule\n\\textbf{The Pile} (300B tokens; Pythia 1.4B) & 1.9430 & 1.7117 & 1.7560 & 1.6358 & 1.6359 & 1.7460 & 1.5191 & 1.7252 \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\caption{We trained 1.4B parameter models for 14.7B tokens on various datasets and measured their perplexity on different mathematics benchmarks. Both OpenWebMath and a 50/50 mixture of ProofPile \\cite{azerbayev2023proofnet} and OpenWebMath perform well - outperforming Pythia 1.4B \\citep{pythia} trained on 300B tokens of The Pile \\citep{gao2020pile}.}\n\\vspace{-1.5em}\n\\label{table:math-perplexity-comparison}\n\\end{table}\n\\begin{table}[b!]\n\\centering\n\\resizebox{\\textwidth}{!}{%\n\\begin{tabular}{l|cccc}\n\\toprule\n\\multicolumn{1}{c}{\\bf Training Dataset} & \\multicolumn{1}{c}{\\bf MATH Algebra-Easy} & \\multicolumn{1}{c}{\\bf \\makecell{MATH Algebra-Easy\\\\maj@16}} & \\multicolumn{1}{c}{\\bf LILA multiarith} \\\\\n\\midrule\n\\textbf{The Pile} (14.7B tokens) & 2.81\\% & 3.93\\% & 9.77\\% \\\\\n\\textbf{ProofPile} (14.7B tokens) & 2.81\\% & 3.93\\% & 8.04\\% \\\\\n\\textbf{OpenWebMath} (14.7B tokens) & \\textbf{5.62\\%} & 9.55\\% & \\textbf{16.67\\%} \\\\\n\\textbf{Mixture} (14.7B tokens) & 5.06\\% & \\textbf{10.11\\%} & 13.22\\% \\\\\n\\midrule\n\\textbf{The Pile} (300B tokens; Pythia 1.4B) & 3.93\\% & 5.62\\% & \\textbf{21.80\\%} \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\caption{Accuracy on Different Math Benchmarks.}\n\\label{table:math-accuracy-comparison}\n\\end{table}\nOur text extraction pipeline consists of four stages: \\LaTeX{} extraction, text extraction, DOM processing, and line processing.\n\n\\paragraph{\\LaTeX{} Extraction} \\citet{lewkowycz2022solving} employ a relatively simple \\LaTeX{} extraction pipeline that extracts equations from \\texttt{<script type=\"math/latex\">}, \\texttt{<script type=\"math/asciimath\">}, and \\texttt{<math>} blocks with \\texttt{<annotation encoding=\"application/x-tex\">} blocks within them and replaces these tags with the extracted equations. When we applied these filters to documents from Common Crawl, we noticed an extremely low number of these tags compared to what was reported. We suspect that this is due to a difference between the HTML files available within Google \\citep{lewkowycz2022solving} and those available on Common Crawl. The majority of the \\LaTeX{} on the internet is written using MathJax, where developers write equations delimited by dollar signs or other delimiters in their HTML pages and then the included javascript code replaces these equations with properly rendered \\LaTeX{} equations within the above script tags when the page is loaded. HTML documents on Common Crawl do not include the changes to the HTML that result from running javascript, requiring that we instead extract the \\LaTeX{} equations by finding delimiters ourselves. This is a significant challenge since we need to detect whether the page contains the required MathJax javascript code, which delimiters were chosen by the user to denote equations, and then match and extract the equations from the text on the page. See Appendix \\ref{appendix:text_extraction} for a more detailed discussion. \n\nIn order to extract MathJax, we first determine whether the page is importing the MathJax javascript code by searching for the word MathJax on the page. If it is not found, we additionally search for common \\LaTeX{} symbols, and if they are found, we treat the page as though it is running MathJax. We use regular expressions to search for code that calls the configuration function for MathJax to extract the delimiters used for equations. We add these delimiters to an extensive list of default delimiters and treat any content between these delimiters as \\LaTeX{} equations.\n\nIn addition to extracting equations from MathJax, we found several more ways that \\LaTeX{} is encoded on the internet. These methods were discovered by filtering small portions of Common Crawl for documents that contain \\texttt{\\textbackslash frac}, one of the most popular \\LaTeX{} commands, and making sure that our processing code supports all the different ways that math could be encoded. We found that \\LaTeX{} on the internet is encoded in the following ways:\n\n\\begin{figure}[t!]\n\\begin{center}\n\\includegraphics[width=\\textwidth]{figures/latex_examples.pdf}\n\\end{center}\n\\vspace{-1.2em}\n\\caption{\\LaTeX{} formulas can be embedded in HTML documents in many ways, including in images, within arbitrary delimiters, and within special tags. Most common text-extraction pipelines do not extract \\LaTeX{} code properly.}\n\\end{figure}\n\n\\begin{enumerate}\n    \\item \\texttt{equation} and \\texttt{align} environments.\n    \\item The \\texttt{alttext} of elements with special classes like \\texttt{tex}.\n    \\item Images from domains like \\texttt{latex.codecogs.com} often include equations encoded in the URL.\n    \\item Special wordpress plugins.\n    \\item \\texttt{<math>} tags with \\texttt{<annotation encoding=\"application/x-tex\">} blocks within them.\n    \\item \\texttt{<math>} tags with MathML content. We use a style sheet to convert these equations into \\LaTeX{}.\n    \\item MathJax equations encoded in the text of the page.\n\\end{enumerate}\n\nThe relative frequencies of the different ways math is encoded can be found in \\autoref{table:latex-types} in the appendix.\n\n\\paragraph{DOM Processing} After extracting the \\LaTeX{} equations from the HTML, we do several processing steps on the DOM-tree of the HTML document. This includes removing invisible elements based on their styles, removing buttons and link clusters, annotating code, tables, and headers, and removing known problematic elements based on class or ID.\n\n\\paragraph{Text Extraction} We use the \\texttt{extract\\_plain\\_text(main\\_content=True)} method in Resiliparse \\citep{bevendorff:2018} to extract the main content text from the DOM following several preprocessing steps to get around common issues with their specific implementation that cause it to be overly sensitive when removing boilerplate.\n\n\\paragraph{Line Processing} After extracting the plain text on the page using Resiliparse, we apply our own processing to remove boilerplate lines based on an iteratively-refined set of common boilerplate phrases, remove empty headers, and escape dollar signs that are not part of \\LaTeX{} equations.\n\n\\subsection{Filtering}\n\\label{sec:filtering}\n\nWe apply filtering with the goal of removing non-English documents (since our filters pipeline is optimized for English), removing documents that are not mathematical, and removing low-quality documents that would be harmful to train a language model on. We apply the following filters in order:\n\n\\begin{figure}[t!]\n\\begin{center}\n\\includegraphics[width=\\textwidth]{figures/MathScore.pdf}\n\\end{center}\n\\vspace{-1em}\n\\caption{The MathScore classifier used in filtering OpenWebMath is trained to predict whether a text has any of the most popular \\LaTeX{} commands based only on surrounding words. This lets us include documents on the web that do not include extractable \\LaTeX{} but still contain technical content.}\n\\vspace{-1em}\n\\label{fig:mathscore}\n\\end{figure}\n\n\\begin{enumerate}\n    \\item We use a FastText language identification model \\citep{joulin2016fasttext} to remove documents that are not in English.\n    \\item We use our \\textit{MathScore} classifier (see \\autoref{sec:math_score}) to get a probability that the document is mathematical. If our previous extraction step found \\LaTeX{} equations, we keep documents with a probability of over 0.17. If no \\LaTeX{} equations were found, we keep documents with a probability of over 0.8.\n    \\item We use a KenLM language model \\citep{heafield2011kenlm} trained on ProofPile \\citep{azerbayev2023proofnet} to get a perplexity score for each document. We remove documents with a perplexity score of more than 15,000.\n\\end{enumerate}\n\n\\subsubsection{Math Score}\n\\label{sec:math_score}\n\nDuring our filtering process, we train a model to predict the probability a document is mathematical, which we call \\textit{MathScore}. We first gather a dataset of hundreds of thousands documents extracted from our pipeline from an early stage of the project, and label them depending on whether they contain one of the top-100 most common \\LaTeX{} commands. We then remove any \\LaTeX{} code from the documents and train a classifier to predict whether the documents contain one of these common \\LaTeX{} commands. The training process for \\textit{MathScore} is depicted in \\autoref{fig:mathscore}. Since we remove all \\LaTeX{} code from the features fed into the model, the model needs to learn the words and phrases most commonly associated with \\LaTeX{} content. We use FastText \\citep{joulin2016fasttext} to train this model, and find based on manual inspection that content with a score of under 0.2 is very unlikely to contain useful mathematical content.\n\n\\subsection{Deduplication}\n\nDue to the large amount of duplicate documents in Common Crawl, we apply a deduplication step to remove near-duplicate documents. We use the SimHash implementation from text-dedup \\citep{text-dedup} to deduplicate the dataset using a threshold of 0.7. We find that this threshold is high enough to remove most duplicate documents even if they have slight differences in their texts.\n\n\\subsection{Manual Inspection}\n\nFinally, we manually inspect the top domains by document count, the top domains by character count, and the longest documents in the dataset to ensure that the documents are high quality. We remove domains that are not high quality or clearly not mathematical by adding domains to a blacklist and adding domain filters such as removing user profile pages, abstract-hosting websites as in \\citet{lewkowycz2022solving}, and removing search result pages.\\section{Dataset Analysis}\n\n\\paragraph{Token count} At 14.7B tokens, OpenWebMath is just below the size of Minerva's Math Web Pages (17.5B tokens) \\citet{lewkowycz2022solving} and significantly larger than the web part of any other dataset. OpenWebMath has around the same number of LLaMA tokens as ProofPile (14.2B) \\citep{azerbayev2023proofnet}, but we note that there is very little overlap between between the two datasets. As a result, OpenWebMath brings a large number of new mathematical tokens that were previously unavailable to the open-source community. Due to differences in data curation strategies, it is hard to compare these datasets other than by training models on them. Since not much is known about how to properly filter a dataset, we opted to keep as much relevant content as possible. However, future work could explore filtering OpenWebMath more aggressively to further improve its quality.\n\n\\paragraph{Data Composition} We measured the distribution of domains in OpenWebMath both by document and by character count. \\autoref{table:common-domains} and \\autoref{table:top-domains-by-characters} show the top twenty most common domains by document and character count respectively. The most common sources of data tend to be discussion forums, blog posts, and scientific papers. We find that the distribution of characters in the dataset is distributed over 131,206 domains, with 46\\% of the characters appearing in the top 100 domains.\n\n\\begin{table}[b!]\n    \\centering\n    \\begin{minipage}{0.48\\textwidth}\n        \\centering\n        \\resizebox{\\textwidth}{!}{\n        \\begin{tabular}{lrr}\n\\multicolumn{1}{c}{\\bf Domain}  &\\multicolumn{1}{c}{\\bf \\# Documents} & \\multicolumn{1}{c}{\\bf \\% Documents}\\\\ \n        \\midrule\n        stackexchange.com & 1,136,407 & 17.99\\% \\\\ \n        physicsforums.com & 300,044 & 4.75\\% \\\\ \n        mathhelpforum.com & 170,721 & 2.70\\% \\\\ \n        socratic.org & 133,983 & 2.12\\% \\\\ \n        mathoverflow.net & 120,755 & 1.91\\% \\\\ \n        gradesaver.com & 96,100 & 1.52\\% \\\\ \n        zbmath.org & 91,939 & 1.46\\% \\\\ \n        wordpress.com & 87,876 & 1.39\\% \\\\ \n        github.io & 81,125 & 1.28\\% \\\\ \n        brilliant.org & 68,573 & 1.09\\% \\\\ \n        gamedev.net & 50,560 & 0.80\\% \\\\ \n        openstudy.com & 49,041 & 0.78\\% \\\\ \n        gmatclub.com & 48,812 & 0.77\\% \\\\ \n        blogspot.com & 48,036 & 0.76\\% \\\\ \n        wikipedia.org & 46,606 & 0.74\\% \\\\ \n        ac.uk & 41,342 & 0.65\\% \\\\ \n        nature.com & 37,403 & 0.59\\% \\\\ \n        aimsciences.org & 36,368 & 0.58\\% \\\\ \n        libretexts.org & 32,216 & 0.51\\% \\\\ \n        readthedocs.io & 31,455 & 0.50\\% \\\\\n        \\end{tabular}\n        }\n        \\caption{Most Common Domains by Document Count.}\n        \\label{table:common-domains}\n    \\end{minipage}\\hfill\n    \\begin{minipage}{0.48\\textwidth}\n        \\centering\n        \\resizebox{\\textwidth}{!}{\n        \\begin{tabular}{lrr}\n        \\multicolumn{1}{c}{\\bf Domain}  &\\multicolumn{1}{c}{\\bf \\# Characters} & \\multicolumn{1}{c}{\\bf \\% Characters}\\\\\n                \\midrule\n                stackexchange.com & 4,655,132,784 & 9.55\\% \\\\ \n                nature.com & 1,529,935,838 & 3.14\\% \\\\ \n                wordpress.com & 1,294,166,938 & 2.66\\% \\\\ \n                physicsforums.com & 1,160,137,919 & 2.38\\% \\\\ \n                github.io & 725,689,722 & 1.49\\% \\\\ \n                zbmath.org & 620,019,503 & 1.27\\% \\\\ \n                wikipedia.org & 618,024,754 & 1.27\\% \\\\ \n                groundai.com & 545,214,990 & 1.12\\% \\\\ \n                blogspot.com & 520,392,333 & 1.07\\% \\\\ \n                mathoverflow.net & 499,102,560 & 1.02\\% \\\\ \n                gmatclub.com & 442,611,169 & 0.91\\% \\\\ \n                gamedev.net & 426,478,461 & 0.88\\% \\\\ \n                ac.uk & 402,111,665 & 0.83\\% \\\\ \n                aimsciences.org & 344,716,386 & 0.71\\% \\\\ \n                mathhelpforum.com & 319,215,756 & 0.65\\% \\\\ \n                deepai.org & 313,512,520 & 0.64\\% \\\\ \n                libretexts.org & 282,014,149 & 0.58\\% \\\\ \n                readthedocs.io & 269,816,413 & 0.55\\% \\\\ \n                tib.eu & 199,714,017 & 0.41\\% \\\\ \n                mit.edu & 198,487,362 & 0.41\\% \\\\\n        \\end{tabular}\n        }\n        \\caption{Most Common Domains by Character Count.}\n        \\label{table:top-domains-by-characters}\n    \\end{minipage}\n\\end{table}\nIn order to get a sense of the types of documents found in the dataset, we analyzed 100,000 randomly sampled documents. First, we created embeddings of this data using \\texttt{all-MiniLM-L12-v2} \\citep{wang2020minilm} in SentenceTransformers \\citep{reimers-2019-sentence-bert}. Then, we clustered these embeddings using $k$-Means with $k=128$. Finally, we took the five closest documents to each cluster center and asked \\texttt{gpt-3.5-turbo} (\\href{https://platform.openai.com/docs/api-reference}{https://platform.openai.com/docs/api-reference}) to classify each cluster as Math, Physics, Statistics, Chemistry, Economics, Computer Science, or Other. We then aggregated these statistics, using the size of each cluster to get an estimate of the final number of documents in each category. We note several potential issues with this methodology, including inaccuracies stemming from using an LLM for classification, and the potential that not every document within a cluster belongs to the predicted category. \\autoref{fig:doublefig} shows the results of this analysis. The majority of the documents in the dataset are directly related to mathematics, while the rest are spread out throughout physics, computer science, statistics, chemistry, and economics, with 12\\% of documents not falling neatly into any of these categories.\n\nWe also used GPT to analyze the types of websites found in OpenWebMath. To do this, we took a sample of 200 documents and asked \\texttt{gpt-3.5-turbo} to classify each as a Forum, Paper, Blog, Reference, Educational, Reference, or other. We also gave the document URL as a feature, since we found GPT is often able to judge the topic from the URL alone. We validated our analysis by asking GPT to do this classification on the top 100 domain names and got similar results. \\autoref{fig:doublefig} shows the results. The highest proportion of documents are forum pages, where users ask and answer questions related to mathematical subjects. There is also a large proportion of educational and reference content.\n\n\\paragraph{Downstream Performance}\n\\label{sec:models}\nWe ran experiments to find out how our dataset compares to other language modeling datasets. We compare models trained on OpenWebMath for a single epoch (14.7B tokens) with models trained for the same number of tokens on The Pile \\citep{gao2020pile}, a general langauge modeling dataset, and ProofPile \\citep{azerbayev2023proofnet}, a dataset of both formal and informal mathematics. We also train a 50/50 mixture of ProofPile and OpenWebMath to evaluate the performance of OpenWebMath when included in a mixture of other datasets, as would be common in practice.\n\nWe train randomly initialized models with the same architecture as Pythia 1.4B \\citep{pythia}. We use a batch size of 1M tokens and the same hyperparameters as Pythia otherwise. These models are evaluated on a collection of mathematics benchmarks which show signal on models of this size. This includes the subset of level-1 algebra questions from MATH, LILA-multiarith to test coding ability, and GSM8k and MATH perplexities, which scale more smoothly than accuracies. We also compare to Pythia 1.4B \\citep{pythia}, which was trained on 300B tokens of The Pile \\citep{gao2020pile} with the same architecture.\n\n\\autoref{table:math-perplexity-comparison} shows the results for our perplexity evaluations. There is a clear performance lead for models trained with OpenWebMath and the mixture seems to perform best. Despite Pythia being trained on over 20x the number of tokens, the performance of our models on the perplexity benchmarks far exceeds its performance, showing the potential of domain-specific models for mathematics. Similarly, \\autoref{table:math-accuracy-comparison} shows the performance of the models on MATH-Algebra-Easy and LILA-multiarith \\citep{lila}. OpenWebMath models outperform models that were not trained on it by a significant margin.\\section{Conclusion}\n\nIn this paper, we describe OpenWebMath, an open dataset of 14.7B high quality mathematical documents from the web. We extensively document our pipeline, including several novel methodologies for extracting \\LaTeX{} formulas, reducing boilerplate, and filtering the dataset. OpenWebMath consists of high quality Q\\&A forum posts, educational documents, blogs, and more spread across mathematics, physics, computer science, and other technical domains. We also train several models on OpenWebMath and other language modeling datasets to compare the downstream performance achievable by training on our dataset. Notably, we find that models trained on OpenWebMath outperform models trained on 20x more general-domain tokens in mathematics. We hope that OpenWebMath can lead to the creation of language models with improved mathematical reasoning capabilities.\\newpage\n\\section*{Acknowledgements}\n\nJB is supported by NSERC Grant [2020-06904], CIFAR AI Chairs program, Google Research Scholar Program, and Amazon Research Award. KP is supported by an NSERC PGS-D award. Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, Fujitsu Limited, and companies sponsoring the Vector Institute for Artificial Intelligence (\\url{www.vectorinstitute.ai/partners}). Computing resources for model training were provided by EleutherAI and Brigham Young University. We thank Finn Paster for the graphic design for the logo. We additionally thank Ziming Chen, Yuhuai Wu, Stella Biderman, Aviya Skowron, Hailey Schoelkopf, and Sean Welleck for their helpful comments.%\\section*{Acknowledgements}\n\\clearpage\n\n\\clearpage\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{\\mathpile: A Billion-Token-Scale Pre-training Corpus for Math}\n\n\\begin{document}\n\n\\doparttoc\n\\faketableofcontents\n\n\\maketitle\n\n\\begin{abstract}\n\nHigh-quality, large-scale corpora are the cornerstone of building foundation models. In this work, we introduce \\mathpile, a diverse and high-quality math-centric corpus comprising about 9.5 billion tokens. Throughout its creation, we adhered to the principle of ``\\emph{less is more}'', firmly believing in the supremacy of data quality over quantity, even in the pre-training phase. Our meticulous data collection and processing efforts included a complex suite of preprocessing, prefiltering, language identification, cleaning, filtering, and deduplication, ensuring the high quality of our corpus. Furthermore, we performed data contamination detection on downstream benchmark test sets to eliminate duplicates and conducted continual pre-training experiments, booting the performance on common mathematical reasoning benchmarks. We aim for our \\mathpile to boost language models' mathematical reasoning abilities and open-source its different versions and processing scripts to advance the field (available at \\url{https://github.com/GAIR-NLP/MathPile/}).\n\n\\end{abstract}\n\n\\section{Introduction}\n\nHigh-quality, diverse pre-training corpora form the cornerstone for developing powerful foundation models, enabling AI assistants like ChatGPT~\\citep{openai-chatgpt} to exhibit balanced competencies across a broad spectrum of tasks~\\citep{DBLP:journals/corr/abs-2303-12712-sparks-agi}. In this work, our concern centers on mathematical reasoning \\begin{wrapfigure}{r}{5.7cm}\n\\centering \n\\includegraphics[width=0.39\\textwidth]{img/mathpile-characteristics-1.pdf} \n\\caption{Key features of \\mathpile.} \n\\label{fig:math-pile-characteristics}\n\\end{wrapfigure}capabilities within foundational language models~\\citeg{abel,DBLP:journals/corr/abs-2310-10631-proofpile-2}, for which can potentially boost the application in education tools, automated problem solving, data analysis, etc., thereby improving user experience. To facilitate this, we are not directly building a model, but rather focusing on a more fundamental aspect: \\emph{creating a high-quality and diverse pre-training corpus tailored for the math domain}, namely \\mathpile. Specifically, our work is significantly different from the previous work in the following characteristics (cf. Table~\\ref{tab:math-corpora-comparison} for comparison):\n\n\\textbf{Math-centric}. Previous open-sourced pre-training corpora have typically focused on general domains, such as Pile~\\citep{DBLP:journals/corr/abs-2101-00027-pile}, RedPajama~\\citep{RedPajama} and Dolma~\\citep{allenai-dolma}. Others have concentrated on multilingual aspects or programming languages, such as ROOTS~\\citep{DBLP:conf/nips/LaurenconSWAMSW22-roots} and The Stack~\\citep{DBLP:journals/corr/abs-2211-15533-stack}, respectively. However, a notable absence in these offerings is a corpus specificlly tailoring for mathematics. While there exist some corpora designed for training or continually improving math-specific language models, such as Minerva's mathematical training dataset~\\citep{DBLP:conf/nips/LewkowyczADDMRS22-minerva} and  OpenAI's MathMix~\\citep{DBLP:journals/corr/abs-2305-20050-lets-verify-step-by-step}, these are not open-sourced. Note that a recent work concurrent with ours, OpenWebMath~\\citep{DBLP:journals/corr/abs-2310-06786-openwebmath}, although math-centric, is solely sourced from web pages. We will discuss the comparison with it later. \nRecognizing this gap, we aim to democratize access to high-quality mathematical corpus, \nfostering inclusive advancements in language models' mathematical reasoning.\n\n\\textbf{Diversity}. While \\citet{DBLP:conf/nips/HendrycksBKABTS21-math} introduced AMPS, a problem set ranging from elementary mathematics to multivariable calculus (K-12 level) for pre-training purposes, it lacks content at the college-level and more challenging competition-level mathematics, focusing instead on a supervised dataset rather than an extensive corpus. The ProofPile corpus, introduced by~ \\citet{DBLP:journals/corr/abs-2302-12433-proofpile}, aims to improve autoformalization and formal proving capabilities in models, yet its scope is confined to formal proving, not covering the broader mathematical domain from K-12 to postgraduate level. Concurrently with our work, \\citet{DBLP:journals/corr/abs-2310-06786-openwebmath}\npropose the OpenWebMath corpus, featuring a corpus composed of mathematical web pages. However, our corpus goes beyond web pages, integrating high-quality mathematics textbooks, lecture notes, scientific papers from arXiv in the field of mathematics, and carefully selected content from StackExchange, ProofWiki, and Wikipedia  among others, which positions our corpus as a richer and more diverse mathematical resource for language models.\n\n\\textbf{High-Quality}. Recent studies have increasingly highlighted the detrimental effects of low-quality and repeated content in pre-training corpora on model training, as evidenced in various works~\\citep{10.1145/3359591.3359735-code-duplication,luccioni-viviano-2021-whats,lee-etal-2022-deduplicating,DBLP:journals/corr/abs-2205-10487-scaling-law-for-training-on-repeated-data,DBLP:journals/corr/abs-2305-13169-a-pretrainer-data-guide}. The importance of high-quality datasets has thus come to the fore. It has been shown that properly filtered and deduplicated web data can yield models as equally powerful as those trained on curated, high-quality corpora~\\citep{DBLP:journals/corr/abs-2306-01116-refinedweb}. This similar practice has been recently adopted in several notable studies~\\citep{SlimPajama,allenai-dolma,RedPajama-v2}. \nA notable example is the 1.3 billion-parameter code-focused model pre-trained on synthetically generated textbooks and filtered web pages, a project that broke existing scaling laws although did not open source its data~\\citep{DBLP:journals/corr/abs-2306-11644-textbooks-are-all-you-need}. It's important to emphasize that quality of the corpus is far more significant than its quantity. For instance, OpenAI's MathMix comprises only 1.5 billion tokens. In this work, we diligently adhere to the principle of \\emph{less is more}, as outlined in \\citet{DBLP:journals/corr/abs-2305-11206-lima}. To achieve a high-quality corpus, \nUnlike other approaches that uniformly process all data, we have conducted specialized preprocessing and prefiltering for each data source before global data processing (including language identification, filtering, cleaning, and deduplication). We're dedicated to refining and optimizing our corpus, making a distinctive contribution to the field.\n\n\\textbf{Data Documentation}. Auditing large-scale pre-training corpora is essential for identifying content characteristics, intended uses, and potential biases, despite challenges due to their size~\\citep{bender-friedman-2018-data,10.1145/3458723-datasheets-for-datasets,10.1145/3594737-data-statements}. However, many such corpora are released without proper documentation~\\citep{DBLP:journals/corr/abs-2212-05129-measuring-data}. Recent audits of certain pre-training corpora have uncovered issues such as irrelevant content~\\citep{luccioni-viviano-2021-whats,kreutzer-etal-2022-quality,DBLP:journals/corr/abs-2310-20707-what-in-my-big-data}, copyright infringement~\\citep{DBLP:journals/corr/abs-2105-05241-datasheet-for-bookcorpus}, and inclusion of test sets for downstream tasks~\\citep{10.1145/3359591.3359735-code-duplication,dodge-etal-2021-documenting}, highlighting the need for detailed data sheets and transparent documentation. To this end, following previous efforts to enhance corpora transparency,  we have provided a dataset sheet for our \\mathpile (see Table~\\ref{tab:mathpile-datasheet}). Throughout our extensive data processing workflow, numerous documents were annotated for quality, such as language identification scores and the ratio of symbols to words (as exemplified in Figure~\\ref{fig:cleaned-example-doc-with-quality-annotation}). These quality annotations enable future users to apply their specific filters based on these scores. Additionally, we have conducted extensive deduplication for this corpus and performed data contamination detection with downstream benchmark test sets, removing any duplicated samples identified (cf. \\S~\\ref{sec:data-contamination-detection}). Interestingly, we have also discovered a significant number of questions from downstream test sets in OpenWebMath (cf. \\S~\\ref{sec:data-contamination-detection}). This underscores the importance of meticulous data documentation. We plan to release different versions to facilitate future use. See Appendix~\\ref{appendix-sec:mathpile-example} for examples.\n\nAdditionally, we conducted continual pre-training experiments on \\mathpile and found that it generally enhances the performance of language models across various mathematical reasoning benchmarks, with an average improvement of up to 5\\% (cf. \\S~\\ref{sec:continual-pretrain-exp}). In conclusion, we hope to facilitate the growth of the field of AI for mathematics by contributing this specialized, high-quality, diverse corpus focused on the mathematical domain while maintaining utmost transparency about the data for practitioners. \n\n\\section{The Collection of Corpora}\n\\label{sec:data-collection}\n\nIn order to construct \\mathpile, we gather data from a variety of sources, which also includes a component of manual collection. We provide an ethics statement regarding copyright in Appendix~\\ref{appendix-sec:ethics-statement}.\n\n\\begin{table*}[h]\n\\caption{The comparison of \\mathpile with other mathematical Corpora, where PS denotes the problem set type. For non-open-sourced corpora, details are inferred from literature, with unknowns marked as ``?''. Token counts may vary by tokenizer; we use statistics from each dataset's report and the GPTNeoX-20B tokenizer~\\citep{black-etal-2022-gpt} for our corpus. DM-Mathematics is from~\\citet{DBLP:conf/iclr/SaxtonGHK19-deepmind-mathematics}. \"Minerva\" refers to its dataset. ProofPile-2~\\citep{DBLP:journals/corr/abs-2310-10631-proofpile-2}, encompassing OpenWebMath and others, is excluded from this comparison.}\n\\label{tab:math-corpora-comparison}\n\\resizebox{\\textwidth}{!}{%\n\\begin{tabular}{lm{1cm}<{\\centering} ccc  m{2cm}<{\\centering} m{2.5cm}<{\\centering} c m{5.5cm}<{\\centering}}\n\\toprule\n\\textbf{Datasets} &\n  \\textbf{Open Source} &\n  \\textbf{Type} &\n  \\textbf{Target Domain} &\n  \\textbf{\\# Textbooks} &\n  \\textbf{Has Synth. Data} &\n  \\textbf{Data Contam. Detection} &\n  \\textbf{\\# Tokens } &\n  \\textbf{Source} \\\\ \\midrule\n\n  \nMinerva       & \\textcolor{black}{\\ding{55}} & Corpus      & General Math     & \\textcolor{black}{\\ding{55}}      & \\textcolor{black}{\\ding{55}} & \\textcolor{black}{\\ding{51}} & 38.5B  & arXiv, Web         \\\\ \\midrule              \n\nMathMix         & \\textcolor{black}{\\ding{55}} & Corpus + PS & General Math     & \\textcolor{black}{\\textbf{?}}       & \\textcolor{black}{\\ding{51}} & \\textcolor{black}{\\ding{51}} & \\textcolor{white}{0}1.5B       & \\textcolor{black}{\\textbf{?}}   \\\\ \\midrule          \n\nProofPile       & \\textcolor{black}{\\ding{51}} & Corpus      & Theorem Proving  & 7      & \\textcolor{black}{\\ding{55}} & \\textcolor{black}{\\ding{55}} & \\textcolor{white}{0}8.3B    & arXiv, Textbooks, Lib., StackExchange, ProofWiki, MATH \\\\ \\midrule \n\nOpenWebMath     & \\textcolor{black}{\\ding{51}} & Corpus      & General Math     & \\textcolor{black}{\\ding{55}}         & \\textcolor{black}{\\ding{55}} & \\textcolor{black}{\\ding{55}} & 14.7B  & Web \\\\ \\midrule\n\nDM-Mathematics  & \\textcolor{black}{\\ding{51}} & PS          & Math Competition & \\textcolor{black}{\\ding{55}}        & \\textcolor{black}{\\ding{51}} & \\textbf{-} & \\textcolor{white}{0}4.4B   & Synthesis \\\\ \\midrule\n\nAMPS   & \\textcolor{black}{\\ding{51}} & PS          & Math Competition & \\textcolor{black}{\\ding{55}}        & \\textcolor{black}{\\ding{51}} & \\textcolor{black}{\\ding{55}} & \\textcolor{white}{0}0.7B  & Khan Academy, Synthesis  \\\\ \\midrule \n \n\\mathpile(Ours)  & \\textcolor{black}{\\ding{51}} & Corpus      & General Math     & 3,979 & \\textcolor{black}{\\ding{51}} & \\textcolor{black}{\\ding{51}} & \\textcolor{white}{0}9.5B    & arXiv, Textbooks, StackExchange, Wikipedia, ProofWiki, Web  \\\\ \\bottomrule\n\n\\end{tabular}%\n}\n\n\\end{table*}\n\n\\noindent\\textbf{Mathematical Textbooks} \\quad Textbooks, covering mathematical concepts, exercises, and solutions, are valuable for \\textit{educational purposes} for both humans and machines. Recent studies, even though not focused on math, support this view with synthesized textbooks~\\citep{DBLP:journals/corr/abs-2306-11644-textbooks-are-all-you-need,DBLP:journals/corr/abs-2309-05463-phi-1.5}. To collect these genuine and high-quality textbooks, we began by conducting extensive manual searches across the internet, seeking open-source and freely accessible mathematics-related textbook websites. Afterwards, we proceeded to download these PDF files, resulting in a collection of 38 K-12 level textbooks, along with 369 college-level mathematics textbooks that cover a wide range of subjects including linear algebra, probability theory, calculus, and optimization. In addition to these textbooks, we also included  467 college course handouts and lecture notes, which tend to be more concise compared to full-length textbooks. Subsequently, we employed the Mathpix API\\footnote{\\url{https://mathpix.com/ocr}} to parse the PDFs into markdown format. Then, we meticulously cleaned up extraneous elements such as parsed image URLs, preface sections, table of contents, acknowledge sections, index sections, and consecutive empty lines within the parsed content, resulting in a  total of 874 documents.\n\nWe also refined high-quality mathematics-related synthetic textbooks from OpenPhi Project.\\footnote{\\url{https://huggingface.co/open-phi}} It is an open-source counterpart to the Phi work~\\citep{DBLP:journals/corr/abs-2306-11644-textbooks-are-all-you-need}. While the underlying model and generation process differ, the output encompasses a broad spectrum of subjects, extending beyond programming. To isolate mathematics-related documents, we employed a straightforward criterion: the presence of the symbol ``\\texttt{\\$\\$}'' combined with common mathematical expressions like ``\\verb|\\|\\texttt{\\{mathbf}'' and  ``\\verb|\\|\\texttt{\\{frac}''.  While ``\\texttt{\\$\\$}'' alone is not always reliable, combining it with these symbols improves accuracy based on manual verification. This approach yielded 3,889 documents from an initial pool of 124,493. As the volume of pre-training data escalates, the synthesis of high-quality data becomes increasingly crucial. More advanced filtering methods and mathematical corpora synthesis are left for future exploration.\n\n\\noindent\\textbf{Mathematical Papers from ArXiv} \\quad ArXiv offers a free distribution service and serves as an open-source archive housing  millions of scientific papers. It also provides  invaluable training data for numerous powerful language models~\\citeg{DBLP:journals/corr/abs-2302-13971-llama,RedPajama}. In our endeavor to collect mathematical papers from ArXiv, we identify 50 sub-subjects spanning Mathematics, Computer Science, Statistics, Physics, Quantitative Finance and Economics. Our process involved filtering ArXiv's metadata\\footnote{\\url{https://www.kaggle.com/datasets/Cornell-University/arxiv}} to focus on the chosen subjects (cf. Table~\\ref{tab:arxiv-subject-list}), followed by accessing the source LaTex files (if available). We exclusively retained the LaTex files and consolidated multiple files based on their respective order as indicated by commands such as ``\\texttt{include}'' and ``\\texttt{input}'' within the main LaTex file of each paper. Subsequently, we undertook extensive transformations to enhance data clarity and consistency, including removing comments, reverting macros, omitting figures but keeping captions, excluding acknowledgements and references, condensing empty lines, replacing some formatting commands, substituting titles, and preserving only the main body content (cf. \\S~\\ref{appendix-sec:data-collection-details} for more details). Finally, we compiled 347,945 meticulously cleaned LaTex documents (around 8.5 billion tokens), with each document corresponding to a single paper. \n\n\\noindent\\textbf{Mathematical Entries in Wikipedia} \\quad Wikipedia is one the largest and most popular free online encyclopedias, offering information on a wide range of topics, including history, science, technology, culture, and more. This extensive knowledge has proven to be highly beneficial for numerous natural language processing tasks~\\citeg{DBLP:conf/nips/LewisPPPKGKLYR020-rag} and pre-trained language models~\\citeg{devlin-etal-2019-bert,DBLP:journals/corr/abs-2302-13971-llama}. To collect mathematical entries from Wikipedia, we downloaded the mathematics-focused (without pictures) dump of Wikipedia in English for the month of August 2023. We extracted the HTML documents from the dump using the library \\texttt{libzim}, resulting in approximately 106,900 documents. \nSubsequently, we converted these HTML documents into markdown format using the  \\texttt{html2text} library\\footnote{We later found that the \\texttt{html2text} library resulted in the LaTeX display issue in the cleaned documents (cf. Figure~\\ref{fig:latex-display-issue-case}). Switching to another library \\texttt{Resiliparse} with DOM parsing resolved this issue, ensuring correct LaTeX display.} while removing the hyperlinks following the practice of LLaMA~\\citep{DBLP:journals/corr/abs-2302-13971-llama}. We retained the alternative text content but excluded image (often in SVG format) paths. Additionally, we eliminated extra newlines within paragraphs and condensed more than three consecutive empty lines to two using regular expressions. Further refinement involved the removal of boilerplate content at the bottom of the pages, typically denoted with phrases like ``\\texttt{This article is issued from Wikipedia. The text is ...}''. In the end, our efforts yielded a collection of 106,881 mathematical Wikipedia entries, about 0.8 billion tokens.\n\n\\noindent\\textbf{Entries from ProofWiki} \\quad ProofWiki, an online compendium of mathematical proofs, has been instrumental in advancing the fields of autoformalization and formal proof proving, as evidenced by NaturalProofs~\\citep{DBLP:conf/nips/Welleck0BHCCC21-naturalproofs} and ProofPile. We sourced data from the ProofWiki dump dated April 9, 2022 (provided by the Internet Archive), mirroring the preprocessing approach employed by NaturalProofs, which was based on the version from November 12, 2020. Specifically, this involved leveraging the \\texttt{BeautifulSoup} library to parse all wiki pages followed by the extraction of raw text content using the \\texttt{wikitextparser} library. This process yielded a substantial collection of mathematical content, totaling about 7.6 million tokens, comprising 10,328 definitions and 13,511 theorem-proof pairs. To facilitate better data organization, we formatted the definitions using the ``\\texttt{definition}'' environment, and the theorem-proof pairs within the ``\\texttt{section}'' environment with their respective titles serving as the section headings, similar to  ProofPile.\n\n\\noindent\\textbf{Mathematical Discussions on StackExchange} \\quad StackExchange, renowned for its network of community-powered question-and-answering websites, spans a wide array of topics, each concentrated on a particular topic. Its high-quality data trove has significantly contributed to the development of various language models~\\citeg{DBLP:journals/corr/abs-2302-13971-llama,DBLP:journals/corr/abs-2305-11206-lima}. In our study, we identify eleven sites within this network, including five dedicated to mathematics (such as Mathematics and MathOverflow) and six others in closely related fields like Physics (cf. Table~\\ref{tab:stackexchange-site-list}). Our data collection process began with downloading the site dumps from August 2023 (provided by the Internet Archive). We only retained the essential components in the posts, namely questions and answers (also associated meta information). To convert HTML documents to raw text, we utilized the \\texttt{BeautifulSoup} library, coupled with a meticulous removal of invalid XML characters. We then systematically paired questions and their respective answers. Each question typically garners multiple responses, each with its own score and in some cases, an endorsement as the accepted answer by the questioner.  To guarantee quality, we applied a quality threshold (i.e., 5) for filtering. Questions underwent filtering based on the threshold, whereas answers were assessed by either the threshold or the score of the accepted answer, whichever was lower. Unanswered questions scoring at least 10 were preserved for potential future use. This rigorous process resulted in a rich collection of data, comprising 267,919 questions, 435,129 answers, and 3,418 unanswered questions, totaling about 254 million tokens.\n\n\\noindent\\textbf{Mathematical Web Pages from Common Crawl} \\quad Common Crawl, an archive of web data since 2007, is crucial for training advanced language models like GPT-3~\\citep{DBLP:conf/nips/BrownMRSKDNSSAA20-gpt-3} and LLaMA. Our work targets extracting math web pages from SlimPajama~\\citep{SlimPajama}, a cleaned and deduplicated version of RedPajama, focusing on its CommonCrawl and C4 subsets. Eschewing the common approach of using neutral network-based filtering, we opt for heuristic rule-based methods. Our procedure began with the creation of TF-IDF features, derived from our curated high-quality textbooks. During this process, we removed the stop words, limited the features to a maximum of 10,000, and employed white space tokenization. Upon the observation of the resulting vocabulary, we identified 11 commonly used LaTex commands, integral to mathematical expressions. We utilize these commands as a basis for a hard match within each document. A document is classified as mathematical if it contains any of these commands along with the symbol  ``\\texttt{\\$\\$}'', typically indicative of a mathematical document. This rule-based approach, though simplistic, proved to be highly effective, especially given the vast size of the Common Crawl corpus\n. We also experimented with more intricate dense embedding-based methods to identify mathematical documents, but these resulted in poor recall. Our efforts resulted in the compilation of a substantial collection of mathematical web pages: 4,307 documents from SlimPajama-C4 and 72,137 documents from SlimPajama-CommonCrawl, totaling approximately 633 million tokens. We acknowledge the potential for more efficient methods to sift mathematical documents from Common Crawl snapshots, an area we plan to explore in future work.\n\n\\begin{figure*}[h]\n\\centering \n\\includegraphics[width=0.82\\textwidth]{img/mathpile-overview-2.pdf} \n\\caption{The creation process of \\mathpile. We additionally perform data contamination detection on benchmark test sets (cf. \\S~\\ref{sec:data-contamination-detection}). We visualize its component ratios by document counts (Right).} \n\\label{fig:math-pile}\n\\end{figure*}\n\n\\section{Global Data Processing}\n\\label{sec:global-data-processing}\n\nAfter conducting specific data preprocessing for each data source during the data collection process, we globally engage in three critical steps: language identification,  filtering, and deduplication, to ensure the quality of the entire corpus, as shown in Figure~\\ref{fig:math-pile}.\n\n\\subsection{Language Identification}\n\nTo filter non-English documents, we utilized the fastText language identifier, which was trained on Wikipedia, Tatoeba, and SETimes~\\citep{joulin-etal-2017-fasttext,grave-etal-2018-learning-vector-157-languages}. A common practice is to classify a document as its respective language if the score exceeds 0.5, a threshold also employed by CCNet~\\citep{wenzek-etal-2020-ccnet}. However, during the application of this practice, we encountered a considerable number of false positivesâ€”cases where documents were erroneously filtered as non-English when, in fact, they were written in English but contained a substantial amount of mathematical symbols. We attribute this issue to the domain gap between the fastText training datasets and the mathematical content. To enhance non-English document filtering, we set customized score thresholds for each data source. Specifically, Wikipedia and StackExchange thresholds were set at 0.1, arXiv at 0.3, and Common Crawl at 0.5. No thresholds were applied to ProofWiki and Textbooks due to manual verification ensuring English content. This refinement removed about 8,400 documents, totaling 231 million tokens.\n\n\\subsection{Data Cleaning and Filtering}\n\nDespite thorough preprocessing, some documents, especially from sources like Wikipedia and Common Crawl, lack quality for language modeling due to brevity or automated content. Existing filtering methods~\\citep{DBLP:journals/jmlr/RaffelSRLNMZLL20-T5,DBLP:journals/corr/abs-2112-11446-gopher,DBLP:journals/corr/abs-2305-13169-a-pretrainer-data-guide,DBLP:journals/corr/abs-2306-01116-refinedweb,SlimPajama}, while detailed, risk excluding valuable documents in our math-focused corpus if directly applying them as-is. To address this issue, we developed a unique set of cleaning and filtering heuristic rules, specifically crafted for the mathematical domain and drawing from past studies. These rules are aimed at removing meaningless lines (such as boilerplate content) and documents. Specifically, we (1)  detect lines containing ``lorem ipsum'' and filter them out if the resulting line is less than 5 characters; (2) detect lines containing ``javascript'' that also include  ``enable'', ``disable'' or ``browser'' and are under 200 characters, and filter them; (3) filter lines containing fewer than 10 words that include keywords like ``Log in'', ``sign-in'', ``read more...'', or ``items in cart.''; (4) filter documents if the ratio of uppercase words exceeds 40\\%; (5) filter lines that end with ``...'' if they constitute more than 30\\% of the entire document; (6)  filter documents if the ratio of non-alphabetic words surpasses 80\\%; (7)  exclude documents with an average English word length outside the range of (3, 10); (8) discard documents that lack at least two common stop words such as ``the'', ``be'' ``to'' ``of'' ``and'' ``that'' or ``have''; (9)  filter out documents if the ratio of ellipses (...) to words exceeds 0.5 (e.g., progress bars); (10)  remove documents where 90\\% of lines start with bullet points; (11)  filter documents including less than 200 characters after removing spaces and punctuation marks.\n\nThese meticulously crafted rules enabled us to curate a high-quality mathematical corpus. They also facilitated the assignment of quality annotations to each document from Wikipedia and Common Crawl. These annotations provide researchers and developers with the flexibility to filter the data according to their criteria, catering to specific needs (as shown in Figure~\\ref{fig:cleaned-example-doc-with-quality-annotation}). This process resulted in filtering approximately 1,100 documents, removing 17 million tokens.\n\n\\subsection{Data Deduplication}\n\nGiven that our corpus originates from diverse sources, it is inevitable that there will be repetitions both within and across these sources. Deduplication is vital for training efficiency and reducing data memorization, addressing both exact and near-duplicates~\\citep{lee-etal-2022-deduplicating}. We utilized the MinHash LSH algorithm~\\citep{DBLP:conf/vldb/GionisIM99-LSH} built on the implementation of \\texttt{text-dup}~\\citep{chenghao_mou_2023_8364980_text-dedup} and \\citet{lee-etal-2022-deduplicating}, to process large-scale corpora efficiently. Specifically, our process involved splitting each document using whitespace and constructing 5-grams, applying the ``\\texttt{sha1}'' hash function, and configuring 450 buckets with 20 minhashes each, totaling 9,000 minhashes per document, as per RefinedWeb's guidelines~\\citep{DBLP:journals/corr/abs-2306-01116-refinedweb}.\n\nDuring the deduplication process within each source, we encountered numerous exact and near-duplicate documents across various sources: 304 in arXiv, 623 in Common Crawl, 83,716 in Wikipedia, 783 in textbooks (primarily synthetic), and 144 duplicate questions in StackExchange. Despite finding many near-duplicates in ProofWiki, they were differentiated as unique lemmas, proofs, or definitions, leading us to retain these entries (cf. Table~\\ref{tab:dup-case-ProofWiki}). Manual review revealed significant duplication in Wikipedia due to collecting multiple historical document versions and in StackExchange from reposts across different forums (e.g., Math and MathOverflow) for broader visibility (cf. Table~\\ref{tab:dup-case-StackExchange}). We provide near-duplicate examples from each data source in Table~\\ref{tab:dup-case-CC}-\\ref{tab:dup-case-StackExchange}. Cross-source deduplication revealed minimal overlap, with a single StackExchange question duplicated in Common Crawl, which was removed. This eliminated around 714 million tokens.\n\nNote that we also experimented with using suffix arrays~\\citep{DBLP:journals/siamcomp/ManberM93-suffix-arrays} to eliminate exact match sequences within documents. However, it tended to remove common phrases like ``Questions: ''. While it can effectively remove some templated content, it also disrupts the contextual integrity of our corpus. Consequently, we decided against employing this in order to preserve the context of our data.\n\n\\subsection{Data Contamination Detection}\n\\label{sec:data-contamination-detection}\n\nAs pre-training corpora grow, encountering data contamination becomes inevitable, where evaluation examples are found in the training set. Traditionally, post-hoc analysis, employing n-gram overlap, assesses contamination levels (e.g., GPT-2~\\citep{radford2019gpt-2}, GPT-3~\\citep{DBLP:conf/nips/BrownMRSKDNSSAA20-gpt-3}, FLAN~\\citep{DBLP:conf/iclr/WeiBZGYLDDL22-FLAN}, LLaMA-2~\\citep{DBLP:journals/corr/abs-2307-09288-llama-2}).  We advocate for early contamination detection during dataset creation to prevent irreversible damage as delaying exacerbates issues (c.f., previous study~\\citep{DBLP:journals/corr/abs-2211-15533-stack}). Here, we utilize popular mathematical reasoning benchmarks, namely GSM8K~\\citep{DBLP:journals/corr/abs-2110-14168-GSM8K}, MATH~\\citep{DBLP:conf/nips/HendrycksBKABTS21-math}, MMLU-STEM~\\citep{DBLP:conf/iclr/HendrycksBBZMSS21-MMLU}, AGIEval-SAT-MATH~\\citep{DBLP:journals/corr/abs-2304-06364-agieval}, MathQA~\\citep{DBLP:conf/naacl/AminiGLKCH19-mathqa} and AQuA~\\citep{ling-etal-2017-program-aqua} to detect data contamination. \n\n\\begin{wraptable}{r}{5.5cm}\n\\caption{Benchmark test set occurrences in pre-training corpora, with numbers representing minimum occurrences, given potential undetected duplicates.\n}\n\\label{tab:data-contamination-stat}\n\\scalebox{0.8}{\n\\begin{tabular}{c|ccc}\n\\toprule\n\\textbf{Corpus} &   \\textbf{MATH} & \\textbf{MMLU-STEM} \\\\ \\midrule\nOurs            &              \\textcolor{white}{0}23            & \\textcolor{white}{0}2                  \\\\\nOpenWebMath     &         195           & 65                 \\\\ \\bottomrule\n\\end{tabular}}\n\n\\end{wraptable}To detect data contamination, we aggregated questions and answers from benchmark tests into a reference set, considering only questions for MMLU, AGIEval, MathQA and AQuA due to its multiple-choice format. Intuitively, math problem solutions often involve diverse reasoning steps, making questions easier to detect for contamination in pre-training data due to their more fixed nature. We utilized line-level exact match detection, dividing documents into lines, hashing each with \\texttt{MD5} (taking the first 64 bits and the line itself to form sets), and applied this to both our corpus and the test sets. If a test set line and its hash match exactly with our dataset, it's marked as contamination.\n\nAfter our detection process, we found 23 questions from MATH and 2 from MMLU-STEM in our corpus (see Table~\\ref{tab:data-contamination-stat}), with no accompanying answers. No contamination was detected in other benchmarks. These duplicates mainly originated from StackExchange, Textbooks, and Common Crawl (see Table~\\ref{tab:data-contamination-case-in-textbooks} and Table~\\ref{tab:data-contamination-case-in-commoncrawl} for examples). Notably, questions from AMC mathematics competition books, also used in the MATH benchmark, were identified in Textbooks. We extended our analysis to OpenWebMath, uncovering more duplicate questions from MATH and MMLU (cf. Table~\\ref{tab:data-contamination-case-in-openwebmath}), although many were repeats. This aligns with similar findings by \\citet{DBLP:journals/corr/abs-2310-10631-proofpile-2}. These instances highlight the importance of vigilance in creating pre-training corpora to avoid undermining downstream benchmarks. We removed all detected exact matches to mitigate data contamination, resulting in \\mathpile corpus.\n\n\\section{Data Analysis}\n\n\\subsection{Statistics}\n\n\\begin{table*}[ht]\n\\centering\n\\caption{The components and data statistics of \\mathpile.}\n\\label{tab:mathpile-count-stat}\n\\scalebox{0.8}{\n\\begin{tabular}{c|cccccc}\n\\toprule\n\\textbf{Components} & \\textbf{Size (MB)} & \\textbf{\\# Documents} & \\textbf{\\# Tokens} & \\textbf{max(\\# Tokens)} & \\textbf{min (\\# Tokens)} & \\textbf{ave (\\# Tokens)}  \\\\ \\midrule\nTextbooks     &  \\textcolor{white}{00}644     & \\textcolor{white}{00}3,979   & \\textcolor{white}{0}187,194,060   & 1,634,015 & 256 & 47,046 \\\\\nWikipedia     & \\textcolor{white}{00}274     & \\textcolor{white}{0}22,795 & \\textcolor{white}{00}59,990,005    & \\textcolor{white}{0}109,282   & \\textcolor{white}{0}56   & \\textcolor{white}{0}2,632 \\\\\nProofWiki     & \\textcolor{white}{000}23      & \\textcolor{white}{0}23,839  & \\textcolor{white}{000}7,608,526     & \\textcolor{white}{000}6,762     & \\textcolor{white}{0}25   & \\textcolor{white}{00}319  \\\\\nCommonCrawl   & \\textcolor{white}{0}2,560    & \\textcolor{white}{0}75,142  & \\textcolor{white}{0}615,371,126   & \\textcolor{white}{0}367,558   & \\textcolor{white}{0}57  & \\textcolor{white}{0}8,189 \\\\\nStackExchange & \\textcolor{white}{0}1,331  & 433,751 & \\textcolor{white}{0}253,021,062   & \\textcolor{white}{0}125,475   & \\textcolor{white}{0}28  & \\textcolor{white}{00}583 \\\\\narXiv         & 24,576   & 343,830 & 8,324,324,917 & 4,156,454 & \\textcolor{white}{0}20  & 24,211 \\\\ \\midrule\nTotal         & 29,408 & 903,336 &  9,447,509,696  & -         & -    & 10,458  \\\\ \\bottomrule\n\\end{tabular}%\n}\n\n\\end{table*}\n\n\\begin{wrapfigure}{r}{9cm}\n\\centering \n\\includegraphics[width=0.62\\textwidth]{img/all-length-dist-v4.pdf} \n\\caption{Document length distribution (log-scale).} \n\\label{fig:length-dist}\n\\end{wrapfigure}\nWe present detailed statistical information for each component of \\mathpile in Table~\\ref{tab:mathpile-count-stat}, such as the number of documents and the count of tokens. Following our meticulous and comprehensive data collection and processing process, we obtain 29GB of high-quality and diverse math-centric corpus, encompassing around 9.5 billion tokens, from an initial volume of 2.2TB of raw data (cf. Figure~\\ref{fig:math-pile}). Compositionally, arXiv constitutes the largest portion of \\mathpile, while Textbooks represent the smallest share but are of exceptionally high quality.\n\nWe analyze the document length (in terms of token numbers) and their respective proportions from each source within \\mathpile, which is visualized in Figure~\\ref{fig:length-dist}. Intuitively, if the data from each source contains a higher amount of near-duplicates or machine-generated content, the distribution of documents of similar lengths becomes more prevalent, leading to a less smooth distribution curve. Figure~\\ref{fig:length-dist} shows that, thanks to our thorough and rigorous processing, the document length distribution in \\mathpile is relatively smooth across different sources. Note that ProofWiki, due to its fixed format of definitions, lemmas, and proofs, naturally contains shorter content, resulting in a distribution with many similar lengths. We can also observe that, on average, the documents from arXiv and Textbooks tend to be lengthier, while those from ProofWiki and StackExchange are generally shorter.\n\n\\subsection{Continual Pre-training Experiments}\n\\label{sec:continual-pretrain-exp}\n\nWe chose \\texttt{Mistral-7B-v0.1}~\\citep{DBLP:journals/corr/abs-2310-06825-mistral-7b} (the state-of-the-art open-source model at the time) for continual pre-training. We segmented packed text into chunks with a window size of 4,096 and continued pre-training for 3 epochs with a global batch size of 1024. We employ a cosine learning rate schedule with a maximum learning rate of 1e-5 and 1\\% warmup steps. All experiments were conducted on NVIDIA A100 8*80GB GPUs. For evaluation, we employ a range of benchmarks - GSM8K, MATH, MMLU-MATH, AGIEval-SAT-MATH, MathQA, AQuA - to assess varying levels of mathematical reasoning abilities, comparing all models using the same few-shot prompting with greedy decoding.\n\n\\begin{wraptable}{r}{9.6cm}\n\\caption{Results on each subset of \\mathpile and sampled OpenWebMath. The numbers in parentheses represent the number of tokens trained. \\textbf{Bold} results denote improvements over the original Mistral.}\n\\label{tab:continual-pretrain-results}\n\\scalebox{0.61}{\n\\begin{tabular}{l| m{1cm}<{\\centering}  m{1cm}<{\\centering} m{1cm}<{\\centering}  m{1.2cm}<{\\centering}  m{1.2cm}<{\\centering}  m{1.2cm}<{\\centering}}\n\\toprule\n\\textbf{Models} & \\textbf{GSM8K} & \\textbf{MATH}  & \\textbf{SAT-MATH} & \\textbf{MMLU-Math} & \\textbf{MathQA} & \\textbf{AQuA}  \\\\ \\midrule\nMistral-7B-v0.1        & 47.38          & 10.08          & 47.27          & 44.92          & 23.51          & 27.95          \\\\ \\midrule\n+ Textbooks  (0.56B)       & \\textbf{48.97} & \\textbf{12.10} & \\textbf{56.36}    & \\textbf{48.93}     & \\textbf{30.38}  & \\textbf{33.07} \\\\\n+ Wikipedia (0.18B)    & \\textbf{49.96} & \\textcolor{white}{0}9.96           & \\textbf{53.63} & \\textbf{47.16} & \\textbf{28.97} & \\textbf{35.43} \\\\\n+ StackExchange (0.87B) & 43.06          & \\textbf{11.66} & 47.27          & 43.51          & \\textbf{27.67} & \\textbf{30.70} \\\\\n+ Common Crawl (1.83B)  & 45.56          & \\textcolor{white}{0}9.88           & \\textbf{50.45} & \\textbf{45.17} & \\textbf{25.79} & \\textbf{31.88} \\\\\n+ arXiv (0.38B) & \\textbf{47.91} & \\textcolor{white}{0}7.50 & 42.72 & \\textbf{46.34} & 18.05 & 27.55 \\\\\n+ Textbooks, Wikipeida, StackEx., CC (4B) &  \\textbf{49.88} &  \\textbf{11.70} & 43.18 & 43.75 & 23.24 & 25.19 \\\\ \n\\midrule \n+ AMPS (1B) & \\textcolor{white}{0}0.08 & \\textcolor{white}{0}0.82 & \\textcolor{white}{0}3.18 & \\textcolor{white}{0}0.47 & \\textcolor{white}{0}10.99 & \\textcolor{white}{0}8.27 \\\\\n+ DM-Mathematics (5B) &\t\\textcolor{white}{0}0.00  &\t\\textcolor{white}{0}0.00  &\t\\textcolor{white}{0}0.00  &\t\\textcolor{white}{0}0.00  &\t\\textcolor{white}{0}0.00  &\t\\textcolor{white}{0}0.00  \\\\\n+ Sampled OpenWebMath (0.59B) & 43.21 & \\textcolor{white}{0}7.86 &  \\textbf{47.72} & \\textbf{47.52} &  21.80  & 24.80 \\\\ \n\\bottomrule \n\\end{tabular}\n}\n\\end{wraptable}\n\n\\noindent\\textbf{The Effectiveness of \\mathpile} \\quad We further pre-trained \\texttt{Mistral-7B-v0.1} on several subsets, respectively.  As shown in Table~\\ref{tab:continual-pretrain-results}, overall, continual pre-training on the subsets generally enhances performance across diverse math benchmarks, albeit to varying degrees. There are exceptions, such as the lack of improvement on GSM8K after training on StackExchange; we suspect this is due to community users rarely asking basic arithmetic questions on StackExchange. Continual pre-training on arXiv leds to a slight performance boost on GSM8K and MMLU-MATH, but a degradation on MATH, SAT-MATH, and MathQA. We attribute this performance degradation to the disparity between the math knowledge present in arXiv papers and that required for the downstream benchmarks. We also conducted pre-training on a collection of Textbooks, Wikipedia, StackExchange, and CC. Experimental results indicate improved performance on GSM8K and MATH, but not on other benchmarks.  Due to limited computational resources,\\footnote{Pre-training 10 billion tokens for 1 epoch requires approximately 1,760 NVIDIA A100 GPU hours, making us keen to partner with well-resourced corporations to gain deeper insights in the future.} we did not extensively experiment with the entire dataset or combine data from \\mathpile's subsets and existing general corpora, leaving these valuable aspects for future work. Note that we also report some evaluation results on general language benchmarks provided in Appendix~\\ref{appendix-sec:general-benchmarks-eval}.\n\nFurthermore, we also conducted continual pre-training on some existing corpora listed in Table~\\ref{tab:math-corpora-comparison} for comparison, including AMPS, DM-Mathematics and a random subset of OpenWebMath, cleansed of data leakage, in volumes approximately equal to that of Textbooks. Surprisingly, pre-training directly with these synthetic datasets degraded model performance. We attribute this to the narrow, monotonous structure of AMPS and DM-Mathematics problem sets, making them unsuitable for standalone pre-training; such datasets generally yield better results when combined with broader corpora for pre-training~\\citep{xu2024lemur}. Additionally, the OpenWebMath subset produced even less improvement than the same or smaller scale subsets of \\mathpile, such as Textbooks and Wikipedia (cf. Table~\\ref{tab:continual-pretrain-results}), likely due to a need for more tokens to show substantial gains. These results underscore the superior quality of our data.\n\n\\begin{table}[ht]\n\\caption{Ablation study on data processing pipeline and LaTeX display issue resolution}\n\\label{tab:ablation-data-processing-pipeline-exp}\n\\scalebox{0.63}{\n\\begin{tabular}{l| m{2cm}<{\\centering}  m{2.2cm}<{\\centering} m{1cm}<{\\centering} m{1cm}<{\\centering} m{1.2cm}<{\\centering} m{1.3cm}<{\\centering} m{1.2cm}<{\\centering} m{1cm}<{\\centering}}\n\\toprule\n\\textbf{Models} &\n  \\textbf{Global Data Processing} &\n  \\textbf{Fix Latex Display Issue} &\n  \\textbf{GSM8K} &\n  \\textbf{MATH} &\n  \\textbf{SAT-MATH} &\n  \\textbf{MMLU-MATH} &\n  \\textbf{MathQA} &\n  \\textbf{AQuA} \\\\ \\midrule\nMistral-v0.1-7B        & - & - & 47.38          & 10.08 & 47.27          & 44.92          & 23.51          & 27.95          \\\\ \\midrule\n+ Sampled raw Wikipedia (0.55B)              & \\ding{55}                & \\ding{55}                & 41.92          & \\textcolor{white}{0}6.28  & 20.90           & 23.70           & 24.72          & 24.01      \\\\\n+ Full raw Wikipedia (2.18B) & \\ding{55} &  \\ding{55} & 32.30 & \\textcolor{white}{0}4.48  & 13.64 & 25.59 & 27.04 & 23.62 \\\\\n+ Full cleaned but LaTeX issued Wikipedia  (0.23B)                  & \\ding{51}                & \\ding{55}               & 47.15          & \\textcolor{white}{0}8.58  & 46.81          & 42.92          & 21.00             & 31.88          \\\\ \\midrule\n+ Full cleaned Wikipedia (0.18B)                    & \\ding{51}                & \\ding{51} & \\textbf{49.96} & \\textcolor{white}{0}9.96  & \\textbf{53.63} & \\textbf{47.16} & \\textbf{28.97} & \\textbf{35.43} \\\\ \\bottomrule\n\\end{tabular}%\n}\n\n\\end{table}\n\n\\noindent\\textbf{The Effectiveness of Data Processing Pipeline} \\quad We utilized the Wikipedia subset as a testbed to evaluate our data processing pipeline. We distinguished between raw Wikipedia, which is collected but not globally processed, and cleaned Wikipedia, which has undergone global data processing. Additionally, we performed an ablation study on LaTeX display issues in Wikipedia (cf. Figure~\\ref{fig:latex-display-issue-case}), attributed to HTML-to-text conversion tools, by comparing documents with problematic and correct LaTeX displays. Following previous settings, we executed continual pre-training on these datasets. Results in Table~\\ref{tab:ablation-data-processing-pipeline-exp} indicate that skipping our pipeline notably reduces Mistral's mathematical reasoning abilities, unaffected by increased training size (i.e., 2.18B). Furthermore, correct LaTeX display in documents is vital for enhancing reasoning capabilities, as shown by the last two rows of Table~\\ref{tab:ablation-data-processing-pipeline-exp}. These findings underscore our pipeline's effectiveness and shed light on the superior importance of data quality over quantity, even in the continual pre-training phase.\n\n\\section{Related Work}\n\n\\noindent\\textbf{Pre-training Corpora for Language Models} \\quad In language modeling, early models like GPT~\\citep{radford2018improving} and BERT~\\citep{devlin-etal-2019-bert} are trained on resources such as Books~\\citep{DBLP:conf/iccv/ZhuKZSUTF15-Books} and Wikipedia. Later models like GPT-2~\\citep{radford2019gpt-2} and T5~\\citep{DBLP:journals/jmlr/RaffelSRLNMZLL20-T5} expand training to include web data from Reddit (WebText) and Common Crawl (C4). GPT-3~\\citep{DBLP:conf/nips/BrownMRSKDNSSAA20-gpt-3} enlarges its corpus to 300 billion tokens, combining Common Crawl, WebText, Books, and Wikipedia. Pile \\citep{DBLP:journals/corr/abs-2101-00027-pile} introduces a diverse collection of 22 datasets for large-scale pre-training. The Gopher project~\\citep{DBLP:journals/corr/abs-2112-11446-gopher} compiles a 10.5TB corpus, and PaLM~\\citep{DBLP:journals/jmlr/ChowdheryNDBMRBCSGSSTMRBTSPRDHPBAI23-palm} is built from a 780 billion-token corpus, both closed-source. BLOOM~\\citep{DBLP:journals/corr/abs-2211-05100-bloom} uses the ROOTS dataset~\\citep{DBLP:conf/nips/LaurenconSWAMSW22-roots} for multilingual pre-training. The Stack~\\citet{DBLP:journals/corr/abs-2211-15533-stack} provides a 3.1 TB code dataset. LLaMA~\\citep{DBLP:journals/corr/abs-2302-13971-llama} utilizes various data sources but doesn't release its corpus, unlike RedPajama~\\citep{RedPajama} and its de-duplicated version SlimPajama~\\citep{SlimPajama}. RefinedWeb shows web-only corpora can rival curated ones~\\citep{DBLP:journals/corr/abs-2306-01116-refinedweb}. Recent models like GPT-4~\\citep{DBLP:journals/corr/abs-2303-08774-gpt-4}, Mistral-7B~\\citep{DBLP:journals/corr/abs-2310-06825-mistral-7b} and the lastest Gemini~\\citep{team2023gemini} have refrained from open-sourcing data. Constructing diverse, high-quality pre-training corpora is crucial for narrowing the performance gap with closed-source models, reflecting our work's aim.\n\n\\noindent\\textbf{Pre-training Benchmarks and Corpora for Mathematical Reasoning} \\quad The challenge of endowing models with human-like mathematical reasoning has attracted significant interest from the machine learning and natural language processing communities. To evaluate models' mathematical capabilities, several benchmark datasets have been developed, including AQuA~\\citep{ling-etal-2017-program}, DM-Mathematics~\\citep{DBLP:conf/iclr/SaxtonGHK19-deepmind-mathematics}, SVAMP~\\citep{patel-etal-2021-nlp},  GSM8K~\\citep{DBLP:journals/corr/abs-2110-14168-GSM8K}, and MATH~\\citep{DBLP:conf/nips/HendrycksBKABTS21-math}, which cover a range of complexities from basic arithmetic to competition-level mathematics. Additionally, benchmarks like NaturalProofs~\\citep{DBLP:conf/nips/Welleck0BHCCC21-naturalproofs} focus on theorem-proving capabilities, while the STEM subset of MMLU~\\citep{DBLP:conf/iclr/HendrycksBBZMSS21-MMLU} evaluates understanding across multiple tasks in science, technology, engineering, and mathematics. To improve models' mathematical reasoning, pre-training corpora like AMPS~\\citep{DBLP:conf/nips/HendrycksBKABTS21-math} (despite a large-scale synthetic exercise set), ProofPile~\\citep{DBLP:journals/corr/abs-2302-12433-proofpile}, and OpenWebMath~\\citep{DBLP:journals/corr/abs-2310-06786-openwebmath} have been introduced, targeting various levels of mathematical problem-solving and theorem proving. Unlike Google's Minerva~\\citep{DBLP:conf/nips/LewkowyczADDMRS22-minerva} and OpenAI's MathMix~\\citep{DBLP:journals/corr/abs-2305-20050-lets-verify-step-by-step}, which are not public, our work focuses on creating a high-quality and diverse mathematical corpus from diverse sources to fill existing gaps.\n\n\\section{Conclusion and Limitations}\n\nIn this work, we present \\mathpile, a specialized corpus centered around mathematics, characterized by its diversity and high quality. Throughout its development, we meticulously source and gather data, applying a rigorous and math-specific pipeline. This pipeline encompasses various stages such as preprocessing, prefiltering, language identification, cleaning and filtering, and deduplication, all aimed at maintaining the high quality of the corpus. We also conduct data contamination detection to remove duplicates from popular mathematical reasoning benchmark test sets, crucial for ensuring their integrity and effectiveness, an aspect often overlooked in other similar works. We aim for our \\mathpile to enhance mathematical reasoning in language models, whether used alone or in conjunction with other datasets, to promote broader applications.\n\nThis dataset also has some limitations. Many detailed decisions in its creation were made empirically, which may not always be optimal, and verifying decisions directly can be challenging. Moreover, the data scale is insufficient for training extra-large models; subsets like the common crawl could be expanded. Furthermore, the dataset is focused primarily on English, highlighting the need to construct high-quality datasets for other languages. Future research could also explore data mixing~\\citep{liu2024regmix} and model-based pre-training corpus refinement~\\citep{yu2024mates,zhou2024programming} to enhance dataset quality and model performance.\n\n\\begin{ack}\n  This work was partially funded by the National Natural Science Foundation of China (62476168), Shanghai Artificial Intelligence Laboratory.\n\\end{ack}\n\n\\clearpage\n\n\\section*{Checklist}\n\n\\begin{enumerate}\n\n\\item For all authors...\n\\begin{enumerate}\n  \\item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?\n    \\answerYes{}\n  \\item Did you describe the limitations of your work?\n     \\answerYes{}\n  \\item Did you discuss any potential negative societal impacts of your work?\n   \\answerNo{}\n  \\item Have you read the ethics review guidelines and ensured that your paper conforms to them?\n    \\answerYes{}\n\\end{enumerate}\n\n\\item If you are including theoretical results...\n\\begin{enumerate}\n  \\item Did you state the full set of assumptions of all theoretical results?\n    \\answerNA{}\n\t\\item Did you include complete proofs of all theoretical results?\n    \\answerNA{}\n\\end{enumerate}\n\n\\item If you ran experiments (e.g. for benchmarks)...\n\\begin{enumerate}\n  \\item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?\n    \\answerYes{}\n  \\item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?\n     \\answerYes{}\n\t\\item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?\n     \\answerNo{}\n\t\\item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?\n    \\answerYes{}\n\\end{enumerate}\n\n\\item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\n\\begin{enumerate}\n  \\item If your work uses existing assets, did you cite the creators?\n    \\answerYes{}\n  \\item Did you mention the license of the assets?\n   \\answerYes{}\n  \\item Did you include any new assets either in the supplemental material or as a URL?\n    \\answerNo{}\n  \\item Did you discuss whether and how consent was obtained from people whose data you're using/curating?\n    \\answerYes{}\n  \\item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?\n    \\answerYes{}\n\\end{enumerate}\n\n\\item If you used crowdsourcing or conducted research with human subjects...\n\\begin{enumerate}\n  \\item Did you include the full text of instructions given to participants and screenshots, if applicable?\n    \\answerNA{}\n  \\item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?\n    \\answerNA{}\n  \\item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?\n    \\answerNA{}\n\\end{enumerate}\n\n\\end{enumerate}\n\n\\clearpage\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2104.01112v2.tex",
        "arXiv-2310.06786v1.tex",
        "arXiv-2312.17120v2.tex"
    ],
    "group_id": "group_99",
    "response": "### Title: Advances in Mathematical Reasoning through Large-Scale Natural Language Datasets\n\n### Introduction\n\nThe field of natural language processing (NLP) has seen significant advancements in recent years, driven by the development of large-scale language models that can understand and generate human-like text. These models have been particularly effective in tasks involving natural language alone, but their application to mathematical reasoning, especially in natural mathematical language (NML), remains a challenging and underexplored area. NML is a unique blend of symbolic and natural language, often used in informal mathematical contexts, which is crucial for human understanding and communication in mathematics. The ability to process and generate NML could lead to transformative developments in education, scientific discovery, and automated reasoning, bridging the gap between informal and formal mathematical systems.\n\nHistorically, efforts to integrate machine learning with mathematical reasoning have largely focused on formalized mathematics, where mathematical statements are written in verifiable formal languages similar to source code. This approach, while valuable, does not fully capture the nuances of informal mathematical language as used by humans. Recent work has highlighted the importance of leveraging both formal and informal mathematics to improve the reasoning capabilities of large language models. This has led to the creation of datasets and benchmarks that aim to facilitate the study of mathematical reasoning in natural language, such as NaturalProofs, OpenWebMath, and MathPile.\n\nCurrent progress in this field has been marked by the development of specialized datasets and benchmarks, each with its own strengths and limitations. These datasets aim to provide a rich and diverse corpus of mathematical content, enabling researchers to evaluate and improve the reasoning abilities of large language models. However, challenges remain, particularly in terms of out-of-domain generalization, the ability to fully recover a proof's references, and the integration of formal and informal mathematical content. This summary will explore three recent papers that contribute to this field: NaturalProofs, OpenWebMath, and MathPile.\n\n### Main Content of Each Paper\n\n#### NaturalProofs\n\nNaturalProofs is a multi-domain dataset designed to study mathematical reasoning in natural language. It unifies broad-coverage data from ProofWiki, deep-coverage data from the Stacks project, and low-resource, real-world data from mathematics textbooks. The dataset contains 32,579 theorems, 14,230 definitions, and 1,974 other types of pages, including axioms and corollaries. Each statement in NaturalProofs is structured as a title, contents, and references, where the contents are a mixture of symbolic and natural language text. The dataset is available at \\url{https://github.com/wellecks/naturalproofs}.\n\nNaturalProofs introduces two main tasks: mathematical reference retrieval and generation. The reference retrieval task involves retrieving the set of references (theorems, lemmas, definitions) that occur in a given theorem's proof, while the reference generation task requires recovering the order and number of references in each proof. These tasks are evaluated using standard retrieval metrics such as mean average precision (mAP), recall, and full recovery rate, which check whether all references in the proof are within the top-$k$ predicted rankings. The dataset is split into training, validation, and test sets, with the test set comprising novel theorems from each domain to evaluate out-of-distribution generalization.\n\nThe authors benchmark several neural methods, including BERT-based models, on these tasks. They find that BERT models show strong in-domain performance compared to classical information retrieval methods like TF-IDF. However, out-of-domain generalization remains a challenge, with BERT models underperforming on low-resource textbook data. The authors also introduce a joint retrieval model that leverages reference embeddings to improve retrieval performance, particularly at the top of the ranked list. This model shows promise in refining the retrieval of key references but struggles with full recovery and out-of-domain generalization.\n\n#### OpenWebMath\n\nOpenWebMath is an open dataset of high-quality mathematical web text, containing 14.7 billion tokens sourced from Common Crawl. The dataset is designed to improve the reasoning abilities of large language models by providing a rich corpus of mathematical content in a natural language format. Unlike other web datasets that may not faithfully preserve mathematical notation, OpenWebMath employs a detailed pipeline to extract and preserve \\LaTeX{} content while removing boilerplate and low-quality text. The dataset is openly released on the Hugging Face Hub and can be accessed at \\url{https://huggingface.co/datasets/open-web-math/open-web-math}.\n\nThe creation process of OpenWebMath involves several stages: prefiltering, text extraction, language identification, quality filtering, and deduplication. Prefiltering uses a stack of filters to identify documents with mathematical content, while text extraction leverages Resiliparse to remove boilerplate and extract main content. Language identification employs fastText to filter non-English documents, and quality filtering uses perplexity scores and a MathScore classifier to remove low-quality documents. Deduplication is performed using SimHash to eliminate exact and near-duplicate documents.\n\nOpenWebMath is analyzed for its composition, with the majority of content coming from Q\\&A forums, educational content, and scientific papers. The dataset is also evaluated for its effectiveness in training language models on various mathematical reasoning benchmarks, including GSM8k, MATH, MMLU-STEM, AGIEval-SAT-MATH, MathQA, and AQuA. The authors find that models trained on OpenWebMath outperform those trained on general language data, particularly in terms of perplexity and accuracy on specific subsets of these benchmarks. However, the dataset is not as effective when used alone, suggesting that it is best suited for use in conjunction with other datasets.\n\n#### MathPile\n\nMathPile is a high-quality, diverse pre-training corpus for mathematical reasoning, consisting of about 9.5 billion tokens from various sources including arXiv, Wikipedia, ProofWiki, StackExchange, and Common Crawl. The dataset is designed to improve the reasoning capabilities of large language models by providing a rich and diverse collection of mathematical content. Unlike other datasets that may prioritize quantity over quality, MathPile adheres to the principle of \"less is more,\" focusing on the quality of the data rather than its sheer volume.\n\nThe collection process of MathPile involves specific data preprocessing for each source, followed by global data processing steps such as language identification, filtering, and deduplication. Language identification uses fastText to filter non-English documents, while filtering removes low-quality content based on heuristic rules. Deduplication is performed using MinHash LSH to eliminate exact and near-duplicate documents. The dataset is also analyzed for data contamination, where evaluation examples are found in the training set, and measures are taken to remove these duplicates.\n\nMathPile is evaluated for its effectiveness in continual pre-training experiments using Mistral-7B-v0.1. The authors find that continual pre-training on subsets of MathPile generally enhances performance on various mathematical reasoning benchmarks, although the improvements vary across different subsets. For example, pre-training on arXiv leads to a slight performance boost on GSM8K and MMLU-MATH but a degradation on MATH and MathQA. This suggests that the effectiveness of pre-training depends on the specific composition of the dataset. The authors also conduct an ablation study to evaluate the impact of their data processing pipeline and find that correct LaTeX display in documents is crucial for enhancing reasoning capabilities.\n\n### Commonalities and Innovations\n\nAll three datasets aim to improve the mathematical reasoning capabilities of large language models by providing a rich corpus of mathematical content in natural language. They differ in their sources and preprocessing methods but share the goal of facilitating research in this domain. NaturalProofs focuses on structured mathematical content from ProofWiki, Stacks, and textbooks, while OpenWebMath and MathPile include a broader range of web-sourced content.\n\nNaturalProofs introduces a unique schema for mathematical statements, proofs, and references, allowing for a comprehensive evaluation of in-domain and out-of-domain performance. It also provides a benchmark for mathematical reference retrieval and generation tasks, which are crucial for theorem proving. OpenWebMath, on the other hand, employs a detailed pipeline for extracting and preserving \\LaTeX{} content, ensuring that mathematical notation is faithfully represented. This dataset is particularly useful for pretraining large language models on high-quality mathematical content.\n\nMathPile innovates by adhering to the principle of \"less is more,\" focusing on the quality of the data rather than its quantity. It includes a range of sources, from arXiv papers to textbooks and web pages, and employs a rigorous data processing pipeline to remove low-quality and duplicate content. The dataset is also evaluated for its effectiveness in continual pre-training experiments, providing insights into how different subsets of mathematical content impact model performance.\n\n### Comparison of Results\n\nThe results from the three datasets highlight the effectiveness of domain-specific pretraining in improving mathematical reasoning capabilities. NaturalProofs shows that BERT models trained on its dataset outperform classical information retrieval methods like TF-IDF in in-domain performance, with improvements in metrics such as mAP, recall, and full recovery rate. However, out-of-domain generalization remains a challenge, with BERT models underperforming on low-resource textbook data.\n\nOpenWebMath demonstrates that models trained on its dataset outperform those trained on general language data, particularly in terms of perplexity and accuracy on specific subsets of mathematical reasoning benchmarks. However, the dataset is not as effective when used alone, suggesting that it is best suited for use in conjunction with other datasets.\n\nMathPile's continual pre-training experiments show that different subsets of mathematical content have varying impacts on model performance. For example, pre-training on arXiv leads to a slight performance boost on GSM8K and MMLU-MATH but a degradation on MATH and MathQA. This underscores the importance of carefully selecting the composition of pretraining data to maximize its effectiveness.\n\n### Conclusion\n\nThe three datasetsâ€”NaturalProofs, OpenWebMath, and MathPileâ€”represent significant advancements in the field of mathematical reasoning through large-scale natural language datasets. They provide rich and diverse corpora of mathematical content, enabling researchers to evaluate and improve the reasoning capabilities of large language models. However, challenges remain, particularly in terms of out-of-domain generalization and the full recovery of a proof's references.\n\nNaturalProofs highlights the importance of structured data and benchmarks for evaluating mathematical reasoning in natural language. OpenWebMath demonstrates the effectiveness of web-sourced content in improving model performance, while MathPile emphasizes the importance of data quality over quantity. Future research could explore the integration of these datasets to create a more comprehensive and effective pretraining corpus, as well as the development of new methods for out-of-domain generalization and the full recovery of proof references.\n\n### Future Research Directions\n\nFuture research could focus on several key areas to further advance the field of mathematical reasoning in large language models:\n\n1. **Out-of-Domain Generalization**: Developing methods that improve out-of-domain generalization, such as domain transfer techniques and few-shot learning approaches, could help models better handle novel mathematical content.\n2. **Proof Generation**: Extending the reference generation task to full proof generation could provide a more comprehensive evaluation of a model's reasoning capabilities.\n3. **Data Mixing and Refinement**: Exploring the benefits of data mixing and model-based pre-training corpus refinement could help create more effective and diverse pretraining datasets.\n4. **Cross-Domain Evaluation**: Conducting cross-domain evaluations to better understand the transferability of mathematical reasoning skills across different mathematical domains and applications.\n5. **Language Model Transparency**: Enhancing the transparency of pretraining corpora through detailed data sheets and quality annotations could help researchers better understand and evaluate the impact of different datasets on model performance.\n\nBy addressing these challenges and exploring these research directions, the field of mathematical reasoning in large language models can continue to make significant strides, ultimately leading to more capable and versatile AI systems in mathematics and related fields."
}