{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{L-Eval: Instituting Standardized Evaluation for Long Context Language Models}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nRecently, there has been growing interest in extending the context length of large language models (LLMs), aiming to effectively process long inputs of one turn or conversations with more extensive histories. While proprietary models such as GPT-4 and Claude can largely preserve the reasoning ability in an extended context, open-source models are still progressing through the early stages of development. \nTo bridge this gap, we propose L-Eval to institute a more standardized evaluation for long context language models (LCLMs) addressing two key aspects: dataset construction and evaluation metrics. On the one hand, we build a new evaluation suite containing 20 sub-tasks, 508 long documents, and over 2,000 human-labeled query-response pairs encompassing diverse question styles, domains, and input length (3k$\\sim$200k tokens). On the other hand, we investigate the effectiveness in evalution metrics for LCLMs. Results show that popular n-gram matching metrics generally can not correlate well with human judgment, and thus we strongly advocate for length-instruction-enhanced (LIE) evaluation and employing LLM judges.  We conducted a comprehensive study of 4 popular commercial LLMs and 12 open-source counterparts using the L-Eval benchmark. Our empirical findings offer useful insights into the study of LCLMs and lay the groundwork for the development of more principled evaluation of these models.\\footnote{We release our new evaluation suite, code, and all generation results on  \\url{https://github.com/OpenLMLab/LEval}}\n\n\\end{abstract}\n\n\\section{Introduction}\nCurrently, a significant amount of effort is being dedicated to research on extending the context length of large language models. Popular solutions mainly involve further pretraining or finetuning standard models on longer inputs using more efficient architectures~\\citep{ding2023longnet, NEURIPS2022_67d57c32, liang2023unleashing, mohtashami2023landmark, li2023incontext}, as well as scaled positional embedding~\\citep{su2022roformer, sun2022lengthextrapolatable, fixedNTK, qin2023linear}. \n\nThere are extensive multi-task benchmarks~\\citep{hendrycks2021measuring, suzgun2022challenging} for language models with short prompts, yet a high-quality one in long context modeling has not yet been established, presenting an opportunity for further development in this area.  Meanwhile, almost all previous long-sequence text generation benchmarks relied primarily on n-gram matching metrics~\\citep{zhang2023cab, shaham2022scrolls}, such as ROUGE~\\citep{lin-2004-rouge}. Whether these commonly used metrics correlate well with human judgment when testing LCLMs in a zero-shot setting remains a question. Furthermore, the open-source community has released a considerable number of language models with 16k, or 32k context length~\\citep{longchat2023, du2022glm}. A comprehensive comparative study of these models can be of great value.\n\nTo address these issues, we propose \\textit{L-Eval} to call for a more standardized evaluation of long context language models. For dataset construction, L-Eval has 20 sub-tasks, 4 sub-tasks are annotated from scratch (\\S\\ref{sec:scratch}), 4 sub-tasks are re-annotated from the public datasets (\\S\\ref{sec:re-anno}), and the remaining 12 sub-tasks are manually cleaned from previous long sequence datasets.  \nWe divide these tasks in L-Eval into two groups: closed-ended tasks and open-ended tasks. The closed-ended group primarily tests the reasoning and understanding ability regarding a longer context, and the open-ended group consists of more summarization tasks that require aggregation of long document information. In the design of L-Eval, we prioritize diversity and quality over quantity, ensuring correctness by manually validating all samples after data collection (\\S\\ref{sec:postprocess}). Our data diversity, indicative in question styles, domain selection, and input lengths, is detailed in Table~\\ref{tab:datasets}.\n\nIn addition, the development of suitable evaluation metrics for LCLMs on open-ended tasks where multiple outputs are acceptable is crucial, yet challenging. In this work, we study the limitations of traditional metrics based on lexical matching. We demonstrate that these metrics often fail to correlate with human evaluation results. Our further experiments suggest that LLM judges~\\citep{alpaca_eval, zheng2023judging} provide superior accuracy in the evaluation of open-ended tasks. \\S\\ref{sec:metric} explains how we set a short-context LLM judge in a long-context evalution setting. \nConsidering the influence of generation length on performance and in order to avoid drawing misleading conclusions, we propose the Length-Instruction-Enhanced (LIE) evaluation technique for all reference-based metrics, including those employing an LLM judger. The empirical results demonstrate a substantial improvement brought by LIE evaluation in the Kendall-Tau correlation coefficient ($\\tau$) with human judgments (Figure~\\ref{fig:cor}), for all automatic metrics.\n\nWe also conducted a comprehensive study with 16 different LLMs (\\S\\ref{sec:baselines}) in L-Eval. Some of our key findings are summarized below:\n(1) There is still a significant gap between open-source LCLMs and commercial models, for both closed-ended tasks (Table~\\ref{table:acc_exam}) and open-ended tasks evaluated by LLMs and human (Table~\\ref{tab:llm_eval},~\\ref{tab:human_eval}). However, this gap is not accurately reflected by n-gram metrics.\n(2) While current efforts on open-source LCLMs improve performance on closed-ended tasks, they significantly fall short on open-ended tasks. This is largely due to the models' misunderstanding of instructions as the input context length increases.\n(3) Experiments on GPT-3.5-Turbo with both dense and sparse retrievers show that end-to-end full-context models outperform traditional retrieval-based systems.\n(4) Training-free scaled positional embeddings can enhance the retrieval capability of LLMs over longer input, while it may adversely affect their reasoning ability.\n    \n\nMore interesting conclusions can be found in \\S\\ref{sec:main_results} and \\S\\ref{sec:analysis_app}. We hope \\textit{L-Eval} and our findings contribute to a deeper understanding of current LCLM research and the further development of models and evaluation metrics.\n\n\\subsection{Long Context Language Models}\\label{sec:lclms}\nFeeding long context leads to bottlenecks in language model training and inference due to computational resources. Some community efforts focus on developing \n {efficient attention} mechanisms to build efficient language models~\\citep{sun2023retentive,ding2023longnet,li2023incontext,fu2023hungry,peng2023rwkv}. \nIn addition to optimizing the attention mechanism, some works~\\citep{bulatov2023scaling,dai-etal-2019-transformer, mohtashami2023landmark} focus on {chunking the input} to model both the current text in the chunk and the previous context states, effectively extending the length of context processing.\nBesides the efficiency challenge, the {scalability of positional embedding} is also crucial. ALiBi~\\citep{press2022train}, and \\textsc{xPos}~\\citep{sun2022lengthextrapolatable} emphasize the significance of local context to enhance the language model's ability to perform extrapolation.\nMoreover, position interpolation (PI)~\\citep{chen2023extending} and NTK-aware~\\citep{fixedNTK, dynamicNTK} are the most popular approaches based on RoPE~\\citep{su2022roformer} to efficiently and effectively extend the context length. However, these works mainly validated their methods with perplexity (PPL)~\\citep{sun2021long, fixedNTK}, and there has not been systematic validation on practical tasks.  \n\n\\subsection{Long Sequences Benchmarks}\n\\cite{tay2020long} introduce the Long Range Arena (LRA), a benchmark encompassing five distinct classification tasks. CAB~\\citep{zhang2023cab} is another benchmark for different efficient attention designs by comparing both efficiency and accuracy. In language domain, \nprevious work on LCLMs tends to report PPL to evaluate language models~\\citep{su2022roformer, peng2023yarn} on longer context. However, PPL may not usually correlate with the actual performance~\\citep{sun2021long}.  ZeroScrolls~\\citep{shaham2022scrolls, shaham2023zeroscrolls} and LongBench~\\citep{bai2023longbench} are concurrent long context evaluation suites. L-Eval differs from them in 3 aspects: (1) Manually selected samples. Testing samples are automatically filtered by their benchmarks, while those for L-Eval are manually filtered. (2) Standardized metrics.  We are the first to investigate the correlations between traditional lexical metrics and recently proposed LLM metrics with human judgment on Long context settings. L-Eval no longer mainly relies on N-gram metrics.  (3) More closed-ended tasks. Due to fairness issues in open-ended tasks. L-Eval has more closed-ended tasks reflecting unbiased results.\n\n\\section{Towards High-Quality and Diverse Long Context DataSets}\n\\label{sec:data}\nIn this section, we highlight some key procedures in L-Eval data construction.\nConcretely, we show the annotation, re-annotation, and manual filtering pipeline and the statistics of L-Eval.  Please refer to Appendix~\\ref{sec:data-appendix} for the complete annotation details and examples. \n\n\\subsection{Data Annotation from Scratch}\\label{sec:scratch}\nThere are 4 datasets annotated from scratch in L-Eval: Coursera, SFcition, CodeU, and LongFQA. The original resources are videos from Coursera, previous open-source datasets, source code from famous Python libraries, and public earning call transcripts, respectively.\n\n\\vspace{-0.7em}\n\\paragraph{Coursera} This dataset originates from the Coursera website.\\footnote{\\url{https://coursera.org/}} To reduce the difficulty of annotation, we choose four public courses related to big data and machine learning (\\S\\ref{sec:coursera}).\nThe input long document is the subtitles of the videos. Questions and the ground truth answers are labeled by the authors. The instruction style of Coursera takes the format of multiple choice. In order to increase the difficulty of the task, we have set \\textbf{multiple correct options}. To the best of our knowledge, this is the first multi-choice dataset with multiple correct answers and it is more challenging than single-option questions (Table~\\ref{table:acc_exam}).\n\n\\vspace{-0.7em}\n\\paragraph{SFcition} \nWe annotate this sub-task to test the loyalty of the LCLM to the input context. We argue that in LCLMs, contextual knowledge (stored in long input) is more crucial than parametric knowledge (gained during pretraining). Practically, many long documents are private and can never be seen during pretraining. LLMs should follow the contextual knowledge instead of parametric knowledge in long context settings. To simulate this scenario, we annotate a science fiction dataset consisting of True or False questions. Most of the answers to these questions contradict real-world principles and do not comply with actual physical laws (\\S\\ref{sec:sfiction}). We find that Turbo-16k struggles on this task, which tends to answer questions relying on parametric knowledge (Table~\\ref{table:acc_exam}).\n\n\\vspace{-0.7em}\n\\paragraph{CodeU} As a code understanding dataset, it requires LLM to infer the output of a lengthy Python program. We mainly use source code from Numpy\\footnote{\\url{https://github.com/numpy/numpy}} and construct a string processing codebase. To prevent LLMs from answering the question based on their parametric knowledge, we replace the original function name. LLMs should first locate where the function is called and determine which functions are invoked. CodeU is the most challenging task in L-Eval (\\S\\ref{sec:codeU}).\n\n\\vspace{-0.7em}\n\\paragraph{LongFQA} We also notice that there is a lack of long context question answering datasets in the finance domain and we annotate the QA pairs based on public earning call transcripts from the \\textit{Investor Relations} section of 6 company websites.  Please refer to \\S\\ref{sec:longfqa} for details.\n\n\\subsection{Data Re-annotation from Public Datasets}\\label{sec:re-anno}\nWe re-annotate 5 publicly available datasets in L-Eval. \n\\textbf{GSM(16-shot)} is derived from 100-grade school math problems in the GSM8k dataset~\\citep{cobbe2021training}. If the LCLM maintain its reasoning ablilty on longer context, ultizing more high-quality examples will a positive effect on solving math problems~\\citep{li2023incontext}. We construct 16 in-context examples with lengthy Chain-of-Thought where 8 examples come from  \\textit{chain-of-thought-hub}\\footnote{\\url{https://github.com/FranxYao/chain-of-thought-hub}} and 8 examples are constructed by us.  We experiment with the newly constructed examples and the accuracy of Turbo-16k-0613 rises from 79 (8-shot) to 84 (16-shot).  \n\nWe inject come new synthesis instructions to test global context modeling into \\textbf{QuALITY}~\\citep{pang2022quality}, such as \\textit{``What can we infer from the longest sentence in this story?''} and \\textit{``How many words are there in the story?''}. Given that these types of questions may rarely occur in real-world conversations, their proportion in L-Eval is extremely small. \nThe \\textbf{Openreview} dataset contains papers collected from \\url{openreview.net}. We ask the model to (1) write an Abstract\nsection, (2) summarize the related work, and (3) finally give feedback including valuable suggestions and \nsome questions for the authors. We select the paper with high-quality related work sections and helpful reviews written by human reviewers to form this test set.\\footnote{Ethic statement:\nwe discourage reviewers from using large models for reviews. Our goal is to assist authors in further improving their papers.} Next, we use \\textbf{SPACE}~\\citep{angelidis-etal-2021-extractive} to test the aspect-based review summarization task, and the instructions for the dataset are annotated by us. We adopt diverse instructions to prevent overfitting.\n\n\\begin{wrapfigure}{r}{0.4\\textwidth}\n  \\centering\n  \\vspace{-4mm}\n  \\includegraphics[width=0.4\\textwidth]{fig/topic_ret.pdf}\n  \\caption{Test Accuracy (\\%) of different models with retrieving the first topic and retrieving the second/third topic.}\n  \\label{fig:topic}\n  \\vspace{-5mm}\n\\end{wrapfigure}\nPrevious work~\\citep{longchat2023,liu2023lost} has used retrieval tasks to test the ability of modeling long context dependency via retrieving something over lengthy context. L-Eval includes a popular first topic retrieval task \\textbf{TopicRet}~\\citep{longchat2023}, formatted as: ``\\textit{[topic-1] Chat History [instruction]} ''. However, as we can see from Figure~\\ref{fig:topic}, retrieving the first topic is too easy to distinguish the ability of different models. However, the task of retrieving the second and the third topics presents a significantly higher level of challenge. It is observed that nearly all open-source models struggle in  task.  So we enhance the task with second/third topic retrieval.\n\n\\subsection{Data Filtering and Correction}\\label{sec:postprocess}\nThe remaining 12 tasks originates from existing datasets following previous evaluation suites~\\citep{zhang2023cab}.  However, L-Eval involves more human labor after data collection because we find the annotation quality of previous long sequence datasets fluctuates severely and there are many unanswerable questions that are unrelated to the context. These mistakes can hardly be corrected using the automatic preprocessing scripts in previous works. In L-Eval, all samples are manually filtered and corrected after data collection. Specifically, we use Claude-100k as our assistant to filter mistaken QAs and unanswerable questions. First, we input the lengthy document into Claude and request it to provide the answer and offer an explanation. If Claude produces an answer greatly mismatching the ground truth or states that we cannot deduce the answer from the context, we will either perform re-annotation or simply remove them. \n\n\\begin{table}[t]\n\\vspace{-1em}\n\\centering  \n\\caption{This table presents the statistics of the L-Eval suite where \\textbf{Question-style} indicates the type of task or the style of instruction in the dataset, \\textbf{\\#Doc} refers to the number of long documents, and \\textbf{\\#Instr} denotes the number of instructions provided for each long input. \\textbf{Avg/Max len} signifies the average/maximum length of the document inputs. We tokenize the raw text with Llama2 tokenizer and report the number of tokens.\n}\n\\vspace{-0.5em}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{lllrrrr}\n\\toprule\n\\bf Dataset &\\bf Question-style &\\bf Domain &\\bf Avg len & \\bf Max len &\\bf \\#Instr  &\\bf \\#Doc \\\\\n\\midrule\n\\rowcolor{mypink!50}\n\\multicolumn{7}{c}{\\textbf{\\textit{Closed - Ended \\, Tasks}}} \\\\\n\\midrule \n\nTOEFL & Multiple choice & English test & 3,907 & 4,171  &  269 & 15\\\\\nGSM(16-shot)$^\\dag$ & Solving math problems & In-context examples & 5,557  & 5,638 & 100 & 100  \\\\\nQuALITY~\\cite{}$^\\dag$ & Multiple choice & Gutenberg & 7,169  & 8,560  & 202 & 15 \\\\\nCoursera$^*$  & Multiple choice  & Advanced courses & 9,075 & 17,185 & 172  & 15 \\\\\nTopicRet$^\\dag$  & Retriving topics & Conversation  &  12,506 & 15,916  & 150 & 50 \\\\\nSFcition$^*$ & True or False Questions & Scientific fictions & 16,381 & 26,918 & 64 & 7 \\\\\nCodeU$^*$ & Deducing program outputs & Python Codebase  &  31,575 & 36,509 & 90 & 90 \\\\\n\n\\midrule\n\\rowcolor{mypink!50}\n\\multicolumn{7}{c}{\\textbf{\\textit{Open - Ended \\, Tasks}}} \\\\\n\\midrule\nMultiDoc2Dial & Goal-oriented dialogues & Grounded documents & 3,905 & 7888  & 136 & 20 \\\\\nQasper & QA on papers & NLP papers & 5,019 & 6,547  & 160 & 20 \\\\\nLongFQA$^*$  & QA on earning call & Finance  & 6,032 & 7824  & 52 & 6 \\\\\nNQ & QA from Google Search & Wikipedia & 23,698 & 47,726 & 104 & 20\\\\\nCUAD & Extracting key information & Law & 30,966 & 68,625  & 130 & 20 \\\\\nNarrativeQA & QA on narratives  & Gutenberg  & 62,335  & 210,541 & 182 & 20 \\\\\n\\midrule\nMulti-News & Multi-doc Summarization & Multiple News articles  & 7,320& 19,278  & 11 &11 \\\\\nGovReport & Single-doc Summarization & Government reports  & 7,495 & 27,128  &13 & 13 \\\\\nBigPatent & Single-doc Summarization & Lengthy patents  & 7,718  & 12,867 & 13 & 13 \\\\\nSummScreen & Transcripts Summarization & TV series transcripts  & 10,688 & 14,544  & 13 & 13 \\\\\nOpenreview$^\\dag$ & Paper writing \\& reviewing & Papers from Openreview  & 11,170 & 33,303 & 60 & 20 \\\\\nQMSum &  Query-based summarization & Meeting transcripts  & 16,692 & 33,310 & 156 & 20 \\\\\nSPACE$^\\dag$ & Aspect-based summarization & Reviews on Hotels  &  19,978 & 22,158 & 120 & 20 \\\\\n\\bottomrule\n\\end{tabular}\n}\n\n\\label{tab:datasets}\n\\end{table}\n\n\\subsection{Statistics}\\label{sec:stat}\nThe statistics of L-Eval are shown in Table~\\ref{tab:datasets}.\nThe L-Eval contains various question styles such as multiple choice questions (TOFEL~\\citep{tseng2016towards}, QuALITY, Coursera), true or false questions (SFiction), math problems (GSM), code understanding (CodeU), goal-oriented dialogues (MultiDoc2Dial~\\citep{Feng_2021}), extractive QA (CUAD~\\citep{hendrycks2021cuad}, NQ~\\citep{kwiatkowski-etal-2019-natural}), abstractive QA (LongFQA, NarrativeQA~\\citep{kočiský2017narrativeqa}, Qasper~\\citep{dasigi2021dataset}), single document summarization (GovReport~\\citep{huang-etal-2021-efficient}, BigPatent~\\citep{sharma-etal-2019-bigpatent}, SummScreen~\\citep{chen2022summscreen}, QMSum~\\citep{zhong2021qmsum}), multi-document summarization (Multi-News~\\citep{fabbri2019multinews}, SPACE~\\citep{angelidis-etal-2021-extractive}), research writing (Openreview) and so on. The long documents in L-Eval across many domains such as law, finance, academic papers, lectures, lengthy conversations, news, famous Python codebase, long-form novels, and meetings. The average input length in L-Eval ranges from 4k to 60k. The maximum sample in L-Eval contains nearly 200k tokens. This diversity represents real-world scenarios where different tasks may require different lengths of context and instructions. The length of reference in L-Eval also varies significantly across tasks.\n\n\\section{Towards Standardized Long Context Evaluation Metrics}\n\\label{sec:metric}\nIn this section, we present various evaluation metrics for text generation, including exam evaluation for close-ended tasks and different levels of open-ended evaluation, most of which are reference-based metrics.\nWe also conduct experiments to study the correlation between automated metrics and human scoring. \n\n\\vspace{-0.7em}\n\\paragraph{Exam evaluation} This is designed for closed-ended tasks, i.e., multiple-choice questions. The evaluation metric used for these tasks follows the exact match format (accuracy \\%), similar to grading exam papers. Each question's score is calculated as 100 divided by the number of questions. \n\n\\vspace{-0.7em}\n\\paragraph{Human evaluation} This is the most accurate evaluation for open-ended tasks. Despite that some works show GPT-4 can be coherent with human judgment, LLMs cannot replace human evaluation. We engage human evaluators to score the outputs on a scale of 1 to 5, which signifies from poor output to excellent output.\nTo save human laboratories, we propose a subset used for the human evaluation which has 12 long documents with 85 open-ended questions (\\textbf{85-question subset}). \n\n\\vspace{-0.7em}\n\\paragraph{Large language model judges for evaluating LCLMs}\nIn short context settings,  evaluation using LLMs is the most accurate metric for automatically evaluating models on open-ended tasks~\\citep{zheng2023judging,alpaca_eval, dubois2023alpacafarm}. These works assume the LLM evaluator is a ``super model'', but this assumption does not hold in long context settings because it's impossible to feed the entire lengthy inputs into LLMs like GPT-4. Unlike short context evaluation, GPT-4 is unable to infer the ground truth answer itself. Consequently, evaluation results mainly depend on the reference answer and user questions. In L-Eval, we take the pair-wise battle format and we select Turbo-16k-0613 as the base model and report the \\textit{win-rate vs. Turbo-16k-0613 \\%} which means how many samples can beat Turbo-16k. We study two LLM judges: GPT-4 and GPT-3.5 in the experiment section.\nLLM evaluators have been reported to favor more detailed and lengthy answers~\\citep{zheng2023judging}. This bias becomes more pronounced in long context settings as the invisible input makes it difficult for the judge to accurately determine the correctness of specific details and information. Therefore, the judgment model must bear in mind that details not corroborated by the reference answers should not be considered beneficial. We enhance the judgment prompt with: \\textit{Additional details or information that are not mentioned in the reference answer cannot be considered as advantages and do not let them sway your judgment.} If you only want to evaluate a portion of the tasks in L-Eval, we recommend using LLM judges.\nVerifying the 1000+ open-ended questions via GPT-4 is unaffordable.\\footnote{Testing the 4 datasets in Table~\\ref{tab:length_bias} needs about \\$100!} Thus we manually split a subset for GPT-4 evaluation consisting of 17 diverse long documents with 96 open-ended questions (\\textbf{96-question subset}).\\footnote{Evaluating outputs from the 96-question subset with GPT-4 only needs about \\$5.}\n\n\\vspace{-0.7em}\n\\paragraph{N-gram matching evaluation}\nConsidering that assessing all tasks is still expensive for human/LLM evaluators, L-Eval also takes into account n-gram metrics. \nN-gram metrics like ROUGE-L (R-L) and F-1 score are widely used in traditional datasets and they are also widely adopted in the text generation benchmarks via performing lexical matching. It is worth noting that n-gram matching metrics are very sensitive to the length of the ground truth, exhibiting a length bias. The related analysis is in the following \\S\\ref{sec:length-instruction}.\n\n\\subsection{Length Instruction Enhanced Long Context Evaluation}\n\\label{sec:length-instruction}\n\n\\begin{wrapfigure}{r}{0.4\\textwidth}\n  \\centering\n  \\vspace{-6mm}\n  \\includegraphics[width=0.4\\textwidth]{fig/kt_cor.png}\n  \\caption{Kendall-Tau correlation coefficient of different automatic metrics with the average human score.}\n  \\label{fig:cor}\n  \\vspace{-5mm}\n\\end{wrapfigure}\nIn preliminary experiments, we find that LLMs tend to generate very long responses bringing obstacles for the reference-based evaluation (see $\\Delta$\\textbf{L} Table~\\ref{tab:length_bias}). This length bias results in a significant influence on the n-gram metrics. For instance, Claude-100k only achieves a 9.84 F-1 score due to undesired output length.\n\nIn L-Eval, we argue that long context language models should further focus on more accurate content rather than accurate length. Practically, issues about undesired generation length can be easily solved by prompting the model.  We first adopt \\textbf{Length-Instruction-Enhanced} (LIE) evaluation in LLMs evaluation benchmarks which is simple but effective in overcoming the length bias, i.e., the number of words of ground truth is directly exposure to LCLMs. \nLIE evaluation in this work is implemented by injecting the model with the desired length into the original instruction (e.g., [Origin Instruction]: \\textit{Please summarize the opinions of the professor}. [Length Instruction]: \\textit{We need a 50-word summary}, where 50 is the number of words in the reference answer). The results of Claude-100k in Table~\\ref{tab:length_bias} demonstrate a substantial improvement in terms of the F-1 score: there is a near \\textbf{50}-point gap depending on whether or not the model generates with the expected length.\n\n\\begin{figure}[t]\n\\vspace{-1em}\n    \\centering\n    \\includegraphics[width=0.85\\textwidth]{fig/metrics_cmp_v2.png}\n    \\vspace{-0.5em}\n    \\caption{The ranking of six models under various evaluation metrics (Human-avg, Human-1, GPT-4, GPT-3.5, R-L, and F-1) with or without length instruction. {Human-avg} represents the average score from human evaluation, and {Human-1} signifies the score given by the first human annotator.}\n    \\label{fig:radar}\n\\end{figure}\n\n\\vspace{-0.7em}\n\\paragraph{Experimental validation} To validate the LIE evaluation, we then conduct a human evaluation on the 85-questions subset. We have 3 annotators to verify 7 models and calculate the Kendall-Tau correlation coefficient ($\\tau$) between these metrics and the average human score. The main results are shown in Figure~\\ref{fig:cor} (Blue bar) and experimental settings are in \\S\\ref{sec:appendix:human-eval}. Results indicate that all these automatic metrics (except GPT-4) \\textbf{fail to correlate} to human judgment. Compared with N-gram metrics, LLM judges are more accurate and robust to output length.\nAs we can see from Figure~\\ref{fig:cor}, the improvements brought by length instruction are marked with yellow, and after adding the length instructions, $\\tau$ has been improved from 0.5 to 0.8 for ROUGE-L and $\\tau$ of GPT-4 evaluator has even reached to 1. In Figure~\\ref{fig:radar}, we convert the score to rankings (the best one is 5 and the worst is 1) and show the score of 6 models evaluated with 6 different evaluation systems. Figure~\\ref{fig:radar} (a) shows the results given by metrics without length instruction. These hexagons are often distorted because these metrics usually cannot achieve good correlation. When comparing the models enhanced with length instruction in (b), it is observed that the hexagons become more regular.\n\n\\begin{table}[t]\n\\vspace{-1em}\n\\centering\n\\caption{Results on 2 open-ended summarization and 2 abstractive QA tasks.  \\textbf{GPT-4} means the win-rate with Turbo-16k using GPT-4 as the judge.  \\textbf{$\\Delta$L} means the difference of generated answer length with ground truth length. The best results are underlined. Results in red mean decoding in a desired length makes a big difference in performance.}\n\\vspace{-0.5em}\n\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{l|ccc|ccc|ccc|ccc}\n\\toprule\n\\multirow{2}{*}{\\textbf{Model}} & \\multicolumn{3}{c|}{\\textbf{SPACE}} & \\multicolumn{3}{c|}{\\textbf{QMSum}} & \\multicolumn{3}{c|}{\\textbf{NQ}} & \\multicolumn{3}{c}{\\textbf{NrtvQA}} \\\\\n\\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13}\n&\\textbf{R-L} &  \\textbf{GPT-4} &  \\textbf{$\\Delta$L} & \\textbf{R-L} & \\textbf{GPT-4} & \\textbf{$\\Delta$L}  & \\textbf{F-1}  & \\textbf{GPT-4} & \\textbf{$\\Delta$L}  & \\textbf{F-1} & \\textbf{GPT-4}  & \\textbf{$\\Delta$L}  \\\\\n\\midrule\n\n\\rowcolor{mypink!60}\nClaude-100k & 15.43 & \\color{red}{45.65} & \\bf165 & 14.04 & 58.77 & \\bf183 & \\color{red}{9.84} & \\underline{56.19} & \\bf135 & \\color{red}{10.39} & \\underline{68.96} & \\bf127 \\\\\n \\quad + Length Instruction &18.61 & \\color{red}{\\underline{61.40}} & 27 & 18.13 & \\underline{58.89} & 22 & \\color{red}{\\underline{57.76}} & {51.00} & 1 & \\color{red}{\\underline{19.09}} & 57.77 & 0 \\\\\n\n\\rowcolor{mypink!60}\nChatglm2-32k & 17.56 & 24.13 & -23 & 20.06 & 38.84 & \\bf287 & 31.45 & 33.71 & 3 & 12.24 & 34.67 & 74 \\\\\n \\quad + Length Instruction & 16.61 & 17.11 & 11 & \\underline{20.83} & 33.75 & 9 & 37.94 & 33.71 & -1 & 14.00 & 34.52 & -2 \\\\\n\n\\rowcolor{mypink!60}\nLongchat-7b-16k  & 15.10 & \\color{red}{15.61} & \\bf120 & 9.31 & 25.56 & 40 & {8.83} & 32.33 & \\bf105 & 8.36 & 31.80 & 83 \\\\\n \\quad + Length Instruction& 17.06 & \\color{red}{36.23} & -3 & 13.21 & 30.20 & 70 & {20.21} & 35.00 & 37 &15.17&43.38&40\\\\\n\n\\rowcolor{mypink!60}\nLlama2-13b-chat & 16.83&32.46&\\bf102&14.72&30.79& \\bf116 &\\color{red}{8.29} &38.99&90&7.20 & \\color{red}{30.69} & \\bf130 \\\\\n \\quad + Length Instruction & \\underline{19.23} & 43.15 &-7 &19.65 &34.82 &-1&\\color{red}{35.43} &41.07 & 6 & 13.48 & \\color{red}{45.07} & 14 \\\\\n\n\\bottomrule\n\\end{tabular}\n}\n\\label{tab:length_bias}\n\\end{table}\n\n\\section{Benchmarking LLMs with L-Eval}\nIn this section, we list our 16 baseline models and the results on both open-ended and closed-ended tasks. Generally, there are considerable gaps between open-source models and commercial models. A detailed description of baseline models can be found in \\S\\ref{sec:baselines_appendix}.  The prompt templates for each task are available in \\S\\ref{sec:data-appendix}. We run all the experiments using FlashAttention~\\citep{NEURIPS2022_67d57c32} on a single NVIDIA A800 GPU. The document input is truncated from the right.\n\n\\vspace{-0.5em}\n\\subsection{Baselines}\\label{sec:baselines}\n\\vspace{-0.3em}\n\\paragraph{Commercial Models}\n(1) {Claude-100k} developed by Anthropic, (2) GPT-4-32k, OpenAI's most powerful long context model, (3)\n{Turbo-4k-0613} and (4) Turbo-16k-0613 is the snapshot of {GPT-3.5} from June 13th 2023 which can handle up to 4k/16k input tokens.\n\\vspace{-0.7em}\n\\paragraph{Open-source Models}\n(5) {Llama1}~\\citep{touvron2023llama}, a widely used open-source model developed by Meta AI with a 2k pre-training length, (6) {Vicuna1.3}~\\citep{vicuna2023}, tuned on shareGPT based on Llama1, (7) Longchat-16k, the long context version of Vicuna1.3 using PI, (8) Llama2, the next version of Llama with 4k pre-training context, (9) Llama2-chat, a finetuned version for dialogue usage, (10) Llama2-NTK, extending the context length of Llama2-chat with NTK-aware RoPE, (11) Vicuna1.5-16k~\\citep{zheng2023judging}, the long context version of Llama2 using PI \\& ShareGPT (12) Longchat1.5-32k, the 32k context version of Llama2 using PI \\& ShareGPT.\n(13) {Chatglm2-8k}, the second version of the Chatglm~\\citep{du2022glm}, (14) Chatglm2-32k, the 32k context length version,  (15)\\ {XGen-8k-inst}~\\citep{XGen}, an 8k context models developed by salesforce  (16) {MPT-7B-StoryWriter-65k}, based on MPT-7B and ALiBi with a context length of 65k tokens on a subset of Books3 dataset.\n\\vspace{-0.7em}\n\\paragraph{Retriever}  We implement the dense retriever with the OpenAI AdaEmbedding as the dense retriever and BM25 as the sparse retriever to extract 4 pieces of most related 1k-chunked documents, which are further provided as the context to answer questions.\n\n\\begin{table}[t!]\n\\vspace{-1.5em}\n\\centering\n\\setlength{\\tabcolsep}{1.0mm}\n\\caption{Exam evaluation results on  \\textbf{closed-ended tasks} for current LCLMs. \\textbf{Ret.} indicates whether we use retrieve-based algorithms for the base model. \\textbf{Tokens} denotes the maximum number of input tokens we feed into the model. {\\color{red}{\\tiny{$\\downarrow$ / $\\uparrow$}}} indicates a remarkable decrease/increase in performance, compared to using the original short context counterpart. {\\color{blue} *} indicates the model is not further trained.}\n\\vspace{-0.5em}\n\\renewcommand\\arraystretch{1.05}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{@{}lcccccccccc@{}}\n\\toprule\n\\textbf{Model} & \\textbf{Ret.} & \\textbf{Tokens} & \\textbf{Coursera} & \\textbf{GSM} & \\textbf{QuALITY} & \\textbf{TOEFL}  & \\textbf{CodeU} & \\textbf{SFiction} &  \\textbf{Avg.} \\\\\n\\midrule\nClaude1.3-100k & \\xmark & \\cellcolor{gray!70}100k & 60.03 & {88.00} & {73.76} & {83.64} & 17.77 & 72.65 & 65.97  \\\\\nGPT-4-32k & \\xmark & \\cellcolor{gray!60}32k & \\textbf{75.58} & \\textbf{96.00} & \\textbf{82.17} & \\textbf{84.38} & \\textbf{25.55} & \\bf74.99 & \\bf73.11 \\\\\nTurbo-16k-0613 & \\xmark & \\cellcolor{gray!40}16k & 63.51 & {84.00} & 61.38 & 78.43 & 12.22 & 64.84 & 60.73 \\\\\nAdaEmb-Turbo-4k-0613 & \\cmark & \\cellcolor{gray!15}4k & 61.77 & {23.00} & 58.91 & 76.95 & {6.66} &71.09 & 49.73 \\\\\nBM25-Turbo-4k-0613 & \\cmark & \\cellcolor{gray!15}4k & 63.80 & {23.00} & 59.40 & 75.09 & {5.55} & 71.09  & 49.65 \\\\\n\n\\midrule\n\\rowcolor{mypink!50}\n\\multicolumn{10}{c}{\\textit{Truncating input tokens to the pretraining context length}} \\\\\n\\midrule\n\nLlama1-7b-2k (w/o SFT) & \\xmark & \\cellcolor{gray!5}2k & 13.37 & 7.00 & 21.78 & 30.85 & 1.11 & 35.15  & 19.22 \\\\\nVicuna1.3-7b-2k & \\xmark & \\cellcolor{gray!5}2k & 34.73 & 19.00 & 32.67 & 43.49 & 1.11 & 60.93 & 30.01 \\\\\n\nLlama2-7b-4k (w/o SFT) & \\xmark & \\cellcolor{gray!15}4k  & 20.05 & 2.00 & 28.71 &  24.53 & 0.00 & 40.62  & 19.31 \\\\\nLlama2-7b-chat & \\xmark & \\cellcolor{gray!15}4k  & 29.21 & {19.00} & 37.62 & 51.67 & 1.11 & 60.15 & 33.12 \\\\\nLlama2-13b-chat & \\xmark & \\cellcolor{gray!15}4k  & 35.75 & \\textbf{39.00} & \\textbf{42.57} & \\textbf{60.96} & 1.11 & 54.68 & \\bf39.01 \\\\\n\nChatglm2-6b-8k & \\xmark & \\cellcolor{gray!5}2k & \\bf43.75 & {13.00} & {40.59} & {53.90} & 2.22 & 54.68 & 34.69  \\\\\nXGen-7b-8k (2k-4k-8k) & \\xmark & \\cellcolor{gray!5}2k & 26.59 & 3.00 &  35.15  & 44.23 & 1.11 & 48.43 & 26.41 \\\\\n\n\\midrule\n\\rowcolor{mypink!50}\n\\multicolumn{10}{c}{\\textit{Truncating input tokens to the further finetuning context length}} \\\\\n\\midrule\nChatglm2-6b-32k & \\xmark & \\cellcolor{gray!60}32k  & \\textbf{47.81} & 27.00\\color{red}{\\tiny{$\\uparrow$}} & 45.04 & 55.01 & 2.22 & 57.02 & 39.01\\color{red}{\\tiny{$\\uparrow$}} \\\\\nLongchat1.5-7b-32k & \\xmark & \\cellcolor{gray!60}32k  & 32.99 & 18.00 & 37.62 & 39.77 & 3.33 & 57.02 & 31.45\\\\\n\nLongchat-7b-16k & \\xmark & \\cellcolor{gray!40}16k  & 29.74 & 10.00\\color{red}{\\tiny{$\\downarrow$}}  & 33.66 & 47.95 & 3.33 & \\bf64.84 & 31.58\\color{red}\\\\ \nVicuna1.5-7b-16k & \\xmark & \\cellcolor{gray!40}16k  & 38.66 & 19.00 & 39.60 & 55.39 & \\bf5.55 & 60.15 & 36.39\\color{red}{\\tiny{$\\uparrow$}} \\\\\nLlama2-7b-NTK\\color{blue}{*} & \\xmark & \\cellcolor{gray!40}16k  & 32.71 & 19.00 & 33.16 & 52.78 & 0.00 & \\bf64.84 & 33.74 \\\\\n\nLongchat-13b-16k & \\xmark & \\cellcolor{gray!40}16k & 31.39 & 15.00 & 40.59 & 55.39 &  2.22 & \\bf64.84  & 34.90 \\\\\nVicuna1.5-13b-16k & \\xmark & \\cellcolor{gray!40}16k & 40.69 & 36.00 & \\textbf{53.96}\\color{red}{\\tiny{$\\uparrow$}} & \\textbf{68.40}\\color{red}{\\tiny{$\\uparrow$}} & 0.00 & 61.71 & \\bf43.46\\color{red}{\\tiny{$\\uparrow$}}\\\\\nLlama2-13b-NTK\\color{blue}{*} & \\xmark & \\cellcolor{gray!40}16k  & 36.48 & 11.00\\color{red}{\\tiny{$\\downarrow$}} & 35.64  & 54.64&  1.11  & 63.28 & 33.69 \\\\\nLlama2-13b-NTK(Dyn)\\color{blue}{*} & \\xmark & \\cellcolor{gray!40}16k  & 30.08 & \\textbf{43.00} & 41.58 & 64.31 & 1.11 & 35.15 & 35.87\\\\\n\nChatglm2-6b-8k & \\xmark & \\cellcolor{gray!25}8k & 42.15 & {18.00} & {44.05} & {54.64} & 2.22 &54.68 & 35.95\\\\\nXGen-7b-8k & \\xmark & \\cellcolor{gray!25}8k & 29.06 & 16.00 &  33.66  & 42.37 & 3.33 & 41.40 & 27.63\\\\\nMPT-7b-65k & \\xmark & \\cellcolor{gray!25}8k & 25.23 & 8.00 & 25.24 & 17.84 & 0.00 & 39.06 & 19.22\\\\\n\n\\bottomrule\n\\end{tabular}\n}\n\n\\label{table:acc_exam}\n\\end{table}\n\n\\begin{table*}[t]\n\\vspace{-1.5em}\n\\caption{In comparing various models to Turbo-16k-0613 on \\textbf{open-ended tasks}. We evaluate these models on the 96-question subset using GPT-4 and two subsets (85+96 questions) using GPT-3.5. We reduce the positional biases by swapping paired predictions, so the GPT-4 evaluator is used in 96$\\times$2 evaluation rounds, while the GPT3.5 evaluator is used in 181$\\times$2 rounds}\n\\vspace{-1.0em}\n\\center\n\\footnotesize\n\\renewcommand\\arraystretch{0.97}\n\\resizebox{0.9\\textwidth}{!}{\n\\tabcolsep 0.035 in\n\\begin{tabular}{lccccccccc}\n\\toprule\n\\multicolumn{1}{c}{\\multirow{2}[1]{*}{\\textbf{Model}}} &\n\\multicolumn{1}{c}{\\multirow{2}[1]{*}{\\textbf{Ret.}}} &\n\\multicolumn{1}{c}{\\multirow{2}[1]{*}{\\textbf{Tokens}}} &\n\\multicolumn{3}{c}{\\textbf{GPT-4 }}&\n\\multicolumn{3}{c}{\\color{darkgray}{\\textbf{GPT-3.5}}} &\n\\multicolumn{1}{c}{\\multirow{2}[1]{*}{\\color{gray}{\\textbf{R-L}}}}\n\\\\\n & & & \\textbf{wins} & \\textbf{ties} & \\textbf{win-rate \\%}  &\n\\textbf{wins} & \\textbf{ties} & \\textbf{win-rate \\%}  \\\\\n\\cmidrule(lr){1-1} \\cmidrule(lr){2-2} \\cmidrule(lr){3-3} \\cmidrule(lr){4-6} \\cmidrule(lr){7-9} \\cmidrule(lr){10-10} \n\nClaude1.3-100k & \\xmark & \\cellcolor{gray!70}100k & \\textbf{96} & 42 & \\textbf{60.94}  & 189 & 34 & {\\textbf{58.68}} & 28.22 \\\\\nGPT-4-32k & \\xmark & \\cellcolor{gray!60}32k & 76 &  56 & 54.16  &  171 & 50 & {56.32} & \\underline{36.18} \\\\\nTurbo-16k-0613 & \\xmark & \\cellcolor{gray!15}4k & 0 & 192 & 50.00  & 0 & 362 & 50.00 & 28.61\\\\\n\nTurbo-4k-0613 & \\xmark & \\cellcolor{gray!15}4k & 38 & 69 & 39.83\\color{red}{\\tiny{$\\downarrow$}}  & 109 &  61 & {41.39} & 26.90\\\\\nAdaEmb-Turbo-4k-0613 & \\cmark & \\cellcolor{gray!15}4k & 61 & 56 &  46.84 & 123 & 77  & 45.36 & 26.09 \\\\\nBM25-Turbo-4k-0613 & \\cmark & \\cellcolor{gray!15}4k & 50 & 69 & 44.01 & 125 & 78 & {45.30} & 26.83\\\\\n\n\\midrule\n\\rowcolor{mypink!50}\n\\multicolumn{10}{c}{\\textit{Truncating input tokens to the pretraining context length}} \\\\\n\\midrule\n\nVicuna1.3-7b-2k  & \\xmark & \\cellcolor{gray!5}2k & 29 & 55 & 29.42 & 97 & 42 & 34.91 & 16.17 \\\\\nLongchat-7b-16k & \\xmark & \\cellcolor{gray!5}2k & 26 & 63 & 29.94  &  87 &  38 & {31.26} & 19.77\\\\\n\nLlama2-7b-chat  & \\xmark & \\cellcolor{gray!15}4k & 48 & 58 & 40.10 & 127 & 44 & {42.45}  & \\underline{24.25}\\\\\nLlama2-13b-chat & \\xmark & \\cellcolor{gray!15}4k & \\textbf{51} & 61 & \\textbf{42.44}  & \\textbf{143} & 49 & \\textbf{47.85} & 24.07  \\\\\n\n\\midrule\n\\rowcolor{mypink!50}\n\\multicolumn{10}{c}{\\textit{Truncating input tokens to the further finetuning context length}} \\\\\n\\midrule\n\nChatglm2-6b-32k & \\xmark & \\cellcolor{gray!60}32k & 28 & 60 & 30.20 &  53 & 65 & {24.63} & \\underline{22.04} \\\\\nLongchat1.5-7b-32k & \\xmark & \\cellcolor{gray!60}32k & \\bf38 & 53 & 33.59 &  136 & 37 & 44.91 & 21.21\\\\\n\nLongchat-7b-16k & \\xmark & \\cellcolor{gray!40}16k & 36 & 56 & 33.68\\color{red}{\\tiny{$\\uparrow$}} & 108 & 42 & {37.94} & 20.59\\\\\nVicuna1.5-7b-16k & \\xmark & \\cellcolor{gray!40}16k & 22 & 54 & 25.52\\color{red}{\\tiny{$\\downarrow$}} & 102 & 52 & {37.86} & 18.05 \\\\\nLlama2-7b-NTK\\color{blue}{*} & \\xmark & \\cellcolor{gray!40}16k & 18 & 49 & 22.13 & 58 & 35 & {23.59} & 11.50 \\\\\n\nLongchat-13b-16k & \\xmark & \\cellcolor{gray!40}16k & 36 & 59 & \\textbf{34.11} & \\bf128 & 24 & 40.11 & 18.98\\\\\nVicuna1.5-13b-16k & \\xmark & \\cellcolor{gray!40}16k & 36 & 59 & \\textbf{34.11}\\color{red}{\\tiny{$\\downarrow$}} &  116 &  43 & \\bf{40.92} & 19.69 \\\\\nLlama2-13b-NTK\\color{blue}{*} & \\xmark & \\cellcolor{gray!40}16k & 31 & 52 & 29.68 & 91 & 44 & 34.55 &  15.63\\\\\nLlama2-13b-NTK(Dyn)\\color{blue}{*} & \\xmark & \\cellcolor{gray!40}16k &  23 & 48 & 24.47 & 55 & 64 & 26.60 & 11.62\\\\\n\nChatglm2-6b-8k & \\xmark & \\cellcolor{gray!25}8k & 18 & 64 & 26.04  &  86 & 54 & {32.84} & 18.19  \\\\\nXGen-7b-8k & \\xmark & \\cellcolor{gray!25}8k & 24 & 62 & 28.64  & 89 & 72 & {36.02} & 20.51 \\\\\n\\bottomrule\n\\end{tabular}\n}\n\n\\label{tab:llm_eval}\n\\end{table*}\n\n\\subsection{Main Results}\\label{sec:main_results}\nThe performance of LCLMs on closed-ended tasks is shown Table~\\ref{table:acc_exam}. As for open-ended tasks, we test the 96-question subset (Table~\\ref{tab:llm_eval}) with GPT-4 evaluation. Results from n-gram metrics on all test sets and the rankings of LLMs can be found in \\S\\ref{sec:analysis_app}. From the main results, we have the following observations. \nGPT-4-32k clearly outperforms all other models by a very significant margin, establishing SOTA in L-Eval closed-ended tasks. \nThere is still a near \\textbf{20}-points gap between the best open-source 16k models and Turbo-16k.  As for open-ended tasks, since the input texts are generally longer and a global understanding of the context is required, Claude-100k, with the longest context length, surpasses all baseline models including GPT-4-32k. Although results of n-gram metrics indicate that open-source LCLMs have achieved performance close to GPT-Turbo on open-ended tasks, the evaluation outcomes from both LLM (Table~\\ref{tab:llm_eval}) and human judges (Table~\\ref{tab:human_eval}) reveal that there is still a significant gap between them. Moreover, retrieval-based methods based on Turbo-4k fall short in comparison to encoding the entire context (Turbo-16k), as certain tasks are difficult to address through simple retrieval.\n\n\\vspace{-0.7em}\n\\paragraph{Fine-tuning longer offers benefits for closed-ended tasks but falls short in open-ended tasks} \n\\begin{wrapfigure}{r}{0.4\\textwidth}\n  \\centering\n  \\vspace{-5mm}\n  \\includegraphics[width=0.4\\textwidth]{fig/split_data.png}\n  \\caption{Number of invalid outputs from Llama2 and Turbo.}\n  \\label{fig:percent}\n  \\vspace{-3mm}\n\\end{wrapfigure}\nIn Table~\\ref{table:acc_exam}, for open-source models using scaled positional embedding, Longchat and Vicuan1.5-16k obviously outperform their original version Vicuna-2k and Llama2-chat. The results suggest that further tuning on longer input from a model with short pretraining context length does benefit long context modeling. However, according to Table~\\ref{tab:llm_eval}, unlike results on closed-ended tasks, the best model Vicuna1.5-13b-16k only wins Turbo-16k by 34\\%, \\textbf{8} points lower than its short version Llama2-13b. Llama2-13b-chat~\\citep{touvron2023llama} is still the strongest open-source baseline, indicating that current LCLMs simply based on scaled position embedding may not be enough for these challenging open generation tasks. Based on our human evaluation, we find that although scaled position embedding techniques such as NTK~\\citep{fixedNTK} or PI~\\citep{sun2022lengthextrapolatable} effectively extend models' context length, the models tend to get lost when facing lengthy input tokens and are unable to follow the instruction. We classify these outputs as ``invalid outputs''. \nTo investigate model performance on different context lengths, we split the 85-questions subset into 2 parts: PART-A contains samples with less than 4k tokens, and PART-B more than 4k tokens. We compare the number of invalid outputs from Llama2/Vicuna1.5-16k and Turbo/Turbo-16k in Figure~\\ref{fig:percent}. Results show that the number of invalid outputs from Turbo-16k remains a very small amount on both PART-A and B while the invalid outputs from Llama2-16k dramatically increase on samples with longer input. Thus, LCLMs are less capable of following instructions on open-ended tasks for long contexts, compared with closed-ended tasks, such as multiple choice. A possible reason is that the pertaining or SFT corpus is highly likely to contain many training samples with similar question styles. This strongly enhances their instruction-following ability on closed-ended tasks. \n\n\\vspace{-1em}\n\\paragraph{Performance on retrieval tasks contradicts reasoning tasks}\n\\begin{wrapfigure}{r}{0.35\\textwidth}\n  \\centering\n  \\vspace{-1mm}\n  \\includegraphics[width=0.35\\textwidth]{fig/ntk.pdf}\n  \\caption{Test retrieval ability and reasoning ability with NTK base.}\n  \\label{fig:ret_reasoning}\n  \\vspace{-2mm}\n\\end{wrapfigure}\nThe most popular NTK-ware positional embedding methods increase the base 10,000 \nin the vanilla RoPE to implement extrapolation without further fine-tuning. However, we find that the performance on topic retrieval tasks does not match the reasoning capability over lengthy context. As can be seen from Figure~\\ref{fig:ret_reasoning}, when we increase the base from 20,000 to 160,000, there is a continuous improvement on topic retrieval. However, performance on math reasoning tasks with lengthy examples exhibits a completely opposite trend, indicating that it is challenging for the model to maintain its reasoning abilities when increasing the base. In contrast, the performance on retrieval tasks seems to remain unaffected after the base reaches 60,000.\n\nWe have further analysis in \\S\\ref{sec:analysis_app}, including full results of n-grams metrics on open-ended tasks, the rankings of current LLMs, NTK-aware positional embedding and retrieval-based systems.\n\n\\section{Conclusion}\nIn conclusion, the much-needed rigorous benchmark L-Eval introduced in this work provides a comprehensive suite of tasks and evaluation metrics to assess the capabilities of long context language models.\nWe tested most of open-source LCLMs and experiments demonstrate promising gains from extending context length and gaps compared to commercial models. Our analysis using L-Eval offers valuable insights into the current state and limitations of LCLMs. We believe that with its focus on practical, long-form documents across domains, L-Eval can serve as a challenging testbed to drive advances in modeling longer contexts.\n\n\\newpage\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{E-Sparse: Boosting the Large Language Model Inference through Entropy-based N:M Sparsity}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nTraditional pruning methods are known to be challenging to work in Large Language Models for Generative AI because of their unaffordable training process and large computational demands.\nFor the first time, we introduce the information entropy of hidden state features into a pruning metric design, namely \\name, to improve the accuracy of N:M sparsity on LLMs.\n\\name employs the information richness to leverage the channel importance, and further incorporates several novel techniques to put it into effect:\n(1) it introduces information entropy to enhance the significance of parameter weights and input feature norms as a novel pruning metric, and performs N:M sparsity without modifying the remaining weights.\n(2) it designs global naive shuffle and local block shuffle to quickly optimize the information distribution and adequately cope with the impact of N:M sparsity on LLMs' accuracy.\n\\name is implemented as a Sparse-GEMM on FasterTransformer and runs on NVIDIA Ampere GPUs.\nExtensive experiments on the LLaMA family and OPT models show that \\name can significantly speed up the model inference over the dense model (up to 1.53$\\times$) and obtain significant memory saving (up to 43.52\\%), with acceptable accuracy loss.\n\\end{abstract}\n\n\\section{Introduction}\n\n\\begin{figure}[t]\n\\centering\n\\subfloat[Evaluate the input features from both cross-channel and intra-channel dimensions.]{\\includegraphics[width=1.0\\linewidth]{images/metric_1_1.pdf} \\label{fig:metric_1}} \\\\\n\\vspace{0.5cm}\n\\subfloat[The entropy-based sparsity metric of \\name.]{\\includegraphics[width=1.0\\linewidth]{images/metric_2_1.pdf} \\label{fig:metric_2}}\n\\caption{Overview of the proposed \\name. It first introduces entropy to quantify the information richness within each channel ( intra-channel ) of the input features, and adopts it to enhance the feature norms ( cross-channel ) as a metric to evaluate parameter importance. Furthermore, it proposes Channel Shuffle to reorder the information distribution in LLMs to obtain N:M Sparsity with less information loss.}\n\\label{fig:overview}\n\\end{figure}\n\nLarge language models (LLMs), such as GPT-3\\cite{brown2020language}, LLaMA\\cite{touvron2023llama}, Bloom\\cite{scao2022bloom}, and others, have recently exhibited outstanding performance across a wide range of tasks, including but not limited to social systems, intelligent conversation, content generation, code creation, etc. However, deploying LLMs poses significant challenges due to their substantial computational demands and high memory requirements. For instance, the most powerful variant, the Bloom model with 176 billion parameters, necessitates a minimum of 350 GB of storage in half-precision (FP16) format. When configured with a batch size of 1 and a sequence length of 128, Bloom-176B inference demands a formidable ensemble of 16 NVIDIA A10 GPUs, each equipped with 24GB memory. Consequently, optimizing these models through compression and pruning has emerged as a critical strategy to reduce parameter counts, thereby decreasing computational overhead and conserving memory resources. %The large overhead has led to a lot of work being devoted to the compression and acceleration of LLMs. %So far, early model compression work for large models has mainly focused on model quantification \\cite{xiao2023smoothquant, dettmers2022llm, frantar2022gptq}.\n\nIn order to harness the acceleration and memory reduction potential offered by sparse neural networks, GPU manufacturers have introduced architectural enhancements. Specifically, the invention of Sparse Tensor Core ~\\cite{a100, h100, yao2019balanced, cao2019efficient} technology has been pivotal in capitalizing on weight sparsity within Deep Neural Network (DNN) models. This innovation employs a fine-grained structural pruning technique, involving a 2-out-of-4 pruning approach within each partitioned sub-vector. This method effectively balances the computational workload while maximizing parallelism within the dot-product unit.\n\nWhile there has been substantial research on compressing LLMs using low-precision quantization\\cite{xiao2023smoothquant, dettmers2022llm, frantar2022gptq}, relatively little effort has been dedicated to fully exploiting Sparse Tensor Core technology for accelerating LLMs. Some prior work, exemplified by Wanda\\cite{sun2023simple}, has proposed the application of a 2-out-of-4 pruning pattern for LLMs. This approach determines channel importance by evaluating input feature norms and weights them against standard parameter magnitudes as pruning metrics. In this studyhe, we introduce an Entropy-based pruning algorithm that builds upon these principles. Our research showcases a remarkable \\textbf{1.32} LLaMA perplexity improvement over state-of-the-art techniques and delivers a \\textbf{19.6\\%-34.8\\%} speedup on an A100 GPU, demonstrating the effective and efficient utilization of Sparse Tensor Core hardware.\n\nOur work is grounded in two crucial observations. Firstly, we note that \\textbf{the richness of information among channels exhibits significant variation}. Even within the same batch of tokens, the entropy of elements within each channel differs considerably, despite some sharing the same input feature norm. Secondly, we observe that \\textbf{channels with close entropy values tend to exhibit relatively concentrated distributions}. These observations naturally inspire us to leverage channel-specific information in order to enhance LLMs inference using $N \\colon M$ sparsity.\n\n\\textbf{Our proposal} \nWe propose entropy-based sparsity (\\name ), a novel method to prune LLMs without modifying the remaining weights. Figure \\ref{fig:overview} shows the key idea of one-shot \\name. \n\nFirstly, inspired by Observation 1, we introduce a novel metric to assess the importance of weights. This metric employs information entropy to quantify the amount of information within each channel of the hidden state features in LLMs. We enhance the significance of parameter weights and input feature norms by incorporating information entropy as a metric for evaluating parameter importance.\n\nSecondly, we implement a channel shuffling mechanism to ensure a more equitable distribution of information among the channels in the hidden features ( Figure~\\ref{fig:shuffle} ). As Observation 2 reveals, the information distribution across channels tends to be highly concentrated, which can impede the accuracy of $N \\colon M$ sparsity due to the need to remove N elements from adjacent M elements. Channel shuffling is instrumental in preserving a greater number of elements within information-rich channels, thereby mitigating the impact of parameter pruning on LLMs accuracy.\n\nLastly, with the robust support of NVIDIA's cuSPARSE\\cite{cuSPARSE} and cuSPARSELt\\cite{cuSPARSELt} libraries, we have crafted an efficient \\name GEMM designed explicitly for LLMs inference and integrated it into FasterTransformer.\n\n\\name enables the N:M sparsity of weights for all the matrix multiplications in LLMs, including the LLaMA family, and OPT. The results show that \\name outperforms the performance of the state-of-the-art training-free sparsity methods \\cite{frantar2023massive,sun2023simple} for LLMs. \nIt has also been demonstrated that \\name can achieve a 1.24--1.53$\\times$ speedup and a 42.64\\%--43.52\\% memory saving for LLMs with negligible loss in accuracy. \n\n\\section{Inspiration from Observations}\n\\label{sec:IC}\n\n\\begin{figure*}[t]\n\t\\begin{center}\n\t\t\\includegraphics[width=1.0\\linewidth]{images/observation4.pdf}\n\t\\end{center}\n\t\\caption{The visualization of the hidden activations in LLMs. The data for each subfigure comes from the activation of the corresponding layer of LLaMA-13B. For clarity, we only capture the norm and entropy values for the 100 channels after norm sorting in (a) and (b). We show the entropy values of all channels in (c) and (d).}\n\t\\label{fig:observation }\n\\end{figure*}\n\nIt has been found that a small subset of hidden state features (named ``outlier\") in LLMs are exceptionally large in magnitude \\cite{dettmers2022llm, xiao2023smoothquant}, and these features are important for LLMs compression~\\cite{sun2023simple}. Then, we visualize the input activations of linear layers in LLMs and find several key observations about these activations that motivate our method:\n\n\\noindent $\\bullet$ \\textbf{The information richness between channels varies greatly.} A recent work ~\\cite{sun2023simple} found that the norm of activation in LLMs can be used to measure channel importance. In addition to the same finding, we also observed that the information entropy between channels also varies greatly. \nTo facilitate observation, we first sort the channels according to the norm value and then compare the entropy of each channel feature according to the same index sorted by the norm in Figure~\\ref{fig:observation }a and Figure~\\ref{fig:observation }b. We find that the entropy of different channels differ considerably, despite some sharing the same input feature norm. The observation above motivates us to enhance evaluation metrics through information richness.\n\n\\noindent $\\bullet$ \\textbf{The entropy values of adjacent channels are relatively close.} As shown in Figure~\\ref{fig:observation }c and Figure~\\ref{fig:observation }d, channels\nwith close entropy tend to exhibit relatively concentrated distributions. \nHowever, N:M sparsity forces the model to prune N values out of M consecutive values in the channel dimension, which makes us inevitably need to prune in M consecutive informative channels and damage the accuracy of LLMs. \nThis observation straightforwardly motivates us to shuffle the channels to preserve a greater number of elements within information-rich channels, thereby mitigating the impact of N:M sparsity on accuracy.\n\n\\section{Method}\n\n\\subsection{Method Overview}\n\n\\name proposes a new entropy-based metric to evaluate the parameter importance in LLMs, and introduces channel shuffling to minimize the information loss brought by N:M sparsity. The key advantages of \\name include: \n1) Sparse the LLMs without modifying the remaining weights. In contrast to channel-by-channel parameter sparse and update \\cite{frantar2023massive}, \\name augments the parameter weights with the information richness and the amplitude of the feature as an evaluation metric, and then adopts it to sparse the weights of a layer at once.\n2) More fine-grained importance evaluation of hidden state channels. Apart from the global information (channel amplitude), \\name introduces entropy to measure the local information of channels (information richness), thereby comprehensively measuring the importance of channels.\n3) More flexible sparse mode. Traditional N:M sparsity forces pruning of N out of M consecutive values, \\name introduces channel shuffle mechanism, which is more adaptable to the feature information distribution of LLMs and reduces accuracy loss.\n\n\\subsection{Information Richness - Entropy}\n\nThe observation in Section~\\ref{sec:IC} motivates us to enhance the evaluation metrics of LLMs pruning through information richness. Entropy~\\cite{shannon1948mathematical} is a key indicator in the field of information theory to measure the amount of information and uncertainty. \nThe larger the entropy, the higher the information richness.\nTherefore, we introduce entropy to evaluate the channel information of activation for augmenting the standard weight magnitude and channel norm as a novel pruning metric.\n\nLet $X\\in \\mathbb{R}^{o\\times C}$ denote the hidden feature of a fully connected layer in LLMs, where $C$ is the number of channels, and $o$ is the dimension of each channel. To compute the entropy, we first divide it into $K$ different bins and then calculate the probability of an element in the channel falling into each bin. Then, the information richness (entropy) of channel $c$ can be formulated as:\n\n\\begin{equation}\n \\mathcal{IR}_{c}=-\\sum_{k=1}^{K}p_{k}^{c} log\\left ( p_{k}^{c}\\right ) \n\\end{equation}\nin which, $p_{k}^{c}$ is the probability of bin $k$ in channel $c$, and ${IR}_{c}\\in [0,+\\infty )$. \nWe set $K$ to 100 empirically, which can achieve good results. \nInformation entropy can be used as a good fine-grained metric to evaluate information richness. \nThe larger ${IR}_{c}$ value means higher information richness. \n\nNext, regarding coarse-grained evaluation, we follow \\cite{sun2023simple} and adopt the input feature norm to measure the amplitude:\n\n\\begin{equation}\n \\mathcal{AM}_{c}=\\left\\| X_{c}\\right\\|_{2}\n\\end{equation}\nwhere $\\left\\| X_{c}\\right\\|_{2}$ represents the $L^{2}$ norm of the channel $ X_{c}$.\n\nFinally, to comprehensively evaluate the importance of channels and obtain more reasonable weight evaluation metric, we integrated the fine-grained indicator and the coarse-grained indicator above to get the following evaluation metric for pruning redundant weights in LLMs:\n\n\\begin{equation}\n \\mathcal{\\xi} _{cj}=\\left| w_{cj}\\right|\\cdot \\left ( \\mathcal{IR}_{c}+\\alpha\\cdot \\mathcal{AM}_{c} \\right )\n\\label{metric}\n\\end{equation}\nin which, $w_{cj}$ is the $j$-th element in channel $c$ of the fully connected layer in LLMs, and $\\xi _{cj}$ is the final important score of $w_{cj}$ in the sparsity metric.  The larger $\\xi _{cj}$ value means higher importance of the element in this layer.\n\n\\subsection{Information Reorder - Channel Shuffle}\n\\label{sec:CS}\n\n\\begin{figure}[t]\n\\centering\n\\subfloat[Global Naive Shuffle. ]{\\includegraphics[width=1.0\\linewidth]{images/naive_shuffle_1.pdf} \\label{fig:naive}} \\\\\n\\vspace{2mm}\n\\subfloat[Local Block Shuffle.]{\\includegraphics[width=1.0\\linewidth]{images/block_shuffle_1.pdf} \\label{fig:block}}\n\\caption{Channel Shuffle of \\name. Take 2:4 sparsity as an example. \\name first sorts the channels \\textbf{globally} according to the channel mean of the sparsity metric, and then divides the channels with close mean into different groups, which is \\textbf{coarse-grained but faster}.  Then, \\name splits the channel into multiple blocks and performs channel shuffle within the blocks, which is slightly slower than the global shuffling but more accurate.}\n\\label{fig:shuffle}\n\\end{figure}\n\nInspired by the observation in Section \\ref{sec:IC}, \\name implements a channel shuffling mechanism to ensure a more equitable distribution of information among the channels in the hidden features.\nBy reordering the channel index of the hidden state feature and the layer parameter, \\name aims to make the channels with higher information richness distributed more evenly, thus minimizing the information loss caused by N:M sparsity. \n\nFirst, the N:M sparsity can be formulated as a constrained optimization problem:\n\n\\begin{equation}\n\\mathcal{O}=\\mathop{\\min}_{\\theta} \\frac{1}{2}\\left\\| Y - W_{N:M}^{\\theta }\\cdot X \\right\\|_{F}^{2}\n\\label{o1}\n\\end{equation}\nin which, $X$ and $Y$ are the input and original output of a fully connected layer, respectively. $\\theta$ is the index order of channels, and $W_{N:M}^{\\theta }$ is the weight after performing N:M sparsity on W under the current index order. We are committed to finding an optimal channel order $\\theta$, which can minimize the output loss caused by M:N sparsity. However, directly optimizing the above problems in LLMs will bring a large computational overhead. Considering that the importance metric in \\eqref{metric} contains the information from both weights and activation, we simplify the above problem to minimizing the sparse loss of $\\xi_{cj}$:\n\n\\begin{equation}\n\\mathcal{\\acute{O}}=\\mathop{\\max}_{\\theta}\\sum_{c=1}^{C}(\\xi_{cj})_{N:M}^{\\theta }\n\\label{o2}\n\\end{equation}\nin which, $(\\xi_{cj})_{N:M}^{\\theta }$ is the evaluation metric after N:M sparsity under the channel permutation $\\theta$. Compared to \\eqref{o1}, there is no need to repeatedly perform matrix multiplication to calculate the feature map $Y$ and the sparse feature map. \n\nAlthough the optimization problem above has been greatly simplified, performing channel shuffle in LLMs is non-trivial. The large channel size of LLMs results in a big search space, which in turn brings huge computational and time overhead. For a fully connected layer with $C$ channels, there are $C!$ different orderings of channels. For instance, a layer with 1024 channels has a channel ordering of $10^{2640}$. \nIn LLMs, the maximum number of channels can reach more than 10,000, which brings huge resistance to obtaining the optimal permutation.\n\nTo deal with the issue above, \\name introduced the channel shuffle, which consists of two steps: \\textit{global naive shuffle} and \\textit{local block shuffle}. %\\textcolor{red}{change after}\n\n\\textbf{Global Naive Shuffle.} To reduce the complexity of channel shuffle in LLMs as much as possible, \\name first performs a fast global channel shuffle. For the sparsity metric $\\xi \\in \\mathbb{R}^{o\\times C}$, the mean value of each channel is calculated, and based on which the channels are shuffled in descending order. As shown in Figure~\\ref{fig:naive}, according to the sparsity pattern ($M$), \\name shuffles the channels with close means into different sparse groups. Global naive shuffle can achieve fast coarse-grained information reordering.\n\n\\textbf{Local Block Shuffle.} \nTo further minimize the information loss caused by N:M sparsity, \\name introduces local block shuffle.\nFirst, \\name divided the $\\xi$ after global naive shuffle into $n$ blocks, and each block contains $m$ channels ($C=m\\cdot n$), as shown in Figure~\\ref{fig:block}. We use $m = 256$ unless otherwise specified, thus the channel search space is reduced from \\textbf{$C!$} to \\textbf{$n\\cdot256!$}, making the number of unique permutations can be completed in an acceptable amount of time.  \nThen, \\name performs channel shuffling in each small block by adapting the classic greedy search algorithm~\\cite{ji2018tetris, pool2021channel}. \n\nCombining global naive shuffle and local block shuffle, \\name can realize a fast optimization for information distribution and well cope with the challenge of large channel dimensions in LLMs.\n\nTo deploy the proposed method in actual application scenarios, we implemented \\name as a sparse engine for efficient LLMs inference. We choose FasterTransformer\\cite{FasterTransformer} as the backend and implemented the sparse general matrix multiplication (Sparse-GEMM) of \\name for LLMs inference. Taking 2:4 sparsity as an example, the sparse deployment of Sparse-GEMM mainly includes three steps. (1) \\name first compresses the sparse weights $ W_{2:4}\\in \\mathbb{R}^{o\\times C}$ into a compressed format, which includes the non-zero weights $ W_{2:4}\\in \\mathbb{R}^{o\\times \\frac{C}{2}} $ and the indices of these non-zero data values. (2) With the support of NVIDIA's cuSPARSE and cuSPARSELt, \\name searches for the optimal matrix multiplication algorithm according to the shape of each sparse weights tensor in LLMs and saves them. (3) Integrates \\name into FasterTransformer for LLMs inference.\nBased on the saved optimal matrix multiplication algorithm, LLMs can skip 50\\% of matrix multiplication operations and perform faster inference. The experiments in Section \\ref{speedup} have shown that such a design can bring \\textbf{19.6\\%--34.8\\%} latency reduction and \\textbf{42.64\\%--43.52\\%} memory saving.\n\n\\section{Experiments}\n\\begin{table*}[t]\n\\centering\n\\footnotesize\n    \\caption{\\name 's perplexity performance on LLaMA model family. The results show that \\name can outperform state-of-the-art methods by a large margin without updating the remaining weights.\n    As for the more constrained and challenging 2:4 sparsity, \\name can obtain an 8.26 perplexity for LLaMA-13B, which is \\textbf{1.32 better than Wanda and 0.85 better than SparseGPT}.\n    }\n\\label{tab:ppl_llama}\n\\resizebox{0.7\\textwidth}{!}\n    {\n    \\begin{tabular}{cccccccc}\n    \\toprule\n    \\textbf{Methods} & \\textbf{N:M sparsity} & \\textbf{LLaMA-7B} & \\textbf{LLaMA-13B} & \\textbf{LLaMA-30B} & \\textbf{LLaMA-65B}  \\\\\n    \\midrule\n    FP16 & - & 5.68 & 5.09 & 4.10 & 3.56 \\\\\n    \\midrule\n    Magnitude & \\multirow{4}{*}{2:4} & 42.53 & 18.36 & 7.62 & 7.11 \\\\\n    SparseGPT &  & 11.00 & 9.11 & 7.16 & 6.28 \\\\\n    Wanda &  & 11.53 & 9.58 & 6.90 & 6.25 \\\\\n    \\textbf{\\name} & & \\textbf{10.56} & \\textbf{8.26} & \\textbf{6.56} & \\textbf{5.69}\\\\\n    \\midrule\n    Magnitude & \\multirow{4}{*}{4:8} & 16.83 & 13.86 & 9.11 & 6.35 \\\\\n    SparseGPT &  & 8.61 & 7.40 & 6.17 & 5.38 \\\\\n    Wanda &  & 8.56 & 7.40 & 5.97 & 5.30 \\\\\n    \\textbf{\\name} & & \\textbf{8.29} & \\textbf{6.92} & \\textbf{5.74} & \\textbf{5.09}\\\\\n    \\bottomrule\n    \\end{tabular}}\n\\end{table*}\n\n\\begin{table*}[t]\n\\centering\n\\caption{Accuracy of LLaMA under 2:4 sparsity patterns on different Zero-Shot tasks. \nIt shows that \\name consistently outperforms SparseGPT and Wanda, especially in terms of overall average accuracy across five tasks.\n}\n\\label{tab_acc2}\n\\resizebox{0.65\\linewidth}{!}{\n\\begin{tabular}{@{}cccccccccc@{}}\n\\toprule\n\\textbf{Params} & \\textbf{Method} & \\textbf{HellaSwag} & \\textbf{PiQA} & \\textbf{OpenBookQA} & \\textbf{SciQ} & \\textbf{LogiQA} & \\textbf{Avg.} \\\\ \\midrule\n\\multirow{5}{*}{7B} & FP16 & 56.41 & 78.29 & 28.20 & 89.6 & 21.81 & 54.86 \\\\\n\\cmidrule{2-8}\n                    & Magnitude & 41.98 & 68.00 & 22.00 & 74.00 & 21.00 & 45.60 \\\\\n                    & Sparse GPT & 42.95 & 70.78 & 19.80 & \\textbf{85.00} & \\textbf{23.34} & 48.37 \\\\\n                    & Wanda & 41.82 & 70.13 & 21.60 & 83.90 & 21.96 & 47.68 \\\\\n                    & \\textbf{\\name} & \\textbf{43.59} & \\textbf{72.03} & \\textbf{23.00} & 84.10 & 22.27 & \\textbf{49.00} \\\\\n\\midrule\n\\multirow{5}{*}{13B} & FP16 & 59.08 & 78.89 & 30.60 & 93.40 & 26.57 & 59.77 \\\\\n\\cmidrule{2-8}\n                     & Magnitude & 45.06 & 71.27 & 23.20 & 82.80 & 25.80 & 57.71\\\\\n                     & Sparse GPT & 47.34 & 74.48 & 24.00 & \\textbf{88.00} & 21.35 & 51.03 \\\\\n                     & Wanda & 45.99 & 73.55 & \\textbf{25.40} & 87.90 & \\textbf{23.04}& 51.16 \\\\\n                     & \\textbf{\\name} &  \\textbf{49.40} & \\textbf{75.24} & 24.80 & 87.80 &  19.81  & \\textbf{51.41} \\\\\n\\midrule\n\\multirow{5}{*}{30B} & FP16 & 62.64 & 81.55 & 29.06 & 92.50 & 28.41 & 58.83 \\\\\n\\cmidrule{2-8}\n                     & Magnitude & 51.10 & 77.36 & 24.40 & 90.10 & 22.42  & 53.08 \\\\\n                     & Sparse GPT & 52.60 & \\textbf{78.40} & 28.20 & 93.30 & 25.96 & 55.69 \\\\\n                     & Wanda & 53.74 & 77.96 & 27.40 & 92.90 & 27.80 & 56.00 \\\\\n                     & \\textbf{\\name} & \\textbf{56.41} & 77.36 & \\textbf{28.80} &  \\textbf{93.80} & \\textbf{29.03} & \\textbf{57.08} \\\\\n\\midrule\n\\multirow{5}{*}{65B} & FP16 & 62.64 &  81.55\n & 29.60 & 92.50 & 28.41 \n & 58.94\\\\\n \\cmidrule{2-8}\n                     & Magnitude & 57.07 & 77.36 & 30.00 &  90.10 & 23.65  & 55.64 \\\\\n                     & Sparse GPT & 55.23 & 78.40 & 27.60 & 93.30 & 24.42  & 55.79 \\\\\n                     & Wanda & 55.76 & 77.96 & 29.00 &92.90 & \\textbf{26.72} & 56.47\\\\\n                     & \\textbf{\\name} &  \\textbf{58.46} & \\textbf{78.56} & \\textbf{31.60} & \\textbf{93.80} & 23.04 & \\textbf{57.09} \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{table*}\n\n\\begin{table}[tb]\n\\centering\n\\footnotesize\n\\vspace{0.2mm}\n    \\caption{\\name 's perplexity performance on OPT models. The results reveal that \\name achieves higher performance than Magnitude and Wanda on both 2:4 and 4:8 patterns, which demonstrates the good generalization of \\name.}\n    \\vspace{-0.2cm}\n\\label{tab:ppl_opt_bloom}\n\\resizebox{\\linewidth}{!}\n    {\n    \\begin{tabular}{cccccc}\n    \\toprule\n    \\textbf{Methods}  & \n      \\textbf{OPT-6.7b(2:4)} & \\textbf{OPT-30b(2:4)} & \\textbf{OPT-6.7b(4:8)} & \\textbf{OPT-30b(4:8)}  \\\\\n    \\midrule\n    FP16  & 10.86 & 9.56 & 10.86& 9.56 \\\\\n    \\midrule\n    Magnitude & 264.14 & 1980.71 & 196.18 & 563.72 \\\\\n    Wanda   & 15.89 & 13.42 & 13.56 & 10.87 \\\\\n    \\textbf{\\name}  & \\textbf{14.90} & \\textbf{12.35} & \\textbf{13.12} & \\textbf{10.75}\\\\\n    \\bottomrule\n    \\end{tabular}}\n\\end{table}\n\n\\subsection{Experimental Environments}\n\\textbf{Setup.} In our experimental framework, we primarily target the LLaMA model family (LLaMA-7B/13B/30B/65B) and OPT models (OPT-6.7B/30B). To demonstrate the comprehensiveness of E-Sparse, we further extends it to the OPT and BLOOM models. All models are from the HuggingFace Transformers library \\cite{wolf2019huggingface}. We choose two SOTA methods as our baselines: SparseGPT and Wanda. Following the one-shot sparsity setting of Wanda, we sample the same 128 sequences from C4 \\cite{raffel2020exploring} training data as calibration dataset. All our experiments only need read right on the models without modifying the remaining weights. \nIn addition, we demonstrate the real-world inference acceleration of 4:8 and 2:4 sparsity patterns on NVIDIA Ampere Architecture \\cite{a100}.\n\n\\textbf{Datasets \\& Evaluation.} As perplexity is a stable and robust metric to measure the capabilities of LLMs. Importantly, lower perplexity values indicate better model performance. We reported our results on the WikiText \\cite{merity2016pointer} validation dataset, based on the perplexity metric. To further demonstrate the efficiency of our method, we also present the zero-shot performance of the pruned networks. Notably, higher values are indicative of superior model performance. Our evaluation rely on the widely-acknowledged EleutherAI LM Harness benchmark \\cite{gao2021framework}. The zero-shot evaluation benchmark mainly includes the following datasets: HellaSwag \\cite{zellers2019hellaswag}, OpenbookQA \\cite{mihaylov2018can}, PiQA \\cite{bisk2020piqa}, SciQ \\cite{pedersen2020sciq} and LogiQA \\cite{liu2020logiqa}. \n\n\\begin{table*}[tb]\n\\centering\n\\caption{Ablation study on the pruning metric and channel shuffle. Let ${Norm}$ denote the input feature norm (baseline). ${Entropy}$ indicates the information entropy. $GNS$ means the Global Naive Shuffle, and $LBS$ is the Local Block Shuffle. \nThe results show that both the proposed entropy strategy and two shuffling methods can bring noteworthy performance gains.}\n\\label{tab:ablation}\n\\resizebox{0.75\\textwidth}{!}\n{ \n\\begin{tabular}{@{}cccccccc@{}}\n\\toprule\n \\multicolumn{4}{c}{\\textbf{Techniques}} &\\multirow{2}{*}{ \\textbf{LLaMA-7B}} & \\multirow{2}{*}{\\textbf{LLaMA-13B}} & \\multirow{2}{*}{\\textbf{LLaMA-30B}} & \\multirow{2}{*}{\\textbf{LLaMA-65B}}\\\\ \n\\cmidrule{1-4}\n${Norm}$ & ${Entropy}$ & ${GNS}$  &${LBS}$ & & & & \\\\ \n\\midrule\n \\Checkmark & \\XSolidBrush & \\XSolidBrush & \\XSolidBrush & 11.53  & 9.58 & 6.90 & 6.25 \\\\\n \\Checkmark & \\Checkmark & \\XSolidBrush & \\XSolidBrush & 11.42  & 8.82 & 6.80 & 6.05\\\\\n \\Checkmark & \\Checkmark & \\Checkmark & \\XSolidBrush & 10.98 & 8.58 & 6.62 & 5.78 \\\\\n  \\Checkmark & \\Checkmark & \\Checkmark & \\Checkmark & \\textbf{10.56}  &  \\textbf{8.26}  & \\textbf{6.56} & \\textbf{5.69} \\\\\n\\bottomrule\n\\end{tabular}}\n\\end{table*}\n\n\\begin{table*}[tb]\n\\centering\n\\footnotesize\n\\caption{GEMM Speedup of \\name after 2:4 sparsity on LLMs. The inputs and weights are all in half-precision (FP16) format, and the latency is evaluated on a single NVIDIA A100 40GB GPU. %It shows that \\name achieves up to 1.54$\\times$ speedup.\n}\n\\label{tab:latency}\n\\vspace{-0.2cm}\n\\resizebox{1\\textwidth}{!}\n    {\n    \\begin{tabular}{cccccccc}\n    \\toprule\n    & \\textbf{Layer} & \\textbf{Input} & \\textbf{Weights} & \\textbf{Dense GEMM} & \\textbf{\\name GEMM} & \\textbf{Latency Reduction}  \\\\\n    \\midrule\n    \\multirow{4}{*}{Context-stage} & Q/K/V & $16384\\times 14336$ & $14336\\times 5376$ & 8.452ms & 5.815ms & \\textbf{31.2\\%} \\\\\n    & Att\\_Out & $16384\\times 1792$ & $1792\\times 14336$ & 3.488ms & 2.540ms & \\textbf{27.2\\%} \\\\\n    & FFN-1 & $16384\\times 14336$ & $14336\\times 7168$ & 11.487ms & 8.073ms & \\textbf{29.7\\%} \\\\\n    & FFN-2 & $16384\\times 7168$ & $7168\\times 14336$ & 11.478ms & 8.958ms & \\textbf{21.9\\%} \\\\\n    \\midrule\n    \\multirow{4}{*}{Decoder} & Q/K/V & $16\\times 14336$ & $14336\\times 5376$ & 0.122ms & 0.098ms & \\textbf{19.6\\%} \\\\\n    & Att\\_Out & $16\\times 1792$ & $1792\\times 14336$ & 0.046ms & 0.030ms & \\textbf{34.8\\%} \\\\\n    & FFN-1 & $16\\times 14336$ & $14336\\times 7168$ & 0.160ms & 0.112ms & \\textbf{30.0\\%}  \\\\\n    & FFN-2 & $16\\times 7168$ & $7168\\times 14336$ & 0.158ms & 0.109ms & \\textbf{31.0\\%}  \\\\\n    \\bottomrule\n    \\end{tabular}}\n\\end{table*}\n\\begin{table}[tb]\n\\centering\n\\footnotesize\n\\vspace{0.2mm}\n\\caption{Memory saving of \\name on LLaMA family.}\n    \\label{tab:memory}\n\\vspace{-0.2cm}\n\\resizebox{\\linewidth}{!}\n    {\n    \\begin{tabular}{ccccc}\n    \\toprule\n    \\textbf{Models}  & \n      \\textbf{Dense (FP16)} & \\textbf{Sparse (FP16)} & \\textbf{Memory Saving}  \\\\\n    \\midrule\n    LLaMA-7B  & 9.85GB & 5.65GB & \\textbf{42.64\\%}  \\\\\n    LLaMA-13B  & 19.11GB & 10.89GB & \\textbf{43.01\\%} \\\\\n    LLaMA-30B  & 47.99GB & 27.17GB & \\textbf{43.38\\%} \\\\\n    LLaMA-65B  & 96.50GB & 54.50GB & \\textbf{43.52\\%} \\\\\n    \\bottomrule\n    \\end{tabular}}\n\\end{table}\n\n\\subsection{Pruning Results on LLMs}\nTo demonstrate the pruning performance of \\name, we conduct a series of experiments to evaluate its efficacy across various model sizes within the LLaMA model family.\nSimilar to Wanda and SparseGPT, we evaluate the perplexity of WikiText validation on structured 4:8 and 2:4 sparsity. As Table \\ref{tab:ppl_llama} shows, our \\name achieves significant improvements compared with the strong baselines. It is noteworthy that \\name does not require weight updates, yet it outperforms the reconstruction-based SparseGPT across all variants within the LLaMA model family. \nAt the largest LLaMA-65B, the performance of \\name is close to the FP16 baseline. For instance, 4:8 sparsity achieves a perplexity loss of only 1.53 more than FP16. The results indicate that our entropy-based metric and channel shuffle mechanism plays a critical role in N:M sparsity.\n\nTo assess the generalization of our method, we conduct experiments on OPT model family, which is one of the most representative LLMs prior to the release of the LLaMA. \nWe choose two models of varying sizes, specifically the OPT-6.7B and OPT-30B, for our experiments. According to the result in Table \\ref{tab:ppl_opt_bloom}, it is evident that the implementation of \\name can lead to a substantial enhancement in WikiText validation. For instance, \\name can  achieve a perplexity score of 14.9 at 2:4 sparsity, markedly outperforming Wanda baseline, which registers at 15.89. % Overall, under the same condition of no weight updates, our approach more effectively improves the performance of LLMs on the N:M structured sparsity. \n\nTo provide further evidence of our method's performance, we also present results on several ZeroShot tasks for LLaMA under 2:4 sparsity. The comprehensive results have been tabulated in Tab \\ref{tab_acc2}. \nIt can be observed that our \\name consistently exhibits an edge, particularly evident from the superior average accuracy metrics amassed across the quintet of Zero-Shot tasks when compared with other established baseline methods.\n\\name outperforms Wanda by a margin of 3\\% and exceeds SparseGPT by 1\\% on average accuracy for LLaMA-7B.\nDespite the 2:4 pruning being the most constrained sparsity pattern, our method achieves enhanced performance for all model size on HellaSwag. Additionally, our approach either matches or surpasses the performance of Wanda and SparseGPT on the other four datasets.\n\n\\subsection{Ablation Study}\n\nThe good performance of \\name is mainly attributed to the proposed entropy-based pruning metric and two channel shuffle strategies. \nTo validate the effectiveness of these strategies, we conduct a series of ablation studies on LLaMA models in 2:4 sparse pattern.\nWe take the input feature norm ($Norm$ \\cite{sun2023simple}) as the baseline strategy. \n\nThe results are shown in Table~\\ref{tab:ablation}. Firstly, it shows that simply introducing \\textit{Entropy} to build the pruning metric can bring up to 0.76 perplexity improvement, demonstrating the effectiveness of information entropy on LLM pruning. Then, the introduction of the global naive shuffle and the local block shuffle successively brought the perplexity gains of up to 0.44 and 0.42 respectively, which reveals that $GNS$ and $LBS$ are two complementary channel shuffle strategies.\nThe results above prove that the three proposed new techniques are efficient and effective.\n\n\\subsection{Speedup and Memory Saving}\n\\label{speedup}\n\nIn this section, we show the measured speedup and memory saving of \\name integrated into FasterTransformer. \n\n\\textbf{Speedup.} With the \\name integrated into FasterTransformer, we measure the latency of GEMM in the Context-stage and the Decoder for a batch of 4 and a sequence length of 1024. Due to the lack of support for 4:8 sparsity pattern in NVIDIA Ampere architecture, we only measure the latency of GEMM with 2:4 sparsity on a single A100 40GB GPU. As shown in Table~\\ref{tab:latency}, \\name is consistently faster than the dense FP16 GEMM baseline, delivering up to 34.8\\% latency reduction. It shows that \\name can work well on both the context-stage and the decoder in LLMs.\n\n\\textbf{Memory Saving.} In Table \\ref{tab:memory}, we give the memory saving brought by \\name on LLaMA family. The results reveal that it can save 42.64\\%--43.52\\% memory usage on LLaMA models. We can also see a trend that the larger the model, the more significant the memory saving.\n\n\\section{Related Work}\n\n \\textbf{Traditional Network Pruning.} \n Network pruning was proposed to remove redundant parts of the DNN models, thereby reducing the computational and memory demands of neural networks without accuracy loss \\cite{liu2018rethinking,louizos2017learning, han2015deep, hassibi1993optimal}. \n Traditional network pruning techniques usually fall into two primary categories: unstructured pruning~\\cite{hassibi1993optimal, han2015learning, han2015deep} and structured pruning~\\cite{li2016pruning, luo2017thinet, liu2017learning, li2020weight, li2022weight, ding2021resrep, li2021boosting, xia2022structured}.  \n Unstructured pruning methods \\cite{han2015deep, han2015learning} aim to iteratively prune unimportant connections whose absolute weights are smaller than a given threshold, which achieves good performance on parameter compression. \n However, such kind of methods are implementation-unfriendly.\n  Structured pruning methods \\cite{li2016pruning, luo2017thinet, liu2019metapruning} prune or sparse entire parts of the network (e.g., channels, blocks) instead of individual weights, thus require less specialized libraries to achieve inference speedup.\n A common feature of the traditional pruning techniques mentioned above is that the pruned network usually needs to be retrained to recover the accuracy loss, which hinders their application on LLMs that consume huge training resources.\n \n\n\\textbf{N:M Sparsity.} N:M sparsity~\\cite{mishra2021accelerating,pool2021channel,akiva2022searching,zhou2021learning} is a kind of special pruning technique that introduces an intermediate sparsity pattern between unstructured and structured pruning, called semi-structured sparsity. N:M sparsity aims to prune N out of every M consecutive parameters, rather than pruning individual weights or entire channels/blocks. The appeal of N:M sparsity is its ability to reason for specific hardware architectures (such as NVIDIA Ampere\\cite{pool2020accelerating}), enabling efficient computation. \\cite{akiva2022searching} suggests a Neural Architecture Search (NAS) strategy to sparse both activations and weights throughout the network. \\cite{zhou2021learning} defines a metric, Sparse Architecture Divergence (SAD) to learn N:M sparse neural networks. However, these are only designed for CNNs or small models, and how to design efficient N:M sparsity for LLMs has been rarely studied.\n\n\\textbf{Pruning for LLMs.} Due to the massive size and computational costs of large language models, training-based pruning methods \\cite{ma2023llm, xia2023sheared, singh2023exploiting} will bring a large overhead. So existing popular solutions aim at post-training pruning strategy\\cite{frantar2023massive, sun2023simple}. Such methods only need a small number of calibration data to prune the pre-trained LLMs models, which is suitable for rapid deployment.\nSparseGPT\\cite{frantar2023massive} develops a layer-wise weight update for LLMs via an approximate second-order Hessian. This schema is iteratively executed between weight pruning and weight update at each layer, which is computationally expensive.\nWanda\\cite{sun2023simple} presents to remove the insignificant weights based on the magnitude and norm of corresponding input activations, without updating the remaining weights.\nOur work further proposes a new metric based on the information richness and designs an effective search strategy for N:M sparsity.\n\n\\section{Conclusion}\n\nIn this paper, we propose a novel entropy-based pruning method, called \\name, to carry out N:M sparsity on LLMs in a one-shot manner. \nThe design of our pruning metric is based on the observation of the information richness of hidden state channels and relatively concentrated distributions of information-rich channels.\nExtensive experiments show the superior performance of our proposal against existing LLMs pruning methods.\n\\section{Limitations}\n\nBeyond NLP tasks, the applicability of \\name to other tasks (including computer vision or speech recognition), remains to be tested. For fair comparison with other methods, we only conducted experiments on public datasets with limited sentence lengths. In addition, the combined optimization of \\name and other orthogonal methods (quantization or distillation) has not yet been studied.\n\n\\newpage\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{LongBench v2: Towards Deeper Understanding and\\\\ Reasoning on Realistic Long-context Multitasks}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\n\nThis paper introduces LongBench v2, a benchmark designed to assess the ability of LLMs to handle long-context problems requiring \\emph{deep understanding and reasoning} across real-world multitasks. LongBench v2 consists of 503 challenging multiple-choice questions, with contexts ranging from 8k to 2M words, across six major task categories: single-document QA, multi-document QA, long in-context learning, long-dialogue history understanding, code repository understanding, and long structured data understanding.\nTo ensure the breadth and the practicality, we collect data from nearly 100 highly educated individuals with diverse professional backgrounds. We employ both automated and manual review processes to maintain high quality and difficulty, resulting in human experts achieving only 53.7\\% accuracy under a 15-minute time constraint.\nOur evaluation reveals that the best-performing model, when directly answers the questions, achieves only 50.1\\% accuracy. In contrast, the o1-preview model, which includes longer reasoning, achieves 57.7\\%, surpassing the human baseline by 4\\%. \nThese results highlight the importance of enhanced reasoning ability and scaling inference-time compute to tackle the long-context challenges in LongBench v2.\n\n\\end{abstract}\n\\section{Introduction}\n\nOver the past year, research and products on long-context large language models (LLMs) have made remarkable progress: in terms of context window length, advancing from the initial 8k to the current 128k and even 1M tokens~\\cite{GPT-4o,claude-3-5,reid2024gemini,glm2024chatglm}; and achieving promising performance on long-context benchmarks. However, beneath these advancements lies an urgent and practical question: \\textbf{Do these models truly comprehend the long texts they process, i.e., are they capable of deeply understanding, learning, and reasoning based on the information contained in these long texts?}\n\nCritically, existing long-context understanding benchmarks~\\cite{bai2024longbench,zhang2024infty,hsieh2024ruler} fail to reflect the long-context LLMs' \\emph{deep} understanding capabilities across diverse tasks.\nThey often focus on extractive questions, where answers are directly found in the material, a challenge easily handled by modern long-context models and RAG systems, as evidenced by their perfect recall in the Needle-in-a-Haystack test~\\cite{needleinhaystack}.\nFurthermore, many of these benchmarks rely on synthetic tasks, which limits their applicability to real-world scenarios, and their adopted metrics like F1 and ROUGE are unreliable.\n\nTo address these issues, we aim to build a benchmark with the following features: \n(1) \\textbf{Length}: Context length ranging from 8k to 2M words, with the majority under 128k.\n(2) \\textbf{Difficulty}: Challenging enough that even human experts, using search tools within the document, cannot answer correctly in a short time.\n(3) \\textbf{Coverage}: Cover various realistic scenarios.\n(4) \\textbf{Reliability}: All in a multiple-choice question format for reliable evaluation.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figs/length.pdf}\n    \\caption{Length distribution (left) and human expert solving time distribution (right) of LongBench v2.}\n    \\label{fig:length}\n\\end{figure}\n\nWith the above goal in mind, we present \\emph{LongBench v2}.\nLongBench v2 contains 503 multiple-choice questions and is made up of 6 major task categories and 20 subtasks to cover as many realistic deep comprehension scenarios as possible, including \\emph{single-document QA}, \\emph{multi-document QA}, \\emph{long in-context learning}, \\emph{long-dialogue history understanding}, \\emph{code repository understanding}, and \\emph{long structured data understanding} (detailed in Table~\\ref{tb:stat}).\nAll the test data in LongBench v2 are in English, and the length distribution of each task category is shown on the left of Figure~\\ref{fig:length}.\n\nTo ensure the quality and difficulty of test data, we combine automated and manual reviews during data collection. \nWe first recruit 97 data annotators with diverse academic backgrounds and grades from top universities and then select 24 data reviewers from this group.\nAnnotators provide data including long documents, questions, options, answers, and evidence.\nWe then leverage three long-context LLMs for an automated review, where a question is considered too easy if all three LLMs answer it correctly.\nData passing the automated review are assigned to the reviewers, who answer the questions and determine whether the questions are appropriate (meet our requirements) and if the answers are correct.\nIn our criteria, a qualified data point should have (1) an appropriate question with an objective, correct answer; (2) sufficient difficulty, such that all three LLMs cannot answer correctly at the same time, and the human reviewer cannot answer correctly within 3 minutes, even with searching tools within the document.\nIf data do not meet these criteria, we request modifications from the annotator.\nWe also set length and difficulty incentives to encourage longer and harder test data.\nFigure~\\ref{fig:length} (right) visualizes the distribution of expert solving times along with human accuracy.\n\nOverall, our data shows a median word count of 54k and an average of 104k words. \nHuman experts are able to achieve an accuracy of only 53.7\\% within 15 minutes, compared to 25\\% accuracy with random guessing, highlighting the challenging nature of the test.\nIn the evaluation, the best-performing model achieves only 50.1\\% accuracy when directly outputting the answer. In contrast, the o1-preview model, which incorporates longer reasoning during inference, reaches 57.7\\%, surpassing human experts. This implies that LongBench v2 places greater demands on the reasoning ability of current models, and incorporating more inference-time thinking and reasoning appears to be a natural and crucial step in addressing such long-context reasoning challenges.\nWe hope LongBench v2 will accelerate the exploration of how scaling inference-time compute will affect deep understanding and reasoning in long-context scenarios.\n\\section{Related Work}\n\nWe divide existing long-context benchmarks for LLMs into two types. \nThe first consists of comprehensive benchmarks that combine multitasks such as QA, retrieval, and summarization. \nSorted by publication date, these benchmarks include ZeroSCROLLS~\\cite{shaham2023zeroscrolls}, L-Eval~\\cite{an2024leval}, LongBench~\\cite{bai2024longbench}, BAMBOO~\\cite{dong2024bamboo}, LooGLE~\\cite{li2023loogle}, $\\infty$-bench~\\cite{zhang2024infty}, Ruler~\\cite{hsieh2024ruler}, and HELMET~\\cite{yen2024helmet}.\nIt is noteworthy that most of these multitask benchmarks were proposed last year, which corresponds to the thrive of long-context LLMs, whose context length has been extended to 128k tokens or more~\\cite{claude-3-5,GPT-4o,reid2024gemini,glm2024chatglm,dubey2024llama} through continual training~\\cite{xiong2024effective,pmlr-v235-fu24d,bai2024longalign,gao2024train}.\n\nThe other category of long-context benchmarks is more targeted, evaluating models on specific types of long-context tasks, including document QA~\\cite{kovcisky2018narrativeqa,dua2019drop,dasigi2021dataset,pang2022quality,wang2024leave}, summarization~\\cite{zhong2021qmsum,huang2021efficient,wang2022squality}, retrieval and attributing~\\cite{needleinhaystack,kuratov2024babilong,song2024counting,laban2024summary,zhang2024longcite,vodrahalli2024michelangelo,krishna2024fact}, conversation~\\cite{bai2024longalign}, coding~\\cite{liu2023repobench,bogomolov2024long}, many-shot learning~\\cite{agarwal2024many}, and long-text generation~\\cite{bai2024longwriter,wu2024longgenbench,liu2024longgenbench,que2024hellobench}.\n\nIn our view, existing long-context benchmarks generally have the following issues: (1) \\emph{Lack of deep reasoning}: While a few benchmarks contain longer examples of around 100k, most of these data have not been human-examined, and many of these samples can be solved through shallow understanding such as retrieval, thus failing to reflect a model's deep reasoning capabilities.\n(2) \\emph{Unreliable metrics}: Many datasets use metrics like ROUGE and F1 for evaluation, which are known to be unreliable~\\cite{novikova2017we}. Additionally, some datasets adopt LLM-as-a-judge~\\cite{zheng2023judging,li2024generation} for evaluation, which can be costly and may introduce biases in their assessments~\\cite{bai2024benchmarking,ye2024justice}.\nTo construct a more challenging, reliable, and comprehensive long-context benchmark, we employ a uniform multiple-choice format and manually verify each data point to ensure it meets the required level of difficulty.\n\\begin{table*}[t]\n\\centering  \n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{llrrrr}\n\\toprule\n\\textbf{Dataset} & \\textbf{Source} & \\textbf{\\#data} & \\textbf{Length} & \\textbf{Expert Acc} & \\textbf{Expert Time$^*$} \\\\\n\\midrule\n\\multicolumn{2}{l}{\\cellcolor{mypink}\\emph{I. Single-Document QA}} & \\cellcolor{mypink}175 & \\cellcolor{mypink}51k & \\cellcolor{mypink}55\\% & \\cellcolor{mypink}8.9 min \\\\\nAcademic & Paper, textbook & 44 & 14k & 50\\% & 7.3 min \\\\\nLiterary & Novel & 30 & 72k & 47\\% & 8.5 min \\\\\nLegal & Legal doc & 19 & 15k & 53\\% & 13.1 min \\\\\nFinancial & Financial report & 22 & 49k & 59\\% & 9.0 min \\\\\nGovernmental & Government report & 18 & 20k & 50\\% & 9.5 min \\\\\nDetective & Detective novel & 22 & 70k & 64\\% & 9.3 min \\\\\nEvent ordering & Novel & 20 & 96k & 75\\% & 9.4 min \\\\\n\\midrule\n\\multicolumn{2}{l}{\\cellcolor{mypink}\\emph{II. Multi-Document QA}} & \\cellcolor{mypink}125 & \\cellcolor{mypink}34k & \\cellcolor{mypink}36\\% & \\cellcolor{mypink}6.1 min \\\\\nAcademic & Papers, textbooks & 50 & 27k & 22\\% & 6.1 min \\\\\nLegal & Legal docs & 14 & 28k & 64\\% & 8.8 min \\\\\nFinancial & Financial reports & 15 & 129k & 40\\% & 7.0 min \\\\\nGovernmental & Government reports & 23 & 89k & 22\\% & 6.0 min \\\\\nMulti-news & News & 23 & 15k & 61\\% & 5.3 min \\\\\n\\midrule\n\\multicolumn{2}{l}{\\cellcolor{mypink}\\emph{III. Long In-context Learning}} & \\cellcolor{mypink}81 & \\cellcolor{mypink}71k & \\cellcolor{mypink}63\\% & \\cellcolor{mypink}8.3 min \\\\\nUser guide QA & Electronic device, software, instrument & 40 & 61k & 63\\% & 9.9 min \\\\\nNew language translation & Vocabulary book (\\textit{Kalamang}, \\textit{Zhuang}) & 20 & 132k & 75\\% & 5.4 min \\\\\nMany-shot learning & Multi-class classification task & 21 & 71k & 52\\% & 8.0 min \\\\\n\\midrule\n\\multicolumn{2}{l}{\\cellcolor{mypink}\\emph{IV. Long-dialogue History Understanding}} & \\cellcolor{mypink}39 & \\cellcolor{mypink}25k & \\cellcolor{mypink}79\\% & \\cellcolor{mypink}8.2 min \\\\\nAgent history QA & LLM agents conversation & 20 & 13k & 70\\% & 8.3 min \\\\\nDialogue history QA & User-LLM conversation & 19 & 77k & 89\\% & 6.5 min \\\\\n\\midrule\n\\multicolumn{2}{l}{\\cellcolor{mypink}\\emph{V. Code Repository Understanding}} & \\cellcolor{mypink}50 & \\cellcolor{mypink}167k & \\cellcolor{mypink}44\\% & \\cellcolor{mypink}6.4 min \\\\\nCode repo QA & Code repository & 50 & 167k & 44\\% & 6.4 min \\\\\n\\midrule\n\\multicolumn{2}{l}{\\cellcolor{mypink}\\emph{VI. Long Structured Data Understanding}} & \\cellcolor{mypink}33 & \\cellcolor{mypink}49k & \\cellcolor{mypink}73\\% & \\cellcolor{mypink}6.4 min \\\\\nTable QA & Table & 18 & 42k & 61\\% & 7.4 min \\\\\nKnowledge graph reasoning & KG subgraph & 15 & 52k & 87\\% & 6.2 min \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\caption{Tasks and data statistics in LongBench v2. `Source' denotes the origin of the context. `Length' is the \\emph{median} of the number of words. `Expert Acc' and `Expert Time' refer to the average accuracy and the \\emph{median} time spent on answering the question by human experts. \n$^*$: We allow human experts to respond with ``I don't know the answer'' if it takes them more than 15 minutes.\nAs a result, most expert times are under 15 minutes, but this doesn't necessarily mean that the questions are fully answered within such a time.}\n\\label{tb:stat}\n\\end{table*}\n\n\\section{LongBench v2: Task and Construction}\n\nOur design principle focuses on four aspects: (1) The context should be sufficiently long to cover scenarios ranging from 8k to 2M words, with a relatively even distribution across texts up to 128k words.\n(2) The question should be challenging, requiring the model to deeply understand the context to answer. It should avoid questions that can be answered based on memory or those where the answer can be directly extracted from the context.\n(3) The data should cover a wide range of real-world long-context scenarios and reflect the model's holistic ability to reason, apply, and analyze information drawn from the lengthy text.\n(4) The data should be in English and in a multiple-choice question format, containing a long text, a question, four choices, a groundtruth answer, and an evidence. Distractors should be included to prevent the model from guessing the correct answer based on option patterns.\n\n\\subsection{Task Overview}\nBased on the testing scenarios and the types and sources of long texts, we propose six major task categories and further divide them into 20 subtasks.\nWe introduce the tasks included in LongBench v2 in the following. A list of task statistics and detailed descriptions can be found in Table~\\ref{tb:stat} and Appendix~\\ref{sec:task}.\n\n\\xhdr{Single-Doc QA}\nWe integrate subtask categories from previous datasets~\\cite{bai2024longbench,an2024leval} and expand them to include QA for \\emph{academic}, \\emph{literary}, \\emph{legal}, \\emph{financial}, and \\emph{governmental} documents. \nConsidering that \\emph{detective} QA~\\cite{xu2024detectiveqa} requires in-depth reasoning based on case background, we introduce such a task that requires identifying the killer or motive based on information provided in detective novels.\nWe also include \\emph{Event ordering}, where the goal is to order minor events according to the timeline of a novel.\n\n\\xhdr{Multi-Doc QA}\nTo distinguish from single-doc QA, multi-doc QA requires answers drawn from multiple provided documents.\nBesides the categories in single-doc QA, multi-doc QA also includes \\emph{multi-news QA}, which involves reasoning across multiple news articles, events, and timelines.\n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figs/pipeline.pdf}\n    \\caption{Data collection pipeline of LongBench v2. The annotator first uploads the document(s) and proposes a multiple-choice question based on the content. After that, automated and manual reviews will be conducted to ensure the data meets our requirements. Only data that passes these reviews is eligible for annotation rewards, meaning the annotator must revise the data until it passes all review stages. More details are in section~\\ref{sec:data_collection}.}\n    \\label{fig:pipeline}\n\\end{figure*}\n\n\\xhdr{Long In-context Learning}\nLearning from a long context, such as acquiring new skills, requires the ability to comprehend and reason based on that context. Hence, we consider it as a major category of tasks.\nLongBench v2 includes several key tasks, including \\emph{User guide QA}, which answers questions with information learnt from user guides for electronic devices, software, etc.; \\emph{New language translation}~\\cite{tanzerbenchmark,zhang2024teaching}, which involves learning to translate an unseen language from a vocabulary book; \\emph{Many-shot learning}~\\cite{agarwal2024many}, which involves learning to label new data from a handful of examples.\n\n\\xhdr{Long-dialogue History Understanding}\nLLMs, as more intelligent chatbots or agents, require enhanced memory capabilities to handle longer histories. Therefore, we integrate long-dialogue history understanding tasks to test whether LLMs can handle information from long conversation histories.\nThese tasks are divided into two subtasks based on the source of the conversation history: one involving the history of interactions between multiple LLM agents, i.e., \\emph{Agent history QA}~\\cite{huang2024far}, and the other involving the dialogue history between a user and an LLM acting as an assistant, i.e., \\emph{Dialogue history QA}~\\cite{wu2024longmemeval}.\n\n\\xhdr{Code Repository Understanding}\nCode repository contains long code content, and question answering over a code repository requires understanding and reasoning across multiple files, making it a common yet challenging long-context task.\n\n\\xhdr{Long Structured Data Understanding}\nIn addition to textual data, much information is presented in structured forms, so we introduce the long structured data QA task to test the LLM's understanding of long structured data, including reasoning on long tables, i.e., \\emph{Table QA}~\\cite{zhang2024tablellm}, and answering complex queries on knowledge graphs (KGs), i.e., \\emph{Knowledge graph reasoning}~\\cite{cao2022kqa,bai2023answering}.\nWe anonymize the entities in the KG to prevent the model from directly deriving the answers through memorization.\n\n\\subsection{Data Collection}\n\\label{sec:data_collection}\n\nTo collect high-quality and challenging data for long-context tasks, we hire 97 annotators who are either holding or pursuing a bachelor's degree from top universities and are proficient in English, with detailed statistics shown in Appendix~\\ref{sec:stat}. We also select 24 professional human experts based on their major and year of study for conducting manual reviews.\nFigure~\\ref{fig:pipeline} illustrates the overall pipeline of our data collection process, which consists of five steps: document collection, data annotation, automated review, manual review, and data revision (optional). \nWe develop an online annotation platform to implement this pipeline, with further details provided in Appendix~\\ref{sec:platform}.\n\n\\xhdr{Step 1: Document Collection}\nUnlike previous benchmarks~\\cite{bai2024longbench,an2024leval}, where long documents are pre-defined or synthesized by the benchmark designers, we aim to gather documents that reflect more diverse scenarios and are more likely to be used in everyday contexts. To achieve this, we ask annotators to upload one or multiple files they have personally read or used, such as research papers, textbooks, novels, etc., according to the task type.\nOur platform first converts the uploaded files into plain text using tools such as \\href{https://github.com/pymupdf/PyMuPDF}{\\texttt{PyMuPDF}}.\nThe input documents then undergo two automatic checks. If the length is less than 8,192 words, it is rejected as too short. Documents with a high overlap with previous annotations are also rejected to ensure diversity.\n\n\\xhdr{Step 2: Data Annotation}\nDuring data annotation, the annotator is tasked with proposing a multiple-choice question based on their submitted documents. The question should be accompanied with four choices, a groundtruth answer, and the supporting evidence. We provide the annotators with a detailed question design principle that specifies our requirement (Appendix~\\ref{sec:guide}). To summarize, the following types of questions should be avoided:\n(1) \\emph{Counting questions}: Avoid questions that require counting large numbers.\n(2) \\emph{Simple retrieval questions}: Do not ask basic information retrieval questions, as these are too easy for modern LLMs~\\cite{song2024counting}.\n(3) \\emph{Overly professional questions}: Questions should not demand extensive external knowledge; they should rely on minimal expertise.\n(4) \\emph{Tricky questions}: Do not create questions that are deliberately difficult; the goal is to keep the questions natural and straightforward.\n\n\\xhdr{Step 3: Automated Review}\nUpon submission, each question undergoes an initial automated review process to ensure it is not too easy. \nWe employ three fast and powerful LLMs with a 128k context length to answer the questions: GPT-4o-mini~\\cite{GPT-4o-mini}, \\href{https://open.bigmodel.cn/pricing}{GLM-4-Air}, and \\href{https://open.bigmodel.cn/pricing}{GLM-4-Flash}.\nInputs that exceed the context length are truncated from the middle.\nIf all three LLMs answer the question correctly, it is considered too easy. In such cases, annotators will be required to revise the question and choices to increase its difficulty.\n\n\\xhdr{Step 4: Manual Review}\nData passing the automated review is sent to a human expert for manual review. \nOur manual review serves two purposes: first, to filter out unqualified questions and data with incorrect answers; second, to establish a human baseline while also determining the difficulty of the questions and filter out those that are too easy (i.e., questions that humans can answer correctly in a short amount of time).\nIn practice, the reviewer first goes through a checklist to determine whether the question meets the specified requirements (outlined in Appendix~\\ref{sec:guide}). Next, the reviewer downloads the raw document files and attempts to answer the question. The reviewer is encouraged to use searching tools within the files to solve the problem more promptly. Once a choice is submitted, the reviewer can view the groundtruth answer and the evidence provided by the annotators. The reviewer will then decide whether the answer is objective and fully correct. Our platform tracks the time spent on each question, and if the human expert answers correctly within 3 minutes, the question will be considered too easy, demanding a revision from its annotator.\nSince answering some questions may require spending several hours reading the material, which implies a significant review time cost, we allow human experts to respond with ``I don't know the answer'' after 15 minutes.\n\n\\xhdr{Data Revision}\nAs mentioned above, questions deemed unqualified during either automated or manual review will require revision by its annotator. We set up a separate page in our platform for annotator to track their rejected data. For each rejected data, we provide the annotator with a reason for the rejection, classified into three categories: (1) \\emph{Illegal question}: Rejected by human reviewers due to the question being unqualified, (2) \\emph{Insufficient difficulty}: Rejected by automated review or due to human reviewer answering the question correctly within 3 minutes, and (3) \\emph{Wrong answer}: Rejected by human reviewers. Based on this feedback, annotators will refine their data until it passes the review process. To avoid wasting too much manual resources on low-quality data, we will terminate the review-revision cycle if the data has been revised more than five times without passing.\n\n\\xhdr{Mechanism Design}\nTo incentivize annotators to provide high-quality, challenging, and longer test data, our reward mechanism is set as follows. First, annotators can receive a base reward of \\texttt{100} \\texttt{CNY} only if the data passes the review process; no reward is given for data that does not pass. To encourage annotators to provide longer data, we offer additional length rewards of \\texttt{20}, \\texttt{40}, and \\texttt{50} \\texttt{CNY} for passed data in the length ranges $(32k, 64k]$, $(64k, 128k]$, and over $128k$, respectively (in word count). To motivate annotators to provide more difficult data, we define \\emph{hard} set data as data where at least two out of three models do not answer correctly in automated review and the human reviewer is unable to solve it within 10 minutes; all other data is considered \\emph{easy} data. For hard data, annotators can earn an additional difficulty reward of \\texttt{50} \\texttt{CNY}.\nEach human expert is rewarded \\texttt{25} \\texttt{CNY} for reviewing each piece of data. We also conduct random checks on their reviews, and any human expert whose reviews repeatedly fail these checks will have all of their reviewing rewards revoked.\n\n\\subsection{Data Verification}\nFor a final check, we sample 70 test data and invite our authors to verify their correctness and whether they are Google-proofed~\\cite{rein2023gpqa}.\n\n\\xhdr{Correctness} \nCheck the selected answer based on the provided evidence to determine if it is correct, with all other options being incorrect. An answer is also deemed incorrect if there is any controversy, ambiguity, or reliance on subjective judgment.\n\n\\xhdr{Google-proof} Search for the answer to the question on the internet (Google). The data is considered Google-proof if the answer cannot be found within 15 minutes of searching.\n\nThrough our verification, we find that \\texttt{68/70} of the data are completely correct, and \\texttt{67/70} are Google-proofed. Therefore, we estimate that the error rate of our data is around 3\\%, and the majority of the questions cannot be answered by memorizing existing data on the internet.\nWe review all the data to ensure that it does not contain any sensitive information related to privacy or copyrights.\n\n\\subsection{Data Statistics}\nWe categorize the 503 data entries in Longbench v2 based on their difficulty, length, and task types. According to the difficulty criteria defined in the previous section, 192 are classified as ``Easy'', while 311 are deemed ``Hard''. Based on word count, the data is divided into three groups: ``Short'' ($<$32k), ``Medium'' (32k-128k), and ``Long'' ($>$128k), containing 180, 215, and 108 entries, respectively, exhibiting a relatively balanced distribution.\nFor the data distribution across task types, please see Table~\\ref{tb:stat}.\nAlso, the questions with answers A, B, C, and D account for approximately 19\\%, 25\\%, 30\\%, and 26\\% of the total, respectively, showing that the distribution of answers across the four options is relatively even.\nWe also analyze the proportion of data submissions rejected during manual review and find that 4\\% of the submissions are rejected for \\emph{illegal question}; 7\\% are rejected for \\emph{insufficient difficulty}; and 4\\% are rejected for \\emph{wrong answer}.\n\\section{Evaluation}\n\n\\subsection{Baselines}\n\n\\xhdr{Setup}\nWe evaluate 10 open-source LLMs, all of which have a context window size of 128,000 tokens, along with 7 proprietary LLMs.\nWe apply middle truncation as described in~\\citet{bai2024longbench} for sequences exceeding the model's context window length.\nGiven the complex reasoning required by our test data, we adopt two evaluation settings: zero-shot and zero-shot + CoT. Following~\\citet{rein2023gpqa}, in the CoT setting, the model is first prompted to generate a chain of thought~\\cite{wei2022chain}, after which it is asked to produce the final answer based on the chain of thought.\nFor details on reproducing our results, please refer to Appendix~\\ref{sec:setup}. \nFor a fair comparison, the Qwen2.5 series models are evaluated without YaRN~\\cite{peng2024yarn}. Their performance when combining YaRN are provided in Table~\\ref{tb:exp_yarn}.\nThe code is available at \\url{https://github.com/THUDM/LongBench}.\n\n\\begin{table*}[t]\n\\centering\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{p{5.7cm}|m{0.7cm}m{0.7cm}|m{0.7cm}m{0.7cm}|m{0.7cm}m{0.7cm}|m{0.7cm}m{0.7cm}|m{0.7cm}m{0.7cm}|m{0.7cm}m{0.7cm}}\n\\toprule\n &  & & \\multicolumn{4}{|c}{\\textbf{Difficulty}} & \\multicolumn{6}{|c}{\\textbf{Length (<32k; 32k-128k; >128k)$^\\diamond$}} \\\\\n\\cmidrule(r){1-3} \\cmidrule(lr){4-7} \\cmidrule(l){8-13}\n\\textbf{Model} & \\multicolumn{2}{c|}{\\textbf{Overall}} & \\multicolumn{2}{c|}{\\textbf{Easy}} & \\multicolumn{2}{c|}{\\textbf{Hard}} & \\multicolumn{2}{c|}{\\textbf{Short}} & \\multicolumn{2}{c|}{\\textbf{Medium}} & \\multicolumn{2}{c}{\\textbf{Long}} \\\\ \n\\midrule\n\\multicolumn{13}{l}{\\emph{Open-source models}} \\\\\n\\texttt{GLM-4-9B-Chat} & 30.2 & \\cellcolor{mygray}30.8 & 30.7 & \\cellcolor{mygray}34.4 & 29.9 & \\cellcolor{mygray}28.6 & 33.9 & \\cellcolor{mygray}35.0 & 29.8 & \\cellcolor{mygray}30.2 & 25.0 & \\cellcolor{mygray}25.0 \\\\\n\\texttt{Llama-3.1-8B-Instruct} & 30.0 & \\cellcolor{mygray}30.4 & 30.7 & \\cellcolor{mygray}36.5 & 29.6 & \\cellcolor{mygray}26.7 & 35.0 & \\cellcolor{mygray}34.4 & 27.9 & \\cellcolor{mygray}31.6 & 25.9 & \\cellcolor{mygray}21.3 \\\\\n\\texttt{Llama-3.1-70B-Instruct} & 31.6 & \\cellcolor{mygray}36.2 & 32.3 & \\cellcolor{mygray}35.9 & 31.2 & \\cellcolor{mygray}36.3 & 41.1 & \\cellcolor{mygray}45.0 & 27.4 & \\cellcolor{mygray}34.0 & 24.1 & \\cellcolor{mygray}25.9 \\\\\n\\texttt{Llama-3.3-70B-Instruct} & 29.8 & \\cellcolor{mygray}36.2 & 34.4 & \\cellcolor{mygray}38.0 & 27.0 & \\cellcolor{mygray}35.0 & 36.7 & \\cellcolor{mygray}45.0 & 27.0 & \\cellcolor{mygray}33.0 & 24.1 & \\cellcolor{mygray}27.8 \\\\\n\\texttt{Llama-3.1-Nemotron-70B-Inst.} & 31.0 & \\cellcolor{mygray}35.2 & 32.8 & \\cellcolor{mygray}37.0 & 29.9 & \\cellcolor{mygray}34.1 & 38.3 & \\cellcolor{mygray}46.7 & 27.9 & \\cellcolor{mygray}29.8 & 25.0 & \\cellcolor{mygray}26.9 \\\\\n\\texttt{Qwen2.5-7B-Instruct} & 27.0 & \\cellcolor{mygray}29.8 & 29.2 & \\cellcolor{mygray}30.7 & 25.7 & \\cellcolor{mygray}29.3 & 36.1 & \\cellcolor{mygray}35.6 & 23.7 & \\cellcolor{mygray}26.5 & 18.5 & \\cellcolor{mygray}26.9 \\\\\n\\texttt{Qwen2.5-72B-Instruct} & \\textbf{39.4} & \\cellcolor{mygray}38.8 & \\textbf{43.8} & \\cellcolor{mygray}42.2 & \\textbf{36.7} & \\cellcolor{mygray}\\textbf{36.7} & \\textbf{44.4} & \\cellcolor{mygray}\\textbf{50.0} & \\textbf{34.0} & \\cellcolor{mygray}28.8 & \\textbf{41.7} & \\cellcolor{mygray}\\textbf{39.8} \\\\\n\\texttt{Mistral-Large-Instruct-2407} & 26.6 & \\cellcolor{mygray}33.6 & 29.7 & \\cellcolor{mygray}34.4 & 24.8 & \\cellcolor{mygray}33.1 & 37.8 & \\cellcolor{mygray}41.1 & 19.5 & \\cellcolor{mygray}31.2 & 22.2 & \\cellcolor{mygray}25.9 \\\\\n\\texttt{Mistral-Large-Instruct-2411} & 34.4 & \\cellcolor{mygray}\\textbf{39.6} & 38.0 & \\cellcolor{mygray}\\textbf{43.8} & 32.2 & \\cellcolor{mygray}37.0 & 41.7 & \\cellcolor{mygray}46.1 & 30.7 & \\cellcolor{mygray}\\textbf{34.9} & 29.6 & \\cellcolor{mygray}38.0 \\\\\n\\texttt{c4ai-command-r-plus-08-2024} & 27.8 & \\cellcolor{mygray}31.6 & 30.2 & \\cellcolor{mygray}34.4 & 26.4 & \\cellcolor{mygray}29.9 & 36.7 & \\cellcolor{mygray}39.4 & 23.7 & \\cellcolor{mygray}24.2 & 21.3 & \\cellcolor{mygray}33.3 \\\\ \n\\midrule\n\\multicolumn{13}{l}{\\emph{Proprietary models}} \\\\\n\\texttt{GLM-4-Plus} & 44.3 & \\cellcolor{mygray}46.1 & 47.4 & \\cellcolor{mygray}52.1 & 42.4 & \\cellcolor{mygray}42.4 & 50.0 & \\cellcolor{mygray}53.3 & 46.5 & \\cellcolor{mygray}44.7 & 30.6 & \\cellcolor{mygray}37.0 \\\\\n\\texttt{GPT-4o-mini-2024-07-18} & 29.3 & \\cellcolor{mygray}32.4 & 31.1 & \\cellcolor{mygray}32.6 & 28.2 & \\cellcolor{mygray}32.2 & 31.8 & \\cellcolor{mygray}34.8 & 28.6 & \\cellcolor{mygray}31.6 & 26.2 & \\cellcolor{mygray}29.9 \\\\\n\\texttt{GPT-4o-2024-08-06} & 50.1 & \\cellcolor{mygray}51.2 & 57.4 & \\cellcolor{mygray}57.9 & 45.6 & \\cellcolor{mygray}47.1 & 53.3 & \\cellcolor{mygray}53.9 & 52.4 & \\cellcolor{mygray}\\textbf{50.7} & 40.2 & \\cellcolor{mygray}47.7 \\\\\n\\texttt{GPT-4o-2024-11-20} & 46.0 & \\cellcolor{mygray}51.4 & 50.8 & \\cellcolor{mygray}54.2 & 43.0 & \\cellcolor{mygray}49.7 & 47.5 & \\cellcolor{mygray}59.6 & 47.9 & \\cellcolor{mygray}48.6 & 39.8 & \\cellcolor{mygray}43.5 \\\\\n\\texttt{o1-mini-2024-09-12} & 37.8 & \\cellcolor{mygray}38.9 & 38.9 & \\cellcolor{mygray}42.6 & 37.1 & \\cellcolor{mygray}36.6 & 48.6 & \\cellcolor{mygray}48.9 & 33.3 & \\cellcolor{mygray}32.9 & 28.6 & \\cellcolor{mygray}34.3 \\\\\n\\texttt{o1-preview-2024-09-12} & \\textbf{57.7} & \\cellcolor{mygray}\\textbf{56.2} & \\textbf{66.8} & \\cellcolor{mygray}\\textbf{58.9} & \\textbf{52.1} & \\cellcolor{mygray}\\textbf{54.6} & \\textbf{62.6} & \\cellcolor{mygray}\\textbf{64.6} & \\textbf{53.5} & \\cellcolor{mygray}50.2 & \\textbf{58.1} & \\cellcolor{mygray}\\textbf{54.3} \\\\\n\\texttt{Claude-3.5-Sonnet-20241022} & 41.0 & \\cellcolor{mygray}46.7 & 46.9 & \\cellcolor{mygray}55.2 & 37.3 & \\cellcolor{mygray}41.5 & 46.1 & \\cellcolor{mygray}53.9 & 38.6 & \\cellcolor{mygray}41.9 & 37.0 & \\cellcolor{mygray}44.4\\\\\n\\midrule\n\\cellcolor{mypink}\\emph{Human$^*$} & \\multicolumn{2}{c|}{\\cellcolor{mypink}53.7} & \\multicolumn{2}{c|}{\\cellcolor{mypink}100} & \\multicolumn{2}{c|}{\\cellcolor{mypink}25.1} & \\multicolumn{2}{c|}{\\cellcolor{mypink}47.2} & \\multicolumn{2}{c|}{\\cellcolor{mypink}59.1} & \\multicolumn{2}{c}{\\cellcolor{mypink}53.7} \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\caption{Evaluation results (\\%) on LongBench v2. Results under \\colorbox{mygray}{CoT} prompting are highlighted with a gray background. Note that random guessing yields a baseline score of 25\\%. To account for model responses and human responses that do not yield a valid choice, we report the \\emph{compensated} results in Table~\\ref{tb:exp_comp}, where these cases are counted towards the accuracy with a random probability of 25\\%. $^*$: The human expert's accuracy is based on their performance within a 15-minute time limit, after which they are allowed to respond with ``I don't know the answer''. This occurred for 8\\% of the total test data. $^\\diamond$: Models do not show lower scores on subsets with longer length ranges because the distribution of tasks differs significantly across each length range (Figure~\\ref{fig:length}).}\n\\label{tb:exp}\n\\end{table*}\n\n\\xhdr{Results}\nWe report the evaluation results along with human expert performance in Table~\\ref{tb:exp}. The results under the CoT evaluation setting are highlighted with a gray background, while the highest scores among open-source models and proprietary models are in bold.\nThe results indicate that LongBench v2 presents a significant challenge to the current model---The best-performing o1-preview model achieves only 57.7\\% accuracy, which is 4\\% higher than the performance of human experts under a 15-minute time limit. Additionally, the scaling law effect on our benchmark is striking: smaller models such as GLM-4-9B-Chat, Qwen2.5-7B-Instruct, and GPT-4o-mini perform poorly in our tests that require deep understanding and reasoning over long contexts, with accuracy around 30\\%. In contrast, their larger counterparts like GLM-4-Plus, Qwen2.5-72B-Instruct, and GPT-4o show a notable improvement, achieving overall accuracy around or above 40\\%.\nSimilar to reasoning tasks in mathematics and coding~\\cite{wei2022chain,sprague2024cot,o1-preview}, we also find that incorporating explicit reasoning in the model’s responses significantly improves its performance in our long-context reasoning tests.\nThis includes the use of CoT, which results in an average 3.4\\% improvement for open-source models. Additionally, scaling test-time compute with longer reasoning thought shows further improvements, with o1-preview vs. GPT-4o (+7.6\\%) and o1-mini vs. GPT-4o-mini (+8.5\\%).\nFrom the performance across different length intervals, compared to human, the models perform best on data $<$32k (Short), with the best-performing model surpassing human performance by 15.4\\%. However, even the top model shows a 5.6\\% performance gap compared to human accuracy in the 32k-128k data length range. This highlights the importance of developing methods to maintain strong reasoning capabilities under longer contexts.\n\nTo better distinguish the capability of the models across tasks, we present the performance charts of several representative models across tasks in Figure~\\ref{fig:radar}.\nWe find that the performance gap between LLMs and humans is largest on long structured data understanding tasks, whereas, on single-doc and multi-doc QA tasks, the models perform at par with or even surpass human levels.\nWe hypothesize that this is because the models have seen much more document-type data compared to long structured data during long context training, resulting in poorer understanding of the latter.\nCompared to GPT-4o, we observe that through integrating more thinking steps during inference, o1-preview shows superior performance on multi-doc QA, long in-context learning, and code repository understanding tasks, with a substantial lead over other models.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=0.9\\linewidth]{figs/radar.pdf}\n    \\caption{Average scores across tasks, normalized by the highest score on each task. All scores are evaluated in the zero-shot + CoT setting, except for o1-preview, since it latently performs CoT under zero-shot prompting.}\n    \\label{fig:radar}\n\\end{figure}\n\n\\subsection{Retrieval-Augmented Baselines}\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figs/curve.pdf}\n    \\caption{RAG performance across different context lengths, varied by including the top 4, 8, 16, 32, 64, 128, and 256 chunks of 512 tokens. The horizontal line show the overall score of each model without RAG at a full context length of 128k tokens.}\n    \\label{fig:rag}\n\\end{figure}\n\nBased on recent studies~\\cite{jiang2024longrag,jin2024long,leng2024long}, we explore incorporating retrieval-augmented generation (RAG,~\\citet{lewis2020retrieval}) into long-context LLM and evaluate its performance on LongBench v2.\nWe first split the long context into chunks of 512 tokens with GLM-4-9B tokenizer. Then, we use \\href{https://open.bigmodel.cn/pricing}{Zhipu Embedding-3} to encode the query, i.e., the concatenation of the question and choices, and the chunks, and sort the chunks based on embedding similarity.\nDuring evaluation, we retrieve the top-$N$ most similar chunks and concatenate them in their original order to form the context input for the model. The model is then prompted to answer the question in a zero-shot setting. For each evaluated model, we take $N = 4, 8, 16, 32, 64, 128, \\text{and}\\ 256$, and the evaluation results form a curve presented in Figure~\\ref{fig:rag}.\n\nWe observe that Qwen2.5 and GLM-4-Plus show no significant improvement as the retrieval context length increases beyond 32k. Both models perform better at a 32k retrieval context length compared to using the entire 128k context window without RAG, with Qwen2.5 showing a notable improvement of +4.1\\%. \nIn contrast, only GPT-4o effectively leverages longer retrieval context lengths, achieving the best RAG performance at 128k, while still lagging behind its overall score without RAG (-0.6\\%).\nThese findings suggest that Qwen2.5 and GLM-4-Plus fall short in effectively utilizing and reasoning with information in context windows longer than 32k compared to GPT-4o.\nIn addition, these experiments also confirm that the questions in LongBench v2 are challenging and cannot be solved solely through retrieval.\n\n\\subsection{Measuring Memorization of Context}\n\n\\begin{table}[t]\n\\centering\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{lccccccc}\n\\toprule\n\\textbf{Model} & \\textbf{Avg} & \\textbf{I} & \\textbf{II} & \\textbf{III} & \\textbf{IV} & \\textbf{V} & \\textbf{VI} \\\\\n\\midrule\n\\cellcolor{mygray}\\texttt{GLM-4-9B-Chat} & \\cellcolor{mygray}30.2 & \\cellcolor{mygray}30.9 & \\cellcolor{mygray}27.2 & \\cellcolor{mygray}33.3 & \\cellcolor{mygray}38.5 & \\cellcolor{mygray}28.0 & \\cellcolor{mygray}24.2 \\\\\n\\quad w/o context & 26.2 & 30.9 & 21.6 & 18.5 & 30.8 & 34.0 & 21.2 \\\\\n\\midrule\n\\cellcolor{mygray}\\texttt{Llama-3.1-8B-Inst.} & \\cellcolor{mygray}30.0 & \\cellcolor{mygray}34.9 & \\cellcolor{mygray}30.4 & \\cellcolor{mygray}23.5 & \\cellcolor{mygray}17.9 & \\cellcolor{mygray}32.0 & \\cellcolor{mygray}30.3 \\\\\n\\quad w/o context & 25.8 & 31.4 & 26.4 & 24.7 & 23.1 & 22.0 & 6.1 \\\\\n\\midrule\n\\cellcolor{mygray}\\texttt{Qwen2.5-72B-Inst.} & \\cellcolor{mygray}39.4 & \\cellcolor{mygray}40.6 & \\cellcolor{mygray}35.2 & \\cellcolor{mygray}42.0 & \\cellcolor{mygray}25.6 & \\cellcolor{mygray}50.0 & \\cellcolor{mygray}42.4 \\\\\n\\quad w/o context & 30.0 & 33.7 & 31.2 & 25.9 & 28.2 & 34.0 & 12.1 \\\\\n\\midrule\n\\cellcolor{mygray}\\texttt{GLM-4-Plus} & \\cellcolor{mygray}44.3 & \\cellcolor{mygray}41.7 & \\cellcolor{mygray}42.4 & \\cellcolor{mygray}46.9 & \\cellcolor{mygray}51.3 & \\cellcolor{mygray}46.0 & \\cellcolor{mygray}48.5 \\\\\n\\quad w/o context & 27.6 & 33.7 & 27.2 & 25.9 & 10.3 & 38.0 & 6.1 \\\\\n\\midrule\n\\cellcolor{mygray}\\texttt{GPT-4o} & \\cellcolor{mygray}50.1 & \\cellcolor{mygray}48.6 & \\cellcolor{mygray}44.0 & \\cellcolor{mygray}58.0 & \\cellcolor{mygray}46.2 & \\cellcolor{mygray}56.0 & \\cellcolor{mygray}51.5 \\\\\n\\quad w/o context & 33.1 & 40.0 & 25.6 & 32.1 & 38.5 & 34.0 & 18.2 \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\caption{Scores (\\%) across 6 tasks: \\emph{I. Single-Doc QA}, \\emph{II. Multi-Doc QA}, \\emph{III. Long ICL}, \\emph{IV. Dialogue History}, \\emph{V. Code Repo}, and \\emph{VI. Structured Data}.}\n\\label{tb:mem}\n\\end{table}\n\nFor an effective long-context benchmark, it is essential to ensure that LLMs cannot rely solely on memorizing previously seen data to answer questions. \nThis necessitates the models to actively read and comprehend the provided long material in order to solve the problems.\nFollowing~\\citet{bai2024longbench}, we also evaluate the models' performance when providing only the questions, without the accompanying long context.\nThe performance comparison between with (w/) and without (w/o) the context is presented in Table~\\ref{tb:mem}.\nAs shown, without context, most models achieve an overall accuracy ranging from 25\\% to 30\\%, which is comparable to random guessing. When comparing scores across different tasks, the memorization effect appears minimal for tasks II, III, and VI.\nThe models perform best without context on tasks I and V, likely because they may have seen some of the documents, novels, or code repositories during training.\n\\section{Conclusion}\nOur work introduces LongBench v2, a challenging multitask benchmark for long-context understanding and reasoning, carefully annotated and reviewed by human experts.\nLongBench v2 presents an equal challenge to both humans and state-of-the-art AI systems, with human performance at 50.1\\% and the best LLM achieving 57.7\\% accuracy, providing a reliable evaluation standard for the development of future superhuman AI systems.\nOur evaluation results also bring forward insights into the impact of scaling inference-time compute and RAG in long-context reasoning.\n\\section{Limitations}\nWe acknowledge certain limitations in our work, which we outline below:\n1. \\textbf{Benchmark size}: The benchmark's size may not be sufficiently large. While this can be seen as an advantage for quick evaluation, it could also lead to less stable results that are more vulnerable to randomness. Due to resource constraints, we are unable to expand the dataset at this time. Collecting the current 503 high-quality samples cost us 100,000 CNY and took more than two months.\n2. \\textbf{Language}: The current dataset is limited to English only. As a result, our benchmark does not yet capture the performance of models across multiple languages.\n3. \\textbf{Length distribution inconsistencies}: The length distribution across different tasks is uneven, with certain tasks concentrated around specific lengths. These differences in task distributions across length ranges make it difficult to provide a fair comparison of a single model's performance across length intervals. We recommend conducting comparisons between models on a per-interval basis. For instance, model A may outperform Model B in the short length range, while model B may outperform model A in the long length range. This would suggest that model B is better at handling longer tasks than model A.\n\n\\section*{Acknowledgements}\nWe would like to express our gratitude to our annotation workers for their dedicated contributions. The authors also extend their thanks to Zijun Yao for his assistance in maintaining the platform, and to Yuze He for his valuable suggestions on the paper.\n\n\\newpage\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2307.11088v3.tex",
        "arXiv-2310.15929v2.tex",
        "arXiv-2412.15204v2.tex"
    ],
    "group_id": "group_38",
    "response": "### Title: Evaluating Long Context Language Models: Recent Advances and Challenges\n\n### Introduction\nThe field of large language models (LLMs) has seen significant advancements in recent years, particularly in their ability to process and understand long sequences of text. This capability is crucial for applications such as document summarization, question answering, and in-context learning, where models must comprehend extensive histories or lengthy documents. However, the evaluation of these models, especially in zero-shot settings, remains a challenge. Traditional benchmarks often rely on short prompts and n-gram matching metrics, which may not accurately reflect the models' reasoning and understanding abilities in long context scenarios. Additionally, the computational demands of LLMs have led to a focus on efficient inference techniques, such as pruning and sparsity, to reduce resource consumption without compromising performance. This summary explores three recent papers that address these challenges by proposing new evaluation methodologies and sparsity techniques for LLMs.\n\nThe history of LLMs dates back to the early days of neural network research, where models were primarily evaluated on their ability to perform well on short sequences. As LLMs have grown in size and complexity, the focus has shifted to their capacity to handle longer inputs. This shift has been driven by the need to simulate real-world scenarios where models must process extensive information, such as lengthy conversations or large documents. However, the evaluation of these models has not kept pace with their development, leading to a gap in understanding how well they truly comprehend and reason with long sequences of text.\n\nCurrent progress in LLMs includes the release of models with extended context lengths, such as GPT-4 and Claude, which can process up to 128k tokens or more. These models have shown promising results on various benchmarks, but the benchmarks themselves often lack the diversity and complexity required to fully assess the models' capabilities. Moreover, the computational overhead of these models remains a significant issue, necessitating efficient inference techniques to make them more practical for deployment.\n\nThe challenges in evaluating LLMs include the lack of standardized benchmarks for long context scenarios, the unreliability of traditional metrics in this context, and the difficulty in ensuring that models do not rely on memorization of training data to answer questions. Additionally, the computational demands of LLMs require innovative methods to reduce inference time and memory usage without sacrificing performance.\n\n### Main Content of Each Paper\n\n#### Paper 1: L-Eval - A Comprehensive Benchmark for Long Context Language Models\nL-Eval is a benchmark designed to evaluate the performance of long context language models (LCLMs) on a variety of tasks. The paper introduces a new evaluation suite containing 20 sub-tasks, 508 long documents, and over 2,000 human-labeled query-response pairs. These tasks are divided into two groups: closed-ended tasks, which primarily test reasoning and understanding abilities, and open-ended tasks, which require summarization and aggregation of long document information. The authors emphasize the importance of diversity and quality in the dataset, ensuring correctness through manual validation after data collection.\n\n**Innovations:**\n- **Dataset Construction:** The dataset is manually curated to ensure high quality and diversity, with tasks ranging from multiple-choice questions to summarization and reasoning tasks.\n- **Evaluation Metrics:** L-Eval proposes length-instruction-enhanced (LIE) evaluation and employs large language model (LLM) judges to assess the performance of models on open-ended tasks. This approach is more accurate and robust compared to traditional n-gram matching metrics.\n- **Empirical Findings:** The study reveals a significant gap between open-source LCLMs and commercial models, with open-source models performing better on closed-ended tasks but falling short on open-ended tasks. The authors also highlight the importance of scaled positional embeddings in improving context length but note that they may adversely affect reasoning abilities.\n\n#### Paper 2: E-Sparse - Entropy-based N:M Sparsity for Efficient LLM Inference\nE-Sparse is a pruning method designed to enhance the efficiency of large language models (LLMs) by introducing N:M sparsity, a technique that prunes N out of every M consecutive parameters. The paper leverages information entropy to evaluate the importance of weights and introduces a channel shuffling mechanism to minimize information loss during pruning. E-Sparse is implemented as a Sparse-GEMM on NVIDIA Ampere GPUs, demonstrating significant speedup and memory savings without a substantial loss in accuracy.\n\n**Innovations:**\n- **Pruning Metric:** E-Sparse uses information entropy to enhance the evaluation of parameter importance, combining it with input feature norms to create a more fine-grained pruning metric.\n- **Channel Shuffling:** The method employs global naive shuffle and local block shuffle to reorder the information distribution in LLMs, ensuring that channels with higher information richness are preserved during pruning.\n- **Experimental Validation:** Extensive experiments on the LLaMA family and OPT models show that E-Sparse can achieve up to 1.53 times speedup and 43.52% memory saving, with only a small accuracy loss.\n\n#### Paper 3: LongBench v2 - A Challenging Multitask Benchmark for Long Context Understanding and Reasoning\nLongBench v2 is a benchmark that aims to assess the deep understanding and reasoning capabilities of LLMs in realistic long-context scenarios. The paper introduces a suite of 503 multiple-choice questions, with contexts ranging from 8k to 2M words, across six major task categories. These tasks are designed to be challenging, requiring models to deeply comprehend and reason with the provided information rather than relying on shallow understanding or retrieval.\n\n**Innovations:**\n- **Task Categories:** LongBench v2 covers a wide range of tasks, including single-document QA, multi-document QA, long in-context learning, long-dialogue history understanding, code repository understanding, and long structured data understanding.\n- **Data Collection:** The data is collected through a rigorous process involving document collection, annotation, automated review, manual review, and data revision to ensure high quality and difficulty.\n- **Evaluation Settings:** The benchmark evaluates models in both zero-shot and zero-shot + chain-of-thought (CoT) settings, highlighting the importance of incorporating explicit reasoning in model responses.\n- **Performance Analysis:** The evaluation results show that larger models generally perform better, with the best-performing model (o1-preview) surpassing human performance by 4%. The study also reveals that retrieval-augmented generation (RAG) techniques are less effective for models in handling contexts longer than 32k tokens.\n\n### Commonalities and Innovations\nAll three papers address the challenges of evaluating LLMs in long context scenarios. They emphasize the need for high-quality, diverse datasets and robust evaluation metrics that accurately reflect the models' reasoning and comprehension abilities. Each paper introduces new methodologies to improve the evaluation process and computational efficiency of LLMs:\n\n- **L-Eval** focuses on standardized evaluation, proposing a new dataset and evaluation metrics that better correlate with human judgment.\n- **E-Sparse** introduces a novel pruning technique based on information entropy, enhancing the efficiency of LLM inference.\n- **LongBench v2** aims to provide a challenging benchmark that requires deep understanding and reasoning, covering a wide range of realistic scenarios.\n\n### Comparison of Results\nThe results from the three papers highlight the varying performance of LLMs across different evaluation settings and tasks:\n\n- **L-Eval** shows that commercial models like GPT-4 and Claude outperform open-source models on both closed-ended and open-ended tasks. However, the gap is not accurately reflected by n-gram metrics, and LLM judges provide a more reliable evaluation.\n- **E-Sparse** demonstrates significant improvements in inference speed and memory usage for LLMs, with negligible accuracy loss. The method is particularly effective for models like LLaMA and OPT.\n- **LongBench v2** reveals that larger models generally perform better, with the best-performing model (o1-preview) achieving 57.7% accuracy, compared to human performance of 53.7%. The study also shows that incorporating explicit reasoning during inference significantly improves model performance.\n\n### Conclusion\nThe main findings from these papers suggest that evaluating LLMs in long context scenarios requires high-quality, diverse datasets and robust evaluation metrics. L-Eval provides a standardized suite of tasks and metrics, while E-Sparse introduces a novel pruning technique to enhance computational efficiency. LongBench v2 offers a challenging benchmark that requires deep understanding and reasoning, highlighting the importance of scaling inference-time compute and incorporating explicit reasoning.\n\nFuture research directions include expanding the scope of benchmarks to cover more languages and tasks, improving the reliability of evaluation metrics, and developing more efficient inference techniques that can handle longer contexts without significant performance degradation. Additionally, further investigation into the impact of retrieval-augmented generation (RAG) techniques on long context understanding and reasoning is warranted.\n\n### Tables for Comparison\nBelow are tables summarizing the key results from each paper:\n\n#### Table 1: L-Eval Performance on Closed-Ended Tasks\n| Model                 | Tokens | Coursera | GSM | QuALITY | TOEFL | CodeU | SFiction | Avg. |\n|-----------------------|--------|----------|-----|---------|-------|-------|----------|------|\n| Claude-1.3-100k       | 100k   | 60.03    | 88.00 | 73.76 | 83.64 | 17.77 | 72.65 | 65.97 |\n| GPT-4-32k             | 32k    | 75.58    | 96.00 | 82.17 | 84.38 | 25.55 | 74.99 | 73.11 |\n| Turbo-16k-0613        | 16k    | 63.51    | 84.00 | 61.38 | 78.43 | 12.22 | 64.84 | 60.73 |\n| AdaEmb-Turbo-4k-0613  | 4k     | 61.77    | 23.00 | 58.91 | 76.95 | 6.66 | 71.09 | 49.73 |\n| BM25-Turbo-4k-0613    | 4k     | 63.80    | 23.00 | 59.40 | 75.09 | 5.55 | 71.09 | 49.65 |\n\n#### Table 2: E-Sparse Performance on LLaMA Models\n| Model                 | N:M sparsity | LLaMA-7B | LLaMA-13B | LLaMA-30B | LLaMA-65B |\n|-----------------------|---------------|----------|-----------|-----------|-----------|\n| FP16                  | -             | 5.68     | 5.09      | 4.10      | 3.56      |\n| Magnitude             | 2:4           | 42.53    | 18.36     | 7.62      | 7.11      |\n| SparseGPT             | 2:4           | 11.00    | 9.11      | 7.16      | 6.28      |\n| Wanda                 | 2:4           | 11.53    | 9.58      | 6.90      | 6.25      |\n| \\name                 | 2:4           | **10.56** | **8.26**  | **6.56**  | **5.69**  |\n\n#### Table 3: LongBench v2 Performance on Different Models\n| Model                 | Overall | I. Single-Doc QA | II. Multi-Doc QA | III. Long ICL | IV. Long-dialogue History | V. Code Repo | VI. Structured Data |\n|-----------------------|---------|------------------|------------------|---------------|---------------------------|--------------|---------------------|\n| GLM-4-9B-Chat         | 30.2    | 30.9             | 27.2             | 33.3          | 38.5                      | 28.0         | 24.2                |\n| Llama-3.1-8B-Inst.    | 30.0    | 34.9             | 30.4             | 23.5          | 17.9                      | 32.0         | 30.3                |\n| Qwen2.5-72B-Inst.     | **39.4** | **40.6**          | **35.2**          | **42.0**      | **25.6**                  | **50.0**      | **42.4**            |\n| GLM-4-Plus            | 44.3    | 41.7             | 42.4             | 46.9          | 51.3                      | 46.0         | 48.5                |\n| GPT-4o                | 50.1    | 48.6             | 44.0             | 58.0          | 46.2                      | 56.0         | 51.5                |\n| o1-preview             | **57.7** | **66.8**          | **58.9**          | **53.5**      | **54.6**                  | **58.1**      | **54.3**            |\n\n### Future Research Directions\nThe field of LLMs and their evaluation in long context scenarios is rapidly evolving. Future research should focus on:\n\n- **Expanding Benchmark Scope:** Developing benchmarks that cover a wider range of languages and tasks to better assess the global performance of LLMs.\n- **Improving Evaluation Metrics:** Investigating more reliable and human-correlated metrics for evaluating open-ended tasks, such as those based on LLM judges.\n- **Efficient Inference Techniques:** Further refining pruning and sparsity techniques to enhance the efficiency of LLM inference without significant performance loss.\n- **Incorporating Explicit Reasoning:** Studying the impact of incorporating explicit reasoning steps during inference on model performance in long context scenarios.\n\nThese advancements will contribute to a deeper understanding of LLMs and the development of more principled evaluation methodologies, ultimately driving the creation of more robust and efficient models capable of handling real-world long context challenges."
}