{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{\\modelname: Exploring Realistic EHR Synthesis with Diffusion Models}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nElectronic health records (EHR) contain a wealth of biomedical information, serving as valuable resources for the development of precision medicine systems. However, privacy concerns have resulted in limited access to high-quality and large-scale EHR data for researchers, impeding progress in methodological development. Recent research has delved into synthesizing realistic EHR data through generative modeling techniques, where a majority of proposed methods relied on generative adversarial networks (GAN) and their variants for EHR synthesis. Despite GAN-based methods attaining state-of-the-art performance in generating EHR data, these approaches are difficult to train and prone to mode collapse. Recently introduced in generative modeling, diffusion models have established cutting-edge performance in image generation, but their efficacy in EHR data synthesis remains largely unexplored. In this study, we investigate the potential of diffusion models for EHR data synthesis and introduce a novel method, \\textsc{EHRDiff}. Through extensive experiments, \\modelname establishes new state-of-the-art quality for synthetic EHR data, protecting private information in the meanwhile.\n\\end{abstract}\n\n\\section{Introduction}\nElectronic health records (EHR) contain vast biomedical knowledge. \nEHR data may enable the development of state-of-the-art computational biomedical methods for dynamical treatment \\citep{ASW2020expertsupervised}, differentiable diagnosis \\citep{adpautodiag}, rare genetic disease identification \\citep{Alsentzer2022.12.07.22283238}, etc. \nHowever, EHRs contain sensitive patients' private health information. Before being publicly accessible, real-world EHRs need to undergo de-identification \\citep{Johnson2016MIMICIIIAF,Johnson2023MIMICIVAF}. \nThe de-identification process uses automatic algorithms and requires tedious thorough human reviewing. Pending releasing approval can take months out of legal or ethical concerns \\citep{10.1001/jama.282.15.1466}. \nSuch circumstances limit the public release of rich EHR data, hence impeding the advancement of precision medicine methodologies. \nTo mitigate the issue of limited publicly available EHR data, researchers alternatively explored generating synthetic EHR data \\citep{pmlr-v68-choi17a,synthea}. \nRealistic synthetic EHR generation has recently become a research field of medical informatics. \n\nA line of work approached EHR data synthesis through generative modeling techniques, where they trained generative models on limited real EHR data to generate synthetic EHR data. \nRecent research developed variants of auto-encoders \\citep{autoencoder,Biswal2020EVAGL} or generative adversarial networks (GAN) \\citep{NIPS2014_gan,pmlr-v68-choi17a}. \nThe majority of EHR data synthesis methodologies have relied on GAN \\citep{pmlr-v68-choi17a,medbgan,EHRgan,Yan2020GeneratingEH}. \nAlthough GAN-based methods achieved state-of-the-art performance with respect to synthetic EHR quality and privacy preservation, they suffer from training instability and mode collapse \\citep{che2017mode}. \nPrevious research proposed different techniques to mitigate the problem, while as shown in our experiments, GAN-based methods still are prone to such problems, resulting in unsatisfactory synthetic data quality. \nThis may raise concerns when developing real-world systems using synthetic EHR data from GAN-based methods.\n\nMost recently, novel diffusion models \\citep{pmlr-v37-sohl-dickstein15} in generative modeling have been proposed and have achieved cutting-edge generation performance in the field of vision \\citep{ddpm,song2021scorebased}, audio \\citep{kong2021diffwave}, or texts \\citep{li2022diffusionlm,gong2023diffuseq,Yuan2022SeqDiffuSeqTD}. \nMany variants of diffusion models have surpassed the generation performance of GANs in sample quality and diversity. \nIn general, starting from random noise features, diffusion models use a trained denoising distribution to gradually remove noise from the features and ultimately generate realistic synthetic features. \nThe efficacy of diffusion models on realistic EHR synthesis is less studied compared to GANs. \nConsidering the superior performance of diffusion models in other domains, our work explores the synthesizing performance of such techniques on EHR data. \nWe introduce \\modelname, a diffusion model-based EHR synthesizing model. \n\nOur work conducts comprehensive experiments using publicly available real EHR data and compares the effectiveness of \\modelname against several other GAN-based EHR data synthesizing methods. \nWe provide empirical evidence that \\modelname is capable of generating synthetic EHR data with a high degree of quality. \nAdditionally, our findings reveal that the synthetic EHR data produced by \\modelname is of superior quality compared to those generated by GAN-based models, and it is more consistent with the distribution of real-world EHR data. \n\nOur research has two primary contributions: \nFirstly, we introduce the use of diffusion models to the realm of realistic EHR synthesis and propose a diffusion-based method called \\modelname. \nSecondly, through extensive experimentation on publicly available EHR data, we demonstrate the superior quality of synthetic EHR data generated by \\modelname in comparison to GAN-based EHR synthesizing methods. \nFurthermore, the synthetic EHR data generated by \\modelname exhibits excellent correlation with real-world EHR data.\n\nOur work is summarized as two following contributions: \n\\begin{enumerate}\n    \\item We introduce diffusion models to the fields of EHR data synthesis and propose a diffusion-based method called \\modelname. \n    \\item Through extensive experiments on publicly available real EHR data, we empirically demonstrate the superior generation quality of \\modelname over GAN-based EHR synthesis methods for various EHR feature formats, including categorical, continuous, and time-series features. \n    In the meanwhile, \\modelname can safeguard private information in real training EHR.\n\\end{enumerate}\n\n \n\\section{Related Work}\n\n\\subsection{EHR data synthesis}\n\nIn the literature on EHR synthesis, researchers are usually concerned with the generation of discrete code features such as ICD codes rather than clinical narratives. Researchers have developed various methods to generate synthetic EHR data. Early work was usually disease-specific or covered a limited number of diseases. \\citet{Buczak2010DatadrivenAF} developed a method that generates EHR including visit records, clinical activity, and laboratory results for 203 synthetic tularemia outbreak patients of tularemia. The features in synthetic EHR data are generated based on retrieving similar real-world EHR which is inflexible and prone to privacy leakage. \\citet{synthea} developed a software named Synthea which generates synthetic EHRs with various patient information based on publicly available data. They build generation workflows based on biomedical knowledge and real-world feature statistics. Various models are aggregated for different feature synthesis. However, Synthea only covered the 20 most common conditions.\n\nRecently, researchers mainly applied generative modeling methods for EHR synthesis \\citep{Ghosheh2022ARO}. Medical GAN (medGAN) \\citep{pmlr-v68-choi17a} introduced GAN to EHR synthesis. medGAN can generate synthetic EHR data with good quality and is free of tedious feature engineering. Following medGAN, various GAN-based methods are proposed, such as medBGAN \\citep{medbgan}, EHRWGAN \\citep{EHRgan}, CorGAN \\citep{Torfi2020CorGANCC}, etc. These GAN-based methods advance synthetic EHR to higher quality. \nHowever, a common drawback of GAN-based methods is that these methods suffer from the mode collapse phenomenon which results in a circumstance where a GAN-based model is capable of generating only a few modes of real data distribution \\citep{ThanhTung2018OnCF}.\nTo mitigate the problem, GAN methods for EHR generation rely on pre-trained auto-encoders to reduce the feature dimensions for training stability. However, inappropriate hyper-parameter choices and autoencoder pre-training will lead to sharp degradation of synthetic EHR quality or even failure to generate realistic data. There is also research that uses GAN-based models for conditional synthetic EHR generation to model the temporal structure of real EHR data \\citep{Zhang2020SynTEGAF}. Since diffusion models are less studied in EHR synthesis, we focus on the unconditional generation of EHR and leave modeling conditional temporal structure with diffusion models to future work.  \n\nBesides GAN-based methods, there also exists research that explores generating synthetic EHR data through variational auto-encoders \\citep{Biswal2020EVAGL} or language models \\citep{Wang2022PromptEHRCE}. \nConcurrently, MedDiff \\citep{meddiff} is proposed and explores diffusion models for synthetic EHR generation, and they propose a new sampling technique without which the diffusion model fails to generate high-quality EHRs. \\citet{ceritli2023synthesizing} directly apply TabDDPM \\citep{kotelnikov2022tabddpm} to synthesizing EHR data. \n\n\\subsection{Diffusion Models}\n\nDiffusion models are formulated with forward and reverse processes. The forward process corrupts real-world data by gradually injecting noise, and harvesting training data with different noise levels for a denoising distribution, while the reverse process generates realistic data by removing noise using the denoising distribution. \\citet{pmlr-v37-sohl-dickstein15} first proposed and provided theoretical support for diffusion models. Denoising diffusion probabilistic models (DDPM) \\citep{ddpm} and noise conditional score networks (NCSN) \\citep{song2021scorebased} have discovered the superior capability in image generation, and diffusion models have become a focused research direction since then. Recent research generalizes diffusion models to the synthesis of other data modalities and achieves excellent performance \\citep{li2022diffusionlm,kong2021diffwave}. Our work is among the first research that introduces diffusion models to realistic EHR synthesis \\citep{meddiff,ceritli2023synthesizing}.\n\n\\section{Method}\n\nIn this section, we give an introduction to the problem formulation of realistic EHR synthesis and the technical details of our proposed \\modelname. \n\n\\subsection{Problem Formulation}\n\nReal-world EHR contains diverse patient information including demographics, physiological conditions, laboratory results, ICD codes, etc. \nSuch information may be presented in various data formats (e.g., categorical, binary, continuous, etc). Features of different information may also have different scales, such as ages and heights. \nIn this work, we use a vector $\\boldsymbol{x}_0$ to represent the EHR, where $\\boldsymbol{x}_0\\in \\mathcal{R}^ {|\\mathcal{C}|}$ and $\\mathcal{C}$ represents the dimension of EHR features of interest. \nWe treat all features of different formats as real numbers ranging from $0$ to $1$. \nFollowing previous research \\citep{pmlr-v68-choi17a,medbgan} which only focused on binary code data, we use $1$ standing for code occurrence and $0$ otherwise.\nFor continuous and categorical features, we normalize each feature within the range. \n\n\\subsection{General Framework for Diffusion Model}\n\nDiffusion models are characterized by forward and reverse Markov processes with latent variables where the forward process transforms the real samples to random Gaussian noise while the reverse process generates synthetic samples by gradual denoising the noise. As demonstrated by \\citet{song2021scorebased}, the forward and reverse processes can be described by stochastic differential equations (SDE). The general SDE form for modeling the forward process follows:\n\\begin{align}\n    \\mathrm{d}\\boldsymbol{x} = f(\\boldsymbol{x}, t)\\mathrm{d}t + g(t)\\mathrm{d}\\boldsymbol{w},\n\\end{align}\nwhere $\\boldsymbol{x}$ represents data points, $\\boldsymbol{w}$ represents the standard Wiener process, $t$ is diffusion time, and ranges from $0$ to $T$. At $t=0$, $\\boldsymbol{x}_0$ follows real data distribution while at $t=T$, $\\boldsymbol{x}_T$ asymptotically follows a random Gaussian distribution. Functions $f$ and $g$ respectively define the sample corruption pattern and the level of injected noises. $f$ and $g$ together corrupt real-world samples to random noise. Based on the forward SDE, the SDE for the reverse process can be derived as follows:\n\\begin{align}\n    \\mathrm{d}\\boldsymbol{x} = \\left(f(\\boldsymbol{x}, t)-g^2(t)\\nabla_{\\boldsymbol{x}}\\log p_t(\\boldsymbol{x})\\right)\\mathrm{d}t + g(t)\\mathrm{d}\\boldsymbol{w},\n\\end{align}\nwhere $p_t(\\boldsymbol{x})$ is the marginal density of $\\boldsymbol{x}$ at time $t$, and $\\nabla_{\\boldsymbol{x}}\\log p_t(\\boldsymbol{x})$ is the score function, which indicates a vector field of which the direction is pointed to the high-density data area. \nWith reparameterization proposed in \\citet{Karras2022edm}, the reverse generation process can also be described with the probability flow ordinary differential equations (ODE) instead of SDEs \\citep{song2021scorebased}:\n\\begin{align}\n    &\\mathrm{d}\\boldsymbol{x}=-\\dot{\\sigma}(t)\\sigma(t)\\nabla_{x}\\log p_t\\big( \\boldsymbol{x};\\sigma(t) \\big)\\mathrm{d}t=-\\dot{\\sigma}(t)\\sigma(t)\\nabla_{x}\\log p_{\\sigma_t}\\big( \\boldsymbol{x} \\big)\\mathrm{d}t, \\\\\n    &h(t)=\\exp\\left(\\int_{0}^{t}f(\\xi)\\mathrm{d}\\xi\\right),\\\\\n&\\sigma(t)=\\sigma_t=\\sqrt{\\int_{0}^t\\frac{g(\\xi)^2}{h(\\xi)^2}\\mathrm{d}\\xi},\n\\label{odefin}\n\\end{align}\nwhere $\\dot{\\sigma}(t)$ represents the derivative of $\\sigma(t)$.\nSDE and probability flow ODE indicate stochastic and deterministic generation processes respectively. To generate synthetic samples following reverse processes, it is required to learn a score function $s_\\theta(\\boldsymbol{x})$ parameterized by $\\theta$ by score matching $\\min_\\theta \\E_{p(\\boldsymbol{x}_0)p_{\\sigma_t}(\\boldsymbol{x}|\\boldsymbol{x}_0)}\\left[\\|s_\\theta(\\boldsymbol{x}) - \\nabla_{\\boldsymbol{x}}\\log p_{\\sigma_t}(\\boldsymbol{x}|\\boldsymbol{x}_0)\\|^2_2\\right]$.\n\n\\subsection{\\modelname}\n\nFor the diffusion process of \\modelname, we use $h(t)=1$ and $\\sigma_t=t$ from previous research \\citep{Karras2022edm}.\nTherefore $\\nabla_{\\boldsymbol{x}}\\log p_{\\sigma_t}(\\boldsymbol{x}|\\boldsymbol{x_0}) = -\\frac{\\boldsymbol{x}-\\boldsymbol{x}_0}{\\sigma_t^2}$ and we reparameterize $s_\\theta(\\boldsymbol{x})=-\\frac{\\boldsymbol{x}-D_\\theta(\\boldsymbol{x},\\sigma_t)}{\\sigma_t^2}$, then the objective can be derived as:\n\\begin{align}\n    &\\min_\\theta \\E_{p(\\boldsymbol{x}_0)p_{\\sigma_t}(\\boldsymbol{x}|\\boldsymbol{x}_0)}\\left[\\left\\|\\frac{D_\\theta(\\boldsymbol{x},\\sigma_t)-\\boldsymbol{x}_0}{\\sigma_t^2}\\right\\|^2_2\\right],\n\\end{align}\nand with further simplification of ignoring ${\\sigma_t^2}$, the final objective becomes:\n\\begin{align} \n    \\min_\\theta \\E_{p(\\boldsymbol{x}_0)p_{\\sigma_t}(\\boldsymbol{x}|\\boldsymbol{x}_0)}\\left[\\|D_\\theta(\\boldsymbol{x},\\sigma_t)-\\boldsymbol{x}_0\\|^2_2\\right]. \\label{objective1}\n\\end{align}\n\nGenerally, $D_\\theta(\\boldsymbol{x},\\sigma_t)$ can be modeled by neural networks, while such direct modeling may cause obstacles for optimization because the variance of $\\boldsymbol{x}_t$ and the scale of $\\sigma_t$ are diverse at different time step $t$. Therefore, we chose the pre-conditioning design of $D_\\theta(\\boldsymbol{x},\\sigma_t)$ \\citep{Karras2022edm} where $D_\\theta(\\boldsymbol{x},\\sigma_t)$ is decomposed as:\n\\begin{align}\nD_{\\theta}(\\boldsymbol{x};\\sigma)=c_{\\text{skip}}(\\sigma)\\boldsymbol{x}+c_{\\text{out}}(\\sigma)F_{\\theta}(c_{\\text{in}}(\\sigma)\\boldsymbol{x};c_{\\text{noise}}(\\sigma)).\n\\label{precond}\n\\end{align}\n$F_{\\theta}$ is modeled with neural networks and such designs of $c_{\\text{in}}$ and $c_{\\text{noise}}$ regulate the input to the network to be unit variance across different time step $t$, and $c_{\\text{out}}$ and $c_{\\text{skip}}$ together set the neural model prediction to be unit variance with minimized scale.\nTherefore in \\modelname, we chose $c_{\\text{out}}=\\sigma\\sigma_{\\text{data}}/\\sqrt{\\sigma^2+\\sigma_{\\text{data}}^2}$ and $c_{\\text{skip}}(\\sigma)=\\sigma_{\\text{data}}^2/(\\sigma^2+\\sigma_{\\text{data}}^2)$. $c_{\\text{noise}}(\\sigma) = 0.25\\ln{\\sigma}$ which is designed empirically with the principle of constraining the input noise scale from varying immensely and $\\ln(\\sigma)\\sim \\mathcal{N}(P_{\\text{mean}}, P_{\\text{std}}^2)$ following \\cite{Karras2022edm}, where $P_{\\text{mean}}$ and $P_{\\text{std}}$ are hyper-parameters to be set.\n\nWith the aforementioned formalization of $h(t)$, $\\sigma(t)$ and the learned score function $s_\\theta$, the generation process of \\modelname can be expressed as the ODE:\n\\begin{align}\n    \\mathrm{d}\\boldsymbol{x}=-t s_\\theta(\\boldsymbol{x})\\mathrm{d}t. \\label{odefin}\n\\end{align}\nSolving the ODE numerically requires discretization of the time step $t$ and a proper design of noise level $\\sigma_t$ along the solution trajectory. Therefore, following previous research \\citep{Karras2022edm}, we set the maximum and minimum noise levels as $\\sigma_{\\text{max}}$ and $\\sigma_{\\text{min}}$, and use the following form of discretization:\n\\begin{align}\nt_i=\\sigma_{t_i}=\\left((\\sigma_{\\text{max}})^{\\frac{1}{\\rho}}+\\frac{i}{N-1}\\left((\\sigma_{\\text{min}})^{\\frac{1}{\\rho}}-(\\sigma_{\\text{max}})^{\\frac{1}{\\rho}}\\right)\\right)^{\\rho},\n\\end{align}\nwhere $i$'s are integers and range from $0$ to $N$, $\\sigma_{t_N} = 0, \\sigma_{t_{N-1}} = \\sigma_{\\text{min}}$, and $\\rho$ controls the schedules of discretized time step $t_i$ and trades off the discretized strides $t_i-t_{i-1}$ the larger value of which indicates a larger stride near $t_0$. \nIn order to solve the ODE more precisely and generate synthetic EHR with higher quality, we use Heun's $2$nd order method, which adds a correction updating step for each $t_i$ and alleviates the truncation errors compared to the $1$st order Euler method. We leave the detailed sampling procedure to Appendix \\ref{app:sampling_alg}.\n\n\\section{Experiments}\n\nTo demonstrate the effectiveness of our proposed \\modelname, we conduct extensive experiments evaluating the quality of synthetic EHRs and the privacy concerns of the method. We also compare \\modelname the several GAN-based realistic EHR synthesis methods to illustrate the performance of \\modelname.\n\n\\subsection{Dataset}\n\nMany previous research uses in-house EHR data which is not publicly available for method evaluation \\citep{EHRgan,Yan2020GeneratingEH}. Such experiment designs set obstacles for later research to reproduce experiments. In this work, we use a publicly available EHR database, MIMIC-III, to evaluate \\modelname. \n\nDeidentified and comprehensive clinical EHR data is integrated into MIMIC-III \\citep{Johnson2016MIMICIIIAF}. The patients are admitted to the intensive care units of the Beth Israel Deaconess Medical Center in Boston. For each patient's EHR for one admission, we extract the diagnosis and procedure ICD-9 code and truncate the ICD-9 code to the first three digits. This preprocessing can reduce the long-tailed distribution of the ICD-9 code distribution and results in a 1,782 code set. Therefore, the EHR for each patient is formulated as a binary vector of 1,782 dimensions. The final extracted number of EHRs is 46,520 and we randomly select 41,868 for model training while the rest are held out for evaluation. \n\nAlthough most of the existing research focused on synthesizing discrete code features, real-world EHR data contains various data formats such as continuous test results values or time series of electrocardiograms (ECG). In this work, we extend the previous research and explore applying \\modelname to the synthesis of EHR data other than binary codes. We use the following two datasets: CinC2012 Data and PTB-ECG Data. CinC2012 Data is a dataset for predicting the mortality of ICU patients and contains various feature formats such as categorical age, or continuous serum glucose values. PTB-ECG Data contains ECG signal data for heart disease diagnosis. Detailed introductions of both datasets are left in \\ref{app:data}.\nAll the categorical features are converted into binary columns by one-hot encoding, and continuous features are normalized to values in the range of $[0.0, 1.0]$. We use sets A and B in CinC2023 Data as training and held-out testing sets respectively. The PTB-ECG Data is split with a ratio of 8:2 for training and held-out testing.\n\n\\subsection{Baselines}\n\nTo better demonstrate EHR synthesis performance, we compare \\modelname to several strong baseline models as follows.\n\n\\paragraph{medGAN} \\citep{pmlr-v68-choi17a} is the first work that introduces GAN to generating realistic synthetic EHR data. Considering the obstacle of directly using GAN to generate high-dimensional binary EHR vectors, medGAN alters to a low-dimensional dense space for generation by taking advantage of pre-trained auto-encoders. The model generates a dense EHR vector and then recovers a synthetic EHR with decoders.\n\n\\paragraph{medBGAN and medWGAN} \\citep{medbgan} are two improved GAN models for realistic EHR synthesis. medGAN is based on the conventional GAN model for EHR synthesis, and such a model is prone to mode collapse where GAN models may fail to learn the distribution of real-world data. medBGAN and medWGAN integrate Boundary-seeking GAN (BGAN) \\citep{devon2018boundary} and Wasserstein GAN (WGAN) \\citep{wgan} respectively to improve the performance of medGAN and stabilize model training.\n\n\\paragraph{CorGAN}\\citep{torfi2020corgan} is a novel work that utilizes convolutional neural networks (CNN) instead of multilayer perceptrons (MLP) to model EHR data. \nSpecifically, they use CNN to model the autoencoder and the generative network. They empirically elucidate through experiments that CNN can perform better than the MLP in this task.\n\n\\paragraph{EMR-WGAN}\\citep{zhang2020ensuring} is proposed to further refine the GAN models from several perspectives. To avoid model collapse, the authors take advantage of WGAN. The most prominent feature of EMR-WGAN is that it is directly trained on the discrete EHR data, while the previous research universally uses an autoencoder to first transform the raw EHR data into low-dimensional dense space. \nThey utilize BatchNorm \\citep{batchnorm} for the generator and LayerNorm \\citep{layernorm} for the discriminator to improve performance.\nAs is shown in their experiments, these modifications significantly improve the performance of GAN.\n\n\\subsection{Evaluation Metric}\nIn our experiments, we evaluate the generative models' performance from two perspectives: utility and privacy \\citep{Yan2022AMB}. Utility metrics evaluate the quality of synthetic EHRs and privacy metrics assess the risk of privacy breaches. In the following metrics, we generate and use the same number of synthetic EHR samples as the number of real training EHR samples. \n\n\\subsubsection{Utility Metrics}\n\nWe follow previous research for a set of utility metrics. The following metrics evaluate synthetic EHR quality from diverse perspectives.\n\n\\paragraph{Dimension-wise distribution} describes the feature-level resemblance between the synthetic data and the real data. \nThe metric is widely used in previous research to investigate whether the generative model is capable of learning the high-dimensional distribution of real EHR data. \nFor each code dimension, we calculate the empirical mean estimation for synthetic and real EHR data respectively. The mean estimation indicates the prevalence of the code. We visualize the dimension-wise distribution using scatter plots where both axes represent the prevalence of synthetic and real EHR respectively. \nMany codes have very low prevalence in real EHR data. The generation model may be prone to mode collapse and fail to generate the codes with low prevalence. Therefore, we count the number of codes that exist in the synthetic EHR samples and dub the quantity non-zero code columns.\n\n\\paragraph{Dimension-wise correlation} measures the difference between the feature correlation matrices of real and synthetic EHR data. The $i,j$ entry of correlation matrices calculates the Pearson correlation between the $i$th and $j$th features. For both the synthetic and real EHR data, we calculate first the correlation matrices, and then the averaged absolute differences between the correlation matrices. We name this metric the correlation matrix distance (CMD).\n\n\\paragraph{Dimension-wise prediction} evaluates whether generative models capture the inherent code feature relation by designing classification tasks. Specifically, we select one of the code features to be the classification target and use the rest of the features as predictors. To harvest a balanced target distribution, we sort the code features according to the entropy $H(p)$ of code prevalence $p$, where $H(p)=-p\\log(p) -(1-p)\\log(1-p)$. We select the top 30 code features according to entropies and form 30 individual classification tasks. For each task, we fit a classification model with logistic regression using real training and synthetic EHR data and assess the F1 score on the preset evaluation real EHR data. \n\n\\subsubsection{Privacy Metrics}\n\nGenerative modeling methods need real EHR data for training which raises privacy concerns among practitioners. Attackers may infer sensitive private information from trained models. Besides the utility of synthetic EHR data, we also evaluate existing models from a privacy protection perspective \\citep{pmlr-v68-choi17a,EHRgan,Yan2022AMB}.\n\n\\paragraph{Attribute inference risk} describes the risk that sensitive private information of real EHR training data may be exposed based on the synthetic EHR data\nIt assumes a situation where the attackers already have several real EHR training samples with partially known features, and try to infer the rest features through synthetic data.  \nSpecifically, we assume that attackers first use the k-nearest neighbors method to find the top $k$ most similar synthetic EHRs to each real EHR based on the known code features, and then recover the rest of unknown code features by majority voting of $k$ similar synthetic EHRs. We set $k$ to 1 and use the most frequent 256 codes as the features known by the attackers. The metric is quantified by the prediction F1-score of the unknown code features.\n\n\\paragraph{Membership inference risk} evaluates the risk that given a set of real EHR samples, attackers may infer the samples used for training based on synthetic EHR data. We mix a subset of training real EHR data and held-out testing real EHR data to form an EHR set. For each EHR in this set, we calculate the minimum L2 distance with respect to the synthetic EHR data. The EHR whose distance is smaller than a preset threshold is predicted as the training EHR. We report the prediction F1 score to demonstrate the performance of each model under membership inference risk.\n\n\\begin{table}[t]\n\\caption{NZC represents Non-Zero code Columns, CMD represents Correlation Matrix Distance. $\\downarrow$ and $\\uparrow$ indicate the respectively lower and higher numbers for better results.}\n\\label{tab:utility}\n\\begin{center}\n\\begin{tabular}{ccccccc}\n\\toprule\n        & NZC ($\\uparrow$)& CMD ($\\downarrow$)  \\\\ \\midrule\nmedGAN   & 643$\\pm$59.4 & 45.652$\\pm$11.911   \\\\\nmedBGAN  & 898$\\pm$36.8  & 63.186$\\pm$16.359  \\\\\nmedWGAN  & 376$\\pm$33.5   & 8.603$\\pm$0.163    \\\\\nCorGAN  & 753$\\pm$125.1   & 10.997$\\pm$0.420   \\\\\nEMR-WGAN & 1060$\\pm$29.6  & 8.173$\\pm$0.274   \\\\\n\\modelname & \\textbf{1770}$\\pm$1.9&\\textbf{7.769}$\\pm$0.013  \\\\ \\bottomrule\n\\end{tabular}\n\\end{center}\n\\end{table}\n\n\\begin{figure*}[t]\n    \\begin{center}\n    \\includegraphics[scale=0.65]{DIM.png}\n    \\end{center}\n    \\caption{The dimension-wise probability scatter plot of synthetic EHR data from different generative models against real EHR data. The diagonal lines represent the perfect match of code prevalence between synthetic and real EHR data. }\n    \\label{fig: dim-wise-scatter}\n\\end{figure*}\n\n\\begin{figure*}[t]\n    \\begin{center}\n    \\includegraphics[scale=0.65]{PRE.png}\n    \\end{center}\n    \\caption{The dimension-wise prediction scatter plot of synthetic EHR data from different generative models against real EHR data. The diagonal lines represent the perfect match of code prediction between synthetic and real EHR data. Each scatter represents a task.}\n    \\label{fig: dim-wise-predict}\n\\end{figure*}\n\n\\subsection{Implementation Detail}\nIn our experiments, for the diffusion noise schedule, we set $\\sigma_{\\text{min}}$ and $\\sigma_{\\text{max}}$ to be $0.02$ and $80$. $\\rho$ is set to $7$ and the time step is discretized to $N=32$. $P_{mean}$ is set to $-1.2$ and $P_{std}$ is set to $1.2$ for noise distribution in the training process. For $F_\\theta$ in Equation \\ref{precond}, it is parameterized by an MLP with ReLU \\citep{Nair2010RectifiedLU} activations and the hidden states are set to $[1024, 384, 384, 384, 1024]$. For the baseline methods, we follow the settings reported in their papers. The reported standard errors marked with $\\pm$ are calculated under 5 different runs.\n\n\\subsection{Results on MIMIC}\n\n\\subsubsection{Utility Results}\n\nFigure \\ref{fig: dim-wise-scatter} depicts the dimension-wise prevalence distribution of synthetic EHR data against real data. The scatters from \\modelname are distributed more closely to the diagonal dashed line compared to other baseline models, and \\modelname and EMR-WGAN achieve near-perfect correlation. \nAs shown in Table \\ref{tab:utility}, \\modelname outperforms all baseline methods in non-zero code column number (NZC) by large margins. This shows that GAN-based baselines all suffer from model collapse to different extents. The GAN-based method of best performance, EMR-WGAN, still fails to generate 722 code features with the same number of synthetic EHR samples as the real data. \nAlthough EMR-WGAN achieves a near-perfect correlation between real and synthetic code prevalence and is slightly better than \\modelname, NZC demonstrates that the correlation can be biased by high prevalence features and overshadow the evaluation of low prevalence features. \nThe results above demonstrate that \\modelname can better capture the code feature prevalence of the real data than the GAN-based baselines, and is free from mode collapse. The synthetic EHR data by \\modelname has better diversity than that by GAN-based methods.\n\nFrom CMD results in Table \\ref{tab:utility}, \\modelname surpasses all baseline models. \nCMD results demonstrate that \\modelname can better capture the inherent pair-wise relations between code features than GAN-based methods. \nThe F1 score scatters in Figure \\ref{fig: dim-wise-predict} of \\modelname are closer to the diagonal lines and achieve the highest correlation value as compared to baselines. This means that training on synthetic EHR data by \\modelname can lead to more similar performance to training on real data, and demonstrates that \\modelname can better model complex interactions between code features than baselines.\nIt is indicated that synthetic EHR data by \\modelname may have superior utility for training downstream models biomedical tasks.\nWe present more results on other utility metrics in \\ref{app:results}.\n\n\\begin{table}[t]\n\\caption{The privacy assessment for each model. $\\downarrow$ indicates the lower numbers for better results.}\n\\label{tab:privacy}\n\\begin{center}\n\\begin{tabular}{ccc}\n\\toprule\n& Attribute Inference Risk ($\\downarrow$)& Membership Inference Risk ($\\downarrow$)\\\\\\midrule\nmedGAN  & \\textbf{0.0011}$\\pm$0.0006 & 0.2941$\\pm$0.0061 \\\\\nmedBGAN  & 0.0069$\\pm$0.0025 & 0.2961$\\pm$0.0047 \\\\\nmedWGAN  & 0.0066$\\pm$0.0014 & 0.2928$\\pm$0.0055 \\\\\nCorGAN & 0.0033$\\pm$0.0025 & \\textbf{0.2413}$\\pm$0.0061 \\\\\nEMR-WGAN & 0.0259$\\pm$0.0008 & 0.2943$\\pm$0.0028 \\\\\n\\modelname & {0.0190}$\\pm$0.0013&{0.2956}$\\pm$0.0013\\\\ \n\\bottomrule\n\\end{tabular}\n\\end{center}\n\\end{table}\n\n\\subsubsection{Privacy Results}\\label{sec:privacy}\n\nIn Table \\ref{tab:privacy}, we list the results against privacy attacks. In terms of attribute inference risk and membership inference risk, \\modelname achieves intermediate results, while medGAN and CorGAN respectively achieve the best results on attribute inference risk and membership inference risk. However, as shown in utility results, the quality of synthetic EHR data by both models is far worse than \\modelname. In an extreme circumstance where a generative model fails to fit the real EHR data distribution, the model may achieve perfect results on both privacy metrics, since attackers can not infer private information through synthetic data of bad quality. Therefore, there exists an implicit trade-off between utility and privacy. We suspect that medGAN and CorGAN can better safeguard privacy due to mediocre synthesis quality. When compared to EMR-WGAN which achieves the best synthesis quality among baselines, \\modelname surpasses EMR-WGAN on attribute inference risk and achieves on-par results in terms of membership inference risk. To conclude, \\modelname can well protect the sensitive private information of real EHR training data.\n\n\\begin{table}[t]\n\\caption{The AUC values on Cinc2012 Data and PTB-ECG Data.}\n\\label{tab:discussion}\n\\begin{center}\n\\begin{tabular}{ccccccc}\n\\toprule\n         &  & Cinc2012 Data                               &  & PTB-ECG Data&  \\\\\nReal     && 0.8479 &&0.9963 &   \\\\ \\hline\nmedGAN   && 0.6176$\\pm$0.0676  && 0.7550$\\pm$0.0375 &    \\\\\nmedBGAN  && 0.5942$\\pm$0.0763   && 0.7301$\\pm$0.0215  &  \\\\\nmedWGAN  && 0.7012$\\pm$0.0471  && 0.8071$\\pm$0.0273 &     \\\\\nCorGAN   && 0.6352$\\pm$0.1259   && 0.4521$\\pm$0.0709 &    \\\\\nEMR-WGAN && 0.8010$\\pm$0.0143   && 0.8011$\\pm$0.0171  &  \\\\\nEHRDiff  && \\textbf{0.8405}$\\pm$0.0024 &&  \\textbf{0.9898}$\\pm$0.0010 &   \\\\ \\toprule\n\\end{tabular}\n\\end{center}\n\\end{table}\n\n\\subsection{Results on PTB-ECG and CinC2023}\n\nSince both datasets are designed for classification, we inspect the utilities of synthesized data by evaluating the Area Under receiver operating characteristic Curve (AUC) of classifiers trained with synthetic data. We use LightGBM \\citep{ke2017lightgbm} as classifiers and train on synthetic data of the same size as real training data. \n\nThe results shown in Table \\ref{tab:discussion} that classifiers trained by synthetic data from \\modelname achieve the highest AUC values and are consistently better than GAN-based methods, reaching 0.8405 and 0.9898 on average on CinC2012 Data and PTB-ECG Data respectively. They also have on-par performance with classifiers trained by real data. The results show the great utility of \\modelname generated EHR data, and the efficacy is consistently good across different EHR data feature formats. This demonstrates \\modelname is practical in real-world scenarios and can approach EHR synthesis of diverse formats. The downstream biomedical models can benefit from training on synthetic EHR data by \\modelname, potentially overcoming the obstacles of limited publicly available real EHR data.\n\n\\section{Discussion}\n\n\\begin{table}[t]\n\\caption{Ablation results of utility and privacy on different aspects of \\modelname designs. VE and VP are short for variance exploding and variance preserving respectively. Pre-Cond represents pre-conditioning. }\n    \\label{tab:ablation}\n\n\\begin{center}\n\\begin{tabular}{l|ccc|cc}\n\\toprule\n&\\multicolumn{3}{c|}{Utility}&\\multicolumn{2}{c}{Privacy}\\\\\n&Corr ($\\uparrow$) &NZC($\\uparrow$)   &CMD ($\\downarrow$)   &AIR ($\\downarrow$)  &MIR ($\\downarrow$) \\\\\n\\hline\n\\modelname    &0.9989$\\pm$0.0001&1771.0$\\pm$1.4&7.768$\\pm$0.013& 0.0187$\\pm$0.0010&0.2954$\\pm$0.0014\\\\\n\n\\multicolumn{6}{l}{\\textbf{on Diffusion Process}}\\\\\n\\hline\n w/ VE  &0.9876$\\pm$0.0003& 1553.0$\\pm$5.0 &8.057$\\pm$0.022& 0.0130$\\pm$0.0010 &0.2549$\\pm$0.0023\\\\\n w/ VP  &0.9976$\\pm$0.0001&1564.3$\\pm$1.2& 8.002$\\pm$0.013& 0.0143$\\pm$0.0007& 0.3024$\\pm$0.0007\\\\\n\n\\multicolumn{6}{l}{\\textbf{on Sampling Method}}\\\\\n\\hline\n w/ 1st Euler& 0.9974$\\pm$0.0001&1541.3$\\pm$4.2&7.889$\\pm$0.006& 0.0155$\\pm$0.0003& 0.3111$\\pm$0.0012\\\\\n\n\\multicolumn{6}{l}{\\textbf{on Network Design}} \\\\\n\\hline\n w/o Pre-Cond&0.9655$\\pm$0.0027& 432.0$\\pm$26.8 &8.719$\\pm$0.058& 0.0030$\\pm$0.0009&0.3184$\\pm$0.0018\\\\\n\\bottomrule\n    \\end{tabular}\n    \\end{center}\n\\end{table}\n\n\\subsection{Ablation Study}\n\nIn this section, we discuss and conduct ablation experiments on the effective designs of \\modelname. The designs of \\modelname include three major aspects: diffusion process, sampling method for inference, and neural network design for modeling denoising functions. We demonstrate the performance for each through the utility and privacy metrics on MIMIC-III. The results are shown in Table \\ref{tab:ablation}. For the diffusion process, we compare the diffusion process of \\modelname to the choice of variance exploding (VE) and variance preserving (VP) diffusion process. VE and VP are diffusion processes originally proposed in \\citet{song2021scorebased}. We can see that \\modelname outperforms both choices in terms of utility metrics while regarding the privacy concerns, \\modelname performs a little inferior to VP and VE diffusion processes. As discussed in Section \\ref{sec:privacy}, the privacy performance can be compromised to better utility performance. For the sampling process, Heun's 2nd order method outperforms Euler's 1st method in utility. A 2nd order sampling method can lead to a better step-wise estimation of each denoising step, and hence has better generation performance than 1st methods when using the same trained denoising function. For the network design, we can see that \\modelname without pre-conditioning fails to generate high-quality EHR data as shown by the lowest NZC results of only 432. The possible explanation for this is that the binary code features may have diverse prevalence which leads to much difference in variance of each feature. Pre-conditioning sets the inputs and outputs of the neural networks to be unit variance and thus eases the modeling difficulty for the network in the denoising function.  \n\nOverall, we have demonstrated the effectiveness of the design choices in \\modelname regarding diffusion processes, sampling methods, and network designs. \\modelname achieves the best performance regarding utility metrics, indicating a superior generation quality, while such designs in \\modelname only compromise the privacy concerns marginally. \n\n\\begin{figure*}[t]\n    \\begin{center}\n    \\includegraphics[scale=0.46]{scale.png}\n    \\end{center}\n    \\caption{The line plots for CinC2012 and PTB-ECG with different data scales. The green star represents the performance of the model trained on real data.}\n    \\label{fig: scale}\n\\end{figure*}\n\n\\subsection{Influence of Synthetic Data Scale}\n\nOur main motivation is to use the synthetic data from \\modelname to aid the downstream methodology development on EHR, emphasizing the limitation of scarce real-world EHR data. Therefore, we further demonstrate the influence of synthetic EHR data scales on training downstream models. We train classifiers of the same type on PTB-ECG and CinC2023 data with different scales of synthetic data from \\modelname and the previous state-of-the-art EMR-WGAN for comparison. The results are shown in Figure \\ref{fig: scale}.\n\nAs can be seen from the results, the downstream performance of models trained on synthetic data from \\modelname improves as the size of synthesized data scales up. Models trained on synthetic data generated from \\modelname outperformed those trained on data from EMR-WGAN across different sizes of synthetic training data for both CinC2012 and PTB-ECG, except on minimal data size of 100 on CinC2012. Notably, our results demonstrate that smaller sample sizes of synthetic EHR data from \\modelname can lead to superior downstream model performance compared to larger sample sizes from EMR-WGAN. \n\n\\section{Potential Social Impacts}\n\nThe generative models including our \\modelname for synthesizing EHR data can potentially reveal their training data, resulting in unexpected patient privacy information leakage. \nAlthough in our experiments we conduct extensive experiments for privacy protection from attacking risks following existing research \\citep{Yan2022AMB}, we also find the potential drawbacks of the privacy assessment metrics from our results and the extent to which applying our generative model in real-world application require further studying.\n\nData-driven generative models are prone to generating content with potential biases in training data. \nThe EHR synthetic models may also suffer from such malicious tendencies of gender or demographic bias. Existing research on generative models has proposed methods to measure \\citep{Teo2021MeasuringFI} or mitigate such problem \\citep{Grover2019BiasCO,Grover2019FairGM,Teo2022FairGM}. \nAssessing and alleviating the bias of generative models for EHR synthesis remains an important research question.\n\n\\section{Conclusion}\n\nIn this work, we explore EHR data synthesis with diffusion models. We proposed \\modelname, a diffusion-based model, for EHR data synthesis. Through comprehensive experiments on binary code EHR data, we empirically demonstrate the superior performance in generating high-quality synthetic EHR data from multiple evaluation perspectives, setting new state-of-the-art EHR synthesis methods. In the meanwhile, we also show \\modelname can safeguard sensitive private information in real EHR training data. Furthermore, beyond binary code features in EHR data, the efficacy of \\modelname consistently excels in continuous and time-series features. \\modelname can help downstream biomedical methodology research overcome the obstacles of limited publicly available real EHR data.\n\n\\subsubsection*{Acknowledgment}\nThis work was supported by the Natural Science Foundation of China (Grant No. 12171270) and the Natural Science Foundation of Beijing Municipality (Grant No. Z190024).\n\n\\newpage\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Synthesize High-dimensional Longitudinal Electronic Health Records via Hierarchical Autoregressive Language Model}\n\n\\begin{document}\n\n\\maketitle\n\n\\section{Introduction}\nThe widespread adoption of electronic health record (EHR) systems has established the foundation for machine learning (ML) and artificial intelligence (AI) applications in healthcare.\nThe EHR data is highly complex, comprising over 10,000 unique medical codes for diagnoses, procedures, and medications, as well as thousands of lab measurements. Each patient record can include multiple visits with combinations of diagnoses, procedures, medications, and labs. These combinations create intricate relationships and complex patterns across tens of thousands of medical codes.\nAI and ML techniques are used to learn and model complex patterns in EHR data, enabling applications such as clinical predictive modeling~\\cite{GRAM, RETAIN}, health monitoring~\\cite{MonitoringBook, REST}, computational phenotyping~\\cite{DDL, che2017deep}, treatment recommendations~\\cite{GAMENET, wang2018supervised, shang2019pre}, and more. \nHowever, the progress of AI and ML in healthcare is often impeded by the difficulty of  accessing and sharing large real EHR datasets. \nSharing EHR data is  challenging due to privacy, security, and legal constraints. While patient de-identification can alleviate some of these concerns  by removing obvious patient identifiers such as name, address, and birth date \\cite{AutomatedDeidentification, StrategiesDeidentification}, studies have shown that the risk of re-identification remains high even after thorough de-identification~\\cite{CanadaReidentification, EvaluatingReidentification, SystematicReidentification}. \n\nUsing synthetic patient data can offer a safer alternative to sharing real EHR data. Generative models can produce synthetic datasets as substitutes for real patient data~\\cite{SynTEG, EVA, CorGAN, CONAN, MedGAN, MedWGAN, EHRMGAN}. \nVarious methods have been proposed in the literature, including structured patient record generation \\cite{MedGAN, EMR-WGAN, HGAN, SmoothGAN, MedWGAN} and longitudinal record generation \\cite{SynTEG, EVA, EHRMGAN}. \n\nTo date, existing methods cannot  generate realistic EHR data in its original, high-dimensional form.\nThe high dimensionality of EHR data, along with rare and sparse variables and complex relationships among variables, makes the generation task a difficult one. \nConsequently, existing approaches all concede to creating lower-dimensional data by either aggregating variables or using a subset of more common variables of a manageable size.\nFor example, the MedGAN method~\\cite{MedGAN} modeled 615 disease categories without longitudinal information; the SynTEG model~\\cite{SynTEG} aggregates codes to higher level phenotypes and then removes rare phenotypes, resulting in only 1,276 variables; the ehrMGAN approach~\\cite{EHRMGAN} reduced the variable dimension to be less than 100, and EVA~\\cite{EVA} models frequent co-occurrence patterns in the original EHR data as one-hot vectors, limiting its ability to generate diverse and novel co-occurrence patterns. Our supplementary information provides a table of these dimensionalities of existing methods. While these low-dimensional approaches may capture the proper statistics on a small number of variables and support narrow \nML use cases relying solely on those variables, the resulting synthetic data is inadequate for broader applications that require high-dimensional data including comprehensive statistical analysis, patient phenotyping, billing prediction and analysis, disease staging, and comprehensive data sharing. \n\n\\begin{figure*}\n    \\centerline{\\includegraphics[scale=0.24]{images/HALOArchitecture.pdf}}\n    \\caption{\\textbf{The proposed \\method model.} The architecture of \\method utilizing an autoregressive multi-granularity approach which analyzes at both the visit and code level to generate next code probabilities based on the history of all previous visits as generated through a stack of transformer decoder layers and the previous codes in the current visit through a series of masked linear layers.}\n    \\label{fig:MediSynArchitecture}\n\\end{figure*}\n\nWe propose an approach for generating high-dimensional EHR data in its native form: the Hierarchical Autoregressive Language Model (\\method). This model, shown in Figure \\ref{fig:MediSynArchitecture}, takes an autoregressive and probabilistic approach and can capture the hierarchical distribution of EHR records and their temporal relationships. Using a hierarchical approach to model binary sequences of over a million variables, \\method can efficiently learn and represent complex patterns in EHR data.\n\n\\method works by utilizing a pair of modules to represent both the visit- and code-level structures of a patient record. First, it uses a coarse, visit-level module to factorize the probability along each of a patient's visits and to efficiently process and represent a patient's past medical history. It then adds fine, code-level modeling to generate each variable in a given visit based on both that past history and also the previous variables in the same visits for maximum intra-visit cohesion.\n\nWe evaluate the performance of \\method by training it on a comprehensive outpatient claims dataset, as well as the MIMIC-III inpatient EHR data \\cite{MIMIC}, and compare the results with a diverse set of existing synthetic EHR data generation techniques such as ~\\cite{SynTEG, EVA, GPT1}. \n\nWe evaluate the data quality based on its utility in modeling the statistical data distribution and for supporting ML models. \\method can accurately synthesize high-dimensional EHR data via modeling disease code probabilities ($d\\approx10,000$), disease code co-occurrence probabilities within a visit ($d\\approx1,000,000$), and conditional probabilities across consecutive visits ($d\\approx5,000,000$). In our experiments, we found that \\method achieves a correlation coefficient of above 0.9 $R^2$ when compared to real EHR data, demonstrating its ability to generate realistic data.\n\nIn addition to generating high-fidelity and granular EHR data, we show that \\method improves predictive modeling on our EHR dataset by more than $17\\%$ compared to the leading baseline. We evaluate the predictive accuracy and perplexity of \\method on a hold-off test set, demonstrating its superiority. Furthermore, the synthetic data generated by \\method enable downstream phenotyping ML models to achieve comparable accuracy to models trained on real data, with an AUC of $0.938$ for \\method data versus $0.943$ for real data.\nWe then demonstrate that combining real and synthetic data generated by \\method can improve the accuracy of ML models even more compared to using just real EHR data. Furthermore, we show that \\method generates realistic data while simultaneously protecting patients' privacy in the training data, as evaluated by a series of privacy metrics.  \n\n\\section{Results}\n\n\\subsection*{Problem Formulation}\nStructured EHRs are multi-level longitudinal records, where each patient is represented by a sequence of visits. Each visit is characterized by a set of medical codes, reflecting the diagnoses, procedures, and medications administered during that visit. Additional patient information, such as demographics, disease phenotype labels, lab test results, and inter-visit time, can also be included. We begin by formalizing the problem and introducing key notations that will be used throughout. \\\\\n\n\\noindent\\textbf{EHR Data} We represent a patient record $\\mathcal{R}$ as a sequence of visits over time such that \n\\begin{equation}\n\\mathcal{R} = \\mathcal{V}^{(1)}, \\mathcal{V}^{(2)}, \\cdots \\mathcal{V}^{(T)}\n\\end{equation}\nwhere each visit $\\mathcal{V}^{(t)}$ contains a varying number of medical codes $m^{(t)}_1, m^{(t)}_2, \\cdots, m^{(t)}_{|\\mathcal{V}^{(t)}_\\mathcal{C}|} \\in \\mathcal{C}$, lab values $l^{(t)}_1, \\cdots, l^{(t)}_{|\\mathcal{V}^{(t)}_\\mathcal{L}|} \\in \\mathcal{L}$, and the inter-visit time gap $g^{(t)}$. $\\mathcal{C}$ is then the set of all medical codes in our vocabulary, including diagnoses, procedures, and medications and $\\mathcal{L}$ is  the set of all labs. \nBeyond the longitudinal records, a patient record also possesses some static information $\\mathcal{S}$ containing demographics such as gender, race, and birth year and disease phenotype label $\\mathcal{D}$ indicating major and persistent disease conditions.\\\\\n\n\\noindent\\textbf{Matrix Representation} To allow input to \\method and other machine learning models, we then convert $\\mathcal{R}$, $\\mathcal{S}$, and $\\mathcal{D}$ into a matrix representation $\\mathbf{R}$. Specifically, we build $\\mathbf{R} = [\\mathbf{v}_s, \\mathbf{v}_l, \\mathbf{v}_1, \\cdots, \\mathbf{v}_{T},\\mathbf{v}_{e}]$, a matrix containing a sequence of the vector representations for each of the patient's $T$ visits, a preceding start visit, label visit and a succeeding end visit.\n\nThe start visit $\\mathbf{v}_s$ is a one-hot vector containing a special start code added to $\\mathcal{C}$ to signify the start of the record often required for certain model architectures. \n\nThe label visit $\\mathbf{v}_l$ similarly contains special codes added to $\\mathcal{C}$ representing demographic and chronic disease phenotypes from $\\mathcal{S}$ and $\\mathcal{D}$, respectively. For example, this label visit will have codes representing the patient's gender, racial and ethnic groups, birth year, and any chronic labels. \n\nEach subsequent visit $\\mathbf{v}_t \\in \\mathbb{R}^{|\\mathcal{C}|}$ is then represented as a multi-hot binary vector representing medical codes, lab values, and inter-visit gaps present in that visit. To represent continuous lab values and visit gaps in a discrete form, we employ a granular discretization. This is achieved by adding multiple range codes to $\\mathcal{C}$ for each lab test and for the intervals between visits.\nBy converting all  medical information into binary variables,  $c_t^i$ represents the presence of the $i$-th code in $\\mathcal{C}$ in the $t$-th visit of the patient record $\\mathcal{R}$. \n\nFinally, to signal the end of the patient record in $\\mathbf{v}_{e}$, a special last visit code is added to $\\mathcal{C}$, serving a similar purpose to a stop token in natural language generation. This not only enables generative models to learn when to terminate records but also allows for $\\mathbf{R}$ to be padded through additional columns into a constant length for batch input without altering its  content.\n\nFigure \\ref{fig:DataRepresentation} depicts the format of the visit vector and the  EHR representation, and we provide a table of notations for reference in our supplementary information.\\\\ \n\n\\noindent\\textbf{Generation task} is to create $\\mathbf{R}'$, a synthetic patient record  that is statistically similar to and offers the utility of $\\mathbf{R}$ without any one-to-one mapping to a real patient. Our \\method method does this by learning distribution $P(\\mathbf{R})$.\n\n\\begin{figure*}\n    \\centerline{\\includegraphics[scale=0.25]{images/DataRepresentation.pdf}}\n    \\caption{\\textbf{The data formatting}. \\textbf{a} The visit representation. Each visit is represented as a multi-hot vector containing indices for medical codes, static label codes to cover demographics and disease phenotypes, and special codes describing the shape and temporal ordering of the patient's visit. \\textbf{b} The EHR representation. An EHR is then represented as a matrix constructed as a series of temporally ordered visit vectors.}\n    \\label{fig:DataRepresentation}\n\\end{figure*}\n\n\\subsection*{Experimental Design}\n\nWe evaluate our method and compare it to several baselines comprising both recently proposed models and other logical autoregressive model architectures on a series of experiments on both outpatient and inpatient EHR datasets.\nTo maintain the fidelity of the original EHR data, our experiments focus on synthesizing original granular medical codes without aggregating or combining codes. \nSpecifically, we seek to answer the following questions.\n\\begin{itemize}\n    \\item Is \\method effective at modeling the underlying data distribution of electronic health records?\n    \\item Can \\method produce a synthetic dataset that is statistically similar to real EHR data?\n    \\item Can \\method augment real data for more accurate disease phenotyping prediction?\n    \\item Can \\method generate realistic continuous variables  such as lab results and visit time gap?\n    \\item Can \\method preserve patient privacy in the training?\n\\end{itemize}\n\n\\subsection*{Datasets and Experimental Setup}\n\\noindent\\textbf{Datasets} \nWe use two datasets for our experiments:\n\\begin{enumerate}\n\\item The outpatient EHR  is from a large real-world US claims data.\nIt contains 929,268 patients and binary labels for 11 chronic diseases (specific diseases and patient counts are included in the supplementary information).\nThis yields a final real-world outpatient EHR dataset with an average of 34.16 visits per record and 3.52 codes per visit with 9,882 unique ICD-10 codes.\n\\item The inpatient EHR  is from the MIMIC-III ICU stay dataset~\\cite{MIMIC}. It contains 46,520 patients with 25 disease phenotype labels as defined by the MIMIC benchmark~\\cite{MimicBenchmark}. This dataset has an average of 1.26 visits per record and 15.11 codes per visit with 6,841 unique ICD-9 codes. Note that this includes patients with just a single visit (and as we will show, \\method's Code-Level Module allow it to be very effective on those patients).\n\\end{enumerate}\nBoth datasets share the same patient representation  as a series of visits along with chronic disease phenotype labels. We keep the ICD  codes in the data without code aggregation or removing any infrequent codes.  \\\\\n\n\\noindent\\textbf{Experiment setup:} We use a 0.8-0.2 training-test split with an additional 0.9-0.1 training-validation split during training for both outpatient and inpatient datasets. We use the Adam optimizer with learning rate 1e-4 (which was arrived upon through experimentation). We use a batch size of 48 and train for 50 epochs, saving the model with the lowest loss on the validation set. We implement the model and train in the Python 3.6.9 coding language using the PyTorch 1.9.0+cu111 framework \\cite{PyTorch} along with the scikit-learn 0.24.2 and NumPy 1.17.2 packages. Finally, all experiments are done via one NVIDIA TESLA V100 GPU with 32 GB RAM. The \\method source code is publicly available on GitHub at \\url{https://github.com/btheodorou99/HALO_Inpatient}. \n\n\\subsection*{Baseline Methods}\nBelow we outline the baseline methods and the necessary alterations to those baselines to adapt to our problem setting. \n\\begin{itemize}\n    \\item\\method $-$ Coarse: This baseline is an ablation baseline consisting of just the coarse, visit-level granularity module of the full \\method architecture. It generates each code probability based on all previous visits (grouped into a multi-hot representation) but without the fine, inter-visit modeling such that $P(c_i^i)$ is modeled by $P(c_i^i|\\mathbf{v}_1, \\cdots, \\mathbf{v}_{t-1})$ instead of $P(c_i^i|\\mathbf{v}_1, \\cdots, \\mathbf{v}_{t-1}, c_i^1, \\cdots, c_i^{i-1})$. It consists predominantly of 12 transformer decoder blocks in the model of \\cite{GPT2} augmented to support multi-hot as opposed to one-hot inputs and outputs within the embedding layer and final activation layer.\n    \\item GPT Model ~\\cite{GPT2}: We applied the GPT model without any augmentation to support multi-hot inputs and outputs but instead with the conversion of EHRs to a fully one-hot sequential representation. However, this model had to be shrunk down to 3 blocks from 12 to fit into memory because this greatly expanded the length of the sequences.\n    \\item LSTM EHR Model ~\\cite{lee2018natural}: is a deep, autoregressive LSTM model, adapted to generate structured patient records rather than unstructured text as it had previously been utilized, which is directly analogous to the \\method $-$ Coarse model but uses LSTM blocks instead of transformer decoder blocks.\n    \\item SynTEG~\\cite{SynTEG}: is a GAN-based model that uses a transformer and LSTM-based encoder model to generate embeddings of EHRs up to a given visit before feeding those embeddings into a conditional GAN which generates the next visit. \n    \\item EVA ~\\cite{EVA}: is a VAE-based model which uses a bidirectional-LSTM encoder and CNN-based decoder (using deconvolutions to expand the latent encoding to the proper temporal dimension and then masked, diluted 1D convolutions to build the records in an autoregressive manner). The only change we made was to convert the output from one-hot code combinations to multi-hot code probabilities to allow for greater representative power.\n\\end{itemize}\n\n\\subsection*{Evaluating EHR Language Modeling} \\label{sec:modeling}\nThe first evaluation is conducted by predicting the probabilities and outputs of the test set. In this phase, we assess the performance of \\method against two multi-hot language model baselines, namely \\method $-$ Coarse and LSTM. These baselines explicitly generate a probability distribution without accessing the entire input. It's worth noting that other baseline models, such as the GAN-based SynTEG model, the VAE-based EVA model, and the GPT model, cannot be directly compared in this task,  because those methods do not make a single probability prediction for each code within the visit.\n\nOur first evaluation aims to assess the capability of the models to predict the presence of potential medical codes, given a patient's past medical history and the previous codes from the current visit. Note that we explore different orderings of codes (such as most to least prevalent, alphanumeric, random, etc.) but find no noticeable differences, displaying the results of such an exploration in our supplementary information and settling on a random ordering throughout our experiments. This evaluation is crucial in showcasing a model's ability to learn patterns from the patient population and its potential to perform well in various patient simulation and extension applications. We show the results in Table \\ref{table:TestSetResults} where we see that \\method outperforms the two compared language model architectures. Upon closer examination, we observed that the LSTM baseline model struggled with the complexity and size of the outpatient EHR dataset, while our proposed model \\method performed comparably to the \\method $-$ Coarse ablation baseline. In contrast, in the inpatient EHR setting, where the visits are shorter but contain more codes, \\method's multi-granularity approach proved to be highly effective. Specifically, the model achieved a notable 4\\% reduction in binary cross-entropy (BCE) loss and a 17\\% increase in F1 Score on test data when compared to the single granularity \\method $-$ Coarse model. Notably, both \\method models significantly outperformed the LSTM baseline in this setting. These results highlight the significant value of our multi-granularity approach in handling the complex and diverse nature of medical codes in different EHR settings.\n\n\\begin{table*}[]\n\\centering\n\\caption{Test set modeling metrics}\n\\begin{threeparttable}\n\\begin{tabular}{c|ccc|ccc}\n\\toprule\n& \\multicolumn{3}{c|}{Outpatient EHR} & \\multicolumn{3}{c}{Inpatient EHR} \\\\\n& BCE Loss %& Precision & Recall           \n& F1 Score & PP Per Code & BCE Loss & F1 Score & PP Per Code    \\\\ \\midrule\nLSTM    & $7.744 \\times 10^{-4}$     & 0     & 660.204\n    & $2.600 \\times 10^{-4}$ & 0.193      & 74.565     \\\\\n\\method $-$ Coarse & $1.631 \\times 10^{-4}$     & \\textbf{0.829}  & 3.927\n    & $2.019 \\times 10^{-4}$ & 0.343  & 28.448 \\\\ \n\\method & $\\mathbf{1.624 \\times 10^{-4}}$   & 0.828    & \\textbf{3.903}\n    & $\\mathbf{1.932 \\times 10^{-4}}$ & \\textbf{0.414} & \\textbf{24.664} \\\\ \\bottomrule\n\\end{tabular}\n\\begin{tablenotes}[flushleft]\n\\item We include each of our autoregressive, predictive, and likelihood-based models. The bold value denotes the best results. Baseline methods SynTEG, EVA, and GPT are all omitted here because they either do not produce a probability distribution, peek at the outputs, or utilize a different, non-comparable data representation. \\method outperforms both of the baselines, achieving up to an 4\\% decrease in testset BCE loss, a 17\\% increase in F1 score, and a 13\\% lower perplexity per present code as compared to the leading \\method $-$ Coarse baseline. Source data are provided as a Source Data file.\n\\end{tablenotes}\n\\end{threeparttable}\n\\label{table:TestSetResults}\n\\end{table*}\n\nAdditionally, we present perplexity, which evaluates the probability or likelihood of the test set as quantified by a model trained on the training set, normalized by the unit of consideration that we are interested in. In our case, this normalizing unit is the number of medical codes in a patient's medical record (or equivalently number of ones in $\\mathbf{R}$). Perplexity is a metric found commonly in the wider generative modeling domain, especially on the task of natural language generation (e.g. \\cite{GPT2}). We introduce it to the task of synthetic EHR generation here. Perplexity is defined mathematically by\n\\begin{align}\n\\begin{split}\n    \\text{PP}(D) &= \\sqrt[N]{\\frac{1}{P(D)}} \\\\\n        &= \\sqrt[N]{\\frac{1}{P(\\mathbf{R}^{(1)},\\cdots,\\mathbf{R}^{(|D|)})}} \\\\\n        &= \\sqrt[N]{\\frac{1}{P(\\mathbf{R}^{(1)})\\cdots P(\\mathbf{R}^{(|D|)})}}\n\\end{split}\n\\end{align}\nwhere $D$ is the test dataset and $\\mathbf{R}^{(t)}$ is the $t$-th record in $D$. In practice we calculate the values by summing their log probabilities, using the equivalent form\n\\begin{equation}\n    \\text{PP}(D) = \\exp\\left({-\\frac{1}{N}\\sum_{\\mathbf{R} \\in D} \\log P(\\mathbf{R})}\\right)\n\\end{equation}\nThe normalized value then also corresponds to how many of the different normalizing units (medical codes) one would have to randomly pick between on average to achieve the same probability. The results of the perplexity evaluation are shown in Table \\ref{table:TestSetResults} as well. We see similar results as with the classification evaluation with both \\method and \\method $-$ Coarse performing very well on the outpatient EHR dataset (with \\method performing slightly better) as the LSTM baseline struggles, and \\method easily outpacing both baseline methods in this likelihood evaluation for the inpatient EHR dataset, producing a 13\\% lower perplexity per present code as compared to the \\method $-$ Coarse architecture without the inter-visit modeling. Thus, in both of these test set evaluations, we see that \\method is much more effective in terms of modeling the underlying distribution of EHRs.\n\n\\subsection*{Statistical Similarity to real EHRs} \\label{sec:mirroring}\nThe second analysis evaluates the statistical similarity of the generated and real data. For each  methods, we generate a synthetic dataset of the same size as the training dataset. We then compare the unigram and bigram (both within the same visit and across consecutive visits) probabilities for each unique code and pair of codes within the real and synthetic datasets.\\\\\n\n\\noindent{\\bf Statistical comparison results:} \nWe evaluate the data at the visit and record level, considering approximately 10,000 individual codes and over a million bigram codes. We also compare various aggregate statistics, such as the number of visits per record, medical codes per visit, and prevalence of chronic disease labels. The code probability results are presented in Figure~\\ref{fig:codeProbs}, and the aggregate statistics in Table~\\ref{table:AggregateStats}.\n\nAdditionally, we provide $R^2$ values for visit-level normalized code probabilities in our high-dimensional outpatient EHR dataset and a lower-dimensional setting. The details can be found in Table~\\ref{table:VisitLevel}.\n\nFurthermore, an interactive visualization of 1000 randomly selected code-level disease prevalence comparisons between our method and real data is accessible at \\href{https://vega.github.io/editor/#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}{https://vega.github.io/}. It allows zooming, panning, and hovering over points for specific disease names.\nFinally, we provide chronic disease label probabilities, full visit level code probability plots, probability densities underlying the aggregated statistics, and a discussion of the various failure modes of our baseline methods for that evaluation in our supplementary information. \\method again outperforms the baseline methods in each evaluation.\\\\\n\n\\begin{table*}[]\n\\small\n\\centering\n\\caption{Aggregate statistics regarding the shape of training and compared synthetic datasets}\n\\resizebox{0.65\\textwidth}{!}{\n\\begin{threeparttable}\n\\begin{tabular}{c|cc|cc}\n\\toprule\n& \\multicolumn{2}{c|}{Outpatient EHR} & \\multicolumn{2}{c}{Inpatient EHR} \\\\\n& \\makecell{Record Length\\\\Mean (Std. Dev.)} & \\makecell{Visit Length\\\\Mean (Std. Dev.)}  & \\makecell{Record Length\\\\Mean (Std. Dev.)} & \\makecell{Visit Length\\\\Mean (Std. Dev.)}  \\\\ \\midrule\nEVA & 29.49 (28.88) & 3.35 (1.71) & 1.20 (0.723)  \n & 11.92 (3.665)            \\\\\nSynTEG   & 93.00 (2.30)   & 3.70 (4.10)   & 27.55 (3.34)   & 5.93 (10.96)  \\\\\nLSTM      & 32.04 (27.14)   & 3.22 (1.64)    & 1.30 (0.56)   & 9.53 (2.91) \\\\\nGPT      & 95.72 (3.37)   & 2.70 (1.73)    & 1.26 (0.73)   & 9.67 (5.45) \\\\\n\\method $-$ Coarse        & 35.26 (31.87)   & 3.77 (2.23)   & 1.13 (0.39)   & 11.21 (3.91) \\\\\n\\method      & 36.19 (33.41)   & 3.93 (2.72)   & 1.31 (0.84)   & 11.93 (6.45)  \\\\ \\hline\nTrain Data   & 34.18 (32.35)   & 3.52 (2.18)   & 1.27 (0.92)   & 11.68 (5.70) \\\\ \\bottomrule\n\\end{tabular}\n\\begin{tablenotes}[flushleft]\n\\item \\footnotesize Aggregate statistics on the number of visits per record and the number of codes per visit. The values are mean (std). \\method outperformed all the baselines while closely approximating the distribution of the true training data. Source data are provided as a Source Data file.\n\\end{tablenotes}\n\\end{threeparttable}}\n\\label{table:AggregateStats}\n\\end{table*}\n\n\\begin{table*}[]\n\\small\n\\centering\n\\caption{Code probability correlations $R^2$ between training and synthetic datasets}\n\\begin{threeparttable}\n\\begin{tabular}{c|ccc|ccc}\n\\toprule\n& \\multicolumn{3}{c|}{High-Dimensional Outpatient EHR} & \\multicolumn{3}{c}{Low-Dimensional Outpatient EHR} \\\\\n& \\makecell{Unigram\\\\Code Probabilities} & \\makecell{Sequential Visit\\\\Bigram Probabilities}  & \\makecell{Same Visit\\\\Bigram Probabilities} & \\makecell{Unigram\\\\Code Probabilities} & \\makecell{Sequential Visit\\\\Bigram Probabilities}  & \\makecell{Same Visit\\\\Bigram Probabilities}  \\\\ \\midrule\nEVA   & 0.910   & 0.082   & 0.128   & 0.957   & 0.134   & 0.225  \\\\\nSynTEG   & \\textbf{0.915}   & 0.355   & 0.082   & 0.784   & 0.315   & 0.211  \\\\\nLSTM   & 0.900   & 0.077   & 0.127   & \\textbf{0.962}   & 0.135   & 0.225  \\\\\nGPT   & 0.743   & 0.382   & 0.262   & 0.924   & 0.626   & 0.515  \\\\\n\\smaller \\method $-$ Coarse   & 0.794   & 0.357   & 0.176   & 0.882   & 0.503   & 0.247  \\\\\n\\method   & 0.914   & \\textbf{0.508}   & \\textbf{0.362}   & 0.949   & \\textbf{0.686}   & \\textbf{0.562}  \\\\ \\bottomrule\n\\end{tabular}\n\\begin{tablenotes}[flushleft]\n\\item The values are $R^2$ values to measure the correlations of the three types of code probabilities for different synthetic datasets against the training data in both high-dimensional and low-dimensional settings. Bold values denote the best results. Although the results showed a drop in performance for each method in the high-dimensional setting, \\method was able to maintain strong performance with minimal decline. Overall, our proposed method achieved state-of-the-art performance, outperforming the baselines in both bigram  evaluations in low and high dimensional settings. Source data are provided as a Source Data file.\n\\end{tablenotes}\n\\end{threeparttable}\n\\label{table:VisitLevel}\n\\end{table*}\n\n\\noindent{\\bf Key findings:} We observe that besides the GPT baseline struggling with the complexity of the outpatient EHR dataset in terms of stopping the record generation (as is common to many language models in the text generation domain as their overall quality decays for long sequences, and the lack of visit level grouping in its data representation causes its sequences to be considerably longer), the language model architectures (GPT, LSTM, \\method $-$ Coarse, and \\method) can model both the shape of the synthetic records and the temporal dependencies much better on average than the VAE and especially GAN-based baselines. While each of the compared methods models the unigram code probabilities relatively well, the temporal modeling is better shown in the overall synthetic record and visit lengths, the generation of chronic disease labels, and the sequential bigram evaluation. SynTEG, EVA, and the LSTM baseline thus struggle with these evaluations (with the LSTM baseline struggling largely due simply to overall weakness).\n\nThe LSTM and \\method $-$ Coarse language model baselines then falter with respect to same-visit bigram probabilities due to their lack of intra-visit dependency modeling while the GPT baseline which models each code individually and so offers that intra-visit modeling can maintain relatively stronger performance there. \\method can combine and build on each baseline's strengths without any weaknesses, using the compact multi-hot representation to offer a powerful model that does not struggle with any length or feature of data while simultaneously maintaining the intra-visit modeling in an even more powerful and structured way. As such, it can best maintain performance in this high-dimensional setting and produces state-of-the-art results that closely model the true training data in all settings from record and visit lengths, label probabilities, and all combinations of code probabilities. This signifies that \\method is capable of generating data that looks realistic.\n\n\\begin{figure*}\n    \\centerline{\\includegraphics[scale=0.7]{images/HALOPlotsPerRecord.pdf}}\n    \\caption{\\textbf{Code probability plots}. These plots show the Unigram, Sequential Visit Bigram, and Same Record Bigram probabilities for each synthetic dataset. With the exception of SynTEG, all models exhibit some correlation in the unigram and temporal bigram evaluations, but many have weak correlation or consistently yield higher synthetic probabilities due to a lack of temporal consistency and repetition across visits in the records. \\method and to a lesser extent, \\method $-$ Coarse perform the best in all settings, while \\method is the only one that can realistically produce pairs of codes within and across visits and achieve state-of-the-art results.}\n    \\label{fig:codeProbs}\n\\end{figure*}\n\n\\subsection*{Accurate Disease Phenotyping Using Synthetic EHRs} \\label{sec:supplementing}\nThe final evaluation explores the utility of the synthetic datasets for training disease classifiers. To this end, we utilize two different synthetically-supplemented data setups and machine learning classifiers to predict chronic disease labels based on patients' visits. In each of the two setups, we use a simple bidirectional LSTM with a single-layer fully connected head classifier to predict chronic disease label(s) based on a patient's visits. \\\\\n\n\\noindent\\textbf{Accurate Disease Phenotyping:} In the first data setup, we assess model performance in real-world scenarios using synthetic training data. We conduct experiments for each of the 11 chronic disease labels in the outpatient EHR dataset, sourced from the Centers for Medicare and Medicaid Services and the SynPUF dataset \\cite{SynPUF}. Additionally, we perform experiments for each of the 25 chronic diseases in the inpatient EHR dataset, based on the benchmark proposed in \\cite{MimicBenchmark}.\n\nFor each chronic disease, we randomly extract 2,500 records for training, ensuring a balanced distribution of positive and negative labels (50-50). This process is repeated across our six synthetic datasets (one for each method) and one real training dataset, resulting in a total of seven balanced training datasets. The selected size of 2,500 records strikes a balance between having enough training data for machine learning models and maintaining sufficient positive labels for each disease.\n\nWe train classifiers on each of these datasets and select the best model for each dataset using a validation set of 250 records, equally representing both classes. Finally, we evaluate the models on test sets consisting of 500 records, equally representing both classes, from the original test set comprising real patient data. \n\nWe display the average accuracy and F1 score for each synthetic dataset from each of the compared models as well as the real training data across each of the chronic disease labels in Table \\ref{table:SyntheticResults}. Note that we provide the standard deviations of each metric in either table as well, but most of that deviation stems from differences between tasks rather than inconsistent performance within each model.\n\nWe provide a full set of results by chronic disease label and also additional synthetically augmented outpatient results in our supplementary information. In both datasets, we can see that each synthetic data of GPT, \\method $-$ Coarse, and \\method largely maintain the performance of real data and offer large improvements over the SynTEG, EVA, and LSTM baselines. \\method's synthetic data offers the best prediction results .\\\\\n\n\\begin{table}[]\n\\centering\n\\caption{Chronic disease classification model performance trained on synthetic data}\n\\resizebox{0.95\\columnwidth}{!}{\n\\begin{threeparttable}\n\\begin{tabular}{c|cc|cc}\n\\toprule\n& \\multicolumn{2}{c|}{Outpatient EHR} & \\multicolumn{2}{c}{Inpatient EHR} \\\\\n& Avg. Accuracy  & Avg. F1 Score   & Avg. Accuracy  & Avg. F1 Score \n\\\\ \\midrule\nEVA & 0.508 $\\pm$ 0.02   & 0.283 $\\pm$ 0.26  \n        & 0.5356 $\\pm$ 0.05  & 0.580 $\\pm$ 0.05 \\\\\nSynTEG  & 0.507 $\\pm$ 0.03   & 0.514 $\\pm$ 0.20 \n        & 0.539 $\\pm$ 0.06  & 0.438 $\\pm$ 0.06 \\\\\nLSTM & 0.506 $\\pm$ 0.02   & 0.467 $\\pm$ 0.28 \n        & 0.522 $\\pm$ 0.04  & 0.565 $\\pm$ 0.04\\\\\nGPT & 0.851 $\\pm$ 0.03   & 0.854 $\\pm$ 0.03 \n        & \\textbf{0.877 $\\pm$ 0.05}  & \\textbf{0.881 $\\pm$ 0.05} \\\\\n\\method $-$ Coarse & 0.867 $\\pm$ 0.03   & 0.863 $\\pm$ 0.03  \n        & 0.863 $\\pm$ 0.05  & 0.865 $\\pm$ 0.05\\\\ \n\\method & \\textbf{0.879 $\\pm$ 0.03}  & \\textbf{0.878 $\\pm$ 0.03}    \n        & \\textbf{0.882 $\\pm$ 0.04}  & \\textbf{0.884 $\\pm$ 0.04} \\\\ \\hline\nReal Data & 0.891 $\\pm$ 0.03   & 0.895 $\\pm$ 0.03   \n        & 0.938 $\\pm$ 0.04 & 0.937 $\\pm$ 0.04 \\\\ \\bottomrule\n\\end{tabular}\n\\begin{tablenotes}[flushleft]\n\\item We compared the average performance in terms of accuracy and F1 Score for each of the 11 chronic disease labels in our outpatient dataset and 25 chronic disease labels in our inpatient dataset. The models were trained on each of our synthetic datasets and tested on real data. The reported values represent the mean and standard deviation across the tasks, with bold values indicating the best results. GPT, \\method $-$ Coarse, and \\method's data offer large improvements over the other baselines and perform similarly to real training data. \\method's synthetic data performs the best with the highest average performance of all synthetic methods. Source data are provided as a Source Data file.\n\\end{tablenotes}\n\\end{threeparttable}}\n\\label{table:SyntheticResults}\n\\end{table}\n\n\\noindent\\textbf{Phenotyping of Rare Conditions:} We evaluate the utility of synthetic EHR data in identifying uncommon conditions. We created a highly imbalanced dataset of patients labeled with cancer chronic disease from the outpatient EHR dataset. The dataset comprised 50,000 EHR records without the cancer chronic disease label and only 1,000 records with the label.\n\nUsing this imbalanced data, we trained a classifier and compared its performance to classifiers trained on balanced datasets. For balancing, we added 49,000 positively labeled synthetic records and also used another classifier trained on a dataset balanced using real records.\n\nThe evaluation results are summarized in Table \\ref{table:RareDiseaseResults}. Notably, \\method outperformed all baselines, exhibiting significant improvements on the original unbalanced dataset as well as the synthetically augmented datasets. It approached the upper bound performance of the ideal balanced dataset.\n\nThis experiment underscores the potential of synthetic EHR data in supporting the identification of uncommon conditions. %It highlights the value of using balanced data for training classifiers, with \\method showcasing superior performance.\n\n\\begin{table}[]\n\\small\n\\centering\n\\caption{Rare disease detection performance on synthetic balanced datasets}\n\\begin{threeparttable}\n\\begin{tabular*}{0.9\\columnwidth}{c|ccccccc}\n\\toprule\n&\\footnotesize BCE Loss  &\\footnotesize Accuracy &\\footnotesize F1 Score  &\\footnotesize AUROC     \n\\\\ \\midrule\n\\footnotesize Original imbalanced & 0.693  & 0.497  & 0.013  & 0.417 \\\\\nBalanced with real data  & 0.127  & 0.951  & 0.951  & 0.989 \\\\ \\hline\nEVA & 0.615  & 0.695  & 0.705  & 0.730 \\\\\nSynTEG & 0.598  & 0.735  & 0.758  & 0.786 \\\\\nLSTM & 0.593  & 0.702  & 0.714  & 0.743 \\\\\nGPT & 0.472  & 0.880  & 0.869  & 0.956 \\\\\n\\method $-$ Coarse & 0.265  & 0.918  & 0.916  & 0.959 \\\\\n\\method   & \\textbf{0.192} & \\textbf{0.931}  & \\textbf{0.931} & \\textbf{0.976} \\\\ \\bottomrule\n\\end{tabular*}\n\\begin{tablenotes}[flushleft]\n\\item We present the classification results on the test set for the simulated rare-disease detection task. We compare models trained on datasets balanced using each synthetic dataset against models trained on the original imbalanced data (representing the rare disease dataset). Additionally, we compare the results against an upper-bound ideal dataset balanced using real data. The best results are highlighted in bold.\nAmong the evaluated models, EVA and SynTEG exhibit limited utility, while the language model architectures LSTM, GPT, and \\method $-$ Coarse offer substantial value. \\method achieves state-of-the-art performance, closely approaching the results of a true, balanced dataset. The source data can be found in the provided Source Data file.\n\\end{tablenotes}\n\\end{threeparttable}\n\\label{table:RareDiseaseResults}\n\\end{table}\n\n\\subsection*{Realistic Continuous Variables in Synthetic EHRs}\n\\label{method:generate-lab}\nWe conclude with a brief exploration to demonstrate the viability of our discretized representation of continuous values, and \\method's effectiveness in using it to model those variables. We build new training datasets including visit gaps in the outpatient EHR dataset and lab values in the inpatient EHR dataset. We use these dataset to train a new version of our model and generate another synthetic dataset of 250,000 and 45,000 records respectively. \n\nWe then show that the distributions of those variables match the real values. In Figure \\ref{fig:ContinuousStats}a and Figure \\ref{fig:ContinuousStats}b, we show that \\method accurately replicates the distribution of gaps between patient visits and the pattern of shorter gaps for longer records, respectively. These captured nuanced patterns are on top of the aggregate mean gaps being very similar as well. There are 33.53 days between visits on average within the real outpatient EHR data and 35.77 days on average for \\method's synthetic data.\n\nUsing the inpatient dataset, we then demonstrate that \\method replicates not only the presence (in Figure \\ref{fig:ContinuousStats}c) but also the average values (in Figure \\ref{fig:ContinuousStats}d) of performed lab tests. Specific labs included (corresponding to points in those two plots) are included in our supplementary information. Overall, \\method's approach to continuous variables is effective, and it has the potential to generate comprehensive synthetic patient records with multiple variables of different types.\n\n\\begin{figure*}\n        \\centerline{\\includegraphics[scale=0.35]{images/ContinuousValues.pdf}}\n    \\caption{Continuous Variable Generation Performance: \\method effectively captures the distribution of continuous variables through its discretization approach, as demonstrated in four scenarios.\na) Inter-visit gap probability density: The probability density of inter-visit gaps indicates that \\method closely approximates the true shape of real data.\nb) Inter-visit gap by visit number: The mean visit gap, as per visit number, across both real and synthetic datasets reveals that \\method accurately captures the pattern of patients with many records, showing shorter gaps in their subsequent visits.\nc) Lab presence probabilities: The probability of binary lab presence demonstrates that \\method accurately generates lab variables, even when discretized across multiple variables.\nd) Mean lab values: The average value of labs, when present, confirms that \\method's synthetic labs closely resemble those of the real dataset. Values in parentheses are $R^2$. }\n    \\label{fig:ContinuousStats}\n\\end{figure*}\n\n\\subsection*{Privacy Evaluation of Synthetic EHRs} \\label{sec:securing}\nIn addition to demonstrating the high fidelity of synthetic EHRs generated by \\method, we want to ensure that the privacy of the patients within the original training dataset is protected. To that end, we conduct a commonly used membership inference attack to test its identification risk, and we provide the results of two more evaluations in our supplementary information. %Our results show that the performance of \\method is not due to memorization or any other violation of patient privacy.\\\\\n\n\\noindent\\textbf{Membership Inference Attack:} The evaluation is the ability to thwart a membership inference attack. These attacks aim to determine whether any specific real patient record was used in the training dataset to generate the synthetic records. Membership inference attacks are a well-known privacy test in the field of synthetic EHR generation, and addressing them is crucial to ensure the privacy and confidentiality of patient identities.\n\nTo demonstrate that \\method is not susceptible to such an attack, we show that we can prevent two different attempts at a membership inference attack based on the synthetic data generator and the synthetic dataset itself. We generate an attack dataset by first selecting 100,000 records from each real dataset used for training and assigning them a positive label. Then we select 100,000 records from the remaining records not used for training as the negative label set.\n\nNext, we conduct two attacks: \n\\begin{itemize}\n\\item \nIn the Model Attack, we label the 100,000 records with the highest log probability from the model as positive, predicting that they were part of the training dataset.\n\\item In the Dataset Attack, we label the 100,000 records with the lowest hamming distance to the closest record in synthetic dataset as positive. We pick hamming distance (equivalent to Manhattan Distance in our binary setting) as our distance metric between patient records throughout our privacy evaluations in accordance with \\cite{yan2022multifaceted}, but any distance metric could be substituted interchangeably.\n\\end{itemize}\nThese two attacks allow us to test the ability of the synthetic dataset to prevent an attacker from inferring whether a real record was used in the training dataset.\n\nWe show the results of the classifications from the attacks in Table \\ref{table:membershipAttack}. The accuracy of both attacks on both datasets is approximately 50\\%, which is similar to a random guess. This shows that neither the model nor the synthetic dataset reveals any meaningful or compromising information about the patient identity in the training dataset. We also perform the dataset attack with each of our baseline datasets and see that each similarly accomplishes it, achieving a similar probability at around 50\\%. \nNote that we do not perform the model attack with the baseline models because most of them cannot offer a probability output of input patient records, and the dataset-based attack is the standard one used throughout literature in this domain.\\\\\n\n\\begin{table}[]\n\\centering\n\\caption{Membership inference attack results}\n\\resizebox{0.95\\columnwidth}{!}{\n\\begin{threeparttable}\n\\begin{tabular}{c|ccc|ccc}\n\\toprule\n& \\multicolumn{3}{c|}{Outpatient EHR} & \\multicolumn{3}{c}{Inpatient EHR} \\\\\n&\\footnotesize Acc.       &\\footnotesize Precision     &\\footnotesize Recall  %&\\footnotesize F1 Score \n&\\footnotesize Acc.       &\\footnotesize Precision     &\\footnotesize Recall  %&\\footnotesize F1 Score\n\\\\ \\midrule\n\\method Dataset Attack   & 0.501   & 0.501   & 0.501  \n& 0.492   & 0.491   & 0.477 \\\\\n\\method Model Attack     & 0.509    & 0.509   & 0.509\n& 0.515    & 0.515   & 0.515 \\\\\\hline\nEVA Dataset Attack    & 0.498    & 0.498   & 0.496\n& 0.493    & 0.493   & 0.477    \\\\\nSynTEG Dataset Attack    & 0.500    & 0.500   & 0.500\n& 0.491    & 0.491   & 0.467    \\\\\nLSTM Dataset Attack    & 0.499    & 0.499   & 0.496\n& 0.494    & 0.494   & 0.481    \\\\\nGPT Dataset Attack    & 0.500    & 0.500   & 0.500\n& 0.492    & 0.491   & 0.455    \\\\\n\\method $-$ Coarse Dataset Attack    & 0.500    & 0.500   & 0.499\n& 0.491    & 0.491   & 0.462 \n\\\\ \\bottomrule\n\\end{tabular}\n\\begin{tablenotes}[flushleft] \n    \\item For each record in the attack dataset, we find both the log probability of the record from the trained model (Model Attack) and the hamming distance to the closest record in the synthetic dataset (Dataset Attack). The attacks then label the half of the records with the highest probability or lowest distance records, respectively, as in the training set. We see that the accuracy for either attack is right around 50\\%, which is similar to a random guess. This indicates that the synthetic dataset and the model do not reveal any patient-identifying information about the original training datasets. We also find that each baseline synthetic dataset similarly thwarts the dataset attack. Source data are provided as a Source Data file.\n\\end{tablenotes}\n\\end{threeparttable}}\n\\label{table:membershipAttack}\n\\end{table}\n\nBeyond membership inference attack, we also show that \\method passes attribute inference attack and nearest neighbor adversarial accuracy \\cite{yale2020generation} evaluations in our supplementary information.% We thus show that \\method's impressive performance does not come at the expense of patient privacy.\n\n\\section{Discussion}\nIn this paper, we proposed a method \\method for generating high-dimensional synthetic longitudinal EHR data. Our method is specifically designed to handle the sequential, multi-granular, and high-dimensional nature of electronic health records by generating an explicit probability distribution over the codes, visits, and records, and \\method can generate realistic data so without needing to aggregate or remove any codes as past approaches have unanimously done. We then showed that \\method can produce incredibly realistic synthetic EHR data. Specifically, we showed that \\method can capture the probability distribution underlying the records better than other language model baselines and then produce a synthetic dataset that both looks similar to and offers the utility of real patient records as measured by medical code occurrence probabilities and machine learning classification tasks augmented with synthetic data. Finally, we also show that our method offers this performance without compromising privacy through several privacy evaluations. \n\nIn conclusion, one of the key advantages of \\method is its ability to generate binary sequences that are over a million variables in length. Its impressive performance makes it a promising avenue for developing and sharing realistic but synthetic EHR datasets that can support diverse applications. This represents an exciting opportunity to expand the use of synthetic data in the healthcare field and could help to address some of the challenges associated with data privacy and security.\n\nWhile we have shown the impressive performance of \\method in both producing high-quality, high-fidelity, and privacy-preserving, we now briefly discuss some remaining limitations. First, the architecture is designed in the model of a large language model. While the multi-modal setup allows the model to condition on more patterns per data point and learn more efficiently, our high-performing generator still requires relatively large training datasets which might not be available in some settings. \n\nAnother important aspect of our model is that it generates synthetic records through a probabilistic process. While it learns real-world patterns during training, there is still a chance that some generated records may not be clinically meaningful. However, this risk can be mitigated through postprocessing with clinical rules that validate the synthetic records. If our model is deployed in the real world, it is important to consider implementing such postprocessing steps to ensure that only clinically relevant synthetic records are produced.\n\nFinally, our \\method model focuses on generating longitudinal EHR data, such as medical codes and lab results. However, other crucial data modalities, such as clinical notes and medical images, are not yet covered by the model. To generate fully comprehensive patient records that include all modalities, it will be necessary to use diverse training data and develop multiple models to handle each modality. This exciting avenue of research is a promising future direction.\n\n\\section{Methods}\nOur study is acquired exempt status from Institutional Review Board (IRB) approval. This study has been found to be exempt pursuant to 45CFR46.104(d)(4) \"Secondary research for which consent is not required: Secondary research uses of identifiable private information, if (i) The identifiable private information is publicly available; AND (ii) Information is recorded by the investigator in such a manner that the identity of the human subjects cannot readily be ascertained directly or through identifiers linked to the subjects, the investigator does not contact the subjects, and the investigator will not re- identify subjects.\"\n\n\\subsection*{Background and Related Work}\nOf all the EHR generation methods, rule-based approaches, such as Synthea \\cite{Synthea} or SynPUF~\\cite{SynPUF}, have proven to be the most effective in delivering practical value. These simple approaches either offer de-identification of real records by combining data across multiple patients in a sufficiently privacy-preserving way \\cite{SynPUF}, simulation of patients within a complex yet constrained rule-based system \\cite{Synthea}, Bayesian probabilistic modeling of aggregated, non-temporal patient records \\cite{Bayesian}, or proprietary method without detailed explanation \\cite{EMRBots, MDClone, Syntegra}. Many of these systems can only produce synthetic patient data with limited capacity in realism and utility. We focus instead on ML methods that have the potential to generate realistic high-dimensional synthetic patient data.\\\\\n\n\\noindent\\textbf{GAN-based Methods}\nMany synthetic data generation methods use Generative Adversarial Networks (GANs), which involve  a generator that creates realistic data, and a discriminator that decides if the data is real or fake~\\cite{GANs}.\nThe GANs has been applied to patient record generation first in \\cite{MedGAN} followed by many other GAN-based approaches \\cite{MedWGAN, EMR-WGAN, HGAN, CorGAN, LongGAN, SmoothGAN, CONAN, EHRMGAN, SynTEG}. However, GANs have limitations when generating sequential data like EHRs. They usually only produce one output (no time connections) and so most EHR generation methods aggregate EHR data into one time step \\cite{EMR-WGAN, HGAN, SmoothGAN}, create a representation of EHR data \\cite{CONAN}, or do both \\cite{MedGAN, MedWGAN}.\n\nGANs also struggle with high dimensional and sparse data like real-world EHR, limiting all existing synthetic EHR GAN approaches to produce relatively low dimensional data through the aggregation of visits and medical codes or removal of rare codes. For example, there are a few methods in this category which do generate longitudinal data. LongGAN \\cite{LongGAN} and EHR-M-GAN \\cite{EHRMGAN} both focus only on dense lab time series of under a hundred dimensions. CorGAN \\cite{CorGAN} generates records with 1,071 distinct codes, and the current state of the art GAN approach that we baseline against, SynTEG \\cite{SynTEG}, both combines and removes rare codes before arriving at a final dimensionality of 1,276.\n\nWhile GANs have the potential to be conditioned on external factors and labels, such as demographics or disease phenotype labels, the ability to do so has not been extensively explored in existing works on EHR generation. Moreover, there are only a limited number of approaches that can generate synthetic EHR data tailored to specific diseases. For example, SmoothGAN \\cite{SmoothGAN} focuses on aggregated lab and medication information and does not model individual visits; EHR-M-GAN \\cite{EHRMGAN}  offers conditional and sequential capabilities, but for low dimensional (under 100 dimensions) lab time-series information; CONAN and MaskEHR \\cite{CONAN, MaskEHR} model only a single rare-disease population for data augmentation; and EMR-WGAN and HGAN \\cite{EMR-WGAN, HGAN} can only model low-dimensional (both under 1000 dimensions) aggregated EHRs.\\\\\n\n\\noindent\\textbf{Deep Sequential Methods}\nAccurately modeling the longitudinal nature of EHRs is crucial for realistic EHR generation. In recent years, two methods have shown progress in generating sequential EHRs by using either a GAN or a VAE to condition on representations of past patient visits to generate current visits \\cite{SynTEG, EVA}. Specifically, SynTEG \\cite{SynTEG} models the time between visits, and EVA \\cite{EVA} offers a conditional variant. In our experiments, we compare \\method to these two models. However, both SynTEG and EVA often need to perform preprocessing steps to reduce the dimensionality of the vocabulary by aggregating medical codes and removing rare codes.\\\\\n\n\\noindent\\textbf{Language Models}\nOur objective is to develop an improved method for generating realistic and high-dimensional EHR data by drawing inspiration from natural language generation. Language generation models predict the next word based on the preceding words, thereby learning a probability distribution of languages. Similarly, EHR models predict the next visit based on past visits. Also our proposed method  provides an explicit probability output that allows for direct modeling and evaluation of the underlying data distribution. This approach is particularly beneficial in accurately capturing the complex and high-dimensional nature of EHR data.\n\nThe Transformer architecture, introduced in \\cite{Attention}, has revolutionized natural language processing and enabled the development of large, attention-based models like BERT \\cite{BERT} and GPT \\cite{GPT1, GPT2, GPT3}. Among these models, we draw inspiration from GPT, which relies on a stack of Transformer decoder blocks that use masking to predict the next set of probabilities in parallel, allowing for fast training and scalability.\nHowever, applying language models directly to EHR data poses unique challenges. Unlike natural language sequences, EHR data exhibits a hierarchical structure that must be captured, with medical codes associated with specific patient visits, and visits associated with individual patients. Additionally, EHR data contains heterogeneous elements, including demographic variables, structured medical codes, and numeric lab measures, not all of which are discrete tokens. Addressing these challenges requires approaches that leverage the strengths of language models while adapting them to the peculiarities of EHR data.\n\n\\subsection*{Hierarchical Autoregressive Language Model (\\method)}\nWe model the probability of patient record  $\\mathbf{R}$, $P(\\mathbf{R})$, via a hierarchical autoregressive model, which utilizes both visit- and code-level structures of a patient record. First, it factorizes the probability along the visit level using the autoregressive identity by\n\\begin{align}\\begin{split}\n    P(\\mathbf{R}) &= P(\\mathbf{v}_s, \\mathbf{v}_l, \\cdots, \\mathbf{v}_T, \\mathbf{v}_e) \\\\\n           &= P(\\mathbf{v}_s)P(\\mathbf{v}_l|\\mathbf{v}_s)P(\\mathbf{v}_1|\\mathbf{v}_s,\\mathbf{v}_l) \\cdots P(\\mathbf{v}_{e}|\\mathbf{v}_s, \\mathbf{v}_l, \\cdots, \\mathbf{v}_{T}) \\\\\n\\end{split}\\end{align} \nto produce what we call our coarse autoregressive sequence. We then continue to factorize the probability of visits further along the code level by converting\n\\begin{align}\\begin{split}\n    P(\\mathbf{v}_t|\\mathbf{v}_s, \\mydots\\,, \\mathbf{v}_{t-1}) &=  P(c_t^1|\\mathbf{v}_s, \\mydots\\,, \\mathbf{v}_{t-1}) P(c_t^2|\\mathbf{v}_s, \\mydots\\,, \\mathbf{v}_{t-1},c_t^1)\\\\\n           &\\;\\;\\;\\;\\mydots\\, P(c_t^C|\\mathbf{v}_s, \\mydots\\,, \\mathbf{v}_{t-1}, c_t^1, \\mydots\\,, c_t^{C-1})\n\\end{split}\\end{align} \ninto what we call our fine autoregressive sequence. This final probability is then rewritten as the product\n\\begin{equation}\\label{eq:probability}\n    P(\\mathbf{R}) = \\prod_t \\prod_i^C P(c_t^i|\\mathbf{v}_s, \\cdots, \\mathbf{v}_{t-1}, c_t^1, \\cdots, c_t^{i-1})\n\\end{equation}\nwhere the probability of each code is based on each of the previous visits and each of the previous codes in the current visit. Our multi-granularity approach enables the modeling of high-dimensional sequences of many binary variables per record. This is achieved by grouping prior information into significantly fewer multivariate time steps for previous visits, while retaining the full autoregressive modeling capability for each current visit.\nOur \\method architecture is designed to reflect this powerful yet compact model, with a powerful and efficient structure divided into two distinct granularity levels: visit level and code level. This allows for each code to be conditioned on all previous visits and the past codes of the current visit.\\\\\n\n\\noindent\\textbf{Visit-Level Module}\nWe begin with the coarse, visit-level granularity. We use a stack of $M$ transformer decoder blocks, which have shown to be effective in the high-dimensional domain of natural language processing, to generate a sequence of visit-level histories, where the $t$-th element in the sequence, $\\mathbf{h}^{(M)}_t \\in \\mathbb{R}^{n_{\\text{emb}}}$, is an embedding that represents all of a patient's medical history through their $t$-th visit. Those histories then combine to form $\\mathbf{H}^{(M)} \\in \\mathbb{R}^{(T+3) \\times n_{\\text{emb}}}$ (where the $3$ in $T+3$ includes the start, label, and end visits), the output of the first module which serves of the purpose of the $\\mathbf{v}_s, \\mathbf{v}_l, \\mathbf{v}_1, \\cdots \\mathbf{v}_{t-1}$ priors in Equation \\ref{eq:probability}.\n\nTo encode each of the multi-hot visit representations $[\\mathbf{v}_1 \\cdots \\mathbf{v}_n]$ into a fixed-length vector in $\\mathbb{R}^{n_\\text{emb}}$, we employ an embedding layer that includes two trainable parameter matrices: a code embedding matrix $\\mathbf{W}_c$ and a positional embedding matrix $\\mathbf{W}_p$. The code embedding matrix maps each visit code to a dense vector representation, while the positional embedding matrix captures the relative position of each visit in the sequence. Next, we use a decoder model consisting of $M = 12$ transformer decoder blocks to generate a series of visit history representations, which summarize the information contained in all previous visits in the coarse, visit-level sequence. The transformer decoder blocks employ masked multi-head self-attention, which allows the model to attend to all previous visits while preventing information leakage from future visits.\nThis process is written more formally as\n\\begin{align}\\begin{split}\n    \\mathbf{H}^{(0)} &= \\mathbf{R}\\mathbf{W}_e + \\mathbf{W}_p \\\\\n    \\mathbf{H}^{(m)} &= \\text{transformer\\_block}(\\mathbf{H}^{(m-1)}) \\;\\;\\;\\forall \\:m \\in [1,M]\n\\end{split}\\end{align}\nwhere $\\mathbf{R} \\in \\mathbb{R}^{(T+3) \\times C}$ is the patient record matrix representation, $\\mathbf{W}_e \\in \\mathbb{R}^{C \\times n_{\\text{emb}}}$ is the code embedding matrix, $\\mathbf{W}_p \\in \\mathbb{R}^{(T+2) \\times n_{\\text{emb}}}$ is the positional embedding matrix (to recapture the position and order of the sequence of visits), and each transformer block is based on a decoder block from the original transformer architecture \\cite{Attention} which we describe in more detail in our supplementary information.\n\nThus, having processed the multi-hot patient visits through the initial, coarse visit-level module of our architecture, we obtain a sequence of visit history representations $\\mathbf{H}^{(M)}$, which capture the collective information of all previous visits up to each time step. These representations provide a compressed summary of the patient's visit history, enabling downstream modules to make predictions based on the patient's medical trajectory.\\\\\n\n\\noindent\\textbf{Code-Level Module}\nHowever, we still need to add in the code-level priors and generate output probabilities.\nTo construct the input for the fine, code-level module, we offset and concatenate the previous module's visit history embedding outputs with the original record input, $\\mathbf{R}$. Specifically, we append the first $T+2$ visit histories with the last $T+2$ visit representations $[\\mathbf{v}_l, \\mathbf{v}_1, \\cdots, \\mathbf{v}_{T}, \\mathbf{v}_e]$ to create $\\mathbf{H}'^{(0)}$. Each of the $T+2$ inputs in $\\mathbf{H}'^{(0)}$ has a representation of the history of all the previous visits and the codes of the current visit, mirroring both the visit and code priors in Equation \\ref{eq:probability}. The final input representation $\\mathbf{H}'^{(0)}$ has size $\\mathbb{R}^{(T+2) \\times (n_{\\text{emb}}+C)}$\n\nTo model the distribution of each $P(c_t^i)$, this $\\mathbf{H'}^{(0)}$ is then fed through $N = 2$ masked linear layers which maintain the same dimensionality and use upper triangular masking of the weight matrix to ensure that they preserve the autoregressive property of the probabilities (and have a ReLU activation function between layers). These linear layers are able to efficiently model the high-dimensional, intra-visit patterns where other sequential approaches such as additional recurrent or transformer modules would run out of memory. The probabilities are generated formally by\n\\begin{align}\\begin{split}\n    \\mathbf{H'}^{(0)} &= \\text{offset\\_and\\_concat}(\\mathbf{H}^{(M)}, \\mathbf{R}) \\\\\n    \\mathbf{H'}^{(n)} &= \\text{masked\\_linear}(\\mathbf{H'}^{(n-1)}) \\;\\;\\;\\forall \\:n \\in [1,N] \\\\\n    \\mathbf{O} &= \\text{sigmoid}(\\mathbf{H'}^{(N)}[\\,:\\;,\\,n_{\\text{emb}}:\\,])\\\\\n\\end{split}\\end{align}\nwhere the submatrix indexing at the end removes the visit-level history embedding portions of each vector to extract just the code probabilities, and the masked linear layers are achieved by\n\\begin{align}\\begin{split}\n    \\mathbf{H'}^{(n)} &= \\max(0,\\mathbf{H'}^{(n-1)}(\\mathbf{W}^{(n)} \\odot \\mathbf{M}) + \\mathbf{b}^{(n)})\n\\end{split}\\end{align}\nwhere the $\\max$ function is omitted for the final fine layer (sigmoid is used instead), $\\odot$ is element-wise matrix multiplication, $\\mathbf{M} \\in \\mathbf{R}^{(n_{\\text{emb}}+C) \\times (n_{\\text{emb}}+C)}$ is the upper triangular masking matrix (with ones in the upper triangular portion and zeros in the lower portion) to preserve the autoregressive property, and $\\mathbf{W}^{(n)} \\in \\mathbb{R}^{(n_{\\text{emb}}+C)\\times(n_{\\text{emb}}+C)}$ \nand $\\mathbf{b}^{(n)} \\in \\mathbb{R}^{n_{\\text{emb}}+C}$ are the trainable parameters of the module.\n\nThe output $\\mathbf{O} \\in \\mathbb{R}^{(T+2) \\times C}$ is then a matrix of probabilities of each code for each visit after the start visit built from the visit histories and each previous code in the same visit.  Each code corresponds to a conditional probability in the product from Equation \\ref{eq:probability}. %which composes $P(\\mathbf{R})$ and completes our \\method architecture. \n\nWe train our model using the binary cross-entropy loss function over each medical code (treating the problem as a multi-label classification problem) with masking applied such that the start visit as well as any padded visits (of all zeros) do not contribute to the loss. The architecture of our model is shown in Figure \\ref{fig:MediSynArchitecture}.\n\n\\subsection*{Additional Features and Considerations}\nFinally, We discuss different variants and add-on features of \\method. \\\\\n\n\\noindent\\textbf{Conditional Generation}\nOur method generates electronic health record (EHR) data by using demographics $\\mathcal{S}$ and chronic disease phenotypes $\\mathcal{D}$ as labels, which are represented in our label vocabulary and applied to individual visits, as shown in Figure \\ref{fig:DataRepresentation}. We selected these labels based on their relevance to downstream use cases. Each label is represented as a binary variable in $\\mathbf{v}_l$, indicating the presence of the corresponding disease or demographics group indicator. These indicators are defined by concepts such as specific categories of genders, races, ethnicity, age groups, and more. We can easily extend this strategy to include other labels of interest, such as various biomarkers, patient outcomes, or even abstract patient embeddings.\\\\\n\n\\noindent\\textbf{Unconditional Generation}\nOur setup generates electronic health record (EHR) data with conditional labels by incorporating a \"label visit\" in the data format, as illustrated in Figure \\ref{fig:DataRepresentation}. This format enables easy generation of labeled and conditional data, which are highly valuable for using synthetic data in machine learning tasks and as an augmentation tool, particularly for rare cohorts. However, it's important to note that this formatting is optional. If desired, the \"label visit\" component can be removed from the EHR representation, and the architecture can be trained to generate unconditioned EHRs without any modification.\\\\\n\n\\noindent\\textbf{Generation of Continuous Variables}\nOur model can generate not only medical codes  but also continuous variables, such as lab values and temporal gaps between visits. However, the availability of these additional variables in the generated data depends on their presence in the original dataset used for training. For example, the outpatient EHR dataset used in our study includes the time between visits, while the inpatient EHR dataset includes lab values.\n\nIn previous models, continuous values were typically generated using either GANs, which lack the autoregressive probabilistic modeling that we employ, or value predictors (such as time series analysis models), which we often found to produce average values with insufficient variance. To overcome these limitations, we model continuous variables within the healthcare domain by discretizing lab values and temporal gaps into clinically equivalent buckets. The resulting binary variables are included in the model's context, denoted as $\\mathcal{C}$, before being converted back to continuous values through random uniform sampling within the corresponding bucket range. By using this approach, our model generates more realistic and diverse continuous variables than previous methods.\n\nMore specifically, to generate discrete versions of continuous variables, such as lab values and temporal gaps, we divide the range of each variable into several \"buckets\", as represented by the values $b_1, b_2, \\cdots, b_{|l^{(t)}_j|}$, where $|l^{(t)}_j|$ refers to the number of buckets required. We determine the bucket ranges by either seeking advice from clinicians on practical ranges,  creating granular but equivalent groupings, or using a histogram construction algorithm \\cite{guha2006approximation}. The same approach is applied to temporal gaps as well.\n\nFor example, the heart rate lab test with possible values ranging from 0 to 400 beats per minute down could be broken down into twenty different buckets splitting the overall span into smaller ranges which offer the same medical meaning for all their contained values. This breakdown could have $b_1 = (0,40)$ and $b_7 = (90, 100)$. These buckets then convert the single continuous variable into many binary variables. Whenever the continuous variable is present in the original EHR, a single one of those variable representing the corresponding bucket is set to 1 with the rest remaining 0. For instance, if a patient has a heart rate lab measurement of 93 bpm in their seventh visit, the seventh of the new heart rate variables would be 1 and the rest would remain 0. If there was no such lab measurement in the visit, they would all be 0. \n\nThese new binary variables are added into the wider code vocabulary $\\mathcal{C}$ and treated in the same way as all of the other medical codes in the vocabulary by our \\method model during learning and generation. After generation, the specific lab values and inter-visit gaps are converted back into a continuous value by uniformly sampling from the corresponding bucket range at the very end.\n\nThis discretization allows us to maintain the same powerful and probabilistic modeling process, matching the probabilistic variance of real continuous values in the same way we match the variance of medical code presences. However, by building appropriately granular buckets, we can avoid losing  meaningful information and maintain a full representation of a patient. We explore the performance of this approach further in our experiments.\n\n\\section{Data Availability}\nThe MIMIC-III inpatient EHR dataset \\cite{MIMIC} that we use is publicly available and may be downloaded and used freely after performing training and applying on \\url{physionet.org}. Furthermore, we also released the synthetic data for each of our compared methods for both the inpatient and outpatient datasets at \\url{https://figshare.com/articles/dataset/HALO_Synthetic_Data/23811162}. % {https://figshare.com/articles/dataset/HALO_Synthetic_Data/23811162/}. \nThese datasets can then be used to reproduce the results and data statistics. % that do not rely on the real dataset. %Finally, the original outpatient claims EHR dataset is available from the providing clinical trial company, but restrictions apply to the availability of the data, which are proprietary to the company and so not publicly available. Data are however available from the authors upon reasonable request and with permission of that company. Please contact the corresponding author for more information. Source data of tables and figures are provided with this paper. Source data are provided with this paper.\n\n\\section{CODE AVAILABILITY}\nWe make our code for the inpatient dataset experiments, including dataset construction, modeling building, training, and evaluation, available at \\url{https://github.com/btheodorou99/HALO_Inpatient} \\cite{btheodorou99_2023_8041405}. Between this and public availability of that dataset, all inpatient results can be fully reproduced. Furthermore,  \\method  is also included in the open-source machine learning package for healthcare PyHealth \\cite{pyhealth2022github}, where it is available for easy use in concert with various machine learning tasks.\n\n\\begin{thebibliography}{10}\n\\expandafter\\ifx\\csname url\\endcsname\\relax\n  \\def\\url#1{\\texttt{#1}}\\fi\n\\expandafter\\ifx\\csname urlprefix\\endcsname\\relax\\def\\urlprefix{URL }\\fi\n\\providecommand{\\bibinfo}[2]{#2}\n\\providecommand{\\eprint}[2][]{\\url{#2}}\n\n\\bibitem{GRAM}\n\\bibinfo{author}{Choi, E.}, \\bibinfo{author}{Bahadori, M.~T.},\n  \\bibinfo{author}{Song, L.}, \\bibinfo{author}{Stewart, W.~F.} \\&\n  \\bibinfo{author}{Sun, J.}\n\\newblock \\bibinfo{title}{Gram: graph-based attention model for healthcare\n  representation learning}.\n\\newblock In \\emph{\\bibinfo{booktitle}{Proceedings of the 23rd ACM SIGKDD\n  international conference on knowledge discovery and data mining}},\n  \\bibinfo{pages}{787--795} (\\bibinfo{year}{2017}).\n\n\\bibitem{RETAIN}\n\\bibinfo{author}{Choi, E.} \\emph{et~al.}\n\\newblock \\bibinfo{title}{Retain: An interpretable predictive model for\n  healthcare using reverse time attention mechanism}.\n\\newblock \\emph{\\bibinfo{journal}{Advances in neural information processing\n  systems}} \\textbf{\\bibinfo{volume}{29}} (\\bibinfo{year}{2016}).\n\n\\bibitem{MonitoringBook}\n\\bibinfo{author}{Farrar, C.~R.} \\& \\bibinfo{author}{Worden, K.}\n\\newblock \\emph{\\bibinfo{title}{Structural health monitoring: a machine\n  learning perspective}} (\\bibinfo{publisher}{John Wiley \\& Sons},\n  \\bibinfo{year}{2012}).\n\n\\bibitem{REST}\n\\bibinfo{author}{Duggal, R.}, \\bibinfo{author}{Freitas, S.},\n  \\bibinfo{author}{Xiao, C.}, \\bibinfo{author}{Chau, D.~H.} \\&\n  \\bibinfo{author}{Sun, J.}\n\\newblock \\bibinfo{title}{Rest: Robust and efficient neural networks for sleep\n  monitoring in the wild}.\n\\newblock In \\emph{\\bibinfo{booktitle}{Proceedings of The Web Conference\n  2020}}, \\bibinfo{pages}{1704--1714} (\\bibinfo{year}{2020}).\n\n\\bibitem{DDL}\n\\bibinfo{author}{Fu, T.}, \\bibinfo{author}{Hoang, T.~N.},\n  \\bibinfo{author}{Xiao, C.} \\& \\bibinfo{author}{Sun, J.}\n\\newblock \\bibinfo{title}{Ddl: Deep dictionary learning for predictive\n  phenotyping}.\n\\newblock In \\emph{\\bibinfo{booktitle}{IJCAI: proceedings of the conference}},\n  vol. \\bibinfo{volume}{2019}, \\bibinfo{pages}{5857}\n  (\\bibinfo{organization}{NIH Public Access}, \\bibinfo{year}{2019}).\n\n\\bibitem{che2017deep}\n\\bibinfo{author}{Che, Z.} \\& \\bibinfo{author}{Liu, Y.}\n\\newblock \\bibinfo{title}{Deep learning solutions to computational phenotyping\n  in health care}.\n\\newblock In \\emph{\\bibinfo{booktitle}{2017 IEEE International Conference on\n  Data Mining Workshops (ICDMW)}}, \\bibinfo{pages}{1100--1109}\n  (\\bibinfo{organization}{IEEE}, \\bibinfo{year}{2017}).\n\n\\bibitem{GAMENET}\n\\bibinfo{author}{Shang, J.}, \\bibinfo{author}{Xiao, C.}, \\bibinfo{author}{Ma,\n  T.}, \\bibinfo{author}{Li, H.} \\& \\bibinfo{author}{Sun, J.}\n\\newblock \\bibinfo{title}{Gamenet: Graph augmented memory networks for\n  recommending medication combination}.\n\\newblock In \\emph{\\bibinfo{booktitle}{proceedings of the AAAI Conference on\n  Artificial Intelligence}}, vol.~\\bibinfo{volume}{33},\n  \\bibinfo{pages}{1126--1133} (\\bibinfo{year}{2019}).\n\n\\bibitem{wang2018supervised}\n\\bibinfo{author}{Wang, L.}, \\bibinfo{author}{Zhang, W.}, \\bibinfo{author}{He,\n  X.} \\& \\bibinfo{author}{Zha, H.}\n\\newblock \\bibinfo{title}{Supervised reinforcement learning with recurrent\n  neural network for dynamic treatment recommendation}.\n\\newblock In \\emph{\\bibinfo{booktitle}{Proceedings of the 24th ACM SIGKDD\n  international conference on knowledge discovery \\& data mining}},\n  \\bibinfo{pages}{2447--2456} (\\bibinfo{year}{2018}).\n\n\\bibitem{shang2019pre}\n\\bibinfo{author}{Shang, J.}, \\bibinfo{author}{Ma, T.}, \\bibinfo{author}{Xiao,\n  C.} \\& \\bibinfo{author}{Sun, J.}\n\\newblock \\bibinfo{title}{Pre-training of graph augmented transformers for\n  medication recommendation}.\n\\newblock \\emph{\\bibinfo{journal}{arXiv preprint arXiv:1906.00346}}\n  (\\bibinfo{year}{2019}).\n\n\\bibitem{AutomatedDeidentification}\n\\bibinfo{author}{Neamatullah, I.} \\emph{et~al.}\n\\newblock \\bibinfo{title}{Automated de-identification of free-text medical\n  records}.\n\\newblock \\emph{\\bibinfo{journal}{BMC medical informatics and decision making}}\n  \\textbf{\\bibinfo{volume}{8}}, \\bibinfo{pages}{1--17} (\\bibinfo{year}{2008}).\n\n\\bibitem{StrategiesDeidentification}\n\\bibinfo{author}{Kushida, C.~A.} \\emph{et~al.}\n\\newblock \\bibinfo{title}{Strategies for de-identification and anonymization of\n  electronic health record data for use in multicenter research studies}.\n\\newblock \\emph{\\bibinfo{journal}{Medical care}} \\textbf{\\bibinfo{volume}{50}},\n  \\bibinfo{pages}{S82} (\\bibinfo{year}{2012}).\n\n\\bibitem{CanadaReidentification}\n\\bibinfo{author}{El~Emam, K.} \\emph{et~al.}\n\\newblock \\bibinfo{title}{The re-identification risk of canadians from\n  longitudinal demographics}.\n\\newblock \\emph{\\bibinfo{journal}{BMC medical informatics and decision making}}\n  \\textbf{\\bibinfo{volume}{11}}, \\bibinfo{pages}{1--12} (\\bibinfo{year}{2011}).\n\n\\bibitem{EvaluatingReidentification}\n\\bibinfo{author}{Benitez, K.} \\& \\bibinfo{author}{Malin, B.}\n\\newblock \\bibinfo{title}{Evaluating re-identification risks with respect to\n  the hipaa privacy rule}.\n\\newblock \\emph{\\bibinfo{journal}{Journal of the American Medical Informatics\n  Association}} \\textbf{\\bibinfo{volume}{17}}, \\bibinfo{pages}{169--177}\n  (\\bibinfo{year}{2010}).\n\n\\bibitem{SystematicReidentification}\n\\bibinfo{author}{El~Emam, K.}, \\bibinfo{author}{Jonker, E.},\n  \\bibinfo{author}{Arbuckle, L.} \\& \\bibinfo{author}{Malin, B.}\n\\newblock \\bibinfo{title}{A systematic review of re-identification attacks on\n  health data}.\n\\newblock \\emph{\\bibinfo{journal}{PloS one}} \\textbf{\\bibinfo{volume}{6}},\n  \\bibinfo{pages}{e28071} (\\bibinfo{year}{2011}).\n\n\\bibitem{SynTEG}\n\\bibinfo{author}{Zhang, Z.}, \\bibinfo{author}{Yan, C.}, \\bibinfo{author}{Lasko,\n  T.~A.}, \\bibinfo{author}{Sun, J.} \\& \\bibinfo{author}{Malin, B.~A.}\n\\newblock \\bibinfo{title}{Synteg: a framework for temporal structured\n  electronic health data simulation}.\n\\newblock \\emph{\\bibinfo{journal}{Journal of the American Medical Informatics\n  Association}} \\textbf{\\bibinfo{volume}{28}}, \\bibinfo{pages}{596--604}\n  (\\bibinfo{year}{2021}).\n\n\\bibitem{EVA}\n\\bibinfo{author}{Biswal, S.} \\emph{et~al.}\n\\newblock \\bibinfo{title}{Eva: Generating longitudinal electronic health\n  records using conditional variational autoencoders}.\n\\newblock In \\emph{\\bibinfo{booktitle}{Machine Learning for Healthcare\n  Conference}}, \\bibinfo{pages}{260--282} (\\bibinfo{organization}{PMLR},\n  \\bibinfo{year}{2021}).\n\n\\bibitem{CorGAN}\n\\bibinfo{author}{Torfi, A.} \\& \\bibinfo{author}{Fox, E.~A.}\n\\newblock \\bibinfo{title}{Corgan: Correlation-capturing convolutional\n  generative adversarial networks for generating synthetic healthcare records}.\n\\newblock In \\emph{\\bibinfo{booktitle}{The Thirty-Third International Flairs\n  Conference}} (\\bibinfo{year}{2020}).\n\n\\bibitem{CONAN}\n\\bibinfo{author}{Cui, L.} \\emph{et~al.}\n\\newblock \\bibinfo{title}{Conan: Complementary pattern augmentation for rare\n  disease detection}.\n\\newblock In \\emph{\\bibinfo{booktitle}{Proceedings of the AAAI Conference on\n  Artificial Intelligence}}, vol.~\\bibinfo{volume}{34},\n  \\bibinfo{pages}{614--621} (\\bibinfo{year}{2020}).\n\n\\bibitem{MedGAN}\n\\bibinfo{author}{Choi, E.} \\emph{et~al.}\n\\newblock \\bibinfo{title}{Generating multi-label discrete patient records using\n  generative adversarial networks}.\n\\newblock In \\emph{\\bibinfo{booktitle}{Machine learning for healthcare\n  conference}}, \\bibinfo{pages}{286--305} (\\bibinfo{organization}{PMLR},\n  \\bibinfo{year}{2017}).\n\n\\bibitem{MedWGAN}\n\\bibinfo{author}{Baowaly, M.~K.}, \\bibinfo{author}{Lin, C.-C.},\n  \\bibinfo{author}{Liu, C.-L.} \\& \\bibinfo{author}{Chen, K.-T.}\n\\newblock \\bibinfo{title}{Synthesizing electronic health records using improved\n  generative adversarial networks}.\n\\newblock \\emph{\\bibinfo{journal}{Journal of the American Medical Informatics\n  Association}} \\textbf{\\bibinfo{volume}{26}}, \\bibinfo{pages}{228--241}\n  (\\bibinfo{year}{2019}).\n\n\\bibitem{EHRMGAN}\n\\bibinfo{author}{Li, J.}, \\bibinfo{author}{Cairns, B.~J.}, \\bibinfo{author}{Li,\n  J.} \\& \\bibinfo{author}{Zhu, T.}\n\\newblock \\bibinfo{title}{Generating synthetic mixed-type longitudinal\n  electronic health records for artificial intelligent applications}.\n\\newblock \\emph{\\bibinfo{journal}{arXiv preprint arXiv:2112.12047}}\n  (\\bibinfo{year}{2021}).\n\n\\bibitem{EMR-WGAN}\n\\bibinfo{author}{Zhang, Z.}, \\bibinfo{author}{Yan, C.}, \\bibinfo{author}{Mesa,\n  D.~A.}, \\bibinfo{author}{Sun, J.} \\& \\bibinfo{author}{Malin, B.~A.}\n\\newblock \\bibinfo{title}{Ensuring electronic medical record simulation through\n  better training, modeling, and evaluation}.\n\\newblock \\emph{\\bibinfo{journal}{Journal of the American Medical Informatics\n  Association}} \\textbf{\\bibinfo{volume}{27}}, \\bibinfo{pages}{99--108}\n  (\\bibinfo{year}{2020}).\n\n\\bibitem{HGAN}\n\\bibinfo{author}{Yan, C.}, \\bibinfo{author}{Zhang, Z.},\n  \\bibinfo{author}{Nyemba, S.} \\& \\bibinfo{author}{Malin, B.~A.}\n\\newblock \\bibinfo{title}{Generating electronic health records with multiple\n  data types and constraints}.\n\\newblock In \\emph{\\bibinfo{booktitle}{AMIA Annual Symposium Proceedings}},\n  vol. \\bibinfo{volume}{2020}, \\bibinfo{pages}{1335}\n  (\\bibinfo{organization}{American Medical Informatics Association},\n  \\bibinfo{year}{2020}).\n\n\\bibitem{SmoothGAN}\n\\bibinfo{author}{Rashidian, S.} \\emph{et~al.}\n\\newblock \\bibinfo{title}{Smooth-gan: towards sharp and smooth synthetic ehr\n  data generation}.\n\\newblock In \\emph{\\bibinfo{booktitle}{International Conference on Artificial\n  Intelligence in Medicine}}, \\bibinfo{pages}{37--48}\n  (\\bibinfo{organization}{Springer}, \\bibinfo{year}{2020}).\n\n\\bibitem{MIMIC}\n\\bibinfo{author}{Johnson, A.~E.} \\emph{et~al.}\n\\newblock \\bibinfo{title}{Mimic-iii, a freely accessible critical care\n  database}.\n\\newblock \\emph{\\bibinfo{journal}{Scientific data}}\n  \\textbf{\\bibinfo{volume}{3}}, \\bibinfo{pages}{1--9} (\\bibinfo{year}{2016}).\n\n\\bibitem{GPT1}\n\\bibinfo{author}{Radford, A.}, \\bibinfo{author}{Narasimhan, K.},\n  \\bibinfo{author}{Salimans, T.} \\& \\bibinfo{author}{Sutskever, I.}\n\\newblock \\bibinfo{title}{Improving language understanding by generative\n  pre-training}  (\\bibinfo{year}{2018}).\n\n\\bibitem{MimicBenchmark}\n\\bibinfo{author}{Harutyunyan, H.}, \\bibinfo{author}{Khachatrian, H.},\n  \\bibinfo{author}{Kale, D.~C.}, \\bibinfo{author}{Ver~Steeg, G.} \\&\n  \\bibinfo{author}{Galstyan, A.}\n\\newblock \\bibinfo{title}{Multitask learning and benchmarking with clinical\n  time series data}.\n\\newblock \\emph{\\bibinfo{journal}{Scientific Data}}\n  \\textbf{\\bibinfo{volume}{6}}, \\bibinfo{pages}{96} (\\bibinfo{year}{2019}).\n\\newblock \\urlprefix\\url{https://doi.org/10.1038/s41597-019-0103-9}.\n\n\\bibitem{PyTorch}\n\\bibinfo{author}{Paszke, A.} \\emph{et~al.}\n\\newblock \\bibinfo{title}{Pytorch: An imperative style, high-performance deep\n  learning library}.\n\\newblock In \\emph{\\bibinfo{booktitle}{Advances in Neural Information\n  Processing Systems 32}}, \\bibinfo{pages}{8024--8035}\n  (\\bibinfo{publisher}{Curran Associates, Inc.}, \\bibinfo{year}{2019}).\n\\newblock\n  \\urlprefix\\url{http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}.\n\n\\bibitem{GPT2}\n\\bibinfo{author}{Radford, A.} \\emph{et~al.}\n\\newblock \\bibinfo{title}{Language models are unsupervised multitask learners}.\n\\newblock \\emph{\\bibinfo{journal}{OpenAI blog}} \\textbf{\\bibinfo{volume}{1}},\n  \\bibinfo{pages}{9} (\\bibinfo{year}{2019}).\n\n\\bibitem{lee2018natural}\n\\bibinfo{author}{Lee, S.~H.}\n\\newblock \\bibinfo{title}{Natural language generation for electronic health\n  records}.\n\\newblock \\emph{\\bibinfo{journal}{NPJ digital medicine}}\n  \\textbf{\\bibinfo{volume}{1}}, \\bibinfo{pages}{63} (\\bibinfo{year}{2018}).\n\n\\bibitem{SynPUF}\n\\bibinfo{author}{Borton, J.} \\emph{et~al.}\n\\newblock \\bibinfo{title}{Data entrepreneurs synthetic puf: A working puf as\n  an alternative to traditional synthetic and non-synthetic pufs}.\n\\newblock \\emph{\\bibinfo{journal}{JSM Proceedings, Survey Research Methods\n  Section}}  (\\bibinfo{year}{2010}).\n\n\\bibitem{yan2022multifaceted}\n\\bibinfo{author}{Yan, C.} \\emph{et~al.}\n\\newblock \\bibinfo{title}{A multifaceted benchmarking of synthetic electronic\n  health record generation models}.\n\\newblock \\emph{\\bibinfo{journal}{Nature Communications}}\n  \\textbf{\\bibinfo{volume}{13}}, \\bibinfo{pages}{7609} (\\bibinfo{year}{2022}).\n\n\\bibitem{yale2020generation}\n\\bibinfo{author}{Yale, A.} \\emph{et~al.}\n\\newblock \\bibinfo{title}{Generation and evaluation of privacy preserving\n  synthetic health data}.\n\\newblock \\emph{\\bibinfo{journal}{Neurocomputing}}\n  \\textbf{\\bibinfo{volume}{416}}, \\bibinfo{pages}{244--255}\n  (\\bibinfo{year}{2020}).\n\n\\bibitem{Synthea}\n\\bibinfo{author}{Walonoski, J.} \\emph{et~al.}\n\\newblock \\bibinfo{title}{Synthea: An approach, method, and software mechanism\n  for generating synthetic patients and the synthetic electronic health care\n  record}.\n\\newblock \\emph{\\bibinfo{journal}{Journal of the American Medical Informatics\n  Association}} \\textbf{\\bibinfo{volume}{25}}, \\bibinfo{pages}{230--238}\n  (\\bibinfo{year}{2018}).\n\n\\bibitem{Bayesian}\n\\bibinfo{author}{Tucker, A.}, \\bibinfo{author}{Wang, Z.},\n  \\bibinfo{author}{Rotalinti, Y.} \\& \\bibinfo{author}{Myles, P.}\n\\newblock \\bibinfo{title}{Generating high-fidelity synthetic patient data for\n  assessing machine learning healthcare software}.\n\\newblock \\emph{\\bibinfo{journal}{NPJ digital medicine}}\n  \\textbf{\\bibinfo{volume}{3}}, \\bibinfo{pages}{1--13} (\\bibinfo{year}{2020}).\n\n\\bibitem{EMRBots}\n\\bibinfo{author}{Kartoun, U.}\n\\newblock \\bibinfo{title}{Advancing informatics with electronic medical records\n  bots (emrbots)}.\n\\newblock \\emph{\\bibinfo{journal}{Software Impacts}}\n  \\textbf{\\bibinfo{volume}{2}}, \\bibinfo{pages}{100006} (\\bibinfo{year}{2019}).\n\\newblock\n  \\urlprefix\\url{https://www.sciencedirect.com/science/article/pii/S2665963819300065}.\n\n\\bibitem{MDClone}\n\\bibinfo{author}{Foraker, R.} \\emph{et~al.}\n\\newblock \\bibinfo{title}{Analyses of original and computationally-derived\n  electronic health record data: The national covid cohort collaborative.}\n\\newblock \\emph{\\bibinfo{journal}{Journal of Medical Internet Research}}\n  (\\bibinfo{year}{2021}).\n\n\\bibitem{Syntegra}\n\\bibinfo{author}{Philippidis, A.}\n\\newblock \\bibinfo{title}{Synthetic data for a real pandemic: Syntegra applying\n  machine learning-based engine to create replica of nih's national covid\n  cohort collaborative (n3c) dataset}.\n\\newblock \\emph{\\bibinfo{journal}{GEN Edge}} \\textbf{\\bibinfo{volume}{3}},\n  \\bibinfo{pages}{42--47} (\\bibinfo{year}{2021}).\n\n\\bibitem{GANs}\n\\bibinfo{author}{Goodfellow, I.} \\emph{et~al.}\n\\newblock \\bibinfo{title}{Generative adversarial nets}.\n\\newblock \\emph{\\bibinfo{journal}{Advances in neural information processing\n  systems}} \\textbf{\\bibinfo{volume}{27}} (\\bibinfo{year}{2014}).\n\n\\bibitem{LongGAN}\n\\bibinfo{author}{Sun, S.} \\emph{et~al.}\n\\newblock \\bibinfo{title}{Generating longitudinal synthetic ehr data with\n  recurrent autoencoders and generative adversarial networks}.\n\\newblock In \\emph{\\bibinfo{booktitle}{Heterogeneous Data Management,\n  Polystores, and Analytics for Healthcare}}, \\bibinfo{pages}{153--165}\n  (\\bibinfo{publisher}{Springer}, \\bibinfo{year}{2021}).\n\n\\bibitem{MaskEHR}\n\\bibinfo{author}{Ma, F.}, \\bibinfo{author}{Wang, Y.}, \\bibinfo{author}{Gao,\n  J.}, \\bibinfo{author}{Xiao, H.} \\& \\bibinfo{author}{Zhou, J.}\n\\newblock \\bibinfo{title}{Rare disease prediction by generating quality-assured\n  electronic health records}.\n\\newblock In \\emph{\\bibinfo{booktitle}{Proceedings of the 2020 SIAM\n  International Conference on Data Mining}}, \\bibinfo{pages}{514--522}\n  (\\bibinfo{organization}{SIAM}, \\bibinfo{year}{2020}).\n\n\\bibitem{Attention}\n\\bibinfo{author}{Vaswani, A.} \\emph{et~al.}\n\\newblock \\bibinfo{title}{Attention is all you need}.\n\\newblock In \\emph{\\bibinfo{booktitle}{Advances in neural information\n  processing systems}}, \\bibinfo{pages}{5998--6008} (\\bibinfo{year}{2017}).\n\n\\bibitem{BERT}\n\\bibinfo{author}{Devlin, J.}, \\bibinfo{author}{Chang, M.-W.},\n  \\bibinfo{author}{Lee, K.} \\& \\bibinfo{author}{Toutanova, K.}\n\\newblock \\bibinfo{title}{Bert: Pre-training of deep bidirectional transformers\n  for language understanding}.\n\\newblock \\emph{\\bibinfo{journal}{arXiv preprint arXiv:1810.04805}}\n  (\\bibinfo{year}{2018}).\n\n\\bibitem{GPT3}\n\\bibinfo{author}{Brown, T.~B.} \\emph{et~al.}\n\\newblock \\bibinfo{title}{Language models are few-shot learners}.\n\\newblock \\emph{\\bibinfo{journal}{CoRR}}\n  \\textbf{\\bibinfo{volume}{abs/2005.14165}} (\\bibinfo{year}{2020}).\n\\newblock \\urlprefix\\url{https://arxiv.org/abs/2005.14165}.\n\\newblock \\eprint{2005.14165}.\n\n\\bibitem{guha2006approximation}\n\\bibinfo{author}{Guha, S.}, \\bibinfo{author}{Koudas, N.} \\&\n  \\bibinfo{author}{Shim, K.}\n\\newblock \\bibinfo{title}{Approximation and streaming algorithms for histogram\n  construction problems}.\n\\newblock \\emph{\\bibinfo{journal}{ACM Transactions on Database Systems (TODS)}}\n  \\textbf{\\bibinfo{volume}{31}}, \\bibinfo{pages}{396--438}\n  (\\bibinfo{year}{2006}).\n\n\\bibitem{btheodorou99_2023_8041405}\n\\bibinfo{author}{btheodorou99}.\n\\newblock \\bibinfo{title}{btheodorou99/halo\\_inpatient: First release}\n  (\\bibinfo{year}{2023}).\n\\newblock \\urlprefix\\url{https://doi.org/10.5281/zenodo.8041405}.\n\n\\bibitem{pyhealth2022github}\n\\bibinfo{author}{Yang, C.}, \\bibinfo{author}{Wu, Z.}, \\bibinfo{author}{Jiang,\n  P.}, \\bibinfo{author}{Lin, Z.} \\& \\bibinfo{author}{Sun, J.}\n\\newblock \\bibinfo{title}{{PyHealth}: A deep learning toolkit for healthcare\n  predictive modeling} (\\bibinfo{year}{2022}).\n\\newblock \\urlprefix\\url{https://github.com/sunlabuiuc/PyHealth}.\n\n\\end{thebibliography}\n\n\\section{Acknowledgements} This work is in part supported by National Science Foundation award SCH-2014438, IIS-1418511, CCF-1533768, IIS-2034479, the National Institute of Health award NIH R01 1R01NS107291-01 and R56HL138415, all to J.S. \n\n\\section{Author Contributions} B.T. and J.S. proposed the method, B.T. and conducted all the experiments, B.T., C.X. and J.S. wrote the manuscript.\n\n\\section{Competing Interests} The authors declare no competing interests.\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{CEHR-GPT: Generating Electronic Health Records with Chronological Patient Timelines}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{multicols}{2}\n\\raggedcolumns\n\n\\section*{Abstract}\nSynthetic Electronic Health Records (EHR) have emerged as a pivotal tool in advancing healthcare applications and machine learning models, particularly for researchers without direct access to healthcare data. Although existing methods, like rule-based approaches and generative adversarial networks (GANs), generate synthetic data that resembles real-world EHR data, these methods often use a tabular format, disregarding temporal dependencies in patient histories and limiting data replication. Recently, there has been a growing interest in leveraging Generative Pre-trained Transformers (GPT) for EHR data. This enables applications like disease progression analysis, population estimation, counterfactual reasoning, and synthetic data generation. In this work, we focus on synthetic data generation and demonstrate the capability of training a GPT model using a particular patient representation derived from CEHR-BERT, enabling us to generate patient sequences, which can be seamlessly converted to the Observational Medical Outcomes Partnership (OMOP) data format. \n\n\\section*{Keywords}\nGenerative Pre-trained Transformer, Synthetic Electronic Health Records, Patient Representation, Observational Medical Outcomes Partnership - Common Data Model, Observational Health Data Sciences and Informatics\n\n\\section{Introduction}\nAccess to electronic health records (EHRs) is fundamental to healthcare research, drug surveillance, clinical machine learning, and system development. However, the use of real-world EHR data comes with considerable challenges such as privacy and security issues, institutional consent, and restrictions on data sharing. Synthetic data emerges as a promising solution, offering a more expedient and secure pathway to healthcare information, which could accelerate progress across various sectors, including academic research, clinical settings, and the pharmaceutical industry \\cite{murray2011design}.\n\nSynthetic data is not real data, that is, it doesn't relate to any specific individual. However, it mimics the statistical characteristics and journeys of specific patient populations. Synthetic data enables a broader range of researchers to answer their questions of interest without going through the cumbersome process of accessing real data and worrying about patient privacy \\cite{Ghosheh2022}. In recent years, many machine learning, specifically deep learning and generative artificial intelligence (AI) models, have been developed to derive synthetic data from real EHR data \\cite{Choi2017}. However, most existing methods for synthetic EHR data generation fail to adequately capture the temporal dependencies that are often critical in medical scenarios. These temporal aspects, such as medication schedules, symptom progression, and lab result timelines, are vital for a comprehensive understanding of patient health trajectories and for developing effective treatment strategies. An ideal synthetic dataset derived from institutional data should maintain the inherent correlations among time-series features, thus enabling researchers to externally validate machine learning models in different populations. Crucially, the synthetic dataset must preserve accurate patient timelines, as predictive tasks are highly susceptible to temporal variations. A synthetic dataset is considered to exhibit comparable machine learning utility to the original data if it meets two key criteria: 1) it demonstrates similar outcome prevalence to the source data; 2) machine learning models trained on the synthetic data achieve performance metrics akin to those trained with the original data.\n\nThe majority of the existing research focuses on developing new deep learning models in generative EHR research but without adequate emphasis on retaining accurate temporal information \\cite{Li2017, Li2023, Lee2020}. Unfortunately, synthetic EHR datasets developed as such will not support use cases that require the accurate construction of a patient timeline e.g., 30-day readmission, one-year risk of heart failure, and disease progression. This limits existing work to only perform simple code prediction in their evaluations instead of comprehensive phenotype predictions. Another challenge in using synthetic EHR data in practice is its difficulty with dissemination due to a lack of standards. Synthetic patient sequences cannot be widely adopted for analyses without use of a common data model, however, none of the existing works have included such a component in their frameworks to present the synthetic data in an easy-to-consume fashion.  \n\nIn our view, time-series synthetic data should not only capture the underlying characteristics of heterogeneous EHRs but also satisfy the following temporal requirements, 1) a matching distribution of the starting age; 2) a matching distribution of the starting year; 3) a matching distribution of the inpatient duration; 4) a matching distribution of time intervals between neighboring visits. Furthermore, synthetic EHR data should be stored in common data models such as the Observational Medical Outcomes Partnership (OMOP) Common Data Model, which is used in many large data networks, \\cite{Hripcsak2013} for easy dissemination and consumption. Although creating such a time-series EHR dataset seems to be a challenging task, we think that this problem can be solved through a patient representation approach. The key is to focus on designing a good patient representation rather than creating a sophisticated architecture to model time and medical events simultaneously. In this paper, we present the CEHR-GPT framework for building an end-to-end workflow to generate time-series synthetic EHR data. Our contributions are summarized below, \n\\begin{itemize}\n  \\item We design a novel patient representation that captures visit types, discharge facilities for inpatient visits, and all temporal data, such as starting year, age, intervals between visits, and inpatient visit duration. This is the first instance of fully preserving such temporal information, to our knowledge.\n  \\item We treat patient sequence generation as a language modeling problem, which allowed us to use the state-of-the-art language model Generative Pre-trained Transformers (GPT) to learn the distribution of patient sequences to generate new synthetic sequences \\cite{Radford, Vaswani}.  \n  \\item We converted synthetic sequences to the common data format OMOP with almost no loss of temporal information. Synthetic OMOP can be easily evaluated using the OHDSI tools and disseminated to others.  \n  \\item We evaluated the synthetic EHR data on three levels, dimension-wise distribution (marginal distribution), co-occurrence relationship, and machine learning model performance metrics. \n  \\item We utilized the state-of-the-art evaluation framework proposed by Yan \\textit{et al.} \\cite{Yan_Brad_2022} to quantify the privacy risk by calculating a number of privacy metrics for synthetic Electronic Health Records (EHR) data, and demonstrated that the privacy risk associated with this synthetic data is low.\n\\end{itemize}\n\n\\section{Related work}\nWith the adoption of Generative Adversarial Networks (GANs) \\cite{Goodfellow2014}, researchers have found creative ways to generate synthetic EHR data. Since 2017, several groups have applied GANs to tabular EHRs and developed several evaluation and privacy metrics to quantify the performance of GANs \\cite{Choi2017}. Despite the success, one limitation is that the tabular format fails to capture the temporal nature of EHR data because they are constructed from patient histories using a bag-of-words approach. It was not until 2020 that researchers started developing new GAN architectures to tackle the time series data. Dual adversarial autoencoder (DAAE) \\cite{Lee2020} used a combination of a variational autoencoder (VAE) and two GAN components, where the inner GAN was trained to replicate the encoded representation generated by the encoder, and the outer GAN was trained to generate realistic-looking patient sequences in addition to the reconstruction error. Another model called EHR-M-GAN \\cite{Li2023} employed a similar autoencoder architecture with two main differences 1) they used a dual-VAE framework to handle the continuous and discrete valued features, where the continuous and discrete representations were generated by encoders first and then collectively used for decoding; 2) in the GAN generator, they used two parallel recurrent neural networks (one for sampling continuous noise and one for discrete noise) with a so-called Bilateral LSTM cell to allow the continuous and discrete noise vectors to interact with each other to generate better sampling vectors. Although these GANs took the temporal order of events into consideration, they did not generate timestamps for visits or medical events, therefore limiting the use of the synthetic data. \n\nImproving upon the previous works, a two-stage learning algorithm (dependency learning and conditional simulation) named SynTEG was proposed to generate timestamped synthetic data \\cite{Zhang2021}. In dependency learning, transformer encoders learned visit representations, which were used to feed into an recurrent neural network (RNN) model to learn the dependencies between visits. Two self-supervising learning tasks were used for training  prediction of the timestamp and diagnoses at the next visit. In the conditional simulation, the learned visit representations generated from the previous step were used to train a conditional GAN to generate the diagnosis codes at each visit. This approach achieved superior performance in the time-sensitive evaluations over the previous methods. However, there are still ongoing challenges that have not been addressed in their work, 1) other EHR data was not utilized because only diagnosis codes were included in training; 2) the visits were assumed to start and end on the same day, which would fail to model in-patient visits that normally span days; therefore, the constructed patient timeline would be inaccurate; 3) the synthetic data didn't include visit types and discharge facilities. \n\nUntil now, almost all the existing approaches used some variation of GAN for learning the data distribution, but unfortunately, GANs are notoriously difficult to train and easily subject to mode collapse. Despite the recent advancements in optimization techniques such as Wasserstein-GAN \\cite{Gulrajani2017}, it would require a significant amount of time to tune hyperparameters to train GANs. As the previous works have demonstrated \\cite{Pang2021, Li2020, Rasmy2021}, patient sequence generation could be conceptually represented as a language modeling problem. Foresight \\cite{Kraljevic2022} adapted GPT to forecast patient trajectories. Their method used a name entity recognition (NER) tool to extract medical concepts from discharge summaries, based on which a patient sequence was constructed chronologically. Then they trained a standard GPT model using all patient sequences constructed from the previous step. To forecast future events, they fed a patient history to prompt the model and employed a Monte Carlo sampling strategy to calculate the probability of developing certain conditions. One limitation of this work was that the model could not predict when a certain condition would happen due to a lack of temporal information in their patient sequence. Nevertheless, their work demonstrated the potential of using GPT for modeling patient sequences.  We aim to address these limitations in our work. \n\n\\vspace{20pt}\n\n\\begin{Figure}\n    \\includegraphics[width=\\linewidth]{overview.png}\n    \\captionof{figure}{The OMOP data is first converted to patient sequences by an OMOP encoder based on the patient representation that preserves demographics, visit types, and temporal intervals between visits. Then a generative model is trained to learn the sequence distribution to generate new sequences. Next, the generated sequences are converted back to the OMOP format using an OMOP decoder.}\n    \\label{figure1}\n\\end{Figure}\n\n\\section{Methods}\nIn Figure \\ref{figure1}, we present the framework for generating synthetic EHR data from an OMOP source. To retain the temporal dependencies, we opted to work directly with time-series patient sequences instead of using a bag-of-words (BOW) representation by building on our previous work \\cite{Pang2021}. We first encoded the OMOP data into patient sequences using a specific patient representation, described later in this section. Secondly, we trained a generative model on the converted patient sequences and utilized it to generate new synthetic patient sequences. Finally, we fed the synthetic patient sequences into an OMOP decoder to create a synthetic OMOP dataset. Furthermore, an evaluation procedure was developed to assess the similarity between the synthetic OMOP and the source OMOP data.\n\n\\subsection{Patient Representation}\nWe designed a patient representation in CEHR-BERT \\cite{Pang2021} that captures medically relevant events and their timelines while exhibiting certain characteristics of a sentence. In order to fully leverage Large Language Models (LLM) on patient sequences, we further extended this patient representation to include demographic information, patient history, and temporal dependencies as shown in Figure \\ref{patient_representation}. The start of the sequence defines the demographic prompt containing EHR start year, age, gender, and race. It is followed by visit blocks separated by artificial time tokens (ATT) representing time intervals in days, e.g., \\(D_1\\) represents an interval of 1 day.  For time intervals surpassing 1080 days, we grouped these into a single Long Term (LT) token, a decision guided by the low occurrence rate in this time frame. Each visit block starts with a visit type token (VTT) to signify the type of visit and is then followed by domain records arranged in chronological order. In the case of inpatient visits, a distinct inpatient ATT (IATT) was inserted between neighboring inpatient spans, defined as the groups of records that occurred on the same day. These are distinct from ATT and are used for capturing the time between multiple events characterized by concepts within the same visit. In addition, a discharge facility code (e.g. discharge home and long-term care) was inserted at the end of the inpatient visit. The use of IATT, which is distinct from ATT, was necessary since they are attributed to two different contexts, and resulted in better performance than using the same ATT across both contexts.\n\nThis patient representation allows us to convert from any common data model (e.g. OMOP) to patient sequences and vice versa without any loss of temporal information. To formulate this property, let's denote \\(D_i\\) to be data associated with the \\(i\\)th patient in the source format, \\(P_i\\) to be the patient sequence converted from \\(D_i\\), and \\(F\\) to be the function that converts source data to patient sequences, represented as \\(P_i = F(D_i)\\). Let's then denote \\(D_i'\\) to be the reconstructed source data and \\(F'\\) to be the inverse function that converts patient sequences back to original source format. This indicates \\(D_i' = F'(P_i)\\). Finally, let's denote \\(T\\) to be a function that extracts all the dates for a set of patient records. The patient representation is said to preserve temporal information perfectly if and only if the following statement is true for every single patient, \\(T(D_i) = T(D_i') + C_i\\), where \\(C_i\\) is a constant that represents a consistent time shift e.g., \\(C_i=4\\) days.\n\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=0.9\\linewidth]{patient_representation.png}\n    \\captionof{figure}{The patient representation preserves demographics, visit types, and temporal intervals between visits and inpatient duration. It's designed to have the demographic prompt at the beginning including year at the first visit, age at the first visit, gender and race tokens, then followed by a series of visit blocks to represent the complete patient timeline. An artificial time token (ATT) is inserted between the neighboring visit blocks to keep track of the time intervals in days. In each visit block, all the essential information is retained including the visit type and domain records. In the case of inpatient visits, the inpatient ATT tokens (representing time intervals in days) are inserted between groups of concepts that occur on the same day, in addition, a discharge token is provided at the end of the visit block. }\n    \\label{patient_representation}\n\\end{figure*}\n\\subsection{OMOP Encoder}\nTo create a patient sequence, we began by generating a demographic prompt using data from the OMOP person and visit tables. This prompt included essential demographic information such as the patient's age at their initial visit, the year of their first visit, their gender, and their race. Subsequently, we constructed a series of visit blocks to represent the patient's entire medical history. We inserted an ATT token between these visit blocks to signify the time intervals between them. Within each visit block, we gathered all relevant records from OMOP domain tables (e.g., condition) and arranged them chronologically based on their respective timestamps. In cases where there are timestamp ties, we sorted the concepts. Additionally, three artificial tokens (VS, VE, and VTT) were added at the beginning and end of each visit block to denote the start, end, and type of the visit, as illustrated in Figure \\ref{patient_representation}. For inpatient visit blocks, extra processing steps were necessary. Initially, we grouped records by their timestamps to identify all inpatient spans, arranging them in chronological order. Next, we inserted IATT between these spans, aligning them with the respective time intervals. Finally, the discharge facility code was extracted from the OMOP visit table and appended to the end of the block.\n\n\\subsection{Generative Model}\nWe used a GPT model with standard transformer decoders, where the input layer utilized concept embedding and trainable positional embedding. The model was trained using the Next Word Prediction learning objective. When generating a patient sequence, we randomly sampled a demographic prompt from our source sequences, which served as the input to the GPT model. Using these prompts, the entire patient history was generated autoregressively by sampling tokens from the predictive distribution at the final layer. \n\n\\subsection{OMOP Decoder}\nThe patient sequence was converted back to the OMOP format using the OMOP decoder. The start-year prompt determined the EHR history's beginning, using January 1st as the default. Demographic data was stored in the person table, while concepts were transformed into condition, drug, and procedure tables. A date cursor was used to represent the current time as we were processing each patient sequence, it was initially set to the star year and was updated whenever an ATT token was encountered. We first parsed out the number of days represented by the ATT token, and then moved the data cursor by the same number of days to the future. During the sequence processing, the VS token marked the start of a new visit block. We, therefore, extracted the token corresponding to the visit type (the token immediately followed by VS) and created a new visit record with the corresponding type. \n\nAll the tokens subsumed by this visit block were converted to condition, drug, and procedure records and linked to the current visit. For outpatient visits, we assumed that all data points were generated on the same day; therefore, we set the start and end dates of the visit to the current value of the date cursor. Similarly, all the domain records were set to the date cursor as well. \n\nWhen processing inpatient visits, the date cursor was updated inside the visit block based on the time intervals represented by IATT tokens between inpatient spans. Domain records were generated with the current value of the date cursor. Towards the end of the inpatient visit block, we extracted the last token (the token right before VE) corresponding to a discharge facility and updated the visit end date using the current value of the date cursor (as the date cursor was frequently updated inside an inpatient visit block). This allowed us to preserve the complete information about inpatient visits. If generated sequences do not follow patterns presented in the patient representation, they will be discarded to ensure the quality of the synthetic data.\n\n\\section{Experiments and Results}\n\\subsection{Data and Preprocessing} \\label{data_processing}\nThe source patient sequences were generated from the OMOP database derived from Columbia University Irving Medical Center-New York Presbyterian Hospital EHR data, which includes 3.7 million unique patients' medical histories including condition, medication, and procedure. Unknown concepts (i.e., \\(concept\\_id=0\\)) were removed from all domains except for the visit type when constructing the patient sequences using the proposed patient representation. Patients with less than 20 tokens were removed from the training dataset, and approximately 2.3 million patients were included for training whereas 75,000 patients were held out for privacy evaluations. For the GPT model, we used a context window of 512, 16 transformer decoders, 8 attention heads with a dropout rate of 0.1, and 128 dimensions for both the embedding and hidden units. All patients with longer than 512 tokens were post-truncated to fit the context window. The statistics of training data was summarized in Table \\ref{tab:summary-stats}. We trained the model for 2 epochs on 2 Nvidia 2080 TI GPUs with a batch size of 32 and a learning rate of 0.0002. The model checkpoint was created every 10,000 steps. \n\n\\begin{center}\n\\captionsetup{justification=raggedright,singlelinecheck=false}  % Left-align the caption\n\\renewcommand{\\arraystretch}{1.2} % Default value: 1\n  \\begin{tabular}{cp{3.5cm}p{3.5cm}}\n  \\toprule \n  \\textit{}  & \n  \\textit{No. of visits per patient} & \\textit{Sequence length per patient}\\\\\n  \\midrule\n  mean & 16 & 148\\\\\n  std & 19 & 154\\\\\n  min & 2 & 20\\\\\n  25\\% & 4 & 38\\\\\n  50\\% & 8 & 78\\\\\n  75\\% & 21 & 198\\\\\n  max & 102 & 512\\\\\n    \\hline\n  \\end{tabular}\n  \\captionof{table}{Summary statistics of the CUIMC-NYP OMOP training data}\n  \\label{tab:summary-stats}\n\\end{center}\n\nDuring the first epoch, we used a standard data generator strategy, where every training example was fed to the model; however, we switched to a random sampling strategy to draw training examples during the second epoch. For synthetic data generation, we used the 10th model snapshot because early experiments showed its superior performance compared to other snapshots. Furthermore, we used several sampling hyper-parameters including top k=300, top k=200, top k=100, top p=90\\%, top=95\\%, and top=100\\% to generate different synthetic OMOP datasets. Using specific top p/k values is a common technique in language models for data generation. For instance, the top k approach limits the selection of the k most probable tokens in the prediction distribution during sampling. On the other hand, the top p method selects a set of most likely tokens whose combined probability reaches p\\% (e.g. 90\\%) in the predictive distribution. For each sampling strategy, 1M synthetic sequences were generated and converted to OMOP. On average, 98\\% of the generated sequences passed the validation and were converted to OMOP.\n\n\\subsection{Evaluations}\nWe followed the evaluation procedures proposed by \\cite{Yan_Brad_2022} to compute the data utility metrics including dimension-wise distribution, co-occurrence relationship, and machine learning model performance. Some of the metrics were originally designed for tabular EHR data; therefore, we adapted them to the time-series setting. When using Kullback-Leibler (KL) divergence to evaluate source and synthetic datasets, we use concept probabilities defined as,\n\\[\nP_{prob}(c)=\\frac{\\sum_i^n \\mathbbm{1}\\Big[c \\in h_i \\Big]}{\\sum_i^n \\sum_j^m \\mathbbm{1} \\Big[c_j \\in h_i \\Big]}\n\\]\nwhere $c$ denotes the target concept, $h_i$ denotes the $ith$ patient history, $n$ and $m$  denote the total number of patients and concepts respectively. Due to the small probability values, we opted to use the prevalence instead for data visualization using a slightly modified formula below, \n\\[\nP_{prev}(c)=\\frac{\\sum_i^n \\mathbbm{1}\\Big[c \\in h_i \\Big]}{n}\n\\]\nThe only difference between the prevalence and the probability is the normalization constant used as the denominator. \n\nFor baseline comparison, we added three variants of GPT models trained on slightly different patient representations. The first baseline model was trained on the adjusted CEHR-BERT representation, which differs from the proposed patient representation by 1) CEHR-BERT ATT tokens contain a mix of day/week/month/year tokens to represent time intervals while CEHR-GPT only used the day tokens and 2) the IATT tokens and discharge facility tokens were not used in the CEHR-BERT representation. The second baseline model used the proposed patient representation with IATT tokens removed. We will refer to this baseline as GPT-OUTPAT. The last baseline GPT was trained on the patient representation widely used in time-series EHR research, where the concepts were simply ordered chronologically and put in a sequence without any additional artificial tokens. To use such sequences for comparison, we assumed that all medical events in the patient sequence belonged to a single visit. This baseline model will be referred to as GPT-Vanilla. We only used the top\\_p=95\\% sampling strategy to generate synthetic OMOPs for these baseline models. The comparison between these patient representations can be seen in Supplementary Figure \\ref{patient_history_comparison}.\n\n\\subsubsection{Dimension-wise Distribution}\nKL divergence was assessed to compare the concept probability distributions between synthetic and source datasets among the entire population. In Figure \\ref{concept_kl_divergence}, synthetic datasets generated by different patient representations and sampling strategies were evaluated against real patient data. The results showed that baseline models CEHR-BERT, GPT-Vanilla and GPT-OUTPAT with sampling strategy using the threshold of top\\_p$=95\\%$ diverged the least from real concept probability distributions followed by GPT models trained with top\\_p$=100\\%$ and top\\_p$=95\\%$. GPT models with threshold of top\\_k$=300$ and top\\_k$=200$ with relatively similar divergence. However, GPT models with threshold of top\\_k$=100$ sampling strategy had the largest KL divergence.\n\n\\begin{Figure}\n    \\includegraphics[width=\\linewidth]\n    {concept_kl_divergence.png}\n    \\captionof{figure}{KL divergence for comparing concept probability distribution between synthetic data and real data. The probabilities of concepts were calculated on the scale of the entire population.}\n    \\label{concept_kl_divergence}\n\\end{Figure}\n\nFurthermore, we conducted a qualitative analysis using the synthetic data top\\_p=95\\% to gain a more comprehensive understanding. The dimension-wise distributions between the synthetic and source datasets were compared at three distinct levels: the entire population, specific sub-groups (e.g., female population), and particular cohorts (e.g., hospitalization cases). Figure \\ref{concept_prevalence} illustrates the concept prevalence comparison between the original OMOP dataset and the generated OMOP dataset, using a threshold of top\\_p$=95\\%$. In the high-frequency regions, most data points cluster closely around the diagonal line, indicating a strong agreement between the source and synthetic data. Conversely, data points appear more dispersed for low-frequency concepts. Notably, in the subplot representing female conditions (located in the first column, second row), there is an unusual cluster of concepts positioned above the diagonal line. Further examination revealed that these concepts were male-specific and should not appear in the female population. Although there are a few instances of such cases in the source data, GPT amplified such cases in synthetic data. \n\n\\begin{Figure}\n    \\includegraphics[width=\\linewidth]{concept_prevalence.png}\n    \\captionof{figure}{Concept prevalence comparison between the source OMOP and generated OMOP using top $p=95\\%$ in the log scale stratified by domain in columns and by population in rows, where x-axis and y-axis represent the source and the synthetic data respectively, and each dot represents a concept}\n    \\label{concept_prevalence}\n\\end{Figure}\n\nIn addition, we conducted a detailed comparison of the visit tables between the two OMOP datasets. Our specific focus was on performing demographic breakdowns and analyses for gender, race, and age group. The supplementary Figures \\ref{visit_age_breakdown}, \\ref{visit_gender_breakdown}, \\ref{visit_race_breakdown}  highlight the top 10 most prevalent visits for each demographic breakdown, showcasing that the trends in both the source and generated visit tables exhibited notable similarities.\n\n\\subsubsection{Co-occurrence Relationship}\nTo measure how closely the generated datasets resemble the source, we computed the KL divergence between their co-occurrence matrices. The matrix was constructed temporally with the following logic: 1) for each concept in the patient sequence, only the future concepts were taken into consideration for creating concept pairs; 2) each patient could only contribute to the same concept pair once; 3) the matrix was normalized into a proper probability distribution by dividing the occurrences of each concept pair by the overall number of pairs. Additionally, we set benchmarks for our analysis: a lower-bound and an upper-bound. The lower bound was determined by applying the KL divergence method to two random samples from the source data. The upper bound was established by creating a theoretical co-occurrence matrix under the assumption that all concepts in the source data were independent. The KL divergence was then applied to this hypothetical matrix to calculate the upper bound. Figure \\ref{cooccurrence_cleveland} illustrates that the datasets with CEHR-BERT and top\\_k=300 most closely approach the lower bound, with the top\\_p=95\\% and GPT-OUTPAT baselines coming next. Datasets with top\\_k=100 and top\\_k=200 had marginally higher KL divergence values, while the top\\_p=100\\%, the GPT-Vanilla datasets exhibited the largest KL divergence. \n\nTo thoroughly examine the similarities between the original and the synthetic data, we carried out a qualitative analysis using one of the synthetic datasets. This involved comparing the most frequent pairs of co-occurring concepts within each category, such as condition-condition (interpreted as one condition concept followed by another condition). \n\n\\begin{Figure}\n    \\includegraphics[width=\\linewidth]{cooccurrence_cleveland.png}\n    \\captionof{figure}{KL divergence associated with different synthetic data. The closer to the lower bound in the bottom left corner, the better the synthetic data.}\n    \\label{cooccurrence_cleveland}\n\\end{Figure}\n\n\\begin{Figure}\n    \\includegraphics[width=\\linewidth]{concept_conditional_prevalence.png}\n    \\captionof{figure}{Top 100 co-occurring concept pairs for each co-occurrence category e.g. condition-condition interpreted as a condition concept followed by another condition concept. The x and y axes represent the synthetic and source data respectively}\n    \\label{concept_conditional_prevalence}\n\\end{Figure}\n\nThe results, illustrated in Figure \\ref{concept_conditional_prevalence}, display the top 100 pairs for each type of co-occurrence relationship. The analysis revealed that the synthetic data accurately mirrored the co-occurrence patterns in most categories, with the exception of the categories ending with a drug concept (as shown in the second column of Figure \\ref{concept_conditional_prevalence}), where the data points were more scattered. Notably, a majority of the points in condition-condition and procedure-procedure pairs aligned along the diagonal line.\n\n\\begin{Figure}\n    \\includegraphics[width=\\linewidth]{diabetes_comparison.png}\n    \\captionof{figure}{Comparison of temporal co-occurrence networks around Type 2 Diabetes Mellitus (T2DM) for real (left) and synthetic data (right). Networks were constructed with T2DM's (large circle) top 5 co-occurring condition concepts and their own top 5 co-occurring condition concepts. The metrics Prevalence (top) and Pointwise Mutual Information-3 (PMI$^3$) (bottom) were used to quantify co-occurrence. Gold edges indicate edges that are shared by both real and synthetic data, edge thickness indicates the strength of co-occurrence, and arrow direction indicates the direction of the temporal association.} \n    \\label{Diabetes_network}\n\\end{Figure}\n\nFor an in-depth analysis, we focused on examining the co-occurrence relationships associated with Type 2 Diabetes Mellitus (T2DM). Figure \\ref{Diabetes_network} qualitatively compares the real and top\\_p=95\\% synthetic data networks of condition concepts that co-occur around T2DM. The degree of co-occurrence is calculated with prevalence, which evaluates the frequency of concept co-occurrence, and Pointwise Mutual Information-3 (PMI$^3$), which evaluates the probabilistic association of concepts. There is much overlap between the real and synthetic networks when prevalence is used to construct them, but connections are more disparate when PMI$^3$ is used. \n\n\\subsubsection{Predictive performance}\nFor this analysis, we constructed five prediction tasks using the method described in \\cite{Pang2021} and the Book of OHDSI \\cite{ohdsi2019book}. Table \\ref{tab:cohort_definition} shows the cohort and the corresponding definition.\n\\begingroup\n\\setlength{\\tabcolsep}{10pt} % Default value: 6pt\n\\renewcommand{\\arraystretch}{1.8} % Default value: 1\n\\begin{table*}[t]\n  \\centering\n  \\begin{tabular}{p{4.0cm}p{12.0cm}}\n  \\toprule \n  \\textit{Cohort}  & \\textit{Definition}\\\\\n  \\midrule\n    HF readmission & HF patients who have a 30-day all-cause readmission. Observation window: 360 days, Prediction windows 30 days \\\\\n    Hospitalization & 2-year risk of hospitalization starting from the 3rd year since the initial entry into the EHR system. Observation window: 540 days, hold-off window: 180 days, prediction windows: 720 days  \\\\\n    COPD readmission & COPD  patients who have a 30-day all-cause readmission. Observation window: 360 days, prediction windows: 30 days \\\\\n    Afib ischemic stroke & Afib patients with 1-year risk since the initial diagnosis of afib ischemic stroke. Observation window: 720 days, prediction windows: 360 days \\\\\n    CAD CABG & Patients initially diagnosed with Coronary Arterial Disease (CAD) without any prior stent graft will receive the  Coronary artery bypass surgery (CABG) treatment. Observation window: 720 days, prediction windows: 360 days\\\\\n    \\hline\n  \\end{tabular}\n  \\caption{Cohort definitions}\n  \\label{tab:cohort_definition}\n\\end{table*}\n\\endgroup\nTo extract features, we first rolled up the medical concepts using ontological hierarchies to reduce dimensionality [see supplementary materials] and used the bag-of-word (BOW) approach, where we counted the frequency of each concept in a given observation window. For each task, we split the cohort data (both synthetic and real) into training and testing sets with a split ratio of 85:15. We ran logistic regression using Sklearn's implementation with the default configuration. Finally, the area under the receiver operating characteristics curve (AUC) was calculated using the test set. In addition, we reported PR-AUC (precision-recall) due to the class imbalance often present in EHR data. Table \\ref{tab:prediction_model_results} shows the prevalence of the positive cases, ROC-AUC, and PR-AUC for each synthetic data. The metrics associated with the baseline CEHR-BERT are reported in the Supplementary Materials Table \\ref{tab:supple_prediction_model_results}. For easy comparison of synthetic datasets, we defined a consolidated distance metric as the weighted average of the relative differences of the three aforementioned metrics,\n\\[\n    dist = \\frac{|\\delta_{Pre}|\\times 0.5}{Pre_{true}} + \\frac{|\\delta_{AUC}| \\times 0.25}{AUC_{true}} + \\frac{|\\delta_{PR}| \\times 0.25}{PR_{true}}\n\\]\nwhere $\\delta_{Pre}$, $\\delta_{AUC}$, and $\\delta_{PR}$ represent the differences in prevalence, ROC-AUC, and PR-AUC between the source and the synthetic data; $Pre_{true}$, $AUC_{true}$, and $PR_{true}$ denote the ground truth metrics generated from the source data. \n\nFigure \\ref{ml_combined_metrics} presents the distance metrics for various synthetic datasets across different cohorts. The dataset created with top\\_k=300 displayed the best performance in HF and COPD Readmission but was less effective in other cohorts. The top\\_p=95\\% dataset maintained consistent performance levels, ranking as the best in Hospitalization and Afib ischemic stroke,  and as second-best in HF and COPD readmission. In comparison, the GPT-OUTPAT baseline dataset exhibited a slightly higher divergence across the Hospitalization, Afib Ischemic Stroke, and CAD CABG groups. However, the CEHR-BERT baseline showed similar patterns in most cohorts, with a notably lower divergence in CAD CABG. It is noted that both GPT-OUTPAT and CEHR-BERT were not included in the HF Readmission and COPD Readmission analyses due to the absence of synthetic patients meeting the selection criteria for these cohorts. Other synthetic datasets, specifically those generated with top\\_p=100\\% and top\\_k=200 lagged behind in performance. Interestingly, top\\_k=100 showed a unique pattern, being the closest in the distance for the CAD CABG cohort but underperforming in the others. \n\n\\begin{Figure}\n    \\includegraphics[width=\\linewidth]{ml_combined_metrics.png}\n    \\captionof{figure}{The consolidated distance metrics for different synthetic datasets stratified by cohort. GPT-Vanilla and CEHR-BERT were omitted as the cohorts couldn't be constructed due to the loss of temporal information. GPT-OUTPAT was omitted from HF readmission and COPD readmission as the cohorts for the same reason.}\n    \\label{ml_combined_metrics}\n\\end{Figure}\n\n\\begingroup\n\\setlength{\\tabcolsep}{4pt} % Default value: 6pt\n\\footnotesize\n\\begin{table*}[hbt]\n  \\centering\n  \\begin{tabular}{c|c|cccccc}\n  \\toprule \n  \\textit{Cohort}  & \\textit{Real} & \\textit{p=95\\%} & \\textit{p=100\\%} & \\textit{k=100} & \\textit{k=200} & \\textit{k=300} & \\textit{GPT-OUTPAT} \\\\\n  \\midrule\nHF readmission & \\makecell{\\\\Pre = 25.7\\\\AUC = 65.7\\\\PR = 39.3} & \\makecell{\\\\Pre = 27.6\\\\AUC = 69.2\\\\PR = 45.7} & \\makecell{\\\\Pre =27.7 \\\\ AUC = 52.4 \\\\ PR = 29.0} & \\makecell{\\\\Pre = 30.7 \\\\ AUC = 68.1 \\\\ PR = 47.8} & \\makecell{\\\\Pre = 29.3 \\\\AUC = 54.0 \\\\PR = 32.9} & \\makecell{\\\\Pre = 26.5 \\\\ AUC = 61.1 \\\\ PR = 33.8} & \\makecell{\\\\Pre =100.0 \\\\ AUC = NA \\\\ PR = NA}\\\\\n    Hospitalization & \\makecell{\\\\Pre = 5.6 \\\\ AUC = 75.3 \\\\ PR = 19.5} & \\makecell{\\\\Pre = 5.2 \\\\ AUC = 77.1 \\\\ PR = 21.4} & \\makecell{\\\\Pre = 7.4 \\\\ AUC = 71.3 \\\\ PR = 20.2} & \n    \\makecell{\\\\Pre = 2.8 \\\\ AUC = 87.0 \\\\ PR = 22.1} &\n    \\makecell{\\\\Pre = 5.2 \\\\ AUC = 84.2 \\\\ PR = 20.8} & \\makecell{\\\\Pre = 6.3 \\\\ AUC = 78.7 \\\\ PR = 24.6} & \\makecell{\\\\Pre = 5.2 \\\\ AUC = 70.2 \\\\ PR = 14.3}\\\\\n    COPD readmission &  \\makecell{\\\\Pre = 34.5 \\\\ AUC = 74.2 \\\\ PR = 83.8} & \\makecell{\\\\Pre = 37.8 \\\\ AUC = 76.4 \\\\ PR = 84.4} & \\makecell{\\\\Pre = 47.2 \\\\ AUC = 74.1 \\\\ PR = 67.2} & \n    \\makecell{\\\\Pre = 26.4 \\\\ AUC = 75.9 \\\\ PR = 90.3} &\n    \\makecell{\\\\Pre = 28.3 \\\\ AUC = 70.1 \\\\ PR = 82.8} & \\makecell{\\\\Pre = 34.5 \\\\ AUC = 68.8 \\\\ PR = 80.2} & \\makecell{\\\\Pre = NA \\\\ AUC = NA \\\\ PR = NA}\\\\\n    Afib ischemic stroke & \\makecell{\\\\Pre = 8.7 \\\\ AUC = 84.0 \\\\ PR = 48.5} & \\makecell{\\\\Pre = 10.2 \\\\ AUC = 78.9 \\\\ PR = 41.2} & \\makecell{\\\\Pre = 10.4 \\\\ AUC = 70.7 \\\\ PR = 39.1} & \\makecell{\\\\Pre = 16.6 \\\\ AUC = 77.1 \\\\ PR = 50.5} & \\makecell{\\\\Pre = 15.8 \\\\ AUC =68.9 \\\\ PR = 36.6} & \\makecell{\\\\Pre = 10.8 \\\\ AUC = 76.8 \\\\ PR = 38.5} & \\makecell{\\\\Pre = 9.7 \\\\AUC = 67.2 \\\\PR = 27.2}\\\\\n    CAD CABG & \\makecell{\\\\Pre = 7.1 \\\\ AUC = 88.4 \\\\ PR = 55.9} & \\makecell{\\\\Pre = 4.1 \\\\ AUC = 81.5 \\\\ PR = 25.2} & \\makecell{\\\\Pre = 4.4 \\\\ AUC = 52.9 \\\\ PR = 4.3} & \\makecell{\\\\Pre = 7.2 \\\\ AUC = 84.7 \\\\ PR = 31.3} & \\makecell{\\\\Pre = 4.9 \\\\ AUC = 73.5 \\\\ PR = 24.3} & \\makecell{\\\\Pre = 4.0 \\\\ AUC = 79.0 \\\\ PR = 24.1\n} & \\makecell{\\\\Pre = 3.5 \\\\AUC = 81.5 \\\\PR = 44.4}\\\\\n\\hline\n  \\end{tabular}\n  \\captionsetup{width=.90\\textwidth}\n  \\caption{Logistic regression model performance across different datasets. In each cell, three numbers were reported including the prevalence of the positive cases, ROC-AUC, and PR-AUC}\n  \\label{tab:prediction_model_results}\n\\end{table*}\n\\endgroup\n\n\\subsection{Privacy Evaluations}\nWe adopted the privacy evaluation framework outlined by Yan \\textit{et al.} \\cite{Yan_Brad_2022}, focusing on quantifying the risk of privacy breaches through various types of attacks, including membership inference, attribute inference, meaningful identity disclosure, and nearest neighborhood adversary attacks (NNAA). We detailed each metric in the subsequent sections and summarized the results in Table \\ref{tab:privacy_metrics_results}, which include a comparison to the privacy risk quantification calculated by Yan \\textit{et al.} using the University of Washington (UW) dataset. Our results are comparable to those from all models tested by Yan \\textit{et al.}. Furthermore, according to Yan \\textit{et al.}, a risk score below $0.333$ for all defined metrics indicate a low risk to privacy \\cite{Yan_Brad_2022}.  Note that this evaluation framework requires a separate evaluation dataset not used in training. To ensure a fair evaluation, we held out $11,066$ patients ($0.5\\%$ of the training set) to create an evaluation set. Subsequently, we re-trained the model using the remaining patients, following the procedure detailed in Section \\ref{data_processing}. We then generated 1 million synthetic patients using a top\\_p$=95\\%$ specifically for evaluating the privacy metrics. \n\n\\begingroup\n\\setlength{\\tabcolsep}{4pt} % Default value: 6pt\n\\footnotesize\n\\begin{table*}[hb]\n  \\centering\n  \\begin{tabular}{c|c|cccccc}\n  \\toprule \n  \\textit{Privacy Metric}  & \\textit{CEHR-GPT} & \\textit{medGAN} & \\textit{medBGAN} & \\textit{EMR-WGAN} & \\textit{WGAN} & \\textit{DPGAN} \\\\\n  \\midrule\nAttribute Inference & 0.027 & 0.0078 & 0.0117 & 0.0680 & 0.0042 & 0.0136 \\\\\n    Membership Inference & 0.1266 & 0.1506 & 0.1828 & 0.2966 & \n    0.1758 &\n    0.0000 \\\\\n    Meaningful Identity Disclosure &  0.002 & 0.0021 & \n    0.0027 &\n    0.00361 &  0.0034 & 0.0004 \\\\\n    NNAA Risk &  -0.0011 & 0.0008 &  0.0004 & 0.0198 &  0.0085 & 0.0017\\\\\n\\hline\n  \\end{tabular}\n  \\captionsetup{width=.90\\textwidth}\n  \\caption{Privacy metrics computed on the synthetic data generated by CEHR-GPT. In addition, we include the benchmark metrics for the UW dataset from Yan, C. \\textit{et al.}. \\cite{Yan_Brad_2022}}\n  \\label{tab:privacy_metrics_results}\n\\end{table*}\n\\endgroup\n\n\\subsubsection{Membership Inference Attack}\nIn a membership inference attack, the objective is to determine whether a real patient record was utilized in the training of a generative model using the synthetic data \\cite{Yan_Brad_2022, Theodorou2023}. Such attacks can occur if the model overfits to the training data, causing the synthetic data to resemble the training dataset more closely than the non-training dataset.\n\nThe attack dataset was constructed as follows: 1) We generated negative examples by randomly selecting 10,000 samples from the holdout set and labeling them as negative; 2) We created positive examples by randomly selecting 10,000 patients from the training set and labeling them as positive. We then matched the patients in the attack dataset to the training dataset by calculating Euclidean distance between the patients in attack dataset and training dataset. Following the guidelines suggested in \\cite{Yan_Brad_2022}, we used a threshold distance of 5, below which patients were classified as positive (i.e., their data is present in the training dataset) and above which as negative (i.e., their data is not present in the training dataset). This procedure was repeated 100 times to ensure robustness. We evaluated the method by calculating recall, precision, and the F1 score based on the ground truth labels and the inferred predictions. The F1 score was then used to assess the risk associated with the data.  The full results can be found in Supplementary Materials \\ref{mem_inf_supp}.\n\n\\subsubsection{Attribute Inference Attack}\nIn the attribute inference attack, we assumed the attacker has access to a group of target patients along with their demographic data, including age, gender, race, the year of their initial clinical visit, and prevalent clinical conditions like hypertension, abdominal pain, chest pain, etc. With these features, the attacker identifies the patient in the synthetic dataset with the closest attribute resemblance to the target individual. Then the attacker aims to use the sensitive attributes of the matched patient from the synthetic dataset to infer the corresponding sensitive attributes of the target patient.\n\nTo evaluate the attribute inference risk, we randomly sampled 150,000 patients from the real patient dataset as a target group. The search group comprised 1 million generated synthetic patients. To find the most similar patients from the search group, we created a set of common attributes including the demographic data along with the top 1\\% of the most prevalent condition concepts, represented as one-hot encoded features. Then a k-nearest neighbors (KNN with k = 1) algorithm was applied to each target patient and a synthetic patient with the smallest Euclidean distance was found. Finally, we extracted the sensitive attributes (condition concepts not in the top 1\\% tier) from matched target and synthetic patients. F1 scores were computed for the sensitive attributes of each matched patient pair and aggregated across all matched patients.\n\nA baseline analysis was conducted by comparing real patients to other real patients. A result lower than the baseline suggests that the likelihood of finding synthetic data similar to a real patient is lower than finding a real patient similar to a real patient who share sensitive attributes. This implies a lower attribute inference risk which could be acceptable. Our training set was randomly divided into two halves, with 1 million real patients assigned as the target group, and the remaining 1 million as the search group. The matching process was consistently applied and an aggregated F1 score was computed. The full results can be found in Supplementary Materials \\ref{att_inf_supp}.\n\n\\subsubsection{Meaningful Identity Disclosure Attack}\nThe meaningful identity disclosure risk measures the likelihood of identifying a synthetic record by linking it to patient records in an identified population dataset that contains the training data. This linkage could potentially reveal sensitive information about individuals in the population. Adopted from the metrics developed by \\cite{el2020evaluating}, we can calculate the metrics using the following equation:\n\\small\n\\[\n    max\\Big(\\frac{1}{N}\\sum^n_{s=1}(\\frac{1}{f_s}\\times\\frac{1+\\lambda_s}{2}\\times I_s \\times R_s), \\frac{1}{n}\\sum^n_{s=1}(\\frac{1}{F_s}\\times\\frac{1+\\lambda_s}{2}\\times I_s \\times R_s)\\Big)\n\\]\n\\normalsize\nwhere $N$ denotes the size of the population cohort, $n$ denotes the size of the real sample, $s$ denotes the record index in the real sample, $f_s$ represents the number of records matched with record $s$ in the real sample and $F_s$ represents the number of records matched with record $s$ in the population cohort. $\\lambda_s$ is the upper bound of the adjustment parameter for incorrect matches. Following the same configuration in \\cite{el2020evaluating}, the midpoint between $\\lambda_s$ and the maximum value of 1 is used to be more conservative. $I_s$ is the binary indicator if any real record is matched with the synthetic record. $R_s$ is another binary indicator if the adversary could use at least L\\% sensitive attributes from matched synthetic records to learn something new about the target.\n\nTo assess the risk, we implemented the risk model applied by \\cite{Yan_Brad_2022} on the CUIMC patient population. We used 3.7M as the population cohort, 2.3M patients as the real sample, and 1M synthetic patients as the target cohort. The matching was performed using 10 QID (Quasi Identifiers) which consist of age, gender, and race demographic attributes and 7 most common diseases in CUIMC patients. Moreover, we established $\\lambda_s$ at 0.23, setting it as the upper bound to gauge the maximum risk level. Since we have around 17K attributes in total across the condition, drug, and procedure domains, to be aligned with the threshold in \\cite{Yan_Brad_2022}, we chose 0.1 for L, which resulted in approximately 17 sensitive attributes. We further assessed the risk scores based on the matching across two generalization levels of age group (e.g. age 32 vs age group 30-40) and any combinations of 7 common diseases, resulting in 256 runs. The full results can be found in Supplementary Materials \\ref{reid_supp}.\n\n\\subsubsection{Nearest Neighbor Adversary Attack}\nThe Nearest Neighbor Adversary Attack (NNAA) risk is a measure that assesses model overfitting by comparing the closeness of training and synthetic data. If the distance between the training and synthetic data is less than that between the evaluation and synthetic data, it indicates potential overfitting. This concept forms the basis for the metric developed by the researchers. Let's denote $S_T$, $S_E$, and $S_S$ to be random samples from the training, evaluation, and synthetic datasets. The risk score is defined as the following,\n\\[\n    AA_{ES} - AA_{TS}\n\\]\nwhere $AA_{ES}$ and $AA_{TS}$ are defined below,\n\\small \n\\[\n    AA_{ES} = \\frac{1}{2} \\Big( \\frac{1}{n} \\sum_{i=1}^n \\mathbbm{1} \\big(d_{ES}(I) > d_{EE}(i) \\big) + \\frac{1}{n} \\sum_{i=1}^n \\mathbbm{1} \\big(d_{SE}(i) > d_{SS}(i) \\big) \\Big)\n\\]\n\\[\n    AA_{TS} = \\frac{1}{2} \\Big( \\frac{1}{n} \\sum_{i=1}^n \\mathbbm{1} \\big(d_{TS}(i) > d_{TT}(i) \\big) + \\frac{1}{n} \\sum_{i=1}^n \\mathbbm{1}\\big(d_{ST}(i) > d_{SS}(i) \\big) \\Big)\n\\]\n\\normalsize\nHere, $d_{ES}(i)=\\min_j||x_E^i - x_S^j||$ indicates the distance between the evaluation data point $x_E^i \\in S_E$ and its nearest synthetic neighbor $x_S^j \\in S_S$; $d_{EE}(i)=\\min_{j, j \\neq i}||x_E^i - x_E^j||$ represents the distance between the evaluation data point $x_E^i$ and its nearest neighbor within the same evaluation set $S_E$, excluding itself. Analogous definitions apply for $d_{SE}$, $d_{SS}$, $d_{TS}$, $d_{TT}$, $d_{ST}$, and $d_{SS}$. For our study, we randomly chose samples of 10,000 patients from the training, evaluation, and synthetic datasets to compute the metric as per the outlined formula. The full results can be found in Supplementary Materials \\ref{nnaa_supp}.\n\n\\section{Discussion}\nTo the best of our knowledge, this is the first attempt to utilize GPT for generating time-series heterogeneous EHR data, while preserving patient privacy. Our main contribution lies in designing a novel patient representation that preserves a complete timeline of the patient's history, along with crucial visit details, thereby enabling GPT to create realistic patient sequences. Importantly, this representation facilitates seamless conversion back to the OMOP format, simplifying dissemination and analysis. This patient representation could serve as an effective messenger for transferring information across various standard data models. At present, our system is tailored to the OMOP format. However, it is designed with adaptability in mind, enabling us to seamlessly integrate new encoder/decoder pairs. This flexibility would facilitate the conversion of patient sequences to other widely-used data models, such as i2b2\\cite{murphy2010serving}.\n\nThe study undertakes a three-tiered evaluation approach, systematically comparing synthetic and real datasets based on their marginal (column-wise distribution), conditional (co-occurrence relationship), and joint distributions (predictive performance). Concurrently, as the evaluation progresses through these three levels, there is a corresponding escalation in the complexity and challenge of the tasks involved. The outcomes of the KL divergence analysis revealed a nuanced relationship between the top k/p sampling strategies and the performance across the evaluation levels. Specifically, an increase in top k/p values enhanced performance in level 1 concept prevalence. However, an excessively high or low top k/p value adversely affects both level 2 co-occurrence metrics and level 3 machine learning predictions. This pattern suggests that including more tokens in the predictive distribution introduces greater uncertainty and a wider array of potential patient trajectory variations in the data generation process, thereby escalating the difficulty of achieving comparable performance outcomes.\n\nThe sampling strategies of top\\_p=95\\% and top\\_k=300 seem to be most effective for generating the synthetic data. For instance, the synthetic data created with top\\_p=95\\% demonstrates the second smallest divergence in both dimension-wise distribution and co-occurrence relationship. Simultaneously, the corresponding synthetic cohorts successfully replicated the performance metrics in all predictive tasks, with the exception of CAD CABG. Finally, the significance of this patient representation transcends synthetic data generation; we believe it has the potential to establish the groundwork for integrating time into patient representations across diverse EHR-based deep-learning models.\n\n\\subsection{Loss of Temporal Information}\nThe reason that CEHR-GPT replicated the performance metrics of the prediction tasks can be attributed to the use of time tokens in its underlying patient representation. The majority of the prediction problems are phrased as For a group of target patients who share similar characteristics, who would experience a particular medical event in one year from the index event? in EHR research, therefore maintaining a complete patient timeline is crucial for time-sensitive cohort constructions \\cite{ohdsi2019book}.\n\nWe claimed that the proposed patient representation had almost zero loss of temporal information, although this makes intuitive sense, there does not exist a formal metric to quantify this. To bridge this gap, we conceived a new metric named loss of temporal information (LOTI) to estimate the shrinkage of the patient timeline due to the use of the patient representation in an EHR dataset. Let's denote $T$ to be the time interval measured in days, ATT to be an artificial time token that represents a time interval (\\(W_0\\)), $F$ to be a function that maps $T$ to an ATT token (four days to \\(W_0\\)). In addition, let $G$ be the inverse function of $F$ that converts an ATT to $T$, moreover, we impose the constraint on G such that it takes the lower bound of ATT e.g., \\(W_0 \\implies 0\\) days. Formally, we define LOTI as the expected difference between the original time interval $T$ and the reconstructed time interval $G(F(T))$ as the following,  \n\\[\n  LOTI=E_{p(T)}\\Big[T-G\\big(F(T)\\big)\\Big]\n\\]\nwhere $P(T)$ is the probability of $T$ observed in the training data defined as, \n\\[\n  P(T)=\\frac{\\textit{Freq of T}}{\\Sigma\\textsuperscript{T} \\textit{Freq of T}}\n\\]\nWe computed LOTI for the patient representations utilized in CEHR-GPT and baseline models, shown in Figure \\ref{patient_history_comparison}. As Table \\ref{tab:loti} shows, CEHR-GPT has the least LOTI compared to the other patient representations while GPT-OUTPAT has a slightly higher time shrinkage because the inpatient duration was not retained. CEHR-BERT has a relatively large LOTI compared to the previous two representations due to the use of coarse ATT tokens. Finally, GPT-Vanilla has the most LOTI, which is equal to the expected length of the timeline in the source population due to the complete collapse of the timeline. \n\n\\begingroup\n\\setlength{\\tabcolsep}{6pt} % Default value: 6pt\n\\renewcommand{\\arraystretch}{2} % Default value: 1\n\\begin{table*}[t]\n  \\centering\n  \\begin{tabular}{cccc}\n  \\toprule \n  \\textit{Representation}  & \\textit{Between visit ATT token} & \\textit{Between inpatient span ATT token} & \\textit{LOTI}\\\\\n  \\midrule\n    CEHR-GPT & \\makecell{\\\\Day token for $T \\leq 1080$ \\\\ LT token for $T > 1080$ \\\\\\\\} & Day token & 7.739 \\\\\n    GPT-OUTPAT & \\makecell{Day token for $T \\leq 1080$ \\\\ LT token for $T > 1080$ \\\\\\\\} & N/A & 7.962 \\\\\n    CEHR-BERT &   \\makecell{Day token for $T < 7$ \\\\ Week token for $7 \\leq T < 30$ \\\\ Month token for $30 \\leq T < 360$ \\\\ LT token $T \\geq 360$}   &  N/A & 31.482 \\\\\n    GPT-Vanilla & N/A & N/A & 111.164 \\\\\n    \\hline\n  \\end{tabular}\n  \\caption{Loss of Temporal Information for Different Patient Representations}\n  \\label{tab:loti}\n\\end{table*}\n\\endgroup\n\n\\subsection{Time Invariance and Sensitivity}\nAcross all synthetic datasets, the dimension-wise distribution (marginal distribution) and co-occurrence relationship were well preserved, regardless of the patient timeline's integrity within the models used. Even with a high LOTI, baseline models such as CEHR-BERT accurately mirrored both marginal distribution and co-occurrence relationship, yielding results similar to those from CEHR-GPT. This implies that these two measures may be largely time-invariant, unaffected by any shrinkage in the patient timeline. The rationale behind this is rooted in their construction methods, which either disregard or marginalize the temporal factor. Marginal distribution was constructed by counting the unique number of patients associated with the target concept, which was then normalized by a constant. The construction disregarded temporality, as the placement of a concept on the timeline was not a factor of consideration. The co-occurrence matrix was created using time initially but was marginalized after all co-occurring pairs were collected from the patient population. \n\nOn the contrary, the predictive performance is extremely sensitive to any change made to the patient timeline as the cohort construction requires the integrity of the patient timeline. For example, HF readmission requires a 30-day prediction window from the index event (defined as the hospitalization episode with a heart failure diagnosis). Any shrinkage to the patient timeline will disrupt the construction of this cohort. The synthetic HF readmission cohort produced by CEHR-BERT showed a readmission rate of 100\\% due to the shrinkage of the timeline in this patient representation; whereas, the actual expected rate of readmission should be approximately 25\\%. Compared to CEHR-GPT, GPT-OUTPAT encoded time intervals between visits but did not preserve the duration of inpatient visits, therefore having a slightly higher LOTI. As a consequence, it showed reasonable performance in the cohorts (Hospitalization and Afib ischemic stroke), which had a 360-day prediction window and were thus less impacted by timeline shrinkage. However, in the case of HF Readmission (where a 100\\% readmission rate was observed) and COPD Readmission (which identified no patients), GPT-OUTPAT was less successful. These cohorts used a short 30-day prediction window, making them highly sensitive to any distortions in the timeline, which likely led to synthetic patients not meeting the cohort selection criteria. Interestingly, the CAD CABG cohort presents a notable deviation from the general trend, where the CEHR-GPT dataset with top\\_k=100 outperformed both the top\\_p=95\\% and top\\_k=300 configurations. Additionally, the CEHR-BERT synthetic data accurately replicated the machine learning performance metrics as well. This indicates that the CAD CABG cohort was less affected by time shrinkage. \n\nTherefore, selecting the appropriate patient representation is pivotal in maintaining specific properties of the source data when generating synthetic data. The choice hinges on the intended application of synthetic data, ensuring that critical features and patterns inherent to the patient information are accurately reflected and retained.\n\n\\subsection{Time Sensitive Forecasting}\nBecause the patient representation encodes all the temporal information in the sequence, the trained GPT model could be used potentially for time-sensitive forecasting. We could prompt the trained GPT model with a patient history and estimate the time of the next visit via a Monte Carlo Sampling approach shown in the following equation, \n\\[\n    P(\\delta_{t}|h) \\approx \\frac{\\sum^n_{i=1}\\mathbbm{1}\\Big[M_{gpt}(h) = \\delta_{t} \\Big]}{n} \n\\]\nwhere $M_{gpt}$ denotes the GPT model, $h$ denotes a patient history, $\\delta_t$ denotes any time interval, and $n$ represents the number of samples. Then we can use the expectation $E\\big(\\delta_{t}\\big)$ as the predicted time interval. In addition, we can quantify the confidence by calculating the standard deviation e.g. $sd(\\delta_{t})$. Similarly, we can predict the visit type ($v$)  using the same Monte Carlo approach.\n\\[\n    P(v|E\\big(\\delta_{t}\\big), h) \\approx \\frac{\\sum^n_{i=1}\\mathbbm{1}\\Big[M_{gpt}\\Big(E\\big(\\delta_{t}\\big), h\\Big) = v \\Big]}{n} \n\\]\nFinally, we can predict the most likely medical event ($c$) given the predicted visit type $v$,\n\\[\n    P(c|v, E\\big(\\delta_{t}\\big), h) \\approx \\frac{\\sum^n_{i=1}\\mathbbm{1}\\Big[M_{gpt}\\Big(v, E\\big(\\delta_{t}\\big), h\\Big) = c \\Big]}{n} \n\\]\nThis approach goes beyond conventional prediction methods by not only forecasting future medical events but also determining the timing of the next visit and the specific medical events associated with that visit type. This predictive model could provide a more detailed and actionable timeline for patient care.\n\n\\subsection{Limitations}\nWhile synthetic datasets demonstrated a high degree of similarity to source data, they are subject to certain known constraints. Firstly, a selection bias was present in the training data due to the constraints on sequence length, ranging from 20 to 512. This limitation resulted in the partial inclusion of patients with chronic conditions, which typically require a longer context window for accurate representation. While extending the context window of the model could potentially address this issue, it may introduce unforeseen effects. Finding the optimal configuration to accommodate a broader context window would require comprehensive experiments. \n\nSecondly, identifying an optimal sampling strategy for generating synthetic data remains a challenge due to the presence of numerous hyperparameters such as temperature, top\\_p, and top\\_k. These parameters, when used in conjunction, could yield a wide array of configurations. While the top\\_p=95\\% strategy showed the least divergence from the source data, it was unable to accurately replicate performance metrics for CAD CABG. As an interim solution, we may publish multiple versions of synthetic data, along with their corresponding performance metrics. This approach would allow researchers to select the most suitable dataset for their specific use case. \n\nThirdly, the GPT model showed a propensity to over-represent prevalent concepts, skewing towards those with higher frequencies in the dataset. An illustration of this is seen in the synthetic data, where 78\\% of patients had at least one outpatient visit, in contrast to the actual data where this figure was 73\\%. This discrepancy indicates a bias in the model towards more common occurrences. In addition, the co-occurrence analysis also demonstrated that the synthetic reconstruction faithfully represents the frequent concept pairs in the original data but may be less effective at recovering the finer associations between rare concepts as shown in Figure \\ref{Diabetes_network}. To address the over-representation of prevalent concepts by the GPT model, future work will look into implementing regularization techniques. One promising approach is adaptive regularization, which can be outlined in several steps: 1) Model Training: Begin by training the GPT model for a predetermined number of steps; 2) Sequence Generation and Analysis: Generate a sample of patient sequences from the trained model and calculate the marginal distribution of the concepts within these sequences; 3) Distribution Comparison and Adjustment Score Calculation: Compare the model-generated distribution to the empirical distribution derived from the actual data. From this comparison, calculate an adjustment score for each concept; 4) Logit Adjustment: Modify the logits for each concept in the model according to the calculated adjustment scores. By implementing this procedure, the influence of each concept on the models learning process would be adaptively modified during back-propagation, allowing for an update in the model parameters that takes into account the disparity between the generated and actual data distributions. This should help in reducing the bias towards over-represented concepts.\n\nFurthermore, improving patient representation is an area for further development. Currently, the model's representation is limited to daily intervals and does not capture more precise measurements like hours or minutes. This limitation is particularly relevant for intensive care unit (ICU) data where time-sensitive decisions are critical. Furthermore, the current model framework assigns the first visit of every synthetic patient to the start of the year, as it only includes a year token to denote the commencement of patient history. To refine the accuracy of patient history commencement, integrating a month token alongside the year token is being considered to properly represent seasonality. This would provide a more accurate reconstruction of the starting point for a patient's first visit in the generated data. \n\nLastly, there is a necessity to incorporate the death event within patient sequences. Including this event would allow the synthetic data to more accurately represent mortality, enhancing its utility for predictions related to patient outcomes and lifespan. These enhancements aim to create a more precise and clinically relevant synthetic dataset that better mirrors the complexities of real-world patient trajectories.\n\n\\section{Conclusion}\nTo our knowledge, this is the first attempt to utilize GPT for generating time-series EHR data. Our main contribution lies in the design of a patient representation that captures temporal dependencies among token types, enabling GPT to generate realistic patient sequences. Moreover, this representation allows for easy conversion back to the OMOP format. Comprehensive evaluations showed that the synthetic data effectively captures the intricate patterns present in EHR data.\n\n\\end{multicols}\n\n\\newpage\n\\section{Supplementary Materials}\n\\subsection{Patient Representation comparison}\n\\begin{Figure}\n    \\centering\n    \\includegraphics[width=0.8\\linewidth]{patient_history_comparison.jpg}\n    \\captionof{figure}{The comparison of the patient representations. }\n    \\label{patient_history_comparison}\n\\end{Figure}\n\n\\newpage\n\\subsection{Synthetic cohort comparison by patient representation}\n\\begingroup\n\\setlength{\\tabcolsep}{4pt} % Default value: 6pt\n\\footnotesize\n\\begin{table*}[tbh]\n  \\centering\n  \\begin{tabular}{c|c|cc}\n  \\toprule \n  \\textit{Cohort}  & \\textit{Real} & \\textit{GPT-OUTPAT} & \\textit{CEHR-BERT}\\\\\n  \\midrule\nHF readmission & \\makecell{\\\\Pre = 25.7\\\\AUC = 65.7\\\\PR = 39.3} & \\makecell{\\\\Pre =100.0 \\\\ AUC = NA \\\\ PR = NA} & \\makecell{\\\\Pre =100.0 \\\\ AUC = NA \\\\ PR = NA}\\\\\n    Hospitalization & \\makecell{\\\\Pre = 5.6 \\\\ AUC = 75.3 \\\\ PR = 19.5}  & \\makecell{\\\\Pre = 5.3 \\\\ AUC = 70.2 \\\\ PR = 14.3} & \\makecell{\\\\Pre = 0.3 \\\\ AUC = 76.0 \\\\ PR = 1.0}\\\\\n    COPD readmission &  \\makecell{\\\\Pre = 34.5 \\\\ AUC = 74.2 \\\\ PR = 83.8} & \\makecell{\\\\Pre = NA \\\\ AUC = NA \\\\ PR = NA}  &  \\makecell{\\\\Pre = NA \\\\ AUC = NA \\\\ PR = NA}\\\\\n    Afib ischemic stroke & \\makecell{\\\\Pre = 8.7 \\\\ AUC = 84.0 \\\\ PR = 48.5} & \\makecell{\\\\Pre = 9.7 \\\\AUC = 67.2 \\\\PR = 27.2} & \\makecell{\\\\Pre = 19.5 \\\\AUC = 72.4 \\\\PR = 60.0} \\\\\n    CAD CABG & \\makecell{\\\\Pre = 7.1 \\\\ AUC = 88.4 \\\\ PR = 55.9} & \\makecell{\\\\Pre = 3.5 \\\\AUC = 81.5 \\\\PR = 44.4} & \\makecell{\\\\Pre = 5.4 \\\\AUC = 77.2 \\\\PR = 44.1}\\\\\n\\hline\n  \\end{tabular}\n  \\captionsetup{width=.60\\textwidth}\n  \\caption{Logistic regression (LR) model performance across baseline synthetic datasets. In each cell, three numbers were reported including the prevalence of the positive cases, ROC-AUC, and PR-AUC. If the prevalence=NA, this indicates that $0$ patients were identified in the cohort. AUC=NA and PR=NA indicate the LR model could not be run successfully due to either 100\\% or 0\\% prevalence. } \n  \\label{tab:supple_prediction_model_results}\n\\end{table*}\n\\endgroup\n\n\\subsection{Visit Prevalence analyses}\n\\begin{Figure}\n    \\centering\n    \\includegraphics[width=0.8\\linewidth]{visit_age_breakdown.png}\n    \\captionof{figure}{The visit prevalence stratified by age group}\n    \\label{visit_age_breakdown}\n\\end{Figure}\n\n\\begin{Figure}\n    \\centering\n    \\includegraphics[width=0.8\\linewidth]{visit_race_breakdown.png}\n    \\captionof{figure}{The visit prevalence stratified by race}\n    \\label{visit_race_breakdown}\n\\end{Figure}\n\n\\begin{Figure}\n    \\centering\n    \\includegraphics[width=0.8\\linewidth]{visit_gender_breakdown.png}\n    \\captionof{figure}{The visit prevalence stratified by gender}\n    \\label{visit_gender_breakdown}\n\\end{Figure}\n\n\\newpage\n\n\\subsection{Privacy Evaluation Metrics}\n\\subsubsection{Membership Inference} \\label{mem_inf_supp}\nTable \\ref{tab:membership_inference_results} shows that the accuracy of both attacks is slightly less than 50\\%, indicating that the performance of such attacks is worse than a random guess.\n\n\\begin{center}\n\\renewcommand{\\arraystretch}{1.2} % Default value: 1\n  \\begin{tabular}{ccccc}\n  \\toprule \n  \\textit{}  & \n  \\textit{Accuracy} & \\textit{Recall} & \\textit{Precision} & \\textit{F1}\\\\\n  \\midrule\n  Data Attack & $0.4997 \\pm 0.0008$ & $0.0725 \\pm 0.0026$ &  $0.4996 \\pm 0.0055$ & $0.1266 \\pm 0.0040$\\\\\n    \\hline\n  \\end{tabular}\n  \\captionof{table}{Membership Inference Attack metrics}\n  \\label{tab:membership_inference_results}\n\\end{center}\n\n\\subsubsection{Attribute Inference} \\label{att_inf_supp}\nTable \\ref{tab:attribute_inference_results} shows that the F1 score of synthetic vs real is less than real vs real scenario. But it's still higher than other models that didn't capture the real patterns as effectively. \\cite{Theodorou2023}\n\n\\begin{center}\n\\renewcommand{\\arraystretch}{1.2} % Default value: 1\n  \\begin{tabular}{ccccc}\n  \\toprule \n  \\textit{}  & \n  \\textit{Recall} & \\textit{Precision} & \\textit{F1}\\\\\n  \\midrule\n  Synthetic vs Real & 0.0350 & 0.0343 & 0.0271\\\\\n  Real vs Real & 0.0612 & 0.0468 &  0.0421\\\\\n    \\hline\n  \\end{tabular}\n  \\captionof{table}{Attribute Inference Attack metrics}\n  \\label{tab:attribute_inference_results}\n\\end{center}\n\n\\subsubsection{Meaningful Identity Risk} \\label{reid_supp}\nTable \\ref{tab:reid} displays the maximum risk scores for two terms in the equation, calculated across all 256 combinations.  The ultimate risk score, derived from the highest of these two components, is 0.002053. This risk score is significantly below the threshold of 0.09 proposed by the creator of this metric El Emam et al \\cite{el2020evaluating} to determine whether the synthetic data violates privacy. \n\n\\begin{center}\n\\renewcommand{\\arraystretch}{1.2} % Default value: 1\n  \\begin{tabular}{cc}\n  \\toprule \n  \\textit{First Term}  & \n  \\textit{Second Term} \\\\\n  \\midrule\n    0.001353 & 0.002053 \\\\\n    \\hline\n  \\end{tabular}\n  \\captionof{table}{Meaningful Identity Disclosure Attack}\n  \\label{tab:reid}\n\\end{center}\n\n\\subsubsection{Nearest Neighbor Adversary Attack} \\label{nnaa_supp}\nThis procedure was conducted five times to obtain an average distance difference $-0.0011$ as shown in Table \\ref{tab:nnaa}. Following the guidelines in \\cite{Yan_Brad_2022}, an NNAA risk below 0.03 is deemed minimal, validating the privacy adequacy of our synthetic data.\n\n\\begin{center}\n\\renewcommand{\\arraystretch}{1.2} % Default value: 1\n  \\begin{tabular}{cc}\n  \\toprule \n  \\textit{}  & \n  \\textit{NNAA} \\\\\n  \\midrule\n   Run 1 & -0.0031 \\\\\n   Run 2 & -0.0012 \\\\\n   Run 3 & -0.0045 \\\\\n   Run 4 & 0.0006 \\\\\n   Run 5 & 0.0024 \\\\\n   \\hline\n  \\end{tabular}\n  \\captionof{table}{Nearest Neighbor Adversary Attack}\n  \\label{tab:nnaa}\n\\end{center}\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2303.05656v3.tex",
        "arXiv-2304.02169v3.tex",
        "arXiv-2402.04400v2.tex"
    ],
    "group_id": "group_53",
    "response": "### Title: Synthesizing Electronic Health Records (EHR) Using Generative Models: A Comparative Analysis of Diffusion Models, Hierarchical Autoregressive Language Models, and GPT-based Methods\n\n### Introduction\nElectronic Health Records (EHRs) are crucial for advancing precision medicine and healthcare informatics. However, the widespread use of EHRs is often hindered by privacy concerns, legal restrictions, and ethical considerations, which limit the availability of high-quality, large-scale datasets for researchers. To overcome these limitations, synthetic EHR generation has emerged as a viable alternative. Generative models, such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and more recently, diffusion models and autoregressive language models, have been employed to create synthetic EHRs that mimic real-world data distributions. These models aim to provide realistic synthetic data while ensuring privacy protection. Despite significant progress, existing methods often struggle with generating high-dimensional, longitudinal EHRs without compromising the quality or privacy of the data. This summary explores three recent research papers that introduce novel approaches to EHR synthesis using diffusion models, hierarchical autoregressive language models, and Generative Pre-trained Transformers (GPTs).\n\n### Main Content of Each Paper\n\n#### Paper 1: \\modelname - Exploring Realistic EHR Synthesis with Diffusion Models\nThis paper introduces \\modelname, a diffusion model-based approach for generating realistic synthetic EHRs. Diffusion models, which have shown state-of-the-art performance in image generation, are adapted here to EHR synthesis. The authors argue that GANs, despite their success, suffer from training instability and mode collapse, leading to suboptimal synthetic data quality. \\modelname, on the other hand, uses a deterministic process to generate synthetic EHRs, which are more consistent with real-world data distributions.\n\nThe paper uses MIMIC-III, a publicly available EHR database, to evaluate the performance of \\modelname against several GAN-based methods. The MIMIC-III dataset includes 46,520 EHRs, each represented as a binary vector of 1,782 dimensions. The authors also extend their evaluation to datasets containing continuous and time-series features, such as CinC2012 and PTB-ECG data.\n\n**Key Contributions:**\n- Introduces diffusion models to EHR synthesis.\n- Proposes \\modelname, a diffusion-based method for generating synthetic EHRs.\n- Demonstrates superior quality and diversity in synthetic EHRs compared to GAN-based methods.\n- Evaluates utility and privacy metrics to ensure the synthetic data is both useful and secure.\n\n**Main Equations:**\n- The forward process of diffusion models corrupts real-world data by gradually injecting noise, described by the stochastic differential equation (SDE):\n\\[\n\\mathrm{d}\\boldsymbol{x} = f(\\boldsymbol{x}, t)\\mathrm{d}t + g(t)\\mathrm{d}\\boldsymbol{w}\n\\]\n- The reverse process generates synthetic data by gradually removing noise, described by:\n\\[\n\\mathrm{d}\\boldsymbol{x} = \\left(f(\\boldsymbol{x}, t)-g^2(t)\\nabla_{\\boldsymbol{x}}\\log p_t(\\boldsymbol{x})\\right)\\mathrm{d}t + g(t)\\mathrm{d}\\boldsymbol{w}\n\\]\n- The objective function for training the diffusion model is:\n\\[\n\\min_\\theta \\E_{p(\\boldsymbol{x}_0)p_{\\sigma_t}(\\boldsymbol{x}|\\boldsymbol{x}_0)}\\left[\\|D_\\theta(\\boldsymbol{x},\\sigma_t)-\\boldsymbol{x}_0\\|^2_2\\right]\n\\]\n\n**Evaluation Metrics:**\n- **Utility Metrics:** Dimension-wise distribution, dimension-wise correlation, and dimension-wise prediction.\n- **Privacy Metrics:** Attribute inference risk and membership inference risk.\n\n**Results:**\n- \\modelname outperforms baseline GAN models in terms of utility metrics, such as non-zero code columns (NZC) and correlation matrix distance (CMD).\n- \\modelname achieves intermediate results on privacy metrics, indicating a balanced trade-off between utility and privacy.\n- \\modelname generates high-quality synthetic data that can be used to train downstream biomedical models, improving their performance compared to models trained on real EHRs.\n\n#### Paper 2: Hierarchical Autoregressive Language Model (\\method) - Synthesizing High-dimensional Longitudinal EHRs\nThis paper proposes \\method, a hierarchical autoregressive language model for generating high-dimensional, longitudinal EHRs. The model is designed to handle the sequential and multi-granular nature of EHRs by generating an explicit probability distribution over medical codes, visits, and records. The authors argue that existing methods often aggregate or remove codes to reduce dimensionality, which limits their ability to generate realistic and comprehensive synthetic EHRs.\n\nThe paper evaluates \\method on two datasets: a comprehensive outpatient claims dataset and the MIMIC-III inpatient EHR data. The outpatient dataset contains 929,268 patients with binary labels for 11 chronic diseases, while the inpatient dataset includes 46,520 patients with 25 disease phenotype labels.\n\n**Key Contributions:**\n- Proposes \\method, a hierarchical autoregressive model for generating high-dimensional EHRs.\n- Demonstrates that \\method can accurately capture the underlying data distribution and generate realistic synthetic EHRs.\n- Evaluates the utility and privacy of synthetic EHRs using various metrics and datasets.\n\n**Main Equations:**\n- The hierarchical autoregressive model is formulated as:\n\\[\nP(\\mathbf{R}) = \\prod_t \\prod_i^C P(c_t^i|\\mathbf{v}_s, \\cdots, \\mathbf{v}_{t-1}, c_t^1, \\cdots, c_t^{i-1})\n\\]\n- The visit-level module uses a stack of transformer decoder blocks to generate visit-level histories:\n\\[\n\\mathbf{H}^{(m)} = \\text{transformer\\_block}(\\mathbf{H}^{(m-1)}) \\;\\;\\;\\forall \\:m \\in [1,M]\n\\]\n- The code-level module uses masked linear layers to generate code probabilities:\n\\[\n\\mathbf{H'}^{(n)} = \\max(0,\\mathbf{H'}^{(n-1)}(\\mathbf{W}^{(n)} \\odot \\mathbf{M}) + \\mathbf{b}^{(n)})\n\\]\n\n**Evaluation Metrics:**\n- **Utility Metrics:** Record length, visit length, unigram code probabilities, sequential visit bigram probabilities, and same visit bigram probabilities.\n- **Privacy Metrics:** Membership inference attack, attribute inference attack, meaningful identity disclosure, and nearest neighbor adversary attack.\n\n**Results:**\n- \\method outperforms all baseline models in terms of utility metrics, closely approximating the distribution of real EHRs.\n- \\method's synthetic data improves predictive modeling accuracy by more than 17\\% compared to leading baseline methods.\n- The model passes privacy evaluations, indicating a low risk of privacy breaches.\n\n#### Paper 3: CEHR-GPT - Generating Electronic Health Records with Chronological Patient Timelines\nThis paper presents CEHR-GPT, a framework for generating time-series EHRs using Generative Pre-trained Transformers (GPTs). The authors focus on preserving the temporal dependencies in patient histories, which are critical for many healthcare applications. CEHR-GPT uses a patient representation derived from CEHR-BERT to maintain temporal information, enabling the generation of realistic patient sequences that can be converted back to the Observational Medical Outcomes Partnership (OMOP) format.\n\n**Key Contributions:**\n- Designs a novel patient representation that captures visit types, discharge facilities, and all temporal data.\n- Treats patient sequence generation as a language modeling problem, allowing the use of GPT to learn the distribution of patient sequences.\n- Converts synthetic sequences to the OMOP format for easy dissemination and analysis.\n- Evaluates the synthetic EHR data using multi-level metrics, including dimension-wise distribution, co-occurrence relationship, and machine learning performance.\n\n**Main Equations:**\n- The loss of temporal information (LOTI) metric is defined as:\n\\[\n  LOTI=E_{p(T)}\\Big[T-G\\big(F(T)\\big)\\Big]\n\\]\n- The membership inference attack metric is:\n\\[\n    AA_{ES} - AA_{TS}\n\\]\n- The attribute inference attack metric is:\n\\[\n    max\\Big(\\frac{1}{N}\\sum^n_{s=1}(\\frac{1}{f_s}\\times\\frac{1+\\lambda_s}{2}\\times I_s \\times R_s), \\frac{1}{n}\\sum^n_{s=1}(\\frac{1}{F_s}\\times\\frac{1+\\lambda_s}{2}\\times I_s \\times R_s)\\Big)\n\\]\n\n**Evaluation Metrics:**\n- **Utility Metrics:** Dimension-wise distribution, co-occurrence relationship, and machine learning performance.\n- **Privacy Metrics:** Membership inference, attribute inference, meaningful identity disclosure, and nearest neighbor adversary attack.\n\n**Results:**\n- CEHR-GPT outperforms baseline models in terms of utility metrics, especially for time-sensitive tasks.\n- CEHR-GPT maintains a low risk of privacy breaches as indicated by the privacy metrics.\n- The model's performance is sensitive to the patient timeline's integrity, highlighting the importance of accurate temporal representation.\n\n### Commonalities and Innovations\nAll three papers address the challenge of generating realistic synthetic EHRs while preserving privacy. They utilize different generative models: diffusion models, hierarchical autoregressive language models, and GPT-based models. Each approach has its unique strengths and limitations:\n\n- **Diffusion Models (\\modelname):** Focus on deterministic generation processes to avoid mode collapse and ensure high-quality synthetic data.\n- **Hierarchical Autoregressive Language Models (\\method):** Emphasize the sequential and multi-granular nature of EHRs, generating explicit probability distributions over medical codes and visits.\n- **GPT-based Models (CEHR-GPT):** Utilize advanced language modeling techniques to capture temporal dependencies and convert synthetic sequences to standard data formats like OMOP.\n\n### Comparison of Results\nThe results from the three papers show varying degrees of success in generating high-quality synthetic EHRs and preserving privacy:\n\n- **Utility Metrics:**\n  - \\modelname outperforms GAN-based methods in terms of non-zero code columns (NZC) and correlation matrix distance (CMD), indicating better feature-level resemblance and feature correlation.\n  - \\method achieves the highest accuracy and F1 scores in predicting chronic disease labels, surpassing GAN-based methods and closely approximating real data performance.\n  - CEHR-GPT demonstrates the best performance in time-sensitive tasks, such as hospitalization and Afib ischemic stroke, while showing sensitivity to timeline integrity in other tasks.\n\n- **Privacy Metrics:**\n  - \\modelname achieves intermediate results, suggesting a balance between utility and privacy.\n  - \\method passes privacy evaluations, indicating low risk of privacy breaches.\n  - CEHR-GPT maintains low privacy risks, with results comparable to those from other models.\n\n### Conclusion\nThe three papers collectively advance the field of EHR synthesis by introducing novel methods that address the limitations of existing approaches. \\modelname leverages diffusion models to generate high-quality synthetic EHRs with minimal mode collapse, \\method uses a hierarchical autoregressive model to capture the sequential and multi-granular nature of EHRs, and CEHR-GPT employs GPT models to maintain temporal dependencies and convert synthetic sequences to standard formats.\n\n**Main Findings:**\n- Diffusion models (\\modelname) provide a robust alternative to GANs for generating high-dimensional EHRs.\n- Hierarchical autoregressive models (\\method) can accurately model both the shape and temporal dependencies of synthetic EHRs.\n- GPT-based models (CEHR-GPT) are effective in generating time-series EHRs while preserving privacy.\n\n**Future Research Directions:**\n- Further exploration of the impact of synthetic data scale on downstream model performance.\n- Development of more sophisticated patient representations to capture precise temporal information.\n- Implementation of regularization techniques to mitigate over-representation of prevalent concepts in GPT models.\n- Integration of additional data modalities, such as clinical notes and medical images, into synthetic EHR generation frameworks.\n- Comprehensive studies on the potential biases in synthetic EHR data and methods to mitigate them.\n\nBy synthesizing EHRs with these advanced methods, researchers can overcome the obstacles of limited publicly available real EHR data, enabling broader applications in healthcare informatics and precision medicine."
}