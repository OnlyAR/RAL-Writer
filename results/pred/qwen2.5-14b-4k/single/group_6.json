{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Schema-Guided Dialogue State Tracking Task at DSTC8}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nThis paper gives an overview of the \\textit{Schema-Guided Dialogue State Tracking} task of the 8th Dialogue System Technology Challenge. The goal of this task is to develop dialogue state tracking models suitable for large-scale virtual assistants, with a focus on data-efficient joint modeling across domains and zero-shot generalization to new APIs. This task provided a new dataset consisting of over 16000 dialogues in the training set spanning 16 domains to highlight these challenges, and a baseline model capable of zero-shot generalization to new APIs. Twenty-five teams participated, developing a range of neural network models, exceeding the performance of the baseline model by a very high margin. The submissions incorporated a variety of pre-trained encoders and data augmentation techniques. This paper describes the task definition, dataset and evaluation methodology. We also summarize the approach and results of the submitted systems to highlight the overall trends in the state-of-the-art.\n\\end{abstract}\n\n\\section{Introduction}\nVirtual assistants help users accomplish tasks including but not limited to finding flights, booking restaurants, by providing a natural language interface to services and APIs on the web. Large-scale assistants like Google Assistant, Amazon Alexa, Apple Siri, Microsoft Cortana etc. need to support a large and constantly increasing number of services, over a wide variety of domains. Consequently, recent work has focused on scalable dialogue systems that can handle tasks across multiple application domains. Data-driven deep learning based approaches for multi-domain modeling have shown promise, both for end-to-end and modular systems involving dialogue state tracking and policy learning. This line of work has been facilitated by the release of multi-domain dialogue corpora such as MultiWOZ \\cite{budzianowski2018multiwoz}, Taskmaster-1 \\cite{byrne2019taskmaster}, M2M \\cite{shah2018building} and FRAMES \\cite{el2017frames}.\n\nHowever, building large-scale assistants, as opposed to dialogue systems managing a few APIs, poses a new set of challenges. Apart from the handling a very large variety of domains, such systems need to support heterogeneous services or APIs with possibly overlapping functionality.  It should also offer an efficient way of supporting new APIs or services, while requiring little or no additional training data. Furthermore, to reduce maintenance workload and accommodate future growth, such assistants need to be robust to changes in the API's interface or addition of new slot values. Such changes shouldn't require collection of additional training data or retraining the model.\n\nThe Schema-Guided Dialogue State Tracking task at the Eighth Dialogue System Technology Challenge explores the aforementioned challenges in context of dialogue state tracking. In a task-oriented dialogue, the dialogue state is a summary of the entire conversation till the current turn. The dialogue state is used to invoke APIs with appropriate parameters as specified by the user over the dialogue history. It is also used by the assistant to generate the next actions to continue the dialogue. DST, therefore, is a core component of virtual assistants. \n\nIn this task, participants are required to develop innovative approaches to multi-domain dialogue state tracking, with a focus on data-efficient joint modeling across APIs and zero-shot generalization to new APIs. The task is based on the Schema-Guided Dialogue (SGD) dataset\\footnote{Available at github.com/google-research-datasets/dstc8-schema-guided-dialogue}, which, to the best of our knowledge, is the largest publicly available corpus of annotated task-oriented dialogues. With over 16000 dialogues in the training set spanning 26 APIs over 16 domains, it exceeds the existing dialogue corpora in scale. SGD is the first dataset to allow multiple APIs with overlapping functionality within each domain. To adequately test generalization in zero-shot settings, the evaluation sets contain unseen services and domains. The dataset is designed to serve as an effective testbed for intent prediction, slot filling, state tracking and language generation, among other tasks in large-scale virtual assistants.\n\n\\section{Related Work}\nDialogue systems have constituted an active area of research for the past few decades. The advent of commercial personal assistants has provided further impetus to dialogue systems research. As virtual assistants incorporate diverse domains, zero-shot modeling \\cite{bapna2017towards,xia2018zero,shah-etal-2019-robust}, domain adaptation and transfer learning techniques \\cite{yang2017transfer,rastogi2017scalable,zhu2018concept} have been explored to support new domains in a data efficient manner.\n\nDeep learning based approaches to DST have recently gained popularity. Some of these approaches estimate the dialogue state as a distribution over all possible slot-values \\cite{henderson2014,wen2017network}  or individually score all slot-value combinations \\cite{mrkvsic2017neural,zhong-etal-2018-global}. Such approaches are, however, hard to scale to real-world virtual assistants, where the set of possible values for certain slots may be very large (date, time or restaurant name) and even dynamic (movie or event name). Other approaches utilizing a dynamic vocabulary of slot values \\cite{rastogi2018multi,goel2019hyst} still do not allow zero-shot generalization to new services and APIs \\cite{wu-etal-2019-transferable}, since they use schema elements i.e. intents and slots as fixed class labels.\n\nAlthough such systems are capable of parsing the dialogue semantics in terms of these fixed intent labels, they lack understanding of the semantics of these labels. For instance, for the user utterance ``I want to buy tickets for a movie.\", such models can predict \\textit{BuyMovieTickets} as the correct intent based on patterns observed in the training data, but don't model either its association with the real world action of buying movie tickets, or its similarity to the action of buying concert or theatre tickets. Furthermore, because of their dependence on a fixed schema, such models are not robust to changes in the schema, and need to be retrained as new slots or intents are added. Use of domain-specific parameters renders some approaches unsuitable for zero-shot application.\n\n\\section{Task}\n\nThe primary task of this challenge is to develop multi-domain models for DST suitable for the scale and complexity of large scale virtual assistants. Supporting a wide variety of APIs or services with possibly overlapping functionality is an important requirement of such assistants. A common approach to do this involves defining a large master schema that lists all intents and slots supported by the assistant. Each service either adopts this master schema for the representation of the underlying data, or provides logic to translate between its own schema and the master schema. \n\nThe first approach involving adoption of the master schema is not ideal if a service wishes to integrate with multiple assistants, since each of the assistants could have their own master schema. The second approach involves definition of logic for translation between master schema and the service's schema, which increases the maintenance workload. Furthermore, it is difficult to develop a master schema catering to all possible use cases.\n\n\\begin{figure}[ht]\n\\centering\n\\includegraphics[width=0.36\\textwidth]{schema_example_v2.png}\n\\caption{Example schema for a digital wallet service.}\n\\label{fig:schema-example}\n\\end{figure}\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=0.99\\textwidth]{DSTCFlights_v2.png}\n    \\caption{Dialogue state tracking labels after each user utterance in a  dialogue in the context of two different flight services. Under the schema-guided approach, the annotations are conditioned on the schema (extreme left/right) of the underlying service.}\n    \\label{fig:track4-model}\n\\end{figure*}\n\nAdditionally, while there are many similar concepts across services that can be jointly modeled, for example, the similarities in logic for querying or specifying the number of movie tickets, flight tickets or concert tickets, the master schema approach does not facilitate joint modeling of such concepts, unless an explicit mapping between them is manually defined. To address these limitations, we propose a schema-guided approach, which eliminates the need for a master schema.\n\n\\subsection{Schema-Guided Approach}\n\nUnder the Schema-Guided approach, each service provides a schema listing the supported slots and intents along with their natural language descriptions (Figure \\ref{fig:schema-example} shows an example). The dialogue annotations are guided by the schema of the underlying service or API, as shown in Figure \\ref{fig:track4-model}. In this example, the departure and arrival cities are captured by analogously functioning but differently named slots in both schemas. Furthermore, values for the \\textit{number\\_stops} and \\textit{direct\\_only} slots highlight idiosyncrasies between services interpreting the same concept.\n\nThe natural language descriptions present in the schema are used to obtain a semantic representation of intents and slots. The assistant employs a single unified model containing no domain or service specific parameters to make predictions conditioned on these schema elements. Using a single model facilitates representation and transfer of common knowledge across related concepts in different services. Since the model utilizes semantic representation of schema elements as input, it can interface with unseen services or APIs on which it has not been trained. It is also robust to changes like the addition of new intents or slots to the service. In addition, the participants are allowed to use any external datasets or resources to bootstrap their models.\n\n\\section{Dataset}\n\nAs shown in Table \\ref{table:datasets}, our Schema-Guided Dialogue (SGD) dataset exceeds other datasets in most of the metrics at scale. The especially larger number of domains, slots, and slot values, and the presence of multiple services per domain, are representative of these scale-related challenges. Furthermore, our evaluation sets contain many services, and consequently slots, which are not present in the training set, to help evaluate model performance on unseen services.\n\n\\subsection{Data Representation}\n\nThe dataset consists of conversations between a virtual assistant and a user. Each conversation can span multiple services across various domains. The dialogue is represented as a sequence of turns, each containing a user or system utterance. The annotations for each turn are grouped into frames, where each frame corresponds to a single service. The annotations for user turns include the active intent, the dialogue state and slot spans for the different slots values mentioned in the turn. For system turns, we have the system actions representing the semantics of the system utterance. Each system action is represented using a dialogue act with optional parameters.\n\nIn addition to the dialogues, for each service used in the dataset, a normalized representation of the interface exposed is provided as the schema. The schema contains details like the name of the service, the list of tasks supported by the service (intents) and the attributes of the entities used by the service (slots). The schema also contains natural language descriptions of the service, intents and slots which can be used for developing models which can condition their predictions on the schema.\n\n\\subsection{Comparison With Other Datasets}\n\nTo reflect the constraints present in real-world services and APIs, we impose a few constraints on the data. Our dataset does not expose the set of all possible values for certain slots. Having such a list is impractical for slots like date or time because they have infinitely many possible values or for slots like movie or song names, for which new values are periodically added. Such slots are specifically identified as non-categorical slots. In our evaluation sets, we ensured the presence of a significant number of values which were not previously seen in the training set to evaluate the performance of models on unseen values. Some slots like gender, number of people, etc. are classified as categorical and we provide a list of all possible values for them. However, these values are assumed to be not consistent across services. E.g., different services may use (`male', `female'), (`M', `F') or (`he', `she') as possible values for gender slot.\n\nReal-world services can only be invoked with certain slot combinations: e.g. most restaurant reservation APIs do not let the user search for restaurants by date without specifying a location. Although this constraint has no implications on the dialogue state tracking task, it restricts the possible conversational flows. Hence, to prevent flows not supported by actual services, we restrict services to be called with a list of slot combinations. The different service calls supported by a service are listed as intents with each intent specifying a list of required slots. The intent cannot be called without providing values for these required slots. Each intent also contains a list of optional slots with default values which can be overridden by the user.\n\nIn our dataset, we also have multiple services per domain with overlapping functionality. The intents across these services are similar but differ in terms of intent names, intent arguments, slot names, etc. In some cases, there is no one to one mapping between slot names (e.g., the \\textit{num\\_stops} and \\textit{direct\\_only} slots in Figure \\ref{fig:track4-model}). With an ever increasing number of services and service providers, we believe that having multiple similar services per domain is much closer to the situation faced by virtual assistants than having one unique service per domain. \n\n\\subsection{Data Collection And Dataset Analysis}\\label{sec:data-collect}\n\nOur data collection setup uses a dialogue simulator to generate dialogue outlines first and then paraphrase them to obtain natural utterances. Using a dialogue simulator offers us multiple advantages. First, it ensures the coverage of a large variety of dialogue flows by filtering out similar flows in the simulation phase, thus creating a much diverse dataset. \nSecond, simulated dialogues do not require manual annotation, as opposed to a Wizard-of-Oz setup \\cite{kelley1984iterative}, which is a common approach utilized in other datasets~\\cite{budzianowski2018multiwoz}. It has been shown that such datasets suffer from substantial annotation errors~\\cite{eric2019multiwoz}. Thirdly, using a simulator greatly simplifies the data collection task and instructions as only paraphrasing is needed to achieve a natural dialogue. This is particularly important for creating a large dataset spanning multiple domains.\n\nThe 20 domains present across the train, dev and test datasets are listed in Table \\ref{table:domains}, as are the details regarding which domains are present in each of the datasets. We create synthetic implementations of a total of 45 services or APIs over these domains. Our simulator framework interacts with these services to generate dialogue outlines, which are structured representations of dialogue semantics. We then use a crowd-sourcing procedure to paraphrase these outlines to natural language utterances. Our novel crowd-sourcing procedure preserves all annotations obtained from the simulator and does not require any extra annotations after dialogue collection. In this section, we describe these steps briefly and then present analyses of the collected dataset. \n\nAll the services are implemented using a SQL engine. Since entity attributes are often correlated, we decided not to sample synthetic entities and instead relied on sampling entities from Freebase. The dialogue simulator interacts with the services to generate valid dialogue outlines. The simulator consists of two agents  playing  the  roles  of  the  user  and  the  system.  Both agents interact with each other using a finite set of actions specified through dialogue acts over a probabilistic automaton designed to capture varied dialogue trajectories. At the start of the conversation, the user agent is seeded with a scenario, which is a sequence of intents to be fulfilled. The user agent generates dialogue acts to be output and combines them with values retrieved from the service/API to create the user actions. The system agent responds by following a similar procedure but also ensures that the generated flows are valid.  We identified over 200 distinct scenarios for the training set each consisting up to 5 intents from various domains. Finally, the dialogue outlines generated are paraphrased into a natural conversation by crowd workers. We ensure that the annotations for the dialogue state and slots generated by the simulator are preserved and hence need no other annotation. We omit details for brevity: please refer to \\citet{rastogi2019towards} for more details.\n\nThe entire dataset consists of over 16K dialogues spanning multiple domains. Overall statistics of the dataset and comparison with other datasets can be seen in Table \\ref{table:datasets}. Figure \\ref{fig:dialogue_lengths} shows the details of the distribution of dialogue lengths across single-domain and multi-domain dialogues. The single-domain dialogues in our dataset contain an average of 15.3 turns, whereas the multi-domain ones contain 23 turns on average. Figure \\ref{fig:dialogue_act_distribution} shows the frequency of the different dialogue acts contained in the dataset. The dataset also contains a significant number of unseen domains/APIs in the dev and test sets. 77\\% of the dialogue turns in the test set and 45\\% of the turns in dev set contain at least one service not present in the training set. This facilitates the development of models which can generalize to new domains with very few labelled examples.\n\n\\begin{figure}\n    \\centering\n    \\subfloat[Histogram of lengths of training set dialogues.]{  \\includegraphics[width=0.95\\columnwidth]{dialogue_lengths_v3.png} \\label{fig:dialogue_lengths}} \\qquad\n    \\subfloat[Distribution of dialogue acts in training set.]{\\includegraphics[width=0.95\\columnwidth]{dialogue_act_distribution.png}\n    \\label{fig:dialogue_act_distribution}}\n\\caption{Detailed statistics of the SGD dataset.}\n\\end{figure}\n\n\\begin{table*}[t!]\n\\centering\n    \\begin{tabular}[t]{ c | c c c c c c }\n\n    \\textbf{Metric $\\downarrow$ Dataset $\\rightarrow$} & \\textbf{DSTC2} & \\textbf{WOZ2.0} & \\textbf{FRAMES} & \\textbf{M2M} & \\textbf{MultiWOZ} & \\textbf{SGD} \\\\\\hline\n    No. of domains & 1 & 1 & 3 & 2 & 7 & \\textbf{16}\\\\\n    No. of dialogues &  1,612 & 600 & 1,369 & 1,500 & 8,438 & \\textbf{16,142}\\\\\n    Total no. of turns &  23,354 & 4,472 & 19,986 & 14,796 & 113,556 & \\textbf{329,964}\\\\\n    Avg. turns per dialogue & 14.49 & 7.45 & 14.60 & 9.86 & 13.46 & \\textbf{20.44} \\\\\n    Avg. tokens per turn & 8.54 & 11.24 & 12.60 & 8.24 & \\textbf{13.13} & 9.75\\\\\n    Total unique tokens & 986 & 2,142 & 12,043 & 1,008 & 23,689 & \\textbf{30,352}\\\\\n    No. of slots &  8 & 4 & 61 & 13 & 24 & \\textbf{214}\\\\\n    No. of slot values & 212 & 99 & 3,871 & 138 & 4,510 & \\textbf{14,139}\\\\\n\n    \\end{tabular}\n    \\caption{Comparison of our SGD dataset to existing related datasets for task-oriented dialogue. Note that the numbers reported are for the training portions for all datasets except FRAMES, where the numbers for the complete dataset are reported.}\n    \\label{table:datasets}\n\\end{table*}\n\n\\begin{table}[!htb]\n    \\centering\n    \\def\\arraystretch{1.23}\n    \\resizebox{\\columnwidth}{!}{\n        \\begin{tabular}{ l | cc||l | cc } \n            \\textbf{Domain} & \\textbf{\\#Intents}  & \\textbf{\\#Dialogs} & \\textbf{Domain} & \\textbf{\\#Intents}  & \\textbf{\\#Dialogs} \\\\ \\hline\n            Alarm$^{2,3}$ & 2 (1) & 324 & Movies$^{1,2,3}$ & 5 (3) & 2339\\\\\n            Banks$^{1,2}$ & 4 (2) & 1021 & Music$^{1,2,3}$ & 6 (3) & 1833\\\\\n            Buses$^{1,2,3}$ & 6 (3) & 3135 & Payment$^3$ & 2 (1) & 222\\\\\n            Calendar$^1$ & 3 (1) & 1602 & RentalCars$^{1,2,3}$ & 6 (3) & 2510\\\\\n            Events$^{1,2,3}$ & 7 (3) & 4519 & Restaurants$^{1,2,3}$ & 4 (2) & 3218\\\\\n            Flights$^{1,2,3}$ & 10 (4) & 3644 & RideSharing$^{1,2,3}$ & 2 (2) & 2223\\\\\n            Homes$^{1,2,3}$ & 2 (1) & 1273  & Services$^{1,2,3}$ & 8 (4) & 2956\\\\\n            Hotels$^{1,2,3}$ & 8 (4) & 4992  & Train$^{3}$ & 2 (1) & 350\\\\\n            Media$^{1,2,3}$ & 6 (3) & 1656 & Travel$^{1,2,3}$ & 1 (1) & 2808\\\\\n            Messaging$^3$ & 1 (1) & 298 & Weather$^{1,2,3}$ & 1 (1) & 1783\\\\\n          \\end{tabular}\n    }\n    \\caption{The total number of intents (services in parentheses) and dialogues for each domain across train$^1$, dev$^2$ and test$^3$ sets. Superscript indicates the datasets in which dialogues from the domain are present. Multi-domain dialogues contribute to counts of each domain. The domain Services includes salons, dentists, doctors, etc.}\n    \\label{table:domains}\n\\end{table}\n\n\\section{Submissions}\n\n\\begin{table*}[ht] \n\\centering\n    \\setlength\\tabcolsep{2pt}\n    \\def\\arraystretch{1.35}\n\\resizebox{2\\columnwidth}{!}{\n\\begin{tabular}{c!{\\vrule width 2pt}c|c|c|c!{\\vrule width 2pt}c|c|c|c!{\\vrule width 2pt}c|c|c|c}\n\n\\multirow{2}{*}{\\textbf{Team name}} & \n  \\multicolumn{4}{c!{\\vrule width 2pt}}{\\textbf{All services}} &\n  \\multicolumn{4}{c!{\\vrule width 2pt}}{\\textbf{Seen services}} &\n  \\multicolumn{4}{c}{\\textbf{Unseen services}} \\\\ \\cline{2-13}\n& \\textbf{Joint GA} & \\textbf{Avg GA} & \\textbf{Intent Acc} & \\textbf{Req Slot F1} & \\textbf{Joint GA} & \\textbf{Avg GA} & \\textbf{Intent Acc} & \\textbf{Req Slot F1} &  \\textbf{Joint GA} & \\textbf{Avg GA} & \\textbf{Intent Acc} &  \\textbf{Req Slot F1} \\\\ \\hline \n\\textbf{Team 9*} & \\textbf{0.8653} & \\textbf{0.9697} & 0.9482 & 0.9847 & \\textbf{0.9241} & \\textbf{0.9799} & 0.9571 & 0.9936 & \\textbf{0.8456} & \\textbf{0.9662} & 0.9452 & 0.9817 \\\\\\hline\nTeam 14 & 0.7726 & 0.9217 & \\textbf{0.9674} & 0.9932 & 0.9005 & 0.9606 & 0.9578 & 0.9963 & 0.7299 & 0.9081 & \\textbf{0.9706} & 0.9921 \\\\\\hline\nTeam 12* & 0.7375 & 0.9199 & 0.9234 & \\textbf{0.9948} & 0.8795 & 0.9566 & 0.9581 & 0.9965 & 0.6901 & 0.9071 & 0.9118 & \\textbf{0.9943} \\\\\\hline\nTeam 8 & 0.7344 & 0.9251 & N.A. & 0.8713 & 0.9106 & 0.9708 & N.A. & 0.8475 & 0.6757 & 0.9093 & N.A. & 0.8793 \\\\\\hline\nTeam 5* & 0.7303 & 0.9249 & 0.9426 & 0.9814 & 0.8936 & 0.9662 & \\textbf{0.9594} & 0.9920 & 0.6758 & 0.9105 & 0.9370 & 0.9779 \\\\\\hline\nTeam 10 & 0.6946 & 0.9105 & 0.9509 & 0.8713 & 0.9203 & 0.9780 & 0.9560 & 0.8475 & 0.6193 & 0.8871 & 0.9492 & 0.8793 \\\\\\hline\nTeam 13 & 0.6616 & 0.9037 & 0.9368 & 0.9854 & 0.8584 & 0.9527 & 0.9534 & 0.9960 & 0.5960 & 0.8867 & 0.9312 & 0.9819 \\\\\\hline\nTeam 7 & 0.6316 & 0.8595 & 0.9231 & 0.9797 & 0.8410 & 0.9356 & 0.9449 & 0.9951 & 0.5617 & 0.8331 & 0.9158 & 0.9746 \\\\\\hline\nTeam 6 & 0.6102 & 0.8430 & 0.9041 & 0.8713 & 0.6764 & 0.8397 & 0.9483 & 0.8475 & 0.5881 & 0.8442 & 0.8893 & 0.8793 \\\\\\hline\nTeam 18 & 0.6099 & 0.9049 & 0.9423 & 0.9723 & 0.8223 & 0.9601 & 0.9540 & 0.9876 & 0.5390 & 0.8858 & 0.9384 & 0.9672 \\\\\\hline\nTeam 21 & 0.5475 & 0.8670 & 0.9344 & 0.8713 & 0.7514 & 0.9190 & 0.9418 & 0.8475 & 0.4795 & 0.8489 & 0.9319 & 0.8793 \\\\\\hline\nTeam 16* & 0.5410 & 0.8027 & 0.9137 & 0.8713 & 0.5289 & 0.7515 & 0.9561 & 0.8475 & 0.5450 & 0.8205 & 0.8995 & 0.8793 \\\\\\hline\nTeam 3 & 0.5035 & 0.7853 & 0.8789 & 0.9581 & 0.6172 & 0.8174 & 0.9565 & 0.9902 & 0.4656 & 0.7741 & 0.8530 & 0.9474 \\\\\\hline\nTeam 25 & 0.4801 & 0.7706 & 0.8765 & 0.9862 & 0.5412 & 0.7659 & 0.9379 & 0.9960 & 0.4597 & 0.7722 & 0.8560 & 0.9829 \\\\\\hline\nTeam 20 & 0.4774 & 0.7148 & 0.8400 & 0.9453 & 0.7847 & 0.9209 & 0.9416 & 0.9840 & 0.3748 & 0.6432 & 0.8061 & 0.9324 \\\\\\hline\nTeam 23* & 0.4647 & 0.7500 & 0.7474 & 0.9703 & 0.5275 & 0.7391 & 0.8710 & 0.9710 & 0.4438 & 0.7538 & 0.7061 & 0.9700 \\\\\\hline\nTeam 11 & 0.4212 & 0.7056 & 0.9070 & 0.9663 & 0.6375 & 0.8226 & 0.9397 & 0.9964 & 0.3490 & 0.6649 & 0.8961 & 0.9563 \\\\\\hline\nTeam 15 & 0.3907 & 0.6874 & 0.9379 & 0.9799 & 0.4965 & 0.7357 & 0.9516 & \\textbf{0.9970} & 0.3554 & 0.6706 & 0.9333 & 0.9742 \\\\\\hline\nTeam 2* & 0.3647 & 0.7438 & 0.9243 & 0.9764 & 0.7363 & 0.9132 & 0.9492 & 0.9925 & 0.2406 & 0.6850 & 0.9160 & 0.9710 \\\\\\hline\nTeam 22 & 0.3259 & 0.6714 & 0.9077 & 0.9525 & 0.6772 & 0.8966 & 0.7855 & 0.9504 & 0.2285 & 0.6082 & 0.9416 & 0.9530 \\\\\\hline\nTeam 24 & 0.3198 & 0.6347 & 0.8764 & 0.9729 & 0.7077 & 0.8888 & 0.9413 & 0.9846 & 0.1903 & 0.5464 & 0.8548 & 0.9690 \\\\\\hline\nTeam 19 & 0.3052 & 0.6302 & 0.9240 & 0.9668 & 0.5140 & 0.7476 & 0.9607 & 0.9953 & 0.2355 & 0.5894 & 0.9118 & 0.9572 \\\\\\hline\nTeam 17 & 0.2525 & 0.5721 & 0.8875 & 0.9680 & 0.4179 & 0.6858 & 0.9433 & 0.9952 & 0.1973 & 0.5326 & 0.8689 & 0.9590 \\\\\\hline\nTeam 1 & 0.2511 & 0.5609 & 0.8406 & 0.9648 & 0.4255 & 0.6825 & 0.9164 & 0.9949 & 0.1929 & 0.5187 & 0.8153 & 0.9547 \\\\\\hline\nTeam 4 & 0.2354 & 0.5365 & 0.8841 & 0.9445 & 0.4004 & 0.6333 & 0.9228 & 0.9523 & 0.1803 & 0.5029 & 0.8712 & 0.9419 \\\\\\hline\nBaseline & 0.2537 & 0.5605 & 0.9064 & 0.9651 & 0.4125 & 0.6778 & 0.9506 & 0.9955 & 0.2000 & 0.5192 & 0.8915 & 0.9547  \\\\ \\hline\n\\toprule\n\\end{tabular}\n}\n\\caption{The best submission from each team, ordered by the joint goal accuracy on the test set. Teams marked with * submitted their papers to the workshop. We could not identify the teams for three of the submitted papers.}\n\\label{table:metrics}\n\\end{table*}\n\nThe submissions from 25 teams included a variety of approaches and innovative solutions to specific problems posed by this dataset. For the workshop, we received submissions from 9 of these teams. In this section, we provide a short summary of the approaches followed by these teams. For effective generalization to unseen APIs, most teams used pre-trained encoders to encode schema element descriptions. Unless otherwise mentioned, a pre-trained BERT \\cite{devlin2019bert} encoder was used.\n\n\\begin{itemize}\n    \\item \\textbf{Team 2 \\cite{lo2020dstc}:} This was the only paper not using a pre-trained encoder, thus providing another important baseline. They rely on separate RNNs to encode service, slot and intent descriptions, and a BiRNN to encode dialogue history. Slot values are inferred using a TRADE-like encoder-decoder setup with a 3-way slot status gate, using the utterance encoding and schema element embeddings as context.\n    \n    \\item \\textbf{Team 5 \\cite{lei2020dstc}:} They predict values for categorical slots using a softmax over all candidate values. Non-categorical slot values are predicted by first predicting the status of each slot and then using a BiLSTM-CRF layer for BIO tagging \\cite{ramshaw1995text}. They also utilize a slot adoption tracker to predict if the values proposed by the system are accepted by the user.\n\n    \\item \\textbf{Team 9 \\cite{ma2020dstc}:} This team submitted the winning entry, beating the second-placed team by around 9\\% in terms of joint goal accuracy. They use two separate models for categorical and non-categorical slots, and treat numerical categorical slots as non-categorical. They also use the entire dialogue history as input. They perform data augmentation by back translation between English and Chinese, which seems to be one of the distinguishing factors resulting in a much higher accuracy.\n    \n    \\item \\textbf{Team 12 \\cite{ruan2020dstc}:} They use auxiliary binary features to connect previous intent to current intent, slots to dialogue history and source slots to target slots for slot transfer. Non-categorical slots are modeled similar to question answering by adding a null token and predicting spans for slot values. In-domain and cross-domain slot transfers are modeled as separate binary decisions by passing the slot descriptions as additional inputs.\n\n    \\item \\textbf{Team 16 \\cite{shi2020dstc}:} They convert the tracking task for both categorical and non-categorical slots into a question answering task by feeding in the schema and the previous turns as the context. Similar to the baseline model, prediction is performed in two stages. The status of each slot (active/inactive/dontcare) is predicted using a classifier, following which the value is predicted as a span in the context. The same network is used for the different prediction tasks but the leading token and separator tokens used are different. They observe large gains by fine-tuning the schema embeddings and increasing the number of past turns fed as context.\n    \n    \\item \\textbf{Team 23 \\cite{gulyaev2020dstc}:} They use a large scale multi-task model utilizing a single pass of a BERT based model for all tasks. Embeddings are calculated for the intents and slot value by using dialogue history, service and slot descriptions, possible values for categorical slots and are used for the various predictions.\n    \n    \\item \\textbf{Anonymous Team A \\cite{balaraman2020dstc}:} We could not identify which team submitted this model. They use multi-head attention twice to obtain domain-conditioned and slot-conditioned representations of the dialogue history. These representations are concatenated to obtain the full context which is used for the various predictions.\n\n    \\item \\textbf{Anonymous Team B \\cite{li2020dstc}:} We could not identify which team submitted this model. They use separate NLU systems for the sub tasks of predicting intents, requested slots, slot status, categorical and non-categorical slot values. They use a rule-based DST system with a few additions resulting in significant improvement. The improvements include adding dropout to intent prediction to account for train-test mismatch, using the entire predicted slot status distribution and separate binary predictions for slot transfer.\n    \n    \\item \\textbf{Anonymous Team C \\cite{zheng2020dstc}:} They use a two-stage model with a candidate tracker for NLU and a candidate classifier to update the dialogue state. A slot tagger identifies slot values, which are used to update the candidate tracker. The candidate classifier uses the utterances and slot/intent descriptions to predict the final dialogue state. They also use an additional loss to penalize incorrect prediction on which slots appear in the current turn.\n\n\\end{itemize}\n\n\\section{Evaluation}\nWe consider the following metrics for automatic evaluation of different submissions. Joint goal accuracy has been used as the primary metric to rank the submissions.\n\n\\begin{enumerate}\n    \\item \\textbf{Active Intent Accuracy:} The fraction of user turns for which the active intent has been correctly predicted.\n    \\item \\textbf{Requested Slot F1:} The macro-averaged F1 score for requested slots over all eligible turns. Turns with no requested slots in ground truth and predictions are skipped.\n    \\item \\textbf{Average Goal Accuracy:} For each turn, we predict a single value for each slot present in the dialogue state. This is the average accuracy of predicting the value of a slot correctly.\n    \\item \\textbf{Joint Goal Accuracy:} This is the average accuracy of predicting \\textit{all} slot assignments for a given service in a turn correctly. \n\\end{enumerate}\n\nIn order to better reflect model performance in our task's specific setting, we introduce changes in the definitions of evaluation metrics from prior work. These are listed below:\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{Joint goal accuracy calculation:} Traditionally, joint goal accuracy has been defined as the accuracy of predicting the dialogue state for all domains correctly. This is not practical in our setup, as the large number of services would result in near zero joint goal accuracy if the traditional definition is used. Furthermore, an incorrect dialogue state prediction for a service in the beginning of a dialogue degrades the joint goal accuracy for all future turns, even if the predictions for all other services are correct. Hence, joint goal accuracy calculated this way may not provide as much insight into the performance on different services. To address these concerns, only the services which are active or pertinent in a turn are included in the dialogue state. Thus, a service ceases to be a part of the dialogue state once its intent has been fulfilled. \n\n    \\item \\textbf{Fuzzy matching for non-categorical slot values:} The presence of non-categorical slots is another distinguishing feature of our dataset. These slots don't have a predefined vocabulary, and their values are predicted as a substring or span of the past user or system utterances. Drawing inspiration from the metrics used for slot tagging in spoken language understanding, we use a fuzzy matching score for non-categorical slots to reward partial matches with the ground truth.\n\n    \\item \\textbf{Average goal accuracy:} To calculate average goal accuracy, we do not take into account instances when both the ground truth and the predicted values for a slot are empty. Since for a given slot, a large number of utterances have an empty assignment, models can achieve a relatively high average goal accuracy just by predicting an empty assignment for each slot unless specifically excluded as in our evaluation.\n\\end{itemize}\n\n\\section{Results}\n\nThe test set contains a total of 21 services, among which 6 services are also present in the training set (seen services), whereas the remaining 15 are not present in the training set (unseen services). Table \\ref{table:metrics} shows the evaluation metrics for the different submissions obtained on the test set. It also lists the performance of different submissions on seen and unseen services, helping evaluate the effectiveness in zero-shot settings. Team 9 achieved a very high joint goal accuracy of \\textbf{86.53\\%}, around 9\\% higher than the second-placed team. We observed the following trends across submissions:\n\n\\begin{itemize}\n    \\item For unseen services, performance on categorical slots is comparable to that on non-categorical slots. On the other hand, for seen services, the performance on categorical slots is better. This could be because there is less signal to differentiate between the different possible values for a categorical slot when they have not been observed in the training set.\n    \n    \\item The winning team's performance on seen services is similar to that of the other top teams. However, the winning team has a considerable edge on unseen services, outperforming the second team by around 12\\% in terms of joint goal accuracy. This margin was observed across both categorical and non-categorical slots.\n    \n    \\item Among unseen services, when looking at services belonging to unseen domains, the winning team was ahead of the other teams by at least 15\\%. The performance on categorical slots for unseen domains was about the same as that for seen services and domains. For other teams, there was at least a 20\\% drop in accuracy of categorical slots in unseen domains vs seen domains and services.\n    \n    \\item The joint goal accuracy of most of the models was worse by 15 percentage points on an average on the test set as compared to the dev set. This could be because the test set contains a much higher proportion of turns with at least one unseen services as compared to the dev set (77\\% and 45\\% respectively).\n\\end{itemize}\n\n\\section{Summary}\nIn this paper, we summarized the Schema-Guided Dialogue State Tracking task conducted at the Eighth Dialogue System Technology Challenge. This task challenged participants to develop dialogue state tracking models for large scale virtual assistants, with particular emphasis on joint modeling across different domains and APIs for data-efficiency and zero-shot generalization to new/unseen APIs. In order to encourage the development of such models, we constructed a new dataset spanning 16 domains (and 4 new domains in dev and test sets), defining multiple APIs with overlapping functionality for each of these domains. We advocated the use of schema-guided approach to building large-scale assistants, facilitating data-efficient joint modeling across domains while reducing maintenance workload.\n\nThe Schema-Guided Dialogue dataset released as part of this task is the first to highlight many of the aforementioned challenges. As a result, this task led to the development of several models utilizing the schema-guided approach for dialogue state tracking. The models extensively utilized pre-trained encoders like BERT \\cite{devlin2019bert}, XLNet \\cite{yang2019xlnet} etc. and employed data augmentation techniques to achieve effective zero-shot generalization to new APIs. The proposed schema-guided approach is fairly general and can be used to develop other dialogue system components such as language understanding, policy and response generation. We plan to explore them in future works.\n\n\\subsubsection{Acknowledgements} The authors thank Guan-Lin Chao, Amir Fayazi and Maria Wang for their advice and assistance.\n\n\\fontsize{9.0pt}{10.0pt} \\selectfont\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{\\datasetName: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Dataset}\n\n\\begin{document}\n\n\\maketitle\n\n\\renewcommand{\\thefootnote}{\\fnsymbol{footnote}}\n\\footnotetext[1]{Corresponding author.}\n\n\\begin{abstract}\nTo advance multi-domain (cross-domain) dialogue modeling as well as alleviate the shortage of Chinese task-oriented datasets, we propose \\textbf{\\datasetName}, the first large-scale Chinese Cross-Domain Wizard-of-Oz task-oriented dataset. It contains 6K dialogue sessions and 102K utterances for 5 domains, including hotel, restaurant, attraction, metro, and taxi. Moreover, the corpus contains rich annotation of dialogue states and dialogue acts at both user and system sides. About 60\\% of the dialogues have cross-domain user goals that favor inter-domain dependency and encourage natural transition across domains in conversation. We also provide a user simulator and several benchmark models for pipelined task-oriented dialogue systems, which will facilitate researchers to compare and evaluate their models on this corpus.\nThe large size and rich annotation of \\textbf{\\datasetName} make it suitable to investigate a variety of tasks in cross-domain dialogue modeling, such as dialogue state tracking, policy learning, user simulation, etc.\n\\end{abstract}\n\n\\section{Introduction}\n\\label{section1}\nRecently, there have been a variety of task-oriented dialogue models thanks to the prosperity of neural architectures \\cite{yao2013rnnNLU,wen2015sclstm,mrkvsic2017neuralDST,HRL,lei2018sequicity,gur2018user}. \nHowever, the research is still largely limited by the availability of large-scale high-quality dialogue data. Many corpora have advanced the research of task-oriented dialogue systems, most of which are single domain conversations, including ATIS \\cite{ATIS1990}, DSTC 2 \\cite{DSTC2}, Frames \\cite{frames2017}, KVRET \\cite{KVRET2017}, WOZ 2.0 \\cite{Camrest2017} and M2M \\cite{M2M2018}. \n\nDespite the significant contributions to the community, these datasets are still limited in size, language variation, or task complexity.\nFurthermore, there is a gap between existing dialogue corpora and real-life human dialogue data.\nIn real-life conversations, it is natural for humans to transition between different domains or scenarios while still maintaining coherent contexts. Thus, real-life dialogues are much more complicated than those dialogues that are only simulated within a single domain. To address this issue, some multi-domain corpora have been proposed \\cite{Multiwoz2018,SchemaGuided}. \nThe most notable corpus is MultiWOZ \\cite{Multiwoz2018}, a large-scale multi-domain dataset which consists of crowdsourced human-to-human dialogues. It contains 10K dialogue sessions and 143K utterances for 7 domains, with annotation of system-side dialogue states and dialogue acts. However, the state annotations are noisy \\cite{eric2019multiwoz21}, and user-side dialogue acts are missing. The dependency across domains is simply embodied in imposing the same pre-specified constraints on different domains, such as requiring both a hotel and an attraction to locate in the center of the town.\n\n\\begin{figure}[htbp]\n    \\includegraphics{exampleBold.jpeg}\n    \\caption{A dialogue example.\n    The user state is initialized by the user goal: finding an attraction and one of its nearby hotels, then book a taxi to commute between these two places. \n    In addition to expressing pre-specified informable slots and filling in requestable slots, users need to consider and modify cross-domain informable slots (\\textbf{bold}) that vary through conversation.\n    We only show a few turns (turn number on the left), each with either user or system state of the current domain which are shown above each utterance.\n    }\n    \\label{fig:example}\n\\end{figure}\n\nIn comparison to the abundance of English dialogue data, surprisingly, there is still no widely recognized Chinese task-oriented dialogue corpus. \nIn this paper, we propose \\textbf{\\datasetName}, a large-scale Chinese  multi-domain (cross-domain) task-oriented dialogue dataset. \nAn dialogue example is shown in Figure \\ref{fig:example}. We compare \\textbf{\\datasetName} to other corpora in Table \\ref{tab:cmp} and \\ref{tab:cmp_case}. Our dataset has the following features comparing to other corpora (particularly MultiWOZ \\cite{Multiwoz2018}):\n\\begin{enumerate}\n    \\item The dependency between domains is more challenging because the choice in one domain will affect the choices in related domains in \\datasetName.\n    As shown in Figure \\ref{fig:example} and Table \\ref{tab:cmp_case}, the hotel must be near the attraction chosen by the user in previous turns, which requires more accurate context understanding.\n    \n    \n    \\item It is the first Chinese corpus that contains large-scale multi-domain task-oriented dialogues, consisting of 6K sessions and 102K utterances for 5 domains (attraction, restaurant, hotel, metro, and taxi). \n    \n    \\item Annotation of dialogue states and dialogue acts is provided for both the system side and user side. The annotation of user states enables us to track the conversation from the user's perspective and can empower the development of more elaborate user simulators.\n\\end{enumerate}\n\nIn this paper, we present the process of dialogue collection and provide detailed data analysis of the corpus. Statistics show that our cross-domain dialogues are complicated. \nTo facilitate model comparison, benchmark models are provided for different modules in pipelined task-oriented dialogue systems, including natural language understanding, dialogue state tracking, dialogue policy learning, and natural language generation. We also provide a user simulator, which will facilitate the development and evaluation of dialogue models on this corpus. \nThe corpus and the benchmark models are publicly available at \\url{https://github.com/thu-coai/CrossWOZ}.\n\n\\begin{table*}[ht]\n\\centering\n\\setlength{\\tabcolsep}{1.5mm}{\n\\begin{tabular}{l|ccccc|ccc}\n\\toprule\n\\multicolumn{1}{l|}{Type} & \\multicolumn{5}{c|}{Single-domain goal}           & \\multicolumn{3}{c}{Multi-domain goal}                 \\\\ \\hline\nDataset      & DSTC2  & WOZ 2.0 & Frames & KVRET  & M2M    & MultiWOZ & Schema & \\textbf{\\datasetName} \\\\\n\\hline\nLanguage     & EN     & EN      & EN     & EN     & EN     & EN       & EN      & \\textbf{CN}      \\\\\nSpeakers      & H2M    & H2H     & H2H    & H2H    & M2M    & H2H      & M2M     & \\textbf{H2H}     \\\\\n\\# Domains   & 1      & 1       & 1      & 3      & 2      & 7        & 16      & \\textbf{5}       \\\\\n\\# Dialogues & 1,612  & 600     & 1,369  & 2,425  & 1,500  & 8,438    & 16,142  & \\textbf{5,012}   \\\\\n\\# Turns     & 23,354 & 4,472   & 19,986 & 12,732 & 14,796 & 115,424  & 329,964 &  \\textbf{84,692}       \\\\\nAvg. domains     & 1 & 1   & 1 & 1 & 1 &  1.80 & 1.84 &  \\textbf{3.24}       \\\\\nAvg. turns     & 14.5 & 7.5   & 14.6 & 5.3 & 9.9 & 13.7  & 20.4 &  \\textbf{16.9}       \\\\\n\\# Slots     & 8      & 4       & 61     & 13     & 14     & 25       & 214     & \\textbf{72}      \\\\\n\\# Values    & 212    & 99      & 3,871  & 1363   & 138    & 4,510    & 14,139  & \\textbf{7,871}        \\\\ \n\\bottomrule\n\\end{tabular}}\n\\caption{Comparison of \\textbf{\\datasetName}  to other task-oriented corpora (training set). H2H, H2M, M2M represent human-to-human, human-to-machine, machine-to-machine respectively. The average numbers of domains and turns are for each dialogue.\n} \n\\label{tab:cmp}\n\\end{table*}\n\n\\begin{table*}[ht]\n\\begin{CJK*}{UTF8}{gbsn}\n\\footnotesize\n\\setlength{\\tabcolsep}{0.5mm}{\n\\begin{tabular}{|p{5.9cm}|p{9.9cm}|}\n\\hline\n\\multicolumn{1}{|c|}{\\textbf{MultiWOZ}} & \\multicolumn{1}{c|}{\\textbf{\\datasetName}} \\\\ \n\\hline\nusr: I'm looking for a college type attraction. & usr: 你好，可以帮我推荐一个评分是4.5分以上的景点吗？\\\\\n\\qquad ... & \\qquad Hello, could you recommend an attraction with a rating of 4.5 or higher? \\\\\nusr: I would like to visit \\underline{in town centre please}. & sys: 天安门城楼，簋街小吃和\\underline{北京欢乐谷}都是很不错的地方呢。\\\\\n\\qquad ... & \\qquad Tiananmen, Gui Street, and \\underline{Beijing Happy Valley} are very nice places. \\\\\nusr: Can you find an Indian restaurant for me & usr: 我喜欢\\underline{北京欢乐谷}，你知道这个景点周边的酒店都是什么吗？\\\\\nthat is \\textbf{also in the town centre}? & \\qquad I like \\underline{Beijing Happy Valley}. What hotels are around this attraction?\\\\ \\cline{1-1}\n\\multicolumn{1}{|c|}{\\textbf{Schema}} & sys: 那可多了，有A酒店, B酒店, C酒店。\\\\ \\cline{1-1}\nusr: I want a hotel in \\underline{San Diego} and I want to & \\qquad There are many, such as hotel A, hotel B, and hotel C. \\\\\ncheck out on Thursday next week. & usr: 太好了，我正打算在\\textbf{景点附近}找个酒店住宿呢，知道哪家评分 \\\\\n\\qquad ... & 是4分以上，提供叫醒服务的不？\\\\\nusr: I need a one way flight to \\textbf{go there}. & \\qquad Great! I am planning to find a hotel to stay \\textbf{near the attraction}. Which one has a rating of 4 or higher and offers wake-up call service? \\\\ \n\\hline\n\\end{tabular}}\n\\caption{\nCross-domain dialog examples in MultiWOZ, Schema, and \\datasetName. The value of cross-domain constraints(\\textbf{bold}) are \\underline{underlined}. Some turns are omitted to save space. Names of hotels are replaced by A,B,C for simplicity. Cross-domain constraints are pre-specified in MultiWOZ and Schema, while determined dynamically in \\datasetName. In \\datasetName, the choice in one domain will greatly affect related domains.\n}\n\\label{tab:cmp_case}\n\\end{CJK*}\n\\end{table*}\n\n\\section{Related Work}\n\\label{section2}\nAccording to whether the dialogue agent is human or machine, we can group the collection methods of existing task-oriented dialogue datasets into three categories. The first one is \\textbf{human-to-human} dialogues. One of the earliest and well-known ATIS dataset \\cite{ATIS1990}  used this setting, followed by \\citet{frames2017}, \\citet{KVRET2017}, \\citet{Camrest2017}, \\citet{dealornot2017}, \\citet{MedicalCH2018} and \\citet{Multiwoz2018}. Though this setting requires many human efforts, it can collect natural and diverse dialogues. The second one is \\textbf{human-to-machine} dialogues, which need a ready dialogue system to converse with humans. The famous Dialogue State Tracking Challenges provided a set of human-to-machine dialogue data \\cite{DSTC2013,DSTC2}. The performance of the dialogue system will largely influence the quality of dialogue data. The third one is \\textbf{machine-to-machine} dialogues. It needs to build both user and system simulators to generate dialogue outlines, then use templates \\cite{HRL} to generate dialogues or further employ people to paraphrase the dialogues to make them more natural \\cite{M2M2018,SchemaGuided}. It needs much less human effort. However, the complexity and diversity of dialogue policy are limited by the simulators. To explore dialogue policy in multi-domain scenarios, and to collect natural and diverse dialogues, we resort to the human-to-human setting. \n\nMost of the existing datasets only involve single domain in one dialogue, except MultiWOZ \\cite{Multiwoz2018} and Schema \\cite{SchemaGuided}. MultiWOZ dataset has attracted much attention recently, due to its large size and multi-domain characteristics. It is at least one order of magnitude larger than previous datasets, amounting to 8,438 dialogues and 115K turns in the training set. It greatly promotes the research on multi-domain dialogue modeling, such as policy learning \\cite{takanobu2019guided}, state tracking \\cite{TRADE}, and context-to-text generation \\cite{MDRG}. Recently the Schema dataset is collected in a machine-to-machine fashion, resulting in 16,142 dialogues and 330K turns for 16 domains in the training set. However, the multi-domain dependency in these two datasets is only embodied in imposing the same pre-specified constraints on different domains, such as requiring a restaurant and an attraction to locate in the same area, or the city of a hotel and the destination of a flight to be the same (Table \\ref{tab:cmp_case}).\n\nTable \\ref{tab:cmp} presents a comparison between our dataset with other task-oriented datasets. In comparison to MultiWOZ, our dataset has a comparable scale: 5,012 dialogues and 84K turns in the training set. The average number of domains and turns per dialogue are larger than those of MultiWOZ, which indicates that our task is more complex. The cross-domain dependency in our dataset is natural and challenging. \nFor example, as shown in Table \\ref{tab:cmp_case}, the system needs to recommend a hotel near the attraction chosen by the user in previous turns. Thus, both system recommendation and user selection will dynamically impact the dialogue. \nWe also allow the same domain to appear multiple times in a user goal since a tourist may want to go to more than one attraction. \n\nTo better track the conversation flow and model user dialogue policy, we provide annotation of \\textbf{user states} in addition to system states and dialogue acts.\nWhile the system state tracks the dialogue history, the user state is maintained by the user and indicates whether the sub-goals have been completed, which can be used to predict user actions. This information will facilitate the construction of the user simulator. \n\nTo the best of our knowledge, \\textbf{\\datasetName} is the first large-scale Chinese dataset for task-oriented dialogue systems, which will largely alleviate the shortage of Chinese task-oriented dialogue corpora that are publicly available. \n\n\\section{Data Collection}\nOur corpus is to simulate scenarios where a traveler seeks tourism information and plans her or his travel in Beijing. Domains include hotel, attraction, restaurant, metro, and taxi.\nThe data collection process is summarized as below: \n\\begin{enumerate}\n    \\item \\textbf{Database Construction}: we crawled travel information in Beijing from the Web, including Hotel, Attraction, and Restaurant domains (hereafter we name the three domains as HAR domains). Then, we used the metro information of entities in HAR domains to build the metro database.\n    For the taxi domain, there is no need to store the information. Instead, we can call the API directly if necessary.\n    \n    \\item \\textbf{Goal Generation}: a multi-domain goal generator was designed based on the database. The relation across domains is captured in two ways. One is to constrain two targets that locate near each other. The other is to use a taxi or metro to commute between two targets in HAR domains mentioned in the context. To make workers understand the task more easily, we crafted templates to generate natural language descriptions for each structured goal. \n    \n    \\item \\textbf{Dialogue Collection}: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states.\n    \n    \\item \\textbf{Dialogue Annotation}: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. \n    To evaluate the quality of the annotation of dialogue acts and states, three experts were employed to manually annotate dialogue acts and states for 50 dialogues. The results show that our annotations are of high quality.\n    Finally, each dialogue contains a structured goal, a task description, user states, system states, dialogue acts, and utterances. \n\n\\end{enumerate}\n\n\\begin{table}[b]\n\\centering\n\\setlength{\\tabcolsep}{2.2mm}{\n\\begin{tabular}{lccc}\n\\toprule\n  Domain & Attract.   & Rest.  & Hotel   \\\\\n\\midrule\n \\# Entities & 465    & 951    & 1133    \\\\ \n \\# Slots & 9    & 10    & 8+37$^*$    \\\\ \n Avg. nearby attract. & 4.7   & 3.3   & 0.8   \\\\\n Avg. nearby rest. & 6.7 & 4.1 & 2.0 \\\\\n Avg. nearby hotels& 2.1   & 2.4   & -  \\\\\n\\bottomrule\n\\end{tabular}}\n\\caption{Database statistics. $^*$ indicates that there are 37 binary slots for hotel services such as wake-up call. The last three rows show the average number of nearby attractions/restaurants/hotels for each entity. We did not collect nearby hotels information for the hotel domain.}\n\\label{tab:db}\n\\end{table}\n\n\\subsection{Database Construction}\nWe collected 465 attractions, 951 restaurants, and 1,133 hotels in Beijing from the Web. Some statistics are shown in Table \\ref{tab:db}. There are three types of slots for each entity: common slots such as name and address; binary slots for hotel services such as wake-up call; nearby attractions/restaurants/hotels slots that contain nearby entities in the attraction, restaurant, and hotel domains. Since it is not usual to find another nearby hotel in the hotel domain, we did not collect such information. This nearby relation allows us to generate natural cross-domain goals, such as \"find another attraction near the first one\" and \"find a restaurant near the attraction\". Nearest metro stations of HAR entities form the metro database. In contrast, we provided the pseudo car type and plate number for the taxi domain. \n\n\\begin{table}[t]\n    \\centering\n    \\setlength{\\tabcolsep}{2.25mm}{ \n    \\begin{tabular}{cccc}\n        \\toprule\n        Id & Domain & Slot & Value \\\\\n        \\midrule\n        1 & Attraction & fee & \\textit{free}\\\\\n        1 & Attraction & name & \\underline{\\quad\\quad\\quad}\\\\\n        1 & Attraction & nearby hotels & \\underline{\\quad\\quad\\quad}\\\\\n        2 & Hotel & name & \\textbf{near (id=1)} \\\\\n        2 & Hotel & wake-up call & \\textit{yes} \\\\\n        2 & Hotel & rating & \\underline{\\quad\\quad\\quad}\\\\\n        3 & Taxi & from & \\textbf{(id=1)} \\\\\n        3 & Taxi & to & \\textbf{(id=2)} \\\\\n        3 & Taxi & car type & \\underline{\\quad\\quad\\quad}\\\\\n        3 & Taxi & plate number & \\underline{\\quad\\quad\\quad}\\\\\n        \\bottomrule\n    \\end{tabular}}\n    \\caption{A user goal example (translated into English). Slots with bold/italic/blank value are cross-domain informable slots, common informable slots, and requestable slots. In this example, the user wants to find an attraction and one of its nearby hotels, then book a taxi to commute between these two places.} \n    \\label{tab:goal_example}\n\\end{table}\n\n\\subsection{Goal Generation}\n\\label{goal_generation}\nTo avoid generating overly complex goals, each goal has at most five sub-goals. To generate more natural goals, the sub-goals can be of the same domain, such as two attractions near each other. The goal is represented as a list of (sub-goal id, domain, slot, value) tuples, named as \\textbf{semantic tuples}. The sub-goal id is used to distinguish sub-goals which may be in the same domain. \nThere are two types of slots: informable slots which are the constraints that the user needs to inform the system, and requestable slots which are the information that the user needs to inquire from the system.\nAs shown in Table \\ref{tab:goal_example}, besides common informable slots (italic values) whose values are determined before the conversation, we specially design cross-domain informable slots (bold values) whose values refer to other sub-goals. \nCross-domain informable slots utilize sub-goal id to connect different sub-goals. Thus the actual constraints vary according to the different contexts instead of being pre-specified. The values of common informable slots are sampled randomly from the database. Based on the informable slots, users are required to gather the values of requestable slots (blank values in Table \\ref{tab:goal_example}) through conversation.\n\nThere are four steps in goal generation. \nFirst, we generate independent sub-goals in HAR domains.\nFor each domain in HAR domains, with the same probability $\\mathcal{P}$ we generate a sub-goal, while with the probability of $1-\\mathcal{P}$ we do not generate any sub-goal for this domain. \nEach sub-goal has common informable slots and requestable slots. As shown in Table \\ref{tab:slot_type}, all slots of HAR domains can be requestable slots, while the slots with an asterisk can be common informable slots. \n\nSecond, we generate cross-domain sub-goals in HAR domains. For each generated sub-goal (e.g., the attraction sub-goal in Table \\ref{tab:goal_example}), if its requestable slots contain \"nearby hotels\", we generate an additional sub-goal in the hotel domain (e.g., the hotel sub-goal in Table \\ref{tab:goal_example}) with the probability of $\\mathcal{P}_{attraction\\rightarrow hotel}$. Of course, the selected hotel must satisfy the \\emph{nearby} relation to the attraction entity. \nSimilarly, we do not generate any additional sub-goal in the hotel domain with the probability of $1-\\mathcal{P}_{attraction\\rightarrow hotel}$.\nThis also works for the attraction and restaurant domains. $\\mathcal{P}_{hotel\\rightarrow hotel}=0$ since we do not allow the user to find the nearby hotels of one hotel.\n\n\\begin{table}[t]\n    \\centering\n    \\setlength{\\tabcolsep}{1.5mm}{ \n    \\begin{tabular}{l}\n        \\toprule\n        Attraction domain \\\\\n        \\textbf{name}$^*$, rating$^*$, fee$^*$, duration$^*$, address, phone, \\\\\n        nearby attract., nearby rest., nearby hotels\\\\\n        \\midrule\n        Restaurant domain \\\\\n        \\textbf{name}$^*$, rating$^*$, cost$^*$, dishes$^*$, address, phone,\\\\\n        open, nearby attract., nearby rest., nearby hotels\\\\\n        \\midrule\n        Hotel domain \\\\\n        \\textbf{name}$^*$, rating$^*$, price$^*$, type$^*$, 37 services$^*$,\\\\ \n        phone, address, nearby attract., nearby rest.\\\\\n        \\midrule\n        Taxi domain \\\\\n        \\textbf{from}, \\textbf{to}, car type, plate number\\\\\n        \\midrule\n        Metro domain\\\\\n        \\textbf{from}, \\textbf{to}, from station, to station\\\\\n        \\bottomrule\n    \\end{tabular}}\n    \\caption{All slots in each domain (translated into English). Slots in bold can be cross-domain informable slots. Slots with asterisk are informable slots. All slots are requestable slots except \"from\" and \"to\" slots in the taxi and metro domains. The \"nearby attractions/restaurants/hotels\" slots and the \"dishes\" slot can be multiple valued (a list). The value of each \"service\" is either yes or no.}\n    \\label{tab:slot_type}\n\\end{table}\n\nThird, we generate sub-goals in the metro and taxi domains. With the probability of $\\mathcal{P}_{taxi}$, we generate a sub-goal in the taxi domain (e.g., the taxi sub-goal in Table \\ref{tab:goal_example}) to commute between two entities of HAR domains that are already generated. It is similar for the metro domain and we set $\\mathcal{P}_{metro}=\\mathcal{P}_{taxi}$.\nAll slots in the metro or taxi domain appear in the sub-goals and must be filled. As shown in Table \\ref{tab:slot_type}, \\textbf{from} and \\textbf{to} slots are always cross-domain informable slots, while others are always requestable slots.\n\nLast, we rearrange the order of the sub-goals to generate more natural and logical user goals. We require that a sub-goal should be followed by its referred sub-goal as immediately as possible.\n\nTo make the workers aware of this cross-domain feature, we additionally provide a task description for each user goal in natural language, which is generated from the structured goal by hand-crafted templates.\n\nCompared with the goals whose constraints are all pre-specified, our goals impose much more dependency between different domains, which will significantly influence the conversation. The exact values of cross-domain informable slots are finally determined according to the dialogue context.\n\n\\subsection{Dialogue Collection}\nWe developed a specialized website that allows two workers to converse \\emph{synchronously} and make annotations online. On the website, workers are free to choose one of the two roles: tourist (user) or system (wizard). Then, two paired workers are sent to a chatroom. The user needs to accomplish the allocated goal through conversation while the wizard searches the database to provide the necessary information and gives responses. Before the formal data collection, we trained the workers to complete a small number of dialogues by giving them feedback. Finally, 90 well-trained workers are participating in the data collection.\n\nIn contrast, MultiWOZ \\cite{Multiwoz2018} hired more than a thousand workers to converse \\emph{asynchronously}. Each worker received a dialogue context to review and need to respond for only one turn at a time. The collected dialogues may be incoherent because workers may not understand the context correctly and multiple workers contributed to the same dialogue session, possibly leading to more variance in the data quality. For example, some workers expressed two mutually exclusive constraints in two consecutive user turns and failed to eliminate the system's confusion in the next several turns.\nCompared with MultiWOZ, our synchronous conversation setting may produce more coherent dialogues.\n\n\\subsubsection{User Side}\n\\label{sec:user-side}\nThe \\textbf{user state} is the same as the user goal before a conversation starts.\nAt each turn, the user needs to 1) modify the user state according to the system response at the preceding turn, 2) select some semantic tuples in the user state, which indicates the dialogue acts,\nand 3) compose the utterance according to the selected semantic tuples. \nIn addition to filling the required values and updating cross-domain informable slots with real values in the user state, the user is encouraged to modify the constraints when there is no result under such constraints. \nThe change will also be recorded in the user state. Once the goal is completed (all the values in the user state are filled), the user can terminate the dialogue.\n\n\\subsubsection{Wizard Side}\n\\label{sec:3.3.2}\nWe regard the database query as the \\textbf{system state}, which records the constraints of each domain till the current turn. \nAt each turn, the wizard needs to 1) fill the query according to the previous user response and search the database if necessary, 2) select the retrieved entities, and 3) respond in natural language based on the information of the selected entities. \nIf none of the entities satisfy all the constraints, the wizard will try to relax some of them for a recommendation, resulting in multiple queries. The first query records original user constraints while the last one records the constraints relaxed by the system. \n\n\\subsection{Dialogue Annotation}\nAfter collecting the conversation data, we used some rules to annotate dialogue acts automatically. Each utterance can have several dialogue acts. Each dialogue act is a tuple that consists of intent, domain, slot, and value. We pre-define 6 types of intents and use the update of the user state and system state as well as keyword matching to obtain dialogue acts. \nFor the user side, dialogue acts are mainly derived from the selection of semantic tuples that contain the information of domain, slot, and value. For example, if (1, Attraction, fee, free) in Table \\ref{tab:goal_example} is selected by the user, then (\\textbf{Inform}, Attraction, fee, free) is labelled. If (1, Attraction, name, \\underline{\\quad}) is selected, then (\\textbf{Request}, Attraction, name, none) is labelled. If (2, Hotel, name, near (id=1)) is selected, then (\\textbf{Select}, Hotel, src\\_domain, Attraction) is labelled. This intent is specially designed for the \"nearby\" constraint. For the system side, we mainly applied keyword matching to label dialogue acts. \\textbf{Inform} intent is derived by matching the system utterance with the information of selected entities. When the wizard selects multiple retrieved entities and recommend them, \\textbf{Recommend} intent is labeled. When the wizard expresses that no result satisfies user constraints, \\textbf{NoOffer} is labeled. For \\textbf{General} intents such as \"goodbye\", \"thanks\" at both user and system sides, keyword matching is applied.\n\nWe also obtained a binary label for each semantic tuple in the user state, which indicates whether this semantic tuple has been selected to be expressed by the user. This annotation directly illustrates the progress of the conversation.\n\nTo evaluate the quality of the annotation of dialogue acts and states (both user and system states), three experts were employed to manually annotate dialogue acts and states for the same 50 dialogues (806 utterances), 10 for each goal type (see Section \\ref{sec:statistics}). \nSince dialogue act annotation is not a classification problem, we didn't use Fleiss' kappa to measure the agreement among experts.\nWe used dialogue act F1 and state accuracy to measure the agreement between each two experts' annotations. The average dialogue act F1 is 94.59\\% and the average state accuracy is 93.55\\%.\nWe then compared our annotations with each expert's annotations which are regarded as gold standard. The average dialogue act F1 is 95.36\\% and the average state accuracy is 94.95\\%, which indicates the high quality of our annotations.\n\n\\section{Statistics}\n\\label{sec:statistics}\nAfter removing uncompleted dialogues, we collected 6,012 dialogues in total. The dataset is split randomly for training/validation/test, where the statistics are shown in Table \\ref{tab:split}. The average number of sub-goals in our dataset is 3.24, which is much larger than that in MultiWOZ (1.80) \\cite{Multiwoz2018} and Schema (1.84) \\cite{SchemaGuided}. The average number of turns (16.9) is also larger than that in MultiWOZ (13.7). These statistics indicate that our dialogue data are more complex. \n\nAccording to the type of user goal, we group the dialogues in the training set into five categories:\n\\begin{description}\n    \\item[Single-domain (S)] 417 dialogues have only one sub-goal in HAR domains.\n    \n    \\item[Independent multi-domain (M)] 1573 dialogues have multiple sub-goals (2$\\sim$3) in HAR domains. However, these sub-goals do not have cross-domain informable slots.\n    \n    \\item[Independent multi-domain + traffic (M+T)] 691 dialogues have multiple sub-goals in HAR domains and at least one sub-goal in the metro or taxi domain (3$\\sim$5 sub-goals). The sub-goals in HAR domains do not have cross-domain informable slots.\n    \n    \\item[Cross multi-domain (CM)] 1,759 dialogues have multiple sub-goals (2$\\sim$5) in HAR domains with cross-domain informable slots. \n    \n    \\item[Cross multi-domain + traffic (CM+T)] 572 dialogues have multiple sub-goals in HAR domains with cross-domain informable slots and at least one sub-goal in the metro or taxi domain (3$\\sim$5 sub-goals). \n\\end{description}\n\n\\begin{table}[]\n    \\centering\n    \\setlength{\\tabcolsep}{1.6mm}{\n    \\begin{tabular}{lccc}\n    \\toprule\n       & Train   & Valid  & Test   \\\\\n    \\midrule\n    \\# Dialogues                                                           & 5,012    & 500    & 500    \\\\ \n    \\# Turns                                                               & 84,692   & 8,458   & 8,476   \\\\\n    \\# Tokens                                                              & 1,376,033 & 137,736 & 137,427 \\\\\n    Vocab                                                                  & 12,502   & 5,202   & 5,143  \\\\\n    Avg. sub-goals  & 3.24    & 3.26   & 3.26   \\\\\n    Avg. STs & 14.8 & 14.9 & 15.0\\\\\n    Avg. turns     & 16.9   & 16.9  & 17.0  \\\\\n    Avg. tokens         & 16.3   & 16.3  & 16.2  \\\\\n    \\bottomrule\n    \\end{tabular}}\n    \\caption{Data statistics. The average numbers of sub-goals, turns, and STs (semantic tuples) are for each dialogue. The average number of tokens is for each turn.\\quad~}\n    \\label{tab:split}\n\\end{table}\n\nThe data statistics are shown in Table \\ref{tab:5_type}. \nAs mentioned in Section \\ref{goal_generation}, we generate independent multi-domain, cross multi-domain, and traffic domain sub-goals one by one. \nThus in terms of the task complexity, we have \\textbf{S<M<CM} and \\textbf{M<M+T<CM+T}, which is supported by the average number of sub-goals, semantic tuples, and turns per dialogue in Table \\ref{tab:5_type}. The average number of tokens also becomes larger when the goal becomes more complex. \nAbout 60\\% of dialogues (\\textbf{M+T}, \\textbf{CM}, and \\textbf{CM+T}) have cross-domain informable slots.\nBecause of the limit of maximal sub-goals number, the ratio of dialogue number of \\textbf{CM+T} to \\textbf{CM} is smaller than that of \\textbf{M+T} to \\textbf{M}.\n\n\\textbf{CM} and \\textbf{CM+T} are much more challenging than other tasks because additional cross-domain constraints in HAR domains are strict and will result in more \"NoOffer\" situations (i.e., the wizard finds no result that satisfies the current constraints). \nIn this situation, the wizard will try to relax some constraints and issue multiple queries to find some results for a recommendation while the user will compromise and change the original goal.\nThe negotiation process is captured by \"NoOffer rate\", \"Multi-query rate\", and \"Goal change rate\" in Table \\ref{tab:5_type}. \nIn addition, \"Multi-query rate\" suggests that each sub-goal in \\textbf{M} and \\textbf{M+T} is as easy to finish as the goal in \\textbf{S}.\n\nThe distribution of dialogue length is shown in Figure \\ref{fig:turns_num_dist}, which is an indicator of the task complexity. Most single-domain dialogues terminate within 10 turns. The curves of \\textbf{M} and \\textbf{M+T} are almost of the same shape, which implies that the traffic task requires two additional turns on average to complete the task. The curves of \\textbf{CM} and \\textbf{CM+T} are less similar. This is probably because \\textbf{CM} goals that have 5 sub-goals (about 22\\%) can not further generate a sub-goal in traffic domains and become \\textbf{CM+T} goals.\n\n\\begin{table}[t]\n    \\setlength{\\tabcolsep}{0.7mm}{\n    \\begin{tabular}{lccccc}\n    \\toprule\n        Goal type & S & M & M+T & CM & CM+T \\\\\n    \\midrule\n        \\# Dialogues & 417 & 1573  & 691 & 1759 & 572 \\\\\n        NoOffer rate & 0.10 & 0.22 & 0.22 & 0.61 & 0.55\\\\\n        Multi-query rate & 0.06 & 0.07 & 0.07 & 0.14 & 0.12 \\\\  \n        Goal change rate & 0.10 & 0.28 & 0.31 & 0.69 & 0.63\\\\\n        Avg. dialogue acts & 1.85 & 1.90 & 2.09 & 2.06 & 2.11\\\\\n        Avg. sub-goals & 1.00 & 2.49 &  3.62 & 3.87 & 4.57\\\\\n        Avg. STs & 4.5 & 11.3 & 15.8 & 18.2 & 20.7\\\\\n        Avg. turns & 6.8 & 13.7 & 16.0 & 21.0 & 21.6\\\\\n        Avg. tokens & 13.2 & 15.2 & 16.3 & 16.9 & 17.0\\\\\n        \n    \\bottomrule\n    \\end{tabular}}\n    \\tabcaption{Statistics for dialogues of different goal types in the training set. NoOffer rate and Goal change rate are for each dialogue. Multi-query rate is for each system turn. The average number of dialogue acts is for each turn.}\n    \\label{tab:5_type}\n\\end{table}\n\n\\begin{figure}[h]\n    \\includegraphics[width=\\linewidth]{turns-num-dist-2.png}\n    \\caption{Distributions of dialogue length for different goal types in the training set.}\n    \\label{fig:turns_num_dist}\n\\end{figure}\n\n\\section{Corpus Features}\nOur corpus is unique in the following aspects:\n\\begin{itemize}\n    \\item Complex user goals are designed to favor inter-domain dependency and natural transition between multiple domains. In return, the collected dialogues are more complex and natural for cross-domain dialogue tasks. \n    \n    \\item A well-controlled, synchronous setting is applied to collect human-to-human dialogues. This ensures the high quality of the collected dialogues.\n    \n    \\item Explicit annotations are provided at not only the system side but also the user side. This feature allows us to model user behaviors or develop user simulators more easily.  \n\\end{itemize}\n\n\\section{Benchmark and Analysis}\n\n\\begin{table*}[ht]\n\\centering\n\\setlength{\\tabcolsep}{1.3mm}{\n\\begin{tabular}{llcccccc}\n\\toprule\n                           &                        & S     & M     & M+T   & CM    & CM+T  & Overall \\\\\n\\midrule\nBERTNLU   & \\multirow{2}{*}{Dialogue act F1}        & 96.69 & 96.01 & 96.15 & 94.99 & 95.38 & 95.53       \\\\\n\\ -- context & & 94.55 & 93.05 & 93.70 & 90.66 & 90.82 & 91.85 \\\\\n\\midrule\nRuleDST                    & Joint state accuracy (single turn)  & 84.17      &  78.17     & 81.93      & 63.38      & 67.86      & 71.33        \\\\\nTRADE                      & Joint state accuracy   &  71.67     & 45.29      & 37.98      &       30.77 & 25.65       & 36.08         \\\\\n\\midrule\n\\multirow{2}{*}{SL policy} & Dialogue act F1        & 50.28      & 44.97   & 54.01       & 41.65       & 44.02       &  44.92       \\\\\n                          & Dialogue act F1 (delex) & 67.96      & 67.35      & 73.94      & 62.27       & 66.29       & 66.02         \\\\\n\\midrule\n\\multirow{2}{*}{Simulator} & Joint state accuracy (single turn)   & 63.53 & 48.79 & 50.26 & 40.66 & 41.76 & 45.00   \\\\\n                          & Dialogue act F1 (single turn)        & 85.99      & 81.39      & 80.82      & 75.27      &  77.23     &   78.39      \\\\\n\\midrule\nDA Sim  & \\multirow{3}{*}{Task finish rate} & 76.5 & 49.4 & 33.7 & 17.2 & 15.7 & 34.6\\\\\n NL Sim (Template)  & &67.4&33.3&29.1& 10.0 & 10.0 & 23.6 \\\\\n NL Sim (SC-LSTM)  & &60.6&27.1&23.1& 8.8 & 9.0 & 19.7\\\\\n\\bottomrule\n\\end{tabular}}\n\\caption{Performance of Benchmark models. \"Single turn\" means having the gold information of the last turn. Task finish rate is evaluated on 1000 times simulations for each goal type. It's worth noting that \"task finish\" does not mean the task is successful, because the system may provide wrong information. Results show that cross multi-domain dialogues (\\textbf{CM} and \\textbf{CM+T}) is challenging for these tasks.}\n\\label{tab:experiments}\n\\end{table*}\n\n\\textbf{\\datasetName} can be used in different tasks or settings of a task-oriented dialogue system. To facilitate further research,\nwe provided benchmark models for different components of a pipelined task-oriented dialogue system (Figure \\ref{fig:framework}), including natural language understanding (NLU), dialogue state tracking (DST), dialogue policy learning, and natural language generation (NLG). \nThese models are implemented using ConvLab-2 \\cite{zhu2020convlab2}, an open-source task-oriented dialog system toolkit.\nWe also provided a rule-based user simulator, which can be used to train dialogue policy and generate simulated dialogue data.\nThe benchmark models and simulator will greatly facilitate researchers to compare and evaluate their models on our corpus.\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=0.8\\linewidth]{framework.png}\n    \\caption{\n    Pipelined user simulator (left) and Pipelined task-oriented dialogue system (right). Solid connections are for natural language level interaction, while dashed connections are for dialogue act level. The connections without comments represent dialogue acts.\n    }\n    \\label{fig:framework}\n\\end{figure}\n\n\\subsection{Natural Language Understanding}\n\\textbf{Task}:\nThe natural language understanding component in a task-oriented dialogue system takes an utterance as input and outputs the corresponding semantic representation, namely, a dialogue act. The task can be divided into two sub-tasks: intent classification that decides the intent type of an utterance, and slot tagging which identifies the value of a slot.\\\\\n\n\\noindent\\textbf{Model}:\nWe adapted BERTNLU from ConvLab-2. \nBERT \\cite{bert2019} has shown strong performance in many NLP tasks. \nWe use Chinese pre-trained BERT\\footnote{BERT-wwm-ext model in \\url{https://github.com/ymcui/Chinese-BERT-wwm}} \\cite{chineseBERT2019}\nfor initialization and then fine-tune the parameters on \\datasetName.\nWe obtain word embeddings and the sentence representation (embedding of [CLS]) from BERT. \nSince there may exist more than one intent in an utterance, we modify the traditional method accordingly. \nFor dialogue acts of \\emph{inform} and \\emph{recommend} intents such as (intent=Inform, domain=Attraction, slot=fee, value=free) whose values appear in the sentence, we perform sequential labeling using an MLP which takes word embeddings (\"free\") as input and outputs tags in BIO schema (\"B-Inform-Attraction-fee\"). \nFor each of the other dialogue acts (e.g., (intent=Request, domain=Attraction, slot=fee)) that do not have actual values, we use another MLP to perform binary classification on the sentence representation to predict whether the sentence should be labeled with this dialogue act.\nTo incorporate context information, we use the same BERT to get the embedding of last three utterances. We separate the utterances with [SEP] tokens and insert a [CLS] token at the beginning. \nThen each original input of the two MLP is concatenated with the context embedding (embedding of [CLS]), serving as the new input.\nWe also conducted an ablation test by removing context information. \nWe trained models with both system-side and user-side utterances.\n\n\\begin{table}[]\n    \\footnotesize\n    \\centering\n    \\setlength{\\tabcolsep}{0.45mm}{\n    \\begin{tabular}{lcccccc}\n    \\toprule\n        & General & Inform & Request & Recom & NoOffer & Select \\\\\n        BERTNLU & 99.45 & 94.67 & 96.57 & 98.41 & 93.87 & 82.25 \\\\\n        \\ -- context & 99.69 & 90.80 & 91.98 & 96.92 & 93.05 & 68.40 \\\\\n    \\bottomrule\n    \\end{tabular}}\n    \\caption{F1 score of different intent type. \"Recom.\" represents \"Recommend\".}\n    \\label{tab:nlu_res}\n\\end{table}\n\n\\noindent\\textbf{Result Analysis}:\nThe results of the dialogue act prediction (F1 score) are shown in Table \\ref{tab:experiments}. We further tested the performance on different intent types, as shown in Table \\ref{tab:nlu_res}. \nIn general, BERTNLU performs well with context information.\nThe performance on cross multi-domain dialogues (\\textbf{CM} and \\textbf{CM+T}) drops slightly, which may be due to the decrease of \"General\" intent and the increase of \"NoOffer\" as well as \"Select\" intent in the dialogue data.\nWe also noted that the F1 score of \"Select\" intent is remarkably lower than those of other types, but context information can improve the performance significantly.\nSince recognizing domain transition is a key factor for a cross-domain dialogue system, natural language understanding models need to utilize context information more effectively.\n\n\\subsection{Dialogue State Tracking}\n\\textbf{Task}:\nDialogue state tracking is responsible for recognizing user goals from the dialogue context and then encoding the goals into the pre-defined system state.\nTraditional state tracking models take as input user dialogue acts parsed by natural language understanding modules, while recently there are joint models obtaining the system state directly from the context.\\\\\n\n\\noindent\\textbf{Model}:\nWe implemented a rule-based model (RuleDST) and adapted TRADE (Transferable Dialogue State Generator)\\footnote{\\url{https://github.com/jasonwu0731/trade-dst}} \\cite{TRADE} in this experiment. RuleDST takes as input the previous system state and the last user dialogue acts. Then, the system state is updated according to hand-crafted rules. \nFor example, If one of user dialogue acts is (intent=Inform, domain=Attraction, slot=fee, value=free), then the value of the \"fee\" slot in the attraction domain will be filled with \"free\". \nTRADE generates the system state directly from all the previous utterances using a copy mechanism. \nAs mentioned in Section \\ref{sec:3.3.2}, the first query of the system often records full user constraints, while the last one records relaxed constraints for recommendation. Thus the last one involves system policy, which is out of the scope of state tracking. We used the first query for these models and left state tracking with recommendation for future work.\\\\\n\n\\noindent\\textbf{Result Analysis}:\nWe evaluated the joint state accuracy (percentage of exact matching) of these two models (Table \\ref{tab:experiments}).\nTRADE, the state-of-the-art model on MultiWOZ, performs poorly on our dataset, indicating that more powerful state trackers are necessary.\nAt the test stage, RuleDST can access the previous gold system state and user dialogue acts, which leads to higher joint state accuracy than TRADE. \nBoth models perform worse on cross multi-domain dialogues (\\textbf{CM} and \\textbf{CM+T}).\nTo evaluate the ability of modeling cross-domain transition, we further calculated joint state accuracy for those turns that receive \"Select\" intent from users (e.g., \"Find a hotel near the attraction\").\nThe performances are 11.6\\% and 12.0\\% for RuleDST and TRADE respectively, showing that they are not able to track domain transition well.\n\n\\subsection{Dialogue Policy Learning}\n\\textbf{Task}:\nDialogue policy receives state $s$ and outputs system action $a$ at each turn. Compared with the state given by a dialogue state tracker, $s$ may have more information, such as the last user dialogue acts and the entities provided by the backend database.\\\\\n\n\\noindent\\textbf{Model}:\nWe adapted a vanilla policy trained in a supervised fashion from ConvLab-2 (SL policy). \nThe state $s$ consists of the last system dialogue acts, last user dialogue acts, system state of the current turn, the number of entities that satisfy the constraints in the current domain, and a terminal signal indicating whether the user goal is completed. The action $a$ is delexicalized dialogue acts of current turn which ignores the exact values of the slots, where the values will be filled back after prediction.\\\\\n\n\\noindent\\textbf{Result Analysis}:\nAs illustrated in Table \\ref{tab:experiments}, there is a large gap between F1 score of exact dialogue act and F1 score of delexicalized dialogue act, which means we need a powerful system state tracker to find correct entities.\nThe result also shows that cross multi-domain dialogues (\\textbf{CM} and \\textbf{CM+T}) are harder for system dialogue act prediction. \nAdditionally, when there is \"Select\" intent in preceding user dialogue acts, the F1 score of exact dialogue act and delexicalized dialogue act are 41.53\\% and 54.39\\% respectively. This shows that the policy performs poorly \nfor cross-domain transition.\n\n\\subsection{Natural Language Generation}\n\\textbf{Task}:\nNatural language generation transforms a structured dialogue act into a natural language sentence. It usually takes delexicalized dialogue acts as input and generates a template-style sentence that contains placeholders for slots. Then, the placeholders will be replaced by the exact values, which is called lexicalization.\\\\\n\n\\noindent\\textbf{Model}:\nWe provided a template-based model (named TemplateNLG) and SC-LSTM (Semantically Conditioned LSTM) \\cite{wen2015sclstm}  for natural language generation. For TemplateNLG, we extracted templates from the training set and manually added some templates for infrequent dialogue acts. For SC-LSTM we adapted the implementation\\footnote{\\url{https://github.com/andy194673/nlg-sclstm-multiwoz}} on MultiWOZ and trained two SC-LSTM with system-side and user-side utterances respectively.\\\\ \n\n\\noindent\\textbf{Result Analysis}:\nWe calculated corpus-level BLEU as used by \\citet{wen2015sclstm}. We took all utterances with the same delexcalized dialogue acts as references (100 references on average), which results in high BLEU score.\nFor user-side utterances, the BLEU score for TemplateNLG is 0.5780, while the BLEU score for SC-LSTM is 0.7858. For system-side, the two scores are 0.6828 and 0.8595.\nAs exemplified in Table \\ref{tab:nlg_case}, the gap between the two models can be attributed to that SC-LSTM generates common pattern while TemplateNLG retrieves \noriginal sentence which has more specific information.\nWe do not provide BLEU scores for different goal types (namely, \\textbf{S}, \\textbf{M}, \\textbf{CM}, etc.) because BLEU scores on different corpus are not comparable.\n \n\n\\begin{table}[]\n\\begin{CJK*}{UTF8}{gbsn}\n    \\small\n    \\centering\n    \\setlength{\\tabcolsep}{0.6mm}{\n    \\begin{tabular}{p{7.5cm}}\n    \\toprule\n        Input: \\\\\n        (Inform, Restaurant, name, \\$name)\\\\ \n        (Inform, Restaurant, cost, \\$cost)\\\\\n    \\midrule\n        SC-LSTM: \\\\\n        为您推荐\\$name，人均消费\\$cost。\\\\\n        I Recommend you \\$name. It costs \\$cost. \\\\\n    \\midrule\n        TemplateNLG: \\\\\n        1) \\$name是个不错的选择，但是它的人均消费是\\$cost。\\\\\n        $\\quad $ \\$name is a nice choice. But it costs \\$cost. \\\\\n        2) 您想吃的菜不需要花那么多钱呢。为您推荐\\$name，人均消费\\$cost哟。\\\\\n        $\\quad $ The dish you want doesn't cost so much. I recommend you \\$name. It costs \\$cost.\\\\\n    \\bottomrule\n    \\end{tabular}}\n    \\caption{\n    Comparison of SC-LSTM and TemplateNLG. The input is delexicalized dialogue acts, where the actual values are replaced with \\$name and \\$cost. Two retrieved results are shown for TemplateNLG.\n    }\n    \\label{tab:nlg_case}\n\\end{CJK*}\n\\end{table}\n\n\\subsection{User Simulator}\n\n\\textbf{Task}:\nA user simulator imitates the behavior of users, which is useful for dialogue policy learning and automatic evaluation. \nA user simulator at dialogue act level (e.g., the \"Usr Policy\" in Figure \\ref{fig:framework}) receives the system dialogue acts and outputs user dialogue acts, while a user simulator at natural language level (e.g., the left part in Figure \\ref{fig:framework}) directly takes system's utterance as input and outputs user's utterance.\\\\\n\n\\noindent\\textbf{Model}:\nWe built a rule-based user simulator that works at dialogue act level. Different from agenda-based \\cite{agenda2007schatzmann} user simulator that maintains a stack-like agenda, our simulator maintains the user state straightforwardly (Section \\ref{sec:user-side}). \nThe simulator will generate a user goal as described in Section \\ref{goal_generation}. \nAt each user turn, the simulator receives system dialogue acts, modifies its state, and outputs user dialogue acts according to some hand-crafted rules.\nFor example, if the system inform the simulator that the attraction is free, then the simulator will fill the \"fee\" slot in the user state with \"free\", and ask for the next empty slot such as \"address\".\nThe simulator terminates when all requestable slots are filled, and all cross-domain informable slots are filled by real values. \\\\\n\n\\noindent\\textbf{Result Analysis}:\nDuring the evaluation, we initialized the user state of the simulator using the previous gold user state. The input to the simulator is the gold system dialogue acts. We used joint state accuracy (percentage of exact matching) to evaluate user state prediction and F1 score to evaluate the prediction of user dialogue acts. \nThe results are presented in Table \\ref{tab:experiments}. \nWe can observe that the performance on complex dialogues (\\textbf{CM} and \\textbf{CM+T}) is remarkably lower than that on simple ones (\\textbf{S}, \\textbf{M}, and \\textbf{M+T}). This simple rule-based simulator is provided to facilitate dialogue policy learning and automatic evaluation, and our corpus supports the development of more elaborated simulators as we provide the annotation of user-side dialogue states and dialogue acts.\n\n\\subsection{Evaluation with User Simulation}\n\nIn addition to corpus-based evaluation for each module, we also evaluated the performance of a whole dialogue system using the user simulator as described above. Three configurations were explored:\n\\begin{description}\n    \\item[DA Sim] Simulation at dialogue act level. As shown by the dashed connections in Figure \\ref{fig:framework}, we used the aforementioned simulator at the user side and assembled the dialogue system with RuleDST and SL policy.\n    \n    \\item[NL Sim (Template)] Simulation at natural language level using TemplateNLG. As shown by the solid connections in Figure \\ref{fig:framework}, the simulator and the dialogue system were equipped with BERTNLU and TemplateNLG additionally.\n    \n    \\item[NL Sim (SC-LSTM)] Simulation at natural language level using SC-LSTM. TemplateNLG was replaced with SC-LSTM in the second configuration.\n\\end{description}\n\nWhen all the slots in a user goal are filled by real values, the simulator terminates. This is regarded as \"task finish\". It's worth noting that \"task finish\" does not mean the task is success, because the system may provide wrong information. We calculated \"task finish rate\" on 1000 times simulations for each goal type (See Table \\ref{tab:experiments}). \nFindings are summarized below:\n\n\\begin{enumerate}\n    \\item Cross multi-domain tasks (\\textbf{CM} and \\textbf{CM+T}) are much harder to finish. \n    Comparing \\textbf{M} and \\textbf{M+T}, although each module performs well in traffic domains, additional sub-goals in these domains are still difficult to accomplish. \n    \n    \n    \\item The system-level performance is largely limited by RuleDST and SL policy.\n    Although the corpus-based performance of NLU and NLG modules is high, the two modules\n    still harm the performance. Thus more powerful models are needed for all components of a pipelined dialogue system.\n    \n    \\item TemplateNLG has a much lower BLEU score but performs better than SC-LSTM in natural language level simulation. This may be attributed to that BERTNLU prefers templates retrieved from the training set.\n    \n    \n\\end{enumerate}\n\n\\section{Conclusion}\nIn this paper, we present the first large-scale Chinese Cross-Domain task-oriented dialogue dataset, \\textbf{\\datasetName}. \nIt contains 6K dialogues and 102K utterances for 5 domains, with the annotation of dialogue states and dialogue acts at both user and system sides.\nAbout 60\\% of the dialogues have cross-domain user goals, which encourage natural transition between related domains.\nThanks to the rich annotation of dialogue states and dialogue acts at both user side and system side, this corpus provides a new testbed for a wide range of tasks to investigate cross-domain dialogue modeling, such as dialogue state tracking, policy learning, etc.\nOur experiments show that the cross-domain constraints are challenging for all these tasks. The transition between related domains is especially challenging to model.\nBesides corpus-based component-wise evaluation, we also performed system-level evaluation with a user simulator, which requires more powerful models for all components of a pipelined cross-domain dialogue system.\n\n\\section*{Acknowledgments}\nThis work was supported by the National Science Foundation of China (Grant No. 61936010/61876096) and the National Key R\\&D Program of China (Grant No. 2018YFC0830200). \nWe would like to thank THUNUS NExT JointLab for the support. \nWe would also like to thank Ryuichi Takanobu and Fei Mi for their constructive comments. We are grateful to our action editor, Bonnie Webber, and the anonymous reviewers for their valuable suggestions and feedback.\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{MultiWOZ 2.2 : A Dialogue Dataset with Additional Annotation Corrections and State Tracking Baselines}\n\n\\begin{document}\n\n\\maketitle\n\\begin{abstract}\nMultiWOZ \\cite{budzianowski2018multiwoz} is a well-known task-oriented dialogue dataset containing over 10,000 annotated dialogues spanning 8 domains. It is extensively used as a benchmark for dialogue state tracking. However, recent works have reported presence of substantial noise in the dialogue state annotations. MultiWOZ 2.1 \\cite{eric2019multiwoz} identified and fixed many of these erroneous annotations and user utterances, resulting in an improved version of this dataset. This work introduces MultiWOZ 2.2, which is a yet another improved version of this dataset. Firstly, we identify and fix dialogue state annotation errors across 17.3\\% of the utterances on top of MultiWOZ 2.1. Secondly, we redefine the ontology by disallowing vocabularies of slots with a large number of possible values (e.g., restaurant name, time of booking). In addition, we introduce slot span annotations for these slots to standardize them across recent models, which previously used custom string matching heuristics to generate them. We also benchmark a few state of the art dialogue state tracking models on the corrected dataset to facilitate comparison for future work. In the end, we discuss best practices for dialogue data collection that can help avoid annotation errors. \n\\end{abstract}\n\n\\section{Introduction}\nTask-oriented dialogue systems have become very popular in the recent years. Such systems assist the users in accomplishing different tasks by helping them interact with APIs using natural language. Dialogue systems consist of multiple modules which work together to facilitate such interactions. Most architectures have a natural language understanding and dialogue state tracking module to generate a structured representation of user's preferences from the dialogue history. This structured representation is used to make API calls and as a signal for other modules. Then, the dialogue policy module determines the next actions to be taken by the dialogue system. This is followed by the natural language generation module, which converts the generated actions to a natural language utterance, which is surfaced to the user.\n\nRecently, data-driven techniques have achieved state-of-the-art performance for the different dialogue systems modules~\\cite{Wen_2017, Ren_2018, zhang2019find,chao2019bert}. However, collecting high quality annotated dialogue datasets remains a challenge for the researchers because of the extensive annotation required for training the modules mentioned above. Many public datasets like DSTC2 \\cite{henderson2014second}, WOZ~\\cite{wen2017network}, SimulatedDialogue~\\cite{shah2018building}, MultiWOZ~\\cite{budzianowski2018multiwoz}, TaskMaster~\\cite{byrne2019taskmaster}, SGD~\\cite{rastogi2019scalable}, etc. have been very useful to facilitate research in this area. Among these datasets, MultiWOZ is the most widely used benchmark for dialogue state tracking. It contains over 10,000 dialogues spanning 8 domains, namely - Restaurant, Hotel, Attraction, Taxi, Train, Hospital, Bus, and Police. \n\nSince its inception, the MultiWOZ dataset has undergone a few updates. \\citet{lee2019convlab} introduced user dialogue actions providing a structured semantic representation for user utterances. \\citet{eric2019multiwoz} fixed 32\\% of dialogue state annotations across 40\\% of the turns and introduced slot descriptions, culminating in MultiWOZ 2.1, a new version of the dataset. Despite the large scale of corrections introduced in MultiWOZ 2.1, there are still many unaddressed annotation errors ~\\cite{zhang2019find}. Furthermore, several approaches to dialogue state tracking use span annotations identifying the locations in the user and system utterances where slot values have been mentioned, to make the system efficient and generalizable to new slot values~\\cite{rastogi2017scalable,WuTradeDST2019,zhang2019find,rastogi2019scalable, xu-hu-2018-end,zhou2019multidomain, Gao_2019}. Because of the absence of these span annotations in MultiWOZ, these approaches resort to generating them using custom string matching heuristics, making their comparison difficult. \n\nTo address these limitations, we introduce MultiWOZ 2.2\\footnote{The dataset is available at \\url{https://github.com/budzianowski/multiwoz}.}, an updated version of the MultiWOZ dataset. Our contributions are threefold. \n\\begin{enumerate}[leftmargin=*]\n    \\item We identify the annotation errors, inconsistencies, and ontology issues in MultiWOZ 2.1, and publish its improved version.\n    \\item We add slot span annotations for user and system utterances to standardize them across future models. We also annotate the active user intents and requested slots for each user utterance.\n    \\item We benchmark a few state-of-the-art dialogue state tracking models on the corrected dataset to facilitate comparison for future work.\n\\end{enumerate}\n\nThe paper is organized as follows. First we describe the different types of annotation errors and inconsistencies we observed in MultiWOZ 2.1 (Section \\ref{sec:issues}). Then, we outline the redefinition of ontology (Section \\ref{sec:ontology}), followed by the description of correction procedure (Section \\ref{sec:correction}) and new annotations we introduce (Section~\\ref{sec:additional}). Finally, in Section \\ref{sec:benchmark}, we present the performance of a few recent dialogue state tracking models on MultiWOZ 2.2.\n\n\\begin{figure*}[t]\n\\centering\n\\begin{minipage}{15.6cm}\\vspace{0mm}    \\centering\n\\begin{tcolorbox}[colback=yellow!5!white]\n    \\centering\n    \\small\n    \\begin{tabular}{ L {61mm}  C {35mm}  C {35mm}}\n     \\textbf{Example Dialogue Segment}& \\textbf{MultiWOZ 2.1} & \\textbf{MultiWOZ 2.2} \\\\\n    \\midrule\n    \\textbf{1. Early Markup} & & \\\\\n        \\textcolor{blue}{User:} Help me find a moderate priced british food place please. & r-food=british, r-pricerange=moderate, \\textcolor{red}{r-name=one seven} & r-food=british, r-pricerange=moderate  \\\\\n    \\textcolor{blue}{Sys:} restaurant one seven is a nice place. Do you want to book? & & \\\\ \\midrule\n    \n    \\textbf{2. Annotation from Database} & & \\\\[3pt]\n    \\textcolor{blue}{User:} Can you give me the address to the hospital in Cambridge? & \\textcolor{red}{hospital-department=acute medical assessment unit} & -no update- \\\\\n    \\textcolor{blue}{Sys:} The address is Hills Rd, Cambridge Postcode: CB20QQ & & \\\\ \\midrule\n    \n    \\textbf{3. Typo} & & \\\\\n    \\textcolor{blue}{Sys:} Okay, I can help with that. What day and time would you like to dine and how many people should I have the reservation for? & r-bookday=thursday, \\textcolor{red}{r-booktime=15:00}, hotel-area=west & r-bookday=thursday, \\textcolor{OliveGreen}{r-booktime=5:00}, hotel-area=west\\\\\n    \\textcolor{blue}{User:} On Thursday at 5:00. I also need a hotel in the same area. No need to have free parking. & & \\\\ \\midrule\n    \n    \\textbf{4. Implicit Time Processing} & & \\\\\n    \\textcolor{blue}{User:} Can I get the postcode for that? I also need to book a taxi to the Golden Wok. & r-name=Golden Wok, r-bookday=friday, r-booktime=11:00, \\textcolor{red}{taxi-leaveAt=friday}, taxi-destination=Golden Wok& r-name=Golden Wok, r-bookday=friday, r-booktime=11:00, taxi-destination=Golden Wok \\\\\n    \\textcolor{blue}{Sys:} The postcode is cb21tt. Are you looking for a taxi from Old Schools to the Golden Wok? & & \\\\\n    \\textcolor{blue}{User:} Yes I do. I'd like to make sure I arrive at the restaurant by the booked time. Can you check? & r-name=Golden Wok, r-bookday=friday, r-booktime=11:00, \\textcolor{red}{taxi-leaveAt=friday}, \\textcolor{red}{taxi-arriveby=10:45} & r-name=Golden Wok, r-bookday=friday, r-booktime=11:00, \\textcolor{OliveGreen}{taxi-arriveby=11:00}\\\\\n \\vspace{-5mm}\n    \\end{tabular}\n\\end{tcolorbox}\n\\vspace{-2mm}\n\\caption{Examples of hallucinated values in MultiWOZ 2.1 and the corrections in MultiWOZ 2.2. Please note that we omit state annotations unrelated to the extracted utterances. ``r\" used in the slot name in the right two columns is an abbreviation of restaurant.}\n    \\label{fig:ann_error}\n\\end{minipage}\n\\end{figure*}\n\n\\section{Annotation Errors}\\label{sec:issues}\nThe MultiWOZ dataset was collected using a Wizard-of-Oz setup~\\cite{kellywoz1984}. In this setup, two crowd-workers are paired together, one acting as a user and the other as the dialogue agent. Each dialogue is driven by a unique set of instructions specifying the user goal, which are shared with the crowd-worker playing the role of the user. After every user turn, the crowd-worker playing the role of the dialogue agent (wizard) annotates the updated dialogue state. After updating the state, the tool shows the set of entities matching the dialogue state to the wizard, who then uses it to generate a response which is sent to the user. Remaining annotations such as the system actions are collected using a second annotation task.\n\nThe Wizard-of-Oz setup is widely considered to produce natural conversations, as there is no turn level intervention guiding the flow of the dialogue. However, because of its heavy reliance on humans for generating the correct annotations, the procedure is prone to noisy annotations. We identified two major classes of errors outlined below, which were not corrected in MultiWOZ 2.1.\n\n\\subsection{Hallucinated Values}\n\\label{sec:hallucinated}\nHallucinated values are present in dialogue state without being specified in the dialogue history. We observed four different types of such errors, which are shown in Figure~\\ref{fig:ann_error} and described below. \n\n\\begin{enumerate}[style=nextline,leftmargin=*]\n    \\item \\textbf{Early Markups:} These values have been mentioned by the agent in a future utterance. Since the user has not accepted them yet, they should be excluded from the dialogue state.\n    \\item \\textbf{Annotations from Database:}\n    These values are not mentioned in the dialogue at all, even in the future utterances. They appear to be incorrectly added by the wizard based on results of the database call.%contents of the API results.\n    \\item \\textbf{Typos:} These values cannot be found in the dialogue history because of a typographical error. These errors occur since slot values are entered as free-form text in the annotation interface.\n    \\item \\textbf{Implicit Time Processing:} This specifically relates to slots taking time as a value. Sometimes, the value is obtained by adding or subtracting some pre-determined duration from the time specified in dialogue history (Figure~\\ref{fig:ann_error}). In other cases, it is implicitly rounded off to closest quarter (Dialogue 1 in Figure~\\ref{fig:inconsis_update}). This further burdens models with learning temporal arithmetic.\n\\end{enumerate}\n\nWe observed that the errors mentioned above are quite frequent.\nIn total we found that hallucinated values appear in 3128 turns across 948 dialogues in the MultiWOZ 2.1 dataset.\n\n\\subsection{Inconsistent State Updates}\n\\label{sec:inconsis_issues}\n\n\\begin{table*}[ht]\n\\centering\n    \\begin{tabular}[t]{|L {20mm} | L {60mm} | L {50mm}|}\\hline\n            \\textbf{Source} & \\textbf{User utterance} & \\textbf{Dialogue state update}\\\\\\hline\n            Ontology & I need to arrive by 8:00. & train-arriveby=08:00\\\\ \\hline\n            Dialogue history& Sometime after 5:45 PM would be great.& train-leaveat=5:45pm\\\\ \\hline\n            None & I plan on getting lunch first, so sometime after then I'd like to leave. & train-leaveat=after lunch\\\\ \\hline\n    \\end{tabular}\n    \\caption{Example of slot values annotated using different strategies in ``PMUL0897.json\", `MUL0681.json`\", and ``PMUL3200.json\" in MultiWOZ 2.1.}\n    \\label{table:diff_anns}\n\\end{table*}\n\nWe also encountered annotations in MultiWOZ 2.1 that are semantically correct, but don't follow consistent annotation guidelines. Inconsistencies arise in the dialogue state because of three main reasons:\n\\begin{enumerate}[leftmargin=*]\n    \\item \\textbf{Multiple Sources:} A slot value may be introduced in the dialogue state through various sources. It may either be mentioned by the user, offered by the system, carried over from another slot in the dialogue state of a different domain, or be a part of the ontology.\n    \\item \\textbf{Value Paraphrasing:} The same slot value can be mentioned in many different ways, often within the same dialogue e.g. the value ``18:00\" for the slot time may be mentioned as ``6 pm\", ``1800\", ``0600 pm\", ``evening at 6\" etc.\n    \\item \\textbf{Inconsistent tracking strategy:} Crowd-workers have inconsistent opinions on which slot values should be tracked in the same dialogue context. For example, some workers track all slot values that the user agrees with while others only track user-specified slot values.\n\\end{enumerate}\n\nTable~\\ref{table:diff_anns} shows dialogue state update from three different sources for similar slots from different dialogues in MultiWOZ 2.1. In the first case, the value ``08:00\" for slot \\textit{train-arriveby} comes from the ontology, despite the presence of an equivalent value ``8:00\" in the user utterance. On the other hand, in the second example, the slot value in the dialogue state comes from the user utterance despite the ontology listing ``17:45\" as a value for the slot \\textit{train-leaveat}. In the third example, the value of \\textit{train-leaveat} is not derived from any of the sources mentioned above, but is generated by incorporating the semantics. The slot value can be mentioned in multiple ways, but in order to evaluate a dialogue system fairly, it's necessary to either maintain a consistent rule for deciding how the value is picked among all the mentions or consider all the mentions as the correct answer. MultiWOZ 2.1 gives one unique correct answer for each dialogue state but lacks an explicit rule on how it is determined. This inconsistency confuses the model during training and unfairly penalizes it during evaluation if it outputs a slot value which is different but equivalent to the one listed in ground truth.\n\n\\begin{figure*}[t]\n\\centering\n\\includegraphics[width=0.98\\textwidth]{figures/state_inconsistency.png}\n\\caption{Example of dialogues states being updated differently in similar scenarios. In both dialogues, user accepts a train offered by the system. In dialogue 1, \\textit{train-arriveby} is annotated in the dialogue state after user's agreement, but not in dialogue 2. Dialogue 1 also shows implicit time processing, where the value 12:08 in the system utterance is rewritten to 12:15 in the subsequent dialogue state.}\n\\label{fig:inconsis_update}\n\\end{figure*}\n\nFigure~\\ref{fig:inconsis_update} shows another example where dialogue states are updated differently in similar scenarios. In both dialogues, the system offers an instance that fulfills the user's requirement, but the dialogue states are updated differently after user shows an intent to book the ticket. Specifically, in dialogue 1 the value for \\textit{train-arriveby} provided by the system is tracked in the dialogue state while not in dialogue 2. Dialogue 1 also showcases the implicit time processing issue discussed in Section \\ref{sec:hallucinated}, where the time ``12:08\" has been rounded to ``12:15\" in the dialogue state.\n\n\\begin{table*}[]\n\\centering\n    \\begin{tabular}[t]{|C {20mm} | L{60mm}  | L{60mm} |} \\hline\n        \\textbf{Domain} & \\textbf{Categorical slots} & \\textbf{Non-categorical slots} \\\\ \\hline\n        Restaurant & pricerange, area, bookday, bookpeople & food, name, booktime \\\\ \\hline\n        Attraction & area, type & name \\\\ \\hline\n        Hotel & pricerange, parking, internet, stars, area, type, bookpeople, bookday, bookstay & name \\\\ \\hline\n        Taxi & - & destination, departure, arriveby, leaveat \\\\ \\hline\n        Train & destination, departure, day, bookpeople & arriveby, leaveat \\\\ \\hline\n        Bus & day & departure, destination, leaveat \\\\ \\hline\n        Hospital & - & department \\\\ \\hline\n        Police & - & name \\\\ \\hline\n    \\end{tabular}\n    \\caption{Categorical and non-categorical slots defined for 8 domains in MultiWOZ 2.2.}\n    \\label{table:schema}\n\\end{table*}\n\n\\section{Ontology Issues}\n\\label{sec:ontology}\nAlthough MultiWOZ 2.0 provides a predefined ontology which is claimed to enumerate all slots and the possible values for every slot, it has been reported to be incomplete. As a result, many researchers have built their own ontology to achieve a better performance~\\cite{WuTradeDST2019, Goel_2019}. To fix the problem of incompleteness, MultiWOZ 2.1 rebuilt the ontology by listing all values present in dialogue states across the dataset, but it still has some unaddressed issues.\n\nFirst, for some slots, multiple values sharing the same semantics are listed. Some examples are ``8pm\" and ``20:00\", ``a and b guesthouse\" and ``a and b guest house\", ``cheap$|$moderate\" and ``moderate$|$cheap\" for the slots \\textit{restaurant-book-time}, \\textit{hotel-semi-name} and \\textit{hotel-semi-pricerange} respectively. We find that 51\\% of the values for the slot \\textit{hotel-name} are not semantically unique, and similar figures for the \\textit{restaurant-name} and \\textit{attraction-name} slots. Such duplicate values make evaluation hard since MultiWOZ 2.1 only assumes one correct value for each slot in the dialogue state.\n\nSecond, we observe multiple slot values in the ontology that can't be associated with any entities in the database. Values like ``free\" for slot \\textit{attraction-name}; ``cam\", ``dif\", and ``no\" for slot \\textit{restaurant-name} are some examples. Such values could be introduced in the ontology because of typographical errors in the utterances or annotation errors. Our investigation shows that 21.0\\% of the slot values in the ontology can't be directly mapped back to the values in the database through exact string matching. We also observed a few logical expressions like ``cheap$|$moderate\", ``NOT(hamilton lodge)\" etc. in the ontology. We believe that these expressions, although semantically correct, add noise during training. The ontology should either omit such expressions altogether or include all possible expressions to enable generalization to cases not observed in the training data.\n\n\\section{Correction Procedure}\n\\label{sec:correction}\n\nTo avoid the issues described above, we advocate the definition of ontology prior to data collection. This not only serves as a guideline for annotators, but also prevents annotation inconsistencies in the dataset and corruption of the ontology from typographical and annotation errors. This section describes our definition of the new ontology, which we call \\textit{schema}, followed by the corrections made to the state and action annotations. Lastly, we also show the statistics of our modifications.\n\n\\subsection{Schema Definition}\nIt is not realistic for the ontology to enumerate all the possible values for some slots like \\textit{restaurant-name} and \\textit{restaurant-booktime}, which can take a very large set of values. With addition or removal of entities in the database, the set of possible values also keeps changing continuously. \\citet{rastogi2019scalable} proposed a representation of ontology, called \\textit{schema}, to facilitate building a scalable dialogue system that is capable of handling such slots. A \\textit{schema} divides the different slots into two categories - \\textit{non-categorical} and \\textit{categorical}. Slots with a large or dynamic set of possible values are called \\textit{non-categorical}. Unlike ontology, the schema doesn't provide a pre-defined list of values for such slots. Their value is extracted from the dialogue history instead.\n\nOn the other hand, slots like \\textit{hotel-pricerange} or \\textit{hotel-type}, which naturally take a small finite set of values are called \\textit{categorical}. Similar to the ontology, the schema lists all possible values for such slots. Furthermore, during annotation, the values of these slots in the dialogue state and user or system actions must be selected from a pre-defined candidate list defined in the schema. This helps achieve sanity and consistency in annotations.\n\nWe define categorical and non-categorical slots for each domain as shown in Table~\\ref{table:schema}. The idea of splitting the slots in MultiWOZ into categorical and non-categorical is not new. Many models have used the number of possible slot values as the classification criterion~\\cite{zhang2019find}. Similarly, we classify slots with fewer than 50 different slot values in the training set as categorical, and the others as non-categorical. \n\nNote that since the Bus and Police domains have very few dialogues in the training set (5 and 145 respectively), the number of possible slot values in this domain does not reflect the true attributes of the slots. Thus, we classify them by referring to similar slots in different domains instead of following the threshold rule.\n\n\\subsection{Categorical Slots}\n\nThe list of all possible values for categorical slots is built from the corresponding database provided with MultiWOZ 2.1. In addition, we allow ``dontcare\" as a special value, which is used when user doesn't have a preference.  We also observe cases where the mentioned value is outside the scope of the database, such as the example below, where MultiWOZ 2.1 specifies ``\\$100\" as the value for \\textit{hotel-pricerange} in the dialogue state. \n\n\\begin{description}[leftmargin=*, labelsep=0.2em, itemsep=0em]\n\\item[User:] \\textit{Well,I want it cheaper than AIRBNB,so how about \\$100 a night?}\n\n\\item[System:] \\textit{Unfortunately, we do not have such specific price ranges, but our options are divided into 3 categories: cheap, moderate or expensive. Which would you prefer?}\n\\end{description}\nSince ``\\$100\" is not a part of the schema, we use ``unknown\" as the slot value in the dialogue state to express that the requirement specified by the user can not be fulfilled by the schema.\n\n\\subsection{Non-categorical Slots}\n\nValues of non-categorical slots are extracted from the dialogue history. Due to the typographical errors and slot value paraphrasing, the exact value can not be found in many cases. Some examples are ``el shaddia guest house\" being written as ```el shaddai\" or ``18:00\" being written as ``6pm\" in the dialogue utterances.\nSince in practice, typographical errors are inevitable and the same value can be mentioned in variable ways, we try to not modify the utterance to keep the dialogue natural. We also allow the presence of more than one value in the dialogue state. During evaluation, a prediction listing either of the listed values is considered correct.\n\nWe use a customized string matching method that takes into consideration the possible typos and alternative expressions to locate all values semantically similar to the annotation. If there are multiple matches, we select the most recently mentioned value and annotate its span. We also add this value to the dialogue state, while preserving the original value. Figure~\\ref{fig:noncat_slot} shows the differences between the annotations in MultiWOZ 2.1 and MultiWOZ 2.2. The former only assumes a single value for each slot, even though the slot values can be mentioned in multiple ways and predicting any one of these variants should be considered correct. Thus, in MultiWOZ 2.2, the dialogue state can contain a list of values for a slot: predicting any value in this list is considered correct.\n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=0.42\\textwidth]{figures/noncat_slots.png}\n\\setlength{\\belowcaptionskip}{-10pt}\n\\caption{Example of the difference between dialogue state annotation in MultiWOZ 2.1 and MultiWOZ 2.2 and span annotations in MultiWOZ 2.2.}\n\\label{fig:noncat_slot}\n\\end{figure}\n\nIn some cases, slot value is carried over from other slots without being explicitly mentioned in the dialogue. For instance, in the utterance ``I need to book a taxi from the museum to the restaurant\", the slot value for \\textit{taxi-destination} is copied from the value for \\textit{restaurant-name} populated earlier. Instead of annotating the span for \\textit{taxi-destination}, we note down the original slot that \\textit{taxi-destination} copies its value from. The span annotation for such slots can be obtained by tracing back the copy chain. We posit that this information can be beneficial for state tracking models utilizing a copy mechanism. \n\n\\subsection{User and System Actions}\nThe user and system action annotations provide a semantic representation of the respective utterances. These annotations were not part of the original MultiWOZ 2.0 release. They were created by \\citet{lee2019convlab} and were subsequently added to MultiWOZ 2.1. However, around 5.82\\% of turns have missing action annotations. We use crowdsourcing to obtain annotations for these 8,333 dialogue turns (7,339 user and 994 system). The slot names used in dialogue acts are slightly different from the ones used in dialogue state annotations. We rename the slots in the dialogue acts to remove this mismatch.\n\nMultiWOZ 2.1 uses domain-specific prefixes to associate actions with a certain domain. A few dialogue acts also have the ``Booking\" prefix, which is used in a few domains including Restaurant, Hotel and Train whenever a reservation is involved. In these cases, it is difficult to identify the domain corresponding to the action since the same prefix is used across many domains. We eliminate the domain and ``Booking\" prefixes from the dialogue acts, so that a uniform representation of actions can be used across all domains. To retain the association with the domain, actions for the same domain are grouped together into \\textit{frames}, following the representation used by \\citet{rastogi2019scalable}.\n\n\\subsection{Statistics}\nTable~\\ref{tab:correction_rate} contains statistics on the corrections in the training, dev, and test sets. We observe that the errors are relatively uniformly distributed across the three splits. Combining all the aforementioned procedures, we modify dialogue states in 17.3\\% of the user utterances across 28.2\\% of all dialogues. Out of the total modified 12,375 utterance annotations, a majority of the corrections fix the state update inconsistencies described in Section~\\ref{sec:inconsis_issues} by listing all the different ways in which a value has been mentioned over the dialogue context in the dialogue state. Of these state updates, 1497, or just over 12\\% involved corrections for two or more slots. Missing action annotations were added in a total of 8,333 utterances, whereas pre-existing actions in MultiWOZ 2.1 were verified and fixed for around 10\\% of the utterances.\n\n\\begin{table}[t]\n\\centering \n    \\begin{tabular}[t]{|C{14mm}|C{20mm}|C{28mm}|}\\hline\n            \\textbf{Dataset} & \\textbf{\\% of state} & \\textbf{\\% of dialogues} \\\\\\hline\n           train & 17.3 & 27.9 \\\\ \\hline\n           dev & 17.3 & 28.7  \\\\\\hline\n           test & 17.6 & 29.5 \\\\\\hline\n    \\end{tabular}\n    \\setlength{\\belowcaptionskip}{-10pt}\n    \\caption{The ratio of the modified dialogue states (same as the number of user utterances) and  modified dialogues in the training, dev, and test sets.}\n    \\label{tab:correction_rate}\n\\end{table}\n\n\\section{Additional annotations}\\label{sec:additional}\nBesides the span annotations, we also add active user intents and requested slots for every user turn. Predicting active user intents and requested slots are two new sub-tasks that can be used to evaluate model performance and facilitate dialogue state tracking. Prediction of active intents or APIs is also essential for efficiency in large-scale dialogue systems which support hundreds of APIs.\n\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{Active intents:} It specifies all the intents expressed in the user utterance. Note that utterances may have multiple active intents. For example, in ``can i get the college's phone number. i am also looking for a train to birmingham new street and should depart from cambridge looking for a train\", the user exhibits the intent both to know more about an attraction and to search for a train.\n\n    \nBased on the action and state annotations, we define a single search intent for the Attraction, Bus, Hotel, and Police domains and a single booking intent for Taxi domain, whereas for the Restaurant, Hotel, and Train domains, both search and booking intents are defined. \n\n    \\item \\textbf{Requested slots:} It specifies the slots that the user requests information about from the system. This field is generated based on the user actions in each turn. These annotations find direct applicability in developing  dialogue policy models, since requesting additional information about entities is very common in task-oriented dialogue.\n\\end{itemize}\n\n\\section{Dialogue State Tracking Benchmarks}\n\\label{sec:benchmark}\nRecent data-driven dialogue state tracking models that achieve state-of-the-art performance mainly adopt two classes of methods: span-based and candidate-based. Span-based methods extract values from dialogue history and are suitable for tracking states of non-categorical slots, while candidate-based methods that perform classification on pre-defined candidate lists to extract values are better-suited for categorical slots. \nTo test models' performance on both categorical and non-categorical slots, we selected three dialogue state tracking models that use a mixture of both methods to benchmark the performance on the updated dataset: SGD-baseline~\\cite{rastogi2019scalable}, TRADE~\\cite{WuTradeDST2019}, and DS-DST~\\cite{zhang2019find}.\n\nTRADE considers each slot as a mixture of categorical and non-categorical slot. It uses a pointer generator architecture to either generate the slot value from a pre-defined vocabulary or tokens in the dialogue history.\nOn the contrary, SGD-baseline has separate tracking strategies for categorical and non-categorical slots. It first uses a shared pretrained BERT~\\cite{devlin2018bert} to encode a context embedding for each user turn, a slot embedding for each slot, and a slot value embedding for each slot value in the candidate list of the categorical slots. Then, it utilizes linear networks to perform classification for the categorical slot and to find start and end span indices for non-categorical slots. DS-DST is a recently proposed model achieving state-of-the-art performance on MultiWOZ 2.1 using pre-trained BERT. The main difference between DS-DST and SGD-baseline is that the context embedding used in DS-DST is conditioned on the domain-slot information while it is not in SGD-baseline.\n\nWe use joint goal accuracy as our metric to evaluate the models' performance. The joint goal accuracy is defined as the average accuracy of predicting all the slot values for a turn correctly. The performance of different models is shown in Table~\\ref{tab:result}. In general, we observe similar performance on MultiWOZ 2.1 and MultiWOZ 2.2 across the three models. Table~\\ref{tab:result2} compares the joint goal accuracy over only the categorical slots (cat-joint-acc) and only the non-categorical slots (noncat-joint-acc) across all the models. It shows that TRADE and SGD-baseline demonstrate considerably higher performance on non-categorical slots than categorical slots. We infer that it may be caused by the corrections ensuring that the value in the dialogue state is also present in the dialogue history for all non-categorical slots.\n\\begin{table}[t]\n\\centering\n    \\begin{tabular}[t]{|C{15mm}|C{14mm}|C{14mm}|C{14mm}|}\\hline\n            \\textbf{Model} & \\textbf{MultiWOZ 2.0} & \\textbf{MultiWOZ 2.1} & \\textbf{MultiWOZ 2.2}\\\\\\hline\n            TRADE & 0.486 & 0.460 & 0.454\\\\\\hline\n            SGD-baseline & - & 0.434 & 0.420\\\\ \\hline\n            DS-DST & 0.522 & 0.512& 0.517\\\\ \\hline\n    \\end{tabular}\n    \\caption{Joint goal accuracy of TRADE, SGD-baseline and DS-DST models on MultiWOZ 2.0, MultiWOZ 2.1 and MultiWOZ 2.2 datasets.}\n    \\label{tab:result}\n\\end{table}\n\n\\begin{table}[t]\n\\centering\n    \\begin{tabular}[t]{|C{23mm}|C{20mm}|C{20mm}|}\\hline\n            \\textbf{Model} & \\textbf{Cat-joint-acc} & \\textbf{Noncat-joint-acc} \\\\\\hline\n            TRADE & 0.628 & 0.666 \\\\\\hline\n            SGD-baseline & 0.570 & 0.661 \\\\ \\hline\n            DS-DST & 0.706 & 0.701 \\\\ \\hline\n    \\end{tabular}\n    \\setlength{\\belowcaptionskip}{-10pt}\n    \\caption{Performance of TRADE, SGD-baseline, and DS-DST models on predicting categorical and non-categorical slots. Cat-joint-acc and noncat-joint-acc denote joint goal accuracy on categorical and non-categorical slots respectively.}\n    \\label{tab:result2}\n\\end{table}\n\n\\section{Discussion}\n\nThe Wizard-of-Oz paradigm is a very powerful technique to collect natural dialogues. However, the process of annotating these dialogues is prone to noise. In this section, we discuss some of the best practices to follow during task-oriented dialogue data collection so as to minimize annotation errors.\n\nIt is important to define an ontology or schema before data collection, listing the interface of all the domains and APIs. The schema should identify categorical slots, which have a fixed set of possible values, and the annotation interface should enforce the correctness of these slots. In particular, the interface should only allow the annotator to pick one of the values specified in the schema. For non-categorical slots, the interface should only allow values which have been mentioned in the dialogue history, and display an error otherwise. These simple checks help avoid typographical errors and value paraphrasing issues, discussed in Section~\\ref{sec:issues}. The annotation task can be followed by simple validation checks to identify erroneous annotations, which can be fixed by a follow-up crowd-sourcing task. For instance, listing the set of all possible values for every slot in the dataset helped us quickly identify instances listing ``thursday\" as the value for a time slot or ``no\" as the name of a hotel.\n\nWe also observed a few annotations utilizing logical expressions to represent the dialogue state. For instance, some dialogue state annotations utilize string ``cheap$>$moderate\" to mean that cheap is preferred over moderate, and ``cinema$|$entertainment$|$museum$|$theatre\" to mean that all values separated by ``$|$\"are acceptable. However, such values are disproportionately rare in the dataset ($<$1\\% of dialogues), thus making it difficult for models to handle such cases. It brings into question how to define a more expressive representation which can support such complex annotations and how we should design the model capable of handling such cases. We hope that as the state tracking technology advances, there will be more focus on this direction. On the other hand, it is important to ensure that such complex constraints are proportionately represented in the dataset if the system is intended to support them.\n\n\\section{Conclusion}\nMultiWOZ 2.1 \\cite{eric2019multiwoz} is an improved version of the MultiWOZ 2.0 dataset, which is extensively used as a benchmark for dialogue state tracking. We identify annotation errors, inconsistencies and ontology related issues which were left unaddressed in MultiWOZ 2.1, and publish a corrected version -- MultiWOZ 2.2. We added a new schema, standardized slot values, corrected annotation errors and standardized span annotations. Furthermore, we annotated active intents and requested slots for each user turn, and added missing user and system actions besides fixing existing ones. We benchmark a few state-of-the-art models on the new dataset: experimental results show that the models' performance is similar between MultiWOZ 2.1 and MultiWOZ 2.2. We hope the cleaned dataset helps make fairer comparisons among models and facilitate research in this field.\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2002.01359v1.tex",
        "arXiv-2002.11893v2.tex",
        "arXiv-2007.12720v1.tex"
    ],
    "group_id": "group_6",
    "response": "### Title: Advances in Task-Oriented Dialogue State Tracking: Schema-Guided, Cross-Domain, and Corrected Ontology Approaches\n\n### Introduction\n\nTask-oriented dialogue systems have gained significant traction in recent years, primarily due to their ability to assist users in accomplishing specific tasks by interacting with various APIs using natural language. These systems are composed of several modules, including natural language understanding (NLU), dialogue state tracking (DST), dialogue policy learning, and natural language generation (NLG). The DST module is particularly critical as it maintains a summary of the conversation, enabling the system to make accurate API calls and generate appropriate responses. However, developing effective DST models for large-scale virtual assistants presents unique challenges, such as managing a vast array of domains and APIs, ensuring robustness to changes in API interfaces, and facilitating zero-shot generalization to new APIs without requiring additional training data.\n\nThe field of task-oriented dialogue systems has seen substantial progress with the advent of large-scale annotated dialogue corpora and the use of deep learning techniques. Early datasets like ATIS and DSTC2 were pivotal in advancing research, but they were limited to single-domain dialogues. More recent datasets, such as MultiWOZ, Taskmaster-1, M2M, and FRAMES, have introduced multi-domain dialogues, but they still face limitations in terms of scale, annotation quality, and the ability to handle unseen APIs or domains. The Schema-Guided Dialogue (SGD) dataset, introduced in the 8th Dialogue System Technology Challenge (DSTC8), has been a significant step forward, offering a large-scale, multi-domain dataset that includes over 16,000 dialogues and supports multiple APIs with overlapping functionality within each domain. This paper reviews three key research papers that address these challenges, each contributing to the advancement of DST models in different ways.\n\nThe first paper, \"Schema-Guided Dialogue State Tracking Task at DSTC8,\" focuses on developing DST models that can handle large-scale virtual assistants efficiently. The second paper, \"CrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Dataset,\" introduces a new large-scale Chinese dataset for multi-domain dialogue systems. The third paper, \"MultiWOZ 2.2: A Dialogue Dataset with Additional Annotation Corrections and State Tracking Baselines,\" presents an updated version of the MultiWOZ dataset, correcting annotation errors and introducing new annotations to improve the quality of the dataset. These papers collectively highlight the ongoing efforts to develop more robust, efficient, and generalizable DST models for complex, multi-domain task-oriented dialogue systems.\n\n### Main Content of Each Paper\n\n#### Paper 1: Schema-Guided Dialogue State Tracking Task at DSTC8\n\nThis paper outlines the Schema-Guided Dialogue State Tracking (SGD-DST) task conducted at the 8th Dialogue System Technology Challenge (DSTC8). The primary goal of this task is to develop DST models suitable for large-scale virtual assistants, emphasizing data-efficient joint modeling across domains and zero-shot generalization to new APIs. The task introduced a new dataset, SGD, consisting of over 16,000 dialogues spanning 16 domains, and a baseline model capable of zero-shot generalization to new APIs. The SGD dataset is designed to serve as a testbed for various tasks, including intent prediction, slot filling, state tracking, and language generation. It is the first dataset to allow multiple APIs with overlapping functionality within each domain, making it a valuable resource for evaluating DST models in complex, multi-domain scenarios.\n\nThe dataset is constructed using a dialogue simulator that generates dialogue outlines and then paraphrases them to create natural utterances. This process ensures a diverse set of dialogues while mitigating the issues of annotation errors and incoherent conversations. The dataset includes annotations for user and system turns, with each turn containing a user or system utterance and corresponding dialogue state and slot spans. The evaluation metrics for the task include joint goal accuracy, active intent accuracy, requested slot F1, and average goal accuracy. The task's evaluation set contains unseen services and domains, allowing for the assessment of model performance in zero-shot settings.\n\nThe paper also discusses the submissions from 25 teams, highlighting the use of pre-trained encoders like BERT and XLNet, as well as innovative data augmentation techniques. The winning submission, from Team 9, achieved a joint goal accuracy of 86.53%, significantly outperforming other teams. The team's model utilized back translation between English and Chinese as a distinguishing factor, contributing to its high accuracy. Other teams used a variety of approaches, including separate RNNs for encoding service, slot, and intent descriptions, and a two-stage model with a candidate tracker and classifier for slot values. These models demonstrate the effectiveness of pre-trained encoders and data augmentation in achieving zero-shot generalization.\n\n#### Paper 2: CrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Dataset\n\nThe second paper introduces CrossWOZ, a new large-scale Chinese multi-domain task-oriented dialogue dataset. This dataset contains 6,012 dialogues and 102,000 utterances for 5 domains: hotel, restaurant, attraction, metro, and taxi. Unlike previous datasets, CrossWOZ provides annotations for both user and system dialogue states and dialogue acts, enabling a more comprehensive evaluation of dialogue systems. The dataset is designed to simulate scenarios where a traveler seeks tourism information and plans her or his travel in Beijing, emphasizing the natural transition between different domains.\n\nThe data collection process for CrossWOZ involves constructing a database of travel information, generating multi-domain goals, and collecting dialogues through human-to-human conversations. The goals are structured to include cross-domain dependencies, such as finding a hotel near a specific attraction. This setup ensures that the dialogues are more complex and natural, reflecting real-life conversations. The dataset is annotated using a combination of automatic rules and manual verification, ensuring high-quality annotations.\n\nThe paper also provides benchmark models for different components of a pipelined task-oriented dialogue system, including NLU, DST, dialogue policy learning, and NLG. These models are implemented using ConvLab-2, an open-source dialogue system toolkit. The results show that the cross-domain constraints are challenging for all components, particularly for the DST and policy learning modules. The authors note that the performance of these models on cross-domain dialogues is significantly lower than on single-domain dialogues, indicating the need for more powerful models capable of handling complex, multi-domain scenarios.\n\n#### Paper 3: MultiWOZ 2.2: A Dialogue Dataset with Additional Annotation Corrections and State Tracking Baselines\n\nThe third paper presents MultiWOZ 2.2, an updated version of the MultiWOZ dataset. MultiWOZ 2.2 aims to address the issues of annotation errors, inconsistencies, and ontology incompleteness present in previous versions. The authors identify and correct hallucinated values, inconsistent state updates, and typographical errors in the dataset. They redefine the ontology by categorizing slots into categorical and non-categorical, facilitating the development of more robust DST models. The dataset also includes span annotations for non-categorical slots and active user intents and requested slots for each user turn, providing a more comprehensive and consistent representation of dialogue states.\n\nThe corrections in MultiWOZ 2.2 involve modifying dialogue states to ensure that all values mentioned in the dialogue history are included in the state annotations. This process helps in standardizing the evaluation of DST models, as it ensures that the values predicted by the model are present in the dialogue history. The authors benchmark three recent DST models—TRADE, SGD-baseline, and DS-DST—on the updated dataset, observing similar performance across MultiWOZ 2.1 and MultiWOZ 2.2. However, the models show higher performance on non-categorical slots, suggesting that the corrections in MultiWOZ 2.2 may have improved the consistency and quality of non-categorical slot annotations.\n\n### Commonalities and Innovations\n\nAll three papers focus on advancing the field of task-oriented dialogue state tracking by addressing common challenges such as annotation errors, inconsistencies, and the need for robust, efficient, and generalizable models. The SGD-DST task at DSTC8 introduced a new dataset, SGD, which is the largest publicly available corpus of annotated task-oriented dialogues, spanning 16 domains and 26 APIs. CrossWOZ, on the other hand, is the first large-scale Chinese dataset for multi-domain dialogue systems, emphasizing natural transitions between domains. MultiWOZ 2.2 builds upon the existing MultiWOZ dataset by correcting annotation errors and introducing new annotations, such as span annotations and active intents, to improve the dataset's quality and consistency.\n\nThe innovations in these papers include:\n- **Schema-Guided Approach**: The SGD-DST task advocates the use of a schema-guided approach for dialogue state tracking, which eliminates the need for a master schema and facilitates zero-shot generalization to new APIs.\n- **Chinese Dataset**: CrossWOZ introduces a large-scale Chinese dataset for task-oriented dialogue systems, providing a valuable resource for researchers working on multi-domain dialogue modeling.\n- **Ontology Redefinition**: MultiWOZ 2.2 redefines the ontology by categorizing slots into categorical and non-categorical, and introduces span annotations for non-categorical slots to improve model performance.\n\n### Comparison of Results and Discussion\n\nThe results from the three papers reveal several trends and insights into the performance of DST models in different settings. The SGD-DST task at DSTC8 showed that the winning model, from Team 9, achieved a joint goal accuracy of 86.53% on unseen services, significantly outperforming other teams. This model utilized back translation between English and Chinese as a key innovation, demonstrating the effectiveness of data augmentation techniques in achieving zero-shot generalization.\n\nCrossWOZ's benchmark models showed that the cross-domain constraints are challenging for all components of a pipelined dialogue system, particularly for the DST and policy learning modules. The performance on cross multi-domain dialogues (CM and CM+T) was notably lower than on simpler dialogues, indicating the need for more powerful models capable of handling complex, multi-domain scenarios.\n\nMultiWOZ 2.2's benchmarking results revealed that the models' performance on categorical slots was lower than on non-categorical slots. This suggests that the corrections in MultiWOZ 2.2 may have improved the consistency and quality of non-categorical slot annotations, leading to better performance on these slots. The joint goal accuracy of the models was similar across MultiWOZ 2.1 and MultiWOZ 2.2, indicating that the corrections did not significantly alter the overall performance but improved the consistency of slot annotations.\n\n### Conclusion\n\nIn summary, these three papers contribute significantly to the advancement of task-oriented dialogue state tracking models. The SGD-DST task at DSTC8 introduced a large-scale, multi-domain dataset that supports multiple APIs with overlapping functionality, facilitating the development of more robust and generalizable models. CrossWOZ provided a valuable resource for researchers working on Chinese task-oriented dialogue systems, emphasizing the importance of natural transitions between domains. MultiWOZ 2.2 addressed the issues of annotation errors and inconsistencies in the MultiWOZ dataset, introducing a new schema and standardized slot annotations to improve model performance.\n\nThe main findings from these papers include the effectiveness of pre-trained encoders and data augmentation techniques in achieving zero-shot generalization, the challenges posed by cross-domain dependencies in dialogue systems, and the importance of consistent and high-quality annotations for fair model evaluation. Future research directions could include exploring more sophisticated user simulators, developing models that can handle complex logical expressions in dialogue states, and investigating the impact of different data collection methods on annotation quality and model performance."
}