{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{Hungry Hungry Hippos: Towards Language Modeling with State Space Models}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\n\nState space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling.\nMoreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization.\nIn this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention.\nFirst, we use synthetic language modeling tasks to understand the gap between SSMs and attention.\nWe find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence.\nTo understand the impact on language modeling, we propose a new SSM layer, \\hthree, that is explicitly designed for these abilities.\n\\hthree matches attention on the synthetic languages and comes within \\num{0.4} PPL of Transformers on OpenWebText.\nFurthermore, a hybrid 125M-parameter \\hthree-attention model that retains two attention layers\nsurprisingly outperforms Transformers on OpenWebText by \\num{1.0} PPL.\nNext, to improve the efficiency of training SSMs on modern hardware,\nwe propose \\fastfft.\n\\fastfft uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences.\n\\fastfft yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text \\num{2.4$\\times$} faster than Transformers.\nUsing \\fastfft, we scale hybrid \\hthree-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n\\section{Introduction}\n\\label{sec:intro}\n\nState space models (SSMs) have achieved state-of-the-art sequence modeling performance in domains ranging from time series analysis~\\citep{gu2022efficiently} to audio generation~\\citep{goel2022s}.\nHowever, they have yet to match the performance of Transformers on language modeling, often underperforming Transformers by multiple points in perplexity~\\citep{gu2022efficiently}.\nAn natural question is whether this gap in performance is due to inherent inductive biases and capabilities in attention~\\citep{edelman2022inductive,olsson2022context}, or whether it is a function of the significant organizational resources that have been spent training and tuning large attention-based language models~\\citep{chowdhery2022palm,hoffmann2022training,zhang2022opt}, as well as specialized hardware support for attention, ranging from tensor cores~\\citep{nvidia2017nvidia} to transformer chips~\\citep{nvidia2022nvidia,kao2021optimized}.\n\nWe take first steps towards answering these questions in this paper.\nFirst, we use synthetic language modeling tasks to show that there is an expressivity gap between SSMs and attention.\nUsing our insights, we design a new SSM layer that nearly matches attention in language modeling.\nSecond, we propose better hardware-aware algorithms for SSMs that allow them to take advantage of modern accelerators---and run faster than attention.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figs/banner_pdf.pdf}\n    \\caption{\\label{fig:banner}\n    Left: \\hthree stacks two discrete SSMs with shift and diagonal matrices and uses multiplicative interactions between input projections and their outputs to model comparisons between points in a sequence.\n    Middle: \\hthree can perform associative recall---which is easy for attention, but not existing SSMs.\n    Right: \\fastfft uses a new state-passing algorithm over fused block FFTConv to increase hardware efficiency of SSMs, allowing \\hthree to scale to billion-parameter models.\n    }\n    \\vspace{-1.25em}\n\\end{figure}\n\\textbf{Understanding the Expressivity Gap.}\nTo understand the gap between SSMs and attention, we draw on synthetic language modeling tasks that have been proposed as a mechanistic basis for in-context learning in Transformers~\\citep{olsson2022context}\nThese synthetic languages focus on the ability to manipulate text---recalling tokens from earlier time steps, or comparing tokens from different points in a sequence.\nWe find that existing SSMs struggle to model these synthetic languages.\nTo probe how important these skills are for language modeling, we propose \\hthree (Hungry Hungry Hippo), a new SSM-based layer designed to solve these language modeling tasks.\n\\hthree stacks two SSMs, with multiplicative interactions between their outputs and input projections.\nThe SSMs allow \\hthree to keep a log of tokens (to recall them later), while the multiplicative interactions allow for comparisons across the sequence.\n\n\\hthree matches attention on the synthetic languages and almost closes the gap with Transformers on language modeling---coming within \\num{0.4} perplexity of Transformers on OpenWebText (compared to \\num{3.4} ppl for existing SSMs---even those explicitly designed for language modeling~\\citep{mehta2022long}).\nFurthermore, a simple hybrid \\hthree-attention model that retains two attention layers surprisingly \\textit{outperforms} Transformers on OpenWebText by \\num{1.0} perplexity.\nTo further evaluate \\hthree on language modeling, we train 125M-, 355M-, 1.3B-, and 2.7B-parameter hybrid \\hthree-attention language models on the Pile~\\citep{gao2020pile}, using hyperparameters from GPT-3~\\citep{brown2020language}.\nThese hybrid models outperform Transformer-based language models of the same size in perplexity, and match or outperform them on a majority of tasks in the SuperGLUE benchmark in zero- and few-shot learning.\nSince the SSM layers in these hybrid models admit a recurrent view, they can also perform \\num{2.4$\\times$} faster inference than Transformers.\n\n\\textbf{Scaling SSMs.}\nNext, we improve the efficiency of SSMs on modern hardware, to reduce the hardware barrier between attention and SSMs.\nSSMs scale nearly linearly in sequence length instead of quadratically like attention, but still run slower on modern hardware due to poor hardware utilization.\nTo close this gap, we propose \\fastfft, a hierarchical algorithm for computing SSMs, inspired by IO-Aware attention~\\citep{dao2022flashattention}.\nThe technical challenge is that SSMs require a FFT-based convolution over the input sequence, which requires an FFT, pointwise multiply, and inverse FFT.\nWhen implemented in cuFFT~\\citep{cufft}, this operation incurs expensive GPU memory reads/writes, and cannot utilize the specialized matrix multiply units available on modern hardware\\footnote{An A100 GPU has a maximum of 312 TFLOPs/s of FP16 with\ntensor cores, but only 20 TFLOPs/s of FP32 (and 40 TFLOPs/s of FP16) without\ntensor cores~\\citep{nvidia2020nvidia}. This trend started with the V100 GPUs~\\citep{nvidia2017nvidia} and has continued with the\nH100 GPUs~\\citep{nvidia2022nvidia}.}.\nTo use specialized matrix multiply units, we appeal to classical techniques that split the FFT into blocks and compute it using a series of matrix multiplications.\nCombined with kernel fusion, this ``block'' FFT solution increases hardware efficiency, but only as long as the sequence length can fit into GPU SRAM (on-chip memory, analogous to L1 cache on the CPU)---up to sequence length 8K on modern A100.\n\nTo scale to sequences longer than 8K, we propose a \\textit{state passing} algorithm (Figure~\\ref{fig:banner} right), specialized to SSMs.\nThe key insight is that we can use the recurrent properties of SSMs to process the input in chunks---as long as we keep track of an additional state vector.\nThe state passing algorithm splits the input into the largest chunks that can fit into GPU SRAM, efficiently computes the FFT-based convolution using block FFT, and updates an intermediate state to start the next chunk.\nUsing this state-passing algorithm, \\fastfft can scale SSMs to \\textit{any} sequence length---even longer than can fit on GPU SRAM at once---while maintaining a \\textit{near linear} compute complexity.\n\\fastfft sets state-of-the-art speed on long range arena using S4~\\citep{gu2022efficiently}, outperforming Transformers by \\num{5.8$\\times$} and previous S4 models by \\num{2$\\times$}.\n\\fastfft trains \\hthree\\ \\num{4-8$\\times$} times faster than attention for long sequences, and is a critical component for scaling to billion-parameter models\\footnote{Code for H3 is available at \\url{https://github.com/HazyResearch/H3} }.\n\n\\section{Background}\n\\label{sec:background}\n\nWe present some background on state space models and linear attention, which inspired our H3 layer.\n\n\\subsection{State Space Models}\n\nA continuous-time state-space representation~\\citep{brogan1974modern} defines a linear mapping from an\ninput signal $u(t) \\in \\mathbb{R}$ (as a function of time $t$) to an output signal $y(t) \\in \\mathbb{R}$ through a state-variable\n$x(t) \\in \\mathbb{R}^m$, with the following differential equation, for some matrices $\\vA \\in \\mathbb{R}^{m \\times m}$, $\\vB \\in \\mathbb{R}^{m \\times 1}$, $\\vC \\in \\mathbb{R}^{1 \\times m}$,\n$\\vD \\in \\mathbb{R}^{1 \\times 1}$: $\\dot{x}(t) = \\vA x(t) + \\vB u(t)$, $y(t) = \\vC x(t) + \\vD u(t)$.\n\nSimilarly, a discrete-time state-space representation defines a linear mapping\nfrom a discrete input signal $u_i$ (for $i = 1, 2, \\dots$) to a discrete output signal\n$y_i$ though a state-variable $x_i \\in \\mathbb{R}^m$:\n\\begin{align*}\n  x_i &= \\vA x_{i-1} + \\vB u_i \\\\\n  y_i &= \\vC x_i + \\vD u_i.\n\\end{align*}\n\nA state-space model (SSM) uses these representations as a layer in a deep learning\npipeline, where the matrices $\\vA, \\vB, \\vC, \\vD$ are learned from data (e.g.,\nwith gradient-based optimization).\nOne often has $d$ of these SSMs in parallel, each corresponding to one hidden\ndimension.\nTo preserve the sequence history, HiPPO~\\citep{gu2020hippo} projects the history\non a basis of orthogonal polynomials, which translates to having SSMs whose\n$\\vA, \\vB$ matrices are initialized to some special matrices.\n\nThis recurrent form of SSMs allows efficient inference (i.e., generation): to\ngenerate the output of the next time-step, one only needs the state of the\ncurrent time-step, not the entire input history.\nFurthermore, SSMs can freely extrapolate to sequences longer than seen during training.\n\n\\textbf{SSMs as Convolution.}\nFor efficient training, given the\nentire sequence of the input $u_1, \\dots, u_N$, the output sequence\n$y_1, \\dots, y_N$ can also be written as the convolution of the input with the\nfilter~\\citep{gu2021combining}:\n\\begin{equation*}\n  f = [\\vC \\vB, \\vC\\vA\\vB, \\vC\\vA^2\\vB, \\dots, \\vC\\vA^{N-1}\\vB].\n\\end{equation*}\nThat is, from an initial condition $x_0$, we have\n$y_i = \\vC\\vA^i\\vB x_0 + (f \\ast u)_i + \\vD u_i$, where $(f \\ast u)$ denotes a\nlinear convolution between $f$ and $u$.\nIf we set the initial condition $x_0$ to be zero, then $y$ is exactly a\nlinear convolution of $u$, with a residual connection $\\vD u$.\nMore generally, any linear time-invariant system (of which SSMs are a special case) can\nbe written as a convolution.\n\nGiven a 1D input sequence $u \\in \\mathbb{R}^{N}$ of length $N$, we denote the 1D output\nsequence $y \\in \\mathbb{R}^N$ of an SSM parameterized by matrices $\\vA, \\vB, \\vC, \\vD$ as\n\\begin{equation*}\n  y = \\mathrm{SSM}_{\\vA, \\vB, \\vC, \\vD}(u).\n\\end{equation*}\nTo simplify notation, we omit the reference to $\\vA, \\vB, \\vC, \\vD$ and write\n$y = \\mathrm{SSM}(u)$ if they are clear from context.\nWhen $u$ is multidimensional of dimension $d$, we stack $d$ of these SSMs\ntogether that defines a mapping from $u \\in \\mathbb{R}^{N \\times d}$ to $y \\in \\mathbb{R}^{N \\times d}$, using\nthe same notation $y = \\mathrm{SSM}(u)$.\n\nTo construct the filter $f$ from $\\vA, \\vB, \\vC$ efficiently, $\\vA$ is often constrained to\nbe diagonal~\\citep{gupta2022diagonal,gu2022parameterization}, or diagonal plus\nlow-rank~\\citep{gu2022efficiently}.\n\n\\textbf{SSM through FFTs.}\nComputing the convolution naively through conventional matrix operations is expensive\nfor long kernels, scaling as $O(N^2)$.\nInstead, we can use FFTs: take the FFT of $f$ and $u$, multiply them together pointwise, and then take the inverse FFT.\nThis yields an $O(N \\log N)$ algorithm.\n\n\\subsection{Linear attention}\n\nWe describe linear attention~\\citep{katharopoulos2020transformers} and its connection to RNNs, which inspired our model design~(\\cref{sec:method}).\n\nIn standard attention~\\citep{vaswani2017attention}, we have $N$ query/key/value tokens $Q_i, K_i, V_i \\in \\mathbb{R}^d$ for\n$i = 1, \\dots, N$, where $N$ is the sequence length and $d$ is the head dimension.\nFor some similarity metric $\\Sim \\colon \\mathbb{R}^d \\times \\mathbb{R}^d \\to \\mathbb{R}$, we want to compute the output:\n\\begin{equation*}\n  O_i = \\frac{\\sum_{j=1}^i\\Sim(Q_i, K_j) V_j}{\\sum_{j=1}^i \\Sim(Q_i, K_j)} \\in \\mathbb{R}^d.\n\\end{equation*}\nFor standard softmax attention, $\\Sim(q, k) = e^{q^\\top k}$ (often the dot\nproduct is scaled by $1/\\sqrt{d}$).\nLinear attention makes the assumption that $\\Sim$ has the form\n$\\Sim(q, k) = \\phi(q)^\\top \\phi(k),$\nfor some (nonlinear) function $\\phi$.\nThe output is then $O_i = \\frac{\\phi(Q_i)^\\top \\sum_{j=1}^{i} \\phi(K_j) V_j^\\top }{\\phi(Q_i)^\\top \\sum_{j=1}^{i} \\phi(K_j)}$.\nLet\n$S_i = \\sum_{j=1}^{i} \\phi(K_j) V_j^\\top \\in \\mathbb{R}^{d \\times d}$, $z_i = \\sum_{j=1}^{i} \\phi(K_j) \\in \\mathbb{R}^d$, $d_i = \\phi(Q_{i})^\\top z_i \\in \\mathbb{R}$.\nThen $O_i = \\frac{\\phi(Q_i)^\\top S_i}{d_i}$.\nThis connects linear attention to RNNs: the output $O_i$ is a function of $S_i$\nand $z_i$, both of which are incrementally updated (as cumulative sums).\n\n\\section{Hungry Hungry Hippos Layer to Model Discrete Sequences}\n\\label{sec:method}\n\nTo understand the gap between SSMs and attention on language modeling, we\nexamine two synthetic language modeling tasks.\nThese tasks motivate our \\hthree layer to add a discrete SSM (based on shift matrix) and multiplicative interaction to effectively model discrete sequences.\nWe then show that the \\hthree layer is expressive enough to solve these synthetic tasks, and that this understanding leads to better performance on a real language modeling benchmark.\n\n\\subsection{Motivation: Synthetic Language Modeling Tasks\\label{sec:synthetics}}\n\nWe describe two closely-related synthetic tasks, summarized in Table~\\ref{table:synthetic_tasks}. \nOlsson et al.~\\citep{olsson2022context} argue that the ability to solve (variants of) these tasks accounts for the majority of the in-context learning capability of Transformers, and more intuition is given in Appendix~\\ref{sec:app_exp_details}. \n\\begin{table}[h]\n    \\small\n    \\centering\n    \\caption{\\label{table:synthetic_tasks} Synthetic language modeling tasks.}\n    {\n        \\begin{tabular}{@{}|l|l|l|c|c|@{}}\n        \\hline\n        Task & Input & Output & Sequence Length & Vocab Size  \\\\ % & Training time \\\\\n        \\hline\n        Induction Head & \\textit{a b c d e $\\vdash$ f g h i $\\dots$ x y z $\\vdash$} & \\textit{f} & 30 & 20 \\\\\n        Associative Recall & \\textit{a 2 c 4 b 3 d 1 a} & \\textit{2} & 20 & 10 \\\\ \\hline\n        \\end{tabular}\n    }\n\\end{table}\nThe \\textbf{Induction Head} task tests how well a model can recall content after a special token (e.g., $\\vdash$ in~\\cref{table:synthetic_tasks}).\nAt the end of the sequence, the model must recall the token that appeared immediately after the special token earlier in the sequence. \\textbf{Associative Recall}~\\citep{ba2016using} is similar to the induction head task, but requires the model to remember multiple key-value pairs.\nAt the end of the sequence, the model must recall a specific value belonging to a specific key.\n\\begin{table}[h]\n    \\small\n    \\centering\n    \\caption{\\label{table:synthetics} Evaluation of 2-layer models on synthetic language tasks.}\n    {\n        \\begin{tabular}{@{}|c|c|ccc|c|@{}}\n        \\hline\n        Task & Random & S4D & Gated State Spaces & H3 & Attention  \\\\ % & Training time \\\\\n        \\hline\n        Induction Head & 5.0 & 35.6 & 6.8 & \\textbf{100.0} & \\textbf{100.0} \\\\\n        Associative Recall & 25.0 & 86.0 & 78.0 & 99.8 & \\textbf{100.0}  \\\\ \\hline\n        \\end{tabular}\n    }\n\\end{table}\n\nWe argue that these failures suggest two missing capabilities: (i) to remember tokens that appear after a particular event (e.g., the special token in the induction head task), and (ii) to compare tokens across the sequence (e.g., comparing keys to decide which value to recall).\nAttention has both these capabilities: it can compare tokens by constructing the \\textit{quadratic} attention matrix $\\vQ \\vK^\\top$, and it can recall tokens by direct copying (multiplying $\\softmax(\\vQ \\vK^\\top)$ with $\\vV$).\nIn Section~\\ref{sec:method_h3}, we design our new layer \\hthree to enable these capabilities in SSMs, narrowing the expressivity gap between SSMs and attention.\n\n\\subsection{\\hthree Layer\\label{sec:method_h3}}\n\\hthree uses SSMs with shift and diagonal matrices, along with multiplicative\noperations against projections of the input to capture the missing capabilities identified by the synthetics.\n\n\\textbf{High-level Intuition.}\n(i) To remember tokens from the past, we want the state $x_i$ to copy from the input $u_i$, and then pass that information to the next state $x_{i+1}$. As $x_{i+1}$ relates to $x_i$by $\\vA x_i$, we use a discrete SSM with a shift matrix $\\vA$ (described formally below) that shifts the elements of a state vector (e.g., mapping $[a, b, c] \\to [0, a, b]$).\n(ii) To compare tokens across the sequence, we use multiplicative interaction: the output of an SSM, containing information from previous time steps, is multiplied with the input at the current time steps, thus measuring similarity between tokens.\n\n\\hthree is loosely inspired by linear attention (\\cref{sec:background}): we project the input $u$ to get three signals $\\vQ, \\vK, \\vV$.\nThen we replace the non-linearity $\\phi(\\vK)$ with an SSM where $\\vA$ is a shift matrix ($\\mathrm{SSM}_\\mathrm{shift}$), and we replace the summation $S_i$ with a SSM with diagonal $\\vA$ ($\\mathrm{SSM}_\\mathrm{diag}$).\nThe output, for the case of head dimension $d_h = 1$, is:\n\\begin{equation*}\n  \\vQ \\odot \\mathrm{SSM}_\\mathrm{diag}(\\mathrm{SSM}_\\mathrm{shift}(\\vK) \\odot \\vV),\n\\end{equation*}\nwhere $\\odot$ denotes pointwise multiplication.\nWe can view this form as stacking two SSMs with multiplicative\ninteraction (each is a ``hungry hippo'', hence the name of our layer).\nA more formal connection between linear attention, time-varying systems, and \\hthree can be found in Appendix~\\ref{app:linear_attention}.\n\n\\textbf{Remembering Key Tokens: Shift and Diagonal SSMs.}\nThe shift and diagonal SSMs are designed to address the capability to log tokens after particular events.\nIn the shift SSM, we constrain $\\vA \\in \\mathbb{R}^{m \\times m}$ to be a shift matrix\n$\n\\vA_{i, j} = \n    \\begin{cases}\n    1 & \\text{for } i - 1 = j\\\\\n    0 & \\text{otherwise}\n    \\end{cases}\n$.\nThe action of this matrix on the hidden state $x_i$ is to shift each coordinate\ndown by one---thereby creating a ``memory'' of the previous states.\nFor example, if $\\vB = e_1$, the first basis vector, then\n$x_i = [u_i, u_{i-1}, \\dots, u_{i-m+1}]$ contains the inputs from the previous $m$\ntime steps.\nWe learn $\\vB$ and $\\vC$ ($\\vB$ can also be fixed to $e_1$ for\nsimplicity, in which case the output is a 1D conv.\\ with kernel size $m$).\n\nThe diagonal SSM constrains $\\vA$ to be diagonal and initializes it from the diagonal version of HiPPO (S4D~\\citep{gu2022parameterization}).\nThis parameterization allows the model to remember state over the entire sequence.\nThe shift SSM can detect when a particular event occurs, and the diagonal SSM can remember a token afterwards for the rest of the sequence.\n\n\\textbf{Multiplicative Interaction for Comparison.}\nWe take the multiplicative interactions from linear attention, but they provide another missing capability when combined with a shift matrix: comparing tokens across the sequence.\nThe multiplicative interactions between the output of the shift SSM and the $\\vV$ projection mimics local multiplicative interactions in linear attention (depending on the size of the hidden state).\nSimilarly, multiplicative interactions with the $\\vQ$ projection and the output of the diagonal SSM allows comparisons between tokens over the entire sequence.\n\n\\textbf{\\hthree Layer.} The overall layer is given in Algorithm~\\ref{alg:h3} and shown schematically in Figure~\\ref{fig:banner} (left).\nWe use the \\hthree layer to construct a model in the same style as Transformers by interleaving it with MLPs,\nconnected by residual connection and layer norm (i.e., pre-norm architecture~\\citep{baevski2018adaptive}).\nWe will also consider a hybrid \\hthree-attention model (two attention layers while the rest are \\hthree, \\cref{sec:expressivity,sec:evaluation}).\n\\begin{algorithm}[H]\n  \\algsetup{linenosize=\\tiny}\n  \\caption{\\label{alg:h3} H3 Layer}\n  \\small\n  \\begin{algorithmic}[1]\n    \\REQUIRE Input sequence $u \\in \\mathbb{R}^{N \\times d}$ from the previous layer, weight\n    matrices $\\vW_Q, \\vW_K, \\vW_V, \\vW_O \\in \\mathbb{R}^{d \\times d}$, a shift SSM $\\mathrm{SSM}_\\mathrm{shift}$, a diagonal SSM $\\mathrm{SSM}_{\\mathrm{diag}}$, head dimension $d_h$.\n    \\STATE Compute $\\vQ = u \\vW_Q, \\vK = u \\vW_K, \\vV = u \\vW_V \\in \\mathbb{R}^{N \\times d}$.\n    \\STATE Pass $\\vK$ through the shift SSM: $\\overline{\\vK} = \\mathrm{SSM}_\\mathrm{shift}(\\vK) \\in \\mathbb{R}^{N \\times d}$.\n    \\STATE Split $\\vQ, \\overline{\\vK}, \\vV$ into $H$ ``heads'' ($\\vQ^{(h)}, {\\overline{\\vK}}^{(h)}, \\vV^{(h)}$\n    for $h = 1, \\dots, H$), each a sequence of $N$ vectors of size $d_{h} = d / H$.\n    \\FOR{$1 \\leq h \\leq H$}\n    \\STATE Take the batched outer product ${\\overline{\\vK}}^{(h)} (\\vV^{(h)})^\\top \\in \\mathbb{R}^{N \\times d_h \\times d_h}$ (batched in the $N$-dimension) and pass it through a diagonal SSM: $\\vK\\vV^{(h)} = \\mathrm{SSM}_{\\mathrm{diag}}({\\overline{\\vK}}^{(h)} (\\vV^{(h)})^\\top) \\in \\mathbb{R}^{N \\times d_{h} \\times {d_{h}}}$.\n    \\STATE Batch-multiply by $\\vQ$:  $\\vO^{(h)} = [\\vQ^{(h)}_1 \\vK\\vV^{(h)}_1, \\dots, \\vQ^{(h)}_N \\vK\\vV^{(h)}_N]\\in  \\mathbb{R}^{N \\times d_h}$ (batched in the $N$-dimension).\n    \\ENDFOR\n    \\STATE Concatenate the output $\\vO^{(h)}$ of each head, and multiply by the output\n    projection matrix $\\vW_O \\in \\mathbb{R}^{d \\times d}$.\n  \\end{algorithmic}\n\\end{algorithm}\n\n\\paragraph{Efficiency}\nWe show that \\hthree scales as $O(N \\log N)$ with\nsequence length $N$---asymptotically more efficient than attention, which typically requires $O(N^2d)$ time and $O(N^2)$ space\\footnote{There are several memory-efficient\nalgorithms for attention~\\citep{rabe2021self,dao2022flashattention}, though\ntheir time complexity is still quadratic in $N$, which is a lower-bound for attention~\\citep{keles2022computational}.} (proof in~\\cref{sec:app_h3_complexity}).\n\\begin{proposition}\\label{thm:h3_complexity}\n  Let $N$ be the sequence length, $d$ be the hidden dimension, and assume that\n  the head dimension $d_h$ is of order $O(1)$.\n  Then the H3 layer takes $O(d^2N + d N \\log N)$ time and $O(dN)$ space to compute.\n\\end{proposition}\n\n\\subsection{Expressivity}\n\\label{sec:expressivity}\n\nWe show that \\hthree can model our synthetic languages, as well as natural language on OpenWebText~\\citep{Gokaslan2019OpenWeb}.\nWe also present a hybrid \\hthree-attention extension that outperforms Transformers on OpenWebText.\n\n\\textbf{Mechanism for Solving Associative Recall with H3.}\n\\hthree is expressive enough to solve our synthetic language modeling tasks, as shown in Table~\\ref{table:synthetics}.\nFigure~\\ref{fig:banner} (middle) shows a mechanism for a single \\hthree layer to solve the associative recall task for a particular key-value pair $(a, 3)$.\nThe shift SSM and following multiplicative interaction act as a gate on whether to let a value through to the diagonal SSM, based on whether the previous token was key $a$.\nThe diagonal SSM stores the value $3$ in memory, and continually outputs it.\nThe final multiplicative interaction gates whether to let the diagonal SSM's output through---based on whether the current input token is the key $a$.\nWe formally construct the weights of an \\hthree layer to solve this task in Appendix~\\ref{sec:app_expressivity}.\n\n\\begin{table}[h]\n    \\small\n    \\centering\n    \\caption{\\label{table:ablations} Perplexity of SSM variants compared to\n      Transformers on OpenWebText. All models have 12 layers, with size around 125M, and are trained\n      with the same hyperpameters, for 50B tokens.}\n    {\n        \\begin{tabular}{@{}|ccccc|c|@{}}\n            \\hline\n        \\hthree & \\hthree Hybrid (2 Attn) & S4D & GSS & GSS Hybrid (2 Attn) & Transformer  \\\\ % & Training time \\\\\n        \\hline\n        21.0 & \\textbf{19.6} & 24.9 & 24.0 & 19.8 & 20.6 \\\\ \\hline\n        \\end{tabular}\n    }\n\\end{table}\n\\textbf{Better Synthetic Language Modeling Translates to Better Natural Language Modeling.}\nWe validate that when H3 can solve these synthetic tasks, it also improves the modeling capability on natural language (e.g., on the OpenWebText dataset).\nAs shown in Table~\\ref{table:ablations}, \\hthree comes within 0.4 perplexity points of Transformers when trained for 50B tokens on OpenWebText, and performs much better than existing SSM variants (S4D, GSS), by $3-3.9$ points.\n\n\\textbf{Extension: H3-attention Hybrid Model.}\nA simple hybrid \\hthree-attention language model surprisingly outperforms Transformers on OpenWebText by 1.0 point.\nOur hybrid model simply retains two self-attention layers: one in the second layer, and one in the middle (layer $2 + N / 2$ for an $N$-layer model, $N$ even).\nThe \\hthree-attention hybrid also outperforms the GSS-attention hybrid~\\citep{mehta2022long}.\n\n\\section{\\fastfft: Efficiently Training SSMs\\label{sec:efficiency}}\n\nTo improve the efficiency of SSMs on modern hardware, we propose \\fastfft.\n\\fastfft fuses the FFT, pointwise multiply, and inverse FFT to reduce memory\nreads/writes.\nIt also uses a block FFT algorithm to make use of specialized matrix multiply units (e.g., tensor cores on A100) for sequence lengths up to 8K.\nFor sequences longer than 8K, the computation no longer fits in GPU SRAM\\footnote{SRAM, or on-chip memory, is much faster than off-chip GPU memory, but usually much smaller, on the order of around 100KB for each streaming processor.}, so we\npropose a novel state-passing algorithm that splits the sequence into chunks to\ncompute the FFT convolution one chunk at a time.\n\\fastfft can speed up any SSMs (not just \\hthree).\n\n\\subsection{Fused Block FFTConv}\n\\label{sec:systolic_fft}\n\nWe deploy two techniques to speed up the FFT-based convolution for sequences shorter than 8K: kernel fusion and block FFT.\nKernel fusion addresses IO bottlenecks due to reading and writing of intermediate results, while block FFT allows the FFT-based convolution to utilize specialized matrix multiplication units.\nThese techniques allow us to speed up FFTConv\nby 2$\\times$ (\\cref{sec:eval_efficiency}) for sequences shorter than 8k.\n\n\\textbf{Kernel Fusion.}\nNaive implementations of FFTConv using standard libraries such as cuFFT are IO-bound due to repeated reading and writing of intermediate results.\nThe FFT convolution in an SSM with input $u$ and filter $f$ has the form $iFFT(FFT(u) \\odot FFT(f))$ (where $\\odot$ denotes pointwise multiplication).\nIt requires reading and writing intermediate results to GPU memory---which can dominate the runtime.\nFollowing \\textsc{FlashAttention}~\\citep{dao2022flashattention}, we first fuse the entire FFTConv into a single kernel and compute it in SRAM to avoid this overhead.\n\n\\textbf{Block FFT.}\nTo further speed up the computation of FFT-based convolution, we exploit specialized matrix multiplication hardware on modern GPUs\n(e.g., Tensor Cores on Nvidia GPUs perform fast $16 \\times 16$ matrix multiplication).\nWe appeal to classical results that show that the FFT can be written as a series of block-diagonal matrix\nmultiplications interleaved with permutation.\nWe note that such algorithms are not new, but our setting (fused FFTConv on GPU) introduces new bottlenecks---by removing the IO bottlenecks, compute becomes the bottleneck (note that a single FFT on GPU is usually IO bound).\n\nSuppose that we want to perform an $N$-point FFT, which is equivalent to\nmultiply by the DFT matrix $\\vF_N$.\nSuppose that $N = N_1 N_2$ for some integers $N_1, N_2$.\nBy the Cooley-Tukey decomposition of the DFT~\\citep{cooley1965an,\n  bailey1990ffts} (also known as the four-step FFT algorithm),\nwe can write $\\vF_N = \\vP (\\vI_{N_2} \\otimes \\vF_{N_1}) \\vP^\\top \\vD (\\vI_{N_1} \\otimes \\vF_{N_2}) \\vP$,\nwhere $\\vP$ denotes a fixed permutation that reshapes the input as a $N_1 \\times N_2$\narray and then transpose it, $\\otimes$ denotes Kroneker product, $\\vD$ is a $N \\times N$\ndiagonal matrix (called the twiddle factors)~\\citep{dao2022monarch}, and $\\vI_{N_i}$ and $\\vF_{N_i}$ are the identity and DFT matrix of size $N_i \\times N_i$.\nAs $\\vI_{N_2} \\otimes \\vF_{N_1}$ and $\\vI_{N_1} \\otimes \\vF_{N_2}$ are just block-diagonal matrices,\nwe can make use of specialized matmul units to perform these multiplications.\nSimilarly, if $N = N_1 N_2 N_3$ then we can decompose the $N$-point FFT into a\nseries of (block) FFT of size $N_1$, $N_2$, and $N_3$, interleaved by\npermutation.\n\nThe block FFT algorithm incurs $O(N r \\log N / \\log r)$ FLOPs for a sequence length $N$, if $N$ can be written as $r^p$ for two integers $r, p$.\nThis incurs more FLOPs than standard FFT $(O(N\\log N))$, but can run faster when\nwe using specialized matrix multiplication hardware.\n\n\\subsection{State-Passing}\nHowever, the fused kernel cannot run if the sequence is too long to fit into GPU SRAM (longer than 8K on A100).\nWe show how to exploit the particular form of the FFT in SSM to speed it up for long\nsequences.\n\nThe recurrent nature of SSMs allows us to split the FFTConv of a length-$N$ sequence into chunks of size $N'$ each ($N'$ is the longest FFT we can fit into SRAM), assuming $N$ is a multiple of $N'$).\nWe use FFTConv to compute each chunk and use a recurrence to connect the chunks.\nIn particular, we split the inputs $u$ into $C = N/N'$ chunks $u^{(c)} \\in \\mathbb{R}^{N'}$ for $c=1, \\dots, C$.\nSimilarly, split the states $x$ into $x^{(c)} \\in \\mathbb{R}^{N' \\times m}$ and the output $y$ into $y^{(c)} \\in \\mathbb{R}^{N'}$ for $i = 1, \\dots, C$.\nWe will only need the end-state $x_{N'}^{(c)}$ of each chunk $c$.\n\nLet $f = [\\vC \\vB, \\vC\\vA\\vB, \\vC\\vA^2\\vB, \\dots, \\vC\\vA^{N'-1}\\vB]$ be the SSM filter.\nRecall from \\cref{sec:background} that for each chunk $c$, $y_i^{(c)} = \\vC\\vA^i\\vB x_{N'}^{(c-1)} + (f \\ast u^{(c)})_i + \\vD u_i^{(c)}$, since $x_{N'}^{(c-1)}$, the end-state of the previous chunk $(c-1)$ is the initial condition for the current chunk $c$.\nIn vector notation, $y^{(c)} = \\vM_{xy} x_{N'}^{(c-1)} + f \\ast u^{(c)} + \\vD u^{(c)}$ for some matrix $\\vM_{xy} \\in \\mathbb{R}^{N' \\times m}$.\nAdditionally we need to update the end-state of each chunk with $x_{N'}^{c} = \\vA^{N'} x_{N'}^{(c-1)} + \\vM_{ux} u^{(c)}$ for some matrix $\\vM_{ux}^{m \\times N'}$ (derivation in Appendix~\\ref{sec:state-passing-matrices}).\nIn essence, we can compute the output for each chunk with FFT-based convolution as long as we remember the end-state of the previous chunk, and the end-state of each chunk can be updated recurrently.\nThis yields a state-passing algorithm for long sequences, where we only compute FFT of length $N'$, and update some hidden state each iteration.\n\nLet \\textsc{BlockFFTConv} refer to our fused block FFTConv kernel.\nThen, the state-passing algorithm for 1D input is given by Algorithm~\\ref{alg:statepassing}.\nFor inputs of dimension $d$ where we stack $d$ SSMs, we simply batch~\\cref{alg:statepassing} along the $d$-dimension.\n\\begin{algorithm}[h]\n  \\algsetup{linenosize=\\tiny}\n  \\caption{\\label{alg:statepassing} State Passing Algorithm}\n  \\small\n  \\begin{algorithmic}[1]\n    \\REQUIRE Input $u \\in \\mathbb{R}^{N}$, SSM parameterized by matrices $\\vA \\in \\mathbb{R}^{m \\times m}$, $\\vB \\in \\mathbb{R}^{m \\times 1}$, $\\vC \\in \\mathbb{R}^{1 \\times m}$, $\\vD \\in \\mathbb{R}^{1 \\times 1}$, chunk size $N'$ where $N$ is a multiple of $N'$.\n    \\STATE Precompute $\\vA^{N'} \\in \\mathbb{R}^{m \\times m}$, $\\vM_{ux} = [\\vA^{N'-1}\\vB, \\dots, \\vB] \\in \\mathbb{R}^{m \\times N'}$, $\\vM_{xy} = [\\vC, \\vC\\vA, \\dots, \\vC\\vA^{N'-1}] \\in \\mathbb{R}^{N' \\times m}$.\n    \\STATE Split the inputs $u_{1:N}$ into $C = N/N'$ chunks $u_{1:N'}^{(c)}$ for $c=1, \\dots, C$.\n    \\STATE Let the initial state be $x_{N'}^{(0)} = 0 \\in \\mathbb{R}^m$.\n    \\FOR{$1 \\leq c \\leq C$}\n      \\STATE Compute $y^{(c)} = \\vM_{xy} x_{N'}^{(c-1)} + $ \\textsc{BlockFFTConv}($f$, $u_j$) $+ \\vD u^{(c)} \\in \\mathbb{R}^{N'}$.\n      \\STATE Update state: $x_{N'}^{(c)} = \\vA^{N'} x_{N'}^{(c-1)} + \\vM_{ux} u^{(c)}$.\n    \\ENDFOR\n    \\STATE Return $y = [y^{(1)} \\dots y^{(C)}]$.\n  \\end{algorithmic}\n\\end{algorithm}\n\nWe prove that~\\cref{alg:statepassing} yields the same output as if one has computed the SSM using a large FFT of size $N$ (proof in~\\cref{sec:app_statepassing_correctness_proof}):\n\\begin{proposition}\\label{thm:statepassing_correctness}\n  For input $u \\in \\mathbb{R}^N$ and matrices $\\vA, \\vB, \\vC, \\vD$, the output $y \\in \\mathbb{R}^N$ returned by~\\cref{alg:statepassing} is the same as the output defined by the SSM parameterized by $\\vA, \\vB, \\vC, \\vD$.\n\\end{proposition}\n\n\\section{\\hthree Evaluation}\n\\label{sec:evaluation}\n\n\\begin{table}[t]\n    \\small\n    \\centering\n    \\caption{\\label{table:gpt} Perplexity (lower is better) of models on the Pile, OpenWebText and\n      WikiText-103. GPT-Neo and hybrid \\hthree are trained on the Pile, while GPT2 is\n      trained on WebText. All models use the same GPT2 tokenizer. We report the\n      perplexity of GPT-2 models on the Pile ($^*$) for context, though the performance is not directly comparable since they were trained on different data.}\n    {\n      \\begin{tabular}{@{}|c|c|cc|@{}}\n      \\hline\n        Model & Pile & OpenWebText & WikiText103 \\\\ % & Training time \\\\\n        \\hline\n        GPT-2 small (125M) & 19.0* & 22.6 & 29.9 \\\\\n        GPT-Neo-125M & 9.4 & 22.6 & 26.3 \\\\\n        \\textbf{Hybrid H3-125M} & \\textbf{8.8} & \\textbf{20.9} & \\textbf{23.7} \\\\ \\hline %2.7 days \\\\ \\hline\n        GPT-2 medium (355M) & 13.9* & 17.0 & 21.8 \\\\ % & 11.5 days \\\\\n        \\textbf{Hybrid H3-355M} & \\textbf{7.1} & \\textbf{15.9} & \\textbf{16.9} \\\\ \\hline\n        GPT-2 XL (1.5B) & 12.4* & 12.9 & 17.0 \\\\\n        GPT-Neo-1.3B & 6.2 & 13.1 & 13.3 \\\\\n        \\textbf{Hybrid H3-1.3B} & \\textbf{6.0} & \\textbf{12.4} & \\textbf{12.5} \\\\\n        \\hline\n        GPT-Neo-2.7B & 5.7 & 11.7 & 11.5 \\\\\n        \\textbf{Hybrid H3-2.7B} & \\textbf{5.4} & \\textbf{11.0} & \\textbf{10.6} \\\\\n        \\hline\n      \\end{tabular}\n    }\n  \\end{table}\n\\begin{table}[t]\n    \\scriptsize\n    \\centering\n    \\caption{\\label{table:superglue_zeroshot_logit} Zero-shot acc.\\ on SuperGLUE with logit scoring. Best results in bold, second best underline. }\n    {\n        \\begin{tabular}{@{}|c|cccccccc|c|@{}}\n            \\hline\n        Model & WSC & WIC & RTE & CB & MultiRC & ReCoRD & BoolQ & COPA & Average \\\\ % & Training time \\\\\n        \\hline\n        OPT-125M & \\textbf{39.4} & \\underline{52.0} & 48.7 & 37.4 & \\underline{58.9} & \\underline{44.9} & \\underline{59.6} & \\underline{60.0} & 50.1 \\\\\n        GPT-Neo-125M & \\underline{36.5} & \\textbf{53.6} & \\underline{53.1} & \\underline{41.1} & \\textbf{59.9} & 39.6 & \\textbf{62.2} & \\underline{60.0} & \\underline{50.8} \\\\\n        \\textbf{Hybrid \\hthree-125M} & \\textbf{39.4} & 51.4 & \\textbf{59.2} & \\textbf{48.2} & 51.4 & \\textbf{55.0} & \\underline{59.6} & \\textbf{67.0} & \\textbf{53.9} \\\\ \\hline %2.7 days \\\\ \\hline\n        GPT-2 medium (355M) & \\underline{50.0} & \\textbf{52.0} & 51.3 & 28.6 & \\textbf{59.5} & \\underline{53.3} & \\underline{61.0} & \\underline{65.0} & 52.6 \\\\\n        OPT-350M & \\textbf{53.5} & 50.8 & \\underline{53.4} & \\underline{35.7} & \\underline{58.9} & 51.4 & 60.9 & 60.0 & \\underline{53.1} \\\\\n        \\textbf{Hybrid \\hthree-355M} & 37.5 & \\underline{51.7} & \\textbf{55.2} & \\textbf{41.1} & \\textbf{59.5} & \\textbf{62.3} & \\textbf{61.5} & \\textbf{69.0} & \\textbf{54.7} \\\\ \\hline\n        OPT-1.3B & 36.5 & 49.5 & \\textbf{53.4} & \\textbf{39.3} & \\textbf{58.3} & \\underline{61.8} & 55.0 & \\underline{69.0} & \\underline{52.9} \\\\\n        GPT-Neo-1.3B & \\underline{41.3} & \\underline{50.0} & \\underline{52.3} & \\underline{33.9} & 57.9 & 55.5 & \\underline{59.9} & 66.0 & 52.1 \\\\\n        \\textbf{Hybrid \\hthree-1.3B} & \\textbf{52.9} & \\textbf{50.3} & \\textbf{53.4} & \\underline{33.9} & \\underline{58.2} & \\textbf{67.8} & \\textbf{61.7} & \\textbf{74.0} & \\textbf{56.5} \\\\ \\hline\n        OPT-2.7B & \\textbf{51.0} & \\underline{50.8} & 50.5 & \\underline{41.1} & 57.4 & \\underline{65.9} & 60.9 & 66.0 & \\underline{55.5} \\\\\n        GPT-Neo-2.7B & \\underline{37.5} & 50.0 & \\underline{52.3} & \\textbf{50.0} & \\textbf{59.1} & 60.0 & \\textbf{61.1} & \\underline{67.0} & 54.6 \\\\\n        \\textbf{Hybrid \\hthree-2.7B} & 36.5 & \\textbf{51.3} & \\textbf{57.0} & 37.5 & \\underline{58.7} & \\textbf{71.3} & \\textbf{61.1} & \\textbf{81.0} & \\textbf{56.8} \\\\ \\hline\n        \\end{tabular}\n    }\n\\end{table}\\begin{table}[t]\n    \\scriptsize\n    \\centering\n    \\caption{\\label{table:superglue_fewshot_logit} 3-shot acc.\\ on SuperGLUE with logit scoring. Best results in bold, second best underline. }\n    {\n        \\begin{tabular}{@{}|c|cccccccc|c|@{}}\n            \\hline\n        Model & WSC & WIC & RTE & CB & MultiRC & ReCoRD & BoolQ & COPA & Average \\\\ % & Training time \\\\\n        \\hline\n        OPT-125M & 36.5 & \\textbf{50.2} & 47.3 & \\underline{44.6} & \\textbf{57.9} & \\underline{44.9} & 41.9 & 60.0 & \\underline{47.9} \\\\\n        GPT-Neo-125M & \\underline{38.5} & \\underline{50.0} & \\underline{53.1} & 17.9 & \\underline{56.3} & 39.6 & \\textbf{62.1} & \\underline{60.0} & 47.2 \\\\\n        \\textbf{Hybrid \\hthree-125M} & \\textbf{43.3} & 49.1 & \\textbf{58.1} & \\textbf{51.8} & 48.9 & \\textbf{55.0} & \\underline{56.1} & \\textbf{67.0} & \\textbf{53.7} \\\\ \\hline %2.7 days \\\\ \\hline\n        GPT-2 medium (355M) & 36.5 & \\textbf{50.5} & \\underline{48.0} & 8.9 & 43.5 & \\underline{53.3} & 58.8 & \\underline{65.0} & 45.6 \\\\\n        OPT-350M & \\underline{37.5} & \\underline{50.0} & 45.8 & \\textbf{44.6} & \\underline{49.8} & 51.4 & \\textbf{61.7} & 60.0 & \\underline{50.1} \\\\\n        \\textbf{Hybrid \\hthree-355M} & \\textbf{42.3} & 47.5 & \\textbf{50.5} & \\underline{28.6} & \\textbf{59.7} & \\textbf{62.3} & \\underline{60.5} & \\textbf{69.0} & \\textbf{52.6} \\\\ \\hline\n        OPT-1.3B & \\textbf{44.2} & \\textbf{51.1} & \\underline{53.4} & 16.1 & \\textbf{59.9} & \\underline{62.1} & 38.3 & \\underline{70.0} & 49.4 \\\\\n        GPT-Neo-1.3B & 35.6 & \\underline{50.6} & 47.3 & \\textbf{32.1} & \\textbf{59.9} & 55.7 & \\textbf{61.2} & 67.0 & \\underline{51.2} \\\\\n        \\textbf{Hybrid \\hthree-1.3B} & \\underline{36.5} & 49.2 & \\textbf{55.2} & \\underline{23.2} & \\underline{59.3} & \\textbf{67.6} & \\underline{56.9} & \\textbf{76.0} & \\textbf{53.0} \\\\ \\hline\n        OPT-2.7B & \\underline{44.2} & \\underline{50.5} & \\textbf{53.4} & 17.9 & \\underline{59.2} & \\underline{66.0} & \\textbf{62.0} & \\underline{71.0} & \\underline{53.0} \\\\\n        GPT-Neo-2.7B & \\textbf{49.0} & \\textbf{51.9} & \\underline{51.6} & \\underline{21.4} & 57.0 & 60.0 & 56.0 & 68.0 & 51.9 \\\\\n        \\textbf{Hybrid \\hthree-2.7B} & 36.5 & 45.6 & 47.3 & \\textbf{46.4} & \\textbf{59.4} & \\textbf{71.1} & \\underline{60.6} & \\textbf{77.0} & \\textbf{55.5} \\\\ \\hline\n        \\end{tabular}\n    }\n\\end{table}\nTo understand how well capturing the synthetics in Section~\\ref{sec:synthetics} translates to language modeling, we train two hybrid hybrid \\hthree-attention language models at sizes 125M, 355M, 1.3B, and 2.7B, and we evaluate their performance against Transformers.\nThe hybrid models match or exceed the quality of Transformers in perplexity and zero/few-shot learning.\nWe also validate that \\hthree models retain strong performance on non-text sequence modeling.\nAppendix~\\ref{sec:app_additional_experiments} contains additional experiments on more datasets, length extrapolation, and scaling with data.\n\n\\subsection{Language Modeling}\n\\label{subsec:language_modeling}\n\nWe compare hybrid \\hthree-attention language models against Transformer-based language models.\nWe evaluate language modeling performance using perplexity, zero-shot learning, and few-shot learning performance.\nHybrid \\hthree models outperform Transformers, which suggests that closing the gap between SSMs and attention on the synthetic languages translates to real language modeling capabilities.\nWe also report the generation speed of hybrid \\hthree models compared to Transformers; since SSMs are recurrent models, they can generate tokens \\num{2.4$\\times$} faster than Transformers.\nAppendix~\\ref{sec:app_additional_experiments} shows performance of pure \\hthree language models on these same evaluation metrics.\n\n\\paragraph{Setup}\nWe train hybrid models at sizes 125M, 355M, 1.3B, and 2.7B on the Pile~\\citep{gao2020pile} for 400B tokens.\nWe compare against checkpoints of equivalent sizes from\nOpen-AI~\\citep{radford2019language} and GPT-Neo\\footnote{There\n  is no pretrained GPT-Neo at the 350M size.}~\\citep{gpt-neo},\nfrom HuggingFace~\\citep{wolf-etal-2020-transformers}.\n\n\\paragraph{Perplexity}\nTable \\ref{table:gpt} shows perplexity on the Pile~\\citep{gao2020pile}, OpenWebText~\\citep{Gokaslan2019OpenWeb}, and WikiText-103~\\citep{merity2016pointer}. \nOn the Pile, our 125M hybrid model outperforms GPT-Neo, which was also trained on the Pile.\nOur hybrid models also outperform GPT-Neo models and GPT-2 models on zero-shot transfer to OpenWebText and WikiText103.\nWe report the PPL of GPT-2 models for context, though the performance is not directly comparable since they were trained on different data.\n\n\\paragraph{Zero- and Few-shot Performance}\nWe compare the zero- and few-shot performance of hybrid \\hthree language models against OPT~\\citep{zhang2022opt}, GPT-Neo, and GPT-2 models, where public checkpoints are available.\nWe report performance with rank classification on the logits of the possible choices (see Appendix~\\ref{sec:app_generation} for raw generation).\nTable~\\ref{table:superglue_zeroshot_logit} reports zero-shot performance on the SuperGLUE benchmark, and Table~\\ref{table:superglue_fewshot_logit} reports the 3-shot performance.\nFollowing the perplexity results, the hybrid language models outperform or match the best the Transformer baseline on more than half the tasks.\n\n\\begin{table}[t]\n\\centering\n    \\small\n    \\centering\n    \\caption{\\label{table:training_time} Inference throughput on A100 80GB, 1.3B models.\n    Batch size 64, prompt length 512, 1024, or 1536, and generating 128 tokens\n    per sequence in the batch (i.e., 64 $\\times$ 128 tokens in a batch). Hybrid\n    \\hthree is up to 2.4$\\times$ faster than a Transformer of similar size in inference. The\n    difference is larger for longer sequences.}\n    {\n        \\begin{tabular}{@{}|c|c|c|c|@{}}\n            \\hline\n        Tokens/s & Prompt length 512 & Prompt length 1024 & Prompt length 1536 \\\\ % & Training time \\\\\n        \\hline\n        Transformer-1.3B & 1340 & 770 & 520 \\\\\n        Hybrid \\hthree-1.3B & 1980 & 1580 & 1240 \\\\ \\hline\n        \\end{tabular}\n    }\n\\paragraph{Language Modeling Inference}\nFinally, since SSMs are recurrent models, they admit faster text generation than Transformers.\nTable~\\ref{table:training_time} shows inference throughput of a 1.3B-parameter hybrid model compared to a Transformer.\nThe hybrid model has up to \\num{2.4$\\times$} higher throughput.\n\n\\section{\\fastfft Evaluation \\label{sec:eval_efficiency}}\nWe evaluate how well \\fastfft speeds up SSMs.\n\\fastfft sets state-of-the-art performance on the long range arena benchmark~\\citep{tay2020long} using S4~\\citep{gu2022efficiently}.\nWe report performance of training \\hthree module with \\fastfft compared to attention at various sequence lengths, from 256 to 32K and demonstrate nearly linear scaling.\n\n\\begin{table}[t]\n    \\centering\n    \\caption{\\label{table:lra} Speedup on the LRA benchmark.}\n    \\centering\n    \\small\n    \\begin{tabular}{|c|c|}\n    \\hline\n    Models &  Speedup \\\\\n    \\hline\n    Transformer & 1$\\times$  \\\\\n    FlashAttention~\\citep{dao2022flashattention} & 2.4$\\times$ \\\\\n    Block-sparse FlashAttention~\\citep{dao2022flashattention} & 2.8$\\times$ \\\\\n    \\hline\n    S4~\\citep{gu2022train} & 2.9$\\times$ \\\\\n    S4 with \\fastfft & \\num{5.8$\\times$} \\\\ \\hline\n    \\end{tabular}\n\n\\paragraph{Long Range Arena}\nThe Long Range Arena (LRA) benchmark~\\citep{tay2020long} is a benchmark for long-range sequence modeling.\nThe state-of-the-art approach, S4~\\citep{gu2022train}, is an SSM.\nTable~\\ref{table:lra} shows that \\fastfft accelerates S4 by 2$\\times$, outperforming Transformers by 5.8$\\times$.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figs/benchmark_pdf.pdf}\n    \\caption{\\label{fig:fftconv_speed}\n      We compare the speed of different algorithms to perform FFT-based\n      convolution, along with FlashAttention~\\citep{dao2022flashattention} (the fastest attention\n      implementation we know of).\n      We use batch size 8, hidden dimension 1024, and varying sequence length\n      from 256 to 32k, and measure on an A100-SMX4-40GB GPU.\n      We see that kernel fusion gives up to 3.4$\\times$ speedup over naive FFTConv\n      for short sequences (up to 512), block FFT gives up to 2$\\times$ speedup for\n      medium length sequences (1k - 8k), and state-passing allows 2.3$\\times$ faster\n      FFTConv for long sequences (16k and above).\n    }\n\\end{figure}\\paragraph{Benchmark \\hthree Against Attention}\nWe benchmark the time to run the forward and backward pass of \\hthree with \\fastfft against attention.\n\\fastfft maintains nearly linear scaling, even to very long sequence lengths.\n\\cref{fig:fftconv_speed} shows overall 2-3$\\times$ speedup over FFTConv with cuFFT using our techniques\n(block FFT, state-passing).\nSimple kernel fusion (even without block FFT) can yield speedup over cuFFT for short sequences, since memory reads/writes are the bottleneck for short sequences.\nFor long sequences, SSMs using state passing can be dozens of times faster\nthan even the fastest attention implementation.\n\n\\section{Conclusion}\n\\label{sec:conc}\n\nOur main goal is to understand and narrow the gap between attention and SSMs in\nlanguage modeling in terms of modeling capabilities and hardware efficiency.\nOur exploration based on synthetic language tasks motivated us to design the\n\\hthree layer, which is surprisingly competitive with attention.\nOur \\textsc{BlockFFTConv} algorithm exploits matrix multiplication units and the\ndual recurrent--convolution view of SSMs to substantially speed up SSMs, reducing\nthe hardware barrier between attention and SSMs.\nWe are excited about several future directions.\nOur \\hthree layer is a simple combination of two SSMs, and more\nsophisticated designs could be more expressive.\nOur encouraging results on language models up to 1.3B parameters suggests that\nscaling SSMs to larger sizes is a promising avenue.\nSince simply adding two attention layers to \\hthree models already\noutperforms both the pure \\hthree model and Transformers, we are optimistic\nabout combining the complementary strengths of SSMs and attention in the future.\n\n\\ifarxiv\n\n\\else\n\\textbf{Reproducibility Statement.} To facilitate the reproducibility of our\nalgorithms and results, (i) we include a link to downloadable source code in\nsupplementary materials, (ii) for our theoretical statements and results, we\ninclude clear explanations of any assumptions and a complete proof of the claims\nin~\\cref{sec:proofs}; for any datasets used in the experiments, a complete description of the data processing steps is in~\\cref{sec:app_exp_details}.\nWe will also release model checkpoints for all our models.\n\n\\textbf{Ethics Statement.}\nOur work seeks to understand the fundamental capabilities and limitations of newly-emerging model architectures.\nAs the amount of data and model size grows, we also week to understand how to make training these models more efficient---and run inference more efficiently.\nThis potentially connects to energy savings during model development and deployment.\nWe also note that the relative underutilization of tensor cores in the FFT convolutions of state space models (even with our block FFT) suggests that consumer GPUs may be able to train models at a cheaper price point.\n\nHowever, as with any language model training, developing new techniques may impact a wide range of applications, each with potential benefits and harms.\nFor example, making language model training cheaper and making inference more efficient make it cheaper to spread disinformation.\nSimilarly, improving the efficiency of model training may not reduce the overall environmental footprint of training, since the same resources may be used to train more models, or train the same models for longer.\nWhile our work makes partial progress on the fronts of efficiency and understanding, it does not explicitly address the issues of fairness and bias in language models.\n\\fi\n\n\\section*{Acknowledgments}\n\nWe thank Albert Gu for helpful discussion regarding the model architecture, and\nmore importantly for sending us daily hippo videos.\nWe thank Together Computer\nfor providing portions of the compute used to train models in this paper.\nWe gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ARL under No. W911NF-21-2-0251 (Interactive Human-AI Teaming); ONR under No. N000141712266 (Unifying Weak Supervision); ONR N00014-20-1-2480: Understanding and Applying Non-Euclidean Geometry in Machine Learning; N000142012275 (NEPTUNE); NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, Google Cloud, Salesforce, Total, the HAI-GCP Cloud Credits for Research program,  the Stanford Data Science Initiative (SDSI),\nDepartment of Defense (DoD) through the National Defense Science and\nEngineering Graduate Fellowship (NDSEG) Program, \nWu Tsai Neuroscience Stanford Interdisciplinary Graduate Fellowship,\nand members of the Stanford DAWN project: Facebook, Google, and VMWare. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of DARPA, NIH, ONR, or the U.S. Government.\nAtri Rudras research is supported by NSF grant CCF-1763481.\n\n\\ifarxiv\n\\else\n\\fi\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{Gated Linear Attention Transformers with Hardware-Efficient Training}\n\n\\begin{document}\n\n\\twocolumn[\n\\icmltitle{Gated Linear Attention Transformers with Hardware-Efficient Training}\n\n\\icmlsetsymbol{equal}{*}\n\n\\begin{icmlauthorlist}\n\\icmlauthor{Songlin Yang}{yyy,equal}\n\\icmlauthor{Bailin Wang}{yyy,equal}\n\\icmlauthor{Yikang Shen}{comp}\n\\icmlauthor{Rameswar Panda}{comp}\n\\icmlauthor{Yoon Kim}{yyy}\n\\end{icmlauthorlist}\n\n\\icmlaffiliation{yyy}{Massachusetts Institute of Technology}\n\\icmlaffiliation{comp}{MIT-IBM Watson AI Lab}\n\n\\icmlcorrespondingauthor{Songlin Yang}{\\url{yangsl66@mit.edu}}\n\\icmlcorrespondingauthor{Bailin Wang}{\\url{bailinw@mit.edu}}\n\n\\icmlkeywords{Machine Learning, ICML}\n\n\\vskip 0.3in\n]\n\n\\printAffiliationsAndNotice{\\icmlEqualContribution} %\n\\vspace{-2mm}\n\\begin{abstract}\n\\vspace{-1mm}\nTransformers with linear attention allow for efficient parallel training  but can simultaneously  be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying  linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention  lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work  describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed \\textsc{FlashLinearAttention}, is faster than \\textsc{FlashAttention-2} \\cite{flashattention2} as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to \n a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform  competitively against the LLaMA-architecture Transformer \\citep{touvron2023llama} as well recent linear-time-inference baselines such as RetNet \\citep{sun2023retentive} and Mamba \\citep{Gu2023MambaLS} on moderate-scale language modeling experiments.  GLA Transformer is especially effective at length generalization, enabling a model trained on 2K  to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n\\begin{minipage}{\\linewidth}\n\n   \\faGithub \\quad  \\url{https://github.com/sustcsonglin/flash-linear-attention}\n\\end{minipage}\n\n \n\\end{abstract}\n\n\\vspace{-2mm}\n\\section{Introduction}\n\\vspace{-4mm}\n\\label{sec:intro}\nTransformers with softmax attention \\citep{vaswani2017attention}  enjoy efficient parallel training but suffer from quadratic (in sequence length) complexity, thus motivating more RNN-like models that allow for linear-time sequence modeling. Linear attention, which replaces the exponential similarity function with a simple dot product over (possibly transformed) key/query vectors, has emerged as a promising alternative to classic softmax attention \\citep{katharopoulos2020transformers,performer,kasai-etal-2021-finetuning,peng2021random}. An attractive property of linear attention  is that it admits a  ``recurrent form'' in which it can be formulated as a linear RNN with 2D hidden states \\citep{katharopoulos2020transformers}, thus enabling linear-time  inference. For training, linear attention  also admits a subquadratic ``chunkwise parallel form'' which divides the sequence into non-overlapping chunks and performs (serial) inter-chunk recurrent computations followed by (parallel) intra-chunk computations \\citep{GAU,sun2023retentive,VQ-Transformer}, thus (partially) maintaining  parallel training. However,  existing algorithms for linear attention are not I/O aware and thus, in practice, slower than  optimized implementations of softmax attention \\cite{flashattention1,flashattention2} on moderate sequence lengths.\n\nFrom a performance standpoint,  linear attention has generally been found to underperform ordinary softmax attention, often by a significant margin in language modeling \\cite{kasai-etal-2021-finetuning}. Recent variants of linear attention such as RetNet \\citep{sun2023retentive} and TransNormerLLM \\citep{qin2023scaling} obtain significant improvements by multiplying the current hidden state with a decay factor before the RNN update. However,  these works use a global, \\emph{data-independent} decay factor, despite the fact that in 1D RNNs, a \\emph{data-dependent} gating mechanism has been shown to be crucial for performance \\citep{unreasonable-forget-gate,HGRN}. And even with the decay factor, linear attention Transformers underperform the strongest Transformer architectures when pretrained from scratch. \n\nThis work develops a hardware-efficient algorithm for linear attention, and applies it to train a gated variant of linear attention that is competitive with softmax attention. We first discuss  aspects of optimizing ordinary linear attention on modern GPUs  and give two I/O-aware algorithms (tailored for different training settings) based on these principles (\\S\\ref{sec:algorithm}). Our implementation of the algorithm, called \\textsc{FlashLinearAttention}, is faster than \\textsc{FlashAttention-2} \\citep{flashattention2} even on short (e.g., 1K) sequences. \nWe then describe a gated linear attention layer with a data-dependent gating mechanism and show how \\textsc{FlashLinearAttention} can be generalized to the gated case (\\S\\ref{sec:gla}). \nWe study the resulting \\emph{gated linear attention (GLA) Transformer} on moderate-scale language modeling benchmarks, where we train models with 340M/1.3B parameters on 15B/100B tokens, respectively. We find that the GLA Transformer performs favorably against a strong LLaMA architecture Transformer baseline that makes use of recent recipes \\citep[Transformer++;][]{touvron2023llama} as well as recent linear-time sequence models such as RetNet \\cite{sun2023retentive} and Mamba \\citep{Gu2023MambaLS}. GLA Transformer is found to be particularly strong at length generalization and recall-intensive tasks among linear recurrent models. For training speed,  the GLA Transformer has significantly  higher throughput than a similarly sized Mamba model.\n\n\\vspace{-3mm}\n\\section{Background: Linear Attention}\n\\vspace{-3mm}\nWe first give a brief background on linear attention layers. For notation we  use bold upper-case letters for matrices (e.g., $\\rmS$, $\\rmQ$),  bold lower-case letters  for vectors (e.g., $\\vq_t$, $\\vk_t$), and italic upper-case  for learnable parameters matrices (e.g., $\\mW_K$). We generally use the same alphabet to show the rows of a matrix, e.g., $\\vq_t$ is the $t$-th row of $\\rmQ$.\n\n\\label{sec:background}\n\\vspace{-2mm}\n\\subsection{Parallel and Recurrent Forms}\n\\vspace{-2mm}\n\\label{subsec:background-lin}\nStandard autoregressive Transformers employ a softmax attention mechanism which takes an input sequence $\\rmX \\in \\R^{L \\times d}$ (here $L$ is the length and $d$ is the hidden dimension) and computes the output $\\rmO \\in \\R^{L \\times d}$ through,\n\\begin{align*}\n\\rmQ, \\rmK, \\rmV &= \\rmX \\mW_Q, \\rmX \\mW_K, \\rmX \\mW_V, \\\\  \\rmO &= \\softmax\\big((\\rmQ \\rmK^\\intercal) \\odot \\rmM \\big) \\rmV,\n\\end{align*}\nwhere $\\mW_Q, \\mW_K, \\mW_V \\in \\R^{d \\times d}$ are learnable matrices and \n$\\rmM \\in \\{-\\infty,1\\}^{L \\times L}$ is a mask that prevents the model from attending to  future tokens, i.e., $\\rmM_{ij}=1$ if $i\\ge j$ and $\\rmM_{ij}=-\\infty$ if $i<j$. (Here we assume a single attention head for simplicity.) \nThe above \\textit{parallel form} of attention can compute $\\rmO$ in parallel given the full input $\\rmX$, thus enabling efficient training. However, during inference Transformers must use the following \\emph{recurrent form},\n\\begin{align*}\n\\vq_t, \\ \\vk_t, \\ \\vv_t &= \\vx_t \\mW_Q, \\  \\vx_t \\mW_K, \\ \\vx_t \\mW_V, \\\\ \\vo_t &= \\frac{\\sum_{i=1}^{t} \\exp(\\vq_t  \\vk_i^\\intercal)\\vv_i}{\\sum_{i =1} ^{t} \\exp(\\vq_t  \\vk_i^\\intercal)},\n\\end{align*}\nwhich calculates the query ($\\vq_t$), key ($\\vk_t$), and value ($\\vv_t$) vectors given the current token's representation $\\vx_t \\in \\R^{1 \\times d}$ and the performs attention over the (growing) set of keys $\\{\\vk_1, \\dots, \\vk_t\\}$ and values   $\\{\\vv_1, \\dots, \\vv_t\\}$ (i.e., the ``KV cache''). \n\nLinear attention mechanisms~\\citep{katharopoulos2020transformers} replace $\\exp(\\vq_t \\vk_i^\\intercal)$ with a kernel $k(\\vx, \\vy)$ with an associated feature map $\\phi$ (i.e., $k(\\vx, \\vy) = \\langle\\phi(\\vx), \\phi(\\vy)\\rangle$).  This simplifies the calculation of $\\vo_t$ since we have\n\\begin{align*}\n \\vo_t &= \\frac{\\sum_{i=1}^{ t}\\phi(\\vq_t)\\phi(\\vk_i)^\\intercal \\vv_i}{\\sum_{i=1}^{t} \\phi(\\vq_t)\\phi(\\vk_i)^\\intercal  }  \n = \\frac{\\phi(\\vq_t)   \\sum_{i=1}^{t}\\phi(\\vk_i)^\\intercal \\vv_i}{\\phi(\\vq_t) \\sum_{i=1}^{t}\\phi(\\vk_i)^\\intercal}.\n\\end{align*}\nLetting $\\rmS_t=\\sum_{i=1}^{t}\\phi(\\vk_i)^\\intercal \\vv_i$ and $\\vz_t=\\sum_{i=1}^{t}\\phi(\\vk_i)^\\intercal$ where $\\rmS_t \\in \\mathbb{R}^{d\\times d}, \\vz_t \\in \\mathbb{R}^{d\\times 1}$, we can rewrite the above as an RNN,\n\\begin{align*}\n\\rmS_t = \\rmS_{t-1} &+ \\phi(\\vk_t)^\\intercal \\vv_t, \\hspace{1mm} \\vz_t = \\vz_{t-1} + \\phi(\\vk_t)^\\intercal, \\hspace{1mm} \\vo_t = \\frac{\\phi(\\vq_t) \\rmS_t}{ \\phi(\\vq_t) \\vz_t}.\n\\end{align*}\n Although various kernels have been explored~\\citep{kasai-etal-2021-finetuning,peng2021random}, recent work has found that a linear kernel (i.e., setting $\\phi$ to be the identity) without a normalizer  works well in practice \\cite{sun2023retentive}. \nThis results in an (unnormalized) linear attention layer with the following update equation,\n\\begin{align}\n& \\rmS_t = \\rmS_{t-1} + \\vk_t^\\intercal \\vv_t, \\quad \\vo_t = \\vq_t \\rmS_t   .\n\\label{eq:simple_linear_attention}\n\\end{align}\nEq.~\\ref{eq:simple_linear_attention} makes it clear that  a linear attention layer is essentially a linear recurrent layer with matrix-valued hidden states $\\rmS_t$ that is updated via the outer-product $\\vk_t^\\intercal\\vv_t = (\\vx_t \\mW_K)^\\intercal (\\vx_t \\mW_V)$.\\footnote{This type of model with matrix-valued hidden states that change over time is also known as ``fast weights'' \\citep{hinton1987using,schmidhuber1992learning,ba2016using}, whose connection to Transformers was  explored in recent work \\citep{linear-xmr-fastweight,irie2021going,mao-2022-fine}.} \nThe parallel form of causal linear attention, whose complexity is still quadratic in $L$, is given by\n$\\rmO = \\big((\\rmQ \\rmK^\\intercal) \\odot \\rmM \\big)\\rmV$,\nwhere $\\rmM \\in \\{0,1\\}^{L \\times L}$ is a mask such that $\\rmM_{ij}=1$ if $i\\ge j$ and $\\rmM_{ij}=0$ if $i<j$. Due to $\\rmM$ it is not possible to exploit the associative property of matrix multiplication to reduce the parallel form complexity from quadratic to linear.\\footnote{Without $\\rmM$, one can transform $(\\rmQ \\rmK^\\intercal)\\rmV$ to $\\rmQ( \\rmK^\\intercal\\rmV)$ reducing the complexity from quadratic ($O(L^2d)$) to linear ($O(Ld^2)$).}\n\n\\vspace{-3mm}\n\\subsection{Chunkwise Parallel Form} \n\\vspace{-2mm}\n\\label{background:lin-chunkwise}\n\nThe \\emph{chunkwise} parallel form of linear attention  strikes a balance between parallel and recurrent form \\cite{GAU,sun2023retentive}, and allows for subquadratic, partially parallel training.\n Formally, suppose the input $\\rmX$ is now split into non-overlapping chunks, where each chunk is of length $C$. \nLet $\\rmS_{[i]} \\in \\R^{d \\times d}$ be the chunk-level hidden state after processing $i$ chunks, i.e., $\\rmS_{[i]}:=\\rmS_{iC}$. Further let $\\rmQ_{[i]}:=\\rmQ_{iC+1:(i+1)C+1} \\in \\mathbb{R}^{C\\times d}$ be the query vectors corresponding to the $i$-th chunk; let $\\rmK_{[i]}$, $\\rmV_{[i]}$, $\\rmO_{[i]}$ be similarly defined. We then have the following inter-chunk recurrence (for $i \\in [0, 1, \\dots \\frac{L}{C}-1]$):\n    \\begin{equation}\n\\rmS_{[i+1]} = \\rmS_{[i]} + \\underbrace{\\sum_{j=iC + 1}^{(i+1)C} \\vk_{j}^\\intercal \\vv_{j}}_{\\rmK^\\intercal_{[i]}\\rmV_{[i]}} \\quad \\hspace{1mm} \\in \\mathbb{R}^{d\\times d}.\n\\label{eq:la-inter-chunk}\n    \\end{equation}\nHere  $\\rmS_{[0]}$ can be initialized to  zero or from the previous segment's hidden state. The sum of all RNN inputs from a chunk (i.e., $\\rmK^\\intercal_{[i]}\\rmV_{[i]}$) can be computed in $O(C^2d)$ in parallel.\nThe intra-chunk parallel computation for the output is given by\n\\begin{equation*}    \n\\rmO_{[i+1]} = \\underbrace{\\rmQ_{[i+1]}\\rmS_{[i]}}_{\\text{inter-chunk}: \\rmO^\\text{inter}_{[i+1]}} + \\underbrace{\\big((\\rmQ_{[i+1]}\\rmK_{[i+1]}^{\\intercal})\\odot\\rmM\\big)\\rmV_{[i+1]}}_{\\text{intra-chunk}: \\rmO^{\\text{intra}}_{[i+1]}},\n\\label{eq:la_block_wise}\n\\end{equation*}\nwhere $\\rmO_{[i+1]} \\in \\mathbb{R}^{C\\times d}$. Here the ``intra-chunk'' component $\\rmO^\\text{intra}_{[i+1]}$  has exactly the same parallel form as Eq.~\\ref{eq:simple_linear_attention} and thus takes $O(C^2d + Cd^2)$. The ``inter-chunk'' component $\\rmO^\\text{inter}_{[i+1]}$ accounts for the contribution from the hidden state from the previous chunk, and takes $O(Cd^2)$. Training complexity is thus  $O\\left(\\frac{L}{C}(C^2d + Cd^2) \\right)=O(LCd+Ld^2)$, which is less than $O(L^2d)$ when $L>d$.  Note that setting $C = L$ recovers the parallel form, and $C=1$ recovers the recurrent form.\n\n\\vspace{-2mm}\n\\section{Hardware-Efficient Linear Attention}\n\\vspace{-2mm}\n\\label{sec:algorithm}\nWe describe \\textsc{FlashLinearAttention}, an I/O-aware, hardware-efficient algorithm for linear attention in the spirit of \\textsc{FlashAttention} \\cite{flashattention1,flashattention2}. We first discuss aspects of hardware that should be taken into account for a practically efficient implementation.\n\n\\vspace{-2mm}\n\\subsection{Principles of Hardware-Efficient Algorithms}\n\\vspace{-2mm}\n An efficient  algorithm should be aware of the compute model, memory hierarchy, and specialized compute units on modern hardware. \n\n\\vspace{-2mm}\n\\paragraph{Occupancy.} GPUs have many threads executed in parallel; threads are grouped into thread blocks, which execute on streaming multiprocessors (SMs). To maintain a high GPU occupancy (i.e., fraction of GPU resources being used), it is necessary to use a sufficient number of SMs. \nIn large-scale training and long-sequence modeling scenarios where the batch size tends to be small, \nparallelizing over the temporal dimension enables high GPU occupancy \\cite{flashattention2}. \n\n\\vspace{-2mm}\n\\paragraph{Specialized compute units.} Modern hardware for neural network training typically have specialized compute units (e.g., {tensor cores} on NVIDIA GPUs, matrix mutiply units on TPUs), which can significantly accelerate matmuls; for example half-precision matmuls\non an A100 can be roughly 16 times faster on tensor cores than on CUDA cores. These specialized units are crucial for large-scale training.\n\n\\vspace{-2mm}\n\\paragraph{Memory hierarchy.} GPUs have a memory hierarchy with larger but slower global GPU memory (high bandwidth memory; HBM) and smaller but faster shared memory (SRAM). Optimal utilization of SRAM to reduce HBM I/O cost can therefore lead to significant speed-ups.\n\n\\begin{algorithm}[t!]\n\\vspace{-0.5mm}\n\\scriptsize\n\\caption{\\textsc{FlashLinearAttention}: Forward Pass}\n\\label{algo:la-chunk-fwd}\n\\begin{algorithmic}\n    \\Require $\\rmQ, \\rmK, \\rmV \\in \\R^{L \\times d}, \\rmV  \\in \\R^{L \\times d}$, chunk size $C \\in [L]$, \\texttt{materialize} $\\in$ \\{\\texttt{True,False}\\} \n    \\State Divide $\\rmQ, \\rmK, \\rmV$ into $N = \\frac{L}{C}$ blocks $\\{ \\rmQ_{[1]} \\dots \\rmQ_{[N]} \\} $, $\\{ \\rmK_{[1]} \\dots \\rmK_{[N]} \\}$ of size $C \\times d$ each.\n     \\State{Initialize $\\rmS = \\bm{0} \\in \\mathbb R^{d\\times d} $ on SRAM}\n    \\State{On chip, construct causal mask $\\rmM\\in \\R^{C\\times C}$}\n    \\If{\\texttt{materialize}} \\Comment{the materialization version}\n    \\For{$n \\gets 1, N$}\n      \\State{Store $\\rmS$ to HBM as \n      $\\rmS_{[n]}$.}\n        \\State{Load $\\rmK_{[n]}, \\rmV_{[n]} \\in \\mathbb{R}^{C \\times d}$ from HBM to SRAM}\n        \\State{On chip, compute $\\mathbf{S} = \\mathbf{S} + \\rmK_{[n]}^{\\top}  \\rmV_{[n]}$.}\n        \\color{black}\n    \\EndFor\n    \\ParFor{$n \\gets 1, N$}\n        \\State Load $\\rmQ_{[n]}, \\rmK_{[n]}, \\rmV_{[n]}, \\rmS_{[n]}$ from HBM to SRAM.\n        \\State On chip, compute $\\rmO^{\\prime} = \\rmQ_{[n]} \\rmS_{[n]} + (\\rmQ_{[n]} \\rmK^\\intercal_{[n]} \\odot \\rmM)\\rmV_{[n]}$  \n        \\State Store $\\rmO^{\\prime}$ to HBM as $\\rmO_{[n]}$.\n    \\EndParFor\n  \\State \\Return $\\rmO = \\{ \\rmO_{[1]} \\dots \\rmO_{[N]} \\}$, $\\rmS=\\{ \\rmS_{[1]} \\dots \\rmS_{[N]} \\}$.\n        \\Else \\Comment{the non-materialization version}\n         \\For{$n \\gets 1, N$}\n      \\color{black}\n        \\State{Load $\\rmQ_{[n]}, \\rmK_{[n]}, \\rmV_{[n]} \\in \\mathbb{R}^{C \\times d}$ from HBM to SRAM}\n        \\State{On chip, compute $\\rmO' = \\rmQ_{[n]}\\rmS + (\\rmQ_{[n]}\\rmK_{[n]}^{\\top}\\odot\\rmM)\\rmV_{[n]}$}\n        \\State{On chip, compute $\\mathbf{S} = \\mathbf{S} + \\rmK_{[n]}^{\\top}  \\rmV_{[n]}$.}\n        \\State{Store $\\rmO'$ to HBM as $\\rmO_{[n]}$ }\n        \\color{black}\n    \\EndFor\n   \\State  \\Return $\\rmO = \\{ \\rmO_{[1]} \\dots \\rmO_{[N]} \\}$   \n\\EndIf \n\\vspace{-0.5mm}\n\\end{algorithmic}\n\\end{algorithm}\n\n\\vspace{-2mm}\n\\subsection{Hardware Considerations for Linear Attention}\n\\vspace{-2mm}\nWe now discuss hardware considerations pertaining to the efficiency of the different forms of linear attention.  \n\n\\vspace{-2mm}\n\\paragraph{Recurrent form.}  A basic implementation of the recurrent form  stores the 2D hidden states of all time steps in HBM, resulting in  high I/O cost \\cite{mao-2022-fine}. I/O cost could be reduced by avoiding such materialization and recomputing the hidden states during the backward pass, as in \\citet{katharopoulos2020transformers}, but the elementwise operations in the recurrent update cannot make use of tensor cores and result in low arithmetic intensity. Hence, while the recurrent form generally has the lowest total FLOPs among the three forms, this does not translate to actual wall-time efficiency.  And while it is theoretically possible to parallelize  linear recurrences via the parallel scan algorithm, this method requires materializing the 2D hidden state for each time step. This  incurs a significant memory I/O burden, thereby offsetting the benefits of parallelism over the sequence length and resulting in slow actual running speeds, as in \n \\citet{gatedloop}. \n\n\\vspace{-2mm}\n\\paragraph{Parallel form.} The parallel form could be as efficient as \\textsc{FlashAttention} using similar I/O optimization techniques, as demonstrated by \\citet{qin2023scaling}. However, the high number of FLOPs (due to the quadratic complexity) makes the long-sequence training expensive, the same issue that the na\\\"ive implementation of softmax attention would suffer from. %\n\n\\vspace{-2mm}\n\\paragraph{Chunkwise form.} The chunkwise parallel form, which interpolates between the parallel and recurrent forms with an extra ``parameter'' $C$, makes it possible to more easily make the above tradeoffs for fine-grained  optimization. \nUnlike the recurrent form, most operations can be done via matmuls, enabling the use of tensor cores (if $C$ is set to a multiple of 16). \nThough the chunkwise training algorithm has been discussed before in the literature \\cite{GAU, sun2023retentive}, most implementations are not I/O-aware and thus slower than \\textsc{FlashAttention} for moderate sequence lengths (e.g., 2K-4K). \n\n\\usetikzlibrary{arrows.meta,\n                positioning,\n                shadows}\n\n\\usetikzlibrary{shapes.multipart,positioning}\n\n\\usetikzlibrary{shapes, arrows, arrows.meta, fit,backgrounds, positioning, calc}\n\n\\tikzstyle{process} = [rectangle, minimum width=2cm, minimum height=1cm, text centered, text width=2cm,  draw=black, fill=orange!30]\n\\tikzstyle{arrow} = [thick,->,>=stealth]\n\\tikzstyle{container1} = [draw, rectangle, inner sep=0.3cm, fill=gray!20]\n\\tikzset{\n  mybackground/.style={execute at end picture={\n      \\begin{scope}[on background layer]\n        \\node[] at (current bounding box.north){\\bottom{1cm} #1};\n        \\end{scope}\n    }},\n}\n\n\\tikzset{\n    font=\\sffamily,\n    BLOCK/.style={\n        draw,\n        align=center,\n        draw=red!50,\n        fill=red!20,\n        rectangle split, \n        rectangle split horizontal,\n        rectangle split parts=#1, \n    }\n}\n\\tikzset{cross/.style={cross out, draw, \n         minimum size=2*(#1-\\pgflinewidth), \n         inner sep=0pt, outer sep=0pt}}\n\\begin{figure}[t]\n\\centering  \n\t\t\\resizebox{.8\\columnwidth}{!}{  \n        \\begin{tikzpicture}[%\n            every path/.style={thick,%\n            },\n                           dcs/.style = {double copy shadow, shadow xshift=4pt, shadow yshift=-4pt}\n          ]\n        \\node [rectangle, minimum width=.5cm, minimum height=1cm, text centered, text width=1cm,  draw=black, fill=blue!30] (process3) at (-2, -1.5) {$\\mathbf{S}_{[i-1]}$};\n\n        \\node [rectangle, minimum width=.5cm, minimum height=.75cm, text centered, text width=.75cm,  draw=black, fill=red!20] (hh1) at (-2, -6.5) {$\\mathbf{S}_{[i-1]}$};\n        \\node [rectangle, minimum width=.5cm, minimum height=.75cm, text centered, text width=.75cm,  draw=black, fill=red!20] (hh2) at (-0.5, -6.5) {$\\mathbf{S}_{[i]}$};\n        \\node [rectangle, minimum width=.5cm, minimum height=.75cm, text centered, text width=.75cm,  draw=black, fill=red!20] (hh3) at (1, -6.5) {$\\mathbf{S}_{[i+1]}$};\n\n        \\node [rectangle, minimum width=.5cm, minimum height=.7cm, text centered, text width=.7cm,  draw=black, fill=orange!30] (hh4) at (3, -6.5) {$\\mathbf{S}_{[i]}$};\n        \n        \\node [rectangle, minimum width=.5cm, minimum height=1cm, text centered, text width=1cm,  draw=black, fill=blue!30] (h2) at (2, -1.5) {$\\rmS_{[i]}$};\n        \\node [rectangle, minimum width=.5cm, minimum height=1cm, text centered, text width=1cm,  draw=black, fill=blue!30] (h3) at (6, -1.5) {$\\rmS_{[i+1]}$};\n        \n        \\node[draw, align=center,        draw=red!50,fill=red!20,rectangle split, \n        rectangle split horizontal,\n        rectangle split parts=2, \n    ] (block4) at (4, 0){\n        \\nodepart{one} $\\rmO^{\\text{inter}}_{[i+1]}$ \\nodepart{two} $\\rmO^{\\text{intra}}_{[i+1]}$ };\n            \n\\node[draw,\n        align=center,\n        draw=red!50,\n        fill=red!20,\n        rectangle split, \n        rectangle split horizontal,\n        rectangle split parts=2, \n    ] (blockk4) at (5.5, -6.5){\n        \\nodepart{one} $\\rmO^{\\text{inter}}_{[i+1]}$ \\nodepart{two} $\\rmO^{\\text{intra}}_{[i+1]}$\n    };\n\n    \\node[draw,\n        align=center,\n        draw=red!50,\n        fill=red!20,\n        rectangle split, \n        rectangle split horizontal,\n        rectangle split parts=2, \n    ] (block2) at (0,0)\n    {\n        \\nodepart{one} $\\rmO^{\\text{inter}}_{[i]}$ \\nodepart{two} $\\rmO^{\\text{intra}}_{[i]}$\n    };\n    \n    \\node[BLOCK=3, fill=orange!30, draw=orange!80\n    ] (block) at (0,-3) {\n        \\nodepart{one} $\\rmQ_{[i]}$ \\nodepart{two} $\\rmK_{[i]}$ \\nodepart{three} $\\rmV_{[i]}$\n};\n\n    \\node[BLOCK=2, fill=orange!30, draw=orange!80\n    ] (blockk1) at (-2,-8) {\n      \\nodepart{one} $\\rmK_{[i]}$ \\nodepart{two} $\\rmV_{[i]}$\n};\n    \\node[BLOCK=2, fill=orange!30, draw=orange!80\n    ] (blockk2) at (0.5,-8) {\n      \\nodepart{one} $\\rmK_{[i+1]}$ \\nodepart{two} $\\rmV_{[i+1]}$\n};\n    \\node[BLOCK=3,  fill=orange!30, draw=orange!80\n    ] (block3) at (4,-3) {\n        \\nodepart{one} $\\rmQ_{[i+1]}$ \\nodepart{two} $\\rmK_{[i+1]}$ \\nodepart{three} $\\rmV_{[i+1]}$\n        \n    };\n    \\node[BLOCK=3,  fill=orange!30, draw=orange!80\n    ] (blockk3) at (4.75,-8) {\n        \\nodepart{one} $\\rmQ_{[i+1]}$ \\nodepart{two} $\\rmK_{[i+1]}$ \\nodepart{three} $\\rmV_{[i+1]}$\n    };\n\n    \\node [rectangle, text centered, text width=.5cm, text height=.5cm, draw=black, fill=orange!30, label=right:{Load from HBM}] (node1) at (-2.5, -4.25){};\n    \\node [rectangle, text centered, text width=.5cm, text height=.5cm, draw=black, fill=red!20, label=right:Store to HBM] (node2) at (.9, -4.25){};\n    \\node [label=right:(a)] (labell) at (6, -3.3){};\n        \\node [label=right:(c)] (labelc) at (6, -8.6){};\n            \\node [label=right:(b)] (labelb) at (1, -8.6){};\n    \\node [rectangle, text centered, text width=.5cm, text height=.5cm, draw=black, fill=blue!30, label=right:On-chip construct] (node3) at (4, -4.25){};\n\n            \\draw [arrow] (process3) -- (block2.one south);\n                \\draw [arrow] (h2) -- (block4.one south);\n            \\draw [arrow] (block.one north) -- (block2.one south);\n            \\draw [arrow, dashed] (process3) -- (h2);\n            \\draw [arrow, dashed] (h2) -- (h3);\n            \\draw [arrow] (block.one north) -- (block2.two south);\n                    \\draw [arrow] (block3.one north) -- (block4.one south);\n                                    \\draw [arrow] (block3.two north) -- (block4.two south);\n            \\draw [arrow] (block3.three north) -- (block4.two south);\n            \\draw [arrow] (block3.three north) -- (h3);\n            \\draw [arrow] (block3.one north) -- (block4.two south);\n            \\draw [arrow] (block.two north) -- (block2.two south);\n            \\draw [arrow] (block.three north) -- (block2.two south);\n            \\draw [arrow] (block.two north) -- (h2);\n            \\draw [arrow] (block.three north) -- (h2);\n            \\draw [arrow, dashed] (hh1) -- (hh2);\n            \\draw [arrow, dashed] (hh2) -- (hh3);\n            \\draw [arrow] (blockk1.one north) -- (hh2);\n            \\draw [arrow] (blockk1.two north) -- (hh2);\n            \\draw [arrow] (blockk2.two north) -- (hh3);\n            \\draw [arrow] (blockk2.one north) -- (hh3);\n            \\draw [arrow] (hh4) -- (blockk4.one west);\n            \\draw [arrow] (blockk3.one north) -- (blockk4.one west);\n                        \\draw [arrow] (blockk3.one north) -- (blockk4.two south);\n            \\draw [arrow] (blockk3.two north) -- (blockk4.two south);\n            \\draw [arrow] (blockk3.three north) -- (blockk4.two south);\n                        \\draw [arrow, dashed] ($(hh1)-(1,0)$) -- (hh1);\n                        \\draw [arrow, dashed] (hh3) -- ($(hh3)+(1,0)$);\n                        \\draw [arrow, dashed] (h3) -- ($(h3)+(1.5,0)$);\n                        \\draw [arrow, dashed] ($(process3)-(1.5,0)$) -- (process3);\n\n        \\begin{scope}[on background layer]\n    \\node  [container1,fit= {(process3) (h2) (block) (block2) (h3) (block3) (labell)}, label=north:\\large Sequential] (contain1) {};\n    \\end{scope}\n                        \\begin{scope}[on background layer]\n                \\node  [container1,fit= { (hh1) (hh2) (hh3) (blockk1)  (blockk2) (labelb)}, label=north:\\large Sequential] (contain2) {};\n            \\end{scope}\n                        \\begin{scope}[on background layer]\n                \\node  [container1,dcs, fit= { (hh4) (blockk3) (blockk4) (labelc)}, label={[label distance=0.25cm]north:\\large Chunkwise Parallel}, fill=yellow!20] (contain3) {};\n            \\end{scope}\n\n        \\end{tikzpicture}   \n}\n        \\vspace{-2mm}\n        \\caption{(a) \\textsc{FlashLinearAttention} without materialization. This version is more memory-efficient. (b-c) \\textsc{FlashLinearAttention} with materialization. This version enables sequence-level chunkwise parallelism.} \\label{fig:chunk}\n              \n    \\end{figure}\n\\vspace{-2mm}\n\\subsection{\\textnormal{\\textsc{FlashLinearAttention}}: Hardware-Efficient  Linear Attention with the Chunkwise Form}\n\\label{sec:fla}\n\\vspace{-2mm}\nWe describe our I/O-aware, hardware-efficient implementation of the chunkwise form. We give two versions, whose forward and backward passes differ depending on whether the chunk-level hidden states  $\\rmS_{[n]}$ are materialized in HBM. See Alg.~\\ref{algo:la-chunk-fwd} and Fig.~\\ref{fig:chunk} for the forward pass. (Alg.~\\ref{algo:la-chunk-bwd} in the appendix describes the backward pass.)  At a high level, we use tiling to load tensors block-by-block and re-use tensor blocks on chip to avoid multiple HBM I/O as {much} as possible. For example, when $\\rmQ_{[n]}$ is loaded to SRAM, both $\\rmQ_{[n]}\\rmS$ and $(\\rmQ_{[n]}\\rmK_{[n]}^{\\top}\\odot\\rmM)\\rmV_{[n]}$ can be computed on chip, which avoids loading $\\rmQ_{[n]}$ twice, thus saving HBM I/O. \n\n\\textbf{The non-materialization version} computes $\\rmO_{[n]}$ sequentially for $n \\in [N]$, using SRAM to temporarily store $\\rmS_{[n]}$, which is memory-efficient. This version parallelizes across batch size, number of heads, and head dimensions, but lacks sequence-level parallelim. When the batch size is large, this level of parallelism is sufficient to enable high GPU occupancy. In long-sequence and large scale training settings where batch size is small, the SMs cannot be fully exploited in this case. \\textbf{The materialization version} first performs the inter-chunk recurrence (Eq.~\\ref{eq:la-inter-chunk}) and stores all $\\rmS_{[n]}$ for $n \\in [N]$ in HBM. Then, the $\\rmO_{[n]}$'s can be computed in parallel for all chunks.  \n This approach offers better parallelism but increases the memory footprint by approximately 10-20\\%. We mitigate this through \\textit{recomputation}, where the hidden states discarded  after the forward pass and recomputed during the backward pass. We find this introduces a small runtime  overhead but significantly reduces the memory footprint, and  we adopt this strategy by default.\n\n\\begin{figure}[t]\n\\centering  \n\t\t\\resizebox{\\columnwidth}{!}{  \n\\begin{tikzpicture}\n\\scalefont{1.4} %\n\\begin{axis}[%\nname=plot1,\ntitle={Running speed},\nscatter/classes={%\n1={mark=diamond*,draw=black},\n0={mark=o,draw=black}},\nlegend style={at={(5.5,-1.6)},   \n                anchor=north,legend columns=2,\n                column sep=0cm,\n                font=\\normalsize,\n                row sep=0cm\n                },     \n    legend cell align=left,\n    tick label style={font=\\large},\n    label style={font=\\large},\nlegend to name={mylegend},\n\t\t\tymajorgrids=true, %\n\t\t\txmajorgrids=true, %\n\t\t\tgrid style=dashed, %\nxmode=log,\nymode=log,\nlog basis x={2},\nylabel={Time (ms)},\nxlabel={Sentence length},\nwidth=8cm,\nheight=6cm,\nevery axis plot/.append style={ultra thick}\n]\n\\addplot[smooth,bcyan,tension={0.15}]\ncoordinates{\n            (512,0.9)\n            (1024,2.0)\n            (2048,6.1)\n            (4096,20.6)\n            (8192,76.3)\n            (16384,291.4)\n            (32768,1160.3)\n            };\n\\addplot[smooth,bgreen,tension={0.15}]\ncoordinates{\n            (512,0.9)\n            (1024,1.3)\n            (2048,2.6)\n            (4096,5.2)\n            (8192,10.2)\n            (16384,20.6)\n            (32768, 40.8)\n            };\n\\addplot[smooth,borange,tension={0.15}]\ncoordinates{\n            (512,0.9)\n            (1024,1.8)\n            (2048,3.6)\n            (4096,7.0)\n            (8192,13.9)\n            (16384,27.8)\n            (32768, 54.5)\n            };\n\\addplot[smooth,bred,tension={0.15}]\ncoordinates{\n            (512,1.99)\n            (1024,3.96)\n            (2048,7.91)\n            (4096,15.71)\n            (8192,30.92)\n            (16384,61.79)\n            (32768, 122.55)\n            };\n\\legend{FlashAttention-2 (CUDA), FlashLinearAttention (\\textit{w/ m.} Triton),  FlashLinearAttention (\\textit{w/o m.} Triton), Chunkwise Linear Attention (Pytorch) }\n\\end{axis}  \n\\begin{axis}[%\nat={(plot1.south east) },\ntitle={Memory footprint},\nscatter/classes={%\n1={mark=diamond*,draw=black},\n0={mark=o,draw=black}},\nxmode=log,\nymode=log,\n    ylabel near ticks, yticklabel pos=right,\nlog basis x={2},\nlog basis y={2},\n    tick label style={font=\\large},\n    label style={font=\\large},\nylabel={GPU memory (GB)},\nxlabel={Sentence length},\n\t\t\tymajorgrids=true, %\n\t\t\txmajorgrids=true, %\n\t\t\tgrid style=dashed, %\nwidth=6cm,\nheight=6cm,\n    every axis plot/.append style={ultra thick}\n]\n\\addplot[smooth,bcyan,tension={0.15}]\ncoordinates{\n(1024, 0.6289067268371582)\n(2048, 1.2578129768371582)\n(4096, 2.515625476837158)\n};\n\\addplot[smooth,bgreen,tension={0.15}]\ncoordinates{\n(1024, 0.6875)\n(2048, 1.375)\n(4096, 2.75)\n};\n\\addplot[smooth,borange,tension={0.15}]\ncoordinates{\n(1024, 0.5625)\n(2048, 1.125)\n(4096, 2.25)\n};\n\\addplot[smooth,bred]\ncoordinates{\n(1024, 0.8125)\n(2048, 1.5625)\n(4096, 3.0625)\n};\n\\end{axis}  \n\\ref{mylegend}\n\\end{tikzpicture}   \n}\n\\vspace{-6mm}\n\\caption{Speed comparison on a single H100 GPU with batch size 32, number of heads 16, head dimension 64, and chunk size 64. Both x- and y-axes are on log scale. \\textit{w/ m.} and \\textit{w/o m.} denotes using \\textsc{FlashLinearAttention} \\textit{with} or \\textit{without materialization} of hidden states in HBM.}\\label{fig:flashlinearattn}\n\\end{figure}\n\n Figure \\ref{fig:flashlinearattn} shows the speed and memory footprint of our implementation. Both versions of \\textsc{FlashLinearAttention} are substantially faster than \\textsc{FlashAttention-2} \\cite{flashattention2} and a pure PyTorch (i.e., I/O-unaware) implementation of chunkwise linear attention, showing the benefits of I/O-awareness. \n\n\\vspace{-2mm}\n\\section{Gated Linear Attention}\n\\vspace{-2mm}\n\\label{sec:gla}\n\nThe linear recurrence in Eq.~\\ref{eq:simple_linear_attention} does not have a decay term or a forget gate, which has been shown to be crucial in RNNs~\\citep{HochSchm97,cho2014learning,unreasonable-forget-gate}. The lack of a decay term makes it difficult for a model to ``forget'' information, and has been hypothesized to be partially responsible for the instability of linear attention in long-context tasks~\\citep{buckman2024}. Recent works~\\citep{sun2023retentive,qin2023scaling} obtain better performance through incorporating a global, \\emph{non-data-dependent}  decay factor\\footnote{This can be viewed as linear attention with  ALiBi position encodings \\cite{alibi2021}.\nIn practice these works also incorporate  rotary position embeddings \\citep[RoPE;][]{rope}.}\n$\\gamma \\in (0, 1)$ into linear attention:\n$ \\rmS_t = \\gamma \\rmS_{t-1} + \\vk_t^\\intercal \\vv_t$.\nThe use of a single $\\gamma$ is designed to preserve the attention-style parallel form for efficient training. \nIn this work, we consider a data-dependent gating mechanism for linear attention. We show that despite having a more expressive gating factor, the resulting gated linear attention (GLA) layer still admits a hardware-efficient chunkwise form for efficient training.\n\n\\vspace{-2mm}\n\\subsection{Recurrent and Parallel Form of GLA}\n\\vspace{-2mm}\n\\label{ssec:gla}\n\\begin{table*}[t!]\n\\footnotesize\n\\centering\n\\scalebox{0.8}{\n\\begin{tabular}{l l l }\n\\toprule\n   Model  &  Parameterization  & Learnable parameters   \\\\\n   \\midrule\n    Mamba \\cite{Gu2023MambaLS} & $\\rmG_t = \\exp(-( \\mathbf{1}^{\\intercal}\\balpha_t)\\odot \\exp(\\mA)),\\quad \\balpha_t =\\operatorname{softplus}(\\vx_t\\mW_{\\alpha_1}\\mW_{\\alpha_2})$ & $\\mA \\in \\mathbb{R}^{d_k \\times d_v}, \\quad \\mW_{\\alpha_1} \\in \\mathbb{R}^{d \\times \\frac{d}{16}}, \\quad \\mW_{\\alpha_2} \\in  \\mathbb{R}^{\\frac{d}{16} \\times d_v}$ \\\\\n    Mamba-2 \\cite{mamba2} & $\\rmG_t = \\gamma_t \\mathbf{1}^\\intercal\\mathbf{1}, \\quad \\gamma_t = \\exp\\left(-\\operatorname{softplus}\\left(\\vx_t \\mW_{\\gamma}\\right)\\exp\\left(a\\right)\\right)$  &  $\\mW_{\\gamma}\n    \\in \\mathbb{R}^{d \\times 1}, \\quad a\\in \\mathbb{R}$ \\\\\n    mLSTM \\cite{beck2024xlstm, peng2021random} &  $\\rmG_t = \\gamma_t \\mathbf{1}^\\intercal\\mathbf{1}, \\quad \\gamma_t = \\sigma\\left(\\vx_t \\mW_{\\gamma}\\right)$ &  $\\mW_{\\gamma} \\in \\mathbb{R}^{d \\times 1}$ \\\\\n    Gated Retention \\cite{Sun2024YouOC} &\n  $\\rmG_t = \\gamma_t \\mathbf{1}^\\intercal\\mathbf{1}, \\quad \\gamma_t = \\sigma\\left(\\vx_t \\mW_{\\gamma}\\right)^{\\frac{1}{\\tau}}$ &  $\\mW_{\\gamma} \\in \\mathbb{R}^{d \\times 1}$ \\\\\n        DFW \\cite{mao-2022-fine,recurrent_linear_xfmr}\n        & $\\rmG_t = \\balpha_t^\\intercal \\bbeta_t, \\quad \\balpha_t = \\sigma(\\vx_t \\mW_{\\alpha}), \\quad \\bbeta_t = \\sigma(\\vx_t \\mW_{\\beta})$ & $ \\mW_{\\alpha} \\in \\mathbb{R}^{d \\times d_k}, \\quad \\mW_{\\beta} \\in \\mathbb{R}^{d \\times d_v}$   \\\\\n    GateLoop \\cite{gatedloop} & $\\rmG_t = \\balpha_t^\\intercal \\mathbf{1}, \\quad \\balpha_t = \\sigma\\left(\\vx_t\\mW_{\\alpha_1}\\right) \\exp\\left( \\vx_t\\mW_{\\alpha_2} \\mathbf{i} \\right) $ & $\\mW_{\\alpha_1} \\in \\mathbb{R}^{d\\times d_k}, \\quad \\mW_{\\alpha_2}\\in \\mathbb{R}^{d \\times d_k}$ \\\\\n  HGRN-2 \\cite{qin2024hgrn2} &  $\\rmG_t = \\balpha_t^\\intercal\\mathbf{1}, \\quad \\balpha_t = \\boldsymbol{\\gamma} + (1-\\boldsymbol{\\gamma} ) \\sigma(\\vx_t \\mW_{\\alpha})$   & $\\mW_{\\alpha} \\in \\mathbb{R}^{d \\times d_k}, \\quad \\boldsymbol{\\gamma}  \\in (0, 1)^{d_k}$\n  \\\\\n  RWKV-6 \\cite{peng2024eagle} & $\\rmG_t =\\balpha_t^\\intercal\\mathbf{1}, \\quad \\balpha_t = \\exp\\left(-\\exp\\left(\\vx_t\\mW_{\\alpha}\\right)\\right)$ & $\\mW_{\\alpha} \\in \\mathbb{R}^{d \\times d_k} $ \\\\\n  Gated Linear Attention (GLA) & $\\rmG_t = \\balpha_t^\\intercal \\mathbf{1}, \\quad \\balpha_t = \\sigma\\left(\\vx_t\\mW_{\\alpha_1}\\mW_{\\alpha_2}\\right)^{\\frac{1}{\\tau}}$ & $\\mW_{\\alpha_1} \\in \\mathbb{R}^{d\\times 16}, \\quad \\mW_{\\alpha_2}\\in \\mathbb{R}^{16 \\times d_k}$ \\\\\n  \\bottomrule\n\\end{tabular}\n}\n\\vspace{1mm}\n\\vspace{-4mm}\n\\caption{Gated linear attention formulation of recent  models, which vary in their parameterization of $\\rmG_t$. The bias terms are omitted.}\n\\vspace{-4mm}\n\\label{tab:G_param}\n\\end{table*}\n\\paragraph{Recurrent form.} GLA has a 2D forget gate \\(\\rmG_t \\in (0,1)^{d_k \\times d_v}\\) that varies over time:\n\\[\n\\rmS_t = \\rmG_t \\odot \\rmS_{t-1} + \\vk_t^{\\top} \\vv_t,\n\\]\nwhere we now allow the hidden state to have varying dimensions.\nThis Hadamard product-based  recurrent form is very general and encompasses many recent RNNs with 2D hidden states, as listed in Table~\\ref{tab:G_param}. \n\nCentral to the design of gated linear attention  is the parameterization of \\(\\rmG_t\\) which requires a balance between \\textit{parameter-efficiency}, \\textit{state size}, \nand \\textit{training efficiency}.\nA na\\\"{i}ve mapping $\\vx_t \\mapsto \\rmG_{t}$ to obtain a data-dependent gating matrix would require a matrix of size $d \\cdot d_k \\cdot d_v$, which would be parameter-inefficient. \\citet{mao-2022-fine} propose a more efficient outer-product-based low-rank parameterization (\\(\\rmG_t = \\balpha_t^{\\top} \\bbeta_t\\)), which requires $d \\cdot d_v + d  \\cdot d_k$ parameters.\\footnote{However, \\citet{mao-2022-fine} works with only the recurrent form and materializes the hidden states for all time steps in HBM. In Appendix \\ref{sec:ggla} we give a new algorithm that reformulates the model in a matrix-multiply-based parallel form, which can make use of (an extension of) \\textsc{FlashLinearAttention} for efficient training.}\n\nIn Mamba~\\cite{Gu2023MambaLS}, \\(\\rmG_t\\) is obtained by combining a  \\emph{data-independent} learnable matrix \\(\\mA\\) with a data-dependent vector $\\balpha_t$, which allows the matrix to be full rank. However, this prevents the use of tensor cores because it cannot be reformulated into a matrix-multiply format, as discussed in  \\citet{mamba2}. The lack of a compact matrix-multiply form  necessitates the materialization of each time step's hidden states. To reduce high I/O costs, \\citet{Gu2023MambaLS} develop a hardware-aware algorithm that materializes the hidden states {exclusively} in  SRAM rather than in HBM. Due to limited SRAM capacity, this approach cannot scale to larger hidden states, which, as we will show in our experiments, results in suboptimal performance on recall-intensive tasks. Mamba-2 \\cite{mamba2} addresses this limitation with a more {restricted} gating mechanism: \\(\\rmG_t = \\gamma_t \\mathbf{1}^T \\mathbf{1}\\), where \\(\\gamma_t \\in (0,1)\\) is a scalar, which makes it possible to to reformulate the recurrence in matrix-multiply form, enabling the use of tensor cores and larger  state sizes. This \\emph{scalar} data-dependent gating is also used in \\citet{peng2021random}, \\citet{Sun2024YouOC}, and \\citet{beck2024xlstm}. \n\n \n\n \n This paper adopts a middle ground between the scalar and the fully low-rank parameterization by using $\\rmG_{t} = \\balpha_t ^\\top \\mathbf{1}$.\\footnote{Our preliminary experiments with the $\\rmG_t = \\balpha_t^\\top \\bbeta_t$ parameterization resulted in only marginal improvements over $\\rmG_{t} = \\balpha_t ^\\top \\mathbf{1}$.}\nThis results in the following recurrent form,\n\\begin{align}\n\\rmS_t =  (\\balpha_t^{\\top} \\mathbf{1}) \\odot \\rmS_{t-1} + \\vk_t^{\\top} \\vv_t = \\text{Diag}(\\balpha_t)  \\rmS_{t-1} + \\vk_t^{\\top} \\vv_t, \n\\label{eq:gla-simple}\n\\end{align}\nwhere $\\balpha_t$ is parameterized via a low-rank linear layer followed by sigmoid on $\\vx_t$ (see \\S\\ref{sec:gla-full}). Note that the above formulation \n is general and  encompasses several recent RNNs \\cite{gatedloop, qin2024hgrn2, peng2024eagle}. Thus, the hardware-efficient GLA implementation (described next) could be directly used or adapted to other models. \n\n\\vspace{-2mm}\n\\paragraph{Parallel form.} \nWe now describe a parallel form GLA for parallelizing across sequence length. Unrolling Eq.~\\ref{eq:gla-simple} gives\n\\begin{equation*}\n\\rmS_t = \\sum_{i=1}^{t} \\left(\\left(\\prod_{j=i+1}^{t}\\balpha_{j}^{\\top}\\mathbf{1}\\right) \\odot \\vk_i^{\\top} \\vv_i \\right)    \n\\label{eq:gla_parallel}\n\\end{equation*}\nLetting $\\vb_t := \\prod_{j=1}^{t} \\balpha_j$, we can rewrite the above as \n\\begin{align*}\n\\vo_t = \\vq_t \\rmS_t &= \\vq_t \\sum_{i=1}^{t} \\left( \\left(\\frac{\\vb_t}{\\vb_i} \\right)^{\\top} \\mathbf{1} \\right) \\odot \\vk_i^{\\top} \\vv_i \\\\ &= \\sum_{i=1}^{t}(\\vq_t \\odot \\vb_t) \\left(\\frac{\\vk_i}{\\vb_i}\\right)^{\\top} \\vv_i\n\\end{align*}\nwhere the division is element-wise. Letting $\\rmB \\in (0,1)^{L \\times d}$ be the matrix obtained from stacking $\\vb_t$'s,  the parallel form is:\n\\begin{equation*}\n\\rmO = \\left(\\left(\\underbrace{(\\rmQ \\odot \\rmB) \\left(\\frac{\\rmK}{\\rmB}\\right)^{\\top}}_{\\mathbf{P}} \\right) \\odot \\rmM \\right) \\rmV.\n\\label{eq:parallel_gla}\n\\end{equation*}\nHowever, this form is not numerical stable as $\\vb_t$  is the cumulative product of gate values in $\\balpha_j \\in (0, 1)^{1 \\times d}$, and thus can be extremely small when $t$ is large, making $ \\frac{\\mathbf{K}}{\\rmB}$ explode. To handle this, we can compute in log space for $\\mathbf{P}$,\\footnote{This form resembles extrapolatable position encoding~\\citep{xpos} in that the term inside the exponential can be viewed as a  \\textit{data-dependent} relative position factor. }\n\\begin{align}\n    \\vspace{-6mm}\n    \\mathbf{P}_{ij} = \\sum_{k=1}^{d} \\mathbf{Q}_{ik} \\mathbf{K}_{jk} \\, \\exp(\\log \\rmB_{ik}-\\log\\rmB_{jk}), \\quad i \\ge j,\n    \\label{eq:log-semiring}\n    \\vspace{-6mm}\n\\end{align}\nwhere $k$ denotes feature indices. \nHowever, unlike vanilla linear attention, as Eq. \\ref{eq:log-semiring} cannot be represented via a standard matmul, and it cannot make use of half-precision matmuls on tensor cores. We will show in \\S\\ref{subsec:flash-gla} how a secondary-level chunking mechanism can enable the use of half-precision matmuls for most computations while maintaining numerical stability, as illustrated in Figure~\\ref{fig:gla_tiling}.\n\n\\def\\width{16}\n\\def\\hauteur{16}\n\n\\definecolor{orange}{RGB}{250,230,200}\n\\definecolor{gray}{RGB}{220,220,220}\n\\definecolor{pink}{RGB}{247,206,205}\n\n\\begin{figure}[t]\n\\vspace{-2mm}\n    \\centering\n        \\begin{tikzpicture}[x=0.2cm, y=0.2cm]\n    \n\n    \\foreach \\y in {0, ..., 15}{\n        \\foreach \\x in {0, ...,  15} {%\n            \\pgfmathsetmacro{\\col}{ifthenelse(\\x<15-\\y,\"orange\",ifthenelse(\\x==\\y+1,\"white\",\"white\"))}\n       \\fill[\\col] (\\x,\\y) rectangle (\\x+1,\\y+1);\n       }\n    }\n    \\foreach \\y in {0, ..., 15}{\n       \\fill[pink] (\\y,15-\\y) rectangle (\\y+1,15-\\y+1);\n       }\n    \n   \n    \\draw[step=0.20cm, line width=0.05mm, black!15!white, fill=orange] (0,0) grid (\\width,\\hauteur);\n    \\foreach \\x in {0, ..., 8} {%\n    \\fill[gray] (\\x,0) rectangle (\\x+4,4);\n    }\n        \\foreach \\x in {0, ..., 4} {%\n    \\fill[gray] (\\x,4) rectangle (\\x+4,8);\n    }\n    \\foreach \\x in {0, ..., 1} {%\n    \\fill[gray] (\\x,8) rectangle (\\x+3,12);\n    }\n    \n    \\foreach \\x in {0, ..., 1} {%\n    \\fill[gray] (\\x,8) rectangle (\\x+3,12);\n    }\n\n    \\draw[step=0.8cm, line width=0.1mm, black!10!black, fill=orange] (0,0) grid (\\width,\\hauteur);\n\n      \n        \\node \n        [shape=rectangle,fill=white, \n        align=center](table2) at (28.0,7.0) {\n\n            \\begin{tabular}{lcc} \\toprule\n                 & level & tensor core   \\\\ \n            \\begin{tikz}\n                \\node [draw,fill=gray] {};\n            \\end{tikz} & 1 & $\\checkmark$\\\\\n                \\begin{tikz}\n                \\node [draw,fill=orange] {}; \n            \\end{tikz} & 2 & $\\checkmark$\\\\\n             \\begin{tikz}\n                \\node [draw,fill=pink] {}; \n            \\end{tikz} & 2 & $\\text{\\xmark}$\\\\\n                 \\begin{tikz}\n                \\node [draw,fill=white] {}; \n            \\end{tikz} & \\multicolumn{2}{ c}{causal mask}\\\\\n                \\bottomrule\n            \\end{tabular}\n        };\n\n    \\end{tikzpicture}\n\\vspace{-2mm}\n    \\caption{\n    Attention-style map to illustrate the chunkwise computations in GLA. The inter-chunk dependencies (in gray) \n    are not directly computed in the chunkwise form (only computed in the parallel form).\n    The intra-chunk dependencies are modeled via secondary chunking/tiling where the inter-sub-chunk part (in orange) is computed by half-precision matmuls while the intra-sub-chunk part (in pink) is computed in full precision in log space. \n    }\n    \\label{fig:gla_tiling}\n\n\\end{figure}\n\n\\vspace{-2mm}\n\\subsection{Chunkwise Parallel Form of GLA}\n\\vspace{-2mm}\nWe derive a chunkwise form of GLA similar to the chunkwise form of basic linear attention (\\S\\ref{background:lin-chunkwise}). Here the intra-chunk operation implements the above parallel form at the chunk-level to obtain $\\rmO^{\\text{intra}}$. For inter-chunk, we have\n\\begin{align*}\n\\mathbf{\\Lambda}_{iC+j} &= \\frac{\\vb_{iC+j}}{\\vb_{iC}},  \\mathbf{\\Gamma}_{iC+j} = \\frac{\\vb_{(i+1)C}}{\\vb_{iC+j}}, \n\\bgamma_{i+1} = \\frac{\\vb_{(i+1)C}}{\\vb_{iC}},  \n \\\\\n\\rmS_{[i+1]} &= \\left(\\bgamma_{i+1}^{\\top}\\mathbf{1}\\right)  \\odot \\rmS_{[i]} + \\left(\\rmK_{[i+1]} \\odot \\mathbf \\Gamma_{[i+1]} \\right)^{\\top}\n\\rmV_{[i+1]},  \\\\\n\\rmO^{\\text{inter}}_{[i+1]} &= \\left(\\rmQ_{[i+1]} \\odot \\mathbf \\Lambda_{[i+1]}\\right) \\rmS_{[i]}. \n\\end{align*}\nIntuitively, $\\mathbf \\Lambda_{[i+1]} $ encodes the cumulative decay from the start of a chunk which will be used to propagate the hidden states from the previous chunk $\\rmS_{[i]}$, while   $\\mathbf \\Gamma_{[i+1]}$ encodes the decay to the end of a chunk which will be used to accumulate information to be added to the next hidden state $\\rmS_{[i+1]}$.\n\\vspace{-2mm}\n\\subsection{Hardware-Efficient GLA}\n\\vspace{-2mm}\n\\label{subsec:flash-gla}\nWith the chunkwise form in hand, we can adapt the \\textsc{FlashLinear Attention} algorithm presented in \\S\\ref{sec:algorithm} to the gated case. The adaptation additionally relies on two crucial techniques described below. \nWe give high-level intuitions in this section\nand defer the full algorithms to Alg. \\ref{algo:gla-chunk-scan-1-fwd}-\\ref{algo:gla-chunk-scan-2-bwd} of Appendix~\\ref{sec:gla_algo}.\n\n\\vspace{-2mm}\n\n\\paragraph{Secondary-level chunking.}\nUnlike in ordinary linear attention, the intra-chunk computations in GLA {cannot} leverage half-precision matmuls (and thus tensor cores) due to log space computations (Eq.~\\ref{eq:log-semiring}). To make better use of tensor cores, we use secondary-level chunking scheme, where a chunk is further divided into sub-chunks (i.e., another level of tiling) in the spirit of classic tiling techniques~\\cite{flashattention1}. The  attention-like matrix $\\rmP \\in \\R^{L\\times L}$ is then computed in a chunkwise manner, as illustrated in Figure~\\ref{fig:gla_tiling}. \n Concretely, the interactions  between sub-chunks are computed via half-precision matmuls,\\footnote{To reduce notational clutter, here we use the notations from the first-level chunking to express the key idea. The actual implementation is done with secondary-level chunks.}\n\\begin{align*}\n \\rmP_{[i][j]} &=  \\Big(\\rmQ_{[i]} \\odot \\bLambda_{[i]} \\Big) \\Big(\\rmK_{[j]} \\odot \\bGamma_{[j]} \\odot  \n \\frac{\\vb_{iC}}{\\vb_{(j+1)C}} \\Big)^{\\intercal} \\in \\mathbb{R}^{C\\times C}.\n \\label{eq:gla-subchunk-compute-p}\n\\end{align*}\nThis corresponds to the orange tiles in Figure~\\ref{fig:gla_tiling}. For the intra-sub-chunk part (pink tiles in Figure~\\ref{fig:gla_tiling}) we have to resort to Eq.~\\ref{eq:log-semiring} and perform the matmul in full precision for stability.\nWith this two-level tiling strategy, the total amount of non-half-precision matmul FLOPs are greatly reduced, thus leading to wallclock improvements. We provide the Pytorch-style pseudo-code in Listing~\\ref{a} of Appendix~\\ref{sec:gla_algo}.\n\n\\vspace{-2mm}\n\\paragraph{Memory-efficient  \\textnormal{$\\dbalpha_t$} computation.} \nPast work \\citep[\\S 3.1]{mao-2022-fine} has claimed that GLA-like models have to materialize the matrix-valued hidden states of size $L \\times d \\times d$ in HBM to compute all the gradients $\\dbalpha_t$, since $\\dbalpha_t =  (\\rmS_{t-1} \\odot \\mathbf d\\rmS_{t}) \\mathbf{1}$. \nWe instead give the following \\emph{closed form} formula for $\\dblogalpha_t$, \n\\begin{align*}\n    \\vdlogb_t &= \\vq_t \\odot \\vdq_t - \\vk_t \\odot \\vdk_t,  \\hspace{4mm}\n   \\dblogalpha_t  = \\sum_{t \\leq i \\leq L} \\vdlogb_i,\n\\end{align*}\nwhich can be easily obtained by taking the derivative with respect to \n Eq.~\\ref{eq:log-semiring} (see Appendix~\\ref{sec:gla_algo} for full derivation). $\\vdq_t$ and $\\vdk_t$ can be  computed  as in Alg.~\\ref{algo:la-chunk-bwd}.\n \n\\vspace{-2mm}\n\\subsection{GLA Transformer}\n\\vspace{-2mm}\n\\label{sec:gla-full}\nWe generalize the GLA layer to the multi-head case. Given $H$ heads, we have the following for each head $h \\in [1, H]$,\n\\begin{align*}\n    &\\rmS^h_{t} =  \\left( \\left(\\balpha_{t}^h\\right)^{\\top} \\mathbf{1} \\right)\n    \\odot \\rmS_{t-1}^h + \\vk_t^{h \\intercal} \\, \\vv^h_t   \\in \\mathbb{R}^{d'_k \\times d'_v}, \\\\  %\n& \\vo^h_{t} = \\vq_t^{h}\\rmS_t^h    \\in \\mathbb{R}^{1 \\times d'_v}, \\\\ &\n\\vo'_t = \\operatorname{concat}(\\lnorm(\\vo^1_t), \\dots, \\lnorm(\\vo^H_t)) \\in \\R^{1 \\times d_v}, \n  \\\\\n &\\vr_t = \\operatorname{Swish}(\\vx_t \\mW_r + \\vb_r)  \\in \\mathbb{R}^{1 \\times d_v}, \n \\\\ & \\vy_t = (\\vr_t \\odot \\vo'_t) \\mW_{O}  \\in \\R^{1 \\times d}. \n\\end{align*}\nHere we use separate key ($d_k$) and value ($d_v$) dimensions; $d'_k = d_k / H, d'_v = d_v / H$ are the per-head key/value dimensions. LayerNorm ($\\lnorm$) is applied after the output of each head, while the output projection and output gating operate on the concatenation of head outputs \\cite{sun2023retentive}. \n\nWe then build up a Transformer-like model by interleaving multi-head GLA layers with feed-forward networks (FFN). Concretely, given layer $l$'s contextualized representation $\\rmX^{(l)}$, we obtain  $\\rmX^{(l+1)}$ via,\n\\begin{align*}\n   &\\rmY^{(l)} = \\operatorname{GLA}(\\lnorm(\\rmX^{(l)})) + \\rmX^{(l)} \\\\ \n   & \\rmX^{(l+1)} = \\operatorname{SwiGLU}(\\lnorm(\\rmY^{(l)})) + \\rmX^{(l)},\n\\end{align*}\nwhere the $\\swiglu$ FFN layer~\\citep{touvron2023llama} is,\n\\begin{align*}\n    \\operatorname{SwiGLU}(\\rmZ) = (\\operatorname{Swish}(\\rmZ \\mW_1) \\odot \\rmZ \\mW_2) \\mW_{3}.\n\\end{align*}\n\n\\begin{table*}[t!]\n\\centering\n\\small\n\\addtolength{\\tabcolsep}{-2.5pt}    \n\\begin{tabular}{l l|cc|cccccc|c}\n\\toprule\n&   & \\textbf{Wiki.}  &  \\textbf{LMB.} &  \\textbf{LMB.} & \\textbf{PIQA} &    \\textbf{Hella.} & \\textbf{Wino.} & \\textbf{ARC-e} &  \\textbf{ARC-c} &  \\textbf{Avg.}  \\\\\n\\textbf{Scale} & \\textbf{Model}  & ppl $\\downarrow$  &  ppl $\\downarrow$  &  acc $\\uparrow$  & acc $\\uparrow$ &   acc\\_norm $\\uparrow$  & acc $\\uparrow$  & acc $\\uparrow$ & acc\\_norm $\\uparrow$ &  $\\uparrow$ \\\\\n\\midrule\n\\textit{340M Params} &  Transformer++ & \\text{28.39} & 42.69 & \\text{31.0} & 63.3 & 34.0 &\t50.4 & 44.5\t& \\text{24.2} &  41.2  \\\\\n\\textit{15B Tokens} &   RetNet  & 32.33 &\t49.19 &\t28.6 &\t63.5 & 33.5 &\t\\text{52.5} &\t44.5 &\t23.4 &\t 41.0 \\\\\n&   Mamba  & \\text{28.39} & \\text{39.66} & 30.6 & \\text{65.0} & \\text{35.4} & 50.1 & \\text{46.3} & 23.6 & \\text{41.8} \\\\ \n&   GLA   & 28.65 &\t43.35 &\t30.3 &\t64.8  & 34.5 & 51.4  & 45.1\t & 22.7 &\t 41.5\\\\\n\\midrule \n \\textit{1.3B Params} &  Transformer++ & \\text{16.85} & \\text{13.44} &  \\text{48.9} & 70.8 & 49.6 & 53.6 & 56.0 & 26.5 & 50.9 \\\\\n\\textit{100B Tokens}&   RetNet  & 18.64 & 17.27 & 43.3 & 70.0 & 47.3 & 52.5 & 54.8 & 25.6 & 48.9 \\\\\n&  Mamba  & 17.06 & 13.89 & 46.2 & \\text{72.2} &  40.1 & \\text{54.1} &  \\text{59.0} &\t\\text{28.2} & 50.0 \\\\\n&  GLA  & 17.22 & 14.47 & 46.9 & 71.8 & \\text{49.8} & 53.9 & 57.2 & 26.6 & \\text{51.0}  \\\\\n\\bottomrule\n\\end{tabular}\n\\addtolength{\\tabcolsep}{2.5pt}    \n\\centering\n\\vspace{-2mm}\n\\caption{GLA Transformer results against Transformer++ \\citep{touvron2023llama}, RetNet \\citep{sun2023retentive}, and Mamba \\citep{Gu2023MambaLS}. All models are trained on the same subset of the SlimPajama dataset with the Mistral tokenizer. The 340M/1.3B models are trained for 15B/100B tokens respectively. The individual task performance is via zero-shot. We report the main results on the same set of tasks reported by \\citet{Gu2023MambaLS}. See Appendix~\\ref{app:d} for results on other benchmarks, including 5-shot results. The last column shows the average over all benchmarks that use (normalized) accuracy as the metric.}\n\\vspace{-3mm}\n\\label{tab:main_results}\n\\end{table*}\n\\vspace{-2mm}\n\\paragraph{Parameter allocation.}  As presented, our GLA layer employs two additional matrices  for predicting $\\balpha_t, \\vr_t$ (i.e., $\\mW_\\alpha, \\mW_r$) compared to a regular softmax attention layer. %\nFor parameter-efficiency, we use a low-rank parameterization \n\\begin{align*}\n   \\balpha_t = \\sigma((\\vx_t \\mW^1_\\alpha \\mW^2_\\alpha + \\vb_\\alpha)))^{\\frac{1}{\\tau}}   \\in \\R^{1 \\times d_k},\n\\end{align*}\nwhere $\\mW^1_\\alpha \\in \\R^{d \\times 16}$, $\\mW^2_\\alpha \\in \\R^{16 \\times d_k}$, and $\\tau = 16$ is a temperature term to encourage model to have a slower forgetting rate. \nWe further set $d_k = \\frac{d}{2}$ and $d_v = d$ and use  full-rank parameterizations for ($\\mW_Q, \\mW_K, \\mW_V, \\mW_O, \\mW_r$). \nUltimately, one GLA layer collectively needs (roughly) $4d^2$ parameters, as in regular softmax attention. \n\n\\vspace{-2mm}\n\\section{Empirical Study}\n\\vspace{-2mm}\n\\label{sec:experiments}\n\n\\subsection{Experimental Setup}\n\\vspace{-2mm}\nOur main experiments are on language modeling, where we study whether  GLA  can perform competitively against a (i) strong Transformer baseline with modern architectural recipes and (ii) recent linear-time models. We use the SlimPajama dataset~\\citep{cerebras2023slimpajama} and tokenize it using the Mistral tokenizer~\\citep{jiang2023mistral}. The original dataset contains 627B tokens; we use a 100B subset.\n\n\\vspace{-2mm}\n\\paragraph{Baselines.} We evaluate  GLA  against three baselines: Transformer++ \\citep{touvron2023llama}, RetNet \\citep{sun2023retentive}, and Mamba \\citep{Gu2023MambaLS}. \nTransformer++ is the LLaMA  architecture with Rotary Positional Embeddings~\\citep{rope}, SWiGLU \\citep{shazeer2020glu}, and RMSNorm~\\citep{rmsnorm}; we also use SwiGLU in the RetNet to replace its original FFN for fair comparison. For Mamba, we use the open source code. All our baselines are trained for the exact same number of tokens on the same dataset for fair comparison.\n\n\\vspace{-2mm}\n\\paragraph{Training details.} We train all models from scratch at two scales: 340M and 1.3B.  All models are trained with AdamW~\\citep{loshchilov2018fixing} using a maximum learning rate of 3e-4. The 340M models are trained on 15B tokens with a batch size of 0.5M tokens, while the 1.3B models are trained on 100B tokens with a batch size of 2M tokens. We use a cosine learning rate schedule with a warmup of 0.5B/1B tokens for the 340M/1.3B settings, respectively. The initial and final learning rates are 3e-5. We use a weight decay of 0.01, and gradient clipping of 1.0.\n\n\\vspace{-2mm}\n\\subsection{Main Results}\n\\vspace{-2mm}\n\nIn addition to perplexity (ppl) on Wikitext (Wiki.), we consider a wide range of downstream tasks covering common-sense reasoning and question-answering as was used in \\citet{Gu2023MambaLS}: LAMBADA~\\citep[LMB.; ][]{paperno2016lambada}, PiQA~\\citep{bisk2020piqa}, HellaSwag~\\citep[Hella.; ][]{zellers2019hellaswag}, WinoGrande~\\citep[Wino.;][]{sakaguchi2021winogrande}, ARC-easy (ARC-e) and ARC-challenge (Arc-c) \\citep{arc-ce}.\nIn Appendix~\\ref{app:d}, we also include results on additional tasks: Copa~\\citep{copa}, SciQA~\\citep{SciQA2023}, OpenbookQA~\\citep{openbookqa}, BoolQA~\\citep{clark2019boolq}. We report perplexity (ppl) on WikiText and LAMBADA, accuracy normalized by length on\nHellaSwag, ARC-challenge and OpenbookQA, and accuracy on the other tasks. All evaluations are performed using the LM evaluation harness~\\citep{eval-harness}.\n\n\\definecolor{color1}{RGB}{116, 184, 22}\n\\definecolor{color2}{RGB}{77, 171, 247}\n\\definecolor{color3}{RGB}{99, 230, 190}\n\\definecolor{color4}{RGB}{132, 94, 247}\n\\definecolor{color5}{RGB}{250, 176, 5}\n\t\n\t\\begin{figure}[t] %\n     \\vspace{-4mm}\n\t\t\\centering %\n\t\t\\resizebox{0.9 \\columnwidth}{!}{  %\n\t\t\t\\begin{tikzpicture} %\n\t\t\t\\scalefont{1.2} %\n\t\t\t\\begin{axis}[\n                name=plot1,\n\t\t\tsharp plot, %\n        title style={align=right}, title={Sequence Length: 256\\\\Key-Value Pairs: 16},\n\t\t\txmode=normal,%\n\t\t\txlabel=Model dimension, %\n\t\t\tylabel=Accuracy, %\n\t\t\twidth=6cm, height=5cm,  %\n\t\t\tymin=0, ymax=100,  %\n                symbolic x coords={64,128,256,512},\n\t\t\tytick={25,50,75,100}, %\n\t\t\txlabel near ticks, %\n\t\t\tylabel near ticks, %\n\t\t\tymajorgrids=true, %\n                xmajorgrids=true,\n\t\t\tgrid style=dashed, %\n\t\t\t]\n\t\t\t\\addplot+[very thick,mark=x,mark options={scale=0.5}, color=color1] plot coordinates { \n\t\t\t\t(64,100)\n\t\t\t\t(128,100)\n\t\t\t\t(256,100)\n\t\t\t\t(512,100)\n\t\t\t};\n\t\t\t\n\t\t\t\\addplot+[very thick, mark=square*, mark options={scale=1}, color=color2] plot coordinates {\n\t\t\t\t(64,99)\n\t\t\t\t(128,99)\n\t\t\t\t(256,99)\n\t\t\t\t(512,99)\n\t\t\t};\n\t\t\t\n\t\t\t\\addplot+[very thick,mark=star,mark options={scale=1}, color=color3] plot coordinates {\n\t\t\t\t(64,97)\n\t\t\t\t(128,100)\n\t\t\t\t(256,100)\n\t\t\t\t(512,100)\n\t\t\t};\n   \t\t\t\\addplot+[very thick,mark=+,mark options={scale=1}, color=color4] plot coordinates {\n\t\t\t\t(64,0)\n\t\t\t\t(128,0)\n\t\t\t\t(256,70)\n\t\t\t\t(512,78)\n\t\t\t};\n\n\t\t\t\\addplot+[very thick,mark=o,mark options={scale=1}, color=color5] plot coordinates {\n\t\t\t\t(64,0)\n\t\t\t\t(128,12)\n\t\t\t\t(256,50)\n\t\t\t\t(512,70)\n\t\t\t};\n\n\t\t\t\\end{axis}\n\t\t\t\\begin{axis}[\n                name=plot2,\n                at={(plot1.south east) },\n\t\t\tsharp plot, %\n        title style={align=right}, title={Sequence Length: 512\\\\Key-Value Pairs: 64},\n\t\t\txmode=normal,%\n\t\t\txlabel=Model dimension, %\n\t\t\twidth=6cm, height=5cm,  %\n\t\t\tymin=0, ymax=100,  %\n                symbolic x coords={64,128,256,512},\n   \t\t\tytick={0,25,50,75,100}, %\n         yticklabels={,,},\n\t\t\txlabel near ticks, %\n\t\t\tymajorgrids=true, %\n\t\t\txmajorgrids=true, %\n\t\t\tgrid style=dashed, %\n\t\t\tlegend style={at={(1.6,0.5)},anchor=east, legend cell align=left, font=\\small}, \n\t\t\t]\n\t\t\t\n\t\t\t\\addplot+[very thick,mark=+,mark options={scale=1}, color=color1] plot coordinates { \n\t\t\t\t(64,81)\n\t\t\t\t(128,100)\n\t\t\t\t(256,100)\n\t\t\t\t(512,100)\n\t\t\t};\n\t\t\t\\addlegendentry{Mamba} \n\t\t\t\n\t\t\t\\addplot+[very thick, mark=square*, mark options={scale=1}, color=color2] plot coordinates {\n\t\t\t\t(64,22)\n\t\t\t\t(128,95)\n\t\t\t\t(256,100)\n\t\t\t\t(512,100)\n\t\t\t};\n\t\t\t\\addlegendentry{GLA} \n\t\t\t\n\n\t\t\t\\addplot+[very thick, mark=star, mark options={scale=1}, color=color3] plot coordinates {\n\t\t\t\t(64,5)\n\t\t\t\t(128,87)\n\t\t\t\t(256,96)\n\t\t\t\t(512,97)\n\t\t\t};\n\t\t\t\\addlegendentry{RetNet}  \n\n\t\t\t\\addplot+[very thick, mark=+, mark options={scale=1}, color=color4] plot coordinates {\n\t\t\t\t(64,0)\n\t\t\t\t(128,0)\n\t\t\t\t(256,20)\n\t\t\t\t(512,35)\n\t\t\t};\n\t\t\t\\addlegendentry{RWKV-4}  \n\n\t\t\\addplot+[very thick, mark=o, mark options={scale=1}, color=color5] plot coordinates {\n\t\t\t\t(64,0)\n\t\t\t\t(128,0)\n\t\t\t\t(256,12)\n\t\t\t\t(512,20)\n\t\t\t};\n\t\t\t\\addlegendentry{Hyena} \n\n\t\t\t\\end{axis}\n\n\t\t\t\\end{tikzpicture}\n\t\t}\n     \\vspace{-3mm}\n\t\t\\caption{Accuracy (\\%) on the synthetic MQAR task.} \n\t\t\\label{fig:mqar} \n\n\t\\end{figure}\n\n\\definecolor{color1}{RGB}{77, 171, 247}\n\\definecolor{color2}{RGB}{132, 94, 247}\n\\definecolor{color3}{RGB}{255, 135, 135}\n\\definecolor{color4}{RGB}{11, 114, 133}\n\\definecolor{color5}{RGB}{59, 201, 219}\n\\definecolor{color6}{RGB}{132, 94, 247}\n\n\t\n\\begin{figure*}[t] %\n     \\vspace{-3mm}\n    \\centering %\n    \\resizebox{0.85\\linewidth}{!}{  %\n\\begin{tikzpicture} %\n        \\scalefont{1.0} %\n        \\begin{axis}[\n            name=plot1,\n        sharp plot, %\n        title style={align=right}, title={\n        PG19}\n        ,\n\t\t\txmode=normal,%\n\t\t\txlabel=Position bucket (K), %\n\t\t\tylabel=Perplexity, %\n\t\t\twidth=9cm, height=6cm,  %\n\t\t\tymin=10,\n               ymax=25,  %\n\t\t\txlabel near ticks, %\n\t\t\tylabel near ticks, %\n\t\t\tymajorgrids=true, %\n                xmajorgrids=true,\n\t\t\tgrid style=dashed, %\n\t\t\t]\n\n\\addplot+[very thick,mark=o,mark options={scale=1}, color=color6] plot coordinates { \n(2, 11.09)\n(4, 18.31)\n(6, 64.2)\n};\n\n\\addplot+[very thick,mark=o,mark options={scale=1}, color=color1] plot coordinates { \n(2, 12.334416871694545)\n(4, 16.50338878264782)\n(6, 15.665969802718664)\n(8, 15.775098093939631)\n(10, 16.28891258448325)\n(12, 15.237502382790032)\n(14, 15.371704482324168)\n(16, 14.94472773435605)\n(18, 15.426467447568902)\n(20, 16.095521862334422)\n(22, 16.089831898143757)\n(24, 13.870173108390853)\n(26, 16.652435886300694)\n(28, 17.05460165670472)\n(30, 19.899968051899997)\n};\n\t\t\t\n\\addplot+[very thick, mark=square*, mark options={scale=1}, color=color1] plot coordinates {\n(2, 12.272513995002527)\n(4, 15.915017718544624)\n(6, 15.12648400208903)\n(8, 15.126682357304789)\n(10, 15.707573736739828)\n(12, 14.711491336539076)\n(14, 14.941795590117444)\n(16, 14.499628795410947)\n(18, 14.852281588773375)\n(20, 15.52356082374071)\n(22, 15.57131917875397)\n(24, 13.332259116220783)\n(26, 16.26334764613957)\n(28, 16.90620689149074)\n(30, 19.575600800473826)\t\n};\n\n\\addplot+[very thick,mark=star,mark options={scale=1}, color=color3] plot coordinates {\n(2, 11.513764074464719)\n(4, 14.821545167608454)\n(6, 14.27181440333812)\n(8, 14.745159645587211)\n(10, 15.277090611163125)\n(12, 14.503881505445126)\n(14, 14.867072874125807)\n(16, 14.785802233682734)\n(18, 15.075549715264373)\n(20, 15.74728691109622)\n(22, 15.796882231799712)\n(24, 13.665838224478483)\n(26, 16.75761700477185)\n(28, 17.122188320649027)\n(30, 19.772022829026763)\t\t\t\n};\n\n\\addplot+[very thick,mark=square*,mark options={scale=1}, color=color3] plot coordinates {\n(2, 11.420266254807174)\n(4, 14.647489438851771)\n(6, 13.602399098526789)\n(8, 13.702136965988363)\n(10, 13.94407030138506)\n(12, 13.096784138731092)\n(14, 13.358225399949397)\n(16, 13.069880104168242)\n(18, 13.369938037195737)\n(20, 13.859198521307533)\n(22, 13.75584104547106)\n(24, 12.071451636850792)\n(26, 14.451484019606044)\n(28, 15.095705584933034)\n(30, 17.893014429053675)\t\n};\n\n\\addplot+[very thick,mark=star,mark options={scale=1}, color=color5] plot coordinates {\n(2, 11.688854804956376)\n(4, 14.943035357330759)\n(6, 14.119485985291082)\n(8, 14.300316332465233)\n(10, 14.748137591710703)\n(12, 13.762720187004728)\n(14, 14.227475231817666)\n(16, 14.04244932822711)\n(18, 14.34961326894769)\n(20, 15.074579287890959)\n(22, 15.446429587912565)\n(24, 13.412177117829918)\n(26, 16.474816040581416)\n(28, 17.556498322948684)\n(30, 21.106791888790198)\t\n};\n\n\\addplot+[very thick,mark=square*,mark options={scale=1}, color=color5] plot coordinates {\n(2, 11.7457580111041)\n(4, 14.920393773306506)\n(6, 13.791731057875367)\n(8, 14.025104116895617)\n(10, 14.419469470911702)\n(12, 13.408896666345553)\n(14, 13.464048125083734)\n(16, 13.417051316705868)\n(18, 13.58252067910424)\n(20, 14.157938178864061)\n(22, 14.239108084631486)\n(24, 12.368099123010358)\n(26, 14.665233740423941)\n(28, 15.31096912196873)\n(30, 18.632639168708696)\n};\n\n\\addplot+[very thick, mark= mark options={scale=1},  color=color4] plot coordinates {\n(2, 11.872848224059455)\n(4, 14.972456535259505)\n(6, 13.906079604037489)\n(8, 13.917668674699033)\n(10, 14.450326377207194)\n(12, 13.422948137513114)\n(14, 13.644676392922488)\n(16, 13.46075819589147)\n(18, 13.653410626666675)\n(20, 14.103521696916971)\n(22, 14.265809961807271)\n(24, 12.348204946403571)\n(26, 14.846425830412182)\n(28, 15.410869769068253)\n(30, 18.799468389849164)\n};\n\n\\end{axis}\n\n\\begin{axis}[\n            name=plot2,\n            at={(plot1.south east) },\n        sharp plot, %\n        title style={align=right}, title={\n        SlimPajama}\n        ,\n\t\t\txmode=normal,%\n\t\t\txlabel=Position bucket (K), %\n      ylabel near ticks, yticklabel pos=right,\n\t\t\twidth=9cm, height=6cm,  %\n   ymax=10,  %\n   ymin=6,\n\t\t\txlabel near ticks, %\n\t\t\tylabel near ticks, %\n\t\t\tymajorgrids=true, %\n                xmajorgrids=true,\n\t\t\tgrid style=dashed, %\n   \t\t\tlegend style={at={(1.5,0.5)},anchor=east,font=\\small}, \n                legend cell align=left]\n\n\t\t\t\\addplot+[very thick,mark=o,mark options={scale=1}, color=color6] plot coordinates { \n(2, 9.11)\n(4, 11.77)\n};\t\t\t\n\\addlegendentry{Transformer++ (2K)} ;\n\n\t\t\t\\addplot+[very thick,mark=o,mark options={scale=1}, color=color1] plot coordinates { \n(2, 9.703324579039524)\n(4, 8.851140172053045)\n(6, 8.64224978739121)\n(8, 8.523894072608067)\n(10, 8.337787326052817)\n(12, 7.993996058951587)\n(14, 7.807283256053823)\n(16, 7.508849714252663)\n(18, 7.450385184846358)\n(20, 7.696067587961978)\t\t\t};\n\t\\addlegendentry{RetNet (2K)};  \n\t\t\n\t\t\t\\addplot+[very thick, mark=square*, mark options={scale=1}, color=color1] plot coordinates {\n(2, 9.60288660871554)\n(4, 8.47661116643274)\n(6, 8.294459081541115)\n(8, 8.138966746337)\n(10, 7.979737282823978)\n(12, 7.65726433276238)\n(14, 7.470669790601427)\n(16, 7.187401744904839)\n(18, 7.1132779884136905)\n(20, 7.3631378999335295)\t\t};\n\t\\addlegendentry{RetNet$^*$ (12x2K) };  \n\n\t\t\t\\addplot+[very thick,mark=star,mark options={scale=1}, color=color3] plot coordinates {\n(2, 9.07)\n(4, 7.95)\n(6, 7.99)\n(8, 8.22)\n(10, 8.43)\n(12, 8.38)\n(14, 8.38)\n(16, 8.21)\n(18, 8.31)\n(20, 8.63)\t\t};\n\t\\addlegendentry{Mamba (2K) };  \n\n   \t\t\t\\addplot+[very thick,mark=square*,mark options={scale=1}, color=color3] plot coordinates {\n(2, 9.074799787988505)\n(4, 7.7780675203930025)\n(6, 7.508237473960171)\n(8, 7.3529524666619945)\n(10, 7.216372863755205)\n(12, 6.940692359274163)\n(14, 6.760323233107172)\n(16, 6.49872027703192)\n(18, 6.460961900906283)\n(20, 6.6330331428715725)\t};\n\t\\addlegendentry{Mamba (8K) };  \n\n\\addplot+[very thick,mark=star,mark options={scale=1}, color=color5] plot coordinates {\n(2, 9.22)\n(4, 7.94)\n(6, 7.73)\n(8, 7.65)\n(10, 7.55)\n(12, 7.3)\n(14, 7.16)\n(16, 6.92)\n(18, 6.89)\n(20, 7.12)\t\t};\n\\addlegendentry{GLA (2K) };  \n\n\\addplot+[very thick,mark=square*,mark options={scale=1}, color=color5] plot coordinates {\n  (2, 9.134076157537933)\n(4, 7.763725571624473)\n(6, 7.470406185573722)\n(8, 7.307020316318466)\n(10, 7.162166118009677)\n(12, 6.8859825063022155)\n(14, 6.69981277659273)\n(16, 6.443211499058021)\n(18, 6.393198065692631)\n(20, 6.54966772918719)\n};\n\\addlegendentry{GLA$^{*}$ (12x2K) };  \n\\addplot+[very thick, mark options={scale=1},  color=color4] plot coordinates {\n(2, 9.195530723286662)\n(4, 7.773488412387971)\n(6, 7.445253416398666)\n(8, 7.276337205511129)\n(10, 7.131082988866674)\n(12, 6.858498031943685)\n(14, 6.67630503682912)\n(16, 6.426108888086446)\n(18, 6.382840753376257)\n(20, 6.543979086587886)\n};\n\\addlegendentry{GLA (8K) };  \n\\end{axis}\n\\end{tikzpicture}\n}\n \\vspace{-3mm}\n\\caption{Length extrapolation on the test set of SlimPajama and  PG19. We pretrain 1.3B models from scratch on SlimPajama for 100B tokens with different training length. $^{\\ast}$ indicates models using truncated BPTT with over 12 segments that are each of 2K-length.}\n\t\t\\label{fig:length_extrapolate} \n     \\vspace{-4.5mm}\n\t\\end{figure*}\n\nOur main results are shown in Table~\\ref{tab:main_results}. Compared to RetNet which uses a data-independent decay rate, the GLA Transformer with data-dependent gates shows improved results on all tasks. Both GLA Transformer and Mamba show comparable performance to Transformer++. \n\\vspace{-2mm}\n\n\\paragraph{Recall-intensive tasks.} While subquadratic models can achieve competitive language modeling performance to Transformers, \\citet{Arora2024SimpleLA} show that they  lag behind softmax attention in recall-intensive tasks. We next  evaluate GLA on real and synthetic tasks that focus on recall. \n\nThe synthetic MQAR task \\cite{zoology} is a more challenging multi-query version of the induction head task \\cite{h3} in which a model has to recall the token following a query token multiple times.   We follow \n\\citet{zoology}'s experimental setting and compare GLA against recent subquadractic models, including RetNet \\cite{sun2023retentive}, Mamba \\cite{Gu2023MambaLS}, Hyena \\cite{hyena} and  RWKV-4 \\cite{rwkv}. For RetNet and GLA the number of heads is set to 2; for other models we follow the default settings in \\citet{zoology}. The results are shown in Figure~\\ref{fig:mqar}. Standard quadratic attention achieves perfect scores in all settings and is thus omitted. We find that models with matrix-valued hidden states (i.e., Mamba/RetNet/GLA) outperform Hyena/RWKV, and our GLA outperforms RetNet, confirming the benefits of using data-dependent gates.\n\nFollowing \\citet{Arora2024SimpleLA}, we also test our models on three real recall-intensive tasks: FDA \\cite{arora_language_2023}, SWDE \\cite{lockard_openceres_2019}, and SQUAD \\cite{rajpurkar_know_2018}. These tasks focus on information extraction or reading comprehension. As illustrated in Table~\\ref{tab:recall-intensive}, subquadratic models significantly underperform Transformers on the FDA and SWDE, both of which are information extraction tasks. However, GLA outperforms other subquadractic models, likely due to its larger recurrent state (compared to Mamba) and selection mechanism (compared to RetNet).\n\n\\begin{table}\n\n\\centering\n\\small \n        \\begin{tabular}{l lccc}\n        \\toprule[0.5mm]\n  \\textbf{Scale} &   \\textbf{Model}  &  \\textbf{FDA} & \\textbf{SWDE} & \\textbf{SQUAD} \\\\\n      \\bottomrule\n \\textit{340M Params}  &   Transformer++ &  21.4 & 42.2 & 22.1 \\\\\n \\textit{15B Tokens}  &   RetNet & 2.9 & 13.3 & 27.6 \\\\\n   &   Mamba & 2.1 & 12.4 & 23.0 \\\\\n   &   GLA  & 8.1 & 18.6 & 27.2 \\\\      \n      \\midrule\n \\textit{1.3B Params}   &   Transformer++ & 27.4 & 66.6 & 31.5 \\\\\n \\textit{100B Tokens}   &   RetNet & 14.3 & 42.8 & 34.7 \\\\\n   &   Mamba &  6.2 & 41.4 & 35.2 \\\\\n   &   GLA  & 19.9 &  50.6 & 42.6 \\\\      \n    \\bottomrule\n    \\end{tabular}\n    \\vspace{-2mm}\n    \\caption{Comparison of different models in three recall-intensive tasks tested in \\citet{Arora2024SimpleLA}. Higher is better for all tasks. \n    }\n    \\label{tab:recall-intensive}\n\\end{table}\n\n\\vspace{-2mm}\n\\paragraph{Long sequence training and length extrapolation.} \nOne advantage of linear attention models is that they allow for efficient long sequence training in linear time.\nTo showcase this feature, we consider two training settings: (i) direct training on 8K-length contexts, (ii) training on 24K-length contexts through {truncated backpropagation through time} (TBPP) over 2K-length segments.\\footnote{We split a 24K input sequence into 12 segments. The final state of the previous segment is used as the initial state for the current segment.} In the latter case the gradients are not back-propagated across segments, and hence this approach has minimal overhead comparable to the standard 2K-length training strategy (where the initial hidden state is always set to zero). We pretrain 1.3B Mamba, RetNet, and GLA models on SlimPajama for 100B tokens on these settings and test them on both SlimPajama test set and PG19 \\citep{pg19} test set. \n\nFigure \\ref{fig:length_extrapolate} shows the perplexities of the tokens calculated in different position groups.  For models trained on 2K-length contexts, GLA extrapolates better than Mamba/RetNet in most position buckets on the PG19 test set; Mamba struggles to extrapolate beyond 4K, while GLA/RetNet can generalize to 18K on the Slimpajama test set.\nTransformers cannot extrapolate beyond training length, which is a known failure mode.\\footnote\n{Although there are positional encoding schemes that enable better length extrapolation, these methods still have difficulty generalizing significantly beyond context lengths seen during training~\\citep{alibi2021,xpos,fire2024}.} Pretraining in a long sequence consistently improves perplexities for all three models. We found marginal perplexity difference in the two settings for GLA, indicating that TBPTT might be a more economic approach to long-sequence training. Mamba benefits significantly from 8K-length training, and it performs similarly as GLA in the same training setting.\n\\vspace{-2mm}\n\\paragraph{Ablations.}\nWe conduct a small-scale ablation study by training the 340M GLA variants for 7B tokens. We investigate (i) the importance of having both \\emph{fine-grained} and \\emph{data-dependent} gating  and (ii) the influence of head dimension size. The results are shown in Table~\\ref{tab:ablation}. For (i), we find that while data dependent scalar gates substantially improve upon RetNet, a finer-grained gating mechanism is still necessary. For (ii) we tune the number of heads to vary head dimensions, where by default GLA uses 4 heads. Increasing it to 8 (\\text{i.e., smaller head dimension}) leads to relatively large perplexity degradation; reducing it to 1 (i.e., larger head dimension) actually performs best, but results in only marginal improvement while requiring much higher GPU memory. We thus choose 4 heads for our experiments.\n\\begin{table}\n\n\\centering\n\\small \n        \\begin{tabular}{ll}\n        \\toprule[0.5mm]\n     Model variants  &   Training ppl. \\\\\n      \\bottomrule\n        {GLA Transformer} (4 heads)  & 14.77 \\\\\n\\hspace{2mm} {No gate} (i.e., Linear Attention) & 23.21\\\\\n\\hspace{2mm} {\\emph{Data independent} scalar decay} (i.e., RetNet) & 16.55 \\\\\n\\hspace{2mm} {\\emph{Data dependent} scalar gate} & 15.56 \\\\\n\\hspace{2mm} {Small head dimension (8 heads)} & 15.29 \\\\\n\\hspace{2mm} {Large head dimension (1 head)} &  14.61 \\\\\n        \\bottomrule\n        \\end{tabular}\n    \\vspace{-2mm}\n    \\caption{Ablation study results on the 340M model trained for 7B tokens. We evaluate the model variants via the average perplexity of the last 200 training steps. \n    }\n        \\label{tab:ablation}\n\n\\end{table}\n\n\\vspace{-2mm}\n\\subsection{Training Efficiency}\n\\vspace{-2mm}\n\nFig.~\\ref{fig:tps} shows the throughput and memory usage as a function of the sequence length and batch size for the different 1.3B models on a single H100 GPU.\\footnote{We use the official implementation for Mamba, the fused version of SwiGLU  for Transformer++ and GLA, and FlashAttention-2 for Transformer++.} Here GLA adopts the materialization version of \\textsc{FlashLinearAttention} with recomputation of hidden state (\\S\\ref{sec:fla}). All models have linear space complexity, and the total GPU footprint difference among them is minimal. In terms of training throughput, Mamba  lags behind Transformer++ and GLA, with GLA shows greater advantages in training lengths beyond 4096.\n\n\\vspace{-2mm}\n\\subsection{Limitations \\& Future Work}\n\\vspace{-2mm}\nWhile our experiments with the GLA Transformer were on a respectable scale, we were unable to perform larger-scale experiments due to limited compute resources.\nAlthough it is unclear at this point how GLA would scale to even larger models/datasets, we anticipate that training efficiency of GLA become even more favorable compared to Mamba at larger scales. Specifically, when scaled to larger sizes (e.g., $>7$B), GLA can be more efficient than Mamba because of better use of tensor cores and GLA's compatibility with tensor parallelism.\\footnote{In particular, since Mamba is not a  multi-head model it is not as amenable to tensor parallelism.} Insofar as we are interested in leveraging the efficiency of linear attention, it would be interesting to apply GLA to other modalities (especially modalities with long-range dependencies), in line with recent work on applying state-of-the-art state-space models to other types of data \\citep[][\\textit{inter alia}]{Yan2023DiffusionMW, zhu2024vision,ma2024u,liu2024vmamba,xing2024segmamba,wang2024graph,wang2024mambabyte,yang2024vivim}.\n\n\\vspace{-2mm}\n\\section{Related Work}\n\\label{sec:related}\n\\vspace{-2mm}\nWe briefly discuss related work here and give an extended discussion of the related work in Appendix~\\ref{appdx:extended_rw}.\n\nTraditional RNNs are difficult to scale  due to the nonlinear dependencies between the  hidden states and expensive matmul-based sequential hidden state updates. Linear RNNs/State-Space Models (SSMs)/Transformers eliminate nonlinear dependencies, making training parallelizable along the temporal dimension \\citep{parallel-martin, s4, s5}. Such models have been the focus of much recent work as a competitive sub-quadratic alternative to the Transformer architecture \\citep{rwkv, Gu2023MambaLS,HGRN,qin2023scaling,sun2023retentive,pretrain_wo_attn}.\n\n\\begin{figure}[t] \n\\vspace{-3mm}\n\t\t\\centering \n\t\t\\resizebox{1\\columnwidth}{!}{  \n\\begin{tikzpicture}\n\\scalefont{1.5} %\n\\begin{axis}[\n    name=plot1,\n    title={Training throughput},\n        xlabel={Training length/Batch size},\n    ylabel={Tokens per second (Kt/s)},\n    major x tick style=transparent,\n    ybar=2*\\pgflinewidth,\n    bar width=8pt,\n    x tick label style={rotate=0, anchor=center},\n    symbolic x coords={2048/8,4096/4,8192/2,16284/1},\n    xticklabel style={yshift=-2mm}, \n    ymin=0,\n    xtick=data,\n    tick label style={font=\\large},\n    label style={font=\\large},\n    legend style={at={(7,-1.6)},   \n                anchor=north,legend columns=3,\n                column sep=0.3cm,\n                font=\\normalsize,\n                row sep=-0.15cm\n                },     \n    legend cell align=left,\n    ymajorgrids=true,\n    grid style=dashed,\n    enlarge x limits=0.25,\n    point meta=explicit symbolic,\n    legend to name={mylegend},\n]\n\\addplot[draw=bred,thick,fill=bred,fill opacity=0.6] coordinates { \n  (2048/8, 51.3)[\\textcolor{bred!60}]\n  (4096/4, 46.7)[\\textcolor{bred!60}]\n  (8192/2, 38.7)[\\textcolor{bred!60}]\n   (16284/1, 29.1)[\\textcolor{bred!60}]\n};\n\n\\addplot[draw=bblue,thick,fill=bblue,fill opacity=0.6] coordinates {\n  (2048/8, 22.8)[\\textcolor{bblue!60}{1408}]\n  (4096/4, 22.8)[\\textcolor{bblue!60}{352}]\n  (8192/2, 22.8)[\\textcolor{bblue!60}{1056}]\n  (16284/1, 26.0)[\\textcolor{bblue!60}{1056}]\n};\n\n\\addplot[draw=bcyan,thick,fill=bcyan,fill opacity=0.6] coordinates {\n  (2048/8, 43.8)[\\textcolor{bcyan!60}{1056}]\n  (4096/4, 43.5)[\\textcolor{bcyan!60}{352}]\n  (8192/2, 43.2)[\\textcolor{bcyan!60}{1056}]\n  (16284/1, 41.1)[\\textcolor{bcyan!60}{1056}]\n  };\n\n\\legend{Transformer++, Mamba, GLA}\n\n\\end{axis}\n\\begin{axis}[\n at={(plot1.south east) },\n    title={GPU memory usage},\n    xlabel={Training length/Batch size},\n    ylabel near ticks, yticklabel pos=right,\n    ylabel={Gigabyte (GB)},\n    major x tick style=transparent,\n    ybar=2*\\pgflinewidth,\n    bar width=8pt,\n    x tick label style={rotate=0, anchor=center},\n    symbolic x coords={2048/8, 4096/4, 8192/2, 16284/1},\n    xticklabel style={yshift=-2mm},\n    xtick=data,\n    ymin=0,\n      tick label style={font=\\large},\n  label style={font=\\large},\n    ymajorgrids=true,\n    grid style=dashed,\n    enlarge x limits=0.25,\n    point meta=explicit symbolic\n]\n\\addplot[draw=bred,thick,fill=bred,fill opacity=0.6] coordinates { \n  (2048/8,33)[\\textcolor{bred!60}]\n  (4096/4, 33)[\\textcolor{bred!60}]\n  (8192/2, 33)[\\textcolor{bred!60}]\n   (16284/1, 33)[\\textcolor{bred!60}]\n};\n\n\\addplot[draw=bblue,thick,fill=bblue,fill opacity=0.6] coordinates {\n  (2048/8, 36.0)[\\textcolor{bblue!60}{1408}]\n  (4096/4, 36.0)[\\textcolor{bblue!60}{352}]\n  (8192/2, 36.0)[\\textcolor{bblue!60}{1056}]\n  (16284/1, 36.0)[\\textcolor{bblue!60}{1056}]\n};\n\n\\addplot[draw=bcyan,thick,fill=bcyan,fill opacity=0.6] coordinates {\n  (2048/8, 37.0)[\\textcolor{bcyan!60}{1056}]\n  (4096/4, 37.0)[\\textcolor{bcyan!60}{352}]\n  (8192/2, 37.0)[\\textcolor{bcyan!60}{1056}]\n  (16284/1, 37.0)[\\textcolor{bcyan!60}{1056}]\n  };\n\\end{axis}\n\\ref{mylegend}\n\n\\end{tikzpicture}\n\t\t}\n     \\vspace{-6mm}\n\t\t\\caption{Training throughput and  memory footprint on an H100.} \n\\label{fig:tps}\n\t\\end{figure}\n\nData-dependent decay rates have always been regarded important for  RNNs  \\citep{DBLP:journals/neco/GersSC00,unreasonable-forget-gate}. Typical forget gate values depend on both the previous hidden state and the current input. However \\citet{parallel-martin} suggest that forget gate values should depend solely on the current inputs to enable parallel training. This simple strategy has been shown to be effective in moderate-scale experiments conducted by HGRN \\citep{qin2023scaling}.  RWKV-v6 \\citep{peng2024eagle} and Mamba \\citep{Gu2023MambaLS} also use data-dependent decay rates that are reminiscent of forget gates. In the context of linear Transformers, \\citet{peng2021random} employ a coarse-grained position-wise forget gate, while \\citet{mao-2022-fine} and \\citet{gatedloop} use a more fine-grained forget gate. \n\nRNNs rely on fixed-dimensional hidden states to encode their entire history. The hidden state dimension  serves as a proxy for memory capacity and thus significantly influences their expressive power. \nLinear Transformers expand the hidden dimension of RNNs via the outer-product parameterization, as discussed \\S\\ref{subsec:background-lin}. Linear SSMs on the other hand expand their hidden dimension via a single-input-single-output (SISO) strategy. \nWithout data-dependent SSM parameters, this can be done efficiently during training via the Fast Fourier Transform (FFT). However, with data-dependent SSM parameters, FFT-based training is not possible, and thus \\citet{Gu2023MambaLS} implements a custom CUDA kernel to train a selective state-space model using the parallel scan algorithm \\citep{s5}. To fit all the hidden states into SRAM, they can only afford an expansion rate up to 16. In contrast our hardware-aware training algorithm provides an alternative, efficient approach for expanding the hidden dimension to a wider range, which we have shown useful in recall-intensive tasks.\n\n\\vspace{-2mm}\n\n\\vspace{-2mm}\n\\section{Conclusion}\n\\vspace{-2mm}\nWe propose an efficient algorithm for training linear attention Transformers with data-dependent gating mechanisms. Our algorithm makes it possible to balance FLOPs against parallellism, while still allowing for the use of half-precision matmuls which can take advantage of tensor core units on modern GPUs. Experiments on language modeling demonstrate that gated linear attention Transformers can perform respectably compared to strong baselines.\n\\label{sec:conc}\n\n\\section*{Impact Statement}\n\nThis paper aims to improve the training efficiency of a new model family of (gated) linear attention models. The efficiency advantage of such models might help democratize access of language models. On the other hand, whether such new architectures would affect known issues such as biased and harmful outputs of language models remains an unexplored research question.\n\n\\section*{Acknowledgments}\n\nThis work was supported by MIT-IBM Watson AI Lab. We thank Yutao Sun, Zhen Qin, Li Dong, Xinyu Yang, Jiacheng You, Huanqi Cao, Yu Zhang, and Shida Wang for their insightful discussions. We also thank Yu Zhang, Fares Obeid, Daniel Goldstein, and Liliang Ren for their proofreading. Special thanks to Yu Zhang for contributing to the \\textsc{FlashLinearAttention} library.\n\n\\newpage\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{You Only Cache Once:\\\\Decoder-Decoder Architectures for Language Models}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nWe introduce a decoder-decoder architecture, \\our{}, for large language models, which only caches key-value pairs once. It consists of two components, i.e., a \\textit{cross-decoder} stacked upon a \\textit{self-decoder}. The self-decoder efficiently encodes global key-value (KV) caches that are reused by the cross-decoder via cross-attention. The overall model behaves like a decoder-only Transformer, although \\our{} only caches once. The design substantially reduces GPU memory demands, yet retains global attention capability. Additionally, the computation flow enables prefilling to early exit without changing the final output, thereby significantly speeding up the prefill stage. Experimental results demonstrate that \\our{} achieves favorable performance compared to Transformer in various settings of scaling up model size and number of training tokens. We also extend \\our{} to 1M context length with near-perfect needle retrieval accuracy. The profiling results show that \\our{} improves inference memory, prefill latency, and throughput by orders of magnitude across context lengths and model sizes. Code is available at \\url{https://aka.ms/YOCO}.\n\\end{abstract}\n\n\\vfill{}\n\n\\begin{figure*}[ht]\n\\centering\n\\captionsetup{type=figure}\n\\includegraphics[width=0.92\\textwidth]{figure/highlight-cropped.pdf}\n\\caption{We propose a decoder-decoder architecture, \\our{}, for large language model, which only caches key/value once.\n\\our{} markedly reduces the KV cache memory and the prefilling time, while being scalable in terms of training tokens, model size, and context length.\nThe inference cost is reported to be 512K as the context length, and \\Cref{fig:memory,fig:cache:mem:per:token,fig:prefilling,fig:throughput} present more results for different lengths.\n}\n\\label{fig:highlight}\n\\end{figure*}\n\n\\vfill{}\n\n\\newpage\n\\section{Introduction}\n\\label{sec:intro}\n\nThe decoder-only Transformer~\\cite{transformer} has become the de facto architecture for language models.\nNumerous efforts have continued to develop suitable architectures for language modeling.\nThere have been main strands of explorations.\nFirst, encoder-only language models, such as BERT~\\cite{bert}, bidirectionally encode the input sequence.\nSecond, encoder-decoder models, such as T5~\\cite{t5}, use a bidirectional encoder to encode input and a unidirectional decoder to generate output.\nBoth of the above layouts struggle with autoregressive generation due to bidirectionality.\nSpecifically, encoders have to encode the whole input and output tokens again for the next generation step.\nAlthough encoder-decoder can use only decoder to generate, the output tokens do not fully leverage the parameters of encoder, especially for multi-turn conversation.\nThird, decoder-only language models, such as GPT~\\cite{gpt3}, generate tokens autoregressively.\nBy caching the previously computed key/value vectors, the model can reuse them for the current generation step.\nThe key-value (KV) cache avoids encoding the history again for each token, greatly improving the inference speed.\nThis compelling feature establishes the decoder-only language model as the standard option.\n\nHowever, as the number of serving tokens increases, the KV caches occupy a lot of GPU memory, rendering the inference of large language models memory-bounded~\\cite{scaling:inference}.\nFor the example of a 65B-size language model (augmented with grouped-query attention~\\cite{gqa} and 8-bit KV quantization), 512K tokens occupy about 86GB GPU memory, which is even larger than the capacity of one H100-80GB GPU.\nIn addition, the prefilling latency of long-sequence input is extremely high.\nFor instance, using four H100 GPUs, the 7B language model (augmented with Flash-Decoding~\\cite{flashdec} and kernel fusion) requires about 110 seconds to prefill 450K tokens, and 380 seconds for 1M length.\nThe above bottlenecks make it difficult to deploy long-context language models in practice.\n\nIn this work, we propose a decoder-decoder architecture, \\our{}, for large language models, which only caches KV pairs once.\nSpecifically, we stack cross-decoder upon self-decoder.\nGiven an input sequence, the self-decoder utilizes efficient self-attention to obtain KV caches.\nThen the cross-decoder layers employ cross-attention to reuse the shared KV caches.\nThe decoder-decoder architecture is conceptually similar to encoder-decoder, but the whole model behaves more like a decoder-only model from the external view.\nSo, it naturally fits into autoregressive generation tasks, such as language modeling.\nFirst, because \\our{} only caches once\\footnote{The word ``once'' refers to global KV cache. Strictly, self-decoder also needs to store a certain number of caches. As the self-decoder utilizes an efficient attention module, the cache size is bounded to a constant, which can be ignored compared to global caches when the sequence length is large.}, the GPU memory consumption of KV caches is significantly reduced.\nSecond, the computation flow of the decoder-decoder architecture enables prefilling to early exit before entering the self-decoder.\nThe nice property speeds up the prefill stage dramatically, improving user experience for long-context language models.\nThird, \\our{} allows for more efficient system design for distributed long-sequence training.\nIn addition, we propose gated retention for self-decoder, which augments retention~\\cite{retnet} with a data-controlled gating mechanism.\n\nWe conduct extensive experiments to show that \\our{} achieves favorable language modeling performance and has many advantages in terms of inference efficiency.\nExperimental results demonstrate that \\our{} can be scaled up with more training tokens, larger model size, and longer context length.\nSpecifically, we scale up the 3B \\our{} model to trillions of training tokens, attaining results on par with prominent Transformer language models, such as StableLM~\\cite{stablelm}.\nMoreover, the scaling curves ranging from 160M to 13B show that \\our{} are competitive compared to Transformer.\nWe also extend the context length of \\our{} to 1M tokens, achieving near perfect needle retrieval accuracy.\nIn the multi-needle test, \\our{} obtains competitive results even compared to larger Transformers.\n\nIn addition to good performance on various tasks, the profiling results show that \\our{} improves the GPU memory footprint, prefill latency, throughput, and serving capacity.\nIn particular, the memory of KV caches can be reduced by about $80\\times$ for 65B models.\nEven for a 3B model, the overall inference memory consumption can be reduced by two times for 32K tokens and by more than nine times for 1M tokens.\nThe prefill stage is speeded up by $71.8\\times$ for the 1M context and $2.87\\times$ for the 32K input.\nFor example, for a 512K context, \\our{} reduces the Transformer prefilling latency from 180 seconds to less than six seconds.\nThe results position \\our{} as a strong candidate model architecture for future large language models with native long-sequence support.\n\n\\section{You Only Cache Once (YOCO)}\n\\label{sec:arch}\n\n\\begin{figure*}[t]\n\\centering\n\\captionsetup{type=figure}\n\\includegraphics[width=0.62\\textwidth]{figure/arch.png}\n\\caption{Overview of the decoder-decoder architecture. Self-decoder generates the global KV cache. Then cross-decoder employs cross-attention to reuse the shared KV caches.\nBoth self-decoder and cross-decoder use causal masking.\nThe overall architecture behaves like a decoder-only Transformer, autoregressively generating tokens.\n}\n\\label{fig:yoco}\n\\end{figure*}\n\nThe proposed architecture, named \\our{}, is designed for autoregressive modeling, such as large language models (LLMs).\nAs shown in \\Cref{fig:yoco}, the decoder-decoder architecture has two parts, i.e., self-decoder and cross-decoder.\nSpecifically, \\our{} is stacked with $L$ blocks, where the first $\\frac{L}{2}$ layers are self-decoder while the rest modules are cross-decoder.\nGiven an input sequence $x = x_1 \\cdots x_{|x|}$, the input embeddings are packed into $X^0 = [\\vx_1, \\cdots, \\vx_{|x|}] \\in \\mathbb{R}^{|x|\\times d_\\text{model}}$, where $d_\\text{model}$ is hidden dimension.\nWe first obtain contextualized vector representations $X^{l} = \\operatorname{Self-Decoder}(X^{l-1}), l \\in [1, \\frac{L}{2}]$, where $X^{\\nicefrac{L}{2}}$ is used to produce KV caches $\\hat{K}, \\hat{V}$ for cross-decoder.\nThen we compute $X^{l} = \\operatorname{Cross-Decoder}(X^{l-1}, \\hat{K}, \\hat{V}), l \\in [\\frac{L}{2}+1, L]$ to get the output vectors $X^{L}$.\n\nBoth self- and cross-decoder follow a similar block layout (i.e., interleaved attention and feed-forward network) as in Transformer~\\cite{transformer}. We also include pre-RMSNorm~\\cite{rmsnorm}, SwiGLU~\\cite{glu}, and grouped-query attention~\\cite{gqa} as improvements.\nThe difference between the two parts lies in attention modules.\nSelf-decoder (\\Cref{sec:self:decoder}) uses efficient self-attention (e.g., sliding-window attention).\nIn comparison, cross-decoder (\\Cref{sec:cross:decoder}) uses global cross-attention to attend to the shared KV caches produced by the output of the self-decoder.\n\n\\subsection{Self-Decoder}\n\\label{sec:self:decoder}\n\nSelf-decoder takes token embeddings $X^0$ as input and compute intermediate vector representation $M=X^{\\nicefrac{L}{2}}$:\n\\begin{equation}\n\\begin{aligned}\nY^{l}&=\\operatorname{ESA}(\\operatorname{LN}(X^{l}))+X^{l}\\\\\nX^{l+1}&=\\operatorname{SwiGLU}(\\operatorname{LN}(Y^{l}))+Y^{l}\\\\\n\\end{aligned}\n\\end{equation}\nwhere $\\operatorname{ESA}(\\cdot)$ represents efficient self-attention, $\\operatorname{SwiGLU}(X) = (\\operatorname{swish}(X W_G)\\odot X W_1)W_2$, and RMSNorm~\\cite{rmsnorm} is used for $\\operatorname{LN}(\\cdot)$.\nCausal masking is used for efficient self-attention.\n\nThe key property of the efficient self-attention module is $\\mathcal{O}(1)$ inference memory, i.e., constant number of KV caches.\nFor example, the cache size of sliding-window attention~\\cite{sparsetransformer} depends on the window size instead of the input length.\nMore design choices (e.g., gated retention) of the efficient self-attention module are detailed in \\Cref{sec:design:self:decoder}.\n\n\\subsection{Cross-Decoder}\n\\label{sec:cross:decoder}\n\nFirst, the output of the self-decoder $X^{\\nicefrac{L}{2}}$ generates global KV caches $\\hat{K}, \\hat{V}$ for cross-decoder:\n\\begin{equation}\n\\hat{K} = \\operatorname{LN}(X^{\\nicefrac{L}{2}}) W_K ,\\quad \\hat{V} = \\operatorname{LN}(X^{\\nicefrac{L}{2}}) W_V\n\\end{equation}\nwhere $W_K, W_V \\in \\mathbb{R}^{d\\times d}$ are learnable weights.\nThen, cross-decoder layers are stacked after the self-decoder to obtain the final output vectors $X^{L}$.\nThe KV caches $\\hat{K}, \\hat{V}$ are reused by all the $\\frac{L}{2}$ cross-decoder modules:\n\\begin{equation}\n\\begin{aligned}\n\\hat{Q}^{l} &= \\operatorname{LN}(X^{l}) W_Q^l \\\\\nY^{l}&=\\operatorname{Attention}(\\hat{Q}^{l}, \\hat{K}, \\hat{V})+X^{l}\\\\\nX^{l+1}&=\\operatorname{SwiGLU}(\\operatorname{LN}(Y^{l}))+Y^{l}\\\\\n\\label{eq:cross:decoder}\n\\end{aligned}\n\\end{equation}\nwhere $\\operatorname{Attention}(\\cdot)$ is standard multi-head attention~\\cite{transformer}, and $W_Q^l \\in \\mathbb{R}^{d\\times d}$ is a learnable matrix.\nCausal masking is also used for cross-attention.\nBecause cross-attention is compatible with group query attention~\\cite{gqa}, we can further save the memory consumption of KV caches.\nAfter obtaining $X^L$, a $\\softmax$ classifier performs next-token prediction.\n\n\\subsection{Inference Advantages}\n\\label{sec:infer:adv}\n\nIn addition to competitive language modeling results, \\our{} significantly reduces serving costs and improves inference performance.\nWe report detailed inference comparisons in \\Cref{sec:exp:infer}.\n\n\\begin{table*}[t]  \n\\centering  \n\\begin{minipage}[b]{0.52\\linewidth}  \n\\centering  \n\\captionsetup{type=figure}\n\\includegraphics[width=\\textwidth]{figure/inference.pdf}\n\\caption{\\our{} Inference.\n\\textbf{Prefill}: encode input tokens in parallel.\n\\textbf{Generation}: decode output tokens one by one.\nThe computation flow enables prefilling to early exit without changing the final output, thereby significantly speeding up the prefill stage.\n}\n\\label{fig:arch:inference}\n\\end{minipage}\n\\hfill  \n\\begin{minipage}[b]{0.47\\linewidth}\n\\centering\n\\begin{tabular}{lc}\n\\toprule\n& \\textbf{KV Cache Memory}  \\\\\n\\midrule\nTransformer & $\\mathcal{O}(LND)$ \\\\\n\\our{} & $\\mathcal{O}((N+L)D)$ \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Inference memory complexity of KV caches. $N, L, D$ are the sequence length, number of layers, and hidden dimension.}\n\\label{tbl:complexity:memory}\n\\begin{tabular}{lc}\n\\toprule\n& \\textbf{Prefilling Time} \\\\\n\\midrule\nTransformer & $\\mathcal{O}(LN^2D)$ \\\\\n\\our{} & $\\mathcal{O}(LND)$ \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Prefilling time complexity of attention modules. $N, L, D$ are the same as above.}  \n\\label{tbl:complexity:prefill}\n\\end{minipage}\n\\end{table*}  \n\n\\mypara{Saving GPU Memory and Serving More Tokens.}\n\\Cref{tbl:complexity:memory} compares the memory complexity between Transformers and \\our{}.\nSpecifically, because global KV caches are reused and efficient self-attention needs constant caches, the number of caches is $\\mathcal{O}(N + CL)$, where $N$ is the input length, $C$ is a constant (e.g., sliding window size), and $L$ is the number of layers.\nFor long sequences, $CL$ is much smaller than $N$, so about $\\mathcal{O}(N)$ caches are required, i.e., you only cache once.\n\nIn comparison, Transformer decoders have to store $N \\times L$ keys and values during inference.\nSo \\our{} roughly saves $L$ times GPU memory for caches compared to Transformer decoders.\nBecause the inference capacity bottleneck becomes KV caches (\\Cref{fig:memorybar}), our method enables us to serve many more tokens without being out of GPU memory.\nThe increased batch size is also beneficial to inference throughput.\n\n\\mypara{Reducing Prefilling Time and Improving Throughput.}\nAs shown in \\Cref{fig:arch:inference}, because the cross-decoder reuses the outputs of self-decoder, we can exit early before entering the cross-decoder during the prefill stage.\nThe intriguing property of computation dependency greatly accelerates the prefilling speed.\n\nFirst, only half the layers are needed for forward computation, i.e., at least half prefilling latency reduction.\nSecond, the efficient attention modules of the self-decoder are usually fast.\nFor the example of 512K context length, we can decrease the prefilling latency from 180 seconds (Transformer with optimized inference, such as Flash-Decoding and kernel fusion) to less than 6 seconds (\\Cref{fig:prefilling}).\nEven for 32K length, \\our{} has about three times speedup in terms of prefilling time.\n\\Cref{tbl:complexity:prefill} compares prefilling time complexity of attention modules between Transformer and \\our{}.\n\n\\section{Design Choices of Self-Decoder}\n\\label{sec:design:self:decoder}\n\nWe can choose various efficient self-attention methods for self-decoder.\nAs long as the module only requires constant inference memory, the cache memory complexity of the self-decoder depends on the number of layers.\nMoreover, a good module choice improves both training and deployment costs.\nIn this work, we use gated retention (\\Cref{sec:gret}) or sliding-window attention (\\Cref{sec:swa}).\n\n\\subsection{Gated Retention}\n\\label{sec:gret}\n\nGated retention (gRet, aka gRetNet or RetNet-3) augments retention~\\cite{retnet} with a data-dependent gating mechanism, which achieves training parallelism, good performance, and low inference cost simultaneously for sequence modeling.\nWe use gRet as the default efficient self-attention module in the experiments.\nThe method unifies the parallel, recurrent, and chunkwise recurrent computation paradigms. These three representations are equivalent and can obtain the same computation results.\nThe training process usually uses the parallel or chunkwise recurrent paradigms, while the inference stage can employ the recurrent paradigm for constant KV memory.\nWe describe the three representations as follows:\n\n\\mypara{The Parallel Representation}\nThe gated retention is defined as:\n\\begin{equation}\n\\begin{aligned}\nQ = (X W_Q) \\odot \\Theta ,& \\quad K = (X W_K) \\odot \\overline{\\Theta} ,\\quad V = X W_V ,\\quad \\Theta_n = e^{in\\theta} \\\\\n\\gamma = \\operatorname{sigmoid} &(X W_{\\gamma})^{1/\\tau} , \\quad D_{nm} =\n\\left\\{\n\\begin{aligned}\n& \\prod_{i=m+1}^{n}\\gamma_i, &n\\ge m \\\\\n& 0, &n < m \\\\\n\\end{aligned}\n\\right. \\\\\n&\\operatorname{gRet}(X) = (Q K^\\intercal \\odot D)V\n\\label{eq:gret:parallel}\n\\end{aligned}\n\\end{equation}\nwhere $W_Q, W_K, W_V \\in \\mathbb{R}^{d\\times d}$ and $W_{\\gamma} \\in \\mathbb{R}^{d\\times 1}$ are learnable weights, and the temperature term $\\tau$ encourages $\\gamma$ to 1 for better memorization~\\cite{gla}.\nThe data-controlled decay is head-wise~\\cite{gateloop} rather than element-wise so that the computation can fully utilize NVIDIA tensor cores.\nRefer to \\cite{retnet} for more details about the other designs.\n\n\\mypara{The Recurrent Representation}\nBeing equivalent to \\Cref{eq:gret:parallel}, the output of gated retention can be computed recurrently.\nFor the $n$-th timestep, the output is obtained via:\n\\begin{equation}\n\\begin{aligned}\n&S_n = \\gamma_n S_{n-1} + K_n^{\\intercal} V_n \\\\\n&\\operatorname{gRet}(X_n) = Q_n S_n, \\quad n = 1, \\cdots, |x| \\\\\n\\label{eq:gret:recurrent}\n\\end{aligned}\n\\end{equation}\nwhere $Q, K, V, \\gamma$ are the same as in \\Cref{eq:gret:parallel}.\nDuring auto-regressive inference, the self-decoder maintains $S_n$ as the intermediate state for an efficient generation.\n\n\\mypara{The Chunkwise Recurrent Representation}\nThe chunk-wise representation is a unified formulation of recurrent and parallel representations.\nGiven chunk size $B$, the outputs are computed chunk by chunk.\nThe computation is divided into inner-chunk and cross-chunk parts.\nDenote ${[i]}$ as the $i$-th chunk, i.e., $x_{[i]} = x_{(i-1)B+1} , \\cdots , x_{iB}$, we compute the $i$-th chunk as:\n\\begin{equation}\n\\begin{aligned}\n\\label{eq:gret:chunk}\n\\quad \\beta_{(i-1)B+j} &= \\prod_{k=(i-1)B+1}^{(i-1)B+j} \\gamma_{k},\\quad D_{[i]}(j,k)=\\frac{\\beta_{(i-1)B+k}}{\\beta_{(i-1)B+j}}\\ \\ \\mathrm{if}\\ \\ j \\le k\\ \\ \\mathrm{else}\\ \\ 0 \\\\\nR_{i} &= K_{[i]}^\\intercal (V_{[i]}\\odot \\frac{\\beta_{iB}}{\\beta_{[i]}})+\\beta_{iB} R_{i-1},\\ \\ \\beta_{[i]}(j,k)= \\beta_{(i-1)B+j}\\\\\n\\operatorname{gRet} (X) &= \\underbrace{ (Q_{[i]} K^\\intercal_{[i]} \\odot D_{[i]}) V_{[i]} }_{\\text{Inner-Chunk}} + \\underbrace{ (Q_{[i]} R_{i-1}) \\odot \\beta_{[i]}}_{\\text{Cross-Chunk}}\n\\end{aligned}\n\\end{equation}\nwhere $R_i$ is the intermediate state of the $i$-th chunk, and $\\beta$ summarizes the data-controlled decay $\\gamma$.\nThe proof in \\Cref{app:gret:chunk} shows the equivalence between the computation paradigms.\nThe chunkwise paradigm combines the best of parallelism and recurrence, i.e., saving FLOPs compared with fully parallel computation and reducing the iterations compared to recurrent computation.\nDuring the training and prefill stages, the chunk-wise representation increases throughput and reduces GPU memory consumption.\n\n\\mypara{Multi-Head Gated Retention}\nSimilar to multi-head attention~\\cite{transformer} and multi-scale retention~\\cite{retnet}, we apply gated retention to each head and combine the outputs together:\n\\begin{equation}\n\\begin{aligned}\n\\mathrm{head}_i &= \\operatorname{gRet} (X) \\\\\nY &= \\operatorname{GroupNorm}_{h}( \\operatorname{Concat}(\\mathrm{head}_1, \\cdots, \\mathrm{head}_n) ) \\\\\n\\mathrm{MHGR}(X) &= (\\operatorname{swish}(X W_G) \\odot Y) W_O\n\\label{eq:mhgr}\n\\end{aligned}\n\\end{equation}\nwhere $W_G, W_O \\in \\mathbb{R}^{d\\times d}$ are learnable matrices, and $\\operatorname{GroupNorm}$~\\cite{groupnorm} normalizes each head~\\cite{magneto}.\nWe also apply $\\operatorname{swish}$ gate to increase non-linearity~\\cite{retnet}.\n\n\\subsection{Sliding-Window Attention}\n\\label{sec:swa}\n\nSliding-window attention~\\cite{sparsetransformer} restricts the attention range into a fixed window size $C$.\nIn contrast, vanilla Transformer decoders attend to all previous tokens.\nDuring inference, the KV cache memory complexity can be reduced from $\\mathcal{O}(N)$ to $\\mathcal{O}(C)$, i.e., the memory usage is constant rather than increasing with sequence length.\nSimilar to multi-head self-attention~\\cite{transformer}, we compute the output of sliding-window attention via:\n\\begin{equation}\n\\begin{aligned}\nQ = X W_Q ,&\\quad K = X W_K ,\\quad V = X W_V \\\\\n\\mathrm{head}_i &= \\softmax (Q_{[i]} K_{[i]}^\\intercal + B) V \\\\\nB_{ij} &=\n\\left\\{\n\\begin{aligned}\n& 0, &i - C < j \\le i \\\\\n& -\\infty, &\\text{otherwise} \\\\\n\\end{aligned}\n\\right. \\\\\nY &= \\operatorname{Concat}(\\mathrm{head}_1, \\cdots, \\mathrm{head}_h) \\\\\n\\mathrm{SWA}(X) &= Y W_O \\\\\n\\label{eq:swa}\n\\end{aligned}\n\\end{equation}\nwhere $W_Q, W_K, W_V, W_O \\in \\mathbb{R}^{d\\times d}$ are learnable matrices, and the window causal mask $B$ controls each query only attends to the previous keys whose distances are less than $C$.\nThe pre-normalization and residual connection are also applied to the module.\n\n\\section{Experiments}\n\\label{sec:exp}\n\nWe evaluate \\our{} for large language models from the following perspectives.\nFirst, we follow the setting of StableLM-3B-4E1T~\\cite{stablelm} to scale up training tokens (\\Cref{sec:lm:3b}).\nSecond, we present the scaling curves of the proposed architectures (\\Cref{sec:scaling}).\nThird, we scale up the \\our{} model to 1M context length and evaluate its long-sequence modeling capability (\\Cref{sec:long:eval}).\nFourth, we analyze the deployment advantages, including GPU memory footprint, serving capacity, prefilling time, and throughput (\\Cref{sec:exp:infer}).\nExperimental results show that \\our{} achieves competitive performance across various evaluation metrics. More importantly, the proposed method significantly reduces the inference cost.\n\n\\subsection{Language Modeling Evaluation}\n\\label{sec:lm:3b}\n\nWe train a 3B-size \\our{} language models by scaling up the number of training tokens. Then we compare the checkpoints with strong Transformer-based language models.\n\n\\mypara{Setup}\nWe use a similar training recipe as in StableLM-3B-4E1T~\\cite{stablelm}.\nWe adjust the head dimension to 128 instead of 80 as in StableLM for better kernel support.\nIn order to keep the model size unchanged, we set the hidden size to 3072 and the number of layers to 26.\nGrouped-query attention~\\cite{gqa} is used, where the number of query heads is 24, and the number of key-value heads is 8.\nWe train \\our{} with gated retention (\\Cref{sec:gret}).\nThe non-embedding parameter count is 2.8B. In comparison, StableLM-3B-4E1T is 2.7B and OpenLLaMA-v2-3B~\\cite{openllama} is 3.2B.\nThe training sequence length is 4096.\nThe batch size is 4M tokens.\nWe use the AdamW~\\cite{adamw} optimizer with $\\beta=0.9,0.95$.\nThe maximal learning rate is 3.2e-4 with 1000 warmup steps and linear decay to 1.28e-5.\nThe total schedule is set to 5T tokens. We train the model with 400k steps (i.e., 1.6T tokens) given the resource budget.\nThe curated training corpus is similar to \\cite{stablelm}.\nWe use \\texttt{tiktoken-cl100k\\_base} as the tokenizer.\nDetailed hyperparameters are described in \\Cref{app:hp:3b}.\n\n\\begin{table*}[t]\n\\centering\n\\resizebox{\\columnwidth}{!}{%\n\\begin{tabular}{@{}lccccccccc}\n\\toprule\n\\textbf{Model} & \\textbf{ARC-C} & \\textbf{ARC-E}& \\textbf{BoolQ} & \\textbf{Hellaswag} & \\textbf{OBQA} & \\textbf{PIQA} & \\textbf{Winogrande} & \\textbf{SciQ}  & \\textbf{Avg} \\\\\n\\midrule\n\\multicolumn{10}{l}{\\textit{Training with 1T tokens}} \\\\\nOpenLLaMA-3B-v2 & 0.339 & 0.676 & \\textbf{0.657} & \\textbf{0.700} & 0.260 & 0.767 & 0.629 & \\textbf{0.924} & 0.619 \\\\\nStableLM-base-alpha-3B-v2 & 0.324 & 0.673 & 0.646 & 0.686 & 0.264 & 0.760 & 0.621 & 0.921 & 0.612 \\\\\nStableLM-3B-4E1T  & --- & 0.666 & --- & --- & --- & \\textbf{0.768} & 0.632 & 0.914 & ---  \\\\\n\\our{}-3B    & \\textbf{0.379} & \\textbf{0.731} & 0.645 & 0.689 & \\textbf{0.298} & 0.763 & \\textbf{0.639} & \\textbf{0.924} &  \\textbf{0.634} \\\\\n\\midrule\n\\multicolumn{10}{l}{\\textit{Training with 1.6T tokens}} \\\\\nStableLM-3B-4E1T  & --- & 0.688 & --- & --- & --- & 0.762 & 0.627 & 0.913 & ---  \\\\\n\\our{}-3B & 0.396 & 0.733 & {0.644} & 0.698 & 0.300 & 0.764 & 0.631 & 0.921 & 0.636 \\\\\n\\midrule\n\\multicolumn{10}{l}{\\textit{Extending context length to 1M tokens}} \\\\\n\\our{}-3B-1M & {0.413} & {0.747} & 0.638 & {0.705} & 0.300 & {0.773} & {0.651} & {0.932} & {0.645} \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\caption{Eval Harness~\\cite{eval-harness} results compared with previous well-trained Transformer language models~\\cite{stablelm,stablelm-alphav2,openllama}.\nWe scale the 3B model to 1.6 trillion training tokens.\nThe 1T and 1.6T results of StableLM-3B-4E1T are taken from its technical report~\\cite{stablelm}.\n\\our{}-3B-1M is extended to the context length of 1M tokens.\n}\n\\label{tbl:harness}\n\\end{table*}\n\n\\mypara{Results}\n\\Cref{tbl:harness} compares the \\our{} checkpoints with OpenLLaMA-v2-3B~\\cite{openllama}, StableLM-base-alpha-3B-v2~\\cite{stablelm-alphav2}, and StableLM-3B-4E1T~\\cite{stablelm}.\nWe use LM Eval Harness~\\cite{eval-harness} to evaluate the zero-shot performance on various downstream tasks.\nOpenLLaMA-v2-3B and StableLM-base-alpha-3B-v2 are trained with 1T tokens.\nThe intermediate numbers of StableLM-3B-4E1T are taken from its technical report~\\cite{stablelm}.\nExperimental results across end tasks indicate that \\our{} achieves comparable results with previous well-tuned Transformer language models.\nBoth the checkpoints trained with 1T tokens and 1.6T tokens obtain consistent trend.\nMoreover, the results show that \\our{} is scalable in terms of training tokens.\n\n\\subsection{Scalability Compared with Transformers}\n\\label{sec:scaling}\n\n\\begin{wrapfigure}{r}{0.524\\textwidth}\n\\setlength\\intextsep{0pt}\n\\centering\n\\vspace{-1.5em}\n\\captionsetup{type=figure}\n\\includegraphics[width=0.48\\textwidth]{figure/scaling_curve.pdf}\n\\caption{LM loss decreases along with scaling up the model size (ranging from 160M to 13B).}\n\\vspace{-1.5em}\n\\label{fig:scaling}\n\\end{wrapfigure}\n\nWe compare the scaling curves between Llama Transformer~\\cite{transformer,llama}, \\our{} with gated retention (\\our{}$_\\text{gRet}$; \\Cref{sec:gret}), and \\our{} with sliding-window attention (\\our{}$_\\text{SWA}$; \\Cref{sec:swa}).\nWe train language models of various sizes (i.e., 160M, 400M, 830M, 1.4B, 2.7B, 6.8B, and 13B) using the same training data and settings.\nThe validation loss is used as the evaluation metric.\nThe scaling law~\\cite{scaling:law} is supposed to extrapolate larger-size performance.\n\n\\mypara{Setup}\nWe augment the Transformer architecture with Llama~\\cite{llama} improvements, such as RMSNorm~\\cite{rmsnorm}, SwiGLU~\\cite{glu}, and removing bias.\nThe sliding window size of \\our{}$_\\text{SWA}$ is 1,024.\nWe align the number of parameters by adjusting the FFN intermediate dimension.\nThe training batch size is 0.25M tokens with a 2k sequence length.\nWe train the models with 40k steps, i.e., 10B tokens. In practice, we find that the setting is effective for loss convergence, and the scaling laws can be well-fitted.\nMore hyperparameters are detailed in \\Cref{app:hp:scaling}.\n\n\\mypara{Results}\n\\Cref{fig:scaling} reports the validation loss with various parameter counts.\nWe also fit the scaling curves as in \\cite{scaling:law}.\n\\our{} obtains comparable performance from 160M to 13B compared to the Llama-optimized transformer architecture.\nThe findings demonstrate that \\our{} scales effectively with respect to model size.\nMoreover, \\our{}$_\\text{gRet}$ outperforms Transformer and \\our{}$_\\text{SWA}$.\nThe gains come from hybrid architectures of attention and retention, whose inductive biases tend to be complementary to each other. We observed similar gains by interleaving the attention and retention modules (1:3). Recent hybrid architectures~\\cite{jamba} also confirm similar findings.\n\n\\subsection{Long-Context Evaluation}\n\\label{sec:long:eval}\n\nWe extend the context length of \\our{}-3B (\\Cref{sec:lm:3b}) to 1M tokens.\nWe evaluate long-context models on needle retrieval and language modeling tasks.\n\nWe continue the model training with longer lengths progressively.\nThe length schedule is 64K, 256K, and 1M tokens.\nThe batch size is kept the same as before.\nThe learning rate and RoPE~\\cite{rotary} $\\theta$ are set as in \\Cref{tbl:hp:length}.\nTraining data is up-sampled according to sequence length~\\cite{length:upsampling}.\nFor a fair comparison, we do not use long-instruction tuning data. \nMore training details are described in \\Cref{app:hp:length}.\nA chunk parallelism algorithm for \\our{} is proposed in \\Cref{app:chunk:parallelism}, which reduces communication overhead and GPU memory fragmentation in our experiments of 1M length.\n\n\\begin{figure*}[t]\n\\centering\n\\captionsetup{type=figure}\n\\includegraphics[width=0.8\\textwidth]{figure/1m_retrieval.pdf}\n\\caption{Needle-in-a-haystack results in 1M length.\n}\n\\label{fig:needle}\n\\end{figure*}\n\n\\begin{table*}[t]\n\\centering\n\\begin{tabular}{lccccc}\n\\toprule\n\\bf Model & \\bf Size & $N=1$ & $N=2$ & $N=4$ & $N=8$ \\\\\n\\midrule\nYaRN-Mistral-128K~\\cite{yarn} & 7B & 0.02 & 0.12 & 0.08 & 0.20 \\\\\nLWM-1M-text~\\cite{lwm} & 7B & 1.00 & 0.90 & 0.76 & 0.62 \\\\\nMiniCPM-128K~\\cite{minicpm} & 2.4B & 1.00 & 1.00 & 0.54 & 0.56 \\\\\nChatGLM3-128K~\\cite{glm} & 6B  & 0.94 & 0.72 & 0.52 & 0.44 \\\\\n\\our{}-3B-1M & 3B & 0.98 & 0.98 & 0.84 & 0.56 \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Multi-needle retrieval accuracy. $N$ indicates the number of needles. $N=1$ is single-needle retrieval used as a reference, and $N>1$ indicates the multi-needle test.\nThe evaluation is conducted in 128K length, because most previous long-context models are tuned with this length.\n}\n\\label{tbl:multineedle}\n\\end{table*}\n\n\\mypara{Needle In A Haystack}\nThe pressure test evaluates whether models can retrieve ``needles'' from a long document~\\cite{needle}.\nWe follow the evaluation setting of Gemini 1.5~\\cite{gemini1.5} and LWM~\\cite{lwm}.\nThe needles are constructed as a city with a magic number.\nWe run 10 times at the same depth and length. The averaged accuracy is reported.\n\\Cref{fig:needle} shows that \\our{}-3B-1M passes the Needle-In-A-Haystack test with near perfect accuracy.\nThe results indicate that \\our{} has strong long-context modeling capability.\n\n\\mypara{Multi-Needle Retrieval}\nBesides the above single-needle retrieval, we conduct a multi-needle evaluation.\nWe compare \\our{}-3B-1M with previous long-context language models, including MiniCPM-128K~\\cite{minicpm}, ChatGLM3-128K~\\cite{glm}, YaRN-Mistral-128K~\\cite{yarn}, and LWM-1M-text~\\cite{lwm}.\nThe evaluation is conducted in 128K sequence length, because most previous models are tuned with this length.\n\n\\Cref{tbl:multineedle} reports the accuracy with $N$ needles.\nAmong these models, LWM-1M-text and \\our{}-3B-1M are trained with a 1M context length, while the others are in 128K length.\nAlthough LWM-1M-text continues training of Llama-2-7B, \\our{}-3B-1M can still achieve comparable performance with half the model size.\nMoreover, the 7B-size YaRN-Mistral-128K~\\cite{yarn} obtained by postion interpolation lags behind the other models.\nCompared to MiniCPM-128K and ChatGLM3-128K, \\our{}-3B-1M also outperforms these well-trained language models.\n\n\\begin{figure*}[t]\n\\centering\n\\begin{subfigure}[b]{0.495\\textwidth}\n\\centering\n\\includegraphics[width=\\textwidth]{figure/book-1m-ppl.png}\n\\caption{Book data.}\n\\end{subfigure}\n\\hfill\n\\begin{subfigure}[b]{0.495\\textwidth}\n\\centering\n\\includegraphics[width=\\textwidth]{figure/code-1m-ppl.png}\n\\caption{Repository-level code data.}\n\\end{subfigure}\n\\caption{Cumulative average negative log-likelihood on book and repository-level code. We filter the validation examples that are longer than 1M tokens. \\our{} achieves improved performance with longer context, i.e., utilizing long-distance information for language modeling.}\n\\label{fig:longppl}\n\\end{figure*}\n\n\\mypara{Perplexity over Long Sequences}\n\\Cref{fig:longppl} shows the cumulative average negative log-likelihood (NLL) as a function of context length.\nWe evaluate both book and repository-level code data.\nWe follow the setting of \\cite{gemini1.5} and filter validation data that are longer than 1M tokens.\nNLL decreases consistently with longer sequence length.\nThe results indicate that \\our{} can effectively utilize long-distance dependency for language modeling.\nWe also observe that the NLL-length curves tend to fit the power law, where the gaps are affected by the noise within the validation examples.\n\n\\subsection{Inference Advantages}\n\\label{sec:exp:infer}\n\nWe analyze inference efficiency from various perspectives, such as GPU memory footprint, prefilling latency, throughput, and serving capacity.\nWe demonstrate that \\our{} reduces the deployment cost by orders of magnitude, especially for long-sequence inference.\nMore importantly, the user experience (such as latency) is improved while maintaining good performance and reducing expenses.\n\nWe compare \\our{}$_\\text{gRet}$ with Transformer. The default model configuration follows \\Cref{sec:lm:3b}.\nNotice that Transformer uses grouped-query attention~\\cite{gqa}, Flash-Decoding~\\cite{flashdec}, and kernel fusion for a fair comparison.\nAs described in \\Cref{sec:gret}, gated retention uses the chunk-recurrent representation in the prefill stage, and the recurrent representation in the generation stage.\nThe chunk size is set to 256.\nWe implement a Triton~\\cite{triton} kernel for gated retention.\nThe evaluation sequence length is ranging from 32K to 1M.\nThe last 1,024 tokens are supposed to be generated, while the previous tokens are given input context.\nThe experiments are conducted with H100-80GB GPU cards.\n\n\\begin{figure*}[t]\n\\centering\n\\begin{subfigure}[c]{0.7\\textwidth}\n\\centering\n\\includegraphics[width=\\textwidth]{figure/memory-length.pdf}\n\\caption{Inference memory of Transformer and \\our{} across various lengths.\n}\n\\label{fig:memory-length}\n\\end{subfigure}\n\\hfill\n\\begin{subfigure}[c]{0.28\\textwidth}\n\\centering\n\\includegraphics[width=\\textwidth]{figure/memory-arch.pdf}\n\\caption{Breakdown memory consumption in 1M context length.\n}\n\\label{fig:memorybar}\n\\end{subfigure}\n\\caption{GPU memory consumption during inference.}\n\\label{fig:memory}\n\\end{figure*}\n\n\\mypara{GPU Memory}\nThe inference memory consumption is made up of three parts, namely model weights, intermediate activation, and KV cache.\n\\Cref{fig:memorybar} presents the breakdown memory profiling results.\nAlong with an increase in context length, the main memory bottleneck becomes KV caches, while model weights consume constant memory.\nThe results show that \\our{}$_\\text{gRet}$ alleviates the activation cost and KV cache memory footprint.\n\nAs shown in \\Cref{fig:memory-length}, the memory cost is significantly reduced using \\our{}.\nMoreover, the memory consumption of \\our{} increases slowly along the sequence length.\nFor example of 1M length, the overall inference memory usage is only 12.4GB, while Transformers occupy $9.4\\times$ GPU memory.\n\\our{} makes it feasible to deploy long-sequence modeling on customer-level GPUs.\nEven with a 32K sequence length, \\our{} requires about $2\\times$ less memory than Transformer.\nAlthough we compare 3B-size models here, the reduction ratio becomes larger as the number of layers increases.\n\n\\begin{figure*}[t]\n\\centering\n\\captionsetup{type=figure}\n\\includegraphics[width=0.64\\textwidth]{figure/cachemem-size.pdf}\n\\caption{GPU memory consumption of KV cache for each token with different model size. \\our{} can save more for larger model size.}\n\\label{fig:cache:mem:per:token}\n\\end{figure*}\n\n\\Cref{fig:cache:mem:per:token} reports the GPU memory consumption of KV cache for each token.\nAs \\our{} only caches one layer of global key-value pairs, it needs roughly $L$ times fewer memory compared to Transformer.\nFor example, \\our{} can serve 128K tokens with 1GB GPU memory, while Transformer with GQA~\\cite{gqa} can only support 1.6K tokens at 65B model size.\n\n\\begin{figure*}[t]\n\\centering\n\\captionsetup{type=figure}\n\\includegraphics[width=0.75\\textwidth]{figure/prefilling-length.pdf}\n\\caption{Prefilling latency for different length, i.e., the encoding time of given input prompt before generating the first token. Transformer's time grows quadratically while \\our{}'s grows linearly.\nEven for a short input length, such as 32K, \\our{} can still accelerate $2.87\\times$.\n}\n\\label{fig:prefilling}\n\\end{figure*}\n\n\\mypara{Prefilling Latency}\nIn the prefill stage, the model encodes input tokens in parallel.\nAs shown in \\Cref{fig:prefilling}, the prefilling latency is a pain point of user experience for long-context models.\nFor 512K- and 1M-length input sequences, Transformer needs about 180 seconds and 300 seconds, respectively.\nThe computational complexity of Transformer is $\\mathcal{O}(N^2)$, which requires a large number of FLOPs for long context.\nIn contrast, \\our{}'s prefilling time is $\\mathcal{O}(N)$, growing linearly (\\Cref{sec:infer:adv}) along the sequence length.\n\n\\Cref{fig:prefilling} shows that \\our{} reduces the Transformer prefilling time from 180 seconds to less than 6 seconds for 512K context.\nAs described in \\Cref{sec:infer:adv}, the prefill stage can early exit before entering cross-decoder. So, there is at least two times speedup of prefilling latency even for short context.\nFor example, \\our{} is $2.87\\times$ faster than Transformer for 32K length.\n\n\\begin{figure*}[t]\n\\centering\n\\captionsetup{type=figure}\n\\includegraphics[width=0.6\\textwidth]{figure/throughput-length.pdf}\n\\caption{Inference throughput of Transformer and \\our{} varying the context length.}\n\\label{fig:throughput}\n\\end{figure*}\n\n\\mypara{Throughput}\nThe throughput indicates how many tokens the model can process per second, involving both pre-filling and generation time.\n\\Cref{fig:throughput} shows that \\our{} achieves higher throughput across context lengths compared to Transformer.\nFor the example of 512K queries, Transformer's throughput is 4.5 token/s while \\our{} reaches 43.1 token/s, i.e, achieving $9.6\\times$ speedup.\nThe throughput is improved for the following reasons.\nFirst, \\our{} decreases the time required for prefilling as previously demonstrated.\nSecond, as the memory consumption is reduced, we can use larger batch size for inference, which also contributes to the throughput improvement.\n\n\\section{Conclusion}\n\nIn this work, we propose a decoder-decoder architecture (\\our{}) for large language modeling.\n\\our{} achieves significantly better inference efficiency and competitive performance compared with Transformers.\nExperimental results demonstrate that \\our{} achieves favorable results for large language models under various settings, i.e., scaling up number of training tokens, scaling up model size, and scaling up context length to 1M tokens.\nProfiling results also show that \\our{} improves inference efficiency by orders of magnitude, especially for long-sequence modeling.\n\nThe work can be advanced from the following perspectives:\n\\begin{itemize}[leftmargin=*]\n\\setlength\\itemsep{0.01em}\n\\item \\textbf{\\our{} + BitNet + Groq.} Groq achieves very high throughput by putting all things within SRAM. However, the memory capacity bottleneck limits the model size and input token count. Now, hundreds of chips are connected to host just one model. As a solution, \\our{} reduces KV cache memory, and BitNet reduces model weight memory. The LLM deployment cost is expected to be reduced by orders of magnitude using the above combination.\n\\item \\textbf{\\our{} for Multimodal Large Language Models.} The \\our{} layout is general to the use of multiple self-decoders. The cross-attention layers are natural for multimodal fusion~\\cite{vlmo,beit3}. The causal dependency of self-decoders also perfectly fits in streaming video. The async multimodal large language models can avoid different data steams block each other, which is critical for real-time applications, such as robotics.\n\\item \\textbf{Optimized Mechanism for KV Cache Module.} \\Cref{fig:yoco} explicitly highlights KV cache, which opens up new opportunities to develop native memory mechanisms.\nFirst, we can integrate a cache compression mechanism to obtain more compact memory. Second, we can build an index~\\cite{longmem} for efficient key-value retrieval. As \\our{} reuses caches, it enables us to maintain only one index rather than creating an index for each layer. Third, the disentangled modeling supports pre-caching context, which is potentially useful for native RAG and LLM-native search engines.\n\\end{itemize}\n\n\\section*{Acknowledgement}\n\nWe would like to acknowledge Ben Huntley for maintaining the GPU cluster.\nThe long-sequence training utilizes \\texttt{CUBE}, which is an internal version of \\cite{cube}.\nWe implement the Triton kernel of gated retention based on \\texttt{FLA}~\\cite{fla}.\n\n\\newpage\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2212.14052v3.tex",
        "arXiv-2312.06635v6.tex",
        "arXiv-2405.05254v2.tex"
    ],
    "group_id": "group_39",
    "response": "### Summary of Research Papers on Advanced Language Modeling Architectures\n\n#### Title: Advanced Language Modeling Architectures: State Space Models, Gated Linear Attention, and Decoder-Decoder Designs\n\n#### Introduction\nThe field of large language models (LLMs) has seen significant advancements in recent years, with architectures like the Transformer~\\cite{vaswani2017attention} becoming the standard for sequence modeling tasks. Despite their success, these models face challenges in terms of computational efficiency, especially when dealing with long sequences. The quadratic complexity of attention mechanisms in Transformers leads to high memory usage and slow inference times, which are critical bottlenecks for scaling up models and handling longer input sequences. This has motivated researchers to explore alternative architectures that can maintain or improve performance while being more efficient.\n\nState space models (SSMs) and linear attention mechanisms have emerged as promising alternatives to the traditional softmax attention in Transformers. SSMs, which are inspired by linear time-invariant systems, can model sequences in a recurrent manner, allowing for efficient inference and extrapolation beyond the training sequence length. Linear attention, on the other hand, simplifies the attention mechanism to a linear form, enabling parallel training and faster inference. However, both SSMs and linear attention have faced challenges in matching the performance of Transformers on natural language tasks, particularly in terms of handling long sequences and maintaining high throughput during training and inference.\n\nThis summary explores three recent papers that address these challenges by introducing new architectures and training methods for language models. The first paper introduces the \\hthree (Hungry Hungry Hippos) layer, which combines two SSMs with multiplicative interactions to enhance the model's ability to recall and compare tokens. The second paper presents a hardware-efficient algorithm for linear attention, called \\textsc{FlashLinearAttention}, and applies it to a gated linear attention (GLA) Transformer, which uses data-dependent gating mechanisms to improve performance. The third paper proposes a decoder-decoder architecture, \\our{} (You Only Cache Once), which reduces the memory footprint and inference time by reusing key-value (KV) caches and employing efficient self-attention modules.\n\n#### Main Content of Each Paper\n\n**Paper 1: Hungry Hungry Hippos (H3) Layer to Model Discrete Sequences**\n\nThe first paper focuses on understanding the expressivity gap between SSMs and attention mechanisms in language modeling tasks. It introduces the \\hthree layer, which combines two SSMs (one shift and one diagonal) with multiplicative interactions to enable associative recall and token comparison. The \\hthree layer is designed to address the limitations of existing SSMs, which struggle with recalling earlier tokens and comparing tokens across the sequence. The paper demonstrates that \\hthree can match the performance of attention on synthetic language tasks and almost closes the gap with Transformers on natural language tasks, achieving perplexity (PPL) scores within 0.4 points of Transformers on the OpenWebText dataset. Furthermore, a hybrid model that retains two attention layers in a 125M-parameter \\hthree-attention architecture outperforms Transformers by 1.0 PPL.\n\nTo improve the efficiency of training SSMs, the paper proposes \\fastfft, a hierarchical algorithm that uses fused block FFTConv and a state-passing mechanism to exploit modern hardware accelerators. \\fastfft allows \\hthree to scale to billion-parameter models, achieving lower perplexity and outperforming Transformers in zero- and few-shot learning on the SuperGLUE benchmark. The hybrid \\hthree-attention model is also found to be 2.4 times faster in inference compared to Transformers.\n\n**Paper 2: Gated Linear Attention Transformers with Hardware-Efficient Training**\n\nThe second paper addresses the underperformance of linear attention mechanisms in language modeling tasks and proposes a hardware-efficient algorithm for linear attention, called \\textsc{FlashLinearAttention}. This algorithm is faster than \\textsc{FlashAttention-2} even on short sequences (e.g., 1K). The paper then introduces a gated linear attention (GLA) Transformer, which incorporates data-dependent gating mechanisms into linear attention to improve performance. The GLA Transformer is found to perform competitively against strong Transformer baselines, such as Transformer++ and RetNet, on moderate-scale language modeling experiments.\n\nThe GLA Transformer is particularly effective at length generalization and recall-intensive tasks, achieving better results than other linear attention models. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model. The paper also presents a chunkwise parallel form of GLA that enables efficient training by balancing FLOPs against parallelism, while still allowing the use of half-precision matmuls to take advantage of tensor core units on modern GPUs.\n\n**Paper 3: You Only Cache Once (YOCO) Architecture for Large Language Models**\n\nThe third paper introduces a decoder-decoder architecture, \\our{}, which only caches key-value pairs once, significantly reducing the memory footprint and inference time for large language models. \\our{} consists of a self-decoder that generates global KV caches, and a cross-decoder that reuses these caches via cross-attention. The architecture is designed to be scalable in terms of model size, training tokens, and context length, while maintaining competitive performance.\n\nThe paper evaluates \\our{} on various tasks, including needle retrieval and language modeling, and demonstrates that it achieves comparable results to well-tuned Transformer models. \\our{} is also found to be scalable, achieving favorable results across different model sizes and training token counts. The architecture is particularly effective at handling long sequences, with experiments showing that it can serve up to 1M tokens without running out of GPU memory. Additionally, \\our{} reduces the prefilling latency and increases inference throughput, making it a strong candidate for future large language models with native long-sequence support.\n\n#### Commonalities and Innovations\n\nAll three papers aim to improve the efficiency and performance of language models, particularly in handling long sequences. They each propose novel architectures or training methods that address specific limitations of existing models.\n\n- **Paper 1** introduces the \\hthree layer, which enhances SSMs with multiplicative interactions to improve their performance on language modeling tasks. The paper also proposes \\fastfft, a hardware-efficient algorithm that allows SSMs to scale to longer sequences and larger model sizes.\n- **Paper 2** develops a hardware-efficient algorithm for linear attention, \\textsc{FlashLinearAttention}, and applies it to a gated linear attention (GLA) Transformer. The GLA Transformer uses data-dependent gating mechanisms to improve performance and is found to be competitive with strong Transformer baselines.\n- **Paper 3** proposes the \\our{} architecture, which reduces the memory footprint and inference time by reusing KV caches and employing efficient self-attention modules. The architecture is designed to be scalable and effective at handling long sequences, while maintaining competitive performance.\n\n#### Comparison of Results\n\nThe results of the three papers demonstrate the effectiveness of their proposed methods in improving the performance and efficiency of language models.\n\n- **Paper 1** shows that the hybrid \\hthree-attention model outperforms Transformers on the OpenWebText dataset by 1.0 perplexity points and is up to 2.4 times faster in inference.\n- **Paper 2** reports that the GLA Transformer performs competitively against Transformer++ and RetNet on moderate-scale language modeling tasks, with improved results on recall-intensive tasks.\n- **Paper 3** demonstrates that \\our{} achieves comparable performance to well-tuned Transformer models on various tasks, while significantly reducing the memory footprint and inference time. For example, \\our{} can serve up to 1M tokens with only 12.4GB of GPU memory, compared to 9.4 times the memory required by Transformers.\n\n#### Conclusion\n\nThe three papers collectively demonstrate significant progress in understanding and narrowing the gap between state space models (SSMs) and attention mechanisms in language modeling. They introduce novel architectures and training methods that improve the performance and efficiency of language models, particularly in handling long sequences. The \\hthree layer and \\fastfft algorithm in Paper 1 enhance SSMs to match the performance of attention, while Paper 2 develops a hardware-efficient algorithm for linear attention that incorporates data-dependent gating mechanisms. Paper 3 proposes the \\our{} architecture, which reduces the memory footprint and inference time by reusing KV caches and employing efficient self-attention modules.\n\nThese advancements suggest that future research could focus on further optimizing these architectures for larger models and datasets, as well as exploring their application in multimodal and real-time scenarios. Additionally, integrating cache compression mechanisms and building efficient indexes for KV caches could further enhance the performance and efficiency of these models.\n\n#### Future Research Directions\n\n- **\\hthree + BitNet + Groq**: Combining \\hthree with BitNet and Groq could reduce the deployment cost of large language models by orders of magnitude, as it would allow for larger models and input token counts to be hosted on fewer chips.\n- **\\our{} for Multimodal Large Language Models**: The \\our{} architecture could be adapted for multimodal fusion and streaming video applications, where it could avoid different data streams blocking each other, improving real-time performance.\n- **Optimized Mechanism for KV Cache Module**: Developing native memory mechanisms for KV caches, such as cache compression and efficient retrieval indexes, could further enhance the performance and efficiency of \\our{} and similar architectures.\n\nThese directions offer promising avenues for future research to continue improving the efficiency and scalability of language models, making them more accessible and practical for a wide range of applications."
}