{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{LawBench: Benchmarking Legal Knowledge of Large Language Models}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\n\nLarge language models (LLMs) have demonstrated strong capabilities in various aspects. However, when applying them to the highly specialized, safe-critical legal domain, it is unclear how much legal knowledge they possess and whether they can reliably perform legal-related tasks. To address this gap, we propose a comprehensive evaluation benchmark \\emph{LawBench}. LawBench has been meticulously crafted to have precise assessment of the LLMs' legal capabilities from three cognitive levels: (1) \\emph{Legal knowledge memorization}: whether LLMs can memorize needed legal concepts, articles and facts; (2) \\emph{Legal knowledge understanding}: whether LLMs can comprehend entities, events and relationships within legal text; (3) \\emph{Legal knowledge applying}: whether LLMs can properly utilize their legal knowledge and make necessary reasoning steps to solve realistic legal tasks. LawBench contains 20 diverse tasks covering 5 task types: single-label classification (SLC), multi-label classification (MLC), regression, extraction and generation. We perform extensive evaluations of 51 LLMs on LawBench, including 20 multilingual LLMs, 22 Chinese-oriented LLMs and 9 legal specific LLMs. The results show that GPT-4 remains the best-performing LLM in the legal domain, surpassing the others by a significant margin. While fine-tuning LLMs on legal specific text brings certain improvements, we are still a long way from obtaining usable and reliable LLMs in legal tasks. All data, model predictions and evaluation code are released in \\url{https://github.com/open-compass/LawBench/}. We hope this benchmark provides in-depth understanding of the LLMs' domain-specified capabilities and speed up the development of LLMs in the legal domain.\n\n\\end{abstract}\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{Figure/law_bench_radar_plot_zero_shot_new.pdf}\n    \\caption{Results (zero-shot) of six best-performing LLMs evaluated on 20 diverse legal tests covering three cognitive dimensions: \\textcolor[HTML]{004995}{legal knowledge memorization}, \\textcolor[HTML]{499500}{understanding}, and \\textcolor[HTML]{e17300}{applying}.}\n    \\label{fig:law_bench_radar_plot}\n\\end{figure}\n\n\\section{Introduction}\nThe artificial intelligence community has witnessed notable progress in the large language models (LLMs) recently. The latest large language models, such as GPT-4~\\cite{openai2023gpt4} and LLaMA~\\cite{touvron2023llama}, have showcased remarkable capabilities and intricate skills that are comparable to, and in some cases, even surpass human capabilities~\\cite{huang2023chatgpt}.\nTo quantitatively assess the capabilities of LLMs, evaluation benchmarks serve a pivotal role in their development. Numerous recent endeavors mainly concentrate on enhancing general and universal capabilities, such as understanding world knowledge and executing complex reasoning~\\cite{zhong2023agieval,yu2023kola}.\n\nPrevious research on evaluation has predominantly concentrated on exploring the diverse aspects of the general capabilities of LLMs. SuperGLUE~\\cite{wang2019superglue} is crafted to assess various facets of language understanding, encompassing reading comprehension, commonsense reasoning, and entailment. Benchmarks such as MMLU~\\cite{hendrycks2020measuring} and BIG-bench~\\cite{srivastava2023beyond} have been introduced to cover a wide range of NLP tasks for LLM evaluations. Furthermore, there exist benchmarks~\\cite{fu2023chain,suzgun2022challenging,fu2023codeapex,chen2021evaluating} explicitly designed to scrutinize the advanced abilities of LLMs that manifest with scale, such as reasoning~\\cite{sawada2023arb} and coding~\\cite{lu2021codexglue}. Beyond these specific benchmarks, more recent initiatives, like OpenCompass~\\cite{2023opencompass}  and HELM~\\cite{liang2022holistic}, aspire to offer a comprehensive perspective on the capabilities of LLMs.\n\nHowever, evaluating Large Language Models (LLMs) necessitates not only a focus on general capabilities but also the incorporation of domain-specific benchmarks for assessing models specialized in particular fields~\\cite{zhao2023domain}. Legal tasks encompass a broad spectrum of applications, predominantly text-based, necessitating comprehension and interpretation of highly professional legal texts. Currently, they are primarily conducted by legal experts, who require years of extensive specialized training to process legal cases. Endowing LLMs with legal expertise can not only improve the working efficiency of legal officers, but also address the overwhelming demand of legal assistance from non-professionals, and thereby improve public access to justice~\\cite{cui2022survey,trozze2023large}. Consequently, we prioritize establishing benchmarks to measure the legal knowledge of existing LLMs.\n\nIn contrast to assessments that primarily focus on testing model's ability to pass legal bar exams~\\cite{bommarito2022gpt,zhong2023agieval},  which are not always representative of the actual use-cases for LLMs~\\footnote{As shown in \\cite{chalkidis2022lexglue}, ChatGPT excels in passing legal bar exams but struggles at performing realistic legal tasks.}, our emphasis is on examining a structured set of legal skills required in real-world scenarios. In the legal domain, there have been works ensembling legal-related tasks including the lexglue~\\cite{chalkidis2022lexglue} focusing on EU and American laws, and LBOX OPEN~\\cite{hwang2022multi} focusing on South Korean laws, \\cite{chalkidis2023chatgpt} further transformed lexglue into zero-shot forms to test the ability of LLMs to complete these tasks. LEGALBENCH~\\cite{guha2023legalbench}, in particular, presented the first steps towards constructing an interdisciplinary collaborative legal reasoning benchmark for the English language and evaluated 20 LLMs in 162 legal tasks. \n\nNonetheless, legal systems vary significantly among different countries, highlighting the importance of establishing different standards for each legal system. The Chinese legal system is rooted in the civil law family. Unlike the common law system, which is widely accepted in the U.S. and the U.K., judges in the civil law system are obliged to stay neutral, respect the established statutory law articles and ground their decisions on them. Understanding and applying existing statutes and codes, rather than studies of precedents, are of paramount importance~\\cite{zheng1986china}. Therefore, it is necessary to design a separate set of evaluation tasks to emphasize the required skill set for the Chinese law system. \n\nWith this in mind, we present \\emph{LawBench}:  a meticulously crafted, comprehensive evaluation benchmark to assess the LLMs’ capabilities on performing legal-related tasks under the Chinese civil-law system. LawBench consists of 20 diverse tasks following 5 categories:  single-label classification (SLC), multi-label classification (MLC), regression, extraction and generation.\nWe divided these tasks into 3 skill levels according to widely accepted Bloom's cognitive models~\\cite{krathwohl2002revision}: (1) Legal knowledge memorization: whether LLMs can memorize needed legal concepts, articles and facts; (2) Legal knowledge understanding: whether LLMs can comprehend entities, events and relationships within legal text; (3) Legal knowledge applying: whether LLMs can properly utilize their legal knowledge and make necessary reasoning steps to solve realistic legal tasks. Intuitively LLMs must first obtain lower-level skills before excelling in higher-level skills. This division method provides a structured overview of the skill set required for legal-related tasks. It can also facilitates an exploration of the similarities\nand differences between the learning mechanisms of LLMs and humans.\n\nWe extensively tested 51 popular LLMs,  including 20 multilingual LLMs, 22 Chinese-oriented LLMs and 9 legal specific LLMs. To effectively extract answers from the predictions of LLM generations, we design suitable rules, regular expressions and metrics for every individual task. The benchmark and evaluation code are integrated into the OpenCompass platform~\\cite{2023opencompass} to enable easy reproduction. The evaluation results are shown in Figure \\ref{fig:law_bench_radar_plot}. We find that although legal specific fine-tuning usually improves upon their base model, they are still significantly lagging behind general LLMs, which occupy the top six spots in the averaged zero-shot performance. We analyze the impact of various factors on the results, such as supervised fine-tuning (SFT), reinforcement learning from human feedback (RLHF)~\\cite{christiano2017deep}, model size, legal specific fine-tuning, etc. Important suggestions are summarized to better guide the future development of legal LLMs for the Chinese community.\n\n\\section{Related Work}\n\\paragraph{Large Language Models}\nLarge language models (LLMs) trained on massive amounts of data have shown impressive abilities in generating high-quality, coherent text and following zero-shot or few-shot instructions in a diverse set of tasks~\\cite{openai_chatgpt,openai2023gpt4,chowdhery2022palm,touvron2023llama1,touvron2023llama}. These models are usually trained following three steps: pre-training, supervised fine-tuning (SFT) and alignment with human or AI feedback ~\\cite{christiano2017deep,shen2017estimation,wei2021finetuned,ouyang2022training,lee2023rlaif,casper2023open} etc. As most public available LLMs focus on training on English corpora, many efforts have been devoted to extending LLMs to Chinese. These works either pre-training a new LLM from scratch on Chinese-centric corpora~\\cite{su2022welm,zeng2022glm,du2022glm,sun2023moss,2023internlm}, or expanding the vocabulary of an existing English-centric LLM then performing SFT on Chinese instruction data~\\cite{chinese-llama-alpaca,BELLE,belle2023exploring}. \nThere have also been studies that are dedicated to adapting LLMs to the legal domain by fine-tuning on legal specific corpora~\\cite{cui2023chatlaw,huang2023lawyer,yue2023disclawllm}. However, a comprehensive evaluation to compare the existing LLMs regarding their legal knowledge is still lacking. Our focus is primarily on models that can complete corresponding tasks based on instructions, so we exclude pre-trained small language models such as Lawformer~\\cite{xiao2021lawformer} and LegalBERT~\\cite{chalkidis2020legal}, which require task-specific fine-tuning to perform competiviely. \n\n\\paragraph{Existing Benchmarks}\n\nAs the rapid development of LLMs, conventional approaches of evaluating a model's performance on a single task through fine-tuning~\\cite{xu2020clue,wang2018glue,louis2022statutory,zhang2022mdia,shen2023xpqa} is no longer adequate for evaluating LLMs. A growing body of research works have recently focused on developing more comprehensive and systematic benchmarks to evaluate various capabilities of LLMs. Examples include AGIEval~\\cite{zhong2023agieval} which covers human-centric standardized exams, such as college entrance exams, law school admission tests, math competitions, and lawyer qualification tests, HELM~\\cite{liang2022holistic} which measures 7 metrics (accuracy, calibration, robustness,\nfairness, bias, toxicity, and efficiency) for each of 16 core scenarios to the extent possible, KOLA~\\cite{yu2023kola} which focuses on knowledge-oriented assessment under four-level taxonomy of knowledge-related abilities, and MMBench~\\cite{liu2023mmbench} which is designed specifically to evaluate vision-language models. There have also been efforts in constructing benchmarks for Chinese such as CMMLU~\\cite{li2023cmmlu}, GAOKAO~\\cite{zhang2023evaluating} and C-Eval~\\cite{huang2023c}. Some of them focus on specific domains such as CMB~\\cite{wang2023cmb} in the medical domain and Fin-eval~\\cite{zhang2023fineval} in the finance domain. In the legal domain, there have also been works ensembling legal-related tasks including the lexglue~\\cite{chalkidis2022lexglue} focusing on EU and American laws, and LBOX OPEN~\\cite{hwang2022multi} focusing on South Korean laws, but they did not formulate tasks into the instruction-following formats for LLMs. Recently, legal-bench was released in evaluating LLMs on 162 legal-related tasks based on American laws~\\cite{guha2023legalbench}. Our work follows a similar line to extend the evaluation to Chinese laws. Given the uniqueness of the Law system in People's Republic of China and the diversity of the legal tasks covered in this work, we believe\nthat LawBench will contribute to the multilinguality of global legal research and promote research in specializing language models to a specific domain.\n\n\\section{LawBench Construction}\nIn this section, we provide a detailed description of the principles behind the design of LawBench and the test task selection.\n\n\\subsection{The Hierarchical Ability Taxonomy of LawBench}\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{Figure/lawbench_cognitive_level_new_2.pdf}\n    \\caption{Three cognitive dimensions for evaluating large language models in LawBench. In order to specialize in legal tasks, LLMs must be able to (1) memorize necessary legal concepts, terminologies, articles and facts; (2) understand entities, events and relationships in legal text; and finally (3) simulate law professionals to apply legal knowledge and necessary reasoning in solving realistic tasks.}\n    \\label{fig:cognitive}\n\\end{figure}\n\nWhen evaluating large language models, there is a preference for using a variety of tasks to assess their capabilities. A hierarchical evaluation system allows us to better understand the abilities of large language models in different aspects. Instead of categorizing tasks solely based on their difficulty~\\cite{huang2023c}, we refer to the widely used Bloom's cognitive model~\\cite{krathwohl2002revision} to classify tasks into different dimensions~\\cite{yu2023kola}. Bloom's Taxonomy system was initially proposed by the educational psychologist Benjamin Bloom and his collaborators in 1956 and has been widely applied and developed in the following decades. It has effectively aided teachers in curriculum design and the assessment of student learning outcomes. Bloom's Taxonomy divides learning objectives in the cognitive domain into six levels, from the lowest to the highest: Remember, Understand, Apply, Analyze, Evaluate, and Create. These levels describe the depth and complexity of cognitive learning and provide an organized framework. Teachers can use Bloom's Taxonomy to ensure diversity and completeness in course objectives. By combining learning objectives at different levels, comprehensive student development can be promoted, encouraging them to progress from simple memorization and understanding to higher-level analysis, evaluation, and creation.\n\nInspired by this classification approach, we simplified Bloom's cognitive hierarchy model and kept the first three categories in Bloom's taxonomy to assess the legal knowledge of LLMs:\n\n\\begin{enumerate}\n    \\item \\textbf{Knowledge Memorization}: The memorization level measures the basic requirement of remembering legal-related knowledge. It tests LLMs' ability in memorization and recitation of basic legal-domain knowledge such as regulations, cases, concepts, common sense, legal facts and terminologies.\n    \\item \\textbf{Knowledge Understanding}: The understanding level involves understanding the meanings and connotations of legal documents. This includes the ability to comprehend and interpret legal concepts, text, and issues, e.g., identifying entities and relationships within legal texts, detect types of legal issues and points of dispute, among others.\n    \\item \\textbf{Knowledge Applying}: The applying level requires LLMs to integrate legal knowledge, reason over it and address real-world legal cases. It covers the model's logical reasoning abilities to perform legal consultation, judicial assistance, as well as numerical reasoning abilities to calculate involved amount.\n\\end{enumerate}\n\nNote that under this taxonomy, some tasks may not strictly belong to just one category; they may involve other abilities as well. We have categorized these tasks based on their primary capabilities.\n\n\\begin{table}[htbp]\n  \\centering\n  \\small\n  \\resizebox{\\columnwidth}{!}{\n    \\begin{tabular}{llllcc}\n    \\toprule\n    \\textbf{Cognitive Level} & \\textbf{ID} & \\textbf{Task} & \\textbf{Data Source} & \\textbf{Metric} & \\textbf{Type} \\\\\n    \\midrule\n    \\multirow{2}{*}{\\shortstack[l]{\\textbf{Legal Knowledge}\\\\ \\textbf{Memorization}}} & 1-1 & Article Recitation & FLK & Rouge-L & Generation \\\\\n          & 1-2 & Knowledge Question Answering & JEC\\_QA & Accuracy & SLC \\\\\\midrule\n    \\multirow{10}{*}{\\shortstack[l]{\\textbf{Legal Knowledge}\\\\ \\textbf{Understanding}}} & 2-1 &  Document Proofreading & CAIL2022  & F0.5 & Generation\\\\\n    & 2-2 & Dispute Focus Identification & LAIC2021  & F1 & MLC\\\\\n          & 2-3 & Marital Disputes Identification & AIStudio & F1 & MLC\\\\\n          & 2-4  & Issue Topic Identification & CrimeKgAssitant & Accuracy & SLC\\\\\n        & 2-5 & Reading Comprehension & CAIL2019  & rc-F1 & Extraction\\\\\n          & 2-6 & Named-Entity Recognition & CAIL2022  & soft-F1 & Extraction\\\\\n          & 2-7 & Opinion Summarization & CAIL2021  & Rouge-L & Generation\\\\\n          & 2-8 & Argument Mining  & CAIL2022  & Accuracy &SLC\\\\\n          & 2-9 & Event Detection  & LEVEN  & F1 &MLC\\\\\n          & 2-10 & Trigger Word Extraction & LEVEN  & soft-F1 & Extraction\\\\\n          \\midrule\n    \\multirow{8}{*}{\\shortstack[l]{\\textbf{Legal Knowledge}\\\\ \\textbf{Applying}}} & 3-1 & Fact-based Article Prediction & CAIL2018 & F1 & MLC\\\\\n          & 3-2 & Scene-based Article Prediction & LawGPT & Rouge-L &Generation\\\\\n          & 3-3 & Charge Prediction & CAIL2018 & F1 &MLC\\\\\n          & 3-4 & Prison Term Prediction w.o. Article & CAIL2018 & nLog-distance &Regression\\\\\n          & 3-5 & Prison Term Prediction w. Article & CAIL2018 & nLog-distance &Regression\\\\\n          & 3-6 & Case Analysis & JEC\\_QA & Accuracy &SLC\\\\\n          & 3-7 & Criminal Damages Calculation & LAIC2021  & Accuracy &Regression \\\\\n          & 3-8 & Consultation & hualv.com & Rouge-L &Generation\\\\\n    \\bottomrule\n    \\end{tabular}\n    }\n    \\caption{\\small Task list in LawBench. Tasks correspond to cognitive dimensions: legal knowledge memorization, understanding and applying, and 5 task types: generation, single-label classification (SLC), multi-label classification (MLC), regression, and extraction.}\n  \\label{tab:tasks}%\n\\end{table}%\n\n\\subsection{Data Source and Selected Tasks}\nWe selected 20 tasks falling under the above-mentioned capability levels. Every task is assigned a unique task id for better distinction. The task list is provided in Table~\\ref{tab:tasks}. There can exist datasets belonging to the same tasks. When selecting the dataset for every task, we choose the most recent available version. Furthermore, certain tasks like legal case retrieval requires processing very long documents, which can surpass the length limit for most LLMs, so we do not include them to LawBench for now. When constructing LawBench, we have made efforts to format the prompts in a way that best aligns with user habits with clear instructions about the answer format, so that we can assess the ability of LLMs in assisting legal tasks in realistic scenarios.\n\n\\textbf{Legal Knowledge Memorization Tasks}\n\nLegal knowledge memorization tasks examine to which extent large language models encode legal knowledge within their parameters. While this knowledge can be enhanced with external retrievers, it is still beneficial to memorize necessary legal knowledge because (1) There is currently no reliable mechanism to guarantee the accurate retrieval of legal provisions. Memorizing useful knowledge within model parameters can help combat the noise from external retrievers~\\cite{xie2023adaptive,neeman2023disentqa}; (2) It is very difficult, if not impossible, to retrieve all needed legal knowledge for complicated reasoning tasks. The model must know basic legal concepts to connect the retrieved knowledge smoothly~\\cite{shen2022product,ziems2023large,yu2023generate}; (3) Relying on the parametric knowledge instead of external retrievers can significantly reduce the online latency~\\cite{shen2022semipqa,shen2022low,tay2022transformer}. \n\nThere are two major types of legal knowledge that requires memorizing: (1) core law articles and regulation content and (2) other fundamental legal concepts, notions and rules. We construct two tasks corresponding to these two types of knowledge: \n\\begin{itemize}\n    \\item \\textbf{Article recitation} (1-1): \\emph{Given a law article number, recite the article content}. We collected the contents of laws and regulations from the national database~\\footnote{\\url{https://flk.npc.gov.cn/}} and consulted students with a legal background to select 152 sub-laws under the 5 core laws.\n    We further incorporated updated laws and regulations, including constitutional amendments, to evaluate the model's ability to comprehend legal changes.\n    \\item \\textbf{Knowledge question answering} (1-2): \\emph{Given a question asking about basic legal knowledge, select the correct answer from 4 candidates}. We collect knowledge-based questions from the JEC-QA tasks~\\cite{zhong2020jec}. To simplify the process of locating answers during the test, we exclusively chose single-label questions from them.\n\\end{itemize}\nExamples of these two tasks are in Appendix~\\ref{app:instruction_memorization}.\n\n\\textbf{Legal Knowledge Understanding Tasks}\n\nLegal knowledge understanding tasks examine to which extent large language models can comprehend entities, events, and relationships within legal texts. Understanding legal text is a pre-condition to utilize the knowledge in concrete downstream applications~\\cite{cui2022survey}. In total, we selected 10 tasks corresponding to different levels of legal knowledge understanding:\n\n\\begin{itemize}\n    \\item \\textbf{Document Proofreading} (2-1): \\emph{Given a sentence extracted from legal documents, correct its spelling, grammar and ordering mistakes, return the corrected sentence}. Legal documents, as carriers of judicial authorities and the exercise of legal rights by citizens, demand utmost precision in their textual content. We sample the original and corrected legal sentences from the CAIL2022 document proofreading task. Possible mistake types are inserted into the instructions to let the model directly output the corrected sentence.\n    \\item \\textbf{Dispute Focus Identification} (2-2): \\emph{Given the original claims and responses of the plaintiff and defendant, detect the points of dispute}. In civil cases, the points of dispute represent the core of conflicts, intersection of contradictions, and issues over which the parties involved in the case are in contention. The automated recognition and detection of points of contention have practical significance and necessity for the development of the rule of law in our country. Specifically, we will provide the trial-related content from judgment documents, including the sections on claims and responses. The cases involve various legal matters such as civil loans, divorce, motor vehicle traffic accident liability, financial loan contracts, and more. We have carefully selected common types of points of contention from LAIC2021 to construct this test set.\n    \\item \\textbf{Marital Disputes Identification} (2-3): \\emph{Given a sentence describing marital disputes, classify it into one of the 20 pre-defined dispute types}. Marital disputes refer to the total sum of various disputes arising from love, marriage, and divorce. Among civil disputes, marital disputes are a common type of dispute.  We have selected a publicly available marriage text classification dataset on AiStudio~\\footnote{\\url{https://aistudio.baidu.com/datasetdetail/181754}}. This dataset consists of 20 categories, and a single text entry may have multiple labels.\n    \\item \\textbf{Issue Topic Identification} (2-4): \\emph{Given a user inquiry, assign it into one of pre-defined topics}. User inquiries are typically vague. Identifying the relevant topics in legal consultations can help legal professionals better pinpoint key issues. We obtain the data from the CrimeKgAssistant project~\\footnote{\\url{https://github.com/liuhuanyong/CrimeKgAssitant}}. We keep the most frequent 20 classes and sample 25 questions for each class to form our final test set.\n    \\item \\textbf{Reading Comprehension} (2-5): \\emph{Given a judgement document and a corresponding question, extract relevant content from it to answer the question}. Judicial documents contain rich case information, such as time, location, and character relationships. Intelligently reading and comprehending judicial documents through large language models can assist judges, lawyers, and the general public in obtaining the necessary information quickly and conveniently. We use the CAIL2019 reading comprehension dataset to build this task, removing question types related to binary and unanswerable questions. We retain single and multiple-segment data as our test set.\n\n    \\item \\textbf{Named-Entity Recognition} (2-6): \\emph{Given a sentence from a judgement document, extract entity information corresponding to a set of pre-defined entity types such as suspect, victim or evidence}. We sampled 500 examples from the CAIL2022 Information Extraction dataset as our test set. These 500 samples contain 10 entity types related to theft crimes.\n\n    \\item \\textbf{Opinion Summarization} (2-7): \\emph{Given a legal-related public news report, generate a concise summary}. Legal summaries typically include key facts of the case, points of contention, legal issues, legal principles applied, and the judgment's outcome. It can provide a quick overview of the case content to improve the efficiency of legal professionals. \nWe randomly select 500 samples from the CAIL2021 Legal Public Opinion Summary dataset for this task. We only select samples with less than 400 words to fit the length constraint of LLMs.\n\n    \\item \\textbf{Argument Mining} (2-8): \\emph{Given a plaintiff's perspective and five candidate defendant's viewpoints, select one viewpoint that can form a point of dispute with the plaintiff's perspective}. In court's trial process, judgment documents play a crucial role in recording the arguments and evidence presented by both the plaintiff and the defendant. Due to differences in their positions and perspectives, as well as inconsistencies in their factual statements, disputes arise between the plaintiff and the defendant during the trial process. These points of contention are the key to the entire trial and the essence of judgment documents. This task aims to extract valuable arguments and supporting materials from a large volume of legal texts, providing strong support for legal debates and case analysis. We use CAIL2022's Argument Mining dataset to construct our dataset, transforming the identification of focal points of disputes into a multiple-choice question format.\n    \\item \\textbf{Event Detection} (2-9):\n\\emph{Given a sentence from a legal judgement document, detect which events are mentioned in this sentence}. Events are the essence of facts in legal cases. Therefore, Legal Event Detection is fundamentally important and naturally beneficial to case understanding and other Legal AI tasks. We construct the test set from the LEVEN dataset\\cite{yao2022leven} by sampling sentences corresponding to the top 20 most frequent event types. Multiple events can be mentioned in every sentence.\n    \\item \\textbf{Trigger Word Extraction} (2-10):\n\\emph{Given a sentence from a legal judgment document and its corresponding events, predict which words in the sentence triggered these events}. Trigger words directly cause events and are an important feature that determines the event category, providing post-hoc explanation for the event types we identify. Directly identifying trigger words is very difficult, so we simplified this task by providing the events contained in the text along with the text information,  examining the ability of LLMs to recognize trigger words related to events. When constructing the trigger word test set, we removed trigger words that were the same as the event type, as well as events with multiple or duplicate trigger words from the LEVEN dataset\\cite{yao2022leven}, to include as different trigger words as possible.\n\\end{itemize}\nExamples of the 10 understanding tasks are in Appendix~\\ref{app:instruction_understanding}.\n\n\\textbf{Legal Knowledge Applying Tasks}\n\nLegal knowledge applying tasks primarily examine the ability of LLMs to not only understand legal knowledge but also simulate law professionals to apply the knowledge in solving realistic legal tasks. In the task design, we extensively examine the model's different reasoning abilities, including 3 legal content reasoning tasks: legal judgement prediction, case analysis, consultation, and 1 numerical reasoning task: criminal damages calculation. \n\nWhen predicting case judgments, judges follow a certain order when hearing a case~\\cite{zhong2018legal,huang2021dependency}. Therefore, in constructing the case judgment prediction task, we simulated this process by decomposing the CAIL2018 dataset into three tasks: fact-based article prediction (3-1), charge prediction (3-3) and prison term prediction. We further separate the task of prison term prediction into two scenarios: without article content (3-4) and with article content (3-5) to examine LLMs' capability in utilizing the article content to make accurate judgement predictions. Besides, we also add the task scene-based fact prediction  to simulate judges’\nrecognition of legal provisions (3-2).\n\n\\begin{itemize}\n    \\item \\textbf{Fact-based Article Prediction} (3-1): \\emph{Given a fact statement from the legal judgement document, predict which article items should be applied}.\nWhen judges make decisions, they usually associate relevant articles with the facts of the case~\\cite{ge2021learning,louis2023finding}. Article prediction can assist judges in quickly locating legal articles related to legal texts. Legal articles are written expressions of legal norms, which are rules and regulations with clear meanings and legal effects. \nThe model needs to deduce potentially applicable legal provisions based on the given case description and related background information. We sample 500 cases from the CAIL2018 dataset for this task.\n\n    \\item \\textbf{Scene-based Article Prediction} (3-2): \\emph{Given a described scenario and a related question, predict the corresponding article item}.\nThe CAIL2018 dataset only covers criminal law-related legal provisions. In order to comprehensively evaluate the ability of LLMs to analyze case facts and infer relevant legal provisions, we collected high-quality legal scenario-based question-and-answer data from public sources on GitHub\\cite{LAWGPT-zh}. This dataset was generated by inputting legal provisions into chatGPT to construct corresponding scenario-based questions and answers. We manually selected 5,000 question-and-answer pairs with accurate answers from the generated dataset. Based on this, we selected 252 core legal provisions' scenario-based question-and-answer content as the test dataset.\n\n    \\item \\textbf{Charge Prediction} (3-3): \\emph{Given fact statement from the legal judgement document and the applied article number, predict the cause of action (charge)}.\nCause of action is a summary of the nature of the legal relationship involved in a litigation case, as formulated by the people's court. Accurately predicting the cause of action can help improve judicial efficiency and fairness. In the process of filing and hearing cases, accurate prediction of the cause of action can help the court to allocate cases, allocate resources, and arrange trials, thereby improving judicial efficiency and fairness. \nWe sampled 500 pieces of data from the CAIL2018 cause of action prediction dataset for this task.\n    \\item \\textbf{Prison Term Prediction w.o. Article} (3-4): \\emph{Given fact statement from the legal judgement document, the applied article number and charge, predict the prison term.}\nPrison term prediction refers to the process of predicting and estimating the possible sentence that a defendant may face during the criminal justice process based on the facts of the case, legal provisions, and relevant guiding precedents. It aims to make reasonable inferences about the length and form of the sentence by comprehensively considering various factors such as the nature of the crime, the circumstances of the offense, the social impact, and the defendant's personal situation. \nWe used the prison term prediction dataset from CAIL2018, removed some cases with the death penalty and life imprisonment, and randomly sampled 500 cases as the test dataset. During the process of judges' sentencing, more information is usually taken into account to determine the prison term outcome. We simulated the judge's analysis process by providing the relevant legal provisions and the charge of the case.\n    \\item \\textbf{Prison Term Prediction w. Article} (3-5):\n\\emph{Given fact statement from the legal judgement document, the applied article content and charge, predict the prison term.}\nLarge language models typically use retrieval-argument methods to introduce new information. Some publicly available models also include retrieval modules that provide detailed reference information for the model by retrieving legal provisions. We simulated this process, and unlike the previous task where only the legal provision number was provided, we provided the specific content of the legal provision in this task. When constructing the sentence prediction dataset, we appended the content of the legal provisions to the end of the question, allowing the model to complete the sentence prediction task in this scenario.\n    \\item \\textbf{Case Analysis} (3-6): \\emph{Given a case and a corresponding question, select the correct answer from 4 candidates}. We use the case analysis part from JEC\\_QA dataset~\\cite{zhong2020jec} for this task. The case analysis part tests the ability of models to analyze real cases. Models must possess five types of reasoning in order to perform this analysis including word matching, concept understanding, numerical analysis, multi-paragraph reading, and multi-hop reasoning. In order to reduce the difficulty of the test and facilitate the acquisition of answers, we sampled 500 multiple-choice questions from the JEC\\_QA Case-Analysis part as the testing dataset.\n    \\item \\textbf{Criminal Damages Calculation} (3-7): \\emph{Given a fact description about a criminal process, predict the amount of money involved in this case}.\nThere are some numerical computing tasks in the process of judicial trials, such as the calculation of the total amount of legal crimes. The total amount of the crime is an important sentencing factor. In some charges such as theft, financial fraud, and bribery, China's laws determine the severity of the sentence based on the amount involved in the case. This task mainly tests the computing ability of LLMs. First, we examine whether the model understands the rules of case amount calculation, and second, we examine whether the model can accurately complete numerical calculations. We selected the LAIC2021 numerical computing task to construct our dataset.\n    \\item \\textbf{Consultation} (3-8):\n\\emph{Given a user consultation, generate a suitable answer.}\nLegal consultation is a way for the public to access legal services. Legal consultation can help people understand legal disputes and seek targeted advice and solutions from professional lawyers, as well as receive support and guidance. Some law firms and legal consulting companies also provide online legal consultation services, making it more convenient for people to obtain legal help. \nWe collected legal consultation contents from the Hualv website~\\footnote{\\url{www.66law.com}}, and our dataset contains both the answers to legal consultations and the corresponding legal basis, i.e., legal articles.\n\\end{itemize}\n\nExamples of the 8 applying tasks are in Appendix~\\ref{app:instruction_application}.\n\n\\subsection{Evaluation}\nFor every task,  the evaluation is done following two steps: (1) answer extraction, which extracts the answer from the model prediction, and (2) metric computation, which computes the metric score based on the question, extracted answer and the gold answer. Answer extraction is a necessary step since many LLMs often do not generate output directly comparable with gold labels~\\cite{adlakha2023evaluating}. We explain these two steps in detail in the following section.\n\\paragraph{Answer Extraction}\nMost of the tasks require the prediction to be in the standard format in order to compare with the ground truth, we define a set of task-specific rules to extract the answer from the model prediction.\n\\begin{itemize}\n    \\item Article Number Extraction (3-1): this type of tasks requires us to extract the article numbers predicted by the model. To do this, we use the delimiter \\begin{CJK*}{UTF8}{gbsn} ``、'' \\end{CJK*} to separate the prediction text into chunks of text, and then the cn2an\\footnote{\\url{https://github.com/Ailln/cn2an}} library is used to convert the Chinese numerals to Arabic numerals within each of those chunks. Using a regular expression, we extract the converted Arabic numerals as the expected article numbers; if more than one number appears in the same chunk, only the first number is extracted. All extracted numbers are combined to form the final set of predictions.\n    \\item Prison Term Extraction (3-4, 3-5): for this type of tasks, we need to extract the predicted prison terms from the prediction text. To begin, we use cn2an to convert all the Chinese numerals in the prediction to Arabic numerals; we then extract digits that are followed by time intervals in Chinese, such as \\begin{CJK*}{UTF8}{gbsn} ``个月'' (month) and ``年'' (year)\\end{CJK*}. The extracted prison terms are normalized to months, meaning that the numbers appearing before \\begin{CJK*}{UTF8}{gbsn} ``年'' \\end{CJK*} will be multiplied by 12. Note that the time unit in the ground truth answer is also month.\n    \\item Criminal Damages Extraction (3-7): We extract all numbers appearing in the prediction text using regular expression. The set of of the extracted numbers is considered as the predicted criminal damages.\n    \\item Named-Entity Recognition (2-6): We find all occurrences of entity types from the model prediction, then obtain the substring from its occurrence to the delimiter ``\\textbackslash n'', then apply a regular expression to extract the entity value.\n    \\item Trigger Word Extraction (2-10): We split the model prediction by the delimiter \\begin{CJK*}{UTF8}{gbsn} ``；'' \\end{CJK*}, then treat the split array as a list of extracted key words.\n    \\item Option Extraction (1-2, 2-2, 2-3, 2-4, 2-8, 2-9, 3-3): this type of task is similar to selecting the correct options from a list of options in a multiple-choice task. We run through all possible options and check if they appear in the prediction text. The set of options that occur in the prediction text is collected and used for evaluation.\n    \\item Others (1-1, 2-1, 2-5, 2-7, 3-2, 3-8): we take the model prediction as the answer without performing any extraction step. \n    \n\\end{itemize}\n\n\\paragraph{Metrics}\nAfter the answer extraction phase, we compute the final metric based on the extracted answer. We defined 7 different metrics in total to measure different types of tasks:\n\\begin{itemize}\n    \\item \\textbf{Accuracy}: Accuracy is a binary score that performs exact match between the model prediction and the gold answer. This applies to all single-label classification tasks including task 1-2, 2-4, 2-8, 3-6, and the regression task 3-7. For SLC tasks, if multiple valid answers are extracted from the model prediction, then we always treat it as wrong~\\footnote{For the criminal damages calculation task, we treat the model prediction correct as long as one of the extracted answers match the ground truth as we find LLMs often output the whole calculation process.}.\n    \\item \\textbf{F1}: When there are multiple output labels, F1 score measures the harmonic mean of the precision and recall. This applies to all multi-label classification tasks including task 2-2, 2-3, 2-9, 3-1 and 3-3.\n    \\item \\textbf{rc-F1}: rc-F1 is the F1 score tailored for the reading comprehension task 2-5. It treats every token as a label, removes punctuation, stories, extra whitespace, performs other necessary normalizations then compute the F1 score. We adopt the official script from CAIL2019 to compute the instance-level rc-F1 score~\\footnote{\\url{https://github.com/china-ai-law-challenge/CAIL2019/tree/master}}.\n    \\item \\textbf{soft-F1}: For extraction tasks 2-6 and 2-10, the output is a set of phrases. Instead of using the standard F1 score, we use a soft version by replacing the phrase-level exact match with the rc-F1 score, then computing the F1 on top of it. We find using the soft version helpful since LLMs often use wording choices different from the ground truth.\n    \\item \\textbf{nLog-distance}: For the prison term prediction tasks 3-4 and 3-5, we evaluate them with the normalized log distance (nLog-distance) to capture the continuity of prison terms. We compute the logarithm of the difference between the extracted and gold answer, then normalize it to the space between 0 and 1 for better compatibility with other metrics.\n    \\item \\textbf{F0.5}: For the document proofreading task 2-1, we use the F0.5 metric to evaluate it. The F0. 5 score gives more weight to precision than to recall we want to prevent introducing more false positives than identify every other error in proofreading~\\cite{zhang2022mucgec}. We use the ChERRANT toolkit to align the extracted and gold answer before computing the F0.5 score~\\footnote{\\url{https://github.com/HillZhang1999/MuCGEC/tree/main/scorers/ChERRANT}}. As the alignment can take too long to respond for very bad generations, we add a time-out of 10 seconds. If a time-out happened, then the prediction is assigned a score of 0.\n    \\item \\textbf{Rouge-L}: For other generation tasks 1-1, 2-7, 3-3 and 3-8, we use the Rouge-L score to evaluate them. Rouge-L is a commonly used metric in generation tasks. It takes into account sentence-level structure similarity naturally and identifies longest co-occurring in sequence n-grams automatically to compare the extracted and gold answers~\\cite{lin2004rouge}.\n\\end{itemize}\nSeveral large language models may decline to respond to legal-related inquiries due to security policies or simply fail to follow the instructions. To capture this issue, we also report the \\textbf{abstention rate} of LLMs in each task (how often an LLM abstains to answer). An abstention happens if an answer cannot be extracted from the model prediction. The abstention rate does not apply to task 2-5 and all generation tasks since they do not need the answer extraction step.\n\n\\section{Experiment}\n\\begin{table*}[t]\n    \\centering\n    \\resizebox{13.5cm}{!}{\n    \\begin{tabular}{l|cccccc}\n        \\toprule\n        \\textbf{Model} & \\textbf{Parameters} & \\textbf{SFT} & \\textbf{RLHF} &\\textbf{Access} & \\textbf{BaseModel} &\\\\ \n        \\midrule\n        \\textbf{Multilingual LLMs} \\\\\n        MPT & 7B & \\xmark & \\xmark & Weights & - \\\\\n        MPT-Instruct & 7B & \\cmark & \\xmark & Weights & MPT-7B \\\\\n        LLaMA & 7/13/30/65B & \\xmark & \\xmark & Weights & - \\\\\n        LLaMA-2 & 7/13/70B & \\cmark & \\xmark & Weights & - \\\\\n        LLaMA-2-Chat & 7/13/70B & \\cmark & \\cmark & Weights & LLaMA-2-7/13/70B \\\\\n        Alpaca-v1.0 & 7B & \\cmark & \\xmark & Weights & LLaMA-7B \\\\\n        Vicuna-v1.3 & 7/13/33B & \\cmark & \\xmark & Weights & LLaMA-7/13/33B \\\\\n        WizardLM & 7B & \\cmark & \\xmark & Weights & LLaMA-7B \\\\\n        StableBeluga2 & 70B & \\cmark & \\xmark & Weights & LLaMA-2-70B \\\\\n        ChatGPT & N/A & \\cmark & \\cmark & API & - \\\\\n        GPT-4 & N/A & \\cmark & \\cmark & API & - \\\\\n        \\midrule\n        \\textbf{Chinese-oriented LLMs} \\\\\n        MOSS-Moon & 16B & \\xmark & \\xmark & Weights & - \\\\\n        MOSS-Moon-SFT & 16B & \\cmark & \\xmark & Weights & MOSS-Moon \\\\\n        TigerBot-Base & 7B & \\xmark & \\xmark & Weights & - \\\\\n        TigerBot-SFT & 7B & \\cmark & \\xmark & Weights & TigerBot-Base \\\\\n        GoGPT & 7B & \\cmark & \\xmark & Weights & LLaMA-7B \\\\\n        ChatGLM2 & 6B & \\cmark & \\xmark & Weights & ChatGLM \\\\\n        Ziya-LLaMA & 13B & \\cmark & \\cmark & Weights & LLaMA-13B \\\\\n        Baichuan & 7/13B & \\xmark & \\xmark & Weights & - \\\\\n        Baichuan-13B-Chat & 13B & \\cmark & \\xmark & Weights & Baichuan-13B \\\\\n        XVERSE & 13B & \\xmark & \\xmark & Weights & - \\\\\n        InternLM & 7B & \\xmark & \\xmark & Weights & - \\\\\n        InternLM-Chat & 7B & \\cmark & \\xmark & Weights & InternLM-7B \\\\\n        InternLM-Chat-8K & 7B & \\cmark & \\xmark & Weights & InternLM-7B \\\\\n        Qwen & 7B & \\xmark & \\xmark & Weights & - \\\\\n        Qwen-Chat & 7B & \\cmark & \\xmark & Weights & Qwen-7B \\\\\n        Yulan-Chat-2 & 13B & \\cmark & \\xmark & Weights & LLaMA-2-13B \\\\\n        BELLE-LLaMA-2 & 13B & \\cmark & \\xmark & Weights & LLaMA-2-13B \\\\\n        Chinese-LLaMA-2 & 7B & \\cmark & \\xmark & Weights & LLaMA-2-7B \\\\\n        Chinese-Alpaca-2 & 7B & \\cmark & \\xmark & Weights & LLaMA-2-7B \\\\\n        LLaMA-2-Chinese & 7/13B & \\cmark & \\xmark & Weights & LLaMA-2-7/13B \\\\\n        \\midrule\n        \\textbf{Legal Specific LLMs} \\\\\n        HanFei & 7B & \\cmark & \\xmark & Weights & HanFei \\\\\n        LaWGPT-7B-beta1.0 & 7B & \\cmark & \\xmark & Weights & Chinese-LLaMA \\\\\n        LaWGPT-7B-beta1.1 & 7B & \\cmark & \\xmark & Weights & Chinese-alpaca-plus-7B \\\\\n        LexiLaw & 6B & \\cmark & \\xmark & Weights & ChatGLM-6B \\\\\n        Wisdom-Interrogatory & 7B & \\cmark & \\xmark & Weights & Baichuan-7B \\\\\n        Fuzi-Mingcha & 6B & \\cmark & \\xmark & Weights & ChatGLM-6B \\\\\n        Lawyer-LLaMA & 13B & \\cmark & \\xmark & Weights & LLaMA \\\\\n        ChatLaw & 13/33B & \\cmark & \\xmark & Weights & Ziya-LLaMA-13B/Anima-33B\\\\\n        \\bottomrule\n       \n    \\end{tabular}}\n    \\caption{\\small LLMs tested on LawBench. We classify these models by their main training corpora.}\n    \\label{tab:models}\n\\end{table*}\n\n\\subsection{Models}\nWe evaluate a wide spectrum of large language models of various sizes, grouping them into three major categories based on their pre-training and fine-tuning domains: multilingual LLMs, Chinese-oriented LLMs and legal specific LLMs. We provide a short review over them in the following section. The detailed model list is shown in Table~\\ref{tab:models}.\n\n\\paragraph{Multilingual LLMs} We consider 18 open-source multilingual models: MPT-7B, MPT-Instruct-7B, LLaMA-7B / 13B / 30B / 65B, LLaMA-2-7B / 13B / 70B, LLaMA-2-Chat-7B / 13B / 70B, Alpaca-v1.0-7B, Vicuna-v1.3-7B / 13B / 33B, WizardLM-7B, StableBeluga2. In addition, two commercial models, ChatGPT and GPT-4, are included.\n\n\\paragraph{Chinese Oriented LLMs} A number of Chinese-oriented LLMs are proposed to enhance Chinese comprehension. Their typically perform better than multilingual models on Chinese NLP tasks. We include 21 open-sourced, Chinese-oriented LLMs in our evaluation: CMOSS-Moon / Moon-SFT, TigerBot-Base / SFT, GoGPT, ChatGLM2-6B, Ziya-LLaMA-13B, Baichuan-7B / 13B / 13B-Chat, XVERSE-13B, InternLM-7B / Chat-7B / Chat-7B-8K, Qwen-7B / 7B-Chat, BELLE-LLaMA-2, Chinese-LLaMA-2-7B, Chinese-Alpaca-2-7B, LLaMA-2-Chinese-7B / 13B.\n\n\\paragraph{Legal Specific LLMs} Certain Chinese-oriented LLMs are further fine-tuned on Chinese corpus in legal domain to improve LLMs' understanding of Chinese laws. They are of particular interest to us; through our benchmark, we can rigorously gauge their true advance compared to general-purpose LLMs and identify their limitations. Here, we provide detailed descriptions of these models: \n\\begin{itemize}\n    \\item ChatLaw\\cite{cui2023chatlaw}: ChatLaw-13B is fine-tuned based on Ziya-LLaMA-13B-v1, ChatLaw-33B is fine-tuned based on Anima-33B.\n    \\item LaywerLLaMA\\cite{huang2023lawyer}: based on Chinese-LLaMA-13B, fine-tuned with general and legal instructions.\n    \\item FuziMingcha\\cite{fuzi.mingcha}: based on ChatGLM, fine-tuned with unsupervised Chinese judicial texts along with supervised legal fine-tuning datasets.\n    \\item HanFei\\cite{HanFei2023}: a fully pre-trained and fine-tuned legal model with 7 billion parameters.\n    \\item LaWGPT\\cite{LaWGPT2023}: LaWGPT-7B-beta1.0 is further pre-trained on 500k Chinese judgment documents upon Chinese-LLaMA-7B and fine-tuned based on the Legal-Base-7B with instructions. LaWGPT-7B-beta1.1 is fine-tuned based on the Chinese-alpaca-plus-7B with 350k legal Q\\&A datasets.\n    \\item LexiLaw\\cite{LexiLaw2023}: a fine-tuned Chinese legal model based on the ChatGLM-6B with legal datasets. \n    \\item WisdomInterrogatory\\cite{WisdomInterrogatory2023}: a further pre-trained and fine-tuned model built upon Baichuan-7B.\n\\end{itemize}\n\n\\subsection{Experiment Setting}\nWe employ OpenCompass~\\cite{2023opencompass} to perform model inference. For both ChatGPT and GPT-4, we set the temperature at 0.7 and top $p$ to 1. For other chat models, we tailor the prompt using prefixes and suffixes specific to each model. Greedy decoding is performed during generation for all open-sourced models. We set the input token length limit to 2048 and an output token length to 1024. Right truncation is performed for input prompts exceeding the length limitation. We evaluate all models in both zero-shot and one-shot settings. The model input in zero-shot inference is merely the task instruction and the query (see Appendix \\ref{appendix:details_of_task_intruction} for individual instructions and queries). To build the model input for one-shot inference, a single example including the query and corresponding answer is attached after the instruction, followed by the actual query to the model.\n\n\\subsection{Main Results}\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{Figure/picture_2_5.pdf}\n    \\caption{\\small Average performance (zero-shot) of 51 LLMs evaluated on LawBench.}\n    \\label{fig:overall_performance_one_shot_main_content}\n\\end{figure}\n\n\\begin{table*}\n\\centering\n\\small\n \\resizebox{\\columnwidth}{!}{\n\\begin{tabular}{l|cccc|cccc|cccccccc}\n\\toprule\n\\multicolumn{1}{l|}{\\multirow{3}*{Task}} & \\multicolumn{4}{c}{Multilingual LLMs} & \\multicolumn{4}{c}{Chinese Oriented LLMs} & \\multicolumn{4}{c}{Legal Specific LLMs} \\\\\n\\multicolumn{1}{l|}{} & \\multicolumn{2}{c}{GPT-4} & \\multicolumn{2}{c}{ChatGPT} & \\multicolumn{2}{c}{Qwen-Chat}  & \\multicolumn{2}{c}{InternLM-Chat-8K} & \\multicolumn{2}{c}{Fuzi-Mingcha} & \\multicolumn{2}{c}{ChatLaw-13B} \\\\\n\\multicolumn{1}{l|}{} & \\multicolumn{1}{c}{0-shot} & \\multicolumn{1}{c}{1-shot} & 0-shot & 1-shot & \\multicolumn{1}{c}{0-shot} & \\multicolumn{1}{c}{1-shot} & \\multicolumn{1}{c}{0-shot} & \\multicolumn{1}{c|}{1-shot} & \\multicolumn{1}{c}{0-shot} & \\multicolumn{1}{c}{1-shot} & \\multicolumn{1}{c}{0-shot} & \\multicolumn{1}{c}{1-shot} \\\\ \n\\midrule\n1-1 & \\cellcolor{Green!0.0}15.38 & \\cellcolor{Green!0.0}17.21 & \\cellcolor{Green!0.0}15.86 & \\cellcolor{Green!0.0}16.15 & \\cellcolor{Green!0.0}18.54 & \\cellcolor{Green!0.0}17.73 & \\cellcolor{Green!0.0}15.45 & \\cellcolor{Green!0.0}15.16 & \\cellcolor{Green!0.0}25.22 & \\cellcolor{Green!0.0}20.21 & \\cellcolor{Green!0.0}14.85 & \\cellcolor{Green!0.0}15.98 \\\\\n1-2 & \\cellcolor{Green!0.2}55.2 & \\cellcolor{Green!0.2}54.8 & \\cellcolor{Green!0.0}36.0 & \\cellcolor{Green!0.0}37.2 & \\cellcolor{Green!0.2}34.0 & \\cellcolor{Green!0.0}28.6 & \\cellcolor{Green!0.0}40.4 & \\cellcolor{Green!0.2}40.6 & \\cellcolor{Green!11.4}7.8 & \\cellcolor{Green!5.8}12.8 & \\cellcolor{Green!0.4}28.4 & \\cellcolor{Green!0.0}29.4 \\\\\n\\hline\n2-1 & \\cellcolor{Green!0.0}12.53 & \\cellcolor{Green!0.0}18.31 & \\cellcolor{Green!0.0}9.1 & \\cellcolor{Green!0.0}13.5 & \\cellcolor{Green!0.0}22.56 & \\cellcolor{Green!0.0}25.16 & \\cellcolor{Green!0.0}22.64 & \\cellcolor{Green!0.0}21.64 & \\cellcolor{Green!0.0}4.93 & \\cellcolor{Green!0.0}2.86 & \\cellcolor{Green!0.0}12.22 & \\cellcolor{Green!0.0}13.01 \\\\\n2-2 & \\cellcolor{Green!0.0}41.65 & \\cellcolor{Green!0.0}46.0 & \\cellcolor{Green!3.0}32.37 & \\cellcolor{Green!2.2}40.6 & \\cellcolor{Green!28.4}27.42 & \\cellcolor{Green!38.4}27.4 & \\cellcolor{Green!6.8}35.46 & \\cellcolor{Green!10.0}36.6 & \\cellcolor{Green!23.4}19.59 & \\cellcolor{Green!21.6}2.4 & \\cellcolor{Green!11.6}2.68 & \\cellcolor{Green!17.0}9.0 \\\\\n2-3 & \\cellcolor{Green!0.0}69.79 & \\cellcolor{Green!0.0}69.99 & \\cellcolor{Green!1.0}51.73 & \\cellcolor{Green!0.4}54.01 & \\cellcolor{Green!19.4}31.42 & \\cellcolor{Green!21.8}32.96 & \\cellcolor{Green!14.2}28.96 & \\cellcolor{Green!16.6}30.91 & \\cellcolor{Green!27.2}28.46 & \\cellcolor{Green!42.4}17.44 & \\cellcolor{Green!27.0}42.24 & \\cellcolor{Green!47.6}30.91 \\\\\n2-4 & \\cellcolor{Green!1.6}44.0 & \\cellcolor{Green!3.0}44.4 & \\cellcolor{Green!3.4}41.2 & \\cellcolor{Green!2.8}41.4 & \\cellcolor{Green!13.0}35.0 & \\cellcolor{Green!4.0}31.2 & \\cellcolor{Green!7.8}35.6 & \\cellcolor{Green!6.0}33.2 & \\cellcolor{Green!29.4}18.6 & \\cellcolor{Green!38.6}8.8 & \\cellcolor{Green!15.8}27.6 & \\cellcolor{Green!12.6}26.6 \\\\\n2-5 & \\cellcolor{Green!0.0}56.5 & \\cellcolor{Green!0.0}64.8 & \\cellcolor{Green!0.0}53.75 & \\cellcolor{Green!0.0}61.98 & \\cellcolor{Green!0.0}48.48 & \\cellcolor{Green!0.0}46.71 & \\cellcolor{Green!0.0}54.13 & \\cellcolor{Green!0.0}54.35 & \\cellcolor{Green!0.0}97.59 & \\cellcolor{Green!0.0}93.35 & \\cellcolor{Green!0.0}39.11 & \\cellcolor{Green!0.0}41.41 \\\\\n2-6 & \\cellcolor{Green!0.0}76.6 & \\cellcolor{Green!0.0}79.96 & \\cellcolor{Green!0.0}69.55 & \\cellcolor{Green!0.0}74.04 & \\cellcolor{Green!0.0}37.88 & \\cellcolor{Green!0.0}57.34 & \\cellcolor{Green!0.0}17.95 & \\cellcolor{Green!0.0}26.86 & \\cellcolor{Green!0.0}44.07 & \\cellcolor{Green!0.0}42.28 & \\cellcolor{Green!0.0}54.89 & \\cellcolor{Green!0.0}60.68 \\\\\n2-7 & \\cellcolor{Green!0.0}37.92 & \\cellcolor{Green!0.0}40.52 & \\cellcolor{Green!0.0}33.49 & \\cellcolor{Green!0.0}40.68 & \\cellcolor{Green!0.0}36.04 & \\cellcolor{Green!0.0}42.58 & \\cellcolor{Green!0.0}27.11 & \\cellcolor{Green!0.0}30.56 & \\cellcolor{Green!0.0}54.32 & \\cellcolor{Green!0.0}31.43 & \\cellcolor{Green!0.0}38.45 & \\cellcolor{Green!0.0}42.71 \\\\\n2-8 & \\cellcolor{Green!0.0}61.2 & \\cellcolor{Green!0.0}59.0 & \\cellcolor{Green!0.0}36.4 & \\cellcolor{Green!0.0}37.4 & \\cellcolor{Green!0.0}24.0 & \\cellcolor{Green!0.0}26.8 & \\cellcolor{Green!0.0}36.2 & \\cellcolor{Green!0.0}30.6 & \\cellcolor{Green!20.6}8.8 & \\cellcolor{Green!1.4}11.4 & \\cellcolor{Green!1.0}18.6 & \\cellcolor{Green!1.8}20.2 \\\\\n2-9 & \\cellcolor{Green!2.2}78.82 & \\cellcolor{Green!2.6}76.55 & \\cellcolor{Green!4.8}66.48 & \\cellcolor{Green!5.4}67.59 & \\cellcolor{Green!20.2}44.88 & \\cellcolor{Green!6.4}50.63 & \\cellcolor{Green!10.6}62.93 & \\cellcolor{Green!4.4}63.42 & \\cellcolor{Green!2.4}16.9 & \\cellcolor{Green!23.2}21.26 & \\cellcolor{Green!1.0}31.74 & \\cellcolor{Green!3.2}40.27 \\\\\n2-10 & \\cellcolor{Green!0.0}65.09 & \\cellcolor{Green!0.0}65.26 & \\cellcolor{Green!0.0}39.05 & \\cellcolor{Green!0.0}40.04 & \\cellcolor{Green!0.0}18.9 & \\cellcolor{Green!0.0}21.27 & \\cellcolor{Green!0.0}20.94 & \\cellcolor{Green!0.0}20.69 & \\cellcolor{Green!0.0}7.78 & \\cellcolor{Green!0.0}7.04 & \\cellcolor{Green!0.0}14.56 & \\cellcolor{Green!0.0}17.37 \\\\\n\\hline\n3-1 & \\cellcolor{Green!0.4}52.47 & \\cellcolor{Green!0.4}53.2 & \\cellcolor{Green!1.2}29.5 & \\cellcolor{Green!1.2}30.81 & \\cellcolor{Green!8.4}44.62 & \\cellcolor{Green!3.4}52.86 & \\cellcolor{Green!7.0}34.86 & \\cellcolor{Green!7.4}38.88 & \\cellcolor{Green!0.0}25.19 & \\cellcolor{Green!0.2}3.86 & \\cellcolor{Green!5.8}33.28 & \\cellcolor{Green!0.4}25.99 \\\\\n3-2 & \\cellcolor{Green!0.0}27.54 & \\cellcolor{Green!0.0}33.15 & \\cellcolor{Green!0.0}31.3 & \\cellcolor{Green!0.0}34.49 & \\cellcolor{Green!0.0}33.5 & \\cellcolor{Green!0.0}34.49 & \\cellcolor{Green!0.0}19.11 & \\cellcolor{Green!0.0}28.7 & \\cellcolor{Green!0.0}22.18 & \\cellcolor{Green!0.0}32.96 & \\cellcolor{Green!0.0}31.55 & \\cellcolor{Green!0.0}33.96 \\\\\n3-3 & \\cellcolor{Green!25.0}41.99 & \\cellcolor{Green!23.8}41.3 & \\cellcolor{Green!29.0}35.52 & \\cellcolor{Green!32.4}34.55 & \\cellcolor{Green!24.6}40.67 & \\cellcolor{Green!26.2}39.91 & \\cellcolor{Green!23.0}41.05 & \\cellcolor{Green!22.6}42.25 & \\cellcolor{Green!5.4}55.93 & \\cellcolor{Green!11.6}43.6 & \\cellcolor{Green!35.4}27.9 & \\cellcolor{Green!26.0}12.24 \\\\\n3-4 & \\cellcolor{Green!0.4}82.62 & \\cellcolor{Green!0.4}83.21 & \\cellcolor{Green!3.8}78.75 & \\cellcolor{Green!5.6}77.12 & \\cellcolor{Green!7.6}76.74 & \\cellcolor{Green!6.0}78.47 & \\cellcolor{Green!21.8}63.21 & \\cellcolor{Green!18.8}67.74 & \\cellcolor{Green!2.4}77.23 & \\cellcolor{Green!1.4}78.95 & \\cellcolor{Green!3.8}76.18 & \\cellcolor{Green!5.6}74.31 \\\\\n3-5 & \\cellcolor{Green!0.4}81.91 & \\cellcolor{Green!0.4}82.74 & \\cellcolor{Green!4.0}76.84 & \\cellcolor{Green!5.4}73.72 & \\cellcolor{Green!7.4}77.19 & \\cellcolor{Green!11.6}73.92 & \\cellcolor{Green!16.6}67.2 & \\cellcolor{Green!13.0}71.1 & \\cellcolor{Green!2.0}75.52 & \\cellcolor{Green!1.2}79.0 & \\cellcolor{Green!5.0}73.57 & \\cellcolor{Green!5.4}73.01 \\\\\n3-6 & \\cellcolor{Green!0.0}48.6 & \\cellcolor{Green!0.0}49.6 & \\cellcolor{Green!0.6}27.4 & \\cellcolor{Green!0.0}31.6 & \\cellcolor{Green!0.4}26.8 & \\cellcolor{Green!0.0}26.8 & \\cellcolor{Green!0.0}34.2 & \\cellcolor{Green!0.0}36.2 & \\cellcolor{Green!29.2}7.0 & \\cellcolor{Green!20.0}13.8 & \\cellcolor{Green!0.6}28.8 & \\cellcolor{Green!0.0}26.8 \\\\\n3-7 & \\cellcolor{Green!0.4}77.6 & \\cellcolor{Green!0.2}77.0 & \\cellcolor{Green!0.4}61.2 & \\cellcolor{Green!0.0}66.4 & \\cellcolor{Green!0.4}42.0 & \\cellcolor{Green!0.0}44.6 & \\cellcolor{Green!0.6}43.8 & \\cellcolor{Green!0.8}44.0 & \\cellcolor{Green!0.8}47.2 & \\cellcolor{Green!0.0}38.2 & \\cellcolor{Green!6.0}41.4 & \\cellcolor{Green!3.6}42.0 \\\\\n3-8 & \\cellcolor{Green!0.0}19.65 & \\cellcolor{Green!0.0}19.9 & \\cellcolor{Green!0.0}17.45 & \\cellcolor{Green!0.0}17.17 & \\cellcolor{Green!0.0}19.32 & \\cellcolor{Green!0.0}20.39 & \\cellcolor{Green!0.0}13.37 & \\cellcolor{Green!0.0}12.11 & \\cellcolor{Green!0.0}16.64 & \\cellcolor{Green!0.0}13.95 & \\cellcolor{Green!0.0}17.17 & \\cellcolor{Green!0.0}16.72 \\\\\n\\hline\nAVG &  \\cellcolor{Green!1.5}52.35 & \\cellcolor{Green!1.6}53.85 & \\cellcolor{Green!2.6}42.15 & \\cellcolor{Green!2.8}44.52 & \\cellcolor{Green!6.5}37.00 & \\cellcolor{Green!5.9}38.99 & \\cellcolor{Green!5.4}35.73 & \\cellcolor{Green!5.0}37.28 & \\cellcolor{Green!7.7}33.05 & \\cellcolor{Green!8.4}28.78 & \\cellcolor{Green!5.7}32.76 & \\cellcolor{Green!6.2}32.63 \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\begin{tabular}{ccccccc}\n\t\\multicolumn{2}{c}{\\%abstention\\qquad} & \\cellcolor{Green!0.0}\\quad0\\% \\quad & \\cellcolor{Green!25}\\quad25\\% \\quad & \\cellcolor{Green!50}\\quad50\\% \\quad &\n\t\\cellcolor{Green!75}\\quad75\\% \\quad & \\cellcolor{Green!100}\\quad100\\% \\quad \\\\\n\\end{tabular}\n\\caption{\\small Model performance of top two performing systems from each category. qwen-chat and InternLM-chat both have 7B parameters. Cells are colored according to model abstention rate. Further results are in Appendix \\ref{appendix:details_of_results}. It can be observed that the performance of one-shot surpasses that of zero-shot.}\n\\label{tab:top_2_zero_shot_performance}\n\\end{table*}\n\nFigure \\ref{fig:overall_performance_one_shot_main_content} shows the overall zero-shot performance of each model. As can be seen, GPT-4 and ChatGPT clearly lead the benchmark, substantially outperform all other models. Under the same model size (7B-13B), Chinese oriented LLMs outperform multilingual models such as MPT and Llama by a significant margin, confirming the effectiveness of pre-training and fine-tuning on Chinese data. Interestingly, legal specific LLMs do not necessarily outperform general-purpose Chinese oriented LLMs. Close inspection reveals that existing legal specific LLMs are based on rather weak foundation models, implying that improved models may be obtained by fine-tuning a stronger foundation model (see Section \\ref{sec:analysis} for more detail).\n\nIn Table \\ref{tab:top_2_zero_shot_performance}, we demonstrate the top 2 performing LLMs from each category on all tasks, together with the model abstention rate.\\footnote{We present the complete results with the performance achieved by every model on all tasks in Table \\ref{tab:Zero-shot Results on Legal Knowledge Memorization Tasks}-\\ref{tab:zero-shotResults on Overall} (complete zero-shot results), Table \\ref{tab:one-shotResults on legal knowledge memorisation tasks}-\\ref{tab:overall_results} (complete one-shot results), in Appendix \\ref{appendix:details_of_results}.} We made the following observations. First, there is substantial variation in the distribution of scores across tasks. The best-performing models, for example, can achieve a score of more than 60 on tasks 3-4 and 3-5, yet no models manage to exceed 30 on tasks 1-1 and 2-1. This shows that our benchmark adequately assess model capabilities in various aspects. Second, we notice that GPT-4/ChaGPT not only performs well on the majority of tasks, but also has a low abstention rate, suggesting that they are excellent at following instructions and providing responses that are more relevant to the query. Third, it can be observed that GPT models and the Chinese oriented LLMs can successfully leverage the one-shot example and make more accurate predictions compared to zero-shot cases. The top-performing legal specific LLMs, however, suffers a drop in performance after seeing the one-shot example. We hypothesize that due to the fact that they are primarily trained on legal specific instruction data, their instruction-following skills are negatively impacted. Fourth, Fuzi-Mingcha scores 97.59 on task 2-4, whereas all other models score less than 65. Considering its performance on other legal tasks, we suspect that there is a data contamination. This also highlights potential caveats when evaluating LLMs~\\cite{schaeffer2023pretraining}. Overall, it is promising that most LLMs show some capability in handling legal tasks, but there's still a substantial room for improvement. Even the top-performing model, GPT-4, achieves only an average score of 52.35 (zero-shot) / 53.85 (one-shot), highlighting the need for additional efforts in the future.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{Figure/model_size_result_all.pdf}\n    \\caption{\\small Performances of models with various sizes. It can be observed that scaling up the model size usually improves the performance, but the improvement is more consistent in the one-shot than in the zero-shot scenario. }\n    \\label{fig:effects_scale}\n\\end{figure}\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{Figure/slope_plots_3-4_3-5.pdf}\n    \\caption{\\small Comparison between different models for 3-4 and 3-5 tasks. Most LLMs are \\emph{unable} of properly utilizing legal article content information to aid in judgment prediction. Including the article content often degrades the performance.}\n    \\label{fig:effects_article}\n\\end{figure}\n\n\\subsection{Analysis}\n\\label{sec:analysis}\n\nWe find that the current legal specific LLMs do not necessarily outperform general large language models. We analyze the effect of model size and training approach on large language models to better understand which aspects most influence model performance. \n\n\\paragraph{Scaling up the model size results in better performance in one-shot case.}\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{Figure/picture_2_4.pdf}\n    \\caption{\\small Comparison of base LLMs and their SFT and RLHF variants. We put the abstention rate bar on top of the model score bar to visualize the improvement space for (1) correctly following instructions and (2) task-specific precision. The abstention rate score is averaged only on tasks requiring answer extraction.}\n    \\label{fig:effects_sft}\n\\end{figure}\n\nPrior work shows that larger models perform better in general NLP tasks~\\cite{kaplan2020scaling}. We analyze whether this finding still holds in legal domain. Specifically, we select representative models from different categories with varying model sizes and calculate the overall performance and abstention rate of different tasks. The results are shown in Figure \\ref{fig:effects_scale}. We observe that increasing the model size typically helps improve model performance in one-shot settings. Also, the abstention rate becomes lower, indicating that larger models are better at following instructions. Nevertheless, we find that ChatLaw is an outlier, a larger size results in lower performance. In zero-shot scenarios, mixed results are observed, simply increasing the model size may not automatically lead to better performance.\n\n\\paragraph{Most LLMs can not efficiently leverage article content.} \nRetrieval augmentation is a common way to improve the accuracy of generative models~\\cite{lewis2020retrieval,jiang2023active,shen2023neural}. By including the content of the related legal articles in task 3-4 and feeding them into the LLM input, we replicate the retrieval augmentation scenario to form task 3-5. The goal is to see if the model can successfully use this additional knowledge to predict the proper jail sentence when supplied with the relevant article reference that articulates the range of prison terms. We compare 5 models of various types and visualize the comparison between task 3-4 and 3-5 in Figure \\ref{fig:effects_article}. The results show that the vast majority of models \\emph{fail} to make any progress on the jail sentence prediction challenge by using the provided article content. There is a decrease in performance across the board for most models, including GPT-4. This suggests that simple retrieval enhancement methods may not bring further improvement, and how to obtain LLMs that are able to effectively utilize the retrieval information is still an open problem. \n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\columnwidth]{Figure/picture_2_3.pdf}\n    \\caption{\\small Comparison between different legal specific LLMs and their base models. Legal specific fine-tuning significantly improves the performance and reduces the abstention rate.}\n    \\label{fig:effects_law_adapt}\n\\end{figure}\n\n\\paragraph{SFT may improve the performance but RLHF may not.}\n\nWe illustrate the performance difference of 9 models before and after SFT in Figure \\ref{fig:effects_sft}. In most cases, models perform better through SFT. Notably, the SFT data are collected from the general domain, yet they still substantially improve model performance on legal tasks. The LLaMA-2 series further applies RLHF on top of SFT. However, we do find that RLHF-trained models can refuse to answer some questions~\\cite{ouyang2022training} (resulting in a higher abstention rate and lower scores), which can lead to a drop in performance on legal tasks. This suggests that when creating models for legal tasks, stacking RLHF on top of SFT can be counterproductive.\n\n\\paragraph{Legal specific fine-tuning is helpful.} \n\nTo assess the impact of legal specific fine-tuning, we contrast three LLMs fine-tuned for legal tasks with their corresponding base models, as depicted in Figure~\\ref{fig:effects_law_adapt}. It is evident that after legal specific fine-tuning, there is a consistent enhancement of model scores and reduction of abstention rates. This underscores the efficacy of the training strategies employed. Closer inspection of the three cognitive levels reveals that Baichuan-7B and LLaMA-13B perform very poorly on memorizing tasks, which suggests they have not been pre-trained on large, high-quality legal corpora. Nonetheless, fine-tuning them on legal corpora both lead to significant improvement. Even for LLMs like Ziya-LLama-13b who have excellent memorization of legal principles, fine-tuning that is unique to the field of law can significantly boost performance on tests requiring comprehension and applying skills. Comparing to the LawGPT series, we found that version 1.1, which is fine-tuned only on 350k instruction data, outperformed version 1.0 which is fine-tuned on 500k judgement documents followed by 300k instructions. This suggests that beginning with a lower-performing model and undertaking continuous pre-training, which is both time-consuming and resource-intensive, is inferior to starting with a superior foundation model and fine-tuning on high-quality instruction data. Future research may want to try fine-tuning a stronger base model (like qwen-chat or InternLM-chat) for better performance on legal tasks.\n\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\n\n       \n\n\\section{Conclusion}\nUsing LLMs to benefit the legal domain is a promising topic. However, existing benchmarks to measure the legal knowledge of LLMs either focus on a limited subset of tasks, or are based on American laws in English language. This paper presents \\emph{LawBench}: a meticulously crafted, comprehensive evaluation benchmark to assess LLMs in performing legal-related tasks under the Chinese civil law system. We provide  structured taxonomy of the skill set required for legal-related tasks, including 20 diverse tasks corresponding to 3 cognitive dimensions: legal knowledge memorization, understanding and applying. We undertake a thorough examination of 51 LLMs and assess their performance. The results demonstrate that current LLMs are still unable to give meaningful judicial aid, and their scores on most tasks are often poor. While fine-tuning open-source LLMs on legal specific language results in some advances, they still lag far below GPT-4. As the legal field is highly professional, much of the data used in practical applications is confidential. Developing a high-quality large language model for legal tasks necessitates collaboration among multiple institutions. We hope the release of LawBench can serve as a foundation for future research and we seek to encourage cooperation in order to further this effort.\n\n\\section*{Limitations}\n\nThe majority of our datasets are acquired through sampling publicly available data on the internet. Even though we have made efforts to select newest versions of datasets, there can still be risks of test data leakage given that existing LLMs have been exhaustively trained on massive amount of Internet data. It is possible that LLMs explicitly trained on these task formats, or even the exact test data, can exhibit exceptionally high scores~\\cite{schaeffer2023pretraining}. We will seek more principled ways to prevent data contamination in the future.\n\nAnother notable limitation is the answer extraction methods and evaluation metrics for generative tasks. Even though we have hand-engineered task-specific rules to extract the answer, there still can be cases that the rule fails to match. For generative tasks, we only use Rouge-L to evaluate the model predictions for convenience, which cannot fully reflect the human judgement about the answer quality. Currently, there is a lack of automated methods to effectively evaluate model predictions from legal aspects. We plan to consider training an evaluation model tailored for legal tasks in the future, or experiment with LLM-based evaluations~\\cite{liu2023mmbench,yu2023kola}.\n\n\\section*{Acknowledgments}\nThis work is an extension which was supported by National Key R\\&D Program of China (2016YFC0800803).  This work is also partly\nsupported by the National Key R\\&D Program of China\nNo.2022ZD016100 and Shanghai Postdoctoral Excellence\nProgram (No.2022235). We appreciate Zhixin Yin for helping arrange tables and alter chart layouts. We also thank Qi Li for collecting and organizing some public data.\n\n\\clearpage\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{LAiW: A Chinese Legal Large Language Models Benchmark}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\n\nGeneral and legal domain LLMs have demonstrated strong performance in various tasks of LegalAI. However, the current evaluations of these LLMs in LegalAI are defined by the experts of computer science, lacking consistency with the logic of legal practice, making it difficult to judge their practical capabilities. To address this challenge, we are the first to build the Chinese legal LLMs benchmark LAiW, based on the logic of legal practice. To align with the thinking process of legal experts and legal practice (syllogism), we divide the legal capabilities of LLMs from easy to difficult into three levels: basic information retrieval, legal foundation inference, and complex legal application. Each level contains multiple tasks to ensure a comprehensive evaluation. Through automated evaluation of current general and legal domain LLMs on our benchmark, we indicate that these LLMs may not align with the logic of legal practice. LLMs seem to be able to directly acquire complex legal application capabilities but perform poorly in some basic tasks, which may pose obstacles to their practical application and acceptance by legal experts. To further confirm the complex legal application capabilities of current LLMs in legal application scenarios, we also incorporate human evaluation with legal experts. The results indicate that while LLMs may demonstrate strong performance, they still require reinforcement of legal logic.\n\\end{abstract}\n\n\\section{Introduction}\n\nWith the emergence of ChatGPT\\footnote{\\url{https://openai.com/blog/chatgpt}} and GPT-4\\footnote{\\url{https://openai.com/gpt-4}} and their excellent text processing capabilities \\cite{zhao2023survey}, many researchers have paid attention to the applications of large language models (LLMs) in various fields \\cite{wang2023huatuo, xie2023pixiu, ko2023can}. In the field of legal artificial intelligence (LegalAI), which specifically studies how artificial intelligence can assist in legal construction \\cite{zhong2020does, locke2022case, feng2022legal}, LLMs, especially those specializing in Chinese law, also show strong capabilities in generating legal text \\cite{cui2023chatlaw,LAWGPT,HanFei}.\n\nChinese legal LLMs currently cover a wide range of legal tasks and undergo training through two different methods. \nWhile LLMs like HanFei \\cite{HanFei} are pre-trained with large-scale legal documents, most Chinese legal LLMs, such as Fuzi-Mingcha \\cite{fuzi.mingcha}, LexiLaw\\footnote{\\url{https://github.com/CSHaitao/LexiLaw}}, and ChatLaw \\cite{cui2023chatlaw}, are fine-tuned with specific legal tasks based on the LLMs. The legal tasks they focus on have been well-defined in the field of NLP and can directly reflect their application effectiveness \\cite{zhong2020does, choi2023use, steenhuis2023weaving}, such as legal question answering and consultation. Existing benchmarks for evaluating these models are also constructed based on these tasks \\cite{yue2023disclawllm,fei2023lawbench}. However, due to the feasibility, these tasks are ultimately defined by computer experts, which may lack the training data and evaluation tasks for the logic of legal practice. Whether LLMs can currently be accepted and used by legal experts in practice is worth considering.\n\nThe application of LLMs in law should adhere to the logic of legal practice, known as the legal syllogism, involving the acquisition of evidence, legal articles, conclusions, and their interconnections \\cite{kuppa2023chain, trozze2023large}. We expect legal LLMs to become true legal experts, and their handling of legal issues should involve three stages. Firstly, the ability to extract information from legal texts, then the ability to provide a reliable and reasoned answer based on solid legal knowledge, and ultimately the ability to form a complete response. For example, in criminal law, when judging someone, we need to first find relevant legal articles based on evidence, then calculate the judgment result based on these articles, and provide a well-organized and logical judgment text. \nMore importantly, for courtroom officials, whether are LLMs capable of the above-mentioned abilities is their primary concern, which ensures the fairness of the law\\footnote{\\url{https://github.com/liuchengyuan123/LegalLLMEvaluation/}}.\n\nIn this work, to investigate the above-mentioned issue, we propose the first Chinese legal LLM benchmark LAiW\\footnote{It means \"AI in LAW\".} based on the logic of legal practice. In this benchmark, following the guideline of the logic of legal practice, we categorize the legal capabilities of LLMs into three levels, from simple to difficult: basic information retrieval (BIR), legal foundation inference (LFI), and complex legal application (CLA). Among them, BIR focuses on the general NLP capabilities of LLMs and some legal evidence, knowledge, and category determination; \nLFI emphasizes the performance of LLMs in simple application tasks in the legal domain, which are typically components of more complex legal issues and interim results in legal logic; \nCLA focuses on the performance of LLMs in complex tasks in the legal domain, which require support from the abilities developed in the first two levels and some complex logical reasoning capabilities.\nBased on these capabilities, our benchmark contains 14 tasks, covering most of the existing LegalAI tasks, and some new tasks. \n\nWhen conducting benchmark evaluations, we performed both automated evaluations and additional manual evaluations. For automated evaluations, we not only evaluate existing Chinese legal LLMs such as Lawyer-LLaMA \\cite{lawyer-llama-report}, ChatLaw\\cite{cui2023chatlaw} and HanFei \\cite{HanFei}, but also focused on the base models of these Chinese legal LLMs and more effective general LLMs. \nThe results of automated evaluations indicate that while existing LLMs have strong text generation capabilities for complex legal applications, they are unable to meet the underlying logic in legal applications in basic information retrieval and legal foundation inference. \nThis may lead to significant distrust among legal experts when using LLMs. \nTherefore, we conduct additional manual evaluations specifically to assess the reliability of LLMs in complex legal applications. \nThrough evaluations by legal experts, we find that in some complex legal applications with relatively lenient requirements for legal logic, LLMs' powerful generation ability cleverly bridges the gap in legal logic. \nHowever, in more demanding scenarios, they still exhibit significant discrepancies from real results. \nThis further indicates the need for the tasks to train and evaluate the legal logic capabilities of LLMs.\n\nOur contributions are as follows:\n\\begin{itemize}\n    \\item \n    We are proud to introduce the first Chinese legal LLMs benchmark LAiW, which is designed based on the logic of legal practice. We categorize the legal capabilities of LLMs into three levels: basic information retrieval, legal foundation inference, and complex legal application. This detailed categorization enables a more accurate assessment of the true abilities of LLMs in legal practice.\n    \\item Based on our automated evaluation, we demonstrate that the current capabilities of legal LLMs do not align with the demands of real-world applications. While LLMs demonstrate strong text generation abilities to complete complex legal applications, they struggle to achieve satisfactory performance in foundational legal logic tasks, making it difficult to gain the trust of legal experts.\n    \\item To assess whether the outstanding performance of LLMs in our third level is applicable in real scenarios, we invite legal experts for manual evaluations. Despite the LLMs' strong text generation capability, they still expose limitations with legal logic in practical applications. Therefore, We further advocate for the construction of training tasks to better reflect legal logic.\n\\end{itemize}\n\n\\section{Related Work}\n\n\\textbf{Chinese Legal LLMs.} \nWe summarize the current Chinese legal LLMs and some general models in Table \\ref{tab:LAiW-LLMs}.\nMost of these Chinese legal LLMs focus on the ultimate application tasks in the legal field and are generally fine-tuned on some general LLMs.\nFor instance, LawGPT\\_zh \\cite{LAWGPT-zh}, Lawyer-LLaMA \\cite{lawyer-llama-report}, ChatLaw \\cite{cui2023chatlaw}, Fuzi-Mingcha \\cite{fuzi.mingcha}, and LexiLaw developed the ability to answer legal questions and provide legal consultations by fine-tuning on related legal data. \nTo compensate for the lack of legal knowledge due to only fine-tuning, these LLMs introduce additional legal knowledge databases for retrieval to supplement.\nHowever, the accuracy and comprehensiveness of the knowledge base may be a major limiting factor for these LLMs.\nThe other Chinese legal LLMs adopted the pretraining or continued pretraining to enhance the legal knowledge of LLMs, such as LaWGPT \\cite{LAWGPT}, wisdomInterrogatory\\footnote{\\url{https://github.com/zhihaiLLM/wisdomInterrogatory}}, and HanFei \\cite{HanFei}.\nThey collect a large amount of legal text data, covering a wider range of legal tasks such as element extraction and case classification. These have a noticeable impact on improving the overall effectiveness of LLMs in legal applications.\nHowever, the Chinese legal LLMs mentioned above mainly focus on the outcome of legal application (such as similarity to standard answers and fluency of generated text), which rarely consider whether they meet the logical requirements of legal practice. It is important to evaluate their legal logic, which is of utmost concern to legal experts.\n\n\\textbf{Legal LLMs Benchmark.} \nThe development of LegalAI has led to a large number of tasks that combine law and computer science, from NLP-focused legal NER\\footnote{\\url{https://github.com/china-ai-law-challenge/CAIL2021/tree/main/xxcq}} and legal text summarization \\cite{kanapala2019text} to legal-focused similar case matching \\cite{locke2022case, sansone2022legal}, providing ample data for evaluating Chinese legal LLMs \\cite{zhong2020does}. \nWhen categorizing from a legal perspective, it also encompasses the logic of the entire legal process from the legal elements extraction \\cite{cao2022cailie, zhang2022recognition, zhong2020iteratively} to legal judgment prediction \\cite{feng2022legal, cui2023survey}. \nBased on these tasks, LawBench \\cite{fei2023lawbench} built an automatic evaluation framework for Chinese legal LLMs, which concerns the memorization, understanding, and application of legal knowledge.\nDISC-Law-Eval Benchmark \\cite{yue2023disclawllm} also based on the aforementioned tasks divides the evaluation into objective and subjective parts. The objective section assesses knowledge retention and reasoning abilities in the legal examination, and the subjective part uses GPT-3.5 Turbo to score the accuracy, completeness, and clarity of the answers. These frameworks greatly helped us understand the capabilities and improvement directions of current legal LLMs from the perspective of computer experts, especially the knowledge systems. However, whether these LLMs can be accepted by legal experts is also a question worthy of evaluation. In this work, we focus on addressing this issue from the logic of legal practice.\n\n\\begin{table*}[!htb]\n  \\centering\n  \\resizebox{0.999\\textwidth}{!}{\n  \\begin{threeparttable}\n  \\begin{tabular}{lcccccc}\n    \\toprule\n    Capability & Task & Primary Origin Dataset   & LAiW & Domain & Task Type\\\\\n    \\midrule\n    \n    \\multirow{5}[0]{*}{BIR}  & Legal Article Recommendation & CAIL2018 \\cite{xiao2018cail2018}  &  1000  & Criminal & Classification \\\\\n    \\cmidrule{2-6}  & Element Recognition & CAIL-2019 \\cite{zhang2022recognition}  & 1000 & Civil & Classification \\\\\n    \\cmidrule{2-6}  & Named Entity Recognition & CAIL-2021 \\cite{cao2022cailie} & 1040 & Criminal & Named Entity Recognition \\\\\n    \\cmidrule{2-6}  & Judicial Summarization & CAIL-2020 \\cite{huang2023high} & 364 & Civil &  Text Generation \\\\\n    \\cmidrule{2-6}  & Case Recognition & CJRC \\cite{duan2019cjrc} & 2000 & Criminal, Civil & Classification \\\\\n    \n    \\midrule\n    \\multirow{6}[0]{*}{LFI} & Controversy Focus Mining & LAIC-2021  & 306 & - & Classification \\\\\n    \\cmidrule{2-6}  & Similar Case Matching & CAIL-2019 \\cite{xiao2019cail2019}  & 260 & Civil & Classification \\\\\n    \\cmidrule{2-6} & Charge Prediction & Criminal-S \\cite{hu2018few}  & 827 & Criminal & Classification \\\\\n    \\cmidrule{2-6} & Prison Term Prediction & MLMN \\cite{ge2021learning}  & 349 & Criminal & Classification \\\\\n    \\cmidrule{2-6}  & Civil Trial Prediction & MSJudeg \\cite{ma2021legal} & 800 & Civil & Classification \\\\\n    \\cmidrule{2-6}  & Legal Question Answering & JEC-QA \\cite{zhong2020jec} & 855 & -  & Classification \\\\\n\n    \\midrule\n    \\multirow{3}[0]{*}{CLA} & Judicial Reasoning Generation & AC-NLG \\cite{wu2020biased} & 834 &  Civil & Text Generation \\\\\n    \\cmidrule{2-6}  & Case Understanding & CJRC \\cite{duan2019cjrc} & 1054 & Criminal, Civil & Text Generation \\\\\n    \\cmidrule{2-6}  & Legal Consultation & CrimeKgAssitant \\cite{LAWGPT-zh} & 916 & - & Text Generation \\\\\n    \n    \\bottomrule\n    \\end{tabular}\n\n    \\end{threeparttable}\n}\n  \\centering\n  \\caption{Statistical information of our dataset. All datasets are sourced from open-source.}\n  \\label{tab:dataset}%\n\n\\end{table*}%\n\n\\section{Benchmark Construction}\n\nIn this section, we divide LLMs' abilities levels based on The Logic of Legal practice and construct our Chinese legal LLMs benchmark LAiW based on these levels. To ensure comprehensive evaluation, we cover both automatic evaluation using computable metrics and manual evaluation by legal experts.\n\n\\begin{figure}[!htb]\n\\centering\n\\includegraphics[width=1\\columnwidth]{figs/task_framework_en.png} \n\\caption{Multi-level Legal Capabilities of LLMs.}\n\\label{task_framework_en}\n\\end{figure}\n\n\\subsection{ The Logic of Legal Practice for LLMs} \nIn contemporary legal practice, the logic is primarily based on Syllogism \\cite{wroblewski1974legal, patterson2013logic}. Syllogism typically consists of three parts: the major premise, the minor premise, and the conclusion which is derived from the major and minor premises. In legal practice, this entails assessing the information and evidence pertinent to a case (minor premise), identifying the relevant legal articles (major premise), and reaching a judicial decision based on these factors (conclusion).\n\nTo ensure that LLMs also have the aforementioned logical thinking and remain synchronized with legal practice, we should also divide the abilities of LLMs into the aforementioned logical stages with 14 tasks. \nSpecifically, we categorize the legal abilities of LLMs into three levels and try to align them with the logic of legal practice, shown in Figure \\ref{task_framework_en}. \nBy merging the process of acquiring minor premise and major premise, we construct the capability level of \\textbf{basic information retrieval}. Building upon this foundation, we develop the capability level of \\textbf{legal foundation inference} to draw preliminary conclusions based on the minor and major premises. Additionally, to assess the direct representation of the entire legal syllogism, we have created the capability level of \\textbf{complex legal application}\\footnote{Appendix \\ref{appendix:dataset auto} provides detailed explanations for tasks of each capability level.}.\n\n\\subsubsection{BIR: Basic Information Retrieval}\nWe design the Basic Information Retrieval level with 5 tasks to assess the fundamental abilities of LLMs in legal logic, corresponding to directly accessible text information, minor premises, and major premises, such as legal evidence, legal knowledge, and category determination.\n\nSpecifically, we first consider three tasks that are well-defined in the fields of law and NLP: Named Entity Recognition, Judicial Summarization, and Case Recognition. \nThey identify and summarize the key elements in legal texts, and classify cases as either Criminal or Civil.\nAlthough these tasks may not require extensive legal knowledge from LLMs, they can yield a wealth of foundational information useful for both legal and computational purposes from the text.\n\nWe also consider two other tasks in the legal domain, namely Legal Article Recommendation and Element Recognition. \nThe first task is to catch the major premises by finding relevant legal articles. The second task is to catch minor premises by identifying their relevant elements.\n\n\\subsubsection{LFI: Legal Foundation Inference} \nThe Legal Foundation Inference level follows Syllogism's idea to explore the ability of LLMs to derive basic results and some judgment conclusions from minor premises and major premises.\n\nWe can divide 6 tasks for this capability into three parts. \nThe first part presents the basic results of some simple legal applications, including Controversial Focus Mining and Similar Case Matching. \nControversial Focus Mining is an intermediate result obtained in civil law based on the underlying circumstances and legal articles, used to determine the core issues of concern for both the plaintiff and the defendant.\nSimilar Case Matching involves finding similar cases based on the current case situation and referring to these cases to ensure the fairness of the judgment.\nThe second part involves predicting the outcomes of the court judgment conclusion. Since criminal law and civil law are two main branches of law, we have 3 tasks. Charge Prediction and Prison Term Prediction for criminal law, Civil Trial Prediction for civil law. \nFinally, The third part involves another application task, Legal Question Answering, that requires some fundamental integrated capabilities and focuses on the simple application of legal knowledge. \nBased on the information provided, LLMs provide some basic legal responses.\n\n\\subsubsection{CLA: Complex Legal Application} \nFor this capability, we consider 3 challenging tasks that LLMs may be required to complete a complex legal reasoning and application task according to the entire logical process mentioned above.\nTherefore, we focus on three tasks: Judicial Reasoning Generation, Case Understanding, and Legal Consultation.\nJudicial Reasoning Generation involves the complete reproduction of the logical process from major and minor premises to conclusions in legal judgments. \nCase Understanding, on the other hand, analyzes the logic from the perspective of understanding, from major and minor premises to conclusion. \nLegal Consultation utilizes this logic from the perspective of a legal professional to provide assistance.\n\n\\subsection{Datasets Construction}\n\\label{Tasks-Construct}\nBased on the mentioned criteria for capability division and task preparation, we combined the majority of open-source datasets with a small amount of proprietary data to construct the evaluation dataset for our LAiW benchmark. The dataset is divided into two parts: Automatic and Manual.\n\n\\subsubsection{Automatic Evaluation Legal Tasks}\nTo facilitate a more efficient evaluation of LLMs, we construct all 14 tasks mentioned above into datasets that can be automatically assessed shown in Table \\ref{tab:dataset}.\nThe primary sources of this data include previous years' CAIL competition data \\cite{xiao2018cail2018, zhang2022recognition, huang2023high}, as well as the most commonly used open-source data \\cite{ge2021learning, wu2020biased, LAWGPT-zh} for these tasks, including various types of law, such as criminal law, civil law, constitutional law, social law, and economic law.\n\nAs for the task type of our dataset, it contains three types of tasks: classification, named entity recognition, and text generation. \nThe classification refers to tasks where LLMs provide answers in the form of multiple-choice questions. \nNamed entity recognition involves accurately identifying legal entities in the text, while text generation tasks allow LLMs to freely perform legal tasks and flexibly generate responses. \n\nDuring the construction of the dataset, we designed different prompts for various tasks to support LLMs in providing better answers. We initially validated the quality of prompts using ChatGPT and further confirmed their validity through legal experts. Currently, all tasks exist in a zero-shot format\\footnote{Examples \nand the detailed processing methods can be found in Appendix \\ref{appendix:dataset auto} and Appendix \\ref{sec:Example Prompt}.}.\n\n\\subsubsection{Manual Evaluation Legal Tasks}\nAs shown in automatic evaluation results \\ref{sec:automatic evaluation results}, we observed that these LLMs may not align with the logic of legal practice. LLMs seem to be able to directly acquire complex legal application capabilities but perform poorly in some basic tasks. To further confirm the complex legal application capabilities of current LLMs in legal application scenarios, we add a manual evaluation focus on the ability of the third level.\n\nDue to the cost of manual evaluation, we focus on two tasks that are more oriented toward LLMs for logical reasoning: Judicial Reasoning Generation and Legal Consultation, rather than the Case Understanding task, which is based on the analysis of legal texts with existing logic \\footnote{The detailed processing methods for the datasets are outlined in Appendix \\ref{appendix:dataset manual}.}.\n\n\\begin{table*}[!htb]\n\n  \\centering\n  \n  \\resizebox{0.92\\textwidth}{!}{\n  \\begin{threeparttable}\n  \\begin{tabular}{ccccccc}\n    \\toprule\n    Model & Model Size & Model Domain  & From & Baseline & Creater & Link \\\\\n    \\midrule\n\n    GPT-4 \\cite{openai2023gpt4} & - & General & Api & - & OpenAI & - \\\\\n    \\cmidrule{1-7}\n    ChatGPT & - & General & Api & - & OpenAI & - \\\\\n    \\cmidrule{1-7}\n    Baichuan2-Chat \\cite{baichuan2023baichuan2} & 13B & General & Open & - & Baichuan Inc & \\href{https://github.com/baichuan-inc/Baichuan2}{[1]} \\\\\n    \\cmidrule{1-7}\n     Baichuan & 7B  & General  &  Open  &  - &  Baichuan Inc &  \\href{https://github.com/baichuan-inc/Baichuan-7B}{[2]}  \\\\\n     \\cmidrule{1-7}\n     ChatGLM \\cite{du2022glm} & 6B  & General  & Open  &  - & Tsinghua, Zhipu  &  \\href{https://github.com/THUDM/ChatGLM-6B}{[3]}  \\\\\n     \\cmidrule{1-7}\n     Llama \\cite{touvron2023llama} &  7B & General  &  Application &  - &  Meta AI &  \\href{https://github.com/facebookresearch/llama}{[4]}  \\\\\n     \\cmidrule{1-7}\n     Llama \\cite{touvron2023llama} & 13B  & General  & Application  & -  & Meta AI  &  \\href{https://github.com/facebookresearch/llama}{[4]}  \\\\\n     \\cmidrule{1-7}\n     Llama2-Chat \\cite{touvron2023llama2} & 7B & General & Application & - & Meta AI & \\href{https://huggingface.co/meta-llama/Llama-2-7b-chat-hf}{[5]} \\\\\n     \\cmidrule{1-7}\n     Chinese-LLaMA \\cite{chinese-llama-alpaca}  & 7B  & General  &  Open & Llama-7B   &  Yiming Cui &  \\href{https://github.com/ymcui/Chinese-LLaMA-Alpaca}{[6]}  \\\\\n     \\cmidrule{1-7}\n     Chinese-LLaMA \\cite{chinese-llama-alpaca}  & 13B  & General  &  Open & Llama-13B   &  Yiming Cui &  \\href{https://github.com/ymcui/Chinese-LLaMA-Alpaca}{[6]}  \\\\\n     \\cmidrule{1-7}\n     Ziya-LLaMA\\cite{fengshenbang} &  13B &  General & Open  & Llama-13B &  IDEA-CCNL &  \\href{https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-v1}{[7]}  \\\\\n     \\cmidrule{1-7}\n     HanFei \\cite{HanFei} & 7B  & Law  &  Open &  - & SIAT NLP  & \\href{https://github.com/siat-nlp/HanFei}{[8]}   \\\\\n     \\cmidrule{1-7}\n     wisdomInterrogatory  &  7B &  Law & Open  & Baichuan-7B  &  ZJU, Alibaba, e.t &  \\href{https://github.com/zhihaiLLM/wisdomInterrogatory}{[9]}  \\\\\n     \\cmidrule{1-7}\n     Fuzi-Mingcha \\cite{fuzi.mingcha} & 6B  &  Law & Open  & ChatGLM-6B  & irlab-sdu  &  \\href{https://github.com/irlab-sdu/fuzi.mingcha}{[10]} \\\\\n     \\cmidrule{1-7}\n     LexiLaw & 6B  & Law  &  Open & ChatGLM-6B  &  Haitao Li &  \\href{https://github.com/CSHaitao/LexiLaw}{[11]}  \\\\\n     \\cmidrule{1-7}\n     LaWGPT \\cite{LAWGPT} & 7B  &  Law & Open  & Chinese-LLaMA-7B  & Pengxiao Song  & \\href{https://github.com/pengxiao-song/LaWGPT}{[12]}   \\\\\n     \\cmidrule{1-7}\n     Lawyer-LLaMA \\cite{lawyer-llama-report} &  13B & Law  & Open  &  Chinese-LLaMA-13B & Quzhe Huang  & \\href{https://github.com/AndrewZhe/lawyer-llama}{[13]}   \\\\\n     \\cmidrule{1-7}\n     ChatLaw \\cite{cui2023chatlaw} &  13B & Law  &  Open & Ziya-LLaMA-13B  & PKU-YUAN's Group  & \\href{https://github.com/PKU-YuanGroup/ChatLaw}{[14]} \\\\\n    \n    \\bottomrule\n    \\end{tabular}\n    \\end{threeparttable}\n}\n\\caption{The LLMs evaluated in our work. LaWGPT and wisdomInterrogatory undergo pre-training on Chinese-LLaMA and Baichuan respectively, followed by fine-tuning. HanFei does not have a baseline model. Apart from GPT-4 and ChatGPT, these general LLMs ave a parameter size of 7-13B to ensure a size similar to legal LLMs.}\n  \\label{tab:LAiW-LLMs}%\n\\end{table*}%\n\n\\section{Evaluation for Benchmark}\nIn this section, we provide our metrics and scoring method for automatic evaluation and our metrics and criteria for manual evaluation.\n\n\\subsection{Automatic Evaluation Legal Tasks}\n\\label{sec:Automatic Evaluation Legal Tasks}\nAutomatic Evaluation Legal Tasks contains classification tasks, named entity recognition tasks, and text generation tasks.\nFor classification tasks, we select accuracy (Acc), miss rate (Miss), F1 score (F1), and matthews correlation coefficient (Mcc) as evaluation metrics for these tasks. They can assess LLMs for their understanding of questions (Miss), the effectiveness of their answers (Acc and F1), and their ability to identify imbalances (Mcc). \nFor named entity recognition tasks, we use the accuracy of the LLMs in identifying every legal entities (Entity-Acc). \nFor text generation tasks, we use ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L (RL) as evaluation metrics for this task. \n\nTo evaluate the overall legal capabilities of LLMs, we further select a few key indicators for each task and calculate legal scores for LLMs based on these indicators as shown in Equation (\\ref{eq:scores}).\n\n\\begin{equation}\n\\begin{cases}\n    S_{\\text{classification}}=F1*100, \\\\\n    S_{\\text{text generation}}=\\frac{1}{3}(R1+R2+RL)*100, \\\\\n    S_{\\text{named entity recognition}}=\\text{Entity-Acc}*100.\n\\end{cases}\n\\label{eq:scores}\n\\end{equation}\n\nThen, the total score is calculated by averaging the scores of the three levels, which in turn are determined by averaging the scores of tasks within each level.\n\n\\subsection{Manual Evaluation Legal Tasks}\n\\label{sec:Manual Evaluation Legal Tasks}\nFirst, to ensure the reliability of the assessment, we present criteria with several legal experts for manual evaluation. For the Judicial Reasoning Generation task, the criteria focus on completeness, relevance and accuracy. As for the Legal Consultation task, the criteria focus on fluency, relevance, and comprehensibility\\footnote{A more detailed description is provided in Appendix \\ref{Annotation rules and standards}.}.\n\nWe adopt the approach used in studies \\cite{dubois2023alpacafarm,alpaca_eval} for manual evaluation, considering legal experts as evaluators, using reference answers as the baseline to calculate the win rate for the target LLMs. \nFor example, when using the reference answer as the baseline, legal experts comprehensively assess the output of the target LLM and the reference answer from multiple judgment dimensions, and then choose the most satisfactory response. \n\n\\section{Experiment}\n\nIn this section, we present relevant experiment settings and highlight the key results.\n\n\\subsection{Experiment Settings}\n\\label{sec:auto evaluation}\nFor the automatic evaluation, We evaluate 18 LLMs, including 7 mainstream legal LLMs \\cite{cui2023chatlaw, LAWGPT}, their corresponding 6 baseline LLMs \\cite{du2022glm, chinese-llama-alpaca, zhang2022recognition}, and 5 more effective general LLMs \\cite{baichuan2023baichuan2, touvron2023llama} such as GPT-4 and ChatGPT. For fairness in evaluation, all LLMs did not utilize legal knowledge databases. Table \\ref{tab:LAiW-LLMs} lists more detailed information about these LLMs.\n\nFor the manual evaluation, We choose the top-performing four legal LLMs in our automatic evaluation. They are Fuz-Mingcha \\cite{fuzi.mingcha}, HanFei \\cite{HanFei}, Lawyer-LLaMa \\cite{lawyer-llama-report}, and LexiLaw. Furthermore, we also conducted manual assessments of the performance of both GPT-4 and ChatGPT.\n\n    \n\n        \n\n\\begin{table*}[!htb]\n  \\centering\n\n  \\resizebox{0.999\\textwidth}{!}{\n  \\begin{tabular}{ccccccccccccccccccc}\n    \\toprule\n    \\multirow{2}[2]{*}{Model} & \\multicolumn{6}{c}{Basic Information Retrieva}  & \\multicolumn{7}{c}{Legal Foundation Inference} & \\multicolumn{4}{c}{Complex Legal Application} & \\multirow{2}[2]{*}{Total Score} \\\\\n    \\cmidrule(lr){2-7}\n    \\cmidrule(lr){8-14}\n    \\cmidrule(lr){15-18}\n     & $B_{1}$ & $B_{2}$ & $B_{3}$ & $B_{4}$ & $B_{5}$ & Avg. & $L_{1}$ & $L_{2}$ & $L_{3}$ & $L_{4}$ & $L_{5}$ & $L_{5}$ & Avg. & $C_{1}$ & $C_{2}$ & $C_{3}$ & Avg. & \\\\\n     \\midrule \n     \n    GPT-4   & 99.20 & 82.27 & 80.67 & 42.72 & 99.75 & 80.92 & 80.50 & 45.94 & 100.00 & 65.58 & 70.43 & 53.14 & 69.27 & 37.22 & 96.19 & 42.66 & 58.69 & 69.63 \\\\\n    ChatGPT  & 99.05 & 79.32 & 61.73 & 41.01 & 98.85 & 75.99 & 57.16 & 46.17 & 99.28 & 47.35 & 62.85 & 37.08 & 58.32 & 35.64 & 90.70 & 47.55 & 57.96 & 64.09 \\\\\n    \\midrule\n    Baichuan2-13B-Chat & 45.07 & 52.18 & 47.31 & 26.67 & 97.14 & 53.67 & 4.12 & 2.99 & 17.50 & 61.43 & 67.91 & 38.24 & 32.03 & 52.61 & 81.29 & 41.31 & 58.40 & 48.04 \\\\\n     Baichuan-7B   & 17.81 & 2.87 & 0.00 & 26.89 & 58.45 & 21.20 & 1.74 & 0.00 & 1.18 & 1.03 & 64.50 & 24.32 & 15.46 & 40.27 & 33.79 & 18.51 & 30.86 & 22.51 \\\\\n     ChatGLM-6B    & 72.55 & 49.82 & 1.06 & 42.87 & 91.27 & 51.51 & 14.18 & 39.03 & 67.57 & 44.84 & 33.02 & 23.86 & 37.08 & 35.39 & 86.90 & 35.02 & 52.44 & 47.01 \\\\\n     Llama-7B  & 19.53 & 1.43 & 0.00 & 11.40 & 23.23 & 11.12 & 1.31 & 0.00 & 35.19 & 1.03 & 49.15 & 5.74 & 15.40 & 0.61 & 56.08 & 10.93 & 22.54 & 16.35 \\\\\n     Llama-13B   & 28.16 & 7.66 & 0.00 & 9.94 & 46.80 & 18.51 & 1.86 & 0.00 & 36.79 & 5.80 & 40.46 & 5.57 & 15.08 & 11.19 & 65.68 & 11.34 & 29.40 & 21.00 \\\\\n     Llama2-7B-Chat   & 48.24 & 11.93 & 0.19 & 15.79 & 83.17 & 31.86 & 0.74 & 0.00 & 3.88 & 7.31 & 62.09 & 2.59 & 12.77 & 28.76 & 69.51 & 17.65 & 38.64 & 27.76 \\\\\n     Chinese-LLaMA-7B & 24.39 & 7.45 & 0.00 & 30.77 & 48.97 & 22.32 & 2.02 & 0.76 & 31.79 & 1.03 & 65.24 & 8.63 & 18.25 & 26.34 & 62.31 & 13.81 & 34.16 & 24.91 \\\\\n     Chinese-LLaMA-13B  & 30.34 & 5.47 & 0.00 & 7.73 & 61.56 & 21.02 & 3.28 & 5.05 & 20.21 & 5.33 & 64.46 & 16.60 & 19.16 & 18.86 & 73.15 & 12.40 & 34.80 & 24.99 \\\\\n     Ziya-LLaMA-13B & 66.39 & 58.42 & 48.94 & 38.85 & 94.73 & 61.47 & 5.64 & 0.76 & 53.18 & 55.62 & 36.07 & 25.38 & 29.44 & 30.12 & 83.96 & 25.26 & 46.45 & 45.79 \\\\\n    \\midrule\n     HanFei-7B & 24.91 & 7.25 & 51.63 & 21.14 & 82.18 & 37.42 & 1.15 & 0.00 & 5.27 & 2.73 & 66.81 & 22.03 & 16.33 & 51.31 & 81.19 & 27.43 & 53.31 & 35.69 \\\\\n     wisdomInterrogatory-7B  & 0.39 & 0.19 & 0.00 & 34.75 & 27.99 & 12.66 & 3.57 & 35.38 & 2.32 & 1.30 & 16.76 & 3.34 & 10.45 & 13.91 & 68.02 & 18.17 & 33.37 & 18.83 \\\\\n     Fuzi-Mingcha-6B & 58.95 & 12.58 & 0.38 & 47.92 & 78.57 & 39.68 & 4.70 & 20.84 & 31.53 & 48.40 & 32.66 & 26.64 & 27.46 & 49.55 & 80.48 & 34.10 & 54.71 & 40.62 \\\\\n     LexiLaw-6B  & 47.16 & 2.89 & 31.35 & 41.79 & 83.43 & 41.32 & 2.11 & 18.49 & 3.40 & 6.42 & 4.35 & 18.51 & 8.88 & 25.85 & 80.81 & 24.52 & 43.73 & 31.31 \\\\\n     LaWGPT-7B  & 10.15 & 2.59 & 0.00 & 27.69 & 36.92 & 15.47 & 1.62 & 0.00 & 20.04 & 1.03 & 54.55 & 8.40 & 14.27 & 35.23 & 65.62 & 14.11 & 38.32 & 22.69 \\\\\n     Lawyer-LLaMA-13B & 20.26 & 1.52 & 7.88 & 51.13 & 73.44 & 30.85 & 2.19 & 0.76 & 0.24 & 2.12 & 12.75 & 20.26 & 6.39 & 34.00 & 85.68 & 31.83 & 50.50 & 29.25 \\\\\n     ChatLaw-13B& 67.08 & 31.29 & 52.21 & 41.33 & 98.20 & 58.02 & 0.00 & 0.00 & 37.82 & 30.85 & 6.58 & 0.00 & 12.54 & 0.00 & 20.23 & 0.00 & 6.74 & 25.77 \\\\\n     \n    \\bottomrule\n    \\end{tabular}\n    }\n\n    \\caption{The scores of LLMs at various levels of the LAiW based on equation (\\ref{eq:scores}). Here, $B_1$ to $B_5$ respectively represent the tasks: Legal Article Recommendation, Element Recognition, Named Entity Recognition, Judicial Summarization, and Case Recognition. $L_1$ to $L_6$ respectively represent the tasks: Controversy Focus Mining, Similar Case Matching, Charge Prediction, Prison Term Prediction, Civil Trial Prediction, and Legal Question Answering. $C_1$ to $C_3$ respectively represent the tasks: Judicial Reasoning Generation, Case Understanding, and Legal Consultation.}\n    \\label{tab:scores}%\n\\end{table*}%\n\n\\subsection{Automatic Evaluation Results}\n\\label{sec:automatic evaluation results}\n\nThe scores for each level and the total score of our automated evaluation are shown in Table \\ref{tab:scores}\\footnote{The complete results of each task are available in Appendix \\ref{appendix:results of LLMs evaluation}.}.\nWe analyze the results from three different aspects: overall results, the legal logic of Chinese Legal LLMs, and the capabilities of Chinese Legal LLMs.\n\n\\textbf{Overall results.} When compared to GPT-4 and ChatGPT, there still exists a significant gap between the current open-source LLMs and specifically trained legal LLMs.\n\nFrom Table \\ref{tab:scores}, we find that GPT-4 and ChatGPT maintain optimal performance in most tasks. \nThey significantly outperform the current open-source LLMs at various levels of scoring.\nAmong the open-source LLMs, only Baichuan2-Chat, ChatGLM, and Ziya-LLaMA achieve a total score of 45 or above. However, their performance in the BIR and LFI levels still lags far behind GPT-4 and ChatGPT.\nAs for the specifically trained legal LLMs, the top four performing ones are Fuzi-Mingcha, HanFei, LexiLaw, and Lawyer-LLaMA. However, their overall scores are lower, all below 45.\n\nWe believe that the reason for this phenomenon is twofold: first, due to the large number of parameters in GPT-4 and ChatGPT; second, we during the pretraining phase, GPT-4 and ChatGPT may have been exposed to a larger amount of data. \nSince the open-source LLMs we selected are primarily aimed at the Chinese community, the data they collect may be more limited compared to GPT-4 and ChatGPT. \nGPT-4 and ChatGPT cover a wide range of legal data in multiple languages. In this case, it is reasonable for them to have higher scores in the BIR and LFI levels which focus on the basic legal logic and legal knowledge.\n\n\\textbf{ The legal logic of Chinese Legal LLMs.} \nThe Legal LLMs may not have the logic of legal practice. While they demonstrate strong text generation abilities to complete complex legal applications, they perform poorly in basic information retrieval and legal foundation inference tasks.\n\nObserving Table \\ref{tab:scores}, it is evident that the majority of legal LLMs score nearly 20 points higher in the application of direct logic (CLA level) compared to the scores in BIR and LFI levels. \nThis is contrary to the logic typically found in law. \nIt suggests that these LLMs seem to have learned the patterns of generating legal texts directly, but have not grasped the legal reasoning behind these patterns. \nAs a result, LLMs are unable to effectively identify the major and minor premises in law and lack the ability to reason to a conclusion.\nHowever, for the BIR level, ChatLaw stands out among legal LLMs.\nIt instead has a strong ability at the BIR level, which may stem from the outstanding performance of its base model Ziya - LLaMA at this level.\n\nThis raises concerns that current legal LLMs may not meet the expectations of legal experts, posing potential risks to the trust of LLMs in the legal domain.\n\n\\textbf{ The capabilities of Chinese Legal LLMs.} Fine-tuned legal LLMs have improved the normativity of legal text generation but may lose the legal logic. \nIn addition, for legal LLMs, undergoing additional pre-training on legal text may be the path to achieving various legal capabilities and legal logic.\n\nFrom table \\ref{tab:scores}, legal LLMs such as Fuzi-Mingcha, WisdomInterrogatory, LaWGPT and Lawyer-LLaMA compared to their base models exhibit improvements in CLA level. \nMost of these legal LLMs improve the normativity of generated texts in legal text generation through instruction tuning. However, they may lead to a decrease in ability at the BIR and LFI levels, which indicates that instruction tuning may not enable LLMs to possess legal logic.\n\nOn the other hand, legal LLMs like HanFei, which focus more on pre-training, may indicate how Chinese Legal LLMs acquire ability and logic.\nHanFei, based on an older LLM structure (Bloomz), underwent extensive pre-training on legal texts and demonstrated capabilities on par with subsequent legal LLMs.\nFurthermore, GPT-4 and ChatGPT, models with extensive pre-training on large corpora, also showed excellent performance at the BIR and LFI levels. \nThese findings indicate that developing legal reasoning and comprehensive abilities may require learning from a significant amount of pre-training text, rather than just fine-tuning. \n\n    \n        \n\n\\subsection{Manual Evaluation Results}\n\\begin{table}[!htb]\n  \\centering\n\n  \\resizebox{0.49\\textwidth}{!}{\n  \\begin{tabular}{c|ccc|ccc}\n    \\toprule\n    \n    \\multirow{2}[0]{*}{Model} & \\multicolumn{3}{c|}{Judicial Reasoning Generation}  & \\multicolumn{3}{c}{Legal Consultation}  \\\\\n    \\cmidrule{2-7}\n     & Total Score & Win Rate & Std & Total Score  & Win Rate & Std \\\\\n    \\midrule\n    GPT-4 & 44.72 & 0.38 & 0.18 & \\underline{43.97} & \\textbf{0.85} & 0.15 \\\\\n    \\cmidrule{1-7}\n    ChatGPT & 41.74 & 0.35 & 0.27 & \\textbf{48.79} & \\underline{0.79} & 0.12 \\\\\n    \\cmidrule{1-7}\n    Fuzi-Mingcha & \\textbf{63.58} & \\textbf{0.65} & 0.35 & 35.22 & 0.51 & 0.19 \\\\\n    \\cmidrule{1-7}\n    HanFei & \\underline{60.13}  & \\underline{0.59} & 0.26 & 27.06 & 0.33 & 0.06 \\\\\n    \\cmidrule{1-7}\n    LexiLaw & 43.48 & 0.31 & 0.15 & 25.53 & 0.24 & 0.02 \\\\\n    \\cmidrule{1-7}\n    Lawyer-LLaMA & 39.61 & 0.30 & 0.26 & 33.27  & 0.51 & 0.21 \\\\\n        \n    \\bottomrule\n    \\end{tabular}\n}\n\\caption{The average win rate (WR) of LLMs for the Judicial Reasoning Generation and Legal Consultation tasks. The total score represents the score obtained by LLMs through automatic evaluation on our benchmark.}\n  \\label{tab:win rate}%\n\\end{table}%\n\nAccording to the criteria for expert evaluation in Section \\ref{sec:Manual Evaluation Legal Tasks} and the calculated average win rate scores of legal experts shown in Table \\ref{tab:win rate}\\footnote{The complete results are available in Appendix \\ref{appendix:win rate for each expert} and the experts' agreement are available in Appendix \\ref{The agreement scores}.}.\nBased on these results, we have two findings.\n\n\\textbf{The lack of legal logic in LLMs still exists in Complex Legal Applications.}\nFor the task of Judicial Reasoning Generation that requires a strong understanding of legal logic, even models with powerful text generation capabilities like GPT-4 and ChatGPT may have deficiencies in legal logic. \nAs described in Section \\ref{sec:Manual Evaluation Legal Tasks}, the Judicial Reasoning Generation task focuses on accuracy, such as the correct citation of legal articles and reasoning based on the citation. This directly connects to the basic logic of legal.\nTherefore, most of the LLMs' win rates are much lower than 0.5, indicating that strong text generation capabilities cannot directly replace legal logic.\n\nFor tasks like Legal Consultation, there is a lower requirement for legal logic but a higher requirement for fluency. \nTherefore, during the manual evaluation, legal experts tend to prefer models with stronger language capabilities, which is the strength of GPT-4 and ChatGPT. This capability can also be learned by legal LLMs through instruction tuning. \nAs a result, the final evaluation results of legal experts also reflect this, giving higher win rates to all LLMs, with most even surpassing the annotated answers.\n\n\\textbf{Manual evaluation and automatic evaluation share similarities, enhancing the reliability of our automatic evaluation.}\nFrom Table \\ref{tab:win rate}, we can observe that the results of manual evaluation and automated evaluation are similar. \nFor instance, in both evaluation rounds, Fuzi-Mingcha and HanFei performed best in the Judicial Reasoning Generation task, while GPT-4 and ChatGPT excelled in the Legal Consultation task. \nIn addition, despite its shortcomings as an automated evaluation metric in many cases, Rouge still demonstrates a certain level of capability when reflecting legal logic.\nThis indicates that our automatic evaluation can provide a reliable path for the legal logic assessment of legal LLMs and further reduce manual effort. \nAdditionally, our assessment of legal logic is granular, and the degree of emphasis on legal logic in different scenarios can also be reflected by our automatic evaluation of different tasks.\n\n\\section{Conclusion}\nThis paper aims to construct a Chinese Legal LLMs benchmark based on the logic of legal practice. To match the process of syllogism in legal logic, the benchmark categorizes LLM legal capabilities into basic information retrieval, legal foundation inference, and complex legal application, and includes 14 tasks. During benchmark evaluations, automated and manual evaluations were conducted. Automated results showed that existing LLMs excel in text generation for complex legal applications but struggle with basic information retrieval and legal foundation inference, leading to a lack of legal logic and distrust among legal experts. Manual evaluations revealed that while LLMs can bridge the gap in legal logic in some application scenarios, they still exhibit significant discrepancies as legal experts. This highlights the urgent need for better training and evaluation methods for LLMs in the legal domain. Therefore, our benchmark, for the first time from a legal perspective, reveals the professional legal capabilities of existing legal LLMs, greatly facilitating the subsequent evaluation and development work of legal LLMs. \n\n\\newpage\n\\section{Limitations and Future Work}\n\\label{sec:limitations}\nDue to the significant amount of work involved in using a computer-based approach to construct the logic of legal practice related tasks and assessments, we also have the following two limitations and areas for future work:\n\n1) In the manual evaluation experiment, to save workload, only a portion of the data and LLMs are sampled and chosen for evaluation, rather than assessing all of them. In the future, we will collaborate more with legal experts to ensure a more comprehensive human assessment.\n\n2) Most of the tasks are derived from publicly available legal data, which may not fully evaluate the logic of legal practice for LLMs. We will further develop additional tasks to refine the logic of legal practice at each stage.\n\n\\section{Ethics Statement}\nDue to the sensitivity of the legal field, we have conducted a comprehensive review of the relevant data in this benchmark. The open-source datasets we used all have corresponding licenses. We have masked sensitive information, such as names, phone numbers, and IDs, and legal experts have conducted ethical evaluations.\n\n\\section*{Acknowledgements}\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nLarge language models (LLMs) have made significant progress in natural language processing tasks and demonstrate considerable potential in the legal domain.  However, legal applications demand high standards of accuracy, reliability, and fairness. Applying existing LLMs to legal systems without careful evaluation of their potential and limitations could pose significant risks in legal practice.\nTo this end, we introduce a standardized comprehensive Chinese legal benchmark LexEval.\nThis benchmark is notable in the following three aspects: (1) Ability Modeling: We propose a new taxonomy of legal cognitive abilities to organize different tasks. (2) Scale: To our knowledge, LexEval is currently the largest Chinese legal evaluation dataset, comprising 23 tasks and 14,150 questions. (3) Data: we utilize formatted existing datasets, exam datasets and newly annotated datasets by legal experts to comprehensively evaluate the various capabilities of LLMs.\nLexEval not only focuses on the ability of LLMs to apply fundamental legal knowledge but also dedicates efforts to examining the ethical issues involved in their application.\nWe evaluated 38 open-source and commercial LLMs and obtained some interesting findings. \nThe experiments and findings offer valuable insights into the challenges and potential solutions for developing Chinese legal systems and LLM evaluation pipelines.\nThe LexEval dataset and leaderboard are publicly available at \n\\url{https://github.com/CSHaitao/LexEval} and will be continuously updated. \n\n\\end{abstract}\n\n\\section{Introduction}\n\\label{intro}\nRecently, the rapid development of large language models (LLMs) has brought new opportunities to the research of general artificial intelligence. A series of models (e.g., ChatGPT), with their extensive knowledge and outstanding language processing ability, have demonstrated excellent performance in various language processing tasks such as text generation, machine translation, and dialogue systems~\\cite{Flant5,gpt3,chen2021evaluating,wei2022chain,peng2023instruction}.\nMeanwhile, LLMs have profoundly impacted the work patterns of legal practitioners and the development of the legal field. Recent studies show that the GPT-4 has the ability to pass the U.S. Judicial Exam~\\cite{gptbar}.\nBy interacting with large language models, lawyers and judges can analyze legal documents more efficiently, obtaining comprehensive and valuable information and judicial advice. This has led to a growing trend among legal practitioners to incorporate LLMs as a vital supportive instrument in legal proceedings\\cite{cui2023chatlaw,Sailer,li2024deltapretraindiscriminativeencoder}.\n\nDespite the considerable potential of large language models, there are still concerns about their application in the legal domain~\\cite{savelka2023gpt4,nay2023large,li2024bladeenhancingblackboxlarge}. Firstly, unlike human decision-making, which is grounded in professional knowledge and logical reasoning, LLMs derive decisions from patterns and connections extracted from massive amounts of training data. Consequently, these models, predicated on probabilistic frameworks, often fall short of ensuring the reliability and explainability of their output~\\cite{floridi2020gpt}. \nAdditionally, existing research has indicated that LLMs may produce misleading and factually incorrect content~\\cite{manakul2023selfcheckgpt}. Substandard legal texts or flawed judicial guidance may mislead legal practitioners and increase their workload. Finally, the content generated by LLMs may reflect biases present in the training data, leading to unfair treatment of certain groups or specific events. This may undermine the effectiveness and fairness of judicial proceedings and judgments, bringing considerable systemic risks.\n\nThe great potential and inherent risks of LLMs in the legal domain give rise to the urgent need for a standardized and comprehensive benchmark~\\cite{sun2023short,T2Ranking}. Such a benchmark is essential to ensure that LLMs meet the high standards required for legal practice, minimizing the risks while maximizing their beneficial impact.\nAlthough numerous methods for evaluating the abilities of LLMs have been developed, most focus on assessing their generalist abilities on non-professional or semi-professional texts. These benchmarks provide limited guidance for highly specialized fields such as the legal domain\\cite{zhong2023agieval,huang2023c,chalkidis2021lexglue}.\nFor instance, the well-known Chinese language model evaluation framework, C-Eval~\\cite{huang2023c}, primarily uses test questions from high school and university courses.  However, in judicial applications, tasks like case summarization, legal case retrieval, and judgment prediction require LLMs to consider precise legal knowledge and complex legal contexts. These tasks often involves highly specialized elements such as judicial interpretation and reasoning. To the best of our knowledge, existing general evaluation benchmarks are unable to reflect or capture the complexity of judicial cognition and decision-making. \nFurthermore, some researchers have utilized existing traditional natural language processing datasets to construct benchmarks, such as LawBench ~\\cite{fei2023lawbench} and LaiW~\\cite{dai2023laiw}, to evaluate the performance of LLMs in the Chinese legal system.\nHowever, traditional datasets are typically designed to test specific capabilities from a computer-centric perspective, which does not always reflect the practical use of LLMs in legal applications. Moreover, these benchmarks often overlook aspects such as legal ethics, which are crucial for ensuring the safe application of LLMs in the legal domain.\nAlso, the evaluation metrics for previous tasks vary significantly, complicating the standardization of model performance measurement.\nSimply integrating existing datasets cannot provide a standardized and comprehensive evaluation of LLMs' capabilities in the legal domain.\n\nIn light of these limitations, we present LexEval: a comprehensive Chinese legal benchmark for evaluating LLMs. LexEval focuses on practical legal applications, involving how legal professionals manage, contemplate, and resolve legal issues.\nFirstly, to systematically organize various evaluation tasks, we propose a Legal Cognitive Ability Taxonomy (LexAbility Taxonomy), which includes six aspects: Memorization, Understanding, Logic Inference, Discrimination, Generation, and Ethic. \nThis taxonomy comprehensively analyzes various legal tasks and their intrinsic connections, constructing a systematic framework for evaluating LLMs.\nThen, based on the LexAbility Taxonomy, we collect 14,150 questions covering 23 legal tasks.\nTo our knowledge, LexEval is the largest and most comprehensive Chinese legal benchmarking dataset for evaluating LLMs.\nMoreover, LexEval is constructed from existing legal datasets (reorganized into a unified format), exam datasets in reality, and manually curated datasets. It adopts standard evaluation methods and metrics, laying a solid groundwork for future expansion and the integration of diverse tasks.\nIt's important to acknowledge that despite its comprehensiveness, LexEval may not cover every practical application task within the legal field.  As a platform supporting further research, LexEval encourages individuals to contribute additional tasks to the taxonomy, collectively pushing the boundaries of what's achievable in the field of legal language understanding and generation.\nWe conduct a thorough evaluation of 38 popular LLMs, including General LLMs and Legal-specific LLMs. The experimental results show that the existing LLMs are ineffective and unreliable in addressing legal problems. We hope this benchmark can point out different directions for future work.\n\n\\section{Related Work}\n\nIn recent years, large language models (LLMs) have drawn great attention in academia and industry for their excellent performance and wide applicability~\\cite{openai2023gpt4,zeng2022glm,JTR}.\nModels such as ChatGPT and ChatGLM achieve excellent performance across various tasks through mechanisms such as pre-training, supervised fine-tuning, and alignment with human or AI feedback~\\cite{bai2022training,christiano2017deep,dong,dong2}.\nBy learning from massive amounts of text data, LLMs can capture the subtle differences and complex patterns of language, demonstrating the great potential in understanding and generating human language.\n\nHowever, despite great success, they face significant challenges in the legal domain~\\cite{li2023dark,cheong2024not,deroy2023ready,li2023thuircoliee2023incorporatingstructural}.\nIn the legal domain, accuracy, reliability, and fairness are crucial, but LLMs often perform poorly in these aspects due to issues like hallucination~\\cite{li2023dark} and inherent biases~\\cite{zhang2024evaluationethicsllmslegal,chu2024prepeerreviewbased,li2024calibraevalcalibratingpredictiondistribution}.\nHallucination refers to models generating information that is not based on facts, which can lead to misleading or entirely incorrect conclusions in legal documents and consultations. Additionally, due to biases in the training data, the model may inadvertently replicate and amplify these biases, affecting its fairness and accuracy in applications such as legal judgment prediction, case analysis, and contract review.\n\nTo mitigate these issues, the community has proposed a series of evaluation criteria and benchmarks~\\cite{guha2023legalbench,fei2023lawbench,dai2023laiw,lecardv2}. For example, LegalBench~\\cite{guha2023legalbench} is dedicated to the collaborative evaluation of legal reasoning tasks in English LLMs, consisting of 162 tasks contributed by 40 contributors.\nLawbench~\\cite{fei2023lawbench} and LaiW~\\cite{dai2023laiw} have conducted evaluations on the Chinese legal system using existing traditional natural language processing datasets, contributing to the development of the community. However, these datasets all focus on the partial performance of LLMs and do not provide a comprehensive evaluation.\nIn this paper, we devote to a more comprehensive evaluation of the performance of LLMs in the legal domain. Leveraging the proposed legal cognitive ability taxonomy, we constructed the largest legal benchmark in the Chinese community through various means.\n\n\\section{LexEval}\n\n\\begin{figure}[t]\n\\vspace{-3mm}\n\\centering\n\\includegraphics[width=\\columnwidth]{LACT.pdf}\n\\vspace{-3mm}\n\\caption{Overview of the legal cognitive ability taxonomy. }\n\\label{taxonomy}\n\\vspace{-3mm}\n\\end{figure}\n\n\\subsection{Design Principle}\nThe motivation behind LexEval is to help developers quickly understand the capabilities of LLMs within the legal domain across multiple dimensions, enabling them to focus on specific areas for enhancement.\nTo this end, we advocate for considering the hierarchy and connections of abilities, rather than organizing evaluations based solely on difficulty or in a discrete manner. Nevertheless, research on the hierarchical abilities of LLMs is still in the early stages, and to our knowledge, there isn’t a well-developed taxonomy describing the abilities of LLMs in legal applications\\cite{tax}.\nDrawing inspiration from Bloom's taxonomy~\\cite{krathwohl2002revision} and real-world legal application scenarios, we propose a legal cognitive ability taxonomy (LexAbility Taxonomy) to guide the organization of tasks in LexEval.\n\nAs depicted in Figure ~\\ref{taxonomy}, the taxonomy categorizes the application of LLMs in the legal domain into six ability levels: Memorization, Understanding, Logic Inference, Discrimination, Generation, and Ethic.\nAt the Memorization level, LLMs are tasked with memorizing and recalling legal information, including fundamental legal statutes, case law, basic legal principles, and specialized legal terminology, among other essential content.\nMoving to the Understanding level, LLMs must demonstrate an aptitude for comprehending the meaning and implications of legal information. They should possess the ability to interpret legal concepts, texts, and issues accurately.\nLogical Inference involves the capacity for legal reasoning and deductive logic. LLMs should be capable of deducing conclusions based on provided legal facts and rules, identifying and applying legal patterns and principles effectively.\nThe Discrimination level necessitates LLMs to analyze and evaluate the significance of legal information according to specific criteria.\nAt the Generation level, LLMs are expected to produce professional legal documents and argumentative texts within specific legal contexts. This includes drafting legal writings, contracts, and providing legal opinions. LLMs should generate precise, legally sound, and well-structured texts based on given conditions and requirements.\nFinally, the Ethics level requires LLMs to make judgments about ethical issues in the legal domain. Models should identify and analyze legal ethical issues, make ethical decisions, and weigh advantages and disadvantages. They must consider ethical principles of law, professional ethics, and social values in their decision-making processes.\n\nEach level contains several specific evaluation tasks corresponding to the respective abilities. Legal practitioners can employ this taxonomy to identify the cognitive levels attained by LLMs, thereby enhancing the planning of training objectives and downstream applications. \nIt is important to note that this legal cognitive ability taxonomy does not imply a linear learning process. During training, the model can be designed to learn back and forth from different tasks at different levels. Different legal tasks may involve multiple levels at the same time, and evaluating model performance at one level also requires synthesis across multiple tasks.\nAs these ability levels in LexEval are developed and refined, LLMs will become increasingly integrated into legal practice, enhancing the efficiency, accuracy, and ethical standards of legal work. This taxonomy not only provides a framework for assessing the current capabilities of LLMs but also guides future advancements in the field. \nAlthough the taxonomy is primarily designed for the Chinese legal system, we believe it can be extended to involve other legal tasks in other countries as well, as these ability levels are universal across different legal systems. \n\n\\subsection{Data Collection and Processing}\n\\label{data}\n\\textbf{Data Source:} The data in LexEval comes from three sources.  The first source comprises existing datasets and corpora, primarily including CAIL~\\footnote{The data from CAIL can be found on the official website of the competition\\url{http://cail.cipsc.org.cn/}.}, JEC-QA~\\cite{zhong2020jec}, and LeCaRD~\\cite{ma2021lecard}. As these resources are originally designed for non-LLM evaluation settings, we standardize the data format and adjust the prediction targets to align with the evaluation objectives of LLMs.\nThe second source originates from the National Uniform Legal Profession Qualification Examination, which is a uniform national examination for assessing qualifications to practice the legal profession in China. We carefully select and adapt exam questions from previous years to suit our evaluation framework.  The third task source is expert annotation, where we hire 18 experts in the legal field as annotators to craft precise and relevant evaluation questions. Detailed data sources and licenses can be found in Appendix ~\\ref{task detail}.\n\n\\textbf{Data Processing:} We collect data in various formats, including PDF, JSON, LaTeX, and Word, among others. By using techniques such as OCR, we first convert PDF and Word documents into textual form. For those questions that are difficult to parse automatically, we process them manually.\nSubsequently, all questions are converted into structured data using JSON format. \nAll questions (except for the Generation level task) are converted to multiple-choice format. This is because multiple-choice questions have clearly defined metrics (i.e., accuracy) and are also a simple and effective way to evaluate the capabilities of the LLMs, which have been widely used in various benchmarks~\\cite{fei2023lawbench,dai2023laiw,huang2023c}. Detailed construction processes for each task can be found in Appendix ~\\ref{Process}.\nAll questions have been verified by the authors in multiple rounds to ensure their accuracy and reasonableness.\n\n\\textbf{Data Annotation:}\nFor tasks lacking existing datasets, we hire professional annotators to create entirely new datasets.\nOur annotation team consists of 18 legal experts who have all passed the National Uniform Legal Profession Qualification Examination. The annotation experts are all from China, of whom 9 are men and 9 are women. Before the beginning of the annotation work, we signed a legally effective agreement with all annotation experts to protect their rights and interests. To ensure the quality of annotation, all annotators first go through several hours of interpretation to understand their respective tasks. After that, we provide several examples to help them understand the format of tasks. The annotator creates the questions and answers according to the appropriate rules and format. Our gold annotators, who hold a Ph.D. in law, cross-check and inspect all generated questions. \nBefore formal annotation, each annotator creates 100 questions and answers corresponding to the task. Subsequently, only annotators who achieve a 90\\% approved rate through cross-checking and inspection are allowed to annotate formally.\nWe remove questions that are too simple and try to ensure that the distribution of causes is as balanced as possible. For each approved question, we pay the legal expert 0.7 dollars. We have annotated a total of 6,250 questions, with a total payment of 4,375 dollars. Detailed annotation guidance for each task can be found in Appendix ~\\ref{Process} and Appendix ~\\ref{guidelines}.\n \nBuilt upon the above processing, we finally select and construct 23 evaluation tasks in LexEval. For the existing datasets, we try our best to avoid using datasets that have already been extensively mined by existing LLMs (e.g. C-Eval) so that the risk of test data leakage could be minimized. To ensure the quality of LexEval, we also try to balance the distributions of legal documents from different causes, thereby avoiding bias or long-tail effects in the dataset. \n\n\\subsection{Task Definition}\nBased on the legal cognitive ability taxonomy, we construct a series of evaluation tasks. Table \\ref{overview} shows the overview of tasks in LexEval.\nThese tasks may simultaneously evaluate one or multiple ability levels, and we categorize them based on their primary ability level. Each task has at least 100 evaluation samples. Among these tasks, 11 tasks are derived from existing datasets, 2 tasks come from the National Uniform Legal Profession Qualification Examination, and 10 tasks are annotated by experts. Detailed task definition, construction process, and task statistics can be found in Appendix ~\\ref{sec:tasks}. Based on these tasks, LexEval not only provides comprehensive coverage of legal knowledge and reasoning ability but also detects issues such as bias and discrimination in legal ethics, providing valuable insights for in-depth evaluation and analysis.\n\n\\subsection{Legal and Ethical Considerations}\n\\label{ethical}\nDue to the sensitivity of the legal domain, we conducted a thorough review for this benchmark. All open-source datasets we utilized are licensed. LexEval tasks are subject to different licenses. Appendix ~\\ref{license}  provides a summary of the licenses. The authors take full responsibility for any infringement and confirm the authorization of the dataset.\nOur evaluation task strictly avoids involving the speculation of sensitive information about individuals and the generation of insulting or sensitive statements. \nIn addition, we have carefully screened and filtered the data sets in LexEval for any content that contains personally identifiable information, discriminatory content, explicit, violent, or offensive content. The data set has been ethically reviewed by legal experts. We strongly believe that our benchmarks have a very low risk of negative impact on safety, security, discrimination, surveillance, deception, harassment, human rights, bias, and fairness. Appendix ~\\ref{impact} discusses the potential social impacts.\n\n\\begin{table*}[t]\n\\centering\n\\small\n\\caption{Details of tasks within LexEval.}\n\\begin{tabular}{lclclc}\n\\hline\nLevel                            & ID  & Task                          & Metrics & Data Source            & Test Set \\\\ \\hline\n\\multirow{3}{*}{Memorization}    & 1-1 & Legal Concept                 & Accuracy    & JEC-QA~\\cite{zhong2020jec}           & 500      \\\\\n                                 & 1-2 & Legal Rule                    & Accuracy    & Expert Annotation & 1000     \\\\\n                                 & 1-3 & Legal Evolution               & Accuracy    & Expert Annotation & 300      \\\\ \\hline\n\\multirow{5}{*}{Understanding}   & 2-1 & Legal Element Recognition     & Accuracy    & CAIL-2019         & 500      \\\\\n                                 & 2-2 & Legal Fact Verification       & Accuracy    & Expert Annotation & 300      \\\\\n                                 & 2-3 & Reading Comprehension         & Accuracy    & CAIL-2021         & 100      \\\\\n                                 & 2-4 & Relation Extraction           & Accuracy    & CAIL-2022         & 500      \\\\\n                                 & 2-5 & Named-entity Recognition      & Accuracy    & CAIL-2021         & 500      \\\\ \\hline\n\\multirow{6}{*}{Logic Inference} & 3-1 & Cause Prediction              & Accuracy    & CAIL-2018         & 1000     \\\\\n                                 & 3-2 & Article Prediction            & Accuracy    & CAIL-2018         & 1000     \\\\\n                                 & 3-3 & Penalty Prediction            & Accuracy    & CAIL-2018         & 1000      \\\\\n                                 & 3-4 & Multi-hop Reasoning           & Accuracy    & Exams             & 500      \\\\\n                                 & 3-5 & Legal Calculation             & Accuracy    & Expert Annotation & 400      \\\\\n                                 & 3-6 & Argument Mining               & Accuracy    & CAIL-2021         & 500      \\\\ \\hline\n\\multirow{2}{*}{Discrimination}  & 4-1 & Similar Case Identification   & Accuracy    & LeCaRD~\\cite{ma2021lecard}\\&CAIL-2019 & 500      \\\\\n                                 & 4-2 & Document Proofreading         & Accuracy    & Expert Annotation & 300      \\\\ \\hline\n\\multirow{4}{*}{Generation}      & 5-1 & Summary Generation            & Rouge-L   & CAIL-2020         & 1000     \\\\\n                                 & 5-2 & Judicial Analysis Generation  & Rouge-L   & Expert Annotation & 1000     \\\\\n                                 & 5-3 & Legal Translation             & Rouge-L   & Expert Annotation & 250      \\\\\n                                 & 5-4 & Open-ended Question Answering & Rouge-L   & Exams             & 500      \\\\ \\hline\n\\multirow{3}{*}{Ethic}           & 6-1 & Bias and Discrimination       & Accuracy    & Expert Annotation & 1000     \\\\\n                                 & 6-2 & Morality                      & Accuracy    & Expert Annotation & 1000     \\\\\n                                 & 6-3 & Privacy                       & Accuracy    & Expert Annotation & 500      \\\\ \\hline\n\\end{tabular}\n\\vspace{-3mm}\n\n\\label{overview}\n\\vspace{-4mm}\n\\end{table*}\n\n\\section{Evaluation}\nIn this section, we present the experimental setup, evaluated models, and experimental results.\n\\subsection{Setup}\nWe evaluate the LLMs in both zero-shot and few-shot settings. In the zero-shot setting, the inputs to LLMs are only instructions and queries. In the few-shot setting, we design three different examples for each task. These examples can be found on the GitHub website. When evaluating LLMs, we set the temperature to 0 to minimize the variance introduced by random sampling. For chat LLMs, we reserve the format of their dialog prompts. When the input length exceeds the maximum context length of LLMs, we truncate the input sequence from the middle since the front and end of the input may contain crucial information. The input prompts used during our evaluation can be found in the Appendix~\\ref{sec:tasks}. \nWe standardize our evaluation metrics by using Accuracy to evaluate all multiple-choice questions and Rough-L to evaluate tasks at Generation level.\n\nThe evaluation metrics for each task can be found in Table ~\\ref{overview}. We also discuss the limitations of the evaluation metrics in Appendix ~\\ref{limitation}.\n\n\\subsection{Evaluated Models}\nWe evaluate a total of 38 popular models, categorized into two main groups: General LLMs and Legal-specific LLMs.\n\nThere are 29 General LLMs, including GPT-4~\\cite{openai2023gpt4}, ChatGPT~\\cite{gpt3}, LLaMA-2-7B~\\cite{touvron2023llama}, LLaMA-2-7B-Chat~\\cite{touvron2023llama}, LLaMA-2-13B-Chat~\\cite{touvron2023llama}, ChatGLM-6B~\\cite{zeng2022glm}, ChatGLM2-6B~\\cite{zeng2022glm}, ChatGLM3-6B~\\cite{zeng2022glm}, Baichuan-7B-base~\\cite{yang2023baichuan}, Baichuan-13B-base~\\cite{yang2023baichuan}, Baichuan-13B-Chat~\\cite{yang2023baichuan}, Qwen-7B-chat~\\cite{bai2023qwen}, Qwen-14B-Chat~\\cite{bai2023qwen}, MPT-7B~\\cite{MosaicML2023Introducing}, MPT-7B-Instruct~\\cite{MosaicML2023Introducing}, XVERSE-13B, InternLM-7B~\\cite{2023internlm}, InternLM-7B-Chat~\\cite{2023internlm}, Chinese-LLaMA-2-7B~\\cite{Chinese-LLaMA-Alpaca}, Chinese-LLaMA-2-13B~\\cite{Chinese-LLaMA-Alpaca}, TigerBot-Base, Chinese-Alpaca-2-7B~\\cite{Chinese-LLaMA-Alpaca}, GoGPT2-7B, GoGPT2-13B, Ziya-LLaMA-13B~\\cite{fengshenbang}, Vicuna-v1.3-7B, BELLE-LLAMA-2-13B~\\cite{BELLE}, Alpaca-v1.0-7B, MoSS-Moon-sft~\\cite{sun2023moss}.\n\nThe Legal-specific LLMs include 9 models, which are ChatLaw-13B~\\cite{cui2023chatlaw}, ChatLaw-33B~\\cite{cui2023chatlaw}, LexiLaw, Lawyer-LLaMA~\\cite{huang2023lawyer}, Wisdom-Interrogatory, LaWGPT-7B-beta1.0, LaWGPT-7B-beta1.1, HanFei~\\cite{HanFei}, Fuzi-Mingcha~\\cite{sdu_fuzi_mingcha}.. \nThe specific description of evaluated models can be found in the Appendix~\\ref{sec:models}.\n\n\\begin{table*}[t]\n\\scriptsize\n\\centering\n\\caption{Zero-shot performance(\\%) of various models at Memorization, Understanding, and Logic Inference level. Best preformance in each column is marked bold. }\n\\begin{tabular}{l|ccc|ccccc|cccccc}\n\\hline\n\\multirow{2}{*}{Model} & \\multicolumn{3}{c|}{Memorization(Acc.)} & \\multicolumn{5}{c|}{Understanding(Acc.)} & \\multicolumn{6}{c}{Logic Inference(Acc.)}     \\\\\n                       & 1-1       & 1-2       & 1-3       & 2-1   & 2-2   & 2-3  & 2-4  & 2-5  & 3-1  & 3-2  & 3-3  & 3-4  & 3-5  & 3-6  \\\\ \\hline\nGPT-4               & \\textbf{34.0} & 35.4          & 14.0          & 79.8          & \\textbf{51.0} & \\textbf{94.0} & 78.0          & \\textbf{96.2} & \\textbf{80.3} & 68.3          & \\textbf{53.7} & \\textbf{33.2} & \\textbf{66.0} & \\textbf{57.8} \\\\\nQwen-14B-Chat       & 28.0          & 38.6          & 11.4          & \\textbf{93.4} & 45.3          & 90.0          & \\textbf{85.6} & 91.8          & 80.2          & \\textbf{91.0} & 27.9          & 31.6          & 44.7          & 50.4          \\\\\nQwen-7B-Chat        & 22.8          & \\textbf{38.9} & 8.4           & 79.8          & 43.3          & 87.0          & 67.2          & 92.0          & 79.2          & 83.9          & 53.2          & 24.2          & 36.3          & 45.0          \\\\\nChatGPT             & 19.0          & 25.6          & 9.0           & 56.8          & 42.3          & 87.0          & 76.0          & 82.2          & 77.7          & 60.3          & 23.0          & 19.4          & 39.6          & 38.2          \\\\\nInternLM-7B-Chat    & 20.4          & 35.4          & 11.0          & 61.4          & 42.3          & 89.0          & 49.4          & 53.8          & 79.3          & 77.9          & 28.8          & 23.8          & 38.3          & 30.0          \\\\\nBaichuan-13B-Chat   & 14.6          & 33.9          & 10.0          & 54.2          & 35.0          & 72.0          & 62.2          & 75.4          & 77.0          & 58.0          & 41.8          & 20.2          & 33.5          & 21.0          \\\\\nChatGLM3            & 19.2          & 28.9          & 7.7           & 41.0          & 34.3          & 80.0          & 62.8          & 81.4          & 73.4          & 61.2          & 19.4          & 21.4          & 25.6          & 37.0          \\\\\nBaichuan-13B-base   & 22.6          & 23.0          & 9.0           & 43.2          & 26.7          & 75.0          & 59.2          & 74.4          & 58.3          & 25.6          & 12.5          & 23.8          & 31.0          & 19.6          \\\\\nFuzi-Mingcha        & 13.0          & 25.0          & 6.7           & 62.0          & 29.0          & 61.0          & 46.4          & 24.8          & 68.0          & 58.6          & 25.5          & 16.0          & 28.9          & 20.4          \\\\\nChatLaw-33B         & 16.0          & 25.9          & 7.0           & 51.4          & 32.3          & 76.0          & 67.6          & 62.0          & 60.6          & 32.9          & 23.0          & 15.4          & 23.6          & 37.6          \\\\\nChatGLM2            & 28.2          & 13.6          & 16.4          & 22.4          & 24.0          & 61.0          & 40.0          & 29.8          & 77.2          & 54.4          & 24.8          & 19.8          & 27.7          & 8.6           \\\\\nChinese-Alpaca-2-7B & 19.8          & 24.8          & \\textbf{19.7} & 25.0          & 33.3          & 61.0          & 46.6          & 24.2          & 66.8          & 39.4          & 20.6          & 16.4          & 18.0          & 26.6          \\\\\nBELLE-LLAMA-2-Chat  & 15.0          & 25.7          & 7.0           & 31.4          & 27.3          & 77.0          & 61.6          & 46.2          & 64.1          & 47.3          & 8.2           & 19.8          & 33.2          & 24.4          \\\\\nXVERSE-13B          & 25.4          & 29.0          & 12.0          & 47.0          & 21.7          & 71.0          & 48.2          & 32.4          & 54.9          & 44.7          & 9.9           & 19.2          & 27.7          & 14.6          \\\\\nTigerBot-base       & 16.6          & 27.5          & 9.0           & 22.4          & 27.0          & 58.0          & 57.0          & 24.6          & 71.5          & 35.7          & 18.3          & 19.0          & 31.2          & 18.8  \\\\\n\\hline\n\\end{tabular}\n\\label{zero-1}\n\\vspace{-5mm}\n\\end{table*}\n\n\\begin{table*}[t]\n\\centering\n\\scriptsize\n\\caption{Zero-shot performance(\\%) of various models at Discrimination, Generation, and Ethic level. Best preformance in each column is marked bold.}\n\\begin{tabular}{l|cc|cccc|ccc|c|c}\n\\hline\n\\multirow{2}{*}{Model} & \\multicolumn{2}{c|}{Discrimination(Acc.)} & \\multicolumn{4}{c|}{Generation(Rough-L)} & \\multicolumn{3}{c|}{Ethic(Acc.)} & \\multirow{2}{*}{Average} & \\multirow{2}{*}{Rank} \\\\\n                       & 4-1              & 4-2              & 5-1    & 5-2    & 5-3   & 5-4   & 6-1     & 6-2     & 6-3    &                          &                       \\\\ \\hline\nGPT-4               & 35.8          & \\textbf{39.1} & 25.0          & 16.0          & \\textbf{38.3} & 13.6          & \\textbf{65.2} & \\textbf{55.2} & \\textbf{75.8} & \\textbf{52.4} & \\textbf{1} \\\\\nQwen-14B-Chat       & 30.0          & 31.9          & 33.9          & 23.1          & 36.0          & \\textbf{19.1} & 29.2          & 42.0          & 63.0          & 48.6          & 2          \\\\\nQwen-7B-Chat        & 21.0          & 28.6          & 30.8          & 19.0          & 34.7          & 18.3          & 22.1          & 38.9          & 56.8          & 44.8          & 3          \\\\\nChatGPT             & 28.4          & 22.0          & 22.8          & 13.1          & 34.3          & 13.1          & 33.7          & 32.1          & 55.8          & 39.6          & 4          \\\\\nInternLM-7B-Chat    & \\textbf{37.0} & 9.9           & 19.6          & 2.6           & 29.2          & 11.8          & 22.7          & 27.8          & 47.4          & 36.9          & 5          \\\\\nBaichuan-13B-Chat   & 24.4          & 20.4          & 29.2          & 24.2          & 35.7          & 16.0          & 16.4          & 22.0          & 40.8          & 36.4          & 6          \\\\\nChatGLM3            & 25.2          & 14.1          & 28.3          & 17.0          & 29.7          & 14.4          & 21.2          & 29.6          & 49.6          & 35.8          & 7          \\\\\nBaichuan-13B-base   & 15.6          & 23.0          & 21.5          & \\textbf{27.8} & 24.0          & 11.8          & 17.3          & 28.6          & 47.0          & 31.3          & 8          \\\\\nFuzi-Mingcha        & 20.0          & 16.1          & \\textbf{57.8} & \\textbf{27.8} & 21.4          & 17.3          & 10.8          & 13.1          & 25.0          & 30.2          & 9          \\\\\nChatLaw-33B         & 10.0          & 17.1          & 23.8          & 9.9           & 15.2          & 13.3          & 15.3          & 19.1          & 34.2          & 30.0          & 10         \\\\\nChatGLM2            & 20.2          & 21.1          & 28.4          & 15.5          & 24.1          & 14.0          & 36.8          & 27.2          & 52.2          & 29.9          & 11         \\\\\nChinese-Alpaca-2-7B & 27.8          & 24.7          & 28.6          & 15.7          & 31.2          & 14.6          & 21.5          & 28.4          & 40.4          & 29.4          & 12         \\\\\nBELLE-LLAMA-2-Chat  & 3.6           & 20.4          & 28.0          & 11.4          & 25.4          & 15.3          & 13.8          & 16.6          & 30.4          & 28.4          & 13         \\\\\nXVERSE-13B          & 10.4          & 12.2          & 12.1          & 13.9          & 6.8           & 19.0          & 19.9          & 29.4          & 55.0          & 27.7          & 14         \\\\\nTigerBot-base       & 25.8          & 23.0          & 20.8          & 11.3          & 34.5          & 12.6          & 16.3          & 19.0          & 39.2          & 27.3          & 15         \\\\\n\\hline\n\\end{tabular}\n\\label{zero-2}\n\\end{table*}\n\n\\subsection{Experimental Results}\nWe report the zero-shot performance scores of all models in Table \\ref{zero-1} and \\ref{zero-2}. \nDue to space limitations, we only show the performance of the top 15 models. More experimental results can be found in the Appendix~\\ref{sec:results}. From the experimental results, we have the following findings:\n\n\\begin{table*}[t]\n\\scriptsize\n\\centering\n\\vspace{-5mm}\n\\caption{Few-shot performance(\\%) of various models at Memorization, Understanding, and Logic Inference level. Best performance in each column is marked bold. $\\uparrow$/$\\downarrow$ represents the performance increase/decrease compared to the zero-shot setting.}\n\\begin{tabular}{l|ccc|ccccc|cccccc}\n\\hline\n\\multirow{2}{*}{Model} & \\multicolumn{3}{c|}{Memorization(Acc.)} & \\multicolumn{5}{c|}{Understanding(Acc.)} & \\multicolumn{6}{c}{Logic Inference(Acc.)}     \\\\\n                       & 1-1       & 1-2       & 1-3       & 2-1   & 2-2   & 2-3  & 2-4  & 2-5  & 3-1  & 3-2  & 3-3  & 3-4  & 3-5  & 3-6  \\\\ \\hline\nGPT-4             & 31.0          & 42.3          & 16.4          & \\textbf{96.8} & \\textbf{52.3} & \\textbf{95.0} & \\textbf{97.4} & \\textbf{98.0} & \\textbf{79.7} & 66.3          & 53.1          & 27.2          & \\textbf{64.5} & \\textbf{60.0} \\\\\nQwen-14B-Chat     & \\textbf{34.0} & \\textbf{49.7} & 13.4          & 95.4          & 42.7          & 92.0          & 88.4          & 88.2          & 58.6          & \\textbf{90.7} & 61.2          & \\textbf{34.2} & 47.2          & 41.0          \\\\\nQwen-7B-Chat      & 23.2          & 42.3          & 8.7           & 82.2          & 34.7          & 85.0          & 60.4          & 49.2          & 78.1          & 77.2          & \\textbf{61.6} & 24.4          & 35.8          & 41.8          \\\\\nChatGPT           & 22.0          & 26.8          & 7.0           & 85.4          & 36.3          & 84.0          & 83.0          & 59.2          & 76.8          & 58.8          & 24.3          & 21.8          & 42.1          & 36.4          \\\\\nInternLM-7B-Chat  & 20.8          & 33.7          & 8.4           & 84.0          & 39.0          & 83.0          & 73.4          & 85.4          & 79.4          & 77.7          & 33.4          & 24.0          & 36.5          & 36.4          \\\\\nChatGLM3          & 20.6          & 31.8          & 6.4           & 69.0          & 36.3          & 76.0          & 66.8          & 68.0          & 73.9          & 64.5          & 16.0          & 19.0          & 28.2          & 38.2          \\\\\nBaichuan-13B-base & 21.6          & 28.1          & \\textbf{17.1} & 82.2          & 24.0          & 75.0          & 83.4          & 72.0          & 74.0          & 52.1          & 40.0          & 19.8          & 33.0          & 27.4          \\\\\nBaichuan-13B-Chat & 15.8          & 33.4          & 8.4           & 58.8          & 27.7          & 55.0          & 54.6          & 67.4          & 56.6          & 46.9          & 36.2          & 21.6          & 29.9          & 29.6          \\\\\nLLaMA-2-13B-Chat  & 14.6          & 25.8          & 6.0           & 75.4          & 32.0          & 71.0          & 64.4          & 58.6          & 59.8          & 55.1          & 25.4          & 14.6          & 33.5          & 32.6          \\\\\nChatGLM2          & 23.6          & 27.4          & 10.0          & 55.2          & 34.3          & 56.0          & 39.8          & 36.4          & 76.2          & 49.2          & 28.5          & 20.4          & 27.7          & 26.4                            \\\\\n\\hline\n\\end{tabular}\n\\vspace{-3mm}\n\\label{few-1}\n\\end{table*}\n\n\\begin{table*}[t]\n\\scriptsize\n\\centering\n\\caption{Few-shot performance(\\%) of various models at the Discrimination, Generation, and Ethic level. Best performance in each column is marked bold. $\\uparrow$/$\\downarrow$ represents the performance increase/decrease compared to the zero-shot setting.}\n\\begin{tabular}{l|cc|cccc|ccc|c|c}\n\\hline\n\n\\multirow{2}{*}{Model} & \\multicolumn{2}{c|}{Discrimination(Acc.)} & \\multicolumn{4}{c|}{Generation(Rough-L)} & \\multicolumn{3}{c|}{Ethic(Acc.)} & \\multirow{2}{*}{Average} & \\multirow{2}{*}{Rank} \\\\\n                       & 4-1              & 4-2              & 5-1    & 5-2    & 5-3   & 5-4   & 6-1     & 6-2     & 6-3    &                          &                       \\\\ \\hline\nGPT-4             & \\textbf{32.3} & \\textbf{36.5} & 22.4          & 19.1          & \\textbf{37.9} & 16.4          & \\textbf{65.6} & \\textbf{52.8} & \\textbf{72.2} & \\textbf{53.7}$\\uparrow$ & \\textbf{1} \\\\\nQwen-14B-Chat     & 26.0          & 32.2          & 12.0          & 23.9          & 37.0          & \\textbf{23.4} & 34.3          & 51.9          & 70.8          & 49.9$\\uparrow$          & 2          \\\\\nQwen-7B-Chat      & 24.8          & 30.3          & 27.1          & 18.4          & 34.5          & 21.5          & 27.9          & 38.9          & 59.6          & 42.9$\\downarrow$          & 3          \\\\\nChatGPT           & 31.3          & 26.3          & 17.2          & 14.1          & 35.0          & 16.6          & 41.0          & 32.9          & 61.8          & 40.9$\\uparrow$          & 4          \\\\\nInternLM-7B-Chat  & 32.2          & 15.8          & 16.7          & 0.9           & 24.1          & 13.4          & 21.3          & 29.3          & 44.0          & 39.7$\\uparrow$          & 5          \\\\\nChatGLM3          & 15.8          & 13.2          & \\textbf{27.8} & 19.1          & 29.4          & 16.1          & 20.6          & 28.8          & 46.6          & 36.2$\\uparrow$          & 6          \\\\\nBaichuan-13B-base & 1.0           & 12.5          & 4.6           & \\textbf{28.8} & 6.3           & 9.5           & 16.6          & 27.5          & 29.4          & 34.2$\\uparrow$          & 7          \\\\\nBaichuan-13B-Chat & 27.2          & 18.1          & 19.8          & 18.0          & 34.7          & 18.2          & 18.7          & 27.6          & 46.6          & 33.5$\\downarrow$          & 8          \\\\\nLLaMA-2-13B-Chat  & 27.2          & 17.1          & 18.5          & 12.5          & 17.5          & 15.3          & 17.0          & 16.7          & 39.6          & 32.6$\\uparrow$          & 9          \\\\\nChatGLM2          & 19.0          & 19.1          & 15.6          & 14.9          & 21.3          & 16.8          & 35.6          & 26.3          & 55.4          & 32.0$\\uparrow$          & 10   \\\\\n \\hline\n\\end{tabular}\n\\vspace{-3mm}\n\n\\label{few-2}\n\\end{table*}\n\n\\begin{itemize}[leftmargin=*]\n    \\item The open-source model perform slightly worse compared to the closed-source model GPT-4, which achieve the best performance in the benchmark. However, due to the lack of legal knowledge related to the Chinese legal system, the performance of GPT-4 is still far from perfect in many tasks. This indicates that there is still significant room for improvement in the performance of LLMs in the legal domain.\n    \\item Increasing model size leads to better performance, which is equally applicable in the legal domain. For example, Qwen-14B performs better than Qwen-7B. Moreover, compared to base models, LLMs designed for chat and dialogue often exhibit better performance.  For example, Baichuan-13B-Chat performs better than Baichuan-13B-base. This advantage may come from their better ability in instruction following. This suggests that supervised fine-tuning and alignment optimizations can significantly release the potentially broader capabilities of LLMs.\n    \\item Surprisingly, Legal-specific LLMs do not always perform better than General LLMs. We speculate that there are two possible reasons. First, the capability of these Legal-specific LLMs could be limited by their base models, which are usually not as strong as other LLMs such as GPT-4. Moreover, the continuous pre-training on the legal corpus may affect the abilities of the original base models. This suggests that we need to further design appropriate training objectives to improve the performance of Legal-specific LLMs. \n    \\item In tasks at the Memorization level, most models perform poorly on legal evolution (1-3) tasks. Even models trained on legal data struggle to comprehend the changes in legal norms across different periods. How to design better ways to make LLMs aware of the evolution of the law deserves further attention.\n\\end{itemize}\n\nTables \\ref{few-1} and \\ref{few-2} show the few-shot performance of top 10 LLMs at different levels. Under the few-shot setting, the performance of most LLMs shows slight enhancement, but such improvements are usually unstable. \nThe improvement brought by few-shot examples varies across different models. Some models (e.g. GPT-4) experience performance improvements, while others (e.g. Qwen-14B-Chat) may suffer degradation.\nWe speculate that the few-shot setting may generate inputs that are overly lengthy for certain LLMs, posing challenges for them to comprehend the overall text provided with examples. Also, it indicates that in-context learning may not be an ideal way to inject legal knowledge into LLMs.\n\nFinally, in Figure \\ref{chart}, we show the zero-shot performance of the best six models in different legal cognitive ability levels. We derive the following observations from the experiment results.\n\n\\begin{itemize}[leftmargin=*]\n    \\item LLMs perform poorly at the Memorization level, which may be the critical obstacle to performing tasks at a higher level. Given that even Legal-specific LLMs also exhibit weaknesses (see Appendix ~\\ref{sec:results}), merely increasing legal corpora during pre-training may not be the optimal solution.\n    \\item Most models perform best at the Understanding and Logic Inference levels. Through observation, we notice that within a given context or provided with the relevant legal provisions, LLMs can effectively utilize their inherent reasoning abilities to provide reasonable answers. Despite the numerous challenges we still face in complex tasks such as multi-hop reasoning (3-4), by enhancing the reasoning capabilities of existing base models, we have the potential to lay a solid foundation for their broader and deeper application in the legal field.\n    \\item The performance on the Discrimination level indicates that current LLMs do not yet possess the ability to discern and evaluate legal content. Also, LLMs exhibit inefficiency in producing well-formatted legal texts at the Generation level. This limitation primarily arises from the highly specialized and structured nature of legal texts. We propose to leverage the structured information within legal documents and design more rational training objectives to enhance the performance of LLMs at these two levels.\n    \\item At the Ethic level, although GPT-4 shows relatively good performance, its performance is still far from satisfactory.\n    The unsatisfactory performance of LLMs in ethics-related tasks poses serious challenges to their safe application in real-life scenarios. Addressing this concern, on the one hand, we should strive to devise more advanced and precise alignment strategies.  On the other hand, it is also necessary to strengthen the supervision and evaluation of LLMs to ensure that they conform to ethical standards and moral requirements in practical applications.\n    \\item Overall, at present, LLMs cannot effectively solve the legal problems under the Chinese legal system. Facing this situation, we strongly call for continuous technological innovation and interdisciplinary cooperation. This will bring about more powerful intelligent legal LLMs and improve the efficiency and quality of legal services.\n\\end{itemize}\n\n\\begin{figure}[t]\n\\vspace{-3mm}\n\\centering\n\\includegraphics[width=0.6\\columnwidth]{chart.pdf}\n\\vspace{-3mm}\n\\caption{The zero-shot performance of the six best models at different legal cognitive ability levels. }\n\\label{chart}\n\\vspace{-3mm}\n\\end{figure}\n\n\\section{Conclusion \\& Future Work}\n\nIn this paper, we introduce LexEval, which is the largest comprehensive benchmark for evaluating LLMs in the Chiese Legal Domain.\nWith 14,150 questions covering 6 legal cognitive ability levels in LexEval, we extensively evaluate the ability of 38 common LLMs. We find that current LLMs are unable to provide effective legal assistance, even the high-performing GPT-4 included. \nWe call for more technological innovations and interdisciplinary collaborations to advance the development of legal LLMs.\nIn the future, we will further enrich our benchmarks to achieve a more comprehensive evaluation. Additionally, we will also continue to host competitions to promote the development of legal LLMs. Also, LexEval always welcomes open participation and contributions.\n\n\\clearpage\n\n\\newpage\n\\clearpage\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2309.16289v1.tex",
        "arXiv-2310.05620v2.tex",
        "arXiv-2409.20288v4.tex"
    ],
    "group_id": "group_74",
    "response": "### Title: Evaluating Large Language Models in the Chinese Legal Domain: A Comparative Analysis of LexEval, LawBench, and LAiW\n\n### Introduction\nThe field of artificial intelligence (AI) has seen significant advancements with the development of large language models (LLMs), which have demonstrated remarkable capabilities in natural language processing (NLP) tasks. However, the application of these models in specialized domains such as the legal field poses unique challenges and opportunities. The legal domain, characterized by its reliance on precise, context-specific knowledge and reasoning, requires LLMs to not only generate coherent text but also to accurately recall, comprehend, and apply legal principles. This has led to the creation of specialized benchmarks designed to assess the performance of LLMs in the Chinese legal system, which is distinct from Western legal systems due to its civil law foundation and emphasis on statutory law over case law. Previous research has focused on evaluating LLMs through general NLP tasks or specific legal tasks that do not fully capture the complexity and nuances of legal reasoning and application. This paper summarizes three recent studies that introduce benchmarks tailored to the Chinese legal system: LexEval, LawBench, and LAiW. These benchmarks aim to provide a structured and comprehensive evaluation of LLMs' legal capabilities, addressing the gap between general NLP tasks and the specific requirements of legal practice.\n\nThe history of LLMs in the legal domain is marked by initial optimism followed by a realization of the limitations and risks associated with their deployment. Early studies focused on the ability of LLMs to pass legal exams, which, while indicative of certain capabilities, do not fully reflect the practical demands of legal tasks. More recent benchmarks, such as LexEval, LawBench, and LAiW, have shifted towards a more nuanced and comprehensive evaluation, covering memorization, understanding, and application of legal knowledge. These benchmarks also consider ethical implications and the potential for bias in the models, reflecting the growing awareness of the need for responsible AI development in the legal field.\n\nCurrent progress in evaluating LLMs for legal tasks includes the development of benchmarks that align with the logical structure of legal practice, such as the syllogism used by legal experts. However, challenges remain in ensuring that LLMs can effectively handle complex legal reasoning and provide reliable, ethical, and fair legal advice. The benchmarks discussed in this paper aim to address these challenges by providing a detailed taxonomy of legal cognitive abilities and a wide range of tasks that reflect the practical demands of legal professionals. This summary will delve into the specifics of each benchmark, their commonalities, and the unique contributions they offer to the field of legal AI.\n\n### Main Content of Each Paper\n\n#### LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models\nLexEval is a benchmark designed to evaluate the performance of LLMs in the Chinese legal domain. It introduces a taxonomy of legal cognitive abilities, including Memorization, Understanding, Logic Inference, Discrimination, Generation, and Ethics. The benchmark comprises 23 tasks and 14,150 questions, drawing from existing datasets, legal exams, and expert-annotated data. LexEval evaluates models in both zero-shot and few-shot settings, using metrics such as accuracy, F1 score, and ROUGE-L for classification, named entity recognition, and text generation tasks, respectively. The evaluation results indicate that while LLMs excel in text generation and logical inference, they struggle with foundational legal knowledge and ethical considerations. The top-performing model, GPT-4, demonstrates strong capabilities in logical inference and text generation but shows weaknesses in memorization and ethics. This suggests that further improvements are needed in the memorization and ethical reasoning of LLMs to make them more reliable and trustworthy in legal applications.\n\n#### LawBench: Benchmarking Legal Knowledge of Large Language Models\nLawBench is another benchmark focused on evaluating the legal knowledge of LLMs under the Chinese civil law system. It categorizes tasks into three cognitive levels: Legal Knowledge Memorization, Understanding, and Applying. The benchmark includes 20 diverse tasks covering single-label classification (SLC), multi-label classification (MLC), regression, extraction, and generation. LawBench evaluates 51 LLMs, including multilingual, Chinese-oriented, and legal-specific models, and finds that GPT-4 remains the best-performing model in the legal domain. However, even GPT-4 struggles with tasks requiring memorization and application of legal knowledge, achieving an average score of only 52.35 in zero-shot settings. The study highlights the need for models to be fine-tuned on legal-specific data to improve their performance in legal tasks. It also emphasizes the importance of evaluating LLMs' ability to apply legal knowledge in realistic scenarios, as opposed to merely memorizing or understanding it.\n\n#### LAiW: A Chinese Legal Large Language Models Benchmark\nLAiW is a benchmark that evaluates LLMs based on the logic of legal practice, dividing legal capabilities into three levels: Basic Information Retrieval (BIR), Legal Foundation Inference (LFI), and Complex Legal Application (CLA). The benchmark includes 14 tasks and is designed to assess the logical reasoning and application abilities of LLMs. LAiW evaluates a range of models, including general and legal-specific LLMs, and finds that while models like GPT-4 and ChatGPT perform well in complex legal applications, they struggle with foundational tasks such as information retrieval and legal inference. This discrepancy suggests that LLMs may lack the necessary logical reasoning skills to fully align with the requirements of legal practice. LAiW also incorporates human evaluation by legal experts to assess the reliability and ethical considerations of LLMs in legal applications. The results indicate that LLMs need to be better trained to meet the logical and ethical standards expected by legal professionals.\n\n### Commonalities and Innovations\nAll three benchmarks share the common goal of evaluating LLMs in the Chinese legal domain, focusing on their ability to recall, understand, and apply legal knowledge. They also emphasize the importance of logical reasoning and ethical considerations in legal tasks. However, each benchmark introduces unique aspects and methodologies:\n\n- **LexEval** proposes a detailed taxonomy of legal cognitive abilities, covering memorization, understanding, logic inference, discrimination, generation, and ethics. It is notable for its scale, with 14,150 questions across 23 tasks, and its comprehensive evaluation of ethical issues.\n- **LawBench** is specifically designed to assess the capabilities of LLMs in the Chinese civil law system, using a hierarchical evaluation system based on Bloom's Taxonomy. It includes tasks that simulate the judicial process and evaluates models on their ability to follow instructions and provide relevant responses.\n- **LAiW** aligns its tasks with the logical structure of legal practice, focusing on information retrieval, legal inference, and complex application. It incorporates both automated and manual evaluations, with the latter aimed at assessing the ethical and logical reasoning abilities of LLMs.\n\n### Comparison of Results and Discussion of Differences\nThe results from the three benchmarks reveal consistent trends and differences in the performance of LLMs across various legal tasks. GPT-4 consistently outperforms other models in logical inference and text generation tasks, but shows weaknesses in memorization and ethics. For instance, in LexEval, GPT-4 scores 34.0% in memorization tasks and 52.4% in ethics tasks, indicating that while it excels in complex reasoning, it struggles with foundational knowledge and ethical considerations. Similarly, in LawBench, GPT-4 achieves an average score of 52.35 in zero-shot settings, with significant improvements in one-shot settings, but still shows limitations in memorization tasks. LAiW also finds that GPT-4 performs well in complex legal applications but poorly in basic information retrieval and legal inference tasks.\n\nIn contrast, Chinese-oriented LLMs like Qwen-Chat and InternLM-Chat show better performance in tasks related to Chinese legal texts and memorization. However, they still lag behind GPT-4 in logical inference and complex application tasks. For example, in LexEval, Qwen-14B-Chat scores 33.9% in memorization tasks but only 42.9% in ethics tasks. In LawBench, Qwen-Chat achieves an average score of 37.28 in zero-shot settings, with improvements in one-shot settings, but still falls short compared to GPT-4. LAiW also notes that while these models perform well in basic tasks, they struggle with complex reasoning and ethical considerations.\n\nLegal-specific LLMs, such as Fuzi-Mingcha and ChatLaw, show mixed results. They often improve upon their base models in specific legal tasks but still lag behind general-purpose models in overall performance. For instance, in LexEval, Fuzi-Mingcha scores 39.68% in memorization tasks and 40.62% in complex application tasks, indicating that while it performs better in legal-specific tasks, it still struggles with foundational knowledge and complex reasoning. In LawBench, Fuzi-Mingcha achieves an average score of 33.05 in zero-shot settings, with improvements in one-shot settings, but still shows limitations in memorization tasks. LAiW also finds that these models perform well in specific tasks but struggle with logical inference and ethical reasoning.\n\nThe benchmarks also highlight the importance of instruction-following and fine-tuning in improving LLM performance. For example, in LexEval, models like Qwen-Chat and InternLM-Chat show better performance in tasks requiring instruction-following, such as summarization and argument mining, compared to their base models. Similarly, in LawBench, models like Qwen-Chat and InternLM-Chat show improvements in one-shot settings, indicating that providing examples can enhance their performance in legal tasks. LAiW also notes that instruction-tuned models like Qwen-Chat and InternLM-Chat perform better in complex application tasks, but still struggle with foundational tasks.\n\n### Conclusion\nThe main findings from these benchmarks indicate that while LLMs demonstrate strong capabilities in text generation and logical inference, they still struggle with foundational legal knowledge and ethical reasoning. GPT-4 remains the best-performing model across all benchmarks, but its performance is far from perfect, especially in memorization and ethics tasks. Chinese-oriented LLMs show better performance in tasks related to Chinese legal texts and memorization, but still lag behind in complex reasoning and ethical considerations. Legal-specific LLMs often improve upon their base models in specific legal tasks but still show limitations in overall performance.\n\nFuture research directions should focus on developing more effective training strategies for LLMs in the legal domain, such as fine-tuning on larger and more diverse legal corpora, and incorporating ethical reasoning into the training process. Additionally, there is a need for more interdisciplinary collaboration between legal experts and AI researchers to ensure that LLMs meet the high standards required for legal practice. The benchmarks also suggest that further work is needed to standardize evaluation metrics and tasks, and to continuously update and enrich the benchmarks to reflect the evolving demands of legal practice."
}