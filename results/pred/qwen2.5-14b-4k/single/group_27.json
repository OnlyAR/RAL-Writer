{
    "instruction": "You are an experienced researcher, I will give you some scientific research papers in the same field. Please read them carefully and write a summary about them.\n\nHere are the papers:\n\n<paper 1>\n\\title{\\modelfullns: \\\\ A Multi-Task Transformer for Robotic Manipulation}\n\n\\begin{document}\n\n\\maketitle\n\n\\vspace{-1.0cm}\n\\begin{abstract}\n    Transformers have revolutionized vision and natural language processing with their ability to scale with large datasets. But in robotic manipulation, data is both limited and expensive.  \n    Can manipulation still benefit from Transformers with the right problem formulation?\n    We investigate this question with \\model, a language-conditioned behavior-cloning agent for multi-task 6-DoF manipulation. \n    \\model~encodes language goals and RGB-D voxel observations with a Perceiver Transformer~\\citep{jaegle2021perceiver}, and outputs discretized actions by ``detecting the next best voxel action''. \n    Unlike frameworks that operate on 2D images, the voxelized 3D  observation and action space provides a strong structural prior for efficiently learning 6-DoF actions.\n    With this formulation, we train a single multi-task Transformer for \\highlight{18 RLBench tasks (with 249 variations) and 7 real-world tasks (with 18 variations)} from just a few demonstrations per task.\n    Our results show that \\model~significantly outperforms unstructured image-to-action agents and 3D ConvNet baselines for a wide range of tabletop tasks.  \n    \n    \n\\end{abstract}\n\n\\keywords{Transformers, Language Grounding, Manipulation, Behavior Cloning} \n\n\\section{Introduction}\n\nTransformers~\\citep{vaswani2017attention} have become  prevalent in natural language processing and computer vision. By framing problems as sequence modeling tasks, and training on large amounts of diverse data, Transformers have achieved groundbreaking results in several domains~\\citep{brown2020language,dosovitskiy2020image,jumper2021highly,vinyals2019alphastar}. Even in domains that do not conventionally involve sequence modeling~\\citep{chen2021pix2seq,chen2021decision}, Transformers have been adopted as a \\textit{general} architecture~\\citep{reed2022generalist}. But in robotic manipulation, data is both limited and expensive. \nCan we still bring the power of Transformers to 6-DoF manipulation with the right problem formulation?\n\nLanguage models operate on sequences of tokens~\\citep{devlin2018bert}, and vision transformers operate on sequences of image patches~\\citep{dosovitskiy2020image}. While pixel transformers~\\citep{jaegle2021perceivericml,jaegle2021perceiver} exist, they are not as data efficient as approaches that use convolutions or patches to exploit the 2D structure of images. Thus, while Transformers may be domain agnostic, they still require the right problem formulation to be data efficient. \nA similar efficiency issue is apparent in behavior-cloning (BC) agents that directly map 2D images to 6-DoF actions.\n\\highlight{Agents like Gato~\\citep{reed2022generalist} and BC-Z~\\citep{jang2022bc,ahn2022can} have shown impressive multi-task capabilities, but they require several weeks or even months of data collection}.\n\\highlight{In contrast, recent works in reinforcement-learning like C2FARM~\\citep{c2farm} construct a voxelized  observation and action space to efficiently learn visual representations of 3D actions with 3D ConvNets.}  \nSimilarly, in this work, we aim to exploit the 3D structure of \\textit{voxel patches} for efficient 6-DoF behavior-cloning with Transformers (analogous to how vision transformers~\\citep{dosovitskiy2020image} exploit the 2D structure of image patches).\n\n\\begin{figure*}[!t]\n    \\centering\n    \\hspace*{-1.45cm}\n    \\includegraphics[width=1.2\\textwidth]{figures/tasks_v1.pdf}\n    \\caption{\\textbf{Language-Conditioned Manipulation  Tasks:} \\model~is a language-conditioned multi-task agent capable of imitating a wide range of 6-DoF manipulation tasks. We conduct experiments on 18 simulated tasks in RLBench~\\citep{james2020rlbench} (a-j; only 10 shown), with several pose and semantic variations. We also demonstrate our approach with a Franka Panda on 7 real-world tasks (k-o; only 5 shown) with a multi-task agent trained with just 53 demonstrations. See the supplementary video for simulated and real-world rollouts.}\n    \\label{fig:tasks}\n    \\vspace{-1.5em}\n\\end{figure*}\n\nTo this end, we present \\model~(short for \\modelfullns), a language-conditioned BC agent that can learn to imitate a wide variety of 6-DoF manipulation tasks with just a few demonstrations per task. \\model~encodes a sequence of RGB-D voxel patches and predicts discretized translations, rotations, and gripper actions that are executed with a motion-planner in an observe-act loop. \\model~is essentially a classifier trained with supervised learning to \\textit{detect actions} akin to prior work like CLIPort~\\citep{cliport,zengTransporterNetworksRearranging2021},\nexcept our observations and actions are represented with 3D voxels instead of 2D image pixels. \nVoxel grids are  less prevalent than images in end-to-end BC approaches often due to scaling issues with high-dimensional inputs. But in \\model, we use a Perceiver\\footnote{Throughout the paper we refer to PerceiverIO~\\citep{jaegle2021perceiver} as Perceiver for brevity.} Transformer~\\citep{jaegle2021perceiver} to encode very high-dimensional input of up to 1 million voxels with only a small set of latent vectors. This voxel-based formulation provides a strong structural prior with several benefits: a natural method for fusing multi-view observations, learning robust action-centric\\footnote{\\highlight{Action-centric refers to a  system that learns perceptual representations of actions; see \\appsecref{app:more_qpred}.}} representations~\\citep{gibson2014ecological,brooks1991new}, and enabling data augmentation in 6-DoF -- all of which help learn generalizable skills by focusing on \\textit{diverse} rather than narrow multi-task data.  \n\nTo study the effectiveness of this formulation, we conduct large-scale experiments in the RLBench~\\citep{james2020rlbench} environment. We train a single multi-task agent on 18 diverse tasks with 249 variations that involve a range of prehensile and non-prehensile behaviors like placing wine bottles on a rack and dragging objects with a stick (see \\figref{fig:tasks} a-j). Each task also includes several pose and semantic variations with objects that differ in placement, color, shape, size, and category. \nOur results show that \\model~significantly outperforms image-to-action agents \\highlight{(by $34\\times$)} and 3D ConvNet baselines \\highlight{(by $2.8\\times$)}, without using any explicit representations of instance segmentations, object poses, memory, or symbolic states. \nWe also validate our approach on a Franka Panda with a multi-task agent trained \\textit{from scratch} on 7 real-world tasks  with a \\textbf{total of just 53 demonstrations} (see \\figref{fig:tasks} k-o).\n\nIn summary, our contributions are as follows:\n\\vspace{-0.2em}\n\\begin{itemize}[leftmargin=0.8cm,itemsep=0.05em]\n    \\item A \\textbf{novel problem formulation} for perceiving, acting, and specifying goals with Transformers.\n    \\item An efficient \\textbf{action-centric} framework for \\textbf{grounding language in 6-DoF actions}.\n    \\item \\textbf{Empirical results} investigating multi-task agents on a range of simulated and real-world tasks. \n\\end{itemize}\nThe code and pre-trained models will be made available  at \\url{peract.github.io}. \n\n\\section{Related Work}\n\\label{sec:related_work}\n\\textbf{Vision for Manipulation.} Traditionally, methods in robot perception have used explicit ``object'' representations like instance segmentations, object classes, poses  \\citep{he2017mask,Xiang-RSS-18,zhu2014single,zeng2017multi,deng2020self,xie2020best}. Such methods struggle with deformable and granular items like cloths and beans that are hard to represent with geometric models or segmentations. \nIn contrast, recent methods~\\citep{zeng2019robotic,zengTransporterNetworksRearranging2021,cliport,stengel2022guiding} learn action-centric representations without any ``objectness'' assumptions, but they are limited to top-down 2D settings with simple pick-and-place primitives. \\highlight{In 3D, James et al. proposed C2FARM~\\citep{c2farm}, an action-centric reinforcement learning (RL) agent with a coarse-to-fine-grain 3D-UNet backbone. The coarse-to-fine-grain scheme has a limited receptive field that cannot look at the entire scene at the finest level. In contrast, \\model~learns action-centric representations with a global-receptive field through a Transformer backbone. Also, \\model~does BC instead of RL, which enables us to easily train a multi-task agent for several tasks by conditioning it with language goals}. \n\n\\textbf{End-to-End Manipulation} approaches~\\citep{kalashnikov2018qt,Wu-RSS-20,levine2016end,finn2017deep} make the least assumptions about objects and tasks, but are often formulated as an image-to-action prediction task. Training directly on RGB images for 6-DoF tasks is often inefficient, generally requiring several  demonstrations or episodes just to learn basic skills like rearranging objects. In contrast, \\model~uses a voxelized observation and action space, which is dramatically more efficient and robust in 6-DoF settings.\nWhile other works in 6-DoF grasping~\\citep{song2020grasping,murali20206,mousavian20196,xu2022umpnet,agrawal2021scene,simeonov2021neural} have used RGB-D and pointcloud input, they have not been applied to sequential tasks or used with language-conditioning.\nAnother line of work tackles data inefficiency by using pre-trained image representations ~\\citep{cliport,nair2022r3m,yuan2021sornet} to bootstrap BC. Although our framework is trained from scratch, such pre-training approaches could be integrated together in future works for even greater efficiency and generalization to unseen objects. \n\n\\textbf{Transformers for Agents and Robots.} Transformers have become the prevalent architecture in several domains. Starting with NLP~\\citep{vaswani2017attention,brown2020language,liu2019roberta}, recently in vision~\\citep{dosovitskiy2020image,liu2021swin}, and even RL~\\citep{chen2021decision,janner2021offline,lee2022multi}. In robotics, Transformers have been applied to assistive teleop~\\citep{clever2021assistive},  legged locomotion~\\citep{yang2021learning},  path-planning~\\citep{chaplot2021differentiable,johnson2021motion}, imitation learning~\\citep{dasari2020transformers,kim2021transformer}, morphology controllers~\\citep{gupta2022metamorph}, spatial rearrangement~\\citep{liu2022structformer}, and grasping~\\citep{han2021learning}. Transformers have also achieved impressive results in multi-domain settings like in Gato~\\cite{reed2022generalist} where a single Transformer was trained on 16 domains such as captioning, language-grounding, robotic control etc. However, Gato relies on extremely large datasets like 15K episodes for block stacking and 94K episodes for Meta-World~\\citep{yu2020meta} tasks. Our approach might complement agents like Gato, which could use our 3D formulation for greater efficiency and robustness. \n\n\\textbf{Language Grounding for Manipulation.} Several works have proposed methods for grounding language in robot actions~\\citep{Shridhar-RSS-18,matuszek2014learning,bollini2013interpreting,misra2016tell,bisk2016natural,thomason2015learning,interact_picking18,chenJointNetworkGrasp2021, blukis2020few, paxton2019prospection,tellex2011understanding,nguyen2020robot}. However, these methods use disentangled pipelines for perception and action, with the language primarily being used to guide perception~\\citep{egl}. \nRecently, a number of end-to-end approaches \\citep{ahn2022can,jang2022bc,nair2022learning,mees2022matters,lynch2020grounding} have been proposed for conditioning BC agents with language instructions. These methods require thousands of human demos or autonomous episodes that are collected over several days or even months. In contrast, \\model~can learn robust multi-task policies with just a few minutes of training data. For benchmarking, several simulation environments exist~\\citep{mees2021calvin,zengTransporterNetworksRearranging2021,yu2020meta}, but we use RLBench~\\citep{james2020rlbench} for its diversity of 6-DoF tasks and ease of generating demonstrations with templated language goals. \n\n\\section{\\modelfull}\n\n\\model~is a language-conditioned behavior-cloning agent for 6-DoF manipulation. The key idea is to learn perceptual representations of actions conditioned on language goals. Given a voxelized reconstruction of a scene, we use a Perceiver Transformer~\\citep{jaegle2021perceiver} to learn per-voxel features. Despite the extremely large input space ($100^3$), Perceiver uses a small set of latent vectors to encode the input. The per-voxel features are then used to predict the next best action in terms of discretized translation, rotation, and gripper state at each timestep. \\model~relies purely on the current observation to determine what to do next in sequential tasks. \nSee \\figref{fig:peract} for an overview. \n\n\\secref{sec:demos} and \\secref{sec:keyframe_and_voxel} describe our dataset setup. \\secref{sec:peract} describes our problem formulation with \\model, and \\secref{sec:training} provides details on training \\model. Further implementation details are presented in \\appsecref{app:peract_details}. \n\n\\newpage\n\\begin{figure*}[!t]\n    \\centering\n    \\hspace*{-2.2cm}\n    \\includegraphics[width=1.3\\textwidth]{figures/peract_main_fig_v6.pdf}\n    \\vspace{-0.4cm}\n    \\caption{\\textbf{\\model~Overview.} \\model~is a language-conditioned behavior-cloning agent  trained with supervised learning to \\textit{detect actions}. \\model~takes as input a language goal and a voxel grid reconstructed from RGB-D sensors. The voxels are split into 3D patches, and the language goal is encoded with a pre-trained language model. These language and voxel features are appended together as a sequence and encoded with a Perceiver transformer~\\citep{jaegle2021perceiver}. \n    Despite the extremely long input sequence, Perceiver uses a small set of latent vectors to encode the input (see \\appfigref{fig:perceiver_arch} for an illustration). \n    These encodings are upsampled back to the original voxel dimensions with a decoder and reshaped with linear layers to predict a discretized translation, rotation, gripper open, and collision avoidance action. This action is executed with a motion-planner after which the new observation is used to predict the next discrete action in an observe-act loop until termination. \n    }\n    \\label{fig:peract}\n    \\vspace{-1.5em}\n\\end{figure*}\n \n\\vspace{-0.3cm} \n\\subsection{Demonstrations} \\label{sec:demos}\n\\vspace{-0.2cm}\nWe assume access to a dataset  $\\mathcal{D} = \\{\\zeta_1, \\zeta_2, \\ldots, \\zeta_n\\}$ of $n$ expert demonstrations, each paired with English language goals $\\mathcal{G} = \\{\\mathbf{l}_{1}, \\mathbf{l}_{2}, \\ldots, \\mathbf{l}_{n}\\}$. \nThese demonstrations are collected by an expert with the aid of a motion-planner to reach intermediate poses. Each demonstration $\\zeta$ is a sequence of continuous actions $\\mathcal{A} = \\{a_{1}, a_{2}, \\ldots, a_{t}\\}$ paired with observations $\\mathcal{O} = \\{\\Tilde{o}_{1}, \\Tilde{o}_{2}, \\ldots \\Tilde{o}_{t}\\}$. An action $a$ consists of the 6-DoF pose, gripper open state, and whether the motion-planner used collision avoidance to reach an intermediate pose: $a = \\{a_{\\textrm{pose}}, a_{\\textrm{open}}, a_{\\textrm{collide}}\\}$. \nAn observation $\\Tilde{o}$ consists of RGB-D images from any number of cameras. We use four cameras for simulated experiments $\\Tilde{o}_{\\textrm{sim}} = \\{o_{\\textrm{front}}, o_{\\textrm{left}}, o_{\\textrm{right}}, o_{\\textrm{wrist}} \\}$, but just a single camera  for real-world experiments $\\Tilde{o}_{\\textrm{real}} = \\{o_{\\textrm{front}} \\}$.  \n\\vspace{-0.2cm}\n\\subsection{Keyframes and Voxelization}\n\\label{sec:keyframe_and_voxel}\n\\vspace{-0.2cm}\n\nFollowing prior work by James et al.~\\citep{c2farm}, we construct a  structured observation and action space through keyframe extraction and voxelization.\n\nTraining our agent to directly predict continuous actions is inefficient and noisy. So instead, for each demonstration $\\zeta$, we extract a set of keyframe actions $\\{\\mathbf{k}_{1}, \\mathbf{k}_{2}, \\ldots, \\mathbf{k}_{m}\\} \\subset \\mathcal{A}$ \\highlight{that capture bottleneck end-effector poses}~\\citep{johns2021coarse} in the action sequence with a simple heuristic: an action is a keyframe if (1) the joint-velocities are near zero and (2) the gripper open state has not changed. \nEach datapoint in the demonstration $\\zeta$ can then be cast as a \\highlight{``predict the next (best) keyframe action'' task~\\citep{c2farm,armpaper,liu2022auto_lambda}}. See \\appfigref{app:keypoints_and_demo} for an illustration of this process. \n\nTo learn action-centric representations~\\cite{gibson2014ecological} in 3D, we use a voxel grid~\\citep{moravec1996robot,30724} to represent both the observation and action space. The observation voxels $\\mathbf{v}$ are reconstructed from RGB-D observations $\\Tilde{o}$ fused through triangulation $\\Tilde{o} \\Rightarrow \\mathbf{v}$ from known camera extrinsics and intrinsics. By default, we use a voxel grid of $100^3$, which corresponds to a volume of $1.0\\textrm{m}^3$ in metric scale. The keyframe actions $\\mathbf{k}$ are discretized such that training our BC agent can be formulated as a \\highlight{``next best action'' classification task~\\citep{c2farm}}.\nTranslation is simply the closest voxel to the center of the gripper fingers. Rotation is discretized into 5 degree bins for each of the three rotation axes. Gripper open state is a binary value. Collide is also a binary value that indicates if the motion-planner should avoid everything in the voxel grid or nothing at all; switching between these two modes of collision avoidance is crucial as tasks often involve both contact based (\\eg pulling the drawer open) and non-contact based motions (\\eg reaching the handle without colliding into anything). \n\\vspace{-0.2cm}\n\n\\subsection{\\model~Agent} \\label{sec:peract}\n\\vspace{-0.2cm}\n\\model~is a Transformer-based~\\citep{vaswani2017attention} agent that takes in a voxel observation and language goal \\inputvl, and outputs a discretized translation, rotation, and gripper open action. This action is executed with a motion-planner, after which this process is repeated until the goal is reached.\n\nThe language goal $\\mathbf{l}$ is encoded with a pre-trained language model. \nWe use CLIP's~\\citep{radfordLearningTransferableVisual2021} language encoder, but any pre-trained language model would suffice~\\citep{ahn2022can,lynch2020grounding}. Our choice of CLIP opens up possibilities for future work to use pre-trained vision features that are aligned with the language for better generalization to unseen semantic categories and instances~\\citep{cliport}. \n\nThe voxel observation $\\mathbf{v}$ is split into 3D patches of size $5^3$ (akin to vision-transformers like ViT~\\citep{dosovitskiy2020image}). In implementation, these  patches are extracted with a 3D convolution layer with a kernel-size and stride of 5, and then flattened into a sequence of voxel encodings. The language encodings are fine-tuned with a linear layer and then appended with the voxel encodings to form the input sequence. We also add learned positional embeddings to the sequence to incorporate voxel and token positions.\n\nThe input sequence of language and voxel encodings is extremely long. \nA standard Transformer with $\\mathcal{O}(n^2)$ self-attention connections and an input of  $(100/5)^3 = 8000$ patches is hard to fit on the memory of a commodity GPU.  \nInstead, we use the Perceiver~\\citep{jaegle2021perceiver} Transformer. Perceiver is a latent-space Transformer, where instead of attending to the entire input, it first computes cross-attention between the input and a much smaller set of latent vectors (which are randomly initialized and trained). These latents are encoded with self-attention layers, and for the final output, the latents are again cross-attended with the input to match the input-size. See \\appfigref{fig:perceiver_arch} for an illustration. By default, we use $2048$ latents of dimension 512 : $\\mathbb{R}^{2048 \\times 512}$, but in \\appsecref{app:ablations} we experiment with different latent sizes. \n\nThe Perceiver Transformer uses 6 self-attention layers to encode the latents and outputs a sequence of patch encodings from the output cross-attention layer. These patch encodings are upsampled with a 3D convolution layer and tri-linear upsampling to decode 64-dimensional voxel features. The decoder includes a skip-connection from the encoder (like in UNets~\\citep{ronneberger2015u}). The per-voxel features are then used to predict discretized actions~\\citep{c2farm}. For translation, the voxel features are reshaped into the original voxel grid ($100^3$) to form a 3D $\\mathcal{Q}$-function of action-values. For rotation, gripper open, and collide, the features are max-pooled and then decoded with linear layers to form their respective $\\mathcal{Q}$-function. The best action $\\mathcal{T}$ is chosen by simply maximizing the $\\mathcal{Q}$-functions:\n\\begin{align*}\n\\mathcal{T}_{\\textrm{trans}} = \\underset{(x,y,z)}{\\argmax} \\  \\mathcal{Q}_{\\textrm{trans}}((x,y,z) \\ | \\ \\mathbf{v}, \\mathbf{l} \\,), \\hspace{1cm}  &\n\\mathcal{T}_{\\textrm{rot}} = \\underset{(\\psi,\\theta,\\phi)}{\\argmax} \\  \\mathcal{Q}_{\\textrm{rot}}((\\psi,\\theta,\\phi) \\ | \\ \\mathbf{v}, \\mathbf{l} \\,), \\\\\n\\mathcal{T}_{\\textrm{open}} = \\underset{\\omega}{\\argmax} \\  \\mathcal{Q}_{\\textrm{open}}(\\, \\omega \\ | \\ \\mathbf{v}, \\mathbf{l} \\,), \\hspace{1cm}  &\n\\mathcal{T}_{\\textrm{collide}} = \\underset{\\kappa}{\\argmax} \\ \\mathcal{Q}_{\\textrm{collide}}(\\, \\kappa \\ | \\ \\mathbf{v}, \\mathbf{l} \\,),\n\\end{align*}\nwhere $(x, y, z)$ is the voxel location in the grid, $(\\psi, \\theta, \\phi)$ are discrete rotations in Euler angles, $\\omega$ is the gripper open state and $\\kappa$ is the collide variable. See \\figref{fig:q_pred} for examples of  $\\mathcal{Q}$-predictions.\n\n\\vspace{-0.1cm}\n\\subsection{Training Details}\n\\label{sec:training}\n\\vspace{-0.1cm}\n\n\\model~is trained through supervised learning with discrete-time input-action tuples from a dataset of demonstrations. These tuples are composed of voxel observations, language goals, and keyframe actions $\\{(\\mathbf{v}_{1}, \\mathbf{l}_{1}, \\mathbf{k}_{1}), (\\mathbf{v}_{2}, \\mathbf{l}_{2}, \\mathbf{k}_{2}), \\ldots\\}$. During training, we randomly sample a tuple and supervise the agent to predict the keyframe action $\\mathbf{k}$ given the observation and goal \\inputvl. For translation, the ground-truth action is represented as a one-hot voxel encoding $Y_{\\textrm{trans}} : \\mathbb{R}^{H \\times W \\times D}$. Rotations are also represented with a one-hot encoding per rotation axis with $R$ rotation bins $Y_{\\textrm{rot}} : \\mathbb{R}^{(360/R) \\times 3}$  ($R=5$ degrees for all experiments). Similarly, open and collide variables are binary one-hot vectors $Y_{\\textrm{open}} : \\mathbb{R}^{2}$, $Y_{\\textrm{collide}} : \\mathbb{R}^{2}$. The agent is trained with cross-entropy loss like a  classifier: \n\\begin{equation*}\n    \\mathcal{L}_{\\textrm{total}} = - \\mathbb{E}_{Y_{\\textrm{trans}}}[\\textrm{log} \\mathcal{V}_{\\textrm{trans}}] - \\mathbb{E}_{Y_{\\textrm{rot}}}[\\textrm{log} \\mathcal{V}_{\\textrm{rot}}] - \\mathbb{E}_{Y_{\\textrm{open}}}[\\textrm{log} \\mathcal{V}_{\\textrm{open}}] -\n    \\mathbb{E}_{Y_{\\textrm{collide}}}[\\textrm{log} \\mathcal{V}_{\\textrm{collide}}],\n\\end{equation*}\nwhere $\\mathcal{V}_{\\textrm{trans}} = \\textrm{softmax}(\\mathcal{Q}_{\\textrm{trans}}((x,y,z) | \\mathbf{v}, \\mathbf{l}))$, $\\mathcal{V}_{\\textrm{rot}} = \\textrm{softmax}(\\mathcal{Q}_{\\textrm{rot}}((\\psi, \\theta, \\phi) | \\mathbf{v}, \\mathbf{l}))$, $\\mathcal{V}_{\\textrm{open}} = \\textrm{softmax}(\\mathcal{Q}_{\\textrm{open}}(\\omega | \\mathbf{v}, \\mathbf{l}))$, $\\mathcal{V}_{\\textrm{collide}} = \\textrm{softmax}(\\mathcal{Q}_{\\textrm{collide}}(\\kappa | \\mathbf{v}, \\mathbf{l}))$ respectively. For robustness, we also augment $\\mathbf{v}$ and $\\mathbf{k}$ with translation and rotation perturbations. See \\appsecref{app:data_aug} for more details.\n\nBy default, we use a voxel grid size of $100^3$. We conducted validation tests by replaying expert demonstrations with discretized actions to ensure that $100^3$ is a sufficient resolution for execution. The agent was trained with a batch-size of 16 on 8 NVIDIA V100 GPUs for 16 days (600K iterations). We use the LAMB~\\citep{you2019large} optimizer following Perceiver~\\citep{jaegle2021perceiver}.\n\nFor multi-task training, we simply sample input-action tuples from all tasks in the dataset. To ensure that tasks with longer horizons are not over-represented during sampling, each batch contains a uniform distribution of tasks. That is, we first uniformly sample a set of tasks of batch-size length, then pick a random input-action tuple for each of the sampled tasks. With this strategy, longer-horizon tasks need more training steps for full coverage of input-action pairs, but all tasks are given equal weighting during gradient updates.\n\n\\section{Results}\n\\vspace{-0.2cm}\nWe perform experiments to answer the following questions: (1) How effective is \\model~compared to unstructured image-to-action frameworks and standard architectures like 3D ConvNets? And what are the factors that affect \\model's performance? (2) Is the global receptive field of Transformers actually beneficial over methods with local receptive fields? (3) Can \\model~be trained on real-world tasks with noisy data?\n\n\\vspace{-0.2cm}\n\\subsection{Simulation Setup}\n\\vspace{-0.2cm}\n\nWe conduct our primary experiments in simulation for the sake of reproducibility and benchmarking.\n\n\\textbf{Environment.} The simulation is set in CoppelaSim~\\citep{coppelasim} and interfaced through PyRep~\\citep{james2019pyrep}. All experiments use a Franka Panda robot with a parallel gripper. The input observations are captured from four RGB-D cameras positioned at the front, left shoulder, right shoulder, and on the wrist, as shown in \\appfigref{fig:sim_setup}. All cameras are noiseless and have a resolution of $128 \\times 128$. \n\n\\textbf{Language-Conditioned Tasks.}  \\highlight{We train and evaluate on 18 RLBench~\\citep{james2020rlbench} tasks. See \\href{https://peract.github.io}{peract.github.io} for examples and \\appsecref{app:task_details} for details on individual tasks. Each task includes several variations, ranging from 2-60 possibilities, \\eg in the \\texttt{stack blocks} task, \\textit{``stack 2 red blocks''} and \\textit{``stack 4 purple blocks''} are two variants}. These variants are randomly sampled during data generation, but kept consistent during evaluations for one-to-one comparisons. \nSome RLBench tasks were modified to include additional variations to stress-test multi-task and language-grounding capabilities. \nThere are a total of 249 variations across 18 tasks, and the number of extracted keyframes range from 2-17.\n\\highlight{All keyframes from an episode have the same language goal, which is constructed from templates} (but human-annotated for real-world tasks). Note that in all experiments, we do not test for generalization to unseen objects, \\ie~our train and test objects are the same. \n\\highlight{However during test time, the agent has to handle novel object poses, randomly sampled goals, and randomly sampled scenes with different semantic instantiations of object colors, shapes, sizes, and categories}.\nThe focus here is to evaluate the performance of a single multi-task agent trained on all tasks and variants.\n\n\\textbf{Evaluation Metric.} Each multi-task agent is evaluated independently on all 18 tasks. Evaluations are scored either 0 for failures or 100 for complete successes. There are no partial credits. We report average success rates on 25 evaluation episodes per task ($25 \\times18 = 450$ total episodes) for agents trained with $n=10,100$ demonstrations per task. During evaluation, an agent keeps taking actions until an oracle indicates task-completion or reaches a maximum of 25 steps.\n\n\\vspace{-0.2cm}\n\\subsection{Simulation Results}\n\\label{sec:sim_results}\n\\vspace{-0.1cm}\n\n\\tabref{table:rlbench} reports success rates of multi-task agents trained on all 18 tasks. \nWe could not investigate single-task agents due to resource constraints of training 18 individual agents. \n\n\\textbf{Baseline Methods.} We study the effectiveness of our problem formulation by \\highlight{benchmarking against two language-conditioned baselines}: \\bcz and C2FARM-BC.\n\\highlight{\\bcz is an image-to-action agent similar to BC-Z~\\citep{jang2022bc}}. Following BC-Z, \\highlight{we use FiLM~\\citep{perez2018film} for conditioning with   CLIP~\\citep{radfordLearningTransferableVisual2021} language features}, but the vision encoders take in RGB-D images instead of just RGB. We also  study both CNN and ViT vision encoders. \\unet is a 3D fully-convolutional network by James et al.~\\citep{c2farm} that has achieved state-of-the-art results on RLBench tasks. Similar to our agent, \\unet also detects actions in a voxelized space, however it uses a coarse-to-fine-grain scheme to detect actions at two-levels of voxelization: $32^3$ voxels with a $1^3$m grid, and $32^3$ voxels with a $0.15^3$m grid after ``zooming in'' from the first level. Note that at the finest level, \\unet has a higher resolution ($0.47$cm) than \\model~($1$cm). We use the same 3D ConvNet architecture as James et al.~\\citep{c2farm}, but instead of training it with RL, we do BC with cross-entropy loss (from \\secref{sec:training}). \\highlight{We also condition it with CLIP~\\citep{radfordLearningTransferableVisual2021} language features at the bottleneck like in LingUNets~\\citep{misra2018mapping,cliport}}. \n\n\\begin{table}[!t]\n\\centering\n\\scriptsize\n\n\\vspace{-1.0cm}\n\\hspace*{-1.02cm}\n\\begin{tabular}{lcccccccccccccccccc} \n\\toprule\n                  & \\multicolumn{2}{c}{\\begin{tabular}[c]{@{}c@{}}\\texttt{open} \\\\\\texttt{drawer}\\end{tabular}}     & \\multicolumn{2}{c}{\\begin{tabular}[c]{@{}c@{}}\\texttt{slide} \\\\\\texttt{block}\\end{tabular}} &  \\multicolumn{2}{c}{\\begin{tabular}[c]{@{}c@{}}\\texttt{sweep to} \\\\\\texttt{dustpan}\\end{tabular}}   &\n                  \\multicolumn{2}{c}{\\begin{tabular}[c]{@{}c@{}}\\texttt{meat off}\\\\\\texttt{grill}\\end{tabular}} &\n                  \\multicolumn{2}{c}{\\begin{tabular}[c]{@{}c@{}}\\texttt{turn} \\\\\\texttt{tap}\\end{tabular}}   & \\multicolumn{2}{c}{\\begin{tabular}[c]{@{}c@{}}\\texttt{put in} \\\\\\texttt{drawer}\\end{tabular}}         & \\multicolumn{2}{c}{\\begin{tabular}[c]{@{}c@{}}\\texttt{close}\\\\\\texttt{jar}\\end{tabular}}    &\n                  \\multicolumn{2}{c}{\\begin{tabular}[c]{@{}c@{}}\\texttt{drag} \\\\\\texttt{stick}\\end{tabular}}    &\n                  \\multicolumn{2}{c}{\\begin{tabular}[c]{@{}c@{}}\\texttt{stack}\\\\\\texttt{blocks}\\end{tabular}}  \\\\\n                  \\cmidrule(lr){2-3} \\cmidrule(lr){4-5} \\cmidrule(lr){6-7} \\cmidrule(lr){8-9} \\cmidrule(lr){10-11} \\cmidrule(lr){12-13} \n                  \\cmidrule(lr){14-15} \\cmidrule(lr){16-17}\n                  \\cmidrule(lr){18-19}\n                  \\\\[-13pt]                                                         \\\\\n\\vcell{Method}    & \\vcell{10}       & \\vcell{100}                                                & \\vcell{10}       & \\vcell{100}                                                      & \\vcell{10}       & \\vcell{100}                                                  & \\vcell{10}       & \\vcell{100}                                                 & \\vcell{10}       & \\vcell{100}                                           & \\vcell{10}       & \\vcell{100}                                                         & \\vcell{10}       & \\vcell{100}                                            & \\vcell{10}       & \\vcell{100}                                                  & \\vcell{10}       & \\vcell{100}                                             \\\\[-\\rowheight]\n\\printcellbottom  & \\printcellbottom & \\printcellbottom                                           & \\printcellbottom & \\printcellbottom                                                 & \\printcellbottom & \\printcellbottom                                             & \\printcellbottom & \\printcellbottom                                            & \\printcellbottom & \\printcellbottom                                      & \\printcellbottom & \\printcellbottom                                                    & \\printcellbottom & \\printcellbottom                                       & \\printcellbottom & \\printcellbottom                                             & \\printcellbottom & \\printcellbottom                                        \\\\[0pt] \n\\hline \\\\[-6pt]\n\\bczcnn         & 4                    & 4                                                      & 4                    & 0                                                            & 0                    & 0                                                        & 0                    & 0                                                          & 20                   & 8                                                & 0                    & 8                                                        & 0                    & 0                                                       & 0                    & 0                                                     & 0                    & 0                                                   \\\\\n\\highlight{\\bczvit}   & 16                   & 0                                                  & 8                    & 0                                                  & 8                    & 0                                                       & 0                    & 0                                                     & 24                   & 16                                               & 0                    & 0                                                    & 0                    & 0                                                & 0                    & 0                                                 & 0                    & 0                                                   \\\\\n\\unet            & 28                   & 20                                                     & 12                   & 16                                                           & 4                    & 0                                                        & 40                   & 20                                                         & 60                   & 68                                               & 12                   & 4                                                        & 28          & 24                                                      & \\textbf{72}          & 24                                                    & 4                    & 0                                                   \\\\\n\\model~(w/o Lang) & 20                   & 28                                                     & 8                    & 12                                                           & 20                   & 16                                                       & 40                   & 48                                                         & 36                   & 60                                               & 16                   & 16                                                       & 16                   & 12                                                      & 48                   & 60                                                    & 0                    & 0                                                   \\\\\n\\rowcolor[rgb]{0.9,1.0,0.9}\\model           & \\textbf{68}          & \\textbf{80}                                            & \\textbf{32}          & \\textbf{72}                                                  & \\textbf{72}          & \\textbf{56}                                              & \\textbf{68}          & \\textbf{84}                                                & \\textbf{72}          & \\textbf{80}                                      & \\textbf{16}          & \\textbf{68}                                              & \\textbf{32}          & \\textbf{60}                                             & 36                   & \\textbf{68}                                           & \\textbf{12}          & \\textbf{36}                                         \\\\[1pt]  \n\\hline \\\\\n                  & \\multicolumn{2}{c}{\\begin{tabular}[c]{@{}c@{}}\\texttt{screw}\\\\\\texttt{bulb}\\end{tabular}} & \\multicolumn{2}{c}{\\begin{tabular}[c]{@{}c@{}}\\texttt{put in}\\\\\\texttt{safe}\\end{tabular}}      & \\multicolumn{2}{c}{\\begin{tabular}[c]{@{}c@{}}\\texttt{place}\\\\\\texttt{wine}\\end{tabular}} & \n                  \\multicolumn{2}{c}{\\begin{tabular}[c]{@{}c@{}}\\texttt{put in}\\\\\\texttt{cupboard}\\end{tabular}} &\n                  \\multicolumn{2}{c}{\\begin{tabular}[c]{@{}c@{}}\\texttt{sort}\\\\\\texttt{shape}\\end{tabular}} & \n                  \\multicolumn{2}{c}{\\begin{tabular}[c]{@{}c@{}}\\texttt{push}\\\\\\texttt{buttons}\\end{tabular}}& \\multicolumn{2}{c}{\\begin{tabular}[c]{@{}c@{}}\\texttt{insert}\\\\\\texttt{peg}\\end{tabular}} &\n                  \\multicolumn{2}{c}{\\begin{tabular}[c]{@{}c@{}}\\texttt{stack}\\\\\\texttt{cups}\\end{tabular}}         & \\multicolumn{2}{c}{\\begin{tabular}[c]{@{}c@{}}\\texttt{place}\\\\\\texttt{cups}\\end{tabular}}    \\\\\n        \n                  \\cmidrule(lr){2-3} \\cmidrule(lr){4-5} \\cmidrule(lr){6-7} \\cmidrule(lr){8-9} \\cmidrule(lr){10-11} \\cmidrule(lr){12-13} \n                  \\cmidrule(lr){14-15} \\cmidrule(lr){16-17}\n                  \\cmidrule(lr){18-19}\n                  \\\\[-6pt]          \n\\vcell{}          & \\vcell{10}       & \\vcell{100}                                                & \\vcell{10}       & \\vcell{100}                                                      & \\vcell{10}       & \\vcell{100}                                                  & \\vcell{10}       & \\vcell{100}                                                 & \\vcell{10}       & \\vcell{100}                                           & \\vcell{10}       & \\vcell{100}                                                         & \\vcell{10}       & \\vcell{100}                                            & \\vcell{10}       & \\vcell{100}                                                  & \\vcell{10}       & \\vcell{100}                                             \\\\[-\\rowheight]\n\\printcellbottom  & \\printcellbottom & \\printcellbottom                                           & \\printcellbottom & \\printcellbottom                                                 & \\printcellbottom & \\printcellbottom                                             & \\printcellbottom & \\printcellbottom                                            & \\printcellbottom & \\printcellbottom                                      & \\printcellbottom & \\printcellbottom                                                    & \\printcellbottom & \\printcellbottom                                       & \\printcellbottom & \\printcellbottom                                             & \\printcellbottom & \\printcellbottom                                        \\\\[1pt]\n\\hline \\\\[-6pt]\n\\bczcnn          & 0                    & 0                                                      & 0                    & 4                                                            & 0                    & 0                                                        & 0                    & 0                                                          & 0                    & 0                                                & 4                    & 0                                                        & 0                    & 0                                                       & 0                    & 0                                                     & 0                    & 0                                                   \\\\\n\\highlight{\\bczvit}   & 0                    & 0                                                  & 0                    & 0                                                  & 4                    & 0                                                       & 4                    & 0                                                     & 0                    & 0                                                & 16                   & 0                                                    & 0                    & 0                                                & 0                    & 0                                                 & 0                    & 0                                                   \\\\\n\\unet             & 12                   & 8                                                      & 0                    & 12                                                           & \\textbf{36}          & 8                                                        & \\textbf{4}           & 0                                                          & 8           & 8                                                & \\textbf{88}          & \\textbf{72}                                              & 0                    & \\textbf{4}                                                       & 0                    & 0                                                     & 0                    & 0                                                   \\\\\n\\model~(w/o Lang) & 0                    & \\textbf{24}                                            & 8                    & 20                                                           & 8                    & \\textbf{20}                                                       & 0                    & 0                                                          & 0                    & 0                                                & 60                   & 68                                                       & 4                    & 0                                                       & 0                    & 0                                                     & 0                    & 0                                                   \\\\\n\\rowcolor[rgb]{0.9,1.0,0.9}\\model~ & \\textbf{28}          & \\textbf{24}                                            & \\textbf{16}          & \\textbf{44}                                                  & 20                   & 12                                                       & 0                    & \\textbf{16}                                                & \\textbf{16}          & \\textbf{20}                                      & 56                   & 48                                                       & \\textbf{4}           & 0                                                       & 0                    & 0                                                     & 0                    & 0                                                   \\\\[-1pt]\n\\bottomrule\n\\end{tabular}\n\\vspace{2pt}\n\\caption{\\textbf{Multi-Task Test Results.} Success rates (mean \\%) of various multi-task agents tasks trained with either 10 or 100 demonstrations per task and evaluated on 25 episodes per task. Each evaluation episode is scored either a 0 for failure or 100 for succces. \\model~outperforms \\unet\\citep{c2farm}, \\highlight{the most competitive baseline, with an average improvement of $1.33\\times$ with 10 demos and $2.83\\times$ with 100 demos.}}\n\\vspace{-0.8cm}\n\\label{table:rlbench}\n\\end{table}\n\\textbf{Multi-Task Performance.} \\tabref{table:rlbench} compares the performance of \\bcz and \\unet against \\model. \\highlight{With insufficient demonstrations, \\bcz has near zero performance on most tasks. \\bcz is disadvantaged with single-view observations and has to learn hand-eye coordination from scratch. In contrast, \\model's voxel-based formulation naturally allows for integrating multi-view observations, learning 6-DoF action representations, and data-augmentation in 3D, all of which are non-trivial to achieve in image-based methods}. \\unet is the most competitive baseline, but it has a limited receptive field mostly because of the coarse-to-fine-grain scheme and partly due to the convolution-only architecture. \\model~outperforms \\unet in $25/36$~evaluations in \\tabref{table:rlbench} \\highlight{with \\textbf{an average improvement of} $\\mathbf{1.33\\times}$ \\textbf{with 10 demonstrations and} $\\mathbf{2.83\\times}$\\textbf{ with 100 demonstrations}}. For a number of tasks, \\unet actually performs worse with more demonstrations, likely due to insufficient capacity. Since additional training demonstrations include additional task variants to optimize for, they might end up hurting performance. \n\n\\vspace{-0.05cm}\nIn general, 10 demonstrations are sufficient for \\model~to achieve $>65\\%$ success on tasks with limited variations like \\texttt{open drawer} (3 variations). But tasks with more variations like \\texttt{stack blocks} (60 variations) need substantially more data, sometimes to simply cover all possible concepts like ``\\textit{teal color block}'' that might have not appeared in the training data. See the simulation rollouts in the supplementary video to get a sense of the complexity of these evaluations. For three tasks: \\texttt{insert peg}, \\texttt{stack cups}, and \\texttt{place cups}, all agents achieve near zero success. These are  very high-precision tasks where being off by a few centimeters or degrees could lead to unrecoverable failures. \\highlight{But in \\appsecref{app:high_pres} we find that training single-task agents, specifically for these tasks, slightly alleviates this issue.}\n\n\\begin{wrapfigure}{r}{0.4\\textwidth}\n  \n  \\begin{center}\n    \\vspace{-0.8cm}\n    \\includegraphics[width=0.42\\textwidth]{figures/ablation_results.pdf}\n\n  \\vspace{-0.05cm}\n  \\caption{\\textbf{Ablation Experiments.} Success rate of \\model~after ablating key components.}\n  \\label{fig:ablations}\n  \\end{center}\n  \\vspace{-1em}\n\\end{wrapfigure}\n\\vspace{-0.05cm}\n\\textbf{Ablations.} \\tabref{table:rlbench} reports \\model~w/o Lang, an agent without any language conditioning. Without a language goal, the agent does not know the underlying task and performs at chance. We also report additional ablation results on the \\texttt{open drawer} task in \\figref{fig:ablations}. To summarize these results: (1) the skip connection helps train the agent slightly faster, (2) the Perceiver Transformer is crucial for achieving good performance with the global receptive field, and (3) extracting good keyframes actions is essential for supervised training as randomly chosen or fixed-interval keyframes lead to zero-performance.\n\n\\begin{wrapfigure}{r}{0.4\\textwidth}\n    \\vspace{-1.63cm}\n  \\begin{center}\n    \\includegraphics[width=0.55\\textwidth]{figures/recep_results.pdf}\n  \\end{center}\n  \\vspace{-1.2em}\n  \\caption{\\textbf{Global vs. Local Receptive Field Experiments.} Success rates of \\model~against various \\unet~\\citep{c2farm} baselines}\n  \\label{fig:recep_results}\n  \\vspace{-0.5cm}\n\\end{wrapfigure}\n\\textbf{Sensitivity Analysis.} In \\appsecref{app:ablations} we investigate factors that affect \\model's performance: the number of Perceiver latents, voxelization resolution, and data augmentation. We find that more latent vectors generally improve the capacity of the agent to model more tasks, but for simple short-horizon tasks, fewer latents are sufficient. Similarly, with different voxelization resolutions, some tasks are solvable with coarse voxel grids like $32^3$, but some high-precision tasks require the full $100^3$ grid. Finally, rotation perturbations in the data augmentation generally help in improving robustness essentially by exposing the agent to more rotation variations of objects.\n\n\\begin{figure*}[!t]\n    \\centering\n    \\vspace{-1.2cm}\n    \\hspace*{-1.5cm}\n    \\includegraphics[width=1.2\\textwidth]{figures/q_pred_v3.pdf}\n    \\caption{\\textbf{Q-Prediction Examples}: Qualitative examples of translation $\\mathcal{Q}$-Predictions from \\model~along with expert actions, highlighted with dotted-circles. The left two are simulated tasks, and the right two are real-world tasks. See \\appsecref{app:more_qpred} for more examples.}\n    \\label{fig:q_pred} \n    \\vspace{-1em}\n\\end{figure*}\n\n\\subsection{Global vs. Local Receptive Fields}\nTo further investigate our Transformer agent's global receptive field, we conduct additional experiments on the \\texttt{open drawer} task. \nThe \\texttt{open drawer} task has three variants: \\textit{``open the top drawer''}, \\textit{``open the middle drawer''}, and \\textit{``open the bottom drawer''}, and with a limited receptive field it is hard to distinguish the drawer handles, which are all visually identical. \n\\figref{fig:recep_results} reports \\model~and \\unet agents trained with 100 demonstrations. Although the \\texttt{open drawer} tasks can be solved with fewer demonstrations, here we want to ensure that insufficient data is not an issue. We include several versions of \\unet with different voxelization schemes. For instance, $[16,16]$ indicates two levels of $16^3$ voxel grids at $1\\textrm{m}^3$ and $0.15\\textrm{m}^3$, respectively. And $[64]$ indicates a single level of a $64^3$ voxel grid without the coarse-to-fine-grain scheme. \\model~is the only agent that achieves $>70\\%$ success, whereas all \\unet versions perform at chance with $\\sim 33\\%$, indicating that the global receptive field of the Transformer is crucial for solving the task. \n\n\\begin{wraptable}{r}{0.3\\textwidth}\n  \\vspace{-1.3em}\n  \\setlength\\tabcolsep{2.3pt}\n  \\centering\n  \\scriptsize\n\\begin{tabular}{lccc} \n\\toprule\nTask          & \\# Train~ & \\# Test & Succ. \\%  \\\\ \n\\midrule\nPress Handsan & 5         & 10      & 90        \\\\\nPut Marker    & 8         & 10      & 70        \\\\\nPlace Food    & 8         & 10      & 60        \\\\\nPut in Drawer & 8         & 10      & 40        \\\\\nHit Ball      & 8         & 10      & 60        \\\\\nStack Blocks  & 10        & 10      & 40        \\\\\nSweep Beans   & 8         & 5       & 20        \\\\\n\\bottomrule\n\\end{tabular}\n    \\caption{\\scriptsize{Success rates (mean \\%) of a multi-task model trained an evaluated 7 real-world tasks (see \\figref{fig:tasks}).}} %\n  \\vspace{-2em}\n  \\label{table:real}\n\\end{wraptable}\\subsection{Real-Robot Results} \\label{sec:real_robot_results}\nWe also validated our results with real-robot experiments on a Franka Emika Panda. See  \\appsecref{app:robot_setup} for setup details. Without any sim-to-real transfer or pre-training, we trained a multi-task \\model~agent \\textit{from scratch }on 7 tasks (with 18 unique variations) from a total of just 53 demonstrations. See the supplementary video for qualitative results that showcase the diversity of tasks and robustness to scene changes. \\tabref{table:real} reports success rates from small-scale evaluations. Similar to the simulation results, we find that \\model~is able to achieve $>65\\%$ success on simple short-horizon tasks like pressing hand-sanitizers from just a handful number of demonstrations. The most common failures involved predicting incorrect gripper open actions, which often lead the agent into unseen states. This could be addressed in future works by using HG-DAgger style approaches to correct the agent~\\citep{jang2022bc}. Other issues included the agent exploiting biases in the dataset like in prior work~\\citep{cliport}. This could be addressed by scaling up expert data with more diverse tasks and task variants.  \n\n\\vspace{-0.1cm}\n\\section{Limitations and Conclusion}\n\\vspace{-0.1cm}\nWe presented \\model, a Transformer-based multi-task agent for 6-DoF manipulation. Our experiments with both simulated and real-world tasks indicate that the right problem formulation, \\ie~detecting voxel actions, makes a substantial difference in terms of data efficiency and robustness. \n\nWhile \\model~is quite capable, extending it to dexterous continuous control remains a challenge. \\model~is at the mercy of a sampling-based motion-planner to execute discretized actions, and is not easily extendable to N-DoF actuators like multi-fingered hands. See \\appsecref{app:limitations} for an extended discussion on \\model's limitations.\nBut overall, we are excited about scaling up robot learning with Transformers by focusing on \\textit{diverse} rather than narrow multi-task data for robotic manipulation. \n\n\\acknowledgments{We thank Selest Nashef and Karthik Desingh for their help with the Franka setup at UW. We thank Stephen James for helping with RLBench and ARM issues. We are also grateful to Valts Blukis, Zoey Chen, Markus Grotz, Aaron Walsman, and Kevin Zakka, for providing feedback on the initial draft. And thanks to Shikhar Bahl for initial discussions. This work was funded in part by ONR under award \\#1140209-405780. Mohit Shridhar is supported by the NVIDIA Graduate Fellowship, and was also a part-time intern at NVIDIA throughout the duration of this project.}\n\n\\newpage\n\n\\end{document}\n</paper 1>\n\n<paper 2>\n\\title{\\methodname: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control}\n\n\\begin{document}\n\n\\maketitle\n\n\\section{Introduction}\n\\label{sec:intro}\n\nHigh-capacity models pretrained on broad web-scale datasets provide an effective and powerful platform for a wide range of downstream tasks: large language models can enable not only fluent text generation~\\citep{brohan2022rt,openai2023gpt4,anil2023palm} but emergent problem-solving~\\citep{cobbe2021training,lewkowycz2022solving,polu2022formal} and creative generation of prose~\\citep{brown2020language,openai2023gpt4} and code~\\citep{chen2021evaluating}, while vision-language models enable open-vocabulary visual recognition~\\citep{radford2021learning,minderer2022simple,kirillov2023segment} and can even make complex inferences about object-agent interactions in images~\\citep{alayrac2022flamingo,hao2022language,wang2022git,chen2023pali,chen2023palix,driess2023palm,huang2023language}. Such semantic reasoning, problem solving, and visual interpretation capabilities would be tremendously useful for generalist robots that must perform a variety of tasks in real-world environments. However, it is unclear how robots should acquire such capabilities. While a brute force approach might entail collecting millions of robotic interaction trials, the most capable language and vision-language models are trained on billions of tokens and images from the web~\\citep{alayrac2022flamingo,chen2023pali,chen2023palix,huang2023language} -- an amount unlikely to be matched with robot data in the near future. On the other hand, directly applying such models to robotic tasks is also difficult: such models reason about semantics, labels, and textual prompts, whereas robots require grounded low-level actions, such as Cartesian end-effector commands. While a number of recent works have sought to incorporate language models (LLMs) and vision-language models (VLMs) into robotics~\\citep{ahn2022can,driess2023palm,vemprala2023chatgpt}, such methods generally address only the ``higher level'' aspects of robotic planning, essentially taking the role of a state machine that interprets commands and parses them into individual primitives (such as picking and placing objects),\nwhich are then executed by separate low-level controllers that themselves do not benefit from the rich semantic knowledge of Internet-scale models during training. Therefore, in this paper we ask: can large pretrained vision-language models be integrated directly into low-level robotic control to boost generalization and enable emergent semantic reasoning?\n\n\\begin{figure}[t!]\n    \\centering\n    \\includegraphics[width=0.99\\textwidth]{figures/rt2_teaser.pdf}\n    \\caption{\n\\methodname overview: we represent robot actions as another language, which can be cast into text tokens and trained together with Internet-scale vision-language datasets. \nDuring inference, the text tokens are de-tokenized into robot actions, enabling closed loop control.\nThis allows us to leverage the backbone and pretraining of vision-language models in learning robotic policies, transferring some of their generalization, semantic understanding, and reasoning to robotic control. \nWe demonstrate examples of \\methodname execution on the project website: \\texttt{\\href{https://robotics-transformer2.github.io/}{robotics-transformer2.github.io}}.}\n\\vspace{-0.2cm}\n    \\label{fig:teaser}\n\\end{figure}\n\\vspace{-0.1cm}\n\nTo this end, we explore an approach that is both simple and surprisingly effective: we directly train vision-language models designed for open-vocabulary visual question answering and visual dialogue to output low-level robot actions, along with solving other Internet-scale vision-language tasks. Although such models are typically trained to produce natural language tokens, we can train them on robotic trajectories by {\\em{tokenizing the actions into text tokens}} and creating ``multimodal sentences''~\\citep{driess2023palm} that ``respond'' to robotic instructions paired with camera observations by producing corresponding actions. \nIn this way, vision-language models can be directly trained to act as instruction following robotic policies. This simple approach is in contrast with prior alternatives for incorporating VLMs into robot policies~\\citep{shridhar2022cliport} or designing new vision-language-action architectures from scratch~\\citep{reed2022generalist}: instead, pre-existing vision-language models, with already-amortized significant compute investment, are trained without any new parameters to output text-encoded actions. We refer to this category of models as \\categoryfullname (\\categoryname) models.\nWe instantiate \\categoryname models by building on the protocol proposed for RT-1~\\citep{brohan2022rt}, using a similar dataset, but expanding the model to use a large vision-language backbone. Hence we refer to our model as \\methodname (\\methodfullname). We provide an overview in Figure~\\ref{fig:teaser}.\n\nWe observe that robotic policies derived from such vision-language models exhibit a range of remarkable capabilities, combining the physical motions learned from the robot data with the ability to interpret images and text learned from web data into a single model. \nBesides the expected benefit of dramatically improving generalization to novel objects and semantically varied instructions, we observe a number of emergent capabilities. While the model's physical skills are still limited to the distribution of skills seen in the robot data, the model acquires the ability to deploy those skills in new ways by interpreting images and language commands using knowledge gleaned from the web. \nSome example highlights are shown in Figure~\\ref{fig:qualitative_emergent}. The model is able to re-purpose pick and place skills learned from robot data to place objects near semantically indicated locations, such as specific numbers or icons, despite those cues not being present in the robot data. The model can also interpret relations between objects to determine which object to pick and where to place it, despite no such relations being provided in the robot demonstrations. Furthermore, if we augment the command with chain of thought prompting, the model is able to make even more complex semantic inferences, such as figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink).\n\nOur main contribution is \\methodname, a family of models derived from fine-tuning  large vision-language models trained on web-scale data to directly act as generalizable and semantically aware robotic policies. Our experiments investigate models with up to 55B parameters trained on Internet data and  instruction-annotated robotic trajectories from previous work~\\citep{brohan2022rt}. Over the course of 6k robotic evaluations, we show that \\methodname enable significant improvements to generalization over objects, scenes, and instructions, and exhibit a breadth of emergent capabilities inherited from web-scale vision-language pretraining.\n\n\\section{Related Work}\n\\label{sec:rw}\n\n\\textbf{Vision-language models.}\nThere are several categories of \\emph{Vision-Language Models} (VLMs)~\\citep{gan2022vision}, with perhaps two most relevant: (1) representation-learning models, e.g. CLIP~\\citep{radford2021learning}, which learn common embeddings for both modalities, and (2) visual language models of the form $\\{\\text{vision},\\text{text}\\} \\rightarrow \\{\\text{text}\\}$ which learn to take vision and language as input and provide free-form text. \nBoth categories have been used to provide pretraining for a wide variety of applied to downstream applications such as object classification~\\citep{radford2021learning}, detection~\\citep{gu2021open}, and segmentation~\\citep{ghiasi2021open}.\nIn this work, we focus on the latter category~\\citep{alayrac2022flamingo, chen2023pali, chen2023palix, driess2023palm, li2019visualbert, lu2019vilbert, hao2022language, li2023blip}. \nThese models are generally trained on many different tasks, such as image captioning, vision-question answering (VQA), and general language tasks on multiple datasets at the same time.\nWhile prior works study VLMs for a wide range of problems and settings including in robotics, our focus is on how the capabilities of VLMs \ncan be extended to robotics closed-loop control by endowing them with the ability to predict robot actions, thus leveraging the knowledge already present in VLMs to enable new levels of generalization.\n\n\\textbf{Generalization in robot learning.} Developing robotic controllers that can broadly succeed in a variety of scenarios is a long-standing goal in robotics research~\\citep{smith1973design,kaelbling2020foundation}. A promising approach for enabling generalization in robotic manipulation is by learning from large and diverse datasets~\\citep{pinto2016supersizing,levine2018learning,dasari2019robonet}. By doing so, prior methods have demonstrated how robots can generalize to novel object instances~\\citep{pinto2016supersizing,mahler2017dex,levine2018learning,finn2017deep,young2021visual}, to tasks involving novel combinations of objects and skills~\\citep{finn2017one,yu2018one,james2018task,dasari2021transformers,jang2022bc}, to new goals or language instructions~\\citep{pong2019skew,nair2022learning,jang2022bc,jiang2022vima,mees2022matters,liu2022instruction}, to tasks with novel semantic object categories~\\citep{shridhar2021cliport,stone2023open}, and to unseen environments~\\citep{hansen2020self,cui2022play,du2023behavior}. Unlike most of these prior works, we aim to develop and study a single model that can generalize to unseen conditions along all of these axes. A key ingredient of our approach is to leverage pre-trained models that have been exposed to data that is much broader than the data seen by the robot.\n\n\\textbf{Pre-training for robotic manipulation.} Pre-training has a long history in robotic learning. Most works focus on pre-trained visual representations that can be used to initialize the encoder of the robot's camera observations, either via supervised ImageNet classification~\\citep{shah2021rrl}, data augmentation~\\citep{laskin2020reinforcement,laskin2020curl,kostrikov2020image,pari2021surprising} or objectives that are tailored towards robotic control~\\citep{nair2022r3m,ma2022vip,xiao2022masked,karamcheti2023language,majumdar2023we}. Other works have incorporated pre-trained language models, often either as an instruction encoder~\\citep{hill2020human,lynch2020language,nair2022learning,jang2022bc,jiang2022vima,brohan2022rt,shridhar2022perceiver} or for high-level planning~\\citep{huang2022language,ahn2022can,driess2023palm,singh2022progprompt,wu2023tidybot,mu2023embodiedgpt}. Rather than using pre-training vision models or pre-trained language models, we specifically consider the use of pre-trained vision-language models (VLMs), which provide rich, grounded knowledge about the world. Prior works have studied the use of VLMs for robotics~\\citep{shridhar2021cliport,karamcheti2023language,stone2023open,driess2023palm,gadre2022clip,pmlr-v205-shah23b,du2023vision}, and form part of the inspiration for this work. These prior approaches use VLMs for visual state representations~\\citep{karamcheti2023language}, for identifying objects~\\citep{stone2023open,gadre2022clip}, for high-level planning~\\citep{driess2023palm}, or for providing supervision or success detection~\\citep{xiao2022robotic,du2023vision,sumers2023distilling,zhang2023grounding,ma2023liv}. While CLIPort~\\citep{shridhar2021cliport} and MOO~\\citep{stone2023open} integrate pre-trained VLMs into end-to-end visuomotor manipulation policies, both incorporate significant structure into the policy that limits their applicability. Notably, our work does not rely on a restricted 2D action space and does not require a calibrated camera. %\nMoreover, a critical distinction is that, unlike these works, we leverage VLMs that generate language, and the unified output space of our formulation enables model weights to be entirely shared across language and action tasks, without introducing action-only model layer components.   \n\n\\section{Vision-Language-Action Models}\n\n\\label{sec:method}\n\nIn this section, we present our model family and the design choices for enabling training VLMs to directly perform closed-loop robot control.\nFirst, we describe the general architecture of our models and how they can be\nderived from models that are commonly used for vision-language tasks. Then, we introduce the recipe and challenges of fine-tuning large VLMs that are pre-trained on web-scale data to directly output robot actions, becoming VLA models. Finally, we describe how to make these models practical for robot tasks, addressing challenges with model size and inference speed to enable real-time control.\n\n\\subsection{Pre-Trained Vision-Language Models}\n\nThe vision-language models~\\citep{chen2023palix,driess2023palm} that we build on in this work take as input one or more images and produce a sequence of tokens, which conventionally represents natural language text. Such models can perform a wide range of visual interpretation and reasoning tasks, from inferring the composition of an image to answering questions about individual objects and their relations to other objects~\\citep{alayrac2022flamingo,chen2023palix,driess2023palm,huang2023language}. Representing the knowledge necessary to perform such a wide range of tasks requires large models and web-scale datasets. In this work, we adapt two previously proposed VLMs to act as VLA models: PaLI-X~\\citep{chen2023palix} and PaLM-E~\\citep{driess2023palm}. We will refer to vision-language-action versions of these models as \\methodname-PaLI-X and \\methodname-PaLM-E. \nWe leverage instantiations of these models that range in size from billions to tens of billions of parameters. We provide a detailed description of the architecture of these two models in Appendix~\\ref{sec:app_vlm}.\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=0.99\\textwidth]{figures/RT2-capabilities-dm.png}\n    \\caption{\\methodname is able to generalize to a variety of real-world situations that require reasoning, symbol understanding, and human recognition. We study these challenging scenarios in detail in Section \\ref{sec:exp}.\n    }\n    \\label{fig:qualitative_emergent}\n    \\vspace{-0.2cm}\n\\end{figure}\n\n\\subsection{Robot-Action Fine-tuning}\n\\label{sec:action_ft}\n\nTo enable vision-language models to control a robot, they must be trained to output actions. We take a direct approach to this problem, representing actions as tokens in the model's output, which are treated in the same way as language tokens. We base our action encoding on the discretization proposed by \\citet{brohan2022rt} for the RT-1 model. The action space consists of 6-DoF positional and rotational displacement of the robot end-effector, as well as the level of extension of the robot gripper and a special discrete command for terminating the episode, which should be triggered by the policy to signal successful completion. The continuous dimensions (all dimensions except for the discrete termination command) are discretized into 256 bins uniformly. Thus, the robot action can be represented using ordinals of the discrete bins as 8 integer numbers. In order to use these discretized actions to finetune a vision-language into a vision-language-\\emph{action} model, we need to associate tokens from the model's \\emph{existing} tokenization with the discrete action bins. This requires reserving 256 tokens to serve as action tokens. Which tokens to choose depends on the particular tokenization used by each VLM, which we discuss later in this section. In order to define a target for VLM fine-tuning we convert the action vector into a single string by simply concatenating action tokens for each dimension with a space character:\n\\begin{align*}\n    \\text{``terminate} \\enspace \\Delta \\text{pos}_x \\enspace \\Delta \\text{pos}_y \\enspace \\Delta \\text{pos}_z \\enspace \\Delta \\text{rot}_x \\enspace \\Delta \\text{rot}_y \\enspace \\Delta \\text{rot}_z \\enspace \\text{gripper\\_extension''}.\n\\end{align*}\nA possible instantiation of such a target could be: ``1 128 91 241 5 101 127''. The two VLMs that we finetune in our experiments, PaLI-X~\\citep{chen2023palix} and PaLM-E~\\citep{driess2023palm}, use different tokenizations. For PaLI-X, integers up to 1000 each have a unique token, so we simply associate the action bins to the token representing the corresponding integer. For the PaLM-E model, which does not provide this convenient representation of numbers, we simply overwrite the 256 least frequently used tokens to represent the action vocabulary.\nIt is worth noting that training VLMs to override existing tokens with action tokens is a form of symbol tuning~\\citep{wei2023symbol}, which has been shown to work well for VLMs in prior work.\n\nTaking the action representation described above, we convert our robot data to be suitable for VLM model fine-tuning, where our inputs include robot camera image and textual task description (using standard VQA format ``Q: what action should the robot take to [task instruction]? A:''), and our output is formatted as a string of numbers/least frequently used tokens representing a robot action.\n\n\\textbf{Co-Fine-Tuning.} As we will show in our experiments, a key technical detail of the training recipe that improves robot performance is \\textit{co-fine-tuning} robotics data with the original web data instead of na\\\"ive finetuning on robot data only. \nWe notice that co-fine-tuning leads to more generalizable policies since the policies are exposed to both abstract visual concepts from web scale data and low level robot actions during fine-tuning, instead of just robot actions.\nDuring co-fine-tuning we balance the ratios of robot and web data in each training batch by increasing the sampling weight on the robot dataset. \n\n\\textbf{Output Constraint.} One important distinction between \\methodname and standard VLMs is that \\methodname is required to output valid action tokens for execution on the real robot. \nThus, to ensure that \\methodname outputs valid action tokens during decoding, we constrain its output vocabulary via only sampling valid action tokens when the model is prompted with a robot-action task, whereas the model is still allowed to output the full range of natural language tokens on standard vision-language tasks. \n\n\\subsection{Real-Time Inference}\n\nThe size of modern VLMs can reach tens or hundreds of billions of parameters~\\citep{chen2023palix,driess2023palm}. The largest model trained in this work uses 55B parameters. It is infeasible to directly run such models on the standard desktop-style machines or on-robot GPUs commonly used for real-time robot control. To the best of our knowledge, our model is the largest ever, by over an order of magnitude, used for direct closed-loop robotic control, and therefore requires a new set of solutions to enable efficient real-time inference. We develop a protocol that allows us to run \\methodname models on robots by deploying them in a multi-TPU cloud service and querying this service over the network. With this solution, we can achieve a suitable frequency of control and also serve multiple robots using the same cloud service. The largest model we evaluated, the 55B parameter \\methodname-PaLI-X-55B model, can run at a frequency of 1-3 Hz. The smaller version of that model, consisting of 5B parameters, can run at a frequency of around 5 Hz. \n\n\\section{Experiments}\n\\label{sec:exp}\n\nOur experiments focus on real-world generalization and emergent capabilities of \\methodname and aim to answer the following questions:\n\\begin{enumerate}\n\\itemsep0em \n\\item How does \\methodname perform on seen tasks and more importantly, generalize over new objects, backgrounds, and environments? \n\\item Can we observe and measure any emergent capabilities of \\methodname?\n\\item How does the generalization vary with parameter count and other design decisions? \n\\item Can \\methodname exhibit signs of chain-of-thought reasoning similarly to vision-language models?\n\\end{enumerate}\nWe evaluate our approach and several baselines with about 6,000 evaluation trajectories in a variety of conditions, which we describe in the following sections. \nUnless specified otherwise, we use a 7DoF mobile manipulator with the action space described in Sec.~\\ref{sec:action_ft}.\nWe also demonstrate examples of \\methodname execution on the project website: \\texttt{\\href{https://robotics-transformer2.github.io/}{robotics-transformer2.github.io}}. We train two specific instantiations of \\methodname that leverage pre-trained VLMs: (1) \\textbf{\\methodname-PaLI-X} is built from 5B and 55B PaLI-X~\\citep{chen2023palix}, and (2) \\textbf{\\methodname-PaLM-E} is built from 12B PaLM-E~\\citep{driess2023palm}.\n\nFor training, we leverage the original web scale data from \\citet{chen2023palix} and \\citet{driess2023palm}, which consists of visual question answering, captioning, and unstructured interwoven image and text examples. We combine it with the robot demonstration data from \\citet{brohan2022rt}, which was collected with 13 robots over 17 months in an office kitchen environment.\nEach robot demonstration trajectory is annotated with a natural language instruction that describes the task performed, consisting of a verb describing the skill (e.g., ``pick'', ''open'', ``place into'') and one or more nouns describing the objects manipulated (e.g., ``7up can'', ``drawer'', ``napkin'') (see Appendix~\\ref{sec:app_data} for more details on the used datasets). For all \\methodname training runs we adopt the hyperparameters from the original PaLI-X~\\citep{chen2023palix} and PaLM-E~\\citep{driess2023palm} papers, including learning rate schedules and regularizations. More training details can be found in Appendix~\\ref{sec:training_details}.\n\n\\textbf{Baselines.} We compare our method to multiple state-of-the-art baselines that challenge different aspects of our method. All of the baselines use the exact same robotic data. \nTo compare against a state-of-the-art policy, we use \\textbf{RT-1}~\\citep{brohan2022rt}, a 35M parameter transformer-based model.\nTo compare against state-of-the-art pretrained representations, we use \\textbf{VC-1}~\\citep{majumdar2023vc1} and \\textbf{R3M}~\\citep{nair2022r3m}, with policies implemented by training an RT-1 backbone to take their representations as input.\nTo compare against other architectures for using VLMs, we use \\textbf{MOO}~\\citep{stone2023open}, which uses a VLM to create an additional image channel for a semantic map, which is then fed into an RT-1 backbone.\nMore information is provided in Appendix~\\ref{sec:app_baselines}.\n\n\\subsection{How does \\methodname perform on seen tasks and more importantly, generalize over new objects, backgrounds, and environments?}\\label{sec:quant}\n\n\\begin{figure}[h]\n\\vspace{-0.3cm}\n    \\centering\n    \\includegraphics[width=0.99\\textwidth]{figures/generalization_evals_dm.pdf}\n    \\vspace{-0.3cm}\n    \\caption{Example generalization scenarios used for evaluation in Figures~\\ref{fig:main_baselines} and \\ref{fig:ablations} and  Tables~\\ref{table:main_baselines} and~\\ref{table:ablations}.}\n    \\label{fig:generalization_evals}\n\\end{figure}\n\nTo evaluate in-distribution\nperformance as well as generalization capabilities, we compare the \\methodname-PaLI-X and \\methodname-PaLM-E models to the four baselines listed in the previous sections. For the \\textit{seen tasks} category, we use the same suite of seen instructions as in RT-1~\\citep{brohan2022rt}, which include over 200 tasks in this evaluation: 36 for picking objects, 35 for knocking objects, 35 for placing things upright, 48 for moving objects, 18 for opening and closing various drawers, and 36 for picking out of and placing objects into drawers.\nNote, however, that these ``in-distribution'' evaluations still vary the  placement of objects and factors such as time of day and robot position, requiring the skills to generalize to realistic variability in the environment.\n\nFigure~\\ref{fig:generalization_evals} shows example generalization evaluations, which are split into \\textit{unseen} categories (\\textit{objects}, \\textit{backgrounds} and \\textit{environments}), and are additionally split into easy and hard cases.\nFor unseen objects, hard cases include harder-to-grasp and more unique objects (such as toys).\nFor unseen backgrounds, hard cases include more varied backgrounds and novel objects.\nLastly, for unseen environments, hard cases correspond to a more visually distinct office desk environment with monitors and accessories, while the easier environment is a kitchen sink.\nThese evaluations consists of over 280 tasks that focus primarily on pick and placing skills in many diverse scenarios. \nThe list of instructions for unseen categories is specified in Appendix~\\ref{sec:app_eval_instr}.\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=1.0\\textwidth]{figures/rt2_overall_wide_dm.png}\n\\caption{Overall performance of two instantiations of \\methodname and baselines across seen training tasks as well as unseen evaluations measuring generalization to novel objects, novel backgrounds, and novel environments. Appendix Table~\\ref{table:main_baselines} details the full results.}\n\\label{fig:main_baselines}\n\\end{figure}\n\nThe evaluation results are shown in Figure~\\ref{fig:main_baselines} and Appendix Table~\\ref{table:main_baselines}. The performance on seen tasks is similar between the \\methodname models and RT-1, with other baselines attaining a lower success rate. \nThe difference between the \\methodname models and the baseline is most pronounced in the various generalization experiments, suggesting that the strength of vision-language-action models lies in transferring more generalizable visual and semantic concepts from their Internet-scale pretraining data. Here, on average, both instantiations of \\methodname perform similarly, resulting in $\\sim$2x improvement over the next two baselines, RT-1 and MOO, and ${\\scriptsize \\sim}$6x better than the other baselines. The PaLM-E version of \\methodname seems to perform better than the \\methodname-PaLI-X in harder versions of generalization scenarios while under-performing on easier ones, resulting in a similar average performance.\n\n\\textbf{Open Source Language Table Benchmark.} To provide an additional point of comparison using open-source baselines and environments, we leverage the open-source Language-Table simulation environment from~\\citet{lynch2022interactive}. We co-fine-tune a smaller PaLI 3B model on several prediction tasks, including in-domain VQA tasks, for the Language-Table dataset, and evaluate the resulting policy in simulation. For the action prediction task, we discretize and encode actions as text in the format ``\\texttt{X Y}'', where \\texttt{X} and \\texttt{Y} range between \\{-10, -9, \\ldots, +9, +10\\}, and represent delta 2D cartesian setpoints of the end effector. Due to its reduced size, the resulting model can run inference at a similar rate (5 Hz) as the other baselines. \nThe results of this experiment are presented in Table~\\ref{table:sim-langtable}. We observe a significant performance boost when using our model compared to the baselines, indicating that the VLM-based pre-training together with the expressiveness of the large PaLI model can be beneficial in other scenarios, in this case, simulation with a different robot.\nWe also show qualitative real-world out-of-distribution behaviors behaviors in Figure~\\ref{fig:lang-table-real}, demonstrating novel pushing tasks and targeting objects not before seen in this environment. \nMore details about the Language Table experiments can be found in Appendix~\\ref{sec:app_data} and~\\ref{sec:app_vlm}.\n\n\\begin{figure}\n\\begin{floatrow}\n\\capbfigbox{%\n  \\includegraphics[width=0.53\\textwidth]{figures/LangTable-R2-Narrow-DM.png}\n}{%\n  \\caption{\n  Real-world out-of-distribution behaviors in the Language Table environment. Identical \\methodname-PaLI-3B model checkpoint is used as in Tab.~\\ref{table:sim-langtable}.}\n    \\label{fig:lang-table-real}\n}\n\\hspace{-0.2cm}\n\\capbtabbox{%\n\\scriptsize\n  \\begin{tabular}{cc}\n    \\toprule\n    Model & Language-Table \\\\\n    \\midrule\n    BC-Zero~\\citep{jang2022bc} & 72 $\\pm$ 3 \\\\\n    RT-1~\\citep{brohan2022rt} & 74 $\\pm$ 13 \\\\\n    LAVA~\\citep{lynch2022interactive} & 77 $\\pm$ 4 \\\\\n    \\textbf{\\methodname-PaLI-3B (ours)} & \\textbf{90 $\\pm$ 10} \\\\\n    \\bottomrule\n    \\end{tabular}\n}{%\n  \\caption{Performance on the simulated Language-Table tasks~\\citep{lynch2020language}.}%\n  \\label{table:sim-langtable}\n}\n\\end{floatrow}\n\\end{figure}\n\n\\subsection{Can we observe and measure any emergent capabilities of \\methodname?}\n\\label{sec:emergent}\nIn addition to evaluating the generalization capabilities of vision-language-action models, we also aim to evaluate the degree to which such models can enable new capabilities beyond those demonstrated in the robot data by transferring knowledge from the web. We refer to such capabilities as \\emph{emergent}, in the sense that they emerge by transferring Internet-scale pretraining. We do not expect such transfer to enable new robotic \\emph{motions}, but we do expect semantic and visual concepts, including relations and nouns, to transfer effectively, even in cases where those concepts were not seen in the robot data. \n\n\\textbf{Qualitative Evaluations.} First, we experiment with our \\methodname-PaLI-X model to determine various emergent capabilities transferred from vision-language concepts. We demonstrate some examples of such interactions in Figure~\\ref{fig:qualitative_emergent}.\nWe find through our explorations that \\methodname inherits novel capabilities in terms of semantic understanding and basic reasoning in the context of the scene.\nFor example accomplishing the task ``put strawberry into the correct bowl'' requires a nuanced understanding of not only what a strawberry and bowl are, but also reasoning in the context the scene to know the strawberry should go with the like fruits.\nFor the task ``pick up the bag about to fall off the table,'' \\methodname demonstrates physical understanding to disambiguate between two bags and recognize the precariously placed object.\nAll the interactions tested in these scenarios have never been seen in the robot data, which points to the transfer of semantic knowledge from vision-language data.\n\n\\textbf{Quantitative Evaluations.}\nTo quantify these emergent capabilities, we take the top two baselines from the previous evaluations, RT-1 and VC-1, and compare them against our two models: \\methodname-PaLI-X and \\methodname-PaLM-E. To reduce the variance of these experiment, we evaluate all of the methods using the A/B testing framework~\\citep{fisher1936design}%\n, where all four models are evaluated one after another in the exact same conditions.\n\nWe' split the emergent capabilities of \\methodname into three categories covering axes of reasoning and semantic understanding (with examples of each shown in  Appendix Figure~\\ref{fig:quant_eval_collage}). \nThe first we term \\textit{symbol understanding}, which explicitly tests whether the \\methodname policy transfers semantic knowledge from vision-language pretraining that was not present in any of the robot data. Example instructions in this category are ``move apple to 3'' or ``push coke can on top of heart''. \nThe second category we term \\textit{reasoning}, which  demonstrates the ability to apply various aspects of reasoning of the underlying VLM to control tasks.\nThese tasks require visual reasoning (``move the apple to cup with same color''), math (``move X near the sum of two plus one''), and multilingual understanding (``mueve la manzana al vaso verde'').\nWe refer to the last category as \\textit{human recognition} tasks, which include tasks such as ``move the coke can to the person with glasses'', to demonstrate human-centric understanding and recognition.\nThe full list of instructions used for this evaluation is specified in Appendix~\\ref{sec:app_eval_instr}.\n\nWe present the results of this experiment in Figure~\\ref{fig:emergent} with all the numerical results in Appendix~\\ref{sec:app_emergent}. We observe that our \\categoryname models significantly outperform the baselines across all categories, with our best \\methodname-PaLI-X model achieving more than 3x average success rate over the next best baseline (RT-1). We also note that while the larger PaLI-X-based model results in better symbol understanding, reasoning and person recognition performance on average, the smaller PaLM-E-based model has an edge on tasks that involve math reasoning. We attribute this interesting result to the different pre-training mixture used in PaLM-E, which results in a model that is more capable at math calculation than the mostly visually pre-trained PaLI-X. \n\n\\begin{figure}[h]\n\\centering\n    \\begin{subfigure}[b]{0.49\\textwidth}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/rt2_emergent_dm.png}\n\\caption{Performance comparison on various emergent skill evaluations (Figure~\\ref{fig:quant_eval_collage}) between \\methodname and two baselines.}\n\\label{fig:emergent}\n\\end{subfigure}\n\\begin{subfigure}[b]{0.49\\textwidth}\n\\centering\n    \\includegraphics[width=\\textwidth]{figures/rt2_ablations_dm.png}\n    \\caption{Ablations of \\methodname-PaLI-X showcasing the impact of parameter count and training strategy on generalization.}\n    \\label{fig:ablations}\n    \\end{subfigure}\n\\caption{Quantitative performance of \\methodname across (\\ref{fig:emergent}) emergent skills and (\\ref{fig:ablations}) size and training ablations. Appendix Tables~\\ref{table:emergent} and \\ref{table:ablations} detail the full numerical results.}\n\\label{fig:quant}\n\\end{figure}\n\n\\subsection{How does the generalization vary with parameter count and other design decisions?} \\label{sec:ablations}\n\nFor this comparison, we use \\methodname-PaLI-X model because of its flexibility in terms of the model size (due to the nature of PaLM-E, \\methodname-PaLM-E is restricted to only certain sizes of PaLM and ViT models). In particular, we compare two different model sizes, 5B and 55B, as well as three different training routines: training a model from scratch, without using any weights from the VLM pre-training; fine-tuning a pre-trained model using robot action data only; and co-fine-tuning (co-training with fine-tuning), the primary method used in this work where we use both the original VLM training data as well as robotic data for VLM fine-tuning. Since we are mostly interested in the generalization aspects of these models, we remove the \\textit{seen tasks} evaluation from this set of experiments.\n\nThe results of the ablations are presented in Figure~\\ref{fig:ablations} and Appendix Table~\\ref{table:ablations}. First, we observe that training a very large model from scratch results in a very poor performance even for the 5B model. Given this result, we decide to skip the evaluation of an even bigger 55B PaLI-X model when trained from scratch. Second, we notice that co-fine-tuning a model (regardless of its size) results in a better generalization performance than simply fine-tuning it with robotic data. We attribute this to the fact that keeping the original data around the fine-tuning part of training, allows the model to not forget its previous concepts learned during the VLM training. Lastly, somewhat unsurprisingly, we notice that the increased size of the model results in a better generalization performance.\n\n\\subsection{Can \\methodname exhibit signs of chain-of-thought reasoning similarly to vision-language models?}\n\\label{sec:cot}\n\nInspired by the chain-of-thought prompting method in LLMs~\\citep{wei2022chain}, we fine-tune a variant of \\methodname with PaLM-E for just a few hundred gradient steps to increase its capability of utilizing language and actions jointly with the hope that it will elicit a more sophisticated reasoning behavior. \nWe augment the data to include an additional ``Plan'' step, which describes the purpose of the action that the robot is about to take in natural language first, which is then followed by the actual action tokens, e.g. ``Instruction: I'm hungry. Plan: pick rxbar chocolate. Action: 1 128 124 136 121 158 111 255.''\nThis data augmentation scheme acts as a bridge between VQA datasets (visual reasoning) and manipulation datasets (generating actions).\n\nWe qualitatively observe that \\methodname with chain-of-thought reasoning is able to answer more sophisticated commands due to the fact that it is given a place to plan its actions in natural language first. This is a promising direction that provides some initial evidence that using LLMs or VLMs as planners~\\citep{ahn2022can, driess2023palm} can be combined with low-level policies in a single VLA model.\nRollouts of \\methodname with chain-of-thought reasoning are shown in Figure~\\ref{fig:cot} and in Appendix~\\ref{sec:app_cot}.\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/RT2CoT_Narrow.pdf}\n    \\caption{Rollouts of \\methodname with chain-of-thought reasoning, where \\methodname generates both a plan and an action.}\n    \\label{fig:cot}\n\\end{figure}\n\n\\section{Limitations}\n\\label{sec:limitations}\nEven though \\methodname exhibits promising generalization properties, there are multiple limitations of this approach. First, although we show that including web-scale pretraining via VLMs boosts generalization over semantic and visual concepts, the robot does not acquire any ability to perform new \\emph{motions} by virtue of including this additional experience. The model's physical skills are still limited to the distribution of skills seen in the robot data (see Appendix \\ref{sec:failure-cases}), but it learns to deploy those skills in new ways. \nWe believe this is a result of the dataset not being varied enough along the axes of skills. An exciting direction for future work is to study how new skills could be acquired through new data collection paradigms such as videos of humans.\n\nSecond, although we showed we could run large VLA models in real time, the computation cost of these models is high, and as these methods are applied to settings that demand high-frequency control, real-time inference may become a major bottleneck. An exciting direction for future research is to explore quantization and distillation techniques that might enable such models to run at higher rates or on lower-cost hardware. This is also connected to another current limitation in that there are only a small number of generally available VLM models that can be used to create \\methodname. We hope that more open-sourced models will become available (e.g. \\url{https://llava-vl.github.io/}) and the proprietary ones will open up their fine-tuning APIs, which is a sufficient requirement to build \\categoryname models. \n\n\\section{Conclusions}\n\\label{sec:conclusions}\n\nIn this paper, we described how vision-language-action (VLA) models could be trained by combining vision-language model (VLM) pretraining with robotic data. We then presented two instantiations of VLAs based on PaLM-E and PaLI-X, which we call \\methodname-PaLM-E and \\methodname-PaLI-X. These models are co-fine-tuned with robotic trajectory data to output robot actions, which are represented as text tokens. We showed that our approach results in very performant robotic policies and, more importantly, leads to a significantly better generalization performance and emergent capabilities inherited from web-scale vision-language pretraining. We believe that this simple and general approach shows a promise of robotics directly benefiting from better vision-language models, which puts the field of robot learning in a strategic position to further improve with advancements in other fields.\n\n\\section*{Acknowledgments}\nWe would like to acknowledge Fred Alcober, Jodi Lynn Andres, Carolina Parada, Joseph Dabis, Rochelle Dela Cruz, Jessica Gomez, Gavin Gonzalez, John Guilyard, Tomas Jackson, Jie Tan, Scott Lehrer, Dee M, Utsav Malla, Sarah Nguyen, Jane Park, Emily Perez, Elio Prado, Jornell Quiambao, Clayton Tan, Jodexty Therlonge, Eleanor Tomlinson, Wenxuan Zhou, and the greater Google DeepMind team for their feedback and contributions.\n\n\\clearpage\n\n\\clearpage\n\n\\end{document}\n</paper 2>\n\n<paper 3>\n\\title{Open X-Embodiment: Robotic Learning Datasets and RT-X Models}\n\n\\begin{document}\n\n\\makeatletter\n\\let\\@oldmaketitle\\@maketitle%\n\\renewcommand{\\@maketitle}{\\@oldmaketitle%\n\\vspace{0.15em}\n  \\includegraphics[width=\\linewidth]\n    {figures/data_vis_v3.jpg}\n    \\captionof{figure}{\\small We propose an open, large-scale dataset for robot learning curated from $\\numinstitution$ institutions across the globe. The dataset represents diverse behaviors, robot embodiments and environments, and enables learning generalized robotic policies.}\n    \\vspace{-1.35em}\n    \\label{fig:teaser}\n    }%\n\\makeatother\n\n\\maketitle\n\\pagestyle{empty}\n\n\\addtocounter{figure}{-1}\n\n\\begin{abstract}\nLarge, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train ``generalist’’ \\cro-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective \\cro-robot policies. We assemble a dataset from $\\numembodiment$ different robots collected through a collaboration between $\\numinstitution$ institutions, demonstrating $\\numskill$ skills ($\\numtask$ tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms. The project website is \\repourl.\n\\end{abstract}\n\\vspace{-0.4em}\n\n\\vspace{-0.1cm}\n\\section{Introduction}\n\\vspace{-0.1cm}\n\n\\footnotetext{\\scriptsize $^{1}$Allen Institute for AI; $^{2}$Arizona State University; $^{3}$California Institute of Technology; $^{4}$Carnegie Mellon University; $^{5}$Columbia University; $^{6}$EPFL; $^{7}$ETH Zürich; $^{8}$Flexiv Robotics; $^{9}$Georgia Institute of Technology; $^{10}$German Aerospace Center; $^{11}$Google DeepMind; $^{12}$Google Research; $^{13}$IO-AI TECH; $^{14}$Imperial College London; $^{15}$Intrinsic LLC; $^{16}$Istituto Italiano di Tecnologia; $^{17}$Korea Advanced Institute of Science \\& Technology; $^{18}$Max Planck Institute; $^{19}$Meta AI; $^{20}$Microsoft Research; $^{21}$Mila Quebec; $^{22}$NVIDIA; $^{23}$New York University; $^{24}$Princeton University; $^{25}$Queensland University of Technology; $^{26}$RIKEN; $^{27}$Shanghai Jiao Tong University; $^{28}$Stanford University; $^{29}$Technische Universität Darmstadt; $^{30}$The University of Texas at Austin; $^{31}$The University of Tokyo; $^{32}$Toyota Research Institute; $^{33}$Tsinghua University; $^{34}$University of California, Berkeley; $^{35}$University of California, Davis; $^{36}$University of California, San Diego; $^{37}$University of Edinburgh; $^{38}$University of Freiburg; $^{39}$University of Illinois Urbana-Champaign; $^{40}$University of Michigan; $^{41}$University of Montreal; $^{42}$University of Pennsylvania; $^{43}$University of Southern California; $^{44}$University of Technology, Nuremberg; $^{45}$University of Texas at Austin; $^{46}$University of Washington\n}\n\nA central lesson from advances in machine learning and artificial intelligence is that large-scale learning from diverse datasets can enable capable AI systems by providing for general-purpose pretrained models. In fact, large-scale general-purpose models typically trained on large and diverse datasets can often outperform their \\emph{narrowly targeted} counterparts trained on smaller but more task-specific data. For instance, open-vocab classifiers (e.g., CLIP~\\cite{radford2021learning}) trained on large datasets scraped from the web tend to outperform fixed-vocabulary models trained on more limited datasets, and large language models~\\cite{openai2023gpt4,anil2023palm} trained on massive text corpora tend to outperform systems that are only trained on narrow task-specific datasets. Increasingly, the most effective way to tackle a given narrow task (e.g., in vision or NLP) is to adapt a general-purpose model. However, these lessons are difficult to apply in robotics: any single robotic domain might be too narrow, and while computer vision and NLP can leverage large datasets sourced from the web, comparably large and broad datasets for robotic interaction are hard to come by. Even the largest data collection efforts still end up with datasets that are a fraction of the size and diversity of benchmark datasets in vision (5-18M)~\\cite{Weyand_2020_CVPR, tencent-ml-images-2019} and NLP (1.5B-4.5B)~\\cite{lehmann-2017, muhleisen2012web}. More importantly, such datasets are often still narrow along some axes of variation, either focusing on a single environment, a single set of objects, or a narrow range of tasks. How can we overcome these challenges in robotics and move the field of robotic learning toward large data regime that has been so successful in other domains?\n\nInspired by the generalization made possible by pretraining large vision or language models on diverse data, we take the perspective that the goal of training generalizable robot policies requires \\textbf{\\cro-embodiment training}, i.e., with data from multiple robotic platforms.\nWhile each individual robotic learning dataset might be too narrow, the union of all such datasets provides a better coverage of variations in environments and robots. Learning generalizable robot policies requires developing methods that can utilize \\cro-embodiment data, tapping into datasets from many labs, robots, and settings. Even if such datasets in their current size and coverage are insufficient to attain the impressive generalization results that have been demonstrated by large language models, in the future, the union of such data can potentially provide this kind of coverage. \nBecause of this, \\textbf{we believe that enabling research into \\cro-embodiment robotic learning is critical at the present juncture}.\n\nFollowing this rationale, we have two goals: \\textbf{(1)} Evaluate whether policies trained on data from many different robots and environments enjoy the benefits of positive transfer, attaining better performance than policies trained only on data from each evaluation setup. \\textbf{(2)} Organize large robotic datasets to enable future research on \\cro-embodiment models.\n\nWe focus our work on robotic manipulation. Addressing goal \\textbf{(1)}, our empirical contribution is to demonstrate that several recent robotic learning methods, with minimal modification, can utilize \\cro-embodiment data and enable positive transfer. \nSpecifically, we train the RT-1~\\cite{brohan2023rt1} and RT-2~\\cite{brohan2023rt2} models on  $\\nummanipulatorusedintraining$ different robotic manipulators.\nWe show that the resulting models, which we call RT-X, can improve over policies trained only on data from the evaluation domain, exhibiting better generalization and new capabilities. Addressing \\textbf{(2)}, we provide the \\repo (\\repoacronym) Repository, which includes a dataset with $\\numembodiment$ different robotic embodiments from $\\numinstitution$ different institutions that can enable the robotics community to pursue further research on \\cro-embodiment models, along with open-source tools to facilitate such research. Our aim is not to innovate in terms of the particular architectures and algorithms, but rather to provide the model that we trained together with data and tools to energize research around \\cro-embodiment robotic learning.\n\n\\begin{figure*}\n    \\centering\n    \\includegraphics[width=0.86\\linewidth]\n    {figures/rtx_data_analysis.pdf}\n    \\vspace{-0.25em}\n    \\caption{\\small The \\repo Dataset. \\textbf{(a)}: the dataset consists of \\numdatasets~individual datasets across $\\numembodiment$ embodiments. \\textbf{(b)}: the Franka robot has the largest diversity in visually distinct scenes due to the large number of Franka datasets, \\textbf{(c)}: xArm and Google Robot contribute the most number of trajectories due to a few large datasets, \\textbf{(d, e)}: the dataset contains a great diversity of skills and common objects. \n    \\label{fig:data_vis}}\n    \\vspace{-1.8em}\n\\end{figure*}\n\n\\vspace{-0.5em}\n\\section{Related Work}\n\\vspace{-0.15em}\n\n\\noindent \\textbf{Transfer across embodiments.} A number of prior works have studied methods for transfer across robot embodiments in simulation~\\cite{devin2017learning,chen2018hardware,sanchez2018graph,pathak2019learning,martin2019iros,huang2020smp,kurin2020my,zakka2021xirl,ghadirzadeh2021bayesian,gupta2021metamorph,schubert2023generalist, shah2023gnm, Zhou2023Modularity} and on real robots~\\cite{dasari2019robonet, hu2022know,bousmalis2023robocat,yang2023polybot,reed2022a,salhotra2023bridging,radosavovic2023robot}. These methods often introduce mechanisms specifically designed to address the embodiment gap between different robots, such as shared action representations~\\cite{martin2019iros,shao2020unigrasp}, incorporating representation learning objectives~\\cite{zakka2021xirl,yang2023polybot}, adapting the learned policy on embodiment information~\\cite{shao2020unigrasp,xu2021adagrasp,chen2018hardware,ghadirzadeh2021bayesian,huang2020smp}, and decoupling robot and environment representations~\\cite{hu2022know}. \nPrior work has provided initial demonstrations of \\cro-embodiment training \\cite{reed2022a} and transfer \\cite{bousmalis2023robocat, shah2023vint, radosavovic2023robot} with transformer models. %\nWe investigate complementary architectures and provide complementary analyses, and, in particular, study the interaction between \\cro-embodiment transfer and web-scale pretraining.\nSimilarly, methods for transfer across human and robot embodiments also often employ techniques for reducing the embodiment gap, i.e. by translating between domains or learning transferable representations~\\cite{liu2018imitation,yu2018one,sharma2019third, smith2019avid,bonardi2020learning,schmeckpeper2021reinforcement,xiong2021learning,jang2022bc,bahl2022whirl,ding2023embodied,Bahl_2023_CVPR}. Alternatively, some works focus on sub-aspects of the problem such as learning transferable reward functions~\\cite{sermanet2016unsupervised,zakka2021xirl,shao2020concept,chen2021learning,kumar2023graph,alakuijala2023learning}, goals~\\cite{zhou2021manipulator,wang2023mimicplay}, dynamics models~\\cite{schmeckpeper2020learning}, or visual representations~\\cite{nair2022r3m,xiao2022masked,radosavovic2022real,ma2022vip,majumdar2023we,karamcheti2023language,mu2023ec2,bahl2023affordances} from human video data. Unlike most of these prior works, we directly train a policy on \\cro-embodiment data, without any mechanisms to reduce the embodiment gap, and observe positive transfer by leveraging that data.\n\n\\noindent \\textbf{Large-scale robot learning datasets.} The robot learning community has created open-source robot learning datasets, spanning grasping~\\cite{jiang2011efficient,Pinto2015SupersizingSL,bohg2015dataset,Mahler2017DexNet2D,depierre2018jacquard,levine2018learning,kalashnikov2018qt,Brahmbhatt2019,fang2020graspnet,acronym2020,bousmalis2018grasping,zhu2023fanuc}, pushing interactions~\\cite{yu2016more,finn2017deep,ebert2018visual,dasari2019robonet}, sets of objects and models~\\cite{shilane_princeton_2004,wohlkinger_3dnet_2012,kit2012,singh_bigbird_2014,Calli2015YCB,zhirong_wu_3d_2015,xiang_objectnet3d_2016,morrison2020egad,gao2021objectfolder,downs2022google,kalashnikov2021mt}, and teleoperated demonstrations~\\cite{DBLP:journals/corr/abs-1811-02790,sharma2018multiple,mandlekar2019scaling,ebert2021bridge,robomimic2021,brohan2023rt1,lynch2023interactive,fang2023rh20t,roboagent,heo2023furniturebench,walke2023bridgedata}. With the exception of RoboNet~\\cite{dasari2019robonet}, these datasets contain data of robots of the same type,\nwhereas we focus on data spanning multiple embodiments. The goal of our data repository is complementary to these efforts: we process and aggregate a large number of prior datasets into a single, standardized repository, called \\repo, which shows how robot learning datasets can be shared in a meaningul and useful way. \n\n\\noindent \\textbf{Language-conditioned robot learning.} Prior work has aimed to endow robots and other agents with the ability to understand and follow language instructions~\\cite{WINOGRAD19721,macmahon2006, kollar2010,chen2011,duvallet2015,Luketina2019ASO}, often by learning language-conditioned policies~\\cite{shao2020concept,stepputtis2020language, nair2022learning,mees2022calvin,mees2022matters,jang2022bc,shridhar2022perceiver,brohan2023rt1}. We train language-conditioned policies via imitation learning like many of these prior works but do so using large-scale multi-embodiment demonstration data. Following previous works that leverage pre-trained language embeddings~\\cite{hill2020human,shao2020concept,lynch2021grounding,nair2022learning,jang2022bc,ahn2022saycan,jiang2022vima,brohan2023rt1,vemprala2023chatgpt,huang2023voxposer} and pre-trained vision-language models~\\cite{shridhar2022cliport,stone2023moo,mu2023embodiedgpt,brohan2023rt2} in robotic imitation learning, we study both forms of pre-training in our experiments, specifically following the recipes of RT-1~\\cite{brohan2023rt1} and RT-2~\\cite{brohan2023rt2}.\n\n\\begin{figure*}[h]\n    \\centering\n    \\includegraphics[width=0.88\\textwidth]{figures/rt12_teaser_model_vertical.pdf}\n    \\vspace{-0.1em}\n    \\caption{\n    \\small RT-1-X and RT-2-X both take images and a text instruction as input and output discretized end-effector actions. \n    RT-1-X is an architecture designed for robotics, with a FiLM~\\cite{perez2017film} conditioned EfficientNet~\\cite{tan2019efficientnet} and a Transformer~\\cite{vaswani2017attention}.\n    RT-2-X builds on a VLM backbone by representing actions as another language, and training action text tokens together with vision-language data.\n    }\n\\label{fig:rt12}\n\\vspace{-1.8em}\n\\end{figure*}\n\n\\vspace{-0.3em}\n\\section{The \\repo Repository}\n\\vspace{-0.02em}\n\nWe introduce the \\repo Repository (\\repourl) -- an open-source repository which includes \\textbf{large-scale data} along with \\textbf{pre-trained model checkpoints} for \\cro-embodied robot learning research. More specifically, we provide and maintain the following open-source resources to the broader community: \n\\begin{itemize}\n    \\item \\textbf{\\repo Dataset}: robot learning dataset with \\emph{1M+ robot trajectories} from \\emph{$\\numembodiment$ robot embodiments}.\n    \\item \\textbf{Pre-Trained Checkpoints}: a selection of RT-X model checkpoints ready for inference and finetuning.\n\\end{itemize}\n\\vspace{-0.2em}\nWe intend for these resources to form a foundation for \\cro-embodiment research in robot learning, but they are just the start. \\repo is a community-driven effort, currently involving $\\numinstitution$ institutions from around the world, and we hope to further broaden participation and grow the initial \\repo Dataset over time. In this section, we summarize the dataset and \\cro-embodiment learning framework, before discussing the specific models we use to evaluate our dataset and our experimental results.\n\n\\vspace{-0.3em}\n\\subsection{The \\repo Dataset}\n\\vspace{-0.1em}\n\nThe \\repo Dataset contains 1M+ real robot trajectories spanning $\\numembodiment$ robot embodiments, from single robot arms to bi-manual robots and quadrupeds. The dataset was constructed by pooling \\numdatasets~\\emph{existing} robot datasets from $\\numresearchlab$ robotic research labs around the world and converting them into a consistent data format for easy download and usage.\nWe use the \\href{https://github.com/google-research/rlds}{RLDS} data format~\\cite{ramos2021rlds}, which saves data in serialized \\href{https://www.tensorflow.org/tutorials/load_data/tfrecord}{\\texttt{tfrecord}} files and accommodates the various action spaces and input modalities of different robot setups, such as differing numbers of RGB cameras, depth cameras and point clouds. It also supports efficient, parallelized data loading in all major deep learning frameworks. For more details about the data storage format and a breakdown of all \\numdatasets~datasets, see \\repourl.\n\n\\vspace{-0.25em}\n\\subsection{Dataset Analysis}\n\\vspace{-0.2em}\n\n\\cref{fig:data_vis} analyzes the \\repo Dataset. %\n\\cref{fig:data_vis}(a) shows the breakdown of datasets by robot embodiments, with the Franka robot being the most common. This is reflected in the number of distinct scenes (based on dataset metadata) per embodiment (\\cref{fig:data_vis}(b)), where Franka dominates. \n\\cref{fig:data_vis}(c) shows the breakdown of trajectories per embodiment. \nTo further analyze the diversity, we use the language annotations present in our data. We use the PaLM language model~\\citep{anil2023palm} to extract objects and behaviors from the instructions. \\cref{fig:data_vis}(d,e) show the diversity of skills and objects. While most skills belong to the pick-place family, the long tail of the dataset contains skills like ``wiping'' or ``assembling''. Additionally, the data covers a range of household objects, from appliances to food items and utensils. \n\n\\vspace{-0.2em}\n\\section{RT-X Design}\n\\vspace{-0.16em}\n\nTo evaluate how much \\cro-embodiment training can improve the performance of learned policies on individual robots, we require models that have sufficient capacity to productively make use of such large and heterogeneous datasets. To that end, our experiments will build on two recently proposed Transformer-based robotic policies: RT-1~\\cite{brohan2023rt1} and RT-2~\\cite{brohan2023rt2}. We briefly summarize the design of these models in this section, and discuss how we adapted them to the \\cro-embodiment setting in our experiments.\n\n\\vspace{-0.3em}\n\\subsection{Data format consolidation}\n\\vspace{-0.15em}\nOne challenge of creating \\cro-embodiment models is that observation and action spaces vary significantly across robots.\nWe use a coarsely aligned action and observation space across datasets.\nThe model receives a history of recent images and language instructions as observations\nand predicts a 7-dimensional action vector controlling the end-effector ($x$, $y$, $z$, roll, pitch, yaw, and gripper opening or the rates of these quantities). We select one canonical camera view from each dataset as the input image, resize it to a common resolution and convert the original action set into a 7 DoF end-effector action. \nWe normalize each dataset's actions prior to discretization.\nThis way, an output of the model can be interpreted (de-normalized) differently depending on the embodiment used.\nIt should be noted that despite this coarse alignment, the camera observations still vary substantially across datasets, e.g.\\ due to differing camera poses relative to the robot or differing camera properties, see Figure~\\ref{fig:rt12}.\nSimilarly, for the action space, we do not align the coordinate frames across datasets in which the end-effector is controlled, and allow action values to represent either absolute or relative positions or velocities, as per the original control scheme chosen for each robot. \nThus, the same action vector may induce very different motions for different robots.\n\n\\vspace{-0.2em}\n\\subsection{Policy architectures}\n\\vspace{-0.1em}\n\nWe consider two model architectures in our experiments: (1) RT-1~\\cite{brohan2023rt1}, an efficient Transformer-based architecture designed for robotic control, and (2) RT-2~\\cite{brohan2023rt2} a large vision-language model co-fine-tuned to output robot actions as natural language tokens.\nBoth models take in a visual input and natural language instruction describing the task, and output a tokenized action. \nFor each model, the action is tokenized into 256 bins uniformly distributed along each of eight dimensions; one dimension for terminating the episode and seven dimensions for end-effector movement. %\nAlthough both architectures are described in detail in their original papers~\\cite{brohan2023rt1,brohan2023rt2}, we provide a short summary of each below:\n\n\\textbf{RT-1~\\cite{brohan2023rt1}} is a 35M parameter network built on a Transformer architecture~\\cite{vaswani2017attention} and designed for robotic control, as shown in~\\cref{fig:rt12}. \nIt takes in a history of $15$ images along with the natural language. Each image is processed through an ImageNet-pretrained EfficientNet~\\cite{tan2019efficientnet} and the natural language instruction is transformed into a USE~\\cite{cer2018universal} embedding. \nThe visual and language representations are then interwoven via FiLM~\\cite{perez2017film} layers, producing 81 vision-language tokens. \nThese tokens are fed into a decoder-only Transformer, which outputs the tokenized actions.\n\n\\begin{figure*}[!htbp]\n    \\centering\n    \\includegraphics[width=0.9\\linewidth]{figures/rtx_barplot_10.png}\n    \\caption{\\small RT-1-X mean success rate is $50\\%$ higher than that of either the Original Method or RT-1. RT-1 and RT-1-X have the same network architecture. Therefore the performance increase can be attributed to co-training on the robotics data mixture. \n    The lab logos indicate the physical location of real robot evaluation, and the robot pictures indicate the embodiment used for the evaluation.}\n    \\label{fig:rt_x_barplot}\n    \\vspace{-1.9em}\n\\end{figure*}\n\n\\textbf{RT-2~\\cite{brohan2023rt2}} is a family of large vision-language-\\textit{action} models (VLAs) trained on Internet-scale vision and language data along with robotic control data. \nRT-2 casts the tokenized actions to text tokens, e.g., a possible action may be ``1 128 91 241 5 101 127''. \nAs such, any pretrained vision-language model (VLM~\\cite{chen2023palix, alayrac2022flamingo, driess2023palme}) can be finetuned for robotic control, thus leveraging the backbone of VLMs and transferring some of their generalization properties.\nIn this work, we focus on the RT-2-PaLI-X variant~\\cite{chen2023palix} built on a backbone of a visual model, ViT~\\cite{dosovitskiy2021image}, and a language model, UL2~\\cite{tay2023ul2}, and pretrained primarily on the WebLI~\\cite{chen2023palix} dataset.\n\n\\vspace{-0.1em}\n\\subsection{Training and inference details}\n\\vspace{-0.1em}\n\\label{sec:training_details}\n\nBoth models use a standard categorical cross-entropy objective over their output space (discrete buckets for RT-1 and all possible language tokens for RT-2). \n\nWe define the robotics data mixture used across all of the experiments as the data from $\\nummanipulatorusedintraining$ manipulators, and taken from RT-1~\\cite{brohan2023rt1}, QT-Opt~\\cite{kalashnikov2018qt}, Bridge~\\cite{walke2023bridgedata}, Task Agnostic Robot Play~\\cite{rosetebeas2022latent, mees2023grounding}, Jaco Play~\\cite{dass2023jacoplay}, Cable Routing~\\cite{luo2023multistage}, RoboTurk~\\cite{DBLP:journals/corr/abs-1811-02790}, NYU VINN~\\cite{pari2021surprising}, Austin VIOLA~\\cite{zhu2023viola}, Berkeley Autolab UR5~\\cite{BerkeleyUR5Website}, TOTO~\\cite{zhou2023train} and Language Table~\\cite{lynch2023interactive} datasets.\nRT-1-X is trained on only robotics mixture data defined above, whereas RT-2-X is trained via co-fine-tuning (similarly to the original RT-2~\\cite{brohan2023rt2}), with an approximately one to one split of the original VLM data and the robotics data mixture.\nNote that the robotics data mixture used in our experiments includes $\\numembodimentusedintraining$ embodiments which is fewer than the entire \\repo dataset ($\\numembodiment$) -- the practical reason for this difference is that we have continued to extend the dataset over time, and at the time of the experiments, the dataset above represented all of the data. In the future, we plan to continue training policies on the extended versions of the dataset as well as continue to grow the dataset together with the robot learning community.\n\nAt inference time, each model is run at the rate required for the robot (3-10~Hz), with RT-1 run locally and RT-2 hosted on a cloud service and queried over the network. \n\n\\begin{table}\n\\vspace{1em}\n        \\centering\n        \\scriptsize\n        \\setlength{\\tabcolsep}{3pt}\n        \\begin{tabular}{@{}lccc@{}}\n        \\toprule\n        Evaluation Setting & Bridge & Bridge & RT-1 paper 6 skills \\\\\n        \\midrule\n        Evaluation Location & IRIS (Stanford) & RAIL Lab (UCB) & Google Robotic Lab \\\\\n        Robot Embodiment &  WidowX & WidowX & \\edrbot \\\\\n        Original Method & LCBC \\cite{walke2023bridgedata} & LCBC \\cite{walke2023bridgedata} & - \\\\\n        \\midrule\n        Original Method & $13\\%$ & $13\\%$ & - \\\\\n        RT-1 & $40\\%$ & $\\textbf{30\\%}$ & $\\textbf{92\\%}$ \\\\\n        RT-1-X & $27\\%$ & $27\\%$ & $73\\%$ \\\\  \n        RT-2-X (55B) & $\\textbf{50\\%}$ & $\\textbf{30\\%}$ & $\\textbf{91\\%}$ \\\\\n        \\bottomrule\n        \\end{tabular}\n        \\vspace{-0.15em}\n        \\caption{ \\small Parameter count scaling experiment to assess the impact of capacity on absorbing large-scale diverse embodiment data. For these large-scale datasets (Bridge and RT-1 paper data), RT-1-X underfits and performs  worse than the Original Method and RT-1. \n        RT-2-X model with significantly many more parameters can obtain strong performance in these two evaluation scenarios.}         \n        \\label{tab:result_on_dataset_with_more_than_5k_episodes}\n        \\vspace{-3em}\n\\end{table}\n\n\\vspace{-0.2em}\n\\section{Experimental Results}\n\\vspace{-0.1em}\n\nOur experiments answer three questions about the effect of \\cro-embodiment training:\n(1) Can policies trained on our \\cro-embodiment dataset effectively enable positive transfer, such that co-training on data collected on multiple robots improves performance on the training task? \n(2) Does co-training models on data from multiple platforms and tasks improve generalization to new, unseen tasks? \n(3) What is the influence of different design dimensions, such as model size, model architecture or dataset composition, on performance and generalization capabilities of the resulting policy?\nTo answer these questions we conduct the total number of 3600 evaluation trials across 6 different robots.\n\n\\subsection{In-distribution performance across different embodiments}\n\nTo assess the ability of RT-X models to learn from \\cro-embodiment data, we evaluate performance on in-distribution tasks.  We split our evaluation into two types: evaluation on domains that have small-scale datasets (\\cref{fig:rt_x_barplot}), where we would expect transfer from larger datasets to significantly improve performance, and evaluation on domains that have large-scale datasets (\\cref{tab:result_on_dataset_with_more_than_5k_episodes}), where we expect further improvement to be more challenging.\nNote that we use the same robotics data \\emph{training} mixture (defined in Sec.~\\ref{sec:training_details}) for all the evaluations presented in this section. \nFor small-scale dataset experiments, we use Kitchen Manipulation~\\cite{dass2023jacoplay}, Cable Routing~\\cite{luo2023multistage}, NYU Door Opening~\\cite{pari2021surprising}, AUTOLab UR5 \\cite{BerkeleyUR5Website}, and Robot Play~\\cite{TacoPlayWebsite}. We use the same evaluation and robot embodiment as in the respective publications. \nFor large-scale dataset experiments, we consider Bridge~\\cite{walke2023bridgedata} and RT-1~\\cite{brohan2023rt1} for in-distribution evaluation and use their respective robots: WidowX and \\edrbot.\n\nFor each small dataset domain, we compare the performance of the RT-1-X model, and for each large dataset we consider both the RT-1-X and RT-2-X models. For all experiments, the models are co-trained on the full \\cro-embodiment dataset. Throughout this evaluation we compare with two baseline models: (1) The model developed by the creators of the dataset trained only on that respective dataset. This constitutes a reasonable baseline insofar as it can be expected that the model has been optimized to work well with the associated data; we refer to this baseline model as the \\emph{Original Method} model. (2) An RT-1 model trained on the dataset in isolation; this baseline allows us to assess whether the RT-X model architectures\nhave enough capacity to represent policies for multiple different robot platforms simultaneously, and whether co-training on multi-embodiment data leads to higher performance.\n\n\\textbf{Small-scale dataset domains} (\\cref{fig:rt_x_barplot}). RT-1-X outperforms Original Method trained on each of the robot-specific datasets on 4 of the 5 datasets, with a large average improvement, demonstrating domains with limited data benefit substantially from co-training on \\cro-embodiment data.\n\n\\begin{table*}\n        \\centering\n        \\scriptsize\n        \\setlength{\\tabcolsep}{3pt}\n        \\begin{tabular}{@{}ccccccccc@{}}\n        \\toprule\n        Row & Model & Size & History Length & Dataset & Co-Trained w/ Web & Initial Checkpoint & Emergent Skills Evaluation & RT-2 Generalization Evaluation \\\\\n        \\midrule\n        (1) & RT-2 & 55B & none & \\edrbot action & Yes & Web-pretrained & $27.3\\%$ & $\\textbf{62\\%}$ \\\\\n        (2) & RT-2-X & 55B & none & Robotics data & Yes & Web-pretrained & $\\textbf{75.8\\%}$ & $\\textbf{61\\%}$ \\\\\n (3) & RT-2-X & 55B & none & Robotics data except Bridge & Yes & Web-pretrained & $42.8\\%$ & $54\\%$ \\\\\n (4) & RT-2-X & 5B & 2 & Robotics data & Yes  & Web-pretrained & $44.4\\%$ & $52\\%$ \\\\\n (5) & RT-2-X & 5B & none & Robotics data & Yes & Web-pretrained & $14.5\\%$ & $30\\%$ \\\\\n (6) & RT-2-X & 5B & 2 & Robotics data & No  & From scratch & $0\\%$ & $1\\%$ \\\\\n(7) & RT-2-X & 5B & 2 & Robotics data & No  & Web-pretrained & $48.7\\%$ & $47\\%$ \\\\ \n        \\bottomrule\n        \\end{tabular}\n        \\vspace{-0.5em}\n        \\caption{\\small Ablations to show the impact of design decisions on generalization (to unseen objects, backgrounds, and environments) and emergent skills (skills from other datasets on the \\edrbot), showing the importance of Web-pretraining, model size, and history.} \n        \\label{tab:rt_2_x_generalization_and_ablation_results}\n        \\vspace{-3em}\n\\end{table*}\n\\textbf{Large-scale dataset domains} (\\cref{tab:result_on_dataset_with_more_than_5k_episodes}).\nIn the large-dataset setting, the RT-1-X model does not outperform the RT-1 baseline trained on only the embodiment-specific dataset, which indicates underfitting for that model class. \nHowever, the larger RT-2-X model outperforms both the Original Method and RT-1 suggesting that \\cro-robot training can improve performance in the data-rich domains, but only when utilizing a sufficiently high-capacity architecture. \n\n\\vspace{-0.2em}\n\\subsection{Improved generalization to out-of-distribution settings}\n\\vspace{-0.1em}\n\nWe now examine how \\cro-embodiment training can enable better generalization to out-of-distribution settings and more complex and novel instructions. These experiments focus on the high-data domains, and use the RT-2-X model.\n\n\\textbf{Unseen objects, backgrounds and environments.} \nWe first conduct the same evaluation of generalization properties as proposed in~\\cite{brohan2023rt2}, testing for the ability to manipulate unseen objects in unseen environments and against unseen backgrounds.\nWe find that RT-2 and RT-2-X perform roughly on par (\\cref{tab:rt_2_x_generalization_and_ablation_results}, rows (1) and (2), last column). This is not unexpected, since RT-2 already generalizes well (see~\\cite{brohan2023rt2}) along these dimensions due to its VLM backbone.\n\n\\textbf{Emergent skills evaluation.}\nTo investigate the transfer of knowledge across robots, we conduct experiments with the \\edrbot, assessing the performance on tasks like the ones shown in~\\cref{fig:emergent_skill}. These tasks involve objects and skills that are not present in the RT-2 dataset but occur in the Bridge dataset~\\cite{walke2023bridgedata} for a different robot (the \\textit{WidowX robot}).\nResults are shown in~\\cref{tab:rt_2_x_generalization_and_ablation_results}, Emergent Skills Evaluation column. Comparing rows (1) and (2), we find that RT-2-X outperforms RT-2 by  $\\sim3\\times$, suggesting that  incorporating data from other robots into the training improves the range of tasks that can be performed even by a robot that already has large amounts of data available. \nOur results suggest that co-training with data from other platforms imbues the RT-2-X controller with additional skills for the platform that are not present in that platform's original dataset. \n\nOur next ablation involves removing the Bridge dataset from RT-2-X training: Row (3) shows the results for RT-2-X that includes all data used for RT-2-X except the Bridge dataset. This variation significantly reduces performance on the hold-out tasks, suggesting that transfer from the \\textit{WidowX} data may indeed be responsible for the additional skills that can be performed by RT-2-X with the \\edrbot.\n\n\\vspace{-0.4em}\n\\subsection{Design decisions} \n\\vspace{-0.25em}\n\nLastly, we perform ablations to measure the influence of different design decisions on the generalization capabilities of our most performant RT-2-X model, which are presented in~\\cref{tab:rt_2_x_generalization_and_ablation_results}.\nWe note that including a short history of images significantly improves generalization performance (row (4) vs row (5)). \nSimilarly to the conclusions in the RT-2 paper~\\cite{brohan2023rt2}, Web-based pre-training of the model is critical to achieving a high performance for the large models (row (4) vs row (6)).\nWe also note that the $55B$ model has significantly higher success rate in the Emergent Skills compared to the $5B$ model (row (2) vs row (4)), demonstrating that higher model capacity enables higher degree of transfer across robotic datasets. Contrary to previous RT-2 findings, co-fine-tuning and fine-tuning have similar performance in both the Emergent Skills and Generalization Evaluation (row (4) vs row (7)), which we attribute to the fact that the robotics data used in RT-2-X is much more diverse than the previously used robotics datasets. \n\n\\vspace{-0.3em}\n\\section{Discussion, Future Work, and Open Problems}\n\\vspace{-0.3em}\n\\label{sec:conclusions}\n\nWe presented a consolidated dataset that combines data from $\\numembodiment$ robotic embodiments collected through a collaboration between $\\numinstitution$ institutions, demonstrating $\\numskill$ skills ($\\numtask$ tasks). We also presented an experimental demonstration that Transformer-based policies trained on this data can exhibit significant positive transfer between the different robots in the dataset. Our results showed that the RT-1-X policy has a $50\\%$ higher success rate than the original, state-of-the-art methods contributed by different collaborating institutions, while the bigger vision-language-model-based version (RT-2-X) demonstrated $\\sim3\\times$ generalization improvements over a model trained only on data from the evaluation embodiment. In addition, we provided multiple resources for the robotics community to explore the \\cro-embodiment robot learning research, including: the unified \\cro-robot and \\cro-institution dataset, sample code showing how to use the data, and the RT-1-X model to serve as a foundation for future exploration. \n\n\\begin{figure}[t]\n\\vspace{0.2em}\n    \\centering\n    \\includegraphics[width=0.4\\textwidth]{figures/rt2x_eval_v9.png}\n    \\vspace{-1.5em}\n    \\caption{\\small To assess transfer \\emph{between} embodiments, we evaluate the RT-2-X model on out-of-distribution skills. \n    These skills are in the Bridge dataset, but not in the \\edrbot dataset (the embodiment they are evaluated on).\n    }\n\\label{fig:emergent_skill}\n\\vspace{-2.3em}\n\\end{figure}\n\nWhile RT-X demonstrates a step towards a \\cro-embodied robot generalist, many more steps are needed to make this future a reality.\nOur experiments do not consider robots with very different sensing and actuation modalities.\nThey do not study generalization to new robots, and provide a decision criterion for when positive transfer does or does not happen. \nStudying these questions is an important future work direction. \nThis work serves not only as an example that \\cro-robot learning is feasible and practical, but also provide the tools to advance research in this direction in the future.\n\n\\end{document}\n</paper 3>\n\nYour summary should follow these steps:\n- Title: Clearly state the main subject or topic of the summary.\n- Introduction: Describe the field and briefly introduce its history. Then introduce current progress and challenges.\n- Introduce the main content of each paper separately. Then summarize their commonalities and innovations.\n- Compare the results of the papers and discuss difference of the results.\n- Conclusion: Summarize the main findings and suggest future research directions.\n\n<keys>\nThe following are the key points to note:\n- If there are important data or mian equation in the given papers, remember to mention them in your summary using Markdown.\n- Use of tables to clearly compare different approaches is encouraged.\n- The first appearance of a professional term must be marked with the full English name and abbreviation.\n- Don't directly copy the papers, write the summary in your own words.\n- Do not include the titles of reference papers directly in your paper.\n</keys>\n\nTotal word count should be about 4000 words.",
    "paper": [
        "arXiv-2209.05451v2.tex",
        "arXiv-2307.15818v1.tex",
        "arXiv-2310.08864v8.tex"
    ],
    "group_id": "group_27",
    "response": "### Title: Advances in Vision-Language-Action Models for Robotic Manipulation\n\n### Introduction\n\nThe field of robotic manipulation has seen significant advancements driven by the integration of machine learning techniques, particularly those leveraging large datasets and pre-trained models. Historically, robotic learning methods have often been tailored to specific tasks, robots, and environments, necessitating extensive and specialized data collection. However, recent trends in natural language processing (NLP) and computer vision (CV) have shown that large, pre-trained models can be adapted to a wide range of tasks with minimal fine-tuning, thanks to their broad exposure to diverse data. This has led to the development of generalist models like Gato~\\citep{reed2022generalist}, which can perform multiple tasks across different domains but require extensive data collection and training. In contrast, the challenge in robotic manipulation lies in the limited and expensive nature of data collection, making it difficult to apply the same level of generalization seen in NLP and CV to robotics. This summary focuses on three recent papers that explore the application of large pre-trained models to robotic manipulation, each presenting unique approaches and addressing specific challenges in the field.\n\nThe papers discussed here aim to bridge the gap between the rich, diverse datasets available in NLP and CV and the limited, specialized datasets common in robotics. They investigate how vision-language-action (VLA) models can be adapted for robotic manipulation, thereby enabling robots to learn from a broader range of experiences and improving their generalization capabilities. The first paper introduces \\model, a multi-task Transformer-based agent for 6-DoF manipulation, trained with a small number of demonstrations per task. The second paper presents \\methodname, a family of models derived from fine-tuning large vision-language models to output robot actions directly, thereby leveraging web-scale data to improve generalization and emergent capabilities. The third paper introduces the \\repo Repository, an open-source dataset and model checkpoints for cross-robot learning (CRO-embodiment), demonstrating the benefits of training on diverse robotic data.\n\n### Main Content of Each Paper\n\n#### \\model: A Multi-Task Transformer for Robotic Manipulation\n\nThe first paper introduces \\model, a language-conditioned behavior-cloning agent designed for multi-task 6-DoF manipulation. \\model uses a Perceiver Transformer~\\citep{jaegle2021perceiver} to encode language goals and RGB-D voxel observations, and it predicts discretized actions by detecting the next best voxel action. Unlike traditional image-to-action frameworks, \\model's voxelized 3D observation and action space provides a strong structural prior for efficiently learning 6-DoF actions. This formulation allows \\model to train a single multi-task Transformer on a diverse set of tasks, including 18 RLBench tasks with 249 variations and 7 real-world tasks with 18 variations, from just a few demonstrations per task.\n\nThe key innovation of \\model is its voxel-based formulation, which enables the fusion of multi-view observations, learning robust action-centric representations, and data augmentation in 6-DoF manipulation. The agent is trained through supervised learning with discrete-time input-action tuples, which consist of voxel observations, language goals, and keyframe actions. The training process involves randomly sampling tuples from all tasks in the dataset, ensuring that longer-horizon tasks are not over-represented during sampling. This approach significantly outperforms unstructured image-to-action frameworks and 3D ConvNet baselines, achieving an average improvement of $1.33\\times$ with 10 demonstrations and $2.83\\times$ with 100 demonstrations.\n\n#### \\methodname: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control\n\nThe second paper, \\methodname, explores the integration of large vision-language models into robotic control tasks. The authors train these models to output robot actions directly, enabling them to leverage web-scale pretraining data to improve generalization and emergent capabilities. \\methodname uses a co-fine-tuning strategy, where the model is trained on both web-scale vision-language data and robotic trajectory data. This approach allows the model to retain its broad visual and semantic understanding while learning to perform specific robotic tasks.\n\nThe paper evaluates the performance of \\methodname on a variety of tasks, including seen tasks and unseen tasks with novel objects, backgrounds, and environments. The results show that \\methodname significantly outperforms baselines like RT-1~\\citep{brohan2022rt} and VC-1~\\citep{majumdar2023vc1} in generalization scenarios, with the best model achieving more than 3x average success rate over the next best baseline. Additionally, \\methodname exhibits emergent capabilities, such as reasoning about object relations and understanding complex instructions, which are not explicitly present in the training data. These capabilities are attributed to the web-scale pretraining of the vision-language models, which imbue the policy with a broader understanding of the world.\n\n#### \\repo Repository: Cross-Robot Learning Datasets and RT-X Models\n\nThe third paper introduces the \\repo Repository, an open-source dataset and model checkpoints for cross-robot learning (CRO-embodiment). The repository contains over 1M real robot trajectories from $\\numembodiment$ different robot embodiments, collected through a collaboration between $\\numinstitution$ institutions. The dataset is processed and aggregated into a standardized format, enabling researchers to explore the benefits of training on diverse robotic data.\n\nThe authors evaluate the impact of CRO-embodiment training on the performance of learned policies using two model architectures: RT-1~\\citep{brohan2023rt1}, a Transformer-based model designed for robotic control, and RT-2~\\citep{brohan2023rt2}, a large vision-language-action model. The results show that RT-1-X, a variant of RT-1 trained on the CRO-embodiment dataset, outperforms the original method trained only on the embodiment-specific dataset on 4 out of 5 small-scale datasets. For large-scale datasets, the larger RT-2-X model outperforms both the original method and RT-1, indicating that CRO-embodiment training can improve performance even in data-rich domains, but only when utilizing a sufficiently high-capacity architecture.\n\n### Commonalities and Innovations\n\nAll three papers explore the use of large pre-trained models to enhance robotic manipulation policies. \\model and \\methodname both leverage Transformers, with \\model using a Perceiver Transformer to efficiently encode high-dimensional voxel inputs, and \\methodname using vision-language models to output robot actions directly. \\repo Repository, on the other hand, focuses on the consolidation of diverse robotic datasets and the evaluation of policies trained on this data.\n\nThe primary innovation of \\model is its voxel-based formulation, which enables the efficient learning of 6-DoF actions from limited demonstrations. \\methodname innovates by directly training vision-language models to output robot actions, thereby leveraging web-scale pretraining data to improve generalization and emergent capabilities. \\repo Repository provides a standardized dataset and model checkpoints for CRO-embodiment research, demonstrating the benefits of training on diverse robotic data.\n\n### Comparison of Results\n\nThe performance of \\model, \\methodname, and RT-X models is evaluated across different tasks and scenarios. \\model significantly outperforms image-to-action baselines and 3D ConvNet models in both simulated and real-world tasks, achieving an average improvement of $1.33\\times$ with 10 demonstrations and $2.83\\times$ with 100 demonstrations. \\methodname shows a substantial improvement in generalization performance over baselines like RT-1 and VC-1, with the best model achieving more than 3x average success rate over the next best baseline. RT-X models, particularly RT-2-X, demonstrate significant positive transfer between robots, outperforming the original method and RT-1 on unseen tasks and objects.\n\n### Conclusion\n\nThe main findings of these papers highlight the potential of large pre-trained models to enhance robotic manipulation policies. \\model demonstrates the effectiveness of voxel-based formulations for efficient learning of 6-DoF actions, while \\methodname shows the benefits of directly training vision-language models to output robot actions. \\repo Repository provides a standardized dataset and model checkpoints for CRO-embodiment research, demonstrating the positive transfer of skills across different robots.\n\nFuture research directions include exploring the integration of web-scale pretraining with more diverse robotic datasets, studying the impact of different design decisions on the performance of VLA models, and investigating the transfer of skills to robots with different sensing and actuation modalities. Additionally, the development of more efficient inference protocols and the exploration of quantization and distillation techniques for VLA models could enable real-time control in high-frequency settings. These advancements could further consolidate the field of robotic manipulation, making it more accessible to a broader range of applications and environments."
}